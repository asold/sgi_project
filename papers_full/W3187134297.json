{
  "title": "Pretrained Language Model for Text Generation: A Survey",
  "url": "https://openalex.org/W3187134297",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2129999472",
      "name": "Junyi Li",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2133263866",
      "name": "Tianyi Tang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2307999729",
      "name": "Wayne Xin Zhao",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962785754",
    "https://openalex.org/W4287749601",
    "https://openalex.org/W3022903564",
    "https://openalex.org/W2949178656",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2794421626",
    "https://openalex.org/W3034961030",
    "https://openalex.org/W3004304303",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W3106321930",
    "https://openalex.org/W4289549143",
    "https://openalex.org/W2963227052",
    "https://openalex.org/W3035317912",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3209274285",
    "https://openalex.org/W4288624561",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W3119558324",
    "https://openalex.org/W3090516735",
    "https://openalex.org/W3177423701",
    "https://openalex.org/W3100124323",
    "https://openalex.org/W3035565536",
    "https://openalex.org/W2985808369",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W3116342879",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3153729783",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3104777900",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4229543565",
    "https://openalex.org/W3093669700",
    "https://openalex.org/W2982095018",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3105912780",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3098427234",
    "https://openalex.org/W3034731179",
    "https://openalex.org/W3173273620",
    "https://openalex.org/W3105245805",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2950342809"
  ],
  "abstract": "Text generation has become one of the most important yet challenging tasks in natural language processing (NLP). The resurgence of deep learning has greatly advanced this field by neural generation models, especially the paradigm of pretrained language models (PLMs). In this paper, we present an overview of the major advances achieved in the topic of PLMs for text generation. As the preliminaries, we present the general task definition and briefly describe the mainstream architectures of PLMs for text generation. As the core content, we discuss how to adapt existing PLMs to model different input data and satisfy special properties in the generated text. We further summarize several important fine-tuning strategies for text generation. Finally, we present several future directions and conclude this paper. Our survey aims to provide text generation researchers a synthesis and pointer to related research.",
  "full_text": "Pretrained Language Models for Text Generation: A Survey\nJunyi Li1;3y, Tianyi Tang2y, Wayne Xin Zhao1;3\u0003 and Ji-Rong Wen1;2;3\n1Gaoling School of Artiﬁcial Intelligence, Renmin University of China\n2School of Information, Renmin University of China\n3Beijing Key Laboratory of Big Data Management and Analysis Methods\nflijunyi,steven tang,jrweng@ruc.edu.cn, batmanﬂy@gmail.com\nAbstract\nText generation has become one of the most im-\nportant yet challenging tasks in natural language\nprocessing (NLP). The resurgence of deep learning\nhas greatly advanced this ﬁeld by neural generation\nmodels, especially the paradigm of pretrained lan-\nguage models (PLMs). In this paper, we present\nan overview of the major advances achieved in the\ntopic of PLMs for text generation. As the pre-\nliminaries, we present the general task deﬁnition\nand brieﬂy describe the mainstream architectures\nof PLMs for text generation. As the core content,\nwe discuss how to adapt existing PLMs to model\ndifferent input data and satisfy special properties\nin the generated text. We further summarize sev-\neral important ﬁne-tuning strategies for text gen-\neration. Finally, we present several future direc-\ntions and conclude this paper. Our survey aims to\nprovide text generation researchers a synthesis and\npointer to related research.\n1 Introduction\nText generation, which is often formally referred as natural\nlanguage generation, has become one of the most important\nyet challenging tasks in natural language processing (NLP).\nIt aims to produce plausible and readable text in human lan-\nguage from input data (e.g., a sequence and keywords). Re-\nsearchers have developed numerous techniques for a wide\nrange of applications of text generation [Li et al., 2021a ].\nFor example, machine translation generates text in a differ-\nent language based on the source text [Yang et al., 2020a ];\nsummarization generates an abridged version of the source\ntext to include salient information [Guan et al., 2020].\nWith the recent resurgence of deep learning, various works\nhave been proposed to solve text generation tasks based on\nrecurrent neural networks (RNN) [Li et al., 2019 ], convolu-\ntional neural networks (CNN) [Gehring et al., 2017 ], graph\nneural networks (GNN) [Li et al., 2020], and attention mech-\nanism [Bahdanau et al., 2015 ]. One of the advantages of\nthese neural models is that they enable end-to-end learning\nyEqual contribution.\n\u0003Corresponding author.\nof semantic mappings from input to output in text generation.\nBesides, neural models are able to learn low-dimensional,\ndense vectors to implicitly represent linguistic features of\ntext, which is also useful to alleviate data sparsity.\nDespite the success of neural models for text generation, a\nmajor performance bottleneck lies in the availability of large-\nscale datasets. Existing datasets for most of supervised text\ngeneration tasks are rather small (except machine transla-\ntion). Deep neural networks usually have a large number of\nparameters to learn, which are likely to overﬁt on these small\ndatasets and do not generalize well in practice.\nIn recent years, the paradigm of pretrained language mod-\nels (PLMs) is thriving [Peters et al., 2018 ]. The idea is to\nﬁrst pretrain the models in large-scale corpus and then ﬁne-\ntune these models in various downstream tasks to achieve\nstate-of-the-art results. It is widely recognized that PLMs\ncan encode a large amount of linguistic knowledge from cor-\npus and induce universal representations of language. There-\nfore, PLMs are generally beneﬁcial for downstream tasks\nand can avoid training a new model from scratch [Brown\net al., 2020 ]. Moreover, with the increasing of computa-\ntional power and the emergence of Transformer architec-\nture [Vaswani et al., 2017], PLMs have advanced from shal-\nlow to deep and achieved outstanding performance in many\ntasks, such as BERT [Devlin et al., 2019 ] and GPT [Rad-\nford et al., 2019]. Therefore, researchers have proposed vari-\nous methods to solve text generation tasks based on PLMs.\nPretrained on large-scale corpus, PLMs are able to under-\nstand natural language accurately and express in human lan-\nguage ﬂuently, both of which are critical abilities to fulﬁll\nthe text generation tasks. Existing surveys in this area have\nonly partially reviewed some related topics. Zaibet al. [2020]\nand Guan et al. [2020] provided a synthesis to the research\non some text generation subtasks, i.e., dialogue systems and\nsummarization, but did not go broader to the other important\ngeneration tasks. Qiu et al. [2020] summarized two gener-\nations of PLMs for the whole NLP domain and introduced\nvarious extensions and adaption approaches of PLMs. To\nthe best of our knowledge, our survey is the ﬁrst work that\npresents a comprehensive review of PLMs for text generation.\nIt aims to provide text generation researchers a synthesis and\npointer to related research.\nTo start with, we ﬁrst present a general task deﬁnition with\nthe formulations of different text generation tasks in Sec-\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSurvey Track\n4492\ntion 2, and then brieﬂy describe the mainstream architectures\nof PLMs that are used in text generation in Section 3. Since\nthe core of text generation is to model the semantic mappings\nfrom input to output, we further organize the major advances\nwith respect to the two aspects of input and output in Sec-\ntion 4-5. For input, we mainly discuss how to adapt existing\nPLMs to different data types. For output, we study how to\nsatisfy special properties for the generated text. Furthermore,\nwe summarize several important ﬁne-tuning strategies for text\ngeneration in Section 6. Finally, we present several future di-\nrections and conclude this paper in Section 7.\n2 Task and Typical Applications\nIn what follows, we formally deﬁne the text generation task.\nThe core of text generation is to generate a sequence of dis-\ncrete tokens Y = hy1; : : : ; yj; : : : ; yni, where each yj is\ndrawn from a word vocabulary V. In most cases, text gen-\neration is conditioned on input data, such as attributes, text\nand structured data, which is denoted as X . Formally, the\ntext generation task can be described as:\nP(YjX ) =P(y1; : : : ; yj; : : : ; ynjX ): (1)\nAccording to input X , we next introduce several typical\napplications of text generation:\n• If X is not provided or a random noise vector z, this\ntask will degenerate into language modeling or unconditional\ngeneration task [Radford et al., 2019], which aims to generate\ntext without any constraint.\n• If X is a set of discrete attributes (e.g., topic words, sen-\ntiment labels), the task becomes topic-to-text generation or\nattribute-based generation[Keskar et al., 2019]. The informa-\ntion in X plays the role of guiding the text generation process\nand controlling the modes of the generated text.\n• IfX is structured data like knowledge graph or table, this\ntask will be considered as KG-to-text or table-to-text gener-\nation, called data-to-text generation [Li et al., 2021c ]. This\ntask aims to generate descriptive text about structured data.\n• If X is multimedia input such as image and speech, the\ntask becomes image caption [Xia et al., 2020 ] or speech\nrecognition [Fan et al., 2019]. The core of image caption is to\ngenerate a description of an image, while speech recognition\nenables programs to process human speech into a text format.\n• The most common form of X is also a text sequence,\nand there exist several applications such as machine trans-\nlation, summarization and dialogue system. Machine trans-\nlation [Conneau and Lample, 2019 ] aims to translate text\nfrom one language into another language automatically, sum-\nmarization [Zhang et al., 2019b ] is focused on generating\ncondensed summary of a long document, and dialogue sys-\ntem [Wolf et al., 2019] is designed to converse with humans\nusing natural language.\nWe present the formulations for the major text generations\nin Table 1.\n3 Standard Architectures for Text Generation\nPretrained language models (PLMs) are pretrained with a\nmass of unlabelled text data and can be ﬁne-tuned on down-\nstream generation tasks. Pretrained on large-scale corpus,\nInput X Tasks\nRandom noise Unconditional text generation\nDiscrete attributes\nTopic-to-text generation\nAttribute-based generation\nStructured data Data-to-text generation\nMultimedia\nImage caption\nSpeech recognition\nText sequence\nMachine translation\nSummarization\nDialogue system\nTable 1: Major tasks and inputs for text generation.\nPLMs encode massive linguistic and world knowledge into\nvast amounts of parameters, which can enhance the under-\nstanding of language and improve the generation quality. The\nidea of pretraining is inspired by human beings,i.e., we trans-\nfer and reuse our old knowledge of what we have learned in\nthe past to understand new knowledge and handle a variety\nof new tasks. In this way, PLMs can successfully perform on\nnew tasks with their old experience and knowledge.\nOwing to the great achievements that Trans-\nformer [Vaswani et al., 2017 ] has made, almost all PLMs\nemploy the backbone of Transformer. For the text generation\ntasks, some of PLMs utilize the standard Transformer\narchitecture following basic encoder-decoder framework,\nwhile the others apply a decoder-only Transformer. Next, we\nwill introduce these two methods successively.\nEncoder-decoder Transformer. A standard Transformer\nutilizes the encoder-decoder architecture, which is composed\nof two stacks of Transformer blocks. The encoder is fed with\nan input sequence, while the decoder aims to generate the out-\nput sequence based on encoder-decoder self-attention mech-\nanism. Based on aforementioned architecture, models such\nas MASS [Song et al., 2019 ], T5 [Raffel et al., 2020 ], and\nBART [Lewis et al., 2020] have improved quality of the gen-\nerated text.\nDecoder-only Transformer. Models such as GPT [Rad-\nford et al., 2019; Brown et al., 2020 ] and CTRL [Keskar\net al., 2019 ] employ a single Transformer decoder blocks,\nwhich is typically used for language modeling. They apply\nunidirectional self-attention masking that each token can only\nattend to previous tokens. Besides language modeling, sev-\neral works also utilize the decoder-only achitecture to gen-\nerate text conditioned on input text. However, these mod-\nels do not have an independent module to encode input se-\nquence. Interestingly, they concatenate the input and output\nsequence with a special token (e.g., “[SEP]”) and employ a\nnovel seq2seq masking [Dong et al., 2019] that each token in\nthe input sentence can attend to each other and generated to-\nkens can attend to all input tokens and previous generate ones.\nCompared to unidirectional masking, seq2seq masking is a\nnatural way for decoder-only PLMs to solve conditional gen-\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSurvey Track\n4493\neration tasks, which is similar to the encoder-decoder archi-\ntecture. Raffel et al. [2020] has researched the performance\nbetween the above two methods and made a conclusion that\nthe addition of an explicit encoder-decoder attention is bene-\nﬁcial.\nThe core of text generation tasks is to learn the semantic\nmappings from input to output. On one hand, different tasks\nwill correspond to a variety of input data, and we need to\ndevelop special techniques to model different data types. On\nthe other hand, the generated text should satisfy important\nproperties in order to cope with different task requirements.\nNext, we discuss the recent advances with respect to the two\naspects, i.e., input and output.\n4 Modeling Different Data Types from Input\nAs discussed in Section 2, different text generation tasks usu-\nally involve speciﬁc kinds of input. In this section, we will\nintroduce three main kinds of input for text generation, i.e.,\nunstructured input, structured input, and multimedia input,\nand discuss how to model these input data in PLMs.\n4.1 Unstructured Input\nIn NLP research, most of studies focus on modeling unstruc-\ntured text input (e.g., sentence, paragraph, and document).\nTo generate satisfactory output text, it requires an excellent\ncapacity of language understanding beyond surface meaning\nof individual words in the input text. Thus, Liu and Lap-\nata [2019] and Zheng and Lapata [2019] employed PLMs\n(e.g., BERT [Devlin et al., 2019 ]) as text encoder for con-\ndensing text into low-dimensional vectors while preserving\nmost of its meaning. Compared with traditional shallow neu-\nral models (e.g., CNN), PLMs have a large number of pa-\nrameters encoding massive world knowledge, which is po-\ntentially beneﬁcial to capture the core meaning of text.\nIn some cases, the input text might be a long document\nconsisting of several sentences and paragraphs. For PLMs\ntrained on sentences or short paragraphs, they are less capa-\nble of accurately modeling long-range dependencies in a doc-\nument. Considering this challenge, Zhang et al. [2019b] and\nXu et al. [2020b] proposed hierarchical BERT to learn inter-\nactions between sentences with self-attention for document\nencoding. Besides, for capturing inter-sentential relations,\nDiscoBERT [Xu et al., 2020a ] stacked graph convolutional\nnetwork (GCN) on top of BERT to model structural discourse\ngraphs. By directly operating on the discourse units, Dis-\ncoBERT retains capacities to include more concepts or con-\ntexts, leading to more concise and informative output text.\nWe observe that most recent PLMs are pretrained on En-\nglish text. While, many multilingual generation tasks such\nas machine translation involve multiple languages and cer-\ntain languages are low-resource. This challenge hinders the\nwide application of monolingual PLMs to multilingual text\ngeneration tasks. Therefore, Conneau and Lample [2019]\nproposed to learn cross-lingual language models (XLMs) for\nmultilingual language understanding. Based on cross-lingual\nPLMs, text generation models can still obtain effective input\nword embeddings even in a low-resource language[Wada and\nIwata, 2018].\n4.2 Structured Input\nStructured data (e.g., graph and table) is also a critical kind\nof input for text generation in many real-world applications\nsuch as weather report generation. However, in real-world\nscenario, it is difﬁcult to collect a large amount of labelled\nstructured data with ground-truth text for training. Since pre-\ntrained on large-scale corpus, PLMs encode a large amount\nof linguistic knowledge and show excellent few-shot capabil-\nities in many tasks. Motivated by this, Chenet al. [2020b] and\nGong et al. [2020] explored incorporating PLMs for data-to-\ntext generation, especially in few-shot settings.\nWhen applying PLMs to structured data, a major challenge\nis how to feed structured data into PLMs, which are originally\ndesigned for sequential text. To adapt to the sequential nature\nof PLMs, Ribeiro et al. [2020] and Mager et al. [2020] lin-\nearized input knowledge graph (KG) and abstract meaning\nrepresentation (AMR) graph into a sequence of triples, Li et\nal. [2021b] introduced an additional graph encoder to encode\nthe input KG, and Gong et al. [2020] employed a template-\nbased method to serialize input table into text sequence. For\nexample, the attribute-value pair “name: jack reynolds” will\nbe serialized as a sentence “name is jack reynolds”. How-\never, direct linearization will lose the structural information\nof original data, which may lead to generating unfaithful text\nabout data. Thus, in addition to generating faithful text, Gong\net al. [2020] proposed an auxiliary reconstruction task for re-\ncovering the structural information of input data, which can\nenhance the capacity of modeling structural information.\nIn general, the output text should retain as much as impor-\ntant information from structured data. Therefore, to gener-\nate high-ﬁdelity text adhereing to input, the pointer genera-\ntor mechanism [See et al., 2017 ] is adopted to copy words\nfrom input knowledge data [Chen et al., 2020b ]. Through\ngrounding PLMs on external knowledge, it is likely to endow\na generative model with both rich knowledge and good gen-\neralization ability. Besides, Gong et al. [2020] proposed a\ncontent matching loss for measuring the distance between the\ninformation in input data and the output text.\n4.3 Multimedia Input\nIn addition to the above textual data, several attempts have\nbeen made to take as input multimedia data (e.g., image,\nvideo, and speech) such as image caption and speech recog-\nnition. Both VideoBERT [Sun et al., 2019b ] and CBT [Sun\net al., 2019a ] conducted pretraining for the video caption\ntask. While, they performed pretraining only for the BERT-\nbased encoder to learn bidirectional joint distributions over\nsequences of visual and linguistic tokens. So they have\nto train a separate video-to-text decoder, which tends to\ncause a pretrain-ﬁnetune discrepancy. In contrast, Uniﬁed\nVLP [Zhou et al., 2020 ] used a shared multi-layer Trans-\nformer network for both encoding and decoding. Following\nUniLM [Dong et al., 2019], they pretrained the model on two\nmasked language modeling (MLM) tasks, like cloze tasks de-\nsigned for sequence-to-sequence LM. Inspired by generative\npretraining objectives in GPT, Xia et al. [2020] proposed a\ncross-modal pretrained model (XGPT) by taking images as\ninputs and using the image caption task as the basic genera-\ntive task in the pretraining stage.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSurvey Track\n4494\nBesides image and video, speech recognition is also hun-\ngry for human-transcripted supervised data. So a number of\nunsupervised and semi-supervised methods are developed to\nintegrate PLMs for weakly-supervised learning. For exam-\nple, Fan et al. [2019] proposed an unsupervised approach to\npretraining encoder-decoder model with unpaired speech and\ntranscripts. Two pretraining stages are used to extract acous-\ntic and linguistic information with speech and transcripts,\nwhich is useful for downstream speech recognition task.\n5 Satisfying Special Properties for Output\nText\nIn different text generation tasks, the generated text should\nsatisfy several key properties. In this section, we will intro-\nduce three key properties in text generation, i.e., relevance,\nfaithfulness, and order-preservation.\nRelevance. According to the linguistic literatures [Li et al.,\n2021c], in text generation, relevance refers that the topics\nin output text is highly related to the input text. A repre-\nsentative example is the task of dialogue systems, which re-\nquires the generated response to be relevant to the input di-\nalogue history. In addition to the dialogue history, a con-\ndition corresponding to the type of response might be also\nprovided as an external input such as the topic of response\nand the persona of speaker. The generated responses should\nalso be relevant to the condition. Recently, due to the ab-\nsence of long-term memory, RNN-based models still tend to\ngenerate irrelevant output text and lack consistency with in-\nput. Therefore, through applying PLMs to the task of dia-\nlogue systems, TransferTransfo [Wolf et al., 2019 ] and Di-\naloGPT [Zhang et al., 2020] were able to generate more rel-\nevant and context-consistent responses than traditional RNN-\nbased models. Furthermore, to generalize to various types\nof conditions, Zeng and Nie [2020] utilized elaborated con-\ndition blocks to incorporate external conditions. They used\nBERT for both encoder and decoder by utilizing different in-\nput representations and self-attention masks to distinguish the\nsource and target sides of dialogue. On the target (generation)\nside, a new attention routing mechanism is adopted to gener-\nate context-related words. Similar approaches have been used\nin non-conditioned dialogue [Bao et al., 2020].\nFaithfulness. Similarly, faithfulness is also a critical prop-\nerty of text, which means the content in generated text should\nnot contradict the facts in input text. Sometimes, it further\nmeans the generated text is in accord with the world facts.\nA representative example is the task of text summarization,\nwhich aims to generate faithful text representing the most im-\nportant information within the original content. Pretrained\non large collections of text, PLMs are potentially beneﬁcial\nto generate faithful text by utilizing background knowledge.\nRothe et al. [2020] experimented with a large number of set-\ntings to initialize the encoder and decoder with three out-\nstanding PLMs, i.e., BERT, GPT and RoBERTa. With pre-\ntraining, the models are more aware of the domain character-\nistics and less prone to language model vulnerabilities. Con-\nsequently, they are more conﬁdent in predicting tokens from\nthe document, hence, improving faithfulness. To improve the\nlevel of faithfulness of summary, Kryscinskiet al. [2018] pro-\nposed to decompose the decoder into a contextual network\nthat retrieves relevant parts of the source document and a\nPLM that incorporates prior knowledge about language gen-\neration. Also, to generate faithful text in different target do-\nmains, Yang et al. [2020b] ﬁne-tuned PLMs on target do-\nmains through theme modeling loss. The role of the theme\nmodeling module is to make the generated summary seman-\ntically close to the original article.\nOrder-preservation. In NLP area, order-preservation de-\nnotes that the order of semantic units (word, phrase, etc.)\nin both input and output text is consistent. The most repre-\nsentative example is the task of machine translation. When\ntranslating from source language to target language, keep-\ning the order of phrases consistent in source language and\ntarget language will ensure the accuracy of the translation\nresults to some extent. One line of research to achieve\nthe order-preservation property is to perform semantic align-\nment in machine translation. Yang et al. [2020a] proposed\nCode-Switching Pre-training (CSP) for machine translation.\nThey extracted the word-pair alignment information from the\nsource and target language, and then applied the extracted\nalignment information to enhance order-preserving. Besides,\nit is more common to perform translation across multiple\nlanguages, called multilingual machine translation [Conneau\nand Lample, 2019]. However, little work can effectively en-\nhance order-preservation for any pairs of languages. Thus,\nLin et al. [2020] proposed mRASP, an approach to pretrain-\ning a universal multilingual machine translation model. The\nkey to mRASP is the technique of randomly aligned substi-\ntution, which enforces words and phrases with similar mean-\nings across multiple languages to be aligned in the representa-\ntion space. Also, Wada and Iwata [2018] focused on aligning\nword representations of each language, making it possible to\npreserve the word order consistent cross multiple languages.\n6 Fine-tuning Strategies for Text Generation\nFor text generation with PLMs, a key factor is how to design\nsuitable ﬁne-tuning strategies. In this part, we review several\ncommonly-used ﬁne-tuning strategies from different views.\n6.1 Data View\nWhen applying PLMs to text generation tasks especially in a\nnew domain, how to design suitable and effective ﬁne-tuning\nstrategies adapting to the characteristics of new domain is an\nimportant consideration.\nFew-shot Learning. In many text generations, it is difﬁcult\nand expensive to obtain sufﬁcient annotated data. Owing to\nthe success of pretraining, PLMs can encode massive linguis-\ntic and world knowledge, which provides an effective solution\nto data scarcity. A commonly adopted approach is to plug the\nexisting module with pretrained parameters. Then we ﬁne-\ntune it with a few, one, or even no examples for the studied\ntask, which are so-called few-shot, one-shot and zero-shot,\nrespectively. For example in multilingual translation, some\nlow-resource languages lack sufﬁcient parallel corpus. XLM\n[Conneau and Lample, 2019] proposed to learn cross-lingual\nlanguage models and can leverage the knowledge learned\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSurvey Track\n4495\nData Categories Methods\nInput\nUnstructured\nBERT acts as text encoders [Liu and Lapata, 2019; Zheng and Lapata, 2019],\nhierarchical PLMs for document modeling [Zhang et al., 2019b; Xu et al., 2020b], and cross-\nlingual PLMs for multilingual input text [Conneau and Lample, 2019; Wada and Iwata, 2018].\nStructured\nLinearize KG and AMR graph as triple sequence [Mager et al., 2020; Ribeiro et al., 2020],\ngraph encoder for encoding KG [Li et al., 2021b], and serialize table into template-based\ntext sequence [Gong et al., 2020].\nMultimedia Video caption [Sun et al., 2019b; Sun et al., 2019a], image caption [Xia et al., 2020],\nand speech recognition [Fan et al., 2019].\nOutput\nRelevance\nFine-tune PLMs in dialogue systems for generating more relevant and context related responses\n[Wolf et al., 2019; Zhang et al., 2020], and generalize to any type of input conditions based on\nBERT [Zeng and Nie, 2020].\nFaithfulness\nImprove faithfulness with several PLMs [Rothe et al., 2020], retrieve relevant parts from input\nand incorporate prior knowledge of PLMs [Kryscinski et al., 2018], and generate faithful text in\ndifferent target domains through theme modeling loss [Yang et al., 2020b].\nOrder-\npreservation\nWord-pair alignment [Yang et al., 2020a], a universal multilingual machine translation model\n[Lin et al., 2020], and word representation alignment [Wada and Iwata, 2018].\nTable 2: Categories of input types and output properties for text generation.\nin high-resource languages to low-resource languages. Us-\ning the method proposed in Section 4, few-shot learning can\nalso be applied in data-to-text tasks, such as table-to-text\ngeneration[Chen et al., 2020b; Gong et al., 2020] and KG-to-\ntext generation[Li et al., 2021b]. Chen et al. [2020b] directly\nfed GPT-2 with a small amount of serialized attribute-value\npairs and Gong et al. [2020] further applied multiple tasks to\nbetter leverage structured information of tables. Moreover,\nLi et al. [2021b] proposed representation alignment to bridge\nthe semantic gap between KG encodings and PLMs in order\nto enhance the correspondence between KG and text.\nDomain Transfer. Equipped with vast amounts of parame-\nters and pretrained on large-scale corpus, PLMs have pow-\nerful generalization capability. However, they still cannot\ndirectly adapt to new domains with large distribution dis-\ncrepency from pretraining domain [Hendrycks et al., 2020 ].\nAn effective solution is to continue training PLMs on spe-\nciﬁc data with pretraining objectives before ﬁne-tuning them\non target tasks. Mask prediction is a widely used method,\nattempting to predict the masked tokens using the remaining\nones. There exist several variants of masking ways in domain\ntransfer. Zeng and Nie [2020] proposed TF-IDF based mask-\ning to select more condition-related tokens to mask, in order\nto focus on domain features. Document masking is usually\nutilized in summarization task to capture document-level fea-\ntures of long documents [Zhang et al., 2019b].\n6.2 Task View\nBesides characteristics of new domains, it is also meaningful\nto consider some special concerns such as language coher-\nence and text ﬁdelity in speciﬁc generation tasks when ﬁne-\ntuning PLMs.\nEnhancing Coherence. To enhance the language coher-\nence, an important approach is to better model language con-\ntext during ﬁne-tuning. Models ﬁne-tuned by contrastive\nlearning are good at distinguishing whether a sentence pair\nis similar or not. Through this method, PLMs are forced to\nunderstand the positional or semantic relationship between\ntwo sentences, so that they can derive better representations.\nNext sentence prediction (NSP) is a commonly adopted way\nto judge whether two input sentences are consecutive seg-\nments, which can be applied to summarization [Yang et al.,\n2020b] and dialog system [Wolf et al., 2019]. Zheng and La-\npata [2019] proposed to rearrange the sentence order accord-\ning to their semantic similarities. CBT [Sun et al., 2019a ]\nproposed noise contrastive estimation (NCE) in cross-modal\ntraining to encourage the model to identify the correct video-\ntext pair compared to a set of negative distractors. Denois-\ning autoencoding (DAE) takes the corrupted text as input and\naims to recover the original text. The model ﬁne-tuned with\nDAE has a strong ability to understand the overall sentences\nand capture longer-range correlations. For example, TED\n[Yang et al., 2020b] utilized DAE to reﬁne essential seman-\ntic information for abstractive summarization. XGPT [Xia\net al., 2020 ] attempted to model the underlying text-image\nalignments using image-conditioned denoising autoencoding\n(IDA), in order to force the model to reconstruct the whole\nsentence.\nPreserving Fidelity. Text ﬁdelity refers that how the gen-\nerated text adheres to the original input information, which is\nan important aspect to consider in many text generation tasks.\nThe universal structure in PLMs is unable to retain the text ﬁ-\ndelity in speciﬁc text generation tasks. For the table-to-text\ngeneration task, the structure information of table is required\nto be encoded. Gong et al. [2020] proposed to utilize multi-\ntask learning, in order to reconstruct from table embeddings\nand enforce the match between table embeddings and content\nembeddings. Besides, the pointer generator [See et al., 2017]\ncan be applied to KG-to-text generation to copy the entity and\nrelation information in KG [Chen et al., 2020b].\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSurvey Track\n4496\n6.3 Model View\nTo enhance the quality of generated text, a key is to well train\nthe parameters of PLMs according to task-speciﬁc data, so\nthat PLMs can capture the semantic characteristics specially\nfor the generation task. However, as mentioned above, task-\nspeciﬁc data is inadequate, thus it is likely to occur the over-\nﬁtting case when ﬁne-tuned on limited data. In this part, we\nwill introduce several ﬁne-tuning methods in view of models.\nGu et al. [2020] employed a ﬁxed teacher GPT to preserve\nthe knowledge encoded in another ﬁne-tuned GPT. Chen et\nal. [2020a] proposed to utilize a BERT model (teacher) as\nsupervision to guide the Seq2Seq model (student) for better\ngeneration performance. Besides, Liu and Lapata [2019] uti-\nlized two optimizers to update the parameters of PLM and\ninitial module separately, in order to solve the discrepancy\nbetween two modules. There also exist other ways to guide\nthe ﬁne-tuning process. For example, Reinforcement learning\ncan be applied to directly guide models by non-differentiable\nmetrics [Zhang et al., 2019a ], such as ROUGE. Zhao et\nal. [2020] utilized curriculum learning to let the model learn\nfrom easy documents to hard documents. Moreover, DI-\nALOGPT [Zhang et al., 2020] implemented a maximum mu-\ntual information (MMI) scoring function to alleviate generat-\ning bland, uninformative responses.\n7 Conclusion and Future Outlooks\nThis paper presents an overview of the recent advances\nachieved in pretrained language models for text generation.\nWe mainly summarize the extensions of PLMs in modeling\ndifferent data types in input and satisfy special text proper-\nties in output. We also discussed several useful ﬁne-tuning\nstrategies for text generation.\nTo advance this ﬁeld, there are several promising future\ndirections for applying PLMs to text generation.\nModel Extension. Although various extensions have been\nproposed in Section 3, there still exist discrepancies between\npretraining and downstream generation tasks. For example,\nthe “[MASK]” token in pretraining stage will not be used in\nﬁne-tuning stage, which further aggravates the pretraining-\nﬁnetuning discrepancy. Thus, it further desires to design an\nappropriate pretraining paradigm for text generation. More-\nover, incorporating external knowledge into PLMs during\npretraining has been shown to be effective [Zhang et al. ,\n2019c], and it is promising to investigate how to inject related\nknowledge for text generation.\nControllable Generation. Controllable text generation\nwith PLMs is an interesting direction but still at a very early\nstage. Controlling some attributes of the generated text has\nmany useful applications such as generating positive response\nto patients with depression in dialogue systems. However,\nPLMs are usually pretrained in universal corpus, which is\ndifﬁcult to control the multi-grained attributes of the gener-\nated text (e.g., sentiment, topic, and coherence). Keskar et\nal. [2019] has explored text generation with control codes that\ngovern style, content and task-speciﬁc behavior. While, these\ncontrol codes are preset and coarse-grained. Future work can\nexplore multi-grained control and develop PLMs that are suf-\nﬁciently steerable.\nModel Compression. Although PLMs with large-scale pa-\nrameters have achieved success in text generation, these mod-\nels are challenging to be deployed in resource constrained\nenvironments. As a result, it is meaningful to study how\nto achieve competitive performance with a small number of\nparameters. Several methods have been proposed to com-\npress PLMs, such as parameter sharing [Lan et al., 2020] and\nknowledge distillation [Sanh et al., 2019 ], whereas most of\nthem focused on BERT-based models, and little attention has\nbeen paid to compressing PLMs for text generation.\nFine-tuning Exploration. The direct intention of pretrain-\ning is to distill the linguistic knowledge learned in PLMs to\ndownstream generation tasks. And, ﬁne-tuning is the pre-\ndominant transfer method at present. There could be vari-\nous ways to transfer knowledge from PLMs to downstream\nmodels. For example, Chen et al. [2020a] exploited knowl-\nedge distillation by adopting BERT as teacher model and a\nvanilla RNN generation model as student model. Through\nthis method, the linguistic knowledge of BERT can be dis-\ntilled into the downstream model.\nLanguage-agnostic PLMs. Nowadays, almost all the\nPLMs for text generation are mainly based on English.\nThese PLMs will encounter challenges when dealing with\nnon-English generation tasks. Therefore, language-agnostic\nPLMs are worthy to be investigated, which need to capture\nuniversal linguistic and semantic features across different lan-\nguages. An interesting direction is how to reuse existing\nEnglish-based PLMs for text generation in non-English lan-\nguages.\nEthical Concern. Currently, PLMs are pretrained on large-\nscale corpus crawled from the web without ﬁne-grained ﬁl-\ntering, potentially causing ethical issues such as generating\nprivate content about users. Therefore, researchers should try\ntheir best to prevent misusing PLMs. For this purpose, we\ncan follow the key steps provided by Ross [2012], such as\nidentifying threats and potential impacts and assessing likeli-\nhood. Besides, the text generated by PLMs might be preju-\ndiced, which is in line with the bias in training data along the\ndimensions of gender, race, and religion[Brown et al., 2020].\nHence, we ought to intervene PLMs for preventing such bi-\nases. The research on the general approach is extensive but\nstill preliminary for PLMs.\nAcknowledgments\nThis work was partially supported by the National Key R&D\nProgram of China under Grant No. 2020AAA0105200, Na-\ntional Natural Science Foundation of China under Grant No.\n61872369 and 61832017, Beijing Academy of Artiﬁcial In-\ntelligence (BAAI) under Grant No. BAAI2020ZJ0301, Bei-\njing Outstanding Young Scientist Program under Grant No.\nBJJWZYJH012019100020098, the Fundamental Research\nFunds for the Central Universities, and the Research Funds\nof Renmin University of China under Grant No.18XNLG22\nand 19XNQ047. Xin Zhao is the corresponding author.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSurvey Track\n4497\nReferences\n[Bahdanau et al., 2015] Dzmitry Bahdanau, Kyunghyun\nCho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. In ICLR, 2015.\n[Bao et al., 2020] Siqi Bao, Huang He, Fan Wang, Hua Wu,\nHaifeng Wang, Wenquan Wu, Zhen Guo, Zhibin Liu,\nand Xinchao Xu. PLATO-2: towards building an open-\ndomain chatbot via curriculum learning. arXiv preprint\narXiv:2006.16779, 2020.\n[Brown et al., 2020] Tom B. Brown, Benjamin Mann, and\nNick Ryder et al. Language models are few-shot learners.\nIn NeurIPS, 2020.\n[Chen et al., 2020a] Yen-Chun Chen, Zhe Gan, Yu Cheng,\nJingzhou Liu, and Jingjing Liu. Distilling knowledge\nlearned in BERT for text generation. In ACL, 2020.\n[Chen et al., 2020b] Zhiyu Chen, Harini Eavani, Wenhu\nChen, Yinyin Liu, and William Yang Wang. Few-shot\nNLG with pre-trained language model. In ACL, 2020.\n[Conneau and Lample, 2019] Alexis Conneau and Guil-\nlaume Lample. Cross-lingual language model pretraining.\nIn NeurIPS, 2019.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understand-\ning. In NAACL-HLT, 2019.\n[Dong et al., 2019] Li Dong, Nan Yang, Wenhui Wang, Furu\nWei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. Uniﬁed language model pre-\ntraining for natural language understanding and genera-\ntion. In NeurIPS, 2019.\n[Fan et al., 2019] Zhiyun Fan, Shiyu Zhou, and Bo Xu. Un-\nsupervised pre-training for sequence to sequence speech\nrecognition. CoRR, arXiv preprint arXiv:1910.12418,\n2019.\n[Gehring et al., 2017] Jonas Gehring, Michael Auli, David\nGrangier, Denis Yarats, and Yann N. Dauphin. Convo-\nlutional sequence to sequence learning. In ICML, 2017.\n[Gong et al., 2020] Heng Gong, Yawei Sun, Xiaocheng\nFeng, Bing Qin, Wei Bi, Xiaojiang Liu, and Ting Liu.\nTablegpt: Few-shot table-to-text generation with table\nstructure reconstruction and content matching. In COL-\nING, 2020.\n[Gu et al., 2020] Jing Gu, Qingyang Wu, Chongruo Wu,\nWeiyan Shi, and Zhou Yu. A tailored pre-training\nmodel for task-oriented dialog generation. arXiv preprint\narXiv:2004.13835, 2020.\n[Guan et al., 2020] Wang Guan, Ivan Smetannikov, and Man\nTianxing. Survey on automatic text summarization and\ntransformer models applicability. In CCRIS, 2020.\n[Hendrycks et al., 2020] Dan Hendrycks, Xiaoyuan Liu,\nEric Wallace, Adam Dziedzic, Rishabh Krishnan, and\nDawn Song. Pretrained transformers improve out-of-\ndistribution robustness. In ACL, 2020.\n[Keskar et al., 2019] Nitish Shirish Keskar, Bryan McCann,\nLav R. Varshney, Caiming Xiong, and Richard Socher.\nCTRL: A conditional transformer language model for con-\ntrollable generation. arXiv preprint arXiv:1909.05858,\n2019.\n[Kryscinski et al., 2018] Wojciech Kryscinski, Romain\nPaulus, Caiming Xiong, and Richard Socher. Improving\nabstraction in text summarization. In EMNLP, 2018.\n[Lan et al., 2020] Zhenzhong Lan, Mingda Chen, Sebastian\nGoodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. ALBERT: A lite BERT for self-supervised learning of\nlanguage representations. In ICLR, 2020.\n[Lewis et al., 2020] Mike Lewis, Yinhan Liu, and Na-\nman Goyal et al. BART: denoising sequence-to-sequence\npre-training for natural language generation, translation,\nand comprehension. In ACL, 2020.\n[Li et al., 2019] Junyi Li, Wayne Xin Zhao, Ji-Rong Wen,\nand Yang Song. Generating long and informative reviews\nwith aspect-aware coarse-to-ﬁne decoding. In ACL, pages\n1969–1979, 2019.\n[Li et al., 2020] Junyi Li, Siqing Li, Wayne Xin Zhao, Gaole\nHe, Zhicheng Wei, Nicholas Jing Yuan, and Ji-Rong Wen.\nKnowledge-enhanced personalized review generation with\ncapsule graph neural network. In CIKM, pages 735–744,\n2020.\n[Li et al., 2021a] Junyi Li, Tianyi Tang, Gaole He, Jinhao\nJiang, Xiaoxuan Hu, Puzhao Xie, Zhipeng Chen, Zhuo-\nhao Yu, Wayne Xin Zhao, and Ji-Rong Wen. TextBox:\nA uniﬁed, modularized, and extensible framework for text\ngeneration. In ACL, 2021.\n[Li et al., 2021b] Junyi Li, Tianyi Tang, Wayne Xin Zhao,\nZhicheng Wei, Nicholas Jing Yuan, and Ji-Rong Wen.\nFew-shot knowledge graph-to-text generation with pre-\ntrained language models. In Findings of ACL, 2021.\n[Li et al., 2021c] Junyi Li, Wayne Xin Zhao, Zhicheng Wei,\nNicholas Jing Yuan, and Ji-Rong Wen. Knowledge-based\nreview generation by coherence enhanced text planning.\nIn SIGIR, 2021.\n[Lin et al., 2020] Zehui Lin, Xiao Pan, Mingxuan Wang,\nXipeng Qiu, Jiangtao Feng, Hao Zhou, and Lei Li. Pre-\ntraining multilingual neural machine translation by lever-\naging alignment information. In EMNLP, 2020.\n[Liu and Lapata, 2019] Yang Liu and Mirella Lapata. Text\nsummarization with pretrained encoders. In EMNLP,\n2019.\n[Mager et al., 2020] Manuel Mager, Ram ´on Fernandez As-\ntudillo, Tahira Naseem, Md. Arafat Sultan, Young-Suk\nLee, Radu Florian, and Salim Roukos. Gpt-too: A\nlanguage-model-ﬁrst approach for amr-to-text generation.\nIn ACL, 2020.\n[Peters et al., 2018] Matthew E. Peters, Mark Neumann,\nMohit Iyyer, Matt Gardner, Christopher Clark, Kenton\nLee, and Luke Zettlemoyer. Deep contextualized word\nrepresentations. In NAACL-HLT, 2018.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSurvey Track\n4498\n[Qiu et al., 2020] Xipeng Qiu, Tianxiang Sun, Yige Xu,\nYunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained\nmodels for natural language processing: A survey. arXiv\npreprint arXiv:2003.08271, 2020.\n[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon\nChild, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9, 2019.\n[Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam\nRoberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits\nof transfer learning with a uniﬁed text-to-text transformer.\nJMLR, 2020.\n[Ribeiro et al., 2020] Leonardo F. R. Ribeiro, Martin\nSchmitt, Hinrich Sch ¨utze, and Iryna Gurevych. Inves-\ntigating pretrained language models for graph-to-text\ngeneration. arXiv preprint arXiv:2007.08426, 2020.\n[Ross, 2012] R.S. Ross. Guide for conducting risk assess-\nments. In NIST Special Publication, 2012.\n[Rothe et al., 2020] Sascha Rothe, Shashi Narayan, and Ali-\naksei Severyn. Leveraging pre-trained checkpoints for se-\nquence generation tasks. TACL, 2020.\n[Sanh et al., 2019] Victor Sanh, Lysandre Debut, Julien\nChaumond, and Thomas Wolf. Distilbert, a distilled ver-\nsion of BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108, 2019.\n[See et al., 2017] Abigail See, Peter J. Liu, and Christo-\npher D. Manning. Get to the point: Summarization with\npointer-generator networks. In ACL, 2017.\n[Song et al., 2019] Kaitao Song, Xu Tan, Tao Qin, Jianfeng\nLu, and Tie-Yan Liu. MASS: masked sequence to se-\nquence pre-training for language generation. In ICML,\n2019.\n[Sun et al., 2019a] Chen Sun, Fabien Baradel, Kevin Mur-\nphy, and Cordelia Schmid. Contrastive bidirectional\ntransformer for temporal representation learning. arXiv\npreprint arXiv:1906.05743, 2019.\n[Sun et al., 2019b] Chen Sun, Austin Myers, Carl V ondrick,\nKevin Murphy, and Cordelia Schmid. Videobert: A joint\nmodel for video and language representation learning. In\nICCV, 2019.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, 2017.\n[Wada and Iwata, 2018] Takashi Wada and Tomoharu Iwata.\nUnsupervised cross-lingual word embedding by mul-\ntilingual neural language models. arXiv preprint\narXiv:1809.02306, 2018.\n[Wolf et al., 2019] Thomas Wolf, Victor Sanh, Julien Chau-\nmond, and Clement Delangue. Transfertransfo: A transfer\nlearning approach for neural network based conversational\nagents. arXiv preprint arXiv:1901.08149, 2019.\n[Xia et al., 2020] Qiaolin Xia, Haoyang Huang, Nan Duan,\nDongdong Zhang, Lei Ji, Zhifang Sui, Edward Cui, Taroon\nBharti, Xin Liu, and Ming Zhou. XGPT: cross-modal gen-\nerative pre-training for image captioning. arXiv preprint\narXiv:2003.01473, 2020.\n[Xu et al., 2020a] Jiacheng Xu, Zhe Gan, Yu Cheng, and\nJingjing Liu. Discourse-aware neural extractive text sum-\nmarization. In ACL, 2020.\n[Xu et al., 2020b] Shusheng Xu, Xingxing Zhang, Yi Wu,\nFuru Wei, and Ming Zhou. Unsupervised extractive sum-\nmarization by pre-training hierarchical transformers. In\nEMNLP, 2020.\n[Yang et al., 2020a] Zhen Yang, Bojie Hu, Ambyera Han,\nShen Huang, and Qi Ju. CSP: code-switching pre-training\nfor neural machine translation. In EMNLP, 2020.\n[Yang et al., 2020b] Ziyi Yang, Chenguang Zhu, Robert\nGmyr, Michael Zeng, Xuedong Huang, and Eric Darve.\nTED: A pretrained unsupervised summarization model\nwith theme modeling and denoising. In EMNLP (Find-\nings), 2020.\n[Zaib et al., 2020] Munazza Zaib, Quan Z. Sheng, and\nWei Emma Zhang. A short survey of pre-trained lan-\nguage models for conversational AI-A new age in NLP.\nIn ACSW, 2020.\n[Zeng and Nie, 2020] Yan Zeng and Jian-Yun Nie. General-\nized conditioned dialogue generation based on pre-trained\nlanguage model. arXiv preprint arXiv:2010.11140, 2020.\n[Zhang et al., 2019a] Haoyu Zhang, Jingjing Cai, Jianjun\nXu, and Ji Wang. Pretraining-based natural language gen-\neration for text summarization. In CoNLL, 2019.\n[Zhang et al., 2019b] Xingxing Zhang, Furu Wei, and Ming\nZhou. HIBERT: document level pre-training of hierarchi-\ncal bidirectional transformers for document summariza-\ntion. In ACL, 2019.\n[Zhang et al., 2019c] Zhengyan Zhang, Xu Han, Zhiyuan\nLiu, Xin Jiang, Maosong Sun, and Qun Liu. ERNIE: en-\nhanced language representation with informative entities.\nIn ACL, 2019.\n[Zhang et al., 2020] Yizhe Zhang, Siqi Sun, Michel Galley,\nYen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng\nGao, Jingjing Liu, and Bill Dolan. DIALOGPT : Large-\nscale generative pre-training for conversational response\ngeneration. In ACL, 2020.\n[Zhao et al., 2020] Xueliang Zhao, Wei Wu, Can Xu,\nChongyang Tao, Dongyan Zhao, and Rui Yan.\nKnowledge-grounded dialogue generation with pre-\ntrained language models. In EMNLP, 2020.\n[Zheng and Lapata, 2019] Hao Zheng and Mirella Lapata.\nSentence centrality revisited for unsupervised summariza-\ntion. In ACL, 2019.\n[Zhou et al., 2020] Luowei Zhou, Hamid Palangi, Lei\nZhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao.\nUniﬁed vision-language pre-training for image captioning\nand VQA. In AAAI, 2020.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\nSurvey Track\n4499",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8287804126739502
    },
    {
      "name": "Text generation",
      "score": 0.7259190678596497
    },
    {
      "name": "Artificial intelligence",
      "score": 0.66922527551651
    },
    {
      "name": "Natural language processing",
      "score": 0.584371030330658
    },
    {
      "name": "Deep learning",
      "score": 0.49985337257385254
    },
    {
      "name": "Pointer (user interface)",
      "score": 0.49427008628845215
    },
    {
      "name": "Language model",
      "score": 0.4941568672657013
    },
    {
      "name": "Natural language generation",
      "score": 0.489317387342453
    },
    {
      "name": "Task (project management)",
      "score": 0.48213550448417664
    },
    {
      "name": "Field (mathematics)",
      "score": 0.438416063785553
    },
    {
      "name": "Natural language",
      "score": 0.32241305708885193
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}