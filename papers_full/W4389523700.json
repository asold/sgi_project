{
  "title": "Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models",
  "url": "https://openalex.org/W4389523700",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2609011906",
      "name": "James Michaelov",
      "affiliations": [
        "Institute for Cognitive Science Studies"
      ]
    },
    {
      "id": "https://openalex.org/A5093063060",
      "name": "Catherine Arnett",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2320927890",
      "name": "Tyler Chang",
      "affiliations": [
        "Institute for Cognitive Science Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2062467649",
      "name": "Ben Bergen",
      "affiliations": [
        "Institute for Cognitive Science Studies"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W4287018479",
    "https://openalex.org/W1975576046",
    "https://openalex.org/W4322006551",
    "https://openalex.org/W3178340556",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W2559671171",
    "https://openalex.org/W2324482021",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3035287519",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3175567752",
    "https://openalex.org/W1550933260",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W4320925724",
    "https://openalex.org/W3166037607",
    "https://openalex.org/W101361534",
    "https://openalex.org/W2782849379",
    "https://openalex.org/W2118382933",
    "https://openalex.org/W2983962589",
    "https://openalex.org/W2890225082",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2008401431",
    "https://openalex.org/W1967340337",
    "https://openalex.org/W2375011798",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W3213418658",
    "https://openalex.org/W4404783665",
    "https://openalex.org/W3037575273",
    "https://openalex.org/W1986743249",
    "https://openalex.org/W3175306105",
    "https://openalex.org/W3171355829",
    "https://openalex.org/W2031473602",
    "https://openalex.org/W4384264777",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2761555949",
    "https://openalex.org/W382421368",
    "https://openalex.org/W4291961401",
    "https://openalex.org/W4281652360",
    "https://openalex.org/W2782397922",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W1981969954",
    "https://openalex.org/W2020046942",
    "https://openalex.org/W4286988498",
    "https://openalex.org/W2008175971",
    "https://openalex.org/W4285232264",
    "https://openalex.org/W2135183840",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W4296564631",
    "https://openalex.org/W4385573436",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W1991294536",
    "https://openalex.org/W2121352910",
    "https://openalex.org/W2322128339",
    "https://openalex.org/W2076198154",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2104537947",
    "https://openalex.org/W4361766487",
    "https://openalex.org/W4225707828",
    "https://openalex.org/W4313568641",
    "https://openalex.org/W4285208773",
    "https://openalex.org/W3176357828",
    "https://openalex.org/W4285193917",
    "https://openalex.org/W2986128786",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W2110065044",
    "https://openalex.org/W4321494373",
    "https://openalex.org/W2056422723",
    "https://openalex.org/W2014236569",
    "https://openalex.org/W2105192446",
    "https://openalex.org/W4316015028",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4221157597",
    "https://openalex.org/W3200065025",
    "https://openalex.org/W4387634965",
    "https://openalex.org/W2795342569",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W2098212071",
    "https://openalex.org/W4385573174",
    "https://openalex.org/W4225383339",
    "https://openalex.org/W4385565879",
    "https://openalex.org/W4385573123",
    "https://openalex.org/W2077442692",
    "https://openalex.org/W2938397304",
    "https://openalex.org/W2288212835",
    "https://openalex.org/W2509818368",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2553832225",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W1966199533",
    "https://openalex.org/W4285077564",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3185293939"
  ],
  "abstract": "grammatical knowledgeâ€”of parts of speech and grammatical patternsâ€”is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from structural priming. A sentence that shares the same grammatical structure as a preceding sentence is processed and produced more readily. Because confounds exist when using stimuli in a single language, evidence of abstraction is even more compelling from crosslingual structural priming, where use of a syntactic structure in one language primes an analogous structure in another language. We measure crosslingual structural priming in large language models, comparing model behavior to human experimental results from eight crosslingual experiments covering six languages, and four monolingual structural priming experiments in three non-English languages. We find evidence for abstract monolingual and crosslingual grammatical representations in the models that function similarly to those found in humans. These results demonstrate that grammatical representations in multilingual language models are not only similar across languages, but they can causally influence text produced in different languages.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3703â€“3720\nDecember 6-10, 2023 Â©2023 Association for Computational Linguistics\nStructural Priming Demonstrates Abstract Grammatical Representations\nin Multilingual Language Models\nJames A. Michaelova* Catherine Arnett b* Tyler A. Changa Benjamin K. Bergena\naDepartment of Cognitive Science,\nbDepartment of Linguistics,\nUniversity of California San Diego\n{j1michae, ccarnett, tachang, bkbergen}@ucsd.edu\nAbstract\nAbstract grammatical knowledgeâ€”of parts of\nspeech and grammatical patternsâ€”is key to the\ncapacity for linguistic generalization in humans.\nBut how abstract is grammatical knowledge in\nlarge language models? In the human literature,\ncompelling evidence for grammatical abstrac-\ntion comes from structural priming. A sentence\nthat shares the same grammatical structure as a\npreceding sentence is processed and produced\nmore readily. Because confounds exist when\nusing stimuli in a single language, evidence\nof abstraction is even more compelling from\ncrosslingual structural priming, where use of a\nsyntactic structure in one language primes an\nanalogous structure in another language. We\nmeasure crosslingual structural priming in large\nlanguage models, comparing model behavior to\nhuman experimental results from eight crosslin-\ngual experiments covering six languages, and\nfour monolingual structural priming experi-\nments in three non-English languages. We find\nevidence for abstract monolingual and crosslin-\ngual grammatical representations in the models\nthat function similarly to those found in hu-\nmans. These results demonstrate that grammat-\nical representations in multilingual language\nmodels are not only similar across languages,\nbut they can causally influence text produced\nin different languages.\n1 Introduction\nWhat do language models learn about the structure\nof the languages they are trained on? Under both\nmore traditional generative (Chomsky, 1965) and\ncognitively-inspired usage-based theories of lan-\nguage (Tomasello, 2003; Goldberg, 2006; Bybee,\n2010), the key to generalizable natural language\ncomprehension and production is the acquisition of\ngrammatical structures that are sufficiently abstract\nto account for the full range of possible sentences in\na language. In fact, both theoretical and experimen-\ntal accounts of language suggest that grammatical\nâˆ—Equal contribution.\nrepresentations are abstract enough to be shared\nacross languages in both humans (Heydel and Mur-\nray, 2000; Hartsuiker et al., 2004; Schoonbaert\net al., 2007) and language models (Conneau et al.,\n2020b,a; Jones et al., 2021).\nThe strongest evidence for grammatical abstrac-\ntion in humans comes from structural priming ,\na widely used and robust experimental paradigm.\nStructural priming is based on the hypothesis that\ngrammatical structures may be activated during\nlanguage processing. Priming then increases the\nlikelihood of production or increased ease of pro-\ncessing of future sentences sharing the same gram-\nmatical structures (Bock, 1986; Ferreira and Bock,\n2006; Pickering and Ferreira, 2008; Dell and Fer-\nreira, 2016; Mahowald et al., 2016; Branigan and\nPickering, 2017). For example, Bock (1986) finds\nthat people are more likely to produce an active sen-\ntence (e.g. one of the fans punched the referee) than\na passive sentence (e.g. the referee was punched\nby one of the fans ) after another active sentence.\nThis has been argued (Bock, 1986; Heydel and\nMurray, 2000; Pickering and Ferreira, 2008; Reit-\nter et al., 2011; Mahowald et al., 2016; Branigan\nand Pickering, 2017) to demonstrate common ab-\nstractions generalized across all sentences with the\nsame structure, regardless of content.\nResearchers have found evidence that structural\npriming for sentences with the same structure oc-\ncurs even when the two sentences are in different\nlanguages (Loebell and Bock, 2003; Hartsuiker\net al., 2004; Schoonbaert et al., 2007; Shin and\nChristianson, 2009; Bernolet et al., 2013; van Gom-\npel and Arai, 2018; Kotzochampou and Chondro-\ngianni, 2022). This crosslingual structural priming\ntakes abstraction one step further. First, it avoids\nany possible confounding effects of lexical rep-\netition and lexical priming of individual wordsâ€”\nwithin a given language, sentences with the same\nstructure often share function words (for discus-\nsion, see Sinclair et al., 2022). More fundamentally,\n3703\ncrosslingual structural priming represents an extra\ndegree of grammatical abstraction not just within a\nlanguage, but across languages.\nWe apply this same logic to language models\nin the present study. While several previous stud-\nies have explored structural priming in language\nmodels (Prasad et al., 2019; Sinclair et al., 2022;\nFrank, 2021; Li et al., 2022; Choi and Park, 2022),\nto the best of our knowledge, this is the first to look\nat crosslingual structural priming in Transformer\nlanguage models. We replicate eight human psy-\ncholinguistic studies, investigating structural prim-\ning in English, Dutch (Schoonbaert et al., 2007;\nBernolet et al., 2013), Spanish (Hartsuiker et al.,\n2004), German (Loebell and Bock, 2003), Greek\n(Kotzochampou and Chondrogianni, 2022), Polish\n(Fleischer et al., 2012), and Mandarin (Cai et al.,\n2012). We find priming effects in the majority of\nthe crosslingual studies and all of the monolingual\nstudies, which we argue supports the claim that\nmultilingual models have shared grammatical rep-\nresentations across languages that play a functional\nrole in language generation.\n2 Background\nStructural priming effects have been observed in\nhumans both within a given language (Bock, 1986;\nFerreira and Bock, 2006; Pickering and Ferreira,\n2008; Dell and Ferreira, 2016; Mahowald et al.,\n2016; Branigan and Pickering, 2017) and crosslin-\ngually (Loebell and Bock, 2003; Hartsuiker et al.,\n2004; Schoonbaert et al., 2007; Shin and Chris-\ntianson, 2009; Bernolet et al., 2013; van Gompel\nand Arai, 2018; Kotzochampou and Chondrogianni,\n2022). In language models, previous work has\ndemonstrated structural priming effects in English\n(Prasad et al., 2019; Sinclair et al., 2022; Choi and\nPark, 2022), and initial results have found prim-\ning effects between English and Dutch in LSTM\nlanguage models (Frank, 2021). As these studies ar-\ngue, the structural priming approach avoids several\npossible assumptions and confounds found in previ-\nous work investigating abstraction in grammatical\nlearning. For example, differences in language\nmodel probabilities for individual grammatical vs.\nungrammatical sentences may not imply that the\nmodels have formed abstract grammatical represen-\ntations that generalize across sentences (Sinclair\net al., 2022); other approaches involving probing\n(e.g. Hewitt and Manning, 2019; Chi et al., 2020)\noften do not test whether the internal model states\nare causally involved in the text predicted or gener-\nated by the model (V oita and Titov, 2020; Sinclair\net al., 2022). The structural priming paradigm al-\nlows researchers to evaluate whether grammatical\nrepresentations generalize across sentences in lan-\nguage models, and whether these representations\ncausally influence model-generated text. Further-\nmore, structural priming is agnostic to the specific\nlanguage model architecture and does not rely on\ndirect access to internal model states.\nHowever, the structural priming paradigm has\nnot been applied to modern multilingual language\nmodels. Previous work has demonstrated that mul-\ntilingual language models encode grammatical fea-\ntures in shared subspaces across languages (Chi\net al., 2020; Chang et al., 2022; de Varda and\nMarelli, 2023), largely relying on probing meth-\nods that do not establish causal effects on model\npredictions. Crosslingual structural priming would\nprovide evidence that the abstract grammatical rep-\nresentations shared across languages in the models\nhave causal effects on model-generated text. It\nwould also afford a comparison between grammati-\ncal representations in multilingual language models\nand human bilinguals. These shared grammatical\nrepresentations may help explain crosslingual trans-\nfer abilities in multilingual models, where tasks\nlearned in one language can be transferred to an-\nother (Artetxe et al., 2020; Conneau et al., 2020a,b;\nK et al., 2020; Goyal et al., 2021; Ogueji et al.,\n2021; Armengol-EstapÃ© et al., 2021, 2022; Blevins\nand Zettlemoyer, 2022; Chai et al., 2022; Muen-\nnighoff et al., 2023; Wu et al., 2022; Guarasci et al.,\n2022; Eronen et al., 2023).\nThus, this study presents what is to our knowl-\nedge the first experiment testing for crosslingual\nstructural priming in Transformer language models.\nThe findings broadly replicate human structural\npriming results: higher probabilities for sentences\nthat share grammatical structure with prime sen-\ntences both within and across languages.\n3 Method\nWe test multilingual language models for structural\npriming using the stimuli from eight crosslingual\nand four monolingual priming studies in humans.\nIndividual studies are described in Â§4.\n3.1 Materials\nAll replicated studies have open access stimuli with\nprime sentences for different constructions (Â§3.3).\n3704\nWhere target sentences are not provided (because\nparticipant responses were manually coded by the\nexperimenters), we reconstruct target sentences and\nverify them with native speakers.\n3.2 Language Models\nWe test structural priming in XGLM 4.5B (Lin\net al., 2022), a multilingual autoregressive Trans-\nformer trained on data from all languages we study\nin this paper, namely, English, Dutch, Spanish, Ger-\nman, Greek, Polish, and Mandarin. To the best of\nour knowledge, this is the only available pretrained\n(and not fine-tuned) autoregressive language model\ntrained on all the aforementioned languages. To\navoid drawing any conclusions based on the id-\niosyncrasies of a single language model, we also\ntest a number of other multilingual language mod-\nels trained on most of these languages, namely the\nother XGLM models, i.e., 564M, 1.7B, 2.9B, and\n7.5B, which are trained on all the languages ex-\ncept for Dutch and Polish; and PolyLM 1.7B and\n13B (Wei et al., 2023), which are trained on all the\nlanguages except for Greek.\n3.3 Grammatical Alternations Tested\nWe focus on structural priming for the three alter-\nnations primarily used in existing human studies.\nDative Alternation (DO/PO) Some languages\npermit multiple orders of the direct and indirect\nobjects in sentences. In PO (prepositional object)\nconstructions, e.g., the chef gives a hat to the swim-\nmer (Schoonbaert et al., 2007), the direct object a\nhat immediately follows the verb and the indirect\nobject is introduced with the prepositional phraseto\nthe swimmer. In DO (double object) constructions,\ne.g., the chef gives the swimmer a hat, the indirect\nobject the swimmer appears before the direct ob-\nject a hat and neither is introduced by a preposition.\nResearchers compare the proportion of DO or PO\nsentences produced by experimental participants\nfollowing a DO or PO prime.\nActive/Passive In active sentences the syntactic\nsubject is the agent of the action, while in pas-\nsive sentences the syntactic subject is the patient\nor theme of the action. E.g., the taxi chases the\ntruck is active, and the truck is chased by the taxi is\npassive (Hartsuiker et al., 2004). Researchers com-\npare the proportion of active or passive sentences\nproduced by experimental participants following\nan active or passive prime.\nOf-/S-Genitive Of - and S-Genitives represent\ntwo different ways of expressing possessive mean-\ning. In an of -genitive, the possessed thing is fol-\nlowed by a preposition such as of and then the\npossessor, e.g., the scarf of the boy is yellow . In\ns-genitives in the languages we analyze (English\nand Dutch), the possessor is followed by a word\nor an attached morpheme such as â€™s which is then\nfollowed by the possessed thing, e.g., the boyâ€™s\nscarf is yellow (Bernolet et al., 2013). Researchers\ncompare the proportion of of -genitive or s-genitive\nsentences produced by experimental participants\nfollowing an of -genitive or s-genitive prime.\n3.4 Testing Structural Priming in Models\nIn human studies, researchers test for structural\npriming by comparing the proportion of sentences\n(targets) of given types produced following primes\nof different types. Analogously, for each experi-\nmental item, we prompt the language model with\nthe prime sentence and compute the normalized\nprobabilities of each of the two target sentences.\nWe illustrate our approach to computing these nor-\nmalized probabilities below.\nFirst, consider the example dative alternation\nstimulus sentences from Schoonbaert et al. (2007):\n(1) (a) DO prime:The cowboy shows the\npirate an apple.\n(b) PO prime:The cowboy shows an apple\nto the pirate.\n(c) DO target:The chef gives the swimmer\na hat.\n(d) PO target:The chef gives a hat to the\nswimmer.\nWe can use language models to calculate the proba-\nbility of each target following each prime by taking\nthe product of the conditional probabilities of all to-\nkens in the target sentence given the prime sentence\nand all preceding tokens in the target sentence. In\npractice, these probabilities are very small, but for\nillustrative purposes, we can imagine these have\nthe probabilities in (2).\n(2) (a) P(PO Target | DO Prime) = 0.03\n(b) P(DO Target | DO Prime) = 0.02\n(c) P(PO Target | PO Prime) = 0.04\n(d) P(DO Target | PO Prime) = 0.01\nWe then normalize these probabilities by calculat-\ning the conditional probability of each target sen-\ntence given that the model response is one of the\ntwo target sentences, as shown in (3).\n3705\n(3) (a) PN(PO | DO) = 0.03/(0.03+0.02) = 0.60\n(b) PN(DO | DO) = 0.02/(0.03+0.02) = 0.40\n(c) P N(PO | PO) = 0.04/(0.04+0.01) = 0.80\n(d) PN(DO | PO) = 0.01/(0.04+0.01) = 0.20\nBecause the normalized probabilities of the two tar-\ngets following a given prime sum to one, we only\nconsider the probabilities for one target type in our\nanalyses (comparing over the two different prime\ntypes). For example, to test for a priming effect,\nwe could either compare the difference between\nPN(PO | PO) and PN(PO | DO) or the difference\nbetween PN(DO | PO) and PN(DO | DO). We fol-\nlow the original human studies in the choice of\nwhich target construction to plot and test.\nWe run statistical analyses, testing whether ef-\nfects are significant for each language model on\neach set of stimuli. To do this, we construct a linear\nmixed-effects model predicting the target sentence\nprobability (e.g. probability of a PO sentence) for\neach item. We include a random intercept for ex-\nperimental item, and we test whether prime type\n(e.g. DO vs. PO) significantly predicts target struc-\nture probability. All reportedp-values are corrected\nfor multiple comparisons by controlling for false\ndiscovery rate (Benjamini and Hochberg, 1995).\nAll stimuli, data, code, and statistical analyses are\nprovided at https://osf.io/2vjw6/.\n4 Results\nIn reporting whether the structural priming effects\nfrom human experiments replicate in XGLM lan-\nguage models, we primarily consider the direction\nof each effect in the language models (e.g. whether\nPO constructions are more likely after PO vs. DO\nprimes) rather than effect sizes or raw probabilities.\nThe mean of the relative probabilities assigned by\nlanguage models to the different constructions in\neach condition may not be directly comparable to\nhuman probabilities of production. Humans are\nsensitive to contextual cues that may not be avail-\nable to language models; notably, in these tasks, hu-\nmans are presented with pictures corresponding to\nevents in the structural priming paradigm. Further-\nmore, construction probabilities in language mod-\nels may be biased by the frequency of related con-\nstructions in any of the many languages on which\nthe models are trained. Thus, we focus only on\nwhether the language models replicate the direc-\ntion of the principal effect in each human study.\n4.1 Crosslingual Structural Priming\nWe test whether eight human crosslingual struc-\ntural priming studies replicate in language models.\nThese studies cover structural priming between En-\nglish and Dutch (Schoonbaert et al., 2007; Bernolet\net al., 2013), Spanish (Hartsuiker et al., 2004), Ger-\nman (Loebell and Bock, 2003), Greek (Kotzocham-\npou and Chondrogianni, 2022), and Polish (Fleis-\ncher et al., 2012). For each experiment, we show\nthe original human probabilities and the normalized\nprobabilities calculated using each language model,\nas well as whether there is a significant priming\neffect (Figure 1). The full statistical results are\nreported in Appendix B.\n4.1.1 Schoonbaert et al. (2007):\nDutchâ†’English\nSchoonbaert et al. (2007) prime 32 Dutch-English\nbilinguals with 192 Dutch sentences with either\nprepositional (PO) or dative object (DO) construc-\ntions. Schoonbaert et al. (2007) find that exper-\nimental participants produce more PO sentences\nwhen primed with a PO sentence than when primed\nwith a DO sentence (see Figure 1A). We see the\nsame pattern with nearly all the language models\n(Figure 1A). With the exception of XGLM 1.7B,\nwhere the effect is only marginally significant after\ncorrection for multiple comparisons, all language\nmodels predict English PO targets to be signifi-\ncantly more likely when they follow Dutch PO\nprimes than when they follow Dutch DO primes.\n4.1.2 Schoonbaert et al. (2007):\nEnglishâ†’Dutch\nSchoonbaert et al. (2007) also observe DO/PO\nstructural priming from English to Dutch (32 par-\nticipants; 192 primes). As seen in Figure 1B, all\nlanguage models show a significant priming effect.\n4.1.3 Bernolet et al. (2013): Dutchâ†’English\nBernolet et al. (2013) conduct a Dutch â†’English\nstructural priming experiment with 24 Dutch-\nEnglish bilinguals on 192 prime sentences, and\nthey find that the production of s-genitives is signif-\nicantly more likely after an s-genitive prime than\nafter an of -genitive prime. We also observe this in\nall of the language models, as seen in Figure 1C.\n4.1.4 Hartsuiker et al. (2004):\nSpanishâ†’English\nHartsuiker et al. (2004) investigate\nSpanishâ†’English structural priming with\n3706\nğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ n.s. n.s. ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶\nğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ n.s. n.s. ğŸ¶ ğŸ¶ n.s. n.s. ğŸ¶ ğŸ¶ n.s. n.s. ğŸ¶ ğŸ¶\nğŸ¶ ğŸ¶ n.s. n.s. n.s. n.s. n.s. n.s. n.s. n.s. ğŸ¶ ğŸ¶ n.s. n.s. n.s. n.s. ğŸ¶ ğŸ¶ n.s. n.s. n.s. n.s. ğŸ¶ ğŸ¶ n.s. n.s. n.s. n.s. n.s. n.s. ğŸ¶ ğŸ¶\nğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ n.s. n.s. n.s. n.s. ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ n.s. n.s. ğŸ¶ ğŸ¶ n.s. n.s. ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ n.s. n.s.\n(A) Schoonbaert et al. (2007): Dutchâ†’English\nNormalized Probability PO Targets\n(B) Schoonbaert et al. (2007): Englishâ†’Dutch\nNormalized Probability PO Targets\n(C) Bernolet et al. (2013): Dutchâ†’English\nNormalized Probability S-Genitive Targets\n(D) Hartsuiker et al. (2004): Spanishâ†’English\nNormalized Probability Passive Targets\n(E) Loebell et al. (2003): Germanâ†’English\nNormalized Probability PO Targets\n(F) Loebell et al. (2003): Englishâ†’German\nNormalized Probability PO Targets\n(G) Kotzochampou et al. (2022): Greekâ†’English Targets\nNormalized Probability Passive Targets\n(H) Fleischer et al. (2012): Polishâ†’English\nNormalized Probability Passive Targets\nHuman XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B Human XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B\nHuman XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B Human XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B\nHuman XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B Human XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B\nHuman XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B Human XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B\nActive Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive\nDO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO\nof-Gen s-Gen of-Gen s-Gen of-Gen s-Gen of-Gen s-Gen of-Gen s-Gen of-Gen s-Gen of-Gen s-Gen of-Gen s-Gen Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive\nDO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nPrime Type\nNormalized Probability\nFigure 1: Human and language model results for crosslingual structural priming experiments.\nğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶\nğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶\n(A) Schoonbaert et al. (2007): Dutchâ†’Dutch\nNormalized Probability PO Targets\n(B) Kotzochampou et al. (2022): Greekâ†’Greek Targets\nNormalized Probability Passive Targets\n(C) Cai et al. (2012) Experiment 1/2: Mandarinâ†’Mandarin\nNormalized Probability DO Targets\n(D) Cai et al. (2012) Experiment 3: Mandarinâ†’Mandarin\nNormalized Probability DO Passive Targets\nHuman XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B Human XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B\nHuman XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B Human XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B\nDO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO\nDO PO DO PO DO PO DO PO DO PO DO PO DO PO DO PO Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nPrime Type\nNormalized Probability\nFigure 2: Human and language model results for within-language structural priming experiments.\n24 Spanish-English bilinguals on 128 prime\nsentences, finding a significantly higher proportion\nof passive responses after passive primes than\nactive primes. As shown in Figure 1D, this effect\nis replicated by XGLM 564M, 2.9B, and 7.5B as\nwell as PolyLM 13B, with XGLM 4.5B showing a\nmarginal effect (p= 0.0565).\n4.1.5 Loebell and Bock (2003):\nGermanâ†’English\nLoebell and Bock (2003) find a small but signifi-\ncant priming effect of dative alternation (DO/PO)\nfrom German to English with 48 German-English\nbilinguals on 32 prime sentences. As can be seen\nin Figure 1E, while all language models show a\nnumerical effect in the correct direction, the effect\nis only significant for XGLM 7.5B.\n4.1.6 Loebell and Bock (2003):\nEnglishâ†’German\nLoebell and Bock (2003) also test 48 German-\nEnglish bilinguals for a dative alternation (DO/PO)\npriming effect from English primes to German tar-\ngets (32 prime sentences), finding a small but sig-\nnificant priming effect. As we show in Figure 1F,\nthe models are relatively varied in direction of nu-\nmerical difference. However, only XGLM 2.9B\nand PolyLM 13B display a significant effect, and\nin both cases the effect is in the same direction as\nthat found with human participants.\n3707\n4.1.7 Kotzochampou and Chondrogianni\n(2022): Greekâ†’English\nKotzochampou and Chondrogianni (2022) find ac-\ntive/passive priming from Greek to English in 25\nGreek-English bilinguals. Participants are more\nlikely to produce passive responses after passive\nprimes (48 prime sentences). As shown in Fig-\nure 1G), all XGLMs display this effect, while the\nPolyLMs, which are not trained on Greek, do not.\n4.1.8 Fleischer et al. (2012): Polishâ†’English\nSimilarly, Fleischer et al. (2012) find active/passive\npriming from Polish to English in 24 Polish-\nEnglish bilinguals on 64 prime sentences. As we\nsee in Figure 1H, while all models show a numeri-\ncal difference in the correct direction, the effect is\nonly significant for XGLM 564M, 2.9B, and 7.5B,\nand for PolyLM 1.7B.\n4.2 Monolingual Structural Priming\nIn the previous section, we found crosslingual prim-\ning effects in language models for the majority of\ncrosslingual priming studies in humans. However,\nsix of the eight studies have English target sen-\ntences. Our results up to this point primarily show\nan effect of structural priming on English targets.\nWhile both previous work (Sinclair et al., 2022)\nand our results in Â§4.1 may indeed demonstrate the\neffects of abstract grammatical representations on\ngenerated text in English, we should not assume\nthat such effects can reliably be observed for other\nlanguages. Thus, we test whether multilingual lan-\nguage models exhibit within-language structural\npriming effects comparable to those found in hu-\nman studies for Dutch (Schoonbaert et al., 2007),\nGreek (Kotzochampou and Chondrogianni, 2022),\nand two studies in Mandarin (Cai et al., 2012).\n4.2.1 Schoonbaert et al. (2007):\nDutchâ†’Dutch\nUsing Dutch prime and target sentences (192\nprimes), Schoonbaert et al. (2007) find that Dutch-\nEnglish bilinguals (N=32) produce PO sentences at\na higher rate when primed by a PO sentence com-\npared to a DO sentence. As we see in Figure 2A,\nall language models display this effect.\n4.2.2 Kotzochampou and Chondrogianni\n(2022): Greekâ†’Greek\nIn their Greekâ†’Greek priming experiment, Kot-\nzochampou and Chondrogianni (2022) find an ac-\ntive/passive priming effect in native Greek speakers\n(N=25) using 48 primes. As shown in Figure 2B,\nthis effect is replicated by all language models.\n4.2.3 Cai et al. (2012): Mandarinâ†’Mandarin\nUsing two separate sets of stimuli, Cai et al. (2012)\nfind within-language DO/PO priming effects in na-\ntive Mandarin speakers (N=28, N=24).1 As seen\nin Figure 2C and 2D, all language models show\nsignificant effects for both sets of stimuli (48 prime\nsentences in their Experiments 1 and 2, and 68\nprime sentences in their Experiment 3).\n4.3 Further Tests of Structural Priming\nWe have now observed within-language structural\npriming in multilingual language models for lan-\nguages other than English. In Â§4.1, we found robust\nEnglishâ†’Dutch structural priming (Schoonbaert\net al., 2007) but only limited priming effects for\ntargets in German. Although there are no human re-\nsults for the non-English targets in the other studies\nin Â§4.1, we can still evaluate crosslingual structural\npriming with non-English targets in the language\nmodels by switching the prime and target sentences\nin the stimuli. Specifically, we test structural prim-\ning from English to Dutch (Bernolet et al., 2013),\nSpanish (Hartsuiker et al., 2004), Polish (Fleischer\net al., 2012), and Greek (Kotzochampou and Chon-\ndrogianni, 2022).\nAll models show a significant effect on the re-\nversed Bernolet et al. (2013) stimuli (Figure 3A;\nEnglishâ†’Dutch), and all models but PolyLM\n1.7B show the same for the reversed Hartsuiker\net al. (2004) stimuli (Figure 3B; Englishâ†’Spanish).\nThe other results are less clear-cut. While\nXGLM 564M, 2.9B, and 4.5B and the PolyLMs\nshow a numerical effect in the correct direction\nfor the reversed Fleischer et al. (2012) stimuli\n(Englishâ†’Polish; Figure 3C), only PolyLM 1.7B\nshows a significant effect. For the reversed Kot-\nzochampou and Chondrogianni (2022) stimuli\n(Englishâ†’Greek; Figure 3D), all the XGLMs and\nPolyLM 13B show a numerical tendency in the\ncorrect direction, but only XGLM 564M and 4.5B\nshow a significant effect.\n1The original study tests the effect of variants of DO/PO\nprimes (topicalized DO/PO and Ba-DO; see Cai et al., 2012).\nTo unify our analyses across studies, we only look at structural\npriming following the canonical DO and PO primes used in\nboth Experiments 1 and 2 of the original study, as well as\nthose used in Experiment 3.\n3708\nğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ n.s. n.s. ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ ğŸ¶ n.s. n.s. ğŸ¶ ğŸ¶\nn.s. n.s. n.s. n.s. n.s. n.s. n.s. n.s. n.s. n.s. ğŸ¶ ğŸ¶ n.s. n.s. ğŸ¶ ğŸ¶ n.s. n.s. n.s. n.s. ğŸ¶ ğŸ¶ n.s. n.s. n.s. n.s. n.s. n.s.\n(A) Bernolet et al. (2013): Englishâ†’Dutch\nNormalized Probability S-Genitive Targets\n(B) Hartsuiker et al. (2004): Englishâ†’Spanish\nNormalized Probability Passive Targets\n(C) Fleischer et al. (2012): Englishâ†’Polish\nNormalized Probability Passive Targets\n(D) Kotzochampou et al. (2022): Englishâ†’Greek Targets\nNormalized Probability Passive Targets\nXGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B\nXGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B XGLM 564M XGLM 1.7B XGLM 2.9B XGLM 4.5B XGLM 7.5B PolyLM 1.7B PolyLM 13B\nActive Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive\nof-Gen s-Gen of-Gen s-Gen of-Gen s-Gen of-Gen s-Gen of-Gen s-Gen of-Gen s-Gen of-Gen s-Gen Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive Active Passive\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nPrime Type\nNormalized Probability\nFigure 3: Language model results for structural priming experiments with no human baseline.\n5 Discussion\nWe find structural priming effects in at least one\nlanguage model on each set of stimuli (correcting\nfor multiple comparisons). Moreover, we observe\na significant effect in all models with the mono-\nlingual stimuli, and in the majority of the models\nfor 8 of the 12 crosslingual stimuli. In line with\nprevious work (Hewitt and Manning, 2019; Chi\net al., 2020), this supports the claim that language\nmodels learn generalized, abstract, and multilin-\ngual representations of grammatical structure. Our\nresults further suggest that these shared grammat-\nical representations are causally linked to model\noutput.\n5.1 Differences between models\nIn some ways, we see expected patterns across mod-\nels. For example, for the XGLMs trained on 30 lan-\nguages (XGLM 564M, 1.7B, 2.9B, and 7.5B), the\nlarger models tend to display larger effect sizes than\nthe smaller models, in line with the idea that model\nperformance can scale with number of parame-\nters (Brown et al., 2020; Kaplan et al., 2020; Rae\net al., 2022; Hoffmann et al., 2022; Touvron et al.,\n2023). Additionally, the PolyLMs, which are not\ntrained on Greek, do not show crosslingual struc-\ntural priming for Greek (neither Greek â†’English\nnor Englishâ†’Greek).\nOn the other hand, one surprising finding is that\ndespite not being trained on Greek, the PolyLMs\nare able to successfully model monolingual struc-\ntural priming in Greek. The most likely expla-\nnation for this is what Sinclair et al. (2022) re-\nfer to as â€˜lexical overlapâ€™â€”the overlap of func-\ntion words between primes and targets substan-\ntially boosts structural priming effects. In the\nsame way that humans find it easier to process\nwords that have recently been mentioned (Rugg,\n1985, 1990; Van Petten et al., 1991; Besson et al.,\n1992; Mitchell et al., 1993; Rommers and Feder-\nmeier, 2018), language models may predict that\npreviously-mentioned words are more likely to oc-\ncur again (a familiar phenomenon in the case of\nrepeated text loops; see Holtzman et al., 2020; See\net al., 2019; Xu et al., 2022) even if they are not\ntrained on the words explicitly. This would explain\nthe results for the Kotzochampou and Chondro-\ngianni (2022) stimuli, as the Greek passive stimuli\nalways include the word Î±Ï€Â´o.\nSuch an explanation could also account for the\nperformance of XGLM 564M, 1.7B, 2.9B, and\n7.5B on the Dutch and Polish stimuli. Despite\nnot being intentionally trained on Dutch or Pol-\nish, we see robust crosslingual Dutch â†’English\nand English â†’Dutch structural priming, as well\nas Polishâ†’English structural priming, in three of\nthese models. However, as discussed previously,\ncrosslingual structural priming avoids the possi-\nble confound of lexical overlap. For these results,\ntherefore, a more likely explanation is language\ncontamination. In contemporaneous work, we find\nthat training on fewer than 1M tokens in a second\nlanguage is sufficient for structural priming effects\nto emerge (Arnett et al., 2023); our estimates of\nthe amount of language contamination in XGLM\n564M, 1.7B, 2.9B, and 7.5B range from 1.77M\ntokens of Dutch and 1.46M tokens of Polish at the\nmost conservative to 152.5M and 33.4M tokens\nrespectively at the most lenient (see Appendix A).\nThe smaller amount of Polish contamination,\nas well as the fact that Polish is less closely re-\nlated to English, may explain the less consistent\n3709\nPolishâ†’English structural priming effects and the\nvirtually non-existent English â†’Polish effects in\nthese models, but as will be discussed in Â§5.2, there\nmay be other reasons for this latter pattern.\n5.2 Null Effects and Asymmetries\nMore theoretically interesting is the question of\nwhy some language models fail to display crosslin-\ngual structural priming on some sets of stimuli,\neven when trained on both languages. For ex-\nample, in the Loebell and Bock (2003) replica-\ntions, only XGLM 7.5B shows a significant ef-\nfect of Germanâ†’English structural priming, and\nonly XGLM 2.9B and PolyLM 13B show a signifi-\ncant effect of Englishâ†’German structural priming.\nThis may be due to the grammatical structures used\nin the stimuli (DO/PO). While the original study\ndoes find crosslingual structural priming effects,\nthe effect sizes are small; the authors suggest that\nthis may partly be because â€œthe prepositional form\nis used more restrictively in Germanâ€ (Loebell and\nBock, 2003, p. 807).\nWe also see an asymmetry in the crosslingual\nstructural priming effects between some languages.\nWhile the effects in the Dutchâ†’English (Bernolet\net al., 2013) and Spanish â†’English (Hartsuiker\net al., 2004) studies mostly remain when the di-\nrection of the languages is reversed, this is not\nthe case for the Polishâ†’English (Fleischer et al.,\n2012) and Greek â†’English (Kotzochampou and\nChondrogianni, 2022) results. This may be due\nto the smaller quantity of training data for Polish\nand Greek compared to Spanish in XGLM. While\nXGLM is only trained on slightly more Dutch than\nPolish, Dutch is also more similar to English in\nterms of its lexicon and morphosyntax, so it may\nbenefit from more effective crosslingual transfer\n(Conneau et al., 2020b; Gerz et al., 2018; Guarasci\net al., 2022; Winata et al., 2022; Ahuja et al., 2022;\nOladipo et al., 2022; Eronen et al., 2023).\nIf it is indeed the case that structural priming\neffects in language models are weaker when the tar-\nget language is less trained on, this would contrast\nwith human studies, where crosslingual structural\npriming appears most reliable when the prime is in\nparticipantsâ€™ native or primary language (L1) and\nthe target is in their second language (L2). The\nreverse case often results in smaller effect sizes\n(Schoonbaert et al., 2007) or effects that are not sig-\nnificant at all (Shin, 2010). Under this account, lan-\nguage modelsâ€™ dependence on target language train-\ning and humansâ€™ dependence on prime language\nexperience for structural priming would suggest\nthat there are key differences between the models\nand humans in how grammatical representations\nfunction in comprehension and production.\nAn alternative reason for the absence of\ncrosslingual structural priming effects for the\nEnglishâ†’Polish and English â†’Greek stimuli is\na combination of model features and features of\nthe languages themselves. For example, structural\npriming effects at the syntactic level may overall\nbe stronger for English targets. English is a lan-\nguage with relatively fixed word order, and thus,\ncompetence in English may require a more explicit\nrepresentation of word order than other languages.\nIn contrast to English, Polish and Greek are mor-\nphologically rich languages, where important in-\nformation is conveyed through morphology (e.g.\nword inflections), and word orders are less fixed\n(Tzanidaki, 1995; Siewierska, 1993). Thus, struc-\ntural priming effects with Polish and Greek targets\nwould manifest as differences in target sentence\nmorphology. However, contemporary language\nmodels such as XGLM have a limited ability to\ndeal with morphology. Most state-of-the-art mod-\nels use WordPiece (Wu et al., 2016) or Sentence-\nPiece (Kudo and Richardson, 2018) tokenizers, but\nother approaches may be necessary for morpho-\nlogically rich languages (Klein and Tsarfaty, 2020;\nPark et al., 2021; Soulos et al., 2021; Nzeyimana\nand Niyongabo Rubungo, 2022; Seker et al., 2022).\nThus, while humans are able to exhibit crosslin-\ngual structural priming effects between languages\nwhen the equivalent structures do not share the\nsame word orders (Muylle et al., 2020; Ziegler\net al., 2019; Hsieh, 2017; Chen et al., 2013), this\nmay not hold for contemporary language models.\nSpecifically, given the aforementioned limitations\nof contemporary language models, it would be un-\nsurprising that structural priming effects are weaker\nfor morphologically-rich target languages with rel-\natively free word order such as Polish and Greek.\n5.3 Implications for Multilingual Models\nThe results reported here seem to bode well for the\ncrosslingual capacities of multilingual language\nmodels. They indicate shared representations of\ngrammatical structure across languages (in line\nwith Chi et al., 2020; Chang et al., 2022; de Varda\nand Marelli, 2023), and they show that these repre-\nsentations have a causal role in language generation.\n3710\nThe results also demonstrate that crosslinguistic\ntransfer can take place at the level of grammatical\nstructures, not just specific phrases, concepts, and\nindividual examples. Crosslinguistic generaliza-\ntions can extend at least to grammatical abstrac-\ntions, and thus learning a grammatical structure\nin one language may aid in the acquisition of its\nhomologue in a second language.\nHow do language models acquire these abstrac-\ntions? As Contreras Kallens et al. (2023) point\nout, language models learn grammatical knowl-\nedge through exposure. To the degree that similar\noutcomes for models and humans indicate shared\nmechanisms, this serves to reinforce claims of\nusage-based (i.e. functional) accounts of language\nacquisition (Tomasello, 2003; Goldberg, 2006; By-\nbee, 2010), which argue that statistical, bottom-up\nlearning may be sufficient to account for abstract\ngrammatical knowledge. Specifically, the results of\nour study demonstrate the in-principle viability of\nlearning the kinds of linguistic structures that are\nsensitive to structural priming using the statistics of\nlanguage alone. Indeed, under certain accounts of\nlanguage (e.g. Branigan and Pickering, 2017), it is\nprecisely the kinds of grammatical structures that\ncan be primed that are the abstract linguistic repre-\nsentations that we learn when we acquire language.\nOur results are thus in line with Contreras Kallens\net al.â€™s (2023) argument that it may be possible\nto use language models as tests for necessity in\ntheories of grammar learning. Taking this further,\nfuture work might use different kinds of language\nmodels to test what types of priors or biases, if\nany, are required for any learner to acquire abstract\nlinguistic knowledge.\nIn practical terms, the structural priming\nparadigm is an innovative way to probe whether a\nlanguage model has formed an abstract representa-\ntion of a given structure (Sinclair et al., 2022), both\nwithin and across languages. By testing whether\na structure primes a homologous structure in an-\nother language, we can assess whether the modelâ€™s\nrepresentation for that structure is abstract enough\nto generalize beyond individual sentences and has\na functional role in text generation. As language\nmodels are increasingly used in text generation sce-\nnarios (Lin et al., 2022) rather than fine-tuning rep-\nresentations (Conneau et al., 2020a), understanding\nthe effects of such representations on text genera-\ntion is increasingly important. Previous work has\ncompared language models to human studies of\nlanguage comprehension (e.g. Oh and Schuler,\n2023; Michaelov et al., 2022; Wilcox et al., 2021;\nHollenstein et al., 2021; Kuribayashi et al., 2021;\nGoodkind and Bicknell, 2018), and while the de-\ngree to which the the mechanisms involved in com-\nprehension and production differ in humans is a\nmatter of current debate (Pickering and Garrod,\n2007, 2013; Hendriks, 2014; Meyer et al., 2016;\nMartin et al., 2018), our results show that human\nstudies of language production can also be repro-\nduced in language models used for text generation.\n6 Conclusion\nUsing structural priming, we measure changes in\nprobability for target sentences that do or do not\nshare structure with a prime sentence. Analogously\nto humans, models predict that a similar target\nstructure is generally more likely than a different\none, whether within or across languages. We ob-\nserve several exceptions, which may reveal features\nof the languages in question, limitations of the mod-\nels themselves, or interactions between the two.\nBased on our results, we argue that multilingual au-\ntoregressive Transformer language models display\nevidence of abstract grammatical knowledge both\nwithin and across languages. Our results provide\nevidence that these shared representations are not\nonly latent in multilingual modelsâ€™ representation\nspaces, but also causally impact their outputs.\nLimitations\nTo ensure that the stimuli used for the language\nmodels indeed elicit structural priming effects in\npeople, we only use stimuli made available by the\nauthors of previously-published studies on struc-\ntural priming in humans. Thus, our study analyzes\nonly a subset of possible grammatical alternations\nand languages. All of our crosslingual structural\npriming stimuli involve English as one of the lan-\nguages, and all other languages included are, with\nthe exception of Mandarin, Indo-European lan-\nguages spoken in Europe. All are also moderately\nor highly-resourced in the NLP literature (Joshi\net al., 2020). Thus, our study is not able to account\nfor the full diversity of human language.\nAdditionally, while psycholinguistic studies of-\nten take crosslingual structural priming to indicate\nshared representations, there are alternate interpre-\ntations. Most notably, because structurally similar\nsentences are more likely to occur in succession\nthan chance, it is possible that increased proba-\n3711\nbility for same-structure target sentences reflects\nlikely co-occurrence of distinct, associated repre-\nsentations, rather than a single, common, abstract\nrepresentation (Ahn and Ferreira, 2023). While\nthis is a much more viable explanation for mono-\nlingual than crosslingual priming, the presence of\neven limited code-switching in training data could\nin principle lead to similar effects across languages.\nEthics Statement\nOur work complies with the ACL Ethics Policy, and\nwe believe that testing how well language models\nhandle languages other than English is an important\navenue of research to reduce the potential harms\nof language model applications. We did not train\nany models for this study; instead, we used the\npre-trained XGLM (Lin et al., 2022) and PolyLM\n(Wei et al., 2023) families of models made available\nthrough the transformers Python package (Wolf\net al., 2020). All analyses were run on an NVIDIA\nRTX A6000 GPU, running for a total of 4 hours.\nAcknowledgements\nWe would like to thank Sarah Bernolet, Kathryn\nBock, Holly P. Branigan, Zhenguang G. Cai, Vasi-\nliki Chondrogianni, Zuzanna Fleischer, Robert\nJ. Hartsuiker, Sotiria Kotzochampou, Helga Loe-\nbell, Janet F. McLean, Martin J. Pickering, Sofie\nSchoonbaert, and Eline Veltkamp for making their\nexperimental stimuli available; and Nikitas Angele-\ntos Chrysaitis, Pamela D. RiviÃ¨re Ruiz, Stephan\nKaufhold, Quirine van Engen, Alexandra Taylor,\nRobert Slawinski, Felix J. Binder, Johanna Meyer,\nTiffany Wu, Fiona Tang, Emily Xu, and Jason Tran\nfor their assistance in preparing them for use in the\npresent study. Models were evaluated using hard-\nware provided by the NVIDIA Corporation as part\nof an NVIDIA Academic Hardware Grant. Tyler\nChang is partially supported by the UCSD HDSI\ngraduate fellowship.\nReferences\nDanbi Ahn and Victor S Ferreira. 2023. Shared vs sepa-\nrate structural representations: Evidence from cumu-\nlative cross-language structural priming. Quarterly\nJournal of Experimental Psychology.\nKabir Ahuja, Sunayana Sitaram, Sandipan Dandapat,\nand Monojit Choudhury. 2022. On the Calibration\nof Massively Multilingual Language Models. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 4310â€“\n4323, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nRami Al-Rfou. 2020. Gcld3: CLD3 is a neural network\nmodel for language identification.\nJordi Armengol-EstapÃ©, Casimiro Pio Carrino, Carlos\nRodriguez-Penagos, Ona de Gibert Bonet, Carme\nArmentano-Oller, Aitor Gonzalez-Agirre, Maite\nMelero, and Marta Villegas. 2021. Are Multilin-\ngual Models the Best Choice for Moderately Under-\nresourced Languages? A Comprehensive Assessment\nfor Catalan. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n4933â€“4946, Online. Association for Computational\nLinguistics.\nJordi Armengol-EstapÃ©, Ona de Gibert Bonet, and Maite\nMelero. 2022. On the Multilingual Capabilities of\nVery Large-Scale English Language Models. In Pro-\nceedings of the Thirteenth Language Resources and\nEvaluation Conference, pages 3056â€“3068, Marseille,\nFrance. European Language Resources Association.\nCatherine Arnett, Tyler A. Chang, James A. Michaelov,\nand Benjamin K. Bergen. 2023. Crosslingual Struc-\ntural Priming and the Pre-Training Dynamics of Bilin-\ngual Language Models.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the Cross-lingual Transferability of Mono-\nlingual Representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4623â€“4637, Online. Association\nfor Computational Linguistics.\nYoav Benjamini and Yosef Hochberg. 1995. Control-\nling the False Discovery Rate: A Practical and Pow-\nerful Approach to Multiple Testing. Journal of the\nRoyal Statistical Society. Series B (Methodological),\n57(1):289â€“300.\nSarah Bernolet, Robert J. Hartsuiker, and Martin J. Pick-\nering. 2013. From language-specific to shared syntac-\ntic representations: The influence of second language\nproficiency on syntactic sharing in bilinguals. Cogni-\ntion, 127(3):287â€“306.\nMireille Besson, Marta Kutas, and Cyma Van Petten.\n1992. An Event-Related Potential (ERP) Analysis of\nSemantic Congruity and Repetition Effects in Sen-\ntences. Journal of Cognitive Neuroscience, 4(2):132â€“\n149.\nTerra Blevins and Luke Zettlemoyer. 2022. Language\nContamination Helps Explains the Cross-lingual Ca-\npabilities of English Pretrained Models. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 3563â€“3574,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nJ. Kathryn Bock. 1986. Syntactic persistence in lan-\nguage production. Cognitive Psychology, 18(3):355â€“\n387.\n3712\nHolly P. Branigan and Martin J. Pickering. 2017. An\nexperimental approach to linguistic representation.\nBehavioral and Brain Sciences, 40:e282.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage Models are Few-Shot Learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877â€“1901. Curran Associates,\nInc.\nJoan Bybee. 2010. Language, Usage and Cognition .\nCambridge University Press, Cambridge.\nZhenguang G. Cai, Martin J. Pickering, and Holly P.\nBranigan. 2012. Mapping concepts to syntax: Evi-\ndence from structural priming in Mandarin Chinese.\nJournal of Memory and Language, 66(4):833â€“849.\nYuan Chai, Yaobo Liang, and Nan Duan. 2022. Cross-\nLingual Ability of Multilingual Masked Language\nModels: A Study of Language Structure. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 4702â€“4712, Dublin, Ireland. Association\nfor Computational Linguistics.\nTyler Chang, Zhuowen Tu, and Benjamin Bergen. 2022.\nThe geometry of multilingual language model repre-\nsentations. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 119â€“136, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nBaoguo Chen, Yuefang Jia, Zhu Wang, Susan Dunlap,\nand Jeong-Ah Shin. 2013. Is word-order similar-\nity necessary for cross-linguistic structural priming?\nSecond Language Research, 29(4):375â€“389.\nEthan A Chi, John Hewitt, and Christopher D Manning.\n2020. Finding universal grammatical relations in\nmultilingual bert. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5564â€“5577, Online. Association for\nComputational Linguistics.\nSunjoo Choi and Myung-Kwan Park. 2022. Syntac-\ntic priming in the L2 neural language model. The\nJournal of Linguistic Science, 103:81â€“104.\nNoam Chomsky. 1965. Aspects of the Theory of Syntax.\nMIT Press, Cambridge, MA, USA.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440â€“\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b. Emerging\nCross-lingual Structure in Pretrained Language Mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6022â€“6034, Online. Association for Computational\nLinguistics.\nPablo Contreras Kallens, Ross Deans Kristensen-\nMcLachlan, and Morten H. Christiansen. 2023.\nLarge Language Models Demonstrate the Potential of\nStatistical Learning in Language. Cognitive Science,\n47(3):e13256.\nMarta R. Costa-jussÃ , James Cross, Onur Ã‡elebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffer-\nnan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco GuzmÃ¡n, Philipp\nKoehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\n2022. No Language Left Behind: Scaling Human-\nCentered Machine Translation.\nAndrea Gregor de Varda and Marco Marelli. 2023. Data-\ndriven Cross-lingual Syntax: An Agreement Study\nwith Massively Multilingual Models. Computational\nLinguistics, pages 1â€“39.\nGary S. Dell and Victor S. Ferreira. 2016. Thirty years\nof structural priming: An introduction to the special\nissue. Journal of Memory and Language, 91:1â€“4.\nJuuso Eronen, Michal Ptaszynski, and Fumito Masui.\n2023. Zero-shot cross-lingual transfer language se-\nlection using linguistic similarity. Information Pro-\ncessing & Management, 60(3):103250.\nVictor S. Ferreira and Kathryn Bock. 2006. The func-\ntions of structural priming. Language and Cognitive\nProcesses, 21(7-8):1011â€“1029.\nZuzanna Fleischer, Martin J. Pickering, and Janet F.\nMcLean. 2012. Shared information structure: Evi-\ndence from cross-linguistic priming. Bilingualism:\nLanguage and Cognition, 15(3):568â€“579.\nStefan Frank. 2021. Cross-language structural priming\nin recurrent neural network language models. Pro-\nceedings of the Annual Meeting of the Cognitive Sci-\nence Society, 43(43).\nDaniela Gerz, Ivan VuliÂ´c, Edoardo Maria Ponti, Roi Re-\nichart, and Anna Korhonen. 2018. On the Relation\nbetween Linguistic Typology and (Limitations of)\n3713\nMultilingual Language Modeling. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 316â€“327, Brussels,\nBelgium. Association for Computational Linguistics.\nAdele Goldberg. 2006. Constructions at Work: The\nNature of Generalization in Language. Oxford Uni-\nversity Press, Oxford, New York.\nAdam Goodkind and Klinton Bicknell. 2018. Predictive\npower of word surprisal for reading times is a linear\nfunction of language model quality. In Proceedings\nof the 8th Workshop on Cognitive Modeling and Com-\nputational Linguistics (CMCL 2018), pages 10â€“18,\nSalt Lake City, Utah. Association for Computational\nLinguistics.\nNaman Goyal, Jingfei Du, Myle Ott, Giri Ananthara-\nman, and Alexis Conneau. 2021. Larger-Scale Trans-\nformers for Multilingual Masked Language Model-\ning. In Proceedings of the 6th Workshop on Represen-\ntation Learning for NLP (RepL4NLP-2021) , pages\n29â€“33, Online. Association for Computational Lin-\nguistics.\nRaffaele Guarasci, Stefano Silvestri, Giuseppe\nDe Pietro, Hamido Fujita, and Massimo Esposito.\n2022. BERT syntactic transfer: A computational\nexperiment on Italian, French and English languages.\nComputer Speech & Language, 71:101261.\nRobert J. Hartsuiker, Martin J. Pickering, and Eline\nVeltkamp. 2004. Is Syntax Separate or Shared Be-\ntween Languages?: Cross-Linguistic Syntactic Prim-\ning in Spanish-English Bilinguals. Psychological\nScience, 15(6):409â€“414.\nPetra Hendriks. 2014. Asymmetries between Language\nProduction and Comprehension, volume 42 of Stud-\nies in Theoretical Psycholinguistics. Springer Nether-\nlands, Dordrecht.\nJohn Hewitt and Christopher D. Manning. 2019. A\nStructural Probe for Finding Syntax in Word Repre-\nsentations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129â€“4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMaren Heydel and Wayne S. Murray. 2000. Concep-\ntual Effects in Sentence Priming: A Cross-Linguistic\nPerspective. In Marica De Vincenzi and Vincenzo\nLombardo, editors, Cross-Linguistic Perspectives on\nLanguage Processing, Studies in Theoretical Psy-\ncholinguistics, pages 227â€“254. Springer Netherlands,\nDordrecht.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatherine Millican, George van den Driessche, Bog-\ndan Damoc, Aurelia Guy, Simon Osindero, Karen\nSimonyan, Erich Elsen, Oriol Vinyals, Jack William\nRae, and Laurent Sifre. 2022. An empirical analysis\nof compute-optimal large language model training.\nIn Advances in Neural Information Processing Sys-\ntems.\nNora Hollenstein, Federico Pirovano, Ce Zhang, Lena\nJÃ¤ger, and Lisa Beinborn. 2021. Multilingual lan-\nguage models predict human reading behavior. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 106â€“123, Online. Association for Computa-\ntional Linguistics.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The Curious Case of Neural Text\nDegeneration. In International Conference on Learn-\ning Representations.\nYufen Hsieh. 2017. Structural priming during sentence\ncomprehension in Chineseâ€“English bilinguals. Ap-\nplied Psycholinguistics, 38(3):657â€“678.\nAlexander Jones, William Yang Wang, and Kyle Ma-\nhowald. 2021. A Massively Multilingual Analysis of\nCross-linguality in Shared Embedding Space. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 5833â€“\n5847, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The State and\nFate of Linguistic Diversity and Inclusion in the NLP\nWorld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282â€“6293, Online. Association for Computational\nLinguistics.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of Tricks for Efficient\nText Classification. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 427â€“431, Valencia, Spain. Association\nfor Computational Linguistics.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and Dan\nRoth. 2020. Cross-Lingual Ability of Multilingual\nBERT: An Empirical Study. In International Confer-\nence on Learning Representations.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling Laws for Neural Language Models.\nStav Klein and Reut Tsarfaty. 2020. Getting the ##life\nout of living: How Adequate Are Word-Pieces for\nModelling Complex Morphology? In Proceedings\nof the 17th SIGMORPHON Workshop on Computa-\ntional Research in Phonetics, Phonology, and Mor-\nphology, pages 204â€“209, Online. Association for\nComputational Linguistics.\n3714\nSotiria Kotzochampou and Vasiliki Chondrogianni.\n2022. How similar are shared syntactic represen-\ntations? Evidence from priming of passives in\nGreekâ€“English bilinguals. Bilingualism: Language\nand Cognition, 25(5):726â€“738.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for Neural Text Processing.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66â€“71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nTatsuki Kuribayashi, Yohei Oseki, Takumi Ito, Ryo\nYoshida, Masayuki Asahara, and Kentaro Inui. 2021.\nLower Perplexity is Not Always Human-Like. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 5203â€“\n5217, Online. Association for Computational Lin-\nguistics.\nBai Li, Zining Zhu, Guillaume Thomas, Frank Rudzicz,\nand Yang Xu. 2022. Neural reality of argument struc-\nture constructions. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 7410â€“7423,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian Oâ€™Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-\nanov, and Xian Li. 2022. Few-shot Learning with\nMultilingual Generative Language Models. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 9019â€“\n9052, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nHelga Loebell and Kathryn Bock. 2003. Structural\npriming across languages. Linguistics, 41(5):791â€“\n824.\nKyle Mahowald, Ariel James, Richard Futrell, and Ed-\nward Gibson. 2016. A meta-analysis of syntactic\npriming in language production. Journal of Memory\nand Language, 91:5â€“27.\nClara D. Martin, Francesca M. Branzi, and Moshe Bar.\n2018. Prediction is Production: The missing link\nbetween language production and comprehension.\nScientific Reports, 8(1):1079.\nAntje S. Meyer, Falk Huettig, and Willem J.M. Levelt.\n2016. Same, different, or closely related: What is\nthe relationship between language production and\ncomprehension? Journal of Memory and Language,\n89:1â€“7.\nJames A. Michaelov, Seana Coulson, and Benjamin K.\nBergen. 2022. So Cloze yet so Far: N400 Amplitude\nis Better Predicted by Distributional Information than\nHuman Predictability Judgements. IEEE Transac-\ntions on Cognitive and Developmental Systems.\nPenny F. Mitchell, Sally Andrews, and Philip B. Ward.\n1993. An event-related potential study of seman-\ntic congruity and repetition in a sentence-reading\ntask: Effects of context change. Psychophysiology,\n30(5):496â€“509.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023. Crosslingual Generaliza-\ntion through Multitask Finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15991â€“16111, Toronto, Canada. Association\nfor Computational Linguistics.\nMerel Muylle, Sarah Bernolet, and Robert J. Hartsuiker.\n2020. The Role of Case Marking and Word Order\nin Cross-Linguistic Structural Priming in Late L2\nAcquisition. Language Learning, 70(S2):194â€“220.\nAntoine Nzeyimana and Andre Niyongabo Rubungo.\n2022. KinyaBERT: A Morphology-aware Kin-\nyarwanda Language Model. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5347â€“5363, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021.\nSmall Data? No Problem! Exploring the Viabil-\nity of Pretrained Multilingual Language Models for\nLow-resourced Languages. In Proceedings of the 1st\nWorkshop on Multilingual Representation Learning,\npages 116â€“126, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nByung-Doh Oh and William Schuler. 2023. Why Does\nSurprisal From Larger Transformer-Based Language\nModels Provide a Poorer Fit to Human Reading\nTimes? Transactions of the Association for Com-\nputational Linguistics, 11:336â€“350.\nAkintunde Oladipo, Odunayo Ogundepo, Kelechi\nOgueji, and Jimmy Lin. 2022. An Exploration of\nV ocabulary Size and Transfer Effects in Multilingual\nLanguage Models for African Languages. In 3rd\nWorkshop on African Natural Language Processing.\nChanjun Park, Sugyeong Eo, Hyeonseok Moon, and\nHeuiseok Lim. 2021. Should we find another model?:\nImproving Neural Machine Translation Performance\nwith ONE-Piece Tokenization Method without Model\nModification. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\n3715\nTechnologies: Industry Papers, pages 97â€“104, Online.\nAssociation for Computational Linguistics.\nMartin J. Pickering and Victor S. Ferreira. 2008. Struc-\ntural priming: A critical review. Psychological Bul-\nletin, 134:427â€“459.\nMartin J. Pickering and Simon Garrod. 2007. Do peo-\nple use language production to make predictions dur-\ning comprehension? Trends in Cognitive Sciences,\n11(3):105â€“110.\nMartin J. Pickering and Simon Garrod. 2013. An inte-\ngrated theory of language production and comprehen-\nsion. Behavioral and Brain Sciences, 36(4):329â€“347.\nGrusha Prasad, Marten van Schijndel, and Tal Linzen.\n2019. Using Priming to Uncover the Organization\nof Syntactic Representations in Neural Language\nModels. In Proceedings of the 23rd Conference on\nComputational Natural Language Learning (CoNLL),\npages 66â€“76, Hong Kong, China. Association for\nComputational Linguistics.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson dâ€™Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2022. Scal-\ning Language Models: Methods, Analysis & Insights\nfrom Training Gopher.\nDavid Reitter, Frank Keller, and Johanna D. Moore.\n2011. A Computational Cognitive Model of Syntac-\ntic Priming. Cognitive Science, 35(4):587â€“637.\nJoost Rommers and Kara D. Federmeier. 2018. Pre-\ndictabilityâ€™s aftermath: Downstream consequences of\nword predictability as revealed by repetition effects.\nCortex, 101:16â€“30.\nMichael D. Rugg. 1985. The Effects of Semantic Prim-\ning and Word Repetition on Event-Related Potentials.\nPsychophysiology, 22(6):642â€“647.\nMichael D. Rugg. 1990. Event-related brain poten-\ntials dissociate repetition effects of high-and low-\nfrequency words. Memory & Cognition, 18(4):367â€“\n379.\nSofie Schoonbaert, Robert J. Hartsuiker, and Martin J.\nPickering. 2007. The representation of lexical and\nsyntactic information in bilinguals: Evidence from\nsyntactic priming. Journal of Memory and Language,\n56(2):153â€“171.\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila\nYerukola, and Christopher D. Manning. 2019. Do\nMassively Pretrained Language Models Make Better\nStorytellers? In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 843â€“861, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nAmit Seker, Elron Bandel, Dan Bareket, Idan\nBrusilovsky, Refael Greenfeld, and Reut Tsarfaty.\n2022. AlephBERT: Language Model Pre-training\nand Evaluation from Sub-Word to Sentence Level.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 46â€“56, Dublin, Ireland. Asso-\nciation for Computational Linguistics.\nJeong-Ah Shin. 2010. Structural priming and L2 pro-\nficiency effects on bilingual syntactic processing in\nproduction. Korean Journal of English Language\nand Linguistics, 10(3):499â€“518.\nJeong-Ah Shin and Kiel Christianson. 2009. Syntac-\ntic processing in Koreanâ€“English bilingual produc-\ntion: Evidence from cross-linguistic structural prim-\ning. Cognition, 112(1):175â€“180.\nAnna Siewierska. 1993. Syntactic weight vs informa-\ntion structure and word order variation in Polish.\nJournal of Linguistics, 29(2):233â€“265.\nArabella Sinclair, Jaap Jumelet, Willem Zuidema, and\nRaquel FernÃ¡ndez. 2022. Structural Persistence in\nLanguage Models: Priming as a Window into Ab-\nstract Language Representations. Transactions of the\nAssociation for Computational Linguistics, 10:1031â€“\n1050.\nPaul Soulos, Sudha Rao, Caitlin Smith, Eric Rosen,\nAsli Celikyilmaz, R. Thomas McCoy, Yichen Jiang,\nColeman Haley, Roland Fernandez, Hamid Palangi,\nJianfeng Gao, and Paul Smolensky. 2021. Structural\nBiases for Improving Transformers on Translation\ninto Morphologically Rich Languages. In Proceed-\nings of the 4th Workshop on Technologies for MT\nof Low Resource Languages (LoResMT2021), pages\n52â€“67, Virtual. Association for Machine Translation\nin the Americas.\nMichael Tomasello. 2003. Constructing a Language: A\nUsage-Based Theory of Language Acquisition. Har-\nvard University Press.\n3716\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, TimothÃ©e Lacroix,\nBaptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. LLaMA: Open\nand Efficient Foundation Language Models.\nDimitra Irini Tzanidaki. 1995. Greek word order: to-\nwards a new approach. UCL Working Paper in Lin-\nguistics, 7:247â€“277.\nRoger P. G. van Gompel and Manabu Arai. 2018. Struc-\ntural priming in bilinguals. Bilingualism: Language\nand Cognition, 21(3):448â€“455.\nCyma Van Petten, Marta Kutas, Robert Kluender, Mark\nMitchiner, and Heather McIsaac. 1991. Fractionating\nthe Word Repetition Effect with Event-Related Poten-\ntials. Journal of Cognitive Neuroscience, 3(2):131â€“\n150.\nElena V oita and Ivan Titov. 2020. Information-\nTheoretic Probing with Minimum Description\nLength. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 183â€“196, Online. Association for\nComputational Linguistics.\nXiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li,\nPei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei\nCao, Binbin Xie, Tianxiang Hu, Shangjie Li, Binyuan\nHui, Bowen Yu, Dayiheng Liu, Baosong Yang, Fei\nHuang, and Jun Xie. 2023. PolyLM: An Open Source\nPolyglot Large Language Model.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco GuzmÃ¡n, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet: Ex-\ntracting High Quality Monolingual Datasets from\nWeb Crawl Data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n4003â€“4012, Marseille, France. European Language\nResources Association.\nEthan Wilcox, Pranali Vani, and Roger Levy. 2021. A\ntargeted assessment of incremental processing in neu-\nral language models and humans. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers) , pages 939â€“952, Online.\nAssociation for Computational Linguistics.\nGenta Winata, Shijie Wu, Mayank Kulkarni, Thamar\nSolorio, and Daniel Preotiuc-Pietro. 2022. Cross-\nlingual Few-Shot Learning on Unseen Languages. In\nProceedings of the 2nd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 12th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 777â€“791, Online only. Association\nfor Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven\nLe Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander Rush. 2020. Transformers:\nState-of-the-Art Natural Language Processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38â€“45, Online. Association\nfor Computational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing\nLiu, Åukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\nTaku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, Wei Wang, Cliff Young, Jason\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals,\nGreg Corrado, Macduff Hughes, and Jeffrey Dean.\n2016. Googleâ€™s Neural Machine Translation Sys-\ntem: Bridging the Gap between Human and Machine\nTranslation.\nZhengxuan Wu, Isabel Papadimitriou, and Alex Tamkin.\n2022. Oolong: Investigating What Makes Crosslin-\ngual Transfer Hard with Controlled Studies.\nJin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang\nLi, and Jian Li. 2022. Learning to Break the Loop:\nAnalyzing and Mitigating Repetitions for Neural Text\nGeneration. Advances in Neural Information Pro-\ncessing Systems, 35:3082â€“3095.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A Massively Multilingual\nPre-trained Text-to-Text Transformer. In Proceed-\nings of the 2021 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, pages 483â€“\n498, Online. Association for Computational Linguis-\ntics.\nJayden Ziegler, Rodrigo Morato, and Jesse Snedeker.\n2019. Priming semantic structure in Brazilian Por-\ntuguese. Journal of Cultural Cognitive Science ,\n3(1):25â€“37.\nA Language Contamination in\nMultilingual Language Models\nIn this section, we estimate language contamina-\ntion in CC-100-XL, the dataset used to train the\nXGLM models. While the dataset itself is not made\navailable by Lin et al. (2022), the procedure used\nfor language identification is similar to CC-100\n(Conneau et al., 2020a; Wenzek et al., 2020).\nWhile there are some differences in the ap-\nproaches used for filtering languages to ensure high-\nquality data, both corpora are based on Common-\nCrawl snapshots and are divided into languages\n3717\nDutch Polish\nLanguage ID Tool Proportion Estimated Tokens Proportion Estimated tokens\ncld3 0.03051% 152,528,079 0.00668% 33,418,112\nfastText 0.00212% 10,595,403 0.00157% 7,841,824\nConsensus (cld3 + fastText) 0.00035% 1,774,765 0.00029% 1,456,856\nTable 1: Estimated Dutch and Polish contamination in the training data of XGLM 564M, 1.7B, 2.9B, and 7.5B,\nbased on language identification using cld3 and fastText, only considering tokens that both language identification\nmodels predict to be Dutch or Polish.\nusing the fastText language identification model\n(Joulin et al., 2017). Both CC-100 and CC-100-\nXL also involve a further language identification\nstep. For CC-100, an unnamed internal tool is also\nused for language identification; for CC-100-XL,\nan additional step of language identification takes\nplace where text language is also identified at the\nparagraph level.\nTo test for Dutch and Polish contamination, we\nsample roughly 100M tokens (based on the XGLM\n7.5B tokenizer) of all languages in the replicated\nCC-100 dataset2 that XLGM 564M, 1.7B, 2.9B,\nand 7.5B are trained on. We only consider lan-\nguages that have 100M or more tokens in CC-100\nand that either use the Latin alphabet (Spanish,\nFrench, Italian, Portuguese, Finnish, Indonesian,\nTurkish, Vietnamese, Catalan, Estonian, Swahili,\nBasque), are Slavic (Russian, Bulgarian), or both\n(English, German). Specifically, we sample from\neach of these languages until we have enough doc-\numents that the number of tokens in each language\nis at least 100M. Thus, our sample of CC-100 in-\ncludes roughly 1.6B tokens.\nTo replicate the additional filtering of CC-100-\nXL, we split all documents by paragraph and run\nlanguage identification on them using the latest ver-\nsion of the fastText language identification model\nreleased as part of the â€œNo Language Left Behindâ€\nproject (Costa-jussÃ  et al., 2022). We set the identi-\nfication threshold to 0.5, which the authors find to\nbe effective for lower-resource languages (which\nsome of our sampled languages are among). We\nnote that this is a newer and likely more accurate\nversion of the language identification model than\nthat used to create CC-100-XL, and thus it is even\nless likely to include data from languages other\nthan those intended. We only analyze the data from\nparagraphs identified to be the same language as\nthe document label.\nTo identify Dutch and Polish in these paragraphs,\n2https://data.statmt.org/cc-100/\nwe divide paragraphs into sentences by splitting at\neach period character, and we run each sentence\nthrough both the aforementioned latest version of\nthe fastText language identification model (Costa-\njussÃ  et al., 2022; Joulin et al., 2017) and the cld3\nlanguage identifier (Xue et al., 2021) as provided\nin the gcld3 python package (Al-Rfou, 2020). We\nuse a stricter threshold of 0.9 (as recommended for\nhigh-resource languages; Costa-jussÃ  et al., 2022)\nfor the former and use the default threshold of 0.7\nfor the latter.3\nTo estimate the total amount of contamination in\neach of these languages, we calculate the propor-\ntion of each language sample that includes Dutch\nor Polish. We then multiply this by the number of\ntokens in each language, which we estimate by mul-\ntiplying the proportions given in Figure 1 of Lin\net al. (2022) by 500B, the total number of tokens.\nWe first provide two estimates of contamination for\nDutch and Polish in Table 1: the amount of con-\ntamination as identified by the fastText language\nidentification model, and the amount identified by\ncld3. We also provide a third, more conservative\nestimate, that only includes the tokens that both\nlanguage identification models identify as either\nDutch or Polish. We note that because we only\nlook at data from 16 of the 30 training languages,\nthese numbers are likely to substantially underesti-\nmate the amount of language contamination in the\nXGLM pre-training data.\nB Statistical Tests\nWe provide the full results of the statistical tests for\nXGLM 4.5B (Table 2), the PolyLMs (Table 3), and\nthe remaining XGLMs (Table 4).\n3See https://github.com/google/cld3/blob/\nmaster/src/nnet_language_identifier.h and\nhttps://github.com/google/cld3/blob/master/src/\nnnet_language_identifier.cc.\n3718\nLanguage Model Study Language Pair F df 1 df2 p\nXGLM 4.5B Bernolet et al. (2013) Dutch â†’English_Target 151.98 1 144 <0.0001\nBernolet et al. (2013) English â†’Dutch_Target 24.00 1 141 <0.0001\nCai et al. (2012) Experiment 1/2 Mandarin â†’Mandarin_Target 192.37 1 24 <0.0001\nCai et al. (2012) Experiment 3 Mandarin â†’Mandarin_Target 419.66 1 32 <0.0001\nFleischer et al. (2012) English â†’Polish_Target 1.35 1 31 0.2955\nFleischer et al. (2012) Polish â†’English_Target 0.96 1 32 0.3704\nHartsuiker et al. (2004) English â†’Spanish_Target 9.17 1 112 0.0056\nHartsuiker et al. (2004) Spanish â†’English_Target 4.33 1 112 0.0558\nKotzochampou et al. (2022) English â†’Greek_Target 7.28 1 24 0.0201\nKotzochampou et al. (2022) Greek â†’English_Target 5.05 1 24 0.0485\nKotzochampou et al. (2022) Greek â†’Greek_Target 8.40 1 24 0.0132\nLoebell et al. (2003) English â†’German_Target 0.13 1 16 0.7462\nLoebell et al. (2003) German â†’English_Target 0.10 1 16 0.7647\nSchoonbaert et al. (2007) Dutch â†’Dutch_Target 385.71 1 144 <0.0001\nSchoonbaert et al. (2007) Dutch â†’English_Target 57.28 1 144 <0.0001\nSchoonbaert et al. (2007) English â†’Dutch_Target 134.53 1 137 <0.0001\nTable 2: Statistical tests of structural priming for XGLM 4.5B.\nLanguage Model Study Language Pair F df 1 df2 p\nPolyLM 1.7B Bernolet et al. (2013) Dutch â†’English_Target 116.87 1 144 <0.0001\nBernolet et al. (2013) English â†’Dutch_Target 18.80 1 144 <0.0001\nCai et al. (2012) Experiment 1/2 Mandarin â†’Mandarin_Target 164.45 1 24 <0.0001\nCai et al. (2012) Experiment 3 Mandarin â†’Mandarin_Target 228.25 1 32 <0.0001\nFleischer et al. (2012) English â†’Polish_Target 7.50 1 32 0.0165\nFleischer et al. (2012) Polish â†’English_Target 7.34 1 32 0.0174\nHartsuiker et al. (2004) English â†’Spanish_Target 2.47 1 112 0.1498\nHartsuiker et al. (2004) Spanish â†’English_Target 1.76 1 112 0.2280\nKotzochampou et al. (2022) English â†’Greek_Target 0.13 1 24 0.7462\nKotzochampou et al. (2022) Greek â†’English_Target 0.13 1 24 0.7462\nKotzochampou et al. (2022) Greek â†’Greek_Target 8.50 1 24 0.0128\nLoebell et al. (2003) English â†’German_Target 1.39 1 16 0.2955\nLoebell et al. (2003) German â†’English_Target 2.66 1 16 0.1525\nSchoonbaert et al. (2007) Dutch â†’Dutch_Target 105.51 1 144 <0.0001\nSchoonbaert et al. (2007) Dutch â†’English_Target 55.84 1 144 <0.0001\nSchoonbaert et al. (2007) English â†’Dutch_Target 140.97 1 144 <0.0001\nPolyLM 13B Bernolet et al. (2013) Dutch â†’English_Target 193.43 1 144 <0.0001\nBernolet et al. (2013) English â†’Dutch_Target 16.73 1 144 0.0002\nCai et al. (2012) Experiment 1/2 Mandarin â†’Mandarin_Target 141.67 1 24 <0.0001\nCai et al. (2012) Experiment 3 Mandarin â†’Mandarin_Target 257.28 1 32 <0.0001\nFleischer et al. (2012) English â†’Polish_Target 2.45 1 32 0.1570\nFleischer et al. (2012) Polish â†’English_Target 0.29 1 32 0.6275\nHartsuiker et al. (2004) English â†’Spanish_Target 21.87 1 112 <0.0001\nHartsuiker et al. (2004) Spanish â†’English_Target 41.60 1 112 <0.0001\nKotzochampou et al. (2022) English â†’Greek_Target 0.70 1 24 0.4481\nKotzochampou et al. (2022) Greek â†’English_Target 0.54 1 24 0.5062\nKotzochampou et al. (2022) Greek â†’Greek_Target 9.03 1 24 0.0106\nLoebell et al. (2003) English â†’German_Target 5.36 1 16 0.0485\nLoebell et al. (2003) German â†’English_Target 1.51 1 16 0.2794\nSchoonbaert et al. (2007) Dutch â†’Dutch_Target 260.25 1 144 <0.0001\nSchoonbaert et al. (2007) Dutch â†’English_Target 129.76 1 144 <0.0001\nSchoonbaert et al. (2007) English â†’Dutch_Target 58.52 1 144 <0.0001\nTable 3: Statistical tests of structural priming for PolyLM 1.7B and 13B.\n3719\nLanguage Model Study Language Pair F df 1 df2 p\nXGLM 564M Bernolet et al. (2013) Dutch â†’English_Target 12.89 1 144 0.0010\nBernolet et al. (2013) English â†’Dutch_Target 16.59 1 144 0.0002\nCai et al. (2012) Experiment 1/2 Mandarin â†’Mandarin_Target 301.39 1 24 <0.0001\nCai et al. (2012) Experiment 3 Mandarin â†’Mandarin_Target 1006.36 1 32 <0.0001\nFleischer et al. (2012) English â†’Polish_Target 1.05 1 32 0.3497\nFleischer et al. (2012) Polish â†’English_Target 10.30 1 32 0.0056\nHartsuiker et al. (2004) English â†’Spanish_Target 0.51 1 112 0.5076\nHartsuiker et al. (2004) Spanish â†’English_Target 4.72 1 112 0.0471\nKotzochampou et al. (2022) English â†’Greek_Target 5.90 1 24 0.0352\nKotzochampou et al. (2022) Greek â†’English_Target 11.25 1 24 0.0051\nKotzochampou et al. (2022) Greek â†’Greek_Target 10.80 1 24 0.0056\nLoebell et al. (2003) English â†’German_Target 3.65 1 16 0.1001\nLoebell et al. (2003) German â†’English_Target 2.76 1 16 0.1494\nSchoonbaert et al. (2007) Dutch â†’Dutch_Target 545.14 1 144 <0.0001\nSchoonbaert et al. (2007) Dutch â†’English_Target 5.66 1 144 0.0291\nSchoonbaert et al. (2007) English â†’Dutch_Target 55.69 1 144 <0.0001\nXGLM 1.7B Bernolet et al. (2013) Dutch â†’English_Target 17.64 1 144 0.0001\nBernolet et al. (2013) English â†’Dutch_Target 32.57 1 144 <0.0001\nCai et al. (2012) Experiment 1/2 Mandarin â†’Mandarin_Target 751.15 1 24 <0.0001\nCai et al. (2012) Experiment 3 Mandarin â†’Mandarin_Target 1519.71 1 32 <0.0001\nFleischer et al. (2012) English â†’Polish_Target 0.08 1 32 0.7761\nFleischer et al. (2012) Polish â†’English_Target 0.69 1 32 0.4481\nHartsuiker et al. (2004) English â†’Spanish_Target 4.76 1 112 0.0467\nHartsuiker et al. (2004) Spanish â†’English_Target 3.19 1 112 0.1026\nKotzochampou et al. (2022) English â†’Greek_Target 2.62 1 24 0.1502\nKotzochampou et al. (2022) Greek â†’English_Target 11.20 1 24 0.0051\nKotzochampou et al. (2022) Greek â†’Greek_Target 18.49 1 24 0.0005\nLoebell et al. (2003) English â†’German_Target 1.80 1 16 0.2358\nLoebell et al. (2003) German â†’English_Target 3.13 1 16 0.1247\nSchoonbaert et al. (2007) Dutch â†’Dutch_Target 312.38 1 144 <0.0001\nSchoonbaert et al. (2007) Dutch â†’English_Target 3.72 1 144 0.0770\nSchoonbaert et al. (2007) English â†’Dutch_Target 55.88 1 134 <0.0001\nXGLM 2.9B Bernolet et al. (2013) Dutch â†’English_Target 47.12 1 144 <0.0001\nBernolet et al. (2013) English â†’Dutch_Target 27.25 1 144 <0.0001\nCai et al. (2012) Experiment 1/2 Mandarin â†’Mandarin_Target 427.12 1 24 <0.0001\nCai et al. (2012) Experiment 3 Mandarin â†’Mandarin_Target 1363.62 1 32 <0.0001\nFleischer et al. (2012) English â†’Polish_Target 1.31 1 32 0.2988\nFleischer et al. (2012) Polish â†’English_Target 12.11 1 32 0.0031\nHartsuiker et al. (2004) English â†’Spanish_Target 4.61 1 112 0.0489\nHartsuiker et al. (2004) Spanish â†’English_Target 10.42 1 112 0.0033\nKotzochampou et al. (2022) English â†’Greek_Target 3.58 1 24 0.0966\nKotzochampou et al. (2022) Greek â†’English_Target 12.26 1 24 0.0036\nKotzochampou et al. (2022) Greek â†’Greek_Target 16.05 1 24 0.0011\nLoebell et al. (2003) English â†’German_Target 6.22 1 16 0.0362\nLoebell et al. (2003) German â†’English_Target 1.11 1 16 0.3485\nSchoonbaert et al. (2007) Dutch â†’Dutch_Target 327.66 1 144 <0.0001\nSchoonbaert et al. (2007) Dutch â†’English_Target 21.01 1 144 <0.0001\nSchoonbaert et al. (2007) English â†’Dutch_Target 90.89 1 144 <0.0001\nXGLM 7.5B Bernolet et al. (2013) Dutch â†’English_Target 37.88 1 144 <0.0001\nBernolet et al. (2013) English â†’Dutch_Target 21.46 1 144 <0.0001\nCai et al. (2012) Experiment 1/2 Mandarin â†’Mandarin_Target 402.46 1 24 <0.0001\nCai et al. (2012) Experiment 3 Mandarin â†’Mandarin_Target 1193.10 1 32 <0.0001\nFleischer et al. (2012) English â†’Polish_Target 0.08 1 32 0.7761\nFleischer et al. (2012) Polish â†’English_Target 8.96 1 32 0.0093\nHartsuiker et al. (2004) English â†’Spanish_Target 16.41 1 112 0.0002\nHartsuiker et al. (2004) Spanish â†’English_Target 17.28 1 112 0.0002\nKotzochampou et al. (2022) English â†’Greek_Target 3.10 1 24 0.1202\nKotzochampou et al. (2022) Greek â†’English_Target 12.33 1 24 0.0036\nKotzochampou et al. (2022) Greek â†’Greek_Target 9.47 1 24 0.0092\nLoebell et al. (2003) English â†’German_Target 1.86 1 16 0.2310\nLoebell et al. (2003) German â†’English_Target 6.84 1 16 0.0291\nSchoonbaert et al. (2007) Dutch â†’Dutch_Target 402.81 1 144 <0.0001\nSchoonbaert et al. (2007) Dutch â†’English_Target 43.84 1 144 <0.0001\nSchoonbaert et al. (2007) English â†’Dutch_Target 83.09 1 144 <0.0001\nTable 4: Statistical tests of structural priming for XGLM 564M, 1.7B, 2.9B, and 7.5B.\n3720",
  "topic": "Priming (agriculture)",
  "concepts": [
    {
      "name": "Priming (agriculture)",
      "score": 0.6430594325065613
    },
    {
      "name": "Computer science",
      "score": 0.6226513385772705
    },
    {
      "name": "Sentence",
      "score": 0.5770426988601685
    },
    {
      "name": "Natural language processing",
      "score": 0.5366920828819275
    },
    {
      "name": "Linguistics",
      "score": 0.5184549689292908
    },
    {
      "name": "Abstraction",
      "score": 0.5102190971374512
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45421302318573
    },
    {
      "name": "Generalization",
      "score": 0.4483165740966797
    },
    {
      "name": "Mathematics",
      "score": 0.1533123254776001
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Germination",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3145578848",
      "name": "Institute for Cognitive Science Studies",
      "country": "IR"
    },
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ],
  "cited_by": 5
}