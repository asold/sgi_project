{
  "title": "Effective Use of Transformer Networks for Entity Tracking",
  "url": "https://openalex.org/W2971226772",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2117027874",
      "name": "Aditya Gupta",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A1978278429",
      "name": "Greg Durrett",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963925965",
    "https://openalex.org/W2565031282",
    "https://openalex.org/W2124700572",
    "https://openalex.org/W2964285770",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2963984224",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W2963983586",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4253336001",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2561658355",
    "https://openalex.org/W2803267010",
    "https://openalex.org/W2951976932",
    "https://openalex.org/W4302570567",
    "https://openalex.org/W2949493644",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2563734883",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2889317091",
    "https://openalex.org/W2963695529",
    "https://openalex.org/W2964222246",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W2963436881",
    "https://openalex.org/W2897513992",
    "https://openalex.org/W2250379752",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2951025380",
    "https://openalex.org/W2953290652",
    "https://openalex.org/W3011411500"
  ],
  "abstract": "Aditya Gupta, Greg Durrett. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 759–769,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n759\nEffective Use of Transformer Networks for Entity Tracking\nAditya Gupta and Greg Durrett\nDepartment of Computer Science\nThe University of Texas at Austin\n{agupta,gdurrett}@cs.utexas.edu\nAbstract\nTracking entities in procedural language re-\nquires understanding the transformations aris-\ning from actions on entities as well as those en-\ntities’ interactions. While self-attention-based\npre-trained language encoders like GPT and\nBERT have been successfully applied across a\nrange of natural language understanding tasks,\ntheir ability to handle the nuances of proce-\ndural texts is still untested. In this paper, we\nexplore the use of pre-trained transformer net-\nworks for entity tracking tasks in procedural\ntext. First, we test standard lightweight ap-\nproaches for prediction with pre-trained trans-\nformers, and ﬁnd that these approaches under-\nperform even simple baselines. We show that\nmuch stronger results can be attained by re-\nstructuring the input to guide the transformer\nmodel to focus on a particular entity. Sec-\nond, we assess the degree to which transformer\nnetworks capture the process dynamics, inves-\ntigating such factors as merged entities and\noblique entity references. On two different\ntasks, ingredient detection in recipes and QA\nover scientiﬁc processes, we achieve state-of-\nthe-art results, but our models still largely at-\ntend to shallow context clues and do not form\ncomplex representations of intermediate entity\nor process state.1\n1 Introduction\nTransformer based pre-trained language models\n(Devlin et al., 2019; Radford et al., 2018, 2019;\nJoshi et al., 2019; Yang et al., 2019) have been\nshown to perform remarkably well on a range of\ntasks, including entity-related tasks like corefer-\nence resolution (Kantor and Globerson, 2019) and\nnamed entity recognition (Devlin et al., 2019).\nThis performance has been generally attributed to\n1Code to reproduce experiments in this paper is\navailable at https://github.com/aditya2211/\ntransformer-entity-tracking\nthe robust transfer of lexical semantics to down-\nstream tasks. However, these models are still bet-\nter at capturing syntax than they are at more entity-\nfocused aspects like coreference (Tenney et al.,\n2019a,b); moreover, existing state-of-the-art ar-\nchitectures for such tasks often perform well look-\ning at only local entity mentions (Wiseman et al.,\n2016; Lee et al., 2017; Peters et al., 2017) rather\nthan forming truly global entity representations\n(Rahman and Ng, 2009; Lee et al., 2018). Thus,\nperformance on these tasks does not form sufﬁ-\ncient evidence that these representations strongly\ncapture entity semantics. Better understanding the\nmodels’ capabilities requires testing them in do-\nmains involving complex entity interactions over\nlonger texts. One such domain is that of pro-\ncedural language, which is strongly focused on\ntracking the entities involved and their interactions\n(Mori et al., 2014; Dalvi et al., 2018; Bosselut\net al., 2018).\nThis paper investigates the question of how\ntransformer-based models form entity representa-\ntions and what these representations capture. We\nexpect that after ﬁne-tuning on a target task, a\ntransformer’s output representations should some-\nhow capture relevant entity properties, in the sense\nthat these properties can be extracted by shal-\nlow classiﬁcation either from entity tokens or\nfrom marker tokens. However, we observe that\nsuch “post-conditioning” approaches don’t per-\nform signiﬁcantly better than rule-based baselines\non the tasks we study. We address this by propos-\ning entity-centric ways of structuring input to the\ntransformer networks, using the entity to guide the\nintrinsic self-attention and form entity-centric rep-\nresentations for all the tokens. We ﬁnd that our\nproposed methods lead to a signiﬁcant improve-\nment in performance over baselines.\nAlthough our entity-speciﬁc application of\ntransformers is more effective at the entity track-\n760\nSeq. of Steps water mixture sugar\nRoots absorb water from soil. M O O\nThe water flows to the leaf. M O O\nLight from the sun and CO2 enter the leaf. E O O\nLight, water, and CO2 combine into mixture. D C O\nMixture forms sugar. O D C\nSeq. of Steps sugar eggs flour\nCombine sugar, oil, and vanilla 1 0 0\nAdd eggs one at a time 1 1 0\nIn a separate bowl, combine flour, soda, and salt. 0 0 1\nAdd to the sugar mixture alternately with milk 1 1 1\nStir remaining ingredients one at a time. 1 1 1\na) Binary Classification Task for Ingredient Detection (Recipes Dataset) b) Structured Prediction Task for State Changes (ProPara Dataset)\nTracking \nIntermediate \nCompositions\nStructural \nConstraints \nC → M → D \nImplicit Events \nrequiring Global \nKnowledge\nGlobal Tracking \nwithout Explicit \nEntity Mentions\nC  →  Creation\nE  →  Existence\nM →  Movement\nD  →  Destruction\nO  →  Outside Process\n0 → Ingredient  Absent\n1 → Ingredient  Present\nTIME\nFigure 1: Process Examples from (a) R ECIPES as a binary classiﬁcation task of ingredient detection, and (b)\nPROPARA as a structured prediction task of identifying state change sequences. Both require cross-sentence rea-\nsoning, such as knowing what components are in a mixture and understanding verb semantics like combine.\ning tasks we study, we perform additional analy-\nsis and ﬁnd that these tasks still do not encourage\ntransformers to form truly deep entity representa-\ntions. Our performance gain is largely from better\nunderstanding of verb semantics in terms of asso-\nciating process actions with entity the paragraph is\nconditioned on. The model also does not special-\nize in “tracking” composed entities per se, again\nusing surface clues like verbs to identify the com-\nponents involved in a new composition.\nWe evaluate our models on two datasets specif-\nically designed to invoke procedural understand-\ning: (i) R ECIPES (Kiddon et al., 2016), and (ii)\nPROPARA (Dalvi et al., 2018). For the R ECIPES\ndataset, we classify whether an ingredient was af-\nfected in a certain step, which requires understand-\ning when ingredients are combined or the focus of\nthe recipe shifts away from them. The P ROPARA\ndataset involves answering a more complex set of\nquestions about physical state changes of compo-\nnents in scientiﬁc processes. To handle this more\nstructured setting, our transformer produces po-\ntentials consumed by a conditional random ﬁeld\nwhich predicts entity states over time. Using a\nunidirectional GPT-based architecture, we achieve\nstate-of-the-art results on both the datasets; never-\ntheless, analysis shows that our approach still falls\nshort of capturing the full space of entity interac-\ntions.\n2 Background: Process Understanding\nProcedural text is a domain of text involved with\nunderstanding some kind of process, such as a\nphenomenon arising in nature or a set of instruc-\ntions to perform a task. Entity tracking is a core\ncomponent of understanding such texts.\nDalvi et al. (2018) introduced the PROPARA\ndataset to probe understanding of scientiﬁc pro-\ncesses. The goal is to track the sequence of physi-\ncal state changes (creation, destruction, and move-\nment) entites undergo over long sequences of pro-\ncess steps. Past work involves both modeling en-\ntities across time (Das et al., 2019) and captur-\ning structural constraints inherent in the processes\n(Tandon et al., 2018; Gupta and Durrett, 2019)\nFigure 1b shows an example of the dataset posed\nas a structured prediction task, as in (Gupta and\nDurrett, 2019). For such a domain, it is crucial to\ncapture implicit event occurrences beyond explicit\nentity mentions. For example, in fuel goes into the\ngenerator. The generator converts mechanical en-\nergy into electrical energy”, the fuel is implicitly\ndestroyed in the process.\nBosselut et al. (2018) introduced the task of de-\ntecting state changes in recipes in the RECIPES\ndataset and proposed an entity-centric memory\nnetwork neural architecture for simulating action\ndynamics. Figure 1a shows an example from the\nRECIPES dataset with a grid showing ingredient\npresence. We focus speciﬁcally on this core prob-\nlem of ingredient detection; while only one of the\nsub-tasks associated with their dataset, it reﬂects\nsome complex semantics involving understanding\nthe current state of the recipe. Tracking of ingre-\ndients in the cooking domain is challenging owing\nto the compositional nature of recipes whereby in-\ngredients mix together and are aliased as interme-\ndiate compositions.\nWe pose both of these procedural understand-\ning tasks as classiﬁcation problems, predicting the\nstate of the entity at each timestep from a set of\npre-deﬁned classes. In Figure 1, these classes cor-\n761\nrespond to either the presence (1) or absence (0)\nor the sequence of state changes create (C), move\n(M), destroy (D), exists (E), and none (O).\nState-of-the-art approaches on these tasks are\ninherently entity-centric. Separately, it has been\nshown that entity-centric language modeling in a\ncontinuous framework can lead to better perfor-\nmance for LM related tasks (Clark et al., 2018; Ji\net al., 2017). Moreover, external data has shown\nto be useful for modeling process understanding\ntasks in prior work (Tandon et al., 2018; Bosselut\net al., 2018), suggesting that pre-trained models\nmay be effective.\nWith such tasks in place, a strong model will\nideally learn to form robust entity-centric repre-\nsentation at each time step instead of solely rely-\ning on extracting information from the local en-\ntity mentions. This expectation is primarily due to\nthe evolving nature of the process domain where\nentities undergo complex interactions, form inter-\nmediate compositions, and are often accompanied\nby implicit state changes. We now investigate to\nwhat extent this is true in a standard application of\ntransformer models to this problem.\n3 Studying Basic Transformer\nRepresentations for Entity Tracking\n3.1 Post-conditioning Models\nThe most natural way to use the pre-trained trans-\nformer architectures for the entity tracking tasks\nis to simply encode the text sequence and then\nattempt to “read off” entity states from the con-\ntextual transformer representation. We call this\napproach post-conditioning: the transformer runs\nwith no knowledge of which entity or entities we\nare going to make predictions on, but we only\ncondition on the target entity after the transformer\nstage.\nFigure 3 depicts this model. Formally, for\na labelled pair ({s1,s2,...,s t},yet), we encode\nthe tokenized sequence of steps up to the current\ntimestep (the sentences are separated by using a\nspecial [SEP] token), independent of the entity.\nWe denote by X = [h1,h2,...,h m] the contex-\ntualized hidden representation of the m input to-\nkens from the last layer, and by ge = ∑\nent toks\nemb(ei)\nthe entity representation for post conditioning. We\nnow use one of the following two ways to make an\nentity-speciﬁc prediction:\nroots absorb water from soil [CLS]\np1 p2 p3 p4 p5 p6\n+ + ++ + + +\nTransformer\nh1 h2 h3 h4 h5 h6\n...\ng[water]\nBilinear\nAttention\nLinear + \nSoftmax\nClass \nProbabilities\nGeneral Purpose\nTransformer Representation\nTarget Entity\nRepresentation\nPost \nConditioning\nProcess\nTokens\nPositional\nembedding\nContextual\nvectors\n+\nLinear + \nSoftmax\nClass \nProbabilities\nGPTattn GPTindep\nFigure 2: Post-conditioning entity tracking models.\nBottom: the process paragraph is encoded in an\nentity-independent manner with transformer network\nand a separate entity representation g[water] for post-\nconditioning. Top: the two variants for the condition-\ning: (i) GPTattn, and (ii) GPTindep.\nTask Speciﬁc Input Token We append a\n[CLS] token to the input sequence and use the\noutput representation of the [CLS] token denoted\nby h[CLS] concatenated with the learned BPE em-\nbeddings of the entity as the representationce,t for\nour entity tracking system. We then use a linear\nlayer over it to get class probabilities:\nce,t = [h[CLS]; ge]\nP(yt|st,st−1,...,s 1,e) = softmax(ce,tWtask)\nThe aim of the [CLS] token is to encode infor-\nmation related to general entity related semantics\nparticipating in the recipe ( sentence priors). We\nthen use a single linear layer to learn sentence pri-\nors and entity priors independently, without strong\ninteraction. We call this model GPTindep.\nEntity Based Attention Second, we explore a\nmore ﬁne-grained way of using the GPT model\noutputs. Speciﬁcally, we use bilinear attention be-\ntween ge and the transformer output for the pro-\ncess tokens X to get a contextual representation\nce,t for a given entity. Finally, using a feed-\nforward network followed by softmax layer gives\nus the class probabilities:\nai = gT\ne ∗Wsim ∗hi\nα= softmax(a)\nce,t =\n∑\nαi ∗hi\nP(yt|st,st−1,...,s 1,e) = softmax(ce,tWtask)\nThe bilinear attention over the contextual repre-\nsentations of the process tokens allows the model\nto fetch token content relevant to that particular\nentity. We call this model GPTattn.\n762\nVariant Template\nSentence, Entity First [START] Target Entity [SEP] Steps 1 to t −1 [SEP] Step t [CLS]\nSentence, Entity Last [START]Steps 1 to t −1 [SEP] Step t [SEP] Target Entity [CLS]\nDocument, Entity First [START] Target Entity [SEP] Step 1 [CLS] Step 2 [CLS] . . .Step T [CLS]\nDocument, Entity Last [START] Step 1 [SEP] Target Entity [CLS] . . .Step T [SEP] Target Entity [CLS]\nTable 1: Templates for different proposed entity-centric modes of structuring input to the transformer networks.\n3.2 Results and Observations\nWe evaluate the discussed post-conditioning mod-\nels on the ingredient detection task of the RECIPES\ndataset.2 To benchmark the performance, we com-\npare to three rule-based baselines. This includes\n(i) Majority Class, (ii) Exact Matchof an ingredi-\nent ein recipe step st, and (iii) First Occurrence,\nwhere we predict the ingredient to be present in all\nsteps following the ﬁrst exact match. These latter\ntwo baselines capture natural modes of reasoning\nabout the dataset: an ingredient is used when it is\ndirectly mentioned, or it is used in every step af-\nter it is mentioned, reﬂecting the assumption that\na recipe is about incrementally adding ingredients\nto an ever-growing mixture. We also construct\na LSTM baseline to evaluate the performance of\nELMo embeddings (ELMo token and ELMo sent)\n(Peters et al., 2018) compared to GPT.\nTable 2 compares the performance of the dis-\ncussed models against the baselines, evaluating\nper-step entity prediction performance. Using the\nground truth about ingredient’s state, we also re-\nport the uncombined (UR) and combined (CR)\nrecalls, which are per-timestep ingredient recall\ndistinguished by whether the ingredient is explic-\nitly mentioned (uncombined) or part of a mixture\n(combined). Note that Exact Matchand First Occ\nbaselines represent high-precision and high-recall\nregimes for this task, respectively.\nAs observed from the results, the post-\nconditioning frameworks underperform compared\nto the First Occ baseline. While the CR values\nappear to be high, which would suggest that the\nmodel is capturing the addition of ingredients to\nthe mixture, we note that this value is also lower\nthan the corresponding value for First Occ. This\nresult suggests that the model may be approxi-\nmating the behavior of this baseline, but doing so\npoorly. The unconditional self-attention mecha-\n2We discuss training details more in Section 4.1, but\nlargely use a standard GPT training protocol (Radford et al.,\n2018).\nModel P R F1 Acc UR CR\nPerformance Benchmarks\nMajority - - - 57.27 - -\nExact Match 84.94 20.25 32.70 64.39 73.42 4.02\nFirst Occ 65.23 87.17 74.60 74.65 84.88 87.79\nModels\nGPTattn 63.94 71.72 67.60 70.63 54.30 77.04\nGPTindep 67.05 69.07 68.04 72.28 47.09 75.79\nELMotoken 64.96 76.64 70.32 72.35 69.14 78.94\nELMosent 69.09 72.88 70.90 74.48 57.05 77.71\nTable 2: Performance of the rule-based baselines and\nthe post conditioned models on the ingredient detection\ntask of the R ECIPES dataset. These models all under-\nperform First Occ.\nnism of the transformers does not seem sufﬁcient\nto capture the entity details at each time step be-\nyond simple presence or absence. Moreover, we\nsee that GPTindep performs somewhat comparably\nto GPTattn, suggesting that consuming the trans-\nformer’s output with simple attention is not able to\nreally extract the right entity representation.\nFor PROPARA , we observe similar performance\ntrends where the post-conditioning model per-\nformed below par with the state-of-the-art archi-\ntectures.\n4 Entity-Conditioned Models\nThe post-conditioning framework assumes that the\ntransformer network can form strong representa-\ntions containing entity information accessible in a\nshallow way based on the target entity. We now\npropose a model architecture which more strongly\nconditions on the entity as a part of the intrinsic\nself-attention mechanism of the transformers.\nOur approach consists of structuring input to\nthe transformer network to use and guide the self-\nattention of the transformers, conditioning it on\nthe entity. Our main mode of encoding the in-\nput, the entity-ﬁrst method, is shown in Figure 3.\nThe input sequence begins with a [START] to-\nken, then the entity under consideration, then a\n[SEP] token. After each sentence, a [CLS] to-\n763\n[START] water [SEP] roots absorb water from\np1 p2 p3 p4 p5 p6 p7\n+ + + + + + +\n→ Transformer → \nh1 h2 h3 h4 h5 h6 h7\n...\nLinear + \nSoftmax\nClass Probabilities\nsoil [CLS]\np8 p9\n+ +\nh8 h9\nTarget Entity Conditioned \nContextual Embedding\nTarget \nEntity \nProcess \nTokens\nFigure 3: Entity conditioning model for guiding self-\nattention: the entity-ﬁrst, sentence-level input variant\nfed into a left-to-right unidirectional transformer archi-\ntecture. Task predictions are made at [CLS] tokens\nabout the entity’s state after the prior sentence.\nken is used to anchor the prediction for that sen-\ntence. In this model, the transformer can always\nobserve the entity it should be primarily “attend-\ning to” from the standpoint of building representa-\ntions. We also have an entity-last variant where\nthe entity is primarily observed just before the\nclassiﬁcation token to condition the [CLS] to-\nken’s self-attention accordingly. These variants\nare naturally more computationally-intensive than\npost-conditioned models, as we need to rerun the\ntransformer for each distinct entity we want to\nmake a prediction for.\nSentence Level vs. Document Level As an ad-\nditional variation, we can either run the trans-\nformer once per document with multiple [CLS]\ntokens (a document-level model as shown in Fig-\nure 3) or specialize the prediction to a single\ntimestep (a sentence-level model). In a sentence\nlevel model, we formulate each pair of entityeand\nprocess step tas a separate instance for our classi-\nﬁcation task. Thus, for a process with T steps and\nmentities we get T ×minput sequences for ﬁne\ntuning our classiﬁcation task.\n4.1 Training Details\nIn most experiments, we initialize the network\nwith the weights of the standard pre-trained GPT\nmodel, then subsequently do either domain spe-\nciﬁc LM ﬁne-tuning and supervised task speciﬁc\nﬁne-tuning.\nDomain Speciﬁc LM ﬁne-tuning For some\nprocedural domains, we have access to additional\nunlabeled data. To adapt the LM to capture do-\nmain intricacies, we ﬁne-tune the transformer net-\nwork on this unlabeled corpus.\nSupervised Task Fine-Tuning After the do-\nmain speciﬁc LM ﬁne-tuning, we ﬁne-tune our\nnetwork parameters for the end task of entity\ntracking. For ﬁne-tuning for the task, we have\na labelled dataset which we denote by C, the set\nof labelled pairs ({s1,s2,...,s t},yet) for a given\nprocess. The input is converted according to our\nchosen entity conditioning procedure, then fed\nthrough the pre-trained network.\nIn addition, we observed that adding the lan-\nguage model loss during task speciﬁc ﬁne-tuning\nleads to better performance as well, possibly be-\ncause it adapts the LM to our task-speciﬁc input\nformulation. Thus,\nLtotal = Ltask + λLlm\n4.2 Experiments: Ingredient Detection\nWe ﬁrst evaluate the proposed entity conditioned\nself-attention model on the R ECIPES dataset\nto compare the performance with the post-\nconditioning variants.\n4.2.1 Systems to Compare\nWe use the pre-trained GPT architecture in the\nproposed entity conditioned framework with all its\nvariants. BERT mainly differs in that it is bidirec-\ntional, though we also use the pre-trained [CLS]\nand [SEP] tokens instead of introducing new to-\nkens in the input vocabulary and training them\nfrom scratch during ﬁne-tuning. Owing to the\nlengths of the processes, all our experiments are\nperformed on BERT BASE .\nNeural Process Networks The most signiﬁcant\nprior work on this dataset is the work of Bosselut\net al. (2018). However, their data condition dif-\nfers signiﬁcantly from ours: they train on a large\nnoisy training set and do not use any of the high-\nquality labeled data, instead treating it as dev and\ntest data. Consequently, their model achieves low\nperformance, roughly 56 F1 while ours achieves\n82.5 F1 (though these are not the exact same test\nset). Moreover, theirs underperforms the ﬁrst oc-\ncurrence baseline, which calls into question the\nvalue of that training data. Therefore, we do not\ncompare to this model directly. We use the small\nset of human-annotated data for our probing task.\nOur train/dev/test split consists of 600/100/175\nrecipes, respectively.\n4.2.2 Results\nTable 3 compares the overall performances of\nour proposed models. Our best ET GPT model\nachieves an F1 score of 82.50. Comparing to\n764\nModel P R F1 Acc UR CR\nRule Based Benchmarks\nMajority - - - 57.27 - -\nExact 84.94 20.25 32.70 64.39 73.42 4.02\nFirst 65.23 87.17 74.60 74.65 84.88 87.79\nPost Conditioning Models\nGPTattn 63.94 71.72 67.60 70.63 54.30 77.04\nGPTconcat 67.05 69.07 68.04 72.28 47.09 75.79\nELMotoken 64.96 76.64 70.32 72.35 69.14 78.94\nELMosent 69.09 72.88 70.90 74.48 57.05 77.71\nEntity-Centric Models\nETBERT 72.49 80.09 76.10 78.50 84.30 78.82\nETGPT S⃝L⃝ 75.27 83.85 79.33 81.32 87.28 82.81\nETGPT S⃝F⃝ 76.70 83.98 80.17 82.26 88.20 82.69\nETGPT D⃝L⃝ 79.19 83.82 81.44 83.67 88.11 82.51\nETGPT D⃝F⃝ 79.85 84.19 81.96 84.16 87.91 83.05\nTable 3: Performances of different baseline models dis-\ncussed in Section 3, the ELMo baselines, and the pro-\nposed entity-centric approaches with the (D)ocument\nv (S)entence level variants formulated with both entity\n(F)irst v. (L)ater. Our ETGPT variants all substantially\noutperform the baselines.\nthe baselines ( Majority through First) and post-\nconditioned models, we see that the early en-\ntity conditioning is critical to achieve high perfor-\nmance.\nAlthough the First model still achieves the high-\nest CR, due to operating in a high-recall regime,\nwe see that the ET GPT models all signiﬁcantly\noutperform the post-conditioning models on this\nmetric, indicating better modeling of these com-\npositions. Both recall and precision are substan-\ntially increaesd compared to these baseline mod-\nels. Interestingly, the ELMo-based model under-\nperforms the ﬁrst-occurrence baseline, indicating\nthat the LSTM model is not learning much in\nterms of recognizing complex entity semantics\ngrounded in long term contexts.\nComparing the four variants of structuring input\nin proposed architectures as discussed in Section\n4, we observe that the document-level, entity-\nﬁrst model is the best performing variant. Given\nthe left-to-right unidirectional transformer archi-\ntecture, this model notably forms target-speciﬁc\nrepresentations for all process tokens, compared\nto using the transformer self-attention only to ex-\ntract entity speciﬁc information at the end of the\nprocess.\n4.2.3 Ablations\nWe perform ablations to evaluate the model’s de-\npendency on the context and on the target ingredi-\nModel P R F1 Acc UR CR\nETGPT D⃝F⃝\nw/o ing. 67.47 60.46 63.77 70.64 35.82 67.97\nw/ ing. 79.85 84.19 81.96 84.16 87.91 83.05\nETGPT S⃝F⃝\nw/o context 67.88 75.91 71.67 74.36 87.00 72.52\nw/ context 76.70 83.98 80.17 82.26 88.20 82.69\nTable 4: Top: we compare how much the model de-\ngrades when it conditions on no ingredient at all (w/o\ning.), instead making a generic prediction. Bottom: we\ncompare how much using previous context beyond a\nsingle sentence impacts the model.\nent. Table 4 shows the results for these ablations.\nIngredient Speciﬁcity In the “no ingredient”\nbaseline (w/o ing.), the model is not provided with\nthe speciﬁc ingredient information. Table 4 shows\nthat while not being a strong baseline, the model\nachieves decent overall accuracy with the drop in\nUR being higher compared to CR. This indicates\nthat there are some generic indicators ( mixture)\nthat it can pick up to try to guess at overall ingre-\ndient presence or absence.\nContext Importance We compare with a “no\ncontext” model (w/o context) which ignore the\nprevious context and only use the current recipe\nstep in determining the ingredient’s presence. Ta-\nble 4 shows that the such model is able to perform\nsurprisingly well, nearly as well as the ﬁrst occur-\nrence baseline.\nThis is because the model can often recognize\nwords like verbs (for example, add) or nouns (for\nexample, mixture) that indicate many ingredients\nare being used, and can do well without really\ntracking any speciﬁc entity as desired for the task.\n4.3 State Change Detection (P ROPARA )\nNext, we now focus on a structured task to eval-\nuate the performance of the entity tracking archi-\ntecture in capturing the structural information in\nthe continuous self-attention framework. For this,\nwe use the PROPARA dataset and evaluate our pro-\nposed model on the comprehension task.\nFigure 1b shows an example of a short instance\nfrom the P ROPARA dataset. The task of identify-\ning state change follows a structure satisfying the\nexistence cycle; for example, an entity can not be\ncreated after destruction. Our prior work (Gupta\nand Durrett, 2019) proposed a structured model\nfor the task that achieved state-of-the-art perfor-\n765\nmance. We adapt our proposed entity tracking\ntransformer models to this structured prediction\nframework, capturing creation, movement, exis-\ntence (distinct from movement or creation), de-\nstruction, and non-existence.\nWe use the standard evaluation scheme of\nthe P ROPARA dataset, which is framed as an-\nswering the following categories of questions:\n(Cat-1) Is e created (destroyed, moved) in the\nprocess?, (Cat-2) When (step #) is e created\n(destroyed, moved)?, (Cat-3) Where is e cre-\nated/destroyed/moved from/to)?\n4.3.1 Systems to Compare\nWe compare our proposed models to the previ-\nous work on the P ROPARA dataset. This includes\nthe entity speciﬁc MRC models, EntNet (Henaff\net al., 2017), QRN (Seo et al., 2017), and KG-\nMRC (Das et al., 2019). Also, Dalvi et al. (2018)\nproposed two task speciﬁc models, ProLocal and\nProGlobal, as baselines for the dataset. Finally,\nwe compare against our past neural CRF entity\ntracking model (NCET) (Gupta and Durrett, 2019)\nwhich uses ELMo embeddings in a neural CRF ar-\nchitecture.\nFor the proposed GPT architecture, we use the\ntask speciﬁc [CLS] token to generate tag poten-\ntials instead of class probabilities as we did pre-\nviously. For BERT, we perform a similar modiﬁ-\ncation as described in the previous task to utilize\nthe pre-trained [CLS] token to generate tag po-\ntentials. Finally, we perform a Viterbi decoding\nat inference time to infer the most likely valid tag\nsequence.\n4.3.2 Results\nTable 5 compares the performance of the pro-\nposed entity tracking models on the sentence level\ntask. Since, we are considering the classiﬁcation\naspect of the task, we compare our model per-\nformance for Cat-1 and Cat-2. As shown, the\nstructured document level, entity ﬁrst ETGPT and\nETBERT models achieve state-of-the-art results.\nWe observe that the major source of performance\ngain is attributed to the improvement in identify-\ning the exact step(s) for the state changes (Cat-\n2). This shows that the model are able to better\ntrack the entities by identifying the exact step of\nstate change (Cat-2) accurately rather than just de-\ntecting the presence of such state changes (Cat-1).\nThis task is more highly structured and in some\nways more non-local than ingredient prediction;\nModel Cat-1 Cat-2 Ma-Avg Mi-Avg\nBaselines\nEntNet 51.62 18.83 35.22 37. 03\nQRN 52.37 15.51 33.94 35.97\nProGlobal 62.95 36.39 49.67 51.13\nPrevious Work\nKG-MRC 62.86 40.00 51.43 52.69\nNCET 70.55 44.57 57.56 58.99\nNCETELMo 73.68 47.09 60.38 61.85\nThis Work\nETGPT D⃝F⃝ 73.52 52.21 62.87 64.03\nETBERT 73.55 52.59 63.07 64.22\nTable 5: Performance of the proposed models on the\nPROPARA dataset. Our models outperform strong ap-\nproaches from prior work across all metrics.\nthe high performance here shows that the ET GPT\nmodel is able to capture document level struc-\ntural information effectively. Further, the struc-\ntural constraints from the CRF also aid in mak-\ning better predictions. For example, in the process\n“higher pressure causes the sediment to heat up.\nthe heat causes chemical processes. the material\nbecomes a liquid. is known as oil. ”, the material is\na by-product of the chemical process but there’s no\ndirect mention of it. However, the material ceases\nto exist in the next step, and because the model\nis able to predict this correctly, maintaining con-\nsistency results in the model ﬁnally predicting the\nentire state change correctly as well.\n5 Challenging Task Phenomena\nBased on the results in the previous section, our\nmodels clearly achieve strong performance com-\npared to past approaches. We now revisit the chal-\nlenging cases discussed in Section 2 to see if our\nentity tracking approaches are modeling sophisti-\ncated entity phenomena as advertised. For both\ndatasets and associated tasks, we isolate the spe-\nciﬁc set of challenging cases grounded in track-\ning (i) intermediate compositions formed as part of\ncombination of entities leading to no explicit men-\ntion, and (ii) implicit events which change entities’\nstates without explicit mention of the affects.\n5.1 Ingredient Detection\nFor RECIPES , we mainly want to investigate cases\nof ingredients getting re-engaged in the recipe not\nin a raw form but in a combined nature with other\ningredients and henceforth no explicit mention.\nFor example, eggs in step 4 of Figure 1a exem-\n766\npliﬁes this case. The performance in such cases\nis indicative of how strongly the model can track\ncompositional entities. We also examine the per-\nformance for cases where the ingredient is referred\nby some other name.\nIntermediate Compositions Formally, we pick\nthe set of examples where the ground truth is a\ntransition from 0 →1 (not present to present) and\nthe 1 is a “combined” case. Table 6 shows the\nmodel’s performance on this subset of cases, of\nwhich there are 1049 in the test set. The model\nachieves an accuracy of 51.1% on these bigrams,\nwhich is relatively low given the overall model\nperformance. In the error cases, the model defaults\nto the 1 →1 pattern indicative of the First Occ\nbaseline.\n0 →0 0 →1 1 →0 1 →1\n#preds 179 526 43 301\nTable 6: Model predictions from the document level\nentity ﬁrst GPT model in 1049 cases of intermediate\ncompositions. The model achieves only 51% accuracy\nin these cases.\nHypernymy and Synonymy We observe the\nmodel is able to capture ingredients based on\ntheir hypernyms ( nuts →pecans, salad →let-\ntuce) and rough synonymy ( bourbon →scotch).\nThis performance can be partially attributed to\nthe language model pre-training. We can isolate\nthese cases by ﬁltering for uncombined ingredi-\nents when there is no matching ingredient token\nin the step. Out of 552 such cases in the test set,\nthe model predicts 375 correctly giving a recall of\n67.9. This is lower than overall UR; if pre-training\nbehaves as advertised, we expect little degradation\nin this case, but instead we see performance signif-\nicantly below the average on uncombined ingredi-\nents.\nImpact of external data One question we can\nask of the model’s capabilities is to what extent\nthey arise from domain knowledge in the large\npre-trained data. We train transformer models\nfrom scratch and additionally investigate using the\nlarge corpus of unlabeled recipes for our LM pre-\ntraining. As can be seen in Table 7, the incorpora-\ntion of external data leads to major improvements\nin the overall performance. This gain is largely\ndue to the increase in combined recall. One possi-\nble reason could be that external data leads to bet-\nModel P R F1 Acc UR CR\nNo pre-training, 8 heads, 8 layers, 512 embedding size\nNo LM 66.52 73.48 69.83 72.87 79.20 71.73\n20k 72.53 80.32 76.23 78.59 79.49 80.58\n50k 74.40 81.80 77.92 80.19 81.90 81.77\nStandard GPT pre-training\nNo LM 79.85 84.19 81.96 84.16 87.91 83.05\n20k 80.14 85.01 82.50 84.59 88.83 83.84\nTable 7: Performance for using unsupervised data for\nLM training.\nter understanding of verb semantics and in turn the\nspeciﬁc ingredients forming part of the intermedi-\nate compositions. Figure 4 shows that verbs are\na critical clue the model relies on to make predic-\ntions. Performing LM ﬁne-tuning on top of GPT\nalso gives gains.\n5.2 State Change Detection\nFor PROPARA , Table 5 shows that the model does\nnot signiﬁcantly outperform the SOTA models in\nstate change detection (Cat-1). However, for those\ncorrectly detected events, the transformer model\noutperforms the previous models for detecting the\nexact step of state change (Cat-2), primarily based\non verb semantics. We do a ﬁner-grained study in\nTable 8 by breaking down the performance for the\nthree state changes: creation (C), movement (M),\nand destruction (D), separately. Across the three\nstate changes, the model suffers a loss of perfor-\nmance in the movement cases. This is owing to\nthe fact that the movement cases require a deeper\ncompositional and implicit event tracking. Also,\na majority of errors leading to false negatives are\ndue to the the formation of new sub-entities which\nare then mentioned with other names. For exam-\nple, when talking about weak acid in “the water\nbecomes a weak acid. the water dissolves lime-\nstone” the weak acid is also considered to move\nto the limestone.\nModel Cat-1 Cat-2\nC M D C M D\nETBERT 78.51 61.60 71.50 76.68 54.12 58.62\nETGPT 79.82 56.27 73.83 77.24 50.82 56.27\nTable 8: Results for each state change type. Perfor-\nmance on predicting creation and destruction are high-\nest, partially due to the model’s ability to use verb se-\nmantics for these tasks.\n767\nFigure 4: Gradient of the classiﬁcation loss of the gold class with respect to inputs when predicting the status\nof butter in the last sentence. We follow a similar approach as Jain and Wallace (2019) to compute associations.\nExact matches of the entity receive high weight, as does a seemingly unrelated verb dredge, which often indicates\nthat the butter has already been used and is therefore present.\n6 Analysis\nThe model’s performance on these challenging\ntask cases suggests that even though it outper-\nforms baselines, it may not be capturing deep rea-\nsoning about entities. To understand what the\nmodel actually does, we perform analysis of the\nmodel’s behavior with respect to the input to un-\nderstand what cues it is picking up on.\nGradient based Analysis One way to analyze\nthe model is to compute model gradients with re-\nspect to input features (Sundararajan et al., 2017;\nJain and Wallace, 2019). Figure 4 shows that in\nthis particular example, the most important model\ninputs are verbs possibly associated with the entity\nbutter, in addition to the entity’s mentions them-\nselves. It further shows that the model learns to\nextract shallow clues of identifying actions exerted\nupon only the entity being tracked, regardless of\nother entities, by leveraging verb semantics.\nIn an ideal scenario, we would want the model\nto track constituent entities by translating the “fo-\ncus” to track their newly formed compositions\nwith other entities, often aliased by other names\nlike mixture, blend, pasteetc. However, the low\nperformance on such cases shown in Section 5\ngives further evidence that the model is not doing\nthis.\nInput Ablations We can study which inputs are\nimportant more directly by explicitly removing\nspeciﬁc certain words from the input process para-\ngraph and evaluating the performance of the re-\nsulting input under the current model setup. We\nmainly did experiments to examine the importance\nof: (i) verbs, and (ii) other ingredients.\nTable 9 presents these ablation studies. We only\nobserve a minor performance drop from 84.59 to\n82.71 (accuracy) when other ingredients are re-\nmoved entirely. Removing verbs dropped the per-\nformance to 79.08 and further omitting both leads\nto 77.79. This shows the models dependence on\nInput Accuracy\nComplete Process 84.59\nw/o Other Ingredients 82.71\nw/o Verbs 79.08\nw/o Verbs & Other Ingredients 77.79\nTable 9: Model’s performance degradation with input\nablations. We see that the model’s major source of per-\nformance is from verbs than compared to other ingre-\ndient’s explicit mentions.\nverb semantics over tracking the other ingredients.\n7 Conclusion\nIn this paper, we examined the capabilities of\ntransformer networks for capturing entity state se-\nmantics. First, we show that the conventional\nframework of using the transformer networks is\nnot rich enough to capture entity semantics in\nthese cases. We then propose entity-centric ways\nto formulate richer transformer encoding of the\nprocess paragraph, guiding the self-attention in a\ntarget entity oriented way. This approach leads\nto signiﬁcant performance improvements, but ex-\namining model performance more deeply, we con-\nclude that these models still do not model the inter-\nmediate compositional entities and perform well\nby largely relying on surface entity mentions and\nverb semantics.\nAcknowledgments\nThis work was partially supported by NSF\nGrant IIS-1814522 and an equipment grant from\nNVIDIA. The authors acknowledge the Texas Ad-\nvanced Computing Center (TACC) at The Uni-\nversity of Texas at Austin for providing HPC re-\nsources used to conduct this research. Results\npresented in this paper were obtained using the\nChameleon testbed supported by the National Sci-\nence Foundation. Thanks as well to the anony-\nmous reviewers for their helpful comments.\n768\nReferences\nAntoine Bosselut, Corin Ennis, Omer Levy, Ari Holtz-\nman, Dieter Fox, and Yejin Choi. 2018. Simulating\nAction Dynamics with Neural Process Networks.\nIn Proceedings of the International Conference on\nLearning Representations (ICLR).\nElizabeth Clark, Yangfeng Ji, and Noah A. Smith.\n2018. Neural Text Generation in Stories Using En-\ntity Representations as Context. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(ACL): Human Language Technologies.\nBhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau\nYih, and Peter Clark. 2018. Tracking State Changes\nin Procedural Text: a Challenge Dataset and Models\nfor Process Paragraph Comprehension. In Proceed-\nings of the Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nRajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan,\nAdam Trischler, and Andrew McCallum. 2019.\nBuilding Dynamic Knowledge Graphs from Text us-\ning Machine Reading Comprehension. In Proceed-\nings of the International Conference on Learning\nRepresentations (ICLR).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding.\nAditya Gupta and Greg Durrett. 2019. Tracking Dis-\ncrete and Continuous Entity State for Process Un-\nderstanding. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (NAACL) Workshop on\nStructure Predictions for NLP.\nMikael Henaff, Jason Weston, Arthur Szlam, Antoine\nBordes, and Yann LeCun. 2017. Tracking the World\nState with Recurrent Entity Networks. In Proceed-\nings of the International Conference on Learning\nRepresentations (ICLR).\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the Conference\nof the North American Chapter of the Association\nfor Computational Linguistics (NAACL).\nYangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin\nChoi, and Noah A. Smith. 2017. Dynamic Entity\nRepresentations in Neural Language Models. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2019.\nSpanBERT: Improving Pre-training by Representing\nand Predicting Spans.\nBen Kantor and Amir Globerson. 2019. Coreference\nResolution with Entity Equalization. In Proceedings\nof the Annual Meeting of the Association for Com-\nputational Linguistics (ACL).\nChlo´e Kiddon, Luke Zettlemoyer, and Yejin Choi.\n2016. Globally Coherent Text Generation with Neu-\nral Checklist Models. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\nmoyer. 2017. End-to-end Neural Coreference Res-\nolution. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018.\nHigher-Order Coreference Resolution with Coarse-\nto-Fine Inference. In Proceedings of the Conference\nof the North American Chapter of the Association\nfor Computational Linguistics (NAACL).\nShinsuke Mori, Hirokuni Maeta, Yoko Yamakata, and\nTetsuro Sasada. 2014. Flow Graph Corpus from\nRecipe Texts. In Proceedings of the Ninth Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC).\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn Proceedings of the Annual Meeting of the Associ-\nation for Computational Linguistics (ACL).\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (NAACL).\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2.amazonaws.\ncom/openai-assets/research-covers/\nlanguageunsupervised/\nlanguageunderstandingpaper.pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. URL\nhttps://d4mucfpksywv.cloudfront.\nnet/better-language-models/\nlanguage-models.pdf.\nAltaf Rahman and Vincent Ng. 2009. Supervised Mod-\nels for Coreference Resolution. In Proceedings of\nthe Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nMinjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh\nHajishirzi. 2017. Query-Reduction Networks for\nQuestion Answering. In Proceedings of the Inter-\nnational Conference on Learning Representations\n(ICLR).\n769\nMukund Sundararajan, Ankur Taly, and Qiqi Yan.\n2017. Axiomatic Attribution for Deep Networks. In\nProceedings of the International Conference on Ma-\nchine Learning (ICML), pages 3319–3328.\nNiket Tandon, Bhavana Dalvi, Joel Grus, Wen-tau Yih,\nAntoine Bosselut, and Peter Clark. 2018. Reasoning\nabout Actions and State Changes by Injecting Com-\nmonsense Knowledge. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT Rediscovers the Classical NLP Pipeline. In\nProceedings of the Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL).\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019b. What do you learn from\ncontext? Probing for sentence structure in contextu-\nalized word representations. In Proceedings of the\nInternational Conference on Learning Representa-\ntions (ICLR).\nSam Wiseman, Alexander M. Rush, and Stuart M.\nShieber. 2016. Learning Global Features for Coref-\nerence Resolution. In Proceedings of the Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics (NAACL).\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le.\n2019. XLNet: Generalized Autoregressive Pretrain-\ning for Language Understanding.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7406551241874695
    },
    {
      "name": "Transformer",
      "score": 0.6207835078239441
    },
    {
      "name": "Natural language processing",
      "score": 0.46149927377700806
    },
    {
      "name": "Natural language",
      "score": 0.44303038716316223
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40203121304512024
    },
    {
      "name": "Engineering",
      "score": 0.1581609845161438
    },
    {
      "name": "Electrical engineering",
      "score": 0.10812979936599731
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    }
  ],
  "cited_by": 16
}