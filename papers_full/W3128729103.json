{
  "title": "Sparsifying Transformer Models with Trainable Representation Pooling",
  "url": "https://openalex.org/W3128729103",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5089966738",
      "name": "Michał Pietruszka",
      "affiliations": [
        "Jagiellonian University"
      ]
    },
    {
      "id": "https://openalex.org/A5085568088",
      "name": "Łukasz Borchmann",
      "affiliations": [
        "Poznań University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5021496039",
      "name": "Łukasz Garncarek",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2962701888",
    "https://openalex.org/W3155147984",
    "https://openalex.org/W2972114612",
    "https://openalex.org/W3005799618",
    "https://openalex.org/W1521115915",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3116364027",
    "https://openalex.org/W1483927294",
    "https://openalex.org/W2796804544",
    "https://openalex.org/W3036728994",
    "https://openalex.org/W2962985882",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2064604033",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2742461657",
    "https://openalex.org/W3006983028",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3095151720",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2963963993",
    "https://openalex.org/W2963123301",
    "https://openalex.org/W1964548530",
    "https://openalex.org/W3034561418",
    "https://openalex.org/W2964554227",
    "https://openalex.org/W4289362008",
    "https://openalex.org/W3098136301",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W4287181136",
    "https://openalex.org/W2951603207",
    "https://openalex.org/W4214614183",
    "https://openalex.org/W2740515525",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4298776150",
    "https://openalex.org/W2579653291",
    "https://openalex.org/W2890584119",
    "https://openalex.org/W4287901267",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W3098960752",
    "https://openalex.org/W4287183331",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963545005",
    "https://openalex.org/W2889518897"
  ],
  "abstract": "We propose a novel method to sparsify attention in the Transformer model by learning to select the most-informative token representations during the training process, thus focusing on the task-specific parts of an input. A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust trainable top-k operator.Our experiments on a challenging long document summarization task show that even our simple baseline performs comparably to the current SOTA, and with trainable pooling we can retain its top quality, while being 1.8× faster during training, 4.5× faster during inference, and up to 13× more computationally efficient in the decoder.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 8616 - 8633\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nSparsifying Transformer Models with Trainable Representation Pooling\nMichał Pietruszka1, 2 Łukasz Borchmann 1, 3 Łukasz Garncarek1\n1Applica.ai 2Jagiellonian University 3Poznan University of Technology\n{michal.pietruszka, lukasz.borchmann,\nlukasz.garncarek}@applica.ai\nAbstract\nWe propose a novel method to sparsify atten-\ntion in the Transformer model by learning to\nselect the most-informative token representa-\ntions during the training process, thus focusing\non the task-speciﬁc parts of an input. A reduc-\ntion of quadratic time and memory complex-\nity to sublinear was achieved due to a robust\ntrainable top-koperator. Our experiments on a\nchallenging long document summarization task\nshow that even our simple baseline performs\ncomparably to the current SOTA, and with train-\nable pooling we can retain its top quality, while\nbeing 1.8×faster during training, 4.5×faster\nduring inference and up to 13×more computa-\ntionally efﬁcient in the decoder.1\n1 Introduction\nThe introduction of Transformer architecture led to an\nimmense improvement in the performance of Natural\nLanguage Processing systems (Vaswani et al., 2017;\nRadford et al., 2018; Devlin et al., 2019). Neverthe-\nless, the underlying attention mechanism is marked by\nthe original sin of quadratic memory complexity w.r.t.\nthe input sequence length. It results from the attention\nmatrix reﬂecting inter-connections between every two\nrepresentations in the input sequence.\nPrevious approaches either reduce the full con-\nnectivity of its elements to its non-empty subset or\napproximate the self-attention matrix (Dai et al., 2019;\nBeltagy et al., 2020; Kitaev et al., 2020; Tay et al.,\n2020; Zaheer et al., 2020a; Wang et al., 2020; Shen\net al., 2021; Choromanski et al., 2021; Roy et al., 2021).\nIn particular, in these models, each word at every layer\nattends to at least one other word.\nIn contrast, we disregard attention for a given rep-\nresentation completely in the case of non-informative\nones (Figure 1 and 2).\nIn particular, we optimize the attention complexity by\nlearning to select encoded representations for the given\ntask and promoting only the chosen ones to the next\nlayer of the model. This mechanism will be referred to\nas representation pooling. Consequently, a signiﬁcantly\n1Code publicly available at https://github.com/\napplicaai/pyramidions along with trained models.\n(A) Vanilla Transformer\n(B) Blockwise Encoder\n(D) Pyramidion\nTarget length\nInput lengthPooled length\nBlock size\nPoolPool\nPool\n(C) Blockwise Encoder with Representation Pooling\nFigure 1: An illustration of sparse attention matrices as-\nsuming a three-layer encoder and decoder (separated by\nthe dashed line). The blue color reﬂects the memory con-\nsumption of self-attention (encoder) and cross-attention\n(decoder). (A) The complete input consumed at once.\n(B) Memory reduced with blockwise attention and (C)\npooling applied after the encoder. (D) Gradual reduction\nof memory by pooling after every layer.\nV anilla\t Blockwise Pooling\t \nFigure 2: Toy illustration of inter-connections consti-\ntuting the attention matrices in various approaches to\nattention. White dots denote disregarded representa-\ntions that are not attended to and removed from further\nprocessing as they obtained low scores.\n8616\nlower memory consumption and an improved process-\ning time are achieved. As the selection operation has\nto be trainable, we provide a suitable high-performance\ncontinuous relaxation of top-k, robust for every kvalue\nand input sequence length.\nWe demonstrate this idea’s applicability by perform-\ning on par to state-of-the-art on the challenging prob-\nlem of long document summarization. Simultaneously,\nthe proposed end-to-end model is a signiﬁcant theoreti-\ncal improvement over the previous systems, which are\nbased on independently trained extractive and abstrac-\ntive models.\nContribution. The speciﬁc contributions of this paper\nare the following: (1) We propose a method to sparsify\nTransformer architecture in a novel, previously unrecog-\nnized way, achieving sublinear time and memory com-\nplexity. Our model learns to select the subset of best\nrepresentations depending on the advantage they give\non a downstream task. (2) Additionally, we demonstrate\nan improvement of the decoder’s cross-attention com-\nplexity. It is beneﬁcial for both train/inference time and\nmemory consumption. (3) We demonstrate an elegant\nway to train extractive-abstractive models in an end-\nto-end manner with only a cross-entropy loss function.\n(4) We present a Successive Halving Top-koperator that\noutperforms previous approaches in terms of approxima-\ntion quality and speed. We provide a detailed analysis\nof its differential properties and prove that it is trainable\nin an end-to-end manner, making it applicable within\nour neural networks. (5) We achieve state-of-the-art\nperformance level in long document’s summarization\nand show that previous models can be outperformed by\na straightforward baseline.\n2 Related Works\nWord-vector elimination. It has been previously\nshown that the progressive elimination of word vec-\ntors occurring layer after layer can improve inference\ntime of transformer-based language models used in a\ntext classiﬁcation scenario (Goyal et al., 2020). We\nextend this notion to tasks demanding text generation\nin a way that, contrary to previous work, is trainable\nand optimized concerning a downstream task. A simi-\nlar approach has been taken in the Funnel Transformer\nproposed concurrently to our work (Dai et al., 2020).\nWe directly compare to both methods’ adaptations (see\nSection 5), and consider our work to surpass it in two\naspects: 1) results were improved due to a better pooling\nmechanism than mean/max; 2) training was accelerated,\nwhich we attribute to the signiﬁcant reduction of the\ndecoder’s complexity.\nSparse attention. Several authors proposed to limit\nattention connectivity, e.g., by dividing input into\nsmaller ’blocks’ (Child et al., 2019; Beltagy et al., 2020;\nRae and Razavi, 2020). Blockwise attention is an op-\ntional element of our architectures, used in addition to\ntrainable pooling.\nSummarization. In terms of the type of summariza-\ntion task we target, our representation pooling mech-\nanism can be considered an end-to-end extractive-\nabstractive model. This is a conceptual breakthrough\ncompared to recently proposed two-stage hybrids that\nextract and paraphrase in two independent steps, using\nseparately trained modules (Pilault et al., 2020; Hsu\net al., 2018; Gehrmann et al., 2018; Chen and Bansal,\n2018).\n3 Novel Approach of Representation\nPooling\nIt is suspected that when humans engage in information\nsearch, they use various cognitive processes depend-\ning on the relevance level of constituent text fragments\n(Gwizdka et al., 2017).\nThe method we propose is inspired by this search\nfor relevant fragments, which is an important aspect of\nhuman cognition when engaged in reading to do actions\n(Mosenthal, 1996; Mosenthal and Kirsch, 1992). We\nintend to mimic relevance judgments and hypothesize\nthat it is possible to answer problems involving natural\nlanguage with only selected passages of the input text.\nThese passages may be of substantially shorter length\nthan the original text. One may compare this to a per-\nson reading the paper and highlighting in such a way\nthat it is possible to provide a summary using only the\nhighlighted parts.\nThe end-to-end mechanism we introduce performs\nsuch highlighting by scoring the representations and\npasses only the selected ones to the next layer of the\nneural network (Figure 3). The role of the selection is\nto reduce data resolution in a roughly similar way to\nhow pooling works in CNNs, where the feature map is\ndownsampled and only the most informative activations\nare retained. When pooling in a trainable manner at\nthe bottleneck of the encoder-decoder, it impacts the\nencoding process because the additional, orthogonal,\ninformational bottleneck forces the model to compress\nmore context into one representation vector of constant-\nlength, leveraging the already provided capacity.\n3.1 Architecture Outline\nLet ndenote the number of input tokens that are pro-\njected onto ddimensions, resulting in a matrix of em-\nbedding representations E ∈Rn×d. We want to assign\nscores vi to embedding vectors Ei, in such a way that\nvi measures the usefulness of Ei for further layers and\nthe training objective.\nTypically, this can be achieved by deﬁning a scoring\nfunction S: Rd →R (which we allow to depend on\nadditional parameters, thus making it trainable) that\nassigns a usefulness score to every embedding vector,\nand putting\nvi = S(Ei). (1)\nNext, we use our soft top- k operator Γ: Rn×d ×\nRn →Rk×d to reduce the number of embeddings from\n8617\nencoding\nTok 1 Tok N Tok N+1 Tok M Tok M+1 Tok L... ... ...\n... ... ...E1 EMEN+1 EM+1 EL\n... ... ...T1 TMTN+1 TM+1 TL\nEN\nTN\nBlock 1 Block Z...\nrepresentation pooling\nT2 TM\nEncoding can be performed\nas in standard Transformer\narchitecture on the full-\nlength input. It is, however,\npossible to process the text\nin blocks of fixed length.\nThe encoder layer is\nfollowed by representation\npooling. Each representation\nis scored, and then only\nthose with the highest\nscores are passed to the\ndecoder. \nTok 2\nE2\nT2\nTok N+2\nEN+2\nTN+2\nTN+1 TN+2\nTok M+2\nEM+2\nTM+2\nTL\nFigure 3: Transpooler architecture with pooling after one encoder layer. Each representation is scored, and then\nonly those with the highest scores are passed to the decoder. Encoding can be performed on the full length input or\nin blocks of ﬁxed length.\nnto k, based on their usefulness scores. The kvectors\nproduced by Γ form the input for the next network layer.\nThe path of residual connections starts on a reduced\nnumber of tokens.\nFlavors. We consider two architectures in this work:\nwith single or multiple pooling layers (Figure 1). Specif-\nically, the latter is a generalization of the former to any\ngiven number of pooling layers. We use the term Trans-\npooler when a single pooling layer is placed after the\nencoder. This setup directly limits the amount of in-\nformation passed to the decoder through the network’s\nbottleneck.\nHowever, pooling can be applied between any subse-\nquent layers, such that multiple operations of this type\nwill be used in the network and gradually introduce the\nbottleneck along the encoding process. As a result, the\nsame model bottleneck size can be achieved as when us-\ning Transpooler. Moreover, the decision to pool earlier\nhas the advantage of attaining more substantial memory\ncomplexity reduction. This model will be referred to as\nthe Pyramidion.\nBlockwise attention. When propagating through lay-\ners, we use blockwise attention and split input into non-\noverlapping chunks in such a way that the full quadratic\nattention is computed for each chunk. The score is then\ndetermined for each representation vector, and after se-\nlecting with the top-koperator, chosen representations\nare passed to the next layer. We assure our top- k op-\nerator selects representations without permuting their\norder, keeping them in line with their original position.\nScoring functions. Multiple scoring methods can be\nproposed. The most straightforward is to use a linear\nscoring function as used in conventional token classi-\nﬁcation, S(e) = eTw+ b, where w ∈Rd and b ∈R\nare trainable parameters. We found it to work best with\nour pooling method. In the Appendix A we perform\nablations on different scoring functions.\nTable 1: Time complexity of attention in the Trans-\nformer models. Improvements over the vanilla Trans-\nformer are in bold, whereas an underline indicates this\npaper’s contributions. l– number of layers, n– input\nlength, d – hidden state;s size, t – target length, h –\nnumber of hashes LSH, r – rank of the factorization\nmatrix, k– length of selected token’s representation,c–\nan effective number of layers that is smaller than l.\nModel Self-attention Cross-attention\nVanilla l ×n ×n ×d l ×t ×n ×d\nSparse l ×m ×n ×d l ×t ×n ×d\nLinformer l ×n ×r ×d —\nLSH l ×mh ×n ×d —\nEfﬁcient l ×n ×d ×d —\nPoWER c ×n ×n ×d —\nTranspooler l ×m ×n ×d l ×t ×k ×d\nPyramidion c ×m ×n ×d l ×t ×k ×d\n3.2 Complexity Analysis\nTable 1 presents the complexity of attention in our\nmodels, and compares it to different architectures. The\nvanilla encoder depends on the number of layers l,\nthe number of tokens in the input n and the number\nof tokens each attends to n. Likewise, the decoder’s\ncross-attention depends on l, nand the target length t.\nThe mdenotes the effective number of tokens one\ncan attend to, resulting from the attention’s block size,\nallowed window size or the clustering of key-values.\nThe number of parallel LSH hashes is denoted by h.\nThe rank of the factorization matrix is r, which can be\na constant that is independent of n.\nSimilarly, the number of best task-speciﬁc representa-\ntions k, selected after encoding, is independent ofn. cis\nan effective number of layers in a hierarchically decreas-\ning encoder of the Pyramidion. The Pyramidion’sccan\nbe as low as . Blockwise sparse attention improved\n8618\nthe vanilla Transformer’s complexity by limiting the\nnumber of tokens each attends to from n(input length)\nto m(block size) as seen in Table 1. As we keep the\nencoding of blockwise attention, the mimprovement\nalso applies to our self-attention.\nFor the Pyramidion model, we narrow down the\nsize of the representation on the output of each cho-\nsen layer, leading to the exponential reduction of mem-\nory consumption as the encoding proceeds. For ex-\nample, when pooling after every layer is considered,\nthe total memory complexity across llayers would be∑p\ni=0 2−imnd= (2 −k/n)mndwhere pdenotes the\nnumber of passes p= log2(n/k), assuming k≤nand\nn,k ∈{2i |i ∈Z+}. Hence, the effective complex-\nity of all layers is lower than mnd, which means it is\nlower than times the complexity of the full-size ﬁrst\nlayer.\nFor the decoder cross-attention, the number of input\nrepresentations that ttarget tokens can attend to is lim-\nited by k, thus decreasing the memory complexity of\ncross attention from O(tn) to O(tk). Optimization over\nquadratic sentence-length complexity is even more pow-\nerful and needed on the decoder side, asO(tn) complex-\nity hurts performance of real-world applications based\non auto-regressive decoding.\nThe blockwise attention itself reduces encoder com-\nplexity proportionally to the number of chunks. We\nfurther reduce the decoder layer’s complexity in Trans-\npooler models by a factor of n/k, thanks to represen-\ntation pooling. The Pyramidion we propose offers an\nadditional improvement on the encoder side, where time\nand memory consumption are reduced in each of the\nconsecutive layers compared to the Transformer fea-\nturing blockwise attention. In other words, when b\ndenotes the number of blocks, lstands for the number\nof layers, and the sequence length is halved in each\nlayer, we reduce memory from b+ b+ ...+ b= lbto\nb+b/2+ b/4+ ...+b/(2l) ≤2b. Because the beneﬁcial\nimpact of pooling accumulates, we are able to improve\ncomplexity from one that is linearly dependent on lto\none that is constant, independent of l. In the further\nDeepPyramidion’s experiments, we will proceed with a\nhigher reduction factor, where the length of a sequence\nis cut in four.\nAs a result, the Pyramidion achieves an effective self-\nattention time and space complexity linear of n and\nlogarithmic of l. For comparison, other sparse models\nsuch as, e.g., Linformer depend linearly on nand lin-\nearly on l. The analysis of Figure 4 found evidence that\nour method scales well with an increasing number of\nlayers. In the evaluation (see Section 5), we demonstrate\nthat our model achieves a 2.5×computation reduction\nin the encoder’s self-attention and a 16×reduction in\nthe decoder’s cross-attention comparing to blockwise\nbaseline, while both models are close to SOTA results\non the task of long-document summarization. All things\nconsidered, we introduce Pyramidion with sublinear\ncomplexity that achieves remarkable results.\n5 10 15 20 25\nLayers (network depth)\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0Time (seconds)\n(Out of memory)\nType\nPooling\nBlock\nVanilla\nFigure 4: Training time for different model sizes of\nVanilla Transformer, Blockwise, and Pyramidion8k→\n512 with the input sequence length of8192 tokens. Pool-\ning is faster for models with 4 or more layers, achieving\nup to 3.8xspeedup for 16-layer Transformer. Scores of\na 2-layer version of these models do not differ signiﬁ-\ncantly.\nThe advantage of our approach is that it complements\nall other proposed sparsiﬁcation techniques, thus paving\na new interesting avenue of potential research. It can\nbe effortlessly applied in-between layers and simulta-\nneously with other improvements since representation\npooling addresses a different aspect of the attention’s\ncomplexity problem.\n4 Suitable Top- k Operator\nThe choice of the selection operator is challenging, as\nit has to be trainable to instantiate a pooler. In case of\nthe hard top-koperator, back-propagation through the\nscores is impossible and prevents training the scoring\nfunction. It could be seen as an extreme case of the\nvanishing gradient problem. In this section we intro-\nduce a mechanism not prone to this issue, while the\nAppendix B is dedicated to a theoretical analysis of its\ndifferential properties, from a geometrical point of view.\nThe crux of our approach is the Successive Halving\nTop-kselection mechanism that ﬁndskconvex combina-\ntions of vector representations Ei, dominated by those\nachieving the highest scores vi (pseudocode available\nin the Appendix B.1).2 The general idea is to perform a\ntournament soft selection, where candidate vectors are\ncompared in pairs (i,j), until only kremained. After\neach tournament’s round newE′and v′are computed as\nconvex combinations of these pairs with weights based\non their respective scores. Each new vector is calculated\nas:\nE′\ni = wiEi + wjEj,\nwhere the wi,wj are the result of a peaked softmax over\nthe scores vi,vj. Analogously, we usev′\ni = wivi+wjvj\nas the new-round’s scores.\n2Preliminary work regarding this method was previously\npresented in the form of a Student Abstract, see Pietruszka\net al. (2020).\n8619\nTable 2: Scores, complexity and benchmark depending on maximum encoder and decoder lengths, as well as used\nsparsiﬁcation mechanism. All models features a 2-layer encoder and a 2-layer decoder, blocks of size 512. Results\non arXiv summarization dataset (Cohan et al., 2018). Arrow →denotes a pooling operation additional to the one\nbetween encoder and decoder. Note, that for the vanilla Transformer encoder lengths are equal to the decoder’s\nlength, whereas Transpoolers and Pyramidions lower the number of representations passed down to the decoder\nwithout the substantial quality decrese.\n# Architecture Lengths Time ROUGE\nEncoder Decoder Training Inference R-1 R-2\n1\nVanilla\n\n\n\n512 512 0.13 4.23 28.1 8.3\n2 2k 2k 0.60 5.77 38.2 14.0\n3 8k 8k 4.46 13.27 41.8 16.1\n4 Blockwise\n{ 2k 2k 0.31 5.28 38.6 14.1\n5 8k 8k 0.85 11.49 41.9 16.7\n6\nTranspooler\n\n\n\n2k 512 0.54 4.24 39.1 14.6\n7 8k 512 1.44 4.28 41.8 16.4\n8 8k 2k 1.26 5.51 42.7 16.7\n9\nLSH (Kitaev et al., 2020)\n\n\n\n512 512 0.19 4.27 28.5 7.5\n10 2k 2k 0.56 5.92 33.6 10.5\n11 8k 8k 1.69 13.41 35.7 11.2\n12\nEfﬁcient (Shen et al., 2021)\n\n\n\n512 512 0.12 4.20 28.4 7.8\n13 2k 2k 0.29 5.91 34.1 10.4\n14 8k 8k 0.82 13.75 35.0 10.8\n15\nPoWER (Goyal et al., 2020)\n\n\n\n2k →1k 512 1.04 4.28 35.3 12.7\n16 8k →2k 512 1.87 5.33 36.9 14.1\n17 8k →4k 2k 2.06 6.92 42.0 16.5\n18\nFunnel (Dai et al., 2020)\n\n\n\n2k →512 2k 0.61 4.01 38.6 14.3\n19 8k →512 8k 1.78 4.03 41.8 16.5\n20 8k →2k 8k 1.53 5.25 42.0 16.4\nWeights are calculated using aPeakedSoftmax func-\ntion (Goyal et al., 2017), increasing the pairwise differ-\nence in scores between vi and vj. One round halves the\nnumber of elements in E and v. We perform it itera-\ntively unless the size of E and v matches the chosen\nvalue of k.\nTo improve convergence towards selecting the real\ntop-k, it is desired to permute v and E ﬁrst. In our\nalgorithm, we sort the vectors Ei in descending order of\ntheir scores vi and then put them into the tournament in\npairs of the form (i,n + 1 −i). This method of pairing\nguarantees that the weightswidepend monotonically on\nthe scores vi, which is the main motivation for using it.\nExtended benchmarks for time and accuracy are covered\nin details in Appendix B.5.\n5 Evaluation\nThe main focus of the experiments was to understand\nhow to employ the Successive Halving Top- k opera-\ntor within neural networks to build models that have\nbetter training and inference time and are expressive\nenough to achieve results comparable to state-of-the-art\nmodels. The ﬁrst experiment was speciﬁcally designed\nto compare to other sparse Transformers and Vanilla\nbaselines.\nChoice of tasks. We demonstrate the beneﬁt of pool-\ning on the arXiv and PubMed summarization datasets\n(Cohan et al., 2018) available under Apache License 2.0\nlicense. Both tasks demand text generation and have the\nhighest average input sequence length (6k and 3k words\non average for arXiv and PubMed respectively). Assum-\ning an embedding of dimensionality 768, it is important\nto note that for inputs shorter than approx. 4ktokens,\nmore multiplications happen in the Transformer’s FFN\nlayers and projection layers than in the attention layers.\nHence, the validation of the sparsiﬁcation mechanism\nshould be proved by showing that it works for longer\ninputs.\nTime benchmarks. The average time of processing\na batch of documents is reported to evaluate the com-\nputational improvements experimentally. Decoding ex-\nperiments were synthetic with a forced ﬁxed length of\n512 output tokens to discount for the lower processing\ntime of models predicting an earlier sequence end. We\nrecorded time in seconds on batches of size 64 and 8 for\ntraining and generation, respectively. Details regarding\nthe hyperparameters and test environment are reported\nin Appendix C.\n8620\nAblations on input and decoder lengths. Table 2\npresents evaluation metrics and time benchmarks de-\npending on encoder and decoder lengths, as well as\nused sparsiﬁcation mechanisms. At this stage, we use\nshallow 4-layer models to perform ablation studies and\nestimate each approach’s strengths and weaknesses. We\nobserve that all sparse models deliver on the promise of\naccelerating training time over Vanilla Transformers for\nlonger sequences in this setup. Methods requiring the\nelimination of word vectors scale well with the sequence\nlength but incur additional pooling costs, which may be\nnotable for shorter sequences. Nevertheless, inference\ntime was signiﬁcantly reduced only when methods elim-\ninating word vectors were employed. The introduction\nof blockwise attention and pooling does not decrease\nscores while lowering the computational cost. The de-\ntailed training procedure for all models is provided in\nAppendix C.\nScaling deeper. In preliminary experiments it was es-\ntimated that the fastest-to-train model that performs\ncomparably to the Vanilla Transformer is the Blockwise\nTransformer. Here, we scale it to 6-layers in each en-\ncoder and decoder and provide an interesting baseline\nfor our model, since Transpooler’s backbone is block-\nwise attention. We undertook the empirical analysis of\nscaling Transpooler to many layers in Appendix C.2\nand found that in order to balance performance and\nspeed, it is crucial to delay the ﬁrst pooling and not to\nperform it directly on the ﬁrst layer’s output. It was\nalso revealed that appending more layers at the end\nof the encoder (after pooling) results in a negligible\nincrease in time while considerably improving scores.\nBoth changes to the block size and reduction of the\nbottleneck harmed the performance. Thus, the data sup-\nports the premise that the 6-layers encoder should con-\nsume 8ktokens on the input and output representations\nof lengths 8k,8k,2k,512,512,512 after each succes-\nsive layer. We refer to this model as DeepPyramidion\n(note that pooling happens twice in the encoder). The\ndecoder also has six layers, making our model directly\ncomparable to the deeper Blockwise Transformer. We\nconfront DeepPyramidion with the Blockwise baseline\nby training models from scratch on arXiv and PubMed\ndatasets separately and report results in comparison to\nthe state-of-the-art summarization models (Table 3).\nResults. The evaluation of the data presented in Ta-\nble 3 leads to the unexpected conclusion that our Block-\nwise Transformer baseline, despite its simplicity, is suf-\nﬁcient to outperform deeper, denser, and additionally\npretrained models that were recently reported as state-\nof-the-art. We demonstrate that DeepPyramidion retains\nor improves the performance of the competitive base-\nline we produced. The training time speedup by 1.8×\nsupports the notion that our model scales better to long\nsequences, assuming deeper models. This result stands\nin line with evidence in Figure 4. While our baseline\nBlockwise model reduces the computational demand\nof self-attention in encoder by a factor of 16×when\ncomparing to Vanilla Transformer, it does not improve\nthe decoder’s computational complexity. It is interest-\ning to highlight that DeepPyramidion further lowers the\ncost of self-attention by 2.5×and improves 16×over\nBlockwise’s cross-attention in the decoder, and leads to\noverall 13×improvement in the number of multiplica-\ntion operations in the decoder. Time benchmarks show\na 4.5×improvement in the generation times for our\nmethod, proving how vital the improvement in the de-\ncoder’s cross-attention complexity is for inference time.\nDeepPyramidion achieves a ROUGE-2 score\nindistinguishable from SOTA on arXiv and performs\ncompetitively on PubMeb. At the same time, an entire\nDeepPyramidion costs ﬁve times less than a single\nTransformer layer consuming 8k tokens. However,\nwhen comparing our results to those of older studies,\nit must be pointed out that our models were trained\nfrom scratch only on the targeted dataset, whereas\nprior works often base on already pretrained models\nsuch as BART or RoBERTa and leverage unsupervised\ntraining on additional datasets. On the contrary, a longer\ninput sequence was consumed by both Blockwise and\nDeepPyramidion, which we speculate, is the reason for\ntheir strong performance.3\nImpact of longer inputs. The results achieved in our\npaper are comparable to other, much heavier, and more\ncostly models due to two main reasons, that will be\nbrieﬂy discussed below.\nFirstly, to perform well on a long document summa-\nrization task, there is a need to strike the right balance\nnot only between the depth and width of the network but\nalso it is required for design optimization to take into\naccount the length of the input. All previous work seem\nto underperform when considering all three factors, as\nthey were designed and optimized for shorter tasks and\ngenerally have more parameters, denser computations,\nor even a hard limit on the range of positional encod-\ning. The authors were thus bounded by the maximal\nsequence length of 512 or 1024 tokens. One can argue\nthat within this preﬁx (corresponding to the ﬁrst 2 −3\npages), any data point from the arXiv/PubMed datasets\n(a scientiﬁc paper) usually provides enough information\nto write a meaningful summary, but also, important de-\ntails will be missing to some degree. Hence, increasing\nthe length of the input that can be consumed on GPUs,\nat the price of using a shallower network, with sparser\ncomputation, may be considered a better ﬁt for the task.\n3This view is supported by results of PoolingFormer that\nare concurrent to our work (Zhang et al., 2021). Despite that,\nat ﬁrst sight, the methods seem similar and the authors present\nan interesting use of pooling in the attention, we argue that the\nmentioned model suffers from several weaknesses that are not\npresent in our work. First of all, in the PoolingFormer model\nvectors are not removed from computations in further layers.\nHence logarithmic complexity of the number of layers does not\napply. PoolingFormer’s approach suffers from having three\norders of magnitude more calculations than when a global\npooling based on scores of individual tokens is considered.\n8621\nTable 3: Comparison to SOTA on long document summarization tasks. Our models have no pretraining whereas\n†were initialized from BART,‡– from RoBERTa, ∗– from PEGASUS (Zhang et al., 2021; Rohde et al., 2021;\nZaheer et al., 2020b; Gidiotis and Tsoumakas, 2020).\nArchitecture arXiv PubMed Params Time\nR-1 R-2 R-1 R-2 Train. Infer.\nPoolingFormer† 48.47 20.23 – – >406M – –\nHAT-BART† 46.74 19.19 48.25 21.35 >406M – –\nBigBird-PEGASUS‡ 46.63 19.02 46.32 20.65 568M – –\nDancer PEGASUS∗ 45.01 17.60 46.34 19.97 568M – –\nBlockwise (our baseline) 46.85 19.39 – – 124M 4.85 37.15\nDeepPyramidion (our) 47.15 19.99 47.81 21.14 124M 2.71 8.12\nSecondly, we think that pretraining in the Pyramid-\nion’s case may be disregarded due to an interesting\n“length exploiting hypothesis”. That is, while we con-\nsume longer sequences on the input, the network learns\nmore efﬁciently, as more information is available, and\nthus, the training signal is stronger. This can be con-\nvincingly portrayed in the case of embedding layers, as\nduring training they see many more words and sentences\nfrom the chosen dataset, and hence, can provide more\nmeaningful representations to the further layers.\nOne can think that making the most of already avail-\nable domain texts and consuming longer inputs is an\nadvantageous approach to masked pretraining on out-\nof-domain datasets. While the latter approach may\naid ‘general’ language understanding, it has insufﬁcient\ntransferability potential to domain-speciﬁc document\nunderstanding (e.g., scientiﬁc or medical texts).\nTo sum up, the Pyramidion has improvements that\nallow consuming longer inputs cheaply, which turns\nout to be a more cost-effective strategy compared to\nother models. This aspect is crucial for achieving strong\nresults on the presented datasets.\n6 Limitations and Social Impact\nAt this stage of understanding, we believe that sparsi-\nﬁcation based on trainable pooling is unlikely to im-\nprove processing time for short sequences speciﬁc to\nsome NLP tasks, e.g., sentence-level Neural Machine\nTranslation. In addition, the score improvement may be\nattainable for tasks characterized by at least an order of\nmagnitude shorter outputs than inputs, as it was previ-\nously shown on classiﬁcation, or, as in the case of this\nwork, on summarization.\nHowever, the extent to which it is possible to replace\nfull-attention in Transformer with the sparse attention\nwe propose is unknown. However, we argue that the\nbeneﬁts are visible starting from the inputs of length\n4k. As discussed earlier, 4kis a break-even point where\nmore calculations are needed for attention than for FFNs\nand projecting layers. As such, we recommend applying\nsparsiﬁcation methods on datasets featuring sequences\nof length over that value. While we focus on the long\nend of the possible inputs, one can continue our analysis,\nto ﬁnd improvements that work for shortest sequences,\nsuch as, e.g., concentrating on employing lighter projec-\ntion layers and FFNs or stacking more attention blocks.\nAlthough our method is a hybrid extractive-\nabstractive, it does not provide interpretable explana-\ntions to which speciﬁc representations were selected as\nthe pooling operates in the latent space. How to match\nthe selected vectors to the vocabulary tokens remains an\nopen question. Moreover, framing the trainable pooling\nfor language modeling remains a challenge to address\nin future works, especially as in this task the Markov\nassumption may serve as a basis for competitive pooling\nheuristics.\nWe did not consider Relative Positional Encoding in\nour work as pooling mechanism is not trivially appli-\ncable with it and some generalization of our method\nmay be needed. In that case, as it demands more exper-\niments and proofs, we will leave the generalization of\nthe pooling method for future work.\nRegarding the social impact and environmental sus-\ntainability, we actively considered the Earth’s well-\nbeing by contributing a technique for reducing the com-\nputational demand of recent Deep Learning models.\nOur near-state-of-the-art DeepPyramidion model costs\nus 3 days of training on 8 NVIDIA A100 GPUs. Shal-\nlow models featuring trainable pooling were ﬁnished\nin about 2 days each, given the same hardware. Block-\nwise baselines cost us about 3.5xthe price of respective\npooling methods. The most prolonged training of the\n8kVanilla Transformer lasted for about 2 weeks. The\ntotal cost of training the models covered in this paper\nis about 2 months on the mentioned hardware, plus an\nadditional month for models and ablations described in\nthe appendices.\nWe roughly estimate that it is between half and one-\nfourth of the total computation spent, including false\nruns, unpublished work, and initial experiments. The\ndataset preparation took less than 10 hours on 1 CPU.\n7 Summary\nWe propose representation pooling as a method to re-\nduce the complexity of Transformer encoder-decoder\nmodels. Speciﬁcally, we optimize self-attention com-\n8622\nplexity and address the decoder’s cross-attention com-\nplexity optimization, which has so far not been widely\nacknowledged by the research community. Moreover,\nthe DeepPyramidion we introduced establishes results\ncomparable to state-of-the-art, outperforming not only\nother systems relying on progressive word-vector elim-\nination but also deeper, denser, and additionally pre-\ntrained models.\nWe tackle the problem by introducing a novel method\nof applying successive halving to a model’s input in a\ntournament style. It is a theoretical improvement over\nexisting approaches in terms of both computational com-\nplexity and approximation quality. Trainable Top-k se-\nlection allows to train scorer for a task and outperforms\nother pooling methods.\nFrom the summarization task’s point of view, the\nproposed end-to-end model is a signiﬁcant theoretical\nimprovement over the previous systems, where the ex-\ntractive model was trained independently of the abstrac-\ntive one. In contrast, our mechanism does not require\nthe introduction of an additional training objective or\ntraining stage.\nOur approach can be easily applied to other problems\nfrom Natural Language Processing and Computer Vi-\nsion. E.g., in a recent work later than ours, Multiscale\nVision Transformers were proposed. These, similarly to\nour Pyramidion model, introduce the bottleneck gradu-\nally along the encoding process of videos and images,\nleading to better results, and complexity (Fan et al.,\n2021). As it comes to Natural Language Processing,\npossible applications include Key Information Extrac-\ntion, Machine Reading Comprehension, and Question\nAnswering in scenarios where encoder-decoder models\nstruggle or would struggle with input sequence length\n(see, e.g., Choi et al. (2017); Townsend et al. (2021);\nKoˇciský et al. (2018)). We are looking forward to seeing\nthese opportunities exploited.\nAcknowledgments\nFor easy reproduction of the results, we release our\nutilities, code and pretrained models on the MIT li-\ncense for all researchers not afﬁliated or wor-\nking for Russian state-controlled institutions and\npublic companies. The reason to ostracize scientists\nunder those afﬁliations is the violent invasion of\ntheir armed forces on Ukraine, recklessly intended to\ninﬂict pain, threaten world peace and civilians life with\nnonhuman aggression against a sovereign nation.\nThe authors would like to thank Zoﬁa Prochoroff and\nPaweł Morawiecki for the helpful discussions on the\ndraft of the paper. Moreover, we thank the reviewers\nfor their comments and suggestions that helped improve\nthe paper.\nThe Smart Growth Operational Programme supported\nthis research under project no. POIR.01.01.01-00-0877\n/19-00 (A universal platform for robotic automation of\nprocesses requiring text comprehension, with a unique\nlevel of implementation and service automation).\nReferences\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The Long-Document Trans-\nformer. ArXiv, abs/2004.05150.\nGuillaume Calmettes, Gordon B. Drummond, and\nSarah L. V owler. 2012. Making do with what we\nhave: use your bootstraps. The Journal of Physiol-\nogy, 590(15):3403–3406.\nYen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-\ntive summarization with reinforce-selected sentence\nrewriting. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 675–686, Melbourne,\nAustralia. Association for Computational Linguistics.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers.\nEunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia\nPolosukhin, Alexandre Lacoste, and Jonathan Berant.\n2017. Coarse-to-ﬁne question answering for long\ndocuments. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 209–220, Vancouver,\nCanada. Association for Computational Linguistics.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamás\nSarlós, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J. Colwell, and Adrian Weller. 2021.\nRethinking attention with performers. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Nazli\nGoharian. 2018. A discourse-aware attention model\nfor abstractive summarization of long documents. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 615–621, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc Le.\n2020. Funnel-transformer: Filtering out sequential\nredundancy for efﬁcient language processing. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978–2988, Florence, Italy. Asso-\nciation for Computational Linguistics.\n8623\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nHaoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao\nLi, Zhicheng Yan, Jitendra Malik, and Christoph Fe-\nichtenhofer. 2021. Multiscale vision transformers.\nArXiv preprint, abs/2104.11227.\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summarization.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4098–4109, Brussels, Belgium. Association for Com-\nputational Linguistics.\nAlexios Gidiotis and Grigorios Tsoumakas. 2020. A\ndivide-and-conquer approach to the summarization\nof long documents.\nKartik Goyal, Chris Dyer, and Taylor Berg-Kirkpatrick.\n2017. Differentiable scheduled sampling for credit\nassignment. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 366–371, Vancouver,\nCanada. Association for Computational Linguistics.\nKartik Goyal, Graham Neubig, Chris Dyer, and Tay-\nlor Berg-Kirkpatrick. 2018. A continuous relaxation\nof beam search for end-to-end training of neural se-\nquence models. In Proceedings of the Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artiﬁcial In-\ntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artiﬁcial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 3045–3052. AAAI Press.\nSaurabh Goyal, Anamitra Roy Choudhury, Saurabh\nRaje, Venkatesan T. Chakaravarthy, Yogish Sabhar-\nwal, and Ashish Verma. 2020. Power-bert: Accel-\nerating BERT inference via progressive word-vector\nelimination. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research , pages 3690–3699.\nPMLR.\nJacek Gwizdka, Rahilsadat Hosseini, Michael Cole, and\nShouyi Wang. 2017. Temporal dynamics of eye-\ntracking and eeg during reading and relevance de-\ncisions. Journal of the Association for Information\nScience and Technology, 68(10):2299–2312.\nWan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui\nMin, Jing Tang, and Min Sun. 2018. A uniﬁed model\nfor extractive and abstractive summarization using\ninconsistency loss. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 132–141,\nMelbourne, Australia. Association for Computational\nLinguistics.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nTomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris\nDyer, Karl Moritz Hermann, Gábor Melis, and Ed-\nward Grefenstette. 2018. The NarrativeQA reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317–328.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. RoBERTa:\nA Robustly Optimized BERT Pretraining Approach.\nPeter B Mosenthal. 1996. Understanding the strate-\ngies of document literacy and their conditions of use.\nJournal of Educational psychology, 88(2):314.\nPeter B. Mosenthal and Irwin S. Kirsch. 1992. Types of\ndocument knowledge: From structures to strategies.\nJournal of Reading, 36(1):64–67.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 48–53, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada, pages\n8024–8035.\nMichał Pietruszka, Łukasz Borchmann, and Filip\nGrali`nski. 2020. Successive Halving Top-k Operator.\nJonathan Pilault, Raymond Li, Sandeep Subramanian,\nand Chris Pal. 2020. On extractive and abstractive\nneural document summarization with transformer lan-\nguage models. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 9308–9319, Online. Associ-\nation for Computational Linguistics.\nTobias Plötz and Stefan Roth. 2018. Neural near-\nest neighbors networks. In Advances in Neural\nInformation Processing Systems 31: Annual Con-\nference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Montréal,\nCanada, pages 1095–1106.\n8624\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nJack Rae and Ali Razavi. 2020. Do transformers need\ndeep long-range memory? In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 7524–7529, Online. Association\nfor Computational Linguistics.\nTobias Rohde, Xiaoxia Wu, and Yinhan Liu. 2021. Hi-\nerarchical learning for generation with long source\nsequences.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efﬁcient content-based sparse\nattention with routing transformers. Transactions of\nthe Association for Computational Linguistics, 9:53–\n68.\nZhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi,\nand Hongsheng Li. 2021. Efﬁcient attention: Atten-\ntion with linear complexities. In WACV.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and\nDa-Cheng Juan. 2020. Sparse sinkhorn attention.\nIn Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 9438–9447. PMLR.\nBenjamin Townsend, Eamon Ito-Fisher, Lily Zhang,\nand Madison May. 2021. Doc2dict: Information\nextraction as text generation.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity.\nSang Michael Xie and Stefano Ermon. 2019. Repa-\nrameterizable subset sampling via continuous relax-\nations. In Proceedings of the Twenty-Eighth Interna-\ntional Joint Conference on Artiﬁcial Intelligence, IJ-\nCAI 2019, Macao, China, August 10-16, 2019, pages\n3919–3925. ijcai.org.\nYujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo\nZhao, Hongyuan Zha, Wei Wei, and Tomas Pﬁster.\n2020. Differentiable top-k operator with optimal\ntransport.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntañón, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020a. Big bird: Trans-\nformers for longer sequences. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntañón, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020b. Big bird: Trans-\nformers for longer sequences. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nHang Zhang, Yeyun Gong, Yelong Shen, Weisheng\nLi, Jiancheng Lv, Nan Duan, and Weizhu Chen.\n2021. Poolingformer: Long document modeling with\npooling attention. In Proceedings of the 38th Inter-\nnational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of\nProceedings of Machine Learning Research, pages\n12437–12446. PMLR.\n8625\nA Scorers’ Ablations\nLinear. Multiple scoring methods can be proposed.\nThe most straightforward is to use a linear scoring func-\ntion used in conventional token classiﬁcation, S(e) =\neTw+ b, where w∈Rd and b∈R are trainable param-\neters.\nNonlinear. A quite natural next step is to include non-\nlinearity. We follow the speciﬁcation of RoBERTa’s\nclassiﬁcation head (Liu et al., 2019), deﬁned as S(e) =\ntanh(eTw1 + b1) ·w2 + b2, where w1,w2 ∈Rd and\nb1,b2 ∈R.\nPoWER-like. A column-wise sum over attention ma-\ntrices A = Attn(E) from the preceding layer can be\nused as the usefulness score, that is vi = ∑n\nj=1 Ai,j as\nproposed by Goyal et al. (2020) for hard top-kselection.\nEmbedding-based. Scoring can be performed based\non a speciﬁed dimension in encoded space, i.e. by using\na coordinate projection S(e) = ej, where jis a ﬁxed in-\ndex. This is a special case of the linear scoring function\nwith ﬁxed non-trainable weights.\nRandom. The baseline sampling scores randomly\nfrom a uniform distribution.\nIndex-based. A modulo-distributed score, that is non-\nzero for every k-th token, such as:\nvi =\n{\n1 when i≡0 (mod k)\n0 otherwise\nMean/Max Pooling. Pooling baselines characterized\nby aggregating scores within each window either by\ntaking the mean value or the max value. In this case 4\nnearest tokens were aggregated, and the window also\ntraverse with the stride of 4.\nBoth the PoWER-like and embedding-based scoring\nfunctions utilize mechanisms already provided in the\nTransformer model and are easy to use. Similarly to the\nindex-based baseline method and the random one, they\ndo not introduce any additional parameters to the model.\nThe last two do not rely on a pooling operation at all.\nPoWER was proposed assuming that the model’s at-\ntention already contains useful information about the\nmost critical parts of the input sequence (Goyal et al.,\n2018). In principle, it is possible to use its scorer with\nsoft top-k, but we intended to follow the original for-\nmulation where scoring was followed by the hard top-k\noperation.\nA.1 Results\nResults obtained with the same, 4-layer Transpooler but\ndifferent scoring functions are presented in Table 4.\nAll of the methods outperform the random baseline.\nAcross them, the linear scorer achieved the highest eval-\nuation metric. The index-based method we propose\nperforms well, even though it does not require training.\nIn particular, models employing such ﬁxed selec-\ntion achieve better results than those equipped with a\nPoWER-like scorer. This can be attributed to the rela-\ntively low reduction of length required in the presented\nexperiment: a model with index-based selection pre-\nsumably learned to compress groups of the four nearest\ntoken neighbors.\nNevertheless, only nonlinear baseline approaches\nturned out not to be signiﬁcantly worse than the linear\nscorer. Assuming preference towards a simpler method,\nthe rest of the experiments were conducted using only\nthe linear scorer.\nTable 4: Ablation study of different scorers, using the\nsame 4-layer Transpooler model with reduction from\n2048 to 512 representations. The difference of 0.4 is\nsigniﬁcant. (Calmettes et al., 2012).\nScorer ROUGE-1 ROUGE-2\nLinear 39.1 14.6\nNonlinear 38.9 14.6\nRandom 32.3 11 .4\nIndex-based 38.2 13 .9\nEmbedding-based 37.6 14 .0\nPoWER-like 36.9 13 .6\nMean Pooling 38.1 13 .9\nMax Pooling 38.4 14 .2\nB Successive Halving Top- k Algorithm\nGoyal et al. (2018) provides the most similar relaxation\nfor beam search, where they continuously relaxed the\ntop-k-argmax procedure by performing softmaxes iter-\natively k times and masking the previously extracted\nvalues. Each beam can contribute to the newly selected\nbeam in every iteration, based on its distance to the max\nvalue. By replacing one-hot coded vectors with their\nexpectations in a similar vein, Plötz and Roth (2018)\nrelaxed the KNN hard top- k selection rule. Xie and\nErmon (2019) replaced a sampling of kelements from\nthe collection of items with Gumbel trick. Nevertheless,\nall the mentioned top-kapproaches remain too costly as\nthey perform many iterations over a considered vector.\nTheir time performance degrades due to k softmaxes\nover the entire input length of n.\nXie et al. (2020) parametrized the top-koperator in\nterms of an optimal transport problem. Employing such\nan algorithm instead of softmax may induce numerous\nzero weights in the attention matrix. However, this does\nnot reduce the computational complexity of attention, as\nfull-matrix multiplication has to be performed anyway\nand we are not concerned with such a method.\nB.1 Limitations and Assumptions\nThe choice of the selection operator is challenging, as it\nhas to be trainable to instantiate a pooler. Let us view the\nhard top-koperator from a more geometric perspective.\nIn our setting, we consider sequences of nvectors\nfrom some vector space X (token embeddings), accom-\npanied by real-valued scores, which are the basis for\n8626\nchoosing the best kamong nvectors. Thus, formally, a\ntop-koperator should be deﬁned asΓ: Xn×Rn →Xk,\nassigning to a sequence of nvectors xi ∈X and their\nscores vi ∈R a sequence of kvectors yi ∈X. For Γ to\ndeserve the name ‘top-koperator’, the output vectorsyi\nshould depend mostly on the kinput vectors xi with the\nlargest corresponding scores.\nIn case of the hard top-koperator T, the yi are simply\nthe vectors xi with the largest scores, i.e.\nT((xi),(vi)) = (xi1 ,xi2 ,...,x ik), (2)\nwhere the indices i∗are chosen so that vi1 ≥vi2 ≥\n··· ≥vik ≥ vj for all j ̸∈ {i1,...,i k}. In other\nwords, T can be described as a composition of sorting\nthe sequence (xi) according to descending scores vi,\nand projecting onto Xk by discarding all but the ﬁrst k\nelements.\nTo discuss the properties of T, let us denote\nby Sn the set of all permutations of n indices\n{1,2,...,n }. For every sequence (x1,x2,...,x n) of\nlength nthere exists a permutation σ ∈Sn, such that\n(xσ(1),xσ(2),...,x σ(n)) is sorted in descending order.\nWe will refer to σ as the sorting permutation of the\nsequence (xi). It is unique, provided that the elements\nxi are all distinct. Otherwise, the sequence xis invari-\nant under permuting the indices of elements which are\nequal, and every two sorting permutations differ by such\na factor.\nFor a permutation σ ∈Sn, deﬁne Rσ ⊂Rn as the\nset of all vectors v ∈Rn for which σis a sorting per-\nmutation. The regions Rσ cover Rn and have disjoint\ninteriors, containing vectors with pairwise distinct coor-\ndinates. The restriction of T to each region Xn ×Rσ\nis independent of v ∈Rσ, and it reduces to a linear\noperator:\nT((xi),(vi)) = (xσ(1),...,x σ(k)). (3)\nIt follows that T is differentiable in the interior of\neach region Xn×Rσ, and its non-differentiability points\nare constrained to the boundaries of the differentiability\nregions, i.e. the set Xn ×D, where D = {x ∈Rn :\nxi = xjfor some i̸= j}.\nIn particular, since D is a union of hyperplanes of\ncodimension 1 in Rn, the non-differentiability set of T\nhas measure 0. Just as in the simpler case of the ReLU\nactivation function, the non-differentiability of the hard\ntop-k operator is not a serious problem—which is a\npossible misconception here.\nThe real problem is that although the gradient of T\nexists (almost everywhere), it is not particularly useful,\nsince\n∂T\n∂vi\n= 0, (4)\nbecause in each region Xn ×Rσ the operator T is in-\ndependent of vi. This makes back-propagation through\nthe scores impossible, and prevents training the scoring\nfunction. It could be seen as an extreme case of the\nvanishing gradient problem. In the next section, we\nintroduce a mechanism not prone to this issue.\nAlgorithm 1 Successive Halving Top-kSelection\n1: procedure TOPK(E,v)\n2: for i←1,log2(⌈n/k⌉) do\n3: E,v ←SORT(E,v)\n4: E,v ←TOURNAMENT (E,v)\n5: end for\n6: return E\n7: end procedure\n8:\n9: procedure SORT(E,v)\n10: v′←(v1,v2,..),where vi ≥vi+1 and vi ∈v\n11: E′←(E1,E2,..),where vi ≥vi+1 and vi ∈v\n12: return E′,v′\n13: end procedure\n14:\n15: procedure TOURNAMENT (E,v)\n16: n←1\n2 ∥v∥ ⊿Target size\n17: d←∥E∗,1∥ ⊿Representation depth\n18: v′←0n,1\n19: E′←0n,d\n20: for i←1,n do\n21: w←PEAKED SOFTMAX (vi,v2n−i+1)\n22: E′\ni ←Ei ·w0 + E2n−i+1 ·w1\n23: v′\ni ←vi ·w0 + v2n−i+1 ·w1\n24: end for\n25: return E′,v′\n26: end procedure\nB.2 Analysis and Discussion\nWe propose an O(nlog2(n/k)) time-complexity algo-\nrithm for selecting ktop-scoring representations from\na vector of length n. An iterative approach of Goyal\net al. (2018) with O(nk) complexity involves a higher\ncost for almost any k. The total number of exponen-\ntiation operations in the Successive Halving Top- k is\nbounded by 2n, as each round of the tournament halves\nthe input size. Compared to knin the case of the Goyal\net al. (2018) algorithm, orders of magnitude savings in\nexpensive exponentiation operations are obtained.\nAnother key requirement for a robust top-kalgorithm\nis to accurately approximate hard selection. Meanwhile,\niteration-based algorithm disperses the probability mass\nover all items, resulting in a poor approximation of top-\nk. This inefﬁciency of softmax over long vectors can be\novercome by multiplying them by a large constant; how-\never, this leads to numerical instability. Moreover, they\ntend to perform worse when employed as a neural net-\nwork layer due to the long chain of backpropagation’s\ndependencies.\nIn contrast, we always perform softmax over a pair of\nvalues, guaranteeing that there will be a candidate with\na ≥0.5 probability assigned. After each pass, the best\nscoring kvectors with a small noise are obtained. It is\na result of interpolating with the lower-scoring element\nfrom each pair.\nAs stated in the paper, we ensure that strong can-\ndidates have weakly-scoring opponents, strengthening\n8627\ntheir presence in the tournament’s next round. The\nfundamental requirement of this trick is to sort inputs,\nresulting in an additional cost of O(nlog(n)). How-\never, in the case of modern CPUs, this cost is practically\nnegligible. Yet, the sorting step can be omitted, lead-\ning to a slightly degraded top-kapproximation. During\nthe process, a vector with considerable noise may be\nproduced for elements with indexes closer to the n/2.\nNevertheless, some noise itself is desired, as it allows\ngradients to propagate to elements out of the top-k.\nB.3 Differential Properties\nRecall the description of hard top-kfrom Section B.1.\nThe main advantage introduced by soft top-koperator of\nSuccessive Halving, is providing reasonable gradients\nwith respect to the scores vi. This allows to create\na trainable pooling mechanism reducing the number\nof output embeddings. At the same time, it does not\nimprove differentiability—which is another possible\nmisconception we wanted to dispel.\nIn our proposed approach we assume that both n\nand kare powers of 2. The soft top-koperator is then\ndeﬁned through a composition of log2(n/k) halving\noperators Hn: Xn ×Rn →Xn/2 ×Rn/2, reducing\nthe number of vectors and their scores by half (see Ap-\npendix B).\nThe halving operator itself is the composition of sort-\ning the vectors together with their scores, and a transfor-\nmation C: Xn ×Rn →Xn/2 ×Rn/2 producing n/2\nconvex combinations of the form\nyi = wixi + (1 −wi)xn+1−i, (5)\nwhere the weights are the softmax of the pair of scores\n(vi,vn+1−i), i.e.\nwi = evi\nevi + evn+1−i\n. (6)\nSimilarly as in the case of the hard top- k operator,\nthe non-differentiability of Hn arises from sorting. The\nconvex combinations however smooth out some of the\nnon-differentiabilities.\nLet τ ∈Sn be the transposition of iand n+ 1 −i.\nThe transformation Cis then invariant under τ, which\ntransposes both the weights (wi,1 −wi), and vectors\n(xi,xn+1−i). Hence, Cis invariant under the subgroup\nG⊆Sn generated by such transpositions. As a conse-\nquence, on the set Xn ×⋃\nρ∈GσRρ the operator H is\ngiven by\nHn((xi),(vi)) =\n= C((xσ(1),...,x σ(n)),(vσ(1),...,v σ(n))), (7)\nand since Cis differentiable, so is the restriction of H\nto this region.\nIn summary, while in the case of the hard top-koper-\nator there are n! differentiability regions corresponding\nto sorting permutations, for the halving operator the\ndifferentiability regions are their unions corresponding\nto the cosets of Gin Sn. Since the generating transposi-\ntions of Gare disjointly supported, it is isomorphic to\nZn/2\n2 , and therefore there are 2−n/2n! differentiability\nregions.\nThe Successive Halving top-koperator is the compo-\nsition of multiple halving operators, each introducing\nnew non-differentiabilities, and the ﬁnal projection onto\nXk. The arising non-differentiability set is still of mea-\nsure 0, which is covered in detail in Appendix B.4.\nB.4 Differential Properties of Complete\nSuccessive Halving Top-k Operator\nWe have shown that hard top-k operator makes back-\npropagation through the scores impossible, and prevents\ntraining the scoring function (Section B.1), whereas top-\nn\n2 halving is not prone to this problem (Section B.3).\nWe discuss the properties of full-featured Successive\nHalving bellow.\nWe have previously covered the case ofHn. But the\nsuccesive halving top-koperator Γ: Xn ×Rn →Xk\nis the composition\nΓ = prXk ◦H2k ◦H4k ◦···◦ Hn/2 ◦Hn (8)\nof multiple halving operators, each introducing new non-\ndifferentiabilities, and the projectionprXk : Xk×Rk →\nXk. The non-differentiability set of Γ is contained in\nthe preimages of non-differentiability sets of the Hi\nwith respect to the preceding factors in the composition.\nIn such a situation it is generally not obvious that the\nresulting non-differentiability set is still of measure 0.\nTo remedy this, let us ﬁrst make some general observa-\ntions about differentiability sets of mappings between\nmanifolds.\nFor a mapping F: M →N of smooth manifolds,\ndenote by ZF the set of all points p ∈M such that\neither F is not smooth in any neighborhood of p, or the\nrank of the derivative ofF at pis not maximal. Observe\nthat if the closure ZF of ZF ⊆M has measure 0, then\nthe preimage F−1[E] of any set E ⊂N of measure 0\nis itself of measure 0. Indeed, we may decompose such\npreimage as\nF−1[E] = (F−1[E] ∩ZF)∪\n∪(F−1[E] ∩(M \\ZF)), (9)\nwhere the ﬁrst component has measure zero (being a\nsubset of ZF), while the second component can be cov-\nered by a countable family of open sets on which F\nis differentiable, its derivative has maximal rank, and\nthe constant rank theorem applies. Thus, locally on\neach set U of this cover, F is conjugate to a projec-\ntion Rm →Rn, and F|−1\nU [E] has measure 0. In the\nend, F−1[E] is decomposed into a countable union of\nzero-measure sets, so it has measure 0.\nIt follows that if G: N →P is another mapping\nsuch that ZG has measure 0 in N, then ZG◦F also has\n8628\nmeasure 0, since\nZG◦F ⊆ZF ∪F|−1\nM\\ZF\n[ZG] =\n= ZF ∪F|−1\nM\\ZF\n[ZG]. (10)\nAbove, F|−1\nM\\ZF\ncommutes with the closure operator\nbecause the restriction F|M\\ZF is continuous. This re-\nsult extends by induction to compositions of any number\nof mappings.\nIn order to show that Γ deﬁned as the composition (8)\nis almost everywhere differentiable, it therefore sufﬁces\nto prove that ZΓ has measure 0, which in turn amounts\nto showing that ZHi has measure zero for any halving\ntransformation Hi. Recall that the halving transforma-\ntion is the composition of the corresponding sorting\noperator and convex combination operator Cdeﬁned in\n(5) and (6).\nFor the sorting operator, the non-differentiability set\nis a union of a ﬁnite number of hyperplanes, hence\na closed set of measure zero, and outside this set the\nderivative has maximal rank. The operator C on the\nother hand is smooth, and it remains to verify the rank\nof its derivative. Denote ((yi),(ui)) = C((xi),(vi)),\nand observe that∂ui/∂xj = 0. Therefore it is enough to\nshow that the matrices of partial derivatives(∂yi/∂xj)ij\nand (∂ui/∂vj)ij have linearly independent columns.\nFor j ∈{i,2m+ 1 −i}we have\n∂yi\n∂xj\n= evj\nevi + ev2m+1−i\n>0, (11)\nand ∂yi/∂xj = 0 for all other j. Since the sets {i,2m+\n1 −i}are pairwise disjoint, the columns are linearly\nindependent.\nIn case of ∂ui/∂vj the reasoning is similar. They are\nagain nonzero only for j ∈{i,2m+ 1 −i}, for which\n∂ui\n∂vj\n=\n= evj\n(\nev2m+1−j (vj −v2m+1−j) + evj + ev2m+1−j\n)\n(evj + ev2m+1−j )2 ,\n(12)\nand this is strictly positive for at least one j ∈{i,2m+\n1 −i}. It follows that the columns are non-zero and\nhave non-zero entries in different rows, so again they\nare linearly independent.\nWe have therefore shown that the Jacobian matrix\nof C has linearly independent columns, or in other\nwords, its derivative is surjective at every point, which\nis what we needed to complete the proof that the non-\ndifferentiability set of Γ can be covered by a locally\nﬁnite family of codimension 1 submanifolds, thus being\nof measure 0.\n0.01\n0.10\n1.00\nk = 2\n k = 8\n k = 32\n26 28 210212214\n0.01\n0.10\n1.00\nk = 128\n26 28 210212214\nk = 512\n26 28 210212214\nk = 2048\nOur Goyal et al. Our(w/o sorting)\nFigure 5: Number of seconds required to process a batch\nof sequences (Y-axis). The lower the better. Results\ndepending on n(X-axis) for various values ofk, assum-\ning k <n. Depicted solution without sorting partially\ncovers the data points of the solution with sorting(Our).\nB.5 Performance\nIn Figure 5 and 6 we show that our approach is highly\nsimilar to real top-kfor any given k, and is signiﬁcantly\nfaster than alternative solutions, such as, e.g., iterative\ntop-kselection.\nWe assessed the performance of the Successive Halv-\ning Top-k as compared to Goyal et al. (2018) experi-\nmentally, on randomly sampled matrices E such that\nEij ∼U[−1,1] and scores vi ∼U[0,1]. The selected\nktop-scoring vectors were compared to the real top-k\nselection using normalized Chamfer Cosine Similarity\n(nCCS) as given:\nnCCS = 1\nk\nk∑\ni=1\nmax\nj∈[1,k]\n(cos(yi, ˆyj))\nAdditionally, we measured an average time for pro-\ncessing a batch of size 16 on the NVIDIA A100 GPU,\nand addressed the question of how both algorithms dif-\nfer in terms of speed (Figure 5) and quality (Figure 6),\ndepending on k and n choices. One can notice that\nthe higher the choice of k, the faster our algorithm is,\nand the slower is the iterative baseline of Goyal et al.\n(2018) as predicted by their complexities. Our solution’s\nqualitative robustness is proven by achieving higher sim-\nilarity to real top-kfor any given k. The score degrades\nas the number of rounds in the tournament increases, as\neach round introduces additional noise.\nTo assess the importance of the sorting step, we removed\nit from the algorithm and compared with the proposed\ntop-k. The results suggests that sorting is efﬁcient and\nfast, as it is introduces average time overhead of 7.3%,\nwhile allowing error to be reduced by 45.2% on average.\nC Summarization Experiments\nThis appendix covers other ablation studies and details\nof previously-reported experiments.\n8629\n0.0\n0.5\n1.0\nk = 2\n k = 8\n k = 32\n26 28 210212214\n0.0\n0.5\n1.0\nk = 128\n26 28 210212214\nk = 512\n26 28 210212214\nk = 2048\nOur Goyal et al. Our(w/o sorting)\nFigure 6: Approximation quality (Y-axis) in the nCCS\nmetric. The higher the better. Results depending on n\n(X-axis) for various values of k, assuming k<n .\nTable 5: Hyperparameters for shallow models used in\nthe summarization experiments.\nHparam Value\nEncoder Layers 2\nDecoder Layers 2\nV ocab size 32k\nDropouts .1\nActivation ReLU\nEmb dim 512\nFFN emb dim 2048\nEncoder positional emb sinusoidal\nDecoder positional emb None\nBatch size 256\nLearning rate 5e-4\nLearning rate decay –\nShared emb True\nWeight decay .1\nAttention heads 8\nBeam size 8\nTotal parameters 32M\nC.1 Shallow Models Setup\nShared setup. The models were trained using the\nAdam optimizer and cross-entropy loss, with hyperpa-\nrameters speciﬁed in Table 5. Validation was performed\nevery three epochs on a validation set and the train-\ning stopped when no progress was observed taking the\nseven last scores into account. Presented scores are the\nbest scores on a validation set. All of the considerations\nassumed the use of dot-product attention except for LSH\nand Efﬁcient Transformers.\nVanilla. The exact setup of Vanilla Transformer is\nprovided in Table 5.\nBlockwise. We employed block attention with win-\ndow size and stride equal to 512. We use block attention\nin the encoder, and the decoder features dense attention.\nThe rest of the parameters follows shared setup.\nTable 6: Hyperparameters for DeepPyramidion and\ndeep Blockwise baseline models used in the summa-\nrization experiments.\nHparam Value\nEncoder Layers 6\nDecoder Layers 6\nV ocab size 32k\nDropouts .1\nActivation ReLU\nEmb dim 768\nFFN emb dim 3072\nEncoder positional emb sinusoidal\nDecoder positional emb None\nBatch size 256\nLearning rate 5e-4\nLearning rate decay –\nShared emb True\nWeight decay .1\nAttention heads 8\nWarmup steps 5k\nTotal Parameters 124M\nTranspooler. Transpooler features linear scorer and\nsuccessive halving algorithm. It uses Blockwise’s setup\nof blockwise attention. Pooling is performed after the\nlast encoder layer. The number of halving rounds de-\npends on the proportion of maximal input sequence size\nand the desired bottleneck size. Transpoolers models\nwere trained and validated with our soft top-k.\nIn the case of input chunking and use of blockwise\nattention, positions were calculated originating at the\nbeginning of document. For simplicity, no positional\nembeddings were used on the decoder side. We argue,\nthat embeddings passed down have already sufﬁcient\npositional information from the encoder.\nLSH. All of the previous considerations assumed the\nuse of dot-product attention with memory and compu-\ntational costs growing quadratically with the input size.\nBaselines relying on either efﬁcient or LSH-based at-\ntention were conducted with two heads of local window\nattention that has been shown to improve models with\nlong-range sparsity (Rae and Razavi, 2020). Without\nlocal attention, their results were several points lower.\nWe assumed an LSH bucket size of 64 and four parallel\nhashes. Bucket size follows the authors’ recommen-\ndations, whereas the number of hashes is a reasonable\ntrade-off between memory complexity and approxima-\ntion quality (Kitaev et al., 2020). Although one may\nobtain slightly better scores with eight hashes, it would\nresult in higher memory consumption than in the case\nof full attention baselines for all of the considered se-\nquence lengths. The rest of the parameters follow the\nBlockwise baseline.\nEfﬁcient Transformer. The training setup follows\nthe original work. The Efﬁcient Transformer does not\n8630\nhave any speciﬁc parameters to determine, so all other\ntraining/validation choices agree with Blockwise base-\nline.\nFunnel Transformer. The training setup of Funnel\nfollows the original work, with the speciﬁc strided mean\npooling and upsampling before passing to the decoder.\nFor example, in Funnel 8k→512 (pooling from 8k to\n512), 16 consecutive tokens were averaged after the ﬁrst\nencoder layer. The decoder size is 8k, and the residual\nconnections start from the ﬁrst’s layer output (taken just\nbefore pooling).\nPoWER-BERT. As it comes to the PoWER-based\nmodels, we ﬁnetune Vanilla transformers with a pro-\ngressive elimination of word vectors on the encoder\nside, following the approach of Goyal et al. (2020). We\ndo not optimize the number of eliminated embeddings\nbut assume the ﬁxed reduction, similarly to our Pyra-\nmidion models. Additionally, Table 2 reports results\nwith a progressive elimination of word vectors on the\nencoder side, adapted from PoWER-BERT (Goyal et al.,\n2020). Note that models are not trained from scratch in\nthis approach, and we assumed blockwise attention to\nmake it comparable with our models (see Appendix B).\nWe started from appropriate checkpoints of a blockwise\nmodel and ﬁnetuned it for ten epochs. Here, we vali-\ndated every one epoch. As training time, we provide\ntimes achieved during this ﬁnetuning. As presumed, a\nhard selection of word vectors offers an improved in-\nference time for the cost of slightly decreased ROUGE\nscores.\nC.2 Number of Layers, Bottleneck Size\nDeeper Pyramidion and Transpooler models with var-\nious pooling conﬁgurations were further examined in\nTable 7. The training setup follows the previously de-\nscribed Transpooler setup. In the case of Pyramidion,\nwe pool after the ﬁrst or the second layer in the encoder.\nScores of Pyramidion with pooling operation after the\nsecond and subsequent layers are signiﬁcantly higher\nthan #9, presumably because the representations after\nthe ﬁrst layer are not reliable enough to produce mean-\ningful scores.\nThe Pyramidion with a three-layer encoder that re-\nduces the input of 8ktokens gradually to 2k[#13] offers\nresults 1.2 points better than the Vanilla model consum-\ning input of the same length [#3]. Additionally, the\ncomplexity was reduced by a factor of 13 and 4 in the\nencoder and decoder, respectively, while achieving3×\ntraining and 2.4×inference acceleration.\nFinally, a series of Pyramidion experiments con-\nﬁrmed the applicability of gradual pooling with bot-\ntlenecks of 128, 512, and 2k sizes [#12, #11, #13]. It\ncan be noticed that a reduction in the bottleneck’s size\nleads to a decrease in performance.\nC.3 Effect of Block Size\nWe provide ablation experiments on block size effects\nin Table 8. For simplicity, all of the previous experi-\nments were conducted with an attention block size of\n512 where applicable. Block consisting of 128 tokens\nlead to an improved encoder complexity and slightly\nlower computation time [#25, #28, #31]. It is not always\nachieved at the price of decreased ROUGE scores.\nThe scoring mechanism introduces some overhead\nduring the training, which may be noticeable for shorter\nsequences. However, when it comes to the inference\ntime we aimed at when proposing the method, it can be\nobserved that a pooling operation positively impacts it.\nPooling improves the inference time whether or not it is\nused in combination with blockwise attention.\nD Effect of Input Length\nThe importance of the longer input for the overall perfor-\nmance can be deduced by analyzing the performance of\nmodels #1-#8 in Table 2, where we employed different\ninput lengths for different models (Vanilla, Blockwise,\nand Pyramidion), and found out that a steady gain of\n3.3 −3.6 R1 (and 2.1 −2.6 R2) points is observed for\nall of them when the input length is extended from 2k\nto 8k. Please note that while these results are provided\nin the ablation study that features a shallower network,\nthe difference is signiﬁcant and consistent. Hence, we\ndid not repeat the experiment in the deeper setup.\nD.1 Deep Model Setup\nTraining. Table 6 presents the shared setup of a Deep-\nPyramidion and Blockwise, evaluated in the Section 5.\nWe train until the validation score was not achieved for\n7 consecutive validations.\nInference. We follow parameters for the generation\nof HAT-BART (Rohde et al., 2021): a beam width of\n2, length penalty of 1, and minimum and maximum\ngeneration lengths of 72 and 966, respectively. We\nvalidated on the validation set every three epochs and\nchose the best performing model to generate outputs on\nthe test set.\nD.2 Hardware and Software Used\nAll experiments and benchmarks were performed on\na DGX-A100 server equipped with eight NVIDIA\nTesla A100 GPUs. We based our experiments using\nfairseq (Ott et al., 2019) v0.9.0, Python 3.6.10, PyTorch\n1.6.0a0+9907a3e (Paszke et al., 2019), CUDA Version\n11.0 and NVIDIA drivers 450.51.06. We trained in a\nfull precision.\nD.3 Detailed Results\nTable 9 reports ROUGE scores for all of the evaluated\nmodels. In addition, we report 95% bootstrap conﬁ-\ndence intervals of an estimate of the data here to mean\nscores.\n8631\nTable 7: Scores and complexities of the Pyramidion and Transpooler with different encoder and decoder depths, as\nwell as various lengths after pooling. The input of 8k representations pooled gradually to decoder length. Two-layer\ndecoder and encoder of depth ranging from 2 to 4 layers. Arrow →denotes an additional pooling between encoder\nlayers.\n# Architecture Lengths Time ROUGE\nEncoder Decoder Training Inference R-1 R-2\n21\nPyramidion\n\n\n\n8k →2k 512 1.07 4.18 31.1 11.5\n22 8k, 8k →2k 512 1.55 4.26 41.2 16.5\n23 8k, 8k →2k →512 128 1.78 3.74 37.3 14.3\n24 8k, 8k →4k 2k 1.47 5.49 43.0 17.2\n25 Transpooler\n{8k, 8k 2k 1.26 5.51 42.7 16.7\n26 8k, 8k, 8k 2k 1.74 5.54 43.1 17.3\nTable 8: Scores depending on blockwise attention block size and sparsiﬁcation mechanism with 2kand 8kencoder\ninput length considered. Different models with a two-layer encoder and a two-layer decoder.\n# Pooling Block size Lengths Time ROUGE\nEncoder Decoder Training Inference R-1 R-2\n27\nNo pooling\n\n\n\n128 2k 2k 0.25 5.11 39.1 14.4\n28 512 2k 2k 0.31 5.28 38.6 14.1\n29 (without) 2k 2k 0.60 5.77 38.2 14.0\n30\nTranspooler\n\n\n\n128 2k 512 0.49 3.99 38.2 14.1\n31 512 2k 512 0.54 4.24 39.1 14.6\n32 (without) 2k 512 0.82 4.49 37.1 13.7\nThe average time of processing a batch of documents\nis reported in Table 10. We used batch of size 64 for\ntraining, and 8 for inference. Decoding experiments\nwere synthetic. Speciﬁcally, we assumed a ﬁxed length\nof either 256 or 512 tokens to decode to discount for\nlower processing time of models predicting the end of\nsequence token earlier.\n8632\nTable 9: Scores with 95% bootstrap conﬁdence intervals\nof an estimate of the data (Calmettes et al., 2012).\n# ROUGE-1 (CI) ROUGE-2 (CI)\n1 28.1 (27 .8 −28.3) 8 .3 (8 .1 −8.4)\n2 38.2 (37 .9 −38.5) 14 .0 (13 .8 −14.2)\n3 41.8 (41 .6 −42.1) 16 .1 (15 .9 −16.4)\n4 38.6 (38 .3 −38.8) 14 .1 (13 .9 −14.3)\n5 41.9 (41 .6 −42.1) 16 .7 (16 .5 −17.0)\n6 39.1 (38 .9 −39.4) 14 .6 (14 .4 −14.8)\n7 41.8 (41 .6 −42.1) 16 .4 (16 .2 −16.7)\n8 42.7 (42 .4 −43.0) 16 .7 (16 .5 −16.9)\n9 28.5 (28 .3 −28.7) 7 .5 (7 .4 −7.6)\n10 33.6 (33 .4 −33.8) 10 .5 (10 .4 −10.6)\n11 35.7 (35 .5 −36.0) 11 .2 (11 .1 −11.4)\n12 28.4 (28 .2 −28.6) 7 .8 (7 .7 −7.9)\n13 34.1 (33 .9 −34.4) 10 .4 (10 .3 −10.6)\n14 35.0 (34 .7 −35.2) 10 .8 (10 .7 −11.0)\n15 35.3 (35 .0 −35.5) 12 .7 (12 .5 −12.9)\n16 36.9 (36 .6 −37.2) 14 .1 (13 .9 −14.4)\n17 42.0 (41 .7 −42.3) 16 .5 (16 .3 −16.7)\n18 38.6 (38 .3 −38.8) 14 .3 (14 .1 −14.5)\n19 41.8 (41 .6 −42.1) 16 .5 (16 .3 −16.8)\n20 42.0 (41 .7 −42.2) 16 .4 (16 .2 −16.6)\n21 31.1 (30 .7 −31.6) 11 .5 (11 .3 −11.7)\n22 41.2 (40 .9 −41.4) 16 .5 (16 .3 −16.8)\n23 37.3 (37 .1 −37.6) 14 .3 (14 .1 −14.5)\n24 43.0 (42 .7 −43.3) 17 .2 (17 .0 −17.5)\n25 →See #8\n26 43.1 (42 .8 −43.3) 17 .3 (17 .0 −17.5)\n27 39.1 (38 .8 −39.3) 14 .4 (14 .2 −14.6)\n28 38.6 (38 .3 −38.8) 14 .1 (13 .9 −14.3)\n29 →See #2\n30 38.2 (38 .0 −38.4) 14 .1 (13 .9 −14.3)\n31 →See #6\n32 37.1 (36 .9 −37.4) 13 .7 (13 .5 −13.8)\nTable 10: Mean time of processing and inference in\nseconds ±standard deviation. We assumed a ﬁxed\nlength of 256 or 512 tokens to decode to discount for\nlower processing time of models predicting the end of\nsequence token earlier.\n# Training Inference @ 256 Inference @ 512\n1 0.13 ±0.02 2 .05 ±0.01 4 .23 ±0.01\n2 0.60 ±0.03 2 .76 ±0.01 5 .77 ±0.02\n3 4.46 ±0.26 6 .56 ±0.03 13 .27 ±0.06\n4 0.31 ±0.02 2 .58 ±0.00 5 .28 ±0.01\n5 0.85 ±0.12 5 .40 ±0.00 11 .49 ±0.01\n6 0.54 ±0.02 2 .09 ±0.00 4 .24 ±0.01\n7 1.44 ±0.04 2 .14 ±0.00 4 .28 ±0.01\n8 1.26 ±0.06 2 .71 ±0.00 5 .51 ±0.01\n9 0.19 ±0.02 2 .16 ±0.01 4 .27 ±0.01\n10 0.56 ±0.03 3 .01 ±0.01 5 .92 ±0.01\n11 1.69 ±0.12 0 .87 ±0.05 13 .41 ±0.07\n12 0.12 ±0.02 2 .16 ±0.01 4 .20 ±0.01\n13 0.29 ±0.03 2 .98 ±0.02 5 .91 ±0.01\n14 0.82 ±0.10 6 .91 ±0.06 13 .75 ±0.08\n15 1.04 ±0.04 2 .17 ±0.11 4 .28 ±0.18\n16 1.87 ±0.16 2 .71 ±0.09 5 .33 ±0.15\n17 2.06 ±0.16 3 .57 ±0.12 6 .92 ±0.17\n18 0.61 ±0.11 2 .07 ±0.06 4 .01 ±0.04\n19 1.78 ±0.14 2 .08 ±0.07 4 .03 ±0.06\n20 1.53 ±0.13 2 .64 ±0.07 5 .25 ±0.04\n21 1.05 ±0.05 2 .12 ±0.01 4 .18 ±0.01\n22 1.55 ±0.04 2 .12 ±0.01 4 .26 ±0.01\n23 1.78 ±0.05 1 .86 ±0.01 3 .74 ±0.01\n24 1.47 ±0.04 2 .69 ±0.01 5 .49 ±0.01\n25 →See #8\n26 1.74 ±0.05 2 .73 ±0.01 5 .54 ±0.01\n27 0.25 ±0.02 2 .51 ±0.00 5 .11 ±0.01\n28 0.31 ±0.02 2 .58 ±0.00 5 .28 ±0.01\n29 →See #2\n30 0.49 ±0.03 2 .04 ±0.01 3 .99 ±0.01\n31 →See #6\n32 0.82 ±0.03 2 .20 ±0.01 4 .49 ±0.02\n8633",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7635481357574463
    },
    {
      "name": "Transformer",
      "score": 0.6933474540710449
    },
    {
      "name": "Pooling",
      "score": 0.6625837087631226
    },
    {
      "name": "Automatic summarization",
      "score": 0.6610293388366699
    },
    {
      "name": "Security token",
      "score": 0.6145062446594238
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5436134338378906
    },
    {
      "name": "Inference",
      "score": 0.5313107967376709
    },
    {
      "name": "Task (project management)",
      "score": 0.4298642873764038
    },
    {
      "name": "Sublinear function",
      "score": 0.4120134711265564
    },
    {
      "name": "Machine learning",
      "score": 0.3749008774757385
    },
    {
      "name": "Voltage",
      "score": 0.19298100471496582
    },
    {
      "name": "Mathematics",
      "score": 0.10240787267684937
    },
    {
      "name": "Engineering",
      "score": 0.08191302418708801
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}