{
  "title": "Pharmacophoric-constrained heterogeneous graph transformer model for molecular property prediction",
  "url": "https://openalex.org/W4362520745",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2108468231",
      "name": "Yinghui Jiang",
      "affiliations": [
        "Ministry of Culture"
      ]
    },
    {
      "id": "https://openalex.org/A2518583641",
      "name": "Shuting Jin",
      "affiliations": [
        "Ministry of Culture",
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A3006351678",
      "name": "Xurui Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3017330710",
      "name": "Xianglu Xiao",
      "affiliations": [
        "Ministry of Culture"
      ]
    },
    {
      "id": "https://openalex.org/A2653085185",
      "name": "Wenfan Wu",
      "affiliations": [
        "Ministry of Culture"
      ]
    },
    {
      "id": "https://openalex.org/A2111831954",
      "name": "Xiangrong Liu",
      "affiliations": [
        "Xiamen University",
        "Ministry of Culture"
      ]
    },
    {
      "id": "https://openalex.org/A2011352411",
      "name": "Qiang Zhang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2589655975",
      "name": "Xiangxiang Zeng",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A1990782487",
      "name": "Guang Yang",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A3005685717",
      "name": "Zhangming Niu",
      "affiliations": [
        "Imperial College London",
        "Ministry of Culture"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2327127510",
    "https://openalex.org/W1983478747",
    "https://openalex.org/W2163646378",
    "https://openalex.org/W16352594",
    "https://openalex.org/W3206937279",
    "https://openalex.org/W3084084616",
    "https://openalex.org/W4280572290",
    "https://openalex.org/W3095883070",
    "https://openalex.org/W2290847742",
    "https://openalex.org/W2735246657",
    "https://openalex.org/W6680639217",
    "https://openalex.org/W2895884529",
    "https://openalex.org/W2968734407",
    "https://openalex.org/W3138215796",
    "https://openalex.org/W3137551757",
    "https://openalex.org/W3034516664",
    "https://openalex.org/W3000478925",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W2966357564",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3190020173",
    "https://openalex.org/W2743104969",
    "https://openalex.org/W2945266622",
    "https://openalex.org/W2965857891",
    "https://openalex.org/W2022476850",
    "https://openalex.org/W3100157108"
  ],
  "abstract": null,
  "full_text": "ARTICLE\nPharmacophoric-constrained heterogeneous\ngraph transformer model for molecular\nproperty prediction\nYinghui Jiang1,7, Shuting Jin 1,2,3,7, Xurui Jin1,7, Xianglu Xiao1, Wenfan Wu1, Xiangrong Liu2,3, Qiang Zhang 4,\nXiangxiang Zeng5, Guang Yang 6✉ & Zhangming Niu 1,6✉\nInformative representation of molecules is a crucial prerequisite in AI-driven drug design and\ndiscovery. Pharmacophore information including functional groups and chemical reactions\ncan indicate molecular properties, which have not been fully exploited by prior atom-based\nmolecular graph representation. To obtain a more informative representation of molecules for\nbetter molecule property prediction, we propose the Pharmacophoric-constrained Hetero-\ngeneous Graph Transformer (PharmHGT). We design a pharmacophoric-constrained multi-\nviews molecular representation graph, enabling PharmHGT to extract vital chemical infor-\nmation from functional substructures and chemical reactions. With a carefully designed\npharmacophoric-constrained multi-view molecular representation graph, PharmHGT can\nlearn more chemical information from molecular functional substructures and chemical\nreaction information. Extensive downstream experiments prove that PharmHGT achieves\nremarkably superior performance over the state-of-the-art models the performance of our\nmodel is up to 1.55% in ROC-AUC and 0.272 in RMSE higher than the best baseline model)\non molecular properties prediction. The ablation study and case study show that our pro-\nposed molecular graph representation method and heterogeneous graph transformer model\ncan better capture the pharmacophoric structure and chemical information features. Further\nvisualization studies also indicated a better representation capacity achieved by our model.\nhttps://doi.org/10.1038/s42004-023-00857-x OPEN\n1 MindRank AI Ltd., 310000 Hangzhou, China.2 School of Informatics, Xiamen University, 361005 Xiamen, China.3 National Institute for Data Science in\nHealth and Medicine, Xiamen University, 361005 Xiamen, China.4 School of Informatics, Zhejiang University, 310013 Hangzhou, China.5 School of\nInformation Science and Engineering, Hunan University, 410082 Changsha, Hunan, China.6 National Heart and Lung Institute, Imperial College London,\nLondon, UK. 7These authors contributed equally: Yinghui Jiang, Shuting Jin, Xurui Jin.✉email: g.yang@imperial.ac.uk; zhangming@mindrank.ai\nCOMMUNICATIONS CHEMISTRY|            (2023) 6:60 | https://doi.org/10.1038/s42004-023-00857-x | www.nature.com/commschem 1\n1234567890():,;\nT\nhe goal of drug discovery is toﬁnd novel molecules with\ndesired properties, and predicting the properties of mole-\ncules accurately has been one of the critical issues. The key\nstep of molecule properties prediction is how to represent the\nmolecules that map the molecular information to a feature vector.\nConventional methods learn the representations depending on\nthe expert-crafted physic-chemical descriptors\n1, molecular\nﬁngerprints2, or the quantitative structure-activity relationship\n(QSAR) method3,4.\nIn recent decades, deep learning methods have shown strong\npotential to compete with or even outperform conventional\napproaches. Graph neural networks (GNNs) have gained\nincreasing more popular due to their capability of modeling\ngraph-structured data. For the association prediction task of\nbiological network data, the heterogeneous graph neural network\nalgorithms5– 7 have achieved remarkable results. Molecules can be\nnaturally expressed as a graph structure, so the GNNs method\ncan effectively capture molecular structure information, including\nnodes (atoms) and edges (bonds)8. Compared with the conven-\ntional methods, deep learning methods can use SMILES or\nmolecular graph as input which is more informative and lead to\nsigniﬁcant improvement in downstream tasks such as molecules\nproperties prediction. The graph-based molecular property pre-\ndiction models view the molecules as graphs and use graph neural\nnetworks (GNN) to learn the representations and try to capture\nthe topological structure information from atoms and bonds. Due\nto their ability to represent molecules as graphs, they are an\nimportant research area for molecular property prediction tasks.\nThe most representative GNNs, including GCN9– 14, GAT15– 17,\nand MPNN18– 20 etc., have been actively used in the ﬁeld of\nmolecular graphs-based for molecular properties prediction.\nHowever, these models ignore the information of fragments that\ncontain functional groups. Recently, Zhang et al.17,21 works have\nbegun to focus on molecular fragment information to predict the\nproperties of molecules.\nAlthough incorporating fragment information into graph\narchitectures to beneﬁt some molecular property estimation tasks\nhas attracted research attention in recent years, there still are two\nissues that impede the usage of GNNs in this ﬁeld: (1) those\nmodels have not provided a global chemical perspective method\nto better integrate atom and fragment information and both\nignore the reaction information between fragments; (2) lacking\nthe generalization ability of the different types and feature\ndimensions of atoms, fragments, and bonds. To address those two\nissues, more comprehensive information from different levels\nneeds to be embedded and there is still a demand to develop a\nheterogeneous GNNs model for molecular property prediction.\nIn the study, we propose a Pharmacophoric-constrained Het-\nerogeneous Graph Transformer model (PharmHGT) to compre-\nhensively learn different views of heterogeneous molecular graph\nfeatures and boost the performance of molecule property predic-\ntion (the code is available on GitHub: https://github.com/\nmindrank-ai/PharmHGT). Firstly, we use the reaction informa-\ntion of BRICS to divide the molecule into fragments that contain\nfunctional groups and retain the reaction information between\nthese fragments to construct a heterogeneous molecular graph\ncontaining two types of nodes and three types of edges (Fig.1).\nThen, to comprehensively consider the multi-view and multi-scale\ngraph representations of molecules and the reaction information\nconnecting the fragments, we propose a novel heterogeneous graph\ntransformer model based on message passing. Speciﬁcally, we use\ntwo variants of transformers to learn the features of edges and\nnodes in heterogeneous graphs respectively, and aggregate and\nupdate these features of edges and nodes through message passing\nto obtain the representation of heterogeneous molecular graphs.\nExtensive experiments show that the model has outperformed the\nadvanced baselines on multiple benchmark datasets. Further\nablation experiments also showed the effectiveness of learning\nmolecules representation from different perspectives. Our con-\ntributions can be summarized as follows:\n● We obtain the pharmacophore information from the\ncompound reaction and retain the reaction information\nbetween the fragments. On this basis, a heterogeneous\nmolecular graph representation method is constructed.\n● We develop a heterogeneous graph transformer framework,\nwhich is able to ef ﬁciently capture the information of\ndifferent node types and edge types, including the reaction\ninformation between fragments through the fusion of multi-\nviews information of heterogeneous molecular graphs.\n● We evaluate PharmHGT on nine public datasets and\ndemonstrate its superiority over state-of-the-art methods.\nResults and discussion\nIn this section, we present the related work of thisﬁeld and the\nproposed PharmHGT model. We also presented the results of\nPharmHGT for molecular property prediction on ten datasets,\nthese experiments datasets are from Wu et al.\n22, including four\nclassiﬁcation and three regression tasks. More descriptions of the\ndata process can be found in Supplementary Note.\nRelated work. For graph data, if there is only one kind of node and\none kind of connection relationship from one node to another, it is\ncalled a homogeneous graph, otherwise, it is a heterogeneous graph.\nCurrently, most of molecular graph is based on homogeneous\ngraph and using the heterogeneous graph to learn the representa-\ntion is still blank. In this section, we review related prior homo-\ngeneous graph-based molecular representation methods and\nheterogeneous graph embedding. We focused on the homogeneous\ngraphs that have some relevance to our model and those models\nwere used for baseline comparison with PharmHGT.\nFragment-based homogeneous graph-based molecular representa-\ntion. It has been demonstrated that many characteristics of mole-\ncules are related to molecular substructures which contain functional\ngroup information. Zhang et al.\n17 obtained two fragments by\nb r e a k i n gt h ea c y c l i cs i n g l eb o n d si nam o l e c u l ea n de x p l o i t e da\nfragment-oriented multi-scale graph attention network for molecular\nproperty prediction (FraGAT), whichﬁrst proposed the deﬁnition of\nmolecule graph fragments that may contain functional groups.\nHowever, FraGAT directly adopted the Attentive FP15 to get\nmolecular graph embeddings, and the obtained fragments by this\nmethod also is rough because there are multiple substructures in one\nmolecular. Zhang et al.21 proposed the Motif-based Graph Self-\nsupervised Learning (MGSSL) model, which designed a molecule\nfragmentation method that leverages a retrosynthesis-based algo-\nrithm BRICS and additional rules for controlling the size of motif\nvocabulary and used GNNs to capture the rich structural and\nsemantic information from graphmotifs. However, this work still\ndid not consider the reaction information between substructures\nobtained through BRICS and effectively combine the information of\nthe atom and the substructures. Nevertheless, these works still prove\nthat considering more information from molecular substructures\nwith functional groups can provide a more informative representa-\ntion that can signiﬁcantly improve the performance of downstream\ntasks, but how and which kinds of information to be embedded\nneeds more exploratory work.\nMessage passing neural networks. Gilmer et al.18 proposed Mes-\nsage Passing Neural Networks (MPNNs), which is theﬁrst gen-\neral framework for supervised learning on graphs, and can\nARTICLE COMMUNICATIONS CHEMISTRY | https://doi.org/10.1038/s42004-023-00857-x\n2 COMMUNICATIONS CHEMISTRY|            (2023) 6:60 | https://doi.org/10.1038/s42004-023-00857-x | www.nature.com/commschem\neffectively predict the quantum mechanical properties of small\norganic molecules. The MPNNs framework is capable of learning\nthe representations from molecular graphs directly. Many\nresearchers made improvements on this basis and proposed many\nmodels based on MPNN. Yang et al.23 introduced a graph con-\nvolutional model, called Directed MPNN (D-MPNN), which used\nmessages associated with directed bonds to learn molecular\nrepresentations. Song et al.19 proposed a directed graph-based\nCommunicative Message Passing Neural Network (CMPNN) that\ncomprehensively considered the information of atoms and bonds\nto improve the performance of molecular properties prediction.\nHowever, those MPNNs have ignored the chemical reactions\ninformation, which is vital for molecular properties from the\nknowledge of chemistry and pharmacy.\nTransformer architecture. Researchers proposed the Transformer\narchitecture eschewing recurrence and convolutions entirely and\ninstead based solely on the attention mechanism24. Ying et al.25\nhave explored several simple coding methods of the graph,\nmathematically showing that many popular GNN variants are\nactually just special cases of Graph transformers. In theﬁeld of\nrepresentation learning of molecules, Rong et al.\n8 proposed\nGraph Representation frOm self-superVised mEssage passing\ntRansformer (GROVER), which can learn the rich structure and\nsemantic information of molecular from a large amount of\nunlabeled molecular data. Chen et al.26 proposed the Commu-\nnicative Message Passing Transformer (CoMPT), which rein-\nforces message interactions between nodes and edges based on\nthe Transformer architecture.\nHeterogeneous graph-based molecular representation.I nt h e\nﬁeld of recommendation systems, heterogeneous graph models\nare popular for mining scenarios with nodes and/or edges of\nvarious types27– 29. Heterogeneous graphs are notoriously difﬁ-\ncult to mine because of the bewildering combination of het-\nerogeneous contents and structures. The current representation\nlearning for molecules is still at the level of homogeneous\ngraphs, but in addition to the basic atom-based molecular graph\nrepresentation, some fragment-based and motif-based repre-\nsentation schemes have been proposed to represent a molecule.\nObviously, if these representation schemes can be constructed\nfor a comprehensive heterogeneous molecular graph repre-\nsentation, it will be more conducive to capturing the char-\nacterization of molecules and potentially improve the\nperformance of downstream tasks. In this paper, we propose a\nnew molecular heterogeneous graph construction method and\npropose a heterogeneous graph transformers model that can\nefﬁciently learn molecular representations.\nOverview of PharmHGT architecture . The key idea of\nPharmHGT is additionally capturing the pharmacophoric struc-\nture and chemical information feature from the heterogeneous\nmolecular graph. Generally, the heterogeneous graph is associated\nwith the node and edge attributes, while different node and edge\ntypes have unequal dimensions of features. As shown in Fig.2,\nthe proposed PharmHGT consists of three major modules: multi-\nview molecular graph construction, aggregation of nodes and\nedges information by heterogeneous graph transformer, and the\nFig. 1 An example of the overview of the molecular segmentation process and the construction of the heterogeneous molecular graph.In the\nheterogeneous molecular graph at the bottom, the green nodes represent fragments with pharmacophore information, and the blue nodes represent the\natoms of the molecule. The green edges are the reaction information between fragments, the red dotted line edges are the related information of the atoms\nthat connect the fragments, and the edges between atoms are bonds.\nCOMMUNICATIONS CHEMISTRY | https://doi.org/10.1038/s42004-023-00857-x ARTICLE\nCOMMUNICATIONS CHEMISTRY|            (2023) 6:60 | https://doi.org/10.1038/s42004-023-00857-x | www.nature.com/commschem 3\nattention mechanism to integrate multi-view molecular graph\nfeatures for molecular property prediction.\nExperiments\nDatasets. In order to better compare and prove the effectiveness\nof PharmHGT, we select nine benchmark molecular datasets for\nexperiments including Blood-brain barrier permeability (BBBP),\nBACE, ClinTox, Tox21, SIDER, and HIV for classiﬁcation tasks,\nand ESOL, Freesolv and Lipophilicity for regression tasks. Below,\nwe include a brief introduction of these datasets.\n● Classiﬁcation tasks. The BBBP dataset has 2035 molecules\nwith binary labels of permeability properties, which are\noften used to predict the ability of molecules to penetrate\nthe blood-brain barrier. The BACE dataset has 1513\nmolecules, which provides quantitative (IC\n50) and qualita-\ntive (binary label) binding results for a set of inhibitors of\nhuman β-secretase 1 (BACE-1). The ClinTox dataset has\n1468 approved drug molecules and a list of molecules that\nfailed due to toxicity during clinical trials. The Tox21\ndataset has 7821 molecules for 12 different targets relevant\nto drug toxicity and was originally used in the Tox21 data\nchallenge. The SIDER dataset has 1379 approved drug\nmolecules and their side-effect, which are divided into\n27 system organ classes. The HIV dataset has 41127\nmolecules and these molecules are tested for their ability to\ninhibit HIV replication.\n● Regression tasks. The ESOL dataset records the solubility of\n1128 compounds. The FreeSolv includes a total of 642\nmolecules are selected from the Free Solvation Database.\nThe Lipophilicity dataset provides the experimental result\nof octanol/water distribution coefﬁcient (logD at pH 7.4) of\n4198 compounds.\nImplementation details. Following the previous works, we illus-\ntrate the results of each experiment with 5-fold cross-validation\nand replicate trainingﬁve times to increase the credibility of our\nmodel. All benchmark datasets have been split as training,\nvalidation, and test sets with a ratio of 0.8/0.1/0.1, while all\nmodels were evaluated on random or scaffold-based splits as\nrecommended by23. The node and edge features are processed by\nthe open-source package RDKit, and the detail is demonstrated in\nSupplemental Experimental Procedures (Tables S1– S5).\nBaselines. In the study, we compare our model with eight base-\nline methods including 3 types.\n● Fragment-based method: The AttentiveFP15 is a graph\nneural network architecture, which uses a graph attention\nmechanism to learn from relevant drug discovery datasets.\nThe FraGAT17 exploited a fragment-oriented multi-scale\ngraph attention network for molecular property prediction;\nThe MGSSL21 designed Motif-based Graph Self-supervised\nLearning (MGSSL) by introducing a novel self-supervised\nmotif generation framework for GNNs.\n● MPNN baselines: The MPNN18 abstracts the commonal-\nities between several of the most promising existing neural\nmodels into a single common framework, and focused on\nobtaining effective vertices (atoms) embedding by message\npassing module and message updating module; The\nDMPNN23 used messages associated with directed bonds\nrather than those with vertices; The CMPNN19 introduced\na new message booster module to rich the message\ngeneration process.\n● Graph transformer baseline : The CoMPT 26, with a\nTransformer architecture, has learned a more attentive\nmolecular representation by reinforcing the message\ninteractions between nodes and edges; The GROVER\nmodel8 standard for Graph Representation frOm self-\nsuperVised mEssage passing tRansformer, which can learn\nrich structural and semantic information of molecules from\nenormous unlabeled molecular data by carefully designed\nself-supervised tasks in node-level, edge-level, and graph-\nlevel. In addition, the Graphormer model is also based on\ngraph transformer, but Graphormer is a 3D model, which\nrequires the 3D conformation of each small molecules.\nThere are some inconsistencies between our model and\nFig. 2 Illustration of Pharmacophoric-Constrained Heterogeneous Graph Transformer for molecular property prediction (PharmHGT).Firstly, the\nheterogeneous molecular graph is formalized as the feature matrix. Then, the feature matrix of each view willﬁrst do message passing independently to\nobtain the graph feature matrix of each view. Finally, the junction_view feature matrix willﬁrst do attention aggregation with the pharm_view feature matrix\nto obtain the aggregation feature matrix, then that matrix will do attention aggregation with the atom_view feature matrix, andﬁnally, obtain the features of\neach node, and input those nodes as a sequence into the GRU to get the representation vector of the entire small molecule.\nARTICLE COMMUNICATIONS CHEMISTRY | https://doi.org/10.1038/s42004-023-00857-x\n4 COMMUNICATIONS CHEMISTRY|            (2023) 6:60 | https://doi.org/10.1038/s42004-023-00857-x | www.nature.com/commschem\nGraphormer in terms of target tasks and inputs. For better\ncomparability, we have not added Graphormer into the\nbenchmark, but we give the computational results in the\nSupplemental Experimental Procedures (Table S7 and\nTable S8).\nPerformance comparison\nPerformance in classiﬁcation tasks.T a b l e1 presents the area under\nthe receiver operating characteristic curve (ROC-AUC) results of\neight baseline models on six classiﬁcation datasets. The Clintox,\nTox21, ToxCast, and SIDER are all multi-task learning tasks,\nincluding total of 658 classiﬁcation tasks. Compared with tradi-\ntional baselines and several GNN-based models, PharmHGT\nachieved large increases of ROC-AUC in all datasets (we give the\nprediction ROC curved plots in Fig. S1 and Fig. S2). PharmHGT is\ndesigned to be more attentive to pharmacophores, which makes this\nmodel more explainable. To note, the PharmHGT outperformed\nthe pre-train methods with less computational cost. We also give\ncomputing resources performance comparison to the state-of-the-\nart methods base on ESOL datasets, see the Table S6.\nPerformance in regressions tasks. Solubility and lipophilicity are\nbasic physical chemistry property, which is vital for explaining\nhow molecules interact with solvents and cell membrane. Table2\ncompares PharmHGT results to other state-of-the-art model\nresults. The best-case RMSE of the PharmHGT model on ESOL,\nFreeSolv and Lipophilicity are 0.680 ± 0.137, 1.266 ± 0.239, and\n0.583 ± 0.063 in random split, and 0.839 ± 0.049, 1.689 ± 0.516\nand 0.638 ± 0.040 in scaffold split. These results indicate that\nbetter representations of molecular graphs containing more\ninformation could signiﬁcantly increase the model performance\non downstream tasks.\nAblation study. We conducted ablation studies on PharmHGT to\nexplore the effect of atom-level view, pharm-level view, and\njunction-level view. Under the same experimental setup, we\nimplement seven simpliﬁed variants of PharmHGT on the two\nbenchmarks:\n● (1) PharmHGT_α: by only retaining the atom-level graph.\n● (2) PharmHGT_β: by only retaining the pharm-level graph\nwith reaction information.\n● (3) PharmHGT_ γ: by only retaining the junction-\nlevel graph.\n● (4) PharmHGT_βα: by aggregating features of the pharm-\nlevel graph with reaction information to the atom-\nlevel graph.\n● (5) PharmHGT_γα: by aggregating features of the junction-\nlevel graph to the atom-level graph.\n● (6) PharmHGT_βγ: by aggregating features of the pharm-\nlevel with reaction information to the junction-level graph.\n● (7) PharmHGT_ γαβ: by aggregating features of the\njunction-level graph with the atom-level graph, then to\nthe pharm-level graph.\nAs shown in Fig. 3, the PharmHGT considering the\nheterogeneous feature information from all views shows the best\nperformance among all architectures. The exclusions of the atom-\nlevel, pharm-level, or junction-level view both caused decreases in\nperformances and the PharmHGT_β performs the worst when\nonly retaining the pharm-level graph with reaction information.\nIt indicates that lacking information from the atoms can not\neffectively represent the characteristics of the molecule. When\ncombining two kinds of feature information, PharmHGT_ γα\naggregates the junction-level graph into an atom-level graph and\nit has the best performance among the models with one or two\nviews. It proves that integrating the feature information from\nmolecular fragments can improve the prediction performance.\nThe results of PharmHGT demonstrate that further integrating\nthe information from the reaction can obtain the most effective\nmolecular characterization.\nRepresentation visualization . To investigate the molecular\nrepresentations learning ability of PharmHGT, we used\nt-distributed Stochastic Neighbor Embedding (t-SNE) with\ndefault hyper-parameters to visualize molecular representations\nof the Tox21 dataset in Fig. 4. For this result, we deﬁne all\nmolecules with a label of 0 as non-toxic compounds, and any\nmolecule with a label of 1 as a toxic compound, and molecules\nwith similar toxicity tend to have more similar feature spaces.\nTherefore, we visualize their embeddings by t-SNE and evaluate\nwhether the model can learn effective molecular representations\nby whether the toxic and non-toxic molecules have a clear\nboundary. The DMPNN has second performance in Tox21 task\nTable 1 Overall performance comparison to the state-of-the-art methods on molecular property prediction classiﬁcation tasks.\nClassiﬁcation (ROC-AUC%, higher is better↑)\nDataset BBBP BACE ClinTox Tox21 SIDER HIV\nMolecules 2039 1513 1478 7831 1427 41127\nTask 1 1 2 12 27 1\nSplitting strategy Scaffold Scaffold Scaffold Scaffold Scaffold Scaffold\nAttentiveFP 90.8 (5.01) 78.4 (0.02) 93.3 (2.04) 80.7 (2.02) 60.5 (6.01) 75.7 (1.40)\nFragGAT 92.3 (4.04) 80.1 (0.86) 93.9 (2.06) 82.3 (1.62) 63.3 (3.23) 76.1 (0.65)\nMGSSL 69.7 (0.91) - 80.7 (2.12) 76.5 (0.31) 61.8 (0.81) -\nMPNN 91.3 (4.14) 77.9 (1.62) 87.9 (5.25) 80.8 (2.39) 59.5 (3.03) 74.1 (1.15)\nDMPNN 91.9 (3.04) 80.9 (0.60) 89.7 (4.01) 82.6 (2.32) 63.2 (2.28)\n78.6 (1.40)\nCMPNN 92.7 (0.23) 82.1 (0.64) 90.2 (1.20) 80.6 (1.57) 61.6 (0.31) 77.4 (0.50)\nCoMPT 93.8 (2.13) 81.9 (1.26) 93.4 (1.85) 80.9 (1.40) 63.4 (2.97) 78.1 (2.60)\nGROVERbase 93.6 (0.80) 82.6 (0.70) 92.5 (1.30) 81.9 (2.00) 65.6 (0.60) 62.5 (0.90)\nGROVERlarge 94.0 (1.90) 81.0 (1.40) 94.4 (2.10) 83.1 (2.50) 65.8 (2.30) 68.2 (1.10)\nPharmHGT 95.4 (1.15) 86.5 (2.21) 94.5 (0.42) 83.9 (0.56) 66.9 (1.63) 80.6 (0.21)\nThe results of baselines are obtained by us using a 5-fold cross-validation with scaffold split and doing experiment on each task for one time. The values in this table are the Mean and standard deviation\nof ROC-AUC values. The best performance is marked in bold and the second best is underlined to facilitate reading.\nCOMMUNICATIONS CHEMISTRY | https://doi.org/10.1038/s42004-023-00857-x ARTICLE\nCOMMUNICATIONS CHEMISTRY|            (2023) 6:60 | https://doi.org/10.1038/s42004-023-00857-x | www.nature.com/commschem 5\nand achieves reasonable distinction between toxic and non-toxic\nmolecules (Fig. 4a), however, PharmHGT shows a more visible\nboundary to classify toxic and non-toxic compounds (Fig.4c). In\naddition, the single-view (Fig.4b) performance is far inferior to\nthe multi-view PharmHGT (Fig. 4c), which also proves the\nnecessity of considering the molecular multi-view information.\nCase study. Pharmacophore is a molecular framework that\ndeﬁnes the necessary components that are responsible for speciﬁc\nproperties. Accordingly, identifying and adding the\npharmacophore structure information associated with the target\nproperty into the model is vital for molecular representation. To\nillustrate the pharmacophore structure learning ability of\nPharmHGT, we visualize molecular features on the ClinTox\ndataset and select six molecules that are toxic in clinical trials and\nseveral of them have been applied in the clinical setting as che-\nmotherapeutic drugs. The toxicities of these six molecules are\nhighly correlated with the contained pharmacophore (i.e., some\nspeciﬁc sub-structure). Figure5c shows that our PharmHGT can\naggregate molecules with similar toxic pharmacophores together\nand distinguish them from non-toxic samples; PharmHGT_ α\ncannot well aggregate molecules with similar toxic pharmaco-\nphores, and have limited discrimination from negative samples\nwithout the pharm-level view (Fig.5b); The pretraining model\nGrover, which achieves second performance in ClinTox subtask,\ncan only aggregate only a few molecules with similar toxic\npharmacophores (Fig. 5a) and the discrimination for non-toxic\nsamples is far less than PharmHGT. This indicates that the\nembedded representations learned by PharmHGT can capture\nfunctional group structural information more effectively.\nConclusions\nIn this paper, we propose PharmHGT, a pharmacophoric-\nconstrained heterogeneous graph transformer model for mole-\ncular property prediction. We use the reaction information of\nBRICS to decompose molecules into several fragments and con-\nstruct a heterogeneous molecular graph. Furthermore, we develop\na heterogeneous graph transformation model to capture global\ninformation from multi-views of heterogeneous molecules.\nExtensive experiments demonstrate that our PharmHGT model\nachieves state-of-the-art performance on molecular properties\nprediction. The ablation study and case study also demonstrate\nthe effectiveness of using pharmacophore group information and\nheterogeneous molecules information of molecules.\nMethods\nNotation and problem deﬁnition. We use the BRICS30 to decompose molecules\ninto several fragments with pharmacophore, and retain the reaction information\nbetween fragments to construct a heterogeneous molecular graph. The hetero-\ngeneous molecular graph is denotedG = {V, E}, theG associated with a node type\nmapping function φ : V ! O and an edge type mapping functionψ : E ! P,\nwhere O and P represent the set of all node types and the set of all edge types,\nrespectively. We treat molecular structure as heterogeneous graphs to capture the\nchemical information from functional substructures and chemical reactions. We\npropose three views of molecular graph representation schemes, which are the\nTable 2 Overall performance comparison to the state-of-the-art methods on molecular property prediction regression tasks.\nRegression (RMSE, lower is better↓)\nDataset ESOL FreeSolv Lipophilicity ESOL FreeSolv Lipophilicity\nMolecules 1128 642 4200 1128 642 4200\nTasks 1 1 1 1 1 1\nSplitting strategy Random Random Random Scaffold Scaffold Scaffold\nAttentiveFP 0.853 (0.060) 2.030 (0.420) 0.650 (0.030) 0.877 (0.029) 2.073 (0.183) 0.721 (0.001)\nFragGAT 0.878 (0.124)\n1.538 (0.640) 0.645 (0.042) 0.884 (0.041) 2.065 (0.201) 0.750 (0.013)\nMPNN 1.167 (0.430) 2.185 (0.952) 0.672 (0.051) 1.541 (0.630) 2.430 (0.821) 0.730 (0.063)\nDMPNN 0.980 (0.258) 2.177 (0.914) 0.653 (0.046) 1.050 (0.008) 2.182 (0.183) 0.683 (0.016)\nCMPNN 0.789 (0.112) 2.007 (0.442) 0.614 (0.029) 0.845 (0.039) 1.833 (0.580) 0.658 (0.029)\nCoMPT 0.774 (0.058) 1.855 (0.578) 0.592 (0.048) 0.915 (0.042) 1.959 (0.808) 0.646 (0.028)\nGROVERbase 0.888 (0.116) 1.592 (0.072) 0.660 (0.061) 1.185 (0.160) 2.001 (0.081) 0.817 (0.008)\nGROVERlarge 0.831 (0.120) 1.544 (0.397) 0.643 (0.030) 1.098 (0.178) 1.987 (0.072) 0.823 (0.010)\nPharmHGT 0.680 (0.137) 1.266 (0.239) 0.583 (0.026) 0.839 (0.049) 1.689 (0.516) 0.638 (0.040)\nThe results of baselines are obtained by us using a 5-fold cross-validation with scaffold split or Random split and doing experiments on each task for one time. The values in this table are the Mean and\nstandard deviation of RMSE values. The best performance is marked in bold and the second best is underlined to facilitate reading.\nFig. 3 Ablation results on BBBP and ESOL datasets.The “X” represent the\nPharmHGT, the “X_” represents different PharmHGT variants of\naggregating atom-level, junction-level, and pharm-level features.\nARTICLE COMMUNICATIONS CHEMISTRY | https://doi.org/10.1038/s42004-023-00857-x\n6 COMMUNICATIONS CHEMISTRY|            (2023) 6:60 | https://doi.org/10.1038/s42004-023-00857-x | www.nature.com/commschem\natom-level view, pharm-level view containing pharmacophore information as well\nas reaction information, and junction-level view to comprehensively represent a\nmolecule (Fig. 1). The speciﬁcd eﬁnition is as follows:\nDeﬁnition 1. (Atom-level view.) An atom-level view can be denoted as graph\nGα = (Vα, Eα), for each atomvα\ni we havevα\ni 2 Vα where 1≤ i ≤ ∣Nα∣ and ∣Nα∣ is the\ntotal number of atoms, while for each bondeα\nij we haveeα\nij 2 Eα where 1≤ i, j ≤ ∣Nα∣.\nFor featurization, theVα is represented asXα\nv 2 RNα ´Dα\nv where Dα\nv is the dimen-\nsions of atom features, theEα is represented asXα\ne 2 RMα ´Dα\ne where ∣Mα∣ is the\ntotal number of directed bonds,Dα\ne is the dimensions of bond features.\nDeﬁnition 2. (Pharm-level view.) A pharm-level view can be denoted as graph\nGβ = (Vβ, Eβ), for each pharmacophorevβ\ni we havevβ\ni 2 Vβ where 1≤ i ≤ ∣Nβ∣ and\n∣Nβ∣ is the total number of pharmacophores, while for each BRICS reaction typeeβ\nij\nwe have eβ\nij 2 Eβ where 1≤ i, j ≤ ∣Nβ∣. For featurization, theVβ is represented as\nXβ\nv 2 RNβ ´Dβ\nv where Dβ\nv is the dimensions of pharmacophore features, theEβ is\nrepresented as Xβ\ne 2 RMβ ´Dβ\ne where ∣Mβ∣ is the total number of BRICS reaction\ntypes, Dβ\ne is the dimensions of BRICS reaction type features.\nDeﬁnition 3. (Junction-level view.) A junction-level view can be denoted as graph\nGγ = (Vγ, Eγ), for each nodevγ\ni we have vγ\ni 2 Vγ where 1≤ i ≤ ∣Nγ∣ and ∣Nγ∣ is the\ntotal number of atoms and pharmacophores, while for each edgeeγ\nij we have\neγ\nij 2 Eγ where 1≤ i, j ≤ ∣Nγ∣. For featurization, theVγ is represented asXγ\nv 2\nRNγ ´Dγ\nv where Dγ\nv is the dimensions of pharmacophore features, theEγ is repre-\nsented as Xγ\ne 2 RMγ ´Dγ\ne where ∣Mγ∣ is the total number of atoms and pharmaco-\nphores junction relationships,Dγ\ne is the dimensions of junction relationship\ninformation.\nAn example of the heterogeneous molecular graph and its multi-view is\nillustrated in Fig.1, which contains 2 node types and 3 edge types. Given the above\ndeﬁnitions, our main task is to learn representations of heterogeneous molecular\ngraphs.\nOverview of PharmHGT. The key idea of PharmHGT is additionally capturing the\npharmacophoric structure and chemical information feature from heterogeneous\nmolecular graphs. Generally, the heterogeneous graph is associated with node and\nedge attributes, while different node and edge types have unequal dimensions of\nfeatures. The framework consists of three parts: multi-view molecular graph con-\nstruction (Fig. 1), aggregation of nodes and edges information by heterogeneous\ngraph transformer, and the attention mechanism to integrate multi-view molecular\ngraph features for molecular property prediction (Fig.2).\nObtaining the embedding of nodes and edges. The inputs of PharmHGT are the\nfeature matrix of nodeX\nV and the feature matrix of edgeXE, the features of all\nnodes can be obtained according to the intensity of the attention between the node\nand the related edge. The multi-head self-attention mechanism enhances the signal\nof the node in each view. Speciﬁcally, the basic block of PharmHGT is the usual\nattention module:\n½Q; K; V/C138¼ hðXÞ½W\nQ; WK ; WV /C138 ð1Þ\nwhere h(X) is the hidden features,WQ, WK, WV are the projection matrices. The\nnormal attention module is the dot product self-attention, theQ, K, V is considered\nin the same semantic vector space, which is not adapted in heterogeneous graph.\nTherefore, we build a multi-view attention function to get more information from\ndifferent views, and the function can be formulated as:\nAttentionðQ; K; VÞ¼ ∑\np2P\nΩpσ QpKpT\nﬃﬃﬃﬃﬃ\ndk\np\n !\nVp ð2Þ\nwhere σ is active function,P is the view type set,p ∈ P is a view andΩp is a\nlearnable view type weight matrix.KpT is the transpose matrix of viewp key matrix,\nand dk is the variance ofQ and K. In addition, our model assumes that a singleqi\nand ki satisfy the mean of 0 and the variance of 1. Considering the more general\ncase, qi and ki satisfy the mean value of 0 and the variance isσ, thenDðqikT\ni Þ¼ σ4.\nAnd D(QKT) = dkσ4. In any case, divide by\nﬃﬃﬃﬃﬃ\ndk\np\nto ensure thatDðQKT Þ¼ DðqikT\ni Þ.\nThe reason to guarantee this is to make softmax not affected by the dimension of\nthe vector. Furthermore, after adding multi-head attention structures, the\nembedding matrix can be formulated as:\nheadi ¼ AttentionðQi; Ki; ViÞ\nHeadi ¼ Concatðhead1; head2; ¼; headnÞWo\n/C26\nð3Þ\nwhere Wo is the weight matrix of each head. Therefore, we can get the hidden\nnodes and edges features embedding matrix:\nHðXV Þ¼ Concatðh1ðXV Þ; h2ðXV Þ; ¼; hnðXV ÞÞWo\nV\nHðXEÞ¼ Concatðh1ðXEÞ; h2ðXEÞ; ¼; hnðXEÞÞWo\nE\n/C26\nð4Þ\nAggregation nodes and edges information. For each molecular graph view, we\nuse graph transformer to obtain all nodes and edges features. All nodes’ features is\nXvi\n, ∀ vi ∈ V, and the all edge nodes’ features are Xeij\n, eij ∈ E. PharmHGT inter-\nactively operates on edge hidden statesHðXeij\nÞ, node hidden stateHðXvi\nÞ, message\nMV ðXvi\nÞ and MEðXeij\nÞ. To learn different knowledge from multi-view snapshots,\nwe build a view attention message passing strategy that is based on multi-head\nattention structures, the node and edge feature are propagated at each iteration,t\ndenotes the current depth of the message passing, each step proceeds as follows:\nM1\nV ðXvi\nÞ¼ ∑\nΘN ðvi Þ\nHðXΘN ðviÞÞ; t ¼ 1 ð5Þ\nM1\nEðXeij\nÞ¼ HðXvi\nÞ; t ¼ 1 ð6Þ\nMt\nV ðXvi\nÞ¼ ∑\nΘN ðviÞ\nAttention Ht/C0 1ðXvi\nÞWQ\nvi\n/C16\n;\nMt/C0 1ðXΘN ðvi ÞÞWK\nmi\n; Ht/C0 1ðXvi\nÞWV\nvi\n/C17\n; t> 1\nð7Þ\nMt\nEðXeij\nÞ¼ Linear M1\nEðXeij\nÞþ Ht ðXvi\nÞ\n/C16\n/C0 Ht/C0 1ðXeji\nÞ\n/C17\n; t> 1\nð8Þ\nwhere the ΘN ðviÞ is the function toﬁnd edges directed to nodevi. Considering of\nthe vanishing gradient issue, we set a simple residual block to make module\ntraining more stable during multi-views message passing:\nHt ðXvi\nÞ¼ Ht/C0 1ðXvi\nÞþ Mt\nV ðXvi\nÞ\nHt ðXeij\nÞ¼ Ht/C0 1ðXeij\nÞþ Mt\nEðXeij\nÞ\n(\nð9Þ\nFusion multi-views information. For a given molecule, we obtain all types of\nrepresentations of the three views of molecule atom-level, pharm-level, and\njunction-level by the above steps. Besides, the Gated Recurrent Unit is applied as a\nvision readout operator to get all three views feature vector {Z\nα, Zβ, Zγ} of the\nmolecule, whereZα is the vector of atom-level view,Zβ is the vector of pharm-level\nview and Zγ is the vector of junction-level view.\nThen, the acquired three views features are aggregated to theﬁnal features\nthrough the attention layer again, and theﬁnal representation vector of a molecule\nFig. 4 Visualization of molecular features.Visualization of molecular features for Tox21 froma DMPNN, b PharmHGT_α, andc PharmHGT with t-SNE.\nAll molecules with a label of 0 as non-toxic compounds, and any molecule with a label of 1 as a toxic compound, where toxicity compounds are colored in\nred and the non-toxic ones are in blue.\nCOMMUNICATIONS CHEMISTRY | https://doi.org/10.1038/s42004-023-00857-x ARTICLE\nCOMMUNICATIONS CHEMISTRY|            (2023) 6:60 | https://doi.org/10.1038/s42004-023-00857-x | www.nature.com/commschem 7\nis obtained. The readout attention function is:\nReadOutAttentionðX; YÞ¼ σ X /C1 YT\nﬃﬃﬃﬃﬃ\ndk\np\n !\nX ð10Þ\nSpeciﬁcally, the pharm-level-based contains the features of the reaction\ninformation, and weﬁrst aggregate it with the junction-level-based features to\ncapture the associated information of pharmacophores and atoms and the reaction\ninformation between pharmacophores. The formula is as follows:\nZγβ ¼ ReadOutAttentionðZγ; ZβÞ ð11Þ\nThen we are aggregating this information with atom-level-based feature\ninformation to obtain theﬁnal molecular global feature representation (Fig.2). The\nattention layer can distinguish the importance of features and adaptively assign\nmore weight to more important features.\nZ ¼ ReadOutAttentionðZα; ZγβÞ ð12Þ\nFinally, we perform downstream property predictions^y ¼ f ðZÞ where f(⋅) is a fully\nconnected layer for classiﬁcation or regression.\nFig. 5 Case study.Case study by t-SNE visualization of molecular features on ClinTox dataset froma GROVER, b PharmHGT_α, andc PharmHGT. Where\nmolecular with toxicity are colored in gray, non-toxic molecules are in red and blue indicating six molecules are selected for the case study that showed\ntoxicity in clinical trials and is still toxic after marketing.\nARTICLE COMMUNICATIONS CHEMISTRY | https://doi.org/10.1038/s42004-023-00857-x\n8 COMMUNICATIONS CHEMISTRY|            (2023) 6:60 | https://doi.org/10.1038/s42004-023-00857-x | www.nature.com/commschem\nData availability\nAll related data in this paper are public. All downstream datasets can be downloaded\nfrom MoleculeNet.\nCode availability\nThe implementation of PharmHGT is publicly available athttps://github.com/stardj/\nPharmHGT/.\nReceived: 5 August 2022; Accepted: 20 March 2023;\nReferences\n1. Xue, L. & Bajorath, J. Molecular descriptors in chemoinformatics,\ncomputational combinatorial chemistry, and virtual screening.Comb. Chem.\nHigh Throughput Screen.3, 363– 372 (2000).\n2. Cereto-Massagué, A. et al. Molecular ﬁngerprint similarity search in virtual\nscreening. Methods 71,5 8– 63 (2015).\n3. Dudek, A. Z., Arodz, T. & Gálvez, J. Computational methods in developing\nquantitative structure-activity relationships (qsar): a review.Comb. Chem.\nHigh Throughput Screen.9, 213– 228 (2006).\n4. Nantasenamat, C., Isarankura-Na-Ayudhya, C., Naenna, T. &\nPrachayasittikul, V. A practical overview of quantitative structure-activity\nrelationship. https://doi.org/10.17877/DE290R-690 (2009).\n5. Li, Y., Qiao, G., Wang, K. & Wang, G. Drug– target interaction predication via\nmulti-channel graph neural networks.Brief. Bioinform. 23, bbab346 (2022).\n6. Abbasi, K., Razzaghi, P., Poso, A., Ghanbari-Ara, S. & Masoudi-Nejad, A.\nDeep learning in drug target interaction prediction: current and future\nperspectives. Curr. Med. Chem.28, 2100– 2113 (2021).\n7. Tran, H. N. T., Thomas, J. J. & Malim, N. H. A. H. Deepnc: a framework for\ndrug-target interaction prediction with graph neural networks.PeerJ 10,\ne13163 (2022).\n8. Rong, Y. et al. Self-supervised graph transformer on large-scale molecular\ndata. Adv. Neural Inform. Process. Syst.33, 12559– 12571 (2020).\n9. Duvenaud, D. et al. Convolutional networks on graphs for learning molecular\nﬁngerprints. https://arxiv.org/abs/1509.09292 (2015).\n10. Kearnes, S., McCloskey, K., Berndl, M., Pande, V. & Riley, P. Molecular graph\nconvolutions: moving beyondﬁngerprints. J. Comput. Aided Mol. Design30,\n595– 608 (2016).\n11. Coley, C. W., Barzilay, R., Green, W. H., Jaakkola, T. S. & Jensen, K. F.\nConvolutional embedding of attributed molecular graphs for physical\nproperty prediction. J. Chem. Inform. Model.57, 1757– 1772 (2017).\n12. Ryu, S., Lim, J., Hong, S. H. & Kim, W. Y. Deeply learning molecular\nstructure-property relationships using attention-and gate-augmented graph\nconvolutional network. https://arxiv.org/abs/1805.10988 (2018).\n13. Feinberg, E. N. et al. Potentialnet for molecular property prediction.ACS Cent.\nSci. 4, 1520– 1530 (2018).\n14. Liu, S., Qu, M., Zhang, Z., Cai, H. & Tang, J. Structured multi-task learning for\nmolecular property prediction. In:International Conference on Artiﬁcial\nIntelligence and Statistics. 8906– 8920 (PMLR, 2022).\n15. Xiong, Z. et al. Pushing the boundaries of molecular representation for drug\ndiscovery with the graph attention mechanism.J. Med. Chem.\n63, 8749– 8760\n(2019).\n16. Jiang, S. & Balaprakash, P. Graph neural network architecture search for\nmolecular property prediction. In:2020 IEEE International Conference on Big\nData (Big Data)1346– 1353 (IEEE, 2020).\n17. Zhang, Z., Guan, J. & Zhou, S. Fragat: a fragment-oriented multi-scale graph\nattention model for molecular property prediction.Bioinformatics 37,\n2981– 2987 (2021).\n18. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O. & Dahl, G. E. Neural\nmessage passing for quantum chemistry. In:International Conference on\nMachine Learning. 1263– 1272 (PMLR, 2017).\n19. Song, Y. et al. Communicative representation learning on attributed molecular\ngraphs. In: IJCAI. 2831-2838 (IJCAI, 2020).\n20. Withnall, M., Lindelöf, E., Engkvist, O. & Chen, H. Building attention and\nedge message passing neural networks for bioactivity and physical– chemical\nproperty prediction. J. Cheminformat. 12,1 – 18 (2020).\n21. Zhang, Z., Liu, Q., Wang, H., Lu, C. & Lee, C.-K. Motif-based graph self-\nsupervised learning for molecular property prediction.Adv. Neural Inform.\nProcess. Syst. 34 https://arxiv.org/abs/2110.00987 (2021).\n22. Wu, Z. et al. Moleculenet: a benchmark for molecular machine learning.\nChemi. Sci. 9, 513– 530 (2018).\n23. Yang, K. et al. Analyzing learned molecular representations for property\nprediction. J. Chem. Inform. Model.59, 3370– 3388 (2019).\n24. Vaswani, A. et al. Attention is all you need. InAdvances in neural information\nprocessing systems. 31st Conference on Neural Information Processing Systems\n(NIPS 2017), Long Beach, CA, USA.5998– 6008 (2017).\n25. Ying, C. et al. Do transformers really perform bad for graph representation?\nhttps://arxiv.org/abs/2106.05234 (2021).\n26. Chen, J., Zheng, S., Song, Y., Rao, J. & Yang, Y. Learning attributed graph\nrepresentations with communicative message passing transformer.https://\narxiv.org/abs/2107.08773 (2021).\n27. Dong, Y., Chawla, N. V. & Swami, A. metapath2vec: Scalable representation\nlearning for heterogeneous networks. In:Proceedings of the 23rd ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining,\n135– 144 (Association for Computing Machinery, 2017).\n28. Cen, Y. et al. Representation learning for attributed multiplex heterogeneous\nnetwork. In: Proceedings of the 25th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, 1358– 1368 (Association for\nComputing Machinery, 2019).\n29. Zhang, C., Song, D., Huang, C., Swami, A. & Chawla, N. V. Heterogeneous\ngraph neural network. In:Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, 793-803 (Association for\nComputing Machinery, 2019).\n30. Degen, J., Wegscheid-Gerlach, C., Zaliani, A. & Rarey, M. On the art of\ncompiling and using’drug-like’chemical fragment spaces.ChemMedChem 3,\n1503 (2008).\nAcknowledgements\nThis study was supported in part by the ERC IMI (101005122), the H2020 (952172), the\nMRC (MC/PC/21013), the Royal Society (IEC/NSFC/211235), and the UKRI Future\nLeaders Fellowship (MR/V023799/1).\nAuthor contributions\nY.J., S.J., and Z.N. led the research. Y.J., S.J., X.J., and G.Y. contributed technical ideas.\nS.J., Y.J., and W.W. developed the proposed method. S.J., Y.J., Z.N., and X.L. performed\nanalysis. X.J., X.L., X.Z., X.X., Q.Z., and Z.N. provided evaluation and suggestions. All\nauthors contributed to the manuscript.\nCompeting interests\nY.J., X.J., X.X., W.W., and Z.N. were employees at MindRank AI Ltd. The other authors\nhave no conﬂicts of interest.\nAdditional information\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s42004-023-00857-x.\nCorrespondence and requests for materials should be addressed to Guang Yang or\nZhangming Niu.\nPeer review informationCommunications Chemistrythanks Aivett Bilbao and the other,\nanonymous, reviewers for their contribution to the peer review of this work. Peer\nreviewer reports are available.\nReprints and permission informationis available athttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2023\nCOMMUNICATIONS CHEMISTRY | https://doi.org/10.1038/s42004-023-00857-x ARTICLE\nCOMMUNICATIONS CHEMISTRY|            (2023) 6:60 | https://doi.org/10.1038/s42004-023-00857-x | www.nature.com/commschem 9",
  "topic": "Molecular graph",
  "concepts": [
    {
      "name": "Molecular graph",
      "score": 0.8863102197647095
    },
    {
      "name": "Pharmacophore",
      "score": 0.8249942064285278
    },
    {
      "name": "Graph",
      "score": 0.5460387468338013
    },
    {
      "name": "Computer science",
      "score": 0.5434134602546692
    },
    {
      "name": "Representation (politics)",
      "score": 0.49033570289611816
    },
    {
      "name": "Molecular descriptor",
      "score": 0.4399510622024536
    },
    {
      "name": "Biological system",
      "score": 0.38628244400024414
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3859119713306427
    },
    {
      "name": "Chemistry",
      "score": 0.33497896790504456
    },
    {
      "name": "Quantitative structure–activity relationship",
      "score": 0.32664954662323
    },
    {
      "name": "Machine learning",
      "score": 0.23678910732269287
    },
    {
      "name": "Stereochemistry",
      "score": 0.11472123861312866
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}