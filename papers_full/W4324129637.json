{
  "title": "Evaluating the use of large language model in identifying top research questions in gastroenterology",
  "url": "https://openalex.org/W4324129637",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1993841359",
      "name": "Adi Lahat",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A1774138492",
      "name": "Eyal Shachar",
      "affiliations": [
        "Tel Aviv University",
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A165825390",
      "name": "Benjamin Avidan",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A3008927869",
      "name": "Zina Shatz",
      "affiliations": [
        "Tel Aviv University",
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A1498187152",
      "name": "Benjamin S Glicksberg",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1945837108",
      "name": "Eyal Klang",
      "affiliations": [
        "Tel Aviv University",
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A1993841359",
      "name": "Adi Lahat",
      "affiliations": [
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A1774138492",
      "name": "Eyal Shachar",
      "affiliations": [
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A165825390",
      "name": "Benjamin Avidan",
      "affiliations": [
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A3008927869",
      "name": "Zina Shatz",
      "affiliations": [
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A1498187152",
      "name": "Benjamin S Glicksberg",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1945837108",
      "name": "Eyal Klang",
      "affiliations": [
        "Sheba Medical Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3028484854",
    "https://openalex.org/W2997283613",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4226053975",
    "https://openalex.org/W2327037637",
    "https://openalex.org/W4225426732",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W6600466347",
    "https://openalex.org/W4297435087",
    "https://openalex.org/W4311824882",
    "https://openalex.org/W4315784554",
    "https://openalex.org/W4220658139"
  ],
  "abstract": "Abstract The field of gastroenterology (GI) is constantly evolving. It is essential to pinpoint the most pressing and important research questions. To evaluate the potential of chatGPT for identifying research priorities in GI and provide a starting point for further investigation. We queried chatGPT on four key topics in GI: inflammatory bowel disease, microbiome, Artificial Intelligence in GI, and advanced endoscopy in GI. A panel of experienced gastroenterologists separately reviewed and rated the generated research questions on a scale of 1–5, with 5 being the most important and relevant to current research in GI. chatGPT generated relevant and clear research questions. Yet, the questions were not considered original by the panel of gastroenterologists. On average, the questions were rated 3.6 ± 1.4, with inter-rater reliability ranging from 0.80 to 0.98 ( p &lt; 0.001). The mean grades for relevance, clarity, specificity, and originality were 4.9 ± 0.1, 4.6 ± 0.4, 3.1 ± 0.2, 1.5 ± 0.4, respectively. Our study suggests that Large Language Models (LLMs) may be a useful tool for identifying research priorities in the field of GI, but more work is needed to improve the novelty of the generated research questions.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:4164  | https://doi.org/10.1038/s41598-023-31412-2\nwww.nature.com/scientificreports\nEvaluating the use of large \nlanguage model in identifying \ntop research questions \nin gastroenterology\nAdi Lahat 1*, Eyal Shachar 1, Benjamin Avidan 1, Zina Shatz 1, Benjamin S. Glicksberg 2 & \nEyal Klang 3\nThe field of gastroenterology (GI) is constantly evolving. It is essential to pinpoint the most pressing \nand important research questions. To evaluate the potential of chatGPT for identifying research \npriorities in GI and provide a starting point for further investigation. We queried chatGPT on four \nkey topics in GI: inflammatory bowel disease, microbiome, Artificial Intelligence in GI, and advanced \nendoscopy in GI. A panel of experienced gastroenterologists separately reviewed and rated the \ngenerated research questions on a scale of 1–5, with 5 being the most important and relevant to \ncurrent research in GI. chatGPT generated relevant and clear research questions. Yet, the questions \nwere not considered original by the panel of gastroenterologists. On average, the questions were \nrated 3.6 ± 1.4, with inter-rater reliability ranging from 0.80 to 0.98 (p  < 0.001). The mean grades for \nrelevance, clarity, specificity, and originality were 4.9 ± 0.1, 4.6 ± 0.4, 3.1 ± 0.2, 1.5 ± 0.4, respectively. \nOur study suggests that Large Language Models (LLMs) may be a useful tool for identifying research \npriorities in the field of GI, but more work is needed to improve the novelty of the generated research \nquestions.\nThe field of gastroenterology (GI) is constantly evolving, with new advances in technology and research offering \ninsights into the diagnosis and treatment of GI  conditions1. In order to continue pushing the field forward, it is \nessential to identify the most important research questions that require further investigation.\nTraditionally, the identification of research priorities in GI has relied on expert opinion and consensus-\nbuilding among researchers and clinicians. However, this approach may not always capture the full range of \npotential research questions.\nIn recent years, the use of natural language processing (NLP) techniques has gained popularity as a means \nof identifying research priorities. In particular, large language models (LLMs), such as chatGPT, that are trained \non vast amounts of text data have shown promise in suggesting research questions based on their ability to \nunderstand human-like  language2,3.\nPrevious publications evaluating large language models in various other fields of research included for exam-\nple the evaluation of the commonsense ability of GPT, BERT, XLNet, and  RoBERTa4 with promising results, \nevaluation of CODEX, GPT-3 and GPT-J for code generation  capabilities5,evaluation of three approaches to \npersonalizing a language  model6, and evaluating the text to Structured Query Language (SQL) capabilities of \n CODEX7.\nIn this paper, we evaluate the use of newly-released chatGPT in identifying top research questions in the field \nof GI. We focus on four key areas: inflammatory bowel disease (IBD), the microbiome, AI in GI, and advanced \nendoscopy in GI. We prompted the model to generate a list of research questions for each topic. These ques-\ntions were then reviewed and rated by experienced gastroenterologists to assess their relevance and importance.\nWe aimed to evaluate the potential of chatGPT as a tool for identifying important research questions in the \nfield of GI. By utilizing the latest advances in NLP , we hope to shed light on the most pressing and important \nresearch questions in the field, and to contribute to the continued advancement of GI research.\nOPEN\n1Department of Gastroenterology, Chaim Sheba Medical Center, Affiliated to Tel Aviv University, Tel Aviv, \nIsrael. 2Hasso Plattner Institute for Digital Health, Icahn School of Medicine at Mount Sinai, New York, NY, \nUSA. 3The Sami Sagol AI Hub, ARC Innovation Center, Chaim Sheba Medical Center, Affiliated to Tel-Aviv \nUniversity, Tel Aviv, Israel. *email: zokadi@gmail.com\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:4164  | https://doi.org/10.1038/s41598-023-31412-2\nwww.nature.com/scientificreports/\nMethods\nThe study was conducted by utilizing chatGPT, version released on December 15, a recently introduced LLM \n(Nov 2022). The model was trained by OpenAI 2, chatGPT was queried on four key topics in GI: inflammatory \nbowel disease (IBD), microbiome, AI in GI, and advanced endoscopy in GI and was requested to identify the \nmost relevant research questions in each topic.\nA total of 5 research questions were generated for each topic, resulting in a total of 20 research questions. \nThese questions were then reviewed and rated separately by a panel of experienced gastroenterologists with \nexpertise in the respective topic areas. The panel consisted of 3 gastroenterologists, two of them with over \n20 years of experience, and one with over 30 years of experience. All gastroenterologists work in an academic \ntertiary medical center and are the authors of dozens of academic research publications in Gastroenterology, \nand together cover most sub-specialized in Gastroenterology: IBD experts, motility, nutrition, and advanced \nendoscopy. Research key topics were selected in a consensus between all Gastroenterologists and two AI experts.\nChatGPT was prompted with four key topics related to the field of gastrointestinal research. For each topic, \na new thread was started in order to eliminate any potential bias from previous conversations and to ensure that \nthe generated responses were directly related to the current prompt. The four topics were framed as research \nquestions, and were carefully crafted to elicit relevant information about the most important questions in the \nfour chosen topics of gastrointestinal research. Supplementary Table 1 presents the prompts used to generate \nthe research questions in each topic.\nThe gastroenterologists were asked to rate each research question on a scale of 1–5, with 5 being the most \nimportant and relevant to current research in the field of GI. The mean rating ± SD for each research question \nwas calculated. Each question was graded according to 4 parameters: relevance originality, clarity and specificity.\nTo determine inter-rater reliability, we used the intraclass correlation coefficient (ICC) 8 (see statistical \nanalysis).\nAll data were collected and analyzed using standard statistical methods. The research questions generated \nby chatGPT were compared to the current research questions being addressed in the field of GI, as identified \nthrough a comprehensive review of the literature. This allowed for an assessment of the novelty and relevance \nof the questions generated by chatGPT.\nStatistical analysis. In this study, the mean, standard deviation, and median were utilized to describe the \ndata. The Intra-Class analysis (Two-Mixed model, Absolute Agreement) or the Intraclass Correlation Coeffi-\ncient (inter-rater agreement) were employed to assess the data. To determine the significance of the difference \nin grades among the four research topics (each with 20 questions), a Wilcoxon test for non-parametric paired \nsamples was conducted. All calculations were performed using IBM SPSS Statistical Package version 28.\nTo assess the reliability of the rating process, the ICC was calculated. The ICC was selected as the type of reli-\nability estimate, with the ratings made by each of the three observers being treated as separate items. The ICC \nvalue was interpreted as follows: a value of 0 indicated no agreement among the ratings, a value of 1 indicated \nperfect agreement, and values between 0 and 1 indicated some degree of agreement, with higher values indi -\ncating greater agreement. Additionally, the mean ratings and standard deviations of the three observers were \ncompared using the SPSS Explore function, and the correlations among the ratings made by the three observers \nwere examined.\nResults\nA diverse range of research questions was generated by the chat GPT. A panel of 3 expert gastroenterologists \nevaluated the created questions. All questions suggested by the chatGPT on the topics of IBD, microbiome, AI, \nand advanced endoscopy and their ratings by the expert gastroenterologists are shown in Supplementary Table 2.\nIn order to establish the validity of the expert ratings in this study, we first assessed the inter-rater agreement \namong the evaluators. To eliminate the potential confounding influence of intraclass variability, we employed a \nTwo-Mixed Model with random people effects and fixed measures effects to compute the Correlation Coefficient \n(ICC) among the raters. The ICC values obtained in this analysis ranged from 0.8 to 0.98 and were statistically \nsignificant (p < 0.001), indicating a high level of reliability in the expert ratings. This strong agreement among \nthe raters suggests that their assessments can be considered reliable indicators of expert opinion.\nAgreement among the experts according to topics is shown in Table 1.\nThe results of the expert evaluation showed that chatGPT was able to generate research questions that were \nmost relevant to the field of IBD, with the majority of questions receiving a relevance rating of 5—the highest \nTable 1.  Correlation coefficient among experts.\nSubject Intraclass correlation\n95% Confidence interval\nSigLower bound Upper bound\nAdvanced endoscopy 0.928 0.837 0.970 < .001\nAI 0.949 0.893 0.978 < .001\nIBD 0.979 0.953 0.992 < .001\nMicrobiome 0.981 0.959 0.992 < .001\nOverall 0.961 0.941 0.975 < .001\n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:4164  | https://doi.org/10.1038/s41598-023-31412-2\nwww.nature.com/scientificreports/\nrate, and a mean grade of 4.9 ± 0.26. In terms of clarity, chatGPT performed very well, with most questions \nreceiving a rating of 4 or 5, and a mean grade of 4.8 ± 0.41. For specificity, the chatGPT reached a mean grade \nof 2.86 ± 0.64—a moderately good result. However, for originality, all grades were very low—with a mean of \n1.07 ± 0.26.\nWhen assessing microbiome-related topics, results were similar to those achieved for IBD.\nAs in IBD, grades reached almost the maximum for relevance and clarity, and the minimum for originality. \nQuestion 1 was even identical for both topics.\nThe mean ± SD for relevance originality, clarity, and specificity were: 4.93 ± 0.26, 1.13 ± 0.35, 4.93 ± 0.26, and \n3.13 ± 0.64, respectively.\nResults for AI and advanced endoscopy show the same trend- high relevance and clarity, good specificity \nbut the lowest originality. Mean results for AI concerning all the above measures are 5 ± 0, 4.33 ± 0.89, 3.2 ± 0.67 \nand 1.87 ± 0.99, respectively.\nThe mean results for advanced endoscopy for relevance, clarity, specificity, and originality were: 4. ± 0.89, \n4.47 ± 0.74, 3.2 ± 0.77 and 1.73 ± 1.03, respectively.\nAs shown in Table 2, the same trend was continuous in the mean and median grades across all topics, with \nhigh grades for relevance and clarity, good for specificity and very low for originality.\nFigure 1 illustrates the level of inter-rater agreement and the mean grades in all categories for every topic. \nWhen the curves representing the ratings of different evaluators are closer together within the circle, it indicates \na higher level of agreement among them. The further the curve is from the outer edge of the diagram, the higher \nthe grades given by the evaluators. The monotonic nature of the curves suggests that the raters are consistent \nbetween their assessments.\nIn general, ChatGPT demonstrated excellent results in terms of Clarity and Relevance, satisfactory perfor -\nmance in terms of Specificity, but inadequate performance in terms of Originality. Figure  2 presents the mean \nscores for all readers for each category and each research topic.\nDiscussion\nWhen evaluating chatGPT for generating important research questions in IBD, microbiome, AI in gastroenter-\nology, and advanced endoscopy in gastroenterology, we found that the model has the potential to be a valuable \ntool for generating high-quality research questions in these topics. In all the examined topics, chatGPT was able \nto produce a range of relevant, clear, and specific research questions, as evaluated by a panel of expert gastro-\nenterologists. However, none of the questions was original. In fact, in terms of originality, the chatGPT showed \npoor performance.\nOverall, the results of our evaluation show that chatGPT has the potential to be a valuable resource for \nresearchers. Its ability to generate a diverse range of high-quality research questions can help to advance the \nfield by providing researchers with novel ideas for investigation. However, further research and development are \nneeded to enhance chatGPT’s ability in terms of originality.\nThe results of the work reflect the general ability of ChatGPT to produce any type of text. Similar properties \nof clarity and relevance were part of the reward model of ChatGPT’s original training, in which humans rated \nseveral outputs of the model according to their preferences. Thus, the model is able to produce outputs that are \nalso rated as clear and relevant by other human raters. The limitation of the originality of the results is mentioned \nfirst on ChatGPT´s  homepage2, and is further emphasized in our current study.\nOne potential area for future research is to explore the use of chatGPT in conjunction with other natural \nlanguage processing techniques, such as topic  modeling9, to identify relevant research areas and generate more \nfocused and specific research questions. Additionally, further studies could investigate the use of chatGPT in \nother subfields of gastroenterology, such as hepatology and gastrointestinal surgery, to assess its potential. Fur-\nthermore, we believe chatGPT can be relevant to many other fields of medical research.Importantly, the original-\nity of the research topic received very low scores. This result highlights a key disadvantage of the large language \nmodels: NLP models are trained on a vast amount of text data and are able to generate responses based on these \n data10–12. While they are able to provide accurate and informative responses to a wide range of questions, these \nresponses are not original or unique in the sense that they are not generated from their own experiences or \ninsights. Instead, they are based on preexisting information and language patterns that the NLP models have \nlearned from the data they were trained on. As a result, the responses generated by a language model are often \nnot regarded as original ideas or insights.\nIn this study we measured the intraclass correlation coefficient (ICC) between 3 experienced gastroenterolo-\ngists, in order to evaluate the consistency and reliability of the questions’ assessments. A high ICC indicates that \nthe observations made by different observers are highly consistent, which suggests that the results of the study \nare accurate.\nTable 2.  Mean and median grades for all topics across all parameters.\nClarity Originality Relevance Specificity\nN 20 20 20 20\nMean 4.6 1.5 4.9 3.1\nMedian 4.7 1.2 5.0 3.3\nSD 0.51 0.72 0.38 0.48\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:4164  | https://doi.org/10.1038/s41598-023-31412-2\nwww.nature.com/scientificreports/\nDespite the promising results of this study, there are limitations that should be considered when interpreting \nthe findings. First, the expert panel that generated research questions consisted of only three gastroenterologists \nand two AI experts, and the panel that evaluated the questions consisted of three gastroenterologists. Though \nhighly experienced, the results may not be representative of the broader community of researchers in these fields. \nNevertheless, the results are solidified by the high degree of inter-observer agreement, which underscores the \nvalidity of the conclusions reached.\nFigure 1.  Level of inter-rater agreement and the mean grades in all categories for every topic. The figure \nillustrates the level of inter-rater agreement and the mean grades in all categories for every topic. When the \ncurves representing the ratings of different evaluators are closer together within the circle, it indicates a higher \nlevel of agreement among them. The further the curve is from the outer edge of the diagram, the higher the \ngrades given by the evaluators. The monotonic nature of the curves suggests that the raters are consistent \nbetween their assessments.\n0\n1\n2\n3\n4\n5\n6\nRelevanceO riginality ClarityS peciﬁcity\nAI IBD Microbiome Advanced Endoscopy\nFigure 2.  Mean scores for each research topic and category, as rated by all readers.\n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:4164  | https://doi.org/10.1038/s41598-023-31412-2\nwww.nature.com/scientificreports/\nFurther studies with larger and more diverse panels of experts would be needed to confirm the generaliz -\nability of these results.\nSecond, the evaluation of chatGPT’s performance was based on subjective ratings by the expert panel, which \nmay be subject to bias and variability. Objective measures, such as the citation frequency or impact factor of \ncurrent academic papers focusing on the same topics of the research questions generated by chatGPT, would \nprovide a more robust assessment of its performance.\nHowever, research questions often involve complex issues that cannot be easily quantified, such as the rel-\nevance of a question or the originality of a question in the existing literature.\nTherefore, subjective judgment is an essential component of the evaluation of research questions and helps \nto ensure that the questions are relevant, clear, feasible, original, evidence-based, and valid, taking into account \nthe complex and context-specific nature of research questions.\nFurthermore, the quality of a research question can also be influenced by human values, such as ethical con-\nsiderations, societal impact, and personal beliefs. These values cannot be easily quantified, and are best evaluated \nthrough subjective judgment.\nThird, this study focused on the performance of chatGPT in generating research questions in specific sub -\nfields of gastroenterology, but did not investigate its potential for generating research questions in other areas of \nmedicine or science. Further research is needed to evaluate chatGPT’s performance in a wider range of domains.\nFourth, we used a single set of prompts for each of the four research topics to generate the research ques-\ntions. Given that ChatGPT is sensitive to tweaks in the input, more experiments with different prompts would \nhave been valuable in order to fully evaluate the potential of ChatGPT to generate diverse research questions. \nAdditionally, we only used one instance of ChatGPT, and it is possible that the results could have been different \nwith another instance of the model or a different language model. Further research is needed to determine the \ngeneralizability of our results to other models and contexts.\nIt is noteworthy that the text summarization capabilities of GPT-3 were recently evaluated and displayed \nimpressive results utilizing traditional  benchmarks13. Currently, as the utilization of the chatbot GPT is rapidly \nincreasing, a vast amount of data is accumulating at a rapid pace regarding its various  capabilities14,15.\nIn conclusion, our evaluation of chatGPT as a research idea creator for four key topics in gastroenterology—\ninflammatory bowel disease, microbiome, AI in gastroenterology, and advanced endoscopy—showed promising \nresults. ChatGPT was able to generate high-quality research questions in these fields, demonstrating its potential \nas a valuable tool for advancing the field of gastroenterology. While further research and development is needed \nto enhance chatGPT’s performance in terms of relevance and originality, its ability to generate a diverse range of \nclear and specific research questions has the potential to significantly contribute to the advancement of gastroen-\nterology. Overall, chatGPT has the potential to be a valuable tool for researchers in the field of gastroenterology \nspecifically and in other medical fields in general, and we believe it is worth further exploration and development.\nData availability\nThe authors declare that there is no relevant data available for this study. All data used in the analysis and prepa-\nration of this manuscript have been included in the manuscript.\nReceived: 5 January 2023; Accepted: 11 March 2023\nReferences\n 1. Klang, E., Soffer, S., Tsur, A., Shachar, E. & Lahat, A. Innovation in gastroenterology—Can we do better?. Biomimetics (Basel) 7(1), \n33. https:// doi. org/ 10. 3390/ biomi metic s7010 033. PMID: 35323 190; PMCID: PMC89 45015 (2022).\n 2. About OpenAI. Retrieved from https:// openai. com/ about/\n 3. Milne-Ives, M. et al. The effectiveness of artificial intelligence conversational agents in health care: Systematic review. J. Med. \nInternet Res. 22(10), e20346. https:// doi. org/ 10. 2196/ 20346 (2020).\n 4. Zhou, X., Zhang, Y ., Cui, L. & Huang, D. Evaluating commonsense in pre-trained language models. ArXiv. https:// doi. org/ 10. \n48550/ arXiv. 1911. 11931 (2019).\n 5. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P ., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G., Ray, A., Puri, \nR., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P ., Chan, B., Gray, S. & Zaremba, W . Evaluating large language models \ntrained on code. ArXiv. https:// doi. org/ 10. 48550/ arXiv. 2107. 03374 (2021).\n 6. King, M. & Cook, P . Evaluating approaches to personalizing language models. In Proceedings of the 12th Language Resources and \nEvaluation Conference 2461–2469 (2020).\n 7. Rajkumar, N., Li, R. & Bahdanau, D. Evaluating the text-to-SQL capabilities of large language models. ArXiv. https://  doi. org/ 10. \n48550/ arXiv. 2204. 00498 (2022).\n 8. Koo, T. K. & Li, M. Y . A guideline of selecting and reporting intraclass correlation coefficients for reliability research. J. Chiropr. \nMed. 15(2), 155–163. https:// doi. org/ 10. 1016/j. jcm. 2016. 02. 012 (2016) (Erratum in: J Chiropr Med. 2017 Dec;16(4):346).\n 9. Rijcken, E. et al. Topic modeling for interpretable text classification from EHRs. Front. Big Data 5, 846930. https:// doi. org/ 10. 3389/ \nfdata. 2022. 846930 (2022).\n 10. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N. & Polosukhin, I. Attention is all you need. In Advances \nin Neural Information Processing Systems 5998–6008 (2017).\n 11. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P . & Neelakantan, A. Language models are few-shot learners. \narXiv preprint http:// arxiv. org/ abs/ 2005. 14165 (2020).\n 12. Melis, G., Dyer, C., & Blunsom, P . On the state of the art of evaluation in neural language models. arXiv preprint http:// arxiv. org/ \nabs/ 1707. 05589 (2017).\n 13. Goyal, T., Li, J. J. & Durrett, G. News summarization and evaluation in the era of GPT-3. ArXiv. https://  doi. org/ 10. 48550/ arXiv. \n2209. 12356. (2022).\n 14. Castelvecchi, D. Are ChatGPT and AlphaCode going to replace programmers?. Nature https:// doi. org/ 10. 1038/ d41586- 022- 04383-z \n(2022).\n 15. Else, H. Abstracts written by ChatGPT fool scientists. Nature 613(7944), 423 (2023).\n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:4164  | https://doi.org/10.1038/s41598-023-31412-2\nwww.nature.com/scientificreports/\nAuthor contributions\nA.L. and E.K. designed and performed the research and wrote the paper; E.S., B.A., Z.S., and B.G. contributed \nto the analysis and revised the paper critically. All authors approved the version to be published.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 023- 31412-2.\nCorrespondence and requests for materials should be addressed to A.L.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.5349207520484924
    },
    {
      "name": "Relevance (law)",
      "score": 0.4901345670223236
    },
    {
      "name": "CLARITY",
      "score": 0.46471962332725525
    },
    {
      "name": "Gastroenterology",
      "score": 0.4582776725292206
    },
    {
      "name": "Novelty",
      "score": 0.440022736787796
    },
    {
      "name": "Medical education",
      "score": 0.3467799723148346
    },
    {
      "name": "Internal medicine",
      "score": 0.3316606879234314
    },
    {
      "name": "Psychology",
      "score": 0.23463866114616394
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}