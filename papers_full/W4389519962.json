{
  "title": "R3 Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context",
  "url": "https://openalex.org/W4389519962",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2146572958",
      "name": "Qingyuan Tian",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2943066805",
      "name": "Hanlun Zhu",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A120759433",
      "name": "Lei Wang",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A2079581369",
      "name": "Yang Li",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2955016064",
      "name": "Yunshi Lan",
      "affiliations": [
        "East China Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281483047",
    "https://openalex.org/W145832685",
    "https://openalex.org/W4296415404",
    "https://openalex.org/W4366565380",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4385573261",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2251935656",
    "https://openalex.org/W2276364082",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2105717194",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4375957723",
    "https://openalex.org/W4312052651",
    "https://openalex.org/W3198599617",
    "https://openalex.org/W4319049323",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W3138392969",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4281975731",
    "https://openalex.org/W4224308101"
  ],
  "abstract": "With the help of Chain-of-Thought (CoT) prompting, Large Language Models (LLMs) have achieved remarkable performance on various reasoning tasks. However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated. Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction. Inspired by interactive CoT method, where intermediate reasoning steps are promoted by multiple rounds of interaction between users and LLMs, we propose a novel prompting method, namely R3 prompting, for CoT reasoning under noisy context. Specifically, R3 prompting interacts with LLMs to perform key sentence extraction, variable declaration and answer prediction, which corresponds to a thought process of reviewing, rephrasing and resolving. The responses generated at the last interaction will perform as hints to guide toward the responses of the next interaction. Our experiments show that R3 prompting significantly outperforms existing CoT prompting methods on five reasoning tasks under noisy context. With GPT-3.5-turbo, we observe 3.7% accuracy improvement on average on the reasoning tasks under noisy context compared to the most competitive prompting baseline. More analyses and ablation studies show the robustness and generalization of R3 prompting method in solving reasoning tasks in LLMs under noisy context.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1670–1685\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nR3 Prompting: Review, Rephrase and Resolve for Chain-of-Thought\nReasoning in Large Language Models under Noisy Context\nQingyuan Tian1, Hanlun Zhu1, Lei Wang2, Yang Li3, Yunshi Lan1∗\n1East China Normal University, Shanghai, China\n2Singapore Management University, Singapore\n3Alibaba Group, Beijing, China\n{qytian, hlzhu}@stu.ecnu.edu.cn, yslan@dase.ecnu.edu.cn\nlei.wang.2019@phdcs.smu.edu.sg\nly200170@alibaba-inc.com\nAbstract\nWith the help of Chain-of-Thought (CoT)\nprompting, Large Language Models (LLMs)\nhave achieved remarkable performance on var-\nious reasoning tasks. However, most of them\nhave been evaluated under noise-free context\nand the dilemma for LLMs to produce inac-\ncurate results under the noisy context has not\nbeen fully investigated. Existing studies utilize\ntrigger sentences to encourage LLMs to concen-\ntrate on the relevant information but the trigger\nhas limited effect on final answer prediction. In-\nspired by interactive CoT method, where inter-\nmediate reasoning steps are promoted by mul-\ntiple rounds of interaction between users and\nLLMs, we propose a novel prompting method,\nnamely R3 prompting, for CoT reasoning un-\nder noisy context. Specifically, R3 prompting\ninteracts with LLMs to perform key sentence\nextraction, variable declaration and answer pre-\ndiction, which corresponds to a thought pro-\ncess of reviewing, rephrasing and resolving.\nThe responses generated at the last interaction\nwill perform as hints to guide toward the re-\nsponses of the next interaction. Our exper-\niments show that R 3 prompting significantly\noutperforms existing CoT prompting methods\non five reasoning tasks under noisy context.\nWith GPT-3.5-turbo, we observe 3.7% accu-\nracy improvement on average on the reasoning\ntasks under noisy context compared to the most\ncompetitive prompting baseline. More analy-\nses and ablation studies show the robustness\nand generalization of R3 prompting method in\nsolving reasoning tasks in LLMs under noisy\ncontext.\n1 Introduction\nRecent advances in Large Language Mod-\nels (LLMs) like GPT-3 (Brown et al., 2020),\nPaLM (Chowdhery et al., 2022), OPT (Zhang et al.,\n2022a), and LLaMa (Touvron et al., 2023) have\nrevolutionized the landscape of natural language\n∗Corresponding author\nprocessing. A series of studies have proved that\nmulti-step reasoning tasks can be easily solved by\nLLMs via different prompting methods (Kojima\net al., 2022; Wei et al., 2023; Wang et al., 2023;\nZhang et al., 2022b; Zhou et al., 2022; Zheng et al.,\n2023; Shi et al., 2023). Chain-of-thought (CoT),\nas the representative prompting approach, narrows\nthe gap between human intelligence and machine\nintelligence by applying the rationales to few-shot\nprompting (Zhou et al., 2022). It has achieved im-\npressive results on arithmetical reasoning tasks (Ko-\njima et al., 2022; Wei et al., 2023; Wang et al., 2023;\nZhang et al., 2022b; Zhou et al., 2022; Zheng et al.,\n2023; Shi et al., 2023).\nHowever, most CoT approaches are investigated\nunder a noisy-free context, where all the informa-\ntion provided in the problem of interest is relevant\nto the final answer. While in real-world scenarios,\nit is frequent to encounter problems with irrele-\nvant information. Shi et al. (2023) first defined the\nreasoning tasks in LLMs under noisy context. As\nshown in Figure 1, the noisy context in the problem\nof interest includes “Helen baked12 berry cookies\nyesterday”, which can easily distract LLMs. They\nfurther proposed Instructed-CoT prompt, which\nincludes a trigger sentence instructing LLMs to ig-\nnore the irrelevant information given in a question\nahead of one-turn response exemplar. Instructed-\nCoT prompt is intuitive to denoise context but it\nhas limited effect on the final answer prediction.\nInspired by prior studies (Zhou et al., 2022), in-\nteractive CoT prompting approaches show that con-\nducting multi-round of interaction between users\nand LLMs as hints is efficient in progressively\nguiding toward the correct answers for complex\nreasoning tasks (Zheng et al., 2023; Zhou et al.,\n2022). Least-to-Most (Zhou et al., 2022; Zheng\net al., 2023) is a CoT prompting method that guides\nLLMs to decompose a complex problem into sim-\nple sub-questions. The prompt first decomposes the\nproblem into subproblems, then the generated inter-\n1670\nFigure 1: Comparison between R3 prompting and existing CoT prompting baseline methods. The exemplar problems\nare multiple problems we used as exemplars for in-context learning. Rationales are reasoning chains in prompts.\nThe problem of interest is the query problem.\nmediate answer is appended to the next subproblem\nuntil the final answer is returned. PHP (Zheng et al.,\n2023) is another prompting method with interactive\nresponses that combine the candidate answers and\nproblems for re-evaluation purposes, which allows\ndouble-check to the answer prediction. Neverthe-\nless, these interactive CoT prompting methods are\nstill vulnerable to noisy context.\nIn this paper, we propose a new method named\nR3 prompting, that includes sequentially interac-\ntion with LLMs to gradually approach final answers\nvia a thought process of Reviewing, Rephras-\ning and Resolving. In the review stage, a review\nprompt is designed to extract key sentences that are\nessential conditions required for the final answer\nprediction. In the rephrase stage, we guide LLMs\nto reformulate the problem narratives to variables\nwith the hint of extracted key sentences. In the\nresolve stage, LLMs predict the final answers tak-\ning account of the generated variables. All stages\nare implemented under few-shot setting with show-\ning the responses of several exemplar problems. A\ncomparison between the existing prompting meth-\nods is displayed in Figure 1.\nWe evaluate our prompting method on five dif-\nferent benchmarks containing misleading and ir-\nrelevant context, that are AddSub (Hosseini et al.,\n2014), SV AMP (Patel et al., 2021), MultiArith-\nIC, SingleEq-IC, and GSM-IC (Shi et al., 2023).\nThe last three datasets are manually created by in-\nserting noise to MultiArith (Roy and Roth, 2016),\nSingleEq (Koncel-Kedziorski et al., 2015) and\nGSM8K(Shi et al., 2023). We follow Shi et al.\n(2023) to create MultiArith-IC and SingleEq-IC by\nrandomly inserting irrelevant sentences into prob-\nlem descriptions from MultiArith and SingleEq,\nrespectively. Specifically, we follow their template-\nbased method to create templates for noisy context\nand instantiate with role names in the problems.\nFrom the experimental results, we observe that in-\nteractive CoT has some advantages in solving noisy\nproblems and our R3 prompting method could out-\nperform a series of CoT prompting methods for rea-\nsoning tasks in LLMs under noisy context, which\nleads to an average improvement of 3.7 absolute\npoints on accuracy. More analyses shed lights on\nthe robustness and generalization of R3 prompting.\n2 Related Work\nChain-of-thought prompting. CoT prompt-\ning (Wei et al., 2023) is a technique that guides\nLLMs to conduct multi-step reasoning which can\nbe logically sequential without gradient accumu-\nlation. CoT has proved to be effective in solving\nreasoning tasks in few-shot scenarios. There are\ntwo major paradigms for CoT. One is to leverage\na simple instruction sentence to elicit LLMs to\ndo step-by-step thinking before answering a ques-\ntion (Kojima et al., 2022; Wang et al., 2023) or\nprovide a few manual demonstrations with reason-\ning chains to show LLMs how to solve the ex-\nemplar problems (Wei et al., 2023; Zhang et al.,\n2022b). This paradigm is designed with one-turn\nresponse. Another paradigm of CoT has an inter-\nactive framework (Zheng et al., 2023; Zhou et al.,\n2022; Creswell et al., 2022; Yao et al., 2022), which\nenables automatic multiple rounds of interactions\nbetween users and LLMs by appending previously\n1671\nFigure 2: A running example of the inputs and outputs of R3 prompting in LLMs at each prompting stage. Green:\nIn-topic noisy context. Red: Off-topic noisy context. Blue: Key sentences.\ngenerated. For example, Least-to-Most prompt-\ning (Zhou et al., 2022) interacts with LLMs via\nshowing the decomposed sub-questions and cor-\nresponding solutions obtained from the last inter-\naction, which enables complex reasoning. PHP\nprompting (Zheng et al., 2023) attaches the can-\ndidate answers generated from the last interaction\nto ask LLMs to double-check the answers. Our\nproposed method follows the interactive paradigm\nbut is invested with a novel interactive process of\nthinking.\nPrompting with noisy ground truth. A few stud-\nies have investigated the effect of prompts with\nnoisy ground truth, including pairing exemplar\nproblems with incorrect answers (Min et al., 2022;\nKim et al., 2022) and adding irrelevant or mislead-\ning context into exemplars (Webson and Pavlick,\n2021; Shi et al., 2023). In particular, Madaan and\nYazdanbakhsh (2022) proved that the correctness\nof math equations in exemplars does not notably\ninfluence model’s performance, whereas the noise\nincluded in the text could hamper the performance\ndramatically. Similar observation has been dis-\ncussed in another work (Zhang et al., 2022b). To\nsolve the reasoning task under noisy context, Shi\net al. (2023) introduced an intuitive trigger sen-\ntence in prompts to decrease the influence from\nthe noisy context to LLMs. Different from the\nsimple instruction for denoising in their prompts,\nour work focuses on designing a interactive CoT\nprompt that can steadily solve problems without\nbeing misguided by noisy context.\n3 R 3 Prompting\nResearches on human problem-solving indicate\nthat when solving a reasoning task under noisy\ncontexts, the instinct of humanity is to conduct\nmultiple rounds of thinking (Newell et al., 1959).\nIn this paper, we propose that this process can be\nsimulated in LLMs by reviewing, rephrasing, and\nresolving successively. Specifically, we design a\nprompting method with three-stage prompts: (1)\nReview prompt: instruct LLMs to read the ques-\ntion and extract the key sentences from the noisy\ncontexts. (2) Rephrase prompt: with the hint\nof extracted key sentences, reformulate the narra-\ntives into abstractive representations with explicit\nvariables. (3) Resolve prompt: with the hint of\nexplicit variables, solve the question and predict\nthe final answers. Figure 2 illustrates the proposed\ninteraction and prompt design for each stage. Next,\nwe will introduce the detailed design of prompts in\n1672\neach stage.\n3.1 Review Prompt\nThe goal of this stage is to review the input and dis-\ntinguish the key sentences from the noisy context.\nFollowing the prior studies (Shi et al., 2023), we\nfocus on two common categories of noisy informa-\ntion in contexts: 1) Deceptive in-topic sentences\ndesigned to complex the problem, as \"Josh had 7\nmarbles in his collection\" shown in Figure 2; 2)\nOff-topic sentences mixed to distract the solvers,\nas \"Josh’s father works 8 hours a day\" shown in Fig-\nure 2. To prevent LLMs from being trapped by the\nnoisy information, we provide exemplar problems\nand their expected responses before the problem\nof interest in prompts with the following design\nprinciples:\n• The exemplar problem should include both\ncategories of the above noisy information,\nwhich can elicit comprehensive reviewing\nability of LLMs.\n• The demonstrations should show the expected\nkey sentences to be extracted by LLMs, which\nare the essential conditions needed to reason\nthe answers.\nTo apply the first principle, we synthesize ex-\nemplar problems containing diverse noisy contexts.\nSpecifically, we randomly sample several problems\nfrom the training data which includes in-topic noisy\nsentences. Then we write some templates for off-\ntopic sentences which can be instantiated with dif-\nferent role names and we randomly insert them\ninto the problems. For example, in Figure 2, we\nhave exemplar problems with both types of noise\n“Josh had 7 marbles in his collection” and “Josh’s\nfather works 8 hours a day”, which corresponds to\nin-topic and off-topic noisy sentences, respectively.\nIn order to apply the second principle, we orga-\nnize the format of responses as:\nTo answer this question, we need to\nnotice: 1. [C1] 2. [C2] ...\nwhere [Ci] denotes the i-th key sentence in the\nproblem of interest that LLMs should focus on\nduring reasoning. We arrange them via numerical\nindexes to guide LLMs to explicitly specify their\norder. Furthermore, to ensure the semantic integrity\nbetween the key sentences and the original prob-\nlems, we consider directly extracting textual spans\nfrom the original problems as the key sentences.\nAs shown in Figure 2, “ He lost 8 marbles” and\n“He found10 new ones” are directly taken as the\ninteractive response of the review prompt. These\ntwo sentences are also the essential conditions that\nwe can use to solve the problem.\nSo, in the first round of interaction, given the\nexemplar problems, corresponding responses as\nwell as the problem of interest, LLMs will learn to\nreview a problem under noisy context by focusing\non the key sentences.\n3.2 Rephrase Prompt\nIn the second round of interaction, we ask LLMs to\nrephrase the narratives of the extracted sentences\ninto abstractive representation, which explicates\nthe variables and corresponding numeral values.\nTo this end, we append the extracted key sentences\nbehind the problem of interest to guide LLMs to\ntranslate these sentences into variables.\nWe organize the format of response as\n1. [V1] 2. [V2] ...\nwhere Vi denotes the variables corresponding to Ci.\nIn Figure 2, we have “Number of lost marbles =8”\nand “Number of found marbles =10” as output of\nthe rephrase prompt. As opposed to existing zero-\nshot prompting methods (Wang et al., 2023) where\na simple trigger sentence is included in the prompt\nas the instruction, rephrase prompt instructs LLMs\nby providing well-organized exemplar problems as\ndemonstrations.\n3.3 Resolve Prompt\nThe third round of interaction is to resolve the ques-\ntion and predict the answer based on the given\nvariables. Similar with the previous prompt, we\nappend the variables behind the original question\nand request LLMs to provide final answers and we\norganize the format of response as:\n[Question of Interest] = [Equations\ninvolving V1, V2, ...]. The answer is\n[A].\nwhere the equation shows the reasoning process\nand [A] denotes the final answer. We extract the\narabic numeral as our prediction. In Figure 2, we\nhave “2” as the predicted answer to the question.\nIt is worth noting that even thought we conduct\nmultiple rounds of interaction with LLMs, we in-\nclude the same exemplar problems with different\nresponses for each stage interaction in R3 prompt-\ning.\n1673\nDataset #Sample Ave. in-topic Ave. off-topic\nGSM-IC 1000 0.5 0.5\nMultiArith-IC 600 0.5 0.5\nSingEq-IC 508 0.48 0.52\nTable 1: Details of constructed datasets. “Ave. in-topic”\nand “Ave. off-topic” denotes averge number of in-topic\nsentences and off-topic sentences, respectively.\n4 Experimental Setup\n4.1 Datasets\nTo demonstrate the efficacy of R 3 prompting\nmethod under noisy context, our evaluation is con-\nducted on multiple datasets: (1) AddSub (Hos-\nseini et al., 2014) and (2) SV AMP(Patel et al.,\n2021) are two challenges of arithmetic reason-\ning, where AddSub contains one-unknown arith-\nmetic word problems for up-to-4 grad level students\nand SV AMP covers addition and subtraction arith-\nmetic word problems. The problems of both these\ndatasets involve noisy context that is designed to\nconfuse problem solvers. These noisy sentences\nare often in-topic and highly related to the problem\nbut play no role in solving the question. We further\ninclude (3) GSM-IC, which is a dataset introduced\nby Shi et al. (2023) and created for the investigation\non the destructibility of LLMs. It consists of a sub-\nset of GSM8K dataset (Cobbe et al., 2021) and each\nproblem is interrupted by manually inserting some\nirrelevant in-topic and off-topic sentences that are\ngenerated by templates. (4) MultiArith-IC and\n(5) SingleEq-IC are two datasets we constructed\nfollowing the above procedure (Shi et al., 2023)\nto include more noisy context for each problem.\nThe original problems are extracted from Multi-\nArith (Roy and Roth, 2016) and SingleEq (Koncel-\nKedziorski et al., 2015), respectively. Since most of\nthem are noisy-free, we create templates for noisy\ncontext and instantiate with role names in the prob-\nlems The statistics of those constructed dataset is\nshown in Table 1.\n4.2 Baselines\nWe compare R3 prompting with a set of few-shot\nCoT prompting baseline methods, which can be cat-\negorized into two groups based on the interaction\nwith LLMs: (1) Manual-CoT (Zhang et al., 2022b)\nand Auto-CoT (Zhou et al., 2022) are two prompt-\ning methods that provide one-turn response in the\nprompt to show the reasoning process of exemplars\nto LLMs. Instructed-CoT (Shi et al., 2023) is also\na one-turn response prompting method but it is de-\nsigned for tasks with noisy context by including a\ntrigger sentence. (2) Least-to-Most (Zhou et al.,\n2022) and PHP (Shi et al., 2023) are two prompting\nmethods with interactive responses, which lever-\nage the reasoning paths and candidate answers of\nLLMs to refine reasoning, respectively. We test\ntheir distractibility in the presence of noisy context\nas illustrated in Figure 1.\n4.3 Implementation\nWe employ engines including text-davinci-002,\ntext-davinci-003 and GPT-3.5-Turbo1 for ex-\nperiments due to their strong CoT reasoning abil-\nities (Brown et al., 2020; Kojima et al., 2022;\nOuyang et al., 2022). The main results are obtained\nvia GPT-3.5-Turbo API. For temperature, the tem-\nperature of all interactions of comparable prompt-\ning methods is set to 0 except for PHP prompting.\nFollowing setup in the original paper (Zheng et al.,\n2023), the temperature for the second interaction of\nPHP prompting is set to 0.7 and the others are set\nto 0. Meanwhile, following Wei et al. (2023), the\ntotal number of demonstrations k is set to 8. We\nselect the same problems as exemplars for all the\ncomparable methods to make fair comparison.\n5 Experimental Results\n5.1 Main Results\nWe find that our proposed R3 prompting enhances\nLLM’s reasoning ability, especially when facing\nproblems under noisy context.\nR3 prompting performs well for CoT reason-\ning in LLMs under noisy context. R3 prompting\nconsistently outperforms other few-shot prompting\nmethods across all arithmetic reasoning datasets\nunder noisy context. There is a minimum improve-\nment of 2% in accuracy on AddSub dataset and\nan overall average increase of 3.3% in accuracy.\nAmong them, it achieves the largest performance\ngain on SV AMP dataset, where it improves the sec-\nond best result 83.6% produced by Auto-CoT to\n87.3%. Considering that most of the problems in\nSV AMP contain in-topic noisy sentences, we be-\nlieve the improvement is from the accurate recog-\nnition to these sentences. Even though Instructed-\nCoT is designed with a denoising instruction, it\ndoes not provide explicit demonstrations to LLMs\n1we apply GPT-3.5-Turbo-0301 fromhttps://platform.\nopenai.com/docs/models/gpt-3-5\n1674\nMethods SV AMP MultiArith-IC SingleEq-IC AddSub GSM-IC Average\nOne-turn\nManual-CoT 79.9 79.5 77.7 85.3 81.0 79.7\nAuto-CoT 83.6 79.7 77.6 88.0 81.5 82.1\nInstructed-CoT 81.3 80.1 78.2 87.3 82.0 81.8\nInteractive\nLeast-to-Most 80.8 77.4 76.2 85.3 81.1 80.2\nPHP 83.1 80.0 79.0 85.3 85.1 82.5\nR3 Prompting (Ours) 87.3 82.2 81.5 90.0 88.0 85.8\nTable 2: Main result on five evaluated datasets. The best and second best results are boldfaced and underlined\nrespectively.\nsuch that LLMs struggle to learn what kind of infor-\nmation should be ignored. In contrast, R3 prompt-\ning provides strong supervision on the final answer\nprediction by showing demonstrations (81.8% vs.\n85.8%).\nThe design of interactive prompts are important\nfor denoising. Comparing the overall performance\nof one-turn and interactive prompting methods, we\nnotice there is a moderate advantage of interac-\ntive prompting methods for reasoning under noisy\ncontext. We believe this is because interactive\nCoT prompting methods provide more intensive\nguidance toward the answers for LLMs. Since the\nother interactive CoT prompting methods are not\ndesigned for the purpose of denoising, their im-\nprovement is limited. The three-stage prompts of\nR3 prompting are elaborately designed to stimu-\nlate the human ability of thinking and this enables\nLLMs to learn a more comprehensive and efficient\nproblem-solving skill under noisy context.\nMoreover, to demonstrate the generalizability\nof our method, we also evaluate R3 prompting on\nMultiArith, SingleEq and GSM8K. The results in-\ndicate that our method generally shows good results\nfor reasoning in LLMs under noisy context. The\ncomplete results are provided in the Appendix A.1.\n5.2 Performance with Diverse Settings\nImprovement of R 3 prompting is consistent\nover various engines. We evaluate Manual-\nCoT, Instructed-CoT and R3 prompting with var-\nious engines including text-davinci-002 and\ntext-davinci-003 and display the results in Ta-\nble 4. We observe that R 3 prompting generally\noutperforms Manual-CoT and Instructed-CoT un-\nder noisy context with various engines. Further-\nmore, the performance R3 prompt can achieve bet-\nter results with more powerful engine. On AddSub\ndataset, few-shot R3 prompting method surpasses\nthe SoTA results (Roy and Roth, 2016) which is ob-\ntained by training with full data (94.9% → 95.7%).\nImprovement of R 3 prompting is still signifi-\ncant with self-consistency. We combine prompt-\ning methods with self-consistency strategy (Wang\net al., 2022). We sample answers with numbers\n5, 10, 20, 40 and draw Figure 3 to show the per-\nformance change with the increasing number of\nanswers.\nWe discover that the improvement brought by\nR3 prompting is not conflict with self-consistency.\nFor example, on AddSub, when the sample number\nis 5, R3 prompting (90.0%) outperforms Instructed\nCoT (85.3%) with an improvement of 4.7% abso-\nlute point. When the sample number increases to\n40, this gain does not diminish, R3 prompting still\noutperforms Instructed CoT by4.1%. Similar trend\noccurs on SV AMP dataset. This indicates that the\nimprovement brought by R 3 prompting and self-\nconsistency is orthogonal. They play roles on the\ndifferent aspects of prompting.\n5.3 Ablation Study\nTo demonstrate the significance of all stages em-\nployed in R3 prompting, we display the results of\nablation study in Table 3 by testing different com-\nbinations of prompts.\nStages of reviewing, rephrasing and resolving\nare all important. Combination (1) is similar to\ntraditional one-turn CoT method. Combination (2)\nand (3) yield average accuracy (84.1% and 82.0%)\nrespectively, which are significantly higher than\ncombination (1). This indicates that both review\nand rephrase prompts are important. Among them,\nreview prompt provides a larger performance gain\non average, this indicates that reviewing the prob-\nlem and identifying the key sentences is important\nunder the noisy context. Overall, including all the\nprompts achieves the best results, which demon-\nstrates an improvement of 6.1 absolute points over\ncombination (1).\nThe effect of review and rephrase prompts vary\nover different datasets. We notice that combina-\n1675\nFigure 3: (a). The results of prompting methods after adding Self-\nConsistency (SC) on AddSub dataset. (b). The results of prompting\nmethods after adding Self-Consistency (SC) on SV AMP dataset.\nFigure 4: Accuracy change of various\nmethods with the increasing number\nof irrelevant sentences on AddSub.\nStage AddSub MultiArith-IC SingleEq-IC SV AMP GSM-IC AverageReview Rephrase Resolve\n(1) ✗ ✗ ✓ 85.3 79.5 77.8 79.9 81.0 79.7\n(2) ✓ ✗ ✓ 88.6 81.0 78.9 84.4 83.0 84.1\n(3) ✗ ✓ ✓ 88.3 80.0 79.7 80.7 81.5 82.0\n(4) ✓ ✓ ✓ 90.0 82.2 81.5 87.3 88.0 85.8\nTable 3: Ablation result of evaluation.\nEngine Method AddSub GSM-IC\ntext-davinci-002\nManual-CoT 84.8 55.8\nInstructed-CoT 85.8 58.5\nR3 prompting 87.0 59.6\ntext-davinci-003\nManual-CoT 91.4 72.4\nInstructed-CoT 92.0 75.2\nR3 prompting 95.7 78.1\nTable 4: Performance of prompting methods on AddSub\nand GSM-IC datasets with different engines.\ntion (3) shows more observable performance gain\nover combination (1) on AddSub and SingleEQ-IC\ndatasets than the other datasets. In comparison,\ncombination (2) shows more observable perfor-\nmance gain on AddSub and SV AMP than the other\ndatasets. After investigating the datasets in detail,\nwe find that SingleEQ-IC contains more questions\nrequiring multi-step reasoning, GSM-IC contains\nmore questions with noisy context and AddSub\ncontains both. This highlights an intriguing finding\nthat rephrasing is more crucial for complex reason-\ning and reviewing is more significant for denoising.\n5.4 Effect on Different Noisy Context\nTo explore the effect of R 3 prompting on differ-\nent numbers and types of noisy sentences, we\nfirst sample 200 problems from AddSub dataset,\nthen introduce irrelevant sentences to each problem\nwith numbers ranging from 1 to 4. This results\nin datasets AddSub-1, AddSub-2, AddSub-3, and\nAddSub-4, where “-n” denotes the number of irrel-\nevant sentences manually inserted in each problem.\nAs shown in Figure 4, R3 prompting maintains ac-\ncuracy fluctuating around 84% over all the datasets.\nIn contrast, both Instructed-CoT and Manual-CoT\nexperience a decrease in accuracy with the increas-\ning amount of irrelevant sentences involved. It\nproves that R 3 prompting exhibits robust perfor-\nmance under noisy context while Instructed-CoT\nand Manual-CoT are vulnerable when facing a\nlarge amount of noisy information.\nAs we discussed before, this study focuses on\ntwo categories of noisy context, which are in-topic\nnoise and off-topic noise. To investigate which type\nof noise R3 prompting handles better, we collected\n200 problems from the AddSub dataset and intro-\nduce different categories of irrelevant sentences.\nThis form AddSub-in-topic and AddSub-off-topic\ndatasets that contain a single type of noisy context.\nWe discovered that R3 achieves 92% accuracy on\nAddSub-off-topic dataset, while it achieves an ac-\ncuracy of 87.5% on AddSub-in-topic dataset. This\nindicates that compared with off-topic noise, in-\ntopic noise is more difficult to be resolved as it\nprovides more deceptive information that can eas-\nily “trap” LLMs.\n1676\nError Type R3\nCalculation error 50%\nFalse negative in reviewing 4%\nFalse positive in reviewing 36%\nRephrase errors 10%\nTable 5: Error analysis of R3 prompting.\n5.5 Error Analysis\nTo better understand how R3 prompting works un-\nder noisy context, we present an illustrative exam-\nple in Figure 5 that is correctly predicted by R 3\nprompting but incorrectly predicted by other meth-\nods. As we can see, sentence “21 dimes” is a noisy\nsentence that is related to the problem narratives\nbut irrelevant to the final answer prediction. Both\nManual-CoT and Instructed-CoT produce wrong\nrationales that include the noisy sentence but this\neventually results in a wrong predicted answer. For\nR3 prompting, after reviewing the problems, key\nsentences are extracted from the problem so that\nthe rephrasing and resolving stages could escape\nfrom the disturbance of the noisy context and even-\ntually output the correct answer. Interestingly, in\nthe rephrase stage, the hint “The number of dimes is\nirrelevant to this question.” is considered as a weak\nrequirement for a correct equation. Therefore, the\nLLMs can directly benefit and obtain a final output\nwhich is clear, concise, and free from unnecessary\nverbosity.\nWe further sample 50 error cases that R3 prompt-\ning have incorrectly predicted and analyzed them.\nAs shown in Table 5, calculation error is the most\ncommon error accounting for around half of the\ntotal mistakes. This is caused by the fundamen-\ntal mechanism of LLMs, where the probabilistic\nmodel is utilized to conduct calculation and may\nnot exactly follow the mathematical calculation\nrules. This can be deemed as the upper bound-\nary of solving reasoning tasks in LLMs. Regard-\ning the errors caused by problem understanding\nand reasoning, compared with other baselines like\nManual-CoT, R3 prompting method can already\nreduce them into a relatively low proportion (70%\n→ 50%). Among them, errors caused by distin-\nguishing wrong key sentences account for 40% of\nthe total errors, which is significantly lower than\nManual-CoT (66% → 40%). We also notice that\nfalse positive errors occur more frequently than\nfalse negative errors.\nFigure 5: An illustrative problem from AddSub dataset,\nthe result of which is incorrectly predicted by other\nprompting methods but correctly predicted by R 3\nprompting. Incorrect rationales and answers are dis-\nplayed in red color. Correct rationales and answers are\ndisplayed in green color. Noisy context is displayed in\nblue color.\n6 Conclusion\nIn this paper, we proposed R 3 prompting to en-\nable LLMs to solve problems under noisy context.\nIt involves prompts for three different stages: Re-\nviewing, Rephrasing, and Resolving. Evaluation\non eight datasets across two types of noisy context\nsuggests R3 prompting significantly outperforms\nthe previous baselines. More analyses and ablation\nstudies show that the performance of R 3 prompt-\ning is stable and powerful. Overall, the advantages\nof R3 prompting are presented in the following\naspects: 1) Achieving substantial performance im-\nprovement on arithmetic reasoning tasks under the\ninterference of noisy context; 2) Exhibiting strong\n1677\nrobustness, maintaining stable performance even\nwhen facing progressively increasing amounts of\nnoisy context.\nLimitations\nDespite the improvement achieved by R3 prompt-\ning, there are still aspects that can be improved. For\ninstance, greedy decoding may not be the optimal\nstrategy to produce the output answer as the best-\nfirst search may not result in the global best result.\nHow to design an efficient strategy for sampling\nis a direction that many researchers are currently\ninvestigating (Wang et al., 2022; Weng et al., 2022;\nLi et al., 2022). In our method, the output in the\nreview stage is currently obtained through greedy\ndecoding. To achieve higher-quality review out-\ncomes, future work could design a strategy, similar\nto Self-Consistency, to evaluate and sample the re-\nsults from the review stage which are necessary\nand sufficient for problem-solving. Therefore, de-\nveloping effective sampling strategies based on R3\nprompting would be an fascinating direction to ex-\nplore.\nEthics Statement\nThis is a study about Arithmetic Reasoning. It\ndoes not have any data privacy issues; we did not\ncollect any personal information. This is a task that\ninvolved no risk as the participants are not exposed\nto any harmful material or asked to perform any\nrisky tasks.\nAcknowledgments\nThe authors would like to thank the anonymous\nreviewers for their insightful comments. This work\nwas supported by Natural Science Foundation of\nChina (Project No. 62206097) and Shanghai Pu-\njiang Talent Program (Project No. 22PJ1403000).\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2022. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. arXiv\npreprint arXiv:2205.09712.\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren\nEtzioni, and Nate Kushman. 2014. Learning to solve\narithmetic word problems with verb categorization.\nIn EMNLP, pages 523–533.\nJunyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho,\nHwiyeol Jo, Sang-Woo Lee, Sang-goo Lee,\nKang Min Yoo, and Taeuk Kim. 2022. Ground-truth\nlabels matter: A deeper look into input-label demon-\nstrations. arXiv preprint arXiv:2205.12685.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\n2015. Parsing algebraic word problems into equa-\ntions. Transactions of the Association for Computa-\ntional Linguistics, 3:585–597.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,\nJian-Guang Lou, and Weizhu Chen. 2022. On the\nadvance of making language models better reasoners.\narXiv preprint arXiv:2206.02336.\nAman Madaan and Amir Yazdanbakhsh. 2022. Text\nand patterns: For effective chain of thought, it takes\ntwo to tango. arXiv preprint arXiv:2209.07686.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nAllen Newell, John C Shaw, and Herbert A Simon. 1959.\nReport on a general problem solving program. In\nIFIP congress, volume 256, page 64. Pittsburgh, PA.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are nlp models really able to solve\nsimple math word problems? arXiv preprint\narXiv:2103.07191.\nSubhro Roy and Dan Roth. 2016. Solving gen-\neral arithmetic word problems. arXiv preprint\narXiv:1608.01413.\n1678\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed Chi, Nathanael Schärli, and\nDenny Zhou. 2023. Large language models can be\neasily distracted by irrelevant context. arXiv preprint\narXiv:2302.00093.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi\nLan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-\nand-solve prompting: Improving zero-shot chain-of-\nthought reasoning by large language models. arXiv\npreprint arXiv:2305.04091.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models. arXiv\npreprint arXiv:2203.11171.\nAlbert Webson and Ellie Pavlick. 2021. Do prompt-\nbased models really understand the meaning of their\nprompts? arXiv preprint arXiv:2109.01247.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nYixuan Weng, Minjun Zhu, Shizhu He, Kang Liu,\nand Jun Zhao. 2022. Large language models are\nreasoners with self-verification. arXiv preprint\narXiv:2212.09561.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022a. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022b. Automatic chain of thought prompt-\ning in large language models. arXiv preprint\narXiv:2210.03493.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo\nLi, and Yu Li. 2023. Progressive-hint prompting\nimproves reasoning in large language models. arXiv\npreprint arXiv:2304.09797.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\n1679\nA Appendix\nA.1 Experiment Result on More Benchmarks\nMethods AddSub MultiArith SingleEq SV AMP GSM8K Avg.\nOne-turn\nManual-CoT 85.3 97.2 92.9 79.9 78.9 86.8\nAuto-CoT 88.0 98.2 92.3 83.6 79.9 88.4\nInstructed-CoT 87.3 97.2 92.9 81.3 79.1 87.6\nInteractive\nLeast-to-Most 85.3 89.8 85.0 80.8 79.3 84.0\nPHP 85.3 98.0 92.9 83.1 84.5 88.8\nR3 Prompting (Ours) 90.0 98.2 93.1 87.3 80.1 89.7\nTable 6: Accuracy of different methods on benchmarks with the engine of GPT-3.5-turbo\nA.2 R 3 Prompting Details\nA.2.1 R 3 Prompting I: REVIEW (7 Examples)\nQ: Zachary did 46 push-ups and 58 crunches in gym class today. David did 38 more push-ups but 62 less\ncrunches than Zachary. In the same gym class, there were 25 students in total. How many more crunches\nthan push-ups did Zachary do?\nA:We need to notice: 1.\"Zachary did 46 push-ups.\", 2.\"Zachary did 58 push-ups.\", 3.\"The number of\ncrunches Zachary did is irrelevant to this question.\", 4.\"The number of students is irrelevant to this\nquestion.\".\nQ: There are many different books in the ’crazy silly school’ series. If you have read 13 of the\nbooks and are yet to read 8 books. The author of the series, Mr. Johnson, has written a total of 35 books.\nHow many books are there in the ’crazy silly school’ series?\nA: We need to notice: 1.\"You have read 13 of the books.\", 2.\"You are yet to read 8 books.\", 3.\"The sum of\nread and unread books is equal to how many books are here.\", 4.\"How many books did the author write is\nirrelevant to this question.\".\nQ: Every day Ryan spends 6 hours on learning english and 7 hours on learning chinese. Dur-\ning the weekend, Ryan also spends 3 hours on playing video games each day. If he learns for 5 days, how\nmany hours does he spend on learning english and chinese in all?\nA: We need to notice: 1.\"Every day Ryan spends 6 hours on learning english and 7 hours on learning\nchinese.\", 2.\"He learns for 5 days.\", 3.\"How long does Ryan spend on games is irrelevant to this question.\".\nQ: Matthew had 29 crackers and 30 cakes. The total number of cookies Matthew had was 15.\nIf Matthew gave equal numbers of crackers and cakes to his 2 friends. How many cakes did each person\neat?\nA: We need to notice: 1.\"Matthew had 30 cakes.\", 2.\"Matthew gave equal numbers of crackers and cakes\nto his 2 friends.\", 3.\"The number of crackers and cookies is irrelevant to this question.\".\nQ: For Gwen’s birthday she received 8 dollars from her mom. Her dad gave her 5 more dollars.\nThe total number of guests at Gwen’s birthday party was 12. If she spent 4 dollars. How much more\nmoney did she receive from her mom than she did from her dad?\nA: We need to notice: 1.\"For Gwen’s birthday she received 8 dollars from her mom.\", 2.\"Her dad gave\nher 5 more dollars.\", 3.\"The dollars she spent is irrelevant to this question.\", 4.\"The number of guests is\nirrelevant to this question.\".\n1680\nQ: Jack received 5 emails and 6 letters in the morning. He then received 9 emails and 7 letters\nin the afternoon. The total number of text messages Jack received during the day was 24. How many\nletters did jack receive in the day?\nA: We need to notice: 1.\"Jack received 6 letters in the morning.\", 2.\"Jack received 7 letters in the\nafternoon.\", 3.\"The number of emails and text messages Jack received is irrelevant to this question.\".\nQ: Paul had 108 books and 55 pens. After selling some books and pens in a garage sale he had\n66 books and 59 pens left. Paul also sold 4 tables during the garage sale. How many books did he sell in\nthe garage sale?\nA: We need to notice: 1.\"Paul had 108 books.\", 2.\"After selling some books in a garage sale he had 66\nbooks left.\", 3.\"The number of pens and tables is irrelevant to this question.\".\nA.2.2 R 3 Prompting II: REPHRASE (4 Examples)\nJack received 5 emails and 6 letters in the morning. He then received 9 emails and 7 letters in the\nafternoon. The total number of text messages Jack received during the day was 24. How many letters did\njack receive in the day?\nQ: Translate following sentences into equation: 1.’Jack received 6 letters in the morning.’ 2.’Jack received\n7 letters in the afternoon.’ 3.’The total number of letters jack received is what we need to find’\nA: 1.\"Letters received in the morning = 6.\"\n2.\"Letters received in the afternoon = 7.\"\n3.\"Total number of letters = letters received in the morning + letters received in the afternoon = 6 + 7.\"\nMary is baking a cake. The recipe calls for 14 cups of flour and 6 cups of sugar. She already\nput in 5 cups of flour. How many more cups of flour than cups of sugar does she need to add now?\nQ: Translate following sentences into equation: 1.’The recipe calls for 14 cups of flour and 6 cups of\nsugar.’ 2.’She already put in 5 cups of flour.’ 3.’The difference between the cups of flour and cups of sugar\nstill needed is what we need to find.’\nA: 1.\"Cups of flour needed = 14, Cups of sugar needed = 6.\"\n2.\"Cups of flour already added = 5.\"\n3.\"Difference = cups of flour still needed - cups of sugar still needed= cups of flour needed - cups of flour\nadded - cups of sugar needed.\"\nEvery day Ryan spends 6 hours on learning english and 7 hours on learning chinese. During\nthe weekend, Ryan also spends 3 hours on playing video games each day. If he learns for 5 days, how\nmany hours does he spend on learning english and chinese in all?\nQ: Translate following sentences into equation: 1.’Every day Ryan spends 6 hours on learning english and\n7 hours on learning chinese.’ 2.’He learns for 5 days.’\nA: 1.\"Time for english in one day = 6 hours, Time for chinese in one day = 7 hours.\"\n2.\"The total time of learning chinese and english = (Time for english in one day + Time for chinese in one\nday) * 5.\"\nFor Gwen’s birthday she received 8 dollars from her mom. Her dad gave her 5 more dollars.\nIf she spent 4 dollars. How much more money did she receive from her mom than she did from her dad?\nQ: Translate following sentences into equation: 1.’Gwen’s birthday she received 8 dollars from her mom.’\n2.’Her dad gave her 5 more dollars.’\nA: 1.\"dollars received from mom = 8.\"\n2.\"dollars received from dad = 5.\"\n1681\nA.2.3 R 3 Prompting III: RESOLVE (8 Examples)\nQ: Zachary did 46 push-ups and 58 crunches in gym class today. David did 38 more push-ups but 62 less\ncrunches than Zachary. In the same gym class, there were 25 students in total. How many more crunches\nthan push-ups did Zachary do?\nA: With the Equation Hints: ’Push-ups number of Zachary = 46.’, ’Crunches number of Zachary = 58.’,\n’More crunches number = Crunches number of Zachary - Push-ups number of Zachary = 58 - 46.’, we\nwill answer the question. Difference in crunches number and Zachary number = Crunches number of\nZachary - Push-ups number of Zachary = 58 - 46. 58 - 46 = 12. The answer is 12.\nQ: There are many different books in the ’crazy silly school’ series. If you have read 13 of the\nbooks and are yet to read 8 books. The author of the series, Mr. Johnson, has written a total of 35 books.\nHow many books are there in the ’crazy silly school’ series?\nA: With the Equation Hints: ’books (you have read) number = 13, books (yet to read) = 8.’, ’books\nnumber in the series = books (you have read) number + books (yet to read) = 13 + 8.’, we will answer the\nquestion. Books number in the series = books (you have read) number + books (yet to read) = 13 + 8. 13\n+ 8 = 21. The answer is 21.\nQ: Every day Ryan spends 6 hours on learning english and 7 hours on learning chinese. Dur-\ning the weekend, Ryan also spends 3 hours on playing video games each day. If he learns for 5 days, how\nmany hours does he spend on learning english and chinese in all?\nA: With the Equation Hints: ’Time for english in one day = 6 hours, Time for chinese in one day = 7\nhours.’, ’The total time of learning chinese and english = (Time for english in one day + Time for chinese\nin one day)*5 = 13 * 5.’, we will answer the question. The total time of learning chinese and english =\n(Time for english in one day + Time for chinese in one day) * 5 = 13 * 5. So, 13 * 5 = 65. The answer is 65.\nQ: Mary is baking a cake. The recipe calls for 14 cups of flour and 6 cups of sugar. Mary’s\nmother has already made 3 cakes before. She already put in 5 cups of flour. How many more cups of flour\nthan cups of sugar does she need to add now?\nA: With the Equation Hints: ’Cups of flour needed = 14, Cups of sugar needed = 6.’, ’Cups of flour\nalready added = 5.’, ’Difference = cups of flour needed - cups of flour already added - cups of sugar\nneeded = 14 - 5 - 6.’, we will answer the question. There are 14 - 5 - 6 = 9 - 6 = 3 cups of flour than cups\nof sugar she need to add. The answer is 3.\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How\nmany toys does he have now?\nA: With the Equation Hints: ’Shawn started with 5 toys.’, ’Shawn got 2 * 2 = 4 toys.’, we will answer the\nquestion. Shawn started with 5 toys. Then Shawn got 2 * 2 = 4 toys. 5 + 4 = 9. The answer is 9.\nQ: Jack received 5 emails and 6 letters in the morning. He then received 9 emails and 7 letters\nin the afternoon. The total number of text messages Jack received during the day was 24. How many\nletters did jack receive in the day?\nA: With the Equation Hints: ’Letters received in the morning = 6.’, ’Letters received in the afternoon = 7.’,\n’Total number of letters = letters received in the morning + letters received in the afternoon = 6 + 7.’, we\nwill answer the question. Total number of letters = letters received in the morning + letters received in the\nafternoon = 6 + 7. So, 6 + 7 = 13. The answer is 13.\nQ: For Gwen’s birthday she received 8 dollars from her mom. Her dad gave her 5 more dollars.\nIf she spent 4 dollars. How much more money did she receive from her mom than she did from her dad?\nA: With the Equation Hints: ’Dollars received from mom = 8.’, ’Dollars received from dad = 5.’,\nwe will answer the question. She received 8 - 5 = 3 dollar from her mom than from her dad. The answer is 3.\n1682\nQ: Paul had 108 books and 55 pens. After selling some books and pens in a garage sale he had\n66 books and 59 pens left. Paul also sold 4 tables during the garage sale. How many book s did he sell in\nthe garage sale?\nA: With the Equation Hints: ’Original number of books = 108.’, ’Final number of books = 66.’, ’Books sold\n= original number of books - final number of books = 108 - 66.’, we will answer the question. The num-\nber of books he sold = original number of books - final number of books = 108 - 66 = 42. The answer is 42.\nA.3 Success Example Outputs by R 3 Prompting on Different Datasets\nA.3.1 SV AMP\nQuestion:\nBobby ate 33 pieces of candy. Then he ate 4 more. He also ate 14 pieces of chocolate. How many pieces\nof candy and chocolate did Bobby eat altogether?\nReview stage output:\nWe need to notice: 1.\"Bobby ate 33 pieces of candy.\", 2.\"He ate 4 more pieces of candy.\", 3.\"He also\nate 14 pieces of chocolate.\", 4.\"The total number of pieces of candy and chocolate is relevant to this\nquestion.\"\nRephrase stage output:\n1.\"Pieces of candy eaten = 33.\" 2.\"Additional pieces of candy eaten = 4.\" 3.\"Pieces of chocolate eaten =\n14.\" 4.\"Total number of pieces of candy and chocolate eaten = pieces of candy eaten + additional pieces\nof candy eaten + pieces of chocolate eaten.\"\nResolve stage output:\nTotal number of pieces of candy and chocolate eaten = pieces of candy eaten + additional pieces of candy\neaten + pieces of chocolate eaten = 33 + 4 + 14 = 51. The answer is 51.\nA.3.2 MultiArith-IC\nQuestion:\nWendy’s grandmother sold 82 concert tickets. Wendy had 82 files on her computer. Wendy deleted 37 of\nthem from computer and put the rest into folders with 5 files in each one. How many folders did Wendy\nend up with?\nReview stage output:\nWe need to notice: 1.\"Wendy had 82 files on her computer.\", 2.\"Wendy deleted 37 of them from her\ncomputer.\", 3.\"The number of concert tickets sold by Wendy’s grandmother is irrelevant to this question.\",\n4.\"The number of files in each folder is given as 5.\"\nRephrase stage output:\n1.\"Number of files on Wendy’s computer = 82.\" 2.\"Number of files deleted from Wendy’s computer =\n37.\" 3.\"Number of concert tickets sold by Wendy’s grandmother = irrelevant.\" 4.\"Number of files in\neach folder = 5.\" 5.\"Number of folders Wendy ended up with = (Number of files on Wendy’s computer -\nNumber of files deleted from Wendy’s computer) / Number of files in each folder.\"\nResolve stage output:\nNumber of folders Wendy ended up with = (Number of files on Wendy’s computer - Number of files\ndeleted from Wendy’s computer) / Number of files in each folder = (82 - 37) / 5 = 9. The answer is 9.\n1683\nA.3.3 SingleEq-IC\nQuestion:\nLast year, David bought 10.0 books about science. This year, Vincent bought 10 books about animals, 1\nbook about outer space, and 3 books about trains. Each book cost $16. How much did Vincent spend on\nthe books, this year?\nReview stage output:\nWe need to notice: 1.\"Vincent bought 10 books about animals, 1 book about outer space, and 3 books\nabout trains.\", 2.\"Each book cost $16.\", 3.\"How much David spent is irrelevant to this question.\"\nRephrase stage output:\n1.\"Number of books Vincent bought = 10 + 1 + 3 = 14.\" 2.\"Cost of each book = $16.\" 3.\"Total cost of\nbooks Vincent bought = number of books Vincent bought * cost of each book = 14 * $16.\"\nResolve stage output:\nTotal cost of books Vincent bought = number of books Vincent bought * cost of each book = 14 * $16 =\n$224. The answer is $224.\nA.3.4 AddSub\nQuestion:\nJason has 18 books and he has read 9 of them . Mary has 42 books . How many books do they have\ntogether ?\nReview stage output:\nWe need to notice: 1.\"Jason has 18 books and he has read 9 of them.\", 2.\"Mary has 42 books.\", 3.\"The\nnumber of books they have read together is irrelevant to this question.\"\nRephrase stage output:\n1.\"Number of books Jason has = 18, Number of books Jason has read = 9.\" 2.\"Number of books Mary has\n= 42.\" 3.\"Total number of books they have together = Number of books Jason has + Number of books\nMary has = 18 + 42.\"\nResolve stage output:\nTotal number of books they have together = Number of books Jason has + Number of books Mary has =\n18 + 42. So, 18 + 42 = 60. The answer is 60.\nA.3.5 GSMIC\nQuestion:\nJackie spends 8 hours working, 3 hours of exercise and spends 8 hours of sleep per day. Jackie’s sister is 3\nyears old. How much free time does Jackie have?\nReview stage output:\nWe need to notice: 1.\"Jackie spends 8 hours working, 3 hours of exercise and spends 8 hours of sleep per\nday.\", 2.\"The age of Jackie’s sister is irrelevant to this question.\".\nRephrase stage output:\n1.\"Total hours in a day = 24.\" \"Hours spent working = 8, Hours spent exercising = 3, Hours spent sleeping\n= 8.\" \"Total hours spent = Hours spent working + Hours spent exercising + Hours spent sleeping = 8 + 3 +\n1684\n8.\" \"Free time = Total hours in a day - Total hours spent = 24 - (8 + 3 + 8).\" 2. \"The age of Jackie’s sister\nis irrelevant to this question.\"\nResolve stage output:\nTotal hours spent = Hours spent working + Hours spent exercising + Hours spent sleeping = 8 + 3 + 8 =\n19. Free time = Total hours in a day - Total hours spent = 24 - (8 + 3 + 8) = 5. The answer is 5.\n1685",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6219213008880615
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5633497834205627
    },
    {
      "name": "Dilemma",
      "score": 0.45660924911499023
    },
    {
      "name": "Language model",
      "score": 0.4421739876270294
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41424012184143066
    },
    {
      "name": "Natural language processing",
      "score": 0.41157248616218567
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3946002721786499
    },
    {
      "name": "Cognitive science",
      "score": 0.3850112557411194
    },
    {
      "name": "Psychology",
      "score": 0.2683103680610657
    },
    {
      "name": "Epistemology",
      "score": 0.16395670175552368
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I66867065",
      "name": "East China Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I79891267",
      "name": "Singapore Management University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    }
  ]
}