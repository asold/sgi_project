{
  "title": "Consumer-Centric Insights Into Resilient Small Object Detection: SCIoU Loss and Recursive Transformer Network",
  "url": "https://openalex.org/W4388469725",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100350774",
      "name": "Le Wang",
      "affiliations": [
        "Fujian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100673011",
      "name": "Yu Shi",
      "affiliations": [
        "Fujian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5031838724",
      "name": "Guojun Mao",
      "affiliations": [
        "Fujian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5061687830",
      "name": "Fayaz Ali Dharejo",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5071515463",
      "name": "Sajid Javed",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5080217730",
      "name": "Moath Alathbah",
      "affiliations": [
        "King Saud University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4315629955",
    "https://openalex.org/W4313071569",
    "https://openalex.org/W6842315085",
    "https://openalex.org/W4296913793",
    "https://openalex.org/W4382053839",
    "https://openalex.org/W3118257828",
    "https://openalex.org/W2924574544",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2570343428",
    "https://openalex.org/W6750227808",
    "https://openalex.org/W3018757597",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W2810096225",
    "https://openalex.org/W2947871268",
    "https://openalex.org/W2987322772",
    "https://openalex.org/W2990763144",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W6620707391",
    "https://openalex.org/W2109255472",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W6798838024",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3012573144",
    "https://openalex.org/W2193145675",
    "https://openalex.org/W6843239693",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W6780945792",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W4220727424",
    "https://openalex.org/W4294069484",
    "https://openalex.org/W4294069590",
    "https://openalex.org/W4294069468",
    "https://openalex.org/W3210586215",
    "https://openalex.org/W3198544958",
    "https://openalex.org/W3215216361",
    "https://openalex.org/W3181385011",
    "https://openalex.org/W3156982815",
    "https://openalex.org/W3009396058",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W3212960520",
    "https://openalex.org/W3208645658",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W6760424586",
    "https://openalex.org/W4327652243",
    "https://openalex.org/W4293791253",
    "https://openalex.org/W3184439416",
    "https://openalex.org/W4293584584",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3043944662",
    "https://openalex.org/W4297676427"
  ],
  "abstract": "As an emerging consumer electronic product, the use of unmanned aerial vehicle(UAV) for a variety of tasks has received growing attention and favor in the enterprise or individual consumer electronics market in recent years. The deep neural network based object detectors are convenient to embed into the UAV product, however, the drone-captured images could bring the potential challenges of object occlusion, large scale difference and complex background to these methods because they are not desinged for the detection of small and tiny objects within the aerial images. To address the problem, we propose an improved YOLO paradigm called SR-YOLO with an Efficient Neck, Shape CIoU and Recursion Bottleneck Transformer for better object detection performance in consumer-level UAV products. Firstly, an efficient neck structure is presented to retain richer features through a small object detection layer and an up-sampling operator suitable for small object detection. Secondly, we design a new prediction box loss function called shape complete-IoU(SCIoU), which utilizes a width (height) limiting factor to alleviate the deficiency that the CIoU only focuses on aspect ratios by taking into account both the aspect ratio and the ratio of the two boxes' widths. Moreover, combined with recurrent neural network and multi-head self-attention mechanism at the cyclic manner, a recursive bottleneck transformer is constructed to relieve the impact of highly dense scene and occlusion problems exists in UAV images. We conduct the extensive experiments on two public datasets of VisDrone2019 and TinyPerson, where the results show that the proposed model surpasses the compared YOLO by 8.1% and 3.2% in mAP50 respectively. In addition, the analysis and case study also validate our SR-YOLO's superiority and effectiveness.",
  "full_text": "Consumer-Centric Insights into Resilient Small\nObject Detection: SCIoU Loss and Recursive\nTransformer Network\nLe Wang, Yu Shi, Guojun Mao, Fayaz Ali Dharejo, Senior, IEEE Member, Sajid Javed and Moath Alathbah\nAbstract—As an emerging consumer electronic product, the\nuse of unmanned aerial vehicle(UA V) for a variety of tasks has\nreceived growing attention and favor in the enterprise or individ-\nual consumer electronics market in recent years. The deep neural\nnetwork based object detectors are convenient to embed into the\nUA V product, however, the drone-captured images could bring\nthe potential challenges of object occlusion, large scale difference\nand complex background to these methods because they are not\ndesinged for the detection of small and tiny objects within the\naerial images. To address the problem, we propose an improved\nYOLO paradigm called SR-YOLO with an Efficient Neck, Shape\nCIoU and Recursion Bottleneck Transformer for better object\ndetection performance in consumer-level UA V products. Firstly,\nan efficient neck structure is presented to retain richer features\nthrough a small object detection layer and an up-sampling\noperator suitable for small object detection. Secondly, we design\na new prediction box loss function called shape complete-\nIoU(SCIoU), which utilizes a width (height) limiting factor to\nalleviate the deficiency that the CIoU only focuses on aspect\nratios by taking into account both the aspect ratio and the ratio\nof the two boxes’ widths. Moreover, combined with recurrent\nneural network and multi-head self-attention mechanism at the\ncyclic manner, a recursive bottleneck transformer is constructed\nto relieve the impact of highly dense scene and occlusion problems\nexists in UA V images. We conduct the extensive experiments on\ntwo public datasets of VisDrone2019 and TinyPerson, where the\nresults show that the proposed model surpasses the compared\nYOLO by 8.1% and 3.2% in mAP50 respectively. In addition, the\nanalysis and case study also validate our SR-YOLO’s superiority\nand effectiveness.\nIndex Terms—unmanned aerial vehicle (UA V) image, small\nobject detection, you only look once, Bottleneck Transformer.\nI. I NTRODUCTION\nUnmanned aerial vehicle(UA V) with object detection has\npermeated through various fields of consumer electronics [1]–\n[3], such as industrial inspection [4], environmental moni-\ntoring of aquaculture [5], agricultural crop monitoring [6],\nor civilian mountain search and rescue [7]. Although the\nUA V aerial images are of high resolution, they still suffer\nLe Wang †, Yu Shi † and Guojun Mao ∗ are with Fujian Provincial Key\nLaboratory of Big Data Mining and Applications, College of Computer\nScience and Mathematics, FJUT 350118, China (email: lewang@fjut.edu.cn;\n2211308023@smail.fjut.edu.cn; 19662092@fjut.edu.cn);\nFayaz Ali Dharejo and Sajid Javed are with Department of Electrical Engi-\nneering and Computer Science, Khalifa Univeristy, Abu Dhabi, UAE (email:\nfayaz.ali@ku.ac.ae) sajid.javed@ku.ac.ae);\nMoath Alathbah ∗ is with Department of Electrical Engineering, College\nof Engineering, King Saud University, Riyadh 11451, Saudi Arabia (email:\nmalathbah@ksu.edu.sa)∗Corresponding Authors, †These authors contributed equally to this work.\nfrom dramatic changes in their scale, intricate background\nand occlusion.Currently, YOLO [8]–[11] series algorithms are\ngaining importance in object detection algorithms. YOLOv5\nwith high adaptability, easy deployment, and high accuracy\nis widely used in various target detection tasks and industrial\nproduction.However, the YOLO series algorithm, conceived\nfor object detection in general natural scenes (typical natural\nscene datasets include MS COCO [12], PASCAL VOC [13],\netc.). In order to keep YOLOv5 lightweight while better adapt-\ning it to UA Vs for object detection. We improved YOLOv5 for\nthe characteristics of the images in the actual UA V working\nscenarios. So that it can give consumers a better experience.\nFor the above reasons, we propose an improved efficient\nneck, a newly designed Shape CIoU and Recursion Bottle-\nneck Transformer for small object detection in drone-captured\nimages. Firstly, considering that small or tiny objects pos-\nsess low pixels [14], [15] and there exists dramatic scale\nvariation between different objects in aerial images, a new\nneck structure called Efficient Neck by integrating Content-\nAware ReAssembly of Features [16](CARAFE) up-sampling\noperator and small object detection layer. Efficient Neck can\nbetter cope with the drastic changes in different object scales\nand obtain better detection results, such as detecting more\ntiny objects. Secondly, we design a novel prediction box loss\nfunction Shape Complete-IoU (SCIoU) to make the prediction\nbox closer to the real box. Improvement of complete-IoU\n(CIoU) [17] aspect ratio penalty term failure by a newly\ndesigned shape penalty term. In addition, an improved Re-\ncursion Bottleneck Transformer (RBoT) module is proposed to\nobtain global information through recursively utilizing the self-\nattention mechanism to alleviate the issue of object occlusion\nin crowds or highly-density scenes.\nBy incorporating ther advanced components above, an in-\nnovative YOLO paradigm is derived to address the specific\nchallenges associated with small even tiny object detection\nthrough enhancing the spatial resolution of the feature maps,\nrefining the aspect ratio loss and obtaining global contextual\nsignals. Specially, for the validation of our proposed method,\nwe implement the two variants using our proposed SR-YOLO,\nnamly SR-YOLOv5 and SR-YOLOv8(based on YOLOv5 and\nYOLOv8, respectively), and conduct the extensive experi-\nments. We validate them on two benchmark datasets (Vis-\nDrone2019 and TinyPerson). The experimental results show\nthat SR-YOLOv5 surpasses 8.1% on mAP50(%), and the new\nmodel is more suitable for small object detection tasks than the\nThis article has been accepted for publication in IEEE Transactions on Consumer Electronics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCE.2023.3330788\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\noriginal YOLOv5. Experiments have proved that SR-YOLO is\nable to recognize targets more accurately in a variety of com-\nplex backgrounds and poor lighting environments. Combined\nwith the UA V Intelligent System, SR-YOLO can provide better\nintelligent identification services for electronics consumers. In\nsummary, the main innovations of this paper are as follows:\nA new neck network comprised of CARAFE, an additional\nfeature fusion layer, and a detection head is realized, where\nCARAFE provides a large receptive field and will be flexible\nfor up-sampling. The four-detection head structure can better\nalleviate the impact of object scale changes.\nA new prediction box loss function is designed, which\nmakes the prediction box closer to the real box while improv-\ning the width ratio between the prediction box and the real box.\nThe new loss function enables the network to obtain higher-\nquality prediction boxes and maximize detection accuracy.\nAn improved recursion Transformer is proposed to enhance\nthe recognition effect in occlusion and dense scenes. This new\nattention mechanism can help the network better obtain target\ncontext information in complex situations.\nII. R ELATED WORK\nObject detection models: Generally, an image is taken\nas input and output with bounding boxes and labels on\ndetected objects by object detection models, which thus can\nbe mainly divided into two classes from the perspective of\ntheir identifying processes. The first type of models are two-\nstage detectors, in which the detection is separated into two\nphases, namely generating a large number of region proposal\nthat may contain the object and append approximate location\ninformation, then classifying and regressing the RoIs, such as\nR-CNN [18], Fast R-CNN [19], Faster R-CNN [20], SPPNet\n[21], Mask R-CNN [22], FPN [23].\nThe second ones, called one-stage detectors, skip over the\nregion proposal stage of the two-stage models and receive the\ncomplete image as input and regress the object border directly\nupon a dense sampling of locations, such as YOLOX [24],\nDETR [25], CornerNet [26], SSD [27], YOL5Ov6 [28], FCOS\n[29], EfficientNet [30], PP-YOLO [31].\nBounding box regression loss functions: In object de-\ntection, the positioning task needs to determine the position\nof the object in an image and then output its corresponding\ncoordinate information. The positioning task relies on the\nbounding box regression module to locate the object. The\nbounding box regression refers to using a rectangular bounding\nbox to predict the position of the object in an image, and\nthen continuously adjust the position of the prediction box.\nThe bounding box regression loss function has been developed\nrapidly with the introduction of IoU (intersection over union).\nGIoU [32] is proposed to solve the problem that IoU cannot\noptimize the non-intersection between the prediction box and\nthe real box, aiming to make the prediction box and the real\nbox intersected. Then DIoU and CIoU [33] further solved\nthe degradation problem when the prediction and real boxes\noverlapped.\nConsumer Electronic: The development of technology\nthat combined drones and deep learning in the field of crop\nclassification was investigated by Bouguettaya [34] et al, who\npointed out the importance of the integration of UA V-based\nremote sensing technologies and deep learning algorithms,\nespecially object detectors, which provide great convenience\nto the consumers in agriculture. Huang [35] proposed an\nalgorithm based on YOLO-R to deal with the actual situation\nin road object detection for autonomous driving. Chen [36]\net al. improved the YOLOv4 for pavement defect detection,\naiding drivers to get more information about the road surface\nto reduce car damage and safety risk. In addition, Gung [37]\net al. applied the defect detector in the enterprise production\nlines, where their model can effectively assist the workers in\nidentifying the defective products.\nYOLO series: YOLO series models are widely used in\nobject detection because of its high accuracy and detection\nspeed. The TPH-YOLO model proposed by Xingkui Zhu\net al. [38] was in the light of the characteristics of aerial\nimages. Based on YOLOv5, a Transformer layer was added in\nfront of the detection head to form a Transformer Prediction\nHead (TPH) to replace the prediction head in the origi-\nnal model. Convolutional Block Attention Module (CBAM)\nwas employed to find the area of attention in dense object\nscenes, but this structure requires huge computational costs\nand hardware resources. Raian Rahman et al. [39] trained four\ndifferent models on the aerial view of urban traffic and then\nused non-maximum suppression (NMS) for the integration\nof the models. Although this method improves the accuracy,\nits reasoning time increases greatly. Munhyeong Kim et al.\n[40] proposed ECAP-YOLO, introduced an improved IECA\nmodule into the SPP module and network, and modified the\nnumber of detection layers to make the model more suitable\nfor detecting small objects. Because the model deletes the\ndetection layer of large objects, the detection effect of large\nobjects is relatively poor. Tao Liu et al. [41] used YOLOv4\nfor sea surface object detection. They designed a new reverse\ndepth separable convolution to replace the partial convolutions\nin the network, which improved the network a little in terms\nof accuracy, but if all the convolutions in the network are\nreplaced, the effect is not ideal. Mohamad Haniff Junos et al.\n[42] utilized the improved YOLOv5 network to detect the oil\npalm image of the drone. This study added a tightly connected\nneural network, modified the activation function to switch, and\nadded a new detection layer, resulting in the superior detection\neffect. By using the proposed object detection architecture SR-\nYOLO, it can demonstrate the promising results of recognizing\nand classifying both normal and small even tiny objects in\naerial images.\nIII. M ETHOD\nIn order to enable the conventional object detector to\naddress object occlusion, large scale difference and complex\nbackground simultaneously when recognizing and classifying\nthe small even tiny objects in aerial images, we propose a\nnovel SR-YOLO model through the three elaborated compo-\nThis article has been accepted for publication in IEEE Transactions on Consumer Electronics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCE.2023.3330788\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nnents, namely an Efficient Neck, Shape CIoU and Recursion\nBottleneck Transformer and implement its two variants (SR-\nYOLOv5 and SR-YOLOv8), which have been testified the de-\ntection performance in our experiments so that the consumer-\nlevel UA Vs equipped with our method could bring better object\ndetection experience to the consumers. The detail of the three\nproposed components is described as follows.\nA. Efficient Neck\nThere are plenty of small (objects smaller than 32*32\npixels are defined in COCO [12] as small objects) and tiny\n(objects smaller than 20*20 pixels are defined as tiny objects\nin TinyPerson [43]) objects in aerial images. Meanwhile,\nobjects from different classes could be of dramatic scale\nvariance. However, the YOLO’s neck network, for instance,\nthe nearest-neighbor up-sampling used by YOLOv5’s Neck\nprevents the effective propagation of high-level semantic in-\nformation. YOLOv5’s three-prediction head structure is only\nsuitable for medium and large-sized object detection tasks. To\nfill these gaps, we propose a neck network structure composed\nof content-aware reassembly of features and small object path\naggregation network neck to improve detection of tiny objects.\nThis structure includes a small object detection layer and a\nCARAFE up-sampling.\nOn the one hand, the vanilla YOLOv5 uses 8, 16, and 32\ntimes down-sampling, and the feature map obtained is hard to\npreserve the informative feature of small objects. Therefore,\na small object detection layer is employed at four times\ndown-sampling to predict a larger resolution image, and an\nadditional detection head is added to alleviate the problem\nof drastic scale changes. The nearest neighbor up-sampling\nemployed by YOLOv5 is simple and efficient, but it only uses\nthe gray value closest to the sampling point. As a result, it\nproduces discontinuous gray values after sampling, resulting\nin low image quality. Using this approach is difficult for deep\nnetworks to learn the features of smaller objects, which affects\nthe model’s detection accuracy. For this issue, we considered\nreplacing the up-sampling component with one more suitable\nfor aerial images.\nConsider that CARAFE reassembles the feature informa-\ntion from the lower-resolution input feature map to generate\na higher-resolution output feature map, allowing for more\neffective up-sampling for objects with small pixel. Through\ncombining with CARAFE, small object detection layer could\nincorporate context information from surrounding regions to\nenhance the representation of small objects. Thus we em-\nploy this content-aware up-sampling operator and a small\nobject detection layer to construct the Efficient Neck, as\nshown in Figure1.Compared with the original Neck using\nnearest neighbor up-sampling, the Efficient Neck part with\nCARAFE has a larger up-sampling kernel, which allows it\nto summarize contextual information over a large receptive\nfield. CARAFE performs content-aware processing for specific\ninstances, generating adaptive kernels on-the-fly. The up-\nsampling kernel and the feature map are semantically related\nsuch that CARAFE can be based on the input content. As a\nresult, CARAFE better preserves feature information for small\nobjects. Thus Efficient Neck can better utilize the surrounding\nand semantic information of the feature map. Hence, additional\nfeature fusion layer is added and prediction head into Efficient\nNeck.As opposed to the three-prediction head in the vanilla\nmodel, a four-prediction head can better handle the drastic\nscale changes of different objects. The Efficient Neck has a\nsignificant impact on improving detection in scenarios that\ninvolve small objects whose scale changes drastically, resulting\nin missed and false detections. We verify this in the ablation\nexperiments in Section 4.\nFig. 1. Composition of the SR-YOLO structure.\nB. Shape CIoU\nThe loss function for predicted box regression in object\ndetection tasks generally consists of the classification loss\nand the bounding box regression loss. For bounding box\nregression loss, the most commonly used CIoU introduces\ntwo considerations based on the GIOU. The ratio between\nρ and c (as shown in Figure 2) and the penalty/consistency\nterm for the aspect ratio of the real box and the predicted\nbox. ρ is the Euclidean distance between the centre b of the\nreal box and the center bgt of the predicted box. The dotted\nrectangle is the minimum bounding box containing two boxes,\nand c is minimum closure diagonal. The equation of CIoU is\nshown in equation1-3, where V is for calculating the aspect\nratio between the predicted box and the ground truth box, and\nwgt and hgt represent the width and height of the real box,\nrespectively. α is the balance factor of V .\nICIoU = 1− IoU + ρ2(b, bgt)\nc2 + V α (1)\nV = 4\nπ2 + (arctan wgt\nhgt − arctan w\nh )2 (2)\nα = V\n(1 − IoU ) +V (3)\nHowever, as figure 3 shows, when the aspect ratio of the two\nsets of real boxes and the predicted box is equal, the aspect\nratio will fail. The model’s learning capability will weaken by\nthis loss function. To address the above problem we design a\nSCIoU loss function as shown in equation 4. Vs is a newly\nThis article has been accepted for publication in IEEE Transactions on Consumer Electronics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCE.2023.3330788\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nFig. 2. diagonal diagram of minimum closure.\nFig. 3. Green boxes represent real boxes, and two red boxes represent\npredicted boxes of different sizes.\ndesigned penalty with ratio-curbing factor (X) , as shown in\nequation 5. The ratio-curbing factor changes the shortcomings\nof CIoU that only focuses on the aspect ratio. With the addition\nof the ratio-curbing factor, SCIoU is able to focus on both\nthe aspect ratio and the ratio of the widths of the two boxes.\nSCIoU allows the model to be optimized in terms of both the\naspect ratio and the ratio of the two widths in order to avoid\nthe degradation of the loss function as much as possible.\nISCIoU = 1− IoU + ρ2(b, bgt)\nc2 + Vsαs (4)\nVs = 4\nπ2 + (arctan wgt\nhgt − arctan w\nh )2X (5)\nHere the ratio-curbing factor is denoted as X, as shown in\nequation 6. When the value of X is lower, the predicted box\nis closer to the real box, the ratio of the width/height of the\nreal box to the width/height of the predicted box tends to 1.\nSince CIoU has a combined effect on the aspect ratio, a similar\neffect can still be produced if the width w is replaced by h.\nBased on the above Vs and X, the new balance factor can be\nexpressed in a new form, as shown in equation 7.\nXw = (1− w\nwgt )2 (6)\nαs = Vs\n(1 − IoU ) +Vs\n(7)\nC. Recursion Bottleneck Transformer\nIn UA V images, it is prevalent that a number of objects\nare in close proximity to each other, thus from different per-\nspectives it seems overlapped between objeacts and forms\nsevere occlusions. On the other hand, image quality change\ncaused by non-uniform environmental illumination is unavoid-\nable during drone operations, which would increase the rate of\nmissed and false detection. The convolution structure utilzed\nin YOLOv5 extracts limited local signals instead of contextual\ninformation. Global features can help the model to accurately\nlocalize in the presence of dense backgrounds and heavy oc-\nclusions in aerial images. Therefore global feature information\nis very important for small object detection. Transformer’s\ndynamic attention mechanism, global modeling capability has\na strong feature learning ability.Transformer can extract rich\nglobal feature information. DETR, the first object detection\nmodel based on transformer, greatly simplifies the object\ndetection process in an end-to-end fashion. Srinivas [44] et\nal. replaced the convolution operator in the last residual block\nof ResNet with a multi-head self-attention (MHSA) module\n, allowing the network model to improve accuracy while\nreducing the number of parameters. Shen [45] et al. uses the\nrecursive structure in recursive neural network to construct an\niterative transformer block and applies it to the DEiT model.\nTheir experimental results show that this simple recursive\noperation is capable of enhancing feature representation.\nInspired by the above research work, we propose a Recur-\nsive Bottleneck Transformer (RBoT), which has two variants\nof RBoT-a and RBoT-b, as shown in Figure 4.To be spe-\ncific, RBoT-a is constructed with multiple MHSA layers in a\nmanner of recursion based on the BoT ??Alternatively, RBoT-\nb furnishes each subsequent MHSA layer with an additional\nresidual connection operation, as shown in equation 8 and 9.\nBased on the pilot experimental results analysis, we only use\none cycle (namely, two MHSA layers) when implementing this\nproposed module. Although RBoT-b is more complex than\nRBoT-a, the ablation experiment in Section 4.3 shows that\nit has not yet produced more parameters and the detection\naccuracy is slightly improved.\nM(X)a = MSHA (MSHA (f1×1(X))) +X (8)\nM(X)b = MSHA (MSHA (f1×1(X)) +f1×1(X)) +X (9)\nM(X)a and M(X)b represent the outputs of RBoT-a\nand RBoT-b, respectively. MSHA means Multi-Head Self-\nAttention mechanism, and f(1×1) is a convolution operation\nwith a convolution kernel size of 1x1. The MHSA layer\nstructure is shown in Figure 5, where four heads are used\nin RBoT, and only one is shown here for simplicity. The\nrepresentation of the Self-Attention Layer is consistent with\nthat in BoT.\nIt’s worth noting that the RBoT module can be flexibly\napplied to the multiple versions of YOLO, thus actually we\nThis article has been accepted for publication in IEEE Transactions on Consumer Electronics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCE.2023.3330788\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nFig. 4. The architecture of the Recursion Bottleneck Transformer.\nFig. 5. Composition of the MHSA layer structure.\nrepalce the bottleneck module in C3 with the RBoT module\nin both YOLOv5 and YOLOv8 for practice, where a new\nRBoTC3 module is represented. The module obtains rich\ncontextualized information and enhances the ability to capture\ndifferential local signals because it has strong global modeling\nand feature learning capabilities. RBoT makes the model more\neffective in complex background and partially occluded object\ndetection. Compared to simultaneously replacing multiple C3\nmodules with RBoTC3, we only substitute the last C3 in the\nbackbone network because the former way will significantly\nincrease the computational cost, while the latter one can\nalready retain the convolution translation invariance and local\nfeature extraction ability of the model.\nIV. E XPERIMENT AND ANALYSIS\nTo investigate the effectiveness of the model, we conduct\nthe extensive experiments on the two classic and public UA V\nimage datasets, VisDrone2019 [46] and Tiny Person [43].\nA. Datasets and Experimental Setup\nThe VisDrone2019 dataset [46], released by Tianjin Uni-\nversity, consists of images taken by various drones under\nchanging weather, diverse scenes and different attitudes. These\ndrone-captured images abound in small/tiny objects with\ncomplex environments and serious occlusion problems. The\ndataset covers ten predefined categories: pedestrians, people,\ncars, vans, buses, trucks, motorcycles, bicycles, and awning\ntricycles.\nThe TinyPerson dataset [43] is the first benchmark dataset\nfor person detection with long-distance backgrounds released\nby the University of Chinese Academy of Sciences in 2019.\nIt contains two types of person in the sea and on land (sea\nperson and earth person). Low resolution (less than 20 pixels)\nand great variation of aspect ratio for the person are two main\ncharacteristics in this dataset, where the pose and angle of the\nperson is variable, and some of the images have more than\n200 tasks.\nAll the experiments in this paper were carried out on the\nA100 server with 80 G Video storage and Pytorch 1.12.1. In\nthe training phase, the number of training rounds is set to 150,\nthe batch size is 16, and the rest are the same as the default\nparameters of YOLOv5.\nIn order to accurately evaluate the performance of the\nalgorithm, we select Average Precision (AP), Mean Average\nPrecision (mAP) and Recall as the evaluation indicators of\nthis experiment. AP can reflect the detection performance of a\nsingle object category. Mean Average Precision measures the\ncomprehensive detection performance of all categories.\nB. Experimental Result\nIn this section, we choosed YOLOv5s and YOLOv8nas\nour baseline model and show the comparison results between\nSR-YOLO and the other state-of-the-artmodels on the two\ndatasets. Specifically,SR-YOLO is compared with RetinaNet\n[47], CenterNet [48], CornerNet [49], YOLOv4, YOLOv5s,\nYOLOv5m, YOLOx-s, the lastest YOLOv7-tiny [50] and\nthe currently advanced YOLOv8 [51] on metrics mAP50,\nmAP50:95 and recall. From the Table I we can find that SR-\nYOLO obtains higher mAP50, mAP50:95 and Recall than all\nthe other models without adding too many parameters. For\nexample, compared to YOLOv5m, SR-YOLO has only 36.4%\nof its parameters, but mAP50:95 are increased by 8.14%. SR-\nYOLO had 32% fewer parameters than YOLOv8s and was\n2% higher at mAP50:95. It can be seen the superiority of SR-\nYOLO over the other algorithms on UA V aerial images since\nit can obtain better detection results with a lower number of\nparameters. Furthermore, SR-YOLO even surpass YOLOv8s\nwhile reducing its number of parameters by 32.1% In addition,\nto further verify the effectiveness of the proposed model,\nYOLOv3-tiny, YOLOv3, YOLOx-s, YOLOv5m were selected\nfor comparison and validation on the TinyPerson dataset,on\nwhich the experimental results in Table II show that SR-YOLO\noutperforms YOLOv5s by 3.2 %, 1.01 %, and 3.5% in terms of\nThis article has been accepted for publication in IEEE Transactions on Consumer Electronics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCE.2023.3330788\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTABLE I\nRESULTS OF DIFFERENT MODELS ON VISDRONE .\nMethods parm mAP50 mAP50:95 Recall\nRetinaNet [47] – 28.7 11.8 –\nCenterNet [48] 32.67 22.7 12.4 –\nFaster-RCNN [20] – 33.2 17.0 –\nCornerNet [49] – 34.1 17.4 –\nYOLOv4 [11] 64.36 31.2 16.8 –\nYOLOXs [24] 8.94 33.9 18.7 –\nYOLOv5s 7.04 33.5 18.5 34.2\nYOLOv5m 20.89 38.4 22.1 38.0\nYOLOv7tiny [50] 6.2 34.4 17.7 37.3\nYOLOv8n [51] 3.012 33.50 19.27 33.62\nYOLOv8s [51] 11.2 39.54 23.43 38.54\nSR-YOLO 7.61 41.6 23.9 40.9\nSR-YOLOv8n 2.84 37.99 22.54 37.18\nTABLE II\nRESULTS OF DIFFERENT MODELS ON TINYPERSON .\nMethods parm mAP50 mAP50:95 Recall\nCornerNet [49] – 27.9 8.61 –\nYOLOv3tiny [10] 8.47 10.60 2.66 –\nYOLOv3 [10] 60.09 28.12 8.61 29.4\nYOLOXs [24] 8.94 28.9 8.79 –\nYOLOv5s 7.04 27.1 8.23 29.7\nYOLOv5m 20.91 30.2 9.22 33\nYOLOv7tiny [50] 6.2 24.2 7.1 27.8\nSPD-CONV [52] 8.5 26.20 7.59 29.1\nYOLOv8n [51] 3.012 23.50 7.70 25.90\nSR-YOLO 7.61 30.3 9.24 32.8\nSR-YOLOv8n 2.84 29.2 9.68 33.4\nmAP50, mAP50:95 and recall, respectivelyIn summary, both of\nvalidations on the two benchmarks testify that our SR-YOLO\noutperforms the YOLOv5s model in all the metrics, prov-\ning the effectiveness of the proposed three components.SR-\nYOLOv8n similarly outperforms the YOLOv8n model in all\nmetrics, demonstrating the generalizability of the proposed\ncomponents.\nFig. 6. box-Loss comparison chart.\nC. Ablation Experiment\nTo comprehensively verify the effectiveness of Efficient\nNeck, SCIoU, and RBoT, we conducted a series of ablation\nstudies and use YOLOv5s as the benchmark model (and de-\nnote SR-YOLOv5 as SR-YOLO for simplicity in the following\ndescription). As shown in Table III , “E.”, “S.”, “R.” represent\nour Efficient Neck, Shape CIoU, and RBoT, respectively.\nWe can observe that individually adding S. and E. to the\nbaseline(YOLOv5s) implementation improves its performance\nby 0.6 and 4.8 in terms of mAP50.It is expected to witness\nthe efficient neck component archives the most gain because\nof E’s small object detection layer as well as up-sampling are\ndesigned to cue small object detection rates.\nBased on this module, S. and R. are added separately, it\nshows that the mAP50:95 of the model increased by 6.1%\nand 7.2 %. Finally, after combining all the innovative parts\nabove, the SR-YOLO is 8.1%, 5.4%, and 6.7% higher than the\nbenchmark method in terms of mAP50, mAP50:95 and recall,\nrespectively. The comprehensive studied result demonstrates\nthe effectiveness of the proposed model, which can signifi-\ncantly improve the detection accuracy compared to YOLOv5s\nwith only 0.5728M increase in the number of parameters.\nTo observe the specific accuracy of SR-YOLO on each class\nof the objects, it shows the specific detection effect for them\nin Table IV . From the table it can be seen that it gets the\nbest detection with mAP50 of 56.3% on the category of Car,\nfollowed by Bus with mAP50 of 42.8%, while objects with\nvery small size such as Bicycle and Awning-tricycle have\npoorer detection, only gaining mAP50 of 15.9% and 13.5%.\nIn the following experiment, we performed more detailed\nablation experiments for each of the three components:\nEfficient Neck: Firstly, we summed up the changes in\nparameters and accuracy after adding a small object detection\nlayer (P2) and CARAFE on YOLOv5. After integrating the\nP2 layer, the parameter amount increases by 0.68 M, and the\naccuracy is greatly improved. As shown in Table V , YOLOv5\nwith Efficient Neck is 4.8 % higher than the benchmark\nmethod on mAP50.\nSCIoU: To visually demonstrate the effect of SCIoU, we\nselected DIoU, CIoU and SCIoU to conduct comparative\nexperiments on the model after adding Efficient Neck. From\nTable we can see that the SCIoU improves 0.5% and 0.3% in\nmAP50 compared with DIoU and CIoU, respectively.\nIn addition, to demonstrate the role of the control ratio factor\nin SCIoU, we compared the status under the value changes of\nCIoU and SCIoU.Figure 6 shows that the proposed SCIoU\nmodel has a lower loss value and converges faster than the\nsame structure with CIoU. The final result of SCIoU gains\nonly half of the Loss value of CIoU (i.e., 0.044469)\nRBoT: Finally, to compare the effect of the recursion Multi-\nHead Self-Attention module RBoT and the BoT module, we\nconducted our experiments under the four settings, namely\nadding no BoT(-BoT), original BoT (+BoT), RBoT-a and\nRBoT-b together with Efficient Neck and SCIoU. From Table\nVI it can be seen that mAP50 increased by 0.5%, and the\nparameter decreased by 0.11 M after adding BoT. Instead,\nour proposed RBoT has a lower number of parameters than\nthe original BoT, reducing the model by 0.34 M. Meanwhile,\nthe model equipped with both RBoT-a and RBoT-b is 1.6%\nThis article has been accepted for publication in IEEE Transactions on Consumer Electronics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCE.2023.3330788\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTABLE III\nTHE EFFECTS OF THE YOLOV5 COMBINING DIFFERENT MODULES ON THE VISDRONE .\nMethods parm mAP50 mAP50:95 Recall\nYOLOv5 7.037 33.5 18.5 34.2\nYOLOv5+SCIoU 7.037 34.1 18.9 34.7\nYOLOv5+Efficient Neck 7.948 38.3 21.8 37.3\nYOLOv5+Efficient Neck+SCIoU 7.948 39.6 22.7 38.6\nYOLOv5+Efficient Neck+RBoT-b 7.61 40.7 23.1 39.8\nYOLOv5+Efficient Neck+RBoT-b+SCIoU 7.61 41.6 23.9 40.9\nTABLE IV\nRECOGNITION EFFECT OF EACH CLASS ON SR-YOLO.\nClass Precision Recall mAP50 mAP50:95\nall 51.8 40.9 41.6 23.9\nPedestrian 54.8 46.4 49.1 21.6\nPeople 52.4 36.2 37.5 14.1\nBicycle 33.3 18.1 15.9 6.15\nCar 69.7 81.2 81.9 56.3\nVan 54.6 42.4 43.6 30.5\nTruck 55.3 36.8 38.4 24.2\nTricycle 44.8 30.5 27.8 14.8\nAwning-tricycle 24.8 16.7 13.5 8.26\nBus 71.9 55 61.7 42.8\nMotor 56.5 45.3 46.2 19.8\nTABLE V\nPRECISION COMPARISON OF DIOU, CI OU AND SCI OU.\nMethods mAP50 mAP50:95 Recall\nDIoU [33] 39.1 22.4 38.5\nCIoU [33] 39.3 22.5 38.6\nSCIoU 39.6 22.7 38.6\nhigher than with BoT on mAP50, and 0.8%, 1.1% higher\non mAP50:95, respectively. Recall also improved significantly.\nOverall, the performance of RBoT-b is slightly better than\nRBoT-a.\nD. Case study\nConsider that it may encounter complicated situations when\na UA V is working in real environment, we have selected\nseveral representative sets of UA V images from both VisDrone\nand TinyPerson for case study of intricate conditions. To\nanalyze the above experimental results, this section shows\nthe effect of the proposed model on the example images of\nVisDrone and TinyPerson datasets.\nVisDrone: As shown in Figure 7, the left side is the\nYOLOv5 benchmark method, and the right side is the SR-\nYOLO algorithm, where the white box is the area with\nTABLE VI\nCOMPARISON OF BOT, RB OT-A, RB OT-B PARAMETER QUANTITIES AND\nACCURACY .\nMethods parm mAP50 mAP50:95 Recall\n-BoT 7.9479 39.5 22.6 38.6\n+BoT 7.8366 40 22.8 39.1\n+RBoT-a 7.61 41.6 23.6 41.1\n+RBoT-b 7.61 41.6 23.9 40.9\na more obvious contrast effect. Group (a) shows the most\ncommon images using low altitude electronic devices such\nas UA Vs at transportation hubs. The targets in these scenarios\nare overlapped and occluded. Group (b) selected scenes with\nhigh contrast and complex backgrounds to test the model’s\nability to resist interference. Group (c) shows a scene of traffic\nmonitoring using UA V equipment at night. This type of scene\nis darker and vehicles are blurred on the image due to high\nspeed movement. (d) An image showing a situation where\na vehicle is obscured by a tree while the UA V is checking\nstreet parking. In group (a), it is evident that the detection\neffect of YOLOv5 fails to detect many partially obscured cars\nand cyclists. In contrast, the improved model detects more\nobscured objects than the original model, and the boxes for\ndetecting people on cars is accurate from the whole two-\nwheeler to people on cars. Group(b) shows that in the case\nof backlight, building shadow, and complex background, the\noriginal model can only two targets can be detected, while SR-\nYOLOv5 correctly detected four objects in such scenarios. In\ngroup (c), the complex background and the blurred vehicles\nat night with high speed are selected, in which the original\nmodel identifies the white objects on the roof as vehicles\nand the moving cars as trucks, while the model in this paper\ncan correctly identify the category of the objects; in group\n(d), the original model does not detect the vehicles waiting\nfor red lights in the distance and the small two-wheelers in\nthe middle of the road, but SR-YOLO can see these two\ncategories more accurately. From the investigation of the\nvarious cases that objects are hard to distinguish, we can see\nthat our proposed model have been tested in different ambient\nlights, including backlight, low light and drop shadows of the\nbuilding, indicating the robustness of our SR-YOLO under\npoor lighting conditions. Meanwhile, the model also shows\nits excellent performance in traffic intersections when there\nare lots of person and vehicles.These comparisons show that\nthe improved model is more suitable for UA V aerial image\ndetection.\nTinyPerson: The comparison results of the YOLOv5 model\nand SR-YOLO detection are shown in Figure 8, with YOLOv5\non the left and the improved algorithm on the right. We\nselected (a) (b) (c) three sets of images using a drone Here\nwe select the sub-figures (a), (b) and (c) to examine the\ndistribution of people on a beach with tiny characters and a\nhigh amount of useless information in the background. In these\nthree sets of images can be seen that the original YOLOv5\ncan barely detect tiny and dense people in the distance. In the\nThis article has been accepted for publication in IEEE Transactions on Consumer Electronics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCE.2023.3330788\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nFig. 7. Test effect comparison effect.\ngroup (d) of fig 6 we have selected the noise images with\naboundant scale-varying ships images where there is a lot\nof background noise (ships) for comparison. From the sub-\nfigure, it can be seen that the proposed method can detect\nmore person compared to the benchmark. Meanwhile, the\noriginal YOLOv5 recognizes a large number of sea person\nas earth person, instead, SR-YOLO accurately recognizes the\ntype of people. In contrast, SR-YOLO can accurately detect\nmore people and objects compared to the original model. (The\nexperiments show that this model is better than the YOLOv5\nmodel for tiny and dense object images.\nV. C ONCLUSION\nIn this paper, we have presented a novel small object\ndetection model SR-YOLO, which consists of an efficient\nneck, a newly designed prediction box loss function and a\nrecursive bottleneck transformer, exploring the ability to tackle\nthe object occlusion, light variation and intricate background in\nUA V images. It suggests a new YOLO paradigm for utilizing\nin electronic products like drones because it can be flexibly\nintegrated into different versions of YOLO. Moreover, the\nproposed method is efficient due to its number of parameters\nand accuracy. Our study shows that designing and training\nlightweight modules of object detectors for application of\nelectronics is an interesting direction which deserves broader\nfocused researches. This work only make an attempt in ex-\nFig. 8. Comparison of TinyPerson Dataset Detection Effects.\nploration, and could be further improved in more aspects:\nhow to combine the attention mechanism and feature fusion\nto capture better detailed and semantic features, and how to\nfurther improve the real-time by efficient fusion strategy as\nwell as keep the accuracy for a lighter deployment in consumer\nelectronics.\nVI. A CKNOWLEDGMENT\nThe authors would like to acknowledge the support provided\nby Researchers Supporting Project number (2023J01954) the\nNatural Science Foundation of Fujian Province, China. The\nsupport provided by Researchers Supporting Project num-\nber (RSPD2023R868), King Saud University, Riyadh, Saudi\nArabia. Also Khalifa University of Science and Technology\nunder Faculty Start-Up grants FSU-2022-003 Award No.\n8474000401.\nREFERENCES\n[1] Muhammad Zawish, Lizy Abraham, Kapal Dev, and Steven Davy.\nTowards resource-aware dnn partitioning for edge devices with het-\nerogeneous resources. In GLOBECOM 2022-2022 IEEE Global\nCommunications Conference, pages 5649–5655. IEEE, 2022.\n[2] Muhammad Zawish, Nouman Ashraf, Rafay Iqbal Ansari, and Steven\nDavy. Energy-aware ai-driven framework for edge-computing-based iot\napplications. IEEE Internet of Things Journal, 10(6):5013–5023, 2022.\n[3] Muhammad Zawish, Steven Davy, and Lizy Abraham. Complexity-\ndriven cnn compression for resource-constrained edge ai. arXiv preprint\narXiv:2208.12816, 2022.\n[4] Kangcheng Liu and Ben M Chen. Industrial uav-based unsupervised\ndomain adaptive crack recognitions: From database towards real-site\ninfrastructural inspections. IEEE Transactions on Industrial Electronics,\n70(9):9410–9420, 2022.\n[5] Yang Zhiling. Review of the application of machine vision in aquacul-\nture uavs. In 2023 7th International Conference on Machine Vision and\nInformation Technology (CMVIT), pages 68–72. IEEE, 2023.\nThis article has been accepted for publication in IEEE Transactions on Consumer Electronics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCE.2023.3330788\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n[6] Lucas Prado Osco, Mauro dos Santos de Arruda, Diogo Nunes\nGonc ¸alves, Alexandre Dias, Juliana Batistoti, Mauricio de Souza, Felipe\nDavid Georges Gomes, Ana Paula Marques Ramos, L´ucio Andr´e de Cas-\ntro Jorge, Veraldo Liesenberg, et al. A cnn approach to simultaneously\ncount plants and detect plantation-rows from uav imagery. ISPRS\nJournal of Photogrammetry and Remote Sensing, 174:1–17, 2021.\n[7] Dunja Bo ˇzi´c-ˇStuli´c, ˇZeljko Maruˇsi´c, and Sven Gotovac. Deep learning\napproach in aerial imagery for supporting land search and rescue\nmissions. International Journal of Computer Vision, 127(9):1256–1278,\n2019.\n[8] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You\nonly look once: Unified, real-time object detection. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages\n779–788, 2016.\n[9] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 7263–7271, 2017.\n[10] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement.\narXiv preprint arXiv:1804.02767, 2018.\n[11] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao.\nYolov4: Optimal speed and accuracy of object detection. arXiv preprint\narXiv:2004.10934, 2020.\n[12] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro\nPerona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft\ncoco: Common objects in context. In Computer Vision–ECCV 2014:\n13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740–755. Springer, 2014.\n[13] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn,\nand Andrew Zisserman. The pascal visual object classes (voc) challenge.\nInternational journal of computer vision, 88:303–338, 2010.\n[14] Aftab Ahmed, Jiandong Guo, Fayaz Ali, Farha Deeba, and Awais\nAhmed. Lbph based improved face recognition at low resolution. In\n2018 international conference on Artificial Intelligence and big data\n(ICAIBD), pages 144–147. IEEE, 2018.\n[15] Farah Deeba, Hira Memon, Fayaz Ali Dharejo, Aftab Ahmed, and\nAbddul Ghaffar. Lbph-based enhanced real-time face recognition.\nInternational Journal of Advanced Computer Science and Applications,\n10(5), 2019.\n[16] Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, and Dahua\nLin. Carafe: Content-aware reassembly of features. In Proceedings of\nthe IEEE/CVF international conference on computer vision, pages 3007–\n3016, 2019.\n[17] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and\nDongwei Ren. Distance-iou loss: Faster and better learning for bounding\nbox regression. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pages 12993–13000, 2020.\n[18] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich\nfeature hierarchies for accurate object detection and semantic segmen-\ntation. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 580–587, 2014.\n[19] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 1440–1448, 2015.\n[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-\ncnn: Towards real-time object detection with region proposal networks.\nAdvances in neural information processing systems, 28, 2015.\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyra-\nmid pooling in deep convolutional networks for visual recognition. IEEE\ntransactions on pattern analysis and machine intelligence, 37(9):1904–\n1916, 2015.\n[22] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Girshick. Mask\nr-cnn. In Proceedings of the IEEE international conference on computer\nvision, pages 2961–2969, 2017.\n[23] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariha-\nran, and Serge Belongie. Feature pyramid networks for object detection.\nIn Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 2117–2125, 2017.\n[24] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox:\nExceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021.\n[25] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier,\nAlexander Kirillov, and Sergey Zagoruyko. End-to-end object detection\nwith transformers. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16,\npages 213–229. Springer, 2020.\n[26] Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints.\nIn Proceedings of the European conference on computer vision (ECCV),\npages 734–750, 2018.\n[27] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott\nReed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox\ndetector. In Computer Vision–ECCV 2016: 14th European Conference,\nAmsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part\nI 14, pages 21–37. Springer, 2016.\n[28] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang\nLi, Zaidan Ke, Qingyuan Li, Meng Cheng, Weiqiang Nie, et al. Yolov6:\nA single-stage object detection framework for industrial applications.\narXiv preprint arXiv:2209.02976, 2022.\n[29] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully con-\nvolutional one-stage object detection. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 9627–9636, 2019.\n[30] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for\nconvolutional neural networks. In International conference on machine\nlearning, pages 6105–6114. PMLR, 2019.\n[31] Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang, Qingqing\nDang, Yuan Gao, Hui Shen, Jianguo Ren, Shumin Han, Errui Ding, et al.\nPp-yolo: An effective and efficient implementation of object detector.\narXiv preprint arXiv:2007.12099, 2020.\n[32] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian\nReid, and Silvio Savarese. Generalized intersection over union: A metric\nand a loss for bounding box regression. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 658–666,\n2019.\n[33] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and\nDongwei Ren. Distance-iou loss: Faster and better learning for bounding\nbox regression. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pages 12993–13000, 2020.\n[34] Abdelmalek Bouguettaya, Hafed Zarzour, Ahmed Kechida, and\nAmine Mohammed Taberkit. Deep learning techniques to classify\nagricultural crops through uav imagery: A review. Neural Computing\nand Applications, 34(12):9511–9536, 2022.\n[35] Yu-Fang Huang, Tsung-Jung Liu, and Kuan-Hsien Liu. Improved small\nobject detection for road driving based on yolo-r. In 2022 IEEE\nInternational Conference on Consumer Electronics - Taiwan, pages 279–\n280, 2022.\n[36] Jia-Jiun Gung, Chia-Yu Lin, Pin-Fan Lin, and Wei-Kuang Chung. An\nincremental meta defect detection system for printed circuit boards. In\n2022 IEEE International Conference on Consumer Electronics-Taiwan,\npages 307–308. IEEE, 2022.\n[37] Wen-Hui Chen, Hsiu-Jui Hsu, and Yu-Chen Lin. Implementation of a\nreal-time uneven pavement detection system on fpga platforms. In 2022\nIEEE International Conference on Consumer Electronics - Taiwan, pages\n587–588, 2022.\n[38] Xingkui Zhu, Shuchang Lyu, Xu Wang, and Qi Zhao. Tph-yolov5:\nImproved yolov5 based on transformer prediction head for object\ndetection on drone-captured scenarios. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 2778–2788, 2021.\n[39] Raian Rahman, Zadid Bin Azad, and Md Bakhtiar Hasan. Densely-\npopulated traffic detection using yolov5 and non-maximum suppression\nensembling. In Proceedings of the International Conference on Big Data,\nIoT, and Machine Learning: BIM 2021, pages 567–578. Springer, 2022.\n[40] Munhyeong Kim, Jongmin Jeong, and Sungho Kim. Ecap-yolo: Efficient\nchannel attention pyramid yolo for small object detection in aerial image.\nRemote Sensing, 13(23):4851, 2021.\n[41] Tao Liu, Bo Pang, Lei Zhang, Wei Yang, and Xiaoqiang Sun. Sea surface\nobject detection algorithm based on yolo v4 fused with reverse depthwise\nseparable convolution (rdsc) for usv. Journal of Marine Science and\nEngineering, 9(7):753, 2021.\n[42] Mohamad Haniff Junos, Anis Salwa Mohd Khairuddin, Subbiah\nThannirmalai, and Mahidzal Dahari. Automatic detection of oil palm\nfruits from uav images using an improved yolo model. The Visual\nComputer, pages 1–15, 2021.\n[43] Xuehui Yu, Yuqi Gong, Nan Jiang, Qixiang Ye, and Zhenjun Han. Scale\nmatch for tiny person detection. In Proceedings of the IEEE/CVF winter\nconference on applications of computer vision, pages 1257–1265, 2020.\n[44] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter\nAbbeel, and Ashish Vaswani. Bottleneck transformers for visual recog-\nnition. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 16519–16529, 2021.\nThis article has been accepted for publication in IEEE Transactions on Consumer Electronics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCE.2023.3330788\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n[45] Zhiqiang Shen, Zechun Liu, and Eric Xing. Sliced recursive transformer.\nIn Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23–27, 2022, Proceedings, Part XXIV, pages 727–744.\nSpringer, 2022.\n[46] Dawei Du, Pengfei Zhu, Longyin Wen, Xiao Bian, Haibin Lin, Qinghua\nHu, Tao Peng, Jiayu Zheng, Xinyao Wang, Yue Zhang, et al. Visdrone-\ndet2019: The vision meets drone object detection in image challenge\nresults. In Proceedings of the IEEE/CVF international conference on\ncomputer vision workshops, pages 0–0, 2019.\n[47] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ´ar.\nFocal loss for dense object detection. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2980–2988, 2017.\n[48] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb¨uhl. Objects as points.\narXiv preprint arXiv:1904.07850, 2019.\n[49] Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints.\nIn Proceedings of the European conference on computer vision (ECCV),\npages 734–750, 2018.\n[50] Xuehui Yu, Yuqi Gong, Nan Jiang, Qixiang Ye, and Zhenjun Han. Scale\nmatch for tiny person detection. In Proceedings of the IEEE/CVF winter\nconference on applications of computer vision, pages 1257–1265, 2020.\n[51] Ultralytics, “YOLOv8,” 2022, Accessed 23 February 2023. [On-\nline].Available: https://github.com/ultralytics/ultraly.\n[52] Raja Sunkara and Tie Luo. No more strided convolutions or pooling:\nA new cnn building block for low-resolution images and small objects.\nIn Joint European Conference on Machine Learning and Knowledge\nDiscovery in Databases, pages 443–459. Springer, 2022.\nLe Wang received received his Ph.D\ndegree from the Institute of Computer\nNetwork Information Center Chinese\nAcademy of Sciences, University of\nChinese Academy of Sciences, Beijing,\nChina in 2021. He joined College of\nComputer Science and Mathematics of\nFJUT as an instructor and researcher.\nYu Shi received his bachelor degree from\nWuxi Taihu University in 2021. He is cur-\nrently pursuing a Master Degree in the Col-\nlege of Computer Science and Mathematics,\nFujian University of Technology, FuZhou,\nChina. His current research interests include\ncomputer vision and machine learning.\nGuo-jun Mao is currently the dean at\nFujian University of Technology, a member\nof the standing committee of Chinese Asso-\nciation for Artificial Intelligence, a member\nof National Science and Technology Awards\ncommittee, etc.: previously he was a pro-\nfessor at Central University of Finance and\nEconomics and a senior visiting scholar at\nUniversity of Vermont. He received his Ph.D. degree from the\nBeijing University Of Technology in 2003. He works in the\nfield of artificial intelligence, data mining, and big data and\ndistributed computing.\nFayaz Ali Dharejo is Senior IEEE\nMember and currently Postdoc fellow at\nKhalifa University. He received his PhD\nDegree in Computer Applied Engineering\nfrom the Institute of Computer Network\nInformation Center Chinese Academy of\nSciences, University of Chinese Academy\nof Sciences Beijing, China. He received\nB.E. degree in Electronic Engineering from QUEST, Pakistan\nand received M.E. degrees from University of Electronic\nScience and Technology of China, Chengdu, China. He has\npublished more than 30 articles in reputed ISI impact fac-\ntor journals including; IEEE TFS, ACM TIST, International\nJournal of Intelligent Systems, IEEE TCBB, IEEEE GRSL\nand etc. He is a member of professional bodies such as PEC,\nACM, IEEE, IEEE Societies, such as IEEE Geoscience and\nRemote Sensing Society, IEEE Computer Society and IEEE\nSignal Processing Society. His research interests are image\nenhancement, lightweight models, and computer vision.\nSajid Javed is a faculty member at\nKhalifa University (KU), UAE. Prior to\nthat, he was a research fellow at KU\nfrom 2019 to 2021 and at the University\nof Warwick, U.K, from 2017-2018. He\nreceived his B.Sc. degree in computer\nscience from the University of Hertfordshire, U.K, in 2010.\nHe completed his combined Master’s and a Ph.D. degree in\ncomputer science from Kyungpook National University, Re-\npublic of Korea, in 2017. His research interests include visual\nobject tracking in the wild, multi-object tracking, background-\nforeground modeling from video sequences, moving object\ndetection from complex scenes, and cancer image analytics\nincluding tissue phenotyping, nucleus detection, and nucleus\nclassification problems. His research themes involve develop-\ning deep neural networks, subspace learning models, graph\nneural networks.\nMOATH ALATHBAH received his\nPh.D. degree from Cardiff University,\nU.K. He is currently an Assistant Profes-\nsor with King Saud University, Saudi Ara-\nbia. His research interests include the de-\nvelopment of photoelectronic, integrated\nelectronic active and passive discrete de-\nvices, the design, fabrication, and charac-\nterization of MMIC, RF and THz components, smart anten-\nnas, microstrip antennas, microwave filters, meta-materials,\n5G antennas, MIMO antennas miniaturized multiband an-\ntennas/wideband and microwave/millimeter components using\nmicro and nano technology.\nThis article has been accepted for publication in IEEE Transactions on Consumer Electronics. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCE.2023.3330788\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5673357844352722
    },
    {
      "name": "Transformer",
      "score": 0.4204167127609253
    },
    {
      "name": "Electronic engineering",
      "score": 0.3249274492263794
    },
    {
      "name": "Engineering",
      "score": 0.26591378450393677
    },
    {
      "name": "Electrical engineering",
      "score": 0.25368329882621765
    },
    {
      "name": "Voltage",
      "score": 0.2234027087688446
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I83791580",
      "name": "Fujian University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I176601375",
      "name": "Khalifa University of Science and Technology",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I28022161",
      "name": "King Saud University",
      "country": "SA"
    }
  ]
}