{
  "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration",
  "url": "https://openalex.org/W3169769781",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2361843390",
      "name": "Chen, Tianlong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1973031759",
      "name": "Cheng Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2365636915",
      "name": "Gan, Zhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101029194",
      "name": "Yuan Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1842339085",
      "name": "Zhang Lei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2354751725",
      "name": "Wang, Zhangyang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2797569913",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3034733718",
    "https://openalex.org/W2907886210",
    "https://openalex.org/W3015982254",
    "https://openalex.org/W3111921445",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2963813662",
    "https://openalex.org/W2963247446",
    "https://openalex.org/W2948818370",
    "https://openalex.org/W2956434358",
    "https://openalex.org/W3000514857",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W2995197005",
    "https://openalex.org/W2919347051",
    "https://openalex.org/W2953488952",
    "https://openalex.org/W3127824473",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2976833415",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W3123338478",
    "https://openalex.org/W2948635472",
    "https://openalex.org/W2915589364",
    "https://openalex.org/W3101584733",
    "https://openalex.org/W3099365437",
    "https://openalex.org/W2995492258",
    "https://openalex.org/W3035180000",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2114766824",
    "https://openalex.org/W2520760693",
    "https://openalex.org/W2962851801",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2965862774",
    "https://openalex.org/W3203309275",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W3167122801",
    "https://openalex.org/W3129197243",
    "https://openalex.org/W3169793979",
    "https://openalex.org/W3119795462",
    "https://openalex.org/W2808970127",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W2900282770",
    "https://openalex.org/W3038428716",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W3035081900",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W3135285736",
    "https://openalex.org/W3180659574",
    "https://openalex.org/W3120519792",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2952344559",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W2907087576",
    "https://openalex.org/W2986922898",
    "https://openalex.org/W3102093079",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W2945146780",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W3158375352",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W2766839578",
    "https://openalex.org/W2963363373",
    "https://openalex.org/W3101415077",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W2337788193",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W3098372854",
    "https://openalex.org/W2285660444",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2969876226"
  ],
  "abstract": "Vision transformers (ViTs) have recently received explosive popularity, but their enormous model sizes and training costs remain daunting. Conventional post-training pruning often incurs higher training budgets. In contrast, this paper aims to trim down both the training memory overhead and the inference complexity, without sacrificing the achievable accuracy. We carry out the first-of-its-kind comprehensive exploration, on taking a unified approach of integrating sparsity in ViTs \"from end to end\". Specifically, instead of training full ViTs, we dynamically extract and train sparse subnetworks, while sticking to a fixed small parameter budget. Our approach jointly optimizes model parameters and explores connectivity throughout training, ending up with one sparse network as the final output. The approach is seamlessly extended from unstructured to structured sparsity, the latter by considering to guide the prune-and-grow of self-attention heads inside ViTs. We further co-explore data and architecture sparsity for additional efficiency gains by plugging in a novel learnable token selector to adaptively determine the currently most vital patches. Extensive results on ImageNet with diverse ViT backbones validate the effectiveness of our proposals which obtain significantly reduced computational cost and almost unimpaired generalization. Perhaps most surprisingly, we find that the proposed sparse (co-)training can sometimes improve the ViT accuracy rather than compromising it, making sparsity a tantalizing \"free lunch\". For example, our sparsified DeiT-Small at (5%, 50%) sparsity for (data, architecture), improves 0.28% top-1 accuracy, and meanwhile enjoys 49.32% FLOPs and 4.40% running time savings. Our codes are available at https://github.com/VITA-Group/SViTE.",
  "full_text": "Chasing Sparsity in Vision Transformers:\nAn End-to-End Exploration\nTianlong Chen1, Yu Cheng2, Zhe Gan2, Lu Yuan2, Lei Zhang3, Zhangyang Wang1\n1University of Texas at Austin,2Microsoft Corporation, 3International Digital Economy Academy\n{tianlong.chen,atlaswang}@utexas.edu,{yu.cheng,zhe.gan,luyuan}@microsoft.com\nleizhangcn@ieee.org\nAbstract\nVision transformers (ViTs) have recently received explosive popularity, but their\nenormous model sizes and training costs remain daunting. Conventional post-\ntraining pruning often incurs higher training budgets. In contrast, this paper aims\nto trim down both the training memory overhead and the inference complexity,\nwithout sacriﬁcing the achievable accuracy. We carry out the ﬁrst-of-its-kind\ncomprehensive exploration, on taking a uniﬁed approach of integrating sparsity in\nViTs “from end to end”. Speciﬁcally, instead of training full ViTs, we dynamically\nextract and train sparse subnetworks, while sticking to a ﬁxed small parameter\nbudget. Our approach jointly optimizes model parameters and explores connectivity\nthroughout training, ending up with one sparse network as the ﬁnal output. The\napproach is seamlessly extended from unstructured to structured sparsity, the\nlatter by considering to guide the prune-and-grow of self-attention heads inside\nViTs. We further co-explore data and architecture sparsity for additional efﬁciency\ngains by plugging in a novel learnable token selector to adaptively determine the\ncurrently most vital patches. Extensive results on ImageNet with diverse ViT\nbackbones validate the effectiveness of our proposals which obtain signiﬁcantly\nreduced computational cost and almost unimpaired generalization. Perhaps most\nsurprisingly, we ﬁnd that the proposed sparse (co-)training can sometimes improve\nthe ViT accuracy rather than compromising it, making sparsity a tantalizing “free\nlunch”. For example, our sparsiﬁed DeiT-Small at (5%, 50%) sparsity for (data,\narchitecture), improves 0.28% top-1 accuracy, and meanwhile enjoys 49.32%\nFLOPs and 4.40% running time savings. Our codes are available at https:\n//github.com/VITA-Group/SViTE.\n1 Introduction\nRecent years have seen substantial efforts devoted to scaling deep networks to enormous sizes.\nParameter counts are frequently measured in billions rather than millions, with the time and ﬁnancial\noutlay necessary to train these models growing in concert. The trend undoubtedly continues with\nthe recent forefront of transformers [1–3] for computer vision tasks. By leveraging self-attention,\nreducing weight sharing such as convolutions, and feeding massive training data, vision transformers\nhave established many new state-of-the-art (SOTA) records in image classiﬁcation [1, 2], object de-\ntection [4–7], image enhancement [8, 9], and image generation [10–12]. Existing vision transformers\nand variants, despite the impressive empirical performance, have in general suffered from gigantic\nparameter-counts, heavy run-time memory usages, and tedious training. That naturally calls for the\nnext step research of slimming their inference and training, without compromising the performance.\nModel compression and efﬁcient learning are no strangers to deep learning researchers, although their\nexploration in the emerging vision transformer ﬁeld remains scarce [13]. Among the large variety\nof compression means [14], sparsity has been one of the central themes since the beginning [ 15].\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2106.04533v3  [cs.CV]  22 Oct 2021\nConventional approaches ﬁrst train dense networks, and then prune a large portion of parameters in\nthe trained networks to zero. Those methods signiﬁcantly reduce the inference complexity. However,\nthe price is to cost even more signiﬁcant computational resources and memory footprints at training,\nsince they commonly require (multiple rounds of) re-training to restore the accuracy loss [15–17].\nThat price becomes particularly prohibitive for vision transformers, whose vanilla one-pass training is\nalready much more tedious, slow, and unstable compared to training standard convolutional networks.\nAn emerging subﬁeld has explored the prospect of directly training smaller, sparse subnetworks in\nplace of the full networks without sacriﬁcing performance. The key idea is to reuse the sparsity\npattern found through pruning and train a sparse network from scratch. The seminal work of lottery\nticket hypothesis (LTH) [18] demonstrated that standard dense networks contain sparse matching\nsubnetworks (sometimes called “winning tickets”) capable of training in isolation to full accuracy.\nIn other words, we could have trained smaller networks from the start if only we had known which\nsubnetworks to choose. Unfortunately, LTH requires to empirically ﬁnd these intriguing subnetworks\nby an iterative pruning procedure [18–27] , which still cannot get rid of the expensiveness of post-\ntraining pruning. In view of that, follow-up works reveal that sparsity patterns might emerge at the\ninitialization [28, 29], the early stage of training [30, 31], or in dynamic forms throughout training\n[32–34] by updating model parameters and architecture typologies simultaneously. These efforts shed\nlight on the appealing prospect of “end to end” efﬁciency from training to inference, by involving\nsparsity throughout the full learning lifecycle.\nThis paper presents the ﬁrst-of-its-kind comprehensive exploration of integrating sparsity in vision\ntransformers (ViTs) “from end to end”. With (dynamic) sparsity as the uniﬁed tool, we can improve\nthe inference efﬁciency from both model and data perspectives, while also saving training memory\ncosts. Our innovative efforts are unfolded along with the following three thrusts:\n• From Dense to (Dynamic) Sparse: Our primary quest is to ﬁnd sparse ViTs without\nsacriﬁcing the achievable accuracy, and meanwhile trimming down the training memory\noverhead. To meet this challenging demand, we draw inspirations from the latest sparse\ntraining works [34, 35] that dynamically extract and train sparse subnetworks instead of\ntraining the full models. Sticking to a ﬁxed small parameter budget, our technique jointly\noptimizes model parameters and explores connectivity throughout the entire training process.\nWe term our ﬁrst basic approach as Sparse Vision Transformer Exploration(SViTE).\n• From Unstructured to Structured: Most sparse training works [32, 33, 36–39, 38, 34, 40,\n41, 35] restricted discussion to unstructured sparsity. To attain structured sparsity which\nis more hardware-friendly, unlike classical channel pruning available for convolutional\nnetworks, we customize a ﬁrst-order importance approximation [16, 42] to guide the prune-\nand-grow of self-attention heads inside ViTs. This seamlessly extends SViTE to its second\nvariant of Structured Sparse Vision Transformer Exploration(S2ViTE).\n• From Model to Data: We further conduct a uniﬁed co-exploration towards joint data and\narchitecture sparsity. That is by plugging in a novel learnable token selector to determine\nthe most vital patch embeddings in the current input sample. The resultant framework of\nSparse Vision Transformer Co-Exploration(SViTE+) remains to be end-to-end trainable\nand can gain additional efﬁciency.\nExtensive experiments are conducted on ImageNet with DeiT-Tiny/Small/Base. Results of substantial\ncomputation savings and nearly undamaged accuracies consistently endorse our proposals’ effec-\ntiveness. Perhaps most impressively, we ﬁnd that the sparse (co-)training can evenimprove the ViT\naccuracy rather than compromising it, making sparsity a tantalizing “free lunch”. For example, apply-\ning SViTE+ on DeiT-Small produces superior compressed ViTs at50% model sparsity plus 5% data\nsparsity, saving 49.32% FLOPs and 4.40% running time, while attaining a surprising improvement\nof 0.28% accuracy; even when the data sparsity increases to 10% (the model sparsity unchanged),\nthere is still no accuracy degradation, meanwhile saving 52.38% FLOPs and 7.63% running time.\n2 Related Work\nVision Transformer. Transformer [43] stems from natural language processing (NLP) applications.\nThe Vision Transformer (ViT) [1] pioneered to leverage a pure transformer, to encode an image by\nsplitting it into a sequence of patches, projecting them into token embeddings, and feeding them to\n2\ntransformer encoders. With sufﬁcient training data, ViT is able to outperform convolution neural\nnetworks on various image classiﬁcation benchmarks [1, 44]. Many ViT variants have been proposed\nsince then. For example, DeiT [ 2] and T2T-ViT [45] are proposed to enhance ViT’s training data\nefﬁciency, by leveraging teacher-student and better crafted architectures respectively. In addition\nto image classiﬁcation, ViT has attracted wide attention in diverse computer vision tasks, including\nobject detection [4–7], segmentation [46, 47], enhancement [8, 9], image generation [10–12], video\nunderstanding [48, 49], vision-language [50–57] and 3D point cloud [58].\nDespite the impressive empirical performance, ViTs are generally heavy to train, and the trained\nmodels remain massive. That naturally motivates the study to reduce ViT inference and training\ncosts, by considering model compression means. Model compression has been well studied in both\ncomputer vision and NLP applications [ 59–61, 42, 62, 21]. Two concurrent works [ 13, 63] made\ninitial attempts towards ViT post-training compression by pruning the intermediate features and\ntokens respectively, but did not jointly consider weight pruning nor efﬁcient training. Another loosely\nrelated ﬁeld is the study of efﬁcient attention mechanisms [64, 10, 52, 65–75]. They mainly reduce\nthe calculation complexity for self-attention modules via various approximations such as low-rank\ndecomposition. Our proposed techniques represent an orthogonal direction and can be potentially\ncombined with them, which we leave as future work. Another latest concurrent work [76] introduced\nan interpretable module to dynamically and gracefully drop the redundant patches, gaining not only\ninference efﬁciency but also interpretability. Being a unique and orthogonal effort from ours, their\nmethod did not consider the training efﬁciency yet.\nPruning and Sparse Training. Pruning is well-known to effectively reduce deep network inference\ncosts [77, 15]. It can be roughly categorized into two groups: (i) unstructured pruning by removing\ninsigniﬁcant weight elements per certain criterion, such as weight magnitude [78, 15], gradient [16]\nand hessian [79]; (ii) structured pruning [80–82] by remove model sub-structures, e.g., channels [80,\n81] and attention heads [ 42], which are often more aligned with hardware efﬁciency. All above\nrequire training the full dense model ﬁrst, usually for several train-prune-retrain rounds.\nThe recent surge of sparse training seeks to adaptively identify high-quality sparse subnetworks\nand train only them. Starting from scratch, those methods learn to optimize the model weights\ntogether with sparse connectivity simultaneously. [32, 33] ﬁrst introduced the Sparse Evolutionary\nTraining (SET) technique [32], reaching superior performance compared to training with ﬁxed sparse\nconnectivity [83, 36]. [37–39] leverages “weight reallocation\" to improve performance of obtained\nsparse subnetworks. Furthermore, gradient information from the backward pass is utilized to guide\nthe update of the dynamic sparse connectivity [ 38, 34], which produces substantial performance\ngains. The latest investigations [ 40, 41, 35] demonstrate that more exhaustive exploration in the\nconnectivity space plays a crucial role in the quality of found sparse subnetworks. Current sparse\ntraining methods mostly focus on convolutional networks. Most of them discuss unstructured sparsity,\nexcept a handful [84, 30] considering training convolutional networks with structured sparsity.\n3 Methodology\nOur SViTE method (and its variants S2ViTE and SViTE+) is inspired from state-of-the-art sparse train-\ning approaches [34, 35] in CNNs. This section presents the sparse exploration of ViT architectures,\nthen shows the detailed procedure of input token selection for extra efﬁciency gains.\n3.1 Sparse ViT Exploration\nRevisiting sparse training. Sparse training starts from a randomly sparsiﬁed model; after optimiz-\ning several iterations, it shrinks a portion of parameters based on pre-deﬁned pruning criterion, and\nactivates new connections w.r.t. grow indicators. After upgrading the sparse topology, it trains the\nnew subnetwork until the next update of the connectivity. An illustration of the overall procedure is\nshown in Figure 1. The key factors of sparse training are  sparsity distribution,  update schedule,\n pruning and  grow criterion.\nNotations. For a consistent description, we follow the standard notations in [ 34, 35]. Let Dbe\nthe training dataset. bt ∼D is a randomly sampled data batch for iteration t. fW(·) represents\nthe model with parameters W = (W(1),··· ,W(L)), where W(l) ∈RNl,1 ≤l ≤L, Nl is the\n3\nTraining Time\n......\nToken Selection\nClassifier\n...\n...\nUpdate Interval Update IntervalUpdate Sparse Connectivity\n......\nTraining\nPrune\nGrow\nTraining\nTransformer Encoder\nToken Selector\nRemaining Connections\nPruned Connections\nGrown Connections\nPatch Tokens\nSparse ViT Exploration\nTransformer Pruning\nToken Selection for SViTE+\nLinear Projection\n......\n......\nScorer Function\n...... Scores\nLearnable Top-K Selection\n...( )\nSelected Tokens\nLinear Linear Linear\nScaled\nSoftmax\n•  •  • \nConcat\nPruneable Weights\nRemoveable\nFeatures/Modules\nDot-Product\nSkip\nConnection\n•  •  • \n......\nLinear\nLinear\nLinear\nAdd &\nNorm\nAdd &\nNorm\nTransformer Encoders\n......\nToken Selection\nClassifier\nTransformer Encoders\nFigure 1: The overall procedure of our proposed sparse ViT exploration framework. Upper Figure:\nﬁrst training ViT for ∆T iterations, then performing prune-and-grow strategies to explore critical\nsparse connectivities, repreating until convergence. Bottom Left Figure: enforcing either structured\nor unstructured sparsity to transformer layers in ViT.Bottom Right Figure: ﬁrst scoring each input\nembedding and applying the learnable top-kselection to identify the most informative tokens.\nnumber of prunable parameters in the lth layer, and Ldenotes the number of transformer layers.\nNote that the ﬁrst linear projection layer and the classiﬁer of ViT [ 1, 2] are not sparsiﬁed in our\nframework. As illustrated in Figure 1(bottom-left), W(l)\nQ = {W(l,h)\nQ }H\nh=1, W(l)\nK = {W(l,h)\nK }H\nh=1,\nW(l)\nV = {W(l,h)\nV }H\nh=1 are the weights of the self-attention module in the lth layer, W(l,1), W(l,2),\nW(l,3) are the weights of the multilayer perceptron (MLP) module in the lth layer, and W(l) =\n(W(l)\nQ ,W(l)\nK ,W(l)\nV ,W(l,1),W(l,2),W(l,3)) collectively represent all the parameters in the lth layer,\nwhere H denotes the number of attention heads, and 1 ≤h ≤H. X(l), Q(l), K(l), and V(l) are\nthe corresponding input and intermediate features, respectively. Each sparse layer only maintains a\nfraction sl ∈(0,1) of its connections, and the overall sparsity of a sparse subnetwork is calculated as\nthe ratio of pruned elements to the total parameter counts, i.e.,\n∑\nl sl×Nl∑\nl Nl\n.\nSparse Vision Transformer Exploration (SViTE). SViTE explores the unstructured sparse topol-\nogy in vision transformers. To be speciﬁc, we adopt Erd¨os-R´enyi [32] as our  sparsity distribution.\nThe number of parameters in the sparse layer is scaled by 1 −nl−1+nl\nnl−1×nl\n, where nl is the number\nof neurons at layer l. This distribution allocates higher sparsities to the layers with more parame-\nters by scaling the portion of remaining weights with the sum of the number of output and input\nneurons/channels. For the  update schedule, it contains: ( i) the update interval ∆T, which is\nthe number of training iterations between two sparse topology updates; (ii) the end iteration Tend,\nindicating when to stop updating the sparsity connectivity, and we set Tend to 80% of total train-\ning iterations in our experiments; ( iii) the initial fraction αof connections that can be pruned or\ngrow, which is 50% in our case; ( iv) a decay schedule of the fraction of changeable connections\nfdecay(t,α, Tend) = α\n2 (1+cos( t×π\nTend\n)), where a cosine annealing is used, following [34, 35]. During\neach connectivity update, we choose the weight magnitude as  the pruning indicator, and gradient\nmagnitude as  the grow indicator. Speciﬁcally, we eliminate the parameters with the layer-wise\nsmallest weight values by applying a binary mask mprune, then grow new connections with the\nhighest magnitude gradients by generating a new binary mask mgrow. Both masks are employed to\nW(l) via the element-wise dot product, and note that the number of non-zero elements in mprune\nand mgrow are equal and ﬁxed across the overall procedure. Newly added connections are not\nactivated in the last sparse topology, and are initialized to zero since it produces better performance\nas demonstrated in [34, 35].\n4\nInfrequent gradient calculation [34] is adopted in our case, which computes the gradients in an online\nmanner and only stores the top gradient values. As illustrated in [ 34], such fashion amortizes the\nextra effort of gradient calculation, and makes it still proportional to 1 −sas long as ∆T ≥ 1\n1−s,\nwhere sis the overall sparsity.\nStructured Sparse Vision Transformer Exploration (S2ViTE). Although models with unstruc-\ntured sparsity achieve superior performance, structured sparsity [80–82] is much more hardware\nfriendly and brings practical efﬁciency on realistic platforms, which motivates us to propose Struc-\ntured Sparse ViT Exploration(S2ViTE). We inherit the design of sparsity distribution and  update\nschedule from the unstructured SViTE, and a round-up function is used to eliminate decimals in the\nparameter counting. The key differences lie in the new  pruning and  grow strategies.\nAlgorithm 1 Sparse ViT Co-Exploration (SViTE+).\nInitialize: ViT model fW, Dataset D, Sparsity dis-\ntribution S = {s1,··· ,sL}, Update schedule\n{∆T,Tend,α,f decay}, Learning rate η\n1: Initialize fW with random sparsity S ⊿Highly\nreduced parameter count.\n2: for each training iteration tdo\n3: Sampling a batch bt ∼D\n4: Scoring the input token embeddings and selecting\nthe top-kinformative tokens ⊿Token selection\n5: if (tmod ∆T == 0) and t< Tend then\n6: for each layer ldo\n7: ρ= fdecay(t,α, Tend) ·(1 −sl) ·Nl\n8: Performing prune-and-grow with portion ρ\nw.r.t. certain criterion, generating masks\nmprune and mgrow to update fW’s sparsity\npatterns ⊿Connectivity exploration\n9: end for\n10: else\n11: W = W −η·∇WLt ⊿Updating Weights\n12: end if\n13: end for\n14: return a sparse ViT with a trained token selector\nPruning criterion: Let A(l,h) denote fea-\ntures computed from the self-attention\nhead {W(l,h)\nQ , W(l,h)\nK , W(l,h)\nV } and in-\nput embeddings X(l), as shown in Fig-\nure 1. We perform the Taylor expansion\nto the loss function [16, 42], and derive\na proxy score for head importance blow:\nI(l,h)\np =\n⏐⏐⏐⏐AT\n(l,h) ·∂L(X(l))\n∂A(l,h)\n⏐⏐⏐⏐, (1)\nwhere L(·) is the cross-entropy loss as\nused in ViT. During each topology up-\ndate, we remove attention heads with\nthe smallest I(l,h)\np . For MLPs, we score\nneurons with ℓ1-norm of their associ-\nated weight vectors [ 85], and drop in-\nsigniﬁcant neurons. For example, the\njth neuron of W(l,1) in Figure 1 has\nan importance score ∥W(l,1)\nj,· ∥ℓ1 , where\nW(l,1)\nj,· is the jth row of W(l,1).\nGrow criterion: Similar to [34, 35], we\nactive the new units with the highest\nmagnitude gradients, such as ∥∂L(X(l))\n∂A(l,h)\n∥ℓ1 and ∥∂L(X(l))\n∂W(l,1)\nj,·\n∥ℓ1 for the hth attention head and the jth\nneuron of the MLP (W(l,1)), respectively. The gradients are calculated in the same manner as the one\nin unstructured SViTE, and newly added units are also initialized to zero.\n3.2 Data and Architecture Sparsity Co-Exploration for Higher Efﬁciency\nAlgorithm 2 The top-k selector in a PyTorch-like style.\ndef topk_selector(logits, k, tau, dim=-1):\n# Maintain tokens with the top-$k$ highest scores\ngumbels =\n-torch.empty_like(logits).exponential_().log()\ngumbels = (logits + gumbels) / tau\n# tau is the temperature\ny_soft = gumbels.softmax(dim)\n# Straight through\nindex = y_soft.topk(k, dim=dim)[1]\ny_hard = scatter(logits, index, k)\nret = y_hard - y_soft.detach() + y_soft\nreturn ret\nBesides exploring sparse trans-\nformer architectures, we further\nslim the dimension of input token\nembeddings for extra efﬁciency\nbonus by leveraging a learnable to-\nken selector, as presented in Fig-\nure 1. Meanwhile, the introduced\ndata sparsity also serves as an im-\nplicit regularization for ViT train-\ning, which potentially leads to im-\nproved generalization ability, as ev-\nidenced in Table 6. Note that, due\nto skip connections, the number of\ninput tokens actually determines\nthe dimension of intermediate features, which substantially contributes to the overall computation\n5\nTable 1: Details of training conﬁgurations in our experiments, mainly following the settings in [2].\nBackbone Update Schedule{∆T,Tend,α,fdecay} Batch Size Epochs Inherited Settings from DeiT [2]\nDeiT-Tiny {20000,1200000,0.5,cosine} 512 600 AdamW,0.0005×batchsize\n512 , cosine decay\nwarmup 5 epochs, 0.05 weight decay\n0.1 label smoothing, augmentations, etc.\nDeiT-Small {15000,1200000,0.5,cosine} 512 600\nDeiT-Base {7000,600000,0.5,cosine} 1024 600\ncost. In other words, the slimmed input token embeddings directly result in compressed intermediate\nfeatures, and bring substantial efﬁciency gains.\nFor the input tokens X(1) ∈Rn×d, where ndenotes the number of tokens to be shrunk, and dis\nthe dimension of each token embedding that keeps unchanged. As shown in Figure 1, all token\nembeddings are passed through a learnable scorer function which is parameterized by an MLP in our\nexperiments. Then, a selection of the top- kimportance scores (1 ≤k ≤d) is applied on top of it,\naiming to preserve the signiﬁcant tokens and remove the useless ones. To optimize parameters of the\nscorer function, we introduce the popular Gumbel-Softmax [86, 87] and straight-through tricks [88]\nto enable gradient back-propagation through the top-kselection, which provides an efﬁcient solution\nto draw samples from a discrete probability distribution. A detailed implementation is in Algorithm 2.\nThe full pipeline of data and architecture co-exploration is summarized in Algorithm 1. We term this\napproach SViTE+. We ﬁrst feed the randomly sampled data batch to the token selector and pick the\ntop-kinformative token embeddings. Then, we alternatively train the sparse ViT for ∆T iterations\nand perform prune-and-grow to explore the sparse connectivity in ViTs dynamically. In the end, a\nsparse ViT model with a trained token selector is returned and ready for evaluation.\n4 Experiments\nBaseline pruning methods. We extend several effective pruning methods from CNN compression\nas our strong baselines. Unstructured pruning: (i) One-shot weight Magnitude Pruning (OMP) [15],\nwhich removes insigniﬁcant parameters with the globally smallest weight values; (ii) Gradually\nMagnitude Pruning (GMP) [17], which seamlessly incorporates gradual pruning techniques within\nthe training process by eliminating a few small magnitude weights per iteration; and (iii) Taylor\nPruning (TP) [16], which utilizes the ﬁrst-order approximation of the training loss to estimate units’\nimportance for model sparsiﬁcation. Structured pruning: Salience-based Structured Pruning (SSP).\nWe draw inspiration from [ 42, 85], and remove sub-modules in ViT (e.g., self-attention heads)\nby leveraging their weight, activation, and gradient information. Moreover, due to the repetitive\narchitecture of ViT, we can easily reduce the number of transformer layers to create a smaller dense\nViT (Small-Dense) baseline that has similar parameter counts to the pruned ViT model.\n20 40 60 80 100\nFLOPs (×1010)\n72\n74\n76\n78\n80\n82Testing Accuracy (%)\nDeiT-Tiny\nDeiT-Small\nDeiT-Base\nThe Overall Performance of SViTE, S2ViTE, and SViTE+\nSViTE-Small\nS2ViTE-Small\nSViTE-Base\nS2ViTE-Base\nSViTE+-Small\nFigure 2: Top-1 accuracy (%) over FLOPs (×1010) on Im-\nageNet of our methods, i.e., SViTE, S2ViTE, and SViTE+,\ncompared to DeiT baselines, trained on Imagenet-1K only.\nImplementation details. Our experiments\nare conducted on ImageNet with DeiT-\nTiny/Small/Base backbones. The detailed\ntraining conﬁgurations are listed in Table 1,\nwhich mainly follows the default setups in [2].\nAll involved customized hyperparameters are\ntuned via grid search (later shown in Figure 3).\nFor a better exploration of sparsity connec-\ntivities, we increase training epochs to 600\nfor all experiments. GMP [ 17] has an addi-\ntional hyperparameter, i.e., the pruning sched-\nule, which starts from 1\n6 and ends at 1\n2 of the\ntraining epochs with 20 times pruning in total.\nMore details are referred to Appendix A1.\nTraining time measuring protocol. We\nstrictly measure the running time saving of\n(sparse) vision transformers on the ImageNet-1K task using CUDA benchmark mode. To be speciﬁc,\nwe separately calculate the time elapsed during each iteration, to eliminate the impact of the hardware\nenvironment as much as possible. Note that the time for the data I/O is excluded.\nHighlight of our ﬁndings. The overall performance of SViTE, S 2ViTE, and SViTE+ on DeiT\nbackbones are summarized in Figure 2. We highlight some takeaways below.\n6\nTakeaways:  SViTE produces sparse DeiTs with enhanced generalization and substantial reduced\nFLOPs, compared to its dense counterpart ( ⋆). SViTE+ further improves the performance of\nSViTE by selecting the most vital patches.  S2ViTE achieves matched accuracy on DeiT-Small,\nand signiﬁcantly enhances performance on DeiT-Base. Meanwhile, its structural sparsity brings\nconsiderable running time savings.  Appropriate data and architecture sparsities can effectively\nregularize ViT training, leading to a new SOTA win-win between ViT accuracy and efﬁciency.\n4.1 SViTE with Unstructured Sparsity\nWe perform SViTE to mine vital unstructured sparsity in DeiTs [2]. Solid lines in Figure 2 record\nthe top-1 test-set accuracy over FLOPs on ImageNet-1K of SViTE-Small and SViTE-Base with\na range of sparsity from 30% to 70%. In general, we observe that SViTE generates superior\nsparse ViTs with both accuracy and efﬁciency gains. Table 2, 3, and 5 present the comparison\nbetween SViTE and various pruning baselines. From these extensive results, we draw several\nconsistent observations. First, compared to the dense baselines, SViTE-Tiny, -Small, and -Base obtain\n25.56% ∼34.16%, 46.26% ∼55.44%, and 47.95% ∼57.50% FLOPs reduction, respectively,\nat 30% ∼60% sparsity levels with only a negligible accuracy drop within 0.5%. It veriﬁes the\neffectiveness of our proposal, and indicates severe parameter redundancy in ViT.Second, our SViTE\nmodels from dynamic explorations consistently surpass other competitive baseline methods, including\nOMP, GMP, TP, and Small-Dense by a substantial performance margin. Among all the baseline\napproaches, GMP that advocates a gradual pruning schedule achieves the best accuracy with all three\nDeiT backbones. Third, in Figure 2, both SViTE-Small (blue solid line) and SViTE-Base (green\nsolid line) show an improved trade-off between accuracy and efﬁciency, compared to their dense\nDeiT counterparts. Interestingly, we also observe that with similar parameter counts, a large sparse\nViT consistently outperforms the corresponding smaller dense ViT. A possible explanation is those\nappropriate sparse typologies regularize network training and lead to enhanced generalization, which\ncoincides with recent ﬁndings of critical subnetworks (i.e., winning tickets) in dense CNNs [89, 22]\nand NLP transformer [21, 90] models.\nTable 2: Results of SViTE-Tiny on ImageNet-1K.\nAccuracies (%) within/out of parenthesis are the repro-\nduced/reported [2] performance.\nModels Sparsity (#Para.) FLOPs SavingAccuracy (%)\nDeiT-Tiny 0% (5.72M) 0% 72.20(71.80)\nSViTE-Tiny30% (4.02M) 25.56% 71.78OMP 30% (4.02M) 25.56% 68.35GMP 30% (4.02M) 25.56% 69.56TP 30% (4.02M) 25.56% 68.38\nSViTE-Tiny40% (3.46M) 34.16% 71.75OMP 40% (3.46M) 34.16% 66.52GMP 40% (3.46M) 34.15% 68.36TP 40% (3.46M) 34.17% 65.45\nSmall-Dense0% (3.94M) 32.54% 67.33\nTable 3: Results of SViTE-Small on ImageNet-1K.\nAccuracies (%) within/out of parenthesis are the repro-\nduced/reported [2] performance.\nModels Sparsity (#Para.) FLOPs SavingAccuracy (%)\nDeiT-Small 0% (22.1M) 0% 79.90(79.78)\nSViTE-Small50% (11.1M) 46.26% 79.72OMP 50% (11.1M) 46.25% 76.32GMP 50% (11.1M) 46.26% 76.88TP 50% (11.1M) 46.26% 76.30\nSViTE-Small60% (8.9M) 55.44% 79.41OMP 60% (8.9M) 55.44% 75.32GMP 60% (8.9M) 55.44% 76.79TP 60% (8.9M) 55.44% 74.50\nSmall-Dense0% (11.4M) 49.32% 73.93\nTable 4: Results of S2ViTE with structured sparsity on ImageNet-1K with DeiT-Tiny/Small/Base. Accuracies\n(%) within/out of parenthesis are the reproduced/reported [2] performance.\nModels Sparsity (%) Parameters FLOPs Saving Running Time ReducedTop-1 Accuracy (%)\nDeiT-Tiny (Dense) 0% 5.72M 0% 0% 72.20(71.80)\nSViTE-Tiny (Unstructured)30% 4.02M 25.56% 0% 71.78\nSSP-Tiny (Structured) 30% 4.21M 23.69% 10.57% 68.59\nS2ViTE-Tiny (Structured) 30% 4.21M 23.69% 10.57% 70.12\nDeiT-Small (Dense) 0% 22.1M 0% 0% 79.90(79.78)\nSViTE-Small (Unstructured)40% 13.3M 36.73% 0% 80.26\nSSP-Small (Structured) 40% 14.6M 31.63% 22.65% 77.74\nS2ViTE-Small (Structured) 40% 14.6M 31.63% 22.65% 79.22\nDeiT-Base (Dense) 0% 86.6M 0% 0% 81.80(80.98)\nSViTE-Base (Unstructured)40% 52.0M 38.30% 0% 81.56\nSSP-Base (Structured) 40% 56.8M 33.13% 24.70% 80.08\nS2ViTE-Base (Structured) 40% 56.8M 33.13% 24.70% 82.22\n4.2 S 2ViTE with Structured Sparsity\nFor more practical beneﬁts, we investigate sparse DeiTs with structured sparsity. Results are summa-\nrized in Table 4. Besides the obtained 23.79% ∼33.63% FLOPs savings, S2ViTE-Tiny, S2ViTE-\n7\nSmall, and S 2ViTE-Base enjoy an extra 10.57%, 22.65%, and 24.70% running time reduction,\nrespectively, from 30% ∼40% structured sparsity with competitive top-1 accuracies. Further-\nmore, S2ViTE consistently outperforms the baseline structured pruning method (SSP), which again\ndemonstrates the superior sparse connectivity learned from dynamic sparse training.\nThe most impressive results come from S2ViTE-Base at 40% structured sparsity. It even surpasses the\ndense DeiT base model by 0.42% ∼1.24% accuracy with 34.41% parameter counts, 33.13% FLOPs,\nand 24.70% running time reductions. We conclude that (i) an adequate sparsity from S2ViTE boosts\nViT’s generalization ability, which can be regarded as an implicit regularization; (ii) larger ViTs (e.g.,\nDeiT-Base) tend to have more superﬂuous self-attention heads, and are more amenable to structural\nsparsiﬁcation from S2ViTE, based on Figure 2 where dash lines denote the overall performance of\nS2ViTE-Small and S2ViTE-Base with a range of sparsity from 30% to 70%.\nTable 5: Results of SViTE-Base on ImageNet-1K.\nAccuracies (%) within/out of parenthesis are the repro-\nduced/reported [2] performance.\nModels Sparsity (#Para.) FLOPs SavingAccuracy (%)\nDeiT-Base 0% (86.6M) 0% 81.80(80.98)\nSViTE-Base50% (43.4M) 47.95% 81.51OMP 50% (43.4M) 47.94% 80.26GMP 50% (43.4M) 47.95% 80.79TP 50% (43.4M) 47.94% 80.55\nSViTE-Base60% (34.8M) 57.50% 81.28OMP 60% (34.8M) 57.50% 80.25GMP 60% (34.8M) 57.50% 80.44TP 60% (34.8M) 57.49% 80.37\nSmall-Dense0% (44.0M) 49.46% 78.59\nTable 6: Results of SViTE+-Small on ImageNet-1K.\nAccuracies (%) within/out of parenthesis are the repro-\nduced/reported [2] performance.\n#Tokens (%)Time Reduced FLOPs SavingAccuracy (%)\nSViTE+-Small 50% Unstructured Sparsity\n100% 0% 46.26% 79.7295% 4.40% 49.32% 80.1890% 7.63% 52.38% 79.9170% 19.77% 63.95% 77.90\nS2ViTE+-Small 40% Structured Sparsity\n100% 22.65% 31.63% 79.2295% 27.17% 37.76% 78.4490% 29.21% 41.50% 78.1670% 39.10% 54.96% 74.77\n4.3 SViTE+ with Data and Architecture Sparsity Co-Exploration\nIn this section, we study data and architecture sparsity co-exploration for ViTs, i.e., SViTE+. Blessed\nby the reduced input token embeddings, even ViTs with unstructured sparsity can have running time\nsavings. The beneﬁts are mainly from the shrunk input and intermediate feature dimensions. Without\nloss of generality, we consider SViTE+-Small with50% unstructured sparsity and S2ViTE+-Small\nwith 40% structured sparsity as examples. As shown in Table 6 and Figure 2, SViTE+-Small at 50%\nunstructured sparsity is capable of abandoning 5% ∼10% tokens while achieving 4.40% ∼7.63%\nrunning time and 49.32% ∼52.38% FLOPs savings, with even improved top-1 testing accuracy. It\nagain demonstrates that data sparsity as an implicit regularizer plays a beneﬁcial role in ViT training.\nHowever, slimming input and intermediate embedding is less effective when incorporated with\nS2ViTE, suggesting that aggressively removing structural sub-modules hurts ViT’s generalization.\n4.4 Ablation and Generalization Study of SViTEs\nUpdate interval in SViTE. The length of the update interval∆T controls one of the essential trade-\noffs in our proposed dynamic sparse exploration, since ∆T multiplying the number of updates is the\npre-deﬁned Tend. On the one hand, a larger updated interval (i.e., smaller update frequency) produces\na more well-trained model for improved estimation of units’ importance. On the other hand, a larger\nupdate frequency (i.e., smaller ∆T) allows more sufﬁcient exploration of sparse connectivities, which\npotentially generates higher-quality sparse subnetworks, as demonstrated in [35]. We evaluate this\nfactor in our SViTE context, and collect the results in Figure 3 (Left). We observe that ∆T = 20000\nworks the best for SViTE-Tiny, and both larger and smaller∆T degrade the performance.\n65.84\n66.77\n68.45\n69.09\n69.30\n69.60\n68.84\n10002000500010000150002000025000\nACCURACY\n68.84\n69.60\n67.85\n65.72\n1024512256128\nACCURACY\n(∆T) ($)Update IntervalBatch Size\nFigure 3: Accuracy of SViTE-Tiny with 50% unstructured sparsity. Left: ablation studies of the update interval\n(∆T); Right: ablations studies of the adopted batch size (b).\n8\nDense DeiT-Base -Base with 40% Structured SparsitySViTE-Base with 40% Unstructured Sparsity\nFigure 4: Attention probabilities for DeiT-Base, S2ViTE-Base, and SViTE-Base models with 12 layers (rows)\nand 12 heads (columns) using visualization tools provided in [94]. Attention maps are averaged over 100 test\nsamples from ImageNet-1K to present head behavior and remove the dependence on the input content. The\nblack square is the query pixel.\n indicates pruned attention heads. Zoom-in for better visibility.\nFigure 5: Learned patch selection patterns of SViTE+-Small at 10% data and 50% architecture sparsity levels.\nindicates removed inessential patches.\nBatch size in SViTE. Besides the update interval∆T, batch size (b) also affects the aforementioned\ntrade-off, especially for the data-hungry ViT training. We investigate different batch sizes in Figure 3\n(Right), and ﬁnd that b= 512 outperforms other common options for SViTE-Tiny.\nGeneralization study of SViTE and its variants. It is worth mentioning that our proposed frame-\nworks (SViTE, S2ViTE, SViTE+) are independent of the backbone architectures, and can be easily\nplugged in other vision transformer models [ 91, 45, 92, 93]. We implemented both SViTE and\nS2ViTE on TNT-S [91]. SViTE-TNT-S gains 0.13 accuracy improvements (Ours: 81.63 v.s. TNT-S:\n81.50) and 37.54% FLOPs savings at 40% unstructured sparsity; S2ViTE-TNT-S obtains 32.96%\nFLOPs and 23.71% running time reductions at 40% structured sparsity with almost unimpaired\naccuracy (Ours: 81.34 v.s. TNT-S:81.50).\n4.5 Visualization\nSparse connectivity patterns. We provide unit-wise and element-wise heatmap visualizations for\nSViTE-Base with 40% structured sparsity in Figure A7 (in Appendix). Similarly, element-wise\nheatmap visualizations of SViTE-Base with 50% unstructured sparsity are displayed in Figure A6.\nWe ﬁnd that even unstructured sparsity exploration can develop obvious structural patterns (i.e.,\n“vertical lines” in mask heatmaps), which implies a stronger potential for hardware speedup [95].\nSelf-attention heatmaps. As shown in Figure 4, we utilize tools in [94] to visualize attention maps\nof (sparse) ViTs. Multiple attention heads show similar behaviors, which implies the structural\nredundancy. Fortunately, S 2ViTE eliminates unnecessary heads to some extent. With regard to\nSViTE-Base’s visual results, it seems to activate fewer attention heads for predictions (darker colors\nmean larger values), compared to the ones of dense DeiT-Base. We also observe that in the bottom\nlayers, the attention probabilities are more centered at several heads; while in the top layers, the\nattention probabilities are more uniformly distributed. This kind of tendency is well preserved by our\nsparse ViT (SViTE) from Dense ViTs.\nLearned patch selection patterns. Figure 5 presents the learned behaviors of our token selector in\nSViTE+. We observe that the useless removed patches are typically distributed around the main object\n9\nor in the background. Meanwhile, the patches within the objects of interest are largely persevered,\nwhich evidences the effectiveness of our learned patch token selector.\n5 Conclusion and Discussion of Broader Impact\nIn this work, we introduce sparse ViT exploration algorithms, SViTE, and its variants S 2ViTE\nand SViTE+, to explore high-quality sparse patterns in both ViT’s architecture and input token\nembeddings, alleviating training memory bottleneck and pursuing inference ultra-efﬁciency (e.g.,\nrunning time and FLOPs). Comprehensive experiments on ImageNet validate the effectiveness of\nour proposal. Our informative visualizations further demonstrate that SViTE+ is capable of mining\ncrucial connections and input tokens by eliminating redundant units and dropping useless token\nembeddings. Future work includes examining the performance of our sparse ViTs on incoming\nhardware accelerators [96–100], which will provide better supports for sparsity.\nThis work is scientiﬁc in nature, and we do not believe it has immediate negative societal impacts.\nOur ﬁndings of sparse vision transformers are highly likely to reduce both memory and energy costs\nsubstantially, leading to economic deployment in real-world applications (e.g., on smartphones).\nAcknowledgment\nZ.W. is in part supported by an NSF RTML project (#2053279).\nReferences\n[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\net al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020.\n[2] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles,\nand Hervé Jégou. Training data-efﬁcient image transformers & distillation through attention.\narXiv preprint arXiv:2012.12877, 2020.\n[3] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui\nTang, An Xiao, Chunjing Xu, Yixing Xu, et al. A survey on visual transformer. arXiv preprint\narXiv:2012.12556, 2020.\n[4] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao Dong. End-to-end\nobject detection with adaptive clustering transformer. arXiv preprint arXiv:2011.09315, 2020.\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In European Conference\non Computer Vision, pages 213–229. Springer, 2020.\n[6] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. Up-detr: Unsupervised pre-training\nfor object detection with transformers. arXiv preprint arXiv:2011.09094, 2020.\n[7] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable\n{detr}: Deformable transformers for end-to-end object detection. In International Conference\non Learning Representations, 2021.\n[8] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,\nChunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. arXiv\npreprint arXiv:2012.00364, 2020.\n[9] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture\ntransformer network for image super-resolution. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5791–5800, 2020.\n[10] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,\nand Dustin Tran. Image transformer. In International Conference on Machine Learning, pages\n4055–4064. PMLR, 2018.\n10\n[11] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya\nSutskever. Generative pretraining from pixels. In Hal Daumé III and Aarti Singh, editors,\nProceedings of the 37th International Conference on Machine Learning , volume 119 of\nProceedings of Machine Learning Research, pages 1691–1703. PMLR, 13–18 Jul 2020.\n[12] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two transformers can make one\nstrong gan. arXiv preprint arXiv:2102.07074, 2021.\n[13] Mingjian Zhu, Kai Han, Yehui Tang, and Yunhe Wang. Visual transformer pruning, 2021.\n[14] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and\nacceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.\n[15] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural\nnetworks with pruning, trained quantization and huffman coding. In International Conference\non Learning Representations, 2016.\n[16] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance\nestimation for neural network pruning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 11264–11272, 2019.\n[17] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efﬁcacy of pruning\nfor model compression. arXiv preprint arXiv:1710.01878, 2017.\n[18] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable\nneural networks. In International Conference on Learning Representations, 2018.\n[19] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Linear\nmode connectivity and the lottery ticket hypothesis. arXiv preprint arXiv:1912.05671, 2019.\n[20] Zhenyu Zhang, Xuxi Chen, Tianlong Chen, and Zhangyang Wang. Efﬁcient lottery ticket\nﬁnding: Less data is more. In Marina Meila and Tong Zhang, editors, Proceedings of the\n38th International Conference on Machine Learning, volume 139 of Proceedings of Machine\nLearning Research, pages 12380–12390. PMLR, 18–24 Jul 2021.\n[21] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and\nMichael Carbin. The lottery ticket hypothesis for pre-trained bert networks. arXiv preprint\narXiv:2007.12223, 2020.\n[22] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin,\nand Zhangyang Wang. The lottery tickets hypothesis for supervised and self-supervised\npre-training in computer vision models. arXiv preprint arXiv:2012.06908, 2020.\n[23] Xuxi Chen, Zhenyu Zhang, Yongduo Sui, and Tianlong Chen. {GAN}s can play lottery tickets\ntoo. In International Conference on Learning Representations, 2021.\n[24] Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie, and Zhangyang Wang.\nGood students play big lottery better. arXiv preprint arXiv:2101.03255, 2021.\n[25] Zhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu Cheng, Shuohang Wang, and Jingjing\nLiu. Playing lottery tickets with vision and language. arXiv preprint arXiv:2104.11832, 2021.\n[26] Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. A uniﬁed\nlottery ticket hypothesis for graph neural networks. arXiv preprint arXiv:2102.06790, 2021.\n[27] Tianlong Chen, Yu Cheng, Zhe Gan, Jingjing Liu, and Zhangyang Wang. Ultra-data-\nefﬁcient gan training: Drawing a lottery ticket ﬁrst, then training it toughly. arXiv preprint\narXiv:2103.00397, 2021.\n[28] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. Snip: Single-shot network pruning\nbased on connection sensitivity. In International Conference on Learning Representations,\n2019.\n[29] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by\npreserving gradient ﬂow. In International Conference on Learning Representations, 2020.\n11\n[30] Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G.\nBaraniuk, Zhangyang Wang, and Yingyan Lin. Drawing early-bird tickets: Toward more\nefﬁcient training of deep networks. In International Conference on Learning Representations,\n2020.\n[31] Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, and Jingjing Liu.\nEarlybert: Efﬁcient bert training via early-bird lottery tickets.arXiv preprint arXiv:2101.00063,\n2020.\n[32] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine\nGibescu, and Antonio Liotta. Scalable training of artiﬁcial neural networks with adaptive\nsparse connectivity inspired by network science. Nature communications, 9(1):1–12, 2018.\n[33] Shiwei Liu, Decebal Constantin Mocanu, Amarsagar Reddy Ramapuram Matavalam, Yulong\nPei, and Mykola Pechenizkiy. Sparse evolutionary deep learning with over one million artiﬁcial\nneurons on commodity hardware. Neural Computing and Applications, 2020.\n[34] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the\nlottery: Making all tickets winners. In International Conference on Machine Learning, pages\n2943–2952. PMLR, 2020.\n[35] Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Do we actually\nneed dense over-parameterization? in-time over-parameterization in sparse training. arXiv\npreprint arXiv:2102.02887, 2021.\n[36] Utku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen. The difﬁculty of training sparse\nneural networks. arXiv preprint arXiv:1906.10732, 2019.\n[37] Hesham Mostafa and Xin Wang. Parameter efﬁcient training of deep convolutional neural\nnetworks by dynamic sparse reparameterization. In International Conference on Machine\nLearning, 2019.\n[38] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without\nlosing performance. arXiv preprint arXiv:1907.04840, 2019.\n[39] Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, and Mykola Pechenizkiy. Selﬁsh sparse\nrnn training. arXiv preprint arXiv:2101.09048, 2021.\n[40] Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen. Top-kast:\nTop-k always sparse training. Advances in Neural Information Processing Systems, 33, 2020.\n[41] Md Aamir Raihan and Tor M Aamodt. Sparse weight activation training. arXiv preprint\narXiv:2001.01969, 2020.\n[42] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one?, 2019.\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017.\n[44] Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang. Cmt:\nConvolutional neural networks meet vision transformers. arXiv preprint arXiv:2107.06263,\n2021.\n[45] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay,\nJiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.\n[46] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab:\nEnd-to-end panoptic segmentation with mask transformers. arXiv preprint arXiv:2012.00759,\n2020.\n[47] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen,\nand Huaxia Xia. End-to-end video instance segmentation with transformers. arXiv preprint\narXiv:2011.14503, 2020.\n12\n[48] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning joint spatial-temporal transfor-\nmations for video inpainting. In European Conference on Computer Vision, pages 528–543.\nSpringer, 2020.\n[49] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end\ndense video captioning with masked transformer. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 8739–8748, 2018.\n[50] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic\nvisiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265,\n2019.\n[51] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from\ntransformers. arXiv preprint arXiv:1908.07490, 2019.\n[52] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng,\nand Jingjing Liu. Uniter: Universal image-text representation learning. In European Confer-\nence on Computer Vision, pages 104–120. Springer, 2020.\n[53] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert:\nPre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530,\n2019.\n[54] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A\nsimple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557,\n2019.\n[55] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. Unicoder-vl: A universal\nencoder for vision and language by cross-modal pre-training. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 34, pages 11336–11344, 2020.\n[56] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for\nvision-language tasks. In European Conference on Computer Vision, pages 121–137. Springer,\n2020.\n[57] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao.\nUniﬁed vision-language pre-training for image captioning and vqa. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, volume 34, pages 13041–13049, 2020.\n[58] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. Point transformer.\narXiv preprint arXiv:2012.09164, 2020.\n[59] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with\nstructured dropout. In International Conference on Learning Representations, 2020.\n[60] Fu-Ming Guo, Sijia Liu, Finlay S Mungall, Xue Lin, and Yanzhi Wang. Reweighted proximal\npruning for large-scale language representation. arXiv preprint arXiv:1909.12486, 2019.\n[61] Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Deming Chen, Mari-\nanne Winslett, Hassan Sajjad, and Preslav Nakov. Compressing large-scale transformer-based\nmodels: A case study on bert. arXiv preprint arXiv:2002.11985, 2020.\n[62] J. S. McCarley, Rishav Chakravarti, and Avirup Sil. Structured pruning of a bert-based question\nanswering model. arXiv preprint arXiv:1910.06360, 2019.\n[63] Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chao Xu, and Dacheng Tao.\nPatch slimming for efﬁcient vision transformers. arXiv preprint arXiv:2106.02852, 2021.\n[64] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng\nGao. Multi-scale vision longformer: A new vision transformer for high-resolution image\nencoding. arXiv preprint arXiv:2103.15358, 2021.\n[65] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\n13\n[66] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer.\narXiv preprint arXiv:2001.04451, 2020.\n[67] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh.\nSet transformer: A framework for attention-based permutation-invariant neural networks. In\nInternational Conference on Machine Learning, pages 3744–3753. PMLR, 2019.\n[68] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based\nsparse attention with routing transformers. Transactions of the Association for Computational\nLinguistics, 9:53–68, 2021.\n[69] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive\ntransformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.\n[70] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in\nmultidimensional transformers. arXiv preprint arXiv:1912.12180, 2019.\n[71] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers\nare rnns: Fast autoregressive transformers with linear attention. In International Conference\non Machine Learning, pages 5156–5165. PMLR, 2020.\n[72] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking\nattention with performers. arXiv preprint arXiv:2009.14794, 2020.\n[73] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,\nLiu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efﬁcient\ntransformers. arXiv preprint arXiv:2011.04006, 2020.\n[74] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey.\narXiv preprint arXiv:2009.06732, 2020.\n[75] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-\nattention with linear complexity, 2020.\n[76] Bowen Pan, Yifan Jiang, Rameswar Panda, Zhangyang Wang, Rogerio Feris, and Aude Oliva.\nIa-red2: Interpretability-aware redundancy reduction for vision transformers, 2021.\n[77] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural\ninformation processing systems, pages 598–605, 1990.\n[78] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections\nfor efﬁcient neural network. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and\nR. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1135–1143.\nCurran Associates, Inc., 2015.\n[79] Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In D. S. Touretzky, edi-\ntor, Advances in Neural Information Processing Systems 2, pages 598–605. Morgan-Kaufmann,\n1990.\n[80] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang.\nLearning efﬁcient convolutional networks through network slimming. In Proceedings of the\nIEEE International Conference on Computer Vision, pages 2736–2744, 2017.\n[81] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural\nnetworks. In Proceedings of the IEEE International Conference on Computer Vision, 2017.\n[82] Hao Zhou, Jose M Alvarez, and Fatih Porikli. Less is more: Towards compact cnns. In\nEuropean Conference on Computer Vision, pages 662–677. Springer, 2016.\n[83] Decebal Constantin Mocanu, Elena Mocanu, Phuong H Nguyen, Madeleine Gibescu, and\nAntonio Liotta. A topological insight into restricted boltzmann machines. Machine Learning,\n104(2-3):243–270, 2016.\n14\n[84] Sangkug Lym, Esha Choukse, Siavash Zangeneh, Wei Wen, Sujay Sanghavi, and Mattan\nErez. Prunetrain: fast neural network training by dynamic sparse model reconﬁguration. In\nProceedings of the International Conference for High Performance Computing, Networking,\nStorage and Analysis, pages 1–13, 2019.\n[85] Brian R Bartoldson, Ari S Morcos, Adrian Barbu, and Gordon Erlebacher. The generalization-\nstability tradeoff in neural network pruning. arXiv preprint arXiv:1906.03728, 2019.\n[86] Emit J. Gumbel. Statistical theory of extreme values and some practical applications. The\nJournal of the Royal Aeronautical Society, 58(527):792–793, 1954.\n[87] Chris J Maddison, Daniel Tarlow, and Tom Minka. A* sampling. arXiv preprint\narXiv:1411.0030, 2014.\n[88] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin.\nUnderstanding straight-through estimator in training activation quantized neural nets. arXiv\npreprint arXiv:1903.05662, 2019.\n[89] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. The lottery\nticket hypothesis at scale. arXiv preprint arXiv:1903.01611, 2019.\n[90] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks.\narXiv preprint arXiv:1902.09574, 2019.\n[91] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in\ntransformer. arXiv preprint arXiv:2103.00112, 2021.\n[92] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction\nwithout convolutions. arXiv preprint arXiv:2102.12122, 2021.\n[93] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou,\nand Jiashi Feng. Deepvit: Towards deeper vision transformer.arXiv preprint arXiv:2103.11886,\n2021.\n[94] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\nattention and convolutional layers. In International Conference on Learning Representations,\n2020.\n[95] Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan. Fast sparse convnets. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n14629–14638, 2020.\n[96] Peiqi Wang, Yu Ji, Chi Hong, Yongqiang Lyu, Dongsheng Wang, and Yuan Xie. Snrram: An\nefﬁcient sparse neural network computation architecture based on resistive random-access\nmemory. In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), pages 1–6,\n2018.\n[97] Mike Ashby, Christiaan Baaij, Peter Baldwin, Martijn Bastiaan, Oliver Bunting, Aiken Cairn-\ncross, Christopher Chalmers, Liz Corrigan, Sam Davis, Nathan van Doorn, et al. Exploiting\nunstructured sparsity on next-generation datacenter hardware. None, 2019.\n[98] Chen Liu, Guillaume Bellec, Bernhard V ogginger, David Kappel, Johannes Partzsch, Felix\nNeumärker, Sebastian Höppner, Wolfgang Maass, Steve B Furber, Robert Legenstein, et al.\nMemory-efﬁcient deep learning on a spinnaker 2 prototype. Frontiers in neuroscience, 12:840,\n2018.\n[99] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, and\nWilliam J. Dally. Eie: Efﬁcient inference engine on compressed deep neural network. In 2016\nACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA), pages\n243–254, 2016.\n[100] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze. Eyeriss v2: A ﬂexible accelerator\nfor emerging deep neural networks on mobile devices.IEEE Journal on Emerging and Selected\nTopics in Circuits and Systems, 9(2):292–308, 2019.\n15\nChecklist\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes] Please see section 4.3.\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] Please see\nsection 5.\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A] Our work\ndoes not contain theoretical results.\n(b) Did you include complete proofs of all theoretical results? [N/A] Our work does not\ncontain theoretical results.\n3. If you ran experiments...\n(a) Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [Yes] We used publicly\navailable data in all of our experiments. Meanwhile we either provide the detailed imple-\nmentations or cite the papers of them following the authors instructions (See Section 4).\nAll of our codes are provided in https://github.com/VITA-Group/SViTE.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [Yes] We provides all the training details in Section 4.\n(c) Did you report error bars (e.g., with respect to the random seed after running exper-\niments multiple times)? [No] We did not report the error bars since running vision\ntransformer on ImageNet are extremely resource-consuming. For example, each re-\nported number takes around 960 V100 GPU hours. We will continue running the\nexperiments and report the conﬁdence intervals in future versions.\n(d) Did you include the total amount of compute and the type of resources used (e.g.,\ntype of GPUs, internal cluster, or cloud provider)? [Yes] We describe the details of\ncomputation resources in Section A1 of the supplement.\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes] We used publicly\navailable data, i.e., ImageNet, in our experiments. We cited the corresponding papers\npublished by the creators in Section 4.\n(b) Did you mention the license of the assets? [No] The license of ImageNet is included in\nthe paper that we have cited.\n(c) Did you include any new assets either in the supplemental material or as a URL?\n[Yes] The ImageNet we used are publicly available. And all our codes are included in\nhttps://github.com/VITA-Group/SViTE.\n(d) Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? [N/A] We did not collect/curate new data.\n(e) Did you discuss whether the data you are using/curating contains personally identiﬁable\ninformation or offensive content? [N/A] All ImageNet datasets are already publicly\navailable and broadly adopted. I do not think there are any issues of personally\nidentiﬁable information or offensive content.\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N/A]\nA16\nA1 More Implementation Details\nComputing resources. All experiments use Tesla V100-SXM2-32GB GPUs as computing re-\nsources. Speciﬁcally, each experiment is ran with 8 V100s for 4 ∼5 days.\nWhy do we choose 600 training epochs for SViTE experiments? Choosing 600 training epochs\nfor sparse training is to maintain similar training FLOPs compared to dense ViT training. Speciﬁcally,\nif training a dense DeiT model for 300 epochs needs 1x FLOPs, training SViTE-DeiT at 40% sparsity\nfor 600 epochs needs ∼0.95x FLOPs. In summary, we compare our SViTE (600 epochs) and DeiT\nbaselines (300 epochs) based on a similar training budget. Such comparison fashion is widely adopted\nin sparse training literature like [35, 34] (see Table 2’s caption in [35] and Figure 2 in [34] for details).\nMeanwhile, note that the reported running time is per epoch saving (i.e., total running time / total\nepoch), which would not be affected by the number of training epochs.\nBaseline models with longer epochs. Actually, the performance of DeiT training without distilla-\ntion saturates after 300 ∼400 epochs, as stated in [2]. We also conduct longer epoch (600 epochs)\ntraining for DeiT-Small and -Base models. Our results collected in the table A7 align with the original\nDeiT paper [2]. It suggests that our proposed SViTE is still able to achieve better accuracy with fewer\nparameters and fewer training&inference computations. Speciﬁcally, at 40% structured sparsity, our\nsparsiﬁed DeiT-Base can achieve 0.21% accuracy gain, at ∼51% training FLOPs, 33.13% inference\nFLOPs, and 24.70% running time savings, compared to its dense counterpart with 600 epochs.\nTable A7: Performance of DeiT-Small/-Base with longer training epochs on ImageNet-1K.\nMetrics DeiT-Small300Epochs DeiT-Small600Epochs DeiT-Base300Epochs DeiT-Base600Epochs\nAccuracy (%) 79.90 80.02 (+0.12) 81.80 82.01 (+0.21)\nA2 More Experimental Results\nSparse topology of SViTE-Base with unstructured sparsity. As shown in Figure A6, we observe\nthat from the initial random mask to explored mask in SViTE, plenty of structural patterns emerge\n(i.e., the darker “vertical\" lines mean completely pruned neurons in the MLPs). It is supervising that\nunstructured sparse exploration can lead to structured patterns, which implies the great potential to be\naccelerated in real-world hardware devices.\nMLP \nExplore \n Explore \n Explore \nMLP MLP \nSV iTE SV iTE SV iTE \nFigure A6: Binary mask visualizations of SViTE-Base at 50% unstructured sparsity. Within each box, left is\nthe initial random mask; right is the explored mask from SViTE.\nSparse topology of S2ViTE-Base with structured sparsity. Figure A7 shows mask visualizations\nof pruned multi-attention heads and MLPs in vision transformers. It shows that S 2ViTE indeed\nexplores totally different connectivity patterns, compared to the initial topology.\nAblation of only applying our learnable token selector. We compare these three setup: (a) DeiT-\nSmall (79.90 test accuracy); (b) DeiT-Small + Token selector with 10% data sparsity (78.67 test\naccuracy); (c) DeiT-Small + Token selector with10% data sparsity + SViTE with 50% unstructured\nsparsity (79.91 test accuracy). It demonstrates that simultaneously enforcing data and architecture\nsparsity brings more performance gains.\nA17\nAttention Heads MLP\nInitial Explored Initial Explored\nFigure A7: (Left) The “survivors\" summary of existing attention heads in sparse vision transformers from\nS2ViTE. Dark entry is the pruned attention head; bright entry means the remaining attention head. ( Right)\nBinary masks of all W(l,3) MLPs in S2ViTE-Base. Initial denotes the random connectivity in the beginning,\nand Explored is the explored typologies at the end. Visualized S2ViTE-Base has 40% structural sparsity.\nAblation of the layerwise sparsity of attention maps. As shown in Table A8, we investigate the\nlayerwise sparsity of attention maps. Dense DeiT-Small and SViTE+-Small with 10% data sparsity\nand 50% model sparsity are adopted for experiments. We calculate the percentage of elements in\nattention maps whose magnitude is smaller than 10−4. We observe that the bottom layers’ attention\nmaps of SViTE+ are denser than the ones in dense ViT, while it is opposite for the top layers.\nTable A8: Layerwise sparsity of attention maps.\nLayer Index 1 2 3 4 5 6 7 8 9 10 11 12\nDense-Small 28.44 44.70 37.04 13.87 13.62 13.67 12.98 5.99 5.08 3.46 2.74 15.05\nSViTE+-Small24.37 33.24 28.05 19.51 9.03 13.79 13.24 8.88 6.78 5.65 2.88 35.22\nA18",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7822438478469849
    },
    {
      "name": "FLOPS",
      "score": 0.663619339466095
    },
    {
      "name": "Inference",
      "score": 0.5841006636619568
    },
    {
      "name": "Transformer",
      "score": 0.49530282616615295
    },
    {
      "name": "Machine learning",
      "score": 0.4786849617958069
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46177706122398376
    },
    {
      "name": "Generalization",
      "score": 0.43790706992149353
    },
    {
      "name": "Parallel computing",
      "score": 0.16844165325164795
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 85
}