{
  "title": "An Empirical Study of Training End-to-End Vision-and-Language Transformers",
  "url": "https://openalex.org/W3210438479",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227219689",
      "name": "Dou, Zi-Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A84072095",
      "name": "Xu Yi-chong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2365636915",
      "name": "Gan Zhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1901876785",
      "name": "Wang Jian-feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202139473",
      "name": "Wang, Shuohang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1578702133",
      "name": "Wang Li-juan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2177274999",
      "name": "Zhu, Chenguang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2370394005",
      "name": "Zhang, Pengchuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101029194",
      "name": "Yuan Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4213617796",
      "name": "Peng, Nanyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352283912",
      "name": "Liu, Zicheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202139475",
      "name": "Zeng, Michael",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035688398",
    "https://openalex.org/W3106539628",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3215626407",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W3110570034",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W3211483028",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W3128413221",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3181158454",
    "https://openalex.org/W2963323244",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3170016573",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W3173631098",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W2912371042",
    "https://openalex.org/W1773149199",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W3176641147",
    "https://openalex.org/W2962931466",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3167118264",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W3177224328",
    "https://openalex.org/W2888520903"
  ],
  "abstract": "Vision-and-language (VL) pre-training has proven to be highly effective on various VL downstream tasks. While recent work has shown that fully transformer-based VL models can be more efficient than previous region-feature-based methods, their performance on downstream tasks often degrades significantly. In this paper, we present METER, a Multimodal End-to-end TransformER framework, through which we investigate how to design and pre-train a fully transformer-based VL model in an end-to-end manner. Specifically, we dissect the model designs along multiple dimensions: vision encoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa, DeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention), architectural design (e.g., encoder-only vs. encoder-decoder), and pre-training objectives (e.g., masked image modeling). We conduct comprehensive experiments and provide insights on how to train a performant VL transformer. METER achieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images for pre-training, surpassing the state-of-the-art region-feature-based model by 1.04%, and outperforming the previous best fully transformer-based model by 1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy of 80.54%. Code and pre-trained models are released at https://github.com/zdou0830/METER.",
  "full_text": "An Empirical Study of Training End-to-End Vision-and-Language Transformers\nZi-Yi Dou1*, Yichong Xu2, Zhe Gan2, Jianfeng Wang2, Shuohang Wang2, Lijuan Wang2,\nChenguang Zhu2, Pengchuan Zhang2, Lu Yuan2, Nanyun Peng1, Zicheng Liu2, Michael Zeng2\n1University of California, Los Angeles, 2Microsoft Corporation\n{zdou,violetpeng}@cs.ucla.edu\n{yicxu,zhgan,jianfw,shuowa,lijuanw,chezhu,penzhan,luyuan,zliu,nzeng}@microsoft.com\nAbstract\nVision-and-language (VL) pre-training has proven to be\nhighly effective on various VL downstream tasks. While\nrecent work has shown that fully transformer-based VL\nmodels can be more efﬁcient than previous region-feature-\nbased methods, their performance on downstream tasks\noften degrades signiﬁcantly. In this paper, we present\nMETER , a Multimodal End-to-end TransformER frame-\nwork, through which we investigate how to design and\npre-train a fully transformer-based VL model in an end-\nto-end manner. Speciﬁcally, we dissect the model designs\nalong multiple dimensions: vision encoders (e.g., CLIP-\nViT, Swin transformer), text encoders (e.g., RoBERTa, De-\nBERTa), multimodal fusion module (e.g., merged attention\nvs. co-attention), architectural design (e.g., encoder-only\nvs. encoder-decoder), and pre-training objectives (e.g.,\nmasked image modeling). We conduct comprehensive ex-\nperiments and provide insights on how to train a perfor-\nmant VL transformer. METER achieves an accuracy of\n77.64% on the VQAv2 test-std set using only 4M images for\npre-training, surpassing the state-of-the-art region-feature-\nbased model by 1.04%, and outperforming the previous best\nfully transformer-based model by 1.6%. Notably, when fur-\nther scaled up, our best VQA model achieves an accuracy\nof 80.54%. Code and pre-trained models are released at\nhttps://github.com/zdou0830/METER.\n1. Introduction\nVision-and-language (VL) tasks, such as visual ques-\ntion answering (VQA) [1] and image-text retrieval [33, 40],\nrequire an AI system to comprehend both the input im-\nage and text contents. Vision-and-language pre-training\n(VLP) has now become the de facto practice to tackle these\ntasks [6, 30, 32, 38, 49, 51]. Speciﬁcally, large amounts of\nimage-caption pairs are fed into a model that consumes both\nimages and texts to pretrain representations that contain rich\n* Work was done when the author interned at Microsoft.\nCLIP-ViT, Swin,\nBEiT, ....\nBERT, RoBERTa,\nDeBERTa, ...\nMerged Attention/\nCoattention\nDecoder\n (optional)\nMasked Language Modeling,\nImage-Text Matching,\nMasked Image Modeling,\n...\na man hitting a \ntennis ball with \na racquet.\nText Encoder\nVision Encoder\nMultimodal Fusion\nPretraining Objec-\ntives\nMETER\nFigure 1. An overview of the proposed METER framework. We\nsystematically investigate how to train a performant vision-and-\nlanguage transformer, and dissect the model designs along multi-\nple dimensions: vision encoder, text encoder, multimodal fusion\nmodule, architectural design (encoder-only vs. encoder-decoder),\nand pre-training objectives.\nmultimodal knowledge and is helpful for downstream tasks.\nTransformers [55] are prevalent in natural language pro-\ncessing and have recently shown promising performance\nin computer vision [12, 36]. While almost all the existing\nVLP models adopt transformers for their multimodal fusion\nmodule, most of them [6, 30, 32, 38, 49, 51] use pre-trained\nobject detectors (e.g., Faster RCNN [44]) on the vision side\nto extract region features from images. This can lead to\nseveral problems: ﬁrst, the object detectors are not perfect,\nbut are usually kept frozen during VLP, which limits the ca-\npacity of the VLP models; second, it is time-consuming to\nextract region features [25]. On the other hand, vision trans-\nformers (ViTs) have been an increasingly active research\ntopic in computer vision and have shown great potential\nin vision feature extraction. Therefore, a natural question\narises: can we train a fully transformer-based VLP model\nwith ViTs as the image encoder?\nRecent works [25,29,60] that tried to adopt vision trans-\nformers have not shown satisfactory performance and typ-\nically underperform state-of-the-art region-feature-based\nVLP models ( e.g., VinVL [65]). To close the perfor-\nmance gap, we present M ETER , a Multimodal End-to-end\nTransformER framework, through which we thoroughly in-\nvestigate how to design and pre-train a fully transformer-\narXiv:2111.02387v3  [cs.CV]  18 Mar 2022\nModel Vision Encoder Text Encoder Multimodal Fusion Decoder Pre-training Objectives\nViLBERT [38] OD+Xformer Xformer Co-attn.\n\u0017\nMLM+ITM+MIM\nLXMERT [51] MLM+ITM+MIM+VQA\nVisualBERT [30]\nOD Emb. Merged-attn.\nMLM+ITM\nVL-BERT [49] MLM+MIM\nUNITER [6] MLM+ITM+MIM+WRA\nOSCAR [32] MLM+ITM\nVinVL [65] MLM+ITM\nVL-T5 [7] \u0013 MLM+ITM+VQA+Grounding+Captioning\nPixelBERT [20]\nCNN Emb. Merged-attn.\n\u0017\nMLM+ITM\nSOHO [19] MLM+ITM+MIM\nCLIP-ViL [48] MLM+ITM+VQA\nSimVLM [58] \u0013 PreﬁxLM\nMDETR [24] Xformer OD+Token Prediction+Contrastive Alignment\nViLT [25] Patch Emb. Emb. Merged-attn.\n\u0017\nMLM+ITM\nVisual Parsing [60]\nXformer\nMLM+ITM+MIM\nALBEF [29] Xformer Co-attn. MLM+ITM+ITC\nMETER (Ours) MLM+ITM\nCLIP [41] CNN/Xformer Xformer None \u0017 ITCALIGN [22] CNN\nTable 1. Glossary of representative VLP models . OD: objective detector. Xformer: transformer. Emb.: embedding. MLM/MIM:\nmasked language/image modeling. ITM: image-text matching. WRA: word-region alginment. ITC: image-text contrastive learning.\nbased VLP model in an end-to-end manner. Speciﬁ-\ncally, as shown in Figure 1, we dissect the model designs\nalong multiple dimensions, including vision encoders (e.g.,\nCLIP-ViT [41], Swin transformer [36]), text encoders (e.g.,\nRoBERTa [35], DeBERTa [16]), multimodal fusion mod-\nule (e.g., merged attention vs. co-attention), architectural\ndesign (e.g., encoder-only vs. encoder-decoder), and pre-\ntraining objectives (e.g., masked image modeling [2]).\nWe perform the investigation by pre-training models un-\nder METER on four commonly used image-caption datasets:\nCOCO [33], Conceptual Captions [47], SBU Captions [39],\nand Visual Genome [26]. We test them on visual ques-\ntion answering [1], visual reasoning [50], image-text re-\ntrieval [33, 40], and visual entailment [59] tasks. Through\nextensive analyses, we summarize our ﬁndings as follows:\n• Vision transformer (ViT) plays a more vital role than lan-\nguage transformer in VLP, and the performance of trans-\nformers on pure vision or language tasks is not a good\nindicator for its performance on VL tasks.\n• The inclusion of cross-attention beneﬁts multimodal fu-\nsion, which results in better downstream performance\nthan using self-attention alone.\n• Under a fair comparison setup, the encoder-only VLP\nmodel performs better than the encoder-decoder model\nfor VQA and zero-shot image-text retrieval tasks.\n• Adding the masked image modeling loss in VLP will not\nimprove downstream task performance in our settings.\nThese insights, combined with other useful tips and tricks\ndetailed in later sections, enable us to train a strong model\nthat achieves an accuracy of 77.64% on the VQAv2 test-\nstd set, surpassing the previous best region-feature-based\nVinVL model [65] by 1.04% and outperforming the pre-\nviously best ViT-based model (i.e., ALBEF [29]) by 1.6%.\nNotably, when further scaled up, our best M ETER model\nachieves an accuracy of 80.54% on the VQAv2 test-std set.\n2. Glossary of VLP Models\nIn this section, we provide an overview of representative\nVLP models, and divide them into three categories based on\nhow they encode images, as summarized in Table 1.\nOD-based Region Features. Most previous work use pre-\ntrained object detectors (ODs) to extract visual features.\nAmong them, ViLBERT [38] and LXMERT [51] use co-\nattention for multimodal fusion, where two transformers\nare applied independently to region and text features, and\nanother transformer fuses the representations of the two\nmodalities in a later stage. On the other hand, Visual-\nBERT [30], VL-BERT [49], and UNITER [6] use a merged\nattention fusion module that feeds both region and text fea-\ntures together into a single transformer. OSCAR [32] and\nVinVL [65] feed additional image tags into the transformer\nmodel, and demonstrate state-of-the-art performance across\nVL tasks. However, extracting region features can be time-\nconsuming, and the pre-trained ODs are usually frozen dur-\ning pre-training, which limits the capacity of VLP models.\nCNN-based Grid Features. To tackle the above two is-\nsues, researchers have tried different ways to pre-train VL\nmodels in an end-to-end fashion. Among them, Pixel-\nBERT [20] and CLIP-ViL [48] propose to feed grid features\nfrom convolutional neural networks (CNNs) and text di-\nrectly into a transformer. SOHO [19] proposes to to ﬁrst dis-\ncretize grid features using a learned vision dictionary, then\nfeed the discretized features into their cross-modal module.\nWhile using grid features directly can be efﬁcient, incon-\nsistent optimizers are typically used for CNN and trans-\nformer. For example, PixelBERT [20] and CLIP-ViL [48]\nuse AdamW [37] for transformer and SGD for CNN. Re-\ncent work on vision transformers (ViTs) has also shown that\nCNN can achieve slightly worse accuracy/FLOPs trade-offs\nthan their ViT counterparts [36], motivating researchers to\ndevelop ViT-based VLP models.\nViT-based Patch Features. ViLT [25] directly feeds im-\nage patch features and text token embeddings into a pre-\ntrained ViT model, and ﬁne-tunes the model on image-\ncaption datasets. More recently, visual parsing [60] and\nALBEF [29] use ViT as the image encoder and design dif-\nferent pre-training objectives for ViT-based VLP models.\nHowever, all these models lag behind the state-of-the-art\nperformance on downstream tasks such as visual question\nanswering. In this paper, we investigate how to pre-train a\nViT-based model in an end-to-end manner that closes the\nperformance gap while maintaining fast inference speed.\n3. The METER Framework\nBased on the previous work, we identify several impor-\ntant modules in VLP models as in Figure 1. In this section,\nwe ﬁrst illustrate our M ETER framework, then our default\nsettings, which paves the way for our analyses hereinafter.\nOverview. Given a text sentence l and an image v, a VLP\nmodel ﬁrst extracts both text features l = ⟨l1, ··· , lN ⟩and\nvisual features v = ⟨v1, ··· , vM ⟩via a text encoder and a\nvision encoder. The text and visual features are then fed into\na multimodal fusion module to produce cross-modal repre-\nsentations, which are then optionally fed into a decoder be-\nfore generating the ﬁnal outputs.\n3.1. Model Architecture\nVision Encoder. In this paper, we focus on patch features,\nand study the use of vision transformers (ViTs) [12] for\nvision encoder. Speciﬁcally, in ViT, an image is ﬁrst seg-\nmented into patches, and then the patches are fed into a\ntransformer model. ViT has become a popular research\ntopic recently [2, 12, 36, 52, 52, 53, 64], and has been intro-\nduced into VLP [25,29,60]. However, all these models only\nachieve inferior performance compared to state-of-the-art\nregion-feature-based models (e.g., VinVL [65]). Also, dif-\nferent pre-trained ViTs are used, lacking a systematic study\nof which ViTs are the best for VLP. In this work, we com-\npare the original ViT [12], DeiT [52], Distilled-DeiT [52],\nCaiT [53], VOLO [64], BEiT [2], Swin Transformer [36]\nand CLIP-ViT [41], to provide a comprehensive analysis on\nthe role of vision transformers.\nText Encoder. Following BERT [11] and RoBERTa [35],\nVLP models [6, 30, 32, 38, 49, 51] ﬁrst segment the input\nsentence into a sequence of subwords [46], then insert two\nspecial tokens at the beginning and the end of the sentence\nto generate the input text sequence. After we obtain the text\nSelf-Attn\nCross-Attn\nFeedforward\nMx\nVisual Feature Text Feature\nQV KV VV\nQV KL VL\nSelf-Attn\nCross-Attn\nFeedforward\nQL KV VV\nQL KL VL\n(a) Co-attention model.\nSelf-Attn\nFeedforward\nMx\nVisual Feature Text Feature\nQV KV/L VV/L QL KV/L VV/L (b) Merged attention model.\nFigure 2. Illustration of two types of multimodal fusion modules:\n(a) co-attention, and (b) merged attention.\nembeddings, existing works either feed them directly to the\nmultimodal fusion module [6,30], or to several text-speciﬁc\nlayers [38, 51] before the fusion. For the former, the fusion\nmodule is typically initialized with BERT, and the role of\ntext encoding and multimodal fusion is therefore entangled\nand absorbed in a single BERT model. Here, we aim to\ndisentangle the two modules, and use a text encoder ﬁrst\nbefore sending the features into the fusion module.\nLanguage model (LM) pre-training has demonstrated\nimpressive performance across tasks and different pre-\ntrained LMs have been proposed; however, most VLP mod-\nels still only use BERT for initialization [6]. In this work,\nwe study the use of BERT [11], RoBERTa [35], ELEC-\nTRA [8], ALBERT [28], and DeBERTa [16] for text encod-\ning. Besides, we also experiment on only using a simple\nword embedding look-up layer initialized with the BERT\nembedding layer as used in many previous works [6, 65].\nMultimodal Fusion. We study two types of fusion mod-\nules, namely, merged attention and co-attention [17], as il-\nlustrated in Figure 2. In the merged attention module, the\ntext and visual features are simply concatenated together,\nthen fed into a single transformer block. In the co-attention\nmodule, on the other hand, the text and visual features\nare fed into different transformer blocks independently, and\ntechniques such as cross-attention are used to enable cross-\nmodal interaction. For region-based VLP models, as shown\nin [4], the merged attention and co-attention models can\nachieve comparable performance. Yet, the merged atten-\ntion module is more parameter-efﬁcient, as the same set\nof parameters are used for both modalities. Since end-to-\nend VLP models are becoming increasingly popular, in this\nwork, we re-examine the impact of both types of fusion\nmodules in our new context.\nEncoder-Only vs. Encoder-Decoder. Many VLP mod-\nels such as VisualBERT [30] adopt the encoder-only archi-\ntecture, where the cross-modal representations are directly\nfed into an output layer to generate the ﬁnal outputs. Re-\ncently, VL-T5 [7] and SimVLM [58], on the other hand,\nadvocate the use of a transformer encoder-decoder archi-\ntecture, where the cross-modal representations are ﬁrst fed\ninto a decoder and then to an output layer. In their models,\nEncoder\na man hitting a \n[MASK] \n[MASK] with a \nracquet.\ntennis ball\n(a) Encoder-Only\nEncoder\na man hitting a \n[MASK] with a \nracquet.\nDecoder\n<s> tennis ball\n</s>tennis ball (b) Encoder-Decoder\nFigure 3. Illustration of the encoder-only and encoder-decoder\nmodel architectures for VLP.\nthe decoder attends to both the encoder representations and\nthe previously generated tokens, producing the outputs au-\ntoregressively. Figure 3 shows the difference between them\nwhen performing the masked language modeling task. For\nthe encoder-decoder model, when performing classiﬁcation\ntasks such as VQA, we feed the text inputs into its encoder\nand feed a classiﬁcation token into the decoder, and the de-\ncoder then generates the output class accordingly.\n3.2. Pre-training Objectives\nNow, we introduce how we pre-train our models. Specif-\nically, we will ﬁrst brieﬂy review masked language model-\ning and image-text matching, which have been used exten-\nsively in almost every VLP model. Then, we will shift our\nfocus to how we can design and explore interesting masked\nimage modeling tasks.\nMasked Language Modeling. The masked language mod-\neling (MLM) objective is ﬁrst introduced in pure language\npre-training [11, 35]. In VLP, MLM with images has also\nproven to be useful. Speciﬁcally, given an image-caption\npair, we randomly mask some of the input tokens, and the\nmodel is trained to reconstruct the original tokens given the\nmasked tokens lmask and its corresponding visual input v.\nImage-Text Matching. In image-text matching, the model\nis given a batch of matched or mismatched image-caption\npairs, and the model needs to identify which images and\ncaptions correspond to each other. Most VLP models treat\nimage-text matching as a binary classiﬁcation problem.\nSpeciﬁcally, a special token (e.g., [CLS]) is inserted at the\nbeginning of the input sentence, and it tries to learn a global\ncross-modal representation. We then feed the model with\neither a matched or mismatched image-caption pair ⟨v, l⟩\nwith equal probability, and a classiﬁer is added on top of\nthe [CLS] token to predict a binary label y, indicating if\nthe sampled image-caption pair is a match.\nMasked Image Modeling. Similar to the MLM objective,\nresearchers have tried masked image modeling (MIM) on\nthe vision side. For example, many previous work, such\nas LXMERT [51] and UNITER [6], mask some of the in-\nput regions, and the model is trained to regress the original\na man hitting a \ntennis ball with \na racquet.\nModel\nin-batch negatives\n...\n133   422    922\n235    234    287\n576    223    722\n287\ndiscrete code\nFigure 4. Illustration of masked patch classiﬁcation with in-batch\nnegatives and with discrete code.\nregion features. Formally, given a sequence of visual fea-\ntures v = ⟨v1, ··· , vM ⟩, where vi is typically a region fea-\nture, we randomly mask some of the visual features, and the\nmodel outputs the reconstructed visual featuresov given the\nrest of the visual features and the unmasked tokens t, and\nregression aims to minimize the mean squared error loss.\nResearchers [6, 38, 51] have also tried to ﬁrst generate ob-\nject label for each region using a pre-trained object detector,\nwhich can contain high-level semantic information, and the\nmodel is trained to predict the object labels for the masked\nregions instead of the original region features.\nNotably, recent state-of-the-art models ( e.g., AL-\nBEF [29], VinVL [65]) do not apply MIM during VLP.\nIn addition, in ViLT [25], the authors also demonstrate\nthat masked patch regression is not helpful in their setting.\nThese results make it questionable whether MIM is truly ef-\nfective for VLP models. To further investigate this, we treat\nmasked image modeling as a masked patch classiﬁcation\ntask, and propose two ways of implementing the idea.\n1) Masked Patch Classiﬁcation with In-batch Negatives.\nBy imitating MLM which uses a text vocabulary, we ﬁrst\npropose to let the model reconstruct input patches by using\na dynamically constructed vocabulary constructed with in-\nbatch negatives. Concretely, at each training step, we sam-\nple a batch of image-caption pairs {⟨vk, lk⟩}B\nk=1, where B\nis the batch size. We treat all the patches in{vk}B\nk=1 as can-\ndidate patches, and for each masked patch, we mask 15% of\nthe input patches, and the model needs to select the original\npatch within this candidate set. Denoting the original patch\nrepresentations and our model’s output representations of\n{vk}B\nk=1 as {c(vk)}B\nk=1 and {h(vk)}B\nk=1, respectively, we\ncan represent the output probability at positioni for the k-th\ninstance as:\np(vk\ni |[vk,mask; lk]) = eh(vk\ni )Tc(vk\ni )\n∑\nj,k′ eh(vk\ni )Tc(vk′\nj ) . (1)\nThe model is trained to maximize its probability similar to\nnoise contrastive estimation [15, 23].\n2) Masked Patch Classiﬁcation with Discrete Code. In-\nspired by BEiT [2], we also propose to obtain discrete repre-\nsentations of the input patches, and the model is then trained\nto reconstruct the discrete tokens. Speciﬁcally, we ﬁrst use\nthe VQ-V AE [54] model in DALL-E [43] to tokenize each\nimage into a sequence of discrete tokens. We resize each\nimage so that the number of patches is equal to the number\nof tokens, and thus each patch corresponds to a discrete to-\nken. Then, we randomly mask 15% of the patches and feed\nthe masked image patches to the model as before, but now\nthe model is trained to predict the discrete tokens instead of\nthe masked patches.\n3.3. Our Default Settings for METER\nThere are many different model designs under M ETER ,\nand we detail our default settings in this part.\nModel Architecture. The default setting of model archi-\ntecture is shown in Figure 2a. In the bottom part, there are\none pre-trained visual encoder and one pre-trained text en-\ncoder. On top of each encoder, we stackM = 6transformer\nencoding layers, with each layer consisting of one self-\nattention block, one cross-attention block, and one feed-\nforward network block. Unless otherwise stated, the hidden\nsize is set to768, and the number of heads is set to12 for the\ntop layers. Note that there is no decoder and no parameter\nsharing between the vision and language branches.\nPre-training Objectives. Unless otherwise stated, we pre-\ntrain the models with masked language modeling (MLM)\nand image-text matching (ITM) only. For MLM, we mask\n15% of the input text tokens, and the model tries to recon-\nstruct the original tokens. For ITM, we feed the model with\neither matched or mismatched image-caption pairs with\nequal probability, and the model needs to identify whether\nthe input is a match.\nPre-training Datasets. Following previous work [6, 25,\n29], we pre-train models on four commonly used datasets,\nincluding COCO [33], Conceptual Captions [47], SBU\nCaptions [39], and Visual Genome [26]. The statistics of\nthese datasets is shown in Appendix. The combined train-\ning data consists of about 4M images in total.\nDownstream Tasks. For ablation and analysis, we mainly\nfocus on VQAv2 [1], arguably the most popular dataset for\nVLP evaluation. We also test on Flickr30k zero-shot image-\ntext retrieval to remove any confounders that may be intro-\nduced during ﬁnetuning [17]. For VQAv2, we follow the\nstandard practice [6] to train the models with both training\nand validation data, and test the models on the test-dev set.\nFor Flickr30k, we follow the standard splits.\nFor comparison with state of the arts, we also evaluate\nmodels on visual reasoning (NLVR 2 [50]), visual entail-\nment (SNLI-VE [59]), and image-text retrieval (COCO [33]\nand Flickr30k [40]) tasks. For retrieval tasks, we evaluate\nmodels in both zero-shot and ﬁnetuning settings.\nText Enc. VQAv2 VE IR TR SQuAD MNLI\nAcc. Acc. R@1 R@1 EM Acc.\nEmb-only 67.13 74.85 49.06 68.20 - -\nELECTRA 69.22 76.57 41.80 58.30 86.8 88.8\nCLIP 69.31 75.37 54.96 73.80 - -\nDeBERTa 69.40 76.74 51.50 67.70 87.2 88.8\nBERT 69.56 76.27 49.60 66.60 76.3 84.3\nRoBERTa 69.69 76.53 49.86 68.90 84.6 87.6\nALBERT 69.94 76.20 52.20 68.70 86.4 87.9\nTable 2. Comparisons of different text encoders without VLP.\nCLIP-ViT-224/32 is used as the vision encoder. All the text en-\ncoders are in base model size, except ALBERT, which is xlarge.\nEmb-only: only using word embeddings as text encoder. IR/TR:\nFlickr30k image/text retrieval. EM: exact match. The results of\nSQuAD and MNLI are copied from their corresponding papers.\nAll the results on VL tasks are from their test-dev/val sets.\nVision Encoder VQAv2 VE IR TR ImageNet\nDis. DeiT B-384/16 67.84 76.17 34.84 52.10 85.2\nBEiT B-224/16 68.45 75.28 32.24 59.80 85.2\nDeiT B-384/16 68.92 75.97 33.38 50.90 82.9\nViT B-384/16 69.09 76.35 40.30 59.80 83.97\nCLIP B-224/32 69.69 76.53 49.86 68.90 -\nVOLO 4-448/32 71.44 76.42 40.90 61.40 86.8\nCaiT M-384/32 71.52 76.62 38.96 61.30 86.1\nCLIP B-224/16 71.75 77.54 57.64 76.90 -\nSwin B-384/32 72.38 77.65 52.30 69.50 86.4\nTable 3. Comparisons of different vision encoders without\nVLP. RoBERTa is used as the default text encoder. IR/TR:\nFlickr30k image/text retrieval; B: Base. The results of ImageNet\nclassiﬁcation are copied from their corresponding papers. All the\nresults on VL tasks are from their test-dev/val sets. N and M in\nViT-N/M denote the image resolution and patch size, respectively.\nImplementation Details. We pre-train our models using\nAdamW [37] for 100k steps. The learning rates for the bot-\ntom and top layers are set to 1e-5 and 5e-5 respectively dur-\ning pre-training. The warm-up ratio is set to 10%, and the\nlearning rate is linearly decayed to 0 after 10% of the total\ntraining steps. We use center-crop to resize each image into\nthe size of 224×224 or 384×384 depending on the adopted\nvision transformers.\n4. Experiments\nIn this section, we provide comprehensive analysis of\neach individual module design. Speciﬁcally, ( i) we study\nthe impact of vision and language encoders in Section 4.1,\n(ii) we perform analysis on multimodal fusion designs in\nSection 4.2, ( iii) we compare encoder-only and encoder-\ndecoder architectures in Section 4.3, and (iv) we ablate pre-\ntraining objectives in Section 4.4. Finally, we compare with\nstate of the arts in Section 4.5.\n4.1. Impact of Vision and Language Encoders\n4.1.1 Explorations without VLP\nSince pre-training is time-consuming, we ﬁrst perform an\nexploration study by comparing different text and visual en-\nText Enc. Vision Enc. VQAv2 Flickr-ZS\nIR TR\nEmb-only CLIP-32 73.99 60.32 74.10\nBERT CLIP-32 74.98 66.08 78.10\nCLIP-16 76.70 74.52 87.20\nRoBERTa\nCLIP-32 74.67 65.50 76.60\nCLIP-16 77.19 76.64 89.60\nSwin 76.43 71.68 85.30\nTable 4. Comparisons of different vision and text encoders\nwith VLP. Results on VQAv2 are on test-dev set. ZS: zero-shot.\ncoders without VLP for efﬁciency. Concretely, we initial-\nize the bottom layers with speciﬁc pre-trained vision and\ntext encoders, and randomly initialize the top layers. Then,\nwe directly ﬁnetune the models on three tasks, including\nVQAv2, SNLI-VE, and Flickr30k retrieval. The learning\nrates for the bottom and top layers are set to 1e-5 and 1e-\n4, and the number of training epochs is set to 10 for all the\ntasks. We choose CLIP-ViT-224/32 [41] and RoBERTa [35]\nas the default encoders. Here, N and M in ViT-N/M denote\nimage resolution and patch size, respectively.\nImpact of Text Encoders. As shown in Table 2, there are\nno signiﬁcant differences between the model performance\nof different text encoders. RoBERTa seems to achieve the\nmost robust performance in this setting. Also, as can be\nseen from the Emb-only results, it is necessary to have a\npre-trained encoder because otherwise the downstream task\nperformance will be degraded.\nImpact of Vision Encoders. As summarized in Table 3,\nboth CLIP-ViT-224/16 and Swin Transformer can achieve\ndecent performance in this setting. Notably, Swin Trans-\nformer can achieve an VQA score of 72.38 on the test-dev\nset without any VLP, which is already comparable to some\nVLP models after pre-training.\nConclusion. If we directly ﬁnetune the models on down-\nstream tasks without any VLP, RoBERTa and Swin Trans-\nformer or CLIP-ViT perform the best. While models such\nas DeBERTa and BEiT can achieve better performance than\nthe two models on pure language or vision tasks such as\nMNLI [56] or ImageNet classiﬁcation [10], that does not\nnecessarily indicate that they are more suitable for VL tasks.\n4.1.2 Results with VLP\nNow, we follow the default setting in Section 3.3, and\ncompare different vision/text encoders with VLP. Based on\nthe previous results, we compare Embed-only, BERT, and\nRoBERTa on the text side, and CLIP-ViT-224/32, CLIP-\nViT-224/16, and Swin Transformer on the image side.\nResults. As shown in Table 4, after VLP, the difference be-\ntween BERT and RoBERTa seems to be diminished, but it\nis still important to have a pre-trained text encoder on the\nbottom (Embed-only vs. RoBERTa). For vision encoder,\nBottom LR Top LR VQAv2 Flickr-ZS\nIR TR\n1e-5 1e-5 73.16 48.80 63.70\n2e-5 2e-5 73.66 53.14 67.20\n3e-5 3e-5 73.77 56.48 70.90\n5e-5 5e-5 73.54 52.48 65.90\n1e-5 5e-5 74.98 66.08 78.10\nTable 5. Using different learning rates for the randomly-initialized\nand pre-trained parameters is better than using the same learning\nrate. Results on VQAv2 are on test-dev set. ZS: zero-shot.\n224 384 576\nResolution\n73\n74\n75\n76\n77VQAv2 Test-Dev Score\nCLIP-ViT-224/32\nCLIP-ViT-224/16\nFigure 5. Increasing the image resolution during ﬁnetuning\ngreatly improves the performance on the VQAv2 test-dev set.\nboth CLIP-ViT-224/16 and Swin Transformer can achieve\npretty good performance. Especially, CLIP-ViT-224/16 can\nachieve a VQA score of 77.19/77.20 on the test-dev/test-std\nsets, respectively, outperforming the previous state-of-the-\nart region-based VinVL [65] models.\nUseful Tricks. In experiments, we found two tricks for\nViT-based VLP models that can greatly boost the perfor-\nmance. First, it is better to use a larger learning rate for the\nrandomly initialized parameters than parameters initialized\nwith pre-trained models, which is also found useful in some\nother NLP tasks [34]. As shown in Table 5, using the same\nlearning rate for all parts of the model will lead to degraded\nperformance, possibly because the pre-trained parameters\nalready contain certain amounts of knowledge about vision\nand language, and ﬁnetuning them aggressively can result\nin the loss of these valuable information.\nSecond, similar to several previous work [25,64], we ﬁnd\nthat increasing the image resolution during ﬁnetuning can\nimprove the model performance by a large margin, espe-\ncially when the ratio of image resolution to patch size is\nlow. Figure 5 shows that increasing the image resolution\nfrom 224 to 576 can improve the VQA score by about 3\nand 1 points for the CLIP-ViT-224/32 and CLIP-ViT-224/16\nmodel, respectively.\n4.2. Analysis of the Multimodal Fusion Module\nNow, following the default setting in Section 3.3, we per-\nform investigations on multimodal fusion. First, we design\nboth merged attention and co-attention models and inves-\ntigate their performance. For the merged attention model\n(Figure 2b), the top transformer consists ofMmerged encod-\nFusion Decoder VQAv2 Flickr-ZS\nIR TR\nMerged attention \u0017 74.00 57.46 73.10\nCo-attention 74.98 66.08 78.10\n\u0013 74.73 48.96 71.60\nTable 6. Co-attention performs better than merged attention in our\nsetting, and adding a decoder is not helpful for our discriminative\nVL tasks. Results on VQAv2 are on test-dev set. ZS: zero-shot.\ning layer, with each layer consisting of one self-attention\nblock and one feed-forward network block. To help the\nmodel distinguish between the two modalities, we add a\nmodality embedding to the input features before feeding\nthem to the top transformer. For the co-attention model\n(Figure 2a), we feed the text and visual features to twoMco-\nlayer transformers separately, and each top transformer en-\ncoding layer consists of one self-attention block, one cross-\nattention block, and one feed-forward network block. Com-\npared with merged attention, co-attention allows separate\ntransformation functions for the vision and language modal-\nities. We set Mmerged = 12 and Mco = 6 so that the\nnumbers of parameters of the two models are roughly com-\nparable to each other.\nResults. Table 6 reports the downstream performance of\nthe two models. The co-attention model performs better\nthan the merged attention model in our setting, indicating\nthat it is important to have different sets of parameters for\nthe two modalities. Note that this contradicts with the ﬁnd-\nings in region-based VLP models [4], possibly because ( i)\nﬁndings of region-based VLP models cannot directly ap-\nply to ViT-based VLP models; (ii) most region-based VLP\nmodels only use pre-trained visual encoders, and also do not\nhave a pre-trained text encoder included, thus the inconsis-\ntency between the two modalities will not favor symmetri-\ncal architecture like the co-attention model.\n4.3. Encoder-Only vs. Encoder-Decoder\nWe then compare the encoder-only and encoder-decoder\narchitecture. For the encoder-only model, we use the same\nco-attention model as in Section 4.2. For the encoder-\ndecoder model, we set the number of layers to 3 for both the\nencoder and decoder, and each decoding layer has two sepa-\nrate cross-attention blocks that attend to the vision and text\nrepresentations, respectively. According to [7], we adopt\nT5-style [42] language modeling objective as it works well\nfor their model. Speciﬁcally, we mask 15% of input text to-\nkens and replace contiguous text span with sentinel tokens,\nand the decoder is trained to reconstruct the masked tokens.\nFor image-text matching, we feed the decoder with a special\nclass token and it generates a binary output.\nResults. As shown in Table 6, the encoder-only model can\noutperform the encoder-decoder model on our two discrim-\ninative tasks, which is consistent with the ﬁndings in [7].\nPre-training Objectives VQAv2 Flickr-ZS\nIR TR\nMLM 74.19 - -\nITM 72.63 53.74 71.00\nMLM+ITM 74.98 66.08 78.10\nMLM+ITM + MIM (In-batch Negatives) 74.01 62.12 76.90\nMLM+ITM + MIM (Discrete Code) 74.21 59.80 76.30\nTable 7. Masked language modeling (MLM) and image-text\nmatching (ITM) can both improve the model performance, but\nboth of our designed masked image modeling (MIM) objectives\nlead to degraded performance on downstream tasks. Results on\nVQAv2 are on test-dev set. ZS: zero-shot.\nHowever, it should be noted that the encoder-decoder ar-\nchitecture is more ﬂexible, as it can perform tasks such as\nimage captioning which may not be that straightforward for\nan encoder-only model to be applied to.\n4.4. Ablations on Pre-training Objectives\nIn all the previous experiments, we pre-train our mod-\nels with different objectives, following the default setting\nin Section 3.3. Now, we alter the pre-training objectives.\nResults. As summarized in Table 7, both masked language\nmodeling and image-text matching can bring performance\nimprovements on downstream tasks. However, both of our\nmasked image modeling objectives can lead to degraded\nperformance on both VQAv2 and Flickr30k retrieval tasks.\nThis further indicates that conclusions in region-based VLP\nmodels may not necessarily hold in vision transformer-\nbased models. We hypothesize that the performance drop is\ndue to the conﬂicts between different objectives, and some\ntechniques in multi-task optimization [57, 62] may be bor-\nrowed to resolve the conﬂicts, which we list as one of the\nfuture directions. Another possible reason is that image\npatches can be noisy, thus the supervisions on reconstruct-\ning these noisy patches can be uninformative.\n4.5. Comparison with Prior Arts\nIn this section, we evaluate our best-performing mod-\nels (i.e., RoBERTa-base+Swin Transformer and RoBERT-\nbase+CLIP-ViT-224/16 with co-attention fusion module,\nand with image resolutions set to 384 and 288, respec-\ntively), and compare them with previous work. We eval-\nuate the models on visual question answering (VQAv2),\nvisual reasoning (NLVR 2), visual entailment (SNLI-VE),\nFlickr30k retrieval tasks in zero-shot and ﬁnetuning set-\ntings, and COCO retrieval tasks in the ﬁnetuning setting.\nMain Results. As in Table 8 and 9, compared with models\npre-trained with fewer than 10M images, our CLIP-based\nmodel (METER -CLIP-ViTBASE) can achieve either the best\nor the second best scores on all the downstream tasks. No-\ntably, our model can achieve a VQA score of 77.64% on the\nVQAv2 test-std set using only 4M images for pre-training,\nModel VQAv2 NLVR 2 SNLI-VE Flickr-ZS\ntest-dev test-std dev test dev test IR@1 IR@5 IR@10 TR@1 TR@5 TR@10\nPre-trained with >10M images\nALBEF (14M) [29] 75.84 76.04 82.55 83.14 80.80 80.91 82.8 96.3 98.1 94.1 99.5 99.7\nSimVLMBASE (1.8B) [58] 77.87 78.14 81.72 81.77 84.20 84.15 - - - - - -\nSimVLMHUGE (1.8B) [58] 80.03 80.34 84.53 85.15 86.21 86.32 - - - - - -\nPre-trained with <10M images\nUNITERLARGE [6] 73.82 74.02 79.12 79.98 79.39 79.38 68.74 89.20 93.86 83.60 95.70 97.70\nVILLALARGE [14] 74.69 74.87 79.76 81.47 80.18 80.02 - - - - - -\nUNIMOLARGE [31] 75.06 75.27 - - 81.11 80.63 - - - - - -\nVinVLLARGE [65] 76.52 76.60 82.67 83.98 - - - - - - - -\nPixelBERT [20] 74.45 74.55 76.5 77.2 - - - - - -\nCLIP-ViL (ResNet50x4) [48] 76.48 76.70 - - 80.61 80.20 - - - - - -\nViLT [65] 71.26 - 75.70 76.13 - - 55.0 82.5 89.8 73.2 93.6 96.5\nVisual Parsing [60] 74.00 74.17 77.61 78.05 - - - - - - - -\nALBEF (4M) [29] 74.54 74.70 80.24 80.50 80.14 80.30 76.8 93.7 96.7 90.5 98.8 99.7\nMETER -SwinBASE 76.43 76.42 82.23 82.47 80.61 80.45 71.68 91.80 95.30 85.30 97.70 99.20\nMETER -CLIP-ViTBASE 77.68 77.64 82.33 83.05 80.86 81.19 79.60 94.96 97.28 90.90 98.30 99.50\nTable 8. Comparisons with models pre-trained with <10M images on visual question answering, visual reasoning, visual entailment, and\nzero-shot image retrieval (IR) and text retrieval (TR) tasks. The best scores are in bold, and the second best scores are underlined.\nModel Flickr COCO\nIR@1 IR@5 IR@10 TR@1 TR@5 TR@10 IR@1 IR@5 IR@10 TR@1 TR@5 TR@10\nPre-trained with >10M images\nALBEF (14M) [29] 85.6 97.5 98.9 95.9 99.8 100.0 60.7 84.3 90.5 77.6 94.3 97.2\nPre-trained with <10M images\nUNITERLARGE [6] 75.56 94.08 96.76 87.30 98.00 99.20 52.93 79.93 87.95 65.68 88.56 93.76\nVILLALARGE [14] 76.26 94.24 96.84 87.90 97.50 98.80 - - - - - -\nUNIMOLARGE [31] 78.04 94.24 97.12 89.40 98.90 99.80 - - - - - -\nVinVLLARGE [65] - - - - - - 58.8 83.5 90.3 75.4 92.9 96.2\nPixelBERT [20] 71.5 92.1 95.8 87.0 98.9 99.5 50.1 77.6 86.2 63.6 87.5 93.6\nViLT [65] 64.4 88.7 93.8 83.5 96.7 98.6 42.7 72.9 83.1 61.5 86.3 92.7\nVisual Parsing [60] 73.5 93.1 96.4 87.0 98.4 99.5 - - - - - -\nALBEF (4M) [29] 82.8 96.7 98.4 94.3 99.4 99.8 56.8 81.5 89.2 73.1 91.4 96.0\nMETER -SwinBASE 79.02 95.58 98.04 92.40 99.00 99.50 54.85 81.41 89.31 72.96 92.02 96.26\nMETER -CLIP-ViTBASE 82.22 96.34 98.36 94.30 99.60 99.90 57.08 82.66 90.07 76.16 93.16 96.82\nTable 9. Comparisons with models pre-trained with <10M images on Flickr30k and COCO image retrieval (IR) and text retrieval (TR)\ntasks in the ﬁnetuning setting. The best scores are in bold, and the second best scores are underlined.\nModel (#Pre-training Images) test-dev test-std\nSimVLMBASE (1.8B) 77.87 78.14\nSimVLMHUGE (1.8B) 80.03 80.34\nMETER -CoSwinHUGE (14M) 80.33 80.54\nTable 10. Pre-training a huge model under the M ETER frame-\nwork with 14M images can lead to state-of-the-art performance\non VQAv2, surpassing previous models trained with 1.8B images.\nsurpassing the state-of-the-art region-feature-based VinVL\nmodel by 1.04%, and outperforming the previous best fully\ntransformer-based model (i.e., ALBEF) by 1.6%. In addi-\ntion, while ALBEF has specially-designed objectives for re-\ntrieval, our model can still outperform ALBEF on text and\nimage retrieval tasks, further demonstrating the effective-\nness of METER . Also, as shown in Appendix, we can main-\ntain the fast inference speed of ViT-based models.\nScaling the Model. We also investigate if the M ETER\nframework is scalable. To this end, we pre-train our model\nwith more images and larger vision backbone. Speciﬁcally,\nwe pre-train the model with COCO, CC, CC12M [5], SBU,\nand VG datasets, consisting of about 14M images and 20M\nimage-caption pairs in total. We use CoSwin-Huge [63] as\nour vision backbone and RoBERTa-base as our text back-\nbone. The hidden size of the fusion module remains un-\nchanged. As shown in Table 10, our model can achieve\nstate-of-the-art performance on VQAv2, surpassing previ-\nous models trained with 1.8B images. The results indicate\nthat our METER framework is scalable.\nFurther Analysis. We also conduct experiments on image\ncaptioning, investigate multi-scale feature fusion, study the\nmodel performance on unimodal tasks after VLP, and pro-\nvide visualization of learned attention maps. All these re-\nsults are provided in Appendix.\n5. Conclusion\nWe present METER , through which we systematically in-\nvestigate how to train a fully-transformer VLP model in an\nend-to-end manner. Experiments demonstrate that we can\nachieve competitive performance with state-of-the-art mod-\nels with only 4M images for pre-training. When further\nscaled up, METER achieves new state of the art on VQA.\nReferences\n[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\nVQA: Visual question answering. In International Confer-\nence on Computer Vision (ICCV), 2015.\n[2] Hangbo Bao, Li Dong, and Furu Wei. BEiT: Bert pre-\ntraining of image transformers. arXiv preprint, 2021.\n[3] Ankur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao, and\nYonghui Wu. Training deeper neural machine translation\nmodels with transparent attention. In Conference on Em-\npirical Methods in Natural Language Processing (EMNLP),\n2018.\n[4] Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, and\nDesmond Elliott. Multimodal pretraining unmasked: Uni-\nfying the vision and language BERTs. Transactions of the\nAssociation for Computational Linguistics (TACL), 2021.\n[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts. InConference\non Computer Vision and Pattern Recognition (CVPR), 2021.\n[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUNITER: Universal image-text representation learning. In\nEuropean Conference on Computer Vision (ECCV), 2020.\n[7] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying\nvision-and-language tasks via text generation. In Interna-\ntional Conference on Machine Learning (ICML), 2021.\n[8] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christo-\npher D Manning. Electra: Pre-training text encoders as dis-\ncriminators rather than generators. In International Confer-\nence on Learning Representations (ICLR), 2020.\n[9] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In Conference on Computer\nVision and Pattern Recognition (CVPR) Workshops, 2020.\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2009.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics (NAACL), 2019.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. InInternational Con-\nference on Learning Representations (ICLR), 2021.\n[13] Zi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, and\nTong Zhang. Exploiting deep representations for neural ma-\nchine translation. In Conference on Empirical Methods in\nNatural Language Processing (EMNLP), 2018.\n[14] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng,\nand Jingjing Liu. Large-scale adversarial training for vision-\nand-language representation learning. InConference on Neu-\nral Information Processing Systems (NeurIPS), 2020.\n[15] Michael Gutmann and Aapo Hyv ¨arinen. Noise-contrastive\nestimation: A new estimation principle for unnormalized sta-\ntistical models. In International Conference on Artiﬁcial In-\ntelligence and Statistics (AISTATS), 2010.\n[16] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu\nChen. DeBERTa: Decoding-enhanced bert with disentan-\ngled attention. arXiv preprint, 2020.\n[17] Lisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-\nBaptiste Alayrac, and Aida Nematzadeh. Decoupling the\nrole of data, attention, and losses in multimodal transformers.\nTransactions of the Association for Computational Linguis-\ntics (TACL), 2021.\n[18] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional net-\nworks. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017.\n[19] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu,\nDongmei Fu, and Jianlong Fu. Seeing out of the box: End-to-\nend pre-training for vision-language representation learning.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), 2021.\n[20] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and\nJianlong Fu. Pixel-BERT: Aligning image pixels with text by\ndeep multi-modal transformers. arXiv preprint, 2020.\n[21] Ganesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah. What\ndoes BERT learn about the structure of language? In An-\nnual Meeting of the Association for Computational Linguis-\ntics (ACL), 2019.\n[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representation\nlearning with noisy text supervision. arXiv preprint, 2021.\n[23] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. Exploring the limits of language\nmodeling. arXiv preprint, 2016.\n[24] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion. Mdetr-\nmodulated detection for end-to-end multi-modal understand-\ning. In International Conference on Computer Vision\n(ICCV), 2021.\n[25] Wonjae Kim, Bokyung Son, and Ildoo Kim. ViLT: Vision-\nand-language transformer without convolution or region su-\npervision. In International Conference on Machine Learning\n(ICML), 2021.\n[26] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International Journal of Computer Vi-\nsion (IJCV), 2017.\n[27] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009.\n[28] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin\nGimpel, Piyush Sharma, and Radu Soricut. Albert: A lite\nbert for self-supervised learning of language representations.\nIn International Conference on Learning Representations\n(ICLR), 2020.\n[29] Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Got-\nmare, Shaﬁq Joty, Caiming Xiong, and Steven Hoi. Align be-\nfore fuse: Vision and language representation learning with\nmomentum distillation. In Conference on Neural Informa-\ntion Processing Systems (NeurIPS), 2021.\n[30] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,\nand Kai-Wei Chang. VisualBERT: A simple and performant\nbaseline for vision and language. arXiv preprint, 2019.\n[31] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu,\nJiachen Liu, Hua Wu, and Haifeng Wang. Unimo: Towards\nuniﬁed-modal understanding and generation via cross-modal\ncontrastive learning. In Annual Meeting of the Association\nfor Computational Linguistics (ACL), 2021.\n[32] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei\nHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\nWei, et al. Oscar: Object-semantics aligned pre-training for\nvision-language tasks. InEuropean Conference on Computer\nVision (ECCV), 2020.\n[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nEuropean Conference on Computer Vision (ECCV), 2014.\n[34] Yang Liu and Mirella Lapata. Text summarization with pre-\ntrained encoders. In Conference on Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\n[35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. RoBERTa: A robustly opti-\nmized bert pretraining approach. arXiv preprint, 2019.\n[36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nInternational Conference on Computer Vision (ICCV), 2021.\n[37] Ilya Loshchilov and Frank Hutter. Decoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations (ICLR), 2018.\n[38] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vil-\nbert: Pretraining task-agnostic visiolinguistic representations\nfor vision-and-language tasks. In Conference on Neural In-\nformation Processing Systems (NeurIPS), 2019.\n[39] Vicente Ordonez, Girish Kulkarni, and Tamara Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. In Conference on Neural Information Processing\nSystems (NeurIPS), 2011.\n[40] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.\nFlickr30k Entities: Collecting region-to-phrase correspon-\ndences for richer image-to-sentence models. InInternational\nConference on Computer Vision (ICCV), 2015.\n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning\n(ICML), 2021.\n[42] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with a\nuniﬁed text-to-text transformer. Journal of Machine Learn-\ning Research (JMLR), 2020.\n[43] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning (ICML), 2021.\n[44] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards real-time object detection with re-\ngion proposal networks. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence (TPAMI), 2016.\n[45] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-CAM: Visual explanations from deep networks via\ngradient-based localization. In International Conference on\nComputer Vision (ICCV), 2017.\n[46] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neu-\nral machine translation of rare words with subword units. In\nAnnual Meeting of the Association for Computational Lin-\nguistics (ACL), 2016.\n[47] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In An-\nnual Meeting of the Association for Computational Linguis-\ntics (ACL), 2018.\n[48] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal,\nAnna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt\nKeutzer. How much can clip beneﬁt vision-and-language\ntasks? arXiv preprint, 2021.\n[49] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. VL-BERT: Pre-training of generic\nvisual-linguistic representations. In International Confer-\nence on Learning Representations (ICLR), 2019.\n[50] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun\nBai, and Yoav Artzi. A corpus for reasoning about natural\nlanguage grounded in photographs. In Annual Meeting of\nthe Association for Computational Linguistics (ACL), 2019.\n[51] Hao Tan and Mohit Bansal. LXMERT: Learning cross-\nmodality encoder representations from transformers. InCon-\nference on Empirical Methods in Natural Language Process-\ning (EMNLP), 2019.\n[52] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. arXiv preprint, 2020.\n[53] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv´e J´egou. Going deeper with im-\nage transformers. arXiv preprint, 2021.\n[54] Aaron van den Oord, Oriol Vinyals, and Koray\nKavukcuoglu. Neural discrete representation learning.\nIn Conference on Neural Information Processing Systems\n(NeurIPS), 2017.\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Conference on\nNeural Information Processing Systems (NeurIPS), 2017.\n[56] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel Bowman. GLUE: A multi-task\nbenchmark and analysis platform for natural language un-\nderstanding. In International Conference on Learning Rep-\nresentations (ICLR), 2019.\n[57] Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.\nGradient vaccine: Investigating and improving multi-task\noptimization in massively multilingual models. In Inter-\nnational Conference on Learning Representations (ICLR) ,\n2020.\n[58] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia\nTsvetkov, and Yuan Cao. Simvlm: Simple visual language\nmodel pretraining with weak supervision. arXiv preprint,\n2021.\n[59] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual\nentailment: A novel task for ﬁne-grained image understand-\ning. arXiv preprint, 2019.\n[60] Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jian-\nlong Fu, Houqiang Li, and Jiebo Luo. Probing inter-\nmodality: Visual parsing with self-attention for vision-\nlanguage pre-training. In Conference on Neural Information\nProcessing Systems (NeurIPS), 2021.\n[61] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Dar-\nrell. Deep layer aggregation. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2018.\n[62] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine,\nKarol Hausman, and Chelsea Finn. Gradient surgery for\nmulti-task learning. In Conference on Neural Information\nProcessing Systems (NeurIPS), 2020.\n[63] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\nBoxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu,\nYumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao,\nZhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and\nPengchuan Zhang. Florence: A new foundation model for\ncomputer vision. arXiv preprint, 2021.\n[64] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and\nShuicheng Yan. V olo: Vision outlooker for visual recog-\nnition. arXiv preprint, 2021.\n[65] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,\nLei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.\nVinVL: Revisiting visual representations in vision-language\nmodels. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021.\nCOCO VG CC SBU\n#Images 113K 108K 3.1M 875K\n#Captions 567K 5.4M 3.1M 875K\nTable 11. Statistics of the pre-training datasets.\nA. Implementation Details\nDatasets. The statistics of our pre-training datasets is\nshown in Table 11. Following many previous work [6,\n25, 29], we pre-train the models with four datasets, includ-\ning COCO, Visual Genome, Conceptual Captions and SBU\nCaptions, consisting of about 4M images and 9M image-\ncaption pairs in total.\nFor the downstream tasks, we test the models on\nVQAv2 [1] for visual question answering, NLVR2 [50] for\nvisual reasoning, COCO [33] and Flickr30k [40] for image-\ntext retrieval, and SNLI-VE [59] for visual entailment. We\nuse the standard splits for all the datasets except for VQAv2,\nwhere we follow standard practice [6, 25, 29] to train the\nmodels with both its training and development data, and\ntreat its test-dev set as the development set. Note that we\ndo not use the Visual Genome VQA data for data augmen-\ntation in our VQA settings.\nPre-training Settings. We pre-train our best models using\nthe AdamW optimizer [37] with the learning rates set to 1e-\n5 for the bottom image and text encoders and 5e-5 for the\ncross-modal module. The warm-up ratio is set to 10%, and\nthe learning rate is linearly decayed to 0 after 10% of the\ntotal training steps. The batch size, hidden size, and number\nof heads are set to 4096, 768, 12, respectively. We pre-train\nthe models for 100k steps on 8 NVIDIA A100 GPUs, which\ntakes around 3 days for M ETER -CLIP-ViTBASE−32 and 8\ndays for METER -SwinBASE and METER -CLIP-ViTBASE−16.\nFine-tuning Settings. For the downstream tasks, we per-\nform grid searches over the learning rates and image res-\nolutions. The learning rates and image resolutions are se-\nlected from {1e-6, 2e-6, 5e-6, 1e-5 }and {288, 384, 576 },\nrespectively. We apply RandAugment [9] during ﬁnetuning\nfollowing previous work [25, 29].\nB. Inference Time\nWe measure the inference time of different models as in\nTable 12. First, as shown in [25], their ViT-based model is\nmuch faster than previous region-feature-based VLP mod-\nels. In our setting, we measure the average inference time\nof processing 1 VQA instance on 1 NVIDIA V100 GPU.\nWe ﬁnd that while our model can be slower than the ViLT\nmodel, it is still signiﬁcantly faster than region-feature-\nbased models and comparable to other ViT-based ones. In\naddition, we can achieve much stronger performance on\ndownstream tasks than other models.\nModel Time ( [25]) Time (ours) VQAv2\nViLBERT 920 - 70.55\nVisualBERT 925 - 70.80\nLXMERT 900 - 72.42\nUNITER-Base 900 - 72.70\nOSCAR-Base 900 - 73.16\nVinVL-Base 650 - 75.95\nPixelBERT-X152 160 - 74.45\nCLIP-ViL (ResNet50x4) - 57 76.70\nViLT 15 26 71.26\nALBEF (14M) - 52 76.04\nMETER -SwinBASE - 59 76.42\nMETER -CLIP-ViTBASE - 53 77.64\nTable 12. Inference time (ms) of different models. We report the\ninference time measured by [25] and in our setting. We also list\nthe model performance on the VQAv2 test-std set.\nModel (#Pre-training Images) BLEU METEOR Cider SPICE\nOSCARBASE (4M) 36.5 30.3 123.7 23.1\nVinVLBASE (5.6M) 38.2 30.3 129.3 23.6\nSimVLMBASE (1.8B) 39.0 32.9 134.8 24.0\nMETER-CLIP-ViTBASE (4M) 38.8 30.0 128.2 23.0\nTable 13. Image captioning results of different models trained\nwith maximum likelihood estimation on COCO.\nC. Image Captioning\nWhile in this paper we mainly focus on ﬁnetuning our\nmodels for discriminative downstream tasks such as visual\nquestion answering, here we investigate if our models can\nalso be applied to generative tasks. Speciﬁcally, we ﬁnetune\nour models on the COCO image captioning task.\nWe ﬁnetune our M ETER -CLIP-ViTBASE model for 5\nepochs using the standard maximum likelihood estimation\nobjective. At each decoding step, instead of using the causal\nattention mechanism, the input image and all the text tokens\ncan attend to all the generated text tokens so as to minimize\nthe discrepancy between pre-training and ﬁnetuning. We\nuse beam search with the beam size set to 5.\nAs shown in Table 13, we can achieve reasonable perfor-\nmance on image captioning even though our model employs\nan encoder-only architecture. We expect that an encoder-\ndecoder model would be more suitable for generative tasks,\nwhich we leave as future work.\nD. Multi-scale Feature Fusion\nFor the pre-trained text and visual encoders, different\nlayers can contain different types of information. For ex-\nample, [21] ﬁnds that the intermediate layers of BERT en-\ncode a rich hierarchy of linguistic information, with surface\nfeatures at the bottom, syntactic features in the middle and\nsemantic features at the top. Aggregating the features at\ndifferent layers has demonstrated to be helpful in both vi-\nsion [18, 61] and language [3, 13]. Therefore, in this part,\nwe investigate if we can use feature fusion techniques to\nbetter utilize the information embedded at different layers\nModel VQAv2 Flickr-ZS\ntest-dev IR TR\nw/o pre-training\nMETER -SwinBASE w/o fusion 72.38 - -\nMETER -SwinBASE w/ fusion 72.91 - -\nMETER -CLIP-ViTBASE w/o fusion 71.75 - -\nMETER -CLIP-ViTBASE w/ fusion 72.92 - -\nwith pre-training\nMETER -SwinBASE w/o fusion 76.43 71.68 85.30\nMETER -SwinBASE w/ fusion 76.31 70.58 83.70\nMETER -CLIP-ViTBASE w/o fusion 77.19 76.64 89.60\nMETER -CLIP-ViTBASE w/ fusion 77.06 76.26 88.00\nTable 14. The fusion strategy improves the model performance\nwithout vision-and-language pre-training but can degrade the\nmodel performance after pre-training.\nof the pre-trained encoders.\nMethod. Based on some preliminary explorations, here we\nadopt a simple fusion strategy and only fuse the representa-\ntions of the text and image encoders but not the cross-modal\nlayers on the top. Speciﬁcally, given a text token or image\npatch xi, we ﬁrst feed it into a text or image encoder on\nthe bottom of our model ( e.g., BERT), and get its repre-\nsentations {h(xj\ni )}N\nj=0 at different layers, where N is the\nnumber of layers of the encoder. Then, we compute a gate\nvalue for each layer and perform a weighted sum to get the\nﬁnal representation of xi:\no(xi) =h(xN\ni ) +\nN−1∑\nj=0\ng(h(xj\ni ))h(xj\ni ), (2)\nwhere g is a linear transformation function. We then feed\no(xi) to the top cross-modal layers. Note that the fusion\ncan be done in both the text and visual encoders.\nResults. We pre-train the models using the co-attention\nmodel with RoBERTa as the text encoder and Swin Trans-\nformer as the visual encoder. We evaluate the models both\nwith and without VLP following the default settings. Be-\ncause Swin Transformers have different numbers of image\nrepresentations at different layers, we perform an average\npooling so that each layer has 12×12 patch representations.\nAs shown in Tab. 14, while the fusion strategy can improve\nthe model performance by a small margin without VLP, it\ncan degrade the model performance after pre-training. We\nhypothesize that this is because after the pre-training, the\nVLP model can learn how to well utilize the representations\nin the pre-trained encoders and layer fusion is not necessary.\nE. Correlation between Vision-and-Language\nTasks and Vision or Language Tasks\nIn this section, we perform a quantitative analysis of\nthe correlation between model performance on vision-and-\nlanguage tasks and pure vision or language tasks. We vary\n69.2 69.3 69.4 69.5 69.6 69.7 69.8 69.9\nVQA Score\n76\n78\n80\n82\n84\n86SQuAD EMBERT\nRoBERTa\nELECTRA\nDeBERTa\nALBERT\n(a) Vision-and-Language vs. Lan-\nguage\n68 69 70 71 72\nVQA Score\n83.0\n83.5\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5ImageNet Acc.\nViT\nDeiT\nDistileed DeiT\nCaiT\nYOLO\nSwin\nBEiT (b) Vision-and-Language vs. Vision\nFigure 6. Correlation between model performance on vision-and-\nlanguage tasks and pure vision or language tasks.\ndifferent text and vision encoders, and plot the model per-\nformance on the VQAv2 test-dev set and SQuAD or Ima-\ngeNet datasets as in Figure 6. We also compute the Pearson\ncorrelations in both cases. We ﬁnd that the Pearson correla-\ntion coefﬁcients and p-values are -0.09 and 0.88 in the VL\nvs. L setting, and 0.41 and 0.36 in the VL vs. V setting, in-\ndicating that there exists little to none correlations between\nthe model performance on VL tasks and V or L tasks.\nF. Unimodal Tasks\nWe also investigate the model performance on unimodal\ntasks after VLP. For text-only tasks, we ﬁnetune the bottom\ntext encoders on GLUE tasks; for image-only tasks, we ﬁt\na linear classiﬁer on the learned representations of image\nencoders on CIFAR-10 and CIFAR-100 [27].\nWe report results in Table 15 and 16. As shown in the ta-\nbles, our text encoder gets slightly worse performance on\nthe GLUE tasks on average; for image-only tasks, VLP\nseems to improve the model performance for Swin Trans-\nformer but not for CLIP-ViT, possibly because of domain\nissues. Note that in both sets of the experiments, we only\nuse our text or image encoder and discard the rest of the net-\nworks, and how to utilize multi-modal encoder to improve\nuni-modal performance is an open problem and we leave it\nas a future direction.\nG. Analysis on Pre-training Datasets\nWe also perform analysis on our pre-training datasets.\nWe pre-train our model on each of the pre-training datasets.\nWe choose CLIP-ViT-224/32 as the image encoder and\nBERT-base-uncased as the text encoder, and employ the co-\nattention fusion module. We pre-train the model for 50k\nsteps on each dataset and report the evaluation results on\nVQAv2 and Flickr30k zero-shot retrieval tasks.\nAs we can see from Table 17, both data size and do-\nmain similarity contribute to the downstream task perfor-\nmance. CC and VG are the largest datasets and COCO\nmost matches the downstream task domains, thus models\npre-trained on the three datasets obtain the highest scores.\nText Encoder QQP MNLI QNLI SST2 CoLA MRPC STSB RTE\nBefore VLP 91.31 ±0.15 87.53 ±0.24 92.61 ±0.34 94.38 ±0.20 58.72 ±0.73 91.03 ±0.59 90.15 ±0.18 71.24 ±3.07\nAfter VLP 91.34 ±0.08 87.38 ±0.18 92.67 ±0.06 93.92 ±0.50 57.88 ±0.79 90.57 ±0.78 89.93 ±0.46 70.28 ±2.00\nTable 15. Performance of text encoders (RoBERTa-base) on GLUE dev sets before and after VLP. The image encoder during VLP is\nCLIP-ViT-224/16. We report average scores and standard deviations over three runs of different random seeds.\nImage Encoder Before VLP After VLP\nCF10 CF100 CF10 CF100\nSwin-Base-384/32 97.00 89.15 97.99 90.26\nCLIP-ViT-224/16 95.85 82.60 94.92 81.90\nTable 16. Linear probe performance on CIFAR-10 and CIFAR-\n100. The text encoder during VLP is RoBERTa-base.\nPre-training Datasets VQAv2 Flickr-ZS\nIR TR\nCOCO 72.95 46.38 60.20\nCC 73.05 39.84 55.50\nSBU 70.14 21.52 35.90\nVG 73.54 39.24 49.30\nCOCO+CC+SBU+VG 74.98 66.08 78.10\nTable 17. Results of models pre-trained with different datasets.\nH. Visualization\nIn this section, we use Grad-CAM [45] to visualize our\nmodels. Speciﬁcally, we visualize the cross-attention maps\nof the pre-trained models corresponding to individual words\nwhen performing masked language modeling. As shown in\nFigure 7 and 8, both our Swin Transformer-based and CLIP-\nViT-based models can correctly attend to the corresponding\nregions given different words, suggesting that our models\ncan learn visual grounding implicitly during pre-training.\nI. Limitations\nWhile we have demonstrated the effectiveness of our\nmodels across different tasks, our models still have several\nlimitations:\nGenerative Tasks. We mainly focus on discriminative\ntasks such as visual question answering and visual reason-\ning in this paper, while generative tasks such as image cap-\ntioning are under-investigated. We perform experiments on\nthe COCO image captioning data in Appendix, and will in-\nvestigate more on this in the future.\nScalability. In our current settings, we pre-train the models\nwith 4M or 14M images, thus it is unclear how the model\nperformance would be if we pre-train the models with larger\ndatasets and we are actively experimenting in this direction.\nEnglish Data. So far, we only experiment on the English\ndata, and it is worth investigating if our models can gener-\nalize to other languages as well, which we leave as a future\ndirection.\nFigure 7. Visualization of the attention maps of text tokens in the caption “a display of ﬂowers growing out and over the retaining wall in\nfront of cottages on a cloudy day.” The ﬁrst and second rows correspond to the results of M ETER -SwinBASE and METER -CLIP-ViTBASE,\nrespectively.\nFigure 8. Visualization of the attention maps of text tokens in the caption “yellow squash, corn on the cob and green beans laid out on a\nwhite cloth.” The ﬁrst and second rows correspond to the results of METER -SwinBASE and METER -CLIP-ViTBASE, respectively.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7776410579681396
    },
    {
      "name": "Encoder",
      "score": 0.7455554008483887
    },
    {
      "name": "End-to-end principle",
      "score": 0.7421451210975647
    },
    {
      "name": "Computer science",
      "score": 0.7186676263809204
    },
    {
      "name": "Language model",
      "score": 0.49741557240486145
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4966593384742737
    },
    {
      "name": "Computer engineering",
      "score": 0.39055508375167847
    },
    {
      "name": "Machine learning",
      "score": 0.325626939535141
    },
    {
      "name": "Voltage",
      "score": 0.16996511816978455
    },
    {
      "name": "Engineering",
      "score": 0.16776803135871887
    },
    {
      "name": "Electrical engineering",
      "score": 0.09772908687591553
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}