{
    "title": "A Case Study on Test Case Construction with Large Language Models: Unveiling Practical Insights and Challenges",
    "url": "https://openalex.org/W4399205854",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Roberto F. Lima Jr",
            "affiliations": [
                "Popular University of Cesar",
                "Centro de Estudos e Sistemas Avançados do Recife"
            ]
        },
        {
            "id": "https://openalex.org/A5098957068",
            "name": "Luiz Fernando P. B. Presta",
            "affiliations": [
                "Centro de Estudos e Sistemas Avançados do Recife"
            ]
        },
        {
            "id": null,
            "name": "Lucca S. Borborema",
            "affiliations": [
                "Centro de Estudos e Sistemas Avançados do Recife"
            ]
        },
        {
            "id": null,
            "name": "Vanderson N. Silva",
            "affiliations": [
                "Centro de Estudos e Sistemas Avançados do Recife",
                "Popular University of Cesar"
            ]
        },
        {
            "id": null,
            "name": "Marcio L. M. Dahia",
            "affiliations": [
                "Centro de Estudos e Sistemas Avançados do Recife",
                "Popular University of Cesar"
            ]
        },
        {
            "id": "https://openalex.org/A2099190921",
            "name": "Anderson Santos",
            "affiliations": [
                "Centro de Estudos e Sistemas Avançados do Recife",
                "Popular University of Cesar"
            ]
        },
        {
            "id": null,
            "name": "Roberto F. Lima Jr",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5098957068",
            "name": "Luiz Fernando P. B. Presta",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Lucca S. Borborema",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Vanderson N. Silva",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Marcio L. M. Dahia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099190921",
            "name": "Anderson Santos",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3154856009",
        "https://openalex.org/W3119503018",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3098605233",
        "https://openalex.org/W3162689995",
        "https://openalex.org/W2766977941",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4391974543",
        "https://openalex.org/W4221143046"
    ],
    "abstract": "This study delves into the integration of Large Language Models (LLMs) in test case construction within software engineering, exploring their potential to enhance efficiency and effectiveness in test generation. Leveraging LLMs, known for their sophisticated natural language processing abilities, this research conducts a detailed case study on a representative software application to evaluate the practicality of LLMs in creating detailed and accurate test scenarios. The investigation focuses on the challenges and advantages of LLMs in test case development, assessing their impact on test comprehensiveness, accuracy, and the formulation process. By providing a nuanced understanding of LLMs’ role in software testing, this paper aims to inform practitioners and researchers about their potential and limitations, offering insights into their application in real-world testing environments and their contribution to advancing software testing methodologies.",
    "full_text": "A Case Study on Test Case Construction with Large Language\nModels: Unveiling Practical Insights and Challenges\nRoberto F. Lima Jr1, Luiz Fernando P. B. Presta2,\nLucca S. Borborema2, Vanderson N. Silva1,\nMarcio L. M. Dahia1, Anderson Santos1\n1CESAR\nRecife, PE, Brazil\n2CESAR School\nRecife, PE, Brazil.\n{rflj,vns, mlmd, acss}@cesar.org.br, {lfpbp, lsb}@cesar.school\nAbstract. This study delves into the integration of Large Language Models\n(LLMs) in test case construction within software engineering, exploring their\npotential to enhance efficiency and effectiveness in test generation. Leveraging\nLLMs, known for their sophisticated natural language processing abilities, this\nresearch conducts a detailed case study on a representative software applica-\ntion to evaluate the practicality of LLMs in creating detailed and accurate test\nscenarios. The investigation focuses on the challenges and advantages of LLMs\nin test case development, assessing their impact on test comprehensiveness, ac-\ncuracy, and the formulation process. By providing a nuanced understanding\nof LLMs’ role in software testing, this paper aims to inform practitioners and\nresearchers about their potential and limitations, offering insights into their ap-\nplication in real-world testing environments and their contribution to advancing\nsoftware testing methodologies.\n1. Introduction\nSoftware testing constitutes a critical phase in the software development life cycle, en-\nsuring the delivery of reliable and high-quality software products. In recent years, the\nadvent of advanced technologies, particularly Large Language Models (LLMs), has in-\ntroduced novel possibilities for automating and enhancing various facets of software en-\ngineering. LLMs, exemplified by models such as OpenAI’s GPT (Generative Pre-trained\nTransformer)[OpenAI 2023], exhibit remarkable natural language processing capabilities\nand have demonstrated applicability in diverse domains. This paper addresses the inter-\nsection of LLMs and software testing, focusing specifically on their role in the construc-\ntion of test cases.\nThe use of LLMs in software testing holds the promise of increasing the efficiency\nand effectiveness of test case generation. The ability of these models to understand and\ngenerate human-like text prompts paves the way for more natural and expressive artic-\nulation of test scenarios. However, the practical implications of integrating LLMs into\nthe nuanced process of test case construction remain under explored. This study seeks\nto bridge this gap by presenting a detailed case study that investigates the application of\nLLMs in constructing test cases for a real-world software application.\nThe case study methodology adopted in this research involves the selection of\na representative software application, the formulation of test case construction method-\nologies leveraging LLMs, and a thorough evaluation of the outcomes. By examining the\ninterplay between LLMs and the complexities inherent in software testing, this study aims\nto provide insights into the practical advantages, challenges, and considerations associated\nwith their implementation. Through this exploration, the intent is to contribute substan-\ntively to the ongoing dialogue surrounding the integration of cutting-edge technologies,\nsuch as LLMs, in the pursuit of optimizing software testing practices.\n2. Related Work\nThe intersection of Large Language Models (LLMs) and software engineering has wit-\nnessed growing interest in recent literature, reflecting a broader acknowledgment of the\ntransformative potential of these models in many domains. Within the realm of software\ntesting, where the efficacy of test case construction significantly influences software qual-\nity, the exploration of LLM applications remains a relatively nascent but promising area.\nThe preeminent contribution of LLMs to software engineering lies in their ability\nto comprehend and generate human-like text, a feature that has inspired its application\nin diverse natural language processing tasks. Prior studies, such as [Brown et al. 2020]\nshowcase the prowess of LLMs, exemplified by models like GPT-3, in understanding\ncontext, generating coherent responses, and adapting to diverse language inputs. This\nlinguistic versatility positions LLMs as potentially powerful tools for expressing and for-\nmulating test cases in a more natural, context-aware manner.\nTaking advantage of LLMs, many Generative AI models have been devel-\noped to implement diverse use cases, such as generating documentation for code\n[Feng et al. 2020], auto-completing code [Kim et al. 2021] and generating unit tests\n[Tufano et al. 2021].\nIn the specific context of software testing, recent research by [Wang et al. 2024]\nsummarized the use of LLMs in a diverse set of applications, integrating them with tradi-\ntional methods such as differential and mutation testing. Despite showing promise, LLMs\nare recognized as complementary rather than exclusive solutions for software testing. The\nresearch also combined LLMs with metamorphic testing, generating test cases based on\nexpected input-output relationships, and explored their role in model-based testing, en-\nhancing model accuracy through natural language descriptions and adapting to system\nchanges.\nHowever, existing literature lacks comprehensive insights into the practical impli-\ncations, challenges, and considerations associated with integrating LLMs into the intricate\ntask of test case construction for real-world software applications.\n3. Approach\nThis section outlines our approach, consisting of the semi-automated test cases construc-\ntions and the evaluation framework for it. First it was explored the integration of LLMs\nwith software specifications for generating test cases and later elucidates our method for\nqualitatively assess the effectiveness of the generated test cases.\n3.1. Test cases construction\nTo generate test cases from the model, the strategy of prompt engineering was employed.\nThe prompt, which serves as the input for the LLM, comprises a set of instructions and\nnecessary data for task completion. The quality of the model’s response improves with\nclearer and more contextual descriptions provided in the prompt. Another effective strat-\negy for achieving positive outcomes involves allowing the model to engage in reasoning\nby decomposing the task into intermediate steps [Wei et al. 2022].\nFigure 1. The workflow for building test cases using a Large Language Model\n(represented as the robot)\nAn interactive method was devised to derive test cases from the model, as outlined\nin Figure 1. Initially, engineers used a specially designed template to provide detailed ap-\nplication descriptions efficiently. This template, refined through extensive testing, helps\npinpoint the most relevant information for the model to grasp the software context. The\nmodel then generates a requirements document, which, combined with the initial descrip-\ntion, is used to formulate and produce test cases in the required format.\nThe OpenAI API was used with the GPT-3.5 Turbo model [OpenAI 2023] to-\ngether with the Python framework LangChain [LangChain 2023] in order to manage the\nprompts, insert the context, memory, and handle interactivity. The following subsections\ndetails the step by step of this procedure.\n3.1.1. Application Description\nProviding contextual information is crucial for tailoring the test cases to the specific ap-\nplication at hand and preventing hallucinations, as emphasized by [Lewis et al. 2020].\nCrafting this description should not become more challenging than generating the test\ncases themselves. To balance this trade-off and ensure accurate communication of rele-\nvant information, empirical experiments were conducted with various hypothetical soft-\nware. These experiments involved a qualitative analysis to identify potential gaps or miss-\ning information. It was observed that using one template per software feature resulted in\nsignificantly better outcomes than attempting to describe all features in a single template\nfile. Consequently, a template has been devised to guide the application description pro-\ncess:\nWhat is the software name?\n• A single line with the desired name to refer to the software\nWhat is the software’s main purpose?\n• A brief description with no more than 3 lines\nWhat is the platform type?\n• The platform of the software: Web, mobile, ...\nFeature description\n• An overview of each feature from the software\n3.1.2. Prompt Design\nIn the prompt elaboration, throughout the tests and references about prompt engineer-\ning, the input for the model was separated into three parts, with each one comple-\nmenting the other, reducing the complexity and granting better quality in the responses\n[Gu et al. 2023]. The prompts use the concept of Role Prompting [Kong et al. 2023],\nwhich provides the model context in the way it should interpret the input, thus, contribut-\ning into more elaborated responses from the model.\nTo build the software requirements the prompt is consisted of the role description\n(Ex.: QA Engineer), the feature description as stated in section 3.1.1 with all information\nfilled according to the under testing software, and the instructions for the expected output:\n• Project name\n• Project goal\n• List of the software’s non-functional requirements.\n• The list of the software’s functional requirements.\nThe first two attributes are used to confirm that the model has accurately absorbed\nthe given context and can succinctly summarize the software’s purpose. Additionally the\ninstruction also request to format the output as JSON, enabling its seamless integration as\ninput for subsequent prompts.\nThe next step is to produce the test conditions. This includes incorporating the\nrole, the software description, and the generated software requirements from the previous\nstep into the prompt. As part of the prompt’s instructions, the desired output is specified\nto be a list of test conditions for each functional requirement, formatted in JSON.\nIn the final step, test cases are built for each test condition. This process involves\nlooping over the list created in the previous step. Each prompt includes several key ele-\nments: the role, the software description, the requirement, and the test condition itself. For\nthe output, markdown format is specified. This choice facilitates the subsequent conver-\nsion of the test cases into a PDF document, significantly improving their visualization and\norganization. The test cases should incorporate several essential attributes: ID, Title,\nPre-conditions, Steps, Expected results, Test Data.\n3.2. Evaluation setup\nTo effectively evaluate test case generation, we selected Da.tes, an active web platform\nwith a broad range of features for connecting startups, investors, and businesses, as our\nstudy subject. The platform’s diverse functionalities facilitated a comprehensive analysis\nof test case generation effectiveness. Access to Da.tes’ existing manual test cases provided\na crucial comparison point for our research.\nOur evaluation centered on the quality of test case documentation, sidestepping\nmetrics that necessitate code access or historical data, focusing on qualities like Clar-\nity, Correctness, Completeness, Consistency, Understandability, Self-containedness, and\nSpecificity, as outlined in [Barraood et al. 2021a] and [Barraood et al. 2021b].\nThe study employed an unbiased approach, using anonymous, uniformly format-\nted sets of 10 AI-generated and 10 manual test cases, assessed by quality assurance engi-\nneers to gauge documentation quality effectively.\nTwo separate forms were created for the evaluation. The first form was designed\nto focus explicitly on the quality sub-factors of each test case under analysis. To mitigate\nvolunteer fatigue and potential confusion, only the three most critical factors identified by\n[Lai 2017] were used: Correctness, Completeness, and Consistency. V olunteers were\nasked to rate each factor for each test case on a scale of 1-5, in accordance with the criteria\noutlined in Table1.\nScore 1 2 3 4 5\nMeaning Very Poor Poor Regular Good Very Good\nTable 1. Scores definition that was used by volunteers to rate the test cases\nThe second form focused on the implicit quality of the test cases, employing an\nA/B testing format to gauge preferences. In this setup, two similar test cases—each testing\nthe same path in the software—were presented side by side. V olunteers were given 10\nquestions, asking them to choose between test A or test B, without knowing which was\nAI-generated and which was manually crafted. Additionally, a justification field was\nprovided, where they could explain their choice. This feedback, inherently subjective and\nnot based on predefined quality factors like the first form, aims to uncover any discernible\ndifferences between the two groups of test cases that may warrant further investigation\n4. Results and Discussions\nA total of 7 participants completed both forms. While the analysis is based on this limited\nsample, the consistency in responses and the insightful explanations submitted by partic-\nipants offer relevant insights. We start our analysis with the quantitative scores for each\nmetric. To provide an overview, the average scores for each group were calculated. Fig-\nure 2 shows the comparison between AI and Manual test cases. The total average scores\nacross all three metrics are similar - 4.31 for AI versus 4.18 for Manual - with a slight\nadvantage for the AI-generated process.\nThe correctness metric revealed the most significant discrepancy between the AI\nand Manual test groups in our evaluation. A closer examination of individual results\nindicated that AI-generated tests never received the lowest score. However, the manually\nwritten tests did receive the lowest score in a few cases, thereby lowering their average.\nDespite this, the overall assessment suggests that the AI is capable of generating test cases\nwith a level of correctness comparable to human performance.\nFigure 2. Average rate for AI and Manual test cases group split by quality factors\nThe same observation applies to the consistency of the test cases, where some\ninstances of very low scores explain the observed gap. Consistency, defined as the degree\nof uniformity and reliability in test outcomes, reflects the ability of a set of test cases to\nproduce reliable and expected results under varying conditions. In our analysis, it was\nevident that the test cases created by the AI maintained a consistent level of logic, even\nthough they were generated in separate interactions. This consistency is likely attributable\nto the specific prompt that guided the output format, ensuring a uniform approach across\ndifferent test cases.\nIn the analysis of completeness, we observed that the averages were quite similar\nbetween the two groups. However, there was a notable difference in their distributions.\nThe manually created tests received more top scores (35 rated as ’5’) compared to the AI-\ngenerated tests (which received 32 top scores), but they also had a greater number of the\nlowest scores. Completeness, which refers to the extent to which a set of test cases covers\nall possible scenarios and functionalities of the software, can vary significantly. This\nvariation in scores can be attributed to the test engineer’s personal judgment, particularly\nin deciding the level of detail needed based on the perceived importance or rarity of a\nfeature.\nThe second questionnaire provides more insight into the perceptions and prefer-\nences regarding the test cases, deliberately avoiding inference of any specific metrics, as\nit was presented in an A/B testing format. Given the total of the responses for all tests (70\n- 7 evaluators and 10 test cases), the overall preference leaned towards the AI-generated\ncases (58.6%). These results are illustrated in Figure 3.\nIn analyzing the results on a case-by-case basis, we defined a ’clear win’ as a\nscenario where the preference difference exceeds one vote (e.g., a 4 versus 3 vote count\nis considered a tie). Applying this criterion, we found that only two AI-generated tests\nwere preferred over their manual counterparts, and similarly, two manual tests were fa-\nvored over the AI ones. The remaining six test cases resulted in what can be termed as a\ntechnical tie.\nFigure 3. Pick rate for AI and Manual test cases in the A/B test\nIn delving into the reasons provided when Manual test cases were preferred, com-\nmon themes emerged around correctness and completeness, with ’clarity’ being a fre-\nquently cited new factor. Conversely, when AI-generated test cases were favored, the\njustifications predominantly revolved around the level of detail and completeness. The\nrationale behind the ties varied, ranging from perceptions of equivalency between the\ncases to differing opinions on correctness. For example, in the evaluation of the same test\ncase, some evaluators identified a mistake, while others considered it more correct and\ncomplete.\nOur analysis reveals a complex picture of AI-generated versus manually created\ntest cases, highlighting the trade-offs and potential synergies between the two. Prefer-\nences vary, with evaluators considering factors like clarity, correctness, detail, and com-\npleteness, challenging the conventional views on automated versus manual testing. These\ninsights, demonstrating the nuanced aspects of test case quality, pave the way for a deeper\nunderstanding of AI’s role in quality assurance as we delve into the concluding discus-\nsions of this paper.\n5. Conclusion\nThis paper explored the use of Large Language Models (LLMs) in generating test cases\nfor software engineering, focusing on a case study with the software application Da.tes.\nWe found that LLMs can create test cases that rival those made by humans in quality,\nshowcasing their potential as tools in test case development, though human input remains\ncrucial for ensuring accuracy and context-appropriateness. While LLMs enhance effi-\nciency and detail in test creation, their effectiveness varies, indicating the need for a com-\nbined approach with human expertise. Future research should investigate the broader im-\nplications, including cost and time benefits, of integrating LLMs in test case generation.\nDespite some limitations in handling complex inter-feature tests, enhancing the LLM’s\nunderstanding of feature interdependencies could significantly improve its applicability\nin more intricate testing scenarios.\nReferences\nBarraood, S., Mohd, H., and Baharom, F. (2021a). Test case quality factors. Turkish\nJournal of Computer and Mathematics Education (TURCOMAT), 12:1683–1694.\nBarraood, S., Mohd, H., and Baharom, F. (2021b). Test case quality factors: Content\nanalysis of software testing websites. Webology, 18:75 – 87.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan,\nA., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot\nlearners. Advances in neural information processing systems, 33:1877–1901.\nFeng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T.,\nJiang, D., et al. (2020). Codebert: A pre-trained model for programming and natural\nlanguages. In Findings of the Association for Computational Linguistics: EMNLP\n2020, pages 1536–1547.\nGu, J., Han, Z., Chen, S., Beirami, A., He, B., Zhang, G., Liao, R., Qin, Y ., Tresp, V .,\nand Torr, P. (2023). A systematic survey of prompt engineering on vision-language\nfoundation models. arXiv preprint arXiv:2307.12980.\nKim, S., Zhao, J., Tian, Y ., and Chandra, S. (2021). Code prediction by feeding trees to\ntransformers. In 2021 IEEE/ACM 43rd International Conference on Software Engi-\nneering (ICSE), pages 150–162. IEEE.\nKong, A., Zhao, S., Chen, H., Li, Q., Qin, Y ., Sun, R., and Zhou, X. (2023). Better\nzero-shot reasoning with role-play prompting. arXiv preprint arXiv:2308.07702.\nLai, S.-T. (2017). Test case quality management procedure for enhancing the efficiency\nof iid continuous testing. Journal of Software, 12(10):794–806.\nLangChain (2023). Langchain: Building applications with llms through composability.\nhttps://github.com/langchain-ai/langchain.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., K¨uttler, H., Lewis,\nM., Yih, W.-t., Rockt ¨aschel, T., et al. (2020). Retrieval-augmented generation for\nknowledge-intensive nlp tasks. Advances in Neural Information Processing Systems ,\n33:9459–9474.\nOpenAI (2023). Openai: Advancing digital intelligence. https://www.openai.\ncom. Accessed on November 21, 2023.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and Sundaresan, N. (2021).\nUnit test case generation with transformers and focal context. arXiv preprint\narXiv:2009.05617.\nWang, J., Huang, Y ., Chen, C., Liu, Z., Wang, S., and Wang, Q. (2024). Software testing\nwith large language models: Survey, landscape, and vision. IEEE Transactions on\nSoftware Engineering.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V ., Zhou, D., et al.\n(2022). Chain-of-thought prompting elicits reasoning in large language models."
}