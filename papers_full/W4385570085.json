{
  "title": "Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models",
  "url": "https://openalex.org/W4385570085",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2892721462",
      "name": "Ehsan Doostmohammadi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2511463329",
      "name": "Tobias Norlund",
      "affiliations": [
        "Recorded Future (Sweden)",
        "Chalmers University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2164020070",
      "name": "Marco Kuhlmann",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097970814",
      "name": "Richard Johansson",
      "affiliations": [
        "Chalmers University of Technology",
        "University of Gothenburg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4299567010",
    "https://openalex.org/W3180230246",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W4287649493",
    "https://openalex.org/W4226069413",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W2990928880",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4315588390",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2963018920",
    "https://openalex.org/W4388778348",
    "https://openalex.org/W4225909425",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4386566476",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Augmenting language models with a retrieval mechanism has been shown to significantly improve their performance while keeping the number of parameters low. Retrieval-augmented models commonly rely on a semantic retrieval mechanism based on the similarity between dense representations of the query chunk and potential neighbors. In this paper, we study the state-of-the-art RETRO model and observe that its performance gain is better explained by surface-level similarities, such as token overlap. Inspired by this, we replace the semantic retrieval in RETRO with a surface-level method based on BM25, obtaining a significant reduction in perplexity. As full BM25 retrieval can be computationally costly for large datasets, we also apply it in a re-ranking scenario, gaining part of the perplexity reduction with minimal computational overhead.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 521–529\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nSurface-Based Retrieval Reduces Perplexity of\nRetrieval-Augmented Language Models\nEhsan Doostmohammadi1∗ Tobias Norlund2,4 Marco Kuhlmann1 Richard Johansson2,3\n1 Linköping University 2 Chalmers University of Technology\n3 University of Gothenburg 4 Recorded Future\nAbstract\nAugmenting language models with a retrieval\nmechanism has been shown to significantly im-\nprove their performance while keeping the num-\nber of parameters low. Retrieval-augmented\nmodels commonly rely on a semantic retrieval\nmechanism based on the similarity between\ndense representations of the query chunk and\npotential neighbors. In this paper, we study\nthe state-of-the-art RETRO model and observe\nthat its performance gain is better explained by\nsurface-level similarities, such as token over-\nlap. Inspired by this, we replace the semantic\nretrieval in RETRO with a surface-level method\nbased on BM25, obtaining a significant reduc-\ntion in perplexity. As full BM25 retrieval can be\ncomputationally costly for large datasets, we\nalso apply it in a re-ranking scenario, gaining\npart of the perplexity reduction with minimal\ncomputational overhead.\n1 Introduction\nThe introduction of the Transformer architecture\n(Vaswani et al., 2017) has led to a performance\nboost in language modeling (see, e.g., Brown et al.\n2020), but also to a steep increase of computational\ncost, as the number of parameters and data points\nis constantly growing. In reaction to this develop-\nment, there has recently been a surge in work on\nretrieval-augmented language models (Izacard and\nGrave, 2021a; Li et al., 2022), which shows that\nenabling models to retrieve context from large cor-\npora results in lower perplexity and better accuracy\nin downstream tasks such as question answering,\nwhile at the same time using considerably fewer\nparameters. In this paper, we specifically focus on\nthe Retrieval-Enhanced Transformer architecture\n(RETRO ; Borgeaud et al., 2022).\nBy augmenting a language model with a re-\ntrieval mechanism, RETRO , like similar architec-\ntures, tries to decouple memorization of the train-\ning data from the additional generalization that\n∗Correspondence to ehsan.doostmohammadi@liu.se.\ncomes with increasing the number of parameters.\nIn RETRO , when a chunk of text (a sequence of\ntokens) has been generated, a dense representation\nof this chunk is used to retrieve the most similar\nneighboring chunks from a large retrieval set, based\non their L2 distance. Having the previously gen-\nerated chunks and their nearest neighbors in the\nretrieval set, the auto-regressive language model\nhas now access to an extended context when pre-\ndicting the next chunk. The informativeness of this\ncontext depends on the effectiveness of the retrieval\nmethod.\nBorgeaud et al. (2022) note that part of RETRO ’s\nperformance can be attributed to the token over-\nlap between the generated chunks and the retrieval\nset. Our starting point in this paper is the obser-\nvation that the performance gain is actually better\nexplained by such surface-level similarities than by\nthe L2 distance between the dense representations\nthat RETRO uses for retrieval. This is in line with\nrecent work by Norlund et al. (2023), who show\nthat the reduction in loss observed in RETRO “al-\nmost exclusively” stems from such overlap rather\nthan more sophisticated generalization. Based on\nthese findings, we replace the semantic retrieval\nmethod in RETRO with one based on BM25 (Robert-\nson et al., 1995), a surface-level measure. Our\nresults show that retrieving nearest neighbors using\nBM25 during inference leads to a 13.6% lower per-\nplexity, compared to dense retrieval based on sen-\ntence transformers (ST) (Reimers and Gurevych,\n2019), a model trained to represent the semantic\nsimilarity between sentences.1\nFinding the exact neighbors with BM25 is costly\non large retrieval sets and might not meet the\nspeed requirements of all applications of retrieval-\naugmented language models. We therefore explore\na hybrid approach where we first retrieve approxi-\nmate neighbors using ST representations and then\n1The code and the data for this study can be accessed at\ngithub.com/edoost/retro_bm25.\n521\nre-rank them using BM25. We show that this ap-\nproach yields 24.7% of the perplexity reduction we\nget with BM25-based retrieval, with only minimal\ncomputational overhead.\n2 Method\nWe experiment withRETRO (Borgeaud et al., 2022)\nas a state-of-the-art retrieval-augmented language\nmodel.\n2.1 Model\nRETRO is very similar to a standard auto-regressive\nlanguage model such as T5 (Raffel et al., 2020), the\nmain differences being the introduction of the re-\ntrieval mechanism and how the retrieved neighbors\nare used for language modeling.\nNearest Neighbor Retrieval In RETRO , all tex-\ntual data is stored and used in chunks of 64 to-\nkens. When the model has generated a chunk Cu,\nit retrieves the k nearest neighbors N1:k to that\nchunk, together with the chunks F1:k following\nthese neighbor chunks in the retrieval data. It then\ngenerates the next chunk Cu+1 conditioned on the\nretrieved chunk pairs. Retrieval uses the squared L2\ndistance on a dense representation (DR) of chunks:\nd(Cu,Ni) = ∥DR(Cu) −DR(Ni)∥2\n2\nThis leaves us with\nRET(Cu) = ([N1\nu; F1\nu ],..., [Nk\nu ; Fk\nu ])\nas the retrieved neighbors that the model receives as\nadditional context when generating the next chunk.\nThe likelihood of the first chunk (C1) does not de-\npend on any neighbors; the model has access to no\nexternal context when generating that chunk. Dur-\ning training and perplexity evaluation, the retrieval\nprocess is filtered such that chunks originating from\nthe same source document as the training sequence\nare never considered as neighbors.\nIntegration of the NeighborsRETRO improves\nauto-regressive language modeling by conditioning\nthe next token prediction on the retrieved chunks\nof text. This means that the probability of gen-\nerating the next token xt+1 depends not only on\nthe previously generated tokens x1:t but also on\nthe retrieved neighbors of the previously generated\nchunks, as well as their following chunks:\nP(xt+1 |x1:t,RET(C1),..., RET(Cu−1); θ)\nWhen generating the next token, the neighbors\nas well as the current chunk Cu are passed through\na Transformer encoder. In the decoder, cross-\nattention is over the output of that encoder and\nthe concatenation of the intermediary embeddings\nof the last few tokens in the previous chunk Cu−1\nand the already generated tokens in Cu, a mech-\nanism called chunked cross-attention. For more\ndetails, see Borgeaud et al. (2022).\nImplementation Details As an official imple-\nmentation of RETRO is not publicly available,\nwe draw upon the implementation in Norlund\net al. (2023), which is based on the description\nin Borgeaud et al. (2022). Our implementation\ndeviates only in that (1) we use learnable relative\npositional biases as in T5 (Raffel et al., 2020), with\na bucket for each unique relative position; (2) in-\nstead of BERT (Devlin et al., 2019), we use the pre-\ntrained sentence transformers (ST) (Reimers and\nGurevych, 2019) model to embed the chunks for\nthe offline retrieval. ST is preferable over BERT,\nas it is trained for the task of similarity search,\nand produces embeddings of lower dimensionality,\nwhich makes it more efficient. We use PyTorch\n(Paszke et al., 2019) and PyTorch Lightning for\ndistributed training. For the tokenization, we use\nthe pre-trained T5 tokenizer (HuggingFace). For\nretrieving approximate neighbors, we use faiss\n(Johnson et al., 2019), which performs efficient sim-\nilarity search between dense representations with\nGPU support for faster indexing and retrieval.\n2.2 Data\nBorgeaud et al. (2022) use the MassiveText dataset\n(Rae et al., 2021) for both training and retrieval.\nAs this dataset is not publicly available, we set\nout to replicate it using open sources. MassiveText\nconsists of multilingual text data in five categories:\nWikipedia articles, books, GitHub code, news, and\ncommon crawl web data. We use Pile (Gao et al.,\n2021) and RealNews (Zellers et al., 2019) to build\na large dataset resembling MassiveText’s composi-\ntion. The new dataset (see Norlund et al. (2023)\nfor details) consists of 36M documents containing\n52B tokens. For Pile, we keep the training and val-\nidation splits, while for RealNews, we use the full\ntraining set but downsample the validation set to\n16,400 news articles to match the proportions of the\ncategories in Pile. For details on the deduplication\nprocess, we refer to Gao et al. (2021) and Zellers\net al. (2019).\n522\n2.3 Training\nWe use our dataset to train a RETRO model with\napproximately 630M parameters. For more details\nrefer to Norlund et al. (2023). During training, we\nretrieve from the training set; during validation, we\nretrieve from the union of the training and valida-\ntion sets. We train the model on sequences trun-\ncated to 1,024 tokens. The chunk size is 64, as in\nBorgeaud et al. (2022), and the number of retrieved\nneighbors is k= 2 for training and validation. We\ntrain the model for 140k training steps with a batch\nsize of 16, taking seven days on 16 A100 GPUs.\nThis means that we use 6% of the training data dur-\ning training, not including the retrieved neighbors.\nAs our optimizer, we use Adam (Kingma and Ba,\n2015) with a fixed learning rate of 1e−4.\n3 A Study on Correlations\nWe experiment with two settings: RETRO [ON],\nthe language model with retrieval enabled, and\nRETRO [OFF], where there are no chunk cross-\nattention layers and therefore no retrieval, leaving\nus with a decoder-only language model. As shown\nby Borgeaud et al. (2022), the RETRO [ON] model\nperforms better when it can exploit an overlap be-\ntween the generated text and the retrieved neighbor.\nThis is more apparent in text categories with higher\ntoken overlap, such as GitHub. The studies in the\nRETRO paper also show that allowing more over-\nlap when deduplicating the data results in a lower\nbits-per-byte (BPB2). Norlund et al. (2023) take\nthis further to show even minimal overlap results in\nsignificant loss reduction, demonstrating the large\nextent RETRO relies on surface-level similarities.\nThese findings lead us to hypothesize that having a\nretrieval method that can find the highest overlap-\nping neighbors will yield lower perplexity ( PPL).\nBecause BERT, ST and similar deep representa-\ntions of sentences do not always capture surface-\nlevel similarities, we set out to investigate where\nperformance gains come from.\nTo this end, we measure how the PPL difference\n(∆PPL) between RETRO [ON] and RETRO [OFF] for\nthe current chunk (Cu, u≥2) correlates with (1)\nsquared L2 distance between the ST embeddings\nof Cu and RET(Cu−1) (ST), and (2) unigram token\noverlap, based on T5 tokenization, between Cu\n2BPB = ( LT /LB)L/ln(2), where LT and LB are the\nlengths of the validation set in T5 tokens and UTF-8 en-\ncoded bytes, respectively, and L stands for log likelihood\nloss. LT /LB is 0.258415 for our validation set.\nX Y ρ r\nL22 (ST) ∆PPL 0.328 0.134\ntoken overlap ∆PPL 0.494 0.415\nL22 (ST) token overlap 0.464 0.515\nTable 1: Spearmanρand Pearson rbetween variables X\nand Y. L22 (ST) is the (negative) squared L2 distance\nbetween the ST embeddings of RET(Cu−1) and Cu; to-\nken overlap is the unigram token overlap between these\ntwo chunks; and ∆PPL = PPLRETRO [OFF] −PPLRETRO [ON]\nfor the chunk Cu.\nand RET(Cu−1). The results, reported in Table 1,\nshow a considerably stronger correlation between\n∆PPL and unigram token overlap (measure 2) than\nbetween ∆PPL and L2 distance (measure 1). The\ntrend is similar between Spearman and Pearson\ncorrelation coefficients.\n4 Changing the Retrieval Method\nAs the results from the previous section show a\nstronger correlation between performance gain and\nsurface-level similarity than ST similarity, we ex-\nperiment with a retrieval method based on BM25.\n4.1 BM25\nOkapi BM25, introduced by Robertson et al. (1995),\nis a bag-of-words retrieval method based on tf–idf\nscores and some free parameters. These parame-\nters are k1, which normalizes the term frequency,\nand b, which controls how much the length of a\ndocument would affect the term frequency values.\nWe use Pyserini (Lin et al., 2021), a Python inter-\nface to Lucene’s BM25 implementation. We build\nthe BM25 index on the training set and leave the\nfree parameters at their default values ( k1 = 0.9,\nb = 0 .4). These values were also shown to per-\nform the best by Karpukhin et al. (2020a). Using\nLucene’s Analyzer pipeline3 results in more than\n50M unique words for our corpus. We instead use\nthe T5 tokenizer from Hugging Face Transformers\n(Wolf et al., 2020) and limit our vocabulary to 32k\nwords for the reranking experiments.\n4.2 Retrieving with BM25\nWe use the model described in Section 2.3 and\nchange the retrieval method only at inference time\nto retrieve better neighbors. The results can be\nfound in Table 2. The perplexity is 14.00 for\n3Lucene Analyzers (Lucene) are used to extract index\nterms from text, which includes tokenization and preprocess-\ning.\n523\nModel P PL BPB\nRETRO [OFF] 14.00 0.984\nRETRO [ON]-ST 10.87 0.889\nRETRO [ON]-ST + BM25 reranking 10.46 0.875\nRETRO [ON]-BM25 8.95 0.817\nTable 2: PPL and BPB for various retrieval settings on\nthe validation set. The basic RETRO model is the same\nfor all rows.\nRETRO [OFF] and 10.87 forRETRO [ON] with ST re-\ntrieval (RETRO [ON]-ST), corresponding to a 22.3%\nreduction in PPL. Replacing the retrieval method\nwith BM25 (RETRO [ON]-BM25) gives an additional\n13.7% reduction, which is 61.3% of the initial drop.\nFor comparability with Borgeaud et al. (2022), we\nalso report BPB. The results show that using neigh-\nbors with more surface-level similarity to the gen-\nerated chunk is a solid method for leveraging the\nretrieval mechanism to reduce the perplexity. If the\nretrieval augmentation is meant to act as an exter-\nnal memory, or to offload memorization from the\nmodel (Borgeaud et al., 2022), then BM25 is a more\nsuitable method to achieve this goal.\n4.3 Reranking\nWhile the performance gain is significant, finding\nthe exact neighbors using BM25 could be costly, de-\npending on the size of the datasets. On the other\nhand, faiss provides an efficient similarity search\nfor dense vectors to find theapproximate neighbors.\nTherefore, if enough of the BM25-retrieved neigh-\nbors could be found among top-kfaiss-retrieved\nones, with an efficient reranking, we could expect\nat least part of the performance gain with minimal\ncomputational overhead, as long as kis not signifi-\ncantly large. To find an optimal k, we first need to\nknow how many of BM25 neighbors could be found\nin top-kfaiss-retrieved chunks.\nLooking at the faiss-retrieved neighbors, we\nsee that of top-4 BM25-retrieved neighbors, 17.6%\nappear in top-100 faiss-retrieved chunks, while\nthe overlap is 22.1% for top-1000. We decide to\ncontinue our experiment with top-1000 neighbors,\nbut it is obvious that one could get an even higher\noverlap with a higher k, with diminishing returns.\nThe results in Table 2 show that with the proposed\nreranking, RETRO [ON]-ST could achieve21.3% of\nthe PPL reduction of RETRO [ON]-BM25 compared\nto RETRO [ON]-ST. The reranking results are inter-\nesting not only due to their practical implications\nbut also as an analysis revealing the limited num-\nber of high-quality neighbors that can be retrieved\nusing semantic retrieval, even in situations where a\nlarge kis feasible.\n5 Related Work\nAugmenting language models with mechanisms\nthat help them incorporate larger contexts has been\napproached extensively in different forms, such as\nGuu et al. (2018)’s retrieve-and-edit approach to\nreduce the PPL in language generation, and Asai\net al. (2020) that make use of lexical overlap to\nimprove the performance in question answering.\nWhile retrieval-augmentation has been used with\ndifferent objectives in mind, such as language mod-\neling (Khandelwal et al., 2020; Wu et al., 2022)\nand machine translation (Khandelwal et al., 2021),\nquestion answering has been the application to at-\ntract the most interest (Guu et al., 2020; Karpukhin\net al., 2020b; Izacard and Grave, 2021b).\nAn extensive study was performed by Izacard\net al. (2022), showing that while we get perfor-\nmance gains using retrieval augmentation, training\nthe retrieval part of the model would yield even\nmore benefits. RETRO (Borgeaud et al., 2022),\non the other hand, aims at scaling such language\nmodels and therefore opts for keeping the retriever\nfrozen, showing substantial PPL reduction with\nincreasing either the number of language model\nparameters or the size of retrieval set.\nAmong the more recent work, Xu et al. (2023)\nfound that training using approximate neighbors\nresulted in a 2.6% decrease in perplexity. This\nsuggests that non-exact neighbors may have a reg-\nularization effect, leading to improved generaliza-\ntion ability. Additionally, Ram et al. (2023) re-\nport a drop in perplexity using BM25 over BERT\nretrieval using in-context retrieval-augmented lan-\nguage models.\n6 Conclusions and Future Work\nIn this paper, we study the source of performance\ngains in RETRO , which could be generalized to\nsimilar retrieval-augmented language models. Af-\nter observing that the PPL drop correlates more\nstrongly with surface-level overlap between the\nquery and the retrieved text, we replace the retrieval\nmethod with BM25, and observe a significant drop\nin PPL, which confirms us in the findings of the\ncorrelation study. This is also an interesting insight\nas to how these models work, which could be lever-\n524\naged for performance gain in tasks like question\nanswering where model relies on retrieving facts.\nIn the end, we also conduct an analysis to find\nout how much BM25 neighbors overlap with those\nretrieved using ST. The results show that while\nfaiss is able to find some of the neighbors with\nhigh token overlap, the majority of them remain\nunretrieved. This is however, enough to gain part\nof the loss reduction achieved with a pure BM25\nretrieval system.\nThe proposed methods could also be used during\ntraining. By retrieving more overlapping neighbors\nduring training, the process of guiding the model\nto use retrieved neighbors for language modeling\ncould be done more efficiently. This is particu-\nlarly relevant when augmenting an already trained\nlanguage model with a retrieval mechanism. As re-\nported by Borgeaud et al. (2022), retrieval augmen-\ntation results in a larger drop in BPB as the number\nof model parameters and the size of retrieval data\ngrow. This calls for more efficient methods based\non surface-level similarities if we wish to exploit\nthis potential. Furthermore, the retrieval system in\nRETRO is based on semantic retrieval, the model\nseems to rely more on surface-level similarities.\nThis could affect the generalizability capabilities\nof such models, which necessitates further inves-\ntigations. Lastly, we only evaluate our modified\nRETRO model on language modeling. It would be\ninteresting to know the impacts of BM25 retrieval\non downstream tasks where retrieval is of use.\nLimitations\nWe only experiment with one type of retrieval-\naugmented language models, i.e., RETRO . How-\never, the ways the other models retrieve neighbors\nand integrate them are not so much different to\naffect the results in this paper. The experiments\nin this paper are done with a small size RETRO\nmodel and data compared to the sizes considered\nby Borgeaud et al. (2022), due to computational\nlimitations. According to the same authors, how-\never, the gains should be constant with the increase\nof the model and retrieval set size. The larger mod-\nels are mainly different in their behavior when there\nis no overlap. However, this should not affect the\ncopying tendency of these models tremendously,\nas it is still the easiest way to generate the next\ntoken. It is also worth noting that RETRO [OFF],\nwhile not using retrieval at test time, is still trained\nusing retrieval – so it is not a complete retrieval-\nfree model. The results presented by Borgeaud\net al. (2022) however, show that RETRO [OFF] is\non a par with their retrieval-free baseline in terms\nof BPB. Finally, we note that our evaluations have\nonly considered the perplexity under teacher forc-\ning, and we have not investigated the behavior of\nthe model in free-form generation or with any kind\nof fine-tuning.\nAcknowledgements\nThis work was partially supported by the Wal-\nlenberg AI, Autonomous Systems and Software\nProgram (WASP) funded by the Knut and Alice\nWallenberg Foundation. The computations were\nenabled by resources provided by the National\nAcademic Infrastructure for Supercomputing in\nSweden (NAISS) at Alvis partially funded by the\nSwedish Research Council through grant agree-\nment no. 2022-06725, and by the Berzelius re-\nsources provided by the Knut and Alice Wallenberg\nFoundation at the National Supercomputer Center.\nReferences\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learning\nto retrieve reasoning paths over wikipedia graph for\nquestion answering. In International Conference on\nLearning Representations.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nProceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings\nof Machine Learning Research, pages 2206–2240.\nPMLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\n525\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021. The pile: An\n800gb dataset of diverse text for language modeling.\nCoRR, abs/2101.00027.\nKelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren,\nand Percy Liang. 2018. Generating sentences by\nediting prototypes. Transactions of the Association\nfor Computational Linguistics, 6:437–450.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. In Inter-\nnational Conference on Machine Learning, pages\n3929–3938. PMLR.\nHuggingFace. Huggingface T5.\nGautier Izacard and Edouard Grave. 2021a. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nGautier Izacard and Edouard Grave. 2021b. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020a. Dense passage retrieval for\nopen-domain question answering. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 6769–\n6781, Online. Association for Computational Lin-\nguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020b. Dense passage retrieval for\nopen-domain question answering. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 6769–\n6781, Online. Association for Computational Lin-\nguistics.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2021. Nearest neigh-\nbor machine translation. In International Conference\non Learning Representations.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations (ICLR).\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR (Poster).\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and\nLemao Liu. 2022. A survey on retrieval-augmented\ntext generation. arXiv preprint 2202.01110.\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\n2021. Pyserini: A python toolkit for reproducible\ninformation retrieval research with sparse and dense\nrepresentations. In Proceedings of the 44th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, SIGIR ’21,\npage 2356–2362, New York, NY , USA. Association\nfor Computing Machinery.\nApache Lucene. Lucene analyzer.\nTobias Norlund, Ehsan Doostmohammadi, Richard Jo-\nhansson, and Marco Kuhlmann. 2023. On the gener-\nalization ability of retrieval-enhanced transformers.\nIn Findings of the Association for Computational Lin-\nguistics: EACL 2023, pages 1485–1493, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n32, pages 8024–8035. Curran Associates, Inc.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, H. Francis Song, John\n526\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Antonia\nCreswell, Nat McAleese, Amy Wu, Erich Elsen, Sid-\ndhant M. Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy, Chris\nJones, James Bradbury, Matthew J. Johnson, Blake A.\nHechtman, Laura Weidinger, Iason Gabriel, William\nIsaac, Edward Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher. CoRR, abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. arXiv preprint arXiv:2302.00083.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics.\nStephen Robertson, S. Walker, S. Jones, M. M. Hancock-\nBeaulieu, and M. Gatford. 1995. Okapi at TREC-3.\nIn Overview of the Third Text REtrieval Conference\n(TREC-3), pages 109–126. Gaithersburg, MD: NIST.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,\nand Christian Szegedy. 2022. Memorizing transform-\ners. In International Conference on Learning Repre-\nsentations.\nFrank F Xu, Uri Alon, and Graham Neubig. 2023. Why\ndo nearest neighbor language models work? arXiv\npreprint arXiv:2301.02828.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc.\n527\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nIt’s the last section and it is unnumbered.\n□\u0013 A2. Did you discuss any potential risks of your work?\nWe mention that relying on surface-level similarities could affect the generalizability capabilities of\nsuch models, which necessitates further investigations.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nThey could be found in the abstract, section 1 (introduction), and even the other sections.\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nWe used Grammarly to a limited extent.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nIt’s all over the paper, but mainly section 2.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nIt’s all over the paper, but mainly section 2.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nOne should consult to the main papers for that.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. It is not taken care of by us, but by the authors of those datasets.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □\u0013 Did you run computational experiments?\nSections 2, 3, and 4.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 2.3.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n528\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 2.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSome of them are not applicable, but the rest are discussed in Section 2.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nYes. We will even publish our code later for absolute transparency.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n529",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9674384593963623
    },
    {
      "name": "Computer science",
      "score": 0.8456878662109375
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.5979382991790771
    },
    {
      "name": "Language model",
      "score": 0.5634377598762512
    },
    {
      "name": "Security token",
      "score": 0.5330053567886353
    },
    {
      "name": "Information retrieval",
      "score": 0.5123599767684937
    },
    {
      "name": "Artificial intelligence",
      "score": 0.507513165473938
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.45436033606529236
    },
    {
      "name": "Overhead (engineering)",
      "score": 0.4214755594730377
    },
    {
      "name": "Reduction (mathematics)",
      "score": 0.4204927086830139
    },
    {
      "name": "Natural language processing",
      "score": 0.3813876211643219
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I102134673",
      "name": "Linköping University",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I66862912",
      "name": "Chalmers University of Technology",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I4210128114",
      "name": "Recorded Future (Sweden)",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I881427289",
      "name": "University of Gothenburg",
      "country": "SE"
    }
  ],
  "cited_by": 2
}