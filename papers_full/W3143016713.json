{
  "title": "AAformer: Auto-Aligned Transformer for Person Re-Identification",
  "url": "https://openalex.org/W3143016713",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2345300904",
      "name": "Kuan Zhu",
      "affiliations": [
        "Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2160255992",
      "name": "Haiyun Guo",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2099219781",
      "name": "Shiliang Zhang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2109965745",
      "name": "Yaowei Wang",
      "affiliations": [
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1479773632",
      "name": "Jing Liu",
      "affiliations": [
        "Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2097542353",
      "name": "Jin-qiao Wang",
      "affiliations": [
        "Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2108125403",
      "name": "Ming Tang",
      "affiliations": [
        "Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963842104",
    "https://openalex.org/W2963805953",
    "https://openalex.org/W2795758732",
    "https://openalex.org/W3115484111",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4214736485",
    "https://openalex.org/W6682962330",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W1982925187",
    "https://openalex.org/W2585635281",
    "https://openalex.org/W2963047834",
    "https://openalex.org/W2962766801",
    "https://openalex.org/W2964289004",
    "https://openalex.org/W2963690547",
    "https://openalex.org/W2471048925",
    "https://openalex.org/W2342611082",
    "https://openalex.org/W2586899202",
    "https://openalex.org/W2068042582",
    "https://openalex.org/W1949591461",
    "https://openalex.org/W166429404",
    "https://openalex.org/W2300840837",
    "https://openalex.org/W2089074647",
    "https://openalex.org/W2964163358",
    "https://openalex.org/W2724213014",
    "https://openalex.org/W2964304299",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W3037140251",
    "https://openalex.org/W2998261716",
    "https://openalex.org/W2963078173",
    "https://openalex.org/W2981420411",
    "https://openalex.org/W2962926870",
    "https://openalex.org/W2964044605",
    "https://openalex.org/W2896888563",
    "https://openalex.org/W2954765307",
    "https://openalex.org/W2964140013",
    "https://openalex.org/W4312361652",
    "https://openalex.org/W3175823695",
    "https://openalex.org/W3205959870",
    "https://openalex.org/W4312980231",
    "https://openalex.org/W4312651322",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W6770196601",
    "https://openalex.org/W6779997284",
    "https://openalex.org/W2967515867",
    "https://openalex.org/W2598634450",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2798794112",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6746255304",
    "https://openalex.org/W2962691289",
    "https://openalex.org/W2979938149",
    "https://openalex.org/W2988964414",
    "https://openalex.org/W2981393440",
    "https://openalex.org/W3097870364",
    "https://openalex.org/W3133783521",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2980073905",
    "https://openalex.org/W3000133216",
    "https://openalex.org/W2963049565",
    "https://openalex.org/W2984145721",
    "https://openalex.org/W2989753085",
    "https://openalex.org/W3109096434",
    "https://openalex.org/W3202893281",
    "https://openalex.org/W3123416309",
    "https://openalex.org/W3128498158",
    "https://openalex.org/W3164505485",
    "https://openalex.org/W3048070442",
    "https://openalex.org/W2584637367",
    "https://openalex.org/W6754663736",
    "https://openalex.org/W2963330186",
    "https://openalex.org/W6755740987",
    "https://openalex.org/W3100506510",
    "https://openalex.org/W3105046598",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2995543391",
    "https://openalex.org/W2798429327",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4288024349",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W3107634219",
    "https://openalex.org/W3119588134",
    "https://openalex.org/W4287330514",
    "https://openalex.org/W2890159224",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4304892542",
    "https://openalex.org/W2747685395",
    "https://openalex.org/W2158131535",
    "https://openalex.org/W2769346661",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W3204251186",
    "https://openalex.org/W2963637710",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035539956",
    "https://openalex.org/W2897450186",
    "https://openalex.org/W2986405467",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963876278",
    "https://openalex.org/W2796364723",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W603908379"
  ],
  "abstract": "In person re-identification (re-ID), extracting part-level features from person images has been verified to be crucial to offer fine-grained information. Most of the existing CNN-based methods only locate the human parts coarsely, or rely on pretrained human parsing models and fail in locating the identifiable nonhuman parts (e.g., knapsack). In this article, we introduce an alignment scheme in transformer architecture for the first time and propose the auto-aligned transformer (AAformer) to automatically locate both the human parts and nonhuman ones at patch level. We introduce the \"Part tokens ([PART]s),\" which are learnable vectors, to extract part features in the transformer. A [PART] only interacts with a local subset of patches in self-attention and learns to be the part representation. To adaptively group the image patches into different subsets, we design the auto-alignment. Auto-alignment employs a fast variant of optimal transport (OT) algorithm to online cluster the patch embeddings into several groups with the [PART]s as their prototypes. AAformer integrates the part alignment into the self-attention and the output [PART]s can be directly used as part features for retrieval. Extensive experiments validate the effectiveness of [PART]s and the superiority of AAformer over various state-of-the-art methods.",
  "full_text": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1\nAAformer: Auto-Aligned Transformer for Person\nRe-Identification\nKuan Zhu, Haiyun Guo, Member, IEEE, Shiliang Zhang, Senior Member, IEEE, Yaowei Wang, Member, IEEE,\nJing Liu, Member, IEEE, Jinqiao Wang, Member, IEEE, and Ming Tang, Member, IEEE\nAbstract—In person re-identification, extracting part-level fea-\ntures from person images has been verified to be crucial to\noffer fine-grained information. Most of existing CNN-based\nmethods only locate the human parts coarsely, or rely on\npre-trained human parsing models and fail in locating the\nidentifiable non-human parts (e.g., knapsack). In this paper,\nwe introduce an alignment scheme in Transformer architecture\nfor the first time and propose the Auto-Aligned Transformer\n(AAformer) to automatically locate both the human parts and\nnon-human ones at patch-level. We introduce the “Part tokens\n([PART]s)”, which are learnable vectors, to extract part features\nin Transformer. A [PART] only interacts with a local subset of\npatches in self-attention and learns to be the part representation.\nTo adaptively group the image patches into different subsets,\nwe design the Auto-Alignment. Auto-Alignment employs a fast\nvariant of Optimal Transport algorithm to online cluster the\npatch embeddings into several groups with the [PART]s as their\nprototypes. AAformer integrates the part alignment into the\nself-attention and the output [PART]s can be directly used as\npart features for retrieval. Extensive experiments validate the\neffectiveness of [PART]s and the superiority of AAformer over\nvarious state-of-the-art methods.\nIndex Terms—person re-identification, auto-alignment, part-\nlevel representation, Transformer\nManuscript received 5 September 2022; revised 11 May 2023; accepted 24\nJuly 2023. This work was supported in part by the Key-Area Research and De-\nvelopment Program of Guangdong Province under Grant 2021B0101410003;\nin part by the National Natural Science Foundation of China under Grant\n62276260, Grant 62002356, and Grant 61976210; and in part by the Beijing\nNatural Science Foundation under Grant 4244099. (Corresponding author:\nHaiyun Guo.)\nKuan Zhu and Ming Tang are with the Foundation Model Research Center,\nInstitute of Automation, Chinese Academy of Sciences, Beijing 100190, China\n(e-mail: kuan.zhu@nlpr.ia.ac.cn; tangm@nlpr.ia.ac.cn).\nHaiyun Guo is with the Foundation Model Research Center, Institute of\nAutomation, Chinese Academy of Sciences, Beijing 100190, China, also\nwith the School of Artificial Intelligence, University of Chinese Academy of\nSciences, Beijing 100049, China, and also with the Development Research\nInstitute of Guangzhou Smart City, Guangzhou 510805, China (e-mail:\nhaiyun.guo@nlpr.ia.ac.cn).\nShiliang Zhang is with the National Key Laboratory for Multimedia\nInformation Processing, School of Computer Science, Peking University,\nBeijing 100871, China (e-mail: slzhang.jdl@pku.edu.cn).\nYaowei Wang is with the Peng Cheng Laboratory, Shenzhen 518066, China\n(e-mail: wangyw@pcl.ac.cn).\nJing Liu is with the Foundation Model Research Center, Institute of\nAutomation, Chinese Academy of Sciences, Beijing 100190, China, and also\nwith the School of Artificial Intelligence, University of Chinese Academy of\nSciences, Beijing 100049, China (e-mail: jliu@nlpr.ia.ac.cn).\nJinqiao Wang is with the Foundation Model Research Center, Institute\nof Automation, Chinese Academy of Sciences, Beijing 100190, China, also\nwith the School of Artificial Intelligence, University of Chinese Academy of\nSciences, Beijing 100049, China, also with the Wuhan AI Research, Wuhan\n430073, China, and also with the Peng Cheng Laboratory, Shenzhen 518066,\nChina (e-mail: jqwang@nlpr.ia.ac.cn).\nDigital Object Identifier 10.1109/TNNLS.2023.3301856\nFig. 1: (a) The illustration of the added [PART]s. A [PART]\nonly interacts with a subset of the patch embeddings and thus\ncan learn to represent the subset. (b) [PART]s with PCB’s par-\ntitioning [1]. (c) [PART]s with SPReID’s partitioning [2]. (d)\nA toy example of the proposed Auto-Alignment mechanism.\nThe image patches of the same part, which can be a human\npart or non-human one, are adaptively grouped to the identical\n[PART].\nI. I NTRODUCTION\nPerson re-identification (re-ID) aims to associate the specific\nperson across cameras. Lots of existing convolution neural\nnetwork-based methods extract part-level features to obtain\nfine-grained information to alleviate the body part misalign-\nment problem, which can be caused by inaccurate person\ndetection, human pose variations, and the changing of camera\nviewpoints. The stripe-based methods [1], [3] design stripe-\nbased image partitions to locate human parts. Some methods\nadopt pre-trained human parsing models [2] to locate human\nparts of fine granularity. These methods either cannot well\nalign the human parts or fail in locating the identifiable non-\nhuman parts like knapsack [4].\nRecently, the self-attention-based architecture, Transformer\n[5], has shown its scalability and effectiveness in many\ncomputer vision tasks. ViT [6] first employs Transformer\narchitecture to conduct image recognition. They divide the\narXiv:2104.00921v3  [cs.CV]  25 Jun 2024\n2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\ninput image into fixed-size patches, linearly embed each of\nthem, and fed them to a Transformer encoder to obtain the\nimage representation. TransReID [7] proposes to apply the ViT\nto object re-identification. They design patch shuffle operations\nand introduce side information like camera/view IDs to learn\nrobust features, validating the superiority of Transformer in\nre-ID tasks. As they do not take the misalignment issue into\nconsideration, the part alignment scheme in the Transformer\narchitecture is still unexplored.\nIn this paper, we propose a specific alignment scheme for\nTransformer and also alleviate the problems of existing meth-\nods, i.e., aligning human parts coarsely and failing in locating\nnon-human parts. The proposed Auto-Aligned Transformer\n(AAformer) integrates the part alignment into the self-attention\nand adaptively locate both the human parts and non-human\nones at patch-level in an online manner.\nFirst, we propose the “Part tokens ([PART]s)”, which are\nlearnable vectors, for Transformer to learn the representa-\ntions of local parts. Several [PART]s are concatenated to\nthe sequence of patch embeddings and subsequently fed to\nthe Transformer encoder. In self-attention, a [PART] only\nattends to a local subset of patch embeddings and learns\nto represent the subset, which is shown in the first row of\nFigure 1. The existing CNN-based methods can be transferred\nto Transformer by [PART]s, e.g., PCB [1] and SPReID [2],\nwhich are illustrated in the second row of Figure 1, showing\nthe problems of aligning human parts coarsely and failing in\nlocating non-human parts.\nBy means of the [PART], we propose the Auto-Aligned\nTransformer (AAformer) to adaptively group the image\npatches into different subsets. We employ a fast variant of\nOptimal Transport algorithm [8] to online cluster the patch\nembeddings into several groups with the [PART]s as their\nprototypes (e.g., leg prototype, knapsack prototype). The patch\nembeddings of the same semantic part are gathered to the\nidentical [PART] and the [PART] only interacts with these\npatch embeddings. Given an input image, the output [PART]s\nof AAformer learn to be different part prototypes of patch\nsubsets of the input image, and can be directly used as part\nfeatures for re-ID. A toy example of this Auto-Alignment is\nshown in the last row of Figure 1. In each layer of AAformer,\nwe sequentially conduct the Auto-Alignment to align the body\nparts and the Self-Attention to learn the part representations\nin the feedforward propagation. Therefore, AAformer is an\nonline method that simultaneously learns both part alignment\nand part representations, locating both human parts and non-\nhuman ones more accurately.\nThe contributions of this work are summarized as follows:\n1) We introduce the “Part tokens ([PART]s)” for Trans-\nformer to learn part features and integrate the part alignment\ninto the self-attention. A [PART] only interacts with a subset\nof patch embeddings, and thus can learn to be the local\nrepresentation.\n2) We further propose the AAformer to adaptively locate\nboth the human parts and non-human ones online. Instead of\nusing a pre-defined fixed patch assignment, AAformer simul-\ntaneously learns both part alignment and part representations.\n3) Extensive experiments validate the effectiveness of\n[PART] and the superiority of AAformer over various state-\nof-the-art methods on the widely used benchmarks, i.e.,\nMarket-1501 [9], CUHK03 [10], DukeMTMC-reID [11] and\nMSMT17 [12].\nThe rest of this paper is arranged as follows. The second\npart reviews some works related to this paper. The third\npart introduces the main architecture of the proposed method\nand expounds the proposed method in detail. The fourth\npart shows the good performance of the proposed method,\ncompares it with the state-of-the-art methods, and analyzes\nthe reasons why the proposed method is effective. The fifth\npart summarizes the full paper.\nII. R ELATED WORK\nA. Aligned Person Re-identification\nLots of existing CNN-based person re-ID approaches focus\non global representation learning of identity-discriminative\ninformation, e.g., IDE network learning [10], [13]–[17], metric\nlearning [18]–[23]. However, the lacking of explicit alignment\nmechanism largely limits their performance. To remedy this,\nmany efforts have been made to develop the aligned person re-\nID, which aims to extract part-level features. These methods\ncould be roughly summarized to the following streams:\n1) Stripe-based approaches. Some researchers develop\nstripe-based methods to extract part features. They directly\npartition the person images into several fixed horizontal stripes.\nPCB [1] first equally partitions the person images and then\nadopts part classifiers to refine each part in an attention-like\nmanner. MGN [3] enhances the robustness by dividing images\ninto stripes of different granularities and designs overlap\nbetween stripes. However, these stripes are too coarse and with\nfixed heights and positions but do not correspond to specific\nsemantic parts, and therefore fail in aligning different human\nparts well. Besides, there still remains much background noise\nin their stripes. Compared with these methods, our AAformer\ncan adaptively locate different local parts at patch-level, which\nis more accurate.\n2) Bounding box-based Approaches. Some works propose\nto locate the local parts by incorporating a bounding box selec-\ntion sub-network [24]–[26]. MSCAN [24] employs the STN\n[27] to locate the latent discriminative parts and subsequently\nextracts the part-level features. DPL [25] proposes to generate\nbounding boxes from saliency maps and then extract part-\nlevel features from these parts. However, the shape of located\nareas of these methods is limited to the rectangle, thus the\nlocated human parts are still coarse. Besides, there is usually\nmuch overlap between the bounding boxes predicted by these\nmethods. In contrast, AAformer can adaptively locate different\ndiscriminative parts with arbitrary shapes and ensure that there\nis no overlap between each part.\n3) Extra semantics-based methods. Some other works\nemploy extra semantics to locate human parts. SPReID [2]\nproposes to employ a pre-trained human semantic parsing\nmodel to provide the mask of body parts for alignment. PS\n[28] adpots the human segmentation model to conduct the\npart awareness learning process. CDPM [29] also proposes to\nuse the human parsing results to align the human body part\nZHU et al.: AAFORMER: AUTO-ALIGNED TRANSFORMER FOR PERSON RE-IDENTIFICATION 3\nin the vertical direction. DSA-reID [30] adopts dense extra\nsemantic information of 24 regions for a person. However,\nthe identifiable personal belongings like knapsack and reticule,\nwhich are crucial for person re-ID, cannot be recognized\nby the pre-trained models and are ignored as background.\nBesides, the accuracy of extra semantics heavily counts on\nthe pretrained human parsing models or the pose estimators.\nAnd these approaches cannot re-correct the errors of semantic\nestimation in their training processes. In contrast, AAformer\ncan adaptively locate all the identifiable parts including both\nthe human parts and nonhuman ones and the adaptive patch\nassignment is conducted online in every layer of AAformer, so\neven if some patches are assigned incorrectly in one AAformer\nlayer, other layers can also make the right patch assignment,\nwhich increases the robustness.\n4) Attention-based methods. Attention mechanism con-\nstructs alignment by suppressing background noise and en-\nhancing the discriminative regions [31]–[36]. HA-CNN [32]\nformulates the harmonious attention CNN model by joint\nlearning of soft pixel attention and hard regional attention.\nMHN [31] utilizes the complex and high-order statistics in-\nformation in the attention mechanism, so as to capture the\nsubtle but discriminative foreground areas in person images.\nHowever, the part partition of these methods is also coarse\nand these methods cannot explicitly locate the semantic parts.\nBesides, the semantic consistency of the focus area among im-\nages is not guaranteed. By contrast, our AAformer can locate\nthe identifiable local parts at the patch-level and guarantee the\nsemantic consistency among images by the proposed [PART]s.\nApart from AAformer, ISP [4] also proposes to locate both\nhuman parts and non-human ones automatically by iterative\nclustering, but their off-line part partition is a little bit time-\ncosting and prevents their method from end-to-end training.\nB. Visual Transformer\nRecently, the Transformer [5] is showing its superiority over\nconventional methods in many vision tasks. ViT [6] proposes\nthe Vision Transformer to apply a pure Transformer to image\nrecognition. They first divide the input image into image\npatches and then map them to a sequence of vectors with\na linear projection. An extra learnable “class token (CLS)”\nis added to the sequence and the vector sequence is fed to\na typical Transformer encoder. TransReID [7] proposes to\napply ViT to object re-identification. They design patch shuffle\noperations and introduce side information like camera/view\nIDs to learn robust features, validating the superiority of the\nTransformer on re-ID task. DCAL [37] proposes to implicitly\nextract the local features using a Transformer decoder. PAT\n[38] and HAT [39] both integrate Transformer architecture\ninto CNNs. PAT proposes to use Transformer to generate\nthe attention masks for CNN and HAT uses Transformer to\naggregate hierarchical features from CNN backbones. How-\never, as they do not design an explicit alignment scheme for\nTransformer, how to extract discriminative part features from\nperson images is still unexplored in Transformer architecture.\nIn this paper, we aim to propose a specific alignment scheme\nfor Transformer and also alleviate the problems in existing\nCNN-based alignment methods.\nBesides, some methods [40], [41] also design to add learn-\nable tokens to the Transformer network. GroupViT [40] uses\nthe Gumbel Softmax function to group the patches to the\nlearnable group tokens, which assign the image patches to their\nmost similar group tokens with a high probability. Compared\nwith GroupViT, AAformer uses Optimal Transport to assign\nimage patches to [PART]s, which can avoid the trivial solution\nwhere all the patches are assigned to the same [PART]. VPT\n[41] fixes the parameters of a pre-trained model and only trains\nthe added learnable token to reduce the number of parameters\nand calculation cost. The learnable token in VPT still interacts\nwith all the image patches. While in AAformer, a [PART] only\ninteracts with a subset of patch embeddings, thus can learn to\nbe the local representation.\nIII. M ETHODOLOGY\nIn this section, we expound the proposed method in detail.\nWe begin by briefly revisiting the general Transformer archi-\ntecture in Sect. III-A. Then we present the proposed [PART]\nand the Auto-Aligned Transformer (AAformer) in Sect. III-B\nstep by step.\nA. The Main Architecture\nWe follow Vision Transformer (ViT) [6] to construct the\nmain architecture. Given an input image x ∈ RH×W×C\nwith resolution (H, W) and channel C, we reshape it into\na sequence of flattened 2D patches x ∈ RN×(I2·C) to fit\nthe Transformer architecture, where (I, I) is the resolution\nof each image patch and N = (H · W)/I2 is the length of\nimage patch sequence. We map the patches to vectors of D\ndimensions with a trainable linear projection and refer to the\noutput of this projection as the patch embeddings. A standard\nembedding of “class token (CLS)” is added to extract the\nglobal representation. Lastly, the outcome vector sequence\nZ ∈ RL×D is fed to the Transformer encoder, where L=1+N.\nWe also add the standard learnable 1D position embeddings\nto the vector sequence in element-wise to retain positional\ninformation.\nThe standard Transformer layer [5] consists of Multi-\nhead Self-Attention (MSA) and Multi-Layer Perception (MLP)\nblocks. The self-attention mechanism is based on a trainable\nassociative memory with key and value vector pairs. For every\nquery vector in a sequence of query vectors ( Q ∈ RL×D),\nwe calculate its inner products with a set of key vectors\n(K ∈ RL×D). These inner products are then scaled and\nnormalized with a softmax function to obtain L weights. The\noutput of the self-attention for this query is the weighted sum\nof a set of L valuevectors (V ∈ RL×D). For all the queries\nin the sequence, the output matrix of self-attention can be\nobtained by:\nAttention(Q, K, V) = Softmax (QKT\n√\nD\n)V, (1)\nwhere the Softmax function is applied over each row of the\ninput matrix and the\n√\nD term provides appropriate normaliza-\ntion. Query, key and value matrices are all computed from\nthe vector sequence Z using different linear transformations:\n4 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\nFig. 2: (a) The overview of AAformer. We divide the input image into fixed-size patches, linearly embed each of them and add\nthe position embeddings. We add the extra learnable vectors of “Class token (CLS)” and “Part tokens ([PART]s)” to learn the\nglobal and part representations of person images. The [PART]s fed to the first Transformer layer are parameters of AAformer\nwhich learn the part prototypes of the datasets. The [PART]s output by Transformer layers are learned feature embeddings\nto represent human parts for input images. (b) Single-head Auto-Alignment. The self-attention for [PART]s is replaced by\nAuto-Alignment. QPT: query vectors of [PART]s. Φp: the patches assigned to the pth [PART]. Q, K, V: query, key, value\nvectors of patch embeddings.\nQ = ZWQ, K = ZWK, V = ZWV . Finally, the Multi-\nhead Self-Attention layer (MSA) is defined by considering h\nattention “heads”, i.e., h self-attention functions are applied to\nthe input in parallel. Each head provides a sequence of size\nL × d, where d = D/h typically. The outputs of the h self-\nattention are rearranged into a L × D sequence to feed the\nnext Transformer layer.\nThe MSA is followed by a MLP block to build a full Trans-\nformer layer. This MLP contains two linear layers separated\nby a GELU non-linearity [42]. The first linear layer expands\nthe dimension from D to 4D and the second reduces the\ndimension back to D. Both MSA and MLP are operating\nas residual connections and with a layer normalization (LN)\n[43] before them. We change the MSA layer to Multi-head\nAuto-Alignment (MAA) layer to build our AAformer. The\noverview of AAformer is shown in the left part of Figure 2.\nB. Auto-Aligned Transformer\n[PART]s for Transformer. We propose the [PART]s for\nTransformer to extract the part features and integrate the\npart alignment into self-attention. We concatenate P [PART]s,\nwhich are learnable vectors, to the sequence Z, thus now the\nlength L of Z is 1+ P +N. A [PART] only interacts with\na subset of patch embeddings, rather than all of them, and\nthus can learn to be the partial representation of the subset.\nSpecifically, we denote QPT = [QPT\n1 , ...,QPT\nP ] as the query\nvectors of [PART]s and denote Φp as the subset of patch\nembeddings assigned to the pth [PART]. For the query vector\nof the pth [PART], denoted asQPT\np , we only calculate its inner\nproducts with the key vectors belonging to Φp and then scale\nand normalize these inner products with softmax function to\nobtain len(Φp) weights. The output of the part alignment for\nQPT\np is the weighted sum of value vectors belonging to Φp,\nwhich is formulated as:\nAlignment(QPT\np , K, V) = Softmax (\nQPT\np KT\nΦp\n√\nD\n)VΦp , (2)\nwhere KΦp and VΦp are the key and value vectors belonging\nto Φp, respectively. After the part alignment, the output vector\nof [PART] p becomes the part representation of subset Φp.\nThe CLS token and patch embeddings are processed by the\noriginal self-attention (Eq. 1).\nNow, to extract discriminative part features, the core prob-\nlem is how to accurately and adaptively assign image patches\nto Φp, p∈ {1, ..., P}.\nMulti-head Auto-Alignment. The Auto-Alignment aims\nat automatically grouping the image patches into different\nΦp (p ∈ {1, ..., P}), in which both human parts and non-\nhuman ones are included. The Φp for different [PART]s should\nbe mutually exclusive. Our idea is adaptively clustering the\npatch embeddings into P groups with the [PART]s as their\nprototypes. This makes the patch assignment problem the\nsame as the Optimal Transport problem [8]. In detail, the\nquery vectors of [PART]s, QPT ∈ RP×D, are regarded as\nthe part prototypes, and we are interested in mapping the\nkey vectors of image patches K ∈ RN×D to the QPT to\nobtain the patch assignment (CLS token does not participate\nin the clustering and is omitted for simplicity). We denote\nthis mapping by Y ∈ RP×N , and the value in position (p, n)\ndenotes the probability of the nth image patch belonging to\nthe pth [PART]. To maximize the similarity between K and\nQPT, Y is optimized by:\nmax\nY∈Y\nTr\n\u0000\nY⊤QPTK⊤\u0001\n+ εH(Y), (3)\nZHU et al.: AAFORMER: AUTO-ALIGNED TRANSFORMER FOR PERSON RE-IDENTIFICATION 5\nwhere Tr\n\u0000\nY⊤QPTK⊤\u0001\nmeasures the similarity between\nK and QPT, Tr means the trace of a matrix, H(Y) =\n−P\nij Yij log Yij is the entropy function, and ε is the pa-\nrameter that controls the smoothness of the mapping and is\nset to be 0.05 in our experiments.\nTo further prevent the trivial solution where all the patches\nare assigned to the same [PART], we enforce an equal partition\nby constraining the matrix Y to belong to the transportation\npolytope [44], [45]. We adapt this constraint to patch assign-\nment by restricting the transportation polytope to the image\npatches of an image:\nY =\n\u001a\nY ∈ RP×N\n+ | Y1 N = 1\nP 1 P , Y⊤1 P = 1\nN 1 N\n\u001b\n, (4)\nwhere 1 P denotes the vector of ones in dimension P. These\nconstraints enforce that on average each [PART] is selected\nat least N/P times in an image. The soft assignment Y∗ are\nthe solution to Prob. 3 over the set Y and takes the form of a\nnormalized exponential matrix [8]:\nY∗ = Diag(u) exp\n\u0012QPTK⊤\nε\n\u0013\nDiag(v), (5)\nwhere u and v are renormalization vectors in RP and RN\nrespectively, Diag(∗) represents a matrix with ∗ as its diagonal\nelements. These renormalization vectors are computed using\na small number of matrix multiplications by a fast variant\nof Sinkhorn-Knopp algorithm [8]. This algorithm can be effi-\nciently run on GPU and we observe that only 3 iterations are\nsufficient to obtain good performance for patch assignment. In\nour experiments, grouping 576 patch embeddings of an image\nto 5 [PART]s only takes 0.46ms. Once a continuous solution\nY∗ is found, the patch assignment can be obtained by using\na rounding procedure. The patch embeddings mapped to QPT\np\nform the patch subset Φp. We illustrate the Auto-Alignment\nprocess (single-head) in the right part of Figure 2. The Auto-\nAlignment is conducted in parallel in different attention heads,\nwhich enhances the robustness of patch assignment. We name\nthis Multi-head Auto-Alignment (MAA) and the outputs of\ndifferent attention heads are concatenated together. The output\n[PART]s of AAformer are used to perform person re-ID.\nIn each layer of AAformer, we conduct Auto-Alignment to\nalign body parts and the Self-Attention to learn part represen-\ntations in the feedforward propagation. Therefore, AAformer\nis an online and end-to-end method that simultaneously learns\nboth part alignment and part representations.\nDiscussion on [PART]s in AAformer. (1) The [PART]s\nfed to the first Transformer layer are learnable parameters\nwe add to the network, which are shown as dashed lines\nin Figure 3. After continually interacting with the patch\nembeddings of the same semantic part of different person\nidentities during the training, the added [PART]s learn to be the\npart prototypes of the training dataset and are dataset-adaptive.\nThis also guarantees the semantic consistency throughout the\ndataset. (2) After the [PART]s go through the Transformer\nlayers and interact with the patch embeddings of an input\nimage, the output [PART]s of Transformer layers are the part\nrepresentation of patch subsets of the input image, which are\nshown as solid lines in Figure 3. That is, the output [PART]s\nFig. 3: Illustration of [PART] in different layers of AAformer.\nThe [PART]s fed to the first Transformer layer are the learn-\nable parameters we add to the network. They will learn to\nbe the part prototypes of the dataset in the training and\nare dataset-adaptive. The output [PART]s of the Transformer\nlayers are the part representations of the input images and they\nare instance-adaptive.\nof each Transformer layer are instance-adaptive and can be\nemployed directly as part-level features of the input image for\nthe re-ID task.\nC. Objective Function\nIn the training phase, the re-ID heads [46] are attached to the\noutput CLS and [PART]s of AAformer. Specifically, the output\nCLS Z0 represents the global feature of the input image, and\nthe output [PART]s Z1:P are the part features. We employ\ncross-entropy loss and triplet loss with hard sample mining\n[47] to train our model. The cross-entropy loss is calculated\nas:\nLcls = 1\nP + 1\nPX\ni=0\n−log P(Zi), (6)\nwhere P(Zi) is the probability of token Zi belonging to its\nground truth identity. The classifiers of different tokens are\nnot shared. Label smoothing [48] is adopted to improve the\nperformance.\nThe [PART]sZ1:P are concatenated together to calculate the\npart triplet loss. Together with the global triplet loss calculated\nwith CLS, there are two terms in the triplet loss:\nLtri = 1\n2\n\u0000\n[dg\np − dg\nn + α]+ + [dp\np − dp\nn + α]+\n\u0001\n, (7)\nwhere dg\np and dg\nn are feature distances of global representation\n(CLS) from positive pair and negative pair, respectively. dp\np\nand dp\nn are feature distances of concatenated [PART]s from\npositive pair and negative pair, respectively. α is the margin\nof triplet loss and is set to 0.3 . [·]+ equals to max(·, 0).\nThe triplet losses are calculated with hard sample mining\nstrategy [47]. Specifically, for each image, the hard sample\nmining strategy only uses the hardest positive sample (the least\nsimilar positive sample) and the hardest negative sample (the\nmost similar negative sample) to form the positive pair and\nnegative pair. In AAformer, the triplet losses for global feature\n6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\nand part feature are calculated separately. That is, the hardest\npositive and negative pairs found by global and part features\ncan be different. Therefore, there are two groups of positive\nand negative pairs for each image in AAformer. Therefore, the\noverall objective function for our model is:\nL = Lcls + Ltri. (8)\nIn the testing phase, CLS and [PART]s are concatenated\ntogether to represent a person image.\nIV. E XPERIMENTS\nIn this section, we first list the implementation details\nof the proposed method. Then, we compare the proposed\nmethod with the state-of-the-art methods on both holistic and\noccluded person re-ID benchmark. At last, we conduct the\nablation studies including the effectiveness of [PART]s, the\neffectiveness of AAformer, the comparison between Optimal\nTransport and Nearest Neighbor, and the visualization of\nAAformer. All these results validate the effectivenss of the\nproposed method.\nA. Implementation Details and Datasets\nThe Transformer architecture. We use the smallest Vision\nTransformer model proposed in [6], ViT-Base, as the main\narchitecture of AAformer. It contains 12 Transformer layers\nwith the hidden size of 768 dimensions ( D=768). The MLP\nsize is 4 times the hidden size and the head number is 12 for\nmulti-head operations.\nData preprocessing. The input images are resized to 256×\n128 and the patch size is 16×16. We adopt the commonly used\nrandom cropping [49], horizontal flipping and random erasing\n[34] (with a probability of 0.5) for data augmentation.\nTraining. We warm up the model for 10 epochs with a\nlinearly growing learning rate from 8×10−4 to 8×10−3. Then,\nthe learning rate is decreased with the cosine learning rate\ndecay. It takes 120 epochs to finetune on the re-ID datasets.\nAAformer randomly samples 16 identities and 4 images per\nperson to constitute a training batch. The batch size equals to\n16*4=64. SGD optimizer is adopted with a momentum of 0.9\nand the weight decay of 1×10−4 to optimize the model. Our\nmethods are implemented on PyTorch and MindSpore 1. The\nTransformer backbone is pre-trained on ImageNet [50].\nDataset #ID #Train #Test #Images\nDukeMTMC-reID 1402 702 702 36411\nMarket-1501 1501 751 750 32668\nCUHK03 1467 767 700 14097\nMSMT17 4101 1041 3060 126441\nTABLE I: Re-ID datasets and details.\nDatasets and Evaluation Metrics. We conduct experi-\nments on four widely used person re-ID benchmarks, i.e.,\nDukeMTMC-reID [11], Market-1501 [9], CUHK03 (New Pro-\ntocol) [70], and the large-scale MSMT17 [12]. The standard\n1The codes based on MindSpore will be released at\nhttps://gitee.com/typhoonai/AAformer\ntraining/test ID split is used and detailed in Table I. Fol-\nlowing common practices, we use the cumulative matching\ncharacteristics (CMC) and the mean average precision (mAP)\nto evaluate the performance. Euclidean distance is used to\nmeasure the feature distances.\nB. Comparison with State-of-the-art Methods\nWe compare our method with the state-of-the-art methods\non four widely used holistic benchmarks and one occluded\nbenchmark in Table II and Table III. We also show the results\nof the baseline model (ViT-baseline [6]) in the tables. We\nfollow BoT [46], a popular baseline method in person ReID\nfiled, to form the ViT-baseline. Most of the settings refer\nto BoT, including the warmup learning rate, random erasing\naugmentation [34], label smoothing [48] and BNNeck [46].\nDukeMTMC-reID. AAformer obtains the best results and\noutperforms others by considerable margins, e.g., at least 0.5%\non Rank-1 accuracy and 0.8% on mAP accuracy. The com-\npared methods are categorized into four groups, i.e., stripe-\nbased methods, bounding box-based methods, extra semantics-\nbased methods, and others. The others include the general\nattention methods based on CNN [36], [59], the work that\ncombines self-attention with CNN [38], [58] and the method\nbased on generative adversarial network [61]. AAformer sur-\npasses all the above methods by considerable margins, which\nvalidates the superiority of locating both human parts and non-\nhuman ones.\nMarket1501. AAformer achieves state-of-the-art perfor-\nmance. Specifically, AAformer obtains the second-best results\nand is only slightly behind the firsts [67]. As the performance\non this dataset is nearly saturated, the results of sate-of-the-art\nmethods are very close.\nCUHK03. AAformer obtains the best performance on both\nlabeled and detected sets. More specifically, AAformer outper-\nforms the second-best algorithms [7], [60] by 1.4%/2.1% and\n1.9%/4.0% on labeled and detected sets with regards to Rank-\n1 and mAP, respectively. Compared with ISP [4], which is a\nCNN-based method that also conducts adaptive part alignment,\nAAformer obtains much better performance on this difficult\ndataset. We owe this to our online clustering, which can be\nmore accurate than the off-line clustering in ISP. AAformer\nalso significantly surpasses the ViT-baseline by 4.1 ∼ 5.6%,\nwhich validates the effectiveness of MAA.\nMSMT17. On the largest dataset, AAformer also outper-\nforms the state-of-the art results. On the metric of Rank-1,\nAAformer outperforms the second-best [7] by 1.3%. On the\nmetric of mAP, AAformer outperforms the second-best [7] by\n1.6%. It should be noted that we remove the side information\nin [7] for fair comparison. Besides, we strongly recommend\nnot using the side information like camera IDs. Because if the\ncamera IDs are used, the trained model can not be applied to\nother monitoring systems as the cameras are different, which\nlargely limits the generality and practicability.\nOccluded-DukeMTMC. AAformer sets the new state-of-\nthe-art performance and outperforms others by a large margin,\nat least 2.6%/4.6% in Rank-1/mAP. Owing to the Auto-\nAlignment, the [PART]s can adaptively focus on the visible\nZHU et al.: AAFORMER: AUTO-ALIGNED TRANSFORMER FOR PERSON RE-IDENTIFICATION 7\nMethods Ref\nDukeMTMC Market1501 CUHK03 MSMT17-reID Labeled Detected\nRank-1 mAP Rank-1 mAP Rank-1 mAP Rank-1 mAP Rank-1 mAP\nAlignedReID [51] Arxiv18 - - 91.8 79.3 - - - - - -\nPCB+RPP [1] ECCV18 83.3 69.2 93.8 81.6 - - 63.7 57.5 68.2 40.4\nMGN [3] MM18 88.7 78.4 95.7 86.9 68.0 67.4 66.8 66.0 - -\nMSCAN [24] CVPR17 - - 80.8 57.5 - - - - - -\nPAR [26] ICCV17 - - 81.0 63.4 - - - - - -\nSPReID [2] CVPR18 84.4 71.0 92.5 81.3 - - - - - -\nPABR [52] ECCV18 84.4 69.3 91.7 79.6 - - - - - -\nAANet [53] CVPR19 87.7 74.3 93.9 83.4 - - - - - -\nP2-Net [54] ICCV19 86.5 73.1 95.2 85.6 78.3 73.6 74.9 68.9 - -\nPGFA [55] ICCV19 82.6 65.5 91.2 76.8 - - - - - -\nCDPM [29] TIP19 88.2 77.5 95.2 86.0 75.8 71.1 71.9 67.0 - -\nGASM [56] ECCV20 88.3 74.4 95.3 84.7 - - - - 79.5 52.5\nPGFA v2 [57] TNNLS21 86.2 72.6 92.7 81.3 - - - - - -\nNon-Local [58] CVPR18 88.6 78.7 94.9 86.8 66.4 65.0 65.3 63.1 76.2 53.3\nIANet [59] CVPR19 87.1 73.4 94.4 83.1 - - - - 75.5 46.8\nCASN+PCB [36] CVPR19 87.7 73.7 94.4 82.8 73.7 68.0 71.5 64.4 - -\nBAT-net [60] ICCV19 87.7 77.3 95.1 84.7 78.6 76.1 76.2 73.2 79.5 56.8\nJDGL [61] CVPR19 86.6 74.8 94.8 86.0 - - - - 77.2 52.3\nOSNet [62] ICCV19 88.6 73.5 94.8 84.9 - - 72.3 67.8 78.7 52.9\nCI-CNN [63] TIP19 87.6 81.3 94.2 83.9 - - - - - -\nOCLSM [64] TIP20 87.7 79.0 94.6 87.4 - - - - 78.8 57.0\nISP [4] ECCV20 89.6 80.0 95.3 88.6 76.5 74.1 75.2 71.4 - -\nPAT [38] CVPR21 88.8 78.2 95.4 88.0 - - - - - -\nPFE [65] TIP21 88.2 75.9 95.1 86.2 - - - - 79.1 52.3\nFA-Net [66] TIP21 88.7 77.0 95.0 84.6 - - - - 76.8 51.0\nHOReID [67] TIP21 88.1 79.8 95.7 88.7 - - - - 74.4 50.4\nTransReID− [7] ICCV21 89.6 79.8 94.2 87.7 78.9 76.9 75.1 72.9 82.5 63.6\nreID-NAS [68] TNNLS21 88.1 74.6 95.1 85.7 - - - - 79.5 53.3\nMHSA-Net [69] TNNLS22 87.3 73.1 94.6 84.0 75.6 72.7 72.8 69.3 - -\nDCAL [37] CVPR22 89.0 80.1 94.7 87.5 - - - - 83.1 64.0\nViT-baseline - 88.3 78.5 94.2 86.3 75.3 74.9 74.0 71.6 79.7 58.9\nAAformer (ours) this paper 90.1 80.9 95.4 88.0 80.3 79.0 78.1 77.2 84.4 65.6\nTABLE II: Comparison with the state-of-the-art methods for the holistic person re-ID problem. Methods in the 1st group are\nthe stripe-based methods. Methods in the 2nd group are bounding box-based methods. Methods in the 3rd group are extra\nsemantics-based methods. Methods in the 4th group include the general attention methods of CNN [36], [59], the work that\ncombines self-attention with CNN [38], [58] and the method based on pure Transformer [7], [37]. TransReID − means the side\ninformation is removed for a fair comparison.\nMethods Rank-1 Rank-5 mAP\nPCB [1] 42.6 57.1 33.7\nPart Bilinear [71] 36.9 - -\nFD-GAN [72] 40.8 - -\nDSR [73] 40.8 58.2 30.4\nSFR [74] 42.3 60.3 32.0\nPGFA [55] 51.4 68.6 37.3\nPGFA v2 [57] 56.3 72.4 43.5\nHOReID [67] 55.1 - 43.8\nISP [4] 62.8 78.1 52.3\nPAT [38] 64.5 - 53.6\nViT-baseline 60.8 77.8 52.5\nAAformer (ours) 67.1 81.6 58.2\nTABLE III: Comparison with the state-of-the-art methods for\nthe occluded re-ID problem on Occluded-DukeMTMC.\nareas in the occluded images and extract discriminative fea-\ntures from these visible areas. We also visualize the attention\nmaps of [PART]s in the ablation studies to validate this.\nC. Ablation Studies\nThe effectiveness of the [PART]s. We first conduct exper-\niments to validate the effectiveness of [PART]s. We begin by\nnaively adopting the alignment paradigm of CNNs, i.e., part\nmasks with part pooling, to the baseline Transformer model\n(ViT-Base). Three typical methods, i.e., PCB, MGN, and\nSPReID, are used for this experiment. The first two methods\nare stripe-based methods and the third method adopts the\nextra semantics. As shown in Table IV, naively applying the\nalignment paradigm of CNNs even reduces the performance\nof the baseline model, which may be because pooling is not\nsuitable for Transformer-based person re-ID.\nNext, we discard the pooling operation and add the proposed\n[PART]s to ViT to extract part features. These typical CNN-\nbased methods are easily transferred to Transformer by our\n[PART]s, e.g., we assign each [PART] with patches within a\nstripe to simulate the image partition of PCB and divide the\nimage patches according to a pre-trained human parsing model\nto simulate SPReID. Specifically, in the simulation experiment\nof SPReID, if a patch contains multiple semantics, we assign\nthe patch to all the semantics it contains. As shown in Ta-\nble IV, models with [PART]s bring consistent improvement to\nthe baseline, e.g., [PART]s with MGN’s partitioning surpasses\nbaseline by 2.8%/4.5% in terms of Rank-1/mAP on MSMT17,\nvalidating the significant advantages of [PART]s in extracting\npart features for Transformer. This is because [PART]s can\nextract the local features through the self-attention mechanism\nin every layer of Transformer, while the part pooling operation\n8 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\nMethods Scheme MSMT17 CUHK03(L)\nRank-1 mAP Rank-1 mAP\nViT-baseline None 79.7 58.9 75.3 74.9\n+PCB Pool 81.3 59.8 76.3 75.6\n+MGN Pool 81.6 60.5 76.5 75.3\n+SPReID Pool 79.6 58.7 74.9 73.7\n+PCB PT 82.2 62.1 78.4 76.9\n+MGN PT 82.5 63.4 78.8 77.3\n+SPReID PT 81.2 60.3 79.1 77.5\nViT-baseline 6*CLS 80.2 59.4 75.7 75.1\nAAformer PT 84.4 65.6 80.3 79.0\nTABLE IV: The effectiveness of [PART]s and AAformer.\n“Pool” means “Pooling” and “PT” means “[PART]s”.\nFig. 4: The ranking lists of TransReID and AAformer in\nmisalignment scenes. Tiny clues are found by our AAformer.\nonly obtains the local features through the last layer.\nBesides, to prove the improvement is not brought by the\nincrease of feature dimension, we conduct the experiment\nwhich directly adds more CLS tokens to ViT. The results in\nTable IV validate that naively increasing the feature dimension\nonly slightly improves the performance as the information\ncontained in these CLS tokens is similar and redundant.\nThe effectiveness of AAformer. As shown in the last row\nof Table IV, AAformer surpasses all the typical alignment\nmethods reimplemented on [PART]s by large margins, e.g.,\nAAformer outperforms [PART]s with MGN’s partitioning by\n1.9%/2.2% in terms of Rank-1/mAP on MSMT17, which\nvalidates the superiority of online adaptive patch assignment\nover the fixed patterns. Besides, to validate the strong ability\nof AAformer in finding the tiny clues in the misalignment\nscenes, we compare the ranking list between TransReID [7]\nand AAformer in Figure 4. As shown, AAformer can find\nthe discriminative tiny clues from both human parts and non-\nhuman ones, while TransReID fails at this due to the lacking\nof an alignment scheme.\nOptimal Transport vs. Nearest Neighbor. Most other\nclustering methods (K-means, DBSCAN) cannot be used for\nadaptive patch assignment, because they cannot guarantee the\ngrouped clusters of different images (or the clusters of the\nsame image from different layers) to have the same semantics\nand correspond to the consistent [PART]s. A naive alternative\nalgorithm is the Nearest Neighbor (NN), which assigns the\npatch embedding to the nearest [PART]. Unfortunately, in\nour experiments, we observe that most patch embeddings are\ngrouped to the same [PART] when adopting NN as there is no\nconstraint on the patch assignment. The results in Table V also\ndemonstrate the superiority of Optimal Transport over Nearest\nNeighbor.\nSettings MSMT17 CUHK03(L)\nRank-1 mAP Rank-1 mAP\nNearest Neighbor 82.2 61.5 79.1 77.4\nOptimal Transport 84.4 65.6 80.3 79.0\nTABLE V: The comparison of patch assignment with the\nNearest Neighbor and the Optimal Transport algorithm.\nDifferent numbers of [PART]s in AAformer. Intuitively,\nthe number of [PART]s P determines the granularity of the\nbody parts. Thanks to the flexibility of AAformer, we can\npartition the input image with different granularity in paral-\nlel in an AAformer model. For example, granularity {2, 3}\nindicates there are 5 [PART]s added. The first two [PART]s\ndivide the image patches into two groups and the last three\n[PART]s divide them into three. All the [PART]s are randomly\nand independently initialized, but they will be assigned image\npatches of different semantics in the training, and eventually\nlearn to be different part prototypes. The results of AAformer\nwith different numbers of [PART]s are shown in Table VI.\nAs we can observe, the setting of {2, 3} usually obtains the\nbest accuracy, which is consistent with the setting of MGN.\nBesides, the results also show that the number of [PART]s is\nnot the more the better. All the previous results are obtained\nwith the default setting of granularity {2, 3}.\nP MSMT17 CUHK03(L)\nRank-1 mAP Rank-1 mAP\n{4} 82.4 62.3 78.5 76.9\n{5} 82.0 61.6 79.8 78.2\n{6} 81.7 61.4 79.2 77.3\n{2,3} 84.4 65.6 80.3 79.0\n{3,4} 83.1 63.2 79.4 78.0\n{2,3,4} 82.9 63.0 78.8 77.6\nTABLE VI: Ablation studies on different numbers of [PART]s.\nVisualization of AAformer. Lastly, we conduct the visual-\nization experiments to show the focusing areas of [PART]s. We\nvisualize the attention map of [PART]s in the first head of the\nthird Transformer layer, which is shown in Figure 5. As we can\nobserve, AAformer can focus on both human parts and non-\nhuman ones, which are both crucial for re-ID. This is also the\nmain superiority of AAformer over existing methods. Besides,\nwe can also find that AAformer can effectively handle the\npose variation and the occlusion problems. Owing to the self-\nattention mechanism, the [PART]s have very low responses to\nthe background patches, thus it is no need for AAformer to\nremove the background patches. The visualization also proves\nthe semantics consistency of the located parts among images.\nV. C ONCLUSION\nIn this paper, we propose the [PART]s for Transformer to\nextract the part-level features. We propose to integrate the part\nZHU et al.: AAFORMER: AUTO-ALIGNED TRANSFORMER FOR PERSON RE-IDENTIFICATION 9\nFig. 5: The attention map of [PART] (PT). For one [PART], the patches not assigned to it are masked by black. The color\nrange from blue to red indicates increasing attention.\nlocalization process into the self-attention of Transformer to\nonline learn the part representations. The proposed method,\nAuto-Aligned Transformer (AAformer), can adaptively locate\nboth human parts and non-human ones using Optimal Trans-\nport (OT) without the help of extra semantics. It should be\nnoted that most other clustering methods (k-means, DBSCAN)\ncannot be used in AAformer, because they cannot guarantee\nthat the grouped clusters of different images (or the same\nimage from different layers) are of the identical semantics\nand correspond to the consistent [PART]s. It is our novel\ndesign of AAformer that makes it possible to use OT to solve\nthe adaptive alignment problem. Extensive experiments also\nvalidate the effectiveness of [PART]s and the superiority of\nAAformer over lots of state-of-the-art methods.\nREFERENCES\n[1] Y . Sun, L. Zheng, Y . Yang, Q. Tian, and S. Wang, “Beyond part models:\nPerson retrieval with refined part pooling (and a strong convolutional\nbaseline),” in Proceedings of the European Conference on Computer\nVision (ECCV), 2018, pp. 480–496.\n[2] M. M. Kalayeh, E. Basaran, M. G ¨okmen, M. E. Kamasak, and M. Shah,\n“Human semantic parsing for person re-identification,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition ,\n2018, pp. 1062–1071.\n[3] G. Wang, Y . Yuan, X. Chen, J. Li, and X. Zhou, “Learning discriminative\nfeatures with multiple granularities for person re-identification,” in 2018\nACM Multimedia Conference on Multimedia Conference . ACM, 2018,\npp. 274–282.\n[4] K. Zhu, H. Guo, Z. Liu, M. Tang, and J. Wang, “Identity-guided human\nsemantic parsing for person re-identification,” in Proceedings of the\nEuropean Conference on Computer Vision (ECCV) , 2020, pp. 346–363.\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proceedings\nof the 31st International Conference on Neural Information Processing\nSystems, 2017, pp. 6000–6010.\n[6] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[7] S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang, “Transreid:\nTransformer-based object re-identification,” in Proceedings of the IEEE\nInternational Conference on Computer Vision , 2021.\n[8] M. Cuturi, “Sinkhorn distances: Lightspeed computation of optimal\ntransport,” Advances in neural information processing systems , vol. 26,\npp. 2292–2300, 2013.\n[9] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scalable\nperson re-identification: A benchmark,” in Computer Vision, IEEE\nInternational Conference on Computer Vision , 2015, pp. 1116–1124.\n[10] W. Li, R. Zhao, T. Xiao, and X. Wang, “Deepreid: Deep filter pairing\nneural network for person re-identification,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2014, pp. 152–\n159.\n[11] Z. Zheng, L. Zheng, and Y . Yang, “Unlabeled samples generated by gan\nimprove the person re-identification baseline in vitro,” in Proceedings\nof the IEEE International Conference on Computer Vision , 2017, pp.\n3754–3762.\n[12] L. Wei, S. Zhang, W. Gao, and Q. Tian, “Person transfer gan to bridge\ndomain gap for person re-identification,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2018, pp. 79–\n88.\n[13] W. Chen, X. Chen, J. Zhang, and K. Huang, “A multi-task deep\nnetwork for person re-identification,” in Thirty-First AAAI Conference\non Artificial Intelligence , 2017.\n[14] W. Li, X. Zhu, and S. Gong, “Person re-identification by deep joint\nlearning of multi-loss classification,” in Proceedings of the 26th Inter-\nnational Joint Conference on Artificial Intelligence. AAAI Press, 2017,\npp. 2194–2200.\n[15] X. Qian, Y . Fu, Y .-G. Jiang, T. Xiang, and X. Xue, “Multi-scale deep\nlearning architectures for person re-identification,” in Proceedings of the\nIEEE International Conference on Computer Vision , 2017, pp. 5399–\n5408.\n[16] F. Wang, W. Zuo, L. Lin, D. Zhang, and L. Zhang, “Joint learn-\ning of single-image and cross-image representations for person re-\nidentification,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2016, pp. 1288–1296.\n[17] T. Xiao, H. Li, W. Ouyang, and X. Wang, “Learning deep feature\nrepresentations with domain guided dropout for person re-identification,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 1249–1258.\n[18] Y .-C. Chen, X. Zhu, W.-S. Zheng, and J.-H. Lai, “Person re-identification\nby camera correlation aware feature augmentation,” IEEE transactions\non pattern analysis and machine intelligence , vol. 40, no. 2, pp. 392–\n408, 2017.\n[19] M. Koestinger, M. Hirzer, P. Wohlhart, P. M. Roth, and H. Bischof,\n“Large scale metric learning from equivalence constraints,” in 2012\nIEEE conference on computer vision and pattern recognition . IEEE,\n2012, pp. 2288–2295.\n[20] S. Liao, Y . Hu, X. Zhu, and S. Z. Li, “Person re-identification by local\nmaximal occurrence representation and metric learning,” in Proceedings\n10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\nof the IEEE conference on computer vision and pattern recognition ,\n2015, pp. 2197–2206.\n[21] F. Xiong, M. Gou, O. Camps, and M. Sznaier, “Person re-identification\nusing kernel-based metric learning methods,” in European conference\non computer vision . Springer, 2014, pp. 1–16.\n[22] L. Zhang, T. Xiang, and S. Gong, “Learning a discriminative null space\nfor person re-identification,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , 2016, pp. 1239–1248.\n[23] W.-S. Zheng, S. Gong, and T. Xiang, “Reidentification by relative dis-\ntance comparison,” IEEE transactions on pattern analysis and machine\nintelligence, vol. 35, no. 3, pp. 653–668, 2012.\n[24] D. Li, X. Chen, Z. Zhang, and K. Huang, “Learning deep context-\naware features over body and latent parts for person re-identification,”\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2017, pp. 384–393.\n[25] H. Yao, S. Zhang, R. Hong, Y . Zhang, C. Xu, and Q. Tian, “Deep\nrepresentation learning with part loss for person re-identification,” IEEE\nTransactions on Image Processing, vol. 28, no. 6, pp. 2860–2871, 2019.\n[26] L. Zhao, X. Li, Y . Zhuang, and J. Wang, “Deeply-learned part-aligned\nrepresentations for person re-identification,” in Proceedings of the IEEE\nInternational Conference on Computer Vision , 2017, pp. 3219–3228.\n[27] M. Jaderberg, K. Simonyan, A. Zisserman et al. , “Spatial transformer\nnetworks,” in Advances in neural information processing systems , 2015,\npp. 2017–2025.\n[28] H. Huang, W. Yang, J. Lin, G. Huang, J. Xu, G. Wang, X. Chen,\nand K. Huang, “Improve person re-identification with part awareness\nlearning,” IEEE Transactions on Image Processing , vol. 29, pp. 7468–\n7481, 2020.\n[29] K. Wang, C. Ding, S. J. Maybank, and D. Tao, “Cdpm: Convolutional de-\nformable part models for semantically aligned person re-identification,”\nIEEE Transactions on Image Processing, vol. 29, pp. 3416–3428, 2019.\n[30] Z. Zhang, C. Lan, W. Zeng, and Z. Chen, “Densely semantically aligned\nperson re-identification,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 667–676.\n[31] B. Chen, W. Deng, and J. Hu, “Mixed high-order attention network\nfor person re-identification,” in Proceedings of the IEEE International\nConference on Computer Vision , 2019.\n[32] W. Li, X. Zhu, and S. Gong, “Harmonious attention network for person\nre-identification,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2018, pp. 2285–2294.\n[33] J. Si, H. Zhang, C.-G. Li, J. Kuen, X. Kong, A. C. Kot, and G. Wang,\n“Dual attention matching network for context-aware feature sequence\nbased person re-identification,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2018, pp. 5363–5372.\n[34] C. Wang, Q. Zhang, C. Huang, W. Liu, and X. Wang, “Mancs: A\nmulti-task attentional network with curriculum sampling for person re-\nidentification,” in Proceedings of the European Conference on Computer\nVision (ECCV), 2018, pp. 365–381.\n[35] W. Yang, H. Huang, Z. Zhang, X. Chen, K. Huang, and S. Zhang,\n“Towards rich feature discovery with class activation maps augmentation\nfor person re-identification,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 1389–1398.\n[36] M. Zheng, S. Karanam, Z. Wu, and R. J. Radke, “Re-identification\nwith consistent attentive siamese networks,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2019, pp.\n5735–5744.\n[37] H. Zhu, W. Ke, D. Li, J. Liu, L. Tian, and Y . Shan, “Dual cross-\nattention learning for fine-grained visual categorization and object re-\nidentification,” in CVPR, 2022.\n[38] Y . Li, J. He, T. Zhang, X. Liu, Y . Zhang, and F. Wu, “Diverse part dis-\ncovery: Occluded person re-identification with part-aware transformer,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, pp. 2898–2907.\n[39] G. Zhang, P. Zhang, Q. Jinqing, and h. Lu, “Hat: Hierarchical aggrega-\ntion transformers for person re-identification,” in 2021 ACM Multimedia\nConference on Multimedia Conference . ACM, 2021.\n[40] Xu, Jiarui and De Mello, Shalini and Liu, Sifei and Byeon, Wonmin\nand Breuel, Thomas and Kautz, Jan and Wang, Xiaolong, “Groupvit:\nSemantic segmentation emerges from text supervision,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 2022, pp. 18 134–18 144.\n[41] Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire\nand Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam, “Vi-\nsual prompt tuning,” in Computer Vision–ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part\nXXXIII. Springer, 2022, pp. 709–727.\n[42] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” arXiv\npreprint arXiv:1606.08415, 2016.\n[43] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n[44] Y . M. Asano, C. Rupprecht, and A. Vedaldi, “Self-labelling via\nsimultaneous clustering and representation learning,” arXiv preprint\narXiv:1911.05371, 2019.\n[45] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin,\n“Unsupervised learning of visual features by contrasting cluster assign-\nments,” arXiv preprint arXiv:2006.09882 , 2020.\n[46] H. Luo, Y . Gu, X. Liao, S. Lai, and W. Jiang, “Bag of tricks and a strong\nbaseline for deep person re-identification,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition Workshops ,\n2019, pp. 0–0.\n[47] A. Hermans, L. Beyer, and B. Leibe, “In defense of the triplet loss for\nperson re-identification,” arXiv preprint arXiv:1703.07737 , 2017.\n[48] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking\nthe inception architecture for computer vision,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2016, pp.\n2818–2826.\n[49] Y . Wang, L. Wang, Y . You, X. Zou, V . Chen, S. Li, G. Huang, B. Har-\niharan, and K. Q. Weinberger, “Resource aware person re-identification\nacross multiple resolutions,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2018, pp. 8042–8051.\n[50] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in 2009 IEEE conference on\ncomputer vision and pattern recognition . Ieee, 2009, pp. 248–255.\n[51] X. Zhang, H. Luo, X. Fan, W. Xiang, Y . Sun, Q. Xiao, W. Jiang,\nC. Zhang, and J. Sun, “Alignedreid: Surpassing human-level perfor-\nmance in person re-identification,” arXiv preprint arXiv:1711.08184v2 ,\n2018.\n[52] Y . Suh, J. Wang, S. Tang, T. Mei, and K. Mu Lee, “Part-aligned\nbilinear representations for person re-identification,” in Proceedings of\nthe European Conference on Computer Vision (ECCV) , 2018, pp. 402–\n419.\n[53] C.-P. Tay, S. Roy, and K.-H. Yap, “Aanet: Attribute attention network for\nperson re-identifications,” in The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , June 2019.\n[54] J. Guo, Y . Yuan, L. Huang, C. Zhang, J.-G. Yao, and K. Han, “Be-\nyond human parts: Dual part-aligned representations for person re-\nidentification,” in The IEEE International Conference on Computer\nVision (ICCV), October 2019.\n[55] J. Miao, Y . Wu, P. Liu, Y . Ding, and Y . Yang, “Pose-guided feature align-\nment for occluded person re-identification,” in The IEEE International\nConference on Computer Vision (ICCV) , October 2019.\n[56] L. He and W. Liu, “Guided saliency feature learning for person re-\nidentification in crowded scenes,” in European Conference on Computer\nVision, 2020.\n[57] Miao, Jiaxu and Wu, Yu and Yang, Yi, “Identifying Visible Parts via Pose\nEstimation for Occluded Person Re-Identification,” IEEE Transactions\non Neural Networks and Learning Systems , 2021.\n[58] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\nworks,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 7794–7803.\n[59] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, and X. Chen, “Interaction-\nand-aggregation network for person re-identification,” in The IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , June\n2019.\n[60] P. Fang, J. Zhou, S. K. Roy, L. Petersson, and M. Harandi, “Bilinear\nattention networks for person retrieval,” inProceedings of the IEEE/CVF\nInternational Conference on Computer Vision , 2019, pp. 8030–8039.\n[61] Z. Zheng, X. Yang, Z. Yu, L. Zheng, Y . Yang, and J. Kautz, “Joint\ndiscriminative and generative learning for person re-identification,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 2138–2147.\n[62] K. Zhou, Y . Yang, A. Cavallaro, and T. Xiang, “Omni-scale feature\nlearning for person re-identification,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , 2019, pp. 3702–3712.\n[63] W. Song, S. Li, T. Chang, A. Hao, Q. Zhao, and H. Qin, “Context-\ninteractive cnn for person re-identification,”IEEE Transactions on Image\nProcessing, vol. 29, pp. 2860–2874, 2019.\n[64] W. Wang, W. Pei, Q. Cao, S. Liu, G. Lu, and Y .-W. Tai, “Push for\ncenter learning via orthogonalization and subspace masking for person\nre-identification,” IEEE Transactions on Image Processing , vol. 30, pp.\n907–920, 2020.\nZHU et al.: AAFORMER: AUTO-ALIGNED TRANSFORMER FOR PERSON RE-IDENTIFICATION 11\n[65] Y . Zhong, Y . Wang, and S. Zhang, “Progressive feature enhancement\nfor person re-identification,” IEEE Transactions on Image Processing ,\nvol. 30, pp. 8384–8395, 2021.\n[66] Y . Liu, W. Zhou, J. Liu, G.-J. Qi, Q. Tian, and H. Li, “An end-\nto-end foreground-aware network for person re-identification,” IEEE\nTransactions on Image Processing , vol. 30, pp. 2060–2071, 2021.\n[67] P. Wang, Z. Zhao, F. Su, X. Zu, and N. V . Boulgouris, “Horeid:\nDeep high-order mapping enhances pose alignment for person re-\nidentification,” IEEE Transactions on Image Processing , vol. 30, pp.\n2908–2922, 2021.\n[68] Q. Zhou, B. Zhong, X. Liu, and R. Ji, “Attention-based neural architec-\nture search for person re-identification,” IEEE Transactions on Neural\nNetworks and Learning Systems , pp. 1–13, 2021.\n[69] H. Tan, X. Liu, B. Yin, and X. Li, “Mhsa-net: Multihead self-attention\nnetwork for occluded person re-identification,” IEEE Transactions on\nNeural Networks and Learning Systems , pp. 1–15, 2022.\n[70] Z. Zhong, L. Zheng, D. Cao, and S. Li, “Re-ranking person re-\nidentification with k-reciprocal encoding,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2017, pp.\n1318–1327.\n[71] Y . Suh, J. Wang, S. Tang, T. Mei, and K. Mu Lee, “Part-aligned\nbilinear representations for person re-identification,” in Proceedings of\nthe European Conference on Computer Vision (ECCV) , 2018, pp. 402–\n419.\n[72] Y . Ge, Z. Li, H. Zhao, G. Yin, S. Yi, X. Wang et al. , “Fd-gan: Pose-\nguided feature distilling gan for robust person re-identification,” in\nAdvances in neural information processing systems , 2018, pp. 1222–\n1233.\n[73] L. He, J. Liang, H. Li, and Z. Sun, “Deep spatial feature reconstruc-\ntion for partial person re-identification: Alignment-free approach,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 7073–7082.\n[74] L. He, Z. Sun, Y . Zhu, and Y . Wang, “Recognizing partial biometric\npatterns,” arXiv preprint arXiv:1810.07399 , 2018.\nKuan Zhu received the B.E. degree from Xiamen\nUniversity, Xiamen, China, in 2018, and the Ph.D.\ndegree in pattern recognition and intelligence sys-\ntems from the National Laboratory of Pattern Recog-\nnition, Institute of Automation, Chinese Academy\nof Sciences, in 2023. He is currently an Assistant\nProfessor with the Institute of Automation, Chinese\nAcademy of Sciences. His current research interests\ninclude self-supervised learning, open-world visual\nrecognition, large language model and multimodal\nmodel.\nMultimodal model.\nHaiyun Guo received the B.E. degree from Wuhan\nUniversity in 2013 and the Ph.D. degree in pattern\nrecognition and intelligence systems from the Insti-\ntute of Automation, University of Chinese Academy\nof Sciences, in 2018. She is currently an Associate\nResearcher with the Institute of Automation, Chinese\nAcademy of sciences. Her current research interests\ninclude pattern recognition and machine learning,\nimage and video processing, and intelligent video\nsurveillance.\nShiliang Zhang received the Ph.D. degree in com-\nputer science from the Institute of Computing Tech-\nnology, Chinese Academy of Sciences. He was\na Post-Doctoral Scientist with NEC Laboratories\nAmerica and a Post-Doctoral Research Fellow with\nThe University of Texas at San Antonio. He is cur-\nrently an Associate Professor with Tenure with the\nDepartment of Computer Science, School of Elec-\ntronic Engineering and Computer Science, Peking\nUniversity.\nHis research interests include large-scale image retrieval and computer\nvision. He has authored or co-authored over 100 papers in journals and\nconferences, including IJCV , IEEE Trans. on PAMI, IEEE Trans. on Image\nProcessing, IEEE Trans. on NNLS, IEEE Trans. on Multimedia, ACM Multi-\nmedia, ICCV , CVPR, ECCV , NeurIPS, AAAI, IJCAI, etc. He was a recipient\nof the Outstanding Doctoral Dissertation Awards from the Chinese Academy\nof Sciences and Chinese Computer Federation, the President Scholarship\nfrom the Chinese Academy of Sciences, the NEC Laboratories America Spot\nRecognition Award, the NVidia Pioneering Research Award, and the Microsoft\nResearch Fellowship. He served as the Associate Editor (AE) of Computer\nVision and Image Understanding (CVIU) and IET Computer Vision, Guest\nEditor of ACM TOMM, and Area Chair of CVPR, AAAI, ICPR, and VCIP.\nHis research is supported by the The National Key Research and Development\nProgram of China, Natural Science Foundation of China, Beijing Natural\nScience Foundation, and Microsoft Research, etc.\nYaowei Wangreceived his Ph.D. degree in computer\nscience from the University of Chinese Academy of\nSciences in 2005. He is currently a professor with\nthe Peng Cheng Laboratory, Shenzhen, China. He is\nthe author or co-author of more than 120 technical\narticles in international journals and conferences,\nincluding TOMM, ACM MM, IEEE TIP, CVPR,\nICCV , and IJCAI. His current research interests\ninclude multimedia content analysis and understand-\ning, machine learning, and computer vision. He\nserves as the chair of the IEEE Digital Retina\nSystems Working Group and a member of IEEE, CIE, CCF, CSIG. He was the\nrecipient of the second prize of the National Technology Invention in 2017,\nthe first prize of the CIE Technology Invention in 2015, and the first prize of\nthe CIE Scientific and Technological Progress in 2022.\nJing Liu received the B.E. and M.S. degrees from\nShandong University, Shandong, in 2001 and 2004,\nrespectively, and the Ph.D. degree from the Insti-\ntute of Automation, Chinese Academy of Sciences,\nBeijing, in 2008. She is currently a Professor with\nthe Institute of Automation, Chinese Academy of\nSciences. Her current research interests include deep\nlearning, image content analysis and classification,\nand multimedia understanding and retrieval.\nJinqiao Wang received the B.E. degree from the\nHebei University of Technology, China, in 2001, the\nM.S. degree from Tianjin University, China, in 2004,\nand the Ph.D. degree in pattern recognition and\nintelligence systems from the National Laboratory of\nPattern Recognition, Chinese Academy of Sciences,\nin 2008. He is currently a Professor with the Institute\nof Automation, Chinese Academy of Sciences. His\nresearch interests include pattern recognition and\nmachine learning, large multimodal model, Image\nand video analysis, object detection and recognition.\n12 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\nMing Tang received the B.S. degree in computer\nscience and engineering and M.S. degree in artificial\nintelligence from Zhejiang University, Hangzhou,\nChina, in 1984 and 1987, respectively, and the Ph.D.\ndegree in pattern recognition and intelligent system\nfrom the Chinese Academy of Sciences, Beijing,\nChina, in 2002. He is currently a Professor with\nthe Institute of Automation, Chinese Academy of\nSciences. His current research interests include com-\nputer vision and machine learning.",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8150501251220703
    },
    {
      "name": "Computer science",
      "score": 0.7613583207130432
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6235425472259521
    },
    {
      "name": "Parsing",
      "score": 0.5332156419754028
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.46263569593429565
    },
    {
      "name": "Representation (politics)",
      "score": 0.44037994742393494
    },
    {
      "name": "Architecture",
      "score": 0.4394872784614563
    },
    {
      "name": "Knapsack problem",
      "score": 0.4103168845176697
    },
    {
      "name": "Computer vision",
      "score": 0.3865223824977875
    },
    {
      "name": "Algorithm",
      "score": 0.16849175095558167
    },
    {
      "name": "Engineering",
      "score": 0.09295207262039185
    },
    {
      "name": "Voltage",
      "score": 0.09068551659584045
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "topic": "Transformer",
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210112150",
      "name": "Institute of Automation",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    }
  ]
}