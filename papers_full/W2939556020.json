{
  "title": "Visualizing Attention in Transformer-Based Language Representation Models",
  "url": "https://openalex.org/W2939556020",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227146672",
      "name": "Vig, Jesse",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2118463056",
    "https://openalex.org/W2799118206",
    "https://openalex.org/W3128232076",
    "https://openalex.org/W2962958286",
    "https://openalex.org/W2610881169",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W2963503967",
    "https://openalex.org/W2760327630",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2906152891"
  ],
  "abstract": "We present an open-source tool for visualizing multi-head self-attention in Transformer-based language representation models. The tool extends earlier work by visualizing attention at three levels of granularity: the attention-head level, the model level, and the neuron level. We describe how each of these views can help to interpret the model, and we demonstrate the tool on the BERT model and the OpenAI GPT-2 model. We also present three use cases for analyzing GPT-2: detecting model bias, identifying recurring patterns, and linking neurons to model behavior.",
  "full_text": "Visualizing Attention in Transformer-Based\nLanguage Representation Models\nJesse Vig\nPalo Alto Research Center\n3333 Coyote Hill Road\nPalo Alto, CA 94304\njesse.vig@parc.com\nAbstract\nWe present an open-source tool for visualiz-\ning multi-head self-attention in Transformer-\nbased language representation models. The\ntool extends earlier work by visualizing at-\ntention at three levels of granularity: the\nattention-head level, the model level, and the\nneuron level. We describe how each of these\nviews can help to interpret the model, and we\ndemonstrate the tool on the BERT model and\nthe OpenAI GPT-2 model. We also present\nthree use cases for analyzing GPT-2: detect-\ning model bias, identifying recurring patterns,\nand linking neurons to model behavior.\n1 Introduction\nIn 2018, the BERT (Bidirectional Encoder Rep-\nresentations from Transformers) language repre-\nsentation model achieved state-of-the-art perfor-\nmance across NLP tasks ranging from sentiment\nanalysis to question answering (Devlin et al.,\n2018). Recently, the OpenAI GPT-2 (Generative\nPretrained Transformer-2) model achieved state-\nof-the-art results across several language model-\ning benchmarks in a zero-shot setting (Radford\net al., 2019).\nUnderlying BERT and GPT-2 is the Trans-\nformer model, which uses a multi-head self-\nattention architecture (Vaswani et al., 2017a). An\nadvantage of using attention is that it can help in-\nterpret a model’s decisions by showing how the\nmodel attends to different parts of the input (Bah-\ndanau et al., 2015; Belinkov and Glass, 2019).\nVarious tools have been developed to visualize at-\ntention in NLP models, ranging from attention-\nmatrix heatmaps (Bahdanau et al., 2015; Rush\net al., 2015; Rockt ¨aschel et al., 2016) to bipar-\ntite graph representations (Liu et al., 2018; Lee\net al., 2017; Strobelt et al., 2018). A visualization\ntool designed speciﬁcally for the multi-head self-\nattention in the Transformer (Jones, 2017) was in-\ntroduced in (Vaswani et al., 2017b) and released\nin the Tensor2Tensor repository (Vaswani et al.,\n2018).\nIn this paper, we introduce a tool for visualizing\nattention in Transformer-based language represen-\ntation models, building on the work of (Jones,\n2017). We extend the existing tool in two ways:\n(1) we adapt it from the original encoder-decoder\nimplementation to the decoder-only GPT-2 model\nand the encoder-only BERT model, and (2) we add\ntwo visualizations: the model view, which visual-\nizes all of the layers and attention heads in a single\ninterface, and the neuron view, which shows how\nindividual neurons inﬂuence attention scores.\n2 Visualization Tool\nWe present an open-source 1 tool for visualiz-\ning multi-head self-attention in Transformer-based\nlanguage representation models. The tool com-\nprises three views: an attention-head view, a\nmodel view, and a neuron view. Below, we\ndescribe these views and demonstrate them on\nthe OpenAI GPT-2 and BERT models. We also\npresent three use cases for GPT-2 showing how\nthe tool might provide insights on how to adjust\nor improve the model. A video demonstration of\nthe tool is available at https://youtu.be/\n187JyiA4pyk.\n2.1 Attention-head view\nThe attention-head view visualizes the attention\npatterns produced by one or more attention heads\nin a given transformer layer, as shown in Figure 1\n(GPT-22) and Figure 2 (BERT3). In this view, self-\nattention is represented as lines connecting the to-\nkens that are attending (left) with the tokens being\n1https://github.com/jessevig/bertviz\n2GPT-2 smallpretrained model.\n3BERTBASE pretrained model.\narXiv:1904.02679v2  [cs.HC]  11 Apr 2019\nFigure 1: Attention head view for GPT-2, for the input text The quick, brown fox jumps over the lazy. The left and\ncenter ﬁgures represent different layers / attention heads. The right ﬁgure depicts the same layer/head as the center\nﬁgure, but with the token lazy selected.\nFigure 2: Attention head view for BERT, for inputs the cat sat on the mat(Sentence A) and the cat lay on the rug\n(Sentence B). The left and center ﬁgures represent different layers / attention heads. The right ﬁgure depicts the\nsame layer/head as the center ﬁgure, but with Sentence A→Sentence Bﬁlter selected.\nattended to (right). Colors identify the correspond-\ning attention head(s), while line weight reﬂects the\nattention score.\nAt the top of the screen, the user can select the\nlayer and one or more attention heads (represented\nby the colored patches). Users may also ﬁlter at-\ntention by token, as shown in Figure 1 (right); in\nthis case the target tokens are also highlighted and\nshaded based on attention strength. For BERT,\nwhich uses an explicit sentence-pair model, users\nmay specify a sentence-level attention ﬁlter; for\nexample, in Figure 2 (right), the user has selected\nthe Sentence A →Sentence B ﬁlter, which only\nshows attention from tokens in Sentence A to to-\nkens in Sentence B.\nSince the attention heads do not share param-\neters, each head can produce a distinct attention\npattern. In the attention head shown in Figure 1\n(left), for example, each word attends exclusively\nto the previous word in the sequence. The head\nin Figure 1 (center), in contrast, generates atten-\ntion that is dispersed fairly evenly across previous\nwords in the sentence (excluding the ﬁrst word).\nFigure 2 shows attention heads for BERT that\ncapture sentence-pair patterns, including a within-\nsentence attention pattern (left) and a between-\nsentence pattern (center).\nBesides these coarse positional patterns, at-\ntention heads may capture speciﬁc lexical pat-\nterns such as those shown in Figure 3. Other\nattention heads detected named entities (people,\nplaces, companies), beginning/ending punctuation\n(quotes, brackets, parentheses), subject-verb pairs,\nand a variety of other linguistic properties.\nThe attention-head view closely follows the\noriginal implementation from (Jones, 2017). The\nkey difference is that the original tool was de-\nveloped for encoder-decoder models, while the\npresent tool has been adapted to the encoder-only\nBERT and decoder-only GPT-2 models.\nFigure 3: Examples of attention heads in GPT-2 that capture speciﬁc lexical patterns: list items (left); prepositions\n(center); and acronyms (right). Similar patterns were observed in these attention heads for other inputs. (Attention\ndirected toward ﬁrst token is likely null attention, as discussed later.)\nFigure 4: Attention pattern in GPT-2 related to coreference resolution suggests the model may encode gender bias.\nUse Case: Detecting Model Bias\nOne use case for the attention-head view is detect-\ning bias in the model. Consider the following two\ncases of conditional text generation using GPT-2\n(generated text underlined), where the two input\nprompts differ only in the gender of the pronoun\nthat begins the second sentence4:\n•The doctor asked the nurse a question. She\nsaid, “I’m not sure what you’re talking about.”\n•The doctor asked the nurse a question. He\nasked her if she ever had a heart attack.\nIn the ﬁrst example, the model generates a con-\ntinuation that implies She refers to nurse. In the\n4Generated from GPT-2 small model, using greedy top-1\ndecoding algorithm\nsecond example, the model generates text that im-\nplies He refers to doctor. This suggests that the\nmodel’s coreference mechanism may encode gen-\nder bias (Zhao et al., 2018; Lu et al., 2018). To\nbetter understand the source of this bias, we can\nvisualize the attention head that produces patterns\nresembling coreference resolution, shown in Fig-\nure 4. The two examples from above are shown\nin Figure 4 (right), which reveals that the token\nShe strongly attends to nurse, while the token He\nattends more to doctor. This result suggests that\nthe model is heavily inﬂuenced by its perception\nof gender associated with words.\nBy identifying a potential source of model bias,\nthe tool can help to guide efforts to provide solu-\ntions to the issue. For example, if one were able\nto identify the neurons that encoded gender in this\nattention head, one could potentially manipulate\nthose neurons to control for the bias (Bau et al.,\n2019).\n2.2 Model View\nThe model view (Figure 5) provides a birds-eye\nview of attention across all of the model’s lay-\ners and heads for a particular input. Attention\nheads are presented in tabular form, with rows rep-\nresenting layers and columns representing heads.\nEach layer/head is visualized in a thumbnail form\nthat conveys the coarse shape of the attention pat-\ntern, following the small multiplesdesign pattern\n(Tufte, 1990). Users may also click on any head to\nenlarge it and see the tokens.\nFigure 5: Model view of GPT-2, for input text The\nquick, brown fox jumps over the lazy(excludes layers\n6-11 and heads 6-11).\nThe model view enables users to browse the at-\ntention heads across all layers in the model and\nsee how attention patterns evolve throughout the\nmodel. For example, one can see that many atten-\ntion heads in the initial layers tend to be position-\nbased, e.g. focusing on the same token (layer 0,\nhead 1) or focusing on the previous token (layer 2,\nhead 2).\nUse Case: Identifying Recurring Patterns\nThe model view in Figure 5 shows that many of\nthe attention heads follow the same pattern: they\nfocus all of the attention on the ﬁrst token in the\nsequence. This appears to be a type of null pat-\ntern that is produced when the linguistic property\ncaptured by the attention head doesn’t appear in\nthe input text. One possible conclusion from this\nresult is that the model may beneﬁt from a ded-\nicated null position to receive this type of atten-\ntion. While it’s not clear that this change would\nimprove model performance, it would make the\nmodel more interpretable by disentangling the null\nattention from attention related to the ﬁrst token.\n2.3 Neuron View\nThe neuron view (Figure 6) visualizes the indi-\nvidual neurons in the query and key vectors and\nshows how they are used to compute attention.\nGiven a token selected by the user (left), this view\ntraces the computation of attention from that token\nto the other tokens in the sequence (right). The\ncomputation is visualized from left to right with\nthe following columns:\n•Query q: The 64-element query vector of the\ntoken paying attention. Only the query vector\nof the selected token is used in the computa-\ntions.\n•Key k: The 64-element key vector of each\ntoken receiving attention.\n•q ×k (element-wise): The element-wise\nproduct of the selected token’s query vector\nand each key vector.\n•q ·k: The dot product of the selected token’s\nquery vector and each key vector.\n•Softmax: The softmax of the scaled dot-\nproduct from previous column. This equals\nthe attention received by the corresponding\ntoken.\nPositive and negative values are colored blue\nand orange, respectively, with color saturation\nbased on the magnitude of the value. As with\nthe attention-head view, the connecting lines are\nweighted based on attention between the words.\nThe element-wise product of the vectors is in-\ncluded to show how individual neurons contribute\nto the dot product and hence attention.\nUse Case: Linking Neurons to Model Behavior\nTo see how the neuron view might provide action-\nable insights, consider the attention head in Figure\n7. For this head, the attention (rightmost column)\nappears to decay with increasing distance from the\nsource token5. This pattern resembles a context\n5with the exception of the ﬁrst token, which acts as a null\ntoken, as discussed earlier.\nFigure 6: Neuron view of GPT-2 for layer 8 / head 6. Positive and negative values are colored blue and orange\nrespectively, with color saturation reﬂecting magnitude. This is the same attention head depicted in Figure 3 (left).\nwindow, but instead of having a ﬁxed cutoff, the\nattention decays continuously with distance.\nThe neuron view provides two key insights\nabout this attention head. First, the attention\nscores appear to be largely independent of the con-\ntent of the input text, based on the fact that all the\nquery vectors have very similar values (except for\nthe ﬁrst token). The second observation is that\na small number of neuron positions (highlighted\nwith blue arrows) appear to be mostly responsi-\nble for this distance-decaying attention pattern. At\nthese neuron positions, the element-wise product\nq×k decreases as the distance from the source to-\nken increases (either becoming darker orange or\nlighter blue).\nWhen speciﬁc neurons are linked to a tan-\ngible outcome—in this the case decay rate of\nattention—it presents an opportunity for human\nintervention in the model. By altering the val-\nues of the relevant neurons (Bau et al., 2019), one\ncould control the rate at which attention decays for\nthis attention head. This capability might be use-\nFigure 7: Neuron view of GPT-2 for layer 1 / head 10 (same one depicted in Figure 1, center) with last token\nselected. Blue arrows mark positions in the element-wise products where values decrease with increasing distance\nfrom the source token (becoming darker orange or lighter blue).\nful when processing or generating texts of vary-\ning complexity; for example, one might prefer a\nslower decay rate (longer context window) for a\nscientiﬁc text and a faster decay rate (shorter con-\ntext window) for content intended for children.\n3 Conclusion\nIn this paper, we presented a tool for visualizing\nattention in Transformer-based language represen-\ntation models. We demonstrated the tool on the\nOpenAI GPT-2 and BERT models and presented\nthree use cases for analyzing GPT-2. For future\nwork, we would like to evaluate empirically how\nattention impacts model predictions across a range\nof tasks (Jain and Wallace, 2019). Further, we\nwould like to integrate the three views into a uni-\nﬁed interface, and visualize the value vectors in\naddition to the queries and keys. Finally, we would\nlike to enable users to manipulate the model, either\nby modifying attention (Lee et al., 2017; Liu et al.,\n2018; Strobelt et al., 2018) or editing individual\nneurons (Bau et al., 2019).\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In ICLR.\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir\nDurrani, Fahim Dalvi, and James Glass. 2019. Iden-\ntifying and controlling important neurons in neural\nmachine translation. In ICLR.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics (TACL) (to appear).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. ArXiv Computation and Language.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot explanation. CoRR, abs/1902.10186.\nLlion Jones. 2017. Tensor2tensor transformer\nvisualization. https://github.com/\ntensorflow/tensor2tensor/tree/\nmaster/tensor2tensor/visualization.\nJaesong Lee, Joong-Hwi Shin, and Jun-Seok Kim.\n2017. Interactive visualization and manipulation\nof attention-based neural machine translation. In\nEMNLP: System Demonstrations.\nShusen Liu, Tao Li, Zhimin Li, Vivek Srikumar, Vale-\nrio Pascucci, and Peer-Timo Bremer. 2018. Visual\ninterrogation of attention-based models for natural\nlanguage inference and machine comprehension. In\nEMNLP: System Demonstrations.\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-\ncharla, and Anupam Datta. 2018. Gender bias\nin neural natural language processing. CoRR,\nabs/1807.11714.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report.\nTim Rockt ¨aschel, Edward Grefenstette, Karl Moritz\nHermann, Tomas Kocisky, and Phil Blunsom. 2016.\nReasoning about entailment with neural attention.\nIn ICLR.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In EMNLP.\nH. Strobelt, S. Gehrmann, M. Behrisch, A. Perer,\nH. Pﬁster, and A. M. Rush. 2018. Seq2Seq-Vis:\nA Visual Debugging Tool for Sequence-to-Sequence\nModels. ArXiv e-prints.\nEdward Tufte. 1990. Envisioning Information. Graph-\nics Press, Cheshire, CT, USA.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan N. Gomez, Stephan Gouws,\nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki\nParmar, Ryan Sepassi, Noam Shazeer, and Jakob\nUszkoreit. 2018. Tensor2tensor for neural machine\ntranslation. CoRR, abs/1803.07416.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017a. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017b. Attention is all\nyou need. Technical report.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias\nin coreference resolution: Evaluation and debias-\ning methods. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7856195569038391
    },
    {
      "name": "Granularity",
      "score": 0.7761848568916321
    },
    {
      "name": "Computer science",
      "score": 0.7354907989501953
    },
    {
      "name": "Language model",
      "score": 0.5995975732803345
    },
    {
      "name": "Representation (politics)",
      "score": 0.570765495300293
    },
    {
      "name": "Intermediate language",
      "score": 0.44278284907341003
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4286070466041565
    },
    {
      "name": "Natural language processing",
      "score": 0.4034477472305298
    },
    {
      "name": "Machine learning",
      "score": 0.34784403443336487
    },
    {
      "name": "Programming language",
      "score": 0.13082072138786316
    },
    {
      "name": "Engineering",
      "score": 0.11334139108657837
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Compiler",
      "score": 0.0
    }
  ],
  "institutions": []
}