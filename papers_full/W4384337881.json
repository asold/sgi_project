{
    "title": "Evaluating the Performance of Large Language Models on a Neurology Board-Style Examination",
    "url": "https://openalex.org/W4384337881",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4288951127",
            "name": "Marc Cicero Schubert",
            "affiliations": [
                "German Cancer Research Center",
                "National Center for Tumor Diseases",
                "Heidelberg University",
                "University Hospital Heidelberg"
            ]
        },
        {
            "id": "https://openalex.org/A1992121173",
            "name": "Wolfgang Wick",
            "affiliations": [
                "German Cancer Research Center",
                "National Center for Tumor Diseases",
                "Heidelberg University",
                "University Hospital Heidelberg"
            ]
        },
        {
            "id": "https://openalex.org/A2117774879",
            "name": "Varun Venkataramani",
            "affiliations": [
                "Heidelberg University",
                "University Hospital Heidelberg",
                "German Cancer Research Center",
                "National Center for Tumor Diseases"
            ]
        },
        {
            "id": "https://openalex.org/A4288951127",
            "name": "Marc Cicero Schubert",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1992121173",
            "name": "Wolfgang Wick",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117774879",
            "name": "Varun Venkataramani",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4280500298",
        "https://openalex.org/W3041796653",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W4384120659",
        "https://openalex.org/W4361295009",
        "https://openalex.org/W1580749063",
        "https://openalex.org/W4377121468",
        "https://openalex.org/W4378509375",
        "https://openalex.org/W4225576545",
        "https://openalex.org/W2060300932",
        "https://openalex.org/W1185378561",
        "https://openalex.org/W4378510404",
        "https://openalex.org/W4318591734",
        "https://openalex.org/W4376311951",
        "https://openalex.org/W4286233477",
        "https://openalex.org/W3088056511",
        "https://openalex.org/W2962735233",
        "https://openalex.org/W3153712677",
        "https://openalex.org/W3213380581",
        "https://openalex.org/W4321351832",
        "https://openalex.org/W3142164167",
        "https://openalex.org/W2900298334"
    ],
    "abstract": "Summary Background and Objectives Recent advancements in large language models (LLMs) such as GPT-3.5 and GPT-4 have shown impressive potential in a wide array of applications, including healthcare. While GPT-3.5 and GPT-4 showed heterogeneous results across specialized medical board examinations, the performance of these models in neurology board exams remains unexplored. Methods An exploratory, prospective study was conducted between May 17 and May 31, 2023. The evaluation utilized a question bank approved by the American Board of Psychiatry and Neurology, designed as part of a self-assessment program. Questions were presented in a single best answer, multiple-choice format. The results from the question bank were validated with a small question cohort by the European Board for Neurology. All questions were categorized into lower-order (recall, understanding) and higher-order (apply, analyze, synthesize) questions. The performance of GPT-3.5 and GPT-4 was assessed in relation to overall performance, question type, and topic. In addition, the confidence level in responses and the reproducibility of correctly and incorrectly answered questions was evaluated. Univariable analysis was carried out. Chi-squared test and Bonferroni correction were used to determine performance differences based on question characteristics. To differentiate characteristics of correctly and incorrectly answered questions, a high-dimensional tSNE analysis of the question representations was performed. Results In May 2023, GPT-3.5 correctly answered 66.8 % of 1956 questions, whereas GPT-4 demonstrated a higher performance level, correctly answering 85 % of questions in congruence with near-passing and passing of the neurology board exam. GPT-4‚Äôs performance surpassed both GPT-3.5 and question bank users (mean human user score: 73.8%). An analysis of twenty-six question categories showed that GPT-4 outperformed human users in Behavioral, Cognitive and Psych-related questions and demonstrated superior performance to GPT-3.5 in six categories. Both models performed better on lower-order than higher-order questions according to Bloom Taxonomy for learning and assessment (GPT4: 790 of 893 (88.5%) vs. 872 of 1063 (82%), GPT-3.5: 639 of 893 (71.6%) vs. 667 of 1063 (62.7%)) with GPT-4 also excelling in both lower-order and higher-order questions. The use of confident language was observed consistently across both models, even when incorrect (GPT-4: 99.3%, 292 of 294 incorrect answers, GPT-3.5: 100%, 650 of 650 incorrect answers). Reproducible answers of GPT-3.5 and GPT-4 (defined as more than 75 % same output across 50 independent queries) were associated with a higher percentage of correct answers (GPT-3.5: 66 of 88 (75%), GPT-4: 78 of 96 (81.3%)) than inconsistent answers, (GPT-3.5: 5 of 13 (38.5%), GPT-4: 1 of 4 (25%)). Lastly, the high-dimensional embedding analysis of correctly and incorrectly answered questions revealed no clear differentiation into distinct clusters. Discussion Despite the absence of neurology-specific training, GPT-4 demonstrated commendable performance, whereas GPT-3.5 performed slightly below the human average question bank user. Higher-order cognitive tasks proved more challenging for both GPT-4 and GPT-3.5. Notwithstanding, GPT-4‚Äôs performance was equivalent to a passing grade for specialized neurology board exams. These findings suggest that with further refinements, LLMs like GPT-4 could play a pivotal role in applications for clinical neurology and healthcare in general.",
    "full_text": "Evaluating the Performance of Large Language Models on a Neurology \nBoard-Style Examination \nMarc Cicero Schubert, Wolfgang Wick, Varun Venkataramani \nNeurology Clinic and National Center for Tumor Diseases, University Hospital Heidelberg, \nHeidelberg, Germany (Marc Cicero Schubert, Wolfgang Wick Prof., Varun Venkataramani MD, \nPhD) \nClinical Cooperation Unit Neurooncology, German Cancer Consortium (DKTK), German Cancer \nResearch Center (DKFZ), Heidelberg, Germany \n(Marc Cicero Schubert, Wolfgang Wick Prof., Varun Venkataramani MD, PhD) \n \nCorrespondence to: \nVarun Venkataramani MD, PhD \nNeurology Clinic and National Center for Tumor Diseases, University Hospital Heidelberg, \nHeidelberg, Germany \nvarun.venkataramani@med.uni-heidelberg.de \n \nSummary \nBackground and Objectives \nRecent advancements in large language models (LLMs) such as GPT -3.5 and GPT -4 have shown \nimpressive potential in a wide array of applications, including healthcare. While GPT-3.5 and GPT-4 \nshowed heterogeneous results across specialized medical board examinations, the performance of these \nmodels in neurology board exams remains unexplored. \nMethods \nAn exploratory, prospective study was conducted between May 17 and May 31, 2023. The evaluation \nutilized a question bank approved by the American Board of Psychiatry and Neurology, designed as \npart of a self-assessment program. Questions were presented in a single best answer, multiple-choice \nformat. The results from the question bank were validated with a small question cohort by the \nEuropean Board for Neurology. All questions  were categorized into lower-order (recall, \nunderstanding) and higher-order (apply, analyze, synthesize) questions. The performance of GPT -\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n3.5 and GPT-4 was assessed in relation to overall performance, question type, and topic. In addition, \nthe confidence level in responses and the reproducibility of correctly and incorre ctly answered \nquestions was evaluated. Univariable analysis was carried out . Chi-squared test and Bonferroni \ncorrection were used to determine performance differences based on question characteristics.  To \ndifferentiate characteristics of correctly and inco rrectly answered questions, a high -dimensional \ntSNE analysis of the question representations was performed. \nResults \nIn May 2023, GPT-3.5 correctly answered 66.8 % of 1956 questions, whereas GPT-4 demonstrated a \nhigher performance level, correctly answering 85 % of questions  in congruence with near -passing \nand passing of the neurology board exam . GPT -4's performance surpassed both GPT -3.5 and \nquestion bank users (mean human user score: 73.8%). An analysis of twenty-six question categories \nshowed that GPT-4 outperformed human users in Behavioral, Cognitive and Psych-related questions \nand demonstrated superior performance to GPT-3.5 in six categories. Both models performed better \non lower -order than higher -order questions according to Bloom Taxonomy for learning and \nassessment (GPT4: 790 of 893 (88.5%) vs. 872 of 1063 (82%), GPT-3.5: 639 of 893 (71.6%) vs. 667 of \n1063 (62.7%)) with GPT-4 also excelling in both lower-order and higher-order questions. The use of \nconfident language was observed consistently across both models, even when incorrect ( GPT-4: \n99.3%, 292 of 294 incorrect answers, GPT-3.5: 100%, 650 of 650  incorrect answers). Reproducible \nanswers of GPT -3.5 and GPT-4 (defined as more than 75 % same output across 50 independent \nqueries) were associated with a higher percentage of correct answers (GPT-3.5: 66 of 88 (75%), GPT-\n4: 78 of 96 (81 .3%)) than inconsistent answers, (GPT-3.5: 5 of 13 (38.5%), GPT-4: 1 of 4  (25%)). \nLastly, the high-dimensional embedding analysis of correctly and incorrectly answered questions \nrevealed no clear differentiation into distinct clusters.  \nDiscussion \nDespite the absence of neurology-specific training, GPT-4 demonstrated commendable performance, \nwhereas GPT-3.5 performed slightly below the human average question bank user. Higher -order \ncognitive tasks proved more challenging for both GPT -4 and GPT-3.5. Notwithstanding, GPT -4's \nperformance was equivalent to a passing grade for specialized neurology board exams. These findings \nsuggest that with further refinements, LLMs like GPT-4 could play a pivotal role in applications for \nclinical neurology and healthcare in general.  \n \nIntroduction \nDeep learning algorithms have been investigated in neurology for a variety of tasks, such as neurologic \ndiagnosis, prognosis and treatment 1,2. However, the role and potential application of large language models \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \n(LLMs) in neurology have been unexplored. The recent emergence of the powerful transformer-based AI \nmodels GPT -3.5 and GPT -4 3,4 provides a new avenue for e xploring their implications in the field of \nneurology. These Large Language Models (LLMs) undergo training using expansive datasets, \nencompassing more than 45 terabytes of information. This rigorous training process equips them to \nrecognize patterns and associations among words, which, in turn, empowers them to produce responses that \nare both contextually accurate and logically consistent 5. The application of these models in specialized \nmedical examinations has been tested to some extent. GPT-3.5 showed near-pass performance in the United \nStates Medical Licensing Examination (USMLE) 6,7 while it failed to pass the ophthalmology board \nexamination 8. Two recent reports showed slightly deviating results on neurosurgery board -style exams \nwith one report claiming a near-pass with GPT-3.5 while the other showed an approximately 10 % lower \nperformance. GPT -3.5 achieved a near -pass in radiology board -like examinations while GPT -4 in \nneurosurgery board-like examinations successfully passed it  9. In contrast, neurology board -like exams \npresent a different set of challenges. As compared to  radiology, ophthalmology, or n eurosurgery, the \nquestions in neurology board examinations often present complex narratives with s ubtle diagnostic clues \nthat require a nuanced understanding of neuroanatomy, neuropathology, and neurophysiology. The \ncandidate is expected to navigate through these complex narratives, extracting relevant data , and \nsynthesizing this information into a coh erent diagnostic hypothesis and subsequent therapeutic decisions . \nWritten board examinations, designed to test a broad range of neurology topics, are common in the US, \nCanada and Europe. These examinations typically employ multiple-choice questions, a format also adopted \nin the US by the American Board of Psychiatry and Neurology 10, and in Europe by the European Board of \nNeurology (UEMS-EBN) 11.  \nIn this exploratory study, our objective was to evaluate the performance of GPT -3.5 and GPT -4 in \ncomparison to human performance in neurology board -like written examinations. We used the context of \nneurology board-like written examinations as a representative example to scrutinize the complex reasoning \nabilities and the capacity of large language models (LLMs), specifically GPT -3.5 and GPT-4, to navigate \nintricate medical cases, thereby illuminating their potential in more sophisticated, real -world clinical \napplications. Our ultimate aim was not only to determine their accuracy and reliability in this specialized \ncontext but also to characterize their strengths and limitations. As LLMs continue to evolve, understanding \ntheir potential contributio ns and challenges in medical examinations could pave the way for future \napplications in neurology and neurology education. \n \nMethods \nThis exploratory prospective study was performed from May 17 to May 31, 2023. \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \nStandard Protocol Approvals, Registrations, and Patient Consents \nThis study did not involve human subjects or patient data, so it was exempt from institutional review board \napproval. \nMultiple-Choice Question Selection and Classification \nA question bank resembling neurology board questions consisting of 2036 questions (from \nboardvitals.com) 12 was used for the evaluation of GPT -3.5 and GPT -4. Questions including videos or \nimages as well as questions that were based on preceding questions  were excluded in this study (n= 80 \nquestions excluded, n=1956 questions included). This question bank is appr oved by the American Board \nof Psychiatry and Neurology (ABPN) as part of a Self -Assessment program and can be used as a tool for \ncertified medical education 12. Questions were in single best answer, multiple-choice format with three, four \nor five distractors and one correct answer. To validate the results from this question bank, open-book sample \nquestions from 2022 from the European Board of Neurology were used (n=19 questions). These questions \nare either behind a paywall (in the case of the question bank) or published after 2021 and therefore out-of-\ntraining data for both GPT-3.5 and GPT-4.    \nQuestions were then classified by type using principles of Bloom Taxon omy for learning and assessment \nas testing either lower-order (remembering, basic understanding) or higher -order (applying, analyzing, or \nevaluating) thinking 13,14. We both let GPT-4 and the investigators evaluate whether the questions were in \nthe lower-order or higher-order category and  the investigators discussed cases of incongruencies. GPT-4 \nclassified in accordance with the investigators in 87.5% (175 of 200 questions), GPT-3.5 in 84.5% (169 of \n200 questions).  \nThe questions can be further categorized according to 26 topics in the field of neurology that are listed in \nTable 1 . Performance by users per individual question was available from the test portal  while this \ninformation was not available for the sample questions from the EBN. \nLarge language models  \nGPT-3.5 ( version: gpt -3.5-turbo (Chat-Completion); OpenAI) and GPT -4 (version: gpt-4 (Chat-\nCompletion), OpenAI) were used via API. These are two commonly used large language models 3,5.  At the \ntime of this study, we did not have access to other powerful closed -source models such as ClaudeV1 15 or \nPaLM2 16. GPT -3.5 and GPT -4 have been pretrained on over 45 terabytes of text data, including a \nsubstantial portion of internet websites, books, and articles. No additional neurology -specific pretraining \nwas performed. In this study, we used server-contained language models that were trained up to September \n2021. The used models do not have the ability to search the internet or external databases. \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \nData Collection and Assessment \nEach multiple-choice stem along with its answer choices was provided to GPT-3.5 and GPT-4 via its API \ntogether with the following prompt:  \n‚ÄúYou are a medical doctor and are taking the neurology board exam. The board exam consists of multiple \nchoice questions.  \nAll output that you give must be in JSON format. \n- Return the answer letter \n- Give an explanation \n- Rate your own confidence in your answer based on a Likert scale that has the following grades: 1 = no \nconfidence [stating it does not know]; 2 = little confidence [ie, maybe]; 3 = some confidence; 4 = confidence \n[ie, likely]; 5 = high confidence [stating answer and explanation without doubt]) \n- Classify the question into the following two categories: 1. lower order questions that probe remembering \nand basic understanding, and 2. higher order question where knowle dge needs to be applied, analysis \ncapabilities are examined, or evaluation is needed. (return \"Higher\" or \"Lower\") \n- Rate the confidence of your classification into these categories based on the Likert scale that has the \nfollowing grades1 = no confidence [stating it does not know]; 2 = little confidence [ie, maybe]; 3 = some \nconfidence; 4 = confidence [ie, likely]; 5 = high confidence [stating answer and explanation without doubt]) \nYour output must look like the following: \n{\"answerletter\":‚Ä¶,\"reasoning\":‚Ä¶,\"confidence_answer_likert\":‚Ä¶,\"classification\":‚Ä¶,\"confidence_classifi\ncation_likert\":‚Ä¶  ‚Äú \nAll answer choices and responses were recorded. A passing score was considered 70% or above on this \nneurology board‚Äìstyle examination without images to approximate the written examination from the ABPN \nand the European Board of Neurology (EBN). The question bank uses 70 % as passing threshold to gain \ncredits for certified medical education (CME) points. The Royal College examination in Canada considers \n70% or above on all written components a passing score. There, questions undergo psychometric validation, \nwith removal of questions found not discriminatory or too difficult, which was not performed. The ABPN \nand EBN examinations use criterion-referenced scoring, which was not used. \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \nFor all analyses except the reproducibility analyses, the questions were answered once. For the \nreproducibility analyses, 100 questions were answered by GPT-3.5 and GPT-4 with 50 independent queries \nprobing the principle of self-consistency 17. \nHigh-dimensional analysis of question representations by GPT-3.5 and GPT-4 \nFor the high -dimensional analysis of question representations, the embeddings of these questions  were \nanalyzed. These numeric vector representations encompass the semantic and contextual essence of the \ntokens - in this context, the questions - processed by the model 18. The source of these embeddings is the \nmodel parameters or weights, which are emp loyed to code and decode the texts for input and output. A \ndimensionality reduction of the embeddings was performed with a tSNE analysis 19 and clusters were \nsubsequently examined. \nData availability statement \nNo patient data was used in this study. The prompts used in this study are deposited in the Methods \nsection. Software code used for inquiring the Open AI API has been deposited on GitHub: \nhttps://github.com/venkataramani-lab/NeurologyBoard_LLM. Questions from the EBN can be accessed \nat https://www.uems neuroboard.org/web/images/docs/exa19m/2023/Example-Questions-\nselection2023.pdf and questions from the question bank can be found on boardvitals.com. Metadata about \nthe questions (e.g. question length, the models‚Äô score or their embeddings) is available on GitHub: \nhttps://github.com/venkataramani-lab/NeurologyBoard_LLM.  \n \nStatistical Analyses \nFirst, the overall performance was evaluated. Next, we compared the performance across different types of \nquestions (namely, lower and higher order) using a single-variable analysis approach (employing the Chi-\nsquared test). We also executed a subgroup ana lysis for various subclasses of higher -order thinking \nquestions and the 26 topics, where we utilized  the Chi-squared test for multiple comparisons with a \nBonferroni correction. Given that GPT-3.5 and GPT-4 had a probabilistic chance of correctly answering \neach question, we utilized a guessing correction formula to glean further understanding 20: it is computed \nby subtracting the ratio of the number of incorrect responses to (the total number of choice s minus one) \nfrom the number of correct responses:  \nùëõùëêùëúùëüùëüùëíùëêùë° ‚àí  ùëõùë§ùëüùëúùëõùëî\nùëòùëúùëùùë°ùëñùëúùëõùë† ‚àí 1 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \nWe contrasted the confidence level of responses between correct and incorrect answers by employing the \nMann-Whitney U test after testing for normality using the Shapiro -Wilk test. For the correlation analysis \nbetween human performance and model performance, human quartiles were converted to numeric  values \n(1-4). A P-value of less than .05 was deemed indicative of a significan t difference. All these statistical \nexaminations were carried out in R (version 4.0.5; accessible at https://r-project.org) 21. \nResults \nOverall Performance \nFirst, we examined the proficiency of GPT -3.5 and GPT-4 against a question bank set. Evidently, GPT-4 \ndisplayed an 85% accuracy level (1662 correct responses out of 1956 questions), supersed ing GPT-3.5, \nwhich managed a 66.8% accuracy level (1306 correct responses out of 1956 questions). When adjusting for \nrandom guessing, GPT-4 yielded an 80.9% score (1583 out of 1956), as opposed to GPT-3.5's 57.8% score \n(1131 out of 1956). Remarkably, in comparison to the average user of the testing platform (73.8%), GPT-\n4's performance was superior (p <0.0001), whereas GPT -3.5 underperf ormed (p <0.0001, as detailed in \nTable 1). \nTo corroborate these results, we also investigated the performance based on openly available sample \nquestions from the EBN for its board examination. Here, GPT -4 correctly responded in 73 .7 % of the \nquestions (14 out of 19 questions) while GPT -3.5 only gave a correct response in 52 .6% of the questions \n(10 out of 19 questions) , with no significant differences between GPT3.5 and GPT -4 (p=0 .31, \nSupplementary Table 1).  \nTable 1 shows the performance of GPT 3.5 and 4 as well as the average test user overall and stratified for \nquestion type and topic  on the user bank while Supplementary Table 1  shows the results on the sample \nquestions of the EBN. Taken together, this demonstrates that GPT-4 is able to pass a neurology board-like \nexam, whereas GPT-3.5's performance falls short of passing such a specialized examination. \nPerformance By Question Type \nUpon analyzing the performance based on question type, it was discernible that both GPT -3.5 and GPT-4 \nexcelled in lower-order questions (GPT-3.5: 639/893 (71.6%), GPT-4: 790/893 (88.5%)) as compared to \nhigher-order questions (GPT-3.5: 667/1063 (62.7%), GPT-4: 872/1063 (82%), p<0.0001, see Table 1). \nIn the context of lower-order questions, GPT-3.5's performance (639/893 (71.6%)) was akin to human users' \nperformance (73.6%, p=0.73). However, GPT-3.5 lagged in answering higher -order questions (667/1063 \n(62.7%) versus 73.9%, p<0.0001, as exhibited in Table 1)). In both lower and higher-order questions, GPT-\n4 surpassed GPT-3.5 and human users, marking a great difference (p<0.0001, Table 1). \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \nSupplementary Figures 1-4 provide examples of questions, both correctly and incorrectly answered by \nGPT-4, categorized into lower and higher -order categories. Interestingly, when segregating the questions \ninto quartiles according to the average performance of human users ‚Äì easy, intermediate, advanced, and \ndifficult ‚Äì a correlation became evident between the performance of GPT -3.5, GPT-4, and the average \nhuman user (R=0.84, p<0.0001). This correlation potentially suggests shared difficulties faced by humans \nand these Language Learning Models (LLMs), as depicted in Supplementary Figure 5 and Table 2. \nPerformance by Topic \nA comparative evaluation of GPT-3.5, GPT-4, and the average user performance across various topics was \ncarried out (as depicted in Figure 1). In the \"Behavioral, Cognitive, Psych\" category, GPT-4 outperformed \nboth GPT-3.5 and average test bank users (GPT -4: 433/482 (89.8%), GPT-3.5: 362/482 (75.1%), human \nusers: 76%, p<0.0001). GPT-4 also exhibited superior performance in topics such as Basic Neuroscience, \nMovement Disorders, Neurotoxicology, Nutrition, Metabolic, Oncology, and Pain, when compared to \nGPT-3.5, whereas its performance aligned with the human user average (Table 1, Figure 1). \nTo identify any topic -specific strengths or weaknesses displayed by each model, we analyzed their \nperformance in topics that contained over 50 questions. Notably, GPT -3.5 did not display any significant \nperformance variation across topics. In contrast, GPT -4 showed significantly enhanced performance in \nanswering questions related to Behavioral, Cognitive and Psych categories (89 .8%) compared to its \nperformance on questions concerning Epilepsy, Seizures (39/55 (70 .9%), p=0.008) and Neuromuscular \ntopics (145/184 (78.8%), p=0.02). \nLevel of Confidence \nBoth GPT-3.5 and GPT -4 consistently responded to multiple -choice questions using confident or highly \nconfident language (100%, 200 of 200 questions, evaluated by investigators). Self-assessment of confidence \nexpressed by GPT-3.5 and GPT-4 in its answers showed a small difference between incorrect and correct \nresponses (mean Likert score, 4.69 vs 4.79; p<0.0001 for GPT -3.5, mean Likert score 4.77 vs. 4.93; \np<0.0001, for GPT-4). Incorrect GPT-4 and GPT-3.5 answers were all subjectively assessed by the models \nas expressing confidence or high confidence (Likert score 4 or 5, GPT-4: 99.3%, 292 out of 294, GPT-3.5: \n100%, 650 out of 650, Supplementary Figure 6). When prompted with the correct answer after an incorrect \nresponse, GPT-3.5 and GPT-4 responded by apologizing and agreeing with the provided correct answer in \nall cases (100%, n=100 of 100 incorrectly answered questions, both GPT-3.5 and GPT-4).  \nReproducibility of Responses \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \nNext, we in vestigated the reproducibility of responses. For this purpose, the same question (n= 100) was \nindependently queried 50 times and the percentage of each question was recorded. Next, we compared \nanswers with high reproducibility (defined as more than 75% of all queries answered with the same answer) \nto answer without high reproducibility. This analysis revealed that highly reproducible answers are more \nlikely to be answered correctly (66 of 88 (75%)) than inconsistent answers, (5 of 13 (38.5%), p=0.02) by \nGPT-3.5, potentially indicating another marker of confidence of LLMs that might be leveraged to filter out \ninvalid responses. The same observation was made with GPT-4 with 78 of 96 (81.3%) correct answers in \nanswers with high reproducibility compared to 1 of 4 (25%) in answers with low reproducibility (p=0.04). \nCharacteristics of questions using high-dimensional representation analysis of question embeddings \nWe identified an association of question word length and the ability to answer questions correctly in GPT-\n3.5 and GPT -4, incorrectly answered questions being longer on average (Supplementary Figure 7). This \nwas not found in human users, but instead a weak positive correlation between question length and correct \nanswers was observed (R=0.074, p=0.001, Supplementary Figure 7).  \nWhen analyzing the high -dimensional representation of correctly and incorrectly answered questions, no \npattern into distinct clusters was observed (Supplementary Figure 8).  \nTo investigate whether the models use the similarity of question and answers in the multidimensional \nembedding space to select their answer, s imilarity between the question embedding and each answer \nembeddings was compared. It was found that in 28.3 % of questions (476 of 1681), the correct answer was \nthe closest in the multidimensional embedding space. Accordingly, the LLMs labeled the most similar \nanswer as correct in 30.5% of cases (GPT-4, 513 of 1681, p=0.17, GPT-3.5, 524 of 1681 (31.1%), p=0.08), \nindicating that the distance between question and answer did not significantly affect the models‚Äô answer \nchoice. \nQualitative evaluation of GPT-3.5 and GPT-4 \nAll investigators felt that both the performance of GPT -3.5 and especially GPT -4 was impressive. While \nthe performance of GPT-3.5 was sometimes variable and difficult to predict this was more stable with GPT-\n4. While GPT-3.5 was able to answer some challen ging questions correctly, it also incorrectly answered \nsome questions the reviewers perceived as simple. This was not the case with GPT -4 where mostly \nquestions that were perceived as difficult were incorrectly answered. Interestingly, a small portion of \nincorrect answers was due to flawed reasoning potentially indicating problems of LLMs without any \nunderlying world model 22.  \nDiscussion \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \nThe notable progress achieved by GPT -3.5 and GPT -4 has significantly enhanced the potential of Large \nLanguage Models (LLMs) across a wide range of applications  23-25. Despite being extensively pretrained  \non vast data sets, offering promising possibilities within the healthcare sector, their specific application in \nneurology remains relatively uncharted territory. The efficacy of these models in handling specialized \nneurology knowledge also remained indeterminate until this study. \nThis exploratory research revealed GPT-4's proficiency in completing a neurology board-like examination, \na task GPT-3.5 was unable to accomplish. This finding underscores the rapid and significant evolution of \nLLMs.  \nDespite their strengths, GPT-3.5 and GPT-4 demonstrated subpar performance in tasks requiring higher -\norder thinking 13. Yet, GPT-4 still managed to perform satisfactorily. In comparison to its performance on \nthe USMLE Step Examinations 6,26,27, where it did not exceed a 65% accuracy, GPT-3.5 scored surprisingly \nwell in this more specialized examination. \nAs these models are trained to identify patterns and relationships among words in their training data, they \ncan struggle in situations requiring a deeper understanding of context or specialized technical language. \nThis limitation is crucial for neurologists to bear in mind, particularly with LLMs now being incorporated \ninto popular search engines and readily accessible to the public 28. \nInterestingly, both models exhibited confident language when answering questions, even when their \nresponses were incorrect. This trait is a recognized limitation of LLMs 29,30 and originates from the training \nobjective of these models, which is to predict the most likely sequence of words following an input. This \ncharacteristic, coupled with the model's inclination to generate plausible, convincing, and human -like \nresponses, can potentially mislead individuals relying solely on it for information 31,32. However, the model \nwas able to partially differentiate its own confidence level as there were slight but significant differences \nbetween corre ctly and incorrectly answers although the values on the Likert scale predominantly are \nbetween ‚Äúconfident‚Äù and ‚Äúhighly confident‚Äù. Furthermore, we identified that reproducible answers are \ncorrelated with correctness and might serve as an intrinsic, surrogate marker of confidence defined by the \noutput of the LLM. \nThis study has some limitations. The questions used were mostly not official ABPN or EBN questions due \nto their confidential and regulated nature. Additionally, image-based questions were not included as GPT-\n3.5 and current versions of GPT -4 are not equipped to process these. Furthermore, the pas sing grade was \nan approximation based on the threshold by the ABPN for approving of points for certified medical \neducation. The limited number of quest ions in each subgroup in this exploratory study also reduced the \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \npower of subgroup analyses.  We only included GPT-3.5 and GPT-4 in this assessment as other similarly \npowerful closed -source models were not available to us at the time of this study.  Lastly, qualitative \nassessments of GPT-3.5 and GPT -4 responses are inherently subjective.  \nIn conclusion, this study underscored the vast potential of LLMs such as GPT-4, particularly in neurology, \neven without neurology-specific pretraining. GPT-4 passed a neurolo gy board -style examination after \nexclusion of video and image questions . As deep learning architectures are continuously refined for \ncomputer vision and medical imaging 33,34, this image-processing limitation may be addressed in future AI \nmodels, potentially including the upcoming multimodal input functionalities of GPT-4. Despite performing \nadmirably on questions assessing basic knowledge and understanding, the model showed s lightly lower \nperformance on higher -order thinking questions. Consequently, neurologists should be aware of these \nlimitations, including the models' tendency to phrase inaccurate responses confidently, and should be \ncautious r egarding its usage in practice or education. With the anticipated advancements in LLMs, \nneurologists and experts of other clinical disciplines will need to comprehend their performance, reliability, \nand applications within neurology better. Investigating the potential applications of L LMs that have been \nfine-tuned specifically for neurology represents a compelling direction for future research. \nReferences \n1. Hillis JM, Bizzo BC. Use of Artificial Intelligence in Clinical Neurology. Seminars in Neurology. \n2022;42(01):39-47. \n2. Pedersen M, Verspoor K, Jenkinson M, Law M, Abbott DF, Jackson GD. Artificial intelligence for \nclinical decision support in neurology. Brain Commun. 2020;2(2):fcaa096. \n3. OpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt/ Web site. \nhttps://openai.com/blog/chatgpt/. Published 2022. Accessed. \n4. Vaswani A, Shazeer N, Parmar N, et al. Attention Is All You Need. 2017:arXiv:1706.03762. \ndoi:10.48550/arXiv.1706.03762. Accessed June 01, 2017. \n5. OpenAI. GPT -4 Technical Report. 2023:arXiv:2303.08774. doi:10.48550/arXiv.2303.08774. \nAccessed March 01, 2023. \n6. Kung TH, Cheatham M, Medenilla A, et al. Performance of  ChatGPT on USMLE: Potential for \nAI-assisted medical education using large language models. PLOS Digit Health. \n2023;2(2):e0000198. \n7. Gilson A, Safranek CW, Huang T, et al. How Does ChatGPT Perform on the United States Medical \nLicensing Examination? The Implications of Large Language Models for Medical Education and \nKnowledge Assessment. JMIR Med Educ. 2023;9:e45312. \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \n8. Mihalache A, Popovic MM, Muni RH. Performance of an Artificial Intelligence Chatbot in \nOphthalmic Knowledge Assessment. JAMA Ophthalmol. 2023. \n9. Ali R, Tang OY, Connolly ID, et al. Performance of ChatGPT and GPT-4 on Neurosurgery Written \nBoard Examinations. medRxiv. 2023:2023.2003.2025.23287743. \n10. ABPN. Instructions for the Neurology Certification Examination. 2022. \n11. EBN. Examples of op en book questions. https://www.uems-\nneuroboard.org/web/images/docs/exam/2023/Example-Questions-selection2023.pdf. Accessed. \n12. boardvitals.com. https://www.boardvitals.com/neurology-board-review. Accessed. \n13. Anderson LW KD. A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom‚Äôs \nTaxonomy of Educational Objectives. Pearson. 2001. \n14. EI S. Taxonomy of Educational Obj ectives: The Classification of  Educational Goals. Committee \nof College and University Examiners, Benjamin S Bloom Elem Sch J 1957;57(6):343‚Äì344. \n15. Anthropic. Introducing claude. Anthropic Blog. https://www.anthropic.com/index/introducing-\nclaude. Published 2022. Accessed. \n16. Anil R, Dai AM, Firat O, et al. PaLM 2 Technical Report. 2023:arXiv:2305.10403. \ndoi:10.48550/arXiv.2305.10403. Accessed May 01, 2023. \n17. Chen A, Phang J, Parrish A, et al. Two Failures of Self -Consistency in the Multi-Step Reasoning \nof LLMs. 2023:arXiv:2305.14279. doi:10.48550/arXiv.2305.14279. Accessed May 01, 2023. \n18. Neelakantan A, Xu T, Puri R, et al. Text and Code Embeddings by Contra stive Pre -Training. \n2022:arXiv:2201.10005. doi:10.48550/arXiv.2201.10005. Accessed January 01, 2022. \n19. van der Maaten L, Hinton G. Visualizing Data using t-SNE. J Mach Learn Res. 2008;9:2579-2605. \n20. ≈ûenel S, Pehlivan EB, Alatlƒ± B. Effect of Correction -for-Guessing Formula on Psychometric \nCharacteristics of Test. Procedia - Social and Behavioral Sciences. 2015;191:925-929. \n21. (2021). RCT. R: A language and environment for statistical computing. R Foundation for Statistical \nComputing, Vienna, Austria. URL https://wwwR-projectorg/. \n22. Guan L, Valmeekam K, Sreedharan S, Kambhampati S. Leveraging Pre -trained Large Language \nModels to Construct and Utilize World Models for Model -based Task Planning. \n2023:arXiv:2305.14909. doi:10.48550/arXiv.2305.14909. Accessed May 01, 2023. \n23. Sharma G, Thakur A. ChatGPT in drug discovery. 2023. \n24. Biswas S. Role of Chat GPT in Education. Available at SSRN 4369981. 2023. \n25. Lobentanzer S, Saez-Rodriguez J. A Platform for the Biomedical Application of Large Language \nModels. 2023:arXiv:2305.06488. doi:10.48550/arXiv.2305.06488. Accessed May 01, 2023. \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \n26. Li√©vin V, Egeberg Hother C, Winther O. Can large language models reason about medical \nquestions? 2022:arXiv:2207.08143. doi:10.48550/arXiv.2207.08143. Accessed July 01, 2022. \n27. Jin D, Pan E, Oufattole N, Weng W-H, Fang H, Szolovits P. What Disease Does This Patient Have? \nA Large-Scale Open Domain Question Answering Dataset from Medical Exams. Applied Sciences. \n2021;11(14):6421. \n28. Microsoft. Bing. https://www.bing.com/new. Published 2023. Accessed. \n29. Rohrbach A, Hendricks LA, Burns K, Darrell T, Saenko K. Object Hallucination in Image \nCaptioning. oct nov, 2018; Brussels, Belgium. \n30. Xiao Y, Wang WY. On Hallucination and Predictive Uncertainty in Conditional Language \nGeneration. April, 2021; Online. \n31. Lu L, McDonald C, Kelleher T, et al. Measuring consumer -perceived humanness of online \norganizational agents. Computers in Human Behavior. 2022;128:107092. \n32. Alkaissi H, McFarlane SI. Artificial Hallucinations in ChatGPT: Implications in Scientific Writing. \nCureus. 2023;15(2):e35179. \n33. Isensee F, J√§ger PF, Full PM, Vollmuth P, Maier -Hein KH. nnU -Net for Brain Tumor \nSegmentation. 2021; Cham. \n34. Bakas S, Reyes M, Jakab A, et al. Identifying the Best Machine Learning Algorithms for Brain \nTumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS \nChallenge. 2018:arXiv:1811.02629. doi:10.48550/arX iv.1811.02629. Accessed November 01, \n2018. \n \n \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \nTable 1: Performance of GPT-3.5, GPT-4 and Question Bank Users by Question Type and Topic \n \nChi-squared test was used to calculate p -values. P -values were adjusted for multiple testing using the \nBonferroni correction.  \n \n \n \n  \nQuestion Type Question \nN \nHuman \nCorrect \nMean \n% \nGPT-3.5 \nCorrect \nN (%) \nGPT-4 \nCorrect \nN (%) \nAdj. P \nValue \n GPT-3.5 \nvs Human \nAdj. P \nValue \n GPT-4 vs \nHuman \nAdj. P \nValue \n GPT-3.5 \nvs GPT-4 \nAll Questions 1956 73.8 1306 (66.8) 1662 (85) <0.0001 <0.0001 <0.0001 \nOrder of thinking        \nHigher 1063 73.9 667 (62.7) 872 (82) <0.0001 <0.0001 <0.0001 \nLower 893 73.6 639 (71.6) 790 (88.5) 0.73 <0.0001 <0.0001 \nCategory        \nBasic Neuroscience 128 74.1 83 (64.8) 109 (85.2) 1 1 0.008 \nBehavioral, Cognitive, Psych 482 76 362 (75.1) 433 (89.8) 1 <0.0001 <0.0001 \nCerebrovascular 113 77.7 77 (68.1) 93 (82.3) 1 1 0.54 \nChild Neurology 73 69.8 48 (65.8) 57 (78.1) 1 1 1 \nCongenital 20 74.3 14 (70) 18 (90) 1 1 1 \nCranial Nerves 46 70 24 (52.2) 34 (73.9) 1 1 1 \nCritical Care 28 71.3 15 (53.6) 25 (89.3) 1 1 0.2 \nDemyelinating Disorders 49 81.9 37 (75.5) 45 (91.8) 1 1 1 \nEpilepsy, Seizures 55 73.7 34 (61.8) 39 (70.9) 1 1 1 \nEthics 5 84.8 2 (40) 4 (80) 1 1 1 \nGenetic 20 70.4 16 (80) 17 (85) 1 1 1 \nHeadache 59 74 40 (67.8) 50 (84.7) 1 1 1 \nImaging/Diagnostic Studies 10 65.4 5 (50) 9 (90) 1 1 1 \nMovement Disorders 91 75.1 54 (59.3) 79 (86.8) 1 1 0.002 \nNeuro-Ophthalmology 46 72.4 29 (63) 40 (87) 1 1 0.42 \nNeuro-Otology 42 66.9 20 (47.6) 31 (73.8) 1 1 0.66 \nNeuroinfectious Disease 30 68.1 16 (53.3) 25 (83.3) 1 1 0.69 \nNeurologic Complications of Systemic \nDisease \n42 71.8 21 (50) 31 (73.8) 1 1 1 \nNeuromuscular 184 68.9 120 (65.2) 145 (78.8) 1 1 0.14 \nNeurotoxicology, Nutrition, Metabolic 93 70.4 63 (67.7) 82 (88.2) 1 0.1 0.04 \nOncology 72 70 41 (56.9) 62 (86.1) 1 0.71 0.006 \nPain 65 78.8 42 (64.6) 58 (89.2) 1 1 0.05 \nPharmacology 72 74 48 (66.7) 60 (83.3) 1 1 0.89 \nPregnancy 19 69.1 14 (73.7) 15 (78.9) 1 1 1 \nSleep 92 77.4 67 (72.8) 82 (89.1) 1 1 0.22 \nTrauma 20 74.2 14 (70) 19 (95) 1 1 1 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \nTable 2: Comparison of GPT-3.5, GPT-4 and Question Bank Users by Question Type, Difficulty and \nTopic \n \nChi-squared test was used to calculate p -values. P -values were adjusted for multiple testing using the \nBonferroni correction.  \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nOrder of Thinking \n \nQuestions \nN \nHuman \nCorrect  \nMean % \nGPT-3.5 \nCorrect  \nN (%) \nGPT-4 \nCorrect  \nN (%) \nP Value \n GPT-3.5 \nvs Human \nP Value \n GPT-4 vs \nHuman \nP Value \n GPT-3.5 \nvs GPT-4 \nEasy Questions (1st quartile)        \nHigher 283 93.3 234 (82.7) 275 (97.2) 0 0.39 <0.0001 \nLower 226 92.9 199 (88.1) 214 (94.7) 0.87 1 0.15 \nIntermediate Questions (2nd quartile)        \nHigher 271 82.1 189 (69.7) 251 (92.6) 0.01 0.002 <0.0001 \nLower 237 82 193 (81.4) 225 (94.9) 1 <0.0001 <0.0001 \nAdvanced Questions (3rd  quartile)        \nHigher 247 69.6 134 (54.3) 193 (78.1) 0.005 0.32 <0.0001 \nLower 207 69.9 145 (70) 188 (90.8) 1 <0.0001 <0.0001 \nDifficult Questions (4th  quartile)        \nHigher 262 48.5 110 (42) 153 (58.4) 1 0.23 0.002 \nLower 223 48.5 102 (45.7) 163 (73.1) 1 <0.0001 <0.0001 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint \nFigure 1: Percentage of correctly answered questions per topic. \nHuman user score distribution is shown by boxplots (grey), LLM performance is shown in green and pink. \nPercentage of correctly answered questions per topic by human users (grey boxplots, black line indicates \nmedian), GPT -3.5 (green) and GPT -4 (pink). In the majority of topics, GPT -4 performs above human \naverage, while GPT-3.5 performs below human average (Table 1).   \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292598doi: medRxiv preprint "
}