{
  "title": "Large Language Models and the Wisdom of Small Crowds",
  "url": "https://openalex.org/W4398160410",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2480216692",
      "name": "Sean Trott",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6842080761",
    "https://openalex.org/W2897312833",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4386981810",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W1989847457",
    "https://openalex.org/W4381163899",
    "https://openalex.org/W1534477342",
    "https://openalex.org/W4376117416",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W296381919",
    "https://openalex.org/W4380763235",
    "https://openalex.org/W4387378202",
    "https://openalex.org/W4385066501",
    "https://openalex.org/W1998674455",
    "https://openalex.org/W2085876742",
    "https://openalex.org/W4391100869",
    "https://openalex.org/W4317892547",
    "https://openalex.org/W4320481230",
    "https://openalex.org/W2982116886",
    "https://openalex.org/W2779206865",
    "https://openalex.org/W4379662558",
    "https://openalex.org/W2889870962",
    "https://openalex.org/W4384198612",
    "https://openalex.org/W2131694082",
    "https://openalex.org/W4391132527",
    "https://openalex.org/W3177005832",
    "https://openalex.org/W4383058631",
    "https://openalex.org/W4380715494",
    "https://openalex.org/W4366496010",
    "https://openalex.org/W2914304175",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W4311408938"
  ],
  "abstract": "Abstract Recent advances in Large Language Models (LLMs) have raised the question of replacing human subjects with LLM-generated data. While some believe that LLMs capture the “wisdom of the crowd”—due to their vast training data—empirical evidence for this hypothesis remains scarce. We present a novel methodological framework to test this: the “number needed to beat” (NNB), which measures how many humans are needed for a sample’s quality to rival the quality achieved by GPT-4, a state-of-the-art LLM. In a series of pre-registered experiments, we collect novel human data and demonstrate the utility of this method for four psycholinguistic datasets for English. We find that NNB &amp;gt; 1 for each dataset, but also that NNB varies across tasks (and in some cases is quite small, e.g., 2). We also introduce two “centaur” methods for combining LLM and human data, which outperform both stand-alone LLMs and human samples. Finally, we analyze the trade-offs in data cost and quality for each approach. While clear limitations remain, we suggest that this framework could guide decision-making about whether and how to integrate LLM-generated data into the research pipeline.",
  "full_text": null,
  "topic": "Crowds",
  "concepts": [
    {
      "name": "Crowds",
      "score": 0.6426223516464233
    },
    {
      "name": "Computer science",
      "score": 0.548656702041626
    },
    {
      "name": "Human health",
      "score": 0.478963166475296
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.45043161511421204
    },
    {
      "name": "Pipeline (software)",
      "score": 0.44881999492645264
    },
    {
      "name": "Data science",
      "score": 0.4171158969402313
    },
    {
      "name": "Medicine",
      "score": 0.23586300015449524
    },
    {
      "name": "Computer security",
      "score": 0.12927043437957764
    },
    {
      "name": "Environmental health",
      "score": 0.10640904307365417
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ],
  "cited_by": 14
}