{
  "title": "Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection",
  "url": "https://openalex.org/W3170363508",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2131530020",
      "name": "Zhu Lixing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4208804794",
      "name": "Pergola, Gabriele",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114781887",
      "name": "Gui, Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2369476579",
      "name": "Zhou, Deyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742539792",
      "name": "He, Yulan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962830617",
    "https://openalex.org/W2885949735",
    "https://openalex.org/W2963790827",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2970431814",
    "https://openalex.org/W2964042872",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2594155836",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964230360",
    "https://openalex.org/W2964300796",
    "https://openalex.org/W3035317046",
    "https://openalex.org/W2033702744",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2996849360",
    "https://openalex.org/W2993398598",
    "https://openalex.org/W3098196265",
    "https://openalex.org/W2963879591",
    "https://openalex.org/W3030674566",
    "https://openalex.org/W2963686995",
    "https://openalex.org/W2965453734",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2761590056",
    "https://openalex.org/W2188365844",
    "https://openalex.org/W3049143069",
    "https://openalex.org/W2963773425",
    "https://openalex.org/W3205498744",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W3098708719",
    "https://openalex.org/W2971199636",
    "https://openalex.org/W3098556456",
    "https://openalex.org/W2964331476"
  ],
  "abstract": "Emotion detection in dialogues is challenging as it often requires the identification of thematic topics underlying a conversation, the relevant commonsense knowledge, and the intricate transition patterns between the affective states. In this paper, we propose a Topic-Driven Knowledge-Aware Transformer to handle the challenges above. We firstly design a topic-augmented language model (LM) with an additional layer specialized for topic detection. The topic-augmented LM is then combined with commonsense statements derived from a knowledge base based on the dialogue contextual information. Finally, a transformer-based encoder-decoder architecture fuses the topical and commonsense information, and performs the emotion label sequence prediction. The model has been experimented on four datasets in dialogue emotion detection, demonstrating its superiority empirically over the existing state-of-the-art approaches. Quantitative and qualitative results show that the model can discover topics which help in distinguishing emotion categories.",
  "full_text": "Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion\nDetection\nLixing Zhu†, Gabriele Pergola†, Lin Gui†, Deyu Zhou§, Yulan He†\n†Department of Computer Science, University of Warwick, UK\n§School of Computer Science and Engineering, Key Laboratory of Computer Network\nand Information Integration, Ministry of Education, Southeast University, China\n{lixing.zhu,gabriele.pergola,lin.gui,yulan.he}@warwick.ac.uk\nd.zhou@seu.edu.cn\nAbstract\nEmotion detection in dialogues is challenging\nas it often requires the identiﬁcation of the-\nmatic topics underlying a conversation, the rel-\nevant commonsense knowledge, and the intri-\ncate transition patterns between the affective\nstates. In this paper, we propose a Topic-\nDriven Knowledge-Aware Transformer to han-\ndle the challenges above. We ﬁrstly design a\ntopic-augmented language model (LM) with\nan additional layer specialized for topic detec-\ntion. The topic-augmented LM is then com-\nbined with commonsense statements derived\nfrom a knowledge base based on the dialogue\ncontextual information. Finally, a transformer-\nbased encoder-decoder architecture fuses the\ntopical and commonsense information, and\nperforms the emotion label sequence predic-\ntion. The model has been experimented on\nfour datasets in dialogue emotion detection,\ndemonstrating its superiority empirically over\nthe existing state-of-the-art approaches. Quan-\ntitative and qualitative results show that the\nmodel can discover topics which help in dis-\ntinguishing emotion categories.\n1 Introduction\nThe abundance in dialogues extracted from on-\nline conversations and TV series provides unprece-\ndented opportunity to train models for automatic\nemotion detection, which are important for the de-\nvelopment of empathetic conversational agents or\nchat bots for psychotherapy (Hsu and Ku, 2018;\nJiao et al., 2019; Zhang et al., 2019; Cao et al.,\n2019). However, it is challenging to capture the\ncontextual semantics of personal experience de-\nscribed in one’s utterance. For example, the emo-\ntion of the sentence “I just passed the exam” can be\neither happy or sad depending on the expectation\nof the subject. There are strands of works utilizing\nthe dialogue context to enhance the utterance rep-\nresentation (Jiao et al., 2019; Zhang et al., 2019;\n(a): Food and Restaurant\n(b): Marriage and Death\nA: Could I have some fish?\nB: Certainly. And what vegetables would you like?\nA: Oh , spinach , I think.\nA: I like drinking tea at teahouses.\nB: Oh, so do I.\n☺\nB: Great. We can chat while enjoying a cup there.\nA: Why don't we go for one now?\n☺\nA: Let's go!\n☺\n☺\nA: Johnny died yesterday, we knew that it was coming, but...\nB: Like just last week, he was doing so well.☹\nA: Then all of a sudden they gave him a microphone,\n      he asked me to marry him, like, onstage.\nB: He was doing so well.\n☹\n☺\n☺\nFigure 1: Utterances around particular topics carry spe-\nciﬁc emotions. Utterances carrying positive (smiling\nface) or negative (crying face) emotions are highlighted\nin colour. Other utterances are labeled as ‘Neutral’. In\n(a), utterances discussing food and restaurant are more\nlikely carrying positive sentiment. In (b), the similar\nutterance, ‘He was doing so well ’, expressed different\nemotions depending on its associated topic.\nMajumder et al., 2019), where inﬂuences from his-\ntorical utterances were handled by recurrent units,\nand attention signals were further introduced to\nintensify the positional order of the utterances.\nDespite the progress made by the aforemen-\ntioned methods, detecting emotions in dialogues\nis however still a challenging task due to the way\nemotions are expressed and how the meanings of\nutterances vary based on the particular topic dis-\ncussed, as well as the implicit knowledge shared\nbetween participants. Figure 1 gives an example\nof how topics and background knowledge could\nimpact the mood of interlocutors. Normally, dia-\nlogues around speciﬁc topics carry certain language\npatterns (Serban et al., 2017), affecting not only the\nutterance’s meaning, but also the particular emo-\narXiv:2106.01071v1  [cs.CL]  2 Jun 2021\ntions conveyed by speciﬁc expressions. Existing\ndialogue emotion detection methods did not put\nemphasis on modelling these holistic properties of\ndialogues (i.e., conversational topics and tones).\nConsequently, they were fundamentally limited in\ncapturing the affective states of interlocutors re-\nlated to the particular themes discussed. Besides,\nemotion and topic detection heavily relies on lever-\naging underlying commonsense knowledge shared\nbetween interlocutors. Although there have been\nattempts in incorporating it, such as the COSMIC\n(Ghosal et al., 2020), existing approaches do not\nperform ﬁne-grained extraction of relevant infor-\nmation based on both the topics and the emotions\ninvolved.\nRecently, the Transformer architecture (Vaswani\net al., 2017) has empowered language models to\ntransfer large quantities of data to low-resource\ndomains, making it viable to discover topics in\nconversational texts. In this paper, we propose\nto add an extra layer to the pre-trained language\nmodel to model the latent topics, which is learned\nby ﬁne-tuning on dialogue datasets to alleviate the\ndata sparsity problem. Inspired by the success of\nTransformers, we use the Transformer Encoder-\nDecoder structure to perform the Seq2Seq predic-\ntion in which an emotion label sequence is pre-\ndicted given an utterance sequence (i.e., each utter-\nance is assigned with an emotion label). We posit\nthat the dialogue emotion of the current utterance\ndepends on the historical dialogue context and the\npredicted emotion label sequence for the past utter-\nances. We leverage the attention mechanism and\nthe gating mechanism to incorporate commonsense\nknowledge retrieved by different approaches. Code\nand trained models are released to facilitate further\nresearch1. To sum up, our contributions are:\n• We are the ﬁrst to propose a topic-driven ap-\nproach for dialogue emotion detection. We\npropose to alleviate the low-resource setting\nby topic-driven ﬁne-tuning using pre-trained\nlanguage models.\n• We utilize a pointer network and an additive at-\ntention to integrate commonsense knowledge\nfrom multiple sources and dimensions.\n• We develop a Transformer Encoder-Decoder\nstructure as a replacement of the commonly-\nused recurrent attention neural networks for\ndialogue emotion detection.\n1http://github.com/something678/TodKat.\n2 Related Work\nDialogue Emotion Detection Majumder et al.\n(2019) recognized the importance of dialogue con-\ntext in dialogue emotion detection. They used a\nGated Recurrent Unit (GRU) to capture the global\ncontext which is updated by the speaker ad-hoc\nGRUs. At the same time, Jiao et al. (2019) pre-\nsented a hierarchical neural network model that\ncomprises two GRUs for the modelling of tokens\nand utterances respectively. Zhang et al. (2019)\nexplicitly modelled the emotional dependencies\non context and speakers using a Graph Convolu-\ntional Network (GCN). Meanwhile, Ghosal et al.\n(2019) extended the prior work (Majumder et al.,\n2019) by taking into account the intra-speaker de-\npendency and relative position of the target and\ncontext within dialogues. Memory networks have\nbeen explored in (Jiao et al., 2020) to allow bidi-\nrectional inﬂuence between utterances. A similar\nidea has been explored by Li et al. (2020b). While\nthe majority of works have been focusing on tex-\ntual conversations, Zhong et al. (2019) enriched\nutterances with concept representations extracted\nfrom the ConceptNet (Speer et al., 2017). Ghosal\net al. (2020) developed COSMIC which exploited\nATOMIC (Sap et al., 2019) for the acquisition of\ncommonsense knowledge. Different from exist-\ning approaches, we propose a topic-driven and\nknowledge-aware model built on a Transformer\nEncoder-Decoder structure for dialogue emotion\ndetection.\nLatent Variable Models for Dialogue Context\nModelling Latent variable models, normally de-\nscribed in their neural variational inference form\nnamed Variational Autoencoder (V AE) (Kingma\nand Welling, 2014), has been studied extensively\nto learn thematic representations of individual doc-\numents (Miao et al., 2016; Srivastava and Sutton,\n2017; Rezaee and Ferraro, 2020). They have been\nsuccessfully employed for dialogue generation to\nmodel thematic characteristics over dynamically\nevolving conversations. This line of work, which\ninlcudes approaches based on hierarchical recurrent\nV AEs (Serban et al., 2017; Park et al., 2018; Zeng\net al., 2019) and conditional V AEs (Sohn et al.,\n2015; Shen et al., 2018; Gao et al., 2019), encodes\neach utterance with historical latent codes and au-\ntoregressively reconstructs the input sequence.\nOn the other hand, pre-trained language models\nare used as embedding inputs to V AE-based mod-\nels (Peinelt et al., 2020; Asgari-Chenaghlu et al.,\n2020). Recent work by Li et al. (2020a) employs\nBERT and GPT-2 as the encoder-decoder structure\nof V AE. However, these models have to be either\ntrained from scratch or built upon pre-trained em-\nbeddings. They therefore cannot be directly applied\nto the low-resource setting of dialogue emotion de-\ntection.\nKnowledge Base and Knowledge Retrieval\nConceptNet (Speer et al., 2017) captures common-\nsense concepts and relations as a semantic network,\nwhich encompasses the spatial, physical, social,\ntemporal, and psychological aspects of everyday\nlife. More recently, Sap et al. (2019) built ATOMIC ,\na knowledge graph centered on events rather than\nentities. Owing to the expressiveness of events and\nameliorated relation types, using ATOMIC achieved\ncompetitive results against human evaluation in the\ntask of If-Then reasoning.\nAlongside the development of knowledge bases,\nrecent years have witnessed the thrive of new meth-\nods for training language models from large-scale\ntext corpora as implicit knowledge base. As has\nbeen shown in (Petroni et al., 2019), pre-trained\nlanguage models perform well in recalling rela-\ntional knowledge involving triplet relations about\nentities. Bosselut et al. (2019) proposed COM-\nmonsEnse Transformers (COMET ) which learns to\ngenerate commonsense descriptions in natural lan-\nguage by ﬁne-tuning pre-trained language models\non existing commonsense knowledge bases such\nas ATOMIC . Compared with extractive methods,\nlanguage models ﬁne-tuned on knowledge bases\nhave a distinctive advantage of being able to gener-\nate knowledge for unseen events, which is of great\nimportance for tasks which require the incorpora-\ntion of commonsense knowledge such as emotion\ndetection in dialogues.\n3 Methodology\n3.1 Problem Setup\nA dialogue is deﬁned as a sequence of utterances\n{x1,x2,...,x N}, which is annotated with a se-\nquence of emotion labels {y1,y2, ...,y N}. Our\ngoal is to develop a model that can assign the\ncorrect label to each utterance. As for each ut-\nterance, the raw input is a token sequence, i.e.,\nxn = {wn,1,wn,2,...,w n,Mn}where Mn denotes\nthe length of an utterance. We address this prob-\nlem using the Seq2Seq framework (Sutskever et al.,\n2014), in which the model consecutively consumes\nan utterance xn and predicts the emotion label yn\nbased on the earlier utterances and their associated\npredicted emotion labels. The joint probability of\nemotion labels for a dialogue is:\nPθ(y1:N|x1:N) =\nN∏\nn=1\nPθ(yn|x≤n,y<n) (1)\nIt is worth mentioning that the subsequent utter-\nances are unseen to the model at each predictive\nstep. Learning is performed via optimizing the\nlog-likelihoods of predicted emotion labels.\nThe overall architecture of our proposed\nTOpic-Driven and Knowledge-Aware Transformer\n(TODKAT) is shown in Figure 2, which consists of\ntwo main components, the topic-driven language\nmodel ﬁne tuned on dialogues, and the knowledge-\naware transformer for emotion label sequence pre-\ndiction for a given dialogue. In what follows, we\nwill describe each of the components in turn.\n3.2 Topic Representation Learning\nWe propose to insert a topic layer into an existing\nlanguage model and ﬁne-tune the pre-trained lan-\nguage model on the conversational text for topic\nrepresentation learning. Topic models, often for-\nmulated as latent variable models, play a vital role\nin dialogue modeling (Serban et al., 2017) due to\nthe explicit modeling of ‘high-level syntactic fea-\ntures such as style and topic’ (Bowman et al., 2016).\nDespite the tremendous success of applying topic\nmodeling in dialogue generation (Sohn et al., 2015;\nShen et al., 2018; Gao et al., 2019), there is scarce\nwork exploiting latent variable models for dialogue\nemotion detection. To this end, we borrow the ar-\nchitecture from VHRED (Serban et al., 2017) for\ntopic discovery, with the key modiﬁcation that both\nthe encoder RNN and decoder RNN are replaced\nby layers of a pre-trained language model. Further-\nmore, we use a transformer multi-head attention in\nreplacement of the LSTM to model the dependence\nbetween the latent topic vectors. Unlike VHRED,\nwe are interested in the encoder part to extract the\nposterior of the latent topic z, rather than the recur-\nrent prior of zin the decoder part since the latter\nis intended for dialogue generation. We assume\nthat each utterance is mapped to a latent variable\nencoding its internal topic, and impose a sequen-\ntial dependence on the topic transitions. Figure 2a\ngives an overview of the V AE-based model which\nnth utterance\n...\nzn\nEncoder (LMφ)\n...\nnth utterance\n with masks\nn+1th utterance\n...\nzn+1\n...\nn+1th utterance\n   with masks\nDecoder (LMφ)\nLatent Vector\nOutput\nInput\nTopic-driven fine-tuning\n(a) Topic-driven ﬁne-tuning of a pre-trained LM.\nTransformer Encoder-Decoder Classifier\n...\nznh[CLS]\nnth utterance\nun{u0, u1, ..., un-1}\n...\nzi\nh[CLS]\n1:n-1 utterances\nTransformer Encoder\nTransformer Decoder\nyn on\nQVK\nVK\nci\ncn\ncandidate events\nKnowledge Graph\nSBERT COMET\nAttention\nPointer Network\n\n\n\nEvent: PersonX has to to go to work\nxIntent: To get a raise\nxReact: Be tired\noReact: Be worried (b) Knowledge-aware transformer.\nFigure 2: TOpic-Driven and Knowledge-Aware Transformer (TODKAT).\naims at learning the latent topic vector during the\nﬁne-tuning of the language model.\nSpeciﬁcally, the pre-trained language model is\ndecomposed into two parts, the encoder and the\ndecoder. By retaining the pre-trained weights, we\ntransfer representations from high-resource tasks\nto the low-resource setting, which is the case for\ndialogue emotion datasets.\nEncoder The training of topic discovery part of\nTODKAT comprises a V AE at each time step, with\nits latent variable dependent on the previous latent\ncode. Each utterance is input to the V AE encoder\nwith a recurrent hidden state, the output of which is\na latent vector ideally encoding the topic discussed\nin the utterance. The latent vectors are tied through\na recurrent hidden state to constraint a coherent\ntopic over a single dialogue. We use LMφ to de-\nnote the network of lower layers of the language\nmodel (before the topic layer) and xL\nn to denote\nthe output from LMφ given the input xn. The vari-\national distribution for the approximation of the\nposterior will be:\nqφ(zn|x≤n, z<n)\n= N\n(\nzn|fµφ(xL\nn, hn−1), fσφ(xL\nn, hn−1)\n)\n,\n(2)\nwhere hn−1 = fτ(zn−1, xL\nn−1), for n >1. (3)\nHere, fµφ(·) and fσφ(·) are multi-layer percep-\ntrons (MLPs), fτ can be any transition function\n(e.g., a recurrent unit). We employ the transformer\nmulti-head attention with its query being the previ-\nous latent variable zn−1, that is,\nfτ(zn−1, xL\nn−1) = Attention(zn−1, xL\nn−1, xL\nn−1). (4)\nWe initialize h0 = 0 and model the transition\nbetween hn−1 and hn by ﬁrst generating zn from\nhn−1 using Eq. (2), then calculating hn by Eq. (3).\nDecoder The decoder network reconstructs xn\nfrom zn at each time step. We use Gaus-\nsian distributions for both the generative prior\nand the variational distribution. Since we want\nzn to be dependent on zn−1, the prior for zn\nis p(zn|hn−1) = N\n(\nzn|fµγ(hn−1),fσγ(hn−1)\n)\n.\nwhere fµγ(·) and fσγ(·) are MLPs. The posterior\nfor zn is pθ(zn|x≤n,z<n), which is intractable and\nis approximated by qφ(zn|x≤n,z<n) of Eq. 2. We\ndenote the higher layers of the language model as\nLMθ. Then the reconstruction of ˆxn given zn and\nxL\nn can be expressed as:\nˆxn = LMθ(zn,xL\nn). (5)\nNote that this is different from dialogue generation\nin which an utterance is generated from the latent\ntopic vector. Here, we aim to extract the latent topic\nfrom the current utterance and therefore train the\nmodel to reconstruct the input utterance as speciﬁed\nin Eq. (5). To make the combination of zn and\nxL\nn compatible for LMθ, we need to perform the\nlatent vector injection. As in (Li et al., 2020a), we\nemploy the “Memory” scheme that zn becomes an\nadditional input for LMθ, that is, the input to the\nhigher layers becomes [zn,xL\nn].\nTraining The training objective is the Evidence\nLower Bound (ELBO):\nEqφ(z≤N|x≤N)[log pθ(x≤N|z≤N)]\n−KL[qφ(z≤N|x≤N)||p(z≤N)].\n(6)\nEq. 6 factorizes and the expectation term becomes\nEqφ(z≤N|x≤N)\n[N∑\nn=1\nlog pθ(xn|z≤n,x<n)\n]\n, (7)\nand the KL term becomes\nN∑\nn=1\nKL[qφ(zn|x≤n,z<n)||p(zn|z<n,x<n)], (8)\nwhere p(zn|z<n,x<n) is the prior for zn. After\ntraining, we are able to extract the topic represen-\ntation from the encoder part of the model, which\nis denoted as zn = LMenc\nφ (xn). Meanwhile, the\nentire language model has been ﬁne-tuned, which\nis denoted as un = LMCLS(xn).\n3.3 Knowledge-Aware Transformer\nThe topic-driven LM ﬁne-tuning stage makes it\npossible for the LM to discover a topic represen-\ntation from a given utterance. After ﬁne-tuning,\nwe attach the ﬁne-tuned components to a classiﬁer\nand train the classiﬁer to predict the emotion la-\nbels. We propose to use the Transformer Encoder-\nDecoder structure as the classiﬁer, and consider\nthe incorporation of commonsense knowledge re-\ntrieved from external knowledge sources. In what\nfollows, we ﬁrst describe how to retrieve the com-\nmonsense knowledge from a knowledge source,\nthen we present the detailed structure of the classi-\nﬁer.\nCommonsense Knowledge Retrieval We use\nATOMIC 2 as a source of external knowledge. In\nATOMIC , each node is a phrase describing an event.\nEdges are relation types linking from one event\nto another. ATOMIC thus encodes triples such as\n⟨event, relation type, event⟩. There\nare a total of nine relation types, of which three\nare used: xIntent, the intention of the subject\n(e.g., ‘to get a raise’),xReact, the reaction of the\nsubject (e.g., ‘be tired’), andoReact, the reaction\nof the object (e.g., ‘ be worried’), since they are\ndeﬁned as the mental states of an event (Sap et al.,\n2019).\nGiven an utterance xn, we can compare it\nwith every node in the knowledge graph, and re-\ntrieve the most similar one. The method for com-\nputing the similarity between an utterance and\nevents is SBERT (Reimers and Gurevych, 2019).\nWe extract the top- K events, and obtain their\nintentions and reactions, which are denoted as\n{esI\nn,k,esR\nn,k,eoR\nn,k},k = 1,...,K .\nOn the other hand, there is a knowledge gen-\n2https://homes.cs.washington.edu/\n˜msap/atomic/\neration model, called COMET 3, which is trained\non ATOMIC . It can take xn as input and gen-\nerate the knowledge with the desired event rela-\ntion types speciﬁed (e.g., xIntent, xReact or\noReact). The generated knowledge can be un-\nseen in ATOMIC since COMET is essentially a ﬁne-\ntuned language model. We use COMET to generate\nthe Kmost likely events, each with respect to the\nthree event relation types. The produced events are\ndenoted as {gsI\nn,k,gsR\nn,k,goR\nn,k}, k= 1,...,K .\nKnowledge Selection With the knowledge re-\ntrieved from ATOMIC , we build a pointer net-\nwork (Vinyals et al., 2015) to exclusively choose\nthe commonsense knowledge either from SBERT\nor COMET . The pointer network calculates the\nprobability of choosing the candidate knowledge\nsource as:\nP\n(\nI(xn,en,gn) = 1\n)\n= σ\n(\n[xn,en,gn]Wσ\n)\n,\nwhere I(xn,en,gn) is an indicator function with\nvalue 1 or 0, and σ(x) = 1/(1+exp( −x)). We en-\nvelope σwith Gumbel Softmax (Jang et al., 2017)\nto generate the one-hot distribution 4. The inte-\ngrated commonsense knowledge is expressed as\ncn = I(xn,en,gn)en +\n(\n1 −I(xn,en,gn)\n)\ngn,\nwhere cn = {csI\nn,k,csR\nn,k,coR\nn,k}K\nk=1.\nWith the knowledge source selected, we pro-\nceed to select the most informative knowledge. We\ndesign an attention mechanism (Bahdanau et al.,\n2015) to integrate the candidate knowledge. Recall\nthat we have a ﬁne-tuned language model which\ncan calculate both the [CLS] and topic representa-\ntions. Here we apply the language model to the\nretrieved or generated knowledge to obtain the\n[CLS] and the topic representation, denoted as\n[cn,k,zn,k]. The attention mechanism is performed\nby calculating the dot product between the utter-\n3https://github.com/atcbosselut/\ncomet-commonsense\n4We have also experimented with a soft gating mechanism\nby aggregating knowledge from SBERT and COMET in a\nweighted manner. But the results are consistently worse than\nthose using a hard gating mechanism.\nance and each normalized knowledge tuple:\nvk = tanh\n(\n[cn,k,zn,k]Wα\n)\n, (9)\nαk = exp\n(\nvk[zn,un]⊤)\n∑\nkexp\n(\nvk[zn,un]⊤), (10)\ncn =\nK∑\nk=1\nαkcn,k. (11)\nHere, we abuse cn to represent the aggregated\nknowledge phrases. We further aggregate cn by\nevent relation types using a self-attention and the\nﬁnal event representation is denoted as cn.\nTransformer Encoder-Decoder We use a\nTransformer encoder-decoder to map an utterance\nsequence to an emotion label sequence, thus\nallowing for modeling the transitional patterns\nbetween emotions and taking into account the\nhistorical utterances as well. Each utterance is con-\nverted to the [CLS] representation concatenated\nwith the topic representation zn and knowledge\nrepresentation cn. We enforce a masking scheme\nin the self-attention layer of the encoder to make\nthe classiﬁer predict emotions in an auto-regressive\nway, entailing that only the past utterances are\nvisible to the encoder. This masking strategy,\npreventing the query from attending to future keys,\nsuits better a real-world scenario in which the\nsubsequent utterances are unseen when predicting\nan emotion of the current utterance. As for\nthe decoder, the output of the previous decoder\nblock is input as a query to the self-attention layer.\nThe training loss for the classiﬁer is the negative\nlog-likelihood expressed as:\nL= −\nN∑\nn=1\nlog pθ(yn|u≤n,y<n),\nwhere θdenotes the trainable parameters.\n4 Experimental Setup\nIn this section, we present the details of the datasets\nused, the methods for comparison, and the imple-\nmentation details of our models.\nDatasets We use the following datasets for ex-\nperimental evaluation:\nDailyDialog (Li et al., 2017) is collected from daily\ncommunications. It takes the Ekman’s six emotion\ntypes (Ekman, 1993) as the annotation protocol,\nthat is, it annotates an utterance with one of the\nsix basic emotions: anger, disgust, fear, happiness,\nsadness, or surprise. Those showing ambiguous\nemotions are annotated as neutral.\nMELD (Poria et al., 2019) is constructed from\nscripts of ‘Friends’, a TV series on urban life. Same\nas DailyDialog, the emotion label falls into Ek-\nman’s six emotion types, orneutral.\nIEMOCAP (Busso et al., 2008) is built with subti-\ntles from improvised videos. Its emotion labels are\nhappy, sad, neutral, angry, excited and frustrated.\nEmoryNLP (Zahiri and Choi, 2018)5 is also built\nwith conversations from ‘Friends’ TV series, but\nwith a slightly different annotation scheme in which\ndisgust, anger and surprise become peaceful, mad\nand powerful, respectively.\nFollowing Zhong et al. (2019) and Ghosal et al.\n(2020), the ‘neutral’ label of DailyDialog is not\ncounted in the evaluation to avoid highly imbal-\nanced classes. For MELD and EmoryNLP, we\nconsider a dialogue as a sequence of utterances\nfrom the same scene ID. Table 1 summarizes the\nstatistics of each dataset.\nDD MELD IEMOCAP EmoryNLP\n#Dial. 13,118 1,432 151 827\nTrain 11,118 1,038 100 659\nDev. 1,000 114 20 89\nTest 1,000 280 31 79\n#Utt. 102,979 13,708 7,333 9,489\nTrain 87,170 9,989 4,810 7,551\nDev. 8,069 1,109 1,000 954\nTest 7,740 2,610 1,523 984\n#Cat. 7 7 6 7\nTable 1: Statistics of the benchmarks for dialogue emo-\ntion detection. The train/development/test sets are pre-\ndeﬁned in each dataset.\nBaselines We compare the performance of TOD-\nKAT with the following methods:\nHiGRU (Jiao et al., 2019) simply inherits the re-\ncurrent attention framework that an attention layer\nis placed between two GRUs to aggregate the sig-\nnals from the encoder GRU and pass them to the\ndecoder GRU.\nDialogueGCN (Ghosal et al., 2019) creates a graph\nfrom interactions of speakers to take into account\nthe dialogue structure. A Graph Convolutional Net-\nwork (GCN) is employed to encode the speakers.\nEmotion labels are predicted with the combinations\nof the global context and speakers’ status.\n5https://github.com/emorynlp/\nemotion-detection\nModels\nDailyDialog MELD IEMOCAP EmoryNLP\nMacro-F1\n- neutral\nMicro-F1\n- neutral\nweighted\nAvg-F1 Micro-F1 weighted\nAvg-F1 Micro-F1 weighted\nAvg-F1 Micro-F1\nHiGRU 0.4904 0.5190 0.5681 0.5452 0.5854 0.5828 0.3448 0.3354\nDialogueGCN 0.4995 0.5373 0.5837 0.5617 0.6085 0.6063 0.3429 0.3313\nKET – 0.5348 0.5818 – 0.5956 – 0.3439 –\nCOSMIC 0.5105 0.5848 0.6521 – 0.6528* – 0.3811 –\nTODKAT 0.5256 0.5847 0.6823 0.6475 0.6133 0.6111 0.4312 0.4268\n−Topics 0.5136 0.5549 0.6634 0.6352 0.6281 0.6260 0.4180 0.4055\n−KB 0.5003 0.5344 0.6397 0.6111 0.5896 0.5738 0.3379 0.3262\nKATSBERT 0.5173 0.5578 0.6454 0.6188 0.6097 0.6069 0.3734 0.3567\nKATCOMET 0.5102 0.5462 0.6582 0.6307 0.6277 0.6254 0.4110 0.3974\nTable 2: The F1 results of the dialogue emotion detectors on four benchmarks. Here we denote the proposed model\nas TODKAT, of which the results are an average of ten runs. The ablations of different components are reported\nseparately in the bottom, where the model without the incorporation of latent topics is denoted as ‘ −Topics’,\ntransformer encoder-decoder structure without the use of a knowledge base is dnoted as ‘ −KB’. KATCOMET and\nKATSBERT uses the commonsense knowledge obtained with COMET and SBERT, respectively. Results of KET and\nCOSMIC are from (Zhong et al., 2019) and (Ghosal et al., 2020), respectively.\nKET (Zhong et al., 2019) is the ﬁrst model\nwhich integrates common-sense knowledge ex-\ntracted from ConceptNet and emotion information\nfrom an emotion lexicon into conversational text.\nA Transformer encoder is employed to handle the\ninﬂuence from past utterances.\nCOSMIC (Ghosal et al., 2020) is the state-of-the-\nart approach that leverages ATOMIC for improved\nemotion detection. COMET is employed in their\nmodel to retrieve the event-eccentric commonsense\nknowledge from ATOMIC .\nWe modiﬁed the script6 of language model ﬁne-\ntuning in the Hugging Face library (Wolf et al.,\n2020) for the implementation of topic-driven ﬁne-\ntuning. We use one transformer encoder layer. As\nfor the decoder, there are N layers where N is the\nnumber of utterances in a dialogue. We refer the\nreaders to the Appendix for the detailed settings of\nthe proposed models.\n5 Results and Analysis\nComparison with Baselines Experiment results\nof TODKAT and its ablations are reported in Ta-\nble 2. HiGRU and DialogueGCN results were\nproduced by running the code published by the\nauthors on the four datasets. Among the baselines,\nCOSMIC gives the best results. Our proposed\nTODKAT outperforms COSMIC on both MELD\nand EmoryNLP in weighted Avg-F1 with the im-\nprovements ranging between 3-5%. TODKAT also\nachieves superior result than COSMIC on DailyDi-\n6https://huggingface.co/transformers/\nv2.0.0/examples.html\nalogue in Macro-F1 and gives nearly the same re-\nsult in Micro-F1. TODKAT is inferior to COSMIC\non IEMOCAP. It is however worth mentioning that\nCOSMIC was trained with 132 instances on this\ndataset, while for all the other models the training-\nand-validation split is 100 and 20. As such, the\nIEMOCAP results reported on COSMIC (Ghosal\net al., 2020) are not directly comparable here. COS-\nMIC also incorporates the commonsense knowl-\nedge from ATOMIC but with the modiﬁed GRUs.\nOur proposed TODKAT, built upon the topic-driven\nTransformer, appears to be a more effective archite-\ncure for dialogue emotion detection. Compared\nwith KET, the improvements are much more sig-\nniﬁcant, with over 10% increase on MELD, and\nclose to 5% gain on DailyDialog. KET is also built\non the Transformer, but it considers each utterance\nin isolation and applies commonsense knowledge\nfrom ConceptNet. TODKAT, on the contrary, takes\ninto account the dependency of previous utterances\nand their associated emotion labels for the predic-\ntion of the emotion label of the current utterance.\nDialogueGCN models interactions of speakers and\nit performs slightly better than KET. But it is signif-\nicantly worse than TODKAT. It seems that topics\nmight be more useful in capturing the dialogue\ncontext.\nAblation Study The lower half of Table 2\npresents the F1 scores with the removal of vari-\nous components from TODKAT. It can be observed\nthat with the removal of the topic component, the\nperformance of TODKAT drops consistently across\nall datasets except IEMOCAP in which we ob-\nserve a slight increase in both weighted average\nF1 and Micro-F1. This might be attributed to the\nsize of the data since IEMOCAP is the smallest\ndataset evaluated here, and small datasets hinder\nthe model’s capability to discover topics. Without\nusing the commonsense knowledge (‘−KB’), we\nobserve more drastic performance drop compared\nto all other components, with nearly 10% drop in F1\non EmoryNLP, showing the importance of employ-\ning commonsense knowledge for dialogue emotion\ndetection. Comparing two different ways of extract-\ning knowledge from ATOMIC , direct retrieval using\nSBERT or generation using COMET , we observe\nmixed results. Overall, the Transformer Encoder-\nDecoder with a pointer network is a conciliator\nbetween the two methods, yielding a balanced per-\nformance across the datasets.\nRelationships between Topics and Emotions\nTo investigate the effectiveness of the learned topic\nvectors, we perform t-SNE (Van der Maaten and\nHinton, 2008) on the test set to study the rela-\ntionship between the learned topic vectors and the\nground-truth emotion labels. The results on Dai-\nlyDialog and MELD are illustrated in Figure 3(a)\nand (b). Latent topic vectors of utterance are used\nto plot the data points, whose colors indicate their\nground-truth emotion labels. We can see that the\nmajority of the topic vectors cluster into polarized\ngroups. Few clusters are bearing a mixture of po-\nlarity, possibly due to the background topics such\nas greetings in the datasets.\nTopics can be interpreted using the attention\nscores of Eq. 4. The top-10 most-attended words\nare selected as the representative words for each ut-\nterance. As in (Dathathri et al., 2020), we construct\nbag-of-words7 that represent 141 distinct topics.\nGiven the attended words of an utterance cluster\ngrouped based on their latent topic representations,\nwe label the word collection with the dominant\ntheme name. We refer to the theme names as topics\nin Figure 3c. It can be observed that utterances\nassociated with Ofﬁce tend to carry ‘disgust’emo-\ntions, while those related to Family are prone to be\n‘happy’.\nWe further compute the Spearman’s rank-order\ncorrelation coefﬁcient to quantitatively verify the\nrelationship between the topic and emotion vec-\ntors. For an utterance pair, a similarity score is\n7Word lists and their corresponding theme names\nare crawled fromhttps://www.enchantedlearning.\ncom/wordlist/.\nOffice\n(a) DailyDialog\nFamily (b) MELD\nTopic Utterances Emotion\nOfﬁce\nA: How are you doing, Christopher?\nB: To be honest, I’m really fed up with\nwork at the moment. I need a break!\nA: Are you doing anything this weekend?\nB: I have to work on Saturday all day!\nI really hate my job!\ndisgust\nFamily\nA: Yeah, I-I heard. I think it’s great! Ohh,\nI’m so happy for you!\nB: I can’t believe you’re getting married!\nC: Yeah.\nD: Monica and Rachel made out.\nhappy\n(c) Representative utterances and their topics\nFigure 3: T-SNE visualization of the learned topic vec-\ntors of utterances from the test sets of DailyDialog\n(subﬁgure (a)) and MELD (subﬁgure (b)). Colors indi-\ncate the ground-truth emotion label. Neutral utterances\nare omitted here for clarity. Representative utterances\n(highlighted in colors) for the topic ‘ Ofﬁce’ in Daily-\nDialog and the topic ‘Family’ in MELD are shown in\nsubﬁgure (c).\nobtained separately for their corresponding topic\nvectors as well as their emotion vectors. We then\nsort the list of emotion vector pairs according to\ntheir similarity scores to check to what extent their\nranking matches that of topic vector pairs, based\non the Spearman’s rank-order correlation coefﬁ-\ncient. The results are 0.60,0.58,0.42 and 0.54\nwith p-values ≪0.01 respectively for DailyDialog,\nMELD, IEMOCAP and EmoryNLP, showing that\nthere is a strong correlation between the clustering\nof topics and that of emotion labels. IEMOCAP\nhas the lowest correlation score, which is inline\nwith the results in Table 2 that the discovered latent\ntopics did not improve the emotion classiﬁcation\nresults.\nImpact of Relation Type We investigate the\nimpact of commonsense relation types on the\nperformance of TODKAT. We expand the re-\nlation set to ﬁve relation types and all nine re-\nlation types, respectively. According to (Sap\nDataset Relation Type\n{sI, sR, oR, sE, oE} All\nDailyDialog 0.5718↓ 0.5664↓\nMELD 0.6429↓ 0.6322↓\nIEMOCAP 0.6163↑ 0.6073↓\nEmoryNLP 0.4029↓ 0.3885↓\nTable 3: Micro-F1 scores of T ODKAT with more com-\nmonsense relation types retrieved from A TOMIC in-\ncluded for training. Here, “ sE” and “ oE” represent\neffect of subject and effect of object, respectively. “All”\ndenotes the incorporation of all nine commonsense re-\nlation types from ATOMIC .\net al., 2019), there are other relation types includ-\ning {sNeed,sWant,oWant,sEffect,oEffect}, which\nidentiﬁes the prerequisites and post conditions of\nthe given event, and {sAttr}, the “If-Event-Then-\nPersona” category of relation type that describes\nhow the subject is perceived by others. We calcu-\nlate the Micro-F1 scores of TODKAT with these\ntwo categories of relation types added step by step.\nFrom Table 3 we can conclude that the inclusion\nof two extra relation types or all relation types de-\ngrades the F1 scores on almost all datasets. An\nexception occurs on IEMOCAP where the F1 score\nrises by 0.5% when adding “sE” and “oE” rela-\ntions, possibly due to the fact that the dataset is\nabundant in events. Hence the extra event descrip-\ntions offer complementary knowledge to some ex-\ntent. While on other datasets neither the incorpo-\nration of “If-Event-Then-Event” nor the incorpo-\nration of “If-Event-Then-Persona” relation types\ncould bring any beneﬁt.\nImpact of Attention Mechanism With the\nknowledge retrieved from ATOMIC or generated\nfrom COMET , we are able to infer the possible\nintentions and reactions of the interlocutors. How-\never, not all knowledge phrases contribute the same\nto the emotion of the focused utterance. We study\nthe attention mechanism in terms of selecting the\nrelevant knowledge. We show in Table 4 a heat\nmap of the attention scores in Eq. 9 to illustrate\nhow the topic-driven attention could identify the\nmost salient phrase. The utterance ‘ Oh my God,\nyou’re a freak.’ will be erroneously categorized\nas ‘mad’ without using the topic-driven attention\n(shown in the last row of Table 4). In contrast, the\nattention mechanism guides the model to attend\nto the more relevant events and thus predict the\ncorrect emotion label.\nDialogue Context\nA: Alright, go on.\nB: Ok, I have to sleep on the west side\nbecause I grew up in California\nand otherwise the ocean would be\non the wrong side.\nA: Oh my God, you’re a freak.\nB: Yeah. How about that.\nNeutral\nNeutral\nJoyful\nNeutral\nTopic-Driven Attention\nA wants to be liked\nJoyful \u0013\nA wants to be accepted\nA wants to be a freak\nA will feel satisﬁed\nA will feel ashamed\nA will feel happy\nB will feel impressed\nB will feel disgusted\nB will feel surprised\nA: Oh my God, you’re a freak. Mad \u0017\nTable 4: Illustration of the attention mechanism in\nEq. 9 that helps distinguish the retrieved knowledge.\n6 Conclusion\nWe have proposed a Topic-Driven and Knowledge-\nAware Transformer model that incorporates topic\nrepresentation and the commonsense knowledge\nfrom ATOMIC for emotion detection in dialogues.\nA topic-augmented language model based on ﬁne-\ntuning has been developed for topic extraction.\nPointer network and additive attention have been\nexplored for knowledge selection. All the novel\ncomponents have been integrated into the Trans-\nformer Encoder-Decoder structure that enables\nSeq2Seq prediction. Empirical results demonstrate\nthe effectiveness of the model in topic represen-\ntation learning and knowledge integration, which\nhave both boosted the performance of emotion de-\ntection.\nAcknowledgements\nThe authors would like to thank the anonymous\nreviewers for insightful comments and helpful sug-\ngestions. This work was funded by the EPSRC\n(grant no. EP/T017112/1, EP/V048597/1). LZ is\nfunded by the Chancellor’s International Scholar-\nship at the University of Warwick. YH is supported\nby a Turing AI Fellowship funded by the UK Re-\nsearch and Innovation (grant no. EP/V020579/1).\nDZ is funded by the National Key Research and De-\nvelopment Program of China (2017YFB1002801)\nand the National Natural Science Foundation of\nChina (61772132).\nReferences\nMeysam Asgari-Chenaghlu, Mohammad-Reza Feizi-\nDerakhshi, Mohammad-Ali Balafar, and Cina Mo-\ntamed. 2020. Topicbert: A transformer transfer\nlearning based memory-graph approach for multi-\nmodal streaming social media topic detection. arXiv\npreprint arXiv:2008.06877.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of\nthe 3rd International Conference on Learning Rep-\nresentations (ICLR).\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 4762–4779,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nSamuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew\nDai, Rafal Jozefowicz, and Samy Bengio. 2016.\nGenerating sentences from a continuous space. In\nProceedings of The 20th SIGNLL Conference on\nComputational Natural Language Learning , pages\n10–21.\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\nKazemzadeh, Emily Mower, Samuel Kim, Jean-\nnette N Chang, Sungbok Lee, and Shrikanth S\nNarayanan. 2008. Iemocap: Interactive emotional\ndyadic motion capture database. Language re-\nsources and evaluation, 42(4):335.\nJie Cao, Michael Tanana, Zac Imel, Eric Poitras, David\nAtkins, and Vivek Srikumar. 2019. Observing dia-\nlogue in therapy: Categorizing and forecasting be-\nhavioral codes. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5599–5611, Florence, Italy. Associa-\ntion for Computational Linguistics.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. In\nProceedings of the 8th International Conference on\nLearning Representations (ICLR).\nPaul Ekman. 1993. Facial expression and emotion.\nAmerican psychologist, 48(4):384.\nJun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, Guodong\nZhou, and Shuming Shi. 2019. A discrete cvae for\nresponse generation on short-text conversation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1898–\n1908.\nDeepanway Ghosal, Navonil Majumder, Alexander\nGelbukh, Rada Mihalcea, and Soujanya Poria. 2020.\nCOSMIC: COmmonSense knowledge for eMotion\nidentiﬁcation in conversations. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 2470–2481, Online. Association for\nComputational Linguistics.\nDeepanway Ghosal, Navonil Majumder, Soujanya Po-\nria, Niyati Chhaya, and Alexander Gelbukh. 2019.\nDialoguegcn: A graph convolutional neural network\nfor emotion recognition in conversation. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 154–164.\nChao-Chun Hsu and Lun-Wei Ku. 2018. SocialNLP\n2018 EmotionX challenge overview: Recognizing\nemotions in dialogues. In Proceedings of the Sixth\nInternational Workshop on Natural Language Pro-\ncessing for Social Media , pages 27–31, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Cate-\ngorical reparameterization with gumbel-softmax. In\nProceedings of the 5th International Conference on\nLearning Representations (ICLR).\nWenxiang Jiao, Michael Lyu, and Irwin King. 2020.\nReal-time emotion recognition via attention gated\nhierarchical memory network. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 34, pages 8002–8009.\nWenxiang Jiao, Haiqin Yang, Irwin King, and\nMichael R Lyu. 2019. Higru: Hierarchical gated re-\ncurrent units for utterance-level emotion recognition.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 397–406.\nDiederik P Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. In Proceedings of the\n2nd International Conference on Learning Represen-\ntations (ICLR).\nChunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiu-\njun Li, Yizhe Zhang, and Jianfeng Gao. 2020a. Opti-\nmus: Organizing sentences via pre-trained modeling\nof a latent space. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4678–4699, Online. As-\nsociation for Computational Linguistics.\nWei Li, Wei Shao, Shaoxiong Ji, and Erik Cambria.\n2020b. Bieru: bidirectional emotional recurrent\nunit for conversational sentiment analysis. arXiv\npreprint arXiv:2006.00492.\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\nCao, and Shuzi Niu. 2017. Dailydialog: A manually\nlabelled multi-turn dialogue dataset. In Proceedings\nof the Eighth International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 986–995.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nNavonil Majumder, Soujanya Poria, Devamanyu Haz-\narika, Rada Mihalcea, Alexander Gelbukh, and Erik\nCambria. 2019. Dialoguernn: An attentive rnn for\nemotion detection in conversations. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 33, pages 6818–6825.\nYishu Miao, Lei Yu, and Phil Blunsom. 2016. Neural\nvariational inference for text processing. In Inter-\nnational Conference on Machine Learning (ICML) ,\npages 1727–1736.\nYookoon Park, Jaemin Cho, and Gunhee Kim. 2018.\nA hierarchical latent structure for variational conver-\nsation modeling. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n1792–1801, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nNicole Peinelt, Dong Nguyen, and Maria Liakata. 2020.\ntBERT: Topic models and BERT joining forces for\nsemantic similarity detection. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7047–7055, Online. As-\nsociation for Computational Linguistics.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473.\nSoujanya Poria, Devamanyu Hazarika, Navonil Ma-\njumder, Gautam Naik, Erik Cambria, and Rada Mi-\nhalcea. 2019. Meld: A multimodal multi-party\ndataset for emotion recognition in conversations. In\nProceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 527–\n536, Florence, Italy. Association for Computational\nLinguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nbert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3973–3983, Hong Kong, China. Association for\nComputational Linguistics.\nMehdi Rezaee and Francis Ferraro. 2020. A dis-\ncrete variational recurrent topic model without the\nreparametrization trick. In Advances in Neural In-\nformation Processing Systems , volume 33, pages\n13831–13843.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 33, pages\n3027–3035.\nIulian Serban, Alessandro Sordoni, Ryan Lowe, Lau-\nrent Charlin, Joelle Pineau, Aaron Courville, and\nYoshua Bengio. 2017. A hierarchical latent variable\nencoder-decoder model for generating dialogues. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 31, pages 3295–3301.\nXiaoyu Shen, Hui Su, Shuzi Niu, and Vera Demberg.\n2018. Improving variational encoder-decoders in di-\nalogue generation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 32, pages\n5456–5463.\nKihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.\nLearning structured output representation using\ndeep conditional generative models. In Advances in\nneural information processing systems , volume 28,\npages 3483–3491.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 31, pages\n4444–4451.\nAkash Srivastava and Charles Sutton. 2017. Autoen-\ncoding variational inference for topic models. In\nProceedings of the 5th International Conference on\nLearning Representations (ICLR).\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, volume 27, pages 3104–3112.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, volume 30, pages 5998–6008.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Advances in neural\ninformation processing systems , volume 28, pages\n2692–2700.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nSayyed M Zahiri and Jinho D Choi. 2018. Emotion de-\ntection on tv show transcripts with sequence-based\nconvolutional neural networks. In Workshops at the\nThirty-Second AAAI Conference on Artiﬁcial Intelli-\ngence.\nMin Zeng, Yisen Wang, and Yuan Luo. 2019. Dirich-\nlet latent variable hierarchical recurrent encoder-\ndecoder in dialogue generation. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1267–1272, Hong Kong,\nChina. Association for Computational Linguistics.\nDong Zhang, Liangqing Wu, Changlong Sun,\nShoushan Li, Qiaoming Zhu, and Guodong Zhou.\n2019. Modeling both context- and speaker-sensitive\ndependence for emotion detection in multi-speaker\nconversations. In Proceedings of the Twenty-\nEighth International Joint Conference on Artiﬁcial\nIntelligence, IJCAI-19 , pages 5415–5421. Interna-\ntional Joint Conferences on Artiﬁcial Intelligence\nOrganization.\nPeixiang Zhong, Di Wang, and Chunyan Miao. 2019.\nKnowledge-enriched transformer for emotion detec-\ntion in textual conversations. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 165–176.\nA Appendices\nA.1 Settings\nWe modiﬁed the script 8 of language model ﬁne-\ntuning in the Hugging Face library (Wolf et al.,\n2020) for the implementation of topic-driven ﬁne-\ntuning. On each training set, we train the topic\nmodel for 3 epochs, with learning rate set to 5e-5\nto prevent overﬁtting to the low-resource dataset.\nThe classiﬁer is built on the Transformers 9 pack-\nage in Hugging Face. The language model we\nemploy is RoBERTa (Liu et al., 2019). Each utter-\nance is padded by the <pad> token of RoBERTa\nif it is less than the maximum length of 128. The\nmaximum number of utterances in a dialogue is\nset to 36, 25, 72 and 25 respectively for Daily-\nDialog (Li et al., 2017) 10, MELD (Poria et al.,\n2019) 11, IEMOCAP (Busso et al., 2008) 12 and\nEmoryNLP (Zahiri and Choi, 2018) 13. Dialogues\nwith shorter lengths are padded with NULL. It\nis worth noting that this step is performed after\nRoBERTa due to the random noises introduced\nby RoBERTa. The number of retrieved or gener-\nated events from ATOMIC under the relation types\n‘intentions’ and ‘reactions’ is both set to 5, i.e.,\nK = 5.\n8https://huggingface.co/transformers/\nv2.0.0/examples.html\n9https://huggingface.co/transformers/\n10http://yanran.li/dailydialog.html\n11https://github.com/declare-lab/MELD\n12https://sail.usc.edu/iemocap/iemocap_\nrelease.htm\n13https://github.com/emorynlp/\nemotion-detection",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7534278631210327
    },
    {
      "name": "Conversation",
      "score": 0.7184653282165527
    },
    {
      "name": "Transformer",
      "score": 0.7061899900436401
    },
    {
      "name": "Emotion detection",
      "score": 0.6806693077087402
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.67461758852005
    },
    {
      "name": "Natural language processing",
      "score": 0.5097185969352722
    },
    {
      "name": "Encoder",
      "score": 0.44979360699653625
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4246722459793091
    },
    {
      "name": "Knowledge base",
      "score": 0.3978634774684906
    },
    {
      "name": "Emotion recognition",
      "score": 0.2523353099822998
    },
    {
      "name": "Psychology",
      "score": 0.1382187306880951
    },
    {
      "name": "Communication",
      "score": 0.08278638124465942
    },
    {
      "name": "Engineering",
      "score": 0.07899871468544006
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39555362",
      "name": "University of Warwick",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210090971",
      "name": "Southeast University",
      "country": "BD"
    }
  ],
  "cited_by": 11
}