{
  "title": "hULMonA: The Universal Language Model in Arabic",
  "url": "https://openalex.org/W2971016465",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2970201276",
      "name": "Obeida ElJundi",
      "affiliations": [
        "American University of Beirut"
      ]
    },
    {
      "id": "https://openalex.org/A2971064649",
      "name": "Wissam Antoun",
      "affiliations": [
        "American University of Beirut"
      ]
    },
    {
      "id": "https://openalex.org/A2971344181",
      "name": "Nour El-Droubi",
      "affiliations": [
        "American University of Beirut"
      ]
    },
    {
      "id": "https://openalex.org/A1973326217",
      "name": "Hazem Hajj",
      "affiliations": [
        "American University of Beirut"
      ]
    },
    {
      "id": "https://openalex.org/A2069649042",
      "name": "Wassim El-Hajj",
      "affiliations": [
        "American University of Beirut"
      ]
    },
    {
      "id": "https://openalex.org/A2679858240",
      "name": "Khaled Shaban",
      "affiliations": [
        "Qatar University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1560738869",
    "https://openalex.org/W2252067416",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2770803436",
    "https://openalex.org/W2250816155",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2251137535",
    "https://openalex.org/W2735966564",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2807136170",
    "https://openalex.org/W2767784948",
    "https://openalex.org/W2917085252",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1523296404",
    "https://openalex.org/W2735552604",
    "https://openalex.org/W2250594687",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2805744755",
    "https://openalex.org/W1486649854",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2963216505",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4288347888",
    "https://openalex.org/W2158139315",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2149933564",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4299518610",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2252033489",
    "https://openalex.org/W2574075017",
    "https://openalex.org/W2948433920"
  ],
  "abstract": "Arabic is a complex language with limited resources which makes it challenging to produce accurate text classification tasks such as sentiment analysis. The utilization of transfer learning (TL) has recently shown promising results for advancing accuracy of text classification in English. TL models are pre-trained on large corpora, and then fine-tuned on task-specific datasets. In particular, universal language models (ULMs), such as recently developed BERT, have achieved state-of-the-art results in various NLP tasks in English. In this paper, we hypothesize that similar success can be achieved for Arabic. The work aims at supporting the hypothesis by developing the first Universal Language Model in Arabic (hULMonA - حلمنا meaning our dream), demonstrating its use for Arabic classifications tasks, and demonstrating how a pre-trained multi-lingual BERT can also be used for Arabic. We then conduct a benchmark study to evaluate both ULM successes with Arabic sentiment analysis. Experiment results show that the developed hULMonA and multi-lingual ULM are able to generalize well to multiple Arabic data sets and achieve new state of the art results in Arabic Sentiment Analysis for some of the tested sets.",
  "full_text": "Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 68–77\nFlorence, Italy, August 1, 2019.c⃝2019 Association for Computational Linguistics\n68\nhULMonA (A\tJÒÊg): The Universal Language Model in Arabic\nObeida ElJundi (1) Wissam Antoun (1) Nour El Droubi (1)\nHazem Hajj (1) Wassim El-Hajj (2) Khaled Shaban (3)\n(1) American University of Beirut, Electrical and Computer Engineering Department\n(2) American University of Beirut, Computer Science Department\nBeirut, Lebanon\n(3) Qatar University, Computer Science and Engineering Department, Doha, Qatar\n{oae15;wfa07;ngd02;hh63;we07}@aub.edu.lb;khaled.shaban@qu.edu.qa\nAbstract\nArabic is a complex language with limited re-\nsources which makes it challenging to pro-\nduce accurate text classiﬁcation tasks such as\nsentiment analysis. The utilization of transfer\nlearning (TL) has recently shown promising\nresults for advancing accuracy of text classi-\nﬁcation in English. TL models are pre-trained\non large corpora, and then ﬁne-tuned on task-\nspeciﬁc datasets. In particular, universal lan-\nguage models (ULMs), such as recently devel-\noped BERT, have achieved state-of-the-art re-\nsults in various NLP tasks in English. In this\npaper, we hypothesize that similar success can\nbe achieved for Arabic. The work aims at sup-\nporting the hypothesis by developing th e ﬁrst\nUniversal L anguage Mo del in Arabic (hUL-\nMonA - A \tJ Ò Ê g meaning our dream), demon-\nstrating its use for Arabic classiﬁcations tasks,\nand demonstrating how a pre-trained multi-\nlingual BERT can also be used for Arabic. We\nthen conduct a benchmark study to evaluate\nboth ULM successes with Arabic sentiment\nanalysis. Experiment results show that the\ndeveloped hULMonA and multi-lingual ULM\nare able to generalize well to multiple Arabic\ndata sets and achieve new state of the art re-\nsults in Arabic Sentiment Analysis for some\nof the tested sets.\n1 Introduction\nTransfer learning (TL) with universal language\nmodels (ULMs) have recently shown to achieve\nstate of the art accuracy for several natural lan-\nguage processing (NLP) tasks (Devlin et al., 2018;\nHoward and Ruder, 2018; Radford et al., 2018).\nULMs are trained unsupervised to provide an in-\ntrinsic representation of the language using large\ncorpora that do not require annotations. These\nmodels can then be ﬁne-tuned in a supervised\nmode with much smaller annotated training data\nto achieve a particular NLP task. The established\nsuccess in English with limited data sets makes\nULMs an attractive option for Arabic consider-\nation since Arabic has limited amount of anno-\ntated resources. Early language models focused\non vector embeddings for words and provided\nword-level vector representations (Mikolov et al.,\n2013; Pennington et al., 2014; Bojanowski et al.,\n2017), sentence embeddings (Cer et al., 2018), and\nparagraph embeddings (Le and Mikolov, 2014;\nKiros et al., 2015). These early models were\nable to achieve success comparable to models that\nwere trained only on speciﬁc tasks. More re-\ncently, the language model representation was ex-\ntended to cover a broader representation for text.\nBERT (Devlin et al., 2018), ULMFiT (Howard\nand Ruder, 2018), and OpenAI GPT (Radford\net al., 2018) are examples of such new pre-trained\nlanguage models and which were able to achieve\nstate of the art results in many NLP tasks.\nHowever, in the ﬁeld of Arabic NLP, such\nULMs have not been explored yet. The use of\ntransfer learning in Arabic has been mainly fo-\ncused on word embedding models (Dahou et al.,\n2016; Soliman et al., 2017). Among the recently,\ndeveloped ULM models, BERT (Devlin et al.,\n2018) built a multilingual language version using\n104 languages including Arabic but this model has\nonly been tested on Arabic ”sentence contradic-\ntion” task. One advantage of the multi-lingual\nBERT is that it can be used for many languages.\nHowever, one important limitation is that it was\nconstrained to parallel multi-lingual corpora and\ndid not take advantage of much larger corpora set\n69\navailable for Arabic, making its intrinsic repre-\nsentation limited for Arabic. As a result, there\nis an opportunity to further improve the potential\nfor ULM success by developing an Arabic speciﬁc\nULM.\nIn this paper, we aim at advancing perfor-\nmance and generalization capabilities of Arabic\nNLP tasks by developing new ULMs for Ara-\nbic. We develop the ﬁrst Arabic speciﬁc ULM\nmodel, called hULMonA. Furthermore, we show\nhow pre-trained multi-lingual BERT can be ﬁne\ntuned and applied for Arabic classiﬁcation tasks.\nWe also conduct a benchmark study to evaluate\nthe success potentials for the ULMs with Arabic\nsentiment analysis. We consider several datasets\nin the evaluation and show the superiority of the\nmethods’ generalization handling both MSA and\ndialects. The results show the superiority of the\nmodels compared to state of the art. We further\nshow that even though the multi-lingual BERT\nwas not trained for dialects, it still achieves state\nof the art for some of the dialect data sets.\nIn summary, our contributions are: 1. The de-\nvelopment of hULMonA, the ﬁrst Arabic speciﬁc\nULM, 2. the ﬁne tuning of multi-lingual BERT\nULM for Araic sentiment analysis, and 3. the col-\nlection of a benchmark dataset for ULM evalua-\ntion with sentiment analysis\nThe rest of the paper is organized as follows:\nSection 2 provides a survey of previous work in\nlanguage development for English and Arabic.\nSection 3 presents a description of the method-\nologies to develop the targeted ULMs and the de-\nscription of the benchmark data set. Section 4\npresents the experiment results. Finally, section\n5 concludes the paper.\n2 Related Work\nThis section describes the use of language models\nfor NLP tasks. Historically, language models can\nbe categorized into representations at word level\nand representation of larger units of text such as\nphrases, sentences, or documents. We will call the\nsecond sentence level representation.\n2.1 Language Models for English\n2.1.1 Word-level Models for English\nThe word-level language model is based on the\nuse of pre-trained embedding vectors as additional\nfeatures to the model. The most common em-\nbedding vectors used are word embeddings. With\nword embeddings, each word is linked to a vector\nrepresentation in a way that captures semantic re-\nlationships (Mikolov et al., 2013). The most com-\nmon word embeddings used in deep learning are\nword2vec (Mikolov et al., 2013), GloVe (Penning-\nton et al., 2014), and FastText (Bojanowski et al.,\n2017). Other embedding vectors have been also\nproposed for longer texts such as vectors at the\nsentence level (Cer et al., 2018) and at the para-\ngraph level (Le and Mikolov, 2014; Kiros et al.,\n2015). The use of these embedding vectors has\nshown signiﬁcant improvement compared to train-\ning models from scratch (Turian et al., 2010). One\nof the recent feature-based approaches is ELMo\n(Peters et al., 2018) which is based on the use\nof bidirectional LSTM models. Unlike the tradi-\ntional word embedding representations mentioned\npreviously, ELMo word embeddings are functions\nof the whole sentence which enables capturing\ncontext-related meanings. The use of these word\nembeddings was shown to improve the state-of-\nthe-are results in six NLP tasks such as sentiment\nanalysis and question answering.\n2.1.2 Sentence-level Language Models for\nEnglish\nIn contrast to word-level representation, sentence\nlevel representation develops language model\nwhich can then be ﬁne-tuned for a supervised\ndownstream task (Devlin et al., 2018). The ad-\nvantage of these pre-trained language models is\nthat very few parameters have to be learned from\nscratch. The use of the pre-trained language\nmodels has shown to result in a better perfor-\nmance than the use of the feature-based approach\n(Howard and Ruder, 2018). Several pre-trained\nlanguage models have been proposed recently that\nwere able to achieve state-of-the-art results in\nmany NLP tasks. One of these language models is\nOpenAI GPT (Radford et al., 2018) which uses the\nTransformer network (Vaswani et al., 2017) that\nenables them to capture a long range of linguistic\ninformation. This is in contrast with ELMo (Pe-\nters et al., 2018) which uses the short-range LSTM\nmodels. OpenAI GPT was able to achieve state-\nof-the-art results in several sentence-level NLP\ntasks from the GLUE benchmark (Wang et al.,\n2018) such as question-answering and textual en-\ntailment.\nAnother proposed pre-trained language model\nis ULMFiT (Howard and Ruder, 2018) which is\nbased on a three-layer LSTM architecture, called\n70\nAWD-LSTM (Merity et al., 2017). This language\nmodel was able to achieve state-of-the-art results\nin six text classiﬁcation tasks with just a few task-\nspeciﬁc ﬁne-tuning.\nIn addition to these language models, one of the\nmost recent and innovative pre-trained language\nmodels is BERT (Devlin et al., 2018). BERT is\nbased on the use of the recently introduced Trans-\nformer attention networks (Vaswani et al., 2017).\nBERT uses the bidirectional part of the Trans-\nformer architecture which is the encoder which\nenabled the language model to capture both left\nand right context. This innovation enabled BERT\nto achieve remarkable improvements compared to\nprevious models and to achieve state-of-the-art re-\nsults in eleven NLP tasks with the addition of just\none output layer.\n2.2 Language Models for Arabic\nSome word embedding models were built us-\ning multiple languages such as Polyglot (Al-Rfou\net al., 2013) which was built using 117 languages\nincluding the Arabic language. This model was\nthen tested in multilingual NLP tasks. In addition\nto that, building on the word embedding methods\ndeveloped for English, several approaches were\ndone to build word embeddings for MSA and di-\nalectal Arabic. The ﬁrst approach is AraVec (Soli-\nman et al., 2017) which was built using a large\nArabic corpus collected from Twitter, Internet, and\nWikipedia articles. Another model was proposed\nby Dahou et al. (Dahou et al., 2016) in which Ara-\nbic word embeddings were built using a 3.4 billion\nwords corpus.\nFor sentence-level representations, there has\nbeen a development of multi-lingual models us-\ning parallel corpora. As an example, multilingual\nBERT (Devlin et al., 2018) was built using 104\nlanguages including Arabic. However, there has\nnot been any Arabic only language models. More-\nover, Bert was experimented on several NLP tasks,\nbut sentiment analysis was not one of them.\n2.3 Arabic Sentiment Analysis\nIn (Abdul-Mageed and Diab, 2014), a large-scale,\nmulti-genre, multi-dialect lexicon named SANA\nwas built for the sentiment analysis of Arabic di-\nalects. This lexicon covers the MSA, the Egyptian\ndialect, and the Levantine Arabic. SANA has sev-\neral features which are the part of speech (POS)\ntagger and diacritics, number, gender, and ratio-\nnality. Despite this lexicons coverage, it was still\nnot complete, and many terms were not present. In\n(Abdul-Mageed and Diab, 2012), Abdul Majeed et\nal. worked on expanding a polarity lexicon which\nwas built on MSA using existing English polarity\nlexica. The problems faced with this lexicon was\nthat many terms that existed in social media were\nnot found in the lexicon. Hence, the coverage of\ndialectical Arabic was poorly achieved using this\nlexicon.\nIn the work of Duwairi (Duwairi, 2015), senti-\nment analysis was done on tweets where dialec-\ntical Arabic words were present. This work used\nboth the supervised and unsupervised approaches\nto build the model. To deal with dialectical words,\na dialect lexicon was created in which two an-\nnotators mapped each dialectical word to its cor-\nresponding Modern Standard Arabic word. Two\nclassiﬁers were used to train the model which\nare the Naive Bayes (NB) and the Support Vec-\ntor Machines (SVM). The model was then tested\nusing a dataset of 22,550 tweets written in Arabic\nand that contain dialectical Arabic words. Test-\ning was done on the dataset when the dialect lex-\nicon was used and when it was not used. Results\nshowed some improvement on the Macro-Recall\nwhen the dialect lexicon was used on the NB clas-\nsiﬁer. However, the improvement was negligible\non the SVM classiﬁer and the precision and the\nrecall were even negatively affected when classi-\nfying the negative and the Neutral classes using\nboth classiﬁers.\nRecently, deep learning models were the main\nfocus of Arabic NLP researchers (Badaro et al.,\n2019). The ﬁrst deep learning attempt was con-\nducted by (Al Sallab et al., 2015) who explored\nfour deep learning models, namely Deep Neural\nNetwork (DNN), Deep Believe Network (DBN),\nDeep Auto Encoder (DAE), and RAE. The sen-\ntiment lexicon ArSenL (Badaro et al., 2014) was\nutilized to represent the text vector space. In a fol-\nlow up work, (Al-Sallab et al., 2017) proposed a\nrecursive deep learning model for opinion mining\nin Arabic (AROMA) to address some limitations\nof using RAE for Arabic. To address the morpho-\nlogical richness and orthographic ambiguity of the\nArabic language, (Baly et al., 2017) proposed the\nﬁrst Arabic Sentiment Treebank (ARSENTB) and\ntrained RNTN to outperform AROMA. AraVec\nword embeddings (Soliman et al., 2017) were uti-\nlized by (Badaro et al., 2018) to win SemEval\n2018 (Mohammad et al., 2018). (Dahou et al.,\n71\n2016) and (Dahou et al., 2019) investigated a CNN\narchitecture similar to (Kim, 2014) trained on lo-\ncally trained word embeddings to achieve signiﬁ-\ncant results.\nDespite all this emerging progress in Arabic\nsentiment analysis, transfer learning was utilized\nby only using a single layer of weights - usually\nthe ﬁrst layer - known as embeddings. However,\ntypical neural network architecture consists of sev-\neral layers, and utilizing transfer learning for only\nthe ﬁrst layer was clearly just scratching the sur-\nface of what is possible.\n3 Methodology\nIn this section, we describe how we constructed\nhULMonA and how we then tuned both hUL-\nMonA and the multi-lingual BERT ULM for Ara-\nbic classiﬁcation tasks.\nThe high-level architecture for using a ULM\nmodel is shown in Figure 1. The complete model\nconsists of the combination of a pre-trained ULM\nmodel and additional task-speciﬁc layers for the\ndesired tasks. Once a ULM model is developed,\nthe learning process becomes limited to learning\nthe parameters of the additional layers. This trans-\nfer learning process is referred to as ﬁne-tuning\nwith ULM and this is the main beneﬁt of using\nULMs.\nFigure 1: High Level Architecture for ULM Tranfer\nLearning\nBelow, we describe the data pre-processing step\nrequired for Arabic and the ﬁne tuning process for\nthe additional layers.\n3.1 Arabic Speciﬁc ULM: hULMonA\nTransfer Learning implies that training a model\nwhich already has some language knowledge per-\nforms better, converges faster, and requires less\ndata for new task when comparing to training\nfrom raw text. Language modeling is consid-\nered the ideal task to obtain general understand-\ning of a particular language due to its ability of\ncapturing many aspects of language relevant for\ndownstream tasks, such as long-term dependen-\ncies (Linzen et al., 2016), hierarchical relations\n(Gulordava et al., 2018), and sentiment orientation\n(Radford et al., 2017).\nInspired by the Universal Language Model\nFine-tuning (ULMFiT) (Howard and Ruder,\n2018), we propose, develop, and make available\nfor public1, the ﬁrst ULM in Arabic (hULMonA -\nA \tJ Ò Ê g ) that is trained on large general-domain\nArabic corpus and can be ﬁne-tuned on any tar-\nget task to achieve signiﬁcant results. hULMonA,\nillustrated in Figure 2, consists of three main\nstages: 1. pretraining the state-of-the-art language\nmodel AWD-LSTM (Merity et al., 2017) on a\nhuge Wikipedia corpus (section 3.1.1), 2. ﬁne-\ntuning the pretrained language model on a target\ndataset (section 3.1.2), 3. and adding a classi-\nﬁcation layer on top of the ﬁne-tuned language\nmodel for the purpose of text classiﬁcation (sec-\ntion 3.1.3).\n3.1.1 General domain huLMonA pretraining\nTo capture the various properties of a language,\nwe constructed a large scale Arabic language\nmodeling dataset by extracting text from Arabic\nWikipedia. The 600K Wikipedia articles were\nused to train a three layers of the start-of-the-\nart language model architecture, namely AWD-\nLSTM (Merity et al., 2017). The output of this\nstage is the model weights and the distributional\nrepresentations of each word in the constructed\ncorpus, also know as word embeddings. Although\nWikipedia text is mainly in MSA, the resultant\npretrained model can be ﬁne-tuned later on differ-\nent text text genres (e.g., tweets) and Arabic di-\nalects to outperform training from scratch. Due\nto the huge amount of text and model parameters,\nespecially at the last softmax layer which has as\nmany neurons as the vocabulary size, the pretrain-\ning stage consumes much time and computational\npower. Fortunately, pretraining is done once, and\nthe resultant model is made available to the com-\nmunity.\n3.1.2 Target task huLMonA ﬁne-tuning\nRegardless of the diversity of the general-domain\ndata, the target task data will likely come from\n1http://www.oma-project.com/\n72\n(a) LM pre-training\n (b) LM ﬁne-tuning\n (c) classiﬁcation\nFigure 2: Three-step Process for Creating hULMonA\na different distribution. Although the general-\ndomain LM is trained on MSA, most Arabic\ndatasets and social media platforms contains di-\nalects. Unlike MSA, dialects have no standard or\ncodiﬁed form and are inﬂuenced by region speciﬁc\nslang. Thus, ﬁne-tuning the pretrained general-\ndomain LM on the target task data is necessary\nfor the LM to adapt to the new textual properties.\nOne difference though is that ﬁne-tuning utilizes\ndifferent learning rates for different layers, which\nis referred to as discriminative ﬁne-tuning. This\nis crucial since different layers capture different\ntypes of information (Yosinski et al., 2014). Dis-\ncriminative ﬁne-tuning updates the model param-\neters as follows:\nθl\nt = θl\nt−1 −ηl ·∇θl J(θ)\nwhere θl is the model parameters of layer l, and ηl\nis the learning rate of layer l.\n3.1.3 Augmenting hULMonA with target\ntask classiﬁcation layers\nFinally, two fully connected layers are added to\nthe LM for classiﬁcation with ReLU and Softmax\nactivations respectively. At ﬁrst, the two fully con-\nnected layers are trained from scratch, while pre-\nvious layers are frozen. After each epoch, the next\nlower frozen layer is unfrozen and ﬁne-tuned until\nconvergence. This is known as gradual unfreezing,\nand it is essential to avoid catastrophic forgetting\nof the information captured during language mod-\neling.\n3.2 Multi-lingual BERT ULM for Arabic\ntasks\n3.2.1 Data Pre-processing\nThe ULM BERT model requires a special format\nfor the data before feeding the model. A special\ntoken, called [CLS], is added at the beginning of\nevery sentence and a special token, called [SEP]\nis added at the end of every sentence. For Ara-\nbic tokenization, we chose WordPiece(Wu et al.,\n2016) tokenizer as it was also used during the pre-\ntraining of BERT. Figure 3 presents a sentence be-\nfore and after going through the BERT tokenizer.\nFigure 3: BERT Tokenizer Results\nThe tokenizer splits sentences into WordPiece\ntokens separated by ##. After tokenization, each\nword is mapped to an index using a 110k token\nvocabulary ﬁle that is provided by BERT for all\nthe languages.\n73\n3.2.2 Model Fine Tuning\nFor sentiment analysis, or other Multi-label classi-\nﬁcation problems, a linear (fully-connected) layer\nwith a standard softmax activation function is\nadded to the last hidden state of the ﬁrst token (the\n[CLS] token) as shown in Figure 4. With a hidden\nstate vector C ∈RH where H is the dimension of\nthe hidden state and a fully-connected classiﬁca-\ntion layer with weights W ∈RK×H where K is\nthe number of classiﬁcation labels, the label prob-\nability after applying the softmax function is then\nP = softmax(CWT).\nFigure 4: BERT Fine-Tuning Model Architecture\n3.3 Benchmark Dataset for ULM Evaluation\nwith Sentiment Analysis\nTo provide credible evaluation for the performance\nof the two ULM’s, we catalog a benchmark dataset\nfor Arabic which can also be used for future re-\nsearch benchmark evaluations. The data sets vary\nin size allowing us to demonstrate the ULM’s abil-\nities to ﬁne tune with little data and achieve high\nperformance. The benchmark data set is summa-\nrized in table 1 along with statistics on its content.\n3.3.1 HARD data set\nThe Hotel Arabic Reviews Dataset (HARD) (El-\nnagar et al., 2018) is a dataset of hotel reviews\nwritten in Modern Standard Arabic and Arabic di-\nalect classiﬁed into positive and negative. The\ndataset consists of a corpus of 93,700 hotel re-\nviews which are equally divided into 46,850 pos-\nitive reviews and 46.850 negative reviews. The\ndataset is structured in columns containing the\nnumber of the review, the name of the hotel, the\nrating given by the user, the type of the user, the\ntype of the room, the number of nights stayed, and\nthe review. Reviews have been classiﬁed into pos-\nitive and negative according to the rating given by\nthe user. A negative review is deﬁned by a rating\nof 1 or 2 and a positive review is deﬁned by a rat-\ning of 4 or 5. Neutral reviews of rating 3 were\nignored in this dataset.\n3.3.2 ASTD data set\nThe Arabic Sentiment Tweets Dataset (ASTD)\n(Nabil et al., 2015) is a corpus of 10,000 tweets\nwritten in MSA and Egyptian dialect. The un-\nbalanced dataset has been manually annotated and\nstructured in columns containing the tweet and its\nsentiment whether it is objective, neutral, positive,\nor negative. The dataset consists of 777 positive\ntweets, 1,642 negative tweets, 805 neutral tweets,\nand 6,466 objective tweets. A balanced version,\ncalled ASTD-B, is created as well taking into ac-\ncount positive and negative tweets only.\n3.3.3 ArSenTD-Lev\n(Baly et al., 2018) developed The Arabic Sen-\ntiment Twitter Dataset for LEVantine dialect\n(ArSenTD-Lev), a corpus of 4,000 tweets col-\nlected from Levantine countries (Palestine, Jordan,\nSyria, and Lebanon) and annotated for sentiment,\ntopic, target, etc.\n4 Experiments and Results\nIn this section, we discuss in detail the experi-\nments that were conducted to evaluate the devel-\nopment of hULMonA, ﬁne-tuning of hULMonA\nand BERT, and and testing the performance of the\nmodels with sentiment analysis. The benchmark\ndata set was used to ﬁne tune both models and pro-\nvide different evaluations.\n4.1 Experimental Setup\nWe evaluate our work on four widely-studied Ara-\nbic sentiment analysis datasets, with varying num-\nbers of sentences and dialects. All used datasets\nare described in details in section 3.3, and datasets\nstatistics are shown in table 1. Following previous\nworks, 20% of the data was held out for testing for\nsome datasets, while other datasets were tested on\n10%.\n74\nDataset Resource # samples # classes MSA ||Dialect\nHARD Hotel reviews 93,700 2 MSA & Gulf\n(Elnagar et al., 2018) (www.booking.com)\nASTD Twitter 10,000 4 MSA & Egyptian\n(Nabil et al., 2015)\nASTD-B Twitter 1,600 2 MSA & Egyptian\n(Nabil et al., 2015)\nArSenTD-Lev Twitter 4,000 5 Levantine Dialect\n(Baly et al., 2018)\nTable 1: Datasets statistics\nInitial tokens Generated sequence\nPñ\u0010J»YË@ ñJ\n\tKñK\n ú\n\t¯ YËð , ø \n Xñª \u0011IkAK. ð I. \u0010KA¿ , \támÌ'@ YÔ g@ Pñ \u0010J»YË@\n(Doctor) (Doctor Ahmad Al Hassan is a Saudi writer and researcher. He was born in June)\nÐY\u0010¯ èQ» I. «B ¡ð I. «C¿ I. ªÊK\n ú\n¾K\nQÓ@ ÐY \u0010¯ èQ» I. «B\n(football player) (American football player plays as midﬁeld)\néËðX © \u0010®\u0010Kð ¡ðB@ \u0010Qå\u0011Ë@ ú\n\t¯ èYj \u0010JÖÏ@ éJ\nK. QªË@ \u0010H@PAÓB@ éËðX © \u0010®\u0010Kð\n(The country is located) (United Arab Emirates is located in the middle east)\nTable 2: generating text using the pretrained Arabic language model\n4.2 hULMonA Model Training\nhULMonA was constructed by ﬁrst extracting and\npreprocessing all Arabic Wikipedia articles up to\nMarch of 2019. Articles images, links, and HTML\nwere removed using an online tool 2, and articles\nwith less than 100 characters were excluded result-\ning in 600,559 Arabic articles consisting of 108M\nwords, 4M of which were unique.\nThe large number of unique words requires\nmore parameters to be learnt and is more prone to\noverﬁtting. This problem is called lexical sparsity,\nand it is a well-known challenge in Arabic NLP.\nTherefore, text was preprocessed by replacing\nnumbers by a special token, normalizing Alif and\nTa-marbota, separating punctuations from words\nby a white space, and removing diacritics and non-\nArabic tokens. Moreover, MADAMIRA (Pasha\net al., 2014), an Arabic morphological analyzer\nand disambiguator, was utilized to separate words\npreﬁxes, such as Al-taareef (the), and sufﬁxes,\nsuch as possessive pronouns, resulting in words\nstems, thus, reducing lexical sparsity. Table 3\nshows the number of unique words before and af-\nter preprocessing Arabic text using MADAMIRA.\nFinally, tokens that appeared less than 5 times\nwere replaced by a special token.\nThe preprocessed text was then fed to train a\n2https://github.com/attardi/\nwikiextractor\nExample Unique\ntokens\nBefore \u0010ém\u001a\r'@QË@ð \tàñÊË@ \u0010éÖß \nY« \u0014\u0010é \t¯A \t® \u0011 \u0014\u0010èXAÓ ZAÖ Ï@ 4.1M\nAfter\néÖß \nY« é \t¯A \t® \u0011 èXAÓ ZAÓ + È@\nékZ@P + È@ +ð \tàñË + È@ 9.1K\nTable 3: preprocessing reduces lexical sparsity\nthree layers AWD-LSTM for 4 epochs to predict\nnext token given current sequence of tokens. Each\nepoch took around 200 minutes on an i7 CPU with\n32 GB of RAM and Nvidia GTX 1080 GPU. We\nused a dropout of 0.1 with learning rate of 3e-3,\nand to account for GPU VRAM limitations, we\nwere limited with batch sizes equal to 32. 10%\nof the data was held out for testing. Table 2\ndemonstrates the capabilities of the pretrained lan-\nguage model of generating Arabic sequence based\non initial tokens. The Arabic language model\ndataset, code, and pre-trained weights are publicly\navailable through the Opinion Mining for Arabic\n(OMA) website3.\n4.3 hULMonA Evaluation for Arabic\nSentiment Analysis\nTo perform sentiment analysis, we ﬁne-tuned the\npretrained ULMs on a target dataset; meaning we\n3http://www.oma-project.com/\n75\nDataset SOTA Results hULMonA BERT\nHARD 93.1-93.2 (Elnagar et al., 2018) 95.7-95.7 95.7-95.7\nASTD 62.0-68.7 (Nabil et al., 2015) 67.7-69.9 67.0-77.1\nASTD-B 82.5-82.4 (Dahou et al., 2019) 85.8-86.5 80.0-80.1\nArSenTD-Lev 50.0-51.0 (Baly et al., 2018) 51.1-52.4 51.0-51.0\nTable 4: Comparison of results (F1-Accuracy) obtained using hULMonA and other state-of-the-art models\nresume training the language model to predict the\nnext token but with a sentiment dataset instead of\nWikipedia. Fine-tuning improved the model by\nadapting to new words (e.g., dialects) or words\nthat may convey several meanings. Fine-tuning\nwas done on each of the data sets in the afore-\nmentioned benchmark data separately and utiliz-\ning different learning rates for different layers,\nranging from 2e-5 to 1e-3. Finally, after adding a\nclassiﬁcation layer, the network was trained by un-\nfreezing one layer after each epoch, starting from\nthe output layer. Results are reported in table 4.\nNote that hULMonA outperformed the state-of-\nthe-art in four Arabic sentiment analysis datasets,\ndemonstrating the beneﬁt of transferring knowl-\nedge from a large corpus into small and dialectal\ndatasets.\n4.4 BERT ULM Model Fine Tuning for\nArabic Sentiment Analysis\nBERT was ﬁne-tuned on the different datasets in-\ndependently. The learning rate and number of\nepochs used for each dataset are shown in table\n5. Batch size was also ﬁxed for BERT at 32 due\nto our hardware memory limitations. Fine-tuning\ntook 90 ~100 seconds for every 3000 data-point on\nGoogle’s Colaboratory TensorFlow environment\nwith GPU acceleration. BERT Base Multilin-\ngual Cased used as it is recommended in BERT’s\ngithub repository 4 and the pre-trained weights\nwere downloaded from TensorFLow’s Hub5.\nDataset Learning Rate # of Epochs\nHARD 10−5 3\nASTD 10−5 5\nASTD-B 10−5 5\nAJGT 2 ×10−5 6\nArSenTD-Lev 2 ×10−5 5\nTable 5: Learning rate and number of epochs used for\ntraining each dataset\n4https://github.com/google-research/\nbert/blob/master/multilingual.md\n5https://tfhub.dev/f/google\n4.5 BERT ULM Evaluation for Arabic\nSentiment Analysis\nThe results obtained are compared to state-of-\nthe-art models and presented in Table 4. Even-\nthough BERT achieved state-of-the-art results on\ntwo benchmark datasets, during the evaluation,\nwe noticed that the BERT multilingual tokenizer\nfailed to tokenize Arabic sentences as seen in\nFigure 3. This tokenizer could have limited the\nmodel’s accuracy and compromised the model’s\nArabic pre-training.\n5 Conclusion\nThis works aims at utilizing transfer learning to\ndevelop the ﬁrst Arabic universal language model,\nhULMonA, that can be ﬁne-tuned for almost any\nArabic text classiﬁcation task. Language knowl-\nedge learnt unsupervisedly from general-domain\ndataset is transferred to target task to improve\noverall performance and generalization. We show\nthat hULMonA outperforms several state-of-the-\nart Arabic sentiment analysis datasets, and we\nmake hULMonA available for the community. In\naddition, we evaluate another ULM, BERT, and\ncompare results.\nAs a future work, we aim at utilizing hULMonA\nto improve more Arabic NLP tasks such as emo-\ntion recognition, cyberbullying detection, question\nanswering, etc. Moreover, we plan to develop Ara-\nbic speciﬁc BERT by improving its limited tok-\nenizer and training on Arabic only instead of mul-\ntiple languages at once.\nReferences\nMuhammad Abdul-Mageed and Mona Diab. 2012. To-\nward building a large-scale arabic sentiment lexicon.\nIn Proceedings of the 6th international global Word-\nNet conference, pages 18–22.\nMuhammad Abdul-Mageed and Mona T Diab. 2014.\nSana: A large scale multi-genre, multi-dialect lex-\nicon for arabic subjectivity and sentiment analysis.\nIn LREC, pages 1162–1169.\n76\nRami Al-Rfou, Bryan Perozzi, and Steven Skiena.\n2013. Polyglot: Distributed word represen-\ntations for multilingual nlp. arXiv preprint\narXiv:1307.1662.\nAhmad Al-Sallab, Ramy Baly, Hazem Hajj,\nKhaled Bashir Shaban, Wassim El-Hajj, and\nGilbert Badaro. 2017. Aroma: A recursive deep\nlearning model for opinion mining in arabic as a low\nresource language. ACM Transactions on Asian and\nLow-Resource Language Information Processing\n(TALLIP), 16(4):25.\nAhmad Al Sallab, Hazem Hajj, Gilbert Badaro, Ramy\nBaly, Wassim El Hajj, and Khaled Bashir Shaban.\n2015. Deep learning models for sentiment analysis\nin arabic. In Proceedings of the second workshop on\nArabic natural language processing, pages 9–17.\nGilbert Badaro, Ramy Baly, Hazem Hajj, Wassim El-\nHajj, Khaled Bashir Shaban, Nizar Habash, Ahmad\nAl-Sallab, and Ali Hamdi. 2019. A survey of opin-\nion mining in arabic: A comprehensive system per-\nspective covering challenges and advances in tools,\nresources, models, applications, and visualizations.\nACM Transactions on Asian and Low-Resource Lan-\nguage Information Processing (TALLIP), 18(3):27.\nGilbert Badaro, Ramy Baly, Hazem Hajj, Nizar\nHabash, and Wassim El-Hajj. 2014. A large scale\narabic sentiment lexicon for arabic opinion mining.\nIn Proceedings of the EMNLP 2014 workshop on\narabic natural language processing (ANLP), pages\n165–173.\nGilbert Badaro, Obeida El Jundi, Alaa Khaddaj, Alaa\nMaarouf, Raslan Kain, Hazem Hajj, and Wassim El-\nHajj. 2018. Ema at semeval-2018 task 1: Emotion\nmining for arabic. In Proceedings of The 12th Inter-\nnational Workshop on Semantic Evaluation, pages\n236–244.\nRamy Baly, Hazem Hajj, Nizar Habash, Khaled Bashir\nShaban, and Wassim El-Hajj. 2017. A senti-\nment treebank and morphologically enriched recur-\nsive deep models for effective sentiment analysis\nin arabic. ACM Transactions on Asian and Low-\nResource Language Information Processing (TAL-\nLIP), 16(4):23.\nRamy Baly, Alaa Khaddaj, Hazem Hajj, Wassim El-\nHajj, and Khaled Shaban. 2018. Arsentd-lev: A\nmulti-topic corpus for target-based sentiment anal-\nysis in arabic levantine tweets. In Proceedings\nof the Eleventh International Conference on Lan-\nguage Resources and Evaluation (LREC 2018) ,\nParis, France. European Language Resources Asso-\nciation (ELRA).\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\net al. 2018. Universal sentence encoder. arXiv\npreprint arXiv:1803.11175.\nAbdelghani Dahou, Mohamed Abd Elaziz, Junwei\nZhou, and Shengwu Xiong. 2019. Arabic senti-\nment classiﬁcation using convolutional neural net-\nwork and differential evolution algorithm. Compu-\ntational Intelligence and Neuroscience, 2019.\nAbdelghani Dahou, Shengwu Xiong, Junwei Zhou,\nMohamed Houcine Haddoud, and Pengfei Duan.\n2016. Word embeddings and convolutional neural\nnetwork for arabic sentiment classiﬁcation. In Pro-\nceedings of coling 2016, the 26th international con-\nference on computational linguistics: Technical pa-\npers, pages 2418–2427.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nRehab M Duwairi. 2015. Sentiment analysis for di-\nalectical arabic. In 2015 6th International Confer-\nence on Information and Communication Systems\n(ICICS), pages 166–170. IEEE.\nAshraf Elnagar, Yasmin S Khalifa, and Anas Einea.\n2018. Hotel arabic-reviews dataset construction for\nsentiment analysis applications. In Intelligent Natu-\nral Language Processing: Trends and Applications,\npages 35–52. Springer.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Color-\nless green recurrent networks dream hierarchically.\narXiv preprint arXiv:1803.11138.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\nYoon Kim. 2014. Convolutional neural networks\nfor sentence classiﬁcation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746–1751,\nDoha, Qatar. Association for Computational Lin-\nguistics.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems,\npages 3294–3302.\nQuoc Le and Tomas Mikolov. 2014. Distributed repre-\nsentations of sentences and documents. In Interna-\ntional conference on machine learning, pages 1188–\n1196.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. Transactions of the Associ-\nation for Computational Linguistics, 4:521–535.\n77\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and optimizing lstm lan-\nguage models. arXiv preprint arXiv:1708.02182.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nSaif Mohammad, Felipe Bravo-Marquez, Mohammad\nSalameh, and Svetlana Kiritchenko. 2018. Semeval-\n2018 task 1: Affect in tweets. In Proceedings of\nThe 12th International Workshop on Semantic Eval-\nuation, pages 1–17.\nMahmoud Nabil, Mohamed Aly, and Amir Atiya.\n2015. Astd: Arabic sentiment tweets dataset. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, pages\n2515–2519.\nArfath Pasha, Mohamed Al-Badrashiny, Mona T Diab,\nAhmed El Kholy, Ramy Eskander, Nizar Habash,\nManoj Pooleery, Owen Rambow, and Ryan Roth.\n2014. Madamira: A fast, comprehensive tool for\nmorphological analysis and disambiguation of ara-\nbic. In LREC, volume 14, pages 1094–1101.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532–1543.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv:1704.01444.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language under-\nstanding paper. pdf.\nAbu Bakr Soliman, Kareem Eissa, and Samhaa R El-\nBeltagy. 2017. Aravec: A set of arabic word embed-\nding models for use in arabic nlp. Procedia Com-\nputer Science, 117:256–265.\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: a simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th annual meeting of the association for compu-\ntational linguistics, pages 384–394. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amapreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in neural information\nprocessing systems, pages 3320–3328.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8499071598052979
    },
    {
      "name": "Natural language processing",
      "score": 0.7787365913391113
    },
    {
      "name": "Arabic",
      "score": 0.7426756620407104
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6751493215560913
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5816743969917297
    },
    {
      "name": "Task (project management)",
      "score": 0.5757657289505005
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5310718417167664
    },
    {
      "name": "Language model",
      "score": 0.5129536986351013
    },
    {
      "name": "Modern Standard Arabic",
      "score": 0.45113876461982727
    },
    {
      "name": "Meaning (existential)",
      "score": 0.4259517192840576
    },
    {
      "name": "Linguistics",
      "score": 0.26215726137161255
    },
    {
      "name": "Engineering",
      "score": 0.05966201424598694
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98635879",
      "name": "American University of Beirut",
      "country": "LB"
    },
    {
      "id": "https://openalex.org/I60342839",
      "name": "Qatar University",
      "country": "QA"
    }
  ],
  "cited_by": 68
}