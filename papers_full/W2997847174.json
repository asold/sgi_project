{
    "title": "TreeGen: A Tree-Based Transformer Architecture for Code Generation",
    "url": "https://openalex.org/W2997847174",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2124130065",
            "name": "Zeyu Sun",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2230558637",
            "name": "Qihao Zhu",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2150281962",
            "name": "Yingfei Xiong",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2991148458",
            "name": "Yican Sun",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2163770800",
            "name": "Lili Mou",
            "affiliations": [
                "University of Alberta"
            ]
        },
        {
            "id": "https://openalex.org/A2096491802",
            "name": "Lu Zhang",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2124130065",
            "name": "Zeyu Sun",
            "affiliations": [
                "Peking University",
                "Institute of Software"
            ]
        },
        {
            "id": "https://openalex.org/A2230558637",
            "name": "Qihao Zhu",
            "affiliations": [
                "Institute of Software",
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2150281962",
            "name": "Yingfei Xiong",
            "affiliations": [
                "Peking University",
                "Institute of Software"
            ]
        },
        {
            "id": "https://openalex.org/A2991148458",
            "name": "Yican Sun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2163770800",
            "name": "Lili Mou",
            "affiliations": [
                "Peking University",
                "Institute of Software",
                "University of Alberta"
            ]
        },
        {
            "id": "https://openalex.org/A2096491802",
            "name": "Lu Zhang",
            "affiliations": [
                "Peking University",
                "Institute of Software"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2107878631",
        "https://openalex.org/W2798296074",
        "https://openalex.org/W2531409750",
        "https://openalex.org/W6689008102",
        "https://openalex.org/W6755714500",
        "https://openalex.org/W6753916358",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W2175619924",
        "https://openalex.org/W2227250678",
        "https://openalex.org/W2251673953",
        "https://openalex.org/W6697747940",
        "https://openalex.org/W2282866165",
        "https://openalex.org/W2610002206",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W2901813505",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W2103457851",
        "https://openalex.org/W2888128175",
        "https://openalex.org/W2605887895",
        "https://openalex.org/W2890867094",
        "https://openalex.org/W1496189301",
        "https://openalex.org/W2111742432",
        "https://openalex.org/W2963868406",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4293569541",
        "https://openalex.org/W2962728167",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2889467844",
        "https://openalex.org/W2963357517",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2964325845",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2963794306",
        "https://openalex.org/W2963617989"
    ],
    "abstract": "A code generation system generates programming language code based on an input natural language description. State-of-the-art approaches rely on neural networks for code generation. However, these code generators suffer from two problems. One is the long dependency problem, where a code element often depends on another far-away code element. A variable reference, for example, depends on its definition, which may appear quite a few lines before. The other problem is structure modeling, as programs contain rich structural information. In this paper, we propose a novel tree-based neural architecture, TreeGen, for code generation. TreeGen uses the attention mechanism of Transformers to alleviate the long-dependency problem, and introduces a novel AST reader (encoder) to incorporate grammar rules and AST structures into the network. We evaluated TreeGen on a Python benchmark, HearthStone, and two semantic parsing benchmarks, ATIS and GEO. TreeGen outperformed the previous state-of-the-art approach by 4.5 percentage points on HearthStone, and achieved the best accuracy among neural network-based approaches on ATIS (89.1%) and GEO (89.6%). We also conducted an ablation test to better understand each component of our model.",
    "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nTreeGen: A Tree-Based Transformer Architecture for Code Generation\nZeyu Sun,† Qihao Zhu,† Yingfei Xiong,∗† Yican Sun,† Lili Mou,‡ Lu Zhang†\n†Key Laboratory of High Conﬁdence Software Technologies (Peking University), MoE;\nSoftware Institute, Peking University, 100871, P. R. China\n{szy , zhuqh, xiongyf, sycpku, zhanglucs}@pku.edu.cn\n‡University of Alberta, Edmonton, AB, Canada\ndoublepower.mou@gmail.com\nAbstract\nA code generation system generates programming language\ncode based on an input natural language description. State-of-\nthe-art approaches rely on neural networks for code genera-\ntion. However, these code generators suffer from two prob-\nlems. One is the long dependency problem, where a code\nelement often depends on another far-away code element.\nA variable reference, for example, depends on its deﬁnition,\nwhich may appear quite a few lines before. The other problem\nis structure modeling, as programs contain rich structural in-\nformation. In this paper, we propose a novel tree-based neural\narchitecture, TreeGen, for code generation. TreeGen uses the\nattention mechanism of Transformers to alleviate the long-\ndependency problem, and introduces a novel AST reader (en-\ncoder) to incorporate grammar rules and AST structures into\nthe network. We evaluated TreeGen on a Python benchmark,\nHearthStone, and two semantic parsing benchmarks, ATIS\nand GEO. TreeGen outperformed the previous state-of-the-\nart approach by 4.5 percentage points on HearthStone, and\nachieved the best accuracy among neural network-based ap-\nproaches on ATIS (89.1%) and GEO (89.6%). We also con-\nducted an ablation test to better understand each component\nof our model.\nIntroduction\nCode generation is an important artiﬁcial intelligence prob-\nlem that has the potential to signiﬁcantly boost the pro-\nductivity of programmers. Given a speciﬁcation written in\nnatural language, a code generation system translates the\nspeciﬁcation into an executable program. For example, if a\npython programmer gives an instruction “initialize a dictio-\nnary, Dict”, the code generator is expected to automatically\ngenerates “Dict={} ”.\nWith the development deep learning techniques, re-\nsearchers have applied various neural architectures to this\nproblem, such as sequence-to-sequence (Seq2Seq) models\nor sequence-to-tree (Seq2Tree) models (Sutskever, Vinyals,\nand Le 2014; Ling et al. 2016; Yin and Neubig 2017;\nRabinovich, Stern, and Klein 2017; Hayati et al. 2018;\n∗Yingfei Xiong is the corresponding author. The code is avail-\nable at https://github.com/zysszy/TreeGen\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nSun et al. 2019). Especially, state-of-the-art approaches gen-\nerate code by predicting a sequence of grammar rules (Yin\nand Neubig 2017; Rabinovich, Stern, and Klein 2017; Sun\net al. 2019). That is to say, the system keeps a partial ab-\nstract syntax tree (AST) of the already-generated code, and\npredicts the grammar rule to be used to expand a particular\nnode.\nThe classiﬁcation of grammar rules faces two main chal-\nlenges. The ﬁrst challenge is the long-dependency prob-\nlem (Bengio, Simard, and Frasconi 1994). A code element\nmay depend on another far-away element. For example, a\nvariable reference statement “ if len(a) < Max\nLength:”\nat line 100 may depend on a variable deﬁnition statement\n“Max\nLength = 100 ” at line 10. The second challenge\nis the representation of code structures. It is pointed out\nthat the tree-structural information is crucial for modeling\ncode (Mou et al. 2016; Yin and Neubig 2017; Rabinovich,\nStern, and Klein 2017; Sun et al. 2019). However, a “ﬂat”\nneural architecture, such as an RNN, cannot capture struc-\nture information well.\nIn this paper, we propose a novel neural architecture,\nTreeGen, for the code generation. To address the ﬁrst chal-\nlenge, TreeGen adopts the recently proposed Transformer\narchitecture (Vaswani et al. 2017), which is capable of\ncapturing long dependencies. However, the original Trans-\nformer architecture is not designed for programs, and can-\nnot utilize tree structures, i.e., the second above mentioned\nchallenge. A standard way of utilizing structural informa-\ntion, as in graph- and tree-based convolutional neural net-\nworks, is to combine the vector representations of a node\nand its structural neighbors as the output of a structural con-\nvolution sub-layer. However, a standard Transformer archi-\ntecture does not have such structural convolution sub-layers,\nand it is not clear where to add them.\nIt is tempting to add structural convolution sub-layers in\nall the Transformer blocks. Our core conjecture is that when\nconvolving a node and its structural neighbors, the vector\nrepresentation should mainly contain the information from\nthe original node. As the vector representation of the nodes\nis processed by more blocks in the decoder of the Trans-\nformer, they gradually mix in more information from other\nnodes and lose their original information. Therefore, we add\n8984\nroot\nModule\nbody\nAssign\ntargets\nName\nid\nlength\nNum\nn\n10\nvalue\n1: root -> Module\n2: Module -> body\n3: body -> Assign\n10: Assign -> \ntargets value\n11: targets -> Name\n13: Name -> id\n64: id -> length\n18: value -> Num\n19: Num -> n\n8: n -> 10\nRule Sequence:\n1 2 3 10 11 13 64 18 19 8\nFigure 1: A Python AST for code “length = 10”\nthe structural convolution sub-layer only to the ﬁrst several\nTransformer decoder blocks but not all.\nGenerally speaking, the TreeGen architecture consists of\nthree parts: (1) a natural language (NL) reader (encoder)\nencodes the text description; (2) an AST reader (the ﬁrst\nseveral Transformer decoder blocks) encodes the previously\ngenerated partial code with the structural convolution sub-\nlayers; (3) a decoder (the rest Transformer decoder blocks)\ncombines the query (the node to be expanded in AST) and\nthe previous two encoders to predict the next grammar rule.\nWe evaluated our model on an established benchmark\ndataset for Python code generation, HearthStone (Ling et\nal. 2016), which is a Python implementation of a card\ngame HearthStone. The results show that our model sig-\nniﬁcantly outperforms previous models by 4.5 percentage\npoints. We further evaluated our model on two semantic\nparsing datasets, ATIS and GEO, which translate natural lan-\nguage sentences into lambda calculus logical forms. The re-\nsults show that our model has the best accuracy among pre-\nvious neural models, with 89.1% and 89.6% accuracy, re-\nspectively. Our evaluation also shows that adding the struc-\ntural convolution sub-layer to the ﬁrst several Transformer\nblocks signiﬁcantly outperforms a Transformer with struc-\ntural convolution in all blocks.\nOur Model\nWe generate code by predicting the grammar rules of the\nprogramming language. Figure 2 shows the overall picture\nof our model, which comprises three parts: an NL reader, an\nAST reader, and decoder. We introduce them in detail in the\nfollowing subsections.\nGrammar Rule Prediction\nIn this section, we introduce how to model code genera-\ntion as a series of classiﬁcation problems of grammar rules.\nThe programs can be decomposed into several context-free\ngrammar rules and parsed as an AST. For example, Figure 1\nshows a Python AST for the code “ length = 10”, where\ndotted boxes are terminal symbols and solid boxes are non-\nterminal symbols.\nAST-based code generation could be thought of as ex-\npanding a non-terminal node by a grammar rule. This pro-\ncess is repeated until all leaf nodes are terminal. In Figure 1,\n“1: root -> Module” is an example of the grammar rules,\nwhere the preceding number is the ID of the rules. Follow-\ning the pre-order traverse, we could obtain the sequence of\nrules that generate the AST shown in the top right corner.\nFormally, the probability can be factorized as the proba-\nbilities of the rules generating the code following the order.\np(code) =\n∏\nP\ni=1\np(ri | NL input,ri,··· ,ri−1) (1)\nwhere ri is the ith rule in the rule sequence. In this way,\nour task is to train a model to calculate p(ri | NL input,pi),\ni.e., given the natural language description and the currently\ngenerated partial AST the model calculates the probabilities\nof the rules to expand this node.\nNL Reader\nThe input description determines the functionality of the\ncode. It can be a semi-structural description as in the Hearth-\nStone dataset, or a natural language as in ATIS and GEO\nsemantic parsing datasets.\nFor an input description, we ﬁrst tokenize it into to-\nkens n\n1,n2,··· ,nL, where L denotes the length of\nthe input. Each token ni is then split to characters\nc(ni)\n1 ,c(ni)\n2 ,··· ,c(ni)\nS , where S is the number of characters\nin ni. All the tokens and characters are represented as real-\nvalued vectors n1,n2,··· ,nL and c(ni)\n1 ,c(ni)\n2 ,··· ,c(ni)\nS\nby embeddings.\nInput Text Representation\nCharacter Embedding.It often happens that similar words\nhave similar characters (e.g., “program” and “programs”).\nTo utilize this property, we represent a token by character\nembeddings with a fully-connected layer\nn\n(c)\ni = W(c)[c(ni)\n1 ; ··· ; c(ni)\nM ] (2)\nwhere W(c) are the weights and the character sequence is\npadded to a pre-deﬁned maximum lengthM. After the fully-\nconnected layer, we also apply layer normalization (Lei Ba,\nKiros, and Hinton 2016). These vectors are then fed to the\nNL reader, and are integrated with the word embeddings by\na gating sub-layer.\nNeural Structure of NL Reader. The NL reader is com-\nposed of a stack of blocks ( N\nd blocks in total). Each block\ncontains three different sub-layers (namely, self-attention,\ngating mechanism, and word convolution) to extract fea-\ntures, which we introduce in detail in the following subsec-\ntions. Between two sub-layers, we employ a residual con-\nnection (He et al. 2016) followed by a layer normalization.\nSelf-Attention.\nThe self-attention sub-layer follows the\nTransformer’s architecture (Vaswani et al. 2017), and uses\nmulti-head attention to capture long dependency informa-\ntion.\nFor a sequence of input tokens n\n1,n2,··· ,nL, we rep-\nresent them as an embedding n1,n2,··· ,nL by a look-up\n8985\nConv\nGating\nTree Conv\nNL Attention\nSelf attention\nCharacter\nEmbedding Gating\nRule\nDefinition\nEncoding\nDense\nNL Attention\nAST Attention\nSoftmax & Pointer\nNatural Language Description\n(Input)\n+Position\nEmbedding\nSelf Attention\n+Position\nEmbedding\nRule Sequence\n(Generated Code)\nTree Path\n(Query)\nNd xN 1 x N2 x\nNL Reader AST Reader Decoder\nDepth\nEmbedding\nFigure 2: Overview of the TreeGen.\ntable. We also use position embeddings to encode the infor-\nmation of word positions. In particular, we adopt the variant\nin Dehghani et al. (2018), and compute the position embed-\nding for the ith word in the bth Transformer block as\np\nb,i[2j]=s i n ( (i+ b)/(100002j/d)) (3)\npb,i[2j +1 ]=s i n ( (i+ b)/(100002j/d)) (4)\nwhere pi,b[·] indexes a dimension of the vector pi,b, and dis\nthe number of dimensions (i.e., embedding size).\nA Transformer block learns non-linear features by\nmulti-head attention, which yields a matrix Y(self)\nb =\n[y(self)\nb,1 ,y(self)\nb,2 ,··· ,y(self)\nb,L ]⊤ , where Y(self)\nb ∈ RL×d. For no-\ntational simplicity, we omit the subscript b. The multi-head\nlayer is computed by\nY(self) =c o n c a t (head1,··· ,headH )Wh (5)\nwhere H denotes the number of heads andWh is the weight.\nAn attention layer is applied in each head headt, computed\nby\nheadt = softmax(QK⊤\n√dk\n)V (6)\nwhere dk = d/H denotes the length of each features vector.\nQ, K and V are computed by\n[Q,K,V ]=[ x1,··· ,xL]⊤ [WQ,WK ,WV ] (7)\nwhere WQ,WK ,WV ∈ Rd×dk are model parameters. xi\nis the input of this Transformer block. For the ﬁrst block,\nit is the vector sum of the table-lookup embedding and the\nposition embedding, i.e., n\ni + p1,i; for other blocks, it is the\nvector sum of the lower Transformer block’s output and the\nposition embedding that corresponds to this block.\nGating Mechanism.\nAfter the features are computed by\nself-attention, we further incorporate with the information\nof character embeddings. This is given by a gating mech-\nanism based on softmax. For the ith word, we compute a\ncontrol vector q\ni from y(self)\ni by a linear transformation. The\nsoftmax weight k(c)\ni for character embedding is given by a\nlinear transformation from n(c)\ni in Equation 2. The softmax\nweight k(y)\ni for Transformer’s output is given by another lin-\near transformation from y(self)\ni . Then, the gate is computed\nby\n[α(y)\ni,t,α(c)\ni,t] = softmax{q⊤\ni k(y)\ni ,q⊤\ni k(c)\ni } (8)\nThey are used to weigh the feature of the Transformer’s\nlayer v(y)\ni and the feature of character embeddings v(c)\ni , lin-\near transformed from y(self)\ni and n(c)\ni , respectively.\nhi,t =[ α(y)\ni,tv(y)\ni + α(c)\ni,tv(c)\ni ] (9)\nSimilar to Equation 5, the output of our gating mechanism\nis Y(gate) =( hi,t)i,t, where (·)i,t represents a block matrix\nwith the elements being hi,t.\nW ord Convolution.Finally, two convolutional layers\nare applied to the output of the gating mechanism\ny(gate)\n1 ,··· ,y(gate)\nL and to extract the local features around\neach token y(conv,l)\n1 ,··· ,y(conv,l)\nL , where l denotes the layer\nof convolutional layers. The y(conv,l)\ni is computed by\ny(conv,l)\ni = W(conv,l)[y(conv,l−1)\ni−w ; ··· ; y(conv,l−1)\ni+w ] (10)\nwhere W(conv,l) are the convolution weights,w =( k−1)/2,\nand k denotes the window size. In particular, y(conv,0)\ni de-\nnotes the output of gating mechanism y(gate)\ni . In these layers,\nseparable convolution (Chollet 2017) is used. The reason\nis separable convolution has fewer parameters that is easy\nfor training. For the ﬁrst and the last words, we add zero\npadding. Between these layers, we use theGELU activation\nfunction (Hendrycks and Gimpel 2016).\nIn summary, the NL reader has a few Transformer blocks\nof self-attention, the gating mechanism, and word convolu-\ntion. The natural language description is encoded as features\ny\n(NL)\n1 ,y(NL)\n2 ,··· ,y(NL)\nL .\n8986\nAST Reader\nWe design an AST reader to model the structure of the par-\ntial AST that has generated. Although our programs are gen-\nerated by predicting the sequence of grammar rules, these\nrules alone lack a concrete picture of the program and are\ninsufﬁcient for predicting the next rule. Therefore, our AST\nreader considers heterogeneous information, including the\npredicted rules and the tree structures.\nTo incorporate such program-speciﬁc information, we\nﬁrst represent the code as a sequence of rules, then encode\nthe rules with attention mechanism, and ﬁnally use a tree\nconvolution layer to combine the encoded representation of\neach node with its ancestors.\nAST Representation\nRule Sequence Embedding.\nTo encode the rule informa-\ntion, we use the ID of the rules. Suppose we have a sequence\nof rules r\n1,r2,··· ,rP that are been used to generate the par-\ntial AST in a decoding step, where P denotes the length of\nthe sequence. We represent these rules as real-valued vectors\nr\n1,r2,··· ,rP by table-lookup embeddings.\nRule Deﬁnition Encoding. The above table-lookup em-\nbedding treats a grammar rule as an atomic token, and loses\nthe information of the rule’s content.\nTo alleviate this problem, we enhance the representation\nof a rule with the encoding of rule deﬁnition.\nFor a grammar rule i : α → β\n1 ··· βK , where α is the\nparent node and β1 ··· βK are child nodes. They can be ei-\nther terminal or non-terminal symbols. The index iis the ID\nof the rule.\nSimilar to Equation 2, we encode the rule content as a vec-\ntor r(c) by a fully-connected layer with input being the table-\nlookup embeddings α,β1,··· ,βK of respective symbols.\nIt is noted that the sequence is also padded to a maximum\nlength.\nThen the rule deﬁnition features y\n(rule)\n1 ,··· ,y(rule)\nP are\ncomputed by another fully-connected layer as\ny(rule)\ni = W(rule)[ri; r(c); α] (11)\nwhere ri is the table-lookup embedding of the ruleri, r(c)\ni is\nthe content-encoding rule representation, and we emphasize\nthe parent node α again. After that, a layer normalization is\nfollowed.\nPosition and Depth Embeddings.\nSince our AST reader\nwould use self-attention mechanisms, we need to represent\nthe position where a grammar rule is used.\nWe ﬁrst adopt the position embedding as in Equation 4,\nrepresenting when a rule is used in the sequencer\n1,··· ,rP .\nThe position embeddings are denoted by p(r)\n1 ··· ,p(r)\nP\nHowever, such position embedding does not capture the\nposition of a rule in the AST. We further encode such in-\nformation by a depth embedding. If we expand a symbol\nα by the rule r : α → β\n1 ··· βK , we represent the depth\nof the rule by its parent node, i.e., α. In this way, we\nassociate another sequence of table-lookup depth embed-\ndings d\n1,··· ,dP to the sequence of used grammar rules\nr1,··· ,rP .\nNeural Structure of AST Reader. The AST reader is\nalso composed of a stack of blocks ( N1 blocks in total).\nEach block is decomposed into four sub-layers (namely,\nself-attention, a gating mechanism, NL attention, and tree\nconvolution). We employ a residual connection around each\nsub-layer except the layer of tree convolution. After each\nsub-layer, we apply a layer normalization.\nSelf-Attention.\nTo capture the information of AST, we\nbuild a Transformer-like self-attention layer, where the in-\nput is sum of the rule embedding, position embedding, and\ndepth embedding, i.e., r\ni + di + p(r)\ni . The self-attention sub-\nlayer extract featuresy(ast-self)\n1 ,y(ast-self)\n2 ,··· ,y(ast-self)\nP of AST\ninput, using the same mechanism as Equations 4, 5, 6 with\ndifferent weights but add an additional depth embedding to\np\n(r)\ni .\nGating Mechanism. We would like to incorporate\nthe content-encoding rule representation y(rule)\ni into the\nTransformer-extracted features. We adopt a gating mecha-\nnism as in Equations 8, 9, and the fused features becomes\ny\n(ast-g)\n1 ,y(ast-g)\n2 ,··· ,y(ast-g)\nP after this sub-layer.\nNL Attention. During the decoding step, we should be in-\nformed of the input NL description. This is given by a multi-\nhead NL attention, similar to the Transformer decoder’s at-\ntention to its encoder (Vaswani et al. 2017). The extracted\nfeatures are denoted byy\n(ast-nl)\n1 ,y(ast-nl)\n2 ,··· ,y(ast-nl)\nP .\nTree Convolution. Should we consider only the above\nsub-layers, it would be hard for the reader to combine the\ninformation of a node with its ancestors. A node can be far\naway from its ancestors in the rule sequence but is close\nin structure. Therefore, it is difﬁcult for a traditional Trans-\nformer to extract such structural features.\nWe integrate the features of a node with those of its an-\ncestors. We treat the AST as a graph and use an adjacency\nmatrix M to represent the directed graph. If a node α\ni is the\nparent of αj, then Mji =1 . Suppose all the nodes are pre-\nsented by features f1,··· ,fn, their parents’ features can be\ngiven by the multiplication with the adjacency matrix:\n[f(par)\n1 ,··· ,f(par)\nn ]=[ f1,··· ,fn]M (12)\nwhere f(par)\ni denotes the parent of theith node. For the father\nof the root node, we pad it with the feature vector of the root\nnode itself.\nThe tree-based convolution window, applied to the current\nsub-tree, is given by\nY\n(tconv, l) = f(W(tconv, l)[Y(tconv, l−1);\nY(tconv, l−1)M; ··· ; Y(tconv, l−1)Mkt−1])\n(13)\nwhere W(tconv, l) is the weights of the convolutional layer,\nkt denotes the window size (we set to 3 in our experi-\nments), l is the layer of these convolutional layers. In partic-\nular, Y(tconv, 0) =[ y(att)\n1 ,y(att)\n2 ,··· ,y(att)\nP ], where Y(tconv, 0) ∈\nRd×P . For the last layer of the AST reader, we add addi-\ntional two convoluation layers. In the equation,f is the acti-\nvation function and GELU is applied between these layers.\nIn summary, the AST reader has N1 blocks of these four\nsub-layers, and yields the features y(ast)\n1 ,y(ast)\n2 ,··· ,y(ast)\nP .\n8987\nDecoder\nOur ﬁnal component is a decoder that integrates the infor-\nmation of the generated code with the NL description, and\npredicts the next grammar rule. Similar to the AST reader,\na stack of blocks (N\n2 blocks in total) each with several sub-\nlayers is used in the decoder as follows. A residual connec-\ntion is also employed around each sub-layer followed by a\nlayer normalization.\nThe decoder takes the non-terminal node to be expanded\nas a query. Inspired by a previous approach (Sun et al. 2019),\nthe querying node is represented as a path from the root to\nthe node to be expanded. For example, if we are going to\nexpand node “Assign” in Figure 1, the path should be root,\nModule, body, Assign. We represent the nodes in this path as\nreal-valued vectors. Then we apply a fully-connected layer\nlike Equation 2 to these vectors and the output of the path\n(querying node) is q\n(path)\ni .\nWe then apply two attention layers to integrate the outputs\nof the AST reader and the NL reader.\nWe ﬁrst apply an AST attention layer over the out-\nput of the AST reader with queries and extract features\nf(tree)\n1 ,··· ,f(tree)\nP . In this layer, Q is computed from queries\nq(path)\n1 ,··· ,q(path)\nP ; K and V are computed from the code\nfeatures y(ast)\n1 ,··· ,y(ast)\nP . We further integrate the features\nfrom the input description. This integration is also imple-\nmented with an NL attention, where Q is computed by fea-\nture f\n(tree)\n1 ,··· ,f(tree)\nP ; and K and V are computed by the\ninput description y(NL)\n1 ,··· ,y(NL)\nL .\nFinally, a set of two fully-connected layers, where the ﬁrst\nlayer has a GELU activation function, are followed to ex-\ntract features for prediction.\nTraining and Inference\nWe predict the next grammar rule, among all possible candi-\ndates, by softmax based on the decoder’s last layer features.\nWe also introduce the pointer network (See, Liu, and\nManning 2017) (essentially, an attention) that can directly\ncopy a token a from the NL description. In this case, the re-\nsulting grammar rule is α → a, where α is a non-terminal\nsymbol to be expanded and a is a terminal symbol. Such\npointer mechanism is helpful for user-deﬁned identiﬁers\n(e.g., variable and function names).\nThe choice between softmax rule prediction and the\npointer network is given by another gating mechanism p\ng,\nalso computed from the decoder’s last feature. The overall\npredicted probability of the next grammar rule is\np(r\ni|·)=\n{pg p(ri|·) if i ∈ D\n(1 −pg)P r{copy word t at step i|·} if i ∈ C\n(14)\nwhere idenotes the ID of the rule, D is the set of predeﬁned\nrules, and C denotes the set of rules in the form of α → a,\nwhere ais a terminal token that occurs in the NL description.\npg is the probability of using the type of predeﬁned rules,\nand the p(ri|·) (the probability of each predeﬁned rules) are\ncomputed by two single-layer perceptrons with the sigmoid\nand softmax activation functions, respectively, and the input\nof these layers are the features h\n(dec).\nNAME: Darkscale Healer\nATK:4\nDEF:5\nCOST:5\nDUR:- 1\nTYPE: Minion\nPLAYER: Neutral\nRACE: NIL\nRARITY: Common\nDESCRIPTION: \n<b>Battlecry:</b> Restore 2 Health to all \nfriendly characters.\nFigure 3: A example of the implement of HearthStone.\nThe pointer network is computed by\nξt = vT tanh(W1h(dec) + W2y(NL)\nt )\nPr{copy word t at step i|·} = exp {ξt}∑L\nj=1 exp {ξj}\n(15)\nwhere h(dec) denotes the decoder’s last feature. The model\nis optimized by maximizing negative log likelihood loss\nagainst the reference program.\nThe inference starts with a start rule, start : snode −→\nroot, expanding a special symbol snode to the root symbol.\nThe recursive prediction terminates if every leaf node in the\npredicted AST is a terminal. During prediction, we use beam\nsearch with a size of 5. Invalid rules are excluded during\nbeam search.\nEvaluation\nWe evaluated our approach on two types of benchmarks: (1)\na Python code generation benchmark, HearthStone, and (2)\ntwo semantic parsing benchmarks, ATIS and GEO.\nExperiment: HearthStone\nDataset. We ﬁrst evaluated our approach on the Hearth-\nStone benchmark (Ling et al. 2016). The benchmark con-\ntains Python code that implements 665 different cards of\nHearthStone. Each card is composed of a semi-structural\ndescription and a groundtruth Python program. The Python\nprograms have a length of 84 tokens on average. The de-\nscription comes with several attributes such as card name,\ncard type, as well as a natural language description for\nthe functionality of the card. A Python program is mainly\ndecided by the natural language description where the at-\ntributes decide the constants or identiﬁer names. A sample\ndescription and its corresponding Python program are shown\nin Figure 3. When preprocessing the card description into to-\nken sequences, existing approaches consider two methods.\n8988\nThe ﬁrst (Yin and Neubig 2017; Hayati et al. 2018) (called\nplain preprocessing) treats the whole description as plain\ntext and delimit the tokens by standard separators such as\nspace or periods. The second (Rabinovich, Stern, and Klein\n2017) (called structural preprocessing) treats the descrip-\ntions as semi-structural and always treat an attribute as one\ntoken. In this experiment, we consider both methods and de-\nnote the results corresponding to the plain preprocessing as\nTreeGen-A and that corresponding to the structural prepro-\ncessing as TreeGen-B. We followed the train-dev-test split\nin Ling et al. (2016), and the statistic is listed in Table 2.\nMetrics. We measured the performance following the\nmetrics in Sun et al. (2019). We computed the StrAcc, which\nis the percentage of programs that has exactly the same token\nsequence as the ground truth; the BLEU score, which is used\nto measure the similarity between the generated code and\nthe reference code at the token level; and the Acc+, which\nis evaluated manually, allows variable renaming on top of\nStrAcc, for every test case.\nSettings. For neural networks, we set the number of NL\nreader layers N\nd =6 , and N1 = N2 =5 for the AST\nreader as well as the decoder. The size of all embedding is\n256. The hidden sizes were all set to the 256 except each\nfully-connected layers, except the ﬁrst layer was 1024 di-\nmensions. We applied dropout after each layer (including at-\ntention layers, gating mechanism layers, convolutional lay-\ners, and fully-connected layers, where the drop rate is 0.15).\nThe model is optimized by Adafactor (Shazeer and Stern\n2018) with default parameters.\nOverall Results. We show the results in Table 1. In this\ntable, the structural preprocessing has a better performance\ncompared with the plain preprocessing.\nAs shown, our model achieves 6 percentage points ac-\ncuracy improvement with plain preprocessing and 4.5 per-\ncentage points accuracy improvement with structural pre-\nprocessing. For the BLEU score, our model also achieves\nthe best results. These boosts in performance indicate that\nTreeGen successfully alleviates the long dependency prob-\nlem and effectively encodes the structural information in the\ncode generation.\nTime Efﬁciency. We further evaluated the complexity of\nour model on the HearthStone, and the result shows that our\nmodel is faster than the previous ones. It takes 18s for an\nepoch on a single Nvidia Titan XP, whereas 180s for the\nCNN (Sun et al. 2019) and 49s for the RNN (Yin and Neubig\n2017).\nLocation of Structural Convolution Sub-layer. One of\nthe keys of our approach is to add the structural convolution\nsub-layers only to part of the Transformer blocks in the de-\ncoder. To evaluate whether this design decision is effective,\nwe evaluate four competing settings: 1) adding the struc-\ntural convolution sub-layers to all Transformer blocks (i.e.,\nN\n1 =1 0); 2) adding the structural convolution sub-layers to\nthe ﬁrst 7 blocks in AST reader (i.e.,N1 = 10(7)); 3) adding\nthe structural convolution sub-layers to the ﬁrst 8 blocks in\nAST reader (i.e., N\n1 = 10(8)); 4) the other adds to none\n(i.e., N1 =0 ). As we can see, from Table 1 our approach\nadding the sub-layer to all transformer blocks ( N1 =1 0)\nsigniﬁcantly outperforms the last setting ( N1 =0 ), but\nslightly worse than the other two settings.\nAblation Test. We ablated our model (TreeGen-B was\nused) to analyze the contribution of each component, results\nalso shown in Table 1. First, we compared our model with\nthe traditional Transformer, which is a Transformer with-\nout effective structure modeling. We achieved 21 percentage\npoints higher accuracy (p-value is less than 0.001) and 12\nhigher BLEU score. This result provides strong evidence of\nthe effectiveness of the AST reader in our model and the im-\nportance of the structural information. Next, we replaced the\ntree convolutional layers in the AST Reader with two layers\nof fully-connected layers, and we removed the char embed-\nding, rule deﬁnition encoding, self-attention layers in turn.\nThe experimental results show the identiﬁers-encoding, alle-\nviating long-dependency and structural information signiﬁ-\ncantly inﬂuence the accuracy. Please note that in some cases\nBLEU increases while StrAcc and Acc+ decrease. Here we\nconsider StrAcc and Acc+ more important as they guarantee\nthe correctness of the generated programs and correctness is\nusually crucial in code generation.\nExperiment II: Semantic Parsing\nDataset. We further evaluated our approach on the seman-\ntic parsing tasks. Our experiment was conducted on two se-\nmantic parsing datasets, ATIS and GEO. The input of these\ndatasets is a natural language description, while the output\nis a short piece of code in lambda calculus. We followed the\nstandard train-dev-test split of these datasets, and the statis-\ntics are listed in Table 2.\nMetrics and Settings. In this task, we follow the evalua-\ntion of the previous approaches (Dong and Lapata 2016) and\nuse accuracy as the metric, where the tree exact match was\nconsidered to avoid spurious errors. In other words, the or-\nder of the children can be changed within conjunction nodes.\nWe followed all the settings in the HS experiment except that\nwe changed the embedding size and the hidden sizes to 128\ncompared with the setting of the HS experiment.\nResults. Table 3 shows the performance of our TreeGen.\nAs seen, the accuracy of our approach is sightly worse than\nthe traditional approach WKZ14 (Wang, Kwiatkowski, and\nZettlemoyer 2014), which is based on the CCG parser and\nuses a large number of templates. This traditional approach\nis hard to generalize new datasets. However, our model\nwas directly adopted from the HS dataset, and achieved the\nhighest accuracy, among all neural models (Dong and La-\npata 2016; Rabinovich, Stern, and Klein 2017; Dong and\nLapata 2018; Chen, Sun, and Han 2018; Xu et al. 2018;\n8989\nModel StrAcc Acc+ BLEU\nPlain\nLPN (Ling et al. 2016) 6.1 – 67.1\nSEQ2TREE (Dong and Lapata 2016) 1.5 – 53.4\nYN17 (Yin and Neubig 2017) 16.2 ∼18.2 75.8\nASN (Rabinovich, Stern, and Klein 2017) 18.2 – 77.6\nReCode (Hayati et al. 2018) 19.6 – 78.4\nTreeGen-A 25.8 25.8 79.3\nStructural\nASN+SUPATT (Rabinovich, Stern, and Klein 2017) 22.7 – 79.2\nSZM19 (Sun et al. 2019) 27.3 30.3 79.6\nTreeGen-B 31.8 33.3 80.8\nLocation of Structural Convolutional Sub-layer\nN1 =1 0,N2 =0 25.8 27.3 80.4\nN1 = 10(7),N2 =0 27.3 28.8 78.5\nN1 = 10(8),N2 =0 25.8 28.8 78.5\nN1 =0 ,N2 =1 0 21.2 22.7 79.6\nAblation test\nBaseline: Transformer 10.6 ( p =0 .015) 12.1 68.0\n- Tree Convolution 27.3 ( p =0 .015) 27.3 80.9\n- Rule Deﬁnition Encoding 27.3 ( p< 0.001) 28.8 81.8\n- Char Embedding 15.2 ( p< 0.001) 18.2 72.9\n- Self-Attention 28.8 ( p< 0.001) 28.8 81.0\nTable 1: Performance of our model in comparison with previous state-of-the-art results.\nExp II\nStatistics HS ATIS GEO\n# Train 533 4,434 600\n# Dev 66 491 -\n# Test 66 448 280\nAvg. tokens in description 35.0 10.6 7.4\nMax. tokens in description 76.0 48 23\nAvg. tokens in code 83.2 33.9 28.3\nMax. tokens in code 403 113 144\nTable 2: Statistics of the datasets we used.\nMethod ATIS GEO\nTraditional\nZC07 (Zettlemoyer and Collins 2007) 84.6 86.1\nFUBL (Kwiatkowski et al. 2011) 82.8 88.6\nKCAZ13 (Kwiatkowski et al. 2013) - 89.0\nWKZ14 (Wang, Kwiatkowski, and Zettlemoyer 2014) 91.3 90.4\nNeural Networks\nSEQ2SEQ (Dong and Lapata 2016) 84.2 84.6\nSEQ2TREE (Dong and Lapata 2016) 84.6 87.1\nASN (Rabinovich, Stern, and Klein 2017) 85.3 85.7\nASN+SUPATT (Rabinovich, Stern, and Klein 2017) 85.9 87.1\nCOARSE2FINE (Dong and Lapata 2018) 87.7 88.2\nTRANX (Yin and Neubig 2018) 86.2 88.2\nSeq2Act (Chen, Sun, and Han 2018) 85.5 88.9\nGraph2Seq (Xu et al. 2018) 85.9 88.1\nSZM19 (Sun et al. 2019) 85.0 -\nTreeGen 89.1 89.6\nTable 3: Accuracy in semantic parsing (in percentage).\nSun et al. 2019). This experiment shows the effectiveness\nand generalizability of TreeGen.\nRelated Work\nCode generation achieves signiﬁcant progress in recent\nyears. The early approaches are mainly based on tem-\nplates (Zettlemoyer and Collins 2007; 2005; Kushman and\nBarzilay 2013; Wang, Kwiatkowski, and Zettlemoyer 2014).\nWith the prosperity of deep learning, the sequence-to-\nsequence framework has shown to be effective in various\ntasks (Sutskever, Vinyals, and Le 2014). Ling et al. (2016)\napplied this framework to generate code based on tokens.\nUnlike natural languages, it is shown that the code contains\nmuch more structural information. Thus, the abstract syntax\ntree (AST) was used in more recent works (Dong and Lapata\n2016; Yin and Neubig 2017; Rabinovich, Stern, and Klein\n2017; Hayati et al. 2018; Yin and Neubig 2018). However,\nthese studies mainly use recurrent neural networks (RNNs)\nfrom the long dependency problem (Bengio, Simard, and\nFrasconi 1994). Sun et al. (2019) proposed to use the con-\nvolutional neural network (CNN) to handle the long depen-\ndency problem. Our approach addresses this problem by\nTransformer’s intensive attention mechanism (Vaswani et al.\n2017). To incorporate the structural information and the idea\nof self-attention, we propose a tree-based Transformer archi-\ntecture for code generation.\nConclusion\nIn this work, we propose TreeGen for program generation.\nTreeGen uses the attention mechanism of Transformers to\nalleviate the long-dependency problem and introduces the\nAST reader to combine the grammar rules and the AST\nstructure.\nThe evaluation was conducted on a Python dataset,\nHearthStone, and two semantic parsing datasets, ATIS and\nGEO. The experimental results show that our model signiﬁ-\n8990\ncantly outperforms existing approaches. We also conducted\nin-depth ablation tests, which suggests that each component\nin our model plays a signiﬁcant role.\nAcknowledgments\nThis work is sponsored by the National Key Re-\nsearch and Development Program of China under Grant\nNo. 2017YFB1001803, and National Natural Science Foun-\ndation of China under Grant Nos. 61672045, 61529201, and\n61922003. Lili Mou is an Amii Fellow; he is supported by\nthe CCAI Chair Program; and he also thanks AltaML for\nsupport.\nReferences\nBengio, Y .; Simard, P.; and Frasconi, P. 1994. Learning\nlong-term dependencies with gradient descent is difﬁcult.\nIEEE Trans. Neural Networks5(2):157–166.\nChen, B.; Sun, L.; and Han, X. 2018. Sequence-to-Action:\nEnd-to-End Semantic Graph Generation for Semantic Pars-\ning. In ACL, 766–777.\nChollet, F. 2017. Xception: Deep learning with depthwise\nseparable convolutions. In CVPR, 1251–1258.\nDehghani, M.; Gouws, S.; Vinyals, O.; Uszkoreit, J.; and\nKaiser, Ł. 2018. Universal transformers. arXiv preprint\narXiv:1807.03819.\nDong, L., and Lapata, M. 2016. Language to Logical Form\nwith Neural Attention. In ACL, 33–43.\nDong, L., and Lapata, M. 2018. Coarse-to-Fine Decoding\nfor Neural Semantic Parsing. In ACL, 731–742.\nHayati, S. A.; Olivier, R.; Avvaru, P.; Yin, P.; Tomasic, A.;\nand Neubig, G. 2018. Retrieval-Based Neural Code Gener-\nation. In EMNLP, 925–930.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR, 770–778.\nHendrycks, D., and Gimpel, K. 2016. Bridging Nonlineari-\nties and Stochastic Regularizers with Gaussian Error Linear\nUnits. arXiv preprint arXiv:1606.08415.\nKushman, N., and Barzilay, R. 2013. Using semantic uni-\nﬁcation to generate regular expressions from natural lan-\nguage. In NAACL, 826–836.\nKwiatkowski, T.; Zettlemoyer, L.; Goldwater, S.; and Steed-\nman, M. 2011. Lexical generalization in CCG grammar\ninduction for semantic parsing. In EMNLP, 1512–1523.\nKwiatkowski, T.; Choi, E.; Artzi, Y .; and Zettlemoyer, L.\n2013. Scaling semantic parsers with on-the-ﬂy ontology\nmatching. In EMNLP, 1545–1556.\nLei Ba, J.; Kiros, J. R.; and Hinton, G. E. 2016. Layer\nnormalization. arXiv preprint arXiv:1607.06450.\nLing, W.; Blunsom, P.; Grefenstette, E.; Hermann, K. M.;\nKoˇcisk`y, T.; Wang, F.; and Senior, A. 2016. Latent Predictor\nNetworks for Code Generation. In ACL, 599–609.\nMou, L.; Li, G.; Zhang, L.; Wang, T.; and Jin, Z. 2016.\nConvolutional neural networks over tree structures for pro-\ngramming language processing. In AAAI, 1287–1293.\nRabinovich, M.; Stern, M.; and Klein, D. 2017. Abstract\nSyntax Networks for Code Generation and Semantic Pars-\ning. In ACL, 1139–1149.\nSee, A.; Liu, P. J.; and Manning, C. D. 2017. Get to the\npoint: Summarization with pointer-generator networks. In\nACL, 1073–1083.\nShazeer, N., and Stern, M. 2018. Adafactor: Adaptive\nlearning rates with sublinear memory cost. arXiv preprint\narXiv:1804.04235.\nSun, Z.; Zhu, Q.; Mou, L.; Xiong, Y .; Li, G.; and Zhang,\nL. 2019. A grammar-based structural cnn decoder for code\ngeneration. In AAAI, volume 33, 7055–7062.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence\nto sequence learning with neural networks. In NIPS, 3104–\n3112.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NIPS, 6000–6010.\nWang, A.; Kwiatkowski, T.; and Zettlemoyer, L. 2014.\nMorpho-syntactic lexical generalization for CCG semantic\nparsing. In EMNLP, 1284–1295.\nXu, K.; Wu, L.; Wang, Z.; Yu, M.; Chen, L.; and Sheinin, V .\n2018. Exploiting Rich Syntactic Information for Semantic\nParsing with Graph-to-Sequence Model. In ACL, 918–924.\nYin, P., and Neubig, G. 2017. A Syntactic Neural Model for\nGeneral-Purpose Code Generation. In ACL\n, 440–450.\nYin, P., and Neubig, G. 2018. TRANX: A transition-based\nneural abstract syntax parser for semantic parsing and code\ngeneration. In EMNLP, 7–12.\nZettlemoyer, L. S., and Collins, M. 2005. Learning to\nmap sentences to logical form: structured classiﬁcation with\nprobabilistic categorial grammars. In UAI, 658–666.\nZettlemoyer, L., and Collins, M. 2007. Online learning\nof relaxed CCG grammars for parsing to logical form. In\nEMNLP-CoNLL, 678–687.\n8991"
}