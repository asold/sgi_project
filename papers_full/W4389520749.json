{
  "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
  "url": "https://openalex.org/W4389520749",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5024447363",
      "name": "Potsawee Manakul",
      "affiliations": [
        null,
        "University of Cambridge",
        "Bridge University"
      ]
    },
    {
      "id": "https://openalex.org/A5011907463",
      "name": "Adian Liusie",
      "affiliations": [
        null,
        "University of Cambridge",
        "Bridge University"
      ]
    },
    {
      "id": "https://openalex.org/A5050766679",
      "name": "Mark Gales",
      "affiliations": [
        null,
        "University of Cambridge",
        "Bridge University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2053154970",
    "https://openalex.org/W2998230451",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W4389519598",
    "https://openalex.org/W4386576669",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4285265130",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W4389519449",
    "https://openalex.org/W4392669872",
    "https://openalex.org/W3158419028",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4321594373",
    "https://openalex.org/W4361230777",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W1444168786",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3128459263",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W3106234277",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W2964060837",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2305610840",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3034808961",
    "https://openalex.org/W3155332104",
    "https://openalex.org/W3196268181",
    "https://openalex.org/W3153712677",
    "https://openalex.org/W3034383590"
  ],
  "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose “SelfCheckGPT”, a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9004–9017\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSELF CHECK GPT: Zero-Resource Black-Box Hallucination Detection\nfor Generative Large Language Models\nPotsawee Manakul, Adian Liusie, Mark J. F. Gales\nALTA Institute, Department of Engineering, University of Cambridge\npm574@cam.ac.uk, al826@cam.ac.uk, mjfg@eng.cam.ac.uk\nAbstract\nGenerative Large Language Models (LLMs)\nsuch as GPT-3 are capable of generating highly\nfluent responses to a wide variety of user\nprompts. However, LLMs are known to hal-\nlucinate facts and make non-factual statements\nwhich can undermine trust in their output. Ex-\nisting fact-checking approaches either require\naccess to the output probability distribution\n(which may not be available for systems such\nas ChatGPT) or external databases that are in-\nterfaced via separate, often complex, modules.\nIn this work, we propose \"SelfCheckGPT\", a\nsimple sampling-based approach that can be\nused to fact-check the responses of black-box\nmodels in a zero-resource fashion, i.e. with-\nout an external database. SelfCheckGPT lever-\nages the simple idea that if an LLM has knowl-\nedge of a given concept, sampled responses\nare likely to be similar and contain consistent\nfacts. However, for hallucinated facts, stochas-\ntically sampled responses are likely to diverge\nand contradict one another. We investigate this\napproach by using GPT-3 to generate passages\nabout individuals from the WikiBio dataset, and\nmanually annotate the factuality of the gener-\nated passages. We demonstrate that SelfCheck-\nGPT can: i) detect non-factual and factual sen-\ntences; and ii) rank passages in terms of factu-\nality. We compare our approach to several base-\nlines and show that our approach has consider-\nably higher AUC-PR scores in sentence-level\nhallucination detection and higher correlation\nscores in passage-level factuality assessment\ncompared to grey-box methods.1\n1 Introduction\nLarge Language Models (LLMs) such as GPT-3\n(Brown et al., 2020) and PaLM (Chowdhery et al.,\n2022) are capable of generating fluent and realistic\nresponses to a variety of user prompts. They have\nbeen used in many applications such as automatic\n1Code and dataset can be found on the project page at\nhttps://github.com/potsawee/selfcheckgpt.\nLLM\ne.g. GPT-3\nGiuseppe Mariani was\nan Italian professional\nfootballer who played\nas a forward. He was\nborn in Milan, Italy. He\ndied in Rome, Italy.\n[truncated]\nGiuseppe Mariani was an\nItalian painter, sculptor,\nand engraver. He was\nborn in Naples, Italy, in\n1882, and died in Paris,\nFrance, in 1944.\n[truncated]\nGiuseppe Mariani was an\nItalian violinist,\npedagogue and\ncomposer. He was born\nin Pavia, Italy, on 4 June\n1836. [truncated]\nStochastically-generated responses\nSelfCheckGPT Score\n(e.g. how often is the sentence supported by the samples)\nN samples\nDoes {sample1}\nsupport {sentence}?\nAnswer: [Yes/No]\nLLM\nsample1 sampleN\nDoes {sampleN}\nsupport {sentence}?\nAnswer: [Yes/No]\nLLM's passage\nto be evaluated at\nsentence-level \n...\n...\nNo Yes No... ...\nFigure 1: SelfCheckGPT with Prompt. Each LLM-generated\nsentence is compared against stochastically generated re-\nsponses with no external database. A comparison method\ncan be, for example, through LLM prompting as shown above.\ntools to draft reports, virtual assistants and sum-\nmarization systems. Despite the convincing and\nrealistic nature of LLM-generated texts, a growing\nconcern with LLMs is their tendency to halluci-\nnate facts. It has been widely observed that mod-\nels can confidently generate fictitious information,\nand worryingly there are few, if any, existing ap-\nproaches to suitably identify LLM hallucinations.\nA possible approach of hallucination detection\nis to leverage existing intrinsic uncertainty metrics\nto determine the parts of the output sequence that\nthe system is least certain of (Yuan et al., 2021; Fu\net al., 2023). However, uncertainty metrics such\nas token probability or entropy require access to\ntoken-level probability distributions, information\nwhich may not be available to users for example\nwhen systems are accessed through limited exter-\nnal APIs. An alternate approach is to leverage\nfact-verification approaches, where evidence is re-\ntrieved from an external database to assess the ve-\nracity of a claim (Thorne et al., 2018; Guo et al.,\n2022). However, facts can only be assessed relative\nto the knowledge present in the database. Addition-\n9004\nally, hallucinations are observed over a wide range\nof tasks beyond pure fact verification (Kryscinski\net al., 2020; Maynez et al., 2020).\nIn this paper, we propose SelfCheckGPT, a\nsampling-based approach that can detect whether\nresponses generated by LLMs are hallucinated or\nfactual. To the best of our knowledge, SelfCheck-\nGPT is the first work to analyze model halluci-\nnation of general LLM responses, and is the first\nzero-resource hallucination detection solution that\ncan be applied to black-box systems. The motivat-\ning idea of SelfCheckGPT is that when an LLM\nhas been trained on a given concept, the sampled re-\nsponses are likely to be similar and contain consis-\ntent facts. However, for hallucinated facts, stochas-\ntically sampled responses are likely to diverge and\nmay contradict one another. By sampling multiple\nresponses from an LLM, one can measure informa-\ntion consistency between the different responses\nand determine if statements are factual or halluci-\nnated. Since SelfCheckGPT only leverages sam-\npled responses, it has the added benefit that it can\nbe used for black-box models, and it requires no\nexternal database. Five variants of SelfCheckGPT\nfor measuring informational consistency are con-\nsidered: BERTScore, question-answering, n-gram,\nNLI, and LLM prompting. Through analysis of an-\nnotated articles generated by GPT-3, we show that\nSelfCheckGPT is a highly effective hallucination\ndetection method that can even outperform grey-\nbox methods, and serves as a strong first baseline\nfor an increasingly important problem of LLMs.\n2 Background and Related Work\n2.1 Hallucination of Large Language Models\nHallucination has been studied in text generation\ntasks, including summarization (Huang et al., 2021)\nand dialogue generation (Shuster et al., 2021), as\nwell as in a variety of other natural language gen-\neration tasks (Ji et al., 2023). Self-consistency\ndecoding has shown to improve chain-of-thought\nprompting performance on complex reasoning\ntasks (Wang et al., 2023). Further, Liu et al. (2022)\nintroduce a hallucination detection dataset, how-\never, texts are obtained by perturbing factual texts\nand thus may not reflect true LLM hallucination.\nRecently, Azaria and Mitchell (2023) trained a\nmulti-layer perception classifier where an LLM’s\nhidden representations are used as inputs to pre-\ndict the truthfulness of a sentence. However, this\napproach is a white-box approach that uses the\ninternal states of the LLM, which may not be avail-\nable through API calls, and requires labelled data\nfor supervised training. Another recent approach\nis self-evaluation (Kadavath et al., 2022), where an\nLLM is prompted to evaluate its previous predic-\ntion, e.g., to predict the probability that its gener-\nated response/answer is true.\n2.2 Sequence Level Uncertainty Estimation\nToken probabilities have been used as an indica-\ntion of model certainty. For example, OpenAI’s\nGPT-3 web interface allows users to display token\nprobabilities (as shown in Figure 2), and further un-\ncertainty estimation approaches based on aleatoric\nand epistemic uncertainty have been studied for\nautoregressive generation (Xiao and Wang, 2021;\nMalinin and Gales, 2021). Additionally, condi-\ntional language model scores have been used to\nevaluate properties of texts (Yuan et al., 2021; Fu\net al., 2023). Recently, semantic uncertainty has\nbeen proposed to address uncertainty in free-form\ngeneration tasks where probabilities are attached\nto concepts instead of tokens (Kuhn et al., 2023).\nFigure 2: Example of OpenAI’s GPT-3 web interface with\noutput token-level probabilities displayed.\n2.3 Fact Verification\nExisting fact-verification approaches follow a\nmulti-stage pipeline of claim detection, evidence\nretrieval and verdict prediction (Guo et al., 2022;\nZhong et al., 2020). Such methods, however, re-\nquire access to external databases and can have\nconsiderable inference costs.\n3 Grey-Box Factuality Assessment\nThis section will introduce methods that can be\nused to determine the factuality of LLM responses\nin a zero-resource setting when one has full access\n9005\nto output distributions.2 We will use ‘factual’ to\ndefine when statements are grounded in valid infor-\nmation, i.e. when hallucinations are avoided, and\n‘zero-resource’ when no external database is used.\n3.1 Uncertainty-based Assessment\nTo understand how the factuality of a generated\nresponse can be determined in a zero-resource set-\nting, we consider LLM pre-training. During pre-\ntraining, the model is trained with next-word pre-\ndiction over massive corpora of textual data. This\ngives the model a strong understanding of language\n(Jawahar et al., 2019; Raffel et al., 2020), power-\nful contextual reasoning (Zhang et al., 2020), as\nwell as world knowledge (Liusie et al., 2023). Con-\nsider the input \" Lionel Messi is a _ \". Since\nMessi is a world-famous athlete who may have\nappeared multiple times in pre-training, the LLM\nis likely to know who Messi is. Therefore given\nthe context, the token \" footballer\" may be as-\nsigned a high probability while other professions\nsuch as \"carpenter\" may be considered improba-\nble. However, for a different input such as \"John\nSmith is a _\", the system will be unsure of the\ncontinuation which may result in a flat probability\ndistribution. During inference, this is likely to lead\nto a non-factual word being generated.\nThis insight allows us to understand the con-\nnection between uncertainty metrics and factuality.\nFactual sentences are likely to contain tokens with\nhigher likelihood and lower entropy, while halluci-\nnations are likely to come from positions with flat\nprobability distributions with high uncertainty.\nToken-level Probability\nGiven the LLM’s response R, let idenote the i-th\nsentence in R, j denote the j-th token in the i-th\nsentence, J is the number of tokens in the sentence,\nand pij be the probability of the word generated by\nthe LLM at the j-th token of the i-th sentence. Two\nprobability metrics are used:\nAvg(−log p) =−1\nJ\n∑\nj\nlog pij\nMax(−log p) =max\nj\n(−log pij)\nMax(−log p) measures the sentence’s likelihood\nby assessing the least likely token in the sentence.\n2Alternate white-box approaches such as that of Azaria\nand Mitchell (2023) require access to full internal states, and\nis less practical and so not considered in this work.\nEntropy\nThe entropy of the output distribution is:\nHij = −\n∑\n˜w∈W\npij( ˜w) logpij( ˜w)\nwhere pij( ˜w) is the probability of the word ˜wbeing\ngenerated at the j-th token of the i-th sentence, and\nWis the set of all possible words in the vocabu-\nlary. Similar to the probability-based metrics, two\nentropy-based metrics are used:\nAvg(H) = 1\nJ\n∑\nj\nHij; Max(H) = max\nj\n(Hij)\n4 Black-Box Factuality Assessment\nA drawback of grey-box methods is that they re-\nquire output token-level probabilities. Though this\nmay seem a reasonable requirement, for massive\nLLMs only available through limited API calls,\nsuch token-level information may not be available\n(such as with ChatGPT). Therefore, we consider\nblack-box approaches which remain applicable\neven when only text-based responses are available.\nProxy LLMs\nA simple approach to approximate the grey-box\napproaches is by using a proxy LLM, i.e. another\nLLM that we have full access to, such as LLaMA\n(Touvron et al., 2023). A proxy LLM can be used\nto approximate the output token-level probabilities\nof the black-box LLM generating the text. In the\nnext section, we propose SelfCheckGPT, which is\nalso a black-box approach.\n5 SelfCheckGPT\nSelfCheckGPT is our proposed black-box zero-\nresource hallucination detection scheme, which op-\nerates by comparing multiple sampled responses\nand measuring consistency.\nNotation: Let R refer to an LLM response\ndrawn from a given user query. SelfCheckGPT\ndraws a further N stochastic LLM response sam-\nples {S1,S2,...,S n,...,S N}using the same query,\nand then measures the consistency between the\nresponse and the stochastic samples. We design\nSelfCheckGPT to predict the hallucination score of\nthe i-th sentence, S(i), such that S(i) ∈[0.0,1.0],\nwhere S(i) →0.0 if the i-th sentence is grounded\nin valid information and S(i) →1.0 if the i-th sen-\n9006\ntence is hallucinated.3 The following subsections\nwill describe each of the SelfCheckGPT variants.\n5.1 SelfCheckGPT with BERTScore\nLet B(.,.) denote the BERTScore between two sen-\ntences. SelfCheckGPT with BERTScore finds the\naverage BERTScore of the i-th sentence with the\nmost similar sentence from each drawn sample:\nSBERT(i) = 1− 1\nN\nN∑\nn=1\nmax\nk\n(B(ri,sn\nk)) (1)\nwhere ri represents the i-th sentence in Rand sn\nk\nrepresents the k-th sentence in the n-th sample Sn.\nThis way if the information in a sentence appears\nin many drawn samples, one may assume that the\ninformation is factual, whereas if the statement ap-\npears in no other sample, it is likely a hallucination.\nIn this work, RoBERTa-Large (Liu et al., 2019) is\nused as the backbone of BERTScore.\n5.2 SelfCheckGPT with Question Answering\nWe also consider using the automatic multiple-\nchoice question answering generation (MQAG)\nframework (Manakul et al., 2023) to measure con-\nsistency for SelfCheckGPT. MQAG assesses con-\nsistency by generating multiple-choice questions\nover the main generated response, which an inde-\npendent answering system can attempt to answer\nwhile conditioned on the other sampled responses.\nIf questions on consistent information are queried,\nthe answering system is expected to predict similar\nanswers. MQAG consists of two stages: question\ngeneration G and question answering A. For the sen-\ntence ri in the response R, we draw questions q\nand options o:\nq,o ∼PG(q,o|ri,R) (2)\nThe answering stage A selects the answers:\naR = argmax\nk\n[PA(ok|q,R, o)] (3)\naSn = argmax\nk\n[PA(ok|q,Sn,o)] (4)\nWe compare whether aR is equal to aSn for each\nsample in {S1,...,S N}, yielding #matches Nm and\n#not-matches Nn. A simple inconsistency score\nfor the i-th sentence and question qbased on the\nmatch/not-match counts is defined: SQA(i,q) =\n3With the exception of SelfCheckGPT withn-gram as the\nscore of the n-gram language model is not bounded.\nNn\nNm+Nn . To take into account the answerability of\ngenerated questions, we show in Appendix B that\nwe can modify the inconsistency score by applying\nsoft-counting, resulting in:\nSQA(i,q) = γN′\nn\n2\nγN′m\n1 + γN′n\n2\n(5)\nwhere N′\nm = the effective match count, N′\nn = the\neffective mismatch count, with γ1 and γ2 defined\nin Appendix B.1. Ultimately, SelfCheckGPT with\nQA is the average of inconsistency scores across q,\nSQA(i) =Eq[SQA(i,q)] (6)\n5.3 SelfCheckGPT with n-gram\nGiven samples {S1,...,S N}generated by an LLM,\none can use the samples to create a new language\nmodel that approximates the LLM. In the limit as\nN gets sufficiently large, the new language model\nwill converge to the LLM that generated the re-\nsponses. We can therefore approximate the LLM’s\ntoken probabilities using the new language model.\nIn practice, due to time and/or cost constraints,\nthere can only be a limited number of samples N.\nConsequently, we train a simple n-gram model us-\ning the samples {S1,...,S N}as well as the main\nresponse R (which is assessed), where we note\nthat including Rcan be considered as a smoothing\nmethod where the count of each token in Ris in-\ncreased by 1. We then compute the average of the\nlog-probabilities of the sentence in response R,\nSAvg\nn-gram(i) =−1\nJ\n∑\nj\nlog ˜pij (7)\nwhere ˜pij is the probability (of the j-th token of the\ni-th sentence) computed using the n-gram model.\nSimilar to the grey-box approach, we can also use\nthe maximum of the negative log probabilities,\nSMax\nn-gram(i) = max\nj\n(−log ˜pij) (8)\n5.4 SelfCheckGPT with NLI\nNatural Language Inference (NLI) determines\nwhether a hypothesis follows a premise, classified\ninto either entailment/neutral/contradiction. NLI\nmeasures have been used to measure faithfulness in\nsummarization, where Maynez et al. (2020) use\na textual entailment classifier trained on MNLI\n(Williams et al., 2018) to determine if a summary\ncontradicts a context or not. Inspired by NLI-based\n9007\nsummary assessment, we consider using the NLI\ncontradiction score as a SelfCheckGPT score.\nFor SelfCheck-NLI, we use DeBERTa-v3-large\n(He et al., 2023) fine-tuned to MNLI as the NLI\nmodel. The input for NLI classifiers is typically the\npremise concatenated to the hypothesis, which\nfor our methodology is the sampled passage Sn\nconcatenated to the sentence to be assessed ri.\nOnly the logits associated with the ‘entailment’\nand ‘contradiction’ classes are considered,\nP(contradict|ri,Sn) = exp(zc)\nexp(ze) + exp(zc) (9)\nwhere ze and zc are the logits of the ‘entailment’\nand ‘contradiction’ classes, respectively. This nor-\nmalization ignores the neutral class and ensures\nthat the probability is bounded between 0.0 and\n1.0. The SelfCheckGPT with NLI score for each\nsample Sn is then defined as,\nSNLI(i) = 1\nN\nN∑\nn=1\nP(contradict|ri,Sn) (10)\n5.5 SelfCheckGPT with Prompt\nLLMs have recently been shown to be effective in\nassessing information consistency between a doc-\nument and its summary in zero-shot settings (Luo\net al., 2023). Thus, we query an LLM to assess\nwhether the i-th sentence is supported by sample\nSn (as the context) using the following prompt.\n------------------------------------------------\nContext: {}\nSentence: {}\nIs the sentence supported by the context above?\nAnswer Yes or No:\n------------------------------------------------\nInitial investigation showed that GPT-3 (text-\ndavinci-003) will output either Yes or No 98% of\nthe time, while any remaining outputs can be set to\nN/A. The output from prompting when comparing\nthe i-th sentence against sample Sn is converted to\nscore xn\ni through the mapping {Yes: 0.0, No: 1.0,\nN/A: 0.5}. The final inconsistency score is then\ncalculated as:\nSPrompt(i) = 1\nN\nN∑\nn=1\nxn\ni (11)\nSelfCheckGPT-Prompt is illustrated in Figure 1.\nNote that our initial investigations found that less\ncapable models such as GPT-3 (text-curie-001) or\nLLaMA failed to effectively perform consistency\nassessment via such prompting.\n6 Data and Annotation\nAs, currently, there are no standard hallucination\ndetection datasets available, we evaluate our hallu-\ncination detection approaches by 1) generating syn-\nthetic Wikipedia articles using GPT-3 on the indi-\nviduals/concepts from the WikiBio dataset (Lebret\net al., 2016); 2) manually annotating the factuality\nof the passage at a sentence level; 3) evaluating the\nsystem’s ability to detect hallucinations.\nWikiBio is a dataset where each input contains\nthe first paragraph (along with tabular information)\nof Wikipedia articles of a specific concept. We rank\nthe WikiBio test set in terms of paragraph length\nand randomly sample 238 articles from the top\n20% of longest articles (to ensure no very obscure\nconcept is selected). GPT-3 (text-davinci-003) is\nthen used to generate Wikipedia articles on a con-\ncept, using the prompt \" This is a Wikipedia\npassage about {concept} :\". Table 1 provides\nthe statistics of GPT-3 generated passages.\n#Passages #Sentences #Tokens/passage\n238 1908 184.7 ±36.9\nTable 1: The statistics of WikiBio GPT-3 dataset where the\nnumber of tokens is based on the OpenAI GPT-2 tokenizer.\nWe then annotate the sentences of the generated\npassages using the guidelines shown in Figure 3\nsuch that each sentence is classified as either:\n• Major Inaccurate (Non-Factual, 1): The sen-\ntence is entirely hallucinated, i.e. the sentence\nis unrelated to the topic.\n• Minor Inaccurate (Non-Factual, 0.5): The\nsentence consists of some non-factual infor-\nmation, but the sentence is related to the topic.\n• Accurate (Factual, 0): The information pre-\nsented in the sentence is accurate.\nOf the 1908 annotated sentences, 761 (39.9%) of\nthe sentences were labelled major-inaccurate, 631\n(33.1%) minor-inaccurate, and 516 (27.0%) accu-\nrate. 201 sentences in the dataset had annotations\nfrom two different annotators. To obtain a single la-\nbel for this subset, if both annotators agree, then the\nagreed label is used. However, if there is disagree-\nment, then the worse-case label is selected (e.g.,\n{minor inaccurate, major inaccurate} is mapped to\nmajor inaccurate). The inter-annotator agreement,\nas measured by Cohen’s κ(Cohen, 1960), has κ\n9008\nIs it related to\nthe context\nMajor Inaccurate\n(Non-factual 1)\nNo\nIs it Factual?\ne.g. using Wikipedia /\nGoogle Search\nYes\nNo Minor Inaccurate\n(Non-factual 0.5)\nYes\nAccurate\n(Factual 0)\nFigure 3: Flowchart of our annotation process\nvalues of 0.595 and 0.748, indicating moderate and\nsubstantial agreement (Viera et al., 2005) for the\n3-class and 2-class scenarios, respectively.4\nFurthermore, passage-level scores are obtained\nby averaging the sentence-level labels in each pas-\nsage. The distribution of passage-level scores is\nshown in Figure 4, where we observe a large peak\nat +1.0. We refer to the points at this peak as total\nhallucination, which occurs when the information\nof the response is unrelated to the real concept and\nis entirely fabricated by the LLM.\n0.0 0.2 0.4 0.6 0.8 1.0\nAvg. Factuality per Document (0=Factual, +1=Non-Factual)\n0\n5\n10\n15\n20\n25\n30Count\nFigure 4: Document factuality scores histogram plot\n7 Experiments\nThe generative LLM used to generate passages for\nour dataset is GPT-3 (text-davinci-003), the state-\nof-the-art system at the time of creating and anno-\ntating the dataset. To obtain the main response, we\nset the temperature to 0.0 and use standard beam\nsearch decoding. For the stochastically generated\nsamples, we set the temperature to 1.0 and generate\n43-class refers to when selecting between accurate, mi-\nnor inaccurate, major inaccurate. 2-class refers to when mi-\nnor/major inaccuracies are combined into one label.\nN=20 samples. For the proxy LLM approach, we\nuse LLaMA (Touvron et al., 2023), one of the best-\nperforming open-source LLMs currently available.\nFor SelfCheckGPT-Prompt, we consider both GPT-\n3 (which is the same LLM that is used to generate\npassages) as well as the newly released ChatGPT\n(gpt-3.5-turbo). More details about the systems in\nSelfCheckGPT and results using other proxy LLMs\ncan be found in the appendix.\n7.1 Sentence-level Hallucination Detection\nFirst, we investigate whether our hallucination de-\ntection methods can identify the factuality of sen-\ntences. In detecting non-factual sentences, both\nmajor-inaccurate labels and minor-inaccurate la-\nbels are grouped together into thenon-factual class,\nwhile the factual class refers to accurate sentences.\nIn addition, we consider a more challenging task of\ndetecting major-inaccurate sentences in passages\nthat are not total hallucination passages, which we\nrefer to as non-factual∗.5 Figure 5 and Table 2\nshow the performance of our approaches, where\nthe following observations can be made:\n1) LLM’s probabilities pcorrelate well with\nfactuality. Our results show that probability mea-\nsures (from the LLM generating the texts) are\nstrong baselines for assessing factuality. Factual\nsentences can be identified with an AUC-PR of\n53.97, significantly better than the random baseline\nof 27.04, with the AUC-PR for hallucination detec-\ntion also increasing from 72.96 to 83.21. This sup-\nports the hypothesis that when the LLMs are uncer-\ntain about generated information, generated tokens\noften have higher uncertainty, paving a promising\ndirection for hallucination detection approaches.\nAlso, the probability p measure performs better\nthan the entropy Hmeasure of top-5 tokens.\n2) Proxy LLM perform noticeably worse than\nLLM (GPT-3). The results of proxy LLM (based\non LLaMA) show that the entropy Hmeasures\noutperform the probability measures. This sug-\ngests that using richer uncertainty information can\nimprove factuality/hallucination detection perfor-\nmance, and that previously the entropy of top-5\ntokens is likely to be insufficient. In addition, when\nusing other proxy LLMs such as GPT-NeoX or\nOPT-30B, the performance is near that of the ran-\ndom baseline. We believe this poor performance\noccurs as different LLMs have different generating\npatterns, and so even common tokens may have a\n5There are 206 non-factual∗passages (1632 sentences).\n9009\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00Precision\nRandom\nGPT-3 Avg(-logP)\nSelfCk-BERTScore\nSelfCk-QA\nSelfCk-Unigram\nSelfCk-Prompt\nSelfCk-NLI\n(a) Non-Factual Sentences\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Precision\nRandom\nGPT-3 Avg(-logP)\nSelfCk-BERTScore\nSelfCk-QA\nSelfCk-Unigram\nSelfCk-Prompt\nSelfCk-NLI (b) Non-Factual* Sentences\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Precision\nRandom\nGPT-3 Avg(-logP)\nSelfCk-BERTScore\nSelfCk-QA\nSelfCk-Unigram\nSelfCk-Prompt\nSelfCk-NLI (c) Factual Sentences\nFigure 5: PR-Curve of detecting non-factual and factual sentences in the GPT-3 generated WikiBio passages.\nMethod Sentence-level (AUC-PR) Passage-level (Corr.)\nNonFact NonFact* Factual Pearson Spearman\nRandom 72.96 29.72 27.04 - -\nGPT-3 (text-davinci-003)’s probabilities (LLM, grey-box)\nAvg(−logp) 83.21 38.89 53.97 57.04 53.93\nAvg(H)† 80.73 37.09 52.07 55.52 50.87\nMax(−logp) 87.51 35.88 50.46 57.83 55.69\nMax(H)† 85.75 32.43 50.27 52.48 49.55\nLLaMA-30B’s probabilities (Proxy LLM, black-box)\nAvg(−logp) 75.43 30.32 41.29 21.72 20.20\nAvg(H) 80.80 39.01 42.97 33.80 39.49\nMax(−logp) 74.01 27.14 31.08 -22.83 -22.71\nMax(H) 80.92 37.32 37.90 35.57 38.94\nSelfCheckGPT (black-box)\nw/ BERTScore 81.96 45.96 44.23 58.18 55.90\nw/ QA 84.26 40.06 48.14 61.07 59.29\nw/ Unigram (max) 85.63 41.04 58.47 64.71 64.91\nw/ NLI 92.50 45.17 66.08 74.14 73.78\nw/ Prompt 93.42 53.19 67.09 78.32 78.30\nTable 2: AUC-PR for sentence-level detection tasks. Passage-level ranking performances are measured by Pearson correlation\ncoefficient and Spearman’s rank correlation coefficient w.r.t. human judgements. The results of other proxy LLMs, in addition to\nLLaMA, can be found in the appendix. †GPT-3 API returns the top-5 tokens’ probabilities, which are used to compute entropy.\nlow probability in situations where the response\nis dissimilar to the generation style of the proxy\nLLM. We note that a weighted conditional LM\nscore such as BARTScore (Yuan et al., 2021) could\nbe incorporated in future investigations.\n3) SelfCheckGPT outperforms grey-box ap-\nproaches. It can be seen that SelfCheckGPT-\nPrompt considerably outperforms the grey-box ap-\nproaches (including GPT-3’s output probabilities)\nas well as other black-box approaches. Even other\nvariants of SelfCheckGPT, including BERTScore,\nQA, and n-gram, outperform the grey-box ap-\nproaches in most setups. Interestingly, despite be-\ning the least computationally expensive method,\nSelfCheckGPT with unigram (max) works well\nacross different setups. Essentially, when assessing\na sentence, this method picks up the token with\nthe lowest occurrence given all the samples. This\nsuggests that if a token only appears a few times\n(or once) within the generated samples (N=20), it\nis likely non-factual.\n4) SelfCheckGPT with n-gram. When inves-\ntigating the n-gram performance from 1-gram to\n5-gram, the results show that simply finding the\nleast likely token/ n-gram is more effective than\ncomputing the average n-gram score of the sen-\ntence, details in appendix Table 7. Additionally,\nas nincreases, the performance of SelfCheckGPT\nwith n-gram (max) drops.\n5) SelfCheckGPT with NLI . The NLI-based\n9010\n0.0 0.2 0.4 0.6 0.8 1.0\nHuman Score (0=Factual, +1=Non-Factual)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Method Score\n(a) GPT-3 Avg(−log p)\n0.0 0.2 0.4 0.6 0.8 1.0\nHuman Score (0=Factual, +1=Non-Factual)\n5\n10\n15\n20\n25Method Score (b) LLaMA-30B Avg(H)\n0.0 0.2 0.4 0.6 0.8 1.0\nHuman Score (0=Factual, +1=Non-Factual)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Method Score (c) SelfCheckGPT-Prompt\nFigure 6: Scatter plot of passage-level scores where Y-axis = Method scores, X-axis = Human scores. Correlations are reported\nin Table 2. The scatter plots of other SelfCheckGPT variants are provided in Figure 10 in the appendix.\nmethod outperforms all black-box and grey-box\nbaselines, and its performance is close to the per-\nformance of the Prompt method. As SelfCheck-\nGPT with Prompt can be computationally heavy,\nSelfCheckGPT with NLI could be the most practi-\ncal method as it provides a good trade-off between\nperformance and computation.\n7.2 Passage-level Factuality Ranking\nPrevious results demonstrate that SelfCheckGPT\nis an effective approach for predicting sentence-\nlevel factuality. An additional consideration is\nwhether SelfCheckGPT can also be used to de-\ntermine the overall factuality of passages. Passage-\nlevel factuality scores are calculated by averaging\nthe sentence-level scores over all sentences.\nSpassage = 1\n|R|\n∑\ni\nS(i) (12)\nwhere S(i) is the sentence-level score, and |R|\nis the number of sentences in the passage. Since\nhuman judgement is somewhat subjective, averag-\ning the sentence-level labels would lead to ground\ntruths with less noise. Note that for Avg( −log p)\nand Avg(H), we compute the average over all to-\nkens in a passage. Whereas for Max(−log p) and\nMax(H), we first take the maximum operation over\ntokens at the sentence level, and we then average\nover all sentences following Equation 12.\nOur results in Table 2 and Figure 6 show that all\nSelfCheckGPT methods correlate far better with\nhuman judgements than the other baselines, in-\ncluding the grey-box probability and entropy meth-\nods. SelfCheckGPT-Prompt is the best-performing\nmethod, achieving the highest Pearson correlation\nof 78.32. Unsurprisingly, the proxy LLM approach\nagain achieves considerably lower correlations.\n7.3 Ablation Studies\nExternal Knowledge (instead of SelfCheck)\nIf external knowledge is available, one can measure\nthe informational consistency between the LLM\nresponse and the information source. In this exper-\niment, we use the first paragraph of each concept\nthat is available in WikiBio.6\nMethod Sent-lvl AUC-PR Passage-lvl\nNoFac NoFac* Fact Pear. Spear.\nSelfCk-BERT 81.96 45.96 44.23 58.18 55.90\nWikiBio+BERT 81.32 40.62 49.15 58.71 55.80\nSelfCk-QA 84.26 40.06 48.14 61.07 59.29\nWikiBio+QA 84.18 45.40 52.03 57.26 53.62\nSelfCk-1gm 85.63 41.04 58.47 64.71 64.91\nWikiBio+1gm 80.43 31.47 40.53 28.67 26.70\nSelfCk-NLI 92.50 45.17 66.08 74.14 73.78\nWikiBio+NLI 91.18 48.14 71.61 78.84 80.00\nSelfCk-Prompt 93.42 53.19 67.09 78.32 78.30\nWikiBio+Prompt 93.59 65.26 73.11 85.90 86.11\nTable 3: The performance when using SelfCheckGPT samples\nversus external stored knowledge.\nOur findings in Table 3 show the following. First,\nSelfCheckGPT with BERTScore/QA, using self-\nsamples, can yield comparable or even better per-\nformance than when using the reference passage.\nSecond, SelfCheckGPT with n-gram shows a large\nperformance drop when using the WikiBio pas-\nsages instead of self-samples. This failure is at-\ntributed to the fact that the WikiBio reference text\nalone is not sufficient to train an n-gram model.\nThird, in contrast, SelfCheckGPT with NLI/Prompt\ncan benefit considerably when access to retrieved\ninformation is available. Nevertheless, in practice,\n6This method is no longer zero-resource as it requires\nretrieving relevant knowledge from external data.\n9011\nit is infeasible to have an external database for ev-\nery possible use case of LLM generation.\nThe Impact of the Number of Samples\nAlthough sample-based methods are expected to\nperform better when more samples are drawn, this\nhas higher computational costs. Thus, we inves-\ntigate performance as the number of samples is\nvaried. Our results in Figure 7 show that the per-\nformance of SelfCheckGPT increases smoothly as\nmore samples are used, with diminishing gains as\nmore samples are generated. SelfCheckGPT with\nn-gram requires the highest number of samples\nbefore its performance reaches a plateau.\n0 2 4 6 8 10 12 14 16 18 20\nNum. samples\n30\n40\n50\n60\n70\n80Spearman's RankCC\n SelfCk-BERTScore\nSelfCk-QA\nSelfCk-Unigram\nSelfCk-NLI\nSelfCk-Prompt\nFigure 7: The performance of SelfCheckGPT methods on\nranking passages (Spearman’s) versus the number of samples.\nThe Choice of LLM for SelfCheckGPT-Prompt\nWe investigate whether the LLM generating the\ntext can self-check its own text. We conduct this\nablation using a reduced set of the samples (N=4).\nText-Gen SelfCk-Prompt N Pear. Spear.\nGPT-3 ChatGPT 20 78.32 78.30\nGPT-3 ChatGPT 4 76.47 76.41\nGPT-3 GPT-3 4 73.11 74.69\n†SelfCheck w/ unigram (max) 20 64.71 64.91\n†SelfCheck w/ NLI 20 74.14 73.78\nTable 4: Comparison of GPT-3 (text-davinci-003) and Chat-\nGPT (gpt-3.5.turbo) as the prompt-based text evaluator in\nSelfCheckGPT-Prompt. †Taken from Table 2 for comparison.\nThe results in Table 4 show that GPT-3 can self-\ncheck its own text, and is better than the unigram\nmethod even when using only 4 samples. However,\nChatGPT shows a slight improvement over GPT-3\nin evaluating whether the sentence is supported by\nthe context. More details are in Appendix C.\n8 Conclusions\nThis paper is the first work to consider the task\nof hallucination detection for general large lan-\nguage model responses. We propose SelfCheck-\nGPT, a zero-resource approach that is applicable\nto any black-box LLM without the need for ex-\nternal resources, and demonstrate the efficacy of\nour method. SelfCheckGPT outperforms a range\nof considered grey-box and black-box baseline de-\ntection methods at both the sentence and passage\nlevels, and we further release an annotated dataset\nfor GPT-3 hallucination detection with sentence-\nlevel factuality labels.\nLimitations\nIn this study, the 238 GPT-3 generated texts were\npredominantly passages about individuals in the\nWikiBio dataset. To further investigate the nature\nof LLM’s hallucination, this study could be ex-\ntended to a wider range of concepts, e.g., to also\nconsider generated texts about locations and ob-\njects. Further, this work considers factuality at the\nsentence level, but we note that a single sentence\nmay consist of both factual and non-factual infor-\nmation. For example, the following work by Min\net al. (2023) considers a fine-grained factuality eval-\nuation by decomposing sentences into atomic facts.\nFinally, SelfCheckGPT with Prompt, which was\nconvincingly the best selfcheck method, is quite\ncomputationally heavy. This might lead to impracti-\ncal computational costs, which could be addressed\nin future work to be made more efficient.\nEthics Statement\nAs this work addresses the issue of LLM’s halluci-\nnation, we note that if hallucinated contents are not\ndetected, they could lead to misinformation.\nAcknowledgments\nThis work is supported by Cambridge University\nPress & Assessment (CUP&A), a department of\nThe Chancellor, Masters, and Scholars of the Uni-\nversity of Cambridge, and the Cambridge Com-\nmonwealth, European & International Trust. We\nwould like to thank the anonymous reviewers for\ntheir helpful comments.\n9012\nReferences\nAmos Azaria and Tom Mitchell. 2023. The internal\nstate of an llm knows when its lying. arXiv preprint\narXiv:2304.13734.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang,\nMichael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-\nhit, Laria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of BigScience Episode #5 – Workshop on Chal-\nlenges & Perspectives in Creating Large Language\nModels, pages 95–136, virtual+Dublin. Association\nfor Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJacob Cohen. 1960. A coefficient of agreement for\nnominal scales. Educational and Psychological Mea-\nsurement, 20:37 – 46.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire.\nZhijiang Guo, Michael Schlichtkrull, and Andreas Vla-\nchos. 2022. A survey on automated fact-checking.\nTransactions of the Association for Computational\nLinguistics, 10:178–206.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.\nDeBERTav3: Improving deBERTa using ELECTRA-\nstyle pre-training with gradient-disentangled embed-\nding sharing. In The Eleventh International Confer-\nence on Learning Representations.\nYichong Huang, Xiachong Feng, Xiaocheng Feng, and\nBing Qin. 2021. The factual inconsistency problem\nin abstractive text summarization: A survey.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651–3657, Florence, Italy. Association for\nComputational Linguistics.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv., 55(12).\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332–9346, Online. Association for Computa-\ntional Linguistics.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.\nSemantic uncertainty: Linguistic invariances for un-\ncertainty estimation in natural language generation.\nIn The Eleventh International Conference on Learn-\ning Representations.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages 785–\n794, Copenhagen, Denmark. Association for Compu-\ntational Linguistics.\nRémi Lebret, David Grangier, and Michael Auli. 2016.\nGenerating text from structured data with application\nto the biography domain. CoRR, abs/1603.07771.\nTianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao,\nZhifang Sui, Weizhu Chen, and Bill Dolan. 2022.\nA token-level reference-free hallucination detection\nbenchmark for free-form text generation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 6723–6737, Dublin, Ireland. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAdian Liusie, Vatsal Raina, and Mark Gales. 2023.\n“world knowledge” in multiple choice reading com-\nprehension. In Proceedings of the Sixth Fact Ex-\ntraction and VERification Workshop (FEVER), pages\n49–57, Dubrovnik, Croatia. Association for Compu-\ntational Linguistics.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency evaluator\nfor abstractive text summarization. arXiv preprint\narXiv:2303.15621.\n9013\nAndrey Malinin and Mark Gales. 2021. Uncertainty\nestimation in autoregressive structured prediction. In\nInternational Conference on Learning Representa-\ntions.\nPotsawee Manakul, Adian Liusie, and Mark JF Gales.\n2023. MQAG: Multiple-choice question answering\nand generation for assessing information consistency\nin summarization. arXiv preprint arXiv:2301.12307.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. arXiv preprint\narXiv:2305.14251.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nVatsal Raina and Mark Gales. 2022. Answer uncertainty\nand unanswerability in multiple-choice machine read-\ning comprehension. In Findings of the Association\nfor Computational Linguistics: ACL 2022 , pages\n1020–1034, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nJames Thorne, Andreas Vlachos, Oana Cocarascu,\nChristos Christodoulopoulos, and Arpit Mittal. 2018.\nThe Fact Extraction and VERification (FEVER)\nshared task. In Proceedings of the First Workshop on\nFact Extraction and VERification (FEVER).\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAnthony J Viera, Joanne M Garrett, et al. 2005. Under-\nstanding interobserver agreement: the kappa statistic.\nFam med, 37(5):360–363.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nYijun Xiao and William Yang Wang. 2021. On hal-\nlucination and predictive uncertainty in conditional\nlanguage generation. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, pages\n2734–2744, Online. Association for Computational\nLinguistics.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text gener-\nation. Advances in Neural Information Processing\nSystems, 34:27263–27277.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nZhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li,\nShuailiang Zhang, Xi Zhou, and Xiang Zhou. 2020.\nSemantics-aware bert for language understanding. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 9628–9635.\nWanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan\nDuan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020.\nReasoning over semantic-level graph for fact check-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6170–6180, Online. Association for Computational\nLinguistics.\n9014\nA Models and Implementation\nA.1 Entropy\nThe entropy of the output distribution is imple-\nmented as follows,\nHij = 2−∑\n˜w∈Wpij( ˜w) log2 pij( ˜w) (13)\nwhere Wis the set of all possible words in the\nvocabulary.\nA.2 Proxy LLMs\nThe proxy LLMs considered are LLaMA-{7B,\n13B, 30B} (Touvron et al., 2023), OPT-{125m,\n1.3B, 13B, 30B} (Zhang et al., 2022), GPT-J-6B\n(Wang and Komatsuzaki, 2021) and GPT-NeoX-\n20B (Black et al., 2022).\nA.3 SelfCheckGPT’s Systems\nQuestion Answering : The generation systems\nG1 and G2 are T5-Large fine-tuned to SQuAD\n(Rajpurkar et al., 2016) and RACE (Lai et al.,\n2017), respectively. The answering system A is\nLongformer (Beltagy et al., 2020) fine-tuned to the\nRACE dataset. The answerability system U is also\nLongformer, but fine-tuned to SQuAD2.0.\nLLM for Prompting : We consider two LLMs,\nGPT-3 (text-davinci-003) and ChatGPT (gpt-3.5-\nturbo) We note that during the data creation and\nannotation, GPT-3 (text-davinci-003) was the state-\nof-the-art LLM available; hence, GPT-3 was used\nas the main LLM generating WikiBio passages.\nB SelfCheckGPT with QA\nPrevious work showed that implementing question\ngeneration (in Equation 2) with two generators (G1\ngenerates the question and associated answer, and\nG2 generates distractors) yields higher-quality dis-\ntractors (Manakul et al., 2023). Thus, a two-stage\ngeneration is adopted in this work as follows:\nq,a ∼PG1(q,a|ri); o\\a ∼PG2(o\\a|q,a,R )\n(14)\nwhere o = {a,o\\a}= {o1,...,o 4}. In addition, to\nfilter out bad (unanswerable) questions, we define\nan answerability score (Raina and Gales, 2022):\nα= PU(answerable|q,context) (15)\nwhere the context is either the response Ror sam-\npled passages Sn, and α→0.0 for unanswerable\nand α→1.0 for answerable. We use αto filter out\nunanswerable questions which have αlower than\na threshold. Next, we derive how Bayes’ theorem\ncan be applied to take into account the number of\nanswerable/unanswerable questions.\nB.1 SelfCheckGPT-QA with Bayes\nLet P(F) denote the probability of thei-th sentence\nbeing non-factual, and P(T) denote the probability\nof the i-th sentence being factual. For a question q,\nthe probability of i-th sentence being non-factual\ngiven a set of matched answers Lm and a set of\nnot-matched answers Ln is:\nP(F|Lm,Ln)\n= P(Lm,Ln|F)P(F)\nP(Lm,Ln|F)P(F) +P(Lm,Ln|T)P(T)\n= P(Lm,Ln|F)\nP(Lm,Ln|F) +P(Lm,Ln|T) (16)\nwhere we assume the sentence is equally likely to\nbe False or True, i.e. P(F) =P(T). The probabil-\nity of observing Lm,Ln when the sentence is False\n(non-factual):\nP(Lm,Ln|F)\n=\n∏\na∈Lm\nP(a= aR|F)\n∏\na′∈Ln\nP(a′̸= aR|F)\n= (1−β1)Nm (β1)Nn (17)\nand probability of observing Lm,Ln when the sen-\ntence is True (factual):\nP(Lm,Ln|T)\n=\n∏\na∈Lm\nP(a= ar|T)\n∏\na′∈Ln\nP(a′̸= ar|T)\n= (β2)Nm (1 −β2)Nn (18)\nwhere Nm and Nn are the number of matched an-\nswers and the number of not-matched answers, re-\nspectively. Hence, we can simplify Equation 16:\nP(F|Lm,Ln) = γNn\n2\nγNm\n1 + γNn\n2\n(19)\nwhere γ1 = β2\n1−β1\nand γ2 = β1\n1−β2\n. Lastly, instead\nof rejecting samples having an answerability score\nbelow a threshold,7 we find empirically that soft-\ncounting (defined below) improves the detection\nperformance. We set both β1 and β2 to 0.8.\n7αis between 0.0 (unanswerable) and 1.0 (answerable).\nStandard-counting Nm and Nn can be considered as a special\ncase of soft-counting where αis set to 1.0 if αis greater than\nthe answerability threshold and otherwise αis 0.0.\n9015\nN′\nm =\n∑\nn s.t. an∈Lm\nαn; N′\nn =\n∑\nn s.t. an∈Ln\nαn (20)\nwhere αn = PU(answerable|q,Sn). Therefore, the\nSelfCheckGPT with QA score, SQA, is:\nSQA = P(F|Lm,Ln) = γN′\nn\n2\nγN′m\n1 + γN′n\n2\n(21)\nIn Table 5, we show empically that applying Bayes’\ntheorem and soft counting α(in Equation 20) im-\nproves the performance of the SelfCheckGPT with\nQA method.\nVaraint Sentence-lvl Passage-lvl\nNoF NoF* Fact PCC SCC\nSimpleCount 83.97 40.07 47.78 57.39 55.15\n+ Bayes 83.04 38.58 47.41 56.43 55.03\n+ Bayes + α 84.26 40.06 48.14 61.07 59.29\nTable 5: Performance of SelfCheckGPT-QA’s variants.\nC SelfCheckGPT with Prompt\nWe use the prompt template provided in the main\ntext (in Section 5.5) for both GPT-3 (text-davinci-\n003) and ChatGPT (gpt-3.5-turbo). For ChatGPT,\na standard system message \"You are a helpful\nassistant.\" is used in setting up the system.\nAt the time of conducting experiments, the API\ncosts per 1,000 tokens are $0.020 for GPT-3 and\n$0.002 for ChatGPT. The estimated costs for run-\nning the models to answer Yes/No on all 1908 sen-\ntences and 20 samples are around $200 for GPT-3\nand $20 for ChatGPT. Given the cost, we conduct\nthe experiments on 4 samples when performing\nthe ablation about LLM choice for SelfCheckGPT-\nPrompt (Section 7.3). Table 6 shows the breakdown\nof predictions made by GPT-3 and ChatGPT.\nGPT-3\nChatGPT Yes No\nYes 3179 1038\nNo 367 3048\nTable 6: Breakdown of predictions made by GPT-3/ChatGPT\nwhen prompted to answer Yes(supported)/No(not-supported).\nD Additional Experimental Results\nHere, we provide experimental results that are com-\nplementary to those presented in the main paper.\nn-gram Sent-lvl AUC-PR Passage-lvl\nNoFac NoFac* Fact Pear. Spear.\nAvg(−logp)\n1-gram 81.52 40.33 41.76 40.68 39.22\n2-gram 82.94 44.38 52.81 58.84 58.11\n3-gram 83.56 44.64 53.99 62.21 63.00\n4-gram 83.80 43.55 54.25 61.98 63.64\n5-gram 83.45 42.31 53.98 60.68 62.96\nMax(−logp)\n1-gram 85.63 41.04 58.47 64.71 64.91\n2-gram 85.26 39.29 58.29 62.48 66.04\n3-gram 84.97 37.10 57.08 57.34 60.49\n4-gram 84.49 36.37 55.96 55.77 57.25\n5-gram 84.12 36.19 54.89 54.84 55.97\nTable 7: The performance using different n-gram models in\nthe SelfCheckGPT with n-gram method.\n0 2 4 6 8 10 12 14 16 18 20\nNum. samples\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5AUC-PR\nSelfCk-BERTScore\nSelfCk-QA\nSelfCk-Unigram\nSelfCk-NLI\nSelfCk-Prompt\nFigure 8: The performance of SelfCheckGPT methods on\nsentence-level non-factual detection (AUC-PR) versus the\nnumber of samples. This Figure extends the passage-level\nresults in Figure 7.\n125m1.3B 6B 13B 20B 30B\nModel Size\n10\n0\n10\n20\n30\n40\nSpearman\nLLaMA\nOPT,GPT-J,NeoX\nFigure 9: Passage-level ranking performance of the Avg(H)\nmethod using proxy LLM where the sizes are: LLaMA={7B,\n13B, 30B}, OPT={125m, 1.3B, 13B, 30B}, GPT-J=6B,\nNeoX=20B. The full results are provided in Table 8.\n9016\n0.0 0.2 0.4 0.6 0.8 1.0\nHuman Score (0=Factual, +1=Non-Factual)\n0.04\n0.06\n0.08\n0.10\n0.12Method Score\n(a) SelfCheckGPT-BERTScore\n0.0 0.2 0.4 0.6 0.8 1.0\nHuman Score (0=Factual, +1=Non-Factual)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Method Score (b) SelfCheckGPT-QA\n0.0 0.2 0.4 0.6 0.8 1.0\nHuman Score (0=Factual, +1=Non-Factual)\n5.5\n6.0\n6.5\n7.0\n7.5\n8.0Method Score (c) SelfCheckGPT-1gram(max)\n0.0 0.2 0.4 0.6 0.8 1.0\nHuman Score (0=Factual, +1=Non-Factual)\n0.2\n0.4\n0.6\n0.8\n1.0Method Score (d) SelfCheckGPT-NLI\nFigure 10: Scatter plot of passage-level scores where Y-axis = Method scores, X-axis = Human scores. Correlations are reported\nin Table 2. This figure provides results in addition to Figure 6.\nLLM Size Sentence-level (AUC-PR) Passage-level (Corr.)\nNonFact NonFact* Factual Pearson Spearman\nRandom - 72.96 29.72 27.04 - -\nAvg(−logp) Method\nLLaMA 30B 75.43 30.32 41.29 21.72 20.20\nLLaMA 13B 74.16 30.01 37.36 13.33 12.89\nLLaMA 7B 71.69 27.87 31.30 -2.71 -2.59\nOPT 30B 67.70 24.43 25.04 -32.07 -31.45\nNeoX 20B 69.00 24.38 26.18 -31.79 -34.15\nOPT 13B 67.46 24.39 25.20 -33.05 -32.79\nGPT-J 6B 67.51 24.28 24.26 -38.80 -40.05\nOPT 1.3B 66.19 24.47 23.47 -35.20 -38.95\nOPT 125m 66.63 25.31 23.07 -30.38 -37.54\nAvg(H) Method\nLLaMA 30B 80.80 39.01 42.97 33.80 39.49\nLLaMA 13B 80.63 38.98 40.59 29.43 33.12\nLLaMA 7B 78.67 37.22 33.81 19.44 21.79\nOPT 30B 77.13 33.67 29.55 -0.43 3.43\nNeoX 20B 77.40 32.78 30.13 5.41 7.43\nOPT 13B 76.93 33.71 29.68 0.25 1.39\nGPT-J 6B 76.15 33.29 28.30 -2.50 -1.37\nOPT 1.3B 74.05 31.91 26.33 -10.59 -10.00\nOPT 125m 71.51 30.88 25.36 -14.16 -13.76\nMax(−logp) Method\nLLaMA 30B 74.01 27.14 31.08 -22.83 -22.71\nLLaMA 13B 71.12 26.78 28.82 -34.93 -31.70\nLLaMA 7B 69.57 25.91 26.54 -42.57 -38.24\nOPT 30B 67.32 24.40 24.32 -49.51 -45.50\nNeoX 20B 67.51 23.88 24.82 -47.96 -44.54\nOPT 13B 67.36 24.67 24.46 -50.15 -44.42\nGPT-J 6B 67.58 23.94 23.93 -51.23 -47.68\nOPT 1.3B 68.16 25.85 24.66 -45.60 -42.39\nOPT 125m 69.23 27.66 24.14 -39.22 -37.18\nMax(H) Method\nLLaMA 30B 80.92 37.32 37.90 35.57 38.94\nLLaMA 13B 80.98 37.94 36.01 32.07 34.01\nLLaMA 7B 79.65 35.57 31.32 22.10 22.53\nOPT 30B 76.58 33.44 29.31 1.63 6.41\nNeoX 20B 76.98 31.96 29.13 5.97 9.31\nOPT 13B 76.26 32.81 29.25 1.42 2.82\nGPT-J 6B 75.30 32.51 28.13 -2.14 1.41\nOPT 1.3B 73.79 31.42 26.38 -9.84 -9.80\nOPT 125m 71.32 31.65 25.36 -18.05 -17.37\nTable 8: AUC-PR for Detecting Non-Factual and Factual Sentences in the GPT-3 generated WikiBio passages. Passage-level\nPCC and SCC with LLMs used to assess GPT-3 responses. This table is an extension to Table 2.\n9017",
  "topic": "Hallucinating",
  "concepts": [
    {
      "name": "Hallucinating",
      "score": 0.9820280075073242
    },
    {
      "name": "Computer science",
      "score": 0.7362278699874878
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.622717022895813
    },
    {
      "name": "Sentence",
      "score": 0.5926380157470703
    },
    {
      "name": "Generative grammar",
      "score": 0.5603088140487671
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5526127815246582
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5514890551567078
    },
    {
      "name": "Black box",
      "score": 0.5414992570877075
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.5384971499443054
    },
    {
      "name": "Natural language processing",
      "score": 0.5343249440193176
    },
    {
      "name": "Generative model",
      "score": 0.4998776912689209
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.4906269609928131
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.4625721573829651
    },
    {
      "name": "Machine learning",
      "score": 0.4445892572402954
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.44204625487327576
    },
    {
      "name": "Linguistics",
      "score": 0.16691303253173828
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.11598321795463562
    },
    {
      "name": "Computer vision",
      "score": 0.11381605267524719
    },
    {
      "name": "Mathematics",
      "score": 0.10738241672515869
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}