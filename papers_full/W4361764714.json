{
    "title": "Voice Activity Detection Optimized by Adaptive Attention Span Transformer",
    "url": "https://openalex.org/W4361764714",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5028135199",
            "name": "Wenpeng Mu",
            "affiliations": [
                "Nanjing University of Information Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5102840238",
            "name": "Bingshan Liu",
            "affiliations": [
                "Nanjing University of Information Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2156798212",
        "https://openalex.org/W3007258144",
        "https://openalex.org/W2104052971",
        "https://openalex.org/W2131774270",
        "https://openalex.org/W3208650852",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4312899222",
        "https://openalex.org/W2988379071",
        "https://openalex.org/W2031647436",
        "https://openalex.org/W2150658333",
        "https://openalex.org/W3213321795",
        "https://openalex.org/W3201541172",
        "https://openalex.org/W2954930777",
        "https://openalex.org/W2791616807",
        "https://openalex.org/W6757632829"
    ],
    "abstract": "Voice Activity Detection (VAD) is a widely used technique for separating vocal regions from audio signals, with applications in voice language coding, noise reduction, and other domains. While various strategies have been proposed to improve VAD performance, such as ACAM, DCU-10, and Tr-VAD, these approaches often suffer from common limitations, including being unsuitable for long audio and being time-consuming. To address these issues, a new method called AAT-VAD is proposed, which integrates an adaptive width attention learning mechanism into the classic transformer framework. The approach involves extracting Mel-scale Frequency Cepstral Coefficients (MFCC) from the Mel scale frequency domain, adding a masking function to each transformer attention head, and inputting the features processed by the transformer encoder layer into the classifier. Experimental results indicate that a 12.8&#x0025; higher F1-score is achieved by the method compared to DCU-10, and a 0.6&#x0025; higher F1-score is achieved compared to Tr-VAD under different noise interferences. Furthermore, the average detection cost function (DCF) value of the method is only 14.3&#x0025; of DCU-10 and 92.4&#x0025; of Tr-VAD, and the test time of AAT-VAD is only 37.4&#x0025; of that of Tr-VAD for the same noisy vocal mixed audio.",
    "full_text": null
}