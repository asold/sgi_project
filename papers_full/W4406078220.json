{
    "title": "Examining epistemological challenges of large language models in law",
    "url": "https://openalex.org/W4406078220",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2967520414",
            "name": "Ludovica Paseri",
            "affiliations": [
                "University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A2090628909",
            "name": "Massimo Durante",
            "affiliations": [
                "University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A2967520414",
            "name": "Ludovica Paseri",
            "affiliations": [
                "University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A2090628909",
            "name": "Massimo Durante",
            "affiliations": [
                "University of Turin"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4389834551",
        "https://openalex.org/W4312920596",
        "https://openalex.org/W4399564550",
        "https://openalex.org/W2117373207",
        "https://openalex.org/W6839034341",
        "https://openalex.org/W4385397741",
        "https://openalex.org/W3124945098",
        "https://openalex.org/W6856013587",
        "https://openalex.org/W4290994954",
        "https://openalex.org/W4389161416",
        "https://openalex.org/W4401178570",
        "https://openalex.org/W6882201909",
        "https://openalex.org/W6799550805",
        "https://openalex.org/W6850074182",
        "https://openalex.org/W4226155525",
        "https://openalex.org/W6861500077",
        "https://openalex.org/W7029673229",
        "https://openalex.org/W3127986942",
        "https://openalex.org/W4386020174",
        "https://openalex.org/W4396569956",
        "https://openalex.org/W4392035583",
        "https://openalex.org/W4394804610",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W4296963455",
        "https://openalex.org/W2079455354",
        "https://openalex.org/W6980551560",
        "https://openalex.org/W6867088002",
        "https://openalex.org/W4360957277",
        "https://openalex.org/W4392058134",
        "https://openalex.org/W2902517282",
        "https://openalex.org/W4386122321",
        "https://openalex.org/W3138698152",
        "https://openalex.org/W6870806557",
        "https://openalex.org/W3095319910",
        "https://openalex.org/W4229456833",
        "https://openalex.org/W4285272463",
        "https://openalex.org/W2042563853",
        "https://openalex.org/W4396718796",
        "https://openalex.org/W4391774204",
        "https://openalex.org/W582315782",
        "https://openalex.org/W2315404267",
        "https://openalex.org/W2962681511",
        "https://openalex.org/W4400484004",
        "https://openalex.org/W4386729081",
        "https://openalex.org/W1507980802",
        "https://openalex.org/W4391234699",
        "https://openalex.org/W4212837467",
        "https://openalex.org/W4399988903",
        "https://openalex.org/W4281748205",
        "https://openalex.org/W3184537142",
        "https://openalex.org/W4396914038",
        "https://openalex.org/W3210221975"
    ],
    "abstract": "Abstract Large Language Models (LLMs) raises challenges that can be examined according to a normative and an epistemological approach. The normative approach, increasingly adopted by European institutions, identifies the pros and cons of technological advancement. Regarding LLMs, the main pros concern technological innovation, economic development and the achievement of social goals and values. The disadvantages mainly concern cases of risks and harms generated by means of LLMs. The epistemological approach examines how LLMs produce outputs, information, knowledge, and a representation of reality in ways that differ from those followed by human beings. To face the impact of LLMs, our paper contends that the epistemological approach should be examined as a priority: identifying risks and opportunities of LLMs also depends on considering how this form of artificial intelligence works from an epistemological point of view. To this end, our analysis compares the epistemology of LLMs with that of law, in order to highlight at least five issues in terms of: (i) qualification ; (ii) reliability ; (iii) pluralism and novelty ; (iv) technological dependence and (v) relation to truth and accuracy . The epistemological analysis of these issues, preliminary to the normative one, lays the foundations to better frame challenges and opportunities arising from the use of LLMs.",
    "full_text": "Cambridge Forum on AI: Law and Governance (2025), 1, e7, 1–13\ndoi:10.1017/cfl.2024.7\nRESEARCH ARTICLE\nExamining epistemological challenges of large language\nmodels in law\nLudovica Paseri\n and Massimo Durante\nLaw Department, University of Turin, Torino, Italy\nCorresponding author: Ludovica Paseri; Email:ludovica.paseri@unito.it\nLudovica Paseri wrote sections 2, 3.1, 3.2, 3.3, 3.4, 4; Massimo Durante wrote sections 1, 3.5, 4.\n(Received 01 August 2024; revised 30 October 2024; accepted 06 November 2024)\nAbstract\nLarge Language Models (LLMs) raises challenges that can be examined according to anormative and an\nepistemological approach. The normative approach, increasingly adopted by European institutions, identi-\nfiestheprosandconsoftechnologicaladvancement.RegardingLLMs,themainprosconcerntechnological\ninnovation, economic development and the achievement of social goals and values. The disadvantages\nmainlyconcerncasesofrisksandharmsgeneratedbymeansofLLMs.Theepistemologicalapproachexam-\nines how LLMs produce outputs, information, knowledge, and a representation of reality in ways that differ\nfrom those followed by human beings. To face the impact of LLMs, our paper contends that the episte-\nmological approach should be examined as a priority: identifying risks and opportunities of LLMs also\ndepends on considering how this form of artificial intelligence works from an epistemological point of\nview. To this end, our analysis compares the epistemology of LLMs with that of law, in order to highlight\nat least five issues in terms of: (i)qualification; (ii)reliability; (iii)pluralism and novelty; (iv)technological\ndependence and (v)relation to truth and accuracy. The epistemological analysis of these issues, preliminary\nto the normative one, lays the foundations to better frame challenges and opportunities arising from the\nuse of LLMs.\nKeywords: LLMs; artificial intelligence; epistemological approach; ethics of AI\n1. Introduction\nAsimplesearchon ssrn.com onthetopiclargelanguagemodels(LLMs)returnsmorethan400results\nonlyfor2024.ManypapersfocusonthetechnicalaspectsofLLMs,tohighlightglitchesandpotentials\nof these artificial intelligence (AI) models that have had such a strong recent development: these\ncontributions put the focus on innovation aspects (Diab,2024; Moreau et al.,2023). Other papers\nanalyze the risks, challenges or benefits that the use of such models may have in different areas of\napplication or on our world at large: these papers put emphasis on the impact of these models on\ncontemporary society (Ferrara,2024; Novelli et al.,2024; Pagallo,2022; Wachter et al.,2024). Still\nother papers focus on the ability of LLMs to reshape traditional activities, jobs or professional fields:\nthese contributions highlight the transformative power of these technologies (Fagan, forthcoming,\n2025; Nelson,2024; Surden,2023). Finally, some papers focus on the ability of such models to change\nour perception and understanding of reality, creating new pieces of knowledge: these contributions\nexamine the epistemological impact of LLMs (Delacroix,2024; Krook,2024).\n© The Author(s), 2025. Published by Cambridge University Press. This is an Open Access article, distributed under the terms of the Creative\nCommons Attribution licence (http://creativecommons.org/licenses/by/4.0), which permits unrestricted re-use, distribution and\nreproduction, provided the original article is properly cited.\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press\n2 Ludovica Paseri and Massimo Durante\nThis wide range of topics indirectly shows that LLMs potentially have a strong impact on every\naspect of reality, with innovative and transformative effects raising risks, challenges and opportuni-\nties that require to be increasingly addressed not only from a broad normative angle (ethical, legal,\npolitical, economic, social, etc.) but also from a deep epistemological perspective, as they change the\nway we grasp and know our own reality. This raises a complex and thorny governance problem, since\nlaw has traditionally been used to regulatebehaviors and legally relevantconsequences arising from\nbehaviors (whether the product of human actions or, increasingly, also of actions enacted by artificial\nagents) but notmodels (and particularly AI models) that affect the perception and understanding of\nreality or the production of knowledge.\nConsider, for example, the difficulties faced by the European or American lawmakers in legally\ndefining and regulating such new AI models. The European regulation on artificial intelligence (AI\nAct, hereinafter)1 defines AI system in terms of\na machine-based system that is designed to operate with varying levels of autonomy and\nthat may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives,\ninfers, from the input it receives, how to generate outputs such as predictions, content, rec-\nommendations, or decisions that can influence physical or virtual environments (Art. 3(1) AI\nAct).\nHowever,theconsiderabledevelopmentsinthefieldofAIhavepromptedtheEuropeanlawmakers\nto include in Art. 3(63) AI Act a further definition of the so-called “general-purpose AI models,”\ndefined as follows:\nan AI model, including where such an AI model is trained with a large amount of data using\nself-supervision at scale, that displays significant generality and is capable of competently per-\nforming a wide range of distinct tasks regardless of the way the model is placed on the market\nandthatcanbeintegratedintoavarietyofdownstreamsystemsorapplications,exceptAImod-\nels that are used for research, development or prototyping activities before they are placed on\nthe market.\nUS have taken a distinct approach in the Executive Order No. 14110 on the safe, secure, and\ntrustworthy development and use of AI, signed by the US President Joe Biden on 30 October 2023,2\nproposing a definition of the term AI,3 another of the AImodel,4 and a third one about the AIsys-\ntem.5 Furthermore, the §3(p) of the Executive Order also includes a specific mention of generative\nAI, defined as follows:\n1Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonized rules\non artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858,\n(EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence\nAct), OJ L, 2024/1689, 12.7.2024, ELI:http://data.europa.eu/eli/reg/2024/1689/oj.\n2The White House,Executive order on the safe, secure, and trustworthy development and use of artificial intelligence , no.\n14110, 30 October 2023, available athttps://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-\norder-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/.\n3AI is defined as “a machine-based system that can, for a given set of human-defined objectives, make predictions,\nrecommendations, or decisions influencing real or virtual environments. Artificial intelligence systems use machine- and\nhuman-based inputs to perceive real and virtual environments; abstract such perceptions into models through analysis in\nan automated manner; and use model inference to formulate options for information or action,” in §3(b) Executive Ord.\n4TheAImodelisdefinedas“acomponentofaninformationsystemthatimplementsAItechnologyandusescomputational,\nstatistical, or machine-learning techniques to produce outputs from a given set of inputs,” in §3(c) Executive Ord.\n5The term AI system is defined as “any data system, software, hardware, application, tool, or utility that operates in whole\nor in part using AI,” in §3(e) Executive Ord.\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 3\nthe class of AI models that emulate the structure and characteristics of input data in order\nto generate derived synthetic content. This can include images, videos, audio, text, and other\ndigital content.\nThe approaches taken by the US and the EU are different. In the former case, generative AI (Gen\nAI) is explicitly mentioned, whereby LLMs are the best known and widely used example. In the lat-\nter case, the EU legislator ultimately decided not to make explicit reference to Gen AI but instead to\ninclude the general-purpose AI models, an example of “Large generative AI models,” as specified in\nrecital 99 of the AI Act.6 A specific definition of LLMs has not been attempted, although the conse-\nquencesarisingfromtheirabilitytoautomaticallygenerate“contentinresponsetopromptswrittenin\nnatural-language conversational interfaces” (UNESCO,2023, 8), investigated in this study, fall within\nthe scope of the AI Act. This issue is often addressed through a normative approach, which aims to\nidentify the advantages and disadvantages of applying a specific technology.\nHowever, the normative challenges of LLMs require us to adopt an epistemological approach,\nwhich calls for examining the main differences between the representation of reality of human beings\nand that of LLMs. This epistemological dimension underlies the way LLMs operate in practice and\nposes challenges to regulation: it is therefore crucial to preliminarily address the problem from an\nepistemological point of view, as observed and extensively expounded by (Wachter et al.,2024) in a\nsystematic study on the subject.\nTherefore, the paper focuses on the epistemological approach behind LLMs (section 2), won-\ndering what representation of reality LLMs produces and how we interact with this representation.\nFive epistemological challenges of LLMs are identified in terms of qualification (section 3.1); relia-\nbility (section 3.2); pluralism and novelty (section 3.3); technological dependence (section 3.4) and\nrelation to truth and accuracy (section 3.5). In light of these epistemological challenges, the social\nimplications of LLMs are framed, proposing some final remarks (section 4).\n2. Epistemological approach\nLLMs produce outputs in forms of answers to prompts, which can be understood as pieces of knowl-\nedge (Kelemen & Hvorecký,2008; Lenat & Feigenbaum,1991; Lenat & Marcus,2023), offering a\nspecific representation of reality, because of “the human tendency to attribute meaning and intent\nto natural language” (Wachter et al.,2024, 3). From a qualitative point of view, the LLM’s represen-\ntation of reality mayaccidentally reach high standard and, as a result, discerning between synthetic\nandnon-syntheticoutputsbecomeschallengingforhumans.Accordingly,apartfromnormativecon-\nsiderations, it becomes essential to scrutinize the representation of reality offered by LLMs and how\nhumanbeingsbecomesubjectsofinteraction,sincepeopleincreasinglyrelyonAI-generatedoutputs,\nwhich are produced by AI models, trained on ever larger dataset and endowed with ever-increasing\ncomputational power sparking significant consequences on human life (Durante,2021, 13).\nFromanepistemologicalperspective,LLMsandhumansareindeeddifferent.Whilehumanbeings\nare extraordinary “semantic engines” (their understanding and representation of reality is rooted in\nsemantic skills, i.e., the ability to give meaning and sense to the world), a LLM is an extraordinary\n“syntactic engine” (Floridi,2023a, 44), which predicts the most likely string of words that comes next\ninapieceoftextinresponsetoaprompt,operatingonthebasisoftheabilitytoprocesshugeamounts\nof data and parameters through structures of great complexity, such as neural networks, without\npossessing knowledge or awareness of what is being processed. Unlike the scientific method, where\nknowledge entails verifiable predictions based on models and explanations, AI-generated outputs\n6Large generative AI models are a typical example for a general-purpose AI model, given that they allow for flexible gener-\nation of content, such as in the form of text, audio, images or video, that can readily accommodate a wide range of distinctive\ntasks.”, recital 99, AI Act.\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press\n4 Ludovica Paseri and Massimo Durante\nlack such theoretical grounding. Thus, questioning the underlying functioning of LLMs is pivotal\ndue to its profound impact on human actions and the criteria for justifying them.\nIn doing so, the analysis is limited to considering the comparison with a specific epistemology,\nnamely the legal epistemology. Without going into the age-old and thorny question of the defini-\ntion of law and its role, we can say that, from an epistemological standpoint, law acts as a normative\nsystem, which interprets and applies legal statements within a shared legal framework, verifying the\nreliability of this interpretation and application through forms of reasoning and specific tools (prin-\nciples, categories, evidences, review systems, etc.) that the law itself provides and regulates, which\nhave a fundamental common feature. Every piece of knowledge produced by the interpretation and\napplication of the law is offered to the scrutiny, control and discussion of a counterparty and a third\nparty (and more widely to the community of legal scholars over time).7 Legal science is progressive\nand incremental and requires the contribution of all legal experts. This method, and the resulting\nrepresentation of reality that it conveys, may differ from or be in contrast with LLM’s representation\nof reality, for three main reasons.\nFirst, the representation produced by LLMs may happen to be accurate (where there is a validated\nstandard or metric to measure accuracy) but it is not based on the ability to attribute meaning and\nintersubjectively verify its epistemic reliability. In analytical terms, LLMs (and more widely Gen AI)\ndo not have the ability to operate a semantic ascent (Floridi,2023a; Quine,1953) or descent in rela-\ntion to artificially produced sentences; namely, they are not able to move from the linguistic to the\nmetalinguistic level in order to qualify the content of their statements or argue in favor of its accu-\nracy (as remarked with regards to general-purpose LLMs, “despite their generality, responses rarely\nincludelinguisticsignalsormeasureofconfidence”[Wachteretal., 2024,2,alsoreferringtoLinetal.,\n2022; Mielke et al.,2022]).\nSecond,thelawclaimstodefinetheextenttowhichanexplanationislegallyrelevantbothinterms\nofcausalregression(i.e.,determinationofthechainofeventsofalegallyrelevantfact)andintermsof\nprediction (of the legally relevant consequences of the fact in question). Against this epistemological\nbackground, the law has always independently decided where it intends to stop the legally relevant\nretrospective orpredictiveexplanationofanevent.LLMs cannotaccountfora causal retrospectiveor\npredictive explanation of any knowledge it produces. How could the law, from this perspective, regu-\nlateaneventifitcannotappraiseanddeterminewhere(theknowledgeof)itsoriginandconsequences\nbegin and end? (Durante,2021, 9–10).\nThird, each piece of knowledge created by LLMs does not require the existence of a specialized\ncommunity of interpreterstobeconcretelyapplied,whilelawisasocialproductandcannotgiveriseto\nprogressive and incremental knowledge except through the contribution, discussion, interpretation\nand scrutiny by a community of experts (Zeno-Zencovich,2023, 445). While legal epistemology is\nfocused on a shared process of representation and validation, LLMs propose solely the product of\noutput, leading to concerns regarding both reproducibility and control. In particular, the aspect of\ncontroliscentraltotheextentthat“whoevercontrolsthequestionscontrolstheanswersandwhoever\ncontrols the answers control reality” (Floridi,2023b, 5).\nConsider, for instance, the ability of ChatGPT to reiterate a given answer, potentially offering a\nverydifferentresponse,sometimestotallycontradictingtheformer.VariableoutputsstressthatLLM’s\nrepresentation of reality needs mechanisms ensuring accountability. Furthermore, LLM’s inability to\nattribute meaning or validate the reliability of its outputs raises crucial questions about its role in\nproducing pieces of knowledge on which decision-making is grounded. Nevertheless, this should not\n7According to a neopositivist conception of legal science that has also been called logical positivism, “[…] the paradigmatic\ninstance of knowledge is represented by the empirical sciences, which adopt the principle of verification (or, from a certain\npoint on, falsification). The latter thus becomes the fundamental methodological principle of every sphere of knowledge. Just\nas the proposition ‘this is chalk’ implies that if one observes a piece of chalk at the microscope certain structural qualities will\nbecome apparent, similarly the proposition ‘Section 62 of the Uniform Negotiable Instruments Act is valid Illinois law’ implies\nthat Illinois courts, given certain conditions, will behave in a certain way” (Schiavello,2020, 148 [our translation]).\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 5\nbe a limitation for the study of potential opportunities arising from the use of LLMs in legal settings\n(as discussed in Fagan,2025; Nelson,2024; Surden,2023).8\nInvestigating the epistemological dimensions of LLMs involves broader philosophical inquiries\nregarding knowledge acquisition, representation and validation. Understanding how to balance\nLLM’s production of outputs and representation of reality with human interpretation becomes cru-\ncial. What safeguards can be implemented to mitigate the issues emerging with LLMs from an\nepistemological standpoint? These issues require a multifaceted approach that recognizes the epis-\ntemic abilities and limits of LLMs. By critically examining some assumptions and implications of\nthe production of outputs and the representation of reality of LLMs, the study paves the way for\na more informed and coherent approach to its development and deployment. Therefore, a set of\nepistemological challenges have been identified and will be discussed below.\n3. Epistemological challenges\nAI has been described in terms of “epistemic technology,” where the term epistemic “is meant to refer\nto the broader category of practices that relate in a direct manner to the acquisition, retention, use\nand creation ofknowledge” (Alvarado,2023, 12). The epistemic nature of AI derives both from the\ntype of content it manipulates and the types of operations it performs on that content, but also for\n“its own internal capacities” (Alvarado,2023, 15). However, being an epistemic technology does not\nmeanthatthecontentorrepresentationofrealityprovidedis reliable.Inotherwords,“acceptingAIas\nan epistemic technology does not mean that we must also accept that it is an epistemically trustwor-\nthy epistemic technology” (Alvarado,2023, 14). For these reasons, “the need to involve challenging\nepistemic and methodological scrutiny is even more immediately pressing” (Alvarado,2023, 22). In\ndoing so, five major epistemological challenges need to be analyzed: (1) qualification; (2) reliability;\n(3) pluralism and novelty; (4) technological dependence and (5) relation to truth and accuracy.\n3.1 Epistemic qualification\nThe qualification challenge consists in the fact that LLMs defy the human ability to qualify the\nepistemological status of outputs (i.e., the answers and pieces of knowledge) conveyed. This occurs\nprimarily in two ways.\nFirst, LLMs tend to elide the distinction between what is created (i.e., which is the outcome of a\nproper creative process) and what is merely reproduced and mirrored (i.e., which is the result of the\nabilitytomimicacreativeactivity).Ifoneconsidersonlytheresultandnottheprocess,humanbeings\nare often unable to distinguish, for instance, the synthetic from the non-synthetic result. In the legal\ncontext, it remains crucial not only to be able to evaluate a piece of knowledge but also to be able to\ncontrol the process by which a piece of knowledge is produced (Tasioulas,2023, 9).9\nSecond, LLM challenges the human ability to qualify between true and false. Considering the\nstatistical dimension of LLM’s outcomes, pieces of false knowledge (the so-called hallucination, Ji\net al.,2023; Rebuffe et al.,2022) are frequently generated however plausible: “Readers and consumers\n8See, for instance, Frank Fagan, who envisages that “legal tasks will take less time to complete, and language models will\nenhance lawyer productivity” and significantly hypothesizes that “any advantage from using LLMs in law depends upon the\ncost of data and its processing. If costs are structured so that capital investment makes superior performance possible, then\nfirms will be able to differentiate themselves on the basis of their investments in generative A.I.” (Fagan,2025, respectively 1\nand 9).\n9On the one hand, the use of synthetic data may reduce costs in the legal field: “Synthetic data promises to substantially\nlower data processing costs. Human-created data must be cleaned, labelled, and organized prior to its use. By creating ideal\ndata, builders of large language models can train and fine-tune their models more easily and with less strain on computational\nresources;” on the other hand, “there is a cost to using synthetic data. If the artificial data fails to adequately represent the real\nworld, then the LLM will produce errors. Input that is substantially inaccurate can lead to low-quality output” (Fagan,2025,\n11).\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press\n6 Ludovica Paseri and Massimo Durante\nof texts will have to get used to not knowing whether the source is artificial or human. Probably they\nwill not notice, or even mind” (Floridi & Chiriatti,2020, 691). For example, LLMs provide answers,\nwhich they are unable to reflexively qualify as to their degree of truthfulness or reliability. Where\nquestioned in this regard, human beings can reflexively evaluate a statement of their own, qualifying\nit as certain, probable, plausible, etc. (using expressions, which precede the statement itself, such as\n“I am certain that,” “I believe that,” “it is highly probable that,” etc.). Any answer given by an LLM\nshould,inthissense,alwaysbeprecededbyanexpressionsuchas:“Itisonlyprobablethat …,”inorder\nto reveal that any AI-generated statement cannot constitutively raise any claim to truth (UNESCO,\n2023).10\nIt is a crucial matter, since, as argued by Stefano Rodotà, long before the emergence of LLMs, “the\nmeaning of truth in democratic societies […] presents itself as the result of an open process of knowl-\nedge” (Rodotà,2012, 225 [our translation]), remarking that “In a society omnivorous of information,\nand continually productive of representations, the ‘truth’ of the latter takes on a special significance”\n(Rodotà, 2012, 221 [our translation]). This is even more relevant considering the so-called “feedback\nloop:” “[…] Large Language Models learn from content that is fed to them, but because content will\nbe increasingly wholly or partially AI-generated, it will be partly learning from itself, thus creating\na feedback loop” (van der Sloot,2024, 63). This feedback loop leads to the crystallization of LLMs\nand Gen AI’s representation of reality, with serious risks where it produces distorted or unreliable\nknowledge.\n3.2 Epistemic reliability\nThe challenge of epistemic reliability represents the problem of human ability to rely on the answers\nand representation of reality conveyed by LLMs, which reiterates patterns identified starting from\nits training dataset, without having access to real-world understanding and knowledge. As has been\npointed out more generally with regards to AI-generated output, this\ncan lead teachers and students to place a level of trust in the output that it does warrant. […]\nIndeed, Gen AI is not informed by observations of the real world or other key aspects of the\nscientific method, nor it is aligned with human or social values. For these reasons, it can-\nnot generate genuinely novel content about the real world, objects and their relations, people\nand social relations, human-object relations, or human-tech relations. Whether the appar-\nently novel content generated by Gen AI models can be recognized as scientific knowledge\nis contested (UNESCO,2023, 16).\nIn addition, it is worth emphasizing that with regard to the debate on scientific and objective\nknowledge, the results of LLMs may not even be consistent with the main characteristics and princi-\nples of the open science policies and the FAIR (i.e., Findable, Accesible, Interoperable and ReusableI)\ndata requirements (Paseri,2023, 2022). This can raise three main consequences.\nFirst, this produces an impact not only on epistemic trust but, precisely because of the inaccuracy,\nopacity and inexplicability of its model, also on social acceptance. In this perspective, it is indeed\ncrucialtonotethat:“Assessingthelikelihoodortheaccuracyofsuchoutputswilldependonhowwell\nwe think the model analyzed the training data. Hence, epistemic reliability of the analytical processes\nwill be relevant […] to the trust we can allocate to AI generated text, search, etc.” (Alvarado,2023,\n22). Hence, it follows that this epistemic limitation “is also a key cause of trust issues around Gen AI\n10“Gen AI is trained using data collected from webpages, social media conversations and other online media. It generates\nits content by statistically analyzing the distribution of words, pixels or other elements in the data that it has ingested and\nidentifying and repeating common patterns,” (UNESCO,2023, 7). For instance, even if we were to ask the LLM to qualify\nthe status of his own response in degree of truthfulness or epistemic reliability, it would not be able to do so, and its further\nresponse would be merely probabilistic and vitiated by the same original lack of self-qualification.\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 7\n(Nazaretsky et al.,2022a). If users don’t understand how a Gen AI system arrived at a specific output,\nthey are less likely to be willing to adopt or use it (Nazaretsky et al.,2022b)” (UNESCO,2023, 15).\nSecond, this has serious consequences for the allocation of responsibility. In an ever-increasing\nsituation of deep intermingling between human and artificial collaboration in content generation,\n“New mechanisms for the allocation of responsibility for the production of semantic artefacts will\nprobablybeneeded”(Floridi&Chiriatti, 2020,692).Consider,forinstance,inthisregardthegrowing\ndebate in the field of scientific research, which is not only about “reproducibility and robustness in\ntheir results and conclusions” (European Commission,2024, 6) but also about the very authorship of\nthe content produced by LLMs.\nThird,thishasaconsiderableenvironmentalimpactontheinfosphere(Floridi, 2014,205–216).As\nin the case of an upstream polluted source, flawed AI-generated content can not only spread polluted\ncontent downstream, which is inextricably mixed with trustworthy and genuine content, but in the\nlong run it also addicts end users, lowering their defenses.11\n3.3 Epistemic pluralism and novelty\nEvery authentic cognitive inquiry rests on the ability to question. There is nothing more difficult in\nthe search for knowledge than to construct the relevant and pertinent questions, from which any real\ninvestigation can start. However, for an investigation to be fruitful and satisfy our need for research,\nit is necessary that the answer be obtained within the boundaries set by the question, but it is equally\nnecessary that, within those boundaries, the answer brings something new, which is not already\nincluded in the prompt of the question; otherwise, the inquiry remains tautological. Moreover, any\nauthentic answer, which gives us access to the knowledge of something new, raises new questions,\nthatis,thepossibilityofquestioningrealityfromdifferentstandpoints,ensuringpluralismofresearch\nand knowledge: “Each piece of semantic information is a response to a question, which, as a whole,\nraisesfurtherquestionsaboutitself,whichrequirethecorrectflowofinformationtoreceiveadequate\nresponse, through an appropriate network of relationships with some information source” (Floridi,\n2013, 274).\nWhat puzzles us about LLMs is that the ability to produce pieces of knowledge by predicting prob-\nabilitiesoflanguagepatternsfoundindataisbasedonastatictrainingdatasetthatdescribestheworld\nasitis,“assumingwhatweknowabouttheobservedworld”(Pearl&Mackenzie, 2018,9;Vallor, 2024).\nThis is likely to reaffirm the primacy of the existent over openness to novelty and pluralism in a more\nor less surreptitious manner. In this way, the knowledge and representation of reality produced by\nLLMs and Gen AI at large is not instrumental in imagining new and different worlds but rather in\nreaffirming worlds already entrenched in traditional views and conventions. As properly remarked,\n“Gen AI, by definition, reproduces dominant worldviews in its outputs and undermines minority\nand plural opinions” (UNESCO,2023, 26). This brings about two main consequences, according to\nUNESCO’s view of generative AI, which are relevant to epistemology and democracy, respectively.\nFirst, “Accordingly, if human civilizations are to flourish, it is essential that we recognize that Gen AI\ncan never be an authoritative source of knowledgeon whatever topic it engages with” (UNESCO,2023,\n26). Second,\nData-poor populations […] have minimal or limited digital presence online. Their voices are\nconsequently not being heard and their concerns are not represented in the data being used\nto train GPTs, and so rarely appear in the outputs. For these reasons, given the pre-training\n11“This poses a high risk for young learners who do not have solid prior knowledge of the topic in question. It also poses a\nrecursive risk for future GPT models that will be trained on text scraped from the Internet that GPT models have themselves\ncreated which also include their biases and errors” (UNESCO,2023, 16), as we have already pointed out.\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press\n8 Ludovica Paseri and Massimo Durante\nmethodology based on data from internet web pages and social media conversations, GPT\nmodels can further marginalize already disadvantage people (UNESCO,2023, 17).12\nThis last remark raises – or rather reiterates – a crucial concern for democracy. No matter how\ndemocracyisdefined,itisgenerallyagreedthatpeopleandpopulationsshouldbeabletomodifytheir\nstarting conditions. Like other technologies, LLMs can increase or emphasize starting conditions,\nfavoring those who are already advantaged and disfavoring those who are already at a disadvantage.\nThe risk is that LLMs become the battlefield where the challenges of innovation may be won by a few\n(also in the field of legal profession13), and where conformity, division and social exclusion may also\ndevelop and spread, jeopardizing equity and pluralism in democracy.\n3.4 Technological dependence\nTechnological dependence resulting from the increased or systematic use of LLMs, which are\nentrusted with tasks of producing knowledge, or even just aiding or supporting human activities,\nraises at least three different types of issues from an epistemological point of view, which add to the\nalready stressed reliability problem.\nFirst, technological dependence stresses the importance of measuring and comparing the out-\nputs produced by human activity or AI, raising the need of standards for assessing the outputs of\nLLMs. On the one hand, recourse to such systems should not be discouraged in areas where it has\nproved better or safer than human activities; on the other hand, it would be desirable and fair to\nexpect a higher standard from LLMs (Durante & Floridi,2022, 93–112) where they are proved to\nbe able to achieve it (European Commission,2019, 25). In more general terms, comparing human-\nor AI-generated results will raise tensions that may result in third-party protected expectations (e.g.,\nshould the lawyers inform the client that their legal analysis has been generated or supported by an\nAI system?).\nSecond, it is to reaffirm what has been qualified as the “principle of functional equivalence” as\nclosely related to the “principle of equal protection under the law” (Durante & Floridi,2022, 112).\nThe first principle asserts that operation or assistance by an AI system (in this case, LLMs) should be\ntreated no differently from human operation or assistance in case of harm to a third party, for equal\nprotection under the law to be ensured to all those who are the receivers of the effects or the perfor-\nmance of these systems. Similar cases must be dealt with in similar ways unless there is a reason to\nmake a distinction. In this regard, a remark by theExpert Group on Product Liability in the field of AI\nand Emerging T echnologiesis relevant from an epistemological standpoint, although stated in a par-\ntially different context: “[It is] challenging to identify the benchmark against which the operation of\nnon-human helpers will be assessed in order to mirror the misconduct element of human auxiliaries”\n(European Commission,2019, 25).\nThird, technological dependence raises an additional epistemological challenge posed by the need\nfor humans to increasingly rely on technology itself to distinguish synthetic from non-synthetic out-\nputs and to detect the use of LLMs in producing pieces of knowledge. Consider, for example, the\ntools developed in academia to understand whether a text is generated by an artificial or human\nagent, which raises relevant issues in terms of liability, plagiarism and copyright. Accordingly, it is\nimportant to mention the publication of guidelines for the responsible use of LLMs and Gen AI in\n12The risks associated with the under-representation of population segments in the datasets underlying the operation of\nLLMs are widely investigated, see, for instance, Park et al., (2021); (Fosch-Villaronga & Poulsen,2022).\n13“[…] as language model processing costs increase, the greater will be the separation between excellent and mediocre firms\ntotheextentthatprocessingcostsarehigh,butefficientlyscale.[ …]Thelanguagemodelwillprovidethemwithsubstantialpro-\nductivity gains. Investing in a powerful LLM may not be worth it when it is used to service just a few clients, but if, for instance,\nrepeat patterns of computational analysis engender a more efficient processing cost structure, then some consolidation is more\nlikely” (Fagan, forthcoming2025, 9).\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 9\nresearch, according to which researchers “are accountable for the integrity of the content generated\nby or with the support of AI tools” as well as it is expected that they “do not use fabricated mate-\nrial created by generative AI in the scientific process, for example falsifying, altering, manipulating\noriginal research data” (European Commission,2024, 6). This becomes even more challenging when\ndealing with so-called hybrid data, i.e., “the offspring of historical and synthetic data” (Floridi,2023a,\n36).\nAll these cases highlight the crucial importance of standards as they constitute key benchmarks\nfor evaluating the reliability and accuracy of AI-generated outputs. Furthermore, the identification of\nthesestandardsimpliesacomplexassessment(fromalegal,ethicalandsocialstandpoint),inorderto\nguarantee adequate levels of transparency and responsibility when using LLMs and Gen AI systems,\ntaking also into account the rapid pace of technological advancement and the associated need to\nperiodically update measurement criteria.\n3.5 A legal duty to tell the truth?\nA similar set of epistemological concerns has motivated some scholars to advance the hypothesis\nthat there may be a legal duty on LLMs providers to create models that “tell the truth” (Wachter et al.,\n2024). This study represents, at the state of the art, the most systematic examination of what has been\ndefined as the risk of “epistemic harm” (Wachter et al.,2024, 5), meaning the set of dangerous and\nharmfulconsequencesthatcanresultfromrelyingonepistemologicallyflawedAI-generatedcontent.\nThis study, building on the assumption that a risk of epistemic harm depends not only on the\ninherent limitations of LLMSs (which “are not designed to tell the truth in any overriding sense”\n[Wachter et al.,2024, 2]) but also on the extrinsic epistemic limits of users (who “are both encouraged\nand innately susceptible to believing LLMs are telling the truth” [Wachter et al.,2024, 3]), highlights\na key point: namely, that such risk is not specific or limited to a given area but systematic in nature,\nsince it gives rise to “careless speech” which is bound to cause “unique long-term harms to science,\neducation,andsocietywhichresisteasyquantification,measurement,andmitigation”(Wachteretal.,\n2024, 5).\nThe study proposes a detailed analysis of truth-related obligations in EU human rights law, AI Act,\nDigital Services Act, Product Liability Directive, and AI Liability Directive. These legal frameworks\ncontain limited, sector-specific truth duties, from which, according to the authors, it is hard to derive\na general duty to tell the truth:\nWhere these duties exist they tend to be limited to specific sectors, professions, or state institu-\ntions, and rarely apply to private sector. The harms of careless speech are stubbornly difficult to\nregulate because they are intangible, long-term and cumulative. […] Current frameworks are\ndesignedtoregulatespecifictypesofplatformsorpeople(e.g.,professionals)butnottoregulate\na hybrid of the two. Future regulatory instruments need to explicitly target this middle ground\nbetween platforms and people (Wachter et al.,2024, 47–48).\nHowever, the authors trust that: “Despite this general regulatory landscape, the limited duties\nfound in science and academia, education, and libraries and archives offer an interesting avenue to\nexplore as LLMs serve a similar function” (Wachter et al.,2024, 48). On this basis, they believe that a\ngeneral duty to tell the truth can be assumed from more limited, sector-specific duties. In fact, they\ncontend that the “scope of this duty must be broad; a narrow duty would not capture the intangible,\nlongitudinal harms of careless speech, and would not reflect the general-purpose language capacities\nof LLMs” (Wachter et al.,2024, 48).\nAsalreadymentioned,(Wachteretal., 2024)isasystematicandin-depthstudythatwoulddeserve\nextensive analysis and detailed review, particularly with regard to the fields of science, academia,\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press\n10 Ludovica Paseri and Massimo Durante\neducation, archives and libraries14 (which serve as a point of reference for founding a general duty to\ntellthetruth).Here,however,theaimislimitedtoanalyzingafewproblematicfacetsofageneralduty\nto tell the truth, related to what is relevant to this study, regarding the epistemological investigation\nof LLMs.\nIt may be doubted that LLMs can be technologically designed in such terms as to tell the truth (or\nraise a claim to truth), given the inherent statistical dimension with which they provide answers to a\ngiven prompt. This does not prevent their accuracy15 (or “ground truth”) from being improved over\ntime within the inherent epistemic limits on which they are based.16 Moreover, it is open to question\nwhether accuracy, reliability or truthfulnessby design do not risk increasing, rather than decreasing,\nthe tendency to over-rely on AI-generated contents. However, it is not the technological feasibility\nof truth-telling LLMs that we intend to discuss here; rather, the focus is on the desirability of a legal\nduty to tell the truth, from a threefold standpoint: (i) the incentive for accurate information; (ii) the\nrisk of paternalism and (iii) the generalization of context-based truth duties.\nFirst,carefulspeechisessentialfordemocracy,butitiscostly:itrequirestime,effort,resourcesand\noften expertise. There is a basic, deeply rooted asymmetry in the production of information: it is easy\nto produce false or inaccurate information, while it is hard to produce true and accurate information.\nCarefulspeechistheresultofchoice,investmentandeffort,whichcannotsimplybemademandatory\nby law. Commitment to true or accurate information is part of a process for which there must always\nbe public or private incentives other than fear of breaking a legal norm.\nSecond, protecting users from relying on flawed AI-generated contents by providing a duty to tell\nthe truth that takes on a general scope with respect to LLMs sounds somewhat paternalistic. Pointing\nout the epistemic limits, biases and flaws that pollute the ground truth of LLMs is crucial, but it does\nnot automatically lead to embed into them, by design, a supposed predisposition to truth that would\nshelter users from the risk of being exposed to falsehood and careless speech. After all, is such risk\nnot naturally part of the basic dialectics of democracy?\nThird, the law has always resisted the temptation to prosecute and punish the utterance of false-\nhood as such. In fact, the law does only regulate the specific circumstances under which a legally\nrelevant and punishable act is committed by the assertion of a falsehood: e.g., giving false testimony\nin court; attributing an untrue fact to someone by defamation; conducting negotiations in bad faith\nby misrepresenting an item of sale, etc. In all these cases, it is not the misleading or false statement as\n14According to the Mertonian norms of the scientific ethos there are four institutional imperatives that guide the work\nof the scientist: (i) universalism, i.e., the objective nature of the study undertaken, based on previously confirmed knowledge,\nunrelatedtothesubjectiveviewpointoftheindividualscientist;(ii)communalism,whichindicatesthefactthatthediscoveries\nproduced by scientists are the result of social collaboration and, as such, are assigned to the community in its entirety (iii)\ndisinterestedness, which refers to the attitude that must be embodied by any member of the scientific community, subject to\nthe stringent internal control of peers, since research implies the verifiability of the result obtained by the community itself and\n(iv) systematic skepticism, which expresses that aspect of physiological distrust that induces the scientist to be critical of his\nor her own results, in relation to society (Merton,1973, 266–278). The Mertonian imperatives and their interpretations in the\ntradition of the philosophy and sociology of science lack reference to truth, emphasizing instead the notions of verifiability,\nreliability or integrity, as also remarked in (Leonelli,2023, 1): “As argued by philosophers ranging from Karl Popper to J̈urgen\nHabermas, Helen Longino and Philip Kitcher, what distinguishes a dictator from an elected leader – or a scientist from a crook\n– is the extent to which their decision-making processes are visible, intelligible, and receptive to critique.”\n15In this respect, consider Article 13(3)b(ii) of the AI Act, which requires that the deployment of high-risk AI systems shall\nbe accompanied by instructions including: “the level of accuracy, including its metrics, robustness and cybersecurity referred\ntoinArticle15againstwhichthehigh-riskAIsystemhasbeentestedandvalidatedandwhichcanbeexpected,andanyknown\nand foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity.”\n16As the authors underline: “Truth can be optimised in LLMs through a variety of means. Fine-tuning based on authorita-\ntive sources or human-authored truthful responses for difficult prompts can introduce external validity. RLHF [reinforcement\nlearning from human feedback] workers can provide subjective perceptions of the truthfulness or accuracy of statements and\nindicateapreferenceforfactualresponses.[ …]reliabilitycanbeimprovedthroughextensivecurationandannotation,method-\nologically sound benchmarking metrics, long-term fine tuning with expert human feedback, auditing and adversarial testing,\nand perhaps even downsizing models” (Wachter et al.,2024, 7–8).\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 11\nsuch that is legally relevant, but the fact that it represents a form of conduct under the given circum-\nstances that the law sees fit to pursue and punish. Misrepresentation of a fact is usually not legally\nrelevant except in aspecific context, as in the case of perjury in court or bad faith negotiation. The law\nsets the rules for defining the context (e.g., a trial), and then when and how misrepresenting a fact is\nconfigured, for instance, as a perjury in that context. Law does not regulate a falsehoodindependently\nof a given context in which that falsehoodcounts (i.e., is legally relevant) as a tort or crime. A spe-\ncific context is the onlylevel of abstraction (Floridi, 2008) at which a falsehood can be deemed legally\nrelevant.\nThere is actually a good reason for this, which is based on a fundamental principle of legal and\ndemocratic civilization: in a state of law and in a democratic society nobody can or should have sole\npossession of the truth (historically, States have a monopoly on the use of force, but not a monopoly\non truth). If we were to charge a public authority with the responsibility of pursuing and punishing\nall false statements (or careless speeches), we would implicitly be investing this authority with the\npower to distinguishing truth from falsehood and thus, ultimately, the power to declare what is true\nor false in a given society (Durante,2021, 89).\nIt is fundamental to identify specific circumstances in which telling falsehoods (or speaking care-\nlessly) is legally relevant, but this is not the same as assuming a general duty to tell the truth with\nreference to human or artificial agents. In other words, it is certainly important to clarify the specific\ncircumstances in which LLMs are likely to produce legally relevant consequences, such that they may\nentail forms of legal liability on the part of their providers; but it is questionable – and perhaps not\neven desirable for the reasons given above – that a general duty to tell the truth can be derived from\nthese specific circumstances.\nThe reasoning does not lead one to deny the risks implicit in a systematic careless speech, which\ndoes not merely generate specific and limited inaccuracies or misrepresentations of reality as the\ncontentofdiscoursebutiscapableoflong-termalterationoftheveryconditionsofdiscourse.Ratherit\nseems fair to argue that democracy is more protected by the discussion on and unconstrained pursuit\nof truth open to reasonable disagreement than by the imposition of a general duty to tell the truth.\n4. Conclusions\nLLMs raise several challenges that can be examined according to anormative and epistemological\napproach. The former, increasingly adopted by European institutions, identifies the pros and cons of\ntechnological advancement. With regard to LLMs, the main pros concern technological innovation,\neconomic development and the achievement of social goals and values. The disadvantages mainly\nconcern cases of risks and harms produced by means of LLMs. The epistemological approach inves-\ntigates how LLMs produce data, information and pieces of knowledge in ways that differ from – and\noftenareirreducibleto–humans.TofullygraspandfacetheimpactofLLMs,ourpapercontendsthat\nthe epistemological approach should be prioritized, since the identification of the pros and cons of\nLLMs depends on how this model of AI works from an epistemological standpoint and our ability to\ninteractwithit.Tothisend,ouranalysiscomparedLLM’sandlegalepistemologyandhighlightedfive\nmajor problematic issues: (1) qualification; (2) reliability; (3) pluralism and novelty; (4) technological\ndependence and (5) relation to truth and accuracy.\nFurthermore, the analysis of such epistemological challenges contributes to understand some\npotentially long-term consequences of LLMs on relevant democratic aspects of our contemporary\nsociety. These consequences may be understood in terms of an impact on (i) human autonomy; (ii)\nsemantic capital and (iii) level playing field.\nHuman autonomy is impacted by LLMs in several ways: from being unable to verify the accuracy\nofoutputstobeingsubjecttomisrepresentations;fromrelianceonuntrustworthyresultstotheriskof\nmanipulation of opinions formed on biased or hallucinatory content, etc. (Novelli & Sandri,2024).\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press\n12 Ludovica Paseri and Massimo Durante\nIn all these cases, the limitation of human autonomy generates a lack of social trust and a loss of\nindividual self-determination.\nThe production of data, information and pieces of knowledge by LLMs has a strong impact on the\nproduction of semantic capital, which is defined as “any content that can enhance someone’s power to\ngive meaning to and make sense of (semanticise) something” (Floridi,2018, 483). This power plays\na crucial role in every democracy and society. The increasing number of outputs and tasks delegated\nto LLMs can circumscribe or alter human power to give meaning to and make sense of reality.\nItshouldbealsonotedthattheproductionofdata,informationandpiecesofknowledgemadepos-\nsiblebythetechnologicaldevelopmentofLLMsrequiresresources,investment,timeandenergy,with\na great impact on our society (van der Sloot,2024, 66–67) and, above all, the environment (Berthelot\net al., 2024; Mannuru et al.,2023). As we have tried to highlight, this can benefit the few, already\nadvantaged, and weaken the many, already disadvantaged, by altering the level playing field.\nUltimately, the risk from the epistemological perspective is that the focus on outputs tends to\nobscure the importance of the path through which the process of knowledge unfolds.\nCompeting interests. The authors declare none.\nReferences\nAlvarado, R. (2023). AI as an epistemic technology.Science and Engineering Ethics, 29(5), 1–32.\nBerthelot, A., Caron, E., Jay, M., & Lefe ̀vre, L. (2024). Estimating the environmental impact of Generative-AI services using\nan LCA-based methodology. InCIRP LCE 2024 - 31st Conference on Life Cycle Engineering, 1–10.\nDelacroix, S. (2024). Augmenting judicial practices with LLMs: Re-thinking LLMs’ uncertainty communication features in\nlight of systemic risks.SSRN, 1–26.\nDiab, R. (2024). Too dangerous to deploy? The challenge language models pose to regulating AI in Canada and the EU.\nUniversity of British Columbia Law Review, 1–36.\nDurante, M. (2021). Computational power: The impact of ICT on law, society and knowledge. New York-London: Routledge.\nDurante, M., & Floridi, L. (2022). A legal principles-based framework for AI liability regulation. InThe 2021 yearbook of the\ndigital ethics lab (pp. 93–112). Cham: Springer International Publishing.\nEuropean Commission (2019). Directorate-General for Justice and Consumers,Liability for artificial intelligence and other\nemerging digital technologies. Luxembourg: Publications Office of the European Union.https://data.europa.eu/doi/10.2838/\n573689\nEuropean Commission (2024). DG for Research and Innovation,Living guidelines on the responsible use of generative AI in\nresearch. Luxembourg: Publications Office of the European Union, 1–14.\nFagan, F .(forthcoming 2025). A view of how language models will transform law.T ennessee Law Review, 1–64.\nFerrara, E. (2024). GenAI against humanity: Nefarious applications of generative artificial intelligence and large language\nmodels. Journal of Computational Social Science, 7, 1–21.\nFloridi, L. (2004). Outline of a theory of strongly semantic information.Minds and Machines, 14(2), 197–221.\nFloridi, L. (2008). The Method of Levels of Abstraction.Minds and Machines, 18(3), 303–329.\nFloridi, L. (2013). The philosophy of information. Oxford: OUP.\nFloridi, L. (2014). The fourth revolution: How the infosphere is reshaping human reality. Oxford: OUP.\nFloridi, L. (2018). Semantic capital: Its nature, value, and curation.Philosophy and T echnology, 31(4), 481–497.\nFloridi, L. (2023a). The Ethics of Artificial Intelligence: Principles, challenges, and opportunities. Oxford: OUP.\nFloridi, L. (2023b). AI as agency without intelligence : On ChatGPT, large language models, and other generative models.\nPhilosophy and T echnology, 36(1), 1–15.\nFloridi, L., & Chiriatti, M. (2020). GPT-3: Its nature, scope, limits, and consequences.Minds and Machines., 30(4), 681–694.\nFosch-Villaronga, E., & Poulsen, A. (2022). Diversity and inclusion in artificial intelligence. In B. Custers, & E. Fosch-\nVillaronga (Eds.),Law and Artificial Intelligence: Regulating AI and Applying AI in Legal Practice(pp. 109–134). The Hague:\nT.M.C. Asser Press.\nJi, Z., Lee, N., Frieske, R., Y u, T., Su, D., Xu, Y .,… Fung, P .(2023). Survey of hallucination in natural language generation.\nACM Computing Surveys, 55(12), 1–38.\nKelemen, J., & Hvorecký, J.(2008)Onknowledge,knowledgesystems,andknowledgemanagement.In Proc. 9th International\nConference on Computational Intelligence and Informatics, Budapest Tech, Budapest, 27–35.\nKrook, J. (2024). Manipulation and the Ai Act: Large Language Model Chatbots and the Danger of Mirrors. SSRN 4719835,\n1–38.\nLenat, D., & Feigenbaum, E. (1991). On the thresholds of knowledge.Artificial Intelligence, 47, 185–250.\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 13\nLenat, D., & Marcus, G. (2023). Getting from generative AI to trustworthy AI: What LLMs might learn from cyc.arXiv\nPreprint arXiv:2308.04445, 1–21.\nLeonelli, S. (2023). Philosophy of open science. Cambridge University Press.\nLin, S., Hilton, J., & Evans, O.(2022).Teachingmodelstoexpresstheiruncertaintyinwords. arXiv PreprintarXiv:2205.14334,\n1–19.\nMannuru, N. R., Shahriar, S., T eel, Z. A., W ang, T., Lund, B. D., Tijani, S., and V aidya, P . (2023). Artificial intelligence\nin developing countries: The impact of generative artificial intelligence (AI) technologies for development.Information\nDevelopment, 1–19.\nMerton, R. K. (1973). The sociology of science. Theoretical and empirical investigations (pp. 267–278). Chicago: UCP.\nMielke, S. J., Szlam, A., Dinan, E., & Boureau, Y . L.(2022).Reducingconversationalagents’overconfidencethroughlinguistic\ncalibration. Transactions of the Association for Computational Linguistics, 10, 857–872.\nMoreau, P ., Prandelli, E., & Schreier, M. (2023). Generative Artificial Intelligence and Design Co-Creation in Luxury New\nProduct Development: The Power of Discarded Ideas. SSRN 4630856, 1–46.\nNazaretsky, T., Cukurova, M., & Alexandron, G. (2022a). An instrument for measuring teachers’ trust in AI-based edu-\ncational technology. In LAK22: LAK22: 12th International Learning Analytics and Knowledge Conference , Vancouver,\nAssociation for Computing Machinery, 55–66.\nNazaretsky, T., Cukurova, M., & Alexandron, G. (2022b). Teachers’ trust in AI-powered educational technology and a\nprofessional development program to improve it.British Journal of Educational T echnology, 53(4), 914–931.\nNelson, J. (2024). The Other ‘LLM’: Large language models and the future of legal education.European Journal of Legal\nEducation, 5(1), 127–155.\nNovelli, C., Casolari, F ., Hacker, P ., Spedicato, G., & Floridi, L. (2024). Generative AI in EU Law: Liability, Privacy,\nIntellectual Property, and Cybersecurity.arXiv Preprint arXiv:2401.07348, 1–36.\nNovelli, C., & Sandri, G. (2024). Digital democracy in the age of artificial intelligence.SSRN, 1–27.\nOrofino, M. (2022). La questione del sotto utilizzo dell’intelligenza artificiale in campo sanitario: Spunti di rilievo costi-\ntuzionale. Journal of Open Access to Law, 4, 158–171.\nPagallo, U. (2022). The politics of data in EU law: Will it succeed?.Digital Society, 1(3), 1–20.\nPark, J. S., Bernstein, M. S., Brewer, R. N., Kamar, E., & Morris, M. R. (2021). Understanding the representation and\nrepresentativeness of age in AI data sets. InProceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society ,\n834–842.\nPaseri, L. (2022). From the Right to Science to the Right to Open Science. The European Approach to Scientific Research. In\nEuropean Y earbook on Human Rights(pp. 515–541). Cambridge: Intersentia.\nPaseri, L. (2023). Open Science and Data Protection: Engaging Scientific and Legal Contexts.Journal of Open Access to Law,\n11, 1–16.\nPearl, J., & Mackenzie, D. (2018). The Book of Why: The New Science of Cause and Effect . New York: Basic Books.\nQuine, W . V .(1953). From a logical point of view. Cambridge: Harvard University Press.\nRebuffe, C., Roberti, M., Soulier, L., Scoutheeten, G., Cancelliere, R., & Gallinari, P . (2022). Controlling hallucinations at\nword level in data-to-text generation.Data Mining and Knowledge Discovery, 1–37.\nRodotà, S. (2012). Il diritto di avere diritti. Roma-Bari: Laterza.\nSchiavello, A. (2020). La scienza giuridica analitica dalla nascita alla crisi Ragion pratica.Ragion Pratica, 1, 143–163.\nSurden, H. (2023). ChatGPT, AI Large Language Models, and Law.Fordham Law Review, 92, 1939–1970.\nTasioulas, J. (2023). The rule of algorithm and the rule of law.Vienna Lectures on Legal Philosophy, 1–19.\nUNESCO. (2023). Guidance for generative AI in education and research . Author. https://unesdoc.unesco.org/ark:/48223/\npf0000386693\nV allor, S.(2024). The AI Mirror: How to Reclaim Our Humanity in an Age of Machine Thinking. Oxford: OUP.\nvan der Sloot, B. (2024).Regulating the Synthetic Society: Generative AI, Legal Questions, and Societal Challenges. Oxford: Hart\nPublishing.\nW achter, S., Mittelstadt, B., & Russell, C.(2024). Do large language models have a legal duty to tell the truth?.SSRN 4771884,\n1–49.\nZeno-Zencovich, V .(2023).Bigdataeepistemologiagiuridica.InG.Resta&V.Zeno-Zencovich(Eds.), Governance of/through\nbig data (vol II, pp. 439–448). Roma: Roma TrE-Press.\nCite this article: Paseri L and Durante M. (2025). Examining epistemological challenges of large language models in law.\nCambridge Forum on AI: Law and Governance 1, e7, 1–13.https://doi.org/10.1017/cfl.2024.7\nhttps://doi.org/10.1017/cfl.2024.7 Published online by Cambridge University Press"
}