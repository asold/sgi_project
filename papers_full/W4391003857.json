{
    "title": "Spikeformer: Training high-performance spiking neural network with transformer",
    "url": "https://openalex.org/W4391003857",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2111915653",
            "name": "Yudong Li",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2944620014",
            "name": "Yunlin Lei",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2055575727",
            "name": "Xu Yang",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2111915653",
            "name": "Yudong Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2944620014",
            "name": "Yunlin Lei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2055575727",
            "name": "Xu Yang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2006370340",
        "https://openalex.org/W2944996566",
        "https://openalex.org/W2783525259",
        "https://openalex.org/W6802541513",
        "https://openalex.org/W3035644810",
        "https://openalex.org/W2964338223",
        "https://openalex.org/W2984844508",
        "https://openalex.org/W3102750118",
        "https://openalex.org/W6796939779",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6797790494",
        "https://openalex.org/W6803432384",
        "https://openalex.org/W6806752860",
        "https://openalex.org/W4313046728",
        "https://openalex.org/W6753258136",
        "https://openalex.org/W2316725540",
        "https://openalex.org/W3121127024",
        "https://openalex.org/W3186268528",
        "https://openalex.org/W2109596721",
        "https://openalex.org/W3102040318",
        "https://openalex.org/W6785961510",
        "https://openalex.org/W3124478039",
        "https://openalex.org/W6790307280",
        "https://openalex.org/W2963855133",
        "https://openalex.org/W2745933219",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W2619510810",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W4239072543",
        "https://openalex.org/W2949736877",
        "https://openalex.org/W2963727650",
        "https://openalex.org/W6701947533",
        "https://openalex.org/W6766978945",
        "https://openalex.org/W6803524348",
        "https://openalex.org/W4313181051",
        "https://openalex.org/W6649797076",
        "https://openalex.org/W6846580354",
        "https://openalex.org/W4226530256",
        "https://openalex.org/W3214982345",
        "https://openalex.org/W4298417552",
        "https://openalex.org/W3159778524",
        "https://openalex.org/W3212076252",
        "https://openalex.org/W3175544090",
        "https://openalex.org/W4320086974",
        "https://openalex.org/W3208650852",
        "https://openalex.org/W3170540448",
        "https://openalex.org/W4221155907",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3105490104",
        "https://openalex.org/W4212774754",
        "https://openalex.org/W4233600818",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W4313069509",
        "https://openalex.org/W4281556421",
        "https://openalex.org/W4365799799"
    ],
    "abstract": "Although spiking neural networks (SNNs) have made great progress on both performance and efficiency over the last few years, their unique working pattern makes it hard to train high-performance low-latency SNNs and their development still lags behind traditional artificial neural networks (ANNs). To compensate this gap, many extraordinary works have been proposed, but these works are mainly based on the same network structure (i.e. CNN) and their performance is worse than their ANN counterparts, which limits the applications of SNNs. To this end, we propose a Transformer-based SNN, termed \"Spikeformer\", which outperforms its ANN counterpart on both static dataset and neuromorphic datasets. First, to deal with the problem of \"data hungry\" and the unstable training period exhibited in the vanilla model, we design the Convolutional Tokenizer (CT) module, which stabilizes training and improves the accuracy of the original model on DVS-Gesture by more than 16%. Besides, we integrate Spatio-Temporal Attention (STA) into Spikeformer to better incorporate the attention mechanism inside Transformer and the spatio-temporal information inherent to SNN. With our proposed method, we achieve 98.96%/75.89% top-1 accuracy on DVS-Gesture/ImageNet datasets with 16/4 simulation time steps. On DVS-CIFAR10, we further conduct energy consumption analysis and obtain 81.4%/80.3% top-1 accuracy with 4/1 time step(s), achieving 1.7/6.4Ã— energy efficiency over its ANN counterpart. Moreover, our Spikeformer outperforms its ANN counterpart by 3.13% and 0.12% on DVS-Gesture and ImageNet respectively, indicating that Spikeformer may be a more suitable architecture for training SNNs compared to CNN. We believe that this work shall promote the development of SNNs to be in step with ANNs as much as possible. Code will be publicly available.",
    "full_text": null
}