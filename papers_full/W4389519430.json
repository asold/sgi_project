{
    "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
    "url": "https://openalex.org/W4389519430",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3087968919",
            "name": "Zorik Gekhman",
            "affiliations": [
                "Google (Israel)",
                "Technion – Israel Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2122921838",
            "name": "Jonathan Herzig",
            "affiliations": [
                "Google (Israel)",
                "Technion – Israel Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2333063534",
            "name": "Roee Aharoni",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5091986043",
            "name": "Chen Elkind",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A665682607",
            "name": "Idan Szpektor",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4312107301",
        "https://openalex.org/W4367369697",
        "https://openalex.org/W3093517588",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4322760121",
        "https://openalex.org/W2962849707",
        "https://openalex.org/W4389524159",
        "https://openalex.org/W4378508605",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W3173529047",
        "https://openalex.org/W3034850762",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4385571011",
        "https://openalex.org/W4360837628",
        "https://openalex.org/W4385570750",
        "https://openalex.org/W2788496822",
        "https://openalex.org/W4385573914",
        "https://openalex.org/W4312091769",
        "https://openalex.org/W3173360659",
        "https://openalex.org/W3106234277",
        "https://openalex.org/W4389519229",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W4205477024",
        "https://openalex.org/W4287887686",
        "https://openalex.org/W3153947101",
        "https://openalex.org/W3213990450",
        "https://openalex.org/W4287855110",
        "https://openalex.org/W4297435087",
        "https://openalex.org/W3207166518",
        "https://openalex.org/W4283830961",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3170432046",
        "https://openalex.org/W3034383590",
        "https://openalex.org/W4323651265",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W4205737716",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4312091043",
        "https://openalex.org/W2947681066",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W2127978399",
        "https://openalex.org/W4362508231",
        "https://openalex.org/W4312091691",
        "https://openalex.org/W3106445907",
        "https://openalex.org/W2971034336",
        "https://openalex.org/W3034188538",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3168251909",
        "https://openalex.org/W3159259047",
        "https://openalex.org/W2951211142",
        "https://openalex.org/W4385573912",
        "https://openalex.org/W4385574293",
        "https://openalex.org/W4361230777",
        "https://openalex.org/W4226157795",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W4361806892",
        "https://openalex.org/W4309212061"
    ],
    "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2053–2070\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nTrueTeacher: Learning Factual Consistency Evaluation\nwith Large Language Models\nZorik GekhmanT,G,∗ Jonathan HerzigG Roee AharoniG\nChen ElkindG Idan SzpektorG\nT Technion - Israel Institute of Technology GGoogle Research\nzorik@campus.technion.ac.il\n{zorik|jherzig|roeeaharoni|chenel|szpektor}@google.com\nAbstract\nFactual consistency evaluation is often con-\nducted using Natural Language Inference\n(NLI) models, yet these models exhibit lim-\nited success in evaluating summaries. Pre-\nvious work improved such models with syn-\nthetic training data. However, the data is typi-\ncally based on perturbed human-written sum-\nmaries, which often differ in their character-\nistics from real model-generated summaries\nand have limited coverage of possible factual\nerrors. Alternatively, large language models\n(LLMs) have recently shown promising re-\nsults in directly evaluating generative tasks,\nbut are too computationally expensive for prac-\ntical use. Motivated by these limitations,\nwe introduce TrueTeacher, a method for gen-\nerating synthetic data by annotating diverse\nmodel-generated summaries using a LLM. Un-\nlike prior work, TrueTeacher does not rely on\nhuman-written summaries, and is multilingual\nby nature. Experiments on the TRUE bench-\nmark show that a student model trained us-\ning our data, substantially outperforms both\nthe state-of-the-art model with similar capac-\nity, and the LLM teacher. In a systematic study,\nwe compare TrueTeacher to existing synthetic\ndata generation methods and demonstrate its\nsuperiority and robustness to domain-shift. We\nalso show that our method generalizes to multi-\nlingual scenarios. Lastly, we release our large-\nscale synthetic dataset (1.4M examples), gen-\nerated using TrueTeacher, and a checkpoint\ntrained on this data.1\n1 Introduction\nGenerative summarization models are prone to\ngenerate summaries that are factually inconsistent\nwith respect to the corresponding input documents\n(Goodrich et al., 2019; Kryscinski et al., 2019),\nlimiting their applicability in real-world scenarios.\n∗Work done during an internship at Google Research.\n1Our dataset and model are available at:\nhttps://github.com/google-research/\ngoogle-research/tree/master/true_teacher\nFigure 1: A real example from our data generation pro-\ncess. We ﬁne-tune summarization models with differ-\nent capacities, and use them to produce a diverse set\nof model-generated summaries of CNN/DM articles,\nwhich we label for consistency using a 540B LLM.\nSince factual consistency evaluation could be\ncast as a Natural Language Inference (NLI) task,\nNLI models are often used to evaluate consistency\n(Falke et al., 2019a; Maynez et al., 2020; Laban\net al., 2022). However, NLI models exhibit lim-\nited success in evaluating factual consistency in\nsummarization (Falke et al., 2019b; Kryscinski\net al., 2020), since NLI datasets lack the entail-\nment phenomena that naturally arise in abstrac-\ntive summarization (Khot et al., 2018). For ex-\nample, single-sentence premise-hypothesis pairs\nare shorter than document-summary pairs (Mishra\net al., 2021; Schuster et al., 2022).\nTo address this domain mismatch, previous work\nproposed various approaches for generating syn-\nthetic training data (Kryscinski et al., 2020; Yin\net al., 2021; Utama et al., 2022; Balachandran et al.,\n2022). The data is typically generated by perturb-\n2053\ning human-written summaries to introduce factual\ninconsistencies. While these perturbations are ef-\nfective, they are limited to factual error categories\nthat can be covered by the perturbation logic. In\naddition, since simulating factual errors is chal-\nlenging, such perturbations may fail to introduce\nfactual errors, leading to incorrect labels.2 Finally,\nsince the synthetic summaries are based on human-\nwritten summaries, they may differ in style from\nreal model-generated summaries, which can reduce\nthe effectiveness of the synthetic data.\nAn alternative approach to augmenting NLI mod-\nels with synthetic data, is to directly prompt large\nlanguage models (LLMs) to evaluate factual consis-\ntency. Recently, there has been a growing evidence\nfor the effectiveness of LLMs in evaluating gener-\native tasks (Kocmi and Federmann, 2023; Wang\net al., 2023; Liu et al., 2023), including factual\nconsistency in summarization (Chen et al., 2023).\nHowever, LLMs are still too computationally ex-\npensive to be heavily used in practice.\nTo make the best of both worlds we propose\nTrueTeacher, a simple and effective synthetic data\ngeneration method that leverages model-generated\nsummaries and the reasoning abilities of LLMs\n(Huang and Chang, 2022). In TrueTeacher, we ﬁrst\ntrain a diverse collection of summarization models\nwith different capacities. Next, we use these mod-\nels to summarize each document in a given corpus\n(Figure 1). The resulting document-summary pairs\nare then annotated by prompting a LLM to predict\nthe corresponding factual consistency label.\nWe apply TrueTeacher using FLAN-PaLM 540B\n(Chung et al., 2022) to generate a large-scale syn-\nthetic dataset, which is used to train a student\nmodel. Experiments on the summarization sub-\nset of the TRUE benchmark (Honovich et al.,\n2022) show that augmenting existing NLI data\nwith TrueTeacher data improves a state-of-the-art\nmodel’s ROC-AUC from 82.7 to 87.8, while main-\ntaining similar model capacity. The resulting model\neven outperforms its LLM teacher, despite the latter\nhaving a ×50 larger capacity.\nWe also compare TrueTeacher to existing syn-\nthetic data generation methods. To this end, we\ndesign a systematic study to re-evaluate existing\nmethods with a \"fair comparison\" in a challeng-\ning setting. Our results indicate that existing ap-\nproaches fail to generalize to documents derived\nfrom a distribution different from the one used for\n2As we also demonstrate in §4.3.\nsynthetic data generation. In contrast, TrueTeacher\ndemonstrates robustness by successfully generaliz-\ning to documents from new domains.\nFinally, we apply TrueTeacher to generate\nmultilingual synthetic data. While existing data\ngeneration methods are often limited to English\n(Utama et al., 2022; Balachandran et al., 2022),\nTrueTeacher can use a multilingual LLM. Results\non the mFACE dataset (Aharoni et al., 2022), show\nimprovements on 35 out of 45 languages when us-\ning our method. This demonstrates the usefulness\nof multilingual synthetic data and the effectiveness\nof TrueTeacher in generating such data.\nTo summarize, this work includes the following\ncontributions:\n• We introduce TrueTeacher, a synthetic data\ngeneration approach based on annotating\nmodel-generated summaries with LLMs, and\ndemonstrate its effectiveness and robustness.\n• We evaluate FLAN-PaLM 540B on the task of\nfactual consistency evaluation and show that\nits knowledge can be distilled into a signiﬁ-\ncantly smaller model using our method.\n• We conduct a systematic study, re-evaluating\nexisting synthetic data generation methods for\nthe task in an apples-to-apples comparison\nand identify their limitations.\n• We perform the ﬁrst experiment in generating\nmultilingual synthetic data for factual consis-\ntency, and demonstrate its usefulness.\n• We release a large-scale dataset comprised of\n1.4 million TrueTeacher examples, and verify\nits quality with human evaluation. We addi-\ntionally release a state-of-the-art consistency\nevaluation model trained on this data.1\n2 TrueTeacher\nIn this section we describe TrueTeacher, our\napproach for generating synthetic examples for\nthe task of factual consistency evaluation in\nsummarization. Our main motivation is to\nuse factual inconsistencies that occur in real\nmodel-generated summaries, instead of relying on\nperturbed human-written summaries. To this end,\nwe generate a diverse set of summaries using gener-\native summarization models of different capacities,\nand leverage a LLM to label them for factual con-\nsistency. Some of the generated summaries are ex-\npected to contain factual errors, and we hypothesize\n2054\nFigure 2: Our data generation process. We train a col-\nlection of generative summarization models, use them\nto summarize documents and label the resulting sum-\nmaries for factual consistency using a LLM.\nthat a strong-performing LLM can generalize to the\ntask and label them with sufﬁcient quality to be use-\nful for training. The usage of model-generated sum-\nmaries not only yields more realistic texts, but also\nallows to potentially include rare errors, which can\nbe harder to incorporate with perturbation logic.\nOur data generation process is illustrated in\nFigure 2. First, we train a variety of summa-\nrization models (upper diagram). We use a col-\nlection of one or more summarization training\nsets T = {sd1, sd2, . . . , sdn}and different pre-\ntrained LMs = {lm1, lm2, . . . , lmm}to ﬁne-\ntune a collection of summarization models SM =\n{sm1, sm2, . . . , smk}, where k = n ×m.3 Using\ndifferent pretrained LMs allows to diversify the\nexpected consistency errors, e.g., errors made by\nlarge or small models. The choice of summariza-\ntion training sets allows to control for the nature of\nthe resulting summaries, e.g., focusing on abstra-\ntive training sets to increase output abstractiveness.\nNext, we obtainmodel-generated summaries and\nannotate them (lower diagram). We choose a docu-\nments corpus D = {d1, d2, . . . , dr}and use all the\nsummarization models in SM to summarize all the\ndocuments in D, resulting in a collection of model-\ngenerated output summaries O = {s1,1, . . . sr,k},\nwhere si,j is the summary of document di gener-\nated by summarization model smj. TrueTeacher\n3We note that the pretrained LMs here refer to the mod-\nels that we are ﬁne tuning for summarization, and they are\ndifferent from the LLM that we use as the teacher.\ndoes not require gold summaries, which allows it\nto be used with any collection of documentsD, and\nmakes it more scalable than previous methods (Yin\net al., 2021; Utama et al., 2022; Balachandran et al.,\n2022).\nFinally, a LLM is prompted to label all\nthe summaries in O for consistency w.r.t.\ntheir source documents, resulting with labels\n{l1,1, . . . , l1,k, . . . lr,k}.4 Figure 1 illustrates a real\nexample of this process for a single document\ndi ∈ D. Each document, summary, and label\n(di, si,j, li,j) are then used as a synthetic example\nfor training a factual consistency classiﬁer. Since\nwe leverage LLMs for labeling, our approach is\nlikely to beneﬁt from the ongoing progress in\nLLMs quality. Furthermore, previous approaches\noften rely on language-speciﬁc components (e.g.,\nInformation Extraction), which limits their appli-\ncability in multiple languages. Since recent LLMs\nare pretrained on multilingual data, our method can\nbe easily applied to non-English languages, as we\nshow in §5.\n3 Experimental Setup\nWe use TrueTeacher to generate a synthetic dataset\nfor factual consistency evaluation in summariza-\ntion (§3.1), and experiment with it to evaluate the\neffectiveness and usefulness of our method (§4).\n3.1 TrueTeacher Instantiation\nTo apply TrueTeacher, we instantiate the summa-\nrization datasets T, the pre-trained LMs and the\ndocuments corpus D. We use XSum (Narayan\net al., 2018) as T, T5 pre-trained models (Raf-\nfel et al., 2020) as LMs = {T5-small, T5-base,\nT5-large, T5-3B, T5-11B}, and documents from\nCNN/DailyMail (Hermann et al., 2015) as D.\nAs our teacher model, we employ FLAN-PaLM\n540B (Chung et al., 2022). This model was instruc-\ntion ﬁne-tuned, including training on the closely-\nrelated NLI task.5 Therefore, we expect it to gen-\neralize well to factual consistency evaluation.6 We\nuse zero-shot prompting for simplicity, and since\napplying few-shot or chain-of-thought prompting\ndid not improve performance in early experiments.7\n4See §3.1 and §A.1 for our prompting implementation.\n5https://github.com/google-research/FLAN/blob/\ne9e4ec6e2701182c7a91af176f705310da541277/flan/\ntask_splits.py#L109\n6We validate this expectation in §4.1 and §4.4.\n7In §A.1 we discuss potential reasons to this.\n2055\nSummaries Source # Consistent # Inconsistent\nT5-11B 233,815 39,423\nT5-3B 229,097 45,662\nT5-large 195,681 81,986\nT5-base 161,177 118,480\nT5-small 88,129 190,012\nTotal 907,899 475,563\nTable 1: Our generated dataset statistics.\nExtensive implementation details about our FLAN-\nPaLM usage are provided in §A.1 and §A.2.\nApplying TrueTeacher in this setup resulted\nin ∼1.4M synthetic training examples (Table 1),\nwhich we use to train a student model for factual\nconsistency evaluation.8 In §4, we provide evi-\ndence for the dataset’s quality through human eval-\nuation (§4.4), its usefulness for improving NLI\nmodels in a challenging setting (§4.1), and its supe-\nriority over other existing synthetic datasets (§4.2).\nIn early experiments, we also explored data ﬁl-\ntering based on prompting FLAN-PaLM for self-\nveriﬁcation (details in §A.5). This resulted in an\nincrease in the labeling accuracy. Yet, surprisingly,\ntraining the student model on the ﬁltered data did\nnot improve performance in comparison to train-\ning on the full dataset. 9 Thus, for simplicity, we\nconduct experiments using the full dataset.\n3.2 Evaluation\nTo compare between consistency evaluation mod-\nels, we use the TRUE benchmark (Honovich\net al., 2022), focusing on its summarization subset:\nMNBM (Maynez et al., 2020), FRANK (Pagnoni\net al., 2021), SummEval (Fabbri et al., 2020),\nQAGS-X and QAGS-C (Wang et al., 2020). For\nadditional details about these datasets, we refer\nthe reader to Honovich et al. (2022). Following\nHonovich et al., we use ROC-AUC in a binary clas-\nsiﬁcation setting as our evaluation metric.\n3.3 Baselines\nWe compare the performance of factual consistency\nevaluation models trained on TrueTeacher data,\nagainst the top performing models on the TRUE\nbenchmark: QuestEval (Scialom et al., 2021), Q2\n(Honovich et al., 2021),SUMMA CZS (Laban et al.,\n2022), T5-11B ﬁne tuned on ANLI (Honovich\n8Implementation details for our trained models are in §A.3.\n9This could be attributed to the high-quality of the initial\nlabels and the student model’s robustness to noise.\net al., 2022), WeCheck (Wu et al., 2023), and the\nEnsemble from Honovich et al. (2022).10\nWe also compare TrueTeacher data generation\nmechanism to existing methods for synthetic data\ngeneration. We consider the following approaches:\nDocNLI (Yin et al., 2021). Reformatted NLI,\nquestion answering and summarization datasets, in-\ncluding the CNN/DM corpus. The summarization-\nbased positive examples are based on concatenated\ngold summaries. The negative examples are then\ngenerated using word/entity replacements.\nFactCC (Kryscinski et al., 2020). The docu-\nments are from CNN/DM. The consistent sum-\nmaries are randomly sampled sentences from the\ndocument, which are optionally injected with noise\nor paraphrased. The inconsistent summaries are ob-\ntained by rule-based transformations, such as sen-\ntence negation and entity/pronoun/number swaps.\nFactEdit (Balachandran et al., 2022). The posi-\ntive examples are based on gold summaries from\nCNN/DM. For the negative examples, an inﬁlling\nmodel is trained using sentences from the docu-\nments, employing the OpenIE framework (Banko\net al., 2007) to mask predicates and arguments.\nEach predicate and argument phrase in the sum-\nmary is then iterativelly masked and inﬁlled with\nthe model’s lower order beam candidates.\nFalsesum (Utama et al., 2022). The positive\nexamples are based on gold summaries from\nCNN/DM. For the negative examples, predicates\nand arguments are detected in the document and\nthe summary using the OpenIE (Banko et al., 2007)\nframework. Randomly selected predicates and ar-\nguments from the summary are then masked and\ninﬁlled using predicates and arguments from the\ndocument, or by \"hallucinating\" new content. For\nthis purpose a dedicated inﬁlling model is trained.\n4 Experiments and Analysis\nOur main experiments are in §4.1 and §4.2,\nfollowed by various analyses and ablations in §4.3,\n§4.4, §4.5 and §4.6. We design our experiments to\naddress the following research questions (RQs):\n• RQ1: What is the performance of FLAN-PaLM\n540B in factual consistency evaluation in sum-\nmarization? Is it a good choice for a teacher?\n10We discuss WeCheck in §6, and refer the reader to Hon-\novich et al. (2022) for a detailed description of other baselines.\n2056\nMNBM QAGS-X FRANK SummEval QAGS-C Average\nQuestEval (Scialom et al., 2021) 65.3 56.3 84.0 70.1 64.2 68.0\nQ2 (Honovich et al., 2021) 68.7 70.9 87.8 78.8 83.5 77.9\nSUMMACZS (Laban et al., 2022) 71.3 78.1 89.1 81.7 80.9 80.2\nT5-11B w. ANLI (Honovich et al., 2022) 77.9 83.8 82.1 80.5 89.4 82.7\nWeCheck (Wu et al., 2023) 83.0 81.4 88.1 79.8 82.6 83.0\nEnsemble (Honovich et al., 2022) 76.6 85.8 91.2 82.9 87.7 84.8\nFLAN-PaLM 540B (Chung et al., 2022) 76.0 88.1 91.4 83.7 85.2 84.9\nT5-11B w. ANLI + TrueTeacher full 78.1 89.4 93.6 88.5 89.4 87.8\nTable 2: ROC-AUC results on the summarization subset of the TRUE benchmark (Honovich et al., 2022).\n• RQ2: Can TrueTeacher facilitate training of a\ncompetitive model w.r.t. state-of-the-art models?\n• RQ3: What is the quality of the data gener-\nated using TrueTeacher compared to existing syn-\nthetic data generation methods?\nWe address RQ1 and RQ2 in §4.1. To address\nRQ1, we evaluate FLAN-PaLM 540B against com-\npetitive models for factual consistency evaluation.\nTo address RQ2, we use our full dataset from §3.1\nto train our best-performing model, and evaluate\nit in the exact same setting. Finally, RQ3 is ad-\ndressed in §4.2, where we conduct a systematic\nstudy, comparing existing methods to TrueTeacher,\nwhile controlling for factors such as the synthetic\ndata size and the documents used for data synthesis.\n4.1 Main Results on the TRUE Benchmark\nWe address RQ1 by evaluating FLAN-PaLM 540B\non the task and present the results in Table 2.\nFLAN-PaLM 540B achieves an impressive perfor-\nmance, with an average ROC-AUC of 84.9 com-\npared to 83.0 of the bestsingle-model baseline, and\nperforms on-par with the Ensemble. This demon-\nstrates the chosen LLM’s capability for the task,\nand its potential as a teacher for smaller models.\nTo address RQ2, we ﬁne-tune T5-11B (Raffel\net al., 2020) over our full dataset (§3.1) mixed\nwith ANLI (Nie et al., 2020). Table 2 shows\nthat including TrueTeacher data in the training\nset, substantially improves the strong-performing\nT5-11B w. ANLI baseline from an average ROC-\nAUC of 82.7 to 87.8 (+5.1), while maintaining ex-\nactly the same model capacity. This strong result\ndemonstrates the high effectiveness of TrueTeacher\nin a challenging setup. Notably, our model sets\nthe new state-of-the-art result on the benchmark,\noutperforming the ×50 times larger LLM that we\nused as the teacher ( 84.9 →87.8). This can be\nattributed to large-scale knowledge distillation on\na speciﬁc task, while the LLM is trained to per-\nform many tasks. Additionally, the smaller model\nis trained on target-domain data (documents and\nmodel-generated summaries) which can further im-\nprove performance (Gururangan et al., 2020).\n4.2 Re-evaluating Synthetic Data Generation\nMethods – A Study\nPrevious studies on synthetic data generation have\nused different experimental setups, making it dif-\nﬁcult to compare their results. In this section, we\ndesign a systematic study to re-evaluate existing\nmethods in a standardized setup. We ﬁrst discuss\nour study design choices followed by the results.\nPrevious work has demonstrated that synthetic\ndata can improve NLI-based models. However,\nthey typically used relatively small-capacity mod-\nels, whereas Honovich et al. (2022) recently demon-\nstrated signiﬁcant performance gains by scaling up\nto T5-11B ﬁne-tuned on ANLI. We therefore adopt\nthis competitive baseline, to which we add syn-\nthetic data from each method. For ablation, we\ninclude variants trained solely on synthetic data\n(without ANLI), and also repeat our study using\nthe smaller-capacity T5-base model.\nTo preform a fair comparison, we restrict the\nnumber of examples from each evaluated method\nto 100k, randomly sampled with balanced labels.\nTo evaluate domain-shift robustness, we fur-\nther restrict the synthetic training examples to ones\nthat were generated only based on CNN/DM docu-\nments,11 and then consider the XSum-based evalu-\nation sets as out-of-domain.12\n11Some methods are based exclusively on CNN/DM while\nothers use additional datasets, more details in §3.3.\n12SummEval and QAGS-C are based on documents from\nCNN/DM, MNBM and QAGS-X use documents from XSum,\nand FRANK has documents from both CNN/DM and XSum.\nWe split FRANK to FRANK-C and FRANK-X which contain\nits CNN/DN based and XSum based subsets respectively.\n2057\nTraining data CNN/DM-based XSUM-based Average scoresQAGS-CSummEval FRANK-CFRANK FRANK-XQAGS-XMNBMIn-domainOut-of-domainTRUE\nT5-11B\nANLI 83.4 74.2 85.6 90.7 93.2 88.0 73.9 81.1 85.0 82.0\nFactEdit 87.8 77.0 77.2 83.7 76.0 69.4 53.1 80.7 (-0.4) 66.2 (-18.8)74.2 (-7.8)FactEdit + ANLI88.9 78.9 81.1 88.0 86.1 76.2 59.8 83.0 (+1.9)74.0 (-11.0)78.4 (-1.6)DocNLI 89.1 72.9 83.0 89.2 92.4 83.8 67.0 81.7 (+0.6) 81.1 (-3.9)80.4 (-1.6)DocNLI + ANLI87.8 72.0 81.9 88.2 93.7 84.2 68.0 80.6 (-0.5)82.0 (-3.0) 80.0 (-2.0)FactCC 83.1 79.0 81.6 84.1 67.5 72.7 55.0 81.2 (+0.1) 65.1 (-19.9)74.8 (-7.2)FactCC + ANLI84.7 83.3 84.7 89.5 89.6 82.9 71.5 84.2 (+3.1)81.3 (-3.7) 82.4 (+0.4)Falsesum 90.3 85.4 85.8 89.8 84.5 70.8 53.9 87.2 (+6.1) 69.7 (-15.3)78.0 (-4.0)Falsesum + ANLI90.7 85.8 87.0 91.6 90.5 75.2 60.5 87.8(+6.7)75.4 (-9.6) 80.8 (-1.2)TrueTeacher 84.9 85.0 88.8 93.6 94.4 86.5 76.1 86.2 (+5.1) 85.7 (+0.7)85.2 (+3.2)TrueTeacher + ANLI88.4 85.8 89.6 93.9 93.9 87.8 76.3 87.9(+6.8)86.0(+1.0) 86.4(+6.4)\nT5-base\nANLI 74.9 63.7 73.1 81.3 80.6 77.2 77.0 70.6 78.3 74.8\nFactEdit 61.4 59.4 59.4 73.6 51.9 48.0 58.4 60.1 (-10.5) 52.8 (-25.5)60.2 (-14.6)FactEdit + ANLI68.7 60.0 62.2 78.5 73.6 72.2 75.5 63.6 (-7.0)73.8 (-4.5) 71.0 (-3.8)DocNLI 71.4 66.5 66.7 77.9 81.0 75.2 71.6 68.2 (-2.4) 75.9 (-2.4)72.5 (-2.3)DocNLI + ANLI75.2 66.7 74.4 84.9 83.3 78.7 74.8 72.1 (+1.5)78.9 (+0.6) 76.1 (+1.3)FactCC 74.0 72.7 78.7 83.2 71.9 71.0 62.7 75.3 (+4.7) 68.5 (-9.8)72.7 (-2.1)FactCC + ANLI72.8 73.2 78.8 83.2 66.8 71.5 63.2 74.9 (+4.3)67.2 (-11.1)72.8 (-2.0)Falsesum 80.9 74.2 82.0 86.4 71.6 65.0 53.1 79.0 (+8.4) 63.2 (-15.1)71.9 (-2.9)Falsesum + ANLI82.9 73.4 83.3 86.5 72.6 66.0 58.7 79.9 (+9.3)65.8 (-12.5)73.5 (-1.3)TrueTeacher 77.3 73.6 79.1 88.0 82.6 79.9 78.3 76.7 (+6.1) 80.3 (+2.0)79.4 (+4.6)TrueTeacher + ANLI81.9 78.0 81.4 89.3 86.4 81.9 78.5 80.4(+9.8)82.3(+4.0) 81.9(+7.1)\nTable 3: ROC-AUC results on TRUE comparing different synthetic data generation methods. For each model size,\naverage scores are compared to the corresponding ANLI-only baseline (difference is listed in parentheses).\nTable 3 presents the results of our study. We cal-\nculate three average scores: for in-domain test sets\nbased on CNN/DM documents, for out-of-domain\ntest sets based on XSum documents, and for the\noriginal datasets from TRUE.\nIn-Domain Results Most methods outperform\nthe corresponding ANLI-only baseline, demonstrat-\ning the usefulness of synthetic data. Predictably, all\nmethods improve with larger models and a comple-\nmentary effect is often observed when mixing syn-\nthetic data with ANLI. The best results are obtained\nby mixing ANLI with Falsesum or TrueTeacher\ndata and using T5-11B, with a substantial improve-\nment over the corresponding ANLI-only baseline\n(in-domain score increase from 81.1 to 87.9).\nOut-of-domain Results While most methods\nperform well in-domain, their performance drops\nsigniﬁcantly on the out-of-domain test sets. Most\nof the evaluated methods underperform the corre-\nsponding ANLI-only baseline with similar model\ncapacity. For some methods, performance dete-\nriorates dramatically; e.g. Falsesum – despite\nits impressive in-domain performance, its out-of-\ndomain score falls signiﬁcantly below the ANLI-\nonly baseline. This suggests that some methods\noverﬁt to documents from the distribution used to\ngenerate the synthetic data. Based on this ﬁnd-\ning, we encourage future research to prioritize out-\nof-domain evaluation. Interestingly, even though\nTrueTeacher’s relative improvement is smaller com-\npared to the in-domain setup, it is still the only\nmethod with higher out-of-domain score compared\nto the corresponding ANLI-only baseline. This\ndemonstrates the robustness of TrueTeacher to do-\nmain shift, which may be due to the use of model-\ngenerated summaries that increase the variability\nof the resulting synthetic data.\nOverall Results on TRUE Due to the poor out-\nof-domain performance of the existing methods,\nTrueTeacher is the only method that consistently\noutperforms the ANLI-only baseline on the TRUE\nbenchmark. Notably, TrueTeacher + ANLI with T5-\nbase (81.9) performs on par with the ANLI-only\nbaseline using T5-11B (82.0). Additionally, the\nTrueTeacher-based variant using T5-11B (85.2) al-\nready performs on-par with the 540B LLM teacher\n(84.9, Table 2), even though we used only 100k syn-\nthetic examples in this experiment, and did not use\nANLI data. When comparing TrueTeacher + ANLI\nwith T5-11B and 100k examples (Table 3) to the\nequivalent variant using the full dataset (Table 2),\nwe observe a performance increase (86.4 →87.8),\nwhich demonstrates TrueTeacher’s scalability. We\nconclude that TrueTeacher yields high quality data\nand generalizes well for new domains, which we at-\ntribute to the usage of model-generated summaries.\n4.3 Qualitative Analysis\nFigure 3 presents a case study with a randomly sam-\npled document, and the corresponding inconsistent\nsummaries generated with each of the evaluated\n2058\nCNN/DailyMail ID: 372f7e02e5bb17bac3a1b2260c6ac78414f97ee3\nArticle:  LOS ANGELES, California (CNN) -- Los Angeles firefighters and \ncity crews worked for several hours Tuesday to rescue one of their own: a \n22-ton firetruck that was nearly swallowed by a water-logged sinkhole. \nTwo firefighters crawled out of the truck's windows after it sank Tuesday \nmorning. No one was injured. The incident happened after four firefighters \ntook the truck to the San Fernando Valley neighborhood of Valley Village, \nwhere flooding had been reported… … \nGold Summaries: \n1. Los Angeles firetruck nearly swallowed by sinkhole Tuesday morning.\n2. Firefighters in truck were responding to flooding call when incident            \n    happened.\n3. Two firefighters escaped truck through windows; no injuries reported.\nFactEdit Firefighters in truck were responding rescue when \nincident happened .\nDocNLI\nLos Angeles firetruck nearly destroyed by sinkhole \nTuesday night . Firefighters in truck were responding to \nemergency call when it happened . Two firefighters \nescaped truck through windows ; no injuries reported .\nFactCC\nLOS LOS ANGELES, California ((CNN) - Los Angeles \nfirefighters and crews worked Two on Tuesday to rescue \none of their ownown: a 22-ton fire engine nearly \nswallowed by a sinkhole filled with waterwater.\nFalsesum Los Angeles firetruck nearly swallowed by water.\nTrueTeacher A firefighter has rescued a truck that sank in Los Angeles, \ncausing extensive flooding.\nFigure 3: A case study comparing factually inconsis-\ntent summaries of the same document generated us-\ning different methods. Content replacements are high-\nlighted using the same color for the original and the\nreplaced text. Added content is in bold red font.\nmethods. FactEdit used the second gold-summary\nand replaced \"to ﬂooding call\" with \"rescue\", in-\ntroducing a grammatical error rather than a factual\nerror, demonstrating the potential problems with\nusing lower-beam completions as proxy for factual\nerrors. DocNLI uses all the gold summaries con-\ncatenated. While replacing \"morning\" with \"night\"\nintroduces a factual error, three other edits fail to\nintroduce factual errors, demonstrating the limi-\ntations of using simple word/entity replacements.\nFactCC used the ﬁrst sentence from the article and\nsuccessfully introduced factual error by an entity\nswap from \"ﬁretruck\" to \"ﬁre engine\". The para-\nphrase highlighted in green increases the abstrac-\ntiveness, but the paraphrase in orange introduces\na grammatical error that is less likely to be made\nby a strong summarization model. The noise in-\njection used by FactCC (duplicating or removing\nrandom tokens) is colored in red, but its useful-\nness is questionable. Falsesum uses the ﬁrst gold\nsummary, and its perturbation model predicts the\nremoval of \"Tuesday morning\" and the replacement\nof the \"sinkhole\" argument with \"water\", failing\nto introduce a factual error, since the sinkhole is\nreferred to as \"water-logged sinkhole\" in the ar-\nticle. Finally, TrueTeacher uses an abstractive\nsummary generated by a real summarization model.\nClass #Ex. Precision Recall F1\nConsistent 41 80.0 97.6 87.9\nInconsistent 59 98.0 83.1 89.9\nTable 4: Human evaluation results.\nIt introduces a nuanced factual error by replacing\n\"Los Angeles ﬁreﬁghters\" with A ﬁreﬁghter and\nalso by hallucinating new content (the text in bold\nred font). This case study further illustrates the\nchallenges of perturbing texts to introduce factual\ninconsistencies and re-iterates the importance in\nusing model-generated summaries.\n4.4 Human Evaluation\nTo further assess the quality of the synthetic data\nproduced by TrueTeacher, we perform human eval-\nuation carried out by domain experts.13 We evalu-\nate 100 examples from our dataset,14 using binary\njudgements based on the attribution deﬁnition from\nRashkin et al. (2021). The labeling accuracy of\nthe sampled examples from our data stands at 89%,\nwhich demonstrates its high quality. Table 4 further\npresents the precision, recall and F1 scores for the\nconsistent and inconsistent classes. More details\non the human evaluation are available in §A.8.\n4.5 Ablating Summary Distribution and\nLabel Correctness\nThere are two key differences between TrueTeacher\nand perturbation-based synthetic data generation\nmethods: (1) the distribution of the summaries 15\nand (2) the correctness of the generated labels. 16\nEach of these differences may lead to the better\nquality of TrueTeacher w.r.t the baselines. To mea-\nsure the impact of each difference, we isolate them\nin a controlled ablation study. We create 2 ab-\nlated variants, using Falsesum as a recent baseline\nmethod for synthetic data generation. The results\nare presented in Table 5.\nLabelAblation is an ablation created by label-\ning the document-summary pairs from Falsesum’s\ndata using FLAN-PaLM 540B. 17 Comparing\n1310 NLP researchers, each with at least one year of experi-\nence in factual consistency evaluation.\n14We randomly sampled 50 positively and 50 negatively\nlabeled examples from our synthetic dataset.\n15Model-generated vs. human-written perturbed.\n16Both methods may yield wrong labels. Perturbations\nmight not introduce inconsistencies, as seen in §4.3, while\nTrueTeacher can have errors due to LLM mislabeling.\n17We used the same 100k examples as Falsesum + ANLI\nbaseline, and the same LLM prompt as in TrueTeacher.\n2059\nVariant Summary Distribution Labeling Quality T5-11B T5-Base\nFalsesum + ANLI Human-written perturbed Falsesum 80.8 73.5\nTrueTeacher + ANLI Model-generated FLAN-PaLM 540B 86.4 (+6.9%) 81.9 ( +11.4%)\nLabelAblation Human-written perturbed FLAN-PaLM 540B 85.3 (+5.6%) 78.9 ( +7.3%)\nSummaryAblation Model-generated Falsesum (proxy) 85.5 (+5.8%) 79.1 ( +7.6%)\nTable 5: Average ROC-AUC on TRUE for the ablated variants. Falsesum + ANLI and TrueTeacher + ANLI are\ncopied from Table 3 for reference.\nLabelAblation to Falsesum + ANLI allows us\nto examine the effect of using FLAN-PaLM\nlabels instead of the original Falsesum labels,\nwhile controlling for the summaries distribution.\nLabelAblation outperforms Falsesum + ANLI\nby 5.6%, which shows that performance gains can\nbe obtained using summaries generated with exist-\ning synthetic data generation methods combined\nwith second-stage improved labeling quality. How-\never, TrueTeacher is substantially simpler and also\nresults in better performance.\nSummaryAblation is an ablation created by ﬂip-\nping labels on a random portion of TrueTeacher’s\ndata, such that the expected labeling accuracy is\nsimilar to Falsesum (More details in §A.9). Com-\nparing SummaryAblation to Falsesum + ANLI al-\nlows us to examine the effect of changing the sum-\nmary distribution from human-written perturbed\nto model-generated, while controlling for the la-\nbeling quality. SummaryAblation outperforms\nFalsesum + ANLI by 5.8%, a similar improve-\nment as observed forLabelAblation (5.6%). This\ndemonstrates that label correctness and summary\ndistribution have a similar effect on the perfor-\nmance, but they also have a complimentary effect\nas the best performance of 86.4 ROC-AUC is ob-\ntained only when they are combined together.\n4.6 Abstractiveness Analysis\nAdvances in large scale pretraining (Devlin et al.,\n2019; Lewis et al., 2020) and the availability of rel-\nevant datasets (Narayan et al., 2018), enabled rapid\nprogress in abstractive summarization, which bet-\nter imitates the way humans summarize (Koh et al.,\n2023) and is also preferred by humans (Goyal et al.,\n2022). This motivates us to focus on generating\nabstractive synthetic summaries.\nWe compare the abstractiveness degree of differ-\nent methods using the extractive fragmentcoverage\nand density measures from Grusky et al. (2018).\nFollowing Utama et al. (2022) we multiply these\nCoverage↓ Density↓ Combined↓\nFactEdit 0.86 2.92 2.67\nDocNLI 0.85 15.66 15.20\nFactCC 0.93 8.16 7.93\nFalsesum 0.88 2.98 2.76\nTrueTeacher 0.86 2.41 2.15\nTable 6: Average abstractiveness scores (lower is bet-\nter), measured on a random sample of 5k examples.\nmeasures to obtain a combined score.18 Table 6\npresents the abstractiveness scores, and a density\nplot is available in the Appendix (Figure 5). We ob-\nserve higher abstractiveness for model-based meth-\nods (FactEdit, Falsesum and TrueTeacher), suggest-\ning that rule-based methods might be less useful\nwith the recent shift towards abstractive summariza-\ntion. TrueTeacher produces the most abstractive\nsummaries with lowest combined score.\n5 Multi-Lingual Data Generation for\nFactual Consistency Evaluation\nUtilizing a multilingual LLM enables a straightfor-\nward application of TrueTeacher to multiple lan-\nguages. This contrasts with recent approaches that\nrely on NLP components only available for high-\nresource languages, e.g., information extraction\n(Utama et al., 2022; Balachandran et al., 2022). In\nthis section, we examine TrueTeacher’s usefulness\nfor multilingual factual consistency evaluation.\nWe ﬁrst generate multilingual synthetic data us-\ning TrueTeacher. This time we train a single sum-\nmarization model by ﬁne tuning mT5-XXL (Xue\net al., 2021) on XLSum (Hasan et al., 2021) and\nuse it to summarize documents from WikiLingua\n(Ladhak et al., 2020), which we then label for con-\nsistency with our LLM. For the purposes of this\nexperiment we focus on a subset of WikiLingua\ndocuments in 4 languages: English (en), French\n18We provide additional technical details in §A.6.\n2060\nTraining data # Improved Avg. ROC-AUC\nlanguages Per lang. Per ex.\nANLI+XNLI - 73.3 71.6\n+TrueTeacher en 32 / 45 75.7 73.8\n+TrueTeacher en,fe,es,ge35 / 45 77.2 75.3\nTable 7: Multilingual results on the mFACE test set.\n(fe), Spanish (es) and German (de).19. After gener-\nating the dataset for these 4 languages, we sample\n100k examples, by randomly sampling 25k in each\nlanguage with balanced labels (as illustrated in Ta-\nble 9 in the Appendix). For ablation, we also cre-\nate an English-only variant, by randomly sampling\n100k English examples with balanced labels.20\nWe use the resulted data to train multilingual con-\nsistency evaluation models and evaluate them on\nthe mFace test set (Aharoni et al., 2022), containing\n3150 examples in 45 languages. As a strong base-\nline we follow Aharoni et al. and ﬁne-tune mT5-\nXXL (Xue et al., 2021) on the ANLI (Nie et al.,\n2020) and XNLI (Conneau et al., 2018) datasets.\nWe then assess whether adding our synthetic data\nto the training set can improve this model.\nTable 7 presents the results overview, full re-\nsults in all 45 languages are available in Table 10\n(Appendix). Adding English-only summarization-\nbased synthetic data, already improves results on\n32 out of 45 languages and increases the avg. ROC-\nAUC from 71.6 to 73.8. Yet, using the same\namount of multi-lingual examples improved the\nperformance even more, with avg. ROC AUC\nof 75.3. This demonstrates the added value in\ngenerating multi-lingual synthetic examples using\nTrueTeacher, laying the ground for future work.\n6 Related Work\nPrevious work proposed methods for generating\nsynthetic training data for factual consistency eval-\nuation, by perturbing gold summaries (Yin et al.,\n2021; Kryscinski et al., 2020; Balachandran et al.,\n2022; Utama et al., 2022; Soleimani et al., 2023).21\nA key advantage of TrueTeacher, is the ability to\nleverage real model-generated summaries, leading\nto superior performance and robustness. The utility\nof model-generated outputs was also highlighted\nby Wu et al. (2023), who proposed a weakly super-\n19They are the most prevalent languages in PaLM’s pre-\ntraining data (Chowdhery et al., 2022)\n20Also based on WikiLingua, generated with the same pro-\ncess like the 25k English subset of our multilingual dataset.\n21We provide extensive review of these methods in §3.3.\nvised consistency evaluation model that leverages\nprobabilistic labels derived from aggregated scores\nof other consistency evaluation models. Our work\nproposes a simpler solution, that is also inherently\nmultilingual.\nAnother line of work for adapting NLI-based\nmodels for summarization, focuses on better pro-\ncessing of long texts, splitting the documents\ninto sentences to create shorter premise-hypothesis\npairs (Laban et al., 2022; Schuster et al., 2022).\nRecent work attempts to assess LLMs’ capability\nfor evaluating generative tasks (Kocmi and Feder-\nmann, 2023; Wang et al., 2023; Liu et al., 2023).\nLuo et al. (2023) evaluated ChatGPT (OpenAI,\n2022) specifﬁcally on the task of factual consis-\ntency evaluation in summarization. Yet, Aiyappa\net al. (2023) argued that ChatGPT’s \"closed\" nature\nrisks data leakage (training-test contamination).22\nChen et al. (2023) performed a study of LLMs as\nfactual consistency evaluators, using a variety of\nprompting methods.\nPrevious work also attempted to distill knowl-\nedge from LLMs (West et al., 2022; Hsieh et al.,\n2023), as well as to leverage LLMs for data anno-\ntation (Wang et al., 2021; Ding et al., 2022), and\nsynthetic data generation (Agrawal et al., 2022;\nLiu et al., 2022; Bitton et al., 2023). As far as we\naware, our work is the ﬁrst to leverage LLMs for\ndata generation for factual consistency evaluation.\n7 Conclusion\nWe introduced TrueTeacher, a simple and highly\neffective method for generating synthetic data for\nfactual consistency evaluation. Instead of per-\nturbation of human-written summaries like done\nin previous work, TrueTeacher leverages realistic\nmodel-generated summaries, which are annotated\nby prompting a large language model.\nUsing our method, we generate a large-scale\nsynthetic dataset, which we are making publicly\navailable. Our experimental results show that this\ndataset substantially enhances the performance of a\nstate-of-the-art model. In our systematic study, we\ncompare TrueTeacher to existing approaches and\nfurther demonstrate its effectiveness and robust-\nness. Our study highlights the importance of out-of-\ndomain evaluation, which we hope will be adopted\nin future work. Lastly, we show that TrueTeacher\ngeneralizes well to multilingual scenarios, present-\ning additional advantage over existing methods.\n22While FLAN’s instruction ﬁne-tuning data is public.\n2061\n8 Limitations\nNoisy synthetic data TrueTeacher relies on a\nLLM for labeling model generated summaries.\nThis process may result in some frequency of noisy\nsynthetic examples for which the label is incor-\nrect. This can affect the overall quality of the stu-\ndent model that trains on this data. In our experi-\nments we validated the quality of our synthetic data\nwith human evaluation, however this should be re-\nexamined when generating data for new domains.\nIn addition, we experimented with different ﬁlter-\ning approaches, but found that training on ﬁltered\ndata with higher labeling accuracy, did not improve\nthe performance of the student model. We encour-\nage future work to further examine such automatic\nﬁltering.\nReliance on LLMs In this work we use a 540B\nLLM to label 1.4M model generated summaries.\nThis requires non-negligible resources that may not\nbe available to the whole community. To mitigate\nthis, we release our collected synthetic data and\nthe corresponding model checkpoint. In addition,\nthe decreasing inference cost of proprietary LLMs,\nand the availability of open-source LLMs (Touvron\net al., 2023) can further assist.\nEffect of low-resource languages Our multilin-\ngual experiments (§5) focus on a subset of WikiLin-\ngua documents in only 4 languages: English (en),\nFrench (fe), Spanish (es) and German (de), that\nare the most prevalent in our LLM’s pre-training\ndata. As can be seen in our full results (Table 9 in\nthe Appendix), our multilingual data successfully\nimproves low-resource languages as well. We did\nnot fully explore the effect of adding additional\nlanguages to our synthetic data, especially low-\nresource ones. We believe that there is a trade-\noff between language coverage and labeling qual-\nity. i.e, while generating the synthetic data in low-\nresource languages will increase language cover-\nage, it can lead to poor labeling quality by our LLM.\nWe did not fully explore the exact sweet-spot for\nhow many languages to include in our synthetically\nlabeled training data, leaving this for future work.\nReferences\nPriyanka Agrawal, Chris Alberti, Fantine Huot, Joshua\nMaynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev,\nDipanjan Das, and Mirella Lapata. 2022. Qameleon:\nMultilingual QA with only 5 examples. CoRR,\nabs/2211.08264.\nRoee Aharoni, Shashi Narayan, Joshua Maynez,\nJonathan Herzig, Elizabeth Clark, and Mirella La-\npata. 2022. mface: Multilingual summariza-\ntion with factual consistency evaluation. CoRR,\nabs/2212.10622.\nRachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-\nYeol Ahn. 2023. Can we trust the evaluation on chat-\ngpt? CoRR, abs/2303.12767.\nVidhisha Balachandran, Hannaneh Hajishirzi,\nWilliam W. Cohen, and Yulia Tsvetkov. 2022.\nCorrecting diverse factual errors in abstractive\nsummarization via post-editing and language model\ninﬁlling. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022 , pages 9818–9830.\nAssociation for Computational Linguistics.\nMichele Banko, Michael J. Cafarella, Stephen Soder-\nland, Matthew Broadhead, and Oren Etzioni. 2007.\nOpen information extraction from the web. InIJCAI\n2007, Proceedings of the 20th International Joint\nConference on Artiﬁcial Intelligence, Hyderabad, In-\ndia, January 6-12, 2007, pages 2670–2676.\nYonatan Bitton, Shlomi Cohen-Ganor, Ido Hakimi,\nYoad Lewenberg, Roee Aharoni, and Enav Weinreb.\n2023. q2d: Turning questions into dialogs to teach\nmodels how to search.\nShiqi Chen, Siyang Gao, and Junxian He. 2023. Eval-\nuating factual consistency of summaries with large\nlanguage models. CoRR, abs/2305.14069.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nCoRR, abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\n2062\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sha-\nran Narang, Gaurav Mishra, Adams Yu, Vincent Y .\nZhao, Yanping Huang, Andrew M. Dai, Hongkun\nYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V . Le, and Jason\nWei. 2022. Scaling instruction-ﬁnetuned language\nmodels. CoRR, abs/2210.11416.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018 , pages 2475–2485.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nBosheng Ding, Chengwei Qin, Linlin Liu, Lidong\nBing, Shaﬁq R. Joty, and Boyang Li. 2022. Is GPT-3\na good data annotator? CoRR, abs/2212.10450.\nAlexander R Fabbri, Wojciech Kry ´sci´nski, Bryan\nMcCann, Caiming Xiong, Richard Socher,\nand Dragomir Radev. 2020. Summeval: Re-\nevaluating summarization evaluation. arXiv\npreprint arXiv:2007.12626.\nTobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie\nUtama, Ido Dagan, and Iryna Gurevych. 2019a.\nRanking generated summaries by correctness: An in-\nteresting but challenging application for natural lan-\nguage inference. In Proceedings of the 57th Confer-\nence of the Association for Computational Linguis-\ntics, ACL 2019, Florence, Italy, July 28- August 2,\n2019, Volume 1: Long Papers, pages 2214–2220. As-\nsociation for Computational Linguistics.\nTobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie\nUtama, Ido Dagan, and Iryna Gurevych. 2019b.\nRanking generated summaries by correctness: An in-\nteresting but challenging application for natural lan-\nguage inference. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2214–2220, Florence, Italy. Associa-\ntion for Computational Linguistics.\nBen Goodrich, Vinay Rao, Mohammad Saleh, and Pe-\nter J. Liu. 2019. Assessing the factual accuracy of\ngenerated text. CoRR, abs/1905.13322.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of\nGPT-3. CoRR, abs/2209.12356.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018.\nNewsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. In Proceedings of the\n2018 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2018,\nNew Orleans, Louisiana, USA, June 1-6, 2018, Vol-\nume 1 (Long Papers) , pages 708–719. Association\nfor Computational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Samin Mubasshir, Yuan-Fang Li, Yong-\nBin Kang, M. Sohel Rahman, and Rifat Shahri-\nyar. 2021. Xl-sum: Large-scale multilingual ab-\nstractive summarization for 44 languages. In Find-\nings of the Association for Computational Linguis-\ntics: ACL/IJCNLP 2021, Online Event, August 1-\n6, 2021, volume ACL/IJCNLP 2021 of Findings of\nACL, pages 4693–4703. Association for Computa-\ntional Linguistics.\nKarl Moritz Hermann, Tomás Kociský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to\nread and comprehend. In Advances in Neural Infor-\nmation Processing Systems 28: Annual Conference\non Neural Information Processing Systems 2015,\nDecember 7-12, 2015, Montreal, Quebec, Canada ,\npages 1693–1701.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Ha-\ngai Taitelbaum, Doron Kukliansy, Vered Cohen,\nThomas Scialom, Idan Szpektor, Avinatan Hassidim,\nand Yossi Matias. 2022. TRUE: re-evaluating fac-\ntual consistency evaluation. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, NAACL 2022, Seattle,\nWA, United States, July 10-15, 2022 , pages 3905–\n3920. Association for Computational Linguistics.\nOr Honovich, Leshem Choshen, Roee Aharoni, Ella\nNeeman, Idan Szpektor, and Omri Abend. 2021.\n$qˆ2$: Evaluating factual consistency in knowledge-\ngrounded dialogues via question generation and\nquestion answering. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 7-11 November,\n2021, pages 7856–7870. Association for Computa-\ntional Linguistics.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh,\nHootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ran-\njay Krishna, Chen-Yu Lee, and Tomas Pﬁster. 2023.\n2063\nDistilling step-by-step! outperforming larger lan-\nguage models with less training data and smaller\nmodel sizes. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 8003–\n8017, Toronto, Canada. Association for Computa-\ntional Linguistics.\nJie Huang and Kevin Chen-Chuan Chang. 2022. To-\nwards reasoning in large language models: A survey.\nCoRR, abs/2212.10403.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nScitail: A textual entailment dataset from science\nquestion answering. In Proceedings of the Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence,\n(AAAI-18), the 30th innovative Applications of Arti-\nﬁcial Intelligence (IAAI-18), and the 8th AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018, pages 5189–5197. AAAI Press.\nTom Kocmi and Christian Federmann. 2023. Large lan-\nguage models are state-of-the-art evaluators of trans-\nlation quality. CoRR, abs/2302.14520.\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan.\n2023. An empirical survey on long document sum-\nmarization: Datasets, models, and metrics. ACM\nComput. Surv., 55(8):154:1–154:35.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In NeurIPS.\nWojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-\nCann, Caiming Xiong, and Richard Socher. 2019.\nNeural text summarization: A critical evaluation.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019 , pages 540–551.\nAssociation for Computational Linguistics.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020 , pages 9332–\n9346. Association for Computational Linguistics.\nPhilippe Laban, Tobias Schnabel, Paul N. Bennett, and\nMarti A. Hearst. 2022. Summac: Re-visiting nli-\nbased models for inconsistency detection in summa-\nrization. Trans. Assoc. Comput. Linguistics, 10:163–\n177.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kath-\nleen R. McKeown. 2020. Wikilingua: A new bench-\nmark dataset for cross-lingual abstractive summa-\nrization. CoRR, abs/2010.03093.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\npages 7871–7880. Association for Computational\nLinguistics.\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022. W ANLI: worker and AI collabora-\ntion for natural language inference dataset creation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022 , pages 6826–6847.\nAssociation for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNLG evaluation using GPT-4 with better human\nalignment. CoRR, abs/2303.16634.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency eval-\nuator for abstractive text summarization. CoRR,\nabs/2303.15621.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck, Bodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-reﬁne: Iterative reﬁnement with\nself-feedback. CoRR, abs/2303.17651.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan T. McDonald. 2020. On faithfulness and fac-\ntuality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July\n5-10, 2020, pages 1906–1919. Association for Com-\nputational Linguistics.\nAnshuman Mishra, Dhruvesh Patel, Aparna Vijayaku-\nmar, Xiang Lorraine Li, Pavan Kapanipathi, and Kar-\ntik Talamadupula. 2021. Looking beyond sentence-\nlevel natural language inference for question answer-\ning and text summarization. In Proceedings of the\n2021 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 1322–1336. Associ-\nation for Computational Linguistics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018 , pages 1797–1807. Association\nfor Computational Linguistics.\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2020. Ad-\nversarial NLI: A new benchmark for natural lan-\n2064\nguage understanding. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\npages 4885–4901. Association for Computational\nLinguistics.\nOpenAI. 2022. Chatgpt,\nhttps://openai.com/blog/chatgpt/.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with FRANK: A benchmark for\nfactuality metrics. In Proceedings of the 2021 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2021, Online,\nJune 6-11, 2021, pages 4812–4829. Association for\nComputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nMichael Collins, Dipanjan Das, Slav Petrov, Gau-\nrav Singh Tomar, Iulia Turc, and David Reitter. 2021.\nMeasuring attribution in natural language generation\nmodels. CoRR, abs/2112.12870.\nTal Schuster, Sihao Chen, Senaka Buthpitiya, Alex\nFabrikant, and Donald Metzler. 2022. Stretching\nsentence-pair NLI models to reason over long doc-\numents and clusters. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npages 394–412. Association for Computational Lin-\nguistics.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, Jacopo Staiano, Alex Wang,\nand Patrick Gallinari. 2021. Questeval: Summariza-\ntion asks for fact-based evaluation. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 6594–6604. Association for\nComputational Linguistics.\nAmir Soleimani, Christof Monz, and Marcel Worring.\n2023. NonFactS: NonFactual summary generation\nfor factuality evaluation in document summarization.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023 , pages 6405–6419, Toronto,\nCanada. Association for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and efﬁ-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nPrasetya Utama, Joshua Bambrick, Naﬁse Sadat\nMoosavi, and Iryna Gurevych. 2022. Falsesum:\nGenerating document-level NLI examples for recog-\nnizing factual inconsistency in summarization. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL 2022, Seattle, WA, United States, July 10-\n15, 2022, pages 2763–2776. Association for Compu-\ntational Linguistics.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 5008–5020. Association for Computa-\ntional Linguistics.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good NLG evaluator? A prelimi-\nnary study. CoRR, abs/2303.04048.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce label-\ning cost? GPT-3 can help. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021,\nVirtual Event / Punta Cana, Dominican Republic,\n16-20 November, 2021, pages 4195–4205. Associa-\ntion for Computational Linguistics.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He,\nKang Liu, and Jun Zhao. 2023. Large language mod-\nels are better reasoners with self-veriﬁcation. CoRR,\nabs/2212.09561.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguis-\ntics: Human Language Technologies , pages 4602–\n4625, Seattle, United States. Association for Com-\nputational Linguistics.\nWenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian\nLi, and Yajuan Lv. 2023. Wecheck: Strong factual\nconsistency checker via weakly supervised learning.\nProceedings of the 61th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2023.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mt5: A massively\nmultilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021 , pages\n483–498. Association for Computational Linguis-\ntics.\n2065\nWenpeng Yin, Dragomir R. Radev, and Caiming\nXiong. 2021. Docnli: A large-scale dataset for\ndocument-level natural language inference. In Find-\nings of the Association for Computational Linguis-\ntics: ACL/IJCNLP 2021, Online Event, August 1-\n6, 2021, volume ACL/IJCNLP 2021 of Findings of\nACL, pages 4913–4922. Association for Computa-\ntional Linguistics.\n2066\nA Appendix\nA.1 FLAN-PaLM Prompt Design\nTo apply FLAN-PaLM for factual consistency eval-\nuation, we experimented with zero-shot, few-shot\nand chain-of-thought prompting strategies, and var-\nious formats for each strategy. We chose the best\nperforming strategy and format, based on the accu-\nracy on a development set.23 Table 8 presents the\naccuracy of each prompt type on the development\nset. We observed only minor performance differ-\nences, and thus we opted for the simplest solution\nthat is the zero-shot prompt. While we cannot know\nthe exact reasons for why few-shot and chain-of-\nthought did not improve performance, we can offer\npotential explanations. (1) Since the model was\nﬁne-tuned on NLI datasets, it is able to effectively\ngeneralize to factual consistency evaluation, mak-\ning further demonstrations via few-shot prompting\nunnecessary in this case. (2) The performance with\nthe zero-shot prompt is already notably high (89%,\n§4.4) and thus our particular LLM is less likely\nto beneﬁt from chain-of-thought prompting. (3) It\ncould be the case that only a few reasoning steps\nare needed to evaluate consistency in our particular\nsetup and thus chain-of-thought is not necessarily\nbetter in this case.\nBelow, we describe our top-performing zero-\nshot, few-shot and chain-of-thought prompts.\nZero-shot Prompt Since FLAN-PaLM was in-\nstruction ﬁne-tuned on NLI, we designed our\nprompt to resemble an NLI prompt (e.g. using\n\"premise\" and \"hypothesis\" instead of \"document\"\nand \"summary\"). Our ﬁnal prompt is as follows:\nPremise: {document} Hypothesis: {summary} Can the\nhypothesis be inferred from the premise? Answer us-\ning \"Yes\" or \"No\" only.\nFew-shot Prompt We use two few-shot\nexamples, one \"consistent\" and one\n\"inconsistent\". We randomly sample these\nexamples from the development set examples\nshorter than 200 words. 23 We limit ourselves to\ntwo short examples since summarization examples\ncan include long documents, and thus few-shot\nmay lead to too long context length. Our ﬁnal\nprompt is as follows:\n23For development set we use the FactCC dataset (Kryscin-\nski et al., 2020) with 1,431 examples containing summaries\nof documents from CNN/DailyMail, manually annotated for\nfactual correctness. Following (Utama et al., 2022), we merge\nthe dev and test sets.\nPremise: (CNN) Desperate migrants from Africa and\nthe Middle East keep heading to Europe, with 978 res-\ncued Friday in the Mediterranean Sea, the Italian Coast\nGuard said Saturday via Twitter. The migrants were\npicked up 30 miles off the coast of Libya, said European\nParliament member Matteo Salvini, the leader of Italy’s\nfar-right Northern League. In the ﬁrst three months of\n2015, Italy registered more than 10,000 migrants arriv-\ning, the International Organization for Migration said,\nand about 2,000 were rescued at sea during the ﬁrst\nweekend of April in the Channel of Sicily. Most mi-\ngrants recorded this year come from countries in West\nAfrica as well as Somalia and Syria, the IMO said. They\nuse Libya as a country of transit. At least 480 migrants\nhave died while crossing the Mediterranean since the\nbeginning of the year, often because of bad weather and\novercrowded vessels used by smugglers, the IMO said.\nSometimes the captains and crews abandon the ships,\nleaving passengers to fend for themselves. At this time\nlast year, there were fewer than 50 deaths reported, the\nIMO said. Most of the migrants are asylum seekers, vic-\ntims of trafﬁcking or violence, unaccompanied children\nand pregnant women.\nHypothesis: the migrants were picked up 30 miles off\nthe coast of libya.\nCan the hypothesis be inferred from the premise? An-\nswer using \"Yes\" or \"No\" only.\nAnswer: Yes\nPremise: (CNN) A nuclear submarine being repaired at\na Russian shipyard has caught on ﬁre, according to a\nlaw enforcement source speaking to Russia’s state-run\nnews agency ITAR-Tass. \"The submarine is in a dry\ndock,\" Tass reports, citing the source, and there is no\nammunition on board. \"The rubber insulation between\nthe submarine’s light and pressure hull is on ﬁre,\" Tass\nreported. Russia’s RIA Novosti news agency says insu-\nlation caught on ﬁre as welding work was being done\non the submarine. Tass reported that the ﬁre began on a\nsub in the Zvyozdochka shipyard in northwestern Russia.\nZvyozdochka spokesman Yevgeny Gladyshev told the\nnews agency that the sub had been undergoing repairs\nsince November 2013. \"Nuclear fuel from the sub’s re-\nactor has been unloaded,\" he reportedly said. \"There\nare no armaments or chemically active, dangerous sub-\nstances, ﬁssionable materials on it,\" Gladyshev said to\nTass. \"The enterprise’s personnel left the premises when\nthe submarine caught ﬁre, no one has been injured. The\nﬁre presents no threat to people and the shipyard.\"\nHypothesis: \"the rubber insulation between the subma-\nrine’s light and pressure hull is on ﬁre,\" russia’s ria\nnovosti news agency says.\nCan the hypothesis be inferred from the premise? An-\nswer using \"Yes\" or \"No\" only.\nAnswer: No\nPremise: {document}\nHypothesis: {summary}\nCan the hypothesis be inferred from the premise? An-\nswer using \"Yes\" or \"No\" only.\nAnswer:\nChain-of-thought Prompt Following Kojima\net al. (2022) we append \"Let’s think step by\nstep\" to the prompt to facilitate a step-by-step rea-\nsoning before predicting the label. Our ﬁnal prompt\nis as follows:\n2067\nPremise: {document}\nHypothesis: {summary}\nQ: Can the hypothesis be inferred from the premise?\nAnswer using \"Yes\" or \"No\" only.\nA: Let’s think step by step\nThis prompt successfully unlocked a step by step\nreasoning. Below is FLAN-PaLM’s response for-\nmat, where {answer} is either \"yes\" or \"no\".\n{reasoning steps}. So, the answer is{answer}.\nExample input:\nPremise: (CNN) Georgia Southern University was in\nmourning Thursday after ﬁve nursing students were\nkilled the day before in a multivehicle wreck near Sa-\nvannah. Caitlyn Baggett, Morgan Bass, Emily Clark,\nAbbie Deloach and Catherine (McKay) Pittman – all\njuniors – were killed in the Wednesday morning crash\nas they were traveling to a hospital in Savannah, ac-\ncording to the school website. Fellow nursing students\nBrittney McDaniel and Megan Richards were injured\nas was another person, who was not identiﬁed by the\nGeorgia State Patrol. The young women were on their\nway to ﬁnish their ﬁrst set of clinical rotations ... ... ...\nHypothesis: georgia southern university was in mourn-\ning after ﬁve nursing students died.\nQ: Can the hypothesis be inferred from the premise?\nAnswer using \"Yes\" or \"No\" only.\nA: Let’s think step by step\nThe output for this example is:\nGeorgia Southern University was in mourning Thursday\nafter ﬁve nursing students were killed the day before in\na multivehicle wreck near Savannah. So, the answer is\nyes.\nA.2 Inference with FLAN-PaLM\nWe used the zero-shot prompt (see §A.1). The vast\nmajority of FLAN-PaLM’s responses were either\n\"Yes\" or \"No\", and a tiny fraction of the responses\nwere \"It’s impossible to say\".\nDuring the labeling phase, we let FLAN-PaLM\ngenerate the output (predict mode), and label as\n\"consistent\" if the generated output is \"Yes\"\nand \"inconsistent\" in case the output is \"No\".\nWe discard the \"It’s impossible to say\" ex-\namples. In order to measure ROC-AUC in a binary\nclassiﬁcation setting, we compute the model’s prob-\nability of generating \"Yes\" (score mode) and use\nit as the example-level factual consistency score.\nA.3 Fine tuning T5\nWe ﬁne tune our T5 models for factual consistency\nevaluation using the following input format:\nPrompt type Dev accuracy\nzero-shot 93.6\nfew-shot 93.2\nchain-of-thought 93.8\nTable 8: FLAN-PaLM accuracy on the development\nset23 using different prompting strategies.\nLanguage ISO 639-1 consistent inconsistent\nEnglish en 12,500 12,500\nSpanish es 12,500 12,500\nFrench fr 12,500 12,500\nGerman de 12,500 12,500\ntotal 50,000 50,000\nTable 9: Our multilingual dataset statistics.\nPremise: {document} Hypothesis: {summary}\nThe model is trained to predict \"1\" if the sum-\nmary is factually consistent and \"0\" otherwise. We\nuse a learning rate of 10−4 and a batch size of 32.\nDuring training, we use a maximum input length\nof 512 tokens and truncate the premise if needed.24\nDuring inference we use a maximum input length\nof 2048 tokens. We train for a maximum of 20\nepochs, evaluate a checkpoint every 1k steps and\nchoose the checkpoint with the best ROC-AUC on\na development set.23 In our study we make sure to\nuse the same training regime for all baselines.\nThe ANLI-only results in Table 3 are from our\nexperiments, while in Table 2 we use the results\nreported in previous work.\nFor the summarization models we ﬁne tune the\ncorresponding T5 models on the XSum training set\n(Narayan et al., 2018) in a similar fashion and use\nthe ROUGE score on the XSum development set\nas a stopping criteria.\nA.4 Additional Details About Our Dataset\nAs mentioned in §3.1, we create the dataset based\non documents from CNN/DailyMail (Hermann\net al., 2015). We do not use the gold summaries,\nand we only use examples from the training set.\nIn our experiments with the full dataset (§4.1),\nwe balance the labels by randomly sampling\n475,563 positive examples (see Table 1).\n24In early experiments we saw that training with longer\nmaximum input length resulted with comparable performance.\n2068\nFigure 4: Self-veriﬁcation prompting. If the LLM clas-\nsiﬁed the summary as consistent, we prompt it again\nand ask it for its certainty. If the answer is “Yes” (con-\nsistent with the original reasoning), we keep the exam-\nple, otherwise we ﬁlter it out.\nA.5 Data Filtering with Self-veriﬁcation\nAs mentioned in §3 we also explored data ﬁlter-\ning based on prompting FLAN-PaLM for self-\nveriﬁcation. Our proccess is based on 3 steps. (1)\nDetect potential examples in our dataset that are\nlikely to be labeled incorrectly by the LLM. (2)\nPrompt the LLM to self-verify its earlier prediction\nand ﬁlter out examples that the model is uncertain\nof. This leads to a smaller dataset with improved\nlabeling accuracy. (3) Train the factual consistency\nevaluation model on the ﬁltered dataset. This ap-\nproach is based on 2 observations:\n1. In early experiments, we saw that our LLM has\nextremely high precision for the inconsistent\nclass. This can also be seen in our human eval-\nuation (Table 4). This means that almost all\nthe errors occur when the LLM predicts that\nthe summary is consistent. Following this, we\nonly consider ﬁltering examples classiﬁed as\nconsistent by the LLM.\n2. Inspired by the work of Weng et al. (2023) and\nMadaan et al. (2023), we use a self veriﬁcation\nprompt. If the LLM classiﬁed the summary as\nconsistent, we prompt it again and ask it for its\ncertainty. If the answer is “Yes” (i.e. it is consis-\ntent with the original reasoning path), we keep\nthe example, otherwise we ﬁlter it out. This\nproccess is illustrated in Figure 4.\nThe self-veriﬁcation prompt is as follows:\nPremise: {document} Hypothesis: {summary} Are you\nsure that the summary can be inferred from the docu-\nment? Answer using \"Yes\" or \"No\" only.\nThis approach ﬁltered-out 15% of the dataset.\n2.5\n 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\nExtractiveness\n0.0\n0.1\n0.2\n0.3\n0.4Density\nFactEdit\nDocNLI\nFactCC\nFalsesum\nOurs\nFigure 5: Visualization of the density of the combined\nabstractivness score. The plot is actually measuring the\nextractiveness degree, so lower x-values mean higher\nabstractiveness.\nWhen we qualitatively analyzed the ﬁltered exam-\nples, it seems that the majority of the ﬁltered exam-\nples indeed had a wrong label, and that applying\nthis ﬁltering mechanism increases the labeling ac-\ncuracy by approximately 5%.\nWhile this ﬁltering mechanism results in higher\nlabeling accuracy, we did not observe a perfor-\nmance gain when ﬁltering the training data in this\nway. For TrueTeacher + ANLI with T5-11B (on\na sample of 100k examples) we got an average\nof 86 ROC-AUC on TRUE using the ﬁltered data,\nslightly below the 86.4 using the unﬁltered data\n(Table 3). As mentioned in Footnote 9, we attribute\nthis to the fact that the labeling accuracy is high to\nbegin with (89%, section 4.4) and that the model\nis likely robust to some amount of labeling noise.\nFollowing this, for simplicity, our ofﬁcial method\ndoes not use ﬁltering.\nA.6 Abstractiveness Analysis: Additional\nDetails\nAs our backbone metrics we use the Extractive\nFragment Coverage and Density measures deﬁned\nby Grusky et al. (2018). Coverage measures the\npercentage of words in the summary that are part\nof an extractive fragment with the article, quanti-\nfying the extent to which a summary is derivative\nof a text. Density measures the average length of\nthe extractive fragment to which each word in the\nsummary belongs, quantifying how well the word\nsequence of a summary can be described as a series\nof extractions. Our Combined score is obtained by\nmultiplyng the Coverage and the Density scores,\nsimilar to Utama et al. (2022). To further illustrated\nthe differences in the abstractiveness of different\nmethods, we include a visualization of the density\nof the combined abstractivness score in Figure 5.\n2069\nANLI+XNLI +100K en +100K en/es/de/fe\namharic 63.1 67.2 68.6\narabic 87.8 89.0 87.7\nazerbaijani 59.6 68.6 65.5\nbengali 90.4 94.3 98.5\nburmese 59.0 64.5 57.9\nchinesesimp. 87.6 86.4 89.9\nchinese trad. 82.5 82.6 83.2\nenglish 80.2 74.7 80.0\nfrench 91.9 94.1 97.1\ngujarati 50.8 52.0 51.5\nhausa 69.5 67.7 73.7\nhindi 72.2 79.9 86.5\nigbo 62.2 62.8 75.7\nindonesian 77.6 84.1 85.8\njapanese 97.7 98.9 99.6\nkirundi 83.5 89.3 90.4\nkorean 87.3 82.3 89.9\nkyrgyz 70.1 77.4 79.0\nmarathi 75.2 78.7 73.6\nnepali 55.2 59.1 57.2\noromo 81.2 83.7 83.3\npashto 56.4 68.2 67.7\npersian 43.5 42.3 45.8\npidgin 70.0 81.4 77.1\nportuguese 79.6 79.5 79.0\npunjabi 77.7 81.5 78.2\nrussian 88.8 85.1 81.2\nscottish gaelic 59.0 58.8 63.1\nserbian cyrillic 84.2 79.3 85.5\nserbian latin 39.7 42.2 43.6\nsinhala 72.9 74.9 76.1\nsomali 85.1 88.6 86.6\nspanish 80.7 85.9 89.1\nswahili 88.1 89.2 92.2\ntamil 63.9 69.8 66.0\ntelugu 55.9 62.3 60.4\nthai 78.8 83.8 86.8\ntigrinya 79.9 82.9 86.1\nturkish 87.0 86.6 86.6\nukrainian 55.5 67.0 65.9\nurdu 69.0 63.8 75.3\nuzbek 54.6 59.3 58.8\nvietnamese 89.8 84.4 88.1\nwelsh 83.0 83.4 83.9\nyoruba 69.0 69.0 77.2\n# wins 5 15 25\n# > ANLI+XNLI - 32 35\nPer lang. avg. 73.3 75.7 77.2\nPer example avg. 71.6 73.8 75.3\nTable 10: ROC-AUC results on the mFace test set.\nA.7 Using the mFace dataset\nIn §5 we report results on the mFace dataset (Aha-\nroni et al., 2022). Aharoni et al. performed large\nscale human evaluation of summaries of documents\nfrom the XLSum corpus (Hasan et al., 2021), pro-\nduced by different summarization models. Each\nsummary was rated for quality, attribution and in-\nformativeness. We use the attribution scores in\nour work. The attribution evaluation is based on\nthe attribution deﬁnition provided in Rashkin et al.\n(2021), with the participants asked \" Is all the in-\nformation in the summary fully attributable to the\narticle?\". In our work we use the average attribu-\ntion score (between 0 to 1) and treat summaries as\nfactually consistent if the score is larger than 0.5.\nWe focus on the test split of XLSum containing\n3150 examples in 45 languages (i.e., 70 examples\nin each language). In §5 we refer to Table 7 with\nthe results overview, and we provide the full results\nfor all languages in Table 10.\nA.8 Human Evaluation\nWe instructed the participants to review the docu-\nment and its corresponding summary, and to evalu-\nate the summary based on the attribution deﬁnition\nprovided by Rashkin et al. (2021), using binary\njudgements. To avoid a common confusion be-\ntween factual inconsistency and contradiction, we\nalso provided the following instruction:\nIn this task you will evaluate the factual consistency of\na system-generated summary. The system’s goal is to\nsummarize the original source document, while remain-\ning truthful to it. Your goal is to evaluate whether the\nsystem-generated summary is consistent w.r.t. the source\ndocument. Summary will be considered consistent if\nall of the information in the summary can be veriﬁed\nfrom the source document (i.e., for the summary to be\ninconsistent, the document does not necessarily need to\ncontradict it, it can also fail to support some facts).\nIn an early experiment, we found that using\ncrowd workers without domain expertise and sub-\nstantial time investments resulted in extremely low-\nquality ratings. Following this, all our raters were\nNLP researchers, each with at least one year of spe-\nciﬁc experience in the task of factual consistency\nevaluation, with signiﬁcant time allocation and no\nmore than 10 examples per rater. 25 These steps\nensured high quality ratings.\nA.9 Adding noise to TrueTeacher\nIn §4.5 we create SummaryAblation by ﬂipping\nlabels to a random portion of TrueTeacher’s data,\nsuch that the expected labeling accuracy is sim-\nilar to Falsesum. Falsesum’s labeling method is\ncoupled with the data generation, thus we need an\napproximation for its labeling quality. We estimate\nFalesum’s labeling accuracy as 83.5%, according\nto Utama et al. (2022)’s human evaluation (we aver-\nage the Intrinsic and Extrinsic results), while ours\nis 89% (§4.4). So to mimic Falsesum’s quality we\nﬂipped TrueTeacher’s labels in order to add addi-\ntional 5.5% errors.\n25We found that it is sufﬁcient to use one rater per example\n(unlike in our experiments with the crowd workers).\n2070"
}