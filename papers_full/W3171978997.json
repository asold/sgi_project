{
  "title": "Incorporating Word Sense Disambiguation in Neural Language Models",
  "url": "https://openalex.org/W3171978997",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5008679414",
      "name": "Jan Philip Wahle",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5081763922",
      "name": "Terry Ruas",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5060549879",
      "name": "Norman Meuschke",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5058837356",
      "name": "Béla Gipp",
      "affiliations": [
        "University of Wuppertal"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2782863194",
    "https://openalex.org/W2970773744",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963956638",
    "https://openalex.org/W2740782137",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2973827203",
    "https://openalex.org/W2970166416",
    "https://openalex.org/W2296653400",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W3034675880",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2964189868",
    "https://openalex.org/W2436001372",
    "https://openalex.org/W2065157922",
    "https://openalex.org/W2950502294",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3034782826",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2891031945",
    "https://openalex.org/W3081304984",
    "https://openalex.org/W2081580037"
  ],
  "abstract": "We present two supervised (pre-)training methods to incorporate gloss definitions from lexical resources into neural language models (LMs). The training improves our models' performance for Word Sense Disambiguation (WSD) but also benefits general language understanding tasks while adding almost no parameters. We evaluate our techniques with seven different neural LMs and find that XLNet is more suitable for WSD than BERT. Our best-performing methods exceeds state-of-the-art WSD techniques on the SemCor 3.0 dataset by 0.5% F1 and increase BERT's performance on the GLUE benchmark by 1.1% on average.",
  "full_text": "Incorporating Word Sense Disambiguation in Neural Language Models\nJan Philip Wahle1, Terry Ruas1, Norman Meuschke1,2, Bela Gipp1\n1University of Wuppertal, Rainer-Gruenter-Str. 21, D-42119, Wuppertal, Germany\n2University of Konstanz, Universittsstrae 10, 78464, Konstanz, Germany\n1last@uni-wuppertal.de\n2first.last@uni-konstanz.de\nAbstract\nWe present two supervised (pre-)training meth-\nods that incorporate gloss deﬁnitions from lex-\nical resources to leverage Word Sense Dis-\nambiguation (WSD) capabilities in neural lan-\nguage models. Our training focuses on WSD\nbut keeps its capabilities when transferred to\nother tasks while adding almost no additional\nparameters. We evaluate our technique on 15\ndownstream tasks, e.g., sentence pair classiﬁ-\ncation and WSD. We show that our methods\nexceed comparable state-of-the-art techniques\non the SemEval and Senseval datasets as well\nas increase the performance of its baseline on\nthe GLUE benchmark.\n1 Introduction\nWSD tries to determine the meaning of words\ngiven a context and is arguably one of the\noldest challenges in natural language process-\ning (NLP) (Weaver, 1955; Navigli, 2009). In\nknowledge-based methods (Camacho-Collados\net al., 2015), lexical knowledge databases (LKB)\n(e.g., WordNet (Miller, 1995; Fellbaum, 1998)) il-\nlustrate the relation between words and their mean-\ning. Supervised techniques (Pasini and Navigli,\n2020) rely on annotated data to perform disam-\nbiguation while unsupervised ones (Chaplot and\nSalakhutdinov, 2018) explore other aspects, e.g.,\nlarger contexts, topic modeling.\nRecently, supervised methods (Huang et al.,\n2019; Bevilacqua and Navigli, 2020) rely on word\nrepresentations from BERT (Devlin et al., 2019),\nalthough advances in bidirectional transformers\nhave been proposed (Yang et al., 2019; Clark et al.,\n2020). We compare these novel models and vali-\ndate which ones are most suitable for the WSD task.\nThus, we deﬁne an end-to-end approach capable of\nbeing applied to any language model (LM).\nFurther, pre-trained word representations have\nbecome crucial for LMs and almost any NLP task\n(Mikolov et al., 2013a; Radford et al., 2018). LMs\nare trained on large unlabeled corpora and often\nignore relevant information of word senses in LKB\n(e.g., gloss1). As our experiments support, there is\na positive correlation between the LM’s ability to\ndisambiguate words and NLP tasks performance.\nWe propose a set of general supervised meth-\nods that integrate WordNet knowledge for WSD\nin LM during the pre-training phase and validate\nthe improved semantic representations on other\ntasks (e.g., text-similarity). Our technique sur-\npasses comparable methods in WSD by 0.5% F1\nand improves language understanding in several\ntasks by 1.1% on average. The repository for all\nexperiments is publicly available2.\n2 Related Work\nThe same way word2vec (Mikolov et al., 2013b)\ninspired many models in NLP (Bojanowski et al.,\n2017; Ruas et al., 2020), BERT (Devlin et al.,\n2019) echoed in the literature with recent mod-\nels as well (Yang et al., 2019; Clark et al., 2020).\nThese novel models achieve higher performance in\nseveral NLP tasks but are mostly neglected in the\nWSD domain (Wiedemann et al., 2019) with a few\nexceptions (Loureiro et al., 2020).\nBased on the Transformer (Vaswani et al., 2017)\narchitecture, BERT (Devlin et al., 2019) proposes\ntwo pre-training tasks to capture general aspects of\nthe language, i.e., Masked Language Model(MLM)\nand Next Sentence Prediction(NSP). AlBERT (Lan\net al., 2019), DistilBERT (Sanh et al., 2019), and\nRoBERTa (Liu et al., 2019) either boost BERT’s\nperformance through parameter adjustments, in-\ncreased training volume, or make it more efﬁcient.\nXLNet (Yang et al., 2019) focuses on improving the\ntraining objective, while ELECTRA (Clark et al.,\n2020) and BART (Lewis et al., 2020) propose a\ndiscriminative denoising method to distinguish real\n1Brief deﬁnition of a synonym set (synset) (Miller, 1995).\n2https://github.com/jpelhaW/\nincorporating_wsd_into_nlm\narXiv:2106.07967v3  [cs.CL]  15 Mar 2022\nand plausible artiﬁcial generated input tokens.\nDirectly related to our work, GlossBERT (Huang\net al., 2019) uses WordNet’s glosses to ﬁne-tune\nBERT in the WSD task. GlossBERT classiﬁes\na marked word in a sentence into one of its pos-\nsible deﬁnitions. KnowBERT (KBERT) (Peters\net al., 2019) incorporates LKB into BERT with a\nknowledge attention and recontextualization mech-\nanism. Peters et al. (2019) best-performing model,\ni.e., KBERT-W+W, surpasses BERTBASE at the\ncost of ≈400M parameters and 32% more train-\ning time. Our methods do not require embed-\ndings adjustments from the LKB or use word-\npiece attention, resulting in a cheaper alternative.\nEven though recent contributions in WSD such as\nLMMS (Loureiro and Jorge, 2019), BEM (Blevins\nand Zettlemoyer, 2020), GLU (Hadiwinoto et al.,\n2019), and EWISER (Bevilacqua and Navigli,\n2020) enhance the semantic representation via con-\ntext or external knowledge, they do not explore\ntheir generalization to other NLP tasks.\n3 Methods\nCurrent methods (Huang et al., 2019; Du et al.,\n2019; Peters et al., 2019; Levine et al., 2019) mod-\nify the WSD task into a text classiﬁcation problem,\nleveraging BERT’s semantic information through\nWordNet’s resources. Although BERT is a strong\nbaseline, studies show the model does not converge\nto its full capacity, and its training scheme still\npresents opportunities for development (Liu et al.,\n2019; Yang et al., 2019).\nWe deﬁne a general method to perform WSD\nin arbitrary LMs and discuss possible architectural\nalternatives for its improvements (Section 3.1). We\nassume WSD is a suitable task to complement\nMLM as we often ﬁnd polysemous words in natural\ntext. We introduce a second variation in our method\n(Section 3.2) that keeps previous LM capabilities\nwhile improving polysemy understanding.\n3.1 Language Model Gloss Classiﬁcation\nWith Language Model Gloss Classiﬁcation\n(LMGC), we propose a general end-to-end WSD\napproach to classify ambiguous words from sen-\ntences into one of WordNet’s glosses. This ap-\nproach allows us to evaluate different LMs at WSD.\nLMGC builds on the ﬁnal representations of\nits underlying transformer with a classiﬁcation ap-\nproach closely related to (Huang et al., 2019). Each\ninput sequence starts with an aggregate token (e.g.,\nthe “[CLS]” token in BERT), i.e., an annotated sen-\ntence containing the ambiguous word, followed by\na candidate gloss deﬁnition from a lexical resource,\nsuch as WordNet, for that speciﬁc word.\nSentence and gloss are concatenated with a sep-\narator token and pre-processed with the respective\nmodel’s tokenizer. We modify the input sequence\nwith two supervision signals: (1) highlighting the\nambiguous tokens with two special tokens and (2)\nadding the polysemous word before the gloss.\nConsidering Du et al. (2019) and Huang et al.\n(2019) ﬁndings, we apply a linear layer to the ag-\ngregate representation of the sequence to perform\nclassiﬁcation rather than using token embeddings.\nIn contrast, we suggest modifying the prediction\nstep from sequential binary classiﬁcation to a par-\nallel multi-classiﬁcation construct, similar to Kåge-\nbäck and Salomonsson (2016). Therefore, we stack\nthe k candidate sentence-gloss pairs at the batch\ndimension and classify them using softmax.\nWe retrieve WordNet’s gloss deﬁnitions of pol-\nysemous words corresponding to the annotated\nsynsets to create sentence-gloss inputs. To acceler-\nate training time by approximately a factor of three,\ncompared to (Huang et al., 2019), we reduced the\nsequence length of all models from 512 to 1603 as\nthe computational cost of transformers is quadratic\nconcerning the sequence length.\n3.2 LMGC with Masked Language Modeling\nComparable WSD systems (Huang et al., 2019;\nBevilacqua and Navigli, 2020; Blevins and Zettle-\nmoyer, 2020) and LMGC focus on improving the\nperformance in WSD rather than leveraging it with\nlexical resources for language understanding. We\nassume the transfer learning between LM and WSD\nincreases the likelihood of grasping polysemous\nwords in co-related tasks. Thus, we employ LMGC\nas an additional supervised training objective into\nMLM (LMGC-M) to incorporate lexical knowl-\nedge into our pre-training.\nLMGC-M performs a forward pass with anno-\ntated examples from our corpus with words masked\nat a certain probability. Moreover, LMGC-M uses\nLMGC as a second objective, similar to NSP in\nBERT. To prevent underﬁtting, due to task difﬁ-\nculty, we only mask words in the context of the\npolysemous word. Before inference, we ﬁne-tune\nLMGC without masks.\n399.8% of the data set can be represented with 160 tokens;\nwe truncate remaining sequences to this limit.\n4 Experiments\nWe evaluate our proposed methods in two bench-\nmarks, namely SemCor (3.0) (Miller et al., 1993;\nRaganato et al., 2017) and GLUE (Wang et al.,\n2019b). The English all-words WSD benchmark\nSemCor, detailed in Table 1, is popular in the WSD\nliterature (Huang et al., 2019; Peters et al., 2019)\nand one of the largest manually annotated datasets\nwith approximately 226k word sense annotations\nfrom WordNet (Miller, 1995). GLUE (Wang et al.,\n2019b) is a collection of nine language understand-\ning tasks widely used (Devlin et al., 2019; Lan\net al., 2019) to validate the generalization of LMs\nin different linguistic phenomena. All GLUE tasks\nare single sentence or sentence pair classiﬁcation,\nexcept STS-B, which is a regression task.\nDataset POS Tags Class dist.\nNoun Verb Adj. Adv. Total Pos. Neg.\nSemCor 87k 88.3k 31.7k 18.9k 226k 226.5k 1.79m\nSE2 1k 517 445 254 2.3k 2.4k 14.2k\nSE3 900 588 350 12 1.8k 1.8k 15.3k\nSE7 159 296 0 0 455 459 4.5k\nSE13 1.6k 0 0 0 1.6k 1.6k 9.7k\nSE15 531 251 160 80 1k 1.2k 6.5k\nTable 1: SemCor training corpus details: general statis-\ntics (left) and class distribution for LMGC (right).\n4.1 Setup\nAll models were initialized using the base con-\nﬁguration of its underlying transformer (e.g.,\nBERTBASE , L=12, H=768, A=12). Both of our\nmethods have 2 ∗H + 2 more parameters com-\npared to their baseline (e.g., LMGC (BERT) has\n≈110M parameters). We increased the hidden\ndropout probability to 0.2 as we observed overﬁt-\nting for most models. Further, we explicitly treated\nthe class imbalance of positive and negative exam-\nples (Table 1) in LMGC with focal loss (Lin et al.,\n2017) (γ = 2, α= 0.25). Following Devlin et al.\n(2019), we used a batch size of 32 sequences, the\nAdamW optimizer (α= 2e-5), trained three epochs,\nand choose the best model by validation loss. We\napplied the same hyperparameter conﬁguration for\nall models used in both SemCor and GLUE bench-\nmarks. The training was performed on 1 NVIDIA\nTesla V100 GPU for ≈3 hours per epoch.\nFor all GLUE tasks, except for STS-B, we trans-\nformed the aggregate embedding into a classiﬁca-\ntion vector applying a new weight matrix W ∈\nRK×H; where Kis the number of labels. For STS-\nSystem SE7 SE2 SE3 SE13 SE15 All\nBERT 2019 71.9 77.8 74.6 76.5 79.7 76.6\nRoBERTa 2019 69.2 77.5 73.8 77.2 79.7 76.3\nDistilBERT 2019 66.2 74.9 70.7 74.6 77.1 73.5\nAlBERT (2019) 71.4 75.9 73.9 76.8 78.7 75.7\nBART (2020) 67.2 77.6 73.1 77.5 79.7 76.1\nXLNet (2019) 72.5 78.5 75.6 79.1 80.1 77.2\nELECTRA (2020) 62.0 71.5 67.0 73.9 76.0 70.9\nTable 2: SemCor test results of LMGC for base trans-\nformer models. Bold font indicates the best results.\nSystem SE7 SE2 SE3 SE13 SE15 All\nGAS (2018b) - 72.2 70.5 67.2 72.6 70.6\nCAN (2018a) - 72.2 70.2 69.1 72.2 70.9\nHCAN (2018a) - 72.8 70.3 68.5 72.8 71.1\nLMMSBERT(2019) 68.1 76.3 75.6 75.1 77.0 75.4\nGLU (2019) 68.1 75.5 73.6 71.1 76.2 74.1\nGlossBERT (2019) 72.5 77.7 75.2 76.1 80.4 77.0\nBERTWSD(2019) - 76.4 74.9 76.3 78.3 76.3\nKBERT-W+W (2019) - - - - - 75.1\nLMGC (BERT) 71.9 77.8 74.6 76.5 79.7 76.6\nLMGC-M (BERT) 72.9 78.2 75.5 76.3 79.5 77.0\nLMGC (XLNet) 72.5 78.5 75.6 79.1 80.1 77.2\nLMGC-M (XLNet)73.0 79.1 75.9 79.0 80.3 77.5\nTable 3: SemCor test results compared to state-of-the-\nart techniques. Bold font indicates the best results.\nB, we applied a new weight matrix V ∈R1×H\ntransforming the aggregate into a single value.\n4.2 Results & Discussion\nTable 2 reports the results of applying LMGC to\ndifferent transformer models. Our rationale for\nchoosing models was two-fold. First, we explore\nmodels closely related or based on BERT, either by\nimproving it through additional training time and\ndata (RoBERTa), or compressing the architecture\nwith minimal performance loss (DistilBERT, Al-\nBERT). Second, models that signiﬁcantly change\nthe training objective (XLNet), or employ a dis-\ncriminative learning approach (ELECTRA, BART).\nIn Table 3, we compare our techniques to other\ncontributions in WSD. All results of SemCor are\nreported according to Raganato et al. (2017).\nRoBERTa shows inferior F1 when compared to\nBERT although it uses more data and training time.\nAs expected, DistilBERT and AlBERT perform\nworse than BERT, but AlBERT keeps reasonable\nperformance with only ≈10% of BERT’s parame-\nters. ELECTRA and BART results show their de-\nnoising approach is not suitable for our WSD setup.\nBesides, BART presents similar performance to\nBERT, but with 26% more parameters. XLNet\nconstantly performs better than BERT on all evalu-\nSystem\nClassiﬁcation Semantic Similarity Natural Language Inference Average\nCoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE -\n(mc) (acc) (F1) (sc) (acc) m/mm(acc) (acc) (acc) -\nBERTBASE 52.1 93.5 88.9 85.8 89.3 84.6/83.4 90.5 66.4 81.4\nGlossBERT 32.8 90.4 75.2 90.4 68.5 81.3/80 83.6 47.3 70.7\nLMGC (BERT) 31.1 89.2 81.9 89.2 87.4 81.4/80.3 85.4 60.2 74.5\nLMGC-M (BERT)55.0 94.2 87.1 88.1 90.8 85.3/84.2 90.1 69.7 82.5\nTable 4: GLUE test results. As in BERT, we exclude the problematic WNLI set. We report F1-score for MRPC,\nSpearman correlations (sc) for STS-B, Matthews correlations (mc) for CoLA, and accuracy (acc) for the other\ntasks (with matched/mismatched accuracy for MNLI). Bold font indicates the best results.\nation sets with no additional parameters, justifying\nits choice for our models’ variations.\nWe see an overall improvement when compar-\ning LMGC to the other approaches in Table 3.\nLMGC (BERT) generally outperforms the baseline\nBERTWSD approach, and KBERT-W+W which\nhas four times the number of parameters. We show\nby using an optimal transformer (XLNet) and ad-\njustments in the training procedure, we can out-\nperform GlossBERT in all test sets. We exclude\nEWISER (Bevilacqua and Navigli, 2020) which\nexplores additional knowledge other than gloss\ndeﬁnition (e.g, knowledge graph). We leave for\nfuture work the investigation of BEM (Blevins\nand Zettlemoyer, 2020), a recently published bi-\nencoder architecture with two encoders (i.e., con-\ntext and gloss) that are learned simultaneously.\nLMGC-M often outperforms LMGC, which we\nassume is due to the similarity to discriminated\nﬁne-tuning (Howard and Ruder, 2018). We com-\nbine LMGC and MLM in one pass, achieving\nhigher accuracy in WSD and improving general-\nization. Considering large models, preliminary\nexperiments2 showed a difference of 0.08% in\nF1 between BERTBASE and BERTLARGE for the\nSemCor datasets which is in line with Blevins and\nZettlemoyer (2020). Thus, we judged the base con-\nﬁguration sufﬁcient for our experiments.\nTo show that WSD training allows language\nmodels to achieve higher generalization, we ﬁne-\ntune the weights from our approaches in the\nGLUE (Wang et al., 2019b) datasets. Our results\nin Tables 3 and 4 show LMGC-M outperforms\nthe state-of-the-art in the WSD task and success-\nfully transfer the acquired knowledge to general\nlanguage understanding datasets. We exclude XL-\nNet from the comparison to show that the addi-\ntional performance can be contributed mainly to\nour method; not to the improvement of XLNet over\nBERT. The number of polysemous words in the\nGLUE benchmark is high in general, supporting\nthe training design of our method. We provide more\ndetails about polysemy in GLUE in our repository2.\nWe evaluated our proposed methods against the\nbest performing model in WSD (Table 3) on the\nGLUE datasets (Table 4). Comparing LMGC-M\nwith the ofﬁcial BERT BASE model, we achieve\na 1.1% increase in performance on average. In\nthis work, we did not compare LMGC-M to the\nother WSD methods performing worse than Huang\net al. (2019) in the WSD task (Table 3) because of\ncomputational requirements (i.e., KBERT-W+W is\n32% slower). Unsurprisingly, LMGC and Gloss-\nBERT perform well in WSD, but cannot maintain\nperformance on other GLUE tasks. LMGC-M out-\nperforms the underlying baseline (BERT) on most\ntasks and is comparable to the others. Therefore,\nincorporating MLM to our WSD architecture lever-\nages LMGC semantic representation and improves\nits natural language understanding capabilities.\n5 Conclusions and Future Work\nIn this paper, we proposed a set of methods\n(LMGC, LMGC-M) that allows for (pre-)training\nWSD, which is essential for many NLP tasks (e.g.,\ntext-similarity). Our techniques perform WSD\ncombining neural language models with lexical\nresources from WordNet. We exceeded state-of-\nthe-art of WSD methods (+0.5%) and improved the\nperformance over BERT in general language under-\nstanding tasks (+1.1%). Future work will include\ntesting generalization on the WiC (Pilehvar and\nCamacho-Collados, 2019), and SuperGLUE (Wang\net al., 2019a) datasets. Besides, we want to test\ndisciminative ﬁne-tuning against our parallel ap-\nproach (Howard and Ruder, 2018), and perform\nan ablation study to investigate which components\nof our methods lead to the most beneﬁts. We also\nleave for future work to incorporate knowledge\nfrom other sources (e.g., Wikidata, Wikipedia).\nReferences\nMichele Bevilacqua and Roberto Navigli. 2020. Break-\ning Through the 80% Glass Ceiling: Raising the\nState of the Art in Word Sense Disambiguation by In-\ncorporating Knowledge Graph Information. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 2854–\n2864, Online. Association for Computational Lin-\nguistics.\nTerra Blevins and Luke Zettlemoyer. 2020. Moving\nDown the Long Tail of Word Sense Disambiguation\nwith Gloss Informed Bi-encoders. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 1006–1017, On-\nline. Association for Computational Linguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching Word Vectors with\nSubword Information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nJosé Camacho-Collados, Mohammad Taher Pilehvar,\nand Roberto Navigli. 2015. NASARI: A Novel Ap-\nproach to a Semantically-Aware Representation of\nItems. In Proceedings of the 2015 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 567–577, Denver, Colorado. Association\nfor Computational Linguistics.\nDevendra Singh Chaplot and Ruslan Salakhutdinov.\n2018. Knowledge-based Word Sense Disambigua-\ntion using Topic Models. arXiv:1801.01900 [cs].\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining Text Encoders as Discriminators Rather\nThan Generators. arXiv:2003.10555 [cs].\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. arXiv:1810.04805 [cs].\nJiaju Du, Fanchao Qi, and Maosong Sun. 2019.\nUsing BERT for Word Sense Disambiguation.\narXiv:1909.08358 [cs].\nChristiane Fellbaum, editor. 1998. WordNet: An Elec-\ntronic Lexical Database . Language, Speech, and\nCommunication. MIT Press, Cambridge, Mass.\nChristian Hadiwinoto, Hwee Tou Ng, and Wee Chung\nGan. 2019. Improved Word Sense Disambiguation\nUsing Pre-Trained Contextualized Word Represen-\ntations. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5296–5305, Hong Kong, China. Association for\nComputational Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nLanguage Model Fine-tuning for Text Classiﬁcation.\narXiv:1801.06146 [cs, stat].\nLuyao Huang, Chi Sun, Xipeng Qiu, and Xuanjing\nHuang. 2019. GlossBERT: BERT for Word Sense\nDisambiguation with Gloss Knowledge. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 3507–3512, Hong\nKong, China. Association for Computational Lin-\nguistics.\nMikael Kågebäck and Hans Salomonsson. 2016. Word\nSense Disambiguation using a Bidirectional LSTM.\nProceedings of the 5th Workshop on Cognitive As-\npects of the Lexicon (CogALex - V):51–56.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. 2019. ALBERT: A Lite BERT for Self-\nsupervised Learning of Language Representations.\narXiv:1909.11942 [cs].\nYoav Levine, Barak Lenz, Or Dagan, Dan Padnos,\nOr Sharir, Shai Shalev-Shwartz, Amnon Shashua,\nand Yoav Shoham. 2019. SenseBERT: Driving\nSome Sense into BERT. arXiv:1908.05646 [cs].\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming\nHe, and Piotr Dollar. 2017. Focal Loss for Dense\nObject Detection. In 2017 IEEE International Con-\nference on Computer Vision (ICCV) , pages 2999–\n3007, Venice. IEEE.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs].\nDaniel Loureiro and Alípio Jorge. 2019. Language\nModelling Makes Sense: Propagating Represen-\ntations through WordNet for Full-Coverage Word\nSense Disambiguation. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 5682–5691, Florence, Italy.\nAssociation for Computational Linguistics.\nDaniel Loureiro, Kiamehr Rezaee, Mohammad Taher\nPilehvar, and Jose Camacho-Collados. 2020. Lan-\nguage Models and Word Sense Disambiguation: An\nOverview and Analysis. ArXiv200811608 Cs.\nFuli Luo, Tianyu Liu, Zexue He, Qiaolin Xia, Zhifang\nSui, and Baobao Chang. 2018a. Leveraging Gloss\nKnowledge in Neural Word Sense Disambiguation\nby Hierarchical Co-Attention. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1402–1411. Asso-\nciation for Computational Linguistics.\nFuli Luo, Tianyu Liu, Qiaolin Xia, Baobao Chang,\nand Zhifang Sui. 2018b. Incorporating Glosses into\nNeural Word Sense Disambiguation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2473–2482. Association for Computa-\ntional Linguistics.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013a. Efﬁcient Estimation of Word Repre-\nsentations in Vector Space. arXiv:1301.3781 [cs].\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013b. Distributed Repre-\nsentations of Words and Phrases and their Composi-\ntionality. arXiv:1310.4546 [cs, stat].\nGeorge A. Miller. 1995. WordNet: A lexical\ndatabase for English. Communications of the ACM,\n38(11):39–41.\nGeorge A. Miller, Claudia Leacock, Randee Tengi, and\nRoss T. Bunker. 1993. A semantic concordance. In\nProceedings of the Workshop on Human Language\nTechnology - HLT ’93, page 303, Princeton, New Jer-\nsey. Association for Computational Linguistics.\nRoberto Navigli. 2009. Word sense disambiguation: A\nsurvey. ACM Computing Surveys, 41(2):1–69.\nTommaso Pasini and Roberto Navigli. 2020. Train-\nO-Matic: Supervised Word Sense Disambiguation\nwith no (manual) effort. Artiﬁcial Intelligence ,\n279:103215.\nMatthew E. Peters, Mark Neumann, Robert Logan,\nRoy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge Enhanced Con-\ntextual Word Representations. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Process-\ning (EMNLP-IJCNLP) , pages 43–54, Hong Kong,\nChina. Association for Computational Linguistics.\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2019. WiC: The Word-in-Context Dataset\nfor Evaluating Context-Sensitive Meaning Represen-\ntations. In Proceedings of the 2019 Conference of\nthe North, pages 1267–1273. Association for Com-\nputational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2018. Language\nmodels are unsupervised multitask learners.\nAlessandro Raganato, Jose Camacho-Collados, and\nRoberto Navigli. 2017. Word sense disambiguation:\nA uniﬁed evaluation framework and empirical com-\nparison. In Proceedings of the 15th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Volume 1, Long Papers , pages\n99–110, Valencia, Spain. Association for Computa-\ntional Linguistics.\nTerry Ruas, Charles Henrique Porto Ferreira, William\nGrosky, Fabrício Olivetti de França, and Déb-\nora Maria Rossi de Medeiros. 2020. Enhanced\nword embeddings using multi-semantic representa-\ntion through lexical chains. Information Sciences ,\n532:16–32.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled ver-\nsion of BERT: Smaller, faster, cheaper and lighter.\narXiv:1910.01108 [cs].\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc. Https://arxiv.org/abs/1706.03762.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019a. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 3266–3280. Curran Asso-\nciates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel R. Bowman.\n2019b. GLUE: A Multi-Task Benchmark and Anal-\nysis Platform for Natural Language Understanding.\narXiv:1804.07461 [cs].\nWarren Weaver. 1955. Translation. In William N.\nLocke and Donald A. Boothe, editors, Machine\ntranslation of languages : fourteen essays, pages 15–\n23. MIT Press, Cambridge, MA.\nGregor Wiedemann, Steffen Remus, Avi Chawla, and\nChris Biemann. 2019. Does BERT Make Any\nSense? Interpretable Word Sense Disambiguation\nwith Contextualized Embeddings. ArXiv190910430\nCs.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. arXiv:1906.08237 [cs].",
  "topic": "Word-sense disambiguation",
  "concepts": [
    {
      "name": "Word-sense disambiguation",
      "score": 0.8030554056167603
    },
    {
      "name": "Computer science",
      "score": 0.8004951477050781
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6132754683494568
    },
    {
      "name": "Natural language processing",
      "score": 0.6103779077529907
    },
    {
      "name": "Language model",
      "score": 0.5292413234710693
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5081613063812256
    },
    {
      "name": "SemEval",
      "score": 0.46402791142463684
    },
    {
      "name": "Word (group theory)",
      "score": 0.45005321502685547
    },
    {
      "name": "Artificial neural network",
      "score": 0.4117226302623749
    },
    {
      "name": "Linguistics",
      "score": 0.16154614090919495
    },
    {
      "name": "WordNet",
      "score": 0.12091299891471863
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}