{
  "title": "TransReID: Transformer-based Object Re-Identification",
  "url": "https://openalex.org/W3128723389",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2751077163",
      "name": "He, Shuting",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100740392",
      "name": "Luo Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1238505444",
      "name": "Wang, Pichao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111321986",
      "name": "Wang Fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A875780830",
      "name": "Li, Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2013552087",
      "name": "JIANG WEI",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3092740872",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3045817950",
    "https://openalex.org/W3109976024",
    "https://openalex.org/W3098711604",
    "https://openalex.org/W3114896399",
    "https://openalex.org/W3100555577",
    "https://openalex.org/W2981420411",
    "https://openalex.org/W2556967412",
    "https://openalex.org/W2793904650",
    "https://openalex.org/W3004918942",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2946574625",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W3017022649",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2964163358",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3035645942",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3035186652",
    "https://openalex.org/W2997351135",
    "https://openalex.org/W3106883149",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2511791013",
    "https://openalex.org/W2980046511",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2981393440",
    "https://openalex.org/W3096285474",
    "https://openalex.org/W3035539956",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W3100506510",
    "https://openalex.org/W2973183043",
    "https://openalex.org/W2724213014",
    "https://openalex.org/W2470322391",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W2987151592",
    "https://openalex.org/W2967515867",
    "https://openalex.org/W3034303554",
    "https://openalex.org/W2755066373",
    "https://openalex.org/W2971802465",
    "https://openalex.org/W2997987796"
  ],
  "abstract": "Extracting robust feature representation is one of the key challenges in object re-identification (ReID). Although convolution neural network (CNN)-based methods have achieved great success, they only process one local neighborhood at a time and suffer from information loss on details caused by convolution and downsampling operators (e.g. pooling and strided convolution). To overcome these limitations, we propose a pure transformer-based object ReID framework named TransReID. Specifically, we first encode an image as a sequence of patches and build a transformer-based strong baseline with a few critical improvements, which achieves competitive results on several ReID benchmarks with CNN-based methods. To further enhance the robust feature learning in the context of transformers, two novel modules are carefully designed. (i) The jigsaw patch module (JPM) is proposed to rearrange the patch embeddings via shift and patch shuffle operations which generates robust features with improved discrimination ability and more diversified coverage. (ii) The side information embeddings (SIE) is introduced to mitigate feature bias towards camera/view variations by plugging in learnable embeddings to incorporate these non-visual clues. To the best of our knowledge, this is the first work to adopt a pure transformer for ReID research. Experimental results of TransReID are superior promising, which achieve state-of-the-art performance on both person and vehicle ReID benchmarks.",
  "full_text": "TransReID: Transformer-based Object Re-Identiﬁcation\nShuting He1,2*, Hao Luo1, Pichao Wang1, Fan Wang1, Hao Li1, Wei Jiang2\n1Alibaba Group, 2Zhejiang University\n{shuting he,jiangwei zju}@zju.edu.cn {michuan.lh,pichao.wang,fan.w,lihao.lh}@alibaba-inc.com\nAbstract\nExtracting robust feature representation is one of the\nkey challenges in object re-identiﬁcation (ReID). Although\nconvolution neural network (CNN)-based methods have\nachieved great success, they only process one local\nneighborhood at a time and suffer from information loss on\ndetails caused by convolution and downsampling operators\n(e.g. pooling and strided convolution). To overcome\nthese limitations, we propose a pure transformer-based\nobject ReID framework named TransReID. Speciﬁcally,\nwe ﬁrst encode an image as a sequence of patches\nand build a transformer-based strong baseline with a\nfew critical improvements, which achieves competitive\nresults on several ReID benchmarks with CNN-based\nmethods. To further enhance the robust feature learning\nin the context of transformers, two novel modules\nare carefully designed. (i) The jigsaw patch module\n(JPM) is proposed to rearrange the patch embeddings\nvia shift and patch shufﬂe operations which generates\nrobust features with improved discrimination ability and\nmore diversiﬁed coverage. (ii) The side information\nembeddings (SIE) is introduced to mitigate feature bias\ntowards camera/view variations by plugging in learnable\nembeddings to incorporate these non-visual clues. To the\nbest of our knowledge, this is the ﬁrst work to adopt a\npure transformer for ReID research. Experimental results\nof TransReID are superior promising, which achieve state-\nof-the-art performance on both person and vehicle ReID\nbenchmarks. Code is available at https://github.\ncom/heshuting555/TransReID.\n1. Introduction\nObject re-identiﬁcation (ReID) aims to associate a\nparticular object across different scenes and camera views,\nsuch as in the applications of person ReID and vehicle\nReID. Extracting robust and discriminative features is a\ncrucial component of ReID, and has been dominated by\nCNN-based methods for a long time [19, 37, 36, 44, 42].\n*This work was done when Shuting He was intern at Alibaba\nsupervised by Hao Luo and Pichao Wang.\n(a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d)\nFigure 1: Grad-CAM [34] visualization of attention maps: (a)\nOriginal images, (b) CNN-based methods, (c) CNN+attention\nmethods, (d) Transformer-based methods which captures global\ncontext information and more discriminative parts.\nID: 16\n CNN-based\nmethods\nID: 443Transformer-based\nmethods\nCNN-based\nmethods\nTransformer-based\nmethods\nFigure 2: Visualization of output feature maps for 2 hard samples\nwith similar appearances. Transformer-based methods retain\nbackpack details on output feature maps in contrast to CNN-based\nmethods, as noted in red boxes. For better visualization, input\nimages are scaled to size 1024 ×512.\nBy reviewing CNN-based methods, we ﬁnd two\nimportant issues which are not well addressed in the ﬁeld\nof object ReID. (1) Exploiting the rich structural patterns\nin a global scope is crucial for object ReID [54]. However,\nCNN-based methods mainly focus on small discriminative\nregions due to a Gaussian distribution of effective receptive\nﬁelds [29]. Recently, attention modules [54, 6, 4, 48, 21, 2]\nhave been introduced to explore long-range dependencies\n[45], but most of them are embedded in the deep layers and\ndo not solve the principle problem of CNN. Thus, attention-\nbased methods still prefer large continuous areas and are\nhard to extract multiple diversiﬁed discriminative parts (see\nFigure 1). (2) Fine-grained features with detail information\nare also important. However, the downsampling operators\n(e.g. pooling and strided convolution) of CNN reduce\nspatial resolution of output feature maps, which greatly\naffect the discrimination ability to distinguish objects with\nsimilar appearances [37, 27]. As shown in Figure 2, the\ndetails of the backpack are lost in CNN-based feature maps,\nmaking it difﬁcult to differentiate the two people.\nRecently, Vision Transformer (ViT) [8] and Data-\narXiv:2102.04378v2  [cs.CV]  26 Mar 2021\nefﬁcient image Transformers (DeiT) [40] have shown\nthat pure transformers can be as effective as CNN-\nbased methods on feature extraction for image recognition.\nWith the introduction of multi-head attention modules\nand the removal of convolution and downsampling\noperators, transformer-based models are suitable to solve\nthe aforementioned problems in CNN-based ReID for\nthe following reasons. (1) The multi-head self-attention\ncaptures long range dependencies and drives the model\nto attend diverse human-body parts than CNN models\n(e.g. thighs, shoulders, waist in Figure 1). (2) Without\ndownsampling operators, transformer can keep more\ndetailed information. For example, one can observe that\nthe difference on feature maps around backpacks (marked\nby red boxes in Figure 2) can help the model easily\ndifferentiate the two people. These advantages motivate us\nto introduce pure transformers in the object ReID.\nDespite its great advantages as discussed above,\ntransformers still need to be designed speciﬁcally for\nobject ReID to tackle the unique challenges, such as\nthe large variations ( e.g. occlusions, diversity of poses,\ncamera perspective) in images. Substantial efforts have\nbeen devoted to alleviating this challenge in CNN-based\nmethods. Among them, local part features [37, 44, 20, 49,\n28] and side information (such as cameras and viewpoints)\n[7, 61, 35, 30], have been proven to be essential and\neffective to enhance the feature robustness. Learning\npart/stripe aggregated features makes it robust against\nocclusions and misalignments [50]. However, extending\nthe rigid stripe part methods from CNN-based methods to\npure transformer-based methods may damage long-range\ndependencies due to global sequences splitting into several\nisolated subsequences. In addition, taking side information\ninto consideration, such as camera and viewpoint-speciﬁc\ninformation, an invariant feature space can be constructed\nto diminish bias brought by side information variations.\nHowever, the complex designs for side information built on\nCNN, if directly applied to transformers, cannot make full\nuse of the inherent encoding capabilities of transformers.\nAs a result, speciﬁc designed modules are inevitable and\nessential for a pure transformer to successfully handle these\nchallenges.\nTherefore, we propose a new object ReID framework\ndubbed TransReID to learn robust feature representations.\nFirstly, by making several critical adaptations, we construct\na strong baseline framework based on a pure transformer.\nSecondly, in order to expand long-range dependencies\nand enhance feature robustness, we propose a jigsaw\npatches module (JPM) by rearranging the patch embeddings\nvia shift and shufﬂe operations and re-grouping them for\nfurther feature learning. The JPM is employed on the last\nlayer of the model to extract robust features in parallel\nwith the global branch which does not include this special\noperation. Hence, the network tends to extract perturbation-\ninvariant and robust features with global context. Thirdly,\nto further enhance the learning of robust features, a side\ninformation embedding (SIE) is introduced. Instead of\nthe special and complex designs in CNN-based methods\nfor utilizing these non-visual clues, we propose a uniﬁed\nframework that effectively incorporates non-visual clues\nthrough learnable embeddings to alleviate the data bias\nbrought by cameras or viewpoints. Taking cameras for\nexample, the proposed SIE helps address the vast pairwise\nsimilarity discrepancy between inter-camera and intra-\ncamera matching (see Figure 6). SIE can also be easily\nextended to include any non-visual clues other than the ones\nwe have demonstrated.\nTo our best knowledge, we are the ﬁrst to investigate the\napplication of pure transformers in the ﬁeld of object ReID.\nThe contributions of the paper are summarised:\n• We propose a strong baseline that exploits the pure\ntransformer for ReID tasks for the ﬁrst time and\nachieve comparable performance with CNN-based\nframeworks.\n• We design a jigsaw patches module (JPM),\nconsisting of shift and patch shufﬂe operation,\nwhich facilitates perturbation-invariant and robust\nfeature representation of objects.\n• We introduce aside information embeddings(SIE) that\nencodes side information by learnable embeddings,\nand is shown to effectively mitigate the bias of learned\nfeatures.\n• The ﬁnal framework TransReID achieves state-of-\nthe-art performance on both person and vehicle\nReID benchmarks including MSMT17[46],\nMarket-1501[55], DukeMTMC-reID[33], Occluded-\nDuke[31], VeRi-776[24] and VehicleID[23].\n2. Related Work\n2.1. Object ReID\nThe studies of object ReID have been mainly focused on\nperson ReID and vehicle ReID, with most state-of-the-art\nmethods based on the CNN structure. A popular pipeline\nfor object ReID is to design suitable loss functions to train a\nCNN backbone (e.g. ResNet [14]), which is used to extract\nfeatures of images. The cross-entropy loss (ID loss) [56]\nand triplet loss [22] are most widely used in the deep ReID.\nLuo et al . [27] proposed the BNNeck to better combine\nID loss and triplet loss. Sun et al. [36] proposed a uniﬁed\nperspective for ID loss and triplet loss.\nFine-grained Features. Fine-grained features have been\nlearned to aggregate information from different part/region.\nThe ﬁne-grained parts are either automatically generated by\nroughly horizontal stripes or by semantic parsing. Methods\nlike PCB [37], MGN [44], AlignedReID++ [28], SAN\n[32], etc., divide an image into several stripes and extract\nlocal features for each stripe. Using parsing or keypoint\nestimation to align different parts or two objects has also\nbeen proven effective for both person and vehicle ReID\n[25, 30, 47, 31].\nSide Information. For images captured in a cross-\ncamera system, large variations exist in terms of pose,\norientation, illumination, resolution,etc. caused by different\ncamera setup and object viewpoints. Some works [61, 7]\nuse side information such as camera ID or viewpoint\ninformation to learn invariant features. For example,\nCamera-based Batch Normalization (CBN) [61] forces\nthe image data from different cameras to be projected\nonto the same subspace, so that the distribution gap\nbetween inter- and intra- camera pairs is largely diminished.\nViewpoint/Orientation-invariant feature learning [7, 60] is\nalso important for both person and vehicle ReID.\n2.2. Pure Transformer in Vision\nThe Transformer model is proposed in [41] to handle\nsequential data in the ﬁeld of natural language processing\n(NLP). Many studies also show its effectiveness for\ncomputer-vision tasks. Han et al . [11] and Salman et\nal. [18] have surveyed the application of the Transformer\nin the ﬁeld of computer vision.\nPure Transformer models are becoming more and more\npopular. For example, Image Processing Transformer\n(IPT) [3] takes advantage of transformers by using\nlarge scale pre-training and achieves the state-of-the-art\nperformance on several image processing tasks like super-\nresolution, denoising and de-raining. ViT [8] is proposed\nrecently which applies a pure transformer directly to\nsequences of image patches. However, ViT requires a\nlarge-scale dataset to pretrain the model. To overcome\nthis shortcoming, Touvron et al. [40] propose a framework\ncalled DeiT which introduces a teacher-student strategy\nspeciﬁc for transformers to speed up ViT training without\nthe requirement of large-scale pretraining data.\n3. Methodology\nOur object ReID framework is based on transformer-\nbased image classiﬁcation, but with several critical\nimprovements to capture robust feature (Sec. 3.1). To\nfurther boost the robust feature learning in the context\nof transformer, a jigsaw patch module (JPM) and a side\ninformation embeddings (SIE) are carefully devised in\nSec. 3.2 and Sec. 3.3. The two modules are jointly trained\nin an end-to-end manner and shown in Figure 4.\n3.1. Transformer-based strong baseline\nWe build a transformer-based strong baseline for object\nReID, following the general strong pipeline for object\nReID [27, 44]. Our method has two main stages,i.e., feature\nLinear Projection of Flattened Patches\nTransformer Layer\nTransformer Layer⋮!\n0 1 2 7 8\nPosition Embedding\nID LossTriplet LossBN\n⋯\n⋯\n* *\n*\n*Extra learnable[cls] embedding!\nFigure 3: Transformer-based strong baseline framework (a non-\noverlapping partition is shown). Output [cls] token marked with ∗\nis served as the global feature f. Inspired by [27], we introduce\nthe BNNeck after the f.\nextraction and supervision learning. As shown in Figure 3.\nGiven an image x ∈RH×W×C, where H, W, C denote\nits height, width, and number of channels, respectively, we\nsplit it into N ﬁxed-sized patches {xi\np|i = 1,2,··· ,N}.\nAn extra learnable [cls] embedding token denoted as xcls\nis prepended to the input sequences. The output [cls]\ntoken serves as a global feature representation f. Spatial\ninformation is incorporated by adding learnable position\nembeddings. Then, the input sequences fed into transformer\nlayers can be expressed as:\nZ0 = [xcls; F(x1\np); F(x2\np); ··· ; F(xN\np )] +P, (1)\nwhere Z0 represents input sequence embeddings and\nP ∈ R(N+1)×D is position embeddings. F is a\nlinear projection mapping the patches to D dimensions.\nMoreover, l transformer layers are employed to learn\nfeature representations. The limited receptive ﬁeld\nproblem of CNN-based methods is addressed, because all\ntransformer layers have a global receptive ﬁeld. There\nare also no downsampling operations, so the detailed\ninformation is preserved.\nOverlapping Patches. Pure transformer-based models\n(e.g. ViT, DeiT) split the images into non-overlapping\npatches, losing local neighboring structures around the\npatches. Instead, we use a sliding window to generate\npatches with overlapping pixels. Denoting the step size as\nS, size of the patch as P (e.g. 16) , then the shape of the\narea where two adjacent patches overlap is (P −S) ×P.\nAn input image with a resolution H×W will be split into\nN patches.\nN = NH ×NW = ⌊H+ S−P\nS ⌋×⌊ W + S−P\nS ⌋ (2)\nLinear Projection of Flattened Patches\nTransformer Layer\nTransformer Layer⋮\nTransformer Layer\n0 * 1 2 3 4 5 6 7 8Position Embedding Side Information Embedding\nTransformer LayerJigsaw Patch Module\nLoss\n*\n*** *\nCamera-1\nCamera-2\nCamera-3\nCamera-6\nCamera-5\nViewPoint\nR\nF\nL R\nLF\nLR RR\nRF\nCamera-4\nGlobalBranch\n* * * * *LossLossLossLossJigsawBranch\n!−1\n*Extra learnable[cls] embedding\n!! !\"# !\"$ !\"% !\"&\nFigure 4: Framework of proposed TransReID. Side Information Embedding (light blue) encodes non-visual information such as camera\nor viewpoint into embedding representations. It is input into transformer encoder together with patch embedding and position embedding.\nLast layer includes two independent transformer layers. One is standard to encode global feature. The other contains the Jigsaw Patch\nModule (JPM) which shufﬂes all patches and regroups them into several groups. All these groups are input into a shared transformer layer\nto learn local features. Both global feature and local features contribute to ReID loss.\nwhere ⌊·⌋is the ﬂoor function and S is set smaller than\nP. NH and NW represent the numbers of splitting patches\nin height and width, respectively. The smaller S is, the\nmore patches the image will be split into. Intuitively, more\npatches usually bring better performance with the cost of\nmore computations.\nPosition Embeddings. As the image resolution for\nReID tasks may be different from the original one in\nimage classiﬁcation, the position embedding pretrained\non ImageNet cannot be directly loaded here. Therefore,\na bilinear 2D interpolation is introduced to help handle\nany given input resolution. Similar to ViT, the position\nembedding is also learnable.\nSupervision Learning. We optimize the network by\nconstructing ID loss and triplet loss for global features.\nThe ID loss LID is the cross-entropy loss without label\nsmoothing. For a triplet set {a,p,n }, the triplet loss LT\nwith soft-margin is shown as follows:\nLT = log\n[\n1 + exp\n(\n∥fa −fp∥2\n2 −∥fa −fn∥2\n2\n)]\n(3)\n3.2. Jigsaw Patch Module\nAlthough transformer-based strong baseline can\nachieve impressive performance in object ReID, it\nutilizes information from the entire image for object\nReID. However, due to challenges like occlusions and\nmisalignments, we may only have partial observation of an\nobject. Learning ﬁne-grained local features such as striped\nfeatures has been widely used for CNN-based methods to\ntackle these challenges.\nSuppose the hidden features input to the last layer\nare denoted as Zl−1 = [ z0\nl−1; z1\nl−1,z2\nl−1,...,z N\nl−1]. To\nlearn ﬁne-grained local features, a straightforward solution\nis splitting [z1\nl−1,z2\nl−1,...,z N\nl−1] into k groups in order\nwhich concatenate the shared token z0\nl−1 and then feed k\nfeature groups into a shared transformer layer to learn k\nlocal features denoted as {fj\nl |j = 1 ,2,··· ,k}and fj\nl\nis the output token of j-th group. But it may not take\nfull advantage of global dependencies for the transformer\nbecause each local segment only considers a part of the\ncontinuous patch embeddings.\nTo address the aforementioned issues, we propose a\njigsaw patch module (JPM) to shufﬂe the patch embeddings\nand then re-group them into different parts, each of which\ncontains several random patch embeddings of an entire\nimage. In addition, extra perturbation introduced in training\nalso helps improve the robustness of object ReID model.\nInspired by ShufﬂeNet [53], the patch embeddings are\nshufﬂed via a shift operation and a patch shufﬂe operation.\nThe sequences embeddings Zl−1 are shufﬂed as follow:\n• Step1: The shift operation. The ﬁrst m patches\n(except for [cls] token) are moved to the end, i.e.\n[z1\nl−1,z2\nl−1,...,z N\nl−1] is shifted in m steps to become\n[zm+1\nl−1 ,zm+2\nl−1 ,...,z N\nl−1,z1\nl−1,z2\nl−1,...,z m\nl−1].\n• Step2: The patch shufﬂe operation. The shifted\npatches are further shufﬂed by the patch shufﬂe\noperation with kgroups. The hidden features become\n[zx1\nl−1,zx2\nl−1,...,z xN\nl−1],xi ∈[1,N].\nWith the shift and shufﬂe operation, the local feature\nfj\nl can cover patches from different body or vehicle\nparts which means that the local features hold global\ndiscriminative capability.\nAs shown in Figure 4, paralleling with the jigsaw patch,\nanother global branch which is a standard transformer\nencodes Zl−1 into Zl = [ fg; z1\nl ,z2\nl ,...,z N\nl ], where fg\nis served as the global feature of CNN-based methods.\nFinally, the global featurefg and klocal features are trained\nwith LID and LT . The overall loss is computed as follow:\nL= LID (fg)+ LT (fg)+ 1\nk\nk∑\nj=1\n(LID (fj\nl )+ LT (fj\nl )) (4)\nDuring inference, we concatenate the global feature\nand local features [fg,f1\nl ,f2\nl ,...,f k\nl ] as the ﬁnal feature\nrepresentation. Using fg only is a variation with lower\ncomputational cost and slight performance degradation.\n3.3. Side Information Embeddings\nAfter obtaining ﬁne-grained feature representations,\nfeatures are still susceptible to camera or viewpoint\nvariations. In other words, the trained model may\neasily fail to distinguish the same object from different\nperspectives due to scene-bias. Therefore, we propose a\nSide Information Embedding (SIE) to incorporate the non-\nvisual information, such as cameras or viewpoints, into\nembedding representations to learn invariant features.\nInspired by position embeddings which encode\npositional information adopting learnable embeddings, we\nplug learnable 1-D embeddings to retain side information.\nParticularly, as illustrated in Figure 4, SIE is inserted into\nthe transformer encoder together with patch embeddings\nand position embeddings. In speciﬁc, suppose there\nare NC camera IDs in total, we initialize learnable side\ninformation embeddings as SC ∈RNC×D. If camera ID of\nan image is r, then its camera embeddings can be denoted\nas SC[r]. Different from the position embeddings which\nvary between patches, camera embeddings SC[r] are the\nsame for all patches of an image. In addition, if viewpoint\nof the object is available, either by a viewpoint estimation\nalgorithm or human annotations, we can also encode the\nviewpoint label q as SV [q] for all patches of an image\nwhere SV ∈ RNV ×D and NV represents the number of\nviewpoint IDs.\nNow comes the problem about how to integrate two\ndifferent types of information. A trivial solution might be\ndirectly adding the two embeddings together like SC[r] +\nSV [q]. However, it might make the two embeddings\ncounteract each other due to redundant or adversarial\ninformation. We propose to encode the camera and\nviewpoint jointly as S(C,V ) ∈R(NC×NV )×D.\nFinally, the input sequences with camera ID r and\nviewpoint ID qare fed into transformer layers as follows:\nZ\n′\n0 = Z0 + λS(C,V )[r∗Nk + q], (5)\nwhere Z0 is the raw input sequences in Eq. 2 and λ is\na hyperparameter to balance the weight of SIE. As the\nposition embeddings are different for each patch but the\nsame across different images, and S(C,V ) are the same\nfor each patch but may have different values for different\nimages. Transformer layers are able to encode embeddings\nwith different distribution properties which can then be\nadded directly.\nHere we have only demonstrate the usage of SIE\nwith camera and viewpoint information which are\nboth categorical variables. In practice, SIE can be\nfurther extended to encode more kinds of information,\nincluding both categorical and numerical variables. In\nour experiments on different benchmarks, camera and\nviewpoint information is included wherever available.\n4. Experiments\n4.1. Datasets\nWe evaluate our proposed method on four person\nReID datasets, Market-1501 [55], DukeMTMC-reID [33],\nMSMT17 [46], Occluded-Duke [31], and two vehicle ReID\ndatasets, VeRi-776 [24] and VehicleID [23]. It is noted that,\nunlike other datasets, images in Occluded-Duke are selected\nfrom DukeMTMC-reID and the training/query/gallery set\ncontains 9%/ 100%/ 10% occluded images respectively.\nAll datasets except VehicleID provide camera ID for each\nimage, while only VeRi-776 and VehicleID dataset provide\nviewpoint labels for each image. The details of these\ndatasets are summarized in Table 1.\nDataset Object #ID #image #cam #view\nMSMT17 Person 4,101 126,441 15 -\nMarket-1501 Person 1,501 32,668 6 -\nDukeMTMC-reID Person 1,404 36,441 8 -\nOccluded-Duke Person 1,404 36,441 8 -\nVeRi-776 Vehicle 776 49,357 20 8\nVehicleID Vehicle 26,328 221,567 - 2\nTable 1: Statistics of datasets used in the paper.\n4.2. Implementation\nUnless otherwise speciﬁed, all person images are resized\nto 256×128 and all vehicle images are resized to256×256.\nThe training images are augmented with random horizontal\nﬂipping, padding, random cropping and random erasing\n[57]. The batch size is set to 64 with 4 images per ID.\nSGD optimizer is employed with a momentum of 0.9 and\nInference MSMT17 VeRi-776\nBackbone Time mAP R1 mAP R1\nResNet50 1x 51.3 75.3 76.4 95.2\nResNet101 1.48x 53.8 77.0 76.9 95.2\nResNet152 1.96x 55.6 78.4 77.1 95.9\nResNeSt50 1.86x 61.2 82.0 77.6 96.2\nResNeSt200 3.12x 63.5 83.5 77.9 96.4\nDeiT-S/16 0.97x 55.2 76.3 76.3 95.5\nDeiT-B/16 1.79x 61.4 81.9 78.4 95.9\nViT-B/16 1.79x 61.0 81.8 78.2 96.5\nViT-B/16s=14 2.14x 63.7 82.7 78.6 96.4\nViT-B/16s=12 2.81x 64.4 83.5 79.0 96.5\nTable 2: Comparison of different backbones. Inference time is\nrepresented by comparing each model to ResNet50 as only relative\ncomparison is necessary. All the experiments were carried out on\nthe same machine for fair comparison. ViT-B/16 is regarded as\nthe baseline model and abbreviated as Baseline in the rest of\nthis paper.\nthe weight decay of 1e-4. The learning rate is initialized\nas 0.008 with cosine learning rate decay. Unless otherwise\nspeciﬁed, we set m = 5,k = 4 and m = 8,k = 4 for\nperson and vehicle ReID datasets, respectively.\nAll the experiments are performed with one Nvidia Tesla\nV100 GPU using the PyTorch toolbox 1 with FP16 training\n. The initial weights of ViT are pre-trained on ImageNet-\n21K and then ﬁnetuned on ImageNet-1K, while the initial\nweights of DeiT are trained only on ImageNet-1K.\nEvaluation Protocols. Following conventions in the\nReID community, we evaluate all methods with Cumulative\nMatching Characteristic (CMC) curves and the mean\nAverage Precision (mAP).\n4.3. Results of Transform-based Baseline\nIn this section, we compare CNN-based and transformer-\nbased backbones in Table 2. To show the trade-off between\ncomputation and performance, several different backbones\nare chosen. DeiT-small, DeiT-Base, ViT-Base denoted\nas DeiT-S, DeiT-B, ViT-B, respectively. ViT-B/16 s=14\nmeans ViT-Base with patch size 16 and step size S =\n14 in overlapping patches setting. For a comprehensive\ncomparison, inference time consumption of each backbone\nis included as well.\nWe can observe a large gap in model capacity between\nthe ResNet series and DeiT/ViT. DeiT-S/16 is a little bit\nbetter in performance and speed compared to ResNet50.\nDeiT-B/16 and ViT-B/16 achieve similar performance with\nResNeSt50 [51] backbone, with less inference time than\nResNeSt50 (1.79x vs 1.86x). When we reduce the step size\nof the sliding window S, the performance of the Baseline\ncan be improved while the inference time is also increasing.\nViT-B/16s=12 is faster than ResNeSt200 (2.81x vs 3.12x)\n1http://pytorch.org\nMSMT17 VeRi-776\nBackbone #groups mAP R1 mAP R1\nBaseline - 61.0 81.8 78.2 96.5\n+JPM 1 62.9 82.5 78.6 97.0\n+JPM 2 62.8 82.1 79.1 96.4\n+JPM 4 63.6 82.5 79.2 96.8\n+JPM w/o rearrange 4 63.1 82.4 79.0 96.7\n+JPM w/o local 4 63.5 82.5 79.1 96.6\nTable 3: The ablation study of jigsaw patch module. ‘w/o\nrearrange’ means the patch features are split into parts without\nrearrange including shift and shufﬂe operation. ‘w/o local’\nmeans we evaluate the global feature without concatenating local\nfeatures.\n(a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d)\nFigure 5: Grad-CAM visualization of attention maps. (a) Input\nimages, (b) Baseline, (c) JPM w/o rearrange, (d) JPM.\nand performs slightly better than ResNeSt200 on ReID\nbenchmarks. Therefore, ViT-B/16 s=12 achieves better\nspeed-accuracy trade-off than ResNeSt200. In addition,\nwe believe that DeiT/ViT still have lots of room for\nimprovement in terms of computational efﬁciency.\n4.4. Ablation Study of JPM\nThe effectiveness of the proposed JPM module is\nvalidated in Table 3. JPM provides +2.6% mAP and +1.0%\nmAP improvements compared to baseline on MSMT17 and\nVeRi-776, respectively. Increasing the number of groups\nk can improve the performance while slightly increasing\ninference time. In our experiment, k = 4 is a choice\nto trade off speed and performance. Comparing JPM and\nJPM w/o rearrange, we can observe that the shift and\nshufﬂe operation helps the model learn more discriminative\nfeatures with +0.5% mAP and +0.2% mAP improvements\non MSMT17 and VeRi-776, respectively. It is also observed\nthat, if only the global feature fg is used in inference\nstage (still trained with full JPM), the performance (denoted\nas “w/o local”) is nearly comparable with the version of\nfull set of features, which suggests us to only use the\nglobal feature as an efﬁcient variation with lower storage\ncost and computational cost in the inference stage. The\nattention maps visualized in Figure 5 show that JPM with\nthe rearrange operation can help the model learn more\nglobal context information and more discriminative parts,\nwhich makes the model more robust to perturbations.\nMSMT17 VeRi-776\nMethod Camera Viewpoint mAP R1 mAP R1\nBaseline 61.0 81.8 78.2 96.5\n+ SC[r] √ 62.4 81.9 78.7 97.1\n+ SV [q] √ - - 78.5 96.9\n+ S(C,V )\n√ √ - - 79.6 96.9\nTable 4: Ablation study of SIE. Since the person ReID datasets\ndo not provide viewpoint annotations, viewpoint information can\nonly be encoded in VeRi-776.\nPairwise Distance (w/o SIE)\n0.0\n0.02\n0.05\n0.08\n0.1\n0.12Probability\nintra_camera\ninter_camera\n0.2 0.4 0.6 0.8 1.0\nPairwise Distance (w/ SIE)\n0.0\n0.02\n0.05\n0.08\n0.1\n0.12Probability\nintra_camera\ninter_camera\n(a) Distance of camera pairs.\nPairwise Distance (w/o SIE)\n0.0\n0.02\n0.05\n0.08\n0.1\n0.12Probability\nintra_viewpoint\ninter_viewpoint\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nPairwise Distance (w/ SIE)\n0.0\n0.02\n0.05\n0.08\n0.1\n0.12Probability\nintra_viewpoint\ninter_viewpoint (b) Distance of viewpoint pairs.\nFigure 6: We visualize the distance distributions of different\ncamera pairs and viewpoint pairs on VeRi-776. (a) inter-camera\nand intra-camera distance distribution. (b) inter-viewpoint and\nintra-viewpoint distance distribution.\n0.5 1.0 1.5 2.0 2.5 3.0\nValue of \n60.0\n60.5\n61.0\n61.5\n62.0\n62.5\n63.0\n63.5mAP(%)\n60.9\n62.2\n62.4\n63.0\n60.8\n60.5\nPerformance on MSMT17\n80.0\n80.5\n81.0\n81.5\n82.0\n82.5\nRanK-1(%)\n81.2\n81.5\n81.9\n82.4\n80.7\n80.4\nmAP\nRank-1\n(a) MSMT17\n0.5 1.0 1.5 2.0 2.5 3.0\nValue of \n77.5\n78.0\n78.5\n79.0\n79.5\n80.0mAP(%)\n78.2\n79.0\n79.3\n79.4\n79.9\n79.6\nPerformance on VeRi-776\n95.6\n95.8\n96.0\n96.2\n96.4\n96.6\n96.8\n97.0\nRanK-1(%)\n96.1\n96.4\n96.1\n96.6\n96.3\n96.6\nmAP\nRank-1 (b) VeRi-776\nFigure 7: Impact of the hyper-parameter λ.\n4.5. Ablation Study of SIE\nPerformance Analysis. In Table 4, we evaluate the\neffectiveness of the SIE on MSMT17 and VeRi-776.\nMSMT17 does not provide viewpoint annotations, so the\nresults of SIE which only encode camera information are\nshown for MSMT17. VeRi-776 not only have a camera\nID of each image, but is also annotated with 8 different\nviewpoints according to vehicle orientation. Therefore, the\nresults are shown with SIE encoding various combinations\nof camera ID and/or viewpoints information.\nWhen SIE encodes only the camera IDs of images,\nthe model gains 1.4% mAP and 0.1% rank-1 accuracy\nimprovements on MSMT17. Similar conclusion can be\nmade on VeRi-776. Baseline obtains 78.5% mAP when SIE\nencodes viewpoint information. The accuracy increases to\n79.6% mAP when both camera IDs and viewpoint labels\nare encoded at the same time. If the encoding is changed\nto SC[r] + SV [q], which is sub-optimal as discussed in\nSection 3.3, we can only achieve 78.3% mAP on VeRi-776.\nTherefore, the proposedS(C,V ) is a better encoding manner.\nVisualization of Distance Distribution. As shown\nin Figure 6, the distribution gaps with cameras and\nviewpoints variations are obvious in Figure 6a and\nFigure 6b, respectively. When we introduce the SIE\nmodule into Baseline, the distribution gaps between inter-\ncamera/viewpoint and intra-camera/viewpoint are reduced,\nwhich shows that the SIE module weakens the negative\neffect of the scene-bias caused by various cameras and\nviewpoints.\nAblation Study of λ. We analyze the inﬂuence of\nweight λof the SIE module on the performance in Figure 7.\nWhen λ = 0, Baseline achieves 61.0% mAP and 78.2%\nmAP on MSMT17 and VeRi-776, respectively. With λ\nincreasing, the mAP is improved to 63.0% mAP ( λ =\n2.0 for MSMT17) and 79.9% mAP ( λ = 2.5 for VeRi-\n776), which means the SIE module now is beneﬁcial for\nlearning invariant features. Continuing to increase λ, the\nperformance is degraded because the weights for feature\nembedding and the position embedding are weakened.\n4.6. Ablation Study of TransReID\nFinally, we evaluate the beneﬁts of introducing JPM and\nSIE in Table 5. For the Baseline, JPM and SIE improve\nthe performance by +2.6%/+1.0% mAP and +1.4%/+1.4%\nmAP on MSMT17/VeRi-776, respectively. With these\ntwo modules used together, TransReID achieves 64.9%\n(+3.9%) mAP and 80.6% (+2.4%) mAP on MSMT17 and\nVeRi-776, respectively. The experimental results show the\neffectiveness of our proposed JPM, SIE, and the overall\nframework.\nMSMT17 VeRi-776\nMethod JPM SIE mAP R1 mAP R1\nBaseline × × 61.0 81.8 78.2 96.5√ × 63.6 82.5 79.2 96.8\n× √ 62.4 81.9 79.6 96.9\nTransReID √ √ 64.9 83.3 80.6 96.9\nTable 5: The ablation study of TransReID.\n4.7. Comparison with State-of-the-Art Methods\nIn Table 6, our TransReID is compared with state-of-\nthe-art methods on six benchmarks including person ReID,\noccluded ReID and vehicle ReID.\nPerson ReID. On MSMT17 and DukeMTMC-reID,\nTransReID∗ (DeiT-B/16) outperforms the previous state-of-\nthe-art methods by a large margin (+5.5%/+2.1% mAP). On\nMarket-1501, TransReID∗ (256×128) achieves comparable\nperformance with state-of-the-art methods especially on\nMSMT17 Market1501 DukeMTMC Occluded-Duke VeRi-776 VehicleID\nBackbone Method Size mAP R1 mAP R1 mAP R1 mAP R1 Method mAP R1 R1 R5\nCNN\nCBN c⃝[61] 256 ×128 42.9 72.8 77.3 91.3 67.3 82.5 - - PRReID[13] 72.5 93.3 72.6 88.6\nOSNet [58] 256 ×128 52.9 78.7 84.9 94.8 73.5 88.6 - - SAN[32] 72.5 93.3 79.7 94.3\nMGN [44] 384 ×128 52.1 76.9 86.9 95.7 78.4 88.7 - - UMTS [16] 75.9 95.8 80.9 87.0\nRGA-SC [54] 256 ×128 57.5 80.3 88.4 96.1 - - - - V ANetv⃝[7] 66.3 89.8 83.3 96.0\nSAN [17] 256 ×128 55.7 79.2 88.0 96.1 75.7 87.9 - - SPAN v⃝[5] 68.9 94.0 - -\nSCSN [6] 384 ×128 58.5 83.8 88.5 95.7 79.0 91.0 - - PGAN [52] 79.3 96.5 78.0 93.2\nABDNet [4] 384 ×128 60.8 82.3 88.3 95.6 78.6 89.0 - - PVEN v⃝[30] 79.5 95.6 84.7 97.0\nPGFA [31] 256 ×128 - - 76.8 91.2 65.5 82.6 37.3 51.4 SA VER [19] 79.6 96.4 79.9 95.2\nHOReID [43] 256 ×128 - - 84.9 94.2 75.6 86.9 43.8 55.1 CFVMNet [38] 77.1 95.3 81.4 94.1\nISP [59] 256 ×128 - - 88.6 95.3 80.0 89.6 52.3 62.8 GLAMOR[39] 80.3 96.5 78.6 93.6\nDeiT-B/16\nBaseline 256 ×128 61.4 81.9 86.6 94.4 78.9 89.3 53.1 60.6 Baseline 78.4 95.9 83.1 96.8\nTransReID c⃝ 256×128 63.9 82.7 88.0 94.7 81.2 90.1 55.6 62.8 TransReID v⃝ 80.6 96.8 84.6 97.4\nTransReID c⃝ 384×128 65.5 83.5 88.1 94.9 81.3 90.2 - - TransReID b⃝ 81.2 96.8 - -\nTransReID∗ c⃝ 256×128 66.2 84.3 88.4 95.0 81.9 91.1 58.1 66.4 TransReID∗ v⃝ 81.4 96.8 85.2 97.6\nTransReID∗ c⃝ 384×128 66.3 84.5 88.5 95.1 82.1 91.1 - - TransReID∗ b⃝ 82.3 97.1 - -\nViT-B/16\nBaseline 256 ×128 61.0 81.8 86.8 94.7 79.3 88.8 53.1 60.5 Baseline 78.2 96.5 82.3 96.1\nTransReID c⃝ 256×128 64.9 83.3 88.2 95.0 80.6 89.6 55.7 64.2 TransReID v⃝ 79.6 97.0 83.6 97.1\nTransReID c⃝ 384×128 66.6 84.6 88.8 95.0 81.8 90.4 - - TransReID b⃝ 80.6 96.9 - -\nTransReID∗ c⃝ 256×128 67.4 85.3 88.9 95.2 82.0 90.7 59.2 66.4 TransReID∗ v⃝ 80.5 96.8 85.2 97.5\nTransReID∗ c⃝ 384×128 69.4 86.2 89.5 95.2 82.6 90.7 - - TransReID∗ b⃝ 82.0 97.1 - -\nTable 6: Comparison with state-of-the-art methods. DukeMTMC denotes the DukeMTMC-reID benchmark. The star * in the superscript\nmeans the backbone is with a sliding-window setting. Results are shown for person ReID datasets (left) and vehicle ReID datasets (right).\nOnly the small subset of VehicleID is used in this paper. c⃝ and v⃝ indicate the methods are using camera IDs and viewpoint labels,\nrespectively. b⃝means both are used. Viewpoint and camera information are used wherever available. Best results for previous methods\nand best of our methods are labeled in bold.\nmAP. Our method also shows superiority when compared\nwith methods which also integrate camera information like\nCBN [61].\nOccluded ReID. ISP implicitly uses human body\nsemantic information through iterative clustering and\nHOReID introduces external pose models to align body\nparts. TransReID (DeiT-B/16) achieves 55.6% mAP with a\nlarge margin improvement (at least +3.3% mAP) compared\nto aforementioned methods, without requiring any semantic\nand pose information to align body parts, which shows\nthe ability of TransReID to generate robust feature\nrepresentations. Furthermore, TransReID ∗ improves the\nperformance to 58.1% mAP with the help of overlapping\npatches.\nVehicle ReID. On VeRi-776, TransReID∗ (DeiT-B/16)\nreaches 82.3% mAP surpassing GLAMOR by 2.0% mAP.\nWhen only using viewpoint annotations, TransReID ∗\nstill outperforms V ANet and SA VER on both VeRi-\n776 and VehicleID. Our method achieves state-of-the-art\nperformance about 85.2% Rank-1 accuracy on VehicleID.\nDeiT vs ViT vs CNN. TransReID∗ (DeiT-B/16) reaches\ncompetitive performance with existing methods under\na fair comparison (ImageNet-1K pre-training). Extra\nresults of our methods with ViT-B/16 are also reported\nin Table 6 for further comparison. DeiT-B/16 achieves\nsimilar performance with ViT-B/16 for shorter image patch\nsequences. When the number of input patches is increasing,\nViT-B/16 reaches better performance than DeiT-B/16,\nwhich shows ImageNet-21K pre-training provides ViT-\nB/16 better generalization capability. Although CNN-based\nmethods mainly report performance with the ResNet50\nbackbone, they may include multiple branches, attention\nmodules, semantic models, or other modules that increase\ncomputational consumption. We have conducted a fair\ncomparison on inference speed between TransReID ∗ and\nMGN [44] on the same computing hardware. Compared\nwith MGN, TransReID* is 4.8% faster in speed. Therefore,\nTransReID* can achieve more promising performance\nunder comparable computation to most of CNN-based\nmethods.\n5. Conclusion\nIn this paper, we investigate a pure transformer\nframework for the object ReID task, and propose two\nnovel modules, i.e., jigsaw patch module (JPM) and\nside information embedding (SIE). The ﬁnal framework\nTransReID outperforms all other state-of-the-art methods\nby a large margin on several popular person/vehicle ReID\ndatasets including MSMT17, Market-1501, DukeMTMC-\nreID, Occluded-Duke, VeRi-776 and VehicleID. Based on\nthe promising results achieved by TransReID, we believe\nthe transformer has great potential to be further explored\nfor ReID tasks. Based on the rich experience gained from\nCNN-based methods, it is in prospect that more efﬁcient\ntransformer-based networks can be designed with better\nrepresentation power and less computational cost.\nReferences\n[1] Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew\nZhai, and Dmitry Kislyuk. Toward transformer-based object\ndetection. arXiv preprint arXiv:2012.09958, 2020. 12\n[2] Binghui Chen, Weihong Deng, Jiani Hu, Jiani Hu, and Jiani\nHu. Mixed high-order attention network for person re-\nidentiﬁcation. In ICCV, 2019. 1\n[3] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. CVPR,\n2021. 3\n[4] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang\nChen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd-\nnet: Attentive but diverse person re-identiﬁcation. In ICCV,\n2019. 1, 8\n[5] Tsai-Shien Chen, Chih-Ting Liu, Chih-Wei Wu, and Shao-\nYi Chien. Orientation-aware vehicle re-identiﬁcation with\nsemantics-guided part attention network. In ECCV, pages\n330–346. Springer, 2020. 8\n[6] Xuesong Chen, Canmiao Fu, Yong Zhao, Feng Zheng,\nJingkuan Song, Rongrong Ji, and Yi Yang. Salience-guided\ncascaded suppression network for person re-identiﬁcation. In\nCVPR, June 2020. 1, 8\n[7] Ruihang Chu, Yifan Sun, Yadong Li, Zheng Liu, Chi Zhang,\nand Yichen Wei. Vehicle re-identiﬁcation with viewpoint-\naware metric learning. In ICCV, pages 8282–8291, 2019. 2,\n3, 8\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, and Xiaohua et al. Zhai. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ICLR, 2021. 1, 3\n[9] Angela Fan, Edouard Grave, and Armand Joulin. Reducing\ntransformer depth on demand with structured dropout. arXiv\npreprint arXiv:1909.11556, 2019. 11\n[10] Angela Fan, Pierre Stock, Benjamin Graham, Edouard\nGrave, R ´emi Gribonval, Herv ´e J ´egou, and Armand\nJoulin. Training with quantization noise for extreme model\ncompression. arXiv e-prints, pages arXiv–2004, 2020. 11\n[11] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,\nJianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing\nXu, Yixing Xu, et al. A survey on visual transformer. arXiv\npreprint arXiv:2012.12556, 2020. 3\n[12] Boris Hanin and David Rolnick. How to start training:\nThe effect of initialization and architecture. arXiv preprint\narXiv:1803.01719, 2018. 11\n[13] Bing He, Jia Li, Yifan Zhao, and Yonghong Tian. Part-\nregularized near-duplicate vehicle re-identiﬁcation. In\nCVPR, pages 3997–4005, 2019. 8\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\npages 770–778, 2016. 2\n[15] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. InECCV,\npages 646–661. Springer, 2016. 11\n[16] Xin Jin, Cuiling Lan, Wenjun Zeng, and Zhibo Chen.\nUncertainty-aware multi-shot knowledge distillation for\nimage-based object re-identiﬁcation. In AAAI, volume 34,\npages 11165–11172, 2020. 8\n[17] Xin Jin, Cuiling Lan, Wenjun Zeng, Guoqiang Wei, and\nZhibo Chen. Semantics-aligned representation learning\nfor person re-identiﬁcation. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , volume 34, pages\n11173–11180, 2020. 8\n[18] Salman Khan, Muzammal Naseer, Munawar Hayat,\nSyed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. arXiv preprint\narXiv:2101.01169, 2021. 3\n[19] Pirazh Khorramshahi, Neehar Peri, Jun-cheng Chen, and\nRama Chellappa. The devil is in the details: Self-supervised\nattention for vehicle re-identiﬁcation. In ECCV, pages 369–\n386. Springer, 2020. 1, 8\n[20] Dangwei Li, Xiaotang Chen, Zhang Zhang, and Kaiqi\nHuang. Learning deep context-aware features over body and\nlatent parts for person re-identiﬁcation. InCVPR, pages 384–\n393, 2017. 2\n[21] Wei Li, Xiatian Zhu, Xiatian Gong, Shaogang, and Shaogang\nGong. Harmonious attention network for person re-\nidentiﬁcation. In CVPR, pages 2285–2294, 2018. 1\n[22] Hao Liu, Jiashi Feng, Meibin Qi, Jianguo Jiang, and\nShuicheng Yan. End-to-end comparative attention networks\nfor person re-identiﬁcation. IEEE TIP , 26(7):3492–3506,\n2017. 2\n[23] Hongye Liu, Yonghong Tian, Yaowei Yang, Lu Pang, and\nTiejun Huang. Deep relative distance learning: Tell the\ndifference between similar vehicles. In CVPR, pages 2167–\n2175, 2016. 2, 5\n[24] Xinchen Liu, Wu Liu, Huadong Ma, and Huiyuan Fu. Large-\nscale vehicle re-identiﬁcation in urban surveillance videos.\nIn ICME, pages 1–6. IEEE, 2016. 2, 5\n[25] Xinchen Liu, Wu Liu, Jinkai Zheng, Chenggang Yan, and\nTao Mei. Beyond the parts: Learning multi-view cross-part\ncorrelation for vehicle re-identiﬁcation. In ACMMM, pages\n907–915, 2020. 3\n[26] Ilya Loshchilov and Frank Hutter. Fixing weight decay\nregularization in adam. 2018. 11\n[27] Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei\nJiang. Bag of tricks and a strong baseline for deep person\nre-identiﬁcation. In CVPRW, pages 0–0, 2019. 1, 2, 3, 11\n[28] Hao Luo, Wei Jiang, Xuan Zhang, Xing Fan, Jingjing Qian,\nand Chi Zhang. Alignedreid++: Dynamically matching\nlocal information for person re-identiﬁcation. Pattern\nRecognition, 94:53–61, 2019. 2\n[29] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard\nZemel. Understanding the effective receptive ﬁeld in deep\nconvolutional neural networks. In NeurIPS, pages 4905–\n4913, 2016. 1\n[30] Dechao Meng, Liang Li, Xuejing Liu, Yadong Li, Shijie\nYang, Zheng-Jun Zha, Xingyu Gao, Shuhui Wang, and\nQingming Huang. Parsing-based view-aware embedding\nnetwork for vehicle re-identiﬁcation. In CVPR, pages 7103–\n7112, 2020. 2, 3, 8\n[31] Jiaxu Miao, Yu Wu, Ping Liu, Yuhang Ding, and Yi\nYang. Pose-guided feature alignment for occluded person\nre-identiﬁcation. In ICCV, pages 542–551, 2019. 2, 3, 5, 8\n[32] Jingjing Qian, Wei Jiang, Hao Luo, and Hongyan Yu. Stripe-\nbased and attribute-aware network: A two-branch deep\nmodel for vehicle re-identiﬁcation. Measurement Science\nand Technology, 31(9):095401, 2020. 3, 8\n[33] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,\nand Carlo Tomasi. Performance measures and a data set for\nmulti-target, multi-camera tracking. In ECCV, pages 17–35.\nSpringer, 2016. 2, 5\n[34] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam: Visual explanations from deep networks via\ngradient-based localization. In ICCV, pages 618–626, 2017.\n1, 13\n[35] Xiaoxiao Sun and Liang Zheng. Dissecting person re-\nidentiﬁcation from the viewpoint of viewpoint. In CVPR,\nJune 2019. 2\n[36] Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang,\nLiang Zheng, Zhongdao Wang, and Yichen Wei. Circle\nloss: A uniﬁed perspective of pair similarity optimization.\nIn CVPR, pages 6398–6407, 2020. 1, 2\n[37] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\nWang. Beyond part models: Person retrieval with reﬁned\npart pooling (and a strong convolutional baseline). InECCV,\npages 480–496, 2018. 1, 2\n[38] Ziruo Sun, Xiushan Nie, Xiaoming Xi, and Yilong\nYin. Cfvmnet: A multi-branch network for vehicle re-\nidentiﬁcation based on common ﬁeld of view. In ACMMM,\npages 3523–3531, 2020. 8\n[39] Abhijit Suprem and Calton Pu. Looking glamorous: Vehicle\nre-id in heterogeneous cameras networks with global and\nlocal attention. arXiv preprint arXiv:2002.02256, 2020. 8\n[40] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through\nattention. arXiv preprint arXiv:2012.12877, 2020. 2, 3\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. InNeurIPS, pages\n6000–6010, 2017. 3\n[42] Guangcong Wang, Jian-Huang Lai, Wenqi Liang, and\nGuangrun Wang. Smoothing adversarial domain attack\nand p-memory reconsolidation for cross-domain person re-\nidentiﬁcation. In CVPR, pages 10568–10577, 2020. 1\n[43] Guan’an Wang, Shuo Yang, Huanyu Liu, Zhicheng Wang,\nYang Yang, Shuliang Wang, Gang Yu, Erjin Zhou, and Jian\nSun. High-order information matters: Learning relation and\ntopology for occluded person re-identiﬁcation. In CVPR,\npages 6449–6458, 2020. 8\n[44] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and\nXi Zhou. Learning discriminative features with multiple\ngranularities for person re-identiﬁcation. In ACMMM, pages\n274–282, 2018. 1, 2, 3, 8\n[45] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and\nKaiming He. Non-local neural networks. In CVPR, pages\n7794–7803, 2018. 1\n[46] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\nPerson transfer gan to bridge domain gap for person re-\nidentiﬁcation. In CVPR, pages 79–88, 2018. 2, 5\n[47] Longhui Wei, Shiliang Zhang, Hantao Yao, Wen Gao, and Qi\nTian. Glad: Global-local-alignment descriptor for pedestrian\nretrieval. In Proceedings of the 25th ACM international\nconference on Multimedia, pages 420–428. ACM, 2017. 3\n[48] Bryan (Ning) Xia, Yuan Gong, Yizhe Zhang, and Christian\nPoellabauer. Second-order non-local attention networks for\nperson re-identiﬁcation. In ICCV, October 2019. 1\n[49] Hantao Yao, Shiliang Zhang, Richang Hong, Yongdong\nZhang, Changsheng Xu, and Qi Tian. Deep representation\nlearning with part loss for person re-identiﬁcation.IEEE TIP,\n28(6):2860–2871, 2019. 2\n[50] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling\nShao, and Steven CH Hoi. Deep learning for person re-\nidentiﬁcation: A survey and outlook. IEEE TPAMI, 2021.\n2\n[51] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi\nZhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R\nManmatha, et al. Resnest: Split-attention networks. arXiv\npreprint arXiv:2004.08955, 2020. 6\n[52] Xinyu Zhang, Rufeng Zhang, Jiewei Cao, Dong Gong,\nMingyu You, and Chunhua Shen. Part-guided attention\nlearning for vehicle re-identiﬁcation. IEEE TITS, 2020. 8\n[53] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian\nSun. Shufﬂenet: An extremely efﬁcient convolutional neural\nnetwork for mobile devices. In CVPR, pages 6848–6856,\n2018. 4\n[54] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Xin Jin, and\nZhibo Chen. Relation-aware global attention for person re-\nidentiﬁcation. In CVPR, June 2020. 1, 8\n[55] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang,\nJingdong Wang, and Qi Tian. Scalable person re-\nidentiﬁcation: A benchmark. In ICCV, pages 1116–1124,\n2015. 2, 5\n[56] Zhedong Zheng, Liang Zheng, and Yi Yang. A\ndiscriminatively learned cnn embedding for person\nreidentiﬁcation. ACM TOMM, 14(1):13, 2018. 2\n[57] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In AAAI,\nvolume 34, pages 13001–13008, 2020. 5\n[58] Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, and\nTao Xiang. Omni-scale feature learning for person re-\nidentiﬁcation. In ICCV, pages 3702–3712, 2019. 8\n[59] Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao\nWang. Identity-guided human semantic parsing for person\nre-identiﬁcation. ECCV, 2020. 8\n[60] Zhihui Zhu, Xinyang Jiang, Feng Zheng, Xiaowei Guo,\nFeiyue Huang, Xing Sun, and Weishi Zheng. Aware loss\nwith angular regularization for person re-identiﬁcation. In\nAAAI, volume 34, pages 13114–13121, 2020. 3\n[61] Zijie Zhuang, Longhui Wei, Lingxi Xie, Tianyu Zhang,\nHengheng Zhang, Haozhe Wu, Haizhou Ai, and Qi Tian.\nRethinking the distribution gap of person re-identiﬁcation\nwith camera-based batch normalization. In ECCV, pages\n140–157. Springer, 2020. 2, 3, 8\nAppendix\nA. More Experimental Results\nA.1. Study on Transformer-based Strong Baseline\nA transformer-based strong baseline with a few critical\nimprovements has been introduced in Section 3.1 of the\nmain paper. In this section, hyper-parameters and the\nsettings for training such a baseline model will be analyzed\nin detail. Ablation studies are shown in Table 7 for\nperformance on MSMT17 and Veri-776 with different\nvariations of the training settings.\nInitialization and hyper-parameters . For our\nexperiments, we initialize the pure transformer with ViT or\nDeiT ImageNet pre-trained weights and we initialize the\nweights for the SIE with a truncated normal distribution\n[12]. Compared with ViT, DeiT is more sensitive to hyper-\nparameter settings. For the training of DeiT, we use a\nlearning rate of 0.05 on MSMT17 and a high random\nerasing probability with 0.8 on each dataset to avoid\noverﬁtting. Other hyper-parameters settings are the same\nwith ViT.\nOptimizer. Transformers are sensitive to the choice of\nthe optimizer. Directly applying Adam optimizer with the\nhyper-parameters commonly used in ReID community [27]\nto transformer-based models will cause a signiﬁcant drop in\nperformance. AdamW [26] is a commonly used optimizer\nfor training transformer-based models, with much better\nperformance compared with Adam. The best results are\nactually achieved by SGD in our experiments.\nNetwork Conﬁguration . Position embeddings\nincorporate crucial spatial information which provides a\nsigniﬁcant boost in performance and is one of the key\ningredients of our proposed training procedure. Without\nthe position embeddings, the performance decreases by\n38.6% mAP and 10.2% mAP on MSMT17 and VeRi-776,\nrespectively.\nIntroducing stochastic depth [15] can boost the mAP\nperformance by about 1%, and it has also been proved\nto facilitate the convergence of transformer, especially for\nthose deep ones [9, 10]. Regarding other regularization\nmethods, adding either drop out or attention drop out will\nresult in performance drop. In our experiments, we set all\nthe probability of regularization methods as 0.1.\nLoss Function. Different choices of loss functions have\nbeen compared in the bottom section of Table 7. The soft\nversion of triplet loss provides 0.7% mAP improvement\non MSMT17 compared with the regular triplet loss.\nIntroducing label smoothing is harmful to performance,\neven though it has been a widely adopted trick. Therefore,\nthe best combination for loss functions is soft triplet loss\nand cross entropy loss without label smoothing.\nA.2. More Ablation Studies of JPM and SIE\nIn the main paper, we have demonstrated the\neffectiveness of using JPM and SIE based on the Baseline\n(ViT-B/16). More results about JPM and SIE are shown\nin Table 8 and Table 9 respectively, with the Baseline\nViT-B/16s=12, which is supposed to have better feature\nrepresentation ability and higher performance than ViT-\nB/16. From Table 8, we observe that: (1) The proposed\nJPM performs better with the rearrange schemes, indicating\nthat the shift and patch shufﬂe operation help the model\nlearn more discriminative features which are robust against\nperturbations. (2) The JPM module provides a consistent\nperformance improvement over the baselines, no matter\nthe baseline is ViT-B/16 or the stronger ViT-B/16 s=12,\ndemonstrating the effectiveness of the proposed JPM.\nSimilar conclusions can be made from Table 9. (1) We\nmake better use of the viewpoint and camera information so\nthat they are complementary with each other and combining\nthem leads to the best performance. (2) Introducing SIE\nMethod OPT PE SP DO ADO STL LS MSMT17 VeRi-776\nmAP R1 mAP R1\nViT-B/16 Baseline SGD \u0013 \u0013 \u0017 \u0017 \u0013 \u0017 61.0 81.8 78.2 96.5\nOptimizer Adam \u0013 \u0013 \u0017 \u0017 \u0013 \u0017 37.4 (-24.6) 60.2 (-21.6) 65.8 (-12.4) 91.7 (-4.8)\nAdamW \u0013 \u0013 \u0017 \u0017 \u0013 \u0017 60.6 (-0.4) 81.7 (-0.1) 78.0 (-0.2) 96.5 (-0.0)\nNetwork\nConﬁguration\nSGD \u0017 \u0013 \u0017 \u0017 \u0013 \u0017 22.4 (-38.6) 38.3 (-43.5) 68.0 (-10.2) 92.8 (-3.7)\nSGD \u0013 \u0017 \u0017 \u0017 \u0013 \u0017 59.9 (-1.1) 80.2 (-1.6) 77.2 (-1.0) 96.1 (-0.4)\nSGD \u0013 \u0013 \u0013 \u0017 \u0013 \u0017 60.0 (-1.0) 80.7 (-1.1) 78.0 (-0.2) 96.3 (-0.2)\nSGD \u0013 \u0013 \u0017 \u0013 \u0013 \u0017 58.0 (-3.0) 78.8 (-3.0) 74.3 (-3.9) 94.9 (-1.6)\nLoss Function SGD \u0013 \u0013 \u0017 \u0017 \u0017 \u0017 60.3 (-0.7) 81.3 (-0.5) 77.5 (-0.7) 95.6 (-0.9)\nSGD \u0013 \u0013 \u0017 \u0017 \u0013 \u0013 59.8 (-1.2) 80.4 (-1.4) 77.4 (-0.8) 96.5 (-0.0)\nTable 7: Ablation study about training settings on MSMT17 and VeRi-776. The ﬁrst row corresponds to the default conﬁguration employed\nby our transformer-based strong baseline (ViT-B/16 as default backbones). The symbols \u0013 and \u0017 indicate that the corresponding setting\nis included or excluded, respectively. mAP(%) and R1(%) accuracy scores are reported. The abbreviations OPT, PE, SP, DO, ADO, STL,\nLS denote Optimizer, Position Embedding, Stochastic Depth [15], Drop Out, Attention Drop Out, Soft Triplet Loss, Label Smoothing,\nrespectively.\nMSMT17 VeRi-776\nBackbone #groups mAP R1 mAP R1\nBaseline (ViT-B/16) - 61.0 81.8 78.2 96.5\n+JPM 1 62.9 82.5 78.6 97.0\n+JPM 2 62.8 82.1 79.1 96.4\n+JPM 4 63.6 82.5 79.2 96.8\n+JPM w/o rearrange 4 63.1 82.4 79.0 96.7\n+JPM w/o local 4 63.5 82.5 79.1 96.6\nBaseline (ViT-B/16s=12) - 64.4 83.5 79.0 96.5\n+JPM 4 66.5 84.8 80.0 97.0\n+JPM w/o rearrange 4 66.1 84.5 79.6 96.7\n+JPM w/o local 4 66.3 84.5 79.8 96.8\nTable 8: Detailed ablation study of jigsaw patch module\n(JPM). ‘w/o rearrange’ means the patch sequences are split into\nsubsequences without rearrangement. ‘w/o local’ means we\nevaluate the global feature without concatenating local features.\nMSMT17 VeRi-776\nMethod Camera View mAP R1 mAP R1\nBaseline\n(ViT-B/16)\n\u0017 \u0017 61.0 81.8 78.2 96.5\n\u0013 \u0017 62.4 81.9 78.7 97.1\n\u0017 \u0013 - - 78.5 96.9\n\u0013 \u0013 - - 79.6 96.9\nBaseline\n(ViT-B/16s=12)\n\u0017 \u0017 64.4 83.5 79.0 96.5\n\u0013 \u0017 65.9 84.1 79.4 96.4\n\u0017 \u0013 - - 79.3 97.0\n\u0013 \u0013 - - 80.3 96.9\nTable 9: Detailed ablation study of side information embeddings\n(SIE). Experiments of viewpoint information are only conducted\non VeRi-776 as the person ReID datasets do not provide viewpoint\nannotations. The symbols \u0013 and \u0017 indicate that the corresponding\ninformation is included or excluded.\nprovides consistent improvement over the baselines of\neither ViT-B/16 or ViT-B/16s=12.\nB. Analysis on Rearranging Patches in JPM\nAlthough transformers can capture the global\ninformation in the image very well, a patch token still\nhas a strong correlation with the corresponding patch.\nViT-FRCNN [1] shows that the output embeddings of\nthe last layer can be reshaped as a spatial feature map\nthat includes location information. In other words, if we\ndirectly divide the original patch embeddings into k parts,\neach part may only consider a part of the continuous patch\nembeddings. Therefore, to better capture the long-range\ndependencies, we rearrange the patch embeddings and then\nre-group them into different parts, each of which contains\nseveral random patch embeddings of an entire image. In\nthis way, the JPM module help to learn robust features\nwith improved discrimination ability and more diversiﬁed\ncoverage.\nTo verify the above point, we visualize the learned\nattention of local features [f1\nl ,f2\nl ,...,f k\nl ] (k = 4 in our\ncases) by JPM module in Figure 8. Brighter region means\nPart 1 Part 2 Part 3 Part 4 Part 1 Part 2 Part 3 Part 4\n(b) w/o rearrange(a) Original (c) w/ rearrange\nFigure 8: Visualization of the learned attention masks for local\nfeatures by JPM module. Higher weight results in higher\nbrightness of the region. Note that we visualize the learned\nattention weights which are averaged among attention heads in the\nlast layer. Faces in the images are masked for anonymization.\nhigher corresponding weights. Several observations can be\nmade from Figure 8: (1) The attention learned by the “JPM\nw/o rearrange” tends to focus on limited receptive ﬁelds\n(i.e. the range of the corresponding patch sequences) due\nto global sequences being split into several isolated sub-\nsequences. For example, “Part 1” mainly pays attention\nto the head of a person, and “Part 4” is mainly focused\naround the bottom area. (2) In contrast, “JPM w/ rearrange”\nis able to capture long-range dependencies and each part\nhas attention responses across the whole image because it\nis forced to extend its scope to the whole image through the\nrearranging operation. (3) According to the superior ReID\nperformance and the intuitive visualization of rearranging\neffect, JPM is proved to not only capture more details at\nﬁner granularities but also learn robust and discriminative\nrepresentations in the global context.\nC. More Visualization of Attention Maps\nIn the main paper, we use Grad-CAM to visualize the\ngradient responses of our schemes, CNN-based methods,\nand CNN+attention methods. Following the similar setup,\nFigure 9 shows more visualization results, with the similar\nconclusion that transformer-based methods capture global\ncontext information and more discriminative parts, which\nare further enhanced in our proposed TransReID for better\nperformance.\n(a) (b) (c) (d) (e) (f)\n (a) (b) (c) (d) (e) (f)\nFigure 9: Grad-CAM [34] visualization of attention maps. (a) Original images, (b) CNN-based methods, (c) CNN+Attention methods, (d)\nTransformer-based baseline, (e) TransReID w/o rearrange, (f) TransReID. Faces in the images are masked for anonymization.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7492472529411316
    },
    {
      "name": "Transformer",
      "score": 0.6814868450164795
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5700150728225708
    },
    {
      "name": "Feature learning",
      "score": 0.5442782044410706
    },
    {
      "name": "Pooling",
      "score": 0.5244529247283936
    },
    {
      "name": "ENCODE",
      "score": 0.5208666324615479
    },
    {
      "name": "Convolutional neural network",
      "score": 0.45337387919425964
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4468395709991455
    },
    {
      "name": "Engineering",
      "score": 0.09837937355041504
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}