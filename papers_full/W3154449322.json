{
  "title": "Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees",
  "url": "https://openalex.org/W3154449322",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3127335762",
      "name": "Jiangang Bai",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2096802215",
      "name": "Yujing Wang",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2100327471",
      "name": "Yiren Chen",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2177001444",
      "name": "Yaming Yang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1972075961",
      "name": "Jing Bai",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2097531951",
      "name": "Jing Yu",
      "affiliations": [
        "Institute of Information Engineering",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2703420231",
      "name": "Yunhai Tong",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2979748886",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3008282111",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3153543512",
    "https://openalex.org/W1508977358",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2250861254",
    "https://openalex.org/W4289373464",
    "https://openalex.org/W2562220579",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2798560962",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2962982474",
    "https://openalex.org/W2995259046",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964217331",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2987266335",
    "https://openalex.org/W2963411763",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2097606805",
    "https://openalex.org/W2923014074"
  ],
  "abstract": "Jiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu, Yunhai Tong. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021.",
  "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3011‚Äì3020\nApril 19 - 23, 2021. ¬©2021 Association for Computational Linguistics\n3011\nSyntax-BERT: Improving Pre-trained Transformers with Syntax Trees\nJiangang Bai1,‚àó\n, Yujing Wang1,2,‚Ä†\n, Yiren Chen1, Yaming Yang2\nJing Bai2, Jing Yu3, Yunhai Tong1\n1Peking University, Beijing, China\n2Microsoft Research Asia, Beijing, China\n3Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China\n{pku bjg,yujwang,yrchen92,yhtong}@pku.edu.cn\n{yujwang,yayaming,jbai}@microsoft.com\nyujing02@iie.ac.cn\nAbstract\nPre-trained language models like BERT\nachieve superior performances in various NLP\ntasks without explicit consideration of syn-\ntactic information. Meanwhile, syntactic in-\nformation has been proved to be crucial for\nthe success of NLP applications. However,\nhow to incorporate the syntax trees effectively\nand efÔ¨Åciently into pre-trained Transformers\nis still unsettled. In this paper, we address\nthis problem by proposing a novel framework\nnamed Syntax-BERT. This framework works\nin a plug-and-play mode and is applicable\nto an arbitrary pre-trained checkpoint based\non Transformer architecture. Experiments\non various datasets of natural language un-\nderstanding verify the effectiveness of syntax\ntrees and achieve consistent improvement over\nmultiple pre-trained models, including BERT,\nRoBERTa, and T5.\n1 Introduction\nPre-trained language models like BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2020) and\nT5 (Raffel et al., 2019) become popular in recent\nyears and achieve outstanding performances in var-\nious NLP benchmarks. These models often choose\na Transformer architecture largely owing to its at-\ntractive scalability. Studies (Hewitt and Manning,\n2019; Jawahar et al., 2019) have shown that a pre-\ntrained transformer is able to capture certain syn-\ntactic information implicitly by learning from suf-\nÔ¨Åcient examples. However, there is still a big gap\nbetween the syntactic structures implicitly learned\nand the golden syntax trees created by human ex-\nperts.\nOn the other hand, syntax tree is a useful\nprior for NLP-oriented neural networks (Kiper-\nwasser and Ballesteros, 2018). For example,\n‚àóThe work was done when the author visited Microsoft\nResearch Asia.\n‚Ä†Corresponding Author\nTree-LSTM (Tai et al., 2015) extends the sequen-\ntial architecture of LSTM to a tree-structured\nnetwork. Linguistically-informed self-attention\n(LISA) (Strubell et al., 2018) proposes a multi-\ntask learning framework for semantic role labeling,\nwhich incorporates syntactic knowledge into Trans-\nformer by training one attention head to be attended\nto its parent in a syntax tree. In addition, Nguyen\net al. (2020) integrate tree-structured attention in\nTransformer with hierarchical accumulation guided\nby the syntax tree.\nAlthough there are numerous works on syntax-\nenhanced LSTM and Transformer models, none\nof the previous works have addressed the useful-\nness of syntax-trees in the pre-training context. It is\nstraight-forward to ask: it is still helpful to leverage\nsyntax trees explicitly in the pre-training context?\nIf the answer is yes, can we ingest syntax trees\ninto a pre-trained checkpoint efÔ¨Åciently without\ntraining from scratch for a speciÔ¨Åc downstream ap-\nplication? This is an appealing feature in practice\nbecause pre-training from scratch is a huge waste\nof energy and time.\nIn this paper, we propose Syntax-BERT to tackle\nthe raised questions. Unlike a standard BERT,\nwhich has a complete self-attention typology, we\ndecompose the self-attention network into multi-\nple sub-networks according to the tree structure.\nEach sub-network encapsulates one relationship\nfrom the syntax trees, including ancestor, offspring,\nand sibling relationships with different hops. All\nsub-networks share the same parameters with the\npre-trained network, so they can be learned collab-\noratively and inherited directly from an existing\ncheckpoint. To select the task-oriented relation-\nships automatically, we further adopt a topical at-\ntention layer to calculate the relative importance\nof syntactic representations generated by different\nsub-networks. Finally, the customized represen-\ntation is calculated by weighted summation of all\n3012\nsub-networks.\nWe conduct extensive experiments to verify the\neffectiveness of Syntax-BERT framework on vari-\nous NLP tasks, including sentiment classiÔ¨Åcation,\nnatural language inference, and other tasks in the\nGLUE benchmark. Experimental results show that\nSyntax-BERT outperforms vanilla BERT models\nand LISA-enhanced models consistently with mul-\ntiple model backbones, including BERT, RoBERTa,\nand T5. SpeciÔ¨Åcally, it boosts the overall score\nof GLUE benchmark from 86.3 to 86.8 for T5-\nLarge (Raffel et al., 2019) checkpoint, which is\nalready trained on a huge amount of data. This\nimprovement is convincing since only a few extra\nparameters are introduced to the model.\nOur major contributionsare as follows:\n‚Ä¢To the best of our knowledge, Syntax-BERT\nis one of the Ô¨Årst attempts to demonstrate the\nusefulness of syntax trees in pre-trained lan-\nguage models. It works efÔ¨Åciently in a plug-\nand-play fashion for an existing checkpoint\nwithout the need for pre-training from scratch.\n‚Ä¢To integrate syntax trees into pre-trained\nTransformers, we propose a novel method that\ndecomposes self-attention networks into dif-\nferent aspects and adopts topical attention for\ncustomized aggregation. As shown in the ab-\nlation study, this design beneÔ¨Åts from syntac-\ntic structures effectively while retaining pre-\ntrained knowledge to the largest extent.\n‚Ä¢Syntax-BERT shows consistent improvement\nover multiple pre-trained backbone models\nwith comparable model capacities. It can\nbe combined with LISA to achieve further\nenhancement, indicating that these two algo-\nrithms are complementary to each other.\n2 Related Work\n2.1 Pre-trained language models\nRecently, pre-trained language models have re-\nceived signiÔ¨Åcant attention from the natural lan-\nguage processing community. Many excellent pre-\ntrained language models are proposed, such as\nBERT, RoBERTa and T5. Transformer (Vaswani\net al., 2017) is a typical architecture for pre-training\nlanguage models, which is based on the self-\nattention mechanism and is much more efÔ¨Åcient\nthan RNNs. BERT (Devlin et al., 2019) is a repre-\nsentative work that trains a large language model\non the free text and then Ô¨Åne-tunes it on speciÔ¨Åc\ndownstream tasks separately. BERT is pre-trained\non two auxiliary pre-training tasks, Masked Lan-\nguage Model (MLM) and Next Sentence Predic-\ntion (NSP). RoBERTa (Liu et al., 2020) is an im-\nproved variant of BERT which utilizes dynamic\nmasks. In RoBERTa, the NSP task is cancelled,\nbut the full-sentence mechanism is considered. At\nthe same time, the size of RoBERTa‚Äôs training data\n(‚àº160GB) is ten times the size of BERT‚Äôs train-\ning data. Moreover, Raffel et al. (2019) explore\nthe effectiveness of multiple transfer learning tech-\nniques and apply these insights at scale to create a\nnew model T5 (Text to Text Transfer Transformer).\nWith T5, they reform all NLP tasks into a uniÔ¨Åed\ntext-to-text format where the input and output are\nalways text strings. This is in contrast to BERT-\nstyle models that only output either a class label or\nan input span.\n2.2 Syntax-aware models\nSyntax is a crucial prior for NLP-oriented neural\nnetwork models. Along this direction, a range of\ninteresting approaches have been proposed, like\nTree-LSTM (Tai et al., 2015), PECNN (Yang\net al., 2016), SDP-LSTM (Xu et al., 2015), Su-\npervised Treebank Conversion (Jiang et al., 2018),\nPRPN (Shen et al., 2018), and ON-LSTM (Shen\net al., 2019).\nRecent works also investigate syntactic knowl-\nedge in the context of Transformer, which are more\nrelated to this paper. For instance, Syntax-Infused\nTransformer (Sundararaman et al., 2019) feeds the\nextra syntactic features into the Transformer mod-\nels explicitly, but it only considers simple syntac-\ntic features and does not provide a generic so-\nlution to incorporate tree-structured knowledge.\nStrubell et al. (2018) present a neural network\nmodel named LISA (Linguistically-Informed Self-\nAttention) that learns multi-head self-attention in\na multi-task learning framework consisting of de-\npendency parsing, part-of-speech tagging, predi-\ncate detection, and semantic role labeling. They\nalso show that golden syntax trees can dramati-\ncally improve the performance of semantic role\nlabeling. Moreover, Nguyen et al. (2020) propose\na hierarchical accumulation approach to encode\nparse tree structures into self-attention mechanism.\nHowever, these approaches are designed for train-\ning a Transformer from scratch without beneÔ¨Åting\nfrom pre-trained checkpoints. Instead, our frame-\n3013\nMasked Self-Attention\nSyntax Tree Gen.\nSyntax Masks Gen.Parent masksChildren masksSibling masks\nToken EmbeddingE1 E2 En‚Ä¶‚Ä¶Q K V\nMASKs\nTopical Attention\nK VùííùíïùíÇùíîùíå\nInput Sentencetoken1 token2 tokenn‚Ä¶‚Ä¶\nMatMulElement-wise ProductionScale\nSoftMax\nMatMul\nQK\nMASK\nV\nDownstream Tasks\nSyntactic attention layer\nV\nDN N\nJohn  hit   the   ball.\n0100000000010100\nFigure 1: The Overall Architecture of Syntax-BERT. Note that the leftmost part shows an example of syntax tree and its\ncorresponding parent syntax mask (d = 1).\nwork works in a plug-and-play mode and retains\nthe pre-trained knowledge as much as possible for\ndownstream applications. Concurrent to our work,\nSachan et al. (2020) investigate popular strategies\nfor incorporating dependency structures into pre-\ntrained language models, revealing essential design\ndecisions are necessary for strong performances. In\naddition, Hewitt and Manning (2019) design two\nsets of probes to determine whether the embedded\nspace can be converted into syntactic information\nspace through a linear transformation. It gives the\nevaluation metrics to examine how much syntactic\ninformation is included in a model.\n3 Syntax-BERT\nSyntax-BERT is a variant of pre-trained Trans-\nformer models, which changes the Ô¨Çow of infor-\nmation in a standard BERT network via a syntax-\naware self-attention mechanism. First, the overall\narchitecture of Syntax-BERT is presented in Sec-\ntion 3.1. Then, we introduce the construction of\nsyntax trees and corresponding masks in Section\n3.2. The details of syntactic attention layers will be\ndescribed in Section 3.3.\n3.1 Architecture\nAs mentioned earlier, one limitation of vanilla\nTransformer is that it simply uses a fully-connected\ntopology of tokens in the pre-trained self-attention\nlayer. Although the self-attention mechanism au-\ntomatically calculates a relevance score for each\ntoken pair, it still suffers from optimization and\nover-Ô¨Åtting problems, especially when the training\ndata is limited. Some previous works have tried\nto induce syntactic structure explicitly into self-\nattention. For instance, in Linguistically-Informed\nSelf-Attention (LISA) (Strubell et al., 2018), syn-\ntax tree is incorporated by training one attention\nhead to be attended to the parent of each token.\nHowever, other structural features such as siblings\nand children are discarded in the model. Moreover,\nit can not distinguish the usefulness of multiple syn-\ntactic features while largely retain the knowledge\nfrom a pre-trained checkpoint.\nSyntax-BERT is designed to incorporate gram-\nmatical and syntactic knowledge as prior in the\nself-attention layer and support Ô¨Åne-grained adap-\ntation for different downstream tasks. SpeciÔ¨Åcally,\nit generates a bunch of sub-networks based on\nsparse masks reÔ¨Çecting different relationships and\ndistances of tokens in a syntax tree. Intuitively,\nthe tokens inside a sub-network often semantically\nrelated to each other, resulting in a topical repre-\nsentation. Therefore, we can adopt a topical at-\ntention layer to aggregate task-oriented representa-\ntions from different sub-networks.\nThe overall architecture of Syntax-BERT is il-\nlustrated in Figure 1. As shown in the left part\nof this Ô¨Ågure, we generate syntax masks for the\ninput sentence in two steps. First, the input sen-\ntence is converted into the corresponding tree struc-\nture by a syntax parser. Second, we extract a\nbunch of syntax-related masks according to differ-\nent features incorporated in the syntax tree. Next,\nthe sentence is embedded similar to a standard\nBERT (token + positional + Ô¨Åeld embedding) and\nserved as input to the self-attention layer. Each self-\nattention layer in the Syntax-BERT is composed\nof two kinds of attention modules, namely masked\nself-attention and topical attention. In a masked\n3014\nself-attention module, we apply syntactic masks\nto the fully-connected topology, generating topi-\ncal sub-networks that share parameters with each\nother. Furthermore, the representations from differ-\nent sub-networks are aggregated through a topical\nattention module so that the task-related knowledge\ncan be distilled to the Ô¨Ånal representation vector.\n3.2 Masks induced by syntax tree\nGenerically, a syntax tree is an ordered, rooted tree\nthat represents the syntactic structure of a sentence\naccording to some context-free grammar. It can be\ndeÔ¨Åned abstractly as T = {R,N,E}, where Ris\nthe root of syntax tree, Nand Estands for node\nset and edge set respectively. The most commonly-\nused syntax trees are constituency trees (Chen and\nManning, 2014) and dependency trees (Zhu et al.,\n2013), and we use both of them in our experiments\nunless notiÔ¨Åed.\nTo utilize the knowledge in a syntax tree ef-\nfectively, we introduce syntax-based sub-network\ntypologies in the self-attention layer to guide\nthe model. Each sub-network shares the same\nmodel parameters with the global pre-trained self-\nattention layer, while each sub-network reÔ¨Çects a\nspeciÔ¨Åc aspect of the syntax tree. This procedure\ncan be easily implemented by multiple masks ap-\nplied to the complete graph topology.\nWithout loss of generality, we design three cat-\negories of masks reÔ¨Çecting different aspects of a\ntree structure, namely parent mask, child mask,\nand sibling mask. For a pairwise inference task\nthat contains a pair of sentences as input, we also\napply another mask, i.e., pairwise mask, to cap-\nture the inter-sentence attention. Moreover, the\ndistances between nodes (tokens) in a tree incor-\nporate semantic relatedness. Starting from a node\nA, along the edges of a syntax tree, the minimum\nnumber of edges required to reach another node\nBcan be regarded as the distance between Aand\nB, written as dist(A,B). We create Ô¨Åne-grained\nmasks according to the distance between two nodes\nto enable customized aggregation of task-oriented\nknowledge.\nMathematically, a mask can be denoted by M ‚àà\n{0,1}n√ón, where Mi,j ‚àà{0,1}denotes if there is\na connection from token ito token j, and nis the\nnumber of tokens in the current sentence.\nIn the parent maskwith certain distance d, we\nhave Mp\ni,j,d = 1 if and only if the node i is the\nparent or ancestor of node j, at the same time\ndist(i,j) = d. Otherwise, the value will be set\nas zero.\nIn the child maskwith certain distance d, we\nhave Mc\ni,j,d = 1if and only if the nodeiis the child\nor offspring of nodej, at the same timedist(i,j) =\nd. In other words, node jis the parent or ancestor\nof node i.\nIn the sibling maskwith certain distance d, we\nhave Ms\ni,j,d = 1 if and only if we can Ô¨Ånd their\nlowest common ancestor and dist(i,j) =d. Note\nthat if two nodes are in the same sentence, we can\nalways Ô¨Ånd the lowest common ancestor, but the\nvalue should be zero if the corresponding nodes\ncome from different sentences (in pairwise infer-\nence tasks).\nThe pairwise maskcaptures the interaction of\nmultiple sentences in a pairwise inference task. We\nhave Mpair\ni,j = 1 if and only if both node iand j\nare from different sentences. we do not consider\nthe distances in-between as the nodes are from\ndifferent trees.\n3.3 Syntactic attention layers\nA block of Syntax-BERT contains two kinds of at-\ntention modules: masked self-attention and topical\nattention. The operations in a masked self-attention\nare similar to a standard self-attention except that\nwe have sparse network connections as deÔ¨Åned\nin the masks. The masked self-attention can be\nformulated as an element-wise multiplication of\ndot-product attention and its corresponding mask:\nMaskAtt(Q,K,V,M ) =œÉ(QK‚ä§‚äôM‚àö\nd\n)V\nAi,j = MaskAtt(HWQ\ni ,HW K\ni ,HW V\ni ,Mj)\nHj = (A1,j ‚äïA2,j ‚äï...‚äïAk,j)WO,j ‚àà1,...,m\n(1)\nwhere Q, K, V represent for the matrix of query,\nkey and value respectively, which can be calculated\nby the input representation H. M represents for\nthe matrix of syntax mask and ‚äôdenotes an op-\nerator for element-wise production; œÉ stands for\nsoftmax operator; Ai,j denotes the attention-based\nrepresentation obtained by the ith head and jth\nsub-network; WQ\ni , WK\ni and WV\ni represent for the\nparameters for linear projections; Mj denotes the\nmask for the jthsub-network; and Hj denotes the\ncorresponding output representation.\nThe output representations from different sub-\nnetworks embody knowledge from different syntac-\ntic and semantic aspects. Therefore, we leverage\n3015\nTask #Train #Dev #Test #Class\nSST-1 8,544 1,101 2,210 5\nSST-2 6,920 873 1,822 2\nSNLI 549,367 9,842 9,824 3\nMNLI 392,703 9,816/9,833 9,797/9,848 3\nCoLA 8,551 1,042 1,064 2\nMRPC 3,669 409 1,726 2\nSTS-B 5,750 1,501 1,380 *\nQQP 363,871 40,432 390,965 2\nQNLI 104,744 5,464 5,464 2\nRTE 2,491 278 3,001 2\nWNLI 636 72 147 2\nTable 1: Dataset Statistics: the character ‚Äò/‚Äô seperate MNLI-m\nand MNLI-mm, ‚Äò*‚Äô represents for the regression task.\nanother attention layer, named topical attention to\nperform a Ô¨Åne-grained aggregation of these rep-\nresentations. The most distinct part of a topical\nattention is that qtask is a trainable query vector\nfor task-speciÔ¨Åc embedding. Thus, the topical at-\ntention layer is able to emphasize task-oriented\nknowledge captured by numerous sub-networks.\nTopicAtt(qtask,K,V ) =œÉ(qtaskK‚ä§\n‚àö\nd\n)V\nHO = TopicAtt(qtask,HW K,HW V )\n(2)\nwhere d denotes the size of hidden dimension,\nqtask ‚ààR1√ód is a task-related learnable query em-\nbedding vector; œÉstands for the softmax operator;\nH = (H1,H2,...,H m)‚ä§ ‚ààRm√ód is the output\nrepresentation collected by multiple sub-networks;\nWK and WV are parameters in the feed-forward\noperations; and HO stands for the Ô¨Ånal text repre-\nsentation.\n4 Experiments\nFirst, we run experiments on the Stanford Senti-\nment Treebank (SST) dataset (Socher et al., 2013)\nin Section 4.1, which is designed to study the syn-\ntactic and semantic compositionality of sentiment\nclassiÔ¨Åcation. Second, in Section 4.2, we evalu-\nate the performance of Syntax-BERT on two natu-\nral language inference datasets: SNLI and MNLI.\nThen, more empirical results on the GLUE bench-\nmark and a comprehensive ablation study will be\npresented in Section 4.3 and 4.4 respectively. At\nlast, we present the analysis of the structural probes\nin Section 4.5.\nThe statistics of all datasets adopted in this pa-\nper are summarized in Table 1. For each dataset,\nwe optimize the hyper-parameters of Syntax-BERT\nthrough grid search on the validation data. De-\ntailed settings can be found in the appendix. In\nour experiments, we set the maximum value of\ndist(A,B) in a syntax tree as 15 and use both de-\npendency and constituency trees unless speciÔ¨Åed.\nThus, we have totally 90 (15 √ó3 √ó2) sub-networks\nfor single-sentence tasks and 92 ((15 √ó3 + 1)√ó2)\nsub-networks for pairwise inference tasks. We\nadopt Transformer (Vaswani et al., 2017), BERT-\nBase, BERT-Large (Devlin et al., 2019), RoBERTa-\nBase, RoBERTa-Large (Liu et al., 2020) and T5-\nLarge (Raffel et al., 2019) as backbone models and\nperform syntax-aware Ô¨Åne-tuning on them. We\nalso compare with LISA (Linguistically-Informed\nSelf-Attention) (Strubell et al., 2018), a state-of-\nthe-art method that incorporates linguistic knowl-\nedge into self-attention operations. SpeciÔ¨Åcally,\nLISA (Strubell et al., 2018) adopt an additional\nattention head to learn the syntactic dependency\nin the tree structure, and the parameters of this\nadditional head are initialized randomly.\n4.1 Stanford Sentiment Treebank\nThe SST dataset contains more than 10,000 sen-\ntences collected from movie reviews from the rot-\ntentomatoes.com website. The corresponding con-\nstituency trees for review sentences are contained\nin the dataset, where each intermediate node in a\ntree represents a phrase. All phrases are labeled\nto one of Ô¨Åve Ô¨Åne-grained categories of sentiment\npolarity. SST-2 is a binary classiÔ¨Åcation task. We\nfollow a common setting that utilizes all phrases\nwith lengths larger than 3 as training samples, and\nonly full sentences will be used in the validation\nand testing phase. The hyper parameters for each\nmodel are selected by grid search and listed in the\nappendix. We compare Syntax-BERT with vanilla\nbaselines and LISA-enhanced models. The results\nare listed in Table 2. As shown in the table, our\nmodel achieves 4.8 and 4.9 absolute points im-\nprovements respectively against the vanilla Trans-\nformer with comparable parameter size. By com-\nbining our framework with LISA, the results can be\nfurther boosted obviously. This indicates that our\nmechanism is somewhat complementary to LISA.\nLISA captures the syntactic information through\nan additional attention head, whereas our frame-\nwork incorporates syntactic dependencies into orig-\ninal pre-trained attention heads and increases the\nsparsity of the network. We can see that Syntax-\nTransformer + LISA performs the best among all\nsettings, and similar trends are demonstrated on the\nBERT-Base and BERT-Large checkpoints.\n3016\nModel SST-1 SST-2\nTransformer 48.4 86.2\nLISA-Transformer 52.2 89.1\nSyntax-Transformer (Ours) 52.7 90.1\nSyntax-Transformer + LISA (Ours)53.2 91.1\nBERT-Base 53.7 93.5\nLISA-BERT-Base 54.2 93.7\nSyntax-BERT-Base (Ours) 54.4 94.0\nSyntax-BERT-Base + LISA (Ours)54.5 94.4\nBERT-Large 54.8 94.9\nLISA-BERT-Large 55.0 95.9\nSyntax-BERT-Large (Ours) 55.3 96.1\nSyntax-BERT-Large + LISA (Ours)55.5 96.4\nTable 2: Comparison with SOTA models on SST dataset.\nModel SNLI MNLI\nTransformer 84.9 71.4\nLISA-Transformer 86.1 73.7\nSyntax-Transformer (Ours) 86.8 74.1\nSyntax-Transformer + LISA (Ours)87.0 74.5\nBERT-Base 87.0 84.3\nLISA-BERT-base 87.4 84.7\nSyntax-BERT-Base (Ours) 87.7 84.9\nSyntax-BERT-Base + LISA (Ours)87.8 84.9\nBERT-Large 88.4 86.8\nLISA-BERT-Large 88.8 86.8\nSyntax-BERT-Large (Ours) 88.9 86.7\nSyntax-BERT-Large + LISA (Ours)89.0 87.0\nTable 3: Comparison with SOTA models on NLI datasets.\n4.2 Natural Language Inference\nThe Natural Language Inference (NLI) task re-\nquires a model to identify the semantic relationship\n(entailment, contradiction, or neutral) between a\npremise sentence and the corresponding hypothesis\nsentence. In our experiments, we use two datasets\nfor evaluation, namely SNLI (Bowman et al., 2015)\nand MNLI (Williams et al., 2018). We utilize the\nStanford parser (Klein and Manning, 2003) to gen-\nerate constituency and dependency trees for the in-\nput sentences. The MNLI dataset has two separate\nsets for evaluation (matched set and mismatched\nset), and we report the average evaluation score of\nthese two sets.\nThe test accuracies on SNLI and MNLI datasets\nare shown in Table 3. The syntactic prior informa-\ntion helps the Transformer to perform much better\non the NLI tasks. The accuracies on the SNLI\nand MNLI datasets have been improved by 1.9 and\n2.7, respectively, by applying our framework to\na vanilla Transformer. The LISA-enhanced trans-\nformer can also outperform vanilla transformer on\nNLI tasks, but the accuracy improvement is not\nas large as Syntax-Transformer. When the back-\nbone model is BERT-Base or BERT-Large, con-\nsistent conclusions can be drawn from the experi-\nmental results. It is worth noting that the syntax-\nenhanced models for BERT-large do not show\nmuch gain based on the vanilla counterparts. This\nmay because BERT-Large already captures sufÔ¨Å-\ncient knowledge for NLI tasks in the pre-training\nphase.\n4.3 GLUE Benchmark\nThe GLUE benchmark (Wang et al., 2019) offers a\ncollection of tools for evaluating the performance\nof models. It contains single-sentence classiÔ¨Åca-\ntion tasks (CoLA and SST-2), similarity and para-\nphrase tasks (MRPC, QQP, and STS-B), as well as\npairwise inference tasks (MNLI, RTE, and QNLI).\nWe use the default train/dev/test split. The hyper-\nparameters are chosen based on the validation set\n(refer to the appendix for details). After the model\nis trained, we make predictions on the test data and\nsend the results to GLUE online evaluation service1\nto obtain Ô¨Ånal evaluation scores.\nThe evaluation scores on all datasets in GLUE\nbenchmark are illustrated in Table 4. The perfor-\nmances of BERT-Base, BERT-Large, RoBERTa-\nBase, RoBERTa-Large, and T5-Large are repro-\nduced using the ofÔ¨Åcial checkpoint provided by\nrespective authors. We only use self-contained con-\nstituency trees for the SST-2 dataset while other\ndatasets are processed by Stanford parser 2 to ex-\ntract both dependency trees and constituency trees.\nFor a fair comparison, all results of baseline models\nare reproduced by our own, which are close to the\nreported results.\nAs shown in the table, syntax-enhanced models\nalways outperform corresponding baseline mod-\nels. Most notably, Syntax-RoBERTa-Base achieves\nan average GLUE score of 82.1, lifting 1.3 scores\nfrom a standard RoBERTa-Base with the same set-\nting. This is impressive as only a few extra pa-\nrameters are introduced to the baseline model. Par-\nticularly, the improvements on CoLA and SST-2\ndatasets are fairly large, showing the generalization\ncapability of Syntax-BERT and Syntax-RoBERTa\non smaller datasets. Even on T5-Large, which is\ntrained on more data and holds more advanced per-\nformances, our approach still outperforms the base\nmodel marginally (statistically signiÔ¨Åcant under\n4.3 p-value using paired t-test). We can see that\nmore training data will improve the generalization\n1https://gluebenchmark.com\n2https://nlp.stanford.edu/software/lex-parser.shtml\n3017\nModel Avg CoLA SST-2 MRPC STS-B QQP MNLI-m/-mm QNLI RTE WNLI\nTransformer 66.1 31.3 83.9 81.7/68.6 73.6/70.2 65.6/84.4 72.3/71.4 80.3 58.0 65.1\nSyntax-Transformer (Ours) 68.8 36.6 86.4 81.8/69.0 74.0/72.3 65.5/84.9 72.5/71.2 81.0 56.7 65.1\nBERT-Base 77.4 51.7 93.5 87.2/82.1 86.7/85.4 71.1/89.0 84.3/83.7 90.4 67.2 65.1\nSyntax-BERT-Base (Ours) 78.5 54.1 94.0 89.2/86.0 88.1/86.7 72.0/89.6 84.9/84.6 91.1 68.9 65.1\nBERT-Large 80.5 60.5 94.9 89.3/85.4 87.6/86.5 72.1/89.3 86.8/85.9 92.7 70.1 65.1\nSyntax-BERT-Large (Ours) 81.8 61.9 96.1 92.0/88.9 89.6/88.5 72.4/89.5 86.7/86.6 92.8 74.7 65.1\nRoBERTa-Base 80.8 57.1 95.4 90.8/89.3 88.0/87.4 72.5/89.6 86.3/86.2 92.2 73.8 65.1\nSyntax-RoBERTa-Base (Ours) 82.1 63.3 96.1 91.4/88.5 89.9/88.3 73.5/88.5 87.8/85.7 94.3 81.2 65.1\nRoBERTa-Large 83.9 63.8 96.3 91.0/89.4 72.9/90.2 72.7/90.1 89.5/89.7 94.2 84.2 65.1\nSyntax-RoBERTa-Large (Ours) 84.7 64.3 96.9 92.5/90.1 91.6/91.4 73.1/89.8 90.2/90.0 94.5 85.0 65.1\nT5-Large 86.3 61.1 96.1 92.2/88.7 90.0/89.2 74.1/89.9 89.7/89.6 94.8 87.0 65.1\nSyntax-T5-Large (Ours) 86.8 62.9 97.2 92.7/90.6 91.3/90.7 74.3/90.1 91.2/90.5 95.2 89.6 65.1\nTable 4: Comparison with state-of-the-art models without pre-training on GLUE benchmark.\nModel SST-2 CoLA STS-B\nBERT-Large 94.9 60.5 87.6/86.5\nSyntax-BERT-Large 96.1 61.9 89.6/88.5\nw/o topical attention 95.1 61.6 88.4/87.3\nw/o syntax trees 95.0 60.5 88.0/87.1\nw/o dependency trees 95.6 61.4 88.7/88.1\nw/o constituency trees 95.9 61.4 87.6/86.8\nw/o parent masks 95.5 60.9 88.7/87.2\nw/o child masks 95.3 61.2 88.3/86.8\nw/o sibling masks 95.8 61.5 89.0/88.1\nw/o pairwise masks - - 88.8/87.9\nTable 5: Ablation study\ncapability of the model and compensate for the lack\nof syntax priors. On the other hand, syntactic in-\nformation is useful in most cases, especially when\ntraining data or computation power is limited.\n4.4 Ablation Study\nFor a comprehensive understanding of the model\ndesign, we conduct ablation study with the fol-\nlowing settings. (1) without topical attention: the\ntopical attention layer is removed, and a simple\nsummation layer is replaced instead; (2) with-\nout syntax tree: all the syntactic masks gener-\nated by the syntax trees are replaced by randomly\ngenerated masks, while the parameter size of\nthe model remains unchanged; (3) without con-\nstituency/dependency tree: only one kind of syntax\ntree is used in the model; (4)without parent / child /\nsibling / pairwise masks: the corresponding masks\nare removed in the implementation.\nAs shown in Table 5, all datasets beneÔ¨Åt from\nthe usage of syntactic information. Generally, par-\nent/child masks are of more importance than the\nsibling masks. Moreover, the topical attention\nlayer is crucial to the performance of Syntax-BERT\nmodel, indicating the advantage of decomposing\nself-attention into different sub-networks and per-\nModel UUAS Spr.\nBERT-Base (Devlin et al., 2019) 79.8 0.85\nSyntax-BERT-Base 81.1 0.88\nBERT-Large (Devlin et al., 2019) 82.5 0.86\nSyntax-BERT-Large 83.4 0.90\nRoBERTa-Large (Liu et al., 2020) 83.2 0.88\nSyntax-RoBERTa-Large 84.6 0.93\nTable 6: The results of using Structural Probe to test whether\ndifferent models contain syntactic information or not. UUAS\ndenotes undirected attachment score, and Spr. denotes Spear-\nman correlation.\nforming Ô¨Åne-grained aggregation. In addition, the\npairwise mask is important on STS-B dataset and\nshows the beneÔ¨Åt of cross-sentence attention.\n4.5 Structural Probe\nOur method ingests syntax trees into the model\narchitecture directly. To examine if the representa-\ntion learned by the model also captures syntactic\nknowledge effectively, we follow Hewitt and Man-\nning (2019) to reconstruct a syntax tree of the entire\nsentence with linear transformation learned for the\nembedding space. If the syntax tree can be better\nreconstructed, the model is viewed to learn more\nsyntactic information. We evaluate the tree onundi-\nrected attachment score ‚Äì the percent of undirected\nedges placed correctly, and Spearman correlation\nbetween predicted and the actual distance between\neach word pair in a sentence. We probe models for\ntheir ability to capture the Stanford Dependencies\nformalism (de Marneffe et al., 2006). As shown in\nTable 6, for both metrics, the syntax-aware mod-\nels get better scores than corresponding baseline\nmodels, indicating that Syntax-BERT is able to in-\ncorporate more syntax information than its vanilla\ncounterparts.\n3018\n(a) (b)\nFigure 2: For an example sentence input, (a) The self-\nAttention scores of Syntax-Transformer corresponding\nto the sibling mask with dist = 3 . (b) The self-\nattention scores of a vanilla Transformer.\n5 Discussion\n5.1 Complexity analysis\nFirst, we choose BERT-Base as the base model to\nanalyze the space complexity. As reported in (De-\nvlin et al., 2019), the number of trainable parame-\nters in BERT-Base is about 110 million. Following\n(Strubell et al., 2018), LISA-BERT-Base replaces\none attention head in BERT-Base with a bi-afÔ¨Åne\nattention head. Such an operation only adds a train-\nable matrix ‚Äî the bi-afÔ¨Åne transformation matrix\n‚Äî in each layer, which brings about 0.6 million\nextra parameters. Syntax-BERT-Base introduces\na topical attention layer, which contains 1.0 mil-\nlion parameters in total for the BERT-Base version,\nwhile other parameters are inherited from vanilla\nBERT. Therefore, both LISA and Syntax-BERT\nadd few parameters to the model and do not affect\nits original space complexity.\nWe now analyze the time complexity of Syntax-\nBERT. Assume the number of tokens in each sen-\ntence is N. First, constructing syntactic trees\nfor each sentence and extract masking matrices\ncan be prepossessed in the training phase or Ô¨Ån-\nish in O(N2) in the online inference phase. The\ntime complexity of the embedding lookup layer\nis O(N). Then, the attention score is calculated\nby QK‚ä§‚äôM with complexity O(DQN2), where\nDQ is the dimension of Q. Assume we have M\nsub-networks. The complexity of masked self-\nattention is O(MDQN2). In the topical atten-\ntion, the calculation process is very similar to tradi-\ntional self-attention, only replacing Qwith a task-\nrelated vector. So it does not change the time\ncomplexity of BERT. Finally, to get output rep-\nresentation, subsequent softmax and scalar-vector\nmultiplication hold O(DV N) complexity, where\nDV is the dimension of V for the topical attention.\nAs such, the overall time complexity of Syntax-\nBERT is O(N) + O(MDQN2) + O(DV N) =\nO(MDQN2). When M is small, the model has\nthe same time complexity as vanilla BERT. More-\nover, as the sub-networks are usually very sparse,\nthe time complexity can be further improved to\nO(MDQE) by a sparse implementation. Here\nE ‚â™N2 denotes the average number of edges\nin a sub-network.\n5.2 Case Study\nWe select the sentence ‚ÄúJohn slipped in front of any-\none who was there‚Äù in the CoLA dataset for case\nstudy. The task is to examine if a sentence con-\nforms to English grammar. This sentence should\nbe classiÔ¨Åed as negative since we use everyone\ninstead of anyone. Syntax-Transformer classiÔ¨Åes\nit correctly, but the vanilla transformer gives the\nwrong answer.\nAs visualized in Figure 2(a), the relationship\nbetween word pair (‚Äúanyone‚Äù, ‚Äú. ‚Äù) has been\nhighlighted in one of the sub-networks, and the\ncorresponding topical attention score for this\nsub-network in Syntax-Transformer is also very\nhigh. This shows a good explainability of Syntax-\nTransformer by correctly identifying the error term\n‚Äúanyone‚Äù, following a rule that ‚Äúanyone‚Äù is sel-\ndom matched with the punctuation ‚Äú.‚Äù. However,\na vanilla Transformer shows less meaningful self-\nattention scores, as illustrated in Figure 2(b). We\ngive a brieÔ¨Ång here, and please refer to the ap-\npendix for a complete description.\n6 Conclusion\nIn this paper, we present Syntax-BERT, one of the\nÔ¨Årst attempts to incorporate inductive bias of syntax\ntrees to pre-trained Transformer models like BERT.\nThe proposed framework can be easily plugged\ninto an arbitrary pre-trained checkpoint, which un-\nderlines the most relevant syntactic knowledge au-\ntomatically for each downstream task. We eval-\nuate Syntax-BERT on various model backbones,\nincluding BERT, RoBERTa, and T5. The empirical\nresults verify the effectiveness of this framework\nand the usefulness of syntax trees. In the future,\nwe would like to investigate the performance of\nSyntax-BERT by applying it directly to the large-\nscale pre-training phase. Moreover, we are aiming\nto exploit more syntactic and semantic knowledge,\nincluding relation types from a dependency parser\nand concepts from a knowledge graph.\n3019\nReferences\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP, pages 632‚Äì642.\nDanqi Chen and Christopher D. Manning. 2014. A\nfast and accurate dependency parser using neural net-\nworks. In EMNLP.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT, pages 4171‚Äì4186.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,\nXiangyang Xue, and Zheng Zhang. 2019. Star-\ntransformer. In NAACL-HLT, pages 1315‚Äì1325.\nNezihe Merve G ¬®urel, Hansheng Ren, Yujing Wang,\nHui Xue, Yaming Yang, and Ce Zhang. 2019.\nAn anatomy of graph neural networks going deep\nvia the lens of mutual information: Exponen-\ntial decay vs. full preservation. arXiv preprint\narXiv:1910.04499.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for Ô¨Ånding syntax in word represen-\ntations. In NAACL-HLT.\nGanesh Jawahar, Beno ÀÜƒ±t Sagot, and Djam ¬¥e Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651‚Äì3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nXinzhou Jiang, Zhenghua Li, Bo Zhang, Min Zhang,\nSheng Li, and Luo Si. 2018. Supervised treebank\nconversion: Data and approaches. In ACL (Volume\n1: Long Papers), pages 2706‚Äì2716.\nEliyahu Kiperwasser and Miguel Ballesteros. 2018.\nScheduled multi-task learning: From syntax to trans-\nlation. Transactions of the Association for Computa-\ntional Linguistics, 6:225‚Äì240.\nDan Klein and Christopher D. Manning. 2003. Accu-\nrate unlexicalized parsing. In ACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nMarie-Catherine de Marneffe, Bill MacCartney, and\nChristopher D. Manning. 2006. Generating typed\ndependency parses from phrase structure parses. In\nLREC.\nXuan-Phi Nguyen, ShaÔ¨Åq Joty, Steven Hoi, and\nRichard Socher. 2020. Tree-structured attention\nwith hierarchical accumulation. In ICLR.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In EMNLP, pages 1532‚Äì1543.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniÔ¨Åed text-to-text trans-\nformer. CoRR, abs/1910.10683.\nDevendra Singh Sachan, Yuhao Zhang, Peng Qi, and\nWilliam Hamilton. 2020. Do syntax trees help pre-\ntrained transformers extract information? arXiv\npreprint arXiv:2008.09084.\nYikang Shen, Zhouhan Lin, Chin wei Huang, and\nAaron Courville. 2018. Neural language modeling\nby jointly learning syntax and lexicon. In ICLR.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2019. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks. In\nICLR.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In EMNLP, pages 1631‚Äì1642.\nEmma Strubell, Pat Verga, Daniel Andor, David Weiss,\nand Andrew McCallum. 2018. Linguistically-\ninformed self-attention for semantic role labeling.\nIn EMNLP.\nDhanasekar Sundararaman, Vivek Subramanian,\nGuoyin Wang, Shijing Si, Dinghan Shen, Dong\nWang, and Lawrence Carin. 2019. Syntax-infused\ntransformer and bert models for machine translation\nand natural language understanding. arXiv preprint\narXiv:1911.06156.\nKai Sheng Tai, Richard Socher, and Christopher D.\nManning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. In ACL.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS, pages 5998‚Äì6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In ICLR.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNAACL-HLT, pages 1112‚Äì1122.\n3020\nYan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,\nand Zhi Jin. 2015. Classifying relations via long\nshort term memory networks along shortest depen-\ndency paths. In EMNLP, pages 1785‚Äì1794.\nYunlun Yang, Yunhai Tong, Shulei Ma, and Zhi-Hong\nDeng. 2016. A position encoding convolutional neu-\nral network based on dependency tree for relation\nclassiÔ¨Åcation. In EMNLP, pages 65‚Äì74.\nMuhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,\nand Jingbo Zhu. 2013. Fast and accurate shift-\nreduce constituent parsing. In ACL.\nA Detailed settings\nHere we provide detailed settings for reproduction.\nThe open-source code will be released when this\npaper is ofÔ¨Åcially published.\nA.1 Stanford Sentiment Treebank\nFor raw Transformers, the number of layers is\nset as 12 and hidden dimension for each in-\ntermediate layer is set as 512. The probabil-\nity of dropout is 0.1, and the hidden dimension\nof the Ô¨Ånal fully-connected layer is 2000. The\nword embedding vectors are initialized by GloVe\n(glove.840B.300d3) (Pennington et al., 2014) and\nÔ¨Åne-tuned during training. We use Adam optimizer\nwith an initial learning rate 1e-4.\nA.2 Natural Language Inference\nFor raw Transformers, we set layer number as 12,\nthe hidden dimension of intermediate layers as 512,\ndropout ratio as 0.15, and the dimension of fully\nconnected layer before Softmax activation as 2000.\nLearning rate is initialized as 5e-4, and Adam opti-\nmizer is used along with exponential learning rate\ndecay of 0.9.\nB Connection to GNN\nA Transformer layer can be viewed as a special\nkind of Graph Neural Network (GNN), where each\nnode represents for a word and all nodes con-\nstruct a complete graph. To improve training speed\nand generalization ability, there are some previ-\nous works that advocate sparse architectures. For\ninstance, Sparse Transformer (Child et al., 2019)\nseparates the full self-attention operation across\nseveral steps of attention for image classiÔ¨Åcation.\nStar-Transformer (Guo et al., 2019) sparsiÔ¨Åes the\narchitecture by shaping the fully-connected net-\nwork into a star-shaped structure consisting of ring\n3https://nlp.stanford.edu/projects/glove/\nconnections and radical connections. In the archi-\ntecture of Syntax-BERT, we also introduce sparsity\nto the complete graph network by decomposing\nit into multiple sub-networks. The most salient\npart of our approach is that the inductive bias is\ndesigned by syntax tree, which is a crucial prior\nfor NLP tasks. In addition, as shown previously in\nTable 5, a random decomposition of the network\nalso result in moderate performance enhancement.\nSimilar phenomena is also reported in the image\nclassiÔ¨Åcation scenario with Graph Convolutional\nNetwork (GCN) (G¬®urel et al., 2019).",
  "topic": "Syntax",
  "concepts": [
    {
      "name": "Syntax",
      "score": 0.7623646259307861
    },
    {
      "name": "Computer science",
      "score": 0.6660111546516418
    },
    {
      "name": "Natural language processing",
      "score": 0.5807983875274658
    },
    {
      "name": "Abstract syntax tree",
      "score": 0.5415244102478027
    },
    {
      "name": "Linguistics",
      "score": 0.5161529779434204
    },
    {
      "name": "Programming language",
      "score": 0.4677593410015106
    },
    {
      "name": "Computational linguistics",
      "score": 0.45910218358039856
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45514634251594543
    },
    {
      "name": "Chen",
      "score": 0.4493022561073303
    },
    {
      "name": "Transformer",
      "score": 0.43287187814712524
    },
    {
      "name": "Philosophy",
      "score": 0.18202105164527893
    },
    {
      "name": "Engineering",
      "score": 0.13633808493614197
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210156404",
      "name": "Institute of Information Engineering",
      "country": "CN"
    }
  ],
  "cited_by": 50
}