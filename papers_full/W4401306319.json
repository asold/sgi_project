{
  "title": "LawLLM: Law Large Language Model for the US Legal System",
  "url": "https://openalex.org/W4401306319",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2131509014",
      "name": "Dong Shu",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A2159053406",
      "name": "Haoran Zhao",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A2303386819",
      "name": "Xukun Liu",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A2598100472",
      "name": "DÃ¡vid Demeter",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A2436451909",
      "name": "Mengnan Du",
      "affiliations": [
        "New Jersey Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2112127489",
      "name": "Yongfeng Zhang",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4388963559",
    "https://openalex.org/W4386507404",
    "https://openalex.org/W4384822682",
    "https://openalex.org/W3119409427",
    "https://openalex.org/W4388206012",
    "https://openalex.org/W4385572179",
    "https://openalex.org/W4206256378",
    "https://openalex.org/W4327810667",
    "https://openalex.org/W3081523085",
    "https://openalex.org/W4389520406",
    "https://openalex.org/W3177382889",
    "https://openalex.org/W4388955716",
    "https://openalex.org/W2031677098",
    "https://openalex.org/W2999905431"
  ],
  "abstract": "In the rapidly evolving field of legal analytics, finding relevant cases and\\naccurately predicting judicial outcomes are challenging because of the\\ncomplexity of legal language, which often includes specialized terminology,\\ncomplex syntax, and historical context. Moreover, the subtle distinctions\\nbetween similar and precedent cases require a deep understanding of legal\\nknowledge. Researchers often conflate these concepts, making it difficult to\\ndevelop specialized techniques to effectively address these nuanced tasks. In\\nthis paper, we introduce the Law Large Language Model (LawLLM), a multi-task\\nmodel specifically designed for the US legal domain to address these\\nchallenges. LawLLM excels at Similar Case Retrieval (SCR), Precedent Case\\nRecommendation (PCR), and Legal Judgment Prediction (LJP). By clearly\\ndistinguishing between precedent and similar cases, we provide essential\\nclarity, guiding future research in developing specialized strategies for these\\ntasks. We propose customized data preprocessing techniques for each task that\\ntransform raw legal data into a trainable format. Furthermore, we also use\\ntechniques such as in-context learning (ICL) and advanced information retrieval\\nmethods in LawLLM. The evaluation results demonstrate that LawLLM consistently\\noutperforms existing baselines in both zero-shot and few-shot scenarios,\\noffering unparalleled multi-task capabilities and filling critical gaps in the\\nlegal domain.\\n",
  "full_text": "LawLLM: Law Large Language Model for the US Legal System\nDong Shu\nNorthwestern University\nEvanston, IL, United States\ndongshu2024@u.northwestern.edu\nHaoran Zhao\nNorthwestern University\nEvanston, IL, United States\nhaoranzhao2024@u.northwestern.edu\nXukun Liu\nNorthwestern University\nEvanston, IL, United States\nxukunliu2025@u.northwestern.edu\nDavid Demeter\nNorthwestern University\nEvanston, IL, United States\nddemeter@u.northwestern.edu\nMengnan Du\nNew Jersey Institute of Technology\nNewark, NJ, United States\nmengnan.du@njit.edu\nYongfeng Zhang\nRutgers University\nNew Brunswick, NJ, United States\nyongfeng.zhang@rutgers.edu\nAbstract\nIn the rapidly evolving field of legal analytics, finding relevant\ncases and accurately predicting judicial outcomes are challenging\nbecause of the complexity of legal language, which often includes\nspecialized terminology, complex syntax, and historical context.\nMoreover, the subtle distinctions between similar and precedent\ncases require a deep understanding of legal knowledge. Researchers\noften conflate these concepts, making it difficult to develop special-\nized techniques to effectively address these nuanced tasks. In this\npaper, we introduce the Law Large Language Model ( LawLLM), a\nmulti-task model specifically designed for the US legal domain to\naddress these challenges. LawLLM excels at Similar Case Retrieval\n(SCR), Precedent Case Recommendation (PCR), and Legal Judgment\nPrediction (LJP). By clearly distinguishing between precedent and\nsimilar cases, we provide essential clarity, guiding future research\nin developing specialized strategies for these tasks. We propose cus-\ntomized data preprocessing techniques for each task that transform\nraw legal data into a trainable format. Furthermore, we also use tech-\nniques such as in-context learning (ICL) and advanced information\nretrieval methods in LawLLM. The evaluation results demonstrate\nthat LawLLM consistently outperforms existing baselines in both\nzero-shot and few-shot scenarios, offering unparalleled multi-task\ncapabilities and filling critical gaps in the legal domain. Code and\ndata are available at https://github.com/Tizzzzy/Law_LLM.\nCCS Concepts\nâ€¢ Applied computing â†’Law; â€¢ Computing methodologies â†’\nNatural language processing ; Multi-task learning; â€¢ Informa-\ntion systems â†’Top-k retrieval in databases .\nKeywords\nLarge Language Models, Multitask Learning, Legal System, Natural\nLanguage Processing\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0436-9/24/10\nhttps://doi.org/10.1145/3627673.3680020\nACM Reference Format:\nDong Shu, Haoran Zhao, Xukun Liu, David Demeter, Mengnan Du, and Yongfeng\nZhang. 2024. LawLLM: Law Large Language Model for the US Legal System.\nIn Proceedings of the 33rd ACM International Conference on Information and\nKnowledge Management (CIKM â€™24), October 21â€“25, 2024, Boise, ID, USA. ACM,\nNew York, NY, USA, 21 pages. https://doi.org/10.1145/3627673.3680020\n1 Introduction\nThe development of Large Language Models (LLMs) has led to\nsignificant progress in computational linguistics, particularly im-\npacting fields like legal analytics. Given the nature of legal language,\nwhich includes complex terminologies and context-specific logical\nframeworks, LLMs offer unprecedented capabilities in this domain\n[21]. The integration of LLMs into the legal field significantly boosts\nthe efficiency of legal practitioners, such as lawyers and judges, by\naccurately interpreting their natural language input and generating\nmost relevant responses. This reduces the need for extensive man-\nual review of huge legal texts. Moreover, LLMs can provide lawyers\nwith novel insights, revealing overlooked details and perspectives\nthat can be critical in complex cases. Recent developments in legal\ndomain have demonstrated the potential of LLMs in enhancing legal\njudgment predictions and handling various legal tasks effectively.\nFor example, studies such as LM-CompEval-Legal [ 26] have sys-\ntematically evaluated the effectiveness of LLMs, other projects like\nPLJP [33] and LoT [11] have focused on integrating domain-specific\nmodels and advancing LLMsâ€™ understanding of legal reasoning.\nAlthough these models have shown promise, there remain re-\nsearch challenges. First, these models generally address single-task\nchallenges. In contrast, LawLLM innovatively supports multiple le-\ngal tasks simultaneously, providing a more nuanced analysis of\ncomplex legal datasets and filling a critical void in the field. Second,\nanother controversial area in the legal domain is the difference\nbetween precedent cases and similar cases [ 22]. Various models\nhave been developed for precedent case recommendation, ranging\nfrom expert knowledge-based models to models based on natu-\nral language processing [2, 16, 18, 20]. These approaches typically\nconvert legal text into embeddings and calculate similarity at the\nembedding level, which aids in precedent selection. However, we\nbelieve that this approach is more on identifying similar cases with\ntextual and contextual similarities, not precedent cases.\nIn our study, we emphasize the key differences between the two.\nFirstly, a precedent case must have been closed before the input\nlegal case, ensuring its relevance and applicability to the current\ncase under consideration. Secondly, precedent cases are those that\narXiv:2407.21065v1  [cs.CL]  27 Jul 2024\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Dong Shu et al.\n### \nInstruction:\n[...] \nYour \nmain \nfunction \nis \nto \nanticipate \nthe \nlikely \nverdict \nof \nthe \nlegal \ncase \npresented \nby \nthe \nuser \n[...]\nLegal \nJudgement \nPrediction\nLawLLM\n### \nInstruction:\n[...] \nYour \nmain \nfunction \nis \nto \nidentify \nand \noutput \nthe \nprecedent \ncase \nfrom \nthe \nlist \nbased \non \nthe \ndescription \nprovided \n[...]\nPrecedent \nCases \nRecommendation\n### \nInstruction:\n[...] \nYour \nmain \nfunction \nis \nto \nidentify \nand \noutput \nthe \nmost \nsimilar \ncase \nfrom \nthe \nlist \nbased \non \nthe \ndescription \nprovided \n[...]\n Similar Cases Retrieval\n### \nResponse:\nXXX \net \nal. \nv. \nXXX\nThey \nhave \nprecedent \nrelation \nis \nbecause \n...\n Model Response\n### \nResponse:\nSettlement\n### \nResponse:\nXXX \nv. \nThe\n \nXXX\nFigure 1: LawLLM supports three tasks: Similar Case Re-\ntrieval, Precedent Case Recommendation, and Legal Judg-\nment Prediction.\nwere actually considered by judges in making their decisions, unlike\nsimilar cases which might not have been taken into account. Thirdly,\nsimilar cases share textual and thematic similarities in the case\nnarrative or might fall into similar case categories, while precedent\ncases might seem unrelated at face value. It is also worth noting\nthat while a legal caseâ€™s precedent case can sometimes be the same\nas a similar case, this is not always the case.\nIn this paper, we introduce the Law Large Language Model\n(LawLLM), a multi-task LLM capable of Similar Case Retrieval (SCR),\nPrecedent Case Recommendation (PCR) and Legal Judgment Pre-\ndiction (LJP). To build LawLLM, we finetune Gemma-7B [29] using\ninstruction tuning on United State real-life legal datasets and can\nperform LJP, PCR, and SCR tasks. The instructions for all three tasks\nfall under the classification category. By doing so, we can effec-\ntively minimize irrelevant and noisy options for the model, thereby\nimproving its performance. We show the overall idea of LawLLM in\nFigure 1. The development ofLawLLM also includes three innovative\npreprocessing approaches for transforming unstructured legal data\ninto a trainable format. More specifically, for LJP, we summarize\nand extract verdicts from raw datasets and apply zero and few shot\nIn-context learning (ICL) [13, 36] technique to enhance the model\nperformance. In PCR, LawLLM connects ground truth legal casesâ€™\nprecedent relationships as a Knowledge Graph (KG), treating each\ncase as a unique entity linked by precedential connections [ 25].\nAdditionally, the SCR task creates a legal case vector database and\nintegrates advanced Information Retrieval (IR) techniques [8, 12].\nOur study presents LawLLM as a pioneering model in the realm\nof legal LLMs. Our key contributions are given as follows:\nâ€¢We propose LawLLM, which is adept at handling a range of\nlegal tasks, including LJP, PCR, and SCR. This multi-task\nfunctionality is important in addressing the diverse require-\nments of the legal domain.\nâ€¢LawLLM distinguishes between precedent cases and similar\ncases, providing clarity on the objectives of each task. This\nclarification enables the future research to develop tailored\nstrategies for those tasks.\nâ€¢Experimental results indicate that LawLLM outperformed all\nbaseline models, including the GPT-4 model, across all three\ntasks. These results highlight LawLLMâ€™s robust capabilities in\nthe legal domain.\n2 Related Work\nLegal AI is significantly increasing the efficiency and effectiveness\nof the legal community. AI technologies, specifically Large Lan-\nguage Models (LLMs), are leading the way in automating complex\ntasks like document analysis, case prediction, and legal research\n[34, 38]. LLMs utilize advanced algorithms and data analytics to\nprocess and generate legal texts, which leads to significant improve-\nments in speed and accuracy [39]. In this section, we introduce the\nvarious applications of Legal AI and LLMs in legal practices.\n2.1 Precedent Case Recommendation\nThe recommendation of precedent cases is a fundamental aspect\nof legal practice, as previous verdicts significantly affect current\nlegal decisions. The field has evolved from early keyword-based\nsearches and manual annotations to more complicated AI-driven\nmodels that improve retrieval efficiency and contextuality. Wu et al.\n[34] proposed the Precedent-Enhanced Legal Judgment Prediction\nframework, which combines LLMs with domain-specific exper-\ntise to improve legal prediction accuracy significantly. Ma et al .\n[17] developed the Structured Legal Case Retrieval system, which\nuses structural information from legal documents to improve case\nsearch precision and contextual relevance. Moreover, Su et al. [27]\nproposed the Caseformer. This innovative pre-training framework\nlearns from a vast corpus of legal texts to refine case retrieval and\ncontextualization across multiple languages.\n2.2 Similar Case Retrieval\nBesides precedent recommendation, retrieving similar cases, those\nsharing analogous facts or legal issues, is crucial for comprehensive\nlegal analysis and strategy formulation. Traditionally, this process\nrequired extensive manual labor, with professionals needing to\ndig through large case databases [17, 19]. Today advances in NLP\nand machine learning have transformed this task, allowing seman-\ntic content extraction and comparison across documents. Kang\net al. [14] enhanced similarity-based retrieval by incorporating as-\nsociative knowledge. This approach refines retrieval outcomes by\nleveraging similarity and associative analyses, a technique also\nproven effective in other fields such as medical diagnosis and IT\nservice management. Mandal et al. [19] analyzed textual similarity\ntechniques on an Indian Supreme Court dataset and discovered\nthat traditional methods like TF-IDF outperform modern context-\naware models like BERT. Wu et al. [32] studied semantic retrieval in\nthe Chinese judicial system and developed a model that generates\nknowledge graphs for cases to improve trial accuracy and fairness.\nThese technological advances have greatly simplified legal research,\nmaking it more effective and comprehensive.\n2.3 Legal Judgment Prediction\nPredicting legal judgments involves estimating potential verdicts\nbased on a deep analysis of historical data and established legal\nstandards. Initial models in this field were relatively simple, mainly\nLawLLM: Law Large Language Model for the US Legal System CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA\nLegal \nCase \nDatabase\nEmbedding\nModel\n2\nLegal \nCase \nVector \nDatabase\nPrecedent \nKnowledge \nGraph\n1\nGPT-4\nRaw \nLegal \nCase\nLegal \nCase\n3\nPrecedent \nFunction\nTop-1 \nSimiar \nCandidate\nTop-10 \nSimilar \nCandidate\nSCR \nTraining\n \nInput\n1\nTraining\n \nCase\n2\nTesting\n \nCase\n3\n...\nTop-0 \nSimiar \nCandidate\nTop-9 \nSimilar \nCandidate\n...\nLLM\nSCR \nTesting\n \nInput\n4\nJudge?\nAttorneys?\nPlaintiff \nor \nDefendant?\nCase \nNarrative?\nThey \nhave \nprecedent \nrelation \nis \nbecause \n...\nCase \nA\nCase \nB\n1\nPCR \nTraining\n \nInput\n2\nTesting\n \nLegal \nCase\n3\nK\n \nPrecedent \nCases\n4\n(10 \n- \nK) \nSimilar \nCases\nPCR \nTesting\n \nInput\n5\nadd\nLLM\nData \nPreprocessing\nSimilar \nCase \nRetrieval\nPrecedent \nCase \nRecommendation\nLegal \nJudgment \nPrediction\nLJP \nTraining\n \nInput\nLLM\nTesting\n \nCase\nPrecedent \nCase\nSimilar\n \nCase\n2\n3\nLJP \nTesting\n \nInput\n4\nTraining\n \nCase\n1\nFigure 2: An overview of our LawLLM: Data Preprocessing is in the upper left in green, Similar Case Retrieval Processing is in\nthe upper right in yellow, Precedent Case Recommendation is in the lower left in red, and Legal Judgment Prediction is in the\nlower right in blue.\ndepending on linear algorithms incapable of capturing the vari-\nous aspects of legal reasoning. Wang and Jin [31] CNN-BiGRU\nmulti-task learning model improves prediction accuracy through\nthe utilization of shared information from related legal subtasks.\nChalkidis et al . [3] used European Court of Human Rights data\nto establish robust performance benchmarks for long legal texts\nusing hierarchical BERT. Rusnachenko et al. [23] showed attention-\nbased methods could improve system performance by optimizing\ndocument preprocessing and attention mechanisms in competition\ncontexts. These models predict outcomes and are constantly learn-\ning from new cases to improve their accuracy, demonstrating the\nadaptability of LLMs in legal judgment prediction.\n2.4 LLMs in the Legal Domain\nPrior to the development of large language models (LLMs), pre-\ntrained language models (PLMs) for specific domains were explored,\nsuch as Lawformer, which is to process lengthy Chinese legal doc-\numents using a Longformer-based architecture [35]. Researchers\ndiscovered that models like GPT-4 could successfully pass bar ex-\nams as LLMs gained attention, demonstrating profound abilities in\nlegal reasoning and text generation [15]. This success resulted in\nthe growth of legal domain-specific LLMs, such as Chatlaw, which\nutilizes conversational AI to improve user interactions with legal\nsystems [5]. In this vein, SaulLM-7B was introduced as the first LLM\nexplicitly designed for comprehending and generating legal texts,\nleveraging a substantial legal corpus to achieve state-of-the-art\nperformance [4]. LLMsâ€™ influence extends beyond specific tasks to\nbroader legal operations. These applications range from document\nautomation, where LLMs assist in drafting and reviewing legal doc-\numents, to compliance monitoring, which ensures adherence to\nregulatory standards [28]. LLMs simplify complex legal processes\nfor non-specialists and lower barriers to legal advice [9]. This broad\napplication of LLMs demonstrates their broad application and the\npotential for continued innovation in the legal sector.\nDespite the success of those contemporary works, these models\nprimarily focus on utilizing LLMsâ€™ understanding and capabilities\nto perform general legal question answering. However, LawLLM is\ndesigned to leverage the LLMsâ€™ comprehension and learned abilities\nto predict and perform specific tasks within the legal domain.\n3 Methodology\nIn this study, we propose the Law Large Language Model (LawLLM)\nto address three critical tasks within the legal domain: Similar\nCase Retrieval (SCR), Precedent Case Recommendation (PCR), and\nLegal Judgment Prediction (LJP). Our methodological framework,\nillustrated in Figure 2, is divided into four distinct parts: Data Pre-\nprocessing, SCR Processing, PCR Processing, and LJP Processing.\n3.1 Data Preprocessing\nOur approach begins with the systematic collection of case data\nfrom legal databases, denoted as D. We make sure all collected raw\ncase data, ğ‘‘ğ‘– âˆˆD, encompasses a variety of information as below:\nğ‘‘ğ‘– = {Title, Date, Judge, Plaintiff(s), Plaintiffâ€™s Attorney(s),\nDefendant(s), Defendantâ€™s Attorney(s), Case Detail,\nPrecedent Relationship}.\n(1)\nAs depicted in the upper left of Figure 2, data preprocessing\nconsists of three primary steps:\nStep 1. Given the voluminous nature of the textual content within\ncase detail and their often implicit verdicts, we utilize a GPT-4 [1]\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Dong Shu et al.\nmodel to extract core information and summarize each case. This\nstep reduces information overload and ensures the adaptability of\nour dataset to the constraints of Gemma, particularly with token\nsize limitations. The GPT-4 preprocess instruction is shown here:\nI have a legal case description and require two distinct\npieces of information:\n1. Summary: Please provide a detailed summary of the\ncase, focusing on the facts and events. Exclude any in-\nformation about the verdict.\n2. Verdict: State the verdict of the case, consider the fol-\nlowing categories:\n- Plaintiff win\n- Defendant win\n- Settlement\n- Case dismissal\n- Unsure\nIf the verdict is mentioned, respond exclusively with the\nchosen categories ONLY. If the outcome is not explicitly\nmentioned or cannot be inferred from the information\ngiven, please respond with â€˜unsureâ€™ only.\nFormat your responses as follows:\n# - For the summary, begin with â€˜Answer 1:â€™\n# - For the verdict, start with â€˜Answer 2:â€™\nHere is the description of the case:\n[Case Description...]\nThe output of this step includes a summarized case and a labeled\nverdict, formatted as follows:\nCase Summary, Verdict = LLM(Case Detail,\nMaximum Token |ğ‘‘ğ‘– ). (2)\nFor each legal case ğ‘‘ğ‘– , we reorganize the data into a new format\nğ‘‘â€²\nğ‘– , defined as:\nğ‘‘â€²\nğ‘– = {Title, Date, Judge, Plaintiff(s), Plaintiffâ€™s Attorney(s),\nDefendant(s), Defendantâ€™s Attorney(s),\nCase Summary}.\n(3)\nğ·â€²=\n\b\u0000ğ‘‘â€²\n1,ğ‘£â€²\n1\n\u0001 ,\u0000ğ‘‘â€²\n2,ğ‘£â€²\n2\n\u0001 ,..., \u0000ğ‘‘â€²\nğ‘›,ğ‘£â€²\nğ‘›\n\u0001\t\n. (4)\nThere are some constraints when we separate theğ·â€²into training\nand testing data. We make sure that all legal cases have at least five\nprecedent relationships. To ensure a balance training, the training\ndataset has 25% from each of the following categories: plaintiff\nwins, defendant wins, settlements, and case dismissals. We also\nmake sure that all testing legal cases have at least five precedent\nrelationships connect to the training dataset, further explanation is\ngiven in Section 4.1 Data Splits.\nStep 2. After Step 1, all training legal cases ğ‘‘â€²\nğ‘– are transformed\ninto high-dimensional vectors using the OpenAI Embedding model.\nThis vector database is later used to retrieve the top-ğ‘˜similar cases\nbased on semantic and contextual similarities.\nStep 3. This step involves converting the precedent case rela-\ntionships from our training dataset into a knowledge graph (KG).\nDefined as ğ¾ğº = (ğ¸,ğ‘…,ğ¿ ), where ğ¸ represents entities, ğ‘… repre-\nsents binary relationships (indicative of precedent relations), and\nğ¿ âŠ†ğ¸Ã—ğ‘…Ã—ğ¸represents the set of triples forming the graphâ€™s edges.\nEach triple (ğ‘’ğ‘ ,ğ‘Ÿ,ğ‘’ ğ‘¡ ) âˆˆğ¿ indicates a directed edge from source\nentity ğ‘’ğ‘  to target entityğ‘’ğ‘¡ via relationship ğ‘Ÿ. The KG data structure\nsimplifies the complex task of identifying relevant precedent cases,\nturning it into a entity prediction problem, i.e., given a query of\n(ğ‘’ğ‘ ,ğ‘Ÿ, ?), the model will predict the missing entity.\nWe further customize data processing for SCR, PCR, and LJP\ntasks, ensuring a robust and effective implementation of LawLLM.\n3.2 Similar Case Retrieval\nAs depicted in the upper right of Figure 2, the SCR process is divided\ninto two phases: training (Steps 1-2) and testing (Steps 3-4).\nTraining Phase. During training, each training caseğ‘‘â€²\nğ‘– is inputted\ninto the vector database, which generates the top 10 candidate cases.\nThese cases are then randomized in order and formulated into the\nSCR training instruction. Here is an example SCR model input:\n### Instruction:\nYou are a legal expert who specializes in comparing user-\nsupplied legal cases to a list of candidate legal cases,\nwhich includes titles and content. Your main function is\nto identify and output the title of the most similar case\nfrom the list based on the description provided.\nYou should only output the case title and not any other\ninformation.\nConsider the following choices:\nChoice 1:\n[Case 1...]\nChoice 2:\n...\nChoice 10:\n[Case 10...]\n### Input:\n[Input Case...]\nIn this scenario, the SCR task instruction will fall into the classi-\nfication category, which provides the model with 10 cases to choose\nthe most similar one. It is important to note that the top-0 similar\ncase is the case ğ‘‘â€²\nğ‘– itself, so in practice, we retrieve the top-1 to\ntop-10 similar cases from the vector database, and the top-1 case\nfrom this selection serves as the ground truth for this training task.\nTesting Phase. The testing phase mirrors the training process\nas we initially retrieve the top 10 similar cases from the vector\ndatabase. However, during testing, we retrieve cases ranked from\ntop-0 to top-9, as the test case itself is not included in the vector\ndatabase. The modelâ€™s expected response depends on the evaluation\nmetrics we use: top-1, top-3, and top-5. For the top-1 metric, we\nexpect LawLLM to identify the most similar case as the top result.\nThe top-3 metric evaluates whether the modelâ€™s answer falls within\nthe top three retrieved candidates, while the top-5 metric extends\nthis evaluation to include the top five candidates.\n3.3 Precedent Case Recommendation\nThe Precedent Case Recommendation (PCR) within LawLLM utilizes\na unique approach by employing a precedent case knowledge graph\n(KG), which differentiates itself from conventional PCR methods\nLawLLM: Law Large Language Model for the US Legal System CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA\nthat often speculate on potential precedent relationships. Our sys-\ntem instead relies on confirmed precedent pairs, as illustrated in the\nlower left of Figure 2, where Steps 1 and 2 constitute the training\nphase and Steps 3-5 are the testing phase.\nTraining Phase. From the previously established KG, for each con-\nfirmed triple (ğ‘’ğ‘ ,ğ‘Ÿ,ğ‘’ ğ‘¡ ), we utilize BERT embeddings [7] to evaluate\nthe similarity between various case features (e.g., Judge, Case Detail,\nPlaintiff, or Defendant), denoted as {ğ¹1,ğ¹2,...ğ¹ ğ‘— }. We calculate the\nsimilarity score ğ‘†ğ‘– for each feature pair ğ¹1ğ‘– and ğ¹2ğ‘– , as follows:\nğ‘†ğ‘– = sim(ğµğ¸ğ‘…ğ‘‡(ğ¹1ğ‘– ),ğµğ¸ğ‘…ğ‘‡ (ğ¹2ğ‘– )),\nğ‘– âˆˆ{1 âˆ’ğ‘—} (5)\nThe highest similarity score across all features determines the\nprimary factor underlying their precedent relationship:\nPrimary Factor = max(ğ‘†1,ğ‘†2,...,ğ‘† ğ‘— ). (6)\nDuring the training input creation, we present a total of 10\nchoices for the model. The ground truth precedent case ğ‘’ğ‘¡ is ran-\ndomly placed among these choices, with the other 9 selections\nfilled with similar, yet non-precedent, cases from the vector data-\nbase. This setup aims to teach the model that textual similarity\ndoes not necessarily imply a precedent relationship. The modelâ€™s\nexpected output includes the correct precedent case ğ‘’ğ‘¡ and the\nreasoning for its selection (i.e, which primary factor caused this\nprecedent relationship). An example of the model input is:\n### Instruction:\nYou are a legal expert who specializes in comparing user-\nsupplied legal cases to a list of candidate legal cases,\nwhich includes titles and content. Your main function is\nto identify and output the precedent case from the list\nbased on the description provided.\nYou should only output the reasoning process and case\ntitle.\nConsider the following choices:\nChoice 1:\n[Case 1...]\nChoice 2:\n...\nChoice 10:\n[Case 10...]\n### Input:\n[Input Case...]\nTesting Phase. For each test case, since we made sure there are at\nleast five precedent cases in the training dataset, we can identify\nğ‘˜ precedent cases from the KG (which structured by the training\ndataset) as ground truths, where ğ‘˜ aligns with the top-ğ‘˜ evaluation\nmetrics. For the top-1 metric, a single ground truth precedent case\nis selected, while for top-3 and top-5 metrics, 3 and 5 ground truths\nare selected, respectively. The remaining slots of 10 âˆ’ğ‘˜ are filled\nwith similar cases. The model is then tasked with selecting one of\nğ‘˜ precedent cases and explaining the reasoning behind its choice.\n3.4 Legal Judgment Prediction\nThe Legal Judgment Prediction (LJP) processing utilizes the dataset\nğ·â€²constructed during the data preprocessing stage. This dataset\npairs each processed legal case ğ‘‘â€²\nğ‘– with its corresponding verdict\nğ‘£â€²\nğ‘– . As illustrated in the lower right of Figure 2, the training phase\ninvolves step 1 and the testing phase involves rest of steps.\nTraining Phase. We use \u0000ğ‘‘â€²\nğ‘– ,ğ‘£â€²\nğ‘–\n\u0001 to establish a four-category clas-\nsification training input, Plaintiff wins, Defendant wins, Settlement,\nor Case Dismissal. Each caseâ€™s corresponding verdict ğ‘£â€²\nğ‘– serves as\nthe label for training. Here is an example of the model input:\n### Instruction:\nYou are a legal expert who specializes in predicting out-\ncomes for legal cases. Utilize your internal knowledge\nbase to predict verdict. Your main function is to antici-\npate the likely verdict of the legal case presented by the\nuser.\nYou should only output the verdict and not any other\ninformation.\nConsider the following choices:\n1. Defendant Wins\n2. Plaintiff Wins\n3. Settlement\n4. Case Dismissal\n### Input:\n[Input Case...]\nTesting Phase. During the testing phase, we evaluate LawLLM\nwith both zero-shot and few-shot in-context learning (ICL) sce-\nnario. In few-shot ICL, we enhance each test caseğ‘‘â€²\nğ‘– with additional\ncontextual information, one similar case and one precedent case.\nIts precedent cases is sourced from our KG, and one is randomly\nselected to be included in the test input. Simultaneously, a most\nsimilar case is retrieved from the vector database. This approach\nensures that the modelâ€™s predictions are influenced by relevant legal\nprecedents and similar case facts, thereby improving the accuracy\nand reliability of the judgment predictions.\n3.5 Unified Model Fine-Tuning\nOur methodology involves a unified fine-tuning strategy for the\nLawLLM, leveraging a combined dataset with three tasks. This dataset,\ndenoted asDatasetcombined = LJPâŠ•PCRâŠ•SCR. We employ a cutting-\nedge 4-bit quantized Low-Rank Adaptation (LoRA) technique to\ninstruction fine-tune the Gemma model. We use the cross-entropy\nloss functionğ¿during the LoRA. It calculates the difference between\nthe modelâ€™s predicted token probabilities and the actual token prob-\nabilities in the expected output sequence. In the following equation,\nğ‘›represents the length of the expected output sequence, ğ‘¥ repre-\nsents the input instruction, and ğ‘¦ğ‘– denotes the i-th token in the\nexpected output sequence.\nğ¿= âˆ’\nğ‘›âˆ‘ï¸\nğ‘–=1\nlog ğ‘ƒ(ğ‘¦ğ‘– |ğ‘¥,ğ‘¦1,ğ‘¦2,...,ğ‘¦ ğ‘–âˆ’1). (7)\n4 Experiments\nIn this section, we conduct experiments to evaluate the performance\nof LawLLM on three tasks: Similar Case Retrieval (SCR), Precedent\nCase Recommendation (PCR), and Legal Judgment Prediction (LJP).\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Dong Shu et al.\n4.1 Experimental Setup\nDatasets. We conduct our experiments on the CaseLaw dataset,\ninitiated by Harvard Law Schoolâ€™s Library Innovation Lab as part of\nthe CaseLaw Project [10]. This database encompasses a wide range\nof court cases from both state and federal in the United States. The\nproject primarily focuses on democratizing access to American legal\ninformation, particularly through its Case Access Project (CAP),\nwhich is aimed at providing free and public access. The statistics of\nthe CaseLaw dataset used in our experiments are shown in Table 1.\nTable 1: Datasets Statistics\nDATASETS CaseLaw\nLanguage English\n# State and Federal Totals 6,930,777\n# Train case 1,00,000\n# Test case 200,000\nAvg. length per case (words) 2695.38\nEvaluation Metrics. As previously mentioned, we employ top-ğ‘˜\nmetrics to evaluate the performance of the SCR and PCR tasks.\nSpecifically, we use top-1, top-3, and top-5 metrics. These metrics\nmeasure the modelâ€™s precision in identifying the correct response\nfrom a pool of 10 choices. For example, the top-1 metric requires the\nmodel to return the top choice as the answer. The top-3 and top-5\nmetrics provide more flexibility, allowing the correct answer to be\nanywhere within the top three or top five choices, respectively.\nIn addition to top-ğ‘˜metrics, we evaluate the hallucination rate of\nmodels using a â€˜not-foundâ€™ metric. This metric tracks the proportion\nof responses that are entirely fabricated and do not match any of\nthe 10 given choices. By measuring the â€˜not-foundâ€™ rate, we aim\nto understand how often models produce answers unrelated to the\nprovided options, offering insight into their reliability.\nFor the LJP task, we employ accuracy and F1-score [24] metrics to\ngauge the modelâ€™s performance. Accuracy calculates the proportion\nof correctly predicted verdicts across all cases, providing a direct\nmeasure of overall prediction performance. The F1 score ranging\nfrom 0 to 1, combines precision and recall into a single harmonic\nmean, offering a balanced evaluation of the modelâ€™s effectiveness.\nData Splits. As previously noted, our data are split according to\nthree constraints.\nâ€¢Constraint 1: For PCR, we employ top-ğ‘˜ evaluation met-\nrics, which means each case has to have a minimum of five\nprecedent cases, allowing us to identify ğ‘˜ ground truths.\nâ€¢Constraint 2: We must ensure that when a test case is\nevaluated, its ground truth precedent case can be located\nwithin the knowledge graph formed by the training cases.\nTherefore, each test case must have at least five precedent\ncases present in the training data.\nâ€¢Constraint 3: To ensure balanced model training for Legal\nJudgment Prediction (LJP), the training dataâ€™s verdict dis-\ntribution should consist of 25% for each possible outcome:\nplaintiff wins, defendant wins, settlements, and dismissals.\nThese approaches result in a total of 1,000,000 cases for training\nand 200,000 cases for testing.\nComparing Baselines. Our model is evaluated against advanced\nbaselines including LLaMa2-7b [30], Gemma-7b [29], Vicuna-13b\n[37] and Guanaco-13b [6], alongside the larger and more advanced\nGPT-3.5 and GPT-4 models [1]. Each model undergoes the same\ntesting phase to ensure a consistent and fair comparison of their\nmulti-task capabilities within the legal domain.\nImplementation Details. We conducted the training of our model\nover 10 epochs using an A40 GPU. To ensure compatibility, we mon-\nitored the input token size, capping it at 4096 tokens to align with\nGemmaâ€™s maximum token capacity. Additionally, we configured\nthe modelâ€™s dropout rate at 0.1 and set the learning rate to 2ğ‘’âˆ’4.\n4.2 Similar Case Retrieval Results\nAccording to Table 2, LawLLM outperformed the baseline models\nin all categories. Specifically, it achieved the highest accuracy in\ntop-1, top-3, and top-5 retrieval rates, with scores of 29.8%, 63.2%,\nand 81.6% respectively. Remarkably, it also demonstrated minimal\nhallucination, as indicated by the not-found rate of 0.1%.\nTable 2: SCR Test Results\nMethod top-1 â†‘ top-3 â†‘ top-5 â†‘ Not Found â†“\nllama2-7b 0.083 0.197 0.309 0.406\ngemma-7b 0.181 0.428 0.536 0.121\nvicuna-13b 0.185 0.372 0.564 0.187\nguanaco-13b 0.077 0.214 0.375 0.372\ngpt3.5 0.219 0.579 0.691 0.148\ngpt4 0.274 0.526 0.708 0.005\nLawLLM 0.298 0.632 0.816 0.001\nComparatively, GPT-4 showed strong performance with top-1,\ntop-3, and top-5 accuracies of 27.5%, 52.5%, and 70.5%, and a low\nnot-found rate of 0.5%. GPT-3.5 also performed well, especially in\nthe top-3 and top-5 metrics. On the other hand, models like LLaMa2-\n7b and Guanaco-13b displayed higher not-found rates, indicating a\ntendency towards hallucination.\nThe results underscore the effectiveness of our LawLLM model\nin accurately retrieving similar cases while minimizing the risk of\ngenerating irrelevant or nonexistent cases.\n4.3 Precedent Case Recommendation Results\nAccording to Table 3, the LawLLM model again outperformed other\nbaseline methods. It achieved the best results with a top-1 rate of\n31.8%, top-3 rate of 59.7%, and top-5 rate of 83.2%. Additionally, the\nLawLLM model exhibited an low not-found rate of 0.1%.\nAmong the baseline models, GPT-4 was a strong performer, with\nhigh accuracy in top-1, top-3, and top-5 metrics, alongside a very\nlow not-found rate, suggesting reliable and accurate recommenda-\ntions. In contrast, models like LLaMa2-7b and Guanaco-13b showed\nhigher not-found rates, highlighting challenges in providing rele-\nvant case recommendations. The overall results demonstrate the\neffectiveness of theLawLLM model in PCR task, outstripping baseline\nmodels in both accuracy and reliability.\nLawLLM: Law Large Language Model for the US Legal System CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA\nTable 3: PCR Test Results\nMethod top-1 â†‘ top-3 â†‘ top-5 â†‘ Not Found â†“\nllama2-7b 0.069 0.148 0.343 0.479\ngemma-7b 0.187 0.386 0.519 0.124\nvicuna-13b 0.175 0.352 0.506 0.203\nguanaco-13b 0.073 0.198 0.357 0.383\ngpt3.5 0.154 0.325 0.504 0.165\ngpt4 0.262 0.514 0.697 0.007\nLawLLM 0.318 0.597 0.832 0.001\nOne notable insight from comparing SCR and PCR results is that\nmost baseline models exhibited a performance drop in the PCR\ntask compared to SCR. For instance, the GPT-4 model achieved\nscores of 27.4%, 52.6%, 70.8%, 0.5% in SCR top-ğ‘˜ and â€œNot Foundâ€\nmetrics, while in the PCR task, its scores dropped to 26.2%, 51.4%,\n69.7% and 0.7%. This decline underscores the greater difficulty of\nidentifying precedent cases compared to similar cases, as models\ncannot rely solely on textual similarity when determining precedent\nrelationships. Instead, they must consider nuanced factors such as\nlegal relevance. This performance difference reinforces the our\nprevious assertion that precedent cases are distinct from similar\ncases, emphasizing the importance of distinguishing between the\ntwo concepts in the legal domain.\nWe conducted an analysis to identify the factors that are pre-\ndominantly considered by LawLLM when determining a precedent\nrelationship under the top-1, top-3, and top-5 settings. This analysis\ninvolves comparing the frequency with which each factor is cho-\nsen as the primary determinant in our model against the ground\ntruth (GT) distribution. As shown in Table 4, the GT distribution\nis heavily weighted towards the â€˜Case Detailâ€™ factor, with some\ntoward other factors. In the top-1 scenario, where there is only\none correct precedent case among nine similar cases, our model\nstrongly focuses on the â€˜Case Detailâ€™ factor. This bias likely stems\nfrom the GT distributionâ€™s heavy emphasis on â€˜Case Detail, â€™ leading\nour model to prioritize this factor, especially when faced with nu-\nmerous similar cases that serve as potential distractions. However,\nas the pool of correct answers expands to three and five in the\ntop-3 and top-5 scenarios respectively, LawLLM begins to diversify\nits focus slightly to include other factors, although â€˜Case Detailâ€™\ncontinues to dominate. This trend indicates a move towards a more\nbalanced approach in factor consideration as the number of correct\nchoices increases, suggesting that LawLLM adjusts its focus based\non the availability of correct answers, while still reflecting the main\nemphasis observed in the ground truth data.\n4.4 Legal Judgment Prediction Results\nAs shown in Table 5, the LawLLM surpasses all baseline methods in\nboth zero-shot and few-shot scenarios for the LJP task. In the zero-\nshot scenario, LawLLM achieves an accuracy of 0.636 and an F1\nscore of 0.591, significantly outperforming the second best model,\nGPT-4, which scores 0.573 and 0.563 in accuracy and F1, respec-\ntively. In the few-shot scenario, LawLLM maintains its superior\nperformance, reaching an accuracy of 0.794 and an F1 score of 0.758.\nTable 4: Primary Factor Percentage Comparison\nFactor LawLLM LawLLM LawLLM GT\ntop-1 top-3 top-5\nTitle 0.000 0.000 0.000 0.000\nDate 0.000 0.000 0.000 0.000\nJudge 0.027 0.054 0.116 0.149\nPlantiff(s) 0.002 0.009 0.013 0.027\nDefendent(s) 0.004 0.012 0.025 0.041\nCase Detail 0.967 0.925 0.846 0.783\nThese results show a considerable improvement over GPT-4, the\nclosest competitor, which scores 0.732 in accuracy and 0.712 in F1.\nAdditionally, all models demonstrate higher performance in the\nfew-shot in-context learning (ICL) scenario compared to the zero-\nshot setting. For instance, LLaMA2-7b shows an increase from\n0.235 to 0.473 in accuracy, and from 0.239 to 0.455 in F1 score. This\npattern indicates that all models benefit from incorporating a few\nICL examples, which helps them better understand the task.\nTable 5: LJP Test Results\nMethod Accuracy â†‘ F1 â†‘ Accuracy â†‘ F1 â†‘\n(Zero-shot) (Zero-shot) (Few-shot) (Few-shot)\nllama2-7b 0.235 0.239 0.473 0.455\ngemma-7b 0.317 0.287 0.568 0.527\nvicuna-13b 0.503 0.432 0.645 0.594\nguanaco-13b 0.281 0.247 0.491 0.463\ngpt3.5 0.558 0.546 0.679 0.647\ngpt4 0.573 0.563 0.732 0.712\nLawLLM 0.636 0.591 0.794 0.758\n5 Conclusions and Future Work\nIn this study, we introduced the Law Large Language Model (LawLLM),\na multi-task LLM specifically designed for the US legal domain. By\nleveraging unique data processing techniques tailored for each task,\nLawLLM effectively handles Similar Case Retrieval (SCR), Precedent\nCase Recommendation (PCR), and Legal Judgment Prediction (LJP).\nFurthermore, we emphasized the crucial distinctions between prece-\ndent relationships and textual similarity, providing insights that\ncan inform future research in developing task-specific models. Our\nresults consistently demonstrated thatLawLLM outperforms existing\nbaseline models, showcasing its superior multi-task capabilities.\nIn the future, we aim to expand the scope of LawLLM by incorpo-\nrating additional legal tasks to further enhance its versatility and\npractical applicability. This will involve exploring emerging chal-\nlenges in legal analytics and integrating new datasets that reflect\ndiverse legal contexts. Moreover, we plan to refine our data process-\ning techniques and in-context learning methodologies to improve\nthe modelâ€™s understanding of legal nuances and precedents.\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Dong Shu et al.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\n(2023).\n[2] Lang Cao, Zifeng Wang, Cao Xiao, and Jimeng Sun. 2024. PILOT: Legal Case\nOutcome Prediction with Case Law. arXiv preprint arXiv:2401.15770 (2024).\n[3] Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. 2019. Neural legal\njudgment prediction in English. arXiv preprint arXiv:1906.02059 (2019).\n[4] Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo,\nCaio Corro, Andre FT Martins, Fabrizio Esposito, Vera LÃºcia Raposo, Sofia Mor-\ngado, et al. 2024. Saullm-7b: A pioneering large language model for law. arXiv\npreprint arXiv:2403.03883 (2024).\n[5] Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023. Chatlaw: Open-\nsource legal large language model with integrated external knowledge bases.\narXiv preprint arXiv:2306.16092 (2023).\n[6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024.\nQlora: Efficient finetuning of quantized llms. Advances in Neural Information\nProcessing Systems 36 (2024).\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[8] Kawin Ethayarajh. 2019. How contextual are contextualized word representa-\ntions? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. arXiv\npreprint arXiv:1909.00512 (2019).\n[9] Nick Goodson and Rongfei Lu. 2023. Intention and Context Elicitation with Large\nLanguage Models in the Legal Aid Intake Process. ArXiv abs/2311.13281 (2023).\nhttps://doi.org/10.48550/arXiv.2311.13281\n[10] Harvard Law Schoolâ€™s Library Innovation Lab. Accessed in 2024. CaseLaw Project.\nhttps://case.law/. Comprehensive database of U.S. court decisions.\n[11] Cong Jiang and Xiaolei Yang. 2023. Legal Syllogism Prompting: Teaching Large\nLanguage Models for Legal Judgment Prediction. Proceedings of the Nineteenth\nInternational Conference on Artificial Intelligence and Law (2023). https://doi.org/\n10.1145/3594536.3595170\n[12] Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Lizhou Fan, Wenyue Hua,\nSuiyuan Zhu, Yanda Meng, Zhenting Wang, Mengnan Du, et al. 2024. Health-\nllm: Personalized retrieval-augmented disease prediction system. arXiv preprint\narXiv:2402.00746 (2024).\n[13] Mingyu Jin, Qinkai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang,\nMengnan Du, et al. 2024. The impact of reasoning step length on large language\nmodels. arXiv preprint arXiv:2401.04925 (2024).\n[14] Yong-Bin Kang, Shonali Krishnaswamy, and Arkady Zaslavsky. 2013. A retrieval\nstrategy for case-based reasoning using similarity and association knowledge.\nIEEE transactions on cybernetics 44, 4 (2013), 473â€“487.\n[15] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo.\n2024. Gpt-4 passes the bar exam. Philosophical Transactions of the Royal Society\nA 382, 2270 (2024), 20230254.\n[16] Sebastian Lewis. 2021. Precedent and the Rule of Law. Oxford journal of legal\nstudies 41, 4 (2021), 873â€“898.\n[17] Yixiao Ma, Yueyue Wu, Qingyao Ai, Yiqun Liu, Yunqiu Shao, Min Zhang, and\nShaoping Ma. 2023. Incorporating Structural Information into Legal Case Re-\ntrieval. ACM Trans. Inf. Syst. 42, 2, Article 40 (nov 2023), 28 pages. https:\n//doi.org/10.1145/3609796\n[18] Yixiao Ma, Yueyue Wu, Weihang Su, Qingyao Ai, and Yiqun Liu. 2023. CaseEn-\ncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding.arXiv\npreprint arXiv:2305.05393 (2023).\n[19] Arpan Mandal, Kripabandhu Ghosh, Saptarshi Ghosh, and Sekhar Mandal. 2021.\nUnsupervised approaches for measuring textual similarity between legal court\ncase reports. Artificial Intelligence and Law 29 (09 2021), 1â€“35. https://doi.org/10.\n1007/s10506-020-09280-2\n[20] Hugo Mentzingen, Nuno AntÃ³nio, Fernando Bacao, et al. 2023. Automation of\nlegal precedents retrieval: findings from a literature review. International Journal\nof Intelligent Systems 2023 (2023).\n[21] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar,\nMuhammad Usman, Nick Barnes, and Ajmal Mian. 2023. A comprehensive\noverview of large language models. arXiv preprint arXiv:2307.06435 (2023).\n[22] Weicong Qin, Zelin Cao, Weijie Yu, Zihua Si, Sirui Chen, and Jun Xu. 2023.\nIncorporating Judgment Prediction into Legal Case Retrieval via Law-aware\nGenerative Retrieval. arXiv preprint arXiv:2312.09591 (2023).\n[23] Nicolay Rusnachenko, Thanet Markchom, and Huizhi Liang. 2023. nclu_team at\nSemEval-2023 Task 6: Attention-based Approaches for Large Court Judgement\nPrediction with Explanation. In Proceedings of the 17th International Workshop on\nSemantic Evaluation (SemEval-2023) . 270â€“274.\n[24] Ananya B Sai, Akash Kumar Mohankumar, and Mitesh M Khapra. 2022. A survey\nof evaluation metrics used for NLG systems. ACM Computing Surveys (CSUR) 55,\n2 (2022), 1â€“39.\n[25] Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Mengnan Du, and Yongfeng\nZhang. 2024. Knowledge Graph Large Language Model (KG-LLM) for Link\nPrediction. arXiv preprint arXiv:2403.07311 (2024).\n[26] Ruihao Shui, Yixin Cao, Xiang Wang, and Tat-Seng Chua. 2023. A Comprehensive\nEvaluation of Large Language Models on Legal Judgment Prediction. arXiv\npreprint arXiv:2310.11761 (2023).\n[27] Weihang Su, Qingyao Ai, Yueyue Wu, Yixiao Ma, Haitao Li, and Yiqun Liu. 2023.\nCaseformer: Pre-training for Legal Case Retrieval.arXiv preprint arXiv:2311.00333\n(2023).\n[28] ZhongXiang Sun. 2023. A Short Survey of Viewing Large Language Models in\nLegal Aspect. ArXiv abs/2303.09136 (2023). https://doi.org/10.48550/arXiv.2303.\n09136\n[29] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste\nAlayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth,\net al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 (2023).\n[30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[31] Chenlu Wang and Xiaoning Jin. 2020. Study on the multi-task model for legal\njudgment prediction. In2020 IEEE International Conference on Artificial Intelligence\nand Computer Applications (ICAICA) . IEEE, 309â€“313.\n[32] Hao Wu, Songyuan Gu, Zhu Wang, and Yang Weng. 2021. Joint Extraction Meth-\nods for Semantic Retrieval in Chinese Judicial Cases. In 2021 16th International\nConference on Computer Science & Education (ICCSE) . IEEE, 1126â€“1129.\n[33] Yiquan Wu, Siying Zhou, Yifei Liu, Weiming Lu, Xiaozhong Liu, Yating Zhang,\nChanglong Sun, Fei Wu, and Kun Kuang. 2023. Precedent-Enhanced Legal\nJudgment Prediction with LLM and Domain-Model Collaboration. arXiv preprint\narXiv:2310.09241 (2023).\n[34] Yiquan Wu, Siying Zhou, Yifei Liu, Weiming Lu, Xiaozhong Liu, Yating Zhang,\nChanglong Sun, Fei Wu, and Kun Kuang. 2023. Precedent-Enhanced Legal\nJudgment Prediction with LLM and Domain-Model Collaboration. In Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing ,\nHouda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational\nLinguistics, Singapore, 12060â€“12075. https://doi.org/10.18653/v1/2023.emnlp-\nmain.740\n[35] Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, and Maosong Sun. 2021.\nLawformer: A pre-trained language model for chinese legal long documents. AI\nOpen 2 (2021), 79â€“84.\n[36] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An\nexplanation of in-context learning as implicit bayesian inference. arXiv preprint\narXiv:2111.02080 (2021).\n[37] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information\nProcessing Systems 36 (2024).\n[38] Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and\nMaosong Sun. 2020. How Does NLP Benefit Legal System: A Summary of Legal\nArtificial Intelligence. arXiv:2004.12158 [cs.CL]\n[39] Youchao Zhou, Heyan Huang, and Zhijing Wu. 2023. Boosting legal case retrieval\nby query content selection with large language models. In Proceedings of the\nAnnual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval in the Asia Pacific Region . 176â€“184.\nLawLLM: Law Large Language Model for the US Legal System CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA\nA Appendix\nA.1 The Choice of 10\nAccording to Table 6, we have several reasons for giving 10 choices\nto the model in the Similar Case Retrieval (SCR) and Precedent Case\nRecommendation (PCR) tasks:\nâ€¢Given that we utilize top-ğ‘˜ evaluation metrics where ğ‘˜ is 1,\n3, and 5, the number of choices must be greater than 5.\nâ€¢Our exploration revealed that when the number of choices\nexceeds 11, some inputs surpass the maximum token length\nof the Gemma model. Thus, the appropriate range for the\nnumber of choices lies between 6 and 11.\nâ€¢We randomly selected 1,000 test cases to evaluate the perfor-\nmance of LawLLM. From Table 6, we observe that with 6 or 7\nchoices, the modelâ€™s performance in top-5 metric approaches\n100% due to the limited challenge of smaller sets. Therefore,\n6 or 7 choices are not the optimal option. Also, â€œNot Foundâ€\ncases only emerge when the choice size reaches 9 in PCR\ntasks and 10 in SCR tasks. Ultimately, we chose 10 as the\noptimal size, as the model perform similarly, and it provides\nmore challenge to the model.\nTable 6: Choice Size Results\nMethod top-1 top-3 top-5 Not Found\n6 Choices (SCR) 0.493 0.857 0.998 0.000\n7 Choices (SCR) 0.461 0.814 0.982 0.000\n8 Choices (SCR) 0.427 0.779 0.916 0.000\n9 Choices (SCR) 0.354 0.625 0.873 0.000\n10 Choices (SCR) 0.329 0.602 0.848 0.001\n11 Choices (SCR) 0.305 0.571 0.814 0.001\n6 Choices (PCR) 0.478 0.831 0.994 0.000\n7 Choices (PCR) 0.431 0.807 0.973 0.000\n8 Choices (PCR) 0.417 0.742 0.906 0.000\n9 Choices (PCR) 0.362 0.665 0.878 0.001\n10 Choices (PCR) 0.323 0.609 0.839 0.001\n11 Choices (PCR) 0.296 0.584 0.812 0.001\nA.2 Examples\nTo help readers better understand our tasks, we have included\nexample inputs and outputs for each task. Please refer to Table\n7-18.\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Dong Shu et al.\nTable 7: SCR Example\nInput Output\n### Instruction:\nYou are a legal expert who specializes in comparing user-supplied legal cases to a list of candidate\nlegal cases, which includes titles and content. Your main function is to identify and output the title\nof the most similar case from the list based on the description provided.\nYou should only output the case title and not any other information.\nConsider the following choices:\nChoice 1:\n...\nChoice 2:\n...\nChoice 3:\n...\nChoice 4:\nCase Title: BISHOP v. STEWART\nDate: Nov 12, 1940\nCourt: District Court, E.D. Pennsylvania\nJudge: Hopkinson\nPlaintiffs: Bishop Priest, alias Lewis Johnson\nDefendants: Commodore Charles Stewart\nCase detail: This case involved a habeas corpus petition addressed to Commodore Stewart, requiring\nhim to produce the body of Bishop Priest, alias Lewis Johnson, who was alleged to be a minor\nimproperly enlisted in the navy. The case focused on the legality of enlisting minors in the U.S.\nNavy without parental consent. The courtâ€™s decision hinged on the interpretation of various acts\nof Congress regarding naval enlistment and the common law regarding contracts made by minors.\nThe primary legal question was whether the enlistment of minors without parental consent was\nvalid. Ultimately, the court found that the proof of the petitionerâ€™s minority status failed, and he\nwas remanded. The case cited previous decisions and acts of Congress, emphasizing the importance\nand necessity of minors in naval service and the constitutional powers granted to Congress to\nraise and support a navy.\nChoice 5:\n...\nChoice 6:\n...\nChoice 7:\n...\nChoice 8:\n...\nChoice 9:\n...\nChoice 10:\n...\n### Input:\nCase Title: UNITED STATES v. Anderson\nDate: Jan 1, 1869\nCourt: United States Circuit Court for the District of Tennessee\nJudge: McNAIRY, District Judge\nPlaintiffs: George Bigby\nDefendants: Colonel Anderson\nCase Detail: This case involves a writ of habeas corpus directed to Colonel Anderson to bring\nup the body of Zebedee Bigby, who was allegedly enlisted in the army without the necessary\nconsent from his father. The main legal questions addressed were whether the court had the right\nto discharge a soldier before an application to the Secretary of War proved fruitless, and whether\nthe soldier could be discharged when the application was made not by him but by his father. The\ncourt ruled that minors enlisted without parental consent could be discharged on habeas corpus,\nregardless of any application to other government departments.\n### Response:\nBISHOP v. STEWART.\nLawLLM: Law Large Language Model for the US Legal System CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA\nTable 8: SCR Example\nInput Output\n### Instruction:\nYou are a legal expert who specializes in comparing user-supplied legal cases to a list of candidate\nlegal cases, which includes titles and content. Your main function is to identify and output the title\nof the most similar case from the list based on the description provided.\nYou should only output the case title and not any other information.\nConsider the following choices:\nChoice 1:\n...\nChoice 2:\n...\nChoice 3:\n...\nChoice 4:\n...\nChoice 5:\n...\nChoice 6:\n...\nChoice 7:\nCase Title: Robinson v. Campbell\nDate: Jan 1, 1818\nCourt: United States Circuit Court\nJudge: Mr. Justice Todd\nPlaintiffs: The lessor of Robinson\nDefendants: Campbell\nCase Detail: This case involves a dispute over land titles derived from grants issued by the state of\nVirginia, with the land falling within the boundaries of Tennessee following a boundary settlement\nbetween Virginia and Tennessee. The central legal issue was whether an equitable title (settlement\nrights) could be asserted as a valid claim in an action of ejectment in the circuit courts of the\nUnited States, as opposed to being a matter exclusively for equity courts. The plaintiffâ€™s title was\nbased on a grant to John Jones dated 1787, part of which was conveyed to the plaintiffâ€™s lessor. The\ndefendant, Campbell, claimed the land through a subsequent grant dated 1788, which was based\non a settlement right previously established by a settler in 1778 and transferred to Joseph Martin.\nThe defendant attempted to introduce evidence supporting this claim and a deed of conveyance\nduring the pendency of the lawsuit, which could potentially impact the plaintiffâ€™s title. However,\nthe court rejected this evidence on the basis that it was more suitable for consideration in a court\nof equity rather than a law court.s\nChoice 8:\n...\nChoice 9:\n...\nChoice 10:\n...\n### Input:\nCase Title: RULE S LESSEE v. PARKER\nDate: Feb 18, 1815\nCourt: Federal Circuit Court, Nashville\nPlaintiffs: Rule S Lessee\nDefendants: Parker\nCase Detail: This case involves an ejectment proceeding where the title to land was contested. The\nland in question was originally owned by someone named Grant, who then conveyed it to the\nplaintiff. The property was sold for direct taxes as belonging to Grant and was purchased by Smith\nand Bradford. They subsequently sold it to Parker, and the collector issued a deed to Parker. A\nsignificant legal issue in this case was the adequacy of the public notices required by law before\nthe sale. The court focused on whether all statutory requirements for the sale were met to guard\nagainst fraud and imposition, particularly the proper advertisements preceding the sale. The court\nfound that certain procedural safeguards, such as proper advertisement and identification of Grant\nas the owner in the collectorâ€™s book, were not sufficiently demonstrated. This was deemed fatal to\nthe defenseâ€™s case, leading to a verdict for the plaintiff.\n### Response:\nRobinson v. Campbell.\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Dong Shu et al.\nTable 9: SCR Example\nInput Output\n### Instruction:\nYou are a legal expert who specializes in comparing user-supplied legal cases to a list of candidate\nlegal cases, which includes titles and content. Your main function is to identify and output the title\nof the most similar case from the list based on the description provided.\nYou should only output the case title and not any other information.\nConsider the following choices:\nChoice 1:\n...\nChoice 2:\n...\nChoice 3:\nCase Title: The Philadelphia and Trenton Railroad Company vs. James Stimpson\nDate: Jan 1, 1840\nCourt: Circuit Court for the Eastern District of Pennsylvania\nPlaintiffs: The Philadelphia and Trenton Railroad Company\nDefendants: James Stimpson\nCase Detail: This case involves James Stimpson, the defendant in error, who brought an action\nagainst the Philadelphia and Trenton Railroad Company, the plaintiffs in error, for infringing his\npatent right. The patent, initially granted in 1831 and renewed in 1835 after the first was cancelled\ndue to a defective specification, covered a new and useful improvement for turning short curves\non railroads. At trial, significant legal discussions focused on the admissibility and validity of the\nrenewed patent, which lacked specific recitals required by the patent act of 1832. The court ruled\nthat the issuance of the patent under the great seal by high-ranking officials created a presumption\nof regularity and compliance with legal prerequisites, effectively making the patent prima facie\nevidence of its validity. Additionally, objections regarding prior art and the admission of evidence\nwere addressed, with the court reinforcing strict adherence to procedural rules in patent litigation.\nUltimately, the court affirmed the lower courtâ€™s judgment in favor of Stimpson, emphasizing the\ndiscretion courts hold in managing procedural aspects of trials.\nChoice 4:\n...\nChoice 5:\n...\nChoice 6:\n...\nChoice 7:\n...\nChoice 8:\n...\nChoice 9:\n...\nChoice 10:\n...\n### Input:\nCase Title: Pennock v. Dialogue\nDate: Oct 24, 1984\nCourt: United States Supreme Court\nJudge: Mr. Justice Story\nPlaintiffs: Abraham L. Pennock & James Sellers\nDefendants: Adam Dialogue\nCase Detail: This case involves a dispute over a patent related to an improvement in the art of\nmaking leather tubes or hose for conveying various fluids. The plaintiffs, Pennock and Sellers, who\nheld the patent, alleged that Dialogue violated this patent. However, the original trial resulted in\na verdict for Dialogue, which was upheld by the Supreme Court. The Courtâ€™s decision centered\non the principle that public use of an invention without the patent holderâ€™s objection signifies\nan abandonment of the exclusive rights later claimed through a patent. The plaintiffsâ€™ failure to\nsecure a patent while allowing the invention to be used publicly, essentially forfeited their right to\nclaim exclusive rights later. This case underscores the importance of timely patent protection and\nthe implications of public use prior to patent approval.\n### Response:\nThe Philadelphia and Tren-\nton Railroad Company vs.\nJames Stimpson.\nLawLLM: Law Large Language Model for the US Legal System CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA\nTable 10: PCR Example\nInput Output\n### Instruction:\nYou are a legal expert who specializes in comparing user-supplied legal cases to a list of candidate\nlegal cases, which includes titles and content. Your main function is to identify and output the\nprecedent case from the list based on the description provided.\nYou should only output the reasoning process and case title.\nConsider the following choices:\nChoice 1:\n...\nChoice 2:\nCase Title: Clark v. Arnold\nDate: Oct 1, 1803\nCourt: Superior Court of North Carolina, Hillsborough\nJudge: HALL, J.\nPlaintiffs: Trustees of the University (Clark)\nDefendants: Arnold\nCase Detail: In \"Clark v. Arnold\", the dispute centered on the validity of land conveyance practices\nand the impact of deed registration. The case involved the trustees of the University, who claimed\nland under confiscation acts, arguing that the land had belonged to Henry Eustace McCulloch, an\nabsentee during the war. The key issue was whether the deed to the defendant, Arnold, which\nwas delivered but initially taken back by the vendor (McCullochâ€™s attorney) to secure the payment\nof purchase money, effectively transferred title upon its eventual registration. The court ruled\nthat the registration of the deed related back to the time of its initial delivery, thereby confirming\nArnoldâ€™s title before the land could have been confiscated. This verdict upheld the principle that a\nregistered deed, even if initially delivered conditionally, ultimately confirms the transfer of title\nfrom the delivery date, thus rejecting the plaintiffsâ€™ claims based on the confiscation acts.\nChoice 3:\n...\nChoice 4:\n...\nChoice 5:\n...\nChoice 6:\n...\nChoice 7:\n...\nChoice 8:\n...\nChoice 9:\n...\nChoice 10:\n...\n### Input:\nCase Title: PATTON et al. v. REILY\nDate: Jan 1, 1812\nCourt: Circuit Court, D. Tennessee\nJudge: Mâ€™NAIRY, District Judge\nPlaintiffs: PATTON et al.\nDefendants: REILY\nCase Detail: In the case of PATTON et al. v. REILY, the court was faced with the legal intricacies of\nland conveyance through registration. The plaintiffs presented a deed that had not been registered\nat the time of the trial, arguing for its validity based on earlier proofs and registrations. The\ndefendants challenged the legality of the deedâ€™s evidence due to its lack of proper registration as\nmandated by the legislative act of 1715, which clearly required registration for the conveyance of\nlegal estates in lands. This act substituted the common law requirement of livery of seizin with\nregistration. The court ruled that the deed could not be admitted as evidence since it failed to\nmeet the requisite registration requirements set by the legislation, which emphasized that a legal\nestate could only be passed through properly registered deeds. This decision underscored the strict\nadherence to legislative requirements for land conveyance, reflecting the courtâ€™s stance that no\nlegal estate passes without proper procedural compliance, as established in the precedents and\nstatutory framework of property law.\n### Response:\nClark v. Arnold.\nThey have precedent rela-\ntion is due to their similar\ncase details.\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Dong Shu et al.\nTable 11: PCR Example\nInput Output\n### Instruction:\nYou are a legal expert who specializes in comparing user-supplied legal cases to a list of candidate\nlegal cases, which includes titles and content. Your main function is to identify and output the\nprecedent case from the list based on the description provided.\nYou should only output the reasoning process and case title.\nConsider the following choices:\nChoice 1:\n...\nChoice 2:\n...\nChoice 3:\n...\nChoice 4:\n...\nChoice 5:\n...\nChoice 6:\n...\nChoice 7:\n...\nChoice 8:\n...\nChoice 9:\nCase Title: Miller v. White\nDate: March 29, 1934\nCourt: Supreme Court of South Carolina\nJudge: Justice Carter\nPlaintiffs: Thomas E. Miller, Jr.\nDefendants: Benjamin White\nCase Detail: This case concerns an action for specific performance of a written contract wherein\nthe plaintiff, Thomas E. Miller, Jr., agreed to sell a lot in Charleston, SC, to the defendant, Benjamin\nWhite, for $1,800, payable partly in cash and partly secured by a mortgage. Miller alleged that he\ntendered a deed to the property as per the contract, but White refused to comply with the contract\nterms. Whiteâ€™s defense involved alleged defects in Millerâ€™s title, originating from prior foreclosure\nproceedings against other parties, which were not correctly served, potentially affecting the validity\nof the title Miller offered. The trial court sustained a demurrer against Whiteâ€™s defense, leading to\nthis appeal. The Supreme Court found that proper service in the prior foreclosure was crucial for\nestablishing a clear title, disagreed with the trial courtâ€™s dismissal of this defense, and remanded\nthe case for further proceedings, allowing White to prove his defense related to service defects.\nChoice 10:\n...\n### Input:\nCase Title: Ex Parte S.C. Dept. of Social Services\nDate: April 6, 1976\nCourt: Supreme Court of South Carolina\nJudge: Justice Littlejohn\nPlaintiffs: South Carolina Department of Social Services\nDefendants: Not explicitly mentioned, relates to the order concerning Terry Carter\nCase Detail: The case arose from an order by the Richland County Family Court, which directed\nthe South Carolina Department of Social Services (D.S.S.) to fund the enrollment and maintenance\nof Terry Carter, a minor, at Devereaux School in Pennsylvania. Terry Carter had previously\nbeen identified as a runaway and an emotionally disturbed child requiring long-term residential\ntreatment. The Family Courtâ€™s decision aimed to provide necessary treatment for Terry at the\nDevereaux Schools, with expenses estimated at $18,000 annually, which the County Department\nof Social Services argued was beyond their financial capacity. The South Carolina Supreme Court\nreversed the lower courtâ€™s order, concluding that the Family Court lacked the authority to compel\nthe State Department of Social Services to allocate funds for Terryâ€™s treatment outside the state,\nthus invalidating the directive.\n### Response:\nMiller v. White\nThey have precedent rela-\ntion is because they are un-\nder the same court.\nLawLLM: Law Large Language Model for the US Legal System CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA\nTable 12: PCR Example\nInput Output\n### Instruction:\nYou are a legal expert who specializes in comparing user-supplied legal cases to a list of candidate\nlegal cases, which includes titles and content. Your main function is to identify and output the\nprecedent case from the list based on the description provided.\nYou should only output the reasoning process and case title.\nConsider the following choices:\nChoice 1:\n...\nChoice 2:\n...\nChoice 3:\n...\nChoice 4:\n...\nChoice 5:\nCase Title: Simmsâ€™s Lessee v. Baker\nDate: January 1, 1812\nCourt: Circuit Court of the United States, Nashville\nJudge: Mâ€™Nairy, J.\nPlaintiffs: Simmsâ€™s Lessee\nDefendants: Baker\nCase Detail: The case involved an action of ejectment brought by Simmsâ€™s Lessee to recover\npossession of a tract of land granted by North Carolina. The dispute centered on the interpretation\nof boundary descriptions in the land grant, specifically whether the described boundaries could\nextend beyond explicitly stated distances to reach a natural boundary, in this case, Duck River.\nThe plaintiffâ€™s grant started from a point where the lower line of a survey crossed Duck River,\nbut the actual line fell short by one mile and eight poles. The court held that natural boundaries\nmentioned in the grant take precedence over the specified distances. Therefore, even though the\nphysical survey fell short, the grant could extend to the river as intended, validating the plaintiffâ€™s\nclaim up to that boundary. The decision underscored the principle that when a land grant includes\na call for a natural boundary, that boundary governs, even if it contradicts specific measurements\ncited in the grant.\nChoice 6:\n...\nChoice 7:\n...\nChoice 8:\n...\nChoice 9:\n...\nChoice 10:\n...\n### Input:\nCase Title: Doss v. Tenn. Prod. Chem. Corp.\nDate: December 9, 1960\nCourt: Court of Appeals of Tennessee, Middle Section\nJudge: Humphreys, J.\nPlaintiffs: Leonard Doss and Blanche Lee Doss\nDefendants: Tennessee Products Chemical Corporation\nCase Detail: The case involved Leonard Doss and Blanche Lee Doss filing a lawsuit against the\nTennessee Products Chemical Corporation for trespassing and damaging their property through\ntree cutting and strip mining coal. The plaintiffs claimed ownership and actual possession of the\nland, supported by a deed and continuous occupation over several years, sufficient to establish\ntheir claim under the relevant statutes of limitation. The defendant contested the boundaries of the\nproperty, claiming a different boundary line that would exclude their activities from the plaintiffsâ€™\nland. The court found in favor of the plaintiffs, determining that the boundary line supported\nby longstanding recognition and actual possession was as the plaintiffs contended, not as the\ndefendant claimed. The court held that the plaintiffs did not need to establish a valid legal title\nas their actual possession and the absence of valid legal title in the defendant were sufficient.\nThe court affirmed the lower courtâ€™s decision, granting an injunction against further trespass and\nawarding damages for the coal extracted from the property.\n### Response:\nSimmsâ€™s Lessee v. Baker\nThey have precedent rela-\ntion is due to their similar\ncase details.\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Dong Shu et al.\nTable 13: LJP Example (Zero-Shot)\nInput Output\n### Instruction:\nYou are a legal expert who specializes in predicting outcomes for legal cases. Utilize your internal\nknowledge base to predict verdict. Your main function is to anticipate the likely verdict of the\nlegal case presented by the user.\nYou should only output the verdict and not any other information.\nConsider the following choices:\n1. Defendant Wins\n2. Plaintiff Wins\n3. Settlement\n4. Case Dismissal\n### Input:\nCase Title: Newsom v. Newsom\nDate: Decided February 13, 1956\nCourt: Supreme Court of Mississippi\nJudge: Arrington, J.\nPlaintiffs: Mamie Newsom\nDefendants: Other heirs of Mrs. M.J. Newsom\nCase Detail: The case \"Newsom v. Newsom\" involved a dispute over the validity of a deed from\n1922 purportedly given by Mrs. M.J. Newsom to Mamie Newsom. The primary issue was whether\nthe deed had been validly delivered, which is critical for its effectiveness. The appellant, Mamie\nNewsom, argued that the deed was a valid instrument and had been properly delivered, citing a\nstatement in a cross-bill that indicated the deed had been altered after its execution and delivery.\nThe respondents, other heirs of Mrs. M.J. Newsom, contested the validity of the deed, arguing it\nhad not been delivered and had been altered after execution, thereby nullifying it. They also raised\nissues of estoppel and laches to argue that Mamie Newsom was barred from claiming under the\ndeed. The case included several legal debates over the interpretation of these claims and whether\nthey could be properly introduced at different stages of the appeal process.\n### Response:\nPlaintiff Wins\nLawLLM: Law Large Language Model for the US Legal System CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA\nTable 14: LJP Example (Zero-Shot)\nInput Output\n### Instruction:\nYou are a legal expert who specializes in predicting outcomes for legal cases. Utilize your internal\nknowledge base to predict verdict. Your main function is to anticipate the likely verdict of the\nlegal case presented by the user.\nYou should only output the verdict and not any other information.\nConsider the following choices:\n1. Defendant Wins\n2. Plaintiff Wins\n3. Settlement\n4. Case Dismissal\n### Input:\nCase Title: N.Y.S. Rifle & Pistol Assâ€™n, Inc. v. Bruen\nDate: June 23, 2022\nCourt: Supreme Court of the United States\nJudge: Justice Thomas\nPlaintiffs: New York State Rifle & Pistol Association, Inc., et al.\nDefendants: Kevin P. Bruen, in his official capacity as Superintendent of New York State Police, et\nal.\nCase Detail: The case challenged New Yorkâ€™s licensing regime for carrying concealed handguns,\nwhich required applicants to demonstrate a special need for self-protection distinguishable from\nthe general community, known as the \"proper cause\" requirement. The petitioners argued that\nthis standard infringed upon their Second and Fourteenth Amendment rights to bear arms, as it\nrestricted their ability to carry handguns publicly for self-defense. This case arose in the context of\nNew Yorkâ€™s long-standing regulatory approach to the public carry of handguns, starting from\nthe early 20th century Sullivan Law to contemporary requirements that a license applicant\ndemonstrate \"good moral character\" and \"no good cause exists for the denial of the license. \" The\nregulation was contested on the grounds that it was not aligned with the historical tradition of\nfirearm regulation in the United States.\n### Response:\nPlaintiff Wins\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Dong Shu et al.\nTable 15: LJP Example (Zero-Shot)\nInput Output\n### Instruction:\nYou are a legal expert who specializes in predicting outcomes for legal cases. Utilize your internal\nknowledge base to predict verdict. Your main function is to anticipate the likely verdict of the\nlegal case presented by the user.\nYou should only output the verdict and not any other information.\nConsider the following choices:\n1. Defendant Wins\n2. Plaintiff Wins\n3. Settlement\n4. Case Dismissal\n### Input:\nCase Title: Freeman v. South Carolina\nDate: October 6, 2008\nCourt: Supreme Court of the United States\nPlaintiffs: Fred Freeman\nDefendants: South Carolina, et al.\nCase Detail: The case involves Fred Freeman, who petitioned the United States Supreme Court\nseeking a writ of certiorari to review a decision from the United States Court of Appeals for the\nFourth Circuit. The specifics of the underlying case or the issues on appeal are not detailed in the\navailable excerpt. Generally, a petition for a writ of certiorari is requested to appeal a lower courtâ€™s\ndecision to the Supreme Court, indicating that significant legal questions or matters of federal law\nwere likely involved.\n### Response:\nCase Dismissal\nLawLLM: Law Large Language Model for the US Legal System CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA\nTable 16: LJP Example (Few-Shot)\nInput Output\n### Similar Case Example:\nCase Title: J. Aron & Co. v. SemCrude, L.P.\nDate: June 28, 2013\nCourt: United States Bankruptcy Court, D. Delaware\nJudge: Brendan Linehan Shannon\nPlaintiffs: J. Aron & Company, BP Oil Supply Company, et al.\nDefendants: SemCrude, L.P., et al.\nCase Detail: This case arose from SemCrude L.P. â€™s bankruptcy proceedings, where J. Aron & Co.\nand other downstream purchasers filed against SemCrude and associated companies, seeking a\nruling that they purchased oil and gas free of any liens despite SemCrudeâ€™s financial collapse.\nPrior to bankruptcy, SemCrude engaged in substantial trading and midstream oil and gas services,\nwhich faltered due to massive trading losses and a subsequent liquidity crisis. The litigation\naddresses whether downstream purchasers, who bought oil and gas from SemCrude, did so free\nfrom claims by upstream producers who originally supplied the oil and gas. The central legal\nquestion was the applicability of liens and security interests under the U.C.C. and other state laws\nto the transactions made by SemCrude with the plaintiffs.\nVerdict: Case Dismissal\n### Precedent Case Example:\nCase Title: Walker v. Turner\nDate: March 19, 1824\nCourt: Supreme Court of the United States\nJudge: Justice Washington\nPlaintiffs: Walker\nDefendants: Turner\nCase Detail: The case involved a land dispute where Walker, the plaintiff, sought to recover a lot\nin Nashville from Turner, the defendant. Walker based his claim on a deed from 1790. Turner\ndefended his claim with a series of legal and administrative moves starting from 1804, including a\nsheriffâ€™s sale of the property due to a judgment for a small debt against Walker, which resulted\nin Turnerâ€™s predecessor in title acquiring the property. This led to a series of property transfers\nculminating in Turnerâ€™s acquisition and development of the land. Key issues revolved around the\nvalidity of the sheriffâ€™s deed and the application of Tennesseeâ€™s statute of limitations regarding\npossession under such deeds.\nVerdict: Defendant Wins\n### Instruction:\nYou are a legal expert who specializes in predicting outcomes for legal cases. Utilize your internal\nknowledge base to predict verdict. Your main function is to anticipate the likely verdict of the\nlegal case presented by the user.\nYou should only output the verdict and not any other information.\nConsider the following choices:\n1. Defendant Wins\n2. Plaintiff Wins\n3. Settlement\n4. Case Dismissal\n### Input:\nCase Title: MOORE v. BROWN ET AL\nDate: January 1, 1850\nCourt: U.S. Supreme Court\nPlaintiffs: Joshua J. Moore\nDefendants: James Brown, Alfred Brown, Harmon Hogan, and Joseph Froward\nCase Detail: The case centered around a deed issued by the Illinois Auditor of Public Accounts,\npurportedly under authority to sell land for unpaid taxes as per an 1823 act. The deed was challenged\non the basis that it violated statutory requirements, notably because the sale occurred earlier than\npermitted by law. The plaintiffs argued that the deed, showing a sale date that did not comply with\nthe mandatory notice period, was void and could not confer title to the defendants. This raised\nquestions about the application of the Illinois statute of limitations, specifically whether a deed\nvoid on its face due to procedural defects could support a defense of adverse possession under a\ncolor of title.\n### Response:\nDefendant Wins\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA Dong Shu et al.\nTable 17: LJP Example (Few-Shot)\nInput Output\n### Similar Case Example:\nCase Title: Mâ€™Donaldâ€™s Heirs v. Smalley\nDate: January 1, 1832\nCourt: Supreme Court of the United States\nJudge: Chief Justice Marshall\nPlaintiffs: Mâ€™Donaldâ€™s Heirs\nDefendants: Smalley\nCase Detail: The case involved a dispute over land ownership in Ohio, where Mâ€™Donaldâ€™s Heirs\nsought to secure land that was held by Smalley under a senior patent. The plaintiffs claimed the\nland based on a prior entry made in the name of David Anderson, who was deceased at the time of\nthe entry. This prior entry was crucial as it formed the foundation of the plaintiffsâ€™ claim. The case\ncentered on whether an entry made in the name of a deceased person could be valid, a point that\nhad previously been addressed in another case, Galt et al. v. Galloway.\nVerdict: Case Dismissal\n### Precedent Case Example:\nCase Title: De La Vergne Refrigerating Machine Co. v. Featherstone\nDate: Decided January 9, 1893\nCourt: United States Supreme Court\nJudge: Chief Justice Fuller\nPlaintiffs: De La Vergne Refrigerating Machine Co.\nDefendants: Featherstone et al.\nCase Detail: The case centered around the validity of a patent issued after the death of the inventor,\nJames Boyle. The patent was issued to Boyle, \"his heirs or assigns, \" which raised questions about\nits validity since Boyle had died before the patent was granted. This led to a discussion on whether\nthe patent could be validly issued to his legal representatives or heirs under existing patent laws.\nThe case delved into whether the administrative process followed by Boyleâ€™s legal representatives,\nincluding filing amendments and maintaining the application posthumously, adhered to patent\nlaws and whether such actions could legitimately sustain the patentâ€™s validity.\nVerdict: Plaintiff Wins\n### Instruction:\nYou are a legal expert who specializes in predicting outcomes for legal cases. Utilize your internal\nknowledge base to predict verdict. Your main function is to anticipate the likely verdict of the\nlegal case presented by the user.\nYou should only output the verdict and not any other information.\nConsider the following choices:\n1. Defendant Wins\n2. Plaintiff Wins\n3. Settlement\n4. Case Dismissal\n### Input:\nCase Title: Atlas T. W. Mufflers v. McCallum\nDate: January 23, 1929\nCourt: Supreme Court of Texas\nJudge: Justice Speer\nPlaintiffs: Atlas Trailers Water Mufflers, Inc.\nDefendants: Mrs. McCallum, Secretary of State\nCase Detail: This case involved Atlas Trailers Water Mufflers, Inc., seeking a writ of mandamus\nto compel the Secretary of State, Mrs. McCallum, to file the companyâ€™s amended charter, which\nincluded patents valued at $50,000 as capital stock. The Secretary of State had denied the filing\nbased on a long-standing departmental policy that did not recognize patents as tangible property\nsuitable for capitalizing a corporation. The company argued that patents, being property capable\nof assignment and possessing an ascertainable value, should be recognized as valid contributions\ntowards the capital stock under the relevant Texas constitutional and statutory provisions.\n### Response:\nPlaintiff Wins\nLawLLM: Law Large Language Model for the US Legal System CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA\nTable 18: LJP Example (Few-Shot)\nInput Output\n### Similar Case Example:\nCase Title: Stewart v. Griffith\nDate: April 25, 1910\nCourt: Supreme Court of the United States\nJudge: Justice Holmes\nPlaintiffs: The executor of one Ball (Stewart)\nDefendants: Griffith\nCase Detail: This case centers on a dispute over a contract for the sale of real estate, where the\nexecutor of a deceasedâ€™s estate seeks specific performance of a contract made by the appellant to\npurchase land. The contract had provisions that were to result in forfeiture and make the contract\nnull and void if certain conditions were not met. The key issue was whether these conditions\nallowed the appellant to withdraw from the contract or obligated him to complete the purchase\nas per the initial agreement. The executor argued that despite the death of the property owner\njust before the finalization of the sale, the contractual obligations still stood, entitling the estate\nto enforce the contract. The complexities of the case involve interpretations of Maryland real\nestate law, the powers of an executor under a will, and the legal implications of contract terms\nthat stipulate conditions for forfeiture and nullification.\nVerdict: Defendant Wins\n### Precedent Case Example:\nCase Title: Willis v. First Real Estate Investment Co.\nDate: January 24, 1934\nCourt: Circuit Court of Appeals, Fifth Circuit\nJudge: Hutcheson, Circuit Judge\nPlaintiffs: Henry B. Willis\nDefendants: First Real Estate Investment Company and others\nCase Detail: This case involves a property dispute where Henry B. Willis challenged the validity of\nland titles held by the First Real Estate Investment Company and others, based on historical claims.\nThe conflict arises from a Mexican title originating in 1927 and a Texas title from 1861. Willisâ€™s\nclaim is grounded in the assertion that changes in the riverâ€™s courseâ€”specifically the avulsive\nchanges referenced in boundary treatiesâ€”affected the jurisdiction over the land, which was\nlocated along the Texas bank of the Rio Grande. The case examines intricate historical and legal\narguments surrounding land ownership, jurisdictional changes due to natural river movements,\nand the implications of international treaties between the U.S. and Mexico.\nVerdict: Defendant Wins\n### Instruction:\nYou are a legal expert who specializes in predicting outcomes for legal cases. Utilize your internal\nknowledge base to predict verdict. Your main function is to anticipate the likely verdict of the\nlegal case presented by the user.\nYou should only output the verdict and not any other information.\nConsider the following choices:\n1. Defendant Wins\n2. Plaintiff Wins\n3. Settlement\n4. Case Dismissal\n### Input:\nCase Title: San Lorenzo T. I. Co. v. City Mortgage Co.\nDate: June 30, 1934\nCourt: Supreme Court of Texas\nJudge: Justice Pierson\nPlaintiffs: San Lorenzo Title and Improvement Company\nDefendants: City Mortgage Company\nCase Detail: The case involves a trespass to try title suit regarding land along the Rio Grande,\ndesignated as a \"banco\" under treaties between the USA and Mexico. The San Lorenzo Title and\nImprovement Company claimed title to the land, arguing it derived from Mexican governmental\nand court actions before the International Boundary Commission declared the land a banco in\n1930 and stated it belonged to the USA. The core of the dispute rested on the effect of the 1905\ntreaty which aimed to resolve the issues of bancos along the Rio Grande by stipulating those on\nthe north bank would pass to the USA. The company contended that prior Mexican claims to the\nland should be recognized despite the treatyâ€™s provisions.\n### Response:\nDefendant Wins",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5212424397468567
    },
    {
      "name": "Law",
      "score": 0.46401700377464294
    },
    {
      "name": "Political science",
      "score": 0.23250871896743774
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I111979921",
      "name": "Northwestern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I118118575",
      "name": "New Jersey Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I102322142",
      "name": "Rutgers, The State University of New Jersey",
      "country": "US"
    }
  ],
  "cited_by": 16
}