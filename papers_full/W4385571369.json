{
    "title": "Rethinking Semi-supervised Learning with Language Models",
    "url": "https://openalex.org/W4385571369",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2129035059",
            "name": "Zhengxiang Shi",
            "affiliations": [
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A2199241837",
            "name": "Francesco Tonolini",
            "affiliations": [
                "Amazon (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A93365683",
            "name": "Nikolaos Aletras",
            "affiliations": [
                "University of Sheffield",
                "Amazon (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2165725343",
            "name": "Emine Yılmaz",
            "affiliations": [
                "Amazon (United Kingdom)",
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A114700236",
            "name": "Gabriella Kazai",
            "affiliations": [
                "Amazon (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2236555073",
            "name": "Yunlong Jiao",
            "affiliations": [
                "Amazon (United Kingdom)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W3093517588",
        "https://openalex.org/W2061873838",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2104094955",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4224266081",
        "https://openalex.org/W3171608926",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W2962687275",
        "https://openalex.org/W4376864634",
        "https://openalex.org/W4230085078",
        "https://openalex.org/W2970189355",
        "https://openalex.org/W3174231090",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2951970475",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4288020585",
        "https://openalex.org/W4281681567",
        "https://openalex.org/W2970947563",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W3106109117",
        "https://openalex.org/W2163568299",
        "https://openalex.org/W3114610051",
        "https://openalex.org/W3091355780",
        "https://openalex.org/W2122922389",
        "https://openalex.org/W4221158158",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W653435086",
        "https://openalex.org/W4385572905",
        "https://openalex.org/W2950739345",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W4313182274",
        "https://openalex.org/W2995246984",
        "https://openalex.org/W2978426779",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W3168442901",
        "https://openalex.org/W4292122171",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W3205500251",
        "https://openalex.org/W2953070460",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W3102903864",
        "https://openalex.org/W2964159205",
        "https://openalex.org/W2293363371",
        "https://openalex.org/W3035160371",
        "https://openalex.org/W4224253361",
        "https://openalex.org/W3160525311",
        "https://openalex.org/W2891602716",
        "https://openalex.org/W5236451",
        "https://openalex.org/W2964266061",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W4287123151",
        "https://openalex.org/W2952826391",
        "https://openalex.org/W3035542229",
        "https://openalex.org/W2962369866",
        "https://openalex.org/W2963997607",
        "https://openalex.org/W2976223659",
        "https://openalex.org/W3001197829",
        "https://openalex.org/W2101210369",
        "https://openalex.org/W3091002423",
        "https://openalex.org/W3035003500",
        "https://openalex.org/W2963326042",
        "https://openalex.org/W3089635645",
        "https://openalex.org/W4285123725",
        "https://openalex.org/W2296073425",
        "https://openalex.org/W2108281845"
    ],
    "abstract": "Semi-supervised learning (SSL) is a popular setting aiming to effectively utilize unlabelled data to improve model performance in downstream natural language processing (NLP) tasks. Currently, there are two popular approaches to make use of the unlabelled data: Self-training (ST) and Task-adaptive pre-training (TAPT). ST uses a teacher model to assign pseudo-labels to the unlabelled data, while TAPT continues pre-training on the unlabelled data before fine-tuning. To the best of our knowledge, the effectiveness of TAPT in SSL tasks has not been systematically studied, and no previous work has directly compared TAPT and ST in terms of their ability to utilize the pool of unlabelled data. In this paper, we provide an extensive empirical study comparing five state-of-the-art ST approaches and TAPT across various NLP tasks and data sizes, including in- and out-of domain settings. Surprisingly, we find that TAPT is a strong and more robust SSL learner, even when using just a few hundred unlabelled samples or in the presence of domain shifts, compared to more sophisticated ST approaches, and tends to bring greater improvements in SSL than in fully-supervised settings. Our further analysis demonstrates the risks of using ST approaches when the size of labelled or unlabelled data is small or when domain shifts exist, and highlights TAPT as a potential solution.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 5614–5634\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nRethinking Semi-supervised Learning with Language Models\nZhengxiang Shi\n1 ∗ Francesco Tonolini2 Nikolaos Aletras2,3 Emine Yilmaz1,2\nGabriella Kazai2 Yunlong Jiao2\n1 University College London, London, United Kingdom\n2 Amazon, London, United Kingdom\n3 University of Sheffield, Sheffield, United Kingdom\nzhengxiang.shi.19@ucl.ac.uk\n{tonolini,eminey,aletras,gkazai,jyunlong}@amazon.com\nAbstract\nSemi-supervised learning (SSL) is a popular set-\nting aiming to effectively utilize unlabelled data\nto improve model performance in downstream\nnatural language processing (NLP) tasks. Cur-\nrently, there are two popular approaches to\nmake use of unlabelled data: Self-training (ST)\nand Task-adaptive pre-training (TAPT ). ST\nuses a teacher model to assign pseudo-labels\nto the unlabelled data, while TAPT continues\npre-training on the unlabelled data before fine-\ntuning. To the best of our knowledge, the ef-\nfectiveness of TAPT in SSL tasks has not been\nsystematically studied, and no previous work\nhas directly compared TAPT and ST in terms\nof their ability to utilize the pool of unlabelled\ndata. In this paper, we provide an extensive em-\npirical study comparing five state-of-the-art ST\napproaches and TAPT across various NLP tasks\nand data sizes, including in- and out-of-domain\nsettings. Surprisingly, we find that TAPT is a\nstrong and more robust SSL learner, even when\nusing just a few hundred unlabelled samples or\nin the presence of domain shifts, compared to\nmore sophisticated ST approaches, and tends\nto bring greater improvements in SSL than in\nfully-supervised settings. Our further analysis\ndemonstrates the risks of using ST approaches\nwhen the size of labelled or unlabelled data\nis small or when domain shifts exist. We of-\nfer a fresh perspective for future SSL research,\nsuggesting the use of unsupervised pre-training\nobjectives over dependency on pseudo labels.1\n1 Introduction\nPre-training ( PT) language models (LMs) (De-\nvlin et al., 2019; Liu et al., 2019; Radford et al.,\n2019) over large amounts of text data (e.g. with\nmasked language modelling) and then fine-tuning\non task-specific labelled data offer large perfor-\nmance gains across NLP tasks. Semi-supervised\n∗ This work was done during an internship at Amazon,\nAlexa Shopping.\n1 Code is available at https://github.com/amzn/\npretraining-or-self-training .\nlearning (SSL) (Grandvalet and Bengio, 2004;\nChapelle et al., 2009; Kipf and Welling, 2017) is\na powerful and effective approach to utilize unla-\nbelled data. A typical SSL setting assumes access\nto a (relatively small) labelled training set and an\n(often large) unlabelled set. The goal of SSL is\nto make effective use of the unlabelled data to im-\nprove model (i.e. LMs) performance.\nIn NLP, Self-training (ST) approaches have been\nproposed to produce pseudo labels for unlabelled\nexamples to train the model (e.g. in Yarowsky,\n1995; McClosky et al., 2006). With the advent\nof neural networks, ST approaches typically focus\non using student-teacher models to assign pseudo-\nlabels to the unlabelled data (e.g. in Artetxe et al.,\n2018; Cai and Lapata, 2019; Dong and de Melo,\n2019; Xie et al., 2020a; Gera et al., 2022). Apart\nfrom the sophisticated ST approaches, Gururangan\net al. (2020) proposed task adaptive pre-training\n(TAPT ), which is a straightforward yet effective\nmethod for utilising unlabelled examples. This\nmethod involves continuing pre-training the LM\non the task-specific data without using labels, be-\nfore proceeding with fully-supervised fine-tuning.\nTAPT and ST are both motivated by the need for\neffectively leveraging unlabelled examples, rais-\ning the questions of how TAPT performs in SSL\ntasks, as well as how these two approaches perform\nagainst each other.\nIn this work, we investigate the performance of\nTAPT against five state-of-the-art ST approaches\nacross five NLP tasks (§4). We empirically show\nthat TAPT outperforms all state-of-the-art ST ap-\nproaches on several tasks, suggesting that it should\nserve as a strong baseline for SSL methods. Previ-\nous research (Gururangan et al., 2020) has shown\nthat TAPT can improve performance in fully-\nsupervised settings. Our study goes further by\nshowing that TAPT can be even more effective\nin SSL settings (§4).\nWe next study the impact of using different\n5614\namounts of labelled and unlabelled data for SSL\n(§5). Our experiments show that ST approaches\nare prone to suffering from insufficient labelled or\nunlabelled data, while TAPT is more robust across\ndifferent combinations of labelled and unlabelled\ndata sizes. Contrary to the common assumption\nthat TAPT requires a large amount of data to per-\nform well (e.g. Li et al., 2021b; Hou et al., 2022),\nour results show that TAPT improves performance\nwith just a hundred unlabelled samples. We con-\nduct further analysis on the impact of domain shifts\nin labelled or unlabelled data. WhileST approaches\ngenerally suffer from domain shifts, TAPT is more\nrobust and even benefits from domain shifts (§6).\nIn summary, the main contributions of this paper\nare as follows:\n• An extensive empirical study to directly com-\npare five state-of-the-art ST approaches and\nTAPT across various NLP tasks in SSL, with\nvarying amounts of labelled and unlabelled\ndata as well as the effect of domain shifts;\n• Practical insights learned about the limitations\nof ST approaches, alongside an exploration of\nthe often-unrecognized yet impressive capac-\nity of TAPT as a simple, stable and powerful\nSSL learner;\n• A fresh perspective for future SSL research\nby demonstrating that leveraging unsuper-\nvised signals from unlabelled texts presents a\npromising and effective approach alternative\nto dependence on pseudo labels.\n2 Preliminaries\n2.1 Task Adaptive Pre-training (TAPT )\nLMs are adapted to downstream NLP tasks by fine-\ntuning (FT) on task-specific data. TAPT introduces\na simple additional step before fine-tuning by con-\ntinuing pre-training with a masked language mod-\nelling (MLM) objective (Devlin et al., 2019; Liu\net al., 2019) on the task-specific data without re-\nquiring labels. The main advantage of TAPT is that\nit provides a simple way for the LM to explore\nthe task space while it can easily make use of all\navailable labelled and unlabelled data.\n2.2 Self-training (S T)\nThe core idea behind ST approaches is to utilise\na teacher model trained on labelled examples to\nmake predictions for unlabelled examples, and train\na new student model with these predictions. For-\nmally, let L ≜ {(x1, y1), . . . ,(xn, yn)}denote n\nlabelled examples and U ≜ {˜x1, . . . ,˜xm}denote\nm unlabelled examples, where usually m ≫n.\nThe ST framework is trained with three main steps\nas follows.\nStep 1. A teacher model F, parameterized by a\nneural network Θ, is trained via minimizing the\ncross entropy loss ℓ on labelled examples L:\nLteacher(L) =\n∑\nxi,yi∈L\nℓ(yi, F(xi, Θ)), (1)\nStep 2. The teacher model F is used to make\npredictions (referred to as “pseudo-labels”) on un-\nlabelled examples U:\n˜yi = F( ˜xi, Θ), (2)\nwhere ˜yi can be either the continuous logit or the\ndiscrete label induced by an ARGMAX operation.\nStep 3. A student model G, parameterized by a\nfresh neural network Φ, is trained to fit labelled and\npseudo-labelled examples:\nLstudent(L, U) =\n∑\nxi,yi∈L\nℓ(yi, F(xi, Φ))\n+\n∑\n˜xi, ˜yi∈U\nℓ( ˜yi, F( ˜xi, Φ)) (3)\nThis process is repeated for a given number of\ntimes by treating the student as a new teacher to\nre-predict pseudo-labels as in eq. (2) and then train-\ning a new student with eq. (3). In practice, ST\nwith techniques such as consistency regularization\n(Miyato et al., 2018; Clark et al., 2018; Berthelot\net al., 2019b), strong data augmentation (Sohn et al.,\n2020; Xie et al., 2020b,a), confidence threshold\n(Sohn et al., 2020; Zhang et al., 2021; Berthelot\net al., 2022) usually leads to substantial improve-\nments in model performance.\n3 Experimental Setup\nDatasets. We experiment with five datasets used\nin previous related work forSSL (Gururangan et al.,\n2019; Chen et al., 2020b; Xie et al., 2020a; Li\net al., 2021a; Gera et al., 2022), including IMDB\n(Maas et al., 2011), SST-2 (Wang et al., 2018),AG\nNEWS (Zhang et al., 2015), AMAZON REVIEW\n(McAuley and Leskovec, 2013), and YAHOO ! A N-\nSWER (Chang et al., 2008). Table 1 shows data\n5615\nDataset Task Type Train Size Dev. Size Test Size |Y| L\nIMDB (Maas et al., 2011) Movie Review Sentiment 23,000 2,000 25,000 2 149\nSST-2 (Wang et al., 2018) Movie Review Sentiment 60,000 7,349 872 2 37\nAG NEWS(Zhang et al., 2015) News Topic Classification 100,000 10,000 7,600 4 134\nAMAZONREVIEW(McAuley and Leskovec, 2013) Product Review Sentiment 250,000 25,000 650,000 5 79\nYAHOO! ANSWER(Chang et al., 2008) Topic Classification 500,000 50,000 60,000 10 32\nTable 1: Statistics of datasets. |Y|: # of classes for classification tasks. L: average # of words in input sentence(s).\nNote that we only sample examples from the original training set in our experiments.\nstatistics. We also provide descriptions and ex-\namples of datasets in Appendix §A.1. We show\nthe process for quantifying the similarity between\ndatasets in Appendix §A.2. Adhering to previous\nwork (e.g. Chen et al., 2020b; Wang et al., 2022),\nwe sample the same amount of labelled data per\nclass from the train set, given the labelled size, to\nform the labelled set. We re-sample the labelled\ndata using the same five seeds for all different ap-\nproaches and report the average performance with\nan error bar.\nTAPT . Our approach to task adaptive pre-\ntraining (TAPT ) using ROBERTA-BASE (Liu et al.,\n2019) is to further pre-train on the training text\ncorpus including labelled and unlabelled data (see\nTable 12 in Appendix for hyperparameter details).\nThe model is then fine-tuned on the labelled data\nwhere the [CLS] token representation is passed to\nan extra feed-forward layer for classification (see\nTable 13 in Appendix for hyperparameter details).\nThe process of TAPT + FINE -TUNING is simply\ndenoted by TAPT henceforth.\nST. We implement five state-of-the-art ST ap-\nproaches, including V AT (Miyato et al., 2018), Fix-\nMatch (Sohn et al., 2020), Dash (Xu et al., 2021b),\nFlexMatch (Zhang et al., 2021), and AdaMatch\n(Berthelot et al., 2022) (see descriptions of these\napproaches in Appendix §B). We use ROBERTA-\nBASE as the backbone, and the [CLS] token represen-\ntation with an extra feed-forward layer is used for\nclassification (see Table 14 in Appendix for hyper-\nparameter details). Adhering to previous work (Xie\net al., 2020a; Wang et al., 2022), back-translation\n(Ott et al., 2019) is used for data augmentation.\nBaselines. For reference, we also evaluate two\nbaseline models that are only fine-tuned (from\nan off-the-shelf ROBERTA-BASE checkpoint) on:\n(1) the same labelled set as TAPT and ST\n(SUPERVISED ); and (2) the whole training set\n(FULLY-SUPERVISED ).\n4 S T vs TAPT\nOverview. Table 2 shows the performance of\nTAPT against five state-of-the-art ST approaches\nand the baselines ( SUPERVISED and FULLY-\nSUPERVISED ) across five datasets, each with two\ndifferent sizes of labelled data for training follow-\ning Wang et al. (2022). Overall, we observe that:\n(1) TAPT achieves highly competitive results com-\npared with state-of-the-art ST approaches; and (2)\nTAPT gains more improvement compared to the\nSUPERVISED baselines when using fewer labelled\nsamples.\nFor our first finding, the experimental results\nshow that TAPT outperforms all five state-of-the-\nart ST approaches with lower variances on AMA-\nZON REVIEW , and YAHOO ! A NSWER , as shown\nin Table 2. For example, TAPT obtains a F1 score\nof 68.8% compared to the best ST approach’s F1\nscore of 68.0% (using 500 labelled samples) and\n71.5% compared to ST’s 69.6% (using 2000 la-\nbelled samples) on YAHOO ! A NSWER . For an ex-\nample of the second finding, TAPT gains 3.6% F1\nimprovement over SUPERVISED (using 20 labelled\nsamples) compared to 2.2% (using 100 labelled\nsamples) on IMDB . Below we delve deeper into\nthese two findings and discuss them in more detail.\n#1. TAPT is a strong semi-supervised learner\nand can outperform state-of-the-art ST ap-\nproaches. Figure 1 shows how the performance\nof ST, TAPT , and SUPERVISED vary with respect to\nfive different labelled sizes on each dataset, where\ntwo latest ST approaches (ADAMATCH and FLEX -\nMATCH) are selected as representatives for ST. Ex-\nperimental results further verify that TAPT has a\nconsistent advantage over ADAMATCH and FLEX -\nMATCH across different labelled sizes onAMAZON\nREVIEW and YAHOO ! A NSWER . It is also worth\nnoting that, while TAPT brings a stable improve-\nment over SUPERVISED across all datasets with\nvarying labelled sizes, ST can sometimes bring\nmore substantial improvement, for example when\n5616\nMethod IMDB SST-2 AG N EWS AMAZONREVIEW YAHOO! ANSWER\n20 100 40 100 40 200 250 1000 500 2000\nSTApproaches\nVAT 90.2 0.9 92.00.4. 75.012.0 86.23.4 87.51.0 89.50.7 52.21.3 57.50.2 66.90.5 68.60.2\nFIXMATCH 93.40.1 93.40.1 37.38.5 66.421.3 75.68.7 88.80.6 55.91.1 59.00.5 67.51.0 69.60.4\nDASH 93.20.3 93.40.2 38.210.1 73.318.6 74.36.6 88.50.6 56.61.8 59.30.2 67.61.0 69.50.3\nFLEXMATCH 93.30.1 93.40.1 40.67.7 83.08.3 80.64.4 88.20.5 54.93.9 58.80.4 66.60.7 68.70.4\nADAMATCH 94.40.4. 94.70.2 42.613.3 83.14.4 82.75.9 88.60.4 55.52.8 59.00.7 68.00.7 69.50.3\nSUPERVISED 83.37.4 88.70.2 74.76.1 84.02.7 84.61.6 88.00.8 53.10.7 57.20.1 65.40.3 68.50.3\n+ TAPT 86.92.8 90.90.6 82.64.0 85.42.4 84.01.3 88.70.7 58.40.7 60.60.1 68.80.7 71.50.3\nFULLY-SUPERVISED 93.90.1 93.00.6 94.80.1 65.00.2 75.30.2\n+ TAPT 94.00.2 93.50.3 95.00.1 65.60.1 75.40.1\nTable 2: Performance of TAPT , ST approaches and the baselines across five datasets using two different sizes of the\ntraining labelled data. We report average Macro-F1 on the test set across five seeds, with standard deviations in\nsubscripts. Blue and orange represent the best and second-best performance in a column respectively.\n102 103 104\nLabelled Size\n84\n86\n88\n90\n92\n94\nMacro-F1 (%)\n(a) IMDB\n102 103 104\nLabelled Size\n84\n86\n88\n90\n92\n(b) SST-2\n102 103 104\nLabelled Size\n80\n82\n84\n86\n88\n90\n92\n94\n(c) AG NEWS\n103 104 105\nLabelled Size\n54\n56\n58\n60\n62\n64\n66\n(d) AMAZON REVIEW\n103 104 105\nLabelled Size\n66\n68\n70\n72\n74\n(e) YAHOO ANSWERS\nadamatch\nflexmatch\nsupervised\ntapt\nFigure 1: The effect of labelled size on TAPT and ST. Average test Macro-F1 score over 5 seeds is reported. From\nthe left to the right, TAPT and ST utilizes 23k, 60k, 100k, 250k, and 500k unlabelled samples respectively.\n10 3\n 10 2\n 10 1\n 100\nLabelled Size / Unlabelled Size\n0\n2\n4\n6\n8\n10\nMacro-F1 Improvement (%)\nImprovement from TAPT\nImdb\nSST-2\nAg News\nAmazon Review\nYahoo Answers\nFigure 2: The impact of labelled size on theF1 improve-\nment from TAPT over SUPERVISED , where unlabelled\nsize is fixed for each dataset. The red vertical line high-\nlights the FULLY-SUPERVISED setting on which prior\nwork (Gururangan et al., 2020) focuses.\nonly a few hundreds of labelled samples are avail-\nable from IMDB . However, we do not observe\nsimilar phenomena for ST on other datasets. Our\nexperimental results demonstrate that TAPT is a\nsimple, effective and strong learner for SSL tasks,\nand it should serve as a baseline for SSL tasks in\nNLP.\n#2. TAPT tends to bring more improvements in\nSSL than inFULLY-SUPERVISED setting. We\nfurther study the behaviour of TAPT itself under\nSSL, where we select SUPERVISED as the base-\nline rather than ST approaches. Figure 1 shows\nthat the differences in performance (in absolute val-\nues) between TAPT (red lines) and SUPERVISED\n(green lines) generally increase as the labelled size\ndecreases. To gain a better understanding of the im-\npact of labelled data sizes, we plot the improvement\nfrom TAPT over SUPERVISED (in percentages)\nagainst the ratio between labelled size and unla-\nbelled size (unlabelled size is fixed for each dataset)\nin Figure 2. We see that TAPT improves over SU-\nPERVISED further as the ratio of labelled and un-\nlabelled sizes decreases, highlighting the trends of\ngaining greater improvement in low-resource SSL\nsetting. This finding is complementary to prior\nworks (e.g. in Howard and Ruder, 2018; Gururan-\ngan et al., 2020) that focus onTAPT ’s improvement\nfrom the FULLY-SUPERVISED perspective, repre-\nsented by the rightmost red vertical line in Figure 2.\nThe rising trend of the improvement is not mono-\ntonic as the labelled size is reduced. Rather it could\nprovide insight into how TAPT improves over SU-\nPERVISED in SSL and inspire the design of new\n5617\n100 500 2k 10k 23k\nUnlabelled Size\n20k\n5k\n1k\n200\n20\n10\n4\nLabelled Size\n-0.3\n-1.0 -0.6\n-1.0 -1.2 -1.0\n-0.3 -1.6 -2.1 -1.8\n0.0 -3.0 -6.1 -9.6 -7.0\n23.1 -9.9 8.7 7.3 -16.0\n26.1 29.8 29.8 30.1 23.5\nImdb\n100 500 2k 10k 60k\nUnlabelled Size\n20k\n5k\n1k\n200\n40\n-0.8\n0.2 -1.0\n-0.2 0.9 0.1\n1.4 5.3 1.8 -1.0\n21.4 15.7 22.6 29.0 40.0\nSST-2\n1k 5k 20k 100k 250k\nUnlabelled Size\n50k\n10k\n2k\n500\n100\n20\n0.5 1.1\n1.0 1.7 2.2\n0.7 -0.2 0.7 1.6\n1.8 0.7 0.1 1.5 2.2\n5.1 -0.0 -2.8 1.1 -0.3\n6.3 9.6 12.7 15.5 7.5\nAmazon Review\n1k 5k 20k 100k 500k\nUnlabelled Size\n50k\n10k\n2k\n500\n100\n20\n10\n1.4 1.6\n2.3 1.9 2.4\n2.9 1.8 0.8 2.0\n3.8 2.2 1.2 -0.3 0.8\n5.4 1.9 -1.0 -0.6 -1.4\n6.2 1.9 -4.7 -2.4 1.8\n10.8 13.6 6.4 12.9 12.2\nYahoo Answers\n20\n10\n0\n10\n20\n30\n5\n0\n5\n10\n15\n20\n25\n30\n35\n40\n5\n0\n5\n10\n15\n20\n5\n0\n5\n10\n15\n20\nFigure 3: Performance difference between TAPT and ST with varying labelled and unlabelled sizes on IMDB ,\nSST-2 , AMAZON REVIEW and YAHOO ! A NSWER . Positive values indicate that TAPT performs better, while\nnegative values indicate that ST performs better. Average Macro-F1 score on test sets over five seeds is reported.\napproaches.\n5 Exploring the limits of ST and TAPT\nIn §4, our experimental results showed inconsistent\nresults across datasets. For example, ST performs\nbetter on IMDB while TAPT achieves better results\non AMAZON REVIEW and YAHOO ! A NSWER . We\nhypothesize that this might be attributed to the ex-\nposure to different sizes of labelled or unlabelled\ndata. To verify this hypothesis and shed light on\nthe differences in performance between datasets,\nwe compare TAPT and ST (using ADAMATCH and\nFLEX MATCH as representatives) by sampling dif-\nferent labelled and unlabelled sizes inIMDB , SST-\n2, AMAZON REVIEW and YAHOO ! A NSWER .\nFigure 3 visualizes the differences in perfor-\nmance between TAPT and ST, where each cell rep-\nresents the macro- F1 performance difference of\nTAPT over ST (averaged across five seeds). In each\ncase, the highest performance amongFLEX MATCH\nand ADAMATCH is selected to represent the perfor-\nmance of ST. Overall, we observe that: (1) TAPT\nimproves the fine-tuning performance even with a\nfew hundred unlabelled examples; and (2) TAPT\nperforms more stable across the different labelled\nand unlabelled data sizes than ST approaches. Be-\nlow we provide a comprehensive analysis of the\nimpact of labelled and unlabelled sizes.\n#1. TAPT works even with a few hundred un-\nlabelled samples. It is generally assumed that\nTAPT requires a large amount of unlabelled data to\nperform well (e.g. Li et al., 2021b; Hou et al., 2022).\nHowever, we surprisingly observe that TAPT can\nbring substantial improvement over SUPERVISED\nbaseline even with a relatively small number of\nunlabelled samples, as shown in Figure 5. To ex-\nplore the effectiveness of TAPT over SUPERVISED\nin the low-resource setting of unlabelled data, we\nIMDB SST-2\nAmazon ReviewYahoo Answers\n40\n60\n80Macro-F1 (%)\n71.7 74.2 76.4 78.7\n45.3 45.5 43.0\n48.5\nTAPT in Low-Resource Setting\nSupervised\nTAPT\nFigure 4: The performance of TAPT against the SUPER -\nVISED baseline in the low-resource setting of unlabelled\ndata. From the left to the right, TAPT utilizes 100, 100,\n1 000, and 1 000 unlabelled samples respectively.\nselect the performance of TAPT and SUPERVISED\nfrom the first column (the lowest unlabelled size)\nfor each dataset in Figure 3 and plot their average\nperformance over different labelled sizes. Figure 4\nshows that TAPT improves over the SUPERVISED\nbaseline with just one hundred or one thousand\nsamples. For instance, TAPT achieves a 5.5% in-\ncrease in F1 score compared to the SUPERVISED\nbaseline when using only 1k unlabelled samples on\nYAHOO ! A NSWER . Additionally, this performance\nis achieved without the need for large amounts of\ntokens in each sample, as training samples from\nSST-2 , on average, contain only 9 tokens and train-\ning samples from YAHOO ! A NSWER contain about\n32 tokens (see examples in Table 6 of Appendix).\n#2. Scarce labelled data and adequate un-\nlabelled data. TAPT appears to be a more\nfavourable choice than ST approaches in this set-\nting. The bottom of each sub-figure in Figure\n3 shows a clear labelled size boundary, below\nwhich FLEX MATCH and ADAMATCH are outper-\nformed by TAPT with a large margin, regardless\nof datasets and unlabelled size used. This sug-\ngests that ST might not be able to efficiently handle\nlarge amounts of unlabelled data if labelled data\n5618\n103\n104\n105\nUnlabelled Size\n45\n50\n55\n60\n65\nMacro-F1 (%)\nAmazon Review\nLabelled Size | Algorithm\n50k | st\n50k | tapt\n10k | st\n10k | tapt\n2k | st\n2k | tapt\n500 | st\n500 | tapt\n100 | st\n100 | tapt\nst\ntapt\nFully | Supervised\n50k | Supervised\n10k | Supervised\n2k | Supervised\n500 | Supervised\n100 | Supervised\n103\n104\n105\nUnlabelled Size\n55\n60\n65\n70\n75\nYahoo Answers\nFigure 5: The performance of ST and TAPT using dif-\nferent unlabelled sizes. Average test results across five\nseeds are reported where the best result from FLEX -\nMATCH and ADAMATCH is selected to represent ST.\ndo not provide adequate information. This might\nbe attributed to confirmation bias (Tarvainen and\nValpola, 2017; Arazo et al., 2020), which results\nfrom the accumulation of errors in the iterative ST\nprocess caused by incorrect pseudo-labels.\nThe specific value of adequate labelled size\nboundary for ST approaches depends on the na-\nture of the dataset. For example, even though both\nIMDB and SST-2 are binary classification tasks\nfor movie review sentiment analysis, the labelled\nsize boundary for SST-2 is higher (40 > 4), indi-\ncating that this boundary tends to increase as the\ntask becomes more challenging. While it may be\neasy to obtain dozens of labelled data in this case,\nwhen the task becomes more intricate or contains\nnoisy weak labels, it is important to be aware of this\npotential issue with ST approaches. TAPT could\nserve as an alternative in situations where collect-\ning adequate labelled data for training is costly.\nWe provide specific values of the performance of\nST and TAPT , and further verify that this finding\napplies to other ST approaches in Appendix §D.\n#3. Adequate labelled data and scarce unla-\nbelled data. In this setting, TAPT is more ro-\nbust, while ST has a greater chance of performing\nworse than the SUPERVISED baseline. In Figure\n5, we plot the performance of ST approaches and\nTAPT against five different sizes of unlabelled data,\ngrouped by size (using similar colours). We note\n#Unl. 10 50 100 500\nFLEXMATCH 57.317.9 35.23.4 45.122.5 33.40.1\nADAMATCH 53.322.1 36.86.1 33.50.2 33.60.3\nTable 3: Test results onIMDB with 4 fixed labelled data.\nAn average Macro-F1 score over five seeds is reported.\nthat ST approaches perform worse than their corre-\nsponding SUPERVISED baselines (represented by\nhorizontal lines) until a certain amount of unla-\nbelled data has been reached. For example, when\nthe labelled size is 500, ST requires about 20k\nunlabelled samples to achieve the corresponding\nSUPERVISED baseline performance on YAHOO !\nANSWER . On the other hand, TAPT generally out-\nperforms SUPERVISED baselines demonstrating its\nrobustness across various unlabelled sizes.\nTo further quantify the model performance in\ncase of scarce unlabelled and adequate labelled\ndata, we choose the three lowest unlabelled sizes\n(the first three columns) excluding the lowest la-\nbelled size (the last row) in Figure 3 for each\ndataset. Our analysis shows that ST has 67%, 56%\nand 54% probability of falling below the SUPER -\nVISED baselines on SST-2 , AMAZON REVIEW ,\nand YAHOO ! A NSWER respectively. Even on\nIMDB where ST generally performs well, it still\nhas a probability of 33% to fall behind SUPER -\nVISED . In contrast, TAPT never performs worse\nthan SUPERVISED in those cases. We provide com-\nputation details and comparative statistics in Ap-\npendix §C.\nThe specific value of adequate unlabelled size\nboundary for ST approaches depends on the nature\nof the dataset as well as the labelled size. Figure\n5 illustrates that as the size of the labelled data\nincreases, ST approaches require more unlabelled\ndata to surpass the SUPERVISED baselines. For\nexample, on AMAZON REVIEW , ST trained with\n100 labelled samples requires about 5k unlabelled\nsamples to perform better than SUPERVISED , while\nST trained with 10k labelled samples requires about\n100k unlabelled samples. Adjusting the unlabelled\nsize accordingly might be conducive to exploiting\nthe full potential of ST approaches.\n#4. Scarce labelled and unlabelled data.When\nthe labelled data is insufficient, increasing unla-\nbelled size is not helpful or even detrimental to ST\napproaches. This finding is well-illustrated in the\nlast row of results on SST-2 shown in Figure 3. In\n5619\nFlexMatchAdaMatch TAPT Supervised\n0\n20\n40\n60\n80\n100\nMacro-F1 (%) 93.4\n33.3\n94.7\n33.3\n90.9 89.0 88.7 84.5\nT arget Domain: IMDB | Labelled Size: 100\nIMDB SST-2\nFlexMatchAdaMatch TAPT Supervised\n0\n20\n40\n60\n80\n100 93.5\n33.3\n93.6\n33.3\n91.5 90.1 90.4 87.0\nT arget Domain: IMDB | Labelled Size: 200\nIMDB SST-2\nFlexMatchAdaMatch TAPT Supervised\n0\n20\n40\n60\n80\n100\n83.0\n33.4\n83.1\n33.4\n86.4 89.6 84.0 85.7\nT arget Domain: SST-2 | Labelled Size: 100\nSST-2 IMDB\nFlexMatchAdaMatch TAPT Supervised\n0\n20\n40\n60\n80\n100\n87.2\n33.0\n89.5\n33.0\n88.6 89.7 86.8 87.5\nT arget Domain: SST-2 | Labelled Size: 200\nSST-2 IMDB\nFigure 6: Results of UDA experiments. Legends indicate domains of labelled training data. Orange/green represents\nthe performance with/without domain shift. Average Macro-F1 score on test sets over five seeds is reported.\nTrain (Lab.) Train (Unl.) #Lab. F LEX M ATCH A DA M ATCH TAPT S UPERVISED\nIMDB\nIMDB 100 93.4 0.1 94.7 0.2 90.9 0.6 88.7 0.2 ⋆\nSST-2 100 89.1 1.2 (▼4.6 % ) 87.6 2.2 (▼7.5 % ) 89.9 0.6 (▼1.1 % ) 88.7 0.2\nA MAZON R EVIEW 100 92.1 0.7 (▼1.4 % ) 92.4 0.2 (▼2.4 % ) 91.4 0.3 (▲0.6 % ) 88.7 0.2\nIMDB\nIMDB 200 93.5 0.1 93.6 0.1 91.8 0.3 90.3 0.4 ⋆\nSST-2 200 89.5 2.4 (▼4.3 % ) 88.9 1.0 (▼5.0 % ) 90.3 0.4 (▼1.6 % ) 90.3 0.4\nA MAZON R EVIEW 200 92.5 0.4 (▼1.1 % ) 92.7 0.5 (▼1.0 % ) 92.1 0.2 (▲0.3 % ) 90.3 0.4\nSST-2\nSST-2 100 83.0 8.3 83.1 4.4 85.4 2.4 84.0 2.7 ⋆\nIMDB 100 46.7 2.1 (▼43.7 % ) 49.2 7.3 (▼40.8 % ) 88.5 0.9 (▲3.6 % ) 84.0 2.7\nA MAZON R EVIEW 100 46.4 4.9 (▼44.1 % ) 48.2 11 .0 (▼42.0 % ) 88.9 0.9 (▲4.1 % ) 84.0 2.7\nSST-2\nSST-2 200 87.2 3.9 89.5 0.9 88.6 0.9 86.8 0.3 ⋆\nIMDB 200 62.7 7.4 (▼28.1 % ) 61.0 2.8 (▼31.8 % ) 89.1 1.1 (▲0.6 % ) 86.8 0.3\nA MAZON R EVIEW 200 61.8 7.7 (▼29.1 % ) 56.0 10 .3 (▼17.4 % ) 89.4 1.0 (▲0.9 % ) 86.8 0.3\nTable 4: Results of STL experiments. We report the average Macro-F1 score on the test set across five seeds, with\nstandard deviations as subscripts. Blue represents the best result for each row. Stars highlight rows without domain\nshifts. Arrows in colours stand for the changes in performances against the star row result within each cell.\nTask Lab. Unl.\nSemi-supervised Learning Target Target\nUnsupervised Domain AdaptationSource Target\nSelf-taught Learning Target Source\nTable 5: A summary of domain adaptation, where the\ndistribution of source and target domains are different.\nother words, reducing the size of unlabelled data\ncould be beneficial for ST approaches when the\nlabelled size is inadequate. We further zoom in on\nthis phenomenon in Table 3 by selecting 4 fixed\nlabelled and 500 unlabelled samples, and gradually\nremoving unlabelled samples on IMDB . This is a\nstark contrast to the case where more unlabelled\ndata is beneficial for ST approaches when adequate\nlabelled data is available. Meanwhile, TAPT gen-\nerally benefits from training on more in-domain\nunlabelled data, following the scaling law in LMs\n(Kaplan et al., 2020; Hoffmann et al., 2022).\n#5. Adequate labelled and unlabelled data.\nBoth ST and TAPT have demonstrated the ability\nto exploit unlabelled data in this setting. Figure 3\nshows that ST dominates in IMDB when more than\n10 labelled and 100 unlabelled samples are avail-\nable. On the other hand, TAPT generally performs\nbetter than ST on AMAZON REVIEW and YAHOO !\nANSWER , indicating that the answer to which ap-\nproach is better depends on the nature of the dataset\nand task. As labelled and unlabelled data size in-\ncrease, the difference betweenST and TAPT shrinks\n(colours fade and lines converge in Figures 3 and\n5). As the labelled data in size reaches the unla-\nbelled data, the method of ST reduces to FULLY-\nSUPERVISED , which is generally outperformed by\nTAPT (Gururangan et al., 2020).\n6 Domain Adaptation\nWe next investigate how ST and TAPT compare\nin the presence of domain shifts between labelled\nand unlabelled data in two additional settings (refer\nto Table 5). First, we experiment with the Unsu-\npervised Domain Adaptation (UDA) setting, where\ndomain shifts exist between the labelled data from\na source domain and the unlabelled data from the\ntarget domain (Ben-David et al., 2010; Saito et al.,\n2018; Ramponi and Plank, 2020). Then, we ex-\nperiment with Self-taught Learning (STL) (Raina\net al., 2007) in a domain adaptation setting, where\n5620\nthe unlabelled data come from the source domain\nand the labelled data from the target domain. In\nboth settings, we use the (labelled) validation and\ntest sets from the target domain. Validation and\ntest sets are excluded from any pool of labelled or\nunlabelled train data.\n#1. Unsupervised Domain Adaptation (UDA).\nIn this setting, we use two movie sentiment datasets,\nIMDB and SST-2 , as the source and target domain\n(and vice versa) with two different sizes of labelled\ndata (i.e. 100 and 200).\nFigure 6 depicts the performance of ST and\nTAPT in UDA. In case of domain shifts, we ob-\nserve that FLEX MATCH and ADAMATCH fail to\ndeliver satisfactory results and their performance\ndrops to the level of random guessing, with a F1\nscore of 33% across all labelled sizes and datasets.\nThis highlights the vulnerability of ST approaches\nin UDA. In contrast, TAPT demonstrates robust per-\nformance even with domain shifts, on par with its\nown SSL performance without domain shifts. Ad-\nditionally, TAPT even benefits from training on the\nsource domain. For instance, training on IMDB\n(source domain) further improves the performance\nof TAPT on SST-2 (target domain) from 86.4% to\n89.6% with 100 labelled samples and from 88.6%\nto 89.7% with 200 labelled samples.\n#2. Self-taught Learning ( STL). We select\nIMDB , SST-2 , and AMAZON REVIEW for this\nsetting. Although they are all sentiment reviews\ndatasets, IMDB and AMAZON REVIEW are more\nclosely related (see the similarity analysis in Ta-\nble 7 of Appendix) and arguably contain richer\nlanguage than SST-2 (see examples in Table 6 of\nAppendix).\nTable 4 presents the performance of ST and\nTAPT in STL setting. We find that domain shifts in\nunlabelled data consistently hurt the performance\nof ST, depending on the similarity between the\nsource and target domains. The performance of\nST drops sharply if the source and target domains\nare vastly different. For example, when SST-2\nis used as the labelled data (target domain) and\nIMDB or AMAZON REVIEW is used as unlabelled\ndata (source domain), the performance of ST falls\nfrom over 80% to around 60% or lower. On the\nother hand, when using SST-2 and IMDB as the\nsource and target domains, the performance of ST\ndrops by a much smaller margin (a few percentage\npoints). This shows the importance of training ST\napproaches using more informative labelled data,\nwhich is also consistent with our findings in §5.\nTAPT in the STL setting is in fact a variation of\ndomain adaptive pre-training (Beltagy et al., 2019;\nGururangan et al., 2020) applied to SSL tasks. Ta-\nble 4 shows that the performance of TAPT remains\nstable when there exist domain shifts in the unla-\nbelled data. Using more informative unlabelled\ndata can further improve the performance of TAPT .\nFor example, using IMDB or AMAZON REVIEW\nas unlabelled data when SST-2 is a target task, we\nsee an improvement of about 4% with 100 labelled\nsamples. However, it is worth noting that ST meth-\nods can still be competitive compared to TAPT if\nthe source and target domains are relatively similar.\nFor instance, when using AMAZON REVIEW and\nIMDB as the source and target domains, ST still\nachieves better results than TAPT .\n7 Related Work\nLeveraging unlabelled data by Continuing Pre-\ntraining. Previous work has shown that further\npre-training LMs on the unlabelled data of a task\n(e.g. Alsentzer et al., 2019; Mehri et al., 2020;\nMargatina et al., 2022) or in-domain data (e.g. Lo-\ngeswaran et al., 2019; Gururangan et al., 2020;\nXue et al., 2021) is beneficial to downstream tasks.\nHowever, it is unknown whether this is valid in\nSSL settings. Previous studies in computer vision\n(Zoph et al., 2020) and speech recognition (Xu\net al., 2021a) have compared PT and ST. However,\nour study has a different focus, specifically, we\ncompare TAPT and ST in NLP tasks. Concurrently\nto our work, Shi and Lipani (2023) put forward\nprompt-based continued pre-training, which pri-\nmarily aims to enhance the performance of prompt-\nbased fine-tuning techniques (Schick and Schütze,\n2021; Gao et al., 2021). This approach outper-\nforms these state-of-the-art ST approaches (Sohn\net al., 2020; Xu et al., 2021b; Zhang et al., 2021;\nBerthelot et al., 2022) as well as the conventional\nCLS-based fine-tuning with TAPT .\nSemi-supervised Learning. Recent work in SSL\nhas demonstrated great progress in effectively ex-\nploiting unlabelled data. A wide range of ap-\nproaches has been proposed including Pseudo La-\nbeling (Lee et al., 2013), Temporal Ensemble\n(Laine and Aila, 2017), Mean Teacher (Tarvainen\nand Valpola, 2017), Virtual Adversarial Training\n(Miyato et al., 2018), FixMatch (Sohn et al., 2020).\nA major issue for ST approaches is confirmation\n5621\nbias, where the student model would accumulate\nerrors from the teacher model when learning with\ninaccurate pseudo-labels (e.g. Wang et al., 2021;\nGoel et al., 2022; Chen et al., 2022).\nWhile many efforts towards ST (e.g. Ruder and\nPlank, 2018; Gururangan et al., 2019; Li et al.,\n2019; Chen et al., 2020b; Meng et al., 2020; Chen\net al., 2020a; He et al., 2020; Gera et al., 2022)\nhave been made in NLP , the performance of ST\napproaches across various labelled and unlabelled\nsizes has yet to be thoroughly explored. Although\nMukherjee and Awadallah (2020); Li et al. (2021b)\nnoted that training ST approaches from TAPT\ncheckpoints can improve the performance, the per-\nformance of TAPT in SSL tasks has not been either\nwell-researched by previous works or compared\nwith state-of-the-art ST approaches.\n8 Conclusion\nIn this work, we shed light on how TAPT performs\nagainst state-of-the-art ST approaches in various\nSSL settings. Our experiments reveal that TAPT\nachieves strong and robust performance, even with\njust a few hundred unlabelled examples. We further\ndemonstrate that the ST approaches are vulnerable\nto small amounts of either labelled or unlabelled\ndata. We also find that TAPT is more robust than\nST approaches in joint domain adaptation and SSL\nsettings. Overall, our empirical study demonstrates\nthat TAPT is a strong SSL learner, competitive to\nmore sophisticated ST approaches. In future work,\nwe plan to further explore the potential of TAPT\nwith unsupervised learning signals.\nLimitations\nFor easier comparison with previous work, we only\nfocus on text classification tasks, while ST can also\nbe applied to a variety of NLP tasks, such as lan-\nguage generation, conversational systems and com-\nmonsense reasoning (Kedzie and McKeown, 2019;\nHe et al., 2020; Shi et al., 2022a,b; Hendriksen\net al., 2022). We also assume that the datasets are\nroughly balanced. However, real-world datasets are\nusually class-imbalanced (Li et al., 2011), which\nmight impact the performance of TAPT and ST.\nWhile this is out of the scope of this paper, we be-\nlieve that this is an interesting avenue for future\nwork. Additionally, different labelled and unla-\nbelled sizes may impact the performance of ST ap-\nproaches in the domain shift setting. However, this\ndoesn’t alter our conclusion that the effectiveness\nof ST approaches significantly fluctuates across\ndifferent scenarios.\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nEric Arazo, Diego Ortego, Paul Albert, Noel E\nO’Connor, and Kevin McGuinness. 2020. Pseudo-\nlabeling and confirmation bias in deep semi-\nsupervised learning. In 2020 International Joint\nConference on Neural Networks (IJCNN), pages 1–8.\nIEEE.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.\nA robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 789–798, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nShai Ben-David, John Blitzer, Koby Crammer, Alex\nKulesza, Fernando Pereira, and Jennifer Wortman\nVaughan. 2010. A theory of learning from different\ndomains. Machine learning, 79(1):151–175.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th Annual International Confer-\nence on Machine Learning, ICML ’09, page 41–48,\nNew York, NY , USA. Association for Computing\nMachinery.\nDavid Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex\nKurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel.\n2019a. Remixmatch: Semi-supervised learning with\ndistribution alignment and augmentation anchoring.\narXiv preprint arXiv:1911.09785.\nDavid Berthelot, Nicholas Carlini, Ian J. Goodfellow,\nNicolas Papernot, Avital Oliver, and Colin Raffel.\n2019b. Mixmatch: A holistic approach to semi-\nsupervised learning. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 5050–5060.\n5622\nDavid Berthelot, Rebecca Roelofs, Kihyuk Sohn,\nNicholas Carlini, and Alexey Kurakin. 2022.\nAdamatch: A unified approach to semi-supervised\nlearning and domain adaptation. In International\nConference on Learning Representations.\nRui Cai and Mirella Lapata. 2019. Semi-supervised\nsemantic role labeling with cross-view training. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1018–\n1027, Hong Kong, China. Association for Computa-\ntional Linguistics.\nMing-Wei Chang, Lev Ratinov, Dan Roth, and Vivek\nSrikumar. 2008. Importance of semantic representa-\ntion: dataless classification. In Proceedings of the\n23rd national conference on Artificial intelligence-\nVolume 2, pages 830–835.\nOlivier Chapelle, Bernhard Scholkopf, and Alexander\nZien. 2009. Semi-supervised learning (chapelle, o.\net al., eds.; 2006)[book reviews]. IEEE Transactions\non Neural Networks, 20(3):542–542.\nBaixu Chen, Junguang Jiang, Ximei Wang, Pengfei\nWan, Jianmin Wang, and Mingsheng Long. 2022.\nDebiased self-training for semi-supervised learning.\nIn Advances in Neural Information Processing Sys-\ntems, NIPS’22.\nJiaao Chen, Zhenghui Wang, Ran Tian, Zichao Yang,\nand Diyi Yang. 2020a. Local additivity based data\naugmentation for semi-supervised NER. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n1241–1251, Online. Association for Computational\nLinguistics.\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020b. Mix-\nText: Linguistically-informed interpolation of hid-\nden space for semi-supervised text classification. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2147–\n2157, Online. Association for Computational Lin-\nguistics.\nKevin Clark, Minh-Thang Luong, Christopher D. Man-\nning, and Quoc Le. 2018. Semi-supervised sequence\nmodeling with cross-view training. In Proceedings\nof the 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1914–1925, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nXin Dong and Gerard de Melo. 2019. A robust self-\nlearning framework for cross-lingual text classifi-\ncation. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n6306–6310, Hong Kong, China. Association for Com-\nputational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nAriel Gera, Alon Halfon, Eyal Shnarch, Yotam Perlitz,\nLiat Ein-Dor, and Noam Slonim. 2022. Zero-shot\ntext classification with self-training. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing. Association for Com-\nputational Linguistics.\nArushi Goel, Yunlong Jiao, and Jordan Massiah. 2022.\nPars: Pseudo-label aware robust sample selection\nfor learning with noisy labels. arXiv preprint\narXiv:2201.10836.\nYves Grandvalet and Yoshua Bengio. 2004. Semi-\nsupervised learning by entropy minimization. Ad-\nvances in neural information processing systems, 17.\nSuchin Gururangan, Tam Dang, Dallas Card, and\nNoah A. Smith. 2019. Variational pretraining for\nsemi-supervised text classification. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5880–5894, Florence,\nItaly. Association for Computational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio\nRanzato. 2020. Revisiting self-training for neural\nsequence generation. In International Conference on\nLearning Representations.\nMariya Hendriksen, Maurits Bleeker, Svitlana Vaku-\nlenko, Nanne van Noord, Ernst Kuiper, and Maarten\nde Rijke. 2022. Extending clip for category-to-image\nretrieval in e-commerce. In Advances in Information\nRetrieval: 44th European Conference on IR Research,\nECIR 2022, Stavanger, Norway, April 10–14, 2022,\nProceedings, Part I, page 289–303, Berlin, Heidel-\nberg. Springer-Verlag.\n5623\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nZejiang Hou, Julian Salazar, and George Polovets. 2022.\nMeta-Learning the Difference: Preparing Large Lan-\nguage Models for Efficient Adaptation. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:1249–1265.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nChris Kedzie and Kathleen McKeown. 2019. A good\nsample is hard to find: Noise injection sampling and\nself-training for neural language generation models.\nIn Proceedings of the 12th International Conference\non Natural Language Generation , pages 584–593,\nTokyo, Japan. Association for Computational Lin-\nguistics.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks. In International Conference on Learning\nRepresentations (ICLR).\nSamuli Laine and Timo Aila. 2017. Temporal ensem-\nbling for semi-supervised learning. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Con-\nference Track Proceedings. OpenReview.net.\nDong-Hyun Lee et al. 2013. Pseudo-label: The simple\nand efficient semi-supervised learning method for\ndeep neural networks. In Workshop on challenges in\nrepresentation learning, ICML, page 896.\nChangchun Li, Ximing Li, and Jihong Ouyang. 2021a.\nSemi-supervised text classification with balanced\ndeep representation distributions. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 5044–5053, Online.\nAssociation for Computational Linguistics.\nShiyang Li, Semih Yavuz, Wenhu Chen, and Xifeng Yan.\n2021b. Task-adaptive pre-training and self-training\nare complementary for natural language understand-\ning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 1006–1015,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nShoushan Li, Zhongqing Wang, Guodong Zhou, and\nSophia Yat Mei Lee. 2011. Semi-supervised learning\nfor imbalanced sentiment classification. In Proceed-\nings of the Twenty-Second International Joint Con-\nference on Artificial Intelligence - Volume Volume\nThree, IJCAI’11, page 1826–1831. AAAI Press.\nZhenghua Li, Xue Peng, Min Zhang, Rui Wang, and\nLuo Si. 2019. Semi-supervised Domain Adaptation\nfor Dependency Parsing. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2386–2395, Florence, Italy. Asso-\nciation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLajanugen Logeswaran, Ming-Wei Chang, Kenton Lee,\nKristina Toutanova, Jacob Devlin, and Honglak Lee.\n2019. Zero-shot entity linking by reading entity de-\nscriptions. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3449–3460, Florence, Italy. Association for\nComputational Linguistics.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nUSA. Association for Computational Linguistics.\nKaterina Margatina, Loic Barrault, and Nikolaos Ale-\ntras. 2022. On the importance of effectively adapting\npretrained language models for active learning. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 825–836, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nJulian McAuley and Jure Leskovec. 2013. Hidden fac-\ntors and hidden topics: Understanding rating dimen-\nsions with review text. In Proceedings of the 7th\nACM Conference on Recommender Systems, RecSys\n’13, page 165–172, New York, NY , USA. Association\nfor Computing Machinery.\nDavid McClosky, Eugene Charniak, and Mark Johnson.\n2006. Effective self-training for parsing. In Proceed-\nings of the Human Language Technology Conference\nof the NAACL, Main Conference , pages 152–159,\nNew York City, USA. Association for Computational\nLinguistics.\nShikib Mehri, Mihail Eric, and Dilek Z. Hakkani-Tür.\n2020. Dialoglue: A natural language understand-\ning benchmark for task-oriented dialogue. ArXiv,\nabs/2009.13570.\nYu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong,\nHeng Ji, Chao Zhang, and Jiawei Han. 2020. Text\n5624\nclassification using label names only: A language\nmodel self-training approach. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 9006–9017,\nOnline. Association for Computational Linguistics.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama,\nand Shin Ishii. 2018. Virtual adversarial training:\na regularization method for supervised and semi-\nsupervised learning. IEEE transactions on pattern\nanalysis and machine intelligence, 41:1979–1993.\nSubhabrata Mukherjee and Ahmed Awadallah. 2020.\nUncertainty-aware self-training for few-shot text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems , volume 33, pages 21199–21212.\nCurran Associates, Inc.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of NAACL-HLT\n2019: Demonstrations.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8).\nRajat Raina, Alexis Battle, Honglak Lee, Benjamin\nPacker, and Andrew Y . Ng. 2007. Self-taught learn-\ning: Transfer learning from unlabeled data. In Pro-\nceedings of the 24th International Conference on Ma-\nchine Learning, ICML ’07, page 759–766, New York,\nNY , USA. Association for Computing Machinery.\nAlan Ramponi and Barbara Plank. 2020. Neural Unsu-\npervised Domain Adaptation in NLP—A Survey. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 6838–6855,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nSebastian Ruder and Barbara Plank. 2018. Strong Base-\nlines for Neural Semi-Supervised Learning under\nDomain Shift. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1044–1054,\nMelbourne, Australia. Association for Computational\nLinguistics.\nKuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and\nTatsuya Harada. 2018. Maximum classifier discrep-\nancy for unsupervised domain adaptation. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition, pages 3723–3732.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nZhengxiang Shi, Yue Feng, and Aldo Lipani. 2022a.\nLearning to execute actions or ask clarification ques-\ntions. In Findings of the Association for Computa-\ntional Linguistics: NAACL 2022, pages 2060–2070,\nSeattle, United States. Association for Computational\nLinguistics.\nZhengxiang Shi and Aldo Lipani. 2023. Don’t stop pre-\ntraining? make prompt-based fine-tuning powerful\nlearner. arXiv preprint arXiv:2305.01711.\nZhengxiang Shi, Qiang Zhang, and Aldo Lipani. 2022b.\nStepgame: A new benchmark for robust multi-hop\nspatial reasoning in texts. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 36,\npages 11321–11329.\nKihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao\nZhang, Nicholas Carlini, Ekin D. Cubuk, Alex Ku-\nrakin, Han Zhang, and Colin Raffel. 2020. Fixmatch:\nSimplifying semi-supervised learning with consis-\ntency and confidence. In Proceedings of the 34th\nInternational Conference on Neural Information Pro-\ncessing Systems, NIPS’20, Red Hook, NY , USA. Cur-\nran Associates Inc.\nAntti Tarvainen and Harri Valpola. 2017. Mean teach-\ners are better role models: Weight-averaged consis-\ntency targets improve semi-supervised deep learning\nresults. In Proceedings of the 31st International Con-\nference on Neural Information Processing Systems,\nNIPS’17, page 1195–1204, Red Hook, NY , USA.\nCurran Associates Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nXimei Wang, Jinghan Gao, Mingsheng Long, and Jian-\nmin Wang. 2021. Self-tuning for data-efficient deep\nlearning. In International Conference on Machine\nLearning (ICML).\nYidong Wang, Hao Chen, Yue Fan, Wang SUN, Ran\nTao, Wenxin Hou, Renjie Wang, Linyi Yang, Zhi\nZhou, Lan-Zhe Guo, Heli Qi, Zhen Wu, Yu-Feng\nLi, Satoshi Nakamura, Wei Ye, Marios Savvides,\nBhiksha Raj, Takahiro Shinozaki, Bernt Schiele, Jin-\ndong Wang, Xing Xie, and Yue Zhang. 2022. USB:\nA unified semi-supervised learning benchmark for\nclassification. In Thirty-sixth Conference on Neural\nInformation Processing Systems Datasets and Bench-\nmarks Track.\nQizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Lu-\nong, and Quoc V . Le. 2020a. Unsupervised data aug-\nmentation for consistency training. In Proceedings\nof the 34th International Conference on Neural In-\nformation Processing Systems, NIPS’20, Red Hook,\nNY , USA. Curran Associates Inc.\n5625\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and\nQuoc V Le. 2020b. Self-training with noisy student\nimproves imagenet classification. In Proceedings of\nthe IEEE/CVF conference on computer vision and\npattern recognition, pages 10687–10698.\nQiantong Xu, Alexei Baevski, Tatiana Likhomanenko,\nPaden Tomasello, Alexis Conneau, Ronan Collobert,\nGabriel Synnaeve, and Michael Auli. 2021a. Self-\ntraining and pre-training are complementary for\nspeech recognition. In ICASSP 2021-2021 IEEE In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), pages 3030–3034. IEEE.\nYi Xu, Lei Shang, Jinxing Ye, Qi Qian, Yu-Feng Li,\nBaigui Sun, Hao Li, and Rong Jin. 2021b. Dash:\nSemi-supervised learning with dynamic thresholding.\nIn International Conference on Machine Learning,\npages 11525–11536. PMLR.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 189–196, Cambridge, Mas-\nsachusetts, USA. Association for Computational Lin-\nguistics.\nBowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu,\nJindong Wang, Manabu Okumura, and Takahiro Shi-\nnozaki. 2021. Flexmatch: Boosting semi-supervised\nlearning with curriculum pseudo labeling. In Pro-\nceedings of the 35th International Conference on\nNeural Information Processing Systems, volume 34.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Proceedings of the 28th International\nConference on Neural Information Processing Sys-\ntems - Volume 1, NIPS’15, page 649–657, Cambridge,\nMA, USA. MIT Press.\nBarret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui,\nHanxiao Liu, Ekin D. Cubuk, and Quoc V . Le. 2020.\nRethinking pre-training and self-training. In Pro-\nceedings of the 34th International Conference on\nNeural Information Processing Systems , NIPS’20,\nRed Hook, NY , USA. Curran Associates Inc.\n5626\nAppendix Overview\nThe appendix is structured as follows:\nAppendix §A provides a brief description and\nexample for each dataset (subsection §A.1). Addi-\ntionally, a similarity analysis among datasets and an\nillustration of overlaps between IMDB and AMA-\nZON REVIEW are included (subsection §A.2).\nAppendix §B presents a brief description of state-\nof-the-art ST approaches.\nAppendix §C includes a supplementary Table\nthat examines the effect of low unlabelled data\nsizes.\nAppendix §D presents additional experiments to\nverify our findings using other ST approaches.\nAppendix §E includes additional experiments to\ntrain ST approaches using TAPT checkpoints.\nAppendix §F provides implementation details\nand hyperparameters for TAPT , ST, and FT meth-\nods used in our experiments.\nA Datasets\nIn this section, we briefly introduce the datasets\nused in our work and provide additional analysis\nof the similarity among them. Specifically, we\nprovide four examples to demonstrate the overlap\nbetween IMDB and AMAZON REVIEW , as a sup-\nplement to our domain adaptation analysis (§6).\nA.1 Description\nIn this section, we briefly introduce IMDB , SST-\n2, AG N EWS , AMAZON REVIEW , and YAHOO !\nANSWER datasets. Table 6 list examples for each\ndataset.\nIMDB . The IMDB dataset (Maas et al., 2011)\ncontains a collection of 50 000 reviews from the\nInternet Movie Database, with no more than 30\nreviews per movie. This dataset contains an equal\nnumber of positive and negative reviews, yielding\na 33% Marco-F1 score for random guessing. There\nare 25 000 and 25 000 for training and testing, re-\nspectively. We follow Wang et al. (2022) to split\nthe dataset by selecting 12 500 samples and 1 000\nsamples per class from the train set to form a train\nand validation set, respectively.\nSST-2. The SST-2 dataset (Wang et al., 2018)\nconsists of sentences from movie reviews and hu-\nman annotations of their sentiment. The task is to\npredict the sentiment of a given sentence. Similar\nto IMDB , this is also a binary classification task.\nThere are 67 349 and 872 for training and testing.\nWe select 60 000and 7 349samples from the train\nset to form a train and validation set, respectively,\nwhere the validation set contains 3 675 and 3 674\nsamples for two classes, respectively.\nAG NEWS . The AG NEWS topic classification\ndataset is constructed by Zhang et al. (2015), where\n4 classes are used. Each class contains 30 000train-\ning samples and 1 900 test samples. We follow\nWang et al. (2022) to split the dataset by selecting\n25 000 samples and 2 500 samples per class from\nthe train set samples to form a train and validation\nset, respectively.\nAMAZON REVIEW . The AMAZON REVIEW\ndataset (McAuley and Leskovec, 2013) is a senti-\nment classification dataset, with five classes. There\nare 600 000train samples and 130 000test samples\nper class. We follow Wang et al. (2022) to split\nthe dataset by selecting 50 000 samples and 5 000\nsamples per class from the train set samples to form\na train and validation set, respectively.\nYAHOO ! A NSWER . The YAHOO ! A NSWER\ndataset (Chang et al., 2008) is a topic classifica-\ntion dataset, with ten classes. There are 140 000\ntrain samples and 6 000 test samples per class. We\nfollow Wang et al. (2022) to split the dataset by\nselecting 50 000 samples and 5 000 samples per\nclass from the train set samples to form a train and\nvalidation set, respectively.\nA.2 Dataset Similarity\nWe provide an analysis of the vocabulary overlap of\nthe datasets, as shown in Figure 7. Additionally, in\nTable 7, we provide some examples to illustrate the\noverlap between IMDB and A MAZON REVIEW .\nAs shown in Table 6, although both the SST-2\nand IMDB datasets are sentiment analysis tasks for\nmovie reviews, the SST-2 datasets contain shorter\nand vaguer sentences than the IMDB dataset. This\ndifference could be a potential reason for poor per-\nformance of ST approaches in the UDA setting (§6).\nIn contrast, the AMAZON REVIEW dataset, which\nis a product review sentiment analysis dataset, is\nmore similar to the IMDB dataset than the SST-2\ndataset, as shown in Table 7. This suggests a poten-\n5627\nIMDB SST-2\nAG News\nAmazon ReviewYahoo! Answer\nIMDB\nSST-2\nAG News\nAmazon Review\nYahoo! Answer\n100.0 33.4 36.3 33.9 29.8\n33.4 100.0 21.6 15.4 13.8\n36.3 21.6 100.0 28.0 29.9\n33.9 15.4 28.0 100.0 40.2\n29.8 13.8 29.9 40.2 100.0\nFigure 7: V ocabulary overlap (%) across datasets.\ntial reason for the performance of ST and TAPT in\nthe STL setting (§6).\nB S T Frameworks\nVAT. VAT(Miyato et al., 2018) proposed a regu-\nlarization technique that forces pairs of data points\nthat are very close in the input space to be close\nto each other in the output space. VAT adds small\nperturbation to the input data and forces the model\nto produce similar predictions.\nFIXMATCH. FIXMATCH (Sohn et al., 2020) gen-\nerates artificial labels using both consistency regu-\nlarization and pseudo-labelling, where the artificial\nlabels are produced based on weakly-augmented\nunlabelled data. These artificial labels are then used\nas targets to train the model on strongly-augmented\nunlabelled data. FIXMATCH only retains an artifi-\ncial label if the model assigns a high probability to\none of the possible classes.\nDASH . DASH (Xu et al., 2021b) extends FIX-\nMATCH by introducing a mechanism with a dynam-\nically adjusted threshold of loss to select a subset\nof training examples from the unlabelled data for\nperforming SSL.\nFLEX MATCH. FLEX MATCH (Zhang et al.,\n2021) also extends FIXMATCH by introducing the\nconcept of curriculum learning (Bengio et al., 2009)\nto flexibly adjust thresholds for different classes at\neach time step and select unlabelled data and their\npseudo labels that are more likely to be informative.\nADAMATCH. ADAMATCH (Berthelot et al.,\n2022) aims to solve domain adaptation problems\nin SSL and build a high-accuracy model that\ntrains on and tests on different data distributions.\nADAMATCH builds on FIXMATCH and introduces\na relative confidence threshold and a modified dis-\ntribution alignment from (Berthelot et al., 2019a).\nC Probability of performing worsen than\nSUPERVISED .\nIn §5, we discuss that we select the model perfor-\nmance with the three lowest unlabelled sizes (the\nfirst three columns in Figure 3) for each dataset\nand exclude the model performance with the low-\nest labelled size (the last row in Figure 3). This\nresults in 9 cells in IMDB , 3 cells in SST-2 , 9\ncells in AMAZON REVIEW , and 12 cells in YA-\nHOO ! A NSWER , where TAPT has one run per cell\nand ST (FLEX MATCH and ADAMATCH) has two\nruns per cell. We consider a run to be a failure\nif its performance is worse than its corresponding\nSUPERVISED baseline.\nTable 8 lists the probability of ST and TAPT\nof falling below the SUPERVISED baseline with\n5628\nselected combinations of labelled and unlabelled\nsizes.\nD Further validations with other ST\napproaches\nIn this section, we conduct additional experiments\non ST approaches, including VAT, DASH , and FIX-\nMATCH to demonstrate that our findings are appli-\ncable to other ST approaches as well.\nIn Table 9, we select several combinations of\nlabelled and unlabelled sizes on IMDB , SST-\n2, AMAZON REVIEW , and YAHOO ! A NSWER\ndatasets. Our experimental results show that other\nST approaches do not perform well when the la-\nbelled size is low, and that other ST approaches\nhave a high probability to perform worsen than\nSUPERVISED baselines when the unlabelled size\nis low. This suggests that poor performance when\nthe labelled or unlabelled size is inadequate may\nbe a common problem of state-of-the-art ST ap-\nproaches.\nE Train S T approaches with TAPT\ncheckpoints\nPrevious works (Mukherjee and Awadallah, 2020;\nLi et al., 2021b) have suggested that training ST\napproaches from a TAPT checkpoint may be ben-\neficial. Here we also provide some additional ex-\nperiments to train ST approaches with TAPT check-\npoints to further corroborate our findings.\nTable 10 shows that TAPT outperforms\nADAMATCH +TAPT or FLEX MATCH +TAPT with\ntwo different labelled sizes on the YAHOO ! A N-\nSWER dataset.\nTable 11 shows that trainingST approaches from\nTAPT checkpoints could improve the performance\nof ST but cannot solve the issue of ST approaches\nwhen labelled or unlabelled data is not adequate.\nSpecifically, the performance of ST +TAPT is still\npoor when labelled data is insufficient, as discussed\nin §5. Meanwhile, in Table 11, the performance\nof ST +TAPT could be outperformed by the SU-\nPERVISED baselines when unlabelled data is inad-\nequate, while TAPT consistently outperforms the\nSUPERVISED baselines. When the labelled size\nis 10, the performance of ST trained with fewer\nunlabelled samples tends to be better, indicating\nthat reducing the number of unlabelled data can be\nhelpful, as discussed in §5.\nF Implementation Details\nWe consistently use five random seeds, ranging\nfrom 1 to 5, for all algorithms. The sampled la-\nbelled data is the same for all algorithms for a\ngiven seed. The development and test sets remain\nunchanged for all different labelled and unlabelled\ndata sizes.\nOur model implementation uses open-source\nlibraries including HuggingFace Transformers 2,\nFairseq3, and USB4. Our experiments of TAPT are\nperformed on 8x32GB V100 GPUs, with a batch\nsize of 16 per device and 2 gradient accumulation\nsteps.\nTable 12 lists the hyperparameters used for the\nTAPT phrase. Table 13 lists the hyperparameters\nused for the fine-tuning phrase. Table 14 lists the\nhyperparameters used for ST approaches.\n2https://huggingface.co\n3https://github.com/facebookresearch/fairseq\n4https://github.com/microsoft/Semi-supervised-learning\n5629\nTable 6: Examples for datasets.\nDataset Example\nIMDB I watched this movie after seeing other comments on IMDb, even convincing my wife that it was a \"unique horror\nmovie.\" I wanted to like this movie, but was unable to.The \"love story\" was good, but the horror aspect was quite bad. If\nthe story was just about a young man who fell in love with a girl suffering from parasomnia, then it would have been a\nbetter movie.The care centre stretched credulity well past the limits, in fact it was quite ridiculous. The doctor happily\nignors privacy laws and professionalism. A nurse goes into a room for a routine feeding of a dangerous patient (without\nsecurity escort), and drops the tray and runs out of the room screaming for no apparent reason. The forensic patient (and\nthe film’s villain) is tied up in a standing position fully clothed - apparently for years? None of it makes much sense.The\nmovie even had some actors that I’ve liked in other things, such as the detectives, but still I can’t recommend this movie.\nSST-2 a rewarding work of art for only the most patient and challenge-hungry moviegoers.\nAG NEWS Teen flies in plane #39;s landing gearA homeless teenager who hid in the landing gear of a passenger plane survived\na 700-kilometre flight across south-western China but his companion fell and probably died, state media reported on\nFriday.\nAMAZONREVIEW THIS is MUSIC at its BESTRob Dougan has done it. He’s crafted musical perfection, or close to it anyway. I have\nfinally found the music I’ve been waiting for my whole life in this album - Rob D you are a genius. I think a lot of us\nwanted to know more about this guy as soon as we heard the track playing to the \"Woman in the Red Dress\" scene. Now\nI know why the Wachowski brothers have enlisted his musical talents to flesh out their movies.I know I should be trying\nto write a more helpful, objective review but I can do nothing but wax poetic for Rob Dougan and his debut album. He\nhas mixed classical melodies with awesome electric beats and it all comes together in an audio orgy. Just buy the album\nalready and let’s get Rob some more mainstream recognition.\nYAHOO! ANSWER Does anybody know a great deal about angels? I’m looking for names, if they’re good or bad, what they look like, etc.\nThe more detail the better. All religions accepted\nTable 7: Similarity analysis between IMDB and AMAZON REVIEW with four examples that highlight the overlap.\nIMDB A MAZON REVIEW\nI loved thismoviesince I was 7 and I saw it on the opening day.\nIt was sotouchingand beautiful. I strongly recommend seeing\nfor all. It’s amovieto watch with your family by far. My\nMPAA rating: PG-13 for thematic elements, prolonged scenes\nof disastor, nudity/sexuality and some language.\nThis is a verytouching, spiritualmovie! When I first saw this\nfilm, [...]. I was deeply moved by this motion picture, and the\nDVD brings the story to your own home. The bonus materials\ncould be better, but the main part of the DVD is the actualmovie.\nGreat, great, great film... [...]\nPacino is over-the-top but to good effect as he’s clearly having\nloads offun. Beatty isgreat[...] The lighting, velvet overtones\nand smog/smoke combine to create agreateffect.There are\nsome reallyfunnycameos [...]Highly recommended. 4.5/5\nstars. [...]\nMakes agreatgift! We bought this book for my dad for Father’s\nDay this year, and thought he would havefun reading it since\nhe has four granddaughters. He loved it and has even selected\nstories to read to the girls during over-nights with Grandpa and\nGrandma. Ihighly recommendit as agreatgift.\nThe late [...] scripted this tale ofterrorand it was absolutely\none of thescariest moviesI ever saw as a kid. (I had to walk\nMILES just to see amovie, and it was usually dark when I\nemerged from the theater; seeing a horrormoviewas always\nunnerving [...]\nMovia ... please .... Thismovieis a masterpiece ofterror& sus-\npence & Beautifully filmed & acted.Comparisons to reality are\nnot allowed when reviewing films of this caliber. Your reaction\n(though it MAY besarcastic) is EXACT proof of it’s genius!\nWatch it again...and this time....bask in all it’s glory!\nFabulous actors, beautiful scenery, stark reality [...] I tried to\nbuy the video for several years, finally bought it used from a\nvideo store that went out of business. But Yippee! The DVD is\nnow for sale, I purchased it on amazon.com. Not cheap, butwell\nworthit to me. [...]\nWell worththe import price. My first impression of this album\nwas a good one, but as time went on it came to grow on me more\nand more. This is certainly one of the better Costes albums. The\nmixing is nothing revolutionary, but it is well done and all tracks\nflow into each other very well. [...].\nTable 8: Results on the effect of low unlabelled sizes on ST and TAPT . Failure means performing worsen than\nSUPERVISED .\nTask #Unl. #Lab. Prob. of S T Failure Prob. of TAPT Failure\nIMDB 100, 500, 2k 10, 20, 200, 1k 6/18 (33%) 0/9 (0%)\nSST-2 100, 500, 2k 40, 200, 1k, 5k 4/6 (67%) 0/3 (0%)\nAMAZON REVIEW 1k, 5k, 20k 100, 500, 2k, 10k 10/18 (56%) 0/9 (0%)\nYAHOO! ANSWER 1k, 5k, 20k 20, 100, 500, 2k, 10k 13/24 (54%) 0/12 (0%)\n5630\nTable 9: We further verify our conclusion on VAT, DASH , FIXMATCH that . We report the average Macro-F1 score\non the test set across five seeds, with standard deviations as subscripts. Blue represents the best results for each row.\nDataset #Unl. #Lab. VAT F IXMATCH DASH FLEXMATCH ADAMATCH TAPT SUPERVISED\nIMDB\n100 4 33.5 0.2 33.40.1 33.40.1 35.74.2 34.10.7 61.86.7 59.44.8\n100 10 61.6 20.1 45.421.6 34.72.2 49.019.9 52.421.0 75.56.9 71.88.5\n100 20 87.12.2 64.616.5 67.816.6 85.52.9 79.17.6 85.51.0 84.11.9\n500 4 33.4 0.0 33.40.1 33.40.1 33.40.1 33.60.3 63.47.2 58.27.1\n2k 4 33.3 0.0 33.30.0 33.30.0 33.30.0 33.30.0 63.16.2 60.95.6\n10k 4 33.3 0.0 33.50.3 33.30.0 34.01.2 33.60.4 64.18.9 62.47.9\n23k 4 33.3 0.0 33.30.0 57.429.4 45.323.9 33.30.0 68.85.6 65.610.4\nSST-2\n100 40 63.3 10.6 46.99.7 47.97.0 57.24.5 51.014.0 78.72.5 76.43.7\n500 40 55.7 16.8 53.88.9 51.210.0 67.710.7 59.111.4 83.34.8 72.97.9\n500 200 83.0 1.6 84.52.8 82.63.5 83.83.0 87.41.9 88.80.9 88.30.9\n2k 40 55.9 24.2 36.43.0 35.32.0 56.66.7 49.313.8 79.35.9 71.78.2\n10k 40 73.5 20.5 38.911.4 35.62.6 56.912.5 36.22.9 85.91.0 78.57.5\n60k 40 79.6 13.4 32.61.7 33.40.6 40.67.7 42.613.3 82.64.0 75.37.2\nAMAZONREVIEW\n1k 20 13.5 5.2 14.95.6 20.33.0 25.83.2 20.71.1 32.01.8 32.52.2\n1k 100 46.1 2.2 36.33.1 35.36.2 43.41.7 40.32.2 48.50.9 48.22.2\n1k 500 52.6 0.2 50.81.5 49.51.0 54.11.0 52.81.1 55.90.3 55.30.5\n5k 20 15.5 7.8 13.53.3 22.25.2 23.27.3 16.96.9 32.83.4 32.32.5\n20k 20 19.3 7.5 15.23.9 20.56.4 19.110.0 19.36.3 32.03.2 31.63.6\n100k 20 14.1 7.3 11.92.9 20.75.2 15.32.6 12.53.7 30.73.6 30.83.9\n250k 20 10.3 5.0 10.93.6 22.05.7 22.74.9 14.45.6 30.22.4 32.13.1\nYAHOO! ANSWER\n1k 10 1.9 0.1 2.00.1 4.62.9 15.72.6 18.87.9 29.65.8 23.54.5\n1k 20 6.7 2.8 10.14.2 9.63.2 32.79.1 28.85.8 38.94.1 34.13.6\n1k 100 55.2 1.7 46.94.4 45.33.7 54.21.4 53.91.3 59.70.8 57.41.6\n1k 500 59.2 0.4 61.60.6 60.71.3 61.91.1 61.50.9 65.80.3 65.50.2\n5k 10 1.8 0.0 3.22.6 3.72.7 16.410.8 17.811.7 31.45.1 25.73.9\n20k 10 2.4 0.9 2.00.3 4.93.1 7.34.7 25.212.2 32.45.6 27.24.4\n100k 10 2.3 0.6 3.82.5 3.42.9 2.91.1 17.711.4 30.83.8 28.05.0\n500k 10 2.0 0.4 1.80.0 2.61.2 2.50.9 14.36.0 27.34.6 24.74.8\nTable 10: Results of ADAMATCH +TAPT and FLEX MATCH +TAPT on YAHOO ! A NSWER with two different\nlabelled sizes.\nYAHOO ! ANSWER\n500 2000\nADAMATCH 68.00.7 69.50.3\n+ TAPT 68.21.0 69.80.3\nFLEX MATCH 66.60.7 68.70.4\n+ TAPT 66.71.2 69.00.5\nSUPERVISED 65.40.3 68.50.3\n+ TAPT 68.80.7 71.50.3\nFULLY-SUPERVISED . 75.3 0.2\n+ TAPT 75.40.1\nTable 11: We further verify our conclusion on FLEX MATCH +TAPT . We report the average Macro-F1 score on the\ntest set across five seeds, with standard deviations as subscripts. Blue represents the best results for each row.\nDataset #Unl. #Lab. F LEX M ATCH + T APT F LEX M ATCH T APT S UPERVISED\nY AHOO ! A NSWER\n1k 10 17.0 4 . 9 15.7 2 . 6 29.6 5 . 8 23.5 4 . 5\n1k 20 39.4 2 . 0 32.7 9 . 1 38.9 4 . 1 34.1 3 . 6\n1k 100 55.2 1 . 8 54.2 1 . 4 59.7 0 . 8 57.4 1 . 6\n1k 500 62.0 0 . 7 61.9 1 . 1 65.8 0 . 3 65.5 0 . 2\n20k 10 4.0 1 . 4 7.3 4 . 7 32.4 5 . 6 27.2 4 . 4\n100k 10 5.1 6 . 1 2.9 1 . 1 30.8 3 . 8 28.0 5 . 0\n500k 10 2.5 1 . 1 2.5 0 . 9 27.3 4 . 6 24.7 4 . 8\n5631\nHyperparameter Assignment\nnumber of steps 100 epochs\nbatch size 256\nmaximum learning rate 1e-06, 1e-4\nlearning rate optimizer AdamW\nAdam epsilon 1e-6\nAdam beta weights 0.9, 0.98\nlearning rate scheduler Warmup linear\nWeight decay 0.01\nWarmup proportion 0.06\nlearning rate decay linear\nTable 12: Hyperparameters for task-adaptive pretraining. The learning rate and unlabelled size are tightly connected\nand need to be adjusted together. We generally recommend increasing the learning rate as you increase the unlabelled\nsize. Different from its predecessor, BERT (Devlin et al., 2019), where the next sentence prediction objective is\nused, ROBERTA (Liu et al., 2019) is only trained with the MLM objective (i.e., cross-entropy loss on predicting\nrandomly masked tokens), dynamically changing the masking pattern applied to the training examples and typically\nusing the masking probability of 0.15.\nHyperparameter Assignment\nnumber of steps 10 or 50 epochs\nbatch size 16 or 32\nmaximum learning rate 2e-05\nlearning rate optimizer AdamW\nmaximum sequence length 256\nlearning rate scheduler Warmup linear\nWarmup proportion 0.06\nlearning rate decay linear\nTable 13: Hyperparameters for fine-tuning. More epochs are used when the labelled size is low.\nHyperparameter Assignment\nnumber of steps 25 600or 51 200steps\nbatch size 16\nmaximum learning rate 2e-05\nlearning rate optimizer AdamW\nmaximum sequence length 256\nlearning rate scheduler Warmup linear\nWarmup proportion 0.05\nlearning rate decay linear\nTable 14: Hyperparameters for self training. Algorithm-specific hyperparameters will be released in configuration\nfiles with the code.\n5632\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nRight before the reference section.\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNot applicable. Left blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNot applicable. Left blank.\nC □\u0013 Did you run computational experiments?\nSection 4, 5, 6\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix F\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5633\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix F\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4, 5, 6 and Appendix C, D, E\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix F\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n5634"
}