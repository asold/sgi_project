{
  "title": "Coordinating the Complexity of Tools, Tasks, and Users: On Theory-based Approaches to Authoring Tool Usability",
  "url": "https://openalex.org/W2283951625",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A2157347360",
      "name": "Tom Murray",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2157347360",
      "name": "Tom Murray",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2089674328",
    "https://openalex.org/W152650724",
    "https://openalex.org/W119271599",
    "https://openalex.org/W1517725046",
    "https://openalex.org/W6637636544",
    "https://openalex.org/W3125774253",
    "https://openalex.org/W2499850977",
    "https://openalex.org/W2243496244",
    "https://openalex.org/W4301041736",
    "https://openalex.org/W4247890353",
    "https://openalex.org/W2143579043",
    "https://openalex.org/W2138253426",
    "https://openalex.org/W2131601454",
    "https://openalex.org/W2093099366",
    "https://openalex.org/W2093976904",
    "https://openalex.org/W2145180153",
    "https://openalex.org/W2139984026",
    "https://openalex.org/W39850317",
    "https://openalex.org/W6677115739",
    "https://openalex.org/W3046617626",
    "https://openalex.org/W2163697079",
    "https://openalex.org/W3018866669",
    "https://openalex.org/W6629931046",
    "https://openalex.org/W2061718109",
    "https://openalex.org/W6784114196",
    "https://openalex.org/W6786216559",
    "https://openalex.org/W2134643965",
    "https://openalex.org/W2123169053",
    "https://openalex.org/W1533202337",
    "https://openalex.org/W1480007060",
    "https://openalex.org/W1995864528",
    "https://openalex.org/W2120810355",
    "https://openalex.org/W1553579741",
    "https://openalex.org/W7020828652",
    "https://openalex.org/W1988674822",
    "https://openalex.org/W2182476371",
    "https://openalex.org/W4207084002",
    "https://openalex.org/W6635122804",
    "https://openalex.org/W2171076782",
    "https://openalex.org/W6676934228",
    "https://openalex.org/W1964594003",
    "https://openalex.org/W6600690125",
    "https://openalex.org/W1970264500",
    "https://openalex.org/W1481417764",
    "https://openalex.org/W2991956848",
    "https://openalex.org/W1806662972",
    "https://openalex.org/W2097679245",
    "https://openalex.org/W1829827875",
    "https://openalex.org/W22364961",
    "https://openalex.org/W1565084608",
    "https://openalex.org/W1597609424",
    "https://openalex.org/W589000207",
    "https://openalex.org/W6704462221",
    "https://openalex.org/W1982451263",
    "https://openalex.org/W6607789082",
    "https://openalex.org/W229986046",
    "https://openalex.org/W6646918945",
    "https://openalex.org/W2160133757",
    "https://openalex.org/W4245946003",
    "https://openalex.org/W1988644426",
    "https://openalex.org/W2039955996",
    "https://openalex.org/W2579648960",
    "https://openalex.org/W4255675220",
    "https://openalex.org/W4243048058",
    "https://openalex.org/W1597758989",
    "https://openalex.org/W6682949015",
    "https://openalex.org/W1562748096",
    "https://openalex.org/W2086973658",
    "https://openalex.org/W2169490753",
    "https://openalex.org/W1573837287",
    "https://openalex.org/W1490935989",
    "https://openalex.org/W2505424521",
    "https://openalex.org/W2964752397",
    "https://openalex.org/W594614676",
    "https://openalex.org/W1708874574",
    "https://openalex.org/W1499005519",
    "https://openalex.org/W2021944601",
    "https://openalex.org/W1528027857",
    "https://openalex.org/W1569055712",
    "https://openalex.org/W2798445359",
    "https://openalex.org/W17743224",
    "https://openalex.org/W2398146152",
    "https://openalex.org/W2052658973",
    "https://openalex.org/W4299678639",
    "https://openalex.org/W2040908214",
    "https://openalex.org/W2342091124",
    "https://openalex.org/W3038002524",
    "https://openalex.org/W1499631876",
    "https://openalex.org/W2093692407",
    "https://openalex.org/W2112564774",
    "https://openalex.org/W3107271803",
    "https://openalex.org/W2032152873",
    "https://openalex.org/W2155416760",
    "https://openalex.org/W2914763987",
    "https://openalex.org/W1585851245",
    "https://openalex.org/W2113709181",
    "https://openalex.org/W1503894330",
    "https://openalex.org/W1986164068",
    "https://openalex.org/W4239720002",
    "https://openalex.org/W1545003070",
    "https://openalex.org/W370177488",
    "https://openalex.org/W2564114053"
  ],
  "abstract": null,
  "full_text": "ARTICLE\nCoordinating the Complexity of Tools, Tasks, and Users:\nOn Theory-based Approaches to Authoring Tool\nUsability\nTom Murray1\nPublished online: 5 November 2015\n# International Artificial Intelligence in Education Society 2015\nAbstract Intelligent Tutoring Systems authoring tools are highly complex educational\nsoftware applications used to produce highly complex software applications (i.e. ITSs).\nHow should our assumptions about the target users (authors) impact the design of\nauthoring tools? In this article I first reflect on the factors leading to my original 1999\narticle on the state of the art in ITS authoring tools and consider some challenges facing\nauthoring tool researchers today. Then, in the bulk of the paper, I propose some\nprincipled foundations for future authoring tool design, focusing on operationalizing\nthe construct of complexity —for tool, task, and user. ITS authoring tools are major\nundertakings and to redeem this investment it is important to anticipate actual user\nneeds and capacities. I propose that one way to do this is to match the complexity of\ntool design to the complexity of authoring tasks and the complexity capacity of users\nand user communities. Doing so entails estimating the complexity of the mental models\nthat a user is expected to build in order to use a tool as intended. The goal is not so\nmuch to support the design of more powerful authoring tools as it is to design tools that\nmeet the needs of realistic user audiences. This paper presents some exploratory ideas\non how to operationalize the concept of complexity for tool, task, and user. The paper\ndraws from the following theories and frameworks to weave this narrative: Complexity\nScience, Activity Theory, Epistemic Forms and Games, and adult cognitive develop-\nmental theory (Hierarchical Complexity Theory). This exploration of usability and\ncomplexity is applicable to the design of any type of complex authoring application,\nthough the application area that motivated the exploration is ITS authoring.\nInt J Artif Intell Educ (2016) 26:37 –71\nDOI 10.1007/s40593-015-0076-6\nA shorter version of this article appears as a chapter in “Theory-based Authoring Tool Design: Considering the\nComplexity of Tasks and Mental Models”, Chapter 2 in R. Sottilare, A. Graesser, X. Hu, & K. Brawner (Eds),\nDesign Recommendations For Intelligent Tutoring Systems: V olume 3 - Authoring Tools and Expert\nModeling Techniques (2015).\n* Tom Murray\ntommurray.us@gmail.com\n1 School of Computer Science, University of Massachusetts, Amherst, MA, USA\nKeywords Authoring tools . Intelligent tutoring systems. Complexity science.\nEpistemic forms . Activity theory. Hierarchical complexity theory\nIntroduction\nIn 1999 I wrote an article on the “state of the art ” in ITS authoring tools R&D, and in\n2003 I co-edited a book on that subject with Sharon Ainsworth and Stephen Blessing.\nApparently the state of the art paper (Murray 1999, and its update in Murray 2003a)h a s\nbeen cited many times, prompting the managing editors of this journal to ask me, along\nwith authors of other “classic” IJAIED papers, to write a reflective piece about that paper\nand its topic for a special issue of the journal. I baulked at first because, though I have\ncontinued to build in-house authoring tools for all of my projects over succeeding years\n(it adds up to about 10 of them), it has been some time since I considered myself doing\nresearch on that subject and have not been keeping up with the associated literature. I was\nassured by the editors that the purpose was not to create a revised updated analysis of the\nstate of the art, but rather was invited to be somewhat of a storyteller about my journey to\nand after the original paper and to reflect in any way that seemed fit on the topic in the\ncontemporary context (for more recent work in the field, see Aleven and Sewall 2010;\nCristea2005; Olsen et al. 2013;S p e c h t2012; Suraweera et al. 2010; Mitrovic et al. 2009;\nSottilare et al. 2012, 2014;R a z z a qe ta l .2009;A l e v e ne ta l .2015; Ritter 2015).\nAs I thought about it realized that I did indeed have some thoughts on the subject\nthat reflect what I have learned on my winding interdisciplinary scholarly path over the\nlast decade. The authoring tools I have been building of late are for myself and research\ncollaborators for specific purposes (to visualize, edit, verify, and analyze complex\ninformation in data-driven instructional systems), but for “real” authoring tool projects\nthe question that seems to perpetually haunt is “who are going to use these tools and\nhow do we ensure that the tools meet end user needs? ” My approach to answering this\nquestion spirals around the theme of complexity. It turns out complexity is a complex\ntopic, especially for one with an interdisciplinary lens, and my musings are rather\nlonger than the Editors in Chief might have expected in a retrospective article. They\nwill think twice before asking me again in another 10 yeas. But before going any\nfurther I will start with a short introduction to the field of ITS authoring systems.\nITS Authoring Tool Design Tradeoffs Intelligent Tutoring Systems (ITSs) are\nhighly complex educational software appli cations (or learning environments) that\ncan include these components: User Interface (which might include a simulated\nphenomena or task environment), Expert Knowledge Model (of the task and/or\nknowledge), Learner Knowledge Model, Pedagogical Model, and Curriculum\nModel (also collaborative learning envir onments may include group-level aspects\nof any of these) (see Woolf 2010). For several decades developers and researchers\nhave been investigating the possibilities for creating ITS authoring tools because\nthese are hoped to (1) reduce the effort and cost of building or customizing ITSs,\nand (2) allow non-programmers, including teachers and domain experts (and even\nstudents) to participate fully or partly in building or customizing ITSs (Murray et al.\n2003;A l e v e ne ta l . 2006; Suraweera et al. 2010;C o n s t a n t i ne ta l .2013; Ainsworth\net al. 2003; Ritter and Blessing 1998).\n38 Int J Artif Intell Educ (2016) 26:37 –71\nThere are many design tradeoffs involved —the primary one being that in general the\neasier or more efficient a tool is to use, the more simplistic or constrained are the ITSs\nthat can be built from it. 1 Trivial examples at two extremes are: a tool that allows the\nauthor to select among checkboxes and lists to order and toggle and sequence features\nand curriculum items in an otherwise fixed system; vs. a tool that is so complicated and\nmulti-featured that building an ITS with it is not much easier than traditional software\nprogramming. One can imagine a design tradeoff space among usability, depth, and\nflexibility (see Murray 2004). Depth, which refers to the structural or casual depth of\nany of the ITS sub-models, is usually at odds with flexibility, which is the ability to\nauthor a diversity of types of ITSs. Usability is usually at odds with both depth and\nflexibility, i.e. a system that facilitates building deep models or many types of models\ntends to be more powerful yet less usable. A main theme of this article will be to\nprovide some rough metrics to help with these design tradeoffs.\nTowards Theoretical Foundations Unlike educational software, whose user audience is\nrelatively well defined and known, target users of authoring tools are less well defined and\nunderstood (unless the tool is intended for in-house use by a few specialized personnel). We\ncan draw from the standard literature on usability and Human-Computer Interaction\n(including user-participatory design), for tool design principles, but in addition there are\nsome issues specific to authoring tools (of an y sort, not just for ITSs) that I find quite\ninteresting. Influenced by topics I have studied since my early papers on the subject, I have\ncome to believe that a key issue is in how one matches the complexity of the authoring task\nto the complexity of a tool and the complexity-capacity of the target user. Thus, in the bulk\nof this paper I will sketch some preliminary considerations and principles that are intended\nto initiate inquiry in this direction. The suggestions are speculative, and are motivated by\nmy belief that the set of theoretical frameworks I will weave together are useful, unexplored\nwithin the ITS community, and have not been integrated in any prior work.\nTaking a theoretical approach to ITS (or any) authoring tool usability is rarely done,\nand risks being too theoretical for a field with such practical goals, but my aim here is to\npoint toward possible theoretical foundations for the (sub-) field. “Theory” can some-\ntimes refer to a mere conceptual framework (without any underlying causal theory), but\nhere I mean cognitive, social, epistemological, and/or information science theories that\nprovide theoretical underpinnings. Foundational theories (especially the Learning and\nCognitive Sciences) are now routinely considered in the design of ITSs and other\neducational software, but are rarely brought into discussions about the design or use of\nauthoring tools. Note however that the goal is not to produce a unified “theory” of\nauthoring tool usability, but to take a novel look at how one can base design usability\nprinciples upon several theoretical frameworks that seem relevant but have not been\nlinked with ITS authoring as yet.\n2\n1 These are generic tradeoffs, all other things begin equal. But note that complex systems can be constructed\nusing relatively simple tools, as in the CTAT system (Aleven et al., in submission). Representational\nformalisms that well match the domain and task needs will go a long way towards making authoring easier\nwithout compromising the complexity or scope of the resulting ITSs.\n2 Learning theories can and should be brought to bear on ITS authoring tool design, for example to constrain\nthe design of ITSs to adhere to known principles of learning (thanks to comments from Vincent Aleven and\nSteve Ritter for pointing this out). My focus here is on the more general theme of tool complexity and\nusability, which is not concerned with the content or model used by the tutoring system.\nInt J Artif Intell Educ (2016) 26:37 –71 39\nDesign Science and Usability Theory draw on socio-cognitive theories to\nexplore the relationships between the design of artifacts and the needs, capa-\nbilities, and limitations of in tended users (and other stakeholders) (see Oja\n2010;N o r m a n 1988;N i e l s e n 1993). Originally these theories were in response\nto the (now more accepted) realization that domain experts (those who are not\ninstructors), traditional software arch itects, and academics all historically have\ndifficulty predicting or imagining the needs and limitations of the average\nsoftware user and the average real-life task scenario (or difficulty predicting\nthe range of users and task scenarios). Thus software design, and artifact design\nin general, is increasingly understood as needing (1) empirical trial-and-error\ndevelopment, (2) the skills of rigorous empathy and imagination to put oneself\nin the shoes of a range of types of users and situations, and (3) some basis in\nunderlying psycho-socio-technical theory (Brown and Campione 1996; Cobb\net al. 2003).\nThe notion of assessing and coordinating complexity among tool, task, and user is a\ncentral theme in this particular theoretical exploration. In what follows, I will first\nreflect on the factors leading to my 1999 article on authoring tools. I will then consider\nsome challenges facing authoring tool researchers today. Then, in the bulk of the paper,\nI will propose some theoretical foundations for future authoring tool design. I will draw\nfrom the following theories and frameworks to weave this particular theoretical\nnarrative:\n& Complexity in software design\n& Activity Theory\n& Epistemic Forms and Games, and\n& Adult cognitive developmental theory (i.e. Hierarchical Complexity Theory).\nTheories of complex software design will be used to emphasize some of the\nissues, because ITS authoring tools are complex artifacts designed to produce\ncomplex artifacts . Complexity Science will also help us operationalize what is\nmeant by complexity in general. Activity Theory, which highlights the relationships\nbetween an artifact and its usage- tasks,u s a g e -rules,a n d community of practice, will\nprovide an orientation and basic vocabulary for the task of ITS design by various\ntypes of users in an authoring role. We can ask whether a tool and its “rules” of use\nafford the accomplishment of a particular task for a particular class of users. Much\nof the process of matching tool/task compl e x i t yt ou s e r( a n dc o m m u n i t y )c o m p l e x -\nity capacity revolves around the complexity of the mental models that a user is\nexpected to build in order to use a tool as intended. Collins ’sw o r ko n Epistemic\nForms and Games provides a highly useful framework for talking about this\ntool-rule-user match in holistic terms at the right level of granularity (Collins and\nFerguson 1993). At this point we will have a framework for describing many\nsources of complexity in tools, tasks, and users (cognition or mental models), but\nno good way to order or coordinate these types of complexity. For that we will draw\non Hierarchical Complexity Theory and related theories of adult cognitive devel-\nopment to suggest this order as a final step in matching the complexity of an\nauthoring tool to the complexity capacity of its target users (Commons and\nRichards 1984;F i s c h e r1980).\n40 Int J Artif Intell Educ (2016) 26:37 –71\nOverview Following is an extended overview of the narrative arc of the paper.\nFraming the context:\n& As a prelude I describe my history with the subject of authoring tools, and then\noutline some challenges facing authoring tool designers and researchers today.\n& How can we decide whether it is advantageous to build an authoring tool (vs.\nbuilding ITSs from scratch) and what features to include in an authoring tool? I\nframe questions about ITS authoring tool design in terms of design decisions and\ntradeoffs.\n& In designing ITS authoring tools there are a number of design tradeoffs that can be\nsummarized in terms of finding the right balance between usability (simplicity and\nefficiency of use) and power (flexibility/breadth and depth) for the intended author\ncommunity and the end-product educational software systems.\n& Authoring tools are quite labor-intensive to build, and yet the realistic demand (in\nterms of markets or needs for building many ITSs) and the realistic availability of\nITS authors and designers (e.g. the time-availability of teachers and experts to learn\nand use complex tools) is limited. Authoring tools may be practical only for\ndomains with high demand and reasonably established pedagogy (such as mathe-\nmatics).\n3 (However, increasingly learning and instruction are happening online,\nwhich argues for an increasing demand in general.) We can speak of finding the\nsweet spot that balances the cost of investing in authoring tool construction vs. the\n“risk” that the investment will not be worth it. This is also about matching the\nvision of a powerful and useful tool with the likely reality awaiting its release.\n& Classical design and usability theory applies to all of the tradeoffs mentioned above.\nSpecifically, the principle to “match between system and the real world ” speaks to\nusing vocabulary, mental models, and task-demands that users already have (or can\neasily learn). ITS authoring tools are complex systems created to build complex\nsystems. Though there are many design decisions to make, I propose that the\noverall lens of “complexity” is most useful, and informs the principles given in\nclassical usability theory. In a very general sense, complexity tends to increase with\npower and decrease with usability —and we are concerned with balancing com-\nplexity (which supports more power) with usability. That is, for tradeoffs related to\nusability, power, cost, risk, mental models, etc., we can speak in general of\nmatching the level of complexity of the tool with the “complexity capacity” of\nthose expected to use the tool.\n& The bulk of the paper explores a diverse collection of frameworks that are woven\ntogether to answer the question of how one might go about specifying the\ncomplexity of the system and the complexity capacity of intended users. This\nensemble of frameworks is only a beginning step pointing to additional work that\nwould need to be done to operationalize their implementation.\n& Complexity Science (including Information Theory) can be used to characterize\nthe complexity of software artifacts (i.e. authoring tools) in terms of the quantity\n3 There is of course much controversy about how to teach mathematics, and no consensus on pedagogy in any\narea. However, it seems more likely for mathematics (and perhaps physical sciences and basic computer\nprogramming), vs. many other domains, that content map and a pedagogical approach can be created that is\nacceptable to many.\nInt J Artif Intell Educ (2016) 26:37 –71 41\nand variety of parts and interactions within a system. Similar methods can be used\nto characterize the complexity of a task in terms of the quantity and variety of steps\nand decisions involved. “Dynamic complexity” must also be taken into account in\nauthoring tools, as authors are tasked with building and debugging systems that run\nor “behave” according to a specification. ITS authors are not only “constructing”\nartifacts but debugging them, which is much more challenging, and thus the\nusability principle to “help users recognize, diagnose, and recover from errors ”\nmay be the greatest limiting factor in matching the authoring tool and task to the\nskill set of intended users.\nDrawing insights from existing theories:\n& Complexity Science provided principles for characterizing systems, artifacts, and\nprocedures for using them, but says little about measuring complexity from the\nhuman and cognitive perspectives. For this we first bring in Activity Theory ,\nwhich provides a robust vocabulary and theoretical framework for coordinating\ntools, tasks, users, and user communities. We can layer our focus on complexity on\ntop of this framework, and speak about coordinating the complexity of tools, tasks,\nusers (mental models and skills), and the communities of practice that support\nusers. This provides an opportunity to speak not only to authors as individuals, but\nas members of knowledge building, design, or practice communities.\n& Focusing in on the user, we look at approaches for specifying the level of “cogni-\ntive complexity” in terms of the complexity of the mental model that the user\nneeds to understand or construct in order to use a tool. A useful framework for\ndoing this is Epistemic Forms (and Epistemic Games), which allows us to classify\nfeatures (interface components and procedures) into epistemic classes. These clas-\nses can roughly be ordered in terms of complexity. For example, filling in a text\nform or setting a slider level are examples of simple tasks, while filling in a table or\nhierarchy have medium complexity, and creating and debugging formal procedures\nhas much complexity.\n& The complexity of both the user/cognitive factors (mental models and task proce-\ndures) and various dimensions of system/tool properties could be measured quan-\ntitatively through systematic analysis of all of the parts, relationships, etc. (as\nindicated from Complexity Theory). One of the challenges here is the “dimension-\nality issue”of how to combine what we determine about the complexity of each\ndimension into a holistic impression of overall complexity. For example, we might\nqualitatively analyze the complexity of the number of components, but how to we\ncombine that with the number of types of components, and with the level of\ndynamic complexity— is it possible to “compare apples with oranges, ”so to speak,\nin a holistic assessment?\n& We suggest that the effortful detail of quantitative complexity analysis is not\nnecessary for our goals (though it may be useful for some applications), and that\nam o r e qualitative and comparative approach will suffice. That is, we can catego-\nrize system complexity and cognitive capacity qualitatively roughly into types or\ngroupings that will provide sufficient insight into the design questions about\nmatching the complexity of tools to the complexity capacity of users. We began\ndoing this with the categories of epistemic forms above, which partially address the\n42 Int J Artif Intell Educ (2016) 26:37 –71\ndimensionality issue because they provide a more holistic assessment of the\nstructure of a system.\n& We can thus construct rough categories of low, medium, and high complexity for\nmatching tools to users. When more detail or specificity is needed, the frameworks\nmentioned in this paper can be employed to place tools and users into more precise\ncategories.\n& Using Epistemic Forms only partially solves the dimensionality (apples vs. or-\nanges) challenge of integrating various types of complexity into a holistic picture.\nTo further address this I draw from theories of Hierarchical Complexity(including\nSkill Theory, from neo-Piagetian developmental research). Hierarchical complexity\ntheories give research-based support for the idea of content-general and holistic\nmethods of specifying cognitive and task complexity (we use them here quantita-\ntively but are also applicable for precise quantitative metrics of complexity). I use\nthe principles of Complexity Theory to suggest a complexity ordering from simple\nobjects, to abstractions/mappings, to formal systems, to dynamic systems, to\nsystems of dynamic systems (architectures and ecosystems) that gives a theoretical\nbasis for ordering epistemic forms (and thus mental models and tasks) in terms of\ncomplexity.\nIn the end we have (1) a rough model showing one example of what a mapping from\nauthoring system features to user characteristics (complexity capacity) might look\nlike—which describes low, medium and high complexity levels; and (2) a framework\nfor creating more valid, tailored, or detailed models that accomplish the same goal.\nSuch models can help authoring tool designers (A) assess the complexity capacity of\nintended author tool users, (B) weigh the risk involved in investing various levels of\neffort in authoring tool design; and (C) make design tradeoffs in usability and power\nsuch that tools are appropriate to realistic use contexts.\nThe (MY) Story So Far\nBecause authors for this special issue were invited to include some personal retrospec-\ntive narrative, I will describe how it was that I ended up working on ITS authoring tools\nin the first place. After graduating from college with an undergraduate degree in\nPhysics (and a minor in Philosophy) I worked in industry in an “advanced prototyping”\nR&D unit in a leading semiconductor manufacturer. At the end of three years I found\nthe corporate life to be unpalatable and I bid it goodbye for a time. After a year of soul\nsearching and travel, in 1983, I found myself at UMass in a radical “create your own\ndegree” graduate program called the Future Studies Program (a department in the\nSchool of Education that had close links to Buckminster Fuller and other luminaries).\nThe doctoral program I wanted to create involved a study of Physics and Philosophy,\nwith a goal of becoming a writer who would further enlighten the masses about the\ndeep themes I was excited about in popular books such as The Dancing Wu Li Masters\nand The Tao of Physics. Having had some experience with the early days of computing\n(in the punch-card days), I also took up with faculty and students in the Instructional\nTechnology program, which had ever-closer links with Future Studies because the\nfuture was looking very digital. Look ing back, I never really had the skills to\nInt J Artif Intell Educ (2016) 26:37 –71 43\naccomplish my original goal, and was therefor quite lucky to have been drawn to the\ncourses on Intelligent and Adaptive Tutoring and Learning Systems taught by Beverly\nWoolf. Before long this was my new academic focus and I was spending more time in\nthe Computer Science Department than in the Future Studies department. I dove into\nclasses in Artificial Intelligence, Cognitive/Learning Science, and constructivist edu-\ncational theories, in addition to slogging through course requirements for a degree in\nComputer Science.\nThe penchant for taking an ever wider system ’s perspective that drew me to Physics\nand Philosophy eventually caught up with me in my work in instructional systems. After\nsome time doing research assistant work on a couple of ed-tech projects I began to form\nthe plan for my dissertation research.\n4 Woolf’s dissertation work, and, at the point, her\nmain scholarly “claim to fame ” was a formalism for representing tutoring strategies\ncalled Discourse Action Transition Networks. My studies in Instructional Design Theory,\nLearning Theory, and my work with Clement had introduced me to the wide variety and\ncomplexity of strategies that were being proposed for teaching/learning specific types of\nknowledge (facts, concepts, principles, skills, analogies, mental models, problem solv-\ning, scientific inquiry, etc. —and numerous sub-classes of all of these). Woolf ’s formal\nDACTN framework (Woolf and McDonald 1984) had only been implemented in\nprototype domains, and I was interested in what it would take to represent multiple\nstrategies and meta-strategies in an intelligent tutor. I wanted the research to be general,\ngrounded, and extensible by working with real instructors in several domains. I wanted\nthe system to be usable —to have real instructors be able to create, modify, or at least\ninspect and select, among a library of theoretically grounded teaching strategies.\nAs is often the case for dissertation plans, the scope of my work was reduced by half,\nand half again, before I completed. I realized that the first thing I needed to do to be\nable to work flexibly with a wide range of tutoring strategies and domains was to build\nan authoring tool so that strategies and domain knowledge could be visually represent-\ned clearly enough for domain experts and teachers to understand them; and for the\nresearcher to easily modify and experiment with alternative strategies. Creating a\ndomain-independent ITS authoring tool was to be the first step; researching and\nrepresenting a wide variety of strategies was to be the second; and working with\ninstructors to represent domain knowledge and test and improve the teaching strategies\nwas the final planned step. Looking back of course, the plan was amusingly over-am-\nbitious, as dissertation proposals can be. The task of just creating the authoring tool\noccupied the entire project, and continued to be extended in grant-funded research for\nmany years (with the KAFITS and Eon systems).\n5 Results of Encoding Knowledge\nwith Tutor Construction Tools. In the Proc. of the Tenth National Conference on\nArtificial Intelligence (AAAI-92). San Jose, July 1992, pp. 17 –23.). We did represent\na number of domains and teaching strategies in our ITS authoring tools, but the vision\nof having an overarching system for knowledge types and teaching strategies was\nelusive (and, I now know, may always be elusive, though the team working on the\n4 My first major research project was with Clement, who was a senior member in a novel program called the\nScientific Reasoning Research Institute housed in the Physics department. The department was known as a\nhotbed of radical constructivists, including Ernst V on Glasersfeld, and sponsored leading-edge research and\napplied programs in constructivist approaches to math and science.\n5 KAFITS: Murray and Woolf 1992; Murray 1996;E o n :M u r r a y2003b and www.tommurray.us/eon_www/\neon.html.\n44 Int J Artif Intell Educ (2016) 26:37 –71\nGIFT authoring system has come the furthest thus far; see Kumar et al. 2010; Sottilare\net al. 2012).\nThe topic of authoring tools within the ITS community was relatively new and fast\nexpanding. I am the kind of person who hates the thought of “reinventing the wheel” or\nof missing some important relevant corner of the R&D literature. As I continued my\nwork I kept up an interdisciplinary overview of related work. It seems that I was the\nonly researcher at the time motivated to write a full overview of the state of the art.\nFrom my later studies in “Hierarchical Complexity Theory” (Commons and Richards\n1984; Commons et al. 1998) I now know that what I accomplished was a\n“meta-systematic” or perhaps “paradigmatic” overview the field. That is, in addition\nto describing the authoring system projects and theoretical frameworks in existence, I\norganized and coordinated these perspectives into a coherent whole, and, so to speak,\norganized perspectives on perspectives in my overview publication(s). One could say\nthat the design of authoring tools is “ITS-complete” (a play on the term NP-complete),\nin that to design a generic software tool one needs to create a generic representational\nframework that anticipates both practical and theoretical issues. Therefor, a full treat-\nment on authoring tools will tend to include a type of overview of ITS theory in\ngeneral. My 1999 Overview paper included sections on:\n& Types of ITSs and domains that have been authored— a categorization of 29\nexisting ITS authoring tools into 7 primary categories for analysis;\n6\n& Methods for authoring the Interface, Domain, Teaching, and Student Models\n& General authoring/knowledge acquisition methods\n& Design tradeoffs —including Power/flexibility (=Breadth x Depth); Usability\n(=Learnability x Productivity); Fidelity; and Cost. These tradeoffs exists for all\nITS components individually (domain, tutoring, and student models; and learning\nenvironment).\n& An outline of 5 ITS authoring roles and their related skill-sets, with an analysis of\nwhat types tool features should exist for each role (similar to Table 1 shown later).\n& Case studies and pragmatics (actual use scenarios and descriptive statistics, effi-\nciency estimations, evaluations performed)\nThough authoring tool R&D has progressed, the categories and frameworks I\nsuggested seem just as relevant today as then, which may explain why the paper is\nstill cited.\nThe focus of my work has moved away from research per-se on ITS authoring,\nbut I have continued to build special-pur pose authoring tools for almost every\ncomputer-mediated learning project I have been involved with (MetaLinks, Rashi,\nSETS, Wayang, and Simforest-G —see descriptions at www.tommurray.us ).\nThough these projects did not aim fo r as much generalizability as the Eon or\nKAFITS frameworks, I continued to face a nd learn from the issues highlighted in\nthe earlier work.\n6 The paper described the Strengths, Limits, and Variations among ITSs with these areas of strength or\nspecialization: Curriculum Sequencing and Planning, Tutoring Strategies, Device Simulation and Equipment\nTraining, Domain Expert System, Multiple Knowledge Types, Special Purpose, Intelligent/Adaptive\nHypermedia.\nInt J Artif Intell Educ (2016) 26:37 –71 45\nTable 1 Authoring tool user roles and complexity capacity estimates\nRoles\n(tool use roles)\nBenefits\n(of that role)\nProblems\n(of that role)\nComplexity\nCapacity for\nITS design\nTeachers\nPRACTICAL\nPractical experience Not good at artic ulating or abstracting expertise LOW\nDomain Experts & Content Developers\nPARTIAL\nAuth. tool infers the instructional methods A fixed instructional method MED\nInstructional Designers & Learning\nTheorists\nTHEORETICAL\nKnow learning theories & research Rare; not trained in knowledge engineering MED\nKnowledge Engineers and ITS Developers\nEXPERIENCED\nKnow the tools; Are sometimes also plugged into user testing May not know what it is like to teach or learn the material MED-HIGH\nComputer scientists & Software developers\n(ACTUAL?!)\nComplexity capacity. Do not have to build to a real user base. Bits intuitively obvious to the casual observer…^ HIGH\n46 Int J Artif Intell Educ (2016) 26:37 –71\nChallenges Facing Authoring Tool Research Today\nPredicting Future Flying Machines ITS authoring tool research is in an interesting\nsocio-techno–historical position. Intelligent Tutors, despite 30 years of R&D, are not\nyet common in mainstream education or training, though a few notable systems have\nachieved wide-spread use (Koedinger et al. 1997; Heffernan and Heffernan 2014;\nGraesser et al. 2005; Vanlehn et al. 2005;M i t r o v i c2012; Johnson and Valente 2008;\nSitaram and Mostow 2012). This may be a completely appropriate development and\nadoption arc for a technology this complex and innovative, and we have every reason to\nbelieve that the results of advanced technology learning systems (A TLS) research will\ncontinue to influence on-the-ground computer-mediated learning. However, authoring\ntool researchers are in the awkward position of developing the cart before the horse. Or,\nworse yet, developing the cart-factory before the horse. It is as if, as the Wright brothers\nwere experimenting with the first airplanes, a group of researchers and academics were\nobserving on the side, working out how to design airplane factories that would make\nairplane production efficient and flexible. As those first manned flight contraptions\nwere being developed, it would have been difficult to predict what future flying\nmachines would look like, never mind what the market would be like or how to best\nmass-produce and easily customize them for typical users.\nOf course, ITS work is well beyond its first prototypes, so this analogy is stretched.\nStill, authoring tool designers work under considerable uncertainty as to what types of\nsystems will find their way to substantial use and benefit from the scale and flexibility\nthat authoring tools enable. However, we are talking about software here, not equip-\nment manufacturing. Building abstractions and design tools is a natural impulse in\nsoftware design (procedural-, data-, and knowledge-abstraction are basic computer\nscience principles—see Abelson and Sussman 1983). As indicated in the history of\nmy own projects, it can be beneficial to build authoring tools merely to facilitate local\nor small scale R&D projects. A company that makes a decent profit on one single piece\nof widely used software (say an ITS) would benefit from building authoring tools to\ncustomize and enter content for the ITS. However, for systems built for in-house use it\nis more difficult to frame scholarly research questions and findings.\nOld vs. New Conceptions of What an ITS is The original understanding of compu-\ntational “intelligence” in ITS ’s involved mostly modeling and knowledge representa-\ntion tasks —learner, domain, and instructional models. The more deeply Cognitive\nScience understands knowledge and learning the more difficult these modeling tasks\nappear for authentic situated tasks. In general the most successful ITS ’s are those\nfocusing on knowledge that is the easiest to represent, including declarative facts and\nprocedural steps. Yet developments in Learning Theory increasingly emphasize the\nimportance of forms of knowledge that are more difficult to formalize, such as\nmetacognition, conceptual understanding, problems solving, open ended inquiry, col-\nlaboration, communication, argumentation, hypothetical and analogical thinking, etc.\nThe more basic forms of knowledge (fact, skills, and concept-map-like relationships)\ncontinue to have fundamental importance as building blocks for more sophisticated\nskills, but the more exciting work in ITS/ATLS has been moving into a wide variety of\nareas that do not involve “deep modeling ” of knowledge or expertise. These new\nresearch trends include: recognizing and responding to affect; using big data to classify\nInt J Artif Intell Educ (2016) 26:37 –71 47\nand predict learner behavior, wearable gadgets, immersive experiences, natural lan-\nguage understanding and production, game-ification and social-media-ification. For a\nproject to be considered “ITS” research it no longer requires computational intelligence\nper se, but only the inclusion of some state-of-the-art computational technology (or\nleading edge techno-socio-psycho theory). While the idea of a generic ITS framework\nrequires some commonality of basic components and/or representational frameworks,\nthe scope of ITS ’s is becoming increasingly diverse, and overarching frameworks are\nincreasingly difficult to envision. However, once could counter that as diversity\nincreases, so does the number of projects, so that the actual impact of designing generic\nframeworks still serves a significant (if smaller percentage-wise) potential user base.\nAuthoring tools are still essential for scale-up, wide adoption, and easy customiza-\ntion of learning systems, though each may need to be specific to a very specific genre of\ninstructional systems. If so, authoring tool design may become more of an engineering\nchallenge than a research area. However, there are still important theoretical issues that\ncan be investigated, which we explore next.\nSoftware Usability and Complexity\nUsability and Managing Software Development Risks Bracketing the above con-\ncerns, let us assume that ITSs of some sort will indeed become mainstream and that\nauthoring tools will become increasingly important—a safe bet I think. Other than tools\ndesigned for in-house use by highly trained specialists, authoring tools, by their nature,\nmust be usable by some anticipated user audience. As mentioned, with any tool there\nare context-specific usability concerns that can be worked out through good design and\nHCI practices (prototyping, early feedback from authentic users, etc.), but here I would\nlike to look at very general usability concerns, having to do with the complexity of\nthese systems.\nITSs are complex software applications and full-featured ITS authoring tools can be\nan order of magnitude larger and more complex —just as a machine designed to build\nmany types of lamps is much more complex than a lamp (though the machine itself\nmay be relatively easy for the end user/author to use, its interiors will be more\ncomplex). Next we will look to the literature on the design and usability of complex\nsoftware systems for advice relevant to ITS authoring tool design.\nDesign tasks such as authoring ITSs fall under the “ill-defined” and “wicked”\nproblems characteristic of real-world projects (Conklin 2005; Mirel 2004). In his\ntreatment of usability of complex systems Oja ( 2010) defines complex software\ndevelopment in terms of Mirel ’s definition of complex problem-solving, which in-\nvolves “ill-defined situations; vague or broad goals; large volumes of data from many\nsources...; nonlinear, often uncharted analytical paths; no pre-set entry or stopping\npoints; many contending legitimate options; collaborators with different priorities;\n[and] ‘good enough ’ solutions with no one right answer. ” Chilana et al. ( 2010)g i v e\nthree additional factors that contribute to the complexity of designing usable software:\ndomain-specific terminology, every situation is unique, and limited access to domain\nexperts. ITS/ATLSs and their authoring tools certainly have all these characteristics.\nOja contends that Nielson ’s classic usability heuristics are even more critical for\ncomplex software development (Nielsen 1994). Nielson’s usability heuristics include:\n48 Int J Artif Intell Educ (2016) 26:37 –71\nreification (visualization of key abstractions and relationships; minimize working\nmemory load), user control and freedom (not constraining user actions any more than\nis necessary), flexibility in outcomes (to allow for variations in style and needs), match\nbetween system and the real world (using the vocabulary and mental models users\nalready have); help users recognize, diagnose, and recover from errors, user control and\nfreedom and flexibility and efficiency of use.\nEchoing the heuristic to “match between system and the real world, ” Johnson (2006)\nanalyzed software usability failures in the Healthcare sector that imposed significant\nfinancial and acceptance burdens within that sector, and found that “many usability\nproblems stem from the inability of suppliers and manufacturers to anticipate [user]\nrequirements.” The educational technology R&D community is poised to create ITS\nauthoring tools that could be used on a large scale. As the investment in authoring tools\nincreases, there is a corresponding increased “risk” that investment in design, outreach,\netc., will outweigh the benefits if the tools do not directly meet the needs of a wide\nvariety of users (or if the ITS that is built with the tools does not reach a large number of\nlearners).\nFigure 1 illustrates the type of risk management and risk reduction principles\nincreasingly being used in software and other industries.\n7 Additional investments in\nsoftware can follow the “80/20” rule where perfecting the last 10 % or 20 % can take a\ndisproportionate amount of effort. Meanwhile, the return on user value gets propor-\ntionately less. The goal is to find the sweet spot where risk is acceptably low and\nexpected value is relatively high ( “optimum” in the Figure). To mitigate this risk\nusability principles recommend both empirical and theoretical grounding: i.e. usability\nevaluation and user-feedback from authentic contexts done “early and often; ” and a\ngood theoretical understanding of the user and task. Complexity is a useful construct\nfor operationalizing Johnson ’s “[ability] of suppliers and manufacturers to anticipate\n[user] requirements, ” but the construct needs better definition for this to happen —\nwhich is what we hope to contribute to here.\nComplexity Science and Information Theory Next we branch away from com-\nplexity in software and usability theory t o consider how complexity is theorized in\nmore general terms. Complexity Science points to various methods for measuring\ncomplexity which are all related to the amount of information contained in an\nobject, system, or process, with “information” being closely related to the con-\ncepts of difference, discernibility, and degrees of freedom. Information and com-\nmunication theories also quantify information (and “meaning”) in terms of entro-\npy, randomness, chaos, “surprise,” and “shortest possible description ” (Grünwald\nand Vitányi 2003). There are many individual metrics that contribute to overall\ncomplexity, including the number and diversity of components and their structural\nor functional relationships (Benbya and McKelvey 2006). Complexity Science\nalso deals with time-based phenomena: cha nge, feedback loops, self-organization,\nevolution, and emergence in dynamic systems —so-called “complex adaptive\nsystems.”\n7 Image adapted from http://www.labcompliance.com/tutorial/risk/default.aspx?sm=d_a, “Risk Management\nin the (Bio)Pharmaceutical and Device Industry,” L Huber & Labcompliance Inc., http://www.labcompliance.\ncom/tutorial/risk/default.aspx?sm=d_a.\nInt J Artif Intell Educ (2016) 26:37 –71 49\nCampbell ( 1988) describes three sources of complexity: number of dimensions of\ninformation, the rate of information change, and the number of alternatives associated\nwith each dimension (i.e., information diversity). We will modify and generalize this\nscheme as in Fig. 2, using the categories of structural, dynamic, and perspectival\ncomplexity.\nFor structural complexity, other things being equal, systems are more complex if\nthey have: more parts (e.g. an ant colony or huge Lego project); more types of parts\n(e.g. a car or human anatomy); more properties in each part; more relationships or\nconstraints among the components; and more types of relationships. In part relation-\nships, one-to-one mappings (relationships) are the simplest, one-to-many mappings are\nmore difficult, and many-to-many mappings are most complex to manage and\nconceptualize.\nIn addition to these structural dimensions (which are metaphorically space-like),\nsystems whose properties, relationships, and objects change over time are more\ncomplex (the dynamic or temporal dimension). Dynamic complexity is represented\nFig. 1 Cost vs. value in software risk assessment\nFig. 2 Sources of system complexity\n50 Int J Artif Intell Educ (2016) 26:37 –71\nusing laws, rules, mechanisms, or influences. Feedback loops and nonlinear dynamics,\nall outside our scope to elaborate on, come into play.\nAs indicated above, complexity is related to information intricacy, space of\npossibility, and even “meaning,” and is thus not simply an objective property of\nsystems, but it has a quasi-subjective component that involves human context,\nactivity, and the reasons for doing the complexity analysis. In software, information\nsystems, and usability analysis there are cognitive and epistem ic considerations.\nByström & Järvelin ’s analysis of task complexity includes factors such as: repeti-\ntiveness, analyzability, a-priori determin ability, number of alternative paths, out-\ncome novelty, number of goals and conflictin g dependencies, uncertainties between\nperformance and goals, number of input s, and time-varyin g conditions of task\nperformance (Byström and Järvelin 1995,p .5 ) .Z h a n ge ta l . ’s( 2009) “epistemic\ncomplexity” measures complexity in terms of the movement from facts to expla-\nnations and from unelaborated to elaborated knowledge —both of which indicate\nincreasing depth and complexity. Epistemic complexity includes measurement of\nthe “diversity ” and “messiness ” one encounters in a situation (Bereiter and\nScardamalia 2006). Thus concepts of nuance/subtlety, abstraction/generalization,\nuncertainty/ambiguity must be considered.\nTherefore, in Fig. 2 we have the third category “perspectival” complexity, which is\ncomplexity due to multiplicity and uncertainty, including conflicting goals or subtasks;\ndiverse perspectives among stakeholders; stochastic randomness and indeterminacy;\nand vagueness and uncertainty in any of the structural or dynamic elements (measuring\nthese would be more heuristic than the other two complexity factor types). Perspectival\nfactors relate as much to subjectivity and the nature of cognition as to the objective\nnature of the artifact.\nUsability Complexity and Runnable Artifacts In terms of software systems, specif-\nically authoring tools, the factors mentioned above can be applied to the software\nartifacts (code and interface), development (programming or authoring) or the com-\nplexity of use (the user interface understanding and the mental model a user must\nacquire to understand a system). Theoretically each of the sources of complexity in\nFig. 2 could be enumerated or estimated and combined to measure the complexity of a\nsystem toward the goal of comparative analysis of the complexity of systems.\nArtifacts that ‘run’ or behave dynamically are of course more difficult to author.\nWith authoring tools and educational software such as Scratch and StarLogo, and\nscripting languages in Office applications, the line between programming and using\nsoftware is increasingly blurred. ITS authoring can fall anywhere along a spectrum of\ncomplexity from customizing parameters and choosing content to creating teaching\nstrategies, which is closer to software programming.\nITSs are dynamic systems that must be run to test them. They have multiple learning\npaths and it is intractable to test every possible student behavior. Unpredictable\nbehaviors inevitably occur in complex software. The simplest systems have predicable\npaths with little interaction or parameterization, such as Scripts and story-board type\nprocedural flows. If an authoring tool allows branches, if/then rules, procedures, loops,\nparameterized subroutines, or recursion (in rough order of difficulty) the level of\nauthoring complexity jumps dramatically. The author is essentially doing software\nprogramming. Writing and debugging computer programs is a complex task requiring\nInt J Artif Intell Educ (2016) 26:37 –71 51\nspecial skill and tools. Without these skills, and even with them, it can be quite difficult\nto determine the source of a run-time software bug.\nCreators of authoring tools that allow authors to enter into this level of task\ncomplexity must (1) not underestimate the complexity of the task or overestimate the\nskill of the typical user, and (2) provide real debugging and tracing tools —for the\nsystems to be viable. One of Nielsen ( 1994) “Top 10″ recommendation for usability is\nto “help users [authors in this case] recognize, diagnose, and recover from errors. ” This\ncan be as simple as providing an Undo feature for authored content, but for systems\nwith dynamic complexity special tools are needed to trace and debug procedural\nrepresentations.\nLike most software systems, ITSs should be designed in user-participatory feedback\nloops, where, as Benbya & McKelvey note, “the critical factor in all information\nsystems is continual change” (from “Toward a complexity theory of information\nsystems development”, 2006, p. 20). This might even imply that viable authoring tools\nshould have some sort of “version control” subsystem.\nThe above discussion suggests factors that could be considered in characterizing the\ncomplexity of software tasks and interfaces. It is implied that for some tasks, such as\nversion control and debugging, there is a need for special skill such as knowledge\nengineering. Thus it is also important to consider the “complexity capacity” of users\nand communities of practice —and for this we turn next to Activity Theory.\nActivity Theory— Users, Tasks, Tools, and Communities\nWe borrow concepts from Activity Theory, set of principles about socio-technical\nhuman action and design, which stresses the mediating role of tools (artifacts) and their\nusage rules in collective human activity and development (Jonassen and\nRohrer-Murphy 1999; Stahl 2006; Engestrom et al. 1999). Here rules indicate the\n(sometimes implicit) skills, understandings, and habits held by a community of practice.\nThus we can frame our exploration of authoring tool usability in terms of the interaction\nbetween users, tools, rules,a n dtasks . We can ask whether a tool and its “rules” of use\nafford the accomplishment of a particular task for a particular class of users. Clearly our\nusers are authoring tool users and the task is to design or customize an ITS; and later we\nwill introduce “epistemic forms/games” as a way to describe the rules of use.\nFigure 3 illustrates these factors in Activity Theory terms (adapted from Jonassen\nand Rohrer-Murphy 1999; Engestrom et al. 1999). Thus, from our focus on the concept\nof complexity, we must consider:\n& Task and Rule complexity (user activity methods and goals)\n& Tool (artifact) complexity\n& Socio-cognitive complexity (community of practice and division of labor)\nWe are concerned with the match between:\n& User vs. Tool complexity\n& Task vs. User complexity\n& Community of Practice vs. Tool complexity\n52 Int J Artif Intell Educ (2016) 26:37 –71\nWhen we speak of users we are really speaking of users in particular roles.T h i s\ndistinction is important when we begin to speak of the complexity capacity of a user (or\ntype of user) —we are not referring to a person ’s general ability to handle complexity,\nbut to one ’s ability within a certain role (ITS author, content developer, tester, etc.),\nwhich might depend more on training and experience than on innate intellectual\nsophistication).\nCampbell notes that there are several approaches to assessing complexity: as a\nsubjective psychological experience of th e user, as an objective measure of the task,\nand as an interaction between subjective and objective elements ( 1988,p g .4 4 ) .\nWhile measuring complexity in terms of user (author) experience is important,\nmethods for doing so are outside our scope here. However, we will describe\nmethods for describing user capacity, and we assume that on average complexity\ncapacity is closely related to the complexity experience of the user (they will be\nfrustrated or confused if their complexity c apacity in a particular role is mismatched\nfor the task). In the prior section we sketched heuristic frameworks for assessing\ntask and tool complexity objectively. Our eventual goal is to assess the match (or\ninteraction) between user capacities and the measures of tool/task complexity (user\ncapacities will be roughly estimated, wh ile tool/task complexity affords more\nobjective measurement).\nNote that in the prior section tool and task complexity were treated together. Unlike\nsimple tools such as a hammer, for which the task a tool is used for (e.g. building a\nbarn) is usually much more complex than the tool itself, for most software tools the\ncomplexity of the tool features can stand as a fair indication of the complexity of the\ntask. This is of course not strictly true, as building an ITS involves much more than\nusing an authoring tool (e.g. applying learning theory, paper mock-up design, etc.), but\nfor simplicity we will assume that the complexity analysis given above of artifacts\n(tools) maps well to complexity analysis of tasks. Task-related issues of how the tool is\nused and learned will be categorized in Rules or COP (community of practice) elements\nof Activity Theory, rather than with the artifact.\nFig. 3 Activity theory\nInt J Artif Intell Educ (2016) 26:37 –71 53\nEpistemic Complexity and Complexity Capacity Oja quotes Haynes and\nKannampallil (2004) who say that “complex software applications require great cog-\nnitive skill, integration of knowledge from various areas, and advanced instruction and\nlearning; thus, it is not surprising that ‘screen deep’ interfaces to such systems may not\nyield the best results in terms of usability. ” This is one reason why understanding the\nintended user is so important —because making a tool more easy to use, i.e. “usable,”\nmay dumb it down too much for some users or tasks, and decrease “user control and\nfreedom” and “flexibility and efficiency of use ” (from Nielson ’s model) for those\ncontexts. Oja ( 2010): “As Mirel ( 2004) points out, most current HCI practices concen-\ntrate on ease of use or simplifying the work, and this may lead to ‘producing good\ndesigns but for the wrong problems ”’“(p. 3800). The design goal is thus to make tools\n“operationally simple, while intellectually sophisticated and nuanced ” (ibid).\n“Cognitive complexity” is one term used to describe a person ’sc a p a c i t yt op e r f o r m\ncomplex mental or behavioral tasks. Cognitive complexity involves not only the\nnumber and complexity of the objects and relationships as described above, but also\nthe ability to perceive nuances and subtle differences —i.e. it can involve both integra-\ntive and differentiating capacities (Mirel 2004). Jordan uses the term “complexity\nawareness” for “ap e r s o n’s propensity to notice...that phenomena are compounded\nand variable, depend on varying conditions, are results of causal processes that may\nbe...multivariate and systemic, and are embedded in processes [that involve non-simple\ninformation feedback loops] ” (Jordan et al. 2013, p. 41). As mentioned above, Zhang\net al. ( 2009) use the term “epistemic complexity” which includes an understanding of\nunderlying reasons, theoretical explanations, or hidden mechanisms within phenomena.\nBelow will use the term “complexity capacity” to remind us that cognitive complexity\nrequired for a task is about the context and role a person is in, and depends on\nexperience in addition to any general complexity “intelligence” they may have.\n8\nIn our exploratory discussion of software usability and complexity we enumerated\nmany factors and it remains for future work to determine how these factors are\noperationalized, weighted, and combined in any overall complexity metric (a process\nthat may be quite context-specific, as complexity components will have different\nweights for different situations). As we move from characterizing the complexity of\ntools (software) and tasks (authoring) to that of users, the approach will continue to be\npreliminary and suggestive, with many details remaining to be worked out beyond this\npaper. Lets assume, for simplicity, that we have worked out the details of a scheme such\nas the one described in prior sections of this paper, and have devised a method to\ncharacterize task/tool complexity level, and that we have collapsed the dimensionality\nof analysis to rate tasks/tools on a scale of low...medium...high complexity. How might\nwe map this to user (or community of practice) complexity capacity? Table 1 illustrates\nwhat such a mapping might look like, showing types of authors, benefits and problems\ntypical of each author type, and the level of design complexity one can typically expect\nin the authoring task.\nTeachers have on-the-ground experience of the needs of students and classroom\nsituations, and, while their input should be included in the iterative design process, they\n8 Cognitive complexity might be used as an informal construct useful to differentiate different potential users,\nbut it may also be possible to measure it in a way relevant to ITS authoring. A Google Scholar search of\n“measuring cognitive complexity” yields pointers to many attempts to do so.\n54 Int J Artif Intell Educ (2016) 26:37 –71\ncannot be expected to have the skill, nor the time, to use or learn how to use complex\nauthoring tools. Domain experts and content developers are more typically used to\ndefine knowledge and expertise, though they may have little practical or theoretical\nknowledge of pedagogy. Instructional designers and learning theorists bring different\nsources of pedagogical knowledge and epistemological knowledge (understanding how\nknowledge is structured), though they may not have the time to dedicate to a steep tool\nlearning curve.\nFor all of the above user types, the task of representing knowledge in a computa-\ntionally usable fashion may be foreign —while knowledge engineers are trained in\nexactly that task. It is only with this level of skill and higher that we can expect\nsophisticated authoring tasks to be managed.\n9 Most user communities will not have\npeople with knowledge engineering (or ITS design) skills, meaning that users at this\nlevel will usually be part of a dedicated ITS design team, which would only exist in an\nacademic lab, a company dedicated to building learning systems, or an educational\norganization large enough to form such a team to be shared widely (e.g. a university or\ncity school district).\n10\nThe final category of users in Table 1 is computer scientists and software developers.\nThis category connotes the unfortunate yet understandable fact that many ITS authoring\ntools never see a robust user community and are only used within the confines of the\nteam or organization that built the tool. This stakeholder group tends to be the most\nsophisticated in terms of designing complex structural and procedural models. The\nbenefit is that more powerful ITSs can be built, but the drawback is that without\nusability input from “real” users, the tools may be too complex to expect many others to\npick up, and the tool designers may be out of touch with the needs of intended users.\nIn authentic contexts the actual “capacity” o fau s e rt ou s eat o o lt oa c c o m p l i s hat a s k\ndepends on “community of practice” considerations as well as the potential complexity\ncapacity level of the individual (Fig. 3). These considerations include: (1) opportunities,\ninvestment and incentives in training; (2) community of practice peer and mentor\nsupport; and (3) time available to author. Thus, even if a user, say an unusual teacher,\nhas a high level of generic complexity capacity, in order to successfully make use of an\nITS authoring tool they would need to be able to invest time in the learning curve, have\nthe support of peers and superiors in adopting this new technology, and have the\nongoing time available to do the authoring. Contexts satisfying these conditions are\nindeed rare. It should also be noted that actually it is the capacity and skill set of the\ndesign/authoring team, not of any individual, that is important.\nIn addition, for newly introduced artifacts there is a dynamic, often evolutionary,\ninterplay between artifacts (their design), the standard and novel ways that artifacts are\nput to use, and the human capacities enabled by artifacts. That is, new tools create new\ncapacities, which create new possibilities and new goals/tasks; around which new (or\nimproved) communities of practice develop —all of which in turn prompt new\n9 Here “sophisticated” refers to the complexity of the authoring task, not the domain knowledge of, say a\nteacher, which may be sophisticated in another way.\n10 Note that this specific scheme is suggestive and meant to illustrate a framework rather than the “content” of\nthe framework—i.e. I do not need to make a strong argument here that, e.g. “Domain Experts & Content\nDevelopers” have a limited or “fixed” understanding of instructional methods, as is given in the Table. Of\ncourse, the roles in the Table can be combined in any individual, but it would be rare that, for example, a\nclassroom instructor would also be a learning theorist or knowledge engineer.\nInt J Artif Intell Educ (2016) 26:37 –71 55\ninnovations (tools) to continue the cycle. Benbya & McKelvey ( 2006, p. 14) refer to the\n“co-evolutionary ” aspects and “adaptive tension ” of the “complex adaptive ”\nsocio-technological systems, and discuss the problem of “accumulating requirements”.\nSo, an important community-of-practice question is: How effective are the feedback\nand development learning loops between users, trainers, and designers?\nThus far we have described what a tool/task/user complexity mapping scheme might\nlook like, without saying much about the nature of user cognitive complexity. A user ’s\nunderstanding of tools, tasks, and methods can be described in terms of the mental\nmodels one has of these things (Gentner and Stevens 1983;J o h n s o n - L a i r d1983).\nMental models are cognitive representations of external systems that include structures\nand processes that a person simulates (runs or visualizes) mentally. One task of the\nauthoring tool is to help the user maintain a valid mental model of the ITS building\nblocks, range of configurations, and design steps that the authoring tool affords.\nOja notes that “Cognitive engineering (Gersh et al. 2005) and learner-centered\ndesign (Soloway et al. 1994) focus on improving system-human cognitive fit and\nallowing users to construct better mental models (knowledge) of the system ” (p.\n3801), and that “reification is the basis for successful communication and the estab-\nlishment of a shared goal in human-computer collaboration ” (p. 3803). Thus it is\nimportant that the authoring tool interface accurately and powerfully reify the struc-\ntures, objects, constraints, decision rules, and procedures involved in authoring, so that\nauthors can build correct mental models, and can use these mental models to coordinate\nthe various steps and roles within a design process. The complexity of mental model\nthat is supported in the authoring tool should match the complexity capacity of the user.\nCollins and Ferguson’sw o r ko n “epistemic forms” provides a valuable link between\ntask/tool complexity and the user ’s complexity capacity in terms of the mental models\nthat the user must construct and maintain. In its concept of “epistemic games” it also\nanticipates the community-of-practice element of Activity Theory. We discuss episte-\nmic forms and games next.\nEpistemic Forms and Games\nCollins and Ferguson first articulated the concepts of epistemic games and epistemic\nforms (Collins and Ferguson 1993; and Morrison and Collins 1995;S h a f f e r2006).\nEpistemic forms are “target structures, like mental models, that guide inquiry ” and are\n“recurring forms that are found among theories in science and history. ” Epistemic\ngames are “general purpose strategies for analysing phenomena in order to fill out a\nparticular epistemic form ” that are shared within a community of practice (p. 25).\nExample epistemic forms include lists, hierarchy or tree structures, tables, networks,\nif-then rules, and constraint-based systems. They are “generative frameworks with slots\nand constraints on filling in those slots, ” and in this sense are like domain-independent\nscripts, templates, or grammars that specify the structural properties of a phenomena.\nThey serve as commonly understood mental models for understanding tasks and tools.\nThe theory of Epistemic Forms/Games considers not only the structure of informa-\ntion, but also the ways (i.e. games) communities use, understand, and build knowledge\nusing that structure. For example, perhaps the simplest epistemic form is the list.\nKnowing how to play an epistemic game includes knowing its constraints, strategies,\n56 Int J Artif Intell Educ (2016) 26:37 –71\nand moves. For the “list game” this includes knowing how to add, remove, combine,\nsplit, and arrange (classify, filter or sort) items, and knowing when the “list form ” is\nmost appropriate for a particular problem or inquiry. This framing is compatible with\nActivity Theory, which highlights the interplay between cognition, artifact design, and\ncommunities of practice.\nMorrison and Collins coined the term “epistemic fluency ” to refer to the ability\nto use and choose appropriately among the repertoire or ecology of epistemic games\navailable within a community of practice. Epistemic games are rarely used in\nisolation, and are combined with other games as well as transformed into other\ngames, as when one representation (a concept network) is seen as more appropriate\nthan another (a table). Tables can be seen as composed of lists; even more complex\nforms might combine tables with networks (e.g. a network of tables, or a table of\nnetworks). Table 2 lists some Epistemic Forms/Games mentioned by Collins &\nFerguson.\nEpistemic games can be framed in terms of the key questions driving an inquiry.\nKnowing an epistemic game includes knowing how to evaluate whether it is being\nplayed well. Example quality/validity criteria for the list game include coverage (is\nanything missing?), similarity (do the items belong together, or should it be split into\ntwo lists —apples and oranges?), distinctness (are the items actually different?), and\nperspicuity (is it sufficiently short, simple, efficient, and understandable?). Vibrant\ncommunities of practice will be creating, tweaking, and evolving, and mashing up\ntheir epistemic games.\nAuthoring Tool Epistemic Forms Epistemic forms/games allow for a compact meth-\nod of classifying tool/task complexity. In our original discussion of artifact complexity\nwe suggested that one could enumerate the number and types of parts, properties,\nrelationships, etc. in a system. This may be useful to do but also quite cumbersome.\nMeanwhile, epistemic forms serve well as a first-pass description of the complexity of\nend-user software systems. Epistemic forms also address one difficult issue in the\ncharacterization of an artifact, which we will call the “dimension compression prob-\nlem”: it may not be difficult to classify and compare artifacts along any single\ndimension (in Fig. 2), but we have little guidance thus far on how to combine and\nprioritize the many dimensions into a single (or simple) complexity characterization.\nEpistemic forms are holistic and representationally efficient in that they incorporate\nmany of these dimensions into each category.\nTable 2 Epistemic forms and\ngames (mental models) (from\nCollins and Ferguson 1993)\n• list • street map\n• matrix or table • org. chart\n• molecular model • musical score\n• periodic table • timeline\n• web page menu • cause/effect diagram\n• x-y graph • network\n• pert chart • relational database\n• binary tree • sentence diagram\n• floor plan • term paper outline\nInt J Artif Intell Educ (2016) 26:37 –71 57\nIn discussing authoring tools we are interested specifically in design activities or\ndesign games (a term not used by Collins and colleagues). In all epistemic games one\nof the evaluation criteria is whether one ’s product is understandable or meaningful to\nothers within one’s community, while design games are distinguished by the additional\nneed to assess how understandable and useable the product will be to users (who\nbelong to a community related to but different than the designer community). Thus, the\nset of design game quality/validity criteria is extended to a group that requires some\ncognitive empathy (and design/test iterations) to serve well.\nIn surveying a set of 14 authoring tools mentioned in Murray et al. ( 2003) one can\nclearly see a set of epistemic forms that are repeated numerous times throughout most\nof these systems. This list of forms will not be surprising —they are seen in most\nsoftware tools, as shown in Fig. 4. The basic elements include: check boxes and choice\nlists; sliders, dials, and meters; graphical networks and trees; and interactive hierarchi-\ncal and tabular textual representations. As discussed, to compare across and within any\nclass of epistemic forms (say a hierarchical menu system) we can use the elements\nsuggested in the earlier discussion of Complexity Science —i.e. the complexity of an\ninterface and task includes the number and diversity of such elements and the degree of\ntheir inter-relationship or coupling in an overall system.\nIntuitively one can roughly compare or rate the complexity of epistemic forms. Lists,\nsliders, and checkboxes are simpler than hierarchies, tables, and concept networks,\nwhich are in turn simpler than the complex systems/mental models that are composed\nof dynamic the interactions amon gm a n ys i m p l es u b - c o m p o n e n t s .Hierarchical\nComplexity Theory offers a more rigorous and more theory-based foundation for rating\nand comparing complexity components, and it was developed to apply to human tasks\nand skills. Next we explore HCT next as the last theoretical territory of exploration in\nour journey to link several interdisciplinary fields.\nFig. 4 Epistemic forms in authoring tools\n58 Int J Artif Intell Educ (2016) 26:37 –71\nHierarchical Complexity and Skill/Task Development\nAbove we drew from information/systems theories and socio-technology theories\n(Activity Theory and usability theory) to suggest ways to characterize the complexity\nof systems in general terms. Epistemic Forms suggest a way of ameliorating the\n“dimensionality issue ” by enumerating common forms that are more intuitive and\nready-to-hand than a list of low level complexity dimensions. But we are still far from\na quantitative or semi-quantitative method for combining the factors involved to be able\nto make comparative complexity judgments. To move in this direction I will draw from\nan area of cognitive/learning science that has significant implications for learning\ntheory and ATLS design in general, yet, curiously, is rarely referenced in these fields:\nNeo-Piagetian developmental theories. Cognitive developmentalists (Neo-Piagetian\ntheorists) have undertaken a deep study of complexity, because human development\nand learning can be described in terms of “qualitative differences in mental complexity”\nrelative to various tasks, skills, or life contexts (Kegan 1994,p .1 5 2 ) .\nBecause these theories have untapped value for the ATLS/ITS community I\ninclude an extended description in the Appendix but here I will only mention the\nfeatures of these theories that are relevant to our authoring tool complexity analysis.\nThe key insight is that development, and complexity in general, advance through\nboth horizontal and vertical ( “hierarchical”) movement, and do so through a partic-\nular alternating or spiraling pattern.\n11 The structure and nature of horizontal growth\nis different than the structure and nature of vertical growth. Vertical growth is more\nquantized or punctuated, and the vertical leaps involve particular challenges. If we\nframe authoring tool features, authoring tasks, and epistemic games in terms of\nvertical and horizontal differences in complexity we have additional tools for\ncomparing complexity, and we gain insight into why certain forms may be partic-\nularly difficult for users to learn.\nNeo-Piagetian (adult) developmental theories go beyond early developmental work\n(E.g. Piaget, Perry, Kohlberg) to add a hierarchical “structural perspective in analyzing\nchanges in the organization of …actions and thought ” (Fischer and Yan 2002,p .2 8 3 ) .\nThese theories propose underlying representations for skills and suggest rules for the\ntransformation of skills to higher level skills.\n12 These theories apply principles from\nComplexity Science to human cognition and behavior, which can be easily mapped onto\nartifacts (tools). As stated by Commons & Pekker: “Theories of difficulty have generally\nnot addressed the hierarchical complexity of tasks. Within developmental psychology,\nnotions of hierarchical complexity have come into being in the last 20 years. [...] a model\nof hierarchical complexity, which assigns an order of hierarchical complexity to every\ntask regardless of domain, may help account for difficulty ” (2009;p .2 ) .\nHorizontal increases in complexity involve adding more of what already exists to an\nobject, process, or structure (more parts, relationships, steps, etc. —adding more “bits”\n11 These models have been empirically demonstrated in many hundreds of studies. For example Commons\nand Pekker ( 2009)s t a t e s :“Using [Rasch analysis we have] found that hierarchical complexity of a given task\npredicts task performance with the correlation being r = 0.92. [which] has been shown to account for\nperformance in a variety of different domains ” (p. 4).\n12 Fischer’s Skill Theory (Fischer 1980; Fischer and Yan 2002) and other Neo-Piagetian models, including\nCommons’ Hierarchical Complexity Model (Commons and Richards 1984, Commons and Pekker 2008),\nKegan’ss t a g em o d e l(1994, 1982); and Cook-Greuter’s ego development model ( 2000, 2005).\nInt J Artif Intell Educ (2016) 26:37 –71 59\nof information without adding new structural emergence). Commons suggests that\nincreases in the horizontal complexity of tasks (which he calls the “classical” model\nof information complexity) are analogous to increases in cognitive load (Commons and\nPekker 2009 unpublished). Horizontal growth can also be roughly compared to Piaget ’s\nassimilation, as it adds new knowledge in the form of existing structures (Piaget 1972).\nVertical growth related to accommodation, in which new structures are created to\nunderstand the world in new ways. Horizontal growth tends to be continuous, while\nvertical growth follows a more discrete model, and occurs after a sufficient amount of\nhorizontal growth allows for a reorganization at the next higher level.\nVertical increases in complexity lead to a new level or stage by applying an\noperation upon, or “coordinating and transforming, ” the objects of the lower layer.\nEach artifact or skill at a given hierarchical level consolidates a set of items at the lower\nlevel into a single whole, transcending and yet including them. Completely new\nproperties and concerns arise at each level (a phenomena called emergence).\nExamples of increasing levels of hierarchical complexity include the development (or\nevolution) from: words to sentences; addition to multiplication; single celled to\nmulti-celled organisms; concrete to formal operational concepts; from using to design-\ning an artifact; and from doing a task to managing others doing it.\nThere are numerous operations that can produce the next hierarchical level. Examples\ninclude: abstraction and generalization operate on lower level objects to create higher\nlevel ones; compilation or aggregation can create higher level units; steps are combined\ntogether to create processes; going “meta” is in “thinking about thinking,” moving from\nstatic to dynamic systems or linear dependency to mutual dependency also involve\nhierarchical transformations. Kegan notes that increasing complexity and sophistication\nmoves (vertically) from entities to processes, from static to dynamic systems, and from\ndichotomous to dialectical relationships (Kegan 1994,p .1 3 ) .\nHorizontal growth also follows a pattern in natural systems including human\nlearning. The sequence is from single objects, to multiple independent objects, to\nmultiple interacting objects, to massively interconnected object, and finally to an\nemergent whole that transitions to the n ext hierarchical level (see more in the\nAppendix). It makes intuitive sense that it is easy to learn a few more words\n(horizontal) but the leap to speaking sentences is comparatively momentous (which\nis not to say that it comes on line all of a sudden, i.e. children produce quasi-sentences\nfirst). And this difference is quantitative. If we wanted to measure language complexity\nwe can count the size of vocabulary and the length of words, but no amount of increase\nin vocabulary will “equal” the shift from words to sentences.\nHierarchical Complexity (which is Commons ’ term, while other developmentalists\nuse different terms) contributes to our analysis of authoring tool complexity in several\nways. First, it ameliorates the “dimensionality issue” by providing another tool for\norganizing the plethora of complexity dimensions, i.e. according to horizontal and\nvertical differences in complexity, toward our goal of coordinating the complexities of\ntool vs. task vs. user; and in our goal to compare two (or more) tools (or tasks, or types\nof users). Second, because it is primarily a learning or developmental theory, it provides\nimportant insights into the effort and prerequisite knowledge a new user needs to use an\nauthoring tool. Vertical growth is typically more difficult than horizontal growth, and\nthe emergence of a new level of organization can come with some disequilibrium or\ndissonance, which in turn means there can be resistance or hesitancy.\n60 Int J Artif Intell Educ (2016) 26:37 –71\nWe can begin with a rough characterization of the level of software tool complexity that\na hypothetical user already has, and then ask whether the features and tasks of an authoring\ntool represent horizontal or vertical types of learning for the skill acquisition learning\ncurve. We must not assume that new user skilllevel can be increased in any sort amount of\ntime with something like a training interve ntion if vertical learning is involved.\nHierarchical Complexity and Epistemic Forms The analysis of tool/task/user com-\nplexity can proceed in two directions: more rigorous quantitative analysis, and more\nheuristic qualitative analysis. For our purposes we will focus on heuristic estimations.\nOur goal is to either start with a particular authoring tool/task and identify the commu-\nnities of practice and training needs that will match the tool/task; or,s t a r t i n gw i t hat a r g e t\nuser group, to design the tool/task to match the estimated complexity of a community of\npractice. One can use the concepts introduced in this paper, including the dimensions of\ncomplexity, types of Epistemic Forms, and the distinction between horizontal and\nvertical differences in complexity, to make subjective shoot-from-the-hip assessments\nand inform design discussions as is usually done in software design. Alternatively, and\nleft for others to carry forward, one can use these concepts to construct detailed\nquantitative metrics and formulas for calculating task/tool/knowledge complexity —\nbut such in not necessary to make solid progress in matching tools/tasks to users.\nMorrison and Collins mention the “epistemic complexity” of epistemic forms and\ngames, but they do not define it precisely. What we contribute here is an attempt to link\nepistemic games to cognitive developmental theory in an attempt to create a grounded\nframework for assessing the relative complexity of epistemic forms/games, which will\nthen provide a framework for describing the complexity of authoring tool features. These\nepistemic forms can be sequenced according to complexity level modeled on the levels\nmentioned in Hierarchical Complexity Theory (see Appendix), as shown in Table 3.\nFigures 5 and 6 contain a series of figures illustrating these five complexity levels.\nFigure 5 summarizes the five levels of epistemic forms detailed in Fig. 6. In Fig. 5 we\nTable 3 Epistemic forms organized by complexity level\nEpistemic Form for Tool/Task/Mental Model Complexity Level\n• Text information fill-in boxes\n• Lists, choices, sliders, and check boxes\nSimple Objects\n• Forms, schemas, or templates\n• Tables and matrices\n• Hierarchies and trees\nAbstractions & Mappings\n• Scripts (with branches)\n• Equations and Boolean logic\n• Structural models: concept networks, boxology diagrams\nFormal Systems\n• Causal and constraint models (and using variables)\n• Behavioral/procedural models: If/then and rule-based\nprocedural representations\n• (Authoring of) Decision trees, Bayesian Nets, etc.\nDynamic Systems\n• Coordination of dynamic modules, e.g. complex interactions\nbetween expert, student, teaching modules, and dynamic\nuse scenarios.\n• Design that takes into account emergent and chaotic interactions.\nArchitectures and Ecosystems\n(systems of dynamic systems)\nInt J Artif Intell Educ (2016) 26:37 –71 61\nlink these complexity levels to the low/medium/high level of complexity associated\nwith different categories of users from Table 2. Again, this mapping is a heuristic\nestimation that is intended to illustrate the type of analysis; no strong claims are made\nfor the specific mappings.\nDiscussion\nBeginning with a summary of my article on ITS authoring tool design, I described some of\nthe challenges facing authoring tool designers and researchers today. Consonant with this\nSpecial Issue’s theme of personal retrospectives on classic papers, I also included a\nnarrative look at what brought me to authoring tools work, and mentioned that my\nacademic journey since then has included interdisciplinary tributaries outside of ITS and\neducational technology per-se. The invitation to write this article has given me the happy\nopportunity to apply new frames of reference to an old topic. The reader hoping for\ndefinitive answers to questions about software complexity may have been disappointed—\nwhat I have done is exploratory theorizing to help frame important questions by suggest-\ning certain theories, principles, and concepts amenable to ITS authoring tool R&D.\nIn this article I have explored some theoretical bases for assessing the appropriate-\nness of ITS authoring tools, and any type of software artifact, to intended user\ncommunities. The analysis is based on general notions of complexity from\nComplexity Science and Hierarchical Complexity Theory. The importance of consid-\nering tools, tasks, user capacity, and community of practice in an integrated way was\nsupported through the inclusion of the models of Activity Theory and Epistemic Forms.\nMatching tool/task complexity to user/community complexity capacity is important\nbecause authoring tools are complex and expensive to build, and, using a “risk analysis”\nFig. 5 Complexity levels of epistemic forms (overview)\n62 Int J Artif Intell Educ (2016) 26:37 –71\nframework, we can say that the more expensive a system is to build, the larger the risk if\nuser needs and capacities are not understood and anticipated. The design goal is to find\nthe sweet spot where risk is acceptably low and expected value is relatively high. Oja ’s\nFig. 6 Complexity levels of epistemic forms (details)\nInt J Artif Intell Educ (2016) 26:37 –71 63\n(2010) study of improving usability in complex software systems concludes that systems\nshould anticipate that projects usually involve a variety of roles and areas of expertise,\nand that interfaces should allow for the “distribution of tasks according to participant\nstrengths” (p. 3800). Thus, the goal is not so much to match the affordances of an\nauthoring tool to an intended user type, but to anticipate the range of user types involved\nin an ITS design and build tools that clearly meet the needs of each design role. Also, any\nplans for large scale adoption of authoring tools should include plans for learning and\npeer-mentoring within specific communication pathways in communities of learning.\nThe inclusion of Complexity Science and theories of dynamic systems in our narrative\nsupports a bigger picture consideration of authoring that considers, not only how tools\nshould be build to match user capacities, but the reciprocal evolution of tools and human\ncapacities over longer periods of time. As Jerome Bruner notes“through using tools, man\nchanges himself and his culture...human evolution is altered by man-made tools” (Bruner\n1987). Thus tools can not only support the construction of advanced learning systems,\nbut might also be designed to help users (especially instructors) more deeply understand\nand incorporate leading edge learning theories and mental models of the learning process\n(or build more adequate mental models of their content domain). We can move beyond\nseeing authoring tools primarily in terms of time and effort savings and consider their role\nin empowering content and pedagogy experts, including teachers; and in terms of\npropelling the evolution of computer-mediated learning in general.\nAppendix: Neo-Piagetian Developmental Models: Skill Theory\nand Hierarchical Complexity Theory\nIn terms of this article as a personal retrospective, one question is “what have I learned\nsince my seminal work on authoring tools that is applicable to the field? ” Some of what\nI have learned is from my interdisciplinary dabbling in the areas of usability, Activity\nTheory, and Complexity Science, as discussed in this paper. But the most significant\nfield that I have been exposed to in the last decade is adult developmental theories in\nthe Neo_Piagetian tradition (from Fischer, Commons, Kegan, Cook-Greuter, and\nothers, as discussed below). This literature could have significant impact in learning\ntheory and educational technology R&D, and yet is hardly cited or known in these\nacademic communities. These theories come from rigorous empirical studies and\ndeeply inform questions about knowledge, learning and development, transfer, meta-\ncognition, reflective reasoning, socialization, and even motivation and ethics.\n13 Thus I\ntake pleasure in the opportunity to support a bit more familiarity to Neo_Piagetian\ntheories within the ILS/CSCL/ITS communities. In this paper I focus on how these\ntheories can inform the analysis of tool, task, and cognitive complexity, but in this\nAppendix I will say a bit more about these developmental theories, extending the\ndescription found in the section “Human Development and Hierarchical Complexity. ”\n13 Commons notes that “The Model of Hierarchical Complexity and Skill Theory...have ordered\nproblem-solving tasks of various kinds, including ”: Social perspective-taking, Workplace organization,\nPolitical development, Political development, Writing, Epistemology, Algebra, Music, Animal intelligence,\nCounseling, and many more. See Commons and Richards 1984.\n64 Int J Artif Intell Educ (2016) 26:37 –71\nNeo-Piagetian developmental theories deepen and generalize early work by Piaget,\nKohlberg, Perry and others. They are more nuanced and flexible than the original stage\ntheories and, developmental scholars would say, adequately address problems earlier\ntheories had with rigid stages, domain-specific learning, not accounting for individual\ndifferences, and the potential for cultural and other biases. They extend earlier work on\nchildhood development into adult development in to post-formal-operational levels. They\nare also more compatible with contemporary dynamic systems, information science, and\nsocial-constructivist theories vs. earlier developmental theories. They also address the\nintegration of bio/neuro/psycho/social/eco factors in human development more fully than\nprior theories (Knight and Sutton 2004; Demetrio et al. 2005; Dawson and Stein 2011).\nAs mentioned in the body of the paper, Neo-Piagetian developmental theories (hence-\nforth“developmental theories”) describe two general types of growth or learning, and thus\ntwo directions for increasing complexity, w hich can be called horizontal and vertical.\nHorizontal learning involves learning more of the same type of thing, for example,\nlearning more plant species or violin concertos. Horizontal learning includes increasing\ndifferentiation among exemplars, for example, as one learns about more types of wine one\nbecomes facile in the many ways that they differ, and gains nuance in noticing differences.\nAt some point, after many exemplars and much practice, learning takes a vertical leap\nto a new level of abstraction, integration, or consolidation. Just as biological cells and\ninformation networks tend to self-organize into larger wholes, ideas or knowledge itself\nappears to do something similar. This creates a qualitatively different form of under-\nstanding or skill with each vertical reintegration. In learning the component notes of a\nsong, or the component actions of riding a bicycle, at some point the song or the action\nof riding emerges cognitively and is understood as a whole. The move from speaking\nindividual words to sentences is a similar jump, as is the jump from learning addition\nand subtraction (horizontally related skills) to learning multiplication and division.\nCook-Greuter (2005) explains it this way: “most growth in adults is of the horizontal,\nexpansion kind. People learn new skills, new methods, new facts, even new ways of\norganizing knowledge, but their current stage or mental model of the world remains the\nsame…Developmental Theory, on the other hand, describes a sequence of how mental\nmodels themselves evolve over time. Each new level contains the previous ones as subsets.\nEach new level is both a new whole logic with its own coherence, and– a tt h es a m et i m e–\nalso a part of a larger, more complex meaning system. ” Robert Kegan describes develop-\nment in terms of an evolving sequence of “distinctly different ways of making meaning”\n(1994, p. 90). Research has shown that such reorganizations or consolidations of horizontal\nknowledge are indeed non-linear events indicated by a sharp“spurts” (and then leveling off\nto another gradual increase) in skill level (Fischer and Yan2002;D a w s o n - T u n i ke ta l .2005).\nNeo-Piagetian (adult) developmental theories posit generalized developmental levels\nor tiers that grow over the lifespan but are also useful for the analysis of any specific\nhuman task, knowledge, or skill. The sequence of “complexity orders,” which extends\nand refines the original work by Piaget, goes roughly: sensory-motor schema, conceptual\ncategories and preoperational thinking, concrete operational thinking (representational);\nformal operational thinking (a. abstract objects, b. formal rules, c. systematic structures);\nand meta-principles and post-formal thinking (metasystematic principles and paradig-\nmatic frames) (I have combined terms used in Skill Theory and Hierarchical Complexity\nTheory here). Development (in adults) happens in response to specific life demands, and\nskills in different domain or task areas evolve at different rates.\nInt J Artif Intell Educ (2016) 26:37 –71 65\nIn vertical growth a new level of understanding is created through applying an operation\nupon, or “coordinating and transforming,” the objects of the hierarchically lower layer.\nEach skill or knowledge at a given level consolidates a set of items at the prior level into a\nsingle whole, transcending and yet including them. In addition to the vertical growth\nmechanism of hierarchical inclusion, horizontal growth also follows a pattern. It starts with\nbeing able to recognize (or enact) a new type of object —you “see” or have a concept of\nsomething you have never known before. In seeing more of them one notices how they\nrelate and begins to notice simple relationships between pairs, then to coordinate multiple\nobjects (or actions). Interrelationships progress from individual and linear to interdependent\nand non-linear. At the final stage within a level objects are experienced in a dense network\nof elaborated interrelationships which takes on the wholeness of a new object at the next\nhigher level. There is also a temporal element to the learning sequence within a level. We\nprogress from engaging with a construct through afterthought and retrospective reflection,\nto increasingly being able to operate with it in real time. We also increasingly improve our\nability to respond rapidly to dynamic and evolving contexts (until eventually our responses\nbecome automatic and unconscious). (These ideas are not unique to developmental\ntheorists, and are reflected in other learning theories, such as Anderson’sA C Tm o d e l1983).\nFigure 7 shows the sequence of developmental levels in Fisher ’s skill theory (the\nmost widely cited framework; and one that is compatible with Commons ’,K e g a n’s,\nand Dawson’s frameworks). It shows a number of “tiers” representing the hierarchical\nlevel of the task or mental construct. Each builds upon the prior as mentioned above.\nWithin each tier, or class of objects-to-be-operated upon, is the same sequence of\noperations: (1) single objects, (2) mappings (linear relationships) of objects, (3) systems\n(and non-leaner relationships) of objects; (4) systems of systems which organize (or\nchunk) into a single object at the next higher level.\n14\nAs an example, a child might first learn what a “lunch” is (a concrete tier object). They\ncan recognize lunch and name it. As understanding progresses they coordinate lunch with\nother concrete constructs, perhaps soups and sandwiches, lunch times and locations such\nas school cafeterias, and are also learning what breakfast and dinner is (representational\nmappings). At the level of representational systems they understand and can talk about\nthe system of breakfast-lunch-dinner, perhaps why certain foods or certain rules apply for\neach one, and can imagine related possibilities in the concrete realm ( “what if we always\nate at midnight—what would that be like?”). As a system is understood it is chunked into\na schema at the next higher level, which is this case includes abstractionssuch as meal,\netiquette, nutrition, etc. (e.g. concrete systems of systems become single abstractions).\nThis framework is generic to all human tasks and skills (knowledge), and has been\napplied to algebra, reading, piano playing, tennis, parenting, leadership development, etc.\nFor Fischer and other developmentalists, mental growth is closely tied in to\ngoal-directed activity in life. Skills are acquired only in response to “tasks” that one\nis challenged to succeed at as one interacts with artifacts and other individuals in\nauthentic contexts. In this sense contemporary developmental theory is compatible with\nActivity Theory and socio-constructivist theories of learning (see section on Activity\nTheory).\n14 Within each tier are 4 numbered levels, but the highest level “systems” in any tier is equivalent to the most\nbasic unit ( “single”) at the next tier —thus indicators such as “R4/A1” in the Figure.\n66 Int J Artif Intell Educ (2016) 26:37 –71\nTable 4 illustrates a series of children ’s verbalizations from different ages illustrating\nhow the concept of “fun” develops through single (concrete) representations, represen-\ntational mappings, representational systems, single abstractions, and abstract mappings.\nThese illustrations are for verbal/cognitive domains. Commons and Pekker ( 2008)\nshows how these ideas can be applied generally to complex systems and artifacts.\nMy goal in this paper is to apply these ideas to the design of interfaces and software-use\ntasks. What is important then is that the development of skills (and tasks) has a\nsemantic/conceptual aspect, i.e. the types of objects in each successively more complex\ntier; and a syntactic/structural aspect, i.e. the four phases within each tier that describe\nhow the elements are combined from simple to increasingly complex ways.\nDevelopmental theory makes use of and parallels many aspects of Complexity\nScience. Developmental progressions in the horizontal direction include increase in\nthese forms of complexity that have been mentioned: single to multiple items; inde-\npendent to interacting items; static to dynamic contexts; linear to non-linear relation-\nships; predicable/definitive to more unpredictable and fuzzy. Structural, dynamic, and\nperspectival modes of complexity increase, often in parallel.\nFischer’s Skill Theory ( 1980) establishes a solid theoretical link between cognitive\nscience and models from socio-technical fields such as Activity Theory.\n15 Skill Theory\nframes the development (i.e. learning) of all human capacities, including intellectual,\nemotional, physical, musical, etc., in terms of “skills.” Skills develop in response to,\nand only in response to, actual life “tasks” (i.e. situations calling for a response). In this\nframework, it does not make sense to describe a skill without describing the task, or\ngeneral type of task, it is meant to address. In this model the analysis of cognitive\ncapacities (learning, knowledge, etc.) is always coordinated with the analysis of tasks.\n16\n15 Fischer is one of the leading developmental theorists following the long lineage of neo-Piagetian theorists.\nHis status within certain academic communities is on par with John Anderson ’s status in the Learning\nSciences. Related and compatible frameworks include those from Commons (Commons and Richards\n1984) and Dawson (Dawson and Stein 2011).\n16 Fischer is not a behaviorist. He does not deny the usefulness or realty of cognitive capacities that can not be\ndirectly measured. He merely grounds them in actual tasks. The tasks need not be physical and can be mental\ntasks, as they must be in moving from concrete to formal operational thinking (e.g. planning or mathematics).\nFig. 7 Fisher’ss k i l lt h e o r y\nInt J Artif Intell Educ (2016) 26:37 –71 67\nThis is compatible with Activity Theory and situated and Vygotsky-inspired learning\ntheories that critique the separating cognition from activity in empirical or theoretical\nanalysis.\nReferences\nAbelson, H., & Sussman, G. J. (1983). Structure and interpretation of computer programs. Cambridge: MIT\nPress.\nAinsworth, S., Major, N., Grimshaw, S., Hayes, M., Underwood, J., Williams, B. & Wood, D. (2003).\nREDEEM: simple intelligent tutoring systems from usable tools. Chapter 8 In MURRAY , T.,\nBLESSING, S. & AINSWORTH, S. (Eds.). Authoring tools for advanced technology learning environ-\nments. Springer: Netherlands.\nAleven, V ., & Sewall, J. (2010). Hands-on introduction to creating intelligent tutoring systems without\nprogramming using the cognitive tutor authoring tools (CTAT). In Proceedings of the 9th International\nConference of the Learning Sciences-Volume 2 (pp. 511– 512). International Society of the Learning\nSciences.\nAleven, V ., McLaren, B. M., Sewall, J., & Koedinger, K. R. (2006). The cognitive tutor authoring tools\n(CTAT): Preliminary evaluation of efficiency gains. In Intelligent Tutoring Systems (pp. 61–70). Springer\nBerlin Heidelberg.\nAleven, V ., McLaren, B., Sewall, J., V AN Velsen, M., Popescu, O., Demi, S., Koedinger, K. (2015). An\nEffective Paradigm for Intelligent Tutoring Systems: Example-Tracing Tutors.\nAnderson, J. (1983). The architecture of cognition.C a m b r i d g e :H a r v a r dU n i v .P r e s s .\nBenbya, H., & McKelvey, B. (2006). Toward a complexity theory of information systems development.\nInformation Technology & People, 19(1), 12–34.\nBereiter, C., & Scardamalia, M. (2006). Education for the knowledge age: design-centered models of teaching\nand instruction. In P. A. Alexander, & P. H. Winne (Eds.), Handbook of educational psychology (2nd ed.,\npp. 695–713). Mahwah: Lawrence Erlbaum Associates.\nBrown, A. L., & Campione, J. C. (1996). Psychological theory and design of innovative learning environ-\nments: On procedures, principles, and systems. In L. Schauble, & R. Glaser (Eds.), Innovations in\nlearning: New environments for education (pp. 289–325). Mahwah: Lawrence Erlbaum Associates.\nBruner, J. (1987/2004), ‘Life as narrative ’, Social Research, 71: 691 –710.\nByström, K., & Järvelin, K. (1995). Task complexity affects information seeking and use. Information\nProcessing and Management, 31(2), 191–213.\nCampbell, D. J. (1988). Task complexity: a review and analysis. Academy of Management Review, 13(1), 40–\n52.\nChilana, P. K., Wobbrock, J. O., & Ko, A. J. (2010). Understanding usability practices in complex domains. In\nProceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 2337 –2346).\nACM.\nCobb, P., Confrey, J., diSessa, A. Lehrer R. Schauble, L. (2003). Design experiments in educational research.\nEducational Researcher 2003; 32, 1.\nCollins, A., & Ferguson, W. (1993). Epistemic forms and epistemic games: structures and strategies to guide\ninquiry. Educational Psychologist, 28(1), 25–42.\nTable 4 Development examples: the concept of fun\nSingle Ref. (unconnected list) Fun is swinging on a swing. It ’s sliding on a slide.\nRef. Mapping (connections) Fun is when Tommy and I put blocks together and then knock them\ndown so that they make a loud noise that make us laugh.\nRef. System (interconnections) Fun is different things. Sometimes I like to climb...that makes me...\nSingle Abstr. (unconnected list) Fun is a way of enjoying yourself. It is a form of pleasure.\nAbst. Mapping (connections) There are a variety of ways that a person can have fun. Some people\nenjoy physical activities, like sports or just exercise. Some people...\n68 Int J Artif Intell Educ (2016) 26:37 –71\nCommons, M. L., & Pekker, A. (2008). Presenting the formal theory of hierarchical complexity. World\nFutures: Journal of General Evolution , 64(5–7), 375–382.\nCommons, M. L., & Pekker, A. (2009, unpublished). Hierarchical complexity and task difficulty. http://\ndareassociation.org/papers.php. Accessed Monday, November 30, 2009.\nCommons, M. L., & Richards, F. A. (1984). A general model of stage theory. In M. L. Commons, F. A.\nRichards, & C. Armon (Eds.), Beyond formal operations: Late adolescent and adult cognitive develop-\nment (pp. 120–141). New York: Praeger.\nCommons, M. L., Trudeau, E. J., Stein, S. A., Richards, F. A., & Krause, S. R. (1998). Hierarchical\ncomplexity of tasks shows the existence of developmental stages. Developmental Review, 18, 238–278.\nConklin, J. (2005). Wicked Problems & Social Complexity. Chapter 1 of Dialogue Mapping: Building Shared\nUnderstanding of Wicked Problems, Wiley.\nConstantin, A., Pain, H., & Waller, A. (2013). Informing the design of an authoring tool for developing social\nstories. In Human-Computer Interaction–INTERACT 2013 (pp. 546–553). Springer Berlin Heidelberg.\nCook-Greuter, S. R. (2000). Mature ego development: a gateway to ego transcendence. Journal of Adult\nDevelopment, 7(4), 227–240.\nCook-Greuter, S.R. (2005). Ego development: nine levels of increasing embrace. Available at www.cook-\ngreuter.com.\nCristea, A. (2005). Authoring of adaptive hypermedia. Journal of Educational Technology & Society , 8(3).\nDawson, T. L., & Stein, Z. (2011). We are all learning here: Cycles of research and application in adult\ndevelopment. Hoare, C. (Ed.). (2011). The Oxford handbook of reciprocal adult development and\nlearning. Oxford: Oxford University Press.\nDawson-Tunik, T. L., Commons, M., Wilson, M., & Fischer, K. (2005). The shape of development. The\nEuropean Journal of Developmental Psychology, 2 ,1 6 3–196.\nDemetriou, A., Efklides, A., & Shayer, M. (Eds.). (2005). Neo-Piagetian theories of cognitive development :\nImplications and applications for education . Routledge.\nEngestrom, Y ., Miettinen, R., & Punamaki, R.-L. (Eds.) (1999). Perspectives on activity theory . New York:\nCambridge University Press.\nFischer, K. (1980). A theory of cognitive development: the control and construction of hierarchies of skills.\nPsychological Review, 87(6), 477–531.\nFischer, K., & Yan, Z. (2002). The development of dynamic skill theory. In Conceptions of development :\nLessons from the laboratory, 279-312.\nGentner, D., & Stevens, A. (Eds.) (1983). Mental models. Hillsdale: Lawrence Erlbaum Assoc.\nGersh, J. R., McKneely, J. A., & Remington, R. W. (2005). Cognitive engineering: understanding human\ninteraction with complex systems. Johns Hopkins APL Technical Digest , 26(4), 377–382.\nGraesser, A. C., Chipman, P., Haynes, B. C., & Olney, A. (2005). AutoTutor: an intelligent tutoring system\nwith mixed-initiative dialogue. I EEE Transactions on Education, 48,6 1 2–618.\nGrünwald, P. D., & Vitányi, P. M. (2003). Kolmogorov complexity and information theory. With an\ninterpretation in terms of questions and answers. Journal of Logic, Language and Information , 12(4),\n497–529.\nHaynes, S. R., & Kannampallil, T. G. (2004). Learning, Performance, and Analysis Support for Complex\nSoftware Applications. Proc. of the 3rd Ann. Workshop on HCI Research in MIS ,3 0 –34.\nHeffernan, N., & Heffernan, C. (2014). The ASSISTments ecosystem: building a platform that brings\nscientists and teachers together for minimally invasive research on human learning and teaching.\nInternational Journal of Artificial Intelligence in Education , 24(4), 470–497.\nJohnson, C. W. (2006). Why did that happen? Exploring the proliferation of barely usable software in\nhealthcare systems. Quality & Safety in Health Care , 15,i 7 6–i81.\nJohnson, W. L., & Valente, A. (2008). Tactical Language and Culture Training Systems: Using Artificial\nIntelligence to Teach Foreign Languages and Cultures. In AAAI (pp. 1632 –1639).\nJohnson-Laird, P. N. (1983). Mental models: Towards a cognitive science of language, inference, and\nconsciousness. Cambridge: Harvard University Press.\nJonassen, D., & Rohrer-Murphy, L. (1999). Activity theory as a framework for designing constructivist\nlearning environments. Educational Technology Research and Development, 47(1), 61–79.\nJordan, T., Andersson P., & Ringnér, H. (2013). The Spectrum of Responses to Complex Societal Issues:\nReflections on Seven Years of Empirical Inquiry. Integral Review, February 2013, V ol. 9, No. 1.\nKegan, R. (1982). The evolving self. Cambridge: Harvard University Press.\nKegan, R. (1994). In over our heads: The mental demands of modern life . Cambridge: Harvard University\nPress.\nKnight, C. C., & Sutton, R. E. (2004). Neo-Piagetian theory and research: enhancing pedagogical practice for\neducators of adults. London Review of Education , 2(1), 47–60.\nInt J Artif Intell Educ (2016) 26:37 –71 69\nKoedinger, K. R., Anderson, J. R., Hadley, W. H., & Mark, M. A. (1997). Intelligent tutoring goes to school in\nthe big city. International Journal of Artificial Intelligence in Education , 8,3 0 –43.\nKumar, P., Samaddar, S. G., Samaddar, A. B., & Misra, A. K. (2010, June). Extending IEEE LTSA e-Learning\nframework in secured SOA environment. In Education Technology and Computer (ICETC), 2010 2nd\nInternational Conference (V ol. 2, pp. V2-136). IEEE.\nMirel, B. (2004). Interaction design for complex problem solving . San Francisco: Morgan Kaufman.\nMitrovic, A. (2012). Fifteen years of constraint-based tutors: what we have achieved and where we are going.\nUser Modeling and User-Adapted Interaction , 22(1–2), 39–72.\nMitrovic, A., Martin, B., Suraweera, P., Zakharov, K., Milik, N., Holland, J., & McGuigan, N. (2009).\nASPIRE: an authoring system and deployment environment for constraint-based tutors. Artificial\nIntelligence in Education, 19(2), 155–188.\nMorrison, D., & Collins, A. (1995). Epistemic fluency and constructivist learning environments. Educational\nTechnology, 35(5), 39–45.\nMurray, T. (1996). Having It All, Maybe: Design Tradeoffs in ITS Authoring Tools. In Intelligent Tutoring\nSystems: Third International Conference, ITS’96, Montreal, Canada, June 12–14, 1996. Proceedings (V ol.\n1086, p. 93). Springer.\nMurray, T. (1999). Authoring intelligent tutoring systems: analysis of the state of the art. International Journal\nof Artificial Intelligence in Education , 10(1), 98–129.\nMurray, T. (2003a). An overview of intelligent tutoring system authoring tools: updated analysis of the state of\nthe art. In T. Murray, S. Blessing, S. Ainsworth (Eds.), Authoring tools for advanced technology learning\nenvironments. Netherlands: Springer.\nMurray, T. (2003b). Eon: Authoring tools for content, instructional strategy, student model, and interface\ndesign. In T. Murray, S. Blessing, S. Ainsworth (Eds.), Authoring tools for advanced technology learning\nenvironments. Netherlands: Springer.\nMurray, T. (2004). Design tradeoffs in usability and power for advanced educational software authoring tools.\nEducational Technology Journal, 2004,1 0–16.\nMurray, T., & Woolf, B. (1992). Tools for teacher participation in ITS design. In Frasson, Gauthier, & McCalla\n(Eds.), Intelligent Tutoring Systems, Second Int. Conf. (pp. 593–600). New York: Springer Verlag.\nMurray, T., Blessing, S., & Ainsworth, S. (Eds.) (2003). Authoring tools for advanced technology learning\nenvironments: Toward cost-effective adaptive, interactive, and intelligent educational software .\nNetherlands: Springer.\nNielsen, J. (1993). Usability engineering. AP Professional: Boston.\nNielsen, J. (1994). Enhancing the explanatory power of usability heuristics. In Proceedings of the SIGCHI\nConference on Human Factors in Computing Systems (pp. 152–158 ). ACM.\nNorman, D. (1988). The design of everyday things .N Y :D o u b l e d a y .\nOja, M. K. (2010). Designing for collaboration: improving usability of complex software systems. In CHI’10\nextended abstracts on human factors in computing systems (pp. 3799–3804). Chicago: ACM.\nOlsen, J. K., Belenky, D. M., Aleven, V., & Rummel, N. (2013). Intelligent Tutoring Systems for\nCollaborative Learning: Enhancements to Authoring Tools. In Artificial Intelligence in Education (pp.\n900–903). Springer Berlin Heidelberg.\nPiaget, J. (1972). The principles of genetic epistemology . NY: Basic Books.\nRazzaq, L., Patvarczki, J., Almeida, S. F., Vartak, M., Feng, M., Heffernan, N. T., & Koedinger, K. R. (2009).\nThe assistment builder: supporting the life cycle of tutoring system content creation . IEEE Transactions\non Learning Technologies, 2(2), 157–166.\nRitter, S. (2015). Authoring for the product lifecycle. Available from the author at Carnegie Learning.\nRitter, S., & Blessing, S. (1998). Authoring tools for component-based learning environments. The Journal of\nthe Learning Sciences, 7(1), 107–132.\nShaffer, D. W. (2006). Epistemic frames for epistemic games. Computers & Education, 46(3), 223–234.\nSitaram, S., & Mostow, J. (2012). Mining data from project LISTEN ’s reading tutor to analyze development of\nchildren’s oral reading prosody. In Proceedings of the 25th Florida artificial intelligence research society\nconference (FLAIRS-25), 478–483. Marco Island, Florida.\nSoloway, E., Guzdial, M., & Hay, K. E. (1994). Learner- centered design: the challenge for HCI in the 21st\ncentury. Interactions, 1(2), 36–48.\nSottilare, R. A., Brawner, K. W., Goldberg, B. S., & Holden, H. K. (2012). The generalized intelligent\nframework for tutoring (GIFT). Orlando, FL: US Army Research Laboratory –Human Research &\nEngineering Directorate (ARL-HRED).\nSottilare, R., Graesser, A., Hu, X. & Goldberg, B. (2014). Design recommendations for intelligent tutoring\nsystems: volume 2 : instructional management. U.S. Army Research Laboratory Human Research &\nEngineering Directorate.\n70 Int J Artif Intell Educ (2016) 26:37 –71\nSpecht, M. (2012). E-Learning Authoring Tools. In Encyclopedia of the Sciences of Learning (pp. 1111–\n1113). Springer US.\nStahl, G. (2006). Group cognition: Computer support for building collaborative knowledge.C a m b r i d g e :M I T\nPress.\nSuraweera, P., Mitrovic, A., & Martin, B. (2010). Widening the knowledge acquisition bottleneck for\nconstraint-based tutors. International Journal of Artificial Intelligence in Education , 20(2), 137–173.\nVanlehn, K., Lynch, C., Schulze, K., Shapiro, J. A., Shelby, R., Taylor, L., … Wintersgill, M. (2005). The\nAndes physics tutoring system: lessons learned. International Journal of Artificial Intelligence in\nEducation, 15(3), 147–204.\nWoolf, B. P. (2010). Building intelligent interactive tutors: Student-centered strategies for revolutionizing e-\nlearning. Morgan Kaufmann.\nWoolf, B. & McDonald, D. (1984). Design issues in building a computer tutor. IEEE Computer, Sept. 1984.\nZhang, J., Scardamalia, M., Reeve, R., & Messina, R. (2009). Designs for collective cognitive responsibility in\nknowledge-building communities.The Journal of the Learning Sciences , 18(1), 7–44.\nInt J Artif Intell Educ (2016) 26:37 –71 71",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8133895993232727
    },
    {
      "name": "Operationalization",
      "score": 0.7556372880935669
    },
    {
      "name": "Usability",
      "score": 0.654953122138977
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5839760303497314
    },
    {
      "name": "Task (project management)",
      "score": 0.5366467833518982
    },
    {
      "name": "Construct (python library)",
      "score": 0.4770433306694031
    },
    {
      "name": "Software",
      "score": 0.4218246340751648
    },
    {
      "name": "World Wide Web",
      "score": 0.33250725269317627
    },
    {
      "name": "Programming language",
      "score": 0.0927824079990387
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24603500",
      "name": "University of Massachusetts Amherst",
      "country": "US"
    }
  ]
}