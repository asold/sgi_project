{
    "title": "End-to-End Transformer for Compressed Video Quality Enhancement",
    "url": "https://openalex.org/W4389253021",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A1969430579",
            "name": "Li Yu",
            "affiliations": [
                "Nanjing University of Information Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4282012225",
            "name": "Wenshuai Chang",
            "affiliations": [
                "Nanjing University of Information Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2111094036",
            "name": "Shiyu Wu",
            "affiliations": [
                "Nanjing University of Information Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A237246515",
            "name": "Moncef Gabbouj",
            "affiliations": [
                "Tampere University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2140199336",
        "https://openalex.org/W2146395539",
        "https://openalex.org/W4282981352",
        "https://openalex.org/W6787254463",
        "https://openalex.org/W2911075534",
        "https://openalex.org/W3118212025",
        "https://openalex.org/W3126721948",
        "https://openalex.org/W3175859344",
        "https://openalex.org/W2955874753",
        "https://openalex.org/W3133235857",
        "https://openalex.org/W4210850244",
        "https://openalex.org/W3176978712",
        "https://openalex.org/W2142683286",
        "https://openalex.org/W2519021537",
        "https://openalex.org/W2753574358",
        "https://openalex.org/W2740336064",
        "https://openalex.org/W2508457857",
        "https://openalex.org/W2962935835",
        "https://openalex.org/W4316660395",
        "https://openalex.org/W2792766850",
        "https://openalex.org/W2915260409",
        "https://openalex.org/W2985680812",
        "https://openalex.org/W2997563066",
        "https://openalex.org/W3205818350",
        "https://openalex.org/W3176238949",
        "https://openalex.org/W4281667594",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2962687329",
        "https://openalex.org/W2553668389",
        "https://openalex.org/W6752237900",
        "https://openalex.org/W6846846049",
        "https://openalex.org/W6758681311",
        "https://openalex.org/W6726381175",
        "https://openalex.org/W6704408313",
        "https://openalex.org/W2613155248",
        "https://openalex.org/W2510648513",
        "https://openalex.org/W2890577598",
        "https://openalex.org/W2612222456",
        "https://openalex.org/W2757828535",
        "https://openalex.org/W2752154940",
        "https://openalex.org/W4312865536",
        "https://openalex.org/W4312957279",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W6763509872",
        "https://openalex.org/W3097217077",
        "https://openalex.org/W2963703197",
        "https://openalex.org/W6763310536",
        "https://openalex.org/W3035160371",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W6784094891",
        "https://openalex.org/W6797399245",
        "https://openalex.org/W4312815172",
        "https://openalex.org/W2963351113",
        "https://openalex.org/W3095422700",
        "https://openalex.org/W6790703111",
        "https://openalex.org/W4214755140",
        "https://openalex.org/W3111535274",
        "https://openalex.org/W4225672218",
        "https://openalex.org/W2126926806",
        "https://openalex.org/W4225293088",
        "https://openalex.org/W4307638415",
        "https://openalex.org/W3100087914",
        "https://openalex.org/W2979990382",
        "https://openalex.org/W3153465022",
        "https://openalex.org/W3104772632",
        "https://openalex.org/W2345337169",
        "https://openalex.org/W3113370935",
        "https://openalex.org/W3132890542"
    ],
    "abstract": "Convolutional neural networks have achieved excellent results in compressed video quality enhancement task in recent years. State-of-the-art methods explore the spatio-temporal information of adjacent frames mainly by deformable convolution. However, the CNN-based methods can only exploit local information, thus lacking the exploration of global information. Moreover, current methods enhance the video quality at a single scale, ignoring the multi-scale information, which corresponds to information at different receptive fields and is crucial for correlation modeling. Therefore, in this work, we propose a Transformer-based compressed video quality enhancement ( <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TVQE</i> ) method, consisting of Transformer based Spatio-Temporal feature Fusion ( <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TSTF</i> ) module and Multi-scale Channel-wise Attention based Quality Enhancement ( <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">MCQE</i> ) module. The proposed <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TSTF</i> module learns both local and global features for correlation modeling, in which window-based Transformer and the encoder-decoder structure greatly improve the execution efficiency. The proposed <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">MCQE</i> module calculates the multi-scale channel attention, which aggregates the temporal information between channels in the feature map at multiple scales, achieving efficient fusion of inter-frame information. Extensive experiments on the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">JCT-VT</i> test sequences show that the proposed method increases PSNR by up to 0.98 dB when QP <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$=$</tex-math> </inline-formula> 37. Meanwhile, the inference speed is improved by up to 9.4%, and the number of Flops is reduced by up to 84.4% compared to competing methods at 720p resolution. Moreover, the proposed method achieves the BD-rate reduction up to 23.04%.",
    "full_text": "IEEE TRANSACTIONS ON BROADCASTING 1\nEnd-to-End Transformer for Compressed\nVideo Quality Enhancement\nLi Yu , Wenshuai Chang, Shiyu Wu, and Moncef Gabbouj , Fellow, IEEE\nAbstract—Convolutional neural networks have achieved excel-\nlent results in compressed video quality enhancement task\nin recent years. State-of-the-art methods explore the spatio-\ntemporal information of adjacent frames mainly by deformable\nconvolution. However, the CNN-based methods can only exploit\nlocal information, thus lacking the exploration of global\ninformation. Moreover, current methods enhance the video\nquality at a single scale, ignoring the multi-scale information,\nwhich corresponds to information at different receptive ﬁelds\nand is crucial for correlation modeling. Therefore, in this\nwork, we propose a Transformer-based compressed video quality\nenhancement ( TVQE) method, consisting of Transformer based\nSpatio-Temporal feature Fusion ( TSTF) module and Multi-scale\nChannel-wise Attention based Quality Enhancement ( MCQE)\nmodule. The proposed TSTF module learns both local and\nglobal features for correlation modeling, in which window-based\nTransformer and the encoder-decoder structure greatly improve\nthe execution efﬁciency. The proposed MCQE module calculates\nthe multi-scale channel attention, which aggregates the temporal\ninformation between channels in the feature map at multiple\nscales, achieving efﬁcient fusion of inter-frame information.\nExtensive experiments on the JCT-VT test sequences show that\nthe proposed method increases PSNR by up to 0.98 dB when\nQP=37. Meanwhile, the inference speed is improved by up to\n9.4%, and the number of Flops is reduced by up to 84.4%\ncompared to competing methods at 720p resolution. Moreover,\nthe proposed method achieves the BD-rate reduction up to\n23.04%.\nIndex Terms—Compressed video quality enhancement, video\ncompression, transformer, deep learning.\nI. I NTRODUCTION\nT\nHE MULTIMEDIA industry is growing rapidly and\nconsumers are expecting videos of higher quality. On the\none hand, video is becoming the main form of information\ncarrier in increasing applications, including remote education,\ntelemedicine, live broadcasting, digital TV , video conference\nManuscript received 9 May 2023; revised 14 October 2023; accepted\n23 October 2023. This work was supported in part by the National Natural\nScience Foundation of China under Grant 62002172; in part by the Business\nFinland AMALIA-2023 under Grant 97/31/2023; and in part by the High\nPerformance Computing Center of Nanjing University of Information Science\n& Technology. (Corresponding author: Moncef Gabbouj.)\nLi Yu is with the School of Computer Science, the Engineering Research\nCenter of Digital Forensics, Ministry of Education, and the Jiangsu\nCollaborative Innovation Center of Atmospheric Environment and Equipment\nTechnology (CICAEET), Nanjing University of Information Science and\nTechnology, Nanjing 214500, China (e-mail: li.yu@nuist.edu.cn).\nWenshuai Chang and Shiyu Wu are with the School of Software and the\nEngineering Research Center of Digital Forensics, Ministry of Education,\nNanjing University of Information Science and Technology, Nanjing 214500,\nChina.\nMoncef Gabbouj is with the Department of Computing Sciences, Tampere\nUniversity, 33100 Tampere, Finland (e-mail: moncef.gabbouj@tuni.ﬁ).\nDigital Object Identiﬁer 10.1109/TBC.2023.3332015\nand so on. On the other hand, the demand for video resolution\nis constantly increasing, from 1080 p to 2 K,3 K,4 K,a s\nwell as 8 K. The extremely large amount of video data has\nto be compressed by video compression standards, such as\nH.264/A VC [1] and H.265/HEVC [2], to ﬁt the available\nstorage and network bandwidth. As the compression ratio\nincreases, the encoder signiﬁcantly reduces the bit rate while\nintroducing undesirable artifacts that severely degrade the\nquality of experience ( QoE). The introduced artifacts also\nimpair the performance of downstream video-oriented tasks\n(e.g., action recognition [3], [4], object tracking [5], [6], video\nunderstanding [7], [8], [9] and frame interpolation [10], [11]).\nTherefore, it is necessary to enhance the quality of the\ncompressed video and improve the efﬁciency of video\nstreaming [12].\nConvolutional neural networks (CNNs) have achieved mile-\nstones in the task of video quality enhancement ( VQE).\nThe CNN-based approaches can generally be classiﬁed into\ntwo categories: single-frame based methods [13], [14], [15],\n[16], [17], [18], [19] and multi-frame based methods [20],\n[21], [22], [23], [24], [25], [26]. The single-frame based\nvideo enhancement method is equivalent to image enhance-\nment, which explores the contextual information within the\nframe/image by CNNs to reduce compression artifacts and\nimprove the visual quality. However, the temporal correlations\nbetween adjacent frames in the video are ignored, which\nseverely limits the performance. In multi-frame based meth-\nods, the temporal information between adjacent frames are\nexplored. Since there are motions between adjacent frames,\nthe inter-frame information cannot be used directly. Some\nworks use the optical ﬂow to compensate the motion between\nframes. For example, [20], [21] use dense optical ﬂow for\nmotion compensation. However, the optical ﬂow calculated\nfrom compressed video tends to be inaccurate. Thus, some\nwork [23] utilizes deformable convolution ( DCN) to capture\nthe dependencies between multiple adjacent frames and the\nDCN-based approaches have made great progress in this task.\nThen, [24] proposed the Recursive Fusion ( RF) module based\non [23], which saves the temporal information of previously\nenhanced video frames for correlation modeling, implicitly\nexpanding the temporal range and achieving superior results.\nHowever, deformable convolutional alignment modules in\nDCN-based approaches are difﬁcult to train, and its instability\nin training often leads to offset overﬂow, which ultimately\nreduces the efﬁciency of correlation modeling.\nIn order to capture the long-range correlations between\nframes, we propose the Transformer based Spatio-Temporal\nfeature Fusion ( TSTF) module, which introduces Vision\nc⃝ 2023 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by/4.0/\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n2 IEEE TRANSACTIONS ON BROADCASTING\nFig. 1. An illustration showing the quality enhancement performance of our TVQE method, compared with DCN-based method ( STDF) and HEVC (Class\nC, BasketballDrill).\nTransformer into the VQE task for its strong capability to\nlearn long-range dependencies between image patches and\nits adaptability to image content. Since the computational\ncomplexity of the traditional Vision Transformer [27] grows\nquadratically with the increase of image resolution, we build\nour model based on improved Swin Transformer [28] in this\nwork, along with the autoencoder structure. The window-based\nTransformer and multi-scale encoder-decoder structure with\nskip connections can improve the inference efﬁciency and\nreduce GPU consumption. The proposed method is proved to\nfacilitate the mining of spatio-temporal information, as well as\ncorrelation modeling of temporal information among multiple\nframes. As shown in Fig. 1, the compressed frame 210 is\nenhanced with the information from frames 207 to 213 (only\nframe 208 to 212 are drawn for illustration). It can be seen\nthat the green line on the ﬂoor (below the athlete’s arm) is\ntotally or partially occluded in frames from 207 to 209, and\nbecomes gradually clear from frame 210 to 213. In order to\nrecover the green line in frame 210, proper correlation among\npixels should be modeled. The results show that our method\nachieves better recovery result in this region, which veriﬁes its\neffective correlation modeling of temporal information among\nmultiple frames.\nOn the other hand, a multi-scale channel attention enhance-\nment module is proposed to calculate channel attention at\nmultiple scales, which aggregates the multi-scale temporal\ninformation from the output of TSTF. Existing methods\noverlook the signiﬁcance of multiscale enhancement, which is\nessential for capturing and enhancing both local and global\nfeatures in video frames. In TSTF, the large-scale feature\nmaps have a small receptive ﬁeld, which can model the\nspatio-temporal correlation of small motions at pixel level.\nWhile the small-scale feature maps have a large receptive\nﬁeld and are capable of modeling spatio-temporal correlations\nof large motions. The proposed multi-scale channel attention\nenhancement module can efﬁciently fuse the multi-scale\ninter-frame information and ﬁnally generates the high-quality\nvideo frame.\nThe main contributions of this paper are summarized as\nfollows:\n• A novel network for enhancing compressed videos is\nproposed, which is entirely based on a Transformer-based\narchitecture.\n• A Transformer-based spatio-temporal information explo-\nration module is proposed, which is capable of long-range\nand adaptive correlation modeling between video frames.\n• A multi-scale channel-wise attention based quality\nenhancement module is designed to integrate information\nfrom both large and small motions.\nThe rest of the paper is organized as follows. In Section II,\nthe deep learning based compressed video enhancement meth-\nods and Vision Transformer are reviewed. SectionIII describes\nthe task of VQE, the proposed TVQE method and the training\nscheme. Section IV presents the experiments and results.\nFinally, conclusions are provided in Section V.\nII. R ELATED WORK\nIn this section, we ﬁrst review recent works on deep\nlearning-based quality enhancement of compressed video,\nincluding single-frame based methods and multi-frame based\nmethods. Then, a brief overview on Vision Transformer is\nprovided.\nA. Single-Frame Based Video Enhancement Method\nSingle-frame video enhancement task is equivalent to image\nenhancement. Earlier works [13], [15], [17], [18], [29], [30],\n[31], [32], [33] mainly focus on the quality enhancement\nof JPEG compressed images. Speciﬁcally, AR-CNN [13] ﬁrst\nuses a convolutional neural network for image enhancement,\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nYU et al.: END-TO-END TRANSFORMER FOR COMPRESSED VQE 3\nand learns the nonlinear mapping between the original image\nand the compressed image with four convolutional layers.\nSubsequently, several works [15], [16], [17], [34], [35],\n[36] propose using deeper networks to further improve the\nperformance. With batch normalization and residual learning\nemerged, DnCNN [17] effectively solves the problem of\ngradient disappearance in deep image enhancement networks.\nNLRN [31] and RNAN [33] propose the residual non-\nlocal attention mechanism to capture long-range dependencies\nbetween pixels. In addition to exploiting the information in\nthe spatial domain, methods such as [14], [18], [32] exploit\nthe relevant information in the frequency domain to further\nimprove the subjective visual quality. In particular, [37], [38],\n[39], [40], [41] utilize the prior knowledge to improve the\nenhancement performance. For example, DS-CNN [41] and\nQE-CNN [40] use different methods to deal with intra-frame\ncoding (e.g., AI) and inter-frame coding (e.g., LDP, LDB, RA).\nB. Multi-Frame Based Video Enhancement Method\nMulti-frame video enhancement methods utilize the spatio-\ntemporal information of multiple adjacent frames for\nenhancement. Yang et al. [20] proposed the Multi-Frame\nQuality Enhancement ( MFQE 1.0) method, which ﬁrst uses\nSVM to distinguish high and low quality frames, and then\nuses two adjacent high quality frames to enhance the low\nquality frame by optical ﬂow guided motion estimation.\nAs an enhanced version of MFQE 1.0, MFQE 2.0 [21]\nproposes an end-to-end quality enhancement network, which\npre-trained a bidirectional long short-term memory ( BiLSTM)\nbased model to detect peak quality frame ( PQF). The QE-\nsubnet is also advanced by introducing the multi-scale strategy,\nbatch normalization and dense connection. However, the video\nis compressed, and the compressed video can be severely\ndistorted by various compression artifacts, so the estimated\noptical ﬂow is often inaccurate and unreliable, resulting in\nineffective motion compensation. To this end, Deng et al.\nproposed a sliding window based method STDF [23], which\nutilizes deformable convolution to avoid explicit calculation\nof optical ﬂow. This method innovatively proposed to perform\nfeature alignment of moving objects on input multi-frame\nimages through deformable convolution. Based on STDF,\nRFDA [24] proposes the recursive fusion (\nRF) module, which\nnot only ultilizes the reference frames within the current\ntime window, but also exploits the temporal information of\npreviously enhanced video frames. By implicitly expanding\nthe time window, RFDA leverages a larger range of tem-\nporal information for better spatio-temporal compensation.\nHowever, the computational complexity of RF module is\nhuge. STDR [42] proposes a Multi-path Deformable Alignment\n(MDA) module based on STDF, which integrates the alignment\nfeatures of different receptive ﬁelds to obtain more accurate\ndeformation offsets. Luo et al. [43] proposed a recurrent\ndeformable fusion method. Instead of aligning multiple frames\nto the target frame simultaneously, it aligns each pair of the\ntarget frame and adjacent frame according to the timeline,\nwhich uses the compensation information between the adjacent\nframes more efﬁciently.\nC. Vision Transformers\nTransformer is a deep neural network based on self-attention\nmechanism and parallel processing. Transformer [44] ﬁrst\nemerged in the ﬁeld of NLP. The successful application of\nTransformer in NLP has made relevant scholars begin to inves-\ntigate its application in the ﬁeld of computer vision [27], [45].\nImage Transformer [46] was the ﬁrst to migrate the\nTransformer architecture to the ﬁeld of computer vision.\nSubsequently, Dosovitskiy et al. [27] proposed the Visual\nTransformer (Vit), and Vit completely replaces the Transformer\nstructure with the convolutional structure to deal with the\nclassiﬁcation task, and achieves results beyond CNN on\nextremely large-scale datasets [47], [48], [49], [50]. However,\nthe self-attention mechanism calculates the global similarity,\nand its computational complexity grows quadratically with\nthe expansion of spatial resolution. To improve operational\nefﬁciency, an efﬁcient and effective Vision Transformer called\nSwin Transformer was proposed in [28]. Based on the shift\nwindow mechanism, Swin Transformer achieves state-of-\nthe-art performance in image classiﬁcation [27], [28], [51],\nobject detection [45], [52], image segmentation [53], [54],\nvideo understanding [55], [56], image generation [57] and\npoint clouds processing [58], [59]. Zamir et al. proposed\nRestomer [60]\n, which computes self-attention across channels\nrather than spatial dimensions, and its complexity grows\nlinearly with image resolution. Thus, Restomer achieves state-\nof-the-art performance for high-resolution images restoration.\nIII. M ETHODOLOGY\nGiven a compressed video consisting of T frames V =\n[X1,X2,..., Xt,..., XT ], where Xt ∈ RH×W represents the\ncompressed frame at time t. H and W are the height and\nwidth of Xt. The task of compressed video enhancement is\nto generate an enhanced video Ve = [Xe\n1,Xe\n2,..., Xe\nt ,..., Xe\nT ]\nfrom the input compressed video V.\nThe overall framework of the proposed method is shown in\nFig. 2, which consists of two modules: (a) Transformer based\nSpatio-Temporal feature Fusion (TSTF) module and (b) Multi-\nscale Channel-wise Attention based Quality Enhancement\n(MCQE) module. The TSTF module explores the spatio-\ntemporal information across multiple frames by modeling the\ncorrelations among these frames. After TSTF, the information\nbetween channels in the feature map is further fused by\nMCQE, and ﬁnally generate the residual of the enhanced\nframe. For each compressed frame X\nt, its R preceding frames\nand R succeeding frames are used to exploit correlated tempo-\nral information. With the input Vt ={ Xt−R,..., Xt,..., Xt+R},\nthe whole process can be expressed as:\nXm\nt = T(Vt;φ), (1)\nIt = M\n(\nXm\nt ;ϕ\n)\n, (2)\nXe\nt = It + Xt, (3)\nwhere T(·) denotes the process of TSTF, and M(·) denotes\nthe process of MCQE. Xm\nt is the output of TSTF, which is a\nset including four scales features (TXm\nt , SXm\nt , MXm\nt , LXm\nt ). It\nis the output of MCQE, and Xe\nt is the ﬁnal output. φ and ϕ\nrepresent the parameters to be learned in the TSTF and MCQE\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n4 IEEE TRANSACTIONS ON BROADCASTING\nFig. 2. The framework of our proposed TVQE method, which consists of the Transformer based Spatio-Temporal feature Fusion (TSTF) Module and the\nMulti-scale Channel-wise Attention based Quality Enhancement (MCQE) Module. The TSTF module is designed to exploit spatio-temporal correlation between\nmultiple frames. After TSTF, the multi-scale information between channels in the feature map is further fused by the MCQE module, and ﬁnally generate the\nenhanced frame.\nmodules, respectively. The residual learning is used to improve\nthe training efﬁciency.\nA. Transformer Based Spatio-Temporal Feature Fusion\nModule\nOur proposed TSTF is able to explore wider spatio-temporal\ninformation than the DCN-based method. As the DCN-based\nmethod uses local offsets to build inter-frame correlations,\nwhich only matches critical information within a local range\nbetween adjacent frames. While the critical information out-\nside the range is ignored. The proposed TSTF characterizes\ndifferent scales of motion by using different scales of features\nwithin a ﬁxed size window, thanks to the U-net struc-\nture. The small-scale features contain global spatio-temporal\ninformation, which enables modeling the spatio-temporal\ncorrelation of large motions. While the large-scale features\nare able to capture the spatio-temporal information of small\nmotions at the pixel level. Thus, TSTF is able to capture the\nspatio-temporal association of both large and small motions\nbetween adjacent frames.\nThe TSTF module is an autoencoder, consisting of the Patch\nPartition layer , adaptive Swin Transformer Block ( ASwin-\nTB) and Pixel Shufﬂe layer . First, the target frame and the\nadjacent reference frames are partitioned into non-overlapping\npatches by the Patch Partition layer . For the consideration\nof computing speed, the Patch Partition layer downsamples\nthe features and restores it to the original resolution at the\nﬁnal stage by the Pixel Shufﬂe layer . Between the Patch\nPartition layer and the Pixel Shufﬂe layer , the spatio-temporal\ninformation is aggregated with our improved ASwin-TB.\nThe original Swin Transformer only supports input image of\nﬁxed-resolution. For the same window size, input images with\ndifferent resolution lead to different segmentation. So, when\nthe input image size changes, the overall feature map size and\nthe number of segmented windows will also change, which in\nturn affects the calculation of masks. Therefore, we propose\nthe improved ASwin-TB, which pads the length and width of\nthe feature map to an integer multiple of the window size to\nsupport input image of any resolution. In TSTF, each patch\nafter segmentation is treated as a token and then calculated\nthe spatio-temporal attention. In the encoder, Patch Merging\nLayer increases the number of channels while features are\ndownsampled, and ASwin-TB further enhances the features.\nFor V\nt ={ Xt−R,..., Xt,..., Xt+R}, the whole Encoder process\ncan be expressed as\nE1 = Estage1(Vt), (4)\nE2 = Estage2(E1), (5)\nE3 = Estage3(E2), (6)\nwhere Estage denotes the combination of Patch Merging and\nASwin-TB, and 1, 2, 3 represent each stage of the encoder.\nCorresponding to the encoder, the decoder uses a Patch\nExpanding Layer to upsample the deep features. For each\nscale, the low-level features in the encoder are connected to the\nhigh-level features in the decoder through skip connections,\nto reduce the loss of spatial information caused by downsam-\npling. The whole Encoder process can be expressed as\nTX\nm\nt = Dstage1(E3) + E3, (7)\nSXm\nt = Dstage2\n(\nTXm\nt\n)\n+ E2, (8)\nMXm\nt = Dstage3\n(\nSXm\nt\n)\n+ E1, (9)\nLXm\nt = UP\n(\nMXm\nt\n)\n(10)\nwhere Dstage denotes the combination of Patch Expanding\nand ASwin-TB, UP represents the upsampling layer of Pixel\nShufﬂe, and 1, 2, 3 represent each stage of the decoder.\nBy using TSTF, the compressed frame at time t can\naggregate the temporal information from the adjacent reference\nframes and generate four different scales of features, from\nsmall to large (TX\nm\nt , SXm\nt , MXm\nt , LXm\nt ) in order.\nB. Multi-Scale Channel-Wise Attention Based Quality\nEnhancement Module\nTo efﬁciently fuse the temporal information between chan-\nnels and generate the residual map for the target frame, we\npropose an efﬁcient multi-scale channel-wise attention module\n(MCQE). In TSTF, the encoder and decoder structure produces\nmulti-scale information. As the resolution of feature map\ndecreases, the number of channels doubles, allowing for a\nlarger receptive ﬁeld within a ﬁxed window size. Thus for\nfeature map of low resolution, the channel information of\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nYU et al.: END-TO-END TRANSFORMER FOR COMPRESSED VQE 5\nFig. 3. The architecture of Channel Attention block, consisting of (a) Multi-\nHead Transposed Attention (MTA) and (b) Feed-Forward Network (FN). MTA\ncalculates channel-level attention and FN performs feature transformation.\nlarge inter-frame motion can be effectively extracted. While\nwith the increased resolution, the channel information of the\nfeature map contains information at the pixel level, enabling\nthe correlation modeling of small movements. In MCQE,t h e\nchannel attention of each small-scale feature from the TSTF\nis calculated and then fused with the next level of large-scale\nfeatures. Finally, features containing multi-scale information\nare obtained, containing information of different motion levels.\nThe proposed MCQE module is shown in Fig. 2-(b). The\ndecoder of TSTF contains four different scales of features\n(TX\nm\nt , SXm\nt , MXm\nt , LXm\nt ). To efﬁciently fuse multi-scale fea-\ntures, the channel attention is ﬁrst computed for small-scale\nfeatures. Next, upsampling is performed with Pixel Shufﬂe .\nThen, they are fused with the next level of features by\nsummation operation. Finally, the texture features at different\nscales are fused into the enhanced residual map. Speciﬁcally,\nthe minimum scale feature TX\nm\nt calculates the channel-level\nattention, and after Pixel Shufﬂe upsampling, it is summed\nwith SXm\nt to obtain SXm′\nt . Accordingly, SXm′\nt calculates the\nattention and then sums with MXm\nt to obtain MXm′\nt . MXm′\nt\ncalculates the attention and then sums with LXm′\nt to obtain\nLXm′\nt . Then, LXm′\nt fuses the information of four different scales\nin the decoder from TSTF and undergoes a ﬁnal calculation of\nchannel attention. The temporal information between channels\nin the temporal feature map is further fused and enhanced.\nFinally the number of channels is reduced to 1 by a 3 × 3\nconvolution layer to get the ﬁnal residual map. The whole\nprocess are as follows:\nTX\nm′\nt = M\n(\nTXm\nt\n)\n, (11)\nSXm′\nt = UP\n(\nCA(TXm′\nt + SXm\nt )\n)\n, (12)\nMXm′\nt = UP(CA(SXm′\nt + MXm\nt ), (13)\nLXm′\nt = UP(CA(MXm′\nt + LXm\nt )), (14)\nIt = Conv(CA(LXm′\nt )). (15)\nwhere CA denotes Channel Attention block , UP denotes the\nupsample layer of Pixel Shufﬂe and Conv is the convolution\nlayer.\nA ss h o w ni nF i g .3,t h e Channel Attention block consists\nof two parts: Multi-Head Transposed Attention ( MTA) and\nFeed-Forward Network ( FN). The MTA computes the cross-\ncovariance between channels instead of spatial dimensions,\nwhich reduces the computational overhead of the network. The\ninput feature Xt\nm is fed into one Convolution layer to generate\nQ ∈ RH×W×C, K ∈ RH×W×C, V ∈ RH×W×C. Then use the\nreshape operation to get ˆQ ∈ RHW×C, ˆK ∈ RHW×C, ˆV ∈\nRHW×C. Finally, MTA calculates the dot product of ˆQ and ˆK\nto generate the channel attention map M with size C × C,\nwhich can be expressed as:\nM = Softmax( ˆQ · ˆK) (16)\nTo get more accurate residual information, we utilize FN to\nenhance the details.\nC. Training Scheme\nFor frame Xt at time t, we use a two-stage training strategy\nto enhance its quality. In the ﬁrst stage, we use Charbonnier\nLoss [61] to optimize the parameters of TVQE. In the second\nstage, we use L\n2 Loss to further ﬁne-tune the model for a\nbetter visual result. Finally, the loss functions are deﬁned as\nbelow:\nL\ncharb =\n√ (\nXet − Xrawt\n)2 + ϵ, (17)\nLmse =\n Xe\nt − Xraw\nt\n 2\n2, (18)\nL = α × Lcharb + β × Lmse, (19)\nwhere Xe\nt denotes the enhanced video frame at time t, Xraw\nt\ndenotes the original uncompressed frame (ground truth), and\nϵ is a constant set to 10 −6. α and β are the weights of the\nloss.\nIV . EXPERIMENT\nA. Datasets\nFollowing [21], [23],W eu s et h e MFQE 2.0 [21] and\nLDV [25] datasets for training, and JCT-VC [64] dataset\nfor testing. All sequences were compressed by HEVC HM\n16.51 with LDP (Low-Delay-P) conﬁguration. Five QPs\n(quantization parameters), i.e., 22, 27, 32, 37,42 at different\ncompression bit rates are used for experiments.\n1) MFQE 2.0: It contains totally 128 sequences, in which\ntraining set contains 108 sequences. The sequences are\nacquired from Xiph.org [65] and VGQE [66], with resolutions\nranging from SIF (352 × 240) to WQXGA (2560 × 1600).\n2) LDV: It is proposed in NTIRE 2021 challenge [25] with\n240 sequences, which consists of training set, validation set\nand test set. We use 200 sequences from the training set as\ntraining data and 40 sequences from the validation and test\nsets for validation, all sequences have the same resolution of\n960 × 536.\n3) JCT-VC: The test set has 18 sequences, delivered by\nJCT-VC (Joint Collaborative Team on Video Coding) for\nevaluating the performance of our model. There are totally\nﬁve resolutions ranging from 240p (416 × 240) to WQXGA\n(2560 × 1600), named as Class A to E.\nB. Implementation Details\nFor network structure, the window size is set to 8 in TSTF.\nIn the ﬁrst to third stages of the encoder in TSTF, the number\nof ASwin-TB modules is set to [2, 2, 2], and the number\nof attention heads is set to [2, 2, 2]. The Patch embedding\ndimension is set to 48, and the MLP-ratio is 1.\n1https://hevc.hhi.fraunhofer.de/trac/hevc/milestone/HM-16.5\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n6 IEEE TRANSACTIONS ON BROADCASTING\nTABLE I\nOBJECTIVE RESULTS OF /Delta1PSNR ( DB) / /Delta1SSIM (×10−2) ON JCT-VC D ATASET AT 5D IFFERENT QPS.T HE BEST AND\nSECOND BEST PERFORMANCE ARE IN BOLD AND UNDERLINED ,R ESPECTIVELY\nIn the training stage, we crop 128 × 128 patches randomly\nfrom the compressed video and the corresponding raw video\nas training samples. Random ﬂips and rotations are also used\nfor data augmentation. The batch size is set to 32. we train\nthe model using the Adam optimizer (β\n1=0.9, β2=0.999,\nϵ=10−8). The learning rate is 10 −4 throughout the training\nprocess. In the ﬁrst stage of training, α is set to 1 and β is set\nto 0 in Eq. (19). In the second stage, α is set to 0 and β is set\nto 1. All experiments are performed on the NVIDIA TITAN\nRTX.\nFor testing, we evaluate the results using /Delta1PSNR and\n/Delta1SSIM, as well as BD-rate. All tests are performed on the\nY-channel in YUV space.\nC. Comparison With State of the Art Methods\nTo demonstrate the effectiveness of our method, we compare\nthe proposed method with seven state-of-the-art meth-\nods, including single-frame based methods ( AR-CNN [13],\nRNAN [33], DnCNN [17], DCAD [39]), DS-CNN [40]) and\nmulti-frame based methods ( MFQE 1.0 [20], MFQE 2.0 [21],\nSTDF [23], RFDA [24], MRDN [62] and FastCNN [63].)\n1) Objective Results: Table I presents the Objective results\nof our method and seven state-of-the-art methods on /Delta1PSNR\nand /Delta1SSIM. It can be seen that our method outperforms\nthe current state-of-the-art methods on most sequences when\nQP=37. Besides, our method TVQE outperforms the seven\nmodels in terms of the average /Delta1PSNR and /Delta1SSIM for all\nQPs. Meanwhile, the gain of our method TVQE on SSIM is\nhigher than PSNR obviously. Such as QP =37, our method\nTVQE gains 7.7% on /Delta1PSNR and 12.3% on /Delta1SSIM over the\nsecond best method RFDA, which indicates that our method\nTVQE provides better visual effects.\nAs for the single-frame based methods, RNAN proposes\nnon-local attention blocks to obtain the remote dependency\nof the feature map, and ﬁnally achieves the best performance\namong all single-frame based methods. RNAN gains about\n69% over DnCNN, which reﬂects the superiority of the\nTransformer-based method. However, the single-frame based\nmethod cannot use temporal information and has limited\nperformance. Our method computes spatial attention and\nchannel attention over multiple frames, and achieves /Delta1PSNR\nof 0.98, which is about 123% compared to RNAN.\nAs for the multi-frame based methods, MFQE 2.0 calculates\nthe explicit optical ﬂow of compressed video and achieves\nan average /Delta1PSNR of 0 .56. STDF proposes deformable\nconvolution to align video frames, which solves the problem\nof inaccurate optical ﬂow estimation, and achieves an average\n/Delta1PSNR of 0 .83. RFDA utilizes the RF (Recursive Fusion)\nmodule to exploit temporal information within a longer time\nrange, and obtains /Delta1PSNR of 0.91. Our method TVQE utilizes\nthe long-range modeling property of Transformer to exploit\nthe temporal information, which further increases the PSNR\nwith an average /Delta1PSNR of 0 .98, which demonstrates the\neffectiveness of our method.\n2) Comparison of Model Complexity: Table III shows the\nParameters, FLOPs, GPU consumption, Inferred speed and\n/Delta1PSNR of our method, STDF [23] and RFDA [24]. As can\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nYU et al.: END-TO-END TRANSFORMER FOR COMPRESSED VQE 7\nTABLE II\nRESULTS OF BD-B ATE REDUCTION (%) AT QP = 22, 27,32,37 AND 42 WITH THE HEVC AS ANCHOR .T HE BEST AND\nSECOND BEST PERFORMANCE ARE IN BOLD AND UNDERLINED ,R ESPECTIVELY\nTABLE III\nCOMPARISON OF PARAMETERS ,F L O PS, GPU C ONSUMPTION ,I NFERRED\nSPEED AND /Delta1PSNR B ETWEEN OUR METHOD AND SOME MAINSTREAM\nMETHODS .F OR A FAIR COMPARISON ,A LL METHODS WERE RE-TESTED\nON THE NVIDIA TITAN RTX. T HE FLOP S AND FRAMES PER SECOND\n(FPS) A RE EV ALUATED AT RESOLUTION 720 P.T HE GPU C ONSUMPTION\nIS EV ALUATED AT RESOLUTION 1080 P.T HE BEST AND SECOND BEST\nPERFORMANCE ARE IN BOLD AND UNDERLINED ,R ESPECTIVELY\nbe seen, although our model is based on Transformer, it still\nhas a fast inference speed (FPS) and low FLOPs, thanks to the\nencoder-decoder structure and skip connections. At the same\ntime, our method is hardware friendly compare to RFDA [24]\nas it requires less GPU memory. More speciﬁcally, comparing\nto STDF at 720p resolution, our method is 9 .4% faster at\ninference speed (from 6 .4t o7 .0, in Table III), and with a\n84.4% reduction in FLOPs (from 0.77 to 0.12, in Table III), as\nwell as a 18.1% improvement in average /Delta1PSNR performance\nwhen QP= 37 (from 0 .83 to 0 .98, in Table I). RFDA is based\non STDF by adding RF module, and thus consumes more\nGPU resources. Comparing to RFDA, our method outperforms\nRFDA in terms of inference speed and GPU consumption at all\nresolutions. Speciﬁcally, under 1080p resolution, the inference\nspeed is improved by 7 .7% (from 6.5t o7 .0, in Table III) and\nFLOPs is reduced by 77 .4% (from 0 .53 to 0 .12, in Table III).\nOverall, although our proposed method has a larger parameters\ncompared to the CNN-based methods, it brings faster inference\nspeed and better performance with much lower computational\ncomplexity and moderate GPU consumption.\n3) Quality Fluctuation: High quality ﬂuctuation will\ndamage the user’s viewing experience. Thus, quality\nFig. 4. Illustration of quality ﬂuctuations of two sequences. (Top: Class C,\nBasketballDrill. Bottom: Class D, BQSquare.).\nﬂuctuation is evaluated in this section following the settings\nin [21]. The PSNR of each frame in two sequences are plotted\nin Fig. 4, where the horizontal axis represents the frame index,\nand the vertical axis represents the frame quality. It can be\nobserved that the HEVC compressed sequences have severe\nquality ﬂuctuations (i.e., quality differences between high\nquality frames and adjacent low quality frames). Compared\nto both STDF and RFDA, our method provides better PSNR\nand smaller quality ﬂuctuations, effectively improving the\nQoE.\n4) Rate-Distortion Performance: Fig. 6 presents the rate\ndistortion curves for the four sequences. It can be seen that\nour method outperforms other methods on both sequences\nwith huge motion (e.g., Class C, BasketballDrill) and smooth\nmotion (e.g., Class E, Johnny). In addition, we also calculate\nthe BD-rate reduction of PSNR on ﬁve QPs ( = 22, 27, 32,\n37, 42) in Table II. Our method provides an average BD-rate\nreduction of 23 .58%, which is better than the state-of-the-art\nCNN-based method RFDA with 21.73%. It demonstrates that\nTVQE exhibits a better rate distortion performance, which can\nprovide superior visual effects under the same bit-rate.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n8 IEEE TRANSACTIONS ON BROADCASTING\nFig. 5. Subjective results at QP 37. Video from top to bottom: BasketballDrill, Racehorses, BQSquare, BQTerrace, BasketballPass. For a fair comparison,\nfor each method we only enhance on Y component.\nFig. 6. Rate-Distortion curves of sequences BasketballDrill, Johnny,\nKristenAndSara and PeopleOnStreet.\n5) Subjective Results: Fig. 5 gives the subjective results for\nthe ﬁve sequences. It can be seen that the single-frame based\nquality enhancement methods AR-CNN [13] and DnCNN [17]\ndo not make use of temporal information, so the enhanced\nvideo frames still have serious compression artifacts (e.g.,\nblock effect, ringing effect). With the help of temporal\ninformation, CNN-based multi-frame quality enhancement\nmethods MFQE 2.0 [21] and STDF [23] provide better visual\neffects with the help of reference frames, but the locality of\nthe convolution operation prevents these methods from taking\nfull advantage of the temporal information of reference frames,\nresulting in enhanced video frames that are too smooth and\nlack of detailed texture. RFDA [24] further implicitly expands\nthe temporal range with RF to better recover details, but theRF\nTABLE IV\nCOMPARING TVQE AND RFDA ON VVC T EST SEQUENCES .T HE BEST\nAND SECOND BEST RESULTS ARE IN BOLD AND\nUNDERLINED ,R ESPECTIVELY\nmodule consumes large computational resources and decreases\nthe inference speed. TVQE is based on Transformer, which\nhas better remote correlation capability than convolution, thus\nresulting in better exploration of spatio-temporal information\nand ﬁner recovery of textures. For example in Fig. 5,t h e\nplayer’s ﬁngers in BaskerballDrill, the rope on the horse in\nRacehorses, the textures on the railings in BQSquare and\nBQTerrace, and the shadow of the sneaker in BaskerballPass\ncan be better recovered by our method than other methods.\n6) Results on VVC Test Sequences: To demonstrate the\neffectiveness of our proposed method on VVC compressed\nvideo sequences, we test both our method TVQE and RFDA\nmethod on videos compressed with VVC and QP = 37.\nTable IV shows that TVQE yields better results than RFDA on\nVVC compressed sequences. The proposed method achieves\nan average /Delta1PSNR / /Delta1SSIM of 0 .04 / 0 .10 (up to 0 .05 /\n0.27), which proves the applicability of the TVQE method on\nthe latest VVC compression standard.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nYU et al.: END-TO-END TRANSFORMER FOR COMPRESSED VQE 9\nTABLE V\nABLATION STUDY ON THE PROPOSED TSTF AND MCQE MODULES . STDF\nAND QE A RE FROM METHOD STDF, AND STSF IS FROM METHOD RFDA.\nTHE BEST AND SECOND BEST PERFORMANCE ARE IN BOLD AND\nUNDERLINED ,R ESPECTIVELY\nD. Ablation Study\nIn this section, we perform ablation experiments as well\nas speciﬁc analysis of the proposed method. We take STDF\nas baseline and replace the modules from different models\nto analyze their effects. For a fair comparison, all models\nare retrained by the same training approach as the proposed\nmethod. The results of FLOPs and inference speed FPS are\nobtained at 720p resolution. The GPU consumption is obtained\nat 1080p resolution, and /Delta1PSNR / /Delta1SSIM takes the average\nresults of the test sequences in ClassA-E at QP =37.\n1) Effectiveness of TSTF: To illustrate the effectiveness of\nthe TSTF module, we compare the proposed TSTF with the\nbaseline STDF (from STDF-R3L) and STFF (from RFDA). As\nshown in Table V, we replaced the STDF module with STFF\nand TSTF (the second and third row), where the TSTF provides\nbest performance while having the fastest inference speed and\nsecond lowest GPU consumption. Speciﬁcally, compared to\nthe baseline STDF, the Transformer-based TSTF is able to\nexplore global temporal information within a time window,\nwith an improvement of /Delta1PSNR by 0 .10 (from 0 .83 to 0 .93)\nand /Delta1SSIM (×10\n−2) by 0.24 (from 1 .51 to 1 .75). Moreover,\nbeneﬁt from the autoencoder structure and skip connections,\nTSTF has a 6 .3% speedup (from 6 .4t o6 .8) inference speed\ncompared to STDF and 8.6% reduction in GPU consumption\n(from 5.8t o5 .3). Compared with STFF, TSTF does not have\nto utilize additional information outside the time window,\nresulting in 16 .7% lower GPU consumption (from 10 .8t o\n9.0),6 7.2% lower FLOPs(from 0.61 to 0.20) and 4.6% higher\ninference express (from 6 .5t o6 .8), which demonstrates the\neffectiveness of the TSTF module.\n2) Effectiveness of MCQE: To illustrate the effectiveness of\nthe MCQE module, we replace the baseline quality enhance-\nment module QE with MCQE (fourth to sixth rows). As\nshown in the results (third and sixth lines), MCQE provides\na /Delta1PSNR gain of 0 .05 (from 0 .93 to 0 .98) and a /Delta1SSIM\ngain of 0 .07 (from 1 .75 to 1 .82) compared to the CNN-based\nQE. Meanwhile, the multi-scale channel attention resulted\nin a further 40% reduction in FLOPs (from 0 .20 to 0 .12)\nand 6 .7% reduction in memory consumption (from 9 .0t o\n8.4). In addition, the inference speed improved from 6 .8t o\n7.0 and outperformed all methods, which demonstrates the\neffectiveness of the MCQE module.\n3) Inﬂuence of the Number of ASwin-TB: Table VI shows\nthe impact of the number of ASwin-TB units on the\nTSTF module at different resolutions. Through comparison,\nTABLE VI\nINFLUENCE OF THE NUMBER OF ASwin-TB.D EPTHS REPRESENTS THE\nNUMBER OF ASwin-TB IN TSTF F ROM STAGE1 TO STAGE3. T HE FLOP S\nAND FRAMES PER SECOND (FPS) A RE EV ALUATED AT RESOLUTION\n720 P.T HE GPU C ONSUMPTION IS EV ALUATED AT RESOLUTION 1080 P.\nTHE BEST AND SECOND BEST PERFORMANCE ARE IN BOLD AND\nUNDERLINED ,R ESPECTIVELY\nTABLE VII\nINFLUENCE OF THE WINDOW SIZE .T HE FLOP S AND FRAMES PER\nSECOND (FPS) A RE EV ALUATED AT RESOLUTION 720 P.T HE GPU\nCONSUMPTION IS EV ALUATED AT RESOLUTION 1080 P.T HE BEST AND\nSECOND BEST PERFORMANCE ARE IN BOLD AND\nUNDERLINED ,R ESPECTIVELY\nprogressive growth increases the parameter volume by 41.1%\n(from 2.09 to 2.95) and the FLOPs by 5.2% (from 0.116\nto 0.122), while also resulting in faster inference speeds.\nHowever, the performance gain shows a signiﬁcant decline.\nTherefore, it can be concluded that information mining at each\nresolution is crucial.\n4) Window Size Impact: From the analysis above, we know\nthat ASwin-TB based on Transformer is more suitable for min-\ning spatio-temporal information in videos than Convolutions.\nWe further investigate the impact of different window sizes,\nand the results are shown in Table VII. It can be seen from\nthe table that as the window size increases, the model’s\nparameters and complexity remain the same. With a larger\nwindow size (from 7 to 8), a single window covers more\ninformation, enabling the model to capture a larger range of\nspatio-temporal information in videos, resulting in a higher\n/Delta1PSNR gain. However, there is a bottleneck in performance\ngain, as we found that when the window size is set to 9, the\n/Delta1PSNR performance is at the same level as that of a window\nsize of 8, while the /Delta1SSIM gain decreases. Additionally,\nlarger windows consume more GPU and have lower inference\nspeeds. Therefore, in this paper, we choose the ASwin-TB\nmodule with a window size of 8 as the basic unit to construct\nTSTF.\nV. C\nONCLUSION\nIn this paper, we propose an end-to-end Transformer based\nnetwork for compressed video enhancement ( TVQE), which\nmainly consists of two modules, TSTF module and MCQE\nmodule. The TSTF module can efﬁciently explore temporal\ninformation within the time window, while the MCQE mod-\nule can well fuses the temporal information. The proposed\nmethod outperforms the CNN-based methods both in terms\nof performance and inference speed. The proposed modules\ncan be used as plug-ins in other applications, such as video\nsuper-resolution and video interpolation, to explore and fuse\ntemporal information more effectively.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n10 IEEE TRANSACTIONS ON BROADCASTING\nREFERENCES\n[1] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra, “Overview\nof the H.264/A VC video coding standard,” IEEE Trans. Circuits Syst.\nVideo Technol., vol. 13, no. 7, pp. 560–576, Jul. 2003.\n[2] G. J. Sullivan, J.-R. Ohm, W.-J. Han, and T. Wiegand, “Overview of the\nhigh efﬁciency video coding (HEVC) standard,” IEEE Trans. Circuits\nSyst. Video Technol., vol. 22, no. 12, pp. 1649–1668, Dec. 2012.\n[3] Z. Sun, Q. Ke, H. Rahmani, M. Bennamoun, G. Wang, and J. Liu,\n“Human action recognition from various data modalities: A review,”\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 45, no. 3, pp. 3200–3225,\nMar. 2023.\n[4] Y . Zhu et al., “A comprehensive study of deep video action recognition,”\n2020, arXiv:2012.06567.\n[5] Y . Xu, X. Zhou, S. Chen, and F. Li, “Deep learning for multiple object\ntracking: A survey,”IET Comput. Vis., vol. 13, no. 4, pp. 355–368, 2019.\n[6] W. Luo, J. Xing, A. Milan, X. Zhang, W. Liu, and T.-K. Kim, “Multiple\nobject tracking: A literature review,” Artif. Intell., vol. 293, Apr. 2021,\nArt. no. 103448.\n[7] G. Bertasius, H. Wang, and L. Torresani, “Is space-time attention all\nyou need for video understanding?” in Proc. ICML, vol. 2, 2021, p. 4.\n[8] C.-Y . Wu and P. Krahenbuhl, “Towards long-form video understand-\ning,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2021,\npp. 1884–1894.\n[9] C.-Y . Wu, C. Feichtenhofer, H. Fan, K. He, P. Krahenbuhl, and\nR. Girshick, “Long-term feature banks for detailed video understand-\ning,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2019,\npp. 284–293.\n[10] Z. Shi et al., “Learning for unconstrained space-time video super-\nresolution,” IEEE Trans. Broadcast. , vol. 68, no. 2, pp. 345–358,\nJun. 2022.\n[11] W. Zhang, M. Zhou, C. Ji, X. Sui, and J. Bai, “Cross-frame transformer-\nbased spatio-temporal video super-resolution,” IEEE Trans. Broadcast.,\nvol. 68, no. 2, pp. 359–369, Jun. 2022.\n[12] J. Yue, Y . Gao, S. Li, H. Yuan, and F. Dufaux, “A global appearance and\nlocal coding distortion based fusion framework for CNN based ﬁltering\nin video coding,” IEEE Trans. Broadcast., vol. 68, no. 2, pp. 370–382,\nJun. 2022.\n[13] C. Dong, Y . Deng, C. C. Loy, and X. Tang, “Compression artifacts\nreduction by a deep convolutional network,” in Proc. IEEE Int. Conf.\nComput. Vis., 2015, pp. 576–584.\n[14] J. Guo and H. Chao, “Building dual-domain representations for com-\npression artifacts reduction,” in Proc. Eur. Conf. Comput. Vis. , 2016,\npp. 628–644.\n[15] K. Li, B. Bare, and B. Yan, “An efﬁcient deep convolutional neural\nnetworks model for compressed image deblocking,” in Proc. IEEE Int.\nConf. Multimedia Expo (ICME) , 2017, pp. 1320–1325.\n[16] F. Jiang, W. Tao, S. Liu, J. Ren, X. Guo, and D. Zhao, “An end-to-end\ncompression framework based on convolutional neural networks,” IEEE\nTrans. Circuits Syst. Video Technol. , vol. 28, no. 10, pp. 3007–3018,\nOct. 2018.\n[17] K. Zhang, W. Zuo, Y . Chen, D. Meng, and L. Zhang, “Beyond a\nGaussian denoiser: Residual learning of deep CNN for image denoising,”\nIEEE Trans. Image Process. , vol. 26, no. 7, pp. 3142–3155, Jul. 2017.\n[18] H. Chen, X. He, L. Qing, S. Xiong, and T. Q. Nguyen, “DPW-SDNet:\nDual pixel-wavelet domain deep CNNs for soft decoding of jpeg-\ncompressed images,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit.\nWorkshops, 2018, pp. 711–720.\n[19] L. Yu, W. Chang, Q. Liu, and M. Gabbouj, “High-frequency guided\nCNN for video compression artifacts reduction,” inProc. IEEE Int. Conf.\nVisual Commun. Image Process. (VCIP) , 2022, pp. 1–5.\n[20] R. Yang, M. Xu, Z. Wang, and T. Li, “Multi-frame quality enhancement\nfor compressed video,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2018, pp. 6664–6673.\n[21] Z. Guan, Q. Xing, M. Xu, R. Yang, T. Liu, and Z. Wang, “MFQE\n2.0: A new approach for multi-frame quality enhancement on com-\npressed video,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 43, no. 3,\npp. 949–963, Mar. 2021.\n[22] Y . Xu, L. Gao, K. Tian, S. Zhou, and H. Sun, “Non-local convLSTM\nfor video compression artifact reduction,” in Proc. IEEE/CVF Int. Conf.\nComput. Vis., 2019, pp. 7043–7052.\n[23] J. Deng, L. Wang, S. Pu, and C. Zhuo, “Spatio-temporal deformable\nconvolution for compressed video quality enhancement,” in Proc. AAAI\nConf. Artif. Intell. , vol. 34, 2020, pp. 10696–10703.\n[24] M. Zhao, Y . Xu, and S. Zhou, “Recursive fusion and deformable\nspatiotemporal attention for video compression artifact reduction,” in\nProc. 29th ACM Int. Conf. Multimedia , 2021, pp. 5646–5654.\n[25] R. Yang, “NTIRE 2021 challenge on quality enhancement of com-\npressed video: Methods and results,” in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit., 2021, pp. 647–666.\n[26] L. Yu, W. Chang, W. Quan, J. Xiao, D.-M. Yan, and M. Gabbouj,\n“Neural texture transfer assisted video coding with adaptive up-\nsampling,” Signal Processing: Image Communication , vol. 107, Sep.\n2022, Art. no. 116754.\n[27] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers\nfor image recognition at scale,” in Proc. Int. Conf. Learn. Represent. ,\n2020, pp. 1–21.\n[28] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. , 2021,\npp. 10012–10022.\n[29] L. Galteri, L. Seidenari, M. Bertini, and A. Del Bimbo, “Deep generative\nadversarial compression artifact removal,” in Proc. IEEE Int. Conf.\nComput. Vis., 2017, pp. 4826–4835.\n[30] J. Guo and H. Chao, “One-to-many network for visually pleasing\ncompression artifacts reduction,” in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2017, pp. 3038–3047.\n[31] D. Liu, B. Wen, Y . Fan, C. C. Loy, and T. S. Huang, “Non-local recurrent\nnetwork for image restoration,” in Proc. Adv. Neural Inf. Process. Syst. ,\nvol. 31, 2018, pp. 1680–1689.\n[32] J. Yoo, S.-H. Lee, and N. Kwak, “Image restoration by estimating\nfrequency distribution of local patches,” in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2018, pp. 6684–6692.\n[33] Y . Zhang, K. Li, B. Zhong, and Y . Fu, “Residual non-local attention\nnetworks for image restoration,” in Proc. Int. Conf. Learning Represent.,\n2019, pp. 1–18.\n[34] X. Mao, C. Shen, and Y .-B. Yang, “Image restoration using very\ndeep convolutional encoder-decoder networks with symmetric skip\nconnections,” in Proc. Adv. Neural Inf. Process. Syst. , vol. 29, 2016,\npp. 2802–2810.\n[35] P. Svoboda, M. Hradiš, D. Ba ˇrina, and P. Zemˇcík, “Compression artifacts\nremoval using convolutional neural networks,” 2016, arXiv:1605.00366.\n[36] K. Zhang, W. Zuo, S. Gu, and L. Zhang, “Learning deep cnn denoiser\nprior for image restoration,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2017, pp. 3929–3938.\n[37] Y . Dai, D. Liu, and F. Wu, “A convolutional neural network approach for\npost-processing in HEVC intra coding,” in Proc. Int. Conf. Multimedia\nModel., 2017, pp. 28–39.\n[38] Z. Jin, P. An, C. Yang, and L. Shen, “Quality enhancement for intra\nframe coding via CNNs: An adversarial approach,” in Proc. IEEE Int.\nConf. Acoust. Speech Signal Process. (ICASSP) , 2018, pp. 1368–1372.\n[39] T. Wang, M. Chen, and H. Chao, “A novel deep learning-based method\nof improving coding efﬁciency from the decoder-end for HEVC,” in\nProc. Data Compress. Conf. (DCC) , 2017, pp. 410–419.\n[40] R. Yang, M. Xu, T. Liu, Z. Wang, and Z. Guan, “Enhancing quality for\nHEVC compressed videos,” IEEE Trans. Circuits Syst. Video Technol. ,\nvol. 29, no. 7, pp. 2039–2054, Jul. 2019.\n[41] R. Yang, M. Xu, and Z. Wang, “Decoder-side HEVC quality enhance-\nment with scalable convolutional neural network,” in Proc. IEEE Int.\nConf. Multimedia Expo (ICME) , 2017, pp. 817–822.\n[42] L. Peng, A. Hamdulla, M. Ye, S. Li, and H. Guo, “Recurrent deformable\nfusion for compressed video artifact reduction,” in Proc. IEEE Int. Symp.\nCircuits Syst. (ISCAS) , 2022, pp. 3175–3179.\n[43] D. Luo, M. Ye, S. Li, C. Zhu, and X. Li, “Spatio-temporal\ndetail information retrieval for compressed video quality enhance-\nment,” IEEE Trans. Multimedia , vol. 25, pp. 6808–6820, 2022,\ndoi: 10.1109/TMM.2022.3214775.\n[44] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 30, 2017, pp. 5998–6008.\n[45] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in Proc.\nEur. Conf. Comput. Vis., 2020, pp. 213–229.\n[46] N. Parmar et al., “Image transformer,” in Proc. Int. Conf. Mach. Learn. ,\n2018, pp. 4055–4064.\n[47] A. Kolesnikov et al., “Big transfer (BiT): General visual representation\nlearning,” in Proc. Eur. Conf. Comput. Vis. , 2020, pp. 491–507.\n[48] D. Mahajan et al., “Exploring the limits of weakly supervised pretrain-\ning,” in Proc. Eur. Conf. Comput. Vis. (ECCV) , 2018, pp. 181–196.\n[49] H. Touvron, A. Vedaldi, M. Douze, and H. Jégou, “Fixing the train-test\nresolution discrepancy,” in Proc. Adv. Neural Inf. Process. Syst., vol. 32,\n2019, pp. 8250–8260.\n[50] Q. Xie, M.-T. Luong, E. Hovy, and Q. V . Le, “Self-training with noisy\nstudent improves imagenet classiﬁcation,” in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. , 2020, pp. 10687–10698.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nYU et al.: END-TO-END TRANSFORMER FOR COMPRESSED VQE 11\n[51] W. Wang et al., “Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions,” in Proc. IEEE/CVF Int. Conf.\nComputer Vision, 2021, pp. 568–578.\n[52] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable DETR:\nDeformable transformers for end-to-end object detection,” in Proc. Int.\nConf. Learn. Represent. , 2020, pp. 1–26.\n[53] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n“SegFormer: Simple and efﬁcient design for semantic segmentation with\ntransformers,” in Proc. Adv. Neural Inf. Process. Syst. , vol. 34, 2021,\npp. 12077–12090.\n[54] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar,\n“Masked-attention mask transformer for universal image segmenta-\ntion,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2022,\npp. 1290–1299.\n[55] L. Zhou, Y . Zhou, J. J. Corso, R. Socher, and C. Xiong, “End-to-end\ndense video captioning with masked transformer,” in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. , 2018, pp. 8739–8748.\n[56] Y . Zeng, J. Fu, and H. Chao, “Learning joint spatial-temporal transfor-\nmations for video inpainting,” in Proc. Eur. Conf. Comput. Vis. , 2020,\npp. 528–543.\n[57] Y . Jiang, S. Chang, and Z. Wang, “TransGAN: Two transformers can\nmake one strong GAN,” 2021, arXiv:2102.07074.\n[58] H. Zhao, L. Jiang, J. Jia, P. Torr, and V . Koltun, “Point transformer,” in\nProc. IEEE/CVF Int. Conf. Comput. Vis. , 2021, pp. 16259–16268.\n[59] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu,\n“PCT: Point cloud transformer,” Comput. Vis. Media , vol. 7, no. 2,\npp. 187–199, 2021.\n[60] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M.-H. Yang,\n“Restormer: Efﬁcient transformer for high-resolution image restora-\ntion,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2022,\npp. 5728–5739.\n[61] P. Charbonnier, L. Blanc-Feraud, G. Aubert, and M. Barlaud, “Two\ndeterministic half-quadratic regularization algorithms for computed\nimaging,” in Proc. 1st Int. Conf. Image Process. , vol. 2, 1994,\npp. 168–172.\n[62] J. Liu, M. Zhou, and M. Xiao, “Deformable convolution dense network\nfor compressed video quality enhancement,” in Proc. IEEE Int. Conf.\nAcoust. Speech Signal Process. (ICASSP) , 2022, pp. 1930–1934.\n[63] Z. Huang, J. Sun, and X. Guo, “FastCNN: Towards fast and accurate\nspatiotemporal network for HEVC compressed video enhancement,”\nACM Trans. Multimedia Comput. Commun. Appl. , vol. 19, no. 3,\npp. 1–22, 2023.\n[64] F. Bossen, “Common test conditions and software reference conﬁgura-\ntions,” JCT-VC, Geneva, Switzerland, JCTVC-L1100, 2013.\n[65] C. Montgomery et al. “Xiph.org video test media (derf’s collec-\ntion), the xiph open source community.” 1994. [Online]. Available:\nhttps://media.xiph.org/video/derf\n[66] “VQEG video datasets and organizations.” Accessed: Mar. 1, 2021.\n[Online]. Available: https://www.its.bldrdoc.gov/vqeg/video-datasets-\nand- organizations.aspx\nLi Yu received the B.S. degree from Soochow\nUniversity, Suzhou, China, in 2012, and the\nPh.D. degree in electrical engineering and electron-\nics from the University of Liverpool, Liverpool,\nU.K., in 2017. From 2017 to 2018, she was\na Postdoctoral Researcher with the Department\nof Signal Processing, Tampere University of\nTechnology, Tampere, Finland. Since 2018, she has\nbeen a Faculty Member with the Nanjing University\nof Information Science and Technology, Nanjing,\nChina. Her research interests include video stream-\ning, video coding, image and video processing, computer vision, and deep\nlearning.\nWenshuai Chang received the B.S. and M.S.\ndegrees from the School of Software, Nanjing\nUniversity of Information Science and Technology\nin 2020 and 2023, respectively. His research interests\nprimarily focus on video coding, computer vision,\nand deep learning.\nShiyu Wu is currently pursuing the master’s\ndegree with the School of Software, Nanjing\nUniversity of Information Science and Technology,\nNanjing, China. His research interests include video\nrestoration, semi-supervised learning, and machine\nlearning.\nMoncef Gabbouj (Fellow, IEEE) is a Professor\nof Information Technology with the Department of\nComputing Sciences, Tampere University, Finland.\nHe was an Academy of Finland Professor. His\nresearch interests include big data analytics,\nmultimedia analysis, artiﬁcial intelligence, machine\nlearning, pattern recognition, nonlinear signal pro-\ncessing, video processing, and coding. He is the\nFinland Site Director of the NSF IUCRC funded\nCenter for Big Learning. He is a Fellow of the\nAsia–Paciﬁc Artiﬁcial Intelligence Association. He\nis member of the Academia Europaea, the Finnish Academy of Science and\nLetters, and the Finnish Academy of Engineering Sciences.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. "
}