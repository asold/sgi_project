{
    "title": "Assessing Robustness of ML-Based Program Analysis Tools using Metamorphic Program Transformations",
    "url": "https://openalex.org/W3199249334",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2982563198",
            "name": "Leonhard Applis",
            "affiliations": [
                "Delft University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A290562820",
            "name": "Annibale Panichella",
            "affiliations": [
                "Delft University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2155442793",
            "name": "Arie van Deursen",
            "affiliations": [
                "Delft University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2982563198",
            "name": "Leonhard Applis",
            "affiliations": [
                "Delft University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A290562820",
            "name": "Annibale Panichella",
            "affiliations": [
                "Delft University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2155442793",
            "name": "Arie van Deursen",
            "affiliations": [
                "Delft University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6631148570",
        "https://openalex.org/W2041650849",
        "https://openalex.org/W6713374835",
        "https://openalex.org/W3174220201",
        "https://openalex.org/W3164087770",
        "https://openalex.org/W2888328667",
        "https://openalex.org/W3186179984",
        "https://openalex.org/W2324595780",
        "https://openalex.org/W2583471401",
        "https://openalex.org/W6778782612",
        "https://openalex.org/W1974758710",
        "https://openalex.org/W2806377938",
        "https://openalex.org/W2516621648",
        "https://openalex.org/W3098605233",
        "https://openalex.org/W6765411980",
        "https://openalex.org/W1979345446",
        "https://openalex.org/W2041713059",
        "https://openalex.org/W2958754741",
        "https://openalex.org/W4288080275",
        "https://openalex.org/W6769216610",
        "https://openalex.org/W6898505805",
        "https://openalex.org/W6768003788",
        "https://openalex.org/W4229977739",
        "https://openalex.org/W1857789879",
        "https://openalex.org/W3007157104",
        "https://openalex.org/W2405597963",
        "https://openalex.org/W3109966548",
        "https://openalex.org/W1519683776",
        "https://openalex.org/W3031175349",
        "https://openalex.org/W4394638297",
        "https://openalex.org/W3099640733"
    ],
    "abstract": "Metamorphic testing is a well-established testing technique that has been successfully applied in various domains, including testing deep learning models to assess their robustness against data noise or malicious input. Currently, metamorphic testing approaches for machine learning (ML) models focused on image processing and object recognition tasks. Hence, these approaches cannot be applied to ML targeting program analysis tasks. In this paper, we extend metamorphic testing approaches for ML models targeting software programs. We present LAMPION, a novel testing framework that applies (semantics preserving) metamorphic transformations on the test datasets. LAMPION produces new code snippets equivalent to the original test set but different in their identifiers or syntactic structure. We evaluate LAMPION against CodeBERT, a state-of-the-art ML model for Code-To-Text tasks that creates Javadoc summaries for given Java methods. Our results show that simple transformations significantly impact the target model behavior, providing additional information on the models reasoning apart from the classic performance metric.",
    "full_text": "Assessing Robustness of ML-Based Program\nAnalysis Tools using Metamorphic Program\nTransformations\nLeonhard Applis, Annibale Panichella, Arie van Deursen\nTechnische Universiteit Delft, Netherlands\n{L.H.Applis, A.Panichella, Arie.vanDeursen}@tudelft.nl\nAbstract—Metamorphic testing is a well-established testing\ntechnique that has been successfully applied in various domains,\nincluding testing deep learning models to assess their robustness\nagainst data noise or malicious input. Currently, metamorphic\ntesting approaches for machine learning (ML) models focused\non image processing and object recognition tasks. Hence, these\napproaches cannot be applied to ML targeting program analysis\ntasks. In this paper, we extend metamorphic testing approaches for\nML models targeting software programs. We present LAMPION ,a\nnovel testing framework that applies (semantics preserving) meta-\nmorphic transformations on the test datasets. LAMPION produces\nnew code snippets equivalent to the original test set but different\nin their identiﬁers or syntactic structure. We evaluate LAMPION\nagainst CodeBERT, a state-of-the-art ML model for Code-To-Text\ntasks that creates Javadoc summaries for given Java methods.\nOur results show that simple transformations signiﬁcantly impact\nthe target model behavior, providing additional information on\nthe models reasoning apart from the classic performance metric.\nI. I NTRODUCTION\nArtiﬁcial Intelligence (AI) has been applied to software en-\ngineering (SE) to address many tasks, such as fault localization\n[1], test-case generation [2], fuzzing [3] or optimizing meta-\nparameters [4]. Recently, modern sequence-to-sequence deep\nlearning models have shown promising results sparking new\ntypes of applications. Among them is the creation of code\nfrom verbatim description (tex-to-code)[ 5], or generation of\ndocumentation for source-code of previously unseen methods\n(code-to-text)[ 6, 7]. Y et, we argue thatit is not clear the extent\nto which these models truly behave as intended, apart from\ntheir reported accuracy. Hence, applying testing strategies for\nML-based program analysis solutions is critical.\nIn recent years, there has been great interest in Testing ML,\nwhere the goal is indeed to go beyond assessing accuracy (see\nthe survey by Zhang et al. [8]). Many of the approaches have\nbeen taken from classic software testing and have been adapted\nfor ML. One example is metamorphic testing, which is a well-\nestablished technique that is considered a powerful approach\nas it addresses the Oracle Problem [9] in test generation.\nMetamorphic testing has been successfully used in ML [10, 11]\nfor image processing and object recognition. For example,\nimage rotation is an information-preserving transformation as it\nalters the pixels in the image without changing its label (oracle).\nIn computer vision, a robust ML model must not provide\ndifferent predictions for the image altered with metamorphic\ntransformations. Hence, quantifying the number of transformed\nimages on which an ML model provides different answers\nquantiﬁes its robustness against different transformations.\nWhile extensive research has been conducted on meta-\nmorphic testing for vision computing tasks [ 10–12], the\nexisting metamorphic transformations are domain-speciﬁc.\nConsequently, they cannot be applied and do not hold for\ndifferent domains and types of data. In this paper, weextend\nthe concept of metamorphic testing to machine learning models\ntrained on and targeting source code.\nWe deﬁne a set of transformations that alter features of code\nbut yield the effectively equal program, such as introducing\nif(true)-conditions or +0 behind integer expressions. Using\nthose, we modify the test-datapoints (programs) in order to\ndetect differences in the models’ predictions and metrics. We\nexpect that the models are robust towards some transformations\nwhile others affect the metrics (negatively). The information\ngained could help to evaluate existing models, compare them to\neach other and provide suggestions and warnings for end-users\nand researchers alike. With our research and tool, we contribute\non the following points:\n1) Create a systematic approach, namely L AMPION ,t o\nquantify the robustness of a source-code-based model\n2) Enable researchers to compare the robustness of models\nsimilarly to existing quality metrics\n3) Groundwork for data augmentation in the ﬁeld of ML4SE.\n4) Empirically show the importance of robustness and testing\nwhen referring to ML-based program analysis.\nTo the best of our knowledge, we are the ﬁrst to propose\nthe use of metamorphic transformations for assessing ML-\nbased program analysis tools. Our initial experiments on\nCodeBERT [5], a state-of-the-art ML model widely used in\nthe SE literature [13–16], demonstrate the feasibility of the\napproach, and the type of lessons that can be learned from\napplying LAMPION .\nII. B ACKGROUND AND RELA TED WORK\nMetamorphic testing is a technique based upon the concept\nof metamorphic relations, which is a property-based technique\nthat exploits known equality of certain output values. Prominent\nexamples are programs that implement mathematical functions;\nThe sine function has a well-known metamorphic relation:\n∀x ∈ R : sin(x)= sin(x +2 π). Testers can easily create\nnew test cases based on this relation and assess the program\n1377\n2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)\nWork licensed under Creative Commons Attribution 4.0 License. https://creativecommons.org/licenses/by/4.0/\nDOI 10.1109/ASE51524.2021.00172\n2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678706\ncorrectness. A broad view of metamorphic testing studies\nand applications can be found in the survey by Segura et\nal. [\n17]. While metamorphic testing has not been applied to\nML models for SE, metamorphic transformations and relations\nare known in software engineering and are tightly coupled to\nrefactoring, program optimization, and linting. Metamorphic\ntransformations are also used for compiler optimization to\ncreate more efﬁcient code, using techniques like loop unrolling\nor function inlining [18].\nMetamorphic Testing for ML. Metamorphic testing has\nbeen applied recently to machine learning, especially to\nimage-based object-detection tasks [11][10]. A metamorphic\ntransformation on images performs information-preserving\nalternations on an image. For example, the image of a cat might\nbe mirrored, yet a classiﬁer should still be able to recognize\nit as such. Blurring or saturating of images [19] change the\ndata signiﬁcantly; nevertheless, they are still easily classiﬁable\nby humans. These transformed images can be used to access\nrobustness by generating more datapoints in the test set [11].\nIt can also be applied to generate more training data, which\ncan result in a more robust or precise model [12].\nThe existing literature focuses on MTs that are speciﬁc to\nimages and pixels. In this paper, we transplant the testing\nmethodology to a new domain, namely ML models designed\nfor program analysis. This requires deﬁning new metamorphic\nrelations and transformations for our domain, which we\ndescribe in Section III.\nAdversarial attacks. Related work stems from Compton\net al. [ 20] that introduces randomization of variable-names\nin the training dataset of a code2vec model for training data\naugmentation. Their study shows that the model trained on the\naugmented training dataset achieves slightly better accuracy\nthan the model trained on the original dataset which motivates\nto systematically investigate for overﬁtting. Similarly, Y efet\net al. [\n21] prove that they can generate adversarial attacks\non Code2V ec-based classiﬁers by changing variable names or\nintroducing new variables. As this existing research motivates\nto inspect identiﬁer names, we include them into our approach\nin addition to other transformations.\nIII. O UR FRAMEWORK :L AMPION\nOverview. Figure 1 depicts the metamorphic testing ap-\nproach, we named L AMPION , and designed for testing ML\nmodels trained on source-code programs. L AMPION relies\non the MTs deﬁned in the subsections below. Our approach\nconsists of three main steps. First, LAMPION takes as inputs a\npre-trained model and a program not used during the training\nprocess (items 5 and 1 in Figure 1). It generates program\nvariants (item 4 ) by using the MTs (item 2 ) and based on a\ngiven conﬁguration ﬁle (item 3 ). The conﬁguration speciﬁes\nthe type of transformation applied and the number of repetitions\n(order). Then, the original program and its equivalent variants\nare fed to the pre-trained model. Finally, LAMPION compares\nthe outcome produced by the pre-trained model for the original\nprogram (item 6 ) and its metamorphic variants (item 7 ). If\n\u0005\u000b\u0017\b\u0011\u0013\u0015\u0014\u000e\u000f\t\n\u0007\u0015\b\u0012\u0016\f\u0013\u0015\u0011\u000b\u0015\n\u0003\u0013\u0012\f\u000f\r\u0018\u0015\b\u0017\u000f\u0013\u0012\n\u0002\u0001\n\u0002\r\u0006\u0007\n\u0006\u0015\u000f\r\u000f\u0012\b\u0010\n\u0006\u0018\u0017\t\u0013\u0011\u000b\n\u0004\u0019\b\u0010\u0018\b\u0017\u000f\u0013\u0012\n\u0002\u0010\u0017\u000b\u0015\u000b\n\u0001\n\u0006\u0018\u0017\t\u0013\u0011\u000b\n\u0013\n\u0014\n\u0016\n\u0017 \u0018\n\u0019\n\u001a\n\u0015\n\u0003\u000e\r\b\u000e\u0005\u000b\n\u0003\u000e\r\b\u000e\u0005\u000b\n\u0004\u0005\u000e\t\u0005\f\u0010\u000f\nFigure 1. L AMPION — Metamorphic Testing Framework for ML-based\nProgram Analysis\nthere is no difference in the outcome, it means that the model\nis robust to the MT. Otherwise, we found a weakness in the\nmodel.\nMetamorphic Relation for Programs . The ﬁrst step is\nto identify metamorphic relations for software programs,\nwhich are the data points for ML-based program analysis.\nMetamorphic relations (MRs) relate multiple programs that\ndiffer in their structures (e.g., AST) but that are effectively\nequivalent. As such, ML models should provide the very same\noutput (e.g., same label) for programs that are related to one\nother according to an MR. Therefore, given a program P,w e\nuse MRs to generate equivalent yet different programsP′\n1, ... ,\nP′\nk to test a given ML model under analysis.\nIn ML applications, the oracle function corresponds to\nthe labels that humans provide for a given program P. The\ntype of label for each program (data point) is task-dependent.\nFor example, in ML-based program documentation, the label\n(oracle) is the natural language description developers write\nfor the program P.\nWe identify two types of metamorphic relations for programs\nwhich are useful to test ML models for program analysis:\nMR-1: Addition of uninformative code elements . Such a\ncode element (e.g., comments, un-used variables, etc.) does\nnot change the behavior of the target program\nP. As such,\nthe label (oracle) for P and its variants with MR-1 relation\nremains the same.\nMR-2: Replace a code element with another equivalent\nelement. Equivalent program elements (e.g., different variable\nnames) do not change the AST of the programs but the labels\nof the nodes within the AST. Using different yet equivalent\nelements does not change the behaviors of a programP either.\nMetamorphic Transformations. Given the two MRs deﬁned\nabove, we can deﬁne a set of metamorphic transformations\nthat satisfy our MRs. A metamorphic transformation (MT) is\na procedure that generates new programs P′\n1, ... , P′\nk (follow-\nup programs) starting from an input program P and using a\nmetamorphic relation. We have two constraints for MTs: First,\nthe oracle function must give the same output for the initial\nprogram P and the transformed program f(P). Second, P is\na valid input for the ML model, thenf(P) must be valid input\nfor the model too.\n1378\nTable I\nOVERVIEW OF MET AMORPHIC TRANSFORMA TIONS FOR PROGRAMS\nTransformation Short Description Estimated Effect V ariations\nif-true MT-IF Wrapping a random expression in an\nif(true) statement\nStructural Changes, introduction of con-\nditions, introduction of keywords\nif-false-else\nadd-unused-\nvariable\nMT-UV Add a random unused variable Introduction of names, introduction of\ntypes\nFull random and pseudo random names\n(Postﬁx R & P), names looked up from\na dictionary or the program under test\nrename-entity MT-RE Rename a class, method or variable Introduction of names, removal of\nknown names\nFor variables, classes and member-types\nseparate\nlambda-identity MT-ID Wrap an expression in an identity-lambda\nfunction (including function call)\nIntroduction of complex structure, intro-\nduction of operators\n-\ndelegation-method MT-\nDM\nextract an expression to a function, invoke\nthe function instead of the method\nStructural changes, change of scope for\ninformation, introduction of names\nsame as MT -UV\ncomment-\nalternation\nMT-CO Add,remove or move comments Introduction or removal of natural lan-\nguage\nFull or pseudo random comments gen-\nerated\nparameter-\nintroduction\nMT-PI Introduce an unused parameter Change of method signature, introduc-\ntion of names, introduction of types\nsame as MT -UV\nwhitespace-\nalternation\nMT-WS Add or remove whitespace Change of code-layout -\nadd-neutral-\nelement\nMT-NE Add the neutral element to a primitively\ntyped expression\nChange of structure, introduction of\ntokens\nComplex equivalent transformations\n(e.g. replacing true with 01 == 1)\nA summary of MTs is presented in Table I. They target\nvarious features of the code, such as structure, tokens, and\nidentiﬁer names. Different models are known to have constraints\nby their design. For example, Code2V ec deﬁnes an AST -depth;\nhence, the model is known to break when introducing many\nredundant structure elements. Other models — especially deep\nlearning models like CodeBERT — do not specify the features\nthey target and were not previously inspected.\nThe presented table can be considered a starting point for\nmetamorphic transformations applied to ML-based program\nanalysis solutions.\nIV . EMPIRICAL STUDY\nWe ﬁrst want to assess whether the proposed MTs impact\nthe performance of machine learning models. In an ideal\ncase, ML models should not be affected by the metamorphic\ntransformations, i.e., the model is not sensitive to changes that\ndo not alter the code behavior. Hence, RQ1 should cover the\ngeneral impact of applying one single transformation at the\ntime, hereafter referred to as ﬁrst-order MTs:\nResearch Question 1\nTo what extent do metamorphic transformations affect the\nperformance of ML models?\nWe also want to compare the different types of transfor-\nmations w.r.t the benchmark. We may expect that different\ntransformations have different impacts on ML models. Further-\nmore, we aim to understand which model features are more\nrobust, e.g., whether name-changes affect the modelmore than\nstructural AST changes.\nResearch Question 2\nTo what extent do different types of MTs have a different\nimpact on the performance of ML models?\nBenchmark. For an initial study, we picked CodeBERT [5],\nparticularly its downstream task of code-summarization. Code-\nsummarization should clarify what the model understands, and\nthe output can give clearer insights than cold metrics.W e\ntrained a CodeBERT -Java Model as described in the ofﬁcial\nrepository by Microsoft [\n22], using the standard parameters\ngiven in the readme. CodeBERT has been trained on 6\nprogramming languages with a total of 8.3M datapoints (code\nsnippets) and achieves state-of-the-art results of an average\nBLEU4-Score of 17.65 in the CodeSearchNet-challenge [23].\nMethodology / Experiment Design . We developed a\nmetamorphic transformer for Java-Programs that works at the\nsource-code level. In addition, we need a (pretrained) model\nand an existing benchmark that either consists of .java ﬁles or\nprovides sufﬁcient pre- and post-processing to transform the\ndatapoints. To answer RQ1, we apply MTs to all datapoints\nin the test set, resulting in a set of variant-code-snippets. We\nthen re-calculate the performance metrics for the variant-code\nsnippets (metamorphic test cases) as well as for the original\nones. We use the BLEU4-Score [24] as performance metric,\nwhich is the standard metric used in code-to-text and text-to-\ncode generation tasks [16, 24]. The BLEU4-score is computed\nby tokenizing the gold standard and the generated text into\nn-grams, and comparing the resulting sets of n-grams. The\nmetric value ranges from 1.0 (perfect translation) to 0 (not\na single matching word or n-gram). In addition, we use the\nJaccard-distance to measure the percentage of words that differ\nbetween the two Java-doc-comments generated by CodeBERT\nbefore and after applying an MT. To assess the signiﬁcance\nof the differences in BLEU4-score achieved by the model\nwith and without metamorphic transformations, we use the\nWilcoxon rank-sum test [25]. We veriﬁed beforehand that the\nachieved results follow a non-normal distribution by applying\nthe Shapiro-Wilkinson test [26]. We answer RQ2by grouping\nthe existing results by type of MT. On the MT -groupings\nwe use the Friedman test [ 27] and the post-hoc Nemenyi\ntest [28]. The Friedman test tests for signiﬁcant differences\namong the different MTs in terms of their impact on the\nBLEU4-score. Furthermore, we use the post-hoc Nemenyi test\n1379\nFigure 2. Overview of changes for ﬁrst-order MTs\nto perform a pairwise comparison. The Nemenyi test measures\nthe difference across the MTs by computing the average rank\nof each treatment across all datapoints in the test set, where\nthe lowest rank is the most signiﬁcant.\nV. P RELIMINARY RESUL TS\nResults for RQ1. Figure 2 shows the histogram of deltas\nin the BLEU4-Score produced by CodeBERT before and after\napplying MTs. The Figure also shows the histogram of Jaccard\ndistance between the reference and the post-MT generated\nJavaDoc. We observe that out of 72,989 produced JavaDoc\nsummaries, for 16,566 datapoints CodeBERT generated sum-\nmaries with a non-zero delta in BLEU-Score (22.6%). For\nthese 16,566 datapoints with changes, the average difference\nin BLEU-Score is 0.06. Many summaries change when we\napply the MTs, but they perform comparably in terms of\nBLEU-Scores to the unaltered; For example, both summaries\ncould miss the same number of keywords, just by different\ntokens. More in detail, there are 52,838 Java methods in the\ntest set out of 72,989 with zero Jaccard distance. This results in\n20,151 snippets that do not pass the metamorphic tests (27.6%).\nFinally, the Wilcoxon rank-sum test revealed that there is a\nstatistically signiﬁcant (p-value<0.01) difference in the BLEU-\nScores achieved by CodeBERT for the code snippets with\nchanges between pre- and post-transformations.\nResults for RQ2. We applied the Friedman test and the post-\nhoc Nemenyi procedure to analyze the impact of the different\nMTs on the BLEU-Score. With a p-value<0.01, the Friedman\ntest indicates a statistical difference across the different types of\nMTs. The results of the post-hoc Nemenyi test are reported in\nFigure 3. From the ranking, we can see that the most impactful\ntransformations are MT -UVR and MT -UVP , while MT -IF and\nMT -NE are the least impactful on the BLEU-Score. In terms\nof signiﬁcance, we can conclude that MT -UVP is statistically\nmore impactful than the other MTs.\nVI. D ISCUSSION\nWe presented an effective approach for testing the robustness\nof a model towards metamorphic transformations on source\ncode. According to the empirical results, our approach is\ncapable of producing signiﬁcant changes in the summaries\ngenerated by CodeBERT, highlighting potential weaknesses\nin the model as it does not satisfy metamorphic relations. In\nFriedman: 0.000 (Ha: Different) \n Critical distance: 0.088\nMean ranks\nMTíUVP í 3.93\nMTíUVR í 3.97\nMTíRER + MTíUVR í 3.98\nMTíREP + MTíUVP í 3.99\nMTíIF + MTíNE í 4.03\nMTíNE í 4.04\nMTíIF í 4.05\n3.90 3.95 4.00 4.05 4.10\nFigure 3. Results of the Friedman test and Nemenyi post-hoc procedure for\ndifferent MTs.\nother words, slightly different variants of the same program can\nlead to signiﬁcantly different results. While in this paper we\nfocus on the Code-To-Text tasks of CodeBERT, we expect the\nfound implications to hold true for other down-stream tasks as\nwell. This can be considered a call-to-arms for researchers and\npractitioners to test machine learning models trained on source\ncode using metamorphic testing in addition to the traditional\nperformance metric (e.g., accuracy). We envision that a handful\nrobustness-criteria are deﬁned for the next generation of ML-\nbased program analysis solutions, documented and tested using\nmetamorphic transformations. We encourage reviewers of future\nresearch to perform sanity checks on newly published models\nand add robustness as a mandatory attribute of being SOTA.\nLAMPION can also be used to increase the size of the test-set\nby generating new program variants, without requiring human\nlabeling. This could be potentially beneﬁcial for SE tasks where\nlabeling data is very expensive.\nVII. C ONCLUSION\nThis paper introduces metamorphic relations to test ML-\nmodels program analysis solutions. Using this technology, our\nobjective is to gain further information on the model’s behavior\napart from the performance metric (e.g., accuracy). To achieve\nthis, we presented a generic approach (LAMPION ) and applied\nit in a case study on CodeBERT’s Code-To-Text tasks. To\nevaluate the case study, we perform various statistical tests to\nprove or disprove changes in the resulting performance metric.\nOur approach and framework can empower experts and lay-\nmen alike to assess the robustness of their models and provide\nadditional tests on quality. We tried to keep the approach 1\nlightweight in concept, 2 expendable in implementation (due\nto plug-in MTs),\n3 independent of the task (any language and quality metric).\nWhile our initial implementation is in Java, we expect that\na re-implementation for any language is an easy task and the\nstatistical analysis can be reused for most experiments.\nVIII. O\nNLINE RESOURCES\nThe code for a sample metamorphic transformer, the grid\nexperiment and the evaluation can be found on Github under\nthe Lampion repository1. The model,cleaned test-set and post-\ntransformation datasets can be found on SurfDrive2.\n1https://github.com/ciselab/Lampion\n2https://surfdrive.surf.nl/ﬁles/index.php/f/8713322177\n1380\nREFERENCES\n[1] X. Li, W . Li, Y . Zhang, and L. Zhang, “Deepﬂ: Integrating multiple fault\ndiagnosis dimensions for deep fault localization,” in Proceedings of the\n28th ACM SIGSOFT International Symposium on Software Testing and\nAnalysis, 2019, pp. 169–180.\n[2] S. Anand, E. K. Burke, T. Y . Chen, J. Clark, M. B. Cohen, W . Grieskamp,\nM. Harman, M. J. Harrold, P . McMinn, A. Bertolino et al. ,“ A n\norchestrated survey of methodologies for automated software test case\ngeneration,” Journal of Systems and Software, vol. 86, no. 8, pp. 1978–\n2001, 2013.\n[3] J. Li, B. Zhao, and C. Zhang, “Fuzzing: a survey,”Cybersecurity, vol. 1,\nno. 1, pp. 1–13, 2018.\n[4] S. Shaﬁq, A. Mashkoor, C. Mayr-Dorn, and A. Egyed, “Machine\nlearning for software engineering: A systematic mapping,”arXiv preprint\narXiv:2005.13299, 2020.\n[5] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,\nT. Liu, D. Jiang et al., “Codebert: A pre-trained model for programming\nand natural languages,” arXiv preprint arXiv:2002.08155, 2020.\n[6] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, “Summarizing source\ncode using a neural attention model,” in Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (V olume 1:\nLong Papers), 2016, pp. 2073–2083.\n[7] B. Li, M. Y an, X. Xia, X. Hu, G. Li, and D. Lo, DeepCommenter:\nA Deep Code Comment Generation Tool with Hybrid Lexical and\nSyntactical Information . New Y ork, NY , USA: Association for\nComputing Machinery, 2020, p. 1571–1575. [Online]. Available:\nhttps://doi.org/10.1145/3368089.3417926\n[8] J. M. Zhang, M. Harman, L. Ma, and Y . Liu, “Machine learning\ntesting: Survey, landscapes and horizons,”IEEE Transactions on Software\nEngineering, 2020.\n[9] E. T. Barr, M. Harman, P . McMinn, M. Shahbaz, and S. Y oo, “The\noracle problem in software testing: A survey,” IEEE Transactions on\nSoftware Engineering, vol. 41, no. 5, pp. 507–525, 2015.\n[10] C. Murphy, G. Kaiser, L. Hu, and L. Wu, “Properties of machine\nlearning applications for use in metamorphic testing,” 20th International\nConference on Software Engineering and Knowledge Engineering, SEKE\n2008, pp. 867–872, 2008.\n[11] X. Xie, J. W . Ho, C. Murphy, G. Kaiser, B. Xu, and T. Y . Chen, “Testing\nand validating machine learning classiﬁers by metamorphic testing,”\nJournal of Systems and Software, vol. 84, no. 4, pp. 544 – 558, 2011, the\nNinth International Conference on Quality Software. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0164121210003213\n[12] M. Sharif, S. Mohsin, M. Y . Javed, and M. A. Ali, “Single image face\nrecognition using laplacian of gaussian and discrete cosine transforms.”\nInt. Arab J. Inf. Technol., vol. 9, no. 6, pp. 562–570, 2012.\n[13] E. Mashhadi and H. Hemmati, “Applying codebert for automated program\nrepair of java simple bugs,” arXiv preprint arXiv:2103.11626, 2021.\n[14] C. Pan, M. Lu, and B. Xu, “An empirical study on software defect\nprediction using codebert model,” Applied Sciences, vol. 11, no. 11, p.\n4793, 2021.\n[15] M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, and\nD. Poshyvanyk, “An empirical investigation into learning bug-ﬁxing\npatches in the wild via neural machine translation,” inProceedings of\nthe 33rd ACM/IEEE International Conference on Automated Software\nEngineering, 2018, pp. 832–837.\n[16] T.-H. Jung, “Commitbert: Commit message generation using pre-trained\nprogramming language model,” arXiv preprint arXiv:2105.14242, 2021.\n[17] S. Segura, G. Fraser, A. B. Sanchez, and A. Ruiz-Cortés, “A survey\non metamorphic testing,” IEEE Transactions on software engineering,\nvol. 42, no. 9, pp. 805–824, 2016.\n[18] K. Cooper and L. Torczon, Engineering a compiler. Elsevier, 2011.\n[19] C. N. V asconcelos, A. Paes, and A. Montenegro, “Towards deep\nlearning invariant pedestrian detection by data enrichment,” in2016 15th\nIEEE International Conference on Machine Learning and Applications\n(ICMLA), 2016, pp. 837–841.\n[20] R. Compton, E. Frank, P . Patros, and A. Koay, “Embedding java classes\nwith code2vec,” Proceedings of the 17th International Conference\non Mining Software Repositories , Jun 2020. [Online]. Available:\nhttp://dx.doi.org/10.1145/3379597.3387445\n[21] N. Y efet, U. Alon, and E. Y ahav, “Adversarial examples for models of\ncode,” arXiv preprint arXiv:1910.07517, 2019.\n[22] “Codexglue: A benchmark dataset and open challenge for code intelli-\ngence,” 2020.\n[23] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\n“Codesearchnet challenge: Evaluating the state of semantic code search,”\narXiv preprint arXiv:1909.09436, 2019.\n[24] K. Papineni, S. Roukos, T. Ward, and W .-J. Zhu, “Bleu: a method for\nautomatic evaluation of machine translation,” in Proceedings of the 40th\nannual meeting of the Association for Computational Linguistics, 2002,\npp. 311–318.\n[25] F. Wilcoxon, “Individual comparisons by ranking methods,” inBreak-\nthroughs in statistics. Springer, 1992, pp. 196–202.\n[26] S. S. Shapiro and M. B. Wilk, “An analysis of variance test for normality\n(complete samples),” Biometrika, vol. 52, no. 3/4, pp. 591–611, 1965.\n[27] M. Friedman, “The use of ranks to avoid the assumption of normality\nimplicit in the analysis of variance,” Journal of the american statistical\nassociation, vol. 32, no. 200, pp. 675–701, 1937.\n[28] P . Nemenyi, “Distribution-free mulitple comparisons phd thesis princeton\nuniversity princeton,” 1963.\n1381"
}