{
  "title": "An Algorithm‚ÄìHardware Co-Optimized Framework for Accelerating N:M Sparse Transformers",
  "url": "https://openalex.org/W4291653336",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1977036178",
      "name": "Chao Fang",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2592263814",
      "name": "Aojun Zhou",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2145204843",
      "name": "Zhongfeng Wang",
      "affiliations": [
        "Nanjing University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2998135987",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6780125659",
    "https://openalex.org/W3207622241",
    "https://openalex.org/W6789946416",
    "https://openalex.org/W6803607599",
    "https://openalex.org/W6794457956",
    "https://openalex.org/W3196923642",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3134012069",
    "https://openalex.org/W3118709344",
    "https://openalex.org/W3118337360",
    "https://openalex.org/W2981124343",
    "https://openalex.org/W3179312164",
    "https://openalex.org/W3158912502",
    "https://openalex.org/W3034100311",
    "https://openalex.org/W3038838661",
    "https://openalex.org/W3041223772",
    "https://openalex.org/W2946355854",
    "https://openalex.org/W2977634443",
    "https://openalex.org/W3164217046",
    "https://openalex.org/W4251575795",
    "https://openalex.org/W2906043559",
    "https://openalex.org/W4230989867",
    "https://openalex.org/W3012178976",
    "https://openalex.org/W2625457103",
    "https://openalex.org/W3047848469",
    "https://openalex.org/W3017024317",
    "https://openalex.org/W3159727696",
    "https://openalex.org/W3206453033",
    "https://openalex.org/W3162542754",
    "https://openalex.org/W6771852177",
    "https://openalex.org/W6790503700",
    "https://openalex.org/W4240168186",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W6768817161",
    "https://openalex.org/W2809679348",
    "https://openalex.org/W2490345425",
    "https://openalex.org/W3211907200",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W4287208846",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3043034704",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2995463996",
    "https://openalex.org/W4287363917",
    "https://openalex.org/W3037132819",
    "https://openalex.org/W2952682304",
    "https://openalex.org/W3127067080"
  ],
  "abstract": "The Transformer has been an indispensable staple in deep learning. However,\\nfor real-life applications, it is very challenging to deploy efficient\\nTransformers due to immense parameters and operations of models. To relieve\\nthis burden, exploiting sparsity is an effective approach to accelerate\\nTransformers. Newly emerging Ampere GPUs leverage a 2:4 sparsity pattern to\\nachieve model acceleration, while it can hardly meet the diverse algorithm and\\nhardware constraints when deploying models. By contrast, we propose an\\nalgorithm-hardware co-optimized framework to flexibly and efficiently\\naccelerate Transformers by utilizing general N:M sparsity patterns. (1) From\\nalgorithm perspective, we propose a sparsity inheritance mechanism along with\\nan inherited dynamic pruning (IDP) method to obtain a series of N:M sparse\\ncandidate Transformers rapidly. A model compression scheme is further proposed\\nto significantly reduce the storage requirement for deployment. (2) From\\nhardware perspective, we present a flexible and efficient hardware\\narchitecture, namely STA, to achieve significant speedup when deploying N:M\\nsparse Transformers. STA features not only a computing engine unifying both\\nsparse-dense and dense-dense matrix multiplications with high computational\\nefficiency but also a scalable softmax module eliminating the latency from\\nintermediate off-chip data communication. Experimental results show that\\ncompared to other methods, N:M sparse Transformers, generated using IDP,\\nachieves an average of 6.7% improvement on accuracy with high training\\nefficiency. Moreover, STA can achieve 14.47x and 11.33x speedup compared to\\nIntel i9-9900X and NVIDIA RTX 2080 Ti, respectively, and perform 2.00-19.47x\\nfaster inference than the state-of-the-art FPGA-based accelerators for\\nTransformers.\\n",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nAn Algorithm-Hardware Co-Optimized Framework\nfor Accelerating N:M Sparse Transformers\nChao Fang, Graduate Student Member, IEEE, Aojun Zhou, and Zhongfeng Wang, Fellow, IEEE\nAbstract‚ÄîThe Transformer has been an indispensable staple in\ndeep learning. However, for real-life applications, it is very chal-\nlenging to deploy efÔ¨Åcient Transformers due to immense param-\neters and operations of models. To relieve this burden, exploiting\nsparsity is an effective approach to accelerate Transformers.\nNewly emerging Ampere GPUs leverage a 2:4 sparsity pattern to\nachieve model acceleration, while it can hardly meet the diverse\nalgorithm and hardware constraints when deploying models.\nBy contrast, we propose an algorithm-hardware co-optimized\nframework to Ô¨Çexibly and efÔ¨Åciently accelerate Transformers\nby utilizing general N:M sparsity patterns. (1) From algorithm\nperspective, we propose a sparsity inheritance mechanism along\nwith inherited dynamic pruning (IDP) to obtain a series of N:M\nsparse candidate Transformers rapidly. A model compression\nscheme is further proposed to signiÔ¨Åcantly reduce the storage\nrequirement for deployment. (2) From hardware perspective, we\npresent a Ô¨Çexible and efÔ¨Åcient hardware architecture, namely\nSTA, to achieve signiÔ¨Åcant speedup when deploying N:M sparse\nTransformers. STA features not only a computing engine uni-\nfying both sparse-dense and dense-dense matrix multiplications\nwith high computational efÔ¨Åciency but also a scalable softmax\nmodule eliminating the latency from intermediate off-chip data\ncommunication. Experimental results show that compared to\nother methods, N:M sparse Transformers, generated using IDP,\nachieves an average of 6.7% improvement on accuracy with\nhigh training efÔ¨Åciency. Moreover, STA can achieve 14.47√ó\nand 11.33√óspeedup compared to Intel i9-9900X and NVIDIA\nRTX 2080 Ti, respectively, and perform 2.00 ‚àº19.47√ófaster\ninference than the state-of-the-art FPGA-based accelerators for\nTransformers.\nIndex Terms‚ÄîAlgorithm-hardware co-design, Transformer,\nhardware accelerator, pruning, model compression.\nI. I NTRODUCTION\nT\nRANSFORMER-BASED networks are a formidable\nforce in deep learning [1]. Tremendous impact in many\nÔ¨Åelds, such as neural machine translation [2], language under-\nstanding [3], and image processing [4], has been made since\nthe innovation of Transformers. Nevertheless, the impressive\nperformance of Transformers comes with heavy computing\nand memory costs, which become a signiÔ¨Åcant barrier to\nthe efÔ¨Åcient deployment of Transformer-based applications.\nNotably, BERT, a representative Transformer-based model [3],\nThis work was supported in part by the National Natural Science Foundation\nof China under Grant 62174084, 62104097 and in part by the High-Level\nPersonnel Project of Jiangsu Province under Grant JSSCBS20210034, the\nKey Research Plan of Jiangsu Province of China under Grant BE2019003-4.\n(Corresponding author: Zhongfeng Wang.)\nC. Fang and Z. Wang are with the School of Electronic Science and\nEngineering, Nanjing University, Nanjing 210008, China (e-mail: fantasy-\nsee@smail.nju.edu.cn; zfwang@nju.edu.cn).\nA. Zhou is with CUHK-Sensetime Joint Lab, CUHK, Hong Kong, China\n(e-mail: aojunzhou@gmail.com).\nrequires 440MB memory and over 176G Ô¨Çoating-point opera-\ntions. Such severe requirements on memory and computation\nmake it critical to Ô¨Ånd an efÔ¨Åcient solution for deploying\nTransformers.\nSparse Transformer\nAmpere GPUDense Transformer\nTrain Deploy\n2:4(50%)\n(a) Deploy2:4sparse Transformers\non Ampere GPU\n3:4(25%)\nSparse Transformers\n2:4(50%)\nDeploy\nDeploy\nDeploy\nTrain\nDense Transformer\n1:4(75%)\nDiverse\nFPGA\ndevices\nGeneralN:Msparsity\n(b) Deploy generalN:Msparse Transformers\non diverse FPGA devices\nFig. 1. Accelerating N:M sparse Transformer-based models (a) using modern\nAmpere GPUs and (b) using diverse FPGAs with our framework. Compared\nto (a), (b) can generate a series of N:M sparse Transformers along with the\ndedicated accelerators for efÔ¨Åcient model deployment.\nSparsity is an important feature that can be utilized to im-\nprove the efÔ¨Åciency of DNNs deployment in dedicated acceler-\nators. In the pioneering works, OPTIMUS [5] and EdgeBERT\n[6], the latest ASIC accelerators, leverage unstructured sparsity\nto realize efÔ¨Åcient deployment for Transformers. Nevertheless,\nit is hard to predict the unstructured sparsity in advance,\nand therefore the acceleration performance can be greatly\ndragged. Recent studies [7] demonstrate deep neural networks\nleveraging N:M Ô¨Åne-grained structured sparsity, where N out\nof M parameters are zeros for every continuous M parameters,\ncan achieve comparable performance over those leveraging\nunstructured sparsity [8]. However, it is signiÔ¨Åcantly restricted\nto accelerate N:M sparse networks on current hardware plat-\nforms. As shown in Fig. 1 (a), the only existing solution is\nASP with Ampere GPUs that focuses on the middle-level (2:4),\narXiv:2208.06118v1  [cs.AR]  12 Aug 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\ni.e., 50%, sparse ratio. Based on our experiments, a heavy\nTransformer can be dramatically slimmed by weight pruning\nwith the aggressive N:M pattern, e.g., 2:8 or 1:8, achieving\na considerable reduction in the amount of both parameters\nand operations. The only choice of uniform 2:4 sparsity limits\nperformance when deploying Transformer-based models, mak-\ning it inÔ¨Çexible to meet different hardware constraints (e.g.,\nlatency, energy). Compared with the uniform 2:4 sparsity,\nthe more Ô¨Çexible general N:M sparsity in real applications\ncan satisfy various algorithm and hardware constraints under\ndifferent deployment scenarios. However, there is currently a\nlack of an integrated framework to investigate the deployment\nof Transformer with general N:M sparse patterns. To bridge\nthis gap, as presented in Fig. 1 (b), we propose an algorithm-\nhardware co-optimized framework for accelerating N:M sparse\nTransformers, which addresses two signiÔ¨Åcant issues: 1). how\nto produce a series of N:M sparse Transformers in an\nefÔ¨Åcient way; 2). how to design a Ô¨Çexible and efÔ¨Åcient\ndedicated architecture for N:M sparse Transformers on\ndiverse FPGA platforms.\nAlthough advanced optimization algorithms ASP [9] and\nSR-STE [7] can maintain the middle-level ( 2:4) sparsity via\nstatic and dynamic Ô¨Åne-tuning, we observe existing methods\ndegrade the performance signiÔ¨Åcantly under high sparse ratio\n(e.g., ‚â• 75%). In addition, the ASP and SR-STE schemes\nleverage single-shot magnitude-based pruning for a speciÔ¨Åed\nhyper-parameter N and M. This traditional recipe results in a\nsigniÔ¨Åcant performance drop with a higher sparse ratio and\nrestricts the deployment of Ô¨Çexible N:M sparse models on the\nFPGA platform. To overcome the aforementioned problems,\nwe propose a sparsity inheritance mechanism, which increases\nthe sparsity progressively to enable efÔ¨Åcient searching for\nN:M sparse Transformers under various sparsity conÔ¨Ågura-\ntions (e.g., 2:8, 1:8). We also propose a pruning method,\nnamely inherited dynamic pruning (IDP), which shrinks pre-\npruning models progressively, and the convergent pre-pruning\ninitialized models can aid in the convergence of the follow-\ning sub-networks. Extensive experiments are conducted on\nTransformer-based models, showing that models generated by\nIDP with the sparsity inheritance mechanism have superior\nperformance on various sparsity ratios than those using the\nASP and SR-STE. Moreover, for efÔ¨Åcient model deployment,\nwe apply a simple but effective bitmap-based compression\nscheme, which dramatically reduce the storage requirements\nfor N:M sparse Transformers.\nTo enable Ô¨Çexible and efÔ¨Åcient deployment on various\nFPGA devices, we design a highly conÔ¨Ågurable dedicated\naccelerator for N:M sparse Transformers, namely STA. STA\nfully explores the parallelism of Transformers in three aspects,\nincluding head parallelism, row parallelism, and column paral-\nlelism, which signiÔ¨Åcantly improves computational efÔ¨Åciency.\nIt features two computing cores, a diverse matrix multiplica-\ntion (MatMul) engine, called DMME, and a scalable softmax\nmodule, both of which are highly conÔ¨Ågurable. Operations\nof N:M sparse Transformers are dominated by two types of\nMatMuls. One is the sparse-dense MatMul with N:M sparse\nnetwork parameters, and the other is dense-dense MatMul\nfree of parameters. DMME performs both sparse-dense and\ndense-dense MatMuls on-the-Ô¨Çy, and achieves much higher\ncomputational efÔ¨Åciency over the prior work [10] under both\nmodes. Especially for sparse-dense MatMul, DMME only\nperforms operations related to those remaining non-zero pa-\nrameters, which greatly improves computational efÔ¨Åciency.\nThe scalable softmax module can perform the softmax func-\ntion in Transformers. It keeps all the intermediate results\nfully local, eliminating latency from intermediate off-chip data\ncommunication. According to the given architectural settings,\nSTA can be rapidly implemented on FPGAs to realize efÔ¨Åcient\ndeployment for speciÔ¨Åc N:M sparse Transformers.\nTo summarize, the contributions of our paper are as follows:\n‚Ä¢ To our best knowledge, this is the Ô¨Årst work that\npresents an algorithm-hardware co-optimized framework\nto systematically study the efÔ¨Åciency of Ô¨Åne-grained N:M\nsparse Transformers on FPGA. The proposed framework\ncan adjust to diverse hardware constraints for Ô¨Çexible and\nefÔ¨Åcient model deployment.\n‚Ä¢ To generate a series of N:M sparse Transformers simul-\ntaneously, we propose a sparsity inheritance mechanism\nalong with the inherited dynamic pruning (IDP) algo-\nrithm, which can signiÔ¨Åcantly achieve about 6.7% ac-\ncuracy improvement of Transformers under high sparsity\ncompared with state-of-the-art methods.\n‚Ä¢ We present a simple but effective bitmap-based com-\npression scheme for N:M sparse Transformers compared\nto multiple sparse indexing formats, which dramatically\nreduces the storage requirements up to 5.33√ó.\n‚Ä¢ We propose a dedicated hardware architecture, namely\nSTA, to realize Ô¨Çexible and efÔ¨Åcient deployment of\nN:M sparse Transformers. It features two novel hardware\nmodules handling intensive operations of Transformers,\nincluding a diverse matrix multiplication engine (DMME)\nthat uniÔ¨Åes dense and sparse MatMul operations in high\ncomputational efÔ¨Åciency, and a scalable softmax module\nto avoid frequent off-chip memory accesses.\n‚Ä¢ Extensive experiments have been conducted on four NLP\ntasks and four Transformer-based models to evaluate the\neffectiveness of the proposed framework, which achieves\nup to 19.47√ó speedup over Intel i9-9900X, NVIDIA\nRTX 2080 Ti, and prior FPGA-based accelerators for\nTransformers.\nThe rest of this paper is organized as follows: Section II\npresents an overview of Transformers, and state-of-the-art\nworks for accelerating Transformers with innovations on hard-\nware architecture. Section III introduces the workÔ¨Çow of\nour proposed algorithm-hardware co-optimization framework.\nSection IV and Section V elaborate optimizations on pruning\nalgorithm and hardware architecture, respectively. Compre-\nhensive experimental results are presented in Section VI to\nshow signiÔ¨Åcant potential of our proposed co-optimization\nframework in Transformer-based applications.\nII. B ACKGROUND AND MOTIVATION\nIn this section, we provide an overview of key structures in\nTransformers, and review related work on hardware accelera-\ntors for Transformers.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nA. Transformer Overview\nThe key architectures of the Transformer [11] are char-\nacterized by a multi-head attention (MHA) residual block\n(ResBlock) and a position-wise feed-forward network (FFN)\nResBlock. Fig. 2 (a) and (b) illustrate the inner structures of\nMHA and FFN ResBlocks, respectively. The input and output\nof the FFN ResBlock are connected by a residual connector.\nAnd two linear transformation modules along with a activation\nfunction are inside the FFN ResBlock. The structure of MHA\nResBlock is more complicated. The inputs of MHA ResBlock\nare split into multiple parallel heads with corresponding linear\nprojection at Ô¨Årst. Then the results are as input fed into\nthe attention mechanism in parallel, and Ô¨Ånally, the results\nof attention heads are concatenated together and passed into\na linear layer to obtain the output linear projection. Note\nthat the attention mechanism is totally different from the\nlinear layer, performing parameter-free MatMuls. Thus, the\ncomputing engine for Transformers is required to support both\nsparse and dense MatMuls even though sparsity is introduced\nto parameters. The residual connector of FFN ResBlock is\norganized the same as the FFN ResBlock.\nLinearLinearLinearLinear Linear Linear\nMatMul\nSoftmax\nMatMul\nScale\nConcatenate\nLinear\nMHA ResBlock\nAttention\nMechanism\n(a)\nLinear\nActivation Function\nLinear\nFFN ResBlock\n(b) (c)\n2:4Sparse\nLinear Parameters\nNon-linearSparse-Dense MatMul\nwithN:Msparse parameters\nDense-Dense MatMul\nwithout parameters\nFig. 2. The operations in (a) the MHA ResBlock and (b) the FFN ResBlock\nunder N:M sparsity pattern. Both Resblocks are the key structures of the\nTransformer. (c) An illustration of 2:4 sparse parameters in a linear layer.\nB. Recent Advances for Transformer Acceleration\nExtensive research has concentrated on the design of high-\nperformance and energy-efÔ¨Åcient DNN hardware accelerators\n[12]‚Äì[28]. However, most of these works focus on CNN and\nRNN computations, and not as much scrutiny has been given\nto accelerating Transformer-based networks with self-attention\nmechanisms.\nAs a pioneer work, [10] proposed a dense systolic array\naccelerator along with the partitioning scheme for FPGA-\nbased acceleration of Transformers. Moreover, FTRANS [29]\nexploited block-circulant matrix-based weight representation\nfor Transformer acceleration. However, both of them fail to\nutilize the sparsity of parameters in Transformers, thereby\nlimiting the speedup of model deployment. A 3 [30], SpAtten\n[31], and Sanger [32] merely focused on the speedup po-\ntential for the sparse attention mechanism, all of which can\nhardly satisfy the needs of agile and efÔ¨Åcient deployment of\nTransformer models. OPTIMUS [5] and EdgeBERT [6] holis-\ntically accelerate Transformers with unstructured sparse matrix\nmultiplications and save energy by skipping the computations\nrelated to those zero-value parameters. Nevertheless, the un-\nstructured sparsity leads to irregular data access, making both\ndesigns suffer low computational efÔ¨Åciency. [33] exploited the\ncoarse-grained block-based sparsity pattern for accelerating\nTransformers, while this sparsity pattern is so coarse that the\nmodels can hardly achieve a considerable sparsity ratio with\nacceptable accuracy.\nIn summary, it is hard for all these works to achieve\nsatisfying speedup and efÔ¨Åciency of Transformer deployment\ndue to the lack of attention to model sparsity, limited sparse\npotential exploration on the whole Transformer models, or\nrestricted computational efÔ¨Åciency for sparse Transformers.\nTo address above issues, this work presents an algorithm-\nhardware co-optimization framework to realize Ô¨Çexible and\nefÔ¨Åcient deployment of Transformers by leveraging general\nN:M sparsity patterns. For algorithm optimization, we focus\non how to generate a series of N:M sparse Transformers in\nhigh quality and efÔ¨Åciency. For hardware optimization, we\nconcentrate on designing a Ô¨Çexible and efÔ¨Åcient dedicated\narchitecture that can accelerate N:M sparse Transformers with\nhigh computational efÔ¨Åciency.\nIII. O VERVIEW OF CO-OPTIMIZATION\nTo achieve agile and efÔ¨Åcient deployment of Transformers,\nwe propose an algorithm-hardware co-optimized framework.\nThe overview of our framework is presented in Fig. 3. Ac-\ncording to the given speciÔ¨Åc requirements, our framework can\nquickly obtain the required N:M sparse Transformer model\nwith high accuracy, and provide corresponding Transformer\naccelerators on FPGA devices to realize efÔ¨Åcient model de-\nployment. In this section, we elaborate on the workÔ¨Çow of\nour algorithm-hardware co-optimized framework.\nAt the algorithm level, we focus on quickly obtaining\nany desired N:M sparse Transformer model, and achieving\neffective compression of the N:M sparse Transformer. The\nalgorithm optimization is divided into two stages. As shown in\nFig. 3, the Ô¨Årst stage is IDP based on the sparsity inheritance\nmechanism. Compared with single-shot training [7], [9], our\nmethod can utilize the knowledge of the previous N:M sparse\nmodel, which contributes to faster and better convergence.\nThe second stage is model compression. Only the non-zero\nparameters in the N:M sparse Transformer would be stored,\nalong with an additional binary mask that indicates position\nof all recorded elements. The methods of pruning and model\ncompression are presented in Sec. IV.\nAt the hardware level, we concentrate on efÔ¨Åcient and Ô¨Çexi-\nble hardware architecture design that boosts the computational\nefÔ¨Åciency for N:M sparse Transformers. The hardware opti-\nmization features an efÔ¨Åcient hardware architecture for N:M\nsparse Transformers along with an automatic hardware gen-\nerator, which can meet requirements on various Transformer\nmodels, FPGA devices, and N:M sparsity. The automatic\nhardware generator is composed of instruction generator and\nhardware template library. According to the N:M conÔ¨Ågura-\ntion of the winner model and the network structure of the\ndeployment model, the instruction generator can automatically\nproduce instructions that guide STA to perform operations\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nInherited Dynamic Pruning & Model Compression\n(Algorithm)\nDense\nTransformer\nN:MSparse\nCandidate\nParameter \nUpdate\nEpochs\nEnd?\nNreached\nterminaln?\nWinner\nN:MModel\nUpdate\nN:MSparse\nWinners‚Äô Pool\nAdjustN\nfor higher sparsity ratio\nBitmask &\nNonzero\nParameters\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n0\n1\n1\n0\n0\nModel\nCompression\nMask Update\nYes\nNo Loss\nYes\nNo\nN:M Sparse Transformer Accelerator\n(Hardware)\nLoad Weight: layer X, matQ, 64x512\nLoad Activation: layer X, activation, 512x64\nSparse MatMul.: matA, matB, matC, 512, 512, 512\nDense MatMul.: matA, matB, matC, 64, 64, 64\nFused Vector Ops: Softmax/ ReLU/ ResAdd\nStore Activation: output, 64x512\nModel\nStructure\nHardware \nTemplate\nLibrary\nN:M Sparse\nTransformer Accelerator (STA)\nN:MWinner\nConfiguration\nN:MSparse\nCompact\nParameters\nInstruction\nGenerator\nHardware\nConfiguration\nFig. 3. The workÔ¨Çow of our proposed algorithm-hardware co-optimized framework. At the algorithm level, N:M sparse Transformers can be rapidly generated\nby inherited dynamic pruning, and signiÔ¨Åcantly compressed for further deployment. At the hardware level, the dedicated accelerator, STA, is implemented on\nthe FPGA platform to accelerate the deployed N:M sparse Transformer.\nof the winner N:M sparse Transformer. As shown in Fig 3,\ninstructions are divided into three categories: load/store data,\nsparse/dense MatMul operators, and fused vector operators.\nThe hardware template library can quickly generate a dedi-\ncated STA based on the pre-deÔ¨Åned hardware conÔ¨Ågurations\nand the N:M conÔ¨Åguration of the winner model. STA performs\ninference tasks for the Transformer model by accessing the\ncompact sparse parameters and the pre-generated instructions.\nSTA, whose hardware architecture is elaborated in Sec. V, can\nachieve signiÔ¨Åcant improvement on computational efÔ¨Åciency\nby eliminating all zero-valued parameter operations.\nIn real-life deployment, the choice of N:M may change\nif there are multiple FPGA devices with different hardware\nresources, and varying deployment constraints including la-\ntency and model accuracy. However, considering all the above\nfactors, once an N:M model is determined to be deployed, the\nmodel can meet the needs of practical applications. Therefore,\nthe N:M would change before deploying models while would\nnot change after the model deployment. Compared to the Am-\npere GPU dedicated for 2:4 sparse acceleration, speciÔ¨Åc N:M\nSTA can be Ô¨Çexibly conÔ¨Ågured and automatically generated on\nthe selected FPGA device with signiÔ¨Åcant performance gains\nbeneÔ¨Åted from dedicated N:M sparse acceleration. As N:M\nchanges, our framework would efÔ¨Åciently beneÔ¨Åt from the\nalgorithm-hardware co-optimization. At the algorithm level,\nthe proposed IDP could provide a series of N:M models with\nvarying computing complexity and model accuracy, among\nwhich we could select the most suitable one for further\nmodel deployment. At the hardware level, the proposed STA\ncould be Ô¨Çexibly generated based on the selected N:M and\nother conÔ¨Ågurations, achieving signiÔ¨Åcant acceleration of N:M\nTransformers.\nIV. A LGORITHM OPTIMIZATION\nIn this section, we elaborated on algorithm optimizations\nof our framework. Firstly, we demonstrate advantages of N:M\nsparsity pattern in Sec. IV-A by comparing it with other popu-\nlar sparsity patterns. Then, pruning algorithm and compression\nscheme of N:M sparse Transformers are presented in Sec. IV-B\nand Sec. IV-C, respectively.\nA. N:M Sparsity Pattern\nA dense parameter matrix, can be pruned with a sparsity\nratio of 50% using three existing sparsity patterns, unstructured\nsparsity [5], block-based structured sparsity [33], and N:M\ngroup-based structured sparsity [7], respectively. Table I sum-\nmarizes all these pruning patterns. Elements in any position of\nthe parameter matrix can be pruned if the unstructured spar-\nsity pattern is employed. The unstructured sparse model can\nachieve a considerable compression ratio while maintaining\ncomparable accuracy to the dense model. However, there is a\nlimited speedup of the unstructured sparse model on hardware\n[5], [6] due to the irregular pattern. For block-based pruning,\nthe parameter matrix is Ô¨Årstly divided into multiple blocks, and\nthen some unimportant blocks was dropped to reduce storage\nand computing. The block-based sparse model having regular\npattern can achieve high computational efÔ¨Åciency on hardware.\nNevertheless, the speedup of block-based sparse models [33]\nis inefÔ¨Åcient since there is limited compression ratio using\nthe block-based pattern. As for N:M group-based structured\nsparsity, the parameter matrix is divided into multiple groups.\nHere we consider consecutive column-wise elements in the\nmatrix gather as a group. Each group has M elements and\ncontains N nonzero elements at most. The N:M sparsity\ncan achieve high compression ratio along with computational\nefÔ¨Åciency on hardware due to its Ô¨Åned-grained regular pattern.\nHence, Transformers with N:M sparsity pattern, which remains\na lot to be explored, have much more speedup potential than\nthat with unstructured and block-based sparsity.\nB. Pruning Algorithm\nGiven a pretrained dense Transformer model, generally, a\nN:M sparse Transformer can be trained with the objective as\nmin\nS(W,N,M)\nL(W; D), (1)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nTABLE I\nCOMPARISON BETWEEN EXISTING THREE SPARSITY PATTERNS\nUnstructured Block-based N:M Group-based\nVisualization\nAccuracy High‚úì Medium High‚úì\nEfÔ¨Åciency Low High‚úì High‚úì\nSpeedup Medium High‚úì High‚úì\nwhere D denotes the observed data, L represents the loss\nfuntion, Windicates the parameters of the Transformer, and\nS(W,N,M ) is the sparse Transformer with N:M sparsity\npattern. N is the number of non-zero values. For dense model\nW, it can be equivalent to S(W,N = M,M ).\nExisting methods NVIDIA ASP [9] and SR-STE [7] lever-\nage the single-shot magnitude-based pruning and dynamic\nsparse training from dense models Wrespectively. The spe-\nciÔ¨Åc sparse models S(W,N,M ) inherit from global dense\nmodels S(W,M,M ) with pre-trained weights and random\ninitialization in ASP and SR-STE. This may lead to suboptmial\nproblems, and we observe the ASP and SR-STE hurt the\nperformance signiÔ¨Åcantly on Transformer-based models with\nthe higher sparse ratio (e.g., ‚â•75%). In addition, the ASP and\nSR-STE undesirably require intensive training computation if\nwe have different hardware constraints with multiple sparsity\nlevels (e.g., 1:8, 2:8, 3:8 and 4:8).\nTherefore, we propose a general and simple algorithm for\ngenerating models with general N:M sparse patterns, namely\nIDP, which can produce a series of sparse models with\ndifferent N:M conÔ¨Ågurations. Algorithm 1 presents the detail\nof IDP. To handle the optimization difÔ¨Åculty of the sparse sub-\nnetworks inherited from large dense models, we introduce a\nnovel co-training scheme, which optimize different multiple-\nlevel N:M sparse models simultaneously (e.g., 1:8, 2:8, 3:8,\n4:8 and 5:8). During the training phase, we gradually reduce\nthe non-zeros parameters N, which can guarantee the super\nmodels converge well. We can give the general inheritance\nmechanism of the IDP as follows:\nS(W, N1, M) ‚ÜêS(W, N2, M) ‚Üê¬∑¬∑¬∑‚Üê S(W, M, M), (2)\nwhere S(W,Ni,M) are N:M sparse models, and the\nS(W,M,M ) represents the dense model, where N1 <N2 <\n¬∑¬∑¬∑ < M, ‚Üê means the smaller model S(W,N ‚àí1,M)\nprune from S(W,N,M ), named inheritance mechanism . It\nrequires merely a hyper-parameter n denoted as the end of\niterations of N. With the novel inheritance mechanism, our\nIDP training method can be summarized with four steps:\n‚Ä¢ Step 1: Initialize N = M‚àí1 and set the dense pretrained\nmodel as the Ô¨Årst winner model.\n‚Ä¢ Step 2: Sparsity inheritance applies the kept parameters\nof the winner of all the S(W,N + 1,M) candidates to\ninitialize following sparse model S(W,N,M ).\n‚Ä¢ Step 3: Sparse training for N:M sparse candidates in\nseveral epochs. Parameters are adjusted in every epoch by\nupdating the mask based on the their magnitude. This step\ngenerates a new winner model, which is the convergent\nmodel at the last epoch.\n‚Ä¢ Step 4: If N = n, the whole process is Ô¨Ånished, or\notherwise N ‚ÜêN ‚àí1, and then go to Step 2 .\nWe expect to obtain M-N+1 preserved winner models with\ndifferent N:M sparsity for subsequent deployment. Addition-\nally, in the forward pass, we leverage the popular group-wise\nmagnitude pruning [7], [9]. Parameter matrices are partitioned\ninto multiple groups, every one of which contains M con-\nsecutive column-wise elements, as shown in Table I. And we\nkeep the N-largest parameters in these groups and generate\ncorresponding masks B ‚àà {0,1}d. SpeciÔ¨Åcally, if the i-th\nparameter of Wsurvived in the pruned sub-network, we set\nBi = 1, or else Bi = 0. In the backward pass, recent studies\n[7], [34] demonstrate that the dynamic sparse training can\nbeneÔ¨Åt both model convergence and accuracy, and we follow\ntheir methods to calculate gradients.\nAlgorithm 1 Inherited Dynamic Pruning\nInput: Pre-trained dense weights W, datasets D, initial learn-\ning rate Œ≥0 and the end of iterations n.\n1: for N = M ‚àí1,M ‚àí2,..,n do\n2: Sparsity Inheritance:\nS(W,N,M ) ‚Üêthe winner of S(W,N + 1,M).\n3: for each training iteration t do\n4: Forward Pass:\ngenerate Bt by group-wise magnitude pruning.\n5: Backward Pass:\nWt+1 = Wt ‚àíŒ≥tg(Wt ‚äôBt) +Œª((1 ‚àíBt) ‚äôWt).\n6: end for\n7: end for\nOutput: A series of N:M sparse models with different compu-\ntation complexity and corresponding masks: the winners\nof S(W,N = M ‚àí1,M), S(W,N = M ‚àí2,M),..,\nS(W,N = n,M).\nG0 G1 G2 G3\nZero element\nC\nR\n2:4sparse matrixùëä\nRxCelements\nR\nR/2\nbitmask\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n0\n1\n1\n0\n0\nG0 G1 G2 G3\nC\n2:4compact matrixùëä‚Ä≤\nR/2xCelements +\nRxCbits for mask\nFig. 4. Compact storage scheme example for N:M sparse parameter matrix.\nC. Packing N:M Sparse Parameters\nAn N:M sparse Transformer can be obtained after IDP,\nwhere each group of all parameter matrices only contains\nat most N non-zero elements. However, it occupies a large\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\namount of memory since the parameter store scheme is\nthe same as the dense Transformer. We apply the bitmap-\nbased compression scheme to obtain a compact N:M sparse\nTransformer, which greatly achieves saving on storage for\ndeployment. Compared to COO, CSC, CSR [35], and step\nindexing [36], our scheme has better compression performance\nin the range of practical N:M sparsity. Fig. 4 presents the\ncompression scheme of N:M sparse parameter matrix using\n2:4 sparsity as an example. For a parameter matrix W ‚àà\nRR√óC, after IDP, there are at most 2 non-zero elements in\na group. The entire parameter matrix has R\n2 √óC remaining\nnon-zero elements. In our scheme, we merely preserve non-\nzero elements in each group, and use a binary mask to indicate\nthe elements‚Äô position. By using our scheme, a 2:4 parameter\nmatrix W ‚ààRR√óC can be stored with R\n2 √óC valid elements\nand R√óC bits for the mask, instead of the R√óC elements.\nConsidering a dense parameter matrix W ‚àà RR√óC, in\nwhich all elements are quantized using q bits. W can be\ncompressed to ÀúW ‚ààRR√ó‚åàC\nM ‚åâN , where there are R‚åàC\nM ‚åâgroups\nand each group has N non-zero parameters at most. The\nstorage requirement of W is qRC bits, and after pruning,\nwe can only store the N:M sparse matrix ÀúW in a compact\nway with only qR‚åàC\nM ‚åâN bits, and an additional binary mask\nwith RC bits. Therefore, the compression ratio (CR) can be\nrepresented as:\nCR = qC\nq‚åàC\nM ‚åâN + C. (3)\nV. H ARDWARE OPTIMIZATION\nThe Ô¨Çexible and efÔ¨Åcient hardware architecture, namely\nSTA, is developed forN:M sparse Transformers in this section.\nWe Ô¨Årst present the overall architecture of STA, and then\nelaborate on designs of its core computing engines, including\nDMME and scalable softmax module.\nVector\nAdd & ReLU\n& Quant\nReshuffle\nNetwork\nScalable\nSoftmax\nIntermediate\nMem.\nComputing\nSoftmax Ctrl.\nAddr. Generator\nTop Ctrl.\nControl\nStorage\nInput Mem.\nWeight Mem.\nMatMul Ctrl.\nConfigurable\nRegisters\nDMA\nSoC Bus Interface\nDiverse\nMatMul.\nComputing\nEngine\nFig. 5. The overall architecture of STA. It is composed of computing, storage,\nand control function blocks. These red arrows pass control signals, while those\nblack arrows transfer data.\nA. Overall Architecture\nThe overall architecture of STA is shown in Fig. 5, which\nconsists of three major function blocks, including computing,\nstorage, and control. The computing blocks consist of a\ndiverse MatMul computing engine , namely DMME, a scalable\nsoftmax module, a vector unit, and a data reshufÔ¨Çe network.\nDominated operations of N:M sparse Transformers, i.e. sparse-\ndense or dense-dense MatMuls, are performed by DMME on-\nthe-Ô¨Çy with the dynamic conÔ¨Åguration under high computa-\ntional efÔ¨Åciency. The scalable softmax module is responsible\nfor the softmax operation in MHA ResBlocks, eliminating the\noff-chip transfer for intermediate data. The vector unit takes\ncharge of operations with low computational density including\nbias addition, residual addition, and activation functions. The\nreshufÔ¨Çe network reorders the temporary results before writing\nback to the intermediate on-chip memory. As for on-chip\nstorage, it can be partitioned into three parts, including the\nweight memory, the input memory, and the intermediate mem-\nory. The weight and input memory store model parameters\nand input data of Transformers from the off-chip memory,\nrespectively. The results of a ResBlock are also written back\nto the input memory, and pass to the off-chip memory. And\nall the temporary results in a ResBlock will be stored in the\nintermediate memory with no communication to the external\nmemory.\n0\nbIn\n(From West)\ncIn\n(From West)\n0\ncOut\n(To East)\nbOut\n(To East)\naOut\n(To South)\naIn\n(From North)\nmaskIn\n(From West)\nNon-zero\nElement\nSelector\nUnified\nSystolic PE\n0\n0\nmaskOut\n(To East)\nMAC\nxH\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nUnified MatMul.\nComputing Engine\nxC\nxR\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nN-parallel MAC\nbIn1\nbIn0\nbInN-1\naIn‚Äô1\naIn‚Äô0\naIn‚ÄôN-1\ncSumo\ncSumi\nAdder Tree\n(a) (b)\n(c) (d)\nNon-zero\nElement Selector\n‚Ä¶\naIn‚Äô1\naIn‚Äô0\naIn‚ÄôN-1\nx & (-x)\nxor\nxor x & (-x)\naIn0‚Ä¶M-1\nmaskIn\nxN\n1101 0001\n0100\n1000\n‚Ä¶\nFig. 6. The hierarchical architecture of DMME. It consists of H parallel\nuniÔ¨Åed MatMul computing engine (a). Each engine contains R √ó C uniÔ¨Åed\nsystolic PE capable of handling both sparse-dense and dense-dense dot\nproducts (b). The key components of PEs, non-zero element selector and\nthe N-parallel MAC, are in (c) and (d), respectively.\nB. DMME\nDMME uniÔ¨Åes both sparse-dense and dense-dense MatMuls\nwith a high computational efÔ¨Åciency in N:M sparse Transform-\ners. When it performs sparse-dense MatMuls, it merely loads\nnonzero weight parameters and selects corresponding activa-\ntions to compute, thereby improving computational efÔ¨Åciency.\nThe architecture of the DMME is illustrated in Fig. 6.\nIt is a two-level hierarchy design with a full exploration of\nparallelism inside the MatMuls of N:M sparse Transformers.\nThe exploited parallelism consists of head parallelism, row\nparallelism, and column parallelism, which are denoted as\nH, R, and C, respesctively. The DMME is composed of H\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nparallel R√óC uniÔ¨Åed MatMul computing engine (Fig. 6 (a)),\nevery one of which can efÔ¨Åciently realize both sparse-dense\nand dense-dense MatMuls in a time-division multiplexing\nmanner. The capability of performing sparse-dense and dense-\ndense MatMuls comes from the inner uniÔ¨Åed systolic PE\n(Fig. 6 (b)). It is composed of a non-zero element selector, a\nN-parallel MAC, multiple multiplexers, and registers. The non-\nzero element selector, only being activated in the sparse-dense\nMatMul mode, is to select the proper activation according\nto the input bitmask. The N-parallel MAC accepts N 16 -bit\ninput data and parameters, realizes inner product, and then\naccumulates the result with the local 32-bit output partial sum.\nThe multiplexers and registers are used for datapath selection\nand temporary data storage, respectively. The design of non-\nzero element selector is presented in Fig. 6 (c). It takes as\ninput an M-bit mask, in which only N bits are set as 1 to\nindicate the position of non-zero elements, and then generates\nN one-hot encoding masks to select the corresponding N data\nfor dot product computation. The translation to N one-hot\nencoding masks is performed by cascading the simple bit-\narithmetic blocks and XOR gates. With the help of N one-hot\nencoding masks, data related to non-zero parameters are fed\ninto the N-parallel MAC along with these non-zero parameters\nin one group. It could be pointed out that the non-zero element\nselector can be further optimized by pruning the redundant\nindexing indicators and element candidates. The N-parallel\nMAC, as shown in Fig. 6 (d), is composed of N parallel\nmultipliers, an adder tree, and a Ô¨Ånal accumulator. Both non-\nzero element selector and N-parallel MAC are fully pipelined\nto maximize the throughput of DMME.\n(a) Dense-dense MatMul\nMAC\nNelements\n1element\nNelements\nNelements\nNelements\nNon-zero\nElement\nSelector\nMAC\n(b) Sparse-dense MatMul\nMelements\nMbits\nNelements\n1element\nMbits\nNelementsNelements\nMelements\nFig. 7. The activated datapath of PEs under (a) dense-dense and (b) sparse-\ndense modes.\nThe activated datapath under dense-dense and sparse-dense\nMatMuls are presented in Fig. 7 (a) and (b), respectively.\nThe dense-dense mode of PEs would be only activated when\nperforming the self-attention operation of MHA. Both input\noperands are arranged in dense sequences in this mode. In this\ncase, N elements, as an operand, in the input sequences are\nin parallel streamed into the PE from the west and the north,\nrespectively. In a cycle, the PE performs a dot product with a\nsize of N under dense-dense mode. The partial sum is stored\nin the local registers, and the input operands from the west and\nthe north stored in the registers are passed into adjacent PEs\non the east and south, respectively. For energy saving, the non-\nzero element selector is bypassed to avoid signal switching.\nUnder sparse-dense MatMul mode, as shown in Fig. 7 (b), the\ninput operands is different from that under the dense-dense\nmode. In a cycle, N non-zero parameters in a group along with\nthe corresponding M-bit mask are streamed into the PE from\nthe west, while M data in one group are fed from the north.\nThe N valid data in pair with the input parameters is picked\nup by the non-zero element selector, and then performs a dot\nproduct with these input parameters. When the computing task\nis done, under either dense-dense or sparse-dense modes, the\nPE turns into the shifting mode, accepts the results from its\nwestern PE to its local registers and transfers its local result\nregisters to the east.\n(c)\nSparse-dense MatMul\non the unifed computing engine\nt = 0\nt = 1\nt = 2 1\n2\n-3\n1\n‚ÄîRedundant computation ‚Äî\n2 3\n1 0 0 1\nbitmask\n3 Cycle\n2-2\n1 0 1 0\n1\n-2\n2\n-2\n(b)\nSparse-dense MatMul\non the classic array\nt = 0\nt = 1\nt = 2\nt = 3\nt = 4\n-2\n-2\n1\n2\n2\n1\n1\n-3\n5 Cycle\nRedundant computation\n2 0 -2 0\n0 2 3 0\n(a)\nDense-dense MatMul\nt = 0\nt = 1\nt = 2\nt = 3\nt = 4\n-1 2 3 -2\n-2\n-2\n1\n2\n2\n1\n1\n-3\n2 1 -2 -1\n5 Cycle\nFig. 8. Computing dataÔ¨Çows of DMME when it performs (a) dense-dense and\n(c) sparse-dense MatMuls. Compared to (b) as a baseline, (c) eliminates all\nzero-valued redundant operations under sparse-dense mode, thus improving\ncomputational efÔ¨Åciency.\nC. Supporting EfÔ¨Åcient Matrix Computations\nSTA is capable of supporting both sparse-dense and dense-\ndense MatMuls of N:M sparse Transformers in an efÔ¨Åcient\nway. We demonstrate this signiÔ¨Åcant capability of STA by\nexploiting four aspects: the computing dataÔ¨Çow of DMME,\ndata access pattern of DMME, data mapping of input memory,\nand datapath from input memory to DMME.\nFig. 8 illustrates efÔ¨Åcient computing dataÔ¨Çows of DMME\nunder both dense-dense and sparse-dense modes. For simplic-\nity, we assume N:M is 1:2, and MatMul is performed by 2√ó4\ninput sequences and 4 √ó2 parameter sequences. Here, we\nconsider the computing engine in [10] as a baseline, which is\norchestrated as a classic systolic array. In Fig. 8 (a), DMME\nÔ¨Ånishes the dense-dense MatMul in the given computing\ntask using 5 cycles, which consumes the same cycles as the\nbaseline. Hence, DMME achieves the same computational efÔ¨Å-\nciency as the baseline when performing dense-dense MatMuls.\nAs for sparse-dense MatMuls, Fig. 8 (b) and (c) present the\ncomputational manner of the baseline and DMME, respec-\ntively. The baseline takes 5 cycles to Ô¨Ånish the task, while it\ncost merely 3 cycles by DMME since the redundant operations\ncan be skipped with no waste on computing cycles. For\nsparse-dense MatMuls, DMME improves the computational\nefÔ¨Åciency by eliminating redundant computations, thereby\nsigniÔ¨Åcantly reducing latency and energy.\nFig. 9 (a) and (b) presents data access patterns of DMME\nwhen it performs dense-dense and sparse-dense MatMuls,\nrespectively. For dense-dense MatMuls, attention heads as\ninput are both separated into H tiles. In this case, DMME\ncan be decomposed as H independent systolic arrays, every\none of which fetches elements from the corresponding tiles to\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nxH\nDMME\nxC\nxR\n‚Ä¶\n(a) Dense-Dense MatMul\nAttention Head\n1 2 H ‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nAttention Head\n1\n2\nH‚Ä¶\nUnicast\nNxHxCelements\nUnicast\nNxHxRelements\nxH\nDMME\nxC\nxR\n(b) Sparse-Dense MatMul\nActivation\nWeight Tile\nMulticast\nMxCelements\nUnicast\nNxHxRelements\n‚Ä¶\n‚Ä¶\n1\n2\nH‚Ä¶\n‚Ä¶\nFig. 9. Data access pattern of DMME to support efÔ¨Åcient MatMuls under (a)\ndense-dense and (b) sparse-dense modes.\nthe top-most and left-most PEs, respectively. For sparse-dense\nMatMuls, compressed weight parameters are divided into H\ntiles. Every cycle DMME fetches NR weight elements from\nall H tiles in parallel, and casts them one-on-one to the left-\nmost systolic PEs in H systolic arrays. DMME is also required\nto access MC activation elements, and broadcasts them to the\ntop-most PEs in all H uniÔ¨Åed systolic arrays.\nTo balance the bandwidth of input memory when switching\nbetween dense-dense and sparse-dense modes, we make NH\nequal to M of STA. There are C banks of STA for input data\nstorage. Fig. 10 illustrates data mapping of input memory and\ndatapath from input memory to DMME by assuming N is 2,\nM is 4, H is 2, and C is 4. The data storage storage structure\nin the input memory is varied for different computing modes.\nIn dense-dense mode, DMME performs 2 parallel dense-dense\nMatMuls for attention mechanism of Transformers. There are\n2 tiles for the loaded input data. As shown in Fig. 10 (a),\nthe Ô¨Årst bank is connected to the Ô¨Årst column of DMME,\nand the Ô¨Årst address of the bank indexes the data from the\nÔ¨Årst 2 elements at the Ô¨Årst column in the tile one and two,\nrespectively. In sparse-dense mode, DMME performs MatMuls\nwith the N:M sparse parameters. As depicted in Fig. 10 (b),\nwe do not tile input data for sparse-dense MatMul. The Ô¨Årst\naddress of bank one that connected to the Ô¨Årst column of\nDMME, indexes the data from the Ô¨Årst 4 elements at the Ô¨Årst\ncolumn of input data. It is the same of the indexing principle\nfor the other banks in both computing modes.\nActivation\nMulticast\n4x4elements\n‚Ä¶\n(C1, H1)\n(C1, H2)\n‚Ä¶\n(C4, H1)\n(C4, H2)\nM valid elements\n(b) Sparse-Dense MatMul\nBankcol1\nBankcol2\nBankcol4\nBankcol3\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n(C1, H1)\n(C1, H2)\n(C4, H1)\n(C4, H2)\nBankcol1\nBankcol2\nBankcol4\n1 2\n‚Ä¶\nAttention Head\nUnicast\n2x2x4elements\n‚Ä¶ ‚Ä¶\nBankcol3\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nN valid elements\n(a) Dense-Dense MatMul\nN M H C\n2 4 2 4\nSettings\nFig. 10. Data mapping of input memory and datapath from input memory to\nDMME under (a) dense-dense and (b) sparse-dense modes.\nAs for the datapath from input memory to DMME, we\ntakes the Ô¨Årst column of DMME as an example. There are\n4 elements accessed from the Ô¨Årst bank streaming to the Ô¨Årst\ncolumn with a head dimension of 2. In dense-dense mode, as\nshown in Fig. 10 (a), the uniÔ¨Åed systolic PE in the Ô¨Årst head\nof the Ô¨Årst column directly receives these 4 elements, and the\nlowest 2 elements are fed into N-parallel MAC for computing.\nHowever, the uniÔ¨Åed systolic PE in the second head of the Ô¨Årst\ncolumn requires 2-to-1 MUXs to select the correct 2 elements\nfrom the 4 accessed elements for computing. In sparse-dense\nmode, as presented in Fig. 10 (b), data accessed from the Ô¨Årst\nbank broadcast to uniÔ¨Åed systolic PEs in all head dimensions\nof the Ô¨Årst column of DMME.\nD. Scalable Softmax Module\nThe softmax function takes as input a vector x of n real\nnumbers, and normalizes it into a probability distribution\nconsisting of n probabilities proportional to the exponentials\nof the input numbers. It is critical for Transformer dedicated\naccelerators to contain a softmax hardware implementation\nsince the softmax function appears in every MHA module\nof Transformers. Fig. 11 presents the details of our proposed\nscalable softmax architecture, which is capable of performing\nsoftmax functions of arbitrary length. It keeps all the interme-\ndiate results fully local, avoiding off-chip data communication.\nThe architecture has two adjustable parameters, P and Q,\nwhere P denotes the parallelism of the architecture, and Q\nrepresents the pipeline depth, as well as the output precision.\nP input data are streamed into the softmax module in parallel,\nand transformed into the exponent outputs. The exponent\noutputs are not only temporarily stored in the data buffer,\nbut also used as input for further accumulation. Once the\naccumulation process is done, the divider module takes both\naccumulated results and exponent outputs as input to perform\nQ-level pipelined division, and generates P softmax function\noutputs represented by Q-bit.\nData Buffer\nCNT\nAdder \nTree\nCMP\n-\nDivider Block\nCMP\n-\nDivider Block\n‚Ä¶\nScalable Softmax\nxin\nexout\nA\nB\nA\nB\ncfg_acc_len\n‚Ä¶\n‚Ä¶\n‚Ä¶ ‚Ä¶\n‚Ä¶\n‚Ä¶\nLUT\nFunction ex1\nxP\nxQ\nxP\nDelay\nx(msb-1)\nDelay\nx(msb) ‚Ä¶\nCmsb Cmsb-1\n‚Ä¶\n‚Ä¶\nFig. 11. The architecture of the scalable softmax operator.\nAs shown in Fig. 11, the scalable softmax module consists\nof three major parts: an area-efÔ¨Åcient exponential function, a\npartial sum accumulator, and a scalable divider. The exponen-\ntial function is approximated using a lookup table combined\nwith a Ô¨Årst-order Taylor expansion. An exponential operator\ncan be implemented using only one multiplier and one adder.\nThe conÔ¨Ågurable partial sum accumulator can adapt to input\nvectors of various lengths, which improves the Ô¨Çexibility\nof the hardware. To reduce the latency of the division, we\ndesign a highly parallel divider by cascading multiple divider\nblocks with pipelines, where a divider block is composed of\nsubtractors and shifters with little cost on hardware.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nVI. E XPERIMENTAL RESULTS\nIn this section, we comprehensively evaluate both algorithm\nand hardware optimizations of the proposed framework. Three\nbenchmark sets with varying size and complexity are applied\nto evaluate the proposed framework.\nA. Experimental Setup\n1) Benchmark Sets:\nThe Ô¨Årst set focuses on the evaluation of algorithm opti-\nmizations, by comprehensively presenting improvements on\nboth model accuracy and compression ratio under various\nN:M conÔ¨Ågurations. This benchmark set comprises a BERT\nmodel [3], the well-known Transformer-based model, and four\nevaluation datasets from the GLUE benchmark [37], including\nWNLI, QNLI, QQP, and MRPC. WNLI is a reading compre-\nhension task. QNLI is a question-answering dataset consisting\nof question-paragraph pairs. QQP is a collection of question\npairs from the community question-answering website Quora.\nMPRC is a corpus of sentence pairs automatically extracted\nfrom online news sources. For these four tasks, we report\naccuracy of the validation sets. We also report the compression\nratio on BERT by setting various N:M conÔ¨Ågurations and\nquantized bitwidth of parameters.\nThe second set dives into hardware resource consumption.\nFirstly, we study consumption of DMMEs, the core computing\nengine in the STA, at any common scale of N:M sparsity,\nand then we explore hardware utilization of representative\nSTAs under various FPGA devices. For the former evaluation,\nDMMEs are not allowed to be synthesized using DSP blocks,\nwhich can be done by setting the property MAX DSP as zero.\nHence, the consumption of LUTs and FFs can measure the\ncost of combinational logic and sequential logic for DMMEs,\nrespectively. The utilization of LUTs and FFs are reported\nat the synthesis stage as the metric of hardware resource\nrequirements. For the latter evaluation, the resource utilization\nof STAs on various FPGA devices are reported at the imple-\nmentation stage, including the consuming amount of LUTs,\nFFs, BRAMs, and DSPs.\nThe third set studies performance improvements of overall\nSTA hardware system on multiple FPGA platforms when\ndeploying various Transformer-based models. All key conÔ¨Ågu-\nrations of evaluated models in benchmark sets are presented in\nTable II. At Ô¨Årst, we evaluate the processing time with a single\nbatch on all MHA and FFN ResBlocks in varying models\nfrom TinyBERT [38], Dino [39], and the Transformer-base\nmodel [11]. The selected models target different applications.\nTinyBERT is a lightweight BERT model for many language\ntasks. Dino, a tiny vision Transformer, can be the backbone\nfor a lot of computer vision tasks. Transformer-base model is\nthe classic one for the neural machine translation (NMT) task.\nConsidering NMT is one of the sequence-to-sequence tasks,\nhence we split the Transformer-base model into two parts, the\nstacked encoders and decoders, respectively. We Ô¨Ånally make a\nfair comparison of the implemented STAs with previous works\nand commercial products using a shallow Transformer, which\nis the commonly used benchmark model in [29], [33]. Latency,\nthroughput, power, energy efÔ¨Åciency and MAC efÔ¨Åciency are\nkey metrics for applications, and thus used for performance\nevaluation.\nTABLE II\nKEY CONFIGURATIONS OF TRANSFORMER -BASED MODELS IN\nBENCHMARK SETS\nBenchmark Model Num. ofEncodersNum. ofDecodersSequencelength Attentionheads Hiddensize Intermediatesize\nSet I BERT 12 0 128 12 768 3072\nSet III\nTinyBERT4 4 0 128 12 312 1200\nDino-vits8 12 0 64 6 384 1536\nTransformer-basestacked encoders6 0 64 8 512 2048\nTransformer-basestacked decoders0 6 64 8 512 2048\nShallowTransformer 2 1 64 4 200 800\n2) Implementation Details:\nFor algorithm implementation (Set I), the pre-trained mod-\nels, the scripts and datasets are provided by the HuggingFace\nrepository [40]. All models are implemented and executed\nusing PyTorch v1.5.\nAs for hardware implementation (Set II & III), all modules\nof STA are designed in synthesizable SystemVerilog with the\naid of hardware components from the BaseJump standard\ntemplate library [41] and the PULP platform [42]. Xilinx\nVivado 2018.2 is the tool for synthesis and implementation.\nWe implement STA on three types of FPGA devices with\nvarious scales, including Xilinx ZYNQ Z7020 (XC7Z020),\nXilinx Virtex-7 FPGA (XC7VX485T), and Xilinx UltraScale+\nFPGA (XCVU13P). SpeciÔ¨Åcally, XC7Z020 is low-cost and\nlow-resource System-on-Chip device equipped with a dual-\ncore ARM Cortex-A9 processor and FPGA, which is fabri-\ncated in the 28 nm technology node. XC7VX485T is a rela-\ntively large FPGA device fabricated in the 28 nm technology\nnode. XCVU13P, fabricated in the 16 nm technology node,\nis an extremely expensive and advanced FPGA device with\nabundant hardware resource.\nB. Benchmark Set I: Algorithm Optimizations\nFor benchmark set I, ASP [9] and SR-STE [7], the two\nexisting methods for acquiring N:M sparse models, are se-\nlected as our baselines. The reported accuracy of baselines is\nobtained by training with released open-source code. For a\nfair comparison, the generated N:M sparse models using ASP,\nSR-STE, and our method are achieved with identical Ô¨Ånetune\nepochs. For all tasks, we use a batch size of 32 and a initial\nlearning rate of 2e-5. For WNLI, QNLI, QQP, there are 3\nepochs to recover accuracy for every step of N, while there\nare 5 epochs for MRPC.\nCompared with existing methods, Fig. 12 shows that IDP\ncan achieve comparable or better accuracy under 75.00%\nsparse ratio. In addition, the IDP can outperform the ASP and\nSR-STE method signiÔ¨Åcantly with the sparse ratio increases.\nFor instance, we can Ô¨Ånd that under 87.50% ( 2:16) sparse ra-\ntio, IDP consistently obtains large performance improvements\nto the baseline on all tasks ( 5.36% accuracy gain on MNLI,\n10.38% accuracy gain on QNLI, 2.98% accuracy gain on\nQQP, and 8.08% accuracy gain on MRPC). Therefore, we can\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\n50.0056.2562.5068.7575.0081.2587.5093.75\nSparsity (%)\n74.00\n76.00\n78.00\n80.00\n82.00\n84.00Accuracy (%)\ndense\nASP-16\nASP-8\nASP-4\nSRSTE-16\nSRSTE-8\nSRSTE-4\nIDP-16\nIDP-8\nIDP-4\n(a) MNLI\n50.0056.2562.5068.7575.0081.2587.5093.75\nSparsity (%)\n75.00\n77.50\n80.00\n82.50\n85.00\n87.50\n90.00Accuracy (%)\ndense\nASP-16\nASP-8\nASP-4\nSRSTE-16\nSRSTE-8\nSRSTE-4\nIDP-16\nIDP-8\nIDP-4 (b) QNLI\n50.0056.2562.5068.7575.0081.2587.5093.75\nSparsity (%)\n86.00\n87.00\n88.00\n89.00\n90.00\n91.00Accuracy (%)\ndense\nASP-16\nASP-8\nASP-4\nSRSTE-16\nSRSTE-8\nSRSTE-4\nIDP-16\nIDP-8\nIDP-4 (c) QQP\n50.0056.2562.5068.7575.0081.2587.5093.75\nSparsity (%)\n65.00\n70.00\n75.00\n80.00\n85.00Accuracy (%)\ndense\nASP-16\nASP-8\nASP-4\nSRSTE-16\nSRSTE-8\nSRSTE-4\nIDP-16\nIDP-8\nIDP-4 (d) MRPC\nFig. 12. Pruning results on various tasks incluing (a) MNLI, (b) QNLI, (c) QQP, and (d) MRPC in comparison with ASP [9] and SR-STE [7].\nobtain the state-of-the-art N:M sparse models for FPGA-based\nplatform deployment with the plug-and-play IDP algorithm.\nBased on our evaluations of model accuracy with respect\nto parameter sparsity, as shown in Fig. 12, it is observed that\nTransformers can hardly achieve a sparsity over 90% without\nimpacting accuracy. It would be more likely practical for N:M\nsparse Transformers with a sparsity ranging from 50% to\n87.5%. Next, BERT is taken as an example to evaluate the\nstorage reduction of our compression scheme when using mul-\ntiple quantized bits under various N:M sparsity conÔ¨Ågurations.\nWe make a elaborated comparison between our bitmap-based\nscheme, COO, CSR, CSC, and step indexing [36]. Compared\nwith the other mainstream methods, as shown in Fig. 13, our\nscheme can achieve the highest compression ratio when the\nmodel sparsity is varied from 50% to 87.5%. The compression\nratio keeps increasing as the model sparsity increases. An\nN:M sparse BERT, can achieve a higher compression ratio\nwhen quantized in larger bit widths. BERT with 50.00% N:M\nsparsity can reach a 1.78√óreduction on storage of parameters.\nWhen BERT has a sparsity of 87.50%, it achieves a signiÔ¨Åcant\nstorage saving, up to 5.33√ó, on parameters. Our compression\nscheme can efÔ¨Åciently reduce the storage requirement for\nN:M sparse parameters. In subsequent hardware evaluations,\nwe uniformly adopt a 16-bit Ô¨Åxed-point representation for\nTransformers to avoid negative impacts on model accuracy\ndue to quantization.\n50.00 56.25 62.50 68.75 75.00 81.25 87.50 93.75\nSparsity (%)\n0.00\n0.50\n1.00\n1.50\n2.00\n2.50\n3.00\n3.50\n4.00Compression Ratio\nBitmap\nCOO\nCSR\nCSC\nStepIndexing\n(a) Bitwidth = 4bit\n50.00 56.25 62.50 68.75 75.00 81.25 87.50 93.75\nSparsity (%)\n0.00\n1.00\n2.00\n3.00\n4.00\n5.00\n6.00\n7.00Compression Ratio\nBitmap\nCOO\nCSR\nCSC\nStepIndexing (b) Bitwidth = 8bit\n50.00 56.25 62.50 68.75 75.00 81.25 87.50 93.75\nSparsity (%)\n0.00\n2.00\n4.00\n6.00\n8.00Compression Ratio\nBitmap\nCOO\nCSR\nCSC\nStepIndexing\n(c) Bitwidth = 16bit\n50.00 56.25 62.50 68.75 75.00 81.25 87.50 93.75\nSparsity (%)\n0.00\n2.00\n4.00\n6.00\n8.00\n10.00\n12.00Compression Ratio\nBitmap\nCOO\nCSR\nCSC\nStepIndexing (d) Bitwidth = 32bit\nFig. 13. Compression ratio on BERT with various sparsity conÔ¨Ågurations.\nC. Benchmark Set II: Hardware Resource Consumption\nThe second benchmark set veriÔ¨Åes hardware consumption\nof STAs that have not yet deployed Transformer models.\nAt Ô¨Årst, we evaluate the hardware requirements of DMME in\ncommon sparse conÔ¨Ågurations by comparing it against multi-\nple dense computing engines. For a fair comparison, evaluated\ncomputing engines are all synthesized as a 2 √ó2 uniÔ¨Åed\nMatMul computing engine , and only PEs in these engines are\nconÔ¨Ågured into various N:M conÔ¨Ågurations. Note that those\ncomputing engines that make N equal to M, exclude the non-\nzero element selectors and merely support dense computing. It\ncan be regarded as the computing engine in [10] if both N and\nM are 1 in the evaluated DMME. These computing engines are\nnot allowed to be synthesized using DSP blocks, which can\nbe done by setting the property MAX DSP as zero. Hence, the\nutilization of LUTs and FFs from Vivado synthesis reports can\nbe used to measure the consumption of combinational logic\nand sequential logic for DMMEs, respectively.\nFig. 14 presents the comparison of required hardware re-\nsource consumption between DMMEs of various conÔ¨Ågura-\ntions. For simplicity, all results are normalized to the 4:4 dense\nbaseline computing engine. Gray bars are resource consump-\ntion of various computing engines with sole support on dense\nmatrix multiplication. Green, red, and yellow bars represent\nhardware utilization of DMMEs, when N is set as 1, 2, and 3,\nrespectively. In Fig. 14, we can observe hardware resource\nsaved by DMMEs compared to dense baseline computing\nengine under sparse matrix computing mode. When M = 16\nand N is set to 1, 2, and 3, respectively, DMME, in contrast\nto 16:16 baseline, obtains saving of combinational logic up\nto 7.96√ó, 4.76√ó, and 3.17√ó , while achieving reduction\non sequential logic 4.41√ó, 2.63√ó, and 1.99√ó. According to\nFig. 14, we further evaluate the impact of separately increasing\nN and M in DMMEs on hardware resource consumption.\nFor instance, 3:4 DMME costs 2.42√ó combinational logic\nand 2.78√ósequential logic of 1:4 DMME. However, 1:16\nDMME merely requires 1.28√ócombinational logic and 2.00√ó\nsequential logic over 1:4 DMME.\nWe Ô¨Ånally evaluate hardware resource consumption of STA\non three types of FPGA platforms, including XC7Z020,\nXC7VX485T, and XCVU13P. These FPGA platforms are used\nto represent diverse devices in Fig. 2. Considering the hard-\nware resource and cost on these platforms, we intend to deploy\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\n1:1 2:2 3:3\n \n4:4 3:4 2:4 1:4\n \n8:8 3:8 2:8 1:8\n \n16:163:162:161:16\n0.00\n0.50\n1.00\n1.50\n2.00\n2.50\n3.00\n3.50\n4.00Normalized Utilization0.26x\n0.50x\n0.78x\n1.00x 0.92x\n0.59x\n0.38x\n1.95x\n1.03x\n0.67x\n0.46x\n3.90x\n1.23x\n0.82x\n0.49x\nclassic\nN=3\nN=2\nN=1\n(a) Combinational logic\n1:1 2:2 3:3\n \n4:4 3:4 2:4 1:4\n \n8:8 3:8 2:8 1:8\n \n16:163:162:161:16\n0.00\n0.50\n1.00\n1.50\n2.00\n2.50\n3.00\n3.50Normalized Utilization0.28x\n0.55x\n0.75x\n1.00x\n1.11x\n0.82x\n0.40x\n1.79x\n1.23x\n0.91x\n0.48x\n3.53x\n1.77x\n1.34x\n0.80x\nclassic\nN=3\nN=2\nN=1 (b) Sequential logic\nFig. 14. The normalized resource consumption of uniÔ¨Åed computing engine\nover the classic computing engine on various scales including (a) combina-\ntional logic and (b) sequential logic.\nXC7Z020 to the edge and XC7VX485T and XCVU13P on the\nclouds. There are many tunable parameters of STAs, especially\nin DMME, which have great impacts on performance. In order\nto determine the speciÔ¨Åc parameters of STA on each FPGA\nplatforms, we design a cycle-accurate simulator to evaluate\nactual inference performance based on the given speciÔ¨Åcations.\nSTA in XC7Z020 adopts an aggressive 1:8 sparsity since\nlatency is the critical metric on the edge platforms. However,\nSTAs in both XC7VX485T and XCVU13P conÔ¨Ågured N:M as\n2:8 because devices deployed on the clouds concerns latency\nas well as model accuracy. Table III shows resource consump-\ntion of STAs deployed on three scales of FPGA platforms,\nnamely STA-Tiny, STA-Small, and STA-Large, respectively.\nThe FPGA resource and power breakdown of STA-Small\nare presented in Table IV. The N-parallel MAC (N-MAC)\nmodule dominates the DSP consumption of STA since it is the\ncore of computing engine for MAC operations. The non-zero\nelement selector (NZES) module takes the majority of LUT\nconsumption due to the cost for decoding and index selection.\nThe routing module occupies most registers of DMME for\ndatapath selection and temporary data storage. Moreover, the\nproposed DMME and softmax module occupy 47.29% and\n9.42% power consumption, respectively.\nTABLE III\nFPGA RESOURCE UTILIZATION\nPlatform Frequency LUT FF BRAM DSP\n1:8 STA-Tiny\n(XC7Z020) 150MHz 21K\n(40.38%)\n75K\n(71.21%)\n96\n(68.57%)\n132\n(60.00%)\n2:8 STA-Small\n(XC7VX485T) 200MHz 116K\n(38.42%)\n337K\n(55.52%)\n532\n(51.65%)\n1,040\n(37.14%)\n2:8 STA-Large\n(XCVU13P) 200MHz 464K\n(26.88%)\n1,321K\n(38.24%)\n1,192\n(44.35%)\n4,160\n(33.85%)\nD. Benchmark Set III: Overall System Evaluation\nThe third benchmark set is used to evaluate performance\nwhen deploying various Transformer-based models on STAs.\nFirstly, we study the inference speedup of STAs by contrast\nwith CPUs, GPUs, and the prior dedicated accelerators. The\nselected models to be deployed are composed of TinyBERT\n[38], Dino [39], the classic Transformer model [11]. Here\nwe consider single-batch processing time in all the MHA\nand FFN ResBlocks of these Transformer-based models. For\ncross-platform comparison, the hardware setup is as follows to\nexecute the Transformer inference tasks. The CPU results are\nTABLE IV\nFPGA R ESOURCE AND POWER BREAKDOWN OF STA-S MALL\nLUT FF BRAM DSP Power (W)\nN-MAC 16K\n(13.79%)\n68K\n(20.18%) - 1024\n(98.46%)\n2.78\n(28.17%)\nNZES 66K\n(56.90%)\n68K\n(20.18%) - - 0.83\n(8.41%)DMME\nRouting 9K\n(7.76%)\n180K\n(53.41%) - - 1.06\n(10.74%)\nSoftmax 13K\n(11.21%)\n8K\n(2.37%)\n16\n(3.00%)\n16\n(1.54%)\n0.93\n(9.42%)\nOthers 12K\n(10.34%)\n13K\n(3.86%)\n516\n(97.00%) - 4.27\n(43.26%)\nTotal 116K\n(100.00%)\n337K\n(100.00%)\n532\n(100.00%)\n1040\n(100.00%)\n9.87\n(100.00%)\nmeasured using an ARM Cortex A57 and an Intel i9-9900X.\nThe former commonly appeared in mobile devices for edge\napplications, while the latter is a high-end CPU product for\ndeploying cloud applications. The GPU results are measured\nusing an NVIDIA Jeston Nano, an embedded GPU product for\nedge applications, an NVIDIA RTX 2080Ti, and an NVIDIA\nRTX 3090 capable of 2:4 sparse acceleration. Following the\ncomparison method in [32], we apply [10] as baselines on\nFPGA platforms by scaling the size of its computing engine.\nTwo existing sparse accelerators for Transformers, OPTIMUS\n[5] and EdgeBERT [6], are evaluated as well for a more com-\nprehensive comparison of STA. Fig. 15 shows the idealized\nperformance speedup of different hardware platforms, where\nedge and cloud platforms normalized to the ARM Cortex A57\nand Intel i9-9900X, respectively.\nTinyBERT4 Dino-vits8 Transformer-base\nstacked encoders\nTransformer-base\nstacked decoders\nGeometric\nMean\n0.00\n5.00\n10.00\n15.00\n20.00\n25.00\n30.00\n35.00Inference Performance Speedup\n1.00 1.00 1.00 1.00 1.00\n18.73\n9.85\n12.21\n10.32\n12.35\n8.87\n6.40\n4.74 5.20\n6.12\n20.00\n14.14\n10.23\n11.35\n13.46\n36.85\n35.41\n29.63\n30.54\n32.96\nCPU@ARM Cortex A57\nGPU@NVIDIA Jetson Nano\nBaseline-Tiny@150MHz, 128MAC\nEdgeBERT-16@150MHz, 256MAC\n1:8 STA-Tiny@150MHz, 128MAC\n(a)\nTinyBERT4 Dino-vits8 Transformer-base\nstacked encoders\nTransformer-base\nstacked decoders\nGeometric\nMean\n0.00\n2.50\n5.00\n7.50\n10.00\n12.50\n15.00\n17.50Inference Performance Speedup\n1.00 1.00 1.00 1.00 1.00\n2.57 2.46\n4.39\n3.48\n3.13\n6.39\n9.24\n15.02\n12.69\n10.30\n1.09\n1.59 1.70 1.69 1.491.47\n1.96 2.00 2.03 1.85\n2.54\n3.97 4.10 4.13\n3.61\n2.63\n4.65\n5.44 5.22\n4.32\n3.28\n5.38\n5.92 5.83\n4.97\n7.06\n14.57\n17.68\n16.67\n13.19\nCPU@Intel I9-9900X\nGPU@NVIDIA RTX 2080Ti\nGPU@NVIDIA RTX 3090\nBaseline-Small@200MHz, 1024MAC\nEdgeBERT-32@200MHz, 1024MAC\nOPTIMUS@200MHz, 1024MAC\n2:8 STA-Small@200MHz, 1024MAC\nBaseline-Large@200MHz, 4096MAC\n2:8 STA-Large@200MHz, 4096MAC\n(b)\nFig. 15. The processing time of Transformer-based models on various (a)\nedge platforms and (b) cloud platforms.\nAs shown in Fig. 15 (a), among all edge platforms, STA-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nTiny achieves a geometric mean increase of 32.96√ó, 2.69√ó,\nand 5.38√óover CPU, GPU, and the FPGA baseline, respec-\ntively. Fig. 15 (b) presents the performance comparison be-\ntween various cloud platforms. For fair comparison, Baseline-\nSmall (red), EdgeBERT (purple), OPTIMUS (brown), and 2:8\nSTA-Small (pink) in Fig. 15 (b) are evaluated under the same\nnumber of MAC units and clock frequency.\n‚Ä¢ STA v.s. Baseline-Small [10]: 2:8 STA-Small achieves\n2.89√óspeedup on average over Baseline-Small, which\nenables dense Transformer acceleration using a large 2D\nsystolic array. It suffers low utilization of MAC units due\nto inÔ¨Çexible mapping scheme and large skew latency for\nsystolic-arranged data. STA achieves signiÔ¨Åcant perfor-\nmance improvement by: 1) innovation of DMME from\nthe architectural aspects, and 2) reduction on MAC oper-\nations of N:M sparse Transformers from the algorithmic\naspects. We further present a performance breakdown of\nSTA compared to Baseline-Small. As shown in Fig. 16,\nthe architectural innovation for DMME can achieve\n1.08√ó better performance improvement, and there is\n2.68√óspeedup on top of the architectural innovation by\nefÔ¨Åciently enabling 2:8 sparse acceleration.\n‚Ä¢ STA v.s. EdgeBERT [6]: 2:8 STA-Small achieves 2.33√ó\nspeedup on average over EdgeBERT-32, which is an\nenergy-optimized Transformer accelerator exploiting un-\nstructured sparsity. When performing sparse MAC opera-\ntions, processing units of EdgeBERT skip the zero input\nvalue through a gating strategy, which can signiÔ¨Åcantly\nreduce energy consumption, but offer little beneÔ¨Åt to\nlatency. Compared to EdgeBERT, STA performs N:M\nsparse MAC operations by choosing non-zero inputs,\nwhich can both reduce energy and optimize latency.\n‚Ä¢ STA v.s. OPTIMUS [5]: 2:8 STA-Small has a 1.20√ó\nbetter performance on average over OPTIMUS, which is\na high performance sparse accelerator for Transformers\nexploiting unstructured sparsity in weight parameters.\nWhen performing sparse MAC operations, OPTIMUS\ncan hardly achieve high MAC utilization due to load\nimbalance and input load miss. STA can effectively\novercome these two problem suffered by OPTIMUS. STA\ngets rid of load imbalance by arranging each MAC in\nDMME to perform operations in a balanced N:M group.\nIn addition, STA loads a series of input N:M groups at\neach cycle, and utilizes these elements multiple times in\na systolic manner, which effectively addresses input load\nmiss compared to OPTIMUS.\nFinally, we compare STA with other previous FPGA-based\nworks and commercial CPU and GPU products. Table V\npresents a fair performance comparison without batching on\nvarious platforms. Prior FPGA-based works for accelerating\nTransformers, include [10], [29], and [33]. The dedicated\naccelerator in [10], equipped with a large 2D systolic array for\ndense operation, is the pioneer work for Transformer acceler-\nation. FTRANS [29], is another recent specialized accelerator\nfor Transformers, which exploits the speedup potential of\nblock-circulant weight representations. In [33], the proposed\naccelerator utilizes coarse-grained block-based sparsity to\nTinyBERT4 Dino-vits8 Transformer-base\nstacked encoders\nTransformer-base\nstacked decoders\nGeometric\nMean\n0.00\n0.50\n1.00\n1.50\n2.00\n2.50\n3.00Normalized Speedup\nBaseline-Small\n2:8 STA-Small, Dense Only\n2:8 STA-Small, Sparse MHA\n2:8 STA-Small, Sparse MHA + Sparse FFN\n1.00 1.00 1.00 1.00 1.001.09 1.07 1.07 1.08 1.081.34 1.36 1.37 1.60 1.412.41 2.92 3.21 3.09 2.89\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized MAC Operations\n100.00%\n100.00%\n75.81%\n30.65%\n100.00%\n100.00%\n75.71%\n27.14%\n100.00%\n100.00%\n75.61%\n26.83%\n100.00%\n100.00%\n63.86%\n27.11%\n100.00%\n100.00%\n72.55%\n27.89%\nFig. 16. Performance and MAC operation breakdown of STA-Small.\nspeedup Transformer inference. The shallow Transformer used\nin [29] and [33] is applied as a benchmark for a fair evaluation.\nThe comparison is benchmarked on CPU, GPUs, prior cutting-\nedge FPGA solutions, and STA on various FPGA platforms.\nWe evaluate these designs in terms of latency, throughput,\npower, energy efÔ¨Åciency, and MAC efÔ¨Åciency. All of them\nare key metrics for a computing system.\nAs shown in Table V, STA-Tiny far outperforms the em-\nbedded GPU, Jetson Nano, and the high-end CPU, i9-9900X,\nin all evaluated metrics. STA-Small surpasses the CPU and\nGPU platforms in all metrics. Moreover, STA-Small is close\nto [10] and [33] in terms of latency and throughput, while\nusing a relatively small number of MACs compared to them.\nThe energy efÔ¨Åciency and MAC efÔ¨Åciency of STA-Small are\nalso superior to all previous FPGA-based works. Compared to\nprevious FPGA solutions, STA-Large achieves 2.00 ‚àº19.47√ó\nthroughput improvement, achieves 1.26 ‚àº 16.47√ó energy\nefÔ¨Åciency improvement, and 1.80 ‚àº36.00√óMAC efÔ¨Åciency\ngain, respectively. The performance gain of STA comes from\noptimizations from two levels. At the algorithm level, we care-\nfully exploit the potential of N:M sparsity pattern, which can\nsigniÔ¨Åcantly reduce the computational cost of Transformer-\nbased models. At the hardware level, STA can efÔ¨Åciently\nhandle N:M sparse parameters, which signiÔ¨Åcantly improves\nthe utilization of computing units. In addition, our deploy-\nment framework, taking STA-Tiny, STA-Small, and STA-\nLarge as examples, can realize Ô¨Çexible hardware generation\nfor Transformers. The proposed framework can Ô¨Çexibly and\nefÔ¨Åciently meet the requirements for deploying Transformer-\nbased models on various FPGA devices.\nVII. C ONCLUSION\nIn this paper, we present a Ô¨Çexible, agile, and efÔ¨Åcient\nframework for deploying N:M sparse Transformers, which is\nbeneÔ¨Åted from both algorithm and hardware optimizations,\nmaking it practical to signiÔ¨Åcantly accelerate Transformer-\nbased models on diverse FPGA devices. At the algorithm level,\nwe propose a sparsity inheritance mechanism and a inherited\ndynamic pruning (IDP) method to obtain a series of N:M\nsparse Transformers with high accuracy. A further proposed\ncompression scheme greatly reduces the storage requirements\nof models. At the hardware level, we present a Ô¨Çexible and\nefÔ¨Åcient architecture, namely STA, to accelerate N:M sparse\nTransformers. STA is composed of a computing core, DMME,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\nTABLE V\nCOMPARISON OF STAS WITH PREVIOUS WORKS AND COMMERCIAL PRODUCTS\nPlatform\nCPU GPU FPGA\ni9-9900X Jetson Nano RTX 2080 Ti RTX 3090 SOCC‚Äô20\n[10]\nISLPED‚Äô20\n[29]\nISQED‚Äô21\n[33]\nOur work\nSTA-Tiny STA-Small STA-Large\nChip Skylake Tegra X1 TU102 GA102 XCVU13P XCVU9P XCU200 XC7Z020 XC7VX485T XCVU13P\nTechnology 14 nm 20 nm 12 nm 8 nm 16 nm 16 nm 16 nm 28 nm 28 nm 16 nm\nFrequency 3.50 GHz 640 MHz 1.35 GHz 1.70 GHz 200 MHz - - 150 MHz 200 MHz 200 MHz\nMethods - - - 2:4 group-based\npruning\nLow-bit\nquantization\nBlock-circulant\nmatrix with FFT\nBlock-based\npruning N:M group-based pruning\n# MAC units - - - - 4096 ‚àº5647 ‚àº3368 128 1024 4096\nBit Precision FP-32 FP-32 FP-32 FP-32 FIX-8 FIX-16 - FIX-16\nTest Network Shallow Transformer\nLatency (ms) 2.17 16.24 1.70 0.46 0.30 2.94 0.32 2.01 0.42 0.15\nBatch-1 Throughput (GOP/s)101.38 13.55 129.41 478.26 733.33 75.34 687.50 109.45 523.81 1466.67\nPower (W) 165.00 7.56 250.00 350.00 16.70 22.45 - 2.71 9.87 26.59\nEnergy EfÔ¨Åciency (GOP/J)0.61 1.79 0.52 1.37 43.91 3.35 - 40.39 53.07 55.16\nMAC EfÔ¨Åciency (GOP/s/unit)- - - - 0.18 ‚àº0.01 ‚àº0.20 0.86 0.51 0.36\nthat uniÔ¨Åes both sparse and dense intensive matrix multipli-\ncations in N:M sparse Transformers, and a scalable softmax\nmodule, which eliminates intermediate off-chip data accesses.\nThe experimental results show that N:M sparse Transformers\ngenerated by IDP achieves an average of 6.7% improvement\nin accuracy over the state-of-the-art methods. STA implemen-\ntation signiÔ¨Åcantly outperforms CPU, GPU, and prior FPGA-\nbased Transformer accelerators in terms of latency, throughput,\nenergy efÔ¨Åciency, and MAC efÔ¨Åciency, showing its signiÔ¨Åcant\npotential in applications using Transformer-based models.\nACKNOWLEDGMENT\nWe would like to sincerely thank our reviewers for their\nvaluable feedback.\nREFERENCES\n[1] Y . Tay, M. Dehghani, D. Bahri, and D. Metzler, ‚ÄúEfÔ¨Åcient Transformers:\nA Survey,‚Äù arXiv preprint arXiv:2009.06732 , 2020.\n[2] K. Song, K. Wang, H. Yu, Y . Zhang, Z. Huang, W. Luo, X. Duan, and\nM. Zhang, ‚ÄúAlignment-Enhanced Transformer for Constraining NMT\nwith Pre-speciÔ¨Åed Translations,‚Äù in Proceedings of the AAAI Conference\non ArtiÔ¨Åcial Intelligence (AAAI) , vol. 34, no. 05, 2020, pp. 8886‚Äì8893.\n[3] J. D. M.-W. C. Kenton and L. K. Toutanova, ‚ÄúBERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding,‚Äù in\nProceedings of the Conference of the North American Chapter of the\nAssociation for Computational Linguistics ‚Äì Human Language Tech-\nnologies (NAACL-HLT), 2019, pp. 4171‚Äì4186.\n[4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‚ÄúAn Image is Worth 16x16 Words: Trans-\nformers for Image Recognition at Scale,‚Äù in International Conference\non Learning Representations (ICLR) , 2021.\n[5] J. Park, H. Yoon, D. Ahn, J. Choi, and J.-J. Kim, ‚ÄúOPTIMUS: OPTI-\nmized Matrix MUltiplication Structure for Transformer Neural Network\nAccelerator,‚Äù inProceedings of Machine Learning and Systems (MLSys),\n2020.\n[6] T. Tambe, C. Hooper, L. Pentecost, T. Jia, E.-Y . Yang, M. Donato,\nV . Sanh, P. N. Whatmough, A. M. Rush, D. Brooks, and G.-Y .\nWei, ‚ÄúEdgeBERT: Sentence-Level Energy Optimizations for Latency-\nAware Multi-Task NLP Inference,‚Äù in Proceedings of the 54th Annual\nIEEE/ACM International Symposium on Microarchitecture (MICRO) ,\n2021.\n[7] A. Zhou, Y . Ma, J. Zhu, J. Liu, Z. Zhang, K. Yuan, W. Sun, and\nH. Li, ‚ÄúLearning N:M Fine-grained Structured Sparse Neural Networks\nFrom Scratch,‚Äù inInternational Conference on Learning Representations\n(ICLR), 2021.\n[8] W. Sun, A. Zhou, S. Stuijk, R. Wijnhoven, A. O. Nelson, H. Corporaal\net al. , ‚ÄúDominoSearch: Find Layer-wise Fine-grained N: M Sparse\nSchemes from Dense Neural Networks,‚Äù Advances in Neural Informa-\ntion Processing Systems (NeurIPS) , vol. 34, 2021.\n[9] A. Mishra, J. A. Latorre, J. Pool, D. Stosic, D. Stosic, G. Venkatesh,\nC. Yu, and P. Micikevicius, ‚ÄúAccelerating Sparse Deep Neural Net-\nworks,‚Äù arXiv preprint arXiv:2104.08378 , 2021.\n[10] S. Lu, M. Wang, S. Liang, J. Lin, and Z. Wang, ‚ÄúHardware Accelerator\nfor Multi-Head Attention and Position-Wise Feed-Forward in the Trans-\nformer,‚Äù in 2020 IEEE 33rd International System-on-Chip Conference\n(SOCC), 2020.\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention Is All You Need,‚Äù inProceedings\nof the 31st International Conference on Neural Information Processing\nSystems (NeurIPS), 2017.\n[12] D. Wu, X. Fan, W. Cao, and L. Wang, ‚ÄúSWM: A High-Performance\nSparse-Winograd Matrix Multiplication CNN Accelerator,‚Äù IEEE Trans-\nactions on Very Large Scale Integration (VLSI) Systems , vol. 29, no. 5,\npp. 936‚Äì949, 2021.\n[13] S. Colleman and M. Verhelst, ‚ÄúHigh-Utilization, High-Flexibility Depth-\nFirst CNN Coprocessor for Image Pixel Processing on FPGA,‚Äù IEEE\nTransactions on Very Large Scale Integration (VLSI) Systems , vol. 29,\nno. 3, pp. 461‚Äì471, 2021.\n[14] H. E. Yantƒ±r, A. M. Eltawil, and K. N. Salama, ‚ÄúIMCA: An EfÔ¨Åcient\nIn-Memory Convolution Accelerator,‚Äù IEEE Transactions on Very Large\nScale Integration (VLSI) Systems , vol. 29, no. 3, pp. 447‚Äì460, 2021.\n[15] S. Yin, Z. Jiang, M. Kim, T. Gupta, M. Seok, and J.-S. Seo, ‚ÄúVesti:\nEnergy-EfÔ¨Åcient In-Memory Computing Accelerator for Deep Neural\nNetworks,‚Äù IEEE Transactions on Very Large Scale Integration (VLSI)\nSystems, vol. 28, no. 1, pp. 48‚Äì61, 2019.\n[16] G. Paulin, R. Andri, F. Conti, and L. Benini, ‚ÄúRNN-Based Radio Re-\nsource Management on Multicore RISC-V Accelerator Architectures,‚Äù\nIEEE Transactions on Very Large Scale Integration (VLSI) Systems ,\nvol. 29, no. 9, pp. 1624‚Äì1637, 2021.\n[17] C. Fang, L. He, H. Wang, J. Wei, and Z. Wang, ‚ÄúAccelerating 3D\nConvolutional Neural Networks Using 3D Fast Fourier Transform,‚Äù in\n2021 IEEE International Symposium on Circuits and Systems (ISCAS) .\nIEEE, 2021, pp. 1‚Äì5.\n[18] Y . Yu, T. Zhao, M. Wang, K. Wang, and L. He, ‚ÄúUni-OPU: An FPGA-\nbased Uniform Accelerator for Convolutional and Transposed Convo-\nlutional Networks,‚Äù IEEE transactions on very large scale integration\n(VLSI) systems, vol. 28, no. 7, pp. 1545‚Äì1556, 2020.\n[19] C. Zhu, K. Huang, S. Yang, Z. Zhu, H. Zhang, and H. Shen, ‚ÄúAn\nEfÔ¨Åcient Hardware Accelerator for Structured Sparse Convolutional\nNeural Networks on FPGAs,‚Äù IEEE Transactions on Very Large Scale\nIntegration (VLSI) Systems , vol. 28, no. 9, pp. 1953‚Äì1965, 2020.\n[20] A. A. Moreno, J. Olivito, J. Resano, and H. Mecha, ‚ÄúAnalysis of a\nPipelined Architecture for Sparse DNNs on Embedded Systems,‚Äù IEEE\nTransactions on Very Large Scale Integration (VLSI) Systems , vol. 28,\nno. 9, pp. 1993‚Äì2003, 2020.\n[21] X. Lian, Z. Liu, Z. Song, J. Dai, W. Zhou, and X. Ji, ‚ÄúHigh-Performance\nFPGA-based CNN Accelerator with Block-Floating-Point Arithmetic,‚Äù\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\nIEEE Transactions on Very Large Scale Integration (VLSI) Systems ,\nvol. 27, no. 8, pp. 1874‚Äì1885, 2019.\n[22] S. Kala, B. R. Jose, J. Mathew, and S. Nalesh, ‚ÄúHigh-Performance CNN\nAccelerator on FPGA using UniÔ¨Åed Winograd-GEMM Architecture,‚Äù\nIEEE Transactions on Very Large Scale Integration (VLSI) Systems ,\nvol. 27, no. 12, pp. 2816‚Äì2828, 2019.\n[23] X. Xie, J. Lin, Z. Wang, and J. Wei, ‚ÄúAn EfÔ¨Åcient and Flexible\nAccelerator Design for Sparse Convolutional Neural Networks,‚Äù IEEE\nTransactions on Circuits and Systems I: Regular Papers (TCAS-I) ,\nvol. 68, no. 7, pp. 2936‚Äì2949, 2021.\n[24] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and\nA. Moshovos, ‚ÄúCnvlutin: Ineffectual-Neuron-Free Deep Neural Network\nComputing,‚Äù in 2016 ACM/IEEE 43rd Annual International Symposium\non Computer Architecture (ISCA) . IEEE, 2016, pp. 1‚Äì13.\n[25] Y .-H. Chen, J. Emer, and V . Sze, ‚ÄúEyeriss: A Spatial Architecture\nfor Energy-EfÔ¨Åcient DataÔ¨Çow for Convolutional Neural Networks,‚Äù in\n2016 ACM/IEEE 43rd Annual International Symposium on Computer\nArchitecture (ISCA). IEEE, 2016, pp. 367‚Äì379.\n[26] S. Liu, Z. Du, J. Tao, D. Han, T. Luo, Y . Xie, Y . Chen, and T. Chen,\n‚ÄúCambricon: An Instruction Set Architecture for Neural Networks,‚Äù in\n2016 ACM/IEEE 43rd Annual International Symposium on Computer\nArchitecture (ISCA). IEEE, 2016, pp. 393‚Äì405.\n[27] Z.-G. Liu, P. N. Whatmough, and M. Mattina, ‚ÄúSystolic Tensor Array:\nAn EfÔ¨Åcient Structured-sparse GEMM Accelerator for Mobile CNN\nInference,‚Äù IEEE Computer Architecture Letters, vol. 19, no. 1, pp. 34‚Äì\n37, 2020.\n[28] A. Parashar, M. Rhu, A. Mukkara, A. Puglielli, R. Venkatesan,\nB. Khailany, J. Emer, S. W. Keckler, and W. J. Dally, ‚ÄúSCNN: An\nAccelerator for Compressed-sparse Convolutional Neural Networks,‚Äù in\nProceedings of the 44th Annual International Symposium on Computer\nArchitecture (ISCA), 2017, pp. 27‚Äì40.\n[29] B. Li, S. Pandey, H. Fang, Y . Lyv, J. Li, J. Chen, M. Xie, L. Wan, H. Liu,\nand C. Ding, ‚ÄúFTRANS: Energy-EfÔ¨Åcient Acceleration of Transformers\nusing FPGA,‚Äù in Proceedings of the ACM/IEEE International Sympo-\nsium on Low Power Electronics and Design (ISLPED) , 2020.\n[30] T. J. Ham, S. J. Jung, S. Kim, Y . H. Oh, Y . Park, Y . Song, J.-H.\nPark, S. Lee, K. Park, J. W. Lee et al. , ‚ÄúA 3: Accelerating Attention\nMechanisms in Neural Networks with Approximation,‚Äù in 2020 IEEE\nInternational Symposium on High Performance Computer Architecture\n(HPCA), 2020.\n[31] H. Wang, Z. Zhang, and S. Han, ‚ÄúSpAtten: EfÔ¨Åcient Sparse Attention\nArchitecture with Cascade Token and Head Pruning,‚Äù in 2021 IEEE\nInternational Symposium on High-Performance Computer Architecture\n(HPCA), 2021.\n[32] L. Lu, Y . Jin, H. Bi, Z. Luo, P. Li, T. Wang, and Y . Liang, ‚ÄúSanger:\nA Co-Design Framework for Enabling Sparse Attention using Recon-\nÔ¨Ågurable Architecture,‚Äù in Proceedings of the 54th Annual IEEE/ACM\nInternational Symposium on Microarchitecture (MICRO) , 2021.\n[33] H. Peng, S. Huang, T. Geng, A. Li, W. Jiang, H. Liu, S. Wang,\nand C. Ding, ‚ÄúAccelerating Transformer-based Deep Learning Models\non FPGAs using Column Balanced Block Pruning,‚Äù in 2021 22nd\nInternational Symposium on Quality Electronic Design (ISQED) , 2021.\n[34] T. Lin, S. U. Stich, L. Barba, D. Dmitriev, and M. Jaggi, ‚ÄúDynamic\nmodel pruning with feedback,‚Äù arXiv preprint arXiv:2006.07253 , 2020.\n[35] T. HoeÔ¨Çer, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste, ‚ÄúSparsity\nin deep learning: Pruning and growth for efÔ¨Åcient inference and training\nin neural networks,‚Äù Journal of Machine Learning Research (JMLR) ,\nvol. 22, no. 241, pp. 1‚Äì124, 2021.\n[36] S. Zhang, Z. Du, L. Zhang, H. Lan, S. Liu, L. Li, Q. Guo, T. Chen, and\nY . Chen, ‚ÄúCambricon-X: An accelerator for sparse neural networks,‚Äù in\n2016 49th Annual IEEE/ACM International Symposium on Microarchi-\ntecture (MICRO). IEEE, 2016, pp. 1‚Äì12.\n[37] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman,\n‚ÄúGLUE: A Multi-Task Benchmark and Analysis Platform for Natural\nLanguage Understanding,‚Äù in Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP, 2018, pp. 353‚Äì355.\n[38] X. Jiao, Y . Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu,\n‚ÄúTinyBERT: Distilling BERT for Natural Language Understanding,‚Äù in\nProceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP): Findings , 2020, pp. 4163‚Äì4174.\n[39] M. Caron, H. Touvron, I. Misra, H. J ¬¥egou, J. Mairal, P. Bojanowski,\nand A. Joulin, ‚ÄúEmerging Properties in Self-Supervised Vision Trans-\nformers,‚Äù in Proceedings of the International Conference on Computer\nVision (ICCV), 2021.\n[40] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi,\nP. Cistac, T. Rault, R. Louf, M. Funtowicz et al., ‚ÄúHuggingface‚Äôs Trans-\nformers: State-of-the-art Natural Language Processing,‚Äù arXiv preprint\narXiv:1910.03771, 2019.\n[41] M. B. Taylor, ‚ÄúBasejump STL: SystemVerilog Needs a Standard Tem-\nplate Library for Hardware Design,‚Äù in 2018 55th ACM/ESDA/IEEE\nDesign Automation Conference (DAC) . IEEE, 2018, pp. 1‚Äì6.\n[42] D. Rossi, F. Conti, A. Marongiu, A. Pullini, I. Loi, M. Gautschi, G. Tagli-\navini, A. Capotondi, P. Flatresse, and L. Benini, ‚ÄúPULP: A Parallel Ultra\nLow Power Platform for Next Generation IoT Applications,‚Äù in 2015\nIEEE Hot Chips 27 Symposium (HCS) . IEEE, 2015, pp. 1‚Äì39.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7020137906074524
    },
    {
      "name": "Speedup",
      "score": 0.5956673622131348
    },
    {
      "name": "Scalability",
      "score": 0.5004477500915527
    },
    {
      "name": "Computer engineering",
      "score": 0.4790045917034149
    },
    {
      "name": "Sparse matrix",
      "score": 0.46482375264167786
    },
    {
      "name": "Algorithm",
      "score": 0.4633197486400604
    },
    {
      "name": "Transformer",
      "score": 0.4468371868133545
    },
    {
      "name": "Parallel computing",
      "score": 0.4195987284183502
    },
    {
      "name": "Computer hardware",
      "score": 0.3821234405040741
    },
    {
      "name": "Gaussian",
      "score": 0.1378690004348755
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ]
}