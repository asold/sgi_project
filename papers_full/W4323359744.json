{
  "title": "EHR foundation models improve robustness in the presence of temporal distribution shift",
  "url": "https://openalex.org/W4323359744",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2912170880",
      "name": "Lin Lawrence Guo",
      "affiliations": [
        "Hospital for Sick Children",
        "SickKids Foundation",
        "Institute for Clinical Evaluative Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2181729714",
      "name": "Ethan Steinberg",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4224515476",
      "name": "Scott Lanyon Fleming",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2484832969",
      "name": "José Posada",
      "affiliations": [
        "Universidad del Norte"
      ]
    },
    {
      "id": "https://openalex.org/A4224515477",
      "name": "Joshua Lemmon",
      "affiliations": [
        "SickKids Foundation",
        "Hospital for Sick Children",
        "Institute for Clinical Evaluative Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2885265381",
      "name": "Stephen R Pfohl",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2659277696",
      "name": "Nigam Shah",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4222399703",
      "name": "Jason Fries",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2169865483",
      "name": "Lillian Sung",
      "affiliations": [
        "Hospital for Sick Children",
        "SickKids Foundation",
        "Institute for Clinical Evaluative Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2912170880",
      "name": "Lin Lawrence Guo",
      "affiliations": [
        "Institute for Clinical Evaluative Sciences",
        "Hospital for Sick Children",
        "SickKids Foundation"
      ]
    },
    {
      "id": "https://openalex.org/A2181729714",
      "name": "Ethan Steinberg",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4224515476",
      "name": "Scott Lanyon Fleming",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2484832969",
      "name": "José Posada",
      "affiliations": [
        "Universidad del Norte"
      ]
    },
    {
      "id": "https://openalex.org/A4224515477",
      "name": "Joshua Lemmon",
      "affiliations": [
        "SickKids Foundation",
        "Institute for Clinical Evaluative Sciences",
        "Hospital for Sick Children"
      ]
    },
    {
      "id": "https://openalex.org/A2885265381",
      "name": "Stephen R Pfohl",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2659277696",
      "name": "Nigam Shah",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4222399703",
      "name": "Jason Fries",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2169865483",
      "name": "Lillian Sung",
      "affiliations": [
        "SickKids Foundation",
        "Institute for Clinical Evaluative Sciences",
        "Hospital for Sick Children"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2784499877",
    "https://openalex.org/W2028138594",
    "https://openalex.org/W4221077687",
    "https://openalex.org/W3174786846",
    "https://openalex.org/W3177617320",
    "https://openalex.org/W2604834158",
    "https://openalex.org/W1914974178",
    "https://openalex.org/W2079776874",
    "https://openalex.org/W3196529262",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3112116031",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W3004227146",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2157365260",
    "https://openalex.org/W3043023066",
    "https://openalex.org/W2799695199",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2965570621",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2954707123",
    "https://openalex.org/W2967329333",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W3017637887",
    "https://openalex.org/W3098949126",
    "https://openalex.org/W1126991912"
  ],
  "abstract": "Abstract Temporal distribution shift negatively impacts the performance of clinical prediction models over time. Pretraining foundation models using self-supervised learning on electronic health records (EHR) may be effective in acquiring informative global patterns that can improve the robustness of task-specific models. The objective was to evaluate the utility of EHR foundation models in improving the in-distribution (ID) and out-of-distribution (OOD) performance of clinical prediction models. Transformer- and gated recurrent unit-based foundation models were pretrained on EHR of up to 1.8 M patients (382 M coded events) collected within pre-determined year groups (e.g., 2009–2012) and were subsequently used to construct patient representations for patients admitted to inpatient units. These representations were used to train logistic regression models to predict hospital mortality, long length of stay, 30-day readmission, and ICU admission. We compared our EHR foundation models with baseline logistic regression models learned on count-based representations (count-LR) in ID and OOD year groups. Performance was measured using area-under-the-receiver-operating-characteristic curve (AUROC), area-under-the-precision-recall curve, and absolute calibration error. Both transformer and recurrent-based foundation models generally showed better ID and OOD discrimination relative to count-LR and often exhibited less decay in tasks where there is observable degradation of discrimination performance (average AUROC decay of 3% for transformer-based foundation model vs. 7% for count-LR after 5–9 years). In addition, the performance and robustness of transformer-based foundation models continued to improve as pretraining set size increased. These results suggest that pretraining EHR foundation models at scale is a useful approach for developing clinical prediction models that perform well in the presence of temporal distribution shift.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:3767  | https://doi.org/10.1038/s41598-023-30820-8\nwww.nature.com/scientificreports\nEHR foundation models improve \nrobustness in the presence \nof temporal distribution shift\nLin Lawrence Guo 1, Ethan Steinberg 2, Scott Lanyon Fleming 2, Jose Posada 3, \nJoshua Lemmon 1, Stephen R. Pfohl 2, Nigam Shah 2, Jason Fries 2,5 & Lillian Sung 1,4,5*\nTemporal distribution shift negatively impacts the performance of clinical prediction models over \ntime. Pretraining foundation models using self-supervised learning on electronic health records \n(EHR) may be effective in acquiring informative global patterns that can improve the robustness \nof task-specific models. The objective was to evaluate the utility of EHR foundation models in \nimproving the in-distribution (ID) and out-of-distribution (OOD) performance of clinical prediction \nmodels. Transformer- and gated recurrent unit-based foundation models were pretrained on EHR \nof up to 1.8 M patients (382 M coded events) collected within pre-determined year groups (e.g., \n2009–2012) and were subsequently used to construct patient representations for patients admitted \nto inpatient units. These representations were used to train logistic regression models to predict \nhospital mortality, long length of stay, 30-day readmission, and ICU admission. We compared our EHR \nfoundation models with baseline logistic regression models learned on count-based representations \n(count-LR) in ID and OOD year groups. Performance was measured using area-under-the-receiver-\noperating-characteristic curve (AUROC), area-under-the-precision-recall curve, and absolute \ncalibration error. Both transformer and recurrent-based foundation models generally showed better \nID and OOD discrimination relative to count-LR and often exhibited less decay in tasks where there is \nobservable degradation of discrimination performance (average AUROC decay of 3% for transformer-\nbased foundation model vs. 7% for count-LR after 5–9 years). In addition, the performance and \nrobustness of transformer-based foundation models continued to improve as pretraining set size \nincreased. These results suggest that pretraining EHR foundation models at scale is a useful approach \nfor developing clinical prediction models that perform well in the presence of temporal distribution \nshift.\nThe large increase in the adoption of electronic health records (EHR) has enabled the use of machine learning \nto develop highly performant clinical prediction models that have the potential to improve the care of  patients1. \nHowever, the non-stationary healthcare environment can bring about changes in the data distribution between \nmodel development and  deployment2, which can degrade the model’s performance over  time3 and consequently \nits clinical  utility4. In this study, we explored temporal distribution shift alongside the suitability of founda -\ntion models5—deep neural networks trained on large-scale unlabeled data using self-supervised learning—and \nwhether they can be adapted via transfer learning to improve the robustness of clinical prediction models in the \npresence of temporal distribution shift.\nThe cause of temporal distribution shift in clinical medicine is often  subtle6 and the extent of its impact on \nmodel performance is heterogeneous across  tasks3,7–9. Nonetheless, the consequence of the impact on patient \ncare and physician’s trust can be severe. An example is the widely implemented Epic sepsis model developed on \ndata collected between 2013 and 2015 that performed below expectation when evaluated at Michigan Medicine \non data collected between 2018 and 2019 and resulted in a large number of spurious  alerts4.\nRecent approaches that mitigate the impact of temporal distribution shift on model performance in clini -\ncal medicine largely rely on model monitoring and updating policies that do not leverage the entire patient \nOPEN\n1Program in Child Health Evaluative Sciences, The Hospital for Sick Children, Toronto, ON, Canada. 2Stanford \nCenter for Biomedical Informatics Research, Stanford University, Palo Alto, CA, USA. 3Universidad del Norte, \nBarranquilla, Colombia. 4Division of Haematology/Oncology, The Hospital for Sick Children, 555 University Avenue, \nToronto, ON M5G1X8, Canada. 5These authors jointly supervised this work: Jason Fries and Lillian Sung.  *email: \nLillian.sung@sickkids.ca\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:3767  | https://doi.org/10.1038/s41598-023-30820-8\nwww.nature.com/scientificreports/\npopulation  available10. In addition, proactive approaches using domain generalization and adaptation have shown \nlittle to no  success3.\nWhile recent work on medical foundation models has focused on improving sample complexity when fine-\ntuning, little-to-no work has measured a pretrained, medical foundation model’s impact on temporal robustness \nin clinical prediction tasks. Findings from domains outside of clinical medicine suggest significant  performance11 \nand  robustness12,13 benefits to pretraining foundation models, and these benefits tend to increase with  scale14,15. \nAnother major benefit of foundation models is their ability to generalize to tasks not seen during  training16. \nIn this study, we adopt EHR foundation models—deep neural networks pretrained on EHR-derived patient \ntimelines using self-supervised learning. Patient timelines consist of structured medical codes ordered by time, \nwhere each code (e.g., M32.9 for “lupus erythematosus”) functions as a word drawn from a finite vocabulary \ndefined by medical ontologies such as ICD10. This formulation enables using autoregressive sequence modeling, \na self-supervised learning objective used in natural language processing, to train an EHR foundation model by \npredicting the next day’s codes. The resulting pretrained model is then used to generate feature representations for \ndownstream tasks. The foundation modeling approach in this study is referred to as clinical language model based \nrepresentations (CLMBR)17. Transfer of the structure learned by CLMBR from the entire patient population to \ndownstream clinical prediction models have demonstrated performance benefits compared to standard baselines \nincluding count-based models, especially when the number of patient records was  small17. CLMBR’s architec-\nture aligns with other EHR foundation models, such as Med-BERT 18 and  BEHRT19, but uses an autoregressive \ninstead of masked language modeling objective for pretraining to match the next-day prediction task. We refer \nto CLMBR as an EHR foundation model because of its potential to shift practice in the development of machine \nlearning models for clinical medicine. We focus specifically on the implications for temporal robustness when \nadapting task-specific models from a shared, self-supervised model trained on a patient population. However, \nwe recognize that scale (both in parameter count and training data size) is a key aspect of modern foundation \nmodels and that structured EHR models are currently much smaller than their counterparts in language and \nvision. For example, GPT-316 has 175 billion parameters compared to 42 million for the largest CLMBR model \nused in this study. Thus our work is more akin to early NLP foundation models such as  BERT11 or  ELMO20.\nWe evaluated the utility of CLMBR in mitigating the impact of temporal distribution shift on model per -\nformance. We hypothesized that the global patterns embedded in CLMBR can be adapted into models that \nperform better than count-based representations in out-of-distribution (OOD) years. For adaptation, we froze \nCLMBR weights and trained a classification head using logistic regression (CLMBR-LR) for each downstream \nclinical prediction task. The primary objective was to compare the in-distribution (ID) and OOD performance \nof CLMBR-LR to models trained on count-based representations. In addition, we examined the contribution \nof CLMBR pretraining and characterized the performance and robustness of different architecture choices for \nCLMBR (gated recurrent units and transformers) as well as how the performance of each architecture scales \nwith increasing quantity of pretraining data.\nMethods\nData source. We used data from the STAnford medicine Research data Repository (STARR) 21. Data in \nSTARR are routinely collected in the EHR of Stanford Medical Center, comprised of Stanford Health Care (pri-\nmarily adult-directed care) and Lucile Packard Children’s Hospital (primarily pediatric-directed care). These \ndata are mapped to the Observational Medical Outcomes Partnership Common Data Model (OMOP-CDM), \nwhich facilitates multi-center observational  research22,23. This study used de-identified data in which dates \nwithin a patient timeline were jittered by up to 30 days, but their temporal relations remained intact. Because of \nde-identification, there was no requirement for Institutional Review Board approval and informed consent by \nStanford Medical Center. Instead, access to data is restricted and subject to the data use  agreement21. The use of \nthis data and methods were carried out in accordance with relevant guidelines and regulations.\nCohorts. We defined two types of cohorts: pretraining and task-specific. Pretraining cohorts contained \npatients on which CLMBR was pretrained using the self-supervised, autoregressive objective. Several pretraining \ncohorts were defined according to the experimental setups and differ in the number of patients included—from \n36 K (42 M coded events) to 1.8 M (382 M coded events)—and the year range in which patient encounter days \nwere considered for pretraining (see Fig.  2 and flow diagrams of patient cohort allocation in Supplementary \nMethods online for details). Note that while it is possible for a patient to have encounters beyond the year \nrange specified for the pretraining cohort, these were excluded for pretraining. In addition, to be included in a \npretraining cohort the patient must have had at least 3 patient days (of any encounter type) with clinical events \nduring the time window defined by the year range.\nThe task-specific cohort contained patients on which classification heads for clinical prediction tasks were \ntrained and evaluated. This cohort included adult patients over the age of 18 with admissions to the inpatient \nunit between EHR inception (2009) and August 22, 2021. Admissions to the inpatient unit were either direct or \ntransfers from the emergency department. Encounters with clinic visits alone and encounters in which patient \ndeath or discharges occurred on the same day of admission were excluded. For patients with multiple admissions, \none was randomly selected as the index admission for all tasks. Note that patients in the task-specific cohort may \noverlap with the pretraining cohorts, however patients in the validation and test sets of the task-specific cohort \nwere excluded from all pretraining cohorts in order to prevent data leakage.\nOutcomes. We defined four clinical outcomes that are relevant for inpatient admissions for each patient in \nthe task-specific cohort. Hospital mortality was defined as a patient death occurring during the index admission. \nLong length of stay (LOS) (long LOS) was defined as an index admission of seven or more days. Readmission in \n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:3767  | https://doi.org/10.1038/s41598-023-30820-8\nwww.nature.com/scientificreports/\n30 days (30-day readmission) was defined as a readmission to an inpatient unit within 30 days after discharge. \nIntensive care unit (ICU) admission was defined as a patient transfer to the intensive care unit during the index \nadmission. Each outcome was considered as a binary classification task where the prediction time was set as \n11:59PM on the first day of the index admission for the hospital mortality, long LOS and ICU admission tasks, \nand 11:59PM on the day of discharge for the 30-day readmission prediction task. For the 30-day readmission \ntask, we removed patients who were re-admitted on the day of discharge, and for the ICU admission task, we \nremoved patients transferred on the first day of the index admission since these events would have occurred \nbefore prediction time.\nPatient representations. EHR data corresponding to a particular patient can be treated as a sequence of \ndays that is ordered by time, d 1 … dN, where each day consists of a set of events represented by medical codes \nsuch as diagnoses, lab tests, procedures, and medication administrations or prescriptions as examples. In this \nstudy, we considered two approaches to construct patient representations over the patient timelines as illustrated \nin Fig. 1: count-based representations and CLMBR.\nCount based representations. The count-based representations were constructed for patients in the task-spe-\ncific cohort using an open-source count-based  featurizer24 which follows standard practices for patient count-\nbased  featurization1,25. This approach constructed patient representations as binary features based on counts of \nboth unique OMOP CDM concepts and derived elements recorded prior to the time of prediction. The feature \nset consisted of demographic and clinical features. Demographic features included sex at birth, race, ethnicity, \nand age at admission discretized into 5-year intervals. Clinical features were constructed as the concatenation \nof the results of a time-dependent extraction procedure applied independently to data elements recorded in \ntime bins defined relative to the time of prediction. The time bins were as follows: 24 h prior, 1–7 days prior, \n8–30 days prior, and 31 days-any time prior. The time-dependent extraction procedure identified all unique \nOMOP CDM concepts from the following OMOP CDM tables: condition occurrence (diagnosis codes), drug \nexposure (administration or prescription of medications), procedure occurrence, measurement (includes labo-\nratory tests), device exposure (exposure to implantable objects, medical equipment, supplies, and instruments) \nand observation (non-standardized tests or clinical observations). Continuous measurement results were rep-\nFigure 1.  An overview of the two approaches of constructing patient representations used in this study. The \npurple box in the construction of count-based representations represents the reference range comparison and \nbinary feature construction procedures for a specific time-bin. The construction of CLMBR illustrates the self-\nsupervised pretraining stage, hence the inclusion of the self-supervised learning objective. The adaptation of \nCLMBR to specific tasks (e.g., for predicting hospital mortality) does not include the self-supervised learning \nobjective. In addition, during adaptation CLMBR weights were frozen, and a separate classification head is \nlearned on the same patient representations for each clinical prediction task. CLMBR Clinical language model-\nbased representations.\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:3767  | https://doi.org/10.1038/s41598-023-30820-8\nwww.nature.com/scientificreports/\nresented as binary indicators for abnormal results for each measurement on the basis of whether the result was \nabove or below the reference range in the EHR.\nClinical language model-based representations—CLMBR. The core idea behind CLMBR is that if a sequence \nmodel is able to predict sets of medical codes over a patient timeline, then that model may have discovered \ninformative global patterns that can be re-used in various other clinical prediction tasks. Note that the term \n“language model” in CLMBR merely reflects the similarity in the computations involved between sequence \nmodeling of medical codes and language modeling, and therefore does not indicate natural language processing \nof any kind.\nFirst, we mapped clinical codes for labs, medications, diagnoses, and procedures to a finite vocabulary of dis-\ncrete symbols. This vocabulary was then mapped into a clinical ontology to reduce sparsity and used to construct \npatient sequences for the CLMBR encoder. The medical codes were obtained from the same OMOP CDM tables \nas used for count-based representations except for the observation table. The Unified Medical Language System \n(UMLS)26 was used to extend each medical code to the set of parents in its native ontology when applicable \n(ICD10 for diagnoses, CPT or MTHH for procedures, and ATC for medications). For instance, the occurrence \nof the ICD10 code “H61.23” for the diagnosis of impacted cerumen, bilateral, resulted in two additional parent \ncodes, namely “H61.2” (impacted cerumen) and “H61” (other disorders of external ear).\nWe used a transformer and GRU as the architectures for our sequence models as they may exhibit differ -\nent scaling properties and have each demonstrated success in the sequence modeling of medical codes in the \n EHR17–19,27,28. To construct patient representations, sets of codes for each day in the patient timeline were first \npassed through the embedding bag layer of the networks, which computes the mean code embedding for each \nday. Next, each mean embedding was concatenated with a vector that captured time information including the \npatient’s age on that day, the time delta from the previous day, whether that day was the first day of the sequence, \nand the log transform of the age and time delta. These vectors preserve the varying temporal intervals between \npatient days and thereby enable CLMBR to fully leverage temporal  information29. For the transformer architec-\nture, we additionally concatenated two encodings: the first is a standard sine and cosine positional  encoding11 \nfor the day offset, and the second is the encoding of patient age. Patient representations were then computed by \nfeeding the concatenated vectors into the transformer or GRU, followed by a linear layer with output size equal \nto the number of dimensions of the patient representation, which was set to 800 in this study.\nTo predict the set of codes for a given day, di, the patient representation from the previous day, di−1, was used. \nWe formulated the set prediction problem as a series of independent binary classification problems, where the \nprobability of a given code was computed via a sigmoid transformation of the dot product between the code \nembedding and the patient representation. To deal with the computational complexity of the matrix product \ninduced by the large code space, we used a variant of the hierarchical softmax  optimization30 in which we replaced \nthe softmax transformations with sigmoid transformations. The hierarchical structures of the code space were \nthe same as the ones used for ontology extension (e.g., the hierarchical structure in the ICD10 vocabulary for \nICD10 codes). We used the binary cross entropy loss as the loss function during training.\nOnce the sequence models were trained, we froze their weights and used them to construct representations \n(the output of the linear layer) for each patient in the task-specific cohort to be used by downstream models for \nclinical prediction tasks. For hospital mortality, long LOS, and ICU admissions, patient representations were \nobtained up until the day of admission, whereas for 30-day readmission patient representations were obtained \nup until the day of discharge.\nExperimental setup. First, we conducted a baseline experiment to establish count-based model perfor -\nmance for each of the four clinical prediction tasks and to investigate whether model performance degraded over \ntime as a result of temporal distribution shift. We trained logistic regression models on count-based representa-\ntions constructed for patients admitted between 2009 and 2012 (count-LR) and evaluated the models on all years \nfrom 2009 to 2021. The years on which the models were trained (2009–2012) constituted the ID years and the \nsubsequent years (2013–2021) constituted the OOD years for the baseline experiment. We also included oracle \nmodels that were trained and evaluated on each of the OOD years for comparison.\nNext, in Experiment 1, we compared ID and OOD performance of CLMBR-LR to count-LR. For this experi-\nment, ID years were 2009–2012 and the OOD years were 2013–2016 and 2017–2021. To gain insight into relative \nperformance, we subtracted each model’s OOD performance in 2013–2016 and 2017–2021 by its ID performance \nin 2009–2012 for the same representation construction and modeling approach. As a sensitivity analysis, we \ntrained and compared task-specific models on count-based representations and CLMBR using light gradient-\nboosted machines (LightGBM) instead of logistic regression.\nWe additionally conducted analyses in Experiment 1 to examine the contribution of CLMBR pretraining. \nThe first analysis compared CLMBR-LR to end-to-end neural networks (ETE) with the same architecture (that \nis, GRU and transformer) with the hypothesis that CLMBR-LR should perform as well as or better than its ETE \ncounterpart. The second analysis investigated the Pearson correlation between each CLMBR model’s pretraining \nperformance and the downstream logistic regression performance in each clinical prediction task. Performance \nfor this analysis was measured using binary cross-entropy loss.\nIn Experiment 2, we examined whether performance and robustness of CLMBR would improve with scale \nand whether the two CLMBR architectures—GRU and transformer—scale differently. While language founda-\ntion models have been scaled along various factors such as model size, pretraining set size, and the amount of \n compute15,31, here we focused on scaling pretraining set size, from 36 K to 1.8 M patients. The flow diagrams of \npatient cohort allocation in Supplementary Methods online and Fig. 2 illustrate patient selection and the number \nof patients in each pretraining set size. We acknowledge that larger pretraining sets contained data that were also \n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:3767  | https://doi.org/10.1038/s41598-023-30820-8\nwww.nature.com/scientificreports/\ncloser in time to the task-specific test sets (for instance the largest pretraining set spanned 2009 to 2019 while the \nsmallest pretraining set spanned 2009 to 2012). The proximity to task-specific test sets may account for some of \nthe differences in performance across pretraining sets. However, both architectures were affected the same way \nand we were interested in whether there was a difference between the architectures in their performance across \nthese pretraining sets. For this experiment, task-specific logistic regression models were trained on count-based \nrepresentations and CLMBR constructed for patients admitted between 2009 and 2019 and were evaluated on \npatients admitted between 2009 and 2012 (ID) and 2020–2021 (OOD).\nFinally, to aid clinical interpretation of the changes in performance between count-LR and CLMBR-LR, we \nquantified the numbers of decisions that would have been affected if CLMBR-LR was used instead of count-LR \nfor tasks in which performance degraded over time. Specifically, we selected the best CLMBR-LR and calculated \nthe proportion of patients that would have been classified correctly and incorrectly with CLMBR-LR instead of \ncount-LR across various risk thresholds.\nModel development. Figure 2 illustrates the dataset slices for each cohort. We extracted count-based represen-\ntations and CLMBR for each patient in the task-specific cohort. For count-based representations, we additionally \npruned features with less than 25 observations in the training set for each task separately. We then pruned the \nsame features from the validation and test sets. To tune CLMBR, we performed grid search over the hyperpa-\nrameter settings in Experiment 1 and subsequently used the selected hyperparameter setting for each CLMBR \narchitecture in Experiment 2. The hyperparameters consisted of learning rate, dropout rate, and batch size. \nSupplementary Methods online detail the hyperparameter grid and the selected hyperparameter settings for \ntransformer-based CLMBR (see Supplementary GRU Experiment online for details on GRU-based CLMBR). \nFor hyperparameter selection, we trained CLMBR on the timelines of 80% of the patients in the pretraining set, \nand selected hyperparameter settings based on model performance in the left out 20% of the pretraining set. \nOnce pretrained, we constructed CLMBR representations for each patient in the task-specific cohort.\nAfter computing the representations, we trained logistic regression with L2-regularization on count based \nand CLMBR representations for each clinical outcome in the task-specific training set of 2009–2012 in Experi-\nment 1, and 2009–2019 in Experiment 2. Hyperparameter tuning was done on L2 regularization strength, which \nranged from  10−6 to  102 in increments of powers of 10. We selected hyperparameter values based on the model’s \nbinary cross entropy loss in the task-specific validation set of the same year group as the training set. Oracle \nmodels for each OOD year (2013–2021) as comparisons for count-LR in the baseline experiment were trained \non count-based representations in the task-specific training set of the OOD year, and hyperparameters were \nselected based on performance in the task-specific validation set of the OOD year. Finally, the ETE models for \nExperiment 1 were trained for each clinical outcome separately on the task-specific training set of 2009–2012, \nand hyperparameter tuning was conducted using a similar grid as CLMBR. We selected hyperparameter settings \nfor ETE based on model performance in the task-specific validation set of 2009–2012. Supplementary Methods \nonline provide details on the selected hyperparameter setting for logistic regression and ETE for each clinical \nprediction task.\nFigure 2.  Y ear range for cohort selection and the dataset size for CLMBR pretraining and task-specific \nmodels. The Venn diagram (top left) illustrates the overlap between patients in the pretraining cohorts and the \ntraining set of the task-specific cohort. The asterisk in CLMBR [09–12*] indicates a subset of CLMBR [09–12] \nand only included patients who were admitted to an inpatient unit between 2009 and 2012. Flow diagrams of \npatient cohort allocation in supplementary Methods online detail the assignment of patients into each cohort \nand the data splitting procedure. Note that the datasets for the task-specific cohort may vary in the number \nof patients across clinical outcomes due to additional exclusion of patients for each outcome. For instance, for \nICU admission predictions we excluded patients who were transferred on the first day of the index admission \nsince these events would have occurred before prediction time. CLMBR Clinical language model-based \nrepresentation; LR Logistic regression; ID In-distribution; OOD Out-of-distribution.\n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:3767  | https://doi.org/10.1038/s41598-023-30820-8\nwww.nature.com/scientificreports/\nCLMBR and ETE were implemented using  Pytorch32 and were trained on a cluster of Nvidia V100 GPUs. \nWe used the Sci-kit Learn’s33 implementation of logistic regression. Analyses were implemented in Python 3.834.\nModel evaluation. We evaluated each model’s discrimination performance in the task-specific test sets using \nthe area-under-the-receiver-operating-characteristic curve (AUROC) and the calibrated area-under-the-preci-\nsion-recall curve  (AUPRCC)35.  AUPRCC computes the precision using a reference outcome prevalence, here set \nas the prevalence in the ID year group 2009–2012. Thus,  AUPRCC is invariant to change in outcome prevalence \nin OOD years and allows us to better interpret its variation over time. We used the absolute calibration error \n(ACE)24 as a measure of calibration. ACE is similar to the integrated calibration  index36 but applies a logistic \nregression estimator to the logit of the predicted probability outputs rather than locally weighted least squares \nand is thus more computationally efficient. To evaluate the scaling of CLMBR to pretraining set size in Experi-\nment 2, we obtained the slope of the regression line fitted on the performance of each CLMBR architecture along \nthe various pretraining set sizes separately for each performance metric, task, and year group.\nStatistical analysis. For each metric, we computed the median and 95% confidence interval (CI) of the distribu-\ntion over performance in the task-specific test set obtained from 1000 bootstrap samples. To compare models, \nwe computed the 95% CI of their differences between a pair of models over 1000 bootstrap samples. Statistical \nsignificance was defined as comparisons where the 95% CI did not cross 0.\nEthics approval and consent to participate. This study used de-identified data and so the requirement \nfor Institutional Review Board approval and participant informed consent were waived by Stanford Medical \nCenter.\nResults\nSupplementary Table S1 online presents task-specific cohort characteristics for each year and outcome preva-\nlence. Figure 3 shows the impact of temporal distribution shift on performance (AUROC,  AUPRCC, and ACE) \nof count-LR in the baseline experiment. Model degradation occurred in the OOD years (2013–2021) for long \nLOS and ICU admission prediction tasks, with larger degradations observed in 2017–2021.\nFigure  4 shows the comparison of ID (2009–2012) and OOD (2013–2016 and 2017–2021) performance \nbetween CLMBR-LR and count-LR in Experiment 1 (see Supplementary Tables S2 and S3 online for raw per -\nformance scores and relative OOD performance, respectively). For brevity, we display and describe transformer-\nbased CLMBR results. GRU-based CLMBR results are qualitatively similar and can be found in Supplementary \nGRU Experiment online. First, CLMBR-LR outperformed count-LR in discrimination performance in both \nID and OOD year groups across all tasks except for 30-day readmission. In addition, CLMBR-LR displayed \nless degradation in AUROC (mean decay of 1.3% vs. 4.5% for count-LR) and  AUPRCC (mean decay of 1.6% \nvs. 8.9% for count-LR) for long LOS and in AUROC in the 2017–2021 year group for ICU admission(3.3% vs. \n8.0% for count-LR), whereas count-LR displayed less degradation in AUROC in the 2017–2021 year group for \n30-day readmission (4.4% vs. 7.3% for CLMBR-LR). Calibration results were heterogeneous. Furthermore, the \nsensitivity analysis where we trained and compared task-specific models on count-based representations and \nFigure 3.  The impact of temporal distribution shift on the performance (AUROC,  AUPRCC, and ACE) of \nlogistic regression models trained on count-based representations (count-LR). Shaded regions indicate time \nwindows in which performance in out-of-distribution years (2013–2021) is worse (red) or better (green) \nthan performance in the in-distribution year group (2009–2012). A Larger red shaded region indicates more \ndegradation relative to the model’s in-distribution performance. Oracle models were trained and evaluated on \neach of the out-of-distribution years. Error bars indicate 95% confidence interval obtained from 1000 bootstrap \niterations. AUROC Area under the receiver operating characteristics curve; AUPRCC Calibrated area under the \nprecision recall curve; ACE Absolute calibration error; LOS Length of stay; ICU Intensive care unit.\n7\nVol.:(0123456789)Scientific Reports |         (2023) 13:3767  | https://doi.org/10.1038/s41598-023-30820-8\nwww.nature.com/scientificreports/\nCLMBR using LightGBM instead of logistic regression showed qualitatively similar findings (Supplementary \nLightGBM Experiment online).\nTo examine the contribution of CLMBR pretraining, comparison of transformer-based CLMBR-LR and ETE \nin ID and OOD year groups in Supplementary Figure S1 online shows that CLMBR-LR performed as well as or \nbetter than its ETE counterpart in all tasks and metrics except for OOD calibration in 2017–2021 for long LOS \nand hospital mortality. Supplementary Figure S2 online plots the average binary cross entropy loss of CLMBR \nsequence models (trained using various hyperparameter settings) in the pretraining validation set against the \naverage binary cross entropy loss of their downstream logistic regression models in each task and year group. \nThe performance of CLMBR models had strong positive correlations (Pearson correlation coefficient) with the \nID and OOD performance of their downstream logistic regression models in long LOS and ICU admission, and \nweak to strong positive correlation with hospital mortality.\nFigure 5 shows the ID and OOD performance of GRU-based and transformer-based CLMBR as a function of \npretraining set size (see Supplementary Tables S6 and S7 online for raw performance scores and slopes, respec-\ntively). Generally, GRU-based CLMBR performed better with smaller pretraining set sizes whereas transformer-\nbased CLMBR exhibited better scaling of discrimination performance to pretraining set size, outperforming \nGRU-based CLMBR at larger pretraining set sizes. Scaling of calibration was heterogeneous.\nFigure 6 plots the proportion of patients re-classified differently using CLMBR-LR instead of count-LR in \nlong LOS and ICU admission. For this analysis, we selected the transformer-based CLMBR pretrained on the \n2009–2019 pretraining cohort, and logistic regression models were trained on admissions from 2009 to 2019. \nThere were more correct re-classifications than incorrect re-classifications across risk thresholds and year groups \nfor long LOS and ICU admission.\nDiscussion\nWe observed count-LR models resulted in large performance degradation over time for some tasks, namely long \nLOS and ICU admission. CLMBR-LR generally displayed better discrimination than count-LR in ID and OOD \nyear groups (but could result in worse calibration) and often exhibited less decay in tasks where there is observ-\nable degradation of discrimination performance. In addition, whereas GRU-based CLMBR generally performed \nbetter with a smaller pretraining set size, the discrimination performance and robustness of transformer-based \nCLMBR improved more as pretraining set size increased, surpassing GRU-based CLMBR.\nLarge-scale self-supervised pretraining takes place less frequently and centralizes as well as standardizes \nmodel training and feature generation compared to the training of end-to-end models from scratch. Conse-\nquently, machine learning practitioners are able to focus on rapid adaptation of foundation models to down-\nstream tasks. This research paper contributes more evidence that this approach brings not only performance ben-\nefits over traditional count-based models, but robustness benefits in the presence of temporal distribution shifts. \nThese benefits decrease the need for model retraining and preserves the clinical utility of models deployed into \npractice. Whether the potential for worse calibration is clinically meaningful will depend on the specific use case.\nDeveloping intrinsic measures of upstream model quality (e.g., measuring language modeling perplexity) that \nreliably correlate with downstream task accuracy is of great interest and utility to the machine learning commu-\nnity. In NLP , recent work has found poor correlation between perplexity and downstream task  accuracy37. How-\never, in our setting, we observed strong correlations between performance of the upstream language modeling \nobjective (measured using log loss) and the performance of classification heads in some tasks, but not all. This \nFigure 4.  Performance of transformer-based CLMBR-LR and count-LR in the in-distribution (ID) year group \nand their decay (shaded regions) in out-of-distribution (OOD) year groups. A larger shaded region indicates \nmore performance degradation. GRU-based CLMBR-LR results are available in the Supplementary GRU \nExperiment online. Error bars indicate 95% confidence interval obtained from 1000 bootstrap iterations. Raw \nperformance scores and change in OOD performance relative to ID are provided in Supplementary Tables S2 \nand S3 online, respectively. AUROC Area under the receiver operating characteristics curve; AUPRCC Calibrated \narea under the precision recall curve; ACE Absolute calibration error; LOS Length of stay; ICU Intensive care \nunit; CLMBR Clinical language model-based representation; LR Logistic regression.\n8\nVol:.(1234567890)Scientific Reports |         (2023) 13:3767  | https://doi.org/10.1038/s41598-023-30820-8\nwww.nature.com/scientificreports/\nsuggests that the favorable ID and OOD performance are partly attributable to the self-supervised pretraining. \nWe hypothesize that code-based language modeling, unlike in NLP , often directly maps to downstream clinical \nprediction tasks, since codes may denote a clinical condition of interest. However, many other clinical tasks are \nhighly compositional, meaning they do not map to a clear set of codes, making them harder to capture directly in \npretraining. This should be explored in future work alongside other pretraining objectives and intrinsic measures \nof EHR foundation model performance.\nFigure 5.  Scaling of GRU- and transformer-based CLMBR along pretraining set size. A more positively sloped \ncurve (negative for ACE) indicates better performance or robustness (performance in the OOD year group) \nalong increasing pretraining set size. Note that the number of patients include both training and validation \nsets of the pretraining cohort. The asterisk in [09–12*] indicates a subset of patients in [09–12] that have \nbeen admitted to an inpatient unit during the year range. Raw performance scores and slopes are provided in \nSupplementary Tables S6 and S7 online, respectively. AUROC Area under the receiver operating characteristics \ncurve; AUPRCC Calibrated area under the precision recall curve; ACE absolute calibration error; LR Logistic \nregression; CLMBR Clinical language model-based representations; GRU  Gradient recurrent unit; TRANS \nTransformer; LOS Length of stay; ICU Intensive care unit.\nFigure 6.  The proportion of patients correctly and incorrectly re-classified by CLMBR-LR. Green bars indicate \nthe proportion of patients that were incorrectly classified by count-LR but were correctly re-classified by \nCLMBR-LR. Red bars indicate the proportion of patients that were correctly classified by count-LR but were \nincorrectly re-classified by CLMBR-LR. Together, these represent the percentage of decisions that would have \nbeen affected if CLMBR-LR were used instead of count-LR. LR logistic regression; CLMBR clinical language \nmodel-based representations; LOS length of stay; ICU intensive care unit.\n9\nVol.:(0123456789)Scientific Reports |         (2023) 13:3767  | https://doi.org/10.1038/s41598-023-30820-8\nwww.nature.com/scientificreports/\nThe finding that the transformer-based CLMBR is more scalable whereas GRU-based CLMBR is more sample-\nefficient is consistent with the idea that models that integrate strong inductive biases like the GRU are more adept \nwith smaller sample sizes, whereas weaker inductive biases of the transformer enable better scaling to larger \nvolumes of data. The latter motivates additional research on learning transformer-based foundation models \non larger-scale data as well as multiple modalities to explore and expand their range of capabilities in clinical \nmedicine. In addition, scaling along other factors such as model size and the amount of  compute15,31 are needed \nto better understand the scaling laws of foundation models in clinical medicine.\nVarious EHR foundation models were developed in recent years based on the transformer architecture and \npretraining tasks used in BERT. These include, for example, Med-BERT 18,  BEHRT19, G-BERT28, and CEHR-\nBERT29. These models share similarities with CLMBR in that they are pretrained on EHR-derived patient time-\nlines but differ in the type of medical codes and their vocabulary for inputs; pretraining set size and objective; \nand the adaptation (or transfer learning) approach. Notably, BERT-based models were adapted by fine-tuning \nall weights for each downstream task. In contrast, CLMBR was adapted by freezing the weights and learning just \na classification head for each clinical task. Our adaptation approach is conservative and less resource-intensive, \nwhich enables rapid development of task-specific models. In addition, it has been demonstrated that end-to-end \nfine-tuning does not always improve performance, particularly in OOD  settings38. In addition, very little has \nbeen published that examines the robustness of an EHR foundation model and its scalability in the presence of \ntemporal distribution shift. This work also compares end-to-end models like LR and GRUs to foundation models \n(of which CLMBR is just one example).\nWe postulate that the observed improvement in performance and robustness in CLMBR-LR are enabled in \npart by the pretraining objective and the size and  diversity12,39 of the pretraining set. During pretraining, CLMBR \nlearns a representation that captures relationships between raw features (i.e., medical codes). Such a representa-\ntion is more holistic and is arguably a more time-invariant “view” of the patient. The relationship between this \n“view” and an outcome (e.g., long length of stay) might therefore be less spurious and more robust to distribu -\ntion shifts compared to the relationship between raw features and the outcome (learned by end-to-end models \nlike count-LR).\nWe examined two attributes of OOD performance, namely absolute performance and relative (to ID) per -\nformance. It is notable that while CLMBR resulted in generally better absolute discrimination than count-LR, \nimprovement in relative discrimination was more modest. It is likely that absolute performance is more mean-\ningful to clinicians since better relative performance does not necessarily indicate better absolute  performance40. \nHowever, decision makers may be more concerned about using a model that does not perform as well as that \noriginally promised (relative performance). It is for that reason that we choose to report both aspects. In practice, \nthe foundation model can be retrained, fine-tuned, or updated over time via monitoring and updating  policies41 \nto mitigate the impact of temporal distribution shift. In addition, structure and certain types of invariances could \nbe incorporated at various stages using  metadata42,  regularization43, or contrastive  learning44.\nThe strengths of this study include the evaluation of a novel approach to self-supervised representation \nlearning on electronic health records, namely CLMBR, in both ID and OOD settings. Another strength is the \nadoption of interpretable metrics to evaluate the clinical impact of using CLMBR-based models instead of \nmodels trained on count-based representations. However, this study is limited in several ways. First, we only \nused a single dataset with a limited number of outcomes. Performance of CLMBR may differ in other settings \nand with other tasks. Second, this work focused on the evaluation of a single foundation modelling approach. \nFuture work should explore other approaches. In addition, our evaluation was restricted to temporal robustness \nand we did not explore other properties that may have emerged from pretraining at larger scales. Finally, we \nlack insight into how well CLMBR scales along other factors such as model size and the amount of compute, the \neffect of various adaptation approaches such as end-to-end fine-tuning of CLMBR weights on the robustness of \ntask-specific models, and the scenarios in which CLMBR may be more or less helpful.\nIn conclusion, models trained on CLMBR were better than count-LR in discrimination performance and \noften exhibited less decay in cases where OOD discrimination performance degraded. In addition, performance \nand robustness of transformer-based CLMBR improved further when more data became available for pretrain-\ning. These results suggest that pretraining EHR foundation models at scale is a useful approach for developing \nclinical prediction models that perform well ID as well as OOD.\nData availability\nThe de-identified Stanford Medicine Research Data Repository OMOP common data model is available through \nNero, the highly secure data science platform maintained by Research IT at Stanford Medicine and Stanford \nResearch Computing Center. Access to Nero requires affiliation with a Stanford Principal Investigator and a \nStanford identity. However, data are available from the corresponding author upon reasonable request. The \ncode for all analyses is open-source and available at https:// github. com/ som- shahl ab/ temp_ ds_ shift_ robus tness.\nReceived: 13 May 2022; Accepted: 2 March 2023\nReferences\n 1. Rajkomar, A. et al. Scalable and accurate deep learning with electronic health records. NPJ Digit. Med.  1, 1–10 (2018).\n 2. Moreno-Torres, J. G., Raeder, T., Alaiz-Rodríguez, R., Chawla, N. V . & Herrera, F . A unifying view on dataset shift in classification. \nPattern Recogn. 45, 521–530 (2012).\n 3. Guo, L. L. et al. Evaluation of domain generalization and adaptation on improving model robustness to temporal dataset shift in \nclinical medicine. Sci. Rep. 12, 2726. https:// doi. org/ 10. 1038/ s41598- 022- 06484-1 (2022).\n10\nVol:.(1234567890)Scientific Reports |         (2023) 13:3767  | https://doi.org/10.1038/s41598-023-30820-8\nwww.nature.com/scientificreports/\n 4. Wong, A. et al. External validation of a widely implemented proprietary sepsis prediction model in hospitalized patients. JAMA \nIntern. Med. 181, 1065–1070 (2021).\n 5. Bommasani, R. et al. On the opportunities and risks of foundation models. arXiv preprint arXiv: 2108. 07258 (2021).\n 6. Finlayson, S. G. et al. The clinician and dataset shift in artificial intelligence. N. Engl. J. Med. 385, 283–286. https:// doi. org/ 10. 1056/ \nNEJMc 21046 26 (2021).\n 7. Davis, S. E., Lasko, T. A., Chen, G., Siew, E. D. & Matheny, M. E. Calibration drift in regression and machine learning models for \nacute kidney injury. J. Am. Med. Inform. Assoc. 24, 1052–1061. https:// doi. org/ 10. 1093/ jamia/ ocx030 (2017).\n 8. Strobl, A. N. et al. Improving patient prostate cancer risk assessment: Moving from static, globally-applied to dynamic, practice-\nspecific risk calculators. J. Biomed. Inform. 56, 87–93. https:// doi. org/ 10. 1016/j. jbi. 2015. 05. 001 (2015).\n 9. Janssen, K. J., Moons, K. G., Kalkman, C. J., Grobbee, D. E. & Vergouwe, Y . Updating methods improved the performance of a \nclinical prediction model in new patients. J. Clin. Epidemiol. 61, 76–86. https:// doi. org/ 10. 1016/j. jclin epi. 2007. 04. 018 (2008).\n 10. Guo, L. L. et al. Systematic review of approaches to preserve machine learning performance in the presence of temporal dataset \nshift in clinical medicine. Appl. Clin. Inform. 12, 808–815 (2021).\n 11. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. \narXiv preprint arXiv: 1810. 04805 (2018).\n 12. Hendrycks, D., Mazeika, M., Kadavath, S. & Song, D. Using self-supervised learning can improve model robustness and uncertainty. \nAdv. Neural Inf. Process. Syst. 32 (2019).\n 13. Radford, A. et al. in International Conference on Machine Learning. 8748–8763 (PMLR).\n 14. Liu, Y . et al. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv: 1907. 11692 (2019).\n 15. Hoffmann, J. et al. Training compute-optimal large language models. arXiv: 2203. 15556 (2022). https:// ui. adsabs. harva rd. edu/ abs/ \n2022a rXiv2 20315 556H\n 16. Brown, T. et al. Language models are few-shot learners. Adv. Neural. Inf. Process. Syst. 33, 1877–1901 (2020).\n 17. Steinberg, E. et al. Language models are an effective representation learning technique for electronic health record data. J. Biomed. \nInform. 113, 103637 (2021).\n 18. Rasmy, L., Xiang, Y ., Xie, Z., Tao, C. & Zhi, D. Med-BERT: Pretrained contextualized embeddings on large-scale structured elec-\ntronic health records for disease prediction. NPJ Digit. Med. 4, 1–13 (2021).\n 19. Li, Y . et al. BEHRT: Transformer for electronic health records. Sci. Rep. 10, 1–12 (2020).\n 20. Peters, M. E. et al. Deep contextualized word representations. arXiv: 1802. 05365 (2018). https:// ui. adsabs. harva rd. edu/ abs/ 2018a \nrXiv1 80205 365P\n 21. Datta, S. et al. A new paradigm for accelerating clinical data science at Stanford Medicine. arXiv preprint arXiv: 2003. 10534 (2020).\n 22. Hripcsak, G. et al. observational health data sciences and informatics (OHDSI): Opportunities for observational researchers. Stud. \nHealth Technol. Inform. 216, 574 (2015).\n 23. Voss, E. A. et al. Feasibility and utility of applications of the common data model to multiple, disparate observational health \ndatabases. J. Am. Med. Inform. Assoc. 22, 553–564 (2015).\n 24. Pfohl, S. R., Foryciarz, A. & Shah, N. H. An empirical characterization of fair machine learning for clinical risk prediction. J. \nBiomed. Inform. 113, 103621 (2021).\n 25. Reps, J. M., Schuemie, M. J., Suchard, M. A., Ryan, P . B. & Rijnbeek, P . R. Design and implementation of a standardized framework \nto generate and evaluate patient-level prediction models using observational healthcare data. J. Am. Med. Inform. Assoc. 25, 969–975 \n(2018).\n 26. Bodenreider, O. The unified medical language system (UMLS): Integrating biomedical terminology. Nucleic Acids Res. 32, D267–\nD270 (2004).\n 27. Choi, E., Bahadori, M. T., Schuetz, A., Stewart, W . F . & Sun, J. in Machine learning for healthcare conference. 301–318 (PMLR).\n 28. Shang, J., Ma, T., Xiao, C. & Sun, J. Pre-training of graph augmented transformers for medication recommendation. arXiv: 1906. \n00346 (2019). https:// ui. adsabs. harva rd. edu/ abs/ 2019a rXiv1 90600 346S\n 29. Pang, C. et al. CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks. arXiv:  \n2111. 08585 (2021). https:// ui. adsabs. harva rd. edu/ abs/ 2021a rXiv2 11108 585P\n 30. Morin, F . & Bengio, Y . in International workshop on artificial intelligence and statistics. 246–252 (PMLR).\n 31. Kaplan, J. et al. Scaling Laws for Neural Language Models. arXiv: 2001. 08361 (2020). https:// ui. adsabs. harva rd. edu/ abs/ 2020a rXiv2 \n00108 361K\n 32. Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library. Adv. Neural Inform Process. Syst. 32, \n8024–8035 (2019).\n 33. Pedregosa, F . et al. Scikit-learn: Machine learning in Python. J. Mach. Learn. Res. 12, 2825–2830 (2011).\n 34. Van Rossum, G. & Drake, F . Python language reference, version 3.8. Python Software Foundation (2019).\n 35. Siblini, W ., Fréry, J., He-Guelton, L., Oblé, F . & Wang, Y . Q. International Symposium on Intelligent Data Analysis  457–469 (Springer, \n1999).\n 36. Austin, P . C. & Steyerberg, E. W . The integrated calibration index (ICI) and related metrics for quantifying the calibration of logistic \nregression models. Stat. Med. 38, 4051–4065 (2019).\n 37. Liang, P . et al. Holistic evaluation of language models. arXiv: 2211. 09110 (2022). https:// ui. adsabs. harva rd. edu/ abs/ 2022a rXiv2 \n21109 110L\n 38. Kumar, A., Raghunathan, A., Jones, R., Ma, T. & Liang, P . Fine-tuning can distort pretrained features and underperform out-of-\ndistribution. arXiv: 2202. 10054 (2022). https:// ui. adsabs. harva rd. edu/ abs/ 2022a rXiv2 20210 054K\n 39. Hendrycks, D. et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. arXiv: 2006. 16241 (2020). \nhttps:// ui. adsabs. harva rd. edu/ abs/ 2020a rXiv2 00616 241H\n 40. Taori, R. et al. Measuring robustness to natural distribution shifts in image classification. arXiv: 2007. 00644 (2020). https:// ui.  \nadsabs. harva rd. edu/ abs/ 2020a rXiv2 00700 644T\n 41. Davis, S. E. et al. A nonparametric updating method to correct clinical prediction model drift. J. Am. Med. Inform. Assoc. 26, \n1448–1457. https:// doi. org/ 10. 1093/ jamia/ ocz127 (2019).\n 42. Xie, S. M. et al. In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness. arXiv: \n2012. 04550 (2020). https:// ui. adsabs. harva rd. edu/ abs/ 2020a rXiv2 01204 550X\n 43. Bardes, A., Ponce, J. & LeCun, Y . VICReg: Variance-invariance-covariance regularization for self-supervised learning. arXiv: 2105. \n04906 (2021). https:// ui. adsabs. harva rd. edu/ abs/ 2021a rXiv2 10504 906B\n 44. Chen, T., Kornblith, S., Norouzi, M. & Hinton, G. A simple framework for contrastive learning of visual representations. arXiv:  \n2002. 05709 (2020). https:// ui. adsabs. harva rd. edu/ abs/ 2020a rXiv2 00205 709C\nAuthor contributions\nL.L.G., J.F ., and L.S. conceptualized and designed the study with input from all authors. L.L.G. performed all \nexperiments. E.S. and S.R.P . contributed to the codebase. L.L.G., J.F ., and L.S. analyzed and interpreted results \nwith input from all authors. L.L.G. wrote the manuscript. All authors revised and commented on the manuscript. \nAll authors read and approved the final manuscript.\n11\nVol.:(0123456789)Scientific Reports |         (2023) 13:3767  | https://doi.org/10.1038/s41598-023-30820-8\nwww.nature.com/scientificreports/\nFunding\nNo external funding was received for the study.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 023- 30820-8.\nCorrespondence and requests for materials should be addressed to L.S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Logistic regression",
  "concepts": [
    {
      "name": "Logistic regression",
      "score": 0.6909903287887573
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6468448042869568
    },
    {
      "name": "Receiver operating characteristic",
      "score": 0.5946747064590454
    },
    {
      "name": "Transformer",
      "score": 0.5262248516082764
    },
    {
      "name": "Computer science",
      "score": 0.5051198601722717
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46298423409461975
    },
    {
      "name": "Statistics",
      "score": 0.4211423695087433
    },
    {
      "name": "Machine learning",
      "score": 0.4055708348751068
    },
    {
      "name": "Medicine",
      "score": 0.34581130743026733
    },
    {
      "name": "Mathematics",
      "score": 0.21181362867355347
    },
    {
      "name": "Engineering",
      "score": 0.10943254828453064
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801317318",
      "name": "Hospital for Sick Children",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210147079",
      "name": "Institute for Clinical Evaluative Sciences",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210141030",
      "name": "SickKids Foundation",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I142879360",
      "name": "Universidad del Norte",
      "country": "CO"
    }
  ],
  "cited_by": 42
}