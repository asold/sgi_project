{
    "title": "TweetBERT: A Pretrained Language Representation Model for Twitter Text Analysis",
    "url": "https://openalex.org/W3093543164",
    "year": 2022,
    "authors": [
        {
            "id": null,
            "name": "Qudar, Mohiuddin Md Abdul",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3166999186",
            "name": "Mago Vijay",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2735784619",
        "https://openalex.org/W2963756346",
        "https://openalex.org/W2148039410",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3104059174",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2536583325",
        "https://openalex.org/W67382910",
        "https://openalex.org/W2346452181",
        "https://openalex.org/W2963488798",
        "https://openalex.org/W2793978524",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2606964149",
        "https://openalex.org/W3098057198",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2734608416",
        "https://openalex.org/W2914679463",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W2084591134",
        "https://openalex.org/W1603684389",
        "https://openalex.org/W2801930304",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963159690",
        "https://openalex.org/W2190406907",
        "https://openalex.org/W2963625095",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W2142575968",
        "https://openalex.org/W3024622987",
        "https://openalex.org/W2125436846",
        "https://openalex.org/W3105491236",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2169606435",
        "https://openalex.org/W2131462252",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2962815673",
        "https://openalex.org/W3088059392",
        "https://openalex.org/W2922551710",
        "https://openalex.org/W2132267839",
        "https://openalex.org/W2963149412",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3032907911"
    ],
    "abstract": "Twitter is a well-known microblogging social site where users express their views and opinions in real-time. As a result, tweets tend to contain valuable information. With the advancements of deep learning in the domain of natural language processing, extracting meaningful information from tweets has become a growing interest among natural language researchers. Applying existing language representation models to extract information from Twitter does not often produce good results. Moreover, there is no existing language representation models for text analysis specific to the social media domain. Hence, in this article, we introduce two TweetBERT models, which are domain specific language presentation models, pre-trained on millions of tweets. We show that the TweetBERT models significantly outperform the traditional BERT models in Twitter text mining tasks by more than 7% on each Twitter dataset. We also provide an extensive analysis by evaluating seven BERT models on 31 different datasets. Our results validate our hypothesis that continuously training language models on twitter corpus help performance with Twitter.",
    "full_text": "1\nTweetBERT: A Pretrained Language Representation\nModel for Twitter Text Analysis\nMohiuddin Md Abdul Qudar and Vijay Mago\nAbstract\nTwitter is a well-known microblogging social site where users express their views and opinions in real-time. As a\nresult, tweets tend to contain valuable information. With the advancements of deep learning in the domain of natural\nlanguage processing, extracting meaningful information from tweets has become a growing interest among natural\nlanguage researchers. Applying existing language representation models to extract information from Twitter does not\noften produce good results. Moreover, there is no existing language representation models for text analysis speciﬁc\nto the social media domain. Hence, in this article, we introduce two TweetBERT models, which are domain speciﬁc\nlanguage presentation models, pre-trained on millions of tweets. We show that the TweetBERT models signiﬁcantly\noutperform the traditional BERT models in Twitter text mining tasks by more than 7% on each Twitter dataset. We\nalso provide an extensive analysis by evaluating seven BERT models on 31 diﬀerent datasets. Our results validate our\nhypothesis that continuously training language models on twitter corpus help performance with Twitter\nIndex Terms\nLanguage model, BERT models, Natural Language Processing, Twitter\nI. Introduction\nTwitter is a popular social networking platform where users tend to express themselves and share their information in\nreal time [1]. As a result, text from Twitter is widely studied by natural language researchers and social scientists. The\nusers tend to write texts that are very colloquial and casual [1]. The text is are written in a completely diﬀerent way\nthan traditional writings, primarily due to a restriction in their length. However, their contents are powerful enough to\nstart a movement all over the world or to detect a pandemic in the early stages [2]. Hence, usage and style of language\nis extensively studied. The users, in any social media platform, are more comfortable in discussing their views and\nperspectives in an informal manner, usually following very little or no grammatical rules [3] [4]. This is often seen in\nTwitter due to its character limit of only 280 characters 1 per tweet. Using existing language representation models,\nsuch as BERT (Bidirectional Encoder Representations from Transformers) [5] or AlBERT [6], to evaluate such texts\nis a challenge. As a result, a need for a language model speciﬁc to social media domain arises. Deep neural network\nmodels have contributed signiﬁcantly to many recent advancements in natural language processing (NLP), especially\nwith the introduction of BERT. BERT and BioBERT [7] have considerably improved performance on datasets, in the\ngeneral domain and biomedical domain, respectively. State-of-art research indicates that when unsupervised models\nare pre-trained on large corpora, they perform signiﬁcantly better in NLP tasks. However, language models, such as\nBioBERT, cannot achieve high performance on domains like social media corpora. This is mainly due to the fact that\nthese models are trained on other domain corpora and the language in social media is irregular and mostly informal.\nTo address this need, in this article, TweetBERT is introduced, which is a language representation model that has\nbeen pre-trained on a large number of English tweets, for conducting Twitter text analysis. Experimental results show\nthat TweetBERT outperformed previous language models such as SciBERT [8], BioBERT [9] and AlBERT [6] when\nanalyzing twitter texts.\nIn order to study and extract information from social media texts it is neccessary to have a language model speciﬁc\nto social media domain. Futhermore, the TweetBERT models have been evaluated on 31 diﬀerent datasets, including\ndatasets from general, biomedical, scientiﬁc and Twitter domains. These state-of-the-art language representation models\nhave shown promising results in the datasets for conducting text analysis. To show the eﬀectiveness of our approach\nin Twitter text analysis, TweetBERTs were ﬁne-tuned on two main Twitter text mining tasks: sentiment analysis and\nclassiﬁcation. In this paper, the authors made the following contribution:\n• TweetBERT, a domain speciﬁc language representation model trained on Twitter corpora for general Twitter text\nmining, is introduced.\n• TweetBERT is evaluated on various Twitter datasets and is shown that both TweetBERTv1 and TweetBERTv2\noutperform other traditional BERT models, such as BioBERT, SciBERT and BERT itself in Twitter text analysis.\n• A comprehensive and elaborate analysis is provided by evaluating seven diﬀerent BERT models including Tweet-\nBERTs on 31 diﬀerent datasets, and their results are compared.\nMohiuddin Md Abdul Qudar and Vijay Mago are with Lakehead University.\n1https://about.twitter.com/\narXiv:2010.11091v1  [cs.CL]  17 Oct 2020\n2\n• Pre-trained weights of TweetBERT are released and source code is made available to the public 2.\nThe structure of the paper is as follows: The existing work in the ﬁeld of language models is discussed in Section II.\nSection III presents the methodology, where it is described how the data has been collected for pre-training the model,\nand includes the approaches that were taken for implementing the TweetBERT models. In Section IV the datasets\nused for evaluating the model are described in detail. Section V provides a discussion of the experimental results in the\nbenchmark datasets with the various BERT and TweetBERT models. Finally, the conclusion is presented in Section\nVI.\nII. Related works\nRecently a vast amount of work has been done, in the ﬁeld of NLP, using bidirectional language models especially\nby modifying BERT [10]. BERT is a pre-trained neural network word representation model. It uses bidirectional\ntransformer, which considers the sequence of data and, therefore, can understand the context of a text. It was pre-\ntrained using texts from BookCorpus [11] and English Wiki [5]. BERT uses two techniques for pre-training: masked\nlanguage model, and next sentence prediction. Masking is carried out in three diﬀerent ways in a sentence: by replacing\na word with a token, or by replacing the word with a random word, or keeping the sentence as it is. These three ways\nhelp a bidirectional model to maintain and learn the context of a text. On the other hand, the next sentence prediction\nhelps BERT to relate and connect two sentences together [12]. This is useful when evaluating sentiment analysis or\nquestion answering datasets. However, as BERT has been pre-trained on general corpora, it performs poorly in domain\nspeciﬁc tasks. As a result, language models like BioBERT and SciBERT have been introduced. Recent language models\nhave been broken down into two categories: contiual pre-training and pre-training from scratch.\nA. Continual Pre-training\nContinual models are those which use weights from another model and modify themselves for a speciﬁc task [13].\nBioBERT is a continual pre-trained model because it was ﬁrst initialized with the weights of BERT, and then pre-trained\non various biomedical corpora, such as PubMed abstracts and PMC full-text articles, to make it domain speciﬁc [9].\nBioBERT was released as Biomedical documents were increasing and biomedical text analysis was becoming popular\n[14]. For example, more than 2,000 articles are published in biomedical peer-reviewed journals every day [15]. Directly\nusing BERT to evaluate biomedical tasks did not give satisfactory results, thus BioBERT was created [9]. BioBERT\nhas the same architecture as BERT, but it has shown to perform better than BERT on biomedical text analysis [16].\nBioBERT was mainly evaluated in three biomedical tasks: biomedical named entity recognition, biomedical relation\nextraction, and biomedical question answering [13]. Likewise, more models were introduced for speciﬁc domains. Lately,\nCovid-Twitter BERT model (CT-BERT) has been released to analyze tweets related to Covid [17]. CT-BERT has been\nis pre-trained on around 160 million coronavirus tweets collected from the Crowdbreaks platform [17]. CT-BERT is\na continual BERT model and has shown an improvement of more than 10% on classiﬁcation datasets compared to\nthe original BERT [18] model. This model has shown the most improvement in the target coronavirus related tweets.\nFurthermore, other extensions of BERT models, such as the AlBERT [6], were also released. Generally, increasing the\ntraining corpus increases the performance of the NLP tasks. Moreover, the model size is directly proportional to the\nsize of the training corpus. However, as the model size increases, it becomes increasely diﬃcult to pre-train the model\nbecause there are GPU limitations. To address this factor AlBERT was introduced. It uses two parameter-reduction\ntechniques to signiﬁcantly reduces the number of training parameters in BERT: factorized embedding parameterization\n[6], which breaks a large matrix into smaller matrices [10], and performing cross-layer parameter sharing, which cuts\ndown the number of parameters as the neural network size increases. These methods have helped BERT to increase\nits training speed [10].\nB. Pre-training from Scratch\nThere are other domains where both BERT and BioBERT provide unsatisfactory results. For example, when\nextracting information from general scientiﬁc texts, BERT performed poorly because it was only pre-trained on general\ndomain corpora. As a result, SciBERT was released to evaluate scientiﬁc datasets [8]. SciBERT also has the same\narchitecture as BERT, but it is not a continual model. SciBERT is pre-trained from scratch and it uses a diﬀerent\nWordPiece vocabulary called SentencePiece [19]. SentencePiece vocabulary consists of words that are commonly used\nin scientiﬁc domains [20]. When WordPiece and SentencePiece are compared, it is found that there is a similarity of\nonly about 40%. This shows that there is a huge diﬀerence between the words regularly used in general and scientiﬁc\narticles. SciBERT was pre-trained on a corpus from semantic scholar, containing 1.14 million papers from the computer\nscience and biomedical domain [21]. Each paper produced around 3,000 tokens making it similar to the number of\ntokens used to pre-train BERT [21]. Similarly another BERT model was released, called RoBERTa, that showed that\n2https://github.com/mohiuddin02/TweetBERT\n3\nchanging hyperparameter during pre-training BERT signiﬁcantly increased the model’s performance [22]. RoBERTa\nis not a continual model. It has been pre-trained on an extremely large, ﬁve diﬀerent types of corpora: BookCorpus,\nEnglish Wikipedia, CC-News (collected from CommonCrawl News) dataset, OpenWebText, a WebText corpus [23],\nand Stories, a dataset containing story-like content [23]. The overall size of the datasets was more than 160GB [22].\nMoreover, RoBERTa uses 4 diﬀerent techniques, unlike BERT, to pre-train. They are:\n• Segment-pair with next sentence prediction hyperparameter, which is the same as next sentence prediction as\nBERT [22].\n• Sentence-pair next sentence prediction hyperparameter, where back to back sentences from only one document\nare connected [22].\n• Full-sentences hyperparameter, where sentences within a document are connected.\n• Doc-sentences hyperparameter, which is similar to full-sentences but two or more documents are connected [23].\nIII. Methodology\nThis section discusses in detail the source of data collecting, tweet extracting, and corpora used for pre-training\nTweetBERT. An overview of the pre-training approach is shown in Fig. 1. There are two TweetBERT models: Tweet-\nBERTv1 and TweetBERTv2. Each of these models are pre-trained using diﬀerent approaches, but have the same\narchitecture as BERT because it is continual pre-training model. Moreover, Table I shows the diﬀerent variation of\ncorpora and vocabulary used to pre-train each BERT model. For example, SciBERT uses SciVocab vocabulary which\ncontain words popular in the scientiﬁc domain.\nA. Data Collection\nFor domain speciﬁc text mining tasks, language models like BioBERT were pre-trained on PubMed and PMC [9].\nLikewise, TweetBERT was pre-trained on English tweets. TweetBERTv1 was pre-trained on a corpus that consists of\n140 million tweets. The corpus contains tweets from top 100 personalities 3 and top 100 hashtags of Twitter [24]. Top\npersonalities are the group of people who have the highest number of followers, Twitter platform. TweetBERTv2 was\npre-trained on a similar but larger corpus containing 540 million English tweets. Table I shows the diﬀerent combination\nof corpora and WordPiece vocabulary involved in training of BERT models.\nTo create the training datasets tweets were collected and cleaned from big data analytics platform 4 developed in\nDaTALab at Lakehead University, Canada [25]. This platform allows users to extract millions of tweets by simply\ngiving keywords as inputs. The authors generated two corpora: Corpus140 and Corpus540 which indicate corpora with\n140 and 540 million tweets, respectively. Each consists of tweets from top trending hashtags and top 100 personalities\n[24]. The reason behind generating the corpora with the top personalities, followed by millions of Twitter users, was to\nensure that the tweets were taken from authentic proﬁle, since Twitter contains many fake accounts and their tweets\nhave no real meaning. Moreover, tweets from top hashtags were used to analyze the pattern and style of informal\nlanguage used in the Twitter platform by the general users.\nModel Corpora Used WordPiece Vocab\nBERT English Wiki + BookCorpus BaseVocab\nSciBERT Scientiﬁc articles SciVocab\nTweetBERTv1 English Wiki + BookCorpus + Corpus140 BaseVocab\nTweetBERTv2 English Wiki + BookCorpus + Corpus540 BaseVocab + SciVocab\nTABLE I\nShows the different variation of corpora and WordPiece vocabulary involved in BERT models.\nB. TweetBERT\nTweetBERT is a continual pre-trained model since it is initialized with the weight of BERT. BERT was pre-trained\non general domain corpora that is English Wikipedia and BooksCorpus [5]. As a result, the TweetBERT has the\nsame architecture as BERT. Then, TweetBERTv1 is pre-trained on a Corpus140. On the other hand, TweetBERTv2 is\ninitialzed with weights from AlBERT and pre-trained on Corpus540 using baseVocab and SciVocab. AlBERT model is\nthe same as BERT, except that is uses two parameter-reduction techniques to reduce the number of training parameters\nin BERT, which increase the training speed of the model [10] [8]. BaseVocab and SciVocab are the WordPiece and\nSentencePiece vocabulary of BERT and SciBERT, respectively. TweetBERTv1 uses the same WordPiece vocabulary\nas BERT so that the initial pre-trained weights of BERT and ALBERT are compatible with TweetBERT models [6].\nTweetBERT, at the same time, uses the vocabulary of SciBERT so that scientiﬁc analysis can be carried out, for\n3https://www.kaggle.com/parulpandey/100-mostfollowed-twitter-accounts-as-of-dec2019\n4https://twitter.datalab.science/\n4\nFig. 1. Overview of the pre-training TweetBERTs\nexample detecting an epidemic or pandemic as opposed to simple sentiment analysis from tweets. TweetBERT can be\nalso be used to evaluate other datasets in diﬀerent domains, rather than just analyzing tweets. Fig. 1 gives a detailed\noverview of the approach in making TweetBERT models.\nIV. Datasets for Evaluation\nIn this article, results of evaluating seven BERT models on 31 diﬀerent datasets are recorded. This section discusses\nsome of the datasets that are used to evaluate the diﬀerent BERT models including TweetBERTs. The datasets are\ndivided into four domains: general, biomedical, scientiﬁc and Twitter domains. Each domain has its own set of datasets.\nA. General\nThe general domain contains datasets such as GLUE [26], SQuAD [27], SWAG [28] and RACE datasets. These\ndatasets have contents that covers a wide range of general knowledge in basic English.\n1) GLUE: The General Language Understanding Evaluation benchmark (GLUE) consists of datasets used for\n“training, evaluating, and analyzing” language models [26]. GLUE consist of nine diﬀerent datasets designed in such\na way so that it can evaluate a model’s understanding of general language[10][29].\n• Corpus of Linguistic Acceptability (CoLA) is a single-sentence task consisting of more than 10,000 English\nsentences. Each sentence is given a label indicating if its grammatical or ungrammatical English sentence. The\nlanguage model’s task is to predict the label [10].\n• The Stanford Sentiment Treebank (SST) is also a binary single-sentence classiﬁcation task containing sentences\nfrom movie reviews, along with their sentiment, labeled by humans [30]. The task of language model is to predict\nthe sentiment of a given sentence only.\n• The Microsoft Research Paraphrase Corpus (MRPC) is a sentence pair corpus generated from online news sources,\nwith human annotations for whether both the sentences are semantically equivalent or not. Thus, the task is to\npredict if a given sentence-pair has semantic similarity or not [26].\n• The Quora Question Pair(QQP) is similar to MRPC; the task is to predict how similar a given pair of questions\nare in terms of semantic meaning [26]. However, unlike MRPC, QQP dataset is a collection of questions from the\nquestion-answering website Quora5 [10].\n• The Semantic Textual Similarity Benchmark (STS-B) is a collections of sentence pairs extracted from news\nheadlines, video and image captions, and similar sources, where semantic similarity score from one to ﬁve is\nassigned to the sentence pairs. The task is to predict the scores [31].\n• The Multi-Genre Natural Language Inference Corpus (MNLI) is a crowd sourced dataset, consisting of sentence\npairs with a human annotated premise and a hypothesis sentence. The task is to predict whether the premise\nsentence “entails” the hypothesis, contradicts the hypothesis sentence or stays neutral [26].\n• Question Natural Language Inference (QNLI) is a simpliﬁed version of SQuAD dataset which has been converted\ninto a binary classiﬁcation task by forming a pair between each question and each sentence in the corresponding\ncontext [10]. A language model’s task would be to determine if the sentence contains the answer to the question.\nA positive value is assigned if pairs contain the correct answer, similarly a negative value is assigned if the pairs\ndo not contain the answer [26].\n• Recognizing Textual Entailment (RTE) is similar to MNLI, where the language model predicts if a given sentence\nis similar to the hypothesis, contradicts or stays neutral. RTE dataset is very small compared to MNLI [30].\n• The Winograd Schema Challenge (WNLI) is a reading comprehension task, in which a model takes a sentence\nwith a pronoun as an input, and selects an answer from a list of choices that references to the given pronoun [29].\n5https://www.quora.com/\n5\n2) SQuAD: The Stanford Question Answering Dataset is a collection of more than 100,000 questions answered by\ncrowdworkers [27]. It contains 107,785 question-answer pairs on 536 articles [10]. Each question and its following answer\nis from Wikipedia. SQuAD, unlike previous datasets like MCTest dataset [32], does not provide a list of choices. The\ndataset has been created in such a way so that a language model can select the answer from the context of the passage\nand the question. In the beginning when releasing this dataset, logistic regression was performed to evaluate the level\nof diﬃcultly [33]. It was seen that the performance of the model decreases as the diversity of the model increases. The\ndataset helps a model to predict the context of a language [32].\nPassage: Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran\nconcurrently with it from 1962 to 1966. Gemini missions developed some of the space travel techniques that\nwere necessary for the success of the Apollo missions. Apollo used Saturn family rockets as launch vehicles.\nApollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space\nstation that supported three manned missions in 1973–74, and the Apollo–Soyuz Test Project, a joint Earth\norbit mission with the Soviet Union in 1975.\nQuestion:\nWhat space station supported three manned missions in 1973-1974\nAnswer:\nSkylab\nFig. IV-A2 is a sample from SQuAD dataset [27].\n3) RACE: Large-scale ReAding Comprehension Dataset From Examinations is a collection of approximately 28,000\nEnglish passages and 100,000 questions [34]. This dataset was developed by English language professionals in a such a\nway so that a language model can gain an ability to read a passage or paragraph. The dataset is a multiple question\nanswering task, where the model tries to predict the correct answer [35][36][10]. Other existing question answering\ndataset have two signiﬁcant limitations. First, answer from any dataset can be found by simply a word-based search\nfrom the passage, which shows that the model is not able to consider the reasoning factor; this restricts the various\ntypes of questions that can be asked. Secondly, most datasets are crowd sourced [10], which introduces unwanted noise\nand bias in the dataset. Moreover, RACE is the largest dataset that support neural network training and needs logical\nreasoning to answer. It also contains option for an answer that might not be present in the training passage, which\ndiversiﬁes the questions that can be asked [37]. RACE also contains content from various ﬁelds, allowing the language\nmodels to be more generic.\n4) SWAG : Large-Scale Adversarial Dataset for Grounded Commonsense Inference dataset is composed of approx-\nimately 113,000 multiple choice questions, including 73,000 instances for training, 20,000 instances for validating, and\n20,000 instances for testing, respectively [28]. The multiple choice questions are derived from video caption, that are\ntaken from ActivityNet Captions and the Large Scale Movie Description Challenge (LSMDC) [38]. The ActivityNet\nCaptions consists of around 20,000 YouTube clips, in which each clip contains one of 203 activity types such as doing\ngymnastics or playing guitar [39]. LSMDC dataset has approximately 128,000 movie captions including both audio\ndescriptions and scripts. For every captions pairs, constituency parsers have been used for splitting the second sentence\nof each pair into nouns and verb phrases [28]. Each question from the multiple choice questions was annotated by\nworkers from Amazon Mechanical Turk. In order to improve the quality of the dataset, annotation artifacts were\nminimized. Annotation artifacts are the stylistic patterns that unintentionally provide suggestions for the target labels.\nB. Biomedical\nThe biomedical domain contain datasets, such as NCBI, BC5CDR and MedNLI dataset. These datasets only contain\ntexts related to biomedical domain.\n1) NCBI : The national center for biotechnology information disease corpus is a collection of 793 PubMed abstracts\nin which abstracts are manually labelled by annotators, where the name of each disease and their corresponding\nconcepts can be found in Medical Subject Headings [36] or in Online Mendelian Inheritance in Man [40]. Name entity\nrecognition is considered to be an important and challenging task of NLP. For example, adenomatous polyposis coli [13]\nand Friedrich ataxia [13] can be both a gene or a disease name. Also, abbreviated disease names are commonly used\nin biomedical texts, such as AS can stand for Angelanguage modelan syndrome, ankylosing spondylitis,aortic stenosis,\nAsperger syndrome or even autism spectrum [41][10]. Also, doctors have their own way of describing a disease and as a\nresult, it more diﬃcult for any language model to achieve good performance. Evaluating a model on this NCBI dataset\nwould show how the model performs in terms of remembering names, especially in biomedical domains [42].\n6\n2) BC5CDR : BC5CDR dataset consists of chemical induced disease (CID) relation extractions [43]. The corpus is\ncomposed of 1,500 PubMed articles with approximately 4,400 annotated chemicals, 5,818 diseases and 3,116 chemical-\ndisease interactions. To study the chemical interactions within diseases in depth, it is also not only important for the\ncorpus to have the annotations of the chemical/diseases, but also their interactions with one another [9]. Moreover, the\ncorpus consists of disease/chemical annotations and relation annotations from the corresponding series of articles.\nMedical Subject Headings (MeSH) indexers were used for annotating the chemical/disease entities. Comparative\nToxicogenomics Database (CTD) was used for annotating the CID relations. In order to attain a rich quality of\nannotation, comprehensive guidelines along with automatic annotation tools were given. For evaluating the inter-\nannotator agreement (IAA) score between each of the annotators, Jaccard similarity coeﬃcient was calculated separately\nfor the diseases and chemicals. This dataset has been used in multiple BioCreative V challenge assignments of biomedical\ntext mining.\nChemical Disease Relations (CDR) are usually physically curated with the aid of CTD. However, this approach of\ncurating manually is expensive. Thus, multiple alternative approaches have been proposed of guiding curation with\ntext-mining mechanisms, which consist of the automatic extraction of CDRs. However, these proposed approaches have\nnot been signiﬁcantly successful since there are shortages of large training corpora. Moreover, to study the chemical\ninteractions within diseases in depth, it is also not only important for the corpus to have the annotations of the\nchemicals diseases, but also their interactions with one another. However, there are multiple biomedical corpora that\nconsist of only a few selected diseases and chemicals. In addition, none of the previous corpora have the instances of\nchemical-disease relation annotations, which includes abstracts having the entire chemical disease, relation annotation,\nand controlled vocabulary. In the case of the BC5CDR dataset, MeSH vocabulary was used as a controlled vocabulary\nsimilar to the existing biomedical information extraction datasets, BC5CDR that includes protein-protein interaction\nand drug-drug interactions. In contrast to the existing biomedical corpora, BC5CDR dataset is crucially diﬀerent in\nterms of annotations (CID relations) from the 1,500 PubMed abstracts.\n3) MedNLI Dataset: MedNLI dataset is a dataset that consists of medical history of the patients which is annotated\nby doctors. MIMIC-III have been used as the source of sentences. In order to avoid in annotating the data, only the\nmedical prescriptions of deceased patients were used. The doctors performed a natural language inference task (NLI)\ntask on the clinical notes that were provided. The MedNLI dataset has shown to be very handy as it is extremely\nchallenging in having constructive, knowledge speciﬁc domains, where there is a shortage of training data. The clinical\ndomain has a shortage of massive-scale annotated datasets for training machine learning models for natural language\ntasks, such as question answering, or paraphrasing. This makes the MedNLI a suitable resource in the open-medical\nﬁeld, since it is publicly available. Moreover, designing such a knowledge intensive medical domain dataset is expensive\nas well, since common approaches such as crowdsourcing platforms cannot be used for annotating the dataset. This\nis because annotating the dataset requires medical domain experts and thus curating such a dataset is very costly.\nPreviously existing datasets have small sizes, and they target general fundamental natural language tasks such as\nco-reference resolution or information extraction tasks (e.g. named entity extraction).\n4) BIOSSES : Biosses is one of the benchmark dataset for sentence similarity in the biomedical domain. The dataset\nis composed of 100 pairs of sentences. The sentences are selected from the Text Analysis Conference (TAC) containing\nBiomedical Summarization Track Training dataset. The TAC dataset consists of 20 reference articles and for each of\nthe reference articles [44]. The sentence pairs are mainly selected from the citing articles in which the sentence has a\ncitation from any one of the reference articles. The data in TAC dataset is both semantically related. At the same time\nthere are dissimilar sentence pairs that also occur in the annotated texts. Sentences that are citing articles from the\nsame reference article will tend to be somewhat semantically similar [45]. In addition, there are other sentences in which\nthe citing sentence referring to an article is written about diﬀerent ranges of topics or domains. Such sentence pairs\nwill tend to have less or no similarity at all. Thus, sentence pairs covering diﬀerent rates of similarity were obtained\nfrom the TAC dataset. In order to obtain a higher quality of dataset, only the pairs which gave strong alliance between\nthe scores of the annotators were taken into account [44]. Table II shows a sample from the original biosses dataset.\n5) JNLPBA: Joint Workshop on Natural language Processing in Biomedicine and its Application is a corpus of\nPubmed abstracts specialized for NER tasks [46]. The types of entities that are selected from the biomedical domain\ninclude DNA, RNA, protein of cells and its types. However, few of the entities did not turn out to be prominently\nsigniﬁcant. For instance, entities for genes include the DNA as well as other gene entities like the protein and RNA\n[46].\n6) Chemprot: Chemprot is a chemical protein interaction corpus generated from PubMed abstracts [47]. The dataset\nconsists of annotations within protein and chemical entities for identifying chemical protein interactions. The dataset\nis organized in a hierarchical structure with a total of 23 interactions. The author of the dataset has emphasized on\nmainly ﬁve high level interactions that includes: upregulator, downregulator, agonist, antagonist, and substrate [47].\n7) GAD: Genetic Association Database is a dataset that was generated from the Genetic Association Archive\n[48]. The archive mainly contains gene-disease interactions from the sentences of PubMed abstracts. NER tool was\nalso used in this dataset to detect gene-disease interactions and create artiﬁcial positive instances from the labeled\n7\nSentence1 Sentence2 Comment Score\nMembrane proteins are proteins that inter-\nact with biological membranes.\nPrevious studies have demonstrated that\nmembrane proteins are implicated in many\ndiseases because they are positioned at the\napex of signaling pathways that regulate\ncellular processes\nThe two sentences are not equivalent,\nbut are on the same topic 1\nThis article discusses the current data on\nusing anti-HER2 therapies to treat CNS\nmetastasis as well as the newer anti-HER2\nagents\nBreast cancers with HER2 ampliﬁcation\nhave a higher risk of CNS metastasis and\npoorer prognosis\nThe two sentences are not equivalent,\nbut share some details 2\nWe were able to conﬁrm that the cancer\ntissues had reduced expression of miR-126\nand miR-424, and increased expression of\nmiR-15b, miR-16, miR-146a, miR-155 and\nmiR-223\nA recent study showed that the expression\nof miR-126 and miR-424 had reduced by\nthe cancer tissues\nThe two sentences are roughly equiva-\nlent, but some important information\ndiﬀers/ missing\n3\nHydrolysis of b-lactam antibiotics by b-\nlactamases is the most common mecha-\nnism of resistance for this class of antibac-\nterial agents in clinically important Gram-\nnegative bacteria\nIn Gram-negative organisms, the most\ncommon b-lactam resistance mechanism\ninvolves b-lactamase-mediated hydrolysis\nresulting in subsequent inactivation of the\nantibiotic\nThe two sentences are completely or\nmostly equivalent, as they mean the\nsame thing\n4\nTABLE II\nA sample from Biosses dataset showing example annotations[44]\narchive sentences. On the other hand, negative instances from the dataset that were labeled as negative gene-disease\ninteractions.\n8) HOC: Hallmarks of Cancer dataset is generated from cancer hallmarks annotated on 1,499 PubMed abstracts.\nAfterwards, the dataset was broadened to 1,852 abstracts. The dataset has binary labels which focuses on labelling the\ncancer discussions on the abstracts as positive samples. However, the samples which had no mention of cancer were\nﬁltered out [49].\nC. Scientiﬁc\nThe scientiﬁc domain contain datasets such as SciCite and SCIERC that contain texts related to scientiﬁc domain.\n1) SCICITE: SciCite is a dataset composed of citation intents that are extracted from various scientiﬁc ﬁelds [8].\nSciCite has been very recently released [50]. The dataset was extracted from Semantic Scholar corpus of medical and\ncomputer science domains, and was annotated by giving label to citation content in four categories the are: method,\nresult, comparison, background, and other. Language models are used to evaluate how well it performs in classiﬁcation\nand question answering tasks on scientiﬁc domain.\n2) SCIERC: SCIERC dataset is a publicly available dataset that consists of annotations of around 5,000 scientiﬁc\nabstracts. The abstracts are collected from 12 AI conference/workshop proceedings from the Semantic Scholar Corpus.\nSCIERC is an extended version of previous existing similar datasets that are also collected from scientiﬁc articles,\nwhich include SemEval 2017 Task 10 [42] and SemEval 2018 Task 7 [42]. SCIERC dataset is broadened in terms of\nsumming up the cross-sentences related to one another by using conference links, named entity and relation types.\nD. Twitter\nThe Twitter domain contain datasets such as gender classiﬁcation and tweets for sentiment analysis. These datasets\nonly contain tweets.\n1) Twitter US airline dataset: Twitter airline dataset 6 is a collection of 14,640 tweets from six US airlines that\nincludes: United, US Airways, Southwest, Delta and Virgin America. The tweets represent the reviews from each of\nthe customers. The tweets are either labeled as positive, negative or neutral, based on the sentiment expressed. The\nairline company usually checks the feedback of their quality through traditional approaches such as the customer\nsatisfaction questionnaires and surveys that are ﬁlled by customers. However, this approach is time consuming and\ninaccurate as customers might ﬁll up the surveys in a hurry. Hence, designing an airline sentiment dataset as the\nTwitter airline dataset is very helpful since users in social media give genuine feedback and reviews about the airlines.\n2) Twitter User Gender Classiﬁcation: In this dataset, 7 annotators were asked to predict and label if the user of\na certain Twitter account is male, female or a brand by only viewing the account. The dataset contains about 20,000\ninstances with user name, user id, account proﬁle, account image and location.\n6https://www.kaggle.com/crowdﬂower/twitter-airline-sentiment\n7https://www.kaggle.com/crowdﬂower/twitter-user-gender-classiﬁcation\n8\nV. Results\nIn this Section the results of evaluating seven BERT models on 31 distinct datasets are discussed. The datasets used\ncan be divided into four diﬀerent domains. The general domain, includes eight datasets from GLUE [26], SQuAD [27],\nSWAG [28] and RACE datasets. Table III shows the performance of the BERT models on the general domain datasets.\nIt is observed that ALBERT[5] and RoBERTa [22] achieve a higher score than other BERT models. AlBERT performs\nbetter in almost all of the GLUE datasets whereas RoBERTa outperforms in general question answering datasets. In\nTable III the highest accuracies are underlined. The results of TweetBERT are fairly or sometimes extremely close to\nthat of the highest accuracy. For example, on CoLA dataset AlBERT and TweetBERT achieves an accuracy of 71.42%\nand 71% respectively. Moreover, to understand the improvement and eﬀectiveness of each TweetBERT models the\nmarginal performance on each dataset is calculated using equation 1 [17]. Table IV shows the marginal performance\nbetween existing BERT models and TweetBERTv1 and Table V shows the mariginal performance of TweetBERTv2\non general domain datasets. Positive value represents by how much the TweetBERT outperformed a BERT model.\nFor example, from Table V TweetBERT outperformed BioBERT by 12 .81% in SQuAD dataset. On the other hand,\nnegative value represents by how much the existing BERT model outperformed the TweetBERT model. To ﬁnd the\nmost suitable model overall on all the datasets the total of all the marginal performance of each BERT model was\ncalculated. In the Total row positive and negative number indicates the value by which TweetBERT performs better\nor worst than that BERT model. Both Table V and IV show that overall RoBERTa performs the best.\nDomain Type Datasets Metrics BERT Biobert SciBERT RoBERTa Albert TweetBERT v1 TweetBERT v2\nMNLI A 84.43 86.27 84.51 90.28 90.83 90.91 90.51\nQQP A 72.1 85.65 73.47 92.21 92.25 86.37 88.83\nQNLI A 90.51 90.28 88.34 94.72 95.37 91.25 91.21\nSST A 93.58 93.86 94.25 96.4 96.99 92.43 94.38\nCoLA A 60.61 65.83 61.72 68 71.42 68.42 71\nSTS PC 86.51 87.31 87.14 92.41 96.94 90.2 94.41\nMRPC A 89.3 85.04 90.78 90.9 90.9 88.64 91.79\nGLUE\nRTE A 70.11 75.72 66.26 86.65 89.21 75.23 91.3\nSQuad A 81.66 72.22 84.69 94.63 85.3 69.84 75.78\nSWAG A 86.23 82.71 84.44 90.16 88.57 85.47 88.86\nGeneral\nQA\nRACE A 69.23 80.9 78.58 81.31 82.37 81.96 81.74\nTABLE III\nShows the performance of different BERT models on general domain dataset. Highest accuracies are underlined\nDomain Type Datasets BERT Biobert SciBERT RoBERTa Albert\nGeneral\nGLUE\nMNLI 41.61 33.79 41.31 6.48 0.87\nQQP 51.14 5.017 48.62 -74.96 -75.87\nQNLI 7.79 9.97 24.95 -65.71 -88.98\nSST -17.91 -23.28 -31.65 -110.27 -151.49\nCoLA 19.82 7.57 17.50 1.31 -10.49\nSTS 27.35 22.77 23.79 -29.11 -220.26\nMRPC -6.16 24.06 -23.21 -24.83 -24.83\nRTE 17.12 -2.01 26.58 -85.54 -129.56\nQA\nSQuAD -64.44 -8.56 -96.99 -461.63 -105.17\nSWAG -5.519 15.96 6.61 -47.66 -27.12\nRACE 20.77 5.54 15.77 3.47 -2.32\nTotal 91.59 90.84 53.32 -888.49 -835.25\nTABLE IV\nShows the marginal percentage of existing BERT models in comparison to TweetBERT v1 on different General datasets\nDomain Type Datasets BERT Biobert SciBERT RoBERTa Albert\nGeneral\nGLUE\nMNLI 39.049 30.88 38.73 2.36 -3.48\nQQP 59.96 22.16 57.89 -43.38 -44.12\nQNLI 7.37 9.56 24.61 -66.47 -89.84\nSST 12.46 8.46 2.26 -56.11 -86.71\nCoLA 26.37 15.13 24.24 9.37 -1.46\nSTS 58.56 55.94 56.53 26.35 -82.67\nMRPC 23.27 45.12 10.95 9.78 9.78\nRTE 70.89 64.16 74.21 34.83 19.36\nQA\nSQuAD -32.06 12.81 -58.19 -351.0 -64.76\nSWAG 19.09 35.56 45.1 -13.21 2.53\nRACE 19.80 4.39 14.75 2.30 -3.57\nTotal 304.799 304.22 246.00 -445.20 -344.97\nTABLE V\nShows the marginal percentage of existing BERT models in comparison to TweetBERT v2 on different General datasets\n9\n∆ MP = AccuracyBERTmodel − AccuracyTweetBERTs\n100 − AccuracyBERTmodel\n× 100 (1)\nSecondly, the evaluation of the BERT models on 12 diﬀerent biomedical domain datasets is shown in Table VI.\nPrecision, recall, and f1 score are used as metrics for measuring performance. It shows that, although BioBERT was\npre-trained on millions of biomedical corpus, RoBERTa and TweetBERT outperforms BioBERT in all dataset types\nincluding NER and relation extraction. TweetBERTs performed best or very close to the best in many of the biomedical\ndatasets. The the marginal performance of all the biomedical datasets between existing BERT models and TweetBERTs\nwere calculate and reported in Table VII and VIII respectively. Results in both the table indicates that TweetBERT\noutperforms BERT, BioBERT and SciBERT.\nDomain Type Datasets Metrics BERT Biobert SciBERT RoBERTa Albert TweetBERT v1 TweetBERT v2\nNCBI\nP\nR\nF\n88.30\n89.00\n88.60\n88.22\n91.25\n89.71\n88.57\n90.97\n91.15\n90.58\n90.43\n91.22\n89.83\n87.62\n91.33\n89.70\n90.38\n91.62\n89.69\nBC5CDR\nP\nR\nF\n86.47\n87.84\n87.15\n90.01\n90.28\n89.12\n90.64\n90.69\n89.03\n89.51\n89.61\n86.09\n87.83\n89.22\n88.86\n90.41\nSpecies\nP\nR\nF\n69.35\n74.05\n71.63\n72.80\n75.36\n74.06\n70.89\n75.82\n73.68\n84.25\n87.16\n84.76\n83.77\n85.90\n84.06\n85.18\n87.45\n84.89\n85.17\n88.31\n83.53\nBC5CDR A 91.5 93 93.46 93.73 93.14 92.4 92.83\nNER\nJNLPBA A 74.23 77.54 75.63 78.23 78.33 81.63 81.61\nGAD\nP\nR\nF\n79.21\n89.25\n83.25\n77.32\n82.68\n79.83\n80.18\n88.51\n80.28\n83.82\n90.14\n82.78\n83.41\n89.73\n82.01\n78.18\n91.81\n84.45\n78.11\n91.92\n85.57\nEUADR\nP\nR\nF\n75.45\n96.55\n84.62\n84.83\n90.81\n80.92\n74.91\n96.64\n85.41\n85.84\n89.5\n85.24\n85.76\n90.48\n84.11\n77.73\n92.31\n81.36\n75.95\n92.1\n79.39\nRE\nCHEMPROT\nP\nR\nF\n76.02\n71.60\n73.74\n77.02\n75.90\n76.46\n71.3\n80.17\n78.97\n79.32\n85.32\n87.55\n83.29\n86.10\n84.35\n85.63\n85.77\n87.69\n85.04\nMedSTS A 78.6 84.5 78.6 89.06 91.06 86.78 90.89Sentence Biosses A 71.2 82.7 74.23 88.77 91.25 80.27 83.96\nInference MedLNI A 75.4 80.5 75.36 86.39 90.13 82.16 88.41\nBiomedical\nDoc classif HoC A 80 82.9 80.12 87.83 91.48 82.71 86\nTABLE VI\nShows the performance of different BERT models on biomedical domain dataset. Highest accuracies are underlined.\nDomain Type Datasets BERT Biobert SciBERT RoBERTa Albert\nNCBI disease 9.64 0.91 9.88 -9.34 -1.27\nBC5CDR disease -0.14 -21.82 -30.02 -16.01\nSpecies 46.7 49.06 42.59 0.85 5.2\nBC5CDR chemical 10.58 -8.57 -16.20 -21.21 -10.78\nNER\nJNLPBA 28.71 18.21 24.62 15.61 15.22\nGAD 7.16 52.71 21.14 9.69 13.56\nEUADR -21.19 16.32 -27.75 -26.54 -17.3RE\nCHEMPROT 45.27 35.06 49.93 30.51 14\nMedSTS 38.22 14.70 38.22 -20.84 -47.87Sent sim Biosses 31.49 -14.047 23.43 -75.69 -125.48\nInference MedLNI 27.47 8.51 27.59 -31.08 -80.79\nBiomedical\nDoc Classiﬁ HoC 13.55 -1.11 13.02 -42.07 -102.93\nTotal 237.63 171.62 184.67 -200.12 -354.42\nTABLE VII\nShows the marginal percentage of existing BERT models in comparison to TweetBERT v1 on different Biomedical datasets.\nThirdly, BERT models on four scientiﬁc datasets were evaluated. Previously, with the introduction of SciBERT\nthere was statistical evidence that it performed remarkably better on scientiﬁc tasks. Although, Table IX show that\nTweetBERT performed best in only two datasets Table XI show that TweetBERTv2 outperformed SciBERT and\nit is more suitable to use TweetBERTv2 to evaluate scientiﬁc tasks rather than using SciBERT. One of the main\nreason of TweetBERTv2 performing better than SciBERT is because of using SciBERT’s vocabulary when pre-training\nTweetBERTv2.\nFinally, all the BERT models were evaluated on tweets sentiment and classiﬁcation datasets. As the TweetBERTs\nwere pre-trained on millions of tweets it outperformed all existing BERT models, as expected. Table XII records\nthe performance of the BERT models including our TweetBERT. Table XIII shows that the highest total marginal\nperformance is 159 .15% when SciBERT and TweetBERTv1 are compared. Table XIV, on the other hand, shows that\nthe lowest marginal performance, 167 .1%, is greater than the highest marginal performance from Table XIII. As a\nresult, it can concluded that TweetBERTv2 performs signiﬁcantly better than TweetBERTv1 in Twitter domain tasks.\n10\nDomain Type Datasets BERT Biobert SciBERT RoBERTa Albert\nNCBI disease 9.56 -0.19 9.79 -9.44 -1.37\nBC5CDR disease 6.2 25.36 4 -2.45 8.57\nSpecies 41.9 36.5 37.4 -8.07 -3.32\nBC5CDR chemical 15.64 -2.42 -9.63 -14.35 -4.51\nNER\nJNLPBA 28.63 18.12 24.53 15.52 15.13\nGAD 13.85 28.45 29.67 16.2 19.78\nEUADR -30.49 -5.18 -37.55 -35.98 -26.3RE\nCHEMPROT 43.03 36.44 47.87 27.65 10.47\nMedSTS 57.42 41.22 57.42 16.72 -1.90Sen sim Biosses 44.30 7.28 37.75 -42.83 -83.31\nInference MedLNI 52.88 40.56 52.96 14.84 -17.42\nBiomedical\nDoc Classiﬁ HoC 30 18.12 29.57 -15.03 -64.31\nTotal 306.75 244.27 283.81 -37.21 -148.51\nTABLE VIII\nShows the marginal percentage of existing BERT models in comparison to TweetBERT v2 on different Biomedical datasets.\nDomain Type Datasets Metrics BERT Biobert SciBERT RoBERTa Albert TweetBERT v1 TweetBERT v2\nPaper feild A 55.06 56.22 65.71 63.48 62.85 58.12 66.49\nSci-cite A 84.33 85.11 85.42 87.16 86.68 88.5 88.56Text Classi\nSci-RE A 63.55 65.42 65.77 66.79 68.46 68.85 66.82Scientiﬁc\nParsing Genia A 64.81 67.71 72.3 76.95 78.45 67.98 70\nTABLE IX\nShows the performance of different BERT models on scientific domain dataset. Highest accuracies are underlined.\nDomain Type Datasets BERT Biobert SciBERT RoBERTa Albert\npaper feild 6.80 4.33 -22.13 -14.67 -12.73\nsci-cite 26.61 22.76 21.12 10.43 13.66Text Classi\nsci-RE 14.54 9.91 8.99 6.20 1.23Scientiﬁc\nParsing Genia 9.00 0.83 -15.59 -38.91 -48.58\nTotal 56.96 37.86 -7.60 -36.95 -46.41\nTABLE X\nShows the marginal percentage of existing BERT models in comparison to TweetBERT v1 on different scientific datasets.\nDomain Type Datasets BERT Biobert SciBERT RoBERTa Albert\npaper feild 25.43 23.45 2.27 8.24 9.79\nsci-cite 26.99 23.16 21.53 10.90 14.11Text Classiﬁcation\nsci-RE 8.97 4.04 3.06 0.09 -5.19Scientiﬁc\nParsing genia 14.74 7.09 -8.30 -30.15 -39.21\nTotal 76.14 57.76 18.57 -10.91 -20.49\nTABLE XI\nShows the marginal percentage of existing BERT models in comparison to TweetBERT v2 on different scientific datasets.\nDomain Type Datasets Metrics BERT Biobert SciBERT RoBERTa Albert TweetBERTv1 TweetBERTv2\nTwitter\nSentiment Airline Senti A 85.2 84.17 82.73 88.68 87.08 89 92.99\nGender Classi A 80.65 80.22 72.23 80.74 82.22 85.02 89.75\nSentiment140 A 85.63 87.84 82.29 86.71 90.59 92.74 95.18\nPolitical Tweets A 69.99 69.34 64.66 72.01 69.57 75.13 78.79\nTABLE XII\nShows the performance of BERT models in different Twitter datasets. Highest accuracies are underlined.\nDomain Type Datasets BERT Biobert SciBERT RoBERTa Albert\nTwitter Sentiment\nAirline Senti 25.67 30.51 36.30 2.82 14.86\nGender Classi 22.58 24.26 34.21 22.22 15.74\nSentiment140 49.47 40.29 59.0 45.37 22.84\nPolitical Tweets 17.12 18.88 29.62 11.14 18.27\nTotal 114.86 113.95 159.15 81.56 71.72\nTABLE XIII\nShows the marginal percentage of existing BERT models in comparison to TweetBERTv1 on different Twitter datasets.\nDomain Type Datasets BERT Biobert SciBERT RoBERTa Albert\nTwitter Sentiment\nAirline Senti 52.63 55.716 59.40 38.07 45.74\nGender Classi 47.02 48.17 54.98 46.78 42.35\nSentiment140 66.45 60.36 72.78 63.73 48.77\nPolitical Tweets 29.32 30.82 39.98 24.22 30.29\nTotal 195.44 195.08 227.16 172.81 167.17\nTABLE XIV\nShows the marginal percentage of existing BERT models in comparison to TweetBERTv2 on different Twitter datasets.\n11\nVI. Conclusion\nTwitter is a popular social networking site, which contain valuable data, where analyzing the content is particularly\nchallenging. Tweets are usually written in an informal structure, and as a consequence, using language models trained\non general domain corpora like BERT or other domains such as BioBERT often gives unsatisfactory results. Hence,\ntwo versions of TweetBERT are introduced, which are pre-trained language representation models used for Twitter\ntext mining. This paper also discusses how the data was collected from the big data analytics platform for pre-training\nTweetBERT. Millions of tweets were extracted and cleaned from this platform. Moreover, detailed discussion of pre-\ntraining TweetBERT models are included. TweetBERTv1 was initialized using weights from BERT and then pre-trained\non a tweet corpus. In the case of TweetBERTv2, ﬁrst the model is initialized with weights from AlBERT and used\nvocabularies from both BERT and SciBERT. Two main advantages of using BaseVocab and SciVocab are scientiﬁc\nanalysis can be carried out by studying tweets, and ALBERT is compatible with TweetBERTs and can be used in\nother evaluating other datasets in diﬀerent domains rather than just analyzing tweets.\nMoreover, this paper focuses on the datasets used to evaluate BERT models. Evaluation of TweetBERT models and\nﬁve other BERT models on 31 diﬀerent datasets from general, biomedical, scientiﬁc and Twitter domains and provide\na comparison between them. Section IV gives a detail description of most of the datasets used. Finally, the results\nfor the evaluation are released. It is shown that TweetBERT signiﬁcantly outperforms other BERT models on Twitter\ndatasets, and even on some other domain datasets, like BioBERT. The marginal performance that shows the amount\nby which a BERT model outperforms another BERT model is calculate. It shows that, especially in the case of Twitter\ndatasets, TweetBERTs has the best performance. TweetBERTv2 outperforms AlBERT by a total of 167.17% when\nevaluating Twitter datasets.\nOverall, an extensive discussion is provided about the necessity of language model speciﬁc to social media. We\nintroduce TweetBERTs and give comprehensive discussion about the methods, approaches and data used to pre-train\nTweetBERTs. Moreover, a detailed analysis is carried out and released the results by evaluating seven diﬀerent BERT\nmodels on 31 diﬀerent datasets.\nAcknowledgement\nThis research was funded by Natural Sciences and Engineering Research Council (NSERC), Canada. The authors\nwould also like to thank DaTALab researchers Dhivya, Zainab and Punar for providing their valuable inputs, Lakehead\nUniversity’s HPC for providing high-end GPUs and CASES building for providing the infrastructure.\nReferences\n[1] C. Castillo, M. Mendoza, and B. Poblete, “Information credibility on twitter,” in Proceedings of the 20th international conference on\nWorld wide web, 2011, pp. 675–684.\n[2] A. Go, R. Bhayani, and L. Huang, “Twitter sentiment classiﬁcation using distant supervision,” CS224N project report, Stanford, vol. 1,\nno. 12, p. 2009, 2009.\n[3] M. Giatsoglou, M. G. Vozalis, K. Diamantaras, A. Vakali, G. Sarigiannidis, and K. C. Chatzisavvas, “Sentiment analysis leveraging\nemotions and word embeddings,” Expert Systems with Applications, vol. 69, pp. 214–224, 2017.\n[4] X. Wang, A. McCallum, and X. Wei, “Topical n-grams: Phrase and topic discovery, with an application to information retrieval,” in\nSeventh IEEE international conference on data mining (ICDM 2007). IEEE, 2007, pp. 697–702.\n[5] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,”\narXiv preprint arXiv:1810.04805, 2018.\n[6] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “Albert: A lite bert for self-supervised learning of language\nrepresentations,” arXiv preprint arXiv:1909.11942, 2019.\n[7] E. Alsentzer, J. R. Murphy, W. Boag, W.-H. Weng, D. Jin, T. Naumann, W. Redmond, and M. B. McDermott, “Publicly available\nclinical bert embeddings,” NAACL HLT 2019, p. 72, 2019.\n[8] I. Beltagy, A. Cohan, and K. Lo, “Scibert: Pretrained contextualized embeddings for scientiﬁc text,” arXiv preprint arXiv:1903.10676,\n2019.\n[9] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “Biobert: a pre-trained biomedical language representation model\nfor biomedical text mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–1240, 2020.\n[10] M. Qudar and V. Mago, “A survey on language models,” 09 2020. [Online]. Available: https://www.researchgate.net/publication/\n344158120 A Survey on Language Models\n[11] A. Williams, N. Nangia, and S. R. Bowman, “A broad-coverage challenge corpus for sentence understanding through inference,” arXiv\npreprint arXiv:1704.05426, 2017.\n[12] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding by generative pre-training,” URL\nhttps://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.\n[13] W. Yoon, C. H. So, J. Lee, and J. Kang, “Collabonet: collaboration of deep neural networks for biomedical named entity recognition,”\nBMC bioinformatics, vol. 20, no. 10, p. 249, 2019.\n[14] A. Mnih and G. E. Hinton, “A scalable hierarchical distributed language model,” in Advances in neural information processing systems,\n2009, pp. 1081–1088.\n[15] R. I. Do˘ gan, A. N´ ev´ eol, and Z. Lu, “A context-blocks model for identifying clinical relationships in patient records,”BMC bioinformatics,\nvol. 12, no. S3, p. S3, 2011.\n[16] W. A and C. K., “Bert has a mouth, and it must speak: Bert as a markov random ﬁeld language model. ” IEEE Spoken Language\nTechnology Workshop, 2019.\n[17] M. Martin, S. Marcel, and Kummervold, “Covid-twitter-bert: A natural language processing model to analyse covid-19 content on\ntwitter. ”arXiv preprint arXiv:2005.07503, 2020.\n12\n[18] T. Pires, E. Schlinger, and D. Garrette, “How multilingual is multilingual bert?” arXiv preprint arXiv:1906.01502, 2019.\n[19] J. Li, Y. Sun, R. J. Johnson, D. Sciaky, C.-H. Wei, R. Leaman, A. P. Davis, C. J. Mattingly, T. C. Wiegers, and Z. Lu, “Biocreative v\ncdr task corpus: a resource for chemical disease relation extraction,” Database, vol. 2016, 2016.\n[20] W. Ammar, D. Groeneveld, C. Bhagavatula, I. Beltagy, M. Crawford, D. Downey, J. Dunkelberger, A. Elgohary, S. Feldman, V. Ha\net al., “Construction of the literature graph in semantic scholar,” inProceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers), 2018, pp. 84–91.\n[21] M. Gardner, J. Grus, M. Neumann, O. Tafjord, P. Dasigi, N. Liu, M. Peters, M. Schmitz, and L. Zettlemoyer, “Allennlp: A deep\nsemantic natural language processing platform,” arXiv preprint arXiv:1803.07640, 2018.\n[22] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly\noptimized bert pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\n[23] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models are unsupervised multitask learners,” OpenAI\nBlog, vol. 1, no. 8, p. 9, 2019.\n[24] T. J. Steenburgh, J. Avery, and N. Dahod, “Hubspot: Inbound marketing and web 2.0,” HBS Case, no. 509-049, 2009.\n[25] C. H. Mendhe, N. Henderson, G. Srivastava, and V. Mago, “A scalable platform to collect, store, visualize, and analyze big data in real\ntime,” IEEE Transactions on Computational Social Systems, 2020.\n[26] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, “Glue: A multi-task benchmark and analysis platform for natural\nlanguage understanding,” arXiv preprint arXiv:1804.07461, 2018.\n[27] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+ questions for machine comprehension of text,” arXiv preprint\narXiv:1606.05250, 2016.\n[28] R. Zellers, Y. Bisk, R. Schwartz, and Y. Choi, “Swag: A large-scale adversarial dataset for grounded commonsense inference,” in\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 93–104.\n[29] A. Warstadt, A. Singh, and S. R. Bowman, “Neural network acceptability judgments,”Transactions of the Association for Computational\nLinguistics, vol. 7, pp. 625–641, 2019.\n[30] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts, “Recursive deep models for semantic compositionality\nover a sentiment treebank,” in Proceedings of the 2013 conference on empirical methods in natural language processing, 2013, pp. 1631–\n1642.\n[31] Y. Yang, S. Yuan, D. Cer, S.-y. Kong, N. Constant, P. Pilar, H. Ge, Y.-H. Sung, B. Strope, and R. Kurzweil, “Learning semantic textual\nsimilarity from conversations,” in Proceedings of The Third Workshop on Representation Learning for NLP, 2018, pp. 164–174.\n[32] M. Richardson, C. J. Burges, and E. Renshaw, “Mctest: A challenge dataset for the open-domain machine comprehension of text,” in\nProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 2013, pp. 193–203.\n[33] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for word representation,” in Proceedings of the 2014 conference\non empirical methods in natural language processing (EMNLP), 2014, pp. 1532–1543.\n[34] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, “Race: Large-scale reading comprehension dataset from examinations,” in Proceedings\nof the 2017 Conference on Empirical Methods in Natural Language Processing, 2017, pp. 785–794.\n[35] W. Wang, M. Yan, and C. Wu, “Multi-granularity hierarchical attention fusion networks for reading comprehension and question\nanswering,” in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\n2018, pp. 1705–1714.\n[36] M. Habibi, L. Weber, M. Neves, D. L. Wiegandt, and U. Leser, “Deep learning with word embeddings improves biomedical named\nentity recognition,” Bioinformatics, vol. 33, no. 14, pp. i37–i48, 2017.\n[37] B. McCann, J. Bradbury, C. Xiong, and R. Socher, “Learned in translation: Contextualized word vectors,” in Advances in Neural\nInformation Processing Systems, 2017, pp. 6294–6305.\n[38] R. Shetty and J. Laaksonen, “Video captioning with recurrent networks based on frame-and video-level features and visual content\nclassiﬁcation,” arXiv preprint arXiv:1512.02949, 2015.\n[39] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning, “A large annotated corpus for learning natural language inference,” arXiv\npreprint arXiv:1508.05326, 2015.\n[40] J. P. Chiu and E. Nichols, “Named entity recognition with bidirectional lstm-cnns,” Transactions of the Association for Computational\nLinguistics, vol. 4, pp. 357–370, 2016.\n[41] G. Wiese, D. Weissenborn, and M. Neves, “Neural domain adaptation for biomedical question answering,” in Proceedings of the 21st\nConference on Computational Natural Language Learning (CoNLL 2017), 2017, pp. 281–289.\n[42] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia, “Semantic textual similarity-multilingual and cross-lingual focused\nevaluation,” 2017.\n[43] Y. Peng, S. Yan, and Z. Lu, “Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten\nbenchmarking datasets,” arXiv preprint arXiv:1906.05474, 2019.\n[44] G. So˘ gancıo˘ glu, H.¨Ozt¨ urk, and A.¨Ozg¨ ur, “Biosses: a semantic sentence similarity estimation system for the biomedical domain,”\nBioinformatics, vol. 33, no. 14, pp. i49–i58, 2017.\n[45] Q. Chen, Y. Peng, and Z. Lu, “Biosentvec: creating sentence embeddings for biomedical texts,” in 2019 IEEE International Conference\non Healthcare Informatics (ICHI). IEEE, 2019, pp. 1–5.\n[46] M.-S. Huang, P.-T. Lai, R. T.-H. Tsai, and W.-L. Hsu, “Revised jnlpba corpus: A revised version of biomedical ner corpus for relation\nextraction task,” arXiv preprint arXiv:1901.10219, 2019.\n[47] O. Taboureau, S. K. Nielsen, K. Audouze, N. Weinhold, D. Edsg¨ ard, F. S. Roque, I. Kouskoumvekaki, A. Bora, R. Curpan, T. S. Jensen\net al., “Chemprot: a disease chemical biology database,” Nucleic acids research, vol. 39, no. suppl 1, pp. D367–D372, 2010.\n[48] X. Jin, S. Kim, J. Han, L. Cao, and Z. Yin, “Gad: general activity detection for fast clustering on large data,” in Proceedings of the\n2009 SIAM international conference on data mining. SIAM, 2009, pp. 2–13.\n[49] F. Korn, H. V. Jagadish, and C. Faloutsos, “Eﬃciently supporting ad hoc queries in large datasets of time sequences,” Acm Sigmod\nRecord, vol. 26, no. 2, pp. 289–300, 1997.\n[50] A. Cohan, W. Ammar, M. van Zuylen, and F. Cady, “Structural scaﬀolds for citation intent classiﬁcation in scientiﬁc publications,”\narXiv preprint arXiv:1904.01608, 2019."
}