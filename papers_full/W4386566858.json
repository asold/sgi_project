{
  "title": "WinoDict: Probing language models for in-context word acquisition",
  "url": "https://openalex.org/W4386566858",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3014585547",
      "name": "Julian Martin Eisenschlos",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2574943366",
      "name": "Jeremy R. Cole",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2031764755",
      "name": "Fangyu. Liu",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2115385359",
      "name": "William W. Cohen",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1794039122",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W2799068938",
    "https://openalex.org/W2798837230",
    "https://openalex.org/W3198711991",
    "https://openalex.org/W2998554035",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W3209547093",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W3035733645",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2977235550",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W3036074945",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2900167100",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W2144209400",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W3105928338",
    "https://openalex.org/W3153675281"
  ],
  "abstract": "We introduce a new in-context learning paradigm to measure Large Language Models' (LLMs) ability to learn novel words during inference. In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task. Solving this task requires the model to make use of the dictionary definition of the new word given in the prompt. This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs compared to the original Winograd tasks decreases radically in our benchmark, thus identifying a limitation of current models and providing a benchmark to measure future improvements in LLMs ability to do in-context learning.",
  "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 94–102\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nWINO DICT : Probing language models for in-context word acquisition\nJulian Martin Eisenschlos1, Jeremy R. Cole1, Fangyu Liu2, William W. Cohen1\n1 Google Research 2 University of Cambridge\n{eisenjulian,jrcole,wcohen}@google.com fl399@cam.ac.uk\nAbstract\nWe introduce a new in-context learning\nparadigm to measure Large Language Models’\n(LLMs) ability to learn novel words during in-\nference. In particular, we rewrite Winograd-\nstyle co-reference resolution problems by re-\nplacing the key concept word with a synthetic\nbut plausible word that the model must under-\nstand to complete the task. Solving this task re-\nquires the model to make use of the dictionary\ndeﬁnition of the new word given in the prompt.\nThis benchmark addresses word acquisition,\none important aspect of the diachronic degra-\ndation known to afﬂict LLMs. As LLMs are\nfrozen in time at the moment they are trained,\nthey are normally unable to reﬂect the way\nlanguage changes over time. We show that\nthe accuracy of LLMs compared to the origi-\nnal Winograd tasks decreases radically in our\nbenchmark, thus identifying a limitation of\ncurrent models and providing a benchmark to\nmeasure future improvements in LLMs ability\nto do in-context learning.\n1 Introduction\nLarge Language Models (LLMs) such as GPT-\n3 (Brown et al., 2020) and PALM (Chowdhery\net al., 2022) can only learn from information that\nis in their training corpus. However, this is nat-\nurally limiting because the training corpus itself\nis bounded in time to the point of its collection.\nAs a result, recent work has studied how to adapt\nsuch models to new data without an expensive re-\ntraining phase. Methods range from using semi-\nparametric methods with access to external mem-\nory (e.g., Guu et al. 2020; Lewis et al. 2020), to con-\ntinual learning (e.g., Dhingra et al. 2022; Lazaridou\net al. 2021), to parameter efﬁcient ﬁne-tuning (e.g.,\nBen Zaken et al. 2022; Pfeiffer et al. 2021).\nMuch of this work concerns factual knowledge\nor task distribution shifts. However, language\nalso changes subtly: for instance, the popularity\nor meaning of individual words can change over\ntime. In fact, such shifts also cause a consistent de-\ncrease in model performance for downstream tasks\n(Huang and Paul, 2018; Jaidka et al., 2018; Lukes\nand Søgaard, 2018; Florio et al., 2020).\nAcquiring new words through either examples or\ndeﬁnitions is therefore an important test of LLMs’\nability to overcome diachronic degradation. With\nin-context learning having emerged as the primary\nway to interact with LLMs (Brown et al., 2020),\nwe propose to study LLMs capability of acquiring\nnew vocabulary via prompting.\nWe propose WINO DICT , a novel benchmark for\nword acquisition for LLMs. Word acquisition is\nchallenging to study in a realistic setting as it is\nhard to know which terms a model has already been\nexposed to. To overcome this, we rely on a heuris-\ntic method to introduce newly invented words and\ndeﬁne them in terms of existing concepts. Follow-\ning previous work (Chakrabarty et al., 2022), we\nincorporate the required knowledge into the prompt.\nWe then ask models to perform tasks that require\nsuccessfully interpreting the invented words.\nWe consider the English co-reference resolution\ndatasets Winograd Schema Challenge (Levesque\net al., 2012) and WinoGrande (Sakaguchi et al.,\n2020). The examples are built in pairs with mini-\nmal changes, which allow the identiﬁcation of the\nkey concept that must be understood to solve the\nexample. An example of WINO DICT can be seen\nin Figure 1. Our contributions are the following:\n(a) We propose WINO DICT , a method and dataset\nto test models for word acquisition skills.\n(b) We benchmark the performance of several state-\nof-the-art models across scale and number of shots.\n(c) We analyze the effect of prompt, POS tags, word\nlikelihood and similarity for ease of acquisition.\nThese results help us understand the challenges\nfor incorporating new concepts into LLMs. The\ncode to build the dataset has been open-sourced. 1\n1https://github.com/google-\nresearch/language/tree/master/language/wino_dict\n94\nWINOGRAD\nThe city councilmen refused the demonstrators\na permit because they feared violence.\nThe city councilmen refused the demonstrators\na permit because they advocated violence.\nWINO DICT\nThe verb to plest means to be scared of, or\nwant to avoid an object.\nThe verb to sparn means to to publicly recom-\nmend or support.\nThe city councilmen refused the demonstrators\na permit because they plested violence.\nThe city councilmen refused the demonstrators\na permit because they sparned violence.\nFigure 1: An example pair from W INO DICT together with its original W INOGRAD source. The task is to decide\nwhether they refers to the city councilman or the demonstrators. Here, the correct answer is shown in blue and the\nincorrect answer in red. Note that in both cases, it is necessary to understand the meaning of the bolded key concept\nto resolve the co-reference, which we identify in WINOGRAD and substitute for a new word in WINO DICT .\n2 Methods\nWINO DICT , like WINOGRAD and WINO GRANDE ,\nis a co-reference resolution task in a binary choice\nsetup. A model is given two alternative noun\nphrases, and has to decide which one is more likely\nto correspond to a highlighted pronoun or blank.\n2.1 Dataset Construction\nTo buildWINO DICT , we rely on the fact that the ex-\namples from WINOGRAD and WINO GRANDE are\nconstructed from contrasting pairs (Gardner et al.,\n2020; Kaushik et al., 2020). Each instance differs\nin a minimal way from its counterpart with the true\nlabel reversed. This allows the identiﬁcation of\nthe key concept that needs to be parsed in order to\nresolve the task. In Figure 1 for instance, the verbs\nfear and advocate correspond to the key concepts.\nWINOGRAD and WINO GRANDE are similar;\nhowever, WINO GRANDE is larger, uses blanks in-\nstead of pronouns, and the dataset has been ﬁltered\nfor co-occurrence bias between the key concept\nand the correct noun-phrase. This results in some\nexamples that do not have a corresponding paired\nexample with a different key concept.\nTo create our examples, we ﬁrst recover the pair-\ning between the examples, dropping those with no\npairing. Secondly, we identify the key concept to-\nkens that change from one example to the other,\ndropping examples where the key concept consists\nof multiple tokens. Finally we run the sentence\nthrough the spaCy 2 syntactic analyzer and fetch\nWordNet3 deﬁnitions of the key concepts’ lemmas.\nIn the next section we show how the key concept\n2https://spacy.io\n3https://wordnet.princeton.edu (Miller, 1995)\nPOS WINOGRAD WINO GRANDE Total\nVERB 67 56 123\nNOUN 34 24 58\nADV 5 25 30\nADJ 74 211 285\nTotal 180 316 496\nOrig. Size 273 12,282 12,555\nSent. Len 16.34 18.93 17.99\nDef. Len 14.07 14.3 14.22\nTable 1: Statistics for the different part-of-speech tags\nin the synthetic words, as well as average number of\ntokens for the main statement and the word deﬁnition.\nWINO DICT consists of 496 examples.\ntokens are replaced by synthetic words. This re-\nsults in 496 examples: additional information can\nbe found in Table 1.\n2.2 New Word Creation\nOur goal is to create plausible synthetic words. We\ncreate plausible words using a simple probabilis-\ntic model of every one-, two-, and three-letter se-\nquence that is trained on the vocabulary of English\nwords4. These three-letter sequences are then sam-\npled and combined to form new synthetic words.\nWe ﬁlter any words that have a three letter sequence\nthat does not occur in any other English word. We\nthen sample the words based on their log proba-\nbility, placing them into ﬁve buckets and keeping\naround 500 for each bucket.\nThe morphology for each word is created by ag-\ngregating over a sample of proposed synthetic word\nmorphologies. The last 2-4 letters of each word\n(depending on the morphological edit) form a suf-\nﬁx dictionary that is used as a simple substitution\n4https://pypi.org/project/english-words\n95\ndictionary for the remaining words: failures are\ndropped. This produces a combination of regular\nand irregular conjugations over the new words.\n2.3 Answer Scoring\nEach instance in WINO DICT consists of a new\nword with its deﬁnition d, a statement containing\na blank where x and y correspond to the text be-\nfore and after the blank respectively, and two noun\nphrases o1 and o2. The task consists of identifying\nwhich of the noun-phrases better ﬁts the blank.\nPALM, GPT-3 and its predecessors (Radford\net al., 2019) use the method proposed by Trinh\nand Le (2018) to evaluate WINOGRAD and WINO -\nGRANDE , which we explain below. A prediction\nscore is obtained through comparing the log like-\nlihood of the same continuation y of two possi-\nble preﬁx texts ( x : o1 and x : o2) where the\nco-reference pronoun or blank marker has been\nreplaced. It is correct if it scores the sufﬁx higher\nfor the preﬁx with the correct interpretation of the\nco-reference problem.\nln PΘ (y|x : o1) −ln PΘ (y|x : o2)\n=\nn∑\ni=0\n(\nln PΘ (yi|y<i : x : o1) −ln PΘ (yi|y<i : x : o2)\n)\nwhere : denotes concatenation and variables map to:\nx = “The city councilmen refused the\ndemonstrators a permit because”\no1 = “the city councilmen”\no2 = “the demonstrators”\n{yi}n\ni=1 = y = “feared violence.”\nIn our setup we add the deﬁnition of the new con-\ncept as a sufﬁx to the shared term y, thus replacing\nit with y : d. This achieves higher accuracy than\nthe alternatives. Note that this means that the model\nis scoring the deﬁnition rather than conditioning\non it. See Section 4 and Table 4 for a discussion\nof other variants of the setup, including adding the\ndeﬁnition as a preﬁx.\n3 Experiments\nIn this work we test GPT-3 (Brown et al., 2020),\nand PALM (Chowdhery et al., 2022) models of\nvarious sizes, ranging from 3B to 540B parameters.\nAppendix A has more details on the model sizes.\nAs in the original in-context learning evaluations,\nwe try 0, 1, and 5-shot experiments, using random\nexamples to build the prompt. We compare to both\na zero-shot human evaluation as well as the original\nsource datasets with only our ﬁltered examples.\nThe main experimental results are shown in Ta-\nble 2. We observe a consistent gap of 18 or more\npoints between WINO DICT examples and their\noriginal counterparts. Similar to trends observed\nin other datasets (Chowdhery et al., 2022), scaling\nthe number of shots and model size consistently\nimproves accuracy. The three smaller versions of\nGPT-3 and PALM-8B all perform close to random.\nWe verify that omitting any information of the\nnew word yields random results for even the best\nPALM-540B model. We discuss this and other\nprompting strategies in more detail in Appendix B.\n3.1 Human Evaluation\nThe human accuracy on WINO DICT is estimated\nusing the responses of 10 volunteers. No native\nEnglish proﬁciency was required for participation.\nParticipants were told that the aim of the research\nis to study how to use words based on their deﬁni-\ntion. They were presented with 15 sentences that\nincluded a pronoun / blank and asked to select one\nof two noun phrases it most likely refers to.\n3.2 Foreign Inspired Words\nTo explore a more realistic scenario, we conduct an\nexperiment using 20 hand-written WINO DICT -like\nexamples whose deﬁnitions are inspired by foreign\nwords that do not have a clear single-word deﬁni-\ntion in English. For instance, “estrenar” refers to\nwearing a piece of clothing for the ﬁrst time, which\ndoes not have a clear English word equivalent. We\ncan then create an example that requires knowing\nthis deﬁnition, such as “I really [ love | hate] my\nnew dress. I can’t wait to <word> it.\nIn conducting this experiment, we substitute syn-\nthetic words instead of using the original foreign\nwords, and the deﬁnitions of the words themselves\nmay not correspond to native speakers’ precise un-\nderstanding: in other words, these are meant to be\ntrue new words and data leakage should be minimal.\nWe run the same experiment on these examples. Re-\nsults are in Table 3 and full details in Appendix C.\nOverall, the numbers are comparable to the WINO -\nDICT results, suggesting that models are unlikely\nto be solving the task using a reverse dictionary.\n4 Prompt Analysis\nIn this section, we discuss alternative formulations\nfor the prompts used in WINO DICT . We focus on\n96\nWINOGRAD WINOGRANDE\nWINODICT (Ours) Original W INODICT (Ours) Original\nShots 0 1 5 0 1 5 0 1 5 0 1 5\nPALM 8B 59.2±1.6 57.1±2.1 59.1±1.6 83.3 83.3 87.2 51.8±1.6 54.2±0.4 52.4±1.1 69.3 65.5 67.4\nPALM 62B 62.2±0.6 65.9±3.6 70.3±1.3 91.1 90.0 92.2 56.7±1.1 58.2±1.0 59.7±1.1 76.6 77.8 78.2\nPALM 540B 65.9±2.5 75.4±1.3 78.6±0.6 92.8 92.2 95.6 60.3±1.4 63.9±2.3 68.5±1.9 80.1 81.3 85.8\nGPT-3 Ada 51.9±2.2 50.9±1.7 50.2±4.3 60.0 57.8 61.7 52.2±1.2 52.0±3.6 49.4±1.7 48.1 53.8 53.2\nGPT-3 Babbage 51.8±0.8 52.8±2.0 54.4±2.3 75.6 71.7 65.6 50.8±1.7 52.3±1.0 52.2±0.8 52.8 55.1 56.6\nGPT-3 Curie 54.2±1.6 54.6±2.4 59.9±1.5 85.0 81.7 82.8 50.2±1.5 50.6±1.6 52.2±1.0 62.0 61.1 60.8\nGPT-3 Davinci 60.3±1.3 63.6±2.3 72.9±0.5 88.3 85.0 91.1 55.0±1.1 55.7±1.4 61.3±1.4 71.8 69.6 72.5\nHuman 91.7 96.5∗ 83.3 94.0∗\nTable 2: Binary classiﬁcation accuracy on W INO DICT vs. the original datasets using average and standard devi-\nation across 5 sets of new words. Original results may differ from the ones reported by Chowdhery et al. (2022)\nsince only a subset of the examples are used. A consistent gap of 18+ points appears when comparing against the\noriginal sets. The original human evaluation numbers denoted with ∗ are taken from Sakaguchi et al. (2020).\nShots 0 1 5\nPALM 540B 68.7 73.0 76.0\nGPT-3 Davinci 61.0 56.0 68.0\nTable 3: Binary classiﬁcation accuracy on the foreign-\ninspired new words averaged over ﬁve runs. Overall,\naccuracy is comparable to the original dataset.\nWord Type Prompt W INOGRAD WINO GRANDE\nSynthetic Def Preﬁx 72.2 62.7\nDef Sufﬁx ∗ 78.6 68.5\nSyn Preﬁx 74.1 60.5\nSyn Sufﬁx 88.4 78.2\nEmpty 52.0 51.9\nOriginal Def Preﬁx 85.5 74.0\nDef Sufﬁx 93.8 84.4\nSyn Preﬁx 87.2 74.3\nSyn Sufﬁx 91.6 83.2\nEmpty ∗ 95.6 85.8\nMeaning\nshift\nDef Preﬁx 66.1 60.8\nDef Sufﬁx 75.6 60.4\nSyn Preﬁx 69.4 60.1\nSyn Sufﬁx 83.3 74.7\nEmpty 51.1 49.7\nTable 4: Analysis of different prompts. We show the\nresults on the synthetic words, original words, and ex-\nisting words but assigned to a new meaning (“Meaning\nshift”). Preﬁx/Sufﬁx correspond to the location of the\ndeﬁnition, Syn/Def corresponds to using the deﬁnition\nor synonyms of the synthetic word. Empty means nei-\nther (should be random for synthetic words). Provid-\ning synonyms yields the best results. All results are\non PALM-540B 5-shot. The lines marked with ∗ corre-\nspond to the experiments in Table 2.\nthe best-performing PALM-540B model using a\n5-shot setup. See Table 4 for the full results.\nConcretely, we vary the prompts along a few\naxes. First, we test whether the deﬁnition should be\npart of the preﬁx, where the model would condition\non it, or the sufﬁx, where the model would score it.\nNote that in all setups, putting the deﬁnition in the\nsufﬁx works consistently better.\nAdditionally, we test whether the task is made\neasier by using synonyms instead of deﬁnitions.\nThis task indeed appears to be easier, potentially\nbecause the model needs to learn only a simple sub-\nstitution between the new word and the provided\nsynonym, whose deﬁnition it knows. We focus on\ndeﬁnitions in this work as exact synonyms would\nrarely be available for novel words.\nAs a baseline, we also examine the “Empty”\nsetup, where the model is provided no infor-\nmation about the new word. We observe that\nPALM approximates random guessing without be-\ning given the deﬁnition, showing that the task re-\nmains roughly unbiased.\nWe additionally test the model’s performance on\nthe original task where we also provide the deﬁni-\ntion of the key concept. Note that the “Empty” case\nhere corresponds precisely to the original task. In-\nterestingly, the deﬁnition seems to serve as a slight\ndistraction, especially as a preﬁx, though accuracy\nis still well above the model’s performance on the\nsynthetic words.\nFinally, in the “Meaning shift” scenario, we map\nnew deﬁnitions to already known words. This task\nappears to be even more difﬁcult than the standard\nWINO DICT setup, implying that the model is dis-\ntracted by the surface forms of the words.\n5 New Word Analysis\nSeveral factors can affect the capabilities for word\nacquisition of LLMs. We investigate several at-\ntributes, split into quartiles, using PALM-540B\nwith 5-shots, which is the best model from Table 2.\n97\nWe consider the following attributes: (1) the\npart-of-speech of the synthetic word; (2) the av-\nerage model negative log likelihood (NLL) of the\ntwo model predictions, which measures the likeli-\nhood of the sufﬁx for both preﬁxes; (3) the number\nof SentencePiece (Kudo and Richardson, 2018) to-\nkens in the synthetic word, to investigate the effect\nof model tokenization; (4) the number of Sentence-\nPiece tokens in the deﬁnition of the synthetic word,\nto investigate if longer deﬁnitions are more chal-\nlenging; (5) the Levenshtein edit distance between\nthe synthetic and original word, to investigate if\nsimilar words are easier; and (6) the likelihood\nof the new word as computed by our probabilis-\ntic model of three-letter sequences, to see if less\nprobable words are more difﬁcult to acquire.\nOf the six attributes, the two most correlated with\naccuracy are (4) the deﬁnition length and (2) the av-\nerage NLL. We observe no clear pattern in the other\nfour attributes. In Figure 2 we show their effect in\neach quartile. The effect of deﬁnition length indi-\ncated that the 25% longest deﬁnitions are the hard-\nest to acquire by a signiﬁcant margin ( 12% rela-\ntive drop for WINOGRAD , 5% for WINO GRANDE ).\nThe relative accuracy drop for the largest quartile\nof the NLL average is13% for WINOGRAD and 4%\nfor WINO GRANDE . The drop in NLL suggests that\nwhen models assigns low probabilities to answers,\nthey make more mistakes: the low probability may\nindicate the model has a poor understanding of the\npreﬁx so scores the sufﬁx randomly.\nQuartile\nAccuracy\n0.6\n0.7\n0.8\n0.9\n1.0\n1 2 3 4\nWinograd Def. Length Winograd NLL Avg.\nWinoGrande Def. Length WinoGrande NLL Avg.\nFigure 2: Effect on W INO DICT PALM-540B 5-shot\naccuracy on each quartile splitting by deﬁnition length\nand by average NLL score. Longer deﬁnitions and\nhigher NLL correlate with lower accuracy.\n6 Related Work\nWord acquisition for LLMs.Inspired by devel-\nopmental linguistics (Carey and Bartlett, 1978),\nRadford et al. (2019) succeeded to prompt GPT-3\nto generate plausible example sentences based on\ndeﬁnitions of synthetic words. Unlike WINO DICT ,\nthe evaluation was purely qualitative.\nCommon sense. Li et al. (2021) study how\nprompt structures and scoring methods affect the\nperformance of LLMs on common sense tasks in-\ncluding WINO GRANDE , where they observe the\nleast variation. The format from WINOGRAD has\nbeen subsequently used to probe models for other\nphenomena such as explanations (Zhang et al.,\n2020) and gender bias (Zhao et al., 2018).\nBenchmarks for lexical knowledge.Schick and\nSchütze (2020) introduce a benchmark for prob-\ning a model’s knowledge of the properties of rare\nwords. Hill et al. (2016) train models to match\nword and deﬁnition representations, which they\napply to a reverse dictionary task.\n7 Conclusion\nIn this work, we study the question of in-context\nword acquisition by large language models. While\nnon-trivial to measure, the ability to incorporate\nknowledge about new words in-context may be use-\nful to decrease the effect of diachronic degradation.\nWe design a mechanism to transform Winograd-\nstyle tasks into challenging probes for reasoning\non the meaning assigned to synthetic words, al-\nlowing for a more objective measurement of word\nacquisition. We study the results of models of mul-\ntiple sizes and families and conclude that while\nthe problem becomes easier with scale, there is\nstill a substantial gap with human performance and\nthe original WINOGRAD and WINO GRANDE tasks,\ndemonstrating the difﬁculty of the proposed task.\nFinally, we show that acquiring novel deﬁnitions is\nof similar difﬁculty, indicating the task is realistic.\nLimitations\nThe task described in this work is synthetic and\nthus an imperfect measure of the phenomena un-\nder study. The words in WINO DICT are synthetic\nwords with deﬁnitions copied from existing con-\ncepts; the model could thus solve WINO DICT with\na reduction to a reverse dictionary task. To par-\ntially address this, we conducted pilot experiments\ndescribed in Section 3.2.\nAdditionally, the choice of prompts for LLMs\nhas been shown to have a large inﬂuence on the re-\nsulting accuracy (Min et al., 2022; Lu et al., 2022).\nWhile we tried multiple templates, it is possible\nthat substantially better prompts exist for this task.\n98\nAcknowledgements\nWe thank Yasemin Altun, Iulia-Maria Com¸ sa and\nSrini Narayanan, as well as our anonymous review-\ners, for their valuable feedback.\nEthics Statement\nAll annotations for the human evaluation were done\nby English speaking volunteers using a multiple\nchoice survey tool. No personal identiﬁable infor-\nmation was collected.\nReferences\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.\n2022. BitFit: Simple parameter-efﬁcient ﬁne-tuning\nfor transformer-based masked language-models. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 1–9, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nSusan Carey and E. Bartlett. 1978. Acquiring a single\nnew word. Proceedings of the Stanford Child Lan-\nguage Conference, 15:17–29.\nTuhin Chakrabarty, Yejin Choi, and Vered Shwartz.\n2022. It’s not rocket science: Interpreting ﬁgurative\nlanguage in narratives. Transactions of the Associa-\ntion for Computational Linguistics, 10:589–606.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nBhuwan Dhingra, Jeremy Cole, Julian Eisenschlos,\nDaniel Gillick, Jacob Eisenstein, and William Co-\nhen. 2022. Time-aware language models as tempo-\nral knowledge bases. Transactions of the Associa-\ntion for Computational Linguistics, 10(0):257–273.\nKomal Florio, Valerio Basile, Marco Polignano, Pier-\npaolo Basile, and Viviana Patti. 2020. Time of your\nhate: The challenge of time in hate speech detection\non social media. Applied Sciences, 10(12):4180.\nMatt Gardner, Yoav Artzi, Victoria Basmov, Jonathan\nBerant, Ben Bogin, Sihao Chen, Pradeep Dasigi,\nDheeru Dua, Yanai Elazar, Ananth Gottumukkala,\nNitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco,\nDaniel Khashabi, Kevin Lin, Jiangming Liu, Nel-\nson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer\nSingh, Noah A. Smith, Sanjay Subramanian, Reut\nTsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.\n2020. Evaluating models’ local decision boundaries\nvia contrast sets. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1307–1323, Online. Association for Computational\nLinguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Mingwei Chang. 2020. Retrieval aug-\nmented language model pre-training. In Proceed-\nings of the 37th International Conference on Ma-\nchine Learning, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 3929–3938. PMLR.\nFelix Hill, Kyunghyun Cho, Anna Korhonen, and\nYoshua Bengio. 2016. Learning to understand\nphrases by embedding the dictionary. Transactions\nof the Association for Computational Linguistics ,\n4:17–30.\nXiaolei Huang and Michael J. Paul. 2018. Examining\ntemporality in document classiﬁcation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 694–699, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nKokil Jaidka, Niyati Chhaya, and Lyle Ungar. 2018. Di-\nachronic degradation of language models: Insights\nfrom social media. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) , pages 195–\n200, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nDivyansh Kaushik, Eduard Hovy, and Zachary Lipton.\n2020. Learning the difference that makes a differ-\nence with counterfactually-augmented data. In Inter-\nnational Conference on Learning Representations.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAngeliki Lazaridou, Adhi Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\nTomas Kocisky, Sebastian Ruder, et al. 2021. Mind\nthe gap: Assessing temporal generalization in neural\nlanguage models. Advances in Neural Information\nProcessing Systems, 34:29348–29363.\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\n99\nProceedings of the Thirteenth International Confer-\nence on Principles of Knowledge Representation\nand Reasoning, KR’12, page 552–561. AAAI Press.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nXiang Lorraine Li, Adhi Kuncoro, Cyprien de Mas-\nson d’Autume, Phil Blunsom, and Aida Nematzadeh.\n2021. A systematic investigation of commonsense\nunderstanding in large language models. arXiv\npreprint arXiv:2111.00607.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\nRiedel, and Pontus Stenetorp. 2022. Fantastically or-\ndered prompts and where to ﬁnd them: Overcoming\nfew-shot prompt order sensitivity. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8086–8098, Dublin, Ireland. Association for\nComputational Linguistics.\nJan Lukes and Anders Søgaard. 2018. Sentiment anal-\nysis under temporal shift. In Proceedings of the 9th\nWorkshop on Computational Approaches to Subjec-\ntivity, Sentiment and Social Media Analysis , pages\n65–71, Brussels, Belgium. Association for Compu-\ntational Linguistics.\nGeorge A. Miller. 1995. Wordnet: A lexical database\nfor english. Commun. ACM, 38(11):39–41.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2021.\nAdapterFusion: Non-destructive task composition\nfor transfer learning. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume ,\npages 487–503, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. CoRR.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-\nvatula, and Yejin Choi. 2020. Winogrande: An ad-\nversarial winograd schema challenge at scale. Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, 34(05):8732–8740.\nTimo Schick and Hinrich Schütze. 2020. Rare words:\nA major problem for contextualized embeddings and\nhow to ﬁx it by attentive mimicking. Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence ,\n34(05):8766–8774.\nTrieu H. Trinh and Quoc V . Le. 2018. A simple method\nfor commonsense reasoning. CoRR.\nHongming Zhang, Xinran Zhao, and Yangqiu Song.\n2020. WinoWhy: A deep diagnosis of essential\ncommonsense knowledge for answering Winograd\nschema challenge. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5736–5745, Online. Association\nfor Computational Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages 15–20,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\n100\nA Model Sizes\nWhile OpenAI does not ofﬁcially disclose the size\nof their four models Davinci, Curie, Babbage and\nAda, we use the numbers approximated in a blog-\npost as estimates.5 Table 5 contains the number of\nparameters for the models used in our experiments.\nModel # Parameters\nGPT-3-Ada 350M\nGPT-3-Babbage 1.3B\nGPT-3-Curie 6.7B\nGPT-3-Davinci 175B\nPALM-8B 8B\nPALM-62B 62B\nPALM-540B 540B\nTable 5: Number of parameters of the reported models.\nB Prompts\nWe built prompts for deﬁnitions and synonyms to\nmake them sound natural given the structure of\nmost WordNet deﬁnitions for each part-of-speech\ntag. Table 6 shows the different prompt templates\nin each case.\nType Prompt\nSynonym The meaning of {lemma} is\nsimilar to {synonym}.\nVerb deﬁnition The verb to {lemma} means\nto {deﬁnition}.\nNoun deﬁnition The word {lemma} refers to\n{deﬁnition}.\nAdj. Deﬁnition The meaning of {lemma} is\ndeﬁnition.\nAdv. Deﬁnition The word {lemma} means\n{deﬁnition}.\nTable 6: Templates used to integrate the deﬁnition into\nthe prompt for each part-of-speech tag.\nC Foreign Inspired Words\nIn Table 7 we list the word, approximate deﬁnition,\nand WINO DICT -like example. Note that these ex-\namples are handwritten and did not go through a\ndebiasing process like WINO GRANDE . In order to\nreduce the risk of data leakage, in the actual exam-\nples we replace the surface form of the word with\none of the synthetic surface forms using the same\nprocess as in section 2.\n5https://blog.eleuther.ai/gpt3-model-sizes\n101\nExample Deﬁnition\nJohn frequently goes backpacking and Jake never does because [ Jake |\nJohn] disdains the feeling of waldeinsamkeit.\nthe feeling of solitude and connectedness to na-\nture when being alone in the woods\nAfter returning from backpacking, John thought he would go again [\nfrequently | infrequently]. John likely appreciates the feeling ofwaldein-\nsamkeit.\nthe feeling of solitude and connectedness to na-\nture when being alone in the woods\nMary loves going to antique stores and Ashley never does because [\nMary | Ashley] wabi-sabis old things.\nﬁnding beauty in imperfections\nMary loves going to [ antique | modern] stores because she wabi-sabis\nold things.\nﬁnding beauty in imperfections\nPierre is from France and John is from Ireland. Pierre and John like to\ngo to Irish bars and talk about [ Pierre | John]’s feeling ofdepaysement\nthere.\nthe feeling that comes from not being in one’s\nhome country; being a foreigner\nPierre has lived in France all his life. When he’s in [ Ireland | France],\nPierre frequently talks about his feeling of depaysement.\nthe feeling that comes from not being in one’s\nhome country; being a foreigner\nJake and Ashley plan to get married, Ashley’s parents are happy, but\nJake’s parents don’t like it because a friend said they had badyuanfen. [\nJake | Ashley]’s parents are more likely to go to a fortune teller.\nthe fate between two people\nJake and Ashley plan to get married. Ashley’s parents are very practical\nwhile Jake’s parents believe in destiny. When an advisor said Jake and\nAshley had bad yuanfen, [ Jake | Ashley] wanted to call it off.\nthe fate between two people\nTheresa doesn’t get why Martha thinks the statue in the museum was so\nduende that [ Martha | Theresa] spent a lot of time looking at it.\na work of art’s mysterious power to deeply move\na person\nMartha spends a lot of times in museums while Theresa spends little. [\nMartha | Theresa] ﬁnds art duende.\na work of art’s mysterious power to deeply move\na person\nAfter losing his [ religion | job], John fell into a sense of toska. a sensation of great spiritual anguish, often with-\nout a speciﬁc cause; a longing with nothing to\nlong for\nJohn kept yelling at Joey for not doing chores, but Joey wouldn’t even\nrespond. [ Joey | John] really seems tosked.\na sensation of great spiritual anguish, often with-\nout a speciﬁc cause; a longing with nothing to\nlong for\nBecause he [ loves | hates] reptiles, John found seeing that group of\nlizards very gigil.\na situation of such extreme cuteness it’s over-\nwhelming or the irresistable urge to hug some-\nthing cute\nJohn only keeps salamanders as pets and Joey likes more traditional ones,\nso [ John | Joey] found seeing the group of lizards very gigil.\na situation of such extreme cuteness it’s over-\nwhelming or the irresistable urge to hug some-\nthing cute\nJohn thought his marriage with Joey was shougani, so he wanted to hire\na [ lawyer | therapist].\na situation that can’t be helped, or an act of res-\nignation\nJohn thought his marriage with Joey was shougani but Joey disagreed,\nso [ John | Joey] decided to hire a lawyer.\na situation that can’t be helped, or an act of res-\nignation\nJoey still can’t get over when John drunkenly called him Mark at his\nwedding, and now whenever they see each other, [ Joey| John] tartles.\na moment of hesitation when introducing some-\none because you can’t remember their name\nI really [ love | hate] my new dress. I can’t wait toestrene it. wearing something for the very ﬁrst time\nMary and Sue went dress shopping together. Mary hates her dress while\nSue loves hers. [ Sue | Mary] can’t wait toestrene it.\nwearing something for the very ﬁrst time\nAfter a long day of work, James xinkued the job John did. John was [\ngrateful | upset].\nacknowledging someone’s effort for working\nhard or doing you a favor\nTable 7: List of foreign-inspired new words ( bolded) and their corresponding examples and deﬁnitions. The\npossible choices for the example are shown, with the correct choice underlined. The deﬁnition is shown on the\nright. These deﬁnitions may or may not be idiosyncratic to a native speaker; however, the actual examples use a\nsynthetic word to more closely resemble new word acquisition and minimize the risk of data leakage.\n102",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.786471962928772
    },
    {
      "name": "Computer science",
      "score": 0.7297122478485107
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6954653263092041
    },
    {
      "name": "Word (group theory)",
      "score": 0.637187123298645
    },
    {
      "name": "Task (project management)",
      "score": 0.6208791136741638
    },
    {
      "name": "Natural language processing",
      "score": 0.5778725147247314
    },
    {
      "name": "Inference",
      "score": 0.5725265145301819
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5439761281013489
    },
    {
      "name": "Key (lock)",
      "score": 0.4931578040122986
    },
    {
      "name": "Language model",
      "score": 0.47886011004447937
    },
    {
      "name": "Linguistics",
      "score": 0.26052653789520264
    },
    {
      "name": "Computer security",
      "score": 0.07862156629562378
    },
    {
      "name": "Engineering",
      "score": 0.06172552704811096
    },
    {
      "name": "History",
      "score": 0.061172306537628174
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    }
  ]
}