{
    "title": "Explainable Survival Analysis with Convolution-Involved Vision Transformer",
    "url": "https://openalex.org/W4283816523",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2186693307",
            "name": "Yifan Shen",
            "affiliations": [
                "Beijing University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A1947673853",
            "name": "Li Liu",
            "affiliations": [
                "Chinese Academy of Medical Sciences & Peking Union Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A2129485642",
            "name": "Zhi-hao Tang",
            "affiliations": [
                "Beijing University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A2142765811",
            "name": "Zongyi Chen",
            "affiliations": [
                "Beijing University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A2441956473",
            "name": "Guixiang Ma",
            "affiliations": [
                "University of Illinois Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A3104946811",
            "name": "Jiyan Dong",
            "affiliations": [
                "Chinese Academy of Medical Sciences & Peking Union Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A2095972007",
            "name": "Xi Zhang",
            "affiliations": [
                "Beijing University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A2099042467",
            "name": "Lin Yang",
            "affiliations": [
                "Chinese Academy of Medical Sciences & Peking Union Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A2109392211",
            "name": "Qingfeng Zheng",
            "affiliations": [
                "Chinese Academy of Medical Sciences & Peking Union Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A2186693307",
            "name": "Yifan Shen",
            "affiliations": [
                "Beijing University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A1947673853",
            "name": "Li Liu",
            "affiliations": [
                "National Clinical Research",
                "Chinese Academy of Medical Sciences & Peking Union Medical College",
                "Peking Union Medical College Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2129485642",
            "name": "Zhi-hao Tang",
            "affiliations": [
                "Beijing University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A2142765811",
            "name": "Zongyi Chen",
            "affiliations": [
                "Beijing University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A3104946811",
            "name": "Jiyan Dong",
            "affiliations": [
                "National Clinical Research",
                "Chinese Academy of Medical Sciences & Peking Union Medical College",
                "Peking Union Medical College Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2095972007",
            "name": "Xi Zhang",
            "affiliations": [
                "Beijing University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A2099042467",
            "name": "Lin Yang",
            "affiliations": [
                "National Clinical Research",
                "Chinese Academy of Medical Sciences & Peking Union Medical College",
                "Peking Union Medical College Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2109392211",
            "name": "Qingfeng Zheng",
            "affiliations": [
                "Chinese Academy of Medical Sciences & Peking Union Medical College",
                "Peking Union Medical College Hospital",
                "National Clinical Research"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2018116321",
        "https://openalex.org/W2200290088",
        "https://openalex.org/W2767330859",
        "https://openalex.org/W2953567336",
        "https://openalex.org/W2968231730",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W3110045238",
        "https://openalex.org/W2890655214",
        "https://openalex.org/W3018295606",
        "https://openalex.org/W2915853139",
        "https://openalex.org/W1213336605",
        "https://openalex.org/W6682081665",
        "https://openalex.org/W2064512348",
        "https://openalex.org/W2163149604",
        "https://openalex.org/W2272823987",
        "https://openalex.org/W3043535018",
        "https://openalex.org/W2752879928",
        "https://openalex.org/W6784694379",
        "https://openalex.org/W2571620227",
        "https://openalex.org/W2745940724",
        "https://openalex.org/W2807841289",
        "https://openalex.org/W3179262290",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3095602948",
        "https://openalex.org/W3135547872",
        "https://openalex.org/W2150461375",
        "https://openalex.org/W3173365702",
        "https://openalex.org/W2974825848",
        "https://openalex.org/W2415877456",
        "https://openalex.org/W2950852589",
        "https://openalex.org/W2149199519",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2171515720",
        "https://openalex.org/W3099661382",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W4287070718"
    ],
    "abstract": "Image-based survival prediction models can facilitate doctors in diagnosing and treating cancer patients. With the advance of digital pathology technologies, the big whole slide images (WSIs) provide increasing resolution and more details for diagnosis. However, the gigabyte-size WSIs would make most models computationally infeasible. To this end, instead of using the complete WSIs, most of existing models only use a pre-selected subset of key patches or patch clusters as input, which might fail to completely capture the patient's tumor morphology. In this work, we aim to develop a novel survival analysis model to fully utilize the complete WSI information. We show that the use of a Vision Transformer (ViT) backbone, together with convolution operations involved in it, is an effective framework to improve the prediction performance. Additionally, we present a post-hoc explainable method to identify the most salient patches and distinct morphology features, making the model more faithful and the results easier to comprehend by human users. Evaluations on two large cancer datasets show that our proposed model is more effective and has better interpretability for survival prediction.",
    "full_text": "Explainable Survival Analysis with Convolution-Involved Vision Transformer\nYifan Shen1*, Li liu2*, Zhihao Tang1, Zongyi Chen1, Guixiang Ma3, Jiyan Dong2,\nXi Zhang1†, Lin Yang2†, Qingfeng Zheng2\n1Key Laboratory of Trustworthy Distributed Computing and Service (MoE),\nBeijing University of Posts and Telecommunications, China\n2National Cancer Center/National Clinical Research Center for Cancer/Cancer Hospital,\nChinese Academy of Medical Sciences and Peking Union Medical College\n3University of Illinois at Chicago\n{shenyifan, innerone, zongyichen, zhangx}@bupt.edu.cn, guixiang.ma@intel.com,\nyanglin@cicams.ac.cn, {djy0823lucky, 15732027828, qfzhengpku}@163.com\nAbstract\nImage-based survival prediction models can facilitate\ndoctors in diagnosing and treating cancer patients. With\nthe advance of digital pathology technologies, the big\nwhole slide images (WSIs) provide increased resolution\nand more details for diagnosis. However, the gigabyte-\nsize or even terabyte-size WSIs would make most mod-\nels computationally infeasible. To this end, instead of\nusing the complete WSIs, most of the existing models\nonly use a pre-selected subset of key patches or patch\nclusters as input, which might discard some important\nmorphology information. In this work, we propose a\nnovel survival analysis model to fully utilize the com-\nplete WSI information. We show that the use of a Vi-\nsion Transformer (ViT) backbone, together with con-\nvolution operations involved in it, is an effective ap-\nproach to improve the prediction performance. Addi-\ntionally, we present a post-hoc explainable method to\nidentify the most salient patches and distinct morphol-\nogy features, making the model more faithful and the re-\nsults easier to comprehend by human users. Evaluations\non two large cancer datasets show that our proposed\nmodel is more effective and has better interpretability\nfor survival prediction. We would make the code pub-\nlicly available upon acceptance.\nIntroduction\nAutomatic histopathological image analysis can be used\nto improve the diagnosis process by reducing the work-\nloads of pathologists and the chance of diagnosis mis-\ntakes. Histopathological image-based survival analysis aims\nto predict the expected duration of time until patients’ death.\nBased on the histological details in high resolution, it would\nbe of great beneﬁt to make early decisions and provide better\ntreatments for cancer patients.\nRecently, many methods have been proposed for cancer\ndiagnosis and survival prediction using pathological slides.\n*These authors contributed equally.\n†Corresponding author\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nTraditional survival analysis models (Tibshirani 1997; Bair,\nTibshirani, and Golub 2004; Shedden et al. 2008) use sta-\ntistical data and omics data from patients to predict the\npatient’s health, which requires the heavy burden of fea-\nture engineering. With the advance of deep learning models,\nconvolutional neural networks (CNNs) and their extensions\nhave been widely used to capture the rich information of\nthe whole slide images (WSIs) to improve prediction accu-\nracy. However, one unique challenge for WSI-based analy-\nsis is that a pathological image is of high resolution (usually\nfrom one billion to one trillion pixels), making most deep\nlearning models not applicable. To address this issue, these\nmodels (Zhu, Yao, and Huang 2017; Yao et al. 2017) usu-\nally pre-select a subset of key patches from the region of\ninterest (ROI) instead of using all patches as the input of\nthe model. But the manual selection of regions is laborious\nand commonly dependent on the expertise of pathologists to\nthoroughly examine the whole slide. To reduce such over-\nhead, some methods (Zhu et al. 2017; Tang et al. 2019; Li\net al. 2018; Yao et al. 2020) have adopted sampling strategy\nto generate candidate patches not limited to ROI. However,\nonly a small set of patches may not completely capture the\ntumor morphology and suffers from a high risk of discard-\ning informative patterns due to the complexity and hetero-\ngeneity of tumors. It is of prominent importance to design\na framework that can consider all the patches from WSIs to\nimprove the prediction accuracy and minimize the risk of\nlosing important information.\nAnother challenge for image-based survival prediction is\nthe requirement of the model interpretability, which is cru-\ncial to make the model faithful and the results get accepted\nin the diagnostics. Explainable survival analysis is very chal-\nlenging due to several reasons: 1) Different from conven-\ntional image classiﬁcation tasks such as tumor classiﬁca-\ntion, survival analysis is formulated as a regression prob-\nlem that lacks tissue-level or patch-level labels as ground-\ntruth information for visual explanations. 2) Most of the\nexisting methods (Li et al. 2018; Yao et al. 2020) provide\npixel-wise explanations and salient regions, which ignores\nthe domain-speciﬁc biological features and cannot be easily\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2207\nunderstood by pathologists. Although a recent study (Jaume\net al. 2021b) has tried to address this issue by proposing a set\nof quantitative metrics based on domain-speciﬁc concepts,\nit is limited to its graph explainability framework based\non a biological entity graph, which is not suitable for the\nwidely adopted patch-based survival analysis models. Ta-\nble 1 shows the comparison of various models in the ﬁeld\nof survival analysis in terms of whether to manually label,\nselect and gather patches, and interpretability.\nIn this paper, we aim to propose a survival predic-\ntion model named Explainable Survival Analysis using\nConvolution-involved Vision Transformer (ESAT) that can\nfully utilize the WSI information without pre-selecting ROI\nor patches. Enlightened by the remarkable success of Vision\nTransformer (Dosovitskiy et al. 2020) in a variety of com-\nputer vision tasks, we propose a ViT-based model, which\nnaturally coincides with a typical patch-based deep learning\nprocessing paradigm. Our model ﬁrst splits WSIs into dis-\ncrete non-overlapping patches which are treated as tokens\nand then feeds the patches into transformer layers with posi-\ntion encoding to model their global relations for regression.\nTo accommodate the large WSIs, we introduce convolution\nlayers and approximately linear attention layers instead of\nstandard self-attention mechanisms to reduce the compu-\ntation complexity. Additionally, we propose a post-hoc ex-\nplainable method to identify both the salient patches and the\nquantitative morphology indicators as explanations, which\nare more comprehensive and easier to understand by the\npathologist. We evaluate our method on two cancer datasets\nand the results show that our model can outperform the state-\nof-the-art baselines and has better explainability.\nRelated Work\nIn this section, we brieﬂy review relevant works on survival\nprediction, explainability in digital pathology, and Vision\nTransformer.\nSurvival Prediction with Digital Pathology Various\nstudies have shown that tumor morphology and growth dis-\nplayed in the pathological images is useful in cancer di-\nagnosis (Warth et al. 2012; Yuan et al. 2012). Due to the\nlarge image size of WSIs, most approaches only selected a\nset of patches from regions of interest (ROI) to represent\nthe whole WSIs. Among them, early studies commonly ex-\ntracted hand-crafted features from ROI (Yuan et al. 2012;\nWang et al. 2014; Yao et al. 2015; Barker et al. 2016; Cheng\net al. 2018). Speciﬁcally, they used nuclei detection and seg-\nmentation to extract features that rely on human annota-\ntions. More recent studies applied deep learning-based mod-\nels to improve the representation of patches and reduce hu-\nman efforts. For example, CNNs were ﬁrst used by some\nwork (Zhu, Yao, and Huang 2017; Yao et al. 2017) to extract\nthe feature from pre-selected ROI. WSISA (Zhu et al. 2017)\nadopted the way to select important patches and sampled\nmore patches randomly. Then it aggregated them to differ-\nent clusters and selected important clusters to get a patient-\nlevel result. This framework utilized more information from\nWSIs and reduced the chance of diagnosis mistakes. Cap-\nSurv (Tang et al. 2019) followed WSISA by introducing the\ncapsule network. Graph convolutional networks (GCNs) are\nused in the work (Li et al. 2018) to consider the similarity\nrelationship of patches features and learn a WSI-level repre-\nsentation. Another work (Yao et al. 2020) used the siamese\nmultiple instance learning network to learn features from\nclusters and attention-based pooling layer to aggregate the\nclusters. Different from the existing models which use se-\nlective patches, the model we proposed considers all the\npatches on WSIs and takes advantage of the long-distance\ndependence between patches to predict the overall survival\nrisk of the patient.\nExplainability in Digital Pathology Although the deep\nlearning-based diagnostic models have achieved remarkable\nperformance, their lack of interpretability limits their ap-\nplication in practical scenarios. To address this issue, a set\nof explainability methods have been proposed, including\nfeature-attribution and attention-based technique (H ¨agele\net al. 2020; Lu et al. 2021). However, most of the existing\nmethods are designed for cancer classiﬁcation models, aim-\ning to indicate the difference between cancer cells (or re-\ngions) and normal cells (or regions), and can be validated by\nground-truth annotations or expert knowledge. On the con-\ntrary, for the survival analysis problem, almost all instances\ncontain cancer cells and regions, thus we need to show\ndistinct patterns between cancer cells or regions, which is\nmore difﬁcult. Moreover, they commonly provide explain-\nability results in the form of heatmaps (Zhu, Yao, and Huang\n2017), graph nodes (You et al. 2020), and path clusters (Yao\net al. 2020). These forms cannot be intuitively understood\nby human experts such as pathologists due to the ignorance\nof biological entities like the nucleus. Although a recent\nstudy (Jaume et al. 2021b) has proposed to use quantitative\nmetrics involving domain-speciﬁc concepts, it is limited to\nits graph-based model with sparsely distributed cells. To this\nend, we make slight changes to their approach to make it ap-\nplicable to more types of cancers such as small cell lung\ncancer and the more commonly used patch-based models.\nVision Transformer Vision Transformer (ViT) (Dosovit-\nskiy et al. 2020) is the ﬁrst to prove that a pure Transformer\narchitecture can attain state-of-the-art performance in Com-\nputer Vision. Speciﬁcally, ViT decomposes each image into\na series of ﬂattened patches and then applies multiple stan-\ndard Transformer layers to model these tokens. This idea\ncoincides with the processing of handing WSIs and satisﬁes\nthe needs of survival analysis to consider the impact of dif-\nferent patches on the overall WSI. Despite the emergence\nof attention-based models in pathological diagnosis, there is\nstill no survival analysis model that uses a transformer to\ncapture long-distance connections between all patches. The\napplication of ViT needs to deal with the problem caused by\nWSIs’ oversized pixels that leads to excessive computational\noverhead. One idea is to use a small number of large patches\nto reduce the cost of calculating the relationship between\npatches. We tried this method, and the reasons for its poor\nperformance will be discussed in the “Results and Discus-\nsion” section. Inspired by the works which combine CNNs\nand Transformers to model both local and global dependen-\ncies (Wu et al. 2021; Yuan et al. 2021), we incorporate the\n2208\nModels Need annotations Appr\noach of patch selection Appr\noach of patch aggregation Explanation\nNHIA (W\narth et al. 2012) Yes All No aggre\ngation Statistical features\nBOEH\n(Cheng et al. 2018) Yes ROI Cluster Statistical features\nDeepCon\nvSurv (Zhu, Yao, and Huang 2017) No ROI Fully connected\nlayer Statistical features\nRankDeepSurv\n(Jing et al. 2019) No ROI Fully connected\nlayer Statistical features\nWSISA (Zhu\net al. 2017) No Random sample Cluster and\nFully connected layer No\nCapSurv (T\nang et al. 2019) No Random sample Cluster and\ncapsule network No\nDeepGraphSurv (Li\net al. 2018) No Random sample Graph con\nvolutional neural network Heat map\nDeepAttnMISL\n(Yao et al. 2020) No Random sample Siamese MI-FCN\nwith attention Heat map\nESAT (ours) No All Vision\ntransformer Important patches,\nquantitati\nve indicators\nTable 1: Prior survival analysis methods v.s. the proposed method\nconvolutional layers in ViT to aggregate the adjacent small\npatches to reduce the calculations. Besides, we use an ap-\nproximately linear attention layer (Xiong et al. 2021) to re-\nplace the original attention layer to reduce the time complex-\nity of the calculations. In contrast to the concurrent works,\nthis work attempts to use convolutional layers to reduce the\nnumber of patches in the input phase of ViT.\nMethodology\nIn this section, we start by introducing the problem deﬁni-\ntion of survival analysis and the overview of the framework.\nThen we describe the main components of the framework\nin detail, including the convolution-involved Vision Trans-\nformer, the survival risk loss, and the explainable survival\nanalysis.\nPreliminaries\nConsidering a set of N patients, Pi, i= 1;:::;N , each pa-\ntient has a binary label (ti;\u000ei) and a set of {Wj}k\nj=1 ∈Pi,\nwhere the observation time ti is either a survival time or a\ncensored time, and the censorship status \u000ei is the indicator\nwhich is 1 for an uncensored instance (death occurs during\nthe study) and 0 for a censored instance. W is the WSIs\nset of the patient. Survival models aim to predict the hazard\nrisk to present how well the patient behaves. Fig. 1 shows\nthe overview of the proposed ESAT. The WSIs of patients\nare fed into the convolution-involved Vision Transformer\nmodule to extract features, which are then used to calcu-\nlate the survival risk loss. Next, WSIs features are trained\nwith the patient-level ground-truth labels. Additionally, a\npost-hoc explainable module is proposed to provide impor-\ntant patches and quantitative indices to make the prediction\nresults trustworthy. Different from existing models, ESAT\ndiscards the step of randomly or manually selecting patches\nfrom WSIs but considers all the patches. It also adopts a\nmodel-agnostic interpretable module that only uses the in-\nput and output of the model to provide useful explanations.\nConvolution-Involved Vision Transformer\nThe role of this module is to extract effective features from\nWSIs. Its architecture is shown in the bottom right of Fig 1.\nDue to different sizes and shapes of WSIs, we ﬁrst resize\nthem into a uniform square size and then split each WSI\ninto n×npatches. Existing models (Zhu et al. 2017; Yao\net al. 2020) have found that using pre-trained CNNs can\nfacilitate the feature extraction process. Therefore, we use\nResnet34 (He et al. 2016) instead of the original random\nparameters to get initial d-dimensional patch embeddings.\nThen follow the standard procedure in Vision Transform-\ners, we organize the total n2 tokens with dimension size d\ninto X ∈Rn2\u0002d, which is then projected with three ma-\ntrices WQ ∈Rd\u0002dq , WK ∈Rd\u0002dk and WV ∈Rd\u0002dv\nto extract feature representations Q, K, and V, represent-\ning query, key and valueca in the attention mechanism. We\nadopt multi-head attention to model the tokens, that is,\nQ= XWQ; K = XWK; V = XWV (1)\nAttention(Q;K;V ) =softmax(QKT\npdk\n)V (2)\nheadi = Attention(QWQ\ni ;KW K\ni ;VW V\ni ) (3)\nMultiHead(Q;K;V ) =Concat(head1;:::; headh)WO(4)\nThe time complexity of the multi-head attention is\nO(n2d2 + n4d). Since the feature dimension dis indepen-\ndent of the input data, it is necessary to reduce the number\nof patches n2 to reduce the complexity due to the exces-\nsive size of WSIs. An intuitive way is to divide WSIs into\npatches with larger sizes. But experimental results show that\nthe performance drops signiﬁcantly (see Table. 3). We will\ndiscuss it in “Results and Discussion” section. Therefore we\npropose to retain the small patch size and use convolutional\nlayers to aggregate adjacent small patches into a bigger one\nto reduce the number of patches. Speciﬁcally, one ﬁlter in\nthe convolutional layer is used to aggregate one dimension\nof the d-dimensional features of the adjacent patches. We\nmerge such d ﬁlters to obtain the next-layer feature repre-\nsentations as shown in the bottom right of Fig. 1. Next, fol-\nlowing the steps of ViT, we ﬂatten the patch embedding and\nadd position embedding. Meanwhile, to further reduce the\ncomplexity, we adopt the Nystrom-based linear transform-\ners (Xiong et al. 2021) to replace the standard self-attention\ntransformers. The Nystrom method is adopted to approxi-\nmate the softmax matrix in self-attention by sampling a sub-\nset of columns and rows. Consequently, the time complexity\nin this module can be reduced toO(nd2 +n2d) as nis small.\nSurvival Risk Loss\nIn this step, we use WSIs embeddings produced by the\nconvolution-involved Vision Transformer to generate a\n2209\nExplainable\n Survival Analysis\n...\nWSI\nResNet Pre-trained Model\nEmbedded Patches\nConvolutional Layer\nConvolutional Layer\nPosition Encoder\nSequential Patches \nNystromformer \nMulti-Head Attention \nWSI Embedding\nConvolution-involved\n Vision Transformer Encoder\nPatient WSI \nWSI representation\nPatient-level survival result \nImportant patches \nQuantitative statistics\nPatient WSI \nWSI representation\nPatient-level survival result \nImportant patches \nQuantitative statistics\n...\nConvolution-involved\n Vision Transformer Encoder\nSurvival Risk Loss\n...\n...\n...\nConvolution-involved\n Vision Transformer Encoder\nSurvival Risk Loss\n...\n...\nSurvival Prediction\n...\nWSI\nResNet Pre-trained Model\nEmbedded Patches\nConvolutional Layer\nConvolutional Layer\nPosition Encoder\nSequential Patches \nNystromformer \nMulti-Head Attention \nWSI Embedding\nConvolution-involved\n Vision Transformer Encoder\nPatient WSI \nWSI representation\nPatient-level survival result \nImportant patches \nQuantitative statistics\n...\nConvolution-involved\n Vision Transformer Encoder\nSurvival Risk Loss\n...\n...\nSurvival Prediction\n...\nEmbedded Patches\nEmbedded Patches\nFigure 1: Left: An overall framework consisting of a survival prediction module and an explainable survival module. Top right: Legend of\nthe framework. Bottom right: Details of the convolution-involved Vision Transformer and its corresponding three-dimensional display. The\nblue component on the left corresponds to the blue dashed line on the right, and the remaining data layers correspond to colors.\npatient-wise hazard risk which measures the probability of\nthe expected development of cancer. For thei-th patient, the\noutput of this step is denoted asOi, and its patient-level label\nis (ti;\u000ei). As censoring data (\u000ei = 0) means patients were\nalive during the study and their survival time does not rep-\nresent the true survival time, we assume that the censoring\ndata is non-informative and matches the WSIs embeddings\nof the uncensored data from the same patient with the con-\nsistent patient-level label. Let t1 < t2 < ···< tN denote\na sequence of ordered event time and R(ti) denotes the risk\nset of patients who live longer than the i-th patient. In other\nwords, it means the survival time of the patients in the risk\nset is equal or larger than the i-th patient (tj ≥ ti). Fol-\nlowing the work (Katzman et al. 2016; Zhu et al. 2017), we\ndeﬁne the conditional probability upon the existence of the\ndeath event that occurs at some particular time tfor the i-th\npatient is:\nLi = exp(Oi)\nP\nj2R(ti) exp(Oj) (5)\nAssuming the patient’s events are independent, we can get\nthe joint probability of all conditioned probability as the par-\ntial likelihood:\nL =\nY\ni=1\nexp(Oi)P\nj2R(ti) exp(Oj) (6)\nTo maximize the partial likelihood, we take the logarithm\non both sides of the equation. It is equivalent to minimizing\nthe negative log partial likelihood as follows:\nL(Oi) = −\nnX\ni=1\n\u000ei\n0\n@Oi −log\nX\nj2R(ti)\nexp(Oj)\n1\nA (7)\nWe use the negative log partial likelihood as loss function\nwhich is the same as (Zhu et al. 2017; Yao et al. 2020).\nFor a set of model predictions, it can contribute to the con-\nsistency of the risk set and penalize predictions that are not\nin the correct order. For a patient with multiple WSIs, the\nsame number of survival risk predictions can be obtained\nwith our method. As the WSI with a worse predictive result\nis more likely to reﬂect the actual state for cancer patients,\nwe choose the prediction with the highest survival risk as\nour ﬁnal result.\nExplainable Survival Analysis\nTo provide interpretable explanations for predictions, we\npropose a post-hoc explainable method for survival anal-\nysis. Different from most existing explaining methods for\nsurvival analysis, our method is model-agnostic and can\nbe applied to a variety of patch-based survival models.\nSpeciﬁcally, it takes the trained model and its predictions\nas input and returns the explanations in the form of patch-\nlevel salient areas together with human-intelligible cell-level\nquantitative indices.\n2210\nFormally, we usef(xs; \u0012) to represent the trained survival\nmodel. As the parameter \u0012 is ﬁxed, it can thus be omitted.\nGiven the WSI as input, we set M ∈[0;1]n\u0002n as corre-\nsponding masks with the same size as the input. We aim to\nlearn M in a self-supervised learning (SSL) manner to min-\nimize the loss of prediction results with the original input x\nand with the selected input M ⊗x, that is,\nmin\nM\n`(f(x);f(M ⊗x)) +\r∥a(M)∥: (8)\nwhere \r∥a(M)∥is used to sparse the weight of M. With\nthe learned mask M, we can extract the salient areas of x\nto maximally preserve information in x. A larger value in\nM indicates that the corresponding area provides more con-\ntribution to the prediction. In this study, we can thus obtain\nthe most important patches and least important patches by\nranking the element values in M.\nPractically, merely providing important patches is not\neasy to understand by pathologists. Different from classi-\nﬁcation problems with very distinct positive or negative in-\nstances for comparisons, our problem can be viewed as a\nregression problem and the important patches for patients\nwith different survival risks may not be easily differentiated\nfor visualizations. To address this issue, we adopt domain-\nspeciﬁc quantitative metrics that can facilitate the pathol-\nogist in spotting the differences on the selected important\npatches. We will illustrate the metrics and results in detail in\nthe “Experiments” and “Results and Discussion” section.\nExperiments\nDatasets\nWe use two datasets to evaluate the performance of our\nmodel. One is a public National Lung Screening Trial (Team\n2011) (NLST) dataset collected by the National Cancer In-\nstitute’s Division of Cancer Prevention (DCP) and Divi-\nsion of Cancer Treatment and Diagnosis (DCTD), which\ncan be downloaded from Internet via application. The other\nis collected by the Chinese Academy of Medical Science\n(CHCAMS), and has been approved by the Ethics Commit-\ntee and Institutional Review Boards of Cancer Hospital. The\nnumber of WSIs and patients, as well as the types of cancer\nin each dataset are shown in Table 2.\nBaseline Models\nWe reproduce several popular survival models as follows:\nCox Model The Cox proportional hazard model is one of\nthe most commonly used semi-parametric model in survival\nanalysis. We used l1-norm (LASSO-Cox) (Tibshirani 1997)\nmodel as baseline.\nDatasets Patients WSIs Types of cancer\nNLST 449 1041 adenocarcinoma,\nsquamous cell carcinoma\nCHCAMS 343 686 small cell lung cancer\nTable 2: The number of WSIs, patients, and types of cancer\nLogistic Regression Model It formulated the joint proba-\nbility of the uncensored and censored instances as a product\nof death density function and survival function by logistic\ndistribution (Lee and Wang 2003).\nRankDeepSurv It learned the patient-level survival pre-\ndiction based on ranking information on survival data func-\ntion (Jing et al. 2019).\nWSISA WSISA (Zhu et al. 2017) sampled a set of patches\nfrom WSIs and clustered them into different categories to\npredict the risk.\nDeepAttnMISL The DeepAttnMISL (Yao et al. 2020)\nfollowed the way of clustering the patterns and used an\nattention-based pooling layer to consider all the patterns.\nViT Vision Transformer (ViT) (Dosovitskiy et al. 2020)\ndecomposes WSIs into a series of ﬂattened big patches and\nthen applies standard Transformer layers to model these to-\nkens to get the WSIs’ embeddings.\nImplementation Details\nAs cox model and logistic regression model depend on\nhand-crafted features, we use Histocartography (Jaume et al.\n2021a) which is a state-of-the-art medical image feature ex-\ntracting tool to obtain the hand-crafted features. The ex-\ntracted features include size, shape, pixel intensity distribu-\ntion, texture of the objects, as well as the relation between\nneighboring objects. The source codes of WSISA and Deep-\nAttnMISL are obtained from the websites of the authors. All\nother methods are built using the functions from the life-\nlines package, which is a survival analysis library available\non Github 1. For data preprocessing in ViT, we split WSIs\ninto ﬂatten patches with a size of 512 ×512 to meet the con-\nstraints of computing resources. All the experiments run on\nNVIDIA V100 GPU.\nWe split the NLST dataset and CHCAMS dataset into\ntraining, validation, and testing set with a split ratio of 8:1:1.\nFor training, the parameters are optimized using the Adam\nalgorithm, where the learning rate is initialized at 0.01. We\nset the dimension of the hidden feature vector as 256. The\nbatch size is set to 64. The training process is iterated upon\n1000 epochs. The patch size is set to 16 ×16.\nEvaluation Metric\nSurvival Prediction To evaluate the performance in sur-\nvival prediction, following previous studies (Zhu et al. 2017;\nYao et al. 2020; Li et al. 2018), we take the concordance in-\ndex (C-index) as the evaluation metric. It refers to the pro-\nportion of pairs whose predicted results are consistent with\nactual results among all patient pairs. The formal deﬁnition\nof C-index is\nc = 1\nn\nX\ni2f1:::Nj\u000ei=1g\nX\ntj>ti\nI[fi >fj]; (9)\nwhere nis the number of comparable pairs, I[:] is the indi-\ncator function, ti is the actual observation time of patient i\n1https://github.com/CamDavidsonPilon/lifelines\n2211\nand fi means the corresponding risk of patienti. \u000ei is the in-\ndicator which is 1 when death occurs during the study. The\nvalue of C-index ranges from 0 to 1. A larger value indicates\nthe better prediction performance and vice versa. 0.5 is the\nvalue as a random guess.\nExplainable Survival Analysis To evaluate the explain-\nability in our model, we adopt the similar metrics as those\nproposed in a recent study (Jaume et al. 2021b). They de-\nsigned a set of quantitative metrics based on the statistics of\nthe class separability using pathologically measurable con-\ncepts (like nuclear shape) to characterize graph-structural\nmodels. Since it is devised for graph model, to make it ap-\nplicable to our framework, we choose nucleus on important\npatches instead of nucleus on graphs. We also notice that\nthey use a set of features which are suitable for some speciﬁc\ntype of cancer whose WSIs have sparse distributed cells, but\nare not ﬁt for other types of cancers whose cells are closely\ndistributed or even overlapped in WSIs. To address this is-\nsue, we use pathologically measurable features (like nuclear\narea) as our basic unit in order to get the suitable information\nto distinguish more types of cancer.\nBased on the aforementioned motivations, we propose\nour explainability evaluation method as follows. Firstly, we\npick out important patches for every WSI with the approach\nproposed in the “Post-hoc Explanations” Section, and ex-\ntract the nucleus from the patches with pre-trained Hover-net\n(Graham et al. 2019). Then we divide the nucleus into two\ncategories according to the types of patients they belong to.\nNote that the patients are categorized into two types accord-\ning to their survival time lengths, with the median value as\nthe division point (more details are provided in the next sec-\ntion). For both categories of nucleus, we extract pathologi-\ncally measurable features using toolkits provided in (Jaume\net al. 2021a), and calculate the probability distribution for\nevery feature. Given the probability distributions, we con-\nvert them into the probability density functions for every\nfeature in both types. Then we can compute the separabil-\nity score based on the optimal transport as the Wasserstein\ndistance (Panaretos and Zemel 2018) between the feature\ndensity functions of two types. Finally, the evaluation met-\nrics for explainability can be calculated as follows:\nsmax = max\nf2F\nSf\nsavg = 1\n|F|\nX\nf2F\nSf\n(10)\nwhere F is the set of pathologically measurable features,\nand Sf denotes the separability score of feature f. smax and\nsavg represent the utmost and expected separability between\nnuclear features of different categories respectively.\nResults and Discussion\nPrediction Survival\nTable 3 shows the performance of various survival models\non two datasets in terms of C-index values. The results show\nthat our ESAT outperforms all other competitors on both\nMethods NLST CHCAMS\nLASSO-Cox (Tibshirani 1997) 0.517 0.474\nLogistic (Lee and Wang 2003) 0.514 0.500\nViT(Dosovitskiy et al. 2020) 0.563 0.536\nRankDeepSurv (Jing et al. 2019) 0.541 0.541\nWSISA (Zhu et al. 2017) 0.662 0.631\nDeepAttnMISL (Yao et al. 2020) 0.630 0.628\nESAT (ours) 0.730 0.707\nTable 3: Performance comparison of different models using\nC-index values on two datasets\ndatasets. Compared to the state-of-the-art baseline Deep-\nAttnMISL, our model improves C-index by a large margin\n(more than 10%).\nLASSO-Cox and logistic models are traditional methods\nthat use ROI and hand-crafted features which depend on\nthe experience of pathologists in selecting pathological ar-\neas and extracting features. The predicted C-index values of\nthese two models are close to the reported results in previous\nstudies (Zhu et al. 2017; Yao et al. 2020).\nThe performance of RankDeepSurv, DeepAttnMISL, and\nWSISA outperform LASSO-Cox and logistic, demonstrat-\ning the superior ability of deep learning models in learning\neffective image representations for survival analysis. How-\never, they still perform worse than our ESAT. One possi-\nble reason is that ESAT considers all the patches rather\nthan sampled areas. Another reason is that our model can\nbetter capture distinct image patterns with the ability of\nconvolution-involved vision transformer.\nAlthough ViT also uses all the patches, ESAT still outper-\nforms it by a large margin. That’s because the vanilla ViT\nhas a very large computing complexity and has to reduce the\nnumber of patches. Thus, they use a small number of large\npatches as input, and a larger patch may bring more noises\nthat are difﬁcult to ﬁlter, leading to poor performance. On\nthe contrary, in ESAT, with the help of the convolutional\nlayer, we can accommodate a large number of small patches,\nand the patches with a large portion of noises can be easily\ndiscarded by the attention mechanism of ViT, thus yielding\nsuperior performance.\nAblation Study\nTo validate the effectiveness of different components in\nESAT, we use the leave-one-out scheme to evaluate the im-\npact of two modules: ResNet pre-trained model and the sur-\nvival risk loss. Note that, the effectiveness of the convolution\nlayer in our model has been validated in the previous section\nwhen comparing ESAT with ViT. Here we develop two vari-\nants: (1) ESAT-prewhich uses random parameters instead\nof the ResNet pre-trained model; (2)ESAT-SRLwhich uses\nBCEloss instead of the survival risk loss.\nFigure 2 compares the C-index values of these varia-\ntions in two datasets. We can observe that the pre-trained\nResNet and the survival risk loss modules are both ben-\n2212\nNLST CHCAMS\nESAT-pre ESAT-SRL ESAT\n0.715 0.73\n0.556\n0.633\n0.707\n0.605\nFigure 2: The C-index performance of ESAT and its variants\non the NLST and CHCAMS datasets.\nMethods savg smax\nRANDOM 0.0879 0.1146\nDeepAttnMISL 0.0939 0.1315\nESAT 0.1262 0.2271\nTable 4: Explainability comparison of the proposed mod-\nels and other methods using two separability metrics on\nCHCAMS dataset.\neﬁcial to our task. The performance of ESAT-SRL is still\nbetter than the state-of-the-art baselines. This demonstrates\nthat despite using BCEloss, using all the patches with our\nproposed Convolution-involved Vision Transformer can still\nyield superior performance.\nExplainable Result\nWe perform the explainability evaluation from two perspec-\ntives. From the model-level perspective, we compare ESAT\nwith other explainable models in terms of separability scores\ndeﬁned in Eq.(10), aiming to show that our model can suc-\ncessfully identify the important patches that can be used to\ndistinguish patients with different survival times. From the\nfeature-level perspective, we aim to provide a set of most\nimportant features according to their separability scores.\nModel-Level Explainablity Comparison To compare the\nexplainability of models using separability scores, we di-\nvide survival times into two categories by median: (1) 0-\n3 years, (2) 4+ years. For our model, the top 5 important\npatches with size 512 ×512 pixels are extracted with our\npost-hoc explainable module. We compare our model with\nDeepAttnMISL and a RANDOM explainer with the random\npatch selection strategy for each WSI for comparison. For\nDeepAttnMISL, it aggregates patches into different clusters\nand uses the attention mechanism to select the clusters with\nthe largest weight as explanations. Based on the important\npatches obtained by these models and the associated nu-\ncleus, we extract 24 nuclear morphology features as (Jaume\net al. 2021b), and obtain the avg and max separability scores\naccording to Eq. (10). The results on CHCAMS are shown\nin Table 4. Similar results on the NLST dataset can be found\nin the Appendix.\n0\n0.05\n0.1\n0.15\n0.2\n0.25\nroundness solidity glcm_ASM\nstd_crowdedness major_axis_length extent\nroughness area eccentricity\neuler_number\nTop 5 Bottom 5\nFigure 3: The separability scores of the top 5 and bottom\n5 features, which are presented on the left and right parts,\nrespectively.\nIt can be observed that ESAT achieves the best maximum\nand average separability scores. Both our model and Deep-\nAttnMISL outperform RANDOM which conveys that the at-\ntention mechanism in deep learning models are effective in\nselecting important patches that can be used to distinguish\ndifferent categories of patients.\nFeatures Ranked by Separability Scores After obtain\nthe important patches for different patients, we aim to in-\nvestigate their ﬁne-grained differences. Speciﬁcally, we out-\nput the most important nuclear features that can be used to\ndistinguish the two types of patients. Speciﬁcally, we rank\nthe 24 nuclear features by their separability scores and out-\nput the top 5 and bottom 5 in Fig.3. It can be observed that\nthe nuclear roundness is the most distinctive feature between\nthe two types, followed by solidity and glcmASM. In con-\ntrast, the nuclear area is one of the least distinctive features.\nCompared to the important patches, these features can pro-\nvide ﬁne-grained and quantitative characteristics that can be\nmore easily comprehended by the pathologists.\nConclusion\nIn this paper, we propose an explainable survival analysis\nframework with a convolution-involved vision transformer,\nwhich can learn effective and explainable survival patterns\nfrom the whole slide histopathological images. Compared\nto existing image-based survival models, our proposal can\nlearn from the complete image information and thus achieve\nbetter performance. Moreover, the interpretable part of our\nframework can provide both important patches and informa-\ntive quantitative indicators as explanations, which are easy\nto comprehend and can facilitate the doctors in cancer diag-\nnosis. The explanation module is post-hoc, model-agnostic\nand can be easily deployed in other image-based survival\nmodels. Evaluations on two cancer datasets demonstrate that\nour model can outperform the state-of-the-art baselines and\nprovide explanations with better separability ability. In fu-\nture, it would be interesting to introduce the transfer learn-\ning approach to make our model work on some speciﬁc type\nof cancer that lacks sufﬁcient labeled data.\n2213\nAcknowledgements\nThis work was supported by the Natural Science Founda-\ntion of China (No.61976026, No.61902394) and 111 Project\n(B18008). The authors would like to thank the National\nCancer Institute for access to NCI’s data collected by the\nNational Lung Screening Trial. The statements contained\nherein are solely those of the authors and do not represent\nor imply concurrence or endorsement by NCI.\nReferences\nBair, E.; Tibshirani, R.; and Golub, T. 2004. Semi-\nsupervised methods to predict patient survival from gene ex-\npression data. PLoS biology, 2(4): e108.\nBarker, J.; Hoogi, A.; Depeursinge, A.; and Rubin, D. L.\n2016. Automated classiﬁcation of brain tumor type in\nwhole-slide digital pathology images using local represen-\ntative tiles. Medical image analysis, 30: 60–71.\nCheng, J.; Mo, X.; Wang, X.; Parwani, A.; Feng, Q.; and\nHuang, K. 2018. Identiﬁcation of topological features in\nrenal tumor microenvironment associated with patient sur-\nvival. Bioinformatics, 34(6): 1024–1030.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nGraham, S.; Vu, Q. D.; Raza, S. E. A.; Azam, A.; Tsang,\nY . W.; Kwak, J. T.; and Rajpoot, N. 2019. Hover-net: Simul-\ntaneous segmentation and classiﬁcation of nuclei in multi-\ntissue histology images. Medical Image Analysis, 101563.\nH¨agele, M.; Seegerer, P.; Lapuschkin, S.; Bockmayr, M.;\nSamek, W.; Klauschen, F.; M ¨uller, K.-R.; and Binder, A.\n2020. Resolving challenges in deep learning-based analy-\nses of histopathological images using explanation methods.\nScientiﬁc reports, 10(1): 1–12.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nJaume, G.; Pati, P.; Anklin, V .; Foncubierta, A.; and Gabrani,\nM. 2021a. HistoCartography: A Toolkit for Graph Analytics\nin Digital Pathology. arXiv preprint arXiv:2107.10073.\nJaume, G.; Pati, P.; Bozorgtabar, B.; Foncubierta, A.; Anni-\nciello, A. M.; Feroce, F.; Rau, T.; Thiran, J.-P.; Gabrani, M.;\nand Goksel, O. 2021b. Quantifying explainers of graph neu-\nral networks in computational pathology. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 8106–8116.\nJing, B.; Zhang, T.; Wang, Z.; Jin, Y .; Liu, K.; Qiu, W.; Ke,\nL.; Sun, Y .; He, C.; Hou, D.; et al. 2019. A deep survival\nanalysis method based on ranking. Artiﬁcial intelligence in\nmedicine, 98: 1–9.\nKatzman, J. L.; Shaham, U.; Cloninger, A.; Bates, J.; Jiang,\nT.; and Kluger, Y . 2016. Deep survival: A deep cox propor-\ntional hazards network. stat, 1050(2): 1–10.\nLee, E. T.; and Wang, J. 2003. Statistical methods for sur-\nvival data analysis, volume 476. John Wiley & Sons.\nLi, R.; Yao, J.; Zhu, X.; Li, Y .; and Huang, J. 2018.\nGraph CNN for survival analysis on whole slide patholog-\nical images. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, 174–182.\nSpringer.\nLu, M. Y .; Williamson, D. F.; Chen, T. Y .; Chen, R. J.; Bar-\nbieri, M.; and Mahmood, F. 2021. Data-efﬁcient and weakly\nsupervised computational pathology on whole-slide images.\nNature Biomedical Engineering, 5(6): 555–570.\nPanaretos, V . M.; and Zemel, Y . 2018. Statistical Aspects of\nWasserstein Distances. arXiv: Methodology.\nShedden, K.; Taylor, J. M.; Enkemann, S. A.; Tsao, M. S.;\nYeatman, T. J.; Gerald, W. L.; Eschrich, S.; Jurisica, I.;\nVenkatraman, S. E.; Meyerson, M.; et al. 2008. Gene\nexpression-based survival prediction in lung adenocarci-\nnoma: a multi-site, blinded validation study: Director’s\nChallenge Consortium for the molecular classiﬁcation of\nlung adenocarcinoma. Nature medicine, 14(8): 822.\nTang, B.; Li, A.; Li, B.; and Wang, M. 2019. CapSurv: cap-\nsule network for survival analysis with whole slide patho-\nlogical images. IEEE Access, 7: 26022–26030.\nTeam, N. L. S. T. R. 2011. The national lung screening trial:\noverview and study design. Radiology, 258(1): 243–253.\nTibshirani, R. 1997. The lasso method for variable selection\nin the Cox model. Statistics in medicine, 16(4): 385–395.\nWang, H.; Xing, F.; Su, H.; Stromberg, A.; and Yang, L.\n2014. Novel image markers for non-small cell lung cancer\nclassiﬁcation and survival prediction. BMC bioinformatics,\n15(1): 1–12.\nWarth, A.; Muley, T.; Meister, M.; Stenzinger, A.; Thomas,\nM.; Schirmacher, P.; Schnabel, P. A.; Budczies, J.; Hoff-\nmann, H.; and Weichert, W. 2012. The novel histo-\nlogic International Association for the Study of Lung Can-\ncer/American Thoracic Society/European Respiratory Soci-\nety classiﬁcation system of lung adenocarcinoma is a stage-\nindependent predictor of survival. Journal of clinical oncol-\nogy, 30(13): 1438–1446.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. Cvt: Introducing convolutions to vision\ntransformers. arXiv preprint arXiv:2103.15808.\nXiong, Y .; Zeng, Z.; Chakraborty, R.; Tan, M.; Fung, G.; Li,\nY .; and Singh, V . 2021. Nystr\\” omformer: A Nystr\\” om-\nBased Algorithm for Approximating Self-Attention. arXiv\npreprint arXiv:2102.03902.\nYao, J.; Ganti, D.; Luo, X.; Xiao, G.; Xie, Y .; Yan, S.; and\nHuang, J. 2015. Computer-assisted diagnosis of lung cancer\nusing quantitative topology features. In International Work-\nshop on Machine Learning in Medical Imaging, 288–295.\nSpringer.\nYao, J.; Zhu, X.; Jonnagaddala, J.; Hawkins, N.; and Huang,\nJ. 2020. Whole slide images based cancer survival predic-\ntion using attention guided deep multiple instance learning\nnetworks. Medical Image Analysis, 65: 101789.\n2214\nYao, J.; Zhu, X.; Zhu, F.; and Huang, J. 2017. Deep\ncorrelational learning for survival prediction from multi-\nmodality data. In International Conference on Medical Im-\nage Computing and Computer-Assisted Intervention, 406–\n414. Springer.\nYou, Y .; Chen, T.; Sui, Y .; Chen, T.; Wang, Z.; and Shen,\nY . 2020. Graph contrastive learning with augmentations.\nNeurIPS.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.; Tay,\nF. E.; Feng, J.; and Yan, S. 2021. Tokens-to-token vit: Train-\ning vision transformers from scratch on imagenet. arXiv\npreprint arXiv:2101.11986.\nYuan, Y .; Failmezger, H.; Rueda, O. M.; Ali, H. R.; Gr¨af, S.;\nChin, S.-F.; Schwarz, R. F.; Curtis, C.; Dunning, M. J.; Bard-\nwell, H.; et al. 2012. Quantitative image analysis of cellular\nheterogeneity in breast tumors complements genomic pro-\nﬁling. Science translational medicine, 4(157): 157ra143–\n157ra143.\nZhu, X.; Yao, J.; and Huang, J. 2017. Deep convolutional\nneural network for survival analysis with pathological im-\nages. In IEEE International Conference on Bioinformatics\nBiomedicine.\nZhu, X.; Yao, J.; Zhu, F.; and Huang, J. 2017. Wsisa: Mak-\ning survival prediction from whole slide histopathological\nimages. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 7234–7242.\n2215"
}