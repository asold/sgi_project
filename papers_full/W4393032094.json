{
  "title": "Enhancing Hate Speech Detection with Fine-Tuned Large Language Models Requires High-Quality Data",
  "url": "https://openalex.org/W4393032094",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2672145006",
      "name": "Natalia Umansky",
      "affiliations": [
        "ETH Zurich",
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A3119245614",
      "name": "Maël Kubli",
      "affiliations": [
        "ETH Zurich",
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A234839737",
      "name": "Karsten Donnay",
      "affiliations": [
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A1818728020",
      "name": "Fabrizio Gilardi",
      "affiliations": [
        "Policy Analysis (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2252271275",
      "name": "Dominik Hangartner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A177114767",
      "name": "Ana Kotarcic",
      "affiliations": [
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2891795458",
      "name": "Laura Bronner",
      "affiliations": [
        "University of Zurich",
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A4318318919",
      "name": "Selina Kurer",
      "affiliations": [
        "University of Zurich",
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2407384962",
      "name": "Philip Grech",
      "affiliations": [
        "ETH Zurich",
        "University of Zurich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3092093993",
    "https://openalex.org/W4323354733",
    "https://openalex.org/W3034824379",
    "https://openalex.org/W632139601",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4361193900",
    "https://openalex.org/W4318464200",
    "https://openalex.org/W4285138772",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4387430709",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W3128553165",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W4385227045",
    "https://openalex.org/W2980704391",
    "https://openalex.org/W4385571830",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4322421454",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4388488349",
    "https://openalex.org/W4389524317",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4385071300"
  ],
  "abstract": "Efforts to curb online hate speech depend on our ability to reliably detect it at scale. Previous studies have highlighted the strong zero-shot classification performance of large-language models (LLMs), offering a potential tool to efficiently identify harmful content. Yet for complex and ambivalent tasks like hate speech detection, pre-trained LLMs can be insufficient and carry systemic biases. Domain-specific models, fine-tuned for the given task and empirical context could help address these issues but, as we demonstrate, the quality of data used for fine-tuning decisively matters. In this study, we fine-tuned GPT-3.5 using a unique corpus of online comments annotated by diverse groups of coders with varying annotation quality: research assistants, activists, two kinds of crowd workers, and citizen scientists. We find that only annotations from those groups of annotators that are better than zero-shot GPT-3.5 in recognizing hate speech improve the classification performance of the fine-tuned LLM. Specifically, fine-tuning using the two most high quality annotator groups -- research assistants and Prolific crowd workers -- boosts classification performance by increasing the model's precision without notably sacrificing the good recall of zero-shot GPT-3.5. In contrast, low quality annotations do not improve or even decrease the ability to identify hate speech.",
  "full_text": "Enhancing Hate Speech Detection with Fine-Tuned\nLarge Language Models Requires High-Quality Data\nNatalia Umansky†, Maël Kubli‡, Karsten Donnay§,\nFabrizio Gilardi¶, Dominik Hangartner/uni2016, Ana Kotarcic∗∗,\nLaura Bronner††, Selina Kurer‡‡, and Philip Grech\nAbstract\nEﬀorts to curb online hate speech depend on our ability to reliably detect it\nat scale. Previous studies have highlighted the strong zero-shot classiﬁcation per-\nformance of large-language models (LLMs), oﬀering a potential tool to eﬃciently\nidentify harmful content. Yet for complex and ambivalent tasks like hate speech\ndetection, pre-trained LLMs can be insuﬃcient and carry systemic biases. Domain-\nspeciﬁc models, ﬁne-tuned for the given task and empirical context could help ad-\ndress these issues but, as we demonstrate, the quality of data used for ﬁne-tuning\ndecisively matters. In this study, we ﬁne-tuned GPT-3.5 using a unique corpus of\nonline comments annotated by diverse groups of coders with varying annotation\nquality: research assistants, activists, two kinds of crowd workers, and citizen scien-\ntists. We ﬁnd that only annotations from those groups of annotators that are better\nthan zero-shot GPT-3.5 in recognizing hate speech improve the classiﬁcation per-\nformance of the ﬁne-tuned LLM. Speciﬁcally, ﬁne-tuning using the two most high\nquality annotator groups – research assistants and Proliﬁc crowd workers – boosts\nclassiﬁcation performance by increasing the model’s precision without notably sac-\nriﬁcing the good recall of zero-shot GPT-3.5. In contrast, low quality annotations\ndo not improve or even decrease the ability to identify hate speech.\n†Postdoctoral Research Fellow, University of Zurich.\nCorresponding author: umansky@ipz.uzh.ch\n‡PhD Researcher, University of Zurich\n§Assistant Professor of Political Behavior and Digital Media, University of Zurich\n¶Professor of Policy Analysis, University of Zurich\n/uni2016Professor of Public Policy, ETH Zurich\n∗∗Postdoctoral Research Fellow, University of Zurich\n††Senior Applied Scientist, ETH Zurich\n‡‡Project Manager, Immigration Policy Lab, ETH Zurich\nExecutive Director, Immigration Policy Lab, ETH Zurich\nWe are grateful to the team at alliance F and the Public Discourse Foundation, whose StopHate-\n1\n1 Introduction\nHate speech, broadly construed, alludes to any form of harmful content that includes\ntoxic communication by using pejorative, discriminatory, or aggressive language.1 As\nsuch, online hate speech has become a pressing concern in the digital age, necessitating\nthe ability to identify such content on a large scale (Parker & Ruths, 2023; Kotarcic,\nHangartner, Gilardi, Kurer, & Donnay, 2022; Gilardi, Alizadeh, & Kubli, 2023a). Recent\nadvancements in Natural Language Processing (NLP), especially the rapid progress in\nthe area of Large Language Models (LLMs), present new opportunities for addressing\nthis issue rapidly and eﬃciently. These models have demonstrated signiﬁcant zero-shot\ncapabilities across various NLP tasks, highlighting their potential utility in ﬂagging on-\nline harmful content at scale (Gilardi, Alizadeh, & Kubli, 2023b; Kasneci et al., 2023).\nYet, reliably detecting hate speech and other harmful online content using automated\napproaches remains challenging.\nHateful statements may be (intentionally) ambivalent or subtle and there is typically\nno reliable ground-truth for what constitutes oﬀending content. Moreover, pre-trained\nLLMs may carry, and even exacerbate, systemic biases, driven by the characteristics of\nthe datasets they were trained on (Ferrara, 2023; Kasneci et al., 2023; Shaikh, Zhang,\nHeld, Bernstein, & Yang, 2022; Ray, 2023). A domain-speciﬁc model ﬁne-tuned for the\ntask of detecting hate speech and sensitive to the particular context can help address\nthese issues and improve model performance, even with a small number of annotations\n(Fui-Hoon Nah, Zheng, Cai, Siau, & Chen, 2023; Kasneci et al., 2023; Kocoń et al., 2023).\nIn this paper, we contribute to these eﬀorts by ﬁne-tuning GPT-3.5 on a diverse corpus\nof online comments in German, each annotated by coders from ﬁve groups with varying\nannotation quality: research assistants, activists, two kinds of crowd workers, and citizen\nscientists. With expert annotations as gold standard for what constitutes hate speech,\nSpeech campaign inspired this project. We would also like to thank Citizen Science Zurich and the\ndiﬀerent groups of volunteers who contributed their time to this project. And we gratefully acknowledge\nfunding by InnoSuisse (Grant 46165.1 IP-SBM) and the Swiss Federal Oﬃce of Communications\n1For the purpose of this paper we do not distinguish between hate speech in the narrower sense,\ntargeting individuals as representatives of (protected) groups, and non-targeted toxic or harmful speech.\n2\nwe study how the performance of ﬁned-tuned GPT-3.5 varies, relative to the zero-shot\nmodel, depending on the quality of the annotations used for ﬁne-tuning.\nWe ﬁrst demonstrate that annotation quality for what constitutes hate speech no-\ntably diﬀers across our annotator groups. This includes signiﬁcant variation in inter-\ncoder agreement across groups. In addition, the agreement of diﬀerent groups with the\nexpert-based gold standard varies strongly between the best-performing group, trained\nresearch assistants, and the worst-performing group, NGO activists. These diﬀerences in\nannotation quality then directly impact whether the performance of the LLM can actually\nbe boosted through ﬁne-tuning or not. Speciﬁcally, we identify two key characteristics\nthat set high quality ﬁne-tuning data apart.\nFirst, only when using annotations from groups with a balanced ability to recover the\ngold standard, i.e., with relatively similar precision and recall, does ﬁne-tuning boost or at\nleast match the zero-shot performance of GPT-3.5. We observe this balanced annotation\nquality for all groups, except the NGO activists, where very low recall of the annotations\nleads to a marked decrease in classiﬁcation performance when ﬁne-tuning the model.\nSecond and most importantly, we ﬁnd that only ﬁne-tuning with data from groups that\narebetterthanzero-shotGPT-3.5atrecognizinghatespeechactuallyboostsclassiﬁcation\nperformance. Speciﬁcally, ﬁne-tuning using the two most high quality annotator groups\n– research assistants and Proliﬁc crowd workers — noticeably improves classiﬁcation\nperformance as compared to zero-shot GPT-3.5 by increasing precision without sacriﬁcing\nmuch of the good recall of the zero-shot model.\nTaken together, our analysis underscores the nuanced impact of data quality when\nﬁne-tuning LLMs to enhance hate speech detection. On the one hand, we demonstrate\nthat GPT-3.5 performance can be noticeably improved by ﬁne-tuning using only a very\nsmall number of labeled data. On the other hand, the quality of the annotations is\ncrucial: ﬁne-tuning helps when human coders perform better than zero-shot GPT-3.5,\nbut it has no impact or may even hurt classiﬁcation performance otherwise. In line with\nthe emerging literature on supervised ﬁne-tuning of LLMs for real-world classiﬁcation\n3\ntasks (Ma, Zhang, Fu, Zhao, & Wu, 2023), this ﬁnding suggests a move away from sheer\nquantity of labels to focusing on their quality instead. And while data quality is a known\nconcern in practice and has been widely acknowledged in the informal literature, there is\nto date a dearth of systematic evaluations of its impact. Our insights into what exactly\nconstitutes “high-quality” data and how this then translates to the performance of ﬁne-\ntuned LLMs are therefore not only methodologically highly-relevant but also have direct\nimplications for their real-world application.\nEnhancing the quality and precision of detection is a critical prerequisite for any kind\nof intervention that aims to limit the damaging eﬀects of hate speech in online discourses.\nThe development of task-speciﬁc and context-sensitive machine learning classiﬁers for\nhate speech detection demands signiﬁcant time, funding, and computing resources, along\nwithextensivetrainingdata(Laurer, vanAtteveldt, Casas,&Welbers, 2023). Incontrast,\nLLMs can be rapidly deployed and eﬃciently ﬁne-tuned to achieve excellent classiﬁca-\ntion performance, as long as comparably small but high quality datasets are used. As\nwe show, ﬁne-tuning GPT-3.5 with annotations from our best annotator group, trained\nresearch assistants, does not really lead to greater performance gains than the much more\ncost-eﬃcient labels collected from Proliﬁc crowd workers. This underscores the practi-\ncal feasibility of ﬁne-tuning LLMs for complex classiﬁcation tasks such as hate speech\ndetection.\n2 Data and Methods\n2.1 Hate Speech Corpus\nThis study leverages a unique, curated dataset of online comments in German encompass-\ning a diverse set of both hateful and non-hateful content. Overall, the dataset includes\nannotations by 204 individuals across ﬁve distinct groups: research assistants (3 people),\ncrowd workers from Appen (17 people) and from Proliﬁc (53 people), individuals partic-\nipating in a Citizen Science challenge (24 people), and a large group of activists in the\n4\nvolunteer community of a partner NGO (107 people). Each comment in our data was\nindependently annotated by at least three individual coders from each group.\nGiven the lack of an objective ground truth, three experts, all co-authors of this\npaper, additionally assigned gold standard labels for what constitutes hate speech in our\ndataset. In this step, the experts ﬁrst labeled each comment independently. Then, they\ndeliberated to reach a consensus on the comments that were not labeled unanimously in\nthe ﬁrst step. The harmonized labels attained from this procedure were employed as the\nstudy’s gold standard. To ensure comparability, all annotators and the experts received\nthe exact same instructions, asking them to determine whether a comment contained\nhate speech or not.\nOur initial corpus consisted of 500 unique user-produced comments obtained from the\ndiscussion section of collaborating Swiss national media outlets and included published,\nmoderated and deleted comments (Kotarcic et al., 2022). Due to platform settings, six\ncomments could not be consistently annotated across all groups, resulting in a total\nof 494 unique comments for analysis. To ensure consistency across annotator groups,\nwe randomly sampled three annotations per comment for groups where some comments\nwere annotated more than three times (notably Appen and the NGO).2 Finally, within\neach group, we determined a single label for each comment using majority rule, i.e., the\nmajority label selected by at least two of three coders within each group.\n2.2 Annotation quality of human coders\nWhen quantifying the variation in annotation quality among our human coders, we use\nstandard metrics to both measure inter-coder agreement and the quality of annotations\nrelative to the expert-based gold standard labels. Speciﬁcally, we rely on Krippendorﬀ’s\nalpha as the standard metric to evaluate the level of consistency among multiple an-\nnotators when assigning labels to the same data set (Barberá, Boydstun, Linn, McMa-\nhon, & Nagler, 2021; Van Atteveldt, Van der Velden, & Boukes, 2021). The two most\n2Preserving the per-comment distribution of positive and negative labels.\n5\nconsistent groups of coders are the trained research assistants (α = 0.506) and NGO\nactivists (α = 0.458); crowd workers from Proliﬁc (α = 0.391) and the citizen scientists\n(α= 0.335) are both less consistent. The by far lowest level of inter-coder agreement is\nfound for crowd workers from Appen (α= 0.080). The overall relatively low inter-coder\nagreement within all groups is in line with existing literature, emphasizing the inherent\nchallenges in achieving consensus among annotators for subjective tasks like hate speech\ndetection (Akhtar, Basile, & Patti, 2020; Kralj Novak et al., 2022).\nEvaluating the annotation quality for each group relative to the expert-based gold\nstandard, we rely on three standard measures commonly used for quantifying both an-\nnotation and classiﬁcation performance: precision, recall and, as an overall measure of\nannotation quality, F1 scores.3 Taken together, these metrics provide a comprehensive\nview of annotation quality, reﬂecting the reliability in identifying true instances of hate\nspeech (precision), the ability to capture all such instances (recall), and the respective\ntrade-oﬀs (F1). The research assistant coding has the highest quality coming closest to\nthe gold standard labels, followed by crowd workers from Proliﬁc, citizen scientists and,\nwith a notable drop in quality, both crowd workers from Appen and the NGO activists\n(Table 1).\nInterestingly, the annotations by NGO activists have the highest precision in recover-\ning the gold-standard labels but also the lowest recall, i.e., they most precisely identify\ninstances of hate speech but by far do not capture all of them. The other groups are,\nto varying degrees, more balanced in terms of precision/recall. Taken together, both\nthe comparisons among coders and the level of agreement with the gold-standard labels\nillustrate that our diﬀerent annotator groups indeed produced labels of signiﬁcantly dif-\nferent quality. This is a critical prerequisite to systematically evaluate the impact of data\nquality on the LLM ﬁne-tuning.\n3The F1 score is deﬁned in the usual way as the harmonic mean of precision and recall, F1 =\n2 · precision · recall/(precision + recall).\n6\nTable 1: Comparison of Annotation Quality Across Various Coder Groups\nResearch Assistants Proliﬁc Citizen Science Appen NGO\nF1-score 0.84 0.78 0.76 0.65 0.63\nPrecision 0.86 0.81 0.80 0.62 0.87\nRecall 0.82 0.74 0.72 0.68 0.49\nNote: We calculate all annotation quality measures relative to the expert-based gold standard hate\nspeech labels.\n2.3 Fine-Tuning GPT-3.5\nThe key strength of LLMs is that by virtue of their pre-training on vast amounts of data,\nthey already perform well on a wide range of tasks without additional training (Brown\net al., 2020; Liu et al., 2023). The broad scope of the models at the same time typically\nimplies a lack of context- and domain-speciﬁcity (Yang et al., 2023; Au Yeung et al.,\n2023). Fine-tuning here oﬀers a way to fully leverage the strength of a broad pre-trained\nmodel while enhancing performance with speciﬁc nuances or domain-speciﬁc knowledge\n(Wei et al., 2022; Ouyang et al., 2022). On a technical level, ﬁne-tuning adjusts a subset\nof the model’s weight parameters based on additional data with task-speciﬁc labels (Hu et\nal., 2023). In particular, for tasks such as hate speech detection, which are intrinsically\ncomplex and subjective, ﬁne-tuning also explicitly brings human perspectives into the\nloop. As a result, ﬁne-tuning not only improves model performance but also addresses\nconcerns regarding model-induced biases, which can be especially critical for tasks such\nas hate speech detection (Kasneci et al., 2023).\nIn this study, we use GPT-3.5 with the standard temperature setting (Gilardi et\nal., 2023a) as a hate speech classiﬁer, i.e., to determine whether a comment should be\nconsidered as hate speech or not.4 For this task, the model is directly prompted to classify\na text input without any task-speciﬁc ﬁne-tuning. It uses its general understanding of\nlanguage and context, acquired from pre-training, to decide on the classiﬁcation category.\nFor more details on the classiﬁcation, see Appendix A. Fine-tuning of the model was then\n4LLMs can generate some variation in outputs for identical instructions due to their probabilistic\nnature. In the zero-shot GPT-3.5 classiﬁcation, we prompted the model three times to classify each com-\nment to account for this variation and use the majority label. The variations in comment classiﬁcations\nare minor.\n7\nimplemented on OpenAI’s GPT-3.5 architecture by using the openly available API.5 Next\nto the model’s zero-shot performance, i.e., the base GPT-3.5 model performance without\nﬁne-tuning, we tested two diﬀerent ﬁne-tuning regimes: one using 100, the other 250\ncomments. Note that we used the same two random sets of comments across all groups\nassigning to each comment the respective group-speciﬁc (majority) label obtained from\nthe annotations.\nIn order to quantify the hate speech classiﬁcation performance of GPT-3.5 we rely on\nthe same standard metrics introduced above: precision, recall and F1. Relying on the\nsame metrics with the identical expert gold standard used as reference for quantifying\nannotation quality before, makes it possible to directly compare human annotations with\nthe automated LLM classiﬁcation. This will be instrumental when discussing how the\ndiﬀering quality of the human annotations impacts the ﬁne-tuned GPT-3.5 model perfor-\nmance. We also report conﬁdence intervals obtained through bootstrapping as a measure\nof the robustness of the model’s classiﬁcation performance.\n3 Results\n3.1 Zero-Shot Hate Speech Classiﬁcation\nThe reference point for whether ﬁne-tuning boosts the ability of GPT-3.5 to classify hate\nspeech is the performance of the zero-shot model. The baseline performance of zero-shot\nGPT-3.5 is already quite strong, even without prior speciﬁc training (vertical lines in\nFigure 1). With an F1 score of 0.78 it is as good or better than all human annotator\ngroups in identifying hate speech as compared to the expert-based gold standard, with\nthe exception of the trained research assistants (F1 0.84).\nThe caveat of using the zero-shot GPT-3.5 for classiﬁcation though is visible in the\nstark imbalance between its precision (0.67) and recall (0.91). The relatively low level of\nprecision (the proportion of true positive results among all positive predictions made by\n5Additional information on data preparation and analysis for chat model ﬁne-tuning can be found\nhere: https://cookbook.openai.com/examples/chat_ﬁnetuning_data_prep.\n8\nthe model) indicates that the model classiﬁes approximately one comment incorrectly for\nevery two instances of non-hate speech or hate speech it correctly identiﬁes. At the same\ntime, the high value for recall (the proportion of true positive results among all actual\npositives) implies that the zero-shot model successfully identiﬁes a large part of all true\nhate speech instances in the dataset. This, though, comes at the cost of generating a\nlarger number of false positives, as reﬂected in the much lower precision discussed above.\nThis imbalance highlights a trade-oﬀ in the zero-shot model’s performance, where it is\nclearly more eﬃcient in ensuring minimal miss of hate speech instances (high recall) but\nless accurate in distinguishing true hate speech from non-hate speech (lower precision).\nThis behavior of the classiﬁer could be quite problematic for real-world applications as\nit might lead to a comparably high rate of (over-)moderation of comments that are\nnot actually hate speech. The human annotator groups, by comparison, tend to have\nlower recall but greater precision and an overall more balanced annotation quality. This\nsuggests that, through ﬁne-tuning with human labels, GPT-3.5 should be able to gain\nprecision in identifying hate speech, ideally without a signiﬁcant loss in recall. In practice,\nthis would mean preserving the ability to identify relevant instances of hate speech but\nwith less instances of over-moderation.\n3.2 Fine-Tuned Model\nThe ﬁne-tuned GPT-3.5 model indeed shows a pronounced improvement in performance\nrelative to the zero-shot model (horizontal bars in Figure 1). When ﬁne-tuned with just\n100 labels (red bars), GPT-3.5’s performance improved by up to 5 percentage points,\nindicating that even when employing a very small training dataset for ﬁne-tuning, the\nmodel’s learning trajectory improves noticeably. The model’s performance further im-\nproved when increasing the training data from 100 to 250 labels (violet bars), reaching\nF1 scores of up to 0.82. These performance gains are largely due to notable gains in\nprecision as compared to the zero-shot model. The most robust sets of annotations for\nﬁne-tuning are the two sets with the best annotation quality across the diﬀerent groups\n9\nGPT-3.5:  0.78\nResearch Assistants\nProlific\nCitizen Science\nAppen\nNGO\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nF1-Score\nF1-Score of Human\nAnnotations\nGPT-3.5 F1-Score after\nfine-tuning on 100 labels\nGPT-3.5 F1-Score after\nfine-tuning on 250 labels\nGPT-3.5:  0.67\nResearch Assistants\nProlific\nCitizen Science\nAppen\nNGO\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nPrecision\nPrecision of Human\nAnnotations\nGPT-3.5 Precision after\nfine-tuning on 100 labels\nGPT-3.5 Precision after\nfine-tuning on 250 labels\nGPT-3.5:  0.91\nResearch Assistants\nProlific\nCitizen Science\nAppen\nNGO\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nRecall\nRecall of Human\nAnnotations\nGPT-3.5 Recall after\nfine-tuning on 100 labels\nGPT-3.5 Recall after\nfine-tuning on 250 labels\nFigure 1: F1, precision and recall of the ﬁne-tuned GPT-3.5 models for each annotator\ngroup with 100 labels (red bars) and 250 labels (violet bars) used for ﬁne-tuning. Perfor-\nmance is calculated relative to the expert-based gold standard. The vertical lines indicate\nthe performance of the zero-shot model. For comparison, we also provide the quality of\nthe labels used to ﬁne-tune the model for each annotator group (green bars). Error bars\nrepresent 95% conﬁdence intervals obtained through bootstrapping.\n10\n(shown as green bars for comparison): research assistants and Proliﬁc crowd workers.\nThese two groups also have the ﬁne-tuned models with the overall largest recall that is\nalmost on par with the good zero-shot GPT-3.5 recall values.\nA closer look at precision and recall, illustrates how GPT-3.5’s performance changes in\nthe ﬁne-tuning process. When using training data from research assistants, i.e., the most\nhigh-quality annotations, the model’s precision increases signiﬁcantly (from a relatively\nlow level of 0.67 to 0.76) while recall decreases minimally from 0.91 to 0.90, aligning more\nclosely with the degree of precision and recall of the training data. Analyzing the model\nperformance in detail,6 we observe that this can mainly be attributed to a decrease in false\npositive classiﬁcations of the ﬁne-tuned model, without a corresponding increase in false\nnegatives or decrease in true positives. The overall best-performing model ﬁned-tuned on\nthese data just about matches the best human annotation quality. The pattern is a bit\ndiﬀerent for Proliﬁc where the best performing ﬁne-tuned model has a notable boost in\nprecision but at a greater loss in recall for the overall (close) second-best performance. We\ncan again attribute this to a notable drop in false positive classiﬁcations after ﬁne-tuning\nbut a corresponding greater increase in false negatives. In both cases precision/recall\nare much more balanced in the ﬁne-tuned as compared to the zero-shot GPT-3.5 model\nspeaking to an overall more robust classiﬁer.\nWe also observe a signiﬁcant gain in precision for overall lower-quality annotations,\nsuch as those of the NGO, which are characterized by high precision but low recall. Conse-\nquently, ﬁne-tuning with these lower-quality annotations signiﬁcantly increases precision\nbut also reduces recall substantially, resulting in an overall decrease in F1-Score and less\nrobust performance than the zero-shot model. The citizen science annotations were of\nalmost as high quality as those of the Proliﬁc crowd workers. We here see a consistent\ngain in precision through ﬁne-tuning but at the cost of recall performance. Fine-tuning\nwith citizen science labels limits false positives thereby increasing precision but it also\nboth leads to less true positives and more false negatives and, thus, lower recall. With the\ncomparably lower quality Appen crowd worker annotations ﬁne-tuning leads to the least\n6The confusion matrices for all classiﬁers are provided in Appendix D.\n11\npronounced precision gains but almost no loss in recall basically recovering the zero-shot\nperformance in terms of F1.\n4 Conclusion\nEﬀorts to curb online hate speech and to protect the integrity of democratic discourse\ndepend on the capacity to detect instances of harmful speech at scale. This paper demon-\nstratesthepotentialofﬁne-tuned, context-andtask-speciﬁcLLMstoimprovehatespeech\ndetection and contribute to content moderation pipelines with a rapid, robust, and scal-\nable approach. However, as we show in this study, the quality of the ﬁne-tuning data\nmatters decisively for the ability of LLMs like GPT-3.5 to classify hate speech. That\nis, ﬁne-tuning with any data does not guarantee better performance. In fact, we show\nthat using low quality annotations for ﬁne-tuning can even decrease the LLMs zero-shot\nperformance. Only high quality data that balances the precision/recall trade-oﬀ of the\nLLM noticeably boost its classiﬁcation performance.\nA crucial aspect of Natural Language Processing pipelines, both for hate speech detec-\ntion and a range of other similar tasks, is the substantial number of annotations required\nto train context-speciﬁc classiﬁers (Barberá et al., 2021; Benoit, Conway, Lauderdale,\nLaver, & Mikhaylov, 2016; Boukes, Van de Velde, Araujo, & Vliegenthart, 2020; Van At-\nteveldt et al., 2021; Laurer et al., 2023). The use of ﬁne-tuned LLMs here oﬀers a move\naway from from sheer quantity of labels to focusing on less but more high-quality data\ninstead. Our results suggest that high-quality (human) labels will continue to play a\ncritical role in ensuring the kind of task and context-sensitivity required for LLMs to\nmatch the classiﬁcation performance of human annotators. And, as we demonstrate, this\nis reasonably achievable in practice: the amount of high-quality data needed is not that\nlarge, and comparably cost eﬃcient crowd work annotations are suitable to eﬀectively\nboost the model’s performance.\nFine-tuned LLMs have important real-world applications beyond the speciﬁc context\nof hate speech detection. For example, there is an emerging literature that considers the\n12\nrole of supervised ﬁne-tuning for facilitating content moderation decisions more broadly\n(Ma et al., 2023). And, just as for the speciﬁc application to hate speech, this integrates\nLLMs with high-quality labels by human moderators. This hybrid approach leverages the\nstrengths of both AI and human judgement, with the potential to result in more accurate,\nfair, and nuanced content moderation at scale. But more research is needed to develop\nspeciﬁc guidelines and frameworks to govern the use of AI in these contexts. This includes\nguidance on which benchmarks should be used in evaluating classiﬁcation performance,\nespecially with regard to evaluating fairness and guarding against (algorithmic) discrimi-\nnation in content moderation decisions. And, on a methodological level, it remains to be\nshown that boosting performance of LLMs through ﬁne-tuning with high-quality data is\nsomething that generalizes to other linguistic and cultural contexts.\n13\nReferences\nAkhtar, S., Basile, V., & Patti, V. (2020). Modeling annotator perspective and polarized\nopinions to improve hate speech detection. InProceedings of the aaai conference\non human computation and crowdsourcing(Vol. 8, pp. 151–154).\nAu Yeung, J., Kraljevic, Z., Luintel, A., Balston, A., Idowu, E., Dobson, R. J., & Teo,\nJ. T. (2023). Ai chatbots not yet ready for clinical use.Frontiers in Digital Health,\n5, 60.\nBarberá, P., Boydstun, A. E., Linn, S., McMahon, R., & Nagler, J. (2021). Automated\ntext classiﬁcation of news articles: A practical guide. Political Analysis, 29(1),\n19–42.\nBenoit, K., Conway, D., Lauderdale, B. E., Laver, M., & Mikhaylov, S. (2016). Crowd-\nsourcedtextanalysis: Reproducibleandagileproductionofpoliticaldata. American\nPolitical Science Review, 110(2), 278–295.\nBoukes, M., Van de Velde, B., Araujo, T., & Vliegenthart, R. (2020). What’s the tone?\neasy doesn’t do it: Analyzing performance and agreement between oﬀ-the-shelf\nsentiment analysis tools.Communication Methods & Measures, 14(2), 83–104.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... others\n(2020). Language models are few-shot learners. Advances in neural information\nprocessing systems, 33, 1877–1901.\nFerrara, E. (2023). Should chatgpt be biased? challenges and risks of bias in large\nlanguage models. arXiv preprint arXiv:2304.03738.\nFui-Hoon Nah, F., Zheng, R., Cai, J., Siau, K., & Chen, L. (2023).Generative ai and\nchatgpt: Applications, challenges, and ai-human collaboration (Vol. 25) (No. 3).\nTaylor & Francis.\nGilardi, F., Alizadeh, M., & Kubli, M. (2023a).ChatGPT Outperforms Crowd-Workers\nfor Text-Annotation Tasks.\nGilardi, F., Alizadeh, M., & Kubli, M. (2023b). Chatgpt outperforms crowd-workers for\ntext-annotation tasks. arXiv preprint arXiv:2303.15056.\n14\nHu, Z., Wang, L., Lan, Y., Xu, W., Lim, E.-P., Bing, L., ... Lee, R. (2023, December).\nLLM-adapters: An adapter family for parameter-eﬃcient ﬁne-tuning of large lan-\nguage models. In H. Bouamor, J. Pino, & K. Bali (Eds.),Proceedings of the 2023\nconference on empirical methods in natural language processing(pp. 5254–5276).\nSingapore: Association for Computational Linguistics.\nKasneci, E., Seßler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., ...\nothers (2023). Chatgpt for good? on opportunities and challenges of large language\nmodels for education.Learning and individual diﬀerences, 103, 102274.\nKocoń, J., Cichecki, I., Kaszyca, O., Kochanek, M., Szydło, D., Baran, J., ... others\n(2023). Chatgpt: Jack of all trades, master of none.Information Fusion, 101861.\nKotarcic, A., Hangartner, D., Gilardi, F., Kurer, S., & Donnay, K. (2022). Human-in-\nthe-loop hate speech classiﬁcation in a multilingual context. InProceedings of the\n2022 conference on empirical methods in natural language processing.\nKralj Novak, P., Scantamburlo, T., Pelicon, A., Cinelli, M., Mozetič, I., & Zollo, F.\n(2022). Handling disagreement in hate speech modelling. InInformation processing\nand management of uncertainty in knowledge-based systems: 19th international\nconference, ipmu 2022, milan, italy, july 11–15, 2022, proceedings, part ii(pp.\n681–695).\nLaurer, M., van Atteveldt, W., Casas, A., & Welbers, K. (2023). Less annotating,\nmore classifying: Addressing the data scarcity issue of supervised machine learning\nwith deep transfer learning and bert-nli. Political Analysis, 1–17. doi: 10.1017/\npan.2023.20\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt,\nand predict: A systematic survey of prompting methods in natural language pro-\ncessing. ACM Computing Surveys, 55(9), 1–35.\nMa, H., Zhang, C., Fu, H., Zhao, P., & Wu, B. (2023).Adapting large language models\nfor content moderation: Pitfalls in data engineering and supervised ﬁne-tuning.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P.,\n15\n... Lowe, R. (2022). Training language models to follow instruc-\ntions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal,\nD. Belgrave, K. Cho, & A. Oh (Eds.), Advances in neural information\nprocessing systems (Vol. 35, pp. 27730–27744). Curran Associates, Inc.\nRetrieved from https://proceedings.neurips.cc/paper_files/paper/2022/\nfile/b1efde53be364a73914f58805a001731-Paper-Conference.pdf\nParker, S., & Ruths, D. (2023). Is hate speech detection the solution the world wants?\nProceedings of the National Academy of Sciences, 120(10), e2209384120.\nRay, P. P. (2023). Chatgpt: A comprehensive review on background, applications, key\nchallenges, bias, ethics, limitations and future scope.Internet of Things and Cyber-\nPhysical Systems.\nShaikh, O., Zhang, H., Held, W., Bernstein, M., & Yang, D. (2022). On second thought,\nlet’s not think step by step! bias and toxicity in zero-shot reasoning.arXiv preprint\narXiv:2212.08061.\nVanAtteveldt, W., VanderVelden, M.A.,&Boukes, M. (2021). Thevalidityofsentiment\nanalysis: Comparing manual annotation, crowd-coding, dictionary approaches, and\nmachine learning algorithms.Communication Methods and Measures, 15(2), 121–\n140.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., ... Le, Q. V. (2022).\nFinetuned language models are zero-shot learners.\nYang, R., Tan, T. F., Lu, W., Thirunavukarasu, A. J., Ting, D. S. W., & Liu, N. (2023).\nLarge language models in health care: Development, applications, and challenges.\nHealth Care Science, 2(4), 255–263.\n16\nSupplementary Information\nA GPT-3.5 Classiﬁcation Pipeline\nIn this study, we used OpenAI’s ChatGPT (API model 3.5 Turbo) to perform hate\nspeech classiﬁcation. The process is grounded in a predeﬁned codebook, which serves as\na fundamental guide for ChatGPT’s analysis and categorization of comments based on\nspeciﬁc themes.\nA.1 Codebook and classiﬁcation Prompt\nThe codebook encompasses a set of categories or themes against which ChatGPT was\ntasked to evaluate each comment. The model was presented with a German language\nprompt for classiﬁcation, we here provide an English translation; the original German\nprompt can be found in Appendix C. The prompt is structured as follows:7\nInstructions:\nYou will read comments posted to articles on the Internet and determine\nwhether they are hate speech or not. There is no universal deﬁnition of\nhate speech. A widely accepted deﬁnition, also used by the United Nations,\ndeﬁnes hate speech as the use of language that attacks or devalues a person\nor group based on their identity - e.g. based on their gender, age, sexuality,\nreligion, nationality, skin color or origin, mental or physical impairment. A\nbroader deﬁnition of hate speech can also include derogatory language against\na person or group based on their social status (e.g. education or income) or\ntheir political views. In addition, there is toxic/hateful language that, while\nnot necessarily hate speech, nevertheless aﬀects the culture of discussion and\npromotes hostility. This includes general insults, threats or vulgar statements\n7Note that we prompted the GPT-3.5 classiﬁcation with language referring to “annotation.” This is\na deliberate choice of terminology because we wanted to contextualize the LLM to closely emulate the\ntask of a human coder.\n17\nthat are not directed against a person or group based on their identity. We\nwould like to leave it up to you which deﬁnition of hate speech you use for your\nannotations. It is simply important that you not only mark in the annotations\nwhether something is hate speech (or not), but also which group it is directed\nagainst or whether it is toxic language.\nSteps:\n1. Read the comment.\n2. If a comment does not contain hate speech, please answer question 1\nwith ‘False’. If a comment contains hate speech, please answer question\n1 with ‘True’.\n3. Only if the comment contains hate speech, please also answer question 2,\nin which you indicate which group(s) of people the comment is directed\nagainst.\nQuestion 1 (label):\nClassify the tweets as follows:\n• True - The tweet contains hate speech\n• False - The tweet does not contain hate speech\nFormat for the output:\nAlways specify the result in JSON format, do not specify anything else. Here\nis an example of the format you must use:\n{\"Label\": \"<LABEL>\", \"reason\": \"Reasoning or evidence for the <LA-\nBEL> and <TARGET> chosen.\"}\nThis prompt guides the model through the classiﬁcation process, instructing it to\ndiscern whether a comment constitutes hate speech and, if so, to identify the speciﬁc\ntarget group aﬀected.\n18\nA.2 Operational Procedure\nThemodelindividuallyprocessedeachcommentandthenclassiﬁeditbasedonthethemes\nand the conceptual rules deﬁned in the codebook. We repeated the procedure three times\nfor each comment to match the threefold annotation in all (human) annotator groups.\nSpeciﬁcally, classiﬁcation entails feeding a comment as input to the model along with\nthe codebook. In this context, the codebook served as a priming sequence to direct\nthe model’s attention towards the desired facets of the comment text, essentially, the\nattributes we wished to encode. Primed with the themes and the rules from the codebook,\nChatGPT-3.5 Turbo then generated an output representing the annotated comment (in\nthe format we speciﬁed in the prompt). We utilized the standard temperature setting for\nChatGPT for classiﬁcation (0.2).\nB Model Fine-Tuning\nThe zero-shot GPT-3.5 model was ﬁne-tuned using the same stratiﬁed sample of 100 and\n250 comments for each annotator group, maintaining the (gold standard) label distri-\nbution characteristics of the original dataset. This stratiﬁed approach guaranteed that\nboth the 100-comment and 250- random comment subsets faithfully represented the la-\nbel proportions observed in the full dataset. We utilized OpenAI’s API to ﬁne-tune the\nGPT-3.5 model providing the two respective samples as training data respectively.\nIn the ﬁne-tuning step, we conﬁgured our model to run for three epochs (nepochs = 3)\nand set the temperature parameter as 0.0 (temp = 0.0). The temperature parameter\ndirectly inﬂuences the randomness of the model’s output, with our speciﬁed value pro-\nmoting a more deterministic response. Thus, setting this value to the absolute minimum\nreduces variability to a minimum, which allows us to run the classiﬁcation only once since\nthe results should be equal in almost all cases over multiple runs. We are able to assume\nsuch model behavior as we already encountered very low variability in the zero-shot clas-\nsiﬁcations, which we ran three times (Krippendorﬀ’sα0.682). A single epoch constitutes\n19\na full pass of our training data through the model. In simple words, during an epoch,\nthe model weights are ﬁne-tuned to minimize the diﬀerence between the predicted and\nactual class labels. It is essential to note that this process was undertaken independently\nfor both the 100 and 250-row datasets, forming two distinct ﬁne-tuned models for each\nannotation group.\nThe full ﬁne-tuning pipeline proceeds as follows. First, we pre-processed the training\ndataset following the template provided by OpenAI.8 After pre-processing, we uploaded\nthe data to the API and initialized the model with the pre-trained weights. In the\nsecond step, we provided the model with instructions for our particular classiﬁcation\ntask. These instructions are given in a .json friendly format that the language model\nunderstands. This also guides the model’s understanding and learning, in alignment\nwith our classiﬁcation aims. The instructions comprise both system-level and user-level\nprompts. The system-level prompt oﬀers overarching guidance for the entire ﬁne-tuning\nprocess. It outlines the task at hand – reading comments from internet articles and\ndetermining whether they contain Hate Speech or Not Hate Speech. In our case, the\nsystem prompt thus contains the complete codebook (see Appendix A.1).\nThe user-level prompt oﬀers granular instructions for handling individual instances\nwithin the dataset. It directs the model on processing and interpreting each comment,\nguiding its classiﬁcation decisions. It reads as follows for our task (we here provide an\nEnglish translation; the original German prompt can be found in Appendix C):\nUser prompt:\nYou will read the following comment from the internet and determine whether\nit is HATE SPEECH or not. There is no universal deﬁnition of hate speech.\nA widely accepted deﬁnition, also used by the United Nations, deﬁnes hate\nspeech as the use of language that attacks or devalues a person or group\nbased on their identity - e.g. based on their gender, age, sexuality, religion,\nnationality, skin color or origin, mental or physical impairment.\n8Template: https://platform.openai.com/docs/guides/ﬁne-tuning/preparing-your-dataset.\n20\nA broader deﬁnition of hate speech can also include derogatory language\nagainst a person or group based on their social status (e.g. education or\nincome) or political views. In addition, there is toxic/hateful language that,\nwhile not necessarily hate speech, still detracts from the culture of discussion\nand promotes hostility. This includes general insults, threats or vulgar state-\nments that are not directed at a person or group based on their identity.\nDoesthefollowingcommentcontainHATESPEECH(1)orNOHATESPEECH?\n(0)\nBy incorporating both system-level and user-level prompts, we ensure that the model\nreceives comprehensive guidance throughout the ﬁne-tuning process. The system-level\nprompt sets the context and provides overarching guidance, while the user-level prompt\noﬀersspeciﬁcinstructionsforhandlingindividualinstances, ultimatelyguidingthemodel’s\nunderstanding and aligning it with our classiﬁcation aims.\nIn the last step, GPT-3.5 is ﬁne-tuned through multiple epochs of iterative training\nduring which the model dynamically adjusts its parameters based on the training data.\nEach epoch involves a complete pass of the dataset through the model, allowing it to learn\nand reﬁne its representations of the data. This iterative reﬁnement process enables the\nmodel to minimize the discrepancy between predicted and actual class labels, gradually\nenhancing its predictive capabilities. Throughout the ﬁne-tuning process, the model\nadapts its parameters to align with the speciﬁc classiﬁcation task, ultimately improving\nits proﬁciency in accurately classifying comments as containing Hate Speech or Not Hate\nSpeech.\nC German Prompt Texts\nThe following corresponds to the German language ChatGPT prompt provided in its\nEnglish translation in Appendix A.1.\n21\nAnleitung:\nSie werden Kommentare zu Artikeln aus dem Internet lesen und bestimmen,\nob es sich um Hate Speech handelt oder nicht. Es gibt keine allgemeingültige\nDeﬁnition von Hate Speech. Eine weit verbreitete und auch von den Verein-\nten Nationen verwendete Deﬁnition versteht unter Hate-Speech den Gebrauch\nvon Sprache, durch den eine Person oder eine Gruppe aufgrund ihrer Identität\n– z.B. aufgrund ihres Geschlechts, Alters, ihrer Sexualität, Religion, Nation-\nalität, Hautfarbe oder Herkunft, geistigen oder körperlichen Beeinträchtigung\n– angegriﬀen oder abgewertet wird. Eine breitere Deﬁnition von Hate Speech\nkann auch abwertenden Sprachgebrauch gegen eine Person oder Gruppe auf-\ngrund ihres sozialen Status (z.B. Bildung oder Einkommen) oder ihrer poli-\ntischen Einstellung beinhalten. Zusätzlich gibt es toxischen/hasserfüllten\nSprachgebrauch, der zwar nicht unbedingt Hate Speech ist, aber trotzdem die\nDiskussionskultur beeinträchtigt und Feindseligkeit fördert. Dazu gehören all-\ngemeine Beleidigungen, Drohungen oder vulgäre Aussagen, welche sich aber\nnicht gegen eine Person oder eine Gruppe aufgrund ihrer Identität richten.\nWelche Deﬁnition von Hate Speech Sie für Ihre Annotationen verwenden,\nmöchten wir Ihnen überlassen. Wichtig ist einfach, dass Sie bei den Anno-\ntationen nicht nur markieren, ob etwas Hate Speech ist (oder nicht), son-\ndern auch gegen welche Gruppe sich diese richtet oder ob es sich um toxische\nSprache handelt.\nSchritte:\n1. Lesen Sie den Kommentar.\n2. Wenn ein Kommentar keine Hate Speech enthält, beantworten Sie Frage\n1 bitte mit ‘False’. Wenn ein Kommentar Hate Speech enthält, beant-\nworten Sie die Frage 1 bitte mit ‘True’.\n3. Nur wenn der Kommentar Hate Speech enthält, beantworten Sie bitte\n22\nauch Frage 2, in der Sie angeben, gegen welche Personen-Gruppe(n) sich\nder Kommentar richtet.\nFrage 1 (Label):\nKlassiﬁziere die Tweets folgendermassen:\n• True - Der Tweet enthält Hate Speech\n• False - Der Tweet enthält keine Hate Speech\nFormat für die Ausgabe:\nGib das Ergebnis immer im JSON-Format an, gib nichts anderes an. Hier ist\nein Beispiel für das Format, das du verwenden must:\n{\"Label\": \"<LABEL>\", \"reason\": \"Reasoning or evidence for the <LA-\nBEL> and <TARGET> chosen.\"}\nAnd the following is the additional user-level prompt employed during model ﬁne-\ntuning provided in its English translation in Appendix B:\nUser Prompt:\nSie werden den folgenden Kommentar aus dem Internet lesen und bestim-\nmen, ob es sich um HATE SPEECH handelt oder nicht. Es gibt keine allge-\nmeingültige Deﬁnition von Hate Speech. Eine weit verbreitete und auch von\nden Vereinten Nationen verwendete Deﬁnition versteht unter Hate-Speech den\nGebrauch von Sprache, durch den eine Person oder eine Gruppe aufgrund\nihrer Identität – z.B. aufgrund ihres Geschlechts, Alters, ihrer Sexualität,\nReligion, Nationalität, Hautfarbe oder Herkunft, geistigen oder körperlichen\nBeeinträchtigung – angegriﬀen oder abgewertet wird.\nEine breitere Deﬁnition von Hate Speech kann auch abwertenden Sprachge-\nbrauch gegen eine Person oder Gruppe aufgrund ihres sozialen Status (z.B.\n23\nBildungoderEinkommen)oderihrerpolitischenEinstellungbeinhalten. Zusät-\nzlich gibt es toxischen/hasserfüllten Sprachgebrauch, der zwar nicht unbed-\ningt Hate Speech ist, aber trotzdem die Diskussionskultur beeinträchtigt und\nFeindseligkeit fördert. Dazu gehören allgemeine Beleidigungen, Drohungen\noder vulgäre Aussagen, welche sich aber nicht gegen eine Person oder eine\nGruppe aufgrund ihrer Identität richten.\nBeinhaltet der folgende Kommentar HATE SPEECH (1) oder KEINE HATE\nSPEECH? (0)\nD Confusion Matrices for GPT-3.5 Classiﬁcations\nTable 2: Performance comparison of GPT-3.5 across various ﬁne-tuning datasets, high-\nlighting the confusion matrix outcomes. Bold values denote the highest values achieved\nin each category, showcasing the impact of dataset selection on model accuracy.\nTP TN FP FN\nGPT-3.5 Zero Shot 217 151 105 21\nGPT-3.5 Fine-Tuned with Appen (100 rows) 197 188 68 41\nGPT-3.5 Fine-Tuned with Appen (250 rows) 209 150 106 29\nGPT-3.5 Fine-Tuned with Citizen Science (100 rows) 164 219 37 74\nGPT-3.5 Fine-Tuned with Citizen Science (250 rows) 174 216 40 64\nGPT-3.5 Fine-Tuned with NGO (100 rows) 96 247 9 142\nGPT-3.5 Fine-Tuned with NGO (250 rows) 109 249 7 129\nGPT-3.5 Fine-Tuned with Proliﬁc (100 rows) 216 161 95 22\nGPT-3.5 Fine-Tuned with Proliﬁc (250 rows) 194 209 47 44\nGPT-3.5 Fine-Tuned with Research Assistants (100 rows) 187 201 55 51\nGPT-3.5 Fine-Tuned with Research Assistants (250 rows) 213 188 68 25\n24",
  "topic": "Voice activity detection",
  "concepts": [
    {
      "name": "Voice activity detection",
      "score": 0.7113057374954224
    },
    {
      "name": "Computer science",
      "score": 0.650141716003418
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5992549657821655
    },
    {
      "name": "Speech recognition",
      "score": 0.4254264235496521
    },
    {
      "name": "Speech processing",
      "score": 0.21041497588157654
    },
    {
      "name": "Physics",
      "score": 0.08992713689804077
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I202697423",
      "name": "University of Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210122455",
      "name": "Policy Analysis (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1334582091",
      "name": "Migration Policy Institute",
      "country": "US"
    }
  ]
}