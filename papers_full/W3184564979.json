{
  "title": "TransPose: Keypoint Localization via Transformer",
  "url": "https://openalex.org/W3184564979",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2095688776",
      "name": "Sen Yang",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2104215794",
      "name": "Zhibin Quan",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2751757348",
      "name": "Mu Nie",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2137621302",
      "name": "Wankou Yang",
      "affiliations": [
        "Southeast University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6784094891",
    "https://openalex.org/W4229494842",
    "https://openalex.org/W6697925102",
    "https://openalex.org/W3175199633",
    "https://openalex.org/W2052678124",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W6785297999",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W6753441378",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W2964304707",
    "https://openalex.org/W6767763749",
    "https://openalex.org/W6750378959",
    "https://openalex.org/W6786209122",
    "https://openalex.org/W2742737904",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6639204139",
    "https://openalex.org/W3034750257",
    "https://openalex.org/W6771497439",
    "https://openalex.org/W6724569667",
    "https://openalex.org/W2792824754",
    "https://openalex.org/W6787149526",
    "https://openalex.org/W2295107390",
    "https://openalex.org/W1787224781",
    "https://openalex.org/W2080873731",
    "https://openalex.org/W2963781481",
    "https://openalex.org/W6781889015",
    "https://openalex.org/W2606462007",
    "https://openalex.org/W2530887700",
    "https://openalex.org/W6684821475",
    "https://openalex.org/W3034971010",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6759892625",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W6786708909",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W2113325037",
    "https://openalex.org/W6680285999",
    "https://openalex.org/W2958088908",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6745881451",
    "https://openalex.org/W6787329981",
    "https://openalex.org/W6786585107",
    "https://openalex.org/W6748709139",
    "https://openalex.org/W2964221239",
    "https://openalex.org/W3034399482",
    "https://openalex.org/W6786400452",
    "https://openalex.org/W2964105113",
    "https://openalex.org/W6786026227",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6707250398",
    "https://openalex.org/W6737160166",
    "https://openalex.org/W3108651438",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W6774556237",
    "https://openalex.org/W3126974104",
    "https://openalex.org/W6685133223",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2174722029",
    "https://openalex.org/W6763367864",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3000716014",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W2578797046",
    "https://openalex.org/W602397586",
    "https://openalex.org/W3101609372",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2972747457",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3113950706",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W2962773068",
    "https://openalex.org/W2916431757",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3108516375",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2136391815",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W2503388974",
    "https://openalex.org/W3049708585",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2995426144",
    "https://openalex.org/W2963402313",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W3139887931",
    "https://openalex.org/W2963598138",
    "https://openalex.org/W2963448913",
    "https://openalex.org/W3101679430",
    "https://openalex.org/W3107651629",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963749936",
    "https://openalex.org/W2963623257",
    "https://openalex.org/W2950527759"
  ],
  "abstract": "While CNN-based models have made remarkable progress on human pose estimation, what spatial dependencies they capture to localize keypoints remains unclear. In this work, we propose a model called \\textbf{TransPose}, which introduces Transformer for human pose estimation. The attention layers built in Transformer enable our model to capture long-range relationships efficiently and also can reveal what dependencies the predicted keypoints rely on. To predict keypoint heatmaps, the last attention layer acts as an aggregator, which collects contributions from image clues and forms maximum positions of keypoints. Such a heatmap-based localization approach via Transformer conforms to the principle of Activation Maximization~\\cite{erhan2009visualizing}. And the revealed dependencies are image-specific and fine-grained, which also can provide evidence of how the model handles special cases, e.g., occlusion. The experiments show that TransPose achieves 75.8 AP and 75.0 AP on COCO validation and test-dev sets, while being more lightweight and faster than mainstream CNN architectures. The TransPose model also transfers very well on MPII benchmark, achieving superior performance on the test set when fine-tuned with small training costs. Code and pre-trained models are publicly available\\footnote{\\url{https://github.com/yangsenius/TransPose}}.",
  "full_text": "TransPose: Keypoint Localization via Transformer\nSen Yang Zhibin Quan Mu Nie Wankou Yang *\nSchool of Automation, Southeast University, Nanjing 210096, China\n{yangsenius, 101101872, niemu, wkyang}@seu.edu.cn\nAbstract\nWhile CNN-based models have made remarkable\nprogress on human pose estimation, what spatial depen-\ndencies they capture to localize keypoints remains un-\nclear. In this work, we propose a model called Trans-\nPose, which introduces Transformer for human pose esti-\nmation. The attention layers built in Transformer enable\nour model to capture long-range relationships efficiently\nand also can reveal what dependencies the predicted key-\npoints rely on. To predict keypoint heatmaps, the last at-\ntention layer acts as an aggregator, which collects contri-\nbutions from image clues and forms maximum positions of\nkeypoints. Such a heatmap-based localization approach via\nTransformer conforms to the principle of Activation Maxi-\nmization [19]. And the revealed dependencies are image-\nspecific and fine-grained, which also can provide evidence\nof how the model handles special cases, e.g., occlusion.\nThe experiments show that TransPose achieves 75.8 AP and\n75.0 AP on COCO validation and test-dev sets, while being\nmore lightweight and faster than mainstream CNN archi-\ntectures. The TransPose model also transfers very well on\nMPII benchmark, achieving superior performance on the\ntest set when fine-tuned with small training costs. Code and\npre-trained models are publicly available1.\n1. Introduction\nDeep convolutional neural networks have achieved im-\npressive performances in the field of human pose estima-\ntion. DeepPose [56] is the early classic method, directly\nregressing the numerical coordinate locations of keypoints.\nAfterwards, fully convolutional networks like [60, 36, 38,\n63, 12, 40, 61, 51] have become the mainstream by pre-\ndicting keypoints heatmaps, which implicitly learn spatial\ndependencies between body parts. Yet, most prior works\ntake deep CNN as a powerful black box predictor and focus\non improving the network structure, what exactly happens\ninside the models or how they capture the spatial relation-\nships between body parts remains unclear. However, from\n*Corresponding author.\n1https://github.com/yangsenius/TransPose\nLow-level\nConvolutional \nBlocks\nATTENTION \nLayer #1\nATTENTION \nLayer #2\nATTENTION \nLayer #3Image Predicted\nKeypoints\nFigure 1. A schematic diagram of TransPose. Below: The infer-\nence pipeline. Above: Dependency areas for each predicted key-\npoint location. In this example, the person’s left-ankle is occluded\nby a dog. Which exact image clues the model uses to infer the\noccluded joint? The attention map (red box) gives fine-grained\nevidence beyond intuition: such a pose estimator highly relies on\nthe image clues around the left ankle, left upper leg, and joints on\nthe right leg to estimate the location of occluded left ankle.\nthe scientific and practical standpoints, the interpretability\nof the model can aid practitioners the ability to understand\nhow the model associates structural variables to reach the\nfinal predictions and how a pose estimator handles various\ninput images. It also can help model developers for debug-\nging, decision-making, and further improving the design.\nFor existing pose estimators, some issues make it chal-\nlenging to figure out their decision processes. (1) Deep-\nness. The CNN-based models, such as [60, 38, 61, 51],\nare usually very deep non-linear models that hinder the in-\nterpretation of the function of each layer. (2) Implicit re-\nlationships. The global spatial relationships between body\nparts are implicitly encoded within the neuron activations\nand the weights of CNNs. It is not easy to decouple such\nrelationships from large amounts of weights and activations\nin neural networks. And solely visualizing the intermediate\nfeatures with a large number of channels (e.g. 256, 512 in\nSimpleBaseline architecture [61]) provides little meaning-\nStacking Many Convolutional Layers \nto Achieve a Large Receptive Field\nAttend to Any Pairwise Locations\nby One Attention Layer\n…\nConv 1\nConv 2\nConv 3\nConv i\nConv j\nConv k ……\nFigure 2. CNN vs. Attention. Left: The receptive filed enlarges in\nthe deeper convolutional layer.Right: One self-attention layer can\ncapture the pairwise relationship between any pair of locations.\nful explanations. (3) Limited working memory in inferring\nvarious images. The desired explanations for the model pre-\ndictions should be image-specific and fine-grained. When\ninferring images, however, the static convolution kernels\nare limited in the ability to represent variables due to the\nlimited working memory [23, 24, 27]. So it is difficult\nfor CNNs to capture image-specific dependencies due to\ntheir content-independent parameters yet variable input im-\nage contents. (4) Lack of tools. Although there are already\nmany visualization techniques based on gradient or attribu-\ntion [19, 64, 49, 48, 21, 39, 69, 2], most of them focus on\nimage classification rather than localization. They aim to\nreveal class-specific input patterns or saliency maps rather\nthan to explain the relationships between structure variables\n(e.g., the locations of keypoints). By far, how to develop ex-\nplainable pose estimators remains challenging.\nIn this work, we aim to build a human pose estimator that\ncan explicitly capture and reveal the image-specific spatial\ndependencies between keypoints, as shown in Fig. 1. Due to\nthe poor scaling property of convolution [45], we argue that\nconvolution has advantages in extracting low-level features,\nbut deeply stacking convolutions at high-level to enlarge the\nreceptive field is not efficient to capture global dependen-\ncies. And such deepness increases the difficulty in interpret-\ning CNN predictions. Transformer architecture [58] has a\nnatural advantage over CNNs in terms of drawing pairwise\nor higher-order interactions. As shown in Fig. 2, attention\nlayers enable the model to capture interactions between any\npairwise locations, and its attention map acts as an immedi-\nate memory to store these dependencies.\nBased on these considerations, we propose a novel\nmodel called TransPose, using convolutions to extract fea-\ntures at low-level and Transformer to capture global de-\npendencies at high-level. In detail, we flatten the feature\nmaps as input to Transformer and recover its output into\nthe 2D-structure heatmaps. In such a design, the last atten-\ntion layer in Transformer specially acts as an aggregator,\nwhich collects different contributions from all image loca-\ntions by attention scores and finally forms the maximum\npositions in the heatmaps. This type of keypoint localiza-\ntion approach via Transformer establishes a connection with\nthe interpretability of Activation Maximization [19, 49] and\nextends it to the localization task. The resulting attention\nscores can indicate what concrete image clues significantly\ncontribute to the predicted locations. With such evidence,\nwe can further analyze the behaviors of the model by exam-\nining the influence of different experimental variables. In\nsummary, our contributions are as follow:\n• We introduce Transformer for human pose estimation\nto predict heatmap-based keypoints positions, which\ncan efficiently capture the spatial relationships be-\ntween human body parts.\n• We demonstrate that our keypoint localization ap-\nproach based on Transformer conforms to the inter-\npretability of Activation Maximization [19, 49]. Qual-\nitative analysis reveals the dependencies beyond intu-\nition, which are image-specific and fine-grained.\n• TransPose models achieve competitive performances\nagainst state-of-the-arts CNN-based models via fewer\nparameters and faster speeds. TransPose achieves 75.8\nAP and 75.0 AP on COCO validation set and test-dev\nset, with 73 % fewer parameters and 1.4 × faster than\nHRNet-W48. In addition, our model transfers very\nwell on MPII benchmark.\n2. Related Work\n2.1. Human Pose Estimation\nDeep CNNs have achieved great success in human pose\nestimation. The inductive biases of vanilla convolution ker-\nnel [31, 30] are locality and translation equivariance. It\nproves to be efficient to extract low-level image feature.\nFor human pose estimation, capturing global dependencies\nis crucial [46, 55, 60, 40], but the locality nature of con-\nvolution makes it impossible to capture long-range inter-\nactions. A typical but brute solution is to enlarge the re-\nceptive field, e.g. by downsampling the resolution, increas-\ning the depth or expanding the kernel size. Further, so-\nphisticated strategies are proposed such as multi-scale fu-\nsion [38, 43, 63, 12, 51, 15, 13], stacking [60, 61, 38],\nor high-resolution representation [51]; meanwhile, many\nsuccessful architectures have emerged such as CPM [60],\nHourglass Network [38], FPN [63], CPN [12], SimpleBase-\nline [61], HRNet [51], RSN [8], even automated architec-\ntures [62, 22, 37, 14, 68]. But as the architecture is becom-\ning more complex, it is more challenging but imperative\nthan ever to seek the interpretability of human pose esti-\nmation models. In contrast, our model can estimate human\npose in an efficient and explicit way.\n2.2. Explainability\nExplainability means a better understanding for human\nof how the model makes predictions. As surveyed by [47],\nmany works define the goal for explanation is to determine\nwhat inputs are the most relevant to the prediction, which is\nalso the goal we seek in this paper. [19, 32] perform gradi-\nent descent in the input space to find out what input patterns\ncan maximize a given unit. [49, 20] further consider gen-\nerating the image-specific class saliency maps. [64] uses\nDeConvNet to generate feature activities to show what con-\nvolutional layers have learned. Some pose estimation meth-\nods [32, 67] visualize the feature maps by choosing spe-\ncific neurons or channels but the results fail to reveal the\nspatial relationship between parts. [54] estimates the prob-\nability distributions and mutual information between key-\npoints, yet only revealing the statistic information rather\nthan image-specific explanations. There are also works like\nNetwork Dissection [3], Feature Visualization [39], Excita-\ntion Backprop [66], LRP attribution method [2], CAM [69],\nand Grad-CAM [48], which aim to explain the prediction of\nCNN classifier or visualize the saliency area significantly\naffecting the class. Different from most prior works, we\naim to reveal the fine-grained spatial dependencies between\nbody joints variables in the structural skeleton. And our\nmodel can directly exploit the attention patterns to holis-\ntically explain its predictions without the help of external\ntools. We also notice a recent paper [10] that develops LRP-\nbased [2] method to compute relevance to explain the pre-\ndictions of Transformer. It takes ViT model [18] to visualize\nclass-specific relevance map, showing reasonable results.\nUnlike their goal, we focus on revealing what clues con-\ntribute to visual keypoint localizations, and the attentions in\nour model provide clear evidence for the predictions.\nIt is worth noting that there are some works, such as Co-\nordConv [35] and Zero Padding [29], to explain how the\nneural network predicts the positions and stores the posi-\ntion information by designing proxy tasks. We also conduct\nexperiments to investigate the importance of position em-\nbedding for predicting the locations and its generalization\non unseen input scales.\n2.3. Transformer\nTransformer was proposed by Vaswani et al. [58]\nfor neural machine translation (NMT) task [53]. Large\nTransformer-based models like BERT [17], GPT-2 [44] are\noften pre-trained on large amounts of data and then fine-\ntuned for smaller datasets. Recently, Vision Transformer or\nattention-augmented layers have merged as new choices for\nvision tasks such as [42, 45, 5, 18, 57, 9, 11, 16, 70, 59].\nDETR [9] directly predicts a set of object instances by in-\ntroducing object queries. ViT [18] is to pre-train a pure\nTransformer on large data and then fine-tuned on ImageNet\nfor image classification. DeiT [57] introduces a distillation\ntoken to learn knowledge from a teacher. There are also\nworks [26, 28, 33] applying Transformers to 3D pose es-\ntimation. [26] fuses features from multi-view images by\nattention mechanism. [28, 33] output 1D sequences com-\nposed of joint/vertex coordinates of pose. Unlike them, we\nuse Transformer to predict the 2D heatmaps represented\nwith spatial distributions of keypoints for 2D human pose\nestimation problem.\n3. Method\nOur goal is to build a model that can explicitly capture\nglobal dependencies between human body parts. We first\ndescribe the model architecture. Then we show how it ex-\nploits self-attention to capture global interactions and estab-\nlish a connection between our method and the principle of\nActivation Maximization.\n3.1. Architecture\nAs illustrated in Fig. 3, TransPose model consists of\nthree components: a CNN backbone to extract low-level\nimage feature; a Transformer Encoder to capture long-range\nspatial interactions between feature vectors across the loca-\ntions; a head to predict the keypoints heatmaps.\nBackbone. Many common CNNs can be taken as the\nbackbone. For better comparisons, we choose two typical\nCNN architectures: ResNet [25] and HRNet [51]. We only\nretain the initial several parts of the original ImageNet pre-\ntrained CNNs to extract feature from images. We name\nthem ResNet-S and HRNet-S, the parameters numbers of\nwhich are only about 5.5% and 25% of the original CNNs.\nTransformer. We follow the standard Transformer ar-\nchitecture [58] as closely as possible. And only the En-\ncoder is employed, as we believe that the pure heatmaps\nprediction task is simply an encoding task, which com-\npresses the original image information into a compact po-\nsition representation of keypoints. Given an input image\nI ∈ R3×HI×WI , we assume that the CNN backbone out-\nputs a 2D spatial structure image feature Xf ∈ Rd×H×W\nwhose feature dimension has been transformed to d by a\n1×1 convolution. Then, the image feature map is flattened\ninto a sequence X ∈ RL×d, i.e., L d-dimensional feature\nvectors where L = H × W. It goes through N attention\nlayers and feed-forward networks (FFNs).\nHead. A head is attached to Transformer Encoder out-\nput E ∈ RL×d to predict K types of keypoints heatmaps\nP ∈ RK×H∗×W∗\nwhere H∗, W∗ = HI/4, WI/4 by de-\nfault. We firstly reshape E back to Rd×H×W shape. Then\nwe mainly use a 1×1 convolution to reduce the channel di-\nmension of E from d to K. If H, Ware not equal H∗, W∗,\nan additional bilinear interpolation or a 4 ×4 transposed\nconvolution is used to do upsampling before 1 ×1 convo-\nlution. Note, a 1×1 convolution is completely equivalent to\na position-wise linear transformation layer.\nBackbone Head\nTransformer Encoder Layer ×N\nDependencies\nAttention\nAdd\n&\nNorm\nPosition-Wise\nFFN\nAdd\n&\nNorm\nQ\nK\nV\nAttention Maps\nAttention Layer #1\nAttention Layer #2\nAttention Layer #N…\nM\nM\nS\nActivation Maximum Positions\nKeypoint HeatmapsInput Image\nDifferent Types of Keypoints\nForward Explain\nM MatMul\nS Scale & Softmax\nPosition \nEmbedding\nFigure 3. The architecture. Firstly, the feature maps are extracted by a CNN backbone and flattened into a sequence. Next, the Transformer\nencode layers iteratively capture dependencies from the sequences by query-key-value attention. Then, a simple head is used to predict\nthe keypoints heatmaps. The attention map in Transformer can reveal what dependencies (regions or joints) significantly contribute to the\nactivation maximum positions in the predicted keypoint heatmaps.\n3.2. Resolution Settings.\nDue to that the computational complexity of per self-\nattention layer is O\n\u0000\n(HW )2 · d\n\u0001\n, we restrict the attention\nlayers to operate at a resolution withr× downsampling rate\nw.r.t. the original input, i.e., H, W = HI/r, WI/r. In\nthe common human pose estimation architectures [60, 38,\n61, 51], 32× downsampling is usually adopted as a stan-\ndard setting to obtain a very low-resolution map contain-\ning global information. In contrast, we adopt r = 8 and\nr = 4setting for ResNet-S and HRNet-S, which are bene-\nficial to the trade-off between the memory footprint for at-\ntention layers and the loss in detailed information. As a re-\nsult, our model directly captures long-range interactions at\na higher resolution, while preserving the fine-grained local\nfeature information.\n3.3. Attentions are the Dependencies of Localized\nKeypoints\nSelf-Attention mechanism. The core mechanism of\nTransformer [58] is multi-head self-attention. It first\nprojects an input sequence X ∈ RL×d into queries Q ∈\nRL×d, keys K ∈ RL×d and values V ∈ RL×d by three\nmatrices Wq, Wk, Wv ∈ Rd×d. Then, the attention scores\nmatrix2 A ∈ RN×N is computed by:\nA = softmax\n\u0012QK⊤\n√\nd\n\u0013\n. (1)\nEach query qi ∈ Rd of the token xi ∈ Rd (i.e., feature vec-\ntor at location i) computes similarities with all the keys to\nachieve a weight vector wi = Ai,: ∈ R1×L, which deter-\nmines how much dependency is needed from each token in\n2Here we consider single-head self attention. For multi-head self-\nattention, the attention matrix is the average of attention maps in all heads.\nthe previous sequence. Then an increment is achieved by a\nlinear sum of all elements in Value matrixV with the corre-\nsponding weight in wi and added to xi. By doing this, the\nattention maps can be seen as dynamic weights that deter-\nmined by specific image content, reweighting the informa-\ntion flow in the forward propagation.\nSelf-attention captures and reveals how much contribu-\ntion the predictions aggregate from each image location.\nSuch contributions from different image locations can be re-\nflected by the gradient [49, 2, 48]. Therefore, we concretely\nanalyze how xj at image/sequence location j affects the ac-\ntivation hi at location i the predicted keypoint heatmaps,\nby computing the derivative of hi ∈ RK (K types of key-\npoints) w.r.t thexj at location j of the input sequence of the\nlast attention layer. And we further assume G := ∂hi\n∂xj\nas a\nfunction w.r.t. a given attention score Ai,j. We obtain:\nG (Ai,j) ≈ Ai,j · Wf · W⊤\nv + Wf = Ai,j · K + B\n(2)\nwhere K, B ∈ RK×d are static weights (fixed when infer-\nring) and shared across all image locations. The derivations\nof Eq. 2 are shown in supplementary. We can see that the\nfunction G is approximately linear with Ai,j, i.e., the de-\ngrees of contribution to the prediction hi directly depend\non its attention scores at image locations.\nEspecially, the last attention layer acts as an aggrega-\ntor, which collects contributions from all image locations\naccording to attentions and forms the maximum activations\nin the predicted keypoint heatmaps. Although the layers\nin FFN and head cannot be ignored, they are position-\nwise, which means they approximately linearly transform\nthe contributions from all locations by the same transfor-\nmation without changing their relative proportions.\nThe activation maximum positions are the keypoints’\nlocations. The interpretability of Activation Maximization\nModel Name Backbone Downsampling for Attention Upsampling #Layers Heads d h #Params\nTransPose-R-A3* ResNet-Small* 1/8 Bilinear Interpolation 3 8 256 512 5.0M\nTransPose-R-A3 ResNet-Small 1/8 Deconvolution 3 8 256 1024 5.2M\nTransPose-R-A4 ResNet-Small 1/8 Deconvolution 4 8 256 1024 6.0M\nTransPose-H-S HRNet-Small-W32 1/4 None 4 1 64 128 8.0M\nTransPose-H-A4 HRNet-Small-W48 1/4 None 4 1 96 192 17.3M\nTransPose-H-A6 HRNet-Small-W48 1/4 None 6 1 96 192 17.5M\nTable 1. Architecture configurations for different TransPose models. More details about the backbones are described in supplementary.\nMethod Input Size AP AR #Params FLOPs FPS\nSimpleBaseline-Res50 [61] 256×192 70.4 76.3 34.0M 8.9G 114\nSimpleBaseline-Res101 [61] 256×192 71.4 76.3 53.0M 12.4G 92\nSimpleBaseline-Res152 [61] 256×192 72.0 77.8 68.6M 35.3G 62\nTransPose-R-A3* 256×192 71.5 76.9 5.0M (↓85%) 5.4G 137 (↑20%)\nTransPose-R-A3 256×192 71.7 77.1 5.2M (↓85%) 8.0G 141 (↑23%)\nTransPose-R-A4 256×192 72.6 78.0 6.0M (↓82%) 8.9G 138 (↑21%)\nHRNet-W32 [51] 256×192 74.4 79.8 28.5M 7.2G 28\nHRNet-W48 [51] 256×192 75.1 80.4 63.6M 14.6G 27\nTransPose-H-S 256×192 74.2 78.0 8.0M (↓72%) 10.2G 45 (↑61%)\nTransPose-H-A4 256×192 75.3 80.3 17.3M (↓73%) 17.5G 41 (↑52%)\nTransPose-H-A6 256×192 75.8 80.8 17.5M (↓73%) 21.8G 38 (↑41%)\nTable 2. Results on COCO validation set, all provided with the same detected human boxes. TransPose-R-* and TransPose-H-* achieve\ncompetitive results to SimpleBaseline and HRNet, with fewer parameters and faster speeds. The reported FLOPs of SimpleBaseline and\nHRNet only include the convolution and linear layers.\n(AM) [19, 49] lies in: the input region which can maximize\na given neuron activation can explain what this activated\nneuron is looking for.\nIn this task, the learning target of TransPose is to expect\nthe neuron activation hi∗ at location i∗ of the heatmap to\nbe maximally activated where i∗ represents the groundtruth\nlocation of a keypoint:\nθ∗ = arg max\nθ\nhi∗ (θ, I). (3)\nAssuming the model has been optimized with parameters\nθ∗ and it predicts the location of a particular keypoint as i\n(maximum position in a heatmap), why the model predicts\nsuch prediction can be explained by the fact that those lo-\ncations J, whose element j has higher attention score (≥ δ)\nwith i, are the dependencies that significantly contribute to\nthe prediction. The dependencies can be found by:\nJ = {j|Ai,j (θ∗, I) ≥ δ}, (4)\nwhere A ∈ RL×L is the attention map of the last attention\nlayer and also a function w.r.tθ∗ and I, i.e., A = A (θ∗, I).\nGiven an image I and a query location i, Ai,: can reveal\nwhat dependencies a predicted location i highly relies on,\nwe define it dependency area. A:,j can reveal what area a\nlocation j mostly affects, we define it affected area.\nFor the traditional CNN-based methods, they also use\nheatmap activations as the keypoint locations, but one can-\nnot directly find the explainable patterns for the predictions\ndue to the deepness and highly non-linearity of deep CNNs.\nThe AM-based methods [19, 32, 64, 49] may provide in-\nsights while they require extra optimization costs to learn\nexplainable patterns the convolutional kernels prefer to look\nfor. Different from them, we extend AM to heatmap-based\nlocalization via Transformer, and we do not need extra opti-\nmization costs because the optimization has been implicitly\naccomplished in our training, i.e., A = A (θ∗, I). The de-\nfined dependency area is the pattern we seek, which can\nshow image-specific and keypoint-specific dependencies.\n4. Experiments\nDataset. We evaluate our models on COCO [34] and\nMPII [1] datasets. COCO contains 200k images in the wild\nand 250k person instances. Train2017 consists of 57k im-\nages and 150k person instances. Val2017 set contains 5k\nimages and test-dev2017 consists of 20k images. In Sec 4.2,\nwe show the experiments on MPII [1]. And we adopt the\nstandard evaluation metrics of these benchmarks.\nTechnical details. We follow the top-down human pose\nestimation paradigm. The training samples are the cropped\nimages with single person. We resize all input images into\n256 × 192 resolution. We use the same training strate-\ngies, data augmentation and person detected results as [51].\nWe also adopt the coordinate decoding strategy proposed\nby [65] to reduce the quantisation error when decoding\nfrom downscaled heatmaps. The feed forward layers are\ntrained with 0.1 dropout and ReLU activate function. Next,\nwe name the models based on ResNet-S and HRNet-S\nTransPose-R and TransPose-H, abbreviated as TP-R and\nTP-H. The architecture details are reported in Tab. 1. We\nuse Adam optimizer for all models. Training epochs are\n230 for TP-R and 240 for TP-H. The cosine annealing learn-\ning rate decay is used. The learning rates for TP-R-A4 and\nMethod Input size #Params FLOPs FPS AP AP0.5 AP0.75 APM APL\nG-RMI [41] 353×257 42.6M 57G - 64.9 85.5 71.3 62.3 70.0\nIntegral [52] 256×256 45.0M 11.0G - 67.8 88.2 74.8 63.9 74.0\nCPN [12] 384×288 58.8M 29.2G - 72.1 91.4 80.0 68.7 77.2\nRMPE [20] 320×256 28.1M 26.7G - 72.3 89.2 79.1 68.0 78.6\nSimpleBaseline [61] 384×288 68.6M 35.6G - 73.7 91.9 81.1 70.3 80.0\nHRNet-W32 [51] 384×288 28.5M 16.0G 26 74.9 92.5 82.8 71.3 80.9\nHRNet-W48 [51] 256×192 63.6M 14.6G 27 74.2 92.4 82.4 70.9 79.7\nHRNet-W48 [51] 384×288 63.6M 32.9G 25 75.5 92.5 83.3 71.9 81.5\nDarkPose [65] 384×288 63.6M 32.9G 25 76.2 92.5 83.6 72.5 82.4\nTransPose-H-S 256×192 8.0M 10.2G 45 73.4 91.6 81.1 70.1 79.3\nTransPose-H-A4 256×192 17.3M 17.5G 41 74.7 91.9 82.2 71.4 80.7\nTransPose-H-A6 256×192 17.5M 21.8G 38 75.0 92.2 82.3 71.3 81.1\nTable 3. Comparisons with state-of-the-art CNN-based models on\nCOCO test-dev set. Tested on smaller input resolution 256×192 ,\nour models achieve comparable performances with the others.\nTP-H-A6 models decay from 0.0001 to 0.00001, we recom-\nmend using such a schedule for all models. Considering the\ncompatibility with backbone and the memory consumption,\nwe adjust the hyperparameters of Transformer encoder to\nmake the model capacity not very large. In addition, we use\n2D sine position embedding as the default position embed-\nding. We describe it in the supplementary.\n4.1. Results on COCO keypoint detection task\nWe compare TransPose with SimpleBaseline, HRNet,\nand DARK [65]. Specially, we trained the DARK-Res50 on\nour machines according to the official code with TransPose-\nR-A4’s data augmentation, we achieve 72.0AP; when us-\ning the totally same data augmentation and long training\nschedule of TransPose-R-A4 for it, we obtain 72.1AP (+0.1\nAP). The other results showed in Tab. 2 come from the\npapers. We test all models on a single NVIDIA 2080Ti\nGPU with the same experimental conditions to compute\nthe average FPS. Under the input resolution – 256 ×192,\nTransPose-R-A4 and TransPose-H-A6 have obviously over-\nperformed SimpleBaseline-Res152 (+0.6AP) [61], HRNet-\nW48 (+0.7AP) [51] and DARK-HRNet [65] (+0.2AP), with\nsignificantly fewer model parameters and faster speeds.\nTab. 3 shows the results on COCO test set.\nPosition Embedding #Params FLOPs AP\n✗ 4.999M 7.975G 70.4\nLearnable 5.195M 7.976G 70.9\n2D Sine (Fixed) 5.195M 7.976G 71.7\nTable 4. Results for different position embedding schemes for\nTransPose models. The input size is 256 × 192.\n4.2. Transfer to MPII benchmark\nTypical pose estimation methods often separately train\nand evaluate their models on COCO and MPII [1]. Mo-\ntivated by the success of pre-training in NLP and recent\nViT [18], we try to transfer our pre-trained models to\nMPII. We replace the final layer of the pre-trained Trans-\nPose model with a uniform-initialized d × 16 linear layer\nFigure 4. Performances on validation set when fine-tuning models\n(listed in Tab. 5) with different epochs on MPII training set.\nModels Strategy Epochs Mean@0.5 Mean@0.1 #Params\nDARK-HRNet [65] ⟲ 210 90.6 42.0 28.5M\n⇒ 100 92.0 (+1.4) 43.6 (+1.6) 28.5M\nTransPose-R-A4 ⟲ 230 89.3 38.6 6.0M\n⇒↑ 100 92.0 ( +2.7) 44.1 ( +5.5) 6.0M\nTransPose-H-A6 ⟲ 230 90.3 41.6 17.5M\n⇒ 100 92.3 (+2.0) 44.4 (+2.8) 17.5M\nTable 5. Fine-tuning and full-training performances on MPII val-\nidation set. ⟲ means full-training on MPII without COCO pre-\ntraining. ⇒ means transferring the pretrained model and fine-\ntuning on MPII; adding ↑ means fine-tuning MPII on input res-\nolution 384×384 otherwise 256×256.\nMethod Input size Training Data Mean@0.5\nBelagiannis & Zisserman, FG’17 [4] 248×248 COCO+MPII † 88.1\nSu et al., arXiv’19 [50] 384×384 HSSK+MPII ‡ 93.9\nBulat et al., FG’20 [7] 256×256 HSSK+MPII ‡ 94.1\nBin et al., ECCV’20 [6] 384×384 HSSK+MPII ‡ 94.1\nOurs (TransPose-H-A6) 256×256 COCO+MPII † 93.5\nTable 6. Results on MPII benchmark test set.† means pre-training\non COCO dataset and fine-tuning on MPII dataset. ‡ means train-\ning both on MPII and HSSK datasets.\nfor MPII. When fine-tuning, the learning rates for the pre-\ntrained and final layers are 1e-5 and 1e-4 with decay.\nFor comparisons, we fine-tune the pre-trained DARK-\nHRNet on MPII with the same settings, and train these mod-\nels on MPII by standard full-training settings. As shown\nin Tab. 5 and Fig. 4, the results are interesting: even with\nlonger full-training epochs, models perform worse than the\nfine-tuned ones; even with large model capacity (28.5M),\nthe improvement (+1.4 AP) brought by pre-training DARK-\nHRNet is smaller than pre-training TransPose (+2.0 AP).\nWith 256 ×256 input resolution and fine-tuning on MPII\ntrain and val sets, the best result on MPII test set yielded\nby TransPose-H-A6 is 93.5% accuracy, as shown in Fig. 6.\nThese results show that pre-training and fine-tuning could\nsignificantly reduce training costs and improve the perfor-\nmances, particularly for the pre-trained TransPose models.\nDiscussion. The pre-training and fine-tuning for\nTransformer-based models have shown favorable results in\nNLP [17, 44] and recent vision models [18, 11, 16]. Our ini-\ntial results on MPII also suggest that training Transformer-\nbased models on large-scale pose-related data may be a\n74.3 72.6 73.1 73.8\n7.9 3.9 14.4 31.6\n62.7 51.7 66.6 70.3\n39.9 21.6\n49.9 59.1\n020406080AP (OKS) /%\n256x192(training&testing)128x96(testing)384x288(testing)512x384(testing)\nSimpleBaselineTransPosew/o PETransPosew/ LPETransPosew/ Sine PE\nFigure 5. Performances on unseen input resolutions. TransPose\nmodels w/ Position Embedding generalize better.\npromising way to learn powerful and robust representation\nfor human pose estimation and its downstream tasks.\n4.3. Ablations\nThe importance of position embedding. Without posi-\ntion embedding, the 2D spatial structure information loses\nin Transformer. To explore its importance, we conduct ex-\nperiments on TransPose-R-A3 models with three position\nembedding strategies: 2D sine position embedding, learn-\nable position embedding, and w/o position embedding. As\nexpected, the models with position embedding perform bet-\nter, particularly for 2D sine position embedding, as shown\nin Tab. 4. But interestingly, TransPose w/o any position em-\nbedding only loses 1.3 AP, which suggests that 2D-structure\nbecomes less important. See more details in supplementary.\nScaling the Size of Transformer Encoder. We study\nhow performance scales with the size of Transformer En-\ncoder, as shown in Tab. 7. For TransPose-R models, with\nthe number of layers increasing to 6, the performance im-\nprovements gradually tend to saturate or degenerate. But\nwe have not observed such a phenomenon on TransPose-H\nmodels. Scaling the Transformer obviously improves the\nperformance of TransPose-H.\nPosition embedding helps to generalize better on un-\nseen input resolutions. The top-down paradigm scales all\nthe cropped images to a fixed size. But for some cases even\nwith a fixed input size or the bottom-up paradigm, the body\nsize in the input varies; the robustness to different scales\nbecomes important. So we design an extreme experiment\nto test the generalization: we test SimpleBaseline-ResN50-\nDark and TransPose-R-A3 models on unseen 128 ×96,\n384×288, 512 ×388 input resolutions, all of which only\nhave been trained with 256 ×192 size. Interestingly, the\nresults in Fig. 5 demonstrate that SimpleBaseline and\nTransPose-R w/o position embedding have obvious per-\nformance collapses on unseen resolutions, particularly on\n128×96; but TransPose-R with learnable or 2D Sine posi-\ntion embedding have significantly better generalization, es-\npecially for 2D Sine position embedding.\nDiscussion. For the input resolution, we mainly trained\nour models on 256×192 size, thus 768 and 3072 sequence\nlengths for Transformers in TP-R and TP-H models. Higher\ninput resolutions such as 384 ×288 for our current models\nwill bring prohibitively expensive computational costs in\nself-attention layers due to the quadratic complexity.\nModel #Layers d h #Params FLOPs FPS AP AR\nTransPose-R\n2 256 1024 4.4M 7.0G 174 69.6 75.0\n3 256 1024 5.2M 8.0G 141 71.7 77.1\n4 256 1024 6.0M 8.9G 138 72.6 78.0\n5 256 1024 6.8M 9.9G 126 72.2 77.6\n6 256 1024 7.6M 10.8G 109 72.2 77.5\nTransPose-H\n4 64 128 17.0M 14.6G - 75.1 80.1\n4 192 384 18.5M 27.0G - 75.4 80.5\n4 96 192 17.3M 17.5G 41 75.3 80.3\n5 96 192 17.4M 19.7G 40 75.6 80.6\n6 96 192 17.5M 21.8G 38 75.8 80.8\nTable 7. Ablation study on the size of Transformer Encoder. #Lay-\ners, d and h are the number of encoder layers, the dimensions d,\nand the number of hidden units of FFN.\n4.4. Qualitative Analysis\nThe hyperparameter configurations for TransPose model\nmight affect the its behavior in an unknown way. In this\nsection, we choose trained models, types of predicted key-\npoints, depths of attention layers, and input images as con-\ntrolled variables to observe the model behaviors.\nThe dependency preferences are different for mod-\nels with different CNN extractors. To make comparisons\nbetween ResNet-S and HRNet-S based models, we use the\ntrained models TP-R-A4 and TP-H-A4 performances as ex-\nemplars. Illustrated in Fig. 6, we choose two typical inputs\nA and B as examples and visualize the dependency areas\ndefined in Sec. 3.3. We find that although the predictions\nfrom TP-R-A4 and TP-H-A4 are exactly the same loca-\ntions of keypoints, TP- H-A4 can exploit multiple longer-\nrange joints clues to predict keypoints. In contrast, TP- R-\nA4 prefers to attend to local image cues around the target\njoint. This characteristic can be further confirmed by the\nvisualized affected areas in supplementary, in which key-\npoints have larger and non-local affected areas in TP-H-A4.\nAlthough such results are not as commonly expected, they\nreflect: 1) a pose estimator uses global information from\nlong-range joints to localize a particular joint; 2) HRNet-S\nis better than ResNet-S at capturing long-range dependency\nrelationships information (probably due to its multi-scale\nfusion scheme).\nDependencies and influences vary for different types\nof keypoints. For keypoints in the head, localizing them\nmainly relies on visual clues from head, but TP- H-A4 also\nassociates them with shoulders and the joints of arms. No-\ntably, the dependencies of predicting wrists, elbows, knees\nor ankles have obvious differences for two models, in which\nTP-R-A4 depends on the local clues at the same side while\nTP-H-A4 exploits more clues from the joints on the sym-\nmetrical side. As shown in Fig. 6(b), Fig. 6(d), and Fig. 7,\nwe can further observe that a pose estimator might gather\nstrong clues from more parts to predict the target keypoint.\nThis can explain why the model still can predict the loca-\ntion of an occluded keypoint accurately, and the occluded\nkeypoint with ambiguity location will have less impact on\nthe other predictions or larger uncertain area to rely on (e.g.\nthe occluded left ankle – last map of Fig. 6(c) or Fig. 6(d)).\n(a) TP-R-A4: predicted keypoints and their dependency areas for input A.\n (b) TP-H-A4: predicted keypoints and their dependency areas for input A.\n(c) TP-R-A4: predicted keypoints and their dependency areas for input B.\n (d) TP-H-A4: predicted keypoints and their dependency areas for input B.\nFigure 6. Predicted locations and the dependency areas for different types of keypoints by different models: TP-R-A4 (left column) and\nTP-H-A4 (right column). In each sub-figure, the first one is the original input image plotted with predicted skeleton. The other maps\nvisualized by the defined dependency area ( Ai,:) of the attention matrix in the last layer with a threshold value (0.00075). The predicted\nlocation of a keypoint is annotated by a WHITE color pentagram (⋆) in each sub-map. Redder area indicates higher attention scores.\nEnc.Att.\nLayer 0\nEnc.Att.\nLayer 1\nEnc.Att.\nLayer 2\nEnc.Att.\nLayer 3\nnose\n eye(l)\n eye(r)\n ear(l)\n ear(r)\n sho.(l)\n sho.(r)\n elb.(l)\n elb.(r)\n wri.(l)\n wri.(r)\n hip(l)\n hip(r)\n kne.(l)\n kne.(r)\n ank.(l)\n ank.(r)\n random\n random\n0.0\n(a) TP-R-A4: predictions and dependency areas for Input C.\nEnc.Att.\nLayer 0\nEnc.Att.\nLayer 1\nEnc.Att.\nLayer 2\nEnc.Att.\nLayer 3\nnose\n eye(l)\n eye(r)\n ear(l)\n ear(r)\n sho.(l)\n sho.(r)\n elb.(l)\n elb.(r)\n wri.(l)\n wri.(r)\n hip(l)\n hip(r)\n kne.(l)\n kne.(r)\n ank.(l)\n ank.(r)\n random\n random\n0.0 (b) TP-H-A4: predictions and dependency areas for Input C.\nFigure 7. Dependency areas for the particular positions in the different attention layers by the same visualization method of Fig.6.\nAttentions gradually focus on more fine-grained de-\npendencies with the depth increasing. Observing all of\nattention layers (the 1,2,3-th rows of Fig. 7), we surprisingly\nfind that even without the intermediate GT locations super-\nvision, TP-H-A4 can still attend to the accurate locations of\njoints yet with more global cues in the early attention layers.\nFor both models, with the depth increasing, the predictions\ngradually depend on more fine-grained image clues around\nlocal parts or keypoints positions (Fig. 7).\nImage-specific dependencies and statistical common-\nalities for a single model. Different from the static re-\nlationships encoded in the weights of CNN after training,\nthe attention maps are dynamic to inputs. As shown in\nFig. 6(a) and Fig. 6(c), we can observe that despite the sta-\ntistical commonalities on the dependency relationships for\nthe predicted keypoints (similar behaviors for most com-\nmon images), the fine-grained dependencies would slightly\nchange according to the image context. With the existence\nof occlusion or invisibility in a given image such as input\nB (Fig. 6(c)), the model can still localize the position of the\npartially obscured keypoint by looking for more significant\nimage clues and reduces reliance on the invisible keypoint\nto predict the other ones. It is likely that future works can\nexploit such attention patterns for parts-to-whole associa-\ntion and aggregating relevant features for 3D pose estima-\ntion or action recognition.\n5. Conclusion\nWe explored a model – TransPose – by introducing\nTransformer for human pose estimation. The attention lay-\ners enable the model to capture global spatial dependen-\ncies efficiently and explicitly. And we show that such a\nheatmap-based localization achieved by Transformer makes\nour model share the idea with Activation Maximization.\nWith lightweight architectures, TransPose matches state-of-\nthe-art CNN-based counterparts on COCO and gains sig-\nnificant improvements on MPII when fine-tuned with small\ntraining costs. Furthermore, we validate the importance of\nposition embedding. Our qualitative analysis reveals the\nmodel behaviors that are variable for layer depths, keypoints\ntypes, trained models and input images, which also gives us\ninsights into how models handle special cases such as oc-\nclusion.\nAcknowledgment. This work was supported by the Na-\ntional Natural Science Foundation of China (61773117 and\n62006041).\nReferences\n[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter\nGehler, and Bernt Schiele. 2d human pose estima-\ntion: New benchmark and state of the art analysis. In\nCVPR, pages 3686–3693, 2014. 5, 6\n[2] Sebastian Bach, Alexander Binder, Gr ´egoire Mon-\ntavon, Frederick Klauschen, Klaus-Robert M¨uller, and\nWojciech Samek. On pixel-wise explanations for\nnon-linear classifier decisions by layer-wise relevance\npropagation. PloS one, 10(7):e0130140, 2015. 2, 3, 4,\n13\n[3] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva,\nand Antonio Torralba. Network dissection: Quantify-\ning interpretability of deep visual representations. In\nCVPR, pages 3319–3327, 2017. 3\n[4] Vasileios Belagiannis and Andrew Zisserman. Recur-\nrent human pose estimation. In FG, pages 468–475.\nIEEE, 2017. 6\n[5] Irwan Bello, Barret Zoph, Quoc Le, Ashish Vaswani,\nand Jonathon Shlens. Attention augmented convolu-\ntional networks. In ICCV, pages 3286–3295, 2019. 3\n[6] Yanrui Bin, Xuan Cao, Xinya Chen, Yanhao Ge, Ying\nTai, Chengjie Wang, Jilin Li, Feiyue Huang, Changxin\nGao, and Nong Sang. Adversarial semantic data aug-\nmentation for human pose estimation. InECCV, pages\n606–622. Springer, 2020. 6\n[7] Adrian Bulat, Jean Kossaifi, Georgios Tzimiropoulos,\nand Maja Pantic. Toward fast and accurate human\npose estimation via soft-gated skip connections. arXiv\npreprint arXiv:2002.11098, 2020. 6\n[8] Yuanhao Cai, Zhicheng Wang, Zhengxiong Luo,\nBinyi Yin, Angang Du, Haoqian Wang, Xinyu Zhou,\nErjin Zhou, Xiangyu Zhang, and Jian Sun. Learning\ndelicate local representations for multi-person pose es-\ntimation. In ECCV, 2020. 2\n[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with trans-\nformers. In ECCV, pages 213–229, Cham, 2020. 3,\n12\n[10] Hila Chefer, Shir Gur, and Lior Wolf. Transformer\ninterpretability beyond attention visualization. arXiv\npreprint arXiv:2012.09838, 2020. 3\n[11] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu,\nYiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,\nChao Xu, and Wen Gao. Pre-trained image processing\ntransformer. arXiv preprint arXiv:2012.00364, 2020.\n3, 6\n[12] Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang\nZhang, Gang Yu, and Jian Sun. Cascaded pyramid\nnetwork for multi-person pose estimation. In CVPR,\npages 7103–7112, 2018. 1, 2, 6\n[13] Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui\nShi, Thomas S. Huang, and Lei Zhang. Higherhrnet:\nScale-aware representation learning for bottom-up hu-\nman pose estimation. In CVPR, June 2020. 2\n[14] Hsin-Pai Cheng, Feng Liang, Meng Li, Bowen Cheng,\nFeng Yan, Hai Li, Vikas Chandra, and Yiran Chen.\nScalenas: One-shot learning of scale-aware repre-\nsentations for visual recognition. arXiv preprint\narXiv:2011.14584, 2020. 2\n[15] Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma,\nAlan L. Yuille, and Xiaogang Wang. Multi-context\nattention for human pose estimation. In CVPR, pages\n5669–5678, 2017. 2\n[16] Zhigang Dai, Bolun Cai, Yugeng Lin, and Juny-\ning Chen. Up-detr: Unsupervised pre-training for\nobject detection with transformers. arXiv preprint\narXiv:2011.09094, 2020. 3, 6\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv\npreprint arXiv:1810.04805, 2018. 3, 6\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. An image is worth\n16x16 words: Transformers for image recognition at\nscale. arXiv preprint arXiv:2010.11929, 2020. 3, 6\n[19] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and\nPascal Vincent. Visualizing higher-layer features of a\ndeep network. Technical Report, University of Mon-\ntreal, 1341(3):1, 2009. 1, 2, 3, 5\n[20] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu\nLu. Rmpe: Regional multi-person pose estimation. In\nICCV, pages 2334–2343, 2017. 3, 6\n[21] Ruth C. Fong and Andrea Vedaldi. Interpretable ex-\nplanations of black boxes by meaningful perturbation.\nIn ICCV, pages 3449–3457, 2017. 2\n[22] Xinyu Gong, Wuyang Chen, Yifan Jiang, Ye Yuan, Xi-\nanming Liu, Qian Zhang, Yuan Li, and Zhangyang\nWang. Autopose: Searching multi-scale branch\naggregation for pose estimation. arXiv preprint\narXiv:2008.07018, 2020. 2\n[23] Alex Graves, Greg Wayne, and Ivo Danihelka. Neu-\nral turing machines. arXiv preprint arXiv:1410.5401,\n2014. 2\n[24] Alex Graves, Greg Wayne, Malcolm Reynolds,\nTim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwi´nska, Sergio G ´omez Colmenarejo, Edward\nGrefenstette, Tiago Ramalho, John Agapiou, et al. Hy-\nbrid computing using a neural network with dynamic\nexternal memory. Nature, 538(7626):471–476, 2016.\n2\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. In\nCVPR, pages 770–778, 2016. 3, 13\n[26] Yihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-\nI Yu. Epipolar transformers. In CVPR, pages 7779–\n7788, 2020. 3\n[27] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-\nterm memory. Neural computation, 9(8):1735–1780,\n1997. 2\n[28] Lin Huang, Jianchao Tan, Ji Liu, and Junsong\nYuan. Hand-transformer: non-autoregressive struc-\ntured modeling for 3d hand pose estimation. InECCV,\npages 17–33. Springer, 2020. 3\n[29] Amirul Islam, Sen Jia, and Neil D. B. Bruce. How\nmuch position information do convolutional neural\nnetworks encode. In ICLR, 2020. 3\n[30] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-\nton. Imagenet classification with deep convolutional\nneural networks. In NeurIPS, pages 1097–1105, 2012.\n2\n[31] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner.\nGradient-based learning applied to document recog-\nnition. Proceedings of the IEEE , 86(11):2278–2324,\n1998. 2\n[32] Sijin Li, Zhi-Qiang Liu, and Antoni B. Chan. Hetero-\ngeneous multi-task learning for human pose estima-\ntion with deep convolutional neural network. IJCV,\n113(1):19–36, 2015. 3, 5\n[33] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end\nhuman pose and mesh reconstruction with transform-\ners. In CVPR, pages 1954–1963, 2021. 3\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and\nC Lawrence Zitnick. Microsoft coco: Common ob-\njects in context. In ECCV, pages 740–755. Springer,\n2014. 5\n[35] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Pet-\nroski Such, Eric Frank, Alex Sergeev, and Jason\nYosinski. An intriguing failing of convolutional neu-\nral networks and the coordconv solution. In NeurIPS,\npages 9605–9616, 2018. 3\n[36] Jonathan Long, Evan Shelhamer, and Trevor Darrell.\nFully convolutional networks for semantic segmenta-\ntion. In CVPR, pages 3431–3440, 2015. 1\n[37] William McNally, Kanav Vats, Alexander Wong, and\nJohn McPhee. Evopose2d: Pushing the boundaries\nof 2d human pose estimation using neuroevolution.\narXiv preprint arXiv:2011.08446, 2020. 2\n[38] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked\nhourglass networks for human pose estimation. In\nECCV, pages 483–499. Springer, 2016. 1, 2, 4\n[39] Chris Olah, Alexander Mordvintsev, and Ludwig\nSchubert. Feature visualization. Distill, 2017.\nhttps://distill.pub/2017/feature-visualization. 2, 3\n[40] George Papandreou, Tyler Zhu, Liang-Chieh Chen,\nSpyros Gidaris, Jonathan Tompson, and Kevin Mur-\nphy. Personlab: Person pose estimation and instance\nsegmentation with a bottom-up, part-based, geometric\nembedding model. In ECCV, 2018. 1, 2\n[41] George Papandreou, Tyler Zhu, Nori Kanazawa,\nAlexander Toshev, Jonathan Tompson, Chris Bregler,\nand Kevin Murphy. Towards accurate multi-person\npose estimation in the wild. In CVPR, pages 4903–\n4911, 2017. 6\n[42] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit,\nLukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In ICML, 2018. 3,\n12\n[43] Tomas Pfister, James Charles, and Andrew Zisser-\nman. Flowing convnets for human pose estimation in\nvideos. In ICCV, pages 1913–1921, 2015. 2\n[44] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language mod-\nels are unsupervised multitask learners. OpenAI blog,\n1(8):9, 2019. 3, 6\n[45] Prajit Ramachandran, Niki Parmar, Ashish Vaswani,\nIrwan Bello, Anselm Levskaya, and Jonathon Shlens.\nStand-alone self-attention in vision models. In NIPS,\npages 68–80, 2019. 2, 3\n[46] Varun Ramakrishna, Daniel Munoz, Martial Hebert,\nJames Andrew Bagnell, and Yaser Sheikh. Pose ma-\nchines: Articulated pose estimation via inference ma-\nchines. volume 8690, pages 33–47, 2014. 2\n[47] Wojciech Samek, Gr ´egoire Montavon, Andrea\nVedaldi, Lars Kai Hansen, and Klaus-Robert M ¨uller.\nExplainable AI: interpreting, explaining and visualiz-\ning deep learning , volume 11700. Springer Nature,\n2019. 3, 13\n[48] Ramprasaath R Selvaraju, Michael Cogswell, Ab-\nhishek Das, Ramakrishna Vedantam, Devi Parikh, and\nDhruv Batra. Grad-cam: Visual explanations from\ndeep networks via gradient-based localization. In\nICCV, pages 618–626, 2017. 2, 3, 4, 13\n[49] Karen Simonyan, Andrea Vedaldi, and Andrew Zis-\nserman. Deep inside convolutional networks: Visual-\nising image classification models and saliency maps.\nIn ICLR (Workshop Poster), 2013. 2, 3, 4, 5, 13\n[50] Zhihui Su, Ming Ye, Guohui Zhang, Lei Dai, and\nJianda Sheng. Cascade feature aggregation for human\npose estimation. arXiv preprint arXiv:1902.07837 ,\n2019. 6\n[51] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang.\nDeep high-resolution representation learning for hu-\nman pose estimation. In CVPR, June 2019. 1, 2, 3, 4,\n5, 6, 15\n[52] Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang,\nand Yichen Wei. Integral human pose regression. In\nECCV, pages 529–545, 2018. 6\n[53] Ilya Sutskever, Oriol Vinyals, and Quoc V . Le. Se-\nquence to sequence learning with neural networks. In\nNIPS, volume 27, pages 3104–3112, 2014. 3\n[54] Wei Tang and Ying Wu. Does learning specific fea-\ntures for related parts help human pose estimation? In\nCVPR, June 2019. 3\n[55] Jonathan J Tompson, Arjun Jain, Yann LeCun, and\nChristoph Bregler. Joint training of a convolutional\nnetwork and a graphical model for human pose esti-\nmation. In NeurIPS, pages 1799–1807, 2014. 2\n[56] Alexander Toshev and Christian Szegedy. Deeppose:\nHuman pose estimation via deep neural networks. In\nCVPR, pages 1653–1660, 2014. 1\n[57] Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Herv ´e\nJ´egou. Training data-efficient image transformers\n& distillation through attention. arXiv preprint\narXiv:2012.12877, 2020. 3\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\nIn NeurIPS, pages 5998–6008, 2017. 2, 3, 4, 12\n[59] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chun-\nhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia.\nEnd-to-end video instance segmentation with trans-\nformers. arXiv preprint arXiv:2011.14503, 2020. 3\n[60] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and\nYaser Sheikh. Convolutional pose machines. In\nCVPR, pages 4724–4732, 2016. 1, 2, 4\n[61] Bin Xiao, Haiping Wu, and Yichen Wei. Simple base-\nlines for human pose estimation and tracking. In\nECCV, pages 466–481, 2018. 1, 2, 4, 5, 6\n[62] Sen Yang, Wankou Yang, and Zhen Cui. Pose neu-\nral fabrics search. arXiv preprint arXiv:1909.07068 ,\n2019. 2\n[63] Wei Yang, Shuang Li, Wanli Ouyang, Hongsheng Li,\nand Xiaogang Wang. Learning feature pyramids for\nhuman pose estimation. In ICCV, pages 1281–1290,\n2017. 1, 2\n[64] Matthew D. Zeiler and Rob Fergus. Visualizing and\nunderstanding convolutional networks. In ECCV,\npages 818–833, 2014. 2, 3, 5\n[65] Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and\nCe Zhu. Distribution-aware coordinate representation\nfor human pose estimation. In CVPR, pages 7093–\n7102, 2020. 5, 6\n[66] Jianming Zhang, Zhe L. Lin, Jonathan Brandt, Xiao-\nhui Shen, and Stan Sclaroff. Top-down neural atten-\ntion by excitation backprop. InECCV, pages 543–559,\n2016. 3\n[67] Shanshan Zhang, Jian Yang, and Bernt Schiele. Oc-\ncluded pedestrian detection through guided attention\nin cnns. In CVPR, pages 6995–7003, 2018. 3\n[68] Wenqiang Zhang, Jiemin Fang, Xinggang Wang, and\nWenyu Liu. Efficientpose: Efficient human pose esti-\nmation with neural architecture search. arXiv preprint\narXiv:2012.07086, 2020. 2\n[69] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude\nOliva, and Antonio Torralba. Learning deep features\nfor discriminative localization. In CVPR, pages 2921–\n2929, 2016. 2, 3\n[70] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. arXiv\npreprint arXiv:2010.04159, 2020. 3\nA. 2D Sine Position Embedding\nWithout the position information embedded in the in-\nput sequence, the Transformer Encoder is a permutation-\nequivariant architecture:\nEncoder (ρ (X)) =ρ (Encoder (X)) , (5)\nwhere ρ is any permutation for the pixel locations or the\norder of sequence. To make the order of sequence or the\nspatial structure of the image pixels matter, we follow the\nsine positional encodings but further hypothesize that the\nposition information is independent at x (horizontal) and y\n(vertical) direction of an image, like the ways of [42, 9].\nConcretely, we keep the original 2D-structure respectively\nwith d/2 channels for x, y-direction:\nP E(2i,py,:) = sin\n\u0010\n2π ∗ py/(H ∗ 100002i/ d\n2 )\n\u0011\n,\nP E(2i+1,py,:) = cos\n\u0010\n2π ∗ py/(H ∗ 100002i/ d\n2 )\n\u0011\n,\nP E(2i,:,px) = sin\n\u0010\n2π ∗ px/(W ∗ 100002i/ d\n2 )\n\u0011\n,\nP E(2i+1,:,px) = cos\n\u0010\n2π ∗ px/(W ∗ 100002i/ d\n2 )\n\u0011\n,\n(6)\nwhere i = 0, 1, ..., d/2 − 1, px or py is the position index\nalong x or y-direction. Then they are stacked and flattened\ninto a shape RL×d. The position embedding is injected into\nthe input sequences before self-attention computation. We\nuse 2D sine position embedding by default for all models.\nB. What position information has been learned\nin the TransPose model with learnable po-\nsition embedding?\nWe show what position information has been learned in\nthe TransPose (TransPose-R) with learnable position em-\nbedding. It has been discussed in the paper. As shown in\nFig. 8, we visualize the similarities by calculating the co-\nsine similarity between vectors at any pair of locations of\nthe learnable position embedding and reshaping it into a 2D\ngrid-like map. We find that the embedding in each location\nof learnable position embedding has a unique vector value\nin the d-dim vector space, but it has relatively higher cosine\nsimilarity values with the neighbour locations in 2D-grid\nand lower values with those far away from it. The results\nindicate the coarse 2D position information has been im-\nplicitly learned in the learnable position embedding. We\nsuppose that the learning sources of the position informa-\ntion might be the 2D-structure groundtruth heatmaps and\nthe similar features existing in the 1D-structure sequences.\nThe model learns to build associations between position\nembedding and input sequences, as a result it can predict the\ntarget heatmaps with 2D Gaussian peaking at groundtruth\nkeypoints locations.\n0\n1\n2\n3\n4\n0\n5\n1\n 2\n 3\n 4\n 5\n 6\n 7\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nFigure 8. The cosine similarities between the learned position em-\nbedding vectors, which have been reshaped into 2D grid and inter-\npolated with 0.25 scale factor for a better illustration (the original\nshape is (24, 32)). Each map in x-row and y-col of the figure\nrepresents the cosine similarities between the embedding vector in\nposition (x, y) and the embedding vectors at other locations.\nIn the paper, we find that position embedding helps to\ngeneralize better on unseen input resolutions, particularly\n2D sine position embedding. We conjecture that 1) the\nmodels with a fixed receptive field may be hard to adapt\nthe changes in scales; 2) building associations withposition\ninformation encoded in Sine position embedding [58] may\nhelp model generalize better on different sizes.\nC. Transformer Encoder Layer\nThe Transformer Encoder layer [58] we used can be for-\nmulated as:\nZ = LayerNorm (MultiheadSelfAttention (X) +X) ,\nX∗ = LayerNorm (FFN (Z) +Z) ,\n(7)\nwhere X is the original input sequence that has not yet been\nadded with position embedding. The position embedding\nwill be added to X for computing querys and keys exclud-\ning values. X∗ is the output sequence of the current Trans-\nformer Encoder layer, as the input sequence of next encoder\nlayer. The formulations of Multihead Self-Attention and\nFFN are defined in [58].\nD. Gradient Analysis\nFrom the view of an activation at some location i of the\npredicted heatmaps, the network weights associating all in-\nput tokens across the whole image/sequence with this acti-\nvation can be seen as a discriminator that judges the pres-\nence or absence of a certain keypoint at this location. As\nrevealed by [47, 49, 2, 48], the gradient information can in-\ndicate the importance (sensitivity) of the input features to a\nspecific output of a non-linear model. That assumption is\nbased on that tiny change in the input (pixel/feature/token)\nwith the most important feature value causes a large change\nin what the output of the model would be.\nSuppose we have a trained model and a specific image,\nhi ∈ RK is the scores for all K types of keypoints at loca-\ntion i of the predicted heatmaps; zi ∈ Rd is the intermedi-\nate feature outputted by the last self-attention layer before\nbeing fed into FFN. There is only a ReLU excluding the\nlinear and convolutions 3 (head) layers after the last atten-\ntion layer. ReLU (rectified linear unit) activation function\nin FFN can be empirically regarded as a negative contri-\nbution filter, which only retains positive contributions and\nmaintains the linearity. Next, we choose numerator lay-\nout for computing the derivative of a vector with respect\nto a vector. We thus assume the mapping from zi to hi\ncan be approximated as a linear function f with learned\nweights Wf ∈ RK×d and bias b ∈ RK by computing the\nfirst-order Taylor expansion at a given local point z0\ni , i.e.,\nhi ≈ Wf zi + b, Wf = ∂hi\n∂zi\n\f\f\f\nz0\ni\n. Then we compute the\npartial derivative of hi at location i of the output heatmaps\nw.r.t the token xj at location j of the input sequence of the\nlast attention layer:\n∂hi\n∂xj\n= ∂hi\n∂zi\n∂zi\n∂xj\n= ∂f (zi)\n∂zi\n(1 + ∂wiV\n∂xj\n)\n≈ Wf (1 + ∂wi,0v0 + ... + wi,jvj + ... + wi,L−1vL−1\n∂xj\n)\n= Wf (1 + ∂wi,jvj\n∂xj\n)\n= Wf (1 + ∂Ai,jW⊤\nv xj\n∂xj\n)\n(8)\nwhere vj ∈ Rd is the value vector transformed by: vj =\nW⊤\nv xj. Ai,j is a scalar value that is computed by the dot-\nproduct between qi and kj. We assume G := ∂hi\n∂xj\nas a\nfunction w.r.t. a given attention score Ai,j. Under this as-\nsumption Ai,j is deemed as an observed variable that has\n3a 1 × 1 convolution is also a position-wise linear layer; the 4 × 4\ndeconvolution used in TP-R acts as the upsampling operation.\nBackbone ResNet-S\nStem Conv-k7-s2-c64, BN, ReLU\nPooling-k3-s2\nBlocks\n3×Bottleneck-c64\nBottleneck-s2-c128\n3×Bottleneck-c128\nConv-k1-s1-c256\nTable 8. The detailed configurations for ResNet-S. Conv-k7-s2-\nc64 means a convolutional layer with 7 ×7 kernel size, 2 stride,\nand 64 output channels, followed by a BN and ReLU; the\nsame below. The Bottleneck-c64 includes Conv-k1-s1-c64-BN-\nReLU, Conv-k3-s1-c64-BN-ReLU, and Conv-k1-s1-c256-BN.\nBottleneck-c128 includes Conv-k1-s1-c128-BN-ReLU, Conv-k3-\ns1-c128-BN-ReLU, and Conv-k1-s1-c512-BN. See details in [25].\nblocked its parent nodes. Then we define:\nG (Ai,j) =Wf (1 + ∂Ai,jW⊤\nv xj\n∂xj\n)\n= Wf\n\u0000\n1 + Ai,jW⊤\nv\n\u0001\n= Ai,jWf W⊤\nv + Wf\n= Ai,j|{z}\nImage-Specific: dynamic weights\n·Wf · W⊤\nv + Wf| {z }\nLearned: static weights\n= Ai,j · K + B\n(9)\nwhere K, B ∈ RK×d are static weights shared across all\npositions. We can see that the function G is approximately\nlinear with Ai,j, i.e., the degrees of contribution to the pre-\ndiction hi directly depend on its attention scores at those\nlocations.\nThe last attention layer in Transformer Encoder, whose\nattention scores are seen as the image-specific weights, ag-\ngregate contributions from all locations according to at-\ntention scores and finally form the maximum activations\nin the output heatmaps. Though the layers in FFN and\nhead cannot be ignored 4, they are position-wise operators,\nwhich almost linearly transform the attention scores from\nall the positions with the same transformation. In addition,\nQ = (X + P) Wq, K = (X + P) Wk, V = XWv where\nP is the position embedding. Because Ai,j ∝ QiK⊤\nj , the\nposition embedding values also affect the attention scores\nto some extent.\nE. Architecture Details\nWe report the architecture details of ResNet-S and\nHRNet-S-W32(48) in Tab. 8 and Tab. 9. The ResNet-\nS* only differs from ResNet-S in that ResNet-S* has 10\n41. Assuming that the used convolutions extract feature in a limited\npatch, the global interactions mostly occur at the attention layers. 2. The\nlayer normalization does not affect the interactions between locations.\n(a) TP-R-A4: predictions and dependency areas for input 1.\n (b) TP-H-A4: predictions and dependency areas for input 1.\n(c) TP-R-A4: predictions and dependency areas for input 2.\n (d) TP-H-A4: predictions and dependency areas for input 2.\n(e) TP-R-A4: predictions and dependency areas for input 3.\n (f) TP-H-A4: predictions and dependency areas for input 3.\n(g) TP-R-A4: predictions and dependency areas for input 4.\n (h) TP-H-A4: predictions and dependency areas for input 4.\n(i) TP-R-A4: predictions and dependency areas for input 5.\n (j) TP-H-A4: predictions and dependency areas for input 5.\n(k) TP-R-A4: predictions and dependency areas for input 5.\n (l) TP-H-A4: predictions and dependency areas for input 5.\nFigure 9. Predicted locations and the dependency areas for different types of keypoints in different models: TP-R-A4 (left column) and\nTP-H-A4 (right column). In each sub-figure, the first one is the original input image plotted with predicted skeleton. The other maps\nvisualized by the defined dependency area ( Ai,:) of the attention matrix in the last layer with a threshold value (0.00075). The predicted\nlocation of a keypoint is annotated by a WHITE color pentagram (⋆) in each sub-map. Redder area indicates higher attention scores.\nAtten.\nLayer 0\nAtten.\nLayer 1\nAtten.\nLayer 2\nAtten.\nLayer 3\nnose\n eye(l)\n eye(r)\n ear(l)\n ear(r)\n sho.(l)\n sho.(r)\n elb.(l)\n elb.(r)\n wri.(l)\n wri.(r)\n hip(l)\n hip(r)\n kne.(l)\n kne.(r)\n ank.(l)\n ank.(r)\n random\n random\n0.5\n1.0\n(a) TP-R-A4: predictions and dependency areas of each keypoint in\ndifferent attention layers.\nAtten.\nLayer 0\nAtten.\nLayer 1\nAtten.\nLayer 2\nAtten.\nLayer 3\nnose\n eye(l)\n eye(r)\n ear(l)\n ear(r)\n sho.(l)\n sho.(r)\n elb.(l)\n elb.(r)\n wri.(l)\n wri.(r)\n hip(l)\n hip(r)\n kne.(l)\n kne.(r)\n ank.(l)\n ank.(r)\n random\n random\n(b) TP-H-A4: predictions and dependency areas of each keypoint in\ndifferent attention layers.\nAtten.\nLayer 0\nAtten.\nLayer 1\nAtten.\nLayer 2\nAtten.\nLayer 3\nnose\n eye(l)\n eye(r)\n ear(l)\n ear(r)\n sho.(l)\n sho.(r)\n elb.(l)\n elb.(r)\n wri.(l)\n wri.(r)\n hip(l)\n hip(r)\n kne.(l)\n kne.(r)\n ank.(l)\n ank.(r)\n random\n random\n0.5\n1.0\n(c) TP-R-A4: predictions and dependency areas of each keypoint in\ndifferent attention layers.\nAtten.\nLayer 0\nAtten.\nLayer 1\nAtten.\nLayer 2\nAtten.\nLayer 3\nnose\n eye(l)\n eye(r)\n ear(l)\n ear(r)\n sho.(l)\n sho.(r)\n elb.(l)\n elb.(r)\n wri.(l)\n wri.(r)\n hip(l)\n hip(r)\n kne.(l)\n kne.(r)\n ank.(l)\n ank.(r)\n random\n random\n(d) TP-H-A4: predictions and dependency areas of each keypoint in\ndifferent attention layers.\nAtten.\nLayer 0\nAtten.\nLayer 1\nAtten.\nLayer 2\nAtten.\nLayer 3\nnose\n eye(l)\n eye(r)\n ear(l)\n ear(r)\n sho.(l)\n sho.(r)\n elb.(l)\n elb.(r)\n wri.(l)\n wri.(r)\n hip(l)\n hip(r)\n kne.(l)\n kne.(r)\n ank.(l)\n ank.(r)\n random\n random\n0.0\n(e) TP-R-A4: predictions and affect areas of each keypoint in different\nattention layers.\nAtten.\nLayer 0\nAtten.\nLayer 1\nAtten.\nLayer 2\nAtten.\nLayer 3\nnose\n eye(l)\n eye(r)\n ear(l)\n ear(r)\n sho.(l)\n sho.(r)\n elb.(l)\n elb.(r)\n wri.(l)\n wri.(r)\n hip(l)\n hip(r)\n kne.(l)\n kne.(r)\n ank.(l)\n ank.(r)\n random\n random\n0.0\n(f) TP-H-A4: predictions and affect areas of each keypoint in different\nattention layers.\nFigure 10. Dependency areas (the first two rows) and Affected areas (the last row) in different attention layers for different input images.\nBackbone HRNet-S-W32(48)\nStem\nConv-k3-s2-c64, BN, ReLU\nConv-k3-s2-c64, BN, ReLU\n4×Bottleneck-c64\nBlocks\ntransition1∼stage2\ntransition2∼stage3\nConv-k1-s1-c64(92)\nTable 9. The detailed configurations for HRNet-S-W32(48). More\ndetailed information about the transition layer and stage blocks are\ndescribed in the HRNet paper [51].\nBottleneck-c128 blocks. More details about HRNet-W32\nand HRNet-W48 are described in [51].\nF. More Attention Maps Visualizations\nIn this section, we show more visualization results of the\nattention maps from TP-R-A4 (TransPose-R-A4) and TP-\nH-A4 (TransPose-H-A4) models. The attention maps of the\nlast attention layers of two models are shown in Fig. 9. The\nattention maps in different attention layers of two models\nare shown in Fig. 10.",
  "topic": "Transpose",
  "concepts": [
    {
      "name": "Transpose",
      "score": 0.8666943907737732
    },
    {
      "name": "Transformer",
      "score": 0.7654874324798584
    },
    {
      "name": "Computer science",
      "score": 0.6522375345230103
    },
    {
      "name": "Maximization",
      "score": 0.45994895696640015
    },
    {
      "name": "Synthetic data",
      "score": 0.44909414649009705
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.44238218665122986
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4124460518360138
    },
    {
      "name": "Mathematics",
      "score": 0.19073989987373352
    },
    {
      "name": "Mathematical optimization",
      "score": 0.11817175149917603
    },
    {
      "name": "Engineering",
      "score": 0.08366701006889343
    },
    {
      "name": "Cartography",
      "score": 0.0770198404788971
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Eigenvalues and eigenvectors",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210090971",
      "name": "Southeast University",
      "country": "BD"
    }
  ],
  "cited_by": 18
}