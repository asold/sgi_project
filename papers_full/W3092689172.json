{
  "title": "Probing for Multilingual Numerical Understanding in Transformer-Based Language Models",
  "url": "https://openalex.org/W3092689172",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5031913902",
      "name": "Devin S. Johnson",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5051397326",
      "name": "Denise Mak",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5023424645",
      "name": "D. J. Barker",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5055569072",
      "name": "Lexi Loessberg-Zahl",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2516090925",
    "https://openalex.org/W2042432705",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2986266667"
  ],
  "abstract": "Natural language numbers are an example of compositional structures, where larger numbers are composed of operations on smaller numbers. Given that compositional reasoning is a key to natural language understanding, we propose novel multilingual probing tasks tested on DistilBERT, XLM, and BERT to investigate for evidence of compositional reasoning over numerical data in various natural language number systems. By using both grammaticality judgment and value comparison classification tasks in English, Japanese, Danish, and French, we find evidence that the information encoded in these pretrained models' embeddings is sufficient for grammaticality judgments but generally not for value comparisons. We analyze possible reasons for this and discuss how our tasks could be extended in further studies.",
  "full_text": "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 184–192\nOnline, November 20, 2020.c⃝2020 Association for Computational Linguistics\n184\nProbing for Multilingual Numerical Understanding in Transformer-Based Language\nModels\nDevin Johnson, Denise Mak, Drew Barker, Lexi Loessberg-Zahl\nDepartment of Linguistics\nUniversity of Washington\n{dj1121, dpm3, barkand, lexilz}@uw.edu\nAbstract\nNatural language numbers are an example of\ncompositional structures, where larger num-\nbers are composed of operations on smaller\nnumbers. Given that compositional reason-\ning is a key to natural language understanding,\nwe propose novel multilingual probing tasks\ntested on DistilBERT, XLM, and BERT to in-\nvestigate for evidence of compositional rea-\nsoning over numerical data in various natu-\nral language number systems. By using both\ngrammaticality judgment and value compari-\nson classiﬁcation tasks in English, Japanese,\nDanish, and French, we ﬁnd evidence that the\ninformation encoded in these pretrained mod-\nels’ embeddings is sufﬁcient for grammatical-\nity judgments but generally not for value com-\nparisons. We analyze possible reasons for this\nand discuss how our tasks could be extended\nin further studies.\n1 Introduction\nIn recent years, transformer-based language mod-\nels such as BERT (Devlin et al., 2018), XLM\n(Lample and Conneau, 2019), and DistilBERT\n(Sanh et al., 2020) have achieved unprecedented\nresults on a wide array of natural language un-\nderstanding tasks, even when such models are\ntrained on other tasks (i.e. transfer learning). In\nlight of this success, there has been increased\ninterest in investigating what particular informa-\ntion transformer-based language models encode in\ntheir word embeddings during pretraining that al-\nlows them to perform well in transfer learning ex-\nperiments. Put into the context of this paper, we\nmay ask: do such models gain certain linguis-\ntic/compositional understanding from pretraining?\nAttempts to assess such phenomena are commonly\nreferred to as probing experiments. In this paper,\nwe introduce a novel probing task using targeted\ndatasets and classiﬁcation tasks aimed at evaluat-\ning models’ compositional reasoning capabilities\nin relation to numbers 1 in multiple natural lan-\nguages.\nWe choose both grammaticality judgment and\nvalue comparison tasks (Section 3) to assess mul-\ntilingual DistilBERT, XLM, and BERT2 over var-\nious number systems. We argue that high per-\nformance on these tasks indicates some ability of\nreasoning over compositional structures, particu-\nlarly over the rules generating valid compositional\nstructures (task 1) and the resultant meanings of\nthe structures (task 2). After probing the selected\nmodels on our tasks, we discuss explanations for\nthe performance of models, as well as possible fu-\nture extensions to this probing task schema. Ad-\nditionally, studies in cognitive psychology such\nas Miller et al. (1995) assert that children learn-\ning more transparent number systems (i.e. those\nexhibiting more regularity in their surface forms\nsuch as Japanese) have a greater counting proﬁ-\nciency in several tasks compared to those learning\nless transparent (opaque) systems, such as English\nor French. Although it is not the focus of our work,\ngiven the multilingual setting, we will refer to the\nidea of number system transparency when analyz-\ning possible explanations of results.3\n2 Related Work\nOur approach is informed by previous\nlinguistically-motivated probing studies such\n1For our purposes, numbers are spelled out, i.e. written\nout as words such as “ninety”.\n2Models were chosen for their varied sizes (num. param-\neters) as well as our access to computing resources.\n3Number system complexity could be the subject of its\nown paper. However, as a small example, we can look at\n“thirty” in English and “ 三 三 三十 十 十” in Japanese. In English,\nthere is no previous number such as “three” that appears (un-\nchanged) in the word “thirty”. In Japanese, however, the word\nconsists of the kanji for 3 (“三 三 三”) and the kanji for 10 (“十 十 十”).\nIf we continue comparing in this way, we would see com-\npositionality more clearly and regularly in Japanese’s surface\nforms, thus forming our intuitions.\n185\nat those discussed in Belinkov and Glass (2019)\nand Ettinger (2020). Though Ettinger (2020)\ndiscusses important ﬁndings on psycholinguistic\nprobing experiments of BERT, we ﬁnd Ettinger\net al. (2016) particularly useful for our study\ndue to its clear explanations of linguistic probing\nexperiment setup. In their study, Ettinger et al.\npresent methods for constructing linguistically-\ntargeted datasets (including example sentences,\nas we use) and classiﬁcation tasks for probing\nword embeddings for semantic knowledge. As\none of our tasks also seeks to probe for semantic\nknowledge, we were able to use this setup as\na rough guideline. In addition, the authors are\ncareful to create linguistic data with sufﬁcient\ndiversity as not to give potentially helpful cues\nto their classiﬁer which are not related to the\nknowledge they wish to probe for. We thus\ncarefully create our task data in a similar manner\nby limiting the distribution of our data (described\nmore later) and forbidding duplicates.\nTo our knowledge, there have been few stud-\nies conducted on investigating numerical under-\nstanding speciﬁcally in transformer-based lan-\nguage models with a multilingual approach. How-\never, one particularly relevant study on English\ncomes from Wallace et al. (2019). Wallace et al.\nprobe the embeddings of various models (BERT,\nELMo, word2vec, etc.) using three tasks: ﬁnd the\nmaximum of a list, decode a number word to its\nnumerical form, and add number words to produce\na numerical form. In their results, the authors note\nthat all embeddings contain some understanding,\nthough standard embeddings perform particularly\nwell and character-level models perform best. Al-\nthough we investigate similar phenomena as Wal-\nlace et al., our methodology includes several key\ndifferences:\n•Our focus is ﬁrst and foremost to present a\nnovel probing task schema/data and test it\non a selection of transformer-based models\n- not to compare performance of differing\nlanguage model architectures on previously-\nmade tasks.\n•We seek to draw conclusions from our task\nperformance about model weaknesses over\nvaried languages and suggest ways in which\nfurther probing experiments in this area can\nbe designed in the future.\n•We assert that the inclusion of other lan-\nguages besides English is an important ad-\ndition to probing experiments, as variation\nin language structure may help point to\npreviously-unseen weaknesses in pretrained\nmodels.\n•We include spelled-out numbers above 100\n(up to 1000), which were not used in Wal-\nlace et al. We believe having a larger range of\nnumbers might highlight weaknesses of mod-\nels in handling multiple identical tokens in\none word.\n•We use only spelled-out number words, and\ndo not include tasks where both Arabic nu-\nmerals and spelled-out words might be used.\nOur reasoning for this choice is our desire\nto leave out the possibility of models merely\nlearning a mapping from number words to\nnumerals in order to perform well on tasks.\nIn this way, we hope to make our tasks/data\nas restrictive as possible in order that they\nrequire a certain compositional/linguistic un-\nderstanding.\n3 Methods\nWe propose and perform two classiﬁcation tasks\nin English, Danish, Japanese, and French. Task\n1 is a probe for underlying syntactic information\nencoded in pretrained word embeddings, while\ntask 2 is a probe of underlying semantic informa-\ntion. We run our tasks on all three models over\ntwo different datasets which we have generated:\none where number words are inserted into sen-\ntences (e.g. “There are seven hundred books in\nthe library.”) and one with numbers alone (e.g.\n“seven-hundred”). Our probing model features a\nmultilayer perceptron (a NN with a single hid-\nden layer) classiﬁer on top of the existing trans-\nformer language model architecture. In this man-\nner, pretrained word embeddings from the lan-\nguage model (BERT, DistilBERT, XLM) are fed\nas input to the MLP classiﬁer which itself is then\ntrained on our tasks. A depiction of the probing\nmodel structure is shown in Figure 1.\n3.1 Task 1: Grammaticality Judgment\nWe specify the ﬁrst task as follows:\n•Let v ∈{bare, sentence}specify the vari-\nant of our task. If v = bare, then only\ntraining examples with numbers not inserted\n186\ninto sentences will be used for grammatical-\nity judgments. Otherwise, only training ex-\namples with number inserted into sentences\nare used. A mixture of two input data types\nis never used.\n•Let the training set of task 1,T, be deﬁned by\npairs t0...tn where ti = (xti , yti )\n•Let xti be the input of the i’th training exam-\nple ti such that xti = s. s is a string consist-\ning of a number word such as “thirty-two” or\na sentence containing a number word such as\n“He could eat thirty-two oranges” (depending\non the value of v)\n•Let yti ∈{0, 1}be the corresponding label\nof input xti of training sample ti. yti = 1\nif the input string s of xti is ungrammatical,\notherwise yti = 0.\nWe argue high accuracy on this task is evidence\nof some understanding of the underlying compo-\nsitional/syntactic rules for the process which gen-\nerates natural language numbers.\nFigure 1: Probing model; Inputs can vary from sen-\ntences with numbers to standalone numbers. All\nweights are ﬁxed except the connections to the MLP\nlayer\nAs an example, “two hundred three and ﬁfty”\nis not a number in the English language, while\n“ﬁve hundred” is. We also assert that the im-\nportance of using spelled-out numbers comes into\nplay since no string of Arabic numerals is ungram-\nmatical except a small amount of strings such as\n“0112”. Often in written text, Arabic numerals are\nused in place of number words; However, given\nhuman ability to generalize compositional rules\nfrom other structures in order to learn new struc-\ntures, even if it were the case that fewer number\nwords were seen in pretraining, we would hope to\nsee a similar compositional generalization capabil-\nity present in pretrained language models’ embed-\ndings which would allow them to perform well on\nthis task.\n3.2 Task 2: Value Comparison\nWe specify the second task as follows:\n•Let v ∈{bare, sentence}specify the vari-\nant of our task. If v = bare, then only train-\ning examples with numbers not inserted into\nsentences will be used for value comparsion.\nOtherwise, only training examples with num-\nber inserted into sentences are used. A mix-\nture of two input data types is never used.\n•Let the training set of task 2, U, be deﬁned\nby pairs u0...un where ui = (xui , yui )\n•Let xui be the input of the i’th training ex-\nample ui such that xui = (s0, s1). s0 and\ns1 represent bare number words or number\nwords inserted into sentences (depending on\nthe value of v).\n•Let yui ∈{0, 1}be the corresponding label\nof input xui of training sample ui. yui = 0\nif for s0 and s1 of xui , s0 refers to a value\nlarger than that of s1. yui = 1if for s0 and s1\nof xui , s0 refers to a value smaller than that\nof s1.\nWith this task, we take high accuracy as ev-\nidence of some understanding of the composi-\ntional semantic information carried by the number,\ni.e. its magnitude. For example, given the pair\n(s0 =“twelve”, s1 =“ﬁfteen”) the correct output\nshould be 1, since the ﬁrst number in the pair is\nless than the second. This task is similar in form\nto the list maximum task of Wallace et al. (2019);\nHowever, notable differences include our usage of\nnumber words in sentences, our inclusion of num-\nber words above 100, and languages other than\nEnglish.\n187\nTask (v) Input Output\nTask 1 (sentence) “He could eat three hundred and two oranges” 0 ( s grammatical)\nTask 1 (bare) “seventy-four six hundred and thirty-eight” 1 ( s ungrammatical)\nTask 2 (sentence) “There are seven hundred and eighty-six books in the library” 0 (s0 greater)\n“There are thirty-eight books in the library”\nTask 2 (bare) “ﬁve hundred” 1 ( s0 less)\n“six hundred”\nTable 1: Sample of generated English data for our tasks\n4 Data\nSeparate datasets for each variant of each task\nwere made, each with a 60-20-20 train-validation-\ntest split. To generate number words to cre-\nate training example inputs, the python package\nnum2words (Ogawa) was used for text conversion\nfrom numerical to standard spelled-out numbers.\nFor task 1, it was necessary to create ungrammati-\ncal numbers in each language. These ungrammat-\nical number words were created by randomly ap-\npending grammatical number words (or parts of\nthem) together and controlling for length. Lastly,\nit was also necessary to create custom sentences\nto insert our number words into. Eleven sentence\ntemplates were used, translated into each of our\nlanguages and veriﬁed by native speakers. A sam-\nple of our data can be seen in Table 1. Detailed\ninformation on dataset statistics and data genera-\ntion techniques can be found in appendix B.\n5 Results\nThe following sections show probing results on\nboth tasks. Before probing and as a precaution,\nwe ﬁne-tuned our models on both tasks. As we ex-\npected, we ﬁnd that both tasks are learnable to ac-\ncuracies above 95%. A further discussion of ﬁne-\ntuning results is left for appendix A.\n5.1 Task 1 Results\nOur results on task 1 (Figure 2) show that the pre-\ntrained embeddings of multilingual BERT, Dis-\ntilBERT and XLM seem to have sufﬁcient infor-\nmation to be able to determine grammaticality of\nnumber words in Japanese, English, Danish, and\nFrench at better-than-chance performance. We\nthus argue that these results suggest that the pre-\ntrained embeddings of these models contain some\nunderstanding of the compositional rules for gen-\nerating number words in the languages we’ve se-\nlected.\nThere are a few patterns worth noting in the\nresults. Firstly, accuracy on bare numbers was\nalways better than accuracy on numbers in sen-\ntences. Though accuracy on numbers in sentences\nwas not extremely poor, our initial prediction was\nthat they would perform better as they would re-\nsemble the type of data which the models were\npretrained on (Wikipedia). Secondly, in terms of\noverall model performance, DistilBERT performs\nbest (sometimes even at 100% accuracy) in the\nmajority of cases, followed by XLM and BERT.\nA further analysis of these patterns is left for our\ndiscussion (6).\n5.2 Task 2 Results\nOn our second task, we ﬁnd that overall, the in-\nformation in the pretrained embeddings of BERT,\nDistilBERT, and XLM is mostly insufﬁcient for\ncomparing number magnitudes in our tested lan-\nguages with high accuracy. This suggests that\nthese pretrained embeddings may struggle with\nunderstanding the compositional semantics of\nnumber words (i.e. how compositional elements\nin number words form to create meaning).\nAs for patterns in these results, we can again see\nthat bare number performance is always equal to\nor better than when numbers are inserted into sen-\ntences. Looking at bare results, DistilBERT again\nperforms well, but this time XLM performs best\non Japanese and English. A further analysis of\nthese patterns is also left for our discussion (6).\n188\n/uni00000027/uni0000002e/uni00000003/uni00000028/uni00000031/uni00000003/uni00000029/uni00000035/uni00000003/uni0000002d/uni00000024/uni00000003\n/uni0000002f/uni00000024/uni00000031/uni0000002a\n/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000027/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni00000025/uni00000028/uni00000035/uni00000037\n/uni0000003b/uni0000002f/uni00000030\n/uni00000030/uni00000032/uni00000027/uni00000028/uni0000002f\n/uni00000013/uni00000011/uni00000019/uni0000001a/uni00000013/uni00000011/uni00000019/uni00000014/uni00000013/uni00000011/uni00000018/uni00000019/uni00000013/uni00000011/uni00000019/uni00000015\n/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000013/uni00000011/uni0000001a/uni0000001a/uni00000013/uni00000011/uni0000001a/uni00000017\n/uni00000013/uni00000011/uni0000001a/uni0000001b/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000013/uni00000011/uni0000001a/uni0000001c\n/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000033/uni00000055/uni00000052/uni00000045/uni0000004c/uni00000051/uni0000004a/uni0000000c/uni00000003/uni00000010/uni00000003/uni00000036/uni00000048/uni00000051/uni00000057/uni00000048/uni00000051/uni00000046/uni00000048/uni00000056\n/uni00000027/uni0000002e/uni00000003/uni00000028/uni00000031/uni00000003/uni00000029/uni00000035/uni00000003/uni0000002d/uni00000024/uni00000003\n/uni0000002f/uni00000024/uni00000031/uni0000002a\n/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000027/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni00000025/uni00000028/uni00000035/uni00000037\n/uni0000003b/uni0000002f/uni00000030\n/uni00000030/uni00000032/uni00000027/uni00000028/uni0000002f\n/uni00000013/uni00000011/uni0000001b/uni00000017/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000013/uni00000011/uni00000019/uni00000017/uni00000013/uni00000011/uni0000001b/uni00000016\n/uni00000013/uni00000011/uni0000001c/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000013/uni00000011/uni0000001c/uni0000001b\n/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000017\n/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000033/uni00000055/uni00000052/uni00000045/uni0000004c/uni00000051/uni0000004a/uni0000000c/uni00000003/uni00000010/uni00000003/uni00000025/uni00000044/uni00000055/uni00000048\n/uni00000027/uni0000002e/uni00000003/uni00000028/uni00000031/uni00000003/uni00000029/uni00000035/uni00000003/uni0000002d/uni00000024/uni00000003\n/uni0000002f/uni00000024/uni00000031/uni0000002a\n/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000027/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni00000025/uni00000028/uni00000035/uni00000037\n/uni0000003b/uni0000002f/uni00000030\n/uni00000030/uni00000032/uni00000027/uni00000028/uni0000002f\n/uni00000013/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014\n/uni00000013/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000018/uni00000019/uni00000013/uni00000011/uni00000018/uni00000019/uni00000013/uni00000011/uni00000018/uni00000014\n/uni00000013/uni00000011/uni00000018/uni0000001a/uni00000013/uni00000011/uni00000019/uni00000016/uni00000013/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000018/uni00000017\n/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000033/uni00000055/uni00000052/uni00000045/uni0000004c/uni00000051/uni0000004a/uni0000000c/uni00000003/uni00000010/uni00000003/uni00000036/uni00000048/uni00000051/uni00000057/uni00000048/uni00000051/uni00000046/uni00000048/uni00000056\n/uni00000027/uni0000002e/uni00000028/uni00000031/uni00000029/uni00000035/uni0000002d/uni00000024\n/uni0000002f/uni00000024/uni00000031/uni0000002a\n/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000027/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni00000025/uni00000028/uni00000035/uni00000037\n/uni0000003b/uni0000002f/uni00000030\n/uni00000030/uni00000032/uni00000027/uni00000028/uni0000002f\n/uni00000013/uni00000011/uni00000018/uni00000015/uni00000013/uni00000011/uni00000018/uni00000019/uni00000013/uni00000011/uni00000018/uni00000017/uni00000013/uni00000011/uni00000019/uni00000016\n/uni00000013/uni00000011/uni00000019/uni00000017/uni00000013/uni00000011/uni0000001a/uni00000017/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000011/uni00000019/uni00000014\n/uni00000013/uni00000011/uni00000018/uni0000001c/uni00000013/uni00000011/uni0000001a/uni0000001a/uni00000013/uni00000011/uni00000019/uni0000001c/uni00000013/uni00000011/uni0000001a/uni00000016\n/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000033/uni00000055/uni00000052/uni00000045/uni0000004c/uni00000051/uni0000004a/uni0000000c/uni00000003/uni00000010/uni00000003/uni00000025/uni00000044/uni00000055/uni00000048\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\nFigure 2: Task test accuracy on probing per language/model.\n6 Discussion\n6.1 Pretraining Data\nSince this probing experiment is using pretrained\ntransformer-based language models (Wolf et al.,\n2019), we can brieﬂy discuss pretraining meth-\nods in order to ascertain potential effects on re-\nsults. Each model was pretrained using masked\nlanguage modelling and all except XLM (unspec-\niﬁed) were pretrained on multilingual Wikipedia\ndata. With this, one may ask whether this method\nof training should be expected to encode the in-\nformation we probe for. Particularly, one may\npoint to the usage of Arabic numerals in Wikipedia\ntext and less-frequent usage of spelled-out num-\nbers as a cause for poor performance on task 2.\nIn imagining alternatives to Wikipedia, a consid-\neration of Arabic numerals must still be present,\nwhich serves as a reminder that there is no perfect\npretraining set.\nHowever, we do not believe this would prevent\nlanguage models from capturing the information\nnecessary to complete our task 2. We assert that\neven if these models have seen fewer spelled-out\nnumbers, they could still learn compositional rules\nfrom other linguistic structures and generalize to\nour probing task. If Arabic numerals proved to\nbe an issue, we would expect to see our results\nbe worse across the board, not only on task 2.\nThus, from our results, it seems that pretraining\non Wikipedia was certainly not sufﬁcient for en-\ncoding a highly accurate sense of number magni-\ntude for any of the models/languages, but this was\nlikely not due to pretraining methods.\n6.2 Worse Performance on Task 2\nWhy might models have more difﬁculty in ascer-\ntaining magnitude of number words? For one, we\nbelieve this task is naturally more difﬁcult than\nthe ﬁrst because of the deep semantic informa-\ntion necessary to succeed on it. In the ﬁrst task, it\nmay have been possible to leverage at least some\nof the surface level characteristics of grammatical\nand ungrammatical words, whereas in the second\ntask there is no such leveraging possible. That is\nto say, in the ﬁrst task, a model can learn syntac-\ntic information more directly from surface level\npatterns. Instead, in task 2, the models need to\nhave encoded some semantic information about\nthe magnitude of number words, where the surface\nforms of these words gives less indication of their\nunderlying meaning (except for the possibility of\nlonger words having larger quantities, though this\nis not always the case). Given this is true, this\nmay point to a weakness in models to make ﬁne-\ngrained semantic distinctions regarding quantities,\nespecially when quantities are used in sentences\nand not left bare.\n6.3 Language Transparency\nIn terms of number system transparency (as men-\ntioned in our introduction), we loosely presumed\nthat accuracies might follow the order of Japanese\n> English/Danish > French, with Japanese per-\nforming best given its higher transparency and\nFrench the worst due to its vigesimal number sys-\ntem. Again, we choose not to formally deﬁne\ntransparency, as such a formal deﬁnition is an in-\ndepth topic of its own. To our slight surprise, our\n189\nresults did not match these predictions, with rank-\nings of performance by language varying by task\nand task variation.\nWe argue there could be many reasons (such as\npretraining data) for the unexpected results pat-\ntern. However, since there is more of a consis-\ntent performance pattern across models than there\nis across languages, we believe it is far more likely\nthat differences in performance are not necessarily\ndue to language transparency, but rather model ar-\nchitecture and therefore that the structural differ-\nences between these languages is not a signiﬁcant\ncontributing factor to performance patterns. If this\nwere true (which we think is probable), this is a\ngood sign for these models, since language struc-\nture differences are not proving to be a challenge\nto performance, but rather some other factor.\n6.4 Model Architecture and Performance\nOne clear pattern we can see is that multilingual\nBERT’s embeddings consistently perform much\nworse than both XLM and DistilBERT. This is es-\npecially clear in French results, where the gap be-\ntween BERT and the other two models is some-\ntimes more than 20 points. A relatively simple ex-\nplanation for this is the size of each model in terms\nof number of parameters. Indeed, as model size\nincreases, performance on both tasks increases\n(BERT -> DistilBERT -> XLM). Of course, cor-\nrelation is no evidence of causation; However, if\nthis were true, it is quite consistent with other\ntrends in recent NLP studies. In the case of this\nstudy, we can say that bigger is (almost) always\nbetter, with XLM mostly performing best, fol-\nlowed by DistilBERt and BERT. Though, this is\nsomewhat undesirable on a larger scale since, ide-\nally, we would hope that it would not require such\na large model to encode the information we probe\nfor.\n6.5 Bare Number Words Perform Better\nOn both tasks, our results also show that bare num-\nbers performed equal to or better than numbers in\nsentences. We propose that this is due to the sen-\ntences creating noisiness, thus creating more difﬁ-\nculty for a model to know exactly where it should\nbe looking for the necessary information to com-\nplete the tasks. This is very much the case for task\n2, where we believe it would be harder to know the\nmagnitude a sentence is referring to than merely\nif the sentence is grammatical. We argue that, be-\nsides adding noise, this method of probing exploits\na possible weakness in masked language model-\ning as a pretraining method. That is, given that\nmasked language modeling’s task it to predict ap-\npropriate (grammatical) words, there may be less\nemphasis on learning the underlying semantics of\nthose words, thus the better performance on task\n1 sentences and worse performance on task 2 sen-\ntences.\n7 Further Work\nAs this work is an exploration of a new probing\nmethod for state-of-the-art language model archi-\ntecture, there are surely a number of ways to ex-\ntend from it.\nThough we discussed it brieﬂy here, explor-\ning the architectural reasons for the shortcomings\nof these pretrained embeddings, especially in the\ncase of task 2 and with sentences is an impor-\ntant area for future work. Indeed, in a similar\ntask from Wallace et al. (2019), BERT was also\nfound to have poor performance. In the future,\nseveral more speciﬁc probes could be designed to\ntest for understanding of magnitude in various lin-\nguistic contexts to ﬁnd strengths and weaknesses\nof transformer-based models. A particularly inter-\nesting case would be in testing magnitude com-\nprehension in sentences of varying structures. Our\nsentence templates used in this study are few, and\nexperimenting with other varieties could prove to\nbe insightful.\nOur experiment also made use of the idea of\nlanguage transparency. We also ﬁnd this to be\na topic for possible further work. Namely, is\nthere a method to reliably measure transparency\nof languages to predict performance on numeri-\ncal understanding tasks such as these? We believe\nthis may be possible through measuring complex-\nities of grammars which generate number words\nin each language. Overall, in future extensions of\nthis study, there is room for more languages, sen-\ntence types, task renditions, and models.\n8 Conclusion\nIn this paper, we introduced methods for probing\nthe multilingual compositional reasoning capabil-\nities of transformer-based models’ pretrained em-\nbeddings over natural language numbers. From\nour experiments, we’ve shown that these pre-\ntrained embeddings show some capabilities in\nmaking grammatical judgments of number words,\nthough they are less capable of making value com-\n190\nparisons. In addition, we ﬁnd that results gener-\nally follow a trend based upon model size. Our\nresults are in accord with previous work such as\nWallace et al. (2019); However, we have also\nhighlighted further model weaknesses through our\nprobing methods. Therefore, the opportunities for\nfuture work, especially with a multilingual focus,\nare plenty.\nAcknowledgements\nThank you to all anonymous reviewers for your\nhelpful comments. Thank you to Professor Shane\nSteinert-Threlkeld for your guidance and through-\nout all stages of this paper. Thank you to Professor\nLuke Zettlemoyer for your advice on our paper in\nits earlier stages. Thank you to the several volun-\nteer native speakers for grammaticality judgments.\n–This work was facilitated through the use of ad-\nvanced computational, storage, and networking in-\nfrastructure provided by the Hyak supercomputer\nsystem and funded by the STF at the University of\nWashington.–\nReferences\nYonatan Belinkov and James R. Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.\n2018. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding.\nAllyson Ettinger. 2020. What bert is not: Lessons from\na new suite of psycholinguistic diagnostics for lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nAllyson Ettinger, Ahmed Elgohary, and Philip Resnik.\n2016. Probing for semantic evidence of composition\nby means of simple classiﬁcation tasks. In Proceed-\nings of the 1st Workshop on Evaluating Vector-Space\nRepresentations for NLP, pages 134–139, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. CoRR,\nabs/1901.07291.\nK. F. Miller, C. M. Smith, J. Zhu, and H. Zhang.\n1995. Preschool origins of cross-national differ-\nences in mathematical competence: The role of\nnumber-naming systems. Psychological Science,\n6:56–60.\nT. Ogawa. num2words.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do nlp models know num-\nbers? probing numeracy in embeddings. Proceed-\nings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP).\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\n191\nA Fine-Tuning\nThe ﬁne-tuning results shown in (Figure 3) are\nvalidation accuracies after one epoch of training.\nWhen trained for 20 epochs (Figure 4), the mod-\nels reach over 99% accuracy on all languages and\ntasks except for Danish which reaches 95% on\nsentence tasks.\nFigure 3: Fine-tuned task validation accuracy per lan-\nguage/model, when trained for 1 epoch.\nFigure 4: Fine-tuned task validation accuracy per lan-\nguage/model, when trained for 20 epochs.\nB Data Generation\nB.1 Dataset Parameters\nThe parameters below were used to generate data\nfor each language per each model per each task\nvariation:\n•Data Gen. Seed: 1\n•Data Gen. Number Range: [0-999]\n•Train Set Size: 30,000\n•Validation Set Size: 10,000\n•Test Set Size: 10,000\n•Shufﬂe = True\nB.2 Task 1 Data\nFor both variants of task 1 (sentences/bare), gram-\nmatical data are generated by creating random\nnumbers then converting them to text through the\n192\nnum2words (Ogawa) package. For the ungram-\nmatical data, two grammatical numbers are ran-\ndomly generated, both converted to text, then ap-\npended together to create an ungrammatical num-\nber. For example the ungrammatical number\n“ﬁfty-ﬁve two hundred” is the combination of\n“ﬁfty-ﬁve” and “two hundred”. Another example\nmade from the same original elements could be:\n“two ﬁfty-ﬁve hundred”. Grammatical numbers\nused in splits, however, were only split such that\nthe resulting elements were grammatical words\nthemselves. So, for example, an non-continuous\nnumber word string like ”ﬁ-tfy te nnine” would\nnever occur.\nSince generating numbers with appendage can\nnaturally occur in ungrammatical number words\nbeing longer than grammatical, we control for\nlength by limiting our set of ungrammatical num-\nber words to words that are at most as long as\nthe longest grammatical number. Lastly, we en-\nsure no grammatical numbers are accidentally cre-\nated in this process by keeping a list of known\ngrammatical numbers in text form (generated by\nnum2words) that ranges from number sufﬁciently\nhigher than our generation range. For example,\nif our number word generation ranges from 0-\n1000, we would make this list of known grammat-\nical numbers from 1-100,000,000. These number\nwords are ﬁnally either left bare or inserted into\nsentences to form our x inputs and are labeled 0 if\ngrammatical and 1 if ungrammatical.\nB.3 Task 2 Data\nThe data for the semantic task are generated by\ncreating pairs of random (grammatical) number\nwords and labeling the pair with one of two cat-\negories: 0 if the ﬁrst numbers is larger than the\nsecond and 1 if the second is larger than the ﬁrst.\nThrough our process of data generation, we ensure\nthat there are never two pairs using the same num-\nber. They are then converted to text form for input\nto a model. These number words are ﬁnally either\nleft bare or inserted into sentences to form our x\ninputs. When numbers are used in sentence tem-\nplates, it is ensured that the numbers are used in\nthe same template. For example, given the num-\nber pair “ﬁve” and “six”, we could compare the\nsentences: “There are ﬁve apples.” and “There are\nsix apples.”.\nC Modeling\nC.1 Pytorch Hugging Face Transformers\nWe use the conﬁgurations below of transform-\ners from Hugging Face Transformers (Wolf et al.,\n2019) in Pytorch on all of our reported experimen-\ntal runs. Average runtimes were all around 1 hour\nor less.\n• DistilBERT:\n– Class: DistilBertForSequenceClassiﬁcation\n– Conﬁg: distilbert-base-multilingual-cased\n– Tokenizer: DistilBertTokenizer\n– Num. Parameters: 134 million total\n• BERT\n– Class: BertForSequenceClassiﬁcation\n– Conﬁg: base-multilingual-cased\n– Tokenizer: BertTokenizer\n– Num. Parameters: 110 million total\n• XLM\n– Class: XLMForSequenceClassiﬁcation\n– Conﬁg: xlm-mlm-100-1280\n– Tokenizer: XLMTokenizer\n– Num. Paremeters: ∼550 million total (inexact)\nC.2 Hyperparameters\nAll experiments which produced our ﬁnal results\nshown in the paper were run with the following\nhyperparemeters which were selected manually by\ntuning for accuracy over a validation set:\n•Epochs: 20 (Range: 10-20)\n•Learning Rate: 0.00001 (Range: 1e-5 - 1e-4)\n•Minibatch size: 32\nC.3 Infrastructure\n•GPU: Nvidia Tesla P100\n•CUDA Version: 10.1\n•Python Version: 3.7\nC.4 Code Repository\nOur Github repository can be found here. Code\nis subject to change after publishing of this paper.\nRefer to the Github README for latest informa-\ntion.",
  "topic": "Grammaticality",
  "concepts": [
    {
      "name": "Grammaticality",
      "score": 0.9878031015396118
    },
    {
      "name": "Computer science",
      "score": 0.7445629835128784
    },
    {
      "name": "Transformer",
      "score": 0.6181809306144714
    },
    {
      "name": "Natural language",
      "score": 0.6083773374557495
    },
    {
      "name": "Natural language processing",
      "score": 0.5948307514190674
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5447210073471069
    },
    {
      "name": "Value (mathematics)",
      "score": 0.5157445073127747
    },
    {
      "name": "Language model",
      "score": 0.49512162804603577
    },
    {
      "name": "Question answering",
      "score": 0.46132391691207886
    },
    {
      "name": "Linguistics",
      "score": 0.3329607844352722
    },
    {
      "name": "Machine learning",
      "score": 0.18679434061050415
    },
    {
      "name": "Grammar",
      "score": 0.07828012108802795
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}