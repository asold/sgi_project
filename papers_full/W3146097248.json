{
  "title": "Going deeper with Image Transformers",
  "url": "https://openalex.org/W3146097248",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2951399046",
      "name": "Hugo Touvron",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156581121",
      "name": "Matthieu Cord",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2526820065",
      "name": "Alexandre Sablayrolles",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A294368734",
      "name": "Gabriel Synnaeve",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1140831031",
      "name": "Hervé Jégou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3035743198",
    "https://openalex.org/W6752356114",
    "https://openalex.org/W6779248606",
    "https://openalex.org/W6757468910",
    "https://openalex.org/W6775845032",
    "https://openalex.org/W6784637704",
    "https://openalex.org/W6787972765",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W6784614252",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W6701947533",
    "https://openalex.org/W6743731764",
    "https://openalex.org/W6752910514",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W6638667902",
    "https://openalex.org/W6780086851",
    "https://openalex.org/W6626481562",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W6770506093",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6763310536",
    "https://openalex.org/W6775248243",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W6774776664",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W3128099838",
    "https://openalex.org/W6775717222",
    "https://openalex.org/W6768080748",
    "https://openalex.org/W6631943919",
    "https://openalex.org/W6751979845",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W6746208923",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W6780493881",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W6774970780",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6637551013",
    "https://openalex.org/W6764990469",
    "https://openalex.org/W6763367864",
    "https://openalex.org/W2097073572",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W3034421924",
    "https://openalex.org/W6761628794",
    "https://openalex.org/W6771378132",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W6779724054",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W6788467338",
    "https://openalex.org/W6779602356",
    "https://openalex.org/W6758139636",
    "https://openalex.org/W6790428460",
    "https://openalex.org/W6788627230",
    "https://openalex.org/W6775876232",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W2944255943",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W2922509574",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W6746052068",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W2963139417",
    "https://openalex.org/W3010768098",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W2907121943",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2971315489",
    "https://openalex.org/W3128633047",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W3093579165",
    "https://openalex.org/W3012324658",
    "https://openalex.org/W2964065616",
    "https://openalex.org/W3017022649",
    "https://openalex.org/W2993466051",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3035618017",
    "https://openalex.org/W3103005696",
    "https://openalex.org/W1690739335",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3093533780",
    "https://openalex.org/W2953937638",
    "https://openalex.org/W3035794324",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3180562345",
    "https://openalex.org/W3120885796",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3121334524",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W2991213871",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W1026270304",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W2911925209",
    "https://openalex.org/W2950181225",
    "https://openalex.org/W3034363135",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2949427019"
  ],
  "abstract": "Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers has been little studied so far. In this work, we build and optimize deeper transformer networks for image classification. In particular, we investigate the interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models whose performance does not saturate early with more depth, for instance we obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters. Moreover, our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data. We share our code and models.",
  "full_text": "Going deeper with Image Transformers\nHugo Touvron⋆,† Matthieu Cord† Alexandre Sablayrolles⋆\nGabriel Synnaeve⋆ Herv´e J´egou⋆\n⋆Facebook AI †Sorbonne University\nAbstract\nTransformers have been recently adapted for large scale image classiﬁ-\ncation, achieving high scores shaking up the long supremacy of convolu-\ntional neural networks. However the optimization of image transformers\nhas been little studied so far. In this work, we build and optimize deeper\ntransformer networks for image classiﬁcation. In particular, we investigate\nthe interplay of architecture and optimization of such dedicated transform-\ners. We make two transformers architecture changes that signiﬁcantly im-\nprove the accuracy of deep transformers. This leads us to produce models\nwhose performance does not saturate early with more depth, for instance\nwe obtain 86.5% top-1 accuracy on Imagenet when training with no exter-\nnal data, we thus attain the current SOTA with less FLOPs and parameters.\nMoreover, our best model establishes the new state of the art on Imagenet\nwith Reassessed labels and Imagenet-V2 / match frequency, in the setting\nwith no additional training data. We share our code and models1.\n1 Introduction\nResidual architectures are prominent in computer vision since the advent of\nResNet [27]. They are deﬁned as a sequence of functions of the form\nxl+1 = gl(xl) + Rl(xl), (1)\nwhere the function gl and Rl deﬁne how the network updates the input xl\nat layer l. The function gl is typically identity, while Rl is the main building\nblock of the network: many variants in the literature essentially differ on how\none deﬁnes this residual branch Rl is constructed or parametrized [50, 62, 72].\nResidual architectures highlight the strong interplay between optimization and\narchitecture design. As pointed out by He et al.[27], residual networks do not\noffer a better representational power. They achieve better performance because\n1https://github.com/facebookresearch/deit\n1\narXiv:2103.17239v2  [cs.CV]  7 Apr 2021\nthey are easier to train: shortly after their seminal work, Heet al.discussed [28]\nthe importance of having a clear path both forward and backward, and advo-\ncate setting gl to the identity function.\nThe vision transformers [19] instantiate a particular form of residual ar-\nchitecture: after casting the input image into a set x0 of vectors, the network\nalternates self-attention layers (SA) with feed-forward networks (FFN), as\nx′\nl = xl + SA(η(xl))\nxl+1 = x′\nl + FFN(η(x′\nl)) (2)\nwhere η is the LayerNorm operator [1]. This deﬁnition follows the original\narchitecture of Vaswani et al.[66], except the LayerNorm is applied before the\nblock (pre-norm) in the residual branch, as advocated by He et al.[28]. Child et\nal. [13] adopt this choice with LayerNorm for training deeper transformers for\nvarious media, including for image generation where they train transformers\nwith 48 layers.\nHow to normalize, weigh, or initialize the residual blocks of a residual ar-\nchitecture has received signiﬁcant attention both for convolutional neural net-\nworks [7, 8, 28, 75] and for transformers applied to NLP or speech tasks [2,\n34, 75]. In Section 2, we revisit this topic for transformer architectures solving\nimage classiﬁcation problems. Examples of approaches closely related to ours\ninclude Fixup [75], T-Fixup [34], ReZero [2] and SkipInit [16].\nFollowing our analysis of the interplay between different initialization, op-\ntimization and architectural design, we propose an approach that is effective to\nimprove the training of deeper architecture compared to current methods for\nimage transformers. Formally, we add a learnable diagonal matrix on output of\neach residual block, initialized close to (but not at) 0. Adding this simple layer\nafter each residual block improves the training dynamic, allowing us to train\ndeeper high-capacity image transformers that beneﬁt from depth. We refer to\nthis approach as LayerScale.\nSection 3 introduces our second contribution, namely class-attention lay-\ners, that we present in Figure 2. It is akin to an encoder/decoder architecture,\nin which we explicitly separate the transformer layers involving self-attention\nbetween patches, from class-attention layers that are devoted to extract the con-\ntent of the processed patches into a single vector so that it can be fed to a linear\nclassiﬁer. This explicit separation avoids the contradictory objective of guiding\nthe attention process while processing the class embedding. We refer to this\nnew architecture as CaiT (Class-Attention in Image Transformers).\nIn the experimental Section 4, we empirically show the effectiveness and\ncomplementary of our approaches:\n• LayerScale signiﬁcantly facilitates the convergence and improves the ac-\ncuracy of image transformers at larger depths. It adds a few thousands\nof parameters to the network at training time (negligible w.r.t. the total\nnumber of weights).\n• Our architecture with speciﬁc class-attention offers a more effective pro-\ncessing of the class embedding.\n2\n• Our best CaiT models establish the new state of the art on Imagenet-\nReal [6] and Imagenet V2 matched frequency [52] with no additional\ntraining data. On ImageNet1k-val [54], our model is on par with the state\nof the art (86.5%) while requiring less FLOPs (329B vs 377B) and having\nless parameters than the best competing model (356M vs 438M).\n• We achieve competitive results on Transfer Learning.\nWe provide visualizations of the attention mechanisms in Section 5. We\ndiscuss related works along this paper and in the dedicated Section 6, before\nwe conclude in Section 7. The appendices contain some variations we have\ntried during our exploration.\n2 Deeper image transformers with LayerScale\nOur goal is to increase the stability of the optimization when training trans-\nformers for image classiﬁcation derived from the original architecture by Vaswani\net al.. [66], and especially when we increase their depth. We consider more\nspeciﬁcally the vision transformer (ViT) architecture proposed by Dosovitskiy\net al.[19] as the reference architecture and adopt the data-efﬁcient image trans-\nformer (DeiT) optimization procedure of Touvron et al. [63]. In both works,\nthere is no evidence that depth can bring any beneﬁt when training on Ima-\ngenet only: the deeper ViT architectures have a low performance, while DeiT\nonly considers transformers with 12 blocks of layers. The experimental sec-\ntion 4 will conﬁrm that DeiT does not train deeper models effectively.\nFigure 1 depicts the main variants that we compare for helping the opti-\nmization. They cover recent choices from the literature: as discussed in the in-\ntroduction, the architecture (a) of ViT and DeiT is a pre-norm architecture [19,\n63], in which the layer-normalisation ηoccurs at the beginning of the residual\nbranch. Note that the original architecture of Vaswani et al. [66] applies the\nnormalization after the block, but in our experiments the DeiT training does\nnot converge with post-normalization.\nFixup [75], ReZero [2] and SkipInit [16] introduce learnable scalar weight-\ning αl on the output of residual blocks, while removing the pre-normalization\nand the warmup, see Figure 1(b). This amounts to modifying Eqn. 2 as\nx′\nl = xl + αl SA(xl)\nxl+1 = x′\nl + α′\nl FFN(x′\nl). (3)\nReZero simply initializes this parameter to α = 0 . Fixup initializes this\nparameter to α = 1 and makes other modiﬁcations: it adopts different poli-\ncies for the initialization of the block weights, and adds several weights to the\nparametrization. In our experiments, these approaches do not converge even\nwith some adjustment of the hyper-parameters.\nOur empirical observation is that removing the warmup and the layer-\nnormalization is what makes training unstable in Fixup and T-Fixup. There-\nfore we re-introduce these two ingredients so that Fixup and T-Fixup converge\n3\nFFN or SA\n(a) (d) \nFFN or SA\n(b) \nFFN or SA FFN or SA\n(c) \nFigure 1: Normalization strategies for transformer blocks. (a) The ViT image classiﬁer\nadopts pre-normalization like Child et al.[13]. (b) ReZero/Skipinit and Fixup remove\nthe ηnormalization and the warmup (i.e., a reduced learning rate in the early training\nstage) and add a learnable scalar initialized to α= 0 and α= 1, respectively. Fixup ad-\nditionally introduces biases and modiﬁes the initialization of the linear layers. Since\nthese methods do not converge with deep vision transformers, (c) we adapt them by re-\nintroducing the pre-norm ηand the warmup. Our main proposal (d) introduces a per-\nchannel weighting (i.e, multiplication with a diagonal matrix diag(λ1,...,λ d), where\nwe initialize each weight with a small value as λi = ε.\nwith DeiT models, see Figure 1(c). As we see in the experimental section, these\namended variants of Fixup and T-Fixup are effective, mainly due to the learn-\nable parameter αl. When initialized at a small value, this choice does help the\nconvergence when we increase the depth.\nOur proposal LayerScale is a per-channel multiplication of the vector pro-\nduced by each residual block, as opposed to a single scalar, see Figure 1(d).\nOur objective is to group the updates of the weights associated with the same\noutput channel. Formally, LayerScale is a multiplication by a diagonal matrix\non output of each residual block. In other terms, we modify Eqn. 2 as\nx′\nl = xl + diag(λl,1,...,λ l,d) ×SA(η(xl))\nxl+1 = x′\nl + diag(λ′\nl,1,...,λ ′\nl,d) ×FFN(η(x′\nl)), (4)\nwhere the parameters λl,i and λ′\nl,i are learnable weights. The diagonal values\nare all initialized to a ﬁxed small value ε: we set it to ε = 0.1 until depth 18,\nε= 10−5 for depth 24 and ε= 10−6 for deeper networks. This formula is akin\nto other normalization strategies ActNorm [37] or LayerNorm but executed on\noutput of the residual block. Yet we seek a different effect: ActNorm is a data-\ndependent initialization that calibrates activations so that they have zero-mean\nand unit variance, like batchnorm [35]. In contrast, we initialize the diagonal\nwith small values so that the initial contribution of the residual branches to\nthe function implemented by the transformer is small. In that respect our mo-\ntivation is therefore closer to that of ReZero [2], SkipInit [16], Fixup [75] and\nT-Fixup [34]: to train closer to the identity function and let the network inte-\n4\ngrate the additional parameters progressively during the training. LayerScale\noffers more diversity in the optimization than just adjusting the whole layer\nby a single learnable scalar as in ReZero/SkipInit, Fixup and T-Fixup. As we\nwill show empirically, offering the degrees of freedom to do so per channel is a\ndecisive advantage of LayerScale over existing approaches.In Appendix A, we\npresent other variants or intermediate choices that support our proposal, and\na control experiment that aims at disentangling the speciﬁc weighting of the\nbranches of LayerScale from its impact on optimization.\nFormally, adding these weights does not change the expressive power of the\narchitecture since, as they can be integrated into the previous matrix of the SA\nand FFN layers without changing the function implemented by the network.\n3 Specializing layers for class attention\nIn this section, we introduce the CaiT architecture, depicted in Figure 2 (right).\nThis design aims at circumventing one of the problems of the ViT architec-\nture: the learned weights are asked to optimize two contradictory objectives:\n(1) guiding the self-attention between patches while (2) summarizing the infor-\nmation useful to the linear classiﬁer. Our proposal is to explicitly separate the\ntwo stages, in the spirit of an encoder-decoder architecture, see Section 6.\nLater class token. As an intermediate step towards our proposal, we insert\nthe so-called class token, denoted by CLS, later in the transformer. This choice\neliminates the discrepancy on the ﬁrst layers of the transformer, which are\ntherefore fully employed for performing self-attention between patches only.\nAs a baseline that does not suffer from the contradictory objectives, we also\nconsider average poolingof all the patches on output of the transformers, as\ntypically employed in convolutional architectures.\nArchitecture. Our CaiT network consists of two distinct processing stages\nvisible in Figure 2:\n1. The self-attention stage is identical to the ViT transformer, but with no\nclass embedding (CLS).\n2. The class-attention stage is a set of layers that compiles the set of patch\nembeddings into a class embedding CLS that is subsequently fed to a\nlinear classiﬁer.\nThis class-attention alternates in turn a layer that we refer to as a multi-head\nclass-attention (CA), and a FFN layer. In this stage, only the class embedding is\nupdated. Similar to the one fed in ViT and DeiT on input of the transformer, it\nis a learnable vector. The main difference is that, in our architecture, we do no\ncopy information from the class embedding to the patch embeddings during\nthe forward pass. Only the class embedding is updated by residual in the CA\nand FFN processing of the class-attention stage.\n5\nS A \nFFN \nS A \nFFN \nS A \nFFN \nS A \nFFN \nclass \nS A \nFFN \nS A \nFFN \nS A \nFFN \nS A \nFFN \nclass \nCLS \nCLS \nS A \nFFN \nS A \nFFN \nCA \nFFN \nCA \nFFN \nclass \nCLS \nclass- attention self - attention \nFigure 2: In the ViT transformer (left), the class embedding (CLS) is inserted along with\nthe patch embeddings. This choice is detrimental, as the same weights are used for two\ndifferent purposes: helping the attention process, and preparing the vector to be fed\nto the classiﬁer. We put this problem in evidence by showing that inserting CLS later\nimproves performance (middle). In the CaiT architecture (right), we further propose to\nfreeze the patch embeddings when inserting CLS to save compute, so that the last part\nof the network (typically 2 layers) is fully devoted to summarizing the information to\nbe fed to the linear classiﬁer.\nMulti-heads class attention. The role of the CA layer is to extract the infor-\nmation from the set of processed patches. It is identical to a SA layer, except\nthat it relies on the attention between (i) the class embedding xclass (initialized\nat CLS in the ﬁrst CA) and (ii) itself plus the set of frozen patch embeddings\nxpatches. We discuss why we include xclass in the keys in Appendix B.\nConsidering a network with hheads and ppatches, and denoting by dthe\nembedding size, we parametrize the multi-head class-attention with several\nprojection matrices, Wq,Wk,Wv,Wo ∈ Rd×d, and the corresponding biases\nbq,bk,bv,bo ∈Rd. With this notation, the computation of the CA residual block\nproceeds as follows. We ﬁrst augment the patch embeddings (in matrix form)\nas z = [xclass,xpatches] (see Appendix B for results when z = xpatches). We then\nperform the projections:\nQ= Wq xclass + bq, (5)\nK = Wk z+ bk, (6)\nV = Wv z+ bv. (7)\n6\nThe class-attention weights are given by\nA= Softmax(Q.KT /\n√\nd/h) (8)\nwhere Q.KT ∈Rh×1×p. This attention is involved in the weighted sum A×V\nto produce the residual output vector\noutCA = Wo AV + bo, (9)\nwhich is in turn added to xclass for subsequent processing.\nThe CA layers extract the useful information from the patches embedding\nto the class embedding. In preliminary experiments, we empirically observed\nthat the ﬁrst CA and FFN give the main boost, and a set of 2 blocks of layers\n(2 CA and 2 FFN) is sufﬁcient to cap the performance. In the experimental sec-\ntion, we denote by 12+2 a transformer when it consists of 12 blocks of SA+FFN\nlayers and 2 blocks of CA+FFN layers.\nComplexity. The layers contain the same number of parameters in the class-\nattention and self-attention stages: CA is identical to SA in that respect, and we\nuse the same parametrization for the FFNs. However the processing of these\nlayers is much faster: the FFN only processes matrix-vector multiplications.\nThe CA function is also less expensive than SA in term of memory and\ncomputation because it computes the attention between the class vector and\nthe set of patch embeddings: Q∈Rd means that Q.KT ∈Rh×1×p. In contrast,\nin the “regular self-attention” layers SA, we have Q ∈ Rp×d and therefore\nQ.KT ∈ Rh×p×p. In other words, the initially quadratic complexity in the\nnumber of patches becomes linear in our extra CaiT layers.\n4 Experiments\nIn this section, we report our experimental results related to LayerScale and\nCaiT. We ﬁrst study strategies to train at deeper scale in Section 4.1, including\nour LayerScale method. Section 4.2 shows the interest of our class-attention\ndesign. We present our models in Subsection 4.3. Section 4.4 details our results\non Imagenet and Transfer learning. We provide an ablation of hyper-parameter\nand ingredients in Section 4.5. Note, in Appendix A and B, we provide varia-\ntions on our methods and corresponding results.\nExperimental setting. Our implementation is based on the Timm library [68].\nUnless speciﬁed otherwise, for this analysis we make minimal changes to hyper-\nparameters compared to the DeiT training scheme [63]. In order to speed up\ntraining and optimize memory consumption we have used a sharded training\nprovided by the Fairscale library2 with fp16 precision.\n2https://pypi.org/project/fairscale/\n7\n4.1 Preliminary analysis with deeper architectures\nIn our early experiments, we observe that Vision Transformers become increas-\ningly more difﬁcult to train when we scale architectures. Depth is one of the\nmain source of instability. For instance the DeiT procedure [63] fails to properly\nconverge above 18 layers without adjusting hyper-parameters. Large ViT [19]\nmodels with 24 and 32 layers were trained with large training datasets, but\nwhen trained on Imagenet only the larger models are not competitive.\nIn the following, we analyse various ways to stabilize the training with\ndifferent architectures. At this stage we consider a Deit-Small model 3 during\n300 epochs to allow a direct comparison with the results reports by Touvronet\nal. [63]. We measure the performance on the Imagenet1k [17, 54] classiﬁcation\ndataset as a function of the depth.\n4.1.1 Adjusting the drop-rate of stochastic depth.\nThe ﬁrst step to improve convergence is to adapt the hyper-parameters that\ninteract the most with depth, in particular Stochastic depth [33]. This method\nis already popular in NLP [21, 22] to train deeper architectures. For ViT, it\nwas ﬁrst proposed by Wightman et al.[68] in the Timm implementation, and\nsubsequently adopted in DeiT [63]. The per-layer drop-rate depends linearly\non the layer depth, but in our experiments this choice does not provide an\nadvantage compared to the simpler choice of a uniform drop-ratedr. In Table 1\nwe show that the default stochastic depth of DeiT allows us to train up to 18\nblocks of SA+FFN. After that the training becomes unstable. By increasing\nthe drop-rate hyper-parameter dr, the performance increases until 24 layers. It\nsaturates at 36 layers (we measured that it drops to 80.7% at 48 layers).\n4.1.2 Comparison of normalization strategies\nWe carry out an empirical study of the normalization methods discussed in\nSection 2. As previously indicated, Rezero, Fixup and T-Fixup do not con-\nverge when training DeiT off-the-shelf. However, if we re-introduce Layer-\nNorm4 and warmup, Fixup and T-Fixup achieve congervence and even im-\nprove training compared to the baseline DeiT. We report the results for these\n“adaptations” of Fixup and T-Fixup in Table 1.\nThe modiﬁed methods are able to converge with more layers without sat-\nurating too early. ReZero converges, we show (column α = ε) that it is better\nto initialize αto a small value instead of 0, as in LayerScale. All the methods\nhave a beneﬁcial effect on convergence and they tend to reduce the need for\nstochastic depth, therefore we adjust these drop rate accordingly per method.\nFigure 3 provides the performance as the function of the drop ratedr for Layer-\nScale. We empirically use the following formula to set up the drop-rate for the\n3https://github.com/facebookresearch/deit\n4Bachlechner et al.report that batchnorm is complementary to ReZero, while removing Layer-\nNorm in the case of transformers.\n8\nTable 1: Improving convergence at depthon ImageNet-1k. The baseline is DeiT-S with\nuniform drop rate of d = 0.05 (same expected drop rate and performance as progres-\nsive stochastic depth of 0.1). Several methods include a ﬁx scalar learnable weight α\nper layer as in Figure 1(c). We have adapted Rezero, Fixup, T-Fixup, since the origi-\nnal methods do not converge: we have re-introduced the Layer-normalization η and\nwarmup. We have adapted the drop rate dr for all the methods, including the baseline.\nThe column α = εreports the performance when initializing the scalar with the same\nvalue as for LayerScale. †: failed before the end of the training.\ndepth baseline scalar αweighting LayerScale\ndr = 0.05 adjust [dr] Rezero T-Fixup Fixup α= ε\n12 79.9 79.9 [0.05] 78.3 79.4 80.7 80.4 80.5\n18 80.1 80.7 [0.10] 80.1 81.7 82.0 81.6 81.7\n24 78.9† 81.0[0.20] 80.8 81.5 82.3 81.1 82.4\n36 78.9† 81.9[0.25] 81.6 82.1 82.4 81.6 82.9\n0.0 0.1 0.2 0.3 0.4\nstochastic depth coefficient\n78\n80\n82top-1 accuracy (%)\n12 blocks\n18 blocks\n24 blocks\n36 blocks\n48 blocks\nFigure 3: We measure the impact of stochastic depth on ImageNet with a DeiT-S with\nLayerScale for different depths. The drop rate of stochastic depth needs to be adapted\nto the network depth.\nCaiT-S models derived from on Deit-S: dr = min(0.1 ×depth\n12 −1,0).This formu-\nlaic choice avoids cross-validating this parameter and overﬁtting, yet it does\nnot generalize to models with different d: We further increase (resp. decrease)\nit by a constant for larger (resp. smaller) working dimensionality d.\nFixup and T-Fixup are competitive with LayerScale in the regime of a rel-\natively low number of blocks (12–18). However, they are more complex than\nLayerScale: they employ different initialization rules depending of the type of\nlayers, and they require more changes to the transformer architecture. There-\nfore we only use LayerScale in subsequent experiments. It is much simpler and\nparametrized by a single hyper-parameter ε, and it offers a better performance\nfor the deepest models that we consider, which are also the more accurate.\n9\n0 10 20 30\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0 10 20 30\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0 10 20 30\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0 10 20 30\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nFigure 4: Analysis of the contribution of the residual branches (Top: Self-attention ; Bot-\ntom: FFN) for a network comprising 36 layers, without (red) or with (blue) Layerscale.\nThe ratio between the norm of the residual and the norm of the main branch is shown\nfor each layer of the transformer and for various epochs (darker shades correspond to\nthe last epochs). For the model trained with layerscale, the norm of the residual branch\nis on average 20% of the norm of the main branch. We observe that the contribution\nof the residual blocks ﬂuctuates more for the model trained without layerscale and in\nparticular is lower for some of the deeper layers.\n4.1.3 Analysis of Layerscale\nStatistics of branch weighting. We evaluate the impact of Layerscale for a\n36-blocks transformer by measuring the ratio between the norm of the residual\nactivations and the norm of the activations of the main branch ∥gl(x)∥2/∥x∥2.\nThe results are shown in Figure 4. We can see that training a model with Layer-\nscale makes this ratio more uniform across layers, and seems to prevent some\nlayers from having a disproportionate impact on the activations. Similar to\nprior works [2, 75] we hypothetize that the beneﬁt is mostly the impact on op-\ntimization. This hypothesis is supported by the control experiment that we\ndetail in Appendix A.\n10\nTable 2: Variations on CLS with Deit-Small (no LayerScale): we change the layer at\nwhich the class embedding is inserted. In ViT and DeiT, it is inserted at layer 0 jointly\nwith the projected patches. We evaluate a late insertion of the class embedding, as well\nas our design choice to introduce speciﬁc class-attention layers.\ndepth: SA+CA insertion layer top-1 acc. #params FLOPs\nBaselines: DeiT-S and average pooling\n12: 12 + 0 0 79.9 22M 4.6B\n12: 12 + 0 n/a 80.3 22M 4.6B\nLate insertion of class embedding\n12: 12 + 0 2 80.0 22M 4.6B\n12: 12 + 0 4 80.0 22M 4.6B\n12: 12 + 0 8 80.0 22M 4.6B\n12: 12 + 0 10 80.5 22M 4.6B\n12: 12 + 0 11 80.3 22M 4.6B\nDeiT-S with class-attention stage (SA+FFN)\n12: 9 + 3 9 79.6 22M 3.6B\n12: 10 + 2 10 80.3 22M 4.0B\n12: 11 + 1 11 80.6 22M 4.3B\n13: 12 + 1 12 80.8 24M 4.7B\n14: 12 + 2 12 80.8 26M 4.7B\n15: 12 + 3 12 80.6 27M 4.8B\n4.2 Class-attention layers\nIn Table 2 we study the impact on performance of the design choices related\nto class embedding. We depict some of them in Figure 2. As a baseline, aver-\nage pooling of patches embeddings with a vanilla DeiT-Small achieves a better\nperformance than using a class token. This choice, which does not employ any\nclass embedding, is typical in convolutional networks, but possibly weaker\nwith transformers when transferring to other tasks [20].\nLate insertion. The performance increases when we insert the class embed-\nding later in the transformer. It is maximized two layers before the output.\nOur interpretation is that the attention process is less perturbed in the 10 ﬁrst\nlayers, yet it is best to keep 2 layers for compiling the patches embedding into\nthe class embedding via class-attention, otherwise the processing gets closer to\na weighted average.\nOur class-attention layers are designed on the assumption that there is no\nbeneﬁt in copying information from the class embedding back to the patch\nembeddings in the forward pass. Table 2 supports that hypothesis: if we com-\npare the performance for a total number of layers ﬁxed to 12, the performance\nof CaiT with 10 SA and 2 CA layers is identical to average pooling and better\n11\nTable 3: CaiT models: The design parameters are depth and d. The mem columns cor-\nrespond to the memory usage. All models are initially trained at resolution 224 during\n400 epochs. We also ﬁne-tune these models at resolution 384 (identiﬁed by↑384) or train\nthem with distillation (Υ). The FLOPs are reported for each resolution.\nCAIT depth d #params FLOPs (×109) Top-1 acc. (%): Imagenet1k-val\nmodel (SA+CA) ( ×106) @224 @384 @224 ↑384 @224Υ ↑384Υ\nXXS-24 24 + 2 192 12.0 2.5 9.5 77.6 80.4 78.4 80.9\nXXS-36 36 + 2 192 17.3 3.8 14.2 79.1 81.8 79.7 82.2\nXS-24 24 + 2 288 26.6 5.4 19.3 81.8 83.8 82.0 84.1\nXS-36 36 + 2 288 38.6 8.1 28.8 82.6 84.3 82.9 84.8\nS-24 24 + 2 384 46.9 9.4 32.2 82.7 84.3 83.5 85.1\nS-36 36 + 2 384 68.2 13.9 48.0 83.3 85.0 84.0 85.4\nS-48 48 + 2 384 89.5 18.6 63.8 83.5 85.1 83.9 85.3\nM-24 24 + 2 768 185.9 36.0 116.1 83.4 84.5 84.7 85.8\nM-36 36 + 2 768 270.9 53.7 173.3 83.8 84.9 85.1 86.1\nTable 4: Hyper-parameters for training CaiT models: The only parameters that we ad-\njust per model are the drop ratedr of stochastic depth and the LayerScale initializationε.\nCAIT modelXXS-24 XXS-36 XS-24 XS-36 S-24 S-36 S-48 M-24 M-36 M-48\nhparamsdr 0.05 0.1 0.05 0.1 0.1 0.2 0.3 0.2 0.3 0.4\nε 10−5 10−6 10−5 10−6 10−5 10−6 10−6 10−5 10−6 10−6\nthan the DeiT-Small baseline with a lower number of FLOPs. If we set 12 lay-\ners in the self-attention stage, which dominates the complexity, we increase the\nperformance signiﬁcantly by adding two blocks of CA+FFN.\n4.3 Our CaiT models\nOur CaiT models are built upon ViT: the only difference is that we incorporate\nLayerScale in each residual block (see Section 2) and the two-stages architec-\nture with class-attention layers described in Section 3. Table 3 describes our\ndifferent models. The design parameters governing the capacity are the depth\nand the working dimensionality d. In our case d is related to the number of\nheads h as d = 48 ×h, since we ﬁx the number of components per headto\n48. This choice is a bit smaller than the value used in DeiT. We also adopt the\ncrop-ratio of 1.0 optimized for DeiT by Wightman [68]. Table 9 and 10 in the\nablation section 4.5 support these choices.\nWe incorporate talking-heads attention [55] into our model. It increases the\nperformance on Imagenet of DeiT-Small from 79.9% to 80.3%.\nThe hyper-parameters are identical to those provided in DeiT [63], except\nmentioned otherwise. We use a batch size of 1024 samples and train during 400\n12\nepochs with repeated augmentation [5, 29]. The learning rate of the AdamW\noptimizer [44] is set to 0.001 and associated with a cosine training schedule, 5\nepochs of warmup and a weight decay of 0.05. We report in Table 4 the two\nhyper-parameters that we modify depending on the model complexity, namely\nthe drop ratedr associated with uniform stochastic depth, and the initialization\nvalue εof LayerScale.\nFine-tuning at higher resolution (↑) and distillation (Υ). We train all our\nmodels at resolution 224, and optionally ﬁne-tune them at a higher resolution\nto trade performance against accuracy [19, 63, 64]: we denote the model by\n↑384 models ﬁne-tuned at resolution 384 ×384. We also train models with dis-\ntillation (Υ) as suggested by Touvron et al.. [63]. We use a RegNet-16GF [50] as\nteacher and adopt the “hard distillation” [63] for its simplicity.\n4.4 Results\n4.4.1 Performance/complexity of CaiT models\nTable 3 provides different complexity measures for our models. As a general\nobservation, we observe a subtle interplay between the width and the depth,\nboth contribute to the performance as reported by Dosovitskiy et al.. [19] with\nlonger training schedules. But if one parameter is too small the gain brought\nby increasing the other is not worth the additional complexity.\nFine-tuning to size 384 (↑) systematically offers a large boost in performance\nwithout changing the number of parameters. It also comes with a higher com-\nputational cost. In contrast, leveraging a pre-trained convnet teacher with hard\ndistillation as suggested by Touvron et al. [63] provides a boost in accuracy\nwithout affecting the number of parameters nor the speed.\n4.4.2 Comparison with the state of the art on Imagenet\nOur main classiﬁcation experiments are carried out on ImageNet [54], and also\nevaluated on two variations of this dataset: ImageNet-Real [6] that corrects and\ngive a more detailed annotation, and ImageNet-V2 [52] (matched frequency)\nthat provides a separate test set. In Table 5 we compare some of our mod-\nels with the state of the art on Imagenet classiﬁcation when training without\nexternal data. We focus on the models CaiT-S36 and CaiT-M36, at different\nresolutions and with or without distillation.\nOn Imagenet1k-val, CaiT-M48↑448Υ achieves 86.5% of top-1 accuracy, which\nis a signiﬁcant improvement over DeiT (85.2%). It is the state of the art, on par\nwith a recent concurrent work [8] that has a signiﬁcantly higher number of\nFLOPs. Our approach outperforms the state of the art on Imagenet with re-\nassessed labels, and on Imagenet-V2, which has a distinct validation set which\nmakes it harder to overﬁt.\n13\nTable 5: Complexity vs accuracyon Imagenet [54], Imagenet Real [6] and Imagenet\nV2 matched frequency [52] for models trained without external data. We compare\nCaiT with DeiT [63], Vit-B [19], TNT [26], T2T [74] and to several state-of-the-art\nconvnets: Regnet [50] improved by Touvron et al. [63], EfﬁcientNet [14, 62, 71], Fix-\nEfﬁcientNet [65] and NFNets [8]. Most reported results are from corresponding papers,\nand therefore the training procedure differs for the different models. For Imagenet V2\nmatched frequency and Imagenet Real we report the results provided by the authors.\nWhen not available (like NFNet), we report the results measured by Wigthman [68]\nwith converted models, which may be suboptimal. The RegNetY-16GF is the teacher\nmodel that we trained for distillation. We report the best result in bold and the second\nbest result(s) underlined.\nnb of nb of image size ImNet Real V2\nNetwork param. FLOPs train test top-1 top-1 top-1\nRegNetY-16GF 84M 16.0B 224 224 82.9 88.1 72.4\nEfﬁcientNet-B5 30M 9.9B 456 456 83.6 88.3 73.6\nEfﬁcientNet-B7 66M 37.0B 600 600 84.3\nEfﬁcientNet-B5 RA 30M 9.9B 456 456 83.7\nEfﬁcientNet-B7 RA 66M 37.0B 600 600 84.7\nEfﬁcientNet-B7 AdvProp 66M 37.0B 600 600 85.2 89.4 76.0\nFix-EfﬁcientNet-B8 87M 89.5B 672 800 85.7 90.0 75.9\nNFNet-F0 72M 12.4B 192 256 83.6 88.1 72.6\nNFNet-F1 133M 35.5B 224 320 84.7 88.9 74.4\nNFNet-F2 194M 62.6B 256 352 85.1 88.9 74.3\nNFNet-F3 255M 114.8B 320 416 85.7 89.4 75.2\nNFNet-F4 316M 215.3B 384 512 85.9 89.4 75.2\nNFNet-F5 377M 289.8B 416 544 86.0 89.2 74.6\nNFNet-F6+SAM 438M 377.3B 448 576 86.5 89.9 75.8\nTransformers\nViT-B/16 86M 55.4B 24 384 77.9 83.6\nViT-L/16 307M 190.7B 224 384 76.5 82.2\nT2T-ViT t-14 21M 5.2B 224 224 80.7\nTNT-S 24M 5.2B 224 224 81.3\nTNT-S + SE 25M 5.2B 224 224 81.6\nTNT-B 66M 14.1B 224 224 82.8\nDeiT-S 22M 4.6B 224 224 79.8 85.7 68.5\nDeiT-B 86M 17.5B 224 224 81.8 86.7 71.5\nDeiT-B↑384 86M 55.4B 224 384 83.1 87.7 72.4\nDeiT-B↑384Υ1000 epochs 87M 55.5B 224 384 85.2 89.3 75.2\nOur deep transformers\nCaiT-S36 68M 13.9B 224 224 83.3 88.0 72.5\nCaiT-S36↑384 68M 48.0B 224 384 85.0 89.2 75.0\nCaiT-S48↑384 89M 63.8B 224 384 85.1 89.5 75.5\nCaiT-S36Υ 68M 13.9B 224 224 84.0 88.9 74.1\nCaiT-S36↑384Υ 68M 48.0B 224 384 85.4 89.8 76.2\nCaiT-M36↑384Υ 271M 173.3B 224 384 86.1 90.0 76.3\nCaiT-M36↑448Υ 271M 247.8B 224 448 86.3 90.2 76.7\nCaiT-M48↑448Υ 356M 329.6B 224 448 86.5 90.2 76.9\n14\n↑\n↑\n↑\n↑\nFigure 5: We represent FLOPs and parameters for our best CaiT ↑ 384 and ↑ 448 Υ\nmodels trained with distillation. They are competitive on ImageNet-1k-val with the sota\nin the high accuracy regime, from XS-24 to M-48. Convolution-based neural networks\nlike NFNets and EfﬁcientNet are better in low-FLOPS and low-parameters regimes.\nTable 6: Datasets used for our different tasks.\nDataset Train size Test size #classes\nImageNet [54] 1,281,167 50,000 1000\niNaturalist 2018 [30] 437,513 24,426 8,142\niNaturalist 2019 [31] 265,240 3,003 1,010\nFlowers-102 [46] 2,040 6,149 102\nStanford Cars [38] 8,144 8,041 196\nCIFAR-100 [39] 50,000 10,000 100\nCIFAR-10 [39] 50,000 10,000 10\n4.4.3 Transfer learning\nWe evaluated our method on transfer learning tasks by ﬁne-tuning on the\ndatasets in Table 6.\nFine-tuning procedure. For ﬁne-tuning we use the same hyperparameters\nas for training. We only decrease the learning rates by a factor 10 (for CARS,\nFlowers, iNaturalist), 100 (for CIFAR-100, CIFAR-10) and adapt the number\nof epochs (1000 for CIFAR-100, CIFAR-10, Flowers-102 and Cars-196, 360 for\niNaturalist 2018 and 2019). We have not used distillation for this ﬁnetuning.\nResults. Table 7 compares CaiT transfer learning results to those of Efﬁcient-\nNet [62], ViT [19] and DeiT [63]. These results show the excellent generalization\nof the transformers-based models in general. Our CaiT models achieve excel-\nlent results, as shown by the overall better performance than EfﬁcientNet-B7\nacross datasets.\n15\nTable 7: Results in transfer learning. All models are trained and evaluated at resolution\n224 and with a crop-ratio of 0.875 in this comparison (see Table 10 for the comparison\nof crop-ratio on Imagenet).\nModel\nImageNet\nCIFAR-10\nCIFAR-100\nFlowers\nCars\niNat-18\niNat-19\nFLOPs\nEfﬁcientNet-B7 84.3 98.9 91.7 98.8 94.7 37.0B\nViT-B/16 77.9 98.1 87.1 89.5 55.5B\nViT-L/16 76.5 97.9 86.4 89.7 190.7B\nDeit-B 224 81.8 99.1 90.8 98.4 92.1 73.2 77.7 17.5B\nCaiT-S-36 224 83.4 99.2 92.2 98.8 93.5 77.1 80.6 13.9B\nCaiT-M-36 224 83.7 99.3 93.3 99.0 93.5 76.9 81.7 53.7B\nCaiT-S-36Υ 224 83.7 99.2 92.2 99.0 94.1 77.0 81.4 13.9B\nCaiT-M-36Υ 224 84.8 99.4 93.1 99.1 94.2 78.0 81.8 53.7B\n4.5 Ablation\nIn this section we provide different sets of ablation, in the form of a transition\nfrom DeiT to CaiT. Then we provide experiments that have guided our hyper-\nparameter optimization. As mentioned in the main paper, we use the same\nhyperparameters as in DeiT [63] everywhere except stated otherwise. We have\nonly changed the number of attention for a given working dimension (see Sec-\ntion 4.5.2), and changed the crop-ratio (see Section 4.5.3).\n4.5.1 Step by step from DeiT-Small to CaiT-S36\nIn Table 8 we present how to gradually transform the Deit-S [63] architecture\ninto CaiT-36, and measure at each step the performance/complexity changes.\nOne can see that CaiT is complementary with LayerScale and offers an im-\nprovement without signiﬁcantly increasing the FLOPs. As already reported in\nthe literature, the resolution is another important step for improving the per-\nformance and ﬁne-tuning instead of training the model from scratch saves a\nlot of computation at training time. Last but not least, our models beneﬁt from\nlonger training schedules.\n4.5.2 Optimization of the number of heads\nIn Table 9 we study the impact of the number of heads for a ﬁxed working\ndimensionality. This architectural parameter has an impact on both the accu-\nracy, and the efﬁciency: while the number of FLOPs remain roughly the same,\nthe compute is more fragmented when increasing this number of heads and on\ntypical hardware this leads to a lower effective throughput. Choosing 8 heads\n16\nTable 8: Ablation: we present the ablation path from DeiT-S to our CaiT models. We\nhighlight the complementarity of our approaches and optimized hyper-parameters.\nNote, Fine-tuning at higher resolution supersedes the inference at higher resolution.\nSee Table 1 for adapting stochastic depth before adding LayerScale. †: training failed.\nImprovement top-1 acc. #params FLOPs\nDeiT-S[d=384,300 epochs] 79.9 22M 4.6B\n+ More heads[8] 80.0 22M 4.6B\n+ Talking-heads 80.5 22M 4.6B\n+ Depth[36 blocks] 69.9† 64M 13.8B\n+ Layer-scale[initε= 10−6] 80.5 64M 13.8B\n+ Stch depth. adaptation[dr=0.2] 83.0 64M 13.8B\n+ CaiT architecture[specialized class-attention layers] 83.2 68M 13.9B\n+ Longer training[400 epochs] 83.4 68M 13.9B\n+ Inference at higher resolution[256] 83.8 68M 18.6B\n+ Fine-tuning at higher resolution[384] 84.8 68M 48.0B\n+ Hard distillation[teacher: RegNetY-16GF] 85.2 68M 48.0B\n+ Adjust crop ratio[0.875→1.0] 85.4 68M 48.0B\nTable 9: Deit-Small: for a ﬁxed 384 working dimensionality and number of parameters,\nimpact of the number of heads on the accuracy and throughput (images processed per\nsecond at inference time on a singe V100 GPU).\n# heads dim/head throughput (im/s) GFLOPs top-1 acc.\n1 384 1079 4.6 76.80\n2 192 1056 4.6 78.06\n3 128 1043 4.6 79.35\n6 64 989 4.6 79.90\n8 48 971 4.6 80.02\n12 32 927 4.6 80.08\n16 24 860 4.6 80.04\n24 16 763 4.6 79.60\nin the self-attention offers a good compromise between accuracy and speed. In\nDeit-Small, this parameter was set to 6.\n4.5.3 Adaptation of the crop-ratio\nIn the typical (“center-crop”) evaluation setting, most convolutional neural net-\nworks crop a subimage with a given ratio, typically extracting a224×224 center\ncrop from a256×256 resized image, leading to the typical ratio of 0.875. Wight-\nman et al.[68] notice that setting this crop ratio to 1.0 for transformer models\nhas a positive impact: the distilled DeiT-B ↑ 384 reaches a top1-accuracy on\nImagenet1k-val of 85.42% in this setting, which is a gain of +0.2% compared to\nthe accuracy of 85.2% reported by Touvronet al.[63].\nOur measurements concur with this observation: We observe a gain for\nalmost all our models and most of the evaluation benchmarks. For instance\n17\nTable 10: We compare performance with the defaut crop-ratio of 0.875 usually used with\nconvnets, and the crop-ratio of 1.0 [68] that we adopt for CaiT.\nNetwork Crop Ratio ImNet Real V2\n0.875 1.0 top-1 top-1 top-1\nS36 ✓ 83.4 88.1 73.0\n✓ 83.3 88.0 72.5\nS36↑384 ✓ 84.8 88.9 74.7\n✓ 85.0 89.2 75.0\nS36Υ ✓ 83.7 88.9 74.1\n✓ 84.0 88.9 74.1\nM36Υ ✓ 84.8 89.2 74.9\n✓ 84.9 89.2 75.0\nS36↑384Υ ✓ 85.2 89.7 75.7\n✓ 85.4 89.8 76.2\nM36↑384Υ ✓ 85.9 89.9 76.1\n✓ 86.1 90.0 76.3\nour model M36↑384Υ increases to 86.1% top-1 accuracy on Imagenet-val1k.\n4.5.4 Longer training schedules\nAs shown in Table 8 , increasing the number of training epochs from 300 to\n400 improves the performance of CaiT-S-36. However, increasing the number\nof training epochs from 400 to 500 does not change performance signiﬁcantly\n(83.44 with 400 epochs 83.42 with 500 epochs). This is consistent with the ob-\nservation of the DeiT [63] paper, which notes a saturation of performance from\n400 epochs for the models trained without distillation.\n5 Visualizations\n5.1 Attention map\nIn Figure 6 we show the attention maps associated with the individual 4 heads\nof a XXS CaiT model, and for the two layers of class-attention. In CaiT and\nin contrast to ViT, the class-attention stage is the only one where there is some\ninteraction between the class token and the patches, therefore it conveniently\nconcentrates all the spatial-class relationship. We make two observations:\n• The ﬁrst class-attention layer clearly focuses on the object of interest, cor-\nresponding to the main part of the image on which the classiﬁcation de-\ncision is performed (either correct or incorrect). In this layer, the different\nheads focus either on the same or on complementary parts of the objects.\nThis is especially visible for the waterfall image;\n18\nHead 1 ↓ Head 2 ↓ Head 3 ↓ Head 4 ↓\nFigure 6: Visualization of the attention maps in the class-attention stage, obtained with\na XXS model. For each image we present two rows: the top row correspond to the\nfour heads of the attention maps associated with the ﬁrst CA layer. The bottom row\ncorrespond to the four heads of the second CA layer.\n19\nfountain (57%)\nAmerican alligator (77%)\nAfrican chameleon (24%), leaf beetle (12%)\nlakeside (40%), alp (19%), valley(17%)\nambulance (24%), trafﬁc light (23%)\nbarbershop (84%)\nsoap dispenser (40%), lavabo (39%)\n70 % volcano\nracket (73%), tennis ball (10%)\ngolf ball (71%)\nAfrican elephant (57%), water buffalo (15%)\nviaduct (87%)\nbaboon (22%), black bear (17%), hyena (16%)\nconvertible (27%), taxi (15%), sport car (12%), wagon (11%)\ncatamaran (61%)\nbarrow (50%), plow (26%)\nmonarch butterﬂy (80%)\nminibus (21%), recreational vehicle (18%)\ncup (43%), notebook computer (19%)\nairliner (83%)\nlakeside (24%), coral fungus (16%), coral reef (10%)\nplate (50%), carbonara (18%)\nﬂagpole (14%), dam (11%)\ntelevision (69%)\nFigure 7: Illustration of the regions of focus of a CaiT-XXS model, according to the\nresponse of the ﬁrst class-attention layer.\n20\n• The second class-attention layer seems to focus more on the context, or at\nleast the image more globally.\n5.2 Illustration of saliency in class-attention\nIn ﬁgure 7 we provide more vizualisations for a XXS model. They are just\nillustration of the saliency that one may extract from the ﬁrst class-attention\nlayer. As discussed previously this layer is the one that, empirically, is the\nmost related to the object of interest. To produce these visual representations\nwe simply average the attention maps from the different heads (depicted in\nFigure 6), and upsample the resulting map to the image size. We then modulate\nthe gray-level image with the strength of the attention after normalizing it with\na simple rule of the form (x−xmin)/(xmax −xmin). We display the resulting\nimage with cividis colormap.\nFor each image we show this saliency map and provides all the class for\nwhich the model assigns a probability higher than 10%. These visualizations\nillustrate how the model can focus on two distinct regions (like racket and ten-\nnis ball on the top row/center). We can also observe some failure cases, like\nthe top of the church classiﬁed as a ﬂagpole.\n6 Related work\nSince AlexNet [40], convolutional neural networks (CNN) are the standard in\nimage classiﬁcation [27, 62, 64], and more generally in computer vision. While\na deep CNN can theoretically model long range interaction between pixels\nacross many layers, there has been research in increasing the range of inter-\nactions within a single layer. Some approaches adapt the receptive ﬁeld of\nconvolutions dynamically [15, 42]. At another end of the spectrum, attention\ncan be viewed as a general form of non-local means, which was used in ﬁlter-\ning (e.g. denoising [10]), and more recently in conjunction with convolutions\n[67]. Various other attention mechanism have been used successfully to give\na global view in conjunction with (local) convolutions [4, 51, 12, 76, 78], most\nmimic squeeze-and-excitate [32] for leveraging global features. Lastly, Lamb-\ndaNetworks [3] decomposes attention into an approximated content attention\nand a batch-amortized positional attention component.\nHybrid architectures combining CNNs and transformers blocks have also\nbeen used on ImageNet [57, 69] and on COCO [11]. Originally, transformers\nwithout convolutions were applied on pixels directly [47], even scaling to hun-\ndred of layers [13], but did not perform at CNNs levels. More recently, a trans-\nformer architecture working directly on small patches has obtained state of\nthe art results on ImageNet [19]. Nevertheless, the state of the art has since re-\nturned to CNNs [8, 48]. While some small improvements have been applied on\nthe transformer architecture with encouraging results [74], their performance\nis below the one of DeiT [63], which uses a vanilla ViT architecture.\n21\nEncoder/decoder architectures. Transformers were originally introduced for\nmachine translation [66] with encoder-decoder models, and gained popularity\nas masked language model encoders (BERT) [18, 43]. They yielded impressive\nresults as scaled up language models, e.g. GPT-2 and 3 [49, 9]. They became\na staple in speech recognition too [45, 36], being it in encoder and sequence\ncriterion or encoder-decoder seq2seq [60] conformations, and hold the state of\nthe art to this day [73, 77] with models 36 blocks deep. Note, transforming only\nthe class token with frozen trunk embeddings in CaiT is reminiscent of non-\nautoregressive encoder-decoders [25, 41], where a whole sequence (we have\nonly one prediction) is produced at once by iterative reﬁnements.\nDeeper architectures usually lead to better performance [27, 56, 61], how-\never this complicates their training process [58, 59]. One must adapt the ar-\nchitecture and the optimization procedure to train them correctly. Some ap-\nproaches focus on the initialization schemes [24, 27, 70], others on multiple\nstages training [53, 56], multiple loss at different depth [61], adding compo-\nnents in the architecture [2, 75] or regularization [33]. As pointed in our paper,\nin that respect our LayerScale approach is more related to Rezero [2] and Skip-\ninit [16], Fixup [75], and T-Fixup [34].\n7 Conclusion\nIn this paper, we have shown how train deeper transformer-based image clas-\nsiﬁcation neural networks when training on Imagenet only. We have also in-\ntroduced the simple yet effective CaiT architecture designed in the spirit of\nencoder/decoder architectures. Our work further demonstrates that trans-\nformer models offer a competitive alternative to the best convolutional neural\nnetworks when considering trade-offs between accuracy and complexity.\n8 Acknowledgments\nThanks to Jakob Verbeek for his detailled feedback on an earlier version of\nthis paper, to Alaa El-Nouby for fruitful discussions, to Mathilde Caron for\nsuggestions regarding the vizualizations, and to Ross Wightman for the Timm\nlibrary and the insights that he shares with the community.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016. 2\n[2] Thomas C. Bachlechner, Bodhisattwa Prasad Majumder, H. H. Mao, G. Cottrell,\nand Julian McAuley. Rezero is all you need: Fast convergence at large depth.arXiv\npreprint arXiv:2003.04887, 2020. 2, 3, 4, 10, 22\n22\n[3] Irwan Bello. Lambdanetworks: Modeling long-range interactions without atten-\ntion. In International Conference on Learning Representations, 2021. 21\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. At-\ntention augmented convolutional networks. In Conference on Computer Vision and\nPattern Recognition, 2019. 21\n[5] Maxim Berman, Herv ´e J ´egou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs\nDouze. Multigrain: a uniﬁed image embedding for classes and instances. arXiv\npreprint arXiv:1902.05509, 2019. 13\n[6] Lucas Beyer, Olivier J. H ´enaff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron\nvan den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n3, 13, 14\n[7] Andrew Brock, Soham De, and Samuel L Smith. Characterizing signal prop-\nagation to close the performance gap in unnormalized resnets. arXiv preprint\narXiv:2101.08692, 2021. 2\n[8] A. Brock, Soham De, S. L. Smith, and K. Simonyan. High-performance large-scale\nimage recognition without normalization. arXiv preprint arXiv:2102.06171, 2021. 2,\n13, 14, 21\n[9] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Ka-\nplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 22\n[10] Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local algorithm for image\ndenoising. In Conference on Computer Vision and Pattern Recognition, 2005. 21\n[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander\nKirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In\nEuropean Conference on Computer Vision, 2020. 21\n[12] Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and\nZicheng Liu. Dynamic convolution: Attention over convolution kernels. In Con-\nference on Computer Vision and Pattern Recognition, 2020. 21\n[13] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long se-\nquences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. 2, 4, 21\n[14] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V . Le. Randaugment:\nPractical automated data augmentation with a reduced search space.arXiv preprint\narXiv:1909.13719, 2019. 14\n[15] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen\nWei. Deformable convolutional networks. In Conference on Computer Vision and\nPattern Recognition, 2017. 21\n[16] Soham De and Samuel L Smith. Batch normalization biases residual blocks to-\nwards the identity function in deep networks. arXiv e-prints, pages arXiv–2002,\n2020. 2, 3, 4, 22\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet:\nA large-scale hierarchical image database. In Conference on Computer Vision and\nPattern Recognition, pages 248–255, 2009. 8\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805, 2018. 22\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\n23\nHeigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for im-\nage recognition at scale. International Conference on Learning Representations, 2021.\n2, 3, 8, 13, 14, 15, 21\n[20] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Herv ´e J ´egou. Training\nvision transformers for image retrieval. arXiv preprint arXiv:2102.05644, 2021. 11\n[21] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth\non demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019. ICLR\n2020. 8\n[22] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R ´emi Gribonval,\nHerv´e J ´egou, and Armand Joulin. Training with quantization noise for extreme\nmodel compression. arXiv preprint arXiv:2004.07320, 2020. 8\n[23] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding\nsparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. I\n[24] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep\nfeedforward neural networks. In AISTATS, 2010. 22\n[25] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-\nautoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017.\n22\n[26] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.\nTransformer in transformer. arXiv preprint arXiv:2103.00112, 2021. 14\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\nfor image recognition. In Conference on Computer Vision and Pattern Recognition,\nJune 2016. 1, 21, 22\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in\ndeep residual networks. arXiv preprint arXiv:1603.05027, 2016. 2\n[29] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel\nSoudry. Augment your batch: Improving generalization through instance repeti-\ntion. In Conference on Computer Vision and Pattern Recognition, 2020. 13\n[30] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig\nAdam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2018 dataset.\narXiv preprint arXiv:1707.06642, 2018. 15\n[31] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig\nAdam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2019 dataset.\narXiv preprint arXiv:1707.06642, 2019. 15\n[32] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. arXiv preprint\narXiv:1709.01507, 2017. 21\n[33] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep\nnetworks with stochastic depth. In European Conference on Computer Vision, 2016. 8,\n22\n[34] Xiao Shi Huang, Felipe Perez, Jimmy Ba, and Maksims Volkovs. Improving trans-\nformer optimization through better initialization. InInternational Conference on Ma-\nchine Learning, pages 4475–4483. PMLR, 2020. 2, 4, 22\n[35] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep net-\nwork training by reducing internal covariate shift. In International Conference on\nMachine Learning, 2015. 4\n[36] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, et al. A comparative study on\ntransformer vs rnn in speech applications. arXiv preprint arXiv:1909.06317, 2019.\n22\n24\n[37] Diederik P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible\n1x1 convolutions. arXiv preprint arXiv:1807.03039, 2018. 4\n[38] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations\nfor ﬁne-grained categorization. In 4th International IEEE Workshop on 3D Represen-\ntation and Recognition (3dRR-13), 2013. 15\n[39] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical\nreport, CIFAR, 2009. 15\n[40] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation\nwith deep convolutional neural networks. In NIPS, 2012. 21\n[41] Jason Lee, Elman Mansimov, and Kyunghyun Cho. Deterministic non-\nautoregressive neural sequence modeling by iterative reﬁnement. arXiv preprint\narXiv:1802.06901, 2018. 22\n[42] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks.\nConference on Computer Vision and Pattern Recognition, 2019. 21\n[43] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly\noptimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 22\n[44] I. Loshchilov and F. Hutter. Fixing weight decay regularization in adam. arXiv\npreprint arXiv:1711.05101, 2017. 13\n[45] Christoph L ¨uscher, Eugen Beck, Kazuki Irie, et al. Rwth asr systems for librispeech:\nHybrid vs attention. Interspeech 2019, Sep 2019. 22\n[46] M-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large\nnumber of classes. In Proceedings of the Indian Conference on Computer Vision, Graph-\nics and Image Processing, 2008. 15\n[47] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,\nAlexander Ku, and Dustin Tran. Image transformer. In International Conference\non Machine Learning, pages 4055–4064. PMLR, 2018. 21\n[48] H. Pham, Qizhe Xie, Zihang Dai, and Quoc V . Le. Meta pseudo labels. arXiv\npreprint arXiv:2003.10580, 2020. 21\n[49] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. Language models are unsupervised multitask learners. 22\n[50] Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr\nDoll´ar. Designing network design spaces.Conference on Computer Vision and Pattern\nRecognition, 2020. 1, 13, 14\n[51] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, I. Bello, Anselm Levskaya,\nand Jonathon Shlens. Stand-alone self-attention in vision models. In Neurips, 2019.\n21\n[52] B. Recht, Rebecca Roelofs, L. Schmidt, and V . Shankar. Do imagenet classiﬁers\ngeneralize to imagenet? arXiv preprint arXiv:1902.10811, 2019. 3, 13, 14\n[53] A. Romero, Nicolas Ballas, S. Kahou, Antoine Chassang, C. Gatta, and Yoshua\nBengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2015. 22\n[54] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan-\nder C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. In-\nternational journal of Computer Vision, 2015. 3, 8, 13, 14, 15\n[55] Noam Shazeer, Zhenzhong Lan, Youlong Cheng, N. Ding, and L. Hou. Talking-\nheads attention. arXiv preprint arXiv:2003.02436, 2020. 12\n[56] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale\nimage recognition. In International Conference on Learning Representations, 2015. 22\n25\n[57] A. Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, P . Abbeel, and\nAshish Vaswani. Bottleneck transformers for visual recognition. arXiv preprint\narXiv:2101.11605, 2021. 21\n[58] R. Srivastava, Klaus Greff, and J. Schmidhuber. Highway networks. arXiv preprint\narXiv:1505.00387, 2015. 22\n[59] R. Srivastava, Klaus Greff, and J. Schmidhuber. Training very deep networks. In\nNIPS, 2015. 22\n[60] Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard\nGrave, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, and Ronan Collobert.\nEnd-to-end asr: from supervised to semi-supervised learning with modern archi-\ntectures. arXiv preprint arXiv:1911.08460, 2019. 22\n[61] C. Szegedy, Wei Liu, Yangqing Jia, P . Sermanet, S. Reed, D. Anguelov, D. Erhan, V .\nVanhoucke, and A. Rabinovich. Going deeper with convolutions. In Conference on\nComputer Vision and Pattern Recognition, 2015. 22\n[62] Mingxing Tan and Quoc V . Le. Efﬁcientnet: Rethinking model scaling for convo-\nlutional neural networks. arXiv preprint arXiv:1905.11946, 2019. 1, 14, 15, 21\n[63] Hugo Touvron, M. Cord, M. Douze, F. Massa, Alexandre Sablayrolles, and H.\nJ´egou. Training data-efﬁcient image transformers & distillation through attention.\narXiv preprint arXiv:2012.12877, 2020. 3, 7, 8, 12, 13, 14, 15, 16, 17, 18, 21, III\n[64] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the\ntrain-test resolution discrepancy. Neurips, 2019. 13, 21\n[65] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv ´e J ´egou. Fixing the\ntrain-test resolution discrepancy: Fixefﬁcientnet. arXiv preprint arXiv:2003.08237,\n2020. 14\n[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.\narXiv preprint arXiv:1706.03762, 2017. 2, 3, 22\n[67] X. Wang, Ross B. Girshick, A. Gupta, and Kaiming He. Non-local neural networks.\nConference on Computer Vision and Pattern Recognition, 2018. 21\n[68] Ross Wightman. Pytorch image models. https://github.com/rwightman/\npytorch-image-models, 2019. 7, 8, 12, 14, 17, 18\n[69] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi\nTomizuka, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image\nrepresentation and processing for computer vision.arXiv preprint arXiv:2006.03677,\n2020. 21\n[70] L. Xiao, Y. Bahri, Jascha Sohl-Dickstein, S. Schoenholz, and Jeffrey Pennington.\nDynamical isometry and a mean ﬁeld theory of cnns: How to train 10, 000-layer\nvanilla convolutional neural networks. arXiv preprint arXiv:1806.05393, 2018. 22\n[71] Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, A. Yuille, and Quoc V . Le.\nAdversarial examples improve image recognition. Conference on Computer Vision\nand Pattern Recognition, 2020. 14\n[72] Saining Xie, Ross B. Girshick, Piotr Doll ´ar, Zhuowen Tu, and Kaiming He. Aggre-\ngated residual transformations for deep neural networks. Conference on Computer\nVision and Pattern Recognition, 2017. 1\n[73] Qiantong Xu, Alexei Baevski, Tatiana Likhomanenko, Paden Tomasello, Alexis\nConneau, Ronan Collobert, Gabriel Synnaeve, and Michael Auli. Self-training\nand pre-training are complementary for speech recognition. arXiv preprint\narXiv:2010.11430, 2020. 22\n26\n[74] L. Yuan, Y. Chen, Tao Wang, Weihao Yu, Yujun Shi, F. Tay, Jiashi Feng, and S. Yan.\nTokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv\npreprint arXiv:2101.11986, 2021. 14, 21\n[75] Hongyi Zhang, Yann Dauphin, and Tengyu Ma. Fixup initialization: Residual\nlearning without normalization. arXiv preprint arXiv:1901.09321, 2019. 2, 3, 4, 10,\n22\n[76] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi-Li Zhang, Haibin Lin,\nYu e Sun, Tong He, Jonas Mueller, R. Manmatha, M. Li, and Alex Smola. Resnest:\nSplit-attention networks. arXiv preprint arXiv:2004.08955, 2020. 21\n[77] Yu Zhang, James Qin, Daniel S Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang,\nQuoc V Le, and Yonghui Wu. Pushing the limits of semi-supervised learning for\nautomatic speech recognition. arXiv preprint arXiv:2010.10504, 2020. 22\n[78] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for im-\nage recognition. In Conference on Computer Vision and Pattern Recognition, 2020. 21\n27\nGoing deeper with Image Transformers\nAppendix\nIn this supplemental material, we provide variations on the architecture\npresented in our main paper. These experiments have guided some of our\nchoices for the design of class-attention layers and LayerScale.\nA Variations on LayerScale init\nFor the sake of simplicity and to avoid overﬁtting per model, we have chosen\nto do a constant initialization with small values depending on the model depth.\nIn order to give additional insight on the importance of this initialization we\ncompare in Table A.1 other possible choices.\nLayerScale with 0 init. We initialize all coefﬁcients of LayerScale to 0. This\nresembles Rezero, but in this case we have distinct learnable parameters for\neach channel. We make two observations. First, this choice, which also starts\nwith residual branches that output 0 the beginning of the training, gives a clear\nboost compared to the block-wise scaling done by our adapted ReZero. This\nconﬁrms the advantage of introducing a learnable parameter per channel and\nnot only per residual layer. Second, LayerScale is better: it is best to initialize\nto a small εdifferent from zero.\nRandom init. We have tested a version in which we try a different initial\nweight per channel, but with the same average contribution of each residual\nblock as in LayerScale. For this purpose we initialize the channel-scaling val-\nues with the Uniform law (U[0,2ε]). This simple choice choice ensures that the\nexpectation of the scaling factor is equal to the value of the classical initializa-\ntion of LayerScale. This choice is overall comparable to the initialization to 0 of\nthe diagonal, and inferior to LayerScale.\nRe-training. LayerScale makes it possible to get increased performance by\ntraining deeper models. At the end of training we obtain a speciﬁc set of scal-\ning factors for each layer. Inspired by the lottery ticket hypothesis [23], one\nquestion that arises is whether what matters is to have the right scaling fac-\ntors, or to include these learnable weights in the optimization procedure. In\nother terms, what happens if we re-train the network with the scaling factors\nobtained by a previous training?\nIn this experiment below, we try to empirically answer that question. We\ncompare the performance (top-1 validation accuracy, %) on ImageNet-1k with\nDeiT-S architectures of differents depths. Everything being identical otherwise,\nI\nTable A.1: Performance when increasing the depth. We compare different strategies\nand report the top-1 accuracy (%) on ImageNet-1k for the DeiT training (Baseline) with\nand without adapting the stochastic depth rate dr (uniform drop-rate), and a modiﬁed\nversion of Rezero with LayerNorm and warmup. We compare different initialisation of\nthe diagonal matrix for LayerScale. We also report results with 0 initialization, Uniform\ninitialisation and small constant initialisation. Except for the baseline dr = 0.1, we have\nadapted the stochastic depth rate dr.\ndepth baseline baseline ReZero LayerScale [ε]\ndr = 0.1 [dr] α= 0 λi = 0 λi = U[0,2ε] λi = ε\n12 79.9 79.9 [0.05] 78.3 79.7 80.2 [0.1] 80.5[0.1]\n18 80.1 80.7 [0.10] 80.1 81.5 80.8 [0.1] 81.7[0.1]\n24 78.9† 81.0[0.20] 80.8 82.1 82.1 [10−5] 82.4[10−5]\n36 78.9† 81.9[0.25] 81.6 82.7 82.6 [10−6] 82.9[10−6]\nin the ﬁrst experiment we use LayerScale, i.e. we have learnable weights initial-\nized at a small value ε. In the control experiment we use ﬁxed scaling factors\ninitialised at values obtained by the LayerScale training.\nDepth→ 12 18 24 36\nLayerScale 80.5 81.7 82.4 82.9\nRe-trained with ﬁxed weights 80.6 81.5 81.2 81.6\nWe can see that the control training with ﬁxed weights also converges, but\nit is only slightly better than the baseline with adjusted stochastic depth drop-\nrate dr. Nevertheless, the results are lower than those obtained with the learn-\nable weighting factors. This suggests that the evolution of the parameters dur-\ning training has a beneﬁcial effect on the deepest models.\nB Design of the class-attention stage\nIn this subsection we report some results obtained when considering alterna-\ntive choices for the class-attention stage.\nNot including class embedding in keys of class-attention.In our approach\nwe chose to insert the class embedding in the class-attention: By deﬁning\nz= [xclass,xpatches], (B.1)\nwe include xclass in the keys and therefore the class-attention includes attention\non the class embedding itself in Eqn. 6 and Eqn. 7. This is not a requirement as\nwe could simply use a pure cross-attention between the class embedding and\nthe set of frozen patches.\nIf we do not include the class token in the keys of the class-attention layers,\ni.e., if we deﬁne z = xpatches, we reach 83.31% (top-1 acc. on ImageNet1k-val)\nII\nTable B.1: CaiT models with and without distillation token. All these models are trained\nwith the same setting during 400 epochs.\nDistillation token\nModel \u0017 ✓\nXXS-24Υ 78.4 78.5\nM-24Υ 84.8 84.7\nwith CaiT-S-36, versus 83.44% for the choice adopted in our main paper. This\ndifference of +0.13% is likely not signiﬁcant, therefore either choice is reason-\nable. In order to be more consistent with the self-attention layer SA, in the sense\nthat each query has its key counterpart, we have kept the class embedding in\nthe keys of the CA layers as stated in our paper.\nRemove LayerScale in Class-Attention.If we remove LayerScale in the Class-\nAttention blocks in the CaiT-S-36 model, we obtain a top-1 accuracy of 83.36%\non ImageNet1k-val, versus 83.44% with LayerScale. The difference of +0.08%\nis not signiﬁcant enough to conclude on a clear advantage. For the sake of\nconsistency we have used LayerScale after all residual blocks of the network.\nDistillation with class-attention In the main paper we report results with the\nhard distillation proposed by Touvron et al.[63], which in essence replaces the\nlabel by the average of the label and the prediction of the teacher output. This is\nthe choice we adopted in our main paper, since it provides better performance\nthan traditional distillation.\nThe DeiT authors also show the advantage of considering an additional\n“distillation token”. In their case, employed with the ViT/DeiT architecture,\nthis choice improves the performance compared to hard distillation. Notice-\nably it accelerates convergence.\nIn Table B.1 we report the results obtained when inserting a distillation to-\nken at the same layer as the class token, i.e., on input of the class-attention\nstage. In our case we do not observe an advantage of this choice over hard\ndistillation when using class-attention layers. Therefore in our paper we have\nonly considered the hard distillation also employed by Touvronet al.[63].\nIII",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7982251644134521
    },
    {
      "name": "FLOPS",
      "score": 0.6540100574493408
    },
    {
      "name": "Computer science",
      "score": 0.6473785638809204
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6057085990905762
    },
    {
      "name": "Architecture",
      "score": 0.5754969120025635
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5200138092041016
    },
    {
      "name": "Machine learning",
      "score": 0.3486975431442261
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34678560495376587
    },
    {
      "name": "Engineering",
      "score": 0.23570263385772705
    },
    {
      "name": "Electrical engineering",
      "score": 0.11921754479408264
    },
    {
      "name": "Parallel computing",
      "score": 0.10124069452285767
    },
    {
      "name": "Geography",
      "score": 0.09849926829338074
    },
    {
      "name": "Voltage",
      "score": 0.08392399549484253
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}