{
  "title": "\"Hinglish\" Language -- Modeling a Messy Code-Mixed Language",
  "url": "https://openalex.org/W2997993595",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2186010204",
      "name": "Gupta Vivek Kumar",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2971296908"
  ],
  "abstract": "With a sharp rise in fluency and users of \"Hinglish\" in linguistically diverse country, India, it has increasingly become important to analyze social content written in this language in platforms such as Twitter, Reddit, Facebook. This project focuses on using deep learning techniques to tackle a classification problem in categorizing social content written in Hindi-English into Abusive, Hate-Inducing and Not offensive categories. We utilize bi-directional sequence models with easy text augmentation techniques such as synonym replacement, random insertion, random swap, and random deletion to produce a state of the art classifier that outperforms the previous work done on analyzing this dataset.",
  "full_text": "\"Hinglish\" Language - Modeling a Messy\nCode-Mixed Language\nVivek Kumar Gupta\nvkgupta@stanford.edu\nAbstract\nWith a sharp rise in ﬂuency and users of \"Hinglish\" in linguistically diverse country,\nIndia, it has increasingly become important to analyze social content written in this\nlanguage in platforms such as Twitter, Reddit, Facebook. This project focuses on\nusing deep learning techniques to tackle a classiﬁcation problem in categorizing\nsocial content written in Hindi-English into Abusive, Hate-Inducing and Not\noffensive categories. We utilize bi-directional sequence models with easy text\naugmentation techniques such as synonym replacement, random insertion, random\nswap, and random deletion to produce a state of the art classiﬁer that outperforms\nthe previous work done on analyzing this dataset.\n1 Introduction\nHinglish is a linguistic blend of Hindi (very widely spoken language in India) and English (an\nassociate language of urban areas) and is spoken by upwards of 350 million people in India. While\nthe name is based on the Hindi language, it does not refer exclusively to Hindi, but is used in India,\nwith English words blending with Punjabi, Gujarati, Marathi and Hindi. Sometimes, though rarely,\nHinglish is used to refer to Hindi written in English script and mixing with English words or phrases.\nThis makes analyzing the language very interesting. Its rampant usage in social media like Twitter,\nFacebook, Online blogs and reviews has also led to its usage in delivering hate and abuses in similar\nplatforms. We aim to ﬁnd such content in the social media focusing on the tweets. Hypothetically, if\nwe can classify such tweets, we might be able to detect them and isolate them for further analysis\nbefore it reaches public. This will a great application of AI to the social cause and thus is motivating.\nAn example of a simple, non offensive message written in Hinglish could be:\n\"Why do you waste your time with <redacted content>. Aapna ghar sambhalta\nnahi(<redacted content> ). Chale dusro ko basane..!!\"\nThe second part of the above sentence is written in Hindi while the ﬁrst part is in English. Second\npart calls for an action to a person to bring order to his/her home before trying to settle others.\n1.1 Modeling challenges\nFrom the modeling perspective there are couple of challenges introduced by the language and the\nlabelled dataset. Generally, Hinglish follows largely fuzzy set of rules which evolves and is dependent\nupon the users preference. It doesn’t have any formal deﬁnitions and thus the rules of usage are\nambiguous. Thus, when used by different users the text produced may differ. Overall the challenges\nposed by this problem are:\n• Geographical variation: Depending upon the geography of origination, the content may\nbe be highly inﬂuenced by the underlying region.\nPreprint. Under review.\narXiv:1912.13109v1  [cs.CL]  30 Dec 2019\n• Language and phonetics variation: Based on a census in 2001, India has 122 major\nlanguages and 1599 other languages. The use of Hindi and English in a code switched\nsetting is highly inﬂuenced by these language.\n• No grammar rules: Hinglish has no ﬁxed set of grammar rules. The rules are inspired from\nboth Hindi and English and when mixed with slur and slang produce large variation.\n• Spelling variation: There is no agreement on the spellings of the words which are mixed\nwith English. For example to express love, a code mixed spelling, specially when used\nsocial platforms might be pyaar, pyar or pyr.\n• Dataset: Based on some earlier work, only available labelled dataset had 3189 rows of text\nmessages of average length of 116 words and with a range of 1, 1295. Prior work addresses\nthis concern by using Transfer Learning on an architecture learnt on about 14,500 messages\nwith an accuracy of 83.90. We addressed this concern using data augmentation techniques\napplied on text data.\n2 Related Work\n2.1 Transfer learning based approaches\nMathur et al. in their paper for detecting offensive tweets proposed a Ternary Trans-CNN model\nwhere they train a model architecture comprising of 3 layers of Convolution 1D having ﬁlter sizes of\n15, 12 and 10 and kernel size of 3 followed by 2 dense fully connected layer of size 64 and 3. The\nﬁrst dense FC layer has ReLU activation while the last Dense layer had Softmax activation. They\nwere able to train this network on a parallel English dataset provided by Davidson et al. The authors\nwere able to achieve Accuracy of 83.9%, Precision of 80.2%, Recall of 69.8%.\nThe approach looked promising given that the dataset was merely 3189 sentences divided into three\ncategories and thus we replicated the experiment but failed to replicate the results. The results were\npoor than what the original authors achieved. But, most of the model hyper-parameter choices where\ninspired from this work.\n2.2 Hybrid models\nIn another localized setting of Vietnamese language, Nguyen et al. in 2017 proposed a Hybrid\nmulti-channel CNN and LSTM model where they build feature maps for Vietnamese language using\nCNN to capture shorterm dependencies and LSTM to capture long term dependencies and concatenate\nboth these feature sets to learn a uniﬁed set of features on the messages. These concatenated feature\nvectors are then sent to a few fully connected layers. They achieved an accuracy rate of 87.3% with\nthis architecture.\n3 Dataset and Features\nWe used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they\nannotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian\nsubcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study\nconducted by Davidson et al. This dataset was important to employ Transfer Learning to our task\nsince the number of labeled dataset was very small. Basic summary and examples of the data from\nthe dataset are below:\nTable 1: Annotated Data set\nHinglish and English Data\nLabel HOT English\nNon-Offensive 1121 7274\nOffensive 303 4836\nHate Inducing 1765 2399\nTotal 3189 14509\n2\nTable 2: Examples in the dataset\nHinglish and English Data\nLabel HOT English\nNon-Offensive Hum sab ghumne jaa\nrahe hain? http://t.\nWe all are going out-\nside? http://t...\nOffensive\n@username1\n<redacted con-\ntent>! Mujhe mat\nsikha:/\n@username1\n<redacted con-\ntent>! Do not teach\nme:/\nHate Inducing\n<redacted content>\nterrorist Akbaar kill\nSaveWorld\n<redacted content>\nKill terrorist Akbaar\nSaveWorld\n3.1 Challenges\nThe obtained data set had many challenges and thus a data preparation task was employed to clean\nthe data and make it ready for the deep learning pipeline. The challenges and processes that were\napplied are stated below:\n1. Messy text messages: The tweets had urls, punctuations, username mentions, hastags,\nemoticons, numbers and lots of special characters. These were all cleaned up in a prepro-\ncessing cycle to clean the data.\n2. Stop words: Stop words corpus obtained from NLTK was used to eliminate most unproduc-\ntive words which provide little information about individual tweets.\n3. Transliteration: Followed by above two processes, we translated Hinglish tweets into\nEnglish words using a two phase process\n4. Transliteration: In phase I, we used translation API’s provided by Google translation ser-\nvices and exposed via a SDK, to transliteration the Hinglish messages to English messages.\n5. Translation: After transliteration, words that were speciﬁc to Hinglish were translated to\nEnglish using an Hinglish-English dictionary. By doing this we converted the Hinglish\nmessage to and assortment of isolated words being presented in the message in a sequence\nthat can also be represented using word to vector representation.\n6. Data augmentation: Given the data set was very small with a high degree of imbalance in\nthe labelled messages for three different classes, we employed a data augmentation technique\nto boost the learning of the deep network. Following techniques from the paper by Jason et\nal. was utilized in this setting that really helped during the training phase.Thsi techniques\nwasnt used in previous studies. The techniques were:\n• Synonym Replacement (SR):Randomly choose n words from the sentence that are\nnot stop words. Replace each of these words with one of its synonyms chosen at\nrandom.\n• Random Insertion (RI):Find a random synonym of a random word in the sentence\nthat is not a stop word. Insert that synonym into a random position in the sentence. Do\nthis n times.\n• Random Swap (RS):Randomly choose two words in the sentence and swap their\npositions. Do this n times.\n• Random Deletion (RD):For each word in the sentence, randomly remove it with\nprobability p.\n7. Word Representation: We used word embedding representations by Glove for creating\nword embedding layers and to obtain the word sequence vector representations of the\nprocessed tweets. The pre-trained embedding dimension were one of the hyperparamaters\nfor model. Further more, we introduced another bit ﬂag hyperparameter that determined if\nto freeze these learnt embedding.\n8. Train-test split: The labelled dataset that was available for this task was very limited in\nnumber of examples and thus as noted above few data augmentation techniques were applied\n3\nto boost the learning of the network. Before applying augmentation, a train-test split of\n78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were\nheld out for testing. All model evaluation were done in on the test set that got generated by\nthis process. The results presented in this report are based on the performance of the model\non the test set. The training set of 2489 messages were however sent to an ofﬂine pipeline\nfor augmenting the data. The resulting training dataset was thus 7934 messages. the ﬁnal\ndistribution of messages for training and test was thus below:\nTable 3: Train-test split\nHinglish and English Data\nLabel Train Test\nNon-Offensive 3572 228\nOffensive 1638 69\nHate Inducing 2724 403\nTotal 7934 700\n4 Model Architecture\nWe tested the performance of various model architectures by running our experiment over 100 times\non a CPU based compute which later as migrated to GPU based compute to overcome the slow\nlearning progress. Our universal metric for minimizing was the validation loss and we employed\nvarious operational techniques for optimizing on the learning process. These processes and its\nimplementation details will be discussed later but they were learning rate decay, early stopping, model\ncheckpointing and reducing learning rate on plateau.\n4.1 Loss function\nFor the loss function we chose categorical cross entropy loss in ﬁnding the most optimal\nweights/parameters of the model. Formally this loss function for the model is deﬁned as below:\n−1\nN\n∑N\ni=1\n∑C\nc=1\n1yi∈Cc Pmodel[yi ∈Cc] (1)\nThe double sum is over the number of observations and the categories respectively. While the model\nprobability is the probability that the observation i belongs to category c.\n4.2 Models\nAmong the model architectures we experimented with and without data augmentation were:\n• Fully Connected dense networks: Model hyperparameters were inspired from the previous\nwork done by V o et al and Mathur et al. This was also used as a baseline model but we did\nnot get appreciable performance on such architecture due to FC networks not being able to\ncapture local and long term dependencies.\n• Convolution based architectures: Architecture and hyperparameter choices were chosen\nfrom the past study Deon on the subject. We were able to boost the performance as compared\nto only FC based network but we noticed better performance from architectures that are\nsuitable to sequences such as text messages or any timeseries data.\n• Sequence models: We used SimpleRNN, LSTM, GRU, Bidirectional LSTM model archi-\ntecture to capture long term dependencies of the messages in determining the class the\nmessage or the tweet belonged to.\n4\nBased on all the experiments we conducted below model had best performance related to metrics -\nRecall rate, F1 score and Overall accuracy.\nFigure 1: Deep learning network used for the modeling\n4.3 Hyper parameters\nChoice of model parameters were in the above models were inspired from previous work done but\nthen were tuned to the best performance of the Test dataset. Following parameters were considered\nfor tuning.\n1. Learning rate: Based on grid search the best performance was achieved when learning rate\nwas set to 0.01. This value was arrived by a grid search on lr parameter.\n2. Number of Bidirectional LSTM units: A set of 32, 64, 128 hidden activation units were\nconsidered for tuning the model. 128 was a choice made by V o et al in modeling for\nVietnamese language but with our experiments and with a small dataset to avoid overﬁtting\nto train dataset, a smaller unit sizes were considered.\n3. Embedding dimension: 50, 100 and 200 dimension word representation from Glove word\nembedding were considered and the best results were obtained with 100d representation,\nconsistent with choices made in the previous work.\n4. Transfer learning on Embedding; Another bit ﬂag for training the embedding on the train\ndata or freezing the embedding from Glove was used. It was determined that set of pre-\ntrained weights from Glove was best when it was ﬁne tuned with Hinglish data. It provides\nevidence that a separate word or sentence level embedding when learnt for Hinglish text\nanalysis will be very useful.\n5. Number of dense FC layers.\n6. Maximum length of the sequence to be considered: The max length of tweets/message in\nthe dataset was 1265 while average was 116. We determined that choosing 200 resulted in\nthe best performance.\n5 Results\nFigure 2: Results of various experiments\n5\nDuring our experimentation, it was evident that this is a hard problem especially detecting the hate\nspeech, text in a code- mixed language. The best recall rate of 77 % for hate speech was obtained\nby a Bidirectional LSTM with 32 units with a recurrent drop out rate of 0.2. Precision wise GRU\ntype of RNN sequence model faired better than other kinds for hate speech detection. On the other\nhand for detecting offensive and non offensive tweets, fairly satisfactory results were obtained. For\noffensive tweets, 92 % precision was and recall rate of 88% was obtained with GRU versus BiLSTM\nbased models. Comparatively, Recall of 85 % and precision of 76 % was obtained by again GRU and\nBiLSTM based models as shown and marked in the results.\n6 Conclusion and Future work\nThe results of the experiments are encouraging on detective offensive vs non offensive tweets and\nmessages written in Hinglish in social media. The utilization of data augmentation technique in\nthis classiﬁcation task was one of the vital contributions which led us to surpass results obtained\nby previous state of the art Hybrid CNN-LSTM based models. However, the results of the model\nfor predicting hateful tweets on the contrary brings forth some shortcomings of the model. The\nbiggest shortcoming on the model based on error analysis indicates less than generalized examples\npresented by the dataset. We also note that the embedding learnt from the Hinglish data set may\nbe lacking and require extensive training to have competent word representations of Hinglish text.\nGiven this learning’s, we identify that creating word embeddings on much larger Hinglish corpora\nmay have signiﬁcant results. We also hypothesize that considering alternate methods than translation\nand transliteration may prove beneﬁcial.\nReferences\n[1] Mathur, Puneet and Sawhney, Ramit and Ayyar, Meghna and Shah, Rajiv,Did you offend me?\nclassiﬁcation of offensive tweets in hinglish language, Proceedings of the 2nd Workshop on Abusive\nLanguage Online (ALW2)\n[2] Mathur, Puneet and Shah, Rajiv and Sawhney, Ramit and Mahata, DebanjanDetecting offensive\ntweets in hindi-english code-switched language Proceedings of the Sixth International Workshop on\nNatural Language Processing for Social Media\n[3] V o, Quan-Hoang and Nguyen, Huy-Tien and Le, Bac and Nguyen, Minh-Le Multi-channel\nLSTM-CNN model for Vietnamese sentiment analysis2017 9th international conference on knowledge\nand systems engineering (KSE)\n[4] Hochreiter, Sepp and Schmidhuber, Jürgen Long short-term memory Neural computation 1997\n[5] Sinha, R Mahesh K and Thakur, Anil Multi-channel LSTM-CNN model for Vietnamese sentiment\nanalysis 2017 9th international conference on knowledge and systems engineering (KSE)\n[6] Pennington, Jeffrey and Socher, Richard and Manning, Christopher Glove: Global vectors for\nword representation Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP)\n[7] Zhang, Lei and Wang, Shuai and Liu, Bing Deep learning for sentiment analysis: A survey Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge Discovery\n[8] Caruana, Rich and Lawrence, Steve and Giles, C Lee Overﬁtting in neural nets: Backpropagation,\nconjugate gradient, and early stopping Advances in neural information processing systems\n[9] Beale, Mark Hudson and Hagan, Martin T and Demuth, Howard B Neural network toolbox user’s\nguide The MathWorks Incs\n[10] Chollet, François and others Keras: The python deep learning library Astrophysics Source Code\nLibrary\n[11] Wei, Jason and Zou, Kai EDA: Easy Data Augmentation Techniques for Boosting Performance\non Text Classiﬁcation TasksProceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP)\n6",
  "topic": "Offensive",
  "concepts": [
    {
      "name": "Offensive",
      "score": 0.7808500528335571
    },
    {
      "name": "Computer science",
      "score": 0.7513285875320435
    },
    {
      "name": "Fluency",
      "score": 0.6643906235694885
    },
    {
      "name": "Natural language processing",
      "score": 0.637198805809021
    },
    {
      "name": "Hindi",
      "score": 0.5964677929878235
    },
    {
      "name": "Random forest",
      "score": 0.5881935358047485
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5853222012519836
    },
    {
      "name": "Language model",
      "score": 0.5409191250801086
    },
    {
      "name": "Synonym (taxonomy)",
      "score": 0.5088520050048828
    },
    {
      "name": "Classifier (UML)",
      "score": 0.47722509503364563
    },
    {
      "name": "Swap (finance)",
      "score": 0.4514724612236023
    },
    {
      "name": "Linguistics",
      "score": 0.21861955523490906
    },
    {
      "name": "Operations research",
      "score": 0.08004200458526611
    },
    {
      "name": "Mathematics",
      "score": 0.07862129807472229
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Genus",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ]
}