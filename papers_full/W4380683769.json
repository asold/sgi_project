{
  "title": "MSATNet: multi-scale adaptive transformer network for motor imagery classification",
  "url": "https://openalex.org/W4380683769",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2171258536",
      "name": "Lingyan Hu",
      "affiliations": [
        "Nanchang University",
        "Shanghai University of Engineering Science"
      ]
    },
    {
      "id": "https://openalex.org/A2753112971",
      "name": "Weijie Hong",
      "affiliations": [
        "Nanchang University"
      ]
    },
    {
      "id": "https://openalex.org/A2123714036",
      "name": "Lingyu Liu",
      "affiliations": [
        "Shanghai Sunshine Rehabilitation Center"
      ]
    },
    {
      "id": "https://openalex.org/A2171258536",
      "name": "Lingyan Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753112971",
      "name": "Weijie Hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123714036",
      "name": "Lingyu Liu",
      "affiliations": [
        "Shanghai Sunshine Rehabilitation Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6679555981",
    "https://openalex.org/W3004193878",
    "https://openalex.org/W2100409538",
    "https://openalex.org/W3209470862",
    "https://openalex.org/W2810453816",
    "https://openalex.org/W3093527683",
    "https://openalex.org/W4302275416",
    "https://openalex.org/W3044202552",
    "https://openalex.org/W2971518519",
    "https://openalex.org/W3158818505",
    "https://openalex.org/W3131225866",
    "https://openalex.org/W3129964406",
    "https://openalex.org/W3032957021",
    "https://openalex.org/W2559463885",
    "https://openalex.org/W4200061731",
    "https://openalex.org/W4214809163",
    "https://openalex.org/W3040552008",
    "https://openalex.org/W2140413964",
    "https://openalex.org/W2142280324",
    "https://openalex.org/W3090080743",
    "https://openalex.org/W2910098374",
    "https://openalex.org/W4221022811",
    "https://openalex.org/W6755310813",
    "https://openalex.org/W2741907166",
    "https://openalex.org/W2119163516",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3170476868",
    "https://openalex.org/W2896120927",
    "https://openalex.org/W2052059892",
    "https://openalex.org/W3166600546",
    "https://openalex.org/W3113064029",
    "https://openalex.org/W3135511716",
    "https://openalex.org/W3134832814",
    "https://openalex.org/W4226006764",
    "https://openalex.org/W3102455230",
    "https://openalex.org/W4247993926",
    "https://openalex.org/W4297779039"
  ],
  "abstract": "Motor imagery brain-computer interface (MI-BCI) can parse user motor imagery to achieve wheelchair control or motion control for smart prostheses. However, problems of poor feature extraction and low cross-subject performance exist in the model for motor imagery classification tasks. To address these problems, we propose a multi-scale adaptive transformer network (MSATNet) for motor imagery classification. Therein, we design a multi-scale feature extraction (MSFE) module to extract multi-band highly-discriminative features. Through the adaptive temporal transformer (ATT) module, the temporal decoder and multi-head attention unit are used to adaptively extract temporal dependencies. Efficient transfer learning is achieved by fine-tuning target subject data through the subject adapter (SA) module. Within-subject and cross-subject experiments are performed to evaluate the classification performance of the model on the BCI Competition IV 2a and 2b datasets. The MSATNet outperforms benchmark models in classification performance, reaching 81.75 and 89.34% accuracies for the within-subject experiments and 81.33 and 86.23% accuracies for the cross-subject experiments. The experimental results demonstrate that the proposed method can help build a more accurate MI-BCI system.",
  "full_text": "Frontiers in Neuroscience 01 frontiersin.org\nMSATNet: multi-scale adaptive \ntransformer network for motor \nimagery classification\nLingyan Hu 1,2, Weijie Hong 3 and Lingyu Liu 4*\n1 School of Information and Engineering, Nanchang University, Nanchang, Jiangxi, China, 2 School of \nElectronic and Electrical Engineering, Shanghai University of Engineering Science, Shanghai, China, \n3 School of Qianhu, Nanchang University, Nanchang, Jiangxi, China, 4 Shanghai Yangzhi Rehabilitation \nHospital (Shanghai Sunshine Rehabilitation Center), Shanghai, China\nMotor imagery brain-computer interface (MI-BCI) can parse user motor imagery \nto achieve wheelchair control or motion control for smart prostheses. However, \nproblems of poor feature extraction and low cross-subject performance exist \nin the model for motor imagery classification tasks. To address these problems, \nwe  propose a multi-scale adaptive transformer network (MSATNet) for motor \nimagery classification. Therein, we  design a multi-scale feature extraction \n(MSFE) module to extract multi-band highly-discriminative features. Through \nthe adaptive temporal transformer (ATT) module, the temporal decoder and \nmulti-head attention unit are used to adaptively extract temporal dependencies. \nEfficient transfer learning is achieved by fine-tuning target subject data through \nthe subject adapter (SA) module. Within-subject and cross-subject experiments \nare performed to evaluate the classification performance of the model on the \nBCI Competition IV 2a and 2b datasets. The MSATNet outperforms benchmark \nmodels in classification performance, reaching 81.75 and 89.34% accuracies for \nthe within-subject experiments and 81.33 and 86.23% accuracies for the cross-\nsubject experiments. The experimental results demonstrate that the proposed \nmethod can help build a more accurate MI-BCI system.\nKEYWORDS\nelectroencephalogram, motor imagery classification, multi-scale convolution, \ntransformer, transfer learning\n1. Introduction\nA brain-computer interface (BCI) establishes a direct connection between the human brain \nand a computer or external device, without requiring muscular stimulation (Saha et al., 2021). \nA BCI system decodes the patient’s intentions to move specific limbs, and subsequently uses \nthese decoded intentions to provide corresponding sensorimotor feedback to the patient in \nvarious forms (Mane et al., 2020). In addition to being widely employed in the field of medical \nrehabilitation, BCI offers significant room for growth in fields like sleep monitoring, brain \ndisease detection, and game entertainment (Chen et al., 2018; Arpaia et al., 2020; Lee et al., 2021).\nAs a non-invasive approach, motor imagery brain-computer interface (MI-BCI) has the \ncharacteristics of high safety and low power consumption. Motor imagery can alter neuronal \nactivity in primary sensorimotor areas in a manner similar to that of performing actual \nmovements (Pfurtscheller and Neuper, 2001). When motor imagery occurs, energy in different \nsensory regions of the cerebral cortex changes and leads to event-related desynchronization \n(ERD) and event-related synchronization (ERS). Compared to other BCI paradigms, motor \nimagery is stimulus-independent and does not require external stimuli (Khan et al., 2020). A \nOPEN ACCESS\nEDITED BY\nBilge Karacali,  \nIzmir Institute of Technology, Türkiye\nREVIEWED BY\nBenito de Celis Alonso,  \nMeritorious Autonomous University of Puebla,  \nMexico\nYizhen Peng,  \nChongqing University, China\nArpan Pal,  \nTata Consultancy Services, India\n*CORRESPONDENCE\nLingyu Liu  \n 201619.happyneurologist@163.com\nRECEIVED 25 February 2023\nACCEPTED 18 May 2023\nPUBLISHED 14 June 2023\nCITATION\nHu L, Hong W and Liu L (2023) MSATNet: \nmulti-scale adaptive transformer network for \nmotor imagery classification.\nFront. Neurosci. 17:1173778.\ndoi: 10.3389/fnins.2023.1173778\nCOPYRIGHT\n© 2023 Hu, Hong and Liu. This is an open-\naccess article distributed under the terms of \nthe Creative Commons Attribution License \n(CC BY). The use, distribution or reproduction \nin other forums is permitted, provided the \noriginal author(s) and the copyright owner(s) \nare credited and that the original publication in \nthis journal is cited, in accordance with \naccepted academic practice. No use, \ndistribution or reproduction is permitted which \ndoes not comply with these terms.\nTYPE Original Research\nPUBLISHED 14 June 2023\nDOI 10.3389/fnins.2023.1173778\nHu et al. 10.3389/fnins.2023.1173778\nFrontiers in Neuroscience 02 frontiersin.org\nmulti-scale adaptive transformer network (MSATNet) is designed in \nthis paper to classify and recognize users’ motor imagery intention by \nacquiring their electroencephalogram (EEG) signals through \nMI-BCI. The category information predicted by the network can \nbe used as output commands for wheelchair control or motion control \nof smart prostheses.\nDue to the low signal-to-noise ratio and non-stationarity of EEG \nsignals, it is difficult to extract features with high discrimination \nabilities. Therein, traditional machine learning methods have been \nwidely applied to EEG decoding. The main steps include feature \nextraction and classification. For feature extraction, EEG features are \ndivided into temporal features, spectral features, and spatial features. \nTemporal features are extracted from time points or time segments, \nsuch as by the mean and variance, while spectral features include \nfrequency-domain and time-frequency features, such as the power \nspectral density and wavelet transform. The common spatial pattern \n(CSP) (Ramoser et al., 2000) is the most widely-used spatial feature \nextraction algorithm, and many researchers have attempted to \nimprove its baseline implementation. To mitigate the negative impact \nof outliers and noise on the performance of conventional CSP method, \nresearchers have proposed several modifications and enhancements, \nsuch as Sparse Common Spatial Pattern (SCSP)(Arvaneh et al., 2011), \nCSP-L1(Wang et al., 2012), CSP-QMEE (Chen et al., 2020 ). These \nmodifications can enhance the robustness and accuracy of the CSP \nalgorithm and have shown promising results in improving the \nperformance of BCIs. For the classification stage, several classifiers \nhave been used to distinguish high-dimensional features, such as the \nsupport vector machine (SVM) and linear discriminant analysis \n(LDA). However, these methods rely on feature selection and require \nextensive professional experience.\nDeep learning can automatically perform representation \nlearning without tedious preprocessing and feature engineering. \nTwo classical deep learning models are the convolutional neural \nnetwork (CNN) and recurrent neural network (RNN), which are \nwidely used for EEG classification in motor imagery. Existing CNN \nmodels can be divided into two types based on the different input \nforms of the model. One is to directly input the original signal into \nthe model, such as deep Convnet ( Schirrmeister et al., 2017 ) and \nEEGNet ( Lawhern et  al., 2018 ), and the other is to input the \nextracted features into the model. Xue et  al. used the FBCSP \nalgorithm to extract spatial features and the multilayer brain \nnetwork into their respective CNN models before finally performing \nfeature fusion (Xue et al., 2020). Sujit Roy et al. used the short-time \nFourier transform to extract time-frequency maps of the EEG as the \nmodel input (Roy et al., 2020 ).\nExisting CNN-based methods perform better in terms of \nclassification performance, but most only use single-scale convolution, \nwhich is inadequate to extract EEG signals using individual and \ntemporal variability and has poor recognition accuracy. Dai et  al. \nimproved the accuracy of EEG classification using mixed-size \nconvolution kernels for feature extraction (Dai et al., 2020). Inspired \nby the inception network structure in computer vision, EEG-Inception \nuses one-dimensional convolutions of different sizes for feature \nextraction (Zhang et al., 2021). Further, Jia et al. used a multi-branch \nmulti-scale structure to achieve state of the art (SOTA) in EEG \nrecognition (Jia et al., 2021). However, these methods have too many \nmodel training parameters and are prone to overfitting in the face of \nsmall datasets, which limits their recognition performance.\nRecent studies have found the existence of long-range temporal \ncorrelation (LRTC) in EEG signals during motion imagery, which \nchanges dramatically over time (Wairagkar et al., 2021). Therefore, \ncapturing long-range temporal dependencies in EEG signals is \nimportant for feature extraction. To this end, many studies have \napplied RNN-based models for temporal modeling. Wang et al. used \na one-dimensional aggregation approximation for dimensionality \nreduction and input the results into the long-short term memory \n(LSTM) for feature extraction (Wang et al., 2018). The gate recurrent \nunit (GRU) simplifies the model structure and improves the training \nefficiency. Liu et  al. applied the GRU to extract the temporal \ndependence in deep features ( Liu et  al., 2022 ). Although these \nmethods exploit the time series features of EEGs, the models are too \ncomplex for parallel training. The temporal convolutional network \n(TCN) improves upon these drawbacks, and the training efficiency is \ngreatly improved as no gradient disappearance or gradient explosion \noccurs when training on long input sequences (Ingolfsson et al., 2020). \nHowever, the above methods can only mine a small range of time-\ndependent relationships, and there are still limitations when modeling \nlong-sequence EEG signals.\nAn important function of a practical MI-BCI system is to \naccurately recognize different subjects. Although previous models \nhave achieved high performances for within-subject tests, these \nmodels are heavily data-dependent due to the individual variability of \nEEG data, which results in poor model generalization performance. \nTo address these issues, transfer learning, which is a machine learning \nmethod that reduces data shifts between different domains, has \nachieved better performance in BCI classification. Domain adaptation \nis a common transfer learning approach that reduces the gap between \nthe source and target domains via feature transformation. Wei et al. \n(2021) proposed aligning feature distributions using the maximum \nmean discrepancy. Chen et al. then used domain adversarial training \nto reduce the gap in the depth features between individuals ( Chen \net  al., 2022 ). However, this domain adaptation-based approach \nrequires access to the data of all individuals in the target domain, \nwhich is difficult to implement in practice.\nTo solve these problems, we propose a novel multi-scale adaptive \ntransformer model (MSATNet) for motor imagery decoding to obtain \nuser motion imagery intention from collected EEG data. The \ncontributions of this paper are given as follows.\n (1) In terms of feature extraction, in order to solve the problem \nthat the models in the past methods are too complex and prone \nto overfitting, we  propose a Multi-Scale Feature Extraction \n(MSFE) Module, which uses two branches and different \nconvolution kernels to extract features of different \nfrequency bands.\n (2) To address the limitation of previous methods, which could \nonly capture small-scale time dependencies, we propose an \nAdaptive Temporal Transformer (ATT) Module. By combining \ntemporal convolution with multi-head attention mechanism, \nwe  were able to capture long-range temporal dependencies \nfrom deep features of EEG signals.\n (3) In terms of model generalization, in order to solve the \nlimitations of previous methods that need to obtain all the data \nof the target subject, we designed a Subject Adapter Module. \nBy fine-tuning a pre-trained model with a small amount of data \nfrom new individuals, we attained good performance.\nHu et al. 10.3389/fnins.2023.1173778\nFrontiers in Neuroscience 03 frontiersin.org\nThe rest of the paper is organized as follows. Section 2 describes \nthe structure of MSATNet, Section 3 describes the experimental setup, \nSection 4 analyzes the experimental results, and conclusions are \ndrawn in Section 5.\n2. Materials and methods\n2.1. Data description\nThis paper uses the BCI Competition IV 2a and 2b datasets \n(Tangermann et al., 2012 ) to evaluate the validity of the proposed \nmodel. The BCI Competition IV 2a is a multiclass motor imagery \ndataset that contains EEG recordings from nine participants during \nimagining movements for the left hand, right hand, feet, and \ntongue. Data were collected through a band pass filter from \n0.5–100 Hz with a sampling rate of 250 Hz. The signals consist of 22 \nEEG channels and 3 electro-oculogram (EOG) channels. Based on \nthe requirements of the dataset, we removed the data of the EOG \nchannel during the experiments. In addition, the dataset consists of \ntwo sessions that were collected on different days for each subject. \nEach session consists of 288 trials with the same number of trials \nfor each category. The first session was used as the training set, \nwhile the second session was used as the test set. Each trial in the \nparadigm began with a 2 s prep time and was followed by a cue that \nlasted 1.25 s to represent the imagined class. The imagination period \nlasted 4 s after the cue was initiated and was terminated by the \nrest period.\nThe BCI Competition IV 2b is an EEG dataset based on visually \nevoked left- and right-handed motor imagery. The EEG signals of nine \nparticipants were collected in the dataset. Each experimenter’s EEG \ndata set consists of five sessions, the first two sessions are EEG imagery \ndata without visual feedback, and the last three sessions are EEG \nimagery data with visual feedback. Each session with visual feedback \nhad a total of 120 motor imagery data segments, and each session \nwithout visual feedback contained 160 motor imagery EEG segments. \nWe utilized a total of five sessions from the dataset, with the first three \nsessions used as the training set and the remaining two sessions used \nas the test set. The data in all experiments were band-pass filtered at \n0.5–100 Hz and trap filtered at 50 Hz. The sampling frequency of the \nentire experiment was 250 Hz.\n2.2. EEG representations\nThe initial EEG signal is defined as Xy iKii,|() =…{} 12,, , , where \nX Ri CT∈ ×  is a representation of thei-th trial consisting of C channels \nand T  sampling time points, yi is the sample label corresponding to \nX i, and K is the total number of trials.\n2.3. Overall model framework\nA multi-scale adaptive transformer model called MSATNet is \nproposed to decode the acquired EEG signals and obtain the user \nmotor imagery awareness. The overall framework of MSATNet is \nshown in Figure 1. The MSATNet model consists of three modules: \nMSFE, ATT, and SA. First, a multiscale CNN network is used in the \nMSFE module to extract the input EEG signals from a local \nperspective. Large (small) convolutional kernels are used to extract the \nlow (high)-frequency features. After the MSFE module, the ATT \nmodule adaptively extracts the temporal information of the EEG \nsignal from a global perspective. This module consists of a temporal \ndecoder and a multi-head attention unit. To enhance the cross-subject \nperformance of the model, the SA module is introduced for fine-\ntuning the target subject data. Finally, the prediction category is \noutput after a fully-connected layer and a softmax activation function.\nThe motor imagery recognition task can be defined as learning the \nnonlinear mapping between EEG signals and their \ncorresponding categories:\n YF Xclass = ()\nWhere X  represents the representation of EEG signals, F denotes \nthe learned nonlinear mapping, and Yclass is the classification result.\n2.4. Multi-scale feature extraction module\nThe multi-branch convolutional structure can extract rich multi-\nscale signal features using variable-sized convolutional kernels in \ndifferent branches, but too many branches make the model too \nparametric and prone to overfitting for small EEG datasets, which \nlimits the enhancement of the classification performance. Therefore, \nwe design an MSFE module with a two-branch structure, as shown in \nFigure 2. The MSFE is performed through two convolutional layers of \ndifferent sizes, which ensures the richness of feature extraction while \ncontrolling the complexity of the model and avoiding overfitting. In \neach branch, the signals of each channel are first processed using \none-dimensional temporal convolution as a frequency filter. Different \nconvolution kernel sizes can capture various time step ranges and \nextract features from different frequency bands (Eldele et al., 2021). \nTherein, a large (small) convolution kernel is used to extract low \n(high)-frequency features. Then, depth-wise convolution is used to \nextract the features between the channels. After processing each \nbranch with depthwise convolution, we fused the features and further \nprocessed the extracted multi-scale features with convolution. This \napproach improves upon previous methods, which utilized multiple \nbranches and various convolutional layers, by reducing the number of \nparameters required for training while maintaining high classification \naccuracy. Each branch has two convolutional layers and an average \npooling layer with each convolutional layer followed by batch \nnormalization (Santurkar et al., 2018) and ELU function activation. \nThe mathematical expression of the ELU function is:\n \nELU x ex\nxx\nx\n() = −<\n≥\n\n\n\n10\n0\nThe fused features are then extracted in the feature dimension. To \navoid overfitting, a dropout layer is added at the end of each branch \nand MSFE module. The detailed configuration of this module is \nprovided in Table 1.\nHu et al. 10.3389/fnins.2023.1173778\nFrontiers in Neuroscience 04 frontiersin.org\n2.5. Adaptive temporal transformer module\nEEG signals contain rich temporal information. Previous \nmethods often utilized TCN, GRU, and similar methods for feature \nextraction. However, these methods only capture information from \na relatively small range, which is insufficient for EEG signals with \nlong-term dependencies. Therefore, we designed an ATT module \nconsisting of a temporal decoder and a multi-head self-attention \nmechanism unit. After the MSFE module, the temporal decoder \nextracts deeper temporal features, and the multi-headed attention \nunit focused on more important information in the time series. The \ntemporal decoder is utilized to extract additional features and \nprovide temporal encoding for the subsequent self-attention \nmechanism unit.\nThe TCN is distinct from the LSTM and GRU as it uses \nconvolutions for sequence modeling, which can be  processed in \nparallel and have a higher computational efficiency with lower \nmemory requirements. We  improve TCN and design a temporal \ndecoder unit. After each original TCN, the ELU activation function is \nadded to enhance the expressive ability of the model. The temporal \nencoder consists of a one-dimensional fully-convolutional network \nand a causal convolution. The one-dimensional fully-convolutional \nnetwork uses zero-padding to ensure the input and output time steps \nare equal and that each time has a corresponding output. Causal \nconvolution ensures that the features at each time point are determined \nonly by the previous time points. A dilated convolution is introduced \nto expand the receptive field while avoiding layers that are too deep, \nwhich is the dilated casual convolution. For the input \nFIGURE 1\nDiagram showing the general framework of the MSATNet. For the within-subject experiments, the model consists of the MSFE and ATT modules. For \nthe cross-subject experiments, the model adds the SA module to the original model.\nFIGURE 2\nStructure of the proposed MSFE module, where N and d donate the sequence length and feature dimension, respectively.\nHu et al. 10.3389/fnins.2023.1173778\nFrontiers in Neuroscience 05 frontiersin.org\nsequence x n∈ , sequence element t , and filter f k:0 1,,…−{} → , \nthe dilated convolution F is calculated as:\n \nFt xf tf ixd\ni\nk\nsd i() =∗() () = ()\n=\n−\n−⋅∑\n0\n1\n•\nwhere d is the expansion factor, and k  is the convolution kernel size.\nThe temporal decoder is implemented by the modified TCN, and \nthe structure diagram is shown in Figure 3. We design the temporal \ndecoder with two residual blocks to achieve a global perspective \ntemporal feature extraction. Each residual block is composed of two \nlayers of dilated casual convolutions, where the convolution \nexpansion of the identical residual blocks is the same. The dilated \ncasual convolution is accompanied by batch normalization and the \nELU activation function after each convolution operation. For n \nresidual blocks, m convolution layers in each block, convolution \nkernel of size K, and expansion of size b, the perceptual field of the \ntemporal encoder is calculated as:\n \nrm K b\nb\nn\n=+ −() −\n−11 1\n1\nThe attention mechanism imitates human cognitive attention, \nwhich enhances the weight of some parts of the input data while \nweakening the weight of others. This focuses attention in the network \non the most important parts of the data. The self-attention mechanism \nis a variant of the attention mechanism, which reduces the dependence \non external information and better captures the internal correlation \nof data or features. Vaswani et al. proposed the transformer model for \nsequence-to-sequence learning on text data and achieved a new state-\nof-the-art approach (Vaswani et al., 2017) that has been extended to \nvarious modern deep learning algorithms, including for language, \nvision, speech, and reinforcement learning. The transformer model is \nbased entirely on the self-attention mechanism without any \nconvolutional or recurrent neural network layers.\nWe incorporated the design principles of transformers into \nmotion recognition, but applying this method to EEG signals \npresents several challenges. Compared to the vast amount of \nTABLE 1 Parameter settings for the proposed MSFE module, where p donates the dropout probability.\nLayer Filters Size Stride Activation Options\nConv2D 16 Branch1: (1,64) Branch2: (1,16) 1\nBatchNorm\nDepthwiseConv2D (22,1) 1\nBatchNorm\nActivation ELU\nAveragePooling2D (1,8) 1\nDropout 0.3\nConv2D 16 (1,16) 1\nBatchNorm\nActivation ELU\nAveragePooling2D (1,7) 1\nDropout ELU 0.3\nFIGURE 3\nStructure of the temporal decoder, where the structure of the input features is the same as the previous module output.\nHu et al. 10.3389/fnins.2023.1173778\nFrontiers in Neuroscience 06 frontiersin.org\nlanguage data used in transformer training, EEG data is limited, \nmaking it insufficient for training transformer architectures that \nrequire large amounts of data. Additionally, EEG signals are \none-dimensional time-series data with high sampling frequencies, \nmaking it computationally intensive to directly input them into a \ntransformer. To address these challenges, we utilized multi-scale \nfeature extraction and temporal convolution to extract small-\ndimensional features that still contain rich signal information, which \nwere then used as inputs for the multi-head self-attention unit. \nFurthermore, we reduced the model size by using only two parallel \nattention heads for computation, allowing us to maintain accuracy \nwhile reducing computational cost. Therefore, we designed a multi-\nhead attention unit, which is mainly composed of a multi-head self-\nattention mechanism (Figure 4).\nThe multi-head attention mechanism sets multiple attention heads \nbased on the self-attention mechanism, which divides the input \nfeatures and increases the learning space of the model. This provides \na mechanism to comprehensively focus on information at different \ntimes and representation subspaces, which enhances the model’s \nability to decode complex information in EEGs.\nThe multi-head self-attention mechanism consists of multiple self-\nattention layers. Each self-attention layer is composed of a query Q, \nkeys K, and values V. The features after the temporal decoder have the \nform X RNd∈ × , where N is the sequence length and d is the feature \ndimension. Under H heads, each X  is split into H spaces, and the \nconverted features are expressed as ′=…{}X XX H1 , , where \nX Rh\nN d\nH′∈\n×\n and 1 ≤ h ≤ H. Then, Qh, Kh, and Vh are multiplied by \nthe transformation matrices W h\nQ dd k∈ × , W h\nKd dk∈ × , and \nW h\nVd dv∈ × . The formulas are given as:\n QW Xh h\nQ\nh=′\n K WXh h\nK h=′\n VW Xh h\nV h=′\nNext, we use the three matrices Qh, Kh, and Vh to calculate the \nattention scoreZh of each attention head as:\n \nZ softmaxQK\nd\nVh hh T\nh\nN d\nH=\n\n\n\n\n\n ∈\n×\n\nFinally, we connect the H representations and perform a spatial \ntransformation to obtain the final output as:\n MultiHeadQK VC oncatZZ Wh o N d\nH,, ,,() =… () ∈\n×\n1 \nwhere the projection is the parameter matrix W oH ddv∈ × . For each \nof these, we use dd dHkv= =  .\n2.6. Subject adapter module\nDue to the individual variability of EEG signals, how to improve \nthe generalization ability of the model has always been a challenge. \nThe original transfer learning method based on domain adaptation \nneeds to obtain all the data of the new individual, which is obviously \ndifficult to use in practice. To address the limitations of previous \nmethods, we designed our own adapter module. By retraining the \nadapter module with partial data from new individuals, efficient \ntransfer learning can be  achieved. Detailed training and testing \nstrategies will be described in the Experimental Setup section. As \nshown in Figure 5 , the SA module is a bottleneck structure that \nconsists of two feedforward layers and the ELU activation function. \nThe feedforward down-project layer transforms high-dimensional \nfeatures into low-dimensional features and then transforms them \ninto their original dimension using the feedforward up-project layer \nafter passing the ELU activation function. The SA module also \ncontains a residual connection to prevent model performance \ndegradation. The pre-trained fine-tuning approach obtains a more \ngeneralized model by pre-training on a larger source domain and \nthen fine-tuning it using a smaller amount of data from the target \ndomain. This method ensures a higher cross-subject performance \nwith a shorter calibration time. The SA module can achieve subject-\nspecific adaptation of the pre-trained model by introducing a very \nsmall number of parameters.\n3. Experiments\nThis paper conducts within-subject and cross-subject experiments \nto illustrate the effectiveness of the MSATNet in classifying and \nidentifying motor imagery signals. The within-subject experiments \nillustrate the effectiveness of MSATNet in EEG feature extraction, and \nthe cross-subject experiments illustrate the effectiveness of MSATNet \nin model transfer after adding the SA module.\nFIGURE 4\nStructure of the multi-head attention unit.\nHu et al. 10.3389/fnins.2023.1173778\nFrontiers in Neuroscience 07 frontiersin.org\n3.1. Benchmark models\nWe choose FBCSP , EEGNet, EEG-ITNet, and SHNN for within-\nsubject performance comparison and DJDAN and JDAO-Mix for \ncross-subject performance comparison. The FBCSP applies the SVM \nclassifier for classification by slicing bands and selecting features based \non the CSP algorithm ( Ang et  al., 2008 ). The EEGNet applies \nfrequency filters via temporal convolution, applies depth-wise \nconvolution to learn frequency-specific spatial filters, and then applies \nseparable convolution to learn the features from different feature maps \n(Lawhern et  al., 2018 ). The EEG-ITNet performs multi-domain \nfeature extraction using a multi-branch CNN and dilated casual \nconvolution (Salami et al., 2022). The SHNN uses the SincNet-based \nCNN structure to extract spatial and spectral features of EEG signals, \nuses the SE module to recalibrate the features to obtain a sparse \nrepresentation of the EEG, and applies the GRU module to extract the \nsequence relationship of the data (Liu et al., 2022). The DJDAN uses \ntemporal and spatial convolutions for feature extraction and applies \nan adversarial learning strategy for domain adaptation (Hong et al., \n2021). The JDAO-Mix uses optimal transport for joint distribution \nadaptation, which is the latest SOTA method (Chen et al., 2022).\n3.2. Experimental setup\nFor the within-subject experiments, only the MSFE and ATT \nmodules were available as the SA module was not added. Both the \ntraining and test data were obtained from the same subject. The \ntraining set was taken from session 1 and the test set was from session \n2; thus, the effect of different sessions on the EEG data was ignored. \nWe  used the training set from each subject for training and then \nperformed validation with the test set from the corresponding subject. \nThe EEG data were taken from the dataset without any other \npre-processing. The data division standard for all comparison methods \nis the same to ensure a fair comparison. The EEG-ITNet is tested \nunder the same conditions as the proposed model, and the results of \nthe remaining comparison models are taken from their original \npapers. We apply accuracy as the evaluation metric.\nFor the cross-subject experiments, the SA module is added, and \nthe model evaluation is divided into four parts. (1) We first divide the \nentire dataset into two parts: the i-th subject is a randomly-selected \ntarget domain, and the remaining N −( )1 subjects are the source \ndomain. Half of the samples in the target domain are used for \nfine-tuning, while the other half is used to evaluate the classification \nperformance. (2) We pre-train the proposed model on all samples of \nthe source domain and update all trainable parameters. (3) Based on \nthe training samples of the target domain, we  only fine-tune the \nparameters embedded in the SA module to narrow the gap between \nthe target and source domains. (4) Finally, the trained model is \nevaluated on the test samples of the target domain. The EEG data were \ntaken from the dataset without any other pre-processing. The results \nof the benchmark models were taken from their original papers. The \nmodel parameters, evaluation indexes, and experimental assumptions \nwere the same as those of the within-subject experiments.\nWe built the model in TensorFlow and trained it using an RTX \n3060 GPU. The batch size was set to 64 and the Adam optimizer was \nused with a learning rate of 0.0008. All convolutional layers were \ninitialized using parameters based on the Glorot method, and the final \nfully-connected layer was constrained with maximum parametric \nweights having a parameter value of 0.25. The default epoch of the \nexperiments was 1,000 iterations, and an early stopping mechanism \nwas used to prevent overfitting. For the temporal decoder, the \nconvolution kernel size was 4 and the expansion factor was 2. We used \n2 attention heads in parallel for the multi-head attention unit.\n4. Results and analysis\nThis section analyzes the experimental results of MSATNet in the \nwithin-subject experiments for the BCI Competition IV 2a and 2b \ndatasets and describes the effects of the MSFE and ATT modules with \nthe ablation experiments in the BCI Competition IV 2a dataset. Then, \nthe experimental results of MSATNet in the cross-subject experiments \nin the BCI Competition IV 2a and 2b datasets are analyzed, and the \neffects of the SA module are considered in the ablation experiments \nfrom the BCI Competition IV 2a dataset.\n4.1. Within-subject experimental results \nand analysis\n4.1.1. Analysis of model effect\nTo evaluate the performance of the model for the within-subject \nexperiments, the proposed MSATNet model was tested on the BCI \nCompetition 2a and 2b datasets, and the experimental results were \ncompared with FBCSP (Ang et al., 2008), EEGNet (Lawhern et al., \n2018), EEG-ITNet (Salami et al., 2022), and SHNN (Liu et al., 2022) \nmodels. Table 2 summarizes the accuracy of the different subjects and \naverage accuracy under the BCI Competition IV 2a dataset for the \nproposed and benchmark methods. The powerful feature extraction \nability of the neural network gives a higher classification performance \nfor deep learning methods than in traditional machine learning \napproaches. In particular, the proposed MSATNet achieved the \ngreatest accuracy among most subjects and the highest average \naccuracy. Compared with EEGNet, we use multi-scale convolution \nand adaptively extract features from a global perspective, which \nachieves a performance improvement of 8.06%. Compared with \nEEG-ITNet and SHNN, we use the multi-head attention unit to focus \non information related to EEGs and attain a higher accuracy in EEG \nrecognition. Figure 6 shows the confusion matrix of the number of \ncorrect and incorrect predictions generated under the BCI \nFIGURE 5\nStructure of the SA module.\nHu et al. 10.3389/fnins.2023.1173778\nFrontiers in Neuroscience 08 frontiersin.org\ncompetition IV 2a dataset. As can be seen in Figure 6, the data in the \nconfusion matrix is mainly distributed on the diagonal of the matrix, \nindicating that MSATNet correctly predicted most of the imagined \nactions of the subjects with a low error rate.\nTo further verify the effectiveness of the proposed model, it \nwas tested on the BCI Competition IV 2b dataset. Table 3 shows \nthe accuracy of different subjects and average accuracy under the \nBCI Competition IV 2b dataset based on the proposed and \nbenchmark methods. The proposed model still achieves the highest \naccuracy in most subjects and achieves the highest average \naccuracy. The data show that the performance of the FBCSP is \ngreater than that of the EEGNet, but the FBCSP has the worst \nperformance in the BCI Competition IV 2a dataset. This indicates \nthat the generalization performance of the method extracted by the \nartificial design is poor, and the extracted features cannot be widely \napplied. However, the remaining deep learning methods perform \nbetter in both datasets. The MSATNet achieves effective feature \nextraction through the MSFE module extracting multi-band rich \nfeatures and the ATT module adaptively capturing the temporal \ndependencies of EEG signals. In summary, the MSATNet performs \nwell on both datasets and surpasses the benchmark methods in \nterms of performance. Thus, MSATNet can identify users’ motor \nimagery consciousness more accurately when used for motor \nimagery BCI data extraction.\n4.1.2. Analysis of the effect of the MSFE module\nFive models were used for comparative experiments to explore the \nimpact of the MSFE module on the classification accuracy. (1) A \nsingle-scale feature extraction model that only retains a single branch \nwith a convolution kernel size of 64. (2) The MSATNet model, which \nhas the 2-branch structure shown in Figure 2. (3) A three-branch \nfeature extraction model with three convolution kernels sized at 64, \n32, and 16. (4) Three convolution kernels with sizes of 64, 32, 16, 8, \nand 4 are used with a five-branch feature extraction model. (5) The \nexisting multi-branch multi-scale model MMCNN (Jia et al., 2021), \nwhich replaces the MSFE module in MSATNet while keeping the rest \nof the model the same. We conducted within-subject classification \nexperiments using these five models on the BCI Competition IV 2a \ndataset. The accuracy of different subjects and the average accuracy \nare shown in Figure 7.\nThe two-branch feature extraction model can extract more \nabundant signal features than the single-branch model, but by adding \nmore branches, feature redundancy may occur when using a three- or \nfive-branch feature model, which degrades the classification \nperformance. This is why the final design adopts two branches. \nFigure 8 shows that the complexity of the comparison model gives a \nsignificant overfitting phenomenon when faced with EEG datasets \nhaving a small amount of data. This significantly reduces the \nclassification effect. At the same time, the MMCNN has a poor \nperformance compared with the comparison model with five \nbranches, indicating that the convolutional structure can effectively \nextract features. Additionally, the number of parameters (5.7×104) of \nthe multi-scale multi-branch module in MMCNN is approximately \nTABLE 2 Classification performance of the MSATNet and benchmark \nmodels using the BCI Competition IV 2a dataset for the within-subject \nexperiments.\nSubject FBCSP EEGNet EEG-\nITNet\nSHNN Proposed \nmethod\nS1 76.00 81.94 84.38 82.76 90.62\nS2 56.50 56.95 62.85 68.97 65.97\nS3 81.25 90.62 89.93 79.31 95.14\nS4 61.00 67.01 69.1 65.52 78.12\nS5 55.00 72.57 74.31 58.62 79.86\nS6 42.25 58.68 57.64 48.28 62.50\nS7 82.75 76.04 88.54 86.21 91.67\nS8 81.25 81.25 83.68 89.66 88.89\nS9 70.75 78.12 80.21 89.87 82.99\nMean 67.42 73.69 76.76 74.26 81.75\nFIGURE 6\nConfusion matrix of a single subject (left, S3 as a representative) and of all subjects (right, with mean value processing), where L, H, F, and T are \nabbreviations for left-hand, right-hand, feet, and tongue, respectively.\nHu et al. 10.3389/fnins.2023.1173778\nFrontiers in Neuroscience 09 frontiersin.org\ntwice that of the proposed multi-scale feature extraction module \n(3.1×104). Under the same experimental conditions, the training time \nof the MSATNet is about 32.7% that of the MMCNN. Therefore, the \nMSFE module can ensure a moderate complexity and extract more \nabundant EEG features.\n4.1.3. Analysis of the effect of the ATT module\nWe designed an ablation experiment to verify the effectiveness of \nthe ATT module at improving the accuracy of EEG recognition. Four \nmodels are used for the comparative experiments. The first \ncomparison model is composed of only the MSFE module, the second \nis composed of the MSFE module and temporal decoder, the third is \ncomposed of the MSFE module and multi-head attention unit, and \nthe fourth is the MSATNet. Figure  9 shows the accuracy of the \ndifferent subjects and the average accuracy of the four comparison \nmodels under the BCI Competition IV 2a dataset. The model \nperformance improved after adding the temporal decoder due to the \nfeature extraction of the local temporal information. Feature \nextraction of the multi-head attention mechanism improved the \nclassification performance after adding the attention mechanism. \nHowever, the classification performances of the model adding only the \ntemporal decoder or only the multi-head attention unit are lower than \nMSATNet. It shows that on the basis of local feature extraction, the \nATT module further adaptively extracts shallow features from a global \nperspective. The experimental results show that the ATT module can \neffectively capture information related to motor imagery and improve \nthe recognition accuracy.\n4.2. Cross-subject experimental results and \nanalysis\n4.2.1. Analysis of model effect\nTo evaluate the cross-subject classification performance of the \nmodel, we compare MSATNet with the benchmark models DJDAN \n(Hong et al., 2021) and JDAO-Mix (Chen et al., 2022 ) on the BCI \nCompetition 2a and 2b datasets. In addition to performing well on the \nwithin-subject experiments, our method has good performance on the \ncross-subject experiments. We use eight subjects in the dataset for \npre-training and the remaining subject for fine-tuning and \nperformance evaluation. Table  4 shows the accuracy of different \nsubjects and the average accuracy of the proposed and benchmark \nmethods under the BCI Competition IV 2a and 2b datasets, \nrespectively. The cross-subject classification accuracies of the model \nreached 81.33 and 86.23% respectively, which exceeds the DJDAN and \nJDAO-Mix models. After the MSFE module and ATT module, the \nmodel learns features with high discrimination and achieves efficient \ntransfer in target subjects using only a small number of parameters. \nThis method does not need to obtain all the target subject data, which \nshortens the tedious calibration time of the EEG and is important in \npractical applications.\n4.2.2. Analysis of the effect of the SA module\nWe designed two comparison models to verify the effectiveness of \nthe SA module at improving the cross-subject performance. One \nTABLE 3 Classification performance of the MSATNet and comparison \nbenchmark methods using the BCI Competition IV 2b dataset for the \nwithin-subject experiment.\nSubject FBCSP EEGNet EEG-\nITNet\nSHNN Proposed \nmethod\nS1 70.00 67.50 67.5 83.33 81.87\nS2 60.36 60.35 71.43 61.76 72.14\nS3 60.94 62.81 86.88 58.33 87.81\nS4 97.50 91.25 98.44 97.30 97.81\nS5 93.12 83.44 94.06 91.89 99.38\nS6 80.63 61.56 86.25 88.89 88.44\nS7 78.13 83.75 90.00 86.11 93.13\nS8 92.50 91.88 93.44 92.11 94.37\nS9 86.88 82.50 55.31 91.67 89.06\nMean 80.00 76.12 82.59 83.49 89.34\nFIGURE 7\nClassification performance of different branches for the MSFE module.\nHu et al. 10.3389/fnins.2023.1173778\nFrontiers in Neuroscience 10 frontiersin.org\nTABLE 4 Classification performance of the MSATNet with benchmark \nmethods using the BCI Competition IV 2a and 2b datasets in cross-\nsubject experiments.\nMethod BCI IV 2a BCI IV 2b\nDJDAN 53.20 76.24\nJDAO-Mix 60.69 76.65\nProposed MSATNet 81.33 86.23\nmodel is composed only of the MSFE module and the ATT module \nand is directly tested on the target subjects after pre-training on eight \nsubjects without the SA module for fine-tuning. The other model is \ncomposed of the MSFE, ATT, and SA modules, and the target subjects \nis adapted by fine-tuning the SA module. Figure 10 shows the accuracy \nof the different subjects and average accuracy under the BCI \nCompetition IV 2a dataset for the model with and without the SA \nmodule. The MSATNet performs poorly in the face of new subjects, \nand features learned by the MSFE and ATT modules do not have \nbetter generalization. The addition of the SA module allows complete \nadaptation of the target domain with only a very small increase in the \nnumber of parameters to obtain better cross-subject performance \nunder the common feature distribution of the learned source domain. \nThus, the SA module helps adapt the model to the target subjects and \ncan achieve more accurate transfer learning.\n5. Conclusion\nWe propose a multi-scale adaptive transformer network called \nMSATNet. First, the MSFE module extracts rich features in \ndifferent frequency bands. Then, the ATT module adaptively learns \nthe information related to motion imagery from a global \nperspective. Finally, we  achieve effective transfer learning with \nrelatively few extra parameters using the SA module. Our approach \nFIGURE 8\nModel loss change diagram during training and testing. The left picture is the MSATNet, and the right picture is the comparison model MMCNN.\nFIGURE 9\nClassification performance of the ATT module ablation experiment.\nHu et al. 10.3389/fnins.2023.1173778\nFrontiers in Neuroscience 11 frontiersin.org\nwas evaluated on two publicly available datasets, and the results \nindicate that it outperforms existing methods. Future work will \nextend the methodology to additional activities to include disease \ndiagnostics. Although our work achieves good classification \nperformance, it still has some limitations. First, we only recognize \nmotor imagery patterns of different limbs, such as the left hand, \nright hand, and foot. In the future, our proposed model will \ndistinguish more complex motor imagery patterns, such as small \narm rotation and elbow flexion ( Chu et  al., 2020 ), which is \nimportant for patients’ rehabilitation training. Second, our adapter-\nbased approach still requires data from the user for calibration, but \nthe calibration process is time-consuming and requires significant \ntime. Therefore, future work will push toward calibration-free BCI \nclassification techniques.\nData availability statement\nPublicly available datasets were analyzed in this study. The \ndatasets for this study can be found at: http://www.bbci.de/\ncompetition/iv/.\nAuthor contributions\nLH conceptualized the study, presented the main idea of this \nmanuscript. WH performed the experiments and analyses, wrote the \nfirst draft of the manuscript. All authors contributed to the article and \napproved the submitted version.\nFunding\nThis work was partially supported by the National Natural Science \nFoundation of China under grants 81960327, the foundation of \nScience and Technology Department of Shanghai of China under \ngrant 23010501700, and the foundation of Health Commission of \nJiangxi Province under (Grant NO2023ZD008).\nConflict of interest\nThe authors declare that the research was conducted in the \nabsence of any commercial or financial relationships that could \nbe construed as a potential conflict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors \nand do not necessarily represent those of their affiliated organizations, \nor those of the publisher, the editors and the reviewers. Any product \nthat may be evaluated in this article, or claim that may be made by its \nmanufacturer, is not guaranteed or endorsed by the publisher.\nReferences\nAng, K. K., Chin, Z. Y ., Zhang, H., and Guan, C. (2008). “Filter Bank common spatial \npattern (FBCSP) in brain-computer Interface” in 2008 IEEE international joint conference \non neural networks (IEEE world congress on computational intelligence), 2390–2397. doi: \n10.1109/IJCNN.2008.4634130\nArpaia, P ., Duraccio, L., Moccaldi, N., and Rossi, S. (2020). Wearable brain–computer \nInterface instrumentation for robot-based rehabilitation by augmented reality. IEEE \nTrans. Instrum. Meas. 69, 6362–6371. doi: 10.1109/TIM.2020.2970846\nArvaneh, M., Guan, C., Ang, K. K., and Quek, C. (2011). Optimizing the channel \nselection and classification accuracy in EEG-based BCI. IEEE Trans. Biomed. Eng. 58, \n1865–1873. doi: 10.1109/TBME.2011.2131142\nChen, P ., Gao, Z., Yin, M., Wu, J., Ma, K., and Grebogi, C. (2022). Multiattention \nadaptation network for motor imagery recognition. IEEE Trans. Syst. Man Cybern. Syst. \n52, 5127–5139. doi: 10.1109/TSMC.2021.3114145\nChen, T., Huang, H., Pan, J., and Li, Y . (2018). “ An EEG-based brain-computer \ninterface for automatic sleep stage classification” in In 2018 13th IEEE conference on \nindustrial electronics and applications (ICIEA) (Wuhan: IEEE), 1988–1991. doi: 10.1109/\nICIEA.2018.8398035\nChen, B., Li, Y ., Dong, J., Lu, N., and Qin, J. (2020). Common spatial patterns based \non the quantized minimum error entropy criterion. IEEE Trans. Syst. Man Cybern. Syst. \n50, 4557–4568. doi: 10.1109/TSMC.2018.2855106\nFIGURE 10\nComparison of the MSATNet and MSATNet with the SA module removed.\nHu et al. 10.3389/fnins.2023.1173778\nFrontiers in Neuroscience 12 frontiersin.org\nChen, P ., Wang, H., Sun, X., Li, H., Grebogi, C., and Gao, Z. (2022). Transfer learning \nwith optimal transportation and frequency Mixup for EEG-based motor imagery \nrecognition. IEEE Trans. Neural Syst. Rehabil. Eng.  30, 2866–2875. doi: 10.1109/\nTNSRE.2022.3211881\nChu, Y ., Zhao, X., Zou, Y ., Xu, W ., Song, G., Han, J., et al. (2020). Decoding multiclass \nmotor imagery EEG from the same upper limb by combining Riemannian geometry \nfeatures and partial least squares regression. J. Neural Eng.  17:046029. doi: \n10.1088/1741-2552/aba7cd\nDai, G., Zhou, J., Huang, J., and Wang, N. (2020). HS-CNN: a CNN with hybrid \nconvolution scale for EEG motor imagery classification. J. Neural Eng. 17:016025. doi: \n10.1088/1741-2552/ab405f\nEldele, E., Chen, Z., Liu, C., Wu, M., Kwoh, C. K., Li, X., et al. (2021). An \nattention-based deep learning approach for sleep stage classification with Single-\nChannel EEG. IEEE Trans. Neural Syst. Rehabil. Eng.  29, 809–818. doi: 10.1109/\nTNSRE.2021.3076234\nHong, X., Zheng, Q., Liu, L., Chen, P ., Ma, K., Gao, Z., et al. (2021). Dynamic joint \ndomain adaptation network for motor imagery classification. IEEE Trans. Neural Syst. \nRehabil. Eng. 29, 556–565. doi: 10.1109/TNSRE.2021.3059166\nIngolfsson, T. M., Hersche, M., Wang, X., Kobayashi, N., Cavigelli, L., and Benini, L. \n(2020). “EEG-cent: an accurate temporal convolutional network for embedded motor-\nimagery brain-machine interfaces. ” arXiv. Available at: http://arxiv.org/abs/2006.00622 \n(Accessed December 30, 2022).\nJia, Z., Lin, Y ., Wang, J., Y ang, K., Liu, T., and Zhang, X. (2021). “MMCNN: a multi-\nbranch multi-scale convolutional neural network for motor imagery classification” in \nMachine learning and knowledge discovery in databases. eds. F . Hutter, K. Kersting, J. \nLijffijt and I. Valera , vol. 12459 (Cham: Springer International Publishing), 736–751. \ndoi: 10.1007/978-3-030-67664-3_44Lecture Notes in Computer Science\nKhan, M. A., Das, R., Iversen, H. K., and Puthusserypady, S. (2020). Review on motor \nimagery based BCI systems for upper limb post-stroke neurorehabilitation: from \ndesigning to application. Comput. Biol. Med.  123:103843. doi: 10.1016/j.\ncompbiomed.2020.103843\nLawhern, V . J., Solon, A. J., Waytowich, N. R., Gordon, S. M., Hung, C. P ., and \nLance, B. J. (2018). EEGNet: a compact convolutional neural network for EEG-based \nbrain-computer interfaces. J. Neural Eng. 15:056013. doi: 10.1088/1741-2552/aace8c\nLee, J., Lee, D., Jeong, I., and Cho, J. (2021). “ A study on the content of mental and \nphysical stability game in virtual reality through EEG detection” in 2021 international \nconference on information and communication technology convergence (ICTC), 693–696. \ndoi: 10.1109/ICTC52510.2021.9620932\nLiu, C., Jin, J., Daly, I., Li, S., Sun, H., Huang, Y ., et al. (2022). SincNet-based hybrid \nneural network for motor imagery EEG decoding. IEEE Trans. Neural Syst. Rehabil. Eng. \n30, 540–549. doi: 10.1109/TNSRE.2022.3156076\nMane, R., Chouhan, T., and Guan, C. (2020). BCI for stroke rehabilitation: motor and \nbeyond. J. Neural Eng. 17:041001. doi: 10.1088/1741-2552/aba162\nPfurtscheller, G., and Neuper, C. (2001). Motor imagery and direct brain-computer \ncommunication. Proc. IEEE 89, 1123–1134. doi: 10.1109/5.939829\nRamoser, H., Muller-Gerking, J., and Pfurtscheller, G. (2000). Optimal spatial filtering \nof single trial EEG during imagined hand movement. IEEE Trans. Rehabil. Eng.  8, \n441–446. doi: 10.1109/86.895946\nRoy, S., Chowdhury, A., McCreadie, K., and Prasad, G. (2020). Deep learning based \ninter-subject continuous decoding of motor imagery for practical brain-computer \ninterfaces. Front. Neurosci. 14:918. doi: 10.3389/fnins.2020.00918\nSaha, S., Mamun, K. A., Ahmed, K., Mostafa, R., Naik, G. R., Darvishi, S., et al. (2021). \nProgress in brain computer Interface: challenges and opportunities. Front. Syst. Neurosci. \n15:578875. doi: 10.3389/fnsys.2021.578875\nSalami, A., Andreu-Perez, J., and Gillmeister, H. (2022). EEG-ITNet: an explainable \ninception temporal convolutional network for motor imagery classification. IEEE Access \n10, 36672–36685. doi: 10.1109/ACCESS.2022.3161489\nSanturkar, S., Tsipras, D., Ilyas, A., and Ma, A. (2018). How does batch normalization \nhelp optimization? Adv. Neural Inf. Process. Syst. 31.\nSchirrmeister, R. T., Springenberg, J. T., Fiederer, L. D. J., Glasstetter, M., \nEggensperger, K., Tangermann, M., et al. (2017). Deep learning with convolutional \nneural networks for EEG decoding and visualization: convolutional neural networks in \nEEG analysis. Hum. Brain Mapp. 38, 5391–5420. doi: 10.1002/hbm.23730\nTangermann, M., Müller, K. R., Aertsen, A., Birbaumer, N., Braun, C., Brunner, C., \net al. (2012). Review of the BCI Competition IV . Front. Neurosci. 6:55. doi: 10.3389/\nfnins.2012.00055\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al. \n(2017). Attention is all you need. arXiv. doi: 10.48550/arXiv.1706.03762\nWairagkar, M., Hayashi, Y ., and Nasuto, S. J. (2021). Dynamics of long-range temporal \ncorrelations in broadband EEG during different motor execution and imagery tasks. \nFront. Neurosci. 15:660032. doi: 10.3389/fnins.2021.660032\nWang, P ., Jiang, A., Liu, X., Shang, J., and Zhang, L. (2018). LSTM-based EEG \nclassification in motor imagery tasks. IEEE Trans. Neural Syst. Rehabil. Eng.  26, \n2086–2095. doi: 10.1109/TNSRE.2018.2876129\nWang, H., Tang, Q., and Zheng, W . (2012). L1-norm-based common spatial patterns. \nIEEE Trans. Biomed. Eng. 59, 653–662. doi: 10.1109/TBME.2011.2177523\nWei, X., Ortega, P ., and Faisal, A. A. (2021). “Inter-subject deep transfer learning for \nmotor imagery EEG decoding” in 2021 10th international IEEE/EMBS conference on \nneural engineering (NER), 21–24. doi: 10.1109/NER49283.2021.9441085\nXue, J., Ren, F ., Sun, X., Yin, M., Wu, J., Ma, C., et al. (2020). A multifrequency brain \nnetwork-based deep learning framework for motor imagery decoding. Neural Plast. \n2020, 8863223–8863211. doi: 10.1155/2020/8863223\nZhang, C., Kim, Y .-K., and Eskandarian, A. (2021). EEG-inception: an accurate and \nrobust end-to-end neural network for EEG-based motor imagery classification. J. Neural \nEng. 18:046014. doi: 10.1088/1741-2552/abed81",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7957385778427124
    },
    {
      "name": "Motor imagery",
      "score": 0.7955952882766724
    },
    {
      "name": "Brain–computer interface",
      "score": 0.7695209980010986
    },
    {
      "name": "Discriminative model",
      "score": 0.6277669668197632
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5762940049171448
    },
    {
      "name": "Transformer",
      "score": 0.5404921174049377
    },
    {
      "name": "Feature extraction",
      "score": 0.5143266916275024
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.39932385087013245
    },
    {
      "name": "Speech recognition",
      "score": 0.3253164291381836
    },
    {
      "name": "Electroencephalography",
      "score": 0.1700088381767273
    },
    {
      "name": "Voltage",
      "score": 0.10044822096824646
    },
    {
      "name": "Engineering",
      "score": 0.09286132454872131
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141649914",
      "name": "Nanchang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I141962983",
      "name": "Shanghai University of Engineering Science",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210146112",
      "name": "Shanghai Sunshine Rehabilitation Center",
      "country": "CN"
    }
  ],
  "cited_by": 25
}