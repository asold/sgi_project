{
  "title": "A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model",
  "url": "https://openalex.org/W4285600138",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098036497",
      "name": "Xin Sun",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2133805355",
      "name": "Tao Ge",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2099570760",
      "name": "Shuming Ma",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2097841574",
      "name": "Jing-Jing Li",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2113814419",
      "name": "Houfeng Wang",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2997636815",
    "https://openalex.org/W3175441946",
    "https://openalex.org/W3043430239",
    "https://openalex.org/W3171975879",
    "https://openalex.org/W2885928764",
    "https://openalex.org/W3174851730",
    "https://openalex.org/W2153013403",
    "https://openalex.org/W2741494657",
    "https://openalex.org/W3102135593",
    "https://openalex.org/W2798416860",
    "https://openalex.org/W2913917571",
    "https://openalex.org/W2885213066",
    "https://openalex.org/W3156962704",
    "https://openalex.org/W3136804045",
    "https://openalex.org/W2970521905",
    "https://openalex.org/W3168817639",
    "https://openalex.org/W3095033764",
    "https://openalex.org/W3035010485",
    "https://openalex.org/W3175746962",
    "https://openalex.org/W2886474102",
    "https://openalex.org/W2964082031",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2885779207",
    "https://openalex.org/W2951652751",
    "https://openalex.org/W2936597270",
    "https://openalex.org/W2972529197",
    "https://openalex.org/W3034216012",
    "https://openalex.org/W2986388218",
    "https://openalex.org/W2810035278",
    "https://openalex.org/W2988975212"
  ],
  "abstract": "Synthetic data construction of Grammatical Error Correction (GEC) for non-English languages relies heavily on human-designed and language-specific rules, which produce limited error-corrected patterns. In this paper, we propose a generic and language-independent strategy for multilingual GEC, which can train a GEC system effectively for a new non-English language with only two easy-to-access resources: 1) a pre-trained cross-lingual language model (PXLM) and 2) parallel translation data between English and the language. Our approach creates diverse parallel GEC data without any language-specific operations by taking the non-autoregressive translation generated by PXLM and the gold translation as error-corrected sentence pairs. Then, we reuse PXLM to initialize the GEC model and pre-train it with the synthetic data generated by itself, which yields further improvement. We evaluate our approach on three public benchmarks of GEC in different languages. It achieves the state-of-the-art results on the NLPCC 2018 Task 2 dataset (Chinese) and obtains competitive performance on Falko-Merlin (German) and RULEC-GEC (Russian). Further analysis demonstrates that our data construction method is complementary to rule-based approaches.",
  "full_text": "A Unified Strategy for Multilingual Grammatical Error Correction\nwith Pre-trained Cross-Lingual Language Model\nXin Sun1∗ , Tao Ge2 , Shuming Ma2 , Jingjing Li3 , Furu Wei2 , Houfeng Wang1\n1MOE Key Lab of Computational Linguistics, School of Computer Science, Peking University\n2Microsoft Research Asia\n3The Chinese University of Hong Kong\n{sunx5, wanghf}@pku.edu.com {tage,shumma,fuwei}@microsoft.com lijj@cse.cuhk.edu.hk\nAbstract\nSynthetic data construction of Grammatical Error\nCorrection (GEC) for non-English languages relies\nheavily on human-designed and language-specific\nrules, which produce limited error-corrected pat-\nterns. In this paper, we propose a generic\nand language-independent strategy for multilingual\nGEC, which can train a GEC system effectively for\na new non-English language with only two easy-\nto-access resources: 1) a pre-trained cross-lingual\nlanguage model (PXLM) and 2) parallel transla-\ntion data between English and the language. Our\napproach creates diverse parallel GEC data with-\nout any language-specific operations by taking the\nnon-autoregressive translation generated by PXLM\nand the gold translation as error-corrected sentence\npairs. Then, we reuse PXLM to initialize the\nGEC model and pre-train it with the synthetic data\ngenerated by itself, which yields further improve-\nment. We evaluate our approach on three pub-\nlic benchmarks of GEC in different languages. It\nachieves the state-of-the-art results on the NLPCC\n2018 Task 2 dataset (Chinese) and obtains compet-\nitive performance on Falko-Merlin (German) and\nRULEC-GEC (Russian). Further analysis demon-\nstrates that our data construction method is com-\nplementary to rule-based approaches.\n1 Introduction\nGrammatical Error Correction (GEC) is a monolingual text-\nto-text rewriting task where given a sentence containing\ngrammatical errors, one needs to modify it to the corre-\nsponding error-free sentence. In recent years, pre-training on\nsynthetic erroneous data and then fine-tuning on annotated\nsentence pairs has become a prevalent paradigm [Grund-\nkiewicz and Junczys-Dowmunt, 2019; Lichtarge et al., 2019;\nZhang et al., 2019; Zhou et al., 2021] in English GEC, ad-\nvancing the state-of-the-art results [Ge et al., 2018b; Sun et\nal., 2021; Rothe et al., 2021] with various novel data synthe-\nsis approaches [Ge et al., 2018a; Grundkiewicz and Junczys-\nDowmunt, 2019; Kiyono et al., 2019].\n∗This work was done during the author’s internship at MSR Asia.\nContact: Tao Ge (tage@microsoft.com)\n1. generate\n3. pretrain2. initialize\n4. fine-tune\nTranslation \nData(en-zh)\nPretrained  \nCross-lingual \nLanguage Model\nSynthetic  \nError-Corrected  \nData(zh)\nAnnotated  \nError-corrected  \nData(zh)\nSeq2seq \nGEC \nModel\nFigure 1: The overall framework of our approach. We use PXLM\nand a large-scale translation corpus to produce synthetic error-\ncorrected sentence pairs. The seq2seq GEC model is initialized by\nPXLM and pre-trained by the synthetic data. Then we fine-tune it\nwith language-specific annotated GEC data. En and Zh denote En-\nglish and Chinese, respectively.\nAs GEC in other languages has drawn increasing atten-\ntion [Flachs et al., 2021; Rothe et al., 2021], synthetic er-\nroneous data construction has been borrowed to non-English\nlanguages for improving the results given a lack of anno-\ntated data. For instance, the rule-based approaches ob-\ntain promising results [Grundkiewicz and Junczys-Dowmunt,\n2019; N´aplava and Straka, 2019; Wang et al., 2020a]. How-\never, these approaches require language-specific rules and\nconfusion sets for word replacement based on expertise to\nsimulate diverse linguistic phenomena across multiple lan-\nguages, e.g., homophones in Chinese characters and morpho-\nlogical variants in Russian. Moreover, rule-based approaches\nalways produce erroneous data with limited error-corrected\npatterns [Zhou et al., 2020].\nTo address the above limitations, we propose a generic\nstrategy for training GEC systems in non-English languages.\nOur approach is easily adapted to new languages if only pro-\nvided with two relatively easy-to-obtain resources: 1) a pre-\ntrained cross-lingual language model (PXLM); 2) the parallel\ntranslation data between English and the language. In this pa-\nper, we choose InfoXLM [Chi et al., 2020] as the PXLM in\nour implementation.\nOur approach consists of synthetic data construction and\nmodel initialization. Since InfoXLM was pre-trained with\ntranslation language modeling objective, which requires the\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4367\nmodel to recover the masked tokens conditioned on the con-\ncatenation of a translation pair, it already possesses the ca-\npability of Non-Autoregressive Translation (NAT). That is,\nwhen presented with an English sentence, InfoXLM can pro-\nvide a rough translation in dozens of non-English languages.\nCompared with AT, NAT sacrifices translation quality due\nto the multimodality problem [Gu et al., 2017; Ghazvinine-\njad et al., 2019]. When vanilla NAT performs independent\npredictions at every position, it tends to consider many pos-\nsible translations of the sentence at the same time and output\ninconsistent results, such as token repetitions, missing or mis-\nmatch [Ran et al., 2020; Duet al., 2021]. Compared with pre-\ndefined rules, such error-corrected patterns are more reason-\nable and diverse with large model capacity and dependency in\nsentence context. We regard the rough translation generated\nby InfoXLM as a source sentence and the gold translation as\na corrected sentence for pre-training. To further improve the\ngeneralization ability of the seq2seq GEC model, we initial-\nize the GEC model with InfoXLM and pre-train it with the\nsynthetic data generated by itself.\nWe conduct experiments on Chinese, German and Russian\nGEC benchmarks. Our approach achieves the state-of-the-\nart results on the NLPCC 2018 Task 2 dataset (Chinese) and\nobtains competitive performance on Falko-Merlin (German)\nand RULEC-GEC (Russian). The results also demonstrate\nthat our approach can effectively complement rule-based cor-\nruption methods.\nThe contributions of this paper are as follows:\n• We propose a unified strategy for GEC in the non-\nEnglish languages consisting of synthetic data construc-\ntion and model initialization.\n• We propose a novel NAT-based synthetic data construc-\ntion approach, which generates diverse error-corrected\ndata for pre-training. To the best of our knowledge, it\nis the first to utilize the non-autoregressive translation\nability of a PXLM for GEC erroneous data construction.\nThe generated sentence pairs perform promising results\nalone and also nicely complement rule-based corruption\nmethods.\n• Our approach achieves the state-of-the-art performance\non the Chinese benchmark and very competitive results\nfor German and Russian benchmarks as well.\n2 Methodology\nIn this section, we present the unified strategy for non-English\nlanguages. At first, we briefly describe Translation Language\nModeling (TLM) objective and Non-Autoregressive Transla-\ntion (NAT) ability of InfoXLM. Then, we introduce two steps\nin our framework: NAT-based synthetic data construction and\nmodel initialization. Figure 2 shows the overview of our data\nconstruction approach.\n2.1 Background: Translation Language Modeling\nThe basis of our data construction is the non-autoregressive\ntranslation ability of InfoXLM, owing to its Translation Lan-\nguage Modeling (TLM) objective during pre-training. Given\na sentence x = x1 ··· x|x| in the source language (e.g.,\nEnglish) and the corresponding translation y = y1 ··· y|y|\nin another language (e.g., Chinese), the input sequence of\nTLM is the concatenation of these two parallel sentences\nS = ⟨s⟩ x ⟨/s⟩ y ⟨/s⟩and some percentage of tokens\nare replaced with [MASK] at random. Formally, let M =\n{m1, ··· , m|M|}denote the positions of the masks:\nmi ∼uniform{1, |x|+ |y|+ 3} for i = 1,··· , |M| (1)\nSM = replace(S, M,[MASK]) (2)\nwhere the replace denotes the replacement operation at the\ncertain positions. By leveraging bilingual context, the model\nis required to predict the original tokens with cross entropy\nloss. The TLM loss is computed as:\nLTLM = −\nX\nS∈T\nlog\nY\nm∈M\np(Sm|S\\M ) (3)\nwhere S\\M = {Si}i̸∈M means tokens that are not included\nin the M and T is the translation corpus.\nTo the extreme, we can use InfoXLM as a non-\nautoregressive translator. Specifically, we concatenate an En-\nglish sentence x with enough placeholders ([MASK]) as the\ninput. InfoXLM is capable of translating it to other languages\nby predicting tokens at all masked positions in parallel. For-\nmally, M = {|x|+ 3,··· , |x|+ |y|+ 2}denotes all target\ntokens are replaced with [MASK] and the predicted transla-\ntion y∗is derived by maximizing the following equation:\nS′= arg max\nSm\nlog\nY\nm∈M\np(Sm|S\\M ) (4)\ny∗= replace(SM , M, S′) (5)\nwhich infills the words with the highest probability.\nIn practice, following Mask-predict [Ghazvininejad et al.,\n2019], we partially mask some percentage of target transla-\ntion (m ∈ [|x|+ 3,|x|+ |y|+ 2]) rather than all of them,\nwhich ensures the outputs are of appropriate quality.\n2.2 NAT-based Data Construction\nTo generate diverse error-corrected sentences for GEC in a\nnon-English language (e.g., Chinese), our approach utilizes\nsentence pairs of machine translation (e.g., English-Chinese\nparallel corpus). Our approach starts by adding noise to the\ntarget sentence and then masking sampled tokens. We feed\nthe corrupted target sentence with the original English sen-\ntence as the input of InfoXLM. InfoXLM performs TLM pre-\ndictions at every masked position. To obtain poor sentences\ncontaining grammatical errors, we randomly sample the word\nfrom the top predictions.\nSpecifically, given a sentence y in the target language,\nwe select tokens for modification with a certain probability\npnoise and perform the following operations:\nMask. Replace the token with [MASK] with a probability\nof pmask.\nInsert. Add a [MASK] after the token with a probability\nof pinsert.\nDelete. Delete the token with a probability of pdelete.\nSwap. Replace the token with its right token with a proba-\nbility of pswap.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4368\nEn：Thank you for inviting me to attend this party \nand I have a great time. \nZh： 谢谢你邀请我来参加这次聚会，我玩得很开⼼。\nPre-trained Cross-lingual Language Model\nsrc: 感恩你邀请我赶来这次聚会，我玩得很享受。\ntgt: 谢谢你邀请我来参加这次聚会，我玩得很开⼼。\n(Gratitude you for inviting me to come to this party and I enjoyed it a lot.)\n(Thank you for inviting me to attend this party and I have a great time.)\nTransla'on\tcorpus\t(En-Zh): Synthe'c\terror-corrected\tcorpus\t(Zh):\n2. predict\n1. Tokenize & Corruption \n(Mask Insert Delete Swap)\n你 邀请 我[M] [M] … <s> Thank you … </s>\n赶\n。得很\ngratitude                          come                 enjoy\n来 [M] 你 邀请 我[M] [M] … 。来 [M] </s>\n感恩 享受\nFigure 2: The overview of NAT-based data construction. Given a translation sentence pair (e.g., English-Chinese), our approach applies four\noperations (Mask, Insert, Delete and Swap) randomly to the non-English sentence. Then, PXLM predicts the possibility over the vocabulary\nat every masked position based on the concatenation of the English sentence and the corrupted sentence. Finally, we sample the predicted\nwords and regard the recovered sequence as the source sentence containing grammatical errors and the gold non-English sentence as the\ncorrected target sentence. [M] denotes [MASK].\nWe get the noisy textey = NOISE (y) and the corresponding\npositions of the masks M. Then, we concatenate the English\nsentence x with the corrupted sequence ey containing enough\nmasked tokens as the input of InfoXLM. The predicted words\nare sampled for every[MASK] according to the output distri-\nbution:\ny′\nm ∼p(Sm|S\\M ) for m ∈M (6)\ny∗= replace(ey, M, y′) (7)\nwhere we produce erroneous sentence by replacing [MASK]\nwith sampled tokens.\nOur artificial corruption by four operations before TLM\nprediction improves the difficulty of translation. The inde-\npendent assumption between target tokens brings in more er-\nrors and less fluency. The predicted words are sampled based\non distribution rather than the best predictions to create more\ninconsistencies. It resembles the scenario where elementary\nlanguage learners render a low-quality sentence when com-\npleting the cloze task. However, we only mask some per-\ncentage of target tokens and the English sentence restricts In-\nfoXLM to recover original information, which ensures that\nthe sampled tokens are plausible.\nSince the recovered sentence contains diverse and reason-\nable word-level grammatical errors, we apply character-level\ncorruption operations to add more spelling errors: 1) insert;\n2) substitute; 3) delete; 4) swap-right; 5) change the casing.\nWe call it post edit. Finally, we regard the gold translation\nas the corrected sentence and the corrupted prediction as the\nerroneous sentence.\n2.3 Model Initialization\nTo further improve the generalization ability of the GEC\nmodel, we use InfoXLM to initialize the seq2seq model. We\nfollow [Ma et al., 2021] and use DeltaLM for multilingual\nGEC. DeltaLM is an InfoXLM-initialized encoder-decoder\nmodel trained in a self-supervised way. We continue pre-\nDataset\nLanguage Train Valid Test\nNLPCC 2018 Task 2 Chinese 1.2M 5000 2000\nFalko-Merlin German 19237 2503 2337\nRULEC-GEC Russian 4980 2500 5000\nTable 1: Statistics of the benchmarks for evaluation. The numbers\nin the table indicate the count of sentence pairs.\ntraining it with synthetic data generated by our NAT-based\napproach.\nOverall, our unified strategy exploits InfoXLM in two\nways. We make use of its NAT ability to produce synthetic\nGEC data and its pre-trained weights to initialize our GEC\nmodel.\n3 Experiments\n3.1 Data\nWe conduct our experiments on three GEC datasets: NLPCC\n2018 Task 2 [Zhao et al., 2018 ] in Chinese, Falko-\nMerlin [Boyd, 2018 ] in German and RULEC-GEC [Ro-\nzovskaya and Roth, 2019 ] in Russian. The statistics of the\ndatasets are listed in Table 1. We use the official Max-\nMatch [Dahlmeier and Ng, 2012 ] scripts1 to evaluate preci-\nsion, recall and F0.5.\nFor non-autoregressive translation generation, we use\ndatasets of the WMT20 news translation task 2 – UN Paral-\nlel Corpus v1.0 for Chinese and Russian, the combination of\nEuroparl v10, ParaCrawl v5.1 and Common Crawl corpus for\nGerman. We construct 10M synthetic sentence pairs in every\nlanguage for pre-training and then fine-tune the GEC model\non respective annotated datasets.\n1https://github.com/nusnlp/m2scorer\n2https://www.statmt.org/wmt20/translation-task.html\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4369\nModel NLPCC-18\nP R F 0.5\nYouDao 35.24 18.64 29.91\nAliGM 41.00 13.75 29.36\nBLCU 47.23 12.56 30.57\nBERT-encoder 41.94 22.02 35.51\nBERT-fuse 32.20 23.16 29.87\nDropout-Src 39.08 18.80 32.15\nMaskGEC 44.36 22.18 36.97\n- Our Implementation 41.66 25.81 37.10\n[Wang et al., 2020a] 39.43 22.80 34.41\nRule(10M) 44.66 26.54 39.30\nOurs(10M) 44.27 26.76 39.15\n- w/ DeltaLM 45.95 27.94 40.70\nOurs(10M) + Confusion set 45.17 26.11 39.42\nOurs(5M) + Rule(5M) 45.33 27.61 40.17\nTable 2: Performance of systems on the NLPCC-2018 Task 2\ndataset. The results of different model architectures are shown at\nthe top group. Different training strategies are shown in the middle.\nThe approaches with pre-training are shown at the bottom. Rule de-\nnotes the synthetic data generated by rule-based corruption. Ours\ndenotes data generated by our approach.\n3.2 Implementation Details\nUnless explicitly stated, we use Transformer (base) model in\nfairseq3 as our GEC model. For Chinese, we construct a\ncharacter-level vocabulary consisting of 7K tokens. We ap-\nply Byte Pair Encoding to preprocess German and Russian\nsentences and obtain the vocabularies with size of 32K to-\nkens, respectively. When using DeltaLM, we utilize its shared\nvocabulary of 250000 tokens. During pre-training for Ger-\nman and Russian, following [N´aplava and Straka, 2019], we\nuse source and target word dropouts and edit-weighted MLE\n[Junczys-Dowmunt et al., 2018]. We leave the detailed hy-\nperparameters in the supplementary notes.\n3.3 Baselines\nMost of the previous studies for Chinese GEC focus on\nmodel architecture or training strategy, which are orthogo-\nnal with our synthetic data construction method. For exam-\nple, YouDao [Fu et al., 2018] combines five hybrid correc-\ntion models and a language model together. AliGM [Zhou\net al., 2018 ] combines NMT-based, SMT-based and rule-\nbased models together. BLCU [Ren et al., 2018] uses multi-\nlayer convolutional seq2seq model. BERT-encoder [Wang\net al., 2020b] initializes the encoder of seq2seq model with\nBERT . BERT-fuse [Wang et al., 2020b] incorporates BERT\nfor additional features. As for training strategy, Dropout-\nSrc [Junczys-Dowmunt et al., 2018] sets the full embeddings\nof randomly selected source words to 0 during the training\nprocess. MaskGEC [Zhao and Wang, 2020 ] performs dy-\nnamic masking method by substituting the source word with\na padding symbol or other word.\nThe most comparable approach is [Wang et al., 2020a],\nwhich constructs pre-training data using the rule-based cor-\nruption method. For our approach, we implement MaskGEC\nduring the fine-tuning stage. To make a fair comparison, we\n3https://github.com/pytorch/fairseq\nalso construct synthetic data with rule-based corruption in\nthe same setting as baseline. It incorporates a character-level\nconfusion set 4 and uses pypinyin5 to perform homophony re-\nplacement.\nFor German and Russian, the main data construc-\ntion method is rule-based corruption. [Grundkiewicz and\nJunczys-Dowmunt, 2019 ] and [N´aplava and Straka, 2019 ]\nbuild confusion sets with edit distance, word embedding or\nspell-checker (e.g., Aspell dictionary6). [Flachs et al., 2021]\nuitlizes Unimorph which provides morphological variants of\nwords for word replacement operations. They also incorpo-\nrate WikiEdits and Lang8 as additional training resources.\n[Rothe et al., 2021] only applies language-agnostic opera-\ntions without any confusion set. They pre-train a unified\nseq2seq model for 101 languages and fine-tune for respec-\ntive languages. [Katsumata and Komachi, 2020] proposes to\ndirectly use mBART without pre-training.\n3.4 Main Results\nTable 2 shows the performance of our approach and previ-\nous methods on the NLPCC-2018 Chinese benchmark. Our\nNAT-based synthetic data construction approach is compa-\nrable with the rule-based corruption approach. We assume\nthat 0.15 F0.5 descend comes from that rule-based corruption\nleverages many useful confusion sets. When combined with\nthe confusion sets, our approach obtains 39.42 F0.5 which\noutperforms the rule-based counterpart. If combining two\ndata sources from the rule-based and NAT-based approaches,\nwe obtain better performance which demonstrates two meth-\nods complement each other. Initializing the GEC model with\nDeltaLM achieves 40.70F0.5, which is the state-of-the-art re-\nsult of the dataset.\nTable 11 shows the performance for German and Russian\ndatasets. In the same setting, our NAT-based synthetic ap-\nproach outperforms rule-based corruption methods and most\nbaselines with two exceptions. For instance, [N´aplava and\nStraka, 2019] leverages more training strategies during fine-\ntuning phase such as mixing pre-training data and oversam-\npled fine-tuning data, checkpoint averaging and so on. Al-\nthough [Rothe et al., 2021 ] obtains the best performance\nwith the pre-trained T5-XXL (11B parameters), its base-size\nmodel lags behind ours significantly under the similar model\ncapacity. Overall, the performance on the German and Rus-\nsian datasets demonstrates the effectiveness of our unified\nstrategy and NAT-based synthetic approach, which performs\ncompetitive results alone and also nicely complements rule-\nbased corruption methods.\nTo make fair comparison with multiple synthetic con-\nstruction approaches, we follow the experimental setting and\nmodel hyperparameters 7 in [Flachs et al., 2021 ]. The re-\nsults on the German dataset are shown in Table 4. Our ap-\nproach significantly outperforms commonly used synthetic\nmethods such as the rule-based approach with Unimorph, As-\npell word replacement and Wikipedia edits extracted from the\n4http://nlp.ee.ncu.edu.tw/resource/csc.html\n5https://github.com/mozillazg/python-pinyin\n6http://aspell.net/\n7We use the “transformer clean big tpu” setting.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4370\nModel Size German Russian\nLayer, Hidden, FFN P R F 0.5 P R F 0.5\n[Grundkiewicz and Junczys-Dowmunt, 2019] 6, 512, 2048 73.0 61.0 70.24 36.3 28.7 34.46\n[N´aplava and Straka, 2019] ♣ 6, 512, 2048 78.11 59.13 73.40 59.13 26.05 47.15\n[Rothe et al., 2021] ♠ 12, 768, 2048 - - 69.21 - - 26.24\nRule(10M) 6, 512, 2048 73.71 59.28 70.29 49.38 23.49 40.46\nOurs(10M) 6, 512, 2048 73.86 60.74 70.80 57.96 23.51 44.82\n- w/ DeltaLM 12-6, 768, 2048 75.59 65.19 73.25 59.31 27.07 47.90\nOurs(5M) + Rule(5M) 6, 512, 2048 74.31 61.46 71.33 61.40 27.47 49.24\n[Flachs et al., 2021] 6, 1024, 4096 - - 69.24 - - 44.72\n[N´aplava and Straka, 2019] ♣ 6, 1024, 4096 78.21 59.94 73.71 63.26 27.50 50.20\n[Katsumata and Komachi, 2020] 12, 1024, 4096 73.97 53.98 68.86 53.50 26.35 44.36\n[Rothe et al., 2021] ♠ 24, 4096, 10240 - - 75.96 - - 51.62\nTable 3: Performance of systems on German and Russian datasets. Layer, Hidden and FFN denote depth, embedding size and feed forward\nnetwork size of Transformer. 12-6 denotes that DeltaLM-initialized model has a 12-layer encoder and a 6-layer decoder. The top and bottom\ngroup shows the results of base-scale models and large-scale models, respectively. ♣ Our re-implementation of this approach is Rule(10M),\nwhose results are inferior to Ours(10M). We use synthetic data generated by their released codes and the same training strategy as ours. ♠\nWhile [Rothe et al., 2021] obtains the best performance with the pre-trained T5-XXL (11B parameters), its base-size model lags behind ours\nsignificantly under the similar model size.\nMethod F0.5\nRule(Unimorph)⋆ 60.87\nRule(Aspell)⋆ 63.49\nRule(Combine)⋆ 62.55\nWikiEdits⋆ 58.00\nRule + WikiEdits⋆ 66.66\nBack-translate 61.37\nRound-trip translation 62.91\nOurs 69.17\nTable 4: F0.5 scores of different data construction approaches on the\nGerman dataset. For the approaches with ⋆, their results are from\n[Flachs et al., 2021].\nrevision history. Although back-translate is effective for En-\nglish, it performs poorly with limited annotated sentence pairs\nto learn diverse error-corrected patterns. Round-trip transla-\ntion utilizes the same translation corpus as us but achieves in-\nferior performance since it usually produces sentences with-\nout grammatical errors.\n3.5 Ablation Study\nMethod P R F 0.5\nOurs 73.86 60.74 70.80\n- [MASK] replacement 71.17 55.07 67.24\n- [MASK] insert 72.52 59.00 69.34\n- post edit 73.00 61.36 70.34\n- bilingual constraint 71.17 55.89 67.48\nw/ an autoregressive translator 66.99 55.44 64.31\nTable 5: Performance of our approach with different schemes on the\nGerman dataset. - denotes removing the component or replacing it\nwith the rule-based operation.\nWe further conduct an ablation study as shown in Table 5.\nOverall, we find that all of these variants perform worse than\nthe original strategy. From the last row, PXLM is much better\nthan a regular translation model under the same setup (i.e.,\ntraining data and sample strategy). Our approach can con-\nMethod None Rule Ours Both\nError Type Ratio F0.5 F0.5 F0.5 F0.5\nPunctuation 14.9 58.67 72.41 73.98 73.75\nSpelling 14.0 43.89 76.73 77\n.64 78.76\nOther 9.8 8.64 29.18 35.66 31.51\nDeterminer:FORM 9.7 58.43 80.62 81.39 81.90\nOrthograph\ny 8.3 66.38 76.33 73.70 77.72\nAdposition 5.6 28.15 52.29 53.20 51.71\nDeterminer 4.7 25.00 50.10 55.91 57.14\nAdjecti\nve:FORM 4.0 57.14 81.44 82.29 83.41\nPronoun 3.9 19.44 47.87 45.81 51.40\nTable 6: Performance of synthetic data construction approaches on\ntop 9 error types of the German dataset.\ntrol the degree of overlap and errors, while the sentences gen-\nerated by AT have few grammatical errors or little overlap\nwith the original sentences. The removal of NAT-based re-\nplacement and bilingual constraint also results in a significant\ndegradation, which indicates substitution with similar seman-\ntic meanings plays a crucial role in our strategy.\n3.6 Error-type Analysis\nWe analyze the GEC performance of data construction ap-\nproaches on different error types. We use the German ex-\ntension [Boyd, 2018] of the automatic error annotation tool\nERRANT [Bryant et al., 2017] for evaluation. Table 6 shows\nthe F0.5 score of top 9 error types on the German dataset. We\ncan observe that our approach improves the model in all er-\nror types significantly compared with that trained from the\nscratch and outperforms that with rule-based corruption in\n7 out of 9 error types. For example, the largest improve-\nment comes from the ‘Other’ type by 6.5 F0.5 score, which\nis defined as the errors that do not fall into any other specific\ntype, such as paraphrasing (e.g., feel happy→be delighted)\n[Bryant et al., 2017]. Such error type is beyond pre-defined\nrules and hard to simulate [Zhou et al., 2020].\nTwo exceptions are ‘Orthography’ and ‘Pronoun’. ‘Or-\nthography’ refers to the error related with case or whitespace\nerrors (e.g., Nexttime →next time), which the specific rules\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4371\nSPELL\n38.6%\nOTHER\n32.7%\nNOUN\n14.6%\n4.3% ORTH \n9.8%   PNOUN\n(a) Rule-based corruption.\nSPELL\n28.9%OTHER \n36.3%\nNOUN\n14.3%\nPUNCT \n11.4%\nADP\n9.2% (b) Our approach.\nFigure 3: Top 5 error types distribution of different synthetic data\nconstruction approaches.\nare able to simulate very well. ‘Pronoun’ denotes the sub-\nstitutes for the noun (e.g. you →yourself ) which also fall\ninto the strength of the rule-based approach with language-\nspecific confusion set. We also observe the combination of\nthese two sources of synthetic data yields better results, which\ndemonstrates that they are helpful to complement each other\nto enrich error-corrected patterns.\nTo verify our explanation, we present the top 5 error types\ndistribution of rule-based corruption and our approach in Fig-\nure 3. Our approach yields more ‘Other’ type errors com-\npared with ‘Spell’ errors, which may account for the improve-\nment in that category. The large ratio of ‘Orthography’ and\n‘Pronoun’ errors generated by rule-based corruption is con-\nsistent with its better performance on these two types.\n3.7 Case Study\nSource 总之，\n她们的生活质量非常低。\n(In short, their quality of life is very poor.)\nBT 总之她们的生活质量非常不好1。\n(In short their quality of life is very bad1.)\nRule 总，之她闷1的生活效率2非常低。\n(In, short her boring1 life’s efficiency2 is very poor.)\nRT 简言1之，他们的生活质量很2低。\n(In short, their quality of life is very poor.)\nOurs 可以说1，她们的生命2素质3非常弱4。\n(So to speak1, their quality3 of life2 is very weak4.)\nTable 7: Examples of synthetic erroneous sentences. The rewrit-\nten tokens are highlighted in blue. BT, Rule and RT denote back-\ntranslation, rule-based corruption and round-trip translation.\nTo give a qualitative analysis of generated erroneous sen-\ntences, we present an example of our approach and exist-\ning synthetic methods in Table 7. We can see that back-\ntranslation tends to generate similar modifications such as to-\nken deletion and simple paraphrasing. The rule-based cor-\nruption approach is hard to simulate human writing since it\ndirectly swaps adjacent tokens and performs word replace-\nment without consideration of the sentence context. Round-\ntrip translation generates an error-free sentence. In contrast,\nour approach generates the less fluent sentence by paraphras-\ning the corrupted contents and maintaining the meaning of\nthe corresponding English sentence.\n4 Related Work\nPre-training a seq2seq model on synthetic data and then fine-\ntuning on annotated error-corrected sentence pairs is the com-\nmon practice for GEC. Available datasets in non-English lan-\nguages such as German [Boyd, 2018], Russian [Rozovskaya\nand Roth, 2019] and Czech [N´aplava and Straka, 2019] only\ncontain a lack of annotated data, which requires high-quality\nsynthetic data construction.\nBack-translation is the reverse of GEC, which takes cor-\nrected sentences as input and error sentences as output. It is\npopular and effective for English GEC [Kiyono et al., 2019;\nZhang et al., 2019] but difficult to adapt to these low-resource\nscenarios, since it is hard to learn diverse error-corrected pat-\nterns with less annotated sentence pairs. Round-trip transla-\ntion (e.g., translating German to English then back to Ger-\nman) [Lichtarge et al., 2019] has been blamed for errors that\nit introduced are relatively clean.\nThe most effective construction method for non-English\nlanguages is rule-based corruption [N´aplava and Straka,\n2019; Grundkiewicz and Junczys-Dowmunt, 2019]. Most of\nthem rely on word substitutions with ASpell or language-\nspecific confusion sets. It requires well-designed rules to\nsimulate diverse linguistic phenomena across different lan-\nguages. [Rothe et al., 2021] only performs language-agnostic\noperations without any confusion set to construct corrupted\nsentences but achieves inferior performance with moderate\nmodel size. Wikipedia edits extracted from the revision\nhistory of each page are also useful GEC pre-training re-\nsources [Boyd, 2018; Flachs et al., 2021]. Most studies for\nChinese GEC focus on model architecture [Fu et al., 2018;\nZhou et al., 2018; Ren et al., 2018 ] and training strategy\n[Zhao and Wang, 2020 ], which are orthogonal with our ap-\nproach.\nThe most similar approach to ours is [Zhou et al., 2020]\nwhich trains two autoregressive translation models with poor\nand good qualities, respectively. With the same sentence\nin the source language, they regard two translations of two\nmodels as error-corrected sentence pairs. In comparison, our\napproach directly utilizes the non-autoregressive translation\nability of the PXLM without training translators additionally,\nwhich is easier to adapt to new languages. Utilizing a pre-\ntrained language model to propose candidate words for re-\nplacement and insertion has also been applied to lexical sub-\nstitution [Zhou et al., 2019], text generation [Li et al., 2020],\netc. By contrast, we adopt bilingual constraints to avoid gen-\nerating candidate words that are inconsistent with the original\nmeaning.\nLeveraging pre-trained language models in GEC seq2seq\nmodels has been extensively explored [Katsumata and Ko-\nmachi, 2020; Wang et al., 2020b; Kaneko et al., 2020]. We\ninitialize the model with DeltaLM [Ma et al., 2021], which\nadjusts the InfoXLM-initialized encoder-decoder model to\ngeneration mode by self-supervised pre-training.\n5 Conclusion and Future Work\nWe propose a unified and generic strategy for training GEC\nsystems in non-English languages given a PXLM and the\nparallel translation data. Our approach obtains state-of-the-\nart results on the NLPCC 2018 Task 2 dataset (Chinese) and\ncompetitive results on German and Russian benchmarks. The\nsynthetic sentence pairs also complement rule-based corrup-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4372\ntion to yield further improvement. Compared with a regular\ntranslator, NAT by the PXLM can control the degree of over-\nlap between the generated sentence and the original sentence.\nThe bilingual constraint also ensures that the sampled tokens\nwill not deviate from the original meaning, which plays an\nimportant role in our strategy.\nWe plan to investigate whether utilizing back-translated\nEnglish sentences rather than gold English sentences leads\nto similar performance, which could get rid of quantitative\nrestriction by the size of the translation corpus to generate an\nunlimited number of error-corrected sentence pairs.\nA Hyper-parameters\nThe parameters for NAT-based data construction and post edit\nare presented in Table 8 and Table 9. The hyper-parameters\nof training the Transformer on NLPCC 2018 Task 2 (Chinese)\nare listed in Table 10. The hyper-parameters for German and\nRussian are shown in Table 11. For Russian, we use rule-\nbased corruption without any language-specific operation and\nconfusion set with 50% probability to assist in synthetic data\nconstruction.\nLanguage pnoise mask insert delete sw\nap\nChinese 0.5 0.7 0.1 0.1\n0.1\nGerman 0.3 0.65 0.15\n0.15 0.05\nRussian 0.15 0.65 0.15 0.15\n0.05\nTable 8: Parameters for NAT-based data construction.\nLanguage pnoise substitute insert delete sw\nap recase\nChinese 0.05 0.3 0.2 0.3\n0.2 0\nGerman 0.02 0.25 0.25 0.2\n0.2 0.1\nRussian 0.02 0.25 0.25 0.2\n0.2 0.1\nTable 9: Parameters for post edit.\nConfigurations Values\nPre-training\nModel Architecture Transformer (base)\nDe\nvices 8 Nvidia V100 GPU\nMax tokens per GPU 5120\nUpdate Frequency 8\nOptimizer Adam (β1=0.9, β2=0.98, ϵ=1 ×10−8)\nLearning rate 7 ×10−4\nLearning rate scheduler polynomial decay\nWarmup 8000\nWeight decay 0.0\nLoss Function label smoothed cross entropy\n(label-smoothing=0.1)\nDropout 0.3\nFine-tuning\nDevices 4 Nvidia V100\nGPU\nTraining Strategy MaskGEC [Zhao and Wang, 2020]\nUpdate Frequency [2, 4]\nLearning rate [5 ×10−4, 7 ×10−4]\nWarmup 4000\nTable 10: Hyper-parameters values of pre-training and fine-tuning\non NLPCC 2018 Task 2 (Chinese).\nAcknowledgments\nWe thank all the reviewers for their valuable comments to im-\nprove our paper. The work is supported by National Natural\nConfigurations Values\nPre-training\nModel Architecture Transformer (base)\nDe\nvices 8 Nvidia V100 GPU\nMax tokens per GPU 5120\nUpdate Frequency 8\nOptimizer Adam (β1=0.9, β2=0.98, ϵ=1 ×10−8)\nLearning rate [5 ×10−4, 7 ×10−4]\nLearning rate scheduler polynomial decay\nWarmup 8000\nWeight decay 0.0\nLoss Function label smoothed cross entropy\n(label-smoothing=0.1)\nDropout [0.1, 0.2]\nSource Dropout 0.2\nTarget Dropout 0.1\nEdit-weighted MLE 3\nFine-tuning\nDevices 1 Nvidia V100\nGPU\nUpdate Frequency [2, 4]\nLearning rate [3 ×10−4, 5 ×10−4, 7 ×10−4]\nDropout [0.1, 0.2, 0.3]\nWarmup 2000\nTable 11: Hyper-parameters values of pre-training and fine-tuning\non Falko-Merlin (German) and RULEC-GEC (Russian).\nScience Foundation of China under Grant No.62036001 and\nPKU-Baidu Fund (No.2020BD021). The corresponding au-\nthor of this paper is Houfeng Wang.\nReferences\n[Boyd, 2018] Adriane Boyd. Using wikipedia edits in low\nresource grammatical error correction. In Proceedings of\nthe 2018 EMNLP Workshop W-NUT: The 4th Workshop on\nNoisy User-generated Text, 2018.\n[Bryant et al., 2017] Christopher Bryant, Mariano Felice,\nand Ted Briscoe. Automatic annotation and evaluation of\nerror types for grammatical error correction. In Proc. of\nACL, 2017.\n[Chi et al., 2020] Zewen Chi, Li Dong, Furu Wei, Nan\nYang, Saksham Singhal, Wenhui Wang, Xia Song,\nXian-Ling Mao, Heyan Huang, and Ming Zhou. In-\nfoxlm: An information-theoretic framework for cross-\nlingual language model pre-training. arXiv preprint\narXiv:2007.07834, 2020.\n[Dahlmeier and Ng, 2012] Daniel Dahlmeier and Hwee Tou\nNg. Better evaluation for grammatical error correction. In\nProc. of ACL, 2012.\n[Du et al., 2021] Cunxiao Du, Zhaopeng Tu, and Jing Jiang.\nOrder-agnostic cross entropy for non-autoregressive ma-\nchine translation. arXiv preprint arXiv:2106.05093, 2021.\n[Flachs et al., 2021] Simon Flachs, Felix Stahlberg, and\nShankar Kumar. Data strategies for low-resource gram-\nmatical error correction. In Proceedings of the 16th Work-\nshop on Innovative Use of NLP for Building Educational\nApplications, 2021.\n[Fu et al., 2018] Kai Fu, Jin Huang, and Yitao Duan.\nYoudao’s winning solution to the nlpcc-2018 task 2 chal-\nlenge: a neural machine translation approach to chinese\ngrammatical error correction. In Proc. of NLPCC, 2018.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4373\n[Ge et al., 2018a] Tao Ge, Furu Wei, and Ming Zhou. Flu-\nency boost learning and inference for neural grammatical\nerror correction. In Proc. of ACL, 2018.\n[Ge et al., 2018b] Tao Ge, Furu Wei, and Ming Zhou.\nReaching human-level performance in automatic gram-\nmatical error correction: An empirical study. arXiv\npreprint arXiv:1807.01270, 2018.\n[Ghazvininejad et al., 2019] Marjan Ghazvininejad, Omer\nLevy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict:\nParallel decoding of conditional masked language models.\nIn Proc. of EMNLP, 2019.\n[Grundkiewicz and Junczys-Dowmunt, 2019] Roman\nGrundkiewicz and Marcin Junczys-Dowmunt. Minimally-\naugmented grammatical error correction. W-NUT 2019,\n2019.\n[Gu et al., 2017] Jiatao Gu, James Bradbury, Caiming\nXiong, Victor OK Li, and Richard Socher. Non-\nautoregressive neural machine translation. arXiv preprint\narXiv:1711.02281, 2017.\n[Junczys-Dowmunt et al., 2018] Marcin Junczys-Dowmunt,\nRoman Grundkiewicz, Shubha Guha, and Kenneth\nHeafield. Approaching neural grammatical error correc-\ntion as a low-resource machine translation task. In Proc.\nof ACL, 2018.\n[Kaneko et al., 2020] Masahiro Kaneko, Masato Mita, Shun\nKiyono, Jun Suzuki, and Kentaro Inui. Encoder-decoder\nmodels can benefit from pre-trained masked language\nmodels in grammatical error correction. In Proc. of ACL,\n2020.\n[Katsumata and Komachi, 2020] Satoru Katsumata and\nMamoru Komachi. Stronger baselines for grammatical\nerror correction using a pretrained encoder-decoder\nmodel. In Proc. of ACL, 2020.\n[Kiyono et al., 2019] Shun Kiyono, Jun Suzuki, Masato\nMita, Tomoya Mizumoto, and Kentaro Inui. An empir-\nical study of incorporating pseudo data into grammatical\nerror correction. In Proc. of EMNLP, 2019.\n[Li et al., 2020] Jingjing Li, Zichao Li, Lili Mou, Xin Jiang,\nMichael Lyu, and Irwin King. Unsupervised text genera-\ntion by learning from search. In Proc. of NeurIPS, 2020.\n[Lichtarge et al., 2019] Jared Lichtarge, Chris Alberti,\nShankar Kumar, Noam Shazeer, Niki Parmar, and Si-\nmon Tong. Corpora generation for grammatical error\ncorrection. In Proc. of ACL, 2019.\n[Ma et al., 2021] Shuming Ma, Li Dong, Shaohan Huang,\nDongdong Zhang, Alexandre Muzio, Saksham Singhal,\nHany Hassan Awadalla, Xia Song, and Furu Wei. Deltalm:\nEncoder-decoder pre-training for language generation and\ntranslation by augmenting pretrained multilingual en-\ncoders. arXiv preprint arXiv:2106.13736, 2021.\n[N´aplava and Straka, 2019] Jakub N ´aplava and Milan\nStraka. Grammatical error correction in low-resource\nscenarios. In Proceedings of the 5th Workshop on Noisy\nUser-generated Text (W-NUT 2019), 2019.\n[Ran et al., 2020] Qiu Ran, Yankai Lin, Peng Li, and Jie\nZhou. Learning to recover from multi-modality errors for\nnon-autoregressive neural machine translation. In Proc. of\nACL, 2020.\n[Ren et al., 2018] Hongkai Ren, Liner Yang, and Endong\nXun. A sequence to sequence learning for chinese gram-\nmatical error correction. In Proc. of NLPCC, 2018.\n[Rothe et al., 2021] Sascha Rothe, Jonathan Mallinson, Eric\nMalmi, Sebastian Krause, and Aliaksei Severyn. A simple\nrecipe for multilingual grammatical error correction.arXiv\npreprint arXiv:2106.03830, 2021.\n[Rozovskaya and Roth, 2019] Alla Rozovskaya and Dan\nRoth. Grammar error correction in morphologically rich\nlanguages: The case of russian. Transactions of the Asso-\nciation for Computational Linguistics, 2019.\n[Sun et al., 2021] Xin Sun, Tao Ge, Furu Wei, and Houfeng\nWang. Instantaneous grammatical error correction with\nshallow aggressive decoding. In Proc. of ACL, 2021.\n[Wang et al., 2020a] Chencheng Wang, Liner Yang, Yingy-\ning Wang, Yongping Du, and Erhong Yang. Chinese gram-\nmatical error correction method based on transformer en-\nhanced architecture. Journal of Chinese Information Pro-\ncessing, 2020.\n[Wang et al., 2020b] Hongfei Wang, Michiki Kurosawa,\nSatoru Katsumata, and Mamoru Komachi. Chinese gram-\nmatical correction using bert-based pre-trained model.\narXiv preprint arXiv:2011.02093, 2020.\n[Zhang et al., 2019] Yi Zhang, Tao Ge, Furu Wei, Ming\nZhou, and Xu Sun. Sequence-to-sequence pre-training\nwith data augmentation for sentence rewriting. arXiv\npreprint arXiv:1909.06002, 2019.\n[Zhao and Wang, 2020] Zewei Zhao and Houfeng Wang.\nMaskgec: Improving neural grammatical error correction\nvia dynamic masking. In Proc. of AAAI, 2020.\n[Zhao et al., 2018] Yuanyuan Zhao, Nan Jiang, Weiwei Sun,\nand Xiaojun Wan. Overview of the nlpcc 2018 shared task:\nGrammatical error correction. In Proc. of NLPCC, 2018.\n[Zhou et al., 2018] Junpei Zhou, Chen Li, Hengyou Liu,\nZuyi Bao, Guangwei Xu, and Linlin Li. Chinese grammat-\nical error correction using statistical and neural models. In\nProc. of NLPCC, 2018.\n[Zhou et al., 2019] Wangchunshu Zhou, Tao Ge, Ke Xu,\nFuru Wei, and Ming Zhou. Bert-based lexical substitution.\nIn Proc. of ACL, 2019.\n[Zhou et al., 2020] Wangchunshu Zhou, Tao Ge, Chang Mu,\nKe Xu, Furu Wei, and Ming Zhou. Improving grammatical\nerror correction with machine translation pairs. In Proc. of\nEMNLP, 2020.\n[Zhou et al., 2021] Wangchunshu Zhou, Tao Ge, Canwen\nXu, Ke Xu, and Furu Wei. Improving sequence-to-\nsequence pre-training via sequence span rewriting. In\nProc. of EMNLP, 2021.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4374",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8711631894111633
    },
    {
      "name": "Natural language processing",
      "score": 0.6451393365859985
    },
    {
      "name": "Sentence",
      "score": 0.6157509684562683
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6032872200012207
    },
    {
      "name": "Machine translation",
      "score": 0.5127502679824829
    },
    {
      "name": "Task (project management)",
      "score": 0.48175597190856934
    },
    {
      "name": "Language model",
      "score": 0.47426068782806396
    },
    {
      "name": "Translation (biology)",
      "score": 0.47141405940055847
    },
    {
      "name": "German",
      "score": 0.4391520619392395
    },
    {
      "name": "Linguistics",
      "score": 0.15866881608963013
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}