{
  "title": "Towards Effective Disambiguation for Machine Translation with Large Language Models",
  "url": "https://openalex.org/W4389524470",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2170912136",
      "name": "Vivek Iyer",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2158079484",
      "name": "PinZhen Chen",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2099661360",
      "name": "Alexandra Birch",
      "affiliations": [
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4318903120",
    "https://openalex.org/W4375957915",
    "https://openalex.org/W4385572225",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W3028987338",
    "https://openalex.org/W3169262646",
    "https://openalex.org/W4385572075",
    "https://openalex.org/W3082928416",
    "https://openalex.org/W4317547647",
    "https://openalex.org/W3093871477",
    "https://openalex.org/W2139183784",
    "https://openalex.org/W2894627709",
    "https://openalex.org/W4300963525",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2963277143",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2962697716",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3174724858",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W2971141904",
    "https://openalex.org/W22168010",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2136925175",
    "https://openalex.org/W4392669908",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4322760121",
    "https://openalex.org/W4285060763",
    "https://openalex.org/W2263338482",
    "https://openalex.org/W2758950307",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4379933104",
    "https://openalex.org/W3105990194",
    "https://openalex.org/W3190186724",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4285077564",
    "https://openalex.org/W3120391121"
  ],
  "abstract": "Resolving semantic ambiguity has long been recognised as a central challenge in the field of Machine Translation. Recent work on benchmarking translation performance on ambiguous sentences has exposed the limitations of conventional Neural Machine Translation (NMT) systems, which fail to handle many such cases. Large language models (LLMs) have emerged as a promising alternative, demonstrating comparable performance to traditional NMT models while introducing new paradigms for controlling the target outputs. In this paper, we study the capabilities of LLMs to translate \"ambiguous sentences\" - i.e. those containing highly polysemous words and/or rare word senses. We also propose two ways to improve their disambiguation capabilities, through a) in-context learning and b) fine-tuning on carefully curated ambiguous datasets. Experiments show that our methods can match or outperform state-of-the-art systems such as DeepL and NLLB in four out of five language directions. Our research provides valuable insights into effectively adapting LLMs to become better disambiguators during Machine Translation. We release our curated disambiguation corpora and resources at https://data.statmt.org/ambiguous-europarl.",
  "full_text": "Proceedings of the Eighth Conference on Machine Translation (WMT), pages 482–495\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n482\nTowards Effective Disambiguation for Machine Translation\nwith Large Language Models\nVivek Iyer Pinzhen Chen Alexandra Birch\nSchool of Informatics, University of Edinburgh\n{vivek.iyer, pinzhen.chen, a.birch}@ed.ac.uk\nAbstract\nResolving semantic ambiguity has long been\nrecognised as a central challenge in the field of\nMachine Translation. Recent work on bench-\nmarking translation performance on ambiguous\nsentences has exposed the limitations of con-\nventional Neural Machine Translation (NMT)\nsystems, which fail to handle many such cases.\nLarge language models (LLMs) have emerged\nas a promising alternative, demonstrating com-\nparable performance to traditional NMT mod-\nels while introducing new paradigms for con-\ntrolling the target outputs. In this paper, we\nstudy the capabilities of LLMs to translate\n“ambiguous sentences\" - i.e. those contain-\ning highly polysemous words and/or rare word\nsenses. We also propose two ways to improve\ntheir disambiguation capabilities, through a)\nin-context learning and b) fine-tuning on care-\nfully curated ambiguous datasets. Experiments\nshow that our methods can match or outper-\nform state-of-the-art systems such as DeepL\nand NLLB in four out of five language direc-\ntions. Our research provides valuable insights\ninto effectively adapting LLMs to become bet-\nter disambiguators during Machine Translation.\nWe release our curated disambiguation corpora\nand resources at https://data.statmt.org/\nambiguous-europarl.\n1 Introduction\nWhile the field of NMT has advanced rapidly in\nrecent times, the disambiguation and translation of\nambiguous words still remain an open challenge.\nNotably, Campolungo et al. (2022) created a bench-\nmark named DiBiMT to study the behaviour of\nstate-of-the-art (SOTA) NMT systems when trans-\nlating sentences with ambiguous words. 1 They\nreported that even the best-performing commercial\nNMT systems yielded accurate translations only\n1https://nlp.uniroma1.it/dibimt/public/\nleaderboard\nSource The horse had a blaze between its eyes.\nDeepL 那匹马的两眼之间有一团火焰。\n(There is a flame between the horse’s eyes.)\nBLOOMZ\n(176B)\n这匹马的眼睛之间有一道白线。\n(There is a white line between the horse’s eyes.)\nTable 1: An example of English-to-Chinese translation\ninvolving an ambiguous term “blaze”. For BLOOMZ,\nwe use 1-shot prompting to obtain the translation.\n50-60% of the time,2 while other open-source mul-\ntilingual models like mBART50 (Tang et al., 2021)\nand M2M100 (Fan et al., 2021) performed much\nworse. This was found to be due to biases against\nrare and polysemous word senses inherited during\npretraining. Table 1 shows an example from the\nDiBiMT benchmark where DeepL3 mistranslates\nan ambiguous word while the LLM BLOOMZ re-\nsolves the word to its correct in-context meaning.\nIn this paper, we explore whether LLMs can\nindeed perform better at translating “ambiguous\nsentences\" – i.e. those containing highly polyse-\nmous and/or rare word senses. The motivation be-\nhind this is that while NMT models can potentially\nlearn biases from noisy or narrow domain parallel\ndata, hurting their ability to detect and translate rare\nword senses, LLMs can potentially be pretrained\non a wider variety of monolingual text – though\nthey might also prefer fluency over accuracy. Still,\nLLMs have shown many emergent abilities due to\nscale (Brown et al., 2020; Chowdhery et al., 2022;\nWei et al., 2022a) and moreover, have demonstrated\ngreat potential for Machine Translation (MT) (Vilar\net al., 2023; Zhang et al., 2023).\nWe comprehensively examine how these trends\nextend to the specific task of translating ambigu-\nous sentences. We select a diverse set of foun-\ndational and instruction-tuned LLMs, of different\n2Subsequent iterations of these commercial models have im-\nproved, but large margins still remain.\n3https://deepl.com/en/translator\n483\nsizes and with varying combinations of languages\nin the pre-training data. We then compare how\nthese LLMs match up against several widely used\nNMT models on the DiBiMT test set, which covers\ntranslation from English to five languages: Span-\nish, Italian, German, Russian and Chinese. We find\nthat, with only 1-shot in-context learning (Brown\net al., 2020), LLMs – in particular, BLOOMZ\n176B (Muennighoff et al., 2023) and LLaMA 65B\n(Touvron et al., 2023) – match or outperform top-\nperforming open-source and commercial MT sys-\ntems, and set a new SOTA in two of the five lan-\nguages we tested. Furthermore, we propose two\nmethods for adapting LLMs for ambiguous transla-\ntion: 1) in-context learning with sentences having\nthe same word sense, and 2) fine-tuning on curated\nambiguous parallel corpora. We show that these\nmethods are highly effective and can further im-\nprove performance by up to 15 points in DiBiMT\naccuracy in the best case.\nOur work thus makes three key contributions:\n1. We evaluate the performance of LLMs com-\npared to top-performing NMT systems in the\nchallenging task of translating ambiguous sen-\ntences. We report SOTA scores on 2 of the\n5 languages tested, and comparable perfor-\nmance otherwise.\n2. We also show that our suggested techniques\nof similar sentence in-context learning and tar-\ngeted disambiguation fine-tuning significantly\noutperform naive few-shot prompting\n3. We conclude our work by evaluating LLMs on\nthe FLORES200 test sets, and confirm that im-\nprovements in disambiguation accuracy corre-\nlate strongly with those in overall MT quality.\n2 Background\n2.1 Ambiguity in machine translation\nResolving ambiguity in the source sentence was\nhistorically framed as one of the most fundamen-\ntal challenges in MT (Weaver, 1952). In an effort\nto address this challenge, traditional works inte-\ngrating Word Sense Disambiguation in Statistical\nMachine Translation (Carpuat and Wu, 2007; Chan\net al., 2007) were followed by those integrating it\nin NMT architectures in various ad-hoc ways (Choi\net al., 2017; Liu et al., 2018; Pu et al., 2018). Later,\nwith the introduction of the Transformer (Vaswani\net al., 2017), it was shown that higher layer encoder\nrepresentations are robust enough to handle disam-\nbiguation (Tang et al., 2019) without any explicit\nhandling of word senses.\nHowever, more recent research creating challeng-\ning evaluation benchmarks has called the purported\nabilities of NMT systems into question once again.\nFollowing the proposal of the MuCoW benchmark\nfor testing WMT19 (Raganato et al., 2019) and\nWMT20 (Scherrer et al., 2020) systems, Raganato\net al. (2020a) showed how Transformer-based\nNMT models, in general, underperform when\ntranslating rare word senses. Campolungo et al.\n(2022), who experimented with SOTA commercial\n(Google Translate, DeepL) and open-source sys-\ntems (mBART50, M2M100, OPUS-NMT (Tiede-\nmann and Thottingal, 2020), etc.), arrived at the\nsame conclusion when they proposed the DiBiMT\nbenchmark for evaluating MT systems between En-\nglish and 5 languages (Spanish, Italian, German,\nRussian, and Chinese). They found similar biases\nagainst low-frequency and highly polysemous word\nsenses. They also noted the accuracies of these sys-\ntems were much lower than the then SOTA WSD\nsystem, ESCHER (Barba et al., 2021) – indicating\nsignificant room for improvement. In this work,\nwe explored whether foundational and instruction-\ntuned LLMs could bridge this gap with minimal\nsupervision (i.e. few-shot prompting).\n2.2 LLMs and machine translation\nPrevious research has found that LLMs can per-\nform machine translation without being specifically\nfine-tuned (Radford et al., 2019). In order to elicit\na translation, research in this direction follows the\nparadigm of LLM prompting:\n1. Zero-shot prompting, where an LLM is di-\nrectly asked to translate a source input into\nthe target language (Radford et al., 2019).\n2. Few-shot prompting, also called in-context\nlearning, where an LLM is supplied with\ndemonstrations of input and output pairs from\nthe same task it is performing, before being\nqueried an input (Brown et al., 2020).\n3. Chain-of-thought (CoT), where an LLM is\nprompted to reason to gain relevant knowl-\nedge about the input before producing an out-\nput (Wei et al., 2022b; Kojima et al., 2022).\nBesides training-free approaches, another route is\ninstruction tuning, which optimizes an LLM on a\n484\nmixed range of downstream tasks and fine-tunes the\nmodel to understand and respond to user intention\nthrough natural language (Wei et al., 2021).\nIt was observed that LLMs might not surpass\nTransformer models solely trained to translate, es-\npecially for non-English and low-resource trans-\nlation directions (Vilar et al., 2023; Hendy et al.,\n2023). Nevertheless, LLMs have been shown to\nachieve superiority in tasks requiring in-depth un-\nderstanding and manipulation of text, primarily\ndue to them being pretrained on very large cor-\npora. For example, without fine-tuning, LLMs\nare good at adapting to word alignments (Moslem\net al., 2023), translation evaluation (Kocmi and Fe-\ndermann, 2023), idiom translation (Raunak et al.,\n2023), iterative refinement (Chen et al., 2023), and\ninteractive translation via CoT (Pilault et al., 2023;\nHe et al., 2023). Related to our work is Pilault\net al. (2023)’s proposal of using interactive question\nanswering as a CoT process for LLMs to disam-\nbiguate source words. As an alternative approach,\nwe aim to generate translations in a single pass by\nleveraging SOTA WSD systems to provide contexts\nthat guide LLMs to disambiguate better.\n3 Methodology\n3.1 Preliminaries\nA word sense is a concept in a Knowledge Base\n(in this work, BabelNet by Navigli et al. (2021))\nthat denotes a distinct meaning of a word in the\ncontext of a sentence. The polysemy degree of an\nambiguous word is defined as the total count of\nall possible senses that a particular word can have.\nThe sense frequency is defined as the occurrence\ncount of that particular sense in a disambiguated\ntraining corpus.\nIn this work, we define an ambiguous word as a\npolysemous term with multiple possible, and likely\nrelated, meanings – with the correct sense inferable\nonly from the sentence-level context. We then re-\nfer to a sentence with an ambiguous word as an\n“ambiguous sentence” for brevity and ease of expla-\nnation. By definition, the DiBiMT test set (Cam-\npolungo et al., 2022) contains only one ambiguous\nword per sentence.\nWord Sense Disambiguation (WSD) is the pro-\ncess of linking an ambiguous word in a sentence\nto its appropriate word sense in the Knowledge\nBase. We use ESCHER-WSD (Barba et al., 2021)\nin this work, a high-performing WSD system that\nhad achieved the SOTA for English.\n3.2 K-shot prompting\nGiven a test sentence X and a Large Language\nModel to prompt for translations, we construct a\nquery with k demonstrations, i.e. parallel sentence\npairs {(X1, Y1), (X2, Y2) . . .(Xk, Yk)} as exam-\nples, followed by the test sentence. As shown\nin Figure 1, for foundation LLMs, we frame\nthe prompt as a text completion task, while for\ninstruction-tuned LLMs (like BLOOMZ) we struc-\nture the last phrase as a question, in order to con-\nform to the latter’s question answering format. In\nthe naive setting, we choose our demonstrations\nrandomly from the development set.\n3.3 In-context learning with similar\nambiguous contexts\nLLMs can effectively gain knowledge relevant to\nthe test domain through prompting, and this process\nis named in-context learning (ICL). We leverage\nICL to help LLMs ingest information on translation\nof ambiguous sentences, by providing related sense\ntranslations as examples in the prompt. To achieve\nthis, we first identify the most polysemous word\nin the input sentence by disambiguating it with a\nWSD system, and then calculate the polysemy de-\ngree of all disambiguated senses with respect to a\nlarge development set. We choose the most polyse-\nmous word sense4 and search for other occurrences\nof the same sense in the same development set. Fi-\nnally, we randomly sample k source-target pairs\nincluding such a sense to use as demonstrations in\nk-shot prompting, instead of using random pairs.\nThis technique seemed to return enough examples\nfor our purposes in most cases – for 5-shot prompt-\ning, given a corpus of 1.8M sentences, we observed\nthat we got all 5 matches 92.5% of the time.\n3.4 Low-rank fine-tuning\nApart from providing relevant examples through\nprompting, another conventional approach is to op-\ntimize the model parameters in a domain adaptation\nfashion for disambiguation. Considering the com-\nputational cost, our work experiments with instruc-\ntion fine-tuning via low-rank adaptation (LoRA).\nThis technique appends trainable lower-rank de-\ncomposition matrices to giant matrices in an LLM\n4Currently, we only explore the case of one ambiguous word\nper sentence, due to the nature of the benchmark. One could\nextend our approach to multiple ambiguous words by sepa-\nrately sampling examples for each polysemous word and con-\nducting higher-shot prompting - but further research would\nbe needed to find the optimal way to combine these examples.\n485\nTranslate the following sentence from {src_lang} to {tgt_lang}: {src_demo1}\nThe translation in {tgt_lang} is: {tgt_demo1}\nTranslate the following sentence from {src_lang} to {tgt_lang}: {src_demok}\nThe translation in {tgt_lang} is: {tgt_demok}\nTranslate the following sentence from {src_lang} to {tgt_lang}: {src_test}\nThe translation in {tgt_lang} is:\nk demonstrations\n[FOUNDATION LLM]\nOR\nTranslate the following sentence from {src_lang} to {tgt_lang}: {src_test}\nCan you translate the input sentence to {tgt_lang}?\n[INSTRUCTION-TUNED LLM]\nFigure 1: Templates used for k-shot LLM prompting, with k >= 0.\nthat can remain frozen during fine-tuning (Hu et al.,\n2021). By sacrificing a little performance, this fine-\ntuning method achieves great parameter efficiency.\nWe aim to adjust LLMs to perform the translation\ntask specifically. In order to maximise an LLM’s\ncapability to disambiguate when translating, we fol-\nlow a careful data selection procedure to identify\nthe most ambiguous sentences in our corpus.\nGiven the size of LLMs, it would be infeasible\nto fine-tune them on a large parallel corpus, so we\nopt to curate a smaller dataset that suits the am-\nbiguous translation task. We would like a balanced\nmix of sentences with highly polysemous words\nas well as those with rare senses of a given word.\nThis is to ensure fine-tuning reduces both polysemy\ndegree-related and sense frequency-related biases,\nas discovered by Campolungo et al. (2022) and con-\nsequently, maximises disambiguation performance.\nWe, thus, sort our corpora in two ways: one, by the\nmaximum polysemy degree (greatest first) and two,\nby the minimum sense frequency (rarest first) of\nall word senses in a given sentence, disambiguated\nwith ESCHER-WSD. We take the top N/2 sen-\ntences from each set and interleave them to create\nour final fine-tuning corpus of size N. We release\nour fine-tuning corpus, along with the ESCHER-\nWSD disambiguation outputs for public use.5\nOnce the data is chosen, we follow the fine-\ntuning paradigm of Alpaca (Taori et al., 2023): the\nmodel is prompted with an instruction specifying\nthe source and target languages, as well as the test\nsentence as an input, and the model is expected to\nrespond with the translation.6\n5https://data.statmt.org/ambiguous-europarl\n6https://github.com/tatsu-lab/stanford_alpaca\n4 Experiments\nIn this section, we seek to answer the following\nresearch questions:\n1. RQ1: How do LLMs perform at translation of\nambiguous sentences compared to traditional\nhigh-performing NMT systems? (Section 4.3)\n2. RQ2: What methods could one use to adapt\nLLMs for this task and improve performance\nover naive few-shot prompting? (Section 4.4)\n3. RQ3: How do these disambiguation-adapted\nLLMs fare in terms of overall translation qual-\nity? (Section 4.5)\n4.1 Models\nTo ensure reproducibility, we pick four well-known\nand high-performing open-source LLMs,7 of which\nwe sample seven versions for experimentation:\n• BLOOM (Scao et al., 2022): A fully open-\nsource, multilingual, foundation LLM that\nsupports 46 languages. To establish the range\nof its capabilities, we explore both the small-\nest (7.1B) and the largest (176B) versions.\n• BLOOMZ (Muennighoff et al., 2023):\nBLOOM instruction-tuned on a multilingual\nprompting set. Again, we choose the smallest\n(7.1B) and the largest (176B) versions.\n• LLaMA (Touvron et al., 2023): The popular\nLLM trained by Meta AI, on gigantic datasets\nranging up to 1.5T tokens. We evaluate the\nsmallest (7B) and the largest (65B) versions.\n7at the time of experiment formulation\n486\n• Alpaca (Taori et al., 2023): A LLaMA model\ninstruction-tuned on a 52K dataset generated\nusing Self-Instruct (Wang et al., 2023).\nTo effectively position these open-source LLMs\nagainst traditional NMT systems, we compare them\nagainst the best-performing and the most widely\nused commercial and open-source models:\n1. DeepL Translator 8: a SOTA commercial\nNMT system (accessed on 24th July 2023).\n2. Google Translate9: Probably the most widely\nused commercial NMT system (accessed on\n24th July 2023).\n3. OPUS (Tiedemann and Thottingal, 2020):\nSmall, bilingual, Transformer-based NMT\nmodels trained on the OPUS parallel corpora.\n4. mBART50 (Tang et al., 2021): Multilingual\nNMT models pretrained on monolingual cor-\npora from 50 languages, and fine-tuned on the\ntranslation task. We report performances of\nboth the English-to-many and many-to-many\nfine-tuned models.\n5. M2M100 (Fan et al., 2021): A massive mul-\ntilingual NMT model that was trained on\n2200 translation directions to support many-\nto-many translation among 100 languages in\ntotal. We compare both the base (418M) and\nthe large (1.2B) versions.\n6. NLLB-200 (NLLB Team et al., 2022): It is\nthe current SOTA in many low-resource pairs,\nscaling to 200 languages. We experiment with\nall its variants, where the largest is a mixture-\nof-experts (MoE) model with 54B parameters.\nWe also benchmark its smaller checkpoints at\n1.3B and 3.3B, as well as distilled versions at\n0.6B and 1.3B.\nWe take the results for mBART50, M2M100,\nand OPUS directly from the DiBiMT leader-\nboard.10 We use Hugging Face11 for accessing and\ninferencing all other models – except for Google\nTranslate and DeepL, which are accessed using\ntheir respective APIs. Despite their presence on\nthe leaderboard, we re-evaluate these systems since\nthey are being constantly updated.\n8https://www.deepl.com/en/translator\n9https://translate.google.com/\n10https://nlp.uniroma1.it/dibimt/public/\nleaderboard\n11https://huggingface.co/\nSystem En-Es En-It\nSimilar contexts dev set 1.81M 1.73M\nFine-tuning corpus 100K 100K\nTable 2: Statistics of data used in our experiments, in\nterms of parallel sentence count.\n4.2 Experimental setup\nDatasets In this study, we use the DiBiMT test\nset for evaluation and measure accuracy across all\nfive translation directions: English to Spanish, Ital-\nian, Chinese, Russian, and German, respectively.\nFor validation, we use the development set from\nFLORES 200 (NLLB Team et al., 2022) in our\nbase setting. To search for similar ambiguous con-\ntexts (Section 3.3), we require a larger develop-\nment set to find relevant examples and also to ac-\ncurately estimate polysemy degree. Hence, we use\nthe Europarl corpus (Koehn, 2005), disambiguated\nwith ESCHER-WSD. We also use the same disam-\nbiguated corpus for fine-tuning, however, we first\nfollow the filtering procedure described in Section\n3.4 to create a small corpus full of ambiguous sen-\ntences. Validation during fine-tuning is done using\n500 randomly sampled sentences from this corpus\nand the rest is used for training. We detail the data\nstatistics used for these experiments in Table 2.\nLLM prompting setup Due to memory con-\nstraints, and to compare all models fairly, we load\nLLMs in 8-bit and use a batch size of 1. For gen-\neration, we set both beam size and temperature to\n1. To prevent repetition in LLM output, we set\nno_repeat_ngram_size to 4. From the LLM’s re-\nsponse, we filter out the sentence before the first\nnewline character as the output translation.\nLoRA fine-tuning We inject LoRA modules into\nall query, key, and value matrices. We set rank to\n8, alpha to 8, and dropout to 0.05. For training, we\nset the effective batch size to 32, the learning rate\nto 3e-4, and the maximum length to 256. The total\ntraining budget is 5 epochs, and we pick the best\nmodel checkpoint based on cross-entropy loss on\nthe validation set. The training data is shuffled after\nevery epoch. Inference is done with a beam size of\n3, and a maximum generation length of 150.\n4.3 LLMs vs NMT systems on DiBiMT\nWe show our results in Table 3. For the subsequent\ndiscussion, we note that LLaMA was not intention-\nally trained on Chinese and is, thus, an ‘unseen’\n487\nSystem # Params Variant En-Es En-It En-Zh En-Ru En-De Average\nCommercial systems\nDeepL Unknown July 2023 63.91 65.47 58.42 67.53 76.64 66.39\nGoogle Translate Unknown July 2023 54.73 53.59 52.09 62.03 67.35 57.96\nOpen-source NMT systems\nOPUS 74M Bilingual En-X models 36.79 29.93 25.94 28.71 27.04 29.68\nmBART50 611M One-to-Many 31.31 26.62 26.63 30.93 26.43 28.38\n611M Many-to-Many 29.98 25.89 28.12 27.54 24.25 27.16\nM2M100 418M Base 22.35 17.27 12.34 17.01 15.62 16.92\n1.2B Large 28.81 23.16 17.30 27.03 22.87 23.83\nNLLB-200\n0.6B Distilled version 40.93 36.38 28.64 47.13 33.41 37.30\n1.3B Distilled version 50.40 53.65 41.15 54.52 52.81 50.51\n1.3B Original checkpoint 48.81 48.43 37.31 54.36 48.93 47.57\n3.3B Original checkpoint 53.23 57.23 39.95 57.44 56.24 52.82\n54B Mixture of Experts 61.33 67.19 48.02 67.88 67.97 62.48\nLLaMA family LLMs\nLLaMA\n7B\n1-shot prompting 53.64 48.84 30.61 † 60.65 57.41 50.23\n3-shot prompting 55.53 50.53 30.52 † 57.31 55.34 49.85\n5-shot prompting 56.33 48.66 27.92 † 56.83 55.26 49.00\n65B\n1-shot prompting 56.57 60.22 44.73 † 65.71 62.05 57.86\n3-shot prompting 59.83 60.18 42.77 † 67.45 63.41 58.73\n5-shot prompting 60.78 63.47 42.49† 66.31 62.98 59.21\nAlpaca 7B 0-shot prompting 49.75 45.24 29.63 † 55.23 51.52 46.27\nBLOOM family LLMs\nBLOOM\n7.1B 1-shot prompting 55.69 28.79 † 51.08 40.00 † 29.67† 41.05\n176B\n1-shot prompting 63.66 42.02 † 60.30 43.22 † 37.04† 49.25\n3-shot prompting 64.52 46.33 † 61.20 44.30 † 36.69† 50.61\n5-shot prompting 65.53 45.99 † 61.73 42.92 † 38.06† 50.85\nBLOOMZ\n7.1B 0-shot prompting 56.89 33.91 † 53.2 33.33 † 21.67† 39.80\n1-shot prompting 60.87 40.68 † 52.37 33.33 † 30.65† 43.58\n176B\n0-shot prompting 62.67 45.78 † 61.87 47.98 † 44.06† 52.47\n1-shot prompting 64.35 49.31† 66.57 51.88† 43.92† 55.21\n3-shot prompting 67.31 45.91† 64.44 53.42† 45.08† 55.23\n5-shot prompting 68.55 49.22† 63.36 52.60† 44.94† 55.73\nTable 3: Accuracies on DiBiMT test for establish NMT systems and LLMs, using naive k-shot prompting. For\nAlpaca, we can only use 0-shot prompting due to its particular prompt template. We highlight the top three scores\nper language in bold, with the best underlined as well, the 2nd best as is, and the 3rd best italicized. We indicate\nscores for unseen languages (ie. not intentionally included in pretraining) with a †.\nlanguage. Similarly, for BLOOM, Chinese and\nSpanish are “seen” and the rest are “unseen”. We\nshare our key observations below:\n1. LLMs usually match or beat massive MT\nmodels on seen languages. Except for the\nvery rich-resourced En-De, where supervised\nMT systems appear to have an edge, LLaMA\n65B mostly matches the SOTA NMT systems\n(namely DeepL and NLLB-200). Further-\nmore, BLOOMZ sets a new SOTA in its seen\nlanguages, Spanish and Chinese, and outper-\nforms DeepL by margins of 7.3% and 12.2%\nrespectively. These improvements against\nsuch strong, supervised massive NMT sys-\ntems are particularly remarkable since our cor-\nresponding setup for inferencing the LLMs\nis quite cheap – as we noted previously, this\nis only naive few-shot prompting of an 8-bit\nquantized model, with a beam size of 1.\n2. LLMs perform relatively worse for unseen\nlanguages, but they can still be much better\nthan some supervised MT models. We note\nthat relative to seen languages, LLaMA under-\nperforms in translation to Chinese. Similarly,\n488\nBLOOM performs worse for its’ unseen lan-\nguages of German, Italian, and Russian. Still,\nLLMs yield reasonable performance here that\nis still much better than some supervised\nNMT systems. For example, BLOOMZ-7B\nachieves 40.68% accuracy in English-Italian,\nwhich is about 35.9% more than OPUS, 52.8%\nmore than mBART50 and 75% more than\nM2M100-1.2B. While NLLB-200 does out-\nperform BLOOMZ-7B, our results just high-\nlight the power of pretraining at scale.\n3. Scale helps improve performance for am-\nbiguity translation . Continuing from the\nlast point, similar to NMT models that im-\nprove with scale (e.g. NLLB-200), we observe\nthat LLMs too perform consistently better at\nambiguous translation on scaling up to their\nlarger variants. This applies to the transla-\ntion of both seen and unseen languages. That\nsaid, the lighter models, such as LLaMA 7B\nor BLOOM 7B, also perform quite well and in\nmany cases, 1-shot prompting of these LLMs\nis almost as good as NLLB translations.\n4. LLM performance does improve on aver-\nage with more demonstrations, but this is\nnot uniform. On average, we observe that\n5-shot prompting works best, followed by 3-\nshot and then 1-shot, though some outliers\nexist for LLaMA 7B. Moreover, when look-\ning at the performance of individual language\npairs, we note that the improvement trend\nis not uniform, and it is possible a 3-shot\ntranslation outperforms a 5-shot one. This\naligns with the finding of Zhang et al. (2023),\nwho reach the same conclusion regarding over-\nall MT quality. Nonetheless, as we show in\nSection 4.4.1, accuracy does significantly im-\nprove when we provide relevant and helpful\nexamples – suggesting quality of demonstra-\ntions matters more than quantity.\n5. General-purpose instruction-tuned LLMs\nconsistently outperform foundation LLMs.\nInterestingly, we observe that 1-shot prompt-\ning of a general-purpose instruction-tuned\nLLM like BLOOMZ often significantly out-\nperforms 5-shot prompting of BLOOM, even\non the very specific task of ambiguity trans-\nlation. In fact, even with 0-shot prompting,\nmodels like Alpaca 7B, BLOOMZ 7B and\nBLOOMZ 176B perform reasonably well,\nmatching some supervised MT systems. We\nobserved that this did not work for foundation\nLLMs like BLOOM 165B and LLaMA 7B,\nand 0-shot prompting of these models yielded\nhallucinations in many cases.\nLastly, we include a qualitative comparison of\nDeepL and BLOOMZ 176B translations for the En-\nZh pair in the Appendix (see Table 8) – where we\nobserve that BLOOMZ generates more contextual\ntranslations, relatively speaking, while its counter-\npart tends to translate literally in many cases.\n4.4 Adapting LLMs for ambiguous MT\nThis section reports experiments with two proposed\nstrategies to enable LLMs to disambiguate bet-\nter and improve performance on the ambiguous\ntranslation task. While both methods are shown\nto significantly improve performance, we include\na discussion of the relative tradeoffs between the\ntechniques in Appendix A.2.\n4.4.1 Improving In-Context Learning by\nleveraging similar ambiguous contexts\nRather than selecting our examples randomly as\nin our naive setting, we employ the data selection\nprocedure described in Section 3.3 to discover other\nexamples that contain the same word sense as the\nmost polysemous sense in the input sentence. We\nreport our scores in Table 4, and our findings below:\n1. Similar contexts yield more improvements\nas the example count increases We observe\nthat for 1-shot prompting, similar contexts per-\nform comparably or slightly better than ran-\ndom examples. However, the gains increase\nsubstantially as we move towards 3-shot and 5-\nshot prompting. We can understand this from\nthe intuition that 1-shot prompting likely just\nguides the LLM towards generating a reason-\nable translation, whereas with more relevant\nexamples, it learns to disambiguate better and\ntranslate in context accordingly.\n2. Larger models observe greater and more\nconsistent gains than smaller LLMs Com-\npared to LLaMA 7B, the other LLMs\n(LLaMA 65B, BLOOM 176B and BLOOMZ\n176B) yield much larger accuracy improve-\nments on a more uniform basis. This is proba-\nbly because scaling up allows LLMs to model\npolysemous words better in their semantic\nspace, facilitating effective in-context learning\nof disambiguation capabilities.\n489\nSystem 1-shot 3-shot 5-shot\nRand. Sim. Rand. Sim. Rand. Sim.\nDeepL —63.91—\nNLLB-200 54B — 61.33—\nLLaMA 7B 53.64 54.01 55.53 52.52 56.33 54.45\nLLaMA 65B 56.57 59.38 59.83 62.44 60.78 63.74\nBLOOM 176B 63.66 62.44 64.52 66.19 65.53 68.22\nBLOOMZ 176B 64.35 69.57 67.31 71.15 68.55 71.33\n(a) English-Spanish\nSystem 1-shot 3-shot 5-shot\nRand. Sim. Rand. Sim. Rand. Sim.\nDeepL —65.47—\nNLLB-200 54B — 67.19—\nLLaMA 7B 48.84 49.47 50.53 53.85 48.66 52.17\nLLaMA 65B 60.22 59.77 60.18 64.94 63.47 65.33\nBLOOM 176B 42.02 43.17 46.33 48.09 45.99 50.00\nBLOOMZ 176B 49.31 49.60 45.91 50.73 49.22 50.53\n(b) English-Italian\nTable 4: 1-shot, 3-shot and 5-shot results for En-Es and En-It prompting with randomised examples (Rand.) versus\nsimilar contexts (Sim.). The best-performing systems from Table 3, i.e. DeepL and NLLB-200 are chosen as\nbaselines. For LLMs, for each setting, the better-performing baseline between Rand. and Sim. is highlighted in\nbold. The overall best score (among all LLMs) is underlined as well, while the best NMT system is also italicized.\n4.4.2 Fine-tuning with ambiguous corpora\nWe fine-tune Alpaca 7B, BLOOM 7B and\nBLOOMZ 7B in En-Es and En-It directions us-\ning the data described in Section 4.2. We show our\nresults when prompting these fine-tuned LLMs in\nTable 5. We make the following observations:\n1. Fine-tuning generally improves perfor-\nmance. We observe that fine-tuned LLMs sig-\nnificantly outperform their non-finetuned ver-\nsions in most cases. The biggest improvement\nis observed for BLOOM 7B in En-It, where\naccuracy increases by as high as 47.73%, in-\ndicating the effectiveness of our method. The\nonly exception to this is when the LLM is al-\nready strong, such as BLOOMZ 7B at En-Es,\nand then the improvements are marginal. But\neven so, strong instruction-tuned LLMs like\nBLOOMZ still gain significantly from fine-\ntuning on the En-It pair – where it was orig-\ninally weaker due to Italian being an unseen\nlanguage during pretraining.\n2. Best Cross Entropy does not necessarily\ntranslate to best disambiguation accuracy.\nLooking at Table 5, we note that the check-\npoints with the best cross-entropy fall short of\nthe topline with the best DiBiMT accuracies,\nsuggesting the former is not an optimal metric\nfor this task. Future work could benefit from\nusing disambiguation-specific metrics for val-\nidation, leveraging other ambiguous test sets\nlike MuCoW (Raganato et al., 2020b).\n3. Fine-tuning for 2-3 epochs is sufficient.\nWe plot the DiBiMT accuracy versus epoch\ncurves in Figure 2 where the performance is\nevaluated after each epoch. We observe that in\nall cases, accuracy peaks between the 1st and\nthe 3rd epoch, after which it mostly plateaus\nor dips slightly - suggesting that one does not\nneed to fine-tune these LLMs for too long.\n4. Fine-tuning improves LLM performance\nuntil about 65K training samples. We now\ntry to answer the Research Question of how\nmany training samples we need for fine-tuning\nthese LLMs, to get optimal performance. We\nplot the Accuracy vs corpus size graph in Fig-\nure 3, where we indicate corpus size by the\nnumber of parallel sentences. We observe that\naccuracy increases non-monotonically with an\nincrease in corpus size, but peaks anywhere\nbetween 36K-63K training samples, which\nseems to depend on the pre-existing capabili-\nties of the LLM. For a raw foundation LLM\nlike BLOOM 7B, relatively more fine-tuning\ndata (54K-63K) appears to be beneficial. Al-\npaca 7B, which has been instruction-tuned on\nan English-only dataset, also seems to benefit\nfrom further fine-tuning—especially for En-\nEs, accuracy peaks after 63K training samples.\nHowever, for a powerful LLM like BLOOMZ\nthat has been instruction-tuned on a large mul-\ntilingual dataset like xP3 (Muennighoff et al.,\n2023), fine-tuning on smaller datasets (at most\n36K sentences, in our case) appears to suffice.\n4.5 Overall MT performance of\ndisambiguation-adapted LLMs\nLastly, for completeness, we evaluate the overall\ntranslation quality of the key LLMs used in this\nwork, since we are interested in noting how well\nthe reported disambiguation accuracies extend to\noverall MT performance. For our test set, we want\nto choose one recently released (ideally within the\n490\nSystem En-Es En-It\nAlpaca 7B BLOOM 7B BLOOMZ 7B Alpaca 7B BLOOM 7B BLOOMZ 7B\nw/o FT 49.75 55.69 60.87 45.24 28.79 40.68\nFT (Best Cross-Entropy Loss) 63.27 57.86 60.39 59.62 37.72 39.73\nFT (Best Attained Acc.) 63.31 59.72 61.56 59.77 42.40 44.73\nTable 5: DiBiMT Accuracies after fine-tuning Alpaca 7B, BLOOM 7B, and BLOOMZ 7B on En-Es and En-It\npairs. The second row indicates checkpoints with the best cross-entropy loss on the validation set, while the last row\nshows the one with the best attained DiBiMT accuracy when evaluating after each epoch, and serves as a “topline”.\n(a) English-Spanish\n (b) English-Italian\nFigure 2: DiBiMT accuracy at the end of every epoch, for the LoRA fine-tuned LLMs\n(a) English-Spanish\n (b) English-Italian\nFigure 3: DiBiMT accuracy vs fine-tuning (FT) corpus size in terms of parallel sentence count. These results are\nobtained from evaluating checkpoints at every 300 steps in the 1st epoch - which roughly corresponds to about 9K\nsentences, since we use a batch size of 32.\nlast year) to minimize the chances of its inclusion\nin the pretraining corpora of LLMs. We, thus, use\nFLORES 200 (NLLB Team et al., 2022) as our\ntest set since it satisfies this criterion and also sup-\nports all our languages of evaluation. We use sp-\nBLEU12 (Goyal et al., 2022), chrF++13 (Popovi´c,\n2017) and COMET22 (Rei et al., 2022) using the\nwmt22-comet-da model as metrics. In this setting,\nwe evaluate Alpaca with 0-shot prompting, while\nLLaMA 7B, LLaMA 65B and BLOOM 176B use\n12nrefs:1|case:mixed|eff:no|tok:flores101|smooth:\nexp|version:2.3.1\n13nrefs:1|case:mixed|eff:yes|nc:6|nw:2|space:\nno|version:2.3.1\nthe 1-shot setup. NLLB-200 is our primary su-\npervised NMT baseline. We also evaluate LoRA\nfine-tuned versions of Alpaca 7B and BLOOM 7B,\nfrom section 4.4.2, on the English-Spanish and\nEnglish-Italian pairs. We exclude BLOOMZ from\nthis evaluation since it is instruction-tuned on FLO-\nRES200. We report our results in Table 6.\nWe observe trends similar to those of our\nDiBiMT experiments. BLOOM 176B performs\nwell in translation of seen languages, performing\ncomparably to NLLB-200 in English-Spanish and\noutperforming it in English-Chinese. This is par-\nticularly the case for COMET22 scores, a metric\nwhich has shown high correlations with human\n491\nSystem En-Es En-It\nspBLEU chrF++ COMET22 spBLEU chrF++ COMET22\nNLLB-200 54B 32.50 53.79 0.86 37.60 57.33 0.89\nAlpaca 7B (0-shot) 23.90 47.30 0.83 23.30 46.40 0.83\nLLaMA 7B (1-shot) 23.20 46.20 0.82 22.10 45.00 0.82\nLLaMA 65B (1-shot) 27.20 49.70 0.83 28.50 50.50 0.85\nBLOOM 7B (1-shot) 24.00 46.30 0.82 10.00 † 33.40† 0.63†\nBLOOM 176B (1-shot) 28.60 51.20 0.85 20.80 † 45.20† 0.81†\nAlpaca 7B (FT, 0-shot) 27.40 50.20 0.85 29.20 51.40 0.87\nBLOOM 7B (FT, 0-shot) 28.70 51.00 0.86 20.90 45.80 0.80\nSystem En-Zh En-Ru En-De\nspBLEU chrF++ COMET22 spBLEU chrF++ COMET22 spBLEU chrF++ COMET22\nNLLB-200 54B 23.10 22.83 0.82 38.00 56.34 0.90 44.80 62.79 0.88\nAlpaca 7B (0-shot) 4.80 † 10.40† 0.62† 21.80 42.60 0.82 27.30 50.30 0.82\nLLaMA 7B (1-shot) 5.60 † 10.80† 0.66† 20.70 41.20 0.79 22.80 45.40 0.78\nLLaMA 65B (1-shot) 13.80 † 17.60† 0.77† 26.70 46.10 0.82 31.80 52.80 0.81\nBLOOM 7B (1-shot) 19.00 19.50 0.83 3.70 † 22.30† 0.46† 8.20† 31.70† 0.51†\nBLOOM 176B (1-shot) 25.10 23.80 0.86 10.30 † 31.80† 0.65† 19.90† 45.40† 0.74†\nTable 6: FLORES 200 results for k-shot prompting of some key LLMs used in this work, compared with the\nNLLB-200 baseline. We also include results for the LoRA fine-tuned models, for the En-Es and En-It pairs. Same\nas the previous notation, we indicate all unseen language results with a †. We observe similar trends in all standard\nMT metrics, as those observed with DiBiMT accuracy.\nspBLEU\nw/ acc.\nChrF++\nw/ acc.\nCOMET22\nw/ acc.\nρ 0.83 0.56 0.76\np-value 0.0001 0.0039 0.0010\nTable 7: Pearson’s correlation ρ (Benesty et al., 2009)\nbetween DiBiMT accuracy and spBLEU, chrF++, and\nCOMET22 respectively, together with p-values.\nevaluation, ranking second in the WMT22 Metrics\nshared task (Freitag et al., 2022). For the other\nlanguages, LLaMA 65B usually performs better\nthan BLOOMZ, but in the 1-shot prompting setup,\nit is unable to beat the NLLB-200 54B MOE. We\nalso notice that the fine-tuned versions of Alpaca\n7B and BLOOM 7B consistently outperform their\nvanilla counterparts – suggesting our techniques\nto improve disambiguation performance also boost\noverall translation quality.\nThus, while we evaluate key LLMs to verify con-\nsistency in trends, we avoid re-running all our base-\nlines on FLORES200. Instead, we try to answer a\nbroader question: how well does DiBiMT disam-\nbiguation accuracy correlate with standard MT met-\nrics? We conduct a Pearson’s correlation test (Ben-\nesty et al., 2009) between the accuracy metric and\nspBLEU, chrF++, and COMET22 respectively. We\nreport our results in Table 7, and find that all MT\nquality metrics correlate positively with accuracy—\nwith p-values of the two-sided alternative hypothe-\nsis being much lesser than 0.05 in all cases. We dis-\ncover that spBLEU and COMET22 exhibit higher\ncorrelations than chrF++. We hypothesize that this\ncould be due to the character-level chrF++ being\nless sensitive to word-level senses. Overall, the\nresults of Tables 6 and 7 suggest that the significant\naccuracy improvements noted earlier are not at the\ncost of translation quality, and in turn, could yield\nimprovements in overall MT scores too.\n5 Conclusion\nIn this work, we studied the capabilities of LLMs to\nhandle ambiguity during machine translation. We\nchoose seven of the most widely used foundation\nand instruction-tuned LLMs and compare accuracy\nwith SOTA commercial and open-source NMT sys-\ntems on the DiBiMT translation benchmark. Out\nof 5 language directions, we report scores compa-\nrable to the SOTA on two (En-Ru, En-It) and set a\nnew SOTA on two others (En-Zh, En-Es). We then\npresent two techniques that significantly improve\ndisambiguation accuracy: in-context learning with\nsimilar contexts, and fine-tuning on an ambigu-\nous corpus. We end the paper with an evaluation\nof overall MT quality. We hope the methods and\nfindings shared in this work could guide future re-\nsearchers studying ambiguity in translation.\n492\nLimitations\nIn this work, we attempt to note overall trends in\nLLM performance as compared to conventional\nNMT systems and, based on our results, suggest\nmethods that generally improve performance. That\nsaid, there are exceptions to these trends - prompt-\ning with similar contexts can, at times, degrade\nperformance and so can increasing the number of\ndemonstrations (see Table 4). But there is some\nconsistency here too that these observations mostly\napply to smaller LLMs (such as LLaMA 7B) while\nthe larger LLMs benefit more significantly. Also,\nas noted in Section 4.4.1, in a small percentage of\ncases (7.5%), we are unable to find 5 matches when\nattempting 5-shot prompting with similar contexts.\nIn such cases, it might be worthwhile, from a perfor-\nmance perspective, to use random demonstrations;\nnonetheless, since we are interested in verifying\nthe utility of similar contexts and also since there\nare only a few cases where it might be pertinent,\nwe do not explore this.\nAcknowledgements\nThis work has received funding from UK Re-\nsearch and Innovation under the UK government’s\nHorizon Europe funding guarantee [grant numbers\n10039436 and 10052546].\nThe computations described in this research\nwere performed using the Baskerville Tier\n2 HPC service (https://www.baskerville.ac.uk/).\nBaskerville was funded by the EPSRC and\nUKRI through the World Class Labs scheme\n(EP/T022221/1) and the Digital Research Infras-\ntructure programme (EP/W032244/1) and is op-\nerated by Advanced Research Computing at the\nUniversity of Birmingham.\nReferences\nEdoardo Barba, Tommaso Pasini, and Roberto Nav-\nigli. 2021. ESC: Redesigning WSD with extractive\nsense comprehension. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 4661–4672.\nJacob Benesty, Jingdong Chen, Yiteng Huang, and Israel\nCohen. 2009. Pearson correlation coefficient. In\nNoise reduction in speech processing, pages 37–40.\nSpringer.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nNiccolò Campolungo, Federico Martelli, Francesco\nSaina, and Roberto Navigli. 2022. DiBiMT: A novel\nbenchmark for measuring Word Sense Disambigua-\ntion biases in Machine Translation. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 4331–4352, Dublin, Ireland. Association for\nComputational Linguistics.\nMarine Carpuat and Dekai Wu. 2007. Improving statisti-\ncal machine translation using word sense disambigua-\ntion. In Proceedings of the 2007 Joint Conference\non Empirical Methods in Natural Language Process-\ning and Computational Natural Language Learning\n(EMNLP-CoNLL), pages 61–72, Prague, Czech Re-\npublic. Association for Computational Linguistics.\nYee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.\nWord sense disambiguation improves statistical ma-\nchine translation. In Proceedings of the 45th annual\nmeeting of the association of computational linguis-\ntics, pages 33–40.\nPinzhen Chen, Zhicheng Guo, Barry Haddow, and Ken-\nneth Heafield. 2023. Iterative translation refinement\nwith large language models. arXiv preprint.\nHeeyoul Choi, Kyunghyun Cho, and Yoshua Bengio.\n2017. Context-dependent word representation for\nneural machine translation. Computer Speech & Lan-\nguage, 45:149–160.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2022. PaLM: Scaling language\nmodeling with pathways. arXiv preprint.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, et al. 2021. Beyond english-centric multi-\nlingual machine translation. The Journal of Machine\nLearning Research, 22(1):4839–4886.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,\nCraig Stewart, Eleftherios Avramidis, Tom Kocmi,\nGeorge Foster, Alon Lavie, and André F. T. Martins.\n2022. Results of WMT22 metrics shared task: Stop\nusing BLEU – neural metrics are better and more\nrobust. In Proceedings of the Seventh Conference\non Machine Translation (WMT), pages 46–68, Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022. The flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522–538.\n493\nZhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng\nZhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shum-\ning Shi, and Xing Wang. 2023. Exploring human-\nlike translation strategy with large language models.\narXiv preprint.\nAmr Hendy, Mohamed Gomaa Abdelrehim, Amr\nSharaf, Vikas Raunak, Mohamed Gabr, Hitokazu\nMatsushita, Young Jin Kim, Mohamed Afify, and\nHany Hassan Awadalla. 2023. How good are GPT\nmodels at machine translation? a comprehensive\nevaluation. arXiv preprint.\nJ. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. LoRA: Low-rank adaptation of large\nlanguage models. arXiv preprint.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of\nmachine translation summit x: papers, pages 79–86.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems , 35:22199–\n22213.\nFrederick Liu, Han Lu, and Graham Neubig. 2018. Han-\ndling homographs in neural machine translation. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1336–1345, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nYasmin Moslem, Rejwanul Haque, and Andy Way. 2023.\nAdaptive machine translation with large language\nmodels. EAMT.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023. Crosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15991–16111, Toronto, Canada. Association\nfor Computational Linguistics.\nRoberto Navigli, Michele Bevilacqua, Simone Conia,\nDario Montagnini, and Francesco Cecconi. 2021.\nTen years of BabelNet: A survey. In IJCAI, pages\n4559–4567.\nNLLB Team, Marta Ruiz Costa-jussà, James Cross,\nOnur cCelebi, Maha Elbayad, Kenneth Heafield,\nKevin Heffernan, Elahe Kalbassi, Janice Lam,\nDaniel Licht, Jean Maillard, Anna Sun, Skyler\nWang, Guillaume Wenzek, Alison Youngblood,\nBapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez,\nPrangthip Hansanti, John Hoffman, Semarley Jarrett,\nKaushik Ram Sadagopan, Dirk Rowe, Shannon L.\nSpruit, C. Tran, Pierre Yves Andrews, Necip Fazil\nAyan, Shruti Bhosale, Sergey Edunov, Angela Fan,\nCynthia Gao, Vedanuj Goswami, Francisco Guzm’an,\nPhilipp Koehn, Alexandre Mourachko, Christophe\nRopers, Safiyyah Saleem, Holger Schwenk, and\nJeff Wang. 2022. No language left behind: Scal-\ning human-centered machine translation. ArXiv,\nabs/2207.04672.\nJonathan Pilault, Xavier Garcia, Arthur Bražinskas, and\nOrhan Firat. 2023. Interactive-chain-prompting: Am-\nbiguity resolution for crosslingual conditional gener-\nation with interaction. arXiv preprint.\nMaja Popovi´c. 2017. chrf++: words helping character\nn-grams. In Proceedings of the second conference on\nmachine translation, pages 612–618.\nXiao Pu, Nikolaos Pappas, James Henderson, and An-\ndrei Popescu-Belis. 2018. Integrating weakly super-\nvised word sense disambiguation into neural machine\ntranslation. Transactions of the Association for Com-\nputational Linguistics, 6:635–649.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAlessandro Raganato, Yves Scherrer, and Jörg Tiede-\nmann. 2019. The mucow test suite at wmt 2019: Au-\ntomatically harvested multilingual contrastive word\nsense disambiguation test sets for machine transla-\ntion. In Proceedings of the Fourth Conference on\nMachine Translation (Volume 2: Shared Task Papers,\nDay 1), pages 470–480.\nAlessandro Raganato, Yves Scherrer, and Jörg Tiede-\nmann. 2020a. An evaluation benchmark for testing\nthe word sense disambiguation capabilities of ma-\nchine translation systems. In Proceedings of The 12th\nLanguage Resources and Evaluation Conference. Eu-\nropean Language Resources Association (ELRA).\nAlessandro Raganato, Yves Scherrer, and Jörg Tiede-\nmann. 2020b. An evaluation benchmark for test-\ning the word sense disambiguation capabilities of\nmachine translation systems. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 3668–3675, Marseille, France. European\nLanguage Resources Association.\nVikas Raunak, Arul Menezes, Matt Post, and Hany Has-\nsan. 2023. Do GPTs produce less literal translations?\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n2: Short Papers), pages 1041–1050, Toronto, Canada.\nAssociation for Computational Linguistics.\n494\nRicardo Rei, José G. C. de Souza, Duarte Alves,\nChrysoula Zerva, Ana C Farinha, Taisiya Glushkova,\nAlon Lavie, Luisa Coheur, and André F. T. Martins.\n2022. COMET-22: Unbabel-IST 2022 submission\nfor the metrics shared task. In Proceedings of the\nSeventh Conference on Machine Translation (WMT),\npages 578–585, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint.\nYves Scherrer, Alessandro Raganato, and Jörg Tiede-\nmann. 2020. The MUCOW word sense disambigua-\ntion test suite at WMT 2020. In Proceedings of the\nFifth Conference on Machine Translation, pages 365–\n370, Online. Association for Computational Linguis-\ntics.\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2019.\nEncoders help you disambiguate word senses in\nneural machine translation. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1429–1435, Hong Kong,\nChina. Association for Computational Linguistics.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2021. Multilingual translation from de-\nnoising pre-training. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 3450–3466, Online. Association for Computa-\ntional Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford Alpaca:\nAn instruction-following LLaMA model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nJörg Tiedemann and Santhosh Thottingal. 2020. Opus-\nmt–building open translation services for the world.\nIn Proceedings of the 22nd Annual Conference of\nthe European Association for Machine Translation.\nEuropean Association for Machine Translation.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. LLaMA: Open and efficient foun-\ndation language models. arXiv preprint.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,\nViresh Ratnakar, and George Foster. 2023. Prompt-\ning PaLM for translation: Assessing strategies and\nperformance. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 15406–\n15427, Toronto, Canada. Association for Computa-\ntional Linguistics.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13484–13508, Toronto, Canada. Association\nfor Computational Linguistics.\nWarren Weaver. 1952. Translation. In Proceedings of\nthe Conference on Mechanical Translation.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research . Survey Certifica-\ntion.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nBiao Zhang, Barry Haddow, and Alexandra Birch. 2023.\nPrompting large language model for machine trans-\nlation: A case study. In International Conference\non Machine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA, volume 202 of Proceedings\nof Machine Learning Research, pages 41092–41110.\nPMLR.\n495\nSource Tap the head of the drum for this roll.\nDeepL 敲击鼓的头部进行滚奏。 head, literal\nBLOOMZ 敲击鼓面，发出这个鼓点。 striking surface of a drum\nSource they tracked him back toward the head of the stream.\nDeepL 他们跟踪他回到溪头. head (literal and unnatural)\nBLOOMZ 他们跟着他回到了河的上游。 upstream\nSource The expedition followed the river all the way to the head.\nDeepL 探险队沿着河流一直走到河头。 head (literal and unnatural)\nBLOOMZ 探险队顺着河一直走到源头。 source\nSource How much head do you have at the Glens Falls feeder dam?\nDeepL 格伦瀑布支坝的水头有多大？ hydraulic head\nBLOOMZ 你有多少头牛在格伦瀑布的蓄水池里？ (a classifier word to express quantities (of cows))\nSource the office was full of secret heads.\nDeepL 办公室里到处都是秘密头目 . leader, ringleader\nBLOOMZ 办公室挤满了神秘的首脑。 leader, head of state\nTable 8: Manual inspection on English-to-Chinese translation focused on the disambiguation of “head”, correspond-\ning to the first five test instances in DiBiMT. The baselines are DeepL and BLOOMZ 176B, the highest performing\nNMT system and LLM for this pair (from Table 3). The reported annotations are obtained from a native Chinese\nspeaker who was invited to label the sense of the translated ambiguous word.\nA Appendix\nA.1 Qualitative comparison: BLOOMZ vs\nDeepL\nWe choose the best-performing LLM and the SOTA\nMT system from Table 3 – focusing on the En-Zh\npair since LLMs seem to yield the highest gains\nthere. With the help of a native Chinese speaker,\nwe got hypotheses from these two systems anno-\ntated, for the first 5 sentences of the DiBiMT test\nset. We observe that although there are cases where\nDeepL gets it right over BLOOMZ (example 4) or\nwhere both are correct (Example 5), in many in-\nstances BLOOMZ appears to generate more contex-\ntual (and less literal) translations. We hypothesize\nthat this could potentially be due to the former’s\npowerful language modelling abilities\nA.2 Trade-off between prompting and\nfine-tuning\nWe show in Section 4.4 that both prompting with\nsimilar contexts through In-Context Learning (ICL)\nand LoRA fine-tuning can significantly improve\nperformance. However, depending on the use case,\nit might be better to favour one over the other. For\ninstance, in production environments, LLMs that\nare LoRA fine-tuned on ambiguous text can pro-\nvide powerful disambiguation performance, while\nalso being more feasible to deploy and run at scale.\nIn contrast, ICL with k-shot prompting, especially\nfor higher values of k, can significantly increase\nquery size and memory consumption, necessitating\nreduced batch size and thus, throughput.\nHowever, conducting ICL with similar ambigu-\nous contexts can be used to query LLMs as large\nas LLaMA 65B and BLOOMZ 176B and yield per-\nformance comparable to SOTA MT systems (see\nTable 4). The preprocessing cost overhead of such a\nmethod, namely disambiguating the test set, is also\nlow - it took us about 13 seconds to disambiguate a\ntest set of about 500 sentences on 1 Nvidia GeForce\nRTX 3090. In contrast, the one-time cost of fine-\ntuning can be quite expensive—for instance, it took\nus 44 hours to fine-tune an Alpaca 7B with LoRA\non a single Nvidia Tesla A100 40G. Thus, in GPU-\nscarce settings where the costs of LoRA fine-tuning\nare prohibitive, it might be favourable to use ICL\nto query massive LLMs and obtain SOTA perfor-\nmances. In contrast, production environments are\nlikely to prefer the fine-tuned LLMs, since the one-\noff fine-tuning costs can be amortized.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8355987071990967
    },
    {
      "name": "Machine translation",
      "score": 0.8350503444671631
    },
    {
      "name": "Ambiguity",
      "score": 0.7498258948326111
    },
    {
      "name": "Natural language processing",
      "score": 0.6825166940689087
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6756166219711304
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5698512196540833
    },
    {
      "name": "Benchmarking",
      "score": 0.5346810817718506
    },
    {
      "name": "Translation (biology)",
      "score": 0.48432910442352295
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    }
  ],
  "cited_by": 7
}