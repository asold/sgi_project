{
  "title": "Large Language Model in Medical Information Extraction from Titles and Abstracts with Prompt Engineering Strategies: A Comparative Study of GPT-3.5 and GPT-4",
  "url": "https://openalex.org/W4393032472",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2113505809",
      "name": "Yiyi Tang",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2729980856",
      "name": "Ziyan Xiao",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2099141215",
      "name": "Xue Li",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2517167524",
      "name": "Qiwen Fang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099333426",
      "name": "Qingpeng Zhang",
      "affiliations": [
        "Chinese University of Hong Kong",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A3163241863",
      "name": "Daniel Yee Tak Fong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4211621406",
      "name": "Francisco Tsz Tsun Lai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3193373618",
      "name": "Celine Sze Ling CHUI",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3151524065",
      "name": "Esther Wai Yin Chan",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A4200959792",
      "name": "Ian Chi Kei Wong",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Research Data Collaboration Task Force",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113505809",
      "name": "Yiyi Tang",
      "affiliations": [
        "University of Hong Kong",
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2729980856",
      "name": "Ziyan Xiao",
      "affiliations": [
        "Chinese University of Hong Kong",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2099141215",
      "name": "Xue Li",
      "affiliations": [
        "Hong Kong Science and Technology Parks Corporation",
        "Department of Health",
        "University of Hong Kong",
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2099333426",
      "name": "Qingpeng Zhang",
      "affiliations": [
        "University of Hong Kong",
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A3151524065",
      "name": "Esther Wai Yin Chan",
      "affiliations": [
        "Department of Health",
        "Chinese University of Hong Kong",
        "University of Hong Kong",
        "Hong Kong Science and Technology Parks Corporation"
      ]
    },
    {
      "id": "https://openalex.org/A4200959792",
      "name": "Ian Chi Kei Wong",
      "affiliations": [
        "Department of Health",
        "Hong Kong Science and Technology Parks Corporation",
        "University of Hong Kong",
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4391069573",
    "https://openalex.org/W4386110374",
    "https://openalex.org/W4385620388",
    "https://openalex.org/W4386120650",
    "https://openalex.org/W4385571667",
    "https://openalex.org/W4381587418",
    "https://openalex.org/W4313371821",
    "https://openalex.org/W2512040454",
    "https://openalex.org/W4385356790",
    "https://openalex.org/W4385297391",
    "https://openalex.org/W4389508866",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W4400036248",
    "https://openalex.org/W4398765017",
    "https://openalex.org/W4394579747",
    "https://openalex.org/W4382678522",
    "https://openalex.org/W4386867830",
    "https://openalex.org/W4402407635",
    "https://openalex.org/W4392602768",
    "https://openalex.org/W4388157511",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2776936737",
    "https://openalex.org/W2181523240",
    "https://openalex.org/W3123893780",
    "https://openalex.org/W2184378182",
    "https://openalex.org/W2799628962"
  ],
  "abstract": "Abstract Background While it is believed that large language models (LLMs) have the potential to facilitate the review of medical literature, their accuracy, stability and prompt strategies in complex settings have not been adequately investigated. Our study assessed the capabilities of GPT-3.5 and GPT-4.0 in extracting information from publication abstracts. We also validated the impact of prompt engineering strategies and the effectiveness of evaluating metrics. Methodology We adopted a stratified sampling method to select 100 publications from nineteen departments in the LKS Faculty of Medicine, The University of Hong Kong, published between 2015 and 2023. GPT-3.5 and GPT-4.0 were instructed to extract seven pieces of information – study design, sample size, data source, patient, intervention, comparison, and outcomes – from titles and abstracts. The experiment incorporated three prompt engineering strategies: persona, chain-of-thought and few-shot prompting. Three metrics were employed to assess the alignment between the GPT output and the ground truth: ROUGE-1, BERTScore and a self-developed LLM Evaluator with improved capability of semantic understanding. Finally, we evaluated the proportion of appropriate answers among different GPT versions and prompt engineering strategies. Results The average accuracy of GPT-4.0, when paired with the optimal prompt engineering strategy, ranged from 0.736 to 0.978 among the seven items measured by the LLM evaluator. Sensitivity of GPT is higher than the specificity, with an average sensiti ity score of 0.8550 while scoring only 0.7353 in specificity. The GPT version was shown to be a statistically significant factor impacting accuracy, while prompt engineering strategies did not exhibit cumulative effects. Additionally, the LLM evaluator outperformed the ROUGE-1 and BERTScore in assessing the alignment of information. Conclusion Our result confirms the effectiveness and stability of LLMs in extracting medical information, suggesting their potential as efficient tools for literature review. We recommend utilizing an advanced version of LLMs and the prompt should be tailored to specific tasks. Additionally, LLMs show promise as an evaluation tool related for complex information.",
  "full_text": "Title \nLarge Language Model in Medical Information Extraction from Titles and Abstracts with \nPrompt Engineering Strategies: A Comparative Study of GPT-3.5 and GPT-4  \n \nAuthors:  \nYiyi Tang a,b,#, Ziyan Xiao b,c,#, Xue Li a,c,d,#, Qingpeng Zhang c,e, Esther W Chan c,d, Ian CK \nWong c,d, Research Data Collaboration Task Force h \n \na Department of Medicine, School of Clinical Medicine, Li Ka Shing Faculty of Medicine, \nThe University of Hong Kong, Hong Kong SAR, China \nb Department of Statistics and Actuarial Science, Faculty of Science, The University of Hong \nKong, Hong Kong SAR, China  \nc  Department of Pharmacology and Pharmacy, Li Ka Shing Faculty of Medicine, The \nUniversity of Hong Kong, Hong Kong SAR, China  \nd Laboratory of Data Discovery for Health (D 24H), Hong Kong Science Park, Hong Kong \nSAR, China \ne Musketeers Foundation Institute of Data Science, The  University of Hong Kong, Hong \nKong SAR, China \nh Li Ka Shing Faculty of Medicine, The University of Hong Kong, Hong Kong SAR, China \n \n# Co-first author with equal contribution \n* Corresponding author  \n \nXue Li \nAssistant Professor \nDepartment of Medicine and Department of Pharmacology & Pharmacy \nLKS Faculty of Medicine, The University of Hong Kong \nPB306, 3/F, Professional Block, Queen Mary Hospital \n102 Pok Fu Lam Road, Hong Kong \nTel: +852 2255 3319 \nEmail: sxueli@hku.hk\n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nCo-author: \nYiyi Tang: yiyitang@connect.hku.hk \nZiyan Xiao: xiaozy@connect.hku.hk \nQingpeng Zhang: qpzhang@hku.hk \nEsther W Chan: ewchan@hku.hk \nIan CK Wong: wongick@hku.hk \n \nAuthor contributions: \nStudy concept and design: YT, ZX, XL, EWC, QZ, ICKW.  \nInformation collecting and screening: YT, ZX.  \nData extraction, analysis, and cross-checking: YT, ZX.  \nDrafting of the manuscript: XL, YT, ZX.  \nData interpretation: all authors.  \nCritical revision of the manuscript of significant intellectual contribution: all authors. \nFunding acquisition: XL \nStudy supervision: XL, QZ. \n \n \nAbstract \nBackground: Large language models (LLMs) have significantly enhanced the Natural \nLanguage Processing (NLP), offering significant potential in facilitating medical literature \nreview. However, the accuracy, stability and prompt strategies associated with LLMs in \nextracting complex medical information have not been adequately investigated. Our study \nassessed the capabilities of GPT-3.5 and GPT-4.0 in extracting or summarizing seven crucial \nmedical information items from the title and abstract of research papers. We also validated \nthe impact of prompt engineering strategies and the effectiveness of evaluating metrics. \n \nMethodology: We adopted a stratified sampling method to select 100 papers from the \nteaching schools and departments in the LKS Faculty of Medicine, University of Hong Kong, \npublished between 2015 and 2023. GPT-3.5 and GPT-4.0 were instructed to extract seven \npieces of information, including study design, sample size, data source, patient, intervention, \ncomparison, and outcomes. The experiment incorporated three prompt engineering strategies: \npersona, chain-of-thought and few-shot prompting. We employed three metrics to assess the \nalignment between the GPT output and the ground truth: BERTScore, ROUGE-1 and a \nself-developed GPT-4.0 evaluator. Finally, we evaluated and compared the proportion of \ncorrect answers among different GPT versions and prompt engineering strategies. \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nResults: GPT demonstrated robust capabilities in accurately extracting medical information \nfrom titles and abstracts. The average accuracy of GPT-4.0, when paired with the optimal \nprompt engineering strategy, ranged from 0.688 to 0.964 among the seven items, with sample \nsize achieving the highest score and intervention yielding the lowest. GPT version was shown \nto be a statistically significant factor in model performance, but prompt engineering strategies \ndid not exhibit cumulative effects on model performance. Additionally, our results showed \nthat the GPT-4.0 evaluator outperformed the ROUGE-1 and BERTScore in assessing the \nalignment of information (Accuracy: GPT-4.0 Evaluator: 0.9714, ROUGE-1: 0.9429, \nBERTScore: 0.8714). \n \nConclusion: Our result confirms the effectiveness of LLMs in extracting medical \ninformation, suggesting their potential as efficient tools for literature review. We recommend \nutilizing an advanced version of LLMs to enhance the model performance, while prompt \nengineering strategies should be tailored to the specific tasks. Additionally, LLMs show \npromise as an evaluation tool to assess the model performance related to complex \ninformation processing.   \nIntroduction  \nLarge language models (LLM), including the GPT series, have emerged as a promising tool to \nrevolutionize many domains in medicine [1-2]. LLMs distinguished themselves from \ntraditional natural language processing models by their ability to generate responses that align \nwith users’ expectations [3], without requiring dedicated fine-tuning on specialized tasks [4]. \nMedical evidence summarization is one of these areas where GPT is promising to improve the \ntraditional process of extracting information from the vast amount of medical research papers \n[5-7]. \n  \nMedical literature screening is one of the major application domains of automatic medical \ninformation summarization and extraction. Before the advent of ChatGPT, one prominent \napproach to streamlining the screening process involved recommending articles based on \nrelevance, thereby facilitating the prioritization of manual screening or providing suggestions \nfor inclusion and exclusion [8]. Numerous software tools have been developed to realize these \nfunctions, such as Rayyan [9], RobotReviewer [10], and SWIFT-Review [11]. These tools \nutilized machine learning techniques related to Natural Language Processing (NLP) [12], and \nthe Supportive Vector Machine (SVM) was the prevailing algorithm [13]. Research has also \ndemonstrated the cost efficiency of employing these automated tools with text-mining-based \nsingle screening, reducing workload by over 60% compared to alternative methods [14]. \nHowever, the reliability and transparency of applying artificial intelligence to literature review \nhave long been a concern. At the current stage, multiple studies have highlighted the necessity \nof incorporating manual screening while leveraging the existing tools [12-13]. The updated \nPRISMA guidance for reporting systematic reviews also expressed concerns about the \nerroneous exclusion of relevant studies using machine learning tools while recognizing its role \nin priority screening [15]. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \n  \nLLMs exhibit the potential to expand functionalities and enhance accuracy beyond existing \ntools. A recent study conducted by Matsui et al. (2023) has demonstrated that, with an \nappropriate prompt setting, GPT-3.5 can achieve exceptional sensitivity in literature screening, \nclose to human evaluators, and even surpass them when confronted with a massive volume of \narticles.[16] LLMs can also be applied to less-explored steps of medical literature review, such \nas the risk of bias assessment and data extraction.[17] In another experiment, Hill et al. (2023) \nshowcased the accurate performance of Microsoft Bing AI in retrieving information and \nconstructing study characteristics tables from full-text PDF files.[18] Additionally, Shaib et al. \n(2023) attempted to synthesise medical evidence from multiple documents, although the \ncurrent results are less optimistic than summarisation from single documents.[5] \n \nDespite the promising potential of LLMs in the literature review, there remain a need for \ncomprehensive empirical research addressing common concerns surrounding LLMs, including \ntheir evidence level and consistency [19]. The ability to extract complex information from \nresearch papers is essential in validating the reliability of LLMs for assisting medical \nliterature review. Furthermore, while prompt engineering was recommended as a useful \nstrategy to increase the performance of ChatGPT [20], its role in the context of medical \nliterature review has yet to be thoroughly investigated. \n  \nIn this study, we aim to design an experiment to answer the above questions. The research \nobjectives of this study can be summarised as follows. \n1. To implement and assess the capability of large language models in \nextracting critical information from titles and abstracts of medical research papers. \n2. To compare GPT-3.5 and GPT-4 performance in the aforementioned task. \n3. To validate the effects of prompt engineering strategies on the performance \nimprovement of LLMs in the aforementioned tasks. \n \nMethodology \nStudy design \nThe scope of this study encompassed 100 research papers randomly selected from the \npublication pool of the Li Ka Shing Faculty of Medicine, University of Hong Kong.  The \nchosen papers consisted solely of original research articles published between January 1, 2015, \nand December 31, 2023, with their titles and abstracts fully available on Scopus. To ensure \nrepresentativeness, we adopted a stratified sampling method to randomly select papers from \neach department in proportion to the total number of publication records affiliated with that \ndepartment. The paper’s affiliation is the institution affiliated with the corresponding author. \nThe departments and their related domains of the paper selected are presented in \nSupplementary Material 1. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \n \nFigure 1 presents the overall study design. All titles and abstracts were obtained from the \nScopus online dataset and pre-processed to remove unreadable characters. Two researchers \nmanually labeled the information to be extracted according to pre-defined criteria, to obtain the \nground truth. To ensure accuracy, we employed cross-checking of their results to establish the \nground truth. \n \nSubsequently, the titles and abstracts were proceeded to GPTs to extract information. We \nimplemented several prompt sets to compare the effectiveness of prompt engineering. The \nassessment of the information extraction performance was based on semantic similarity \nbetween GPT’s output and ground truth, measured by several NLP metrics and a \nself-developed independent GPT evaluator. Finally, we perform a statistical analysis on the \nresults. \n \nFigure 1. Flowchart of overall study design; \n  \n \n \n \nTo compare the performance of GPT-3.5 and GPT-4.0, our study conducts independent \nevaluations using the latest model versions at the time of study: gpt-3.5-turbo-0125 and \ngpt-4-0125-preview, referred to as GPT-3.5 and GPT-4.0 in later script. These models \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nrepresent the most advanced version of their respective series and are provided by OpenAI \nthrough the API platform. Experiments will be executed using Python scripts to interact with \nthe OpenAI API. Each model will receive prompts via individual API requests without the \nconversation history, maintaining the independence of each interaction and preventing prior \ncontext from influencing the model's performance. All experiments would be repeated five \ntimes to evaluate the performance stability. In total, there are 8,000 experiments. \n \nThis design aims to yield a fair and thorough comparison of the two models, highlighting their \nrespective strengths and limitations in processing and analyzing medical research literature. \n \nInformation Extraction \nInformation extraction is a pivotal stage in a literature review. It not only facilitates the \nidentification of related papers, but also has the potential to enhance the transparency of LLM’s \ndecision as an intermediate step in automatic literature screening. In this study, we identified \nseven important items in literature screening as representative samples, including study design, \nsample size, data source, and PICOs (Patient, Intervention, Comparison Outcomes). Their \nrespective definitions are provided in Table 1.  \n \nTable 1. Definition of Information to extract (Duke University, 2019) [21] \nItem Definition \nStudy design Type of study, such as randomized controlled trail, cohort study, \ncase-control study and systematic review \nSample size The number of participants involved in the study, and the basic \ncharacteristics of the participants. \nData source Source of the experimental data, such as databases, previous \nstudies or surveys \nPatient The patient involved in the experiment with some most important \ncharacteristics of patients \nIntervention Main intervention, exposure, or prognostic factor in the \nexperiment \nComparison Main alternative group being considered. \nOutcomes The outcome that the experiment trying to accomplish, measure, \nimprove or affect. \n \nWe believe these elements are the basis of efficient and precise literature screening, providing \nresearchers with a clear and standardized framework for evaluation. Particularly, the PICOs, as \nthe gold standard for clinical study assessment, offer a systematic approach to identify relevant \nresearch questions and assess the quality of studies. \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nValidation on the effects of Prompt Engineering   \nPrompt engineering is an essential mechanism for optimizing the interaction with LLMs, \nserving to refine and enhance users’ query in order to improve the performance on tasks. In this \nsection, we will examine and identify the effect of several prompt engineering strategies \ndiscussed in current directions of research, including Adopting a Persona, Chain of Thought \nand Few-shot Learning. \n \n \nAdopting a Persona [22-23] is often achieved by instructing the LLMs to adopt the role of an \nexpert in the related field of research. Chain of Thought [24-25] asks the model to explain the \nreasoning or the rationale behind each step in its problem-solving process. Though our task \nmay not involve complicated logical reasoning, we are interested in investigating whether \nincorporating requests for justification could lead to improved performance and greater \ntransparency. Few-shot Learning [26-27] refers to the process in which we provide LLMs with \nexpert output examples for similar tasks, which can serve as a guide for the model's responses. \n \nWe adopted the following approach for our study. We first established a standard prompt \nwithout any specialized engineering strategy to serve as a control. This prompt simply asked \nthe LLM to perform the task without additional instruction or context. We then selected three \nprompt engineering strategies as mentioned previously. For each strategy, we crafted a series of \nprompts that incorporated the specific tactic. After that, we systematically removed one \nstrategy at a time from the prompts, creating various ablated conditions for comparison against \nthe baseline prompt and each other. For each prompt condition, we then evaluate the LLM’s \nperformance using several metrics. \n \nThe table below outlines the specific prompts that have been designed for each of these prompt \nengineering strategies. \n \nTable 2 Prompt Setting for information extraction  \nGroup Prompt \nControl # Context \n[a] You will be provided with titles and abstracts of medical papers, and \nyour task is to parse it into structured data, including Study Design, Data \nSource, Sample Size, Patient, Intervention, Comparison and Outcomes, and \nseparate them by semicolon. \n \n# Input \n[insert paper title and abstract] \n \n# Instruction \nPlease read, extract and concisely report the following key details from \nthe abstract: \nStudy Design: What type of methodology was employed in the study? \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nSample Size: How many participants were included in the study? \nData Source: Where was the data for this study sourced from? \nBased on the Study Design, if the paper is a review paper OR a laboratory \nstudy, please marks Patient, Intervention, Comparison and Outcomes all as \nNA. Else, answer the following PICO question: \n- Patients: Who is the study's targeted patient or population group? \n- Intervention: What is the key intervention that the study assesses? \n- Comparison: Is there a comparison group or control used, and what does \nit consist of? \n- Outcomes: What outcomes are being measured to determine the \nintervention's success? \nAnswer “NA” if any of the item is not mentioned in the abstract. \n \n# Output \nPlease [b1] output the structured data separated by semicolon, such as: \n[b2] \nStudy Design: [output]; \nData Source: [output]; \nSample Size: [output]; \nPatient: [output]; \nIntervention: [output]; \nComparison: [output]; \nOutcomes: [output]; \n[c] \n \nStrategy 1: \nPersona \n# Inserted at [a] \nImagine you are an expert in research methodology. Your role is essential \nin supporting a team of researchers by meticulously extracting critical \ninformation from medical paper abstracts. You have been trained to identify \nand collate specific elements that are crucial for the team's meta-analysis \nand database entry tasks. \nStrategy 2: \nChain of \nthought \n# Inserted at [b1]  \npresent a concise reasoning for each step you take, and how you arrive at \nthe final structured data. Also, please \n \n# Inserted at [b2] \nReasoning: [output]; \n \n# Inserted at [b3] \nReasoning: The abstract explicitly indicates that the study is a \nretrospective cohort study. The sample size is explicitly mentioned, \nconsisting of three distinct groups with their respective counts. The data \nsource is not explicitly named, so we mark it with NA. Since this is a cohort \nstudy (an epidemiological study) instead of a review paper or a laboratory \nstudy, we proceed with identifying the PICO elements. The patient \npopulation is women with PCOS, PCO, and age-matched controls undergoing \nIVF. The intervention is the IVF treatment itself. The comparison is made \nbetween the women with PCOS, those with PCO, and the age-matched controls. \nThe outcomes being measured include various obstetric complications and \noutcomes such as GDM, GHT, PET, IUGR, gestation at delivery, baby's Apgar \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nscores, and NICU admissions; \nStrategy 3: \nFew-shot \nPrompting \n# Inserted at [c] \nHere is an example for your reference: \n# Input \nTitle: Obstetric outcomes in women with polycystic ovary syndrome and \nisolated polycystic ovaries undergoing in vitro fertilization: a \nretrospective cohort analysis \nAbstract: Objective: This retrospective cohort study evaluated the \nobstetric outcomes in women with polycystic ovary syndrome (PCOS) and \nisolated polycystic ovaries (PCO) undergoing in vitro fertilization (IVF) \ntreatment. Methods: We studied 104 women with PCOS, 184 with PCO and 576 \nage-matched controls undergoing the first IVF treatment cycle between 2002 \nand 2009. Obstetric outcomes and complications including gestational \ndiabetes (GDM), gestational hypertension (GHT), gestational proteinuric \nhypertension (PET), intrauterine growth restriction (IUGR), gestation at \ndelivery, baby's Apgar scores and admission to the neonatal intensive care \nunit (NICU) were reviewed. Results: Among the 864 patients undergoing IVF \ntreatment, there were 253 live births in total (25 live births in the PCOS \ngroup, 54 in the PCO group and 174 in the control group). The prevalence \nof obstetric complications (GDM, GHT, PET and IUGR) and the obstetric \noutcomes (gestation at delivery, birth weight, Apgar scores and NICU \nadmissions) were comparable among the three groups. Adjustments for age \nand multiple pregnancies were made using multiple logistic regression and \nwe found no statistically significant difference among the three groups. \nConclusion: Patients with PCO±PCOS do not have more adverse obstetric \noutcomes when compared with non-PCO patients undergoing IVF treatment. © \n2014 Informa UK Ltd. All rights reserved: reproduction in whole or part \nnot permitted. \n# Output \n[b3] \nStudy Design: Retrospective cohort study; \nSample Size: 864; \nData Source: NA; \nPopulation: Women with polycystic ovary syndrome (PCOS) and isolated \npolycystic ovaries (PCO); \nIntervention: In vitro fertilization (IVF) treatment; \nComparison: Age-matched controls; \nOutcomes: Obstetric complications (GDM, GHT, PET and IUGR) and the \nobstetric outcomes (gestation at delivery, birth weight, Apgar scores and \nNICU admissions);\n \n \nEvaluation \nTo evaluate the accuracy of the generated outcomes, we employed the established automatic \nmetrics in NLP, including ROUGE-1[28] and BERTScore[29]. These metrics were specifically \ndesigned to measure the quality of generated text compared to the reference text produced by \nhuman. ROUGE-N, a metric based on n-gram analysis, examined the overlap of common \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nwords and phrases between the two summaries. On the other hand, BERTScore uses contextual \nembeddings from a pre-trained large language model to derive a similarity score between the \ngenerated and reference texts. Unlike N-gram (ROUGE-1) method that relies on exact matches, \nBERTScore can account for semantic similarities at the word and sentence level. For both \nmetrics, we utilize the F1-score – which is the harmonic mean of the precision and recall scores \n– as our final standard for analysis. The score ranges from 0 to 1, and is calculated as \n \nF1-score /g3404 2/g3400 Prec isi o n /g3400 Rec all\nPrec is i o n /g3397 Rec all   \nwhere \nPre cisi o n RO U G E /g3404 /g1307 1- gr a m  H y p . /g1514 1- g r a m  R ef .\n/g1307 | Hyp . |   \nRec all ROU G E /g3404 /g1307 1- gr a m  H y p . /g1514 1-gram Re f .\n/g1307 | Ref . |   \nAnd  \nPrec is i on BE RT /g3404 1\n| /g1876 /g3548 | /g3533m a x\n/g3051 /g3284 /g1488/g3051\n/g3435 x /g3036\nT /g1876 /g3548 /g3037 /g3439\n/g3051 /g3548 /g3285 /g1488/g3051 /g3548\n/g4669  \nRec all BE R T /g3404 1\n| /g1876 | /g3533m a x\n/g3051 /g3548 /g3285 /g1488/g3051 /g3548\n/g3435 x /g3036\nT\n/g1876 /g3548 /g3037 /g3439\n/g3051 /g3284 /g1488/g3051\n \n \nin which x /g3036  represents the reference token and /g1876 /g3548 /g3037  represents the candidate token (hypothesis). \n \nNoticeably, recent research papers highlighted the inherent challenges in assessing the \nresponses of LLM using traditional automatic metrics in NLP . Consequentially, we also \ndesigned an independent evaluation mechanism in the form of a separate and specifically \ntailored GPT algorithm to evaluate whether the generated responses correspond with the \nground truth. To improve the stability of GPT’s measurement, the temperature of this machine \nwas set to 0, and no history of previous output was incorporated as input. The detailed prompt \nis outlined below.   \n \nPrompt of evaluation \n \n# Input \nHypothesis: {h} \nReference: {r} \n \n# Query \nYou will be given a hypothesis and a reference that represent the {element} element \nextracted from a medical paper. Your task is to compare the semantic similarity between \nthe hypothesis and the reference. The similarity score should be between 0 and 1, where \n0 means no similarity and 1 means the highest similarity \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \n   \n# Output \nPlease output your similarity score in the format: The similarity score is {your_score} \n \nTo ensure the validity of the evaluators, a cross-evaluation was performed on the element \nextracted by two independent researchers. Specifically, an accordance dataset is first produced \nby manually comparing the results for each label pair in the 100 papers, in which a score of 1 is \nassigned if the labels from both researchers matched (indicating agreement), and a score of 0 if \nthey differed (indicating disagreement). Subsequently, each evaluator would compute the \nsimilarity for each pair of labels, which will then be compared against the true accordance data \nto assess the consistency. \n \nDuring this process, we also calculated threshold values for the metric score produced by the \nevaluators for each element category, in order to define what constitutes an acceptable level of \nagreement. Specifically, we iterate over the potential threshold value from 0 to 1 with stepsize \nof 0.01, and assign a True prediction for metric scores above the threshold, False for scores \nbelow. Then, we determine which threshold yields the highest accuracy rate of F1-score across \nall comparisons between the evaluators and the accordance ground truth, and select it as the \neventual standard. \n \nTo assess the overall performance of the models, we employed this predefined threshold to \ncalculate the accuracy, sensitivity and specificity in GPT’s information extraction results. \nWe defined the accuracy rate \n/g1868 /g3030/g3042 /g3045/g3045/g3032 /g3030/g3047  as the proportion of GPT’s outputs that align with the \nground truth in the five repetitive trials. It is calculated separately across the 100 papers as \nfollows \n/g1307/g1868 /g3030 /g3042/g3045/g3045 /g3032/g3030/g3047 /g3404/g1307 1\n100 /g3533\n/g1307 ∑ max\n/g3046/g1488 /g3020\n/g46660, /g4666 /g1871 /g3047, /g3036 /g3398 thr e s hold /g3046 /g4667/g4667/g2873\n/g3047/g2880 /g2869\n5\n/g2869/g2868 /g2868\n/g3036/g2880 /g2869\n \n \nwhere /g1871 /g3047,/g3036  is the metric score for the /g1861 /g3047/g3035  paper in trial /g1872 , and threshold s is the threshold \ncalculated for the specific element nature. The average /g1868 /g3030/g3042/g3045 /g3045/g3032/g3030 /g3047  was employed to horizontally \ncompare the GPT models and prompt engineering strategies. \n \nGiven the risk of hallucination (producing information not grounded in the source material) \nand the possibility that not all elements of interest are present in a given abstract, we further \ndefine the following standard for a deeper analysis in sensitivity and specificity: \n \n- True Positive (TP): An element that is both present in the abstract as labeled in \ngroundtruth and correctly extracted by ChatGPT.   \n- True Negative (TN): An element that is neither present in the abstract nor falsely \nidentified by ChatGPT.   \n- False Positive (FP): An element that ChatGPT incorrectly reports as present in the \nabstract (hallucination).  \n- False Negative (FN): An element that is present in the abstract but is missed or wrongly \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nextracted by ChatGPT.  \n \nAnd sensitivity and specificity is calculated as \ns e n s it iv it y  /g3404\n/g1846/g1842\n/g1846/g1842 /g3397 /g1832/g1840  \nspeci /g976ici t y  /g3404\n/g1846/g1840\n/g1846/g1840 /g3397 /g1832/g1842  \n \nwhich measures the proportion of actual positives and negatives that are correctly identified by \nthe GPT, respectively. \nStatistical Methods \nFor each extracted item evaluated by one metric, a 2-way Analysis of Variance (ANOV A) \nmodel was used to analyze the impact of two factors, GPT versions and prompt engineering \nstrategies. We summarized all p-values across items and evaluators in one table, to analyze the \nsignificance of GPT model and prompts effects on the performance. The statistical analysis \nwas performed using the python package statsmodels (version 0.14.1) [30]. All significance \nlevel was set as 0.05, with all necessary assumptions for ANOV A, including normality and \nhomogeneity of variances, being assessed and satisfied. \n \nResults \nPaper selection and Data source \nFigure 2 represents the distribution of paper affiliation as a result of stratified sampling. The \ncollected dataset covered the six teaching schools in HKU LKS Faculty of Medicine, Public \nHealth, Nursing, Pharmacology and Pharmacy, Biomedical Sciences, Chinese Medicine and \nClinical Medicine, and it included at least one paper for each school. Among them, the School \nof Clinical Medicine took 74% of the papers, and thus the affiliation of these papers was traced \ndown to clinical departments, represented in green in Figure 2.  \n \nFigure 2. Paper affiliation distribution \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \n \nThe selected papers also provided comprehensive coverage across study types, sample size, \npublication year, and abstract length. The labeled ground truth indicated that the dataset \nconsisted of 22 retrospective studies, 13 laboratory studies, 10 prospective studies, 7 case \nreports, 5 reviews, 4 randomized controlled trials, and other types of study design. The sample \nsize ranged from 0 to 229428, with seven papers analyzing over 10,000 samples. Publication \ntime was evenly distributed from 2015 to 2023. The average length of the inputted abstract was \n1719.54 characters, with a standard deviation of 427.85. \nEvaluator Performance \nThe process of selecting the optimal threshold is presented in supplementary material 2.  \n \nWe randomly select the content and metric scores from three evaluators of 10 paper * 7 \nelements from different combination of models (GPT-3.5, GPT-4.0), prompt types and trials, \nand manually marked down the accordance between extracted information and ground truth as \na test set. Table 3 presented are performance metrics for three different evaluators to assess the \nquality of semantic similarity rating, and they are compared based on their accuracy, precision, \nand recall. \n \nTable 3. Accuracy, precision and recall of evaluators. \n \n BERT ROUGE-1 ChatGPT-4.0 \nAccuracy 0.8714 0.9429 0.9714 \nPrecision 0.8983 0.9643 0.9821 \nRecall 0.9464 0.9643 0.9821 \n \nThe ChatGPT Evaluator outperforms the other two evaluators in all three metrics. It is the most \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \naccurate and provides the highest precision and recall. The BERTScore Evaluator has the \nlowest accuracy and precision but maintains a high recall. The lower precision could imply that \nwhile BERT can identify many of the true positives, it may also mistakenly label some distinct \nelements as similar. Its relatively lower accuracy suggests that it doesn't perform as \nconsistently across all cases compared to the other evaluators. \n \nThe high recall rates across all evaluators suggest that they are generally good at identifying \nrelevant, semantically similar elements. In contrast, the variation in precision and accuracy \nindicates differences in their ability to exclude non-similar elements and correctly label the full \nrange of elements, respectively. \n \nOverall Performance \nIn our experiment, GPTs achieve considerable accuracy in extracting information from papers \nacross medical disciplines. Measured by the BERTScore, GPT-4.0 achieves over 80% \ncorrectness in five out of the seven items, and GPT-3.5 achieves over 80% in three items. \nSupplementary Material 3 includes a comprehensive table summarizing the average \nproportions of correctness, covering all 7 items under 8 prompt settings, generated by GPT-3.5 \nand GPT-4.0 and measured by the three different metrics. For clarity, Table 4 selectively \npresented each item's optimal performance and corresponding prompt engineering strategy. \nThe results also compare the optimal performance between GPT-3.5 and GPT4.0. \n \nTable 4. Optimal performance for each item and the corresponding prompt engineering \nstrategies. Mean and standard deviation across the five trials. \n \n \nItems GPT \nmodel \n/g1868 /g3030 /g3042/g3045/g3045 /g3032/g3030/g3047  \n(BERT) \n/g1868 /g3030/g3042 /g3045/g3045/g3032 /g3030/g3047  \n(GPT) \n/g1868 /g3030/g3042 /g3045/g3045/g3032 /g3030/g3047  \n(ROUGE) \nOptimal Prompt \nEngineering Strategies\n \nStudy \nDesign\n \nGPT-3.5 0.798 \n(0.4)\n \n0.862(0.86) 0.798(0.4) Alpha(BERT/ROUGE/\nGPT)\n \nGPT-4.0 0.796(0.3\n4)\n \n0.86(0.86) 0.796(0.34) Beta+Gamma(BERT/R\nOUGE), Alpha(GPT)\n \nSample \nSize \nGPT-3.5 0.992(0.0\n8)\n \n0.955(0.95) 0.992(0.08) Alpha+Gamma(BERT/\nROUGE), Alpha(GPT)\n \nGPT-4.0 0.964(0.1\n6)\n \n0.97(0.97) 0.964(0.16) Gamma(BERT/ROUGE\n), Alpha(GPT)\n \nData \nSource\n \nGPT-3.5 0.804(0.3\n8)\n \n0.716(0.72) 0.804(0.38) Alpha(BERT/ROUGE/\nGPT)\n \nGPT-4.0 0.852(0.3\n2) \n0.773(0.77) 0.852(0.32) Gamma(BERT/ROUGE\n/GPT)\n \nPatient GPT-3.5 0.844(0.3\n3) \n0.8(0.8) 0.844(0.33) Alpha(BERT/ROUGE/\nGPT)\n \nGPT-4.0 0.924(0.2 0.864(0.86)  0.924(0.24) Control(BERT/ROUGE\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \n4) /GPT) \nInterve\nntion \nGPT-3.5 0.568(0.4\n1)\n \n0.595(0.6) 0.568(0.41) \n  \nAlpha+Beta+Gamma(B\nERT/ROUGE), Control \n(GPT)\n \nGPT-4.0 0.688(0.4\n1)\n \n0.722(0.72) 0.688(0.41) Gamma(BERT/ROUGE\n/GPT)\n \nCompar\nison\n \nGPT-3.5 0.764(0.3\n9)\n \n0.809(0.81) 0.67(0.46) Alpha+Gamma(BERT/\nROUGE), Alpha(GPT)\n \nGPT-4.0 0.804(0.3\n6)\n \n0.845(0.85) 0.804(0.36) Gamma(BERT/ROUGE\n), Alpha(GPT)\n \nOutcom\nes\n \nGPT-3.5 0.706(0.3\n7)\n \n0.638(0.64) 0.706(0.37) Alpha+Beta(BERT/RO\nUGE), Alpha(GPT)\n \nGPT-4.0 0.864(0.3\n1)\n \n0.825(0.83) 0.864(0.31) Alpha(BERT/ROUGE/\nGPT)\n \n \nThe performance of GPT in extracting information across seven items can be categorized into \nthree distinct levels of complexity. The first level encompasses questions where a direct answer \ncan typically be found in the raw text. The sample size is an example of this level, and both \nGPT-3.5 and GPT-4.0 achieve accuracy levels exceeding 0.95 in extracting sample size. The \nsecond level pertains to questions requiring understanding and summarization skills to extract \nanswers. Most extracted items, including study design, data source, patient, comparison, and \noutcomes, belong to this category. Table 3 shows that GPT-3.5 achieves optimal performance \nfrom 0.7 to 0.8 for these items and GPT-4.0 from 0.8 to 0.9. Finally, intervention represents the \nthird level, which demands a high level of understanding and domain expertise to discern the \ncorrect answer accurately from potentially misleading information. In this regard, GPT-3.5 \nperformed under 0.6 while GPT-4.0 demonstrated accuracy around 0.7. \n \nFigure 3 employs a violin plot to illustrate the distribution of model performance. Noticeably, \nthe plots reveal bimodal distributions, with performance clustering at high and low accuracy \nextremes, representing that the GPT models are either all correct or all incorrect in their \nextractions. This observation demonstrates the stability of GPT’s performance in such \ninformation extraction tasks. Despite the common issue of the randomness of generative \nlanguage models, these results demonstrated that GPTs are controllable and consistent in \nevidence summarization. \n \nFigure 3. Violin plots of the performance distribution of GPT-3.5 and GPT-4.0 on each item to \nextracted. Y label represents the metrics and the dashed lines inside violins represent the \nquartiles.\n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \n \n \n \nPerformance of GPT-3.5 and GPT-4.0 \nThe GPT version is a statistically significant factor influencing model performance. As \npresented in Table 5, the ANOV A analysis reveals that 20 out of the 21 p-values assessing the \nimpact of GPT are significantly lower than 0.05. The only exception of p-value is associated \nwith the item Sample Size measured by BERTScore. \n \nTable 5. Summary of p-values in ANOV A analysis. For example, the first cell represents the \np-value corresponding to the factor “GPT versions”, utilizing study design data evaluated by \nthe ROUGE metric as input data for the ANOV A analysis. \nEvaluator Factor Study \nDesign \nSample \nSize \nData \nSource Patient Inter- \nvention Comparison Outcomes \n \nROUGE \n \nGPT <0.0001 0.0069 0.0107 <0.0001 <0.0001 0.0086 <0.0001 \nPrompt 0.0721 0.0005 0.0000 0.4689 0.9667 0.0022 0.9982 \nInteraction 0.0053 0.0134 0.9352 0.7269 0.9860 0.6483 0.9986 \nBERTScore \nGPT 0.0210 0.8896 <0.0001 <0.0001 <0.0001 0.0001 <0.0001 \nPrompt 0.3537 <0.0001 0.0000 0.9949 0.9470 0.0007 0.5658 \nInteraction 0.0554 <0.0001 0.9218 0.9934 0.9647 0.7438 0.2389 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nGPT \nGPT 0.0012 <0.0001 0.0011 <0.0001 <0.0001 0.0002 <0.0001 \nPrompt 0.0026 <0.0001 <0.0001 0.1556 0.8368 0.0434 0.1503 \nInteraction 0.0012 <0.0001 0.9431 0.8998 0.9806  0.6406 0.3165 \n \n \nIn pair-wise comparison, GPT-4.0 tends to surpass GPT-3.5, particularly in complex tasks \nwhere direct answers are in the raw text. Table 4 demonstrated the significant edges of GPT-4.0 \nover GPT-3.5, characterized by both enhanced proportion of correctness and a reduced standard \ndeviation,  on Data Source (GPT-4.0: /g1868 /g3030/g3042/g3045 /g3045/g3032/g3030 /g3047  = 0.852; GPT-3.5, SD=0.32: /g1868 /g3030/g3042 /g3045/g3045/g3032 /g3030/g3047 =0.852, \nSD=0.38), Patient (GPT-4.0: /g1868 /g3030/g3042 /g3045/g3045/g3032 /g3030/g3047  = 0.924, SD=0.24; GPT-3.5: /g1868 /g3030/g3042 /g3045/g3045/g3032 /g3030/g3047 =0.844, SD=0.33), \nIntervention(GPT-4.0:  /g1868 /g3030/g3042/g3045 /g3045/g3032/g3030 /g3047 = 0.688, SD=0.41; GPT-3.5: /g1868 /g3030/g3042/g3045 /g3045/g3032/g3030/g3047 =0.568, SD=0.41), \nComparison (GPT-4.0:  /g1868 /g3030 /g3042/g3045/g3045 /g3032/g3030/g3047 = 0.804, SD=0.36; GPT-3.5: /g1868 /g3030/g3042 /g3045/g3045/g3032 /g3030/g3047 =0.764, SD=0.39), and \nOutcomes (GPT-4.0:  /g1868 /g3030/g3042 /g3045/g3045/g3032 /g3030/g3047 = 0.864, SD=0.31; GPT-3.5: /g1868 /g3030/g3042 /g3045/g3045/g3032 /g3030/g3047 =0.706, SD=0.37). However, \nit is worth noting that GPT-4.0 may not necessarily outperform its predecessors. For example, \nGPT-4.0 does not reach the same level as GPT-3.5 on Sample Size (GPT-4.0:  /g1868 /g3030/g3042 /g3045/g3045/g3032 /g3030/g3047 = 0.964, \nSD=0.16; GPT-3.5: /g1868 /g3030/g3042/g3045 /g3045/g3032/g3030/g3047 =0.992, SD=0.08), even though the extraction is relatively simple \nand there tends to be a finite numeric answer in the abstract. \n \nFigure 3 visually compares the performance distribution of GPT-3.5 and GPT-4.0 on the seven \nitems. Noticeably, it can be found that the performance improvement of GPT-4.0 is brought by \nshrinking tails near 0, which means the occurrence of all false values. This can verify the fact \nthat the enhanced ability of GPT-4.0 in extracting the expected information. \nEffects of Prompt Engineering Strategies \nPrompt engineering strategies are likely to influence model performance positively, but their \neffectiveness is not guaranteed. As presented in Table 5, the ANOV A analysis reveals that the \nimpact of the GPT prompt is statistically significant for three extracted items, Sample Size, \nData Source, and Comparison, measured by all three evaluators. There needs to be more \nevidence for other items to prove the impact of prompt engineering strategies. The discordance \nof impact can also be verified in Table 4, where different extraction tasks exhibit a different \npreference for specific prompt engineering strategies to achieve optimal performance. \n \nIt is also noticeable that prompt engineering strategies may not have additive effects with each \nother. For example, in Figure 4, the combination Alpha+Beta does not perform as well as either \nAlpha or Beta. Combined strategies, such as Alpha+Beta+Gamma, may lead to inferior results \ncompared to the simple one. The control set produces a considerably good performance, and it \nis the optimal strategy for GPT-4.0 to extract the patient information, as shown in Table 4. \n \nFigure 4. Violin plots of the performance distribution of GPT-3.5 and GPT-4.0 using different \nprompt engineering strategies. Y label represents the metrics and the dashed lines inside violins \nrepresent the quartiles. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \n  \n \n \n \n \n \nThe effects of GPT versions and prompt engineering strategies will likely interact. In ANOV A \nanalysis, the interaction between the GPT version and prompt engineering strategies is \nstatistically significant based on the Sample Size extraction, as assessed by all three \nevaluators (ROUGE, p < .001; BERTScore, p<.001; GPT, p<.001). However, for other items, \ninteraction may exist but needs more statistical strength. Table 4 demonstrates that the optimal \nprompt engineering strategy differs between GPT-3.5 and GPT-4.0 for each item. In general, \nGPT-3.5 tends to favour the Alpha strategy, persona (optimal strategy for Study Design, Data \nSource, Patient). In contrast, GPT-4.0 tends to prefer the Gamma strategy, with few-shot \nprompting (optimal strategy for sample size, data source, intervention, comparison).\n \nDiscission \nOur study presents a rigorous evaluation of GPT-3.5 and GPT-4.0 in extracting medical \ninformation from titles and abstracts. To ensure comprehensive coverage across various \nmedical domains, a stratified sampling method was adopted for paper selection from almost all \naffiliated medical schools and departments of a university. We employed multiple evaluators, \nrepetitive trials, and experiments on prompt engineering strategies to enhance the integrity of \nresults. Our findings demonstrated that GPTs can effectively extract or summarize information \ndescribed in the abstracts. Notably, GPT-4.0 exhibits robust performance in providing thorough \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nanswers and understanding and summarizing abstracts. However, there is still room for \nimprovement in accurately discerning information that requires sophisticated understanding \nand domain expertise. When combined with appropriate prompt engineering strategies, the \naccuracy level achieves over 0.8 in extracting information related to study design, sample size, \ndata source, patient, comparison and outcomes. Moreover, the improvement in efficiency is \nremarkable by reducing 8 to 10 hours of human labor to under 5 minutes (GPT-3.5) or 40 \nminutes (GPT-4.0). Our research pioneers the exploration of a new generation of Large \nLanguage Models in medical evidence summarization and offers potential applications in \nvarious scenarios. It provides empirical evidence to support the development of credible \nautomatic tools for medical literature screening and review. With critical information extracted, \nautomatic tools can strike a balance between efficiency and transparency. \n \nThe field of large language models is rapidly advancing. Our investigations reveal that the \neffect of the GPT version on the accuracy of information extraction is significant when \ncomparing GPT-3.5 and GPT-4.0 (Table 5). In particular, GPT-4.0 presents a more robust \nperformance in summarizing complex information that may not be readily apparent in the raw \ntext, such as the PICOs. On the other hand, the drawback of GPT-4.0 compared to its \npredecessor is associated with the time and cost. According to the OpenAI website, by March \n2024, the price of GPT-3.5 Turbo is one-twentieth of that of GPT-4.0 Turbo [31]. In our \nexperiment, we found that the time required for GPT-3.5 to label 100 papers is approximately \none-tenth of the time taken by GPT-4.0. This significant difference may be attributed to the rate \nlimits imposed by the API, as noted on OpenAI's website. Specifically, the rate limit for \nGPT-4-turbo is 500 RPM (Requests Per Minute) for Tier 1 users, while GPT-3.5-turbo offers a \nhigher rate limit of 3500 RPM [32].  \n \nPrompt engineering strategies play an essential role in enhancing LLMs’ performance. This \nstudy find that the optimal prompt engineering strategies vary depending on the extraction \ntasks and GPT versions employed. Overall, two useful strategies are recommended to attempt: \npersona and few-shot prompting. Although, the chain of thought strategy might help guide \nmulti-step tasks, it might not be effective in straightforward tasks like the information \nextraction in this study (Table 4). It is worth noting that the combination of prompt engineering \nstrategies may not yield additive effects on the final results (Figure 4). Considering the cost \nassociated with input tokens, it is recommended to use a conservative approach to employ \nprompt engineering strategy in prompt development. \n \nMoreover, this study extensively examines and compares the performance of evaluators \nutilized in the experiment, including two well-established NLP metrics, ROUGE-1 and \nBERTScore, and one newly developed GPT evaluator. Overall, the three evaluators provide \nconsistent performance evaluation across various extraction items and prompt engineering \nstrategies (Figure 3 and Figure 4). However, each metric has its limitations. ROUGE-1, as a \nbasic metric relying on common words, is susceptible to issues like spelling variations and \nabbreviations. For example, when the output is “cluster randomised controlled trial” while the \nground truth is “cluster randomized controlled trial”, ROUGE-1 assigns a score of 0.5, which \ntends to be labeled as a mismatch with the ground truth (record 17, Supplementary Material 5). \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nTo overcome these limitations, BERTScore incorporates synonyms and offers a continuous \nscoring system. In this study, we employed a threshold to transfer from a continuous value to a \nbinary label of a correct answer. While the threshold is optimized based on available data, it \nmay not effectively decide on the alignment between the generated answer and ground truth. \nTake record 89 as an illustrative example (Supplementary Material 5). The labeled ground truth \nof record 89 is “survey,” and the generated answer by GPT-4.0 is “school-based survey.” In this \ncase, the BERTScore output is 0.834 under the optimal threshold, resulting in a labeled \nmismatch with the ground truth.   \n \nOur study revealed an interesting observation regarding the potential of GPT as a promising \nand unique tool to assess the accuracy of generated text compared to the ground truth. Notably, \nGPT evaluators can leverage their pre-trained knowledge base to evaluate text based not only \non lexical similarity but also on semantic similarity. This ability effectively addresses some \nsignificant limitations of existing NLP metrics. To illustrate, consider the extracted \nintervention for record 26 as an example (Supplementary Material 6). The labeled ground truth \nis “Pre-emptive use of proton pump inhibitors (PPI),” while the generated output of GPT-4.0 is \n“Pre-emptive PPI (intravenous esomeprazole followed by high-dose oral esomeprazole).” In \nthis instance, the generated answer is not only correct but also superior to the ground truth, as it \nintegrates the information from another helpful sentence: “The PPI group received intravenous \nesomeprazole 4 h before the EST and then every 12 h for 1 day, followed by high-dose oral \nesomeprazole for 10 days.”[33]. However, both ROUGE-1 and BERTScore fail to label this \ncorrect answer due to a lack of overlapping words. In contrast, the GPT evaluator assigns a \nscore of 0.85, acknowledging the reasonableness of the generated output. This outcome \nhighlights the potential of the GPT evaluator that evaluating not merely the words but also \nmeanings can offer a more authentic and accurate evaluation. We also found that GPT exhibits \nlogical thought when evaluating the answer during development. For instance, it can \ndistinguish the difference between the outcome of HbA1c and the drop of HbA1c. These \nproperties allow GPT to be further developed into a powerful and systematic tool to evaluate \nthe performance of complex information extraction. \n \nHowever, it is important to acknowledge several limitations in this study. First, while covering \na wide range of medical domains, the labeled ground truth represents the assessment level of \nhuman evaluators. It may not necessarily serve as the golden standard due to a lack of domain \nknowledge. As a result, the performance of GPTs could be underestimated. To address this, \nfuture research is encouraged to validate GPT’s performance in one specific medical domain. \nWhen the targeted literature focuses on one area, domain knowledge can be provided as \ncontextual information to enhance performance. Another limitation of this study is that we \nsolely tested GPT from the abstracts. Given the exploding ability of LLMs in handling long \ntext, figures, and tables, it is recommended that future researchers extend the GPT tools to \noperate on full text or PDF level. This expansion would extract more valuable information \nsources and open up broader possibilities for GPT to facilitate medical research.   \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nConclusion \nIn this study, we have developed a robust method based on GPT for extracting or summarizing \ninformation from the abstract of medical research papers. We conducted thorough experiments \nto systematically evaluate the effects of GPT versions and prompt engineering strategies on the \nperformance of models. The evaluation was carried out utilizing both well-established NLP \nmetrics and a newly developed GPT evaluator. Notably, the GPT evaluator demonstrated its \neffectiveness and advantage by leveraging its capability of semantic understanding. Our result \nvalidates the potential of GPT as a reliable, stable, and accurate tool for summarizing medical \nevidence, particularly when appropriate prompt settings are employed. We encourage further \nresearch and studies to continue refining and advancing this tool, unlocking the potential of the \nnew generation of technology in medical research. \nSupplementary Material \nSupplementary material 1 Summary table of selected papers \nSupplementary material 2 Optimal threshold and the process of grid search \nSupplementary material 3 Summary of model performance \nSupplementary material 4 ANOV A tables \nSupplementary material 5 Illustrative example of study design.  \nSupplementary material 6 Illustrative example of intervention \nReference  \n1. Li, J., Dada, A., Puladi, B., Kleesiek, J., & Egger, J. (2024). ChatGPT in \nhealthcare: a taxonomy and systematic review. Computer Methods and Programs in \nBiomedicine, 108013. \n2. Lim, Z. W., Pushpanathan, K., Yew, S. M. E., Lai, Y ., Sun, C. H., Lam, J. S. \nH., ... & Tham, Y . C. (2023). Benchmarking large language models’ performances for \nmyopia care: a comparative analysis of ChatGPT -3.5, ChatGPT-4.0, and Google Bard. \nEBioMedicine, 95. \n3. Shah NH, Entwistle D, Pfeffer MA. Creation and Adoption of Large \nLanguage Models in Medicine. JAMA. 2023;330(9):866-9. \n4. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of gpt-4 \non medical challenge problems. arXiv preprint arXiv:230313375. 2023. \n5. Tang, L., Sun, Z., Idnay, B., Nestor, J. G., Soroush, A., Elias, P. A., ... & Peng, \nY . (2023). Evaluating large language models on medical evidence summarization. npj \nDigital Medicine, 6(1), 158. \n6. Shaib, C., Li, M. L., Joseph, S., Marshall, I. J., Li, J. J., & Wallace, B. C. \n(2023). Summarizing, simplifying, and synthesizing medical evidence using gpt-3 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \n(with varying success). arXiv preprint arXiv:2305.06299. \n7. Tian, S., Jin, Q., Yeganova, L., Lai, P. T., Zhu, Q., Chen, X., ... & Lu, Z. \n(2024). Opportunities and challenges for ChatGPT and large language models in \nbiomedicine and health. Briefings in Bioinformatics, 25(1), bbad493. \n8. O’Mara-Eves A, Thomas J, McNaught J, Miwa M, Ananiadou S. Using text \nmining for study identification in systematic reviews: a systematic review of current \napproaches. Systematic Reviews. 2015;4(1):5. \n9. Ouzzani, M., Hammady, H., Fedorowicz, Z., & Elmagarmid, A. (2016). \nRayyan—a web and mobile app for systematic reviews. Systematic reviews, 5, 1-10. \n10. Marshall, I. J., Kuiper, J., & Wallace, B. C. (2016). RobotReviewer: \nevaluation of a system for automatically assessing bias in clinical trials. Journal of the \nAmerican Medical Informatics Association, 23(1), 193-201. \n11. Howard, B. E., Phillips, J., Miller, K., Tandon, A., Mav, D., Shah, M. R., ... & \nThayer, K. (2016). SWIFT-Review: a text-mining workbench for systematic review. \nSystematic reviews, 5, 1-16. \n12. Blaizot AA-O, Veettil SK, Saidoung P, Moreno-Garcia CF, Wiratunga N, \nAceves-Martins MA-OX, et al. Using artificial intelligence methods for systematic \nreview in health sciences: A systematic review. (1759-2887 (Electronic)) \n13. Feng Y , Liang S, Zhang Y , Chen S, Wang Q, Huang T, et al. Automated \nmedical literature screening using artificial intelligence: a systematic review and \nmeta-analysis. (1527-974X (Electronic)). \n14. Shemilt I, Khan N, Park S, Thomas J. Use of cost-effectiveness analysis to \ncompare the efficiency of study identification methods in systematic reviews. \nSystematic Reviews. 2016;5(1):140. \n15. Matthew JP, David M, Patrick MB, Isabelle B, Tammy CH, Cynthia DM, et \nal. PRISMA 2020 explanation and elaboration: updated guidance and exemplars for \nreporting systematic reviews. BMJ. 2021;372:n160. \n16. Matsui KaU, Tomohiro and Aoki, Yumi and Maruki, Taku and Takeshima, \nMasahiro and Yoshikazu, Takaesu. Large Language Model Demonstrates \nHuman-Comparable Sensitivity in Initial Screening of Systematic Reviews: A \nSemi-Automated Strategy Using GPT-3.5. SSRN:4520426. 2023. \n17. Mahuli SA, Rai A, Mahuli A V , Kumar A. Application ChatGPT in \nconducting systematic reviews and meta-analyses. British Dental Journal. \n2023;235(2):90-2. \n18. Hill, J. E., Harris, C., & Clegg, A. (2023). Methods for using Bing's \nAI\n/i1 powered search engine for data extraction for a systematic review. Research \nSynthesis Methods. \n19. Gilbert S, Harvey H, Melvin T, V ollebregt E, Wicks P. Large language model \nAI chatbots require approval as medical devices. Nature Medicine. \n2023;29(10):2396-8. \n20. Chen, P., Huang, Z., Deng, Z., Li, T., Su, Y ., Wang, H., ... & He, J. (2023). \nEnhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt \nEngineering Strategies. arXiv preprint arXiv:2312.04344. \n21. Duke University. (2019). LibGuides: Evidence-Based Practice: PICO. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nDuke.edu. https://guides.mclibrary.duke.edu/ebm/pico \n22. Grabb, D. (2023). The impact of prompt engineering in large language model \nperformance: a psychiatric example. Journal of Medical Artificial Intelligence, 6. \n23. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y ., & Iwasawa, Y . (2022). Large \nlanguage models are zero-shot reasoners. Advances in neural information processing \nsystems, 35, 22199-22213. \n24. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ... & Zhou, D. \n(2022). Chain-of-thought prompting elicits reasoning in large language models. \nAdvances in Neural Information Processing Systems, 35, 24824-24837. \n25. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., ... & Zhou, D. \n(2022). Self-consistency improves chain of thought reasoning in language models. \narXiv preprint arXiv:2203.11171. \n26. Zhao, Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021, July). Calibrate \nbefore use: Improving few-shot performance of language models. In International \nConference on Machine Learning (pp. 12697-12706). PMLR. \n27. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... \n& Amodei, D. (2020). Language models are few-shot learners. Advances in neural \ninformation processing systems, 33, 1877-1901. \n28. Lin, C. Y . (2004, July). Rouge: A package for automatic evaluation of \nsummaries. In Text summarization branches out (pp. 74-81). \n29. Zhang, T., Kishore, V ., Wu, F., Weinberger, K. Q., & Artzi, Y . (2019). \nBertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675. \n30. Seabold, Skipper, and Josef Perktold. “statsmodels: Econometric and \nstatistical modeling with python.” Proceedings of the 9th Python in Science Conference. \n2010. \n31. OpenAI. (2023). Pricing. Openai.com. https://openai.com/pricing\n \n32. OpenAI. (2023). Rate Limits. Openai.com. \nhttps://platform.openai.com/docs/guides/rate-limits/usage-tiers?context=tier-one \n33. Leung, W. K., But, D. Y ., Wong, S. Y ., Tong, T. S., Liu, K. S., Cheung, K. \nS., ... & Hung, I. F. (2018). Prevention of post /i1 sphincterotomy bleeding by proton \npump inhibitor: A randomized controlled trial. Journal of digestive diseases, 19(6), \n369-376. \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nFigure 1. Flowchart of overall study design; \n  \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nFigure 2. Paper affiliation distribution \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nFigure 3. Violin plots of the performance distribution of GPT-3.5 and GPT-4.0 on each \nitem to extracted. Y label represen ts the metrics and the dashed line s inside violins \nrepresent the quartiles. \n \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint \nFigure 4. Violin plots of the performance distribution of GPT -3.5 and GPT-4.0 using \ndifferent prompt engineering strategies. Y label represent s the metrics and the dashed \nlines inside violins represent the quartiles. \n \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted March 21, 2024. ; https://doi.org/10.1101/2024.03.20.24304572doi: medRxiv preprint ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4724319875240326
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.47047215700149536
    },
    {
      "name": "Information extraction",
      "score": 0.4296375811100006
    },
    {
      "name": "Library science",
      "score": 0.3771299719810486
    },
    {
      "name": "Data science",
      "score": 0.33338475227355957
    },
    {
      "name": "Computational biology",
      "score": 0.32661953568458557
    },
    {
      "name": "Information retrieval",
      "score": 0.251964271068573
    },
    {
      "name": "Biology",
      "score": 0.18197807669639587
    },
    {
      "name": "Chemistry",
      "score": 0.14253756403923035
    },
    {
      "name": "Chromatography",
      "score": 0.0836687684059143
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 7
}