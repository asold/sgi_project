{
  "title": "Leveraging Large Language Models for Generating Responses to Patient Messages",
  "url": "https://openalex.org/W4384455669",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2555062769",
      "name": "Siru Liu",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2157272911",
      "name": "Allison B. McCoy",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2231056103",
      "name": "Aileen P Wright",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2984257326",
      "name": "Babatunde Carew",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2227829513",
      "name": "Julian Z. Genkins",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2720371605",
      "name": "Sean S. Huang",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2142701830",
      "name": "Josh F Peterson",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2783176254",
      "name": "Bryan Steitz",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2035694469",
      "name": "Adam. Wright",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2555062769",
      "name": "Siru Liu",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2157272911",
      "name": "Allison B. McCoy",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2231056103",
      "name": "Aileen P Wright",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2984257326",
      "name": "Babatunde Carew",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2227829513",
      "name": "Julian Z. Genkins",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2720371605",
      "name": "Sean S. Huang",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2142701830",
      "name": "Josh F Peterson",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2783176254",
      "name": "Bryan Steitz",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2035694469",
      "name": "Adam. Wright",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2991399758",
    "https://openalex.org/W2795347911",
    "https://openalex.org/W3093164586",
    "https://openalex.org/W2754518417",
    "https://openalex.org/W3205513304",
    "https://openalex.org/W4293457270",
    "https://openalex.org/W4200043507",
    "https://openalex.org/W2996946491",
    "https://openalex.org/W3005890082",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4380995257",
    "https://openalex.org/W4381192815",
    "https://openalex.org/W4381893163",
    "https://openalex.org/W4382395040",
    "https://openalex.org/W4313451803",
    "https://openalex.org/W4366769280",
    "https://openalex.org/W4381185932",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4381930847",
    "https://openalex.org/W4309791331",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W6851960618",
    "https://openalex.org/W3006900481",
    "https://openalex.org/W2948715311",
    "https://openalex.org/W2093274439",
    "https://openalex.org/W6761205521",
    "https://openalex.org/W2327037637",
    "https://openalex.org/W6852449896"
  ],
  "abstract": "ABSTRACT Objective This study aimed to develop and assess the performance of fine-tuned large language models for generating responses to patient messages sent via an electronic health record patient portal. Methods Utilizing a dataset of messages and responses extracted from the patient portal at a large academic medical center, we developed a model (CLAIR-Short) based on a pre-trained large language model (LLaMA-65B). In addition, we used the OpenAI API to update physician responses from an open-source dataset into a format with informative paragraphs that offered patient education while emphasizing empathy and professionalism. By combining with this dataset, we further fine-tuned our model (CLAIR-Long). To evaluate the fine-tuned models, we used ten representative patient portal questions in primary care to generate responses. We asked primary care physicians to review generated responses from our models and ChatGPT and rated them for empathy, responsiveness, accuracy, and usefulness. Results The dataset consisted of a total of 499,794 pairs of patient messages and corresponding responses from the patient portal, with 5,000 patient messages and ChatGPT-updated responses from an online platform. Four primary care physicians participated in the survey. CLAIR-Short exhibited the ability to generate concise responses similar to provider’s responses. CLAIR-Long responses provided increased patient educational content compared to CLAIR-Short and were rated similarly to ChatGPT’s responses, receiving positive evaluations for responsiveness, empathy, and accuracy, while receiving a neutral rating for usefulness. Conclusion Leveraging large language models to generate responses to patient messages demonstrates significant potential in facilitating communication between patients and primary care providers.",
  "full_text": "Leveraging Large Language Models for Generating Responses to Patient Messages \nSiru Liu, PhD1, Allison B. McCoy, PhD1, Aileen P. Wright, MD, MS1,2, Babatunde Carew, MD3, \nJulian Z. Genkins, MD4, Sean S. Huang, MD1,2, Josh F. Peterson, MD, MPH1,2, Bryan Steitz, \nPhD1, Adam Wright, PhD1 \n1Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN, USA; \n2Department of Medicine, Vanderbilt University Medical Center, Nashville, TN, USA; \n3Department of General Internal Medicine and Public Health, Vanderbilt University Medical Center, \nNashville, TN, USA \n4Department of Medicine, Stanford University, Stanford, CA, USA \n \n \n \n \nCorresponding Author: \nSiru Liu, PhD \nDepartment of Biomedical Informatics \nVanderbilt University Medical Center \n2525 West End Ave #1475 \nNashville, TN, 37212 \nPhone: 615-875-5216 \nEmail: siru.liu@vumc.org \n  \nKeywords: artificial intelligenc e; clinical decision support; large language model ; patient portal ; \nprimary care \nWord count: 3766 words (up to 4000 words) \n  \nTables: 2 (up to 4) \nFigures: 6 (up to 6) \n \n \n \n \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nABSTRACT (249 words)  \nObjective: This study aimed to develop and assess the performance of fine-tuned large language models \nfor generating responses to patient messages sent via an electronic health record patient portal. \nMethods: Utilizing a dataset of messages and responses extracted from the patient portal at a large \nacademic medical center, we developed a model (CLAIR-Short) based on a pre -trained large language \nmodel (LLaMA-65B). In addition, we used the OpenAI API to update physician responses from an open -\nsource dataset into a format with informative paragraphs that offered patient education while emphasizing \nempathy and professionalism. By combining with this dataset, we further fine-tuned our model ( CLAIR-\nLong). To evaluate the fine-tuned models, we used ten representative patient portal questions in primary \ncare to generate responses. We asked primary care physicians to review generated responses from our \nmodels and ChatGPT and rated them for empathy, responsiveness, accuracy, and usefulness.  \nResults: The dataset consisted of a total of 499,794 pairs of patient messages and corresponding \nresponses from the patient portal, with 5,000 patient messages and ChatGPT-updated responses from an \nonline platform. Four primary care physicians participated in the survey. CLAIR-Short exhibited the ability \nto generate concise responses similar to provider’s responses. CLAIR-Long responses provided increased \npatient educational content compared to CLAIR -Short and were rated similarly to ChatGPT's  responses, \nreceiving positive evaluations for responsiveness, empathy, and accuracy, while receiving a neutral rating \nfor usefulness. \nConclusion: Leveraging large language models to generate responses to patient messages demonstrates \nsignificant potential in facilitating communication between patients and primary care providers. \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nINTRODUCTION \nSupported by more than $34 billion in government subsidies, the rise in adoption of electronic health records \n(EHRs) has led to a significant increase in the use of patient portals as a means of communication between \nhealthcare providers and patients.[1,2] As a result, effectively managing patient messages in EHR inboxes \nhas become an important clinical issue that needs to be addressed urgently. As an example, primary care \nphysicians typically spend 1.5 hours per day processing approximately 150 inbox messages, continuing \ntheir work even after regular clinic hours.[3,4] This challenge is escalating due to several factors. First, the \nvolume of patient messages is projected to grow significantly due to federal laws such as the 21st Century \nCures Act which requires the instant release of test results .[5] The out-of-pocket expenses of in -person \nvisits has also led to a preference for consultations via patient portals.[6] Finally, the pandemic prompted a \n157% surge in patient messages, a trend that persisted even post -pandemic.[7] Research indicates that \npatients have developed an expectation for dire ct and prompt  communication with their healthcare \nproviders through patient portals ;[6] certain time -sensitive messages, such as requests for COVID-19 \nantiviral medications within a five-day onset period, add to this pressure.[8] Overall, the constant influx of \npatient messages has evolved into a prominent stressor in clinics, particularly among primary care \nphysicians, contributing to burnout.[9] \n \nLarge language models present a promising solution to this dilemma by enabling the automated generation \nof draft responses for healthcare providers. These models, trained on extensive textual data with billions of \nparameters, are capable of generating human-like text and performing a variety of tasks, from answering \nquestions to summarizing and brainstorming .[10] A recent development in this domain, ChatGPT, has \nattracted significant attention within the medical community.[11–15] Despite not being specifically trained \non medical text, ChatGPT has demonstrated impressive proficiency in medical contexts, including passing \nthe U.S. Medical Licensing Examination (USMLE), clinical informatics board examination, and refining alert \nlogic to improve clinical decision support (CDS).[16–18] In particular, a recent study used ChatGPT to \ngenerate responses to 195 patient questions from social media forums . The study found that ChatGPT \nresponses outperform ed those of physicians, receiving significantly higher ratings for quality and \nempathy.[19] This study used an “out of the box” version of ChatGPT, but it is possible to further optimize \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nlarge language models’ performance on specialized domains by fine-tuning them for specific tasks.[20] For \ninstance, a recent study utilized 100,000 patient-doctor online conversations to fine-tune the open-source \nLarge Language Model Meta AI ( LLaMA)-7B model, which showed improved  performance in similarity \nmetric (e.g., BERTScores) in comparison to ChatGPT when answering patient questions.[21] However, it \nis worth noting that these studies collected patient questions from online platforms, not from patient portals. \nA challenge with the use of the similarity metric is that it mainly measures similarity to the physician’s \nresponse, rather than accuracy or usefulness, so if the generated message is good but different from the \nreference response, it may score poorly. \n \nThe objectives of this study  were 1) to fine -tune a large language model locally using messages and \nhealthcare provider responses from the patient portal, and 2) to assess the generated responses from the \nfine-tuned model  and compare them to actual provider  responses and generated responses from \nChatGPT3.5 and ChatGPT4. Our key advantages over prior studies are 1) our use of actual patient portal \nmessages, 2) development of a custom model for patient message-answering and 3) scoring of responses \nby blinded physicians rather than similarity metrics like BERTScore. \n \nMETHODS \nData Collection and Preprocessing \nWe conducted this project at Vanderbilt University Medical Center (VUMC), a large healthcare system in \nthe Southeastern United States using the Epic (Epic Systems Co., Verona, WI) EHR. We extracted patient \nmessages sent to adult primary care providers along with corresponding responses from January 1, 2022 \nuntil March 7, 2023 from VUMC’s clinical data warehouse. When multiple messages were sent by a patient \nor a provider prior to receiving a response, we combin ed the messages into one. Patient messages and \nresponses from January 1, 2022 to Feb 28, 2023 were used to develop models. To remove PHI and de-\nidentify our dataset, we used an automated deidentification pipeline  – Stanford & Penn & The Medical \nImaging Data Resource Center ( MIDRC) Deidentifier.[22] For instance, i t replaced patient name s with \n[PATIENT], provider names with [HCW], and telephone numbers with [PHONE].  \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nTo augment the local dataset, we randomly selected 5,000 patient questions and physician responses from \nan open-source dataset (including 200,000 real conversations between patients and providers on an online \nplatform).[21] We then applied the OpenAI API (gpt-3.5-turbo) to improve the original response s into \ninformative paragraphs with empathy and professionalism and prioritize the patient's well-being and comfort \nthroughout the response as a third source (Figure 1). An example of the updated response is shown in \nFigure 2. In our prompts, we emphasized the role by using the phase “imagine that you are a primary care \ndoctor” to avoid GPT declining to answer medical questions. Full text of prompts is provided in Appendix 1.   \n \nModel Development \nWe developed our model using LLaMA-65B, the largest version of LLaMA models.[17,23] Leveraging low-\nrank adaptation, we performed supervised fine-tuning using a high-quality dataset crafted for instruction-\nfollowing tasks, including data generated by GPT -4 from 52,000 prompts in Alpaca .[24,25] After gaining \nbasic conversation capabilities, we developed two models : 1 ) Comprehensive Large Language Model \nArtificial Intelligence Responder (CLAIR)-Short: fine-tuned using the local dataset of patient messages and \nresponses from VUMC, and 2) CLAIR-Long: fine-tuned using a combination of the local dataset augmented \nwith 5,000 open-source patient questions + ChatGPT updated responses. The fine-tuning process was \nconducted on four A100-80G GPUs over five days with the following hyperparameters, optimizer: AdamW, \nbatch size: 128, learning rate: 3e-4, number of epochs: 3, lora_r: 8, lora_alpha: 16, and lora_dropout: 0.05. \nThe overview of the model development and evaluation process is shown in Figure 1.  \n \nEvaluation Dataset \nTo evaluate the models, we curated a dataset from patient messages and healthcare provider responses \nbetween March 1, 2023 and March 7, 2023. We reviewed and selected 40 questions that could be answered \ncomprehensively with minimal additional patient information and did require utilization of other tools to \ncomplete the task . A primary care physician further reviewed and ultimately selected 10 representative \nquestions based on a patient message framework.[26] Along with removing PHI, the primary care physician \ncreated a new, rephrased message inspired by the content of the original message. The rephrased patient \nmessages, healthcare provider responses, and corresponding categories are listed in Table 1.  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nTable 1. Selected patient messages (rephrased), the actual provider’s responses and categories. \nCategory Rephrased patient message Actual provider response \nIllness requiring \nin-person \nevaluation \nHello Dr. [HCW]! I think I might have a \nbladder infection (urinary frequency, \ndysuria, urgency and lower back pain.) \nI've been taking AZO the past few days. \nWhat would you recommend? I \nappreciate it! \nHello, We advise going to the urgent care clinic \nor walk in clinic, to have urine tested and to \nidentify which bacteria is growing and prescribe \nthe right antibiotic. [HCW] \nRecommendation \nneeded for over-\nthe-counter \nmedication \nI could really use a sleep aid. Recently \nI've been having a night or two, \nsometimes 4, where I just can't sleep. I'm \nfeeling desperate due to lack of sleep and \nreally need something to help me get \nthrough this. Is there something you'd \nrecommend? Thanks \nI would suggest to try melatonin 6 to 9 mg at \nbedtime to see if that would help you with your \nsleep. Thank you Dr. [HCW] \nRequest for \nprescription \nmedication \nGot an upcoming trip to Mexico. Can't \ncontrol lack of clean water there - might \nget diarrhea. Could ruin my trip. How \nabout some diarrhea pills for this trip? My \nfriend and their spouse got Rifaximin and \nZithromax from their doctor for their trip. \nHi [PATIENT]! I definitely think you should take \nantibiotics along on your trip. I prefer \nazithromycin (Zithromax) - I think it works a little \nbetter than rifaximin. I sent in six 500mg tablets. \nThe course for travelers diarrhea is 3 days, but \nas you will be in Mexico for a while, I want you to \nhave an additional 3 days if you have diarrhea \ntwice. I put the instructions on the bottle at the \npharmacy as well! [HCW] \nRequest for \nmedication refill \nHi Doctor [HCW]. I did something to my \nback this week and I'm having back \nspasms again. This happens once in a \nwhile. Last time, which was a few months \nago, I was prescribed cyclobenzaprine 5 \nmg tablet (FLEXERIL). This really helped \nme. Can you please renew this \nprescription and send it to my pharmacy? \nThank you! \nRefill for Flexeril sent to your pharmacy. If back \npain is severe, not improving, or associated with \nnew leg weakness please let us know. When \ntaking Flexeril, avoid taking it while driving. It can \nmake you very drowsy. [HCW] \nMedication side \neffect \nHello doctor. I've had a nonproductive dry \ncough for about 3 weeks. I've tried cough \nsyrup and cough drops, but nothing \nseems to help and it's keeping me up at \nnight. My sister mentioned she had \nsomething similar happen with a dry \nhacking cough when she took lisinopril, \nand her doctor said it was a side effect. I \nnoticed the cough and the tickle in my \nthroat after we last increased the dose of \nlisinopril. Could I be having a side effect \ntoo? Thanks, [PATIENT] \n[PATIENT], [HCW] reviewed your message and \nwould like you to stop the Lisinopril, she sent in \nLosartan 50mg to take daily. The cough should \nimprove over 2 weeks. Let us know if you have \nfurther questions/concerns. Thank you \n \n \n \n \n \n \n \n \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nTable 1. Continued. \nCategory Rephrased patient message Actual provider response \nInformation-\nseeking about \nillness \nGood afternoon. I recently had some \ngenetic testing performed, since I am \ntrying to conceive with my partner. My \nresults showed that I'm a carrier for \nhemochromatosis. The fertility clinic \nrecommended I reach out to you about \nthese results. Is there anything I need to \ndo? Thanks in advance, [Patient] \nHi, [PATIENT]! Thanks for letting me know! \nFortunately, your most recent liver labs look \ngood. Hemochromatosis is a disease where you \nabsorb too much iron due to a genetic defect and \nthe iron gets stored in your organs. We can \nmonitor it over time. Sometimes, people are \ntreated later in life with intermittent phlebotomy \n(removing blood to take away excess iron). I will \nsend labs to check your iron levels and see how \nthings are doing for now. Please run by the lab at \nyour convenience, and I will follow up! Here is a \nnice, reputable summary of HH: cdc.gov/ \ngenomics/disease/hemochromatosis.htm#  \n:~:text=Hereditary%20hemochromatosis%20is \n%20a%20genetic, about%20testing%20for%20 \nhereditary%20hemochromatosis. We can also \ntalk at our next clinic visit in more detail. [HCW] \nQuestion \nregarding upper \nrespiratory tract \ninfection \nDear Doctor [HCW], I had two weeks of a \nbad cold. Never had a fever, and I tested \nnegative for covid, but my cough won't go \naway even a couple weeks later, and my \nenergy level isn't great. I'm having to take \nan allergy pill every day just to go to sleep. \nDo I need a flu test, or an allergy test? \nOk, Dr. [HCW] asks do you have other symptoms \nor is it just a lingering cough? That's a typical \npattern after a respiratory infection because the \nairways are still irritated. The cough can linger for \nseveral weeks. No testing needed right now. \nWould you like us to send you in some tessalon \nperles to help your cough? If so, which pharmacy \ndo you prefer? Thank you,[HCW] \nSymptoms \nrequiring \nreferral to \nspecialist \nI'm currently pregnant and have been \nhaving an issue with passing bright red \nblood with my bowel movements over the \npast year. At first it was infrequent, but for \nthe past week has been almost daily. \nEvery time I pass stool, there's bright red \nblood, as well as some abdominal pain \nwhich goes away after the BM. I'm \nguessing they may not do colonoscopies \nduring pregnancy, but I was thinking I \nshould get this checked out. Please let me \nknow any advice you have? Thanks, \n[PATIENT] \nHi [PATIENT]. Given your symptoms, I would \nabsolutely recommend a check in with GI. I am \nnot sure whether they would proceed with \ncolonoscopy, but it is worth a discussion with the \nprovider. I would be happy to initiate a referral for \nyou--would you like for me to pull that trigger? \nThank you for reaching out, Dr. [HCW] \nMessage \nrequiring follow \nup questions \nHello, I wanted to let you know I tested \npositive for covid today. I'm having a \ncough, dry throat, feeling tired, and a small \nheadache. No fevers or aches. I'm up-to-\ndate with my covid vaccine so I'm hoping \nthings won't get worse. I'm trying to rest, \ndoing some nasal rinses, and steam. \nAnything else specific you'd recommend \nme to do to treat? Thanks. [PATIENT] \nHi [PATIENT], When did your symptoms start? \nThat will determine if you are eligible for the \nantiviral, Paxlovid. Best, [HCW] \nClinical update \nDear Dr. [HCW], I wanted to let you know \nthat my mother was admitted to \n[HOSPITAL] on [DATE] for overnight \nobservation, due to having a fast heart \nrate. She was started on a number of \nmedications (amiodarone, Eliquis, \nmetoprolol), and they recommended she \nfollow up with you in a week. I'll follow up \nand call to schedule an appointment. Good \nnews is she's feeling better now and her \nheart rate is better (in the 60s). Thank you. \nGood morning, I am sorry to hear that this \nhappened, but am glad to hear she is back home. \nCould you bring her in on [DATE]? [HCW] \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nWe used the reph rased patient messages as input to evaluate our two fine -tuned models using a web \ninterface developed with Gradio.[27] For comparison, w e also used ChatGPT 3.5 and ChatGPT 4 to \ngenerate corresponding responses to the ten rephrased patient messages.  \n \nPrimary Care Physicians Review of Responses \nFor each patient message in the evaluation dataset, we randomized the order of 7 responses for review by \nprimary care physicians: 3 from CLAIR-Short, 1 from CLAIR-Long, 1 from ChatGPT 3.5, 1 from ChatGPT \n4, and 1 rephrased actual provider’s response. Participants rated each response in a survey using a 5-point \nLikert scale (1 —strongly disagree, 5 —strongly agree) from 4 perspectives: (1)  Empathy: The answer \nexpresses appropriate empathy given the question. (2) Responsiveness: The answer is responsive to the \npatient's question. (3)  Accuracy: The answer is clinically accurate, and soundly answers the patient's \nquestion. (4) Usefulness: I can use it as a template to write my response to this question.  Participants \ncould also provide free-text comments for each response. Participants completed the survey using REDCap \nand were blinded to which model generated a given response.[28] \n \nEvaluation \nTo automatically evaluate the generated responses, we calculated BERTScore,[29] a widely used metric \nfor evaluating generated text exhibits excellent correlation with human judgment at both sentence-level and \nsystem-level evaluations. We also computed precision, recall, and F1 scores based on BERTScore.  For \nexpert ratings, we calculated the frequencies and median and performed a Kruskal-Wallis test to compare \nthe ratings of generated responses from different methods. To evaluate interrater reli ability, we reported \nthe intraclass correlation coefficient (ICC) and 95% confidence intervals (CIs) using a 2-way mixed-effects \nmodel.[30] The statistical analysis was performed using Python3.6.  \n \nRESULTS \nWe collected  499,794 pairs of patient messages and corresponding provider responses, including \ninteractions from 98,808 unique patients and 2,974 providers. After the removal of duplicate entries and \nde-identification of the data, we ended up with a final training dat aset consisting of 499,286 message-\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nresponse pairs. The median length was 210 characters  for patient messages  and 162 characters for \nprovider responses. From the open-source dataset, median length for patient questions was 363 characters \nand 562 characters for provider responses . Updating the responses using the OpenAI API (Turbo -3.5), \nincreased the length to a median 1243 characters . Figure 2 provides an example of these updated \nresponses. \n \nResults of Physician Review of Responses \nFour primary care physicians participated in the survey with an ICC of 0.68 [0.61, 0.74], indicating moderate \nreliability. We used median values of three CLAIR-Short responses as the final ratings for the CLAIR-Short \nmodel. Figure 3 displays stacked bar charts for each. Participant evaluation of ChatGPT3.5 and ChatGPT4 \nresponses had median values leaning towards agreement in terms of empathy, responsiveness, accuracy, \nand usefulness, while  evaluation of  CLAIR-Long responses indicated agreement in empathy, \nresponsiveness, and accuracy, but neutrality in usefulness. On the other hand, evaluation of actual provider \nresponses and CLAIR-Short responses leaned towards disagreement in usefulness, neutrality in empathy \nand accuracy, and agreement in responsiveness. Pairwise comparisons of CLAIR-Long responses versus \nother responses revealed that CLAIR-Long responses were rated significantly higher than CLAIR-Short \nresponses in terms of empathy (P<0.001), accuracy (P <0.001), and usefulness (P <0.001). CLAIR-Long \nresponses were rated significantly lower than ChatGPT responses in responsiveness (P=0.005, P= 0.001). \nHowever, no statistically significant differences were observed between CLAIR-Long responses and \nChatGPT3.5 or ChatGPT4 responses  in terms of empathy, accuracy, and usefulness . Pairwise \ncomparisons between other responses were notable for no statistical significance between provider’s \nresponses and CLAIR-Short responses as well as ratings for most evaluation items between ChatGPT4 \nand ChatGPT3.5 responses were similar. Finally, we sought to rank the performance of each response by \nsumming the medians of the four survey rating dimensions. Including instances of tied rankings, ChatGPT4 \nresponses achieved the highest ranking in 6 questions, CLAIR -Long responses in 4 questi ons, and \nChatGPT3 in 3 questions.  Among 20 top -rated responses, 7 were from ChatGPT4, 5 were from \nChatGPT3.5, 4 were from CLAIR -Long, 3 from CLAIR -Short, and only 1 from the actual doctor.  Table 2 \ndisplays two highest-rated generated responses from each me thod. Medians and interquartile ranges of \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nthe survey items, detailed p values for pairwise comparisons, and median  values for each response and \nrelated ranks can be found in Appendix 2. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nTable 2. Top-rated generated responses (selected 2 responses from each generation methods). The complete set of generated responses, along \nwith corresponding patient information and provider responses, can be found in Appendix 3.  \nQuestion and Generated Response  \nQuestion: Hello Dr. [HCW]! I think I might have a bladder infection (urinary frequency, dysuria, urgency and lower \nback pain.) I've been taking AZO the past few days. What would you recommend? I appreciate it! \n \nChatGPT4 (rank 1/70):  \n \nDear [Patient's Name], \n \nThank you for reaching out and letting me know about the symptoms you've been experiencing. I'm sorry to hear \nthat you're not feeling well. \n \nBased on the symptoms you've described - urinary frequency, dysuria (painful or difficult urination), urgency, and \nlower back pain - it does indeed sound like you might have a urinary tract infection (UTI), which can involve the \nbladder (a condition specifically known as cystitis). These symptoms can be quite uncomfortable, and I appreciate \nyour proactive efforts in trying to manage them with AZO.  \n \nAZO, or Phenazopyridine, is a medication often used to alleviate symptoms of a UTI, including pain and \ndiscomfort. It's important to know, though, that while AZO can help with symptom relief, it does not treat the \nunderlying infection. \n \nAs a next step, I would recommend coming in for an appointment as soon as possible so we can properly \ndiagnose your condition. UTIs are typically diagnosed with a urine test, which can confirm the presence of \nbacteria causing the infection. If confirmed, we usually treat UTIs with a course of antibiotics. Please be sure to \ndrink plenty of water in the meantime to help flush out your system.  \n \nFurthermore, if you are experiencing severe pain, high fever, chills, nausea, vomiting or if your symptoms persist \nafter treatment, please seek immediate medical attention as these can be signs of a more serious condition like a \nkidney infection. \n \nIt's always important to fully understand your health conditions and the medications you're taking, so I encourage \nyou to ask any questions you might have during your appointment. We're here to help you feel better and ensure \nyou receive the best possible care. \n \nLooking forward to seeing you soon, \n \nDr. [Your Name] \n \nActual Provider Response (rank 55/70): \nHello, We advise going to the urgent care \nclinic or walk in clinic, to have urine tested \nand to identify which bacteria is growing \nand prescribe the right antibiotic. [HCW] \n \n \n \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nTable 2. Continued. \nQuestion and Generated Response  \nQuestion: I could really use a sleep aid. Recently I've been having a night or two, sometimes 4, where I just can't \nsleep. I'm feeling desperate due to lack of sleep and really need something to help me get through this. Is there \nsomething you'd recommend? Thanks. \n \nChatGPT3.5 (Rank: 1/70) \n \nDear [Patient], \n \nThank you for reaching out to me about your recent sleep troubles. I am sorry to hear that you have been \nexperiencing difficulty sleeping and I understand how distressing this can be. I will do my best to provide you with \nhelpful guidance and support. \n \nThere are various reasons why someone may have trouble sleeping, such as stress, anxiety, depression, or \nphysical conditions. Based on what you've shared, it appears that your sleep issues have been ongoing and \ncausing you distress. \n \nBefore considering any sleep aids, I would recommend making some lifestyle changes to improve your sleep \nhygiene. This includes avoiding caffeine, alcohol, and nicotine before bedtime, establishing a regular sleep \nschedule, and creating a relaxing bedtime routine. Additionally, make sure your sleeping environment is \nconducive to sleep, which means keeping it cool, dark, and quiet. \n \nIf these changes do not provide relief, there are non-medical options available, such as cognitive-behavioral \ntherapy for insomnia (CBT-I), which can be effective for treating chronic insomnia. CBT-I focuses on changing \nnegative thought patterns and behaviors that can contribute to insomnia. \n \nIf lifestyle changes and non-medical options are not successful, there are prescription sleep aids available. \nHowever, it's important to note that sleep aids should be used as a last resort and only under the guidance of a \nhealthcare professional, due to the potential for side effects and dependency. \n \nI recommend making an appointment to come in and discuss your options further. During our appointment, we \ncan discuss your medical history and any potential underlying causes for your sleep issues. Together, we can \ncome up with a treatment plan that is tailored to your specific needs and concerns. \n \nIn the meantime, I encourage you to try some of the lifestyle changes I mentioned and to practice relaxation \ntechniques, such as deep breathing and meditation, to help you fall asleep. \n \nPlease don't hesitate to contact me if you have any further questions or concerns. My goal is to help you get the \nbest possible care and support, and I am here to help you through this. \n \nBest regards, \n[Your Name] \nActual Provider Response (Rank: 62/70) \nI would suggest to try melatonin 6 to 9 mg \nat bedtime to see if that would help you \nwith your sleep. Thank you Dr. [HCW] \n \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nTable 2. Continued. \nQuestion and Generated Response  \nQuestion: Hello Dr. [HCW]! I think I might have a bladder infection (urinary frequency, dysuria, urgency and lower \nback pain.) I've been taking AZO the past few days. What would you recommend? I appreciate it! \n \nCLAIR-Short (Rank: 3/70) \n \nI'm sorry you are not feeling well. Given your symptoms, I would recommend a urinalysis to determine if you have \na bladder infection. I have placed an order for this, and you can go to any [HOSPITAL] lab or [HOSPITAL] to \nleave a urine sample. If the urinalysis is positive for a bladder infection, I can send you in an antibiotic. Please let \nme know if you have any questions. [HCW] \n \nActual Provider Response (Rank: 55/70) \nHello, We advise going to the urgent care \nclinic or walk in clinic, to have urine tested \nand to identify which bacteria is growing \nand prescribe the right antibiotic. [HCW] \n \nQuestion: Hi Doctor [HCW]. I did something to my back this week and I'm having back spasms again. This \nhappens once in a while. Last time, which was a few months ago, I was prescribed cyclobenzaprine 5 mg tablet \n(FLEXERIL). This really helped me. Can you please renew this prescription and send it to my pharmacy? Thank \nyou! \n \nChatGPT3.5 (Rank: 3/70) \nDear [Patient], \n \nThank you for reaching out to me regarding your back spasms. I am sorry to hear that you are experiencing \ndiscomfort again. I understand how frustrating it can be to deal with chronic pain. \n \nBased on your previous experience with cyclobenzaprine 5 mg tablet (FLEXERIL), it sounds like this medication \nhas been helpful for you in managing your back spasms. However, before renewing your prescription, I would like \nto schedule an appointment with you to discuss your symptoms and review your medical history. \n \nDuring this appointment, we can also discuss alternative treatment options and any potential side effects or \ninteractions with other medications you may be taking. It's important to ensure that the medication is still the best \ncourse of action for your condition, and that there are no changes to your overall health that may impact its \neffectiveness or safety. \n \nPlease let me know your availability, and I will be happy to schedule an appointment for you at the earliest \nconvenience. In the meantime, I recommend that you try applying heat or ice to the affected area, and gentle \nstretches to help alleviate your discomfort. \n \nThank you for trusting me with your care. I look forward to seeing you soon. \n \nBest regards, \n[HCW] \nActual Provider Response (Rank: 51/70) \nRefill for flexeril sent to your pharmacy. If \nback pain is severe, not improving, or \nassociated with new leg weakness please \nlet us know. When taking flexeril, avoid \ntaking it while driving. It can make you very \ndrowsy. [HCW] \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nTable 2. Continued. \nQuestion and Generated Response  \nQuestion: Good Afternoon. I recently had some genetic testing performed, since I am trying to conceive with my \npartner. My results showed that I'm a carrier for hemochromatosis. The fertility clinic recommended I reach out to \nyou about these results. Is there anything I need to do? Thanks in advance, [Patient] \n \n \nChatGPT4 (Rank: 3/70) \n \nDear [Patient], \n \nThank you for reaching out and for being proactive about your health, especially in regard to your plans of \nconceiving a child. Genetic testing is a valuable tool that can give us insight into potential health risks, and in your \ncase, it has identified you as a carrier for hemochromatosis.  \n \nHemochromatosis is a genetic condition that causes the body to absorb too much iron from the diet. This excess \niron is then stored in the body's organs, particularly the liver, heart, and pancreas, which can lead to life-\nthreatening conditions such as liver disease, heart problems, and diabetes. \n \nAs a carrier, you have one copy of the gene mutation but not the disease itself. This generally means you are \nunlikely to develop hemochromatosis or show symptoms. However, the implications of this result may be different \nfor your future child, depending on whether your partner is also a carrier. \n \nIf both parents are carriers, there is a 25% chance with each pregnancy that the child could inherit two copies of \nthe gene mutation, one from each parent, and therefore develop the disease. Alternatively, there is a 50% chance \nthe child could be a carrier like you, and a 25% chance the child will neither be a carrier nor have the disease. \n \nTo better understand this risk, it would be beneficial for your partner to also have genetic testing performed. If \nyour partner is found to be a carrier, you may wish to consult with a genetic counselor who can provide more \ndetailed information and discuss possible options with you. \n \nIn terms of your personal health, being a carrier for hemochromatosis doesn't usually require specific \nmanagement, but routine check-ups and maintaining a healthy lifestyle are always recommended. However, in \nthe context of your fertility journey, it would be wise to avoid iron supplements unless specifically recommended \nby your healthcare provider. \n \nThank you again for your proactive approach to your health. Please let me know if your partner decides to \nundergo genetic testing or if you have any further questions or concerns. Remember, I'm here to support you in \nyour journey to parenthood and overall well-being. \n \nBest regards, \n \n[Your Name] \nActual Provider Response (Rank: 7/70) \nHi, [PATIENT]! Thanks for letting me know! \nFortunately, your most recent liver labs \nlook good. Hemochromatosis is a disease \nwhere you absorb too much iron due to a \ngenetic defect and the iron gets stored in \nyour organs. We can monitor it over time. \nSometimes, people are treated later in life \nwith intermittent phlebotomy (removing \nblood to take away excess iron). I will send \nlabs to check your iron levels and see how \nthings are doing for now. Please run by the \nlab at your convenience, and I will follow \nup! Here is a nice, reputable summary of \nHH:cdc.gov/genomics/disease/hemochrom\natosis.htm#:~:text=Hereditary%20hemochr\nomatosis%20is%20a%20genetic,about%20\ntesting%20for%20hereditary%20hemochro\nmatosis. We can also talk at our next clinic \nvisit in more detail. [HCW] \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nTable 2. Continued. \nQuestion and Generated Response  \nQuestion: Dear Dr. [HCW], I wanted to let you know that my mother was admitted to [HOSPITAL] on [DATE] for \novernight observation, due to having a fast heart rate. She was started on a number of medications (amiodarone, \nEliquis, metoprolol), and they recommended she follow up with you in a week. I'll follow up and call to schedule \nan appointment. Good news is she's feeling better now and her heart rate is better (in the 60s). Thank you. \n \nCLAIR-Long (Rank: 7/70) \n \nDear Patient, \n \nThank you for reaching out regarding your mother's recent hospitalization. I am glad to hear that she is doing \nbetter and that her symptoms have improved since being discharged. I do recommend scheduling a follow up \nappointment with me as soon as possible so we can ensure that her recovery continues to progress. Please feel \nfree to call our office at any time if you wish to speak with one of our staff members about making an \nappointment. \nActual Provider Response (Rank: 25/70) \nGood morning, I am sorry to hear that this \nhappened, but am glad to hear she is back \nhome. Could you bring her in on [DATE]? \n[HCW] \n \nQuestion: I could really use a sleep aid. Recently I've been having a night or two, sometimes 4, where I just can't \nsleep. I'm feeling desperate due to lack of sleep and really need something to help me get through this. Is there \nsomething you'd recommend? Thanks. \n \nCLAIR-Short (Rank: 7/70) \n \nI am sorry to hear that you are not sleeping well. It is important to make sure that you are doing things to help \nyour sleep. Do you have a bedtime routine? Do you limit screen time before bed? Do you have a good sleep \nenvironment? Do you try to avoid caffeine after 2 pm? Do you make sure to get regular exercise? I would \nrecommend trying some over the counter melatonin 3 - 5 mg to help you sleep. If this is not helping, we can \ndiscuss other options. [HCW] \nActual Provider Response (Rank: 62/70) \nI would suggest to try melatonin 6 to 9 mg \nat bedtime to see if that would help you \nwith your sleep. Thank you Dr. [HCW] \n \n \nQuestion: Dear Doctor [HCW], I had two weeks of a bad cold. Never had a fever, and I tested negative for covid, \nbut my cough won't go away even a couple weeks later, and my energy level isn't great. I'm having to take an \nallergy pill every day just to go to sleep. Do I need a flu test, or an allergy test? \n \nCLAIR-Long (Rank: 7/70) \nDear Patient, \n \nThank you for reaching out to me regarding your lingering cough after having a bad cold. I understand how \nfrustrating it can be when you think you are over an illness only to find yourself still feeling unwell. It sounds like \nyou may have developed a postinfectious cough that has been persisting since your cold and causing you to feel \nfatigued. This type of cough can last up to eight weeks after the initial upper respiratory tract infection. There are \nseveral treatment options available to help reduce the duration and severity of symptoms associated with this \ntype of cough. \nActual Provider Response (Rank: 33/70) \nOk, Dr. [HCW] asks do you have other \nsymptoms or is it just a lingering cough? \nThat's a typical pattern after a respiratory \ninfection because the airways are still \nirritated. The cough can linger for several \nweeks. No testing needed right now. Would \nyou like us to send you in some tessalon \nperles to help your cough? If so, which \npharmacy do you prefer? Thank you, \n[HCW] \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nTable 2. Continued. \nQuestion and Generated Response  \nI would recommend using a humidifier at night to help loosen mucus in your airways, which will allow you to \ncough it up more easily. If you do not already own one, you can purchase one online or at any major pharmacy. \nAnother option is to use a saline nasal spray once daily to help thin mucus in your nose and sinuses. You can \nalso try drinking warm liquids such as tea with honey to help soothe your throat and loosen congestion. Gargling \nwith warm salt water a few times per day can also help alleviate some of your symptoms. Over-the-counter \nmedications such as Mucinex can be used to thin secretions in your lungs to make them easier to expel through \ncoughing. As always, please consult with your pharmacist before taking any new medication to ensure there are \nno interactions with other medications you are currently taking. \n \nI hope this information helps you manage your symptoms. Please let me know if you have any questions or \nconcerns. \n \nBest, \n[HCW] \n \n \n \n \n \n \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nCLAIR-Short Generated Responses \nThe responses generated by CLAIR-Short had a median length of 200 characters. The ratings of these \nresponses varied across different questions. When comparing the best generated responses from three \nrounds in each question with the actual provider responses, CLAIR-Short outperformed the providers in all \nquestions except for Q6 (hemochromatosis question), where the provider responses received higher ratings. \nFor instance, in the first scenario (Q1), where the patient had been taking AZO for several days without \nrelief and suspected a bladder infection, the healthcare provider recommended visiting an urgent care or \nwalk-in clinic for a urinalysis and a suitable antibiotic prescription. The CLAIR-Short generated response \nalso suggested a urinalysis  but expressed empathy. Moreover, it mentioned that a lab order had been \nplaced, provided information on where the patient should go to provide the sample, and outlined the \nsubsequent steps (antibiotic prescription) if the test came back positive. Reviewers also suggested that \nmentioning AZO could further enhance this generated response.  In the second scenario (Q2), which \ninvolved a patient experiencing sleep difficulties for four consecutive days and seeking assistance, the \nphysician suggested trying melatonin at a dose of 6 to 9 mg.  The CLAIR-Short generated response \ndisplayed empathy, inquired about the patient's sleep problems in detail, and recommended a different \ndose of melatonin: 3 to 5 mg. Reviewers favored the generated response and suggested that it could be \nfurther improved by discussing sleep hygiene more comprehensively. One reviewer noted a preference for \ndiscussing sleep aids with the patient before prescribing and expresse d concern about the high dose \nmentioned in the physician's response. Another similar scenario was presented in Q7, where a patient had \na lingering cough after a cold and inquired about flu or allergy testing. The provider response requested \nadditional information about the symptoms and offered Tessalon Perles as a cough treatment. The CLAIR-\nShort response included a series of follow -up questions (e.g., regarding over -the-counter cough \nmedications, Mucinex usage, shortness of breath or chest pain, dizziness or weakness). The reviewer s \nconsidered this response to be reasonable and valuable, suggesting that it could be sent to patients as an \nautomated preliminary request for addition information before message was actually received by a provider \nor care team.  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nThe average BERTScore metrics for the generated responses from CLAIR-Short, in comparison to the \nactual provider’s responses, were as follows: precision of 0.87±0.02, recall of 0.84±0.03, and F1 score of \n0.85±0.02. The boxplot is in Figure 4.  \n   \nCLAIR-Long, ChatGPT3.5, and ChatGPT4 Generated Responses \nThe median length of responses generated from CLAIR-Long, ChatGPT3.5, and ChatGPT4 were 1593, \n1591, and 2025 characters, respectively . In Q1, all generated responses advised patients to seek \nimmediate medical attention and explained why their previous medication, AZO, was not sufficient for \ntreatment. The responses from ChatGPT3.5 emphasized the importance of urine testing for diagnosis and \nthe consideration of antibiotics based on the test results. Additionally, ChatGPT4 responses mentioned \nsymptoms of kidney infection, urging patients to watch out for them. On the other hand, CLAIR -Long \nsuggested evaluation at a walk -in clinic and provided a link to relevant information about urinary tract \ninfections (UTIs). The reviewers noted that this question might require more information, such as whether \nthe patient is pregnant. They also mentioned that UTIs involving only the bladder don't necessarily require \nan appointment and can be addressed through the patient portal, while the patient's back pain could be a \nsymptom of a kidney infection. Another point raised was that the responses generated by ChatGPT were \nconsidered too lengthy and required a relatively high reading level. One reviewer believed that the CLAIR-\nLong response was the best response, while another reviewer felt it was more suitable as a nurse-directed \nprotocol. In Q2 (sleep aid request), CLAIR-Long generated responses asked specific questions to gather \nmore information about the patients' symptoms, triggers, and past experiences . One reviewer noted that \nthis response assumed that insomnia is solely caused by stress. Another reviewer mentioned that it may \ncontain an excessive amount of empathy. On the other hand, the ChatGPT3.5 response received feedback \nas being highly accurate  with a suggestion to make it more concise . The ChatGPT4 response r eceived \nfeedback suggesting that it could serve as a good template after incorporating low -risk medications and \nmaking it more concise. Generated responses are listed in Appendix 3. \n \nUsing the actual healthcare provider responses as the reference dataset, the BERTScore values for CLAIR-\nLong generated responses were: Precision: 0.82±0.02, Recall: 0.84±0.01, F1: 0.83±0.01. The BERTScore \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nvalues of ChatGPT3.5 and ChatGPT4 generated responses compared with the CLAIR-Long generated \nresponse were Precision: 0.88±0.01, Recall: 0.86±0.01, F1: 0.87±0.01, and Precision: 0.87±0.01, Recall: \n0.85±0.01, F1: 0.86±0.01, respectively. The boxplot is shown in Figure 5.  \n \nDISCUSSION \nIn this study, we utilized GPT4 instruction data to train LLaMA-65B and developed two models for \nresponding to patient messages . The first model,  CLAIR-Short, was developed using patient messages \nwith responses from primary care providers at VUMC . The second model, CLAIR -Long was augmented \nwith an open-source dataset and OpenAI GPT3.5. We mixed generated responses from CLAIR-Short and \nCLAIR-Long with actual provider responses as well as  responses from non-specialized large language \nmodels - ChatGPT3.5 and ChatGPT4. Primary care physicians evaluate d these responses in terms of \nempathy, responsiveness, accuracy, and usefulness. The results indicated that  responses generated by \nChatGPT models achieved highest ratings, followed by responses generated by CLAIR-Long, both of which \noutperformed CLAIR-Short and the doctor’s responses significantly. In addition, we provided a set of typical \npatient messages and provider responses for future evaluation of response generation models in the patient \nportal. \n \nBenefits of Fine-Tuning \nAlthough ChatGPT-generated responses received highest ratings on average, fine-tuning large language \nmodels for patient responses offers several benefits.  Firstly, the fine -tuned model generates concise \nresponses with a distinctive voice similar to local doctors. For example, CLAIR-Short-generated responses \nwere rated as more typical of primary care physicians as compared to  ChatGPT-generated responses \nwhich experts described as robot-like. Training AI generated responses to match the syntax and tone of \nphysician authored messages may be critical to enhance both physician acceptance and patient satisfaction \nwere such tools applied in practice . Secondly, only hospitals collaborating with Epic and Microsoft Azure \nhave the possibility to use large language models from Open AI with PHI, such as patient messages, in a \nHIPAA compliant way . Fine-tuning publicly available large language models , such as LLaMA-65B, fine-\ntuned on local datasets could empower any researcher within a ny healthcare organization to do work in \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nthis area, regardless of external partnerships . Compared with CLAIR-Short’s performance limited by the \nquality of local data, our CLAIR-Long generated responses improved significantly by using an open-source \ndataset augmented with OpenAI GPT3.5. Experts generally expressed positive views on the \nresponsiveness, empathy, and accuracy of CLAIR-Long responses, while maintaining a neutral stance on \nusefulness. Therefore, combining the local patient messages dataset with an augmented open-source \ndataset allowed effective fine-tuning of the large language model, generating responses that reflect local \nprovider practice preferences while incorporating comprehensive information, empath y, and relevant \npatient education. \n \nChatGPT is able to Generate Useful Draft Messages without Training on Local Data \nThe responses generated by ChatGPT received higher ratings compared to our fine-tuned models, which \ncould be attributed to the superior performance of ChatGPT over the open -source large language model \nLLaMA. Moreover, the performance of the fine-tuned large language models depends heavily on the quality \nof the training dataset rather than its size. [31] In this study , the ratings for responses generated by our \nCLAIR-Short, which was fine-tuned solely on local data, were not significantly different from the ratings of \nthe original physician responses  across all items: empathy, responsiveness, accuracy, and usefulness . \nTherefore, future studies about using large language models in replying to patient messages can focus on \nprompt engineering, integrating large language models with EHR data and clinical knowledge dataset, \nhelping patients draft messages, and performing patient portal tasks.  \n \nPrompt Engineering \nPrompt engineering should highlight taking the role of a primary care doctor, providing helpful guidance and \npatient education, and using empathy. Physician reviewers responded favorably to drafted messages that \nwere empathetic and included patient educati on. Writing thorough, empathetic responses that include \npatient education may be beneficial for the patient but is also time-consuming, revealing a key opportunity \nfor AI to augment clinical work. \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nClinical Context and Existing Patient-Physician Relationship \nFurther work is needed to incorporate patient history (e.g., medication history , diagnosis), historical \nconversations, and local care delivery practice preferences into prompts. During the evaluation, reviewers \nnoted that some provider responses are based on having an established patient-provider relationship. For \ninstance, a primary care provider may not refill Flexeril for a patient over messages alone unless they have \nan existi ng agreement and previous expectations set for short term use.  In addition, using context \ninformation, we could further refine generated responses based on user types, care protocols, and patient \neducation levels. Another finding was that some of the generated responses related to drug prescriptions \ndid not explicitly mention specific drug names. Upon reviewing the database, we found that this \ncommunication pattern of excluding specific drug names matched with the responses from physicians, likely \nbecause the Epic EHR system had automatically generated a message to the patient earlier in the \nconversation which provided detailed prescription information. Therefore, when collecting training data, the \nprescription messages automatically generated by the system could also be collected to help improve the \naccuracy and completeness of the generated responses, especially when specific drug information is \nneeded. \n \nClinical Knowledge \nTraining, either on local datasets, or on older data may perpetuate use of out -of-date clinical guidelines. \nFor example, in Q3 about medication request of antibiotics for traveler's diarrhea,  while the Centers for \nDisease Control and Prevention (CDC) Yellow Book 2024 recommends azithromycin as an alternative to \nfluoroquinolones, one of the generated responses still opted for ciprofloxacin. After reviewing the dataset, \nwe found several reasons leading to t his discrepancy, including provider recommending a non guideline-\nbased antibiotic or patients explicitly requesting a specific drug based on their  previous prescriptions or \nallergy to azithromycin. Another example is the Q9 regarding COVID-19 treatment. The doctor’s responses \nreferred to the antiviral medication Paxlovid, which has been available from December 2021. However, \nresponses from ChatGPT did not mention this treatment option. It might because ChatGPT only contains \ninformation from September 2021 and before.  Large language models learn text patterns from the training \ndata, which means they predict the next word based on the provided context. Therefore, if clinical guidelines \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nchange, the large language model will not update until it is retrained and, in that case, only if enough of the \ntraining text it uses reflects the new guideline. To address this, it is crucial to incorporate updated clinical \nguidelines into AI models by either updating the model's knowledge, or integrating rule-based systems, or \nusing semantic search to link with up-to-date clinical knowledge. \n \nMessage Response Styles and Practice Patterns \nProviders and care delivery systems may have different approaches, protocols, or standards of care when \nresponding to patient messages. For example, some may attempt to diagnose and give complete treatment \nplans through patient portal message conversations while others prefer to have patients schedule in-person \nvisits. Consequently, this led to different perspectives among the reviewers assessing the generated \nresponses, and means that the definition of an ideal response is appropriately variable and organization - \nor provider -specific. Future tools may incorporate provider preferences into prompts, e.g., generally \nencouraging patients to come into clinic if treatment decisions need to be made. \n \nQuestion Generation and Chat Capability \nLarge language models may be useful for automatically generating questions to gather additional details \nfrom a patient before providing a manual or automated response. For some patient messages, instead of \ndirectly answering questions, our models generated a series of information-seeking questions as a reply. \nFurther analysis of the training dataset revealed that , in clinical practice, healthcare providers often need \nto ask follow -up questions to gather the necessary details before communicating a finalized plan to the \npatient. An AI model can serve as a useful intermediary in message conversations by prompting patients \nwith clarifying questions as they compose their messages , leveraging known strength of large language \nmodels in chat-based infrastructures. This approach could help patients provide complete information with \ntheir initial message, streamlining the subsequent conversation and minimizing back-and-forth exchange. \nIn Figure 6, we present a prototype of an AI patient message editor as a potential integration within a patient \nportal interface. Future research could focus on using a similar chat-based conversation with a large \nlanguage model to quickly enhance their messages by engaging with the chatbot, ensuring clarity and \nconciseness before sending the information to the provider.  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nPatient Portal Tasks \nResponses to patient portal messages often include certain tasks, like ordering tests, writing prescriptions, \nor scheduling appointments. Many self-service tools already exist in patient portals, such as self-scheduling \nor refill requests, with which patie nts can have their needs met in a more streamlined way without an \nunstructured message conversation. Furthermore, many tasks requested via messages that require care \nteam attention can have components of the task automated, such as pending orders for medic ation \nrequests or drafting letters. Future work should focus on how to  use large language models to identify \npotential self-service redirection or automated task-completion assistance as part of the patient message \nresponse process. \n \nLimitations \nThis study has several limitations. First, the selection of patient messages  to evaluate our AI models  \nfocused on single events, which might not capture the full spectrum of messages in patient portals. In reality, \nsome patient messages require additional context, such as current medications or medical history, to \nprovide accurate responses. Second, the models developed in this study generated responses based on \nprevious responses stored at VUMC. Response content from a set of historical messages will not account \nfor updates in clinical guidelines or scientific advances which occurred after the data set was created (March \n7, 2023). Third, this study primarily focused on the technical feasibility of generating responses from AI \nmodels and evaluations from the physician perspective. The attitudes and preferences of patients towards \nthese generated responses remain unknown. Future research should include qualitative studies to explore \npatient preferences regarding AI-generated responses and investigate any workflow issues that may arise \nwhen integrating AI-generated responses into primary care providers' clinic work. \n \nCONCLUSION \nUsing an augmented open-source dataset can effectively improve the empathy, responsiveness, accuracy, \nand usefulness of responses generated by large language models fine-tuned using local data. Such open \nsource, locally-finetuned models can perform well in generating replies to patient message s, better than \nactual provider responses. Generalized models like ChatGPT also outperform actual provider responses \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nwithout fine-tuning on local data, as well as large language models fine-tuned with local patient message \ndata. Locally derived models still play an important role in enabling research and clinical practice when PHI-\ncompliant generalized large language models cannot be accessed.  Further work is needed toto increase \nthe u sefulness of AI -drafted replies to patient messages, includin g incorporating up-to-date clinical \nguidelines, incorporating patient history and care -delivery context into prompts, and integrating common \npatient portal tasks such as pending of orders and scheduling of appointments into responses. \n \nFUNDING STATEMENT \nThis work was supported by NIH grants: K99LM014097-01, R01AG062499-01, and R01LM013995-01. \nCOMPETING INTERESTS STATEMENT \nThe authors do not have conflicts of interest related to this study. \nCONTRIBUTORSHIP STATEMENT \nSL extracted data, developed models, and drafted the work. SL, APW, ABM, and AW designed the \nresearch. SL, APW, ABM, and AW developed the questionnaire. BC, JZG, SH, and JFP participated in \nthe questionnaire. BS provided suggestions in model development. All authors revised the draft and \napproved the submitted version.  \nDATA AVAILABILITY STATEMENT \nThe updated responses based on the 5k open-source patient physician communication are available \nbased on request.  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nREFERENCE \n1  Sorace J, Wong H-H, DeLeire T, et al. Quantifying the competitiveness of the electronic health \nrecord market and its implications for interoperability. Int J Med Inform 2020;136:104037. \ndoi:10.1016/j.ijmedinf.2019.104037 \n2  Tarver WL, Menser T, Hesse BW, et al. Growth Dynamics of Patient-Provider Internet \nCommunication: Trend Analysis Using the Health Information National Trends Survey (2003 to \n2013). J Med Internet Res 2018;20:e109. doi:10.2196/jmir.7851 \n3  Akbar F, Mark G, Warton EM, et al. Physicians’ electronic inbox work patterns and factors \nassociated with high inbox work duration. J Am Med Informatics Assoc 2021;28:923–30. \ndoi:10.1093/jamia/ocaa229 \n4  Arndt BG, Beasley JW, Watkinson MD, et al. Tethered to the EHR: Primary Care Physician \nWorkload Assessment Using EHR Event Log Data and Time-Motion Observations. Ann Fam Med \n2017;15:419–26. doi:10.1370/afm.2121 \n5  Steitz BD, Sulieman L, Wright A, et al. Association of Immediate Release of Test Results to \nPatients with Implications for Clinical Workflow. JAMA Netw Open 2021;4:e2129553–e2129553. \ndoi:10.1001/jamanetworkopen.2021.29553 \n6  Sinsky CA, Shanafelt TD, Ripp JA. The Electronic Health Record Inbox: Recommendations for \nRelief. J Gen Intern Med 2022;37:4002–3. doi:10.1007/s11606-022-07766-0 \n7  Holmgren AJ, Downing NL, Tang M, et al. Assessing the impact of the COVID-19 pandemic on \nclinician ambulatory electronic health record use. J Am Med Informatics Assoc 2022;29:453–60. \ndoi:10.1093/jamia/ocab268 \n8  Lieu TA, Altschuler A, Weiner JZ, et al. Primary Care Physicians’ Experiences With and Strategies \nfor Managing Electronic Messages. JAMA Netw Open 2019;2:e1918287. \ndoi:10.1001/jamanetworkopen.2019.18287 \n9  Adler-Milstein J, Zhao W, Willard-Grace R, et al. Electronic health records and burnout: Time \nspent on the electronic health record after hours and message volume associated with exhaustion \nbut not with cynicism among primary care clinicians. J Am Med Informatics Assoc 2020;27:531–8. \ndoi:10.1093/jamia/ocz220 \n10  Kasneci E, Sessler K, Küchemann S, et al. ChatGPT for good? On opportunities and challenges of \nlarge language models for education. Learn Individ Differ 2023;103:102274. \ndoi:10.1016/j.lindif.2023.102274 \n11  ChatGPT: Optimizing Language Models for Dialogue. https://openai.com/blog/chatgpt/ (accessed \n25 Dec 2022). \n12  Liu J, Wang C, Liu S. Utility of ChatGPT in Clinical Practice. J Med Internet Res 2023;25:e48568. \ndoi:10.2196/48568 \n13  Liu J, Liu S. The application of ChatGPT in nursing education. doi:10.35542/OSF.IO/2X7A8 \n14  Liu J, Liu S. The application of ChatGPT in medical education. doi:10.35542/OSF.IO/WZC2H \n15  Wang1 C, Liu3 S, Li4 A, et al. Text dialogue analysis Based ChatGPT for Primary Screening of \nMild Cognitive Impairment. medRxiv 2023;:2023.06.27.23291884. \ndoi:10.1101/2023.06.27.23291884 \n16  Kung TH, Cheatham M, Medenilla A, et al. Performance of ChatGPT on USMLE: Potential for AI-\nAssisted Medical Education Using Large Language Models. medRxiv \n2022;:2022.12.19.22283643. doi:10.1101/2022.12.19.22283643 \n17  Liu S, Wright AP, Patterson BL, et al. Using AI-generated suggestions from ChatGPT to optimize \nclinical decision support. J Am Med Informatics Assoc Published Online First: 22 April 2023. \ndoi:10.1093/JAMIA/OCAD072 \n18  Kumah-Crystal Y, Mankowitz S, Embi P, et al. ChatGPT and the clinical informatics board \nexamination: the end of unproctored maintenance of certification? J Am Med Informatics Assoc \n2023;:ocad104. doi:10.1093/jamia/ocad104 \n19  Ayers JW, Poliak A, Dredze M, et al. Comparing Physician and Artificial Intelligence Chatbot \nResponses to Patient Questions Posted to a Public Social Media Forum. JAMA Intern Med \nPublished Online First: 28 April 2023. doi:10.1001/JAMAINTERNMED.2023.1838 \n20  Min B, Ross H, Sulem E, et al. Recent Advances in Natural Language Processing via Large Pre-\nTrained Language Models: A Survey. Published Online First: 1 November \n2021.http://nlp.seas.harvard.edu/2018/04/ (accessed 6 May 2023). \n21  Li Y, Li Z, Zhang K, et al. ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nMedical Domain Knowledge. Published Online First: 24 March \n2023.https://arxiv.org/abs/2303.14070v4 (accessed 26 Apr 2023). \n22  Chambon PJ, Wu C, Steinkamp JM, et al. Automated deidentification of radiology reports \ncombining transformer and “hide in plain sight” rule-based methods. J Am Med Informatics Assoc \n2023;30:318–28. doi:10.1093/jamia/ocac219 \n23  Touvron H, Lavril T, Izacard G, et al. LLaMA: Open and Efficient Foundation Language Models. \nPublished Online First: 27 February 2023.https://arxiv.org/abs/2302.13971v1 (accessed 25 Apr \n2023). \n24  Hu EJ, Shen Y, Wallis P, et al. LoRA: Low-Rank Adaptation of Large Language Models. Published \nOnline First: 17 June 2021.https://github.com/microsoft/LoRA. (accessed 26 Apr 2023). \n25  Peng B, Li C, He P, et al. Instruction Tuning with GPT-4. Published Online First: 6 April \n2023.https://arxiv.org/abs/2304.03277v1 (accessed 26 Apr 2023). \n26  Heisey-Grove DM, DeShazo JP. Look Who’s Talking: Application of a Theory-Based Taxonomy to \nPatient–Clinician E-mail Messages. Telemed e-Health 2020;26:1345–52. \ndoi:10.1089/tmj.2019.0192 \n27  Abid A, Abdalla A, Abid A, et al. Gradio: Hassle-Free Sharing and Testing of ML Models in the \nWild. Published Online First: 6 June 2019.https://arxiv.org/abs/1906.02569v1 (accessed 16 May \n2023). \n28  Harris PA, Taylor R, Thielke R, et al. Research electronic data capture (REDCap)—A metadata-\ndriven methodology and workflow process for providing translational research informatics support. \nJ Biomed Inform 2009;42:377–81. doi:10.1016/j.jbi.2008.08.010 \n29  Zhang T, Kishore V, Wu F, et al. BERTScore: Evaluating Text Generation with BERT. Published \nOnline First: 21 April 2019.https://arxiv.org/abs/1904.09675v3 (accessed 9 May 2023). \n30  Koo TK, Li MY. A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for \nReliability Research. J Chiropr Med 2016;15:155–63. doi:10.1016/j.jcm.2016.02.012 \n31  Zhou C, Liu P, Xu P, et al. LIMA: Less Is More for Alignment.  \n \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nFigure 1. Overview of data collection , training process, and evaluation . The logos of CLAIR-Short and \nCLAIR-Long were generated by Midjourney.  \n \nFigure 2. An example of updated response using OpenAI API (Turbo-3.5). \nFigure 3. Stacked bar charts of the ratings of empathy, responsiveness, accuracy, and usefulness. \nFigure 4. The boxplot comparing BERTScore values of generated responses from CLAIR-Short to actual \nprovider responses.  \n \nFigure 5. Boxplot of BERTScore of the generated responses from CLAIR-Long compared with the \nresponses from actual providers, ChatGPT(GPT3.5), and ChatGPT(GPT4). \n \nFigure 6. A prototype of potential implementation in of an AI Patient Message Editor in a patient portal \ninterface. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 16, 2023. ; https://doi.org/10.1101/2023.07.14.23292669doi: medRxiv preprint ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6528111696243286
    },
    {
      "name": "Language model",
      "score": 0.43383505940437317
    },
    {
      "name": "Natural language processing",
      "score": 0.37765228748321533
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I901861585",
      "name": "Vanderbilt University Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}