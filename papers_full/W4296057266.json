{
  "title": "RPLHR-CT Dataset and Transformer Baseline for Volumetric Super-Resolution from CT Scans",
  "url": "https://openalex.org/W4296057266",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2742653759",
      "name": "Pengxin Yu",
      "affiliations": [
        "InferVision (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2154133041",
      "name": "Haoyue Zhang",
      "affiliations": [
        "Computational Diagnostics (United States)",
        "InferVision (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2107434326",
      "name": "Han Kang",
      "affiliations": [
        "InferVision (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2101050108",
      "name": "Wen Tang",
      "affiliations": [
        "InferVision (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2140006220",
      "name": "Corey W. Arnold",
      "affiliations": [
        "Computational Diagnostics (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2107842282",
      "name": "Rongguo Zhang",
      "affiliations": [
        "InferVision (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2742653759",
      "name": "Pengxin Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154133041",
      "name": "Haoyue Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107434326",
      "name": "Han Kang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101050108",
      "name": "Wen Tang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2140006220",
      "name": "Corey W. Arnold",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107842282",
      "name": "Rongguo Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3092678201",
    "https://openalex.org/W2964297772",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2980002832",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3014974815",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W2164392783",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W3209967316",
    "https://openalex.org/W3089508953",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3204148491",
    "https://openalex.org/W3129053565",
    "https://openalex.org/W3035505623",
    "https://openalex.org/W3201158390",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W3013529009",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W3186860278",
    "https://openalex.org/W3170879743",
    "https://openalex.org/W3022845895",
    "https://openalex.org/W3101204238",
    "https://openalex.org/W3074741277",
    "https://openalex.org/W6600277285",
    "https://openalex.org/W3134475970",
    "https://openalex.org/W2915647411",
    "https://openalex.org/W3112701542",
    "https://openalex.org/W4305011991",
    "https://openalex.org/W3038857985",
    "https://openalex.org/W3099683629"
  ],
  "abstract": null,
  "full_text": "arXiv:2206.06253v1  [eess.IV]  13 Jun 2022\nRPLHR-CT Dataset and Transformer Baseline\nfor Volumetric Super-Resolution from CT Scans\nPengxin Yu1, Haoyue Zhang 1,2, Han Kang 1, Wen Tang 1, Corey W. Arnold 2,\nand Rongguo Zhang 1(/A0 )\n1 Infervision Medical Technology Co., Ltd. Beijing, China\nzrongguo@infervision.com\n2 Computational Diagnostics Lab, UCLA, Los Angeles, USA\nAbstract. In clinical practice, anisotropic volumetric medical imag es\nwith low through-plane resolution are commonly used due to s hort ac-\nquisition time and lower storage cost. Nevertheless, the co arse resolu-\ntion may lead to diﬃculties in medical diagnosis by either ph ysicians\nor computer-aided diagnosis algorithms. Deep learning-ba sed volumet-\nric super-resolution (SR) methods are feasible ways to impr ove resolu-\ntion, with convolutional neural networks (CNN) at their cor e. Despite\nrecent progress, these methods are limited by inherent prop erties of\nconvolution operators, which ignore content relevance and cannot eﬀec-\ntively model long-range dependencies. In addition, most of the existing\nmethods use pseudo-paired volumes for training and evaluat ion, where\npseudo low-resolution (LR) volumes are generated by a simpl e degra-\ndation of their high-resolution (HR) counterparts. Howeve r, the domain\ngap between pseudo- and real-LR volumes leads to the poor per formance\nof these methods in practice. In this paper, we build the ﬁrst public\nreal-paired dataset RPLHR-CT as a benchmark for volumetric SR, and\nprovide baseline results by re-implementing four state-of -the-art CNN-\nbased methods. Considering the inherent shortcoming of CNN , we also\npropose a transformer volumetric super-resolution networ k (TVSRN)\nbased on attention mechanisms, dispensing with convolutio ns entirely.\nThis is the ﬁrst research to use a pure transformer for CT volu metric\nSR. The experimental results show that TVSRN signiﬁcantly o utper-\nforms all baselines on both PSNR and SSIM. Moreover, the TVSR N\nmethod achieves a better trade-oﬀ between the image quality , the num-\nber of parameters, and the running time. Data and code are ava ilable at\nhttps://github.com/smilenaxx/RPLHR-CT.\nKeywords: Volumetric super-resolution · CT · Deep learning · Trans-\nformer\n1 Introduction\nVolumetric medical imaging, such as computed tomography (CT) and magnetic\nresonance imaging (MRI), is an important tool in diagnostic radiology . Although\nPengxin Yu and Haoyue Zhang contribute equally to this work.\n2 Pengxin Yu, Haoyue Zhang et al.\nhigh-resolution volumetric medical imaging provides more anatomical and func-\ntional details that beneﬁt diagnosis [\n3, 22, 24], long acquisition time and high\nstorage cost limit the wide application in clinical practice. As a result, it is\nroutine to acquire anisotropic volumes in practice, which have high in- plane res-\nolution and low through-plane resolution. However, the disparity in r esolution\ncan lead to several challenges: (1) the inability to display sagittal or coronal\nviews with adequate detail [ 10]; (2) the insuﬃciency of spatial resolution to ob-\nserve the details of lesions [ 24] and; (3) the challenge to the robustness of 3D\nmedical image processing algorithms [ 8, 16]. A feasible solution is to use super-\nresolution (SR) algorithms [ 26] to upsample anisotropic volumes along the depth\ndimension, in order to restore high resolution (HR) from low resolutio n (LR).\nThis approach is referred to as ”volumetric SR.”\nCNN-based algorithms have achieved outstanding performance in S R for\nnatural images [ 20] and these techniques have been introduced for volumetric\nSR [ 1,4,6,12,13,15,17,18,23,25]. Though signiﬁcant advances have been made,\nCNN-based algorithms remain limited by the inherent weaknesses of c onvolution\noperators. On the one hand, using the same convolution kernel to restore various\nregions may neglect the content relevance. Liu et al. [ 13] take this into consid-\neration and propose a multi-stream architecture based on lung seg mentation to\nrecover diﬀerent regions separately, but this is hard to be a one-s ize-ﬁts-all so-\nlution. On the other hand, the non-local content similarity of images has been\nused as an eﬀective prior in image restoration [ 27]. Unfortunately, the local pro-\ncessing principle of the convolution operator makes algorithms diﬃcu lt to eﬀec-\ntively model long-range dependence. Recently, transformer net works have shown\ngood performance in several visual problems of natural image [ 5, 14], including\nSR [ 2, 11]. Self-attention mechanism is the key to the success of transform er.\nCompared to CNN-based algorithms, transformer can model long- range depen-\ndence in the input domain and perform dynamic weight aggregation of features\nto obtain input-speciﬁc feature representation enhancement [ 9]. These results\nprompted us to explore a transformer-based SR method.\nAnother impediment to the application of volumetric SR methods is dat a.\nMost relevant studies use HR volume as ground truth and degrade it to construct\npaired pseudo-LR volumes with which to train and evaluate methods [ 4, 15, 17,\n18, 23, 25]. For instance, Peng et al. [ 17] perform sparse sampling on the depth\ndimension of thin CT to obtain pseudo thick CT. Zhao et al. [ 25] simulate\npseudo-LR MRI by applying an ideal low-pass ﬁlter to the isotropic T2 -weighted\nMRI followed by an anti-ringing Fermi ﬁlter. However, the performa nce will be\naﬀected when test on the real-LR volume [ 1] because of the domain gap between\npseudo- and real-LR volume. To avoid it, some studies collect real-pa ired LR-HR\nvolumes [ 1,6,12,13,16]. For example, Liu et al. [ 13] collect 880 real pairs of chest\nCTs and construct a progressive upsampling model to reconstruc t 1mm CT from\n5mm CT. In the ﬁeld of MRI, a large data set containing 1,611 real pair s of T1-\nweighted MRIs have been used to develop the proposed SCSRN meth od [ 12].\nHowever, a benchmark to objectively evaluate various volumetric S R methods\nis still lacking.\nRPLHR-CT Dataset and Transformer for Volumetric SR 3\nTo address this deﬁciency, the ﬁrst goal of this work is to curate a medium-\nsized dataset, named Real-Paired Low- and High-Resolution CT (RPL HR-CT),\nfor volumetric SR. RPLHR-CT contains real-paired thin-CTs (slice th ickness\n1mm) and thick-CTs (slice thickness 5mm) of 250 patients. To the be st of our\nknowledge, RPLHR-CT is the ﬁrst benchmark for volumetric SR, whic h enables\nmethod comparison. The other goal of our work is to explore the po tential of\ntransformer for volumetric SR. Speciﬁcally, we propose a novel Tr ansformer Vol-\numetric Super-Resolution Network (TVSRN). TVSRN is designed as a n asym-\nmetric encoder-decoder architecture with transformer layer, w ithout any convo-\nlution operations. TVSRN is the ﬁrst pure transformer used for CT volumetric\nSR. We re-implement and benchmark state-of-the-art CNN-base d volumetric SR\nalgorithms developed for CT and show that our TVSRN outperforms existing al-\ngorithms signiﬁcantly. Additionally, TVSRN achieves a better trade- oﬀ between\nimage quality, the number of parameters, and running time.\n2 Dataset and Methodology\n2.1 RPLHR-CT Dataset\nDataset Description. The RPLHR-CT dataset is composed of 250 paired\nchest CTs from patients. All data have been anonymized to ensure privacy.\nPhilips machines were used to perform CT scans and the raw data wer e then\nreconstructed to thin CT (1mm) and thick CT (5mm) images. Thus, r ecovering\nthin CT (HR volume) from thick CT (LR volume) for this dataset is a volu metric\nSR task with an upsampling factor of 5 in the depth dimension. The CT s cans are\nsaved in NIFTI (.nii) format with volume sizes of L × 512 × 512, where 512 × 512\nis the size of CT slices, and L is the number of CT slices, ranging from 191 to\n396 for thin CT and 39 to 80 for thick CT. The thin CT and the corresp onding\nthick CT have the same in-plane resolution, ranging in [0 .604,0.795], and are\naligned according to spatial location.\nDataset split and Evaluation Metric.We randomly split the RPLHR-CT\ndataset into 100 train, 50 validation and 100 test CT pairs. For evalu ation, we\nquantitatively assess the performance of all methods in terms of p eak signal to\nnoise ratio (PSNR) and structural similarity (SSIM) [\n21]. Signiﬁcance is tested\nby one-sided Wilcoxon signed-rank test.\nDataset Analysis. To analysis the diﬀerence between the thin CT and thick\nCT, we group slices in thin CT and thick CT into three categories of slice -pairs\naccording to their spatial relationship, as shown on the left side of F ig.\n1. We\nuse PSNR and SSIM to access the changes in the similarity of three slic e-pairs\nin train, validation and test CT pairs. As shown on the right side of Fig. 1,\nthe results indicate that the similarity of slice-pairs at the same spat ial location\nin thin CT and thick CT, namely Match, is the highest, while the similarity\ndecreases signiﬁcantly as the spatial distance becomes larger.\n4 Pengxin Yu, Haoyue Zhang et al.\nMatch Near Far \n0 30 31 32 33 34 35 36 37 38 \nTest \nValidation \nTrain \nPSNR \n0 0.86 0.88 0.90 0.92 0.94 0.96 \nSSIM \nThick CT Thin CT \n(a) (b) \nFig. 1.(a) Three categories of slice-pairs according to their spat ial relationship in thin\nCT and thick CT. Match: same position, shown in blue; Near: 1m m apart, shown in\nred; Far: 2mm apart, show in green. (b) The degree of similari ty between the three\nslice-pairs on the three datasets. (Color ﬁgure online)\n·\n·\n·\nLinear \nEmbedding \nReshape \n×N \nEncoder \n·\n·\n·\nReshape \nLinear Projection ·\n·\n·\nPermute Permute \nSTL \nSTL \nSTL \nSTL \nSTL \nSTL \nSTL \nSTL \nRe-permute Re-permute \nLayer Norm \nWindow \nMulti-head \nSelf-Attention \nMulti-Layer \nPerceptron \nLayer Norm \nLayer Norm \nShift-Window \nMulti-head \nSelf-Attention \nMulti-Layer \nPerceptron \nLayer Norm \n(a) Overall architecture \nShare \nweights \nTAB \n×1 \n×M \nDecoder \nReshape \nReshape \nFeature Interaction Module \n(b) (W\n4 ×\n)\n4C × D × H\n(H\n4 ×\n)\n4C × D × W\nC × D × H × W C × D × H × W\n·\n·\n·\nTAB \nSTL \nSTL \n×4 \nFig. 2.(a) Illustration of the proposed Transformer Volumetric Su per-Resolution Net-\nwork architecture. (b) Details of TAB. The purple dashed box represents two consec-\nutive swin transformer layers. The batch dimension is indic ated in parentheses.\nRPLHR-CT Dataset and Transformer for Volumetric SR 5\n2.2 Network Architecture\nInspired by MAE [\n7], we treat volumetric SR as a task to recover the masked\nregions from the visible regions, where the visible regions refer to th e slices in the\nLR volume and the masked regions refer to the slices in the correspo nding HR\nvolume. As illustrated in Fig. 2, we also design our TVSRN with an asymmetric\nencoder-decoder architecture, but with several targeted mod iﬁcations. First, in\nTVSRN, the encoder and the decoder are equally important, and to better model\nthe relationship between the visible regions and the masked regions, the decoder\nuses a larger amount of parameters than the encoder. Second, in stead of the\nstandard transformer layer [ 5], we use the swin transformer layer (STL) [ 14],\nwhich is less computationally intensive and more suitable for high resolu tion\nimage, as the basic component of TVSRN. Third, we propose Throug h-plane\nAttention Blocks to exploit the spatial positional relationship of volu metric data\nto achieve better performance.\nEncoder is used to map the LR volume to a latent representation. The con-\nsecutive slices from LR volumes are denoted as the input Xin\ne ∈ R1×D×H×W of\nencoder, where D, H and W are the depth, height and width, and the channel is\n1. Xin\ne is ﬁrstly fed into the Linear Embedding, whose number of feature chan-\nnel is C, to extract shallow features and output Fs ∈ RC×D×H×W . Then, Fs is\nreshaped to F0 ∈ RCD×H×W . We stack N STLs to extract deep features from\nF0 as:\nFi = HST L\ni (Fi−1), i = 1 ,2, ..., N (1)\nwhere HST L\ni (·) denotes the i-th STL. Finally, FN is reshaped to 3D output\nXout\ne ∈ RC×D×H×W .\nDecoder is used to recovery the HR volume from the latent representation. As\nshown in Fig.\n2(a), mask tokens are introduced after the encoder, and the full\nset of Xout\ne and mask tokens is input to the decoder as Xin\nd ∈ RC×D′×H×W ,\nwhere D′ is the depth of ground truth. The mask tokens are learned vector that\nindicates the missing slices in the LR volumes compared to the HR count erpart.\nDecoder stack M Feature Interaction Modules (FIMs), which consists of one\nThrough-plane Attention Block (TAB), four STLs and two reshape operations.\nThe reshape operations are used to reshape the input feature ma p into the size\nexpected by the next block. The output of the decoder is Xout\nd with the same\nsize as Xin\nd . Note that the design of asymmetric decoder can easily be adapted\nto other upsampling rates by changing the number of mask tokens.\nThe details of TAB are illustrated in Fig. 2(b). TAB is the ﬁrst block in\neach FIM. There are two parallel branches in TAB that perform self -attention\non the input from coronal and sagittal views, respectively. In bot h views, the\ndepth dimension will become an axis of the STL’s window, so the relative po-\nsition relationship between slices will be incorporated into the calculat ion. The\nparameter weights of the corresponding STL on the two parallel br anches are\n6 Pengxin Yu, Haoyue Zhang et al.\nshared. Given the input feature zin of TAB, the output is computed as:\nzsag\n0 = P sag(zin), zcor\n0 = P cor(zin)\nzsag\nj = HST L\nj (zsag\nj−1), zcor\nj = HST L\nj (zcor\nj−1), j = 1 ,2,3,4\nzout = zin + P sag\nre (zsag\n4 ) + P cor\nre (zcor\n4 ) (2)\nwhere P sag(·) and P cor(·) are permutation operations that transform the in-\nput to sagittal and coronal view, respectively. P sag\nre (·) and P cor\nre (·) denote re-\npermutation operations that reshape the input back to original siz e. In addition,\nTAB contains residual connection, which allow the aggregation of diﬀ erent levels\nof features.\nReconstruction Target.The Xout\nd is fed into the Linear Projectionto obtain\nthe pixel-wise prediction ˆY ∈ RD′×H×W . The L1 pixel loss is formulated as:\nLpixel = 1\nD′ × H × W\n∑\nk,i,j\n|ˆYk,i,j − Yk,i,j| (3)\nwhere Y is the ground truth HR volume.\nArchitecture Hyper-parameters. For each STL, the patch size is 1 × 1 and\nthe window sizes of x-axis, y-axis and z-axis are set to 8, 8 and 4. Fo r Linear\nEmbedding, the channel number C is 8. The number of STLs in encoder and\nFIMs in decoder is set to N = 4 and M = 1, respectively.\n3 Experiments and Results\nImplementation Details. We normalize the intensity of the CT images from\n[− 1024,2048] to [0 ,1]. During training, 4 × 256 × 256 cubes from thick CTs are\nused as input and the corresponding 16 × 256 × 256 cubes from thin CTs are\nused as ground truth, in where 16 = (4 − 1) × 5 + 1. During inference, we feed\ncubes from thick CTs to the model in a sliding window manner, in which th e\noverlap of depth dimension is 1 and the rest is 0. If the depth of unte sted cubes\nis less than 4, we feed the last 4 slices into the model. For multiple predic tions on\nthe same coordinate, we take the average as the ﬁnal value. TVSR N is trained\nwith Adam optimizer. The learning rate is 0 .0001 and the batch size is 1. For the\ncomparison methods, we follow descriptions provided in the original p apers to re-\nimplement the models, as none have public code available. Settings not detailed\nin the original paper will remain consistent with our work. Data augme ntation\ninclude random cropping and horizontal ﬂipping. The framework is imp lemented\nin PyTorch, and trained on NVIDIA A6000 GPUs.\n3.1 Results and analysis\nFig.\n3(a) summarizes the quantitative comparisons of our method and ot her\nstate-of-the-art CT volumetric SR methods: ResVox [ 6], MPU-Net [ 13], SAINT\n[17] and DA-VSR [ 18]. For ResVox, the noise reduction part is removed. For\nRPLHR-CT Dataset and Transformer for Volumetric SR 7\n0\n36 \n37 \n38 \n39 \n40 \n41 PSNR(dB) \n0\n0.91 \n0.92 \n0.93 \n0.94 \n0.95 \n0.96 \n0.97 SSIM \nMPU-Net SAINT DA-VSR TVSRN ResVox \n*\n*\n*\n*\n*\n*\n*\n*\n0 30 32 34 36 38 40 42 \nResVox \nMPU-Net \nSAINT \nTVSRN \nPSNR \n0 0.87 0.89 0.91 0.93 0.95 0.97 \nSSIM \nTrain with real-LR Train with pseudo-LR \n(a) (b) (c) \n* *\n*\n*\n*\n*\n*\n*\nFig. 3.(a) Quantitative comparisons of our TVSRN and other state-o f-the-art meth-\nods. ∗ indicates p <0.001. (b) PSNR vs. processing time of each volume with number\nof parameters shown in circle size. (c) quantitative result s of pseudo images experiment.\nMPU-Net, we do not use the multi-stream architecture due to the la ck of avail-\nable lung masks. TVSRN achieves PSNR of 38 .609 ± 1.721 and SSIM of 0 .936 ±\n0.024, outperforms others signiﬁcantly ( p < 0.001). Moreover, as shown in\nFig. 3(b), compared to other methods, TVSRN achieves a better trade -oﬀ in\nterms of the PSNR (optimal), the number of parameters (optimal), and the run-\nning time (suboptimal). We also perform the comparison on an extern al test set,\nwhere TVSRN also achieved the best performance. Detailed numeric al results\non the internal test set and external test set are presented in t he supplementary\nmaterial. In addition, a sample-by-sample performance scatterplo t is given in\nthe supplementary material.\nWe visualize the axial, coronal and sagittal views of HR CT volume obta ined\nby diﬀerent methods. It is clear in Fig. 4 that TVSRN has the richest details\nand the least amount of structural artifacts remaining in diﬀerent views.\n(a) Bicubic (b) ResVox (c) MPU-Net (d) SAINT (e) TVSRN (f) Ground Truth \nFig. 4.Visual comparisons of diﬀerent methods against TVSRN. The ﬁ rst and second\nrows show the axial view and coronal view respectively, disp layed as lung window. The\nthird row is sagittal view, displayed as bone window. Yellow arrows point to areas of\nmarked diﬀerence.\n8 Pengxin Yu, Haoyue Zhang et al.\n3.2 Domain gap analysis\nWe conduct a pseudo images experiment to illustrate the eﬀect of th e domain\ngap. Speciﬁcally, we degrade the training data to obtain pseudo-LR volumes,\nand use these data to train several diﬀerent methods. All setting s are the same\nas those in the previous section, except for the training data. For testing, real-\nLR volumes in the internal test set are used as input to calculate the PSNR\nand SSIM. As shown in Fig.\n3(c), the results show that both PSNR and SSIM\nof various methods are signiﬁcantly decreased to varying degrees (p < 0.001).\nPlease refer to the supplemental material for more details of degr adation.\n3.3 Ablation Study\nThe ablation study is used to verify the contribution of each compon ent in\nTVSRN on performance. The full TVSRN is compared to:\n– TVSRNEncoder\nV iT . A standard transformer-based method based on [\n5]. We map\neach patch of size 1 × 16 × 16 to token with length of 512 and set the number\nof transformer layers to eight. Instead of asymmetric decoder, it uses subpixel\nconversion [ 19] to perform upsampling.\n– TVSRNEncoder . Only the encoder of TVSRN was used. N is increasd to\neight and C is increased to 32. The upsampling method is subpixel convert.\n– TVSRNw/o T AB. TAB is not used in TVSRN, that is, the relative position\nrelationship among slices is ignored in the network.\nTable 1.Results of ablation study for TVSRN in terms of PSNR and SSIM. The best\nresults are bolded, and the second best results are underlined . * denotes statistically\nsigniﬁcant ( p <0.001) against above method with one-sided Wilcoxon signed-r ank test.\nDesigns Param PSNR(↑) SSIM(↑)\nTVSRNEncoder\nV iT\n17.15M 35.537 ± 1.353 0.918 ± 0.026\nTVSRNEncoder 1.58M 38.364 ± 1.675∗ 0.934 ± 0.024∗\nTVSRNw/o T AB. 1.56M 38.497 ± 1.700∗ 0.935 ± 0.024∗\nTVSRN 1.73M 38.609 ± 1.721∗ 0.936 ± 0.024∗\nModel performance is summarized in Table. 1. Notable observations include:\n1) among all designs, TVSRN Encoder\nV iT has the most parameters but the worst\nperformance, which indicates that it is not feasible to simply apply the trans-\nformer to the volumetric SR; 2) replacing standard transformer la yer with STL\ncan greatly reduce the number of parameters and improve the per formance by a\nlarge margin (up to 2.827dB); 3) asymmetric decoder can improve pe rformance\nslightly without changing the number of parameters; 4) improvemen ts can be\nseen from TVSRN w/o T AB to TVSRN, indicating the eﬀectiveness of modeling\nthe relative position relationship among slices. A sample-by-sample pe rformance\nscatterplots in supplemental material is used to further illustrate the eﬀective-\nness of individual components.\nRPLHR-CT Dataset and Transformer for Volumetric SR 9\n4 Conclusion\nA persistent problem with volumetric SR is the lack of real-paired data for\ntraining and evaluation, which makes it challenging generalize algorithm s to\nreal-world datasets for practical applications. In this paper, we p resented the\nRPLHR-CT Dataset, which is the ﬁrst open real-paired dataset for volumet-\nric SR, and provided baseline results by re-implementing four state- of-the-art\nSR methods. We also proposed a convolution-free transformer-b ased network,\nwhich signiﬁcantly outperformed existing CNN-based methods and h as the least\nnumber of parameters and the second shortest running time. In t he future, we\nwill enlarge the RPLHR-CT Dataset and investigate new volumetric SR training\nstrategies, such as semi-supervised learning or using unpaired rea l data.\nAcknowledgment. This work was funded by the National Key Research and\nDevelopment Project (2021YFC2500703).\nReferences\n1. Bae, W., Lee, S., Park, G., Park, H., Jung, K.H.: Residual c nn-based image super-\nresolution for ct slice thickness reduction using paired ct scans: preliminary vali-\ndation study. MIDL 2018 (2018)\n2. Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S. , Xu, C., Xu, C., Gao,\nW.: Pre-trained image processing transformer. In: Proceed ings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 12299–12310 (2021)\n3. Chen, M., Herregods, N., Jaremko, J.L., Carron, P., Elewa ut, D., Van den Bosch,\nF., Jans, L.: Diagnostic performance for erosion detection in sacroiliac joints on\nmr t1-weighted images: Comparison between diﬀerent slice t hicknesses. European\nJournal of Radiology 133, 109352 (2020)\n4. Chen, Y., Shi, F., Christodoulou, A.G., Xie, Y., Zhou, Z., Li, D.: Eﬃcient and\naccurate mri super-resolution using a generative adversar ial network and 3d multi-\nlevel densely connected network. In: International Confer ence on Medical Image\nComputing and Computer-Assisted Intervention. pp. 91–99. Springer (2018)\n5. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn , D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al .: An image is worth\n16x16 words: Transformers for image recognition at scale. I n: International Con-\nference on Learning Representations (2020)\n6. Ge, R., Yang, G., Xu, C., Chen, Y., Luo, L., Li, S.: Stereo-c orrelation and noise-\ndistribution aware resvoxgan for dense slices reconstruct ion and noise reduction in\nthick low-dose ct. In: International Conference on Medical Image Computing and\nComputer-Assisted Intervention. pp. 328–338. Springer (2 019)\n7. He, K., Chen, X., Xie, S., Li, Y., Doll´ ar, P., Girshick, R. : Masked autoencoders are\nscalable vision learners. arXiv preprint arXiv:2111.0637 7 (2021)\n8. Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maie r-Hein, K.H.: nnu-net: a\nself-conﬁguring method for deep learning-based biomedica l image segmentation.\nNature methods 18(2), 203–211 (2021)\n9. Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., S hah, M.: Transformers\nin vision: A survey. arXiv preprint arXiv:2101.01169 (2021 )\n10 Pengxin Yu, Haoyue Zhang et al.\n10. Kodama, F., Fultz, P.J., Wandtke, J.C.: Comparing thin- section and thick-section\nct of pericardial sinuses and recesses. American Journal of Roentgenology 181(4),\n1101–1108 (2003)\n11. Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., Timof te, R.: Swinir: Image\nrestoration using swin transformer. In: Proceedings of the IEEE/CVF International\nConference on Computer Vision. pp. 1833–1844 (2021)\n12. Liu, G., Cao, Z., Xu, Q., Zhang, Q., Yang, F., Xie, X., Hao, J., Shi, Y., Bernhardt,\nB.C., He, Y., et al.: Recycling diagnostic mri for empowerin g brain morphometric\nresearch–critical & practical assessment on learning-bas ed image super-resolution.\nNeuroImage 245, 118687 (2021)\n13. Liu, Q., Zhou, Z., Liu, F., Fang, X., Yu, Y., Wang, Y.: Mult i-stream progressive up-\nsampling network for dense ct image reconstruction. In: Int ernational Conference\non Medical Image Computing and Computer-Assisted Interven tion. pp. 518–528.\nSpringer (2020)\n14. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S ., Guo, B.: Swin\ntransformer: Hierarchical vision transformer using shift ed windows. arXiv preprint\narXiv:2103.14030 (2021)\n15. Lu, Z., Li, Z., Wang, J., Shi, J., Shen, D.: Two-stage self -supervised cycle-\nconsistency network for reconstruction of thin-slice mr im ages. In: International\nConference on Medical Image Computing and Computer-Assist ed Intervention.\npp. 3–12. Springer (2021)\n16. Park, S., Lee, S.M., Kim, W., Park, H., Jung, K.H., Do, K.H ., Seo, J.B.: Computer-\naided detection of subsolid nodules at chest ct: improved pe rformance with deep\nlearning–based ct section thickness reduction. Radiology 299(1), 211–219 (2021)\n17. Peng, C., Lin, W.A., Liao, H., Chellappa, R., Zhou, S.K.: Saint: spatially aware\ninterpolation network for medical slice synthesis. In: Pro ceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 7750–7759 (2020)\n18. Peng, C., Zhou, S.K., Chellappa, R.: Da-vsr: Domain adap table volumetric super-\nresolution for medical images. In: International Conferen ce on Medical Image Com-\nputing and Computer-Assisted Intervention. pp. 75–85. Spr inger (2021)\n19. Shi, W., Caballero, J., Husz´ ar, F., Totz, J., Aitken, A. P., Bishop, R., Rueckert,\nD., Wang, Z.: Real-time single image and video super-resolu tion using an eﬃcient\nsub-pixel convolutional neural network. In: Proceedings o f the IEEE conference on\ncomputer vision and pattern recognition. pp. 1874–1883 (20 16)\n20. Wang, Z., Chen, J., Hoi, S.C.: Deep learning for image sup er-resolution: A survey.\nIEEE transactions on pattern analysis and machine intellig ence (2020)\n21. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: I mage quality assessment:\nfrom error visibility to structural similarity. IEEE trans actions on image processing\n13(4), 600–612 (2004)\n22. Xu, F., Liang, Y., Guo, W., Liang, Z., Li, L., Xiong, Y., Ye , G., Zeng, X.: Diagnos-\ntic performance of diﬀusion-weighted imaging for diﬀerent iating malignant from\nbenign intraductal papillary mucinous neoplasms of the pan creas: A systematic\nreview and meta-analysis. Frontiers in Oncology 11, 2550 (2021)\n23. Xuan, K., Si, L., Zhang, L., Xue, Z., Jiao, Y., Yao, W., She n, D., Wu, D., Wang,\nQ.: Reducing magnetic resonance image spacing by learning w ithout ground-truth.\nPattern Recognition p. 108103 (2021)\n24. Yang, J., He, Y., Huang, X., Xu, J., Ye, X., Tao, G., Ni, B.: Alignshift: bridging the\ngap of imaging thickness in 3d anisotropic volumes. In: Inte rnational Conference\non Medical Image Computing and Computer-Assisted Interven tion. pp. 562–572.\nSpringer (2020)\nRPLHR-CT Dataset and Transformer for Volumetric SR 11\n25. Zhao, C., Dewey, B.E., Pham, D.L., Calabresi, P.A., Reic h, D.S., Prince, J.L.:\nSmore: A self-supervised anti-aliasing and super-resolut ion algorithm for mri using\ndeep learning. IEEE transactions on medical imaging 40(3), 805–817 (2020)\n26. Zhou, S.K., Greenspan, H., Davatzikos, C., Duncan, J.S. , Van Ginneken, B., Mad-\nabhushi, A., Prince, J.L., Rueckert, D., Summers, R.M.: A re view of deep learning\nin medical imaging: Imaging traits, technology trends, cas e studies with progress\nhighlights, and future promises. Proceedings of the IEEE (2 021)\n27. Zhou, S., Zhang, J., Zuo, W., Loy, C.C.: Cross-scale inte rnal graph neural network\nfor image super-resolution. arXiv preprint arXiv:2006.16 673 (2020)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7280409932136536
    },
    {
      "name": "Convolutional neural network",
      "score": 0.632924497127533
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5487155318260193
    },
    {
      "name": "Superresolution",
      "score": 0.5407400727272034
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5191192030906677
    },
    {
      "name": "Deep learning",
      "score": 0.5023529529571533
    },
    {
      "name": "Transformer",
      "score": 0.47389355301856995
    },
    {
      "name": "Algorithm",
      "score": 0.34028518199920654
    },
    {
      "name": "Data mining",
      "score": 0.3247973322868347
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3215201497077942
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3045167326927185
    },
    {
      "name": "Engineering",
      "score": 0.08545622229576111
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210111607",
      "name": "InferVision (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210121124",
      "name": "Computational Diagnostics (United States)",
      "country": "US"
    }
  ]
}