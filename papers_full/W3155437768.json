{
  "title": "IIITT@LT-EDI-EACL2021-Hope Speech Detection: There is always Hope in Transformers",
  "url": "https://openalex.org/W3155437768",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4225837633",
      "name": "Puranik, Karthik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226429555",
      "name": "Hande, Adeep",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223330829",
      "name": "Priyadharshini, Ruba",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2522504641",
      "name": "Thavareesan Sajeetha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223330833",
      "name": "Chakravarthi, Bharathi Raja",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3103061166",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3124917515",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3154997565",
    "https://openalex.org/W3118477442",
    "https://openalex.org/W2975437251",
    "https://openalex.org/W3099919888",
    "https://openalex.org/W3094306499",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2964583233",
    "https://openalex.org/W3119228675",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W2981515358",
    "https://openalex.org/W2786315637",
    "https://openalex.org/W3154552322",
    "https://openalex.org/W3102007290",
    "https://openalex.org/W3099150152",
    "https://openalex.org/W3017367042",
    "https://openalex.org/W2913148947",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W3030198970",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W3126113954",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3101284630",
    "https://openalex.org/W3044864684",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3018024187",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2767327746",
    "https://openalex.org/W3091510409"
  ],
  "abstract": "In a world filled with serious challenges like climate change, religious and political conflicts, global pandemics, terrorism, and racial discrimination, an internet full of hate speech, abusive and offensive content is the last thing we desire for. In this paper, we work to identify and promote positive and supportive content on these platforms. We work with several transformer-based models to classify social media comments as hope speech or not-hope speech in English, Malayalam and Tamil languages. This paper portrays our work for the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021- EACL 2021.",
  "full_text": "IIITT@LT-EDI-EACL2021-Hope Speech Detection: There is always\nHope in Transformers\nKarthik Puranik1, Adeep Hande1, Ruba Priyadharshini2,\nSajeetha Thavareesan3, Bharathi Raja Chakravarthi4\n1Indian Institute of Information Technology Tiruchirappalli\n2ULTRA Arts and Science College, India,3Eastern University, Sri Lanka\n4National University of Ireland Galway\nkarthikp18c@iiitt.ac.in\nAbstract\nIn a world ﬁlled with serious challenges like\nclimate change, religious and political con-\nﬂicts, global pandemics, terrorism, and racial\ndiscrimination, an internet full of hate speech,\nabusive and offensive content is the last thing\nwe desire for. In this paper, we work to iden-\ntify and promote positive and supportive con-\ntent on these platforms. We work with sev-\neral transformer-based models to classify so-\ncial media comments as hope speech or not-\nhope speech in English, Malayalam and Tamil\nlanguages. This paper portrays our work for\nthe Shared Task on Hope Speech Detection for\nEquality, Diversity, and Inclusion at LT-EDI\n2021- EACL 2021. The codes for our best sub-\nmission can be viewed1.\n1 Introduction\nSocial Media has inherently changed the way peo-\nple interact and carry on with their everyday lives\nas people using the internet (Jose et al., 2020;\nPriyadharshini et al., 2020). Due to the vast\namount of data being available on social media\napplications such as YouTube, Facebook, an Twit-\nter it has resulted in people stating their opinions\nin the form of comments that could imply hate or\nnegative sentiment towards an individual or a com-\nmunity (Chakravarthi et al., 2020c; Mandl et al.,\n2020). This results in people feeling hostile about\ncertain posts and thus feeling very hurt (Bhardwaj\net al., 2020).\nBeing a free platform, social media runs on\nuser-generated content. With people from multi-\nfarious backgrounds present, it creates a rich so-\ncial structure (Kapoor et al., 2018) and has become\nan exceptional source of information. It has laid\nit’s roots so deeply into the lives of people that\nthey count on it for their every need. Regardless,\n1https://github.com/karthikpuranik11/\nHope-Speech-Detection-\nthis tends to mislead people in search of credible\ninformation. Certain individuals or ethnic groups\nalso fall prey to people utilizing these platforms\nto foster destructive or harmful behaviour which\nis a common scenario in cyberbullying (Abaido,\n2020).\nThe earliest inscription in India dated from\n580 BCE was the Tamil inscription in pottery\nand then, the Asoka inscription in Prakrit, Greek\nand Aramaic dating from 260 BCE. Thunchaththu\nRamanujan Ezhuthachan split Malayalam from\nTamil after the 15th century CE by using Pallava\nGrantha script to write religious texts. Pallava\nGrantha was used in South India to write San-\nskrit and foreign words in Tamil literature. Mod-\nern Tamil and Malayalam have their own script.\nHowever, people use the Latin script to write\non social media (Chakravarthi et al., 2018, 2019;\nChakravarthi, 2020b).\nThe automatic detection of hateful, offensive,\nand unwanted language related to events and sub-\njects on gender, religion, race or ethnicity in so-\ncial media posts is very much necessary (Rizwan\net al., 2020; Ghanghor et al., 2021a,b). Such harm-\nful content could spread, stimulate, and vindicate\nhatred, outrage, and prejudice against the targeted\nusers. Removing such comments was never an\noption as it suppresses the freedom of speech of\nthe user and it is highly unlikely to stop the per-\nson from posting more. In fact, he/she/they would\nbe prompted to post more of such comments 2\n(Yasaswini et al., 2021; Hegde et al., 2021). This\nbrings us to our goal to spread positivism and hope\nand identify such posts to strengthen an open-\nminded, tolerant, and unprejudiced society.\n2https://www.qs.com/negative-comments-on-social-\nmedia/\narXiv:2104.09066v1  [cs.CL]  19 Apr 2021\nText Language Label\nGod gave us a choice my choice is to love, I would die for that kid English Hope\nThe Democrats are.Backed powerful rich people like Soros English Not hope\nESTE PSIC˜A“PATA MAS˜A“N LUCIFERIANO ES HOMBRE TRANS English Not English\nNeega podara vedio nalla iruku ana subtitle vainthuchu ahh yella language papagaTamil Hope\nAvan matum enkita maatunan... Avana kolla paniduven Tamil Not hope\nI can’t uninstall mY Pubg Tamil Not Tamil\nooororutharum avarude ishtam pole jeevikatte . k. Malayalam Hope\nEtraem aduthu nilkallae Arunae Malayalam Not hope\nPhoenix contact me give you’re mail I’d I hope I can support you sure! Malayalam Not Malayalam\nTable 1: Examples of hope speech or not hope speech\n2 Related Works\nThe need for the segregation of toxic comments\nfrom social media platforms has been identiﬁed\nback in the day. Founta et al. (2018) has tried\nto study the textual properties and behaviour of\nabusive postings on Twitter using a Uniﬁed Deep\nLearning Architecture. Hate speech can be classi-\nﬁed into various categories like hatred against an\nindividual or group belonging to a race, religion,\nskin colour, ethnicity, gender, disability, or nation3\nand there have been studies to observe it’s evolu-\ntion in social media over the past thirty years (Ton-\ntodimamma et al., 2021). Deep Learning methods\nwere used to classify hate speech into racist, sexist\nor neither in Badjatiya et al. (2017).\nHope is support, reassurance or any kind\nof positive reinforcement at the time of crisis\n(Chakravarthi, 2020a). Palakodety et al. (2020)\nidentiﬁes the need for the automatic detection of\ncontent that can eliminate hostility and bring about\na sense of hope during times of wrangling and\nbrink of a war between nations. There have also\nbeen works to identify hate speech in multilingual\n(Aluru et al., 2020) and code-mixed data in Tamil,\nMalayalam, and Kannada language (Chakravarthi\net al., 2020b,a; Hande et al., 2020). However, there\nhave been very fewer works in Hope speech detec-\ntion for Indian languages.\n3 Dataset\nThe dataset is provided by (Chakravarthi, 2020a)\n(Chakravarthi and Muralidaran, 2021) and con-\ntains 59,354 comments from the famous online\nvideo sharing platform YouTube out of which\n28,451 are in English, 20,198 in Tamil, and 10,705\ncomments are in Malayalam (Table 2) which can\n3http://www.ala.org/advocacy/\nintfreedom/hate (Accessed January 16, 2021)\nbe classiﬁed as Hope speech, not hope speech and\nother languages. This dataset is split into train\n(80%), development (10%) and test (10%) dataset\n(Table 3).\nSubjects like hope speech might raise confu-\nsions and disagreements between annotators be-\nlonging to different groups. The dataset was an-\nnotated by a minimum of three annotators and the\ninter-annotator agreement was determined using\nKrippendorff’s alpha (krippendorff, 2011). Re-\nfer table 1 for examples of hope speech, not hope\nspeech and other languages for English, Tamil and\nMalayalam datasets respectively.\nClass English Tamil Malayalam\nHope 2,484 7,899 2,052\nNot Hope 25,940 9,816 7,765\nOther lang 27 2,483 888\nTotal 28,451 20,198 10,705\nTable 2: Classwise Data Distribution\nSplit English Tamil Malayalam\nTraining 22,762 16,160 8564\nDevelopment 2,843 2,018 1070\nTest 2,846 2,020 1071\nTotal 28,451 20,198 10,705\nTable 3: Train-Development-Test Data Distribution\n4 Experiment Setup\nIn this section, we give a detailed explanation of\nthe experimental conditions upon which the mod-\nels are developed.\nFigure 1: Context-independent representations in BERT and CharacterBERT (Source: El Boukkouri et al. (2020))\n4.1 Architecture\n4.1.1 Dense\nThe dense layers used in CNN (convolutional neu-\nral networks) connects all layers in the next layer\nwith each other in a feed-forward fashion (Huang\net al., 2018). Though they have the same for-\nmulae as the linear layers i.e. wx+b, the out-\nput is passed through an activation function which\nis a non-linear function. We implemented our\nmodels with 2 dense layers, rectiﬁed linear units\n(ReLU) (Agarap, 2019) as the activation function\nand dropout of 0.4.\n4.1.2 Bidirectional LSTM\nBidirectional LSTM or biLSTM is a sequence pro-\ncessing model (Schuster and Paliwal, 1997). It\nuses both the future and past input features at a\ntime as it contains two LSTM’s, one taking input\nin the forward direction and another in the back-\nward direction (Schuster and Paliwal, 1997). The\nbackward and forward pass through the unfolded\nnetwork just like any regular network. However,\nBiLSTM requires us to unfold the hidden states\nfor every time step. It produces a drastic increase\nin the size of information being fed thus, improv-\ning the context available (Huang et al., 2015). Re-\nfer Table 4 for the parameters used in the BiLSTM\nmodel.\nParameter Value\nNumber of LSTM units 256\nDropout 0.4\nActivation Function ReLU\nMax Len 128\nBatch Size 32\nOptimizer AdamW\nLearning Rate 2e-5\nLoss Function cross-entropy\nNumber of epochs 5\nTable 4: Parameters for the BiLSTM model\n4.2 Embeddings\n4.2.1 BERT\nBidirectional Encoder Representations from\nTransformers (BERT) (Devlin et al., 2019). The\nmultilingual base model is pretrained on the\ntop 104 languages of the world on Wikipedia\n(2.5B words) with 110 thousand shared wordpiece\nvocabulary. The input is encoded into vectors with\nBERT’s innovation of bidirectionally training the\nlanguage model which catches a deeper context\nand ﬂow of the language. Furthermore, novel\ntasks like Next Sentence Prediction (NSP) and\nMasked Language Modelling (MLM) are used to\ntrain the model.\nThe pretrained BERT Multilingual model bert-\nbase-multilingual-uncased (Pires et al., 2019)\nfrom Huggingface4 (Wolf et al., 2020) is executed\nin PyTorch (Paszke et al., 2019). It consists of 12-\nlayers, 768 hidden, 12 attention heads and 110M\nparameters which are ﬁne-tuned by concatenating\nwith bidirectional LSTM layers. The BiLSTM\nlayers take the embeddings from the transformer\nencoder as the input which increases the informa-\ntion being fed, which in turn betters the context\nand accuracy. Adam algorithm with weight decay\nﬁx is used as an optimizer. We train our models\nwith the default learning rate of 2e−5. We use\nthe cross-entropy loss as it is a multilabel classiﬁ-\ncation task.\n4.2.2 ALBERT\nIt has a similar architecture as that of BERT but\ndue to memory limitations and longer training\nperiods, ALBERT or A Lite BERT introduces\ntwo parameter reduction techniques (Chiang et al.,\n2020). ALBERT distinguishes itself from BERT\nwith features like factorization of the embedding\nmatrix, cross-layer parameter sharing and inter-\nsentence coherence prediction. We implemented\nalbert-base-v2 pretrained model with 12 repeating\nlayers, 768 hidden, 12 attention heads, and 12M\nparameters for the English dataset.\n4.2.3 DistilBERT\nDistilBERT is a distilled version of BERT to\nmake it smaller, cheaper, faster, and lighter (Sanh\net al., 2019). With up to 40% less number of\nparameters than bert-base-uncased, it promises\nto run 60% faster while preserving 97% of\nit’s performance. We employ distilbert-base-\nuncased for the English dataset and distilbert-\nbase-multilingual-cased for the Tamil and Malay-\nalam datasets. Both models have 6-layers, 768-\nhidden, 12-heads and while the former has 66M\nparameters, the latter has 134M parameters.\n4.2.4 RoBERTa\nA Robustly optimized BERT Pretraining Ap-\nproach (RoBERTa) is a modiﬁcation of BERT (Liu\net al., 2020). RoBERTa is trained for longer, with\nlarger batches on 1000% more data than BERT.\nThe Next Sentence Prediction (NSP) task em-\nployed in BERT’s pre-training is removed and dy-\nnamic masking during training is introduced. It’s\nadditionally trained on a 76 GB large new dataset\n4https://huggingface.co/transformers/\npretrained_models.html\n(CC-NEWS). roberta-base follows the BERT ar-\nchitecture but has 125M parameters and is used\nfor the English dataset.\n4.2.5 CharacterBERT\nCharacterBERT (CharBERT) (El Boukkouri et al.,\n2020) is a variant of BERT (Devlin et al., 2019)\nwhich uses CharacterCNN (Zhang et al., 2015)\nlike ELMo (Peters et al., 2018), instead of relying\non WordPieces (Wu et al., 2016). CharacterBERT\nis highly desired as it produces a single embedding\nfor any input token which is more suitable than\nhaving an inconstant number of WordPiece vec-\ntors for each token. It furthermore replaces BERT\nfrom domain-speciﬁc wordpiece vocabulary and\nenables it to be more robust to noisy inputs.\nWe use the pretrained model general-\ncharacter-bert5 which was pretrained on the\nsame corpus of that of BERT, but with a different\ntokenization approach. A CharacterCNN module\nis used that produces word-level contextual repre-\nsentations and it can be re-adapted to any domain\nwithout needing to worry about the suitability\nof any wordpieces (Figure 1). This approach\nhelps for superior robustness by approaching the\ncharacter of the inputs.\n4.2.6 ULMFiT\nUniversal Language Model Fine-tuning, or ULM-\nFiT, was a transfer learning method introduced to\nperform various NLP tasks (Howard and Ruder,\n2018). Training of ULMFiT involves pretraining\nthe general language model on a Wikipedia-based\ncorpus, ﬁne-tuning the language model on a tar-\nget text, and ﬁnally, ﬁne-tuning the classiﬁer on\nthe target task. Discriminative ﬁne-tuning is ap-\nplied to ﬁne-tune the model as different layers cap-\nture the different extent of information. It is then\ntrained using the learning rate scheduling strategy,\nSlanted triangular learning rates (STLR), where\nthe learning rate increases initially and then drops.\nGradual unfreezing is used to ﬁne-tune the target\nclassiﬁer rather than training all layers at once,\nwhich might lead to catastrophic forgetting.\nPretrained model, AWD-LSTM (Merity et al.,\n2017) with 3 layers and 1150 hidden activation per\nlayer and an embedding size of 400 is used as the\nlanguage model for the English dataset. Adam op-\ntimizer with β1 = 0.9 and β2 = 0.99 is imple-\nmented. Later, the start and end learning rates are\n5https://github.com/helboukkouri/\ncharacter-bert\nArchitecture Embeddings F1-Score validation F1-Score test\nBiLSTM bert-base-uncased 0.9112 0.9241\nDense\nbert-base-uncased 0.9164 0.9240\nalbert-base 0.9143 0.9210\ndistilbert-base-uncased 0.9238 0.9283\nroberta-base 0.9141 0.9235\ncharacter-bert 0.9264 0.9220\nULMFiT 0.9252 0.9356\nTable 5: Weighted F1-scores of hope speech detection classiﬁer models on English dataset\nArchitecture Embeddings F1-Score validation F1-Score test\nBiLSTM\nmbert-uncased 0.8436 0.8545\nmbert-cased 0.8280 0.8482\nxlm-roberta-base 0.8271 0.8233\nMuRIL 0.8089 0.8212\nDense\nmbert-uncased 0.8373 0.8433\nindic-bert 0.7719 0.8264\nxlm-roberta-base 0.7757 0.7001\ndistilmbert-cased 0.8312 0.8395\nMuRIL 0.8023 0.8187\nTable 6: Weighted F1-scores of hope speech detection classiﬁer model on Malayalam dataset\nArchitecture Embeddings F1-Score Validation F1-Score test\nBiLSTM\nmbert-uncased 0.6124 0.5601\nmbert-cased 0.6183 0.5297\nxlm-roberta-base 0.5472 0.5738\nMuRIL 0.5802 0.5463\nDense\nmbert-uncased 0.5916 0.4473\nmbert-cased 0.5946 0.4527\nindic-bert 0.5609 0.5785\nxlm-roberta-base 0.5481 0.3936\ndistilmbert-cased 0.6034 0.5926\nMuRIL 0.5504 0.5291\nTable 7: Weighted F1-scores of hope speech detection classiﬁer models on Tamil dataset\nset to 1e-8 and 1e-2respectively and ﬁne-tuned by\ngradually unfreezing the layers to produce better\nresults. Dropouts with a multiplier of 0.5 were ap-\nplied.\n4.2.7 XLM-RoBERTa\nXLM-RoBERTa (Ruder et al., 2019) is a pre-\ntrained multilingual language model to execute\ndiverse NLP transfer tasks. It’s trained on over\n2TB of ﬁltered CommonCrawl data in 100 differ-\nent languages. It was an update to the XLM-100\nmodel (Lample and Conneau, 2019) but with in-\ncreased training data. As it shares the same train-\ning routine with the RoBERTa model, ”RoBERTa”\nwas included in the name. xlm-roberta-base with\n12 layers, 768 hidden, 12 heads, and 270M pa-\nrameters were used. It is ﬁne-tuned for classifying\ncode-mixed Tamil and Malayalam datasets.\n4.2.8 MuRIL\nMuRiL6 was introduced by Google Research In-\ndia to enhance Indian NLU (Natural Language\nUnderstanding). The model has a BERT based\narchitecture trained on 17 Indian languages with\n6https://tfhub.dev/google/MuRIL/1\nLanguage Hope-Speech Not-hope speech Other Language Macro Avg Weighted Avg\nPrecision\nEnglish 0.9464 0.6346 0.0000 0.5270 0.9193\nMalayalam 0.6540 0.9032 0.8941 0.8171 0.572\nTamil 0.4824 0.5819 0.5709 0.5451 0.5403\nRecall\nEnglish 0.9781 0.4108 0.0000 0.4630 0.9293\nMalayalam 0.7113 0.9021 0.7525 0.7886 0.8534\nTamil 0.2687 0.7812 0.6525 0.5675 0.5579\nF1-Score\nEnglish 0.9620 0.4987 0.0000 0.4869 0.9220\nMalayalam 0.6815 0.9026 0.8172 0.8004 0.8545\nTamil 0.3452 0.6670 0.6090 0.5404 0.5207\nTable 8: Classiﬁcation report for our system models based on the results of test set\nWikipedia, Common Crawl 7, PMINDIA 8 and\nDakshina9 datasets. MuRIL is trained on trans-\nlation and transliteration segment pairs which give\nan advantage as the transliterated text is very com-\nmon in social media. It is used for the Malayalam\nand Tamil datasets.\n4.2.9 IndicBERT\nIndicBERT (Kakwani et al., 2020) is an ALBERT\nmodel pretrained on 12 major Indian languages\nwith a corpus of over 9 billion tokens. It per-\nforms as well as other multilingual models with\nconsiderably fewer parameters for various NLP\ntasks. It’s trained by choosing a single model\nfor all languages to learn the relationship be-\ntween languages and understand code-mixed data.\nai4bharat/indic-bert model was employed for the\nTamil and Malayalam task.\n5 Results\nIn this section, we have compared the F1-scores\nof our transformer-based models to successfully\nclassify social media comments/posts into hope\nspeech or not hope speech and detect the usage\nof other languages if any. We have tabulated the\nweighted average F1-scores of our various models\nfor validation and test dataset for English, Malay-\nalam and Tamil languages in tables 5, 6 and 7 re-\nspectively.\nTable 5 demonstrates that the character-aware\nmodel CharacterBERT performed exceptionally\n7http://commoncrawl.org/the-data/\n8http://lotus.kuee.kyoto-u.ac.jp/WAT/\nindic-multilingual/index.html\n9https://github.com/\ngoogle-research-datasets/dakshina\nwell for the validation dataset. It beat ULMFiT\n(Howard and Ruder, 2018) by a mere difference of\n0.0012, but other BERT-based models like BERT\n(Devlin et al., 2019) with dense and BiLSTM ar-\nchitecture, ALBERT (Chiang et al., 2020), Distil-\nBERT (Sanh et al., 2019) and RoBERTa (Liu et al.,\n2020) by about a percent. This promising result\nshown by character-bert for the validation dataset\nmade it our best model. Unfortunately, few mod-\nels managed to perform better than it for the test\ndataset. The considerable class imbalance of about\n2,484 hope to 25,940 not hope comments and the\ninterference of comments in other languages have\nsigniﬁcantly affected the results.\nSimilar transformer-based model trained on\nmultilingual data was used to classify Malay-\nalam and Tamil datasets. Models like multilin-\ngual BERT, XLM-RoBERTa (Ruder et al., 2019),\nMuRIL, IndicBERT 10 and DistilBERT multilin-\ngual with both BiLSTM and Dense architectures.\nmBERT (Multilingual BERT) uncased with BiL-\nSTM concatenated to it outperformed the other\nmodels for the Malayalam validation dataset and\ncontinued its dominance for the test data as well.\nThe data distribution for the Tamil dataset\nseemed a bit balanced with an approximate ratio\nof 4:5 between hope and not-hope. mBERT cased\nwith BiLSTM architecture appeared to be the best\nmodel with an F1-score of 0.6183 for validation\nbut dropped drastically by 8% for the test data. We\nwitnessed a considerable fall in the scores of other\nmodels like mBERT and XLM-RoBERTa with lin-\near layers of up to 15%.\nMultilingual comments experience an enor-\n10https://indicnlp.ai4bharat.org/indic-bert/\nmous variety of text as people tend to write\nin code-mixed data and other non-native scripts\nwhich are inclined to be mispredicted. A varia-\ntion in the concentration of such comments be-\ntween train, validation and test can result in a\nﬂuctuation in the test results. The precision, re-\ncall and F1-scores of CharacterBERT, mBERT-\nuncased, and mBERT-cased are tabulated under\nEnglish, Malayalam, and Tamil respectively, as\nshown in Table 8. They were the best performing\nmodels on the validation set.\n6 Conclusion\nDuring these unprecedented times, there is a need\nto detect positive, enjoyable content on social me-\ndia in order to help people who are combating\ndepression, anxiety, melancholy, etc. This pa-\nper presents several methodologies that can de-\ntect hope in social media comments. We have tra-\nversed through transfer learning of several state-\nof-the-art transformer models for languages such\nas English, Tamil, and Malayalam. Due to its su-\nperior ﬁne-tuning method, ULMFiT achieves an\nF1-score of 0.9356 on English data. We observe\nthat mBERT achieves 0.8545 on Malayalam test\nset and distilmBERT achieves 0.5926 weighted\nF1-score on Tamil test set.\nReferences\nGhada M. Abaido. 2020. Cyberbullying on social\nmedia platforms among university students in the\nunited arab emirates. International Journal of Ado-\nlescence and Youth, 25(1):407–420.\nAbien Fred Agarap. 2019. Deep learning using recti-\nﬁed linear units (relu).\nSai Saketh Aluru, Binny Mathew, Punyajoy Saha, and\nAnimesh Mukherjee. 2020. Deep learning models\nfor multilingual hate speech detection.\nPinkesh Badjatiya, Shashank Gupta, Manish Gupta,\nand Vasudeva Varma. 2017. Deep learning for hate\nspeech detection in tweets. Proceedings of the 26th\nInternational Conference on World Wide Web Com-\npanion - WWW ’17 Companion.\nMohit Bhardwaj, Md Shad Akhtar, Asif Ekbal, Ami-\ntava Das, and Tanmoy Chakraborty. 2020. Hostility\ndetection dataset in hindi.\nBharathi Raja Chakravarthi. 2020a. HopeEDI: A mul-\ntilingual hope speech detection dataset for equality,\ndiversity, and inclusion. In Proceedings of the Third\nWorkshop on Computational Modeling of People’s\nOpinions, Personality, and Emotion’s in Social Me-\ndia, pages 41–53, Barcelona, Spain (Online). Asso-\nciation for Computational Linguistics.\nBharathi Raja Chakravarthi. 2020b. Leveraging ortho-\ngraphic information to improve machine translation\nof under-resourced languages. Ph.D. thesis, NUI\nGalway.\nBharathi Raja Chakravarthi, Mihael Arcan, and John P.\nMcCrae. 2018. Improving wordnets for under-\nresourced languages using machine translation. In\nProceedings of the 9th Global Wordnet Confer-\nence, pages 77–86, Nanyang Technological Univer-\nsity (NTU), Singapore. Global Wordnet Association.\nBharathi Raja Chakravarthi, Mihael Arcan, and John P.\nMcCrae. 2019. WordNet gloss translation for under-\nresourced languages using multilingual neural ma-\nchine translation. In Proceedings of the Second\nWorkshop on Multilingualism at the Intersection of\nKnowledge Bases and Machine Translation, pages\n1–7, Dublin, Ireland. European Association for Ma-\nchine Translation.\nBharathi Raja Chakravarthi, Navya Jose, Shardul\nSuryawanshi, Elizabeth Sherly, and John Philip Mc-\nCrae. 2020a. A sentiment analysis dataset for code-\nmixed Malayalam-English. In Proceedings of the\n1st Joint Workshop on Spoken Language Technolo-\ngies for Under-resourced languages (SLTU) and\nCollaboration and Computing for Under-Resourced\nLanguages (CCURL), pages 177–184, Marseille,\nFrance. European Language Resources association.\nBharathi Raja Chakravarthi and Vigneshwaran Mural-\nidaran. 2021. Findings of the shared task on Hope\nSpeech Detection for Equality, Diversity, and Inclu-\nsion. In Proceedings of the First Workshop on Lan-\nguage Technology for Equality, Diversity and Inclu-\nsion. Association for Computational Linguistics.\nBharathi Raja Chakravarthi, Vigneshwaran Murali-\ndaran, Ruba Priyadharshini, and John Philip Mc-\nCrae. 2020b. Corpus creation for sentiment anal-\nysis in code-mixed Tamil-English text. In Pro-\nceedings of the 1st Joint Workshop on Spoken\nLanguage Technologies for Under-resourced lan-\nguages (SLTU) and Collaboration and Computing\nfor Under-Resourced Languages (CCURL), pages\n202–210, Marseille, France. European Language\nResources association.\nBharathi Raja Chakravarthi, Ruba Priyadharshini,\nVigneshwaran Muralidaran, Shardul Suryawanshi,\nNavya Jose, Elizabeth Sherly, and John P. McCrae.\n2020c. Overview of the Track on Sentiment Analy-\nsis for Dravidian Languages in Code-Mixed Text. In\nForum for Information Retrieval Evaluation, FIRE\n2020, page 21–24, New York, NY , USA. Associa-\ntion for Computing Machinery.\nCheng-Han Chiang, Sung-Feng Huang, and Hung-yi\nLee. 2020. Pretrained language model embryology:\nThe birth of ALBERT. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6813–6828, On-\nline. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHicham El Boukkouri, Olivier Ferret, Thomas\nLavergne, Hiroshi Noji, Pierre Zweigenbaum, and\nJun’ichi Tsujii. 2020. CharacterBERT: Reconciling\nELMo and BERT for word-level open-vocabulary\nrepresentations from characters. In Proceedings\nof the 28th International Conference on Compu-\ntational Linguistics, pages 6903–6915, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nAntigoni-Maria Founta, Despoina Chatzakou, Nicolas\nKourtellis, Jeremy Blackburn, Athena Vakali, and Il-\nias Leontiadis. 2018. A uniﬁed deep learning archi-\ntecture for abuse detection.\nNikhil Kumar Ghanghor, Parameswari Krishna-\nmurthy, Sajeetha Thavareesan, Ruba Priyad-\nharshini, and Bharathi Raja Chakravarthi. 2021a.\nIIITK@DravidianLangTech-EACL2021: Offensive\nLanguage Identiﬁcation and Meme Classiﬁcation\nin Tamil, Malayalam and Kannada. In Proceedings\nof the First Workshop on Speech and Language\nTechnologies for Dravidian Languages , Online.\nAssociation for Computational Linguistics.\nNikhil Kumar Ghanghor, Rahul Ponnusamy,\nPrasanna Kumar Kumaresan, Ruba Priyad-\nharshini, Sajeetha Thavareesan, and Bharathi Raja\nChakravarthi. 2021b. IIITK@LT-EDI-EACL2021:\nHope Speech Detection for Equality, Diversity, and\nInclusion in Tamil, Malayalam and English. In\nProceedings of the First Workshop on Language\nTechnology for Equality, Diversity and Inclusion,\nOnline. Association for Computational Linguistics.\nAdeep Hande, Ruba Priyadharshini, and Bharathi Raja\nChakravarthi. 2020. KanCMD: Kannada\nCodeMixed dataset for sentiment analysis and\noffensive language detection. In Proceedings of the\nThird Workshop on Computational Modeling of Peo-\nple’s Opinions, Personality, and Emotion’s in Social\nMedia, pages 54–63, Barcelona, Spain (Online).\nAssociation for Computational Linguistics.\nSiddhanth U Hegde, Adeep Hande, Ruba\nPriyadharshini, Sajeetha Thavareesan, and\nBharathi Raja Chakravarthi. 2021. UVCE-\nIIITT@DravidianLangTech-EACL2021: Tamil\nTroll Meme Classiﬁcation: You need to Pay more\nAttention. In Proceedings of the First Workshop\non Speech and Language Technologies for Dra-\nvidian Languages. Association for Computational\nLinguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 328–339, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nGao Huang, Zhuang Liu, Laurens van der Maaten, and\nKilian Q. Weinberger. 2018. Densely connected\nconvolutional networks.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-\ntional lstm-crf models for sequence tagging.\nNavya Jose, Bharathi Raja Chakravarthi, Shardul\nSuryawanshi, Elizabeth Sherly, and John P. Mc-\nCrae. 2020. A Survey of Current Datasets for Code-\nSwitching Research. In 2020 6th International Con-\nference on Advanced Computing and Communica-\ntion Systems (ICACCS), pages 136–141.\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish\nGolla, Gokul N.C., Avik Bhattacharyya, Mitesh M.\nKhapra, and Pratyush Kumar. 2020. IndicNLPSuite:\nMonolingual Corpora, Evaluation Benchmarks and\nPre-trained Multilingual Language Models for In-\ndian Languages. In Findings of EMNLP.\nKawaljeet Kapoor, Kuttimani Tamilmani, Nripendra\nRana, Pushp Patil, Yogesh Dwivedi, and Sridhar\nNerur. 2018. Advances in social media research:\nPast, present and future. Information Systems Fron-\ntiers, 20.\nklaus krippendorff. 2011. Computing krippendorff’s\nalpha-reliability.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRo{bert}a: A robustly optimized {bert}pretraining\napproach.\nThomas Mandl, Sandip Modha, Anand Kumar M, and\nBharathi Raja Chakravarthi. 2020. Overview of the\nHASOC Track at FIRE 2020: Hate Speech and Of-\nfensive Language Identiﬁcation in Tamil, Malay-\nalam, Hindi, English and German. In Forum for\nInformation Retrieval Evaluation, FIRE 2020, page\n29–32, New York, NY , USA. Association for Com-\nputing Machinery.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and optimizing lstm lan-\nguage models.\nShriphani Palakodety, Ashiqur R. KhudaBukhsh, and\nJaime G. Carbonell. 2020. Hope speech detection:\nA computational analysis of the voice of peace.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Advances in Neural Information Pro-\ncessing Systems, volume 32, pages 8026–8037. Cur-\nran Associates, Inc.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nRuba Priyadharshini, Bharathi Raja Chakravarthi,\nMani Vegupatti, and John P. McCrae. 2020. Named\nEntity Recognition for Code-Mixed Indian Corpus\nusing Meta Embedding. In 2020 6th International\nConference on Advanced Computing and Communi-\ncation Systems (ICACCS), pages 68–72.\nHammad Rizwan, Muhammad Haroon Shakeel, and\nAsim Karim. 2020. Hate-speech and offensive lan-\nguage detection in Roman Urdu. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 2512–\n2522, Online. Association for Computational Lin-\nguistics.\nSebastian Ruder, Anders Søgaard, and Ivan Vuli ´c.\n2019. Unsupervised cross-lingual representation\nlearning. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics:\nTutorial Abstracts, pages 31–38, Florence, Italy. As-\nsociation for Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter.\nM. Schuster and K. K. Paliwal. 1997. Bidirectional re-\ncurrent neural networks. IEEE Transactions on Sig-\nnal Processing, 45(11):2673–2681.\nMike Schuster and K. Paliwal. 1997. Bidirectional re-\ncurrent neural networks. IEEE Trans. Signal Pro-\ncess., 45:2673–2681.\nAlice Tontodimamma, Eugenia Nissi, Annalina Sarra,\nand Lara Fontanella. 2021. Thirty years of research\ninto hate speech: topics of interest and their evolu-\ntion. Scientometrics, 126(1):157–179.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing.\nY . Wu, Mike Schuster, Z. Chen, Quoc V . Le, Mo-\nhammad Norouzi, Wolfgang Macherey, M. Krikun,\nYuan Cao, Q. Gao, Klaus Macherey, Jeff Klingner,\nApurva Shah, M. Johnson, X. Liu, L. Kaiser,\nS. Gouws, Y . Kato, Taku Kudo, H. Kazawa,\nK. Stevens, G. Kurian, Nishant Patil, W. Wang,\nC. Young, J. Smith, Jason Riesa, Alex Rudnick,\nOriol Vinyals, G. S. Corrado, Macduff Hughes, and\nJ. Dean. 2016. Google’s neural machine translation\nsystem: Bridging the gap between human and ma-\nchine translation. ArXiv, abs/1609.08144.\nKonthala Yasaswini, Karthik Puranik, Adeep\nHande, Ruba Priyadharshini, Sajeetha Thava-\nreesan, and Bharathi Raja Chakravarthi. 2021.\nIIITT@DravidianLangTech-EACL2021: Transfer\nLearning for Offensive Language Detection in\nDravidian Languages. In Proceedings of the First\nWorkshop on Speech and Language Technolo-\ngies for Dravidian Languages . Association for\nComputational Linguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in Neural Information Pro-\ncessing Systems, volume 28, pages 649–657. Curran\nAssociates, Inc.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5174973011016846
    },
    {
      "name": "Computer science",
      "score": 0.3436206579208374
    },
    {
      "name": "Engineering",
      "score": 0.16738075017929077
    },
    {
      "name": "Electrical engineering",
      "score": 0.12431865930557251
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}