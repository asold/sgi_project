{
    "title": "Swin Transformer V2: Scaling Up Capacity and Resolution",
    "url": "https://openalex.org/W3212191691",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2184294903",
            "name": "Liu Ze",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117015939",
            "name": "Hu Han",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2753706201",
            "name": "Lin, Yutong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224962495",
            "name": "Yao, Zhuliang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2744126523",
            "name": "Xie, Zhenda",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3086230319",
            "name": "Wei, Yixuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1945286813",
            "name": "Ning Jia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2113229502",
            "name": "Cao Yue",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2003519119",
            "name": "Zhang Zheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097573093",
            "name": "Dong Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2389670735",
            "name": "Wei, Furu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3010309834",
            "name": "Guo, Baining",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3174738881",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W2507296351",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3175343838",
        "https://openalex.org/W3097217077",
        "https://openalex.org/W3172507542",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3214897340",
        "https://openalex.org/W3203701986",
        "https://openalex.org/W3211432419",
        "https://openalex.org/W3170197681",
        "https://openalex.org/W2963711743",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W2964080601",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2338908902",
        "https://openalex.org/W3037983807",
        "https://openalex.org/W3176258108",
        "https://openalex.org/W3170863103",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W3196107618",
        "https://openalex.org/W3171087525",
        "https://openalex.org/W2962900737",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W3109646990",
        "https://openalex.org/W3121334524",
        "https://openalex.org/W2619947201",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3213165621",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W3176153963",
        "https://openalex.org/W3173631098",
        "https://openalex.org/W3113747735",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W3129831491",
        "https://openalex.org/W2998228095",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3139633126",
        "https://openalex.org/W3172942063",
        "https://openalex.org/W3211749483",
        "https://openalex.org/W3135715136",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3212756788",
        "https://openalex.org/W3170943566",
        "https://openalex.org/W3111156583",
        "https://openalex.org/W3122159272",
        "https://openalex.org/W2983943451",
        "https://openalex.org/W2502312327",
        "https://openalex.org/W3164612304"
    ],
    "abstract": "Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536$\\times$1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \\url{https://github.com/microsoft/Swin-Transformer}.",
    "full_text": "Swin Transformer V2: Scaling Up Capacity and Resolution\nZe Liu* Han Hu*‚Ä† Yutong Lin Zhuliang Yao Zhenda Xie Yixuan Wei Jia Ning\nYue Cao Zheng Zhang Li Dong Furu Wei Baining Guo\nMicrosoft Research Asia\n{v-zeliu1,hanhu,t-yutonglin,t-zhuyao,t-zhxie,t-yixuanwei,v-jianing}@microsoft.com\n{yuecao,zhez,lidong1,fuwei,bainguo}@microsoft.com\nAbstract\nLarge-scale NLP models have been shown to signiÔ¨Å-\ncantly improve the performance on language tasks with\nno signs of saturation. They also demonstrate amazing\nfew-shot capabilities like that of human beings. This pa-\nper aims to explore large-scale models in computer vision.\nWe tackle three major issues in training and application of\nlarge vision models, including training instability, resolu-\ntion gaps between pre-training and Ô¨Åne-tuning, and hunger\non labelled data. Three main techniques are proposed:\n1) a residual-post-norm method combined with cosine at-\ntention to improve training stability; 2) A log-spaced con-\ntinuous position bias method to effectively transfer mod-\nels pre-trained using low-resolution images to downstream\ntasks with high-resolution inputs; 3) A self-supervised pre-\ntraining method, SimMIM, to reduce the needs of vast la-\nbeled images. Through these techniques, this paper suc-\ncessfully trained a 3 billion-parameter Swin Transformer\nV2 model, which is the largest dense vision model to date,\nand makes it capable of training with images of up to\n1,536√ó1,536 resolution. It set new performance records\non 4 representative vision tasks, including ImageNet-V2\nimage classiÔ¨Åcation, COCO object detection, ADE20K se-\nmantic segmentation, and Kinetics-400 video action clas-\nsiÔ¨Åcation. Also note our training is much more efÔ¨Åcient\nthan that in Google‚Äôs billion-level visual models, which con-\nsumes 40 times less labelled data and 40 times less train-\ning time. Code is available at https://github.com/\nmicrosoft/Swin-Transformer.\n1. Introduction\nScaling up language models has been incredibly success-\nful. It signiÔ¨Åcantly improves a model‚Äôs performance on lan-\nguage tasks [19, 24, 49, 50, 52, 53] and the model demon-\n*Equal. ‚Ä†Project lead. Ze, Yutong, Zhuliang, Zhenda, Yixuan, Jia are\nlong-term interns at MSRA.\nùíôùíç\nV1\nMLP\nùëæùëÑ ùëæùêæ ùëæùëâ\nùíí ùíå\nùííùíåùëª\nùêµ\nùíÉ\nSoftmax\nùíó\nParameterized\nRPB ùíõ\nùíõ‚Ä≤\nLayer Norm\nAttention\nLayer Norm\nùíôùíç‚àíùüè\nV2\nùíôùíç\nLayer Norm\nùëæùëÑ ùëæùêæ ùëæùëâ\nùíí ùíå\nùíÑùíêùíîùíäùíèùíÜ(ùíí,ùíå)/ùùâ\n‚àÜùë•,‚àÜùë¶\nùíÉ\nSoftmax\nùíó\nùíõ\nùíõ‚Ä≤\nMLP\nLayer Norm\nAttention\nMLP\nùíôùíç‚àíùüè\nLog-CPB\nlog\nFigure 1. To better scale up model capacity and window resolu-\ntion, several adaptions are made on the original Swin Transformer\narchitecture (V1): 1) A res-post-norm to replace the previous pre-\nnorm conÔ¨Åguration; 2) A scaled cosine attention to replace the\noriginal dot product attention; 3) A log-spaced continuous relative\nposition bias approach to replace the previous parameterized ap-\nproach. Adaptions 1) and 2) make it easier for the model to scale\nup capacity. Adaption 3) makes the model to be transferred more\neffectively across window resolutions. The adapted architecture is\nnamed Swin Transformer V2.\nstrates amazing few-shot capabilities similar to that of hu-\nman beings [7]. Since the BERT large model with 340 mil-\nlion parameters [19], language models are quickly scaled\nup by more than 1,000 times in a few years, reaching 530\nbillion dense parameters [50] and 1.6 trillion sparse param-\neters [24]. These large language models are also found to\npossess increasingly strong few-shot capabilities akin to hu-\nman intelligence for a broad range of language tasks [7].\nOn the other hand, the scaling up of vision models has\nbeen lagging behind. While it has long been recognized\nthat larger vision models usually perform better on vision\ntasks [29,60], the absolute model size was just able to reach\nabout 1-2 billion parameters very recently [17,27,39,56,80].\nMore importantly, unlike large language models, the exist-\narXiv:2111.09883v2  [cs.CV]  11 Apr 2022\ning large vision models are applied to the image classiÔ¨Åca-\ntion task only [17, 56, 80].\nTo successfully train large and general vision model, we\nneed to address a few key issues. Firstly, our experiments\nwith large vision models reveal an instability issue in train-\ning. We Ô¨Ånd that the discrepancy of activation amplitudes\nacross layers becomes signiÔ¨Åcantly greater in large models.\nA closer look at the original architecture reveals that this is\ncaused by the output of the residual unit directly added back\nto the main branch. The result is that the activation values\nare accumulated layer by layer, and the amplitudes at deeper\nlayers are thus signiÔ¨Åcantly larger than those at early lay-\ners. To address this issue, we propose a new normalization\nconÔ¨Åguration, called res-post-norm, which moves the LN\nlayer from the beginning of each residual unit to the back-\nend, as shown in Figure 1. We Ô¨Ånd this new conÔ¨Åguration\nproduces much milder activation values across the network\nlayers. We also propose a scaled cosine attention to replace\nthe previous dot product attention. The scaled cosine at-\ntention makes the computation irrelevant to amplitudes of\nblock inputs, and the attention values are less likely to fall\ninto extremes. In our experiments, the proposed two tech-\nniques not only make the training process more stable but\nalso improve the accuracy especially for larger models.\nSecondly, many downstream vision tasks such as object\ndetection and semantic segmentation require high resolu-\ntion input images or large attention windows. The win-\ndow size variations between low-resolution pre-training and\nhigh-resolution Ô¨Åne-tuning can be quite large. The current\ncommon practice is to perform a bi-cubic interpolation of\nthe position bias maps [22, 46]. This simple Ô¨Åx is some-\nwhat ad-hoc and the result is usually sub-optimal. We in-\ntroduce a log-spaced continuous position bias (Log-CPB),\nwhich generates bias values for arbitrary coordinate ranges\nby applying a small meta network on the log-spaced co-\nordinate inputs. Since the meta network takes any coor-\ndinates, a pre-trained model will be able to freely transfer\nacross window sizes by sharing weights of the meta net-\nwork. A critical design of our approach is to transform\nthe coordinates into the log-space so that the extrapolation\nratio can be low even when the target window size is sig-\nniÔ¨Åcantly larger than that of pre-training. The scaling up\nof model capacity and resolution also leads to prohibitively\nhigh GPU memory consumption with existing vision mod-\nels. To resolve the memory issue, we incorporate several\nimportant techniques including zero-optimizer [54], activa-\ntion check pointing [12] and a novel implementation of se-\nquential self-attention computation. With these techniques,\nthe GPU memory consumption of large models and resolu-\ntions is signiÔ¨Åcantly reduced with only marginal effect on\nthe training speed.\nWith the above techniques, we successfully trained a 3\nbillion Swin Transformer model and effectively transferred\nit to various vision tasks with image resolution as large as\n1,536√ó1,536, using Nvidia A100-40G GPUs. In our model\npre-training, we also employ self-supervised pre-training to\nreduce the dependency on super-huge labeled data. With\n40√óless labelled data than that in previous practice (JFT-\n3B), the 3 billion model achieves the state-of-the-art accu-\nracy on a broad range of vision benchmarks. SpeciÔ¨Åcally,\nit obtains 84.0% top-1 accuracy on the ImageNet-V2 image\nclassiÔ¨Åcation validation set [55], 63.1 / 54.4 box / mask AP\non the COCO test-dev set of object detection, 59.9 mIoU\non ADE20K semantic segmentation, and 86.8% top-1 accu-\nracy on Kinetics-400 video action classiÔ¨Åcation, which are\n+NA%, +4.4/+3.3, +6.3 and +1.9 higher than the best num-\nbers in the original Swin Transformers [46,47], and surpass\nprevious best records by +0.8% ( [80]), +1.8/+1.4 ( [74]),\n+1.5 ( [4]) and +1.4% ( [57]).\nBy scaling up both capacity and resolution of vision\nmodels with strong performance on general vision tasks,\njust like a good language model‚Äôs performance on general\nNLP tasks, we aim to stimulate more research in this di-\nrection so that we can eventually close the capacity gap be-\ntween vision and language models and facilitate the joint\nmodeling of the two domains.\n2. Related Works\nLanguage networks and scaling up Transformer has\nserved the standard network since the pioneer work of [65].\nThe exploration of scaling this architecture has since begun,\nand the progress has been accelerated by the invention of ef-\nfective self-supervised learning approaches, such as masked\nor auto-regressive language modeling [19,52], and has been\nfurther encouraged by the discovery of a scaling law [36].\nSince then, the capacity of language models has increased\ndramatically by more than 1,000 times in a few years, from\nBERT-340M to the Megatron-Turing-530B [7, 49, 50, 53]\nand sparse Switch-Transformer-1.6T [24]. With increased\ncapacity, the accuracy of various language benchmarks has\nbeen signiÔ¨Åcantly improved. The zero-shot or few-shot per-\nformance is also signiÔ¨Åcantly improved [7], which is a foun-\ndation of human generic intelligence.\nVision networks and scaling up CNNs have long been\nthe standard computer vision networks [40, 41]. Since\nAlexNet [40], architectures have become deeper and larger,\nwhich has greatly advanced various visual tasks and largely\nfueled the wave of deep learning in computer vision, such as\nVGG [60], GoogleNet [62] and ResNet citehe2015resnet.\nIn the past two years, the CNN architectures have been fur-\nther scaled up to about 1 billion parameters [27, 39], how-\never, absolute performance may not be so encouraging, per-\nhaps due to inductive biases in the CNN architecture limit-\ning modeling power.\nLast year, Transformers started taking over one represen-\ntative visual benchmark after another, including ImageNet-\n1K image-level classiÔ¨Åcation benchmarks [22], COCO\nregion-level object detection benchmark [46], ADE20K\npixel-level semantic segmentation benchmark [46, 83],\nKinetics-400 video action classiÔ¨Åcation benchmark [2], etc.\nSince these works, numerous vision Transformer variants\nhave been proposed to improve the accuracy at relatively\nsmall scale [14, 21, 34, 42, 63, 68, 71, 75, 77, 78, 82]. Only a\nfew works have attempted to scale up the vision Transform-\ners [17,56,80]. However, they rely on a huge image dataset\nwith classiÔ¨Åcation labels, i.e., JFT-3B, and are only applied\nto image classiÔ¨Åcation problems.\nTransferring across window / kernel resolution For\nCNNs, previous works typically Ô¨Åxed kernel size during\npre-training and Ô¨Åne-tuning. Global vision Transformers,\nsuch as ViT [22], compute attention globally, with the\nequivalent attention window size linearly proportional to the\nincreased input image resolution. For local vision Trans-\nformer architectures, such as Swin Transformer [46], the\nwindow size can be either Ô¨Åxed or changed during Ô¨Åne-\ntuning. Allowing variable window sizes is more convenient\nin use, so as to be divisible by the probably variable en-\ntire feature map and to tune receptive Ô¨Åelds for better ac-\ncuracy. To handle the variable window sizes between pre-\ntraining and Ô¨Åne-tuning, bi-cubic interpolation was the pre-\nvious common practice [22, 46]. In this paper, we propose\na log-spaced continuous position bias approach (Log-CPB)\nthat more smoothly transfers pre-trained model weights at\nlow resolution to deal-with higher resolution windows.\nStudy on bias terms In NLP, the relative position bias\nmethod proved beneÔ¨Åcial [53], compared to the absolute\nposition embedding used in the original Transformer [65].\nIn computer vision, the relative positional bias method is\nmore commonly used [31,46,75], probably because the spa-\ntial relationships of visual signals play a more important\nrole in visual modeling. A common practice is to directly\nlearn the bias values as model weights. There are also a\nfew works particularly study how to set and learn the bias\nterms [38, 69].\nContinuous convolution and variants Our Log-CPB ap-\nproach is also related to earlier works on continuous convo-\nlution and variants [30, 45, 58, 67], which utilize a meta\nnetwork to handle irregular data points. Our Log-CPB ap-\nproach is inspired by these efforts while solving a differ-\nent problem of transferring relative position biases in vision\nTransformers across arbitrary window sizes. We also pro-\npose log-spaced coordinates to alleviate the difÔ¨Åculty of ex-\ntrapolation when transferring between large size changes.\n3. Swin Transformer V2\n3.1. A Brief Review of Swin Transformer\nSwin Transformer is a general-purpose computer vision\nbackbone that has achieved strong performance in vari-\nous granular recognition tasks such as region-level object\ndetection, pixel-level semantic segmentation, and image-\nlevel image classiÔ¨Åcation. The main idea of Swin Trans-\nformer is to introduce several important visual priors into\nthe vanilla Transformer encoder, including hierarchy, local-\nity, and translation invariance, which combines the strength\nof both: the basic Transformer unit has strong modeling ca-\npabilities, and the visual priors make it friendly to a variety\nof visual tasks.\nNormalization conÔ¨Åguration It is widely known that\nnormalization technologies [3, 35, 64, 70] are crucial in\nstably training deeper architectures. The original Swin\nTransformer inherits the common practice in the language\nTransformers [52] and vanilla ViT [22] to utilize a pre-\nnormalization conÔ¨Åguration without extensive study, as\nshown in the Ô¨Ågure 1. In the following subsections, we will\nexamine this default normalization conÔ¨Åguration1.\nRelative position bias is a key component in the original\nSwin Transformer which introduces an additional paramet-\nric bias term to encode the geometric relationship in self-\nattention calculation:\nAttention(Q,K,V ) = SoftMax(QKT /\n‚àö\nd+ B)V, (1)\nwhere B ‚ààRM2√óM2\nis the relative position bias term for\neach head; Q,K,V ‚ààRM2√ód are the query, key and value\nmatrices; dis the query/key dimension, and M2 is the num-\nber of patches in a window. The relative position bias en-\ncodes relative spatial conÔ¨Ågurations of visual elements and\nis shown critical in a variety of visual tasks, especially for\ndense recognition tasks such as object detection.\nIn Swin Transformer, the relative positions along each\naxis are within the range of [‚àíM + 1,M ‚àí1] and the rel-\native position bias is parameterized as a bias matrix ÀÜB ‚àà\nR(2M‚àí1)√ó(2M‚àí1), and the elements in Bare taken from ÀÜB.\nWhen transferring across different window sizes, the learnt\nrelative position bias matrix in pre-training is used to ini-\ntialize the bias matrix of a different size in Ô¨Åne-tuning by\nbi-cubic interpolation.\nIssues in scaling up model capacity and window resolu-\ntion We observe two issues when we scale up the capacity\nand window resolution of the Swin Transformer.\n1There have been a few alternative normalization conÔ¨Ågurations, such\nas post-normalization [65] and sandwich normalization [20]. Post-\nnormalization harms training stability [73], and sandwich normalization\nsacriÔ¨Åces representation power due to too many normalization layers.\nImageNet* ImageNet‚Ä† COCO ADE20k\nmethod W8, I256\ntop-1 acc\nW12, I384\ntop-1 acc\nW16, I512\ntop-1 acc\nW20, I640\ntop-1 acc\nW24, I768\ntop-1 acc\nW16\nAPbox\nW32\nAPbox\nW16\nmIoU\nW20\nmIoU\nW32\nmIoU\nParameterized position bias [46] 81.7 79.4/82.7 77.2/83.0 73.2/83.2 68.7/83.2 50.8 50.9 45.5 45.8 44.5\nLinear-Spaced CPB 81.7\n(+0.0)\n82.0/82.9\n(+2.6/+0.2)\n81.2/83.3\n(+4.0/+0.3)\n79.8/83.6\n(+6.6/+0.4)\n77.6/83.7\n(+8.9/+0.5)\n50.9\n(+0.1)\n51.7\n(+0.8)\n47.0\n(+1.5)\n47.4\n(+1.6)\n47.2\n(+2.7)\nLog-Spaced CPB 81.8\n(+0.1)\n82.4/83.2\n(+3.0/+0.5)\n81.7/83.8\n(+4.5/+0.8)\n80.4/84.0\n(+7.2/+0.8)\n79.1/84.2\n(+10.4/+1.0)\n51.1\n(+0.3)\n51.8\n(+0.9)\n47.0\n(+1.5)\n47.7\n(+1.9)\n47.8\n(+3.3)\nTable 1. Comparison of different position bias computation approaches using Swin-T. * indicates the top-1 accuracy on ImageNet-1k\ntrained from scratch. The models in * column will be used for testing on the ImageNet-1K image classiÔ¨Åcation task using larger im-\nage/window resolutions, marked by ‚Ä†. For these results, we report both the results w.o./with Ô¨Åne-tuning. These models are also used for\nÔ¨Åne-tuning on COCO object detection and ADE20K semantic segmentation tasks.\n1.0E-02\n1.0E+00\n1.0E+02\n1.0E+04\n1.0E+06\n1.0E+08\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nAverage Feature Variance\nS-Post B-Post L-Post H-Post (SSL)\nS-Pre B-Pre L-Pre H-Pre (SSL*)\nBlock-ID\nFigure 2. The Signal Propagation Plot [6, 76] for various model\nsizes. H-size models are trained at a self-supervised learning\nphase, and other sizes are trained by an image classiÔ¨Åcation task.\n* indicates that we use a 40-epoch model before it crashes.\n‚Ä¢ An instability issue when scaling up model capacity .\nAs shown in Figure 2, when we scale up the original\nSwin Transformer model from small size to large size,\nthe activation values at deeper layers increase dramat-\nically. The discrepancy between layers with the high-\nest and the lowest amplitudes has reached an extreme\nvalue of 104. When we scale it up further to a huge\nsize (658 million parameters), it cannot complete the\ntraining, as shown in Figure 3.\n‚Ä¢ Degraded performance when transferring models\nacross window resolutions . As shown in the Ô¨Årst\nrow of Table 1, the accuracy decreases signiÔ¨Åcantly\nwhen we directly test the accuracy of a pre-trained\nImageNet-1K model ( 256 √ó256 images with 8 √ó8\nwindow size) at larger image resolutions and window\nsizes through the bi-cubic interpolation approach. It\nmay be worth re-examining the relative position bias\napproach in the original Swin Transformer.\nIn the following subsections, we present techniques to\naddress these issues, including residual post normalization\nand scaled cosine attention to address the instability issue,\nand a log-spaced continuous position bias approach to ad-\ndress the issue in transferring across window resolutions.\n3.2. Scaling Up Model Capacity\nAs mentioned in Section 3.1, the original Swin Trans-\nformer (and most vision Transformers) adopts a layer norm\nlayer at the beginning of each block, inherited from vanilla\nViT. When we scale up the model capacity, a signiÔ¨Åcant in-\ncrease in activation values is observed at deeper layers. In\nfact, in a pre-normalization conÔ¨Åguration, the output activa-\ntion values of each residual block are merged directly back\nto the main branch, and the amplitude of the main branch\ngrows larger and larger at deeper layers. Large amplitude\ndiscrepancy in different layers causes training instability.\nPost normalization To ease this problem, we propose\nto use a residual post normalization approach instead, as\nshown in Figure 1. In this approach, the output of each\nresidual block is normalized before merging back into the\nmain branch, and the amplitude of the main branch does not\naccumulate when the layer goes deeper. As shown in Fig-\nure 2, the activation amplitudes by this approach are much\nmilder than in the original pre-normalization conÔ¨Åguration.\nIn our largest model training, we introduce an additional\nlayer normalization layer on the main branch every 6 Trans-\nformer blocks, to further stabilize training.\nScaled cosine attention In the original self-attention\ncomputation, the similarity terms of the pixel pairs are com-\nputed as a dot product of the query and key vectors. We\nÔ¨Ånd that when this approach is used in large visual mod-\nels, the learnt attention maps of some blocks and heads are\nfrequently dominated by a few pixel pairs, especially in the\nres-post-norm conÔ¨Åguration. To ease this issue, we propose\na scaled cosine attention approach that computes the atten-\ntion logit of a pixel pair iand jby a scaled cosine function:\nSim(qi,kj) = cos(qi,kj)/œÑ + Bij, (2)\nwhere Bij is the relative position bias between pixeliand j;\nœÑ is a learnable scalar, non-shared across heads and layers.\nœÑ is set larger than 0.01. The cosine function is naturally\nnormalized, and thus can have milder attention values.\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n100 200 300 400 500 600 700 800\nTraining Loss\nH-Pre (SSL)\nH-Post (SSL)\nEpoch\nFigure 3. SwinV1-H versus SwinV2-H in training [72].\n3.3. Scaling Up Window Resolution\nIn this subsection, we introduce a log-spaced continuous\nposition bias approach, so that the relative position bias can\nbe smoothly transferred across window resolutions.\nContinuous relative position bias Instead of directly op-\ntimizing the parameterized biases, the continuous position\nbias approach adopts a small meta network on the relative\ncoordinates:\nB(‚àÜx,‚àÜy) = G(‚àÜx,‚àÜy), (3)\nwhere Gis a small network, e.g., a 2-layer MLP with a\nReLU activation in between by default.\nThe meta network Ggenerates bias values for arbitrary\nrelative coordinates, and thus can be naturally transferred\nto Ô¨Åne-tuning tasks with arbitrarily varying window sizes.\nIn inference, the bias values at each relative position can\nbe pre-computed and stored as model parameters, such that\nthe inference is the same as the original parameterized bias\napproach.\nLog-spaced coordinates When transferring across\nlargely varying window sizes, a large portion of the relative\ncoordinate range needs to be extrapolated. To ease this\nissue, we propose using log-spaced coordinates instead of\nthe original linear-spaced ones:\nÀÜ‚àÜx= sign(x) ¬∑log(1 + |‚àÜx|),\nÀÜ‚àÜy= sign(y) ¬∑log(1 + |‚àÜy|),\n(4)\nwhere ‚àÜx, ‚àÜy and ÀÜ‚àÜx, ÀÜ‚àÜy are the linear-scaled and log-\nspaced coordinates, respectively.\nBy using the log-spaced coordinates, when we transfer\nthe relative position biases across window resolutions, the\nrequired extrapolation ratio will be much smaller than that\nof using the original linear-spaced coordinates. For an ex-\nample of transferring from a pre-trained 8 √ó8 window size\nto a Ô¨Åne-tuned 16 √ó16 window size, using the original\nraw coordinates, the input coordinate range will be from\n[‚àí7,7]√ó[‚àí7,7] to [‚àí15,15]√ó[‚àí15,15]. The extrapolation\nratio is 8\n7 = 1.14√óof the original range. Using log-spaced\ncoordinates, the input range will be from [‚àí2.079,2.079] √ó\n[‚àí2.079,2.079] to [‚àí2.773,2.773] √ó[‚àí2.773,2.773]. The\nextrapolation ratio is 0.33√óof the original range, which is\nan about 4 times smaller extrapolation ratio than that using\nthe original linear-spaced coordinates.\nTable 1 compares the transferring performance of dif-\nferent position bias computation approaches. It can be\nseen that the log-spaced CPB (continuous position bias)\napproach performs best, particularly when transferred to\nlarger window sizes.\n3.4. Self-Supervised Pre-training\nLarger models are more data hungry. To address the\ndata hungry problem, previous large vision models typically\nutilize huge labelled data such as JFT-3B [17, 56, 80]. In\nthis work, we exploit a self-supervised pre-training method,\nSimMIM [72], to alleviate the demands on labelled data.\nBy this approach, we successfully trained a powerful Swin\nTransformer model of 3 billion parameters which achieves\nstate-of-the-art (SOTA) on 4 representative visual bench-\nmarks, by using only 70 million labelled images (1/40 of\nthat in JFT-3B).\n3.5. Implementation to Save GPU Memory\nAnother issue lies in the unaffordable GPU memory con-\nsumption with a regular implementation when both the ca-\npacity and resolution are large. To facility the memory is-\nsue, we adopt the following implementations:\n‚Ä¢ Zero-Redundancy Optimizer (ZeRO) [54]. In a general\ndata-parallel implementation of optimizers, the model\nparameters and optimization states are broadcasted to\nevery GPU. This implementation is very unfriendly on\nGPU memory consumption, for example, a model of\n3 billion parameters will consume 48G GPU memory\nwhen an AdamW optimizer and fp32 weights/states\nare used. With a ZeRO optimizer, the model param-\neters and the corresponding optimization states will be\nsplit and distributed to multiple GPUs, which signif-\nicantly reduces memory consumption. We adopt the\nDeepSpeed framework and use the ZeRO stage-1 op-\ntion in our experiments. This optimization has little\neffect on training speed.\n‚Ä¢ Activation check-pointing [12]. Feature maps in the\nTransformer layers also consume a lot of GPU mem-\nory, which can create bottlenecks when image and\nwindow resolutions are high. The activation check-\npointing technology can signiÔ¨Åcantly reduce the mem-\nory consumption, while the training speed is up to 30%\nslower.\n‚Ä¢ Sequential self-attention computation . To train large\nmodels on very large resolutions, for example, an im-\nage of 1,536√ó1,536 resolution with a window size of\n32√ó32, regular A100 GPUs (40GB memory) are still\nunaffordable, even with the above two optimization\ntechnologies. We found that in this case, the self-\nattention module constitutes a bottleneck. To alleviate\nthis problem, we implement self-attention computation\nsequentially, instead of using the previous batch com-\nputation approach. This optimization is applied to the\nlayers in the Ô¨Årst two stages and has little impact on\nthe overall training speed.\nWith these implementations, we managed to train a 3B\nmodel using the Nvidia A100-40G GPUs for COCO object\ndetection with an input image resolution of 1,536 √ó1,536,\nand Kinetics-400 action classiÔ¨Åcation with an input resolu-\ntion of 320 √ó320 √ó8.\n3.6. Model conÔ¨Ågurations\nWe maintain the stage, block, and channel settings of\nthe original Swin Transformer for 4 conÔ¨Ågurations of Swin\nTransformer V2:\n‚Ä¢ SwinV2-T: C= 96, #. block = {2,2,6,2}\n‚Ä¢ SwinV2-S/B/L: C=96/128/192, #.block={2,2,18,2}\nwith Cthe number of channels in the Ô¨Årst stage.\nWe further scale up Swin Transformer V2 to its huge size\nand giant size, with 658 million parameters and 3 billion\nparameters, respectively:\n‚Ä¢ SwinV2-H: C = 352, #. block = {2,2,18,2}\n‚Ä¢ SwinV2-G: C = 512, #. block = {2,2,42,4}\nFor SwinV2-H and SwinV2-G, we add an additional layer\nnormalization layer on the main branch every 6 layers. To\nsave experimental time, we only employ SwinV2-G for\nlarge-scale experiments. SwinV2-H is employed for an-\nother parallel study about self-supervised learning [72].\n4. Experiments\n4.1. Tasks and Datasets\nWe conduct experiments on ImageNet-1K image clas-\nsiÔ¨Åcation (V1 and V2) [18, 55], COCO object detec-\ntion [44], and ADE20K semantic segmentation [85]. For\nthe 3B model experiments, we also report the accuracy on\nKinetics-400 video action recognition [37].\n‚Ä¢ Image classiÔ¨Åcation. ImageNet-1K V1 and V2 val are\nused [18,55] for evaluation. ImageNet-22K [18] which\nhas 14M images and 22K categories is optionally em-\nployed for pre-training. For the pre-training our largest\nmodel SwinV2-G, a privately collected ImageNet-\n22K-ext dataset with 70 million images is used. For\nthis dataset, a duplicate removal process [51] is con-\nducted to exclude overlapping images with ImageNet-\n1K V1 and V2 validation sets.\n‚Ä¢ Object detection. COCO [44] is used for evaluation.\nFor our largest model experiments, we employ an addi-\ntional detection pre-training phase using Object 365 v2\ndataset [59], in-between the image classiÔ¨Åcation pre-\ntraining phase and the COCO Ô¨Åne-tuning phase.\n‚Ä¢ Semantic segmentation. ADE20K [85] is used.\n‚Ä¢ Video action classiÔ¨Åcation. Kinetics-400 (K400) [37]\nis used in evaluation.\nThe pre-training and Ô¨Åne-tuning settings will be detailed\nin Appendix.\n4.2. Scaling Up Experiments\nWe Ô¨Årst present the results on various representative vi-\nsual benchmarks by scaling up models to 3 billion parame-\nters and to high image/window resolutions.\nSettings for SwinV2-G experiments We adopt a smaller\n192 √ó192 image resolution in pre-training to save on train-\ning costs. We take a 2-step pre-training approach. First, the\nmodel is pre-trained using a self-supervised method [72] on\nthe ImageNet-22K-ext dataset by 20 epochs. Second, the\nmodel is further pre-trained by 30 epochs using the image\nclassiÔ¨Åcation task on this dataset. Detailed pre-training and\nÔ¨Åne-tuning setups are described in the appendix.\nIn the following paragraphs, we report the accuracy of\nSwinV2-G on representative vision benchmarks. Note that\nsince our main goal is to explore how to feasibly scale up\nmodel capacity and window resolution, and whether the vi-\nsion tasks can beneÔ¨Åt from signiÔ¨Åcantly larger capacity, we\ndid not particularly align complexities or pre-training data\nin comparisons.\nImageNet-1K image classiÔ¨Åcation results Table 2 com-\npares the SwinV2-G model with previously largest/best vi-\nsion models on ImageNet-1K V1 and V2 classiÔ¨Åcation.\nSwinV2-G is the largest dense vision model to present. It\nachieves a top-1 accuracy of 84.0% on the ImageNet V2\nbenchmark, which is +0.7% higher than previous best one\n(83.3%). Our accuracy on ImageNet-1K V1 is marginally\nlower (90.17% vs 90.88%). The performance differ-\nence might come from different degrees of dataset over-\ntuning [55]. Also note we employ much less training it-\nerations and lower image resolutions than those in previous\nefforts, while performing very well.\nMethod param pre-train\nimages\npre-train\nlength (#im)\npre-train\nim size\npre-train\ntime\nÔ¨Åne-tune\nim size\nImageNet-1K-V1\ntop-1 acc\nImaegNet-1K-V2\ntop-1 acc\nSwinV1-B 88M IN-22K-14M 1.3B 224 2 <30‚Ä† 3842 86.4 76.58\nSwinV1-L 197M IN-22K-14M 1.3B 224 2 <10‚Ä† 3842 87.3 77.46\nViT-G [80] 1.8B JFT-3B 164B 224 2 >30k 518 2 90.45 83.33\nV-MoE [56] 14.7B* JFT-3B - 224 2 16.8k 518 2 90.35 -\nCoAtNet-7 [17] 2.44B JFT-3B - 224 2 20.1k 512 2 90.88 -\nSwinV2-B 88M IN-22K-14M 1.3B 192 2 <30‚Ä† 3842 87.1 78.08\nSwinV2-L 197M IN-22K-14M 1.3B 192 2 <20‚Ä† 3842 87.7 78.31\nSwinV2-G 3.0B IN-22K-ext-70M 3.5B 192 2 <0.5k‚Ä† 6402 90.17 84.00\nTable 2. Comparison with previous largest vision models on ImageNet-1K V1 and V2 classiÔ¨Åcation. * indicates the sparse model; the\n‚Äúpre-train time‚Äù column is measured by the TPUv3 core days with numbers copied from the original papers. ‚Ä†That of SwinV2-G is\nestimated according to training iterations and FLOPs.\nMethod train\nI(W) size\ntest\nI(W) size\nmini-val (AP) test-dev (AP)\nbox mask box mask\nCopyPaste [25] 1280(-) 1280(-) 57.0 48.9 57.3 49.1\nSwinV1-L [46] 800(7) ms(7) 58.0 50.4 58.7 51.1\nYOLOR [66] 1280(-) 1280(-) - - 57.3 -\nCBNet [43] 1400(7) ms(7) 59.6 51.8 60.1 52.3\nDyHead [16] 1200(-) ms(-) 60.3 - 60.6 -\nSoftTeacher [74] 1280(12) ms(12) 60.7 52.5 61.3 53.0\nSwinV2-L\n(HTC++) 1536(32)\n1100(32) 58.8 51.1 - -\n1100 (48) 58.9 51.2 - -\nms (48) 60.2 52.1 60.8 52.7\nSwinV2-G\n(HTC++) 1536(32)\n1100(32) 61.7 53.3 - -\n1100 (48) 61.9 53.4 - -\nms (48) 62.5 53.7 63.1 54.4\nTable 3. Comparison with previous best results on COCO object\ndetection and instance segmentation. I(W) indicates the image and\nwindow size. ms indicate multi-scale testing is employed.\nMethod train I(W) size test I(W) size mIoU\nSwinV1-L [46] 640(7) 640(7) 53.5*\nFocal-L [75] 640(40) 640(40) 55.4*\nCSwin-L [21] 640(40) 640(40) 55.7*\nMaskFormer [13] 640(7) 640(7) 55.6*\nFaPN [33] 640(7) 640(7) 56.7*\nBEiT [4] 640(40) 640(40) 58.4*\nSwinV2-L\n(UperNet) 640(40) 640(40) 55.9*\nSwinV2-G\n(UperNet) 640(40)\n640(40) 59.1\n896 (56) 59.3\n896 (56) 59.9*\nTable 4. Comparison with previous best results on ADE20K se-\nmantic segmentation. * indicates multi-scale testing is used.\nWe also compare the SwinV2-B and SwinV2-L to the\noriginal SwinV1-B and SwinV1-L, respectively, where a\n+0.8% and +0.4% gains are observed. The shrunken gains\nby SwinV2-L than that of SwinV2-B may imply that if ex-\nceeding this size, more labeled data, stronger regularization,\nor advanced self-supervised learning methods are required.\nMethod train I(W) size test I(W) size views top-1\nViViT [2] -(-) -(-) 4 √ó3 84.8\nSwinV1-L [47] 480(12)2√ó16(8) 480(12) 2√ó16(8) 10√ó5 84.9\nTokenLearner [57] 256(8)2√ó64(64) 256(8) 2√ó64(64) 4√ó3 85.4\nVideo-SwinV2-G 320(20)2√ó8(8)\n320(20)2√ó8(8) 1√ó1 83.2\n384(24)2√ó8(8) 1√ó1 83.4\n384(24)2√ó8(8) 4√ó5 86.8\nTable 5. Comparison with previous best results on Kinetics-400\nvideo action classiÔ¨Åcation.\nCOCO object detection results Table 3 compares the\nSwinV2-G model with previous best results on COCO\nobject detection and instance segmentation. It achieves\n63.1/54.4 box/max AP on COCO test-dev, which is +1.8/1.4\nhigher than previous best numberw (61.3/53.0 by [74]).\nThis suggests that scaling up vision model is beneÔ¨Åcial for\nthe dense vision recognition task of object detection. Our\napproach can use a different window size at test to addition-\nally beneÔ¨Åt, probably attributed to the effective Log-spaced\nCPB approach.\nADE20K semantic segmentation results Table 4 com-\npares the SwinV2-G model with previous best results on the\nADE20K semantic segmentation benchmark. It achieves\n59.9 mIoU on ADE20K val set, +1.5 higher than the pre-\nvious best number (58.4 by [4]). This suggests scaling up\nvision model is beneÔ¨Åcial for pixel-level vision recognition\ntasks. Using a larger window size at test time can addition-\nally bring +0.2 gains, probably attributed to the effective\nLog-spaced CPB approach.\nKinetics-400 video action classiÔ¨Åcation results Table 5\ncompares the SwinV2-G model with previous best results\non the Kinetics-400 action classiÔ¨Åcation benchmark. It\nachieves 86.8% top-1 accuracy, +1.4% higher than previous\nbest number [57]. This suggests that scaling up vision mod-\nels also beneÔ¨Åts video recognition tasks. In this scenario,\nusing a larger window size at test time can also bring addi-\ntional beneÔ¨Åts of +0.2%, probably attributed to the effective\nBackbone res-post-norm scaled cosine\nattention\nImageNet\ntop-1 acc\nSwin-T\n81.5\n‚úì 81.6\n‚úì ‚úì 81.7\nSwin-S\n83.2\n‚úì 83.3\n‚úì ‚úì 83.6\nSwin-B\n83.6\n‚úì 83.8\n‚úì ‚úì 84.1\nViT-B 82.2\n‚úì ‚úì 82.6\nTable 6. Ablation on res-post-norm and cosine attention.\nBackbone pre-norm sandwich [20] post-norm [65] our\nSwin-S 83.2 82.6 83.3 83.6\nSwin-B 83.6 - 83.6 84.1\nTable 7. Comparison with other normalization methods. The post-\nnorm method diverges at the default learning rate, and we use 1/4\nof the default learning rate for this method. Sandwich performs\nworse than ours, probably because it sacriÔ¨Åces expressiveness.\nLog-spaced CPB approach.\n4.3. Ablation Study\nAblation on res-post-norm and scaled cosine attention\nTable 6 ablates the performance of applying the proposed\nres-post-norm and scaled cosine attention approaches to\nSwin Transformer. Both techniques improve the accuracy\nat all the tiny, small and base size, and the overall improve-\nments are +0.2%, +0.4% and +0.5% respectively, indicat-\ning the techniques are more beneÔ¨Åcial for larger models.\nIt also turns out to beneÔ¨Åt ViT architecture (+0.4%). The\nproposed normalization approach also performs better than\nsome other normalization methods, as shown in Table 7.\nMore importantly, the combination of post-norm and\nscaled cosine attention stabilize the training. As shown in\nFigure 2, while the activation values at deeper layers for the\noriginal Swin Transformer are almost exploded at large (L)\nsize, those of the new version have much milder behavior.\nOn a huge size model, the self-supervised pre-training [72]\ndiverges using the original Swin Transformer, while it trains\nwell by a Swin Transformer V2 model.\nScaling up window resolution by different approaches\nTable 1 and 8 ablate the performance of 3 approaches\nby scaling window resolutions from 256 √ó256 in pre-\ntraining to larger sizes in 3 down-stream vision tasks of\nImageNet-1K image classiÔ¨Åcation, COCO object detection,\nand ADE20K semantic segmentation, respectively. It can be\nseen that: 1) Different approaches have similar accuracy in\npre-training (81.7%-81.8%); 2) When transferred to down-\nstream tasks, the two continuous position bias (CPB) ap-\nImageNet* ImageNet‚Ä†\nBackbone L-CPB W8, I256 W12, I384 W16, I512\nSwinV2-S 83.7 81.8/84.5 79.4/84.9\n‚úì 83.7 84.1/84.8 82.9/85.4\nSwinV2-B 84.1 82.9/85.0 81.0/85.3\n‚úì 84.2 84.5/85.1 83.8/85.6\nTable 8. Ablation on Log-CPB using different model sizes.\nproaches perform consistently better than the parameterized\nposition bias approach used in Swin Transformer V1. Com-\npared to the linear-spaced approach, the log-spaced version\nis marginally better; 3) The larger the change in resolutions\nbetween pre-training and Ô¨Åne-tuning, the larger the beneÔ¨Åt\nof the proposed log-spaced CPB approach.\nIn Table 1 and 8, we also report the accuracy using tar-\ngeted window resolutions without Ô¨Åne-tuning (see the Ô¨Årst\nnumber in each column in the ImageNet-1K experiments).\nThe recognition accuracy remains not bad even when the\nwindow size is enlarged from8 to 24 (78.9% versus 81.8%),\nwhile the top-1 accuracy of the original approach signiÔ¨Å-\ncantly degrades from 81.7% to 68.7%. Also note that with-\nout Ô¨Åne-tuning, using a window size of 12 that the pre-\ntrained model has never seen before can even be +0.4%\nhigher that the original accuracy. This suggests that we can\nimprove accuracy through test-time window adjustment, as\nalso observed in Table 3, 4 and 5.\n5. Conclusion\nWe have presented techniques for scaling Swin Trans-\nformer up to 3 billion parameters and making it capable\nof training with images of up to 1,536 √ó1,536 resolution,\nincluding the res-post-norm and scaled cosine attention\nto make the model easier to be scaled up in capacity, as\nwell a log-spaced continuous relative position bias approach\nwhich lets the model more effectively transferred across\nwindow resolutions. The adapted architecture is named\nSwin Transformer V2, and by scaling up capacity and reso-\nlution, it sets new records on 4 representative vision bench-\nmarks. By these strong results, we hope to stimulate more\nresearch in this direction so that we can eventually close\nthe capacity gap between vision and language models and\nfacilitate the joint modeling of the two domains.\nAcknowledgement\nWe thank many colleagues at Microsoft for their help,\nin particular, Eric Chang, Lidong Zhou, Jing Tao, Aaron\nZhang, Edward Cui, Bin Xiao, Lu Yuan, Peng Cheng, Fan\nYang for useful discussion and the help on GPU resources\nand datasets.\nA1. Experimental Settings for Ablation\nThis section describes the experimental settings for ab-\nlation, including models of SwinV2-T, SwinV2-S, and\nSwinV2-B, and tasks of ImageNet-1K image classiÔ¨Åcation,\nCOCO object detection and ADE semantic segmentation.\nA1.1. ImageNet-1K Pre-training\nAll ablation study use the ImageNet-1K image classiÔ¨Å-\ncation task for pre-training. We adopt an input image size\n(window size) of 256√ó256 (8√ó8)2. Following [46], we em-\nploy an AdamW [48] optimizer for 300 epochs using a co-\nsine decay learning rate scheduler with 20 epochs of linear\nwarm-up. A batch size of 1024, an initial learning rate of\n1√ó10‚àí3, a weight decay of 0.05, and gradient clipping with\na max norm of 5.0 are used. Augmentation and regular-\nization strategies include RandAugment [15], Mixup [81],\nCutmix [79], random erasing [84] and stochastic depth [32].\nAn increasing degree of stochastic depth augmentation is\nemployed for larger models, i.e. 0.2,0.3,0.5 for tiny, small,\nand base models, respectively.\nA1.2. Fine-tuning on various tasks\nImageNet-1K image classiÔ¨Åcation For ImageNet-1K\nimage classiÔ¨Åcation experiments, we conduct a Ô¨Åne-tuning\nstep if the input image resolution is larger than that in the\npre-training step. The Ô¨Åne-tuning lasts for 30 epochs, with\nan AdamW [48] optimizer, a cosine decay learning rate\nscheduler with an initial learning rate of 4 √ó10‚àí5, a weight\ndecay of 1 √ó10‚àí8, and the same data augmentation and\nregularizations as those in the Ô¨Årst stage.\nCOCO object detection We use cascade mask R-\nCNN [8,28] implemented in mmdetection [11] as the object\ndetection framework. In training, a multi-scale augmenta-\ntion [9, 61] with the shorter side between 480 and 800 and\nthe longer side of 1333 is used. The window size is set\n16√ó16. An AdamW [48] optimizer with an initial learning\nrate of 1 √ó10‚àí4, a weight decay of 0.05, a batch size of 16,\nand a 3√óscheduler are used.\nADE20K semantic segmentation We adopt an image\nsize (window size) of 512 √ó512 (16√ó16). In training, we\nemploy an AdamW [48] optimizer with an initial learning\nrate of 4 √ó10‚àí5, a weight decay of 0.05, a learning rate\nscheduler that uses linear learning rate decay and a linear\nwarm-up of 1,500 iterations. Models are trained with batch\nsize of 16 for 160K iterations. We follow the mmsegmenta-\ntion codebase to adopt augmentations of random horizontal\n2Most of our experiments have the window size as an even number to\nmake the window shifting offset divisible by the window size. Neverthe-\nless, an odd number of window size also works well, as is right the case in\nthe original Swin Transformer (7 √ó7).\nÔ¨Çipping, random re-scaling within ratio range [0.5, 2.0] and\na random photometric distortion. Stochastic depth with ra-\ntio of 0.3 is applied for all models. A layer-wise learning\nrate decay [4] of 0.95 is adopted for all experiments.\nA2. Experimental Settings for System-Level\nComparison\nA2.1. SwinV2-B and SwinV2-L Settings\nTable 2, 3 and 4 include results of SwinV2-B and\nSwinV2-L. For these experiments, we Ô¨Årst conduct\nImageNet-22K pre-training, and then Ô¨Åne-tune the pre-\ntrained models on individual down-stream recognition\ntasks.\nImageNet-22K pre-training Both models use an input\nimage size (window size) of 192 √ó192 (12√ó12). We em-\nploy an AdamW optimizer [48] for 90 epochs using a co-\nsine learning rate scheduler with 5-epoch linear warm-up.\nA batch size of 4096, an initial learning rate of 0.001, a\nweight decay of 0.1, and gradient clipping with a max norm\nof 5.0 are used. Augmentation and regularization strategies\ninclude RandAugment [15], Mixup [81], Cutmix [79], ran-\ndom erasing [84] and stochastic depth [32] with ratio of 0.2.\nImageNet-1K image classiÔ¨Åcation We consider input\nimage sizes of 256√ó256 and 384√ó384. The training length\nis set 30 epochs, with a batch size of 1024, a cosine de-\ncay learning rate scheduler with an initial learning rate of\n4 √ó10‚àí5, and a weight decay of 1 √ó10‚àí8. The ImageNet-\n1K classiÔ¨Åcation weights are also initialized from the corre-\nsponding ones in the ImageNet-22K model.\nCOCO object detection We adopt HTC++ [10, 46] for\nexperiments. In data pre-processing, Instaboost [23], a\nmulti-scale training [26] with an input image size of\n1536√ó1536, a window size of 32 √ó32, and a random scale\nbetween [0.1,2.0] are used. An AdamW optimizer [48] with\nan initial learning rate of 4 √ó10‚àí4 on batch size of 64, a\nweight decay of 0.05, and a 3√óscheduler are used. The\nbackbone learning rate is set 0.1√óof the head learning rate.\nIn inference, soft-NMS [5] is used. Both single-scale and\nmulti-scale test results are reported.\nADE20K semantic segmentation The input image size\n(window size) is set 640 √ó640 (40 √ó40). We employ an\nAdamW [48] optimizer with an initial learning rate of\n6 √ó10‚àí5, a weight decay of 0.05, a linear decayed learn-\ning rate scheduler with 375-iteration linear warm-up. The\nmodel is trained with batch size of 64 for 40K iterations. We\nfollow the default settings in mmsegmentation for data aug-\nmentation, including random horizontal Ô¨Çipping, random\nre-scaling within ratio range [0.5,2.0] and random photo-\nmetric distortion. Stochastic depth with ratio of 0.3 is ap-\nplied.\nA2.2. SwinV2-G Settings\nStage-1 self-supervised pre-training The model is Ô¨Årst\npre-trained using a self-supervised learning approach [1]\non the ImageNet-22K-ext dataset (70 million images) for\n20 epochs. To reduce experimental overheads, we adopt a\nsmaller image size of 192√ó192. The model is trained using\nthe AdamW [48] optimizer with a cosine decay learning rate\nscheduler with 30000 steps of linear warm-up. A batch size\nof 9216, an initial learning rate of 1.4 √ó10‚àí3, a weight de-\ncay of 0.1, and gradient clipping with a max norm of 100.0\nare used. A light data augmentation strategy is employed:\nrandom resize cropping with scale range of [0.67, 1] and a\naspect ratio range of [3/4, 4/3], followed by a random Ô¨Çip-\nping and a color normalization steps.\nStage-2 supervised pre-training The model is further\npre-trained using the class labels on the ImageNet-22K-\next dataset. We employ an AdamW [48] optimizer for 30\nepochs, using a cosine decayed learning rate scheduler with\n20000 steps of linear warm-up. A batch size of 9216, an\ninitial learning rate of 1.4√ó10‚àí3, a layer-wise learning rate\ndecay of 0.87, a weight decay of 0.1, and gradient clipping\nwith a max norm of 100.0 are used. Augmentation and reg-\nularization strategies include RandAugment [15], random\nerasing [84] and a stochastic depth [32] ratio of 0.3.\nFine-tuning on ImageNet-1K image classiÔ¨Åcation We\nadopt an input image size of 640√ó640 for experiments. An\nAdamW [48] optimizer is employed for 10 epochs, using a\ncosine decayed learning rate scheduler and a 2-epoch lin-\near warm-up. A batch size of 576, an initial learning rate\nof 2.1 √ó10‚àí5, a weight decay of 0.1, and gradient clipping\nwith a max norm of 100.0 are used. Augmentation and reg-\nularization strategies include RandAugment [15], random\nerasing [84] and a stochastic depth [32] ratio of 0.5.\nIn evaluation, we test top-1 accuracy on both ImageNet-\n1K V1 and V2.\nFine-tuning on COCO object detection We Ô¨Årst con-\nduct inter-mediate Ô¨Åne-tuning using the Objects-365 V2\ndataset. In this stage, we remove the mask branch of the\nHTC++ framework [10, 46] because there are no mask an-\nnotations. The input image resolution and window size\nare set as [800,1024] and 32 √ó32, respectively. In train-\ning, an AdamW [48] optimizer with initial learning rate of\n1.2 √ó10‚àí3, a weight decay of 0.05 and a batch size of 96\nare used, and the training length is set 67,500 steps.\nThen we Ô¨Åne-tune the HTC++ model on COCO dataset,\nwith the mask branch randomly initialized and other model\nweights loaded from the Objects-365-V2 pre-trained model.\nIn this training stage, the input image resolution is set\n1536√ó1536 with a multi-scale ratio of [0.1,2.0]. The win-\ndow size is set 32√ó32. The AdamW [48] optimizer is em-\nployed, with an initial learning rate of 6 √ó10‚àí4, a weight\ndecay of 0.05, and a batch size of 96, and is trained 45,000\nsteps.\nIn test, Soft-NMS [5] is used. Both window sizes of32√ó\n32 and 48 √ó48 are considered.\nFine-tuning on ADE20K semantic segmentation The\ninput image size (window size) is set 640 √ó640 (40√ó40).\nAn AdamW optimizer [48] is employed, with an initial\nlearning rate of 4 √ó10‚àí5, a weight decay of 0.05, a lin-\near decayed learning rate scheduler with 80K iterations, a\nbatch size of 32, and a linear warm-up of 750 iterations. For\naugmentations, we follow the default settings in mmseg-\nmentation to include random horizontal Ô¨Çipping, random\nre-scaling within ratio range [0.5,2.0] and random photo-\nmetric distortion. The stochastic depth ratio is set 0.4.\nFine-tuning on Kinetics-400 video action recognition\nA 2-stage Ô¨Åne-tuning process is employed. In the Ô¨Årst stage,\nan input resolution of 256√ó256√ó8 with 16√ó16√ó8 window\nsize is adopted. We employ the AdamW optimizer for 20\nepochs using a cosine decayed learning rate scheduler with\n2.5-epoch linear warm-up. Other training hyper-parameters\nare: batch-size 80, an initial learning rate of3.6√ó10‚àí4, and\na weight decay of 0.1.\nIn the second stage, we further Ô¨Åne-tune the model us-\ning a larger input video resolution of 320 √ó320√ó8 with\n20√ó20√ó8 window size. We employ the AdamW optimizer\nfor 5 epochs using a cosine decayed learning rate scheduler\nwith 1-epoch linear warm-up. A batch-size of 64, an initial\nlearning rate of 5 √ó10‚àí5 and a weight decay of 0.1 are set.\nA3. Learnt Relative Position Bias by Different\nApproaches\nFigure 4 visualizes the relative position bias matrices\n( ÀÜB ‚àà R(2M‚àí1)√ó(2M‚àí1)) learnt by different bias compu-\ntation approaches, using a SwinV2-T model. The bias ma-\ntrices of the 3 heads in the Ô¨Årst block are visualized. The\nleft shows the bias matrices learnt by using an input im-\nage size of 256 √ó256 and a window size of 8 √ó8. The\nright shows the bias matrices after Ô¨Åne-tuning on a larger\ninput image resolution of 512 √ó512 and a larger window\nsize of 16 √ó16. It turns out that the bias matrices learnt\nby two CPB(continuous position bias) approaches are more\nsmoothly than that learnt by P-RPE (parameterized relative\nposition bias). Figure 5 shows more examples using the last\nblock of this model.\nLinear-CPB\nP-RPE\nLog-CPB\nWindow: 8x8 Window: 16x16\nFigure 4. Visualization of the learnt relative position bias matrices by different approaches, using a SwinV2-T model and the 3 heads in\nthe Ô¨Årst block. Left: the bias matrices by pre-training on a 256 √ó256 image and a 8√ó8 window; Right: the bias matrices after Ô¨Åne-tuning\nusing a 512√ó512 image size and 16√ó16 window size. H-x indicates the x-th head.\nReferences\n[1] Anonymous. Simmim: A simple framework for masked im-\nage modeling. In CVPR submission, 2022. 10\n[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Lu Àáci¬¥c, and Cordelia Schmid. Vivit: A video vi-\nsion transformer, 2021. 3, 7\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization, 2016. 3\n[4] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training\nof image transformers, 2021. 2, 7, 9\n[5] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and\nLarry S. Davis. Soft-nms ‚Äì improving object detection with\none line of code. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), Oct 2017. 9, 10\n[6] Andrew Brock, Soham De, and Samuel L Smith. Character-\nizing signal propagation to close the performance gap in un-\nnormalized resnets. arXiv preprint arXiv:2101.08692, 2021.\n4\n[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners, 2020. 1, 2\n[8] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-\ning into high quality object detection. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 6154‚Äì6162, 2018. 9\n[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision , pages 213‚Äì229. Springer, 2020.\n9\n[10] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox-\niao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping\nShi, Wanli Ouyang, et al. Hybrid task cascade for instance\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 4974‚Äì\n4983, 2019. 9, 10\n[11] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, et al. Mmdetection: Open mmlab detection tool-\nbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\n9\n[12] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.\nTraining deep nets with sublinear memory cost, 2016. 2, 5\n[13] Bowen Cheng, Alexander G. Schwing, and Alexander Kir-\nillov. Per-pixel classiÔ¨Åcation is not all you need for semantic\nsegmentation. arXiv, 2021. 7\n[14] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-\ning Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nTwins: Revisiting the design of spatial attention in vision\ntransformers, 2021. 3\n[15] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmenta-\ntion with a reduced search space. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, pages 702‚Äì703, 2020. 9, 10\nP-RPELinear-CPBLog-CPB\nWindow: 8x8 Window: 16x16\nFigure 5. Visualization of the learnt relative position bias matrices by different approaches, using a SwinV2-T model and the 24 heads in\nthe last block. Left: the bias matrices by pre-training on a 256 √ó256 image and a 8√ó8 window; Right: the bias matrices after Ô¨Åne-tuning\nusing a 512√ó512 image size and 16√ó16 window size. H-x indicates the x-th head.\n[16] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen,\nMengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head:\nUnifying object detection heads with attentions, 2021. 7\n[17] Zihang Dai, Hanxiao Liu, Quoc V . Le, and Mingxing Tan.\nCoatnet: Marrying convolution and attention for all data\nsizes, 2021. 1, 2, 3, 5, 7\n[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248‚Äì255. Ieee, 2009. 6\n[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 1, 2\n[20] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,\nChang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, and Jie Tang. Cogview: Mastering text-\nto-image generation via transformers. arXiv preprint\narXiv:2105.13290, 2021. 3, 8\n[21] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming\nZhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo.\nCswin transformer: A general vision transformer backbone\nwith cross-shaped windows, 2021. 3, 7\n[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 2, 3\n[23] Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao\nGou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting\ninstance segmentation via probability map guided copy-\npasting. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 682‚Äì691, 2019. 9\n[24] William Fedus, Barret Zoph, and Noam Shazeer. Switch\ntransformers: Scaling to trillion parameter models with sim-\nple and efÔ¨Åcient sparsity, 2021. 1, 2\n[25] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-\nYi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple\ncopy-paste is a strong data augmentation method for instance\nsegmentation. arXiv preprint arXiv:2012.07177, 2020. 7\n[26] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn:\nLearning scalable feature pyramid architecture for object\ndetection. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 7036‚Äì\n7045, 2019. 9\n[27] Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu,\nPengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchin-\nsky, Ishan Misra, Armand Joulin, and Piotr Bojanowski.\nSelf-supervised pretraining of visual features in the wild,\n2021. 1, 2\n[28] Kaiming He, Georgia Gkioxari, Piotr Doll ¬¥ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961‚Äì2969, 2017. 9\n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770‚Äì778, 2016. 1\n[30] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen\nWei. Relation networks for object detection. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3588‚Äì3597, 2018. 3\n[31] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 3464‚Äì3473, October 2019. 3\n[32] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-\nian Q Weinberger. Deep networks with stochastic depth. In\nEuropean conference on computer vision , pages 646‚Äì661.\nSpringer, 2016. 9, 10\n[33] Shihua Huang, Zhichao Lu, Ran Cheng, and Cheng He.\nFapn: Feature-aligned pyramid network for dense image pre-\ndiction, 2021. 7\n[34] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng,\nGang Yu, and Bin Fu. ShufÔ¨Çe transformer: Rethinking spa-\ntial shufÔ¨Çe for vision transformer, 2021. 3\n[35] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift, 2015. 3\n[36] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models, 2020. 2\n[37] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\nman action video dataset. arXiv preprint arXiv:1705.06950,\n2017. 6\n[38] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional\nencoding in language pre-training, 2021. 3\n[39] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning.\narXiv preprint arXiv:1912.11370, 6(2):8, 2019. 1, 2\n[40] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiÔ¨Åcation with deep convolutional neural net-\nworks. In Advances in neural information processing sys-\ntems, pages 1097‚Äì1105, 2012. 2\n[41] Yann LeCun, L ¬¥eon Bottou, Yoshua Bengio, Patrick Haffner,\net al. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE , 86(11):2278‚Äì2324, 1998.\n2\n[42] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and\nLuc Van Gool. Localvit: Bringing locality to vision trans-\nformers, 2021. 3\n[43] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,\nZhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. Cb-\nnetv2: A composite backbone network architecture for ob-\nject detection, 2021. 7\n[44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740‚Äì755.\nSpringer, 2014. 6\n[45] Ze Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong. A\ncloser look at local aggregation operators in point cloud anal-\nysis, 2020. 3\n[46] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows, 2021. 2, 3, 4, 7, 9, 10\n[47] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer, 2021.\n2, 7\n[48] Ilya Loshchilov and Frank Hutter. Decoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations, 2019. 9, 10\n[49] Microsoft. Turing-nlg: A 17-billion-parameter language\nmodel by microsoft, 2020. 1, 2\n[50] Microsoft. Using deepspeed and megatron to train megatron-\nturing nlg 530b, the world‚Äôs largest and most powerful gen-\nerative language model, 2021. 1, 2\n[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision, 2021. 6\n[52] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. 2019. 1, 2, 3\n[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nuniÔ¨Åed text-to-text transformer. Journal of Machine Learn-\ning Research, 21(140):1‚Äì67, 2020. 1, 2, 3\n[54] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and\nYuxiong He. Zero: Memory optimizations toward training\ntrillion parameter models, 2020. 2, 5\n[55] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and\nVaishaal Shankar. Do imagenet classiÔ¨Åers generalize to im-\nagenet?, 2019. 2, 6\n[56] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim\nNeumann, Rodolphe Jenatton, Andr ¬¥e Susano Pinto, Daniel\nKeysers, and Neil Houlsby. Scaling vision with sparse mix-\nture of experts, 2021. 1, 2, 3, 5, 7\n[57] Michael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa\nDehghani, and Anelia Angelova. Tokenlearner: What can 8\nlearned tokens do for images and videos?, 2021. 2, 7\n[58] Kristof T Sch ¬®utt, Pieter-Jan Kindermans, Huziel E Sauceda,\nStefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert\nM¬®uller. Schnet: A continuous-Ô¨Ålter convolutional neural\nnetwork for modeling quantum interactions. arXiv preprint\narXiv:1706.08566, 2017. 3\n[59] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365:\nA large-scale, high-quality dataset for object detection. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), October 2019. 6\n[60] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. In International\nConference on Learning Representations, May 2015. 1, 2\n[61] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen-\nfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan\nYuan, Changhu Wang, et al. Sparse r-cnn: End-to-end\nobject detection with learnable proposals. arXiv preprint\narXiv:2011.12450, 2020. 9\n[62] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 1‚Äì9, 2015.\n2\n[63] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ¬¥e J¬¥egou. Training\ndata-efÔ¨Åcient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 3\n[64] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In-\nstance normalization: The missing ingredient for fast styliza-\ntion, 2017. 3\n[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998‚Äì6008, 2017. 2,\n3, 8\n[66] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.\nYou only learn one representation: UniÔ¨Åed network for mul-\ntiple tasks, 2021. 7\n[67] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei\nPokrovsky, and Raquel Urtasun. Deep parametric continu-\nous convolutional neural networks. 2018 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, Jun 2018.\n3\n[68] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense pre-\ndiction without convolutions, 2021. 3\n[69] Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and\nHongyang Chao. Rethinking and improving relative position\nencoding for vision transformer, 2021. 3\n[70] Yuxin Wu and Kaiming He. Group normalization. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), pages 3‚Äì19, 2018. 3\n[71] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr\nDoll¬¥ar, and Ross Girshick. Early convolutions help trans-\nformers see better. arXiv preprint arXiv:2106.14881, 2021.\n3\n[72] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\nBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A sim-\nple framework for masked image modeling. In Tech report,\n2022. 5, 6, 8\n[73] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin\nZheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei\nWang, and Tie-Yan Liu. On layer normalization in the trans-\nformer architecture. 2020. 3\n[74] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan\nWang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-\nend semi-supervised object detection with soft teacher, 2021.\n2, 7\n[75] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai,\nBin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention\nfor local-global interactions in vision transformers, 2021. 3,\n7\n[76] Zhuliang Yao, Yue Cao, Yutong Lin, Ze Liu, Zheng Zhang,\nand Han Hu. Leveraging batch normalization for vision\ntransformers. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 413‚Äì422, 2021. 4\n[77] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet, 2021. 3\n[78] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and\nShuicheng Yan. V olo: Vision outlooker for visual recog-\nnition. arXiv preprint arXiv:2106.13112, 2021. 3\n[79] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-\nization strategy to train strong classiÔ¨Åers with localizable fea-\ntures. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 6023‚Äì6032, 2019. 9\n[80] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-\ncas Beyer. Scaling vision transformers, 2021. 1, 2, 3, 5,\n7\n[81] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017. 9\n[82] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu\nYuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision long-\nformer: A new vision transformer for high-resolution image\nencoding, 2021. 3\n[83] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. arXiv preprint arXiv:2012.15840, 2020. 3\n[84] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. InProceedings\nof the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 34,\npages 13001‚Äì13008, 2020. 9, 10\n[85] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset. International\nJournal on Computer Vision, 2018. 6"
}