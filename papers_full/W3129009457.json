{
  "title": "Generative Spoken Language Modeling from Raw Audio",
  "url": "https://openalex.org/W3129009457",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221541402",
      "name": "Lakhotia, Kushal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227681708",
      "name": "Kharitonov, Evgeny",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221724798",
      "name": "Hsu, Wei-Ning",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221344086",
      "name": "Adi, Yossi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225911703",
      "name": "Polyak, Adam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287296658",
      "name": "Bolte, Benjamin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2653674194",
      "name": "Nguyễn Tú Anh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223310311",
      "name": "Copet, Jade",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221724799",
      "name": "Baevski, Alexei",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Mohamed, Adelrahman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202141647",
      "name": "Dupoux, Emmanuel",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2973049979",
    "https://openalex.org/W2100768664",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W3098403858",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W3093096176",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W3095292526",
    "https://openalex.org/W3125087428",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W3096323553",
    "https://openalex.org/W3035202887",
    "https://openalex.org/W2963618559",
    "https://openalex.org/W2973157397",
    "https://openalex.org/W2947445680",
    "https://openalex.org/W2888911345",
    "https://openalex.org/W3003750857",
    "https://openalex.org/W2890983311",
    "https://openalex.org/W3095698432",
    "https://openalex.org/W3112034174",
    "https://openalex.org/W2972943112",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3148101939",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2483390977",
    "https://openalex.org/W2962850167",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3096216486",
    "https://openalex.org/W2937090315",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3003875258",
    "https://openalex.org/W3024040651",
    "https://openalex.org/W3148040514",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2972374322",
    "https://openalex.org/W3114436296",
    "https://openalex.org/W2577366047",
    "https://openalex.org/W3015213852",
    "https://openalex.org/W3049206033",
    "https://openalex.org/W3039910566",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2996383576",
    "https://openalex.org/W3095948607",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2346964103",
    "https://openalex.org/W3015356564",
    "https://openalex.org/W2750248772",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3015265920",
    "https://openalex.org/W2963456134",
    "https://openalex.org/W3096359985",
    "https://openalex.org/W2160473997",
    "https://openalex.org/W3033038061",
    "https://openalex.org/W2963300588",
    "https://openalex.org/W3163296124",
    "https://openalex.org/W3095361818",
    "https://openalex.org/W2950180292",
    "https://openalex.org/W3097787369",
    "https://openalex.org/W2347098582"
  ],
  "abstract": "We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo-text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder-dependent way, and that some combinations approach text-based systems.",
  "full_text": "On Generative Spoken Language Modeling from Raw Audio\nKushal Lakhotia∗, Eugene Kharitonov∗, Wei-Ning Hsu, Yossi Adi, Adam Polyak,\nBenjamin Bolte§, Tu-Anh Nguyen†, Jade Copet, Alexei Baevski,\nAbdelrahman Mohamed, Emmanuel Dupoux‡\nFacebook AI Research\nAuthor’s version of Lakhotia, K., Kharitonov, E., Hsu, W.-N., Adi, Y ., Polyak, A., Bolte, B., Nguyen, T.-A., Copet, J., Baevski, A., Mohamed, A., & Dupoux, E. (2022).\nOn Generative Spoken Language Modeling from Raw Audio, Transactions of the Association for Computational Linguistics\nAbstract\nWe introduce Generative Spoken Language\nModeling, the task of learning the acous-\ntic and linguistic characteristics of a lan-\nguage from raw audio (no text, no labels),\nand a set of metrics to automatically eval-\nuate the learned representations at acous-\ntic and linguistic levels for both encoding\nand generation. We set up baseline systems\nconsisting of a discrete speech encoder (re-\nturning pseudo-text units), a generative lan-\nguage model (trained on pseudo-text), and\na speech decoder (generating a waveform\nfrom pseudo-text) all trained without su-\npervision and validate the proposed metrics\nwith human evaluation. Across 3 speech en-\ncoders (CPC, wav2vec 2.0, HuBERT), we\nﬁnd that the number of discrete units (50,\n100, or 200) matters in a task-dependent and\nencoder-dependent way, and that some com-\nbinations approach text-based systems.1\n1 Introduction\nAn open question for AI research is creating sys-\ntems that learns from natural interactions as in-\nfants learn their ﬁrst language(s): from raw uncu-\nrated data, and without access to text or expert la-\nbels (Dupoux, 2018). Natural Language Process-\ning (NLP) systems are currently far from this re-\nquirement. Even though great progress has been\nmade in reducing or eliminating the need for ex-\npert labels through self-supervised training objec-\ntives (Brown et al., 2020; Peters et al., 2018; Rad-\nford et al., 2019; Devlin et al., 2019; Liu et al.,\n2019b; Dong et al., 2019; Lewis et al., 2020), the\nbasic units on which these systems are trained are\n∗equal contribution. ‡Also at EHESS. †Also at INRIA.\n§Work done while at FAIR.\n1Evaluation code and trained models are here:\nhttps://github.com/pytorch/fairseq/tree/\nmaster/examples/textless_nlp/gslm.\nSample audios are here: https://speechbot.github.\nio/gslm.\nstill textual. Yet, young children learn to speak\nseveral years before they can read and write, pro-\nviding a proof of principle that language can be\nlearned without any text. Being able to achieve\n’textless NLP’ would be beneﬁcial for the majority\nof the world’s languages which do not have large\ntextual resources or even a widely used standard-\nized orthography (Swiss German, dialectal Arabic,\nIgbo, etc.), and which, despite being used by mil-\nlions of users, have little chance of being served\nby current text-based technology. It would also\nbe useful for ’high-resource’ languages, where the\noral and written forms often mismatch in terms\nof lexicon and syntax, and where some linguisti-\ncally relevant signals carried by prosody and into-\nnation are basically absent from text. While text is\nstill the dominant form of language on the web,\na growing amount of audio resources like pod-\ncasts, local radios, social audio apps, on-line video\ngames provide the necessary input data to push\nNLP to an audio-based future and thereby expand\nthe inclusiveness and expressivity of AI systems.\nIs it possible to build an entire dialogue sys-\ntem from audio inputs only? This is a difﬁcult\nchallenge, but breakthroughs in unsupervised rep-\nresentation learning may address part of it. Un-\nsupervised learning techniques applied to speech\nwere shown to learn continuous or discrete rep-\nresentations that capture speaker invariant pho-\nnetic content (Versteegh et al., 2016; Dunbar et al.,\n2020), despite themselves not being phonemic\n(Schatz et al., 2021). Recent developments in self-\nsupervised learning have shown impressive results\nas a pretraining technique (van den Oord et al.,\n2017; Chung et al., 2019; Hsu et al., 2021), to the\nextent that Automatic Speech Recognition (ASR)\non par with the state of the art from two years back\ncan be built with 5000 times less labelled speech\n(Baevski et al., 2020b), or even no with no labelled\nspeech at all (Baevski et al., 2021). Of course,\nASR still assumes access to text to learn a lan-\n1\narXiv:2102.01192v2  [cs.CL]  9 Sep 2021\nEncoding Generation\nLevel Task Automatic metric Task Automatic metric Human\nLanguage Spoken\nLM\nSpot-the-word,\nSyntax-Acc\nSpeech\nGen.\nAUC-of-VERT/PPX, cont-\nBLEU, PPX@o-VERT\nMMOS\nAcoustic Acoustic\nUnit Disc.\nABX-across,\nABX-within\nResynthesis PER-from-ASR, CER-\nfrom-ASR\nCER,\nMOS\nTable 1: Tasks and metricsproposed to evaluate encoding/generation quality of models at the acoustic or language\nlevels. Bold fonts highlights the main metric used for each category (Section 3 for details).\nguage model (LM) and the mapping to the audio\nunits. Here, we study the case where the LM is\ndirectly trained from the audio units without any\nrecourse to text.\nThe high level idea (see Figure 1) is that au-\ntomatically discovered discrete units can be used\nto encode speech into \"pseudo-text\" (speech-to-\nunit, S2u), which is used in turn to train a genera-\ntive language model (unit-based language model,\nuLM) and to train a speech synthesizer (unit-to-\nspeech, u2S). This enables learning an LM from\nscratch without text, and use it to generate speech\nconditionally or unconditionally, essentially repli-\ncating what toddlers achieve before learning to\nread. Early studies using discrete codes learned\nfrom an autoencoder show the feasibility of such\nan approach, but remain at a level of a demo\n(van den Oord et al., 2017).\nIn this paper, we address one major concep-\ntual stumbling block which has, thus far, prevented\nsuch early studies from having the transformative\nimpact they could have in language technology:\nmodel evaluation. We contend that it will be im-\npossible to make progress in this area beyond de-\nmos unless proper evaluation methods enabling\nsystem comparison are etablished.\nEvaluation for speech generation is difﬁcult due\nto the continuous, variable and multi-level nature\nof the speech waveform, and the necessity both to\ncapture ﬁne grained acoustic details to generate in-\ntelligible audio and to abstract away from them to\nlearn higher level language concepts. Text-based\nmodels do not have this problem, since the input\nis already expressed in terms of mid-level discrete\nunits (characters or words), and are typically eval-\nuated with unsupervised metrics close to the learn-\ning objectives like perplexity or log likelihood.\nHere, such an approach is not directly applicable\neven if we rely on discrete pseudo-text units, since\nsuch metrics would depend in an unknown fash-\nion on their granularity (number, duration and dis-\ntribution), making the comparison of models that\nFigure 1: Setup of the baseline model architecture,\ntasks and metrics.\nuse different units infeasible.\nConceptually, generative spoken language mod-\nels can be evaluated at two levels, the acoustic\nand the language levels, and through two modes\nof operation, encoding and generation, resulting\nin 2x2 tasks (see Table & Figure 1). Acoustic Unit\nDiscovery (encoding at the acoustic level) consists\nin representing speech in terms of discrete units\ndiscarding non-linguistic factors like speaker and\nnoise. Spoken Language Modeling(encoding at\nthe language level) consists in learning the prob-\nabilities of language patterns. Speech Resynthesis\n(generation for acoustic modeling) consists in gen-\nerating audio from given acoustic units. This boils\ndown to repeating in a voice of choice an input lin-\nguistic content encoded with speech units. Speech\nGeneration (generation for language modeling)\nconsists in generating novel and natural speech\n(conditioned on some prompt or not). Compared\nto standard text generation, a critical and novel\ncomponent of the audio variant is clearly the dis-\ncovery of units since it conditions all the other\ncomponents. This is why we devote our analyses\nof model architectures to the unit-to-speech com-\nponent speciﬁcally, and leave it for further work\nto evaluate how the downstream components can\n2\nalso be optimized for spoken language generation.\nThe major contributions of this paper are as\nfollows : (1) we introduce two novel evalu-\nation metrics for the generation mode of spo-\nken language modeling at the acoustic and lan-\nguage levels respectively. Our key insight is to\nuse a generic pretrained ASR system to establish\nmodel-independent assessments of the intelligibil-\nity (acoustic level) and meaningfulness (language\nlevel) of the produced outputs. The ASR system\nconverts the generated waveform back to text, en-\nabling us to adapt standard text-based metrics for\nthese two levels. (2) we validate these metrics\nthrough comparison with human evaluation. We\nshow a high degree of concordance between hu-\nman and machine evaluations of intelligibility and\nmeaningfulness of generated audio. (3) we show\nthat these metrics can be predicted by simpler ones\ngeared to evaluate the encoding mode of the spo-\nken LM. Zero-shot metrics borrowed from pre-\nvious studies in the Zero Resource Speech Chal-\nlenges (Versteegh et al., 2016; Nguyen et al., 2020)\ncorrelate well with their generative counterpart,\noffering an easier proxy to rapidly iterate on model\nselection. (4) we systematically study the effect\nof the type of encoding units by factorially cross-\ning three recent speech-to-unit encoders, CPC,\nWave2vec 2.0 and HuBERT, with three codebook\nsizes for the discrete units, 50, 100, 200. We keep\nconstant the rest of the system built from out-of-\nthe-box components (standard Transformer for the\nuLM, Tacotron 2 for u2S). We show that both the\nencoder type and the number of units matter, and\nthat they matter differently depending on the eval-\nuation task. (5) we open source our evaluation\ntools and models to help reproducibility and com-\nparability with future work.\nIn Section 3, we introduce the ASR, zero-shot\nand human evaluation metrics, in Section 4 we\npresent the models, in Section 5, we analyze the\nresults and discuss them in Section 6.\n2 Related work\nUnsupervised speech representation learning\naims to distill features useful for downstream\ntasks, such as phone discrimination (Kharitonov\net al., 2021; Schneider et al., 2019) and seman-\ntic prediction (Lai et al., 2021; Wu et al., 2020),\nby constructing pretext tasks that can exploit large\nquantities of unlabeled speech. Pretext tasks in\nthe literature can be roughly divided into two cat-\negories: reconstruction and prediction. Recon-\nstruction is often implemented in the form of auto-\nencoding (Hsu et al., 2017a), where speech is ﬁrst\nencoded into a low-dimensional space, and then\ndecoded back to speech. Various constraints can\nbe imposed on the encoded space, such as tem-\nporal smoothness (Ebbers et al., 2017; Glarner\net al., 2018; Khurana et al., 2019, 2020), discrete-\nness (Ondel et al., 2016; van den Oord et al.,\n2017), and presence of hierarchy (Lee and Glass,\n2012; Hsu et al., 2017b). Prediction-based ap-\nproaches task a model with predicting informa-\ntion of unseen speech based on its context. Exam-\nples of information include spectrograms (Chung\net al., 2019; Wang et al., 2020; Chi et al., 2021;\nLiu et al., 2020; Chung and Glass, 2020; Liu et al.,\n2020; Ling et al., 2020; Ling and Liu, 2020), clus-\nter indices (Baevski et al., 2019; Hsu et al., 2021),\nderived signal processing features (Pascual et al.,\n2019; Ravanelli et al., 2020), and binary labels of\nwhether a candidate is the target unseen spectro-\ngram (van den Oord et al., 2018; Schneider et al.,\n2019; Baevski et al., 2020a; Kharitonov et al.,\n2021; Baevski et al., 2020b).\nSpeech resynthesis. Recent advancements in neu-\nral vocoders enabled generating natural sound-\ning speech and music (Oord et al., 2016; Ku-\nmar et al., 2019; Kong et al., 2020). These\nare often conditioned on the log mel-spectrogram\nfor the generation process. Learning low bitrate\nspeech representations in an unsupervised man-\nner, has attracted attention from both the machine\nlearning and the speech communities (Liu et al.,\n2019a; Feng et al., 2019; Nayak et al., 2019; Tjan-\ndra et al., 2019; Schneider et al., 2019; Baevski\net al., 2020a; Chen and Hain, 2020; Morita and\nKoda, 2020; Tobing et al., 2020). These repre-\nsentations can later be used for generation with-\nout text, which is particularly important for low-\nresource languages (Dunbar et al., 2019, 2020).\nvan den Oord et al. (2017) proposed a Vector-\nQuantized Variational Auto-Encoder (VQ-V AE)\nmodel to learn discrete speech units, which will be\nlater used for speech synthesis using a WaveNet\nmodel. Eloff et al. (2019) suggested a VQ-V AE\nmodel followed by a FFTNet vocoder model (Jin\net al., 2018). Tjandra et al. (2020) suggested to use\ntransformer (Vaswani et al., 2017) together with\na VQ-V AE model for unsupervised unit discov-\nery, and van Niekerk et al. (2020) combines vector\nquantization together with contrastive predictive\ncoding for acoustic unit discovery. Another line\n3\nof work use representations from an ASR acous-\ntic model that are combined with identity and\nprosodic information for voice conversion (Polyak\net al., 2020b,a, 2021b). In terms of evaluation,\nthe Zero-Resource challenge (Dunbar et al., 2019,\n2020; Nguyen et al., 2020) used bitrate together\nwith human evaluation. In this paper we addition-\nally introduce an ASR based evaluation metric.\n3 Evaluation Methods\nWe present two sets of automatic evaluation met-\nrics; the ﬁrst ones assess the output of generative\nspeech models (ASR metrics, Section 3.1); the\nsecond ones, the encoded representations (zero-\nshot probe metrics, Section 3.2). Finally, we\npresent the human evaluations (Section 3.3).\n3.1 Generation: ASR metrics\nWe present our new evaluation metrics for gener-\nation tasks. The ﬁrst task, speech resynthesis, in-\nvolves S2u which encodes input speech into units\nand u2S which decodes it back to speech. In this\ntask, we wish to evaluate intelligibility of the re-\nsulting speech. The second task, speech genera-\ntion, involves the full S2u →uLM→u2S pipeline,\nand we wish to evaluate meaningfulness of the\ngenerated speech. Our overall idea is to use ASR\nto convert the generated speech back to text and\nthen use text-based metrics.\nSpeech resynthesis intelligibility: ASR-PER.\nThe ideal metric for intelligibility would be to use\nhumans to transcribe the resynthesized speech and\ncompare the text to the original input. An auto-\nmatic proxy can be obtained by using a state-of-\nthe-art ASR system pretrained on a large corpus\nof real speech. 2 Our main metric is Phone Error\nRate (PER), which only uses an acoustic-model\nASR, without fusing with an additional language\nmodel (Chorowski and Jaitly, 2016). In prelim-\ninary experiments we also experimented with a\nfull ASR with an LM and computed Word Error\nRate (WER) and Character Error Rate (CER) to\ngive partial credit. The latter is probably closer\nto humans intelligibility metrics, as humans can-\nnot turn off their lexicon or language model. We\nalso computed such metrics by training a ﬁtted\nASR model for each resynthesis model on a spe-\nciﬁc training corpus (see Supplementary Section\nS2.1). The logic of this last test is that it provides a\nmore direct measure of the information lost in the\n2We use a BASE wav2vec 2.0 phoneme detection model\ntrained on LibriSpeech-960h with CTC loss from scratch.\nS2u→u2S pipeline, because it could adapt to sys-\ntematic errors introduced by the u2S model. Since\nthe scores between these different approaches cor-\nrelated highly, we only report here the results on\nthe PER for a pretrained ASR model which is the\nsimplest to deploy.\nSpeech generation quality and diversity: AUC\non Perplexity and VERT. Text generation eval-\nuation typically involves two axes: the quality\nof the generated text (with automatic metrics like\nmean perplexity or negative log likelihood com-\nputed on a reference large language model) and\nthe diversity (with metrics like self-BLEU 3, Zhu\net al., 2018). Typically, there is a trade-off be-\ntween these two dimensions based on the temper-\nature hyperparameter used for sampling from the\nlanguage model, whereby at low temperature, the\nsystem outputs good sentences but not varied, and\nat high temperatures, it outputs varied sentences,\nbut not very good. This results in model com-\nparison being either based on 2D plots with lines\nrepresenting the trade-off between quality and di-\nversity, or on aggregate metrics like the area un-\nder the curve. Preliminary explorations (see Ap-\npendix Section 7.2) with our models revealed two\nproblems preventing a straightforward application\nof such a scoring strategy.\nFirst, we found that for some models, at a low\nenough temperature, self-BLEU score stopped in-\ncreasing, but the systems started to repeat more\nand more words within a sentence (e.g., “the prop-\nerty the property the property”). We therefore in-\ntroduce a new metric, auto-BLEU, that measures\nwithin-sentence diversity. For a single utterance\nu, auto-BLEU is calculated as the ratio of k-grams\ns ∈NGk(u) that are repeated at least once:\nauto-BLEU(u, k) =\n∑\ns 1 [s ∈(NGk(u)\\s)]\n|NGk(n)| (1)\nAs with BLEU score, to get n-gram auto-\nBLEU we calculate the geometric mean of\nauto-BLEU(u, k) obtained for k ∈ [1, n] and\naverage over the set of generated utterances. By\ncalculating the geometric mean of self- and auto-\nBLEU, we obtain an aggregate metric which we\ncall VERT (for diVERsiTy). We used a bigram\nversion of self- and auto-BLEU.\nSecond, we found that critical temperatures for\nwhich the output was reasonable were not constant\n3Higher self-BLEU scores indicate lower diversity of the\nproduced text.\n4\nFigure 2: Comparison of diversity and perplexity\nof the generated speech. We plot VERT vs. Me-\ndian perplexity. The blue diamond corresponds to the\noracle reference point. It deﬁnes two cut-offs on the\ncurve: VERT @oracle-PPX and PPX @oracle-VERT.\nThe green area corresponds to the AUC metric.\nacross models. This makes sense, because temper-\nature controls the probability of sampling individ-\nual units, and the probabilistic distribution and du-\nration of these units depend on the models. Here,\nwe chose to use the oracle text as an anchor to\ncompute reference temperatures, i.e., the temper-\natures at which the perplexity or the VERT score\nreach the values of the oracle text.\nThis gives us boundary conditions at which we\ncan compare (the perplexity at oracle diversity and\nthe diversity at oracle perplexity), as well as a\nmethod to compute the area under curve (AUC)\nbetween these two boundaries (See Figure 2). As\nAUC decreases, the system gets closer to the ora-\ncle point. Thus with AUC, lower is better.\nTo calculate perplexity of the generated ut-\nterances, we use a pre-trained ASR 4 to convert\nspeech to text, and an off-the-shelf Transformer\nmodel trained on the English NewsCrawl dataset.5\n3.2 Encoding: Zero-shot probe metrics\nThe purpose of the encoding metrics is to evaluate\nthe quality of the learned representations at each\nlinguistic level along the pipeline linking the S2u\nand the uLM. They are inspired by human psy-\ncholinguistics and can be be thought of as unit\ntests providing interpretation and diagnosis. We\nentirely draw on evaluations from the Zero Re-\nsource challenge series (Versteegh et al., 2016;\nDunbar et al., 2019; Nguyen et al., 2020)6 for com-\nparability with published work and refer to these\nchallenges for details. These metrics are “zero-\nshot” because they do not require training any\nclassiﬁer, and are either based on distances over\n4We use a LARGE wav2vec 2.0 model, trained on\nLibriSpeech-960h with CTC loss from scratch. Its decoder\nuses the standard KenLM 4-gram language model.\n5github.com/pytorch/fairseq/.../language_model\n6www.zerospeech.com\nembeddings, or on computing probabilities over\nentire utterances. When they have hyperparame-\nters, these are selected using a validation set.\nFor acoustic-level evaluation, we use the\nbetween-speaker ABX score to quantify how\nwell-separated phonetic categories are. Brieﬂy, it\nconsists in estimating the probability that two to-\nkens of the same category A (x and a) are closer\nto one another than a token of A (x) and of B\n(b). The categories are triphones that only differ\nin the middle phoneme (like bit and bet) and the\nscore is averaged over all possible such pairs. For\nthe across-speaker ABX, a and b are spoken by\nthe same speaker and x by a different one, requir-\ning feature invariance over a speaker change. We\nalso include thebitrate which has been used in the\nTTS-without-T challenges (Dunbar et al., 2019) to\nquantify the efﬁciency of the discrete units used to\nresynthetize speech. It is simply the entropy of the\nsequence of units divided by the total duration.\nFor language-level evaluation, we usespot-the-\nword accuracy from the Zero Resource 2021\nBenchmark (Nguyen et al., 2020). It consists in\ndetecting the real word from a pair of short ut-\nterances like ’brick’ vs ’blick’, matched for un-\nigram and bigram phoneme frequency to ensure\nthat low-level cues do not make the task triv-\nial. This task can be done by computing the\nprobability (or pseudo-probability) of the utter-\nances from the uLM. The test set (sWUGGY) con-\nsists of 5,000 word-pseudoword pairs generated\nby the Google TTS API, ﬁltered for the word be-\ning present in the LibriSpeech 960h training set\n(Panayotov et al., 2015). The ZR21 benchmark\nalso uses higher level metrics, notably, syntactic\n(based on the sBLIMP dataset), which we did not\nuse because the baselines were too close to chance.\n3.3 Human evaluation metrics\nAs above, we asked humans to evaluate two as-\npects of speech generation: intelligibility and\nmeaningfulness. Intelligibility was assessed us-\ning two metrics: i) Mean Opinion Scores (MOS)\nin which raters were asked to evaluate subjec-\ntively how intelligible a given audio sample is; ii)\nCharacter Error Rate (CER) computed from writ-\nten transcriptions providing an objective intelli-\ngibility test. As for meaningfulness, we set up\na meaningfulness-MOS (MMOS) in which raters\nwere asked to evaluate how natural (considering\nboth grammar and meaning) a given sample is. For\nboth subjective tests raters evaluate the samples on\n5\na scale of 1-5 with an increment of 1.\nFor the MMOS, we had to select a temper-\nature to sample from. Preliminary experiments\nshowed that humans preferred lower temperatures\n(yielding also less diverse outputs, see Supple-\nmentary Section S2.2). Here, we settled on se-\nlecting the temperature on a model-by-model basis\nby constructing a continuation task: we take the\n1,000 shortest utterances from LibriSpeech test-\nclean that are at least 6 seconds long, and use the\nﬁrst 3 seconds as prompts for the uLM (after tran-\nscribing them into pseudo-texts). For each prompt,\nwe generated 10 candidate continuations of the\nsame length (in seconds) as the utterance which\nwe took the prompt from. We varied tempera-\nture (0.3, 0.4, ..., 1.4, 1.5, 1.7, 1.9, 2.1, 2.3, 2.5,\n3.0), and selected the one yielding the maximal\nBLEU-2 score with the reference sentence (after\nASR). These temperatures were typically between\nthe two boundary temperatures described above.\nWe evaluated 100 samples from each of the\nevaluated methods while we enforced at least 15\nraters for each sample. The CrowdMOS pack-\nage (Ribeiro et al., 2011) was used for all subjec-\ntive experiments using the recommended recipes\nfor detecting and discarding inaccurate scores.\nThe recordings for the naturalness test were gen-\nerated by the LM unconditionally and condition-\nally from a 3 seconds prompt. Participants were\nrecruited using a crowd-sourcing platform.\n4 Proposed Systems\nHere, we present our S2u (Section 4.1), uLM (Sec-\ntion 4.2) and u2S (Section 4.3) components.\n4.1 Speech-to-unit Models\nWe selected 3 recent state-of-the-art unsupervised\nEncoders, which we used ’out of the box’: we did\nnot retrain them nor change their hyperparameters.\nWe also included a log Mel ﬁlter-bank baseline\n(80 ﬁlters, computed every 10ms). We then dis-\ncretized the embeddings using k-means. We only\ngive a high level description of these models, and\nrefer to the original publications for details.\nCPC. Contrastive Predictive Coding (van den\nOord et al., 2017) as applied to speech consists of\ntwo components: an encoder and a predictor. The\nencoder produces an embedding z from speech in-\nput. The predictor predicts the future states of\nthe encoder based on the past, and the system is\ntrained with a contrastive loss. We use the CPC\nmodel from (Rivière and Dupoux, 2020), which\nwas trained on a “clean” 6k hour sub-sample of\nthe LibriLight dataset (Kahn et al., 2020; Rivière\nand Dupoux, 2020). We extract a representation\nfrom an intermediate layer of the predictor, which\nprovides a 256-dimensional embedding (one per\n10ms), as in the original paper.\nwav2vec 2.0. Similar to CPC, this model uses\nan encoder and a predictor, which is trained con-\ntrastively to distinguish positive and negative sam-\nples from discretized and masked segments of the\nencoder’s output. We use the LARGE variant of\npretrained wav2vec 2.0 (Baevski et al., 2020b)\ntrained on 60k hours of LibriLight dataset (Kahn\net al., 2020). This model encodes raw audio\ninto frames of 1024-dimensional vectors (one per\n20ms). To choose the best layer, we extracted\nfrozen representations of the 10 hour LibriLight\nsubset from every layer of the model and trained\na linear classiﬁer with the CTC loss to predict the\nphonetic version of the text labels. Layer 14 ob-\ntained the lowest PER on LS dev-other (a similar\napproach was done in (Baevski et al., 2021) which\nin this case selected Layer 15).\nHuBERT.Unlike CPC and wav2vec 2.0 that use a\ncontrastive loss, HuBERT is trained with a masked\nprediction task similar to BERT (Devlin et al.,\n2019) but with masked continuous audio signals\nas inputs. The targets are obtained through un-\nsupervised clustering of raw speech features or\nlearned features from earlier iterations, motivated\nby DeepCluster (Caron et al., 2018). We use the\nBASE 12 transformer-layer model trained for two\niterations (Hsu et al., 2021) on 960 hours of Lib-\nriSpeech (Panayotov et al., 2015). This model en-\ncodes raw audio into frames of 768-dimensional\nvectors (one per 20ms) at each layer and we extract\nthose from the 6th layer as in the original paper.\nLogMel. As a baseline, we consider a Log Mel\nFilterbank encoder using 80 frequency bands.\nQuantization. We use k-means to convert con-\ntinuous frame representations into discrete repre-\nsentation by training on LibriSpeech clean-100h\n(Panayotov et al., 2015). We experiment with\ncodebooks that have 50, 100, and 200 units.\n4.2 unit-Language Model\nWe use the Transformer model as implemented in\nfairseq (Ott et al., 2019). We use the trans-\nformer_lm_big architecture: it has 12 layers, 16\nattention heads, embedding size of 1024, FFN size\n6\nFigure 3: Overall results with automatic and human metrics. The results are presented in terms of bitrate for 4\nencoders (LogMel, CPC, HuBERT and wav2vec 2.0) varying in number of units (50, 100, 200). For deﬁnition of\nthe tasks and metrics, see Table 1 and Figure 1. Negative human opinion scores are shown for ease of comparison\nwith automatic metrics (lower is better). The generation metrics have been averaged across LS and LJ (PER and\nMOS; resynthesis task) and across prompted and unprompted conditions (AUC and MMOS; speech generation\ntask). The Log Mel Fbank based systems were not evaluated by humans in the speech generation task.\nof 4096, and dropout probability of 0.1, and we\ntrain it as a causal LM on sequences of pseudo-\ntext units. Each sample contains up to 3,072 units.\nWe use sampling with temperature for generation.\nAll language models are trained on “clean” 6k\nhours sub-sample of LibriLight used in (Rivière\nand Dupoux, 2020), transcribed with correspond-\ning discrete units. In preliminary experiments, we\nfound that removing sequential repetitions of units\nimproves performance, hence we apply it univer-\nsally.7 We hypothesise that this simple modiﬁca-\ntion allows to use Transformer’s limited attention\nspan more efﬁciently as in Hsu et al. (2020).\n4.3 unit-To-Speech Model\nWe adapt theTacotron-2model (Shen et al., 2018)\nsuch that it takes pseudo-text units as input and\noutputs a log Mel spectrogram. To enable the\nmodel to synthesize arbitrary unit sequences, in-\ncluding those representing incomplete sentences,\nwe introduce two modiﬁcations. First, we append\na special “end-of-input” (EOI) token to the input\nsequence, hinting the decoder to predict the “end-\nof-output” token when attending to this new token.\n7For example, a pseudo-text 10 11 11 11 21 32\n32 32 21 becomes 10 11 21 32 21.\nHowever, this modiﬁcation alone may not be suf-\nﬁcient, as the decoder could still learn to ignore\nthe EOI token and correlate end-of-output predic-\ntion with the learned discrete token that represents\nsilence as most of the speech contains trailing si-\nlence. To address this, we train the model us-\ning random chunks of aligned unit sequence and\nspectrogram, and append the EOI token to unit se-\nquence chunks, such that the audio does not al-\nways end with silence. We implement chunking in\nthe curriculum learning fashion, where the chunk\nsize gradually grows (starting with 50 frames with\nan increment of 5 per epoch) to increase the difﬁ-\nculty of the task. For waveform generation, we use\nthe pre-trained ﬂow-based neural vocoder WaveG-\nlow (Prenger et al., 2019). This model outputs\nthe time-domain signal given the log Mel spectro-\ngram as input. All u2S models were trained on LJ\nSpeech (LJ) (Ito and Johnson, 2017).\n5 Results\nIn Figure 3, we report the overall results of our\nmodels and our LogMel baseline as a function of\nthe number of quantized units on our main au-\ntomated and human metrics. More detailed re-\nsults follow in the following sections, including\n7\ntwo character-based toplines: one uses the oracle\ntranscripts for training the LM, the other uses tran-\nscripts produced by the pre-trained ASR model.\n5.1 Results on the resynthesis task\nOverall resynthesis results are shown in the bot-\ntom middle and right cells of Figure 3 for our\nmain automatic (PER) and human scores (MOS),\nrespectively, averaged across the LS and LJ evalu-\nation sets. We observe that across all models, in-\ncreasing the number of units uniformly leads to\nbetter scores suggesting that the u2S component\ncan take beneﬁt from extra details of the input\nto produce a more realistic output. HuBERT and\nCPC seem to be giving the best results, for both\nhumans and models better capturing phonetic in-\nformation than other models at equivalent bitrates.\nMore detailed results are in Table 2 separat-\ning the scores for the LJ and LS resynthesis, and\nadding extra automatic metrics (CER) and human\nmetrics (human CER). On PER, we found a do-\nmain effect: resynthesizing input from LJ Speech\nyields lower PER than from LibriSpeech on all\nunsupervised models. From the viewpoint of the\nencoder, LJ Speech is out-of-domain; therefore,\none would expect that the units are making more\nerrors than for the trained LibriSpeech. On the\nother hand, the u2S component has learned from\nLJ Speech encoded with these units, and might\nhave learned to compensate for these lower qual-\nity units. When LibriSpeech is offered as input,\nthe u2S component cannot adapt to this nominally\nbetter input and ends up yielding lower quality\noutputs. This observation is worth further explo-\nrations, as other metrics like CER (using an LM)\nand human evaluations only replicated this for the\nmodels with the lowest score (like LogMel and\nwav2vec). The automatic PER and CER scores\nand the human MOS and CER scores, all correlate\nwell with one another across the 4 ×3 models and\nbaselines. Within the LJ or LS domain , the Pear-\nson r ranged from .95 to .99; across domains it was\nless good (from .79 to .96) illustrating again the\nexistence of a domain effect. Not shown here, we\nreached similar conclusions with our ﬁtted-ASR\nmetrics, but with less good scores and correlations.\nTable 2 also shows the results of the two toplines\n(original text+TTS and ASR+TTS). Interestingly,\nour best models come within 3% absolute in PER\nor CER compared to these toplines, are quite close\nto them in terms of MOS and even beat them in\nterms of human CER.\n5.2 Results on the generation task\nThe upper mid and right cells of Figure 3 shows\ngeneration results averaging across the uncondi-\ntional and conditional conditions, on automatic\nand human evaluations respectively. The main re-\nsult is that there is both an effect of number of\nunits and of system. As for resynthesis, 50 units\nis always worst, but contrary to resynthesis, 200\nunits is not always better. Overall, the results on\ngeneration are congruent with the idea that speech\ngeneration both requires good scores on language\nmodeling and on speech synthesis. The best re-\nsults for a particular model are then a compromise\nbetween the number of units that give both scores\nto either of these tasks. In terms of systems, the\nbest one here is HuBERT. Regarding human eval-\nuations, they show similar patterns with a clear\ndispreference for 50 units, and either 100 or 200\nbeing better.\nDetailed results are shown in Table 3 with sep-\narate statistics for conditional and unconditional\ngeneration and additional results with PPX@o-\nVERT and VERT@o-PPX. As expected, the per-\nplexity metric improved with prompts, but not the\ndiversity score. The human results are congru-\nent with the automatic scores, although they tend\nto prefer more units, perhaps showing that they\ncannot fully dissociate their judgment of meaning\nfrom their judgment of intelligibility. The three\nmetrics correlate well with one another (r between\n.86 and .99) and correlate with their counterpart\nacross task (prompted vs. unprompted: r between\n.82 and .99). Human evaluations correlated well\nwith the automatic metrics (AUC: r=.87; PPX:\nr=.92; VERT: r=0.75).\n5.3 Results for zero-shot probe metrics\nIn Table 4, we show the results for zero-shot\nmetrics across the different models and baselines.\nOverall, the performances depend on the linguistic\nlevels while remaining above chance. While per-\nformances are excellent at the acoustic level (6.5%\nerror for the best model on ABX-across), they are\nintermediate at the lexical level (31.3% error for\nthe best model on spot-the-word). Not shown, the\nsyntactic test is close to chance (42% error for the\nbest model on the sBLIMP test). These values are\nworse than the ASR-topline (3.1% and 29%, for\nlexicon and syntax resp.), showing room for im-\nprovement.\nThe metrics correlate well: the ABX score pre-\ndicts the lexical score ( r = 0.85) and the syn-\n8\nTable 2: Results on the resynthesis task for 3 unsupervised models plus one LogMel baseline and 3 unit sizes.\nBitrates are in bit/sec, PER are for a pretrained phone recognition model without lexicon and LM, CER are derived\nfrom a full ASR model (lower is better). Human MOS (upper is better) and CER (computed from transcription,\nlower is better) are provided (the 95% conﬁdence interval was on average .32 for MOS and 1.8 for human CER)\nSystems End-to-end ASR-based metrics Human Opinion\nS2u Nb Bit- PER↓ PER↓ CER↓ CER↓ MOS↑ MOS↑ CER↓ CER↓\narchitect. units rate (LJ) (LS) (LJ) (LS) (LJ) (LS) (LJ) (LS)\nToplines\noriginal wav - - - - 4.83 4.30 8.88 6.73\norig text+TTS 7.78 7.92 8.87 5.14 4.02 4.03 13.25 10.73\nASR + TTS 27 9.45 8.18 9.48 5.30 4.04 4.06 15.98 11.56\nBaselines\nLogMel 50 214.8 27.72 49.38 27.73 52.05 2.41 2.07 43.78 66.75\nLogMel 100 292.7 25.83 45.58 24.88 48.71 2.65 2.01 37.39 62.72\nLogMel 200 373.8 19.78 45.16 17.86 46.12 2.96 2.16 23.33 62.6\nUnsupervised\nCPC 50 159.4 10.87 17.16 10.68 12.06 3.63 3.51 13.97 19.92\nCPC 100 213.1 10.75 15.82 9.84 9.46 3.42 3.68 13.53 14.73\nCPC 200 279.4 8.74 14.23 9.20 8.29 3.85 3.54 9.36 14.33\nHuBERT-L6 50 125.7 11.45 16.68 11.02 11.85 3.69 3.49 14.54 13.14\nHuBERT-L6 100 168.1 9.53 13.24 9.31 7.19 3.84 3.68 13.02 11.43\nHuBERT-L6 200 211.3 8.87 11.06 8.88 5.35 4.00 3.85 11.67 10.84\nwav2vec-L14 50 141.3 24.95 33.69 25.42 32.91 2.45 2.87 46.82 54.9\nwav2vec-L14 100 182.1 14.58 22.07 13.72 17.22 3.50 3.32 23.76 28.1\nwav2vec-L14 200 226.8 10.65 16.34 10.21 10.50 3.83 3.51 13.14 15.27\ntax score ( r = 0.71). Across the different mod-\nels, CPC gets the best units (ABX score) and Hu-\nBERT gets the best LM scores. In addition, we see\na clear effect of number of units (Figure 3). For\nwav2vec, the performances on all metrics increase\nwith more units, whereas, for CPC and HuBERT\na U-shaped pattern emerges on most metrics, with\nbest scores for units of intermediate sizes. It is in-\nteresting that the models with the highest bitrate\ndo not always have the best results. This means\nthat encoding too much acoustic information can\nbe detrimental to linguistic encoding in the uLM.\nSee Appendix Section 7.1 showing that ABX has\ngood correlations with automatic and human met-\nrics (r > .88).\n6 Discussion and Conclusion\nWe introducedGenerative Spoken Language Mod-\neling as a new unsupervised task bridging the\ngap between speech and natural language process-\ning and related it conceptually to previously stud-\nied unsupervised tasks: Acoustic Unit Discov-\nery, Spoken Language Modeling, Discrete Speech\nResynthesis and Text Generation. We introduced a\nsuite of metrics, baselines, and ﬁrst results on Lib-\nrilight that sets the playing ﬁeld for future work.\nFor comparability, we open source our evaluation\nstack and the best of our baseline models.\nOur main contributions are as follows: (1) we\nestablished a set of easy to use automatic ASR-\nbased metrics for model comparison at two criti-\ncal levels for this task: intelligibility of the speech\noutput and meaningfulness in terms of higher lin-\nguistic content. We assessed the ﬁrst through\nASR-based PER and CER metrics; and the sec-\nond using text-generation-based metrics (AUC for\nPPX/VERT). (2) We found that these two sets\nof metrics correlated well with human judgement\nand (3) that they can be approximated with their\ninference-mode counterparts, which are faster to\ncomputed using zero-shot probe tasks. (4) Ap-\nplying these metrics to pipeline models based\non current speech representation learning models\nand out-of-the-box LM and TTS components, we\nfound that our basic premise is fulﬁlled: it is pos-\nsible to train a language model from quantized\nunits derived from audio and using it to gener-\nate new speech. The generated speech is English\nsounding, with recognizable phonemes and words\nand locally acceptable syntax (see transcribed ex-\n9\nTable 3: Results on the generation task for three unsupervised models plus the LogMel baseline and 3 unit sizes.\nPPX@-o-VERT and VERT@o-PPX are reported as PPX and VERT. ’-’ : missing or non calculable results. Human\nMMOS are also provided (the 95% conﬁdence interval was on average .29 for uncond. and .61 for cond.).\nSystems Generation based metrics Human Opinion\nEncoder Nb unconditional prompt uncond. prompt\narchitect. units PPX↓ VERT↓ AUC↓ PPX↓ VERT↓ AUC↓ MMOS↑ MMOS↑\nControls\noracle text 154.5 19.43 - 154.5 19.43 - 4.02 4.26\nASR + LM 178.4 21.31 0.18 162.8 20.49 0.04 3.91 4.38\nBaseline\nLogMel 50 1588.97 - 1083.76 - - - - -\nLogMel 100 1500.11 95.50 510.26 - - - - -\nLogMel 200 1539.00 - 584.16 - - - - -\nUnsupervised\nCPC 50 374.26 46.26 19.68 323.9 39.92 18.44 3.31 3.61\nCPC 100 349.56 41.797 15.74 294.7 42.93 14.06 3.65 3.65\nCPC 200 362.84 40.28 16.46 303.5 43.42 26.67 3.58 3.67\nHuBERT-L6 50 376.33 43.06 19.27 339.8 45.85 21.03 3.53 3.00\nHuBERT-L6 100 273.86 31.36 5.54 251.2 33.67 5.88 3.95 3.53\nHuBERT-L6 200 289.36 33.04 7.49 262.4 34.30 6.13 4.01 4.32\nwav2vec-L14 50 936.97 - 307.91 1106.3 - 330.8 2.26 1.91\nwav2vec-L14 100 948.96 79.51 208.38 775.1 - 205.7 2.28 1.92\nwav2vec-L14 200 538.56 61.06 61.48 585.8 - 91.07 2.64 3.04\nTable 4: Results for zero-shot probe metrics for 3\nunsupervised models plus one LogMel baseline and 3\nunit sizes. ABX within and across speakers, spot-the-\nword and acceptability judgments are error rates (lower\nis better); chance is 50%.\nMetrics S2u uLM\nNb ABX ABX spot-the- accept.\nSystem units with.↓acr.↓ word↓ judg.↓\nToplines\nASR+LM - - 3.12 29.02\nBaselines\nLogMel 50 23.95 35.86 48.52 46.78\nLogMel 100 24.33 37.86 48.12 46.83\nLogMel 200 25.71 39.65 49.62 47.76\nUnsupervised\nCPC 50 5.50 7.20 32.18 45.43\nCPC 100 5.09 6.55 31.72 44.35\nCPC 200 5.18 6.83 37.40 45.19\nHuBERT-L6 50 7.37 8.61 32.88 44.06\nHuBERT-L6 100 6.00 7.41 31.30 42.94\nHuBERT-L6 200 5.99 7.31 36.52 47.03\nwav2vec-L14 50 22.30 24.56 51.92 45.75\nwav2vec-L14 100 18.16 20.44 50.24 45.97\nwav2vec-L14 200 16.59 18.69 44.68 45.70\namples in the Appendix and audio snippets here:\nhttps://speechbot.github.io/gslm). Our au-\ntomatic metrics conﬁrm the quality of the rep-\nresentations and outputs at the acoustic/phonetic\nlevel, but show that improvements are needed at\nthe language level. It is to be expected that per-\nformance will increase with larger training sets\nbeyond our 6k hours, as has been noted in the\ncase of text. (5) We also uncovered speciﬁc is-\nsues regarding the number of quantized units. For\nspeech resynthesis, the optimum number of units\nwas always 200 by a large margin, reﬂecting the\nwell known bitrate/intelligibility trade-off (Dun-\nbar et al., 2019). However, for language modeling,\nthis was not necessarily the case, as the more de-\ntailed acoustic information may introduce too nu-\nmerous phonetic details that have no impact at the\nlevel of lexical and syntactic representations. (6)\nFinally, we found that the choice of units also af-\nfected the temperature parameter which is used to\ncontrol the trade-off between quality and diversity\nin text-based language model. To address this ef-\nfect, we proposed a method to normalize the tem-\nperature by using an oracle text to build perplexity\nand diversity anchor points.\nObviously, this is only a ﬁrst step towards build-\ning textless NLP applications that could be ap-\nplied to any language, even low resource ones. To\nreach this long term goal, three important chal-\nlenges need to be addressed.\nFirst, even though we did compare three dif-\nferent encoders and obtained different results we\ncannot conclude that one encoder is deﬁnitely su-\n10\nTable A1: Unconditional uLM (trained on CPC-100 units) samples, transcribed by ASR, at different temperatures.\nTemp Example\nVery low temperature samples (stuttering zone)\n0.3 the property by james resell red for liberata or by jason downy the property by jason\ndowny the property the property the property the property\n0.3 and to take in another path and to take in another path and to take in another path\nand to take in another path and to take in another path and to take in another path\nand take in a\nLow temperature samples (obsessive zone)\n0.7 chapter nineteen of the life of the upper part of the ocean this is ali bravos\nrecording only bravos recordings are in the public domain i for more information or\nto volunteer\n0.7 this is a lipper vox are courting oliver vox or courting are in the public domain for\nafraid art to volunteer pleases it lipper vox dot or this\nMid temperature samples\n1.0 but it is attendant from the people to defend himself from this information pride of\nthe potential in criminal activity a curiosity and impetuosity of the world a war soon\nacquired\n1.0 finally we ought to have a strong plan a without positively the best type of the\npublic with which we ascend it or extend it our business and as we are a persons of\nthe most strong designs and other affairs of the case we\nHigh temperature samples (babble zone)\n1.5 ation of pure blue he said at once a licking streamy at her warm spot of half\nperformed note was a raging oath let it as bir of amole in mood strolling er crass\n1.5 at the swing here as to motions out of the events not time and abe he was any stump\nheaded and flow any he’s the kiln are tama why do ye take the floor\nperior to the others. Our point here was merely\nto use previously published pretrained encoders,\nand study systematically the effect of number of\nunits on these encoders. A fuller study includ-\ning a wider set of encoders and a proper hyperpa-\nrameter search (including the selection of the em-\nbedding layer and the clustering algorithm) would\nbe needed in order to determine which of them is\nmost appropriate for speech generation.\nSecond, it is to be expected that to further im-\nprove generation results, more needs to be done\nthan applying this pipeline to larger training sets.\nContrary to text, speech unfolds through time and\nvaries continuously in phonetic space. Speech also\ncontains multilayered representations (phonetic,\nprosodic, speaker identity, emotions, background\nnoise, etc.). However, both our TTS and our\nLM were out-of-the-box systems typically used\nfor text applications. More work is needed to adapt\nthese architectures to the richness and variability\nof the speech signal (see Polyak et al., 2021a, for\nﬁrst steps towards integrating prosody into dis-\ncrete units). The metrics and baselines we intro-\nduced here provide landmarks against which we\nwill measure future progress.\nThird, the automatic metrics that we deﬁned\nhere depend on textual resources to build the eval-\nuation ASR and LM models, and on linguistic re-\nsources to build the zero-shot metrics. How could\nthis ever be applied to low-resource languages?\nNote that the linguistic resources we require are\nused only for model selection, not model train-\ning. Our metrics allow for fast iterations in archi-\ntecture and hyperparameter search, but the over-\nall algorithm is totally unsupervised. Therefore,\nan important next step is to extend this work to\nother languages, in order to ﬁnd a common archi-\ntecture/hyperparameter set that gives good results\nin held out languages (high or low resource). The\nhope is that once good learning models are tuned\nusing a diverse sample of high resource languages,\nthe same models could be deployed in languages\nwhere no such resources are available, and work\nin a purely unsupervised fashion.\n7 Appendix\n7.1 Zero-shot metrics correlation results\nIn Figure A1, we present the Pearson correlations\nbetween the zero-shot metrics and the human and\nautomatic metrics on downstream tasks. The fact\nthat the ABX metric correlates well with these\ndownstream metrics makes it a useful proxy met-\nric for preliminary model and unit size selection,\nas it is much less costly than generating TTS out-\nput and running human or ASR evaluations.\n7.2 Effect of temperature on outputs\nIn this section, we describe preliminary experi-\nments we conducted to test the effects of tempera-\nture on the generated outputs. As shown in Table\nA1, the temperature deﬁned qualitatively 4 oper-\nating zones. With the lowest temperature, we get\n11\nFigure A1: Patterns of correlations between the zero-shot metrics and the automatic and human metrics.\nColor scale indicates strength of the Pearson correlation coeﬁcient (we used negative MOS and MMOS to enforce\nless is better for all metrics).\nFigure A2: MMOS for unconditional (no prompt)\nand conditional generated speech sampled at the\nthree reference temperatures (oracle VERT, oracle\nPPX, and average temperature) (preliminary experi-\nments).\nrepetitive outputs, where the system keeps repeat-\ning the same few words. At a slightly higher tem-\nperature, the system outputs complete sentences,\nbut they are sampled from a narrow set of topics.\nAt the highest temperature, the system utters an\nunstructured bag of words. In the mid-temperature\nrange, we observe relatively coherent and varied\noutputs. This is the range we want to select for\nour systems. As described in Figure 2, the lowest\nbound was set by using the oracle PPX (temper-\nature range between 0.2 and 0.65. across unsu-\npervised models) and the highest bound by using\nthe oracle VERT (temperature range between 1.1\nto 1.4). In Figure S1 we present human opinion\nresults for samples from these two temperatures,\nplus an extra mean temperature falling in between.\nHumans typically preferred the lower temperature.\nIn Figure A3, we illustrate the continuation\nmethod for selecting a single temperature for hu-\nFigure A3: Method for selecting the continuation\ntemperature for MMOS judgements.\nman meaningfulness judgments in a model-neutral\nway as explained in Section 3.3. It consists in\ngenerating possible continuations of each prompt\nand computing the BLEU-2 score 8 with oracle\ncontinuation. The temp@cont temperature is de-\nﬁned as the temperature maximizing this score.\nComputing these estimates with 10 continuations\ngave continuation temperatures varying between\n0.5 and 0.9 across models and unit sizes. These\nare the temperatures we used for the MMOS re-\nsults reported in the main paper.\nAcknowledgments\nWe thank Michael Auli and Alexis Conneau for\ntheir useful input on wav2vec, and Lior Wolf,\nPierre Emmanuel Mazaré and Gargi Gosh for their\nsupport for this project. We would also like to\nthank the reviewers and editors for their thorough\nreview, and constructive feedback.\n8We used NLTK to compute BLEU (Bird et al., 2009).\n12\nReferences\nAlexei Baevski, Michael Auli, and Abdelrah-\nman Mohamed. 2019. Effectiveness of self-\nsupervised pre-training for speech recognition.\nCoRR, abs/1911.03912.\nAlexei Baevski, Wei-Ning Hsu, and Alexis Con-\nneau. 2021. Unsupervised speech recognition.\narXiv preprint arXiv:2012.15454.\nAlexei Baevski, Steffen Schneider, and Michael\nAuli. 2020a. vq-wav2vec: Self-supervised\nlearning of discrete speech representations. In-\nternational Conference on Learning Represen-\ntations (ICLR).\nAlexei Baevski, Henry Zhou, Abdelrahman Mo-\nhamed, and Michael Auli. 2020b. wav2vec\n2.0: A framework for self-supervised learning\nof speech representations. In Proceedings of the\n34th International Conference on Neural Infor-\nmation Processing Systems, volume 33, pages\n12449–12460.\nSteven Bird, Edward Loper, and Ewan Klein.\n2009. Natural language processing with\npython.\nTom B. Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-V oss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Win-\nter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot\nlearners. In Proceedings of the 34th Interna-\ntional Conference on Neural Information Pro-\ncessing Systems.\nMathilde Caron, Piotr Bojanowski, Armand\nJoulin, and Matthijs Douze. 2018. Deep cluster-\ning for unsupervised learning of visual features.\nIn Proceedings of the European Conference on\nComputer Vision (ECCV), pages 132–149.\nMingjie Chen and Thomas Hain. 2020. Unsuper-\nvised acoustic unit representation learning for\nvoice conversion using WaveNet auto-encoders.\nIn Proc. INTERSPEECH, pages 4866–4870.\nPo-Han Chi, Pei-Hung Chung, Tsung-Han Wu,\nChun-Cheng Hsieh, Shang-Wen Li, and Hung-\nyi Lee. 2021. Audio ALBERT: A lite BERT\nfor self-supervised learning of audio represen-\ntation. In IEEE Spoken Language Technology\nWorkshop (SLT), pages 344–350.\nJan Chorowski and Navdeep Jaitly. 2016. Towards\nbetter decoding and language model integration\nin sequence to sequence models. In Proc. IN-\nTERSPEECH, pages 523–527.\nYu-An Chung and James Glass. 2020. Improved\nspeech representations with multi-target autore-\ngressive predictive coding. In Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics, pages 2353–2358.\nAssociation for Computational Linguistics.\nYu-An Chung, Wei-Ning Hsu, Hao Tang, and\nJames Glass. 2019. An unsupervised autore-\ngressive model for speech representation learn-\ning. In Proc. INTERSPEECH, pages 146–150.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019\nConference of the North American Chapter\nof the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 4171–4186. As-\nsociation for Computational Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. 2019. Uniﬁed\nlanguage model pre-training for natural lan-\nguage understanding and generation. In Ad-\nvances in Neural Information Processing Sys-\ntems, volume 32, pages 13063–13075. Curran\nAssociates, Inc.\nEwan Dunbar, Robin Algayres, Julien Karadayi,\nMathieu Bernard, Juan Benjumea, Xuan-Nga\nCao, Lucie Miskic, Charlotte Dugrain, Lucas\nOndel, Alan W. Black, Laurent Besacier, Sakri-\nani Sakti, and Emmanuel Dupoux. 2019. The\nZero Resource Speech Challenge 2019: TTS\nwithout T. In Proc. INTERSPEECH, pages\n1088–1092.\nEwan Dunbar, Julien Karadayi, Mathieu Bernard,\nXuan-Nga Cao, Robin Algayres, Lucas On-\n13\ndel, Laurent Besacier, Sakriani Sakti, and Em-\nmanuel Dupoux. 2020. The Zero Resource\nSpeech Challenge 2020: Discovering discrete\nsubword and word units. In Proc. INTER-\nSPEECH, pages 4831–4835.\nEmmanuel Dupoux. 2018. Cognitive science in\nthe era of artiﬁcial intelligence: A roadmap for\nreverse-engineering the infant language-learner.\nCognition, 173:43–59.\nJanek Ebbers, Jahn Heymann, Lukas Drude,\nThomas Glarner, Reinhold Haeb-Umbach, and\nBhiksha Raj. 2017. Hidden Markov Model\nvariational autoencoder for acoustic unit dis-\ncovery. In Proc. INTERSPEECH, pages 488–\n492.\nRyan Eloff, André Nortje, Benjamin van Niekerk,\nAvashna Govender, Leanne Nortje, Arnu Pre-\ntorius, Elan van Biljon, Ewald van der West-\nhuizen, Lisa van Staden, and Herman Kamper.\n2019. Unsupervised acoustic unit discovery for\nspeech synthesis using discrete latent-variable\nneural networks. In Proc. INTERSPEECH,\npages 1103–1107.\nSiyuan Feng, Tan Lee, and Zhiyuan Peng. 2019.\nCombining adversarial training and disentan-\ngled speech representation for robust zero-\nresource subword modeling. In Proc. INTER-\nSPEECH, pages 1093–1097.\nThomas Glarner, Patrick Hanebrink, Janek\nEbbers, and Reinhold Haeb-Umbach. 2018.\nFull Bayesian Hidden Markov Model varia-\ntional autoencoder for acoustic unit discovery.\nIn Proc. INTERSPEECH, pages 2688–2692.\nAlex Graves, Santiago Fernández, Faustino\nGomez, and Jürgen Schmidhuber. 2006. Con-\nnectionist temporal classiﬁcation: labelling un-\nsegmented sequence data with recurrent neural\nnetworks. In Proceedings of the 23rd interna-\ntional conference on Machine learning, pages\n369–376.\nWei-Ning Hsu, David Harwath, Christopher Song,\nand James Glass. 2020. Text-free image-to-\nspeech synthesis using learned segmental units.\narXiv preprint arXiv:2012.15454.\nWei-Ning Hsu, Yao-Hung Hubert Tsai, Benjamin\nBolte, Ruslan Salakhutdinov, and Abdelrahman\nMohamed. 2021. HuBERT: How much can a\nbad teacher beneﬁt ASR pre-training? In Neu-\nral Information Processing Systems Workshop\non Self-Supervised Learning for Speech and Au-\ndio Processing Workshop, pages 6533–6537.\nWei-Ning Hsu, Yu Zhang, and James Glass.\n2017a. Learning latent representations for\nspeech generation and transformation. In Proc.\nINTERSPEECH, pages 1273–1277.\nWei-Ning Hsu, Yu Zhang, and James Glass.\n2017b. Unsupervised learning of disentangled\nand interpretable representations from sequen-\ntial data. In Advances in Neural Information\nProcessing Systems, volume 30, pages 1878–\n1889. Curran Associates, Inc.\nKeith Ito and Linda Johnson. 2017. The lj\nspeech dataset. https://keithito.com/\nLJ-Speech-Dataset/.\nZ. Jin, A. Finkelstein, G. J. Mysore, and J. Lu.\n2018. FFTNet: A real-time speaker-dependent\nneural vocoder. In IEEE International Confer-\nence on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 2251–2255.\nJ. Kahn, M. Rivière, W. Zheng, E. Kharitonov,\nQ. Xu, P. E. Mazaré, J. Karadayi, V . Liptchin-\nsky, R. Collobert, C. Fuegen, T. Likhomanenko,\nG. Synnaeve, A. Joulin, A. Mohamed, and\nE. Dupoux. 2020. Libri-light: A benchmark for\nASR with limited or no supervision. In IEEE\nInternational Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 7669–\n7673.\nEugene Kharitonov, Morgane Rivière, Gabriel\nSynnaeve, Lior Wolf, Pierre-Emmanuel\nMazaré, Matthijs Douze, and Emmanuel\nDupoux. 2021. Data augmenting contrastive\nlearning of speech representations in the time\ndomain. arXiv preprint arXiv:2007.00991 ,\npages 215–222.\nSameer Khurana, Shaﬁq Rayhan Joty, Ahmed\nAli, and James Glass. 2019. A factorial deep\nMarkov Model for unsupervised disentangled\nrepresentation learning from speech. In IEEE\nInternational Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 6540–\n6544. IEEE.\n14\nSameer Khurana, Antoine Laurent, Wei-Ning\nHsu, Jan Chorowski, Adrian Lancucki, Ricard\nMarxer, and James Glass. 2020. A convo-\nlutional deep Markov Model for unsupervised\nspeech representation learning. In Proc. IN-\nTERSPEECH, pages 3790–3794.\nJungil Kong, Jaehyeon Kim, and Jaekyoung Bae.\n2020. HiFi-GAN: Generative adversarial net-\nworks for efﬁcient and high ﬁdelity speech syn-\nthesis. In Proceedings of the 34th International\nConference on Neural Information Processing\nSystems, volume 33, pages 17022–17033.\nKundan Kumar, Rithesh Kumar, Thibault\nde Boissiere, Lucas Gestin, Wei Zhen Teoh,\nJose Sotelo, Alexandre de Brébisson, Yoshua\nBengio, and Aaron C Courville. 2019. Mel-\nGAN: Generative adversarial networks for\nconditional waveform synthesis. In Advances\nin Neural Information Processing Systems ,\nvolume 32, pages 14910–14921. Curran\nAssociates, Inc.\nCheng-I Lai, Yung-Sung Chuang, Hung-Yi Lee,\nShang-Wen Li, and James Glass. 2021. Semi-\nsupervised spoken language understanding via\nself-supervised speech and language model pre-\ntraining. In IEEE International Conference\non Acoustics, Speech and Signal Processing\n(ICASSP), pages 7468–7472.\nChia-ying Lee and James Glass. 2012. A nonpara-\nmetric Bayesian approach to acoustic model\ndiscovery. In Proceedings of the 50th Annual\nMeeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 40–\n49.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence\npre-training for natural language generation,\ntranslation, and comprehension. In Proceed-\nings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages\n7871–7880, Online. Association for Computa-\ntional Linguistics.\nShaoshi Ling and Yuzong Liu. 2020. DeCoAR\n2.0: Deep contextualized acoustic representa-\ntions with vector quantization. arXiv preprint\narXiv:2012.06659.\nShaoshi Ling, Yuzong Liu, Julian Salazar, and\nKatrin Kirchhoff. 2020. Deep contextualized\nacoustic representations for semi-supervised\nspeech recognition. In IEEE International Con-\nference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pages 6429–6433. IEEE.\nA. T. Liu, S. Yang, P. Chi, P. Hsu, and H. Lee.\n2020. Mockingjay: Unsupervised speech repre-\nsentation learning with deep bidirectional trans-\nformer encoders. In IEEE International Con-\nference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pages 6419–6423.\nAlexander H Liu, Yu-An Chung, and James Glass.\n2020. Non-autoregressive predictive coding for\nlearning speech representations from local de-\npendencies. arXiv preprint arXiv:2011.00406.\nAndy T. Liu, Po chun Hsu, and Hung-Yi Lee.\n2019a. Unsupervised end-to-end learning of\ndiscrete linguistic units for voice conversion. In\nProc. INTERSPEECH, pages 1108–1112.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoy-\nanov. 2019b. RoBERTa: A robustly optimized\nBERT pretraining approach. arXiv preprint\narXiv:1907.11692.\nTakashi Morita and Hiroki Koda. 2020.\nExploring TTS without T using biologi-\ncally/psychologically motivated neural network\nmodules (ZeroSpeech 2020). In Proc. INTER-\nSPEECH, pages 4856–4860.\nShekhar Nayak, C Shiva Kumar, G Ramesh,\nSaurabhchand Bhati, and K Sri Rama Murty.\n2019. Virtual phone discovery for speech syn-\nthesis without text. In IEEE Global Conference\non Signal and Information Processing (Global-\nSIP).\nTu Anh Nguyen, Maureen de Seyssel, Patricia\nRozé, Morgane Rivière, Evgeny Kharitonov,\nAlexei Baevski, Ewan Dunbar, and Emmanuel\nDupoux. 2020. The Zero Resource Speech\nBenchmark 2021: Metrics and baselines for un-\nsupervised spoken language modeling. In Ad-\nvances in Neural Information Processing Sys-\ntems (NeurIPS) – Self-Supervised Learning for\nSpeech and Audio Processing Workshop.\n15\nBenjamin van Niekerk, Leanne Nortje, and Her-\nman Kamper. 2020. Vector-quantized neural\nnetworks for acoustic unit discovery in the Ze-\nroSpeech 2020 Challenge. In Proc. INTER-\nSPEECH, pages 4836–4840.\nLucas Ondel, Lukáš Burget, and Jan ˇCernock`y.\n2016. Variational inference for acoustic unit\ndiscovery. Procedia Computer Science, 81:80–\n86.\nAaron van den Oord, Yazhe Li, and Oriol\nVinyals. 2018. Representation learning with\ncontrastive predictive coding. arXiv preprint\narXiv:1807.03748.\nAaron van den Oord, Oriol Vinyals, et al. 2017.\nNeural discrete representation learning. In Ad-\nvances in Neural Information Processing Sys-\ntems, pages 6306–6315.\nAaron van den Oord, Sander Dieleman, Heiga\nZen, Karen Simonyan, Oriol Vinyals, Alex\nGraves, Nal Kalchbrenner, Andrew Senior, and\nKoray Kavukcuoglu. 2016. WaveNet: A gen-\nerative model for raw audio. arXiv preprint\narXiv:1609.03499.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. 2019. Fairseq: A fast, ex-\ntensible toolkit for sequence modeling. In Pro-\nceedings of NAACL-HLT, pages 48–53.\nVassil Panayotov, Guoguo Chen, Daniel Povey,\nand Sanjeev Khudanpur. 2015. LibriSpeech:\nan ASR corpus based on public domain au-\ndio books. In IEEE International Conference\non Acoustics, Speech and Signal Processing\n(ICASSP), pages 5206–5210. IEEE.\nJongseok Park, Kyubyong & Kim. 2019. g2pe.\nhttps://github.com/Kyubyong/g2p.\nSantiago Pascual, Mirco Ravanelli, Joan Serra,\nAntonio Bonafonte, and Yoshua Bengio. 2019.\nLearning problem-agnostic speech representa-\ntions from multiple self-supervised tasks. In\nProc. INTERSPEECH, pages 161–165.\nMatthew Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. 2018. Deep contextu-\nalized word representations. In Proceedings\nof the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computa-\ntional Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pages 2227–\n2237, New Orleans, Louisiana. Association for\nComputational Linguistics.\nAdam Polyak, Yossi Adi, Jade Copet, Eu-\ngene Kharitonov, Kushal Lakhotia, Wei-Ning\nHsu, Abdelrahman Mohamed, and Emmanuel\nDupoux. 2021a. Speech resynthesis from dis-\ncrete disentangled self-supervised representa-\ntions. In Proc. INTERSPEECH.\nAdam Polyak, Lior Wolf, Yossi Adi, Ori Ka-\nbeli, and Yaniv Taigman. 2021b. High ﬁdelity\nspeech regeneration with application to speech\nenhancement. In IEEE International Confer-\nence on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 7143–7147.\nAdam Polyak, Lior Wolf, Yossi Adi, and Yaniv\nTaigman. 2020a. Unsupervised cross-domain\nsinging voice conversion. In Proc. INTER-\nSPEECH, pages 801–805.\nAdam Polyak, Lior Wolf, and Yaniv Taigman.\n2020b. TTS skins: Speaker conversion via\nASR. In Proc. INTERSPEECH, pages 786–\n790.\nR. Prenger, R. Valle, and B. Catanzaro. 2019.\nWaveglow: A ﬂow-based generative network\nfor speech synthesis. In IEEE International\nConference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 3617–3621.\nAlec Radford, Jeffrey Wu, Rewon Child, David\nLuan, Dario Amodei, and Ilya Sutskever. 2019.\nLanguage models are unsupervised multitask\nlearners. OpenAI blog, 1(8):9.\nM. Ravanelli, J. Zhong, S. Pascual, P. Swietojan-\nski, J. Monteiro, J. Trmal, and Y . Bengio. 2020.\nMulti-task self-supervised learning for robust\nspeech recognition. In IEEE International Con-\nference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pages 6989–6993.\nF. Ribeiro, D. Florêncio, C. Zhang, and M. Seltzer.\n2011. CROWDMOS: An approach for crowd-\nsourcing mean opinion score studies. In IEEE\nInternational Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 2416–\n2419.\n16\nMorgane Rivière and Emmanuel Dupoux. 2020.\nTowards unsupervised learning of speech fea-\ntures in the wild. In IEEE Spoken Language\nTechnology Workshop (SLT), pages 156–163.\nThomas Schatz, Naomi H Feldman, Sharon Gold-\nwater, Xuan-Nga Cao, and Emmanuel Dupoux.\n2021. Early phonetic learning without pho-\nnetic categories: Insights from large-scale sim-\nulations on realistic input. Proceedings of the\nNational Academy of Sciences, 118(7).\nSteffen Schneider, Alexei Baevski, Ronan Col-\nlobert, and Michael Auli. 2019. wav2vec: Un-\nsupervised pre-training for speech recognition.\nIn Proc. INTERSPEECH, pages 3465–3469.\nJ. Shen, R. Pang, R. J. Weiss, M. Schuster,\nN. Jaitly, Z. Yang, Z. Chen, Y . Zhang, Y . Wang,\nR. Skerrv-Ryan, R. A. Saurous, Y . Agiomvr-\ngiannakis, and Y . Wu. 2018. Natural TTS syn-\nthesis by conditioning WaveNet on MEL spec-\ntrogram predictions. In IEEE International\nConference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 4779–4783.\nAndros Tjandra, Sakriani Sakti, and Satoshi Naka-\nmura. 2020. Transformer VQ-V AE for unsu-\npervised unit discovery and speech synthesis:\nZeroSpeech 2020 Challenge. In Proc. INTER-\nSPEECH, pages 4851–4855.\nAndros Tjandra, Berrak Sisman, Mingyang\nZhang, Sakriani Sakti, Haizhou Li, and Satoshi\nNakamura. 2019. VQV AE unsupervised unit\ndiscovery and multi-scale Code2Spec inverter\nfor Zerospeech Challenge 2019. In Proc. IN-\nTERSPEECH, pages 1118–1122.\nPatrick Lumban Tobing, Tomoki Hayashi, Yi-\nChiao Wu, Kazuhiro Kobayashi, and Tomoki\nToda. 2020. Cyclic spectral modeling for un-\nsupervised unit discovery into voice conversion\nwith excitation and waveform modeling. In\nProc. INTERSPEECH, pages 4861–4865.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In Advances in Neu-\nral Information Processing Systems, volume 30,\npages 5998–6008. Curran Associates, Inc.\nMaarten Versteegh, Xavier Anguera, Aren Jansen,\nand Emmanuel Dupoux. 2016. The Zero Re-\nsource Speech Challenge 2015: Proposed ap-\nproaches and results. Procedia Computer Sci-\nence, 81:67–72.\nW. Wang, Q. Tang, and K. Livescu. 2020. Un-\nsupervised pre-training of bidirectional speech\nencoders via masked reconstruction. In IEEE\nInternational Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 6889–\n6893.\nAnne Wu, Changhan Wang, Juan Pino, and Jiatao\nGu. 2020. Self-supervised representations im-\nprove end-to-end speech translation. In Proc.\nINTERSPEECH, pages 1491–1495.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,\nWeinan Zhang, Jun Wang, and Yong Yu. 2018.\nTexygen: A benchmarking platform for text\ngeneration models. In SIGIR, page 1097–1100.\n17\nSupplementary Materials\nHere, we provide supplementary information\nnot appearing in the TACL version for lack of\nspace.\nS1 Implementation Details\nThis section provides information about model\ntraining.\nS1.1 Speech Decoder\nWe train all speech decoder models referred in\nSection 4.3 on 8 32-GB GPUs using data dis-\ntributed training with a batch size of 32 per GPU\nfor 500 epochs. We compute validation loss ev-\nery 5000 steps, and select model with the lowest\nloss. For chunking (described in Section 4.3), we\ninitialize the chunk size to 50 and increment it by\n5 per epoch. Note that we only do chunking of\nUnit-To-Speech scenario only.\nS1.2 Training of ASR Models for evaluation\nS1.2.1 Frozen ASR Model\nThe frozen ASR model is trained using LARGE\nwav2vec model architecture with Connection-\nist Temporal Classiﬁcation (CTC) loss (Graves\net al., 2006) from scratch (not using the pretrained\nmodel) on LibriSpeech 960hours dataset.\nS1.2.2 Frozen Phoneme Recognition Model\nThe frozen phone recognition model is trained\nusing BASE wav2vec model architecture with\nConnectionist Temporal Classiﬁcation (CTC) loss\n(Graves et al., 2006) from scratch on LibriSpeech\n960hours dataset. We use g2p-en (Park, 2019) for\nobtaining gold phoneme transcriptions.\nS1.2.3 Fitted ASR Model\nFor speed of training, we design a SMALL ver-\nsion of wav2vec architecture with a Transformer\nencoder of 6 layers, 4 attention heads, embedding\nof size 256 and FFN of size 1024. The model is\nalways trained with CTC loss using synthesized\nspeech from the speech decoder.\nS2 Supplementary Results\nIn Section S2.1, we present the full set of metrics\nused the resynthetized task, and in section S2.2,\nthose for the generation task.\nS2.1 Speech Resynthesis Results on ﬁtted\nASR metrics\nIn the table S1 we present the full set of metrics\nthat we have used to evaluate synthesized speech\nusing ﬁtted ASR models. All the speech synthesis\nmodels are trained on LJSpeech and the synthe-\nsized speech is evaluated on LibriSpeech and LJ\nSpeech. Note that we always use dev_clean set for\nLibriSpeech, and a random hold-out set of 1000\nsamples for LJ Speech that were not seen during\ntraining or validation.\nS2.2 Generation Task\nTable S2 shows the results of the meaninfulness\nopinion score (MMOS) detailing the three tem-\nperature settings (oracle PPX, oracle VERT and\nthe average temperature between these two end-\npoints). As seen on Figure S1, human judgements\nare usually higher for the oracle PPX than the\nother two temperatures, and the pattern of results\nacross systems is globally comparable across tem-\nperatures. Figure S2 shows the table of correlation\ncoefﬁcients across human and automatic metrics.\n18\nTable S1: Full results using ﬁtted ASR and Phoneme Recognition models of three unsupervised unit discovery\nmodels as a function of number of quantized units on 4 sets of metrics: PER and CER without LM & Lexicon (ﬁrst\n2 columns), and WER and CER with LM & Lexicon (last 2 columns). For comparison we also show the metrics\nfor our supervised topline system.\nSystems PER CER WER CER (with LM, Lex)\nLJ LS LJ LS LJ LS LJ LS\nGold Text + TTS 8.47 11.95 5.32 10.75 20.56 17.60 9.44 8.62\nPretrained ASR + TTS - - 8.80 11.88 23.86 21.10 11.28 10.34\nLogMelFbank + KM50 34.32 60.76 25.29 48.54 69.36 92.52 39.62 63.56\nLogMelFbank + KM100 24.94 50.58 24.01 47.40 65.27 92.13 36.43 62.64\nLogMelFbank + KM200 22.55 53.21 20.99 46.37 57.61 90.98 30.72 59.01\nCPC + KM50 16.04 26.72 13.27 27.61 39.16 63.10 18.52 33.26\nCPC + KM100 19.23 32.05 12.48 25.93 35.77 58.09 16.63 29.57\nCPC + KM200 15.92 26.78 9.98 23.22 31.23 53.56 14.30 26.73\nHuBERT L6 + KM50 15.37 24.58 12.72 25.66 39.15 58.31 18.30 28.91\nHuBERT L6 + KM100 16.57 25.83 11.74 24.56 31.27 51.67 14.60 26.36\nHuBERT L6 + KM200 15.88 24.11 11.20 21.62 32.91 50.53 16.00 26.20\nwav2vec L14 + KM50 23.89 36.18 27.60 39.55 71.19 86.27 44.52 57.51\nwav2vec L14 + KM100 16.62 27.76 17.11 30.67 48.84 71.53 24.51 38.28\nwav2vec L14 + KM200 13.79 24.06 11.25 23.54 34.55 55.53 15.96 27.48\nFigure S1: MMOS for unconditional (no prompt) and conditional generated speech sampled at the three\nreference temperatures (oracle VERT, oracle PPX, and average temperature).\n19\nFigure S2: Patterns of correlations between automatic and human metrics for generation. Color scale indi-\ncates strength and direction of the Spearman correlation coeﬁcient.\nTable S2: Full Human evaluation MMOS results for three unsupervised models and 3 unit sizes. For each model\nand unit size, samples were generated using three different temperatures corresponding to: VERT, PPX and the\naverage temperature of both.\nSystems MMOS\nEncoder Nb unconditional prompt\narchitect. units PPX VERT MEAN PPX VERT MEAN\nControls\noracle text - - - - - 4.44\nASR + LM 3.72 3.23 3.57 3.24 3.02 3.24\nUnsupervised\nCPC 50 2.85 2.20 3.02 3.10 2.35 3.22\nCPC 100 3.38 2.31 3.16 3.90 2.28 3.12\nCPC 200 3.72 2.11 3.00 3.94 2.08 3.23\nHuBERT-L6 50 3.16 2.15 2.72 3.56 2.31 2.94\nHuBERT-L6 100 3.62 2.31 3.02 3.90 2.81 3.64\nHuBERT-L6 200 3.55 2.22 3.00 4.36 2.71 4.16\nWav2vec-L14 50 - 1.79 1.62 - 1.77 1.92\nWav2vec-L14 100 1.07 1.79 1.95 1.62 1.77 2.48\nWav2vec-L14 200 2.80 1.99 2.37 3.06 1.76 2.65\n20",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7320734858512878
    },
    {
      "name": "Computer science",
      "score": 0.6039920449256897
    },
    {
      "name": "Linguistics",
      "score": 0.5088115334510803
    },
    {
      "name": "Spoken language",
      "score": 0.49767592549324036
    },
    {
      "name": "Natural language processing",
      "score": 0.46210914850234985
    },
    {
      "name": "Speech recognition",
      "score": 0.4096630811691284
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3305727243423462
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I4210151031",
      "name": "Laboratoire de Sciences Cognitives et Psycholinguistique",
      "country": "FR"
    }
  ],
  "cited_by": 31
}