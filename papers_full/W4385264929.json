{
    "title": "Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning",
    "url": "https://openalex.org/W4385264929",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2960126622",
            "name": "Brihat Sharma",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A2144316405",
            "name": "Yanjun Gao",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A2096515443",
            "name": "Timothy Miller",
            "affiliations": [
                "Harvard University",
                "Boston Children's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A4201884599",
            "name": "Matthew Churpek",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A2102230835",
            "name": "Majid Afshar",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A2625526828",
            "name": "Dmitriy Dligach",
            "affiliations": [
                "Loyola University Chicago"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2891113091",
        "https://openalex.org/W4287204036",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4318391587",
        "https://openalex.org/W2902257721",
        "https://openalex.org/W3098136301",
        "https://openalex.org/W2963235246",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W4241361628",
        "https://openalex.org/W2058373465",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3162922479",
        "https://openalex.org/W1507985183",
        "https://openalex.org/W3177464472",
        "https://openalex.org/W4285239851",
        "https://openalex.org/W2947047992",
        "https://openalex.org/W3168090480",
        "https://openalex.org/W1937533433",
        "https://openalex.org/W4226470220",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W3186100284",
        "https://openalex.org/W4288090629",
        "https://openalex.org/W2154652894"
    ],
    "abstract": "Generative artificial intelligence (AI) is a promising direction for augmenting clinical diagnostic decision support and reducing diagnostic errors, a leading contributor to medical errors. To further the development of clinical AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a comprehensive generative AI framework, comprised of six tasks representing key components in clinical reasoning. We present a comparative analysis of in-domain versus out-of-domain language models as well as multi-task versus single task training with a focus on the problem summarization task in DR.BENCH (Gao et al., 2023). We demonstrate that a multi-task, clinically-trained language model outperforms its general domain counterpart by a large margin, establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55. This research underscores the value of domain-specific training for optimizing clinical diagnostic reasoning tasks.",
    "full_text": "Proceedings of the 5th Clinical Natural Language Processing Workshop, pages 78–85\nJuly 14, 2023 ©2023 Association for Computational Linguistics\nMulti-Task Training with In-Domain Language Models for Diagnostic\nReasoning\nBrihat Sharma1, Yanjun Gao1, Timothy Miller2, Matthew M. Churpek1,\nMajid Afshar1 and Dmitriy Dligach3\n1University of Wisconsin-Madison,\n2Boston Children’s Hospital and Harvard Medical School, 3Loyola University Chicago\n1bsharma25@wisc.edu,\n1{ygao, mchurpek, mafshar} @medicine.wisc.edu,\n2Timothy.Miller@childrens.harvard.edu, 3ddligach@luc.edu\nAbstract\nGenerative artificial intelligence (AI) is a\npromising direction for augmenting clinical\ndiagnostic decision support and reducing di-\nagnostic errors, a leading contributor to med-\nical errors. To further the development of\nclinical AI systems, the Diagnostic Reasoning\nBenchmark (DR.BENCH) was introduced as a\ncomprehensive generative AI framework, com-\nprised of six tasks representing key components\nin clinical reasoning. We present a compara-\ntive analysis of in-domain versus out-of-domain\nlanguage models as well as multi-task versus\nsingle task training with a focus on the prob-\nlem summarization task in DR.BENCH (Gao\net al., 2023). We demonstrate that a multi-\ntask, clinically-trained language model out-\nperforms its general domain counterpart by\na large margin, establishing a new state-of-the-\nart performance, with a ROUGE-L score of\n28.55. This research underscores the value of\ndomain-specific training for optimizing clinical\ndiagnostic reasoning tasks.\n1 Introduction\nThe electronic health record (EHR) contains daily\nprogress notes authored by healthcare providers\nto represent the daily changes in care plans for\ntheir patients, including an updated list of active\ndiagnoses. The daily progress note is one of the\nmost important note types in the EHR and con-\ntains the daily subjective and objective details in\nthe patient’s care, which is summarized into an as-\nsessment of the overall leading diagnoses with a\ntreatment plan section (Gao et al., 2022b). How-\never, note bloat is a common phenomenon in medi-\ncal documentation intermixed with billing require-\nments, non-diagnostic information, and copy and\npaste from prior notes (Rule et al., 2021). These\nadditional documentation practices contribute to\nprovider burnout and cognitive overload (Gardner\net al., 2018). Problem-based charting is important\nFigure 1: Training T5 with multi-task setup with six\ntasks from DR.BENCH (Gao et al., 2023)\nto improve care throughput and help reduce diag-\nnostic errors (Wright et al., 2012).\nThe medical reasoning process is complex and\nincorporates medical knowledge representation\nwith analytical and experiential knowledge (Bowen,\n2006). Patel and Groen developed a theory from the\nAI literature that experts use \"forward-reasoning\"\nfrom data to diagnosis 1986. The recently released\nbenchmark DR.BENCH (Diagnostic Reasoning\nBenchmark) is intended to assess the ability of AI\nmodels to perform such reasoning, with multiple\ncomponent tasks including diagnostic reasoning\nwith EHR data for experiential knowledge, medical\nexams for knowledge representation, progress note\nstructure prediction, and problem summarization\ntasks that included both extractive and abstractive\nmedical diagnoses (Gao et al., 2023).\nIn this work, we focus primarily on the problem\nsummarization task from the DR.BENCH suite,\nbut with the hypothesis that using all tasks in\nDR.BENCH would improve the problem summa-\nrization task over the problem summarization task\nbeing fine-tuned alone. We make use of the T5 fam-\n78\nily of sequence-to-sequence language models, (Raf-\nfel et al., 2020), which are first pretrained on a\nlarge unlabeled dataset and then finetuned on spe-\ncific multiple downstream tasks. The text-to-text\napproach in our experiment makes it possible to\nperform multi-task training. Hence, the T5 mod-\nels were ideal for experimenting with single and\nmulti-task techniques.\nFurther, we experimented with a recently\ndeveloped clinically-trained T5 model to\nquantify the value of in-domain pretrain-\ning data (Lehman and Johnson, 2023). We\nmake our software publically available at\nhttps://git.doit.wisc.edu/smph-public/dom/uw-icu-\ndata-science-lab-public/drbench.\n2 Related Work\nIn the clinical domain, biomedical text summa-\nrization is a growing field. Common approaches\nto text summarization include feature-based meth-\nods (Patel et al., 2019), fine-tuning large language\nmodels (Lewis et al., 2020), and domain adapta-\ntion with fine-tuning methods (Xie et al., 2023).\nResearchers developed clinical methods for sum-\nmarization from progress notes but these methods\nwere restricted to specific diseases such as diabetes\nand hypertension (Liang et al., 2019). Moreover,\nthese methods for summarization were more ex-\ntractive than abstractive, using a combination of\nheuristics rules and deep learning techniques, and\ndid not use large language models (Liang et al.,\n2019). In another work, an extractive-abstractive\napproach was used where meaningful sentences\nwere extracted from the clinical notes first; these\nsentences were then fed into the transformer model\nfor abstractive summarization (Pilault et al., 2020).\nUnfortunately, the transformer model frequently\nproduced hallucinated outputs, and was not coher-\nent when compared to the ground truth (Pilault\net al., 2020). In a similar extractive-abstractive\napproach, researchers used a pointer generator net-\nwork to generate a note summary cluster and a\nlanguage model such as T5 to generate an abstrac-\ntive summary (Krishna et al., 2021). None of these\napproaches used multi-task training or focused on\nclinically trained encoder-decoder since clinical T5\n1PubMed is a large open source biomedical and lifescience\ndatabase consists of 35 million citation and abstract, and PMC\n(PubMed Central) consists of full articles. MIMIC-III and\nMIMIC-IV (Medical Information Mart for Intensive Care) are\ndatabases consisting of de-identified datasets from Beth Israel\nDeaconess Medical Center\nwas only recently introduced. Prior work has not\naddressed the challenge of abstractive reasoning,\nor they used a two-step process to create abstrac-\ntions. Recently, researchers used domain adaptive\nT5 model trained on the biomedical dataset but did\nnot experiment with multi-task settings (Gao et al.,\n2023).\n3 Methods\n3.1 Dataset\nIn our experiments, we used DR.BENCH (Gao\net al., 2023), a recently introduced benchmark de-\nsigned to evaluate diagnostic reasoning capabilities\nof generative language models. DR.BENCH con-\nsists of three categories of tasks (two tasks per\ncategory), as shown in Figure 1. From top to the\nbottom, the categories and six tasks are: Medical\nKnowledge Representation:(1) Medical Natural\nLanguage Inference (MedNLI) task that consid-\nered sentence pairs with the objective to determine\nwhether the hypothesis sentence could be inferred\nfrom the premise sentence (Shivade, 2019) (14,049\nsentence pairs total); (2) Assessment and Plan Rea-\nsoning (A/P) task whose objective was to label\nrelations between the assessment and treatment\nplan sections (5,897 samples). Clinical Evidence\nUnderstanding and Integration:(1) Electronic\nMedical Records Question Answering (emrQA)\nwhose objective was to answer questions based\non discharge summaries (53,199 questions total)\n(Pampari et al., 2018); (2) Progress Note Section\nLabeling task whose objective was to labels SOAP\nsections in progress notes (134,089 samples) (Gao\net al., 2022a). Diagnosis Generation and Sum-\nmarization: (1) Medical Board Exam Question\nAnswering (MedQA) task that consisted of med-\nical board exam question-answer pairs (12,725\npairs) (Jin et al., 2021); (2) Problem Summariza-\ntion (ProbSumm) task whose goal was to produce\nthe list of relevant problems and diagnoses based\non the input that consisted of the SOAP sections of\nprogress notes (2,783 samples).\nIn this work, we focused primarily on the prob-\nlem summarization task, which was the most diffi-\ncult but also believed to be the most impactful of\nthe six DR.BENCH tasks for downstream clinical\napplication.\n3.2 Experimental Setup\nIn our experiments, we used six generative lan-\nguage models, all based on the Text-To-Text Trans-\n79\nModel Training Corpus Initialization Citation\nT5 220M Colossal Clean Crawled Corpus (C4) Random (Raffel et al., 2020)T5 770M Random\nSCIFIVE 220M C4 + PubMed (abstracts) + PMC T5 220M (Phan et al., 2021)SCIFIVE 770M T5 770M\nCLINICAL -T5 220M MIMIC-III + MIMIC-IV T5 220M (Lehman and Johnson, 2023)CLINICAL -T5 770M Random\nTable 1: T5 pretrained models used in the experiments. 1\nfer Transformer (T5) model (Raffel et al., 2020).\nThe text-to-text paradigm utilized by T5 was a nat-\nural choice for our stated goal of exploring multi-\ntask learning: transforming T5 into a multi-task\nlearner simply involved prefixing individual task\ninstances with a task-specific prompt after which\nthe model could be trained using the standard cross-\nentropy loss.\nTable 1 provides details about the models. We\ncompared a multi-task scenario in which T5 vari-\nants were fine-tuned on all DR.BENCH tasks and a\nsingle-task scenario in which T5 was fine-tuned on\nthe problem summarization task only. We trained\nT5 models as follows:\nSingle-task training:In single-task training for\nproblem summarization, we used the text of the\nassessment, subjective and objective sections of the\nprogress notes as input and trained T5 to generate\nthe list of problems and diagnoses.\nMulti-task training: In multi-task training,\nwe combined all DR.BENCH tasks into a single\ndataset and trained T5 to generate task-specific out-\nput given task-specific input. Training examples of\neach task were prefixed with a task-specific prompt.\nThe open-book setting only was used for MedQA.\nThe rest of preprocessing follows (Gao et al., 2023).\nTo enable comparison with existing work (Gao\net al., 2023) we used ROUGE-L score (Lin, 2004)\nas our evaluation metric. ROUGE-L uses the\nlongest common subsequence statistics to com-\npare model outputs. A resampling technique with\n1000 bootstrap samples was used to estimate the\n95% confidence intervals (CI) (DiCiccio and Efron,\n1996).\nNote that the Clinical-T5 model used in our ex-\nperiment was pretrained on the same data (MIMIC-\nIII) that was annotated by some DR.BENCH tasks\n(e.g. problem summarization and EmrQA). This\nsetting is known as transductive learning. Truns-\nductive learning is a very realistic scenario for the\nclinical domain where due to privacy issues, lan-\nguage models are likely be pretrained on the data\nfrom the same institution as the data to which they\nwould be applied. Obviously, it would also be in-\nteresting to investigate the performance of a T5\nvariant that was trained on a clinical corpus that\nwas different from which the evaluation data were\nsourced. Unfortunately, this was not possible due\nto the fact that MIMIC was the only publicly avail-\nable corpus of clinical notes and it was used for\ntraining clinical language models.\nThe training data consisted of one progress note\nper unique patient. A separate cohort of unique\npatients was selected for the test set, ensuring no\noverlap between the train and test splits. All exper-\niments used Adam optimizer with a learning rate\nof 1e-5, batch size of 8, beam size of 5, and 100\nepochs with early stopping. The learning rate and\nbatch size were picked based on the best hyper-\nparameters found from the prior work (Gao et al.,\n2023). All experiments were completed on a sin-\ngle A100 GPU with 40 GB memory. The models\nwere reviewed for error analysis by a critical care\nphysician on the full test set of 86 progress notes\nand common observations were highlighted with\nexamples in the error analysis.\n4 Results and Discussion\nThe results of our experiments are summarized\nin Table 2. The full set of results including the\nconfidence intervals is available in the Appendix\n(Table 4).\nClinical-T5 770M trained in the multi-task set-\nting demonstrated the best performance (28.55) for\nthe Summarization task, establishing a new state-\nof-the art for this task. The single-task setting for\nthe same T5 variant was a close second (28.28).\nT5 variants trained on in-domain data (SciFive\nand Clinical-T5) performed better than their gen-\neral domain counterpart T5 models of the same size.\n80\nAll models, except Clinical-T5 experienced a drop\nin performance when trained in a multi-task ap-\nproach. We hypothesize that the models pretrained\non non-clinical data were overwhelmed with out-\nof-domain (i.e. clinical) data when trained in a\nmulti-task way and failed to generalize as a result.\nPredictably, larger models performed at least as\nwell as the smaller models and outperformed the\nsmaller models in most scenarios.\nAdmittedly, our work leaves open the question of\nwhether the state-of-the-art performance obtained\nby Clinical-T5 770M has to do with the fact that it\nwas pretrained on MIMIC notes, which were also\nannotated in the problem summarization task. At\nthe same time, the performance of other T5 vari-\nants, such as SciFive 770M, was close, without it\npretraining on MIMIC. This suggests that another\nT5 variant trained on a corpus of clinical notes that\nwas different from MIMIC would perform at least\nas well or better depending on the size of the pre-\ntraining corpus. It should be noted that the model of\nthis size, 770M parameters, can very likely absorb\nsignificantly larger amounts of clinical notes than\nwhat was available in MIMIC (Hoffmann et al.,\n2022). We leave verifying this hypothesis for fu-\nture work.\nModel Training Summarization\nGao et al., 2023 Single task 7.60 (5.31 - 9.89)\nT5 220M Single task 26.35 (22.18 - 30.52)\nMulti-task 24.84 (20.28 - 29.40)\nT5 770M Single task 26.90 (22.58 - 31.23)\nMulti-task 23.99 (19.86 - 28.13)\nSCIFIVE 220M Single task 25.31 (21.45 - 29.17)\nMulti-task 24.38 (19.99 - 28.78)\nSCIFIVE 770M Single task 27.31 (23.09 - 31.53)\nMulti-task 25.31 (21.45 - 29.17)\nCLINICAL -T5 Single task 25.35 (21.19 - 29.51)\n220M Multi-task 26.21 (21.92 - 30.49)\nCLINICAL -T5 Single task 28.28 (24.17 - 32.38)\n770M Multi-task 28.55 (24.29 - 32.80)\nTable 2: Performance of fine-tuned T5 models on the\nsummarization task. 95% confidence intervals are in-\ncluded. The first row is a baseline representing the\nbest performance on this task to date. Please see the\nAppendix for the full set of results.\nError Analysis: Although both clinical mod-\nels produced similar ROUGE-L scores, the model\ntrained in a single-task setting appeared to achieve\nbetter abstraction during error analysis. For the\nexample in Table 5, the assessment described sep-\nsis but does not mention the source of the sepsis\ninfection in multi-task Clinical-T5 770M. The data\nfrom the subjective and objective sections of the\nprogress note described an abdominal source and\nlab results were consistent with a clostridium diffi-\ncile infection. The multi-task prediction was able\nto generate sepsis but further generated text that the\nsource was unclear. The single task performed bet-\nter abstraction and generated clostridium difficile as\nthe source for the infection, which was more accu-\nrate during expert review. In another diagnosis, the\nground truth label was “EtOH Withdrawal\" (alco-\nhol withdrawal). The multitask extracted “altered\nmental status, hypertensive, tachycardia,\" (symp-\ntoms of withdrawal) whereas the single task was\nable to abstract “DTs EtOH w d,\" (delirium tremens\nalcohol withdrawal - a type of severe alcohol with-\ndrawal in critically ill patients). Again, the sin-\ngle task achieved greater accuracy with abstraction\nfrom symptoms of alcohol withdrawal presented in\nthe earlier sections of the note.\nResource Utilization: The experiments were\nconducted on the Google Cloud Platform using one\nA100 40 GB NVIDIA GPU on a Linux base sys-\ntem. For all experiments, the total training time\nwas approximately 250 hours for both single-task\nand multi-task approaches. The carbon emission\nfootprint was 35.5 kilograms (kg) of CO2. How-\never, the total carbon emission was only 4.5 kg\nof CO2 for the single-task experiments. (Lacoste\net al., 2019)\n5 Conclusion\nIn this work we experiment with the\nDR.BENCH suite of tasks and established a\nnew state-of-the-art result on the problem list\ngeneration task, a task critical for AI-assisted diag-\nnostic reasoning. Our other contribution indicates\nthat multi-task learning does not work well, unless\nin-domain data was used for pretraining and that\nincluded (unlabeled) task data during pretraining (a\nscenario known as transductive learning) leads to\nthe best performance. Finally, our work provides\nevidence that generative models benefit from\npretraining on in-domain data. In future work, we\nplan to explore the utility of decoder-only LLMs\nfor clinical diagnostic reasoning.\n81\n6 Limitations\nThe limitation of this work was the use of\nROUGE-L as the evaluation metric. Given the\nmany acronyms and synonyms in medical writ-\ning, ROUGE-L, based on the longest common se-\nquence, does not capture the many nuances in its\nscore. Researchers have shown concerns for the\nROUGE score and have developed metrics for sum-\nmarization that are more semantically aware of the\nground truth (Akter et al., 2022), but their usability\nis yet to be validated.\nTraining large language models from scratch\nuses a considerable amount of carbon footprint.\n(Patterson et al., 2021) Fine-tuning large language\nmodels for downstream tasks is one way to reduce\ncarbon footprint but still needs to be cost-effective.\nAs the AI community progresses in this field, de-\nveloping a cost-effective and carbon-friendly solu-\ntion is needed. The NLP field is moving towards\nprompt-based methods with larger LLMs (Lester\net al., 2021), so the next step for this research is\nto experiment with soft prompting approaches to\naddress low resource settings and leverage prompt\ntuning in LLMs for the problem summarization\ntask.\n7 Ethics Statement\nThis research utilized a deidentified dataset that\ndoes not include any protected health informa-\ntion. This dataset operates in compliance with the\nPhysioNet Credential Health Data Use Agreement\n(v1.5.0). All experiments conducted adhered to the\nguidelines outlined in the PhysioNet Credentialed\nHealth Data License Agreement. Additionally, this\nstudy has been deemed exempt from human sub-\njects research.\nReferences\nMousumi Akter, Naman Bansal, and Shubhra Kanti\nKarmaker. 2022. Revisiting automatic evaluation\nof extractive summarization task: Can we do better\nthan ROUGE? In Findings of the Association for\nComputational Linguistics: ACL 2022, pages 1547–\n1560, Dublin, Ireland. Association for Computational\nLinguistics.\nJudith L. Bowen. 2006. Educational strategies to pro-\nmote clinical diagnostic reasoning. New England\nJournal of Medicine , 355(21):2217–2225. PMID:\n17124019.\nThomas J. DiCiccio and Bradley Efron. 1996. Bootstrap\nconfidence intervals. Statistical Science, 11(3):189–\n212.\nYanjun Gao, John Caskey, Timothy Miller, Brihat\nSharma, Matthew M. Churpek, Dmitriy Dligach, and\nMajid Afshar. 2022a. Tasks 1 and 3 from progress\nnote understanding suite of tasks: Soap note tagging\nand problem list summarization. PhysioNet.\nYanjun Gao, Dmitriy Dligach, Timothy Miller, John\nCaskey, Brihat Sharma, Matthew M. Churpek, and\nMajid Afshar. 2023. Dr.bench: Diagnostic reasoning\nbenchmark for clinical natural language processing.\nJournal of Biomedical Informatics, 138:104286.\nYanjun Gao, Dmitriy Dligach, Timothy Miller, Samuel\nTesch, Ryan Laffin, Matthew M. Churpek, and Majid\nAfshar. 2022b. Hierarchical annotation for building\na suite of clinical natural language processing tasks:\nProgress note understanding. In Proceedings of the\nThirteenth Language Resources and Evaluation Con-\nference, pages 5484–5493, Marseille, France. Euro-\npean Language Resources Association.\nRebekah L Gardner, Emily Cooper, Jacqueline Haskell,\nDaniel A Harris, Sara Poplau, Philip J Kroth, and\nMark Linzer. 2018. Physician stress and burnout: the\nimpact of health information technology. Journal\nof the American Medical Informatics Association ,\n26(2):106–114.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14).\nKundan Krishna, Sopan Khosla, Jeffrey Bigham, and\nZachary C. Lipton. 2021. Generating SOAP notes\nfrom doctor-patient conversations using modular\nsummarization techniques. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 4958–4972, Online. As-\nsociation for Computational Linguistics.\nAlexandre Lacoste, Alexandra Luccioni, Victor\nSchmidt, and Thomas Dandres. 2019. Quantifying\nthe carbon emissions of machine learning. arXiv\npreprint arXiv:1910.09700.\nEric Lehman and Alistair Johnson. 2023. Clinical-t5:\nLarge language models built using mimic clinical\ntext. PhysioNet.\n82\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nJennifer Liang, Ching-Huei Tsou, and Ananya Poddar.\n2019. A novel system for extractive clinical note\nsummarization using EHR data. In Proceedings of\nthe 2nd Clinical Natural Language Processing Work-\nshop, pages 46–54, Minneapolis, Minnesota, USA.\nAssociation for Computational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nAnusri Pampari, Preethi Raghavan, Jennifer Liang, and\nJian Peng. 2018. emrQA: A large corpus for question\nanswering on electronic medical records. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2357–2368,\nBrussels, Belgium. Association for Computational\nLinguistics.\nDarshna Patel, Saurabh Shah, and Hitesh Chhinkani-\nwala. 2019. Fuzzy logic based multi document sum-\nmarization with improved sentence scoring and re-\ndundancy removal technique. Expert Systems with\nApplications, 134:167–177.\nVimla L. Patel and Guy J. Groen. 1986. Knowledge\nbased solution strategies in medical reasoning. Cog-\nnitive Science, 10(1):91–116.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. 2021. Carbon\nemissions and large neural network training.\nLong N Phan, James T Anibal, Hieu Tran, Shaurya\nChanana, Erol Bahadroglu, Alec Peltekian, and Gré-\ngoire Altan-Bonnet. 2021. Scifive: a text-to-text\ntransformer model for biomedical literature. arXiv\npreprint arXiv:2106.03598.\nJonathan Pilault, Raymond Li, Sandeep Subramanian,\nand Chris Pal. 2020. On extractive and abstractive\nneural document summarization with transformer lan-\nguage models. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 9308–9319, Online. As-\nsociation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nAdam Rule, Steven Bedrick, Michael F. Chiang, and\nMichelle R. Hribar. 2021. Length and Redundancy\nof Outpatient Progress Notes Across a Decade at an\nAcademic Medical Center. JAMA Network Open ,\n4(7):e2115334–e2115334.\nChaitanya Shivade. 2019. Mednli - a natural language\ninference dataset for the clinical domain. PhysioNet.\nAdam Wright, Justine Pang, Joshua C Feblowitz,\nFrancine L Maloney, Allison R Wilcox, Karen Sax\nMcLoughlin, Harley Ramelson, Louise Schneider,\nand David W Bates. 2012. Improving completeness\nof electronic problem lists through clinical decision\nsupport: a randomized, controlled trial. Journal\nof the American Medical Informatics Association ,\n19(4):555–561.\nQianqian Xie, Zheheng Luo, Benyou Wang, and Sophia\nAnaniadou. 2023. A survey on biomedical text sum-\nmarization with pre-trained language model.\nA Appendix\nThis section adds two more tables to show the re-\nsults of the other clinical task. The results are com-\npared with previous results and can be seen in the\nbaseline column. In many cases, such as MedNLI,\nAP, and EmrQA, we can see improvement in the\nmultitask experiments.\n83\nModel Training Summarization SOAP A/P\nGao et al., 2023 Single task 7.60 (5.31 - 9.89) 60.12 (59.33 - 60.90) 80.09 (79.32 - 83.23)\nT5 220M Single task 26.35 (22.18 - 30.52) 60.12 (59.33 - 60.90)* 73.31 (71.34 - 77.65)*\nMulti-task 24.84 (20.28 - 29.40) 56.63 (55.83 - 57.42) 43.25 (41.35 - 66.59)\nT5 770M Single task 26.90 (22.58 - 31.23) 55.57 (54.78 - 56.35)* 77.96 (75.38 - 81.60)*\nMulti-task 23.99 (19.86 - 28.13) 51.10 (50.32 - 51.91) 75.15 (71.93 - 78.19)\nSCIFIVE 220M Single task 25.31 (21.45, 29.17) 57.74 (56.95 - 58.53)* 76.76 (74.81 - 80.92)*\nMulti-task 24.38 (19.99 - 28.78) 54.86 (54.06 - 55.65) 68.87 (65.50 - 72.12)\nSCIFIVE 770M Single task 27.31 (23.09 - 31.53) 47.65 (46.85 - 48.47)* 75.11 (73.10,79.42)*\nMulti-task 25.31 (21.45 - 29.17) 44.51 (43.72- 45.29) 77.50 (74.45 - 80.37)\nCLINICAL -T5 220M Single task 25.35 (21.19 - 29.51) 55.30 (54.51 - 56.11) 80.44 (77.47 - 83.35)\nMulti-task 26.21 (21.92 - 30.49) 52.41 (51.62 - 53.20) 65.49 (62.08 - 68.76)\nCLINICAL -T5 770M Single task 28.28 (24.17 - 32.38) 52.82 (52.03 - 53.61) 78.79 (75.76 - 81.66)\nMulti-task 28.55 (24.29 - 32.80) 54.00 (53.21 - 54.80) 80.58 (77.57 - 83.38)\nTable 3: Finetuned T5 models on various clinical task with 95% confidence interval calculated using the bootstrap-\nping method. A/P represents assessment and plan relational labeling task. Summarization use ROUGE L, A/P use\nF1-macro and SOAP use accuracy score for the evaluation metrics. The first row in the table represents best scores\nreported in the DR.BENCH paper and * in the other rows represent scores for the respective task in DR.BENCH\npaper (Gao et al., 2023)\nModel Training EmrQA MedNLI MedQA\nGao et al., 2023 Single task 39.20 (34.63 - 43.78) 84.88 (82.98 - 86.64) 24.59 (22.31 - 27.02)\nT5 220M Single task 33.40 (29.27 - 37.61)* 79.75 (78.62 - 82.70)* 22.55 (20.01 - 25.69)*\nMulti-task 38.48 (37.24 - 39.79) 72.57 (70.18 - 74.82) 21.75 (19.48 - 24.12)\nT5 770M Single task 38.05 (33.56 - 42.58)* 84.04 (82.14 - 85.86)* 20.97 (18.77 - 23.25)*\nMulti-task 41.42 (40.16, 42.72) 83.19 (81.22, 85.09) 23.25 (20.97, 25.61)\nSCIFIVE 220M Single task 37.28 (32.84 - 42.11)* 82.84 (80.87 - 84.74)* 22.78 (20.50 - 25.14)*\nMulti-task 40.08 (38.82 - 41.39) 78.83 (76.72 - 80.94) 21.52 (19.32 - 23.80)\nSCIFIVE 770M Single task 41.21 (39.93 - 42.49) 83.89 (82.00 - 85.79) 23.09 (20.82 - 25.37)\nMultitask 41.26 (39.98 - 42.56) 84.35 (82.49 - 86.22) 23.72 (21.37 - 26.08)\nCLINICAL -T5 220M Single task 41.35 (40.07 - 42.65) 84.32 (82.42 - 86.15) 21.92 (19.64 - 24.19)\nMulti-task 40.30 (39.02 - 41.62) 71.23 (68.92 - 73.56) 22.46 (20.19 - 24.74)\nCLINICAL -T5 770M Single task 42.69 (41.39 - 43.95) 85.86 (85.02 - 88.47) 24.27 (21.92 - 26.63)\nMulti-task 42.61 (41.34 - 43.92) 86.14 (84.32 - 87.90) 25.84 (23.41 - 28.28)\nTable 4: Finetuned T5 models on various clinical task with 95% confidence interval calculate using the bootstrapping\nmethod. All the evaluation metrics here are the accuracy score. The first row in the table represents best scores\nreported in the DR.BENCH paper and * in the other rows represent scores for the respective task in DR.BENCH\npaper (Gao et al., 2023)\n84\nInput Ground Truth\nDiagnoses/\nProblems\nT5 770M\nSingle task\nClinical-T5\n770M\nSingle task\nClinical-T5\n770M\nMulti-task\nSUMMARIZE: <ASSESSMENT> 48 y/o M\nwith HIV 47M s/p elective spinal surgery\n(anterior and posterior LIFs), intubated - - - -\n<SUBJECTIVE> Agitated, diaphoretic,\naltered, hypertensive and tachy this AM - - - -\n<OBJECTIVE> Last dose of Antibiotics:\nInfusions: Other ICU medications: Heparin\nSodium (Prophylaxis) - - - -\nEtOH\nwithdrawal\nSpinal surgery\nAltered MS\ns p elective\nspinal surgery\nDTs EtOH w d\npain h o chronic\npain\nAltered mental\nstatus\nHypertension\nTachycardia\nAcute renal\nfailure s p spinal\nsurgery\nSUMMARIZE: <ASSESSMENT> SEPSIS\nWITHOUT ORGAN DYSFUNCTION Ms.\n[**Known lastname 10381**] is a 76F with\nmultiple medical problems, who is - - - - - - -\n<SUBJECTIVE> FEVER - 101.7 F - [**2129-\n9-3**] 12:33 PM -received boluses overnight\nfor low SBP - - - - <OBJECTIVE> Last dose\nof Antibiotics: Cefipime - [**2129-9-3**]\n04:05 PM Metronidazole - [**2129-9-4**]\n04:00 AM - - - -\nSepsis Patient\nhas re developed\nfevers on 9 2 on\na regimen of\nvancomycin\nceftriaxone\nPossible sources\ninclude 1 Intra\nabdominal\nsource\nSepsis Thrombo-\ncytopenia\nSepsis Likely\nsource is\nclostridium\ndifficile colitis\nAcute renal\nfailure\nHypotension\nLikely\nsecondary to\nsepsis though\nsource unclear at\nthis time Acute\nrenal failure\nTable 5: The table represents a snippet of the input and output sections of problem summarization. The input data\ncontains an added prefix that denotes the task for T5, \"SUMMARIZE\" in this case, and <prefix> that defines the\nnote section. Finally, \"- - - -\" is the continuation of the section, which was excluded here due to the space constraint.\n85"
}