{
  "title": "LasigeBioTM at MEDIQA 2019: Biomedical Question Answering using Bidirectional Transformers and Named Entity Recognition",
  "url": "https://openalex.org/W2970035587",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A720437692",
      "name": "Andre Lamurias",
      "affiliations": [
        "University of Lisbon"
      ]
    },
    {
      "id": "https://openalex.org/A2046459946",
      "name": "Francisco M Couto",
      "affiliations": [
        "University of Lisbon"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970986790",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2807280045",
    "https://openalex.org/W1964162497",
    "https://openalex.org/W2903305690",
    "https://openalex.org/W2115748294",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3140009346",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Biomedical Question Answering (QA) aims at providing automated answers to user questions, regarding a variety of biomedical topics. For example, these questions may ask for related to diseases, drugs, symptoms, or medical procedures. Automated biomedical QA systems could improve the retrieval of information necessary to answer these questions. The MEDIQA challenge consisted of three tasks concerning various aspects of biomedical QA. This challenge aimed at advancing approaches to Natural Language Inference (NLI) and Recognizing Question Entailment (RQE), which would then result in enhanced approaches to biomedical QA. Our approach explored a common Transformer-based architecture that could be applied to each task. This approach shared the same pre-trained weights, but which were then fine-tuned for each task using the provided training data. Furthermore, we augmented the training data with external datasets and enriched the question and answer texts using MER, a named entity recognition tool. Our approach obtained high levels of accuracy, in particular on the NLI task, which classified pairs of text according to their relation. For the QA task, we obtained higher Spearman's rank correlation values using the entities recognized by MER.",
  "full_text": "Proceedings of the BioNLP 2019 workshop, pages 523–527\nFlorence, Italy, August 1, 2019.c⃝2019 Association for Computational Linguistics\n523\nLasigeBioTM at MEDIQA 2019: Biomedical Question Answering using\nBidirectional Transformers and Named Entity Recognition\nAndre Lamurias∗ and Francisco M. Couto\nLASIGE, Faculdade de Ciˆencias, Universidade de Lisboa, Portugal\nAbstract\nBiomedical Question Answering (QA) aims\nat providing automated answers to user ques-\ntions, regarding a variety of biomedical topics.\nFor example, these questions may ask for re-\nlated to diseases, drugs, symptoms, or medical\nprocedures. Automated biomedical QA sys-\ntems could improve the retrieval of informa-\ntion necessary to answer these questions. The\nMEDIQA challenge consisted of three tasks\nconcerning various aspects of biomedical QA.\nThis challenge aimed at advancing approaches\nto Natural Language Inference (NLI) and Rec-\nognizing Question Entailment (RQE), which\nwould then result in enhanced approaches to\nbiomedical QA.\nOur approach explored a common\nTransformer-based architecture that could be\napplied to each task. This approach shared the\nsame pre-trained weights, but which were then\nﬁne-tuned for each task using the provided\ntraining data. Furthermore, we augmented\nthe training data with external datasets and\nenriched the question and answer texts using\nMER, a named entity recognition tool. Our\napproach obtained high levels of accuracy, in\nparticular on the NLI task, which classiﬁed\npairs of text according to their relation. For\nthe QA task, we obtained higher Spearman’s\nrank correlation values using the entities\nrecognized by MER.\n1 Introduction\nQuestion Answering (QA) is a text mining task\nfor which several systems have been proposed\n(Hirschman and Gaizauskas, 2001). This task is\nparticularly challenging in the biomedical domain\nsince this is a complex subject as answers may\nnot be as straightforward compared to other do-\nmains. However, clinical and health care informa-\ntion systems could beneﬁt greatly from automated\n∗alamurias@lasige.di.fc.ul.pt\nbiomedical QA systems, which could improve the\nretrieval of information necessary to answer these\nquestions.\nTo help progress on this topic, the MEDIQA\nchallenge proposed three tasks in the biomedical\ndomain(Ben Abacha et al., 2019):\n1. Natural Language Inference (NLI) - classify\nthe relation between two sentences as either\nentailment, neutral or contradiction;\n2. Recognizing Question Entailment (RQE) -\nclassify if two questions are entailed with\neach other or not;\n3. Question Answering (QA) - classify which\nanswers are correct for a given answer and\nrank them.\nWe applied the same approach to all three\ntasks since they all could be modelled as text\nclassiﬁcation tasks. The objectives of the tasks\nwere to classify pairs of text: sentence-sentence\n(NLI), question-question (RQE), and question-\nanswer (QA). For the NLI task, we had three pos-\nsible labels for each pair (entailment, neutral or\ncontradiction), while the RQE task was a binary\nclassiﬁcation. For the QA task, each pair should\nbe given a reference score representing how well\nthe question is answered, which ranged between 1\nand 4.\nQA is a complex task that involves various com-\nponents, and can be approached in several ways.\nWhile real-world scenarios require the retrieval of\ncorrect answers from larger databases, the QA task\nof this challenge simpliﬁed this problem by pro-\nviding up to 10 answers retrieved by the medical\nQA system CHiQA. This system also provided a\nranking to each answer, however, we observed that\nthis ranking did not follow the manual ranking in\nmost cases. We also observed that the retrieved\n524\nanswers could consist of one or more sentences.\nWhile in some QA scenarios, systems are required\nto select the text span that contains the answer (Ra-\njpurkar et al., 2016), in this case it was only re-\nquested to re-rank the retrieved answers and clas-\nsify which ones were correct. Although speciﬁc\nranking algorithms exist (Radev et al., 2000), due\nto the nature of the task and the fact that the other\ntwo tasks involved comparison of text, we decided\nto train a classiﬁer that compared each question\nwith a potential answer, i.e., we predicted how\ngood a text is at answering a given question.\nOur approach uses pre-trained weights as a\nstarting point, to ﬁne-tune deep learning models\nbased on the Transformer architecture for each of\nthe challenge tasks (Vaswani et al., 2017). We\nused the BioBERT weights, trained on PubMed\nabstracts and PMC full articles, as the type of text\nshould be more similar to the challenge data than\nthe standard BERT models, which were trained on\nWikipedia and BookCorpus. Furthermore, we in-\ncorporated other datasets into the RQE and QA\ntasks, and enrich the training data with seman-\ntic information obtained using MER (Minimal\nNamed-Entity Recognizer) (Couto and Lamurias,\n2018), a high computing performance named en-\ntity recognition tool.\n2 Related Work\nDeep learning approaches have lead to state-of-\nthe-art results in various text mining tasks. These\napproaches make use of intermediary representa-\ntions of the data to then ﬁne-tune the weights to\ndifferent tasks. Various models have been pro-\nposed, and, recently, the most successful ones\nhave been based around the Transformer archi-\ntecture (Vaswani et al., 2017). An advantage of\nthis type of models is that we can use pre-trained\nweights such as those provided by BERT (Devlin\net al., 2018) as a starting point to train a model for\na speciﬁc task. These weights are tuned on large\ncorpora using the Transformer architecture and\nhave been shown to be effective language models.\nDifferent models were made available by the au-\nthors, with two variations of the architecture, and\nwhether the true case and accent markers of the\ntokens are taken into account.\nDue to the effectiveness of the BERT architec-\nture, it has been already adapted for other do-\nmains. Lee et al. (2019) presented a model speciﬁc\nto biomedical language, which was trained on a\nlarge-scale biomedical corpora: 200k PubMed ab-\nstracts, 270k PMC full texts, and a combination of\nthese two. Although the BioBERT models use the\nsame vocabulary as the BERT models, the same\nWordPiece tokenization is performed. This way,\neven if biomedical documents contain words that\nwere not in the original vocabulary, the tokenizer\nwill separate these words into frequent subwords,\nminimizing out-of-vocabulary issues and keeping\ncompatibility with the original models. The au-\nthors tested these models on several biomedical\ntext mining tasks, obtaining competitive perfor-\nmance when compared with other state-of-the-art\nmodels.\nOne of the most common text mining tasks is\nentity recognition. This task is important because\nit is often the ﬁrst step to other tasks, such as entity\nlinking and relation extraction. MER is a simple\nbut efﬁcient approach to entity recognition, which\nuses vocabularies that can be extracted from on-\ntologies to identify and link entities. MER focuses\non simplicity and ﬂexibility to reduce the process-\ning time and the time necessary to adapt to other\ndomains and entity types.\n3 Methodology\n3.1 Data Preparation\nWe participated in the three tasks using the same\napproach by modeling each one as a text classiﬁ-\ncation problem. We used the training data of each\ntasks as document pairs, where a document could\nbe a sentence, paragraph, question or answer. The\nNLI and RQE data had obvious labels, while for\nthe QA data we used the reference scores. How-\never, to distinguish between correct answers with\nmore detail, we also incorporated the manually as-\nsigned ranks to the answers with reference scores\n3 and 4:\nFinalScore = ReferenceScore + 11 − Rank\n10\nAs there are up to 10 possible answers to each\nquestion, the ﬁnal score will range between 1 and\n5.\nWe removed instances where each element of\nthe pair contained the same text, which happened\nsometimes in the RQE training set. Further-\nmore, we performed named entity recognition us-\ning MER to identify several types of entities men-\ntion in both questions and answers. We used MER\nsince it can provide reliable entity mention anno-\ntations at a reasonable speed. We appended the\n525\ntextual labels of the terms recognized to the end\nof the document, as a list separated by whites-\npaces. Since MER matches ontology concepts, if\nthe synonym of a concept was recognized, it was\nconverted to its main label.\nWe recognized terms from the: Human Pheno-\ntype Ontology, Disease Ontology, Chemical Enti-\nties of Biological Interest (ChEBI) ontology and\nGene Ontology. Our objective was to add to each\ntext a list of the entities that could summarize that\ntext. We chose those ontologies because the ques-\ntions were about biomedical subjects, and there-\nfore the ontologies chosen should reﬂect the main\ndomains of the data. The ontologies that we used\ncomprise a total of 350,233 terms.\nWe also explored additional sources of data to\ntrain the classiﬁers, for the RQE and QA tasks.\nRegarding the RQE task, we employed the NLI\ndataset since it also contained entailment relations.\nEven though these datasets were generated from\ndifferent corpora and the NLI dataset and for dif-\nferent purposes, we considered that additional data\ncould still improve the results. To this end, we\ntransformed the NLI dataset so that all entailment\nrelations were labeled as positive, and the neutral\nand contradiction as negative.\nFor the QA task, we added one of the sug-\ngested MedQuAD datasets, namely the Cancer-\nGov dataset. Although all these additional datasets\nhad a similar structure, we did not have time to\ntrain and test which ones would be more helpful\nfor this task. These datasets contained only exam-\nples of correct answers, which we assigned the ref-\nerence score 4, since it could skew the trained clas-\nsiﬁer towards higher scores. To balance this, we\ngenerated incorrect answers from the other QA of\nthe same document. We assumed that if an answer\nwas correct for one question, it would be incorrect\nfor the other questions about the same topic. To\nmake sure this was true, we took into account the\n“qtype” parameter of each question, since it is un-\nlikely that questions of different types would have\nthe same answers. This parameter indicated the\nnature of the question in the context of the main\ntopic of the document. For example, a document\nabout a speciﬁc cancer type could have the follow-\ning “qtypes”: information, symptoms, exams and\ntests, outlook, and treatment.\nRun Training data Dev Test\n1 NLI training set 0.836 0.724\nTable 1: Accuracy obtained on the NLI task.\n3.2 System architecture\nWe adapted the pytorch implementation of\nBERT1. As such, we used the WordPiece tokeniza-\ntion and Adam optimizer that are implemented by\ndefault. We used the BioBERT PubMed+PMC\npretrained weights, which are based on the bert-\nbase-cased model. The authors chose this model\nas many biomedical entities are case sensitive. We\ninitially tested with the standard BERT weights,\nand observed an improvement when using the\nBioBERT weights instead. A model ﬁne-tuned\nto the clinical domain, which is the domain of\nthe documents of this challenge, would be more\nappropriate, but not such pre-trained model was\navailable at the time.\nUsing the data previously described, we trained\nvariations of the same model, focusing mostly on\nthe RQE and QA tasks. These variations consisted\nof the additional datasets previously described, but\nalso different training parameters, such as initial\ntraining rate, number of epochs, batch size and\nmaximum sequence length. We started with the\ndefault values and made incremental changes to\nunderstand if we could improve the results on the\nvalidation set, while training just with the pro-\nvided training set. After setting the best param-\neters, we then trained the classiﬁers on the addi-\ntional datasets.\nFor the NLI, we tested only the baseline ap-\nproach, which consisted in using the BioBERT\nweights ﬁne-tuned for the task.\n4 Results and discussion\nWe submitted one run to the NLI task, three runs\nto the RQE task and four runs to the QA task. We\nfocused mainly on studying the effect of different\ntraining data on the performance of the classiﬁers.\nWe evaluated on the development sets that were\nprovided for each task, and then submitted our\npredictions for the test sets. The scores obtained\nfor the development and test sets of each task are\nshown in tables 1, 2 and 3, as well as the differ-\nences between each run.\n1https://github.com/huggingface/\npytorch-pretrained-BERT\n526\nRun Training data Dev Test\n1 RQE training set 0.732 0.481\n2 RQE training set + NER 0.752 0.481\n3 RQE and NLI training set 0.749 0.485\nTable 2: Accuracy obtained on the RQE task.\nWe can see that the accuracy obtained during\nthe development phase was considerably higher\nthan on the test set. This could have been due to\nthe test set containing other type of questions from\nthe development set, or due to over-ﬁtting of the\nhyper-parameters on the development set, which\nlimited the performance of the model. Both on the\ntest and development set, we obtained high accu-\nracy on the NLI task, for which we submitted only\none run. The NLI data was generated by asking\nexperts to give one example of each class (neutral,\nentailment and contradiction) to a series of state-\nments. As such, this dataset is highly regular and\nthe model was able to learn from it.\nOn the development set, we can see that adding\nthe named entities recognized by MER (Run 2 of\nRQE and QA) improved the accuracy. However,\nthis effect did not occur on the test set; for the\nRQE task, it did not change the accuracy and it de-\ncreased the accuracy of the QA task. On the other\nhand, adding external training data (Run 3) had a\npositive effect on the test set results of both tasks,\nimproving the accuracy of the QA task.\nFor the QA, we also trained a classiﬁer using\nboth the training and development datasets (Run\n4). We could not evaluate this classiﬁer on the de-\nvelopment set since it had already seen those ex-\namples and the results would have been biased.\nHowever, this classiﬁer achieved the best test set\naccuracy and Mean Reciprocal Rank (MRR) of the\nfour runs submitted to this task.\nThe best results obtained with our approach\nwere on the NLI task. However, we considered\nthe QA task to be the main task of the challenge\nand put most effort into it in terms of exploration\nhyper-parameter tuning. Since the organizers con-\nsidered the accuracy to be the main metrics, we\noptimized our system to that metric. While the\nMRR was high on all three runs, the Spearman’s\ncoefﬁcient was generally much lower. This means\nthat although our system was able to detect correct\nanswers to a certain degree, their ranking matched\npoorly with the gold standard.\n5 Conclusions and Future Work\nFor the MEDIQA challenge, we developed a sys-\ntem that could be used for the 3 proposed tasks\nwith minimal changes. This was possible due\nto the recently introduced Transformer architec-\nture, along with pre-trained weights that severely\nreduce the training time necessary to generate a\nlanguage representation model. The training data\nprovided for each task was used to train classiﬁ-\ncation models for each task. We also explored ex-\nternal datasets to improve the models of the RQE\nand QA tasks. We observed that adding more data\nto train the model leads to better results on the test\nset, as expected.\nIn the future we will improve the capacity of the\nmodels to classify new data by adding more exter-\nnal training data. We observed that Runs 3 and 4 of\nthe QA task achieved higher scores, which could\nhave been due to the larger training set employed\nto train the models. While for Run 3 we used only\none additional set, there were 9 more available of\nthe same type, which were not used due to time\nconstraints. A similar strategy could be used to\nﬁnd more pairs of questions with an entailment re-\nlation.\nAnother way to enrich the training set would be\nto automatically retrieve the descriptions of the en-\ntities identiﬁed in the text, or their ancestors, as\nthey also provide useful information about enti-\nties. A similar approach was shown to improve\nthe results of a relation extraction task using deep\nlearning (Lamurias et al., 2019).\nReferences\nAsma Ben Abacha, Chaitanya Shivade, and Dina\nDemner-Fushman. 2019. Overview of the mediqa\n2019 shared task on textual inference, question en-\ntailment and question answering. In Proceedings of\nthe BioNLP 2019 workshop, Florence, Italy, August\n1, 2019. Association for Computational Linguistics.\nF. Couto and A. Lamurias. 2018. MER: a shell script\nand annotation server for minimal named entity\nrecognition and linking. Journal of Cheminformat-\nics, 10(58).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nLynette Hirschman and Robert Gaizauskas. 2001. Nat-\nural language question answering: the view from\nhere. natural language engineering, 7(4):275–300.\n527\nDev Test\nRun Training data Accuracy Spearman MRR Accuracy Spearman MRR\n1 QA training set 0.782 0.067 0.760 0.585 0.220 0.843\n2 QA training sets + NER 0.791 0.198 0.840 0.551 0.026 0.733\n3 QA training sets + CancerGov 0.756 0.183 0.920 0.600 0.201 0.870\n4 QA training and dev sets - - - 0.637 0.211 0.910\nTable 3: Results obtained on the QA task. Spearman: Spearman’s Rank Correlation Coefﬁcient; MRR: Mean\nReciprocal Rank.\nA. Lamurias, Diana Sousa, L. Clarke, and F. Couto.\n2019. BO-LSTM: classifying relations via long\nshort-term memory networks along biomedical on-\ntologies. BMC Bioinformatics, 20(10).\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2019. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. arXiv preprint arXiv:1901.08746.\nDragomir R Radev, John Prager, and Valerie Samn.\n2000. Ranking suspected answers to natural lan-\nguage questions using predictive annotation. In Pro-\nceedings of the sixth conference on Applied natu-\nral language processing, pages 150–157. Associa-\ntion for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8371050357818604
    },
    {
      "name": "Computer science",
      "score": 0.8160786628723145
    },
    {
      "name": "Natural language processing",
      "score": 0.5628117918968201
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5599455237388611
    },
    {
      "name": "Task (project management)",
      "score": 0.5321319103240967
    },
    {
      "name": "Transformer",
      "score": 0.5257701873779297
    },
    {
      "name": "Named-entity recognition",
      "score": 0.4823117256164551
    },
    {
      "name": "Inference",
      "score": 0.445788711309433
    },
    {
      "name": "Information retrieval",
      "score": 0.4393089711666107
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}