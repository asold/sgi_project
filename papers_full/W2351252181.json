{
    "title": "Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning",
    "url": "https://openalex.org/W2351252181",
    "year": 2016,
    "authors": [
        {
            "id": "https://openalex.org/A2234266251",
            "name": "Yulia Tsvetkov",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2013710467",
            "name": "Sunayana Sitaram",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2953931803",
            "name": "Manaal Faruqui",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2798455563",
            "name": "Guillaume Lample",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2252210186",
            "name": "Patrick Littell",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2102413935",
            "name": "David A. Mortensen",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2138706875",
            "name": "Alan W. Black",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2152972109",
            "name": "Lori Levin",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2107310219",
            "name": "Chris Dyer",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2295584157",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2171361956",
        "https://openalex.org/W61749939",
        "https://openalex.org/W2949402715",
        "https://openalex.org/W2251204185",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2144404214",
        "https://openalex.org/W1783473872",
        "https://openalex.org/W2251066368",
        "https://openalex.org/W2250414191",
        "https://openalex.org/W2250192802",
        "https://openalex.org/W2949563612",
        "https://openalex.org/W2251874715",
        "https://openalex.org/W2404169761",
        "https://openalex.org/W342285082",
        "https://openalex.org/W2949952998",
        "https://openalex.org/W2167949757",
        "https://openalex.org/W2396366106",
        "https://openalex.org/W1605308203",
        "https://openalex.org/W2157807817",
        "https://openalex.org/W2251805006",
        "https://openalex.org/W2095650036",
        "https://openalex.org/W2572569512",
        "https://openalex.org/W2235125105",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2158139315",
        "https://openalex.org/W160224403",
        "https://openalex.org/W2252095989",
        "https://openalex.org/W1810943226",
        "https://openalex.org/W202879582",
        "https://openalex.org/W3144357634",
        "https://openalex.org/W2964084097",
        "https://openalex.org/W2080213370",
        "https://openalex.org/W1527575280"
    ],
    "abstract": "We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences---a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually.",
    "full_text": "Proceedings of NAACL-HLT 2016, pages 1357–1366,\nSan Diego, California, June 12-17, 2016.c⃝2016 Association for Computational Linguistics\nPolyglot Neural Language Models:\nA Case Study in Cross-Lingual Phonetic Representation Learning\nYulia Tsvetkov Sunayana Sitaram Manaal Faruqui Guillaume Lample\nPatrick Littell David Mortensen Alan W Black Lori Levin Chris Dyer\nLanguage Technologies Institute\nCarnegie Mellon University\nPittsburgh, PA, 15213, USA\n{ytsvetko,ssitaram,mfaruqui,glample,plittell,dmortens,awb,lsl,cdyer}@cs.cmu.edu\nAbstract\nWe introduce polyglot language models, re-\ncurrent neural network models trained to pre-\ndict symbol sequences in many different lan-\nguages using shared representations of sym-\nbols and conditioning on typological infor-\nmation about the language to be predicted.\nWe apply these to the problem of modeling\nphone sequences—a domain in which univer-\nsal symbol inventories and cross-linguistically\nshared feature representations are a natural\nﬁt. Intrinsic evaluation on held-out perplexity,\nqualitative analysis of the learned representa-\ntions, and extrinsic evaluation in two down-\nstream applications that make use of phonetic\nfeatures show (i) that polyglot models bet-\nter generalize to held-out data than compara-\nble monolingual models and (ii) that polyglot\nphonetic feature representations are of higher\nquality than those learned monolingually.\n1 Introduction\nNearly all existing language model (LM) architec-\ntures are designed to model one language at a time.\nThis is unsurprising considering the historical im-\nportance of count-based models in which every sur-\nface form of a word is a separately modeled entity\n(English cat and Spanish gato would not likely ben-\neﬁt from sharing counts). However, recent mod-\nels that use distributed representations—in partic-\nular models that share representations across lan-\nguages (Hermann and Blunsom, 2014; Faruqui and\nDyer, 2014; Huang et al., 2015; Lu et al., 2015,inter\nalia)—suggest universal models applicable to mul-\ntiple languages are a possibility. This paper takes a\nstep in this direction.\nWe introduce polyglot language models: neural\nnetwork language models that are trained on and ap-\nplied to any number of languages. Our goals with\nthese models are the following. First, to facilitate\ndata and parameter sharing, providing more training\nresources to languages, which is especially valuable\nin low-resource settings. Second, models trained on\ndiverse languages with diverse linguistic properties\nwill better be able to learn naturalistic representa-\ntions that are less likely to “overﬁt” to a single lin-\nguistic outlier. Finally, polyglot models offer con-\nvenience in a multilingual world: a single model re-\nplaces dozens of different models.\nExploration of polyglot language models at the\nsentence level—the traditional domain of language\nmodeling—requires dealing with a massive event\nspace (i.e., the union of words across many lan-\nguages). To work in a more tractable domain, we\nevaluate our model on phone-based language mod-\neling, the modeling sequences ofsounds, rather than\nwords. We choose this domain since a common\nassumption of many theories of phonology is that\nall spoken languages construct words from a ﬁnite\ninventory of phonetic symbols (represented conve-\nniently as the elements of the the International Pho-\nnetic Alphabet; IPA) which are distinguished by\nlanguage-universal features (e.g., place and manner\nof articulation, voicing status, etc.). Although our\nfocus is on sound sequences, our solution can be\nported to the semantic/syntactic problem as resulting\nfrom adaptation to constraints on semantic/syntactic\nstructure.\nThis paper makes two primary contributions: in\n1357\nmodeling and in applications. In §2, we intro-\nduce a novel polyglot neural language model (NLM)\narchitecture. Despite being trained on multiple\nlanguages, the multilingual model is more effec-\ntive (9.5% lower perplexity) than individual mod-\nels, and substantially more effective than naive base-\nlines (over 25% lower perplexity). Our most effec-\ntive polyglot architecture conditions not only on the\nidentity of the language being predicted in each se-\nquence, but also on a vector representation of its\nphono-typological properties. In addition to learn-\ning representations of phones as part of the poly-\nglot language modeling objective, the model incor-\nporates features about linguistic typology to im-\nprove generalization performance (§3). Our sec-\nond primary contribution is to show that down-\nstream applications are improved by using polyglot-\nlearned phone representations. We focus on two\ntasks: predicting adapted word forms in models of\ncross-lingual lexical borrowing and speech synthe-\nsis (§4). Our experimental results (§5) show that\nin borrowing, we improve over the current state-\nof-the-art, and in speech synthesis, our features are\nmore effective than manually-designed phonetic fea-\ntures. Finally, we analyze the phonological content\nof learned representations, ﬁnding that our polyglot\nmodels discover standard phonological categories\nsuch as length and nasalization, and that these are\ngrouped correctly across languages with different\nphonetic inventories and contrastive features.\n2 Model\nIn this section, we ﬁrst describe in §2.1 the under-\nlying framework of our model—RNNLM—a stan-\ndard recurrent neural network based language model\n(Mikolov et al., 2010; Sundermeyer et al., 2012).\nThen, in §2.2, we deﬁne a Polyglot LM—a modi-\nﬁcation of RNNLM to incorporate language infor-\nmation, both learned and hand-crafted.\nProblem deﬁnition. In the phonological LM,\nphones (sounds) are the basic units. Mapping\nfrom words to phones is deﬁned in pronunciation\ndictionaries. For example, “cats” [kæts] is a se-\nquence of four phones. Given a preﬁx of phones\nφ1,φ2,...,φ t−1, the task of the LM is to estimate\nthe conditional probability of the next phone p(φt |\nφ1,φ2,...,φ t−1).\n2.1 RNNLM\nIn NLMs, a vocabulary V (here, a set of phones\ncomposing all word types in the language) is repre-\nsented as a matrix of parameters X ∈Rd×|V |, with\n|V|phone types represented as d-dimensional vec-\ntors. X is often denoted as lookup table. Phones in\nthe input sequence are ﬁrst converted to phone vec-\ntors, where φi is represented by xi by multiplying\nthe phone indicator (one-hot vector of length |V|)\nand the lookup table.\nAt each time step t, most recent phone preﬁx vec-\ntor1 xt and hidden state ht−1 are transformed to\ncompute a new hidden representation:\nht = f(xt,ht−1),\nwhere f is a non-linear transformation. In the orig-\ninal RNNLMs (Mikolov et al., 2010), the transfor-\nmation is such that:\nht = tanh(Whx xt + Whh ht−1 + bh).\nTo overcome the notorious problem in recurrent\nneural networks of vanishing gradients (Bengio et\nal., 1994), following Sundermeyer et al. (2012),\nin recurrent layer we use long short-term mem-\nory (LSTM) units (Hochreiter and Schmidhuber,\n1997):2\nht = LSTM(xt,ht−1).\nGiven the hidden sequence ht, the output se-\nquence is then computed as follows:\np(φt = i|φ1,...,φ t−1) =\nsoftmax(Woutht + bout)i,\nwhere softmax(xi) = exi∑\nj exj ensures a valid proba-\nbility distribution over output phones.\n1We are reading at each time step the most recent n-gram\ncontext rather than—as is more common in RNNLMs—a sin-\ngle phone context. Empirically, this works better for phone se-\nquences, and we hypothesize that this lets the learner rely on\ndirect connections for local phenomena (which are abundant\nin phonology) and minimally use the recurrent state to model\nlonger-range effects.\n2For brevity, we omit the equations describing the LSTM\ncells; they can be found in (Graves, 2013, eq. 7–11).\n1358\n2.2 Polyglot LM\nWe now describe our modiﬁcations to RNNLM to\naccount for multilinguality. The architecture is de-\npicted in ﬁgure 1. Our task is to estimate the\nconditional probability of the next phone given the\npreceding phones and the language ( ℓ): p(φt |\nφ1,...,φ t−1,ℓ).\nIn a multilingual NLM, we deﬁne a vocabulary\nV∗ to be the union of vocabularies of all training\nlanguages, assuming that all language vocabularies\nare mapped to a shared representation (here, IPA).\nIn addition, we maintain Vℓ with a special symbol\nfor each language (e.g., φenglish, φarabic). Language\nsymbol vectors are parameters in the new lookup ta-\nble Xℓ ∈ Rd×|#langs| (e.g., xenglish, xarabic). The\ninputs to the Polyglot LM are the phone vectors xt,\nthe language character vector xℓ, and the typolog-\nical feature vector constructed externally tℓ. The\ntypological feature vector will be discussed in the\nfollowing section.\nThe input layer is passed to the hidden local-\ncontext layer:\nct = Wcx xt + Wclang xlang + bc.\nThe local-context vector is then passed to the hidden\nLSTM global-context layer, similarly to the previ-\nously described RNNLM:\ngt = LSTM(ct,gt−1).\nIn the next step, the global-context vector gt is\n“factored” by the typology of the training language,\nto integrate manually-deﬁned language features. To\nobtain this, we ﬁrst project the (potentially high-\ndimensional) tℓ into a low-dimensional vector, and\napply non-linearity. Then, we multiply the gt and\nthe projected language layer, to obtain a global-\ncontext-language matrix:\nfℓ = tanh(Wℓtℓ + bℓ),\nGℓ\nt = gt ⊗f⊤\nℓ .\nFinally, we vectorize the resulting matrix into a\ncolumn vector and compute the output sequence as\nfollows:\np(φt = i|φ1,...,φ t−1,ℓ) =\nsoftmax(Woutvec(Gℓ\nt) + bout)i.\nWℓ\nten\nWg\nWout\nLSTM\nWc\ng(t)\nc(t)\nxk\nxӕ\nxt\nxen\ns\ng(t-1)\n G (t)\nx(t)\nf(ℓ)\nℓ\nFigure 1: Architecture of the Polyglot LM.\nModel training. Parameters of the models are the\nlookup tables X and Xℓ, weight matrices Wi, and\nbias vectors bi. Parameter optimization is per-\nformed using stochastic updates to minimize the cat-\negorical cross-entropy loss (which is equivalent to\nminimizing perplexity and maximizing likelihood):\nH(φ,ˆφ) = −Σiˆφi log φi, where φis predicted and\nˆφis the gold label.\n3 Typological features\nTypological information is fed to the model via\nvectors of 190 binary typological features, all of\nwhich are phonological (related to sound structure)\nin their nature. These feature vectors are derived\nfrom data from the W ALS (Dryer and Haspelmath,\n2013), PHOIBLE (Moran et al., 2014), and Ethno-\nlogue (Lewis et al., 2015) typological databases via\nextensive post-processing and analysis. 3 The fea-\ntures primarily concern properties of sound invento-\nries (i.e., the set of phones or phonemes occurring in\na language) and are mostly of one of four types:\n1. Single segment represented in an inventory;\n3This data resource, which provides standardized\nphono-typological information for 2,273 languages,\nis available at https://github.com/dmort27/\nuriel-phonology/tarball/0.1. It is a subset of the\nURIEL database, a comprehensive database of typological\nfeatures encoding syntactic and morphological (as well as\nphonological) properties of languages. It is available at\nhttp://cs.cmu.edu/~dmortens/uriel.html.\n1359\ne.g., does language ℓ’s sound inventory include\n/g/, a voiced velar stop?\n2. Class of segments represented in an inven-\ntory; e.g., does language ℓ’s sound inventory\ninclude voiced fricatives like /z/ and /v/?\n3. Minimal contrast represented in an inven-\ntory; e.g., does language ℓ’s sound inventory\ninclude two sounds that differ only in voicing,\nsuch as /t/ and /d/?\n4. Number of sounds representative of a class\nthat are present in an inventory ; e.g., does\nlanguage ℓ’s sound inventory include exactly\nﬁve vowels?\nThe motivation and criteria for coding each indi-\nvidual feature required extensive linguistic knowl-\nedge and analysis. Consider the case of tense vowels\nlike /i/ and /u/ in “beet” and “boot” in contrast with\nlax vowels like /I/ and /U/ in “bit” and “book.” Only\nthrough linguistic analysis does it become evident\nthat (1) all languages have tense vowels—a feature\nbased on the presence of tense vowels is uninforma-\ntive and that (2) a signiﬁcant minority of languages\nmake a distinction between tense and lax vowels—a\nfeature based on whether languages display a mini-\nmal difference of this kind would be more useful.\n4 Applications of Phonetic Vectors\nLearned continuous word representations—word\nvectors—are an important by-product of neural\nLMs, and these are used as features in numerous\nNLP applications, including chunking (Turian et al.,\n2010), part-of-speech tagging (Ling et al., 2015), de-\npendency parsing (Lazaridou et al., 2013; Bansal et\nal., 2014; Dyer et al., 2015; Watanabe and Sumita,\n2015), named entity recognition (Guo et al., 2014),\nand sentiment analysis (Socher et al., 2013; Wang\net al., 2015). We evaluate phone vectors learned\nby Polyglot LMs in two downstream applications\nthat rely on phonology: modeling lexical borrowing\n(§4.1) and speech synthesis (§4.2).\n4.1 Lexical borrowing\nLexical borrowing is the adoption of words from\nanother language, that inevitably happens when\nspeakers of different languages communicate for\na long period of time (Thomason and Kauf-\nman, 2001). Borrowed words—also called loan-\nwords—constitute 10–70% of most language lexi-\ncons (Haspelmath, 2009); these are content words\nof foreign origin that are adapted in the language\nand are not perceived as foreign by language speak-\ners. Computational modeling of cross-lingual trans-\nformations of loanwords is effective for inferring\nlexical correspondences across languages with lim-\nited parallel data, beneﬁting applications such as\nmachine translation (Tsvetkov and Dyer, 2015;\nTsvetkov and Dyer, 2016).\nIn the process of their nativization in a for-\neign language, loanwords undergo primar-\nily phonological adaptation , namely inser-\ntion/deletion/substitution of phones to adapt to the\nphonotactic constraints of the recipient language. If\na foreign phone is not present in the recipient lan-\nguage, it is usually replaced with its closest native\nequivalent—we thus hypothesize that cross-lingual\nphonological features learned by the Polyglot LM\ncan be useful in models of borrowing to quantify\ncross-lingual similarities of sounds.\nTo test this hypothesis, we augment the hand-\nengineered models proposed by Tsvetkov and Dyer\n(2016) with features from phone vectors learned\nby our model. Inputs to the borrowing framework\nare loanwords (in Swahili, Romanian, Maltese), and\noutputs are their corresponding “donor” words in the\ndonor language (Arabic, French, Italian, resp.). The\nframework is implemented as a cascade of ﬁnite-\nstate transducers with insertion/deletion/substitution\noperations on sounds, weighted by high-level con-\nceptual linguistic constraints that are learned in a\nsupervised manner. Given a loanword, the sys-\ntem produces a candidate donor word with lower\nranked violations than other candidates, using the\nshortest path algorithm. In the original borrow-\ning model, insertion/deletion/substitution operations\nare unweighted. In this work, we integrate tran-\nsition weights in the phone substitution transduc-\ners, which are cosine distances between phone vec-\ntors learned by our model. Our intuition is that\nsimilar sounds appear in similar contexts, even if\nthey are not present in the same language (e.g., / sQ/\nin Arabic is adapted to / s/ in Swahili). Thus, if\nour model effectively captures cross-lingual signals,\nsimilar sounds should have smaller distances in the\nvector space, which can improve the shortest path\nresults. Figure 2 illustrates our modiﬁcations to the\n1360\noriginal framework.\n0\n0.3\n0.8\nweights from phonetic features\n0\nFigure 2: Distances between phone vectors learned by the\nPolyglot LM are integrated as substitution weights in the lexical\nborrowing transducers. An English word cat [kæt] is adapted to\nits Russian counterpart кот[kot]. The transducer has also an er-\nroneous path to кит [kit] ‘whale’. In the original system, both\npaths are weighted with the same feature IDENT -IO-V, ﬁring\non vowel substitution. Our modiﬁcation allows the borrowing\nmodel to identify more plausible paths by weighting substitu-\ntion operations.\n4.2 Speech synthesis\nSpeech synthesis is the process of converting text\ninto speech. It has various applications, such as\nscreen readers for the visually impaired and hands-\nfree voice based systems. Text-to-speech (TTS) sys-\ntems are also used as part of speech-to-speech trans-\nlation systems and spoken dialog systems, such as\npersonal digital assistants. Natural and intelligible\nTTS systems exist for a number of languages in the\nworld today. However, building TTS systems re-\nmains prohibitive for many languages due to the lack\nof linguistic resources and data.\nThe language-speciﬁc resources that are tradition-\nally used for building TTS systems in a new lan-\nguage are: (1) audio recordings with transcripts; (2)\npronunciation lexicon or letter to sound rules; and\n(3) a phone set deﬁnition. Standard TTS systems to-\nday use phone sets designed by experts. Typically,\nthese phone sets also contain phonetic features for\neach phoneme, which are used as features in models\nof the spectrum and prosody. The phonetic features\navailable in standard TTS systems are multidimen-\nsional vectors indicating various properties of each\nphoneme, such as whether it is a vowel or consonant,\nvowel length and height, place of articulation of a\nconsonant, etc. Constructing these features by hand\ncan be labor intensive, and coming up with such fea-\ntures automatically may be useful in low-resource\nscenarios.\nIn this work, we replace manually engineered\nphonetic features with phone vectors, which are then\nused by classiﬁcation and regression trees for mod-\neling the spectrum. Each phoneme in our phone set\nis assigned an automatically constructed phone vec-\ntor, and each member of the phone vector is treated\nas a phoneme-level feature which is used in place of\nthe manually engineered phonetic features. While\nprior work has explored TTS augmented with acous-\ntic features (Watts et al., 2015), to the best of our\nknowledge, we are the ﬁrst to replace manually en-\ngineered phonetic features in TTS systems with au-\ntomatically constructed phone vectors.\n5 Experiments\nOur experimental evaluation of our proposed poly-\nglot models consists of two parts: (i) an intrinsic\nevaluation where phone sequences are modeled with\nindependent models and (ii) an extrinsic evaluation\nof the learned phonetic representations. Before dis-\ncussing these results, we provide details of the data\nresources we used.\n5.1 Resources and experimental setup\nResources. We experiment with the following lan-\nguages: Arabic ( AR), French ( FR), Hindi ( HI), Ital-\nian ( IT), Maltese ( MT), Romanian ( RO), Swahili\n(SW), Tamil (TA), and Telugu (TE). In our language\nmodeling experiments, two main sources of data are\npronunciation dictionaries and typological features\ndescribed in §3. The dictionaries for AR, FR, HI,\nTA, and TE are taken from in-house speech recog-\nnition/synthesis systems. For remaining languages,\nthe dictionaries are automatically constructed using\nthe Omniglot grapheme-to-IPA conversion rules.4\nWe use two types of pronunciation dictionaries:\n(1) AR, FR, HI, IT, MT, RO, and SW dictionaries used\nin experiments with lexical borrowing; and (2) EN,\nHI, TA, and TE dictionaries used in experiments with\nspeech synthesis. The former are mapped to IPA,\nwith the resulting phone vocabulary size—the num-\nber of distinct phones across IPA dictionaries—of\n127 phones. The latter are encoded using the Uni-\nTran universal transliteration resource (Qian et al.,\n2010), with a vocabulary of 79 phone types.\nFrom the (word-type) pronunciation dictionaries,\nwe remove 15% of the words for development, and\na further 10% for testing; the rest of the data is\n4http://omniglot.com/writing/\n1361\nAR FR HI IT MT RO SW\ntrain 1,868/18,485 238/1,851 193/1,536 988/901 114/1,152 387/4,661 659/7,239\ndev 366/3,627 47/363 38/302 19/176 22/226 76/916 130/1,422\ntest 208/2,057 27/207 22/173 11/100 13/128 43/524 73/806\nTable 1: Train/dev/test counts for IPA pronunciation dictionaries for words (phone sequences) and phone tokens, in thousands:\n#thousands of sequences/# thousands of tokens.\nEN HI TA TE\ntrain 101/867 191/1,523 74/780 71/690\ndev 20/169 37/300 14/152 14/135\ntest 11/97 21/171 8/87 8/77\nTable 2: Train/dev/test statistics for UniTran pronunciation dic-\ntionaries for words (phone sequences) and phone tokens, in\nthousands: #thousands of sequences/# thousands of tokens.\nused to train the models. In tables 1 and 2 we\nlist—for both types of pronunciation dictionaries—\ntrain/dev/test data statistics for words (phone se-\nquences) and phone tokens. We concatenate each\nphone sequence with beginning and end symbols\n(<s>, </s>).\nHyperparameters. We used the following net-\nwork architecture: 100-dimensional phone vectors,\nwith hidden local-context and LSTM layers of size\n100, and hidden language layer of size 20. All\nlanguage models were trained using the left con-\ntext of 3 phones (4-gram LMs). Across all lan-\nguage modeling experiments, parameter optimiza-\ntion was performed on the dev set using the Adam\nalgorithm (Kingma and Ba, 2014) with mini-batches\nof size 100 to train the models for 5 epochs.\n5.2 Intrinsic perplexity evaluation\nPerplexity is the standard evaluation measure for\nlanguage models, which has been shown to corre-\nlate strongly with error rates in downstream appli-\ncations (Klakow and Peters, 2002). We evaluated\nperplexities across several architectures, and several\nmonolingual and multilingual setups. We kept the\nsame hyper-parameters across all setups, as detailed\nin §5. Perplexities of LMs trained on the two types\nof pronunciation dictionaries were evaluated sepa-\nrately; table 3 summarizes perplexities of the models\ntrained on IPA dictionaries, and table 4 summarizes\nperplexities of the UniTran LMs.\nIn columns, we compare three model architec-\ntures: baseline denotes the standard RNNLM archi-\ntecture described in §2.1; +lang denotes the Poly-\nglot LM architecture described in §2.2 with input\nlanguage vector but without typological features and\nlanguage layer; ﬁnally, +typology denotes the full\nPolyglot LM architecture. This setup lets us sepa-\nrately evaluate the contribution of modiﬁed architec-\nture and the contribution of auxiliary set of features\nintroduced via the language layer.\nTest languages are IT in table 3, and HI in table 4.\nThe rows correspond to different sets of training lan-\nguages for the models: monolingual is for training\nand testing on the same language; +similar denotes\ntraining on three typologically similar languages:IT,\nFR, RO in table 3, andHI, TA, TE in table 4; +dissim-\nilar denotes training on four languages, three similar\nand one typologically dissimilar language, to eval-\nuate robustness of multilingual systems to diverse\ntypes of data. The ﬁnal sets of training languages\nare IT, FR, RO, HI in table 3, and HI, TA, TE, EN in\ntable 4.\nPerplexity (↓)\ntraining set baseline +lang +typology\nmonolingual 4.36 – –\n+similar 5.73 4.93 4.24 (↓26.0%)\n+dissimilar 5.88 4.98 4.41 (↓25.0%)\nTable 3: Perplexity experiments withIT as test language. Train-\ning languages: monolingual: IT; +similar: IT, FR, RO; +dissim-\nilar: IT, FR, RO, HI.\nPerplexity (↓)\ntraining set baseline +lang +typology\nmonolingual 3.70 – –\n+similar 4.14 3.78 3.35 (↓19.1%)\n+dissimilar 4.29 3.82 3.42 (↓20.3%)\nTable 4: Perplexity experiments with HI as test language.\nTraining languages: monolingual: HI; +similar: HI, TA, TE;\n+dissimilar: HI, TA, TE, EN.\nWe see several patterns of results. First, polyglot\nmodels require, unsurprisingly, information about\n1362\nwhat language they are predicting to obtain good\nmodeling performance. Second, typological in-\nformation is more valuable than letting the model\nlearn representations of the language along with the\ncharacters. Finally, typology-augmented polyglot\nmodels outperform their monolingual baseline, pro-\nviding evidence in support of the hypothesis that\ncross-lingual evidence is useful not only for learning\ncross-lingual representations and models, but mono-\nlingual ones as well.\n5.3 Lexical borrowing experiments\nWe fully reproduced lexical borrowing models de-\nscribed in (Tsvetkov and Dyer, 2016) for three lan-\nguage pairs: AR–SW, FR–RO, and IT–MT. Train and\ntest corpora are donor–loanword pairs in the lan-\nguage pairs. Corpora statistics are given in table 5\n(note that these are extremely small data sets; thus\nsmall numbers of highly informative features a nec-\nessary for good generalization). We use the repro-\nduced systems as the baselines, and compare these\nto the corresponding systems augmented with phone\nvectors, as described in §4.1.\nAR–SW FR –RO IT –MT\ntrain 417 282 425\ntest 73 50 75\nTable 5: Number of training and test pairs the the borrowing\ndatasets.\nIntegrated vectors were obtained from a single\npolyglot model with typology, trained on all lan-\nguages with IPA dictionaries. For comparison with\nthe results in table 3, perplexity of the model on\nthe IT dataset (used for evaluation is §5.2) is 4.16,\neven lower than in the model trained on four lan-\nguages. To retrain the high-level conceptual lin-\nguistic features learned by the borrowing models,\nwe initialized the augmented systems with feature\nweights learned by the baselines, and retrained. Fi-\nnal weights were established using cross-validation.\nThen, we evaluated the accuracy of the augmented\nborrowing systems on the held-out test data.\nAccuracies are shown in table 6. We observe im-\nprovements of up to 5% in accuracies of FR–RO\nand IT–MT pairs. Effectiveness of the same polyglot\nmodel trained on multiple languages and integrated\nin different downstream systems supports our as-\nsumption that the model remains stable and effective\nwith addition of languages. Our model is less effec-\ntive for the AR–SW language pair. We speculate that\nthe results are worse, because this is a pair of (ty-\npologically) more distant languages; consequently,\nthe phonological adaptation processes that happen in\nloanword assimilation are more complex than mere\nsubstitutions of similar phones that we are targeting\nvia the integration of phone vectors.\nAccuracy (↑)\nAR–SW FR –RO IT –MT\nbaseline 48.4 75.6 83.3\n+multilingual 46.9 80.6 87.1\nTable 6: Accuracies of the baseline models of lexical borrow-\ning and the models augmented with phone vectors. In all the\nexperiments, we use vectors from a single Polyglot LM model\ntrained on AR, SW, FR, RO, IT, MT.\n5.4 Speech synthesis experiments\nA popular objective metric for measuring the qual-\nity of synthetic speech is the Mel Cepstral Distortion\n(MCD) (Hu and Loizou, 2008). The MCD metric\ncalculates an L2 norm of the Mel Frequency Cep-\nstral Coefﬁcients (MFCCs) of natural speech from\na held out test set, and synthetic speech generated\nfrom the same test set. Since this is a distance met-\nric, a lower value of MCD suggests better synthesis.\nThe MCD is a database-speciﬁc metric, but experi-\nments by Kominek et al. (Kominek et al., 2008) have\nshown that a decrease in MCD of 0.08 is perceptu-\nally signiﬁcant, and a decrease of 0.12 is equivalent\nto doubling the size of the TTS database. In our ex-\nperiments, we use MCD to measure the relative im-\nprovement obtained by our techniques.\nWe conducted experiments on the IIIT-H Hindi\nvoice database (Prahallad et al., 2012), a 2 hour\nsingle speaker database recorded by a professional\nmale speaker. We used the same front end (UniTran)\nto build all the Hindi TTS systems, with the only dif-\nference between the systems being the presence or\nabsence of phonetic features and our vectors. For all\nour voice-based experiments, we built CLUSTER-\nGEN Statistical Parametric Synthesis voices (Black,\n2006) using the Festvox voice building tools (Black\nand Lenzo, 2003) and the Festival speech synthesis\nengine (Black and Taylor, 1997).\n1363\nThe baseline TTS system was built using no pho-\nnetic features. We also built a TTS system with stan-\ndard hand-crafted phonetic features. Table 7 shows\nthe MCD for the HI baseline, the standard TTS with\nhand-crafted features, and augmented TTS systems\nbuilt using monolingual and multilingual phone vec-\ntors constructed with Polyglot LMs.\nMCD (↓)\nbaseline 4.58\n+monolingual 4.40\n+multilingual 4.39\n+hand-crafted 4.41\nTable 7: MCD for the HI TTS systems. Polyglot LM training\nlanguages: monolingual: HI; +multilingual: HI, TA, TE, EN.\nOur multilingual vectors outperform the baseline,\nwith a signiﬁcant decrease of 0.19 in MCD. Cru-\ncially, TTS systems augmented with the Polyglot\nLM phone vectors outperform also the standard TTS\nwith hand-crafted features. We found that using\nboth feature sets added no value, suggesting that\nlearned phone vectors are capturing information that\nis equivalent to the hand-engineered vectors.\n5.5 Qualitative analysis of vectors\nPhone vectors learned by Polyglot LMs are mere se-\nquences of real numbers. An interesting question\nis whether these vectors capture linguistic (phono-\nlogical) qualities of phones they are encoding. To\nanalyze to what extent our vectors capture linguis-\ntic properties of phones, we use the QVEC —a tool\nto quantify and interpret linguistic content of vec-\ntor space models (Tsvetkov et al., 2015). The tool\naligns dimensions in a matrix of learned distributed\nrepresentations with dimensions of a hand-crafted\nlinguistic matrix. Alignments are induced via cor-\nrelating columns in the distributed and the linguistic\nmatrices. To analyze the content of the distributed\nmatrix, annotations from the linguistic matrix are\nprojected via the maximally-correlated alignments.\nWe constructed a phonological matrix in which\n5,059 rows are IPA phones and 21 columns are\nboolean indicators of universal phonological prop-\nerties, e.g. consonant, voiced, labial.5 We the pro-\njected annotations from the linguistic matrix and\n5This matrix is described in Littell et al. (2016) and is avail-\nable at https://github.com/dmort27/panphon/.\nmanually examined aligned dimensions in the phone\nvectors from §5.3 (trained on six languages). In the\nmaximally-correlated columns—corresponding to\nlinguistic features long, consonant, nasalized—we\nexamined phones with highest coefﬁcients. These\nwere: [ 5:, U:, i:, O:, E: ] for long; [ v, ñ, >dZ, d, f, j,>ts, N] for consonant; and [ ˜ O, ˜ E, ˜ A, ˜ œ] for nasalized.\nClearly, the learned representation discover standard\nphonological features. Moreover, these top-ranked\nsounds are not grouped by a single language, e.g.,\n/>dZ/ is present in Arabic but not in French, and /ñ, N/\nare present in French but not in Arabic. From this\nanalysis, we conclude that (1) the model discovers\nlinguistically meaningful phonetic features; (2) the\nmodel induces meaningful related groupings across\nlanguages.\n6 Related Work\nMultilingual language models. Interpolation of\nmonolingual LMs is an alternative to obtain a mul-\ntilingual model (Harbeck et al., 1997; Weng et\nal., 1997). However, interpolated models still re-\nquire a trained model per language, and do not\nallow parameter sharing at training time. Bilin-\ngual language models trained on concatenated cor-\npora were explored mainly in speech recognition\n(Ward et al., 1998; Wang et al., 2002; Fügen et\nal., 2003). Adaptations have been proposed to ap-\nply language models in bilingual settings in machine\ntranslation (Niehues et al., 2011) and code switching\n(Adel et al., 2013). These approaches, however, re-\nquire adaptation to every pair of languages, and an\nadapted model cannot be applied to more than two\nlanguages.\nIndependently, Ammar et al. (2016) used a dif-\nferent polyglot architecture for multilingual depen-\ndency parsing. This work has also conﬁrmed the\nutility of polyglot architectures in leveraging mul-\ntilinguality.\nMultimodal neural language models. Multi-\nmodal language modeling is integrating image/video\nmodalities in text LMs. Our work is inspired by the\nneural multimodal LMs (Kiros and Salakhutdinov,\n2013; Kiros et al., 2015), which deﬁned language\nmodels conditional on visual contexts, although we\nuse a different language model architecture (recur-\nrent vs. log-bilinear) and a different approach to gat-\n1364\ning modality.\n7 Conclusion\nWe presented a novel multilingual language model\narchitecture. The model obtains substantial gains\nin perplexity, and improves downstream text and\nspeech applications. Although we focus on phonol-\nogy, our approach is general, and can be applied\nin problems that integrate divergent modalities, e.g.,\ntopic modeling, and multilingual tagging and pars-\ning.\nAcknowledgments\nThis work was supported by the National Science\nFoundation through award IIS-1526745 and in part\nby the Defense Advanced Research Projects Agency\n(DARPA) Information Innovation Ofﬁce (I2O). Pro-\ngram: Low Resource Languages for Emergent In-\ncidents (LORELEI). Issued by DARPA/I2O under\nContract No. HR0011-15-C-0114.\nReferences\nHeike Adel, Ngoc Thang Vu, and Tanja Schultz. 2013.\nCombination of recurrent neural networks and factored\nlanguage models for code-switching language model-\ning. In Proc. ACL, pages 206–211.\nWaleed Ammar, George Mulcaire, Miguel Ballesteros,\nChris Dyer, and Noah A. Smith. 2016. Many lan-\nguages, one parser. CoRR, abs/1602.01595.\nMohit Bansal, Kevin Gimpel, and Karen Livescu. 2014.\nTailoring continuous word representations for depen-\ndency parsing. In Proc. ACL.\nYoshua Bengio, Patrice Simard, and Paolo Frasconi.\n1994. Learning long-term dependencies with gradi-\nent descent is difﬁcult. IEEE Transactions on Neural\nNetworks, 5(2):157–166.\nAlan W Black and Kevin A Lenzo. 2003. Building syn-\nthetic voices. http://festvox.org/bsv/.\nAlan W Black and Paul Taylor. 1997. The Festi-\nval speech synthesis system: system documentation.\nTechnical report, Human Communication Research\nCentre, University of Edinburgh.\nAlan W Black. 2006. CLUSTERGEN: a statistical para-\nmetric synthesizer using trajectory modeling. In Proc.\nInterspeech.\nMatthew S. Dryer and Martin Haspelmath, editors. 2013.\nWALS Online. Max Planck Institute for Evolutionary\nAnthropology. http://wals.info/.\nChris Dyer, Miguel Ballesteros, Wang Ling, Austin\nMatthews, and Noah A Smith. 2015. Transition-based\ndependency parsing with stack long short-term mem-\nory. In Proc. ACL.\nManaal Faruqui and Chris Dyer. 2014. Improving vector\nspace word representations using multilingual correla-\ntion. In Proc. EACL.\nChristian Fügen, Sebastian Stuker, Hagen Soltau, Florian\nMetze, and Tanja Schultz. 2003. Efﬁcient handling of\nmultilingual language models. In Proc. ASRU, pages\n441–446.\nAlex Graves. 2013. Generating sequences with recurrent\nneural networks. CoRR, abs/1308.0850.\nJiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu.\n2014. Revisiting embedding features for simple semi-\nsupervised learning. In Proc. EMNLP.\nStefan Harbeck, Elmar Nöth, and Heinrich Niemann.\n1997. Multilingual speech recognition. In Proc. 2nd\nSQEL Workshop on Multi-Lingual Information Re-\ntrieval Dialogs, pages 9–15.\nMartin Haspelmath. 2009. Lexical borrowing: concepts\nand issues. Loanwords in the World’s Languages: a\ncomparative handbook, pages 35–54.\nKarl Moritz Hermann and Phil Blunsom. 2014. Multilin-\ngual Models for Compositional Distributional Seman-\ntics. In Proc. ACL.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9(8):1735–\n1780.\nYi Hu and Philipos C Loizou. 2008. Evaluation of objec-\ntive quality measures for speech enhancement. Audio,\nSpeech, & Language Processing, 16(1):229–238.\nKejun Huang, Matt Gardner, Evangelos Papalexakis,\nChristos Faloutsos, Nikos Sidiropoulos, Tom Mitchell,\nPartha P. Talukdar, and Xiao Fu. 2015. Translation\ninvariant word embeddings. In Proc. EMNLP, pages\n1084–1088.\nDiederik Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nRyan Kiros and Ruslan Salakhutdinov. 2013. Multi-\nmodal neural language models. In Proc. NIPS Deep\nLearning Workshop.\nRyan Kiros, Ruslan Salakhutdinov, and Richard Zemel.\n2015. Unifying visual-semantic embeddings with\nmultimodal neural language models. TACL.\nDietrich Klakow and Jochen Peters. 2002. Testing the\ncorrelation of word error rate and perplexity. Speech\nCommunication, 38(1):19–28.\nJohn Kominek, Tanja Schultz, and Alan W Black. 2008.\nSynthesizer voice quality of new languages calibrated\nwith mean Mel Cepstral Distortion. In Proc. SLTU,\npages 63–68.\n1365\nAngeliki Lazaridou, Eva Maria Vecchi, and Marco Ba-\nroni. 2013. Fish transporters and miracle homes:\nHow compositional distributional semantics can help\nNP parsing. In Proc. EMNLP.\nM Paul Lewis, Gary F Simons, and Charles D Fennig.\n2015. Ethnologue: Languages of the world. Texas:\nSIL International. http://www.ethnologue.\ncom.\nWang Ling, Tiago Luís, Luís Marujo, Ramón Fernan-\ndez Astudillo, Silvio Amir, Chris Dyer, Alan W Black,\nand Isabel Trancoso. 2015. Finding function in form:\nCompositional character models for open vocabulary\nword representation. In Proc. NAACL.\nPatrick Littell, David Mortensen, Kartik Goyal, Chris\nDyer, and Lori Levin. 2016. Bridge-language capi-\ntalization inference in Western Iranian: Sorani, Kur-\nmanji, Zazaki, and Tajik. In Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation (LREC’16).\nAng Lu, Weiran Wang, Mohit Bansal, Kevin Gimpel, and\nKaren Livescu. 2015. Deep multilingual correlation\nfor improved word embeddings. In Proc. NAACL.\nTomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan Cer-\nnock`y, and Sanjeev Khudanpur. 2010. Recurrent neu-\nral network based language model. In Proc. Inter-\nspeech, pages 1045–1048.\nSteven Moran, Daniel McCloy, and Richard Wright, ed-\nitors. 2014. PHOIBLE Online. Max Planck In-\nstitute for Evolutionary Anthropology. http://\nphoible.org/.\nJan Niehues, Teresa Herrmann, Stephan V ogel, and Alex\nWaibel. 2011. Wider context by using bilingual lan-\nguage models in machine translation. In Proc. WMT,\npages 198–206.\nKishore Prahallad, E. Naresh Kumar, Venkatesh Keri,\nS. Rajendran, and Alan W Black. 2012. The IIIT-H\nIndic speech databases. In Proc. Interspeech.\nTing Qian, Kristy Hollingshead, Su-youn Yoon, Kyoung-\nyoung Kim, Richard Sproat, and Malta LREC. 2010.\nA Python toolkit for universal transliteration. In Proc.\nLREC.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang,\nChristopher D. Manning, Andrew Y . Ng, and Christo-\npher Potts. 2013. Recursive deep models for semantic\ncompositionality over a sentiment treebank. In Proc.\nEMNLP.\nMartin Sundermeyer, Ralf Schlüter, and Hermann Ney.\n2012. LSTM neural networks for language modeling.\nIn Proc. Interspeech.\nSarah Grey Thomason and Terrence Kaufman. 2001.\nLanguage contact. Edinburgh University Press Edin-\nburgh.\nYulia Tsvetkov and Chris Dyer. 2015. Lexicon stratiﬁca-\ntion for translating out-of-vocabulary words. In Proc.\nACL, pages 125–131.\nYulia Tsvetkov and Chris Dyer. 2016. Cross-lingual\nbridges with models of lexical borrowing. JAIR,\n55:63–93.\nYulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-\nlaume Lample, and Chris Dyer. 2015. Evalua-\ntion of word vector representations by subspace align-\nment. In Proc. EMNLP. https://github.com/\nytsvetko/qvec.\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: a simple and general method for\nsemi-supervised learning. In Proc. ACL.\nZhirong Wang, Umut Topkara, Tanja Schultz, and Alex\nWaibel. 2002. Towards universal speech recognition.\nIn Proc. ICMI, page 247.\nXin Wang, Yuanchao Liu, Chengjie Sun, Baoxun Wang,\nand Xiaolong Wang. 2015. Predicting polarities\nof tweets by composing word embeddings with long\nshort-term memory. In Proc. ACL, pages 1343–1353.\nTodd Ward, Salim Roukos, Chalapathy Neti, Jerome\nGros, Mark Epstein, and Satya Dharanipragada. 1998.\nTowards speech understanding across multiple lan-\nguages. In Proc. ICSLP.\nTaro Watanabe and Eiichiro Sumita. 2015. Transition-\nbased neural constituent parsing. In Proc. ACL.\nOliver Watts, Zhizheng Wu, and Simon King. 2015.\nSentence-level control vectors for deep neural network\nspeech synthesis. In Proc. Interspeech.\nFuliang Weng, Harry Bratt, Leonardo Neumeyer, and An-\ndreas Stolcke. 1997. A study of multilingual speech\nrecognition. In Proc. EUROSPEECH, pages 359–362.\n1366"
}