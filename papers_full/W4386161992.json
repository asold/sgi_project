{
    "title": "Integration of pre-trained protein language models into geometric deep learning networks",
    "url": "https://openalex.org/W4386161992",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2001693504",
            "name": "Fang Wu",
            "affiliations": [
                "Westlake University"
            ]
        },
        {
            "id": "https://openalex.org/A2225937964",
            "name": "Lirong Wu",
            "affiliations": [
                "Westlake University"
            ]
        },
        {
            "id": "https://openalex.org/A2913816252",
            "name": "Dragomir Radev",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2097366902",
            "name": "Jinbo Xu",
            "affiliations": [
                "Toyota Technological Institute at Chicago",
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2436786329",
            "name": "Stan Z. Li",
            "affiliations": [
                "Westlake University"
            ]
        },
        {
            "id": "https://openalex.org/A2001693504",
            "name": "Fang Wu",
            "affiliations": [
                "Westlake University"
            ]
        },
        {
            "id": "https://openalex.org/A2225937964",
            "name": "Lirong Wu",
            "affiliations": [
                "Westlake University"
            ]
        },
        {
            "id": "https://openalex.org/A2913816252",
            "name": "Dragomir Radev",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2097366902",
            "name": "Jinbo Xu",
            "affiliations": [
                "Tsinghua University",
                "Toyota Technological Institute at Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A2436786329",
            "name": "Stan Z. Li",
            "affiliations": [
                "Westlake University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4221149941",
        "https://openalex.org/W6786423766",
        "https://openalex.org/W2594183968",
        "https://openalex.org/W2969996838",
        "https://openalex.org/W3042283064",
        "https://openalex.org/W4310895557",
        "https://openalex.org/W6814982423",
        "https://openalex.org/W3088578860",
        "https://openalex.org/W4382239609",
        "https://openalex.org/W4206367183",
        "https://openalex.org/W3179485843",
        "https://openalex.org/W4327550249",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W3177500196",
        "https://openalex.org/W4223581484",
        "https://openalex.org/W4317569480",
        "https://openalex.org/W4315928370",
        "https://openalex.org/W2110505462",
        "https://openalex.org/W2902353954",
        "https://openalex.org/W2170463736",
        "https://openalex.org/W2130479394",
        "https://openalex.org/W3095583226",
        "https://openalex.org/W2161062388",
        "https://openalex.org/W2059117301",
        "https://openalex.org/W2171641243",
        "https://openalex.org/W2953524386",
        "https://openalex.org/W3199799076",
        "https://openalex.org/W1030883578",
        "https://openalex.org/W2092285329",
        "https://openalex.org/W2114162221",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3212013219",
        "https://openalex.org/W2117451312",
        "https://openalex.org/W2534288757",
        "https://openalex.org/W2594725344",
        "https://openalex.org/W2905812122",
        "https://openalex.org/W2951690294",
        "https://openalex.org/W569478347",
        "https://openalex.org/W3110563343",
        "https://openalex.org/W4290877452",
        "https://openalex.org/W2809216727",
        "https://openalex.org/W2951433247",
        "https://openalex.org/W2992752586",
        "https://openalex.org/W3096561213",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W3186179742",
        "https://openalex.org/W6802033083",
        "https://openalex.org/W4210997669",
        "https://openalex.org/W4285483966",
        "https://openalex.org/W2726670313",
        "https://openalex.org/W6682839888",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W1512387364",
        "https://openalex.org/W4221159485",
        "https://openalex.org/W2739999456",
        "https://openalex.org/W2301595689",
        "https://openalex.org/W2114850508",
        "https://openalex.org/W6912896983",
        "https://openalex.org/W3103859462",
        "https://openalex.org/W4386161992",
        "https://openalex.org/W4382632527",
        "https://openalex.org/W2971227267",
        "https://openalex.org/W4366998330"
    ],
    "abstract": null,
    "full_text": "ARTICLE\nIntegration of pre-trained protein language models\ninto geometric deep learning networks\nFang Wu 1, Lirong Wu1, Dragomir Radev2, Jinbo Xu 3,4 & Stan Z. Li1✉\nGeometric deep learning has recently achieved great success in non-Euclidean domains, and\nlearning on 3D structures of large biomolecules is emerging as a distinct research area.\nHowever, its efﬁcacy is largely constrained due to the limited quantity of structural data.\nMeanwhile, protein language models trained on substantial 1D sequences have shown bur-\ngeoning capabilities with scale in a broad range of applications. Several preceding studies\nconsider combining these different protein modalities to promote the representation power of\ngeometric neural networks but fail to present a comprehensive understanding of their ben-\neﬁts. In this work, we integrate the knowledge learned by well-trained protein language\nmodels into several state-of-the-art geometric networks and evaluate a variety of protein\nrepresentation learning benchmarks, including protein-protein interface prediction, model\nquality assessment, protein-protein rigid-body docking, and binding afﬁnity prediction. Our\nﬁndings show an overall improvement of 20% over baselines. Strong evidence indicates that\nthe incorporation of protein language models’ knowledge enhances geometric networks’\ncapacity by a signiﬁcant margin and can be generalized to complex tasks.\nhttps://doi.org/10.1038/s42003-023-05133-1 OPEN\n1 AI Research and Innovation Laboratory, Westlake University, 310030 Hangzhou, China.2 Department of Computer Science, Yale University, New Haven, CT\n06511, USA. 3 Institute of AI Industry Research, Tsinghua University, Haidian Street, 100084 Beijing, China.4 Toyota Technological Institute at Chicago,\nChicago, IL 60637, USA. ✉email: stan.zq.li@westlake.edu.cn\nCOMMUNICATIONS BIOLOGY|           (2023) 6:876 | https://doi.org/10.1038/s42003-023-05133-1 | www.nature.com/commsbio 1\n1234567890():,;\nM\nacromolecules (e.g., proteins, RNAs, or DNAs) are\nessential to biophysical processes. While they can be\nrepresented using lower-dimensional representations\nsuch as linear sequences (1D) or chemical bond graphs (2D), a\nmore intrinsic and informative form is the three-dimensional\ngeometry\n1. 3D shapes are critical to not only understanding the\nphysical mechanisms of action but also answering a number of\nquestions associated with drug discovery and molecular design2.\nConsequently, tremendous efforts in structural biology have been\ndevoted to deriving insights from their conformations3– 5.\nWith the rapid advances of deep learning (DL) techniques, it\nhas been an attractive challenge to represent and reason about\nmacromolecules’ structures in the 3D space. In particular, dif-\nferent sorts of 3D information, including bond lengths and\ndihedral angles, play an essential role. In order to encode them, a\nnumber of 3D geometric graph neural networks (GGNNs) or\nCNNs6– 9 have been proposed, and simultaneously achieve several\ncrucial properties of Euclidean geometry such as E(3) or SE(3)\nequivariance and symmetry. Notably, they are essential con-\nstituents of geometric deep learning (GDL), an umbrella term\nthat generalizes networks to Euclidean or non-Euclidean\ndomains10.\nMeanwhile, the anticipated growth of sequencing promises\nunprecedented data on natural sequence diversity. The abun-\ndance of 1D amino acid sequences has spurred increasing interest\nin developing protein language models at the scale of evolution,\nsuch as the series of ESM11– 13 and ProtTrans14. These protein\nlanguage models can capture information about secondary and\ntertiary structures and can be generalized across a broad range of\ndownstream applications. To be explicit, they have recently been\ndemonstrated with strong capabilities in uncovering protein\nstructures12, predicting the effect of sequence variation on\nfunction11, learning inverse folding15 and many other general\npurposes13.\nWith the fruitful progress in protein language models, more and\nmore studies have considered enhancing GGNNs ’ ability by\nleveraging the knowledge of those protein language models12,16,17.\nThis is nontrivial because compared to sequence learning, 3D\nstructures are much harder to obtain and thus less prevalent.\nConsequently, learning about the structure of proteins leads to a\nreduced amount of training data. For example, the SAbDab\ndatabase18 merely has 3K antibody-antigen structures without\nduplicate. The SCOPe database19 has 226K annotated structures,\nand the SIFTS database 20 comprises around 220K annotated\nenzyme structures. These numbers are orders of magnitude lower\nthan the data set sizes that can inspire major breakthroughs in the\ndeep learning community. In contrast, while the Protein Data Bank\n(PDB)21 possesses approximately 182K macromolecule structures,\ndatabases like Pfam22 and UniParc23 contains more than 47M and\n250M protein sequences respectively.\nIn addition to the data size, the beneﬁt of protein sequence to\nstructure learning also has solid evidence and theoretical support.\nRemarkably, the idea that biological function and structures are\ndocumented in the statistics of protein sequences selected\nthrough evolution has a long history24. The unobserved variables\nthat decide a protein’s ﬁtness, including structure, function, and\nstability, leave a record in the distribution of observed natural\nsequences25. Those protein language models use self-supervision\nto unlock the information encoded in protein sequence varia-\ntions, which is also beneﬁcial for GGNNs. Accordingly, in this\npaper, we comprehensively investigate the promotion of GGNNs’\ncapability with the knowledge learned by protein language models\n(see Fig. 1). The improvements come from two major lines.\nFirstly, GGNNs can beneﬁt from the information that emerges in\nthe learned representations of those protein language models\non fundamental properties of proteins, including secondary\nstructures, contacts, and biological activity. This kind of knowl-\nedge may be difﬁcult for GGNNs to be aware of and learn in a\nspeciﬁc downstream task. To conﬁrm this claim, we conduct a toy\nexperiment to demonstrate that conventional graph connectivity\nmechanisms prevent existing GGNNs from being cognizant of\nresidues’ absolute and relative positions in the protein sequence.\nSecondly and more intuitively, protein language models serve as\nan alternative way of enriching GGNNs’ training data and allow\nGGNNs to be exposed to more different families of proteins,\nthereby greatly strengthening GGNNs’ generalization capability.\nWe examine our hypothesis across a wide range of bench-\nmarks, containing model quality assessment, protein-protein\ninterface prediction, protein-protein rigid-body docking, and\nligand binding afﬁnity prediction. Extensive experiments show\nthat the incorporation and combination of pretrained protein\nlanguage models’ knowledge signiﬁcantly improve GGNNs’ per-\nformance for various problems, which require distinct domain\nknowledge. By utilizing the unprecedented view into the language\nof protein sequences provided by powerful protein language\nmodels, GGNNs promise to augment our understanding of a vast\ndatabase of poorly understood protein structures. Our work\nhopes to shed more light on how to bridge the gap between the\nthriving geometric deep learning and mature protein language\nmodels and better leverage different modalities of proteins.\nResults and discussion\nOur toy experiments illustrate that existing GGNNs are unaware\nof the positional order inside the protein sequences. Taking a step\nfurther, we show in this section that incorporating knowledge\nSAbDab structures\n(~3,000)\nSCOPe structures\n(~226,000)\nUniParc sequences \n(250 million)\nUniRef50 sequences \n(12 million)\nE  G  R  L  T  V  Y  C  …Protein Sequence \nProtein Language Models\n(e.g., ESM-2, MSA-Transformer) \n3D Protein Graph\nSequential Models \nY\nC\nE\nV\nT\nL\nRG\nGeometric NNs\ne.g., GVP-GNN, EGNN, \nSE(3)-Transformer, \nSchnet, DimeNet\nFig. 1 Illustration of our framework to strengthen GGNNs with knowledge\nof protein language models.The protein sequence isﬁrst forwarded into a\npretrained protein language model to extract per-residue representations,\nwhich are then used as node features in 3D protein graphs for GGNNs.\nARTICLE COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-023-05133-1\n2 COMMUNICATIONS BIOLOGY|           (2023) 6:876 | https://doi.org/10.1038/s42003-023-05133-1 | www.nature.com/commsbio\nlearned by large-scale protein language models can robustly\nenhance GGNN’s capacity in a wide variety of downstream tasks.\nTasks and datasets.\n● Model Quality Assessment(MQA) aims to select the best\nstructural model of a protein from a large pool of candidate\nstructures and is an essential step in structure prediction26.\nFor a number of recently solved but unreleased structures,\nstructure generation programs produce a large number of\ncandidate structures. MQA approaches are evaluated by\ntheir capability of predicting the global distance test (GDT-\nTS score) of a candidate structure compared to the\nexperimentally solved structure of that target. Its database\nis composed of all structural models submitted to the\nCritical Assessment of Structure Prediction (CASP)27 over\nthe last 18 years. The data is split temporally by\ncompetition year. MQA is similar to the Protein Structure\nRanking (PSR) task introduced by Townshend et al.\n2.\n● Protein-protein Rigid-body Docking (PPRD) computa-\ntionally predicts the 3D structure of a protein-protein\ncomplex from the individual unbound structures. It\nassumes that no conformation change within the proteins\nhappens during binding. We leverage Docking Benchmark\n5.5 (DB5.5)28 as the database. It is a gold standard dataset\nin terms of data quality and contains 253 structures.\n● Protein-protein Interface (PPI) investigates whether two\namino acids will contact when their respective proteins\nbind. It is an important problem in understanding how\nproteins interact with each other, e.g., antibody proteins\nrecognize diseases by binding to antigens. We use the\nDatabase of Interacting Protein Structures (DIPS), a\ncomprehensive dataset of protein complexes mined from\nthe PDB 29, and randomly select 15K samples for\nevaluation.\n● Ligand Binding Afﬁnity (LBA) is an essential task for drug\ndiscovery applications. It predicts the strength of a\ncandidate drug molecule’s interaction with a target protein.\nSpeciﬁcally, we aim to forecastpK ¼/C0 log10K, where K is\nthe binding afﬁnity in Molar units. We use the PDBbind\ndatabase30,31, a curated database containing protein-ligand\ncomplexes from the PDB and their corresponding binding\nstrengths. The protein-ligand complexes are split such that\nno protein in the test dataset has more than 30% or 60%\nsequence identity with any protein in the training dataset.\nExperimental setup. We evaluate our proposed framework on\nthe instances of several state-of-the-art geometric networks, using\nPytorch32 and PyG33 on four standard protein benchmarks. For\nMQA, PPI, and LBA, we use GVP-GNN, EGNN, and Molformer\nas backbones. For PPRD, we utilize a deep learning model,\nEquiDock34, as the backbone. It approximates the binding\npockets and obtains the docking poses using keypoint matching\nand alignment. For more experimental details, please refer to\nSupplementary Note 3.\nSingle-protein representation task. For MQA, we document First\nRank Loss, Spearman correlation (R\nS), Pearson’s correlation (RP),\nand Kendall rank correlation (KR) in Table1. The introduction of\nprotein language models has brought a signi ﬁcant average\nincrease of 32.63% and 55.71% in global and meanRS, of 34.66%\nand 58.75% in global and meanRP, and of 43.21% and 63.20% in\nglobal and mean KR respectively. With the aid of language\nmodels, GVP-GNN achieves the optimal globalRS, globalRP, and\nKR of 84.92%, 85.44%, and 67.98% separately.\nApart from that, we provide a full comparison with all existing\napproaches in Table 2. We elect RWplus 35, ProQ3D 36,\nVoroMQA37, SBROD 38, 3DCNN 2, 3DGNN 2, 3DOCNN 39,\nDimeNet40, GraphQA 41, and GBPNet 42 as the baselines.\nPerformance is recorded in Table 2, where the second best is\nunderlined. It can be concluded that even if GVP-GNN is not the\nbest architecture, it can largely outperform existing methods\nincluding the state-of-the-art no-pretraining method set by\nAyken and Xia 42 (i.e., GBPNet) and the state-of-the-art\npretraining results set by Jing et al.43 if it is enhanced by the\nprotein language model.\nProtein-protein representation tasks. For PPRD, we report three\nitems as measurements: the complex root mean squared deviation\n(RMSD), the ligand RMSD, and the interface RMSD in Table3.\nThe interface is determined with a distance threshold less than\n8Å. It is noteworthy that, unlike the EquiDock paper, we do not\napply the Kabsch algorithm to superimpose the receptor and the\nligand. Contrastingly, the receptor protein isﬁxed during eva-\nluation. All three metrics decrease considerably with improve-\nments of 11.61%, 12.83%, and 31.01% in complex, ligand, and\ninterface median RMSD, respectively. Notably, we also report the\nresult of EquiDock, which isﬁrst pretrained on DIPS and then\nﬁne-tuned on DB5. It can be discovered that DIPS-pretrained\nEquiDock still performs worse than EquiDock equipped with\npretrained language models. This strongly demonstrates that\nstructural pretraining for GGNNs may not beneﬁt GGNNs more\nthan pretrained protein language models.\nFor PPI, we record AUROC as the metric in Fig.2. It can be\nfound that AUROC increases for 6.93%, 14.01%, and 22.62% for\nGVP-GNN, EGNN, and Molformer respectively. It is worth\nnoting that Molformer falls behind EGNN and GVP-GNN\noriginally in this task. But after injecting knowledge learned by\nprotein language models, Molformer achieves competitive or even\nTable 1 Results on MQA.\nModel PLM Model quality assessment\nFirst rank loss↓ Spearman correlation↑ Pearson’s correlation↑ Kendall rank↑\nMean Global Mean Global Mean Global\nGVP-GNN ✗ 0.085 ± 0.002 0.4144 ± 0.010 0.6910 ± 0.008 0.5235 ± 0.013 0.6875 ± 0.006 0.2960 ± 0.010 0.4959 ± 0.004\n✓ 0.033 ± 0.001 0.6121 ± 0.017 0.8492 ± 0.015 0.7399 ± 0.017 0.8544 ± 0.009 0.4530 ± 0.008 0.6798 ± 0.014\nEGNN ✗ 0.054 ± 0.003 0.4249 ± 0.016 0.7341 ± 0.015 0.5315 ± 0.008 0.7336 ± 0.018 0.3004 ± 0.013 0.5344 ± 0.011\n✓ 0.041 ± 0.001 0.5642 ± 0.013 0.8436 ± 0.012 0.6925 ± 0.006 0.8456 ± 0.015 0.4105 ± 0.014 0.6558 ± 0.006\nMolformer ✗ 0.149 ± 0.003 0.1238 ± 0.011 0.3921 ± 0.004 0.1969 ± 0.004 0.3901 ± 0.012 0.0841 ± 0.010 0.2696 ± 0.005\n✓ 0.088 ± 0.002 0.2424 ± 0.015 0.6516 ± 0.009 0.3850 ± 0.011 0.6210 ± 0.014 0.1681 ± 0.012 0.4579 ± 0.007\nThe column of’PLM’ indicates whether the protein language model is used. The First Rank Loss is the average difference between the true scores of the best model and the top-ranked model for each\ntarget. Results are reported with mean ± standard deviation over three repeated runs and the best performance is in bold.\nCOMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-023-05133-1 ARTICLE\nCOMMUNICATIONS BIOLOGY|           (2023) 6:876 | https://doi.org/10.1038/s42003-023-05133-1 | www.nature.com/commsbio 3\nbetter performance than EGNN or GVP-GNN. This indicates\nthat protein language models can realize the potential of GGNNs\nto the full extent and greatly narrow the gap between different\ngeometric deep learning architectures. The results mentioned\nabove are amazing because, unlike MQA, PPRD and PPI study\nthe geometric interactions between two proteins. Though existing\nprotein language models are all trained on single protein\nsequences, our experiments show that the evolution information\nhidden in unpaired sequences can also be valuable to analyze the\nmulti-protein environment.\nTable 3 Performance of PPRD on DB5.5 Test Set.\nModel PLM Protein-protein rigid-body docking\nComplex RMSD↓ Ligand RMSD↓ Interface RMSD↓\nMedian Mean Std Median Mean Std Median Mean Std\nEquiDock ✗♣ 16.88 17.11 5.33 40.35 37.97 12.94 16.19 37.97 4.47\n✗♠ 15.02 14.31 5.28 36.82 35.95 13.18 14.37 35.68 4.12\n✓♣ 14.92 13.14 4.59 35.17 33.48 14.34 11.17 33.48 4.38\nModels with♣ are directly trained and tested on DB5, while EquiDock with♠ is ﬁrst pretrained on DIPS andﬁne-tuned on the DB5 training set. Results are reported with mean ± standard deviation over\nthree repeated runs and the best performance is in bold.\nTable 2 Comparison of performance on MQA.\nModel PLM Model quality assessment\nSpearman correlation↑ Pearson’s correlation↑ Kendall rank↑\nMean Global Mean Global Mean Global\nRWplus35a ✗ 0.167 0.056 0.192 0.033 0.137 0.011\nProQ3D36a ✗ 0.432 0.772 0.444 0.796 0.304 594\nVoroMQA37a ✗ 0.419 0.651 0.412 0.651 0.291 0.505\nSBROD38a ✗ 0.413 0.569 0.431 0.551 0.291 0.393\n3DOCNN39b ✗ 0.432 0.796 0.444 0.772 0.304 0.594\nDimeNet40a ✗ 0.351 0.625 0.302 0.614 0.285 0.431\n3DCNN2b ✗ 0.431 ± 0.013 0.789 ± 0.017 0.557 ± 0.011 0.780 ± 0.016 0.308 ± 0.010 0.592 ± 0.016\n3DGNN2b ✗ 0.411 ± 0.006 0.750 ± 0.018 0.500 ± 0.012 0.747 ± 0.018 0.278 ± 0.005 0.547 ± 0.016\nGVP-GNN7c ✗ 0.414 ± 0.010 0.691 ± 0.008 0.523 ± 0.013 0.687 ± 0.006 0.296 ± 0.010 0.495 ± 0.004\nGraphQA41a ✗ 0.379 0.820 0.357 0.821 0.331 0.618\nGBPNet42a ✗ 0.517 0.856 0.612 0.853 0.372 0.656\nGVP-GNN ✓ 0.612 ± 0.017 0.849 ± 0.015 0.739 ± 0.017 0.854 ± 0.009 0.453 ± 0.008 0.679 ± 0.014\nModels are sorted by the year they are released. Results are reported with mean ± standard deviation over three repeated runs and the best and second best performance are bolded and underlined,\nrespectively.\naThese results are taken from ref.42.\nbThese results are taken from ref.2.\ncThese results are re-produced.\nFig. 2 Some ablation studies. aResults of PPI with and without PLMs.b Performance of GGNNs on MQA with ESM-2 at different scales.\nARTICLE COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-023-05133-1\n4 COMMUNICATIONS BIOLOGY|           (2023) 6:876 | https://doi.org/10.1038/s42003-023-05133-1 | www.nature.com/commsbio\nProtein-molecules representation task . For LBA, we compare\nRMSD, RS, RP, and KR in Table 4. The incorporation of protein\nlanguage models produces a remarkably average decline of\n11.26% and 6.15% in RMSD for 30% and 60% identity, an average\nincrease of 51.09% and 9.52% inRP for the 30% and 60% identity,\nan average increment of 66.60% and 8.90% inRS for the 30% and\n60% identity, and an average increment of 68.52% and 6.70% in\nKR for the 30% and 60% identity. It can be seen that the\nimprovements in the 30% sequence identity is higher than that in\nthe less restrictive 60% sequence identity. This conﬁrms that\nprotein language models beneﬁt GGNNs more when the unseen\nsamples belong to different protein domains. Moreover, con-\ntrasting PPRD or PPI, LBA studies how proteins interact with\nsmall molecules. Our outcome demonstrates that rich protein\nrepresentations encoded by protein language models can also\ncontribute to the analysis of protein’s reaction to other non-\nprotein drug-like molecules. The result of a different data split has\nbeen placed in Supplementary Table 1.\nIn addition, we compare thoroughly with existing approaches\nfor LBA in Table 5, where the second best is underlined. We\nselect a broad range of models including DeepAf ﬁnity44,\nCormorant45, LSTM46, TAPE47, ProtTrans14, 3DCNN2, GNN2,\nMaSIF48, DGAT49, DGIN49, DGAT-GCN49, HoloProt50, and\nGBPNet42 as the baseline. It is clear that even if EGNN is a\nmedian-level architecture, it can achieve the best RMSD and the\nbest Pearson’s correlation when enhanced by protein language\nmodels, beating a group of strong baselines including HoloProt50\nand GBPNet42.\nScale and type of protein language models. It has been observed\nthat as the size of the language model increases, there are con-\nsistent improvements in tasks like structure prediction12. Here we\nconduct an ablation study to investigate the effect of protein\nlanguage models’ sizes on GGNNs. Speciﬁcally, we explore dif-\nferent ESM-2 with the parameter numbers of 8M, 35M, 150M,\n650M, and 3B and plot results in Fig.2. It veriﬁes that scaling the\nTable 4 Results on LBA.\nModel PLM Ligand binding af ﬁnity\nSequence identity (30%)\nRMSD↓ Pearson’s correlation↑ Spearman correlation↑ Kendall rank↑\nGVP-GNN ✗ 1.6480 ± 0.014 0.2138 ± 0.013 0.1648 ± 0.009 0.1107 ± 0.012\n✓ 1.4556 ± 0.011 0.5373 ± 0.010 0.5078 ± 0.005 0.3495 ± 0.009\nEGNN ✗ 1.4929 ± 0.012 0.4891 ± 0.017 0.4725 ± 0.008 0.3291 ± 0.014\n✓ 1.4033 ± 0.013 0.5655 ± 0.016 0.5448 ± 0.005 0.3790 ± 0.007\nMolformer ✗ 1.91f07 ± 0.018 0.4618 ± 0.014 0.4104 ± 0.011 0.2812 ± 0.019\n✓ 1.6028 ± 0.020 0.5351 ± 0.017 0.5372 ± 0.015 0.3758 ± 0.016\nSequence identity (60%)\nGVP-GNN ✗ 1.5438 ± 0.015 0.6608 ± 0.012 0.6668 ± 0.0010 0.4797 ± 0.014\n✓ 1.5137 ± 0.019 0.6680 ± 0.010 0.6716 ± 0.008 0.4786 ± 0.012\nEGNN ✗ 1.5928 ± 0.020 0.6274 ± 0.013 0.6271 ± 0.017 0.4498 ± 0.014\n✓ 1.5595 ± 0.022 0.6445 ± 0.015 0.6463 ± 0.019 0.4656 ± 0.019\nMolformer ✗ 1.8610 ± 0.018 0.5528 ± 0.016 0.5309 ± 0.015 0.3738 ± 0.017\n✓ 1.5926 ± 0.024 0.6524 ± 0.018 0.6528 ± 0.016 0.4367 ± 0.011\nResults are reported with mean ± standard deviation over three repeated runs and the best performance is in bold.\nTable 5 Comparison of performance on LBA.\nModel PLM Ligand binding af ﬁnity (Sequence identity= 30%)\nRMSD↓ Pearson’s correlation↑ Spearman correlation↑ Kendall rank↑\nDeepAfﬁnity44a ✗ 1.893 ± 0.650 0.415 0.426 –\nCormorant45b ✗ 1.568 ± 0.012 0.389 0.408 –\nLSTM46c ✗ 1.985 ± 0.006 0.165 ± 0.006 0.152 ± 0.024 –\nTAPE47c ✗ 1.890 ± 0.035 0.338 ± 0.044 0.286 ± 0.124 –\nProtTrans14c ✗ 1.544 ± 0.015 0.438 ± 0.053 0.434 ± 0.058 –\n3DCNN2a ✗ 1.414 ± 0.021 0.550 0.553 –\nGNN2a ✗ 1.570 ± 0.025 0.545 0.533 –\nMaSIF48c ✗ 1.484 ± 0.018 0.467 ± 0.020 0.455 ± 0.014 –\nDGAT49b ✗ 1.719 ± 0.047 0.464 0.472 –\nDGIN49b ✗ 1.765 ± 0.076 0.426 0.432 –\nDGAT-GCN49b ✗ 1.550 ± 0.017 0.498 0.496 –\nGVP-GNN7d ✗ 1.648 ± 0.014 0.213 ± 0.013 0.164 ± 0.009 0.110 ± 0.012\nEGNN58d ✗ 1.492 ± 0.012 0.489 ± 0.017 0.472 ± 0.008 0.329 ± 0.014\nHoloProt50c ✗ 1.464 ± 0.006 0.509 ± 0.002 0.500 ± 0.005 –\nGBPNet42b ✗ 1.405 ± 0.009 0.561 0.557 –\nEGNN ✓ 1.403 ± 0.013 0.565 ± 0.016 0.544 ± 0.005 0.379 ± 0.007\nModels are sorted by the year they are released. Results are reported with mean ± standard deviation over three repeated runs and the best and second best performance are bolded and underlined,\nrespectively.\naThese results are taken from ref.2.\nbThese results are taken from ref.42.\ncThese results are copied from ref.50.\ndThese results are re-produced.\nCOMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-023-05133-1 ARTICLE\nCOMMUNICATIONS BIOLOGY|           (2023) 6:876 | https://doi.org/10.1038/s42003-023-05133-1 | www.nature.com/commsbio 5\nprotein language model is advantageous for GGNNs. More\nadditional results can be found in Supplementary Note 4. We also\nprovide a comparison of different sorts of PLMs’ inﬂuence in\nSupplementary Table 2. Besides that, we investigate the difference\nof PLMs’ effectiveness with and without MSA in Supplementary\nTable 3.\nLimitations. Despite our successful conﬁrmation that PLMs can\npromote geometric deep learning, there are several limitations\nand extensions of our framework left open for future investiga-\ntion. For instance, our 3D protein graphs are residue-level. We\nbelieve atom-level protein graphs also beneﬁt from our approach,\nbut its increase in performance needs further exploration.\nConclusion\nIn this study, we investigate a problem that has been long ignored\nby existing geometric deep learning methods for proteins. That is,\nhow to employ the abundant protein sequence data for 3D geo-\nmetric representation learning. To answer this question, we\npropose to leverage the knowledge learned by existing advanced\npre-trained protein language models and use their amino acid\nrepresentations as the initial features. We conduct a variety of\nexperiments such as protein-protein docking and model quality\nassessment to demonstrate the efﬁcacy of our approach. Our\nwork provides a simple but effective mechanism to bridge the gap\nbetween 1D sequential models and 3D geometric neural net-\nworks, and hope to throw light on how to combine information\nencoded in different protein modalities.\nMethod\nSequence recovery analysis\nPreliminary and motivations. It is commonly acknowledged that protein structures\nmaintain much more information than their corresponding amino acid sequences.\nAnd for decades long, it has been an open challenge for computational biologists to\npredict protein structure from its amino acid sequence. Though the advancement\nof Alphafold (AF)\n51 and RosettaFold52 has made a huge step in alleviating the\nlimitation brought by the number of available experimentally determined protein\nstructures, neither AF nor its successors such as Alphafold-Multimer53, IgFold54,\nand HelixFold55 are a panacea. Their predicted structures can be severely inac-\ncurate when the protein is orphan and lacks multiple sequence alignment (MSA) as\nthe template. Consequently, it is hard to conclude that protein sequences can be\nperfectly transformed to the structure modality by current tools and be used as\nextra training resources for GGNNs.\nMoreover, we argue that even if conformation is a higher-dimensional\nrepresentation, the prevailing learning paradigm may forbid GGNNs from\ncapturing the knowledge that is uniquely preserved in protein sequences. Recall\nthat GGNNs are mainly diverse in their patterns to employ 3D geometries, the\ninput features include distance\n56, angles40, torsion, and terms of other orders57.\nThe position index hidden in protein sequences, however, is usually neglected\nwhen constructing 3D graphs for GGNNs. Therefore, in this section, we design a\ntoy trial to examine whether GGNNs can succeed in recovering this kind of\npositional information.\nProtein graph construction. Here the structure of a protein can be represented as an\natom-level or residue-level graphG ¼ð V; EÞ, whereV and E ¼ð e\nijÞ correspond to\nthe set ofN nodes and M edges respectively. Nodes have their 3D coordinates\nx 2 RN ´ 3 and the initialψh-dimension roto-translational invariant featuresh 2\nRN ´ ψh (e.g., atom types and electronegativity, residue classes). Normally, there are\nthree types of options to construct connectivity for molecules:r-ball graphs, fully-\nconnected (FC) graphs, andK-nearest neighbors (KNN) graphs. In our setting,\nnodes are linked toK = 10 nearest neighbors for KNN graphs, and edges include all\natom pairs within a distance cutoff of 8Å for r-ball graphs.\nRecovery from graphs to sequences. Since most prior studies choose to establish 3D\nprotein graphs based on purely geometric information and ignore their sequential\nidentities, it provokes the following position identity question:\nCan existing GGNNs identify the sequential position order only from geometric\nstructures of proteins?\nTo answer this question, we formulate two categories of toy tasks (see Fig.3).\nThe ﬁrst one is absolute position recognition (APR), which is a classiﬁcation task.\nModels are asked to directly predict the position index ranging from 1 toN, the\nresidue number of each protein. This task adopts accuracy as the metric and\nexpects models to discriminate the absolute position of the amino acid within the\nwhole protein sequence. We compute the distribution of the protein sequence\nlengths in Supplementary Fig. 1.\nIn addition to that, we propose the second task named relative position\nestimation (RPE) to focus on the relative position of each residue. Models are\nrequired to predict the minimum distance of residue to the two sides of the given\nprotein and the root mean squared error (RMSE) is used as the metric. This task\naims to examine the capability of GGNNs to distinguish which segment the amino\nacid belongs to (i.e., the center section of the protein or the end of the protein).\nExperiments\nBackbones: We adopt three technically distinct and broadly accepted architectures\nof GGNNs for empirical veriﬁcation. To be speciﬁc, GVP-GNN\n7,43 extends stan-\ndard dense layers to operate on collections of Euclidean vectors, performing both\ngeometric and relational reasoning on efﬁcient representations of macromolecules.\nEGNN58 is a translation, rotation, reﬂection, and permutation equivariant GNN\nwithout expensive spherical harmonics. Molformer9 employs the self-attention\nmechanism for 3D point clouds while guarantees SE(3)-equivariance.\nDataset: We exploit a small non-redundant subset of high-resolution structures\nfrom the PDB. To be speciﬁc, we use only X-ray structures with resolution < 3.0Å,\nand enforce a 60% sequence identity threshold. This results in a total of 2643, 330,\nand 330 PDB structures for the train, validation, and test sets, respectively.\nExperimental details, the summary of the database, and the description of these\nGGNNs are elaborated in Supplementary Notes 1 and 2.\nEmpirical results and analysis: Table6 documents the overall results, where metrics\nare labeled with↑/↓ if higher/lower is better, respectively. It can be found that all\nGGNNs fail to recognize either the absolute or the relative positional information\nencoded in the protein sequences with an accuracy lower than 1% and an extremely\nhigh RMSE.\nThis phenomenon stems from the conventional ways to build graph\nconnectivity, which usually excludes sequential information. To be speciﬁc, unlike\ncommon applications of GNNs such as citation networks\n59, social networks60,\nknowledge graphs61, molecules do not have explicitly deﬁned edges or adjacency.\nOn the one hand, r-ball graphs utilize a cut-off distance, which is usually set as a\nhyperparameter, to determine the particle connections. But it is hard to guarantee a\ncut-off to properly include all crucial node interactions for complicated and large\nmolecules. On the other hand, FC graphs that consider all pairwise distances will\ncause severe redundancies, dramatically increasing the computational complexity\nespecially when proteins consist of thousands of residues. Besides, GGNNs also\nTask 1: Absolute Position Recognition \nTask 2: Relative Position Estimation \nr-ball graph KNN graph Sequential Order\n1\n2\n3\n8\na. b.\n2 6\n0 3\n4 3\n8\nFig. 3 Illustration of the sequence recovery problem. aProtein residue graph construction. Here we draw graphs in 2D for better visualization but study\n3D graphs for GGNNs.b Two sequence recovery tasks. Theﬁrst requires GGNNs to predict the absolute position index for each residue in the protein\nsequence. The second aims to forecast the minimum distance of each amino acid to the two sides of the protein sequence.\nARTICLE COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-023-05133-1\n6 COMMUNICATIONS BIOLOGY|           (2023) 6:876 | https://doi.org/10.1038/s42003-023-05133-1 | www.nature.com/commsbio\neasily get confused by excessive noise, leading to unsatisfactory performance. As a\nremedy, KNN becomes a more popular choice to establish graph connectivity for\nproteins34,62,63. However, all of them take no account of the sequential information\nand require GGNNs to learn this original sequential order during training.\nThe lack of sequential information can yield several problems. To begin with,\nresidues are unaware of their relative positions in the proteins. For instance, two\nresidues can be close in the 3D space but distant in the sequence, which can mislead\nmodels toﬁnd the correct backbone chain. Secondly, according to the characteristics\nof the MP mechanism, two residues in a protein with the same neighborhood are\nexpected to share similar representations. Nevertheless, the role of those two\nresidues can be signiﬁcantly separate64 when they are located at different segments\nof the protein. Thus, GGNNs may be incapable of differentiating two residues with\nthe same 1-hop local structures. This restriction has already been distinguished by\nseveral works\n6,65, but none of them make a strict and thorough investigation.\nAdmittedly, sequential order may only be necessary for certain tasks. But this toy\nexperiment strongly indicates that the knowledge monopolized by amino acid\nsequences can be lost if GGNNs only learn from protein structures.\nIntegration of language models into geometric networks. As discussed before,\nlearning about 3D structures cannot directly beneﬁt from large amounts of\nsequential data. Subsequently, the model sizes of GGNNs are limited, or instead,\noverﬁtting may occur\n66. On the contrary, comparing the number of protein\nsequences in the UniProt database67 to the number of known structures in the\nPDB, there are over 1700 times more sequences than structures. More importantly,\nthe availability of new protein sequence data continues to far outpace the avail-\nability of experimental protein structure data, only increasing the need for accurate\nprotein modeling tools.\nTherefore, we introduce a straightforward approach to assist GGNNs with\npretrained protein language models. To this end, we feed amino acid sequences into\nthose protein language models, where ESM-2\n12 is adopted in our case, and extract\nthe per-residue representations, denoted ash0 2 RN ´ ψPLM . HereψPLM = 1280. Then\nh0 can be added or concatenated to the per-atom featureh. For residue-level graphs,\nh0 immediately replaces the originalh as the input node features.\nNotably, incompatibilit y exists between the experimental structure and its\noriginal amino acid sequence. That is, structures stored in the PDBﬁles are\nusually incomplete and some strings of residues are missing due to inevitable\nrealistic issues68. They, therefore, do not perfectly match the corresponding\nsequences (i.e., FASTA sequence). There are two choices to address this\nmismatch. On the one hand, we can simply use the fragmentary sequence as the\nsubstitute for the integral amino acid sequence and forward it into the protein\nlanguage models. On the other hand, wecan leverage a dynamic programming\nalgorithm provided by Biopython\n69 to implement pairwise sequence alignment\nand abandon residues that do not exist in the PDB structures. It is empirically\ndiscovered that no big difference exists between them, so we adopt the former\nprocessing mechanism for simplicity.\nReporting summary. Further information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nThe data of model quality assessment, protein-protein interface prediction, and ligand\nafﬁnity prediction is available byhttps://www.atom3d.ai/. The data of protein-protein\nrigid-body docking can be downloaded directly from the ofﬁcial repository of Equidock\nhttps://github.com/octavian-ganea/equidock_public. Source data forﬁgures can be found\nin Supplementary Data.\nCode availability\nThe code repository is stored athttps://github.com/smiles724/bottleneck. It is also\ndeposited in ref.70.\nReceived: 23 March 2023; Accepted: 11 July 2023;\nReferences\n1. Xu, M. et al. Geodiff: a geometric diffusion model for molecular conformation\ngeneration. In International Conference on Learning Representations(ICLR,\n2022).\n2. Townshend, R. J. et al. Atom3d: tasks on molecules in three dimensions.35th\nConference on Neural Information Processing Systems(NeurIPS 2021).\n3. Wu, Z. et al. Moleculenet: a benchmark for molecular machine learning.\nChem. Sci. 9, 513– 530 (2018).\n4. Lim, J. et al. Predicting drug– target interaction using a novel graph neural\nnetwork with 3d structure-embedded graph representation.J Chem. Inf.\nModel. 59, 3981– 3988 (2019).\n5. Liu, Y., Yuan, H., Cai, L. & Ji, S. Deep learning of high-order interactions for\nprotein interface prediction. InProceedings of the 26th ACM SIGKDD\ninternational conference on knowledge discovery & data mining, 679– 687\n(ACM, 2020).\n6. Ingraham, J., Garg, V., Barzilay, R. & Jaakkola, T. Generative models for\ngraph-based protein design. InAdvances in neural information processing\nsystems 32 (NeurIPS, 2019).\n7. Jing, B., Eismann, S., Suriana, P., Townshend, R. J. & Dror, R. Learning from\nprotein structure with geometric vector perceptrons.arXiv preprint\narXiv:2009.01411 (2020).\n8. Strokach, A., Becerra, D., Corbi-Verge, C., Perez-Riba, A. & Kim, P. M. Fast\nand ﬂexible protein design using deep graph neural networks.Cell Syst. 11,\n402– 411 (2020).\n9. Wu, F. et al. Molformer: Motif-based transformer on 3d heterogeneous\nmolecular graphs. InProceedings of the AAAI Conference on Artiﬁcial\nIntelligence. Vol. 37 (2023).\n10. Atz, K., Grisoni, F. & Schneider, G. Geometric deep learning on molecular\nrepresentations. Nat. Mach. Intell.3, 1023– 1032 (2021).\n11. Meier, J. et al. Language models enable zero-shot prediction of the effects of\nmutations on protein function.Adv. Neural Inf. Process. Syst.34, 29287– 29303\n(2021).\n12. Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure\nwith a language model.Science 379, 1123– 1130 (2023).\n13. Rives, A. et al. Biological structure and function emerge from scaling\nunsupervised learning to 250 million protein sequences.Proc. Natl Acad. Sci.\n118, e2016239118 (2021).\n14. Elnaggar, A. et al. Prottrans: towards cracking the language of life’s code\nthrough self-supervised deep learning and high performance computing.\nIEEE. Trans. Pattern. Anal. Mach. Intell.44, 7112– 7127 (2021).\n15. Hsu, C. et al. Learning inverse folding from millions of predicted structures.\nIn Proceedings of the 39th International Conference on Machine Learning.\nVol. 162, 8946\n– 8970 (PMLR, 2022).\n16. Boadu, F., Cao, H. & Cheng, J. Combining protein sequences and structures\nwith transformers and equivariant graph neural networks to predict protein\nfunction. Preprint athttps://www.biorxiv.org/content/10.1101/2023.01.17.\n524477v1 (2023).\n17. Chen, C., Chen, X., Morehead, A., Wu, T. & Cheng, J. 3d-equivariant graph\nneural networks for protein model quality assessment.Bioinformatics 39,\nbtad030 (2023).\n18. Dunbar, J. et al. Sabdab: the structural antibody database.Nucleic Acids Res.\n42, D1140– D1146 (2014).\n19. Chandonia, J.-M., Fox, N. K. & Brenner, S. E. Scope: classiﬁcation of large\nmacromolecular structures in the structural classiﬁcation of proteins-extended\ndatabase. Nucleic Acids Res.47, D475– D481 (2019).\n20. Velankar, S. et al. Sifts: structure integration with function, taxonomy and\nsequences resource. Nucleic Acids Res.41, D483– D489 (2012).\n21. Berman, H. M. et al. The protein data bank.Nucleic Acids Res.28, 235– 242\n(2000).\n22. Mistry, J. et al. Pfam: the protein families database in 2021.Nucleic Acids Res.\n49, D412– D419 (2021).\n23. Bairoch, A. et al. The universal protein resource (uniprot).Nucleic Acids Res.\n33, D154– D159 (2005).\n24. Yanofsky, C., Horn, V. & Thorpe, D. Protein structure relationships revealed\nby mutational analysis.Science 146, 1593– 1594 (1964).\n25. Göbel, U., Sander, C., Schneider, R. & Valencia, A. Correlated mutations and\nresidue contacts in proteins.Proteins 18, 309– 317 (1994).\n26. Cheng, J. et al. Estimation of model accuracy in casp13.Proteins 87,\n1361– 1377 (2019).\n27. Kryshtafovych, A., Schwede, T., Topf, M., Fidelis, K. & Moult, J. Critical\nassessment of methods of protein structure prediction (casp)-round xiii.\nProteins 87, 1011– 1020 (2019).\nTable 6 Results of two residue position identiﬁcation tasks.\nModels Graph type APR RPE\nAccuracy (%) ↑ RMSE ↓\nGVP-GNN r-ball graph 0.157 ± 0.002 392.38 ± 3.41\nKNN graph 0.158 ± 0.003 392.38 ± 4.05\nEGNN r-ball graph 0.150 ± 0.005 412.70 ± 2.36\nKNN graph 0.131 ± 0.004 403.86 ± 1.77\nMolformer FC graph 0.148 ± 0.007 270.69 ± 4.53\nResults are reported with mean ± standard deviation over three repeated runs and the best\nperformance is in bold.\nCOMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-023-05133-1 ARTICLE\nCOMMUNICATIONS BIOLOGY|           (2023) 6:876 | https://doi.org/10.1038/s42003-023-05133-1 | www.nature.com/commsbio 7\n28. Vreven, T. et al. Updates to the integrated protein– protein interaction\nbenchmarks: docking benchmark version 5 and afﬁnity benchmark version 2.\nJ. Mol. Biol.427, 3031– 3041 (2015).\n29. Townshend, R., Bedi, R., Suriana, P. & Dror, R. End-to-end learning on 3d\nprotein structure for interface prediction. InAdvances in Neural Information\nProcessing Systems 32 (NeurIPS, 2019).\n30. Wang, R., Fang, X., Lu, Y. & Wang, S. The pdbbind database: collection of\nbinding afﬁnities for protein- ligand complexes with known three-\ndimensional structures. J. Med. Chem.47, 2977– 2980 (2004).\n31. Liu, Z. et al. Pdb-wide collection of binding data: current status of the pdbbind\ndatabase. Bioinformatics 31, 405– 412 (2015).\n32. Paszke, A. et al. Pytorch: an imperative style, high-performance deep learning\nlibrary. InAdvances in Neural Information Processing Systems32 (NeurIPS, 2019).\n33. Fey, M. & Lenssen, J. E. Fast graph representation learning with pytorch\ngeometric. In Workshop of International Conference on Learning\nRepresentations (ICLR, 2019).\n34. Ganea, O.-E. et al. Independent se (3)-equivariant models for end-to-end rigid\nprotein docking. InInternational Conference on Learning Representations\n(ICLR, 2022).\n35. Zhang, J. & Zhang, Y. A novel side-chain orientation dependent potential\nderived from random-walk reference state for protein fold selection and\nstructure prediction. PloS one 5, e15386 (2010).\n36. Uziela, K., Menéndez Hurtado, D., Shu, N., Wallner, B. & Elofsson, A. Proq3d:\nimproved model quality assessments using deep learning.Bioinformatics 33,\n1578– 1580 (2017).\n37. Olechnovi č, K. & Venclovas,Č. Voromqa: Assessment of protein structure\nquality using interatomic contact areas.Proteins: Structure, Function, and\nBioinformatics 85, 1131– 1145 (2017).\n38. Karasikov, M., Pagès, G. & Grudinin, S. Smooth orientation-dependent\nscoring function for coarse-grained protein quality assessment.Bioinformatics\n35, 2801– 2808 (2019).\n39. Pagès, G., Charmettant, B. & Grudinin, S. Protein model quality assessment\nusing 3d oriented convolutional neural networks.Bioinformatics 35,\n3313– 3319 (2019).\n40. Klicpera, J., Groß, J. & Günnemann, S. Directional message passing for\nmolecular graphs. InInternational Conference on Learning Representations\n(ICLR, 2020).\n41. Eismann, S. et al. Hierarchical, rotation-equivariant neural networks to select\nstructural models of protein complexes.Proteins 89, 493– 501 (2021).\n42. Aykent, S. & Xia, T. Gbpnet: universal geometric representation learning on\nprotein structures. InProceedings of the 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining\n,4 – 14 (ACM, 2022).\n43. Jing, B., Eismann, S., Soni, P. N. & Dror, R. O. Equivariant graph neural\nnetworks for 3d macromolecular structure. In Preprint athttps://arxiv.org/\nabs/2106.03843 (2021).\n44. Karimi, M., Wu, D., Wang, Z. & Shen, Y. Deepafﬁnity: interpretable deep\nlearning of compound– protein afﬁnity through uniﬁed recurrent and\nconvolutional neural networks.Bioinformatics 35, 3329– 3338 (2019).\n45. Anderson, B., Hy, T. S. & Kondor, R. Cormorant: covariant molecular neural\nnetworks. InAdvances in neural information processing systems32 (NeurIPS,\n2019).\n46. Bepler, T. & Berger, B. Learning protein sequence embeddings using\ninformation from structure. Preprint athttps://arxiv.org/abs/1902.08661(2019).\n47. Rao, R. et al. Evaluating protein transfer learning with tape.Adv Neural Inf.\nProcess. Syst. 32, 9689– 9701 (2019).\n48. Gainza, P. et al. Deciphering interactionﬁngerprints from protein molecular\nsurfaces using geometric deep learning.Nat. Methods 17, 184– 192 (2020).\n49. Nguyen, T. et al. Graphdta: Predicting drug– target binding afﬁnity with graph\nneural networks. Bioinformatics 37, 1140– 1147 (2021).\n50. Somnath, V. R., Bunne, C. & Krause, A. Multi-scale representation learning on\nproteins. Adv. Neural Inf. Process. Syst.34, 25244– 25255 (2021).\n51. Jumper, J. et al. Highly accurate protein structure prediction with alphafold.\nNature 596, 583– 589 (2021).\n52. Baek, M. et al. Accurate prediction of protein structures and interactions using\na three-track neural network.Science 373, 871– 876 (2021).\n53. Evans, R. et al. Protein complex prediction with alphafold-multimer. Preprint\nat https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2 (2022).\n54. Ruffolo, J. A. & Gray, J. J. Fast, accurate antibody structure prediction from\ndeep learning on massive set of natural antibodies.Biophys. J.121, 155a– 156a\n(2022).\n55. Wang, G. et al. Helixfold: an efﬁcient implementation of alphafold2 using\npaddlepaddle. Preprint athttps://arxiv.org/abs/2207.05477 (2022).\n56. Schütt, K. et al. Schnet: a continuous-ﬁlter convolutional neural network for\nmodeling quantum interactions. InAdvances in neural information processing\nsystems 30 (NeurIPS, 2017).\n57. Liu, Y. et al. Spherical message passing for 3d molecular graphs. In\nInternational Conference on Learning Representations(ICLR, 2021).\n58. Satorras, V. G., Hoogeboom, E. & Welling, M. E (n) equivariant graph neural\nnetworks. In International conference on machine learning, 9323– 9332\n(PMLR, 2021).\n59. Sen, P. et al. Collective classiﬁcation in network data.AI Mag.29,9 3– 93 (2008).\n60. Hamilton, W., Ying, Z. & Leskovec, J. Inductive representation learning on\nlarge graphs. InAdvances in neural information processing systems.3 0\n(NeurIPS, 2017).\n61. Carlson, A. et al. Toward an architecture for never-ending language learning.\nIn Twenty-Fourth AAAI conference on artiﬁcial intelligence (AAAI, 2010).\n62. Fout, A., Byrd, J., Shariat, B. & Ben-Hur, A. Protein interface prediction using\ngraph convolutional networks. InAdvances in neural information processing\nsystems, 30 (NeurIPS, 2017).\n63. Stärk, H., Ganea, O., Pattanaik, L., Barzilay, R. & Jaakkola, T. Equibind:\ngeometric deep learning for drug binding structure prediction. In\nInternational Conference on Machine Learning, 20503– 20521 (PMLR, 2022).\n64. Murphy, R., Srinivasan, B., Rao, V. & Ribeiro, B. Relational pooling for graph\nrepresentations. InInternational Conference on Machine Learning, 4663– 4673\n(PMLR, 2019).\n65. Zhang, Z. et al. Protein representation learning by geometric structure pretraining.\nIn International Conference on Learning Representations(ICLR, 2023).\n66. Hermosilla, P. & Ropinski, T. Contrastive representation learning for 3d\nprotein structures. Preprint athttps://arxiv.org/abs/2205.15675 (2022).\n67. Consortium, U. Uniprot: a hub for protein information.Nucleic Acids Res.43,\nD204– D212 (2015).\n68. Djinovic-Carugo, K. & Carugo, O. Missing strings of residues in protein\ncrystal structures. Intrinsically Disord. Proteins3, e1095697 (2015).\n69. Cock, P. J. et al. Biopython: freely available python tools for computational\nmolecular biology and bioinformatics.Bioinformatics 25, 1422– 1423 (2009).\n70. Wu, F. Code for Paper’Integration of pre-trained protein language models\ninto geometric deep learning networks’. Zenodo https://doi.org/10.5281/\nzenodo.8022149 (2023).\nAcknowledgements\nThis work is supported in part by the Institute of AI Industry Research at Tsinghua\nUniversity and the Molecule Mind.\nAuthor contributions\nF.W. and J.X. led the research. F.W. contributed technical ideas. F.W. and Y.T. developed\nthe proposed method. F.W., D.R., and Y.T. performed the analysis. J.X. and D.R. pro-\nvided evaluation and suggestions. All authors contributed to the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s42003-023-05133-1.\nCorrespondence and requests for materials should be addressed to Stan Z. Li.\nPeer review informationCommunications Biology thanks Jianzhao Gao, Arne Elofsson,\nand the other, anonymous, reviewer(s) for their contribution to the peer review of this\nwork. Primary Handling Editors: Yuedong Yang and Gene Chong.\nReprints and permission informationis available athttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2023\nARTICLE COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-023-05133-1\n8 COMMUNICATIONS BIOLOGY|           (2023) 6:876 | https://doi.org/10.1038/s42003-023-05133-1 | www.nature.com/commsbio"
}