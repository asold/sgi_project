{
    "title": "CCTrans: Simplifying and Improving Crowd Counting with Transformer",
    "url": "https://openalex.org/W3203495278",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2024071360",
            "name": "Tian Ye",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2357614981",
            "name": "Chu, Xiangxiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1949393462",
            "name": "Wang Hongpeng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2031454541",
        "https://openalex.org/W2137401668",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W3109157205",
        "https://openalex.org/W3176458063",
        "https://openalex.org/W3203606893",
        "https://openalex.org/W3130071011",
        "https://openalex.org/W2963893037",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2964209782",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2987988567",
        "https://openalex.org/W2886443245",
        "https://openalex.org/W3004672782",
        "https://openalex.org/W3027606690",
        "https://openalex.org/W2967776630",
        "https://openalex.org/W2980794690",
        "https://openalex.org/W2981436300",
        "https://openalex.org/W3164972083",
        "https://openalex.org/W3211432419",
        "https://openalex.org/W2963604034",
        "https://openalex.org/W3034785991",
        "https://openalex.org/W2949162858",
        "https://openalex.org/W3203845557",
        "https://openalex.org/W2963693541",
        "https://openalex.org/W3081099313",
        "https://openalex.org/W3107554785",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2463631526",
        "https://openalex.org/W2412782625",
        "https://openalex.org/W2914913933",
        "https://openalex.org/W2072232009",
        "https://openalex.org/W3101165561",
        "https://openalex.org/W3097407159",
        "https://openalex.org/W3097994591",
        "https://openalex.org/W3035307763",
        "https://openalex.org/W2996703886",
        "https://openalex.org/W3154723007",
        "https://openalex.org/W2207893099",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2274287116",
        "https://openalex.org/W2963838390",
        "https://openalex.org/W2949333977",
        "https://openalex.org/W2982007926",
        "https://openalex.org/W2967069910",
        "https://openalex.org/W2948513880"
    ],
    "abstract": "Most recent methods used for crowd counting are based on the convolutional neural network (CNN), which has a strong ability to extract local features. But CNN inherently fails in modeling the global context due to the limited receptive fields. However, the transformer can model the global context easily. In this paper, we propose a simple approach called CCTrans to simplify the design pipeline. Specifically, we utilize a pyramid vision transformer backbone to capture the global crowd information, a pyramid feature aggregation (PFA) model to combine low-level and high-level features, an efficient regression head with multi-scale dilated convolution (MDC) to predict density maps. Besides, we tailor the loss functions for our pipeline. Without bells and whistles, extensive experiments demonstrate that our method achieves new state-of-the-art results on several benchmarks both in weakly and fully-supervised crowd counting. Moreover, we currently rank No.1 on the leaderboard of NWPU-Crowd. Our code will be made available.",
    "full_text": "CCTrans: Simplifying and Improving Crowd Counting with Transformer\nYe Tian1*, Xiangxiang Chu2, Hongpeng Wang1\n1 Harbin Institute of Technology (Shenzhen)\n2 Meituan\n119S151092@stu.hit.edu.cn, wanghp@hit.edu.cn 2chuxiangxiang@meituan.com\nAbstract\nMost recent methods used for crowd counting are based on\nthe convolutional neural network (CNN), which has a strong\nability to extract local features. But CNN inherently fails\nin modeling the global context due to the limited recep-\ntive Ô¨Åelds. However, the transformer can model the global\ncontext easily. In this paper, we propose a simple approach\ncalled CCTrans to simplify the design pipeline. SpeciÔ¨Åcally,\nwe utilize a pyramid vision transformer backbone to capture\nthe global crowd information, a pyramid feature aggregation\n(PFA) model to combine low-level and high-level features, an\nefÔ¨Åcient regression head with multi-scale dilated convolution\n(MDC) to predict density maps. Besides, we tailor the loss\nfunctions for our pipeline. Without bells and whistles, exten-\nsive experiments demonstrate that our method achieves new\nstate-of-the-art results on several benchmarks both in weakly\nand fully-supervised crowd counting. Moreover, we currently\nrank No.1 on the leaderboard of NWPU-Crowd. Our code\nwill be made available.\nIntroduction\nAs a research hot topic of computer vision, crowd count-\ning is to estimate the number of crowds in a scene, which is\napplied in many Ô¨Åelds such as urban planning and trafÔ¨Åc su-\npervision. Mainstream methods focus on designing various\nconvolutional neural networks.\nHowever, there are still two challenges in crowd counting\nfrom the generation of images. Crowds near the camera have\nlarger scales as well as lower densities and vice versa, which\ncauses dramatic scale and density variations within an im-\nage. One well-recognized solution is enhancing the global\ncontext modeling capability of CNN-based models. But it\nis not perfect due to limited receptive Ô¨Åelds. Therefore, re-\nsearchers propose various mechanisms to reÔ¨Åne CNN-based\nmodels. A popular approach is designing multi-column ar-\nchitectures with input images in different resolution ratios\nto extract features of crowds in different scales and densi-\nties (Zhang et al. 2016; Liu and Qiu 2019; Sam, Surya, and\nBabu 2017; Sindagi and Patel 2017). But the model structure\nof these methods is inefÔ¨Åcient with many redundant blocks.\nAnother approach is introducing auxiliary tasks into crowd\ncounting to capture global semantic information better at the\ncost of higher complexity and training time (Shi et al. 2019;\n*Work done during the internship at Meituan.\nShi, Mettes, and Snoek 2019; Jiang, Zhang, and Xu 2020;\nLiu, van de Weijer, and Bagdanov 2019).\nRecent studies focus on designing different attention\nmechanisms to focus on scale and density variations in\nglobal context(Yan, Yuan, and Zuo 2019; Liu, Weng, and\nMu 2019; Liu, Salzmann, and Fua 2019; Jiang, Zhang, and\nXu 2020; Liu et al. 2019a). However, these pipelines are\nusually complicated with many sensitive hyper-parameters,\nwhich require careful tuning for different datasets.\nAs an orthogonal approach, some work improves the per-\nformance by optimizing novel image augmentation and loss\nfunctions (Li, Zhang, and Chen 2018; Yang and Li 2020;\nWang et al. 2020; Ma et al. 2019). Nevertheless, it usually\nneeds sufÔ¨Åcient data and expert experience. The design is\nalso sophisticated without signiÔ¨Åcant improvements.\nVision transformer has aroused great interest in the whole\ncommunity and shows promising results across many vision\ntasks (Dosovitskiy et al. 2021; Chu et al. 2021a; Touvron\net al. 2020; Liu et al. 2021; Chu et al. 2021b; Zheng et al.\n2021). One great advantage of transformer is able to capture\nlong context dependency and enjoy global receptive Ô¨Åelds.\nRegarding semantic segmentation, which is a point-level\nprediction task like crowd counting, Zheng et al. (2021);\nChu et al. (2021a); Liu et al. (2021) construct transformer-\nbased models to achieve better performance than CNN base-\nlines. A natural question is whether we can simplify the\ncomplicated pipelines of crowd counting by using trans-\nformers. The focus of our paper is Ô¨Ånding the answer.\nIn summary, the contributions of this paper are 4-fold:\n‚Ä¢ To simplify the pipelines, we utilize the transformer to\nconstruct a simple but high-performance crowd counting\nmodel called CCTrans, which can extract semantic fea-\ntures with global context information.\n‚Ä¢ We design an effective feature aggregation block and a\nsimple regression head with multi-scale receptive Ô¨Åelds.\nWith two simple designed blocks, we can strengthen the\ncaptured features and get accurate regression results.\n‚Ä¢ To further strengthen our method, we tailor the loss func-\ntions in both weakly and fully-supervised manners for\nour method. SpeciÔ¨Åcally, we utilize a smooth weighted\nloss for the former and smooth L1 for the latter.\n‚Ä¢ Extensive experiments across Ô¨Åve popular benchmarks\nincluding UCF CC 50, ShanghaiTech Part A and Part B,\narXiv:2109.14483v1  [cs.CV]  29 Sep 2021\n‚òì\n1√ó13√ó35√ó5\nDilation rate=1\nDilation rate=2\nDilation rate=31/161/32\n1/8\nTransformerBackbone Regression Head\n1/8 Density Map\nŒ£\nUpsample ConvDilated ConvElementwise AddŒ£Sum\nCount Number\nGround-truth Density Map\nWeaklySupervised\nFullySupervised\n‚òìConcatenate\n1√ó1\nùê∂!\nùê∂\"\nùê∂#\nùêπ\"\nùêπ#\nùêπ!\nFigure 1: The pipeline of CCTrans. The input image is transformed into a 1D sequence Ô¨Årstly, then the output is fed into the\ntransformer-based backbone. We adopt a pyramid transformer (Chu et al. 2021a) to capture global context through various\ndownsampling stages. The outputs of each stage are reshaped into 2D feature maps for pyramid feature aggregation. Finally,\na simple regression head with multi-scale receptive Ô¨Åelds is used to regress the Ô¨Ånal results. We support two fashions of\nsupervision. For fully-supervised manner, CCTrans regresses a density map. For weakly-supervised manner, CCTrans sums up\nall the pixel values of the predicted density map as the crowd number for counting regression.\nUCF QNRF, and NWPU-Crowd, show that our method\nachieves new state-of-the-art results under both weakly\nand fully-supervised settings. Moreover, CCTrans ranks\nNo.1 on the leaderboard of NWPU-Crowd.\nProposed Method\nThe architecture of our method is shown in Figure 1. Firstly,\nthe input images are split into Ô¨Åxed-size image patches.\nThen, the output is Ô¨Çattened to a 1D sequence of vectors.\nNext, a pyramid transformer backbone is used to extract\nglobal features from the sequences. Then the 1D sequences\nof each stage are reshaped into 2D feature maps and up-\nsampled to the same resolution. And then, an elementwise\naddition is performed on these feature maps. Finally, a sim-\nple regression head with multi-scale receptive Ô¨Åelds is ap-\nplied to regress the density map. The Ô¨Ånal density map and\nthe sum of all its pixel values are used to build the loss func-\ntions of weakly and fully-supervised manners, respectively.\n2D Image to 1D Sequence\nAn image is transformed into 1D sequences before going\ninto the transformer. We denote an input image as I ‚àà\nRH√óW√ó3, where H, W, and 3 are respectively its height,\nwidth, and channel size. We then split it into H\nK √óW\nK image\npatches and each patch is of size K √óK √ó3. We Ô¨Çatten\nthis 2D patch array into a 1D patch sequence x ‚ààRN√óD,\nN = HW\nK2 , D = K √óK √ó3. We use patch embedding\nfor sequence x by applying a learnable projection f : xi ‚Üí\nei ‚ààRD (i=1,..N) to obtain the sequence e ‚ààRN√óD. In\nthis way, spatial and channel features of thei-th image patch\nxi are transformed into embedded features of the i-th em-\nbedding vector ei.\nTransformer Backbone\nWe adopt a pyramid transformer backbone Twins (Chu et al.\n2021a), which features a scheme of alternated local and\nglobal attention. Owning local and global receptive Ô¨Åelds\ncaptures both short and long range relations. Due to this spe-\nciÔ¨Åc design, Twins obtains great performance across several\nbenchmarks compared with its counterparts. Moreover, it is\ndeployment-friendly, which eases practical application.\nBased on the standard transformer module, Twins pro-\nposes the spatially separable self-attention (SSSA) module\nto reduce the amount of calculation. To achieve so, the input\nsequences of l-th layer Zl‚àí1 are reshaped into 2D feature\nmaps Ô¨Årst. Each feature map can then be spatially grouped\nwithin a local window to compute so-called locally-grouped\nself-attention (LSA). Successively for Global sub-sampled\nattention (GSA), Twins adopts subsampling for keys to serve\nas a representative within each window to cut down the cost.\nSpeciÔ¨Åcally, at the stage of LSA, the feature maps are\nequally divided into k1 √ók2 sub-windows. And the self-\nattention computation is locally performed in each sub-\nwindow. In this way, the local features can be obtained ef-\nÔ¨Åciently. As there are no communications among the sub-\nwindows, the following global attention comes into play. At\nthe stage of GSA, each sub-window generates a single rep-\nresentative through a convolution operation. This represen-\ntative summarizes the key information of the sub-window.\nThen a self-attention computation is performed on the repre-\nsentatives of all the sub-windows. In this way, sub-windows\ncan communicate with each other through their representa-\ntives, so that global features can be captured. Interleaving\nthe necessary Multi-layer Perceptron (MLP) modules, layer\nnormalization (LN), and residual connections, the structure\nof the l-th layer of the transformer in Twins can be deÔ¨Åned\nas follows:\nZ‚Ä≤\nl = LSA(LN(Zl‚àí1)) +Zl‚àí1, (1)\nZ‚Ä≤‚Ä≤\nl = MLP(LN(Z‚Ä≤\nl)) +Z‚Ä≤\nl, (2)\nZ‚Ä≤‚Ä≤‚Ä≤\nl = GSA(LN(Z‚Ä≤‚Ä≤\nl )) +Z‚Ä≤‚Ä≤\nl , (3)\nZl = MLP(LN(Z‚Ä≤‚Ä≤‚Ä≤\nl )) +Z‚Ä≤‚Ä≤‚Ä≤\nl . (4)\nPyramid Feature Aggregation\nThough the transformer backbone can extract global fea-\ntures, feature maps from high layers still lack detail infor-\nmation that can not be reconstructed by up-sampling. These\nhigh-level features are too fuzzy to distinguish the bound-\naries of different objects, which makes crowd counting mod-\nels hard to learn the accurate location information of crowds.\nTo address this problem, we construct a model to make full\nuse of both high-level and low-level information.\nFor the speciÔ¨Åc stage of the encoding s, we use the output\nsequence Zs ‚ààRN√ód (s=1...T), which contains global con-\ntext. Then we reshape the one-dimensional sequenceZs into\na two-dimensional grid of embedded vectors, whose height\nand width are both\n‚àö\nN. Regarding the fact that feature maps\nfrom shallow layers contain rich detail information but lack\nsemantics and those from deep layers are just the opposite.\nWe construct a feature pyramid to aggregate the semantic\ninformation from high-level layers with detail information\nfrom low-level layers. SpeciÔ¨Åcally, we upsample the feature\nmaps from all stages to 1\n8 size of the input image, which\nis a common choice in most work (Li, Zhang, and Chen\n2018). This resolution also helps to make fair comparisons\nwith other methods. The structure of Pyramid Feature Ag-\ngregation (PFA) is shown in Figure 1.\nRegression Head with Multi-scale Receptive Fields\nRegarding our transformer-based backbone and PFA block\nhave already captured sufÔ¨Åcient global information. There-\nfore, we only use a simple regression head to regress ac-\ncurate density maps. SpeciÔ¨Åcally, we construct a module\nwith multi-scale receptive Ô¨Åelds to detect the global scale\nand density variances. A straightforward approach is stack-\ning dilated convolution (DConv) layers as (Li, Zhang, and\nChen 2018; Yan, Yuan, and Zuo 2019). DConv layers can\nenlarge receptive Ô¨Åelds while containing the spatial resolu-\ntion. However, this design requires careful design of the di-\nlation coefÔ¨Åcient at each layer to avoid the gridding effect\n(Fang et al. 2020), where some pixels are missed in the later\nconvolutions (see Figure 2). The gridding effect has a great\nimpact on the results of crowd counting. Because the scale of\ncrowds beyond the capture is very small, one missing pixel\nin high-level feature maps will make the regression head ig-\nnore several people. In addition, it is a bit time-consuming\nto stack layers in depth, not width (Szegedy and Ioffe 2016).\nThis will increase the training cost and make the model difÔ¨Å-\ncult to expand to real-time scenarios. The experiment result\nin Table 5 shows this design is worse than stacking in width.\nInspired by DeepLabv3+ (Chen, Papandreou, and Kokki-\nnos 2018) and RFB (Liu, Huang, and Wang 2018) we design\na Multi-scale Dilated Convolution (MDC) block by stacking\nDConv layers with different dilation rates in parallel. But\n(a) dilation rate=2, 2, 2\n (b) dilation rate=1, 2, 3\nFigure 2: The gridding effect in stacking DConv layers with\nthe Ô¨Åxed dilation rate. And stacking DConv layers with dif-\nferent dilation rates can avoid this.\ndifferent from the ASPP (Chen, Papandreou, and Kokkinos\n2018), MDC is more lightweight but powerful enough to ob-\ntain good performance.\nSpeciÔ¨Åcally, it contains three columns ( C1, C2, C3) and\na shortcut path. And each column consists of a single con-\nvolutional layer and a dilated convolutional layer. We set the\ncorresponding kernel sizes and dilation rates as small as pos-\nsible to Ô¨Åt the crowd counting scenes full of small-scale ob-\njects. Each convolutional layer is followed by a batch nor-\nmalization (BN) layer and a ReLU activation function. We\nconcatenate the output feature maps from each column and\nadd them with a shortcut path to make use of multi-scale\nfeatures. Finally, we use a1 √ó1 convolution layer to regress\nthe density map. The structure is shown in Figure 1.\nLoss Function Design\nWe design different loss functions for fully-supervised den-\nsity regression and weakly-supervised counting regression.\nLoss function design for the fully-supervised setting.\nOur design is based on a popular loss from (Wang et al.\n2020), which is formulated by weighted summation of\nCounting loss, Optimal Transport (OT) loss, and Total Vari-\nation (TV) loss. For a predicted density map D and its\nground-truth D‚Ä≤, the loss function is deÔ¨Åned as:\nLdm = L1(P,G) +Œª1LOT + Œª2LTV (D,D\n‚Ä≤\n), (5)\nwhere P and Gdenote the crowd number of Dand D‚Ä≤, re-\nspectively. Œª1 and Œª2 are the loss coefÔ¨Åcient, and they are\nset to 0.01 and 0.1 in DM-Count (Wang et al. 2020).\nOT loss beneÔ¨Åts the model with a strong Ô¨Åtting ability\nto minimize the distribution gap between the predicted den-\nsity map and the ground-truth. However, Wang et al. (2020)\npoint out that it can not approximate well the sparse areas\nof crowds and additionally use an extra TV loss to stabilize.\nBut TV loss uses the original head annotations of ground-\ntruth, which is not smooth enough to build a robust represen-\ntation of people. Especially in some sparse scenes, crowds\nhave a larger scale, and it is unreasonable to represent a per-\nson by a pixel. To address this issue, we utilize the mean\nsquare error i.e. L2 to regularize the gap between prediction\nand smoothed annotation maps. The smooth feature maps\nare generated by applying the adaptive Gaussian kernels (Li,\nZhang, and Chen 2018). The total loss is written as,\nLd = L1(P,G) +Œª1LOT + Œª2L2(D,D\n‚Ä≤\n). (6)\nIn our experiment, we Ô¨Åx Œª1 = 0.01 as (Wang et al. 2020)\nand only Ô¨Åne-tune Œª21. Although this may be sub-optimal\n11.0 is used in our paper.\nfor our method, it still shows good performance across sev-\neral benchmark datasets.\nLoss function design for the weakly-supervised setting.\nTo be robust, we utilize smooth L1 loss instead of L1. Be-\ncause the number of crowds varies greatly in different im-\nages, and L1 is sensitive to outliers. Our weakly-supervised\nloss function is deÔ¨Åned as:\nLc = smoothL1 (D,D\n‚Ä≤\n). (7)\nExperimental results in Table 6 also show this design can\nbring boosted performance.\nExperiments\nImplementation Details.\nDataset. We evaluate our method across Ô¨Åve benchmarks,\nincluding UCF CC 50 (Idrees et al. 2013), ShanghaiTech\nPart A and Part B (Zhang et al. 2016), UCF QNRF (Idrees\nand Tayyab 2018), and NWPU-Crowd (Wang et al. 2020).\nThese datasets differ in image resolution ratios, quantities,\ncrowding degree, and color spaces.\nTraining setting and hyper-parameter. To make fair\ncomparisons, the transformer-based backbone is the ofÔ¨Åcial\nTwins-SVT-large model, which is pretrained on the Ima-\ngeNet 1k dataset (Deng et al. 2009). We only use random\ncropping and random horizontal Ô¨Çipping as data augmenta-\ntions for all experiments, which strictly follows (Wang et al.\n2020; Ma et al. 2019). The crop size of both ST Part B and\nUCF QNRF is 512 and is changed to 256 for UCF CC 50\nand ST Part A. We use AdamW (Loshchilov and Hutter\n2018) with a batch size of 8 because it is more suitable for\ntraining transformer-based models. The initial learning rate\nis set to 1e-5. We use l2 regularization of 0.0001 to avoid\nover-Ô¨Åtting. All experiments are conducted using PyTorch\non a single 32G Tesla V100 GPU.\nEvaluation metric. Previous works in crowd density es-\ntimation use Mean Absolute Error (MAE) and Root Mean\nSquared Error (RMSE or MSE for short) as evaluation met-\nrics in CSRNet. Besides, mean Normalized Absolute Error\n(NAE) is an extra metric from (Wang et al. 2020). They can\nbe formulated as follows:\nMAE = 1\nN\nN‚àë\ni=1\n|Pi ‚àíGi|, (8)\nMSE =\nÓµ™Óµ´Óµ´‚àö 1\nN\nN‚àë\ni=1\n|Pi ‚àíGi|2, (9)\nNAE = 1\nN\nN‚àë\ni=1\n|Pi ‚àíGi|\nGi\n, (10)\nwhere N is the number of testing images, Pi and Gi are\nthe predicted and ground-truth number of crowds in the i-th\nimage, respectively.\nComparisons with State-of-the-art Methods\nPerformace on UCFCC 50. This dataset shows a lot of\nchallenges. It randomly collects only 50 gray images with\nserious perspective distortions from the Internet. We report\nthe result in Table 1. Our method surpasses ASNet by 3.5%\nin MAE and CAN by 3.8% in MSE. It indicates the trans-\nformer based on self-attention mechanism is robust to visual\ndistortion and the deÔ¨Åciency in RGB information.\nPerformace on ShanghaiTech. The result is shown in Ta-\nble 1. For part A, the images are randomly crawled from\nthe Internet. The number of people in these images varies\nlargely with a wide range. For part B, the images are cap-\ntured by the surveillance cameras in the streets of Shang-\nhai. These images have dramatic intra-scene scale and den-\nsity variances. CNN-based methods lack context modeling\nability to deal with them. Our method obtains new state-of-\nthe-art, which outperforms the previous best method P2PNet\n(Song et al. 2021) without complicated designs.\nPerformace on UCFQNRF. The images of this dataset\nare collected from different websites. These images contain\nlarge diversity both in scenes and image sizes. Most of the\nobjects in the pictures are small in scale. The result is shown\nin Table 1. Our method outperforms other methods again.\nThat is because our PFA can contain more detail informa-\ntion, which is helpful for our model to detect small objects.\nAnd our MDC can better capture multi-scale features and\nglobal context information from the transformer to regress\nthe crowd number.\nPerformance on NWPU-Crowd. Different from Shang-\nhaiTech Part B which has a dramatic intra-scene scale and\ndensity variance, this dataset has a serious inter-scene scale\nand density variance. On this new and challenging dataset,\nour method surpasses all previous methods with a large mar-\ngin. The result is shown in Table 2. Our method obtains 38.6\nMAE on the validation dataset, which is 14.4 lower than the\nconcurrent work BCCT (Sun et al. 2021). We also evaluate\nour method on the test set by uploading the results to the\nofÔ¨Åcial server and our method obtains 69.3 MAE, which is\n8.1 lower than the previous best method P2PNet (Song et al.\n2021). Note that we rank No.1 on the leader-board without\nbells and whistles.\nComparison with other Transformer-based approaches.\nThere are two concurrent transformer-based works. One is\nTransCrowd (Liang et al. 2021), which is designed for solv-\ning crowd counting under the weakly-supervised setting. It\nutilizes ViT (Dosovitskiy et al. 2021) to extract limited fea-\ntures from only one stage. And it uses an additional regres-\nsion token and global average pooling to perform counting\nregression, respectively. This design can not make full use of\nthe advantage of global attention. Table 1 and 2 show that\nour method outperforms it with clear margins across several\nbenchmarks. Another is BCCT (Sun et al. 2021), which also\nutilizes ViT as the backbone and designs various attention\nmechanisms and complicated auxiliary losses to boost the\nperformance. Nevertheless, our method outperforms it with\na clear advantage on most benchmarks, which is shown in\nTable 1 and 2.\nMethod Venue Label UCF CC 50 ST Part A ST Part B UCF QNRF\nLocation Number MAE MSE MAE MSE MAE MSE MAE MSE\nCAN (2019) CVPR19 ‚àö ‚àö 212.2 243.7 62.3 100.0 7.8 12.2 107.0 183.0\nSFCN (2019) CVPR19 ‚àö ‚àö 214.2 318.2 59.7 95.7 7.4 11.8 102.0 171.0\nPACNN (2019) CVPR19 ‚àö ‚àö 241.7 320.7 62.4 102.0 7.6 11.8 - -\nS-DCNet (2019) ICCV19 ‚àö ‚àö 204.2 301.3 58.3 95.0 6.7 10.7 104.4 176.1\nDSSI-Net (2019) ICCV19 ‚àö ‚àö 216.9 302.4 60.6 96.0 6.8 10.3 99.1 159.2\nBL (2019) ICCV19 ‚àö ‚àö 229.3 308.2 62.8 101.8 7.7 12.7 88.7 154.8\nRPNet (2020) CVPR20 ‚àö ‚àö - - 61.2 96.9 8.1 11.6 - -\nASNet (2020) CVPR20 ‚àö ‚àö 174.8 251.6 57.8 90.1 - - 91.5 159.7\nLibraNet (2020) ECCV20 ‚àö ‚àö 181.2 262.2 55.9 97.1 7.3 11.3 88.1 143.7\nAMRNet (2020) ECCV20 ‚àö ‚àö 184.0 265.8 61.5 98.3 7.0 11.0 86.6 152.2\nNoisyCC (2020) NeurIPS20 ‚àö ‚àö - - 61.9 99.6 7.4 11.3 85.8 150.6\nDM-Count (2020) NeurIPS20 ‚àö ‚àö 211.0 291.5 59.7 95.7 7.4 11.8 85.6 148.3\nGL(2021) CVPR21 ‚àö ‚àö - - 61.3 95.4 7.3 11.7 84.3 147.5\nSUA-Fully (2021) ICCV21 ‚àö ‚àö - - 66.9 125.6 12.3 17.9 119.2 213.3\nP2PNet (2021) ICCV21 ‚àö ‚àö 172.7 256.2 52.7 85.1 6.3 9.9 85.3 154.5\nBCCT (2021) arxiv21 ‚àö ‚àö - - 53.1 82.2 7.3 11.3 83.8 143.4\nCCTrans (ours) - ‚àö ‚àö 168.7 234.5 52.3 84.9 6.2 9.9 82.8 142.3\nYang et al. (2020)* ECCV20 \u0017 ‚àö - - 104.6 145.2 12.3 21.2 - -\nMATT (2021)* PR21 \u0017 ‚àö 355.0 550.2 80.1 129.4 11.7 17.5 - -\nTransCrowd (2021)* arxiv21 \u0017 ‚àö - - 66.1 105.1 9.3 16.1 97.2 168.5\nCCTrans (ours)* - \u0017 ‚àö 245.0 343.6 64.4 95.4 7.0 11.5 92.1 158.9\nTable 1: Comparison with state-of-the-art methods on UCF CC 50, ShanghaiTech Part A and B, and UCF-QNRF datasets. *\nrepresents the weakly-supervised method. Our method achieves new state-of-the-art results on several standard benchmarks.\nMethod Label Val Test\nL N MAE MSE MAE MSE NAE\nMCNN(2016) ‚àö ‚àö218.5 700.6 232.5 714.6 1.063\nCSRNet (2018) ‚àö ‚àö104.9 433.5 121.3 387.8 0.604\nCAN (2019) ‚àö ‚àö93.5 489.9 106.3 386.5 0.295\nBL (2019) ‚àö ‚àö93.6 470.4 105.4 454.2 0.203\nDM-Count (2020) ‚àö ‚àö70.5 357.6 88.4 388.6 0.169\nGL (2021) ‚àö ‚àö - - 79.3 346.1 0.180\nP2PNet (2021) ‚àö ‚àö - - 77.4 362.0 -\nBCCT (2021) ‚àö ‚àö53.0 170.3 82.0 366.9 0.164\nCCTrans (ours) ‚àö ‚àö38.6 87.8 69.3 299.4 0.135\nTransCrowd (2021)*\u0017 ‚àö 88.4 400.5 117.7 451.0 0.244\nCCTrans (ours)* \u0017 ‚àö 48.6 121.1 79.8 344.4 0.157\nTable 2: Comparisons on the validation and testing set of\nNWPU-Crowd. Our method achieves new state-of-the-art\nresults on this large benchmark with clear margins.\nVisualization\nWe show some visualization results in Figure 3. More analy-\nsis and results are in Figure 4 of the supplementary material.\nAblation Study\nWe make extensive ablation experiments on ST Part A and\nST Part B to evaluate the contribution of each component.\nFor simplicity, we use L and N to denote location and num-\nber label, respectively.\nPFA. We evaluate the performance of PFA using features\nfrom different stages in Table 3. The deeper layer learns\nhigh-level semantic features which are critical to the crowd\nFeatures in PFA Label ST Part A ST Part B\nL N MAE MSE MAE MSE\nF1\n‚àö ‚àö81.5 126.9 9.3 15.0\nF2\n‚àö ‚àö56.2 93.4 6.7 10.4\nF3\n‚àö ‚àö54.8 86.2 6.8 10.7\nF1+F2+F3(CCTrans) ‚àö ‚àö52.3 84.9 6.2 9.9\nTable 3: Feature aggregation from three stages achieves the\nbest performance.\ncounting task. F1, F2, and F3 are the output features from\nthree stages in the transformer-based backbone, and they are\nnamed from shallow layers to deep ones. It‚Äôs interesting that\nusing the feature from the last stage already outperforms\nmost of the state-of-the-art methods. Aggregating features\nfrom other stages can provide rich information which is lost\nin the last stage, thus achieving better performance.\nMDC. In Table 4, we evaluate the performance of MDC\nusing different columns with a shortcut path. Our shortcut\npath only uses a 1 √ó1 Conv layer to adapt the input fea-\nture maps of the following layer. It is included in all the\nexperiments by default. We use C1, C2, and C3 to denote\nthe three columns from up to bottom of the regression head\nin Figure 1. It‚Äôs interesting to see that using C1 or identity\nonly already outperforms most of the recent SOTAs. Proper\naggregation of all columns can further improve the perfor-\nmance, which validates the necessity of MDC.\nTo validate the rationality of our design for MDC, we also\ntry the different stacking strategies in Table 5. Hereidentity\nand C1+C2+C3 have the same meaning as in Table 4. We use\nImageGround-truthPrediction\nFigure 3: Visualization results of CCTrans on the most-\nchallenged NWPR-Crowd. Five images in different scenes\nwith varied scale, density, and illumination differences show\nthe Ô¨Åtting ability of CCTrans.\nColumn Number Label ST Part A ST Part B\nL N MAE MSE MAE MSE\nidentity ‚àö ‚àö55.8 89.8 6.9 11.3\nC1\n‚àö ‚àö55.9 93.3 6.8 11.5\nC2\n‚àö ‚àö57.9 95.2 7.5 12.3\nC3\n‚àö ‚àö56.5 91.3 7.3 11.8\nC1+C2+C3(CCTrans) ‚àö ‚àö52.3 84.9 6.2 9.9\nTable 4: Multi-scale columns achieve the best performance.\nStrategy Label STPart A STPart B Training Time(s)L N MAE MSEMAE MSE\nidentity ‚àö ‚àö55.8 89.8 6.9 11.3 18.6\nC‚Ä≤1 C‚Ä≤2 C‚Ä≤3\n‚àö ‚àö55.5 94.6 7.3 11.7 24.9\nC1 C2 C3\n‚àö ‚àö54.4 89.7 6.8 10.8 22.1\nC1+C2+C3(CCTrans)‚àö ‚àö52.3 84.9 6.2 9.9 19.3\nTable 5: Stacking DConv layers in width achieves the best\nperformance without much time-consuming.\nC1 C2 C3 to denote stacking the three columns in depth and\nC‚Ä≤\n1 C‚Ä≤\n2 C‚Ä≤\n3 to describe the dilation rates of the correspond-\ning column being set to 2. Note that the shortcut path is used\nby default. We can observe that stacking DConv layers with\nthe same dilation rate is bad for performance. Both indica-\nLoss Label ST Part A ST Part B\nL N MAE MSE MAE MSE\nLc+LOT +LTv\n‚àö ‚àö54.8 86.6 8.3 12.6\nL‚Ä≤\nd(CCTrans,Œª2=0.1) ‚àö ‚àö53.7 81.2 6.3 10.1\nLd(CCTrans,Œª2=1) ‚àö ‚àö52.3 84.9 6.2 9.9\nL1 \u0017 ‚àö 65.1 102.7 8.3 13.7\nLc(CCTrans*) \u0017 ‚àö 64.4 94.4 7.0 11.5\nTable 6: Our loss functions achieve better performance.\nMethod Label ST Part A ST Part B\nL CN MAE MSE MAE MSE\nBaseline+Ldm\n‚àö ‚àö 57.9 94.3 7.7 12.8\nBaseline+Ld\n‚àö ‚àö 57.4 89.6 7.6 11.6\nBaseline+Ld+PFA ‚àö ‚àö 54.8 86.2 6.9 11.3\nBaseline+Ld+PFA+MDC‚àö ‚àö 52.3 84.9 6.2 9.9\nTable 7: Pipeline component analysis of CCTrans.\ntors have decreased to varying degrees, compared with the\nidentity. This is possibly due to the above-mentioned grid-\nding effect caused by this stacking strategy, which has an ad-\nverse impact on the feature extraction of small-scale objects.\nStacking DConv layers with varied dilation rates can effec-\ntively solve this problem to improve the results. But stack-\ning too deep Conv layers to strengthen the local features\nseems not helpful for crowd counting. Besides, stacking\nthese blocks in depth also increases the training time by 15%\nto 20%. It is also worth noticing that MDC is lightweight\nwhich brings only less than 5% additional time cost.\nLoss functions. We also quantitatively evaluate the im-\nprovement from our loss design and report the result in Ta-\nble 6. As for the fully-supervised setting, it brings in 2.5 and\n2.1 lower MAE for Part A and B respectively. Compared\nwith the L1 loss, the design of smooth L1 decreases 0.7 and\n1.3 MAE for the ShanghaiTech A and B dataset under the\nweakly-supervised setting.\nSensitivity about Œª2. We investigate the inÔ¨Çuence of Œª2\nand report the result in Table 6. We useŒª2 = 1as our default\nsetting because it outperforms better than Œª2 = 0.1 with\nweak advantages. It indicates that the performance of our\nmethod is somewhat robust to the choice of Œª2.\nComponent analysis of CCTrans. Table 7 shows the\ncomponent analysis of our pipeline. As for the Shang-\nhaiTech dataset, both PFA and MDC signiÔ¨Åcantly contribute\nto the Ô¨Ånal MAE metric. Ld plays an important role in de-\ncreasing the MSE metric.\nRelated work\nCNN-based methods\nBefore the popularity of deep learning, researchers use tra-\nditional machine learning methods to solve crowd counting\n(Wojek et al. 2012; Viola and Jones 2001). But these meth-\nods are only adapted to sparse crowd scenes because these\ndetectors have poor performance to distinguish the occluded\nbodies. Then with the development of deep learning, CNN-\nbased designs are the de facto solutions for crowd counting.\nThese methods can be broadly classiÔ¨Åed into two categories,\ni.e. detection-based and density-based methods.\nDetection-based methods. Point-in-box-out (Liu et al.\n2019b) utilizes CNN to construct a detection model to pre-\ndict bounding boxes for each person, and the number of the\nboxes represents the number of people. But it can not deal\nwith the occlusion that existed in crowds. RGBD-Net (Lian\nand Li 2019) also constructs a detection model but uses the\nadditional depth information to detect the people occluded\nby others. However, it not only needs additional data but also\ngets limited performance in congested scenes.\nDensity-based methods. To achieve high performance in\ncongested scenes, density-based approaches are proposed to\nregress a density map from the input image (Pham et al.\n2015). The density map reÔ¨Çects the probability estimation\nof crowds. The value of each pixel represents the probability\nof people at that location. The sum of pixel values repre-\nsents the number of people. Since CNN has a strong ability\nto extract local features for accurate prediction in local re-\ngions, the results are improved a lot. The existing literature\nfocuses on more challenging problems (e.g. scale variance\nproblem and density problem). For the scale variance prob-\nlem, MCNN (Zhang et al. 2016) and Switch-CNN (Sam,\nSurya, and Babu 2017) design multi-column architectures to\nextract features in different scales. CFF also constructs dif-\nferent branches for different tasks to reÔ¨Åne the model. CAN\n(Liu, Salzmann, and Fua 2019) and PGCNet (Yan, Yuan, and\nZuo 2019) propose scale attention mechanisms, which use\ndifferent sizes of Gaussian kernels or convolution kernels to\ngenerate density maps for scale-varied regions. ADCrowd-\nNet uses an auxiliary network to generates an attention map\nfor the crowds in images. Besides, BL (Ma et al. 2019) de-\nsigns a novel loss function, which calculates the speciÔ¨Åc loss\nbased on the scale of crowds. For the density problem, AS-\nNet (Jiang, Zhang, and Xu 2020) divides the crowds into\ndifferent density levels and assigns the corresponding pre-\ndictions with different weights based on the density level.\nRAZNet (Liu, Weng, and Mu 2019) introduces additional lo-\ncalization branches to learn position information of crowds,\nwhich reduces the estimation errors in varied density re-\ngions. RPNet (Yang and Li 2020) uses transformations be-\ntween the elliptic coordinates and cartesian coordinates to\nalleviate the density variance and scale variance simultane-\nously. For the weakly-supervised method, MATT (Lei et al.\n2021) only uses a small amount of point-level annotations\nto make crowd counting. Yang et al.(Yang et al. 2020) di-\nrectly regress the crowd numbers of input images without\npoint-level annotations data.\nVision Transformer\nA standard transformer encoder of each stage contains L\nlayers of Multi-head self-attention (MSA) and Multi-layer\nPerceptron (MLP) blocks. And each layer l has layer nor-\nmalization (LN) and residual connections. For the speciÔ¨Åc\noutput of the (l ‚àí1)-th layer Zl‚àí1, the output of the l-th\nlayer Zl can be calculated as follows:\nZ‚Ä≤\nl = MSA(LN(Zl‚àí1)) +Zl‚àí1, (11)\nZl = MLP(LN(Z‚Ä≤\nl)) +Z‚Ä≤\nl. (12)\nSpeciÔ¨Åcally, MSA contains mindependent self-attention\n(SA) modules, as well as a re-projection operation W. The\ninputs of each independent SA are three parts, query ( Q),\nkey (K, and value (V). For the speciÔ¨Åc output of the(l‚àí1)-\nth layer Zl‚àí1, the calculation of SA in l-th layer Zl can be\ndeÔ¨Åned as follows:\nQ= Zl‚àí1WQ,K = Zl‚àí1WK,V = Zl‚àí1WV ,\nSA(Zl) =softmax( QKT\n‚àö\nd\n)V, (13)\nwhere WQ, WK, WV ‚ààRd√ód\nm are three learnable matri-\nces. The softmax function is applied for the input matrix.\nMLP consists of two fully-connected (FC) layers with one\nGELU activation function. The Ô¨Årst FC layer expends the\ndimension of embedding from dto 4d, and the second one\ncompresses the dimension back to d.\nMany recent works introduce the transformer into com-\nputer vision tasks. DETR (Carion et al. 2020) Ô¨Årstly utilizes\na CNN backbone to extract the semantic features and then\nuses the transformer blocks to regress the bounding boxes\nwith category information. ViT (Dosovitskiy et al. 2021) re-\ngards an image as a sequence of 16 √ó16 words and directly\nuses the transformer to perform classiÔ¨Åcations. DeiT (Tou-\nvron et al. 2020) uses proper regularization and achieves\nbetter performance than ViT using a much smaller dataset.\nSeveral pyramid transformers (Chu et al. 2021a; Wang et al.\n2021) are designed for dense prediction tasks. SETR (Zheng\net al. 2021) regards semantic segmentation as a sequence-to-\nsequence task with the transformer.\nThere are two concurrent researches TransCrowd (Liang\net al. 2021) and BCCT (Sun et al. 2021), which explore\nthe power of transformers for crowd counting. TransCrowd\nutilizes a ViT (Dosovitskiy et al. 2021) based the trans-\nformer decoder with an additional regression token for\nweakly-supervised crowd counting. BCCT designs a ViT\nbased model with various attention mechanisms for fully-\nsupervised crowd counting. Our method is a hybrid frame-\nwork that utilizes a pyramid vision transformer backbone\nand a simple convolutional decoder head. Our method out-\nperforms these two methods across several benchmarks in\nboth fully-supervised and weakly-supervised settings.\nConclusion\nIn this paper, we propose a simple pipeline for crowd count-\ning under both weakly and fully-supervised settings. This\npipeline contains four components: a pyramid vision trans-\nformer to better capture global context, a feature aggregation\nmodule to make full use of information from coarse to Ô¨Åne,\na regression head to provide multi-scale receptive Ô¨Åelds, and\ntailed loss functions to stabilize the training process. With-\nout bells and whistles, our method pushes the state-of-the-art\nfurther by clear margins across several popular benchmarks.\nWe hope our method can serve as a strong baseline for fur-\nther research or be ported to other counting tasks.\nReferences\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision, 213‚Äì229. Springer.\nChen, L.-C.; Papandreou, G.; and Kokkinos, I. 2018.\nDeepLab: Semantic Image Segmentation with Deep Con-\nvolutional Nets, Atrous Convolution, and Fully Connected\nCRFs. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 40(4): 834‚Äì848.\nChu, X.; Tian, Z.; Wang, Y .; Zhang, B.; Ren, H.; Wei, X.;\nXia, H.; and Shen, C. 2021a. Twins: Revisiting the Design of\nSpatial Attention in Vision Transformers. In NeurIPS 2021.\nChu, X.; Tian, Z.; Zhang, B.; Wang, X.; Wei, X.; Xia, H.;\nand Shen, C. 2021b. Conditional Positional Encodings for\nVision Transformers. arXiv preprint arXiv:2102.10882.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248‚Äì255. Ieee.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations.\nFang, Y .; Li, Y .; Tu, X.; Tan, T.; and Wang, X. 2020.\nFace completion with Hybrid Dilated Convolution. Signal\nProcessing-image Communication, 80: 115664.\nIdrees, H.; Saleemi, I.; Seibert, C.; and Shah, M. 2013.\nMulti-source Multi-scale Counting in Extremely Dense\nCrowd Images. In 2013 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, 2547‚Äì2554.\nIdrees, H.; and Tayyab, M. 2018. Composition Loss for\nCounting, Density Map Estimation and Localization in\nDense Crowds. In Proceedings of the European Conference\non Computer Vision (ECCV).\nJiang, X.; Zhang, L.; and Xu, M. 2020. Attention Scaling for\nCrowd Counting. In 2020 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 4706‚Äì4715.\nLei, Y .; Liu, Y .; Zhang, P.; and Liu, L. 2021. Towards using\ncount-level weak supervision for crowd counting. Pattern\nRecognition, 109: 107616.\nLi, Y .; Zhang, X.; and Chen, D. 2018. CSRNet: Dilated Con-\nvolutional Neural Networks for Understanding the Highly\nCongested Scenes. In 2018 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 1091‚Äì1100.\nLian, D.; and Li, J. 2019. Density Map Regression Guided\nDetection Network for RGB-D Crowd Counting and Local-\nization. In 2019 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 1821‚Äì1830.\nLiang, D.; Chen, X.; Xu, W.; Zhou, Y .; and Bai, X.\n2021. TransCrowd: Weakly-Supervised Crowd Counting\nwith Transformer. arXiv preprint arXiv:2104.09116.\nLiu, C.; Weng, X.; and Mu, Y . 2019. Recurrent Attentive\nZooming for Joint Crowd Counting and Precise Localiza-\ntion. In 2019 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 1217‚Äì1226.\nLiu, L.; Lu, H.; Zou, H.; Xiong, H.; Cao, Z.; and Shen, C.\n2020. Weighing Counts: Sequential Crowd Counting by Re-\ninforcement Learning. In European Conference on Com-\nputer Vision, 164‚Äì181.\nLiu, L.; and Qiu, Z. 2019. Crowd Counting With Deep\nStructured Scale Integration Network. In 2019 IEEE/CVF\nInternational Conference on Computer Vision (ICCV) ,\n1774‚Äì1783.\nLiu, N.; Long, Y .; Zou, C.; Niu, Q.; Pan, L.; and Wu, H.\n2019a. ADCrowdNet: An Attention-Injective Deformable\nConvolutional Network for Crowd Understanding. In 2019\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 3225‚Äì3234.\nLiu, S.; Huang, D.; and Wang, Y . 2018. Receptive Field\nBlock Net for Accurate and Fast Object Detection. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 404‚Äì419.\nLiu, W.; Salzmann, M.; and Fua, P. 2019. Context-Aware\nCrowd Counting. In 2019 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 5099‚Äì5108.\nLiu, X.; van de Weijer, J.; and Bagdanov, A. D. 2019. Ex-\nploiting Unlabeled Data in CNNs by Self-Supervised Learn-\ning to Rank. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 41(8): 1862‚Äì1878.\nLiu, X.; Yang, J.; and Ding, W. 2020. Adaptive mixture re-\ngression network with local counting map for crowd count-\ning. arXiv preprint arXiv:2005.05776.\nLiu, Y .; Shi, M.; Zhao, Q.; and Wang, X. 2019b. Point\nin, Box Out: Beyond Counting Persons in Crowds. In\n2019 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 6469‚Äì6478.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. arXiv preprint\narXiv:2103.14030.\nLoshchilov, I.; and Hutter, F. 2018. Decoupled Weight De-\ncay Regularization. In International Conference on Learn-\ning Representations.\nMa, Z.; Wei, X.; Hong, X.; and Gong, Y . 2019. Bayesian\nloss for crowd count estimation with point supervision. In\nICCV, 6142‚Äì6151.\nMeng, Y .; Zhang, H.; Zhao, Y .; Yang, X.; Qian, X.; Huang,\nX.; and Zheng, Y . 2021. Spatial Uncertainty-Aware Semi-\nSupervised Crowd Counting. In The IEEE International\nConference on Computer Vision (ICCV).\nPham, V .-Q.; Kozakaya, T.; Yamaguchi, O.; and Okada, R.\n2015. COUNT Forest: CO-V oting Uncertain Number of Tar-\ngets Using Random Forest for Crowd Density Estimation.\nIn 2015 IEEE International Conference on Computer Vision\n(ICCV), 3253‚Äì3261.\nSam, D. B.; Surya, S.; and Babu, R. V . 2017. Switching Con-\nvolutional Neural Network for Crowd Counting. In 2017\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 4031‚Äì4039.\nShi, M.; Yang, Z.; Xu, C.; and Chen, Q. 2019. Revisiting\nPerspective Information for EfÔ¨Åcient Crowd Counting. In\n2019 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 1‚Äì10.\nShi, Z.; Mettes, P.; and Snoek, C. G. 2019. Counting with\nfocus for free. In ICCV, 4200‚Äì4209.\nSindagi, V . A.; and Patel, V . M. 2017. Generating High-\nQuality Crowd Density Maps Using Contextual Pyramid\nCNNs. In 2017 IEEE International Conference on Com-\nputer Vision (ICCV), 1879‚Äì1888.\nSong, Q.; Wang, C.; Jiang, Z.; Wang, Y .; Tai, Y .; Wang, C.;\nLi, J.; Huang, F.; and Wu, Y . 2021. Rethinking Counting and\nLocalization in Crowds: A Purely Point-Based Framework.\nIn ICCV.\nSun, G.; Liu, Y .; Probst, T.; Paudel, D. P.; Popovic, N.; and\nVan Gool, L. 2021. Boosting Crowd Counting with Trans-\nformers. arXiv preprint arXiv:2105.10926.\nSzegedy, C.; and Ioffe, S. 2016. Inception-v4, Inception-\nResNet and the Impact of Residual Connections on Learn-\ning. In Proceedings of the Thirty-First AAAI Conference on\nArtiÔ¨Åcial Intelligence.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J¬¥egou, H. 2020. Training data-efÔ¨Åcient image trans-\nformers & distillation through attention. arXiv preprint\narXiv:2012.12877.\nViola, P.; and Jones, M. 2001. Robust real-time face detec-\ntion. In Proceedings Eighth IEEE International Conference\non Computer Vision. ICCV 2001, volume 57, 137‚Äì154.\nWan, J.; and Chan, A. B. 2020. Modeling Noisy Annotations\nfor Crowd Counting. In Advances in Neural Information\nProcessing Systems, volume 33, 3386‚Äì3396.\nWan, J.; Liu, Z.; and Chan, A. B. 2021. A Generalized Loss\nFunction for Crowd Counting and Localization. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 1974‚Äì1983.\nWang, B.; Liu, H.; Samaras, D.; and Nguyen, M. H. 2020.\nDistribution Matching for Crowd Counting. In Advances in\nNeural Information Processing Systems, volume 33, 1595‚Äì\n1607.\nWang, Q.; Gao, J.; Lin, W.; and Li, X. 2020. NWPU-crowd:\nA large-scale benchmark for crowd counting and localiza-\ntion. IEEE transactions on pattern analysis and machine\nintelligence, 43(6): 2141‚Äì2149.\nWang, Q.; Gao, J.; Lin, W.; and Yuan, Y . 2019. Learning\nFrom Synthetic Data for Crowd Counting in the Wild. In\n2019 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 8198‚Äì8207.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid Vision Trans-\nformer: A Versatile Backbone for Dense Prediction without\nConvolutions. In IEEE ICCV.\nWojek, C.; Dollar, P.; Schiele, B.; and Perona, P. 2012.\nPedestrian Detection: An Evaluation of the State of the\nArt. IEEE Transactions on Pattern Analysis Machine In-\ntelligence, 34(4): 743.\nXiong, H.; Lu, H.; Liu, C.; Liu, L.; Cao, Z.; and Shen, C.\n2019. From open set to closed set: Counting objects by spa-\ntial divide-and-conquer. In ICCV, 8362‚Äì8371.\nYan, Z.; Yuan, Y .; and Zuo, W. 2019. Perspective-Guided\nConvolution Networks for Crowd Counting. In 2019\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 952‚Äì961.\nYang, Y .; and Li, G. 2020. Reverse Perspective Network\nfor Perspective-Aware Object Counting. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 4374‚Äì4383.\nYang, Y .; Li, G.; Wu, Z.; Su, L.; Huang, Q.; and Sebe, N.\n2020. Weakly-Supervised Crowd Counting Learns from\nSorting Rather Than Locations. In ECCV (8), 1‚Äì17.\nZhang, Y .; Zhou, D.; Chen, S.; Gao, S.; and Ma, Y . 2016.\nSingle-Image Crowd Counting via Multi-Column Convolu-\ntional Neural Network. In 2016 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 589‚Äì597.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .;\nFu, Y .; Feng, J.; Xiang, T.; Torr, P. H.; and Zhang, L.\n2021. Rethinking Semantic Segmentation from a Sequence-\nto-Sequence Perspective with Transformers. In CVPR.\nAppendix\nDataset.\nWe evaluate our method across Ô¨Åve benchmarks, includ-\ning UCF CC 50 (Idrees et al. 2013), ShanghaiTech Part A\nand Part B (Zhang et al. 2016), UCF QNRF (Idrees and\nTayyab 2018), and NWPU-Crowd (Wang et al. 2020). These\ndatasets differ in image resolution ratios, quantities, crowd-\ning degree, and color spaces. The performance on these\nbenchmarks proves that CCTrans can deal with the crowds\nwell under the different situations.\nTraining setting and hyper-parameter.We use the train-\ning settings as described in the main paper. But for Shang-\nhaiTech Part B, we change the OT loss (Wang et al. 2020)\nto the Bayesian (Ma et al. 2019) loss and use the batch size\nof 16 in two GPUs for more accurate results. Because the\nlatter is more suitable to the scenes with dramatic scale and\ndensity variances. For UCF QNRF dataset, we also use re-\nplace the OT loss with the Bayesian loss but use a single\nA100 GPU for training. And we limit the longer side of the\nvalidation images up to 2400 pixels for this dataset. Note\nthat most of the other methods expand the limitation to 3096\npixels. And all the experiment results are the generally best\nresults on the ofÔ¨Åcial validation set after 1500 epochs train-\ning, which can be easily reproduced under the same training\nsettings.\nUCF CC 50. This dataset shows a lot of chal-\nlenges (Idrees et al. 2013). It randomly collects only\n50 gray images with serious perspective distortions from\nthe Internet. There are a total of 63,974 head annotations\nand the average number per image is 1280. Although this\nsmall dataset lacks the training data and color information,\nCCTrans still has great performance in the crowded scene.\nShanghaiTech Part A. It has 300 images and 182 im-\nages in the training and testing sets, respectively(Zhang et al.\n2016). These images are randomly crawled from the Inter-\nnet. And the number of people in these images varies largely\nwith a wide range. With more training data and color infor-\nmation, our results are improved a lot.\nShanghaiTech Part B. It has 400 training images and\n316 testing images, which are captured by the surveillance\ncameras in the streets of Shanghai(Zhang et al. 2016). And\nthese images have dramatic intra-scene scale and density\nvariations of crowds. But CCTrans can capture these vari-\nances easily because of the context modeling ability of\ntransformer-based backbone.\nUCF QNRF. This dataset contains 1,535 images with a\ntotal of 1,251,642 head annotations (Idrees and Tayyab\n2018). The images are divided into the training set with\n1,201 images and the testing set with 334 images, respec-\ntively. This dataset has much more annotated heads than cur-\nrently available crowd datasets, and most of the objects in\nthe picture are small in scale. Though this dataset is more\ncrowded with much more small-scale objects, our MDC\nclock can use the detail information complemented by PFA\nto extract the features of small-scale objects well.\nNWPU-Crowd. It is also a recently built large-scale and\nchallenging congested dataset (Wang et al. 2020). It consists\nof 5109 images crawled from the InterNet, elaborately anno-\ntating 2133375 people. And these images have varying de-\ngrees of scale, density, and crowd number differences. This\ndataset is the closest to the real distribution of the crowd.\nThe accurate results on this dataset suggest CCTrans is a\ngood option for real production.\nVisualization\nWe visualize the density map across three datasets in Fig 4.\nWe prove that CCTrans is capable of dealing with images\nfrom different sources and colorspaces. And our method also\nranks No.1 on the leaderboard of NWPU-Crowd.\nImageGround-truthPrediction\n(a) Results on UCF CC 50\nImageGround-truthPrediction\n (b) Results on ShanghaiTech Part A\nImageGround-truthPrediction\n(c) Results on ShanghaiTech Part B\nImageGround-truthPrediction\n(d) Results on UCF QNRF\nImageGround-truthPrediction\n (e) Results on NWPU-Crowd\nFigure 4: Visualization results of CCTrans on benchmark datasets. The left column shows the original images; the medium\ncolumn displays the ground-truth density maps while the right column indicates our generated density maps."
}