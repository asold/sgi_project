{
  "title": "Joint Pixel and Frequency Feature Learning and Fusion via Channel-Wise Transformer for High-Efficiency Learned In-Loop Filter in VVC",
  "url": "https://openalex.org/W4387490618",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5036691509",
      "name": "Birendra Kathariya",
      "affiliations": [
        "University of Missouri–Kansas City"
      ]
    },
    {
      "id": "https://openalex.org/A5100380625",
      "name": "Zhu Li",
      "affiliations": [
        "University of Missouri–Kansas City"
      ]
    },
    {
      "id": "https://openalex.org/A5083485280",
      "name": "Geert Van der Auwera",
      "affiliations": [
        "Qualcomm (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2147000487",
    "https://openalex.org/W2146395539",
    "https://openalex.org/W3202918664",
    "https://openalex.org/W3153323090",
    "https://openalex.org/W2123113293",
    "https://openalex.org/W2017801928",
    "https://openalex.org/W2064674198",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W1885185971",
    "https://openalex.org/W3038753503",
    "https://openalex.org/W4287020683",
    "https://openalex.org/W3000775737",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W2985577924",
    "https://openalex.org/W3202040256",
    "https://openalex.org/W2965217508",
    "https://openalex.org/W2966431607",
    "https://openalex.org/W4224130636",
    "https://openalex.org/W4292829111",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2964077901",
    "https://openalex.org/W2942071819",
    "https://openalex.org/W2519021537",
    "https://openalex.org/W6799637880",
    "https://openalex.org/W4312396555",
    "https://openalex.org/W6766847852",
    "https://openalex.org/W2963494934",
    "https://openalex.org/W3090654092",
    "https://openalex.org/W3166014221",
    "https://openalex.org/W2552465432",
    "https://openalex.org/W2969260367",
    "https://openalex.org/W4224294196",
    "https://openalex.org/W2892020820",
    "https://openalex.org/W2999132674",
    "https://openalex.org/W2477177239",
    "https://openalex.org/W2963343778",
    "https://openalex.org/W2968397583",
    "https://openalex.org/W2970872821",
    "https://openalex.org/W4367146862",
    "https://openalex.org/W3080437795",
    "https://openalex.org/W3081342944",
    "https://openalex.org/W6725739302",
    "https://openalex.org/W3034782636",
    "https://openalex.org/W3043029414",
    "https://openalex.org/W3172995745",
    "https://openalex.org/W3164064764",
    "https://openalex.org/W4316660221",
    "https://openalex.org/W4317555198",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2915130236",
    "https://openalex.org/W4220715488",
    "https://openalex.org/W3104949790",
    "https://openalex.org/W3169596802",
    "https://openalex.org/W2604272474",
    "https://openalex.org/W3189180948",
    "https://openalex.org/W2963446712"
  ],
  "abstract": "Block-based video codecs such as Versatile Video Coding (VVC)/H.266, High Efficiency Video Coding (HEVC)/H.265, Advanced Video Coding (AVC)/H.264 etc. inherently introduces compression artifacts. Although these codecs have in-loop filters to correct these distortions, they are not always effective due to the complexity of the noise. Recently, deep-learning approaches emerged as a promising solution for in-loop filtering. However, most of the previous approaches were designed solely for learning from images and neglected the high-frequency signals present in the reconstructed video frames. Furthermore, some previous methods employed a multi-level feature-extraction and feature-fusion strategy to enhance performance. However, they utilized complex feature-extractors while relying on naive feature-fusion methods. In this article, we propose a novel framework called <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TSF-Net</i> , which jointly learns from both the pixel (spatial) and frequency-decomposed information and through powerful capability of a channel-wise transformer, it fuses both these information to improve performance. Our approach deviates from previous approaches by employing a simple feature-extractor coupled with an advanced transformer-based feature-fusion module. Simultaneously, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TSF-Net</i> introduces a few fundamental modifications in the multi-head self-attention module of the channel-wise transformer to make it computationally efficient. Our experimental results show that the proposed <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TSF-Net</i> achieves a Bjøntegaard Delta (BD) - bitrate saving of up to 10.258% for the luma (Y) component under all-intra (AI) profile outperforming the VVC baseline and other state-of-the-art methods. Moreover, the proposed <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TSF-Net</i> with an efficient channel-wise transformer is twice as efficient as <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TSF-Net</i> with a vanilla channel-wise transformer.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 1\nJoint Pixel and Frequency Feature Learning and\nFusion via Channel-wise Transformer for\nHigh-Efficiency Learned In-Loop Filter in VVC\nBirendra Kathariya, Student Member, IEEE, Zhu Li, Senior Member, IEEE,\nGeert Van der Auwera, Senior member, IEEE\nAbstract—Block-based video codecs such as Versatile\nVideo Coding (VVC)/H.266, High Efficiency Video Coding\n(HEVC)/H.265, Advanced Video Coding (A VC)/H.264 etc. inher-\nently introduces compression artifacts. Although these codecs\nhave in-loop filters to correct these distortions, they are not\nalways effective due to the complexity of the noise. Recently,\ndeep-learning approaches emerged as a promising solution for\nin-loop filtering. However, most of the previous approaches\nwere designed solely for learning from images and neglected\nthe high-frequency signals present in the reconstructed video\nframes. Furthermore, some previous methods employed a multi-\nlevel feature-extraction and feature-fusion strategy to enhance\nperformance. However, they utilized complex feature-extractors\nwhile relying on naive feature-fusion methods. In this article,\nwe propose a novel framework called TSF-Net, which jointly\nlearns from both the pixel (spatial) and frequency-decomposed\ninformation and through powerful capability of a channel-wise\ntransformer, it fuses both these information to improve per-\nformance. Our approach deviates from previous approaches by\nemploying a simple feature-extractor coupled with an advanced\ntransformer-based feature-fusion module. Simultaneously, TSF-\nNet introduces a few fundamental modifications in the multi-\nhead self-attention module of the channel-wise transformer to\nmake it computationally efficient. Our experimental results show\nthat the proposed TSF-Net achieves a Bjøntegaard Delta (BD)\n- bitrate saving of up to 10.258% for the luma (Y) component\nunder all-intra (AI) profile outperforming the VVC baseline and\nother state-of-the-art methods. Moreover, the proposed TSF-Net\nwith an efficient channel-wise transformer is twice as efficient as\nTSF-Net with a vanilla channel-wise transformer.\nIndex Terms—in-loop filters, versatile video coding, convolu-\ntional neural network, channel-wise transformer, feature fusion\nI. I NTRODUCTION\nA\nDV ANCEMENTS in the internet, computing, and display\ntechnologies have revolutionized the way we consume\nvideo content. Today, the majority of internet traffic is domi-\nnated by video data from various sources such as streaming,\nconferencing, surveillance, and more. Video compression tech-\nnology is the cornerstone of these video-processing applica-\ntions, enabling the efficient transmission and storage of large\namounts of video data. Over the years, with the continuous\nBirendra Kathariya and Zhu Li are with the Department of Computer\nScience and Electrical Engineering, University of Missouri-Kansas City, MO\n64110 USA (email: bkkvh8@umsystem.edu; zhu.li@ieee.org).\nGeert Van der Auwera is with Qualcomm Technologies Inc., San Diego,\nCA 92121 USA (e-mail: geertv@qti.qualcomm.com).\nManuscript received March 23, 2023\nFig. 1. Proposed TSF-Net as an In-loop Filter in VVC architecture.\nimprovement of video capture and processing technology, sev-\neral video compression standards have been developed to meet\nthe increasing demand. These standards include Advanced\nVideo Coding (A VC)/H.264 [1], High Efficiency Video Coding\n(HEVC)/H.265 [2], and Versatile Video Coding (VVC)/H.266\n[3] among others.\nCurrently, Versatile Video Coding (VVC)/H.266 [3] has\nbeen established as the next generation video coding standard\nby the Joint Video Experts Team (JVET), with over 30% per-\nformance improvement over its predecessor, High Efficiency\nVideo Coding (HEVC)/H.265. VVC was developed to meet\nthe quality-of-experience (QoE) demands of end-users as the\nresolution of displays (and videos) increased, supporting ultra-\nhigh definition (UHD) videos up to 16K. Secondly, VVC\nis designed to be versatile, providing coding and transport\nsupport for a wide range of applications and content types,\nsuch as conventional video streaming, screen content optimiza-\ntion, 360◦ video for AR/VR, live broadcasting, and ultra-low\nlatency applications.\nNonetheless, VVC is not completely a new technology but\nrather an evolution of HEVC, wherein most of the technologies\nfrom HEVC are further improved, and many new coding tools\nare invented to account for larger resolutions and different\ncontents. This implies that VVC is still a block-based hybrid\nvideo coding standard, where a picture is partitioned into\nsmaller, non-overlapping blocks that go through operations\nlike prediction, transform, quantization, etc. in an independent\nmanner. These blocks are named based on the operations\napplied, such as prediction unit (PU), coding unit (CU),\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 2\ntransform unit (TU), etc.\nLike HEVC, VVC also suffers from inherent com-\npression artifacts such as blocking, ringing, blurring, and\nmosquito noise due to block-based operations. In intra-\nframes (I-frames), the transform coefficients from neighboring\nblocks typically undergo quantization differently. Similarly,\nin prediction-frames (P-frames) and bidirectional prediction-\nframes (B-frames), the neighboring blocks are likely to use\ndifferent motion vectors, resulting in discontinuities at the\nblock boundary. Simultaneously, the quantization applied to\ntransform coefficients and prediction residues at the block level\nresults in ringing, blurring, and other noise. If these compres-\nsion artifacts are not handled properly, they can degrade not\nonly the subjective and objective quality of the current frame\nbut also propagate to successive frames if referenced by the\nP- or B-frame.\nVVC, similar to HEVC, is equipped with in-loop filters\n[4] that correct compression artifacts. The in-loop filters in\nVVC include three types of traditional filters, namely deblock-\ning filter (DBF) [5], sample adaptive offset (SAO) [6], and\nadaptive loop filter (ALF) [7]. These filters are applied to\na reconstructed picture one after another in the given order.\nDBF is responsible for suppressing the blocking artifacts at\nblock boundaries, while SAO and ALF filters are designed\nto remove artifacts caused by quantization. Nonetheless, these\nfilters’ quality restoration capability is sub-optimal and leaves\na large room for improvement.\nIn recent times, deep learning has achieved numerous\nbreakthroughs in the field of computer vision. Deep learning\nmethods, such as convolutional neural networks (CNNs) [8]\nand vision transformers [9], have already made great strides in\napplications such as image super-resolution [10]–[12] and im-\nage restoration tasks such as denoising, deblurring, dehazing,\nand image enhancement [13]–[20]. CNN-extracted features are\nlocally correlated, whereas transformers can extract features\nwith long-range correlation. Recently, hybrid architectures that\nemploy both CNN and transformers [15], [21] have emerged.\nThese hybrid models are capable of extracting deep features\nwith better super-resolving and image-restoration power.\nTo improve the feature representation power of CNN, re-\nsearchers are now proposing complex architectures [16]–[18]\nthat operate at multi-scale feature-size. With this “coarse-to-\nfine” strategy, features of various spatial sizes are extracted\n(multi-scale), which are then progressively fused from coarser\nto finer scales until the original scale is achieved. Other\napproaches inspired by Inception-like-network [22] utilize ker-\nnels of variable sizes to derive features with various receptive-\nfield and fuse them to enrich the feature semantics [23].\nPrior works [24]–[29] have also demonstrated the benefit of\nutilizing frequency decomposition (Discrete Cosine Transform\n(DCT) and Wavelet Transform) in conjunction with CNN to\nimprove feature representation for learning low-level vision\ntasks. DCT and Wavelet-transform decompose a signal (image)\ninto multiple frequency bands that can be utilized as crucial\nprior information for image restoration tasks where high-\nfrequency contents such as noises need to be discriminated\nfrom the signal.\nInspired by the success of CNN in image restoration tasks,\nresearchers have successfully implemented CNN to fill the\nperformance gap of the in-loop filter of HEVC and VVC.\nPlenty of works have also implemented CNN as a post-\nprocessing block in both HEVC and VVC to improve the\nfinal picture quality. A few works have even utilized CNN\nto improve or replace conventional coding tools like intra-\nprediction [30], inter-prediction [31], etc. Researchers have\nalso trained CNN-based auto-encoder-style neural networks in\nan end-to-end fashion for applications like image compression\n[32] and video compression [33]. In such approaches, images\nand video frames are represented by features at the bottleneck\nlayer of an auto-encoder. Most of the learning-based in-\nloop filter works are largely inspired by CNN-based image\nrestoration works.\nIn this article, we design a hybrid model consisting of both\nconvolutional and transformer layers for feature extraction\nand feature fusion, respectively, as an in-loop filter for VVC.\nWe refer it as Transformer-based Spatial and Frequency-\nDecomposed Feature Fusion Network (TSF-Net) . Moreover,\nwe not only use pixel information but also frequency-\ndecomposed (DCT’ed image) information to improve the\nfeature representation capability of the model, which was\nlargely ignored by prior articles while designing a learned\nin-loop filter. We treat each DCT coefficient as a separate\nchannel, thereby creating DC and AC channels. Convolutional\nlayers are then utilized to extract deep features from pixel and\nDCT’ed information, and a transformer layer is utilized to fuse\nDCT features into pixel features.\nThe transformer utilized in this work has a self-attention\nmodule designed to perform attention along the channel where\neach channel is treated as a token. We prepare pixel input\nby pixel-unshuffling an image, which distributes local pixels\nalong the channel. Similarly, DCT’ed input is prepared by\nseparating DCT coefficients along the channel. This requires a\nproper mechanism to extract features from local pixels as well\nas DC and AC coefficients distributed along the channel. Thus,\nwe consider a transformer layer with similar functionality\ncalled Spectral-wise Multi-head Self-attention (S-MSA) from\nthe MST++ [34] article. Nonetheless, we fundamentally re-\ndesign the S-MSA layer to accomplish feature fusion between\npixel (spatial) and DCT’ed (frequency-decomposed) features\nefficiently.\nThe contributions of this article are summarized below.\n• We propose TSF-Net, a learned in-loop filter for VVC,\nwhich learns from both spatial (pixel) and frequency-\ndecomposed information and offers state-of-the-art per-\nformance under all-intra (AI) profile for luma (Y) com-\nponent.\n• Our proposed TSF-Net is highly scalable as it can be\nconstructed simply by cascading multiple Residual Fu-\nsion Block (RFB) similar to Residual Blocks in ResNet\narchitectures. Each RFB bundles together convolution-\nbased feature processing and transformer-based feature\nfusion operation.\n• We utilize S-MSA from MST++, a channel-wise trans-\nformer layer, and redesign it for feature fusion. We\nfurther propose efficient S-MSA (E-SMSA) by making\nfundamental modifications in the self-attention module.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 3\nFig. 2. Luma and its side-information from VVC utilized as input to the TSF-Net.\nFig. 3. Frequency-decomposed input generation pipeline. First 4-point 2D-DCT is applied on a N × N image and then it is pixel-unshuffled on 4x4 block\nto get 16-channel (N/4) × (N/4) frequency-decomposed image.\nE-SMSA improves the feature processing time of TSF-\nNet nearly by twice.\n• We utilize large patch inputs and while pixel-unshuffle\nthem, we are able to decrease the feature processing\ntime and increase the receptive field. Additionally, RFB\nprocesses spatial and frequency-decomposed features in\nparallel, contributing to a further reduction in feature\nprocessing time.\nThe rest of the article is organized as follows. In Section\nII, we go over the prior in-loop filtering technologies related\nto classical and deep-learning approaches. In Section III, we\nlay out the details of our proposed architecture and related\nmethods. Section IV describes the implementation and training\ndetails of our method as an in-loop filter. In Section V, we\npresent the findings of the experiments processes. Finally, we\nconclude our work and its findings in Section VI.\nII. R ELATED WORK\nA. Traditional In-loop Filters\nHEVC adopts two filters: deblocking filter (DBF) [5] and\nSample Adaptive Offset (SAO) [6], as in-loop filters. De-\nblocking filter is first applied to the reconstructed sample\nwhich attenuates the discontinuities at the block boundary.\nThe deblocked picture is then processed by the SAO to\nfurther mitigate the ringing artifacts and corrects the signal\n(image) by applying the offsets to the pixels. The offset values\nwhich are calculated based on pixels’ statistics are sent to the\ndecoder. VVC keeps both DBF and SAO filters, however, adds\nthree more filters: Luma Mapping Chroma Scaling (LMCS)\n[35], Adaptive Loop Filter (ALF) [7] and, Cross-Component\nAdaptive Loop Filter (CC-ALF) [36]. LMCS is implemented\nto improve coding efficiency through two processes: luma\nmapping (LM) and chroma scaling (CS). LM remaps the\nluma code values within the complete codeword range at a\nspecific bit-depth. CS compensates for the impact of luma\nmapping on the relative chroma coding bit costs. ALF is\ndesigned to minimize the mean square error (MSE) between\nthe original and reconstructed picture. The idea of ALF is to\nclassify non-overlapping blocks based on their local sample\ngradient and apply a specific Wiener-based filter among many\nto improve signal fidelity. The type of filter used is signaled\nin the bitstream. CC-ALF performs the Wiener-based filtering\ncorrection on the chroma sample utilizing the co-located\nand corrected luma sample. These filters are primarily hand-\ncrafted and statistical-based. While ALF has a slight learning\ncapability, its efficiency is very much limited. This leaves a\nlarge room for improvement to exploit.\nB. Learned In-loop Filters\nA plethora of articles already exist that successfully im-\nplements convolution neural networks (CNN) as both in-loop\nfilters and post-processing block in HEVC and VVC. Recently,\nthe powerful capability of vision transformers (ViT) is also\nbeing exploited in the image and video compression domain.\nIn this section, we will discuss some notable previous learning-\nbased approaches for HEVC and VVC.\n1) Learning-based approaches for HEVC: One of the early\nworks in learned in-loop filters is IFCNN [37]. IFCNN is an\nSRCNN [10] (an image super-resolution) based architecture\nimplemented as an in-loop filter in HEVC by replacing SAO.\nThis 3-layer CNN architecture outperformed HEVC by up\nto 2.8% and 2.6% in low-delay(LD) and random-access(RA)\nconfigurations. Similarly, [38] proposed MIF-Net, a multi-\nframe in-loop filter for HEVC by replacing both DBF and\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 4\nSAO. MIF-Net includes a reference frame selector (RFS) that\nselects the reference frames with the best quality and content\nsimilarity for the current unfiltered frame. Then, MIF-Net\nutilizes both spatial and temporal information across multiple\nframes to enhance the current unfiltered frames. With this\nstrategy, MIF-Net was able to outperform the HEVC baseline\nand other state-of-arts approaches. Likewise [39] proposed\nSqueeze-and-Excitation CNN (SEFCNN), a switchable in-\nloop filter positioned in parallel to HEVC’s in-loop filter.\nBy implementing two subnets: Feature Extraction (FEX) and\nFeature Enhancement (FEN), SEFCNN was able to learn very\nhigh-quality features. FEX utilized convolutional layers to\nextract deeper features, while FEN employed squeeze-and-\nexcitation layers to enhance features by enabling channel\ninteraction. SEFCNN achieved average of 9.96%, 8.04%, and\n7.60% BD-Rate saving on AI, LDP and RA configurations,\nrespectively, compared to HEVC. Further, [40] proposed a\nRecursive Residual Convolutional Network (RRCNN) in-loop\nfilter for HEVC. In RRCNN, the authors demonstrated the\nincreased capability of CNN by proposing multi-path residual\nand recursive learning. With recursive learning, RRCNN uti-\nlized the same layers repeatedly for deeper feature extraction.\nUnlike previous works, this work trained the same model for\nmultiple bitrates achieving an average bitrate savings of up to\n8.7% on intra-frames.\nInspired by RRCNN, [41] proposed Lightweight Multiat-\ntention Recursive Residual CNN-based In-loop Filter (LMA-\nRRCNN) as an alternate to HEVC’s in-loop filter. LMA-\nRRCNN, similar to RRCNN, utilized parameter sharing and\nfeature reuse to reduce model parameters and increase model\ndepth, while simultaneously learning features at multiple spa-\ntial scales and frequently fusing them. LMA-RRCNN was also\nable to handle video compressed at various bitrates and of\nvarious frame-types via single model while achieving excellent\nbitrate savings upto 13.7% and 11.87% in AI and RA profiles,\nrespectively. Another work [42], proposed Frame-wise filtering\nfor Quality Enhancement based on CNN (FQE-CNN), which\nalso adopted the multi-level feature extraction and feature\nfusion approach similar to LMA-RRCNN. FQE-CNN utilized\nan inception-like residual learning block (IResLB) to extract\nfeatures and occasionally aggregated the features between\nspatial levels through concatenation and convolution. With this\nmulti-level feature learning strategy, FQE-CNN achieved supe-\nrior performance compared to the HEVC baseline, surpassing\nit by 11.1% in all-intra (AI) configurations.\n2) Learning-based approaches for VVC: With the emer-\ngence of VVC as the latest video coding framework, a plethora\nof fascinating new research has been generated on the topic\nof learned in-loop filters. For example, [43] proposed a dense\nresidual convolutional neural network (DRN) based in-loop\nfilter (DRNLF) for VVC and placed it after DFB and before\nSAO and ALF. By utilizing dense shortcuts in DenseNet [44]\nand feature reuse, DRNLF achieved 1.52%, 1.45%, and 1.54%\nBD-rate saving in AI, RA, and LD coding configurations.\nSimilarly, [45] proposed a multi-gradient convolutional neural\nnetwork-based in-loop filter (MGNLF) for VVC, where the\nnetwork also considers divergence and second-derivative of\nthe frame along with the frame itself to improve the quality\nof the frame. By utilizing contour and structural information\nof a frame, MGNLF was able to reduce BD-rate savings up\nto 3.29% on average. MFRNet [46], however, is proposed as\nboth a post-processing and an in-loop filter on both HEVC and\nVVC. As an in-loop filter, MFRNet was placed after SAO (for\nHEVC) and ALF (for VVC). MFRNet utilized highly dense\nshortcuts for multi-level feature reuse and achieved coding\ngain (BD-rate VMAF) of up to 16% and 5.1% for HEVC\nand VVC, respectively. Similarly, [47] proposed Variable CNN\n(VCNN), an in-loop filter specifically designed to handle\nvideos compressed at different quantization parameters (QPs)\nand frame-types (FT) using a single model. VCNN incor-\nporated a QP attention module (QPAM) and a FT attention\nmodule (FTAM) in its residual block (RB). These modules\nrecalibrated the features through channel-attention, allowing\nVCNN to adapt to a wide range of QPs and FTs. VCNN\noutperformed VVC baseline by upto 3.63%, 4.36%, 4.23%,\n3.56% in AI, LDP, LD and RA configurations. Likewise, [48]\nproposed MSCNN, in which two U-Net like architectures\nare utilized to extract features from reference and current\nframes while aggregating both feature through gated-fusion.\nMSCNN achieved 3.762% bitrate saving in AI configuration.\nLikewise, as a response to the call for proposal (CfP) on\n“Neural networks on video coding” [49], JVET received\n[50] as a contribution from ByteDance. This work proposed\nDeep In-loop filter with Adaptive Model selection (DAM)\nand is currently adopted as part of the standard for further\nexploration.\nInterestingly, all previous works ignored high-frequency\ninformation while designing an in-loop filter. Nonetheless, [50]\nutilized both spatial(pixel) and frequency decomposed (DCT)\ninformation and designed a CNN architecture that extracted\nboth pixel and DCT features while fusing them at multiple\nstages. The proposed work, on average, achieved a 9.7% BD-\nrate reduction compared to the VVC baseline. In this work,\nconvolution-based feature fusion between pixel and DCT\nfeatures was proposed. Later, the authors extended the work\nby proposing MSTFNet [51] where convolution-based feature\nfusion was replaced by transformer-based feature fusion and\nsignificantly improved the BD-rate savings. MSTFNet utilized\na transformer with channel-wise self-attention to aggregate\npixel and DCT features at multiple stages. This work demon-\nstrated a successful implementation of a transformer along\nwith CNN as an in-loop filter.\nTo improve the performance of learned in-loop filters,\nresearchers extended single-level architecture to a more ad-\nvanced multi-level architecture, incorporating feature inter-\naction among these levels. However, previous methods [42]\n[41] primarily focused more on designing a complex feature-\nextractor at each levels but relied on naive feature-fusion\napproaches. In this article, we take a different approach\nby utilizing a simple feature-extractor while implementing\ntransformers-based a more advanced feature-fusion module.\nFurthermore, our method involves fusion of spatial and\nfrequency-decomposed features. This is in contrast to previous\napproaches where feature-fusion occurs only between spatial-\nfeatures but at different scales. Our current work (TSF-Net) can\nbe considered an extension of our prior work MSTFNet [51].\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 5\nFig. 4. Overall architecture of our proposed Transformer-based Spatial and Frequency-decomposed Feature Fusion Network (TSF-Net) . TSF-Net is divided\ninto 3 parts: head, body and tail. Reconstructed picture, residue and partition information from VVC is processed to generate five different types of inputs:\n3 spatial inputs and 2 frequency-decomposed inputs. The Head of TSF-Net transforms spatial and frequency-decomposed inputs into two types of features:\nspatial and frequency-decomposed features. The body includes a series of Residual Fusion Blocks (RFB) which take both types of features and fuse them into\nthe spatial features. The spatial feature output from the body is finally processed by the tail to produce a clean reconstructed picture.\nAlthough both employ spatial and frequency-decomposed\ninputs and a transformer-based feature aggregation scheme\n(S-MSA), we drastically reduce the number of trainable pa-\nrameters in TSF-Net by revamping the overall architecture\ncompletely. In contrast to MSTFNet, which only fuses features\nat 3 stages, TSF-Net fuses feature at 20 stages using 20 residual\nfusion blocks (RFB). Furthermore, TSF-Net utilizes efficient\nS-MSA that is twice as efficient as the vanilla S-MSA used in\nMSTFNet.\nIII. P ROPOSED METHOD\nIn the decoder of VVC architecture, we integrate our learned\nin-loop filter, Transformer-based Spatial and Frequency-\nDecomposed Feature Fusion Network (TSF-Net), by replacing\nall the modules: DBF, SAO, ALF (CCALF) of the in-built in-\nloop filter. This integration is illustrated in figure 1. Unlike\nthe in-built in-loop filter of VVC, TSF-Net does not utilize\nany filter control data and hence, it is not communicated to\nthe entropy encoder. In this work, we consider three types\nof information: reconstructed picture, partition information,\nand residual image as input to the TSF-Net. The reconstructed\npicture serves as the primary input while partition and residual\nimage are crucial side information that convey information\nrelated to the block boundary and prediction error respectively.\nThe proposed TSF-Net is presented in figure 4. It receives five\ndistinct types of inputs: three spatial inputs and two frequency-\ndecomposed inputs, generated from the reconstructed picture,\npartition, and residual image. In this section, we will first\ndescribe the methods we adopted to prepare the inputs for\nTSF-Net, followed by the details of TSF-Net itself.\nA. Generation of Spatial and Frequency-Decomposed Inputs\nThe proposed TSF-Net utilizes three types of information:\nreconstructed ( I), partition ( P), and residual ( R) image as\ninputs. An example of these input images are shown in figure\n2. The reconstructed picture ( I) is an unfiltered picture gener-\nated after addition of prediction image and residual image. The\nresidual image ( R) is the difference image of the original and\nthe predicted image. The partition ( P) image is the coding-\nunit (CU) level block partition information. In this study, we\napply TSF-Net only on the luma (Y) component. Therefore,\nI represents the luma component of the reconstructed picture,\nwhereas R and P correspond to the associated residue and\npartition information respectively. Let 2H and 2W be the\nheight and width of these I, R, and P images.\nIn the next step, three spatial inputs are created from I, R,\nand P images, and two frequency-decomposed inputs from I\nand R images, respectively. The I and R images are pixel-\nunshuffled on a 2 × 2 block to create two spatial inputs,\nIs ∈ R4×H×W and Rs ∈ R4×H×W , respectively. Here, pixel-\nunshuffling on a 2 × 2 block converts an 2H × 2W image to\na 4-channel H ×W image. Since the boundary information is\njust a single pixel wide, the pixel-unshuffling operation would\nbreak the structure in the partition image P. Therefore, the\npartition image P is fed to the network unchanged, and thus\nspatial input Ps = P ∈ R1×2H×2W . Similarly, two frequency-\ndecomposed inputs are generated by applying a 4-point 2D\ndiscrete cosine transform (DCT) on the reconstructed I and\nresidual R images. The resulting image has a DC coefficient\nand 15 AC coefficients in every non-overlapping 4 × 4 block.\nIn the next step, pixel-unshuffling is applied on the non-\noverlapping 4 × 4 blocks of the 2H × 2W sized DCT’ed\nimage to obtain a 16 ×(H/2) ×(W/2) image, where the first\nchannel is a DC-image and rest of the 15 channels are the AC-\nimages. Let If ∈ R16×(H/2)×(W/2), Rf ∈ R16×(H/2)×(W/2)\nbe the pixel-unshuffled version of the DCT’ed image of I and\nR, respectively. We refer to them as frequency-decomposed\ninputs. The steps to obtain frequency-decomposed inputs are\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 6\nFig. 5. (a) Body of TSF-Net constitutes a cascade of Residual Fusion Block (RFB) with a feature-level skip connection. (b) The internal details of RFB.\nIt includes two branches with a set of “conv→relu→conv” block for extracting deeper features from spatial and frequency-decomposed features, a cross-\nconnection through transposed convolution (T-conv) for upscaling frequency-decomposed feature and an E-SMSA transformer layer for efficient feature fusion\nof concatenated spatial and frequency-decomposed features. Both branches include a skip connection making the fusion block essentially a residual block.\nshown in figure 3.\nThe spatial inputs Is and Rs are normalized by dividing\nthem with the peak value of 2b − 1, where b is the number\nof bits used to represent I. Ps is a binary image with the\nvalue of 1 at the boundary and 0 everywhere else, so it does\nnot require normalization. In the DCT domain, however, the\nDC coefficient has the largest value and AC values gradually\ndecrease. Thus, frequency-decomposed inputs If and Rf\nare normalized channel-wise using “min-max” normalization\nmethod. The channel-wise minimum and maximum values\n{minI, maxI} ∈R16×1×1 and {minR, maxR} ∈R16×1×1\nof If and Rf respectively, are first computed from the training\ndataset. For this, we utilize 25000 randomly cropped co-\nlocated patches of size 256 × 256 from I and R images.\nThe same channel-wise “min-max” values obtained from\nthe training dataset are used for normalizing the frequency-\ndecomposed inputs during inference.\nB. Description of TSF-Net architecture\nThe overall architecture of TSF-Net is presented in figure\n4. The proposed network is slightly inspired by EDSR [52]\nas it includes both global and feature-level skip connections.\nTSF-Net can be better explained by dividing it into three parts:\nhead, body, and tail.\n1) Head: The head of TSF-Net comprises\nfour “conv→PReLU→conv” blocks and one\n“conv→PReLU→conv→avg-pool” block. The two\nspatial inputs Is and Rs and two frequency-decomposed\ninputs If and Rf are processed through their\nrespective “conv→PReLU→conv” blocks. We use the\n“conv→PReLU→conv→avg-pool” block to process\nthe partition input Ps. Since Ps is at its original size\n2H × 2W, the average-pooling layer ( “avg-pool”) of\n“conv→PReLU→conv→avg-pool” reduces it to H ×W. The\n“conv→PReLU→conv” and “conv→PReLU→conv→avg-\npool” blocks transform spatial and frequency-decomposed\ninputs into the initial feature representation. Let\nCn, n = {1, 2, 3, 4, 5} represent the channel-size of\nthe initial features. Similarly, let FI\ns ∈ RC1×H×W ,\nFR\ns ∈ RC2×H×W and FP\ns ∈ RC3×H×W be the initial feature\nrepresentation of spatial inputs Is, Rs, Ps respectively, and\nFI\nf ∈ RC4×(H/2)×(W/2) and FR\nf ∈ RC5×(H/2)×(W/2) be the\ninitial feature representation of frequency-decomposed inputs\nIf , Rf , respectively. Next, spatial feature ( Fs) and frequency-\ndecomposed feature ( Ff ) are obtained by concatenating\ncorresponding spatial and frequency-decomposed initial\nfeatures.\nFs = concat({FI\ns , FR\ns , FP\ns }), F f = concat({FI\nf , FR\nf })\n(1)\nwhere concat(·) is a concatenation operation. In\nthis article, we configure “conv→PReLU→conv” and\n“conv→PReLU→conv→avg-pool” block such that initial\nfeatures channel-size become C1 = 48 , C2 = 24 , C3 =\n8, C4 = 16 , C5 = 16 . Therefore, after concatenation\nFs ∈ RCs×H×W , Cs = 96 and Ff ∈ RCf ×(H/2)×(W/2),\nCf = 32.\n2) Body: The body of TSF-Net is composed of cascading B\nResidual Feature Block (RFB), as illustrated in figure 5 (a). An\nRFB receives spatial features ( Fs) and frequency-decomposed\nfeatures ( Ff ) through its two separate inputs and produces\nmore refined spatial features ( Fs) and frequency-decomposed\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 7\nFig. 6. Efficient Spectral-wise Multihead Self-Attention (E-SMSA).\nfeatures ( Ff ) through its two outputs. In an RFB block, the\nfrequency-decomposed features (Ff ) get fused into the spatial\nfeatures ( Fs) through an S-MSA transformer layer. Thus, by\nstacking B RFB blocks, we fuse and refine spatial feature ( Fs)\nand frequency-decomposed feature ( Ff ) at multiple stages.\nThe body also includes a feature-level skip connection between\nthe initial spatial-feature input and the final spatial-feature\noutput of the body, which is illustrated in figure 5 (a). The\ndetails of the Residual Feature Block (RFB) are described in\nSection III-C.\n3) Tail: The tail of TSF-Net includes a convolution layer\n(“conv”) and a pixel-shuffle layer ( “Pix-SFL”). The final\nspatial feature Fs ∈ RCs×H×W , Cs = 96 output from the\nbody of TSF-Net is processed by the “conv” layer, which\nreduces the channel size to just 4 and produces Fs ∈\nRCs×H×W , where Cs = 4 . Lastly, the “Pix-SFL” layer\nassembles the Fs ∈ R4×H×W into a single channel feature\nFs ∈ R1×(2H)×(2W). The global skip-connection at the end\nadds the input reconstructed picture (I) back to the final spatial\nfeature Fs, resulting in the clean reconstructed picture ( IC).\nC. Residual Fusion Block (RFB) as Feature Fusion Block\nThe primary building block of TSF-Net is the Residual\nFusion Block (RFB) which is illustrated in figure 5 (b). The\ninputs are the spatial ( Fs) and frequency-decomposed ( Ff )\nfeatures, and the outputs are the deeper and more refined\nspatial ( Fs) and frequency-decomposed ( Ff ) features. First,\nthe residual features are extracted from Fs and Ff by em-\nploying two sets of “conv→ReLU→conv” blocks and later\nthey are concatenated. Let, Fr\ns ∈ RCs×H×W and Fr\nf ∈\nRCf ×(H/2)×(W/2) be the residual features corresponding to\nFs and Ff . Since the feature size of Fr\ns is twice the size of\nFr\nf , the frequency-decomposed residual-feature Fr\nf is upscaled\nby a factor of two using transposed convolution ( “T-conv”)\nbefore concatenating it with Fr\ns . The concatenated feature\nF = concat({Fr\ns , Fr\nf }), F ∈ RC×H×W , C = Cs+Cf is then\npassed to Efficient Spectral-wise Multi-head Self-Attention\n(E-SMSA) layer for feature fusion. The E-SMSA transformer\nlayer fuses the residual features Fr\ns and Fr\nf into a next stage\nspatial residual feature Fr1\ns ∈ RCs×H×W . Lastly, two skip\nconnections are introduced to the residual features Fr1\ns by\nadding Fr\ns and Fs. Similarly, one skip connection is also\nintroduced to Fr\nf by adding frequency-decomposed features\nFf to it.\nFs = Fs ⊕ (Fr\ns ⊕ Fr1\ns ), F f = Ff ⊕ Fr\nf (2)\nThus, the final output of the RFB is again the refined spatial\nfeature Fs and frequency-decomposed feature Ff . The skip\nconnections from input to output convert the fusion block into\na residual fusion block . The residual connection, which is\ninspired by the ResNet [53], allows us to stack multiple RFB\nblocks and make the network much deeper while still being\nable to converge during training. The simplicity of the design\nchoice of RFB enables TSF-Net to scale it to various other low-\nlevel vision tasks in two ways: first, by adjusting the arbitrary\nnumber of RFB blocks, and second, by varying the channel\nsize (Cs and Cf ).\nD. Efficient Spectral-wise Multi-Head Self-Attention (E-\nSMSA)\nWe incorporate spectral-wise multi-head self-attention (S-\nMSA) transformer layer, from MST++ [34] into our proposed\nnetwork. However, we have redesigned it to serve as a channel-\nwise feature fusion layer within the RFB block. Additionally,\nwe have modified the self-attention mechanism of the original\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 8\nS-MSA to make it computationally efficient. Since E-SMSA\nis present in each RFB block, these reformed aspects signifi-\ncantly improve computational efficiency. Figure 6 provides an\nillustration of E-SMSA.\nSuppose, Xin is the input to the E-SMSA where Xin =\nF ∈ RC×H×W . Xin is then averaged-pooled ( “avg-pool”)\nover b × b block to get X1 ∈ RC×(H/b)×(W/b). X1 is then\nreshaped as X1 ∈ R(HW/b2)×C and linearly projected into\nquery Q ∈ R(HW/b2)×C and key K ∈ R(HW/b2)×C. Similarly,\nXin is reshaped into X2 ∈ RHW ×C and linearly projected into\nvalue V ∈ RHW ×C.\nQ = X1Wq, K = X1Wk, V = X2Wv (3)\nwhere Wq, Wk and Wv ∈ RC×C are the weights\nof 3 single-layer perceptrone. In a separate branch, Xin\nis reshaped as X3 ∈ RC×(HW ) and processed with 1D-\nconvolution of kernel-size 1 × 1 to reduce the feature to\nX3 ∈ RCs×(HW ). X3 is again reshaped as X3 ∈ RCs×H×W\nand processed by “G-conv→GeLU→G-conv” where “G-\nconv” stands for “ grouped-convolution”. Now the output is\nreshaped into position-embedding Epo ∈ R(HW )×Cs .\nNext, Q, K and V are subdivided along channel into h\nheads, such that each head Qi ∈ R(HW/b2)×(C/h), Ki ∈\nR(HW/b2)×(C/h) and Vi ∈ R(HW )×(C/h) where i = 1, 2, ...h.\nNow, self-attention is computed between the key and the query\nacross the channel within each head. E-SMSA treats each\nchannel as a token and computes attention among them within\na head.\nAi = softmax(σiKT\ni Qi), A i ∈ R(C/h)×(C/h) (4)\nwhere σi ∈ R1 is a learnable parameter employed to\nadapt self-attention Ai during matrix-multiplication KT\ni Qi.\nFor single head h = 1, self-attention matrix Ai ∈ RC×C\nwhich is depicted in figure 6 for simplicity. Next, value Vi is\nmatrix-multiplied with self-attention matrix Ai to generate a\nfused-feature Hi for ith head.\nHi = V T\ni Ai, H i ∈ R(HW )×(C/h) (5)\nThe fused feature Hi from all h heads are now concatenated\nand linearly projected to generate projection-embedding Epr.\nEpr = (concat(Hi))W, E pr ∈ R(HW )×Cs , i = 1, 2, ..., h\n(6)\nwhere W ∈ RC×Cs is the learnable weights of a single-\nlayer perceptrone. Finally, the output of E-SMSA layer is\ncomputed as below.\nXout = Epr + Epo, X out ∈ R(HW )×Cs (7)\nHere, Xout is then reshaped into residual feature output Fr\ns\nas mentioned in the section III-C.\nIV. N ETWORK IMPLEMENTATION\nA. Training/Validation Dataset Preparation\nWe train our proposed TSF-Net with Div2K [54] dataset,\nwhich includes 900 high-quality still images of 8-bit depth.\nOut of 900 images, we consider 875 images for training the\nTSF-Net and the remaining 25 for validating the network.\nTo align the TSF-Net to operate in the YUV color space,\nwe convert the 8-bit RGB images in the Div2K dataset\nto YUV 4:2:0 10-bit color format. Then, we encoded the\nYUV images with the VVC reference software, VTM-11.0,\nat four quantization-parameter (QP) rate points: 27, 32, 37\nand 42 following the common test condition (CTC) under all-\nintra (AI) profile. However, we encoded the YUV images by\ndisabling all the components (DBF, SAO, ALF(CCALF)) of\nthe default VVC in-loop filter as the TSF-Net replaces the\nwhole in-loop filter block in the VVC codec. In this article, we\nonly consider the luma (Y) component for testing with TSF-\nNet. Therefore, for each of the 900 images we collected its\nVTM-11.0 encoded luma (Y) component and corresponding\npartition and residue information to construct training and\nvalidation patches.\nB. Network Training\nTo improve the variance in the training sample, we generate\ntraining patches on the fly during network training. The\ntraining involves cropping 50 patches of size 256 × 256 from\na luma image (I) and its co-located partition (P) and residue\nimage (R). The strides of a window are computed to be random\nbut to lie within a certain range, such that the extracted 50\npatches cover a whole image but are also slightly different\nin the later epochs. During one epoch, the network sees a\ntotal of 875 × 50 = 43750 training patches. To augment the\ndata, the training patches are randomly flipped horizontally and\nvertically. On the other hand, validation patches are prepared\nonly once before network training. From 25 validation images,\n25 × 100 = 2500 validation patches of size 256 × 256 are\ncreated with a fixed stride. Once the stride is fixed, the same\nset of validation patches is created for all sets of experiments.\nSince the training and validation patch size is H = W =\n256, the size of spatial input and frequency-decomposed input\nto TSF-Net is 128 × 128 and 64 × 64, respectively. In this\narticle, we cascade B = 20 RFB blocks. All 2d-convolution\noperations involved in TSF-Net employ a kernel of 3 × 3.\nSimilarly, we set the spatial feature channel size to Cs = 96\nand frequency-decomposed feature channel size to Cf = 32.\nWe set h = 4, so E-SMSA operates with 4 heads and the size\nof each self-attention matrix is Ai ∈ R32×32.\nWe train four separate models for four different QP values.\nHowever, we first train a model from scratch at QP = 42\nutilizing training/validation patches encoded at the same QP.\nThe models for the other QP values (QP = {37, 32, 27}) are\nthen initialized with the weights from the pre-trained model\nat QP = 42 and fine-tuned with the patches encoded at the\nrespective QP values. This approach allows the rest of the\nmodel to train faster and potentially with better performance.\nWe adopt the Adam optimizer with β = (0 .9, 0.999) and\nL1-loss to optimize the network parameters. The model at\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 9\nFig. 7. Rate-Distortion (RD) plots for few test sequences comparing TSF-Net with VVC reference software. The Y-PSNR (dB) are evaluated at 4 QPs= {42,\n37, 32, 27 }.\nTABLE I\nDECODING TIME COMPLEXITY OF VVC WITH TSF-N ET (ON GPU)\nCOMPARED TO VVC WITH ITS IN-LOOP FILTER ENABLED .\nClass Decoding Complexity (%)\nA1 965.87\nA2 713.11\nB 1369.49\nC 1987.32\nD 4966.89\nOverall 2089.54\nQP = 42 is trained for 150 epochs, while the models for\nthe other QP values (QP = {37, 32, 27}) are trained for only\n100 epochs. The initial learning rate of the model at QP = 42\nis set to 10−4, while for the other models, it is initialized to\n0.5 ×10−4. We use a cosine-annealing learning-rate scheduler\nto decrease the learning rate until 10−6 during training. The\nbatch-size is set to 16 for all model training.\nV. E XPERIMENTAL RESULTS AND DISCUSSION\nWe evaluate the BD-rate performance of TSF-Net at four\nQPs={27, 32, 37, 42 } against the VVC reference software,\nVTM-11.0 (with all in-loop filters enabled), under the all-\nintra (AI) configuration. To test TSF-Net, we disable all the\ncomponents of the in-loop filter of VVC reference software\nand replace it with TSF-Net. We choose 19 common test\nsequences [56] recommended by JVET to evaluate the per-\nformance of TSF-Net. The selected 19 sequences are from 5\nclasses: A1 (3840×2160), A2 (3840×2160), B (1920×1080),\nC ( 480 × 832) and D ( 240 × 416). These test sequences are\nlisted in Table II.\nA. Comparison of TSF-Net with VVC’s In-loop Filter\nThe rate-distortion (RD) plots (Bitrate vs ‘Y’-PSNR) for a\nfew test sequences are shown in figure 7. The RD-plots clearly\nindicate that the TSF-Net, as an in-loop filter, outperforms\nVTM-11.0’s in-loop filter at all QP values. Moreover, except\nfor a few sequences, the performance gap between VTM-11.0\nand TSF-Net gets widens at lower QPs or correspondingly at\nlarger bitrates. This suggests that our method tends to perform\nbetter at lower QPs compared to VVC. Blocking is less\npronounced at lower bitrates and the distortions are primarily\ndue to quantization noises. Therefore, the above observation\npoints to the benefit of using frequency-decomposed input as\nprior information to the high-frequency content of an image.\nHere, TSF-Net is able to intercept high-frequency noises even\nin relatively good quality images (i.e. at lower bitrates) and\ncorrect them accordingly. This also demonstrates the powerful\nfeature fusion capability of E-SMSA layer.\nFor example, figure 8 illustrates the difference in the recon-\nstructed picture processed by VVC’s in-loop filter and TSF-\nNet. The middle and right figures are reconstructed versions\nof the left figure, coded and reconstructed by VTM-11.0 at\nQP = 32. While the middle one is processed by VTM-11.0’s\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 10\nFig. 8. Left: An original frame from CatRobot sequence. Middle: The same frame coded at QP=32 and processed with VTM-11.0’s In-loop filters. Right:\nThe same frame coded with VTM-11.0 at QP=32 and processed with TSF-Net.\nFig. 9. Left: An original frame from BasketballPass sequence. Middle: The same frame coded and reconstructed at QP=47 by VTM-11.0 but yet to be\nprocessed by the in-loop filter. This is also an input to TSF-Net. Right: The output frame of TSF-Net after processing the frame shown in the middle.\nin-loop filter, the right one is processed by the proposed TSF-\nNet. The zoomed-in pictures clearly demonstrate the difference\nbetween the two methods. The picture processed by VVC’s in-\nloop filter exhibits ringing effects around the edges of the ring\nand appears slightly fuzzy. In contrast, the picture processed\nby TSF-Net has almost no ringing noises and looks sharper.\nSimilarly, figure 9 reveals an interesting aspect of TSF-\nNet: its slight generative capability. The leftmost figure is an\noriginal frame taken from BasketballPass. It is then coded at\nQP = 47 by VTM-11.0 (with in-loop filter disabled) and\ngiven as an input to TSF-Net which is shown in the middle\nfigure. While the rightmost figure displays the processed\noutput from TSF-Net. The vertical strip shown in the original\nfigure, within the red rectangle, is shorter in the input frame\nand its reflection is absent on the floor. However, in the output\nframe generated by TSF-Net, the same strip is longer than in\nthe input frame, and its reflection on the floor is appropriately\ngenerated. This confirms the generative capability of TSF-Net.\nThis capability may be attributed to the fact that TSF-Net has\nlearned to generate new information employing locality, or that\nthe information was present in the input frame but not visible\nto the human eye, and TSF-Net made it more pronounced.\nWe also present the time complexity of TSF-Net as an in-\nloop filter in Table I. Since, TSF-Net is implemented only on\nthe decoder side, the table includes the decoding time com-\nplexity against VVC when its in-loop filters are enabled. The\ndecoding time complexity is calculated as 100 × (ttest/tref ),\nwhere ttest is the decoding time of VVC with TSF-Net as\nin-loop filter and tref is the decoding time of VVC with its\nin-loop filter enabled. For each sequence, we obtain the final\ndecoding time complexity by averaging them over all the QPs.\nTable I presents these values averaged over per class and all\nthe sequences. From the table, we observe that our proposed\nTSF-Net also suffer the same fate as most other learning-based\nsolutions, which is being slower. While analysing decoding\ntime complexity per class, we notice that the complexity\nincreases when video resolution decreases. One promising\nfuture research direction would be to improve the decoding\ntime complexity.\nB. Comparison of TSF-Net with other Learning-based In-\nLoop Filter\nWe also obtain the BD-rate and BD-PSNR performance of\nthree additional learning-based methods: Deep In-loop Filter\nwith Adaptive Model Selection (DAM) [55] [57], Uformer\n[14], and MSTFNet [51]. These methods were also evaluated\nagainst VTM-11.0, and their BD-rate and BD-PSNR perfor-\nmance, along with TSF-Net, for all 19 sequences can be found\nin Table II. Additionally, Table III presents the number of\nparameters (in million) and inference time (in seconds) for\nall four models. The inference time was tested on a Linux\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 11\nTABLE II\nCOMPARISON OF BD-R ATE(%) AND BD-PSNR( DB) PERFORMANCE ON LUMA COMPONENT UNDER ALL-INTRA (AI) C ONFIGURATION\nClass Test DAM [55] Uformer [14] MSTFNet [51] TSF-Net\nSequences BD-BR BD-PSNR BD-BR BD-PSNR BD-BR BD-PSNR BD-BR BD-PSNR\nA1 (2160x3840)\nTango2 -10.015 0.241 -11.661 0.282 -11.099 0.267 -11.154 0.270\nFoodMarket4 -9.471 0.384 -10.162 0.414 -9.687 0.394 -9.843 0.401\nCampfire -6.759 0.157 -7.640 0.176 -7.401 0.171 -7.799 0.180\nA2 (2160x3840)\nCatRobot -11.290 0.408 -12.694 0.460 -11.910 0.430 -12.129 0.441\nDaylightRoad2 -12.220 0.313 -14.202 0.367 -13.519 0.348 -13.874 0.359\nParkRunning3 -7.233 0.396 -7.042 0.384 -7.109 0.388 -7.365 0.403\nB (1080x1920)\nMarketPlace -7.466 0.298 -8.268 0.332 -8.037 0.322 -8.212 0.330\nRitualDance -10.366 0.529 -10.503 0.536 -10.320 0.526 -10.437 0.532\nCactus -9.301 0.349 -10.892 0.411 -10.476 0.396 -10.785 0.407\nBasketballDrive -10.410 0.318 -11.066 0.339 -10.920 0.335 -11.025 0.338\nBQTerrace -10.813 0.395 -10.485 0.383 -10.553 0.387 -10.741 0.392\nC (480x832)\nBasketballDrill -13.571 0.673 -13.005 0.640 -13.740 0.683 -13.690 0.680\nBQMall -11.285 0.627 -10.314 0.570 -11.364 0.631 -11.431 0.635\nPartyScene -7.931 0.471 -7.482 0.443 -8.049 0.479 -7.974 0.474\nRaceHorses -7.539 0.370 -7.093 0.348 -7.922 0.390 -8.064 0.397\nD (240x416)\nBasketballPass -11.272 0.658 -10.040 0.583 -11.369 0.665 -11.347 0.663\nBQSquare -10.021 0.694 -8.801 0.606 -10.00 0.693 -9.952 0.689\nBlowingBubbles -8.862 0.498 -8.274 0.464 -9.07 0.511 -8.980 0.506\nRaceHorses -10.013 0.557 -8.894 0.492 -10.081 0.561 -10.105 0.562\nAverage-Class A1 -8.748 0.261 -9.821 0.291 -9.396 0.277 -9.599 0.283\nAverage-Class A2 -10.248 0.372 -11.313 0.404 -10.846 0.389 -11.123 0.401\nAverage-Class B -9.671 0.378 -10.243 0.400 -10.061 0.393 -10.240 0.400\nAverage-Class C -10.081 0.535 -9.473 0.500 -10.269 0.546 -10.290 0.546\nAverage-Class D -10.042 0.602 -9.002 0.537 -10.130 0.607 -10.096 0.605\nAverage-Overall -9.781 0.439 -9.922 0.433 -10.138 0.451 -10.258 0.456\nTABLE III\nNO. OF PARAMETERS AND INFERENCE SPEED OF DIFFERENT MODELS\nModel no. of params inference time\n(million) (second)\nBytedance ∼ 5.51 0.0234\nUformer ∼ 5.29 0.0182\nMSTFNet ∼ 25.77 0.0204\nTSF-Net (vanilla) ∼ 5.42 0.051\nTSF-Net ∼ 5.42 0.026\nsystem with PyTorch 2.0 and Nvidia RTX A6000 GPU and\nis the average time required to process an input image of size\n256 × 256 over 100 rounds.\nThe Deep In-loop Filter with Adaptive Model Selection\n(DAM) was proposed by ByteDance to a JVET meeting as a\nresponse to the CfP on “neural network-based video coding”\nand has also been studied in [57]. DAM is placed before ALF\nwhile disabling DBF and SAO filters in VTM-11.0. During\ninference, patches are processed at the coding unit (CU) level,\nand for intra-coding, a CU-level flag is sent to indicate whether\nthe CU-block is processed by DAM or VVC’s default in-\nTABLE IV\nPSNR PERFORMANCE OF TWO VARIANTS OF TSF-N ET (VANILLA AND\nEFFICIENT ) AT QP=42.\nClass TSF-Net TSF-Net\n(vanilla) (efficient)\nA1 36.482 36.486\nA2 33.899 33.898\nB 32.969 32.967\nC 30.299 30.260\nD 29.111 29.108\nAverage PSNR 32.296 32.293\nloop filter as a rate-distortion (RD) optimization. In contrast,\nthere is currently no such optimization proposed with TSF-Net.\nDuring inference, TSF-Net processes high-resolution images\nat a patch size of 512 × 512, while low-resolution images\n(smaller than 512×512) are processed as a whole. Comparing\nthe coding performance from Table II, we see that the BD-\nrate and BD-PSNR gain of our proposed TSF-Net is larger\nthan that of DAM for all sequences (except BQTerrace). On\naverage, TSF-Net outperforms DAM in all classes and leads\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 12\nTABLE V\nCOMPARISON OF BD-R ATE SAVING ON LUMA COMPONENT UNDER\nALL-INTRA CONFIGURATION OF TSF-N ET WHEN “FREQUENCY PATH ” IS\nDISABLED .\nClass TSF-Net (FPD) TSF-Net (FPD) TSF-Net(sallow) (deep)\nAverage-Class A1 -8.956 -9.330 -9.599\nAverage-Class A2 -9.938 -10.586 -11.123\nAverage-Class B -9.318 -9.908 -10.240\nAverage-Class C -9.549 -10.177 -10.290\nAverage-Class D -9.614 -10.061 -10.096\nAverage Overall -9.470 -10.013 -10.258\nby −0.477% and 0.017dB in BD-rate and BD-PSNR gain\nrespectively. From Table III, we observe that both TSF-Net\nand DAM have approximately the same number of parameters\nand almost the same inference speed.\nConsidering the remarkable performance of Uformer [14] in\nvarious image restoration (IR) tasks such as image denoising,\nmotion blur removal, defocus blur removal and rain removal,\nwe investigate its potential as an in-loop filter and compare\nits performance with TSF-Net. Uformer is a transformer-\nbased U-shaped IR model. We choose Uformer-T (Tiny)\n(C = 16, depths of Encoder= {2,2,2,2}) variant which has\napproximately 5.29 millions parameter, similar to TSF-Net’s\nparameters. We follow the training setup from [14] (for e.g.\ninitialize learning rate to 2 ×10−4 and decrease it until 10−6)\nto train a model at QP = 42. For other QP s= {37, 32, 27}\nwe follow the training strategy of TSF-Net (i.e. initialize the\nweights from model trained at QP = 42) while initializing the\nlearning rate to 10−4 and decrease it until 10−6. Analyzing the\nresults in Table II, we observed that Uformer outperforms all\nother methods in Class A1 and Class A2. However, in Class C\nand Class D , Uformer performs less effectively compared to\nother methods. In Class B, it achieves similar performance to\nTSF-Net. Overall, Uformer falls behind TSF-Net by 0.336% in\nterms of BD-rate and 0.023 dB in terms of BD-PSNR gain. Its\ninference speed is slightly better, nonetheless comparable to\nTSF-Net as presented in Table III. The performance analysis of\nUformer emphasizes the strength of transformer-based models\nin exploiting long-range correlations in high-resolution videos,\nfor example, in Class A1 and Class A2 , where CNN-based\nmodels lack severely. Conversely, for low-resolution videos\nsuch as Class C and Class D, CNN-based model proves to be\nthe better choice.\nWe also consider the recently proposed MSTFNet [51] and\ncompared it with TSF-Net. TSF-Net can be considered an\nimprovement over MSTFNet as it is built by cascading only\nRFBs, each containing one S-MSA, resulting in a significantly\nhigher number of feature-fusion operations ( S-MSA) in TSF-\nNet. In contrast, MSTFNet has only three fusion blocks with\nS-MSA and the rest are sub-blocks without S-MSA. MSTFNet\nalso includes very large numbers of convolutional layers\ncompared to TSF-Net. Despite having significantly fewer pa-\nrameters, approximately 5.4 million compared to MSTFNet’s\n25.7 million, TSF-Net performs better than MSTFNet with\na BD-rate and BD-PSNR gain of −0.119% and 0.005dB\nrespectively. This performance boost can be attributed to TSF-\nNet’s more frequent use of feature-fusion operations ( S-MSA)\nthan MSTFNet. Despite having more S-MSA layers, TSF-Net’s\ninference time is still comparable to MSTFNet due to its\nefficient S-MSA (E-SMSA), as shown in Table III.\nC. Ablation Study\n1) Comparison of TSF-Net with efficient vs vanilla S-MSA:\nThe proposed TSF-Net is efficient due to the incorporation\nof efficient S-MSA ( E-SMSA) into the residual fusion block\n(RFB). E-SMSA utilizes average-pooling ( “avg-pool”) to re-\nduce the feature size by half and computes the key (K)\nand query (Q) with dimensions of K ∈ R(HW/b2)×C and\nQ ∈ R(HW/b2)×C, respectively, where b = 2. This reduces the\ncomputation complexity in the self-attention module by 1/4.\nIn contrast, in the vanilla TSF-Net, average-pooling is absent,\nand K and Q are computed at the original size of HW × C,\nmaking it less efficient.\nTable IV displays the PSNR performance of the vanilla and\nefficient TSF-Net on JEVT sequences at QP = 42. From\nthe table, we observe that there is barely any performance\ndifference between the two models. However, it is worth noting\nthat the proposed (efficient) TSF-Net is twice as efficient as\nthe vanilla TSF-Net, as presented in table III. Please note that\nboth models have the same number of parameters. Here, the\nPSNR performance is only presented at QP = 42, which we\nassume to be sufficient to demonstrate the advantage of the\nefficient TSF-Net over the vanilla TSF-Net. This is because the\nmodels at other QPs are initialized from the model trained at\nQP = 42 and are further trained to adapt them to other QPs.\nAs a result, the PSNR performance at other QPs is expected\nto follow a similar pattern and hence is not included.\n2) BD-Rate Performance of TSF-Net with no “Frequency-\nDecomposed Input”: We also investigate the contribution of\nthe “frequency-decomposed input” in the proposed TSF-Net.\nTo do this, we remove the “frequency-decomposed input” and\nall the components responsible for processing it from the\nTSF-Net. In other words, we only keep the “spatial path”\nand disable the “frequency path”. When the “frequency path”\nis disabled, the Residual Fusion Block (RFB) reduces to a\nsimpler ResBlock ( “conv→ReLU→conv”). We refer to this\nmodel as TSF-Net (FPD), where FPD stands for “Frequency\nPath Disabled”. By removing the “conv” and “T-conv” layers\nfrom the “frequency path” and eliminating the need for the\nE-SMSA layer for feature fusion, the total number of trainable\nparameters in the TSF-Net decreases from approximately 5.42\nmillion to around 3.34 million. This configuration is referred\nto as the “shallow” configuration. To ensure a fair comparison\nwith the proposed TSF-Net, we increase the number of RFB\nblocks from 20 to 32 and disable the “frequency path” in\nthe TSF-Net, resulting in another configuration of TSF-Net\n(FPD). This configuration of TSF-Net (FPD) has a total of\napproximately 5.34 million trainable parameters and is referred\nto as the ”deep” configuration.\nWe train TSF-Net (FPD) in both the sallow and deep config-\nurations at four QPs= {27, 32, 37, 42 }. The average BD-Rate\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 13\nsavings per class on the Luma component under the all-intra\n(AI) profile for both configurations are presented in table V.\nWe observe that the proposed TSF-Net offers greater BD-Rate\nsavings compared to TSF-Net (FPD) in both configurations.\nThe TSF-Net (FPD) with the sallow configuration, which\nhas the smallest number of trainable parameters, achieves\nthe lowest BD-Rate gain as expected. However, even with a\nsimilar number of trainable parameters to the proposed TSF-\nNet, the TSF-Net (FPD) in the deep configuration still demon-\nstrates inferior BD-Rate performance compared to TSF-Net.\nThese findings emphasize the significance of the “frequency-\ndecomposed input” and the powerful feature fusion operation\noffered along the “frequency path” in the TSF-Net.\nVI. C ONCLUSION\nIn this article, we introduce a novel neural network-based\nin-loop filter for VVC, referred to as TSF-Net. Previous\napproaches mostly overlooked frequency-decomposed infor-\nmation when designing neural network-based in-loop filters.\nIn contrast, TSF-Net learns jointly from both spatial (pixel)\nand frequency-decomposed (DCT’ed) information to effec-\ntively eliminate distortions in video frames. It utilizes a\nmulti-level feature extraction and feature fusion strategy to\nenhance performance. However, we deviate from previous\nmethods by designing a convolution-based simple feature ex-\ntractor and an advanced feature-aggregator based on channel-\nwise transformer ( S-MSA). These are combined into a block\ncalled Residual Feature Fusion Block (RFB) , which allows\nfor scalability. Additionally, we propose an efficient channel-\nwise transformer (E-SMSA) that improves the efficiency of\nthe vanilla TSF-Net by nearly a factor of two. We evaluate\nTSF-Net on 19 JVET common test sequences and achieve\n−10.258% BD-rate gain and 0.456dB BD-PSNR gain on the\nluma (Y) component under the all-intra (AI) configuration.\nOur proposed TSF-Net also outperforms other state-of-the-art\n(SOTA) methods under similar conditions.\nLimitation and Future Work: The future direction of\nthis work will be to test TSF-Net on the chroma components\n(Cb, Cr) and extend the work to other configurations: Low-\nDelay P (LP), Low-Delay B (LB) and Random-Access (RA).\nAdditionally, introducing a rate-distortion (RD) optimization,\nas proposed in DAM, will certainly bring the extra coding\ngain. Another crucial direction to explore will be to make E-\nSMSA fusion-layer even more efficient and possibly make it\nshared among RFB blocks to reduce the number of parameters.\nFurthermore, Uformer’s results proved that TSF-Net’s perfor-\nmance is subpar in video sequence with larger resolution.\nThis finding motivates to incorporate transformer as feature\nextractor in TSF-Net for future exploration.\nACKNOWLEDGMENTS\nThis work is accomplished in collaboration with Qualcomm\nand partially supported by the NSF award 2148382.\nREFERENCES\n[1] G. Sullivan and T. Wiegand, “Video compression - from concepts to the\nh.264/avc standard,” Proceedings of the IEEE, vol. 93, no. 1, pp. 18–31,\n2005.\n[2] G. J. Sullivan, J.-R. Ohm, W.-J. Han, and T. Wiegand, “Overview of the\nhigh efficiency video coding (hevc) standard,” IEEE Transactions on\nCircuits and Systems for Video Technology , vol. 22, no. 12, pp. 1649–\n1668, 2012.\n[3] B. Bross, Y .-K. Wang, Y . Ye, S. Liu, J. Chen, G. J. Sullivan, and J.-\nR. Ohm, “Overview of the versatile video coding (vvc) standard and\nits applications,” IEEE Transactions on Circuits and Systems for Video\nTechnology, vol. 31, no. 10, pp. 3736–3764, 2021.\n[4] M. Karczewicz, N. Hu, J. Taquet, C.-Y . Chen, K. Misra, K. Andersson,\nP. Yin, T. Lu, E. Franc ¸ois, and J. Chen, “Vvc in-loop filters,” IEEE\nTransactions on Circuits and Systems for Video Technology , vol. 31,\nno. 10, pp. 3907–3925, 2021.\n[5] A. Norkin, G. Bjøntegaard, A. Fuldseth, M. Narroschke, M. Ikeda,\nK. Andersson, M. Zhou, and G. V . der Auwera, “Hevc deblocking\nfilter,” IEEE Transactions on Circuits and Systems for Video Technology,\nvol. 22, pp. 1746–1754, 2012.\n[6] C.-M. Fu, E. Alshina, A. Alshin, Y .-W. Huang, C.-Y . Chen, C.-Y . Tsai,\nC.-W. Hsu, S.-M. Lei, J.-H. Park, and W.-J. Han, “Sample adaptive offset\nin the hevc standard,” IEEE Transactions on Circuits and Systems for\nVideo Technology, vol. 22, no. 12, pp. 1755–1764, 2012.\n[7] C.-Y . Tsai, C.-Y . Chen, T. Yamakage, I. S. Chong, Y .-W. Huang, C.-M.\nFu, T. Itoh, T. Watanabe, T. Chujoh, M. Karczewicz, and S.-M. Lei,\n“Adaptive loop filtering for video coding,” IEEE Journal of Selected\nTopics in Signal Processing , vol. 7, no. 6, pp. 934–945, 2013.\n[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification\nwith deep convolutional neural networks,” in Advances in Neural\nInformation Processing Systems , F. Pereira, C. Burges, L. Bottou,\nand K. Weinberger, Eds., vol. 25. Curran Associates, Inc.,\n2012. [Online]. Available: https://proceedings.neurips.cc/paper/2012/\nfile/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n[9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, and\nt. y. v. n. p. d. Jakobauthor=Zhang, Kai and Zuo, Wangmeng and Chen,\nYunjin and Meng, Deyu and Zhang, Lei, journal=IEEE Transactions on\nImage Processing, “An image is worth 16x16 words: Transformers for\nimage recognition at scale.”\n[10] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution\nusing deep convolutional networks,” IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 38, no. 2, p. 295–307, feb 2016. [Online]. Available:\nhttps://doi.org/10.1109/TPAMI.2015.2439281\n[11] D. Zhang, J. Shao, and H. T. Shen, “Kernel attention network\nfor single image super-resolution,” ACM Trans. Multimedia Comput.\nCommun. Appl. , vol. 16, no. 3, jul 2020. [Online]. Available:\nhttps://doi.org/10.1145/3398685\n[12] Z. Lu, J. Li, H. Liu, C. Huang, L. Zhang, and T. Zeng, “Transformer\nfor single image super-resolution,” in 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition Workshops (CVPRW) .\nLos Alamitos, CA, USA: IEEE Computer Society, jun 2022, pp.\n456–465. [Online]. Available: https://doi.ieeecomputersociety.org/10.\n1109/CVPRW56347.2022.00061\n[13] Y . Zhang, Y . Tian, Y . Kong, B. Zhong, and Y . Fu, “Residual dense\nnetwork for image restoration,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence , vol. 43, no. 7, pp. 2480–2495, 2021.\n[14] Z. Wang, X. Cun, J. Bao, W. Zhou, J. Liu, and H. Li, “Uformer: A\ngeneral u-shaped transformer for image restoration,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2022, pp. 17 683–17 693.\n[15] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte,\n“Swinir: Image restoration using swin transformer,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV)\nWorkshops, October 2021, pp. 1833–1844.\n[16] S. Gu, Y . Li, L. V . Gool, and R. Timofte, “Self-guided network for\nfast image denoising,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV) , October 2019.\n[17] S.-J. Cho, S.-W. Ji, J.-P. Hong, S.-W. Jung, and S.-J. Ko, “Rethinking\ncoarse-to-fine approach in single image deblurring,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV) ,\nOctober 2021, pp. 4641–4650.\n[18] H. Zhang, Y . Dai, H. Li, and P. Koniusz, “Deep stacked hierarchical\nmulti-patch network for image deblurring,” in The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , June 2019.\n[19] P. Maharjan, L. Li, Z. Li, N. Xu, C. Ma, and Y . Li, “Improving\nextreme low-light image denoising via residual learning,” in 2019 IEEE\nInternational Conference on Multimedia and Expo (ICME) , 2019, pp.\n916–921.\n[20] S. W. Zamir, A. Arora, S. H. Khan, H. Munawar, F. S. Khan, M.-H.\nYang, and L. Shao, “Learning enriched features for fast image restoration\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 14\nand enhancement,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2022.\n[21] J. Fang, H. Lin, X. Chen, and K. Zeng, “A hybrid network of cnn and\ntransformer for lightweight image super-resolution,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) Workshops, vol. 26, no. 7, June 2022, pp. 3142–3155.\n[22] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV . Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2015.\n[23] W. Shi, F. Jiang, and D. Zhao, “Single image super-resolution with\ndilated convolution based multi-scale information learning inception\nmodule,” in 2017 IEEE International Conference on Image Processing\n(ICIP), 2017, pp. 977–981.\n[24] T. Guo, H. S. Mousavi, and V . Monga, “Adaptive transform domain\nimage super-resolution via orthogonally regularized deep networks,”\nIEEE Transactions on Image Processing , vol. 28, no. 9, pp. 4685–\n4700, sep 2019. [Online]. Available: https://doi.org/10.1109%2Ftip.\n2019.2913500\n[25] J. Guo and H. Chao, “Building dual-domain representations for compres-\nsion artifacts reduction,” in Computer Vision – ECCV 2016 , B. Leibe,\nJ. Matas, N. Sebe, and M. Welling, Eds. Cham: Springer International\nPublishing, 2016, pp. 628–644.\n[26] S. Herbreteau and C. Kervrann, “Dct2net: an interpretable shallow cnn\nfor image denoising,” 2021. [Online]. Available: https://arxiv.org/abs/\n2107.14803\n[27] Z. Zhao, J. Zhang, S. Xu, Z. Lin, and H. Pfister, “Discrete cosine trans-\nform network for guided depth map super-resolution,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2022, pp. 5697–5707.\n[28] J. Zhao, J. Xie, R. Xiong, S. Ma, T. Huang, and W. Gao, “Pyramid\nconvolutional network for single image deraining,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) Workshops, June 2019.\n[29] P. Liu, H. Zhang, K. Zhang, L. Lin, and W. Zuo, “Multi-level wavelet-\ncnn for image restoration,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) Workshops , June\n2018.\n[30] M. G. Blanch, S. Blasi, A. Smeaton, N. E. O’Connor, and M. Mrak,\n“Chroma intra prediction with attention-based cnn architectures,” in\n2020 IEEE International Conference on Image Processing (ICIP), 2020,\npp. 783–787.\n[31] L. Murn, S. Blasi, A. F. Smeaton, and M. Mrak, “Improved cnn-based\nlearning of interpolation filters for low-complexity inter prediction in\nvideo coding,” IEEE Open Journal of Signal Processing , vol. 2, pp.\n453–465, 2021.\n[32] J. Ball ´e, V . Laparra, and E. P. Simoncelli, “End-to-end optimized image\ncompression,” in International Conference on Learning Representations,\n2017. [Online]. Available: https://openreview.net/forum?id=rJxdQ3jeg\n[33] G. Lu, W. Ouyang, D. Xu, X. Zhang, C. Cai, and Z. Gao, “Dvc: An\nend-to-end deep video compression framework,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2019.\n[34] Y . Cai, J. Lin, Z. Lin, H. Wang, Y . Zhang, H. Pfister, R. Timofte, and\nL. Van Gool, “Mst++: Multi-stage spectral-wise transformer for efficient\nspectral reconstruction,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) Workshops , June\n2022, pp. 745–755.\n[35] T. Lu, F. Pu, P. Yin, T. Chen, and W. Husak, “Adaptive reshaping\nfor next generation video codec,” in Applications of Digital Image\nProcessing XLI, A. G. Tescher, Ed., vol. 10752, International Society for\nOptics and Photonics. SPIE, 2018, p. 107520W. [Online]. Available:\nhttps://doi.org/10.1117/12.2320807\n[36] K. Misra, F. Bossen, and A. Segall, “On cross component adaptive loop\nfilter for video compression,” in 2019 Picture Coding Symposium (PCS),\n2019, pp. 1–5.\n[37] W.-S. Park and M. Kim, “Cnn-based in-loop filtering for coding effi-\nciency improvement,” in 2016 IEEE 12th Image, Video, and Multidi-\nmensional Signal Processing Workshop (IVMSP) , 2016, pp. 1–5.\n[38] T. Li, M. Xu, R. Yang, and X. Tao, “A densenet based approach\nfor multi-frame in-loop filter in hevc,” in 2019 Data Compression\nConference (DCC), 2019, pp. 270–279.\n[39] D. Ding, L. Kong, G. Chen, Z. Liu, and Y . Fang, “A switchable\ndeep learning approach for in-loop filtering in video coding,” IEEE\nTransactions on Circuits and Systems for Video Technology , vol. 30,\nno. 7, pp. 1871–1887, 2020.\n[40] S. Zhang, Z. Fan, N. Ling, and M. Jiang, “Recursive residual convo-\nlutional neural network- based in-loop filtering for intra frames,” IEEE\nTransactions on Circuits and Systems for Video Technology , vol. 30,\nno. 7, pp. 1888–1900, 2020.\n[41] M. Li and W. Ji, “Lightweight multiattention recursive residual cnn-\nbased in-loop filter driven by neuron diversity,” IEEE Transactions on\nCircuits and Systems for Video Technology , pp. 1–1, 2023.\n[42] H. Huang, I. Schiopu, and A. Munteanu, “Frame-wise cnn-based filtering\nfor intra-frame quality enhancement of hevc videos,” IEEE Transactions\non Circuits and Systems for Video Technology, vol. 31, no. 6, pp. 2100–\n2113, 2021.\n[43] S. Chen, Z. Chen, Y . Wang, and S. Liu, “In-loop filter with dense residual\nconvolutional neural network for vvc,” in 2020 IEEE Conference on\nMultimedia Information Processing and Retrieval (MIPR) , 2020, pp.\n149–152.\n[44] G. Huang, Z. Liu, and K. Q. Weinberger, “Densely connected\nconvolutional networks,” CoRR, vol. abs/1608.06993, 2016. [Online].\nAvailable: http://arxiv.org/abs/1608.06993\n[45] Z. Huang, Y . Li, and J. Sun, “Multi-gradient convolutional neural\nnetwork based in-loop filter for vvc,” in 2020 IEEE International\nConference on Multimedia and Expo (ICME) , 2020, pp. 1–6.\n[46] D. Ma, F. Zhang, and D. R. Bull, “MFRNet: A new CNN architecture\nfor post-processing and in-loop filtering,” IEEE Journal of Selected\nTopics in Signal Processing , vol. 15, no. 2, pp. 378–387, feb 2021.\n[Online]. Available: https://doi.org/10.1109%2Fjstsp.2020.3043064\n[47] Z. Huang, J. Sun, X. Guo, and M. Shang, “One-for-all: An efficient\nvariable convolution neural network for in-loop filter of vvc,” IEEE\nTransactions on Circuits and Systems for Video Technology , vol. 32,\nno. 4, pp. 2342–2355, 2022.\n[48] S. Kuanar, D. Mahapatra, V . Athitsos, and K. R. Rao, “Gated fusion\nnetwork for sao filter and inter frame prediction in versatile video\ncoding,” 2021.\n[49] S. Liu, L. Wang, P. Wu, and H. Yang, “Jvet ahg\nreport: Neural networks in video coding (ahg9),” 2018.\n[Online]. Available: https://jvet-experts.org/doc end user/documents/\n10 San%20Diego/wg11/JVET-J0009-v1.zip\n[50] B. Kathariya, Z. Li, H. Wang, and G. Van Der Auwera, “Multi-\nstage locally and long-range correlated feature fusion for learned in-\nloop filter in vvc,” in 2022 IEEE International Conference on Visual\nCommunications and Image Processing (VCIP) , 2022, pp. 1–5.\n[51] B. Kathariya, Z. Li, H. Wang, and M. Coban, “Multi-stage spatial and\nfrequency feature fusion using transformer in cnn-based in-loop filter for\nvvc,” in 2022 Picture Coding Symposium (PCS) , 2022, pp. 373–377.\n[52] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee, “Enhanced deep\nresidual networks for single image super-resolution,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) Workshops, July 2017.\n[53] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , June 2016.\n[54] R. Timofte, S. Gu, J. Wu, L. Van Gool, L. Zhang, M.-H. Yang, M. Haris\net al., “Ntire 2018 challenge on single image super-resolution: Methods\nand results,” in The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) Workshops, June 2018.\n[55] Y . Li, L. Zhang, and K. Zhang, “Ahg11: Deep in-loop filter with adaptive\nmodel selection,” in document JVET-V0100, 22nd JVET meeting , 2021.\n[56] F. Bossen, J. Boyce, K. Suehring, X. Li, and V . Seregin, “Jvet\ncommon test conditions and software reference configurations for sdr\nvideo,” 2018. [Online]. Available: https://jvet-experts.org/doc end user/\ndocuments/12 Macao/wg11/JVET-L1010-v1.zip\n[57] Y . Li, L. Zhang, and K. Zhang, “Idam: Iteratively trained deep\nin-loop filter with adaptive model selection,” ACM Trans. Multimedia\nComput. Commun. Appl. , vol. 19, no. 1s, jan 2023. [Online]. Available:\nhttps://doi.org/10.1145/3529107\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 2023 15\nVII. B IOGRAPHY SECTION\nBirendra Kathariya (Student Member, IEEE) re-\nceived his B.E. degree in Electronics and Commu-\nnication Engineering from Nepal Engineering Col-\nlege, Bhaktapur, Nepal in 2012, and his Master’s\ndegree from the University of Missouri-Kansas City\n(UMKC), Kansas City, MO, USA in 2017. He is\npresently pursuing a Ph.D. in Electrical Engineer-\ning from the University of Missouri-Kansas City\n(UMKC) and working in the Multimedia Compu-\ntation and Communication (MCC) Lab at UMKC.\nHe has also interned at various companies including\nMediaTek (San Jose, CA), Futurewei Technologies (San Jose, CA), Tencent\nAmerica (Palo Alto, CA), and Qualcomm Technologies Inc. (San Diego,\nCA). His research interests include point cloud compression, image/video\ncompression, deep-learning model compression in federated learning.\nZhu Li (Senior Member, IEEE) is a professor with\nthe Dept of Computer Science Electrical Engineer-\ning, University of Missouri, Kansas City, and the\ndirector of NSF I/UCRC Center for Big Learning\n(CBL) at UMKC. He received his PhD in Elec-\ntrical Computer Engineering from Northwestern\nUniversity in 2004. He was AFRL summer faculty\nat the UA V Research Center, US Air Force Academy\n(USAFA), 2016-18, 2020-23. He was senior staff\nresearcher with the Samsung Research America’s\nMultimedia Standards Research Lab in Richardson,\nTX, 2012-2015, senior staff researcher with FutureWei Technology’s Media\nLab in Bridgewater, NJ, 2010 2012, assistant professor with the Dept of\nComputing, the Hong Kong Polytechnic University from 2008 to 2010, and a\nprincipal staff research engineer with the Multimedia Research Lab (MRL),\nMotorola Labs, from 2000 to 2008. His research interests include point cloud\nand light field compression, graph signal processing and deep learning in the\nnext gen visual compression, image processing and understanding. He has 50+\nissued or pending patents, 190+ publications in book chapters, journals, and\nconferences in these areas. He is an IEEE senior member, associate Editor-in-\nChief for IEEE Trans on Circuits System for Video Tech, associated editor for\nIEEE Trans on Image Processing(2020 ), IEEE Trans.on Multimedia (2015-\n18), IEEE Trans on Circuits System for Video Technology(2016-19). He\nreceived the Best Paper Award at IEEE Int’l Conf on Multimedia Expo\n(ICME), Toronto, 2006, and IEEE Int’l Conf on Image Processing (ICIP),\nSan Antonio, 2007.\nGeert Van der Auwera (Senior Member, IEEE) received the Ph.D. degree\nin Electrical Engineering from Arizona State University, Tempe, AZ, USA,\nin 2007, and the Belgian MSEE degree from Vrije Universiteit Brussel\n(VUB), Brussels, Belgium, in 1997. Presently, he is a Director at Qualcomm\nTechnologies Inc. in San Diego, CA, USA, in the Multimedia R&D &\nStandards group where he actively contributed to the standardization efforts\nof JCT-VC’s High Efficiency Video Coding (HEVC) and JVET’s Versatile\nVideo Coding (VVC). Currently, he is participating in MPEG’s Point Cloud\nCompression activity. Until Jan. 2011, he was with Samsung Electronics in\nIrvine, CA, USA. Until Dec. 2004, he was Scientific Advisor with IWT-\nFlanders, the Institute for the Promotion of Innovation by Science and\nTechnology in Flanders, Belgium. In 2000, he joined IWT-Flanders after\nresearching wavelet video coding at IMEC’s Electronics and Information\nProcessing Department (VUB-ETRO) in Brussels, Belgium. In 1998, his\nMSEE thesis on motion estimation in the wavelet domain received the Barco\nand IBM prizes by the Fund for Scientific Research of Flanders, Belgium. His\nresearch interests are video coding, point cloud compression, XR, video traffic\nand quality characterization, video streaming mechanisms and protocols.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3323483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7715117931365967
    },
    {
      "name": "Feature extraction",
      "score": 0.5634316802024841
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5483397841453552
    },
    {
      "name": "Feature learning",
      "score": 0.5343543291091919
    },
    {
      "name": "Transformer",
      "score": 0.4844262897968292
    },
    {
      "name": "Codec",
      "score": 0.45968884229660034
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.45747897028923035
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4106272757053375
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.36261802911758423
    },
    {
      "name": "Speech recognition",
      "score": 0.32227784395217896
    },
    {
      "name": "Computer hardware",
      "score": 0.13156616687774658
    },
    {
      "name": "Engineering",
      "score": 0.09623542428016663
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}