{
  "title": "NetLLM: Adapting Large Language Models for Networking",
  "url": "https://openalex.org/W4391590983",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2254906037",
      "name": "WU Duo",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2351788847",
      "name": "Wang Xian-da",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2390103319",
      "name": "Qiao Ya-qi",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A1842122615",
      "name": "Wang Zhi",
      "affiliations": [
        "Tsinghua University",
        "University Town of Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2358128082",
      "name": "Jiang, Junchen",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2323901126",
      "name": "Cui, Shuguang",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2637981871",
      "name": "Wang, Fangxin",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3046478992",
    "https://openalex.org/W3176390156",
    "https://openalex.org/W2885982708",
    "https://openalex.org/W2950667143",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2824027552",
    "https://openalex.org/W3169291081",
    "https://openalex.org/W2415420807",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W616265588",
    "https://openalex.org/W3012561096",
    "https://openalex.org/W3210129272",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W3141438256",
    "https://openalex.org/W4382202657",
    "https://openalex.org/W2133569115",
    "https://openalex.org/W2968289224",
    "https://openalex.org/W3021314748",
    "https://openalex.org/W2167407752",
    "https://openalex.org/W3213789840",
    "https://openalex.org/W4304080763",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W4387321091",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4389518784",
    "https://openalex.org/W4388320607",
    "https://openalex.org/W4388994228",
    "https://openalex.org/W2017146017",
    "https://openalex.org/W4224315052",
    "https://openalex.org/W4367662893",
    "https://openalex.org/W3136450870",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2744628735",
    "https://openalex.org/W2968986602",
    "https://openalex.org/W3081575380",
    "https://openalex.org/W3038898937",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W2997006708",
    "https://openalex.org/W2900853407",
    "https://openalex.org/W4390692489",
    "https://openalex.org/W2963140444",
    "https://openalex.org/W3121689374",
    "https://openalex.org/W2896840651",
    "https://openalex.org/W2055165550",
    "https://openalex.org/W4312310654",
    "https://openalex.org/W4304142274",
    "https://openalex.org/W2746553466",
    "https://openalex.org/W3006966652",
    "https://openalex.org/W2622637199",
    "https://openalex.org/W2996939355",
    "https://openalex.org/W4385573010",
    "https://openalex.org/W2907492528",
    "https://openalex.org/W4290991577",
    "https://openalex.org/W4382463788",
    "https://openalex.org/W4386365365",
    "https://openalex.org/W1976944900",
    "https://openalex.org/W4320726207",
    "https://openalex.org/W4394877201",
    "https://openalex.org/W4402742293"
  ],
  "abstract": "Many networking tasks now employ deep learning (DL) to solve complex\\nprediction and optimization problems. However, current design philosophy of\\nDL-based algorithms entails intensive engineering overhead due to the manual\\ndesign of deep neural networks (DNNs) for different networking tasks. Besides,\\nDNNs tend to achieve poor generalization performance on unseen data\\ndistributions/environments.\\n Motivated by the recent success of large language models (LLMs), this work\\nstudies the LLM adaptation for networking to explore a more sustainable design\\nphilosophy. With the powerful pre-trained knowledge, the LLM is promising to\\nserve as the foundation model to achieve \"one model for all tasks\" with even\\nbetter performance and stronger generalization. In pursuit of this vision, we\\npresent NetLLM, the first framework that provides a coherent design to harness\\nthe powerful capabilities of LLMs with low efforts to solve networking\\nproblems. Specifically, NetLLM empowers the LLM to effectively process\\nmultimodal data in networking and efficiently generate task-specific answers.\\nBesides, NetLLM drastically reduces the costs of fine-tuning the LLM to acquire\\ndomain knowledge for networking. Across three networking-related use cases -\\nviewport prediction, adaptive bitrate streaming and cluster job scheduling, we\\nshowcase that the NetLLM-adapted LLM significantly outperforms state-of-the-art\\nalgorithms.\\n",
  "full_text": "NetLLM: Adapting Large Language Models for Networking\nDuo Wu1, Xianda Wang1, Yaqi Qiao1, Zhi Wang2, Junchen Jiang3, Shuguang Cui1, Fangxin Wang1‚àó\n1SSE and FNii, The Chinese University of Hong Kong, Shenzhen\n2SIGS, Tsinghua University 3The University of Chicago\nABSTRACT\nMany networking tasks now employ deep learning (DL) to solve\ncomplex prediction and optimization problems. However, current\ndesign philosophy of DL-based algorithms entails intensive engi-\nneering overhead due to the manual design of deep neural net-\nworks (DNNs) for different networking tasks. Besides, DNNs tend\nto achieve poor generalization performance on unseen data distri-\nbutions/environments.\nMotivated by the recent success of large language models (LLMs),\nthis work studies the LLM adaptation for networking to explore a\nmore sustainable design philosophy. With the powerful pre-trained\nknowledge, the LLM is promising to serve as the foundation model\nto achieve ‚Äúone model for all tasks‚Äù with even better performance\nand stronger generalization. In pursuit of this vision, we present\nNetLLM, the first framework that provides a coherent design to\nharness the powerful capabilities of LLMs with low efforts to solve\nnetworking problems. Specifically, NetLLM empowers the LLM to\neffectively process multimodal data in networking and efficiently\ngenerate task-specific answers. Besides, NetLLM drastically reduces\nthe costs of fine-tuning the LLM to acquire domain knowledge for\nnetworking. Across three networking-related use cases - viewport\nprediction, adaptive bitrate streaming and cluster job scheduling, we\nshowcase that the NetLLM-adapted LLM significantly outperforms\nstate-of-the-art algorithms.\nCCS CONCEPTS\n‚Ä¢ Networks ‚Üí Application layer protocols ; ‚Ä¢ Computing\nmethodologies ‚Üí Supervised learning ; Reinforcement\nlearning;\nKEYWORDS\nDeep Learning, Network Optimization, Video Streaming, Job Sched-\nuling, Large Language Model Adaptation\nACM Reference Format:\nDuo Wu, Xianda Wang, Yaqi Qiao, Zhi Wang, Junchen Jiang, Shuguang\nCui, Fangxin Wang. 2024. NetLLM: Adapting Large Language Models for\nNetworking. In ACM SIGCOMM 2024 Conference (ACM SIGCOMM ‚Äô24), Au-\ngust 4‚Äì8, 2024, Sydney, NSW, Australia. ACM, New York, NY, USA, 18 pages.\nhttps://doi.org/10.1145/3651890.3672268\n‚àóFangxin Wang is the corresponding author: wangfangxin@cuhk.edu.cn.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to the\nAssociation for Computing Machinery.\nACM ISBN 979-8-4007-0614-1/24/08.\nhttps://doi.org/10.1145/3651890.3672268\n1 INTRODUCTION\n1.1 The Main Roadmap so far\nOver the past decades, rule-based algorithms built on handcrafted\ncontrol rules have played an important role in optimizing network\nsystems [6, 8, 32, 107]. For instance, Copa [6] adjusts sending rates\nfor congestion control based on measured queueing delay, while\nPANDA [53] switches video streaming bitrates based on heuris-\ntically estimated bandwidth. However, these algorithms heavily\nrely on rule engineering , which involves intensive human efforts\nto devise, implement and validate the control rules for network\noptimization [11, 62, 63, 66].\nIn recent years, the advancements of deep learning have prompted\nextensive research into learning-based algorithms for networking.\nThese algorithms design and train deep neural networks (DNNs)\nwith supervised learning (SL) [95] or reinforcement learning (RL) [88]\ntechniques to automatically discover networking solutions, thus\neliminating the need of rule engineering. Specifically, SL is widely\nadopted to train DNNs for prediction tasks in networking, such as\ntraffic classification [54, 73] and bandwidth prediction [9, 64]. On\nthe flip side, RL is commonly employed to solve decision-making\nproblems in networking, including congestion control [1, 106], adap-\ntive bitrate streaming (ABR) [44, 62] and cloud cluster job schedul-\ning (CJS) [63, 78]. Thanks to the strong capability of DNNs in func-\ntion approximation, learning-based algorithms have demonstrated\nsignificant improvement over handcrafted rule-based algorithms.\nDespite their promising potential, existing learning-based algo-\nrithms still suffer from two key limitations:\n‚Ä¢High model engineering costs. It has been shown that the de-\nsign of DNN architecture is crucial to the final performance [67,\n83]. Therefore, the focus of learning-based algorithms has shifted\nfrom rule engineering to model engineering. Their success is heav-\nily dependent on engineering DNN models for the target network-\ning tasks, which, however, can be difficult and labor-intensive due\nto the complex structures of DNNs [66]. To make things worse,\nthe diversity of networking tasks also prevents sharing the same\nDNN model across different tasks. This necessitates designing\nspecialized DNNs for different tasks (i.e., one model for one task ),\nthus further increasing the engineering costs. Although some\nrecent works attempt to introduce structured Transformer [94]\ninto model design, they still necessitate manual tuning of the\nTransformer architecture (e.g., the number of attention blocks\nand attention heads) [99], or even the design of specialized tok-\nenization scheme [36, 65] and attention mechanism [36, 57], thus\nleading to high engineering costs.\n‚Ä¢Low generalization. DNNs trained on specific data distribu-\ntions/environments may struggle to perform well or even worse\nthan conventional rule-based algorithms on unseen data distribu-\ntions/environments [105]. For example, an ABR model trained on\nsmooth network conditions often achieves poor performance on\narXiv:2402.02338v3  [cs.NI]  6 Aug 2024\nACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia Wu et al.\nnetwork environments with dynamic bandwidth fluctuations [44].\nThe lack of generalization can ultimately hinder the widespread\ndeployment of learning-based algorithms in practice [103, 106],\nas network operators will suspect their superiority over the in-\ncumbent rule-based algorithms in production environments.\n1.2 New Opportunities and Challenges\nUtilizing a single generic model for different tasks has been recog-\nnized as a significant approach to mitigate the costs of handcrafting\nspecialized DNNs for each task and enhance generalization [ 82].\nThis is exemplified by the recent popular large language models\n(LLMs) such as ChatGPT [72], Falcon [77] and Llama2 [92] in the\nfield of natural language processing (NLP). With billions of param-\neters pre-trained on massive data to absorb extensive knowledge,\nLLMs have demonstrated extraordinary capacity in conversations,\nreasoning and text generation in NLP [96]. What‚Äôs more, they also\nexhibit emergent abilities that were not explicitly programmed\ninto them during pre-training, such as planning, pattern mining,\nproblem solving and generalization to unseen conditions [102, 112].\nThese abilities have been proven to be transferable to other domains,\nincluding robotics [23], chip design [59], and protein structure pre-\ndiction [55]. For instance, researchers have shown that LLMs can\ngenerate goal-oriented plans for robotic control, adjusting plans\nin response to environment changes, and generalize to previously\nunseen operating environments [112].\nMotivated by these inspiring outcomes, we believe that LLMs can\nserve as foundation models for networking, as many networking\ntasks can also benefit from their emergent abilities. In the context\nof ABR, for example, the planning ability of LLMs can be utilized\nfor better bitrate decisions to optimize the video streaming sessions\nbased on the changing network conditions. Furthermore, their gen-\neralization capability can be harnessed to generalize across diverse\nnetwork environments. Therefore, we envision LLM as the key to\nachieving one model for all tasks , with little handcraft costs and\nstrong generalization. We try to answer the following key question:\ncan we embrace the era of LLMs and adapt LLMs to solve various\nnetworking tasks efficiently and effectively?\nUnfortunately, as revealed by our analysis in ¬ß3, the adaptation\nof LLMs for networking faces the following challenges.\n‚Ä¢Large input modality gap. In networking tasks, various in-\nformation observed from the system is collected as inputs for\nnetworking algorithms. However, the modalities of these inputs\ndiffer significantly from plain text, i.e., the native input modal-\nity supported by LLMs1 [92, 108]. For example, in ABR, network\nthroughputs and delay are often collected for bitrate decision [62],\nwhich exhibit unique time-varying data patterns typically not\nfound in natural text. This discrepancy prevents LLMs from ef-\nfectively processing the input information of networking tasks.\n‚Ä¢Inefficiency of answer generation. LLMs generate answers\nusing a language modeling (LM) head to predict words (tokens)\none by one (see Figure 1) [46]. While this approach is well-suited\nin NLP, it presents several drawbacks in the networking domain.\nFirst, LLMs are prone to hallucination due to the inherent uncer-\ntainty of token prediction [41, 49]. Their generated answers for\n1Although multimodal LLMs have emerged recently (e.g., GPT4 [71]), their supported\ninput modalities are still limited (mainly vision or audio), hindering directly applying\nthem to process networking task inputs.\nnetworking tasks may seem correct but physically invalid (e.g., a\nnonexistent bitrate for video download in ABR), which can even-\ntually impair the reliability of network systems. Second, since\ntokens are predicted one at a time, LLMs often require multiple\ninferences to generate a complete answer, thus incurring high\nanswer generation latency (i.e., the time to generate a complete\nanswer). Consequently, LLMs may fail to quickly generate an-\nswers to respond to the system changes (e.g., switching to a lower\nbitrate when network bandwidth becomes scarce).\n‚Ä¢High adaptation costs. The large domain gap between network-\ning and NLP necessitates fine-tuning LLMs to acquire domain-\nspecific knowledge for effective adaptation. However, the adapta-\ntion costs can be prohibitively high, especially when fine-tuning\nLLMs for decision-making tasks (e.g., ABR [ 44] and CJS [ 63])\nwhere RL is employed to solve the system optimization problems.\nSpecifically, RL-based decision-making tasks require the active\ninteraction between LLMs and environments (e.g., network en-\nvironments with varying bandwidth or workload patterns) to\ncollect experiences for performance optimization [63, 103]. Due\nto the large parameter size of LLMs, the interaction process can\nbe excessively time-consuming, introducing a large amount of\nadditional training time. What‚Äôs worse, the adaptation costs can\nfurther increase if fine-tuning the full parameters of LLMs, which\nis known to be resource-intensive [55, 59].\n1.3 Design and Contributions\nTo overcome the aforementioned challenges, this work proposes\nNetLLM, the first framework that efficiently adapts LLMs for network-\ning. NetLLM stands out as the first to take LLMs the extra mile\nin context of networking and provides a coherent design to uti-\nlize their powerful capabilities to generalize to various tasks with\nlow efforts. It enables the efficient utilization of a single LLM (e.g.,\nLlama2 [92]) as the foundation model without any modifications to\ntackle a wide range of networking tasks and meanwhile achieves\nenhanced performance. To accomplish this, NetLLM designs a mul-\ntimodal encoder to enable the LLM to process multimodal data in\nnetworking and implements a networking head to improve the\nefficiency of answer generation. Furthermore, while fine-tuning the\nLLM and these two modules on the target task to acquire domain\nknowledge is necessary, NetLLM designs an efficient data-driven\nlow-rank networking adaptation (DD-LRNA) scheme to drastically\nreduce the fine-tuning costs. Specifically, NetLLM incorporates the\nfollowing three core design modules to efficiently adapt LLMs for\nnetworking.\n‚Ä¢Multimodal encoder. NetLLM designs an efficient multimodal\nencoder at the input side of the LLM to effectively process the\nmultimodal input information of networking tasks. The goal of\nthis module is to automatically project task inputs to the same\nfeature space as language tokens, so that the LLM can understand\nand utilize these inputs for task solving. To achieve this, it first\nuses modality-specific feature encoders to extract features from\nraw inputs of various modalities involved in networking. It then\nleverages trainable layers to project these features into token-like\nembedding vectors, which can be directly fed into the LLM for\neffective processing.\nNetLLM: Adapting Large Language Models for Networking ACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia\n‚Ä¢Networking head. To enable efficient answer generation,NetLLM\nremoves the default LM head used by the LLM for token predic-\ntion. Instead, it introduces various networking heads at the output\nside of the LLM to generate answers for specific networking tasks.\nEach networking head is essentially a lightweight trainable pro-\njector that maps the output features of the LLM directly into\ntask-specific answers. In other words, they eliminate token pre-\ndiction and allow direct answer generation from the valid range\nof possible answers (e.g., selecting a bitrate from candidate op-\ntions in ABR). This enables the LLM to generate a valid answer\nin a single inference, thus ensuring its reliability for networking\nand significantly reducing generation latency.\n‚Ä¢DD-LRNA. To reduce the fine-tuning costs, NetLLM designs a\nDD-LRNA scheme for the LLM to efficiently acquire domain\nknowledge for networking. Specifically, DD-LRNA incorporates\na data-driven adaptation pipeline to adapt the LLM for both pre-\ndiction and decision-making tasks. In particular, for decision-\nmaking tasks, it employs the efficient data-driven RL technique [3,\n79, 106] to eliminate the time-consuming interaction between\nthe LLM and environments. It collects an experience pool as\ntraining dataset with existing networking algorithms and fine-\ntunes the LLM over such dataset in the data-driven manner. Be-\nsides, inspired by the advanced parameter-efficient fine-tune tech-\nnique [21, 29], DD-LRNA introduces a set of additional trainable\nlow-rank matrices for the LLM to learn networking knowledge.\nSince the low-rank matrices only account for 0.31% of the to-\ntal parameters, the fine-tune costs are greatly reduced, with the\nreduction of 60.9% GPU memory and 15.1% training time.\nNetLLM has the following important properties. i) Compatibility:\nIt is a generic framework that can be applied to adapt different\nLLMs for various networking tasks. ii) Reliability: It addresses the\nhallucination issue and ensures the LLM generated answers to be\nalways valid. iii) Efficiency: The DD-LRNA scheme significantly\nreduces the costs of fine-tuning LLM to learn domain knowledge\nand the networking heads also reduce the answer generation la-\ntency. iv) Transferability: NetLLM offers an effective solution for\nutilizing LLMs to solve prediction and decision-making tasks at low\ncosts. These two types of tasks, in fact, span across diverse domains\nsuch as medicine [17], finance [52], and telecommunication [111].\nHence, despite its original focus on networking, NetLLM holds the\npotential to be applied and transferred to other fields.\nWe have implementedNetLLM2 for three networking tasks: view-\nport prediction (VP) [85] for immersive video streaming, adaptive\nbitrate streaming (ABR) [103], and cluster job scheduling (CJS) [63].\nWe believe these representative tasks cover the main input modal-\nities of networking problems and span from prediction tasks to\ndecision-making tasks (¬ß3). Through extensive trace-driven sim-\nulation and real-world tests, we demonstrate the effectiveness of\nNetLLM in LLM adaptation for networking. We showcase that across\nthe three use cases, the NetLLM-adapted LLM significantly outper-\nforms state of the arts with performance improvements of 10.1-\n36.6% for VP, 14.5-36.6% for ABR, 6.8-41.3% for CJS, in terms of their\nrespective performance metrics. Besides, our empirical test results\nalso show that we can efficiently utilize the extensive knowledge of\n2The codes available at: https://github.com/duowuyms/NetLLM.\nthe LLM with our proposed NetLLM framework to achieve stronger\ngeneralization on unseen testing environments.\nThe contributions of this paper are summarized as follows:\n‚Ä¢We identify the key challenges of adapting LLMs for network-\ning and demonstrate that some natural alternatives fall short in\naddressing these challenges (¬ß3).\n‚Ä¢We then design NetLLM, the first LLM adaptation framework\nfor networking that incorporates a multimodal encoder module\nto encode multimodal task inputs, networking head module to\ndirectly generate answers and a DD-LRNA scheme to reduce the\nadaptation costs (¬ß4).\n‚Ä¢We extensively evaluate NetLLM across three networking tasks.\nWe showcase that the LLM adapted by our framework can signif-\nicantly surpass state-of-the-art algorithms and achieve superior\ngeneralization performance. We also conduct a deep dive into\nNetLLM to provide an in-depth understanding of it (¬ß5).\n2 BACKGROUND\n2.1 Learning-Based Networking Algorithms\nLearning-based algorithms design and train deep neural networks\n(DNNs) to efficiently learn to solve networking tasks [2, 64, 103]. In\nparticular, there are two learning paradigms commonly adopted to\nenable the learning process: supervised learning (SL) [95] and rein-\nforcement learning (RL) [88]. SL is widely employed for prediction\ntasks in networking, such as traffic classification [ 54, 73], band-\nwidth prediction [64, 105] and viewport prediction [34, 85]. It trains\nDNNs with specific datasets to optimize a pre-defined loss function,\nso that once trained, DNNs can be used for efficient prediction to\nassist the system control. For example, Yan et al. [105] train a DNN\nmodel over real-world bandwidth datasets to predict the future\nbandwidth for bitrate control on video clients. On the flip side, RL\nis well-suited for sequential decision-making tasks in networking,\nincluding congestion control [1, 106], video streaming [44, 62] and\ncluster job scheduling [63, 78]. In these tasks, DNNs actively inter-\nact with the environments to collect experiences, then use them to\noptimize task-specific reward functions so that the performance\nof network systems can be optimized. For instance, Mao et al. [63]\nemploy RL to train a DNN model to allocate resources to job re-\nquests so as to maximize the utilization of computing resources in\na distributed computing cluster.\nThe major limitations of learning-based algorithms are two-\nfold. First, they require designing specialized DNN models for dif-\nferent networking tasks, thus entailing high model engineering\noverhead [66]. Second, they are prone to generalization issues, as\nDNNs may achieve poor performance on unseen data distribu-\ntions/environments [44]. These problems, in fact, are not unique to\nnetworking and have been successfully addressed in other domains\n(e.g., NLP [10, 45] and robotics [23, 112]) by introducing LLMs as\nthe foundation models to solve various downstream tasks. Hence,\nthis work takes a pioneering step by systematically introducing\nLLMs into networking to address these problems.\n2.2 Large Language Models\nThe advent of large language models (LLMs) such as ChatGPT [72],\nPaLM [15], Llama2 [92] and OPT [108] has profoundly revolution-\nized the field of natural language processing (NLP). These LLMs,\nACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia Wu et al.\nTable 1: Information of three learning-based algorithm use cases in the networking area.\nTask DNN Input DNN Output Objective Learning\nParadigm\nViewport\nPrediction (VP)\ntime-series: historical viewports;\nimage: video content information future viewports minimize error between predicted\nand actual viewports SL\nAdaptive Bitrate\nStreaming (ABR)\ntime-series: historical throughputs, delay;\nsequence: chunk sizes at different bitrates;\nscalar: current buffer length\nbitrate selected for the next\nvideo chunk\nmaximize user‚Äôs Quality of\nExperience (QoE) RL\nCluster Job\nScheduling (CJS)\ngraph: DAGs describing dependency and\nresource demands of job execution stages\njob stage to run next, number of\nexecutors allocated to the stage minimize job completion time RL\npre-trained over large public corpora (e.g., wikis and books) to\nacquire extensive knowledge, have demonstrated remarkable capa-\nbility across a wide range of applications that largely affect our daily\nlife, including dialogue systems [31], step-by-step reasoning [45],\nand even code generation [13].\nLLMs are essentially large DNNs built on top of Transformer [94],\nthe de facto standard architecture for sequence modeling in NLP\ntasks. They model the inputs and outputs as sequences of tokens\nrepresenting sub-words in NLP (e.g., a word ‚Äúawesome‚Äù can be split\ninto two tokens: ‚Äúaw‚Äù and ‚Äúesome‚Äù). Specifically, they take as input\na sequence of tokens, and generate another sequence of tokens\nas answer with the assistance of three key components: tokenizer,\nvocabulary and language modeling (LM) head . Figure 1 illustrates the\nanswer generation mechanism of LLM. Given an input sentence, the\ntokenizer splits it into a list of tokens. Then the vocabulary is used\nto map each token into an embedding vector that can be understood\nand effectively processed by LLM. Afterwards, LLM encodes these\ntoken embeddings into high-level features, which are subsequently\npassed to the LM head to predict the probability distribution of next\ntoken. Note that the output tokens are generated one by one in the\nautoregressive manner [46, 61]. Both the sequence of input tokens\nand previously generated tokens are repeatedly fed into the LLM\nto predict the next token, until an end-of-sentence token (<EOS>)\nis emitted.\n2.3 Domain-Adapted LLMs\nThe impressive performance of LLMs has sparked pioneering re-\nsearch to adapt LLMs for other domains [ 23, 55, 59, 112]. For ex-\nample, PaLM-E [ 23] adapts PaLM [ 15] to generate step-by-step\ncontrolling commands for robotic manipulation. ESMFFold [ 55]\nshowcases the successful applications of LLM in biological fields,\nwhich leverages LLM to predict atomic-level protein structure.\nInspired by these promising outcomes, this work explores the\nadaptation of LLMs for networking to address the limitations of ex-\nisting learning-based algorithms, hoping to pave the way for more\nsustainable design philosophy of networking solutions. Unfortu-\nnately, although some prior studies have showcased that LLMs are\ncapable of generating some technical documents for networking\n(e.g., description documents of digital twin for data centers [51]),\nnone of the existing works provide an in-depth investigation on\nwhether and how LLMs can be adapted to solve networking tasks.\nHence, this work tries to bridge this research gap and proposes an\nLLM adaptation framework for networking.\nLarge Language Model (e.g., GPT-3, Llama2)\nTokenizer\n‚Äúthe   earth   average   radius   is‚Äù\n‚Äú6‚Äù ‚Äú371‚Äù ‚Äúkm‚Äù <EOS>\nLanguage Modeling Head\nText\nToken\nToken\nEmbedding\nVocabulary\n‚Äúthe‚Äù ‚Äúearth‚Äù ‚Äúaverage‚Äù ‚Äúradius‚Äù ‚Äúis‚Äù ‚Äú6‚Äù ‚Äú371‚Äù ‚Äúkm‚Äù\nLLM Output Features\nAnswer: ‚Äú6371   km‚Äù\nFigure 1: Illustration of the token-based answer generation mecha-\nnism of LLMs.\n3 MOTIVATION\nIn this section, we identify the key challenges of LLM adaptation\nfor networking, which motivate the design of NetLLM. We use\nthe following three tasks (summarized in Table 1) to make our\ndiscussion concrete:\n‚Ä¢Viewport prediction (VP) serves as a fundamental building\nblock of the emerging streaming systems of immersive videos\n(e.g., 360‚ó¶videos [33] and volumetric videos [37]), where only\nthe video content within the viewer‚Äôs viewport (the region visible\nto viewer) is streamed in high quality to reduce the bandwidth\nconsumption of video transmission [33, 57]. To accomplish this,\nthe VP model predicts viewer‚Äôs future viewport positions based\non historical viewports, and potentially incorporates video con-\ntent information (e.g., video frame) to enhance prediction perfor-\nmance [85, 98]. The goal of VP is to minimize the error between\nthe predicted and viewer‚Äôs actual viewports.\n‚Ä¢Adaptive bitrate streaming (ABR) utilizes a RL model to dy-\nnamically adjust chunk-level bitrates based on the perceived net-\nwork conditions and playback buffer length during the streaming\nsession of a video [44, 103]. The objective of ABR is to maximize\nuser‚Äôs Quality of Experience (QoE), which is quantified by factors\nsuch as chunk bitrate, bitrate fluctuation, and rebuffering time.\n‚Ä¢Cluster job scheduling (CJS) trains a RL scheduler to schedule\nincoming jobs within the distributed computing cluster [63, 78].\nEach job is represented as a directed acyclic graph (DAG), which\ndescribes the dependency of each execution stage of the job and\nNetLLM: Adapting Large Language Models for Networking ACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia\nPrompt\nLearning\nNetLLM3\n6\n9\nAvg. Mean Absolute\nError (Degrees)\nBaseline\nTRACK\nT oken\nPrediction\nNetLLM90\n95\n100\nFraction of Valid\nAnswers (%)\n100%\nT oken\nPrediction\nNetLLM0\n1\n2\n3\n4\nAvg. Ans. Generation\nTime (Seconds)\nResponse\nDeadline\nFigure 2: Illustration of the ineffectiveness for some natural alterna-\ntives with VP task as the example. Left: Prompt learning [ 60, 68] that\ntransforms data into textual prompts achieves sub-optimal perfor-\nmance, while NetLLM with a multimodal encoder to encode task input\ndata effectively outperforms baseline. Middle, Right : Token-based\nprediction with LM head fails to guarantee valid answers and pro-\nduce stale responses, while NetLLM efficiently addresses these issues\nwith the networking head module.\nthe resource demand of each stage. The primary task of RL sched-\nuler is to select the next stage of a job to run and allocate a number\nof executors (computing resources) to that particular stage. The\nobjective is to minimize the average job completion time, so that\nthe system-level utilization of computing resources is optimized\nwithin the cluster.\nWhy these tasks? We choose these tasks for several reasons.\n‚Ä¢First, they cover the two learning paradigms commonly adopted\nin networking, i.e., SL for prediction tasks (VP) and RL for decision-\nmaking tasks (ABR and CJS).\n‚Ä¢Second, they include both centralized control (CJS) and distributed\ncontrol (ABR) networking tasks. Specifically, the CJS scheduler is\nresponsible for the entire cluster, while the ABR client indepen-\ndently selects bitrates without considering other clients.\n‚Ä¢Finally, they involve diverse input modalities, covering the pri-\nmary data modalities in many networking tasks. For example,\nmany continuous signals in network adaptation problems (e.g.,\npacket loss rate in congestion control [106]) are represented as\nscalar data.\nIn particular, we choose VP as it encompasses multiple input modali-\nties and requires cross-modality fusion, making it more challenging\nfor LLM adaptation than other prediction tasks that generally in-\nvolve single input modality (e.g., bandwidth prediction [64]). The\ncharacteristics of these tasks ensure that our subsequent discussion is\nrepresentative and applicable to a wide range of networking scenarios.\nChallenge 1: Large modality gap. As shown in Table 1, different\nnetworking tasks have different input information of diverse modal-\nities, spanning from time-series network throughputs to DAG data.\nHowever, most LLMs are designed to accept plain text as inputs.\nDue to the substantial modality gap, it is impractical to directly feed\nthe input information of networking tasks into LLMs for effective\nprocessing.\nOne seemingly natural approach to tackle this challenge isprompt\nlearning [60, 68, 81], which transforms data into texts through a\nprompt template. Specifically, it designs a textual template that\nprovides the information of task specifications, and uses this tem-\nplate to transform task inputs into textual prompts to instruct the\nLLM to generate desired answers that solve the tasks. While this\napproach shows promise in other fields, it falls short in the network-\ning domain due to the following reasons. First, it is not feasible to\ntransform data of complex modalities (e.g., image in VP and DAG\n0 20 40 60\nTraining Time on ABR (Hours)\nNetLLM\nStandard RL 28.47h(47.73%)\n29.03h(99.63%)\n28.47h\n29.03h\n31.18h(52.27%)\n0.11h(0.37%)\n31.18h\n0.11h\n0 50 100 150\nTraining Time on CJS (Hours)\nNetLLM\nStandard RL 87.33h(60.75%)\n88.51h(98.79%)\n87.33h\n88.51h\n56.42h(39.25%)\n1.08h(1.21%)\n56.42h\n1.08h\nParameter Update Experience Collection\nFigure 3: Using standard RL techniques [ 86, 93] to adapt LLM for\nRL-based decision-making tasks (ABR and CJS) incurs high training\ntime due to the active environment interaction for experience collec-\ntion. NetLLM eliminates this time-consuming process by designing an\nefficient data-driven adaptation pipeline in the DD-LRNA scheme.\nFull\nFine-tune\nNetLLM0\n50\n100\nFraction of Trainable \nParameters (%)\n100%\n0.31%\nFull\nFine-tune\nNetLLM0\n30\n60\nGPU Memory\nConsumption (GB)\n65.88GB\n27.24GB\nFull\nFine-tune\nNetLLM0\n4\n8\nTraining Time\n(Hours)\n7.9h\n6.7h\nFigure 4: Illustration of the high adaptation costs of full-parameter\nfine-tune [16, 92] on the VP task. The DD-LRNA scheme of NetLLM\nefficiently reduces the costs by introducing a set of small trainable\nlow-rank matrices.\nin CJS) into textual prompts. Second, even if certain data can be\nconverted into texts (e.g., time-series viewports in VP), we empiri-\ncally observe that such transformation can be sub-optimal. To give\na concrete example, we use this approach to adapt Llama2-7B [92]\nfor the VP task. We design a template to encapsulate viewport data\ninto prompts (we exclude video content information since it can-\nnot be incorporated into prompts directly). Based on this prompt\ntemplate, we instruct Llama2 to predict the future viewports in the\nnext 1 second based on the historical viewports in the last 1 second\n(detailed setup of this measurement can be found in ¬ßA.1).\nFigure 2 (left) reports the performance of prompt-learning-adapted\nLlama2 in terms of mean absolute error (MAE). Lower MAE means\nbetter prediction performance. As shown, under the prompt learn-\ning framework, Llama2 achieves poor performance with 11.1%\nhigher MAE than the state-of-the-art VP model TRACK [85]. This\ncould be attributed to the significant modality gap between text\nand time-series data, as textual representation may not effectively\nexpress the characteristics of time-series data. For instance, the\ntime-varying patterns are typically not found in natural text.\nChallenge 2: Inefficiency of token-based answer generation.\nAs introduced in ¬ß2.2, by default, LLMs generate answers with LM\nhead by predicting next tokens in an autoregressive manner. This,\nhowever, exhibits two main drawbacks in the networking area.\nFirst, the uncertainty of token prediction increases the risk of\nLLM-generated answers to be physically invalid, a phenomenon\nknown as hallucination [41, 49]. To quantify this issue, we calculate\nthe fraction of valid answers (see ¬ßA.1) when adapting Llama2 for\nVP task based on token prediction. Figure 2 ( middle) shows that\nLlama2 fails to guarantee the generated answers to be 100% valid\nwhen using token prediction. This raises concerns regarding the\nreliability of deploying LLMs for real-world network systems.\nACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia Wu et al.\nSecond, due to the sub-word nature of tokens, a single word may\nspan multiple tokens. In consequence, LLMs often require multiple\ninferences to generate a single answer, as depicted in Figure 1. This\ncan produce delayed or even stale answers that fail to quickly adapt\nto the system changes. For instance, we measure the average time\nfor Llama2 to generate a single answer for the VP task. Figure 2\n(right) shows that it takes up to 3.84s for per-answer generation,\nwhich significantly exceeds the 1-second response deadline required\nfor predicting future viewports in the next second.\nNote that the above problems are not unique to the networking\narea. Nevertheless, they reduce the efficiency of LLMs in network-\ning, since networking tasks often require high reliability and quick\nresponsiveness [11, 66].\nChallenge 3: High adaptation costs. Many networking tasks\nsuch as ABR [103] and CJS [78] employ RL to solve complex system\noptimization problems, which involve active interaction between\nthe environments (e.g., network environments with varying band-\nwidth or workload patterns) to collect experiences for reward opti-\nmization. In this context, simply fine-tuning LLMs for these tasks\nbased on standard RL techniques (e.g., PPO [86] and DQN [93]) will\nintroduce prohibitive adaptation costs due to the time-consuming\nprocess of environment interaction. To be more specific, we mea-\nsure the amount of time of fine-tuning Llama2 over ABR (CJS) task\nfor 10000 (100) iterations with standard RL. Each iteration involves\ninteracting with the environment for one episode3 to collect experi-\nences, followed by optimizing rewards based on these experiences.\nAs depicted in Figure 3, the experience collection caused by active\nenvironment interaction introduces additional 31.18h (56.42h), ac-\ncounting for 52.27% (39.25%) of the total training time on ABR (CJS)\ntask. While this problem has been observed in prior works [62, 63],\nit becomes intractable in the context of adapting LLMs for network-\ning given their large parameter sizes.\nThe adaptation costs become even more expensive when fine-\ntuning the full parameters of LLMs [21, 29]. As shown in Figure 4,\nfully fine-tuning Llama2-7B for the VP task consumes 65.88GB GPU\nmemory and 7.9h of training time. This is because full-parameter\nfine-tune requires extensive memory and computation to store and\nmaintain the training states due to the large parameter size. In\nfact, another practical drawback associated with full-parameter\nfine-tune is that it may disrupt the pre-trained knowledge of LLM\nsince it updates all the parameters of LLM. As a result, this may\nprevent a single LLM to share across different networking tasks.\nSummary. In a nutshell, we observe three challenges of LLM adap-\ntation for networking.\n‚Ä¢The large modality gap makes the input information of network-\ning tasks incompatible with the LLM, preventing the LLM from\neffectively processing task inputs.\n‚Ä¢The default token-based answer generation of the LLM exhibits\ninefficiency in the networking domain, which reduces the relia-\nbility and responsiveness for the LLM to serve network systems.\n‚Ä¢The large parameter size of the LLM leads to significant costs to\nacquire domain-specific knowledge for networking, especially\nfor RL-based tasks which require environment interaction.\n3An episode in RL refers to a single round of a RL model to interact with the environ-\nment from the initial state to final state. For example, in ABR task, the RL model starts\nstreaming the first video chunk (initial state) and stops until all chunks are downloaded\n(final state).\n4 NETLLM DESIGN\nIn this section, we elaborate the detailed design of NetLLM, an inno-\nvative LLM adaptation framework for networking that efficiently\nsolves the aforementioned challenges. As shown in Figure 5,NetLLM\ncomprises three main building blocks:\n‚Ä¢Multimodal encoder. NetLLM solves challenge 1 by designing\nan encoder module to encode multimodal input data of network-\ning tasks into token-like embeddings, which can be effectively\nprocessed by the LLM.\n‚Ä¢Networking head. To address challenge 2 , NetLLM replaces the\nLM head for token prediction with different networking heads,\nwhich enable direct generation of a valid answer for specific tasks\nin a single inference.\n‚Ä¢Data-driven low-rank networking adaptation (DD-LRNA).\nTo reduce the costs of adaptation, NetLLM develops an efficient\nDD-LRNA scheme to solve challenge 3 . It incorporates a data-\ndriven pipeline to adapt LLMs for both prediction and decision-\nmaking tasks, and introduces different low-rank matrices to learn\ndomain knowledge to further minimize the adaptation costs.\nNote that during fine-tune, the parameters of the LLM are frozen\nto preserve pre-trained knowledge, while the multimodal encoder,\nnetworking heads, and low-rank matrices are tunable to optimize\nperformance for different tasks. The details of each blocks are de-\nscribed as follows.\n4.1 Multimodal Encoder\nThe key to processing task-specific information is to project the\nmultimodal input data into token space to enable efficient utiliza-\ntion by the LLM. To achieve this, we design a multimodal encoder\nto automatically learn such projection. Figure 6 illustrates the ar-\nchitecture of this module, which incorporates two blocks.\nFeature encoder. We first employ different feature encoders to\nextract features from raw input data of various modalities. A key\ndesign consideration here is the choice of feature encoder for each\nmodality. Instead of handcrafting feature encoders from scratch,\nwhich entails high model engineering costs, we reuse the exist-\ning well-designed encoders tailored for specific modalities. For\nexample, Decima [63] for CJS task develops a graph neural net-\nwork (GNN) [101] encoder to extract features from DAG informa-\ntion, and Vision Transformer (ViT) [22] has been widely used to\nencode images into hidden features. These designs are precious\nresearch outcomes and prove effective in processing specific modal-\nities. Therefore, we cherish and efficiently utilize these designs by\nintegrating them into our multimodal encoder module. Specifically,\nwe leverage the following feature encoders to encode different data\nmodalities involved in networking: ViT for images, 1D-CNN for\ntime-series and sequence data (e.g., historical throughputs and fu-\nture chunk sizes at different bitrates in ABR), fully connected layer\nfor scalar data (e.g., buffer occupancy in ABR), and GNN for graph\ninformation (e.g., DAGs in CJS).\nLinear projection. The features extracted by encoders, however,\nmay not align to the token space. For instance, features extracted\nby ViT have a dimension of 768 [ 22], while Llama2 requires an\ninput dimension of 4096 [ 92]. To address this issue, we design\na set of trainable linear layers to project the features extracted\nby different encoders. These layers automatically learn a highly\nNetLLM: Adapting Large Language Models for Networking ACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia\nImage\nGraph\nTime-Series\n‚Ä¶\nToken-Like Embedding\n‚Ä¶\nLarge Language Model\nLow-Rank \nMatrices\nLow-Rank \nMatrices\nLow-Rank \nMatrices\n++\n++\n++\n¬ß4.3 Data-Driven Low-Rank \nNetworking Adaptation ¬ß4.2 Networking Head\nCluster Job \nScheduling\nAdaptive Bitrate \nStreaming\nViewport \nPrediction\n¬ß4.1 Multimodal \nEncoder\nCNN\nLinear\nGNN\nLinear\nViT\nLinear\nTrainable\nFrozen\n‚Ä¶\nFigure 5: NetLLM consists of three core components: multimodal encoder to encode task inputs, networking head to generate task-specific\nanswers and data-driven low-rank networking adaptation to efficiently learn domain knowledge for networking. The framework is illustrated\nwith three tasks: VP, ABR and CJS, but all ideas can be easily applied to other networking tasks.\nLarge Language Model\nLayer Normalization\nùëìùëñùëöùëî\nViT\nImage\nùëìùë£ùëù\n1D Convolution\nViewport \nùë£1\nViewport \nùë£‚Ñé\n‚ãØ\nLinear Projection\nFeature Encoder\nMultimodal Data\nToken-Like Embedding\nExample of Viewport Prediction\nFigure 6: Illustration of the multimodal encoder of NetLLM to encode\nmultimodal data.\nefficient mapping from feature space to token space, producing a\nset of token-like embeddings that can be effectively utilized by the\nLLM. Additionally, we further enhance the projection process by\nnormalizing the output embeddings with layer normalization [7]\nto ensure training stability.\nExample. Figure 6 illustrates the multimodal encoder with VP task\nas a concrete example. The ViT and 1D convolution layer (1D-CNN)\nare first used to encode the image and time-series viewport data,\nrespectively. Next, the extracted features are projected into token-\nlike embeddings with separate linear projection layers. Finally, all\nembeddings are normalized through layer normalization to ensure\ntraining stability, and passed to the LLM for further processing.\nFigure 2 ( left) also provides statistical results to confirm the ef-\nfectiveness of multimodal encoder to project task-specific input\ndata. As shown, empowered by this module, NetLLM significantly\noutperforms prompt-learning based data processing scheme [60],\nwith the average reduction of 19.7% MAE for the VP task.\n4.2 Networking Head\nWith the multimodal encoder, the LLM is capable to extract high-\nlevel features that encompass important task-specific information\nfrom input data of various modalities. These features are then fed\ninto the networking head for direct answer generation. Specifically,\nthe networking head is designed as a trainable linear layer to predict\ntask-specific answers based on LLM output features, which can be\nflexibly customized according to specific networking tasks. Unlike\nLM head, the networking head constrains the answer generation\ninto the valid range of possible answers, such as the range of valid\nviewport coordinates in VP and the set of video bitrates in ABR.\nIn this way, all answers generated by LLM are guaranteed to be\nvalid, ensuring the reliability of LLM for networking. Moreover,\nLLM can generate one answer within a single round of inference,\nthus significantly reducing the generation latency.\nExample. Figure 7 compares the difference between LM head for\ntoken prediction and networking head with ABR task as the ex-\nample. As depicted, the LM head generates answers by predicting\nnext tokens autoregressively, which requires multiple rounds of\ninference and thus entails high generation latency. Besides, due to\nthe inherent uncertainty of token-based prediction, the generated\nanswers may be invalid, such as a bitrate that does not exist. In\ncontrast, the networking head is specially designed to predict the\nprobability distribution of bitrates, enabling direct answer gener-\nation within a single round of inference (e.g., the bitrate with the\nmaximum probability is chosen as the answer in Figure 7). Fur-\nthermore, since the outputs of networking head are limited to the\ndiscrete set of candidate bitrates, the generated answers are guar-\nanteed to be always valid. The superiority of networking head over\nLM head is also illustrated in Figure 2 (middle, right ), where NetLLM\nuses it to ensure the validness of answers and quickly produces\nanswers before the response deadline for VP task.\n4.3 Data-Driven Low-Rank Networking\nAdaptation\nIn this part, we delve into the detailed design of the proposed\ndata-driven low-rank networking adaptation (DD-LRNA) scheme\nto efficiently fine-tune LLMs to acquire domain knowledge. The\nDD-LRNA comprises the following two core designs: i) a data-\ndriven adaptation pipeline for both prediction and decision-making\nnetworking tasks; ii) a low-rank adaptation approach to constrain\nthe fine-tuning process to a small number of parameters for more\nefficient adaptation.\nACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia Wu et al.\nLarge Language Model\n‚Äú7‚Äù\n‚Äú0‚Äù\n<EOS>\n700 kbps\n(Invalid)‚Äú0‚Äù\nABR Head\nLM Head\n750 kbps\n(Valid)\n750 kbps\n2850 kbps\n4300 kbps\nVideo chunks in \nDifferent Bitrates\ndownload\ndownload\nFigure 7: Comparison between LM head and networking head with\nABR task as an example. For illustration, we assume that video\nchunks are encoded into three bitrate versions {750,2850,4300}kbps.\nData-driven networking adaptation. In the case of prediction\nnetworking tasks, it is straightforward to fine-tune the LLM through\nthe standard SL data-driven training pipeline. Specifically, given\na task-specific dataset Dùë†ùëô = {X,Y}of inputs ùíô ‚ààX and labels\nùë¶ ‚ààY, we leverage the multimodal encoder to encode input data ùíô,\nelicit prediction results ÀÜùë¶from the LLM with networking head, and\ncompute loss for parameter update by:\nùêøùë†ùëô = ùêπùë†ùëô(ùë¶, ÀÜùë¶) (1)\nwhere ùêπùë†ùëô is the loss function which can be cross entropy (CE) for\nclassification tasks (e.g., traffic classification [73]) or mean square\nerror (MSE) for regression tasks (e.g., bandwidth prediction [ 64]\nand VP [85]).\nNevertheless, when it comes to the decision-making tasks, the\ntraditional RL training pipeline becomes impractical due to the\ntime-consuming process of the interaction between the LLM and\nenvironments. To tackle this challenge, we design our RL adaptation\npipeline based on the efficient data-driven RL techniques [79, 106],\nwhich tackle the same problem as traditional RL but without the\nneed of environment interaction. Specifically, we collect the experi-\nence dataset with any existing (non-LLM) networking algorithms,\nand exploit this dataset to fine-tune the LLM for reward optimiza-\ntion. The intuition behind is to let the LLM learn a better policy by\nobserving the behaviors of existing algorithms [47]. That means,\nthe LLM should not only learn from the good actions that lead to\ngood performance, but also try to understand why some actions\nwill lead to bad performance [106].\nIt is worth noting that, unlike traditional RL that requires periodic\nrefreshing of the experience dataset during training [86, 93], our ap-\nproach allows the dataset to be collectedonly once and used through-\nout the entire training process. As a result, the costs of adapting\nLLMs for RL-based networking tasks can be significantly reduced\n(e.g., with 51.1%/37.7% reduction of training time for ABR/CJS task\nunder the same training iterations, as depicted in Figure 3).\nThe proposed RL adaptation pipeline is described as follows,\nwhich is built on top of the Transformer based data-driven RL [12,\n40] that caters to the sequence modeling nature of Transformer.\nGiven a RL-based networking task, we first employ an existing\npolicy (e.g, Decima [63] for CJS) to collect an experience dataset\nwhich consists of experience trajectories: Dùëüùëô = {ùúè1,¬∑¬∑¬∑ ,ùúè|Dùëüùëô |}.\nEach trajectory ùúè = {ùëüùë°,ùíîùë°,ùíÇùë°}ùëá\nùë°=1 is composed of rewards ùëü, states\nùíî and actions ùíÇ, where ùëá denotes the episode length. For each\nsample in a trajectory {ùëüùë°,ùíîùë°,ùíÇùë°} ‚ààùúè, we substitute the reward\nLarge Language Model\nLow-Rank\nMatrices\nùê¥\nùêµ\nLow-Rank\nMatrices\nùê¥\nùêµ\n+\n+\ninput ùíô\nlabel ùë¶\nDataset\nreturn ùëÖ\nstate ùíî\nDataset\naction ùíÇ\nprediction ‡∑úùë¶\ngenerated \naction ‡∑ùùíÇ\nCompute Loss ùêøùë†ùëô\nupdate\nlabel ùë¶\nCompute Loss ùêøùëüùëô\nupdate\naction ùíÇ\nCollect \nExperiences\nExisting Policy \nEnvironment\nTrainable\nFrozen\nAdaptation for \nPrediction Tasks\nAdaptation for \nDecision-Making Tasks\nFigure 8: Illustration of the data-driven low-rank networking adap-\ntation scheme of NetLLM.\nùëüùë° by return ùëÖùë° = √çùëá\nùëñ=ùë°ùëüùëñ representing the cumulative rewards\nexpected to receive from state ùíîùíï . Additionally, considering that\nthe state or action in some tasks may be constituted by multiple\npieces of information (e.g., the state in ABR includes past network\nthroughput and playback buffer length), we further discretize each\nstate and action: ùíîùë° = {ùë†1\nùë°,¬∑¬∑¬∑ ,ùë†ùëõ\nùë° },ùíÇùë° = {ùëé1\nùë°,¬∑¬∑¬∑ ,ùëéùëö\nùë° }. This leads to\nthe following representation of trajectory:\nùúè = {ùëÖùë°,ùë†1\nùë°,¬∑¬∑¬∑ ,ùë†ùëõ\nùë°,ùëé1\nùë°,¬∑¬∑¬∑ ,ùëéùëö\nùë° }ùëá\nùë°=1 (2)\nBased on the above trajectory representation, we then fine-tune\nthe LLM to learn the distribution of returns. At each training step,\nwe randomly sample a sequence of data from the dataset:\nùëë = {ùëÖùëñ,ùë†1\nùëñ,¬∑¬∑¬∑ ,ùë†ùëõ\nùëñ ,ùëé1\nùëñ,¬∑¬∑¬∑ ,ùëéùëö\nùëñ }ùë°\nùëñ=ùë°‚àíùë§+1 ‚ààDùëüùëô (3)\nwhere ùë§ is the context window to facilitate the learning of return\ndistribution. Next, we feed data ùëë to the LLM to generate actions\n{ÀÜùëé1\nùëñ,¬∑¬∑¬∑ ,ÀÜùëéùëö\nùëñ }ùë°\nùëñ=ùë°‚àíùë§+1. Notably, we consider return and each piece\nof state, action information as different modalities, and process\nthem separately. Finally, the training loss is calculated by:\nùêøùëüùëô = 1\nùë§\nùë§‚àëÔ∏Å\nùëñ=1\nùëö‚àëÔ∏Å\nùëó=1\nùêπùëüùëô(ùëéùëó\nùëñ,ÀÜùëéùëó\nùëñ) (4)\nwhere ùêπùëüùëô measures the difference between action ùëéùëó\nùëñ and the gen-\nerated action ÀÜùëéùëó\nùëñ, which can be CE for discrete actions or MSE for\ncontinuous actions.\nThe underlying rationale of the above training procedure is to\ntrain the LLM to model the distribution of returns conditioned on\nspecific states, so that once trained, it can be used to generate a\nseries of actions that achieve the desired returns [12]. In particular,\nduring the inference stage, we specify a target return based on\nthe desired performance (e.g., maximum possible return to achieve\nexcellent performance) to trigger the LLM to generate answers.\nLow-rank networking adaptation. With the data-driven adap-\ntation pipeline in place, the LLM can now be fine-tuned for net-\nworking adaptation. Given the pre-trained parameters of the LLM\ndenoted as Œ¶0, the goal of fine-tuning is to search for the parameter\nupdate ŒîŒ¶ such that the resulting parameters Œ¶ = Œ¶0 +ŒîŒ¶ are\noptimized for the specific networking task. Nevertheless, due to\nthe large parameter size of the LLM, directly fine-tuning the full\nparameters entails prohibitive computation costs, as the dimension\nof learned parameters |ŒîŒ¶|is equal to |Œ¶0|(e.g., |Œ¶0|= 540 billion\nfor PaLM [15]).\nNetLLM: Adapting Large Language Models for Networking ACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia\nNetLLM\nAdaptor\nSL codebase:\n-   Simulator\n-   Dataset\nRL codebase:\n-   Simulator\n-   Policies\nNetLLM Integration\nDataset = \nRL_Collect(Policies, EnvSettings, NumIters)\nPerformance = \nTest(Adapted_LLM, EnvSettings, NumIters)\nAdapted_LLM = \nAdapt(LLM, Dataset, NumIters)\nFigure 9: Components and interfaces needed to integrate NetLLM with\nan existing SL/RL codebase for LLM adaptation.\nTo combat the above limitation, we freeze the parameters of\nLLM and introduce additional low-rank matrices to approximate\nthe changes needed in the LLM parameters to learn domain-specific\nknowledge. The underlying insight is that the parameter changes\nduring adaptation (i.e., ŒîŒ¶) reside on an intrinsic low rank [4, 38].\nTherefore, for each pre-trained matrix ùëä0 ‚ààŒ¶0 of dimension ùëë√óùëò,\nwe hypothesize the existence of a low rank ùëü ‚â™min{ùëë,ùëò}and\nconstruct two low-rank matrices ùê¥,ùêµ of dimension ùëë√óùëü,ùëü √óùëò to\napproximate the update ofùëä0, i.e.,ùëä = ùëä0 +Œîùëä = ùëä0 +ùê¥ùêµ. During\nadaptation, ùëä0 is frozen and all parameter update is constrained on\nmatrices ùê¥and ùêµ. As shown in Figure 4, this significantly reduces\nthe fine-tuning costs with the reduction of 60.9% GPU memory and\n15.1% training time, since the low-ranks only introduces 0.31% of\ntrainable parameters. Another benefit of this approach is that the\npre-trained knowledge of the LLM is preserved as each ùëä0 ‚ààŒ¶0\nis retained without any update. Hence, the same LLM can serve as\nthe foundation model for different tasks, and train different copies\nof ùê¥,ùêµ to acquire different domain knowledge.\nPutting all together. The DD-LRNA scheme is briefly summarized\nin Figure 8. As shown, we freeze the parameters of LLM and allocate\ndifferent trainable low-rank matrices for each task. These matri-\nces are then fine-tuned over a dataset to acquire domain-specific\nknowledge. For decision-making tasks, the dataset is collected by\nusing existing algorithms to interact with environments. At each\nfine-tuning step, we sample a batch of data from the dataset, feed\ndata to the LLM to generate answers, compute loss according to\nequation (1) for prediction tasks or equation (4) for decision-making\ntasks, and update the low-rank matrices through gradient descent.\nNote that in addition to low-rank matrices, the gradients are also\npropagated to update the parameters of multimodal encoder and\nnetworking head for performance optimization.\n4.4 Implementation\nNetLLM is fully implemented in Python and Bash, and can be easily\nintegrated into existing SL/RL codebases to adapt LLMs for net-\nworking tasks. As depicted in Figure 9, it interacts with an existing\ncodebase with three APIs. First, Adapt triggers NetLLM to use the\nprovided dataset to adapt the LLM to learn domain-specific knowl-\nedge for the target task, and returns the snapshot of the adapted\nLLM. Second, Test evaluates the performance of the adapted LLM\non the testing environments generated with the given simulation\nsettings. Finally, for RL-based tasks without an available dataset\nfor adaptation, NetLLM offers the RL_Collect API to collect the ex-\nperience dataset by using the given RL policies to interact with the\nenvironments. Afterwards, the collected dataset can be plugged\ninto the Adapt API to adapt the LLM.\nLR VelocityTRACKNetLLM\n0\n5\n10\n15\n20Avgerage MAE (Degrees)\nVP ( ‚Üì Better)\nBBA MPC GENETNetLLM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Avgerage QoE Scores\nABR ( ‚Üë Better)\nFIFO Fair DecimaNetLLM\n0\n20\n40\n60\n80Avgerage JCT (Seconds)\nCJS ( ‚Üì Better)\n(a) Average performance with different random seeds\n0 20 40 60 80\nMAE (Degrees)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0CDF\nBetter\nVP\nLR\nVelocity\nTRACK\nNetLLM\n0 1 2 3\nQoE Scores\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0CDF\nBetter\nABR\nBBA\nMPC\nGENET\nNetLLM\n0 100 200 300\nJCT (Seconds)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0CDF\nBetter\nCJS\nFIFO\nFair\nDecima\nNetLLM\n(b) CDF Performance\nFigure 10: Comparing NetLLM-adapted Llama2 for VP, ABR, and CJS,\nwith baselines in testing environments generated with the same\nsettings as training environments.\nWe have integrated NetLLM into three existing codebases for\nVP [99], ABR [103], and CJS [30], and implemented the above APIs\nbased on the functionalities provided in the codebases. More details\nof implementation are provided in ¬ßA.2.\n5 EVALUATION\n5.1 Setup\nSimulation setup. By default, we utilize Llama2-7B [ 92] as the\nfoundation LLM. We then use NetLLM to adapt Llama2 for three\nnetworking tasks VP [99], ABR [103], and CJS [63]. We generate\ndifferent simulation environments with real-world and synthetic\ndatasets for training and testing, following the settings described in\n¬ßA.4 and Table 2, 3, 4. These settings cover the key factors that affect\nthe model performance. For instance, in ABR task, our environment\nsettings consider the range and changing frequency of bandwidth\nas well as video bitrates.\nBaselines. We implement three state-of-the-art learning-based al-\ngorithms for comparison: TRACK [85] for VP, GENET [103] for ABR\nand Decima [63] for CJS. We choose these baselines because they\nhave open source implementation. In addition, we also compare\nwith other rule-based (non-DNN) algorithms for each task: linear\nregression (labeled ‚ÄúLR‚Äù) [80] and velocity-based prediction (labeled\n‚ÄúVelocity‚Äù) [24] for VP, BBA [39] and MPC [107] for ABR, first-in-\nfirst-out (labeled ‚ÄúFIFO‚Äù) and fair scheduling (labeled ‚ÄúFair‚Äù) [87]\nfor CJS. Appendix ¬ßA.3 provides the brief overview of all baselines.\nMetrics. For performance metrics, we consider mean absolute\nerror (MAE) for VP, Quality of Experience (QoE) scores for ABR,\njob completion time (JCT) for CJS. Lower MAE, higher QoE and\nlower JCT indicate better performance. In particular, following\nthe same formula in [62, 103], QoE is calculated as the weighted\nlinear combination of bitrate, rebuffering time and bitrate changes.\nAppendix ¬ßA.6 provides the details of three metrics.\nHardware settings. We conduct experiments on a Linux server\nequipped with eight Intel(R) Xeon(R) Gold 5318Y CPUs and two\nNVIDIA 40GB A100 GPUs.\nACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia Wu et al.\nLR Velocity TRACK NetLLM\n0\n10\n20\n30\n40MAE (Degrees)\nUnseen Setting1\nLR Velocity TRACK NetLLM\n0\n10\n20\n30\n40MAE (Degrees)\nUnseen Setting2\nLR Velocity TRACK NetLLM\n0\n10\n20\n30\n40\n50MAE (Degrees)\nUnseen Setting3\n(a) VP ( ‚ÜìBetter)\nBBA MPC GENETNetLLM\n‚àí0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nQoE Scores\nUnseen Setting1\nBBA MPC GENET NetLLM\n1.2\n1.5\n1.8\n2.1\n2.4QoE Scores\nUnseen Setting2\nBBA MPC GENET NetLLM\n0.9\n1.2\n1.5\n1.8\n2.1QoE Scores\nUnseen Setting3\n(b) ABR ( ‚ÜëBetter)\nFIFO Fair DecimaNetLLM\n0\n60\n120\n180\n240JCT (Seconds)\nUnseen Setting1\nFIFO Fair DecimaNetLLM\n0\n30\n60\n90\n120JCT (Seconds)\nUnseen Setting2\nFIFO Fair DecimaNetLLM\n0\n50\n100\n150\n200JCT (Seconds)\nUnseen Setting3\n(c) CJS ( ‚ÜìBetter)\nFigure 11: Comparing the generalization performance of NetLLM-\nadapted Llama2 for VP, ABR, and CJS, with baselines in testing\nenvironments generated with settings different from training envi-\nronments. The shape of box shows the distribution and the triangle\nin each box denotes average.\n5.2 General Evaluation\nIn this part, we first compare NetLLM-adapted Llama2 with other\nmethods across three different tasks over the testing environments\nwith the same settings as training environments (see¬ßA.4). In other\nwords, for each task, we adapt Llama2 and train learning-based\nalgorithms over the environment generated with the target setting,\nand test all methods in the new environment from the same setting.\nFigure 10 presents the performance of each method for the corre-\nsponding tasks. As shown in Figure 10,NetLLM-adapted Llama2 con-\nsistently outperforms other methods across all cases. It surpasses\nall baselines by reducing 10.1-36.6% of MAE for VP, improving\n14.5-36.6% of QoE for ABR and reducing 6.8-41.3% of JCT for CJS.\nFigure 10 also provides more detailed results in the form of CDF for\neach task. It can be seen that a large proportion of NetLLM-adapted\nLlama2 is concentrated in the range of lower MAE, higher QoE and\nlower JCT. For instance, for CJS task, the 90th percentile JCT of\nLlama2 is 97.3 seconds, while this value dramatically increases to\n109.3 seconds for Decima, 135.6 seconds for Fair and 187.5 seconds\nfor FIFO. The above outcomes highlight the effectiveness ofNetLLM\nin LLM adaption for networking.\nIt can be seen that there is a clear ranking among the three\nbaselines on each task, where the learning-based algorithms con-\nsistently yield improvement over traditional rule-based algorithms.\nThis can be attributed to the inherent strength of DNNs in fitting\ncomplex functions for prediction. However, the LLM demonstrates\neven more powerful capabilities in function approximation, pat-\ntern mining and long-term planning, owing to its large parameter\nsize and large-scale pre-training. Hence, by effectively utilizing the\nstrengths of LLM to solve networking tasks, NetLLM achieves supe-\nrior performance compared to the other learning-based algorithms.\nIn addition, it is worth mentioning that the performance gain of\nlearning-based algorithms relies on engineering specialized DNN\nmodels for the target tasks. In contrast, NetLLM efficiently utilizes\nthe LLM as the foundation model for task solving. That said, it\nuses the same LLM to solve various networking tasks without any\nfurther modification on the model, thus significantly reducing the\noverhead of model engineering.\n5.3 Generalization\nNext, we evaluate the generalization performance of all methods for\neach task in testing environments generated with various settings\ndifferent from the training environments (see ¬ßA.4). As depicted in\nFigure 11, NetLLM-adapted Llama2 consistently outperforms base-\nlines in terms of average values and distributions across all cases.\nFor instance, compared to the learning-based algorithms, it reduces\nthe MAE by 1.7-9.1%, improves the QoE by 3.9-24.8% and reduces the\nJCT by 2.5-6.8% on average. This suggests that, enabled by NetLLM,\nLlama2 demonstrates superior generalization performance.\nFrom Figure 11, we also notice that learning-based algorithms\ndo not always outperform conventional rule-based algorithms for\nthe ABR task. Figure 12 breaks down the QoE scores of all ABR\nmethods for more detailed analysis. As shown, GENET is surpassed\nby MPC with 5.2%/5.9% lower average QoE on unseen setting 1/2.\nMore specifically, on unseen setting 1, where the streaming video is\ndifferent from training one, GENET fails to optimize video bitrates\nand thus achieves worse performance than MPC. On the other hand,\nGENET struggles to adapt to the dynamic fluctuations of bandwidth\non unseen setting 2, where the testing bandwidth traces change\nmore frequently than training ones. It may inappropriately select\nhigh bitrates when the current bandwidth resources become scarce,\nand thus produces the highest rebuffering time among other meth-\nods. In contrast, NetLLM-adapted Llama2 strikes a good balance\nbetween the three QoE factors and thus achieves the highest QoE\nscores on all settings. These cases exemplify that conventional DNN\nmodels may perform poorly in unseen environments. In compari-\nson, leveraging our NetLLM framework, we can indeed efficiently\nutilize the extensive knowledge of the LLM to achieve stronger\ngeneralization.\nReal-world tests. As a final test of generalization, we evaluate\nNetLLM-adapted Llama2 in a real-world client-server ABR system\nunder different network connections (see ¬ßA.5 for detailed setup).\nThe results are reported in Figure 14. On each network, the adapted\nLlama2 outperforms the baselines. This indicates that the LLM\nadapted by NetLLM is able to generalize to real-world scenarios.\n5.4 Deep Dive\nImportance of pre-trained and domain knowledge. To gain a\ndeeper understanding of why LLMs can be adapted for networking,\nwe investigate the importance of both the pre-trained and learned\ndomain knowledge of LLMs in networking adaptation. We use\nNetLLM: Adapting Large Language Models for Networking ACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia\n0.0\n0.2\n0.4\n0.6\n0.8Normalized Mean Values    QoE ‚Üë   Bitrate ‚Üë  Rebuffering ‚Üì Bitrate Variation ‚Üì\nBBA MPC GENET NetLLM\n(a) ABR Unseen Setting 1\n0.0\n0.2\n0.4\n0.6\n0.8Normalized Mean Values    QoE ‚Üë   Bitrate ‚Üë  Rebuffering ‚Üì Bitrate Variation ‚Üì\nBBA MPC GENET NetLLM (b) ABR Unseen Setting 2\n0.0\n0.2\n0.4\n0.6\n0.8Normalized Mean Values    QoE ‚Üë   Bitrate ‚Üë  Rebuffering ‚Üì Bitrate Variation ‚Üì\nBBA MPC GENET NetLLM (c) ABR Unseen Setting 3\nFigure 12: Comparing NetLLM-adapted Llama2 with baselines for ABR by breaking down their performance on individual QoE factors in\ndifferent unseen environments. Results are normalized through min-max. Arrow ‚Üë/ ‚Üìmeans higher/lower is better.\n0\n5\n10\n15\n20\nAverage MAE (Degrees)\nVP ( ‚Üì Better)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage QoE Scores\nABR ( ‚Üë Better)\n0\n20\n40\n60\n80\nAverage JCT (Seconds)\nCJS ( ‚Üì Better)\nno Pre-trained Knowledge no Domain Knowledge Full Knowledge\nFigure 13: Exploring the importance of pre-trained and learned\ndomain-specific knowledge of LLM in networking adaptation.\nLlama2-7B as the LLM for our exploration. First, we disable the pre-\ntrained weights of Llama2 that represent its pre-trained knowledge,\nrandomly initialize its weights and train it from scratch for each task.\nAs depicted in Figure 13, the absence of pre-trained knowledge leads\nto dramatic performance drop across all tasks. This indicates that\nwhile LLMs are pre-trained over text corpora to acquire language\nknowledge, their emergent abilities (e.g., planning [102], pattern\nmining [112]) are indeed universal and applicable across domains,\nincluding networking. For instance, the pattern mining ability of\nLLM can be utilized to mine complex changing patterns of viewport\nmovement for accurate viewport prediction. Hence, the pre-trained\nknowledge of LLMs is crucial for networking adaptation.\nNext, we preserve the pre-trained knowledge of Llama2 but\ndisable the low-rank matrices that represent the learned domain\nknowledge. As reported in Figure 13, the absence of domain knowl-\nedge also results in performance degradation on each task, which\nhighlights the importance of NetLLM to acquire domain knowledge.\nImpacts of different types of LLMs. To validate whetherNetLLM\nis applicable to various LLMs, we employ it to adapt three addi-\ntional LLMs besides Llama2 for the VP and ABR tasks: OPT [108],\nMistral [42] and LLaVa [56]. The size of each LLM is set to 7B for\nfair comparison. It is worth mentioning that LLaVa is a multimodal\nLLM trained on the combination of image and text corpora. We se-\nlect LLaVa for our evaluation to investigate whether the pre-trained\nknowledge of multimodal fusion is applicable for networking.\nAs shown in Figure 15, all the adapted LLMs outperform the\nstate of the arts on both tasks, which confirms the compatibility\nof NetLLM. Interestingly, we observe that the multimodal LLaVa\nperforms worse than to single-modal Llama2. This suggests that\nthe knowledge acquired by LLaVa in multimodal fusion during pre-\ntraining may not be directly beneficial in the networking domain4.\nImpacts of different sizes of LLMs. Next, we investigate the\nimpacts of LLM sizes on the adaptation performance. We select\nOPT [108] as the foundational model for this investigation, which\noffers different versions with varying parameter sizes. The results\n4We leave deeper investigation of the efficacy of multimodal LLMs in networking for\nfuture work.\n        Broadband          Cecullar\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Average QoE Scores\nBBA MPC GENET NetLLM\nFigure 14: Comparing NetLLM-adapted Llama2 with baselines for ABR\non real-world environments with different network connections.\nOPT Mistral LLaVa Llama2 TRACK\n9.0\n10.5\n12.0\n13.5Average MAE (Degrees)\nVP ( ‚Üì Better)\nOPT Mistral LLaVa Llama2 GENET\n0.4\n0.6\n0.8\n1.0Average QoE Scores\nABR ( ‚Üë Better)\nFigure 15: Comparing the performance of different LLMs adapted\nby NetLLM for VP and ABR with learning-based algorithms.\nare presented in Figure 16. As shown, when the parameter size\nexceeds 1B, the adapted OPT achieves superior or comparable per-\nformance to the advanced learning-based algorithms. However,\nfor the ABR task, OPT-0.35B performs significantly worse than\nall baselines, potentially due to the limited common knowledge\nto generalize across tasks. This suggests that, in practice, LLMs\nwith parameter sizes greater than 1B are suitable for networking\nadaptation, while those smaller than 1B may not be the optimal\nchoices for adaptation.\nComputation overhead. To measure the overhead of deploying\nNetLLM-adapted LLM to solve networking tasks, we profile the LLM\nanswer generation process. Overall, loading a 7B LLM like Llama2-\n7B requires 29 GB memory and takes about 0.1s‚àº0.3s to generate\none answer. The computation overhead can be reduced by utilizing\nthe advanced model compression techniques [19, 104] (discussed\nin ¬ß6), or employing smaller LLMs such as OPT-1.3B which also\nachieves superior performance over baselines (see Figure 16). Specif-\nically, for OPT-1.3B, it only takes 7GB to load the model, which can\nbe accommodated by commercial GPUs like NVIDIA 10GB 3080.\nBesides, it takes about 0.04s for OPT-1.3B to generate one answer,\nwhich is acceptable for many networking tasks.\n6 DISCUSSION\nQ1: What considerations are needed when adapting LLMs for\nspecific networking tasks using NetLLM?\nWhile NetLLM‚Äôs overall design is independent of specific net-\nworking tasks, some considerations are needed when applying it\nfor LLM adaptation. Specifically, when adapting LLMs for the target\nACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia Wu et al.\n0.35B 1.3B 2.7B 7B 13B\nParameter Sizes of LLM (Billion)\n‚àí30\n‚àí15\n0\n15\n30\n% of Average MAE\nbetter than baselines\nVP\nBaseline: LR\nBaseline: Velocity\nBaseline: TRACK\n0.35B 1.3B 2.7B 7B 13B\nParameter Sizes of LLM (Billion)\n‚àí75\n‚àí50\n‚àí25\n0\n25\n% of Average QoE\nbetter than baselines\nABR\nBaseline: BBA\nBaseline: MPC\nBaseline: GENET\nFigure 16: Exploring the impacts of LLM sizes in networking adapta-\ntion, with OPT [108] as the foundation model.\nnetworking task, the creation of a new networking head is nec-\nessary, and the selection of a modality-specific feature encoder is\nalso needed when dealing with a new modality. However, NetLLM\nminimizes the ad-hoc design costs associated with these considera-\ntions. On one hand, the networking head in NetLLM is essentially\na simple linear layer that can be easily customized based on the\ntask output space. For example, in VP task, the networking head is\nsimply designed with three neurons to output the viewport coordi-\nnates, i.e., roll, pitch and yaw (see¬ßA.2). On the other hand,NetLLM\nfacilitates efficiency by allowing the reuse of existing encoders to\nprocess specific modalities, eliminating the need of handcrafting\nencoders from scratch. Furthermore, as the current LLM research\nlandscape actively explores multimodality [28, 50, 71, 91], NetLLM\nstands to benefit from the advancements in this field. With the de-\nvelopment of more generic and powerful LLMs that support more\nmodalities, NetLLM can utilize their built-in encoders to effectively\nprocess multimodal data.\nQ2: How does NetLLM compare to retrieval-augmented genera-\ntion (RAG)?\nRetrieval-augmented generation (RAG) [48, 74, 100] has been\nrecently proposed to enhance the capabilities of LLMs. RAG con-\nstructs a large corpus as an external knowledge base to store domain-\nspecific knowledge. During the inference stage, relevant informa-\ntion is retrieved from the knowledge base and attended to the input\ncontext of LLMs to provide domain knowledge. Although RAG has\nshown efficacy in improving the performance of LLMs in NLP tasks,\nit faces challenges when applied in the networking field. This is\nbecause, unlike NLP where knowledge can be stored explicitly in\nthe textual form (e.g., the names of presidents of different coun-\ntries), representing domain knowledge in networking as plain texts\nis challenging due to the abstract and implicit nature of networking\nknowledge (e.g., the ABR policy to dynamically adjust bitrates based\non the changing network conditions). As a result, constructing an\nexternal knowledge base for RAG is rather challenging in the field\nof networking. In contrast, NetLLM takes a different approach by\ndesigning an efficient DD-LRNA module, which enables LLMs to\nautomatically and effectively learn domain-specific knowledge for\nnetworking. With this module, NetLLM facilitates efficient acquisi-\ntion of networking knowledge without relying on the construction\nof an external knowledge base.\nQ3: How to reduce the computation overhead of LLMs?\nThere is a wealth of research in model compression [ 19, 104]\nwhich can be leveraged to reduce the computation overhead of\nLLMs, including model pruning [ 58, 109], quantization [ 89, 90]\nand knowledge distillation [69, 75]. For example, SparseGPT [26]\ndemonstrates that LLMs can be pruned to ignore at least 50% param-\neters with minimal performance loss. OPTQ [27] uses quantization\nto decrease the bit-width of OPT-1.3B from originally 16 bits to 4\nbits with negligible performance degradation while significantly\nreducing the model size by 4√ó. These active lines of research can\nbe integrated into NetLLM to reduce the overhead of LLMs when\ndeploying them for networking in practice. While the trade-off\nbetween performance and resource consumption should be consid-\nered when applying these techniques, we leave further exploration\nof this trade-off for future work.\nQ4: Why can LLMs be useful in networking?\nIn ¬ß5.4, we have identified from the high-level perspective that\nthe pre-trained knowledge of LLMs is one of the dominant factors\nto their success in networking. However, further investigations into\nthe internal working mechanisms of LLMs are crucial to improve\ntheir explainability. Gaining deeper insights into the explainability\nof LLMs enables researchers to comprehend their capabilities, limi-\ntations, and areas for improvement [5, 110]. This understanding, in\nturn, paves the way for the development of more reliable and secure\nLLM-based networking systems that can be trustfully deployed in\nreal-world scenarios. Therefore, a significant future research di-\nrection lies in designing an interpretable system to elucidate the\nbehaviors of LLMs in the context of networking, which will greatly\nfacilitate the effective utilization of LLMs in networking.\n7 CONCLUDING REMARKS\nIn this paper, we for the first time explore the utilization of LLMs\nas foundation models for networking to reduce handcraft costs\ninvolved in algorithm design and achieve strong generalization. To\nachieve this, we proposeNetLLM, the first framework that efficiently\nadapts LLMs for different networking tasks. Across three use cases\nin networking, we show thatNetLLM enables the effective utilization\nof a single LLM to achieve superior performance and generalization\nin multiple networking tasks. WhileNetLLM by no means is the final\nanswer, we hope that it serves as a stepping stone towards a more\nsustainable design philosophy for future networking algorithms\nand demonstrates the potential of adapting LLMs for networking.\nEthics: This work does not raise any ethical issues.\nACKNOWLEDGEMENTS\nWe thank the anonymous SIGCOMM reviewers and our shepherd,\nShay Vargaftik, for their invaluable feedbacks. This work was sup-\nported in part by NSFC with Grant No. 62293482, the Basic Research\nProject No. HZQB-KCZYZ-2021067 of Hetao Shenzhen-HK S&T Co-\noperation Zone, NSFC with Grant No. 62102342, the Shenzhen Sci-\nence and Technology Program with Grant No. RCBS20221008093120\n047, the Young Elite Scientists Sponsorship Program of CAST with\nGrant No. 2022QNRC001, the Shenzhen Outstanding Talents Train-\ning Fund 202002, the Guangdong Research Projects No. 2017ZT07X1\n52 and No. 2019CX01X104, the Guangdong Provincial Key Labora-\ntory of Future Networks of Intelligence (Grant No. 2022B1212010001),\nand the Shenzhen Key Laboratory of Big Data and Artificial Intelli-\ngence (Grant No. ZDSYS201707251409055). Zhi Wang‚Äôs work was\nsupported in part by Shenzhen Science and Technology Program\n(Grant No. JCYJ20220818101014030). Junchen Jiang‚Äôs work was\nsupported in part by NSF CNS 2146496, 1901466, 2313190, 2131826.\nNetLLM: Adapting Large Language Models for Networking ACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia\nREFERENCES\n[1] Soheil Abbasloo, Chen-Yu Yen, and H Jonathan Chao. 2020. Classic meets\nmodern: A pragmatic learning-based congestion control for the internet. In\nProceedings of the 2020 ACM SIGCOMM Conference . 632‚Äì647.\n[2] Soheil Abbasloo, Chen-Yu Yen, and H. Jonathan Chao. 2021. Wanna make your\ntcp scheme great for cellular networks? Let machines do it for you!IEEE Journal\non Selected Areas in Communications 39, 1 (2021), 265‚Äì279.\n[3] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. 2020. An opti-\nmistic perspective on offline reinforcement learning. In Proceedings of the 2020\nInternational Conference on Machine Learning . PMLR, 104‚Äì114.\n[4] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic di-\nmensionality explains the effectiveness of language model fine-tuning. arXiv\npreprint arXiv:2012.13255 (2020).\n[5] J Alammar. 2021. Ecco: An open source library for the explainability of trans-\nformer language models. In Proceedings of the 59th Joint Conference of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing: System Demon-\nstrations. 249‚Äì257.\n[6] Venkat Arun and Hari Balakrishnan. 2018. Copa: Practical delay-based conges-\ntion control for the internet. In 2018 USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI) . USENIX Association, 329‚Äì342.\n[7] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normal-\nization. arXiv preprint arXiv:1607.06450 (2016).\n[8] Wei Bai, Li Chen, Kai Chen, Dongsu Han, Chen Tian, and Hao Wang. 2015.\nInformation-agnostic flow scheduling for commodity data centers. In 2015\nUSENIX Symposium on Networked Systems Design and Implementation (NSDI) .\n455‚Äì468.\n[9] Abdelhak Bentaleb, Christian Timmerer, Ali C. Begen, and Roger Zimmermann.\n2019. bandwidth prediction in low-latency chunked streaming. In Proceedings\nof the 2019 ACM Workshop on Network and Operating Systems Support for Digital\nAudio and Video . 7‚Äì13.\n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances\nin Neural Information Processing Systems , Vol. 33. 1877‚Äì1901.\n[11] Li Chen, Justinas Lingys, Kai Chen, and Feng Liu. 2018. Auto: Scaling deep\nreinforcement learning for datacenter-scale automatic traffic optimization. In\nProceedings of the 2018 ACM SIGCOMM Conference . 191‚Äì205.\n[12] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha\nLaskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision\ntransformer: Reinforcement learning via sequence modeling.Advances in Neural\nInformation Processing Systems 34 (2021), 15084‚Äì15097.\n[13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\nOliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike,\nJosh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight,\nMiles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario\nAmodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evalu-\nating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374\n(2021).\n[14] Tatsuhiro Chiba and Tamiya Onodera. 2016. Workload characterization and\noptimization of TPC-H queries on Apache Spark. In 2016 IEEE International\nSymposium on Performance Analysis of Systems and Software (ISPASS) . IEEE,\n112‚Äì121.\n[15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua\nMaynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar\nPrabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Brad-\nbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke,\nAnselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ip-\npolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan\nSepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023.\nPalm: Scaling language modeling with pathways. Journal of Machine Learning\nResearch 24, 240 (2023), 1‚Äì113.\n[16] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William\nFedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen,\nAakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha\nValter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang,\nAndrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-\nfinetuned language models. arXiv preprint arXiv:2210.11416 (2022).\n[17] Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero,\nJan-Niklas Eckardt, Narmin Ghaffari Laleh, Chiara Maria Lavinia L√∂ffler, Sophie-\nCaroline Schwarzkopf, Michaela Unger, Gregory P Veldhuizen, et al. 2023. The fu-\nture landscape of large language models in medicine. Communications medicine\n3, 1 (2023), 141.\n[18] Federal Communications Commission. 2016. Raw data -\nmeasuring broadband america. (2016). https://www.fcc.\ngov/reports-research/reports/measuring-broadband-america/\nraw-data-measuring-broadband-america-2016\n[19] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. 2020. Model compres-\nsion and hardware acceleration for neural networks: A comprehensive survey.\nProc. IEEE 108, 4 (2020), 485‚Äì532.\n[20] Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng,\nand Maosong Sun. 2022. OpenPrompt: An open-source framework for prompt-\nlearning. In Proceedings of the 2022 Annual Meeting of the Association for Com-\nputational Linguistics: System Demonstrations . 105‚Äì113.\n[21] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su,\nShengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. Parameter-\nefficient fine-tuning of large-scale pre-trained language models.Nature Machine\nIntelligence 5, 3 (2023), 220‚Äì235.\n[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,\nXiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An\nimage is worth 16x16 words: Transformers for image recognition at scale.arXiv\npreprint arXiv:2010.11929 (2021).\n[23] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdh-\nery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,\nWenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey\nLevine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy\nZeng, Igor Mordatch, and Pete Florence. 2023. Palm-e: An embodied multimodal\nlanguage model. arXiv preprint arXiv:2303.03378 (2023).\n[24] Xianglong Feng, Zeyang Bao, and Sheng Wei. 2021. Liveobj: Object semantics-\nbased viewport prediction for live mobile virtual reality streaming. IEEE Trans-\nactions on Visualization and Computer Graphics 27, 5 (2021), 2736‚Äì2745.\n[25] DASH Industry Form. 2016. Reference Client 2.4.0. (2016). http://mediapm.\nedgesuite.net/dash/public/nightly/samples/dash-if-reference-player/index.\nhtml.\n[26] Elias Frantar and Dan Alistarh. 2023. Sparsegpt: Massive language models\ncan be accurately pruned in one-shot. In Proceedings of the 2023 International\nConference on Machine Learning . PMLR, 10323‚Äì10337.\n[27] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Optq:\nAccurate quantization for generative pre-trained transformers. In Proceedings\nof the 2022 International Conference on Learning Representations .\n[28] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin,\nJinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. 2023. Mme: A comprehensive\nevaluation benchmark for multimodal large language models. arXiv preprint\narXiv:2306.13394 (2023).\n[29] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, and\nNigel Collier. 2023. On the effectiveness of parameter-efficient fine-tuning.\nProceedings of the 2023 AAAI Conference on Artificial Intelligence , 12799‚Äì12807.\n[30] Archie Gertsman. 2024. spark-sched-sim: An apache spark job scheduling\nsimulator, implemented as a Gymnasium environment. (2024). https://github.\ncom/ArchieGertsman/spark-sched-sim\n[31] Amelia Glaese, Nat McAleese, Maja Trƒôbacz, John Aslanides, Vlad Firoiu, Timo\nEwalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker,\net al. 2022. Improving alignment of dialogue agents via targeted human judge-\nments. arXiv preprint arXiv:2209.14375 (2022).\n[32] Robert Grandl, Ganesh Ananthanarayanan, Srikanth Kandula, Sriram Rao, and\nAditya Akella. 2014. Multi-resource packing for cluster schedulers. InProceedings\nof the 2014 ACM SIGCOMM Conference . 455‚Äì466.\n[33] Yu Guan, Chengyuan Zheng, Xinggong Zhang, Zongming Guo, and Junchen\nJiang. 2019. Pano: Optimizing 360‚ó¶video streaming with a better understanding\nof quality perception. In Proceedings of the 2019 ACM SIGCOMM Conference .\n394‚Äì407.\n[34] Quentin Guimard, Lucile Sassatelli, Francesco Marchetti, Federico Becattini,\nLorenzo Seidenari, and Alberto Del Bimbo. 2022. Deep variational learning for\nmultiple trajectory prediction of 360‚ó¶head movements. In Proceedings of the\nACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia Wu et al.\n2022 ACM Multimedia Systems Conference . 12‚Äì26.\n[35] Antonio Gulli and Sujit Pal. 2017. Deep learning with Keras . Packt Publishing\nLtd.\n[36] Satyandra Guthula, Navya Battula, Roman Beltiukov, Wenbo Guo, and Arpit\nGupta. 2023. netFound: Foundation Model for Network Security. arXiv preprint\narXiv:2310.17025 (2023).\n[37] Bo Han, Yu Liu, and Feng Qian. 2020. Vivo: Visibility-aware mobile volumetric\nvideo streaming. In Proceedings of the 2020 ACM Annual International Conference\non Mobile Computing and Networking . Article 11, 13 pages.\n[38] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685 (2021).\n[39] Te-Yuan Huang, Ramesh Johari, Nick McKeown, Matthew Trunnell, and Mark\nWatson. 2014. A buffer-based approach to rate adaptation: Evidence from a large\nvideo streaming service. In Proceedings of the 2014 ACM SIGCOMM Conference .\n187‚Äì198.\n[40] Michael Janner, Qiyang Li, and Sergey Levine. 2021. Offline reinforcement\nlearning as one big sequence modeling problem.Advances in Neural Information\nProcessing Systems 34 (2021), 1273‚Äì1286.\n[41] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\nYe Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination\nin natural language generation. Comput. Surveys 55, 12, Article 248 (2023),\n38 pages.\n[42] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-\nvendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux,\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix,\nand William El Sayed. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825 (2023).\n[43] Yili Jin, Junhua Liu, Fangxin Wang, and Shuguang Cui. 2022. Where are you\nlooking? A large-scale dataset of head and gaze behavior for 360-degree videos\nand a pilot study. In Proceedings of the 2022 ACM International Conference on\nMultimedia. 1025‚Äì1034.\n[44] Nuowen Kan, Yuankun Jiang, Chenglin Li, Wenrui Dai, Junni Zou, and Hongkai\nXiong. 2022. Improving generalization for neural adaptive video streaming\nvia meta reinforcement learning. In Proceedings of the 2022 ACM International\nConference on Multimedia . 3006‚Äì3016.\n[45] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. 2022. Large language models are zero-shot reasoners. Advances in\nNeural Information Processing Systems 35 (2022), 22199‚Äì22213.\n[46] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,\nCody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nmemory management for large language model serving with pagedattention.\nIn Proceedings of the 2023 ACM Symposium on Operating Systems Principles .\n611‚Äì626.\n[47] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline\nreinforcement learning: Tutorial, review, and perspectives on open problems.\narXiv preprint arXiv:2005.01643 (2020).\n[48] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim\nRockt√§schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP Tasks. In Advances in Neural Informa-\ntion Processing Systems , Vol. 33. 9459‚Äì9474.\n[49] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nHalueval: A large-scale hallucination evaluation benchmark for large language\nmodels. In Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing . 6449‚Äì6464.\n[50] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Boot-\nstrapping language-image pre-training with frozen image encoders and large\nlanguage models. arXiv preprint arXiv:2301.12597 (2023).\n[51] Minghao Li, Ruihang Wang, Xin Zhou, Zhaomeng Zhu, Yonggang Wen, and\nRui Tan. 2023. Chattwin: Toward automated digital twin generation for data\ncenter via large language models. In Proceedings of the 2023 ACM International\nConference on Systems for Energy-Efficient Buildings, Cities, and Transportation .\n208‚Äì211.\n[52] Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023. Large Language\nModels in Finance: A Survey. InProceedings of the ACM International Conference\non AI in Finance . Association for Computing Machinery, New York, NY, USA,\n374‚Äì382.\n[53] Zhi Li, Xiaoqing Zhu, Joshua Gahm, Rong Pan, Hao Hu, Ali C Begen, and David\nOran. 2014. Probe and adapt: Rate adaptation for http video streaming at scale.\nIEEE Journal on Selected Areas in Communications 32, 4 (2014), 719‚Äì733.\n[54] Xinjie Lin, Gang Xiong, Gaopeng Gou, Zhen Li, Junzheng Shi, and Jing Yu.\n2022. Et-bert: A contextualized datagram representation with pre-training\ntransformers for encrypted traffic classification. In Proceedings of the 2022 ACM\nWeb Conference. 633‚Äì642.\n[55] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu,\nNikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos San-\ntos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, and Alexander\nRives. 2023. Evolutionary-scale prediction of atomic-level protein structure\nwith a language model. Science 379, 6637 (2023), 1123‚Äì1130.\n[56] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual\ninstruction tuning. arXiv preprint arXiv:2304.08485 (2023).\n[57] Junhua Liu, Boxiang Zhu, Fangxin Wang, Yili Jin, Wenyi Zhang, Zihan Xu, and\nShuguang Cui. 2023. Cav3: Cache-assisted viewport adaptive volumetric video\nstreaming. In Proceedings of the 2023 IEEE Conference Virtual Reality and 3D User\nInterfaces (VR) . 173‚Äì183.\n[58] Jing Liu, Bohan Zhuang, Zhuangwei Zhuang, Yong Guo, Junzhou Huang, Jin-\nhui Zhu, and Mingkui Tan. 2022. Discrimination-aware network pruning for\ndeep model compression. IEEE Transactions on Pattern Analysis and Machine\nIntelligence 44, 8 (2022), 4035‚Äì4051.\n[59] Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel Pinck-\nney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee, Ismet\nBayraktaroglu, Bonita Bhaskaran, Bryan Catanzaro, Arjun Chaudhuri, Sharon\nClay, Bill Dally, Laura Dang, Parikshit Deshpande, Siddhanth Dhodhi, Sameer\nHalepete, Eric Hill, Jiashang Hu, Sumit Jain, Brucek Khailany, George Kokai,\nKishor Kunal, Xiaowei Li, Charley Lind, Hao Liu, Stuart Oberman, Sujeet Omar,\nSreedhar Pratty, Jonathan Raiman, Ambar Sarkar, Zhengjiang Shao, Hanfei Sun,\nPratik P Suthar, Varun Tej, Walker Turner, Kaizhe Xu, and Haoxing Ren. 2023.\nChipnemo: Domain-adapted llms for chip design.arXiv preprint arXiv:2311.00176\n(2023).\n[60] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and\nGraham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey\nof prompting methods in natural language processing. Comput. Surveys 55, 9\n(2023), 1‚Äì35.\n[61] Yuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang Huang, Shan\nLu, Michael Maire, Henry Hoffmann, Ari Holtzman, Ganesh Ananthanarayanan,\nand Junchen Jiang. 2023. CacheGen: Fast Context Loading for Language Model\nApplications. arXiv preprint arXiv:2310.07240 (2023).\n[62] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. 2017. Neural adaptive\nvideo streaming with pensieve. In Proceedings of the 2017 ACM SIGCOMM\nConference. 197‚Äì210.\n[63] Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng,\nand Mohammad Alizadeh. 2019. Learning scheduling algorithms for data pro-\ncessing clusters. In Proceedings of the 2019 ACM SIGCOMM Conference . 270‚Äì288.\n[64] Lifan Mei, Runchen Hu, Houwei Cao, Yong Liu, Zifan Han, Feng Li, and Jin Li.\n2020. Realtime mobile bandwidth prediction using lstm neural network and\nbayesian fusion. Computer Networks 182 (2020), 107515.\n[65] Xuying Meng, Chungang Lin, Yequan Wang, and Yujun Zhang. 2023. Net-\ngpt: Generative pretrained transformer for network traffic. arXiv preprint\narXiv:2304.09513 (2023).\n[66] Zili Meng, Minhu Wang, Jiasong Bai, Mingwei Xu, Hongzi Mao, and Hongxin\nHu. 2020. Interpreting deep learning-based networking systems. In Proceedings\nof the 2020 ACM SIGCOMM Conference . 154‚Äì171.\n[67] Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink,\nOlivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy,\nand Babak Hodjat. 2024. Evolving deep neural networks. InArtificial Intelligence\nin the Age of Neural Networks and Brain Computing (Second Edition) (second\nedition ed.), Robert Kozma, Cesare Alippi, Yoonsuck Choe, and Francesco Carlo\nMorabito (Eds.). Academic Press, 269‚Äì287.\n[68] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu\nNguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. Recent\nadvances in natural language processing via large pre-trained language models:\nA survey. Comput. Surveys 56, 2 (2023), 1‚Äì40.\n[69] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Mat-\nsukawa, and Hassan Ghasemzadeh. 2020. Improved knowledge distillation\nvia teacher assistant. In Proceedings of the 2020 AAAI Conference on Artificial\nIntelligence. 5191‚Äì5198.\n[70] Ravi Netravali, Anirudh Sivaraman, Somak Das, Ameesh Goyal, Keith Winstein,\nJames Mickens, and Hari Balakrishnan. 2015. Mahimahi: accurate record-and-\nreplay for http. In 2015 USENIX Annual Technical Conference (USENIX ATC) .\n417‚Äì429.\n[71] OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).\n[72] OpenAI. 2024. Chatgpt. (2024). https://chat.openai.com/chat\n[73] Fannia Pacheco, Ernesto Exposito, Mathieu Gineste, Cedric Baudoin, and Jose\nAguilar. 2018. Towards the deployment of machine learning solutions in network\ntraffic classification: A systematic survey. IEEE Communications Surveys &\nTutorials 21, 2 (2018), 1988‚Äì2014.\n[74] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.\n2024. Unifying Large Language Models and Knowledge Graphs: A Roadmap.\nIEEE Transactions on Knowledge and Data Engineering (2024), 1‚Äì20.\n[75] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. 2019. Relational knowledge\ndistillation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition . 3967‚Äì3976.\n[76] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-\ngory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,\nAlban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,\nNetLLM: Adapting Large Language Models for Networking ACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and\nSoumith Chintala. 2019. Pytorch: An imperative style, high-performance deep\nlearning library. Advances in Neural Information Processing Systems 32 (2019).\n[77] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The refinedweb dataset for falcon llm: Outper-\nforming curated corpora with web data, and web data only. arXiv preprint\narXiv:2306.01116 (2023).\n[78] Yanghua Peng, Yixin Bao, Yangrui Chen, Chuan Wu, Chen Meng, and Wei Lin.\n2021. Dl2: A deep learning-driven scheduler for deep learning clusters. IEEE\nTransactions on Parallel and Distributed Systems 32, 8 (2021), 1947‚Äì1960.\n[79] Rafael Figueiredo Prudencio, Marcos R. O. A. Maximo, and Esther Luna Colom-\nbini. 2023. A survey on offline reinforcement learning: Taxonomy, review, and\nopen problems. IEEE Transactions on Neural Networks and Learning Systems\n(2023), 1‚Äì21.\n[80] Feng Qian, Bo Han, Qingyang Xiao, and Vijay Gopalakrishnan. 2018. Flare:\nPractical viewport-adaptive 360-degree video streaming for mobile devices. In\nProceedings of the 2018 ACM Annual International Conference on Mobile Comput-\ning and Networking . 99‚Äì114.\n[81] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text transformer. The Journal of\nMachine Learning Research 21, 1 (2020), 5485‚Äì5551.\n[82] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexan-\nder Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay,\nJost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards,\nNicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and\nNando de Freitas. 2022. A generalist agent. arXiv preprint arXiv:2205.06175\n(2022).\n[83] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-yao Huang, Zhihui Li, Xiaojiang\nChen, and Xin Wang. 2021. A Comprehensive Survey of Neural Architecture\nSearch: Challenges and Solutions. ACM Comput. Surv. 54, 4, Article 76 (may\n2021), 34 pages.\n[84] Haakon Riiser, Paul Vigmostad, Carsten Griwodz, and P√•l Halvorsen. 2013.\nCommute path bandwidth traces from 3G networks: analysis and applications.\nIn Proceedings of the 2013 ACM Multimedia Systems Conference . 114‚Äì118.\n[85] Miguel Fabi√°n Romero Rond√≥n, Lucile Sassatelli, Ram√≥n Aparicio-Pardo, and\nFr√©d√©ric Precioso. 2022. Track: A new method from a re-examination of deep\narchitectures for head motion prediction in 360‚ó¶videos. IEEE Transactions on\nPattern Analysis and Machine Intelligence 44, 9 (2022), 5681‚Äì5699.\n[86] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347\n(2017).\n[87] Apache Spark. 2024. Job Scheduling - Spark 3.5.0 Documentation. (2024).\nhttps://spark.apache.org/docs/latest/job-scheduling.html Accessed: 2024-01-08.\n[88] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-\nduction. MIT press.\n[89] Chen Tang, Kai Ouyang, Zhi Wang, Yifei Zhu, Wen Ji, Yaowei Wang, and Wenwu\nZhu. 2022. Mixed-precision neural network quantization via learned layer-wise\nimportance. In Proceedings of the 2022 European Conference on Computer Vision .\nSpringer Nature Switzerland, 259‚Äì275.\n[90] Chen Tang, Haoyu Zhai, Kai Ouyang, Zhi Wang, Yifei Zhu, and Wenwu Zhu.\n2022. Arbitrary bit-width network: A joint layer-wise quantization and adaptive\ninference approach. In Proceedings of the 2022 ACM International Conference on\nMultimedia. 2899‚Äì2908.\n[91] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste\nAlayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth,\net al. 2023. Gemini: a family of highly capable multimodal models.arXiv preprint\narXiv:2312.11805 (2023).\n[92] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,\nYasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem\nCucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier\nMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan\nZarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288\n(2023).\n[93] Hado Van Hasselt, Arthur Guez, and David Silver. 2016. Deep reinforcement\nlearning with double q-learning. In Proceedings of the 2016 AAAI Conference on\nArtificial Intelligence, Vol. 30.\n[94] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you\nneed. Advances in Neural Information Processing Systems 30 (2017).\n[95] Xiangwen Wang, Xianghong Lin, and Xiaochao Dang. 2020. Supervised learning\nin spiking neural networks: A review of algorithms and evaluations. Neural\nNetworks 125 (2020), 258‚Äì280.\n[96] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian\nLester, Nan Du, Andrew M Dai, and Quoc V Le. 2022. Finetuned language\nmodels are zero-shot learners. arXiv preprint arXiv:2109.01652 (2022).\n[97] Chenglei Wu, Zhihao Tan, Zhi Wang, and Shiqiang Yang. 2017. A dataset for\nexploring user behaviors in VR spherical video streaming. In Proceedings of the\n8th ACM on Multimedia Systems Conference . 193‚Äì198.\n[98] Chenglei Wu, Ruixiao Zhang, Zhi Wang, and Lifeng Sun. 2020. A spherical\nconvolution approach for learning long term viewport prediction in 360 immer-\nsive video. In Proceedings of the 2020 AAAI Conference on Artificial Intelligence .\n14003‚Äì14040.\n[99] Duo Wu, Panlong Wu, Miao Zhang, and Fangxin Wang. 2023. Mansy: General-\nizing neural adaptive immersive video streaming with ensemble and represen-\ntation learning. arXiv preprint arXiv:2311.06812 (2023).\n[100] Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, and\nSebastian Riedel. 2022. An Efficient Memory-Augmented Transformer for\nKnowledge-Intensive NLP Tasks. In Proceedings of the 2022 EMNLP Conference .\n5184‚Äì5196.\n[101] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and\nPhilip S. Yu. 2021. A Comprehensive Survey on Graph Neural Networks. IEEE\nTransactions on Neural Networks and Learning Systems 32, 1 (2021), 4‚Äì24.\n[102] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming\nZhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential\nof large language model based agents: A survey. arXiv preprint arXiv:2309.07864\n(2023).\n[103] Zhengxu Xia, Yajie Zhou, Francis Y Yan, and Junchen Jiang. 2022. Genet:\nAutomatic curriculum generation for learning adaptation in networking. In\nProceedings of the 2022 ACM SIGCOMM Conference . 397‚Äì413.\n[104] Canwen Xu and Julian McAuley. 2023. A survey on model compression and\nacceleration for pretrained language models. In Proceedings of the 2023 AAAI\nConference on Artificial Intelligence . 10566‚Äì10575.\n[105] Francis Y Yan, Hudson Ayers, Chenzhi Zhu, Sadjad Fouladi, James Hong, Keyi\nZhang, Philip Levis, and Keith Winstein. 2020. Learning in situ: A random-\nized experiment in video streaming. In 2020 USENIX Symposium on Networked\nSystems Design and Implementation (NSDI) . 495‚Äì511.\n[106] Chen-Yu Yen, Soheil Abbasloo, and H Jonathan Chao. 2023. Computers Can\nLearn from the Heuristic Designs and Master Internet Congestion Control. In\nProceedings of the 2023 ACM SIGCOMM Conference . 255‚Äì274.\n[107] Xiaoqi Yin, Abhishek Jindal, Vyas Sekar, and Bruno Sinopoli. 2015. A control-\ntheoretic approach for dynamic adaptive video streaming over http. In Proceed-\nings of the 2015 ACM SIGCOMM Conference . 325‚Äì338.\n[108] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\nChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,\nMyle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali\nSridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068 (2022).\n[109] Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi\nHong, Yanzhi Wang, and Sijia Liu. 2022. Advancing model pruning via bi-level\noptimization. Advances in Neural Information Processing Systems 35 (2022),\n18309‚Äì18326.\n[110] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai,\nShuaiqiang Wang, Dawei Yin, and Mengnan Du. 2024. Explainability for large\nlanguage models: A survey. ACM Transactions on Intelligent Systems and Tech-\nnology 15, 2 (2024), 1‚Äì38.\n[111] Hao Zhou, Chengming Hu, Ye Yuan, Yufei Cui, Yili Jin, Can Chen, Haolun\nWu, Dun Yuan, Li Jiang, Di Wu, et al. 2024. Large Language Model (LLM) for\nTelecommunications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities. arXiv preprint arXiv:2405.10825 (2024).\n[112] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin\nWu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Van-\nhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Ser-\nmanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann,\nKanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey\nLevine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry\nKalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu,\nAlexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu,\nPete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding,\nKrzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal,\nNoah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han.\n2023. Rt-2: Vision-language-action models transfer web knowledge to robotic\ncontrol. In Proceedings of the 2023 PMLR Conference on Robot Learning (CoRL) .\n2165‚Äì2183.\nACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia Wu et al.\nHistory \nViewports\n(Time-series)\nThe past 5 viewports were:\n(6.7602,4.4046,150.33)\n‚Ä¶\n(6.0234,8.7424,151.76)\nWhat are the next 5 viewports?\nPrompt\n(Text)\nLlama2-7B (LLM)\n(5.3556,8.4978,151.91)\n‚Ä¶\n(12.782,0.7158,123.76)\nAnswer\n(Text)\nTimestep\n6\n‚Ä¶\n10\nRoll \n5.3556\n‚Ä¶\n12.782\nPitch \n8.4978\n‚Ä¶ \n0.7158\nYaw \n151.91 \n‚Ä¶\n123.76\nTimestep\n6\n‚Ä¶\n10\nRoll \n5.3556\n‚Ä¶\n12.782\nPitch \n8.4978\n‚Ä¶ \n0.7158\nYaw \n151.91 \n‚Ä¶\n123.76\nPredicted \nViewports\n(Time-series)\nTimestep\n1\n‚Ä¶\n5\nRoll \n6.7602 \n‚Ä¶\n6.0234\nPitch \n4.4046 \n‚Ä¶ \n8.7424\nYaw \n150.33 \n‚Ä¶\n151.76\nTimestep\n1\n‚Ä¶\n5\nRoll \n6.7602 \n‚Ä¶\n6.0234\nPitch \n4.4046 \n‚Ä¶ \n8.7424\nYaw \n150.33 \n‚Ä¶\n151.76 Wrapped \nby \nTemplate\nPost\nProcessing\nTunable\nFigure 17: Illustration of using prompt learning [ 60] to adapt the\nLlama2-7B LLM [92] for the VP task.\nA APPENDICES\nAppendices are supporting material that has not been peer-reviewed.\nA.1 Details of Figure 2\nThe details to produce the results of ‚ÄúPrompt Learning‚Äù and ‚ÄúToken\nPrediction‚Äù in Figure 2 are illustrated in Figure 17 and described\nas follows. We use prompt learning [60] to adapt the Llama2-7B\nLLM for the VP task. Specifically, we design a prompt template\nto encapsulate time-series viewports into textual prompts, and\ninstruct Llama2 [ 92] to generate answers for the VP task based\non token prediction . We conduct our measurements on an existing\nimmersive video viewport dataset [43]. As a motivating example,\nwe use Llama2 to predict future viewports in the next 1 second\nbased on past viewports in the last 1 second. Following previous\nworks [34, 85], the viewport sampling rate is set to 5Hz, thus the\nfuture viewports and historical viewports are both 5-sample long.\nNote that Llama2 initially achieves poor performance without any\nfine-tuning. Hence, we further use OpenPrompt [ 20], an open-\nsource prompt learning framework, to fine-tune Llama2 for 100000\niterations over the viewport dataset.\nNote that in Figure 2 (middle), we calculate the fraction of valid\nanswers generated by token prediction. In our practical implemen-\ntation, we consider an answer as valid if we can extract viewports\nfrom it following a series of pre-defined string parsing operations.\nOn the other hand, an invalid answer is often the case where it\ncontains invalid characters (e.g., unexpected punctuation) or misses\nsome values. While designing complex rules to post-process an-\nswers can alleviate the issue of invalid answers, it introduces the\noverhead of answer engineering [51].\nA.2 Details of NetLLM Implementation\nWe have integratedNetLLM into three existing codebases for VP [99],\nABR [103], and CJS [30]. We have implemented the APIs in Figure 9\nbased on the functionalities provided in the codebases. Additional\ndetails of NetLLM implementation are explained as follows.\nTable 2: Summary of setting information in VP simulation. hw/pw\nis short for historical window/prediction window.\nSetting Viewport Dataset Prediction Setup\ndefault train Jin2022 hw =2s, pw = 4s\ndefault test Jin2022 hw = 2s, pw = 4s\nunseen setting1 Jin2022 hw = 4s, pw = 6s\nunseen setting2 Wu2017 hw = 2s, pw = 4s\nunseen setting3 Wu2017 hw = 4s, pw = 6s\nFor the multimodal encoder, we utilize ViT [22] to encode images,\nand 1D-CNN [62] to encode time-series and sequence data (e.g.,\nhistorical throughputs and future chunk sizes at different bitrates\nin ABR). We leverage fully connected layer to extract features from\nscalar data (e.g., buffer occupancy in ABR), and use GNN [63, 101]\nto process graph information (e.g., DAGs in CJS). By default, the\nmultimodal encoders are trainable, except that the parameters of\nViT are frozen. This is because ViT has open source pre-trained\nweights which can be used to effectively extract image features.\nThe networking heads can be easily customized according to\nthe target networking tasks. Specifically, we design the VP head\nto predict the viewport coordinates of roll, pitch, and yaw values.\nThe ABR head is designed to output the probability distribution of\ncandidate bitrates. As for CJS task, we design two heads for action\ngeneration: one to determine the next job stage to run and the other\nto decide the number of executor resources allocated to that stage.\nRegarding the DD-LRNA scheme, we configure the context win-\ndow ùë§ for learning return distribution as 10 and 20 for ABR and\nCJS, respectively. We then set the rankùëü of low-rank matrices to\nbe 32, 128 and 128, for VP, ABR and CJS, respectively. While ad-\nditional tuning of ùë§,ùëü may be beneficial, we empirically find that\nNetLLM performs well across a wide range of hyperparameter val-\nues (generally, ùë§ ‚â•10 and ùëü ‚â•32 will yield good performance).\nThus, we do not employ sophisticated methods to tune these hy-\nperparameters and keep them fixed throughout the experiments\nin ¬ß5. As for experience collection, we use GENET [103] and Dec-\nima [63] to collect experience datasets for the RL-based ABR and\nCJS tasks, respectively. While using more algorithms to interact\nwith the environments for more epochs to expend the datasets may\nyield potential benefits, we leave this for future exploration.\nA.3 Overview of Baselines\nIn our evaluation, we compare the performance of the LLM adapted\nby our NetLLM framework with three baselines for each task, in-\ncluding state-of-the-art learning-based algorithms and rule-based\nalgorithms. The following provides an overview of each baseline\nused in our evaluation.\nBaselines for VP. We implement the following three baselines\nfor performance comparison for the VP task: TRACK [85], linear\nregression (labeled \"LR\") [ 80], and velocity-based prediction (la-\nbeled \"Velocity\") [24]. TRACK [85] is a learning-based algorithm\nthat designs a DNN model based on Long Short Term Memory\n(LSTM) architecture for VP. It considers both viewer‚Äôs historical\nviewports and video saliency map as inputs to achieve state-of-the-\nart performance, where saliency map is an image that describes\nviewer‚Äôs potential attention on the video content. LR [80] assumes\nthe movement of viewer‚Äôs viewports as a linear function related\nNetLLM: Adapting Large Language Models for Networking ACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia\nTable 3: Summary of setting information in ABR simulation.\nSetting Video Dataset Bandwidth Traces\ndefault train Envivio-Dash3 FCC\ndefault test Envivio-Dash3 FCC\nunseen setting1 Envivio-Dash3 SynthTrace\nunseen setting2 SynthVideo FCC\nunseen setting3 SynthVideo SynthTrace\nto time, then uses linear regression to estimate such function for\npredicting viewer‚Äôs viewports. Velocity [24] calculates the moving\nspeed of viewer‚Äôs historical viewports and uses it to estimate the\npositions of viewer‚Äôs future viewports.\nNote that since the open-source codes of TRACK are originally\nwritten in Keras [35], we carefully convert its codes into PyTorch [76]\nto make TRACK compatible with the VP codebase [99]. We have\nensured that our implementation preserves the same functionality\nof TRACK as its original implementation. Besides, as TRACK does\nnot offer pre-trained model weights, we re-train it from scratch on\nour datasets with the same training hyperparameters described in\nTRACK‚Äôs paper and codes. As for rule-based algorithms LR and\nVelocity, which do not provide open source implementation, we\nimplement them ourselves by strictly following the same ideas and\nformulas presented in their respective papers.\nBaselines for ABR. As for ABR, the following baselines are im-\nplemented for comparison: GENET [103], BBA [39] and MPC [107].\nGENET [103] is a RL-based streaming algorithm improved over\nPensieve [62]. It introduces a curriculum learning technique to\nfacilitate the RL training process to improve convergence perfor-\nmance. BBA [39] considers buffer occupancy as a critical signal\nfor bitrate control and designs an algorithm to maintain the play-\nback buffer occupancy at a desired level. MPC [107] leverages both\nthroughput estimates and buffer occupancy to choose bitrates by\noptimizing a given QoE metric over a future chunk horizon.\nTo implement the aforementioned ABR baselines, we utilize\nthe open-source codes of GENET [103], which already include the\nimplementation of the three algorithms. Furthermore, we re-use\nthe pre-trained model weights of GENET5 for our experiments.\nBaselines for CJS. The following three baselines are implemented\nfor the CJS task: Decima [63], first-in-first-out scheduling (labeled\n\"FIFO\") [87] and fair scheduling (labeled \"Fair\") [87]. Decima [63]\nis a RL model for job scheduling in the distributed computing clus-\nter, which develops a graph neural network (GNN) to efficiently\nprocess DAG information of job properties (e.g., resource demands\nand dependency). Both FIFO and Fair are two common scheduling\nalgorithms used by data processing system Spark [87]. The former\nschedules jobs in the order of their arrival and allocates the re-\nquested amount of resources to each job, while the latter schedules\njobs in a ‚Äúround robin‚Äù fashion to ensure that each job receives a\nroughly equal share of the cluster.\nWe utilize the PyTorch re-implementation of Decima [30] for our\nexperiments as the original implementation is somewhat outdated.\n5https://github.com/GenetProject/Genet/tree/main/src/emulator/abr/pensieve/data/\nmahimahi_new_best_models/ADR_model\nTable 4: Summary of setting information in CJS simulation.\nsetting Job Requests Executor Resources(k)\ndefault train 200 50\ndefault test 200 50\nunseen setting1 200 30\nunseen setting2 450 50\nunseen setting3 450 30\nFurthermore, we make use of the pre-trained model weights of Dec-\nima6 provided in [30]. Additionally, we adopt the implementation\nof FIFO and Fair from the same source [30].\nA.4 Details of Simulation Settings\nWe generate different simulation environments with real-world\nand synthetic datasets for training and testing to comprehensively\nevaluate the performance of the LLM adapted by NetLLM against\nbaselines. The detailed simulation settings for each task are ex-\nplained as follows.\nVP simulation. As shown in Table 2, by default, we train and\ntest each method on a large-scale viewport dataset Jin2022 [43]\nwhich records the viewport traces from 84 viewers7 watching 27\n60-second immersive videos. We randomly select 15 videos and 42\nviewers for training, 6 videos and 21 viewers for validation, 6 videos\nand 21 viewers for testing. This results in a total of 882 traces for\nexperiments. The historical window (hw) and prediction window\n(pw) are set to be 2 seconds and 4 seconds, respectively, for the\ndefault training and testing settings.\nWhen evaluating generalization performance, we test each method\non a new viewport dataset (i.e., new data distributions) and/or with\na new prediction setup (i.e., increasing prediction difficulty). For\ninstance, on unseen setting2 , we evaluate each method on the new\nWu2017 dataset [97]. This dataset contains 9 videos8 with an av-\nerage length of 242 seconds watched by 48 viewers. We randomly\nsample 4 videos and 9 viewers from the dataset, resulting in 36 long\nviewport traces for testing generalization. As for unseen setting1 ,\nwe increase pw to increase the prediction difficulty for each method.\nFollowing the setting in [80], we roughly set hw ‚âàpw / 2 across all\nsettings. Changing the coefficient does not qualitatively affect the\nresults.\nABR simulation. Table 3 summarizes the simulation settings for\nABR. By default, we train and test all methods to stream theEnvivio-\nDash3 video from the DASH-246 JavaScript reference client [25],\nwhose format follows the GENET [103] and Pensieve [62] setting.\nWe use the broadband FCC [18] traces as the default bandwidth\ndataset. In particular, we use the same traces for training and val-\nidation as those used by GENET, which compromise 235 traces\nfor training and 150 traces for validation. Then, we randomly sam-\nple 100 traces from the remaining dataset for testing. This results\nin the use of more than 90 hours of bandwidth traces for exper-\niments. To simulate environments for generalization testing, we\nfollow the method in Pensieve [62] to generate a synthetic video\n6https://github.com/ArchieGertsman/spark-sched-sim/tree/main/models/decima\n7The Jin2022 dataset originally contains the viewport traces from 100 viewers [43].\nWe filter out those incomplete ones (i.e., less than 60 seconds in duration) and finally\nuse the traces from 84 viewers for experiments.\n8The Wu2017 dataset originally includes 18 videos. We use the first 9 videos as viewers\nare free to look around when watching these videos.\nACM SIGCOMM ‚Äô24, August 4‚Äì8, 2024, Sydney, NSW, Australia Wu et al.\nSynthVideo which shares a similar format of Envivio-Dash3 but\nwith a larger video bitrate. Besides, we also generate a new band-\nwidth dataset SynthTrace with 100 traces according to the method\nin Pensieve [62], which exhibits a larger bandwidth range and more\ndynamic fluctuation patterns than FCC.\nCJS simulation. Table 4 provides the detailed information of the\nCJS simulation. Following Decima [63], we simulate different work-\nload traces using a real-world dataset TPC-H [14] which contains\njob requests of large data volumes, high executor demands, and\nhigh degree of complexity. To be consistent with the settings used\nby the pre-trained Decima in [ 30], we set the number of job re-\nquests to be 200 and the number of executor resources (represent-\ning computation resources) to be 50k units as the default training\nand testing settings. To evaluate the generalization performance\nof each method, we simulate various unseen harder workloads by\nincreasing the number of job requests and reducing the number of\nexecutor resources, as also done in Decima [63]. Note that in each\nsetting the job requests are randomly sampled from the TPC-H\ndataset. Besides, when evaluating on the default testing setting, we\nhave ensured that the job requests are different from those in the\ntraining setting. This can be easily done by setting different random\nseeds for data sampling.\nA.5 Real-world ABR Testbed Setup\nWe leverage the testbed from GENET [ 103] to test the NetLLM-\nadapted Llama2 in a real-world client-server ABR system. The\ntestbed modifies dash.js (version 2.4) to support BBA, MPC and\nGENET streaming algorithms. We further modify the dash.js to\nsupport the adapted Llama2. In our real-world tests, the client\nvideo player is a Google Chrome browser (version 87) and the video\nserver (Apache version 2.7) runs on the same machine as the client.\nAll tests are performed on our Linux server, with two different\nports to emulate the ABR client and video server. We then use\nMahimahi [70] to emulate different network environments from\nthe broadband traces [ 18] and cellular mobile traces [ 84], along\nwith an 80ms RTT, between the client and server. In particular, we\nrandomly sample 100 traces from both the broadband and cellular\nmobile bandwidth datasets for network environment emulation.\nA.6 Evaluation Metrics\nMetric for VP. We use mean absolute error (MAE) as the evaluation\nmetric for the VP task. Let ùíóùëù = (ùõºùë°,ùõΩùë°,ùúÅùë°)denote the viewport co-\nordinate at timestepùë°, whereùõº,ùõΩ,ùúÅ represent the roll, pitch and yaw\nvalues, respectively. Givenùíóùëù\nùë°,ùíóùëî\nùë° as the predicted and ground-truth\nviewports and ùêª as the prediction horizon, the MAE is calculated\nby:\nùëÄùê¥ùê∏ = 1\nùêª\nùêª‚àëÔ∏Å\nùë°=1\n|ùõºùëù\nùë° ‚àíùõºùëî\nùë°|+| ùõΩùëù\nùë° ‚àíùõΩùëî\nùë°|+| ùúÅùëù\nùë° ‚àíùúÅùëî\nùë° )|\n3\nMetric for ABR. We use quality of experience (QoE) as the eval-\nuation metric for the ABR task, which is defined as the weighted\nlinear combination of three metrics [62, 103]:\nùëÑùëúùê∏ =\n√çùê∂\nùëñ=1 (ùêµùëñùë°ùëüùëéùë°ùëíùëñ ‚àíùúÜùëÖùëíùëèùë¢ùëìùëñ ‚àíùõæùêµùëñùë°ùëüùëéùë°ùëíùê∂‚Ñéùëéùëõùëîùëí ùëñ)\nùê∂\nwhere ùêµùëñùë°ùëüùëéùë°ùëíùëñ is the bitrate in Mbps of chunk ùëñ, ùëÖùëíùëèùë¢ùëìùëñ is the re-\nbuffering time in seconds of downloading chunk ùëñ, ùêµùëñùë°ùëüùëéùë°ùëíùê∂‚Ñéùëéùëõùëîùëíùëñ\nis the bitrate change in Mbps between consequtive chunks,ùê∂is the\nnumber of chunks of the video and ùúÜ,ùõæ are the weight parameters.\nFollowing Pensieve [62], we set ùúÜ= 4.3 and ùõæ = 1.\nMetric for CJS. We use job completion time (JCT) [ 63] as the\nevaluation metric for the CJS task. Let ùë°ùë† denote the arrival time of\na job and ùë°ùëí denote the finishing time of a job. The JCT is calculated\nby:\nùêΩùê∂ùëá = ùë°ùëí ‚àíùë°ùë†",
  "topic": "Adaptation (eye)",
  "concepts": [
    {
      "name": "Adaptation (eye)",
      "score": 0.7600857019424438
    },
    {
      "name": "Computer science",
      "score": 0.5011289119720459
    },
    {
      "name": "Linguistics",
      "score": 0.3240634799003601
    },
    {
      "name": "Psychology",
      "score": 0.2217138409614563
    },
    {
      "name": "Philosophy",
      "score": 0.062450408935546875
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": []
}