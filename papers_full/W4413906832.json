{
    "title": "Evaluating large language models on business process modeling: framework, benchmark, and self-improvement analysis",
    "url": "https://openalex.org/W4413906832",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5025843920",
            "name": "Humam Kourani",
            "affiliations": [
                "Fraunhofer Institute for Applied Information Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5034644987",
            "name": "Alessandro Berti",
            "affiliations": [
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A5061819674",
            "name": "Daniel Schuster",
            "affiliations": [
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A5069762894",
            "name": "Wil M. P. van der Aalst",
            "affiliations": [
                "Fraunhofer Institute for Applied Information Technology",
                "RWTH Aachen University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W25364003",
        "https://openalex.org/W2098639824",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4390041933",
        "https://openalex.org/W3187134297",
        "https://openalex.org/W4390189053",
        "https://openalex.org/W4392781032",
        "https://openalex.org/W4386315383",
        "https://openalex.org/W4285252517",
        "https://openalex.org/W1843425376",
        "https://openalex.org/W2745975416",
        "https://openalex.org/W3012922460",
        "https://openalex.org/W2069450150",
        "https://openalex.org/W3216499420",
        "https://openalex.org/W1751997605",
        "https://openalex.org/W1983188085",
        "https://openalex.org/W2620170772",
        "https://openalex.org/W4387469928",
        "https://openalex.org/W4404701973",
        "https://openalex.org/W2101238481",
        "https://openalex.org/W2101808238",
        "https://openalex.org/W4288080168",
        "https://openalex.org/W4403689435",
        "https://openalex.org/W4292155354",
        "https://openalex.org/W207431025",
        "https://openalex.org/W4306955967",
        "https://openalex.org/W3089448520",
        "https://openalex.org/W4230145224",
        "https://openalex.org/W4379507314",
        "https://openalex.org/W4386321380",
        "https://openalex.org/W4407743882",
        "https://openalex.org/W4405425460",
        "https://openalex.org/W4403709816",
        "https://openalex.org/W4398161002",
        "https://openalex.org/W4309623083",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W6906809101",
        "https://openalex.org/W6944236093",
        "https://openalex.org/W4403893313",
        "https://openalex.org/W4386321037",
        "https://openalex.org/W4402092309",
        "https://openalex.org/W4390694561",
        "https://openalex.org/W4404888518",
        "https://openalex.org/W4407553807",
        "https://openalex.org/W4402081115",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W4396665924",
        "https://openalex.org/W4405035967",
        "https://openalex.org/W4311991106",
        "https://openalex.org/W4405035625",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W2998127244",
        "https://openalex.org/W4402346235",
        "https://openalex.org/W4407743910",
        "https://openalex.org/W4391212740",
        "https://openalex.org/W4402836060",
        "https://openalex.org/W4378473736",
        "https://openalex.org/W4387819628",
        "https://openalex.org/W4319877694",
        "https://openalex.org/W4378760015",
        "https://openalex.org/W4401023707",
        "https://openalex.org/W4411122028",
        "https://openalex.org/W4385605699",
        "https://openalex.org/W1574032607",
        "https://openalex.org/W4402684163",
        "https://openalex.org/W4400713679"
    ],
    "abstract": "Abstract Large language models (LLMs) are rapidly transforming various fields, including the field of business process management (BPM). LLMs provide new ways for analyzing and improving operational processes. This paper assesses the capabilities of LLMs on business process modeling using a framework for automating this task and a robust evaluation approach. We design a comprehensive benchmark, consisting of 20 diverse business processes, and we demonstrate our evaluation approach by assessing 16 current state-of-the-art LLMs from major AI vendors. Our analysis highlights significant performance variations across LLMs and reveals a positive correlation between efficient error handling and the quality of generated models. It also shows consistent performance trends within similar LLM groups. Furthermore, we use our evaluation approach to investigate LLM self-improvement techniques, encompassing self-evaluation, input optimization, and output optimization. Our findings indicate that output optimization, in particular, offers promising potential for enhancing quality, especially in models with initially lower performance. Our contributions provide insights for leveraging LLMs in BPM, paving the way for more advanced and automated process modeling techniques.",
    "full_text": "Software and Systems Modeling\nhttps://doi.org/10.1007/s10270-025-01318-w\nSPECIAL SECTION PAPER\nEvaluating large language models on business process modeling:\nframework, benchmark, and self-improvement analysis\nHumam Kourani 1,2 · Alessandro Berti 2 · Daniel Schuster 2,3 · Wil M. P. van der Aalst 1,2\nReceived: 15 November 2024 / Revised: 26 July 2025 / Accepted: 4 August 2025\n© The Author(s) 2025\nAbstract\nLarge language models (LLMs) are rapidly transforming various ﬁelds, including the ﬁeld of business process management\n(BPM). LLMs provide new ways for analyzing and improving operational processes. This paper assesses the capabilities of\nLLMs on business process modeling using a framework for automating this task and a robust evaluation approach. We design\na comprehensive benchmark, consisting of 20 diverse business processes, and we demonstrate our evaluation approach by\nassessing 16 current state-of-the-art LLMs from major AI vendors. Our analysis highlights signiﬁcant performance variations\nacross LLMs and reveals a positive correlation between efﬁcient error handling and the quality of generated models. It also\nshows consistent performance trends within similar LLM groups. Furthermore, we use our evaluation approach to investigate\nLLM self-improvement techniques, encompassing self-evaluation, input optimization, and output optimization. Our ﬁndings\nindicate that output optimization, in particular, offers promising potential for enhancing quality, especially in models with\ninitially lower performance. Our contributions provide insights for leveraging LLMs in BPM, paving the way for more\nadvanced and automated process modeling techniques.\nKeywords Business process modeling · Large language models · Generative AI · Benchmarking · Process mining\n1 Introduction\nProcess modeling is a crucial element of business pro-\ncess management (BPM), acting as a comprehensive toolkit\nfor understanding, documenting, analyzing, and optimiz-\ning intricate business operations. It encompasses various\nforms—from textual descriptions to visual diagrams and\nCommunicated by Dominik Bork and Arnon Sturm.\nB Humam Kourani\nhumam.kourani@ﬁt.fraunhofer.de\nAlessandro Berti\na.berti@pads.rwth-aachen.de\nDaniel Schuster\nschuster@pads.rwth-aachen.de\nWil M. P . van der Aalst\nwvdaalst@pads.rwth-aachen.de\n1 Fraunhofer Institute for Applied Information Technology FIT,\nSchloss Birlinghoven, 53757 Sankt Augustin, Germany\n2 RWTH Aachen University, Ahornstraße 55, 52074 Aachen,\nGermany\n3 Process Intelligence Solutions, Kurfürstenstraße 5, 52066\nAachen, Germany\nexecutable models—thereby providing a multi-dimensional\napproach to capturing the nuances of organizational pro-\ncesses.\nBusiness process modeling integrates several key perspec-\ntives, each focusing on distinct aspects of processes. These\ninclude the control-ﬂow perspective , which maps out the\nsequence of activities and their interdependencies; the data\nperspective, which deals with the creation, manipulation,\nand usage of data throughout the process; the resource per-\nspective, which identiﬁes the human and system resources\nrequired for process execution; and the operational per-\nspective, which outlines the rules and execution semantics\ngoverning the process. Our focus in this paper is primarily on\nenhancing the control-ﬂow perspective because it forms the\nfoundational structure that supports the integration of data,\nresources, and operational aspects into a process model.\nTraditionally, business process modeling requires consid-\nerable manual effort and a deep understanding of sophisti-\ncated process modeling languages such as BPMN (Business\nProcess Model and Notation) [ 1] and Petri nets [ 2]. More-\nover, maintaining these process models to reﬂect changes in\nbusiness operations is an ongoing challenge, presenting sig-\nniﬁcant obstacles for individuals lacking expertise in these\n123\nH. Kourani et al.\nlanguages, thus highlighting the need for more streamlined\nmethodologies in process modeling.\nThe emergence of large language models (LLMs) such\nas GPT-4 [ 3] and Gemini [ 4] offers a promising avenue for\nenhancing the efﬁciency and accessibility of process model-\ning. Trained on vast and varied datasets, these models are\nskilled in a range of tasks from generating coherent and\ncontextually relevant text to solving complex problems and\nproducing executable code [ 5–7]. Their capability to pro-\ncess and interpret complex textual inputs in natural language\npositions LLMs as particularly suitable for tasks like pro-\ncess modeling that require the generation and reﬁnement of\nstructured outputs from textual descriptions.\nOur previously introduced framework [ 8] leverages LLMs\nto automate the generation and reﬁnement of process models\nfrom textual descriptions. It employs sophisticated tech-\nniques in prompt engineering, error handling, and code gen-\neration, transforming detailed process descriptions into pro-\ncess models. This framework utilizes the Partially Ordered\nWorkﬂow Language (POWL) [ 9] as an intermediate repre-\nsentation due to the quality guarantees it provides, particu-\nlarly in ensuring soundness [9], and the possibility to export\nPOWL models in standard notations such as BPMN and\nPetri nets. Preliminary results in [ 8] demonstrated the prac-\nticality and effectiveness of this framework, showcasing its\nclear advantage over alternative solutions due to the usage of\nPOWL to ensure soundness.\nThis paper extends the work presented in [ 8]b yp r o -\nviding a robust and repeatable evaluation approach for\ncomparative assessments of LLM capabilities in business\nprocess modeling. This approach incorporates a comprehen-\nsive benchmark consisting of 20 diverse business processes,\neach paired with a ground-truth model and a simulated\nevent log. This setup enables automated quality assess-\nment of generated process models via conformance checking\n[10]. Our evaluation targets diverse LLM capabilities such\nas natural language understanding, code generation, adher-\nence to instructions, and error correction. We demonstrate\nthis evaluation approach by assessing 16 state-of-the-art\nLLMs from various AI vendors, such as Google ( https://\nai.google.dev/), OpenAI ( https://openai.com/), Anthropic\n(https://www.anthropic.com/), Meta ( https://ai.meta.com/),\nand Mistral AI ( https://mistral.ai/).\nBeyond benchmarking, we use our evaluation approach\nto explore the potential of LLM self-improvement tech-\nniques for enhancing the quality of generated process models.\nBy self-improvement, we refer to the ability of LLMs to\nautonomously reﬁne their performance by leveraging their\nreasoning capabilities to detect and correct issues or improve\nclarity. We investigate three self-improvement strategies:\nself-evaluation, input optimization , and output optimization .\nIn self-evaluation, the LLM generates multiple responses,\nevaluates them based on predeﬁned criteria, and selects\nthe most accurate or appropriate one. Output optimization\ninvolves the LLM iteratively revising its own response to\nbetter fulﬁll the task requirements. In input optimization,\nthe LLM reformulates or enriches the input prompt to guide\nitself toward a better output. These techniques are inspired\nby recent advances in AI self-reﬁnement, where models\nlike GPT-4 have demonstrated the ability to improve code\nquality through iterative feedback or enhance text coher-\nence by rephrasing ambiguous statements [ 11]. By applying\nthese strategies, we aim to evaluate whether LLMs can\nautonomously reﬁne their performance within our process\nmodeling framework, resulting in more accurate and reliable\nprocess models.\nThe structure of this paper is as follows. First, Sect. 2\nprovides foundational background knowledge on related\nconcepts, and Sect. 3 discusses related work. Section 4\nprovides a detailed overview of our LLM-based process\nmodeling framework. Section 5 introduces the ProMoAI\ntool, which supports our framework. Section 6 presents the\nbenchmarking analysis, including the experimental setup,\nevaluation metrics, and a discussion of the results. Section 7\ninvestigates the LLM self-improvement strategies and ana-\nlyzes their impact on model quality. Finally, Sect. 8 discusses\nimplications, limitations, and future directions, while Sect. 9\nconcludes the paper.\n2 Background\nThis section provides an overview of the foundational con-\ncepts underpinning our work, including the general principles\nof process modeling, an examination of relevant modeling\nlanguages, the emerging role of large language models in\nprocess modeling, and the essential concepts of event data,\nprocess simulation, and conformance checking. This back-\nground lays the groundwork for understanding our proposed\nprocess modeling framework and evaluation methodology.\nWhile the focus in this section is on introducing the neces-\nsary conceptual foundations, the subsequent section (Sect. 3)\nshifts to a review of related research efforts.\n2.1 Process modeling\nBusiness process modeling is the act of representing the steps,\ndecisions, and ﬂow of a business process in a formal way\n[12]. It is a cornerstone of BPM because a clear process\nmodel enables organizations to understand and analyze how\nwork is done, identify inefﬁciencies, and communicate pro-\ncess knowledge. By creating a visual or formal model of a\nprocess, stakeholders can discuss improvements and perform\nanalysis (e.g., simulation or veriﬁcation) before implement-\ning changes in the real world. In essence, a process model\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nserves as a blueprint of organizational workﬂows, supporting\ntheir documentation, analysis, and improvement.\nOrganizations typically capture their business processes\nby creating process models that document how work is per-\nformed. These models are usually developed by business\nanalysts in close collaboration with domain experts to ensure\naccuracy and shared understanding [ 13]. In practice, process\nmodeling often starts with informal techniques (e.g., inter-\nviews, workshops, simple ﬂow sketches) and then progresses\nto formal modeling using standardized notations and tools.\nOrganizations employ various modeling tools, ranging from\nsimple diagramming software to specialized business process\nmanagement platforms. A widely used notation is BPMN [ 1],\nwhich has become an industry standard for process diagrams\ndue to its expressiveness and relative ease of understanding\nby business users.\nTraditionally, creating business process models demands\nsubstantial manual work and a strong grasp of complex pro-\ncess modeling languages. Before the advent of LLMs, auto-\nmated extraction of process models from textual descriptions\nprimarily relied on traditional Natural Language Process-\ning (NLP) and rule-based techniques. Modeling approaches\nexploited methods such as dependency parsing, part-of-\nspeech tagging, and semantic role labeling to identify verbs\ncorresponding to activities and to infer relationships between\nprocess elements [ 14, 15]. These techniques were often com-\nbined with pattern matching and keyword extraction to map\nunstructured text to formal process constructs (e.g., mapping\nverbs to activities and conjunctions like “then” or “or” to\nBPMN sequence ﬂows or gateways). However, challenges\nsuch as natural language ambiguity and variability in textual\ndescriptions often necessitated signiﬁcant human involve-\nment and hindered the full automation of process modeling\nwith NLP [ 16].\n2.2 Process modeling languages: BPMN, Petri net,\nPOWL\nOver the years, numerous process modeling languages have\nbeen developed, each with different levels of formality and\nexpressiveness. BPMN is one of the most widely used\nstandards for modeling business processes [ 1]. BPMN pro-\nvides a graphical notation (ﬂowchart-like diagrams) that is\naccessible to business users, yet capable of representing\ncomplex control-ﬂow patterns (e.g., parallel branches, deci-\nsions, loops, message ﬂows). It deﬁnes various elements such\nas events (signaling timers, messages, or errors), activities\n(tasks or subprocesses), gateways (for branching and merg-\ning control ﬂow like XOR or AND gateways), and artifacts\n(data objects, annotations, etc.). BPMN’s popularity in indus-\ntry comes from its intuitive visual nature and its coverage of\ncomplex business scenarios.\nPetri nets offer a more formal foundation for modeling the\ncontrol-ﬂow perspective of processes [ 17]. A Petri net is a\nbipartite graph consisting of places (conditions or states) and\ntransitions (activities or events), connected by arcs. Firing\nof transitions produces movement of tokens between places,\ncapturing the state changes in a process. Petri nets have a solid\nmathematical underpinning, enabling formal analysis of pro-\ncess properties (e.g., deadlock detection [ 18]). In the context\nof business processes, a speciﬁc class called Workﬂow-nets\n(WF-net) (a connected Petri net with unique start and end\nplaces) is often used to model processes. Petri nets are very\nexpressive in modeling concurrency, synchronization, and\nresource sharing [ 19, 20]. The trade-off, however, is that Petri\nnet diagrams are less intuitive to non-experts, especially for\nlarge processes, and they are limited to the control-ﬂow per-\nspective of the process (e.g., no distinction between different\ntypes of events and no explicit notion of roles). Nonetheless,\nPetri nets serve as a common formal denominator for many\nprocess analysis and process mining techniques.\nProcess modeling with BPMN and Petri nets can lead\nto quality issues, as it is possible to generate models with\nunreachable parts, for instance. To address this, the concept of\nsoundness is introduced, and many automated process model\ndiscovery methods rely on languages that ensure soundness\n(e.g., process trees [ 21] and POWL [ 22]). A process tree\nis a hierarchical model where leaves represent single tasks\nand inner nodes are control-ﬂow operators used to combine\nsubmodels, forming larger ones. POWL is a hybrid process\nmodeling notation that blends the hierarchical structure of\nprocess trees with the ﬂexibility of partial orders. It addresses\nlimitations in process trees by supporting complex concur-\nrency patterns while preserving the soundness guarantee,\nmaking it a valuable tool for process modeling and discovery.\nPOWL models are deﬁned recursively, starting with\natomic activities—single tasks like “Submit order”—as the\nbase case. These are combined using three control-ﬂow con-\nstructs: partial orders, exclusive choices (XOR), and loops.\nA partial order ≺ is an irreﬂexive, transitive, and asymmetric\nrelation that speciﬁes dependencies among sub-processes.\nFor example, if ψ\n1≺ψ2, all activities in the sub-model ψ1\nmust precede those in the sub-model ψ2, while unordered\nsub-models run concurrently. The XOR operator, written as\n×(ψ\n1,...,ψ n) models mutually exclusive choices, allow-\ning only one sub-model per execution. The loop operator,\n⟲ (ψdo,ψredo), deﬁnes a cycle where ψdo executes at least\nonce, followed by optional alternations of ψredo and ψdo.\nThese constructs are nested hierarchically, enabling complex\nstructures like a partial order containing an XOR sub-model.\nPOWL models can be transformed into widely recognized\nprocess modeling languages, including Workﬂow Nets and\nBPMN. In [ 23], an algorithm is proposed for converting\nPOWL models into sound Workﬂow Nets, which can sub-\nsequently be translated into BPMN diagrams. For example,\n123\nH. Kourani et al.\npartial orders are mapped to parallel gateways in BPMN,\nwhile XORs and loops are represented using exclusive gate-\nways. This convertibility enhances POWL’s utility as a robust\nintermediate representation for modeling business processes,\nfacilitating its integration with BPM tools.\n2.3 Process modeling with large language models\nLLMs are a class of artiﬁcial intelligence models that have\nbeen trained on vast amounts of text data to learn the patterns\nof human language. Technically, most state-of-the-art LLMs\nare based on the transformer architecture[24], which enables\nefﬁcient learning from sequences. By training on billions\nof words (for instance, internet text, books, articles), these\nmodels develop a statistical understanding of how words\nand sentences are formed. Modern LLMs like GPT-3 are\nextremely large neural networks with hundreds of billions\nof parameters, trained in a self-supervised manner to predict\nthe next word in a sentence [ 25]. Through this training, they\ncapture not just grammar and syntax, but also a signiﬁcant\namount of world knowledge and facts that appear in text.\nThe general capabilities of LLMs are broad: they can gen-\nerate human-like coherent text, translate between languages,\nsummarize documents, answer questions, and even write\ncode or structured data when prompted correctly. The key\nfeature is that these models can generalize to a wide range of\nlanguage tasks without task-speciﬁc programming—much\nof the “skill” is implicit in the model after reading so much\ntext. They are called “large” because their size (and the size of\ntheir training corpora) is what differentiates them from ear-\nlier generations of language models, granting them emergent\nabilities in understanding and generating text that smaller\nmodels could not achieve.\nIn the context of BPM, and speciﬁcally process modeling,\nLLMs have recently emerged as a very promising tool. Tradi-\ntionally, creating a process model from a textual description\n(like a paragraph describing how an order is handled) was\na manual task requiring a skilled analyst. Analysts would\ninterpret the natural language, which could be ambiguous\nor incomplete, and translate it into formal elements (events,\ntasks, gateways, etc.). This translation is challenging and\ntime-consuming. The idea of leveraging LLMs is that these\nmodels have absorbed a lot of procedural knowledge and pat-\nterns from textual descriptions of processes (among the vast\ntraining data, there are likely documents like manuals, guide-\nlines, etc., which describe processes). Therefore, an LLM\ncan potentially take a natural language description of a pro-\ncess and automatically produce a candidate process model,\nor at least assist in doing so [ 8]. In essence, the LLM acts\nas a bridge between unstructured descriptions and structured\nprocess representations.\n2.4 Event data and process simulation\nWhen processes are executed in real life or in information\nsystems, they leave behind traces of what happened. These\ntraces are recorded as event data (often organized as event\nlogs). An event log is a collection of records, where each\nrecord (event) corresponds to an occurrence of an activity\nwithin a speciﬁc process instance (often called a case). At\nminimum, each event in a log contains: a case identiﬁer (tying\nthe event to a particular process instance or workﬂow exe-\ncution), an activity name (what step was executed), and a\ntimestamp (when it happened). Often, additional attributes\nare recorded as well, such as the resource or person who\nperformed the activity.\nEvent data is essential for process mining techniques.\nGiven an event log, one can discover a process model that\nreﬂects the observed behavior ( process discovery ), check\nconformance of the log against an existing model ( con-\nformance checking ), or enhance a model with additional\nperspectives ( process enhancement )[ 26]. Event logs thus\nprovide an objective view of how processes actually execute,\nwhich is often more complex or varied than what the ofﬁcial\ndocumentation suggests. By analyzing event logs, organiza-\ntions can ﬁnd bottlenecks, deviations, and opportunities for\nimprovement grounded in real data.\nProcess simulation is a key component of BPM. Process\nsimulation means using a process model to generate artiﬁ-\ncial execution data [ 27]. If we have a process model, we can\nsimulate the process by virtually “executing” it many times,\nfollowing the different paths allowed by the model, to pro-\nduce synthetic event logs. By simulating a modeled process,\nanalysts can obtain metrics like cycle times, waiting times,\nthroughput, and resource utilization, and use these to iden-\ntify potential bottlenecks or inefﬁciencies. Simulation allows\nasking “what-if” questions—for example, what if the arrival\nrate of orders doubles, or what if an extra employee is added to\na particular task—and observing the impact on process per-\nformance without any real-world risk [ 28]. This capability\nmakes simulation a powerful tool for process improvement\nand decision support.\n2.5 Conformance checking\nConformance checking is a key discipline in process mining\nthat focuses on comparing a process model with an event log\nof the same process to see how well the model and the reality\nalign [10]. Given a process model (the expected or normative\nbehavior) and an event log (the observed behavior from exe-\ncutions), conformance checking techniques evaluate to what\nextent the log ﬁts the model and vice versa. In practice, this\nhelps identify deviations: points where the real process devi-\nates from the prescribed model (e.g., steps skipped or done\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nin the wrong order, or extra steps performed that are not in\nthe model).\nSeveral metrics have been deﬁned to quantify confor-\nmance. Two of the core metrics are ﬁtness and precision.\nFitness (sometimes also called recall) measures how much\nof the observed behavior in the log is captured by the model.\nA model has perfect ﬁtness (1.0 or 100%) if all traces in the\nevent log can be completely replayed by the model from start\nto end (i.e., the model can generate every sequence of activi-\nties that actually happened). Low ﬁtness indicates the model\nis missing behavior that actually occurs in reality. Precision,\non the other hand, measures the opposite: how much behav-\nior the model allows that was not seen in the log. A model\nwith perfect precision does not permit any behavior that was\nnot observed in reality. In other words, precision penalizes\nmodels that overgeneralize beyond the recorded behavior.\nConformance checking is typically performed using algo-\nrithms that try to replay the event log on the model [ 29]. One\nclassical approach for Petri nets and similar models is token-\nbased replay, where one ﬁres the transitions of the Petri net\naccording to the sequence of events in a trace and tracks\nwhere tokens get consumed or produced. More advanced\nconformance techniques use alignments. The idea of align-\nments is to ﬁnd the best possible correspondence between a\ntrace from the log and an execution of the model by allow-\ning certain minimal corrections. We can imagine aligning a\ntrace and a model run as two sequences and possibly insert-\ning “gaps” or “moves” where they don’t agree: a log move\n(meaning the log had an event that the model didn’t expect\nat that point) or a model move (meaning the model would\ndo something that the log didn’t have). By ﬁnding an align-\nment with minimal moves, one can pinpoint exactly where\nthe deviations are. From alignments, metrics like ﬁtness can\nbe computed in a robust way (taking into account how many\nmodel/log moves were needed). Alignments also facilitate\ndiagnosing where in the process the differences occur, not\njust aggregate scores.\n3 Related work\nBuilding upon the foundational concepts presented in Sect. 2,\nthis section reviews related research, including traditional\nNLP methods for process modeling, applications of LLMs\nin BPM, approaches to LLM self-improvement, and relevant\nevaluation methods and benchmarks.\nAn overview of various methods for extracting process\ninformation from textual content is presented in [ 30]. The\nstudy in [ 31] utilizes Natural Language Processing (NLP)\nand text mining techniques to derive process models directly\nfrom text, while [ 32] combines NLP with computational lin-\nguistics to generate BPMN models. The approach in [ 33]\napplies NLP to extract structured relationship representa-\ntions, referred to as fact types , from textual data, which\nare then converted into BPMN components. The BPMN\nSketch Miner, as detailed in [ 34], uses process mining [ 35]\nto produce BPMN models from text described in a domain-\nspeciﬁc language . Commercial solutions are also adopting\nAI for process modeling; for example, Process Talks ( https://\nprocesstalks.com) offers an AI-driven platform for generat-\ning process models from textual descriptions.\nRecent studies have explored the integration of LLMs\nin BPM, investigating their potential applications and chal-\nlenges. Several works [ 36, 37] delve into how LLMs can be\nemployed for BPM and process mining tasks. The papers\n[38] and [ 39] explore the application of LLMs in process\ndiscovery and process querying, respectively. Research has\nalso explored on leveraging LLMs for knowledge extrac-\ntion from textual sources. For instance, [ 40] compared NLP\nand LLM-based approaches for constructing a Business Pro-\ncess Knowledge Base from interview texts, ﬁnding that\nLLMs with enriched prompts outperformed traditional NLP .\nSimilarly, [ 41] proposed a prompt-based in-context learn-\ning strategy to extract process knowledge from descriptions\nand construct knowledge graphs, validating its feasibility in\nlow-resource scenarios. Beyond knowledge extraction, the\nresearch in [ 42] employs BERT [ 43] to classify and analyze\nprocess execution logs, aiming to enhance process moni-\ntoring and anomaly detection. Finally, [ 44] investigates the\nbroader impacts of LLMs in conceptual modeling, propos-\ning potential applications that extend beyond traditional BPM\ntasks, and [ 45] discusses the limitations of using GPT-4 for\nconceptual modeling.\nA signiﬁcant line of research investigates the use of LLMs\nfor generating process models directly from various inputs.\nStudies like those in [ 8, 46] applied LLMs to generate process\nmodels directly from unstructured text. In [ 47–49], meth-\nods for generating process models through dialogue-based\ninteractions and chatbots are proposed. The paper [ 50]s h o w -\ncases the ability of LLMs to translate textual descriptions\ninto procedural and declarative process model constraints.\nAddressing practical aspects, the paper [ 51] explores using\nLLMs to generate BPMN models adhering to company-\nspeciﬁc style guides. To improve generation accuracy, the\nmethod proposed in [ 52] employs a multi-step approach,\nbreaking the text-to-model task into smaller, sequential sub-\ntasks for the LLM. Furthermore, [\n53] utilized ChatGPT\nwith prompt engineering techniques to enhance the semantic\nquality of business process models by identifying missing\nconcepts, demonstrating the potential of LLMs to improve\nmodel completeness beyond visual generation.\nThe challenge of hallucination in LLMs, where mod-\nels generate plausible but factually incorrect outputs, has\nbeen extensively surveyed, for example, in [ 54]. To miti-\ngate this, various self-improvement strategiesenabling LLMs\nto enhance their own reliability have been explored; a sur-\n123\nH. Kourani et al.\nvey of automatic correction techniques is provided in [ 55],\nwhile [ 56] investigates enabling LLMs to self-correct their\nreasoning processes. A key underlying concept, termed the\nGeneration-V eriﬁcation Gap (GV-Gap) [57], highlights that\nLLMs are often more accurate at verifying a given answer’s\ncorrectness than generating a correct answer initially. This\nprinciple motivates techniques where multiple candidate out-\nputs are generated, and the LLM itself is used to evaluate\nand select the best one. For instance, [ 58] uses AI feedback\nfor improving safety alignment (Constitutional AI), while\n[59] developed benchmarks to assess LLMs acting as judges.\nAnother prominent paradigm is iterative self-improvement .\nIn [ 11], the Self-Reﬁne framework was proposed, allowing\nmodels to iteratively reﬁne their outputs based on self-\ngenerated feedback. Similarly, [ 60] introduced the Reﬂexion\ntechnique, where agents learn to improve through verbal rein-\nforcement derived from self-reﬂection. In [ 61], the authors\npropose the concept of sharpening, where a pre-trained\nLLM reﬁnes its outputs by using itself as a veriﬁer during\npost-training. The effectiveness of such self-improvement\ncapabilities is often linked to model scale; foundational scal-\ning laws were established in [ 62], and later reﬁned (e.g.,\nin [ 63]), suggesting that larger models generally possess\nenhanced potential for self-correction and reﬁnement.\nBenchmarks like the BPI Challenge datasets have long\nserved as standards for evaluating process mining algorithms\n[64]. However, these benchmarks primarily target traditional\nalgorithmic approaches rather than LLM-based methods,\nlimiting their applicability to our context. Several bench-\nmarks for evaluating LLMs on process mining and BPM tasks\nare proposed. The benchmark in [ 65] assesses LLMs across a\nspectrum of process mining tasks, utilizing self-evaluation by\nLLMs to judge the quality of results. This approach contrasts\nwith the benchmark proposed in this paper, which uses pro-\ncess descriptions aligned with ground truth models, allowing\nfor a more informed and objective assessment of model qual-\nity. Further studies such as [ 66, 67] propose benchmarks that\nfocus on causal reasoning and the explanation of decision\npoints within business processes. Additionally, [ 68]i n t r o -\nduces benchmarks for semantic-aware process mining tasks\nlike semantic anomaly detection and next activity prediction.\nThis paper extends our previous work [ 8] by incorporating\nthe following contributions. The evaluation of the proposed\nLLM-based process modeling framework has been signiﬁ-\ncantly broadened to include a larger and more diverse set of\nprocesses, as well as a wider range of state-of-the-art LLMs.\nAdditionally, a robust qualitative assessment methodology\nhas been implemented, employing conformance checking\nagainst ground truth event logs to enable a more rigorous\nevaluation of the generated process models. Furthermore, we\nexplore LLM self-improvement techniques to assess whether\nLLMs can autonomously reﬁne their performance within our\nframework.\n4 LLM-based process modeling framework\nIn this section, we present a detailed overview of our frame-\nwork, which harnesses the capabilities of LLMs to generate\nand reﬁne process models based on natural language process\ndescriptions.\n4.1 Framework overview\nFigure 1 offers a high-level view of our proposed framework.\nThe framework begins by having users provide a textual\ndescription of a process in natural language. After receiving\nthe process description, additional information is integrated\nto create a comprehensive prompt (the prompt engineering\nstrategies are discussed in Sect. 4.3). This prompt is carefully\ncrafted to instruct the LLM in generating executable code that\ncan then be used to create process models (the selection of\nthe modeling language is discussed in Sect. 4.2). A set of\nfunctions designed speciﬁcally for process model creation\naids in this code generation. Once the prompt is prepared, it\nis sent to the LLM. Our framework is not dependent on any\nspeciﬁc LLM and can function with any advanced LLM that\nsupports a large context window and code generation. After\nreceiving the LLM’s response, we extract the generated code\nand attempt to execute it (details in Sect. 4.4).\nIn case errors are encountered during code extraction or\nexecution, an error-handling mechanism is activated, send-\ning a reﬁned prompt back to the LLM to address the issue\n(discussed in Sect. 4.5). Upon successful execution and pro-\ncess model creation, users can view or export the model using\nestablished process modeling notations such as BPMN and\nPetri nets. Furthermore, the framework allows users to pro-\nvide feedback on the generated model, which can then be\nincorporated to further reﬁne the model, enabling continu-\nous improvement.\n4.2 Process representation\nTo explain the various stages of our framework, we imple-\nment an instance that uses the Partially Ordered Workﬂow\nLanguage (POWL) [ 9] for intermediate process representa-\ntion. The framework’s core principles allow for integration\nwith other modeling languages depending on process model-\ning requirements. This section highlights the reasons behind\nour choice of POWL.\nOur objective is to generate process models using common\nnotations that professionals in the business process man-\nagement and process mining ﬁelds are familiar with, such\nas BPMN and Petri nets. POWL represents a subclass of\nboth BPMN and Petri nets, i.e., POWL models can be auto-\nmatically transformed into Petri nets or BPMN models (cf.\nSect. 2.2). POWL was selected as the intermediate represen-\ntation for the following reasons:\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nFig. 1 LLM-based process modeling framework\n• Soundness Guarantees: Unlike BPMN and Petri nets,\nPOWL inherently guarantees soundness.\n• Simplicity: POWL’s hierarchical structure simpliﬁes model\ngeneration by enabling the recursive creation of sub-\nmodels, which are then combined into larger models. It\nalso assumes tasks are parallel unless otherwise speci-\nﬁed, reﬂecting the concurrent nature of many real-world\nprocesses. This assumption simpliﬁes model generation\nsince task order does not always need to be explicitly\ndeﬁned.\n• Expressive Power: While both POWL and process trees\n[21] ensure soundness, POWL supports a wider range\nof process structures [ 9]. It allows for the modeling of\nmore complex dependencies while retaining the quality\nguarantees of hierarchical process modeling languages.\n4.3 Prompt engineering\nThis section outlines the prompt engineering strategies we\nuse to guide the LLM in generating process models from\nnatural language descriptions.\nThe key strategies we implemented within our framework\nare:\n• Role Prompting: This approach involves assigning a spe-\nciﬁc role to the LLM to shape its behavior [ 69]. We\ninstruct the LLM to act as a process modeling expert, who\nis familiar with common process constructs. Addition-\nally, we ask the LLM to act as a process owner, capable\nof ﬁlling in gaps in the process descriptions based on its\nexpertise.\n• Knowledge Injection: This strategy refers to injecting\nspeciﬁc knowledge that the LLM may not have encoun-\ntered during its training [ 70]. We provide comprehensive\nknowledge about POWL, offering detailed insights into\nits hierarchical structure and the semantics of the different\nPOWL components. Moreover, our framework leverages\nLLM capabilities in generating executable code [ 6]b y\ninstructing the LLM to generate Python code that uti-\nlizes a predeﬁned set of functions we designed for the\nsafe generation of POWL models. We provide a detailed\nexplanation of these predeﬁned methods and how they\ncan be used to generate POWL models. Listing 1 illus-\ntrates the knowledge injected about POWL.\n• Few-Shots Learning: This technique involves providing\nthe LLM with multiple example input–output pairs to\ntrain it on the task [ 25]. For instance, Listing 2 shows\n123\nH. Kourani et al.\nListing 1 Knowledge injection on generating POWL models. Lines extending beyond the displayed text are abbreviated with “...” for compactness.\u0007 \u0004\n1 Use the following knowledge about the POWL m odeling language:\n2 A POWL model is a hierarchical model. POWL models are recursively generated ...\n3 We define three types of POWL models. The first type is the base case ...\n4 For the second type of POWL models , we use an operator (xor or loop) to ...\n5 The third type of POWL models is defined as a partial order over n>=2 ...\n6 Provide the Python code that recursively generates a POWL model. Save the final ...\n7 Assume the class ModelGenerator is prop erly implemented and can be imported ...\n8 ModelGenerator provides the functions described below:\n9 - activity( label) generates an activit y. It takes 1 string argument , which is ...\n10 - xor(*args) takes n >= 2 arguments , which are the submodels. Use it to model ...\n11 - loop(do, redo) takes 2 arguments , which are the do and redo parts. Use it to ...\n12 - partial_order(dependencies) takes 1 argument , which is a list of tuples of ...\n13 Note: for any powl model , you can call powl.copy() to create another instance ...\n\u0006 \u0005\none of the few-shots examples we use for training the\nLLM to generate POWL models starting from process\ndescriptions. Some of the process descriptions we use\nfor implementing few-shots learning are adapted from\n[71].\n• Negative Prompting: Negative prompting involves spec-\nifying what the LLM should avoid in its response [ 72].\nWe apply this strategy by instructing the LLM to avoid\ncommon mistakes that can occur during the genera-\ntion of POWL models, such as generating partial orders\nthat violate irreﬂexivity. Moreover, we extend our few-\nshots demonstrations with common mistakes that should\nbe avoided during the construction of each process.\nFor example, a typical mistake in modeling the bicy-\ncle manufacturing process (cf. Listing 2) is incorrectly\nrepresenting the choice between accepting and rejecting\nthe order as a simple XOR between the two activities\n“Reject order” and “Accept order”. Instead, the choice\nshould be modeled as an XOR between two paths—one\npath for rejecting the order and another path starting with\naccepting the order and including all the steps that fol-\nlow acceptance. In other words, the steps that follow order\nacceptance (e.g., preparing, assembling, and shipping the\nbicycle) must be included within the acceptance path\ninside the XOR construct, so they are not reachable if\nthe order is rejected.\n4.4 Model generation and refinement\nAfter receiving the LLM’s response, the Python code snip-\npet is extracted from the response, which might also include\nadditional text (e.g., intermediate reasoning steps). If the code\nextraction is successful, then the extracted code is executed\nto generate the model. Executing code generated by an LLM\ninvolves multiple considerations to handle security risks and\ninvalid results. The following strategies are implemented to\nensure a safe environment for producing valid process mod-\nels:\n• In order to eliminate the risk of executing unsafe code,\nwe restrict the LLM to use the predeﬁned functions we\ndesigned for the generation of POWL models. We employ\na strict process to verify that the code strictly complies\nwith the prompted coding guidelines, explicitly exclud-\ning the use of external libraries or constructs that may\npose security threats.\n• V alidation rules are in place to verify that the generated\nmodel conforms to the POWL speciﬁcations, such as the\nrequirement that all partial orders respect transitivity and\nirreﬂexivity.\nThe framework supports displaying and exporting the gen-\nerated POWL models as Petri nets or BPMN models for\nbroader use within the business process management and\nprocess mining communities.\nReﬁnement Loop\nThe framework also includes a reﬁnement loop, allowing\nusers to provide textual feedback on the generated models.\nBased on this feedback, the LLM is prompted to revise the\nmodel, ensuring continual improvement.\n4.5 Error handling\nDespite their advanced capabilities, LLMs are not immune to\ngenerating faulty outputs. We employ a robust error-handling\nmechanism tailored to mitigate potential inaccuracies and\nensure the reliability of the generated process models.\nRecognizing the variability in the severity and implica-\ntions of errors, we categorize them into two distinct groups:\n• Critical Errors:These are severe issues that affect func-\ntionality or pose security risks, such as execution failures\nor major model validation violations. Critical errors halt\nthe process and must be resolved before a valid model can\nbe produced. Their presence typically indicates a major\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nListing 2 One of the examples used for few-shots learning and negative prompting. Lines extending beyond the displayed text are abbreviated with\n“...” for compactness.\u0007 \u0004\n1 Process description for example 1:\n2 A small company manufactures customized bicycles . Whenever the sales department ...\n4 Process model for example 1:\n5 ‘‘‘python\n6 from utils. model_generation import ModelGenerator\n7 gen = ModelGenerator()\n8 create_process = gen. activity(’Create process instance ’)\n9 reject_order = gen. activity(’Reject order ’)\n10 accept_order = gen. activity(’Accept order ’)\n11 inform = gen.activity(’ Inform storehouse and engineering department ’)\n12 process_part_list = gen. activity (’Process part list ’)\n13 check_part = gen.activity (’Check requ ired quantity of the part ’)\n14 reserve = gen.activity(’Reserve part ’)\n15 back_order = gen.activity(’ Back - order part ’)\n16 prepare_assembly = gen. activity(’Prepare bicycle assembly ’)\n17 assemble_bicycle = gen. activity(’Assemble bicycle ’)\n18 ship_bicycle = gen.activity(’ Ship bicycle ’)\n19 finish_process = gen. activity(’Finish process instance ’)\n21 check_reserve = gen.xor( reserve , back_order)\n23 single_part = gen.partial_order(dependencies=[( check_part , check_reserve)])\n24 part_loop = gen. loop(do=single_part , redo=None)\n25 accept_poset = gen.partial_order(\n26 dependencies=[( accept_order , inform),\n27 (inform , process_part_list),\n28 (inform , prepare_assembly),\n29 (process_part_list , part_loop),\n30 (part_loop , assemble_bicycle),\n31 (prepare_assembl y , assemble_bicycle),\n32 (assemble_bicycle , ship_bicycle)])\n34 choice_accept_reject = gen.xor( accept_poset , reject_order)\n36 final_model = gen.partial_order(\n37 dependencies=[( create_process , choice_accept_reject),\n38 (choice_accept_reject , finish_process)])\n39 ‘‘‘\n41 Common errors to avoid for example 1:\n42 creating a local choice between ’reject_order ’ and ’accept_order ’ instead of ...\n\u0006 \u0005\nmisunderstanding by the LLM of the task requirements.\nExample critical errors include:\n– Failure to extract a code snippet from the response,\nindicating that the LLM either did not generate code\nas requested or used incorrect formatting.\n– Syntax errors in the generated code that prevent its\nexecution.\n– Inclusion of unsafe constructs, such as attempts to\nimport unauthorized external libraries.\n– Attempts to use non-existent constructs or parameters\ndue to LLM hallucinations.\n– Violations of POWL’s structural constraints, for\nexample, generating partial orders with cyclic depen-\ndencies.\n• Adjustable Errors: These are less severe issues that\naffect the quality or optimality of the process model\nbut do not necessarily prevent its successful generation.\nThese errors can be adjusted automatically, allowing for\na degree of ﬂexibility in their resolution. However, such\nintervention is approached with caution to prevent signiﬁ-\ncant deviations from the behavior of the intended process.\nAn example of an adjustable error is as follows:\n– Error: In an order handling process, the LLM\nattempts to reuse the activity “Pay” twice within the\nprocess—once to model a choice between paying or\ncanceling the order (i.e., ×(Cancel,Pay)) and once in\na partial order to specify that the payment occurs after\nitem selection and before delivery (i.e., a sequence\n→ (Select,Pay,Deliver)).\n123\nH. Kourani et al.\nListing 3 Example illustrating an error-handling prompt sent back to the LLM after a validation failure.\u0007 \u0004\n1 Executing your code led to an error! Please update the model to fix the error.\n2 Make sure to save the updated final model in the va riable ‘final_model ’.\n3 Error occ urred at line 12: ‘ initial_order = gen.partial_order(dependencies)’\n4 Exception: The transitive closure of the provided re lation vio lates irreflexivity!\n\u0006 \u0005\n– Auto-adjustment: This error can be automatically\nresolved by creating two distinct copies of the activity\n“Pay”.\n– Implication: The generated process model after\nauto-adjustment deviates from the intended behav-\nior since there should only be one payment activity\nin the process. Rather than duplicating the activity,\nthe two submodels should be combined to incor-\nporate the delivery activity within the choice (i.e.,\n→ (Select,×(Cancel,→ (Pay,Deliver)))).\nOur framework uses an iterative error-handling loop,\nengaging the LLM to address the identiﬁed issues. A new\nprompt that details the error and requests the LLM to address\nit, along with the conversation history, are submitted to the\nLLM. An example of such a prompt is shown in Listing 3.\nThis iterative cycle facilitates dynamic correction, leveraging\nthe LLM’s capabilities to reﬁne and improve the generated\ncode.\nCritical errors are handled by prompting the LLM repeat-\nedly until a solution is found or the maximum allowed\nattempts are reached. If the LLM fails to ﬁx the error after\nthe allowed number of attempts, the system terminates the\nprocess and marks the model generation as unsuccessful.\nAdjustable errors are resolved automatically if the LLM fails\nto address them within a few iterations.\n5 Tool support\nIn this section, we present the ProMoAI tool [ 73], which\nsupports our LLM-driven process modeling framework.\nProMoAI is implemented as a standalone Python library\n(https://pypi.org/project/promoai/ ) and as a web application\ndeployed using Streamlit ( https://promoai.streamlit.app/). In\na forthcoming release, ProMoAI will also be available as\na data application on the KNIME Analytics Platform [ 74],\nexpanding its reach to a broader audience of data analysts.\n5.1 Backend: the python library\nThe core of ProMoAI is implemented as a modular Python\nlibrary. This backend layer encapsulates the process model-\ning logic and serves as the foundation for all functionalities.\nKey components include:\n• Core Engine:Including prompt engineering, code extrac-\ntion, model generation and validation, error handling, and\niterative model re-design.\n• LLM Integration: Providing seamless support for sev-\neral LLM providers. While originally demonstrated with\nOpenAI’s models [ 73], the architecture supports inte-\ngration with other providers with minimal adaptation,\nenhancing ﬂexibility and future-prooﬁng the system\nagainst advances in AI technology. Currently, the fol-\nlowing providers are supported:\n– Google: Providing the Gemini models ( https://ai.\ngoogle.dev/).\n– OpenAI: Providing GPT-4 and O1 models and their\nvariants ( https://openai.com/).\n– DeepSeek: Providing advanced chatting and reason-\ning DeepSeek models ( https://www.deepseek.com/).\n– Anthropic: Providing Claude models ( https://www.\nanthropic.com/).\n– DeepInfra: Supporting popular open-source large\nlanguage models like Meta’s LLaMa and also enabling\ncustom model deployment ( https://deepinfra.com/).\n– Mistral AI: Providing the Mistral and Mixtral mod-\nels ( https://mistral.ai/).\n• Input Handling: Accepting three types of inputs:\n– Text: A natural language description of a process.\n– Process Model: An existing semi-block-structured\nPetri net or BPMN [ 75]. This additional feature was\nrecently added to ProMoAI and is beyond the scope\nof this paper.\n– Event Data: An event log in the XES format. When\nan event log is uploaded as the initial input, the pro-\ncess discovery algorithm from [ 22] is used to generate\nthe initial process model. Support for this input type\nwas also recently added as a new feature in ProMoAI\nand is beyond the scope of this paper.\n• Model Export and Visualization: Conversion of the\ngenerated POWL models into BPMN and Petri nets,\nexporting them in the corresponding formats, and visu-\nalization of the models. These features are powered by\nalgorithms implemented in the Python library PM4Py\n[76].\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nThe backend library can be installed in Python with the\ncommand pip install promoai . This design ensures\nthat ProMoAI can be easily integrated into custom Python\nworkﬂows independent of its user interface.\n5.2 Frontend: streamlit deployment\nThe current user interface for ProMoAI is built using Stream-\nlit (https://streamlit.io/), providing a web-based environment\nfor users who may not wish to interact directly with the\nPython library. A screenshot of the tool is shown in Fig. 2.\nKey aspects of the Streamlit deployment include:\n• LLM Conﬁguration: Users can select their desired AI\nprovider, specify the LLM name, and enter the required\nAPI key through an intuitive conﬁguration panel.\n• Multi-Input Interface: The frontend supports all three\ninput types through intuitive text and upload widgets.\nUsers can easily enter the process description as plain\ntext or drag and drop ﬁles (existing process models or\nevent logs) to move directly into the reﬁnement step.\n• Visualization and Export Options: Users can select\nbetween three view options (POWL, BPMN, and Petri\nnet); the corresponding visualization will be rendered as\nan SVG image. Users can also use the two export buttons\nto download the model in the BPMN and PNML formats.\n• Interactive Feedback Loop: After generating an ini-\ntial process model, users can provide textual feedback to\nreﬁne the model. After clicking on the “Update Model”\nbutton, the feedback comment is sent to the LLM to reﬁne\nthe model, and the model will be updated accordingly.\nAdditionally, the full feedback history is displayed to the\nuser.\nThe Streamlit application is hosted on the cloud and is\naccessible online at https://promoai.streamlit.app/. It can also\nbe hosted locally by cloning the public GitHub repository\navailable at https://github.com/humam-kourani/ProMoAI .\n6 Benchmarking state-of-the-art large\nlanguage models\nIn this section, we design a robust evaluation approach for\nbenchmarking LLMs on process modeling and we apply it\nto assess a diverse set of state-of-the-art LLMs. The objec-\ntive is to assess our framework’s capability to effectively\ngenerate high-quality business process models from natural\nlanguage descriptions. Additionally, our evaluation serves as\na benchmark for assessing the capabilities of state-of-the-art\nLLMs in a task that involves (1) modeling business processes\nstarting from natural language descriptions, (2) generating\nexecutable code, (3) following instructions embedded in the\ninput prompts, and (4) incorporating feedback to iteratively\nresolve errors and improve the quality of the outputs.\nWe start by detailing the evaluation design in Sect. 6.1.\nAfterwards, we address the selection of LLMs in Sect. 6.2\nand we discuss the obtained results in Sect. 6.3. Finally, in\nSect. 6.4, we conduct two supplementary analyses to explore\nspeciﬁc factors that may inﬂuence LLM behavior within our\nframework. Note that all data and results are available at\nhttps://github.com/humam-kourani/EvaluatingLLMsProces\nsModeling\n.\n6.1 Comparative evaluation approach\nWhile evaluating speciﬁc LLMs may provide useful insights\nat a given point in time, such assessments quickly become\nobsolete as the technology continues to evolve. Therefore,\nrather than focusing solely on individual model evaluations,\nwe propose a comparative approach that offers a more last-\ning contribution. This approach is designed to facilitate the\ncomparison of future LLMs on business process modeling,\nincluding the evaluation of their self-improvement capabili-\nties (cf. Sect. 7). Our evaluation approach not only supports\nthe assessment of LLMs as they are developed but also\nensures that comparisons remain relevant and adaptable to\nthe rapidly changing landscape of LLM technology.\nIn this section, we describe the components of our compar-\native evaluation approach. Speciﬁcally, we outline the design\nof the processes we use in our evaluation in (Sect. 6.1.1). We\nthen explain our method for generating ground truth event\nlogs via simulation (Sect. 6.1.2). Afterwards, we present\nour approach for assessing the quality of LLM-generated\nprocess models using conformance checking (Sect. 6.1.3).\nFinally, we describe the conﬁguration for handling errors\nduring model generation (Sect. 6.1.4).\n6.1.1 Design of processes\nTo evaluate the LLMs within our framework, we designed\na set of 20 distinct pairs of process descriptions and their\ncorresponding ground truth POWL model. We refer to these\nprocesses as p1, ..., p20 throughout this paper. Two processes\nwere adapted from our previous work [ 8]: an order handling\nprocess [ 9] and a hotel service process [ 71]. The remain-\ning 18 processes were created to represent diverse busi-\nness domains, including manufacturing, healthcare, ﬁnance,\nlogistics, customer service, and more. This diversity ensures\nthat the evaluation is not biased toward a speciﬁc industry or\nprocess type.\nBoth the process descriptions and the ground truth pro-\ncess models were manually designed. The POWL models\nwere not discovered from existing descriptions but rather\nconstructed alongside them to ensure alignment. That is,\n123\nH. Kourani et al.\nFig. 2 A screenshot of ProMoAI\nthe processes are constrained to the structures expressible in\nPOWL. This intentional design avoids the inclusion of pro-\ncess elements that lack clear semantics or that our framework\ncannot model due to language limitations. This approach\nallows us to assess the LLM’s performance realistically with-\nout introducing challenges that fall outside the scope of our\nframework.\nThe ground truth POWL models were converted into\nPetri nets (using the algorithm from [ 23]) and saved in the\nPNML format. The process descriptions are available at\nhttps://github.com/humam-kourani/EvaluatingLLMsProces\nsModeling/tree/main/ground_truth/ground_truth_process_\ndescriptions, while the ground truth process models are avail-\nable at https://github.com/humam-kourani/EvaluatingLLMs\nProcessModeling/tree/main/ground_truth/ground_truth_pn .\nThe ground truth processes were intentionally designed to\nvary along several dimensions. This includes:\n• Process Description Length : Ranged from 79 to 230\nwords and from 525 to 1,567 characters.\n• Level of Detail : Process descriptions were crafted to\ninclude essential elements such as key activities and indi-\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nTable 1 Characteristics of the 20 processes used in the evaluation\nProcess Process Description Ground Truth Process Model\n#Words #Chars #Activities (non-silent) #Petri Net Nodes #Choices #Loops #Partial Orders\np1 88 578 8 26 2 0 3\np2 97 629 16 43 2 2 1\np3 94 601 11 27 2 1 1\np4 98 659 10 28 1 0 1\np5 90 573 11 26 1 1 2\np6 92 580 10 27 2 1 1\np7 85 565 13 49 4 2 5\np8 92 593 12 24 1 0 1\np9 97 644 13 34 1 1 2\np10 80 525 9 35 3 1 2\np11 87 605 12 43 2 1 4\np12 93 654 10 28 1 2 2\np13 99 637 9 21 2 0 2\np14 126 837 10 20 0 1 3\np15 79 556 10 26 1 0 2\np16 219 1457 24 63 3 1 7\np17 196 1327 25 62 6 0 4\np18 230 1567 26 78 8 2 7\np19 109 653 8 23 1 1 1\np20 162 959 13 32 2 0 2\ncations of decisions (e.g., describing choices or repeated\nbehavior), ensuring sufﬁcient information for control-\nﬂow modeling. However, the descriptions varied in how\nexplicitly they speciﬁed structural elements. While none\nof the descriptions were truly vague, certain structural\ndetails were deliberately left ambiguous to investigate\nthe LLM’s ability to reason about process structure.\nFor example, some descriptions did not specify whether\ntasks should be executed concurrently or sequentially.\nSimilarly, we did not explicitly state that a task or a\npath is skippable in cases where this could be eas-\nily inferred from the context. This approach reﬂects\nreal-world ambiguity and challenges the LLM to infer\nappropriate behavior based on the process context.\n• Process Size: The number of activities in the ground truth\nmodels ranged from 8 to 26, providing a mix of simple\nand complex processes.\n• Structural Complexity : The processes were designed\nto cover different levels of structural complexity for the\nthree process constructs supported by POWL:\n– Choices: Included processes with skippable activi-\nties, simple choices between single activities, and\nchoices involving complex sub-processes.\n– Loops: Incorporated processes with repeatable activ-\nities, simple loops over single activities, and loops\ninvolving complex sub-processes.\n– Partial Orders : V aried from highly sequential pro-\ncesses to those with high concurrency, as well as\ncomplex partial orders that contain non-hierarchical\ndependencies.\nTo provide further insight into the characteristics of our\nprocess descriptions and their corresponding ground truth\nmodels, Table 1 summarizes key dimensions. For each\nprocess description, we report the number of words and char-\nacters. For each ground truth Petri net, we report the number\nof non-silent activities, the total number of nodes (transitions\nand places), and metrics reﬂecting its structural complex-\nity: the number of choice, loop, and partial order operators\nused in the original POWL model before its conversion to a\nPetri net. Listing 4 and Listing 5 show two example textual\ndescriptions, illustrating variations in length and complex-\nity. The corresponding ground truth models are shown as\nBPMN diagrams in Fig. 3, providing a visual reference for\ntheir structure.\n6.1.2 Event log generation\nAssessing the quality of LLM-generated process models\ndirectly against the input natural language description poses\nsigniﬁcant challenges for objective, automated, and repro-\nducible evaluation. To overcome this, we adopted a quantita-\ntive evaluation approach leveraging conformance checking\n123\nH. Kourani et al.\nListing 4 Textual description for process p9 (644 characters, 97 words).\u0007 \u0004\n1 The process starts with identifying an idea for a new product or improvement to an\n2 existing one. The R\\&D team conducts initial resea rch and feasibility studies ,\n3 followed by drafting design concepts. After selecting a promising design , a\n4 prototype is built using available materials and resources. The prototype\n5 undergoes various tests to assess its functionality , safety , and market potential.\n6 Feedback from the testing phase is collected , and the prototype may be refined\n7 accordingly. If a refinement is needed , then the testing phase is reinitiated. The\n8 process ends when the prototype is either approved for further development or discarded.\n\u0006 \u0005\nListing 5 Textual description for process p18 (1567 characters, 230 words).\u0007 \u0004\n1 A university enrollment system involves the following steps:\n2 Prospective students submit an application online.\n3 The admissions office reviews the application and supporting documents.\n4 If documents are missing , the applicant is notified to provide the missing items.\n5 Upon receiving all documents , the application is evaluated by the admissions\n6 committee.\n7 Concurrently , the finance department processes any application fees or waivers.\n8 If the application is accepted , an acceptance letter is sent. Otherwise , a\n9 rejection letter is sent and the process ends.\n10 After being accepted , the student must then confirm enrollment by a specified\n11 deadline; otherwise the application will be c anceled.\n12 If the student confirms , they receive orientation materials and the IT department\n13 sets up student accounts for email , online portals , and library access.\n14 If the student is international , the international student office assists with visa\n15 processing.\n16 The student obtains a student ID card and starts creating their study plan, which\n17 includes:\n18 Meeting with an academic advisor.\n19 Selecting courses.\n20 Resolving any schedule conflicts.\n21 The student begins attending classes.\n22 Throughout each semester , the student may add or drop courses within the add/drop\n23 period.\n24 At the end of the semester , grades are posted , and the student can review them online.\n25 If the student has any grievances , they can file an appeal , which in cludes:\n26 Submitting an appeal form.\n27 Meeting with the appeals committee.\n28 Awaiting a decision.\n29 The process repeats each semester until the student graduates or withdraws.\n\u0006 \u0005\ntechniques (cf. Sect. 2.5). To facilitate quantitative evalua-\ntion through conformance checking, we simulated event logs\nfrom the ground truth POWL models. These logs serve as the\nbasis for assessing the quality of the models generated by the\nLLMs using our framework.\nThe simulation was meticulously conducted to produce\ncomprehensive and representative event logs from the ground\ntruth models. This process is based on two key assumptions\nto ensure both feasibility and thorough coverage of possible\nprocess behaviors. First, we assume that all decision points\nwithin the process follow an equal distribution, meaning each\npossible choice at a decision point has an equal probability\nof being selected. This assumption simpliﬁes the simulation\nby treating all paths through decision points as equally likely,\nthereby ensuring unbiased exploration of all possible trace\nvariants (i.e., distinct activity sequences). Second, we limit\nloops to a maximum length of two iterations, allowing only\nup to two executions of the loop’s do-part. This constraint is\nnecessary to prevent the generation of inﬁnitely long event\nlogs. By capping the loop iterations, we maintain a manage-\nable size for the event logs while still capturing the essential\nrepetitive behavior of the process.\nWe performed the simulation using both process tree and\nPetri net playout techniques available in PM4Py [ 76]. For\nmodels that can be translated into process trees, we used\nthe process tree playout method because it is highly scal-\nable and allows us to set explicit limits on loop depth to\nmatch our assumption. For models involving complex partial\norder constructs that cannot be captured with process trees,\nwe used the extensive Petri net playout algorithm. In these\ncases, we applied a hybrid approach that uses the Petri net\nplayout method only for the speciﬁc submodels requiring it\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nFig. 3 Ground truth models for processes p9 and p18\n123\nH. Kourani et al.\nand the process tree playout method for simpler submodels.\nThis hybrid strategy ensured that our ﬁnal event logs accu-\nrately reﬂect the intended process behavior without solely\nrelying on the Petri net playout approach, which suffers from\nscalability issues.\nOur simulation method is designed to achieve com-\nplete coverage of all traces, given our assumption on loop\ndepth. Each simulated event log includes one instance of\nevery unique trace variant possible within the ground truth\nmodel, thereby ensuring comprehensive coverage of the pro-\ncess behavior. The simulated event logs are available at\nhttps://github.com/humam-kourani/EvaluatingLLMsProces\nsModeling/tree/main/ground_truth/ground_truth_xes_one_\ntrace_per_variant.\n6.1.3 Quality assessment\nWith the simulated ground truth event logs, we assess the\nquality of the LLM-generated models using conformance\nchecking techniques. We assess the quality of the LLM-\ngenerated models in relation to the ground truth event logs\nby calculating ﬁtness and precision metrics. Speciﬁcally,\nwe employ the token-based ﬁtness metric [ 77] and the ET-\nConformance precision metric [ 78], both implemented in the\nPM4Py library [ 76]. The overall quality score for each gener-\nated model is determined as the harmonic mean of the ﬁtness\nand precision values. High scores (approaching 1.0) indi-\ncate a high level of conformance with the ground truth event\nlogs, whereas lower scores (closer to 0.0) reﬂect poor model\nquality. This quality assessment is performed exclusively on\nthe ﬁnal model that is successfully generated after the error-\nhandling loop concludes; intermediate outputs with errors\nare not evaluated.\nStandardizing Activity Labels\nTo ensure accurate conformance checking between LLM-\ngenerated process models and the ground truth event logs, it\nis essential to standardize activity labels. Without this stan-\ndardization, inconsistencies in activity naming can result in\nmisleading quality scores, as LLM-generated models may\nassign different labels to equivalent activities, introduce new\nactivities, or combine multiple activities into a single one. To\naddress this, we extend the initial prompt with the list of activ-\nity labels of the ground truth log, presented in a random order\nto prevent the LLM from inferring sequential dependencies\nbased on label ordering. We instruct the LLM to generate\na process model using the same activity labels. While this\nmay limit the LLM’s ﬂexibility in naturally expressing pro-\ncess steps, it mirrors real-world scenarios where key process\nsteps are known, even if the overall process model is not\nformally deﬁned. Moreover, this standardizing is crucial for\nenabling conformance checking and automated quantitative\nassessment.\n6.1.4 Error handling settings\nOur framework incorporates an error-handling loop. When\nan error is detected, a new prompt is sent to the LLM contain-\ning details of the error and the relevant context from previous\niterations, instructing it to correct the issue (cf. Sect. 4.5).\nWe use a threshold of ten iterations for addressing adjustable\nerrors by the LLM. If errors persist beyond these ten iter-\nations, the framework automatically attempts to ﬁx them,\nwhich may impact the overall quality of the generated mod-\nels. For example, if the LLM reuses a sub-model within the\nsame POWL model twice, our system automatically applies\na predeﬁned correction rule that creates a copy of the reused\nsub-model to resolve the issue.\nAfter auto-resolving adjustable errors, if critical errors\nremain (e.g., a syntax error), an additional ﬁve iterations for\nhandling the errors by the LLM are permitted. By setting high\niteration thresholds, we aim to assess the LLMs’ proﬁciency\nin understanding and correcting their outputs based on feed-\nback and to ensure fairness by providing ample opportunity\nfor each model to succeed.\n6.2 Selection of large language models\nTo ensure a comprehensive and representative evaluation of\nour process modeling framework, we selected LLMs based\non a diverse set of criteria. Our evaluation methodology is\ndesigned to be ﬂexible and easily extendable, allowing new\nmodels to be seamlessly integrated as advancements in LLM\ntechnology emerge. Below, we list the key criteria that guided\nour selection process:\n• State-of-the-Art Performance: At the time of conduct-\ning the experiments, we aimed at including models that\nwere considered cutting edge.\n• Diverse Vendor Coverage: We included models from\nmajor AI providers (e.g., OpenAI, and Google, and\nAnthropic) to avoid potential vendor bias and capture\na broad market perspective.\n• Inclusion of Open-Source Models: To evaluate freely\navailable alternatives, we ensured that prominent open-\nsource models (e.g., LLaMa and Mistral) were part of the\nselection.\n• Community Adoption: We included models that have\nsubstantial community usage and were heavily featured\nin recent studies (e.g., GPT-4 and Gemini), ensuring that\nour evaluation reﬂects popular, real-world applications.\n• Varied Architectures: We selected models with varied\narchitectures, including transformer-based and mixture-\nof-experts models.\n• Context Window Capacity: We only selected models\nwith a context window of at least 16k tokens, ensuring\ncompatibility with our framework.\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\n• Varied Sizes: We chose models ranging from smaller,\nfaster variants to very large models (e.g., GPT-4o vs.\nGPT-4o-Mini).\n• Varied Specializations: Our selection covers models\nwith different strengths, including those optimized for\nspeed (e.g., Gemini-1.5-Flash-002), reasoning (O1 mod-\nels), explicit prompt following (e.g., LLaMa instruct\nmodels), and code generation (Codestral).\n• Accessibility and Reproducibility: We selected mod-\nels that are readily accessible through public APIs or\nopen-source deployments, facilitating the reproducibil-\nity of our experiments.\nBased on aforementioned criteria, we selected 16 state-\nof-the-art LLMs. The selected models are:\n• GPT-4, GPT-4o, and GPT-4o-Mini: Developed by Ope-\nnAI ( https://openai.com/), GPT-4 is renowned for its\nadvanced reasoning abilities, extensive knowledge base,\nand large context window. It excels in tasks requiring\ndeep understanding and generation of coherent, con-\ntextually relevant text. GPT-4o, and GPT-4o-Mini are\noptimized versions of GPT-4, offering faster performance\nand reduced computational requirements while maintain-\ning high-quality outputs.\n• O1-Preview and O1-Mini: The O1 series contains the\nlatest advancements from OpenAI, outperforming previ-\nous models like GPT-4o across competitive benchmarks.\nO1-Preview is designed for deep reasoning, excelling in\ncomplex tasks like math, coding, and science. O1-Mini\nis a smaller, faster, and more cost-effective version.\n• Gemini-1.5-Pro-002 and Gemini-1.5-Flash-002:D e v e l -\noped by Google ( https://ai.google/), the Gemini models\nare trained on diverse datasets. The Pro version is\ndesigned for enhanced reasoning capabilities, while the\nFlash variant emphasizes speed and efﬁciency.\n• Claude-3.5-Sonnet: From Anthropic (https://www.anthr\nopic.com/), Claude-3.5-Sonnet is designed for high per-\nformance with a focus on safety and reliability. It operates\nwith a 200k token context window and is optimized for\ncode generation and complex reasoning tasks.\n• Mistral-Large-2, Codestral, and Mixtral-8x22B: These\nmodels from Mistral AI ( https://mistral.ai/) are designed\nfor efﬁcient training and inference with support for\nmultilingual tasks. Mistral-Large-2 is top-tier reason-\ning model provided by Mistral AI for high-complexity\ntasks. Mixtral-8x22B is particularly noteworthy due to\nits mixture-of-experts architecture, which allows it to\nactivate only a subset of its parameters during each infer-\nence, making it highly efﬁcient. Codestral is optimized\nfor generating and understanding code in a wide array of\nprogramming languages.\n• Llama-3.1-405B-Instruct and Llama-3.2-90B-Vision-\nInstruct: These open-source models from Meta ( https://\nai.meta.com/) are trained on extensive corpora and\ndesigned for instruction following and complex reason-\ning tasks. Llama-3.1-405B-Instruct is notable for its large\nparameter size, enhancing its capability to handle intri-\ncate tasks.\n• Llama-3.1-Nemotron-70B-Instruct: Developed by\nNvidia ( https://www.nvidia.com/) as an advanced ver-\nsion of Meta’s Llama-3.1-70B, Nemotron leverages\nNvidia’s cutting-edge hardware and ﬁne-tuning tech-\nniques to offer high-performance capabilities.\n• Qwen2.5-72B-Instruct: Developed by Alibaba Cloud\n(https://www.alibabacloud.com/\n), Qwen2.5-72B-Instruct\nis a powerful open-source model known for its mul-\ntilingual support and proﬁciency in handling complex\ninstructions.\n• WizardLM-2-8x22B: This is an advanced open-source\nmodel designed by Microsoft ( https://www.microsoft.\ncom/).\nNote that for the open-source models Llama-3.1-405B-\nInstruct, Llama-3.2-90B-Vision-Instruct, Llama-3.1-Nemotron-\n70B-Instruct, Qwen2.5-72B-Instruct, and WizardLM-2-8x22B,\nwe used the instances hosted by Deep Infra ( https://deepinfra.\ncom/).\n6.3 Evaluation results\nIn this section, we present the results of our evaluation,\nfocusing on the error-handling performance (Sect. 6.3.1),\nquality of the generated models (Sect. 6.3.2), time efﬁciency\n(Sect. 6.3.3), and overall observations (Sect. 6.3.4).\nFigure 4 shows three example models generated for the\nR&D process (p9). The high-quality model generated with\nO1-Mini (Score: 0.97) closely matches the ground truth\nmodel (Fig. 3a). In contrast, the model from Llama-3.2-90B-\nVision-Instruct (Score: 0.83) contains a major ﬂaw: while\nit correctly identiﬁes the loop for reﬁning the prototype, it\nincorrectly places the testing activities outside the loop body.\nThe model generated by Codestral (Score: 0.56) exhibits even\nmore critical issues, introducing two incorrect concurrent\nbranches and duplicating activities within them.\n6.3.1 Error handling performance\nWe analyzed how each LLM performed in terms of the\nnumber of iterations required to generate a valid process\nmodel without errors. In Table 2, we report the average\nnumber of iterations required to produce a valid model by\neach LLM, the number of cases where only one iteration\nwas needed (indicating no errors), the number of instances\nwhere more than ten iterations were necessary (indicating\n123\nH. Kourani et al.\nFig. 4 LLM-generated models for the R&D process (p9)\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nTable 2 Error handling performance metrics\nModel Avg. Num. Iterations Num. Cases without Errors Num. Cases with Auto-Adjustment Num. Cases with Failures\nClaude-3.5-Sonnet 1 .35 16 0 0\nO1-Mini 1 .41 4 0 0\nO1-Preview 1 .51 4 0 0\nGemini-1.5-Pro-002 1 .95 13 0 0\nGPT-4o 2 .25 9 0 0\nLlama-3.1-405B-Instruct 2 .55 9 0 0\nMistral-Large-2 2 .61 0 0 0\nLlama-3.2-90B-Vision-Instruct 2 .95 8 0 0\nGemini-1.5-Flash-002 3 .34 0 0\nMixtral-8x22B 3 .61 0 3 1\nGPT-4 3 .92 0 0\nCodestral 3 .97 2 1\nGPT-4o-Mini 4 .05 6 3 0\nLlama-3.1-Nemotron-70B-Instruct 4 .05 3 1 0\nQwen2.5-72B-Instruct 4 .65 4 2 0\nWizardLM-2-8x22B 5 .28 5 0\n123\nH. Kourani et al.\nfailure to resolve adjustable errors), and the number of cases\nwhere no valid model was generated after 15 iterations (indi-\ncating failure to resolve critical errors). In addition to the\nmetrics reported in this section, Sect. 6.3.3 analyzes time\nperformance and the impact of errors, comparing the aver-\nage time per iteration and the average total generation time\nfor each LLM.\nWe do not undertake a detailed error-type analysis in\nthis evaluation because we observed that the vast major-\nity of errors were validation-related. These errors typically\nstemmed from misinterpretations of POWL semantics (e.g.,\ngenerating partial orders with cycles), particularly in pro-\ncesses with higher structural complexity. This suggests that\nthe LLMs generally succeeded in following our prompt\ninstructions to generate syntactically valid code, and that\nmost issues arose from the inherent complexity of the mod-\neled behavior rather than from basic generation failures.\nClaude-3.5-Sonnet demonstrated exceptional performance,\nwith an average of 1.35 iterations and 16 out of 20 instances\nwhere only one iteration was sufﬁcient to produce a valid\nmodel. This performance was closely mirrored by the O1\nmodels and Gemini-1.5-Pro-002, which averaged less than\ntwo iterations and managed to produce models without any\nerrors in 14 and 13 instances, respectively. These results sug-\ngest that these models possess robust reasoning capabilities\nand an ability to generate high-quality results on the ﬁrst\nattempt.\nAmong the models evaluated, WizardLM-2-8x22B stood\nout with the highest average number of iterations (5.2) and\nﬁve instances where more than 10 iterations were needed.\nHowever, it is notable that despite these higher iteration\ncounts, WizardLM-2-8x22B did not experience any failures.\nModels such as GPT-4 and Gemini-1.5-Flash-002 demon-\nstrated low single-iteration success rates (2 and 4 cases,\nrespectively); however, these models were able to achieve\na moderate average number of iterations and managed to\nresolve both adjustable and critical errors. This underscores\ntheir effectiveness in incorporating feedback to iteratively\nimprove the quality of their outputs. In contrast, other models\nshowed a higher tendency for requiring additional iterations\nand occasional failures. Mixtral-8x22B necessitated man-\nual adjustments in three instances and failed to generate a\nvalid model once after 15 iterations. Similarly, Codestral\nencountered two such instances of manual adjustments and\none failure. The results for Codestral, which is optimized\nfor code generation, were particularly surprising. Despite its\nspecialization, this model performed worse than the other\ntwo Mistral models we considered. This suggests that task-\noptimized models face challenges on their primary task when\napplied to new domains.\n6.3.2 Quality of the generated models\nAs described in Sect. 6.1.3, we assess the quality of the gen-\nerated process models using the harmonic mean of ﬁtness\nand precision scores obtained from conformance checking\nthe models against the ground truth event logs. The average\nobtained quality scores for each LLM are reported in Table 3.\nThe results demonstrate signiﬁcant variation in the quality\nof generated models across different LLMs. Claude-3.5-\nSonnet leads the pack with an impressive average score\nof 0.93, closely approaching the average score of the\nground truth models (0.98).\n1 Following Claude-3.5-Sonnet,\nthe O1-Preview and O1-Mini models also exhibit strong\nperformance, achieving average scores of 0.92 and 0.91,\nrespectively. These high scores underscore the effectiveness\nof the O1 series in generating accurate and reliable results,\nattributable to their advanced reasoning capabilities.\nA closer examination of the results reveals the following\nobservations and patterns:\n(1) Positive Correlation Between Iteration Perfor-\nmance and Quality: There is a notable correlation between\nerror-handling performance and model quality. Models like\nClaude-3.5-Sonnet, O1-Mini, O1-Preview, and Gemini-1.5-\nPro-002, which produced less errors and required fewer\niterations to generate valid process models, also achieved\nhigher quality scores. Conversely, models that required more\niterations or experienced failures, such as Mixtral-8x22B,\nCodestral, and WizardLM-2-8x22B, tended to have lower\nscores, reﬂecting potential compromises in model accuracy\ndue to extended correction processes. This indicates that the\nability to generate error-free outputs promptly contributes to\nthe overall quality of the process models.\n(2) Quality Consistency on Average: Despite the inher-\nent non-determinism in LLM outputs, we observe consistent\nquality trends within similar groups of models. Speciﬁcally,\nthe O1 models achieved average scores of 0.92 and 0.91,\nwhile the two instances of the Llama 3.1 family scored on\naverage 0.86 and 0.83. The three models within the GPT-4\nfamily recorded average scores ranging from 0.74 to 0.76.\nAdditionally, across four independent runs of Gemini-1.5-\nPro-002 (cf. Sect. 7.1), the model consistently exhibited\nsimilar performance with average scores ranging between\n0.86 and 0.88. The observed patterns suggest that, although\nre-executing the same or similar LLMs on identical tasks\ncan yield varying results, the average quality remains stable\nwithin each model group.\n(3) Impact of Speed Optimization: Models optimized\nfor speed exhibited varied behaviors in terms of quality\nscores. Notably, the OpenAI models O1-Mini and GPT-4o-\n1 A perfect score (1.0) is unattainable in many cases due to the presence\nof loops within the process models, which inherently allow for inﬁnite\nbehavior.\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nTable 3 Average quality scores\nModel Avg. Score\nGround Truth 0.98\nClaude-3.5-Sonnet 0.93\nO1-Preview 0.92\nO1-Mini 0.91\nGemini-1.5-Pro-002 0.87\nLlama-3.1-405B-Instruct 0.86\nLlama-3.1-Nemotron-70B-Instruct 0.83\nLlama-3.2-90B-Vision-Instruct 0.80\nQwen2.5-72B-Instruct 0.80\nMistral-Large-2 0.78\nGPT-4 0.76\nGPT-4o 0.76\nGPT-4o-Mini 0.74\nWizardLM-2-8x22B 0.73\nCodestral 0.73\nGemini-1.5-Flash-002 0.73\nMixtral-8x22B 0.72\nMini maintained quality scores comparable to their base\ncounterparts, O1-Preview and GPT-4o, respectively. This\nconsistency suggests that optimization for speed in these\nmodels does not signiﬁcantly compromise the quality of their\noutputs. In contrast, Gemini-1.5-Flash-002 demonstrated\nlower quality scores compared to the Gemini-1.5-Pro-002\nvariant. However, Google promotes Gemini-1.5-Flash-002\nas a small, lightweight model designed for tasks where\nspeed and cost-effectiveness matter the most; therefore, it\nis unfair to compare its performance with the Pro model.\nThese examples illustrate that speed optimization strategies\ncan signiﬁcantly differ based on the desired balance between\nspeed enhancement and quality preservation, either aiming\nfor moderate speed improvements with minimal quality loss\nor prioritizing speed as the primary optimization goal.\n6.3.3 Time efﬁciency\nTime efﬁciency is a critical factor in evaluating the prac-\nticality and scalability of different LLMs in our framework.\nWe assessed the performance of each LLM by measuring\nboth the average cumulative time taken across all iterations\nto generate the process model and the average time per a sin-\ngle iteration. Table 4 reports these metrics, ordered by the\naverage total time in ascending order.\nGemini-1.5-Flash-002 and Codestral emerge as the fastest\nmodels in terms of per-iteration processing speed, with aver-\nage times of 4.03 s and 7.98 s, respectively. However, their\nrapid response times come at the cost of requiring more\niterations and a compromise in quality, diminishing their\noverall effectiveness. In contrast, Claude-3.5-Sonnet and\nGemini-1.5-Pro-002 exemplify models that strike an excel-\nlent balance between speed and quality. Claude-3.5-Sonnet is\nparticularly noteworthy, ranking second in total average time\nat 23.63 s while ranking ﬁrst in average quality at a score of\n0.93. Similarly, Gemini-1.5-Pro-002 demonstrate robust per-\nformance with average total times around 24.86 s and a strong\naverage quality score of 0.87. These models achieve their\nefﬁciency not only through relatively fast per-iteration times\nbut also by requiring less iterations and effectively handling\nerrors.\nThe reasoning models of the O1 series (O1-Mini and\nO1-Preview) exhibit longer per-iteration processing times\n(39.16 s and 90.84 s, respectively). Despite their slower\nspeeds per iteration, they maintain moderate total processing\ntimes due to their high efﬁciency in producing valid models\nquickly, minimizing the need for error handling iterations.\nO1-Mini, in particular, achieves a similar quality score to\nO1-Preview but at a substantially lower total time, making\nit a highly efﬁcient option within the O1 series. This pat-\ntern extends to other OpenAI models, where GPT-4o-Mini\noutperforms GPT-4 and GPT-4o in speed while maintaining\nsimilar quality levels.\nAt the lower end of the efﬁciency spectrum, WizardLM-\n2-8x22B, Mistral-Large-2, and Llama-3.1-Nemotron-70B-\nInstruct stand out as the least time-efﬁcient models, with\naverage total times between 181.47 and 167.98 s.\n6.3.4 Summary of results\nIn summary, our evaluation demonstrates that Claude-3.5-\nSonnet stands out as the best-performing LLM in our\nframework, delivering the highest quality process models\nwith minimal iterations and efﬁcient error handling. Gemini-\n1.5-Pro-002 follows closely behind, with lower quality scores\nbut also offering a good balance between quality and efﬁ-\nciency. The O1 series models (O1-Mini and O1-Preview)\nexhibit excellent performance but fall behind in the overall\ntime performance. Speed-optimized models like Gemini-1.5-\nFlash-002 provide faster per-iteration times but often require\nmore iterations and may compromise on quality.\nThese ﬁndings highlight the trade-offs between speed,\niteration count, quality, and cost across different LLMs,\nemphasizing the importance of selecting the right LLM to\nachieve the best balance. Note that we did not include cost\ninformation in our evaluation due to the rapidly changing\nlandscape of LLM pricing. However, when taking cost into\naccount, Gemini-1.5-Pro-002 offered the best trade-off as it\nwas freely available through APIs for limited, small-scale\ntesting.\n123\nH. Kourani et al.\nTable 4 Time efﬁciency metrics Model Avg. Total Time (s) Avg. Time per Iteration (s)\nGemini-1.5-Flash-002 14 .51 4 .03\nClaude-3.5-Sonnet 23 .63 16 .88\nGemini-1.5-Pro-002 24 .86 12 .06\nCodestral 38 .27 7 .98\nMixtral-8x22B 52 .88 12 .20\nO1-Mini 55 .29 39 .16\nGPT-4o-Mini 56 .20 12 .11\nLlama-3.1-405B-Instruct 67 .86 24 .69\nLlama-3.2-90B-Vision-Instruct 72 .97 21 .56\nGPT-4o 78 .98 33 .72\nGPT-4 108 .55 26 .56\nQwen2.5-72B-Instruct 126 .98 20 .59\nO1-Preview 145 .48 90 .84\nLlama-3.1-Nemotron-70B-Instruct 167 .98 38 .20\nMistral-Large-2 169 .66 59 .46\nWizardLM-2-8x22B 181 .47 29 .57\n6.4 Supplementary analyses\nIn addition to the main benchmarking results, we conducted\nsupplementary analyses to explore speciﬁc aspects of LLM\nbehavior within our framework, namely the effect of stan-\ndardizing activity labels and the inﬂuence of background\nknowledge.\n6.4.1 Effect of standardizing activity labels\nTo explore the potential impact of standardizing activity\nlabels, we conducted a supplementary experiment focused on\nunderstanding how LLMs perform without predeﬁned labels.\nWe selected two processes from our benchmark: the R&D\nprocess (described in Listing 4, 13 activities in its ground\ntruth model) and the university enrollment process (described\nin Listing 5, 26 activities in its ground truth model). We\nused two LLMs for this experiment: Gemini-1.5-Pro-002 and\nGPT-4o. We generated process models based only on the nat-\nural language descriptions, omitting the list of ground truth\nactivity labels from the prompt. Subsequently, we manually\nanalyzed the generated models, mapping the LLM-created\nactivity labels back to the ground truth labels to identify sim-\nilarities and differences. The mappings are shown in Tables 5\nand 6.\nFor both processes, the LLMs generated activity labels\nthat corresponded well with the ground truth activities, often\ninvolving only minor variations in phrasing (e.g., “Identify\nIdea” vs. “Identify idea for new product or improvement”).\nThe core difference observed for the R&D process, how-\never, was a tendency for both LLMs to aggregate concurrent\nactivities into a single step. As detailed in Table 5, concurrent\nactivities like “Conduct initial research” and “Conduct fea-\nsibility studies” were combined by the LLMs into composite\nactivities such as “Research and Feasibility Study”. Simi-\nlarly, distinct testing activities (“Test functionality”, “Test\nsafety”, “Test market potential”) were merged into a single\n“Test Prototype” activity. This suggests that without explicit\nlabel guidance, LLMs may naturally generate models at a\nslightly higher level of abstraction, though the fundamental\nprocess steps were still captured.\nFor the more complex university enrollment process,\n24 out of 26 of activities were correctly identiﬁed and\nlabeled by both LLMs, with only minor phrasing differ-\nences, as shown in Table 6. However, a notable deviation\nconcerned the ﬁnal outcome speciﬁed in the description:\n“The process repeats each semester until the student grad-\nuates or withdraws”. When operating without the predeﬁned\nlabel list, both Gemini-1.5-Pro-002 and GPT-4o omitted\nexplicit activities corresponding to “Graduate” and “With-\ndraw” (highlighted in Table 6).\nThis supplementary analysis conﬁrms that providing\nactivity labels inﬂuences the LLM output, primarily by ensur-\ning granularity consistency and preventing the omission of\ndetails. Nonetheless, the core process structure and main\nactivities were largely captured correctly in our tests even\nwithout this standardization. Therefore, while acknowledg-\ning its effect, we assess the standardization of activity labels\nas a suitable method for our main evaluation, serving its nec-\nessary purpose of enabling rigorous conformance checking\nfor automated quality assessment.\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nTable 5 Mapping activities between ground truth and LLM-generated models for the R&D process without predeﬁned labels\nGround Truth Gemini-1.5-Pro-002 GPT-4o\nIdentify idea for new product or improvement Identify Idea Identify idea for new product or improvement\nConduct initial research\nConduct feasibility studies Research and Feasibility Study Conduct initial research and feasibility studies\nDraft design concepts Draft Design Concepts Draft design concepts\nSelect promising design Select Design Select promising design\nBuild prototype Build Prototype Build prototype using available materials and resources\nTest functionality\nTest safety\nTest market potential Test Prototype Test prototype functionality, safety, and market potential\nCollect feedback from testing phase Collect Feedback Collect feedback from testing phase\nReﬁne prototype Reﬁne Prototype Reﬁne prototype\nApprove prototype for further development Approve Prototype Approve prototype for further development\nDiscard prototype Discard Prototype Discard prototype\nBold rows show where LLMs combined multiple ground truth activities\n123\nH. Kourani et al.\nTable 6 Mapping activities between ground truth and LLM-generated models for the university enrollment process without predeﬁned labels\nGround Truth Gemini-1.5-Pro-002 GPT-4o\nSubmit application online Submit application online Submit application\nReview application and documents Review application and documents Review application and supporting documents\nNotify applicant of missing documents Notify about missing documents Notify applicant to provide missing documents\nProvide missing documents Receive all documents Provide missing documents\nEvaluate application by admissions committee Evaluate application Evaluate application\nProcess fees or waivers Process application fees/waivers Process application fees or waivers\nSend acceptance letter Send acceptance letter Send acceptance letter\nSend rejection letter Send rejection letter Send rejection letter\nConﬁrm enrollment Conﬁrm enrollment Conﬁrm enrollment by deadline\nCancel application Cancel application Application canceled\nSend orientation materials Send orientation materials Send orientation materials\nSet up IT accounts Setup student accounts Set up student accounts\nAssist with visa processing Visa processing Assist with visa processing\nObtain student ID card Obtain student ID card Obtain student ID card\nMeet with academic advisor Meet with academic advisor Meet with academic advisor\nSelect courses Select courses Select courses\nResolve schedule conﬂicts Resolve schedule conﬂicts Resolve schedule conﬂicts\nBegin attending classes Attend classes Begin attending classes\nAdd/drop courses Add/drop courses Add/drop courses during add/drop period\nPost grades Post grades Post grades\nReview grades online Review grades online Review grades online\nSubmit appeal form Submit appeal form Submit appeal form\nMeet with appeals committee Meet with appeals committee Meet with appeals committee\nAwait decision Await decision Await decision\nWithdraw\nGraduate\nBold rows show activities omitted by the LLMs\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\n6.4.2 Effect of background knowledge\nBeyond the direct impact of predeﬁned activity labels, an\nimportant consideration when using LLMs for process mod-\neling is how they balance strict adherence to the input text\nwith their built-in domain knowledge. LLMs are more than\ntext-processing engines—they also carry extensive knowl-\nedge acquired during pre-training. This leads to a key\nquestion in process modeling: when presented with a tex-\ntual description, does the LLM follow the exact sequence\nand structure? Or does it draw on its internal knowledge to\nadjust the process accordingly? Understanding how LLMs\nhandle this trade-off is important, especially since real-world\nprocess descriptions often include ambiguities and inconsis-\ntencies.\nTo explore this, we conducted a focused experiment using\na process description that intentionally included a counter-\nintuitive causal sequence in a typical order handling scenario.\nThe process description is shown in Listing 6. This descrip-\ntion clearly reverses the normal ﬂow (e.g., delivery happens\nbefore login). We gave this description to two different\nLLMs, Gemini-1.5-Pro-002 and GPT-4o, using two types\nof prompts: (i) the standard prompt used in our main experi-\nments, and (ii) a prompt that added the following instruction:\n“Strictly stick to the process description and do not use your\ndomain knowledge”.\nThe results, shown in Fig. 5, revealed interesting dif-\nferences between the models’ default behavior. With the\nstandard prompt, Gemini-1.5-Pro-002 chose to ignore the\nillogical order in the text and instead generated a model fol-\nlowing a standard process ﬂow (login ﬁrst, delivery last),\nas seen in Fig. 5b. In contrast, GPT-4o using the same\nstandard prompt followed the unusual order given in the\ntext (Fig. 5a). Notably, even with just these two LLMs,\nwe observed distinct default approaches. This suggests that\nthe way different LLMs balance textual instructions against\ntheir internal knowledge can vary considerably. However, the\nexperiment also showed that this behavior can be controlled.\nWhen we used the prompt that speciﬁcally told Gemini-1.5-\nPro-002 to stick strictly to the description, it successfully\nignored its domain knowledge and created a model match-\ning the illogical sequence from the text (Fig. 5c).\nThese ﬁndings show that while LLMs might indeed\napply their background knowledge, potentially correcting\nperceived errors or ambiguities in the input, this tendency\nvaries and can be directed using prompt engineering. In\nour process modeling framework, we do not explicitly pro-\nhibit the LLM from using domain knowledge by default. We\nbelieve that in many practical situations, users might pro-\nvide descriptions that are slightly incomplete or ambiguous.\nIn these cases, allowing the LLM to use its general process\nunderstanding could be beneﬁcial, helping to ﬁll gaps or clar-\nify steps in a useful way.\n7 Evaluating LLM self-improvement\nstrategies\nIn this section, we investigate the ability of LLMs to improve\nthe quality of their outputs through self-improvement strate-\ngies. We aim to examine how LLMs can autonomously evalu-\nate, reﬁne, and optimize their performance within our frame-\nwork for process modeling. Speciﬁcally, we explore three\ntechniques: self-evaluation (cf. Sect. 7.1), self-optimization\nof input (cf. Sect. 7.2), and self-optimization of output (cf.\nSect. 7.3).\nWe primarily use Gemini-1.5-Pro-002 in this section\ndue to its optimal trade-off between performance, time,\nand cost, as discussed in Sect. 6. We utilize the same\nset of 20 processes previously described in Sect. 6.A l l\nresults are available at https://github.com/humam-kourani/\nEvaluatingLLMsProcessModeling.\n7.1 LLM self-evaluation\nIn this section, we explore the potential of self-evaluation\nby LLMs to enhance the quality of their outputs within our\nprocess modeling framework.\nSelf-evaluation capitalizes on the reasoning capabilities\nof LLMs, enabling them to assess and potentially reﬁne\ntheir own outputs. The aim is to reduce errors and hallucina-\ntions, thereby increasing the reliability and accuracy of LLM\noutputs [ 79]. Given the inherent non-determinism of LLM\noutputs, where responses can vary signiﬁcantly across dif-\nferent sessions [ 80], self-evaluation might help in stabilizing\noutputs and mitigating variability.\nSelf-evaluation can be utilized in several ways: providing\na quality score to the user to indicate the LLM’s conﬁdence in\nits own answers, generating multiple candidate outputs and\nselecting the best one, or combining several responses into\na single, more robust output. In our framework, we imple-\nment LLM self-evaluation by generating multiple candidate\nprocess models for each process description, and then we\ninstruct the LLM to assess them based on predeﬁned criteria\nand select the best model.\n7.1.1 Implementation and experimental setup\nFor each process description, we conducted four independent\nruns, generating four candidate models (labeled R1 to R4).\nAfter generating the four candidate models, we tasked the\nLLM with self-evaluating them and selecting the best one.\nIn addition to Gemini-1.5-Pro-002, we replicated the exper-\niment using Gemini-1.5-Flash-002 to assess whether the\nimpact of self-evaluation differs between high-performing\nand lower-performing LLMs.\nWe crafted a comprehensive prompt that includes the ini-\ntial task description, the speciﬁc process description, the four\n123\nH. Kourani et al.\nListing 6 Textual description for the reversed order handling process.\u0007 \u0004\n1 The process starts by delivering the package to the customer. Then, the customer\n2 completes the payment and confirm s the order. After that , the cu stomer carefully\n3 reviews the order details , provides shipping information , and selects preferred\n4 delivery options. Next , the customer chooses items from the available products.\n5 Finally , the entire procedure concludes when the customer logs in into the ordering\n6 system.\n\u0006 \u0005\nFig. 5 LLM-generated models for the reversed order handling process description, illustrating the effect of domain knowledge and prompt\ninstructions\ngenerated candidate models, and a request for the LLM to\nself-evaluate these candidates and provide an overall quality\nscore for each. Additionally, we computed the ground truth\nquality scores as described in Sect. 6.1.3 to validate whether\nthe LLM’s selections align with these scores, thereby verify-\ning the effectiveness of its self-evaluation capabilities.\nEvaluation Criteria\nWe instructed the LLM to evaluate the generated models\nbased on sets of criteria to ensure a comprehensive assess-\nment of each candidate model’s quality according to different\nperspectives. We created two sets of evaluation criteria: a\ngeneral set (cf. Listing 7) and a conformance-based set (cf.\nListing 8). The ﬁrst set of criteria focuses on broader aspects\nof model quality, while the second set aligns with the quality\nmetrics used to compute the ground truth scores.\n7.1.2 Results and discussion\nThe results of the LLM self-evaluation experiments are sum-\nmarized in Table 7. It reports the average quality scores of\nthe process models without self-evaluation, the number of\ninstances where the LLM’s selected process models are a\nsubset of the best process models based on the quality assess-\nment, the number of instances of exact matches, and the\naverage quality scores of the process models post LLM self-\nevaluation and selection. To account for minor variations in\nquality scores, we applied a 0.02 buffer when determining\nthe best process models in our match calculations.\nIt is important to note that the variations between the\ncandidate runs (R1 to R4) were minimal in many cases, par-\nticularly for simpler processes or when the LLM consistently\nconverged on a speciﬁc interpretation. When candidates\nare nearly identical in quality, the potential beneﬁt of self-\nevaluation through selection is inherently constrained, and\nassessing the ‘correctness’ of the LLM’s choice becomes\nless meaningful. Therefore, while we report the alignment\nbetween the LLM’s selection and the highest-scoring models\n(subset matches and exact matches in Table 7), we empha-\nsize the change in the average quality score of the selected\nmodels compared to the average scores for the single candi-\ndate runs. This provides a clearer perspective on the overall\npractical impact of this self-evaluation strategy on the ﬁnal\noutput quality, mitigating the issue of limited variability in\nsome instances.\nFor Gemini-1.5-Pro-002, the initial average quality of the\ncandidate process models ranged between 0.86 and 0.88.\nAfter applying self-evaluation and selection, the average\nquality increased to 0.91, indicating an overall improvement.\nHowever, these improvements were not consistent across all\ncases. In 5 out of 20 cases, the LLM’s selected best models\ndid not align with the selection based on the quality assess-\nment. Despite these discrepancies, the increase in average\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nListing 7 Evaluation criteria in the prompt for general self-evaluation.\u0007 \u0004\n1 **Evaluation Criteria:**\n2 - **Behavior Accuracy:** How accurately does the model capture the in tended process\n3 behavior?\n4 - **Completeness:** Does the model include all necessary activities as described?\n5 - **Correctness:** Are the control flows (e.g., partial orders , choices , loops)\n6 correctly implemented?\n\u0006 \u0005\nListing 8 Evaluation criteria in the prompt for conformance-based self-evaluation.\u0007 \u0004\n1 **Evaluation Criteria:**\n2 - **Fitness:** Evaluate how well the process model can reproduce the behaviors of\n3 the process according to the process description.\n4 - **Precision:** Evaluate the extent to which the process model exclusively\n5 represents behaviors that are allowed in the process according to the process\n6 description.\n\u0006 \u0005\nTable 7 Impact of LLM self-evaluation on process model quality\nLLM Avg. Quality\nWithout\nSelf-Eval.\n(R1-R4)\nEvaluation Criteria Subset Match Exact Match Avg. Quality\nWith Self-Eval.\nGemini-1.5-Pro-002 0.86–0.88 General 15/20 5/20 0.91\nConformance 15/20 7/20 0.91\nGemini-1.5-Flash-002 0.73–0.75 General 4/20 0/20 0.72\nConformance 3/20 0/20 0.72\nUnder both sets of evaluation criteria (general and conformance-based), we report the average quality scores of the process models before self-\nevaluation and selection, the number of instances where the LLM’s selected process models are a subset of the best process models based on the\nquality assessment, the number of instances of exact matches, and the average quality scores of the process models post LLM self-evaluation and\nselection\nquality suggests that the self-evaluation strategy was gen-\nerally effective for Gemini-1.5-Pro-002. The LLM’s ability\nto select models that improved the overall average quality\ndemonstrates its capacity to critically assess its outputs.\nIn contrast, Gemini-1.5-Flash-002 exhibited lower per-\nformance. The initial average quality of its candidate models\nwas between 0.73 and 0.75. After self-evaluation, the aver-\nage quality slightly decreased to 0.72. The LLM selected\nthe wrong models in 16 or 17 out of 20 cases, depending\non the evaluation criteria used. Although the ﬁnal impact on\naverage quality was not high, these results suggest that the\nself-evaluation strategy may be disadvantageous for Gemini-\n1.5-Flash-002.\nRegarding the evaluation criteria, the results indicate that\nthere was no signiﬁcant difference between using the general\nevaluation criteria and the conformance-based criteria for\nboth LLMs. The LLMs made similar assessments regardless\nof the prompted criteria, suggesting that the choice of evalua-\ntion criteria may not signiﬁcantly inﬂuence the effectiveness\nof the self-evaluation.\nEffective self-evaluation inherently relies on the LLM’s\ninstruction-following and reasoning capabilities. Our results\nalign with this expectation: Gemini-1.5-Pro-002, the more\ncapable model in our test pair, led to an overall improvement\nin average quality, while Gemini-1.5-Flash-002, the smaller\nand faster model, struggled signiﬁcantly with the task. This\nsuggests that the utility of this self-evaluation approach may\nbe limited for models lacking a sufﬁcient level of reasoning\nability, potentially even proving detrimental. However, draw-\ning broad conclusions is challenging, as this observation is\nbased on comparing only two models within the Gemini fam-\nily.\nIn summary, the effectiveness of employing LLM self-\nevaluation to select the best output among multiple candi-\ndates appears to be highly dependent on the selection of the\nLLM. Our results highlight that this strategy might be more\nbeneﬁcial for higher-performing models like Gemini-1.5-\nPro-002; however, even for such models, the improvements\nare not consistent across all cases. Consequently, while LLM\nself-evaluation holds potential, the question of whether the\npotential improvements justify the additional time and costs\nremains unresolved, as the performance gains vary depend-\ning on the chosen LLM.\n123\nH. Kourani et al.\nWe acknowledge that these ﬁndings are limited as we only\nused two LLMs in our experiments. It is also important to\nnote that our conclusions cannot be generalized to broader\napplications. Given that all candidate process models were\ngenerated by the same LLM, their quality levels are inher-\nently close. We anticipate that LLM self-evaluation might\nyield better results when applied to outputs with larger dis-\nparities in quality.\n7.2 LLM self-optimization of input\nIn this section, we investigate the potential of LLMs to\nenhance the quality of process models by self-optimizing the\ninput process descriptions. Unlike traditional self-improvement\nmethods, which aim at reﬁning the output against a ﬁxed\ninput [ 11], we explore whether improving the clarity and\ndetail of the input itself leads to a better output. The hypoth-\nesis is that by allowing LLMs to reﬁne and enrich the initial\nprocess descriptions, they might produce higher-quality pro-\ncess models.\n7.2.1 Implementation and experimental setup\nFor each process description, we created two additional ver-\nsions: a summarized version that retains 50–80% of the\noriginal description’s length (medium-length version), and\na compact, very high-level version with 15–35% of the orig-\ninal length (short version). By crafting shorter versions of the\nprocess descriptions, we aimed to introduce varying levels of\ndetail and speciﬁcity. The motivation behind this was to give\nthe LLM more latitude to enhance and clarify the descrip-\ntions, potentially leading to improved process models upon\nself-optimization.\nFor each of the three versions (long, medium-length, and\nshort), we instructed Gemini-1.5-Pro-002 to improve the\ndescription using the prompt illustrated in Listing 9.I ti s\nimportant to acknowledge a conceptual distinction between\nthis input optimization experiment and self-improvement\ntechniques focusing purely on output reﬁnement. Modify-\ning the input description, even with the goal of improvement,\ninherently risks altering the target speciﬁcation against which\nthe ﬁnal model is generated. Recognizing this potential issue,\nthe prompt (Listing 9) was deliberately designed with con-\nstraints: while encouraging the LLM to enrich the description\nby adding relevant details, clarifying ambiguities, and mak-\ning process constructs explicit, it crucially instructed the\nLLM to ensure all additions were relevant, accurate, and\ndirectly related to the original process. This aimed to guide\nthe LLM toward clariﬁcation within the bounds of the origi-\nnal process, rather than introducing unrelated information or\nfundamentally altering the described behavior.\n7.2.2 Results and discussion\nWe evaluated the quality of the process models gener-\nated from both the original and the LLM-improved process\ndescriptions. A summary of the results is presented in Table 8.\nContrary to our hypothesis, our investigation reveals\nthat, for our speciﬁc application and framework, LLM self-\noptimization of input does not yield consistent beneﬁts and\nmay even be counterproductive. For the original long descrip-\ntions, the average quality score was 0.87, which decreased\nto 0.79 after LLM self-optimization, with only 6 out of\n20 cases showing improvement. For the medium-length\ndescriptions, the average score increased from 0.75 to 0.82\npost-optimization, with 11 cases showing improvement and\n9 not. For the short descriptions, the average score decreased\nfrom 0.78 to 0.72 after self-optimization, with improvements\nin only 8 cases.\nThese results suggest that the changes introduced by the\nLLM during the self-optimization process did not systemat-\nically contribute to better model quality. While LLMs can\ngenerate coherent text, they may lack the speciﬁc domain\nknowledge required to accurately enrich process descriptions\nin a way that leads to better process models.\nBased on our observations across various processes, the\nprompt designed to discourage hallucinations (cf. Listing 9)\nappeared largely effective; quality degradation primarily\nstemmed from instances where the LLM introduced com-\nplicating details that it subsequently struggled to model\naccurately, rather than from generating entirely irrelevant\ninformation. To better understand how such quality degra-\ndation can occur through this self-optimization process, we\nexamine a speciﬁc case where the negative impact was par-\nticularly pronounced: the inventory replenishment process\n( p3). For this process, self-optimization for the long descrip-\ntion by Gemini-1.5-Pro-002 resulted in a drop in the quality\nscore from 0.94 to 0.59. One notable modiﬁcation involved\nthe description of the reorder trigger:\n• Original: “This process begins with monitoring inventory\nlevels in a warehouse or store. When stock reaches a\npredeﬁned threshold, an automated alert or manual check\nsignals the need to reorder”.\n• LLM-Optimized: “This process begins with continuous\nmonitoring of inventory levels for each product in a ware-\nhouse or store. When the stock level of a product reaches\na predeﬁned minimum threshold, a system generates an\nautomated alert. In parallel, a manual check of inven-\ntory levels is performed periodically. If the manual check\nreveals a product’s stock level below the threshold, it also\ntriggers a reorder signal. These two reorder triggers (auto-\nmated alert or manual check) initiate the next step”.\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nListing 9 Prompt for input optimization.\u0007 \u0004\n1 You are provided with a process description. Your task is to optimize this\n2 description to make it richer and more detailed , while ensuring that all additions\n3 are relevant , accurate , and directly related to the original process. The goal is\n4 to make the description more comprehensive and sui table for process modeling\n5 purposes.\n7 Possible areas for enhancement include:\n8 - **Detail Enhancement:** Add spe cific details that are missing but crucial for\n9 understanding the process flow.\n10 - **Clarity Improvement:** Clarify any ambiguous or vague statements to ensure that\n11 the description is clear and understandable.\n12 - **Explicit Process Constructs:** Rephra se parts of the description to explicitly\n13 incorporate constructs. For example , change ‘X happens in most cases ’ to ‘ there\n14 is an exclusive choice between performing X or skipping it’.\n\u0006 \u0005\nTable 8 Comparison of average quality scores before and after LLM self-improvement of process descriptions\nDescription Length Avg. Quality Before\nSelf-Improvement\nAvg. Quality After\nSelf-Improvement\nCases With Increased\nQuality\nLong (Original) 0.87 0.79 6/20\nMedium-Length (50–80%) 0.75 0.82 11/20\nShort (15–35%) 0.78 0.72 8/20\nFig. 6 Process models fragments for the inventory replenishment process (p3)\nWhile the original text simply implies an exclusive choice\nbetween an automated alert or a manual check, the LLM\nsigniﬁcantly elaborated, describing potentially parallel activ-\nities. This self-introduced complexity subsequently confused\nthe LLM during the process model generation phase. Figure 6\nshows fragments from the ground truth model for this pro-\ncess alongside the models generated by Gemini-1.5-Pro-002\nwith and without self-optimization. With the self-optimized\ninput, the LLM generated a ﬂawed model where the activ-\nities related to monitoring the inventory levels and sending\nthe manual alert were duplicated and placed to run in parallel\nwith the rest of the activities. This example highlights the risk\nof the input self-optimization strategy within our framework.\nSeemingly plausible enrichments introduced by the LLM can\nincrease complexity to the point where the LLM itself fails\nto model the described process accurately, thereby harming\noverall model quality.\nIn conclusion, our ﬁndings suggest that relying on\nLLMs to autonomously enhance process descriptions with-\nout domain-speciﬁc guidance or constraints does not effec-\ntively improve the resultant process models.\n123\nH. Kourani et al.\n7.3 LLM self-optimization of output\nIn this section, we examine the potential of LLMs to enhance\nthe quality of their outputs by self-optimizing the gener-\nated process models. The underlying hypothesis is that by\nenabling LLMs to critically evaluate and reﬁne their own\noutputs, they may identify and correct ﬂaws, leading to\nhigher-quality process models.\n7.3.1 Implementation and experimental setup\nTo investigate this approach, we extended our previous exper-\niments by instructing the LLMs to perform self-optimization\non their initial outputs. Speciﬁcally, after generating the ini-\ntial process model from the process description, we prompted\nthe LLM to critically evaluate the model against the initial\ndescription and improve it accordingly. The prompt used is\nshown in Listing 10. To prevent unnecessary alterations that\nmight degrade the model’s quality, we crafted the prompt\nwith intentional restrictiveness, emphasizing that the LLM\nshould only make genuinely beneﬁcial changes and encour-\naging it to retain the same model if no areas for improvement\nare identiﬁed. This approach addresses the tendency of LLMs\nto respond afﬁrmatively to requests, even when they may lack\nthe knowledge or capability to perform the task, potentially\nleading to unintended hallucinations.\nWe conducted experiments using three different LLMs:\nGemini-1.5-Pro-002, Gemini-1.5-Flash-002, and GPT-4o.\nThe selection of these models was motivated by the desire\nto assess whether self-optimization could yield more sig-\nniﬁcant improvements in less-performing models (Gemini-\n1.5-Flash-002 and GPT-4o) compared to a high-performing\nmodel (Gemini-1.5-Pro-002).\nFor each process in our dataset, we generated the initial\nprocess model using each LLM and then applied the self-\noptimization prompt. We then evaluated the quality of the\ninitial and improved models. For this experiment, we dis-\nabled the error reﬁnement loop. Instead, in cases where errors\noccurred, we repeated the same self-optimization prompt\nmultiple times until an error-free response was achieved. By\nbypassing error-handling loops, we aimed to avoid distract-\ning the LLM with error correction, which might steer it away\nfrom the primary goal of enhancing the already successfully\ngenerated model.\n7.3.2 Results and discussion\nThe results of the experiment are summarized in Table 9.\nThis table includes, for each LLM, the average quality scores\nbefore and after LLM self-optimization of output, as well as\nthe maximum improvement and maximum decline observed.\nFurthermore, the table reports the average number of attempts\nrequired for the LLM to produce an error-free response to the\nself-optimization prompt.\nThe results highlight that while the average improvements\nmay appear modest, self-optimization of output can yield\nsigniﬁcant beneﬁts in speciﬁc instances. GPT-4o showed the\nmost substantial beneﬁt, with the highest average quality gain\n(+0.05) and an instance of very large improvement ( +0.84).\nGemini-1.5-Flash-002 also saw signiﬁcant enhancements,\nachieving gains up to +0.29. For Gemini-1.5-Pro-002, the\nimpact of self-optimization was relatively low, with a slight\naverage increase ( +0.005), reﬂecting the high quality of\nits initial outputs. We also note that in some cases, self-\noptimization of output led to declines in quality. However,\nthe maximum declines were relatively smaller compared to\nthe maximum improvements, suggesting that while there is a\nrisk of degradation, the potential for signiﬁcant enhancement\nis greater.\nThe average number of attempts required, reported in\nTable 9, offers insight into the feasibility of this self-\noptimization step. All tested LLMs required relatively few\nattempts (between 1.1 and 1.4 on average) to produce an\nerror-free response to the optimization prompt. These low\nnumbers suggest that the process of modifying an initially\nvalid output was generally efﬁcient; the LLMs typically suc-\nceeded within one or two tries without frequently introducing\nnew errors.\nIn conclusion, our analysis indicates that allowing LLMs\nto self-optimize their outputs can, in general, be beneﬁcial\nwithin our framework, especially for models that initially\nproduce lower-quality outputs. However, there is a risk of\nquality degradation and it is crucial to design the prompt\ncarefully to discourage hallucinations.\n8 Discussion and future work\nIn this section, we discuss the practical and research implica-\ntions of our work, outline the current limitations and threats\nto validity, and suggest directions for future research.\n8.1 Implications for practice and research\nOur ﬁndings suggest several potential implications for both\nthe practical application of BPM and future research direc-\ntions. For practitioners, our framework points toward meth-\nods that could potentially reduce the time and effort involved\nin initial process modeling. By leveraging LLMs to generate\ndraft process models from textual descriptions like procedu-\nral documents or interview notes, analysts might shift some\nfocus from manual modeling toward validation, reﬁnement,\nand analysis. This approach could also contribute to making\nprocess modeling more accessible; a natural language inter-\nface may lower the barrier for stakeholders less familiar with\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nListing 10 Prompt for output optimization.\u0007 \u0004\n1 Could you further improve the model? Please critically evaluate the process model\n2 against the initial process description and improve it accordingly ** only where\n3 genuinely beneficial**. If you see no significant areas for enhancement , it is\n4 perfectly acceptable to return the same model without any changes.\n\u0006 \u0005\nTable 9 Impact of LLM self-optimization of output on model quality\nLLM Avg. Num. Attempts Avg. Quality\nBefore Self-\nImprovement\nAvg. Quality\nAfter Self-\nImprovement\nMax. Improvement Max. Decline\nGemini-1.5-Pro-002 1.2 0.87 0.87 +0.14 −0.04\nGemini-1.5-Flash-002 1.4 0.73 0.76 +0.29 −0.08\nGPT-4o 1.1 0.76 0.81 +0.84 −0.03\nformal notations like BPMN, encouraging broader participa-\ntion in process documentation efforts. The availability of our\nopen-source tool, ProMoAI, provides a concrete implemen-\ntation of these ideas, offering the community a platform to\nexplore and build upon this LLM-driven approach. There is\nalso potential for integrating such LLM functionalities into\nexisting BPM platforms to assist users.\nFrom a research standpoint, our work provides a speciﬁc\ncontext for examining how LLMs handle tasks requiring\nstructured output generation, adherence to formal con-\nstraints, and logical reasoning from procedural text. Our\nstudy contributes to the broader challenge of automatically\ntranslating unstructured language into formal models, high-\nlighting areas where current LLMs succeed and struggle. Our\nevaluation methodology, employing conformance checking,\noffers a reusable approach for benchmarking future LLMs\non similar tasks. Finally, our initial exploration of self-\nimprovement strategies suggests that this is a complex area\nrequiring more research to understand the limits of LLM\nself-correction, develop effective prompting techniques, and\ncompare autonomous reﬁnement with human-guided or\nknowledge-augmented approaches.\n8.2 Framework limitations and future directions\nOur framework, while pioneering in leveraging LLMs for\nprocess modeling, has limitations. In this section, we outline\nareas for improvement and propose ideas for addressing them\nin future work.\nExpanding Process Perspectives:Our framework addresses\nthe control-ﬂow perspective of process modeling, omit-\nting the data, resource, and operational perspectives, which\nare crucial for a comprehensive understanding of business\nprocesses. The inherent ﬂexibility and understanding capa-\nbilities of LLMs present a signiﬁcant potential for extending\nour framework to incorporate additional process perspec-\ntives.\nDirect BPMN Generation: The current implementation\nof our framework utilizes POWL for intermediate process\nrepresentation. A possible direction for future research is\nthe exploration of the direct generation of BPMN mod-\nels without an intermediate process representation. This\napproach promises to offer greater ﬂexibility in represent-\ning intricate process structures and dynamics and allows for\nthe enrichment of process models with context-rich annota-\ntions. However, moving away from the structured guarantees\nprovided by POWL necessitates the development of more\nadvanced process model generation and validation tech-\nniques.\nEnhanced Interactivity: We intend to enhance the model\nreﬁnement loop to support more nuanced and interactive\nfeedback mechanisms. For example, we aim to empower\nusers to not only provide textual feedback on generated pro-\ncess models but also to manually edit the generated models.\n8.3 Evaluation limitations and threats to validity\nOur experimental evaluation is subject to several limitations\nand potential threats to validity that should be considered\nwhen interpreting the results.\nManual Design of Processes: Process descriptions and\nground truth models were co-designed, constraining them to\nPOWL-expressible structures. While this approach enables\na realistic assessment of the framework’s capabilities by\navoiding elements it cannot capture, it may not reﬂect the\nfull complexity or ambiguity encountered in unconstrained\nreal-world processes. Future studies should consider incor-\nporating real-world datasets to evaluate performance on more\nvaried inputs.\nConformance Checking Assessment: Our evaluation relies\nprimarily on conformance checking metrics and simulation\nto assess model quality. While this approach offers an objec-\ntive measure of performance, it does not fully account for\nthe practical usability or interpretability of the models from\n123\nH. Kourani et al.\na human expert’s perspective. This reliance may limit the\ngeneralizability of our ﬁndings to scenarios where qualita-\ntive human feedback is essential.\nEffect of LLM Design on Model Quality: While our eval-\nuation clearly shows differences in model quality across\nLLMs, pinpointing the exact causes is challenging as the\nLLMs are treated as black boxes. Although architectural vari-\nations (e.g., transformer-based vs. mixture-of-experts) and\ndesign goals (such as task-speciﬁc training) may contribute\nto the observed performance differences, our study does not\ndirectly isolate these factors. Instead, our focus is on provid-\ning a robust, empirical benchmark of overall performance\nwithin our framework. Further research could systematically\ninvestigate how underlying design choices impact quality in\nBPM tasks.\nStandardizing Activity Labels: Another limitation is the\nuse of predeﬁned activity labels, provided to the LLM to\nenable automated conformance checking. This simpliﬁes the\ntask, as the LLM does not need to identify and consistently\nname activities from the raw text. Although our supple-\nmentary analysis in Sect. 6.4.1 suggests that the effect of\nstandardizing activity labels is limited, it remains a factor\nthat could affect the validity of our results, particularly con-\ncerning the activity identiﬁcation sub-task.\nGeneralizability of Findings on LLM Self-Improvement:\nA key threat to the validity of our LLM self-improvement\nexperiments (Sect. 7) arises because they were conducted\nusing a small number of LLMs. While our results provide ini-\ntial insights (e.g., suggesting that more capable models might\nbeneﬁt more from self-evaluation, or that output optimization\nholds potential), these ﬁndings cannot be reliably general-\nized to the broader LLM landscape. V alidation across a more\ndiverse set of LLMs, exhibiting varying sizes and architec-\ntures, is necessary. Future work should undertake a more\nextensive investigation involving additional state-of-the-art\nmodels to ascertain the broader applicability and robustness\nof these self-improvement techniques.\nAbsence of Explicit Improvement Parameters for Out-\nput Optimization: A limitation of our current output self-\noptimization experiment (Sect. 7.3) is the intentionally\nopen-ended nature of the prompt, which instructs the LLM\nto improve the model without specifying explicit parameters,\nperformance measures, or dimensions for enhancement. The\nrationale was to assess the LLM’s ability to autonomously\nidentify improvements based on the process description,\nmimicking user requests for general enhancement without\npre-diagnosed ﬂaws. While this approach tests a valuable\ncapability, future research could explore the effectiveness of\nmore structured self-optimization prompts that provide spe-\nciﬁc criteria. For example, these prompts could ask the LLM\nto verify adherence to certain modeling principles, check for\nspeciﬁc patterns mentioned in the description, or even opti-\nmize toward metrics like simplicity or alignment with known\nbest practices.\n9 Conclusion\nIn this paper, we extended our LLM-driven process model-\ning framework by designing a reusable evaluation approach\nfor benchmarking LLMs on process modeling and explor-\ning LLM self-improvement strategies. Our assessment of\n16 state-of-the-art LLMs revealed substantial performance\nvariations, with Claude-3.5-Sonnet demonstrating excep-\ntional capabilities in generating high-quality process models\nefﬁciently. We found a positive correlation between error-\nhandling performance and the overall quality of the generated\nmodels. Additionally, our analysis indicated consistent qual-\nity trends within similar model families.\nThe investigation of LLM self-improvement strategies\nrevealed that while self-evaluation depends heavily on the\nchosen LLM and input optimization shows limited reliabil-\nity, output optimization demonstrates promising potential for\nenhancing quality. This underscores the possibility of lever-\naging LLMs to autonomously reﬁne their outputs, potentially\nreducing the need for manual intervention. However, care-\nfully crafted prompts are crucial to discourage hallucinations.\nOur work contributes valuable insights into the application\nof LLMs for automated process modeling. The benchmark\nprovides a foundation for comparing LLM performance in\nthis domain, while the self-improvement analysis identiﬁes\npromising avenues for further enhancing LLM-generated\nprocess models. Future research directions include incorpo-\nrating additional process perspectives beyond control-ﬂow,\nexploring direct BPMN generation without intermediate rep-\nresentations, investigating alternative prompting strategies,\nand exploring the integration of external knowledge sources\nto further enhance the accuracy and reliability of LLM-\ngenerated process models.\nFunding Open Access funding enabled and organized by Projekt\nDEAL.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\nReferences\n1. Rosing, M., White, S., Cummins, F., Man, H.: Business process\nmodel and notation—BPMN. In: The Complete Business Process\nHandbook: Body of Knowledge from Process Modeling to BPM,\nV olume I, pp. 429–453 (2015). https://doi.org/10.1016/B978-0-\n12-799959-3.00021-5\n2. Hee, K.M., Sidorova, N., Werf, J.M.E.M.: Business process mod-\neling using petri nets. Trans. Petri Nets Other Model. Concurr. 7,\n116–161 (2013). https://doi.org/10.1007/978-3-642-38143-0_4\n3. OpenAI: GPT-4 technical report (2023). CoRR arXiv:2303.08774.\nhttps://doi.org/10.48550/ARXIV .2303.08774\n4. Anil, R., et al.: Gemini: a family of highly capable multimodal mod-\nels (2023). CoRR arXiv:2312.11805. https://doi.org/10.48550/\nARXIV .2312.11805\n5. Li, J., Tang, T., Zhao, W.X., Wen, J.: Pretrained language model\nfor text generation: a survey. In: Proceedings of the Thirtieth Inter-\nnational Joint Conference on Artiﬁcial Intelligence, IJCAI 2021,\nVirtual Event / Montreal, Canada, 19–27 August 2021, pp. 4492–\n4499 (2021). https://doi.org/10.24963/IJCAI.2021/612\n6. Vidan, A., Fiedler, L.H.: A composable just-in-time program-\nming framework with LLMs and FBP . In: IEEE High Performance\nExtreme Computing Conference, HPEC 2023, Boston, MA, USA,\nSeptember 25–29, 2023, pp. 1–8 (2023). https://doi.org/10.1109/\nHPEC58863.2023.10363587\n7. Zhou, Y ., Muresanu, A.I., Han, Z., Paster, K., Pitis, S., Chan, H.,\nBa, J.: Large language models are human-level prompt engineers.\nIn: The Eleventh International Conference on Learning Represen-\ntations, ICLR 2023, Kigali, Rwanda, May 1–5, 2023 (2023)\n8. Kourani, H., Berti, A., Schuster, D., Aalst, W.M.P .: Process mod-\neling with large language models. In: Enterprise, Business-Process\nand Information Systems Modeling—25th International Confer-\nence, BPMDS 2024, and 29th International Conference, EMMSAD\n2024, Limassol, Cyprus, June 3-4, 2024, Proceedings, pp. 229–244\n(2024). https://doi.org/10.1007/978-3-031-61007-3_18\n9. Kourani, H., Zelst, S.J.: POWL: partially ordered workﬂow lan-\nguage. In: Business Process Management—21st International\nConference, BPM 2023, Utrecht, The Netherlands, September 11–\n15, 2023, Proceedings, pp. 92–108 (2023). https://doi.org/10.1007/\n978-3-031-41620-0_6\n10. Carmona, J., Dongen, B.F., Weidlich, M.: Conformance check-\ning: foundations, milestones and challenges. In: Process Mining\nHandbook, pp. 155–190 (2022). https://doi.org/10.1007/978-3-\n031-08848-3_5\n11. Madaan, A., Tandon, N., Gupta, P ., Hallinan, S., Gao, L., Wiegr-\neffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y ., Gupta,\nS., Majumder, B.P ., Hermann, K., Welleck, S., Yazdanbakhsh, A.,\nClark, P .: Self-reﬁne: iterative reﬁnement with self-feedback. In:\nAdvances in Neural Information Processing Systems 36: Annual\nConference on Neural Information Processing Systems 2023,\nNeurIPS 2023, New Orleans, LA, USA, December 10–16, 2023\n(2023)\n12. Dumas, M., Rosa, M.L., Mendling, J., Reijers, H.A.: Fundamentals\nof Business Process Management, 2nd edn (2018). https://doi.org/\n10.1007/978-3-662-56509-4\n13. Forster, S., Pinggera, J., Weber, B.: Toward an understanding of\nthe collaborative process of process modeling. In: Proceedings\nof the CAiSE’13 Forum at the 25th International Conference on\nAdvanced Information Systems Engineering (CAiSE), V alencia,\nSpain, June 20th, 2013, pp. 98–105 (2013)\n14. Sintoris, K., V ergidis, K.: Extracting business process models using\nnatural language processing (NLP) techniques. In: 19th IEEE Con-\nference on Business Informatics, CBI 2017, Thessaloniki, Greece,\nJuly 24–27, 2017, V olume 1: Conference Papers, pp. 135–139\n(2017). https://doi.org/10.1109/CBI.2017.41\n15. Qian, C., Wen, L., Kumar, A., Lin, L., Lin, L., Zong, Z., Li,\nS., Wang, J.: An approach for process model extraction by\nmulti-grained text classiﬁcation. In: Advanced Information Sys-\ntems Engineering—32nd International Conference, CAiSE 2020,\nGrenoble, France, June 8–12, 2020, Proceedings, pp. 268–282\n(2020). https://doi.org/10.1007/978-3-030-49435-3_17\n16. Aa, H., Carmona, J., Leopold, H., Mendling, J., Padró, L.: Chal-\nlenges and opportunities of applying natural language processing in\nbusiness process management. In: Proceedings of the 27th Interna-\ntional Conference on Computational Linguistics, COLING 2018,\nSanta Fe, New Mexico, USA, August 20–26, 2018, pp. 2791–2801\n(2018)\n17. Salimifard, K., Wright, M.: Petri net-based modelling of workﬂow\nsystems: An overview. Eur. J. Oper. Res. 134(3), 664–676 (2001).\nhttps://doi.org/10.1016/S0377-2217(00)00292-7\n18. Unruh, E., Delfmann, P ., Thimm, M.: Quantitative deadlock anal-\nysis in petri nets using inconsistency measures. In: 23rd IEEE\nConference on Business Informatics, CBI 2021, Bolzano, Italy,\nSeptember 1–3, 2021. V olume 1, pp. 42–51 (2021). https://doi.\norg/10.1109/CBI52690.2021.00015\n19. Menezes, P .B., Costa, J.F.: Synchronization in petri nets. Fun-\ndam. Informaticae 26(1), 11–22 (1996). https://doi.org/10.3233/\nFI-1996-2612\n20. Datta, A.K., Ghosh, S.: High-level petri-net model for a resource-\nsharing problem. Inf. Sci. 51(2), 213–220 (1990). https://doi.org/\n10.1016/0020-0255(90)90027-8\n21. Leemans, S.J.J.: Robust Process Mining with Guarantees—Process\nDiscovery, Conformance Checking and Enhancement. Lecture\nNotes in Business Information Processing, vol. 440 (2022). https://\ndoi.org/10.1007/978-3-030-96655-3\n22. Kourani, H., Schuster, D., Aalst, W.M.P .: Scalable discovery of\npartially ordered workﬂow models with formal guarantees. In: 5th\nInternational Conference on Process Mining, ICPM 2023, Rome,\nItaly, October 23–27, 2023, pp. 89–96 (2023). https://doi.org/10.\n1109/ICPM60904.2023.10271941\n23. Kourani, H., Zelst, S.J., Schuster, D., Aalst, W.M.P .: Discovering\npartially ordered workﬂow models. Inf. Syst. 128, 102493 (2025).\nhttps://doi.org/10.1016/J.IS.2024.102493\n24. V aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\nGomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you\nneed. In: Advances in Neural Information Processing Systems 30:\nAnnual Conference on Neural Information Processing Systems\n2017, December 4–9, 2017, Long Beach, CA, USA, pp. 5998–\n6008 (2017)\n25. Brown, T.B., et al.: Language models are few-shot learners. In:\nAdvances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6–12, 2020, Virtual (2020)\n26. Wil M. P . van der Aalst et al.: Process mining manifesto. In: Busi-\nness Process Management Workshops—BPM 2011 International\nWorkshops, Clermont-Ferrand, France, August 29, 2011, Revised\nSelected Papers, Part I, pp. 169–194 (2011). https://doi.org/10.\n1007/978-3-642-28108-2_19\n27. Freitas, A.P ., Pereira, J.L.M.: Process simulation support in BPM\ntools: the case of BPMN (2015)\n28. Aalst, W.M.P .: Business process simulation revisited. In: Enterprise\nand Organizational Modeling and Simulation—6th International\nWorkshop, EOMAS 2010, Held at CAiSE 2010, Hammamet,\nTunisia, June 7–8, 2010. Selected Papers, pp. 1–14 (2010). https://\ndoi.org/10.1007/978-3-642-15723-3_1\n29. Dunzer, S., Stierle, M., Matzner, M., Baier, S.: Conformance check-\ning: a state-of-the-art literature review. In: Proceedings of the 11th\nInternational Conference on Subject-Oriented Business Process\nManagement, S-BPM ONE 2019, Seville, Spain, June 26-28, 2019,\npp. 4–1410 (2019). https://doi.org/10.1145/3329007.3329014\n123\nH. Kourani et al.\n30. Woensel, W.V ., Motie, S.: NLP4PBM: a systematic review on pro-\ncess extraction using natural language processing with rule-based,\nmachine and deep learning methods. Enterp. Inf. Syst. 18(11)\n(2024). https://doi.org/10.1080/17517575.2024.2417404\n31. Gonçalves, A.R., Santoro, J.C., Baião, F.M.F.A.: Let me tell you\na story—on how to build process models. J. Univers. Comput.\nSci. 17(2), 276–295 (2011). https://doi.org/10.3217/JUCS-017-\n02-0276\n32. Friedrich, F., Mendling, J., Puhlmann, F.: Process model genera-\ntion from natural language text. In: Advanced Information Systems\nEngineering—23rd International Conference, CAiSE 2011, Lon-\ndon, UK, June 20–24, 2011. Proceedings, pp. 482–496 (2011).\nhttps://doi.org/10.1007/978-3-642-21640-4_36\n33. Sholiq, S., Sarno, R., Astuti, E.S.: Generating BPMN diagram from\ntextual requirements. J. King Saud Univ. Comput. Inf. Sci. 34(10\nPart B), 10079–10093 (2022). https://doi.org/10.1016/J.JKSUCI.\n2022.10.007\n34. Ivanchikj, A., Serbout, S., Pautasso, C.: From text to visual\nBPMN process models: design and evaluation. In: MoDELS\n’20: ACM/IEEE 23rd International Conference on Model Driven\nEngineering Languages and Systems, Virtual Event, Canada, 18-\n23 October, 2020, pp. 229–239 (2020). https://doi.org/10.1145/\n3365438.3410990\n35. Aalst, W.M.P .: Process Mining—Discovery, Conformance and\nEnhancement of Business Processes (2011). https://doi.org/10.\n1007/978-3-642-19345-3\n36. Busch, K., Rochlitzer, A., Sola, D., Leopold, H.: Just tell me:\nPrompt engineering in business process management. In: Enter-\nprise, Business-Process and Information Systems Modeling - 24th\nInternational Conference, BPMDS 2023, and 28th International\nConference, EMMSAD 2023, Zaragoza, Spain, June 12-13, 2023,\nProceedings, pp. 3–11 (2023). https://doi.org/10.1007/978-3-031-\n34241-7_1\n37. Vidgof, M., Bachhofner, S., Mendling, J.: Large language models\nfor business process management: Opportunities and challenges.\nIn: Business Process Management Forum - BPM 2023 Forum,\nUtrecht, The Netherlands, September 11–15, 2023, Proceedings,\npp. 107–123 (2023). https://doi.org/10.1007/978-3-031-41623-\n1_7\n38. Norouzifar, A., Kourani, H., Dees, M., Aalst, W.M.P .: Bridging\ndomain knowledge and process discovery using large language\nmodels. In: Business Process Management Workshops—BPM\n2024 International Workshops, Krakow, Poland, September 1–6,\n2024, Revised Selected Papers, pp. 44–56 (2024). https://doi.org/\n10.1007/978-3-031-78666-2_4\n3 9 . K o u r a n i ,H . ,B e r t i ,A . ,H e n r i c h ,J . ,K r a t s c h ,W . ,W e i d l i c h ,R . ,L i ,C . ,\nArslan, A., Schuster, D., Aalst, W.M.P .: Leveraging large language\nmodels for enhanced process model comprehension (2024). CoRR\narXiv:2408.08892. https://doi.org/10.48550/ARXIV .2408.08892\n40. Nicola, A.D., Formica, A., Mele, I., Missikoff, M., Taglino, F.: A\ncomparative study of LLMs and NLP approaches for supporting\nbusiness process analysis. Enterp. Inf. Syst. 18(10) (2024). https://\ndoi.org/10.1080/17517575.2024.2415578\n41. Bellan, P ., Dragoni, M., Ghidini, C.: Process knowledge extrac-\ntion and knowledge graph construction through prompting: A\nquantitative analysis. In: Proceedings of the 39th ACM/SIGAPP\nSymposium on Applied Computing, SAC 2024, Avila, Spain,\nApril 8–12, 2024, pp. 1634–1641 (2024). https://doi.org/10.1145/\n3605098.3635957\n42. Chen, S., Liao, H.: Bert-log: anomaly detection for system logs\nbased on pre-trained language model. Appl. Artif. Intell. 36(1)\n(2022). https://doi.org/10.1080/08839514.2022.2145642\n43. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training\nof deep bidirectional transformers for language understanding.\nIn: Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2–7, 2019, V olume 1 (Long and Short Papers), pp.\n4171–4186 (2019). https://doi.org/10.18653/V1/N19-1423\n44. Fill, H., Fettke, P ., Köpke, J.: Conceptual modeling and large lan-\nguage models: Impressions from ﬁrst experiments with ChatGPT.\nEnterp. Model. Inf. Syst. Archit. Int. J. Concept. Model. 18,3\n(2023). https://doi.org/10.18417/EMISA.18.3\n45. Muff, F., Fill, H.: Limitations of ChatGPT in conceptual modeling:\nInsights from experiments in metamodeling. In: Modellierung 2024\n- Workshop Proceedings, Potsdam, Germany, March 12–15, 2024,\np. 8 (2024). https://doi.org/10.18420/MODELLIERUNG2024-\nWS-008\n46. Busch, K., Leopold, H.: Towards a benchmark for large language\nmodels for business process management tasks (2024). CoRR\narXiv:2410.03255. https://doi.org/10.48550/ARXIV .2410.03255\n47. Klievtsova, N., Benzin, J., Kampik, T., Mangler, J., Rinderle-Ma,\nS.: Conversational process modelling: State of the art, applications,\nand implications in practice. In: Business Process Management\nForum—BPM 2023 Forum, Utrecht, The Netherlands, September\n11–15, 2023, Proceedings, pp. 319–336 (2023). https://doi.org/10.\n1007/978-3-031-41623-1_19\n48. Ziche, C., Apruzzese, G.: LLM4PM: A case study on using large\nlanguage models for process modeling in enterprise organizations.\nIn: Business Process Management: Blockchain, Robotic Process\nAutomation, Central and Eastern European, Educators and Industry\nForum—BPM 2024 Blockchain, RPA, CEE, Educators and Indus-\ntry Forum, Krakow, Poland, September 1–6, 2024, Proceedings, pp.\n472–483 (2024). https://doi.org/10.1007/978-3-031-70445-1_35\n49. Fontenla-Seco, Y ., Winkler, S., Gianola, A., Montali, M., Penín,\nM.L., Diz, A.J.B.: The droid you’re looking for: C-4PM, a conver-\nsational agent for declarative process mining. In: Proceedings of\nthe Best Dissertation Award, Doctoral Consortium, and Demon-\nstration & Resources Forum at BPM 2023 Co-located with 21st\nInternational Conference on Business Process Management (BPM\n2023), Utrecht, The Netherlands, September 11th to 15th, 2023,\npp. 112–116 (2023)\n50. Grohs, M., Abb, L., Elsayed, N., Rehse, J.: Large language models\ncan accomplish business process management tasks. In: Business\nProcess Management Workshops - BPM 2023 International Work-\nshops, Utrecht, The Netherlands, September 11–15, 2023, Revised\nSelected Papers, pp. 453–465 (2023). https://doi.org/10.1007/978-\n3-031-50974-2_34\n51. Wenger, S., Spahic-Bogdanovic, M., Martin, A.: Large language\nmodels for democratizing business process modeling: BPMN\nmodel generation and style guide adherence. In: Southern African\nConference for Artiﬁcial Intelligence Research, pp. 372–389.\nSpringer, Berlin (2024)\n52. Eldin, A.N., Assy, N., Anesini, O., Dalmas, B., Gaaloul, W.:\nNala2BPMN: Automating BPMN model generation with large\nlanguage models. In: Cooperative Information Systems—30th\nInternational Conference, CoopIS 2024, Porto, Portugal, Novem-\nber 19–21, 2024, Proceedings, pp. 398–404 (2024). https://doi.org/\n10.1007/978-3-031-81375-7_27\n53. Ayad, S., Alsayoud, F.: Prompt engineering techniques for seman-\ntic enhancement in business process models. Bus. Process. Manag.\nJ. 30(7), 2611–2641 (2024). https://doi.org/10.1108/BPMJ-02-\n2024-0108\n54. Ji, Z., Lee, N., Frieske, R., Y u, T., Su, D., Xu, Y ., Ishii, E.,\nBang, Y ., Madotto, A., Fung, P .: Survey of hallucination in natu-\nral language generation. ACM Comput. Surv. 55(12), 248–124838\n(2023). https://doi.org/10.1145/3571730\n55. Pan, L., Saxon, M., Xu, W., Nathani, D., Wang, X., Wang, W.Y .:\nAutomatically correcting large language models: surveying the\nlandscape of diverse automated correction strategies. Trans. Assoc.\nComput. Linguist. 12, 484–506 (2024). https://doi.org/10.1162/\nTACL_A_00660\n123\nEvaluating large language models on business process modeling: framework, benchmark, and…\n56. Huang, J., Chen, X., Mishra, S., Zheng, H.S., Y u, A.W., Song, X.,\nZhou, D.: Large language models cannot self-correct reasoning yet.\nIn: The Twelfth International Conference on Learning Represen-\ntations, ICLR 2024, Vienna, Austria, May 7–11, 2024 (2024)\n57. Song, Y ., Zhang, H., Eisenach, C., Kakade, S.M., Foster, D.P ., Ghai,\nU.: Mind the gap: Examining the self-improvement capabilities of\nlarge language models (2024). CoRR arXiv:2412.02674. https://\ndoi.org/10.48550/ARXIV .2412.02674\n58. Y untao Bai et al.: Constitutional AI: harmlessness from AI feed-\nback (2022). CoRR arXiv:2212.08073. https://doi.org/10.48550/\nARXIV .2212.08073\n59. Lianmin Zheng et al.: Judging LLM-as-a-judge with MT-Bench\nand chatbot arena. In: Advances in Neural Information Processing\nSystems 36: Annual Conference on Neural Information Processing\nSystems 2023, NeurIPS 2023, New Orleans, LA, USA, December\n10–16, 2023 (2023)\n60. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., Yao, S.:\nReﬂexion: language agents with verbal reinforcement learning. In:\nAdvances in Neural Information Processing Systems 36: Annual\nConference on Neural Information Processing Systems 2023,\nNeurIPS 2023, New Orleans, LA, USA, December 10–16, 2023\n(2023)\n61. Huang, A., Block, A., Foster, D.J., Rohatgi, D., Zhang, C., Sim-\nchowitz, M., Ash, J.T., Krishnamurthy, A.: Self-improvement\nin language models: the sharpening mechanism (2024). CoRR\narXiv:2412.01951. https://doi.org/10.48550/ARXIV .2412.01951\n62. Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B.,\nChild, R., Gray, S., Radford, A., Wu, J., Amodei, D.: Scaling laws\nfor neural language models (2020). CoRR arXiv:2001.08361\n63. Hoffmann, J., et al.: Training compute-optimal large language mod-\nels (2022). CoRR arXiv:2203.15556. https://doi.org/10.48550/\nARXIV .2203.15556\n64. Lopes, I.F., Ferreira, D.R.: A survey of process mining com-\npetitions: The BPI challenges 2011–2018. In: Business Process\nManagement Workshops—BPM 2019 International Workshops,\nVienna, Austria, September 1–6, 2019, Revised Selected Papers,\npp. 263–274 (2019). https://doi.org/10.1007/978-3-030-37453-\n2_22\n65. Berti, A., Kourani, H., Aalst, W.M.P .: PM-LLM-Benchmark: Eval-\nuating large language models on process mining tasks (2024).\nCoRR arXiv:2407.13244. https://doi.org/10.48550/ARXIV .2407.\n13244\n66. Fournier, F., Limonad, L., Skarbovsky, I.: Towards a benchmark\nfor causal business process reasoning with LLMs. In: Business\nProcess Management Workshops—BPM 2024 International Work-\nshops, Krakow, Poland, September 1–6, 2024, Revised Selected\nPapers, pp. 233–246 (2024). https://doi.org/10.1007/978-3-031-\n78666-2_18\n67. Fahland, D., Fournier, F., Limonad, L., Skarbovsky, I., Swevels,\nA.J.E.: How well can large language models explain business\nprocesses? (2024). CoRR arXiv:2401.12846. https://doi.org/10.\n48550/ARXIV .2401.12846\n68. Rebmann, A., Schmidt, F.D., Glavas, G., Aa, H.: Evaluating the\nability of LLMs to solve semantics-aware process mining tasks.\nIn: 6th International Conference on Process Mining, ICPM 2024,\nKgs. Lyngby, Denmark, October 14–18, 2024, pp. 9–16 (2024).\nhttps://doi.org/10.1109/ICPM63005.2024.10680677\n69. Xu, B., Yang, A., Lin, J., Wang, Q., Zhou, C., Zhang, Y ., Mao, Z.:\nExpertprompting: Instructing large language models to be distin-\nguished experts (2023). CoRR arXiv:2305.14688. https://doi.org/\n10.48550/ARXIV .2305.14688\n70. Martino, A., Iannelli, M., Truong, C.: Knowledge injection to\ncounter large language model (LLM) hallucination. In: The\nSemantic Web: ESWC 2023 Satellite Events—Hersonissos, Crete,\nGreece, May 28–June 1, 2023, Proceedings, pp. 182–185 (2023).\nhttps://doi.org/10.1007/978-3-031-43458-7_34\n71. Bellan, P ., Aa, H., Dragoni, M., Ghidini, C., Ponzetto, S.P .: PET:\nan annotated dataset for process extraction from natural language\ntext tasks. In: Business Process Management Workshops—BPM\n2022 International Workshops, Münster, Germany, September 11–\n16, 2022, Revised Selected Papers, pp. 315–321 (2022). https://\ndoi.org/10.1007/978-3-031-25383-6_23\n72. Miyake, D., Iohara, A., Saito, Y ., Tanaka, T.: Negative-prompt\ninversion: fast image inversion for editing with text-guided diffu-\nsion models (2023). CoRR arXiv:2305.16807. https://doi.org/10.\n48550/ARXIV .2305.16807\n73. Kourani, H., Berti, A., Schuster, D., Aalst, W.M.P .: ProMoAI:\nProcess modeling with generative AI. In: Proceedings of the Thirty-\nThird International Joint Conference on Artiﬁcial Intelligence,\nIJCAI 2024, Jeju, South Korea, August 3–9, 2024, pp. 8708–8712\n(2024)\n74. Kourani, H., Zelst, S.J., Lehmann, B., Einsdorf, G., Helfrich, S.,\nLiße, F.: PM4KNIME: process mining meets the KNIME analytics\nplatform (extended abstract). In: Proceedings of the ICPM Doctoral\nConsortium and Demo Track 2022 Co-located with 4th Interna-\ntional Conference on Process Mining (ICPM 2022), Bolzano, Italy,\nOctober, 2022, pp. 65–69 (2022)\n75. Kourani, H., Park, G., Aalst, W.M.P .: Translating workﬂow nets\ninto the partially ordered workﬂow language. arXiv preprint\narXiv:2503.20363 (2025)\n76. Berti, A., Zelst, S.J., Schuster, D.: PM4Py: A process mining library\nfor python. Softw. Impacts 17, 100556 (2023). https://doi.org/10.\n1016/J.SIMPA.2023.100556\n77. Berti, A., Aalst, W.M.P .: Reviving token-based replay: Increas-\ning speed while improving diagnostics. In: Proceedings of the\nInternational Workshop on Algorithms & Theories for the Anal-\nysis of Event Data 2019 Satellite Event of the Conferences: 40th\nInternational Conference on Application and Theory of Petri Nets\nand Concurrency Petri Nets 2019 and 19th International Confer-\nence on Application of Concurrency to System Design ACSD\n2019, A TAED@Petri Nets/ACSD 2019, Aachen, Germany, June\n25, 2019, pp. 87–103 (2019)\n78. Munoz-Gama, J., Carmona, J.: A fresh look at precision in process\nconformance. In: Business Process Management—8th Interna-\ntional Conference, BPM 2010, Hoboken, NJ, USA, September\n13–16, 2010. Proceedings, pp. 211–226 (2010). https://doi.org/10.\n1007/978-3-642-15618-2_16\n79. Zhang, X., Peng, B., Tian, Y ., Zhou, J., Jin, L., Song, L., Mi, H.,\nMeng, H.: Self-alignment for factuality: mitigating hallucinations\nin LLMs via self-evaluation. In: Proceedings of the 62nd Annual\nMeeting of the Association for Computational Linguistics (V olume\n1: Long Papers), ACL 2024, Bangkok, Thailand, August 11–16,\n2024, pp. 1946–1965 (2024). https://doi.org/10.18653/V1/2024.\nACL-LONG.107\n80. Song, Y ., Wang, G., Li, S., Lin, B.Y .: The good, the bad,\nand the greedy: evaluation of LLMs should not ignore non-\ndeterminism (2024). CoRR arXiv:2407.10457. https://doi.org/10.\n48550/ARXIV .2407.10457\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123\nH. Kourani et al.\nHumam Kourani is a is a Research\nAssociate at the Fraunhofer Insti-\ntute for Applied Information Tech-\nnology (FIT). He is a member\nof the Center for Process Intelli-\ngence and contributes to research\nand software development projects\nwithin the Data Science and Arti-\nﬁcial Intelligence Department. In\naddition, Humam serves as a pro-\ncess mining examiner for the\nFraunhofer Personnel Certiﬁcation\nAuthority. Humam is currently pur-\nsuing his Ph.D. at RWTH Aachen\nUniversity. His research focuses\non business process modeling, process discovery, and the integration\nof generative AI into BPM and process mining.\nAlessandro Berti is a Software\nEngineer in the Process and Data\nScience (PADS) group at RWTH\nAachen University. He is Doctor\nDesignatus in Computer Science\nat RWTH, with a thesis on object-\ncentric process mining. Alessan-\ndro co-founded the widely-used\nprocess mining library pm4py,\nwhich has achieved signiﬁcant\nrecognition within the academic\nand practitioner communities. His\nresearch interests include the devel-\nopment of innovative process min-\ning algorithms and tools, GPU-\naccelerated computations for event log analysis, and integrating gen-\nerative AI with process mining.\nDaniel Schuster is a researcher\nspecializing in process mining. He\nearned his Ph.D. from RWTH\nAachen University on Incremen-\ntal Process Discovery, which\nreceived the Best Ph.D. Disserta-\ntion Award from the IEEE Task\nForce on Process Mining. He pre-\nviously led the Process Mining\nResearch Group at the Fraunhofer\nInstitute for Applied Information\nTechnology and is now Managing\nDirector of Process Intelligence\nSolutions, a Fraunhofer spin-off\nfocused on process mining soft-\nware solutions such as PM4Py. He has published extensively in inter-\nnational conferences and journals, with research focusing on interac-\ntive and incremental process discovery, partially ordered event data,\nand user-centric process mining.\nW i lM .P .v a nd e rA a l s t is a full\nprofessor at RWTH Aachen Uni-\nversity, leading the Process and\nData Science (PADS) group. He is\nalso the Chief Scientist at Celo-\nnis, part-time afﬁliated with the\nFraunhofer FIT. He also has an\nunpaid professorship position at\nQueensland University of Tech-\nnology (since 2003). Currently, he\nis also deputy CEO of the Inter-\nnet of Production (IoP) Cluster\nof Excellence, co-director of the\nRWTH Center for Artiﬁcial Intel-\nligence, and speaker of the RWTH\nICT Proﬁle Area. His research interests include process mining, data\nscience, Petri nets, process management, workﬂow management, pro-\ncess modeling, and process analysis. Wil M. P . van der Aalst has\npublished over 300 journal papers, 35 books (as author or editor), 720\nrefereed conference/workshop publications, and 90 book chapters.\n123"
}