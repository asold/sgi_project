{
    "title": "Evaluating Language Model Finetuning Techniques for Low-resource Languages",
    "url": "https://openalex.org/W2954835819",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4225988164",
            "name": "Cruz, Jan Christian Blaise",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4225988165",
            "name": "Cheng, Charibeth",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2743945814",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2773052985",
        "https://openalex.org/W2741986357",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2951714314",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2809324505",
        "https://openalex.org/W2963088995"
    ],
    "abstract": "Unlike mainstream languages (such as English and French), low-resource languages often suffer from a lack of expert-annotated corpora and benchmark resources that make it hard to apply state-of-the-art techniques directly. In this paper, we alleviate this scarcity problem for the low-resourced Filipino language in two ways. First, we introduce a new benchmark language modeling dataset in Filipino which we call WikiText-TL-39. Second, we show that language model finetuning techniques such as BERT and ULMFiT can be used to consistently train robust classifiers in low-resource settings, experiencing at most a 0.0782 increase in validation error when the number of training examples is decreased from 10K to 1K while finetuning using a privately-held sentiment dataset.",
    "full_text": "Evaluating Language Model Finetuning Techniques for Low-resource\nLanguages\nJan Christian Blaise Cruz and Charibeth Cheng\nCenter for Language Technologies\nCollege of Computer Studies\nDe La Salle University, Manila\n{jan christian cruz, charibeth.cheng}@dlsu.edu.ph\nAbstract\nUnlike mainstream languages (such as En-\nglish and French), low-resource languages of-\nten suffer from a lack of expert-annotated cor-\npora and benchmark resources that make it\nhard to apply state-of-the-art techniques di-\nrectly. In this paper, we alleviate this scarcity\nproblem for the low-resourced Filipino lan-\nguage in two ways. First, we introduce a new\nbenchmark language modeling dataset in Fil-\nipino which we call WikiText-TL-39. Second,\nwe show that language model ﬁnetuning tech-\nniques such as BERT and ULMFiT can be\nused to consistently train robust classiﬁers in\nlow-resource settings, experiencing at most a\n0.0782 increase in validation error when the\nnumber of training examples is decreased from\n10K to 1K while ﬁnetuning using a privately-\nheld sentiment dataset.\n1 Introduction\nThe use of neural networks in Natural Language\nProcessing (NLP) has achieved great successes in\nmultiple areas such as language modeling (Merity\net al., 2016), machine translation (Vaswani et al.,\n2017; Bahdanau et al., 2014), and multitask learn-\ning (Radford et al., 2019; McCann et al., 2018).\nWhile effective, neural network methods are\ndata-hungry and do not operate well in data scarce\nsettings such as with low-resource languages (Zoph\net al., 2016). In addition, such languages may also\nnot have readily-available resources found in main-\nstream languages such as pretrained word embed-\ndings and expert-annotated corpora (Adams et al.,\n2017).\nThis data scarcity problem is best met with\nthe construction of properly annotated corpora for\nsuch tasks, however such annotation work is cost-\nprohibitive and time-consuming (Cotterell and Duh,\n2017). Techniques must be developed to address\nthe low-resource case in NLP and allow robust\nmodels to be trained despite data scarcity (Cotterell\nand Duh, 2017).\nTransfer learning provides one way to offset this\ndata scarcity problem, allowing models to be pre-\ntrained then suibsequently ﬁnetuned on a smaller\ndataset, reducing not only the resource require-\nmens, but also the compute and time requirements\nto achieve a robust model (Howard and Ruder,\n2018).\nIn this paper, we provide two contributions: ﬁrst,\nwe release the ﬁrst open, large-scale preprocessed\nunlabeled text corpora in the low-resource Filipino\nlanguage which we call “WikiText-TL-39.” Sec-\nond, we show that transfer learning techniques\nsuch as BERT (Devlin et al., 2018) and ULMFiT\n(Howard and Ruder, 2018) can be used to train\nrobust classiﬁers in low-resource settings, experi-\nencing at most a 0.0782 increase in error when the\nnumber of training examples is reduced from 10K\nto 1K.\nWe open source all pretrained models and\ndatasets in an open, public repository1.\n2 Methodology\nOur evaluation methodology is as follows: First,\nwe construct a large-scale unlabeled text corpora to\ntrain pretrained language models to transfer from.\nSecond, we evaluate transfer learning performance\non a privately held sentiment dataset. We will then\nsteadily decrease the number of training examples\nand study the changes on validation accuracy.\nWe use two transfer learning techniques, namely\nBERT (Devlin et al., 2018) and (Howard and Ruder,\n2018).\n2.1 ULMFiT\nULMFiT (Howard and Ruder, 2018) was intro-\nduced as a transfer learning method for Natural\n1https://github.com/jcblaisecruz02/Tagalog-BERT\narXiv:1907.00409v1  [cs.CL]  30 Jun 2019\nFigure 1: Overall ULMFiT pretraining and ﬁnetuning framework. An AWD-LSTM (Merity et al., 2017) is pre-\ntrained on a language modeling task. The weights are then reused with no modiﬁcations to the architecture. For\nﬁnetuning, the model is ﬁrst ﬁnetuned, again using language modeling, this time to the text of the target dataset\nto adapt to its own vocabulary and idiosyncracies. Lastly, a “classiﬁcation layer” is added to the model and is\nﬁnetuned for text classiﬁcation. Adapted from Howard and Ruder (2018).\nLanguage Processing that works akin to ImageNet\n(Russakovsky et al., 2015) pretraining in Computer\nVision.\nIt uses an AWD-LSTM (Merity et al., 2017) pre-\ntrained on a language modeling objective as a base\nmodel, which is then ﬁnetuned to a downstream\ntask in two steps.\nFirst, the language model is ﬁnetuned to the text\nof the target task to adapt to it syntactically. Second,\na classiﬁcation layer is appended to the model and\nis ﬁnetuned to the classiﬁcation task conservatively.\nDuring ﬁnetuning, multiple different techniques\nare introduced to prevent catastrophic forgetting,\nwherein the model loses most (if not all) informa-\ntion and relations it has learned during the pretrain-\ning stage.\nULMFiT holds state-of-the-art for text classiﬁca-\ntion, and is notable for being able to set comparable\nscores with as little as 1000 samples of data which\nmakes it attractive for use in low-resource settings.\nAn overview schematic of ULMFiT can be\nfound in ﬁgure 1.\n2.2 BERT\nBERT is a transformer-based (Vaswani et al., 2017)\nlanguage model that is designed to pretrain “deep\nbidirectional representations” that can be ﬁne-\ntuned to different tasks, with state-of-the-art results\nachieved in multiple benchmarks (Devlin et al.,\n2018).\nBERT’s power comes from Attention, a mech-\nanism that allows a network to give more weight\nto certain tokens in a sequence, essentially “paying\nmore attention to important parts” (Vaswani et al.,\n2017). Precisely, we compute attention on a set\nof queries packed as a matrix Qon key and value\nmatrices Kand V, respectively, as:\nAttention(Q,K,V ) =softmax(QKT\n√dk\n)V (1)\nwhere dk is the dimensions of the key matrixK. At-\ntention allows BERT to model not only sequences,\nbut also the importance and weight of each token\nin a sequence with respect to other sequences, as\nwell as itself.\nIn addition to leveraging Attention, it uses the\nTransformer (Vaswani et al., 2017) architecture, to\nwhere BERT gains its bidirectionality. Transform-\ners are sequence models that do not use recurrent\nlayers, instead leveraging only feed-forward lay-\ners and attention mechanisms. The disuse of re-\ncurrences provide two advantages: First, it allows\ntransformers to be parallelized as they are not se-\nquential in nature unlike LSTMs or GRUs. Second,\nthey allow batches of text to be seen at once, again\ndue to its unsequential nature, which also in turn\nallows it to leverage attention mechanisms and be\nbidirectional.\nBERT is unique that it uses modiﬁed tasks for\npretraining. Given that its bidirectionality gives it\naccess to left-context, the model would be able to\n“peek” directly at the next words when following a\nstandard language modeling task. To alleviate this,\nthe authors propose the use of “masked-language\nFigure 2: Overall BERT pretraining and ﬁnetuning framework. Note that the same architecture in pretraining is\nalso used in ﬁnetuning with little-to-no modiﬁcation in structure. After masked-language model and next-sentence\nprediction is pretrained, we transfer the weights of the model to downstream tasks, with question answering and\nentailment shown in this example. Adapted from Devlin et al. (2018).\nmodeling,” which masks a number of words in the\nsentence with the model tasked to identify them\n(Devlin et al., 2018). In addition, a second pre-\ntraining task called “next-sentence prediction” was\nadded to enforce stronger relationships between\ntwo sentences. In this task, a target sentence is\nidentiﬁed if it is likely to precede a source sentence\n(Devlin et al., 2018).\nIn addition to these augmentations, BERT also\nbeneﬁts from being deep, allowing it to capture\nmore context and information. BERT-Base, the\nsmallest BERT model, has 12 layers (768 units in\neach hidden layer) and 12 attention heads for a\ntotal of 110M parameters. Its larger sibling, BERT-\nLarge, has 24 layers (1024 units in each hidden\nlayer) and 16 attention heads for a total of 340M\nparameters.\nAn overview schematic of BERT can be found\nin ﬁgure 2.\n3 WikiText-TL\nA difﬁculty in adapting pretraining methods to low-\nresource languages is the lack of processed datasets\nlarge enough to train robust pretrained models. In-\nspired by the original WikiText Long Term Depen-\ndency Language Modeling Dataset (Merity et al.,\n2016), we introduce a benchmark dataset which we\ncall WikiText-TL-39, where “TL” stands forTaga-\nlog and “39” refers to the dataset having 39 million\ntokens in the training set. The corpus statistics for\nWikiText-TL-39 is shown on table 1.\n3.1 Construction and Pre-processing\nSince Tagalog Wikipedia does not have a list of\nveriﬁed “good” articles (Merity et al., 2016) and\nhas far fewer content pages unlike its English coun-\nterpart (5,800,000 in English vs. 75,000), we opted\nto instead scrape the content from all the listed\npages in the Tagalog Wikipedia table of contents2,\nnarrowing down to just articles with titles that start\nwith letters A-Z. Content was extracted using open-\nsource Python packages Requests3 and Beautiful-\nSoup4.\nAll characters were normalized into unicode and\nall HTML markup were unescaped. Normalization\nand tokenization were performed via the Moses\nTokenizer (Koehn et al., 2007). We split the corpus\ninto training, validation, and test sets with a ratio of\n70%-15%-15%, respectively. When constructing\nthe vocabulary, we opted to not discard words that\nhad a vocabulary count of less than 3, unlike in\n(Chelba et al., 2013). This resulted in a vocabulary\nsize of 279,153 tokens. We replace all tokens in\nthe test set unseen in the training set with special\n<unk>tokens.\n3.2 Model-speciﬁc Pre-processing\nPretraining with BERT requires a trained Word-\nPiece vocabulary. We opted to use the Byte-Pair\nEncoding (BPE) (Sennrich et al., 2016) model in\n2https://tl.wikipedia.org/wiki/Natatangi:Lahat ng mga pahina\n3https://pypi.org/project/requests/\n4https://pypi.org/project/beautifulsoup4/\nSplit Documents Tokens Unique Tokens Num. of Lines\nTraining 120,975 39,267,089 279,153 1,403,147\nValidation 25,919 8,356,898 164,159 304,006\nTesting 25,921 8,333,288 175,999 298,974\nOOV Tokens 28,469 (0.1020%)\nTable 1: Statistics for the WikiText-TL-39 Dataset.\nGoogle’s SentencePiece5 library to train our own\nvocabulary as Google did not release the original\nWordPiece code due to it having dependencies with\ntheir own internal libraries.\nWe experiment with two ﬁxed vocabulary sizes\nin pretraining BERT. We generate a vocabulary\nwith 290,000 tokens, following the original vo-\ncabulary size of the dataset. We also generate a\nvocabulary with a ﬁxed size of 30,000 tokens, fol-\nlowing the original speciﬁcations of Google’s own\npretrained English BERT models6.\nFor use in ULMFiT, we followed a light prepro-\ncessing scheme that involves converting all words\nto lowercase, with a special <maj >token added\nin front of words that originally start with a capital\nletter. We likewise change all unknown words to\nthe <unk >token, and limit the vocabulary to the\ntop 30K words.\n4 Experiments\n4.1 BERT Pretraining\nWe pretrain BERT Base models with 12 layers, 768\nneurons per hidden layer, and 12 attention heads\n(a total of about 110M parameters) on our pre-\npared corpus and SentencePiece vocaularies using\nGoogle’s provided pretraining scripts7.\nWe experiment by varying the casing (cased and\nuncased models), the vocabulary size (full 290K\nvs 30K), and the number of training and warmup\nsteps (1M steps with 10K warmups and 500K steps\nwith 5K warmups).\nFor the masked language model pretraining ob-\njective, we follow the original speciﬁcations and\nuse a 0.15 probability of a word being masked. We\nalso set the maximum number of masked language\nmodel predictions to the original 20. All models\nuse a maximum sequence length of 128 and a batch\nsize of 256. We use a learning rate of 1e-4 for all\nmodels.\n5https://github.com/google/sentencepiece\n6https://github.com/google-research/bert\n7https://github.com/google-research/bert\nAll models are pretrained on Google Cloud Com-\npute Engine using Google’s Tensor Processing\nUnits (TPU) version 2.8.\n4.2 A WD-LSTM Pretraining\nFor ULMFiT, we train an AWD-LSTM language\nmodel using our prepared corpus. We train a 3-\nlayer model and use an embedding size of 400 and\na hidden size of 1150. We set the dropout values\nfor the embedding, the RNN input, the hidden-to-\nhidden transition, and the RNN output to (0.1, 0.3,\n0.3, 0.4) respectively. We use a weight dropout of\n0.5 on the LSTM’s recurrent weight matrices.\nThe model was trained for 30 epochs with a\nlearning rate of 1e-3, a batch size of 128, and a\nweight decay of 0.1. We use the Adam optimizer\nand use slanted triangular learning rate schedules.\nWe train the model on a machine with one NVIDIA\nTesla V100 GPU.\n4.3 Sentiment Classiﬁcation Task\nWe ﬁnetune on a privately held sentiment classi-\nﬁcation dataset containing 10K positive and 10K\nnegative reviews on electronic products.\nTo simulate low-resource settings, we randomly\nsample splits from the original dataset: a full 10K-\n10K split of positive and negative reviews, a 5K-5K\nsplit, a 1K-1K split, and a 100-100 split. For both\nBERT and ULMFiT, we ﬁnetune the pretrained\nmodels to each split to evaluate performance given\nthe scarcity of data.\nTo evaluate the performance, we use a valida-\ntion set of 1500 positive and 1500 negative reviews\nfrom the same source. For each split, we use the\nsame validation split without reducing it. This en-\nsures consistency when evaluating the changes in\nvalidation accuracy once the number of training\nexamples is reduced.\nThe dataset is lightly preprocessed using the\nMoses tokenizer (Koehn et al., 2007), keeping cas-\ning and placing spaces around punctuation. Con-\ntractions with an apostrophe (ie. cannot → can’t)\nare not given special tokens nor are preprocessed\nfurther as such contractions are rare in Filipino.\n4.4 Finetuning\nFor BERT, we ﬁnetune our best cased and uncased\nBERT models on each sentiment classiﬁcation split.\nFor each ﬁnetuning setup, we ﬁnetune for 3 epochs\nwith a learning rate of 2e-5. We use a maximum\nsequence length of 128 and a batch size of 32.\nFor ULMFiT, we ﬁnetune our pretrained AWD-\nLSTM language on each of the sentiment classi-\nﬁcation splits. We ﬁrst perform language model\nﬁnetuning with the sentiment classiﬁcation dataset\nfor 10 epochs, using a learning rate 1e-2. For the\noriginal 10k-10k split, we use weight decay of 0.1\nand a batch size of 80, and for all other splits we use\nweight decay of 0.3 and a batch size of 40. We use\nthis ﬁnal language model to ﬁnetune a sentiment\nclassiﬁcation model in the ﬁnal stage of ULMFiT.\nFor the ﬁnal ULMFiT stage, we ﬁnetune via\ngradual unfreezing. We ﬁnetune for ﬁve epochs,\ngradually unfreezing the last layer until all layers\nare unfrozen on the fourth epoch. We use a learning\nrate of 1e-3 and set Adam’sαand βparameters to\n0.8 and 0.7 respectively.\nWe then evaluate on a ﬁxed validation set and\nrecord changes in the model performance.\n5 Results and Discussion\n5.1 Pretraining Results\nFor BERT pretraining, we were able to train eight\nmodels, varying across vocabulary size, casing, and\npretraining steps. Our best uncased model (reach-\ning a ﬁnal loss of 0.0935) was trained for 500K\nsteps with 5K steps of ﬁnetuning on the smaller\n30K SentencePiece vocabulary. The best cased\nmodel (reaching a ﬁnal loss of 0.0642), on the\nother hand, needed 1M pretraining steps with 10K\nwarmup steps on the same 30K SentencePiece vo-\ncabulary. We surmise that this is due to the model\nneeding more steps to learn and get accustomed to\ncasing.\nThe full results of BERT pretraining can be\nfound on Table 2.\nFor ULMFiT, our AWD-LSTM language model\nreached a ﬁnal validation loss of 4.4857 (which\nequals to 1.5009 perplexity). The model ﬁnished\ntraining for 30 epochs after around 11 hours.\n5.2 Finetuning Results\nFor BERT ﬁnetuning, the uncased model per-\nformed marginally better than the cased model with\na 0.006 increase in accuracy when ﬁnetuning on\nthe original 10K-10K split. We can see that when\nwe reduce the training examples from 10K to 1K,\nwe incur at most a 0.0617 increase of error in the\ncased models, and a 0.0954 increase of error in the\nuncased models. The error signiﬁcantly increases\nonce the number of training examples drop to the\n100-100 split, with an increase of 0.1484 error in\nthe cased model, and an increase of 0.2554 error in\nthe uncased model.\nWhen evaluating on the validation set of the orig-\ninal 10K-10K split, we can see similar results as\nwith evaluating on the validation set of each re-\nspective split. For the cased models, we only incur\na 0.038 increase of error when ﬁnetuning on the\n1K-1K split, and a 0.23 increase of error when\nﬁnetuning on the 100-100 split. For the uncased\nmodels, we get a 0.0437 and 0.248 increase of error\non the 1K-1K split and 100-100 split, respectively.\nThe full results of BERT ﬁnetuning can be found\non table 3.\nFor ULMFiT ﬁnetuning, our best model was\nunsurprisingly the one ﬁnetuned on the entire 10K-\n10K split, getting a ﬁnal validation accuracy of\n0.9018. Reducing the number of examples down to\nthe 1K-1K split incurred only a 0.0835 increase in\nerror. On the 100-100 split, on the other hand, we\ncan see that the error increased by a very large mar-\ngin of 0.4628, reducing the accuracy from 0.9018\nto 0.4390.\nLike in the BERT ﬁnetuning setups, we can see\nthat the ﬁnetuned classiﬁers give consistently ro-\nbust results even when evaluated on the larger 10K-\n10K split validation set. We can see that reducing\nthe examples down to the 1K-1K split increases\nerror by 0.0782, comparable to evaluating on the\n1K-1K split’s validation set. Likewise, we suffer a\nlarge increase in error of 0.4114 when evaluating\non the 100-100 split.\nThe full results of ULMFiT ﬁnetuning can be\nfound on table 3.\n5.3 Discussion\nWe can see that language model pretraining can aid\nin low-resource settings as empirically shown in\nthe experiments above. The ﬁnetuned models were\nshown perform consistently even when the number\nof training examples were reduced by evaluating\non the same validation set.\nULMFiT performed marginally better than\nBERT (a difference of 0.0201) when ﬁnetuned on\nSteps / Warmup Casing V ocab Size Loss MLM Acc NSP Acc Train Time\n500K / 5K Cased 290K 0.3198 0.9158 0.9950 22H\n500K / 5K Cased 30K 0.1046 0.9865 1.0000 33H\n500K / 5K Uncased 290K 0.3396 0.9176 0.9986 24H\n500K / 5K Uncased 30K 0.0935 0.9862 1.0000 33H\n1M / 10K Cased 290K 0.1607 0.9563 0.9988 44H\n1M / 10K Cased 30K 0.0642 0.9971 1.0000 66H\n1M / 10K Uncased 290K 0.0716 0.9965 1.0000 168H\n1M / 10K Uncased 30K 0.2600 0.9426 1.0000 22H\nTable 2: BERT Pretraining Results. MLM Acc refers to Masked Language Modeling objective accuracy. NSP\nAcc refers to Next Sentence Prediction objective accuracy. Figures in bold pertain to the best performing cased\nand uncased models.\nModel Type Splits Val Loss Val Acc 10K Val Acc Err Increase 10K Err Increase\nBERT-Cased 10k-10k 0.3492 0.8817 - - -\nBERT-Cased 5k-5k 0.3841 0.8760 0.8976 +0.0057 -0.0159*\nBERT-Cased 1k-1k 0.4746 0.8200 0.8437 +0.0617 +0.0380\nBERT-Cased 100-100 0.6122 0.7333 0.6517 +0.1484 +0.2300\nBERT-Uncased 10k-10k 0.3401 0.8887 - - -\nBERT-Uncased 5k-5k 0.3727 0.8793 0.8970 +0.0094 -0.0083*\nBERT-Uncased 1k-1k 0.5667 0.7933 0.8450 +0.0954 +0.0437\nBERT-Uncased 100-100 0.6606 0.6333 0.6407 +0.2554 +0.2480\nULMFiT 10k-10k 0.2496 0.9018 - - -\nULMFiT 5k-5k 0.2489 0.8961 0.8887 +0.0057 +0.0194\nULMFiT 1k-1k 0.4193 0.8183 0.8236 +0.0835 +0.0782\nULMFiT 100-100 0.7020 0.4390 0.4904 +0.4628 +0.4114\nTable 3: Finetuning Results. 10K Val Acc refers to validation accuracy when evaluating on the validation set of the\noriginal 10K-10K split. Err Increase refers to the increase in error when number of training examples were reduced\nto a particular split. 10K Err Increase refers to the increase in error when evaluating on the original 10K-10K split\nvalidation set once training examples are reduced. * pertains to instances when the 10K Val Acc is higher than the\nVal Acc of a particular split.\nthe full dataset. ULMFiT has the advantage that it\nrequires less computational power and resources to\neffectively train end-to-end. An AWD-LSTM lan-\nguage model can be trained in a relatively-modern\nGPU and can be ﬁnetuned with relative speed to\nBERT. This makes ULMFiT ideal in most low-\nresource cases when pretrained models are unavail-\nable as it is cheaper to produce AWD-LSTM lan-\nguage models than pretrained BERT models.\nOn the other hand, it is worth to note that BERT\nperformed more consistently on average than ULM-\nFiT. BERT experienced a lower error increase on\naverage compared to ULMFiT, with BERT-Cased,\nBERT-Uncased, and ULMFiT experiencing an av-\nerage validation error increase of 0.0719, 0.1201,\nand 0.1840, respectively. BERT is also more re-\nsilient to drastic reduction in training examples.\nWhen reducing the splits from 1K-1K to 100-100,\nBERT (evaluated on the full 10K-10K split valida-\ntion set) experienced an increase of error by 0.192\non the cased models and 0.2043 on the uncased\nmodels. ULMFiT, on the other hand, experienced\nan error increase of 0.3332.\nBERT also has the advantage of being bidirec-\ntional, which allows it to look at both left and right\ncontext as needed, compared to ULMFiT, where\nthe AWD-LSTM language model only used left\ncontext. BERT is also signiﬁcantly more deep than\nULMFiT’s AWD-LSTM, with BERT-Base having\n12 layers and 12 attention heads as opposed to an\nAWD-LSTM’s 3 layers. This allows it to learn\nmore complex relationships within the data.\nWhile the advantages of the much-larger BERT\nare evident, it is important to note that it requires\ncompute resources orders of magnitude greater\nthan needed when training an AWD-LSTM. Pre-\ntraining BERT requires at least a TPU in order to\nmeet the memory requirements. It also takes much\nlonger to train than an AWD-LSTM. The pretrained\nmodels in this work are all BERT-Base models, us-\ning one whole TPU in order to train and at least a\nlittle over a day to achieve robust results. Larger\ndatasets and model conﬁgurations would naturally\nrequire more time and memory. This makes scal-\ning up to BERT-Large hard. The original BERT\nimplementation used 4 cloud TPUs for BERT-Base\nand 16 TPUs for BERT-Large. Finetuning BERT\nlikewise has sizeable memory requirements, with\nBERT-Base requiring at least a modest-to-high-end\nGPU. BERT-Large will have a difﬁculty in GPU-\nﬁnetuning8, requiring the use of gradient accumu-\nlation and other techniques to simulate larger batch\nsizes as small batch sizes will hurt ﬁnetuning per-\nformance. While powerful, the resources needed\nto use BERT make it restrictive.\n6 Conclusion\nWe show that language model ﬁnetuning methods\naid in low-resource settings, especially when the\nnumber of expert-annotated examples is scarce.\nLanguage model pretraining offers two advan-\ntages: ﬁrst, performing pretraining only requires\nunlabeled text corpora, which is virtually abundant\neven in low-resource settings. Second, once pre-\ntraining is done, ﬁnetuning is inexpensive and can\nbe performed multiple times on the same pretrained\nmodel. This allows researchers to use only a frac-\ntion of resources to create robust baselines even in\nlow-resource settings.\nChoosing the ﬁnetuning technique involves a\ncost-consistency tradeoff. We propose the use of\nULMFiT as a general-case ﬁnetuning-based base-\nline as it’s pretraining step is relatively less ex-\npensive than BERT. While BERT is powerful, it’s\ncompute and memory requirements make it restric-\ntive, and should only be used if a pretrained model\nexists or if the resources available permit it’s use.\nAcknowledgments\nThe authors would like to thank the TensorFlow\nResearch Cloud (TFRC) program, which allowed\nthe pretraining of BERT models more accessible.\n8https://github.com/google-research/bert#out-of-memory-\nissues\nReferences\nOliver Adams, Adam Makarucha, Graham Neubig,\nSteven Bird, and Trevor Cohn. 2017. Cross-lingual\nword embeddings for low-resource language model-\ning. In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Volume 1, Long Papers, pages\n937–947, Valencia, Spain. Association for Compu-\ntational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, and Phillipp Koehn. 2013. One bil-\nlion word benchmark for measuring progress in sta-\ntistical language modeling. CoRR, abs/1312.3005.\nRyan Cotterell and Kevin Duh. 2017. Low-\nresource named entity recognition with cross-\nlingual, character-level neural conditional random\nﬁelds. In Proceedings of the Eighth International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 91–96, Taipei, Tai-\nwan. Asian Federation of Natural Language Process-\ning.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProceedings of the 45th Annual Meeting of the ACL\non Interactive Poster and Demonstration Sessions,\nACL ’07, pages 177–180, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language de-\ncathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and Optimizing\nLSTM Language Models. arXiv preprint\narXiv:1708.02182.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. CoRR, abs/1609.07843.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, An-\ndrej Karpathy, Aditya Khosla, Michael Bernstein,\nAlexander C. Berg, and Li Fei-Fei. 2015. Ima-\ngeNet Large Scale Visual Recognition Challenge.\nInternational Journal of Computer Vision (IJCV),\n115(3):211–252.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1568–1575, Austin,\nTexas. Association for Computational Linguistics."
}