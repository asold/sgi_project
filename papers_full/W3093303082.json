{
  "title": "Transformer-Based Neural Text Generation with Syntactic Guidance",
  "url": "https://openalex.org/W3093303082",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2186438245",
      "name": "Li Yinghao",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2117987791",
      "name": "Feng Rui",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4287574517",
      "name": "Rehg, Isaac",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1851812986",
      "name": "Zhang Chao",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2931212643",
    "https://openalex.org/W2964008635",
    "https://openalex.org/W2970901330",
    "https://openalex.org/W2962951611",
    "https://openalex.org/W2803930360",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2948197522",
    "https://openalex.org/W2758334418",
    "https://openalex.org/W2963888305",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2964202145",
    "https://openalex.org/W2947683321",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2963667126",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2890914727",
    "https://openalex.org/W2964212550",
    "https://openalex.org/W2735642330",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W3014718583",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2793978524",
    "https://openalex.org/W2966223443",
    "https://openalex.org/W3034531294",
    "https://openalex.org/W2951520302",
    "https://openalex.org/W2101105183"
  ],
  "abstract": "We study the problem of using (partial) constituency parse trees as syntactic guidance for controlled text generation. Existing approaches to this problem use recurrent structures, which not only suffer from the long-term dependency problem but also falls short in modeling the tree structure of the syntactic guidance. We propose to leverage the parallelism of Transformer to better incorporate parse trees. Our method first expands a partial template constituency parse tree to a full-fledged parse tree tailored for the input source text, and then uses the expanded tree to guide text generation. The effectiveness of our model in this process hinges upon two new attention mechanisms: 1) a path attention mechanism that forces one node to attend to only other nodes located in its path in the syntax tree to better incorporate syntax guidance; 2) a multi-encoder attention mechanism that allows the decoder to dynamically attend to information from multiple encoders. Our experiments in the controlled paraphrasing task show that our method outperforms SOTA models both semantically and syntactically, improving the best baseline's BLEU score from 11.83 to 26.27.",
  "full_text": "Transformer-Based Neural Text Generation with Syntactic Guidance\nYinghao Li\nGeorgia Institute of Technology\nyinghaoli@gatech.edu\nRui Feng\nGeorgia Institute of Technology\nrfeng@gatech.edu\nIsaac Rehg\nGeorgia Institute of Technology\nisaacrehg@gatech.edu\nChao Zhang\nGeorgia Institute of Technology\nchaozhang@gatech.edu\nAbstract\nWe study the problem of using (partial) con-\nstituency parse trees as syntactic guidance\nfor controlled text generation. Existing ap-\nproaches to this problem use recurrent struc-\ntures, which not only suffer from the long-\nterm dependency problem but also falls short\nin modeling the tree structure of the syntac-\ntic guidance. We propose to leverage the\nparallelism of Transformer to better incorpo-\nrate parse trees. Our method ﬁrst expands a\npartial template constituency parse tree to a\nfull-ﬂedged parse tree tailored for the input\nsource text, and then uses the expanded tree to\nguide text generation. The effectiveness of our\nmodel in this process hinges upon two new at-\ntention mechanisms: 1) a path attention mech-\nanism that forces one node to attend to only\nother nodes located in its path in the syntax\ntree to better incorporate syntax guidance; 2)\na multi-encoder attention mechanism that al-\nlows the decoder to dynamically attend to in-\nformation from multiple encoders. Our experi-\nments in the controlled paraphrasing task show\nthat our method outperforms SOTA models\nboth semantically and syntactically, improving\nthe best baseline’s BLEU score from 11.83 to\n26.27.\n1 Introduction\nGenerating text that conforms to syntactic or se-\nmantic constraints beneﬁts many NLP applications.\nTo name a few, when paired data are limited, Yang\net al. (2019) build templates from large-scale un-\npaired data to aid the training of the dialog genera-\ntion model; Niu et al. (2017) and Liu et al. (2019)\napply style constraints to adjust the formality or\nrhetoric of the utterances; Iyyer et al. (2018) and\nLi et al. (2019a) augment dataset using controlled\ngeneration to improve the model performance.\nWe study the problem of syntactically controlled\ntext generation, which aims to generate target text\nI had a dream about you yesterday\nSource TextI had a dream yesterday and it was about you.\nTemplate ParseS NP NN VP VBD NP1 2  3  2  3   3 \nExpanded Parse\nS NP NN VP VBD NP NP DT NN PP IN NP NN NN1 2  3  2  3   3  4  5  5  4  5  5  6  5 \nGenerated Text\nSyntax \nExpan \nT ext \nGen \nFigure 1: The pipeline of the syntactically guided para-\nphrasing process. The template parse is the top-ℓlevels\nof a full-ﬂedged parse tree. It is ﬁrst expanded to a full\ntree, with which the target text is generated to match\nthe semantics of the source text.\nwith pre-deﬁned syntactic guidance. Most recent\nstudies on this topic (Chen et al., 2019a; Bao et al.,\n2019) use sentences as exemplars to specify syntac-\ntic guidance. However, the guidance speciﬁed by\na sentence can be vague, because its syntactic and\nsemantic factors are tangled. Different from them,\nwe use constituency parse trees asexplicit syntactic\nconstraints. As providing full-ﬂedged parse trees\nof the target text is impractical, we require only a\ntemplate parse tree that sketches a few top levels of\na full tree (§ 2). Figure 1 shows our pipeline.\nIyyer et al. (2018) adopt the same setting as\nours. Their proposed SCPN model uses two LSTM\n(Hochreiter and Schmidhuber, 1997) encoders to\nrespectively encode source text and parse tree, and\nconnects them to one decoder with additional atten-\ntion (Bahdanau et al., 2014) and pointer (See et al.,\n2017) structures. Nonetheless, recurrent encoders\nnot only suffer from information loss by compress-\ning a whole sequence into one vector but also are\nincapable of properly modeling the tree structure\nof constituency parse as well. Consequently, their\nnetwork tends to “translate” the parse tree, instead\nof learning the real syntactic structures from it.\nWe propose a Transformer-based syntax-guided\ntext generation method, named GuiG. It ﬁrst ex-\npands a template constituency parse tree to a full-\narXiv:2010.01737v1  [cs.CL]  5 Oct 2020\nﬂedged parse tree tailored for the input source text,\nand then uses the full tree to guide text generation.\nTo capture the tree structure of the syntax, we ap-\nply a path attention mechanism (§ 3.2) to our text\ngeneration model. It forces one node to attend to\nonly other nodes located in its path (i.e., its ances-\ntors and descendants) instead of all the nodes in the\ntree. Such a mechanism limits the information ﬂow\namong the nodes in the constituency tree that do not\nhave the direct ancestor-descendant relationship,\nforcing the parent nodes to carry more information\nthan their children. In cooperation with path atten-\ntion, we linearize the constituency trees to a more\ncompact node-level format (§ 3.1). Moreover, to\naddress the challenge of properly integrating the\nsemantic and syntactic information, we design a\nmulti-encoder attention mechanism (§ 3.1). It en-\nables the Transformer decoder to accept outputs\nfrom multiple encoders simultaneously.\nWe evaluated our model on the controlled\nparaphrasing task. The experiment results show\nthat GuiG outperforms the state-of-the-art SCPN\nmethod by 6.7% in syntactic quality and 122.1%\nin semantic quality. Human evaluations prove our\nmethod generates semantically and syntactically su-\nperior sentences, with 1.13 semantic and 0.62 syn-\ntactic score improvements. Further, we ﬁnd that the\nmulti-encoder attention mechanism enhances the\nTransformer’s ability to deal with multiple inputs,\nand the path attention mechanism signiﬁcantly con-\ntributes to the model’s semantic performance (§ 4).\nOur contributions include: 1) a multi-encoder\nattention mechanism that allows a Transformer de-\ncoder to attend to multiple encoders; 2) a path at-\ntention mechanism designed to better incorporate\ntree-structured syntax guidance with a special tree\nlinearization format; and 3) a syntax-guided text\ngeneration method GuiG that achieves new state-\nof-the-art semantic and syntactic performance.\n2 Problem Setup\nSyntax-guided text generation aims to generate tar-\nget text stgt from 1) a source sentence ssrc and\n2) a syntax template xtmpl, such that the gener-\nated sentence utilizes the semantics of ssrc and is\nsyntactically aligned with xtmpl.\nFor the sentences, we tokenize them into sub-\nword units using byte pair encoding (BPE) (Sen-\nnrich et al., 2016). This prevailing encoding\nmethod not only solves the out-of-vocabulary\n(OOV) issue but also has the ability to model the\ncharacter of word roots and afﬁxes. Formally, the\ntokenized text sequence is represented by s =\n(s(1),s(2),...,s (M)) with s(i) ∈C, where Cis the\nset of all sub-word units and M is the sequence\nlength. Moreover, we assume the constituency\nparse tree of the source sentence ssrc is also avail-\nable, denoted as the source parse xsrc.\nThe syntax template xtmpl is a partial con-\nstituency parse tree that provides high-level syntax\nsketches. We use the top-ℓ(ℓ= 3in this work) lev-\nels of target parse xtgt, which is the full-ﬂedged\nconstituency tree of stgt. xtmpl can be also fre-\nquent templates mined from any text corpora.\n3 Methodology\nOur method GuiG contains two models (Figure 1)—\na syntax expander (§ 3.1) that expands the template\nparse, and a text generator ( § 3.2) that leverages\nthe expanded parse to control text generation.\n3.1 Syntax Expansion\nThe goal of our syntax expander is to construct a\nvalid full-ﬂedged target parse tree ˆxtgt from the\ntemplate parse xtmpl. To adapt ˆxtgt to the source\ntext ssrc, we use the source parse xsrc of ssrc to\nguide the syntax expansion process.\nParse Tree Linearization We use a pair\nof node and level sequences to represent the\nconstituency parse tree. A constituency parse\ntree x is thus linearized to a node-level format\nsequence x = ( x(1),x(2),...,x (N)) where N\nis the number of nodes in the parse tree x. For\neach x(i) = {p(i),l(i)}, p(i) is the parse node\nand l(i) is its level. For example, the parse of\nthe sentence “I ate an apple” is represented by\nthe node sequence “ S NP PRP VP VBD NP\nDT NN” and the level sequence “ 1 2 3 2 3\n3 4 4”. Comparing with the existing bracketed\nformat, which linearizes the above sentence to\n“(S(NP(PRP))(VP(VBD)(NP(DT)(NN))))”,\nour node-level representation reduces the parse\nsequence length to 1/3. This more compact repre-\nsentation decreases the time consumption for both\nsyntax encoding and prediction, thus facilitating\nthe syntax expansion and text generation steps.\nAt the embedding layer, the parse node tokens\nand level tokens are embedded respectively and\nthen added together to produce the syntax embed-\nding at position i:\nEmb(x(i)) = Emb(p(i)) + Emb(l(i)).\nScaled Dot-Product\nAttention\nLinearLinearn 1 ✕ \nTransformer\nEncoder\nBlock\nxsrc xtmpl\nLinear & Norm\nAdd & Norm\nLinear\nSoftmax\nxtgt (shifted right)\nMulti-Head\nAttention\nParse Node\nProbabilities\nAdd & Norm\nMulti-Encoder\nAttention\nTransformer\nEncoder\nBlock\nn 1 ✕ n 2 ✕ \nS\nNP VP\nDT ADJPJJ NNP\nRB JJ\nNNS PP\nNP\nDT JJ NN\nParse Nodes\n1\n2\n3\n4\n5\nLevels\nIN\nTemplate\nParse xtmpl\nTarget Parse xtgt\nLinearLinear LinearLinear\nScaled Dot-Product\nAttention\nKey Query \nH 1 H 1 \nV alue \nO \nKey Query \nH 2 H 2 \nV alue \nO \nLinear Linear Linear\nConcat\nLinear\nOutput\nH 1 H 2 \nLinear\nScaled Dot-Product\nAttention\nLinearLinear\nScaled Dot-Product\nAttention\nLinearLinearLinear\nScaled Dot-Product\nAttention\nh 1 h 2 \nLinear\nSoftmax Level\nProbabilities\nMulti-Encoder Attention Network StructureTarget and Template\nConstituency Parses\n(Source Parse xsrc is in the same format)\nEncoded\nSequences\nSyntax Expander Network Architecture\nSource\nEncoder\nTemplate\nEncoder\nFigure 2: The left block is the syntactic representation of the sentence “ the very quick brown fox\njumps over the lazy dog” with template parse truncation depth ℓ = 3; in the middle is the model archi-\ntecture of the syntax expander, and on the right is the detailed structure of the multi-encoder attention mechanism.\nn1 and n2 are the number of blocks; h1 and h2 are the number of attention heads; “O” indicates the output of the\nprevious network layer.\nMulti-Encoder Attention Figure 2 illustrates\nthe syntax expansion model in GuiG. As shown,\nthe model has two Transformer encoders: a source\nencoder that encodes xsrc, and a template encoder\nthat encodes xtmpl. Intuitively, xtmpl regulates\nˆxtgt’s high-level syntactic structure, while the ex-\npander ﬁlls the details according to xsrc.\nHow to integrate the information from multiple\nencoders is critical. Wang et al. (2019b) choose to\nuse a linear layer to combine the encoder outputs\nand feed the result into the decoder. The input\nsequences in their work share the same length, and\nthe tokens at the same position are corresponding to\neach other, e.g., one input sequence is the sentence\nand another is its part of speech (POS) tagging. Our\ninputs, however, have various lengths, making the\nsimple integration with linear layer infeasible.\nInspired by the multi-head attention mechanism\n(Vaswani et al., 2017), we propose a multi-encoder\nattention mechanism, which extends the concept\nof multi-head attention by attaching different at-\ntention heads to different Transformer encoders\n(Figure 2). Suppose we have two Transformer en-\ncoders with encoding output H1 ∈Rm1×dm and\nH2 ∈ Rm2×dm, and the decoder’s former layer\noutput O ∈RmO×dm where m1,m2 and mO are\nsequence lengths, the multi-encoder attention is\ncalculated as follows:\nC = Concat(A(1)\n1 ,...,A (h1)\n1 ,A(1)\n2 ,...,A (h2)\n2 ),\nA(j)\ni = Attn(O·W(j)\nQ,i, Hi ·W(j)\nK,i, Hi ·W(j)\nV,i ),\nwhere W(j)\nQ,i,W(j)\nK,i ∈Rdm×dk , W(j)\nV,i ∈Rdm×dv ;\ndm, dk and dv are the vector dimensions; h1 and\nh2 are the number of decoder heads attached to\ndifferent encoders. A(j)\ni ∈RmO×dv is the result\nof the jth attention head connected to encoder i,\ncalculated in the same way as Vaswani et al. (2017):\nAttn(Q,K,V ) = Softmax(Q·KT\n√dk\n) ·V.\nAs each matrix A(j)\ni has the same dimension,\nthe multi-encoder attention can easily integrate\nencoder outputs with different sequence lengths\nthrough concatenation. At last, a linear layer is\nused to fuse the information:\nAttentionMultiEnc = C·WO,\nwith projection matrix WO ∈R(h1+h2)dv×dm. In\nthis way, multiple Transformer encoders can be\nattended by a Transformer decoder, even if their\nencoded sequences have different lengths.\nTraining The last decoder block is followed by\ntwo classiﬁcation modules to make two predictions\nat step i−1: the parse node ˆp(i)\ntgt and the level token\nˆl(i)\ntgt. Given their probabilities ˆy(i)\np ,ˆy(i)\nl and the one-\nhot encoded ground truth y(i)\np ,y(i)\nl , The step loss\nis the weighted sum of two NLL losses:\nloss(i−1)\nsyn = −α⟨log ˆy(i)\np ,y(i)\np ⟩−β⟨log ˆy(i)\nl ,y(i)\nl ⟩,\nwhere ⟨·,·⟩is the inner product; αand β are loss\nweights, which are both set to 0.5 in our work. The\ntraining objective is minimizing the sequence loss,\ni.e. the sum of all step losses.\nS VP NNS S NP JJ \nn2✕ \nTransformer\nEncoder\nBlock\nxtgt ssrc\nLinear & Norm\nAdd & Norm\nLinear&Softmax\nstgt (shifted right)\nMulti-Head\nAttention\nText Probabilities\nAdd & Norm\nMulti-Encoder\nAttention\nn1✕ n3✕ \nS \nNP VP \nDT JJ NNS \n1\n2\n3\nPath Attention\nAdd & Norm\nPath Attention\nAdd & Norm\nLinear & Norm\nS NP DT \nS NP VP DT JJ NNS \nMulti-Head\nAttentionMulti-Head\nAttentionMulti-Head\nAttention\nAverage\nLinearize (levels omitted)\nDuplicate and Mask\nOutput\nPath\nAttention\nGuided Text Generator\nNetwork Architecture\nS \nNP \nDT \nS \nNP \nJJ \nS \nVP \nNNS \nPaths\n(levels omitted)\nSyntax\nEncoder\nText\nEncoder\nFigure 3: The left ﬁgure describes the path attention strategy, and the right ﬁgure illustrates the guided generator’s\nnetwork architecture. The blanks squares are the masked constituency parse nodes.\n3.2 Guided Text Generation\nThe goal of the text generator is to generate the\ntarget text stgt, which is syntactically aligned with\nthe syntactic guidance ˆxtgt and meanwhile utilizes\nthe semantics of the source text ssrc. Similar to\nthe syntax expander, we also use two Transformer\nencoders—a syntax encoder and a text encoder—to\nencode syntax sequence and text sequence sepa-\nrately and a Transformer decoder with the multi-\nencoder attention mechanism for the generation\n(see Figure 3). However, as the syntax and text rep-\nresentations belong to different spaces, the situation\nbecomes tricky: when the provided syntactic struc-\nture is speciﬁc, chances are particular words are\nmapped onto the leaf nodes, resulting in a model\noverﬁtted to the surface names of syntactic tokens.\nPath Attention To address the above issue, we\npropose a path attention strategy that forces the\nnetwork to focus more on general syntactic guid-\nance in higher-level part of the constituency tree. 1\nA path is a route in a tree from the root node to\na leaf node (see Figure 3). Say O ∈RmO×dm is\nthe former layer’s output, withmO as the sequence\nlength and dm as the model dimension. First, it is\nduplicated by np times (np is the total number of\npaths), forming a set {O(i),...,O (np)}in which\neach element corresponds to a path. A mask is\napplied to each element to mask out (set to −∞)\nthose nodes not in the path. Then, each masked\nelement O(i)\nM ,i ∈[1,np] is separately fed into the\nsame self-attention network:\nCi = Concat(A(1)\ni ,...,A (h)\ni ) ·WO,\nA(j)\ni = Attn(O(i)\nM ·W(j)\nQ ,O(i)\nM ·W(j)\nK ,O(i)\nM ·W(j)\nV ),\n1We substitute self-attention layers in the syntax encoder\nwith our path attention layers.\nwhere W(∗)\n∗ are learnable weights; his the number\nof attention heads andj ∈[1,h]. At last, the results\nare averaged to form the path attention output:\nAttentionPath = 1\nnp\nnp∑\ni=1\nCi.\nIntuitively, the self-attention mechanism updates\neach token embedding with a weighted sum of all\nembeddings in the sequence. With path attention,\nhowever, one node can only exchange information\nwith other nodes that share the same path. To ac-\nquire information from a node outside its path, one\nmust turn to their common ancestor, who is able\nto get the desired information from former path\nattention layers, forcing the ancestors (higher-level\nguidance) to be more heavily attended to than the\ndescendants. The path attention is executed twice\nin each block so that the information carried by\neach node ﬂows around the entire sequence.\nThe reason we do not include the path attention\nstrategy in the syntax expander is that the input and\noutput of that model are both linearized parse trees.\nUsing path attention in the encoder would create\na mismatch between the encoding and decoding\nprocess that harms model performance.\nTraining The guided generator is trained by min-\nimizing the NLL loss between the probability ˆy(i)\ns\nand the one-hot encoded ground truth word y(i)\ns :\nlosstxt = −\nM∑\ni=1\n⟨log ˆy(i)\ns ,y(i)\ns ⟩,\nwhere M is the sequence length. During inference,\nthe syntax guidance of the text generator can be\neither the full-ﬂedged target parse tree xtgt or the\noutput of the syntax expander ˆxtgt.\nModel BLEU ↑ ROUGE-1 ↑ ROUGE-2 ↑ ROUGE-L ↑ METEOR ↑ TED-f ↓ TED-8 ↓\nVGV AE 13.6 44.7 21.0 48.3 24.8 6.7 -\nSCPN 23.23 53.21 31.05 57.22 51.91 6.55 6.21\nTransformer 46.00 73.32 54.45 75.47 73.50 6.33 5.95\nw/o syn 15.96 50.11 23.83 49.68 46.84 11.88 11.44\nw/o txt 13.41 39.74 20.62 44.72 37.40 6.35 5.89\nw/o PA 38.91 68.01 47.78 70.67 67.26 6.36 5.88\nGuiG.TG 48.03 74.53 56.05 76.65 75.02 6.23 5.89\nTable 1: Text generation results with the ground truth target parse xtgt as syntactic guidance. “TG” represents\nthe text generator. “Transformer” is introduced in § 4.1. “w/o syn” is a Transformer without syntactic constraint\nwhereas “w/o txt” has no source text input. “w/o PA” is the GuiG.TG without path attention strategy applied to the\nsyntax encoder. The arrows show the direction where better performance is.\n4 Experiments\n4.1 Setup\nTask and Data PreperationWe evaluate GuiG\non the paraphrase generation task. Following Iyyer\net al. (2018), we ﬁrst evaluate the guided text gen-\nerator’s ability to follow the syntactic guidance by\npredicting paraphrases with the source textssrc and\nthe target parse xtgt. Then, we assess the perfor-\nmance of our syntax expander by predicting text\nusing the constituency tree ˆxtgt expanded from the\ntemplate parse xtmpl.\nOur dataset is a subset of ParaNMT-50M (Wi-\neting and Gimpel, 2018) provided by Chen et al.\n(2019a). In our work, the number of total text sub-\nword tokens is 16,000. The constituency parsing\ntool is provided by AllenNLP (Gardner et al., 2018).\nThe sentences whose parse trees contain infrequent\ntokens are excluded, and all trees are truncated to8-\nlevel for simplicity, leaving 74 parse nodes and 12\nlevel tokens in total. The dataset is standardized by\nremoving all paraphrase pairs whose text or syntax\nsequences are longer than 50 or with non-ASCII\ncharacters. After the pre-processing, 447,536 para-\nphrase pairs remain in the dataset, in which 90%\nare randomly selected for training and the rest for\nvalidation. Independent from them, 500 and 800\nhigh-quality paraphrase pairs manually annotated\nby Chen et al. (2019a) are used for model develop-\nment and evaluation. The training details of GuiG\nare described in the Appendix.\nBaselines We include three baselines:\n• SCPN (Iyyer et al., 2018) is based on LSTM,\nattention and copy mechanism (See et al.,\n2017). It is trained and evaluated on the same\ndataset as ours, except that their parse tree is\nlinearized to the bracketed format. The net-\nwork hyper-parameters are set to default.\n• VGV AE (Chen et al., 2019a) uses reference\nsentences as the syntactic constraint instead\nof constituency trees.\n• A standard Transformer (Vaswani et al., 2017)\nfor syntax-guided text generation. We con-\ncatenate the input syntax guidance with the\nsource text and feed the connected sequence\ninto the model to generate target text.\nIn addition to the above baselines, we also include\nablations of our model to study the effectiveness of\ndifferent components in GuiG.\n4.2 Quantitative Evaluation\nTo evaluate the performance of different methods,\nwe use three metrics for semantic congruity and one\nfor syntactic similarity. The semantic metrics are:\n1) BLEU (Papineni et al., 2002); 2) ROUGE (Lin,\n2004), including ROUGE-1, ROUGE-2, ROUGE-\nL; and 3) METEOR (Banerjee and Lavie, 2005) 2.\nTo assess syntactic alignment, we calculate the tree\nedit distance (TED) 3 between the generated and\ntarget sentences’ constituency parse trees. It mea-\nsures the number of insertion, rotation and removal\noperations needed for changing one tree to another.\nText Generation with Target ParseWhen the\nfull constituency trees of target sentences xtgt are\ngiven as syntactic guidance, Table 1 shows that\nour generator has better semantic and syntactic per-\nformance than SCPN and VGV AE by doubling\ntheir BLEU scores as well as presenting a smaller\nTED. Comparing our generator with the standard\nTransformer, we ﬁnd that encoding different infor-\nmation separately is a better way than mixing them\ntogether in the same encoder.\n2BLEU & METEOR: https://www.nltk.org/;\nROUGE: https://github.com/Diego999/\npy-rouge.\n3https://github.com/JoaoFelipe/apted\nSE TG BLEU ↑ ROUGE-1 ↑ ROUGE-2 ↑ ROUGE-L ↑ METEOR ↑ N-TED-f ↓\nSCPN SCPN 11.83 41.83 20.36 45.63 38.59 0.5074\nSCPN GuiG.TG 19.52 55.14 31.11 57.89 51.69 0.5125\nw/ PA GuiG.TG 19.47 53.06 29.39 55.87 50.81 0.4946\nw/o tmpl GuiG.TG 13.29 45.56 18.62 45.38 42.48 0.6843\nxtmpl GuiG.TG 20.22 56.00 32.73 58.57 50.63 0.6227\nGuiG.SE GuiG.TG 26.27 61.10 37.13 63.04 59.88 0.4732\nTable 2: Synthetic evaluation of the syntax expansion and text generation models. SE and TG are syntax expansion\nand text generation models respectively. “w/o tmpl” uses only source parse to predict target parse.xtmpl indicates\nthat the template parses are directly fed into the text generation model without expansion. “w/ PA” is our syntax\nexpander with path attention applied to its source syntax encoder.\n2\n4\n6\n8\nTED-f TED-8 TED-7 TED-6 TED-5 TED-4 TED-3\nSCPN-SCPN\nSCPN-GuiG.TG \nGuiG.SE-GuiG.TG\n(a) TED (the lower the better).\n0.22\n0.27\n0.32\n0.37\n0.42\n0.47\n0.52\nN-TED-f N-TED-8 N-TED-7 N-TED-6 N-TED-5 N-TED-4 N-TED-3\nSCPN-SCPN\nSCPN-GuiG.TG\nGuiG.SE-GuiG.TG\n(b) Normalized TED (the lower the better).\nFigure 4: Original and normalized tree edit distances\nbetween target and generated sentences.\nThe table also shows that both source text and\nsyntactic guidance are indispensable. Paraphras-\ning with only source text gives fair semantics, but\ncompletely fails to control the syntactic structure;\nwhereas text generated without source text, as one\nmay predict, has fair syntactic structure but poor\nsemantics. The last two rows indicate that path\nattention signiﬁcantly contributes to the semantic\nexpression without losing the syntactic integrity.\nThe results support our claim that it encourages the\nmodel to attend to higher-level guidance and learn\nthe real syntactic structure instead of a parse-to-\nword mapping.\nText Generation with Expanded ParseTable 2\nand Figure 4 present the performance of text gen-\nerators when their syntax guidance comes from\nSCPN’s syntax expansion model. With the same\nexpanded parse, our text generator again demon-\nstrates better semantic results and similar syntactic\nresults. This proves our text generator’s superiority\nis independent of the source of syntactic guidance.\nSyntax Expansion Since our ultimate goal is to\ngenerate text, we indirectly evaluate the syntax ex-\npander through the assessment of the text generated\nunder the guidance of the expanded parse ˆxtgt. To\nmake it fair, here we uniformly use our text gen-\nerator to generate sentences with ˆxtgt expanded\nfrom different syntax expanders. Also, the max-\nimum syntax sequence length of SCPN is set to\n150 as their linearization method takes 3 times the\nlength as ours. In addition, TED unfairly favors\nshorter generated sentences. Therefore we report\nits normalized version—N-TED, i.e., TED divided\nby the number of nodes in a tree, when the ex-\npanded syntax sequences are of different lengths.\nSpeciﬁcally, we report N-TED-ℓand “N-TED-f” to\ngive a full description of how the consistency trees\nof generated sentences aligned to the target syntax\nat different levels. ℓindicates how many levels of\nparse trees are kept when we compare the syntax\nof the generated and target sentences, and “f” (full)\nmeans the parse trees are intact.\nTable 2 mainly presents the semantic results\nwhereas Figure 4 illustrates the detailed syntac-\ntic performance. Comparing our syntax expander\nto SCPN’s, one can see that our model is more ca-\npable of predicting reasonable syntactic structures,\nwith which the text generator can generate sen-\ntences more semantically analogous to the targets.\nAlthough Figure 4a implies that SCPN’s syntax\nexpansion model produces better syntactic results,\nit is because our syntax expander predicts larger\nparse trees. When the trees are larger, the text gen-\nerator is prone to output longer sentences, biasing\nthe evaluation against our model. Removing the\ninterference of length, our syntax expander gives\nbetter scores when the trimmed syntax trees are\ndeeper than 4 levels, as shown in Figure 4b. Ben-\neﬁtted from the copy mechanism, SCPN is more\nsyntax\ninput\nMODEL SCORES\nSE TG Semantic Syntactic\nxtgt\n- SCPN 2.93 4.00\n- GuiG.TG 4.21 4.67\nxtmpl\nSCPN SCPN 2.71 3.48\nSCPN GuiG.TG 3.52 3.88\nGuiG.SE GuiG.TG 3.84 4.10\nTable 3: Human evaluation scores. xtgt is the case\nwhere we use target parsesxtgt as generator’s syntactic\nguidance without expansion model. xtmpl is the case\nwhere the generator takes the expanded parse.\ncapable of maintaining the template parse but dis-\nadvantaged in giving convincing predictions.\nThe results in the table also testify the disadvan-\ntage of using path attention in the syntax expander,\nas discussed in § 3.2. Expanding the template parse\ntree xtmpl without the source parse tree xsrc is im-\npractical since the result would lose the ability to\nﬁt the source syntax; and generating target parse\nsolely based on the source parse fails to properly\ncontrol the syntactic structure due to the absence\nof template parse. If we guide the text generator\ndirectly with the template parse xtmpl, the model is\nnot able to acquire adequate syntactic information\nand presents poor results.\n4.3 Human Evaluation\nWe perform a crowdsourced evaluation of the se-\nmantics and syntax of the generated sentences. 200\nexamples are randomly selected. Each of them is\nevaluated by three workers in the way of scoring\nthe semantic and syntactic similarities between the\ngenerated and target sentences from 1 to 5, the\nhigher the better.\nThe results presented in Table 3 are largely paral-\nlel to the objective metrics (§ 4.2). Compared with\nSCPN, our text generator generates more seman-\ntically reasonable text with the syntactic guidance\ncomes from either the target parsextgt (1st and 2nd\nrows) or the parse ˆxtgt expanded from template\nxtmpl by SCPN (3rd and 4th rows). The 4th and\n5th rows in the table prove that our syntax expander\nalso contributes to the performance improvement.\nAnalyzing the sentences that get low ( ⩽ 2) se-\nmantic or syntactic average scores, we ﬁnd our\ntext generator sometimes suffers from several de-\nfects: 1) the generated sentence is semantically\nopposite to the target, especially when the source\ntext has multi-negation; 2) one word gets repeated\nfor several times; and 3) incomprehensible words\nsrc: you told me to look for the wolf where his prey can\nbe found.\ntgt: you said i would ﬁnd a wolf where i can ﬁnd its prey.\nSCPN: i thought you’d ﬁnd it if the wolf saw the wolf.\nGuiG: you said i should look for a wolf where he could\nﬁnd his prey.\nsrc: wounds on the body may easily be healed, but emo-\ntional scars do not go away so easily.\ntgt: physical injuries will heal, but it is not so easy for\nscars on the soul.\nSCPN: it was a lot of the body, but he ’ll have no way of\nthe body.\nGuiG: the wounds on the body can be healed easily, but\nemotional scars do n’t go so easily.\nsrc: we need to further strengthen the agency’s capacities.\ntgt: the capacity of this ofﬁce needs to be reinforced even\nfurther.\nSCPN: the possibility of the agency is to survive.\nGuiG: the capacity of the agency needs to be further\nstrengthened.\nTable 4: Examples generated with expanded parseˆxtgt.\nare given due to the usage of BPE. These issues are\nuniversal in all text generation models and deserve\nfurther investigation. However, these situations\nare rare, and our method generates ﬂuent and well-\nstructured sentences most of the time.\n4.4 Qualitative Analysis\nText Generation with Expanded ParseTable 4\nshows a few examples generated under the guid-\nance of expanded parses. It can be observed that\nmost of the time the semantic meanings of source\nsentences are well-preserved while the syntactic\nstructures are successfully transferred. However,\nin some cases, the predicted text fails to entirely\nfollow the references’ syntactic structures due to\nthe imperfection of the syntax expansion model.\nIn the second example, the syntactic distinction of\nthe source and reference sentences lies in micro\nrather than macro scale. Consequently, the pre-\ndicted parse copies heavily from the source parse,\nmaking the generated sentence more similar to the\nsource text instead of the target. Nonetheless, com-\npared with SCPN, our model is more capable of\nusing appropriate expression and suffers less from\nthe repeated words issue, leading to more compre-\nhensible and better-structured sentences.\nText Generation with Common TemplatesWe\ntake a step further and demonstrate our model’s\nability to generate sentences from the templates\nthat appear most frequently in the dataset. Table 5\nshows that the sentences generated with the same\ntemplate parses have similar high-level structures.\nPARSE NODE:S NP PRP VP MD VP .\nLEVEL: 1 2 3 2 3 3 2\nsrc: he believed his son had died in a terrorist attack.\ngen: he would believe his son was killed in a terrorist\nattack.\nsrc: she seems to have written a book about driving.\ngen: she must have written a book on the driver.\nsrc: it is hard for me to imagine where they could be\nhiding it underground.\ngen: i’d be difﬁcult to imagine where they ’re hiding him\nunderground .\nPARSE NODE:S NP PRP VP VBZ NP .\nLEVEL: 1 2 3 2 3 3 2\nsrc: there were 50 bucks’ worth of merchandise stolen\nby a fucker today .\ngen: it’s 50 bucks for the kind of thing stolen by a moth-\nerfucker.\nsrc: there’s an intelligent way to approach marriage.\ngen: it is a smart way to approach the wedding.\nsrc: stealing state secrets was one thing he was framed\nfor by frank.\ngen: it’s a part of the theft of state secrets that frank has\nbeen framed.\nTable 5: Generated examples with frequently appeared\ntemplate constituency parse trees.\nMoreover, the semantic analogy between the source\nand generated sentences proves our method’s abil-\nity to successfully keep the semantics during the\nsyntax transfer process.\n5 Related Works\nConstrained text generation has attracted much at-\ntention in recent years. Categorized by the object\nto be controlled, there are two tracks of works: one\nseeks to manipulate the semantic attributes (Hu\net al., 2017; Li et al., 2018b,a; Yin et al., 2019;\nWang et al., 2019a). For example, Hu et al. (2017)\ngenerate text with speciﬁed sentiments, whereas Li\net al. (2018b) and Wang et al. (2019a) try to trans-\nfer the sentiments or styles of the source sentences.\nThe other track, to which our research belongs, fo-\ncuses on making generated text follow a particular\nstyle or structure (Niu et al., 2017; Ficler and Gold-\nberg, 2017; Fu et al., 2018; Liu et al., 2018; Iyyer\net al., 2018; Chen et al., 2019a; Li et al., 2019b;\nBalasubramanian et al., 2020). For instance, Niu\net al. (2017) constrain the output styles in neural\nmachine translation task and Liu et al. (2018) im-\npose length limitation to the summarization.\nBased on the constraint source, syntactically con-\ntrolled text generation models can be further di-\nvided into three groups. The ﬁrst group (Chen\net al., 2019b; Bao et al., 2019; Balasubramanian\net al., 2020) takes sentences as syntactic exemplars.\nThey attempt to disentangle the semantic and syn-\ntactic representations into different V AE (Kingma\nand Welling, 2014) latent spaces during training,\nand then use the exemplar to assign a prior distri-\nbution to the syntactic latent space at the inference\nstage. The second group (Iyyer et al., 2018; Zhang\net al., 2019) directly employs the constituency tree\nas an auxiliary input, controlling the syntax of gen-\nerated text with the structure speciﬁed by it. Instead\nof importing externally, the third group (Wiseman\net al., 2018; Akoury et al., 2019; Casas et al., 2020)\nlearns the syntax guidance from the training data\nand apply it in the generation phrase in return.\nConsidering that the fully speciﬁed exemplar\nsentences are hard to be effectively retrieved (Goyal\nand Durrett, 2020), we follow Iyyer et al. (2018)\nand use constituency trees as the syntax guidance.\nWe further take advantage of the parallel attribute\nof Transformer (Vaswani et al., 2017) to accom-\nmodate the tree structure in the encoding process.\nThere are works (Eriguchi et al., 2016; Chen et al.,\n2017; Ding and Tao, 2019) that adapt the recurrent\nencoder to the trees, but the transition matrix that\nRNNs depend on is less effective than our attention\nmechanism, especially when the tree is large.\n6 Conclusion\nWe have proposed a novel syntactically guided text\ngeneration method GuiG. 4 It expands the template\nconstituency parse tree to a full-ﬂedged tree, using\nit as the syntactic constraint to guide the text gen-\neration process. A syntax expander based on the\nmulti-encoder Transformer is designed to predict a\nconvincing target parse tailored for the source text,\nand a guided text generator powered by path atten-\ntion strategy is introduced to generate text that has\nthe semantics speciﬁed by the source text as well as\ncomplies with the syntactic guidance. Evaluated on\nthe paraphrasing task, ablation studies justify the\nnecessity of the components of our method, while\nquantitative and qualitative experiments demon-\nstrate our method’s ability to generates more se-\nmantically reasonable and syntactically aligned\nsentences than SOTA baselines. We believe our\nmethod can play an important role in style transfer\nand text data augmentation applications.\n4The code and data are available at https://github.\ncom/Yinghao-Li/GuiGen.\nReferences\nNader Akoury, Kalpesh Krishna, and Mohit Iyyer.\n2019. Syntactically supervised transformers for\nfaster neural machine translation. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1269–1281, Florence,\nItaly. Association for Computational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. Cite arxiv:1409.0473\nComment: Accepted at ICLR 2015 as oral presenta-\ntion.\nVikash Balasubramanian, Ivan Kobyzev, Hareesh\nBahuleyan, Ilya Shapiro, and Olga Vechtomova.\n2020. Polarized-vae: Proximity based disentangled\nrepresentation learning for text generation.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization , pages 65–72, Ann Ar-\nbor, Michigan. Association for Computational Lin-\nguistics.\nYu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou,\nOlga Vechtomova, Xin-yu Dai, and Jiajun Chen.\n2019. Generating sentences from disentangled syn-\ntactic and semantic spaces. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6008–6019, Florence,\nItaly. Association for Computational Linguistics.\nNoe Casas, Jos ´e A. R. Fonollosa, and Marta R. Costa-\njuss`a. 2020. Syntax-driven iterative expansion lan-\nguage models for controllable text generation.\nHuadong Chen, Shujian Huang, David Chiang, and Ji-\najun Chen. 2017. Improved neural machine trans-\nlation with a syntax-aware encoder and decoder.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1936–1945, Vancouver,\nCanada. Association for Computational Linguistics.\nMingda Chen, Qingming Tang, Sam Wiseman, and\nKevin Gimpel. 2019a. Controllable paraphrase gen-\neration with a syntactic exemplar. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5972–5984, Florence,\nItaly. Association for Computational Linguistics.\nMingda Chen, Qingming Tang, Sam Wiseman, and\nKevin Gimpel. 2019b. A multi-task approach for\ndisentangling syntax and semantics in sentence rep-\nresentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 2453–2464, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLiang Ding and Dacheng Tao. 2019. Recurrent graph\nsyntax encoder for neural machine translation.\nAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa\nTsuruoka. 2016. Tree-to-sequence attentional neu-\nral machine translation. In Proceedings of the 54th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , pages\n823–833, Berlin, Germany. Association for Compu-\ntational Linguistics.\nJessica Ficler and Yoav Goldberg. 2017. Controlling\nlinguistic style aspects in neural language genera-\ntion. In Proceedings of the Workshop on Stylis-\ntic Variation, pages 94–104, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Explo-\nration and evaluation. In Thirty-Second AAAI Con-\nference on Artiﬁcial Intelligence.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A deep semantic natural language pro-\ncessing platform. In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS) , pages 1–\n6, Melbourne, Australia. Association for Computa-\ntional Linguistics.\nTanya Goyal and Greg Durrett. 2020. Neural syntactic\npreordering for controlled paraphrase generation.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Comput. , 9(8):1735–\n1780.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P Xing. 2017. Toward\ncontrolled generation of text. In Proceedings\nof the 34th International Conference on Machine\nLearning-Volume 70, pages 1587–1596. JMLR. org.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018. Adversarial example generation\nwith syntactically controlled paraphrase networks.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1875–1885, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nDiederik P. Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. In 2nd International\nConference on Learning Representations, ICLR\n2014, Banff, AB, Canada, April 14-16, 2014, Con-\nference Track Proceedings.\nChenliang Li, Weiran Xu, Si Li, and Sheng Gao. 2018a.\nGuiding generation for abstractive text summariza-\ntion based on key information guide network. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 55–60, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018b.\nDelete, retrieve, generate: a simple approach to sen-\ntiment and style transfer. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) ,\npages 1865–1874, New Orleans, Louisiana. Associ-\nation for Computational Linguistics.\nJuntao Li, Lisong Qiu, Bo Tang, Dongmin Chen,\nDongyan Zhao, and Rui Yan. 2019a. Insufﬁcient\ndata can also rock! learning to converse using\nsmaller data with augmentation. In The Thirty-Third\nAAAI Conference on Artiﬁcial Intelligence, AAAI\n2019, The Thirty-First Innovative Applications of\nArtiﬁcial Intelligence Conference, IAAI 2019, The\nNinth AAAI Symposium on Educational Advances\nin Artiﬁcial Intelligence, EAAI 2019, Honolulu,\nHawaii, USA, January 27 - February 1, 2019, pages\n6698–6705. AAAI Press.\nZichao Li, Xin Jiang, Lifeng Shang, and Qun Liu.\n2019b. Decomposable neural paraphrase genera-\ntion. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 3403–3414, Florence, Italy. Association for\nComputational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYizhu Liu, Zhiyi Luo, and Kenny Zhu. 2018. Con-\ntrolling length in abstractive summarization using\na convolutional neural network. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4110–4119, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nZhiqiang Liu, Zuohui Fu, Jie Cao, Gerard de Melo,\nYik-Cheung Tam, Cheng Niu, and Jie Zhou. 2019.\nRhetorically controlled encoder-decoder for modern\nChinese poetry generation. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1992–2001, Florence,\nItaly. Association for Computational Linguistics.\nXing Niu, Marianna Martindale, and Marine Carpuat.\n2017. A study of style in machine translation: Con-\ntrolling the formality of machine translation output.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2814–2819, Copenhagen, Denmark. Association for\nComputational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting on Association for Com-\nputational Linguistics , ACL ’02, pages 311–318,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nKe Wang, Hang Hua, and Xiaojun Wan. 2019a. Con-\ntrollable unsupervised text attribute transfer via edit-\ning entangled latent representation. In Advances\nin Neural Information Processing Systems , pages\n11034–11044. Curran Associates, Inc.\nSu Wang, Rahul Gupta, Nancy Chang, and Jason\nBaldridge. 2019b. A task in a suit and a tie: Para-\nphrase generation with semantic augmentation. Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, 33:7176–7183.\nJohn Wieting and Kevin Gimpel. 2018. ParaNMT-\n50M: Pushing the limits of paraphrastic sentence em-\nbeddings with millions of machine translations. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 451–462, Melbourne, Australia.\nAssociation for Computational Linguistics.\nSam Wiseman, Stuart Shieber, and Alexander Rush.\n2018. Learning neural templates for text genera-\ntion. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3174–3187, Brussels, Belgium. Association\nfor Computational Linguistics.\nZe Yang, Wei Wu, Jian Yang, Can Xu, and Zhoujun\nLi. 2019. Low-resource response generation with\ntemplate prior. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1886–1897, Hong Kong, China. As-\nsociation for Computational Linguistics.\nDi Yin, Shujian Huang, Xin-Yu Dai, and Jiajun Chen.\n2019. Utilizing non-parallel text for style transfer\nby making partial comparisons. In Proceedings of\nthe Twenty-Eighth International Joint Conference on\nArtiﬁcial Intelligence, IJCAI-19 , pages 5379–5386.\nInternational Joint Conferences on Artiﬁcial Intelli-\ngence Organization.\nXinyuan Zhang, Yi Yang, Siyang Yuan, Dinghan Shen,\nand Lawrence Carin. 2019. Syntax-infused varia-\ntional autoencoder for text generation. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 2069–2078,\nFlorence, Italy. Association for Computational Lin-\nguistics.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7450043559074402
    },
    {
      "name": "Computer science",
      "score": 0.6083491444587708
    },
    {
      "name": "Natural language processing",
      "score": 0.5738950967788696
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5002381801605225
    },
    {
      "name": "Engineering",
      "score": 0.1151818335056305
    },
    {
      "name": "Electrical engineering",
      "score": 0.10279512405395508
    },
    {
      "name": "Voltage",
      "score": 0.05521753430366516
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 10
}