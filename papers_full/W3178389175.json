{
  "title": "Feature Fusion Vision Transformer for Fine-Grained Visual Categorization",
  "url": "https://openalex.org/W3178389175",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1940314070",
      "name": "Wang Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2202495791",
      "name": "Yu, Xiaohan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2171928974",
      "name": "Gao Yongsheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2895643041",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2997426000",
    "https://openalex.org/W1797268635",
    "https://openalex.org/W3124951096",
    "https://openalex.org/W2998619563",
    "https://openalex.org/W3128449120",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2998345525",
    "https://openalex.org/W1980526845",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W3128099838",
    "https://openalex.org/W1928906481",
    "https://openalex.org/W3139434170",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2951852399",
    "https://openalex.org/W3108870912",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2142697503",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2986456235",
    "https://openalex.org/W2604134068",
    "https://openalex.org/W3035367622",
    "https://openalex.org/W2963393555",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2737701595",
    "https://openalex.org/W2986821660",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2202499615",
    "https://openalex.org/W2981954115",
    "https://openalex.org/W2962761264",
    "https://openalex.org/W2961018736",
    "https://openalex.org/W2891951760",
    "https://openalex.org/W2798365843",
    "https://openalex.org/W2990495699",
    "https://openalex.org/W2997737174"
  ],
  "abstract": "The core for tackling the fine-grained visual categorization (FGVC) is to learn subtle yet discriminative features. Most previous works achieve this by explicitly selecting the discriminative parts or integrating the attention mechanism via CNN-based approaches.However, these methods enhance the computational complexity and make the modeldominated by the regions containing the most of the objects. Recently, vision trans-former (ViT) has achieved SOTA performance on general image recognition tasks. Theself-attention mechanism aggregates and weights the information from all patches to the classification token, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation token in the deep layer pays more attention to the global information, lacking the local and low-level features that are essential for FGVC. In this work, we proposea novel pure transformer-based framework Feature Fusion Vision Transformer (FFVT)where we aggregate the important tokens from each transformer layer to compensate thelocal, low-level and middle-level information. We design a novel token selection mod-ule called mutual attention weight selection (MAWS) to guide the network effectively and efficiently towards selecting discriminative tokens without introducing extra param-eters. We verify the effectiveness of FFVT on three benchmarks where FFVT achieves the state-of-the-art performance.",
  "full_text": ": 1\nFeature Fusion Vision Transformer for\nFine-Grained Visual Categorization\nJun Wang\njun.wang.3@warwick.ac.uk\nUniversity of Warwick, UK\nXiaohan Yu\nxiaohan.yu@grifﬁth.edu.au\nGrifﬁth University, Australia\nY ongsheng Gao\nyongsheng.gao@grifﬁth.edu.au\nGrifﬁth University, Australia\nAbstract\nThe core for tackling the ﬁne-grained visual categorization (FGVC) is to learn subtle\nyet discriminative features. Most previous works achieve this by explicitly selecting the\ndiscriminative parts or integrating the attention mechanism via CNN-based approaches.\nHowever, these methods enhance the computational complexity and make the model\ndominated by the regions containing the most of the objects. Recently, vision trans-\nformer (ViT) has achieved SOTA performance on general image recognition tasks. The\nself-attention mechanism aggregates and weights the information from all patches to the\nclassiﬁcation token, making it perfectly suitable for FGVC. Nonetheless, the classiﬁ-\ncation token in the deep layer pays more attention to the global information, lacking\nthe local and low-level features that are essential for FGVC. In this work, we propose\na novel pure transformer-based framework Feature Fusion Vision Transformer (FFVT)\nwhere we aggregate the important tokens from each transformer layer to compensate the\nlocal, low-level and middle-level information. We design a novel token selection mod-\nule called mutual attention weight selection (MAWS) to guide the network effectively\nand efﬁciently towards selecting discriminative tokens without introducing extra param-\neters. We verify the effectiveness of FFVT on four benchmarks where FFVT achieves\nthe state-of-the-art performance. Code is available at this link.\n1 Introduction\nFine-grained visual categorization (FGVC) aims to solve the problem of differentiating sub-\nordinate categories under the same basic-level category, e.g., birds, cars and plants. FGVC\nhas wide real-world applications, such as autonomous driving and intelligent agriculture.\nSome FGVC tasks are exceedingly hard for human beings due to the small inter-class vari-\nance and large intra-class variance, e.g., recognizing 200 subordinate plant leaves and 200\nsubordinate birds. Therefore, FGVC is an important and highly challenging task.\nOwing to the decent designed networks and large-scale annotated datasets, FGVC has\ngained steady improvements in recent years. Current methods on FGVC can be roughly\ndivided into localization-based methods and attention-based methods. The core for solving\nFGVC is to learn the discriminative features in images. Early localization-based methods\n© 2021. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\narXiv:2107.02341v3  [cs.CV]  28 Feb 2022\n2 :\n[1, 16, 41] achieve this by directly annotating the discriminative parts in images. However, it\nis costly and time-consuming to build bounding box annotations, hindering the applicability\nof these methods on real-world applications. To alleviate this problem, recent localization-\nbased methods normally integrate the region proposal network (RPN) to obtain the potential\ndiscriminative bounding boxes. These selected proposals are then fed into the backbone net-\nwork to gain the local features. After that, most methods often adopt a rank loss [2] on the\nclassiﬁcation outputs for all local features. However, [14] argues that RPN-based methods\nignore the relationships among selected regions. Another problem is that this mechanism\ndrives the RPN to propose large bounding boxes as they are more likely to contain the fore-\nground objects. Confusion occurs when these bounding boxes are inaccurate and cover the\nbackground rather than objects. Besides, some discriminative regions, e.g., leaf vein in plant\nleaves, cannot be simply annotated by a rectangular [35].\nAttention-based [40, 50, 54] methods automatically detect the discriminative regions in\nimages via self-attention mechanism. These methods release the reliance on manually an-\nnotation for discriminative regions and have gained encouraging results. Recently, vision\ntransformer has demonstrated potential performance on general image classiﬁcation [6], im-\nage retrieval [9] and semantic segmentation [54]. This great success shows that the innate\nattention mechanism of a pure transformer architecture can automatically search the impor-\ntant parts in images that contribute to image recognition. However, few study investigate the\nperformance of vision transformer in FGVC. As the ﬁrst work to study the vision transformer\non FGVC, [14] proposed to replace the inputs of the ﬁnal transformer layer with some im-\nportant tokens and gained improved results. Nonetheless, the ﬁnal class token may concern\nmore on global information and pay less attention to local and low-level features, defecting\nthe performance of vision transformer on FGVC since local information plays an important\nrole in FGVC. Besides, previous works focus on FGVC benchmarks containing more than\nten thousands of annotated images, and no study explores the capability of vision transformer\non small-scale and ultra-ﬁne-grained visual categorization (ultra-FGVC) settings.\nIn this paper, we propose a novel feature fusion vision transformer (FFVT) for FGVC.\nFFVT aggregates the local information from low-level, middle-level and high-level tokens\nto facilitate the classiﬁcation. We present a novel important token selection approach called\nMutual Attention Weight Selection (MAWS) to select the representative tokens on each layer\nthat are added as the inputs of the last transformer layer. In addition, we explore the perfor-\nmance of our method on four FGVC datasets to comprehensively verify the capability of our\nproposed FFVT on FGVC. In conclusion, our work has four main contributions.\n1. To our best knowledge, we are the ﬁrst study to explore the performance of vision\ntransformer on both small-scale and ultra-FGVC settings. The two small-scale datasets in\nthis paper are highly challenging due to the ultra-ﬁne-grained inter-category variances and\nfew training data available. Some examples are visualized in Figure 1.\n2. We propose FFVT, a novel vision transformer framework for ﬁne-grained visual cat-\negorization tasks that can automatically detect the distinguished regions and take advantage\nof different level of global and local information in images.\n3. We present a novel important token selection approach called Mutual Attention Weight\nSelection (MAWS). MAWS can effectively select the informative tokens that are having high\nsimilarity to class token both in the contexts of the class token and the token itself without\nintroducing extra parameters.\n4. We verify the effectiveness of our method on four ﬁne-grained benchmarks. Ex-\nperimental results demonstrate that FFVT achieves state-of-the-art performance on them,\noffering an alternative to current CNN-based approaches. Ablation studies show that our\n: 3\nproposed method boost the performance of the backbone model by 5.42%, 4.67% and 0.80%\non CottonCultivar80, SoyCultivarLocal and CUB datasets, respectively.\nFigure 1: Examples of images in SoyCultivarLocal and Cotton datasets. Images in the ﬁrst\nrow come from four species of Soy.Loc, while examples in the second row are selected from\nfour categorizes of Cotton.\n2 Related Works\n2.1 Fine-Grained Visual Categorization\nMethods on FGVC can be coarsely divided into two groups: localization-based methods\nand attention-based methods. Similar to object detection task, localization-based methods\noften detect the foreground objects and perform classiﬁcation based on them. Early works\n[1, 16, 41] achieve this by taking advantage of part annotation to supervise the learning of the\ndetection branch. However, bounding box annotation requires large manual labor, hampering\ntheir real-world applications.\nTo alleviate above problem, recent localization-based methods introduce the weakly su-\npervised object detection (WSOD) technique to predict the potential discriminative regions\nwith only image-level label. Ge et al. [13] used WSOD and instance segmentation tech-\nniques to obtain the rough object instances, and then selected the important instances to\nperform classiﬁcation. He et al. [15] presented two spatial constraints to select the discrimi-\nnative parts obtained by the detection branch. Wang et al. [38] utilized correlations between\nregions to select distinguished parts. However, these methods require a well designed WSOD\nbranch to propose potential discriminative regions. Moreover, the selected parts sent to the\nclassiﬁcation head often cover the whole object instead of the truly discriminative parts.\nAlternatively, attention-based methods automatically localize the discriminative regions\nvia self-attention mechanism without extra annotations. Zhao et al. [50] proposed a diversi-\nﬁed visual attention network which uses the diversity of the attention to collect dicriminative\ninformation. Xiao et al. [40] presented a two-level attention mechanism to steadily ﬁlter\nout the trivial parts. Similar to [40], Zheng et al. [54] proposed a progressive-attention to\nprogressively detect discriminative parts at multiple scales. However, these methods often\nsuffer from huge computational cost.\n2.2 Transformer\nTransformer has achieved huge success in natural language processing [4, 30, 31]. Motivated\nby this, researchers try to exploit the transformers in computer vision. Recent work ViT [6]\n4 :\nachieves the state-of-the-art performance on image classiﬁcation by employing a pure trans-\nformer architecture on a number of ﬁx-sized image patches. Later, researchers explore the\nperformance of the pure transformer in other computer vision tasks. Zheng [54] et al. devel-\noped a pure transformer SETR on semantic segmentation task. Alaaeldin et al. [9] exploited\na transformer to generate the image descriptor for image retrieval task. Nonetheless, few\nstudies explore the vision transformer on FGVC.\nThe most similar to our work is TransFG [14] which is the ﬁrst study to extend the\nViT into FGVC, while there are two notable differences between FFVT and TransFG. First,\nTransFG selects the discriminative tokens and directly send them to the last transformer layer\n(no feature fusion), while FFVT aims to aggregate the local and different level information\nfrom each layer to enrich the feature representation capability via feature fusion. Second, our\nproposed token selection strategy is totally different from that of TransFG which requires the\nattention information from all transformer layer to generate the selected token indexes via\nmatrix multiplication. In contrast, our proposed MAWS utilize attention information from\nonly one transformer layer to produce the corresponding indexes. Hence, MAWS is simple\nand efﬁcient. Our work is also in accordance with the spirit of recent research [11, 33, 34, 35,\n36, 43, 44, 45, 46, 47, 48, 49, 51, 52], which focuses on localizing subtle yet vital regions.\n3 Methods\nTo better comprehend our method, we ﬁrst brieﬂy review the knowledge of vision trans-\nformer ViT in Section 3.1. Our proposed methods are then elaborately described in the\nfollowing subsections.\n3.1 ViT For Image Recognition\nViT follows the similar architecture of transformer in natural language processing with minor\nmodiﬁcation. Transformer in natural language processing takes a sequence of tokens as\ninputs. Similarly, given an image with resolution H ∗W, vision transformer ﬁrst processes\nthe image into N = ⌊H\nP ⌋∗⌊W\nP ⌋ﬁx-sized patches xp. where P is the size for each patch.\nThe patches xp are then linearly projected into a D-dimensional latent embedding space.\nTo introduce the positional differences, a learnable vector called position embedding with\nthe same size of patch embedding is directly added to patch embedding. Similar to the class\ntoken in BERT [4], an extra class token is added to interact with all patch embeddings and\nundertakes the classiﬁcation task. The procedure is shown in Eq (1):\nzzz0 = [xxxclass; xxx1\npEEE; xxx2\npEEE; ...; xxxN\np EEE]+ EEE pos (1)\nWhere EEE ∈R(P2·C)×D is the patch embedding projection and C is the number of the image\nchannels. EEE pos denotes the position embedding.\nAfter that, these patch embeddings are fed into the transformer encoder containing sev-\neral multi-head self-attention (MSA) and multi-layer perceptron (MLP) blocks. Note that all\nlayers retain the same latent vector size D. The outputs of the l-th layer are calculated by\nEqs (2) to (3):\nzzz\n′\nl = MSA(LN(zzzl−1))+ zzzl−1 (2)\nzzzl = MLP(LN(zzz\n′\nl))+ zzz\n′\nl. (3)\n: 5\nWhere LN(·) is the layer normalization operation and zl denotes the encoded image repre-\nsentation. Eventually, a classiﬁcation head implemented by a MLP block is applied to the\nclass token zzz0\nl to obtain the predicted category.\n3.2 FFVT Architecture\n[14] suggests that the ViT cannot capture enough local information required for FGVC. To\ncope with this problem, we propose to fuse the low-level features and middle-level features\nto enrich the local information. We present a novel token selection approach called mutual\nattention weight selection (MAWS) to determine the tokens to be aggregated in the deep\nlayer. This section introduces the details of our proposed FFVT. The overall architecture of\nFFVT is illustrated in Fig 2.\nLinear Projection of Flattened PatchesPosition EmbeddingTransformer Layer \nAttention Scores\n…x9\nTransformer Layer \nTransformer Layer \nSoftmaxon first row\n0*123456 Attention WeightsAttention Weights\nMAW\nSoftmaxon first column\n⨂\n⨂ Token SelectionMAWSModule\nMAWS Modulex9…\n* Feature Fusion\n*Classification Head\nHadamard product\nFigure 2: The overall architecture of the proposed FFVT. Images are split into a sequence of\nﬁx-sized patches which are then linearly projected into the embedding space. Combined with\nthe position embedding, the patch embeddings are fed into the Transformer Encoder to learn\nthe patch features. Feature fusion is exploited before the last transformer layer to aggregate\nthe important local, low-level and middle level information from previous layers. This is\nimplemented by replacing the inputs (exclude classiﬁcation token) of the last transformer\nlayer with the tokens selected by the MAWS Module.\n3.2.1 Feature Fusion Module\nThe key challenge of the FGVC is to detect the discriminative regions that signiﬁcantly\ncontribute to ﬁguring out the subtle differences among subordinate categories. Previous\nworks often achieve this by manually annotating the discriminative regions or integrating\nthe RPN module. However, these methods suffer from some problems discussed in Section\n1&2, limiting their performance on real-world applications.\nThe MSA mechanism in vision transform can perfectly meet the above requirement,\nwhereas MSA in deep layer is likely to pay more attention to the global information. There-\nfore, we propose a feature fusion module to compensate local information. As shown in\n6 :\nﬁgure 2, given the important tokens (hidden features) from each layer selected by MAWS\nmodule, we replace the inputs (except for the class token) of the last transformer layer with\nour selected tokens. In this way, the class token in the last transformer layer fully interacts\nwith the low-level, middle level and high-level features from the previous layers, enriching\nthe local information and feature representation capability.\nSpeciﬁcally, we denote the tokens selected by MAWS module inl-layer as:\nzzzlocal\nl = [z1\nl , z2\nl , ..., zK\nl ] (4)\nWhere K is the number of selected features. The fused features along with the classiﬁcation\ntoken fed into the last transformer layer L are:\nzzzf f = [zzz0\nL−1; zzzlocal\n1 ; zzzlocal\n2 ;...; zzzlocal\nL−1 ] (5)\nEventually, following the ViT, the classiﬁcation token of the ﬁnal transformer layer is\nsent to the classiﬁcation head to perform categorization. The problem turns to how to select\nthe important and discriminative tokens. To that end, we propose an effective and efﬁcient\ntoken selection approach described in the next section.\n3.2.2 Mutual Attention Weight Selection Module\nSince an image is split into many patches, token selection turns to be an important problem.\nNoise is added when the background patches are frequently selected, while discriminative\npatches can boost the model performance. Hence, we propose a token selection approach\nwhich directly utilizes the attention scores generated by multi-head self-attention module.\nTo be speciﬁc, an attention score matrix for one attention head A ∈R(N+1)×(N+1) is\ndenoted as:\nAAA = [aaa0; aaa1; aaa2; ...; aaai; ...; aaaN ] (6)\naaai = [ai,0, ai,1, ai,2, ..., ai, j, ..., ai,N ] (7)\nWhere ai, j is the attention score between token i and j in the context of token i, i.e., dot-\nproduct between the query of token i and the key of token j.\nOne simple strategy is to pick the tokens having the higher attention scores with the\nclassiﬁcation token as the classiﬁcation token contains rich information for categorization.\nWe can do this by sorting the aaa0 and picking the K tokens with the bigger value. We denote\nthis strategy as single attention weight selection (SAWS). However, SAWS may introduce\nnoisy information since the selected tokens could aggregate much information from noisy\npatches. Taking a three-patch attention score matrix γ shown below as an example:\nγ =\n\n\naaa0\naaa1\naaa2\naaa3\n\n =\n\n\n1 2 3 4\n1 2 3 4\n1 2 3 4\n1 4 1 1\n\n\nToken three is selected as it has the biggest value in the attention score vector for clas-\nsiﬁcation token. However, token three aggregates much information from token one (the\nmaximum attention score in aaa3) thus may introduce noises assuming token one is a noisy\ntoken. To cope with this problem, we develop a mutual attention weight selection module\n: 7\nDatasets Category Training Testing\nCUB-birds 200 5994 5794\nStanford Dogs 196 8144 8041\nSoy.Loc 200 600 600\nCotton 80 240 240\nTable 1: Statistics of CUB-200-2011, SoyCultivarLocal (Soy.Loc), CottonCultivar80 (Cot-\nton) and Stanford Dogs datasets.\nwhich requires the selected tokens to be similar to the classiﬁcation token both in the contexts\nof the classiﬁcation token and the tokens themselves.\nIn particular, we denote the ﬁrst column in the attention score matrix as bbb0. Note that bbb0\nis the attention score vector between classiﬁcation token and other tokens in the context of\nother tokens compared with aaa0 in the context of classiﬁcation token. The mutual attention\nweight mmmaaai between the classiﬁcation token and token i is then calculated by Eqs (8) to (9):\nmmmaaai = a\n′\n0,i ∗b\n′\ni,0 (8)\na\n′\n0,i = ea0,i\n∑N\nj=0 ea0, j\n, b\n′\ni,0 = ebi,0\n∑N\nj=0 ebj,0\n(9)\nFor multi-head self-attention, we ﬁrst average the attention scores of all heads. After ob-\ntaining the mutual attention weight, the indexes of important tokens are collected according\nto the mutual attention values. Our approach does not introduce extra learning parameters.\nIt is simple and efﬁcient compared with the matrix multiplication in [14].\n4 Experiments\n4.1 Datasets\nWe explore the effectiveness of FFVT on two widely used FGVC dataset and two small-\nscale ultra-ﬁne-grained datasets, i.e., CUB-200-2011 [32], Stanford Dogs [18], SoyCultivar-\nLocal [46] and CottonCultivar80 [46]. The SoyCultivarLocal and CottonCultivar80 are two\nhighly challenging datasets as they further reduce the granularity of categorization, e.g. from\nspecies to cultivar, and with few training data available. The statistics of four datasets are\nshown in Table 1.\n4.2 Implementation Details\nThe same as the most current transformer-based approaches, the backbone network (ViT)\nof FFVT is pretrained on the ImageNet21K dataset. Following the same data augmentation\nmethods on most existing works, input images are ﬁrst resized to 500×500 for Soy.Loc and\nCotton datasets, and 600 ×600 for CUB and Stanford Dogs. We then crop the image into\n384 ×384 for Soy.Loc and Cotton, and 448 ×448 for CUB and Stanford Dogs (Random\ncropping in training and center cropping in testing). Random horizontal ﬂipping is adopted\nand an extra color augmentation is applied for CUB. K in Eq (4) is set to 12 for CUB,\nSoy.Loc and Cotton, and 24 for Stanford Dogs.\n8 :\nWe select the SGD optimizer to optimize the network with a momentum of 0.9. The\ninitial learning rate is 0.02 with the cosine annealing scheduler for FFVT on CUB, Soy.Loc\nCotton datasets, and 0.003 on the Stanford Dogs dataset. The batch size is set to 8 for\nall datasets except for the Stanford Dogs with a batch size of 4. For fair comparisons, we\nreimplement the experiments of ViT and TransFG on the Stanford Dogs benchmark with\ntheir default settings and the same batch size as FFVT. Experiments are conducted on four\nNvidia 2080Ti GPUs using PyTorch deep learning framework.\n4.3 Comparison with the State-Of-The-Art\nHere, we demonstrate the experimental results on four datasets and compare our method\nwith a number of state-of-the-art works. As shown in Table 2, FFVT obtains the second\nbest-performed method on CUB with an accuracy of 91.6%, beating other methods by a\nlarge margin except for the most recent state-of-the-art ﬁne-grained method TransFG (-\n0.1%). Note that FFVT achieves a comparable accuracy against TransFG with much less\ncomputation cost and GPU memory consumption since the overlapping strategy of TransFG\nsigniﬁcantly increases the number of the input patches from 784 to 1296. Besides, limited by\nour computation resources, the batch size of TransFG on the experiment of CUB dataset is\ntwo times larger than FFVT. This may also account for the relative performance differences.\nFFVT outperforms all the listed approaches on Stanford Dogs with an accuracy of 91.5%,\nstrongly exceeding the second best-performed TransFG by 0.9%.\nTable 2: Comparison of different methods on CUB-200-2011 datasets. The best accuracy is\nhighlighted in bold and the second best accuracy is underlined.\nMethod Backbone Accuracy\nResNet50 [15] ResNet50 84.5\nGP-256 [39] VGG16 85.8\nMaxEnt [8] DenseNet161 86.6\nDFL-CNN [37] ResNet50 87.4\nNTS-Net [42] ResNet50 87.5\nCross-X [23] ResNet50 87.7\nDCL [3] ResNet50 87.8\nCIN [12] ResNet101 88.1\nDBTNet [53] ResNet101 88.1\nACNet [17] ResNet50 88.1\nS3N [5] ResNet50 88.5\nFDL [22] DenseNet161 89.1\nPMG [7] ResNet50 89.6\nAPI-Net [55] DenseNet161 90.0\nStackedLSTM [13] GoogleNet 90.4\nViT [6] ViT-B_16 90.8\nTransFG [14] ViT-B_16 91.7\nFFVT ViT-B_16 91.6\nSoyCultivarLocal and CottonCultivar80 are two extremely challenging ultra-ﬁne-grained\ndatasets. The difﬁculty lies in two folds, i.e., super-subtle inter-class differences and few\ntraining images (three for each category). Some examples are visualized in ﬁgure 1. There-\nfore, locating the discriminative regions plays an essential role in accurate classiﬁcation.\n: 9\nTable 3: Comparison of different methods on Stanford Dogs (Dogs) dataset. The best accu-\nracy is highlighted in bold and the second best accuracy is underlined. Values in parentheses\nare reported results in their papers.\nMethod Backbone Dogs\nMaxEnt [8] DenseNet161 83.6\nFDL [22] DenseNet161 84.9\nRA-CNN [10] VGG19 87.3\nDB [27] ResNet50 87.7\nSEF [24] ResNet50 88.8\nCross-X [23] ResNet50 88.9\nAPI-Net [55] DenseNet161 90.3\nViT [6] ViT-B_16 90.2\nTransFG [14] ViT-B_16 90.6 (92.3)\nFFVT ViT-B_16 91.5\nThe results of experiments on SoyCultivarLocal and CottonCultivar80 are shown in Table\n3. FFVT obtains the highest accuracy of 57.92% on CottonCultivar80, outperforming the\nsecond best-performed method by a large margin (+4.17%). Similarly, our proposed FFVT\nbeats all methods with an accuracy of 44.17% on SoyCultivarLocal.\nTable 4: Comparison of different methods on SoyCultivarLocal (Soy.Loc) and CottonCulti-\nvar80 (Cotton) datasets. The best accuracy is highlighted in bold and the second best accu-\nracy is underlined.\nMethod Backbone Cotton Soy.Loc\nAlexNet [19] AlexNet 22.92 19.50\nVGG16 [26] VGG16 50.83 39.33\nResNet50 [15] ResNet50 52.50 38.83\nInceptionV3 [28] GoogleNet 37.50 23.00\nMobileNetV2 [25] MobileNet 49.58 34.67\nImproved B-CNN [21] VGG16 45.00 33.33\nNTS-Net [42] ResNet50 51.67 42.67\nfast-MPN-COV [20] ResNet50 50.00 38.17\nViT [6] ViT-B_16 51.25 39.33\nDeiT-B [29] ViT-B_16 53.75 38.67\nTransFG [14] ViT-B_16 45.84 38.67\nFFVT ViT-B_16 57.92 44.17\n4.4 Ablation Studies\nWe perform the ablation studies on CottonCultivar80, SoyCultivarLocal and CUB to further\nvalidate the effectiveness of our proposed methods. SAWS is the single attention weight\nselection strategy designed in Section 3.2.2. As shown in Table 5, even the simple SAWS\nstrategy can remarkably boost the performance by 4.58%, 3.50% and 0.64% on CottonCulti-\nvar80, SoyCultivarLocal and CUB, respectively. The results conﬁrm the necessity of aggre-\ngating the local and different level information for vision transformer on FGVC. A bigger\n10 :\nimprovement can be seen when applying the MAWS strategy (+6.67%, 4.84% and 0.80%\non CottonCultivar80, SoyCultivarLocal and CUB, respectively), showing that MAWS bet-\nter exploits the attention information. MAWS explicitly selects the most useful tokens thus\nforces the model to learn from these informative parts.\nTable 5: Ablation studies on CottonCultivar80 (Cotton), SoyCultivarLocal (Soy.Loc), and\nCUB datasets. The best accuracy is highlighted in bold.\nMethod Cotton Soy.Loc CUB\nViT [6] 51.25 39.33 90.85\nViT+Feature Fusion+SAWS 55.83 42.83 91.49\nFFVT(ViT+Feature Fusion+MAWS) 57.92 44.17 91.65\nWe then investigate the inﬂuence of the hyper-parameter K. Table 6 summarizes the\nresults of FFVT on the SoyCultivarLocal dataset with the value of K ranging from 10 to\n14. FFVT achieves the best performance when there are 12 tokens selected for each layer.\nOne possible reason is that the tokens focused by each attention head are selected by the\nproposed MAWS module and contribute positively to the classiﬁcation since this value (12)\nis in accordance with the number of the attention heads. As K increases from 10 to 12,\nthe accuracy steadily enhances from 43.17% to 44.17%. A different pattern can be seen\nwhen K continues increasing to 14, where the accuracy slightly reduces to 42.5%. The\nperformance drop may due to that large K introduces the noisy tokens while small K value\nlead to insufﬁcient discriminative information for classiﬁcation. Note that results of all K\nsettings show a signiﬁcant improvements over that backbone ViT (39.33%), indicating that\nFFVT is not very sensitive to the value of K.\nTable 6: Ablation studies of the hyper-parameter K on SoyCultivarLocal benchmark. The\nbest accuracy is highlighted in bold.\nK 10 11 12 13 14\nß Accuracy(%) 43.17 43.83 44.17 43.00 42.50\n5 Conclusion\nThis paper proposes a novel ﬁne-grained visual categorization architecture FFVT and achieves\nstate-of-the-art performance on four benchmarks. To facilitate the performance of vision\ntransformer in FGVC, we propose a feature fusion approach to enrich the local, low-level\nand middle-level information for the classiﬁcation token. To select the discriminative tokens\nthat to be aggregated, we develop a novel token selection module MAWS which explicitly\ntakes advantage of the attention scores produced by self-attention mechanism. Experimental\nresults show that FFVT signiﬁcantly improve the classiﬁcation accuracy of standard ViT on\ndifferent ﬁne-grained settings, i.e., normal-scale, small-scale and ultra-ﬁne-grained settings.\nWe observe that FFVT is very effective on the challenging datasets, conﬁrming its capability\nof capturing subtle differences and discriminative information.\nBased on our encouraging results, we believe that the pure-transformer model has the\nhuge potential on different FGVC settings, even in the small-scale datasets without the in-\nduction bias like convolutional neural networks.\n: 11\nReferences\n[1] Thomas Berg and Peter N Belhumeur. Poof: Part-based one-vs.-one features for ﬁne-\ngrained categorization, face veriﬁcation, and attribute estimation. InProceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pages 955–962, 2013.\n[2] Wei Chen, Tie-Yan Liu, Yanyan Lan, Zhi-Ming Ma, and Hang Li. Ranking measures\nand loss functions in learning to rank. Advances in Neural Information Processing\nSystems, 22:315–323, 2009.\n[3] Yue Chen, Yalong Bai, Wei Zhang, and Tao Mei. Destruction and construction learning\nfor ﬁne-grained image recognition. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 5157–5166, 2019.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n[5] Yao Ding, Yanzhao Zhou, Yi Zhu, Qixiang Ye, and Jianbin Jiao. Selective sparse\nsampling for ﬁne-grained image recognition. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 6599–6608, 2019.\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[7] Ruoyi Du, Dongliang Chang, Ayan Kumar Bhunia, Jiyang Xie, Zhanyu Ma, Yi-Zhe\nSong, and Jun Guo. Fine-grained visual classiﬁcation via progressive multi-granularity\ntraining of jigsaw patches. In European Conference on Computer Vision, pages 153–\n168. Springer, 2020.\n[8] Abhimanyu Dubey, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Maximum-\nentropy ﬁne-grained classiﬁcation. arXiv preprint arXiv:1809.05934, 2018.\n[9] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Hervé Jégou. Training vision\ntransformers for image retrieval. arXiv preprint arXiv:2102.05644, 2021.\n[10] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see better: Recurrent atten-\ntion convolutional neural network for ﬁne-grained image recognition. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 4438–4446,\n2017.\n[11] Yongsheng Gao and Maylor KH Leung. Face recognition using line edge map. IEEE\ntransactions on pattern analysis and machine intelligence, 24(6):764–779, 2002.\n[12] Yu Gao, Xintong Han, Xun Wang, Weilin Huang, and Matthew Scott. Channel in-\nteraction networks for ﬁne-grained image categorization. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 34, pages 10818–10825, 2020.\n12 :\n[13] Weifeng Ge, Xiangru Lin, and Yizhou Yu. Weakly supervised complementary parts\nmodels for ﬁne-grained image classiﬁcation from the bottom up. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3034–\n3043, 2019.\n[14] Ju He, Jie-Neng Chen, Shuai Liu, Adam Kortylewski, Cheng Yang, Yutong Bai,\nChanghu Wang, and Alan Yuille. Transfg: A transformer architecture for ﬁne-grained\nrecognition. arXiv preprint arXiv:2103.07976, 2021.\n[15] Xiangteng He and Yuxin Peng. Weakly supervised learning of part selection model\nwith spatial constraints for ﬁne-grained image classiﬁcation. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, volume 31, 2017.\n[16] Shaoli Huang, Zhe Xu, Dacheng Tao, and Ya Zhang. Part-stacked cnn for ﬁne-grained\nvisual categorization. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 1173–1182, 2016.\n[17] Ruyi Ji, Longyin Wen, Libo Zhang, Dawei Du, Yanjun Wu, Chen Zhao, Xianglong Liu,\nand Feiyue Huang. Attention convolutional binary neural tree for ﬁne-grained visual\ncategorization. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10468–10477, 2020.\n[18] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel\ndataset for ﬁne-grained image categorization: Stanford dogs. In Proc. CVPR Workshop\non Fine-Grained Visual Categorization (FGVC), volume 2. Citeseer, 2011.\n[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with\ndeep convolutional neural networks. Advances in neural information processing sys-\ntems, 25:1097–1105, 2012.\n[20] Peihua Li, Jiangtao Xie, Qilong Wang, and Zilin Gao. Towards faster training of global\ncovariance pooling networks by iterative matrix square root normalization. In Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n947–955, 2018.\n[21] Tsung-Yu Lin and Subhransu Maji. Improved bilinear pooling with cnns. arXiv\npreprint arXiv:1707.06772, 2017.\n[22] Chuanbin Liu, Hongtao Xie, Zheng-Jun Zha, Lingfeng Ma, Lingyun Yu, and Yongdong\nZhang. Filtration and distillation: Enhancing region attention for ﬁne-grained visual\ncategorization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , vol-\nume 34, pages 11555–11562, 2020.\n[23] Wei Luo, Xitong Yang, Xianjie Mo, Yuheng Lu, Larry S Davis, Jun Li, Jian Yang, and\nSer-Nam Lim. Cross-x learning for ﬁne-grained visual categorization. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision , pages 8242–8251,\n2019.\n[24] Wei Luo, Hengmin Zhang, Jun Li, and Xiu-Shen Wei. Learning semantically enhanced\nfeature for ﬁne-grained image classiﬁcation. IEEE Signal Processing Letters, 27:1545–\n1549, 2020.\n: 13\n[25] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh\nChen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 4510–4520,\n2018.\n[26] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-\nscale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n[27] Guolei Sun, Hisham Cholakkal, Salman Khan, Fahad Khan, and Ling Shao. Fine-\ngrained recognition: Accounting for subtle differences between similar classes. InPro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 34, pages 12047–\n12054, 2020.\n[28] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 2818–2826, 2016.\n[29] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablay-\nrolles, and Hervé Jégou. Training data-efﬁcient image transformers & distillation\nthrough attention. arXiv preprint arXiv:2012.12877, 2020.\n[30] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe\nMorency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multi-\nmodal language sequences. In Proceedings of the conference. Association for Compu-\ntational Linguistics. Meeting, volume 2019, page 6558. NIH Public Access, 2019.\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017.\n[32] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The\ncaltech-ucsd birds-200-2011 dataset. 2011.\n[33] Jun Wang, Qianying Liu, Haotian Xie, Zhaogang Yang, and Hefeng Zhou. Boosted\nefﬁcientnet: detection of lymph node metastases in breast cancer using convolutional\nneural networks. Cancers, 13(4):661, 2021.\n[34] Jun Wang, Zhao Yang, Linglong Qian, Xiaohan Yu, and Yongsheng Gao. EAR-NET:\nError Attention Reﬁning Network For Retinal Vessel Segmentation. In 2021 Interna-\ntional Conference on Digital Image Computing: Techniques and Applications (DICTA).\nIEEE, 2021.\n[35] Jun Wang, Xiaohan Yu, and Yongsheng Gao. Mask guided attention for ﬁne-grained\npatchy image classiﬁcation. In 2021 IEEE International Conference on Image Process-\ning (ICIP), pages 1044–1048, 2021.\n[36] Jun Wang, Hefeng Zhou, and Xiaohan Yu. PGTRNet: Two-phase Weakly Su-\npervisedObject Detection with Pseudo Ground Truth Reﬁning. arXiv preprint\narXiv:2104.00231, 2021.\n[37] Yaming Wang, Vlad I Morariu, and Larry S Davis. Learning a discriminative ﬁlter\nbank within a cnn for ﬁne-grained recognition. In Proceedings of the IEEE conference\non Computer Vision and Pattern Recognition, pages 4148–4157, 2018.\n14 :\n[38] Zhihui Wang, Shijie Wang, Pengbo Zhang, Haojie Li, Wei Zhong, and Jianjun Li.\nWeakly supervised ﬁne-grained image classiﬁcation via correlation-guided discrimina-\ntive learning. InProceedings of the 27th ACM International Conference on Multimedia,\npages 1851–1860, 2019.\n[39] Xing Wei, Yue Zhang, Yihong Gong, Jiawei Zhang, and Nanning Zheng. Grassmann\npooling as compact homogeneous bilinear pooling for ﬁne-grained visual classiﬁcation.\nIn Proceedings of the European Conference on Computer Vision (ECCV), pages 355–\n370, 2018.\n[40] Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, and Zheng\nZhang. The application of two-level attention models in deep convolutional neural\nnetwork for ﬁne-grained image classiﬁcation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 842–850, 2015.\n[41] Lingxi Xie, Qi Tian, Richang Hong, Shuicheng Yan, and Bo Zhang. Hierarchical part\nmatching for ﬁne-grained visual categorization. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, pages 1641–1648, 2013.\n[42] Ze Yang, Tiange Luo, Dong Wang, Zhiqiang Hu, Jun Gao, and Liwei Wang. Learning\nto navigate for ﬁne-grained classiﬁcation. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 420–435, 2018.\n[43] Xiaohan Yu, Shengwu Xiong, and Yongsheng Gao. Leaf image retrieval using com-\nbined feature of vein and contour. In 2015 International Conference on Image and\nVision Computing New Zealand (IVCNZ), pages 1–6. IEEE, 2015.\n[44] Xiaohan Yu, Shengwu Xiong, Yongsheng Gao, Yang Zhao, and Xiaohui Yuan. Multi-\nscale crossing representation using combined feature of contour and venation for leaf\nimage identiﬁcation. In 2016 International Conference on Digital Image Computing:\nTechniques and Applications (DICTA), pages 1–6. IEEE, 2016.\n[45] Xiaohan Yu, Yongsheng Gao, Shengwu Xiong, and Xiaohui Yuan. Multiscale contour\nsteered region integral and its application for cultivar classiﬁcation. IEEE Access, 7:\n69087–69100, 2019.\n[46] Xiaohan Yu, Yang Zhao, Yongsheng Gao, Shengwu Xiong, and Xiaohui Yuan. Patchy\nimage structure classiﬁcation using multi-orientation region transform. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence , volume 34, pages 12741–12748,\n2020.\n[47] Xiaohan Yu, Yang Zhao, Yongsheng Gao, and Shengwu Xiong. Maskcov: A random\nmask covariance network for ultra-ﬁne-grained visual categorization. Pattern Recogni-\ntion, page 108067, 2021.\n[48] Xiaohan Yu, Yang Zhao, Yongsheng Gao, Shengwu Xiong, and Xiaohui Yuan. Bench-\nmark platform for ultra-ﬁne-grained visual categorization beyond human performance.\nIn International Conference on Computer Vision (ICCV), 2021.\n[49] Baochang Zhang, Yongsheng Gao, Sanqiang Zhao, and Jianzhuang Liu. Local deriva-\ntive pattern versus local binary pattern: face recognition with high-order local pattern\ndescriptor. IEEE transactions on image processing, 19(2):533–544, 2009.\n: 15\n[50] Bo Zhao, Xiao Wu, Jiashi Feng, Qiang Peng, and Shuicheng Yan. Diversiﬁed visual\nattention networks for ﬁne-grained object classiﬁcation. IEEE Transactions on Multi-\nmedia, 19(6):1245–1256, 2017.\n[51] Yang Zhao, Chunhua Shen, Xiaohan Yu, Hao Chen, Yongsheng Gao, and Shengwu\nXiong. Learning deep part-aware embedding for person retrieval. Pattern Recognition,\n116:107938, 2021.\n[52] Yang Zhao, Xiaohan Yu, Yongsheng Gao, and Chunhua Shen. Learning discriminative\nregion representation for person retrieval. Pattern Recognition, 121:108229, 2022.\n[53] Heliang Zheng, Jianlong Fu, Zheng-Jun Zha, and Jiebo Luo. Learning deep\nbilinear transformation for ﬁne-grained image representation. arXiv preprint\narXiv:1911.03621, 2019.\n[54] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang,\nYanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with transformers. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n6881–6890, 2021.\n[55] Peiqin Zhuang, Yali Wang, and Yu Qiao. Learning attentive pairwise interaction for\nﬁne-grained classiﬁcation. In Proceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 34, pages 13130–13137, 2020.",
  "topic": "Discriminative model",
  "concepts": [
    {
      "name": "Discriminative model",
      "score": 0.8730345964431763
    },
    {
      "name": "Security token",
      "score": 0.7525622248649597
    },
    {
      "name": "Computer science",
      "score": 0.7387101054191589
    },
    {
      "name": "Categorization",
      "score": 0.6754010915756226
    },
    {
      "name": "Transformer",
      "score": 0.6690411567687988
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6318478584289551
    },
    {
      "name": "Feature selection",
      "score": 0.5933570861816406
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43337082862854004
    },
    {
      "name": "Machine learning",
      "score": 0.40846937894821167
    },
    {
      "name": "Engineering",
      "score": 0.10745996236801147
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}