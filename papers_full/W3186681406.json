{
  "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer",
  "url": "https://openalex.org/W3186681406",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2115976382",
      "name": "Wu Kan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2361504826",
      "name": "Peng Hou-wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2347685921",
      "name": "Chen, Minghao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748452209",
      "name": "Fu, Jianlong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2391043829",
      "name": "Chao, Hongyang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3153465022",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3106210592",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2992308087"
  ],
  "abstract": "Relative position encoding (RPE) is important for transformer to capture sequence ordering of input tokens. General efficacy has been proven in natural language processing. However, in computer vision, its efficacy is not well studied and even remains controversial, e.g., whether relative position encoding can work equally well as absolute position? In order to clarify this, we first review existing relative position encoding methods and analyze their pros and cons when applied in vision transformers. We then propose new relative position encoding methods dedicated to 2D images, called image RPE (iRPE). Our methods consider directional relative distance modeling as well as the interactions between queries and relative position embeddings in self-attention mechanism. The proposed iRPE methods are simple and lightweight. They can be easily plugged into transformer blocks. Experiments demonstrate that solely due to the proposed encoding methods, DeiT and DETR obtain up to 1.5% (top-1 Acc) and 1.3% (mAP) stable improvements over their original versions on ImageNet and COCO respectively, without tuning any extra hyperparameters such as learning rate and weight decay. Our ablation and analysis also yield interesting findings, some of which run counter to previous understanding. Code and models are open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE.",
  "full_text": "Rethinking and Improving Relative Position Encoding for Vision Transformer\nKan Wu1,2,∗, Houwen Peng2,∗,†, Minghao Chen2, Jianlong Fu2, Hongyang Chao1\n1 Sun Yat-sen University 2 Microsoft Research Asia\nAbstract\nRelative position encoding (RPE) is important for trans-\nformer to capture sequence ordering of input tokens. Gen-\neral efﬁcacy has been proven in natural language process-\ning. However, in computer vision, its efﬁcacy is not well\nstudied and even remains controversial, e.g., whether rela-\ntive position encoding can work equally well as absolute\nposition? In order to clarify this, we ﬁrst review exist-\ning relative position encoding methods and analyze their\npros and cons when applied in vision transformers. We\nthen propose new relative position encoding methods dedi-\ncated to 2D images, called image RPE (iRPE). Our methods\nconsider directional relative distance modeling as well as\nthe interactions between queries and relative position em-\nbeddings in self-attention mechanism. The proposed iRPE\nmethods are simple and lightweight. They can be eas-\nily plugged into transformer blocks. Experiments demon-\nstrate that solely due to the proposed encoding methods,\nDeiT [22] and DETR [1] obtain up to 1.5% (top-1 Acc) and\n1.3% (mAP) stable improvements over their original ver-\nsions on ImageNet and COCO respectively, without tuning\nany extra hyperparameters such as learning rate and weight\ndecay. Our ablation and analysis also yield interesting ﬁnd-\nings, some of which run counter to previous understanding.\nCode and models are open-sourced at here.\n1. Introduction\nTransformer recently has drawn great attention in com-\nputer vision because of its competitive performance and su-\nperior capability in capturing long-range dependencies [1,\n6, 22, 25]. The core of transformer is self-attention [23],\nwhich is capable of modeling the relationship of tokens in\na sequence. Self-attention, however, has an inherent deﬁ-\nciency — it cannot capture the ordering of input tokens.\nTherefore, incorporating explicit representations of position\ninformation is especially important for transformer, since\nthe model is otherwise entirely invariant to sequence order-\ning, which is undesirable for modeling structured data.\n∗Equal contributions. Work performed when Kan and Minghao were\ninterns of MSRA. †Corresponding author: houwen.peng@microsoft.com\nThere are mainly two classes of methods to encode po-\nsitional representations for transformer. One is absolute,\nwhile the other is relative. Absolute methods [7, 23] en-\ncode the absolute positions of input tokens from 1 to maxi-\nmum sequence length. That is, each position has an individ-\nual encoding vector. The encoding vector is then combined\nwith the input token to expose positional information to the\nmodel. On the other hand, relative position methods [18, 3]\nencode the relative distance between input elements and\nlearn the pairwise relations of tokens. Relative position\nencoding (RPE) is commonly calculated via a look-up ta-\nble with learnable parameters interacting with queries and\nkeys in self-attention modules [18]. Such scheme allows the\nmodules to capture very long dependencies between tokens.\nRelative position encoding has been veriﬁed to be effective\nin natural language processing [5, 3, 26, 16]. However, in\ncomputer vision, the efﬁcacy is still unclear. There are few\nrecent works [6, 2, 19] shedding light on it, but obtaining\ncontroversial conclusions in vision transformers. For exam-\nple, Dosovitskiy et al. [6] observed that the relative position\nencoding does not bring any gain comparing to the abso-\nlute one (please refer to Tab. 8 in [6]). On the contrary,\nSrinivas et al. [19] found that relative position encoding can\ninduce an apparent gain, being superior to the absolute one\n(please refer to Tab. 4 in [19]). Moreover, the mostly recent\nwork [2] claims that the relative positional encoding cannot\nwork equally well as the absolute ones (please refer to Tab.\n5 in [2]). These works draw different conclusions on the\neffectiveness of relative position encoding in models, that\nmotivates us to rethink and improve the usage of relative\npositional encoding in vision transformer.\nOn the other hand, the original relative position encoding\nis proposed for language modeling, where the input data is\n1D word sequences [23, 3, 18]. But for vision tasks, the\ninputs are usually 2D images or video sequences, where\nthe pixels are highly spatially structured. It is unclear that:\nwhether the naive extension from 1D to 2D is suitable for\nvision models; whether the directional information is im-\nportant in vision tasks?\nIn this paper, we ﬁrst review existing relative position en-\ncoding methods, and then propose new methods dedicated\nto 2D images. We make the following contributions.\n1\narXiv:2107.14222v1  [cs.CV]  29 Jul 2021\n• We analyze several key factors in relative position en-\ncoding, including the relative direction, the importance\nof context, the interactions between queries, keys, val-\nues and relative position embeddings, and computa-\ntional cost. The analysis presents a comprehensive un-\nderstanding of relative position encoding, and provides\nempirical guidelines for new method design.\n• We introduce an efﬁcient implementation of relative\nencoding, which reduces the computational cost from\nthe original O(n2d) to O(nkd), where k ≪n. Such\nimplementation is suitable for high-resolution input\nimages, such as object detection and semantic segmen-\ntation, where the token number might be very large.\n• We propose four new relative position encoding meth-\nods, called image RPE (iRPE), dedicated to vision\ntransformers, considering both efﬁciency and gener-\nalizability. The methods are simple and can be eas-\nily plugged into self-attention layers. Experiments\nshow that, without adjusting any hyperparameters and\nsettings, the proposed methods can improve DeiT-\nS [22] and DETR-ResNet50 [1] by 1.5% (top-1 Acc)\nand 1.3% (mAP) over their original models on Ima-\ngeNet [4] and COCO [12], respectively.\n• We answer previous controversial questions. We em-\npirically demonstrate that relative position encoding\ncan replace the absolute encoding for image classiﬁ-\ncation task. Meanwhile, the absolute encoding is nec-\nessary for object detection, where the pixel position is\nimportant for object localization.\n2. Background\n2.1. Self-Attention\nSelf-attention plays a fundamental role in transformer.\nIt maps a query and a set of key-value pairs to an output.\nMore speciﬁcally, for an input sequence, e.g., the embed-\ndings of words or image patches, x = ( x1,..., xn) of n\nelements where xi ∈Rdx , self-attention computes an out-\nput sequence z = (z1,..., zn) where zi ∈Rdz . Each out-\nput element zi is computed as a weighted sum of input ele-\nments:\nzi =\nn∑\nj=1\nαij(xjWV). (1)\nEach weight coefﬁcient αij is computed using a softmax:\nαij = exp(eij)∑n\nk=1 exp(eik), (2)\nwhere eij is calculated using a scaled dot-product attention:\neij = (xiWQ)(xjWK)T\n√dz\n. (3)\nHere, the projections WQ, WK, WV ∈Rdx×dz are pa-\nrameter matrices, which are unique per layer.\nRather than computing the self-attention once, Multi-\nhead self-attention (MHSA) [23] runs the self-attention\nmultiple times in parallel, i.e., employing hattention heads.\nThe attention head outputs are simply concatenated and lin-\nearly transformed into the expected dimensions.\n2.2. Position Encoding\nAbsolute Position Encoding. Since transformer con-\ntains no recurrence and no convolution, in order for the\nmodel to make use of the order of the sequence, we need\nto inject some information about the position of the to-\nkens. The original self-attention considers the absolute\nposition [23], and add the absolute positional encodings\np = (p1,..., pn) to the input token embedding x as\nxi = xi + pi, (4)\nwhere the positional encoding pi,xi ∈Rd\nx. There are sev-\neral choices of absolute positional encodings, such as the\nﬁxed encodings by sine and cosine functions with differ-\nent frequencies and the learnable encodings through train-\ning parameters [7, 23].\nRelative Position Encoding. Besides the absolute posi-\ntion of each input element, recent works also consider the\npairwise relationships between elements, i.e., relative po-\nsition [18]. Relative relation is presumably important for\ntasks where the relative ordering or distance of the elements\nmatters. This type of methods encode the relative position\nbetween the input elements xi and xj into vectors pV\nij, pQ\nij,\npK\nij ∈Rdz , where dz = dx. The encoding vectors are em-\nbedded into the self-attention module, which re-formulates\nEq. (1) and Eq. (3) as\nzi =\nn∑\nj=1\nαij(xjWV+ pV\nij), (5)\neij =\n(xiWQ+ pQ\nij)(xjWK+ pK\nij)T\n√dz\n. (6)\nIn this fashion, the pairwise positional relation is learned\nduring transformer training. Such relative position encod-\ning can be either shared across attention heads or not.\n3. Method\nIn this section, we ﬁrst review previous relative position\nencoding methods and analyze their differences. Then, we\npropose four new methods dedicated to vision transformer,\nand their efﬁcient implementation.\n2\n(a) bias mode\n (b) contextual mode\nFigure 1: Illustration of self-attention modules with 2D relative position encoding on keys. The blue parts are newly added.\n3.1. Previous Relative Position Encoding Methods\nShaw’s RPE.Shaw et al. [18] propose a relative position\nencoding for self-attention. The input tokens are modeled\nas a directed and fully-connected graph. Each edge between\ntwo arbitrary positions i and j is presented by a learnable\nvector pij ∈Rdz , namely relative position encoding. Be-\nsides, the authors deemed that precise relative position in-\nformation is not useful beyond a certain distance, so intro-\nduced a clip function to reduce the number of parameters.\nThe encoding is formulated as\nzi =\nn∑\nj=1\nαij(xjWV+ pV\nclip(i−j,k)), (7)\neij =\n(xiWQ)(xjWK+ pK\nclip(i−j,k))T\n√dz\n, (8)\nclip(x,k) = max(−k,min(k,x)), (9)\nwhere pV and pK are the trainable weights of relative po-\nsition encoding on values and keys, respectively. pV =\n(pV\n−k,..., pV\nk ) and pK = (pK\n−k,..., pK\nk ) where pV\ni ,pK\ni ∈\nRdz . The scalar kis the maximum relative distance.\nRPE in Transformer-XL. Dai et al. [3] introduce addi-\ntional bias terms for queries, and uses the sinusoid formula-\ntion for relative position encoding, which is formulated as\neij = (xiWQ+ u)(xjWK)T + (xiWQ+ v)(si−jWR)\nT\n√dz\n,\n(10)\nwhere u,v ∈Rdz are two learnable vectors.\nThe sinusoid encoding vector s provides the prior of rel-\native position [23]. WR ∈Rdz×dz is a trainable matrix,\nprojecting si−j into a location-based key vector.\nHuang’s RPE.Huang et al. [11] propose a new method\nconsidering the interactions of queries, keys and relative po-\nsitions simultaneously. The equation is given as follows\neij = (xiWQ+ pij)(xjWK+ pij)T−pijpijT\n√dz\n, (11)\nwhere pij ∈Rdz is the relative position encoding shared by\nqueries and keys.\nRPE in SASA.The above three methods are all designed\nfor 1D word sequence in language modeling. Ramachan-\ndran et al. [17] propose an encoding method for 2D images.\nThe idea is simple. It divides the 2D relative encoding into\nhorizontal and vertical directions, such that each direction\ncan by modeled by a 1D encoding. The method formula-\ntion is given as follows\neij =\n(xiWQ)(xjWK+ concat(pK\nδ˜x,pK\nδ˜y))T\n√dz\n, (12)\nwhere δ˜x = ˜xi −˜xj and δ˜y = ˜yi −˜yj denote the relative\nposition offsets on x-axis and y-axis of the image coordi-\nnate respectively, pK\nδ˜x and pK\nδ˜y are learnable vectors with\nlength 1\n2 dz, the concatoperation concatenates the two en-\ncodings to form a ﬁnal relative encoding with length of dz.\nIn other words, the same offsets on x-axis or y-axis share\nthe same relative position encoding, so this method is able\nto reduce the number of learnable parameters and computa-\ntional cost. However, the encoding is only applied on keys.\nIn our experiments, we observe that the RPE imposed on\nkeys, queries and values simultaneously is the most effec-\ntive one, as presented in Tab. 4 and Tab. 5.\nRPE in Axial-Deeplab. Wang et al. [24] introduce\na position-sensitive method that adds qkv-dependent posi-\ntional bias into self-attention. The position sensitivity is ap-\nplied on axial attention that propagates information along\n3\nheight-axis and width-axis sequentially. However, when the\nrelative distance is larger than a threshold, the encoding is\nset to zero. We observe that long-range relative position\ninformation is useful, as analysed in Tab. 6. The position-\nsensitivity might be competitive when imposed on the stan-\ndard self-attention. If equipped with the proposed piecewise\nfunction, it can be further improved and become more efﬁ-\ncient for modeling long-range dependencies.\n3.2. Proposed Relative Position Encoding Methods\nWe design our image RPE (iRPE) methods to analyze\nseveral factors which are not well studied in prior works\n(see the analysis in Sec. 4.2). First, to study whether the\nencoding can be independent of the input embeddings, we\nintroduce two relative position modes: bias and contextual.\nWe present a piecewise function to map relative positions to\nencodings, being different from the conventional clip func-\ntion. After that, to study the importance of directivity, we\ndesign two undirected and two directed methods. Finally\nwe provide an efﬁcient implementation for our methods.\nBias Mode and Contextual Mode.Previous relative po-\nsition encoding methods all depend on input embeddings.\nIt brings a question, i.e., whether the encoding can be inde-\npendent of the input? We introduce bias mode and contex-\ntual mode of relative position encoding to study the ques-\ntion. The former one is independent of input embeddings,\nwhile the latter one considers the interaction with queries,\nkeys or values. More speciﬁcally, we introduce a uniﬁed\nformulation as\neij = (xiWQ)(xjWK)T+bij√dz\n, (13)\nwhere bij ∈R is the 2D relative position encoding, deﬁning\nthe bias or contextual mode. For bias mode,\nbij = rij, (14)\nwhere rij ∈R is a learnable scalar and represents the rela-\ntive position weight between the position iand j. For con-\ntextual mode,\nbij = (xiWQ)rij\nT, (15)\nwhere rij ∈Rdz is a trainable vector, interacted with the\nquery embedding. There are multiple variants forbij in con-\ntextual mode. For example, the relative position encoding\noperated on both queries and keys can be presented as\nbij = (xiWQ)(rK\nij)T + (xjWK)(rQ\nij)T, (16)\nwhere rK\nij,rQ\nij ∈Rdz are both learnable vectors. Besides,\ncontextual mode can also be applied on value embeddings,\nzi =\nn∑\nj=1\nαij(xjWV+ rV\nij), (17)\nwhere rV\nij ∈ Rdz . The relative position weights rQ\nij, rK\nij\nand rV\nij can be constructed in the same way. For a uniﬁed\nrepresentation, we use rij to denote them in bias mode and\ncontextual mode in the following discussion. Fig. 1 shows\nthe illustration of self-attention modules with 2D relative\nposition encoding on keys in the propsoed two modes.\nA Piecewise Index Function. Before describing the\n2D relative position weight rij, we ﬁrst introduce a many-\nto-one function, mapping a relative distance into an in-\nteger in ﬁnite set, then rij can be indexed by the inte-\nger and share encondings among different relation posi-\ntions. Such index function can largely reduce computa-\ntion costs and the number of parameters for long sequence\n(e.g., high resolution images). Although the clip function\nh(x) = max(−β,min(β,x)) used in [18] also reduces the\ncost, the positions whose relative distance is larger than β\nare assigned to the same encoding. This method inevitably\ndrops out the contextual information of long-range relative\npositions. Inspired by [16], we introduce a piecewise func-\ntion g(x) : R →{y ∈Z|− β ≤y ≤β}for indexing\nrelative distances to corresponding encodings. The function\nis based on a hypothesis that the closer neighbors are more\nimportant than the further ones, and distributes the attention\nby the relative distance. It is presented as\ng(x) =\n{\n[x], |x|≤ α\nsign(x) ×min(β,[α+ ln (|x|/α)\nln (γ/α) (β−α)]), |x|>α\n(18)\nwhere [·] is a round operation,sign() determines the sign of\na number, i.e., returning 1 for positive input, -1 for negative,\nand 0 for otherwise. α determines the piecewise point, β\ncontrols the output in the range of [−β,β], and γ adjusts\nthe curvature of the logarithmic part.\nWe compare the piecewise function g(x) with the clip\nfunction h(x) = min(−β,max(β,x)), i.e. Eq. (9). In\nFig. 2, the clip function h(x) distributes uniform attention\nand leaves out long distance positions, but the piecewise\nfunction g(x) distributes different levels of attention by rel-\native distance. We suppose that the potential information in\nlong-range position should be preserved, especially for high\nresolution images or the tasks requiring long-range feature\ndependencies, so g(x) is selected to construct our mapping\nmethod for rij.\n2D Relative Position Calculation. In order to calculate\nrelative position on 2D image plane and deﬁne the relative\nweight rij, we propose two undirected mapping methods,\nnamely Euclidean and Quantization, as well as two directed\nmapping methods, namely Cross and Product.\nEuclidean method. On image plane, the relative position\n(˜xi−˜xj,˜yi−˜yj) is a 2D coordinate. We compute Euclidean\ndistance between two positions, and maps the distance into\nthe corresponding encoding. The method is undirected and\nformulated as\nrij = pI(i,j), (19)\n4\n20\n 15\n 10\n 5\n 0 5 10 15 20\nx\n5\n3\n1\n1\n3\n5\ny\nthe piecewise function y = g(x)\nthe clipping    function y = h(x)\nFigure 2: The comparison between the piecewise function\ng(x) and the clip function h(x).\nI(i,j) = g(\n√\n(˜xi −˜xj)2 + (˜yi −˜yj)2 ), (20)\nwhere pI(i,j) is either a learnable scalar in bias mode or a\nvector in contextual mode. We regard pI(i,j) as a bucket,\nwhich stores the relative position weight. The number of\nbuckets is 2β+ 1, as deﬁned in Eq. (18).\nQuantization method. In the above Euclidean method,\nthe closer two neighbors with different relative distances\nmay be mapped into the same index, e.g. the 2D relative\npositions (1, 0) and (1, 1) are both mapped into the index 1.\nWe suppose that the close neighbors should be separated.\nTherefore, we quantize Euclidean distance, i.e., different\nreal number is mapped into different integer. We revise\nI(i,j) in Eq. (19) as\nI(i,j) = g(quant(\n√\n(˜xi −˜xj)2 + (˜yi −˜yj)2) ). (21)\nThe operation quant(·) maps a set of real numbers {0, 1,\n1.41, 2, 2.24, ... }into a set of integers {0, 1, 2, 3, 4, ... }.\nThis method is also undirected.\nCross method. Positional direction of pixels is also im-\nportant for images, we thereby propose directed mapping\nmethods. This method is called Cross method, which com-\nputes encoding on horizontal and vertical directions sepa-\nrately, then summarizes them. The method is given as fol-\nlows,\nrij = p˜x\nI˜x(i,j) + p˜y\nI˜y(i,j), (22)\nI˜x(i,j) = g(˜xi −˜xj), (23)\nI˜y(i,j) = g(˜yi −˜yj), (24)\nwhere p˜x\nI(i,j) and p˜y\nI(i,j) are both learnable scalars in bias\nmode, or a learnable vectors in contextual mode. Similar to\nthe encoding in SASA [17], the same offsets on x-axis or\ny-axis share the same encoding, but the main difference is\nthat we use a piecewise function to distribute attention by\nrelative distance. The number of buckets is 2 ×(2β+ 1).\nProduct method. The Cross method encodes different\nrelative positions into the same embedding if the distance\non one direction is identical, either horizontal or vertical.\nBesides, the addition operation in Eq. (22) brings extra com-\nputational cost. To improve efﬁciency and involve more di-\nrectional information, we design Product method which is\nformulated as below\nrij = pI˜x(i,j),I˜y(i,j). (25)\nThe right side of the equation is a trainable scalar in bias\nmode, or a trainable vector in contextual mode. I˜x(i,j)\nand I˜y(i,j) are deﬁned in Eq. (23) and Eq. (24), and the\ncombination of them is a 2D index for p. The number of\nbuckets is (2β+ 1)2.\nAn Efﬁcient Implementation. For the above pro-\nposed methods in contextual mode, there is a common term\n(xiW)pI(i,j)T when putting Eq. (19), Eq. (22) or Eq. (25)\ninto Eq. (15). Let yij denote the common term as follows,\nyij = (xiW)pI(i,j)\nT. (26)\nIt takes time complexity O(n2d) to compute all yij, where\nnand dare the length of the input sequence and the number\nof feature channels, respectively. Due to the many-to-one\nproperty of I(i,j), the set size k of I(i,j) is usually less\nthan nin vision transformer. Therefore, we provide an efﬁ-\ncient implementation as follows,\nzi,t = (xiW)pt\nT,t ∈{I(i,j)|i,j ∈[0,n)}, (27)\nyij = zi,I(i,j). (28)\nIt ﬁrst takes time complexityO(nkd) to pre-compute allzi,t\nby Eq. (27), then assigns zi,t to all yij by the mapping t =\nI(i,j) by Eq. (28). The assignment operation takes time\ncomplexity O(n2), whose cost is much smaller than that\nof the pre-computation procedure. Thus, the computational\ncost of relative position encoding reduces from the original\nO(n2d) to O(nkd).\n4. Experiments\nIn this section, we ﬁrst provide some analysis by com-\nparing different position embeddings, followed by experi-\nments on the effects of key factors in relative position en-\ncoding. Then, we compare the proposed methods with the\nstate-of-the-art methods on image classiﬁcation and object\ndetection tasks. Finally, we visualize the relative position\nencoding and explain why it works.\n4.1. Implementation Details\nWe choose the recent vision transformer model\nDeiT [22] as the baseline for most experiments. The rela-\ntive position encoding is added into all self-attention layers.\nIf not speciﬁed, the relative position encoding is only added\non keys. We set α:β:γ = 1:2:8 for the piecewise function\ng(x), and adjust the number of buckets by changing β. An\n5\nMethod Is Mode Top-1 ∆\nbased on DeiT-S [22]Directed Acc(%) Acc(%)\nOriginal [22] - - 79.9 –\nEuclidean × bias 80.1 +0.2\ncontextual 80.4 +0.5\nQuantization × bias 80.3 +0.4\ncontextual 80.5 +0.6\nCross ✓ bias 80.5 +0.6\ncontextual 80.8 +0.9\nProduct ✓ bias 80.5 +0.6\ncontextual 80.9 +1.0\nTable 1: Ablation of our relative position encoding meth-\nods on ImageNet [4]. The original model is DeiT-S [22],\nwhich only uses absolute position encoding. We equip the\nmodel with the proposed four relative encoding methods,\ni.e., Eq. (19), Eq. (21), Eq. (22) and Eq. (25). We select the\nbest numbers of buckets for each method, which are 20, 51,\n56 and 50 respectively.\nextra bucket is used to store the relative position encodings\nof the classiﬁcation token.\nFor fair comparison, we adopt the same training set-\ntings as DeiT [22]: AdamW [14] optimizer with weight de-\ncay 0.05, initial learning rate 1x10−3 and minimal learning\n1x10−5 with cosine scheduler, 5 epochs warmup, batch size\nof 1024, 0.1 label smoothing [20], and stochastic depth with\nsurvival rate probability of 0.9. For training, the images are\nsplit into 14x14 non-overlapping patches. Data augmenta-\ntion methods [28, 27] are also consistent with DeiT [22].\nAll models are trained from scratch for 300 epochs with 8\nNVIDIA Tesla V100 GPUs.\n4.2. Analysis on Relative Position Encoding\nDirected v.s. Undirected. As shown in Tab. 1, directed\nmethods (Cross and Product), in general, perform better\nthan undirected ones (Euclidean and Quantization) in vision\ntransformer. This phenomenon illustrates that the directiv-\nity is important for vision transformers, because image pix-\nels are highly structured and semantically correlative.\nBias v.s. Contextual. Tab. 1 shows that the contextual\nmode achieves superior performance to that of bias mode,\nregardless of which method uses. The underlying reason\nmight be that contextual mode changes the encoding with\nthe input feature while bias mode keeps static.\nShared v.s. Unshared. Self-attention contains multiple\nheads. The relative position encoding can be either shared\nor unshared across different heads. We show the effects of\nthese two schemes in bias and contextual modes in Tab.\n2, respectively. For bias mode, the accuracy drops signif-\nicantly when sharing encoding across the heads. By con-\ntrast, in contextual mode, the performance gap between two\nschemes is negligible. Both of them achieve an average\ntop-1 accuracy of 80.9%. We conjecture that different head\nMode Shared #Param. MACs Top-1\n(M) (M) Acc(%)\nBias × 22.05 4613 80.54 ±0.06\n✓ 22.05 4613 80.05 ±0.04\nContextual × 22.28 4659 80.99 ±0.16\n✓ 22.09 4659 80.89 ±0.04\nTable 2: Ablation of shared and unshared relative position\nencoding across attention heads. The experiments are con-\nducted over DeiT-S [22] on ImageNet [4] with 50 buckets.\nThe models are trained and evaluated by three times.\nFunction Mode Top-1 Acc(%) Top-5 Acc(%)\nclip bias 80.1 94.9\ncontextual 80.9 95.5\npiecewise bias 80.0 95.0\ncontextual 80.9 95.5\nTable 3: Ablation for clip function and piecewise function.\nThe experiments are conducted over DeiT-S [22] model\nwith product shared-head relative position encoding on Im-\nageNet [4]. The number of buckets is 50.\nneeds different relative position encoding (RPE for short)\nto capture different information. In contextual mode, each\nhead can compute its own RPE by the Eq. (15) while in bias\nmode the shared RPE forces all heads to pay the same atten-\ntion on patches. For parameter-saving, we adopt the share\nscheme in our ﬁnal methods.\nPiecewise v.s. Clip. We compare the efﬁcacy of the\npiecewise function g(x) deﬁned in Eq. (18) and the clip\nfunction h(x) deﬁned in Eq. (9) in Tab. 3. There is a very\nsmall, even negligible, performance gap between these two\nfunctions in image classiﬁcation task. However, in object\ndetection task, we found that clip function is worse than the\npiecewise one as illustrated in Tab. 6 (#5 v.s. #6). The\nunderlying reason is that the two functions are very similar\nwhen the sequence length is short. The piecewise function\nis effective especially when the sequence size is much larger\nthan the number of buckets. Object detection uses a much\nhigher resolution input compared to classiﬁcation, leading\nto a much longer input sequence. We therefore conjecture\nthat when the input sequence is long, the piecewise function\nshould be used since it is able to distribute different atten-\ntions to the positions with relative large distance, while the\nclip function assigns the same encoding when the relative\ndistance is larger than β.\nNumber of buckets. The number of buckets largely af-\nfects model parameters, computational complexities and\nperformance. In order to ﬁnd a balance, we explore the in-\nﬂuence of varying the number of buckets for the contextual\nProduct method. Fig. 3 shows the change of top-1 accuracy\nalong with the number of buckets. The accuracy increase\n6\n0 10 26 50 82 122 226 362 730\nNumber of buckets\n80.0\n80.5\n81.0Top-1 Accuracy (%)\n79.9\n80.7\n80.8\n80.9\n80.9\n80.7\n81.0\n80.8 80.9\nFigure 3: Ablation for the number of buckets in contextual\nproduct model with shared relative position encodings on\nImageNet [4].\n112 224 384 448 512 800\nResolution\n10\n0\n10\n20\n30\nExtra MACC Ratio (%)\n1.1 3.9\n9.9 12.6 15.3 25.9\n1.1 1.0 0.9 0.9 0.9 0.7\nInefficient\nEfficient\nFigure 4: The extra computational cost of relative position\nencoding with different implementation in different resolu-\ntions. The baseline model is DeiT-S [22]. The number of\nbuckets is 50. MACs means multiply-accumulate opera-\ntions.\n# Abs Pos. pQ\nij pK\nij pV\nij Top-1 Top-5\n1 [22] learnable × × × 79.9 95.0\n2 × × × × 77.6(-2.3) 93.8\n3 × ✓ × × 80.9(+1.0) 95.4\n4 × × ✓ × 80.9(+1.0) 95.3\n5 × × × ✓ 80.2(+0.3) 95.0\n6 × ✓ ✓ × 81.0(+1.1) 95.5\n7 × ✓ ✓ ✓ 81.3(+1.4) 95.7\n8 learnable ✓ × × 80.9(+1.0) 95.5\n9 learnable × ✓ × 80.9(+1.0) 95.5\n10 learnable × × ✓ 80.2(+0.3) 95.1\n11 learnable ✓ ✓ × 81.1(+1.2) 95.4\n12 learnable ✓ ✓ ✓ 81.4(+1.5) 95.6\nTable 4: Component-wise analysis on ImageNet [4]. We\nadd contextual product shared-head relative position encod-\nings into DeiT-S [22]. The number of buckets is 50. Abs\nPos. represents the absolute position encoding. pQ\nij, pK\nij and\npV\nij present relative position encodings on queries, keys and\nvalues.\nfrom 79.9 to 80.9 before 50 buckets. After that, there is no\nsigniﬁcant improvement. It shows that the number of buck-\nets 50 is a good balance between the computational cost and\nthe accuracy for 14 ×14 feature map in DeiT-S [22].\nComponent-wise analysis. We perform a component-\nwise analysis to study the effects of different position en-\nModel #Param. Input MACs Top-1\n(M) Acc (%)\nConvnets\nResNet-50 [10] 25M 2242 4121 79.0\nRegNetY-4.0GF [15] 21M 2242 4012 79.4\nEfﬁcientNet-B1 [21] 8M 2402 712 79.1\nEfﬁcientNet-B5 [21] 30M 4562 10392 83.6\nTransformers\nViT-B/16 [6] 86M 3842 55630 77.9\nViT-L/16 [6] 307M 3842 191452 76.5\nDeiT-Ti [22] 5M 2242 1261 72.2\nCPVT-Ti(0-5) [2] 6M 2242 1262 73.4\nDeiT-Ti with iRPE-K(Ours) 6M 2242 1284 73.7\nDeiT-S [22] 22M 2242 4613 79.9\nCPVT-S(0-5) [2] 23M 2242 4616 80.5\nDeiT-S(Shaw’s) [22, 18]+ 22M 2242 4659 80.9\nDeiT-S(Trans.-XL’s) [22, 3]+ 23M 2242 4828 80.8\nDeiT-S(Huang’s) [22, 11]+ 22M 2242 4706 81.0\nDeiT-S(SASA’s) [22, 17]∗ 22M 2242 4639 80.8\nDeiT-S with iRPE-K(Ours) 22M 2242 4659 80.9\nDeiT-S with iRPE-QK(Ours) 22M 2242 4706 81.1\nDeiT-S with iRPE-QKV(Ours) 22M 2242 4885 81.4\nDeiT-B [22] 86M 2242 17592 81.8\nCPVT-B(0-5) [2] 86M 2242 17598 81.9\nDeiT-B with iRPE-K(Ours) 87M 2242 17684 82.4\n+ We utilize our product method to adapt 1D encoding for 2D images with\nthe clip function. The encoding weight is shared across heads.\n* DeiT-S [22] with SASA [17]’s relative position encoding.\nTable 5: Comparison on ImageNet [4].\ncodings for vision transformer models. We select DeiT-S\nmodel [22] as the baseline, and only change the position en-\ncoding methods. The learnable absolute position encoding\nis used in the original model. The relative position encod-\nings are computed by contextual Product method with 50\nbuckets. The conclusions we got from Tab. 4 are as fol-\nlows: 1) Removing absolute position encoding from orig-\ninal DeiT-S will cause that the Top-1 accuracy drops from\n79.9 to 77.6 (#1 v.s. #2). 2) The models with only relative\nposition encoding surpass the one with only absolute posi-\ntion encoding (#3-5 v.s. #1). It shows that relative position\nencoding works well as the absolute one. 3) When equipped\nwith relative position encoding, the absolute one does not\nbring any gains (#3-5 v.s. #8-10). We suppose that the local\ninformation is more important than the global one in clas-\nsiﬁcation task. 4) The relative position encoding on queries\nor keys brings more gain than that on values (#3,4 v.s. #5).\n5) The combination of the encodings on queries, keys and\nvalues brings further improvements (#6,7,11,12v.s. others).\nComplexity Analysis. We evaluate the computational\ncost of our proposed methods with respect to different input\nresolutions. The baseline model is DeiT-S [22] with only\nabsolute position encoding. We adopt contextual product\nshared-head relative position encoding to the baseline with\n50 buckets. Fig. 4 shows our method takes at most 1% extra\ncomputational cost with efﬁcient implementation.\n7\n# Abs Pos. Rel Pos. #buckets epoch AP AP50 AP75 APS APM APL\n1 [1] sinusoid none - 150 39.5 60.3 41.4 17.5 43.0 59.1\n2 none none - 150 30.4(-9.1) 52.5 30.2 9.4 31.2 50.5\n3 sinusoid bias 9 ×9 150 40.6(+1.1) 61.2 42.8 19.0 43.9 60.2\n4 none contextual 9 ×9 150 38.7(-0.8) 60.1 40.4 18.2 41.8 56.7\n5 sinusoid ctx clip 9 ×9 150 40.4(+0.9) 60.9 42.4 19.1 43.7 59.8\n6 sinusoid contextual 9 ×9 150 40.8(+1.3) 61.5 42.5 18.5 44.4 60.5\n7 sinusoid contextual 15 ×15 150 40.8(+1.3) 61.7 42.6 18.5 44.2 61.2\n8 [1] sinusoid none - 300 40.6 61.6 - 19.9 44.3 60.2\n9 sinusoid contextual 9 ×9 300 42.3(+1.7) 62.8 44.3 20.7 46.2 61.1\nTable 6: Component-wise analysis on DETR [1].\n4.3. Comparison on Image Classiﬁcation\nWe compare our proposed methods with the state-of-\nthe-art methods on image classiﬁcation tasks. We select\nDeiT [22] as the baseline. We adopt contextual Product\nshared-head method with a buckets number of 50. As\nshown in Tab. 5, our method brings improvement on all\nthree DeiT models. In particular, we improve the DeiT-\nTi/DeiT-S/DeiT-B models by 1.5%/1.0%/0.6% respectively,\nthrough adding relative position encoding only on keys. We\nshow that the models could be further improved by adding\nthe proposed relative position on both queries and values.\nWhen compared with other methods, ours achieve superior\nperformance with less parameters and MACs.\n4.4. Comparison on Object Detection\nTo verify the generality of our method, we further eval-\nuate it on COCO 2017 detection dataset [12]. We use the\ntransformer-based detection model DETR [1] as our base-\nline. We follow the same training and testing settings (in-\ncluding hyperparameters) as DETR [1], except injecting rel-\native position encoding into all self-attention modules in the\nencoder. As shown in Tab. 6 (#1,6 and #8,9), our method\nconsistently improve the performance of DETR by 1.3mAP\nand 1.7mAP under 150 and 300 training epochs.\nIn addition, we conduct ablation studies analyzing that\nthe effects of position encoding on object detection task.\nComparing #1, #2 and #4 in Tab. 6, we give the conclusion\nthat position encoding is crucial for DETR. We also show\nthat absolute position embedding is better than relative po-\nsition embedding in DETR, which is contrast to the obser-\nvation in classiﬁcation. We conjecture that DETR needs the\nprior of absolute position encoding to locate objects.\n4.5. Visualization\nWe explore the underlying reason of relative position en-\ncoding in this subsection. We visualize the extra weightsbij\n(deﬁned in Eq. (13)) added into the attention by relative po-\nsition encoding for different positions in each block. From\nFig. 5, we could see that relative position encoding makes\n(a) block 0\n (b) block 10\nFigure 5: Visualization of relative position encoding (RPE)\nin contextual product method. We show the extra weights\nadded to the attention by relative position encoding for dif-\nferent position. (a), (b) display the extra weights on atten-\ntion for 5 ×5 reference patches uniformly sampled from\n14 ×14 patches in block 0 and 10.\nthe current patch focus more on its neighboring patches in\nblock 0. However, when it turns to higher block, this phe-\nnomenon disappears. We conjecture this is because after\npassing through multiple layers, the model has already cap-\ntured enough local information. The shallow layers in trans-\nformer are also global attentions, which pay attention to the\nwhole image (consisting of small patches). It is different\nfrom CNN models in which shallow layers only capture lo-\ncal information. In theory, without RPEs (or other addi-\ntional operations such as local windows). transformer does\nnot explicitly capture locality. RPEs inject Conv-like induc-\ntive bias (including locality) into transformer, improving the\nmodel capability of capturing local patterns.\n5. Related Work\nTransformer.Transformer was originally introduced by\nVaswani et al. [23] for natural language processing, and re-\ncently extended to computer vision [6, 22, 1]. In this work\nwe study vision transformers in image classiﬁcation and ob-\nject detection tasks, and select DeiT [22] and DETR [1] as\n8\nour baseline models. In ViT [6] and DeiT [22] models, an\nimage is split into multiple ﬁxed-size patches. The embed-\nded features of patches are added with absolute position en-\ncoding to fed in a standard transformer encoder. An extra\ntrainable classiﬁcation token is added into the sequence for\nclassiﬁcation. In DETR [1], a CNN backbone is used for\nfeature extraction ﬁrst. It outputs a feature map downsam-\npled 32×. Then it is ﬂatten and fed to a transformer. The\ntransformer outputs a certain number of bounding boxes. A\nlearnable or sinusoid absolute position encoding is added in\nboth transformer encoder and decoder.\nRelative Position Encoding. Relative position encod-\ning is proposed ﬁrstly by Shaw et al. [18], where relative\nposition encodings are added into keys and values. Dai et\nal. [3] proposed relative position encoding with the prior of\nthe sinusoid matrix and more learnable parameters. Huang\net al. [11] proposed several 1D encoding variants. The ef-\nfectiveness of relative position encoding has been veriﬁed\nin natural language processing. There are also some works\nutilizing relative position encoding on 2D visual tasks. Ra-\nmachandran et al. [17, 19] proposed 2D relative position en-\ncoding that computes and concatenates separate encodings\nof each dimension. Chu et al. [2] proposed position encod-\ning generator, inserted between encoders. However, the ef-\nﬁcacy of relative position encoding in visual transformer is\nstill unclear, which is discussed and addressed in this work.\n6. Conclusions and Remarks\nIn this paper, we review existing relative position en-\ncoding methods, and propose four methods dedicated to\nvisual transformers. The abundant experiments show that\nour methods bring a clear improvement on both classiﬁca-\ntion and detection tasks with negligible extra complexity.\nOur method could be easily plugged into the self-attention\nmodules in vision models. In addition, we give comparison\nof different methods and analysis on relative position en-\ncoding with following conclusions. 1) Relative position en-\ncoding can be shared among different heads for parameter-\nsaving. It is able to achieve comparable performance with\nthe non-shared one in contextual mode. 2) Relative position\nencoding can replace absolute one in image classiﬁcation\ntask. However, absolute position encoding is necessary for\nobject detection task, which needs to predict locations of\nobjects. 3) Relative position encoding should consider the\npositional directivity, which is important to structured 2D\nimages. 4) Relative position encoding forces the shallow\nlayers in transformers to pay more attention to local patches.\nIn future work, we plan to extend our method to\nother attention-based models and scenarios, such as high-\nresolution input tasks like semantic segmentation [30], and\nnon-pixel input tasks like point cloud classiﬁcation [29, 9].\nReferences\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 1,\n2, 8, 9, 11, 13\n[2] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and\nHuaxia Xia. Do we really need explicit position encodings\nfor vision transformers? arXiv preprint arXiv:2102.10882,\n2021. 1, 7, 9\n[3] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell,\nQuoc Le, and Ruslan Salakhutdinov. Transformer-xl: Atten-\ntive language models beyond a ﬁxed-length context. InACL,\n2019. 1, 3, 7, 9, 13\n[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2, 6, 7, 13\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 1\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. ICLR, 2021. 1, 7, 8,\n9\n[7] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats,\nand Yann N Dauphin. Convolutional sequence to sequence\nlearning. In ICML, 2017. 1, 2\n[8] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-\nculty of training deep feedforward neural networks. In AIS-\nTATS, pages 249–256, 2010. 13\n[9] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang\nMu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud\ntransformer. arXiv preprint arXiv:2012.09688, 2020. 9\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 7, 13\n[11] Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. Im-\nprove transformer models with better relative position em-\nbeddings. In EMNLP, 2020. 3, 7, 9, 13\n[12] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 2, 8\n[13] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030, 2021. 11, 13\n[14] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101 , 2017. 6,\n13\n[15] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll ´ar. Designing network design\nspaces. In CVPR, 2020. 7\n[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\n9\nPeter J. Liu. Exploring the limits of transfer learning with a\nuniﬁed text-to-text transformer. JMLR, 21(140), 2020. 1, 4\n[17] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv:1906.05909, 2019. 3, 5, 7, 9, 13\n[18] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\nattention with relative position representations. ACL, 2018.\n1, 2, 3, 4, 7, 9, 13\n[19] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottle-\nneck transformers for visual recognition. arXiv preprint\narXiv:2101.11605, 2021. 1, 9\n[20] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\nchitecture for computer vision. In CVPR, 2016. 6\n[21] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. In ICML, 2019. 7\n[22] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv’e J’egou. Train-\ning data-efﬁcient image transformers & distillation through\nattention. arXiv preprint arXiv:2012.12877, 2020. 1, 2, 5, 6,\n7, 8, 9, 11, 13\n[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n2, 3, 8\n[24] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-\nalone axial-attention for panoptic segmentation. In ECCV,\n2020. 3\n[25] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018. 1\n[26] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,\nRuss R Salakhutdinov, and Quoc V Le. Xlnet: General-\nized autoregressive pretraining for language understanding.\nNeurIPS, 32, 2019. 1\n[27] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classiﬁers with localizable\nfeatures. In ICCV, 2019. 6\n[28] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017. 6\n[29] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and\nVladlen Koltun. Point transformer. arXiv preprint\narXiv:2012.09164, 2020. 9\n[30] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with\ntransformers, 2020. 9\n10\nRethinking and Improving Relative Position Encoding for Vision Transformer\n—— Supplementary Material ——\nThis supplementary material presents additional details\nof Section 3.2, 4.2, 4.3 and 4.4. Besides, two extra exper-\niments are added to demonstrate the effectiveness and gen-\nerality of the proposed iRPE. We also provide comparisons\non the inference time.\n• Visualization of 2D relative position. To provide an\nintuitive understanding, we visualize the proposed 2D\nrelative position in Section 3.2, including Euclidean,\nQuantization, Cross and Product methods.\n• Weight initialization. We elaborate the weight setting\nof the proposed relative position encoding methods, in-\ncluding the weight initialization and whether to equip\nwith weight decay.\n• Computation complexity. We provide a detailed ex-\nplanation of why the computational costs are the same\nfor shared and unshared relative position encodings\nacross attention heads in Tab. 2 of Section 4.2.\n• Injecting previous RPE methods into DeiT.We elab-\norate how to inject previous relative position encoding\nmethods into DeiT [22] in Tab. 5 of Section 4.3.\n• Training and test settings of DETR. We provide the\ndetails of training and test settings of DETR [1] in Sec-\ntion 4.4.\n• The effectiveness on other vision transformers. We\nshow the effectiveness of the proposed iRPE on the\nrecent Swin transformer [13].\n• Transfer learning on ﬁne-grained datasets. To\nverify the generalizability, we evaluate our models\non ﬁne-grained datasets, including Stanford Cars and\nCUB200 2011 datasets.\n• Inference performance. We compare the proposed\niRPE with previous methods in terms of inference time\nand memory cost.\n1. Visualization of 2D Relative Position\nWe visualize the proposed four relative position meth-\nods, i.e., Euclidean, Quantization, Cross and Product, and\npresent their difference. In DeiT [22], an image is split into\n14 ×14 non-overlapping patches, so the number of tokens\nis 14 ×14 (except for the classiﬁcation token). Therefore,\nin theory, each token has 14 ×14 relative positions. For\nvisualization, we select the top-left position (0,0) and the\ncenter position (7,7) as the reference positions (presented\nby a red star ⋆ in the following ﬁgures), and then compute\nthe relative offsets δx= xi−xj and δy = yi−yj between\nthe reference position and the remaining 14 ×14 −1 posi-\n0 1 2 3 4 5 6 7 8 9 10111213\nx\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\ny\n(a) top-left\n-7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6\nx\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\n6\ny\n (b) center\nFigure 6: Visualization of Euclidean method. The red star\n⋆ presents the reference position. Different color means\ndifferent bucket. The relative positions with the same color\nshare the same encoding.\ntions. Let (δx,δy) denote a 2D relative position. We plot\nthe map of the relative encoding rij, deﬁned in Eq. (19),\nEq. (21), Eq. (22) and Eq. (25), where iis the reference po-\nsition and jis one of the 14 ×14 positions. Notice that rij\nis either a learnable scalar in bias mode or a vector in con-\ntextual mode. Multiple rij may share an identical bucket,\nwhich is presented by the same color in Fig. 6 - 9. Different\nbucket is presented by different color.\nEuclidean method. Fig. 6 shows Euclidean method. It is\nan undirected method, since the relative position encodings\nonly depend on relative Euclidean distance. For example, in\nFig. 6b since the relative positions (−1,0) and (1,0) have\nthe same relative Euclidean distance of 1, they are mapped\ninto the same bucket (the grids with orange color).\nQuantization method. Fig. 7 presents Quantization\nmethod, another undirected method. It is an improved ver-\nsion of Euclidean method, and addresses the problem that\nthe close two neighbors with different relative distances\nmight be mapped into the same bucket ( e.g., he relative\nposition (1,0) and (1,1) are both mapped into the same\nbucket in Euclidean method). Besides, the number of buck-\nets in Quantization method is larger than that in Euclidean\nmethod. The reason is that Quantization method quantize\nEuclidean distance from a set of real numbers {0, 1, 1.41,\n2, 2.24, ...}to a set of integers {0, 1, 2, 3, 4, ...}, increasing\nthe number of buckets for adjacent positions.\nCross method. Fig. 8 shows Cross method. It is a di-\nrected method, in which the relative position encoding de-\npends on relative distances and relative directions simulta-\nneously. It computes the encodings on horizontal and verti-\n11\n0 1 2 3 4 5 6 7 8 9 10111213\nx\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\ny\n(a) top-left\n-7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6\nx\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\n6\ny\n (b) center\nFigure 7: Visualization of Quantization method. The red\nstar ⋆ presents the reference position. Different color\nmeans different bucket. The relative positions with the same\ncolor share the same encoding.\n0 1 2 3 4 5 6 7 8 9 10111213\nx\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\ny\n(a) top-left (horizontal)\n0 1 2 3 4 5 6 7 8 9 10111213\nx\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\ny\n (b) top-left (vertical)\n-7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6\nx\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\n6\ny\n(c) center (horizontal)\n-7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6\nx\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\n6\ny\n (d) center (vertical)\nFigure 8: Visualization of Cross method. The red star ⋆\npresents the reference position. Different color means dif-\nferent bucket. The relative positions with the same color\nshare the same encoding.\ncal directions separately, then summarizes them. The same\noffsets alongx-axis (or y-axis) direction share the same hor-\nizontal (or vertical) encoding. For example, the two relative\npositions (−1,0) and (1,0) share the same encoding on hor-\nizontal in Fig. 8c, but not on vertical in Fig. 8d.\nProduct method. Fig. 9 shows Product method, which\nis also a directed method. Unlike Cross method, Product\nmethod does not share the same encoding even if the offsets\nare the same along x-axis or y-axis direction. For example,\n0 1 2 3 4 5 6 7 8 9 10111213\nx\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\ny\n(a) top-left\n-7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6\nx\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\n6\ny\n (b) center\nFigure 9: Visualization of Product method. The red star\n⋆ presents the reference position. Different color means\ndifferent bucket. The relative positions with the same color\nshare the same encoding.\nin Fig. 9b, the two relative positions(−1,0) and (1,0) have\nindependent encodings. Moreover, it is more efﬁcient than\nCross method, since there is no extra addition operation in\nEq. (22).\n2. Weight Initialization\nThe relative position weight rij in Eq. (14) and Eq. (15)\nis initialized with zero. We found that there is no dif-\nference between zero and normal-distribution initialization.\nBesides, we do not impose weight decay on the weight of\nrelative position encodings, because its effects on the ﬁnal\nperformance is negligible.\n3. Computation Complexity\nAs shown in Tab. 2 (in the main manuscript), the com-\nputational costs MACs of shared and unshared relative po-\nsition encodings across attention heads are the same. Here,\nwe provide the detailed explanation. Let h,n,d,k denote\nthe number of heads, the length of a sequence, the num-\nber of channels and the number of buckets, respectively.\nFor bias mode, in Eq. (13), the broadcast addition on the\ndot-product attention (xiWQ)(xjWK)T with the shape of\nh×n×n and the encoding bij with the shape of n×n\nin shared scheme or h×n×n in unshared scheme takes\nthe computational cost of O(hn2). For contextual mode,\nin Eq. (27), the broadcast multiplication on the input em-\nbedding xiW with the shape h×n×d and the relative\nposition weight p with the shape of d×kin shared scheme\nor h×d×k in unshared scheme takes the computational\ncost of O(hndk). Due to the broadcast operations, the com-\nputational cost of shared and unshared schemes is the same.\n4. Injecting Previous RPE Methods into DeiT\nIn the Tab. 5, in order to compare with previous 1D\nrelative position encoding methods, we utilize our Prod-\n12\ni RPE-\nQKV\n(81.4%)\ni RPE-\nQK\n(81.1%)\nHuang's\n(81.0%)\ni RPE-\nK\n(80.9%)\nShaw's\n(80.9%)\nSASA's\n(80.8%)\nTrans.\n-XL's\n(80.8%)\nCPVT-S\n(80.5%)\nRPE methods on DeiT-S\n0\n20\n40Extra Rate (%)\n(Ours) (Ours) (Ours)\n39\n18\n38\n12\n17 18\n29\n11\n37\n10\n39\n 5\n20\n10\n27\n14\nextra inference-time rate (%)\nextra memory rate (%)\nFigure 10: The extra cost brought by RPEs. The reference\nmodel is DeiT-S [22] without RPE, taking 1,096 images/s\nand 8,930 Mb memory.\nuct method (deﬁned in Sec. 3.2 in the main manuscript) to\nadapt 1D encoding methods for 2D images. We replace the\npiecewise function g(x) with the clip function h(x), which\nis matched with previous methods. The encoding weight\nis shared across attention heads. DeiT-S(Shaw’s), DeiT-\nS(Trans.-XL’s), DeiT-S(Huang’s) are DeiT-S [22] models\nwith Shaw’s relative position encoding [18], relative posi-\ntion encoding in Transformer-XL [3] and Huang’s relative\nposition encoding [11], respectively. Besides, the 2D rela-\ntive position encoding in SASA [17] is equipped on DeiT-\nS [22] directly.\n5. Training and Test Settings of DETR\nWe follow the same training protocol and hyperparam-\neter conﬁgurations as the original DETR [1]. The back-\nbone model of DETR [1] is ResNet-50 [10], pretrained on\nImageNet [4], and the BatchNorm layers are frozen during\ntraining. All transformer blocks are initialized with Xavier\ninitialization [8]. The image is cropped such that the short-\nest side is at least 480 and at most 800 pixels while the\nlongest at most 1333. When training, random horizontal\nﬂipping and random cropping are utilized. The initial learn-\ning rates of transformer and backbone are 10−4 and 10−5,\nrespectively. Learning rates are divided by 10 in the last 50\nepochs in 150 epochs schedule, and the last 100 epochs in\n300 epochs schedule. The optimizer is AdamW [14] with\nweight decay of10−4 and a mini-batch size of 16. The num-\nber of queries is 100. We train the models for 150 epochs\nand 300 epochs.\n6. The Effectiveness on Other Vision Trans-\nformers\nWe further verify the effectiveness of the proposed iRPE\non the recent Swin transformer [13]. Speciﬁcally, the orig-\ninal Swin-T model without RPE obtains a top-1 accuracy\nof 80.5% (Tab. 4 in Swin transformer [13]), while using\nRPE bias mode gets +0.8% improvements. Our contextual\nRPE on QKV can further improve Swin-T to 81.9% on Im-\nageNet.\n7. Transfer Learning on Fine-grained Datasets\nWe ﬁnetune the pretrained models on Stanford Cars and\nCUB200 2011 datasets using the resolution 224x224 and\n300 epochs. DeiT-B [22] with iRPE on keys obtains a top-\n1 accuracy of 93.4% and 84.9% on the two datasets re-\nspectively, outperforming the original DeiT-B (92.1% and\n83.4%) by 1.3% and 1.5% points.\n8. Inference Performance\nThe inference runtime and memory cost are reported in\nFig. 10, tested on Nvidia V100 GPU with a batch size of\n128. We can see that our iRPE on keys is more effective.\n13",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.731637179851532
    },
    {
      "name": "Hyperparameter",
      "score": 0.6198747158050537
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5876991152763367
    },
    {
      "name": "Transformer",
      "score": 0.5039476752281189
    },
    {
      "name": "Position (finance)",
      "score": 0.4925842881202698
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48004019260406494
    },
    {
      "name": "Relative value",
      "score": 0.4254644513130188
    },
    {
      "name": "Computer vision",
      "score": 0.39247167110443115
    },
    {
      "name": "Voltage",
      "score": 0.09264165163040161
    },
    {
      "name": "Engineering",
      "score": 0.08273601531982422
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 24
}