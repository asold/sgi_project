{
    "title": "Language Models as Emotional Classifiers for Textual Conversation",
    "url": "https://openalex.org/W3082961718",
    "year": 2020,
    "authors": [
        {
            "id": null,
            "name": "Heaton, Connor T.",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A4298892347",
            "name": "Schwartz, David M.",
            "affiliations": [
                "Pennsylvania State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2146334809",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2003238582",
        "https://openalex.org/W2251654079",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2171638622",
        "https://openalex.org/W2584561145",
        "https://openalex.org/W2963104701",
        "https://openalex.org/W2907492528",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W4210257598",
        "https://openalex.org/W4212774754",
        "https://openalex.org/W2992984162",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2936215830",
        "https://openalex.org/W2797947982",
        "https://openalex.org/W2250379827",
        "https://openalex.org/W2788420618",
        "https://openalex.org/W2974630751",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W2519887557"
    ],
    "abstract": "Emotions play a critical role in our everyday lives by altering how we\\nperceive, process and respond to our environment. Affective computing aims to\\ninstill in computers the ability to detect and act on the emotions of human\\nactors. A core aspect of any affective computing system is the classification\\nof a user's emotion. In this study we present a novel methodology for\\nclassifying emotion in a conversation. At the backbone of our proposed\\nmethodology is a pre-trained Language Model (LM), which is supplemented by a\\nGraph Convolutional Network (GCN) that propagates information over the\\npredicate-argument structure identified in an utterance. We apply our proposed\\nmethodology on the IEMOCAP and Friends data sets, achieving state-of-the-art\\nperformance on the former and a higher accuracy on certain emotional labels on\\nthe latter. Furthermore, we examine the role context plays in our methodology\\nby altering how much of the preceding conversation the model has access to when\\nmaking a classification.\\n",
    "full_text": "Language Models as Emotional Classifiers\nfor Textual Conversation\nConnor T. Heaton\nPennsylvania State University\nState College, PA\nczh5372@psu.edu\nDavid M. Schwartz\nPennsylvania State University\nState College, PA\ndms7225@psu.edu\nABSTRACT\nEmotions play a critical role in our everyday lives by altering how\nwe perceive, process and respond to our environment. Affective\ncomputing aims to instill in computers the ability to detect and act\non the emotions of human actors. A core aspect of any affective\ncomputing system is the classification of a user’s emotion. In this\nstudy we present a novel methodology for classifying emotion in\na conversation. At the backbone of our proposed methodology is\na pre-trained Language Model (LM), which is supplemented by a\nGraph Convolutional Network (GCN) that propagates information\nover the predicate-argument structure identified in an utterance.\nWe apply our proposed methodology on the IEMOCAP and Friends\ndata sets, achieving state-of-the-art performance on the former\nand a higher accuracy on certain emotional labels on the latter.\nFurthermore, we examine the role context plays in our methodology\nby altering how much of the preceding conversation the model has\naccess to when making a classification.\nKEYWORDS\nAffective computing, Natural Language Processing, Language mod-\neling\n1 INTRODUCTION\nEmotions play a critical role in our everyday lives. They can al-\nter how we perceive, process and respond to our environment.\nFor instance, psychological literature [11] reveals that people with\ndepression interpret stimuli differently than those without depres-\nsion. Furthermore, emotions influence how we express ourselves in\nconversation, revealing more information than just what was said.\nAdditionally, in conversations, emotional responses can display\nempathy, communicating understanding and making two (or more)\npeople feel closer together.\nAffective computing aims to give computers the ability to detect\nand act on human emotions. Understanding a user’s emotional state\nprovides many new opportunities for computer systems. Detecting\nfrustration could allow a computer to identify when a user is hav-\ning trouble performing a task and can suggest help, for example.\nChatbots used in customer support can engage in more realistic\nconversations if they are able to understand the emotional content\nin received messages and incorporate that into a more accurate and\nmeaningful response. Furthermore, affective computing systems\ncan act as a safe guard for individuals with depression; a system\nthat can understand the emotional content of items on the web and\nthe current emotional state of the user can automatically determine\nwhen and what it should filter to prevent the user from feeling\ndistressed.\nThe first step in all of the systems mentioned above is detect-\ning emotion. This study focuses on the construction of an emo-\ntional classifier for textual conversations. Our method leverages\npre-trained language models (BERT and XLNet in our experiments)\nin conjunction with a Graph Convolutional Network (GCN) which\nis used to process the predicate-argument structure of an utterance,\nidentified through semantic role labeling (SRL). We achieved sub-\nstantial improvements to the state-of-the-art when applying our\nmethod on the IEMOCAP data set. When applying it on the Friends\ndata set, our method does not beat the state-of-the-art, but does\na better job of identifying emotions that appear infrequently. We\nalso analyze the importance of context in conversation by altering\nthe amount of preceding utterances the LM’s have access to while\nmaking their classification.\n2 RELATED LITERATURE\nAs our work deals with conversation transcribed as text, we describe\nliterature pertaining to language modelling first. Then, related work\nin emotional classification is mentioned. Finally, the data sets being\nused in the study are discussed.\n2.1 Language Modeling\nLanguage modeling is a natural language processing (NLP) task in\nwhich a model is asked to learn underlying distribution and relation\namong word tokens in a specified vocabulary [8]. It is common for\na language model (LM) to be pre-trained on a large-scale, general\npurpose corpus before being fine-tuned for a specific NLP task.\nLeveraging a LM in such a fashion has shown to be effective for\nperforming a variety of natural language understanding (NLU) and\nnatural language inference (NLI) tasks including speech recognition\n[12], machine translation [15], and text summarization [6].\nUntil recently most LM architectures were based on recurrent\nneural networks [8]. However, in late 2018, Devlin et al released a\nmodel they dubbed BERT (Bidirectional Encoder Representation\nfrom Transformers) [4]. As the name suggests, BERT is a language\nmodel based on the transformer architecture which consists almost\nentirely of attention modules [19]. BERT was pre-trained on the\nBooksCorpus [22] and English Wikipedia data sets using two pre-\ntraining tasks: 1) Masked Language Modeling (MLM) and Next\nSentence Prediction (NSP).\nFor the MLM pre-training task 15% of tokens in the input se-\nquence were randomly masked, and BERT was asked to predict\nthe missing tokens. For the NSP pre-training task, two sentences\nwere concatenated together and then processed by BERT as a single\ninput sequence. The [CLS] token, a special token prepended to all\ninput sequences, was then extracted from the output and used to\narXiv:2008.12360v1  [cs.CL]  27 Aug 2020\nmake a classification as to whether or not the two sentences ap-\npeared next to one another in the source document. BERT achieved\nstate-of-the-art performance on a variety of NLU tasks including\nthe popular GLUE, MultiNLI, and SQuAD benchmarks [4].\nShortly after BERT was released, Yang et al proposed XLNet, a\ngeneralized autoregressive pre-training method for LM’s [21]. XL-\nNet was similar to BERT in that it was pre-trained on a large, general\npurpose corpus and based on the transformer architecture, bet dif-\nfered from BERT on two key respects. First, XLNet was trained\nto make predictions over all permutations of the input sequence\nwhereas BERT made predictions on the raw input sequence. Sec-\nond, XLNet leveraged an autoregressive architecture whereas BERT\nemployed an auto-encoder architecture. These modifications break\nthe independence of the tokens in the input sequence assumed\nby BERT, theoretically allowing XLNet to learn more contextual\nknowledge. However, the autoregressive nature of XLNet means\nthat generated tokens are only conditioned on tokens up to the\ncurrent position in the input (to the left) whereas BERT is able to\naccess context from both the left and right of the current position\nbecause of it’s auto-encoder architecture.\n2.2 Semantic Role Labeling\nSemantic Role Labeling (SRL) is a fundamental NLP task in which\na model is asked to identify the predicate-argument structure of\na sentence, shedding light on “who” did “what” to “whom” in the\ninput text. To this end, Shi et al proposed a BERT-based model\nfor SRL in 2019 [16]. Their model was able to achieve state-of-the-\nart performance in a variety of SRL benchmarks without the use\nof external features, such as part-of-speech tags and dependency\ntrees. The choice to not use any external features was a significant\ndeparture from previous work on SRL and demonstrated the extent\nto which BERT is able to model human language.\n2.3 Graph Neural Networks\nGraph neural networks (GNN’s) are a specialized group of deep\nlearning architectures which are able to work with data represented\nin non-Euclidean domains, such as in a graph [ 20]. While many\nspecialized variants of the GNN have been proposed, one of the\nmost popular GNN architectures is known as a Graph Convolutional\nNetwork (GCN) [9]. A GCN is appealing for applications in which\ndata is represented by a graph and embeddings for each node in\nthe graph is desired. GCN’s, and many GNN’s in general, can be\nseen as a specialized message passing network in which the new\nembedding of a node is informed by it’s previous embedding, it’s\nneighbors, and it’s neighbors’ embeddings.\n2.4 Emotion Classification\nEmotional classification has been done over many modalities. Poria\net al. [14] and Tripathi et al. [18] performed modality analyses for\nemotional classification to understand which modes of commu-\nnication contain the most emotional signal. Their work involved\ncreating many classifiers for different types of data such as video,\naudio, text, and motion capture. Each model was first trained on\ndata from a single medium as well as different pairings of data\nfrom different modalities to compare the performance of unimodal\nand multimodal models. The two studies found that for unimodal\nmodels, performance from best to worst was as follows: text, audio,\nvideo, and motion capture. It is worth noting that textual data per-\nformed substantially better than the rest (with respect to models\nmade by each researcher). As modalities were combined, model\nperformance continued to increase, as did the computational com-\nplexity of the resulting system. A more detailed review of uni- and\nmultimodal emotional models can be found in [13].\nBERT has been used in both text and auditory systems to clas-\nsify emotion. The EmotionX 2019 challenge [17] demonstrates its\napplication in a text-only domain. The challenge was to create an\nemotional classifier. Eleven teams partook in the competition and\nseven submitted reports documenting their models. Five of those\nseven teams used BERT to generate contextual embeddings in their\nemotion classification system and outperformed the two teams who\ndid not incorporate BERT in their system.\nBERT has also been used to classify the emotion of audio in\nIEmoNet [7]. IEmoNet was designed to be a modular system for\nclassifying emotion, where each module can be trained (mostly)\nindependent of the others. The system accepts an audio signal as\ninput and extracts auditory emotional features from it. Concur-\nrently, the audio is transcribed and fed into textual information\nsystem. The output from the text system is combined with the audio\nfeatures extracted earlier and given to a classifier that determines\nthe emotion of the original audio clip. BERT was incorporated into\nIEmoNet as it had the best performance when classifying based on\ntext only. Furthermore, using BERT as a purely text-based emotion\nclassifier achieved an accuracy only three percentage points lower\nthan the complete IEmoNet model.\nOur research differs from these prior uses of pre-trained LM’s\ntowards emotion classification in two key regards. First, we an-\nalyze how changing the amount of context given to the model\nimpacts classification accuracy. Second, we supplement the LM\nwith a GCN which generates an additional representation of an\nutterance, informed by the predicate-argument structure of the\nutterance identified through SRL.\n3 DATA SETS\nDuring this study, two data sets were used: interactive emotional\ndyadic motion capture (IEMOCAP) [1] and Friends [2]. The data\nsets are described in more detail in the sections below.\n3.1 IEMOCAP\nIEMOCAP [1] captures multimodal emotional data. Ten actors were\nrecruited to perform one-on-one scripted and improvised scenarios\ndesigned to show a specific emotion. The dataset includes video\nand audio recordings, motion capture data of the hands and face, as\nwell as the original scripts and transcribed audio. While scenarios\nwere aimed to elicit specific emotions, the emotional labels in the\ndata set were assigned by six evaluators.\nEvaluators categorized each utterance in the data set into one\nof ten emotions: neutral, happiness, sadness, anger, surprise, fear,\ndisgust, frustration, excited, and other. Happiness, sadness, anger,\ndisgust, fear, and surprise were used as labels because they are\nconsidered basic emotions according to [5]. A neutral category was\nadded because it was of interest to the creators. Lastly, frustration\nand excited labels were added because the creators believed they\nwere important categories to accurately represent the data.\nEach utterance was seen by three different evaluators. The as-\nsessments of the displayed emotion from every judge are logged\nin the data. It is common practice for this data set to filter out data\npoints for which the annotators were unable to reach a consensus.\nAfter filtering, about 74% of the data available for analysis. Figure 1\nshows the distribution of labels in scripted and improvised scenes\nfor utterances in the filtered data set.\nAlthough the IEMOCAP data set contains a total of nine possible\nemotional labels, researchers using IEMOCAP typically focus on\nfour emotions: happiness, anger, sadness, and neutral.\n3.2 Friends\nEmotionLines [2] is a data set containing two smaller sets: Emo-\ntionPush and Friends. EmotionPush contains text conversations\nwhere each message is labeled with the emotion it projects. The\nFriends dataset contains scripts from the TV show Friends where\neach line is labeled with an emotional category. The EmotionX\n2019 [17] challenge had researchers develop emotional classifiers\nwith EmotionLines. After the competition was over, the Friends\nportion of the data set was posted for others to use. The complete\nEmotionPush data set can only be obntained via request.\nThe Friends data set has eight emotional categories: non-neutral,\nneutral, joy, sadness, anger, disgust, fear, and surprise. The data was\nlabeled using Amazon Mechanical Turk. Each utterance in the data\nset was seen by five Turkers. The final emotional label assigned to\nthe utterance was the emotion that received a majority vote by the\nannotators. The non-neutral category contains all utterances that\nhad no majority vote. Figure 2 shows the distribution of utterances\nand labels in the Friends data set. The figure demonstrates that the\ndata is rather skewed, with the majority being neutral utterances.\nThe show Friends revolves around six main characters. Thus,\nunlike in IEMOCAP, the conversations involve more than two peo-\nple. This makes capturing context in a conversation more difficult.\nHowever, it also tests the robustness of the model to generalize to\nsituations involving more than two speakers.\nThe EmotionX competition asked researchers to create models\nthat categorize an utterance into four categories:joy, sadness, anger,\nand neutral. Therefore, it mimics the outputs typically used by\nIEMOCAP. However, the EmotionLines paper ([2]), that we later\ncompare our method to, performed an analysis on all output cate-\ngories except non-neutral. As a result, we do not compare to the\nEmotionX results as we used more output categories.\n4 OUR METHODOLOGY\nIn this section, we describe our methodology in detail, highlighting\nthe importance of each module. The backbone of our proposed\nmethodology can be any pre-trained transformer-based LM, such as\nBERT [4] or XLNet [21] described above. Given the relatively small\nsize of most data sets used for identifying emotion in conversation,\nwe believe the use of a LM pre-trained on a general purpose corpus\nis crucial for achieving a high level of performance. We treat the\ntask of classifying the emotion of an utterance in conversation as a\none-versus-all binary classification task for each possible emotion\nlabel, and formulate it such that it closely resembles NSP, a task\ncommonly used as a pre-training task for LM’s. A visual depiction\nof our proposed methodology is presented in figure 3 and described\nin depth in the following sections.\n4.1 Identifying Predicate-Argument Structure\nThe first step in our proposed methodology is the identification of\nthe predicate-argument structure within an individual utterance.\nHere, we employ the BERT-based SRL model proposed by Shi et\nal and introduced above [16]. We extract the predicates and verbs\nidentified by the BERT-based model in each utterance, disregarding\nother identified entities. Next, a graph is constructed to represent\nthe predicate-argument structure identified in each utterance. For\neach predicate-argument set in an utterance, a node is created for\nboth the predicate and argument and an edge is added to connect\nthe two nodes. To increase connectivity of the graph we add an\nedge between node a and node b if the set of tokens represented by\nnode a are a subset of those represented by node b. We note that\nthis process is only performed on the utterance being classified, not\nany of the utterances provided as context.\n4.2 Constructing the Input Representation\nTo assess LM’s ability to identify the emotion of an individual ut-\nterance in a conversation, as opposed to a single, isolated utterance,\nwe explore the LM’s performance when provided with a varying\namount of preceding utterances as context. We frame the problem\nof emotion classification as a one-versus-all binary classification\ntask for each emotion, so an auxiliary sentence must be constructed\nfor all possible emotion labels. The utterance being classified and\nthe preceding N context utterances are concatenated together, sep-\narated by the [SEP] token, and used as text A for the binary clas-\nsification. The auxiliary sentence for each possible emotion label\ntakes the form of “That statement expressed [EMOTION]” and is\nused as text B for the binary classification. The input sequence to\nbe passed to the LM is formed by concatenating text A and text B\nas follows: [CLS] text A [SEP] text B [SEP] .\n4.3 Fine-Tuning with Graph-Reasoning\nThe fine-tuning of vanilla transformer-based LM’s such as BERT\nand XLNet is a relatively straightforward procedure. We utilize\npre-trained models provided by Hugging Face1 in our experiments.\nAlthough a multi-class classification at heart, the LM will first make\nE binary predictions as to whether or not an utterance should be\nlabeled with each of the E emotion labels and takes the emotion\ncorresponding to the highest binary prediction as the overall label.\nAfter the input sequence has been processed, but before each\nbinary prediction is made, a graph is constructed in accordance\nwith the predicate-argument structure identified above. The ini-\ntial embedding of the ith node, h0\ni is obtained by averaging the\nembeddings of the corresponding tokens in the LM’s output rep-\nresentation. This value is then projected to the graph embedding\ndimension, dGCN , via weight matrix W. This process is described\nbelow in equation 1.\nh0\ni = σ (W\nÕ\nwj ∈si\n1\n|si |hwj ) (1)\n1https://huggingface.co/transformers/\nFigure 1: Distribution of labels in datapoints that have a majority agreement on label in IEMOCAP. The pie chart’s (a) and (b)\nrepresent the distribution of labels for scripted and improv scenes respectively.\nFigure 2: Distribution of labels in the Friends data set.\nwhere si = {w0, ..., wt }are tokens represented by nodei, hwj is the\nLM’s contextual representation of the token wj , W ∈RdLM xdGCN\nprojects the embeddings, and σ is an activation function.\nHere, we use a GCN, described above, to process our predicate-\nargument graph [9]. As such, information propagates through the\ngraph in two phases: aggregation and combination. During aggrega-\ntion, an intermediate representation of node i’s neighbors in layer\nl, zl\ni , is influenced by node i’s neighbors, Ni , and the embedding of\nthose neighbors, hl\nj . The aggregation process is described in detail\nin equation 2, where V l is the adjacency matrix of the graph in\nlayer l.\nzl\ni =\nÕ\nj ∈Ni\n1\n|Ni |V lhl\nj (2)\nThe combination phase is then executed to obtain a new embed-\nding of node i in layer l + 1, hl+1\ni . The new embedding is informed\nby node i’s previous embedding, hl\ni , and the intermediate represen-\ntation of i’s neighbors, zl\ni . The combination phase is described in\ndetail in equation 3 below where W l is a weight matrix and σ an\nactivation function.\nhl+1\ni = σ (W lhl\ni + zl\ni ) (3)\nThen, multiplicative attention is used to obtain a unified repre-\nsentation of nodes in the graph [10]. The embedding of the [CLS]\ntoken is extracted from the output of the LM and the attention\nscores between the [CLS] token and each node in the graph are\ncomputed. The unified graph embedding, hд, is then computed as\nthe weighted average of each node in the graph in accordance with\nthe attention scores. This process is described in equations 4 and\n5 below where hc is the embedding of the [CLS] token, W1 is a\nweight matrix, hL\ni is the final embedding of node i, and Ni is the\nneighborhood of node i.\nαi =\nhcσ (W1hl\ni )\nÍ\nj ∈Ni hcσ (W1hL\nj )\n(4)\nhд =\nÕ\nj ∈Ni\nαL\nj hL\nj (5)\nThe unified graph embedding, hд, is then concatenated with the\nembedding of the [CLS] token,hc , and passed through a dense layer\nto make the binary classification. The emotion corresponding to the\nbinary prediction for each emotion is then taken as the predicted\nlabel for the utterance.\n5 EXPERIMENTS\nWe apply our methodology on both the IEMOCAP and Friends data\nsets and present our results below. We explore the performance of\nboth BERT-base-uncased and XLNet-base-cased, the most commonly\nused variant of each model, in our experiments. In all experiments,\nthe learning rate was set to 5e−6, a single GCN layer was used, and\nan Adam optimizer was used with β1 = 0.9 and β2 = 0.999.\nFigure 3: Visual depiction of our proposed methodology. Before being processed by the LM, SRL is performed on the utterance\nbeing classified. Once the input sequence has been processed by the LM, embeddings corresponding to elements identified via\nSRL are extracted and a graph describing the predicate-argument structure is processed by the GNN. Once processed, multi-\nplicative graph attention with respect to the processed [CLS] token is used to create an embedding representative of the graph.\nFinally, a dense layer takes the concatenation of the processed [CLS] token and graph embedding as input to make the final\none-versus-all classification.\n5.1 IEMOCAP\nFor experiments on the IEMOCAP data set, utterances for which less\nthan two annotators agreed on a label were excluded. Furthermore,\nutterances labeled with an emotion other than anger, happiness,\nneutral, and sadness were disregarded as is common practice. Both\nBERT and XLNet were trained for 9 epochs. Our results obtained by\nproviding BERT and XLNet with 0, 1, 2, 4, and 8 previous utterances\nas context are presented below in figure 4.\nTable 1 compares our model with varying levels of context to\nprior models mentioned earlier.\nModel WA UA\nbc-LSTM [14] 73.6%\nLSTM (Text_Model3) [18] 64.78%\nBERT (portion of IEmoNet) [7] 70.9 % 69.1%\nBERT+SRL-GNN-1 70.10% 71.15%\nBERT+SRL-GNN-2 74.66% 75.77%\nBERT+SRL-GNN-4 78.17% 77.12%\nBERT+SRL-GNN-8 80.90% 79.42%\nXLNet+SRL-GNN-1 70.76% 70.77%\nXLNet+SRL-GNN-2 74.16% 75.38%\nXLNet+SRL-GNN-4 77.98% 76.92%\nXLNet+SRL-GNN-8 80.68% 79.42%\nTable 1: Performance comparison on IEMOCAP. WA repre-\nsents weighted average accuracy. UA is unweighted average\naccuracy. Columns with one value reported either the same\nfor WA and UA or did not disclose which metric was re-\nported. The integer at the end of our presented models sig-\nnifies the number of context utterances provided.\n5.2 Friends\nFor experiments on the Friends data set all utterances were con-\nsidered. Both BERT and XLNet were trained for 11 epochs. Our\nresults obtained by providing BERT and XLNet with 0, 1, 2, 4, and\n8 previous utterances as context are presented below in figure 5.\nTables 2 and 4 compare our models with varying amounts of\ncontext over the entire dataset and per category with [2].\nModel WA UA\nCNN-BiLSTM [2] 77.4% 39.4%\nBERT+SRL-GNN-1 70.02% 51.63%\nBERT+SRL-GNN-2 68.78% 49.89%\nBERT+SRL-GNN-4 68.78% 50.27%\nBERT+SRL-GNN-8 72.10% 53.71%\nXLNet+SRL-GNN-1 69.40% 47.68%\nXLNet+SRL-GNN-2 71.47% 48.23%\nXLNet+SRL-GNN-4 71.47% 51.36%\nXLNet+SRL-GNN-8 72.82% 53.41%\nTable 2: Performance comparison on Friends dataset. WA is\nweighted accuracy. UA is unweighted accuracy.\n6 DISCUSSION\nUpon inspecting the results of our experiments we generally see an\nincrease in performance for both BERT and XLNet when applied to\nthe IEMOCAP and Friends data sets when our methodology is used.\nHowever, the realized increase in performance varies by model and\ndata set. When preceding utterances are not provided as context,\nmodels trained both with and without our methodology achieve\nsimilar levels of performance. We hypothesize that this is because\nFigure 4: Weighted and un-weighted average accuracy of BERT and XLNet when applied to IEMOCAP data set.\nFigure 5: Weighted and un-weighted average accuracy of BERT and XLNet when applied to Friends data set for the four emo-\ntions neutral, joy, sadness, and anger.\nboth models are able to identify the relevant predicate-argument\nstructure in an utterance without the use of a GCN. Recent work\npresented by Clark et al suggests that different attention heads in\nBERT are able to attend to specific types of tokens, such as direct\nobjects, noun modifiers, passive auxiliary verbs, and prepositions,\namong others, lending credence to our hypothesis [ 3]. While a\nsimilar analysis has not been done for XLNet, due to their very\nsimilar architecture, we believe it is not unreasonable to assume\nthe attention heads in XLNet perform a similar function.\n6.1 IEMOCAP\nWe see very strong performance gains when our methodology is\napplied to the IEMOCAP data set. Performance of the vanilla XLNet\nand BERT models peak when two and four utterances are provided\nModel Neu Joy Sad Fea Ang Sur Dis\nCNN-BiLSTM [2] 87.0% 60.3% 28.7% 0.0% 32.4% 40.9% 26.7%\nBERT+SRL-GNN-1 82.08% 73.98% 50.00% 37.93% 41.18% 67.55% 8.70%\nBERT+SRL-GNN-2 81.26% 67.48% 51.61% 27.59% 47.06% 65.56% 8.70%\nBERT+SRL-GNN-4 80.86% 70.73% 54.84% 20.69% 41.18% 66.23% 17.39%\nBERT+SRL-GNN-8 84.32% 69.92% 48.39% 31.03% 47.06% 73.51% 21.74%\nXLNet+SRL-GNN-1 81.67% 74.80% 46.77% 0.00% 50.59% 66.89% 13.04%\nXLNet+SRL-GNN-2 85.74% 73.98% 48.39% 0.00% 48.24% 68.21% 13.04%\nXLNet+SRL-GNN-4 83.30% 73.17% 59.68% 0.00% 54.12% 67.55% 21.74%\nXLNet+SRL-GNN-8 85.34% 73.98% 67.74% 3.45% 60.00% 61.59% 21.74%\nTable 3: Performance comparison on Friends dataset per category. The category columns use the first three letters of each\nlabel (e.g., neu corresponds to neutral).\nModel Neu Joy Sad Ang\nIDEA (BERT-based)[17] 87.30% 75.50% 59.60% 68.90%\nKU (BERT-based)[17] 86.00% 72.00% 51.40% 65.6%\nHSU (BERT-based)[17] 85.40% 73.60% 55.60% 65.00%\nAlexU (BERT-based)[17] 84.50% 72.30% 58.00% 59.70%\nBERT+SRL-GNN-0 93.08% 69.92% 48.39% 57.65%\nBERT+SRL-GNN-1 91.45% 65.85% 53.23% 48.24%\nBERT+SRL-GNN-2 91.45% 62.60% 53.23% 43.53%\nBERT+SRL-GNN-4 90.84% 69.11% 46.77% 52.94%\nBERT+SRL-GNN-8 88.80% 69.92% 53.23% 57.65%\nXLNet+SRL-GNN-0 87.37% 80.49% 45.16% 55.29%\nXLNet+SRL-GNN-1 86.35% 77.24% 59.68% 57.65%\nXLNet+SRL-GNN-2 86.56% 71.54% 59.68% 52.94%\nXLNet+SRL-GNN-4 87.78% 77.24% 62.90% 60.00%\nXLNet+SRL-GNN-8 86.76% 75.61% 69.35% 51.76%\nTable 4: Performance comparison on Friends dataset per category. The category columns use the first three letters of each\nlabel (e.g., neu corresponds to neutral).\nas context, respectively, but degrades when eight utterances are pro-\nvided as context. When our methodology is used, however, we see\nperformance continue to increase as more utterances are provided\nas context. We hypothesize that the performance of the vanilla\nmodels degrades when more than four utterances are provided as\ncontext because the length of the input sequence approaches the\nmaximum input length of both BERT and XLNet. As a result, we\nbelieve the models struggle to identify important syntactic and\nsemantic information in each utterance as well as they can when\ndealing with shorter input sequences. The inclusion of our proposed\nGCN module appears to alleviate the issue encountered when deal-\ning with longer sequences.\n6.2 Friends\nThe results of applying our methodology to the Friends data set\ndo not tell as definitive a story as the IEMOCAP results. We do\nnot see a clear difference in the performance of either XLNet or\nBERT when our methodology is employed, nor do we see a trend\nin performance as the number of utterances provided as context\nincreases. However, in general, our methodology does seem to\nprovide an increase in performance for particular experimental\nsettings. For example, when four utterances are provided as context,\nBERT sees roughly a 5% increase in unweighted average accuracy\nby using our methodology, but XLNet achieves the same level of\nperformance whether our methodology is used or not. Furthermore,\nwe observe improved unweighted accuracy metrics when our model\nis applied with both LM’s compared to when it is not applied. This\nwould suggest that our model is not as sensitive to the distribution\nof labels in the data set compared to previous methods [17].\nOne possible explanation for the inconsistent performance on the\nFriends data set is that the models struggled to handle conversations\ncontaining multiple speakers. All conversations in the IEMOCAP\ndata set involveexactly two participants, while conversations in the\nFriends data set all contain at least two participants. Another likely\ncause for the inconsistent performance is the imbalanced nature of\nthe data set. For example, utterances with the neutral label account\nfor roughly 45% of the data set while only 2% of utterances are\nlabeled as fear.\n7 FUTURE WORK\nWhile we showed that our methodology provides significant im-\nprovements over previous state-of-the-art on the IEMOCAP data\nset, we believe even higher levels of performance could be reached\nwith a few modifications to our framework. We briefly describe\nthese possible future directions in the sections below.\n7.1 Fully Exploiting LM’s\nIn our experiments we exclusively took the output from BERT\nand XLNet’s final layer as the embedding for tokens in our input\nsequence. However, the output is actually the 13th representation\nof the input sequence produced by the LM’s (recall that each LM is\nformed by stacking N transformer modules, N = 12 for the base\nmodels used in our experiments). Additionally, the models compute\nan attention matrix for the input sequence in each layer. We believe\nthese 12 internal embeddings and attention matrices may contain\nadditional emotional signal which could be used to further improve\nperformance. Clark et al have recently demonstrated how particular\nlayers in BERT, and specific attention heads in different layers, have\ndifferent functionality, perhaps suggesting this is a fruitful path\nforward [3].\n7.2 Better Leveraging Context Utterances\nAnother interesting path forward could include identifying the\npredicate-argument structure in utterances in the context, as well\nas in the utterance being classified. Having shown that performance\nimproves when SRL is performed on only the utterance being clas-\nsified, it is not outlandish to assume that the performing the same\nprocess on context utterances could further improve performance.\n7.3 More Accurately Modeling Group\nConversation\nAs mentioned above, we believe the multi-party nature (>2 par-\nticipants) of the conversations in the Friends data set is partially\nto blame for the inconsistent performance we observed. A brief\ninspection of the data revealed that in some conversations, a subset\nof participants will go off on a tangent, straying from the rest of\nthe group. Not only does the model need to handle multiple par-\nticipants, but also multiple topics of conversation. To this end, we\nbelieve it would be interesting to explore the effect of introducing\nparticipant-specific special tokens to our model, one for each partici-\npant. We believe this may allow the LM’s to better manage multiple\nparticipants, and perhaps even multiple topics of conversation.\n8 CONCLUSION\nEmotions play a crucial role in our everyday lives. Computers can\nbenefit from having the ability to detect and act on the emotional\nstate of its user. To act on emotions, the user’s state must first be\nknown. Our experiments show pre-trained LM’s, such as BERT and\nXLNet, can be used for the classification of emotion in conversa-\ntion. We supplement the LM’s by extracting the predicate-argument\nstructure of the utterance being classified as a graph, and create a\nunified graph representation through the use of a GCN and multi-\nplicative graph attention. This graph representation is then used as\nadditional input to the models when making a classification for each\nemotion. We see substantial performance improvements when our\nmethodology is applied to the IEMOCAP data set, but performance\nwhen applied to the Friends data set is much more variable.\nREFERENCES\n[1] Busso, C., Bulut, M., Lee, C.-C., Kazemzadeh, A., Mower, E., Kim, S., Chang,\nJ. N., Lee, S., and Narayanan, S. S.Iemocap: Interactive emotional dyadic motion\ncapture database. Language resources and evaluation 42 , 4 (2008), 335.\n[2] Chen, S.-Y., Hsu, C.-C., Kuo, C.-C., Ku, L.-W., et al. Emotionlines: An emotion\ncorpus of multi-party conversations. arXiv preprint arXiv:1802.08379 (2018).\n[3] Clark, K., Khandelwal, U., Levy, O., and Manning, C. D. What does bert look\nat? an analysis of bert’s attention. arXiv preprint arXiv:1906.04341 (2019).\n[4] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805 (2018).\n[5] Ekman, P., and Friesen, W. V.Constants across cultures in the face and emotion.\nJournal of personality and social psychology 17 , 2 (1971), 124.\n[6] Filippova, K., Alfonseca, E., Colmenares, C. A., Kaiser, Ł., and Vinyals, O.\nSentence compression by deletion with lstms. InProceedings of the 2015 Conference\non Empirical Methods in Natural Language Processing (2015), pp. 360–368.\n[7] Heusser, V., Freymuth, N., Constantin, S., and Waibel, A. Bimodal\nspeech emotion recognition using pre-trained language models. arXiv preprint\narXiv:1912.02610 (2019).\n[8] Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu, Y. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 (2016).\n[9] Kipf, T. N., and Welling, M. Semi-supervised classification with graph convo-\nlutional networks. arXiv preprint arXiv:1609.02907 (2016).\n[10] Luong, M.-T., Pham, H., and Manning, C. D. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025 (2015).\n[11] Mikhailova, E. S., Vladimirova, T. V., Iznak, A. F., Tsusulkovskaya, E. J., and\nSushko, N. V.Abnormal recognition of facial expression of emotions in depressed\npatients with major depression disorder and schizotypal personality disorder.\nBiological psychiatry 40 , 8 (1996), 697–705.\n[12] Mikolov, T., Karafiát, M., Burget, L., Černock`y, J., and Khudanpur, S. Re-\ncurrent neural network based language model. In Eleventh annual conference of\nthe international speech communication association (2010).\n[13] Poria, S., Cambria, E., Bajpai, R., and Hussain, A. A review of affective com-\nputing: From unimodal analysis to multimodal fusion. Information Fusion 37\n(2017), 98–125.\n[14] Poria, S., Majumder, N., Hazarika, D., Cambria, E., Gelbukh, A., and Hussain,\nA. Multimodal sentiment analysis: Addressing key issues and setting up the\nbaselines. IEEE Intelligent Systems 33 , 6 (2018), 17–25.\n[15] Schwenk, H., Rousseau, A., and Attik, M. Large, pruned or continuous space\nlanguage models on a gpu for statistical machine translation. InProceedings of the\nNAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On\nthe Future of Language Modeling for HLT (2012), Association for Computational\nLinguistics, pp. 11–19.\n[16] Shi, P., and Lin, J. Simple bert models for relation extraction and semantic role\nlabeling. arXiv preprint arXiv:1904.05255 (2019).\n[17] Shmueli, B., and Ku, L.-W. Socialnlp emotionx 2019 challenge overview: Pre-\ndicting emotions in spoken dialogues and chats. arXiv preprint arXiv:1909.07734\n(2019).\n[18] Tripathi, S., Tripathi, S., and Beigi, H. Multi-modal emotion recognition on\niemocap with neural networks. arXiv preprint arXiv:1804.05788 (2018).\n[19] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,\nKaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in neural\ninformation processing systems (2017), pp. 5998–6008.\n[20] Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Philip, S. Y.A comprehensive\nsurvey on graph neural networks. IEEE Transactions on Neural Networks and\nLearning Systems (2020).\n[21] Y ang, Z., Dai, Z., Y ang, Y., Carbonell, J., Salakhutdinov, R. R., and Le, Q. V.\nXlnet: Generalized autoregressive pretraining for language understanding. In\nAdvances in neural information processing systems (2019), pp. 5754–5764.\n[22] Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A.,\nand Fidler, S.Aligning books and movies: Towards story-like visual explanations\nby watching movies and reading books. In Proceedings of the IEEE international\nconference on computer vision (2015), pp. 19–27."
}