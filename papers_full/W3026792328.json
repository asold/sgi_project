{
  "title": "Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer",
  "url": "https://openalex.org/W3026792328",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5101714573",
      "name": "Joosung Lee",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2952335829",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963631950",
    "https://openalex.org/W2964529779",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2963034998",
    "https://openalex.org/W4289306372",
    "https://openalex.org/W2963366196",
    "https://openalex.org/W2891348164",
    "https://openalex.org/W2963667126",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W2914442349",
    "https://openalex.org/W2965033324",
    "https://openalex.org/W2970562804",
    "https://openalex.org/W2617566453",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964008635",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Text style transfer is the task that generates a sentence by preserving the content of the input sentence and transferring the style. Most existing studies are progressing on non-parallel datasets because parallel datasets are limited and hard to construct. In this work, we introduce a method that follows two stages in non-parallel datasets. The first stage is to delete attribute markers of a sentence directly through a classifier. The second stage is to generate a transferred sentence by combining the content tokens and the target style. We experiment on two benchmark datasets and evaluate context, style, fluency, and semantic. It is difficult to select the best system using only these automatic metrics, but it is possible to select stable systems. We consider only robust systems in all automatic evaluation metrics to be the minimum conditions that can be used in real applications. Many previous systems are difficult to use in certain situations because performance is significantly lower in several evaluation metrics. However, our system is stable in all automatic evaluation metrics and has results comparable to other models. Also, we compare the performance results of our system and the unstable system through human evaluation.",
  "full_text": "Proceedings of The 13th International Conference on Natural Language Generation, pages 195–204,\nDublin, Ireland, 15-18 December, 2020.c⃝2020 Association for Computational Linguistics\n195\nStable Style Transformer: Delete and Generate Approach with\nEncoder-Decoder for Text Style Transfer\nJoosung Lee\nKakao Enterprise Corp., South Korea\nrung.joo@kakaoenterprise.com\nAbstract\nText style transfer is the task that generates a\nsentence by preserving the content of the input\nsentence and transferring the style. Most ex-\nisting studies are progressing on non-parallel\ndatasets because parallel datasets are limited\nand hard to construct. In this work, we intro-\nduce a method that follows two stages in non-\nparallel datasets. The ﬁrst stage is to delete at-\ntribute markers of a sentence directly through\na classiﬁer. The second stage is to generate a\ntransferred sentence by combining the content\ntokens and the target style. We experiment on\ntwo benchmark datasets and evaluate context,\nstyle, ﬂuency, and semantic. It is difﬁcult to\nselect the best system using only these auto-\nmatic metrics, but it is possible to select stable\nsystems. We consider only robust systems in\nall automatic evaluation metrics to be the min-\nimum conditions that can be used in real appli-\ncations. Many previous systems are difﬁcult to\nuse in certain situations because performance\nis signiﬁcantly lower in several evaluation met-\nrics. However, our system is stable in all auto-\nmatic evaluation metrics and has results com-\nparable to other models. Also, we compare the\nperformance results of our system and the un-\nstable system through human evaluation. Our\ncode and data are available at the link 1.\n1 Introduction\nText style transfer is a task that generates a sentence\nwhile preserving the content in a given sentence\nbut changing the source style. The style of the\nsentence refers to a predeﬁned class (e.g. sentiment,\nformality, tense) and the content refers to the rest of\nthe sentence except for the style. Lack of parallel\ndata makes text style transfer task difﬁcult. This\nproblem cannot be solved by supervised learning\nbecause there are no right sentences.\n1https://github.com/rungjoo/Stable-Style-Transformer\nOne previous method (Hu et al., 2017; Shen\net al., 2017; Fu et al., 2018; Prabhumoye et al.,\n2018a; Logeswaran et al., 2018) of text style trans-\nfer is to learn latent representations to separate\nstyle and content from sentences. First, these ap-\nproaches try adversarial training to learn a disentan-\ngled latent representation of the content and style.\nSecondly, a transferred sentence is generated from\nthe decoder by combining the disentangled latent\nrepresentation and the target style. However, the\nexperimental results of (Lample et al., 2019) re-\nport that disentangled latent representation through\nadversarial training is hard to get and not neces-\nsary. Also, adversarial training is not effective to\nencode a sentence of various lengths into a vector\nrepresentation of ﬁxed length. Other methods of\ntext style transfer do not depend on disentangle-\nment. Dai et al. (2019a); Lample et al. (2019); Luo\net al. (2019) do not attempt to ﬁnd the disentangled\nlatent representation in the sentence. Therefore,\nsentences with different styles are mapped to the\nsame space. Xu et al. (2018a); Li et al. (2018a);\nSudhakar et al. (2019); Wu et al. (2019) neutral-\nize sentences by deleting style-dependent attribute\nmarkers. Remaining tokens resulting from the dele-\ntion of attribute markers are style independent, and\nthen the content tokens and a style attribute are\ncombined to generate the transferred sentence.\nWe propose an approach with two stages using\nDelete and Generate without adversarial training\nfor disentanglement. (1) Attribute markers of a sen-\ntence are extracted by using a pre-trained classiﬁer\nas a Delete model. Our method is model-agnostic\nand is not affected by the design of the classiﬁer.\nAttribute markers found in a sentence are deleted.\n(2) A transferred sentence is generated by combin-\ning the target attribute and the content tokens after\nstage-1. The Generate model consists of an encoder\nand decoder with the Transformer structure.\nIn the method of deleting attribute markers, Li\n196\net al. (2018b) deletes attribute markers via a statis-\ntical manner using a frequency ratio and Sudhakar\net al. (2019); Xu et al. (2018b) delete attribute mark-\ners using attention weights of a classiﬁer. Wu et al.\n(2019) deletes attribute markers by fusion of the\nfrequency ratio and the attention weights. We intro-\nduce an intuitive delete method that uses a change\nin classiﬁer probability. If a change in classiﬁer\nprobability is signiﬁcant when limiting certain to-\nkens in a sentence, the token is considered an at-\ntribute marker. Our method does not need to build\nattribute dictionaries or deﬁne attention weights\nlike previous methods and easily control the trade-\noff between content and style.\nWe test our methods on two text style transfer\ndatasets: sentiment of Yelp reviews and Amazon\nreviews. Evaluation metrics are conducted in terms\nof content, ﬂuency, style accuracy, and semantic.\nThe content and style accuracy are measured sim-\nilarly to previous studies. Fluency is measured in\ntwo ways: general-ﬂuency using pre-trained GPT-\n2 (Radford et al., 2019) and data-ﬂuency using ﬁne-\ntuned GPT-1 (Radford, 2018). Semantic is newly\nevaluated using BERTscore (Zhang* et al., 2020)\nin this paper. The goal of BERTscore is to evaluate\nsemantic equivalence between two sentences. In\nthis paper, we use a pre-trained model GPT and\nBERT (Devlin et al., 2019) that perform well in\nnatural language processing/generation to evaluate\ntransferred sentences with various automatic evalu-\nations. Since automatic evaluations are not perfect\nevaluations of generated sentences, it is hard to\nknow which system is the best, but we can deter-\nmine which system has a problem. Comparative\nmodels are unstable in some evaluation metrics.\nBut our proposed model has stable results for all\nautomatic evaluations and is called SST (Stable\nStyle Transformer). In addition, we ﬁrst observe\na point that can enhance the style controlling abil-\nity by generating sentences through latent space\nwalking in the vector space of the style attribute\ntoken.\n2 Related Work\nOne line of text style transfer research (Shen et al.,\n2017; Fu et al., 2018; Hu et al., 2017; Prabhumoye\net al., 2018b; Logeswaran et al., 2018) is to separate\ncontent and style from sentences through disentan-\ngled learning. Hu et al. (2017) uses the V AE model\nto derive the disentanglement of the content be-\ntween the generated sentence and the original sen-\ntence through KL loss. Shen et al. (2017) introduce\nthe aligned auto-encoder and the cross aligned auto-\nencoder using learning discriminators. Fu et al.\n(2018) propose a multi-decoder and StyleEmbed-\nding model. The multi-decoder model has decoders\nfor each style, and the style embedding model uses\nonly one decoder by inserting style embedding\ninto the decoder. The methods of Prabhumoye\net al. (2018b); Logeswaran et al. (2018) used back-\ntranslation to learn latent representations.\nThe second line of text style transfer research is\nnot to rely on learning for latent representation. The\nﬁrst approach (Xu et al., 2018b; Li et al., 2018b;\nSudhakar et al., 2019; Wu et al., 2019) is to ﬁnd\nand delete tokens called attribute markers that are\nhighly related to style. Li et al. (2018b) uses the\ndelete method of attribute markers as a statistical\nmethod based on frequency ratio, and Sudhakar\net al. (2019); Xu et al. (2018b) use the attention\nscores of the Transformer classiﬁer and LSTM clas-\nsiﬁer, respectively. Wu et al. (2019) deletes at-\ntribute markers by fusion of the frequency ratio and\nattention scores. The second approach (Dai et al.,\n2019b; Lample et al., 2019; Luo et al., 2019) does\nnot attempt to control content and style separately.\nTherefore, sentences with different styles are en-\ncoded to gather in the same latent representation\nspace. Dai et al. (2019b); Lample et al. (2019) are\nbased on the learning method using cycle recon-\nstruction loss. Lample et al. (2019) reported that\ndisentanglement is not easy and that latent repre-\nsentations learned through adversarial training are\nunnecessary because learned latent representations\ndepend on style. Unlike the previous models, (Luo\net al., 2019) learns dual models in two directions:\nstyle1 (e.g. negative) to style2 (e.g. positive) and\nstyle2 (e.g. positive) to style1 (e.g. negative) by\nreinforcement learning.\nIn language model research, the RNN-based lan-\nguage model is weak in long dependency. There-\nfore, the recent study of text style transfer (Dai\net al., 2019b; Sudhakar et al., 2019; Wu et al., 2019)\nhas been conducted with Transformer (Vaswani\net al., 2017) which is known to have good perfor-\nmance in language modeling. Dai et al. (2019b)\nuses a method of using the encoder and the decoder\nof the Transformer, and Sudhakar et al. (2019) uses\na method of ﬁne-tuning the decoder to the style\ntransfer datasets with the pre-trained GPT-1 as an\ninitial state. Wu et al. (2019) solved the problem\nof text style transfer in a similar way to Text Inﬁll-\n197\nFigure 1: The proposed model framework consists of Delete and Generate process. Delete process is a method\nusing a pre-trained classiﬁer, and the Generate process consists of an encoder and a decoder. In the training time,\nour model receives feedback from the classiﬁer’s probability of the generated sentence.\ning or Cloze by presenting Attribute Conditional\nMasked Language Model (AC-MLM) using pre-\ntrained BERT.\nIn this paper, we chose the ﬁrst approach (Delete\nand Generate) that does not rely on latent repre-\nsentations in the second research line, referring to\nthe results of Lample et al. (2019). Our system has\na Transformer encoder and decoder because the\nstyle transfer task is given input text. If the system\nuses only a decoder such as Sudhakar et al. (2019),\nthere is a disadvantage that it cannot include bidi-\nrectional encoding of the content token. Or, if only\nbidirectional encoders are used, such as AC-MLM,\nthe position and length of the masking tokens to be\nﬁlled in a sentence is not ﬂexible.\n3 Approach\nIn this section, we introduce our proposed method.\nThe style transfer problem deﬁnition is described\nin Section 3.1. An overview of the model is shown\nin Section 3.2. The proposed generation process is\nintroduced in Sections 3.3 and 3.4. The learning\nmechanism is described in Section 3.5.\n3.1 Problem Statement\nGiven a dataset consist of sentence and label: D=\n{(x1,s1),···,(xN,sN)}where xi is a sentence\nand si is a style attribute (e.g. sentiment) and N is\nthe number of sentences in the dataset. Our goal is\nto train the model to generate a sentence yi with a\ndifferent style while preserving the content of the\nsentence xi. For example, if xi is ”The food is\nsalty and tasteless” and si is ”negative” attribute,\nthen yi is generated to mean ”The food is not salty\nand delicious” which has a ”positive” attribute.\nHowever the dataset is non-parallel, so the model\ncannot access yi aligned with xi.\n3.2 Model Overview\nOur approach consists of two stages: Delete and\nGenerate framework in Fig. 1. The ﬁrst stage is the\nDelete process with a pre-trained style classiﬁer.\nThe pre-trained style classiﬁer ﬁnds and deletes\ntokens that contain a lot of style attributes. The\nsecond stage is encoding the content tokens and\ncombine them with a target style to generate a sen-\ntence. Both the encoder and the decoder have the\nTransformer structure, which is better than RNN\nand robust to long dependency.\n3.3 Stage-1: Delete process\nThe stage-1 is the process of ﬁnding and deleting\ntokens for a given sentence and style attribute. In\nthe previous studies, the strategies of deleting at-\ntribute markers are the frequency-ratio method and\nthe classiﬁer’s attention score (or fusion of both).\nHowever, the frequency ratio method requires a\npre-built vocabulary for the training dataset and it\nis difﬁcult to understand contextual information.\nThe attention score method has a limitation on the\nstructure of the classiﬁer, because it must learn\nthe style classiﬁer using self-attention regardless of\naccuracy. It is also unclear whether the attention\nscore is directly proportional to the attribute.\n198\nWe propose a novel method of removing at-\ntribute markers using a pre-trained classiﬁer with-\nout a pre-built dictionary and attention scores. Our\nmethod is a model-agnostic method and it is more\nintuitive to ﬁnd attribute markers than the previ-\nous method. Given an input sentence x, the style\nprobability follows:\npx = pθC (s|x) (1)\nwhere pis a probability predicted by the classiﬁer\nand s is style label. If we delete token ti from\nthe sentence x, the style probability changes as\nfollows:\npx,ti = pθC (s|x,ti) (2)\nwhere x = (t1,t2,···,tn) and nis the number of\ntokens in x. The probability difference between\nEq. 1 and Eq. 2 is deﬁned as Important Score(IS)\nof the token(ti):\nISk\nti = pxk −pxk,ti (3)\nwhere xk is the remained tokens after ktokens are\ndeleted. The value of ISk\nti determines how much\nthe token ti affects the style classiﬁer. The token\nis deleted in order of the largest IS, and the Delete\nprocess ends if only one of the following two con-\nditions: (1) pxk is less than α, or (2) the ratio of\ncontent tokens is less than β. αis a hyperparame-\nter that determines that a sentence no longer has a\nsource style attribute. β is a hyperparameter that\ndetermines how much of the content is preserved.\nThe two hyperparameters make it easy to control\nthe trade-off of content and style, and the experi-\nmental results are explained in Section 4.8.\n3.4 Stage-2: Generate process\nOur model generates a transferred sentence with\nthe encoder and the decoder of the Transformer.\n3.4.1 Encoder\nAll content tokens given as a result of Delete pro-\ncess are input to a bidirectional self-attention the\nTransformer encoder. Explicitly, the Transformer\nencoder maps content tokens xc = (t1,···,tm) to\nthe continuous representation z = (z1,···,zm) as\nfollow:\n(z1,···,zm) =Encoder(t1,···,tm; θE) (4)\n3.4.2 Decoder\nIn order to generate a sentence with the desired\nstyle, two special tokens, style and start, are ini-\ntially input to the decoder in Fig. 1. The position of\nspecial tokens is always ﬁxed in front, so we did not\nadd positional embedding. We use teacher-forcing\nat training time and no teacher-forcing at test time\nto generate sentences. If the generated token is the\nspecial token end, the Generate process ends. The\ndecoder auto-regressively predicts the conditional\nprobability of the next step token as follows:\nsoftmax(yj) =pθD (t′\nj|t′\n1,···,t′\nj−1,˜s,z) (5)\nwhere yj is the logit vector of the decoder, ˜sis a\ndesired style and t′\nj is the predicted token in jstep.\n3.5 Training\nSince we only have non-parallel datasets, we can’t\ndo supervised learning about transferred sentences.\nTherefore, we train SST to minimize two losses\naccording to style conditions: s(source style) or ˆs\n(target style).\n3.5.1 Reconstruction loss\nSST reconstructs the original sentence x condi-\ntioned on xc and source style s. Reconstruction\nloss follows the equation:\nLrec = −logpθE,θG(x|xc,s) (6)\nIn non-parallel datasets, the reconstruction loss can-\nnot be calculated if the style of the generated sen-\ntence is ˆs.\n3.5.2 Style loss\nIf the model is only trained with reconstruction loss,\nthe decoder will not see how to transform the style.\nTherefore, a discrepancy occurs between training\ntime and test time. To learn how to generate sen-\ntence ˆx with a target style ˆs, we introduce style\nloss as follows:\nLstyle = −logpθC (ˆx = ˆs|xc,ˆs) (7)\nStyle loss is measured by a pre-trained classiﬁer to\ndetermine whether the transferred sentence has a ˆs.\nSince the generated sentence is a discrete space, we\nutilize soft-embedding of predicted tokens to opti-\nmize through style loss. When the SST is trained,\nthe parameters of the classiﬁer are not ﬁnetuned.\n3.6 Model Details\nThe Transformer encoder and decoder consist of\n3 layers, and each layer has 4 heads. The style\nclassiﬁer consists of 5 convolution ﬁlters based on\nKim (2014). Text is tokenized using Byte-Pair-\nEncoding, and (word, style, position) embeddings\n199\nDataset Style Train Dev Test\nYelp Positive 270K 2000 500\nNegative 180K 2000 500\nAmazon Positive 277K 985 500\nNegative 278K 1015 500\nTable 1: (Sentiment) Dataset statistics\nare 256-dimensional vectors. In the Delete process,\n(α,β) is (0.7,0.5) at training time and observes\nthe trade-off of content and style by changing pa-\nrameters during test time.\n4 Experiments\n4.1 Dataset\nIn this paper, we test our model on two datasets,\nYELP and AMAZON, which are provided in Li\net al. (2018b). The Yelp dataset is for business\nreviews, and the Amazon dataset is product reviews.\nBoth datasets are labeled negative and positive and\nstatistics are shown in Table 1.\n4.2 Human References\nHuman references are used to measure human-\nBLEU and BERTscore. We used 2 Yelp human\nreferences and 1 Amazon human reference. Yelp:\nLi et al. (2018b) provides 1 human reference and\n3 additional human references in Luo et al. (2019).\nWe used 2 human references, one from Li et al.\n(2018b) and one (the best performance in automatic\nevaluation) from Luo et al. (2019), to increase re-\nliability. Amazon: We used the human reference\nprovided by Li et al. (2018b).\n4.3 Previous Method\nWe compare the previous models with three\napproaches. The ﬁrst comparisons are\nCrossAligned (Shen et al., 2017), [StyleEm-\nbedding, multi-decoder] (Fu et al., 2018), and\nBackTranslation (Prabhumoye et al., 2018b),\nwhich attempt to separate content and style\nthrough latent representation learning. The\nsecond comparisons are [DeleteOnly, DeleteAn-\ndRetrieve] (Li et al., 2018b), UnpariedRL (Xu\net al., 2018b) and [B-GST, G-GST] (Sudhakar\net al., 2019), which delete attribute markers and\nthen generate the sentence. [TemplateBased,\nRetrieveOnly] (Li et al., 2018b) return the target\nsentence through retrieve without generating. The\nﬁnal comparison is DualRL (Luo et al., 2019),\nwhich does not distinguish between content and\nstyle.\n4.4 Automatic Evaluation\nWe evaluated the systems in 4 ways and results are\nshown in Table 2 and 3.\n4.4.1 Content\nContent preserving intensity is measured by G-\nBLEU, the geometric mean of self-BLEU and\nhuman-BLEU, as in previous works. A high BLEU\nscore indicates that the model is good at content\npreservation.\nIn the Yelp dataset, RetireveOnly and BackTrans-\nlation are considered unstable models because G-\nBELU score is too low compared to other systems.\nIn the Amazon datasets, CrossAligned and Re-\ntrieveOnly are too low compared to other systems.\n4.4.2 Attribute\nMost style transfer studies measure style accuracy\nusing a classiﬁer. We also evaluate style accuracy\nwith a classiﬁer (note that this is different from the\none used in training).\nIn the Yelp dataset, StyleEmbedding, multi-\ndecoder, and UnpairedRL have quite a low accu-\nracy. In the Amazon datasets, StyleEmbedding,\nDeleteOnly, and DeleteAndRetrieve are unstable\nin style transfer.\n4.4.3 Fluency\nFluency is considered the perplexity of the trans-\nferred sentence. We use GPT-1 and GPT-2, which\nis known to perform well as a language model.\nGeneral-ﬂuency (g-PPL) is measured using pre-\ntrained GPT-2 and data-ﬂuency (d-PPL) is mea-\nsured using GPT-1 (instead of GPT-2 due to GPU\nmemory) ﬁnetuned to the dataset. General-Fluency\nis a general view because the language model is\nnot ﬁtted to the data, and data-ﬂuency is an evalu-\nation metric in terms of the speciﬁc data of style\ntransfer tasks. The total-ﬂuency (t-PPL) is the geo-\nmetric mean of d-PPL and g-PPL, and lower values\nindicate better ﬂuency.\nIn the Yelp dataset, TemplateBased is unstable\nbecause t-PPL is much larger than other systems.\nIn the Amazon dataset, it is determined that the\nﬂuency of B-GST and G-GST is unstable.\n4.4.4 Semantic\nSemantic is measured using BERTscore. Un-\nlike BLEU and ROUGE, BERTscore is an eval-\nuation metric deﬁned in continuous space. Pre-\ntrained model is used to calculate cosine simi-\nlarity by extracting the contextual token embed-\n200\ndings from a human reference and a transferred\nsentence. BERTscore solves the limitations of pre-\nvious metrics and measures a better correlation\nbetween the reference and the candidate. The origi-\nnal BERTscore ranged from 0 to 1, but we rescale\nit from 0 to 100 to clearly see the difference.\nWe set the unstable threshold as a margin point\nlower than the mean of all systems. The margin\npoint is a gap between an average and a lower\nbound with 95% conﬁdence considering all sys-\ntems as the gaussian distribution in the BERTscore\nevaluation. CrossAligned, multi-decoder, Re-\ntrieveOnly, and BackTranslation have limitations\non Yelp datasets. CrossAligned, multi-decoder, and\nRetrieveOnly have limitations on Amazon datasets.\nSST : For comparison with other systems, we se-\nlect the αand βof the appropriate trade-off points\nfor style transfer and content preservation. When\nexperimenting with the Yelp datasets, SST model\nis evaluated in two cases where (α,β) is (0.7, 0.5)\nand (0.7, 0.75). SST(0.7, 0.5) changes styles better\nwith style accuracy of 79.5%, but SST(0.7, 0.75) has\nbetter performance on other metrics. In the Ama-\nzon datasets, SST model is evaluated when (α,β)\nis (0.6, 0.5). The effects of αand βare discussed\nin detail in Section 4.8.\n4.5 Human Evaluation\nTable 4 shows human evaluation results for content,\nﬂuecy, and style. Comparison models, StyleEmbed-\nding and TemplatedBased, each have weaknesses\nin attribute and ﬂuency. BackTranslation has weak-\nnesses in content and semantic in automatic evalua-\ntion. In the yelp test set, we randomly sampled 250\nand gave 6 people hired through the Amazon me-\nchanical turk 2 evaluate content, ﬂuency, and style\nbetween 1 and 5 points. As a result, BackTransla-\ntion and StyleEmbedding show the worst results for\ncontent, ﬂuency, and style, respectively. Since hu-\nmans evaluate ﬂuency from a general point of view,\nthe ﬂuency performance of BackTranslation, which\nis poor in overall performance, and TemplateBased,\nwhich has poor t-PPL performance, are similarly\nbad. We conﬁrm that our system has adequate per-\nformance in human evaluation as well as automatic\nevaluation.\n4.6 Result Analysis\nHuman systems do not obtain the highest per-\nformance scores except for human-BLEU and\n2https://www.mturk.com/\nBERTscore, which are calculated using human ref-\nerences. But which of the sentences in human and\nmachines is actually realistic? Probably human. It\nis difﬁcult to determine the best system with only\nautomatic evaluation, but it is possible to determine\nwhich system is stable or unstable. If a system has\nsigniﬁcantly lower performance during the evalua-\ntion, it is considered unstable. The stable systems\nin the Yelp dataset are SST, DeleteOnly, DeleteAn-\ndRetireve, DualRL, B-GST, and G-GST. For the\nAmazon dataset, the stable systems are SST and\nTemplateBased. For all the metrics in both datasets,\nthe stable systems are SST and DualRL. In auto-\nmatic evaluation, DualRL outperforms SST, but\nDualRL does not share the model parameters of\npositive to negative and negative to positive tasks.\nTherefore, direct comparison is difﬁcult because\nDualRL is regarded as two models.\nWe trained SST by changing the random seed of\nthe model initialization several times and found that\nSST can always yield similar and comparable re-\nsults. SST can be inferred as a stable system for the\nfollowing reasons: (1) G-BLEU: Delete and Gen-\nerate approaches show the stable performance of\nG-BLEU because the methods generate a sentence\nbased on content tokens. There is no guarantee that\ncontent tokens will always be maintained, but con-\ntent tokens help the generator. (2) Attribute: Our\ndelete process is a method of determining whether\ncertain tokens are deleted with Important Score.\nThe direct and model-agnostic deletion is effective\nfor neutralizing sentences. SST also improves a\nstyle accuracy by adding style control loss. (3) Flu-\nency: TemplatedBased, B-GST, and G-GST show\nnon-ideal ﬂuency in d-PPL. TemplatedBased is con-\nsidered unstable because it simply inserts attribute\ntokens of training data when generating test sen-\ntences. Since B-GST and G-GST use pre-trained\nGPT, they also have the ability to predict the distri-\nbution of tokens that are not in training data. The\nability to predict generalized tokens is usually help-\nful, but can sometimes be harmful to d-PPL. SST,\nthe Transformer encoder-decoder structure, learns\nonly the distribution of given data and therefore\nhas a stable d-PPL. (4) Semantic: Transformer\nlanguage modeling is known to perform better on\nvarious tasks than RNN. Even in the style trans-\nfer task, the Transformer-based structures seem to\nreﬂect the linguistic characteristics.\nWe observed that unstable systems performed\npoorly in human evaluation as well in automatic\n201\nContent Attribute Fluency Semantic\nModel s-BLEU h-BLEU G-BLEU Classiﬁer(%) d-PPL g-PPL t-PPL BERTscore\nSST(0.7, 0.5) 39.05 10.85 20.58 79.5 185.26 321.84 244.18 88.72\nSST(0.7, 0.75) 49.09 12.66 24.93 70.4 197.82 295.9 241.94 90.65\nCrossAligned 17.02 4.34 8.59 74.8 69.13 319.1 148.53 88.12\nStyleEmbedding 71.8 13.65 31.3 8.9 121.66 379.81 214.96 90.56\nmultidecoder 40.81 8.24 18.33 46.4 201.59 642.13 359.79 88.35\nTemplateBased 48.67 12.86 25.02 79.7 3258.19 375.62 1106.28 89.71\nDeleteOnly 33.94 9.29 17.75 84.8 171.66 279.55 219.06 89.28\nDeleteAndRetrieve34.48 9.82 18.4 87.7 137.04 343.75 217.04 89.39\nRetrieveOnly 0.88 0.43 0.61 98.4 150.54 150.62 150.58 86.33\nBackTranslation 0.67 0.52 0.59 96.2 30.53 148.77 67.39 87.36\nUnpairedRL 42.29 10.6 21.17 47.7 328.8 735.1 491.63 88.51\nDualRL 58.72 17.71 32.25 86.8 87.72 273.73 154.96 92.14\nB GST 43.45 13.49 24.21 86.1 165.59 184.02 174.57 91.78\nG GST 43.94 13.28 24.15 77.2 441.38 274.25 347.92 91.15\nhuman: DRG 26.97 53.35 37.93 72.8 121.17 153.45 136.36 95.83\nhuman: DualRL 36.79 33.02 34.86 77 178.63 196.15 187.19 95.83\ninput copy 100 21.01 45.84 3.5 69.72 131.91 95.9 93.18\nTable 2: Automatic evaluation results of the Yelp dataset (s: self, h: human, G: geometric mean, f: ﬁne-tuned, p:\npre-trained). The red indicates that the evaluation score is signiﬁcantly worse than other systems. Our model is\nreferred to as SST(α,β). The bold black indicates the better performance of our systems for the four metrics that\ndetermine it is a stable system.\nContent Attribute Fluency Semantic\nModel s-BLEU h-BLEU G-BLEU Classiﬁer(%) d-PPL g-PPL t-PPL BERTscore\nSST(0.6, 0.5) 45.47 20.34 30.41 66.5 4.51 367.73 40.72 89.17\nCrossAligned 0.76 0.61 0.68 74.8 1.11 119.37 11.51 85.95\nStyleEmbedding 32.03 12.95 20.37 42.4 3.42 369.24 35.54 87.39\nmultidecoder 16.48 6.61 10.44 70.3 1.39 343.72 21.86 86.09\nTemplateBased 68.54 33.79 48.12 64.8 5.36 368.41 44.44 90.65\nDeleteOnly 57.48 28.56 40.52 50 2.78 251.24 26.43 90.55\nDeleteAndRetrieve60.75 30.83 43.28 52.4 2.43 221.92 23.22 90.92\nRetrieveOnly 2.82 1.23 1.86 82.3 5.65 135.22 27.64 85.54\nB GST 58.21 25.47 38.5 59.1 12448.44 193.73 1552.94 91.23\nG GST 51.02 21.1 32.81 57.3 18106 458.93 2882.6 89.48\nhuman: DRG 47.67 100 69.04 46.9 12.38 132.18 40.45 100\ninput copy 100 47.6 68.99 18.5 3.76 188.33 26.61 93.77\nTable 3: Automatic evaluation results of the Amazon dataset. Evaluation metrics are the same as Yelp, but Back-\nTranslation, UnpariedRL, and DualRL do not provide results from Amazon datasets.\nModel Content Fluency Style\nSST(0.7, 0.75) 3.32 3.37 3.3\nBackTranslation 2.69 3.15 2.99\nTemplatedBased 3.18 3.16 3.19\nStyleEmbedding 3.56 3.49 2.88\nTable 4: Human evaluation results. The higher the num-\nber, the better. Red means the worst result in the corre-\nsponding evaluation term.\nevaluation. However, since performing human eval-\nuation every time is expensive, choosing a stable\nsystem with automatic evaluation can be helpful.\nTable 5 shows the samples of the generation of\nthe models, which shows the lack of comparison\nmodels. In Yelp’s negative to positive example,\nthere are only SST and DualRL models that change\nthe style while preserving content that includes\ntaste and price of the food. In Yelp’s positive to\nnegative example, the professionals word contains\na combination of style and content. In this case, the\ndeletion and generation framework has the disad-\nvantage of corrupting content information.\n4.7 Ablation Study\nIf we use style loss for SST training, Table 6 shows\nthat the style accuracy has 4 point gain. Fluency\n202\nYelp (negativetopositive) Yelp (positivetonegative)\nInput (source) the food was so-so and veryover pricedfor what you get . these two women areprofessionals.\nSST the service is so-so and veryreasonably pricedfor what you get . these two women arerude.\nCrossAligned the food wasfantasticand very verynicefor what you . these two dogs areharddown .\nStyleEmbedding the food was so-so and veryover pricedfor what you get . these two pot everywhere was .\nDeleteOnly the food was so-so and veryover pricedfor what you get .iwould n’t likethese two women are professionals .\nDeleteAndRetrieve the service isfantasticand the food was so-soand the food isvery pricedfor what you get . these twoscamwomen areprofessionals.\nBack-translation the food isdeliciousand the staff are verygoodfor me . this place is justnot good.\nUnpariredRL the food was so-so and veryover pricedfor whatgreatqualities . these two women aregreat.\nDualRL the food wassurprisingand veryreasonably pricedfor what you get . these two women areunprofessional.\nB-GST the food wasamazing- sofreshand verygoodfor what you get . these two women areterribleliars .\nG-GST the food waspriced right- soniceand verygoodfor what you get . these two women arecondescending.\nHumanDRG the food wasgreatandperfectly priced these two women arenot professionals.\nHumanDualRL the food wasgoodand theprice is low. these two women arenot professionalsat all\nAmazon (negativetopositive) Amazon (positivetonegative)\nInput (source) i have tolowerthe rating another notch . it seems to be of verygoodquality in its build .\nSST lovethe rating another one , it seems to be of verypoorquality in its build .\nCrossAligned i wouldrecommendthis for the price . it snot be for a goodgame for my phone .\nStyleEmbedding i have togetby a one market . it seems to be the numextend isgood nicehigh cases .\nDeleteOnly i have tolowerthe rating and it ﬁts into another notch .i have previously charged numnumdifferent bt headsets that last numnum hours longer .\nDeleteAndRetrieve i have tolowerthe rating another notch and iloveit . initially it was verygoodquality in its build .\nB-GST i havelowerlevels for the other notch . it seems to be of verygoodquality in taste .\nG-GST i havelowerthe steel another notch . it seems to be of verygoodvalue in return .\nHumanDRG i have toraisethe rating another notch . it seems to be of verypoorquality in its build\nTable 5: Examples of comparison of generated sentences of AI systems. SST is our model. Attributes are colored.\nRed is negative and blue is positive.\nCon Attr Flu Sem\nModel G-BLEU Cls(%) t-PPL BERTscore\nSST(0.7, 0) 19.11 82.2 306.65 89.96\n- Style loss 19.78 78.2 341.51 89.84\nTable 6: Ablation result of style loss in the Yelp dataset.\n(Con: content, Attr: attribute, Flu: Fluency, Sem: Se-\nmantic)\nFigure 2: (a) Trade-off curve of G-BLEU and Style ac-\ncuracy according to α(at β= 0.5) in Yelp (b) Trade-off\ncurve of G-BLEU and Style accuracy according to β\n(at α= 0.7) in Yelp.\nand semantic are slightly better. It is observed that\nstyle loss improves the data-ﬂuency, resulting in\nbetter total ﬂuency. However, style loss decreases\nG-BLUE slightly by allowing the transferred sen-\ntence to change the attribute better.\n4.8 Trade-off between Content and Style\nWith αand βwe can simply adjust the trade-off of\ncontent and style. The results of Yelp are shown in\nSource(negative)when i was ﬁnally there , i was very disappointed .\nafter deletion when i ﬁnally , i very .\nstyle: negativewhen i ﬁnally left , i was very disappointed .\n↓\nwhen i ﬁnally left , i was very disappointed.\nwhen i ﬁnally walked in , i was very disappointed .\nwhen i ﬁnally got , i was very happy .\nstyle: positive when i ﬁnally got , i was very happy .\nTable 7: One sample of the Yelp dataset. SST gener-\nates a sentence from style vector space to negative to\npositive\nFig. 2. Smaller αand βallow the model to focus\non style changes, while larger αand β allow the\nmodel to focus on content preserving. The trade-\noff of content and style changes linearly withαand\nis sensitive to β. The appropriate αand βdepend\non datasets.\n4.9 Latent Space Walking\nIn this section we observe the transferred sentences\naccording to the weight of positive and negative in\nthe continuous style vector space. Ideally, a neu-\ntral sentence should be generated when the style\nattribute has the same weight for negative and posi-\ntive. An example is shown in Table 7. A lot of data,\nlike this example, don’t show a neutral sentence\neven if the style has the same weight for the nega-\ntive and positive. If we train our model to reﬂect\nthis problem, we can expect better style control.\n203\n5 Conclusion and Future Work\nWe propose Stable Style Transformer (SST) that re-\nwrites the sentences with Delete and Generate. SST\nis a system that can be used in the real world with\noverall stable results compared to other compara-\nble systems. We show that ﬁltering out unstable\nsystems through human evaluation is expensive, so\nselecting a stable system through automatic eval-\nuation can be helpful. The proposed direct and\nmodel-agnostic deletion method allows the classi-\nﬁer to intuitively delete attribute markers and easily\nhandle the trade-off of content and style. In future\nwork, we will study solutions for the case where at-\ntribute markers also contain content in the deletion\nand generation framework.\nReferences\nNing Dai, Jianze Liang, Xipeng Qiu, and Xuanjing\nHuang. 2019a. Style transformer: Unpaired text\nstyle transfer without disentangled latent represen-\ntation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 5997–6007, Florence, Italy. Association\nfor Computational Linguistics.\nNing Dai, Jianze Liang, Xipeng Qiu, and Xuanjing\nHuang. 2019b. Style transformer: Unpaired text\nstyle transfer without disentangled latent represen-\ntation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 5997–6007, Florence, Italy. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Explo-\nration and evaluation. In Thirty-Second AAAI Con-\nference on Artiﬁcial Intelligence.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P Xing. 2017. Toward\ncontrolled generation of text. In Proceedings\nof the 34th International Conference on Machine\nLearning-Volume 70, pages 1587–1596. JMLR. org.\nYoon Kim. 2014. Convolutional neural networks\nfor sentence classiﬁcation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746–1751,\nDoha, Qatar. Association for Computational Lin-\nguistics.\nGuillaume Lample, Sandeep Subramanian, Eric Smith,\nLudovic Denoyer, Marc’Aurelio Ranzato, and Y-\nLan Boureau. 2019. Multiple-attribute text rewrit-\ning. In International Conference on Learning Rep-\nresentations.\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018a.\nDelete, retrieve, generate: a simple approach to sen-\ntiment and style transfer. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) ,\npages 1865–1874, New Orleans, Louisiana. Associ-\nation for Computational Linguistics.\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018b.\nDelete, retrieve, generate: a simple approach to sen-\ntiment and style transfer. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) ,\npages 1865–1874, New Orleans, Louisiana. Associ-\nation for Computational Linguistics.\nLajanugen Logeswaran, Honglak Lee, and Samy Ben-\ngio. 2018. Content preserving text generation with\nattribute controls. In Advances in Neural Informa-\ntion Processing Systems, pages 5103–5113.\nFuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao\nChang, Xu Sun, and Zhifang Sui. 2019. A dual rein-\nforcement learning framework for unsupervised text\nstyle transfer. In Proceedings of the Twenty-Eighth\nInternational Joint Conference on Artiﬁcial Intel-\nligence, IJCAI-19 , pages 5116–5122. International\nJoint Conferences on Artiﬁcial Intelligence Organi-\nzation.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-\ndinov, and Alan W Black. 2018a. Style transfer\nthrough back-translation. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 866–876, Melbourne, Australia. Association\nfor Computational Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-\ndinov, and Alan W Black. 2018b. Style transfer\nthrough back-translation. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 866–876, Melbourne, Australia. Association\nfor Computational Linguistics.\nAlec Radford. 2018. Improving language understand-\ning by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\n204\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi\nJaakkola. 2017. Style transfer from non-parallel text\nby cross-alignment. In Advances in neural informa-\ntion processing systems, pages 6830–6841.\nAkhilesh Sudhakar, Bhargav Upadhyay, and Arjun Ma-\nheswaran. 2019. “transforming” delete, retrieve,\ngenerate approach for controlled text style transfer.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nXing Wu, Tao Zhang, Liangjun Zang, Jizhong Han,\nand Songlin Hu. 2019. Mask and inﬁll: Apply-\ning masked language model for sentiment transfer.\nIn Proceedings of the Twenty-Eighth International\nJoint Conference on Artiﬁcial Intelligence, IJCAI-\n19, pages 5271–5277. International Joint Confer-\nences on Artiﬁcial Intelligence Organization.\nJingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang, Xu-\nancheng Ren, Houfeng Wang, and Wenjie Li. 2018a.\nUnpaired sentiment-to-sentiment translation: A cy-\ncled reinforcement learning approach. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 979–988, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nJingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang, Xu-\nancheng Ren, Houfeng Wang, and Wenjie Li. 2018b.\nUnpaired sentiment-to-sentiment translation: A cy-\ncled reinforcement learning approach. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 979–988, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. {BERTS}core:\nEvaluating text generation with {bert}. In Interna-\ntional Conference on Learning Representations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8851682543754578
    },
    {
      "name": "Sentence",
      "score": 0.7241105437278748
    },
    {
      "name": "Transformer",
      "score": 0.5981652736663818
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5889803171157837
    },
    {
      "name": "Fluency",
      "score": 0.5594385862350464
    },
    {
      "name": "Encoder",
      "score": 0.5536730885505676
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5515508651733398
    },
    {
      "name": "Natural language processing",
      "score": 0.500638484954834
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4118586778640747
    },
    {
      "name": "Machine learning",
      "score": 0.3276416063308716
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}