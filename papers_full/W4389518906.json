{
    "title": "Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning",
    "url": "https://openalex.org/W4389518906",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2340763649",
            "name": "Duarte Alves",
            "affiliations": [
                "Instituto de Telecomunicações",
                "Iscte – Instituto Universitário de Lisboa"
            ]
        },
        {
            "id": "https://openalex.org/A2117548782",
            "name": "Nuno Guerreiro",
            "affiliations": [
                "CentraleSupélec",
                "Iscte – Instituto Universitário de Lisboa",
                "Instituto de Telecomunicações",
                "Université Paris-Saclay"
            ]
        },
        {
            "id": "https://openalex.org/A2129738116",
            "name": "João Alves",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2151591165",
            "name": "José Pombal",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2121219369",
            "name": "Ricardo Rei",
            "affiliations": [
                "Iscte – Instituto Universitário de Lisboa",
                "Instituto de Engenharia de Sistemas e Computadores Investigação e Desenvolvimento"
            ]
        },
        {
            "id": "https://openalex.org/A2098691967",
            "name": "José De Souza",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2282328084",
            "name": "Pierre Colombo",
            "affiliations": [
                "CentraleSupélec",
                "Université Paris-Saclay"
            ]
        },
        {
            "id": "https://openalex.org/A2122534586",
            "name": "André Martins",
            "affiliations": [
                "Iscte – Instituto Universitário de Lisboa",
                "Instituto de Telecomunicações"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2964303773",
        "https://openalex.org/W3034640977",
        "https://openalex.org/W4378468481",
        "https://openalex.org/W4297833460",
        "https://openalex.org/W2909737760",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W630532510",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W3163443091",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W3105214104",
        "https://openalex.org/W2250342921",
        "https://openalex.org/W3109864162",
        "https://openalex.org/W2958953787",
        "https://openalex.org/W3172669006",
        "https://openalex.org/W4285077564",
        "https://openalex.org/W4321472057",
        "https://openalex.org/W3082675885"
    ],
    "abstract": "International audience",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11127–11148\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSteering Large Language Models for Machine Translation\nwith Finetuning and In-Context Learning\nDuarte M. Alves1,4 Nuno M. Guerreiro1,2,4,5 João Alves2 José Pombal2\nRicardo Rei2,3,4 José G. C. de Souza2 Pierre Colombo5,6 André F. T. Martins1,2,4\n1Instituto de Telecomunicações, Lisbon, Portugal 2Unbabel, Lisbon, Portugal,\n3INESC-ID, Lisbon, Portugal 4Instituto Superior Técnico, University of Lisbon, Portugal\n5MICS, CentraleSupélec, Université Paris-Saclay, France 6Equall, Paris, France\nduartemalves@tecnico.ulisboa.pt\nAbstract\nLarge language models (LLMs) are a promis-\ning avenue for machine translation (MT). How-\never, current LLM-based MT systems are brit-\ntle: their effectiveness highly depends on the\nchoice of few-shot examples and they often re-\nquire extra post-processing due to overgenera-\ntion. Alternatives such as finetuning on transla-\ntion instructions are computationally expensive\nand may weaken in-context learning capabil-\nities, due to overspecialization. In this paper,\nwe provide a closer look at this problem. We\nstart by showing that adapter-based finetuning\nwith LoRA matches the performance of tradi-\ntional finetuning while reducing the number\nof training parameters by a factor of 50. This\nmethod also outperforms few-shot prompting\nand eliminates the need for post-processing or\nin-context examples. However, we show that\nfinetuning generally degrades few-shot perfor-\nmance, hindering adaptation capabilities. Fi-\nnally, to obtain the best of both worlds, we\npropose a simple approach that incorporates\nfew-shot examples during finetuning. Experi-\nments on 10 language pairs show that our pro-\nposed approach recovers the original few-shot\ncapabilities while keeping the added benefits\nof finetuning.1\n1 Introduction\nLarge language models (LLMs) have shown re-\nmarkable performance on a wide range of NLP\ntasks by leveraging in-context learning (Brown\net al., 2020). In particular, when provided with few-\nshot examples, these models have demonstrated im-\npressive capabilities for performing machine trans-\nlation (MT) without requiring explicit supervision\non parallel data (Garcia et al., 2023). However, this\napproach exhibits several drawbacks: performance\nis highly dependent on the quality of examples (Vi-\nlar et al., 2022), outputs are plagued by overgenera-\ntion (Bawden and Yvon, 2023), and inference costs\n1Code avaliable at https://github.com/deep-spin/\ntranslation_llm.\nare greatly increased by processing all input pairs.\nWhen parallel data is available, LLMs can alterna-\ntively be finetuned on translation instructions (Li\net al., 2023). This method generally outperforms\nfew-shot prompting and eliminates the need for\nin-context examples. However, it remains unclear\nwhether finetuned models can benefit from the de-\nsirable properties of in-context learning, such as\non-the-fly domain adaptation (Agrawal et al., 2022).\nAdditionally, traditional finetuning (Devlin et al.,\n2019; Radford et al., 2018) incurs a high computa-\ntional overhead due to the cost of updating all the\nmodel weights.\nIn this paper, we provide a closer examination\nof the impact of finetuning and few-shot prompt-\ning for adapting LLMs to perform translation. Our\nexperiments encompass 10 language pairs on gen-\neral and specific domains, comprising over 100,000\ngenerated translations (§2). Our main findings are:\n• We show that finetuning with adapters (Houlsby\net al., 2019; Hu et al., 2022) is a very effec-\ntive method to steer LLMs for translation (§3.1).\nThis method matches the performance of tradi-\ntional finetuning at a fraction of the computa-\ntional cost, by training 50 times fewer parame-\nters. It also achieves better translation quality\nthan in-context learning and eliminates the need\nfor post-processing the generated outputs and\nselecting in-context examples.\n• We show that finetuning large language models\ndegrades their few-shot performance, limiting\ntheir adaptation capabilities (§3.2). In particular,\nwe show that finetuned LLMs perform poorly\non domain adaptation scenarios when provided\nin-context examples.\n• To address this issue, we propose a simple ap-\nproach that introduces few-shot examples during\nfinetuning (§4). Our results show that we can\nrecover few-shot capabilities while retaining the\nbenefits of finetuning.\n11127\n2 Experimental Setup\nIn our experiments, we use LLaMA 7B and\n13B (Touvron et al., 2023) as backbone language\nmodels and finetune them with the standard cross\nentropy loss.\nWe train our models on general domain\nOPUS (Tiedemann, 2012) data from the Eu-\nroparl, Globalvoices, Paracrawl, Tilde, Ubuntu,\nand Wikipedia domains. We consider the lan-\nguages Dutch (nl), French (fr), German (de), Por-\ntuguese (pt) and Russian (ru), both from and into\nEnglish (en).2 To ensure the quality of the training\nrecords, we first apply Bicleaner (Ramírez-Sánchez\net al., 2020) using a threshold of 0.85 and then filter\nthe remaining pairs, ensuring both language direc-\ntions have a COMETKiwi (Rei et al., 2022b) score\nabove 0.8. Finally, we sample 250K records for\neach language pair. During training, we uniformly\nsample from the data to ensure each language pair\nis seen a similar number of times. We perform val-\nidation on the Flores-200 development set for the\nlanguage pairs in the training data.\nFor in-domain evaluation, we consider the\nFlores-200 (NLLB Team et al., 2022) test dataset\non all the translation directions included during\ntraining, as well as the WMT22 test sets 3 for the\nlanguage pairs considered in our training data. Re-\ngarding data for specialized domains, we consider\nthe Medical and Law domains from Aharoni and\nGoldberg (2020), the TICO dataset (Anastasopou-\nlos et al., 2020) and WMT Chat (Farinha et al.,\n2022). We evaluate our models on zero and five\nshot settings, uniformly sampling for each test sen-\ntence five independent few-shot samples from the\nrespective development set.\nOur main evaluation metric is COMET (Rei\net al., 2020, 2022a) 4. We also report results\nwith BLEU (Papineni et al., 2002), chrF (Popovi´c,\n2015) and COMETKiwi (Rei et al., 2022b) in Ap-\npendix G.\nWe refer the reader to Appendix A for full details\non hyperparameters and instruction formats used\nin the following experiments.\n2We also consider Chinese (zh). However, as it is not\nsupported by LLaMA, we examine it in Appendix B.\n3https://www.statmt.org/wmt22/\ntranslation-task.html\n4We use the latest COMET model wmt22-comet-da from\nversion 2.0.1.\nde-enen-defr-enen-frnl-enen-nlpt-enen-ptru-enen-ru\nLanguage Pair\n80\n82\n84\n86\n88\n90COMET\nPretrained Finetuned LoRA\nFigure 1: COMET scores on the Flores-200 test set\nby LLaMA 7B pretrained (few-shot) and LLaMA 7B\ntrained with full finetuning and LoRA (zero-shot).\n0 8 40 80 120 160\nTraining Examples (in thousands)\n70\n75\n80\n85\n90COMET\nPretrained En-XX\nPretrained XX-En\nFinetuned En-XX\nFinetuned XX-En\nFigure 2: COMET scores for zero-shot evaluation on\nthe Flores-200 test set by LLaMA 7B finetuned with\ndiffering amounts of training data.\n3 Finetuning LLMs on MT instructions\nIn this section, we investigate the performance of\nLLMs finetuned on machine translation instruc-\ntions in relation to few-shot prompting with a pre-\ntrained language model.\nNote that, throughout this section, we always\nanalyse few-shot prompting for the pretrained\nmodel. We deem that this offers a fairer compari-\nson to finetuning on translation instructions, since\nboth methods have access to training examples.\nNevertheless, we also provide the results for\nzero-shot translation with the pretrained model in\nAppendix G. Similar to the findings in Bawden\nand Yvon (2023), zero-shot performance is far be-\nhind few-shot performance, in particular for out-\nof-English language pairs, likely due to the preva-\nlence of English data during the pretraining of the\nLLaMA models.\n11128\n78\n80\n82\n84\n86\n88COMET\nFlores WMT 22 Medical Law Tico\n7B\nChat\nFT w/o\nfew-shot\nFT w/\nfew-shot\n78\n80\n82\n84\n86\n88COMET\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\n13B\nGeneral Domain Specialized Domains\nModel Size\nZero-shot Five-shot\nFigure 3: COMET scores for zero-shot and five-shot translation by models finetuning with and without few-shot\nexamples. Scores are averaged across all language pairs. “FT w/o few-shot” refers to finetuning with translation\ninstructions, as in Section 3. “FT w/ few-shot” refers to finetuning with few-shot examples, detailed in Section 4.\n3.1 Efficient finetuning with LoRA\nWe start by studying parameter efficient training\nwith low-rank adaptation (LoRA) (Hu et al., 2022)\nand compare it with traditional finetuning.5\nIn Figure 1, we observe that LoRA performs\ncomparably to traditional finetuning while train-\ning 50 times fewer parameters.6 We also see that\nboth LoRA and traditional finetuning outperform\nthe pretrained model with few-shot prompts—the\nlatter is consistent with the findings in Li et al.\n(2023), which show that finetuning leads to better\ntranslations than few-shot prompting of pretrained\nlanguage models. As a general trend, all methods\nexhibit better translation quality when translating\ninto English, following recent trends in the litera-\nture (Arivazhagan et al., 2019; Vilar et al., 2022).\nWe also find that finetuning LoRA requires a\nvery small number of translations to obtain the\nreported performance, as shown in Figure 2. In\nparticular, it outperforms the few-shot pretrained\nmodel with as few as 2,000 training examples.\nConsidering the high computational costs of full\nfinetuning compared to parameter-efficient finetun-\ning and the negligible degradation obtained with\nthe LoRA-based model, we use LoRA in subse-\nquent experiments.\n5In this section, we only considered the 7B model due to\ncomputational constraints. Concurrent to our work, Xu et al.\n(2023) showed that LoRA is competitive with finetuning when\napplied to LLaMA 13B.\n6LoRA requires only 134M trainable parameters, whereas\ntraditional finetuning requires 6,7B.\n3.2 Few-shot prompting of finetuned models\nWe now direct our attention to comparing zero-\nand five-shot performance. We argue that, even\nwhen an LLM can achieve high zero-shot trans-\nlation quality, few-shot capabilities can be very\nbeneficial for efficient adaptation. As shown by\nAgrawal et al. (2022), LLMs can leverage a very\nsmall pool of few-shot examples to perform trans-\nlation on new domains.\nIn the leftmost plots of Figure 3, we examine\nthe zero- and few-shot performance of our fine-\ntuned models on general domains. Few-shot per-\nformance degrades and is surpassed by zero-shot\nperformance, suggesting that the finetuning proce-\ndure is hindering the in-context learning abilities.7\nIn order to further study this phenomenon, we\nevaluate the above models on specialized domains.\nGeneral domain examples may be of little help for\na model already trained on that domain. On the\ncontrary, in specialized domains, examples should\nbring domain-specific information about the prop-\nerties of the translation, such as style, register, and\nthus help the model achieve better performance.\nIn the rightmost plots of Figure 3, we observe\nthat the above issue happens consistently in all\ndomains, with a larger degradation in performance.\nThis finding further supports our hypothesis that\nfinetuning can degrade the performance of few-shot\nprompting.\n7Regarding the 13B model, the trends are more visible\nwhen evaluating with COMETKiwi (see Appendix G) which\nis shown to correlate well with human judgements when eval-\nuating LLM based MT systems (Hendy et al., 2023).\n11129\nPretrained FT w/o\nfew-shot\nFT w\nfew-shot\nReference\n0\n50\n100\n150\n200Translation Length\nFigure 4: Length of the tokenized outputs when trans-\nlating the Flores-200 test set for the 7B models.\n4 Finetuning with few-shot examples\nIn order to recover few-shot performance, we in-\ntroduce instructions with few-shot examples in\nthe training process: namely, we finetune on data\nwhich contains both zero-shot and few-shot instruc-\ntions. Following Min et al. (2022), we uniformly\nsample between 0 and 5 few-shot examples for\neach training example from an example pool previ-\nously separated from the training data.8 From here,\nwe build an instruction prompt with the training\nexample and the selected examples and proceed\nwith the training.\nIn Figure 3, we observe that the models trained\nwith in-context examples recover their few-shot\ncapabilities, both for the general and specialized\ndomains. The few-shot performance is on par or\nabove the zero-shot performance, further suggest-\ning that the models are extracting helpful informa-\ntion from the examples. In Appendix D, we present\na set of examples that highlight these gains.\n4.1 Analysis on output format\nWe also analyze whether finetuned models continue\nto generate context after the desired translation.\nThis issue is present in pretrained LLM outputs and\nrequires post-processing of the generated content,\ndeleting all words generated after the first new line.\nIn Figure 4, we show the length of the tokenized\noutputs for the 7B models. 9 We observe that the\ndistribution of the length for the outputs generated\nby both finetuned models matches the distribution\nof the references. This shows that the finetuned\n8We also considered a training mixture where 50% of\nthe data contained no examples and the remaining data had\nbetween 1 and 5 uniformly sampled examples. We did not\nfurther explore this as preliminary results (see Appendix C)\nshow the results are similar to the ones obtained with the\nprocedure above.\n9The 13B models follow a similar distribution.\nde-enen-defr-enen-frnl-enen-nlpt-enen-ptru-enen-ru\nLanguage Pair\n40\n20\n0\n20\n40\nCOMET Score \nFigure 5: COMET score difference for zero- vs few-\nshot translations on Flores-200 by the 7B FT w/ few-\nshot model (∆ > 0 indicates higher score for few-shot\ntranslations).\nmodels no longer overgenerate.\nWe also found that these models no longer de-\nlimit their output with the newline symbol and in-\nstead produce the end of sentence token, removing\nthe necessity for post-processing and increasing\ncomputational efficiency. In Appendix F, we pro-\nvide a set of examples to illustrate these findings.\n4.2 Influence of in-context examples\nIn order to obtain a more fine-grained analysis of\nthe gains obtained by adding in-context examples,\nwe analyzed the difference in COMET scores for\neach source sentence when prompting the 7B fine-\ntuned models with and without examples.\nIn Figure 5, we observe that the distributions\nhave a high concentration of points slightly above\n0. However, we also observe very large tails, in\nparticular for out-of-English language pairs.10\nWe manually inspected the examples with the\nhighest differences11 and found that introducing\nexamples can fix the model generating in the wrong\nlanguage, supporting the findings in Bawden and\nYvon (2023). Surprisingly, we also discovered\nexamples where the model correctly generated a\ntranslation in a zero-shot scenario and inserting\nin-context examples lead to hallucinated content.\nTo better characterize this phenomenon, we take\ninspiration from analysis on hallucinations under\nperturbation (Lee et al., 2018), and measured how\nmany times prompting the model without examples\nlead to a translation above 30 BLEU points, and\nintroducing examples reduced the score to below\n10In Appendix E, we show that the model finetuned without\nexamples also has the same behavior.\n11We show several extracted examples in Appendix E.\n11130\n7B 13B\nDomain FT w/o few-shot FT w/ few-shot FT w/o few-shot FT w/ few-shot\nFlores 0.12% 0.00% 0.02% 0.00%\nMedical 1.09% 0.05% 0.29% 0.00%\nLaw 2.70% 0.05% 0.48% 0.15%\nTico 0.60% 0.04% 0.04% 0.04%\nChat 1.80% 0.23% 0.51% 0.00%\nTable 1: Hallucination Rates for finetuned models on each evaluation dataset, considering all languages pairs.\n3 (these thresholds were selected based on previ-\nous work (Lee et al., 2018; Raunak et al., 2021;\nGuerreiro et al., 2023))12.\nIn Table 1, we see that the models finetuned with-\nout examples have higher hallucination rates than\ntheir respective counterparts, further showing their\ndegradation in few-shot performance. Through\na manual inspection of the obtained outputs, we\nobserved that the models generate hallucinations\nof different categories. In particular, they gener-\nate both detached (fully and strongly) and oscilla-\ntory hallucinations, and can also generate off-target\ntranslations. One common case is that the models\ncopy from the instruction (either from the source\nor the examples).\nThe models finetuned with few-shot examples\nexhibit lower hallucination rates, suggesting that\nthe training procedure reduced the prevalence of\nthis issue. In particular, these models no longer\ncopy from the instruction. However, they still pro-\nduce hallucinations and their impact is very serious.\nAs such, we believe that it motivates further study\non the influence of in-context examples and the\ngenerated output.\n5 Conclusion\nIn this paper, we provide a study on finetuning\nand few-shot prompting for adapting LLMs for\ntranslation. We show that adapter-based finetuning\nmatches the performance of traditional finetuning\nwhile training 50 times fewer parameters. Addition-\nally, finetuning with adapters outperforms few-shot\nprompting of large language models and eliminates\nthe need for output post-processing and in-context\nexamples.\nIn addition, we show that finetuned models\nexhibit poor performance when prompted with\n12Note that this analysis is similar to that of hallucinations\nunder perturbation, when considering the introduction of the\nexamples as the input perturbation.\nin-context examples. To address this issue, we\npropose a simple approach that mixes few-shot\nprompts during finetuning. Our results show that\nwe recover the original few-shot capabilities and\nretain the benefits of finetuning.\nLimitations\nIn this paper, we focus on English-centric high-\nresource language pairs. It remains an open ques-\ntion how these findings generalize for non-English\nlanguage pairs or in low-resource settings.\nWe also do not perform a human assessment on\nthe quality of the translations quality due to the\ntime and cost of performing this study. Instead, we\nbase our evaluation on COMET, a state-of-the-art\nmetric for MT evaluation, and provide results for\nother metrics in Appendix G.\nEthics Statement\nThis paper is based on large language models.\nThese models can encompass several risks, which\nare discussed in detail in Brown et al. (2020) and\nChowdhery et al. (2022). Namely, they are trained\non large web corpora, which can contain toxic con-\ntent (Gehman et al., 2020), and have a high energy\nconsumption, in particular during training (Strubell\net al., 2019).\nAdditionally, our evaluation is based on auto-\nmatic metrics finetuned based on human prefer-\nences. In such cases, annotators may not con-\nsider better alternatives when evaluating generated\ntext and wrongfully classify the text as high qual-\nity (Bansal et al., 2021).\nAcknowledgements\nThis work was supported by EU’s Horizon Eu-\nrope Research and Innovation Actions (UTTER,\ncontract 101070631), by the project DECOL-\nLAGE (ERC-2022-CoG 101088763), by Fun-\ndação para a Ciência e Tecnologia through con-\n11131\ntract UIDB/50008/2020, and by the Portuguese\nRecovery and Resilience Plan through project\nC645008882- 00000055 (Center for Responsi-\nble AI). Part of this work was performed using\nHPC resources from GENCI-IDRIS (Grants 2022-\nAD01101838, 2023-103256 and 2023-101838).\nReferences\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke\nZettlemoyer, and Marjan Ghazvininejad. 2022. In-\ncontext examples selection for machine translation.\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7747–\n7763, Online. Association for Computational Lin-\nguistics.\nAntonios Anastasopoulos, Alessandro Cattelan, Zi-\nYi Dou, Marcello Federico, Christian Federmann,\nDmitriy Genzel, Francisco Guzmán, Junjie Hu, Mac-\nduff Hughes, Philipp Koehn, Rosie Lazar, Will\nLewis, Graham Neubig, Mengmeng Niu, Alp Ök-\ntem, Eric Paquin, Grace Tang, and Sylwia Tur. 2020.\nTICO-19: the Translation initiative for COvid-19.\narXiv:2007.01788.\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat,\nDmitry Lepikhin, Melvin Johnson, Maxim Krikun,\nMia Xu Chen, Yuan Cao, George F. Foster, Colin\nCherry, Wolfgang Macherey, Zhifeng Chen, and\nYonghui Wu. 2019. Massively multilingual neural\nmachine translation in the wild: Findings and chal-\nlenges. CoRR, abs/1907.05019.\nGagan Bansal, Tongshuang Wu, Joyce Zhou, Ray-\nmond Fok, Besmira Nushi, Ece Kamar, Marco Tulio\nRibeiro, and Daniel Weld. 2021. Does the whole\nexceed its parts? the effect of ai explanations on\ncomplementary team performance. In Proceedings\nof the 2021 CHI Conference on Human Factors in\nComputing Systems, CHI ’21, New York, NY , USA.\nAssociation for Computing Machinery.\nRachel Bawden and François Yvon. 2023. Investigating\nthe translation performance of a large multilingual\nlanguage model: the case of bloom.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Advances\nin neural information processing systems, volume 33,\npage 1877–1901. Curran Associates, Inc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAna C Farinha, M. Amin Farajian, Marianna Buchic-\nchio, Patrick Fernandes, José G. C. de Souza, He-\nlena Moniz, and André F. T. Martins. 2022. Find-\nings of the WMT 2022 shared task on chat transla-\ntion. In Proceedings of the Seventh Conference on\nMachine Translation (WMT), pages 724–743, Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nXavier Garcia, Yamini Bansal, Colin Cherry, George\nFoster, Maxim Krikun, Fangxiaoyu Feng, Melvin\nJohnson, and Orhan Firat. 2023. The unreasonable\neffectiveness of few-shot learning for machine trans-\nlation.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nNuno M. Guerreiro, Duarte Alves, Jonas Waldendorf,\nBarry Haddow, Alexandra Birch, Pierre Colombo,\nand André F. T. Martins. 2023. Hallucinations in\nlarge multilingual translation models.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Hassan\n11132\nAwadalla. 2023. How good are gpt models at ma-\nchine translation? a comprehensive evaluation.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In\nProceedings of the 36th International Conference\non Machine Learning, volume 97 of Proceedings\nof Machine Learning Research, pages 2790–2799.\nPMLR.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nKatherine Lee, Orhan Firat, Ashish Agarwal, Clara Fan-\nnjiang, and David Sussillo. 2018. Hallucinations in\nneural machine translation.\nJiahuan Li, Hao Zhou, Shujian Huang, Shanbo Chen,\nand Jiajun Chen. 2023. Eliciting the translation abil-\nity of large language models via multilingual finetun-\ning with translation instructions.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048–11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur\nÇelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-\nfernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\n2022. No language left behind: Scaling human-\ncentered machine translation.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMaja Popovi´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nGema Ramírez-Sánchez, Jaume Zaragoza-Bernabeu,\nMarta Bañón, and Sergio Ortiz Rojas. 2020. Bifixer\nand bicleaner: two open-source tools to clean your\nparallel data. In Proceedings of the 22nd Annual\nConference of the European Association for Machine\nTranslation, pages 291–298, Lisboa, Portugal. Euro-\npean Association for Machine Translation.\nVikas Raunak, Arul Menezes, and Marcin Junczys-\nDowmunt. 2021. The curious case of hallucinations\nin neural machine translation. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1172–1183,\nOnline. Association for Computational Linguistics.\nRicardo Rei, José G. C. de Souza, Duarte Alves,\nChrysoula Zerva, Ana C Farinha, Taisiya Glushkova,\nAlon Lavie, Luisa Coheur, and André F. T. Martins.\n2022a. COMET-22: Unbabel-IST 2022 submission\nfor the metrics shared task. In Proceedings of the\nSeventh Conference on Machine Translation (WMT),\npages 578–585, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685–2702, Online. Association\nfor Computational Linguistics.\nRicardo Rei, Marcos Treviso, Nuno M. Guerreiro,\nChrysoula Zerva, Ana C Farinha, Christine Maroti,\nJosé G. C. de Souza, Taisiya Glushkova, Duarte\nAlves, Luisa Coheur, Alon Lavie, and André F. T.\nMartins. 2022b. CometKiwi: IST-unbabel 2022 sub-\nmission for the quality estimation shared task. In\nProceedings of the Seventh Conference on Machine\nTranslation (WMT), pages 634–645, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for Com-\nputational Linguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation (LREC’12), pages 2214–2218, Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\n11133\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,\nViresh Ratnakar, and George Foster. 2022. Prompt-\ning palm for translation: Assessing strategies and\nperformance.\nHaoran Xu, Young Jin Kim, Amr Sharaf, and Hany Has-\nsan Awadalla. 2023. A paradigm shift in machine\ntranslation: Boosting translation performance of\nlarge language models.\n11134\nTranslate the source text from X to Y .\nSource: ...\nTarget:\nTable 2: Prompting template for finetuning without few-\nshot examples.\nA Details on experimental setup\nA.1 Instruction format\nThe training data for finetuning without few-shot\nexamples follows the template shown in Table 2.\nThe same format is used when testing all models\nin a zero-shot setting.\nWe treat the few-shot instruction template as\na hyperparameter and experiment with three dif-\nferent methods, as shown in Table 3. Our first\ntemplate follows recent trends in the literature and\nrepeats the zero-shot instruction for each example\n(Vilar et al., 2022). However, in our experiments,\nwe found that pretrained language models see the\nrepeating pattern and continue to generate more\nexamples besides the target translation. In order\nto circumvent this issue in the finetuned models,\nwe designed the two remaining templates with sep-\narate examples sections. Our goal was to better\nseparate the examples from the input and thus re-\nduce the propensity for overgeneration. We found\nthat all templates lead to overgeneration with the\npretrained model and none suffered from this issue\nwhen the model is finetuned.\nIn order to select the template format for our\nremaining experiments, we test them by finetuning\nwith examples the LLaMA 7B model and choos-\ning the template with the highest average COMET\nscore on the languages in the validation set. In or-\nder to collect examples for few-shot prompting in\nthe validation set, we sampled from the validation\nset ensuring the predicted example was not in the\nin-context examples.\nIn Table 4, we observe that the templates lead to\nvery similar results, suggesting that the finetuning\nprocedure is not very sensitive to the template used.\nNevertheless, their ranking is consistent across met-\nrics, with the second one obtaining the best scores.\nAs such, we use it when prompting models in a\nfew-shot scenario.\nA.2 Training hyperparameters\nIn order to choose the best hyperparameters for\nboth finetuning approaches, we perform a hyper-\nzh-en en-zh\nLanguage Pair\n60\n65\n70\n75\n80\n85COMET\nPretrained Finetuned LoRA\nFigure 6: COMET scores on the Chinese language pairs\nof the Flores-200 test set by LLaMA 7B trained with\nfull finetuning and LoRA.\nparameter search by finetuning the LLaMA 7B\nmodel with each configuration on the training data.\nWe only consider zero-shot translation and use the\ntemplate format in Table 2. We find the best con-\nfiguration based on the average COMET score on\nall language pairs in the validation set.\nTable 5 specifies the hyperparameters experi-\nmented when training LLaMA 7B with traditional\nfinetuning. We first chose the learning rate and\nweight decay, while not using warm-up steps. We\nthen tuned the scheduler and warm-up steps. Our\nfinal configuration has a learning rate of 1e-6, no\nweight decay, and a constant learning scheduler\nwith no warm-up steps.\nTable 6 details the hyperparameters experi-\nmented when finetuning with LoRA. We based our\nexperiments on the best configurations for the GPT-\n2 models trained in Hu et al. (2022). Initial experi-\nments with lower r values lead to an underfitting\nmodel so our configurations focused on increasing\nmodel capacity, with higherr values, while keeping\nregularization through label smoothing and weight\ndecay. In Table 7, we present the results for all the\nruns. We saw very little variation on the obtained\nscores. We adopted the best configuration, with\nan r value of 256, weight decay of 0.0, and label\nsmoothing of 0.001.\nRegarding the 13B models, we used the same\nhyperparameters as in the 7B models.\nB Analysis on Chinese language pairs\nIn this section, we explore the results for the lan-\nguage pairs including Chinese with the LLaMA\n7B model, in order to study if our previous results\nhold.\n11135\nFormat 1 Format 2 Format 3\nTranslate the source text from X to Y .\nSource: ...\nTarget: ...\n...\nTranslate the source text from X to Y .\nSource: ...\nTarget: ...\nTranslate the source text from X to Y .\nSource: ...\nTarget:\nConsider the following N translations\nfrom X to Y .\nExample 1\nSource: ...\nTarget: ...\n...\nExample N\nSource: ...\nTarget: ...\nTranslate the source text from X\nto Y .\nSource: ...\nTarget:\nConsider the following translations\nfrom X to Y .\nSource: ...\nTarget: ...\n...\nSource: ...\nTarget: ...\nTranslate the source text from X\nto Y .\nSource: ...\nTarget:\nTable 3: Prompting templates for finetuning with in-context examples.\nFormat COMET COMETKiwi BLEU chrF\nFormat 1 85.25 82.49 32.44 57.25\nFormat 2 85.34 82.54 32.62 57.37\nFormat 3 85.27 82.51 32.39 57.22\nTable 4: Scores for the few-shot formats on the Flores-200 validation set.\nFT w/o few-shot FT w/ few-shot\n0.74\n0.75\n0.76\n0.77\n0.78COMET\nZero-shot Five-shot\nFigure 7: COMET scores for Chinese language pairs\nby the 7B finetuned models on zero-shot and five-shot\nscenarios for the Flores-200 test set.\nWe start by investigating whether LoRA is still\ncompetitive with full finetuning. In Figure 6, we\nobserve that LoRA performs comparably to the\nfinetuned model and outperforms the pretrained\nLLM, following the trend of other language pairs\n(see Section 3.1).\nWe also investigate the performance of the mod-\nels finetuned with and without examples. In Fig-\nure 7, we observe a similar trend to the results\nabove. The model finetuned without few-shot ex-\namples exhibits a performance degradation, while\nthe model finetuned with few-shot examples ob-\ntains higher performance with few-shot prompting,\nindicating it is extracting helpful information from\nthe examples in the prompt.\nC Experiments with more zero-shot data\nWe explored an alternative method for combining\nfew-shot examples during finetuning. Instead of\nuniformly sampling between 0 and 5 examples, we\nbuild a training mixture where 50% of the training\nexamples were zero-shot and the remaining ones\nhad between 1 and 5 uniformly sampled examples.\nIn Figure 8, we compare the training mixes by\nfinetuning LLaMA 7B. We see that the results are\nvery similar for both configurations. The alterna-\ntive configuration (Unbalanced) obtains slightly\nlower results. As such, we adopted the method de-\nscribed in Section 4 for mixing few-shot examples\nduring finetuning.\nD Examples of domain adaptation\nIn this section, we provide examples of translations\nwhere the LLaMA 7B model trained with few-shot\nexample was able to absorb domain knowledge\nfrom the examples in the prompt.\nIn the first example from Table 8, we see that the\nmodel correctly translates the terminology GVO to\nGMOs (Genetically Modified Organisms), instead\nof adopting the acronym in the source sentence. In\nthe second example, the model is able to correctly\norder the words in the translation.\n11136\nOptimizer AdamW\nBatch Size 256\nLearning Rate 5e-3, 2e-4, 5e-4,\n2e-5, 5e-5, 1e-6\nScheduler Constant, Cosine, Linear\nWarm-up Steps 0, 1000, 2000\nWeight Decay 0.0, 0.1\nTable 5: Hyperparameters for traditional finetuning\nexperiments.\nOptimizer AdamW\nBatch Size 8\nLearning Rate 2e-4\nScheduler Linear\nWarm-up Steps 500\nDropout 0.05\nr 128, 256\nα 2 · r\nLabel Smoothing 0.01, 0.05, 0.1, 0.2\nWeight Decay 0.0, 0.1\nTable 6: Hyperparameters for LoRA experiments.\nLoRA-R Weight Decay Label Smoothing COMET COMETKiwi BLEU chrF\n128 0.0 0.0 84.74 81.96 31.47 56.36\n128 0.1 0.0 84.76 81.98 31.45 56.34\n128 0.0 0.01 84.79 81.99 31.48 56.37\n128 0.0 0.05 84.80 82.00 31.28 56.30\n128 0.0 0.1 84.61 81.85 31.10 56.15\n128 0.0 0.2 84.36 81.61 30.71 55.89\n256 0.0 0.0 84.78 82.01 31.47 56.40\n256 0.1 0.0 84.72 81.94 31.41 56.32\n256 0.0 0.01 84.87 82.08 31.57 56.45\n256 0.0 0.05 84.78 81.97 31.47 56.39\n256 0.0 0.1 84.65 81.92 31.28 56.25\n256 0.0 0.2 84.48 81.77 30.95 56.01\nTable 7: Scores for the LoRA hyperparameters on the Flores-200 validation set.\nE Analysis on the distributions of\nCOMET score differences\nWe also provide a more in-depth analysis on the\ndistributions of COMET score differences, with a\nfocus on the examples with the highest differences.\nIn Figure 9, we observe that the distributions for\nthe LLaMA 7B model finetuned without in-context\nexamples also have large tails, similar to the results\nof the model finetuned with in-context examples\n(see in Section 4.2).\nWe also analyzed whether the same long tails\nappear on the specialized domains. In Figure 10,\nwe observe that this is in fact the case. The dis-\ntributions of the differences are centered around\nzero and have extreme values on both sides for all\ndomains and finetuned models.\nFinally, we show several examples where few-\nshot prompting both helped or degraded the model\nperformance. In Table 9, prompting the model\nwith few-shot examples fixed the generation in\nthe wrong language. In Table 10, introducing in-\ncontext examples in the model prompt leads to\nhallucinated content.\nF Examples of generated outputs\nIn this section, we present translations where\nprompting the pretrained LLaMA 7B model leads\nto overgeneration, and both 7B finetuned models\ncorrectly stopped to translate. In Table 11, we see\nthat, although all models generated the same trans-\nlation, the pretrained model continued to generate,\nrepeating the prompt and translation, while both\nfinetuned models correctly stopped to generate to-\nkens.\nG Results with all evaluation metrics\nWe provide the evaluation for the models con-\nsidered in this paper using three other MT eval-\nuation metrics: BLEU (Papineni et al., 2002),\nchrF (Popovi´c, 2015) and COMETKiwi (Rei et al.,\n2022b).\nIn Figure 11, we show the comparison between\nboth finetuning approaches with the LLaMA 7B\nmodel. The results are consistent across all metrics,\nwith the LoRA model performing similarly to the\nfinetuned and outperforming the pretrained model.\nIn Figures 12, 13 and 13, we compare finetun-\n11137\nBalanced Unbalanced\n80\n82\n84\n86\n88COMET\nFlores\nBalanced Unbalanced\nMedical\nBalanced Unbalanced\nLaw\nBalanced Unbalanced\nGeneral Domain Specialized Domains\nTico\nZero-shot Five-shot\nFigure 8: COMET scores for zero-shot and five-shot translation by finetuning the LLaMA 7B model with the two\nmethods for combining few-shot examples. Balanced is the method described in Section 4 and Unbalanced is the\nalternative method described in Appendix C.\nSource “bezeichnet “genetisch veränderte Futtermittel” Futtermittel, die GVO enthalten, daraus bestehen\noder hergestellt werden;”\nReference ““genetically modified feed” means feed containing, consisting of or produced from GMOs;”\nZero-shot translation ““Genetically modified feed” means feed containing GVO, derived from GVO or produced from\nGVO;”\nFew-shot translation ““genetically modified feed” means feed containing, consisting of or produced from GMOs;”\nSource “VERORDNUNG (EG) Nr. 538/2000 DER KOMMISSION”\nReference “COMMISSION REGULATION (EC) No 538/2000”\nZero-shot translation “(EG) No 538/2000 OF THE COMMISSION”\nFew-shot translation “COMMISSION REGULATION (EC) No 538/2000”\nTable 8: Examples of translations where the LLaMA 7B finetuned with few-shot examples was able to extract\ndomain information from the examples in the prompt.\nde-enen-defr-enen-frnl-enen-nlpt-enen-ptru-enen-ru\nLanguage Pair\n40\n20\n0\n20\n40\nCOMET Score \nFigure 9: Difference in COMET scores for zero- vs few-\nshot translations by the LLaMA 7B FT w/o few-shot\nmodel on Flores-200 ( ∆ > 0 means that the transla-\ntion with few-shot examples was scored higher than the\ntranslation without examples).\ning with and without examples. We observe that\nthe results with COMETKiwi follow the trends\nobtained with COMET, with a performance degra-\ndation when few-shot prompting the model trained\nwith examples and a recovery of the performance\nwhen prompting with few-shot examples.\nFor the lexical metrics, the degradation in few-\nshot performance is not visible on the 13B models.\nHowever, these metrics may not be reliable for eval-\nuating translations from LLMs (Hendy et al., 2023),\nas LLMs tend to produce less literal translations\nwhich are poorly captured by lexical overlap with\nthe reference.\nIn Tables 12, 13, 14, 15, 16, 17 and 18 we also\nprovide the exact scores for all metrics in a tabular\nformat.\n11138\nFlores Medical Law Tico Chat\n60\n40\n20\n0\n20\n40\nCOMET Score \nFT w/o few-shot\nFlores Medical Law Tico Chat\nFT w/ few-shot\nFigure 10: Difference in COMET scores for translations obtained with zero- and few-shot prompting for all domains\nfor the finetuned LLaMA 7B models.\nSource May I take your order number please?\nReference Darf ich bitte Ihre Bestellnummer haben?\nZero-shot translation May I take your order number please?\nFew-shot translation Bitte geben Sie mir Ihren Bestellnummer an.\nSource May i know the class you’re referring to please\nReference A qual aula você está se referindo?\nZero-shot translation May i know the class you’re referring to please\nFew-shot translation Poderia eu saber a classe que você está se referindo por favor\nSource Thanks, so you are still waiting for the #PRS_ORG#-Pendelleuchte, Messing und mehrfarbiges\nGlas?\nReference Vielen Dank, also wartest du immer noch auf die #PRS_ORG#-Pendelleuchte, Messing und\nmehrfarbiges Glas?\nZero-shot translation Thanks, so you are still waiting for the #PRS_ORG#-Pendant lamp, brass and multi-coloured\nglass?\nFew-shot translation Danke, so warten Sie noch auf die #PRS_ORG#-Pendelleuchte, Messing und mehrfarbiges\nGlas?\nTable 9: Examples of translations by the 7B FT w/o few-shot model where adding examples corrected the language\nin which the model was generating.\nSource “Age: Age was the most important factor for the prognosis of SARS, which is also true for\nCOVID-19.”\nReference “Âge : l’âge constituait le principal facteur pour le pronostic du SRAS, et cela semble également\nêtre le cas pour la COVID-19.”\nZero-shot translation “Â Age : L’âge est le facteur le plus important pour la prognostication du SRAS, qui est\négalement vrai pour le COVID-19.”\nFew-shot translation “Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â\nÂ Â Â Â Â Â Â Â Â Â Â”\nSource “Chinese people as well as other Asians in the United Kingdom and the United States have\nreported increasing levels of racist abuse, as well as assaults.”\nReference “O povo chinês, assim como outros asiáticos no Reino Unido e nos Estados Unidos, relataram\nníveis crescentes de insultos racistas, bem como de ataques.”\nZero-shot translation “Pessoas chinesas, bem como outros asiáticos no Reino Unido e nos Estados Unidos, relataram\num aumento de abusos racistas, bem como assaltos.”\nFew-shot translation “Em 6 de abril, as férias foram estendidas em todas as escolas de nível médio do Turcomenistão.”\nTable 10: Examples of translations by the 7B FT w/o few-shot model where adding examples introduced an\nhallucination.\n11139\nSource ““We now have 4-month-old mice that are non-diabetic that used to be diabetic” he added.”\nReference ““Agora temos ratos de 4 meses de idade que não são diabéticos e que antes eram diabéti-\ncos,”complementou.”\nPretrained ““Agora temos ratos de 4 meses que não são diabéticos que eram diabéticos”, acrescen-\ntou.\\n\\nTranslate the source text from English to Portuguese.\\nSource: “We now have 4-month-\nold mice that are non-diabetic that used to be diabetic,” he added.\\nTarget: “Agora temos ratos\nde 4 meses que não são diabéticos que eram di”\nFT w/o Examples ““Agora temos ratos de 4 meses que não são diabéticos que eram diabéticos”, acrescentou.”\nFT w/ Examples ““Agora temos ratos de 4 meses que não são diabéticos que eram diabéticos”, acrescentou.”\nTable 11: Examples of translations where finetuning the LLaMA 7B model eliminated the overgeneration in the\noutputs.\nde-enen-defr-enen-frnl-enen-nlpt-enen-ptru-enen-ruzh-enen-zh\nLanguage Pair\n60\n65\n70\n75\n80\n85\n90\nCOMETKiwi\nPretrained Finetuned LoRA\nde-enen-defr-enen-frnl-enen-nlpt-enen-ptru-enen-ruzh-enen-zh\nLanguage Pair\n0\n10\n20\n30\n40\n50\nBLEU\nde-enen-defr-enen-frnl-enen-nlpt-enen-ptru-enen-ruzh-enen-zh\nLanguage Pair\n0\n20\n40\n60\n80\nchrF\nFigure 11: Scores of the 7B pretrained model (few-shot prompting) and both 7B finetuned models (zero-shot\nprompting) on the Flores-200 test set.\n74\n76\n78\n80\n82\n84\n86COMETKiwi\nFlores WMT 22 Medical Law Tico\n7B\nChat\nFT w/o\nfew-shot\nFT w/\nfew-shot\n74\n76\n78\n80\n82\n84\n86COMETKiwi\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\n13B\nGeneral Domain Specialized Domains\nModel Size\nZero-shot Five-shot\nFigure 12: COMETKiwi scores for zero-shot and five-shot translation by models finetuning with and without\nfew-shot examples. Scores are averaged across all language pairs. “FT w/o few-shot” refers to finetuning with\ntranslation instructions, as in Section 3. “FT w/ few-shot” refers to finetuning with few-shot examples, detailed in\nSection 4.\n11140\n20\n30\n40BLEU\nFlores WMT 22 Medical Law Tico\n7B\nChat\nFT w/o\nfew-shot\nFT w/\nfew-shot\n20\n30\n40BLEU\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\n13B\nGeneral Domain Specialized Domains\nModel Size\nZero-shot Five-shot\nFigure 13: BLEU scores for zero-shot and five-shot translation by models finetuning with and without few-shot\nexamples. Scores are averaged across all language pairs. “FT w/o few-shot” refers to finetuning with translation\ninstructions, as in Section 3. “FT w/ few-shot” refers to finetuning with few-shot examples, detailed in Section 4.\n50\n60chrF\nFlores WMT 22 Medical Law Tico\n7B\nChat\nFT w/o\nfew-shot\nFT w/\nfew-shot\n50\n60chrF\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\nFT w/o\nfew-shot\nFT w/\nfew-shot\n13B\nGeneral Domain Specialized Domains\nModel Size\nZero-shot Five-shot\nFigure 14: chrF scores for zero-shot and five-shot translation by models finetuning with and without few-shot\nexamples. Scores are averaged across all language pairs. “FT w/o few-shot” refers to finetuning with translation\ninstructions, as in Section 3. “FT w/ few-shot” refers to finetuning with few-shot examples, detailed in Section 4.\n11141\nCOMET COMETKiwi BLEU chrF\nLanguage Pair Model Context\nde-en Pretrained Zero-Shot 87.23 82.99 36.77 62.34\nFive-Shot 88.06 83.49 39.01 64.42\nFinetuned Zero-Shot 88.66 83.95 41.05 66.17\nLoRA Zero-Shot 88.66 83.93 41.22 66.15\nen-de Pretrained Zero-Shot 71.56 57.87 13.99 38.73\nFive-Shot 82.74 79.82 24.77 54.00\nFinetuned Zero-Shot 83.47 81.19 28.30 57.69\nLoRA Zero-Shot 83.63 81.30 28.15 57.62\nfr-en Pretrained Zero-Shot 87.04 84.61 37.27 62.79\nFive-Shot 88.41 85.65 40.71 65.85\nFinetuned Zero-Shot 88.80 86.02 42.99 67.30\nLoRA Zero-Shot 88.77 86.04 42.93 67.39\nen-fr Pretrained Zero-Shot 76.54 65.17 23.43 47.02\nFive-Shot 84.52 84.53 36.71 61.40\nFinetuned Zero-Shot 85.39 85.28 39.23 64.10\nLoRA Zero-Shot 85.36 85.10 39.69 64.43\nnl-en Pretrained Zero-Shot 84.60 83.59 25.42 53.94\nFive-Shot 86.46 84.99 28.55 57.22\nFinetuned Zero-Shot 86.44 85.22 29.43 58.26\nLoRA Zero-Shot 86.40 85.27 29.78 58.30\nen-nl Pretrained Zero-Shot 73.99 61.17 12.45 38.88\nFive-Shot 83.12 81.86 19.26 50.58\nFinetuned Zero-Shot 83.40 82.63 20.71 53.10\nLoRA Zero-Shot 83.10 82.50 20.46 52.79\npt-en Pretrained Zero-Shot 87.26 83.21 41.14 65.78\nFive-Shot 88.57 84.22 44.35 68.49\nFinetuned Zero-Shot 88.88 84.60 46.58 70.13\nLoRA Zero-Shot 88.91 84.68 46.32 70.16\nen-pt Pretrained Zero-Shot 74.60 55.91 17.46 40.46\nFive-Shot 86.43 82.03 35.25 61.41\nFinetuned Zero-Shot 87.11 82.86 38.75 64.45\nLoRA Zero-Shot 86.93 82.67 38.94 64.57\nru-en Pretrained Zero-Shot 83.85 82.03 28.37 57.00\nFive-Shot 85.21 83.11 31.08 59.25\nFinetuned Zero-Shot 85.54 83.49 32.70 60.12\nLoRA Zero-Shot 85.42 83.48 32.28 59.59\nen-ru Pretrained Zero-Shot 57.63 46.49 7.16 17.48\nFive-Shot 83.44 79.45 19.28 47.60\nFinetuned Zero-Shot 84.91 81.06 20.76 49.32\nLoRA Zero-Shot 84.73 80.74 20.88 49.53\nzh-en Pretrained Zero-Shot 76.74 77.46 12.54 34.99\nFive-Shot 82.27 79.55 19.23 48.13\nFinetuned Zero-Shot 83.68 80.78 21.73 49.88\nLoRA Zero-Shot 83.15 80.44 21.61 49.71\nen-zh Pretrained Zero-Shot 49.12 45.10 4.54 7.20\nFive-Shot 66.04 61.20 14.68 15.11\nFinetuned Zero-Shot 72.94 68.59 17.07 17.72\nLoRA Zero-Shot 72.43 67.99 16.85 17.74\nTable 12: Scores for the 7B pretrained model and 7B both finetuned models on the Flores-200 test set.\n11142\nCOMET COMETKiwi BLEU chrF\nLanguage Pair Model Context\nde-en FT w/o few-shot Zero-shot 88.66 83.93 41.22 66.15\nFive-shot 88.53 83.88 40.41 65.66\nFT w/ few-shot Zero-shot 88.67 84.09 41.50 66.54\nFive-shot 88.69 84.12 41.47 66.63\nen-de FT w/o few-shot Zero-shot 83.63 81.30 28.15 57.62\nFive-shot 83.44 80.98 28.01 57.43\nFT w/ few-shot Zero-shot 83.99 81.51 29.21 58.98\nFive-shot 84.06 81.49 29.55 59.06\nfr-en FT w/o few-shot Zero-shot 88.77 86.04 42.93 67.39\nFive-shot 88.79 85.95 43.20 67.32\nFT w/ few-shot Zero-shot 88.94 86.17 43.34 67.72\nFive-shot 88.98 86.17 43.60 67.86\nen-fr FT w/o few-shot Zero-shot 85.36 85.10 39.69 64.43\nFive-shot 85.12 84.82 39.09 63.95\nFT w/ few-shot Zero-shot 85.76 85.49 40.57 65.14\nFive-shot 85.98 85.64 40.85 65.25\nnl-en FT w/o few-shot Zero-shot 86.40 85.27 29.78 58.30\nFive-shot 86.44 85.18 29.70 58.09\nFT w/ few-shot Zero-shot 86.67 85.45 30.08 58.74\nFive-shot 86.68 85.47 30.06 58.73\nen-nl FT w/o few-shot Zero-shot 83.10 82.50 20.46 52.79\nFive-shot 82.99 82.29 20.24 52.44\nFT w/ few-shot Zero-shot 83.45 82.84 20.98 53.50\nFive-shot 83.60 83.02 21.09 53.66\npt-en FT w/o few-shot Zero-shot 88.91 84.68 46.32 70.16\nFive-shot 88.79 84.50 46.37 70.02\nFT w/ few-shot Zero-shot 88.92 84.70 46.95 70.45\nFive-shot 88.96 84.70 47.32 70.62\nen-pt FT w/o few-shot Zero-shot 86.93 82.67 38.94 64.57\nFive-shot 86.86 82.46 38.64 64.32\nFT w/ few-shot Zero-shot 87.36 83.06 40.10 65.36\nFive-shot 87.35 83.07 40.11 65.25\nru-en FT w/o few-shot Zero-shot 85.42 83.48 32.28 59.59\nFive-shot 85.30 83.20 32.08 59.18\nFT w/ few-shot Zero-shot 85.68 83.56 33.12 60.31\nFive-shot 85.73 83.62 33.18 60.35\nen-ru FT w/o few-shot Zero-shot 84.73 80.74 20.88 49.53\nFive-shot 84.50 80.18 20.09 48.74\nFT w/ few-shot Zero-shot 85.41 81.40 22.08 50.36\nFive-shot 85.57 81.47 22.19 50.73\nzh-en FT w/o few-shot Zero-shot 83.15 80.44 21.61 49.71\nFive-shot 82.57 79.54 20.62 48.23\nFT w/ few-shot Zero-shot 83.71 81.12 22.26 50.70\nFive-shot 83.73 80.96 22.74 50.83\nen-zh FT w/o few-shot Zero-shot 72.43 67.99 16.85 17.74\nFive-shot 71.77 66.56 16.11 16.93\nFT w/ few-shot Zero-shot 74.25 70.08 19.05 19.20\nFive-shot 74.74 70.69 19.32 19.50\nTable 13: Scores of the 7B finetuned models (zero- and few-shot prompting) on the Flores-200 test set.\n11143\nCOMET COMETKiwi BLEU chrF\nLanguage Pair Model Context\nde-en FT w/o few-shot Zero-shot 82.86 79.66 29.09 53.99\nFive-shot 82.73 79.24 29.27 53.92\nFT w/ few-shot Zero-shot 83.10 79.75 29.43 54.49\nFive-shot 83.06 79.73 29.40 54.58\nen-de FT w/o few-shot Zero-shot 80.02 78.29 23.69 52.80\nFive-shot 80.11 78.01 23.11 52.00\nFT w/ few-shot Zero-shot 80.94 78.94 24.00 53.81\nFive-shot 81.19 79.05 24.33 54.04\nru-en FT w/o few-shot Zero-shot 82.58 79.08 36.18 61.15\nFive-shot 81.90 78.31 35.01 60.35\nFT w/ few-shot Zero-shot 82.97 79.37 37.06 61.93\nFive-shot 83.12 79.46 37.30 62.11\nen-ru FT w/o few-shot Zero-shot 82.04 77.77 20.01 46.18\nFive-shot 81.79 77.41 19.41 45.33\nFT w/ few-shot Zero-shot 82.33 78.31 20.74 46.71\nFive-shot 82.24 78.14 20.88 46.91\nzh-en FT w/o few-shot Zero-shot 74.90 72.17 15.14 43.29\nFive-shot 74.04 71.14 15.52 43.32\nFT w/ few-shot Zero-shot 75.56 72.96 15.91 44.03\nFive-shot 75.52 73.09 15.98 44.53\nen-zh FT w/o few-shot Zero-shot 74.27 69.08 18.19 19.96\nFive-shot 73.86 67.77 17.12 18.88\nFT w/ few-shot Zero-shot 76.05 70.97 20.37 21.54\nFive-shot 76.25 71.14 19.89 21.43\nTable 14: Scores of the 7B finetuned models (zero- and few-shot prompting) on the WMT 2022 test set.\n11144\nCOMET COMETKiwi BLEU chrF\nDomain Language Pair Model Context\nMedical de-en FT w/o few-shot Zero-shot 83.54 80.92 39.98 60.32\nFive-shot 83.03 80.32 39.29 59.43\nFT w/ few-shot Zero-shot 83.73 81.08 40.62 60.88\nFive-shot 83.77 81.15 40.57 60.89\nen-de FT w/o few-shot Zero-shot 80.15 79.64 30.25 53.10\nFive-shot 79.75 78.87 29.86 52.35\nFT w/ few-shot Zero-shot 80.58 79.93 31.61 54.51\nFive-shot 80.58 79.87 31.48 54.32\nLaw de-en FT w/o few-shot Zero-shot 84.11 80.53 38.18 59.38\nFive-shot 82.67 78.90 36.90 57.85\nFT w/ few-shot Zero-shot 84.26 80.81 39.84 60.62\nFive-shot 84.43 80.96 40.11 60.82\nen-de FT w/o few-shot Zero-shot 80.83 78.63 26.04 50.44\nFive-shot 79.35 76.87 25.03 48.88\nFT w/ few-shot Zero-shot 81.55 79.07 27.80 52.12\nFive-shot 81.64 79.11 27.71 52.00\nTico en-fr FT w/o few-shot Zero-shot 78.44 83.99 31.57 56.80\nFive-shot 78.10 83.40 32.72 57.11\nFT w/ few-shot Zero-shot 78.60 84.20 32.47 57.59\nFive-shot 78.62 84.21 32.58 57.66\nen-pt FT w/o few-shot Zero-shot 86.87 81.73 39.76 65.92\nFive-shot 86.48 81.03 39.31 65.26\nFT w/ few-shot Zero-shot 87.03 82.00 40.96 66.81\nFive-shot 87.13 82.07 41.11 66.91\nChat en-de FT w/o few-shot Zero-shot 82.56 77.97 27.32 50.81\nFive-shot 82.70 77.75 28.53 51.76\nFT w/ few-shot Zero-shot 83.54 78.70 28.19 52.54\nFive-shot 84.01 78.76 28.83 53.05\nen-fr FT w/o few-shot Zero-shot 86.71 80.81 44.70 62.90\nFive-shot 86.38 80.12 43.97 61.69\nFT w/ few-shot Zero-shot 86.52 80.61 44.87 63.26\nFive-shot 86.62 80.72 44.89 63.20\nen-pt FT w/o few-shot Zero-shot 89.54 80.58 41.79 62.93\nFive-shot 88.70 79.80 41.06 61.98\nFT w/ few-shot Zero-shot 89.64 80.79 43.52 63.85\nFive-shot 89.98 80.76 43.88 64.17\nTable 15: Scores of the 7B finetuned models (zero- and few-shot prompting) on the test sets for specialized domains.\n11145\nCOMET COMETKiwi BLEU chrF\nLanguage Pair Model Context\nde-en FT w/o few-shot Zero-shot 88.82 84.11 42.03 67.00\nFive-shot 88.86 84.05 41.95 66.93\nFT w/ few-shot Zero-shot 88.94 84.13 42.86 67.40\nFive-shot 89.03 84.20 42.77 67.53\nen-de FT w/o few-shot Zero-shot 84.87 82.38 30.60 60.04\nFive-shot 84.64 82.03 30.73 60.01\nFT w/ few-shot Zero-shot 85.01 82.43 31.52 60.49\nFive-shot 85.01 82.52 31.51 60.50\nfr-en FT w/o few-shot Zero-shot 89.09 86.16 44.14 68.12\nFive-shot 89.12 86.10 44.05 68.10\nFT w/ few-shot Zero-shot 89.13 86.12 44.83 68.46\nFive-shot 89.15 86.14 44.95 68.58\nen-fr FT w/o few-shot Zero-shot 86.08 85.69 41.33 65.59\nFive-shot 86.07 85.75 41.19 65.59\nFT w/ few-shot Zero-shot 86.35 85.93 41.96 66.07\nFive-shot 86.41 85.92 42.10 66.20\nnl-en FT w/o few-shot Zero-shot 86.81 85.39 30.37 59.02\nFive-shot 86.99 85.45 30.83 59.24\nFT w/ few-shot Zero-shot 86.89 85.45 30.71 59.15\nFive-shot 86.90 85.51 30.89 59.25\nen-nl FT w/o few-shot Zero-shot 84.10 83.26 21.87 54.25\nFive-shot 84.63 83.69 22.14 54.20\nFT w/ few-shot Zero-shot 84.16 83.38 22.10 54.61\nFive-shot 84.42 83.57 22.44 54.85\npt-en FT w/o few-shot Zero-shot 89.26 84.86 47.81 71.25\nFive-shot 89.30 84.82 48.07 71.17\nFT w/ few-shot Zero-shot 89.35 84.86 48.30 71.40\nFive-shot 89.32 84.88 48.16 71.39\nen-pt FT w/o few-shot Zero-shot 87.98 83.75 41.26 66.19\nFive-shot 87.88 83.57 41.43 66.24\nFT w/ few-shot Zero-shot 88.01 83.78 41.63 66.49\nFive-shot 88.00 83.81 42.08 66.71\nru-en FT w/o few-shot Zero-shot 85.93 83.78 33.57 60.86\nFive-shot 86.01 83.82 33.82 61.03\nFT w/ few-shot Zero-shot 86.03 83.84 34.02 61.20\nFive-shot 86.13 83.84 34.40 61.44\nen-ru FT w/o few-shot Zero-shot 86.46 82.68 23.37 52.00\nFive-shot 86.08 82.43 23.89 52.21\nFT w/ few-shot Zero-shot 86.92 83.10 24.21 52.80\nFive-shot 86.64 82.92 24.07 52.62\nzh-en FT w/o few-shot Zero-shot 84.16 81.48 23.23 51.41\nFive-shot 83.87 81.26 22.59 50.65\nFT w/ few-shot Zero-shot 84.48 81.81 24.09 51.95\nFive-shot 84.51 81.78 24.28 52.33\nen-zh FT w/o few-shot Zero-shot 76.49 73.15 19.96 20.03\nFive-shot 76.52 72.52 20.13 20.24\nFT w/ few-shot Zero-shot 78.02 74.89 21.60 21.23\nFive-shot 77.89 74.45 21.82 21.32\nTable 16: Scores of the 13B finetuned models (zero- and few-shot prompting) on the Flores-200 test set.\n11146\nCOMET COMETKiwi BLEU chrF\nLanguage Pair Model Context\nde-en FT w/o few-shot Zero-shot 83.40 80.09 30.16 55.15\nFive-shot 83.50 80.02 30.62 55.45\nFT w/ few-shot Zero-shot 83.44 80.03 30.58 55.39\nFive-shot 83.54 80.19 30.64 55.53\nen-de FT w/o few-shot Zero-shot 81.88 79.82 25.33 54.72\nFive-shot 81.87 79.71 25.12 54.62\nFT w/ few-shot Zero-shot 82.43 80.30 25.88 55.42\nFive-shot 82.18 80.18 25.91 55.42\nru-en FT w/o few-shot Zero-shot 83.18 79.54 38.04 62.55\nFive-shot 83.08 79.56 37.55 62.79\nFT w/ few-shot Zero-shot 83.49 79.83 38.23 62.88\nFive-shot 83.51 79.87 38.28 62.94\nen-ru FT w/o few-shot Zero-shot 83.90 79.60 21.57 48.19\nFive-shot 84.03 79.75 22.17 48.51\nFT w/ few-shot Zero-shot 84.46 80.10 22.71 49.21\nFive-shot 84.44 80.13 22.73 49.17\nzh-en FT w/o few-shot Zero-shot 76.11 73.54 16.66 45.52\nFive-shot 75.65 72.88 17.99 46.38\nFT w/ few-shot Zero-shot 76.48 73.91 17.20 45.85\nFive-shot 76.44 73.92 17.51 46.14\nen-zh FT w/o few-shot Zero-shot 77.48 72.69 21.38 22.13\nFive-shot 77.28 72.22 20.75 21.86\nFT w/ few-shot Zero-shot 78.80 74.15 23.31 23.63\nFive-shot 78.94 74.40 23.32 23.49\nTable 17: Scores of the 13B finetuned models (zero- and few-shot prompting) on the WMT 2022 test set.\n11147\nCOMET COMETKiwi BLEU chrF\nDomain Language Pair Model Context\nMedical de-en FT w/o few-shot Zero-shot 83.99 81.19 42.01 62.18\nFive-shot 84.06 80.98 42.39 62.42\nFT w/ few-shot Zero-shot 84.18 81.24 43.15 62.60\nFive-shot 84.26 81.26 43.11 62.76\nen-de FT w/o few-shot Zero-shot 81.31 81.09 32.36 56.33\nFive-shot 81.12 80.92 31.89 56.05\nFT w/ few-shot Zero-shot 81.77 81.40 33.00 57.08\nFive-shot 81.72 81.21 33.28 57.16\nLaw de-en FT w/o few-shot Zero-shot 84.73 81.07 41.08 61.87\nFive-shot 84.57 80.74 41.01 61.77\nFT w/ few-shot Zero-shot 85.03 81.25 42.72 63.04\nFive-shot 85.13 81.40 42.74 63.06\nen-de FT w/o few-shot Zero-shot 83.46 81.73 28.25 54.84\nFive-shot 83.16 81.33 28.35 54.54\nFT w/ few-shot Zero-shot 83.91 82.05 29.97 55.89\nFive-shot 83.87 82.04 29.89 55.90\nTico en-fr FT w/o few-shot Zero-shot 79.56 85.37 34.22 59.72\nFive-shot 79.43 85.24 36.47 60.62\nFT w/ few-shot Zero-shot 79.78 85.62 35.22 60.33\nFive-shot 79.79 85.68 35.18 60.40\nen-pt FT w/o few-shot Zero-shot 87.89 83.36 43.30 69.13\nFive-shot 87.82 83.15 43.31 69.01\nFT w/ few-shot Zero-shot 88.16 83.67 44.21 69.68\nFive-shot 88.29 83.74 44.48 69.87\nChat en-de FT w/o few-shot Zero-shot 84.62 79.89 29.24 54.11\nFive-shot 84.84 79.67 31.11 54.75\nFT w/ few-shot Zero-shot 85.56 80.28 30.33 54.51\nFive-shot 85.64 80.36 30.90 55.09\nen-fr FT w/o few-shot Zero-shot 87.46 81.01 47.39 64.62\nFive-shot 88.18 81.32 49.43 66.37\nFT w/ few-shot Zero-shot 86.41 80.88 47.74 64.97\nFive-shot 86.86 81.04 47.92 64.94\nen-pt FT w/o few-shot Zero-shot 91.02 81.67 45.99 66.36\nFive-shot 90.82 81.49 44.49 66.19\nFT w/ few-shot Zero-shot 90.56 81.31 44.75 65.93\nFive-shot 90.50 81.38 44.89 66.00\nTable 18: Scores of the 13B finetuned models (zero- and few-shot prompting) on the test sets for specialized\ndomains.\n11148"
}