{
  "title": "Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models",
  "url": "https://openalex.org/W3208933101",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2116452857",
      "name": "Bo-Xin Wang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A3208639152",
      "name": "Chejian Xu",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2127805350",
      "name": "Shuohang Wang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2230524742",
      "name": "Zhe Gan",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2125847941",
      "name": "Yu Cheng",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2104437897",
      "name": "Jian-Feng Gao",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2094223786",
      "name": "Ahmed Hassan Awadallah",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2045831236",
      "name": "Bo Li",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2913470588",
    "https://openalex.org/W3101449015",
    "https://openalex.org/W3136077193",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963394326",
    "https://openalex.org/W3017003177",
    "https://openalex.org/W3171654528",
    "https://openalex.org/W2759471388",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3105662186",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W3103934057",
    "https://openalex.org/W2911634294",
    "https://openalex.org/W2795038878",
    "https://openalex.org/W2996403597",
    "https://openalex.org/W2892852825",
    "https://openalex.org/W2951718443",
    "https://openalex.org/W2945067664",
    "https://openalex.org/W3168194750",
    "https://openalex.org/W3125455309",
    "https://openalex.org/W3024608270",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W3006647218",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2964301649",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W3128654100",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W3120706522",
    "https://openalex.org/W2963859254",
    "https://openalex.org/W3104423855",
    "https://openalex.org/W2964082701",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W2970078867",
    "https://openalex.org/W3035164976",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W2970449623",
    "https://openalex.org/W2798302089",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W4226182549",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W2243397390",
    "https://openalex.org/W2922266396",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3087231533",
    "https://openalex.org/W3035736465",
    "https://openalex.org/W3044324512",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W2738015883",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Large-scale pre-trained language models have achieved tremendous success\nacross a wide range of natural language understanding (NLU) tasks, even\nsurpassing human performance. However, recent studies reveal that the\nrobustness of these models can be challenged by carefully crafted textual\nadversarial examples. While several individual datasets have been proposed to\nevaluate model robustness, a principled and comprehensive benchmark is still\nmissing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task\nbenchmark to quantitatively and thoroughly explore and evaluate the\nvulnerabilities of modern large-scale language models under various types of\nadversarial attacks. In particular, we systematically apply 14 textual\nadversarial attack methods to GLUE tasks to construct AdvGLUE, which is further\nvalidated by humans for reliable annotations. Our findings are summarized as\nfollows. (i) Most existing adversarial attack algorithms are prone to\ngenerating invalid or ambiguous adversarial examples, with around 90% of them\neither changing the original semantic meanings or misleading human annotators\nas well. Therefore, we perform a careful filtering process to curate a\nhigh-quality benchmark. (ii) All the language models and robust training\nmethods we tested perform poorly on AdvGLUE, with scores lagging far behind the\nbenign accuracy. We hope our work will motivate the development of new\nadversarial attacks that are more stealthy and semantic-preserving, as well as\nnew robust language models against sophisticated adversarial attacks. AdvGLUE\nis available at https://adversarialglue.github.io.",
  "full_text": "Adversarial GLUE: A Multi-Task Benchmark for\nRobustness Evaluation of Language Models\n∗Boxin Wang1, ∗Chejian Xu2, Shuohang Wang3, Zhe Gan3,\nYu Cheng3, Jianfeng Gao3, Ahmed Hassan Awadallah3, Bo Li1\n1University of Illinois at Urbana-Champaign\n2Zhejiang University, 3Microsoft Corporation\n{boxinw2,lbo}@illinois.edu, xuchejian@zju.edu.cn\n{shuohang.wang,zhe.gan,yu.cheng,jfgao,hassanam}@microsoft.com\nAbstract\nLarge-scale pre-trained language models have achieved tremendous success across\na wide range of natural language understanding (NLU) tasks, even surpassing\nhuman performance. However, recent studies reveal that the robustness of these\nmodels can be challenged by carefully crafted textual adversarial examples. While\nseveral individual datasets have been proposed to evaluate model robustness, a\nprincipled and comprehensive benchmark is still missing. In this paper, we present\nAdversarial GLUE (AdvGLUE), a new multi-task benchmark to quantitatively\nand thoroughly explore and evaluate the vulnerabilities of modern large-scale\nlanguage models under various types of adversarial attacks. In particular, we\nsystematically apply 14 textual adversarial attack methods to GLUE tasks to\nconstruct AdvGLUE, which is further validated by humans for reliable annotations.\nOur ﬁndings are summarized as follows. ( i) Most existing adversarial attack\nalgorithms are prone to generating invalid or ambiguous adversarial examples, with\naround 90% of them either changing the original semantic meanings or misleading\nhuman annotators as well. Therefore, we perform careful ﬁltering process to\ncurate a high-quality benchmark. (ii) All the language models and robust training\nmethods we tested perform poorly on AdvGLUE, with scores lagging far behind\nthe benign accuracy. We hope our work will motivate the development of new\nadversarial attacks that are more stealthy and semantic-preserving, as well as new\nrobust language models against sophisticated adversarial attacks. AdvGLUE is\navailable at https://adversarialglue.github.io.\n1 Introduction\nPre-trained language models [8, 31, 26, 55, 18, 60, 23, 6] have achieved state-of-the-art performance\nover a wide range of Natural Language Understanding (NLU) tasks [49, 48, 21, 45, 38]. However,\nrecent studies [24, 57, 50, 29, 13] reveal that even these large-scale language models are vulnerable to\ncarefully crafted adversarial examples, which can fool the models to output arbitrarily wrong answers\nby perturbing input sentences in a human-imperceptible way. Real-world systems built upon these\nvulnerable models can be misled in ways that would have profound security concerns [27, 28].\nTo address this challenge, various methods [ 23, 61, 51, 30] have been proposed to improve the\nadversarial robustness of language models. However, the adversary setup considered in these\nmethods lacks a uniﬁed standard. For example, Jiang et al. [23], Liu et al. [30] mainly evaluate their\nrobustness against human-crafted adversarial datasets [38, 21], while Wang et al. [51] evaluate the\n∗Equal Contribution\n35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.\narXiv:2111.02840v2  [cs.CL]  10 Jan 2022\nmodel robustness against automatic adversarial attack algorithms [24]. The absence of a principled\nadversarial benchmark makes it difﬁcult to compare the robustness across different models and\nidentify the adversarial attacks that most models are vulnerable to. This motivates us to build a\nuniﬁed and principled robustness evaluation benchmark for natural language models and hope to help\nanswer the following questions: what types of language models are more robust when evaluated on\nthe uniﬁed adversarial benchmark? Which adversarial attack algorithms against language models\nare more effective, transferable, or stealthy to human? How likely can human be fooled by different\nadversarial attacks?\nWe list out the fundamental principles to create a high-quality robustness evaluation benchmark\nas follows. First, as also pointed out by [ 2], a reliable benchmark should be accurately and unam-\nbiguously annotated by humans. This is especially crucial for the robustness evaluation, as some\nadversarial examples generated by automatic attack algorithms can fool humans as well [34]. Given\nour analysis in §3.4, among the generated adversarial data, there are only around 10% adversarial\nexamples that receive at least 4-vote consensus among 5 annotators and align with the original\nlabel. Thus, additional rounds of human ﬁltering are critical to validate the quality of the generated\nadversarial attack data. Second, a comprehensive robustness evaluation benchmark should cover\nenough language phenomena and generate a systematic diagnostic report to understand and analyze\nthe vulnerabilities of language models. Finally, a robustness evaluation benchmark needs to be\nchallenging and unveil the biases shared across different models.\nIn this paper, we introduce Adversarial GLUE (AdvGLUE), a multi-task benchmark for robust-\nness evaluation of language models. Compared to existing adversarial datasets, there are several\ncontributions that render AdvGLUE a unique and valuable asset to the community.\n• Comprehensive Coverage. We consider textual adversarial attacks from different perspectives and\nhierarchies, including word-level transformations, sentence-level manipulations, and human-written\nadversarial examples, so that AdvGLUE is able to cover as many adversarial linguistic phenomena\nas possible.\n• Systematic Annotations. To the best of our knowledge, this is the ﬁrst work that performs\nsystematic and comprehensive evaluation and annotation over 14 different textual adversarial\nexamples. Concretely, AdvGLUE adopts crowd-sourcing to identify high-quality adversarial data\nfor reliable evaluation.\n• General Compatibility. To obtain comprehensive understanding of the robustness of language\nmodels across different NLU tasks, AdvGLUE covers the widely-used GLUE tasks and creates an\nadversarial version of the GLUE benchmark to evaluate the robustness of language models.\n• High Transferability and Effectiveness. AdvGLUE has high adversarial transferability and can\neffectively attack a wide range of state-of-the-art models. We observe a signiﬁcant performance drop\nfor models evaluated on AdvGLUE compared with their standard accuracy on GLUE leaderboard.\nFor instance, the average GLUE score of ELECTRA(Large) [6] drops from 93.16 to 41.69.\nOur contributions are summarized as follows. (i) We propose AdvGLUE, a principled and compre-\nhensive benchmark that focuses on robustness evaluation of language models. (ii) During the data\nconstruction, we provide a thorough analysis and a fair comparison of existing strong adversarial at-\ntack algorithms. (iii) We present thorough robustness evaluation for existing state-of-the-art language\nmodels and defense methods. We hope that AdvGLUE will inspire active research and discussion in\nthe community. More details are available at https://adversarialglue.github.io.\n2 Related Work\nExisting robustness evaluation work can be roughly divided into two categories:Evaluation Toolkits\nand Benchmark Datasets. ( i) Evaluation toolkits, including OpenAttack [ 58], TextAttack [35],\nTextFlint [17] and Robustness Gym [15], integrate various ad hoc input transformations for different\ntasks and provide programmable APIs to dynamically test model performance. However, it is\nchallenging to guarantee the quality of these input transformations. For example, as reported in [57],\nthe validity of adversarial transformation can be as low as 65.5%, which means that more than one\nthird of the adversarial sentences have wrong labels. Such a high percentage of annotation errors\ncould lead to an underestimate of model robustness, making it less qualiﬁed to serve as an accurate\nand reliable benchmark [2]. (ii) Benchmark datasets for robustness evaluation create challenging\n2\nTable 1: Statistics of AdvGLUE benchmark . We apply all word-level perturbations (C1= Embedding-\nsimilarity, C2=Typos, C3=Context-aware, C4=Knowledge-guided, and C5=Compositions) to the ﬁve GLUE\ntasks. For sentence-level perturbations, we apply Syntactic-based perturbations (C6) to the ﬁve GLUE tasks.\nDistraction-based perturbations (C7) are applied to four GLUE tasks without QQP, as they may affect the\nsemantic similarity. For human-crafted examples, we apply CheckList (C8) to SST-2, QQP, and QNLI;StressTest\n(C9) and ANLI (C10) to MNLI; and AdvSQuAD (C11) to QNLI tasks.\nCorpus Task |Train| |Test| Word-Level Sent.-Level Human-Crafted\n(GLUE) (AdvGLUE) C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11\nSST-2 sentiment 67,349 1,420 204 197 91 175 64 211 320 158 0 0 0\nQQP paraphrase 363,846 422 42 151 17 35 75 37 0 65 0 0 0\nQNLI NLI/QA 104,743 968 73 139 71 98 72 159 219 80 0 0 57\nRTE NLI 2,490 304 43 44 31 27 23 48 88 0 0 0 0\nMNLI NLI 392,702 1,864 69 402 114 161 128 217 386 0 194 193 0\nSum of AdvGLUE test set 4,978 431 933 324 496 362 672 1013 303 194 193 57\ntesting cases by using human-crafted templates or rules [45, 43, 36], or adopting a human-and-model-\nin-the-loop manner to write adversarial examples [38, 25, 1]. While the quality and validity of these\nadversarial datasets can be well controlled, the scalability and comprehensiveness are limited by\nthe human annotators. For example, template-based methods require linguistic experts to carefully\nconstruct reasonable rules for speciﬁc tasks, and such templates can be barely transferable to other\ntasks. Moreover, human annotators tend to complete the writing tasks through minimal efforts and\nshortcuts [4, 47], which can limit the coverage of various linguistic phenomena.\n3 Dataset Construction\nIn this section, we provide an overview of our evaluation tasks, as well as the pipeline of how\nwe construct the benchmark data. During this data construction process, we also compare the\neffectiveness of different adversarial attack methods, and present several interesting ﬁndings.\n3.1 Overview\nTasks. We consider the following ﬁve most representative and challenging tasks used in GLUE [49]:\nSentiment Analysis (SST-2), Duplicate Question Detection (QQP), and Natural Language Inference\n(NLI, including MNLI, RTE, QNLI). The detailed explanation for each task can be found in Appendix\nA.3. Some tasks in GLUE are not included in AdvGLUE, since there are either no well-deﬁned\nautomatic adversarial attacks (e.g., CoLA), or insufﬁcient data (e.g., WNLI) for the attacks.\nDataset Statistics and Evaluation Metrics. AdvGLUE follows the same training data and evalua-\ntion metrics as GLUE. In this way, models trained on the GLUE training data can be easily evaluated\nunder IID sampled test sets (GLUE benchmark) or carefully crafted adversarial test sets (AdvGLUE\nbenchmark). Practitioners can understand the model generalization via the GLUE diagnostic test suite\nand examine the model robustness against different levels of adversarial attacks from the AdvGLUE\ndiagnostic report with only one-time training. Given the same evaluation metrics, model developers\ncan clearly understand the performance gap between models tested in the ideally benign environments\nand approximately worst-case adversarial scenarios. We present the detailed dataset statistics under\nvarious attacks in Table 1. Detailed label distribution and evaluation metrics are in Appendix Table 8.\n3.2 Adversarial Perturbations\nIn this section, we detail how we optimize different levels of adversarial perturbations to the benign\nsource samples and collect the raw adversarial data with noisy labels, which will then be carefully\nﬁltered by human annotators described in the next section. Speciﬁcally, we consider the dev sets of\nGLUE benchmark as our source samples, upon which we perform different adversarial attacks. For\nrelatively large-scale tasks (QQP, QNLI, MNLI-m/mm), we sample 1,000 cases from the dev sets for\nefﬁciency purpose. For the remaining tasks, we consider the whole dev sets as source samples.\n3\n...\nGLUE\nData\nSST-2\nMNLI-m\nMNLI-mm\nRTE\nQQP\nQNLI\nAdversarial\nPerturbations\nTypos\n       AdvGLUE Data\nAutomatic \nFiltering\n(Transferability)\nCuration\nHuman-crafted \nExamples\nSentence-level\nPerturbations\nWord-level\nPerturbations\nEmbedding\nContext\nKnowledge\nComposition\nSyntax\nDistraction\nANLI\nCheckList\nAdvSQuAD\nTextBugger\nTextFooler\nBERT-ATTACK\nSememe-PSO\nCompAttack\nT3; AdvFever; SCPN\nStressTest; CheckList\nAutomatic\nFiltering\n(Fidelity)\nHuman Training \n& Filtering\nSampling\nFigure 1: Overview of the AdvGLUE dataset construction pipeline.\n3.2.1 Word-level Perturbation\nExisting word-level adversarial attacks perturb the words through different strategies, such as perturb-\ning words with their synonyms [24] or carefully crafted typo words [27] (e.g., “foolish” to “fo01ish”),\nsuch that the perturbation does not change the semantic meaning of the sentences but dramatically\nchange the models’ output. To examine the model robustness against different perturbation strategies,\nwe select one representative adversarial attack method for each strategy as follows.\nTypo-based Perturbation. We select TextBugger [27] as the representative algorithm for generating\ntypo-based adversarial examples. When performing the attack, TextBugger ﬁrst identiﬁes the\nimportant words and then replaces them with typos.\nEmbedding-similarity-based Perturbation. We choose TextFooler [24] as the representative adver-\nsarial attack that considers embedding similarity as a constraint to generate semantically consistent\nadversarial examples. Essentially, TextFooler ﬁrst performs word importance ranking, and then\nsubstitutes those important ones to their synonyms extracted according to the cosine similarity of\nword embeddings.\nContext-aware Perturbation. We use BERT-ATTACK [29] to generate context-aware perturbations.\nThe fundamental difference between BERT-ATTACK and TextFooler lies on the word replacement\nprocedure. Speciﬁcally, BERT-ATTACK uses the pre-trained BERT to perform masked language\nprediction to generate contextualized potential word replacements for those crucial words.\nKnowledge-guided Perturbation. We consider SememePSO [57] as an example to generate adver-\nsarial examples guided by the HowNet [41] knowledge base. SememePSO ﬁrst ﬁnds out substitutions\nfor each word in HowNet based on sememes, and then searches for the optimal combination based\non particle swarm optimization.\nCompositions of different Perturbations. We also implement a whitebox-based adversarial attack\nalgorithm called CompAttack that integrates the aforementioned perturbations in one algorithm to\nevaluate model robustness to various adversarial transformations. Moreover, we efﬁciently search for\nperturbations via optimization so that CompAttack can achieve the attack goal while perturbing the\nminimal number of words. The implementation details can be found in Appendix A.4.\nWe note that the above adversarial attacks require a surrogate model to search for the optimal\nperturbations. In our experiments, we follow the setup of ANLI [ 38] and generate adversarial\nexamples against three different types of models (BERT, RoBERTa, and RoBERTa ensemble) trained\non the GLUE benchmark. We then perform one round of ﬁltering to retain those examples with high\nadversarial transferability between these surrogate models. We discuss more implementation details\nand hyper-parameters of each attack method in Appendix A.4.\n3.2.2 Sentence-level Perturbation\nDifferent from word-level attacks that perturb speciﬁc words, sentence-level attacks mainly focus\non the syntactic and logical structures of sentences. Most of them achieve the attack goal by either\nparaphrasing the sentence, manipulating the syntactic structures, or inserting some unrelated sentences\nto distract the model attention. AdvGLUE considers the following representative perturbations.\n4\nTable 2: Examples of AdvGLUE benchmark. We show 3 examples from QNLI task. These examples are\ngenerated with three levels of perturbations and they all can successfully change the predictions of all surrogate\nmodels (BERT, RoBERTa and RoBERTa ensemble).\nLinguistic\nPhenomenon\nSamples (Strikethrough = Original Text, red = Adversarial Perturbation) Label →\nPrediction\nTypo\n(Word-level)\nQuestion: What was the population of the Dutch Republic before this\nemigration? False →TrueSentence: This was a huge hu ge inﬂux as the entire population of the\nDutch Republic amounted to ca.\nDistraction\n(Sent.-level)\nQuestion: What was the population of the Dutch Republic before this\nemigration? https://t.co/DlI9kw False →TrueSentence: This was a huge inﬂux as the entire population of the Dutch\nRepublic amounted to ca.\nCheckList\n(Human-crafted)\nQuestion: What is Tony’s profession?\nTrue →FalseSentence: Both Tony and Marilyn were executives, but there was a\nchange in Marilyn, who is now an assistant.\nSyntactic-based Perturbation. We incorporate three adversarial attack strategies that manipulate\nthe sentence based on the syntactic structures. (i) Syntax Tree Transformations. SCPN [20] is trained\nto produce a paraphrase of a given sentence with speciﬁed syntactic structures. Following the default\nsetting, we select the most frequent 10 templates from ParaNMT-50M corpus [ 52] to guide the\ngeneration process. An LSTM-based encoder-decoder model (SCPN) is used to generate parses\nof target sentences according to the templates. These parses are further fed into another SCPN to\ngenerate full sentences. We use the pre-trained SCPNs released by the ofﬁcial codebase. (ii) Context\nVector Transformations. T3 [50] is a whitebox attack algorithm that can add perturbations on different\nlevels of the syntax tree and generate the adversarial sentence. In our setting, we add perturbations\nto the context vector of the root node given syntax tree, which is iteratively optimized to construct\nthe adversarial sentence. ( iii) Entailment Preserving Transformations. We follow the entailment\npreserving rules proposed by AdvFever [45], and transform all the sentences satisfying the templates\ninto semantically equivalent ones. More details can be found in Appendix A.4.\nDistraction-based Perturbation. We integrate two attack strategies: ( i) StressTest [36] appends\nthree true statements (“and true is true”, “and false is not true”, “and true is true” for ﬁve times) to the\nend of the hypothesis sentence for NLI tasks. (ii) CheckList [43] adds randomly generated URLs\nand handles to distract model attention. Since the aforementioned distraction-based perturbations\nmay impact the linguistic acceptability and the understanding of semantic equivalence, we mainly\napply these rules to part of the GLUE tasks, including SST-2 and NLI tasks (MNLI, RTE, QNLI), to\nevaluate whether model can be easily misled by the strong negation words or such lexical similarity.\n3.2.3 Human-crafted Examples\nTo ensure our benchmark covers more linguistic phenomena in addition to those provided by automatic\nattack algorithms, we integrate the following high-quality human-crafted adversarial data from crowd-\nsourcing or expert-annotated templates and transform them to the formats of GLUE tasks.\nCheckList2 [43] is a testing method designed for analysing different capabilities of NLP models using\ndifferent test types. For each task, CheckList ﬁrst identiﬁes necessary natural language capabilities a\nmodel should have, then designs several test templates to generate test cases at scale. We follow the\ninstructions and collect testing cases for three tasks: SST-2, QQP and QNLI. For each task, we adopt\ntwo capability tests: Temporal and Negation, which test if the model understands the order of events\nand if the model is sensitive to negations.\nStressTest2 [36] proposes carefully crafted rules to construct “stress tests” and evaluate robustness\nof NLI models to speciﬁc linguistic phenomena. We adopt the test cases focusing on Numerical\nReasoning into our adversarial MNLI dataset. These premise-hypothesis pairs are able to test whether\n2We note that both CheckList and StressTest propose both rule-based distraction sentences and manually\ncrafted templates to generate test samples. The former is considered as sentence-level distraction-based\nperturbations, while the latter is considered as human-crafted examples.\n5\nthe model can perform reasoning involving numbers and quantiﬁers and predict the correct relation\nbetween premise and hypothesis.\nANLI [38] is a large-scale NLI dataset collected iteratively in a human-in-the-loop manner. In each\niteration, human annotators are asked to design sentences to fool current model. Then the model is\nfurther ﬁnetuned on a larger dataset incorporating these sentences, which leads to a stronger model.\nFinally, annotators are asked to write harder examples to detect the weakness of this stronger model.\nIn the end, the sentence pairs generated in each round form a comprehensive dataset that aims at\nexamining the vulnerability of NLI models. We adopt ANLI into our adversarial MNLI dataset. We\nobtain the permission from the ANLI authors to include the ANLI dataset as part of our leaderboard.\nAdvSQuAD [21] is an adversarial dataset targeting at reading comprehension systems. Adversarial\nexamples are generated by appending a distracting sentence to the end of the input paragraph. The\ndistracting sentences are carefully designed to have common words with questions and look like\na correct answer to the question. We mainly consider the examples generated by ADDSENT and\nADDONESENT strategies, and adopt the distracting sentences and questions in the QNLI format with\nlabels “not answered”. The use of AdvSQuAD in AdvGLUE is authorized by the authors.\nWe present sampled AdvGLUE examples with the word-level, sentence-level perturbations and\nhuman-crafted samples in Table 2. More examples are provided in Appendix A.5.\n3.3 Data Curation\nAfter collecting the raw adversarial dataset, additional rounds of ﬁltering are required to guarantee its\nquality and validity. We consider two types of ﬁltering: automatic ﬁltering and human evaluation.\nAutomatic Filtering mainly evaluates the generated adversarial examples along two fronts: transfer-\nability and ﬁdelity.\n1. Transferability evaluates whether the adversarial examples generated against one source model\n(e.g., BERT) can successfully transfer and attack the other two ( e.g., RoBERTa and RoBERTa\nensemble), given the surrogate models used to generate adversarial examples (BERT, RoBERTa\nand RoBERTa ensemble). Only adversarial examples that can successfully transfer to the other\ntwo models will be kept for the next round of ﬁdelity ﬁltering, so that the selected examples can\nexploit the biases shared across different models and unveil their fundamental weakness.\n2. Fidelity evaluates how the generated adversarial examples maintain the original semantics. For\nword-level adversarial examples, we use word modiﬁcation rate to measure what percentage of\nwords are perturbed. Concretely, word-level adversarial examples with word modiﬁcation rate\nlarger than 15% are ﬁltered out. For sentence-level adversarial examples, we use BERTScore\n[59] to evaluate the semantic similarity between the adversarial sentences and their corresponding\noriginal ones. For each sentence-level attack, adversarial examples with the highest similarity\nscores are kept to guarantee their semantic closeness to the benign samples.\nHuman Evaluation validates whether the adversarial examples preserve the original labels and\nwhether the labels are highly agreed among annotators. Concretely, we recruit annotators from\nAmazon Mechanical Turk. To make sure the annotators fully understand the GLUE tasks, each\nworker is required to pass a training step to be qualiﬁed to work on the main ﬁltering tasks for the\ngenerated adversarial examples. We tune the pay rate for different tasks, as shown in Appendix Table\n11. The pay rate of the main ﬁltering phase is twice as much as that of the training phase.\n1. Human Training Phase is designed to ensure that the annotators understand the tasks. The\nannotation instructions for each task follows [37], and we provide at least two examples for each\nclass to help annotators understand the tasks.3 Each annotator is required to work on a batch of\n20 examples randomly sampled from the GLUE dev set. After annotators answer each example,\na ground-truth answer will be provided to help them understand whether the answer is correct.\nWorkers who get at least 85% of the examples correct during training are qualiﬁed to work on\nthe main ﬁltering task. A total of 100 crowd workers participated in each task, and the number of\nqualiﬁed workers are shown in Appendix Table 11. We also test the human accuracy of qualiﬁed\nannotators for each task on 100 randomly sampled examples from the dev set excluding the\ntraining samples. The details and results can be found in Appendix Table 11.\n3Instructions can be found at https://adversarialglue.github.io/instructions.\n6\nTable 3: Statistics of data curation . We report Attack Success Rate ( ASR) and ASR after data curation\n(Curated ASR) to evaluate the effectiveness of different adversarial attacks. We present the Filter Rate of data\ncuration and inter-annotator agreement rate (Fleiss Kappa) before and after curation to evaluate the validity of\nadversarial examples. Human Accuracy on our curated dataset is evaluated by taking one random annotator’s\nannotation as prediction and the majority voted label as ground truth. SPSO: SememePSO, TF: TextFooler,\nTB:TextBugger, CA: CompAttack, BA:BERT-ATTACK.↑/↓: higher/lower the better.\nTasks Metrics Word-level Attacks Sentence-level Attacks Avg\nSPSO TF TB CA BA T3 SCPN AdvFever\nSST-2\nASR ↑ 89.08 95.38 88.08 31.91 39.77 97.69 65.37 0.57 63.48\nCurated ASR ↑ 8.29 8.97 8.85 4.02 4.04 10.45 6.88 0.23 6.47\nFilter Rate ↓ 90.71 90.62 90.04 86.63 89.81 89.27 89.47 60.00 85.82\nFleiss Kappa ↑ 0.22 0.20 0.50 0.21 0.24 0.23 0.29 0.12 0.26\nCurated Fleiss Kappa ↑ 0.51 0.49 0.67 0.46 0.45 0.44 0.47 0.20 0.52\nHuman Accuracy ↑ 0.85 0.86 0.91 0.88 0.85 0.78 0.85 0.50 0.87\nMNLI\nASR ↑ 78.45 61.50 69.35 68.58 65.02 91.23 87.73 2.25 65.51\nCurated ASR ↑ 3.48 1.55 8.94 3.11 2.58 3.41 6.75 0.30 3.77\nFilter Rate ↓ 95.59 97.55 87.12 95.45 96.10 96.27 92.31 86.63 93.38\nFleiss Kappa ↑ 0.28 0.24 0.53 0.39 0.32 0.28 0.24 0.35 0.33\nCurated Fleiss Kappa ↑ 0.65 0.59 0.74 0.65 0.60 0.56 0.60 0.51 0.67\nHuman Accuracy ↑ 0.85 0.83 0.91 0.89 0.83 0.84 0.91 0.83 0.89\nRTE\nASR ↑ 76.67 75.67 85.89 73.36 72.05 92.39 88.45 6.62 71.39\nCurated ASR ↑ 6.20 8.14 10.03 6.97 5.58 7.05 8.30 2.53 6.85\nFilter Rate ↓ 91.93 89.21 88.29 90.72 92.16 92.31 90.61 61.34 87.07\nFleiss Kappa ↑ 0.30 0.32 0.58 0.35 0.25 0.33 0.43 0.58 0.38\nCurated Fleiss Kappa ↑ 0.49 0.67 0.80 0.63 0.42 0.60 0.64 0.65 0.66\nHuman Accuracy ↑ 0.77 0.95 0.94 0.87 0.79 0.89 0.91 0.86 0.92\nQNLI\nASR ↑ 71.88 67.03 82.54 67.24 60.53 96.41 67.37 0.97 64.25\nCurated ASR ↑ 3.92 2.87 5.87 4.09 2.69 7.59 3.90 0.00 3.87\nFilter Rate ↓ 94.63 95.89 92.89 93.92 95.78 92.16 94.21 100.00 94.93\nFleiss Kappa ↑ 0.07 0.05 0.16 0.10 0.14 0.07 0.12 -0.16 0.11\nCurated Fleiss Kappa ↑ 0.37 0.43 0.49 0.34 0.53 0.37 0.43 - 0.44\nHuman Accuracy ↑ 0.80 0.86 0.85 0.82 0.92 0.89 0.92 - 0.85\nQQP\nASR ↑ 45.86 48.59 57.92 49.33 43.66 48.20 44.37 0.30 42.28\nCurated ASR ↑ 1.52 1.74 5.87 3.05 0.76 1.47 1.50 0.00 1.99\nFilter Rate ↓ 96.73 96.50 89.90 93.83 98.28 97.04 96.62 100.00 96.11\nFleiss Kappa ↑ 0.26 0.27 0.38 0.27 0.24 0.25 0.29 - 0.30\nCurated Fleiss Kappa ↑ 0.32 0.46 0.62 0.48 0.40 0.10 0.47 - 0.51\nHuman Accuracy ↑ 0.84 0.98 0.97 0.89 0.78 0.89 1.00 - 0.89\n2. Human Filtering Phase veriﬁes the quality of the generated adversarial examples and only\nmaintains high-quality ones to construct the benchmark dataset. Speciﬁcally, annotators are\nrequired to work on a batch of 10 adversarial examples generated from the same attack method.\nEvery adversarial example will be validated by 5 different annotators. Examples are selected\nfollowing two criteria: (i) high consensus: each example must have at least 4-vote consensus; (ii)\nutility preserving: the majority-voted label must be the same as the original one to make sure the\nattacks are valid (i.e., cannot fool human) and preserve the semantic content.\nThe data curation results including inter-annotator agreement rate (Fleiss Kappa) and human accuracy\non the curated dataset are shown in Table 3. We will provide more analysis in the next section. Note\nthat even after the data curation step, some grammatical errors and typos can still remain, as some\nadversarial attacks intentionally inject typos (e.g., TextBugger) or manipulate syntactic trees (e.g.,\nSCPN) which are very stealthy. We will retain these samples as their labels receive high consensus\nfrom annotators, which means the typos do not substantially impact humans’ understanding.\n3.4 Benchmark of Adversarial Attack Algorithms\nOur data curation phase also serves as a comprehensive benchmark over existing adversarial attack\nmethods, as it provides a fair standard for all adversarial attacks and systematic human annotations to\nevaluate the quality of the generated samples.\n7\nTable 4: Model performance on AdvGLUE test set . BERT (Large) and RoBERTa (Large) are ﬁne-tuned\nusing different random seeds and thus different from the surrogate models used for adversarial text generation.\nFor MNLI, we report the test accuracy on the matched and mismatched test sets; for QQP, we report accuracy\nand F1; and for other tasks, we report the accuracy. All values are reported by percentage (%). We also report\nthe macro-average (Avg) of per-task scores for different models. (Complete results are listed in our leaderboard.)\nModel SST-2 MNLI RTE QNLI QQP Avg Avg Avg\nAdvGLUE AdvGLUE AdvGLUE AdvGLUE AdvGLUE AdvGLUE GLUE ∆ ↓\nState-of-the-art Pre-trained Language Models\nBERT (Large) 33.03 28.72/27.05 40.46 39.77 37.91/16.56 33.68 85.76 52.08\nELECTRA (Large) 58.59 14.62/20.22 23.03 57.54 61.37/42.40 41.69 93.16 51.47\nRoBERTa (Large) 58.52 50.78/39.62 45.39 52.48 57.11/41.80 50.21 91.44 41.23\nT5 (Large) 60.56 48.43/38.98 62.83 57.64 63.03/ 55.68 56.82 90.39 33.57\nALBERT (XXLarge) 66.83 51.83/44.17 73.03 63.84 56.40/32.35 59.22 91.87 32.65\nDeBERTa (Large) 57.89 58.36/52.46 78.95 57.85 60.43/47.98 60.86 92.67 31.81\nRobust Training Methods for Pre-trained Language Models\nSMART (BERT) 25.21 26.89/23.32 38.16 34.61 36.49/20.24 30.29 85.70 55.41\nSMART (RoBERTa) 50.92 45.56/36.07 70.39 52.17 64.22/44.28 53.71 92.62 38.91\nFreeLB (RoBERTa) 61.69 31.59/27.60 62.17 62.29 42.18/31.07 50.47 92.28 41.81\nInfoBERT (RoBERTa) 47.61 50.39/41.26 39.47 54.86 49.29/35.54 46.04 89.06 43.02\nEvaluation Metrics. Speciﬁcally, we evaluate these attacks along two fronts: effectiveness and\nvalidity. For effectiveness, we consider two evaluation metrics: Attack Success Rate (ASR) and Cu-\nrated Attack Success Rate (Curated ASR). Formally, given a benign datasetD= {(x(i), y(i))}N\ni=1\nconsisting of N pairs of sample x(i) and ground truth y(i), for an adversarial attack method Athat\ngenerates an adversarial example A(x) given an input x to attack a surrogate model f, ASR is\ncalculated as\nASR =\n∑\n(x,y)∈D\n1[f(A(x)) ̸= y]\n1[f(x) =y] , (1)\nwhere 1 is the indicator function. After the data curation phase, we collect a curated adversarial\ndataset Dc. Thus, Curated ASR is calculated as\nCurated ASR =\n∑\n(x,y)∈D\n1[f(A(x)) ̸= y] ·1[A(x) ∈Dc]\n1[f(x) =y] . (2)\nFor validity, we consider three evaluation metrics:Filter Rate, Fleiss Kappa, and Human Accuracy.\nSpeciﬁcally, Filter Rate is calculated by1−Curated ASR\nASR to measure how many examples are rejected\nin the data curation procedures and can reﬂect the noisiness of the generated adversarial examples. We\nreport the average ASR, Curated ASR, and Filter Rate over the three surrogate models we consider in\nTable 3. Fleiss Kappa is a widely used metric in existing datasets (e.g., SNLI, ANLI, and FEVER\n[3, 38, 46]) to measure the inter-annotator agreement rate on the collected dataset. Fleiss Kappa\nbetween 0.4 and 0.6 is considered as moderate agreement and between 0.6 and 0.8 as substantial\nagreement. The inter-annotator agreement rates of most high-quality datasets fall into these two\nintervals. In this paper, we follow the standard protocol and report Fleiss Kappa and Curated Fleiss\nKappa to analyze the inter-annotator agreement rate on the collected adversarial dataset before\nand after curation to reﬂect the ambiguity of generated examples. We also estimate the human\nperformance on our curated datasets. Speciﬁcally, given a sample with 5 annotations, we take one\nrandom annotator’s annotation as the prediction and the majority voted label as the ground truth and\ncalculate the human accuracy as shown in Table 3.\nAnalysis. As shown in Table 3, in terms of attack effectiveness, while most attacks show high\nASR, the Curated ASR is always less than 11%, which indicates that most existing adversarial attack\nalgorithms are not effective enough to generate high-quality adversarial examples. In terms ofvalidity,\nthe ﬁlter rates for most adversarial attack methods are more than 85%, which suggests that existing\nstrong adversarial attacks are prone to generating invalid adversarial examples that either change the\noriginal semantic meanings or generate ambiguous perturbations that hinder the annotators’ unanimity.\nWe provide detailed ﬁlter rates for automatic ﬁltering and human evaluation in Appendix Table 12,\nand the conclusion is that around 60 −80% of examples are ﬁltered due to the low transferability\n8\nTable 5: Diagnostic report of state-of-the-art language models and robust training methods. For each at-\ntack method, we evaluate models against generated adversarial data for different tasks to obtain per-task accuracy\nscores, and report the macro-average of those scores. (C1= Embedding-similarity, C2=Typos, C3=Context-\naware, C4=Knowledge-guided, C5=Compositions, C6=Syntactic-based Perturbations, C7=Distraction-based\nPerturbations, C8=CheckList, C9=StressTest, C10=ANLI and C11=AdvSQuAD).\nModels Word-Level Perturbations Sent.-Level Human-Crafted Examples\nC1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11\nBERT (Large) 42.02 31.96 45.18 45.86 33.85 44.86 24.16 16.33 23.20 13.47 10.53\nELECTRA (Large) 43.07 45.12 47.95 46.33 47.33 43.47 33.30 32.20 26.29 26.94 52.63\nRoBERTa (Large) 56.54 57.19 60.47 49.81 55.92 50.49 41.89 37.78 28.35 16.58 35.09\nT5 (Large) 60.04 67.94 64.60 59.84 58.50 50.54 42.20 69.02 23.20 17.10 52.63\nALBERT (XXLarge) 66.71 67.61 73.49 70.36 59.52 63.76 49.14 45.55 39.69 26.94 43.86\nDeBERTa (Large) 65.07 74.87 68.02 65.30 62.54 57.41 47.22 45.08 52.06 22.80 54.39\nSMART (BERT) 45.17 31.04 42.89 45.23 30.76 40.74 16.62 8.20 18.56 10.36 1.75\nSMART (RoBERTa) 62.93 58.03 65.09 62.65 61.37 55.31 40.13 39.27 28.35 15.54 31.58\nFreeLB (RoBERTa) 51.95 53.23 52.92 51.15 52.18 50.75 37.72 66.87 23.71 29.02 64.91\nInfoBERT (RoBERTa) 55.47 55.78 59.02 51.33 55.48 44.56 31.49 34.31 42.27 14.51 43.86\nand high word modiﬁcation rate. Among the remaining samples, around 30 −40% examples are\nﬁltered due to the low human agreement rates (Human Consensus Filtering), and around 20 −30%\nare ﬁltered due to the semantic changes which lead to the label changes (Utility Preserving Filtering).\nWe also note that the data curation procedures are indispensable for the adversarial evaluation, as the\nFleiss Kappa before curation is very low, suggesting that a lot of adversarial sentences have unreliable\nlabels and thus tend to underestimate the model robustness against the textual adversarial attacks.\nAfter the data curation, our AdvGLUE shows a Curated Fleiss Kappa of near 0.6, comparable with\nexisting high-quality dataset such as SNLI and ANLI. Among all the existing attack methods, we\nobserve that TextBugger is the most effective and valid attack method, as it demonstrates the highest\nCurated ASR and Curated Fleiss Kappa across different tasks.\n3.5 Finalizing the Dataset\nThe full pipeline of constructing AdvGLUE is summarized in Figure 1.\nMerging. We note that distraction-based adversarial examples and human-crafted adversarial exam-\nples are guaranteed to be valid by deﬁnition or crowd-sourcing annotations, and thus data curation\nis not needed on these attacks. When merging them with our curated set, we calculate the average\nnumber of samples per attack from our curated set, and sample the same amount of adversarial\nexamples from these attacks following the same label distribution. This way, each attack contributes\nto similar amount of adversarial data, so that AdvGLUE can evaluate models against different types\nof attacks with similar weights and provide a comprehensive and unbiased diagnostic report.\nDev-Test Split. After collecting the adversarial examples from the considered attacks, we split the\nﬁnal dataset into a dev set and a test set. In particular, we ﬁrst randomly split the benign data into\n9 : 1, and the adversarial examples generated based on 90% of the benign data serve as the hidden\ntest set, while the others are published as the dev set. For human-crafted adversarial examples, since\nthey are not generated based on the benign GLUE data, we randomly select 90% of the data as the\ntest set, and the remaining 10% as the dev set. The dev set is publicly released to help participants to\nunderstand the tasks and the data format. To protect the integrity of our test data, the test set will not\nbe released to the public. Instead, participants are required to upload the model to CodaLab, which\nautomates the evaluation process on the hidden test set and provides a diagnostic report.\n4 Diagnostic Report for Language Models\nBenchmark Results. We follow the ofﬁcial implementations and training scripts of pre-trained\nlanguage models to reproduce results on GLUE and test these models on AdvGLUE. The training\ndetails can be found in Appendix A.6. Results are summarized in Table 4. We observe that although\nstate-of-the-art language models have achieved high performance on GLUE, they are vulnerable to\nvarious adversarial attacks. For instance, the performance gap can be as large as 55% on the SMART\n9\n(BERT) model in terms of the average score. DeBERTa (Large) and ALBERT (XXLarge) achieve\nthe highest average AdvGLUE scores among all the tested language models. This result is also\naligned with the ANLI leaderboard4, which shows that ALBERT (XXLarge) is the most robust to\nhuman-crafted adversarial NLI dataset [38].\nWe note that although our adversarial examples are generated from surrogate models based on BERT\nand RoBERTa, these examples have high transferability between models after our data curation.\nSpeciﬁcally, the average score of ELECTRA (Large) on AdvGLUE is even lower than RoBERTa\n(Large), which demonstrates that AdvGLUE can effectively transfer across models of different\narchitectures and unveil the vulnerabilities shared across multiple models. Moreover, we ﬁnd some\nmodels even perform worse than random guess. For example, the performance of BERT on AdvGLUE\nfor all tasks is lower than random-guess accuracy.\nWe also benchmark advanced robust training methods to evaluate whether these methods can indeed\nprovide robustness improvement on AdvGLUE and to what extent. We observe that SMART\nand FreeLB are particularly helpful to improve robustness for RoBERTa. Speciﬁcally, SMART\n(RoBERTa) improves RoBERTa (Large) over3.71% on average, and it even improves the benign\naccuracy as well. Since InfoBERT is not evaluated on GLUE, we run InfoBERT with different\nhyper-parameters and report the best accuracy on benign GLUE dev set and AdvGLUE test set.\nHowever, we ﬁnd that the benign accuracy of InfoBERT (RoBERTa) is still lower than RoBERTa\n(Large), and similarly for the robust accuracy. These results suggest that existing robust training\nmethods only have incremental robustness improvement, and there is still a long way to go to develop\nrobust models to achieve satisfactory performance on AdvGLUE.\nDiagnostic Report of Model Vulnerabilities. To have a systematic understanding of which adver-\nsarial attacks language models are vulnerable to, we provide a detailed diagnostic report in Table 5.\nWe observe that models are most vulnerable to human-crafted examples, where complex linguistic\nphenomena (e.g., numerical reasoning, negation and coreference resolution) can be found. For\nsentence-level perturbations, models are more vulnerable to distraction-based perturbations than\ndirectly manipulating syntactic structures. In terms of word-level perturbations, models are similarly\nvulnerable to different word replacement strategies, among which typo-based perturbations and\nknowledge-guided perturbations are the most effective attacks.\nWe hope the above ﬁndings can help researchers systematically examine their models against different\nadversarial attacks, thus also devising new methods to defend against them. Comprehensive analysis\nof the model robustness report is provided in our website and Appendix A.9.\n5 Conclusion\nWe introduce AdvGLUE, a multi-task benchmark to evaluate and analyze the robustness of state-\nof-the-art language models and robust training methods. We systematically conduct 14 adversarial\nattacks on GLUE tasks and adopt crowd-sourcing to guarantee the quality and validity of generated\nadversarial examples. Modern language models perform poorly on AdvGLUE, suggesting that\nmodel vulnerabilities to adversarial attacks still remain unsolved. We hope AdvGLUE can serve as a\ncomprehensive and reliable diagnostic benchmark for researchers to further develop robust models.\nAcknowledgments and Disclosure of Funding\nWe thank the anonymous reviewers for their constructive feedback. We also thank Prof. Sam\nBowman, Dr. Adina Williams, Nikita Nangia, Jinfeng Li, and many others for the helpful discussion.\nWe thank Prof. Robin Jia and Yixin Nie for allowing us to incorporate their datasets as part of the\nevaluation. We thank the SQuAD team for allowing us to use their website template and submission\ntutorials. This work is partially supported by the NSF grant No.1910100, NSF CNS 20-46726 CAR,\nthe Amazon Research Award.\n4https://github.com/facebookresearch/anli\n10\nReferences\n[1] M. Bartolo, A. Roberts, J. Welbl, S. Riedel, and P. Stenetorp. Beat the ai: Investigating\nadversarial human annotation for reading comprehension. Transactions of the Association for\nComputational Linguistics, 8:662–678, 2020.\n[2] S. R. Bowman and G. E. Dahl. What will it take to ﬁx benchmarking in natural language\nunderstanding? In NAACL, 2021.\n[3] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning. A large annotated corpus for learning\nnatural language inference. In L. Màrquez, C. Callison-Burch, J. Su, D. Pighin, and Y . Marton,\neditors, EMNLP, 2015.\n[4] K. Burghardt, T. Hogg, R. D’Souza, K. Lerman, and M. Posfai. Origins of algorithmic\ninstabilities in crowdsourced ranking. Proceedings of the ACM on Human-Computer Interaction,\n4(CSCW2):1–20, 2020.\n[5] N. Carlini and D. A. Wagner. Audio adversarial examples: Targeted attacks on speech-to-text.\n2018 IEEE Security and Privacy Workshops (SPW), pages 1–7, 2018.\n[6] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning. Electra: Pre-training text encoders as\ndiscriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\n[7] J. M. Cohen, E. Rosenfeld, and J. Z. Kolter. Certiﬁed adversarial robustness via randomized\nsmoothing. In ICML, 2019.\n[8] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional\ntransformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors,\nNAACL-HLT, 2019.\n[9] K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic, B. O’Donoghue, J. Uesato, and P. Kohli.\nTraining veriﬁed learners with learned veriﬁers. CoRR, abs/1805.10265, 2018.\n[10] J. Ebrahimi, A. Rao, D. Lowd, and D. Dou. Hotﬂip: White-box adversarial examples for text\nclassiﬁcation. In ACL, 2018.\n[11] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and\nD. X. Song. Robust physical-world attacks on deep learning models. 2017.\n[12] Z. Gan, Y .-C. Chen, L. Li, C. Zhu, Y . Cheng, and J. Liu. Large-scale adversarial training for\nvision-and-language representation learning. arXiv preprint arXiv:2006.06195, 2020.\n[13] S. Garg and G. Ramakrishnan. Bae: Bert-based adversarial examples for text classiﬁcation. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 6174–6181, 2020.\n[14] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daumé III, and\nK. Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.\n[15] K. Goel, N. Rajani, J. Vig, S. Tan, J. Wu, S. Zheng, C. Xiong, M. Bansal, and C. Ré. Robustness\ngym: Unifying the nlp evaluation landscape. arXiv preprint arXiv:2101.04840, 2021.\n[16] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples.\nCoRR, abs/1412.6572, 2015.\n[17] T. Gui, X. Wang, Q. Zhang, Q. Liu, Y . Zou, X. Zhou, R. Zheng, C. Zhang, Q. Wu, J. Ye, et al.\nTextﬂint: Uniﬁed multilingual robustness evaluation toolkit for natural language processing.\narXiv preprint arXiv:2103.11441, 2021.\n[18] P. He, X. Liu, J. Gao, and W. Chen. Deberta: Decoding-enhanced bert with disentangled\nattention. arXiv preprint arXiv:2006.03654, 2020.\n[19] P. Huang, R. Stanforth, J. Welbl, C. Dyer, D. Yogatama, S. Gowal, K. Dvijotham, and P. Kohli.\nAchieving veriﬁed robustness to symbol substitutions via interval bound propagation. In\nEMNLP-IJCNLP, 2019.\n[20] M. Iyyer, J. Wieting, K. Gimpel, and L. Zettlemoyer. Adversarial example generation with\nsyntactically controlled paraphrase networks. In NAACL-HLT, 2018.\n[21] R. Jia and P. Liang. Adversarial examples for evaluating reading comprehension systems. In\nM. Palmer, R. Hwa, and S. Riedel, editors, EMNLP, 2017.\n[22] R. Jia, A. Raghunathan, K. Göksel, and P. Liang. Certiﬁed robustness to adversarial word\nsubstitutions. In EMNLP-IJCNLP, 2019.\n11\n[23] H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and T. Zhao. SMART: robust and efﬁcient ﬁne-\ntuning for pre-trained natural language models through principled regularized optimization. In\nD. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, editors,ACL, 2020.\n[24] D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits. Is BERT really robust? A strong baseline for natural\nlanguage attack on text classiﬁcation and entailment. In AAAI, 2020.\n[25] D. Kiela, M. Bartolo, Y . Nie, D. Kaushik, A. Geiger, Z. Wu, B. Vidgen, G. Prasad, A. Singh,\nP. Ringshia, Z. Ma, T. Thrush, S. Riedel, Z. Waseem, P. Stenetorp, R. Jia, M. Bansal, C. Potts,\nand A. Williams. Dynabench: Rethinking benchmarking in nlp. In NAACL, 2021.\n[26] Z.-Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. Albert: A lite bert for\nself-supervised learning of language representations. ArXiv, abs/1909.11942, 2019.\n[27] J. Li, S. Ji, T. Du, B. Li, and T. Wang. Textbugger: Generating adversarial text against real-world\napplications. In NDSS, 2019.\n[28] J. Li, T. Du, S. Ji, R. Zhang, Q. Lu, M. Yang, and T. Wang. Textshield: Robust text classiﬁcation\nbased on multimodal embedding and neural machine translation. In 29th USENIX Security\nSymposium (USENIX Security 20). USENIX Association, 2020.\n[29] L. Li, R. Ma, Q. Guo, X. Xue, and X. Qiu. Bert-attack: Adversarial attack against bert using bert.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 6193–6202, 2020.\n[30] X. Liu, H. Cheng, P. He, W. Chen, Y . Wang, H. Poon, and J. Gao. Adversarial training for large\nneural language models. CoRR, abs/2004.08994, 2020.\n[31] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and\nV . Stoyanov. Roberta: A robustly optimized BERT pretraining approach.CoRR, abs/1907.11692,\n2019.\n[32] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of\nwords and phrases and their compositionality. In C. J. C. Burges, L. Bottou, Z. Ghahramani,\nand K. Q. Weinberger, editors,Advances in Neural Information Processing Systems 26: 27th\nAnnual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting\nheld December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 3111–3119, 2013.\n[33] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: A simple and accurate method\nto fool deep neural networks. CVPR, pages 2574–2582, 2016.\n[34] J. Morris, E. Liﬂand, J. Lanchantin, Y . Ji, and Y . Qi. Reevaluating adversarial examples in\nnatural language. In Findings of the Association for Computational Linguistics: EMNLP 2020,\nOnline, Nov. 2020. Association for Computational Linguistics.\n[35] J. X. Morris, E. Liﬂand, J. Y . Yoo, J. Grigsby, D. Jin, and Y . Qi. Textattack: A framework\nfor adversarial attacks, data augmentation, and adversarial training in nlp. arXiv preprint\narXiv:2005.05909, 2020.\n[36] A. Naik, A. Ravichander, N. Sadeh, C. Rose, and G. Neubig. Stress test evaluation for natural\nlanguage inference. arXiv preprint arXiv:1806.00692, 2018.\n[37] N. Nangia and S. Bowman. Human vs. muppet: A conservative estimate of human performance\non the glue benchmark. In ACL, 2019.\n[38] Y . Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, and D. Kiela. Adversarial NLI: A new\nbenchmark for natural language understanding. In D. Jurafsky, J. Chai, N. Schluter, and J. R.\nTetreault, editors, ACL, 2020.\n[39] N. Papernot, P. D. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to\nadversarial perturbations against deep neural networks. 2016 IEEE Symposium on Security and\nPrivacy (SP), pages 582–597, 2016.\n[40] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation.\nIn A. Moschitti, B. Pang, and W. Daelemans, editors, Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014,\nDoha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1532–1543.\nACL, 2014.\n[41] F. Qi, C. Yang, Z. Liu, Q. Dong, M. Sun, and Z. Dong. Openhownet: An open sememe-based\nlexical knowledge base. ArXiv, abs/1901.09957, 2019.\n12\n[42] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine\ncomprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n[43] M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh. Beyond accuracy: Behavioral testing of NLP\nmodels with CheckList. In ACL, pages 4902–4912, July 2020.\n[44] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y . Ng, and C. Potts. Recursive\ndeep models for semantic compositionality over a sentiment treebank. In Proceedings of the\n2013 conference on empirical methods in natural language processing, pages 1631–1642, 2013.\n[45] J. Thorne and A. Vlachos. Adversarial attacks against fact extraction and veriﬁcation. CoRR,\nabs/1903.05543, 2019.\n[46] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal. Fever: a large-scale dataset for\nfact extraction and veriﬁcation. In NAACL-HLT, 2018.\n[47] E. Wall, A. Narechania, A. Coscia, J. Paden, and A. Endert. Left, right, and gender: Exploring\ninteraction traces to mitigate human biases. arXiv preprint arXiv:2108.03536, 2021.\n[48] A. Wang, Y . Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R.\nBowman. Superglue: A stickier benchmark for general-purpose language understanding\nsystems. In NeurIPS, 2019.\n[49] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task\nbenchmark and analysis platform for natural language understanding. In ICLR, 2019.\n[50] B. Wang, H. Pei, B. Pan, Q. Chen, S. Wang, and B. Li. T3: Tree-autoencoder constrained\nadversarial text generation for targeted attack. In EMNLP, 2020.\n[51] B. Wang, S. Wang, Y . Cheng, Z. Gan, R. Jia, B. Li, and J. Liu. Infobert: Improving robustness\nof language models from an information theoretic perspective. In ICLR, 2021.\n[52] J. Wieting and K. Gimpel. Paranmt-50m: Pushing the limits of paraphrastic sentence embed-\ndings with millions of machine translations. arXiv preprint arXiv:1711.05732, 2017.\n[53] A. Williams, N. Nangia, and S. R. Bowman. A broad-coverage challenge corpus for sentence\nunderstanding through inference. arXiv preprint arXiv:1704.05426, 2017.\n[54] Z. Yang, B. Li, P.-Y . Chen, and D. X. Song. Characterizing audio adversarial examples using\ntemporal dependency. ArXiv, abs/1809.10875, 2018.\n[55] Z. Yang, Z. Dai, Y . Yang, J. G. Carbonell, R. Salakhutdinov, and Q. V . Le. Xlnet: Generalized\nautoregressive pretraining for language understanding. In NeurIPS, 2019.\n[56] M. Ye, C. Gong, and Q. Liu. SAFER: A structure-free approach for certiﬁed robustness to\nadversarial word substitutions. In ACL, 2020.\n[57] Y . Zang, F. Qi, C. Yang, Z. Liu, M. Zhang, Q. Liu, and M. Sun. Word-level textual adversarial\nattacking as combinatorial optimization. In ACL, 2020.\n[58] G. Zeng, F. Qi, Q. Zhou, T. Zhang, B. Hou, Y . Zang, Z. Liu, and M. Sun. Openattack: An\nopen-source textual adversarial attack toolkit. arXiv preprint arXiv:2009.09191, 2020.\n[59] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, and Y . Artzi. Bertscore: Evaluating text\ngeneration with bert. In ICLR, 2019.\n[60] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu. Ernie: Enhanced language representation\nwith informative entities. In ACL, 2019.\n[61] C. Zhu, Y . Cheng, Z. Gan, S. Sun, T. Goldstein, and J. Liu. Freelb: Enhanced adversarial\ntraining for natural language understanding. In ICLR, 2020.\n13\nA Appendix\nA.1 Glossary of Adversarial Attacks\nWe present a glossary of adversarial attacks considered in AdvGLUE in Table 6 and 7.\nA.2 Additional Related Work\nWe discuss more related work about textual adversarial attacks and defenses in this subsection.\nTextual Adversarial Attacks Recent research has shown deep neural networks (DNNs) are vul-\nnerable to adversarial examples that are carefully crafted to fool machine learning models without\ndisturbing human perception [16, 39, 33]. However, compared with a large amount of adversarial\nattacks in continuous data domain [54, 5, 11], there are a few studies focusing on the discrete text\ndomain. Most existing gradient-based attacks on image or audio models are no longer applicable to\nNLP models, as words are intrinsically discrete tokens. Another challenge for generating adversarial\ntext is to ensure the semantic and syntactic coherence and consistency.\nExisting textual adversarial attacks can be roughly divided into three categories: word-level transfor-\nmations, sentence-level attacks, and human-crafted samples. (i) Word-level transformations adopt\ndifferent word replacement strategies during attack. For example, existing work [ 27, 10] applies\ncharacter-level perturbation to carefully crafted typo words (e.g., from “foolish” to “fo0lish”), thus\nmaking the model ignore or misunderstand the original statistical cues. Others adopt knowledge-\nbased perturbation and utialize knowledge base to constrain the search space. For example, Zang\net al. [57] uses sememe-based knowledge base from HowNet [41] to construct a search space for\nword substitution. Some [ 24, 27] use non-contextualized word embedding from GLoVe [ 40] or\nWord2Vec [32] to build synonym candidates, by querying the cosine similarity or euclidean distance\nbetween the original and candidate word and selecting the closet ones as the replacements. Recent\nwork [13, 29] also leverages BERT to generate contextualized perturbations by masked language\nmodeling. (ii) Different from the dominant word-level adversarial attacks, sentence-level adversarial\nattacks perform sentence-level transformation or paraphrasing by perturbing the syntactic structures\nbased on human crafted rules [36, 43] or carefully designed auto-encoders [20, 50]. Sentence-level\nmanipulations are generally more challenging than word-level attacks, because the perturbation\nspace for syntactic structures are limited compared to word-level perturbation spaces that grow\nexponentially with the sentence length. However, sentence-level attacks tend to have higher linguistic\nquality than word-level, as both semantic and syntactic coherence are taken into considerations when\ngenerating adversarial sentences. (iii) Human-crafted adversarial examples are generally crafted in\nthe human-in-the-loop manner [21, 38, 1] or use manually crafted templates to generate test cases\n[36, 43]. Our AdvGLUE incorporates all of the above textual adversarial to provide a comprehensive\nand systematic diagnostic report over existing state-of-the-art large-scale language models.\nDefenses against Textual Adversarial Attacks To defend against textual adversarial attacks,\nexisting work can be classiﬁed into three categories: (i) Adversarial Training is a practical method\nto defend against adversarial examples. Existing work either uses PGD-based attacks to generate\nadversarial examples in the embedding space of NLP as data augmentation [61], or regularizes the\nstandard objective using virtual adversarial training [23, 30, 12]. However, one drawback is that the\nthreat model is often unknown, which renders adversarial training less effective when facing unseen\nattacks. (ii) Interval Bound Propagation (IBP) [9] is proposed as a new technique to consider the\nworst-case perturbation theoretically. Recent work [19, 22] has applied IBP in the NLP domain to\ncertify the robustness of models. However, IBP-based methods rely on strong assumptions of model\narchitecture and are difﬁcult to adapt to recent transformer-based language models. (iii) Randomized\nSmoothing [7] provides a tight robustness guarantee in ℓ2 norm by smoothing the classiﬁer with\nGaussian noise. Ye et al. [56] adapts the idea to the NLP domain, and replace the Gaussian noise with\nsynonym words to certify the robustness as long as adversarial word substitution falls into predeﬁned\nsynonym sets. However, to guarantee the completeness of the synonym set is challenging.\nA.3 Task Descriptions, Statistics and Evaluation Metrics\nWe present the detailed label distribution statistics and evaluation metrics of GLUE and AdvGLUE\nbenchmark in 8.\n14\nTable 6: Glossary of adversarial attacks (word-level and sentence-level) in AdvGLUE.For each adversarial\nattack, we provide a brief explanation and a corresponding example in AdvGLUE.\nPerturbations Explanation Examples(Strikethrough = Original Text, red = Adver-\nsarial Perturbation)\nTextBugger\n(Word-level /\nTypo-based)\nTextBugger ﬁrst identiﬁes the important words\nin each sentence and then replaces them with\ncarefully crafted typos.\nTask:QNLI\nQuestion: What was the population of the Dutch Repub-\nlic before this emigration?\nSentence: This was ahuge hu ge inﬂux as the entire\npopulation of the Dutch Republic amounted to ca.\nPrediction:False→True\nTextFooler\n(Word-level /\nEmbedding-\nsimilarity-\nbased)\nEmbedding-similarity-based adversarial attacks\nsuch as TextFooler select synonyms according\nto the cosine similarity of word embeddings.\nWords that have high similarity scores will be\nused as candidates to replace original words in\nthe sentences.\nTask:QQP\nQuestion 1: I am getting fat on my lower body and on\nthechest torso, is there any way I can get ﬁt without\nlooking skinny fat?\nQuestion 2: Why I am getting skinny instead of losing\nbody fat?\nPrediction:Not Equivalent→Equivalent\nBERT-\nATTACK\n(Word-level /\nContext-aware)\nBERT-ATTACK uses pre-trained BERT to\nperform masked language prediction to generate\ncontextualized potential word replacements for\nthose crucial words.\nTask:MNLI\nPremise: Do you know what this is? With a dramatic\ngesture she ﬂung back the left side of hercoat sleeve and\nexposed a small enamelled badge.\nHypothesis: The coat that she wore was long enough to\ncover her knees .\nPrediction:Neutral→Contradiction\nSememePSO\n(Word-level /\nKnowledge-\nguided)\nKnowledge-guided adversarial attacks such as\nSememePSO use external knowledge base such\nas HowNet or WordNet to search for\nsubstitutions.\nTask:QQP\nQuestion 1: What people who you’ve never met have\ninﬂuenced infected your life the most?\nQuestion 2: Who are people you have never met who\nhave had the greatest inﬂuence on your life?\nPrediction:Equivalent→Not Equivalent\nCompAttack\n(Word-level /\nCompositions)\nCompAttack is a whitebox-based adversarial\nattack that integrates all other word-level\nperturbation methods in one algorithm to\nevaluate model robustness to various adversarial\ntransformations.\nTask:SST-2\nSentence: The primitive force of this ﬁlm seems tobub-\nble bybble up from the vast collective memory of the\ncombatants.\nPrediction:Positive→Negative\nSCPN\n(Sent.-level /\nSyntactic-\nbased)\nSCPN is an attack method based on syntax tree\ntransformations. It is trained to produce a\nparaphrase of a given sentence with speciﬁed\nsyntactic structures.\nTask:RTE\nSentence 1: He became a boxing referee in 1964 and\nbecame most well-known for his decision against Mike\nTyson, during the Holyﬁeld ﬁght, when Tyson bit Holy-\nﬁeld’s ear.\nSentence 2: Mike Tyson bitHolyﬁeld’sear in 1964.\nPrediction:Not Entailment→Entailment\nT3 (Sent.-level /\nSyntactic-\nbased)\nT3 is a whitebox attack algorithm that can add\nperturbations on different levels of the syntax\ntree and generate the adversarial sentence.\nTask:MNLI\nPremise: What’s truly striking, though, is that Jobshas\nhad never really let this idea go.\nHypothesis: Jobs never held onto an idea for long.\nPrediction:Contradiction→Entailment\nAdvFever\n(Sent.-level /\nSyntactic-\nbased)\nEntailment preserving rules proposed by\nAdvFever transform all the sentences satisfying\nthe templates into semantically equivalent ones.\nTask:SST-2\nSentence: I’llbetthevideogameis There exists a lot\nmore fun than the ﬁlm that goes by the name of i ’ll bet\nthe video game.\nPrediction:Negative→Positive\nStressTest\n(Sent.-level /\nDistraction-\nbased)\nStressTest appends three true statements (“and\ntrue is true”, “and false is not true”, “and true is\ntrue” for ﬁve times) to the end of the hypothesis\nsentence for NLI tasks.\nTask:RTE\nSentence 1: Yet, we now are discovering that antibiotics\nare losing their effectiveness against illness. Disease-\ncausing bacteria are mutating faster than we can come\nup with new antibiotics to ﬁght the new variations.\nSentence 2: Bacteria is winning the war against antibi-\notics and true is true.\nPrediction:Entailment→Not Entailment\nCheckList\n(Sent.-level /\nDistraction-\nbased)\nCheckList adds randomly generated URLs and\nhandles to distract model attention.\nTask:QNLI\nQuestion: What was the population of the Dutch Repub-\nlic before this emigration? https://t.co/DlI9kw\nSentence: This was a huge inﬂux as the entire popula-\ntion of the Dutch Republic amounted to ca.\nPrediction:False→True\n15\nTable 7: Glossary of adversarial attacks (human-crafted) in AdvGLUE. For each adversarial attack, we\nprovide a brief explanation and a corresponding example in AdvGLUE.\nPerturbations Explanation Examples(Strikethrough = Original Text, red = Adver-\nsarial Perturbation)\nCheckList\n(Human-\ncrafted)\nCheckList analyses different capabilities of\nNLP models using different test types. We adopt\ntwo capability tests:TemporalandNegation,\nwhich test if the model understands the order of\nevents and if the model is sensitive to negations.\nTask:SST-2\nSentence: I think this movie is perfect, but I used to\nthink it was annoying.\nPrediction:Positive→Negative\nStressTest\n(Human-\ncrafted)\nStressTest proposes carefully crafted rules to\nconstruct “stress tests” and evaluate robustness\nof NLI models to speciﬁc linguistic phenomena.\nHere we adopt the test cases focusing on\nNumerical Reasoning.\nTask:MNLI\nPremise: If Anne’ s speed were doubled, they could\nclean their house in 3 hours working at their respective\nrates.\nHypothesis: If Anne’ s speed were doubled, they could\nclean their house in less than 6 hours working at their\nrespective rates.\nPrediction:Entailment→Contradiction\nANLI (Human-\ncrafted)\nANLI is a large-scale NLI dataset collected\niteratively in a human-in-the-loop manner. The\nsentence pairs generated in each round form a\ncomprehensive dataset that aims at examining\nthe vulnerability of NLI models.\nTask:MNLI\nPremise: Kamila Filipcikova (born 1991) is a female\nSlovakian fashion model. She has modeled in fash-\nion shows for designers such as Marc Jacobs, Chanel,\nGivenchy, Dolce & Gabbana, and Sonia Rykiel. And\nappeared on the cover of V ogue Italia two times in a row.\nHypothesis: Filipcikova lives in Italy.\nPrediction:Neutral→Contradiction\nAdvSQuAD\n(Human-\ncrafted)\nAdvSQuAD is an adversarial dataset targeting\nat reading comprehension systems. Examples\nare generated by appending a distracting\nsentence to the end of the input paragraph. We\nadopt the distracting sentences and questions in\ntheQNLIformat with labels “not answered”.\nTask:QNLI\nQuestion: What day was the Super Bowl played on?\nSentence: The Champ Bowl was played on August\n18th,1991.\nPrediction:False→True\nSST-2 The Stanford Sentiment Treebank [44] consists of sentences from movie reviews and human\nannotations of their sentiment. Given a review sentence, the task is to predict the sentiment of it.\nSentiments can be divided into two classes: positive and negative.\nQQP The Quora Question Pairs (QQP) dataset is a collection of question pairs from the commu-\nnity question-answering website Quora. The task is to determine whether a pair of questions are\nsemantically equivalent.\nMNLI The Multi-Genre Natural Language Inference Corpus [53] consists of sentence pairs with\ntextual entailment annotations. Given a premise sentence and a hypothesis sentence, the task\nis to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis\n(contradiction), or neither (neutral)\nQNLI Question-answering NLI (QNLI) dataset consists of question-sentence pairs modiﬁed from\nThe Stanford Question Answering Dataset [42]. The task is to determine whether the context sentence\ncontains the answer to the question.\nRTE The Recognizing Textual Entailment (RTE) dataset is a combination of a series of data from\nannual textual entailment challenges. Examples are constructed based on news and Wikipedia text.\nThe task is to predict the relationship between a pair of sentences. For consistency, the relationship\ncan be classiﬁed into two classes: entailment and not entailment, where neutral and contradiction are\nseen as not entailment.\nWe also show the detailed per-task model performance on AdvGLUE and GLUE in Table 9.\nA.4 Implementation Details of Adversarial Attacks\nTextBugger To ensure the small magnitude of the perturbation, we consider the following ﬁve\nstrategies: (i) randomly inserting a space into a word; (ii) randomly deleting a character of a word;\n(iii) randomly replacing a character of a word with its adjacent character in the keyboard; ( iv)\nrandomly replacing a character of a word with its visually similar counterpart (e.g., “0” v.s. “o”, “1”\n16\nTable 8: The label distribution of AdvGLUE dataset. For SST-2, we report the label distribution as “nega-\ntive”:“positive”. For QQP, we report the label distribution as “not equivalent”:“equivalent”. For QNLI, we\nreport the label distribution as “true”:“false”. For RTE, we report the label distribution as “entailment”:“not\nentailment”. For MNLI, we report the label distribution as “entailment”:“neutral”:“contradiction”.\nCorpus Task |Dev|\n(GLUE)\n|Test|\n(GLUE)\n|Dev|\n(AdvGLUE)\n|Test|\n(AdvGLUE) Evaluation Metrics\nSST-2 sentiment 428:444 1821 72:76 590:830 acc.\nQQP paraphrase 25,545:14,885 390,965 46:32 297:125 acc./F1\nQNLI NLI/QA 2,702:2,761 5,463 74:74 394:574 acc.\nRTE NLI 146:131 3,000 35:46 123:181 acc.\nMNLI NLI 6,942:6,252:6,453 19,643 92:84:107 706:565:593 matched acc./mismatched acc.\nTable 9: Model performance on AdvGLUE test set and GLUE dev set.\nModels Avg SST-2 MNLI RTE QNLI QQP\nGLUE AdvGLUE GLUE AdvGLUE GLUE AdvGLUE GLUE AdvGLUE GLUE AdvGLUE GLUE AdvGLUE\nBERT(Large) 85.76 33.68 93.23 33.03 85.78/85.57 28.72/27.05 68.95 40.46 91.91 39.77 90.72/87.38 37.91/16.56RoBERTa(Large)91.44 50.21 95.99 58.52 89.74/89.86 50.78/39.62 86.60 45.39 94.14 52.48 91.99/89.37 57.11/41.80T5(Large) 90.39 56.82 95.53 60.56 88.98/89.20 48.43/38.98 84.12 62.83 93.78 57.64 90.82/88.07 63.03/55.68ALBERT(XXLarge)91.87 59.22 95.18 66.83 89.29/89.88 51.83/44.17 88.45 73.03 95.26 63.84 92.26/89.49 56.40/32.35ELECTRA(Large)93.16 41.69 97.13 58.59 90.71 14.62/20.22 90.25 23.03 95.17 57.54 92.56 61.37/42.40DeBERTa(Large)92.67 60.86 96.33 57.89 90.95/90.85 58.36/52.46 90.25 78.94 94.86 57.85 92.29/89.69 60.43/47.98SMART(BERT)85.70 30.29 93.35 25.21 84.72/85.34 26.89/23.32 69.68 38.16 91.71 34.61 90.25/87.22 36.49/20.24SMART(RoBERTa)92.62 53.71 96.56 50.92 90.75/90.66 45.56/36.07 90.98 70.39 95.04 52.17 91.20/88.44 64.22/44.28FreeLB(RoBERTa)92.28 50.47 96.44 61.69 90.64 31.59/27.60 86.69 62.17 95.04 62.29 92.58 42.18/31.07InfoBERT(RoBERTa)89.06 46.04 96.22 47.61 89.67/89.27 50.39/41.26 74.01 39.47 94.62 54.86 92.25/89.70 49.29/35.54\nv.s. “l”); and (v) randomly swapping two characters in a word. The ﬁrst four strategies guarantee the\nword edit distance between the typo word and its original word to be 1, and that of the last strategy is\nlimited to 2. Following the default setting, in Strategy (i), we only insert a space into a word when\nthe word contains less than 6 characters. In Strategy (v), we swap characters in a word only when the\nword has more than 4 characters.\nTextFooler Concretely, for the sentiment analysis tasks, we set the cosine similarity threshold to\nbe 0.8, which encourages the synonyms to be semantically close to original ones and enhances the\nquality of adversarial data. For the rest of the tasks, we follow the default hyper-parameter to set the\ncosine similarity threshold to be 0.7. Besides, the number of synonyms for each word is set to 50\nfollowing the default setting.\nBERT-ATTACK We follow the hyper-parameters from the ofﬁcial codebase, and set the number\nof candidate words to 48 and cosine similarity threshold to 0.4 in order to ﬁlter out antonyms\nusing synonym dictionaries, as BERT masked language model does not distinguish synonyms and\nantonyms.\nSememePSO We adopt the ofﬁcial hyper-parameters in which maximum and minimum inertia\nweights are set to 0.8 and 0.2, respectively. We also set the maximum and minimum movement\nprobabilities of the particles to 0.8 and 0.2, respectively, following the default setting. Population\nsize is set to 60 in every task.\nCompAttack We follow the T3 [ 50] and C&W attack [ 5] and design the same optimization\nobjective for adversarial perturbation generation in the embedding space as:\nL(e∗) =||e∗||p + c ·g(x′), (3)\nwhere the ﬁrst term controls the magnitude of perturbation, while g(·) is the attack objective function\ndepending on the attack scenario. c weighs the attack goal against attack cost. CompAttack constrains\nthe perturbation to be close to pre-deﬁned perturbation space, including typo space (e.g., TextBugger),\nknowledge space (e.g., WordNet) and contextualized embedding space ( e.g., BERT embedding\nclusters) to make sure the perturbation is valid. We can also see from Table 3 that CompAttack overall\nhas lower ﬁlter rate than other state-of-the-art attack methods.\nSCPN We use the pre-trained SCPN models released by the ofﬁcial codebase. Following the\ndefault setting, we select the most frequent 10 templates from ParaNMT-50M corpus [52] to guide\nthe generation process. We ﬁrst parse sentences from GLUE dev set using Stanford CoreNLP. We\nused CoreNLP version 3.7.0 in our experiment, along with the Shift-Reduce Parser models.\n17\nTable 10: Examples of AdvGLUE benchmark.\nTask Linguistic Phe-\nnomenon\nSamples (Strikethrough = Original Text, red = Adversarial Perturbation)Label→Pre-\ndiction\nSST-2 Typo\n(Word-level)\nSentence: The primitive force of this ﬁlm seems tobubble bybble up\nfrom the vast collective memory of the combatants.\nPositive\n→Negative\nSST-2 Context-aware\n(Word-level)\nSentence: In execution , this clever idea is farless smaller funny than\nthe original , killers from space.\nNegative\n→Positive\nSST-2 CheckList\n(Human-crafted)\nSentence: I think this movie is perfect, but I used to think it was annoy-\ning.\nPositive\n→Negative\nQQP Embedding\n(Word-level)\nQuestion 1: I am getting fat on my lower body and on thechest torso, is\nthere any way I can get ﬁt without looking skinny fat? Not Equivalent\n→EquivalentQuestion 2: Why I am getting skinny instead of losing body fat?\nQQP Syntactic\n(Sent.-level)\nQuestion 1: CanI learnMMAat theageof 26? You can learn MMA at\n24? Not Equivalent\n→EquivalentQuestion 2: Can I learn MMA at the age of 24?\nQQP CheckList\n(Human-crafted)\nQuestion 1: Is Alfred Kennedy an analyst? Not Equivalent\n→EquivalentQuestion 2: Is Alfred Kennedy becoming an analyst?\nMNLI Typo\n(Word-level)\nPremise: uh-huh how about anymatching mathcing programs Entailment →\nContradictionHypothesis: What about matching programs?\nMNLI Distraction\n(Sent.-level)\nPremise: You and your friends are not welcome here, said Severn. Entailment→\nContradictionHypothesis: Severn said the people were not welcome there and true is\ntrue.\nMNLI ANLI\n(Human-crafted)\nPremise: Kamila Filipcikova (born 1991) is a female Slovakian fashion\nmodel. She has modeled in fashion shows for designers such as Marc\nJacobs, Chanel, Givenchy, Dolce & Gabbana, and Sonia Rykiel. And\nappeared on the cover of V ogue Italia two times in a row.\nNeutral→\nContradiction\nHypothesis: Filipcikova lives in Italy.\nQNLI Distraction\n(Sent.-level)\nQuestion: What was the population of the Dutch Republic before this\nemigration? https://t.co/DlI9kw False→TrueSentence: This was a huge inﬂux as the entire population of the Dutch\nRepublic amounted to ca.\nQNLI AdvSQuAD\n(Human-crafted)\nQuestion: What day was the Super Bowl played on? False→TrueSentence: The Champ Bowl was played on August 18th,1991.\nRTE Knowledge\n(Word-level)\nSentence 1: In Nigeria, by far the most populous country in sub-Saharan\nAfrica, over 2.7 million peopleare exist infected with HIV . Not Entailment\n→EntailmentSentence 2: 2.7 percent of the people infected with HIV live in Africa.\nRTE Syntactic\n(Sent.-level)\nSentence 1: He became a boxing referee in 1964 and became most\nwell-known for his decision against Mike Tyson, during the Holyﬁeld\nﬁght, when Tyson bit Holyﬁeld’s ear.\nNot Entailment\n→Entailment\nSentence 2: Mike Tyson bitHolyﬁeld’sear in 1964.\nT3 We follow the hyper-parameters in the ofﬁcial setting where the scaling const is set to 1e4 and\nthe optimizing conﬁdence is set to 0. In each iteration, we optimize the perturbation vector for at\nmost 100 steps with learning rate 0.1.\nAdvFever We follow the entailment preserving rules proposed by the ofﬁcial implementation. We\nadopt all 23 templates to transform original sentences into semantically equivalent ones. Many\ncommon sentence patterns in everyday life are included in these templates.\nA.5 Examples of AdvGLUE benchmark\nWe show more comprehensive examples in Table 10. Examples are generated with different levels of\nperturbations and they all can successfully change the predictions of all surrogate models (BERT,\nRoBERTa and RoBERTa ensemble).\nA.6 Fine-tuning Details of Large-Scale Language Models\nFor all the experiments, we are using a GPU cluster with 8 V100 GPUs and 256GB memory.\n18\nTable 11: The statistics of AdvGLUE in the human training phase.\nCorpus Pay Rate #/ Qualiﬁed Human Human Fleiss\n(per batch) Workers Acc. (Avg.) Acc. (vote) Kappa\nSST-2 $0.4 70 89.2 95.0 0.738\nMNLI $1.0 33 80.4 85.0 0.615\nRTE $1.0 66 85.8 92.0 0.602\nQNLI $1.0 41 85.6 91.0 0.684\nQQP $0.5 58 86.4 90.0 0.691\nBERT (Large) For RTE, we train our model for 10 epochs and for other tasks we train our model\nfor 4 epochs. Batch size for QNLI is set to 512, and for other tasks it is set to 256. Learning rates are\nall set to 2e −5.\nELECTRA (Large) We follow the ofﬁcial hyper-parameter setting to set the learning rate to5e−5\nand set batch size to 32. We train ELECTRA on RTE for 10 epochs and train for 2 epochs on other\ntasks. We set the weight decay rate to 0.01 for every task.\nRoBERTa (Large) We train our RoBERTa for10 epochs with learning rate 2e −5 on each task.\nThe batch size for QNLI is 32 and 64 for other tasks.\nT5 (Large) We train our T5 for 10 epochs with learning rate 2e −5 on each task. The batch size\nfor QNLI is 32 and 64 for other tasks. We follow the templates in original paper to convert GLUE\ntasks into generation tasks.\nALBERT (XXLarge) We use the default hyper-parameters to train our ALBERT. For example, max\ntraining steps for SST-2, MNLI, QNLI, QQP, RTE, is20935, 10000, 33112, 14000, 800 respectively.\nFor MNLI and QQP, batch size is set to32 and for other tasks batch size is set to 128.\nDeBERTa (Large) We use the ofﬁcial hyper-parameters to train our DeBERTa. For example,\nlearning rate is set to 1e −5 across all tasks. For MNLI and QQP, batch size is set to 64 and for other\ntasks batch size is set to 32.\nSMART For SMART(BERT) and SMART(RoBERTa), we use grid search to search for the best\nparameters and report the best performance among all trained models.\nFreeLB (RoBERTa) For FreeLB, we test every parameter combination provided by the ofﬁcial\ncodebase and select the best parameters for our training.\nInfoBERT (RoBERTa) We set the batch size to 32 and learning rate to 2e −5 for all tasks.\nA.7 Human Evaluation Details\nHuman Training We present the pay rate and the number of qualiﬁed workers in Table 11. We\nalso test our qualiﬁed workers on another non-overlapping 100 samples of the GLUE dev sets for\neach task. We can see that the human accuracy is comparable to [ 37], which means that most our\nselected annotators understand the GLUE tasks well.\nHuman Filtering The detailed ﬁltering statistics of each stage is shown in Table 12. We can\nsee that around 60 −80% of examples are ﬁltered due to the low transferability and high word\nmodiﬁcation rate. Among the remaining samples, around 30 −40% examples are ﬁltered due to the\nlow human agreement rates (Human Consensus Filtering), and around 20 −30% are ﬁltered due to\nthe semantic changes which lead to the label changes (Utility Preserving Filtering).\nHuman Annotation Instructions We show examples of annotation instructions in the train-\ning phase and ﬁltering phase on MNLI in Figure 2 and 3. More instructions can\nbe found in https://adversarialglue.github.io/instructions. We also provide a\n19\nTable 12: Filter rates during data curation.\nTasks Metrics Word-level Attacks Average\nSememePSO TextFooler TextBugger CombAttack BERT-ATTACK\nSST-2\nTransferability 58.85 63.56 64.87 53.58 66.87 61.54\nFidelity 14.65 11.06 22.40 19.93 12.03 16.01\nHuman Consensus 10.53 10.56 2.27 9.92 7.09 8.07\nUtility Preserving 6.68 5.43 0.51 3.20 3.82 3.93\nFilter Rate 90.71 90.62 90.04 86.63 89.81 89.56\nMNLI\nTransferability 44.16 43.15 42.58 35.08 41.80 41.36\nFidelity 36.57 45.94 37.71 38.14 38.60 39.39\nHuman Consensus 10.37 6.38 5.51 11.15 9.78 8.64\nUtility Preserving 4.49 2.08 1.32 11.07 5.91 4.97\nFilter Rate 95.59 97.55 87.12 95.45 96.10 94.36\nRTE\nTransferability 55.32 67.38 41.96 54.20 60.94 55.96\nFidelity 19.83 7.79 42.18 23.17 14.25 21.44\nHuman Consensus 8.08 7.91 3.55 7.64 8.44 7.12\nUtility Preserving 8.69 6.13 0.60 5.70 8.54 5.93\nFilter Rate 91.93 89.21 88.29 90.72 92.16 90.46\nQNLI\nTransferability 63.36 70.67 59.24 55.47 69.15 63.58\nFidelity 17.73 13.01 25.31 23.53 13.17 18.55\nHuman Consensus 10.06 9.80 6.84 9.98 9.36 9.21\nUtility Preserving 3.48 2.41 1.50 4.94 4.10 3.29\nFilter Rate 94.63 95.89 92.89 93.92 95.78 94.62\nQQP\nTransferability 42.96 58.60 55.09 44.83 51.97 50.69\nFidelity 45.61 29.35 26.46 30.99 37.77 34.04\nHuman Consensus 4.38 4.69 5.19 10.08 3.94 5.66\nUtility Preserving 3.79 3.86 3.16 7.93 4.60 4.67\nFilter Rate 96.73 96.50 89.90 93.83 98.28 95.05\nFAQ document in each task description page https://docs.google.com/document/d/\n1MikHUdyvcsrPqE8x-N-gHaLUNAbA6-Uvy-iA5gkStoc/edit?usp=sharing .\nA.8 Discussion of Limitations\nDue to the constraints of computational resources, we are unable to conduct a comprehensive\nevaluation of all existing language models. However, with the release of our leaderboard website,\nwe are expecting researchers to actively submit their models and evaluate against our AdvGLUE\nbenchmark to have a systematic understanding of model robustness. We are also interested in the\nadversarial robustness of large-scale auto-regressive language models under the few-shot settings,\nand leave it as a compelling future work.\nIn this paper, we follow ANLI [ 38] and generate adversarial examples against surrogate models\nbased on BERT and RoBERTa. However, there are concerns [2] that such adversarial ﬁltering may\nnot be able to fairly benchmark the model robustness, as participants may top the leaderboard by\nproducing different errors from our surrogate models. We note that such concerns can be solved\ngiven systematic data curation. As shown in our main benchmark results, we observe we successfully\nselect the adversarial examples with high adversarial transferability that can unveil the vulnerabilities\nshared across models of different architectures. Speciﬁcally, we observe a huge performance gap in\nELECTRA (Large) that is pre-trained with different data and shown less robust than one of surrogate\nmodel RoBERTa (Large).\nFinally, we emphasize that our AdvGLUE benchmark mainly focuses on robustness evaluation. Thus\nAdvGLUE can also be considered as a supplementary diagnostic test set besides the standard GLUE\nbenchmark. We suggest that participants should evaluate their models against both GLUE benchmark\nand our AdvGLUE to understand both model generalization and robustness. We hope our work can\nhelp researchers to develop models with high generalization and adversarial robustness.\nA.9 Website\nWe present the diagnostic report on our website in Figure 4.\n20\nFigure 2: Human annotation instructions (training phase) for MNLI.\nB Data Sheet\nWe follow the documentation frameworks provided by Gebru et al. [14].\nB.1 Motivation\nFor what purpose was the dataset created? While recently a lot of methods (SMART, FreeLB,\nInfoBERT, ALUM) claim that they can improve the model robustness against adversarial attacks, the\nadversary setup in these methods (i) lacks a uniﬁed standard and is usually different across different\nmethods; (ii) fails to cover comprehensive linguistic transformation (typos, synonymous substitution,\nparaphrasing, etc) to recognize to which levels of adversarial attacks models are still vulnerable. This\nmotivates us to build a uniﬁed and principled robustness benchmark dataset and evaluate to which\nextent the state-of-the-art models have progressed so far in terms of adversarial robustness.\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,\ncompany, institution, organization)? University of Illinois at Urbana-Champaign (UIUC) and\nMicrosoft Corporation.\nB.2 Composition/collection process/preprocessing/cleaning/labeling and uses:\nThe answers are described in our paper as well as website https://adversarialglue.github.\nio.\n21\nFigure 3: Human annotation instructions (ﬁltering phase) for MNLI.\nB.3 Distribution\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution,\norganization) on behalf of which the dataset was created? The dev set is released to the public.\nThe test set is hidden and can only be evaluated by an automatic submission API hosted on CodaLab.\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)? The dev set\nis released on our website https://adversarialglue.github.io. The test set is hidden and\nhosted on CodaLab.\nWhen will the dataset be distributed? It has been released now.\nWill the dataset be distributed under a copyright or other intellectual property (IP) license,\nand/or under applicable terms of use (ToU)? Our dataset will be distributed under the CC BY-\nSA 4.0 license.\nB.4 Maintenance\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)? Boxin\nWang (boxinw2@illinois.edu) and Chejian Xu (xuchejian@zju.edu.cn) will be responsible\nfor maintenance.\n22\nFigure 4: An example of model diagnostic report for BERT (Large).\n23\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete in-\nstances)? Yes. If we include more tasks or ﬁnd any errors, we will correct the dataset and update\nthe leaderboard accordingly. It will be updated on our website.\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for\nthem to do so? They can contact us via email for the contribution.\n24",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.919615626335144
    },
    {
      "name": "Computer science",
      "score": 0.8498934507369995
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6754134297370911
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5998805165290833
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5792207717895508
    },
    {
      "name": "Machine learning",
      "score": 0.5431179404258728
    },
    {
      "name": "Language model",
      "score": 0.5179082751274109
    },
    {
      "name": "Natural language processing",
      "score": 0.4506082236766815
    },
    {
      "name": "Task (project management)",
      "score": 0.42692768573760986
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}