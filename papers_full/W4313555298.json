{
  "title": "Convolutional Transformer-Based Few-Shot Learning for Cross-Domain Hyperspectral Image Classification",
  "url": "https://openalex.org/W4313555298",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2135806602",
      "name": "Yishu Peng",
      "affiliations": [
        "Hunan Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2150906733",
      "name": "Yaru Liu",
      "affiliations": [
        "Hunan Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2105618510",
      "name": "Bing Tu",
      "affiliations": [
        "Hunan Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2103569835",
      "name": "Yuwen Zhang",
      "affiliations": [
        "Hunan Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2985299909",
    "https://openalex.org/W4288073030",
    "https://openalex.org/W4294068665",
    "https://openalex.org/W4312937836",
    "https://openalex.org/W4312906362",
    "https://openalex.org/W4213243561",
    "https://openalex.org/W2085269372",
    "https://openalex.org/W2973263445",
    "https://openalex.org/W2037051094",
    "https://openalex.org/W2898204262",
    "https://openalex.org/W3128776197",
    "https://openalex.org/W6797691975",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2887785636",
    "https://openalex.org/W3215149470",
    "https://openalex.org/W3138725786",
    "https://openalex.org/W3179652040",
    "https://openalex.org/W4205605267",
    "https://openalex.org/W3034096603",
    "https://openalex.org/W4224938958",
    "https://openalex.org/W6791058016",
    "https://openalex.org/W3012405452",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W2221243399",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2796108585",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W4225984886",
    "https://openalex.org/W3205033397",
    "https://openalex.org/W4226438786",
    "https://openalex.org/W3210969440",
    "https://openalex.org/W3119997721",
    "https://openalex.org/W2962679318",
    "https://openalex.org/W2989793328",
    "https://openalex.org/W2151456308",
    "https://openalex.org/W2125763679",
    "https://openalex.org/W2137900926",
    "https://openalex.org/W2986799142",
    "https://openalex.org/W3201461236",
    "https://openalex.org/W3174236562",
    "https://openalex.org/W3158900180",
    "https://openalex.org/W3191518629",
    "https://openalex.org/W3205614732",
    "https://openalex.org/W2948256530",
    "https://openalex.org/W3161216311",
    "https://openalex.org/W2964105864",
    "https://openalex.org/W2136251662",
    "https://openalex.org/W3131937156",
    "https://openalex.org/W2967363891",
    "https://openalex.org/W3034942609",
    "https://openalex.org/W4205362218",
    "https://openalex.org/W3161622534",
    "https://openalex.org/W3047669212",
    "https://openalex.org/W6742288159",
    "https://openalex.org/W6735236233",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3132867842",
    "https://openalex.org/W3104636952",
    "https://openalex.org/W2742093937",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3214821343",
    "https://openalex.org/W3133696297"
  ],
  "abstract": "In cross-domain hyperspectral image (HSI) classification, the labeled samples of the target domain are very limited, and it is a worthy attention to obtain sufficient class information from the source domain to categorize the target domain classes (both the same and new unseen classes). This article investigates this problem by employing few-shot learning (FSL) in a meta-learning paradigm. However, most existing cross-domain FSL methods extract statistical features based on convolutional neural networks (CNNs), which typically only consider the local spatial information among features, while ignoring the global information. To make up for these shortcomings, this article proposes novel convolutional transformer-based few-shot learning (CTFSL). Specifically, FSL is first performed in the classes of source and target domains simultaneously to build the consistent scenario. Then, a domain aligner is set up to map the source and target domains to the same dimensions. In addition, a convolutional transformer (CT) network is utilized to extract local-global features. Finally, a domain discriminator is executed subsequently that can not only reduce domain shift but also distinguish from which domain a feature originates. Experiments on three widely used hyperspectral image datasets indicate that the proposed CTFSL method is superior to the state-of-the-art cross-domain FSL methods and several typical HSI classification methods in terms of classification accuracy.",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023 1335\nConvolutional Transformer-Based Few-Shot Learning\nfor Cross-Domain Hyperspectral Image Classiﬁcation\nYishu Peng , Member, IEEE,Y a r uL i u, Student Member, IEEE,B i n gT u, Member, IEEE,\nand Yuwen Zhang, Student Member, IEEE\nAbstract—In cross-domain hyperspectral image (HSI) classiﬁ-\ncation, the labeled samples of the target domain are very limited,\nand it is a worthy attention to obtain sufﬁcient class information\nfrom the source domain to categorize the target domain classes\n(both the same and new unseen classes). This article investigates this\nproblem by employing few-shot learning (FSL) in a meta-learning\nparadigm. However, most existing cross-domain FSL methods ex-\ntract statistical features based on convolutional neural networks\n(CNNs), which typically only consider the local spatial information\namong features, while ignoring the global information. To make up\nfor these shortcomings, this article proposes novel convolutional\ntransformer-based few-shot learning (CTFSL). Speciﬁcally, FSL\nis ﬁrst performed in the classes of source and target domains\nsimultaneously to build the consistent scenario. Then, a domain\naligner is set up to map the source and target domains to the same\ndimensions. In addition, a convolutional transformer (CT) net-\nwork is utilized to extract local-global features. Finally, a domain\ndiscriminator is executed subsequently that can not only reduce\ndomain shift but also distinguish from which domain a feature\noriginates. Experiments on three widely used hyperspectral image\ndatasets indicate that the proposed CTFSL method is superior to\nthe state-of-the-art cross-domain FSL methods and several typical\nHSI classiﬁcation methods in terms of classiﬁcation accuracy.\nIndex Terms—Convolutional transformer (CT), cross-domain,\nfew-shot learning (FSL), hyperspectral image (HSI), scene\nconsistency.\nI. INTRODUCTION\nH\nYPERSPECTRAL images (HSIs) are 3-D data cubes with\n1-D spectral information in addition to the general 2-D\nManuscript received 4 November 2022; revised 21 December 2022; accepted\n31 December 2022. Date of publication 5 January 2023; date of current version\n20 January 2023. This work was supported in part by the National Natural\nScience Foundation of China under Grant 61977022 and Grant 62271200, in part\nby the Foundation of Department of Water Resources of Hunan Province under\nGrant XSKJ2021000-12, Grant XSKJ2021000-13, and Grant XSKJ2022068-\n48, in part by the Natural Science Foundation of Hunan Province under Grant\n2021JJ40226, in part by the Foundation of Education Bureau of Hunan Province\nunder Grant 21B0590, Grant 21B0595, and Grant 20B062, and in part by the\nScientiﬁc Research Fund of Education Department of Hunan Province under\nGrant 19A200.(Corresponding author: Bing Tu.)\nYishu Peng, Yaru Liu, and Yuwen Zhang are with the School of\nInformation Science and Engineering, Hunan Institute of Science and\nTechnology, Yueyang 414000, China (e-mail: lovepys@hnist.edu.cn; liu-\nyarua@foxmail.com; yuwen_zhang@vip.hnist.edu.cn).\nBing Tu is with the School of Information Science and Engineering, Hu-\nnan Institute of Science and Technology, Yueyang 414000, China, and also\nwith the Guangxi Key Laboratory of Cryptography and Information Security,\nGuilin University of Electronic Technology, Guilin 541000, China (e-mail:\ntubing@hnist.edu.cn).\nDigital Object Identiﬁer 10.1109/JSTARS.2023.3234302\nspatial image [1], [2], [3] that integrate the characteristics of\nimage and spectra. HSIs contain abundant spectral and spatial in-\nformation [4], [5], which have been applied in land-use and land-\ncover classiﬁcation and have gained increasing attention[6], [7],\n[8], [9]. In HSI classiﬁcation, it is sufﬁcient to labeled samples\nin the same scene such that a scene can be classiﬁed correctly.\nHowever, achieving labeling process is difﬁcult for a newly\ncollected HSI.\nCross-domain HSI classiﬁcation was proposed for resolving\nthe problem of difﬁcult classiﬁcation due to the scarcity of\nground-cover labels[10], [11], [12], [13].T h i sa i m st ou s et h e\nsimilarity of covering features between multiple HSIs to form\nclassiﬁcation and recognition criteria from an HSI with sufﬁcient\nlabeled pixels for model training and learning, which is called\nthe source domain or source scene. Then, the model is used to\nidentify and classify another HSI with similar scenes called the\ntarget domain or target scene that is seriously lacking in labeled\npixels or even without available labeled pixels.\nInevitably, difﬁculties and challenges in cross-scene HSI clas-\nsiﬁcation tasks followed. Restricted by factors such as sensor\ndifferences, imaging time, location, and atmospheric environ-\nment, the acquired HSI has heterogeneity[14], [15], [16], [17].\nTherefore, solving the distribution differences of the source and\ntarget domains is the key to cross-scene HSI classiﬁcation, which\nis the domain adaptation problem. In recent years, a series of\nHSI classiﬁcation approaches have been presented to achieve\ncross-scene learning tasks and solve domain adaptation prob-\nlems, which can be roughly deﬁned into two types: heterogeneity\nof feature distribution and heterogeneity of feature space.\nThe former refers to the HSIs collected by the same optical\nsensor under different angles, times, locations, etc., causing\nheterogeneity in the feature distribution between the same land\ncovers in different scenes, which is manifested by the same\nnumber of spectral bands but the spectral curves may differ in the\nsame class. The latter refers to the restriction of the parameters\nof the optical sensor, which leads to feature space heterogeneity\nbetween source and target domain HSIs; this manifests that not\nonly the spectral bands are different in number, but the spectral\ncurves of the identical class in different scenes would also be\nsigniﬁcantly different.\nTo address cross-scene classiﬁcation, from the heterogeneity\nof feature distribution-based perspective, some works are oper-\nated to explore the similarity between the source and target do-\nmains, thus, solving the spectral offset problem. Deng et al.[18]\nproposed a feature embedding model based on deep metric\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n1336 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nlearning, which applies the features learned from the source\nscene to the target scene with an unsupervised domain adap-\ntation technique. A maximum mean difference (MMD)-based\ngraph optimal transmission (GOT) was proposed to align the\ndistribution discrepancy of the source and target domains[19].\nAn unsupervised domain adaptation method was accomplished\nfor cross-scene HSI classiﬁcation by utilizing an integrated\nframework with spectral-spatial feature dense compaction[20].\nThe unsupervised domain adaptation method for feature learning\ndoes not demand labeled data in the target scene, but it requires\nhaving a small enough discrepancy between the source and target\nscenes. Although the heterogeneity of feature distribution-based\nmethods enable to decrease data migration between two do-\nmains, they usually require that the target categories are the same\nas the source and can not classify the new unseen categories.\nFrom the perspective of heterogeneity of feature space, Liu\net al.[21] introduced spectral shift mitigation to simultaneously\nminimize the amplitude shift between source and target domains\nas well as the spectral variation for the target scene. Despite\nthe great similarities in the data between the source and target\ndomains, the classes between the two scenes may differ and\nnew classes need to be considered. Recently, few-shot learning\n(FSL) [22], [23], [24] has been used to address the above\nproblem, the goal of which is to classify a target class data\ngiven just a small number of labeled samples from each class.\nLi et al.[25] proposed a deep cross-domain few-shot learning\n(DCFSL) method for cross-scene classiﬁcation of HSIs in the\ncase of less labeled data, which overcomes domain shift by\nlearning a domain-adaptive feature embedded space through\na 3-D-CNN-based deep residual network from two mapping\nlayers of the source and target sceneries that are used for ensuring\nthat the inputs to the embedded feature extractor share equal\ndimensions. In addition, DCFSL makes it possible to perform\ndomain distribution alignment by the domain discriminator.\nZhang et al.[26] developed a dual graph cross-domain few-shot\nlearning (DG-CFSL) method to mitigate the impact of domain\ntransitions. DG-CFSL designs intradomain distribution extrac-\ntion block (IDE-block) to carry out domain alignment using\nnonlocal spatial information which has powerful corresponding\nproperties.\nThe foregoing FSL approaches enable increased classiﬁcation\naccuracy with limited labels; they commonly extract features\nusing a convolutional neural network (CNN) that have obtained\nsigniﬁcant results for cross-scene HSI classiﬁcation. However, it\nis difﬁcult for CNN to capture the sequence attributes of spectral\nfeatures due to the limitations of its network backbone. In addi-\ntion, the receptive ﬁeld of CNN is limited which may easily cause\nthe missing information in the down-sampling layer, and it needs\nto expand the convolution kernel to expand the receptive ﬁeld,\nwhich causes dimensional disaster. A transformer network[27],\n[28] can be utilized to overcome the above issues because it can\ncapture the sequence attributes of spectral features. Meanwhile,\nvision transformer (ViT) [29] has been proposed to apply a\ntransformer in image classiﬁcation, Chen et al.[30] developed\na multistage vision transformer model to form pyramid fea-\nture extraction. Wu et al.[31] introduced spectrally enhanced\nand densely connected transformer model to capture local\ncontextual and semantic features. Feng et al.[32] developed\na novel spectral transformer with dynamic spatial sampling and\ngaussian positional embedding to take full advantage of the\nﬂexible nature of spatial sampling, to emphasize the importance\nof the central image element for HSI cube classiﬁcation, and to\nimprove the adaptability. Peng et al.[33] proposed a spatial–\nspectral transformer with cross-attention, which is composed\nof a dual-branch structures with spatial and spectral sequence.\nHowever, it tends to overlook some local information that may\nbe important for HSI classiﬁcation. To enhance information\nutilization and extract more discriminative features, we com-\nbine the CNN and transformer module and propose a convolu-\ntional transformer-based few-shot learning (CTFSL) structure\nfor cross-domain HSI classiﬁcation. Speciﬁcally, two FSLs are\nﬁrst executed simultaneously for the source and target domains.\nAfter mapping two domains’ bands to the same dimensions\nthrough the distribution aligner, a feature extractor based on\na convolutional transformer (CT) network is utilized to learn\nspectral-spatial features, which can both expand interclass dis-\ntances and reduce innerclass distances. Furthermore, a domain\ndiscriminator is employed to tackle the domain separability\nproblems that can not only classify the same target domain\nclasses as source domain classes but also classify new unseen\nclasses.\nThe major contributions presented in this article are grouped\nas follows.\n1) A CTFSL framework is proposed where a novel FSL\nmethod is developed to solve classes that are scarcely\nrepresented, and an FSL loss is deﬁned to avoid overﬁtting\nto underrepresented classes.\n2) The CT network is designed by composing the convo-\nlutional neural network and a vision transformer, which\nachieves more effective feature embedding and extracts\nboth local detail and global information for HSI patches.\n3) An adversarial loss is introduced using domain discrimi-\nnator based on FCN to match the prediction between two\ndomains and optimize the proposed network model for a\ncross-domain task.\n4) It can be observed that CTFSL can achieve better classi-\nﬁcation results than other cross-domain FSL methods on\npractical application.\nThe rest of this article is organized as follows. SectionII\nbrieﬂy describes some relevant concepts. SectionIII explicitly\nexplains the full details of the proposed CTFSL for cross-\nscene HSI classiﬁcation. SectionIV shows experimental results\nto demonstrate the superior performance of CTFSL. Finally,\nSection V concludes this article.\nII. RELATED WORK\nThis section introduces several relevant concepts to better\nexplain the proposed CTFSL.\nA. Domain Adaptation\nIn cross-scene HSI classiﬁcation, domain adaption aims to\ntransfer data knowledge from the source domain to the target\ndomain by mapping the data features of two domains into the\nPENG et al.: CONVOLUTIONAL TRANSFORMER-BASED FEW-SHOT LEARNING 1337\nsame feature space[34], [35]. Domain adaptation can solve the\ndistribution discrepancy between the source and target domains\nby learning domain-invariant features. Domain adaption may be\ndescribed in two forms: unsupervised domain adaptation[36],\n[20], [37] and supervised domain adaptation[38], [39], [40].\nIn domain adaptation, the source domain has rich learning\ninformation. Unsupervised domain adaptation refers to the target\ndomain without labeled samples, while supervised domain adap-\ntation means that the target domain has a few labeled samples.\nOur method leans toward supervised domain adaptation and\nproposes cross-scene few-shot domain adaptation.\nB. Cross-Scene Few-Shot Learning\nFSL is one type of meta-learning[41], [42] that processes\nimages given only a small number of labeled samples[43];\nFSL aims to construct a consistent scene of a source and target\ndomain based on an FSL method through meta-learning[44],\n[45], [46]. In cross-scene HSI classiﬁcation, FSL is usually\ndeﬁned as aK-way N-shot task [47] (i.e., N labeled samples\nof K unique classes) and N is very small, e.g., 1 or 5[48].\nFirst, two HSI datasets are given: the source datasetXs ∈ RSD\nand the target datasetXt ∈ RTD , whereXt contains two parts\nDf with labeled few-shot data andDt with unlabeled test data,\ni.e., Xt = Df ∪Dt. Then, the numbers of categories in the\nsource and target domains are marked withCs and Ct separately.\nGenerally, to guarantee diversity in the training samples, we set\nCs >C t, which is beneﬁcial for meta-learning[49], [50].\nIn our method, we take the source dataXs ∈ RSD and the\ntarget labeled few-shot dataDf as the training set for feature\nextraction and the target unlabeled dataDt as the test set for\nmodel evaluation. The FSL model operates on the task-based\nlearning tactic in both the source and target domains, where each\ntask is one single iteration of training. During every iteration,\ntaking the FSL on the source datasetXs as instance. A support\nset is ﬁrst formed with C classes and K samples per class\nare randomly selected from Xs. Therefore, the support set\nis expressed asS = {(xi,yi)}C×K\ni=1 . Analogously, a query set\nQ = {(xj,yj)}C×N\nj=1 consists of N samples randomly selected\nfrom the identicalC classes that are unique from the elements\nof the support set. It is note worthy that the sample labels of\nthe query set are considered as unknown. In experiments, we\nusually setK signiﬁcantly smaller thanN which can simulate\nthe practical few-shot classiﬁcation scenarios. Summarizing, a\nC-way K-shot N-query FSL work is formed for the source\ndataset. The target FSL is similar to the source data.\nC. Vision Transformer\nAfter the publication of the vision transformer (ViT)[29],\nit has been broadly used in various tasks of computer vision\ndue to its excellent performance such as HSI classiﬁcation[51],\n[52], [53], [30]. ViT is derived from the structure of the original\ntransformer [54], [55], [56] and is easy to transplant into different\ntasks. The original transformer, which is a typical encoder–\ndecoder model, is proposed for natural language processing.\nTherefore, the transformer consists of two parts: the encoding\nFig. 1. Structure of vision transformer encoder.\nFig. 2. Model of the multihead attention.\nand decoding components. The encoding component is com-\nposed of multiple encoder layers, each of which is made up\nof two sublayers: self-attention and feed-forward network[57].\nLikewise, the decoding component also consists of a stack of\ndecoder layers, but decoder inserts a third sublayer per layer,\nencoder–decoder attention, in addition the two sublayers of\nthe encoder. The transformer is entirely based on self-attention\nmechanisms, which can realize input parameter sharing by the\nglobal contextual information.\nInspired by the tremendous achievements of the original\ntransformers, ViT is an extension in the ﬁeld of image classiﬁ-\ncation. The original transformer only accepted sequential inputs\n(i.e., the input of the original transformer is 1-D embeddings).\nTherefore, the input image in ViT is ﬁrst divided into a series of\nnonoverlapped ﬁxed-size patches (i.e., 2-D patches) that are then\nprojected into patch embeddings (i.e., ﬂatten the 2-D patches\ninto a 1-D image sequence). Finally, send the patch embeddings\nof the image into the transformer to extract features. Fig.1\nillustrates the Encoder structure of ViT and Fig.2 illustrates\nthe multihead self-attention model. “Q, K and V” in Fig.2 are\nthe new sequence vectors generated by linear projection, and the\n1338 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 3. Schematic of the proposed CTFSL classiﬁcation method, including few-shot learning, distribution aligner, feature extractor, and domain discriminator.\nself-attention can be calculated as follows:\nAttention (Q,K,V )= softmax\n(QKT\n√dk\n)\nV (1)\nwhere dk is the dimension ofK. The attention weights obtained\nfrom the dot product ofQand K are responsible for calculating\nthe attention scores between each pair of different vectors that\ndetermine the level of attention given to the other data when\nencoding the data at the current location.√dk and Softmax\nnormalize the attention scores to enhance the gradient stability to\nimprove the training, and subsequently convert the scores into\nprobabilities. Finally, according to the probability magnitude,\neach value vector is multiplied with the sum of the probabilities\nto assign attention weights to it and produce the ﬁnal output\nvector.\nIII. METHOD\nThis section introduces the convolutional transformer-based\nfew-shot learning (CTFSL) network for cross-scene HSI classi-\nﬁcation. Fig. 3 displays the structure diagram of the suggested\nCTFSL, which contains four parts: few-shot learning (FSL),\ndistribution aligner, feature extractor, and domain discriminator.\nSpeciﬁcally, executing FSL in both the source and target cate-\ngories concurrently. Then, a distribution aligner is used before\nthe feature extractor to map the source and target domains into an\nidentical dimensions. Next, the feature extractor maps features\nfrom two domains into a scene-consistency metric space. The\ndomain discriminator predicts the domain to which a feature\nbelongs and achieves the distinguishability of the two domain\nclasses.\nA. Few-Shot Learning\nGiven the source domain dataXs ∈ RSD having Cs classes\nand the target domain dataXt ∈ RTD having Ct classes sep-\narately, the proposed CTFSL network has two FSL tasks: the\nsource FSL taskSfsl and target FSL taskTfsl. Two kinds of\nFSL are executed in the classes with both the source and target\ndomains simultaneously by episodes, enabling scene consis-\ntency between the source and target domain data and building\ncross-scene classiﬁcation model.\n1) Source FSL: In the source FSL Sfsl task, selecting C\nclasses from the source classesCs to form an episode. In the\nsource episode, source dataXs is divided into a support set\nSs = {(xs\ni,ys\ni )}C×K\ni=1 and a query set Qs = {(xs\nj,ys\nj))}C×N\nj=1 .\nSpeciﬁcally, C categories are randomly selected fromXs, with\nKsamples from each category, forming a support set. Moreover,\na query set is formed by randomly selectingN samples from the\nsame C classes that are distinct to those in the support set. After\nthat, the distribution aligner is ﬁrst applied for dimensionality\nreduction of all samples in the support and query sets, after\nwhich the embedding characteristics are obtained by the feature\nextractor. FSL is executed by comparing the similarity of the\nembedded features between the query and support sets per\ncategory. The class prototype for a support samplexs\ni in the\nsupport setSs is\nck = 1\n|Sks|\n∑\n(xs\ni ,ys\ni )∈Sks\nfϕ(xs\ni) (2)\nwhere Sk\ns is the set belonging to class k in the support set,|Sk\ns|\nis the number of samples inSk\ns, xs\ni denotes a support set sample\nfor which the label isys\ni , andfϕ indicates the feature extractor\nwith argument ϕ. A query samplexs\nj in Qs has the category\ndistributivity computed by the Bregman divergences (i.e., the\nEuclidean distance) based on a softmax function\nPϕ(ys\nj = k|xs\nj ∈ Qs)= exp(−ED( fϕ(xs\nj),ck))\n∑ C\nk=1 exp(−ED( fϕ(xs\nj),ck))\n(3)\nwhere xs\nj represents a support set sample for which the label is\nys\nj, ED(•) denotes a Euclidean distance function,C denotes the\nPENG et al.: CONVOLUTIONAL TRANSFORMER-BASED FEW-SHOT LEARNING 1339\namount of distinct categories per episode. The source FSL loss\nof xs\nj ∈ Qs is calculated into the negative log-probability of its\ncorresponding truth category by cross-entropy loss\nLs\nfsl = −log Pϕ(ys\nj = k|xs\nj ∈ Qs)= ED( fϕ(xs\nj),ck)\n+l o g\nC∑\nk=1\nexp(−ED( fϕ(xs\nj),ck)). (4)\n2) Target FSL: Similar to the source FSL task,C classes\nare selected from target classesCt to form an episode in the\ntarget FSL. In the target episode, target dataXt is similarly\ndivided into a support setSt = {(xt\ni,yt\ni)}C×K\ni=1 and a query set\nQt = {(xt\nj,yt\nj)}C×N\nj=1 . Notice the support set samples are se-\nlected from labeled dataDf with only a few samples. Therefore,\nthe class prototype for a support samplext\ni in the support set\nSt is\nck = 1\nSt\n∑\n(xt\ni,yt\ni)∈St\nfϕ(xt\ni). (5)\nThe class predicted probability for a query samplext\nj in Qt\nexpressed as\nPϕ(yt\nj = k|xt\nj ∈ Qt)= exp(−ED( fϕ(xt\nj),ck))\n∑ C\nk=1 exp(−ED( fϕ(xt\nj),ck))\n.\n(6)\nThe target FSL loss ofxt\nj ∈ Qt is given by\nLt\nfsl = −log Pϕ(yt\nj = k|xt\nj ∈ Qt)= ED( fϕ(xt\nj),ck)\n+l o g\nC∑\nk=1\nexp(−ED( fϕ(xt\nj),ck)). (7)\nB. Distribution Aligner\nThe heterogeneity of feature distribution between the source\nand target domains resulted in inconsistent spectral resolutions\nof the samples. Thus, a distribution aligner is employed for\nmapping the source (the Chikusei dataset with 128 bands) and\ntarget domains (e.g., the Indian Pines dataset with 200 bands) to\nthe same dimensiond. The distribution aligner is implemented\nvia 2-D CNN. First, we ensure the rationality of the selected\nband by selecting9 ×9 neighborhoods to be the input spatial\ndimensions. Thus, assuming thatI ∈ R9×9×b is the input of the\nHSI cube whereb means the bands amount, the result obtained\nfrom the distribution aligner as\nIA = I ×A (8)\nwhere IA ∈ R9×9×100 is the aligned dataset, andA ∈ Rb×100 is\nthe function of the distribution aligner.b×100 denotes learnable\nparameters in the alignment. There are128 ×100 parameters for\nXs, and200 ×100 parameters forXt.\nC. Feature Extractor\nThe feature extractor works for extracting the spatial-spectral\nembedding features and mapping them to a scene-consistency\nmetric space. The feature extractor is based on a convolutional\nFig. 4. CT module of the feature extractor.\ntransformer (CT) network, which effectively combines the con-\nvolutional neural network (CNN) with a vision transformer\n(ViT) structure and can extract both the local and global features\nfor using the spatial and spectral information sufﬁciently. The\nfeature extractor mainly consists of two subblocks, Fig.3 shows\nthe architecture of the feature extractor (see the feature extractor\nmodule). Speciﬁcally, Fig.4 shows the CT module of the feature\nextractor.\nThe input to the feature extractor is the outputIA ∈ R9×9×100\nfrom the distribution aligner. In our method, the input patchIA\nis fed into the CT module which consisted of a CNN block and\na ViT block. The CNN block extracts local featuresfc from IA\nand the ViT block is utilized to extract global featuresfv. Then,\nwe combined the local and global features to form the feature\nrepresentation f of the feature extractor\nf = concat(fc,fv). (9)\nD. Domain Discriminator\nTo reduce domain shift as inspired by[40], a domain discrim-\ninator is explored with adversarial loss to predict the domain to\nwhich a feature belongs. The domain discriminator is built on a\nfully convolutional network (FCN) that contains a convolutional\nlayer with a5 ×5 kernel as a ﬁlter, a convolutional layer with\na 1 ×1 kernel, a residual block, followed by a ﬁnal convolu-\ntional layer with a1 ×1 kernel. Except for the last layer, each\nconvolutional layer is followed by a batch normalization (BN)\nand a rectiﬁed linear unit (ReLU) nonlinear activation function.\n1340 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 3 shows the architecture of the domain discriminator (see\nthe domain discriminator module).\nOur goal is classifying whether the features come from the\nsource or target domain. On the domain discriminator, we deﬁne\nan adversarial loss functionLD to resolve the imbalance among\nclasses while the lossLD should be minimized\nLD = −\n∑\ni∈IA\nlog D(fθ (xs\ni)) + log\n(\n1 −D\n(\nfθ\n(\nxt\ni\n)))\n(10)\nwhere D(·) and 1 −D(·) are the probabilities of a samplei\nbelonging to the source and target domains predicted by the\ndomain discriminator, respectively.fθ denotes the features from\nthe feature extractor with parameterθ, xs\ni and xt\ni are samples\nfrom the source and target domains (i.e.,xs\ni ∈ Xs, xt\ni ∈ Xt),\nrespectively.\nThus, the source domain’s total loss function as\nLs = Ls\nfsl + LD. (11)\nLikewise, the target domain’s total loss function as\nLt = Lt\nfsl + LD. (12)\nFinally, the nearest neighbor (NN) method is utilized to clas-\nsify unlabeled samples in the target domain during the testing\nphase and then generate their classiﬁcation maps to evaluate the\neffectiveness of CTFSL.\nIV . EXPERIMENTAL RESULTS\nThe experiments are performed using software platform Py-\ncharm on a 12th Gen Intel Core TM i9-12900KF processor\nequipped with NVidia GeForceTM RTX 3090 Ti and 64 GB of\nRAM, and all codes executed on Python 3.7.\nA. Experimental Data\nThe proposed CTFSL approach for cross-domain HSI classiﬁ-\ncation is performed employing four public HSI datasets, namely,\nthe Chikusei, Indian Pines, University of Pavia, and Salinas\ndatasets.\n1) Source Domain: The source domain dataset utilizes the\nChikusei dataset. The Chikusei dataset was gathered over\nagricultural and urban areas in Chikusei, Ibaraki, Japan by\na Headwall Hyperspec-VNIR-C imaging sensor, on July 29,\n2014 [58]. It comprises 128 spectral bands with a spectrum of\n363–1018 nm, comprises2517 ×2335 pixels in which each has\na spatial resolution of 2.5 m and comprises 19 unique land-\ncover categories. Fig.5(a)–(c) presents the false-color image,\nthe matching ground-truth map and the matching color card\nof the Chikusei. The classes of the Chikusei dataset and the\ncorresponding sample numbers are shown in TableI.\n2) Target Domain: The Indian Pines, University of Pavia,\nand Salinas datasets are applied as target domains. The Indian\nPines dataset was acquired over the agricultural Indian Pine\ntest site in North-western Indiana by an A VIRIS sensor in\nJune 1992. It comprises 200 spectral bands with a spectrum of\n400–2500 nm, comprises145 ×145 pixels in which each has a\nspatial resolution of 20 m and it comprises 16 unique land-cover\ncategories. Fig. 6(a)–(c) presents the false-color image, the\nFig. 5. Chikusei dataset. (a) False-color image. (b) Groundtruth map. (c) Color\ncoding.\nTABLE I\nCLASS,N AME, AND NUMBER OF SAMPLES ON CHIKUSEI DATASET\nmatching ground-truth map and the matching color card of the\nIndian Pines. The classes of the Indian Pines dataset and the\ncorresponding sample numbers are shown in TableII.\nThe University of Pavia dataset was acquired over Pavia,\nNothern Italy utilizing the ROSIS sensor in a ﬂight campaign. It\ncomprises 103 spectral bands with a spectrum of 430–860 nm,\ncomprises 610 ×340 pixels in which each has a spatial res-\nolution of 1.3 m and it comprises nine unique land-cover cate-\ngories. Fig.7(a)–(c) presents the false-color image, the matching\nground-truth map and the matching color card of the University\nof Pavia. The classes of the University of Pavia dataset and the\ncorresponding sample numbers are shown in TableIII.\nThe Salinas dataset was gathered over Salinas Valley, Cal-\nifornia using A VIRIS sensor. It comprises 204 spectral bands\nPENG et al.: CONVOLUTIONAL TRANSFORMER-BASED FEW-SHOT LEARNING 1341\nFig. 6. Indian Pines dataset. (a) False-color image. (b) Groundtruth map.\n(c) Color coding.\nTABLE II\nCLASS,N AME, AND NUMBER OF SAMPLES ON INDIAN PINES DATASET\nwith a spectrum of 400–2500 nm, comprises512 ×217 pixels\nin which each has a spatial resolution of 3.7 m and it comprises\n16 unique land-cover categories. Fig.8 presents the false-color\ngraph, matching ground-truth map, and matching color card of\nSalinas. The classes of the Salinas dataset and the corresponding\nsample numbers are shown in TableIV.\nB. Experimental Setup\nThe input of the proposed CTFSL is chosen from set with\npatch size {5 ×5, 7 ×7, 9 ×9, 11 ×11, 13 ×13, 15 ×15}.\nFrom Fig.9, for all experiments on different target domains, we\nobserve that with increasing input patch size, the classiﬁcation\naccuracy also increases, but it will decrease after increasing\nbeyond a certain extent and it approximately obeys the Gaussian\ndistribution. Therefore, taking this into account, our method sets\nFig. 7. University of Pavia dataset. (a) False-color image. (b) Groundtruth\nmap. (c) Color coding.\nTABLE III\nCLASS,N AME, AND NUMBER OF SAMPLES ON UNIVERSITY OF PAV I ADATASET\nTABLE IV\nCLASS,N AME, AND NUMBER OF SAMPLES ON SALINAS DATASET\n1342 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 8. Salinas dataset. (a) False-color image. (b) Groundtruth map. (c) Color\ncoding.\nthe input patch size to9 ×9. The CTFSL method is trained via\nan Adaptive Moment Estimation (Adam) optimizer. The training\niterations are setup as 10 000 and the learning rate as 1e-3. In\nthe episodic training phase, each episode represents a C-way\nK-shot mission. C indicates the count of categories and sets it as\nthe class number in the target domain (i.e., setting the University\nof Pavia dataset as 9, the Indian Pines and Salinas datasets as\n16). K indicates samples number per class within support set\nS and is always set to one regardless of source or target FSL.\nIn addition, samples number per class within query setQ is\nNQ, and NQ is setup to 19 to evaluate the learned classiﬁer.\nFurthermore, 200 labeled samples selected arbitrarily from each\ncategory of the source domain to acquire transferred knowledge.\nFinally, classiﬁcation was based on a K-nearest neighbor (KNN)\nclassiﬁer, and the number of nearest neighbors is set to 1.\nTo verify the validity of the proposed CTFSL, we com-\npared our method to some classical and cutting edge cross-\nscene methods used for HSI classiﬁcation, containing typical\nKNN [59], which is one of the simplest classiﬁcation methods,\na common kernel-learning method SVM [9], 3-D-CNN [2],\nDFSL+NN [60], DFSL+SVM[60], and DCFSL[25].\nTo guarantee the equity of the above-mentioned approaches,\nﬁve labeled samples per target domain category were ﬁrst chosen\nfor training within all control experiments. Then, adding random\nGaussian noise to augment the data. The remaining entries in the\ntarget domain are regarded as the testing data. In addition, for\ncross-domain methods, learning portable information by 200\nlabeled samples selected randomly from each source domain\nclass (DFSL+SVM, DFSL+NN, and DCFSL).\nTo assess the classiﬁcation effects of different approaches\nobjectively, we adopted three widely used quality indicators,\nthe overall accuracy (OA), the average accuracy (AA), and the\nkappa coefﬁcient. The training samples chosen at random for all\nexperiments, for which reason ten repetitions were performed\nfor eliminating the inﬂuences, and thus obtaining the means and\nstandard deviations of OA, AA, and Kappa. In addition, the\nvalues reported for each metric were computed by taking the\naverage of the outcomes derived from ten repetitive experiments\nwith arbitrarily chosen training samples.\nC. Comparison of Different Methods\nComparing the proposed CTFSL method with three typical\nclassiﬁcation methods (KNN, SVM, and 3-D-CNN), and three\nFSL classiﬁcation approaches (DFSL+SVM, DFSL+NN, and\nDCFSL) show our method’s advantages and efﬁciency. For the\nsupervised methods (KNN, SVM, and 3-D-CNN), training the\nclassiﬁer can only choose a few-shot data from the target domain.\nThe reason why source domain samples cannot be used as a\ntraining set in these methods is that they demand the same\ntraining as the test categories. In particular, KNN calculates the\nEuclidean distances between test and training samples of distinct\ncategories, and obtains the class to which the test sample belongs\nby comparing the average of the smallest Euclidean distances.\nIt is noteworthy that the number of nearest neighbors is set to 1.\nSVM learns nonlinear support vector machine by kernel method\nto map nonlinear data into a linearly separable space, but the\nstandard SVM method ignores the spatial information, focusing\nonly on spectral information in HSI. The 3-D-CNN method en-\nables effective extraction of deep spectral-spatial characteristics\nthat contribute to the accurate classiﬁcation of HSI.\nNevertheless, in the case of the FSL approaches\n(DFSL+SVM, DFSL+NN, and DCFSL), the samples in the\nsource domain can be utilized to learn transferable knowledge\nsince the classes may differ between the source and target\ndomains. Concretely, learning metric space in DFSL+SVM\nand DFSL+NN methods to extract spectral-special features\nvia a deep residual 3D CNN, and then, such metric space\ncould be used in few-shot classiﬁcation with a SVM or NN\nclassiﬁer. The DCFSL model is based on the DFSL+NN and\nDCFSL, which construct a uniﬁed structure to address FSL\nand domain adaptation problems. With the suggested CTFSL\nscheme, the aforementioned default arguments are applied for\nall experiments.\nTo conﬁrm the effectiveness of suggested CTFSL, experi-\nments on three datasets are compared to the foregoing compar-\nison approaches. The ﬁrst executed on the Indian Pines dataset.\nComparing the different methods’ performances, 5 labeled items\nper category were randomly sampled from the Indian Pines\ndataset. To objectively evaluate the performances of the different\nmethods, 10 classiﬁcation experiments were repeated for elimi-\nnating the inﬂuence from from stochastic sampling. The classi-\nﬁcation performance of all methods was assessed employing the\nmean and standard variance of the OA, AA, and Kappa coefﬁ-\ncients. Speciﬁcally, the optimal values for each class are bolded\nto highlight, and the values in parentheses refer to the standard\nPENG et al.: CONVOLUTIONAL TRANSFORMER-BASED FEW-SHOT LEARNING 1343\nFig. 9. Inﬂuence of the input patch size on the performance of the proposed CTFSL method on different datasets. (a) Indian Pines dataset. (b) Universityo fP a v i a\ndataset. (c) Salinas dataset.\nTABLE V\nCROSS-SCENE CLASSIFICATION PERFORMANCE [%] OF DIFFERENT METHODS ON INDIAN PINES WITH FIVE LABELED SAMPLES PER CLASS\ndeviation of the precisions achieved from ten experimentations.\nTable V shows the classiﬁcation accuracy of every category for\nIndian Pines under different methods. The cross-domain FSL\napproaches (DFSL+SVM, DFSL+NN, DCFSL, and CTFSL)\nare clearly superior to those traditional classiﬁcation approaches\n(KNN, SVM, and 3-D-CNN) in the case of limited methods. In\nparticular, the proposed CTFSL’s OA, AA, and Kappa value\nare at least 4.13, 2.48, and 4.44 percentage points higher than\nthe comparison method, respectively, which indicates that the\nCTFSL method is generally feasible. To visually demonstrate the\nproposed CTFSL’s effectiveness, Fig.10 shows a corresponding\nclassiﬁcation map of all the aforementioned methods. As shown\nin the ﬁgure, the proposed CTFSL can see some noise, but in\ncontrast, it shows a classiﬁcation map still with the smoothest\nspatial distribution and it has the best precision with less misla-\nbeling, which are the concordant outcomes with TableV.\nThe second among them conducted on the University of Pavia\ndataset. TableVI displays the OA, AA, and Kappa coefﬁcient,\nand the detailed classiﬁcation accuracies of each class on the\nUniversity of Pavia with various classiﬁcation approaches. As\nTable VI shows, the KNN, SVM, and 3-D-CNN classiﬁcation\nmethods only consider limited target domain samples to develop\nthe training data, so OA values are only 60.48%, 65.08%, and\n69.87%. By contrast, the OAs of the cross-domain FSL-based\nclassiﬁcation approaches (DFSL+SVM, DFSL+NN, DCFSL,\nand CTFSL) are usually greater than 78%, because they can\nmake full use of the source domain information and the tar-\nget few-shot labeled information. In addition, comparing the\nDCFSL approach, the OA with our suggested approach in-\ncreased from 83.83% to 85.03%, which proves this method’s\neffectiveness. As an instance, the classiﬁcation precision has\nimproved from 74.46% to 80.24% for Class 6 and from 56.62%\nto 90.75% for Class 8 by comparison to DCFSL. The AA and\nKappa of CTFSL are also the highest among all the compared\nclassiﬁcation methods. Fig. 11 shows the classiﬁcation result\nmaps under different methods. In particular, the classiﬁcation\ngraph of CTFSL clearly demonstrates its classiﬁcation advan-\ntages compared to other methods.\n1344 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 10. Classiﬁcation maps obtained by different classiﬁcation methods on the Indian Pines image dataset with ﬁve labeled samples per class. (a) Reference\nmap. (b) KNN. (c) SVM. (d) 3-D-CNN. (e) DFSL+NN. (f) DFSL+SVM. (g) DCFSL. (h) CTFSL.\nTABLE VI\nCROSS-SCENE CLASSIFICATION PERFORMANCE [%] OF DIFFERENT METHODS ON UNIVERSITY OF PAV I AWITH FIVE LABELED SAMPLES PER CLASS\nThe third carried out on the Salinas dataset had the analogous\nﬁndings. Table VII shows the classiﬁcation accuracy values\nyielded by the compared approaches and the suggested CTFSL.\nAs an instance, compared to KNN, the classiﬁcation precision\nhas improved from 75.72% to 98.08% for Class 3, that of\nClass 8 has increased from 48.43% to 83.26%, and that of\nClass 15 has increased from 61.03% to 80.78%. Fig.12 visu-\nally represents the proposed CTFSL’s effectiveness by show-\ning the corresponding classiﬁcation maps yielded by all the\naforementioned methods with the OAs. Apparently, the sug-\ngested CTFSL yields a classiﬁcation map with the smoothest\nspatial distribution and it has the best precision with less mis-\nlabeling from Fig.12, which are the concordant outcomes with\nTable VII.\nTo illustrate the proposed CTFSL’s computational complex-\nity effectively, TableVIII shows the computational efﬁciency\n(including training and testing times) of the above methods\nin different target domains. For three typical classiﬁcation\nmethods (KNN, SVM, and 3D-CNN) without a cross-domain,\ntheir training times are shorter than that of the other cross-\ndomain FSL classiﬁcation methods (DFSL+NN, DFSL+SVM,\nDCFSL, and CTFSL). The table shows that although our method\ntakes a long time to train, it has the highest accuracy.\nD. Parameter Analysis\nTo analyze the performance of the nearest neighbor size,\nthe algorithm comparison experiments under different nearest\nneighbor size are carried out to analyze the sensitivity of the\nCTFSL algorithm on three target domain datasets. We set 1, 2, 3,\n4, 5 as the size of the nearest neighbors to conduct 10 iterations of\nthe experiment and obtain the average of the results to compare\nthe performance, TableIX shows the classiﬁcation accuracy for\nthree datasets under different nearest neighbor size, where the\nbest results are bolded to highlight. As can be seen from the\nresults in the TableIX, the nearest neighbor size set to 1, 2, and\n4 on the Indian Pines, University of Pavia, and Salinas datasets\nexhibit optimal classiﬁcation performance, respectively.\nPENG et al.: CONVOLUTIONAL TRANSFORMER-BASED FEW-SHOT LEARNING 1345\nFig. 11. Classiﬁcation maps obtained by different classiﬁcation methods on the University of Pavia image dataset with ﬁve labeled samples per class.(a) Reference\nmap. (b) KNN. (c) SVM. (d) 3-D-CNN. (e) DFSL+NN. (f) DFSL+SVM. (g) DCFSL. (h) CTFSL.\nTABLE VII\nCROSS-SCENE CLASSIFICATION PERFORMANCE [%] OF DIFFERENT METHODS ON SALINAS WITH FIVE LABELED SAMPLES PER CLASS\n1346 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 12. Classiﬁcation maps obtained by different classiﬁcation methods on the Salinas image dataset with ﬁve labeled samples per class. (a) Reference map.\n(b) KNN. (c) SVM. (d) 3-D-CNN. (e) DFSL+NN. (f) DFSL+SVM. (g) DCFSL. (h) CTFSL.\nTABLE VIII\nCOMPUTATIONALEFFICIENCY (IN SECONDS) OF THE DIFFERENT METHODS IN THREE TARGET DOMAINS\nTABLE IX\nCLASSIFICATION PERFORMANCE OF THEPROPOSED CTFSL METHOD WITH DIFFERENT NEAREST NEIGHBOR SIZE ON THETHREE HYPERSPECTRAL IMAGE\nDATASETS\nIn the comparison experiments, we set the number of nearest\nneighbors to 1, which is not optimal for University of Pavia\nand Salinas, but still shows better performance than the other\nmethods. Although it is best for Indian pines when the number\nof nearest neighbors is set to 1, it can be seen from TablesV\nand IX that there are still better classiﬁcation results than other\nalgorithms when the nearest neighbor size takes other values.\nThis further proves the superiority of our method.\nTo investigate the effect of the labeled sample size on the\nCTFSL method performance, 1, 2, 3, 4, and 5 labeled sam-\nples were also randomly selected from for each class of the\ntarget domains to build few-shot data respectively. Then the\nclassiﬁcation experiments with different numbers of labeled\nsamples were performed ten repetitions, and the classiﬁcation\naccuracies for each number of labeled samples with previously\nmentioned methods under the Indian Pines, University of Pavia,\nand Salinas datasets are shown in TablesX–XII, where the best\nresults are bolded to highlight. To illustrate this visually, Fig.13\nshows the classiﬁcation accuracy curves of different labeled\nsample numbers on three target domain datasets. As shown in\nFig. 13, the OAs of the classiﬁcation results obtained by all the\nmethods are closely related to the change in labeled values, the\nincreased number of labeled samples, the higher classiﬁcation\naccuracy, and using ﬁve labeled samples per class exhibits the\nbest performance. In particular, the CTFSL method is superior\nto the other methods mentioned with the same labeled samples,\nwhich shows the superior stability of the CTFSL method.\nE. Analysis of Practical Applications\nTo verify the effectiveness and superiority of CTFSL method\nin practical application scenarios, we conduct an experimental\nPENG et al.: CONVOLUTIONAL TRANSFORMER-BASED FEW-SHOT LEARNING 1347\nFig. 13. Inﬂuence of the number of labeled samples per class on the performance of the proposed CTFSL method on different datasets. (a) Indian Pines dataset.\n(b) University of Pavia dataset. (c) Salinas dataset.\nTABLE X\nCLASSIFICATION PERFORMANCE [%] OF DIFFERENT NUMBER OF LABELED\nSAMPLES PER CLASS ON INDIAN PINES (N IS THE NUMBER OF LABELED\nSAMPLES PER CLASS)\nTABLE XI\nCLASSIFICATION PERFORMANCE [%] OF DIFFERENT NUMBER OF LABELED\nSAMPLES PER CLASS ON UNIVERSITY OF PAV I A(N IS THE NUMBER OF\nLABELED SAMPLES PER CLASS)\nanalysis of HSI data for a scenario in the Dongting Lake\nBasin. The Dongting Lake Basin dataset was gathered by\nHyper-Spectral Observation Satellite GaoFen (GF)-5 Advanced\nHyperSpectral Imager (AHSI) on December 8, 2019, it consists\nof 2008 ×2083 pixels with a spatial resolution of 30 m and\n330 spectral bands in the wavelength range 400–2500 nm.\nGF-5 is the world’s ﬁrst hyperspectral satellite covering the\nfull spectral range and enables comprehensive observation\nof the land and atmosphere. By processing GF-5 data from\nthe Dongting Lake Basin, a scene with452 ×380 pixels and\nTABLE XII\nCLASSIFICATION PERFORMANCE [%] OF DIFFERENT NUMBER OF LABELED\nSAMPLES PER CLASS ON SALINAS (N IS THE NUMBER OF LABELED SAMPLES\nPER CLASS)\nFig. 14. Application scenario dataset. (a) False-color image. (b) Groundtruth\nmap. (c) Color coding.\n305 effective spectral bands was selected as the experimental\ndataset. The scene contains six different land-cover classes and\n16 584 ground-truth labels, Fig.14(a)–(c) shows the false-color\nimage of the scene and the corresponding ground-truth map\nand the corresponding color code.\nIn the validation experiment, ﬁve labeled samples of each\nclass are randomly selected for training in CTFSL and various\ncomparison methods, and the rest are regarded as the testing\ndata. The number of nearest neighbors is set to 1. The results\nshow that the CTFSL method has more performance advantages\nwith the highest classiﬁcation accuracy that the OA value is\n90.43%, compared to other methods. Fig.15 shows the classiﬁ-\ncation result maps and the corresponding average of OA values\namong ten repetitive experiments under different methods are\n1348 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 15. Classiﬁcation maps obtained by different classiﬁcation methods on\nthe practical HSI dataset with ﬁve labeled samples per class. (a) Reference map.\n(b) KNN (OA = 78.84%). (c) SVM (OA= 79.73%). (d) 3D-CNN (OA=\n82.61%). (e) DFSL+NN (OA= 89.30%). (f) DFSL+SVM (OA= 89.09%).\n(g) DCFSL (OA= 88.54%). (h) CTFSL (OA= 90.43%).\nin parentheses. As shown in the ﬁgure, the visualization map of\nCTFSL can see some noise, but in contrast, it still shows the most\naccurate and spatially smoothest classiﬁcation map of classes\nwith fewer mislabeled pixels, compared to other methods.\nV. CONCLUSION\nThis article proposes convolutional transformer-based few-\nshot learning method for cross-domain hyperspectral image clas-\nsiﬁcation. The method includes three main parts: 1) distribution\naligner based on few-shot learning to achieve the dimensionality\nreduction; 2) feature extractor based on convolutional trans-\nformer network to obtain the local-global features; 3) domain\ndiscriminator based on fully convolutional network to reduce\nthe domain shift. Experiments have been performed on three\ndifferent real hyperspectral images, and the results show that\nthe proposed CTFSL outperformers the existing state-of-the-art\nFSL methods in cross-domain HSI classiﬁcation, thus verifying\nits effectiveness. However, the good performance of the pro-\nposed CTFSL method relies on a relatively large computational\ncost. Further developments of this work should further improve\nits performance while reducing the computation time.\nREFERENCES\n[1] N. He et al., “Feature extraction with multiscale covariance maps for\nhyperspectral image classiﬁcation,”IEEE Trans. Geosci. Remote Sens.,\nvol. 57, no. 2, pp. 755–769, Feb. 2019.\n[2] Y . Li, H. Zhang, and Q. Shen, “Spectral–spatial classiﬁcation of hyper-\nspectral imagery with 3D convolutional neural network,”Remote Sens.,\nvol. 9, no. 1, 2017, Art. no. 67.\n[3] S. Jia et al., “Gradient feature-oriented 3-D domain adaptation for hyper-\nspectral image classiﬁcation,”IEEE Trans. Geosci. Remote Sens., vol. 60,\nApr. 2022, Art. no. 5505517.\n[4] S. Jia et al., “3-D gabor convolutional neural network for hyperspectral im-\nage classiﬁcation,”IEEE Trans. Geosci. Remote Sens., vol. 60, Jun. 2022,\nArt. no. 5509216.\n[5] S. Jia et al., “A semisupervised siamese network for hyperspectral image\nclassiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 60, Oct. 2022,\nArt. no. 5516417.\n[6] B. Tu, Q. Ren, C. Zhou, S. Chen, and W. He, “Feature extraction using\nmultidimensional spectral regression whitening for hyperspectral image\nclassiﬁcation,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.,\nvol. 14, pp. 8326–8340, Aug. 2021.\n[7] M. X. Sen Jia and Z. Zhan, “Shearlet-based structure-aware ﬁltering for\nhyperspectral and lidar data classiﬁcation,”J. Remote Sens., vol. 2021,\nArt. no. 9825415.\n[8] S. Jia, X. Deng, J. Zhu, M. Xu, J. Zhou, and X. Jia, “Collaborative\nrepresentation-based multiscale superpixel fusion for hyperspectral im-\nage classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 10,\npp. 7770–7784, Oct. 2019.\n[9] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote sens-\ning images with support vector machines,”IEEE Trans. Geosci. Remote\nSens., vol. 42, no. 8, pp. 1778–1790, Aug. 2004.\n[10] Y . Zhang, W. Li, and R. Tao, “Domain adaptation based on graph and\nstatistical features for cross-scene hyperspectral image classiﬁcation,” in\nProc. IEEE Int. Geosci. Remote Sens. Symp., 2021, pp. 5374–5377.\n[11] Y . Zhou, P. Chen, N. Liu, Q. Yin, and F. Zhang, “Graph-embedding\nbalanced transfer subspace learning for hyperspectral cross-scene clas-\nsiﬁcation,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 15,\npp. 2944–2955, Mar. 2022.\n[12] Y . Zhang, W. Li, R. Tao, J. Peng, Q. Du, and Z. Cai, “Cross-scene\nhyperspectral image classiﬁcation with discriminative cooperative align-\nment,”IEEE Trans. Geosci. Remote Sens., vol. 59, no. 11, pp. 9646–9660,\nNov. 2021.\n[13] H. Lee, S. Eum, and H. Kwon, “Cross-domain CNN for hyperspectral\nimage classiﬁcation,” inProc. IEEE Int. Geosci. Remote Sens. Symp., 2018,\npp. 3627–3630.\n[14] A. A. Bayanuddin et al., “Nadir vs. off-nadir: Initial look at LAPAN-A3\noff-nadir acquisition mode on its spectral quality,” inProc. IEEE Int. Conf.\nAerosp. Electron. Remote Sens. Technol., 2019, pp. 1–6.\n[15] J. G. Masek et al., “A landsat surface reﬂectance dataset for north America,\n1990-2000,” IEEE Geosci. Remote Sens. Lett., vol. 3, no. 1, pp. 68–72,\nJan. 2006.\n[16] E. F. Vermote, D. Tanré, J. L. Deuze, M. Herman, and J.-J. Morcette,\n“Second simulation of the satellite signal in the solar spectrum, 6S: An\noverview,”IEEE Trans. Geosci. Remote Sens., vol. 35, no. 3, pp. 675–686,\nMay 1997.\n[17] L. Bruzzone and D. F. Prieto, “Unsupervised retraining of a maximum\nlikelihood classiﬁer for the analysis of multitemporal remote sensing\nimages,” IEEE Trans. Geosci. Remote Sens., vol. 39, no. 2, pp. 456–460,\nFeb. 2001.\n[18] B. Deng, S. Jia, and D. Shi, “Deep metric learning-based feature embed-\nding for hyperspectral image classiﬁcation,”IEEE Trans. Geosci. Remote\nSens., vol. 58, no. 2, pp. 1422–1435, Feb. 2020.\n[19] Y . Zhang, W. Li, M. Zhang, Y . Qu, R. Tao, and H. Qi, “Topological structure\nand semantic information transfer network for cross-scene hyperspectral\nimage classiﬁcation,”IEEE Trans. Neural Netw. Learn. Syst., early access,\nSep. 2021, doi:10.1109/TNNLS.2021.3109872.\n[20] C. Yu, C. Liu, H. Yu, M. Song, and C.-I. Chang, “Unsupervised domain\nadaptation with dense-based compaction for hyperspectral imagery,”IEEE\nJ. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 14, pp. 12287–12299,\nNov. 2021.\n[21] H. Liu, W. Li, X.-G. Xia, M. Zhang, C.-Z. Gao, and R. Tao, “Spectral shift\nmitigation for cross-scene hyperspectral imagery classiﬁcation,”IEEE J.\nSel. Topics Appl. Earth Observ. Remote Sens., vol. 14, pp. 6624–6638,\nJun. 2021.\n[22] S. Jia, S. Jiang, Z. Lin, N. Li, M. Xu, and S. Yu, “A survey: Deep\nlearning for hyperspectral image classiﬁcation with few labeled samples,”\nNeurocomputing, vol. 448, pp. 179–204, Dec. 2021.\n[23] D. Alajaji, H. S. Alhichri, N. Ammour, and N. Alajlan, “Few-shot learning\nfor remote sensing scene classiﬁcation,” inProc. Mediterranean Middle-\nEast Geosci. Remote Sens. Symp., 2020, pp. 81–84.\n[24] P.-C. Tu and H.-K. Pao, “A dropout style model augmentation for cross\ndomain few-shot learning,” in Proc. IEEE Int. Conf. Big Data, 2021,\npp. 1138–1147.\n[25] Z. Li, M. Liu, Y . Chen, Y . Xu, W. Li, and Q. Du, “Deep cross-domain few-\nshot learning for hyperspectral image classiﬁcation,”IEEE Trans. Geosci.\nRemote Sens., vol. 60, Feb. 2021, Art. no. 5501618.\n[26] Y . Zhang, W. Li, M. Zhang, and R. Tao, “Dual graph cross-domain few-shot\nlearning for hyperspectral image classiﬁcation,” inProc. IEEE Int. Conf.\nAcoust. Speech Signal Process., 2022, pp. 3573–3577.\n[27] D. Hong et al., “SpectralFormer: Rethinking hyperspectral image classi-\nﬁcation with transformers,”IEEE Trans. Geosci. Remote Sens., vol. 60,\nNov. 2021, Art. no. 5518615.\n[28] X. He, Y . Chen, and Z. Lin, “Spatial-spectral transformer for hyperspectral\nimage classiﬁcation,”Remote Sens., vol. 13, no. 3, 2021, Art. no. 498.\n[29] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” inProc. Int. Conf. Learn. Representations,\nApr. 2020.\nPENG et al.: CONVOLUTIONAL TRANSFORMER-BASED FEW-SHOT LEARNING 1349\n[30] X. Chen, S.-I. Kamata, and W. Zhou, “Hyperspectral image classiﬁcation\nbased on multi-stage vision transformer with stacked samples,” inProc.\nIEEE Region 10 Conf., 2021, pp. 441–446.\n[31] Y . Wu, J. Feng, G. Bai, Q. Gao, and X. Zhang, “Hyperspectral image\nclassiﬁcation based on spectrally-enhanced and densely connected trans-\nformer model,” inProc. IEEE Int. Geosci. Remote Sens. Symp., 2022,\npp. 2746–2749.\n[32] J. Feng, X. Luo, S. Li, Q. Wang, and J. Yin, “Spectral transformer\nwith dynamic spatial sampling and gaussian positional embedding for\nhyperspectral image classiﬁcation,” inProc. IEEE Int. Geosci. Remote\nSens. Symp., 2022, pp. 3556–3559.\n[33] Y . Peng, Y . Zhang, B. Tu, Q. Li, and W. Li, “Spatial–spectral transformer\nwith cross-attention for hyperspectral image classiﬁcation,”IEEE Trans.\nGeosci. Remote Sens., vol. 60, Sep. 2022, Art. no. 5537415.\n[34] R. Gopalan et al., “Domain adaptation for visual recognition,”Foundations\nTrends Comput. Graph. Vis., vol. 8, no. 4, pp. 285–378, 2015.\n[35] J. Zheng et al., “A two-stage adaptation network (TSAN) for remote\nsensing scene classiﬁcation in single-source-mixed-multiple-target do-\nmain adaptation (s2m2t da) scenarios,”IEEE Trans. Geosci. Remote Sens.,\nvol. 60, Aug. 2021, Art. no. 5609213.\n[36] Y . Chen, C. Yang, Y . Zhang, and Y . Li, “Conditional adaptation deep\nnetworks for unsupervised cross domain image classifcation,” inProc.\n14th IEEE Conf. Ind. Electron. Appl., 2019, pp. 517–521.\n[37] R. Gopalan, R. Li, and R. Chellappa, “Unsupervised adaptation across do-\nmain shifts by generating intermediate data representations,”IEEE Trans.\nPattern Anal. Mach. Intell., vol. 36, no. 11, pp. 2288–2302, Nov. 2014.\n[38] Y . Fu, Y . Fu, and Y .-G. Jiang, “Meta-fdmixup: Cross-domain few-shot\nlearning guided by labeled target data,” inProc. 29th ACM Int. Conf.\nMultimedia, 2021, pp. 5326–5334.\n[39] J. Geng, X. Ma, W. Jiang, X. Hu, D. Wang, and H. Wang, “Cross-scene\nhyperspectral image classiﬁcation based on deep conditional distribution\nadaptation networks,” inProc. IEEE Int. Geosci. Remote Sens. Symp.,\n2019, pp. 716–719.\n[40] A. Tavera, F. Cermelli, C. Masone, and B. Caputo, “Pixel-by-pixel\ncross-domain alignment for few-shot semantic segmentation,” inProc.\nIEEE/CVF Winter Conf. Appl. Comput. Vis., 2022, pp. 1626–1635.\n[41] N. Lai, M. Kan, C. Han, X. Song, and S. Shan, “Learning to learn\nadaptive classiﬁer–predictor for few-shot learning,”IEEE Trans. Neural\nNetw. Learn. Syst., vol. 32, no. 8, pp. 3458–3470, Aug. 2021.\n[42] D. Chen, Y . Chen, Y . Li, F. Mao, Y . He, and H. Xue, “Self-supervised\nlearning for few-shot image classiﬁcation,” in Proc. IEEE Int. Conf.\nAcoust. Speech Signal Process., 2021, pp. 1745–1749.\n[43] J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for few-\nshot learning,” inProc. 31st Int. Conf. Neural Inf. Process. Syst., 2017,\npp. 4080–4090.\n[44] Z. Li, F. Zhou, F. Chen, and H. Li, “Meta-SGD: Learning to learn quickly\nfor few-shot learning,” 2017,arXiv:1707.09835.\n[45] M. Yan, “Adaptive learning knowledge networks for few-shot learning,”\nIEEE Access, vol. 7, pp. 119041–119051, 2019.\n[46] X. Sun, B. Wang, Z. Wang, H. Li, H. Li, and K. Fu, “Research progress\non few-shot learning for remote sensing image interpretation,”IEEE J.\nSel. Topics Appl. Earth Observ. Remote Sens., vol. 14, pp. 2387–2402,\nJan. 2021.\n[47] Y . Ding and P. Wang, “Reasearch on cross domain few-shot learning\nmethod based on local feature association,” inProc. 6th Int. Symp. Comput.\nInf. Process. Technol., 2021, pp. 754–759.\n[48] Y . Wang, Q. Yao, J. T. Kwok, and L. M. Ni, “Generalizing from a few\nexamples: A survey on few-shot learning,”ACM Comput. Surv., vol. 53,\nno. 3, pp. 1–34, 2020.\n[49] F. Sung, Y . Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales,\n“Learning to compare: Relation network for few-shot learning,” inProc.\nIEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 1199–1208.\n[50] K. Gao, B. Liu, X. Yu, J. Qin, P. Zhang, and X. Tan, “Deep relation network\nfor hyperspectral image few-shot classiﬁcation,”Remote Sens., vol. 12,\nno. 6, 2020, Art. no. 923.\n[51] N. Parmar et al., “Image transformer,” inProc. 35th Int. Conf. Mach.\nLearn., 2018, pp. 4055–4064.\n[52] Z. Zhao, D. Hu, H. Wang, and X. Yu, “Convolutional transformer network\nfor hyperspectral image classiﬁcation,”IEEE Geosci. Remote Sens. Lett.,\nvol. 19, Apr. 2022, Art. no. 6009005.\n[53] K. Han et al., “A survey on visual transformer,”IEEE Trans. Pattern Anal.\nMach. Intell., vol. 45, no. 1, pp. 87–110, Jan. 2023.\n[54] A. Vaswani et al., “Attention is all you need,” inProc. 31st Int. Conf.\nNeural Inf. Process. Syst., 2017, pp. 6000–6010.\n[55] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, “Transformer in\ntransformer,” in Proc. 35th Int. Conf. Neural Inf. Process. Syst., 2021,\nvol. 34, pp. 15908–15919.\n[56] M. Popel and O. Bojar, “Training tips for the transformer model,” 2018,\narXiv:1804.00247.\n[57] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative posi-\ntion representations,” 2018,arXiv:1803.02155.\n[58] N. Yokoya and A. Iwasaki, “Airborne hyperspectral data over Chikusei,”\nSpace Appl. Lab., Univ. Tokyo, Tokyo, Japan, Tech. Rep. SAL-2016-05-\n27, May 2016.\n[59] K. Huang, S. Li, X. Kang, and L. Fang, “Spectral–spatial hyperspectral\nimage classiﬁcation based on KNN,”Sens. Imag., vol. 17, no. 1, pp. 1–13,\n2016.\n[60] B. Liu, X. Yu, A. Yu, P. Zhang, G. Wan, and R. Wang, “Deep few-shot\nlearning for hyperspectral image classiﬁcation,” IEEE Trans. Geosci.\nRemote Sens., vol. 57, no. 4, pp. 2290–2304, Apr. 2018.\nYishu Peng (Member, IEEE) received the B.E., M.S.,\nand Ph.D. degrees from Northeastern University,\nShenyang, China, in 2009, 2011, and 2017, respec-\ntively, all in mechanical design and theory.\nFrom 2017 to 2019, he was with the School of Me-\nchanical and Engineering, Hunan Institute of Science\nand Technology, Yueyang, China, and since 2019, he\nhas been with the School of Information Science and\nTechnology. His research interests include the image\nprocessing, object detection, and target tracing.\nYaru Liu (Student Member, IEEE) received the\nB.S. degree in communication engineering from the\nLanzhou University of Technology, Lanzhou, China,\nin 2016. She is currently working toward the M.S. de-\ngree in information and communication engineering\nwith the Hunan Institute of Science and Technology,\nYueyang, China.\nHer research interests include hyperspectral image\nprocessing, computer vision, and deep learning.\nBing Tu (Member, IEEE) received the M.S. degree\nin control science and engineering from the Guilin\nUniversity of Technology, Guilin, China, in 2009, and\nthe Ph.D. degree in mechatronic engineering from the\nBeijing University of Technology, Beijing, China, in\n2013.\nFrom 2015 to 2016, he was a Visiting Researcher\nwith the Department of Computer Science and En-\ngineering, University of Nevada, Reno, NV , USA,\nwhich is supported by the China Scholarship Council.\nSince 2018, he had been an Associate Professor with\nthe School of Information Science and Engineering, Hunan Institute of Science\nand Technology, Yueyang, China, where he is currently a Full Professor. His\nresearch interests include sparse representation, pattern recognition, and analysis\nin remote sensing.\nDr. Tu is an Associate Editor of theIEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing.\nYuwen Zhang (Student Member, IEEE) received the\nB.S. degree in electrical engineering and automation\nfrom the Hunan Institute of Science and Technol-\nogy, Yueyang, China, in 2020, where he is currently\nworking toward the M.S. degree in information and\ncommunication engineering.\nHis research interests include image processing,\nclassiﬁcation of multisource remote sensing data, and\nobject detection.",
  "topic": "Hyperspectral imaging",
  "concepts": [
    {
      "name": "Hyperspectral imaging",
      "score": 0.7227848172187805
    },
    {
      "name": "Computer science",
      "score": 0.7009301781654358
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6190482974052429
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5142435431480408
    },
    {
      "name": "Transformer",
      "score": 0.45988380908966064
    },
    {
      "name": "Convolutional neural network",
      "score": 0.43727147579193115
    },
    {
      "name": "Computer vision",
      "score": 0.3967894911766052
    },
    {
      "name": "Physics",
      "score": 0.08257988095283508
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I100286613",
      "name": "Hunan Institute of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I5343935",
      "name": "Guilin University of Electronic Technology",
      "country": "CN"
    }
  ],
  "cited_by": 48
}