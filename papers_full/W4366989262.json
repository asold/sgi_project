{
  "title": "Token-Selective Vision Transformer for fine-grained image recognition of marine organisms",
  "url": "https://openalex.org/W4366989262",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Si, Guangzhe",
      "affiliations": [
        "Ocean University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2034994998",
      "name": "Xiao Ying",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2042991852",
      "name": "Wei Bin",
      "affiliations": [
        "Affiliated Hospital of Qingdao University",
        "Qingdao University"
      ]
    },
    {
      "id": null,
      "name": "Bullock, Leon Bevan",
      "affiliations": [
        "Ocean University of China"
      ]
    },
    {
      "id": "https://openalex.org/A161592578",
      "name": "Wang YueYue",
      "affiliations": [
        "Ocean University of China"
      ]
    },
    {
      "id": "https://openalex.org/A1995646532",
      "name": "Wang Xiao-dong",
      "affiliations": [
        "Ocean University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2138237759",
    "https://openalex.org/W2122085882",
    "https://openalex.org/W6636475194",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3008809756",
    "https://openalex.org/W3198148928",
    "https://openalex.org/W2905050280",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6755069125",
    "https://openalex.org/W6741769367",
    "https://openalex.org/W6692647644",
    "https://openalex.org/W6760333050",
    "https://openalex.org/W3100476583",
    "https://openalex.org/W4295903508",
    "https://openalex.org/W6792309431",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6803341296",
    "https://openalex.org/W2973445298",
    "https://openalex.org/W2148654450",
    "https://openalex.org/W6737012129",
    "https://openalex.org/W6680138032",
    "https://openalex.org/W4293062810",
    "https://openalex.org/W2998543752",
    "https://openalex.org/W6676165494",
    "https://openalex.org/W6641023773",
    "https://openalex.org/W6772623564",
    "https://openalex.org/W6807439159",
    "https://openalex.org/W2595156303",
    "https://openalex.org/W3080525841",
    "https://openalex.org/W4309878927",
    "https://openalex.org/W3172986704",
    "https://openalex.org/W4221161982",
    "https://openalex.org/W2186155590",
    "https://openalex.org/W6766263406",
    "https://openalex.org/W6839636014",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W1530098540",
    "https://openalex.org/W6752285806",
    "https://openalex.org/W1954152232",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4321202100",
    "https://openalex.org/W6850079169",
    "https://openalex.org/W3027362665",
    "https://openalex.org/W6798264847",
    "https://openalex.org/W2763070548",
    "https://openalex.org/W6754854709",
    "https://openalex.org/W6753385296",
    "https://openalex.org/W6602324145",
    "https://openalex.org/W4210447461",
    "https://openalex.org/W6746649447",
    "https://openalex.org/W6760951768",
    "https://openalex.org/W6788620109",
    "https://openalex.org/W4321033361",
    "https://openalex.org/W4220767255",
    "https://openalex.org/W4320081372",
    "https://openalex.org/W2737725206",
    "https://openalex.org/W2773003563",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3178389175",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1616462885",
    "https://openalex.org/W4210394938",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3206734547",
    "https://openalex.org/W2891951760",
    "https://openalex.org/W2951852399",
    "https://openalex.org/W2104657103",
    "https://openalex.org/W56385144",
    "https://openalex.org/W4285309765",
    "https://openalex.org/W2608339816",
    "https://openalex.org/W4323022483",
    "https://openalex.org/W2998345525",
    "https://openalex.org/W2963066927",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2963393555",
    "https://openalex.org/W3139434170",
    "https://openalex.org/W1955942245",
    "https://openalex.org/W2807931652",
    "https://openalex.org/W2883502031",
    "https://openalex.org/W2963407932",
    "https://openalex.org/W2194775991"
  ],
  "abstract": "Introduction The objective of fine-grained image classification on marine organisms is to distinguish the subtle variations in the organisms so as to accurately classify them into subcategories. The key to accurate classification is to locate the distinguishing feature regions, such as the fish’s eye, fins, or tail, etc. Images of marine organisms are hard to work with as they are often taken from multiple angles and contain different scenes, additionally they usually have complex backgrounds and often contain human or other distractions, all of which makes it difficult to focus on the marine organism itself and identify its most distinctive features. Related work Most existing fine-grained image classification methods based on Convolutional Neural Networks (CNN) cannot accurately enough locate the distinguishing feature regions, and the identified regions also contain a large amount of background data. Vision Transformer (ViT) has strong global information capturing abilities and gives strong performances in traditional classification tasks. The core of ViT, is a Multi-Head Self-Attention mechanism (MSA) which first establishes a connection between different patch tokens in a pair of images, then combines all the information of the tokens for classification. Methods However, not all tokens are conducive to fine-grained classification, many of them contain extraneous data (noise). We hope to eliminate the influence of interfering tokens such as background data on the identification of marine organisms, and then gradually narrow down the local feature area to accurately determine the distinctive features. To this end, this paper put forwards a novel Transformer-based framework, namely Token-Selective Vision Transformer (TSVT), in which the Token-Selective Self-Attention (TSSA) is proposed to select the discriminating important tokens for attention computation which helps limits the attention to more precise local regions. TSSA is applied to different layers, and the number of selected tokens in each layer decreases on the basis of the previous layer, this method gradually locates the distinguishing regions in a hierarchical manner. Results The effectiveness of TSVT is verified on three marine organism datasets and it is demonstrated that TSVT can achieve the state-of-the-art performance.",
  "full_text": "Token-Selective Vision\nTransformer forﬁne-grained\nimage recognition of\nmarine organisms\nGuangzhe Si1, YingXiao2, BinWei3, Leon BevanBullock4,\nYueyue Wang5 and XiaodongWang4*\n1College of Electronic Engineering, Ocean University of China, Qingdao, Shandong, China,2School of\nScience, The Hong Kong University of Science and Technology, Hong Kong, Hong Kong SAR, China,\n3The Afﬁliated Hospital of Qingdao University/Shandong Key Laboratory of Digital Medicine and\nComputer Assisted Surgery, Qingdao University, Qingdao, Shandong, China,4College of Computer\nScience and Technology, Ocean University of China, Qingdao, Shandong, China,5Computing Center,\nOcean University of China, Qingdao, Shandong, China\nIntroduction: The objective of ﬁne-grained image classiﬁcation on marine\norganisms is to distinguish the subtle variations in the organisms so as to\naccurately classify them into subcategories. The key to accurate classiﬁcation\nis to locate the distinguishing feature regions, such as theﬁsh’s eye,ﬁns, or tail,\netc. Images of marine organisms are hard to work with as they are often taken\nfrom multiple angles and contain different scenes, additionally they usually have\ncomplex backgrounds and often contain human or other distractions, all of\nwhich makes it difﬁcult to focus on the marine organism itself and identify its\nmost distinctive features.\nRelated work:Most existingﬁne-grained image classiﬁcation methods based on\nConvolutional Neural Networks (CNN) cannot accurately enough locate the\ndistinguishing feature regions, and the identiﬁed regions also contain a large\namount of background data. Vision Transformer (ViT) has strong global\ninformation capturing abilities and gives strong performances in traditional\nclassiﬁcation tasks. The core of ViT, is a Multi-Head Self-Attention mechanism\n(MSA) which ﬁrst establishes a connection between different patch tokens in a\npair of images, then combines all the information of the tokens for classiﬁcation.\nMethods: However, not all tokens are conducive toﬁne-grained classiﬁcation,\nmany of them contain extraneous data (noise). We hope to eliminate the\ninﬂuence of interfering tokens such as background data on the identiﬁcation\nof marine organisms, and then gradually narrow down the local feature area to\naccurately determine the distinctive features. To this end, this paper put forwards\na novel Transformer-based framework, namely Token-Selective Vision\nTransformer (TSVT), in which the Token- Selective Self-Attention (TSSA) is\nproposed to select the discriminat ing important tokens for attention\ncomputation which helps limits the attention to more precise local regions.\nFrontiers inMarine Science frontiersin.org01\nOPEN ACCESS\nEDITED BY\nXuemin Cheng,\nTsinghua University, China\nREVIEWED BY\nNing Wang,\nDalian Maritime University, China\nPeng Ren,\nChina University of Petroleum (East China),\nChina\n*CORRESPONDENCE\nXiaodong Wang\nwangxiaodong@ouc.edu.cn\nSPECIALTY SECTION\nThis article was submitted to\nOcean Observation,\na section of the journal\nFrontiers in Marine Science\nRECEIVED 26 February 2023\nACCEPTED 04 April 2023\nPUBLISHED 25 April 2023\nCITATION\nSi G,Xiao Y,Wei B,Bullock LB,Wang Y\nand Wang X (2023) Token-Selective Vision\nTransformer forﬁne-grained image\nrecognition of marine organisms.\nFront. Mar. Sci.10:1174347.\ndoi: 10.3389/fmars.2023.1174347\nCOPYRIGHT\n©2 0 2 3S i ,X i a o ,W e i ,B u l l o c k ,W a n g\nand Wang. This is an open-access article\ndistributed under the terms of theCreative\nCommons Attribution License (CC BY).The\nuse, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nTYPE Original Research\nPUBLISHED 25 April 2023\nDOI 10.3389/fmars.2023.1174347\nTSSA is applied to different layers, and the number of selected tokens in each\nlayer decreases on the basis of the previous layer, this method gradually locates\nthe distinguishing regions in a hierarchical manner.\nResults: The effectiveness of TSVT is veriﬁed on three marine organism datasets\nand it is demonstrated that TSVT can achieve the state-of-the-art performance.\nKEYWORDS\ntoken-selective, self-attention, vision transformer, ﬁne-grained image classiﬁcation,\nmarine organisms\n1 Introduction\nFine-grained Image Classiﬁcation (FIC) is a challenging task\nwhich utilizes subtle variations of the same species to differentiate\nthe different subcategories, examples include birds (Van Horn et al.,\n2015), dogs (Khosla et al., 2011), and cars (Krause et al., 2013).\nUnlike general image classiﬁcation, FIC requires sufﬁcient attention\nbeing paid to the distinguishing features between the subcategories.\nThere are a large number of highly similarﬁsh and plankton in the\nocean, and the classiﬁcation of these subcategories (Li et al., 2019; Li\net al., 2022) is conducive to the protection of marine ecology and\nbiodiversity. However, the images of marine organisms are often\ntaken in multi-angle and multi-scene situations, additionally, the\nbackground of marine life images is complex, which also increases\nthe difﬁculty of recognition.\nRecently, ﬁne-grained image classiﬁcation methods have made\ngreat progress due to the development of Deep Neural Networks\n(DNNs) (Simonyan and Zisserman, 2015; He et al., 2016; Liu et al.,\n2022; Shi et al., 2022; Wang et al., 2022). Strongly supervisedﬁne-\ngrained classiﬁcation methods (Branson et al., 2014; Zhang et al.,\n2014; Wei et al., 2018) require labor-intensive labeling of images, so\nweakly supervised classiﬁcation methods which rely only on category\nlabels are now commonly preferred. CNN-based weakly supervised\nmethods onﬁne-grained image classiﬁcation can be mainly divided\ninto localization methods and feature-encoding methods.\nLocalization methods ﬁrst locate the distinguishing regions and\nthen extract features from these regions for classi ﬁcation. For\nexample, some works (Ge et al., 2019; Liu et al., 2020) obtain the\ndiscriminating bounding boxes through Region Proposal Networks\n(RPNs) and then feed these regions into the backbone network for\nclassiﬁcation. However, the bounding boxes contain a lot of\nbackground areas with interfering information. Therefore, the\ndiscriminating regions localized by these methods are not precise\nenough. In addition, whilst the feature-encoding methods (Lin T.-Y.\net al., 2015; Yu et al., 2018) make the output of the network change\nfrom semantic features to high-order features which can represent\nﬁne-grained information by means of feature fusion, the high-order\nfeatures obtained by these methods have large dimensions, and the\nﬁne-grained information is not distinguishable.\nRecently, Vision Transformer (ViT) (Dosovitskiy et al., 2021)\nhas demonstrated potent perfo rmance on various visual tasks\n(Carion et al., 2020 ; Zheng et al., 2021 ; Guo et al., 2022 ).\nSpeciﬁcally, in the task of image classiﬁcation, a whole image is\nsplit into several patches, and each patch is converted into a token\nthrough linear projection. Then, the importance of each token is\nobtained through the Multi-Head Self Attention (MSA), andﬁnally\nall of the tokens are combined according to the importance for\nclassi ﬁcation. MSA in Transformer provides long-range\ndependency to enhance the interaction among image patches, so\nTransformer is able to locate subtle features and explore their\nrelations from a large global sc ale perspective, whereas a\ntraditional CNN has limited receptiveﬁelds and weak long range\nrelationship abilities in very high layers with ﬁxed-size\nconvolutional kernels. ViT is therefore better suited to ﬁne-\ngrained classiﬁcation tasks. In addition to the above advantages,\nViT also has certain shortcomings, such as insufﬁcient local sensing\nability, tedious computation of MSA, and the need to consider the\ncorrelation among all tokens, our research is dedicated to\nimproving these deﬁciencies.\nImages of marine organisms are mostly taken from the bottom\nof the sea, the background of the images often contains reefs, corals\nand algae, which interferes with the recognition of the marine\norganisms themselves. A few images of marine life are taken from\nbeaches, ﬁshing boats and other scenes, the change of scenes also\naffects the identiﬁcation of marine life. At the same time, due to the\nirresistible factors of camera angle and distance, images of the same\nsubcategory show diverse global features, so paying too much\nattention to the global information is not conducive to correct\nclassiﬁcation. Examples of the three different scenarios are shown\nin Figure 1.\nIn this paper, to reduce the interference of intra-category\ndiverse global information and useless background information,\nwe propose a novel Token-Selective Vision Transformer (TSVT) for\nﬁne-grained image classiﬁcation of marine organisms, which selects\ndiscriminative tokens layer by layer and gradually excludes\ninterfering tokens. We propose a localized attention mechanism\ncalled Token-Selective Self-Attention (TSSA) to explore contextual\ninformation in discriminating regions and enhance the interaction\namongst selected tokens. Inﬂuenced by the idea of clustering, for\neach discriminative token, only the other discriminative tokens\nrelated to it are selected for information interaction, then the class\ntoken integrates the information of these discriminative tokens for\nSi et al. 10.3389/fmars.2023.1174347\nFrontiers inMarine Science frontiersin.org02\nclassiﬁcation. Finally, we verify the ef ﬁcacy of TSVT for ﬁne-\ngrained image classiﬁcation of marine organisms on three marine\nbiological datasets.\nIn summary, our work has the following three contributions:\n We propose TSVT, a novel Vision Transformer framework\nfor ﬁne-grained image classiﬁcation of marine organisms\nthat excludes background interference and reﬁnes the range\nof distinguishing regions layer by layer.\n We propose Token-Selective Self-Attention (TSSA), which\nremoves the interference of irrelevant tokens, and then\nestablish the association of selected tokens in local regions\nand extract the most discriminative features.\n We conduct experiments on three different datasets to\nverify the effectiveness of our method, and show that\nTSVT achieves state-of-the-art performance. Additionally,\nwe perform comparative experiments on TSSA ’s\nparameters to further explore the impact of applying\nTSSA to different layers, using different methods to select\ntokens and selecting different numbers of tokens on model\nperformance.\n2 Related work\n2.1 Fine-grained image classiﬁcation\n2.1.1 CNN forﬁne-grained image classiﬁcation\nThe ﬁne-grained image classiﬁcation methods based on CNN\nare mainly divided into two categories: localization methods and\nfeature-encoding methods.\nThe basic idea of localization methods is to locate discriminative\nlocal regionsﬁrst, and perform feature extraction on these regions,\nthen cascade the extracted features and then again feed them to the\nsub-network for classiﬁcation. Earlier localization methods (Zhang\net al., 2014; Lin D. et al., 2015) rely on additional manual annotation\ninformation such as object bounding boxes and part annotation to\nhelp the network ﬁnd the region with the most representative\nfeatures. However, since such annotations are time-consuming\nand labor-intensive, more weakly supervised methods which only\nrequire image-level labels are preferred. Some methods (Ge et al.,\n2019; Liu et al., 2020) use RPN to obtain discriminative bounding\nboxes and input the selected feature regions into the network to\ncapture local features. In addition, there are also methods to locate\ndiscriminative regions by utilizing an attention mechanism: RA-\nCNN (Fu et al., 2017) proposed Recurrent Attention to select a\nseries of distinguishing regions for attention mapping in a coarse-\nto-ﬁne manner; MA-CNN (Zheng et al., 2017) adopted a Multi-\nAttention CNN structure to obtain multiple distinguishing regions\nin parallel; MAMC ( Sun et al., 2018 )d i r e c t e dt h eg e n e r a t e d\nattention features to categories to help better classiﬁcation; NTS-\nNet (Yang et al., 2018) used a collaborative learning method to\naccurately identify the feature information regions.\nFeature-encoding methods obtain richerﬁne-grained features\nfor classiﬁcation in the form of high-level feature interactions and\nthe design of loss functions. As the most representative method for\nhigh-level feature interaction, B-CNN (Lin T.-Y. et al., 2015) used\ntwo deep convolutional networks to extract features from the same\nimage, and then performed outer product operations on the feature\nvectors to obtain bilinear features for classiﬁcation. However, the\nlarge feature dimensions of this method leads to a very large\nnumber of parameters, which is not easy to drive during training.\nTo solve this problem, C-BCNN (Gao et al., 2016) adopted tensor\nsketches to reduce the dimensions of high-dimensional features.\nFIGURE 1\nSome examples of marine life images. Three rows sequentially represent images with complex backgrounds, images of multiple scenes, and images\nof marine life taken from multiple angles.\nSi et al. 10.3389/fmars.2023.1174347\nFrontiers inMarine Science frontiersin.org03\nOther methods attempt to capture features at higher levels to obtain\na more distinguishable feature representation. HBP (Yu et al., 2018)\ncombined the features of different layers through bilinear pooling,\nand ﬁnally concatenated them for classiﬁcation. The loss function\nplays the role of a conductor’s baton in Deep Learning and model\nlearning is driven by it. Inﬁne-grained image classiﬁcation tasks,\nthere are corresponding approaches to the design of loss functions:\nMaxEnt (Dubey et al., 2018) provided a training routine that\nmaximizes the entropy of the output probability distribution;\nMC-Loss (Chang et al., 2020) focused on different local areas of\neach channel in the feature map, which is more conducive to\nfeature learning.\n2.1.2 ViT forﬁne-grained image classiﬁcation\nTransformer (Vaswani et al., 2017) wasﬁrst applied to solve the\nsequence to sequence problem in Natural Language Processing\n(NLP) and has achieved better results than both convolutional\nneural networks (CNNs) and recurrent neural networks (RNNs).\nSubsequently, Transformer has been widely used in theﬁeld of\ncomputer vision. ViT ( Dosovitskiy et al., 2021 )w a st h eﬁrst\ntransformer-based model for image classi ﬁcation, which splits\nimages into a number of patches and inputs them to the\ntransformer layer, and then establishes the association between\ndifferent patches with the help of MSA, the classiﬁcation is ﬁnally\ncarried out by using the class token. TransFG (He et al., 2022) was\nthe ﬁrst to verify the effectiveness of vision Transformer onﬁne-\ngrained visual classiﬁcation. The input of its last layer is the class\ntoken and some important tokens representing distinguishing\nfeatures rather than all of the tokens. In addition, RAMS-Trans\n(Hu et al., 2021) locates and extracts discriminative areas based on\nattention weights, and then re-inputs them into ViT for\nclassiﬁcation using multi-scale features.\nIn this paper, we propose TSSA, which allows each token to\nselect its own relevant tokens according to the attention weights for\nattention computation. We integrate the one-to-one selection of\neach token into the attention computation. Furthermore, we apply\nTSSA to different layers of ViT to narrow the selection range layer\nby layer, so as to gradually re ﬁne the distinguishing features,\nyielding the major difference between our work and\nprevious methods.\n2.2 Underwater image classiﬁcation\nDue to the inﬂuence of the complex imaging environment in the\nocean, the underwater images appear blurred, low contrast and low\nresolution, therefore various image preprocessing methods ( Qi\net al., 2022; Zhou et al., 2022; Zhou et al., 2023a; Zhou et al.,\n2023b) such as image enhancement and image restoration are used\nﬁrst to improve classiﬁcation results. Recently, signiﬁcant progress\nhas been made in underwater classiﬁcation, thanks to the inﬂuence\nof deep learning and the creation of several methods for underwater\norganism detection (Chen et al., 2021; Wang et al., 2023a; Wang\net al., 2023b ). The research on underwater biological image\nclassiﬁcation can be mainly divided into two aspects, one is the\nlearning of biological features, the other is the feature fusion of\ndifferent levels or types. For the feature acquisition methods, the\nearlier artiﬁcial methods (Alsmadi et al., 2010; Alsmadi et al., 2011)\nwere only effective for speciﬁc datasets or scenarios, subsequently\nuniversal methods based on deep learning were adopted to learn\nvarious features. DeepFish (Qin et al., 2016) ﬁrst extracted theﬁsh\nregions using matrix decomposition, and then reﬁned and learned\nthese regional features by Principal Components Analysis (PCA)\n(Jackson, 1993) and CNN respectively. MCNN ( Prasenan and\nSuriyakala, 2023) segmented ﬁsh images by the ﬁreﬂy algorithm\nand extracted features from the segmented parts. However, these\nmethods require a large amount of computation, therefore, to\nmaintain the balance between classi ﬁcation effect and cost, a\nnumber of efﬁcient improved CNN networks were proposed:\nFDCNet (Lu et al., 2018) used ﬁltering deep convolutional neural\nnetworks to classify deep-sea species; deconvolutional neural\nnetwork was applied to different squid classiﬁcation (Hu et al.,\n2020). In addition, in order to solve the noise background problem,\nAdaFish (Zhang et al., 2022) adopted adversarial learning to reduce\nthe interference of background on classiﬁcation.\nSome methods (Kartika and Herumurti, 2016\n; Gomez Chavez\net al., 2019 ) have obtained some limited improvement in\nclassiﬁcation accuracy by learning only a single feature such as\nﬁsh color or coral texture, therefore combining multi-level or multi-\npart information to complete classiﬁcation is another direction of\nunderwater image classiﬁcation. One method (Cui et al., 2018)\nintegrated the texture and shape features of plankton to improve\nCNN performance; another method (Mathur et al., 2020) combined\nthe characteristics of different parts of ﬁsh through cross\nconvolutional layer pooling for prediction; whilst yet another\nmethod used a multi-level residual network (Prasetyo et al., 2022)\nwhich fused high and low level information through depth\nseparable convolution was also proposed and achieved a good\nclassiﬁcation effect.\n3 Methodology\n3.1 Preliminary: vision transformer\nThe inputs of ViT are a sequence of serialized tokens. First, an\nimage with resolutionH /C2 W is ﬁrst split intoﬁxed-size patchesxp,\neach of sizeP /C2 P, so the number of patchesN is equal toH\nP /C2 H\nW .\nEach patch is transformed into a tokenxpt by a patch embedding\nlayer consisting of linear projection. In addition to patch tokens,\nthere is a dedicated class tokenxcls for ﬁnal classiﬁcation in the\nclassiﬁcation task. So all tokens include patch tokens and the class\ntoken. The above tokens only contain pixel information, and\nposition encoding adds corresponding position information xpos\nto each token to determine the position of each patch in the original\nimage. All tokens are then fed into the transformer encoder, and the\ninputs of the transformer encoderx0 are represented in Eq. 1:\nx0 = ½xcls; x1\npt; x2\npt; …; xN\npt/C138 + xpos : (1)\nSi et al. 10.3389/fmars.2023.1174347\nFrontiers inMarine Science frontiersin.org04\nTransformer encoder is the core of ViT and contains l\ntransformer layers of MSA and Multi-Layer Perceptron (MLP)\nblocks, as well as residual connections after every block. The\noutput of thelth layer is represented as follows:\nx*\nl = MSA(LN(xl−1)) +xl−1 (2)\nxl = MLP(LN(x*\nl )) +x*\nl , (3)\nwhere xl−1and xl denote the encoded image representation of\nthe l − 1th and lth transformer layers, x*\nl is the output of the MSA\nblock after residual connection,LN represents layer normalization,\nand the class token of the last transformer layer is used for category\nprediction through MLP.\n3.2 Overall architecture\nMarine life images of the same subcategories present different\nglobal information such as posture and viewpoint, so an over-\nreliance on global information and a lack of attention to local\ninformation are not conducive to the correct classi ﬁcation. In\naddition, due to the complexit y of the seabed environment,\nimages of marine organisms often contain complex backgrounds\nsuch as reefs and corals, which will also affect the identiﬁcation of\nmarine organisms. In order to address the above issues, weﬁrst\nconsider eliminating the interference of irrelevant factors such as\nthe background, and locating the marine organisms themselves,\nthen further locating the distinguishing areas. In this manner we\npropose TSVT, which selects tokens layer by layer for more accurate\nclassiﬁcation. By doing so, the number of tokens selected by the\nlatter layer is further reduced on the basis of the preceding layer so\nas to more accurately reﬁne the distinguishing areas and reduce the\ncomputational cost. To this end, we design a local attention module\nnamed TSSA, in which distinguishing tokens only interact with the\nother distinguishing tokens selected according to the attention\nweights, and the interference of background tokens is eliminated\nto obtain the purest distingui shing feature information for\nclassiﬁcation with the class token.\nThe framework of our TSVT is shown inFigure 2, where, the\nﬁrst eight transformers remain unchanged according to the settings\nof ViT, while the last four layers are Token-Selective Transformer\nLayer (TS Transformer Layer). It is different from the standard\ntransformer layer in that it replaces the original MSA with TSSA.\nThe number of tokens selected in each layer is different, and the\nlocal scope of attention is also different. The class token of the last\nlayer aggregates the most discriminating features in the local\nregions and completes category prediction through MLP.\n3.3 Token-selective self-attention\nFine-grained image classiﬁcation requires focusing on local\ndiscriminating regions, but the complex background of marine\nbiological images interferes with accurate localization of these\nregions. To solve the above issue, we propose to eliminate the\ninterference of background tokens to the greatest extent and apply\nlocal attention to the selected important discriminating tokens.\nAll tokens can be divided into two categories: discriminating\nregion tokens that play a positive effect in classi ﬁcation and\nbackground interfering tokens that play a negative effect in\nclassi ﬁcation. Discriminating reg ion tokens and background\nFIGURE 2\nThe framework of our proposed TSVT and the details of our designed TSSA. An image isﬁrst split into a number of patches, each of which is\nmapped into a feature vector by Linear Projection and combined with learnable position embedding. Contextual links between tokens are then\nestablished in the Transformer Layers, and the selection of tokens representing the discriminating regions is performed layer by layer in the latter\nfour TS Transformer Layers with the number of selected tokens in each layer decreasing from the previous layers. In the TS Transformer Layer, TSSA\nis a sparse selective attention mechanism that generates a mask based on the similarity between tokens so as to limit the attention computation\nbetween non-relevant tokens.\nSi et al. 10.3389/fmars.2023.1174347\nFrontiers inMarine Science frontiersin.org05\ntokens are clustered separately for information interaction in TSSA\nto ensure that discriminating tokens are no longer mixed with the\ninterference information of background tokens, and then the class\ntoken integrates the informatio n of distinctive tokens for the\nﬁnal classiﬁcation.\nThe correlation between tokens can be reﬂected by attention\nweights. Previous work (Wang et al., 2021; He et al., 2022) has\nproved that attention weights can be a good indicator for token\nselection. The attention weights of each head in each transformer\nlayer A ∈ R(N+1)/C2 (N+1) can be written as follows:\nA = softmax(Q·K T\nﬃﬃﬃﬃﬃ\ndk\np )= ½a0, a1, a2 …aN /C138 , (4)\nai = ½ai,0, ai,1, ai,2 …… ai,N /C138 , i ∈ (0, N) : (5)\nAccording to the attention weights, the information of the token\nis weighted and summed to obtain the calculation result of the\nattention symbolized as Attention. The following formula is the\ncalculation process of MSA:\nAttention = softmax(Q·K T\nﬃﬃﬃﬃﬃ\ndk\np ) ·V , (6)\nwhere Q, K and V are all obtained by the linear transformations\nof tokens, all of which represent information about the token itself;\ndk represents the dimensionality of K; softmax is a normalized\nexponential function; aij represents the degree of correlation\nbetween the ith token and thejth token, that is, tokeni as Q and\ntoken j as K for the calculation in Eq. 4;ai represents the set of\ncorrelation degrees between the ith token and all tokens; and ·\nrepresents the general matrix product.\nOnly the largestm elements in each row of attention weights are\nselected, the selected elements remain unchanged, and the\nremaining unselected elements are all set to zero, thus generating\nnew selective attention weights, which represent the degree of\ncorrelation between each token and its most relevantm tokens. In\nthe computation of attention, the distinguishing tokens interact\nwith each other and the distinguishing features are strengthened.\nIn the implementation, to ensure parallel computing, a mask\nmatrix M with the same shape as the attention weights isﬁrst\ngenerated, we set themth largest elementai in each row of attention\nweights as the threshold to determine whether the elements at\ndifferent positions of mask matrix are one or zero. The process of\nmask matrix conversion is represented as:\nM(i,j) =\n1  A(i,j) ≥ ai,\n0  otherwise,\n(\n(7)\nwhere (i, j) represents the position of each element in the mask\nmatrix the and attention weights in (n +1 )/C2 (n + 1) positions.\nThen the selective attention weights As are obtained by\ncomputing the Hadamard product of the mask matrix and the\nattention weights, as follows:\nAs = A ⊙M, (8)\nwhere ⊙ is the calculation symbol for Hadamard product.\nWithout changing the relevance of the different tokens, we\nfurther update the elementsas in the selective attention weights so\nthat the sum of the elements in each row is equal to one, which\nfurther increases the proportion of discriminative information in\nthe class token. Take theﬁrst row ofAs as an example, each element\nof this rowas0\nis computed as:\nas0\n0,i = as\n0,i\no\nN\nj=1as\n0,j\n: (9)\nThe new selective attention weightsA\n0\ns represent the correlation\nbetween tokens in local areas, and then after the calculation in Eq.\n10, the information between these tokens interacts and the outputZ\nof TSSA is obtained. In theﬁnal TS Transformer Layer, the class\ntoken combines the token information through MLP for category\npredictions.\nZ = A\n0\ns ·V : (10)\nThe selective attention weights of each token-selective\ntransformer layer are updated on the basis of the previous layer,\nand the number of selected tokensm of each layer is gradually\nreduced narrowing and reﬁning the distinguishing feature regions\nlayer by layer.\nWe apply TSSA to the deep layers of the model without\ndestroying the globality of th e shallow layers, and the local\ninformation based on the global basis is extracted for\nclassiﬁcation. Starting from the ﬁrst token-selective transformer\nlayer, the distinguishing tokens only aggregate important tokens\nrelated to them, so that the class token associated with these\ndistinguishing tokens can minimize the interference of the\nbackground tokens. Our model is actually a trade-off between\nglobality and locality, on the basis of not losing the globality, it\ncan accurately locate the discriminating area and extract\nlocal features.\n4 Experiments\nIn this section, we mainly introduce the experimental process\nand analyze the experiment results. First, we introduce the three\nmarine biological datasets used in experiments, and brie ﬂy\nintroduce the speciﬁc settings. Then, we verify the ef ﬁcacy of\nTSVT by ablation study and analyze the experiment results.\n4.1 Datasets\nWe validated the effectiveness of TSVT on three datasets of\nmarine organisms, namely ASLO-Plankton ( Sosik and Olson,\n2007), Sharks 1,a n dW i l d F i s h(Zhuang et al., 2018 ). ASLO-\nPlankton consists of 22 categories of marine plankton images, its\ntraining set is unbalanced, and the number of images in different\nsubcategories conforms to the lo ng-tail distribution; Sharks\ncontains images of 14 shark species, where the background of the\nimages is complex and the differences between images are subtle;\nWildFish is a large-scale marineﬁsh dataset with 1000 categories\nSi et al. 10.3389/fmars.2023.1174347\nFrontiers inMarine Science frontiersin.org06\nand 54459 images in total, and we randomly select images of 200\ncategories from WildFish to form a new dataset WildFish200. The\nstatistics of the three datasets are shown inTable 1.\n4.2 Implementation details\nThe input image size of the ASLO-Plankton, WildFish200 and\nSharks datasets is 448×448 pixels, the size of each patch is 16×16.\nWe set the batch size on the three datasets to 8. SGD optimizer is\nemployed with a momentum of 0.9. The learning rate is initialized\nas 0.03 and we adopt cosine annealing as the scheduler of optimizer.\nTSVT imports the pre-trained ViT-B_16 on ImageNet21k as the\npretrained model. We complete the construction of the whole\nmodel using PyTorch and run all experiments on four\nNVIDIAGTX 1070 GPUs in one computer.\n4.3 Comparison with the state-of-the-arts\nOur method performs on par with a number of CNN-based\nmethods: B-CNN (Lin T.-Y. et al., 2015), NTS-Net (Yang et al.,\n2018), TASN (Zheng et al., 2019), MC Loss (Chang et al., 2020), and\nthe recent transformer variants: ViT (Vaswani et al., 2017), RAMS-\nTrans (Hu et al., 2021), TransFG (He et al., 2022) on ASLO-\nPlankton, Sharks and WildFish200. The experiment results are\nshown in Table 2. It can be seen from the results that ViT-based\nmethods have a higher classiﬁcation accuracy than CNN-based\nmethods. Meanwhile, TSVT reaches 74.3%, 90.4% and 94.7% top-1\naccuracy on ASLO-Plankton, Sharks and WildFish200 respectively,\nwhich achieves higher accuracy in the identiﬁcation of marine\norganisms compared with other methods. The main reason for\nthe improvement is that our method further eliminates background\ninterference, accurately locates the discriminating areas, thus\nenlarging the differences between categories.\n4.4 Ablation study\nWe verify the efﬁcacy of our proposed TSSA on the three\ndatasets, and further explore the impact of applying TSSA to\ndifferent layers, using different methods to select tokens and\nselecting different numbers of tokens on model performance.\n4.4.1 Impact of applying TSSA to different layers\nWe applied TSSA to the shallow layers (1-4), middle layers (5-8)\nand deep layers (9-12) of TSVT respectively, to explore the\ninﬂuence of token selection in different layers on model\nperformance. The experiment results in the Table 3 show that\napplying TSSA to the deep layers achieves the best performance,\nwhilst starting token selection in the shallow layers achieves worse\nperformance. A possible reason is that the attention weights in\nshallow layers cannot highlight the key points that should be paid\nattention to, which is not enough to be used as the indicator for\nselecting tokens. On the contrary, with the deepening of layers, the\nfeature information is accumulated, and the model starts to notice\ndiscriminating regions. At this time, further eliminating\nbackground and other interference can make the discriminative\nlocal features account for a larger proportion ofﬁnal features used\nfor classiﬁcation. Global information needs to be strengthened by\nlayers of accumulation, premature destruction of the association\namong all tokens at shallow layers is not conducive to extracting\nglobal features of the model. Therefore, establishing the association\namong all tokens at the shallow layersﬁrst, and then discarding\nsome tokens at the deep layers is a trade-off between global\ninformation and local information, which is bene ﬁcial\nfor classiﬁcation.\nWhen TSSA is applied to the deep layers, the classiﬁcation\nperformance of the model is improved. So we further explore the\nimpact of applying TSSA to different deep layers. In different\nTABLE 2 Comparison of TSVT and state-of-the-art methods on three datasets of marine organisms.\nMethod Backbone Accuracy(%)\nASLO-Plankton Sharks WildFish200\nB-CNN VGG-16 61.9 76.2 82.1\nNTS-Net ResNet-50 69.4 84.5 87.3\nTASN ResNet-50 70.0 85.2 88.7\nMC Loss ResNet-50 69.6 86.3 86.2\nViT ViT-B_16 72.6 88.9 93.5\nRAMS-Trans ViT-B_16 73.1 89.2 93.8\nTransFG ViT-B_16 73.7 89.1 94.1\nTSVT (Ours) ViT-B_16 74.3 90.4 94.7\nTABLE 1 Statistics of ASLO-Plankton, Sharks and WildFish200 datasets.\nDataset Classes Training Testing\nASLO-Plankton 22 743 3300\nSharks 14 743 749\nWildFish200 200 7929 3523\nSi et al. 10.3389/fmars.2023.1174347\nFrontiers inMarine Science frontiersin.org07\nablative experiments, the number of selected tokens decreases from\nthe ﬁrst TS transformer layer and the number in theﬁnal layer\nremains the same. As shown from theTable 4, the classiﬁcation\naccuracy is constantly improved with the increase of the number of\nlayers. The best effect is achieved when TSSA is applied to layers 8-\n12, which indicates that the model has been able to accurately locate\nthe distinguishing regions from the 8th layer, and the smaller the\nreduction of tokens between layers, the better the classiﬁcation\nperformance of the model.\n4.4.2 Impact of the number of selected tokens\nTSVT performs token selection layer by layer, and the latter\nlayer continues to select tokens based on those selected in the\nprevious layer in order to pinpoint discriminative regions\nhierarchically. In the experiments, we set a parameterp about the\nselection proportion to indicate the number of selected tokens,\nwhich is the ratio of the number of selected tokens to the number of\nall tokens. We studied the inﬂuence of the parameter p on the\nmodel, and the experiment results are shown inTable 5. Whenp is\n0.7, TSVT achieves the best performance on the three datasets. As\nthe p value increases from 0.7 to 0.9, the accuracy decreases,\nprobably because too many background tokens are not discarded,\nleading to discriminative information being mixed with interference\ninformation. When the value ofp is smaller than 0.7, the accuracy\nalso decreases, which is because the number of tokens is too small\nand too many important tokens are discarded. When the value ofp\nis smaller than 0.2, the number of selected tokens in the last layer is\nless than 1, so we did not conduct related experiments. In\nconclusion, TSVT is sensitive to the number of selected tokens.\n4.4.3 Impact of token-selective methods\nWe select important tokens according to the attention weights.\nIn this part, we select tokens randomly at layers 9-12 with the\nselection ratio p =0 :7 for comparison, which further veriﬁes the\nefﬁcacy of our selection method. The two methods of random\nselection and selection accor ding to attention weights are\nrespectively applied in TSSA for experiments. As can be seen\nfrom Table 6, the accuracy of the former method decreases by\n3.6%, 1.6%, 0.6% respectively compared with the latter method\n(ours) on the three datasets. The reason is that some important\ndistinguishing tokens are discarded in the process of random\nselection, and some tokens that interfere with classi ﬁcation\naccuracy may be selected for classiﬁcation.\n4.4.4 Visualization\nIn order to further verify the effectiveness of our method in\nlocating discriminating regions, we use Grad-CAM (Selvaraju et al.,\n2017) to visualize the attention map generated from the attention\nweights of theﬁnal layer in TSVT and compare them with ViT. As\nshown in Figure 3, for images with complex backgrounds, ViT is\neasily affected by these backgrounds and focuses on objects\nirrelevant to classiﬁcation, such as reefs and corals, while after\nexcluding these interferences, TSVT easily locates marine\norganisms and their most distinctive features, such as patterns\nand spots on the ﬁsh. Taking the image in the ﬁrst row and\ncolumn as an example, ViT considers the human head as the\ndiscriminative region, while our method can accurately use the\neffective information of the hammerhead shark’s head information\nto predict the category. In addition, for images where theﬁsh are\nvisually small due to the long shooting distance, TSVT can locate\nthe positions of the small targets more accurately, whereas ViT\nsometimes cannot achieve such high precision positioning.\n5 Conclusion\nIn this paper, in order to exclude the inﬂuence of the complex\nbackground of the seabed and accurately locate discriminating\nfeatures, we propose a novel framework called TSVT for ﬁne-\ngrained image classiﬁcation of marine organisms, which achieves\nthe best performance on the thr ee marine organism datasets\ncompared with other state-of-the-art works. We propose a local\nattention mechanism called TSSA that excludes interfering tokens.\nTABLE 5 Ablation experiments on the number of selected tokens.\np ASLO-Plankton Sharks WildFish200\n0.9 73.2 89.1 93.8\n0.8 72.9 89.4 94.4\n0.7 74.3 90.3 94.7\n0.6 72.9 89.9 94.3\n0.5 72.1 88.5 93.7\n0.4 71.3 88.1 91.1\n0.3 69.2 87.9 88.7\nTABLE 4 Ablative experiments on applying TSSA to different deep layers.\nLayers ASLO-Plankton Sharks WildFish200\n12 73.2 88.9 93.7\n11-12 73.6 89.4 94.3\n10-12 73.4 90.0 94.3\n9-12 74.3 90.4 94.7\nTABLE 3 Ablative experiments on applying TSSA to different layers.\nLayers ASLO-Plankton Sharks WildFish200\n1-4 69.5 85.7 92.5\n5-8 71.0 88.9 93.4\n9-12 74.3 90.4 94.7\nTABLE 6 . Ablative experiments on token-selective methods.\nSelection Methods ASLO-Plankton Shark WildFish200\nrandom 70.7 88.8 94.1\nmax 74.3 90.4 94.7\nSi et al. 10.3389/fmars.2023.1174347\nFrontiers inMarine Science frontiersin.org08\nEach discriminating token interacts with other discriminating\ntokens in the local area to extract positiveﬁne-grained features to\nthe greatest extent. Then, we explore the impact of applying TSSA\nto different layers, the number of selected tokens and token-selective\nmethods on the performance of TSVT.\nHowever, we still select key tokens through attention weights,\nwhich has the limitation that it must be applied to deep layers to\nensure the reliability of the selection. Meanwhile, the number of key\ntokens in each image is not the same, so selecting tokens through\nmore effective learning methods as well as setting learnable\nparameters to control the number of selected tokens is the\nfuture direction.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material. Further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nGS, YW, and XW designed the study and wrote the draft of the\nmanuscript with contributions from YX and BW. BW and LB\ncollected the marineﬁsh image datasets. YW and XW devised the\nmethod. GS and YX performed the experiments. All authors\ncontributed to the experimental analysis and manuscript writing. All\nauthors contributed to the article and approved the submitted version.\nFunding\nThis work was supported by the National Natural\nScience Foundation of China (No. 32073029) and the Key\nProject of Shandong Provincial Natural Science Foundation\n(No. ZR2020KC027).\nAcknowledgments\nWe thank the Intelligent Information Sensing and Processing\nLab at Ocean University of China for their computing servers and\ncollaboration during experiments. We kindly thank the Editor Dr.\nXuemin Cheng for her efforts to handle this manuscript and all the\nreviewers for their constructiv e suggestions that helped us to\nimprove our present manuscript.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nFIGURE 3\nVisualization results on marine biological datasets, in which theﬁrst and fourth rows are six images in Sharks and WildFish datasets, the second and\nﬁfth rows are visualization of six images in the two datasets on ViT, and the third and sixth rows are visualization on TSVT.\nSi et al. 10.3389/fmars.2023.1174347\nFrontiers inMarine Science frontiersin.org09\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAlsmadi, M. K., Omar, K. B., Noah, S. A., Almarashdeh, I., Al-Omari, S., Sumari, P.,\net al. (2010). Fish recognition based on robust features extraction from size and shape\nmeasurements using neural network. Comput. Sci. 4, 1085– 1091. doi: 10.3844/\njcssp.2010.1088.1094\nAlsmadi, M. K., Omar, K. B., Noah, S. A., et al. (2011). Fish classiﬁcation based on\nrobust features extraction from color signature using back-propagation classi ﬁer.\nComput. Sci.4, 52– 58. doi:10.3844/jcssp.2011.52.58\nBranson, S., Van Horn, G., Belongie, S., and Perona, P. (2014). Bird species\ncategorization using pose normalized deep convolutional nets. in.Br. Mach. Vision\nConference. 2, 1– 14.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S.\n(2020). End-to-end object detection with transformers. in.Eur. Conf. Comput. Vision.\n2, 213– 229. doi:10.1007/978-3-030-58452-8_13\nChang, D., Ding, Y., Xie, J., Bhunia, A. K., Li, X., Ma, Z., et al. (2020). The devil is in\nthe channels: mutual-channel loss forﬁne-grained image classiﬁcation. IEEE Trans.\nImage Process.4 (8), 4683– 4695. doi:10.1109/TIP.2020.2973812\nChen, T., Wang, N., Wang, R., Zhao, H., and Zhang, G. (2021). One-stage CNN\ndetector-based benthonic organisms detection with limited training dataset.Neural\nNetworks 4, 247– 259. doi:10.1016/j.neunet.2021.08.014\nCui, J., Wei, B., Wang, C., Yu, Z., Zheng, H., Zheng, B., et al. (2018). Texture and\nshape information fusion of convolutional neural network for plankton image\nclassiﬁcation. in.OCEANS. 5, 1– 5. doi:10.1109/OCEANSKOBE.2018.8559156\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,\net al. (2021).“An image is worth 16x16 words: transformers for image recognition at\nscale,” in In International Conference on Learning Representations, Vol. 2, 4. 1– 22.\nDubey, A., Gupta, O., Raskar, R., and Naik, N. (2018). Maximum-entropyﬁne\ngrained classiﬁcation. in.Adv. Neural Inf. Process. Systems.4, 1– 11.\nFu, J., Zheng, H., and Mei, T. (2017).“Look closer to see better: recurrent attention\nconvolutional neural network for ﬁne-grained image recognition. in, ” in IEEE\nConference on Computer Vision and Pattern Recognition, 3, 4438– 4446.\nGao, Y., Beijbom, O., Zhang, N., and Darrell, T. (2016).“Compact bilinear pooling.\nin,”\nin IEEE Conference on Computer Vision and Pattern Recognition, Vol. 4. 317– 326.\nGe, W., Lin, X., and Yu, Y. (2019).“Weakly supervised complementary parts models\nfor ﬁne-grained image classiﬁcation from the bottom up. in,” in IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2 (3), 3034– 3043.\nGomez Chavez, A., Ranieri, A., Chiarella, D., Zereik, E., Babić , A., and Birk, A.\n(2019). CADDY underwater stereo-vision dataset for human– robot interaction (HRI)\nin the context of diver activities.Mar. Sci. Eng.5, 1– 14. doi:10.3390/jmse7010016\nGuo, Z., Gu, Z., Zheng, B., Dong, J., and Zheng, H. (2022).“Transformer for image\nharmonization and beyond,” in IEEE Transactions on Pattern Analysis and Machine\nIntelligence,2 ,\nHe, J., Chen, J.-N., Liu, S., Kortylewski, A., Yang, C., Bai, Y., et al. (2022).“TransFG: a\ntransformer architecture for ﬁne-grained recognition,” in In AAAI Conference on\nArtiﬁcial Intelligence,4( 6– 8), 852– 860.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016).“Deep residual learning for image recognition.\nin,” in IEEE Conference on Computer Vision and Pattern Recognition,2 ,7 7 0–778.\nHu, Y., Jin, X., Zhang, Y., Hong, H., Zhang, J., He, Y., et al. (2021).“RAMS-trans:\nrecurrent attention multi-scale transformer forﬁne-grained image recognition,” in In\nACM International Conference on Multimedia, 4 (8), 4239– 4248.\nHu, J., Zhou, C., Zhao, D., Zhang, L., Yang, G., and Chen, W. (2020). A rapid, low-\ncost deep learning system to classify squid species and evaluate freshness based on\ndigital images.Fisheries Res.4, 1– 10. doi:10.1016/j.ﬁshres.2019.105376\nJackson, D. A. (1993). Stopping rules in principal components analysis: a\ncomparison of heuristical and statistical approaches. Ecology 4, 2204– 2214. doi:\n10.2307/1939574\nKartika, D. S. Y., and Herumurti, D. (2016).“Koi ﬁ\nsh classiﬁcation based on HSV\ncolor space, ” in In International Conference on Information Communication\nTechnology and Systems, Vol. 5. 96– 100.\nKhosla, A., Jayadevaprakash, N., Yao, B., and Li, F.-F. (2011). Novel dataset forﬁne-\ngrained image categorization:stanford dogs. in.CVPR Workshop Fine-Grained Visual\nCategorization. 2, 1– 2.\nKrause, J., Stark, M., Deng, J., and Fei-Fei, L. (2013).“3D object representations for\nﬁne-grained categorization. in,” in IEEE International Conference on Computer Vision,\n2, 554– 561.\nLi, J., Xu, W., Deng, L., Xiao, Y., Han, Z., and Zheng, H. (2022). Deep learning for\nvisual recognition and detection of aquatic animals: a review.Rev. Aquaculture2, 1– 24.\ndoi: 10.1111/raq.12726\nLi, J., Xu, C., Jiang, L., Xiao, Y., Deng, L., and Han, Z. (2019). Detection and analysis\nof behavior trajectory for sea cucumbers based on deep learning.IEEE Access2, 18832–\n18840. doi:10.1109/ACCESS.2019.2962823\nLin, T.-Y., RoyChowdhury, A., and Maji, S. (2015).“Bilinear CNN models forﬁne-\ngrained visual recognition,” in IEEE International Conference on Computer Vision, Vol.\n2( 4– 8), 1449– 1457.\nLin, D., Shen, X., Lu, C., and Jia, J. (2015).“Deep LAC: deep localization, alignment\nand classiﬁcation for ﬁne-grained recognition,” in In IEEE Conference on Computer\nVision and Pattern Recognition, 3, 1666– 1674.\nLiu, C., Xie, H., Zha, Z.-J., Ma, L., Yu, L., and Zhang, Y. (2020).“Filtration and\ndistillation: enhancing region attention forﬁne-grained visual categorization. in,” in\nAAAI Conference on Artiﬁcial Intelligence, 2 (3), 11555– 11562.\nLiu, P., Zhang, C., Qi, H., Wang, G., and Zheng, H. (2022). “Multi-attention\nDenseNet: a scattering medium imaging optimization framework for visual data pre-\nprocessing of autonomous driving systems, ” in IEEE Transactions on Intelligent\nTransportation Systems, 2, 25396– 25407.\nLu, H., Li, Y., Uemura, T., Ge, Z., Xu, X., He, L., et al. (2018). FDCNet:ﬁltering deep\nconvolutional network for marine organism classiﬁcation. Multimedia Tools Appl.4,\n21847– 21860. doi:10.1007/s11042-017-4585-1\nMathur, M., Vasudev, D., Sahoo, S., Jain, D., and Goel, N. (2020). ). crosspooled\nﬁshnet: transfer learning basedﬁsh species classiﬁcation model.Multimedia Tools Appl.\n5, 31625– 31643. doi:10.1007/s11042-020-09371-x\nPrasenan, P., and Suriyakala, C. (2023). Novel modi ﬁed convolutional neural\nnetwork and FFA algorithm for ﬁsh species classi ﬁcation. Combinatorial\nOptimization 4, 1– 23. doi:10.1007/s10878-022-00952-0\nPrasetyo, E., Suciati, N., and Fatichah, C. (2022). Multi-level residual network vggnet\nfor ﬁsh species classiﬁcation. King Saud Univ. - Comput. Inf. Sci.5, 5286– 5295. doi:\n10.1016/j.jksuci.2021.05.015\nQi, Q., Li, K., Zheng, H., Gao, X., Hou, G., and Sun, K. (2022). SGUIE-net: semantic\nattention guided underwater image enhancement with multi-scale perception.IEEE\nTrans. Image Process.4, 6816– 6830. doi:10.1109/TIP.2022.3216208\nQin, H., Li, X., Liang, J., Peng, Y., and Zhang, C. (2016). DeepFish: accurate\nunderwater live ﬁsh recognition with a deep architecture.Neurocomputing 4, 49– 58.\ndoi: 10.1016/j.neucom.2015.10.122\nSelvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D.\n(2017). “Grad-cam: visual explanations from deep networks via gradient-based\nlocalization,” in In IEEE International Conference on Computer, 10, 618– 626.\nShi, Z., Guan, C., Li, Q., Liang, J., Cao, L., Zheng, H., et al. (2022).“Detecting marine\norganisms via joint attention-relation learning for marine video surveillance,” in IEEE\nJournal of Oceanic Engineering, 2, 959– 974.\nSimonyan, K., and Zisserman, A. (2015).“Very deep convolutional networks for\nlarge-scale image recognition. in, ” in International Conference on Learning\nRepresentations,3 ,1– 14.\nSosik, H. M., and Olson, R. J. (2007). Automated taxonomic classi ﬁcation of\nphytoplankton sampled with imaging-in-ﬂow cytometry. Limnology Oceanography:\nMethods 8, 204– 216. doi:10.4319/lom.2007.5.204\nSun, M., Yuan, Y., Zhou, F., and Ding, E. (2018). “Multi-attention multi-class\nconstraint for ﬁne-grained image recognition. in, ” in European Conference on\nComputer Vision, 2, 805– 821.\nVan Horn, G., Branson, S., Farrell, R., Haber, S., Barry, J., Ipeirotis, P., et al. (2015).\n“Building a bird recognition app and large scale dataset with citizen scientists: theﬁne\nprint in ﬁne-grained dataset collection. in,” in IEEE Conference on Computer Vision\nand Pattern Recognition, 1, 595– 604.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. in.Adv. Neural Inf. Process. Systems.4 (8), 1– 11.\nWang, N., Chen, T., Liu, S., Wang, R., Karimi, H. R., and Lin, Y. (2023b). Deep\nlearning-based visual detection of marine organisms: a survey.Neurocomputing 1– 32,\n4. doi:10.1016/j.neucom.2023.02.018\nWang, H., Sun, S., Bai, X., Wang, J., and Ren, P. (2023a).“A reinforcement learning\nparadigm of conﬁguring visual enhancement for object detection in underwater\nscenes,” in IEEE Journal of Oceanic Engineering,4 ,1– 19.\nSi et al. 10.3389/fmars.2023.1174347\nFrontiers inMarine Science frontiersin.org10\nWang, N., Wang, Y., and Er, M. J. (2022). Review on deep learning techniques for\nmarine object recognition: architectures and algorithms.Control Eng. Pract.118, 1– 18\n2. doi:10.1016/j.conengprac.2020.104458\nW a n g ,J . ,Y u ,X . ,a n dG a o ,Y .( 2 0 2 1 ) .F e a t u r ef u s i o nv i s i o n\ntransformer for ﬁne-grained visual categorization. arXiv preprint\narXiv :2107.02341 6.\nWei, X.-S., Xie, C.-W., Wu, J., and Shen, C. (2018). Mask-CNN: localizing parts and\nselecting descriptors forﬁne-grained bird species categorization.Pattern Recognition\n704– 714, 2. doi:10.1016/j.patcog.2017.10.002\nYang, Z., Luo, T., Wang, D., Hu, Z., Gao, J., and Wang, L. (2018).“Learning to\nnavigate forﬁne-grained classiﬁcation,” in European Conference on Computer Vision,4\n(8), 420– 435.\nYu, C., Zhao, X., Zheng, Q., Zhang, P., and You, X. (2018).“Hierarchical bilinear\npooling for ﬁne-grained visual recognition,” in European Conference on Computer\nVision, 2 (4), 574– 589.\nZhang, N., Donahue, J., Girshick, R., and Darrell, T. (2014).“Part-based r-CNNs for\nﬁne-grained category detection,” in European Conference on Computer Vision, 2 (3),\n834– 849.\nZhang, Z., Du, X., Jin, L., Wang, S., Wang, L., and Liu, X. (2022). Large-Scale\nunderwater ﬁsh recognitionvia deep adversarial learning.Knowledge Inf. Syst.4, 353–\n379. doi:10.1007/s10115-021-01643-8\nZ h e n g ,H . ,F u ,J . ,M e i ,T . ,a n dL u o ,J .( 2 0 1 7 ) .“Learning multi-attention\nc o n v o l u t i o n a ln e u r a ln e t w o r kf o rﬁne-grained image recognition, ” in IEEE\nInternational Conference on Computer Vision, 3, 5209– 5217.\nZheng, H., Fu, J., Zha, Z.-J., and Luo, J. (2019).“Looking for the devil in the details:\nlearning trilinear attention sampling network forﬁne-grained image recognition,” in\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 8, 5012– 5021.\nZheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., et al. (2021).“Rethinking\nsemantic segmentation from a sequence-to-sequence perspective with transformers,” in\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2, 6881– 6890.\nZhou, J., Sun, J., Zhang, W., and Lin, Z. (2023a). Multi-view underwater image\nenhancement methodvia embedded fusion mechanism.Eng. Appl. Artif. Intell.4, 1– 12.\ndoi: 10.1016/j.engappai.2023.105946\nZhou, J., Yang, T., Chu, W., and Zhang, W. (2022). Underwater image restoration\nvia backscatter pixel prior and color compensation.Eng. Appl. Artif. Intell.4, 1– 16. doi:\n10.1016/j.engappai.2022.104785\nZhou, J., Zhang, D., and Zhang, W. (2023b). Cross-view enhancement network for\nunderwater images. E n g .A p p l .A r t i f .I n t e l l .4, 1 – 11. doi: 10.1016/\nj.engappai.2023.105952\nZhuang, P., Wang, Y., and Qiao, Y. (2018).“WildFish: a large benchmark forﬁsh\nrecognition in the wild,” in ACM International Conference on Multimedia, 8, 1301–\n1309. doi:10.1016/j.engappai.2023.105952\nSi et al. 10.3389/fmars.2023.1174347\nFrontiers inMarine Science frontiersin.org11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7938826084136963
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6645050048828125
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6102232933044434
    },
    {
      "name": "Security token",
      "score": 0.5186490416526794
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.517171323299408
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5061776041984558
    },
    {
      "name": "Transformer",
      "score": 0.4247116148471832
    },
    {
      "name": "Computer vision",
      "score": 0.3622550964355469
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I59028903",
      "name": "Ocean University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4210116869",
      "name": "Affiliated Hospital of Qingdao University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I108688024",
      "name": "Qingdao University",
      "country": "CN"
    }
  ]
}