{
    "title": "Injecting Numerical Reasoning Skills into Language Models",
    "url": "https://openalex.org/W3015339775",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A4221786267",
            "name": "Geva, Mor",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2184172997",
            "name": "Gupta, Ankit",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221504776",
            "name": "Berant, Jonathan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2919420119",
        "https://openalex.org/W2971863715",
        "https://openalex.org/W2475046758",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2995359496",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2983984338",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2951434086",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2970609357",
        "https://openalex.org/W2970742161",
        "https://openalex.org/W2951939640",
        "https://openalex.org/W2986266667",
        "https://openalex.org/W3017541546",
        "https://openalex.org/W2912007050",
        "https://openalex.org/W2984812384",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2276364082",
        "https://openalex.org/W2427527485",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2105717194",
        "https://openalex.org/W2560647685",
        "https://openalex.org/W2903190877",
        "https://openalex.org/W2995628494"
    ],
    "abstract": "Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 $\\rightarrow$ 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946–958\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n946\nInjecting Numerical Reasoning Skills into Language Models\nMor Geva∗\nTel Aviv University,\nAllen Institute for AI\nmorgeva@mail.tau.ac.il\nAnkit Gupta∗\nTel Aviv University\nankitgupta.iitkanpur@gmail.com\nJonathan Berant\nTel Aviv University,\nAllen Institute for AI\njoberant@cs.tau.ac.il\nAbstract\nLarge pre-trained language models (LMs) are\nknown to encode substantial amounts of lin-\nguistic information. However, high-level rea-\nsoning skills, such as numerical reasoning, are\ndifﬁcult to learn from a language-modeling\nobjective only. Consequently, existing mod-\nels for numerical reasoning have used special-\nized architectures with limited ﬂexibility. In\nthis work, we show that numerical reasoning\nis amenable to automatic data generation, and\nthus one can inject this skill into pre-trained\nLMs, by generating large amounts of data,\nand training in a multi-task setup. We show\nthat pre-training our model, G ENBERT, on\nthis data, dramatically improves performance\non DROP ( 49.3 → 72.3 F1), reaching per-\nformance that matches state-of-the-art mod-\nels of comparable size, while using a sim-\nple and general-purpose encoder-decoder ar-\nchitecture. Moreover, G ENBERT generalizes\nwell to math word problem datasets, while\nmaintaining high performance on standard RC\ntasks. Our approach provides a general recipe\nfor injecting skills into large pre-trained LMs,\nwhenever the skill is amenable to automatic\ndata augmentation.\n1 Introduction\nRecently, models trained on large amounts of data\nwith a language modeling (LM) objective, have\nshown great promise in natural language process-\ning, exhibiting surprising amounts of knowledge\nand information (Peters et al., 2018; Devlin et al.,\n2019; Liu et al., 2019; Lan et al., 2019; Petroni\net al., 2019; Hewitt and Manning, 2019). However,\nhigh-level skills, such as the ability to perform nu-\nmerical reasoning over text, can be challenging to\ncapture with a LM objective only. Consider the ex-\nample in Table 1. To solve the ﬁrst question (Q1),\na model must capture the value of numbers in the\n∗These authors contributed equally.\n (b) ﬁne-tuning\npre-trained \nLM\nnumerical \nreasoning\nreading \ncompr.\nNumerical Data (ND)\nQ: -1088.5 + 1337 + 90.6 ? \nA: 339.1\nQ: -1088.5 + 1337 + 90.6 ? \nA: 339.1\nQ: -1088.5 + 1337 + 90.6 ? \nA: 339.1\nTextual Data (TD)\nC: The king had 801 \nsoldiers and 109 citizens. \nThe commander received \n609 soldiers from the king\nQ: -How many more \nsoldiers did the king have \nthan citizens?\nA: 192-109=83\npre-trained \nLM+ND\n C: The king had 801 \nsoldiers and 109 citizens. \nThe commander received \n609 soldiers from the king\nQ: -How many more \nsoldiers did the king have \nthan citizens?\nA: 192-109=83\nC: The king had 801 \nsoldiers and 109 citizens. \nThe commander received \n609 soldiers from the king\nQ: How many more \nsoldiers did the king have \nthan citizens?\nA: 83 (192-109)\npre-trained \nLM+ND+TD\n(a) pre-training\ngenerative\nextractive\ngenerative\nextractive\ngenerative\nextractive\npre-trained \nLM+ND+TD\n generative\nextractive\n pre-trained \nLM+ND+TD\nextractive\nDROP , \nMAWPS\n SQuAD\nFigure 1: An overview of our approach for injecting numeri-\ncal skills into a pre-trained LM. (a) We add two pre-training\nsteps over large amounts of synthetic numerical data (ND)\nand textual data (TD); (b) we further ﬁne-tune the model over\neither numerical reasoning datasets ( DROP , MAWPS ) or\nreading comprehension datasets (SQUAD).\ntext, compute their difference, and generate the to-\nkens corresponding to the result, which generally\ndo not appear in the input passage.\nTo make the task more manageable, state-of-the-\nart models have employed specialized architectures,\nrestricting the space of possible numerical compu-\ntations to a limited set. Modules were designed\nfor counting (but only until ‘9’) and for addition\nand subtraction (but of 2-3 numbers only). Such\nmodels perform well on existing datasets, such as\nDROP (Dua et al., 2019), but do not generalize to\nunsupported computations, which will require mod-\nifying the model architecture. Moreover, current\nmodels marginalize at training time over all numer-\nical expressions that evaluate to the correct answer.\nSince the number of such expressions grows ex-\nponentially, scaling these approaches to arbitrary\ncomputations entails using non-differentiable op-\nerations (sampling or computing top-Knumerical\nexpressions), which can lead to training difﬁculties.\n947\nPassage: Taunton has four art galleries... Hughes/\nDonahue Galleryfounded in 2007, a local community\ngallery serving local Taunton artists... Art Euphoric\nfounded in 2008 has both visual and craft exhibits...\nQ1: How many years afterfounding of Hughes/ Don-\nahue was Art Euphoricfounded?\nA1: 1 (number)\nQ2: Which gallery was founded later, Hughes/ Don-\nahue or Art Euphoric?\nA2: Art Euphoric (span)\nTable 1: Example passage from DROP , and two questions\nwith different answer types.\nIn this work, we propose that reasoning skills,\nsuch as numerical reasoning, are amenable to auto-\nmatic data generation. Hence, one can inject that\nskill directly into the model by adding additional\npre-training steps, allowing the model to learn the\nskill in an end-to-end fashion. This results in a\nfully-differentiable training procedure over a stan-\ndard and general-purpose architecture, where the\noutput space can be easily controlled through the\ndata generation procedure.\nSpeciﬁcally (Figure 1), we add to a large\npre-trained LM two pre-training steps over\nautomatically-generated synthetic data. First, we\ngenerate numerical data of the form 3 + 4 + 11 =\n18. Training on this data teaches the model to com-\npute the value of numbers from their tokens and\nto perform numerical operations. Second, we au-\ntomatically generate question-passage pairs that\nrequire numerical reasoning using a compact gram-\nmar (textual data). Training on this data endows the\nmodel with the ability to understand computations\nexpressed in pseudo-natural language.\nIn both pre-training steps, the model, GEN-\nBERT, generates output numbers token-by-token.\nThus, the model has a standard architecture, where\nan answer can either be extracted from the input\nquestion and passage or generated from a decoder.\nPre-training is done in a multi-task setup with a\nstandard LM objective, in order to avoid “catas-\ntrophic forgetting” (Kirkpatrick et al., 2017) of the\nlinguistic information in the original LM. After\npre-training, the model has sufﬁcient language and\nnumerical skills to be directly ﬁne-tuned on a target\nnumerical reasoning dataset, without resorting to\nspecialized architectures. Augmenting more nu-\nmerical skills does not require changing the model,\nonly generating additional data.\nWe demonstrate the validity of our approach by\na series of experiments showing that:\n(a) GENBERT is able to solve pre-training tasks\nfor numerical reasoning.\n(b) Pre-training on these tasks provides GEN-\nBERT with 1) skills to reach performance\nthat matches state-of-the-art models of com-\nparable size on DROP (Dua et al., 2019), a\nstandard numerical reasoning dataset, as well\nas 2) the ability to generalize to math word\nproblem (MWP) datasets (Koncel-Kedziorski\net al., 2016).\n(c) GENBERT learns these numerical skills while\nmaintaining high performance on SQuAD (Ra-\njpurkar et al., 2016), a standard reading com-\nprehension dataset.\n(d) Initializing models for numerical reasoning\nwith GENBERT’s weights improves their orig-\ninal performance.\nTo conclude, in this work we address the prob-\nlem of injecting LMs with numerical reasoning\nskills. Our contributions are:\n•A method for injecting skills into pre-trained\nLMs, given that automatic data generation is\npossible.\n•GENBERT, an architecture for pre-trained LM\nwith generative and extractive abilities.\n•A framework for generating numerical and tex-\ntual synthetic data for numerical reasoning.\nOur code and data can be downloaded\nfrom https://github.com/ag1988/\ninjecting_numeracy.\n2 Numerical Reasoning Over Text\nNumerical reasoning over text (NRoT) is com-\nmonly set up as a reading comprehension (RC) task.\nGiven a training set of question-context-answer\ntriples {(qi,ci,ai)}N\ni=1, the goal is to learn a func-\ntion that returns the answer ato a question q given\na context c. However, in NRoT the answer gener-\nally requires to internally perform some numerical\ncomputation using the entities and numbers in the\ncontext. Speciﬁcally, the answer is either: (a) a\nspan (or list of spans) from the context c or ques-\ntion q, or (b) a number that is the result of some\ncomputation (see examples in Table 1).\nTwo natural, yet opposing, approaches lend\nthemselves to tackling NRoT: (a) A symbolic ap-\nproach: a model can read the question and context,\noutput a numerical expression and evaluate the an-\nswer with an external symbolic calculator. This\napproach is a particular case of semantic parsing\n(Kamath and Das, 2019), and was common in early\nNRoT datasets (Koncel-Kedziorski et al., 2015;\nRoy and Roth, 2015; Hosseini et al., 2014). How-\n948\never, it suffers from several drawbacks. First, be-\ncause numerical expressions are discrete and their\nspace grows combinatorially, the model must learn\nto search in this space using non-differentiable op-\nerations, which are usually difﬁcult to optimize.\nSecond, numerical expressions are limited to nu-\nmerical answers, while in DROP often a numerical\ncomputation is required but the ﬁnal answer is a\ntext span. (b) A distributed approach: have a model\ndirectly generate the answer given (q,c). When\nthe answer is a text span, the model can extract\nit from the input, and when the answer is a num-\nber that is not in q or c, the model must generate\nit. While this makes training straightforward, the\nmodel must learn to perform numerical computa-\ntions from the relatively small target dataset. We\nempirically show in §3 that this leads to low per-\nformance in general.\nAs a compromise, most NRoT models (Dua\net al., 2019; Kinley and Lin, 2019; Hu et al., 2019;\nEfrat et al., 2019) have taken a hybrid approach:\nthey augment standard extractive QA models with\nspecialized modules for handling a limited set of\nnumerical computations. We brieﬂy describe this\narchitecture, as it is the basis for our model in §3.\nGiven a question with n1 tokens q =\n(q1,...,q n1 ) and a context with n2 tokens c =\n(c1,...,c n2 ), the hybrid model ﬁrst computes con-\ntextualized representations for the n1 + n2 + 3\ntokens ⟨[CLS] q [SEP] c[SEP]⟩using a pre-\ntrained LM, such as BERT (Devlin et al., 2019):\nL = LM(q,c).\nThe representations L are then passed to multiple\nheads, which are small neural networks that esti-\nmate p(a |q,c,h), that is, the probability of the\nanswer given the input and conditioned on a head\nh, corresponding to a particular answer type:\n•Context span head: computes a distribution over\nall spans in the context using a feed-forward net-\nwork (FFN) FFc(L).\n•Question span head : computes a distribution\nover spans in the question using a FFN FFq(L).\n•Count head: computes a distribution over the\nnumbers {0,..., 9}using a FFN FFcnt(L).\n•Arithmetic head: computes a distribution over\nall signed combinations of numbers in the con-\ntext using a FFN FFcmb(L) (the numbers in the\ncontext are identiﬁed in a pre-processing step).\nWhile the ﬁrst two heads are standard in extractive\nQA, the latter two heads are specialized and meant\nto handle answers that do not appear in the input.\nFinally, for deciding which answer head to use\nfor a given input, a type head FFtyp(L) outputs\na probability distribution phead(h |q,c) (using a\nFFN). Thus the model probability for an answer is\np(a|q,c) =\n∑\nh∈heads\nphead(h |c,q) ·p(a|c,q,h).\nTraining is done by enumerating all of the ways in\nwhich the answer can be obtained using all of the\nheads, and maximizing this marginal probability.\nWhile existing models perform well on DROP ,\nthe aforementioned architecture is not ﬂexible.\nFirst, the output space is severely constrained –\nthe model can only count up to ‘9’, and numerical\ncomputations are restricted to signed combinations\nof a few numbers. Second, expanding the space\nof supported numerical computations is non-trivial,\nbecause training involves marginalizing over all\nexpressions that lead to the correct answer. Since\nthe space of numerical expressions grows expo-\nnentially, expanding this space quickly leads to\na difﬁcult search problem. Third, delegating nu-\nmerical computations to an external symbolic cal-\nculator leads to modeling challenges, since there\ncould be interactions between text and numerical\ncomputation: Consider the DROP question “How\nmany total yards did Phil Dawson throw for touch-\ndowns?”. Current models handle such questions\nby computing a sum from numbers in the text and\nreturning the result. However, if the question was\n“Who threw 45 total yards for touchdowns?” , the\nmodel would have to compute the sum internally,\nand then ﬁnd the relevant span in the text. This is\nimpossible when the computation itself is delegated\nto an external calculator. Thus, training models to\nhandle such numerical questions is desirable.\nMotivated by the above arguments, we wish to\npush the frontier of end-to-end differentiable mod-\nels for numerical reasoning. Thus, we will automat-\nically generate large amounts of data that endow a\npre-trained LM with numerical skills.\n3 G ENBERT: A BERT-based Model for\nGenerating Arbitrary Outputs\nWe now describe a simple BERT-based genera-\ntive model that performs numerical computations\ninternally, termed GENBERT. The model com-\nbines the Transformer encoder-decoder architec-\nture (Vaswani et al., 2017) with a pre-trained LM,\nspeciﬁcally, BERT.\nOur architecture is illustrated in Figure 2. Our\nencoder is a standard Transformer, initialized with\n949\nBERT weights. To also enjoy BERT’s represen-\ntations at decoding time, we tie the weights of the\ndecoder and the encoder. Because the Transformer\ndecoder has source attention weights (weights for\nattending to the encoder representations at decod-\ning time) that are not present in BERT, we tie\nthese source-attention weights to the self-attention\nweights of the encoder (which are tied to the self-\nattention weights of the decoder). This fully initial-\nizes the Transformer model with BERT weights.\nSince the encoder and decoder weights are tied,\nwe make them learn distinct representations by\nadding a FFN FFenc that transforms the encoder\ncontextualized representations Lenc as\nHenc = layer-norm(gelu(W ·Lenc)),\nwhere W is a parameter matrix (Hendrycks and\nGimpel, 2016; Ba et al., 2016). Analogously,\nwe add FFdec to the decoder. To further dis-\ntinguish the encoder and decoder, we use dis-\ntinct start and end tokens for input and out-\nput sequences. Given m answer tokens a =\n(a1,...,a m), we form an output sequence with\nm+ 2tokens: ⟨[SOS] a [EOS]⟩. The output\ntokens are passed through the decoder and FFdec\nto obtain Hdec.\nFinally, the probability of an answer is deﬁned in\nthe usual manner: Let ⟨a⟩= (a0 ···am+1) be the\noutput sequence. The decoder outputs the probabil-\nity pdec(ai+1 |a0,..ai,c,q), and the probability of\nan answer is:\npdec(⟨a⟩| c,q) =\nm∏\ni=0\npdec(ai+1 |a0,..ai,c,q).\nAs we have a generative model, we can remove\nthe specialized count and arithmetic heads from\n§2. Thus, the type head FFtyp(Henc) outputs a\ndistribution (pq,pc,pdec) over the context span,\nquestion span, and decoder heads.\nTo improve pre-training on the numeric data (§4),\nwe make two additional modiﬁcations.\nDigit Tokenization (DT) Conventional word-\npiece tokenization treats numbers no differently\nthan any other token. However, computing the\nvalue of numbers should be simpler when using\ndigits directly (Wallace et al., 2019). Hence, we\ntokenize numbers digit-by-digit. For example, a\nwordpiece ##d1 ···dk where di ∈{0,...,9} is\nfurther split into ##d1, ..., ##dk. We show in §5.1\nthat this substantially improves sample complexity\nwhen training to perform numerical operations.\nFFenc\nHenc\nLenc\nFFdec\nHdec\n q1\nc1\neCLS\nen\neSEP\ne1\np1\nq1\nam-1\nam\na1\na1\nam\na2\nHenc\npc\npdec\npq\n(b) GenBERT’s decoder(a) GenBERT\nFigure 2: GENBERT’s network architecture: (a) a high-level\noverview of the network, including a generative head (red),\ntwo span-extraction heads (yellow), and an answer type head.\n(b) a closer overview of GENBERT’s generative head.\nRandom Shift (RS) The original Transformer\nuses absolute positional embeddings for each to-\nken. However, in §4, we train on short inputs such\nas “1086.1 - 2.54 + 343.8”. Thus, the model can\npotentially over-ﬁt and learn to perform numerical\nreasoning only when numbers are at the beginning\nof an input. To prevent this, when the input length\nn1 + n2 + 3<512, we shift all position IDs by a\nrandom integer in (0,1,..., 512 −(n1 + n2 + 3)).\nTraining For each span (i,j), a span extraction\nhead houtputs its probability ph((i,j) |c,q,h) of\nbeing the answer. Let Sbe the set of spans in the\ninput corresponding to the gold answer. The model\nloss Lmodel marginalizes over all ways in which the\nanswer can be predicted:\n−log\n(\npdec·pdec(⟨a⟩) +\n∑\nh∈q,c\nph·\n∑\n(i,j)∈S\nph(i,j)\n)\n,\nwhere conditionals have been dropped for brevity.\nTo evaluate the ability of GENBERT to perform\nnumerical reasoning, we initialize it with BERT\nand ﬁne-tune it on DROP . GENBERT obtains 46.1\nEM and 49.3 F 1, roughly 20 points lower than\nprior models. Thus, we conclude that acquiring\nnumerical reasoning skills from DROP data only\nis difﬁcult. To remedy this, we will automatically\ngenerate training data that will endow GENBERT\nwith numerical skills before training it on DROP.\n950\n   Bridget        adopted       4     brown   dogs \n    CONT       VERB-POS  NUM  ATTR    ENT \n  The king     recruited   1337   Irish   soldiers \n   Jackson           scored           3        running    touchdowns .\nabstraction\nInstantiation\nwith vocab.\nFigure 3: Template extraction and instantiation. A template\n(in red) is extracted from a MWP sentence, using categories for\ncontainers, entities, verbs, attributes and numbers, according\nto Hosseini et al. (2014). For generation, the categories are\ninstantiated with a domain-speciﬁc vocabulary.\n4 Pre-training Tasks for Numerical Skills\nWe now describe two automatically-generated\ndatasets and the multi-task training procedure.\n4.1 Generating Numerical Data (ND)\nOur ﬁrst dataset focuses on learning numerical val-\nues expressed by tokens and computing numerical\noperations, i.e., it does not involve textual content.\nAs such, it is easy to craft templates that corre-\nspond to various numeric operations. We designed\nsix such templates, described in Table 2. Each tem-\nplate consists of an expression to evaluate and its\nsolution. Further details on their instantiation are\nprovided in §A.1. While the numerical operations\nwere chosen based on DROP , it is trivial to extend\nthem to other domains (Saxton et al., 2019) with\ndifferent numerical operations.\n4.2 Generating Textual Data (TD)\nNumeric data is easy to generate, since it does not\ncontain any textual context. However, to tackle\nNRoT, a model needs to comprehend how numer-\nical operations are expressed in text that refers to\nevents, entities and quantities. This primes us to\ngenerate textual data from a simple grammar.\nWhile text generation is hard in the general case,\nwe are speciﬁcally interested in text that focuses\non number manipulations. Therefore, we use the\nframework of Hosseini et al. (2014), who proposed\nto model math word problems with a simple struc-\nture. In their framework a world state consists of\nentities, which are objects that are being counted,\nand containers, which are objects that own entities.\nSentences use verb categories to describe how the\nnumber of entities in a container changes, and thus\na world state can be updated given a sentence.\nConsider the textual example in Figure 1. the\nentities are soldiers and citizens, and thecontainers\nare the king and the commander. The verbs (“had”\nand “received”) describe the entities the king holds,\nand how many were passed to the commander.\nIn this work, we use this framework to automati-\ncally generate examples. We extract templates that\ndescribe changes in the number of entities owned\nby containers, and automatically generate question-\ncontext pairs from these templates.\nTemplate extraction To extract templates, we\ngo over sentences from the corpus provided by\nHosseini et al. (2014). For each sentence, we use\na procedure described by Hosseini et al. (2014) to\nabstract its tokens to the following categories: num-\nbers (NUM), entities (ENT), containers (CONT) and\nattributes (ATTR). In addition, verbs are abstracted\nto six categories, each corresponding to a different\nchange in the number of entities owned by con-\ntainers. Thus, each template fully speciﬁes how to\nupdate a world state, i.e., the number of entities\neach container owns. The top part of Figure 3 illus-\ntrates the abstraction process. Finally, we count for\neach extracted template its frequency in the data,\nand use the top- 12 templates for passage genera-\ntion. Details on the abstraction process, categories\nused, and extracted templates are in §A.2.\nPassage generation Using the extracted tem-\nplates, we can generate sentences and maintain\na world state of all containers and the number of\nentities they own. We construct a small vocabu-\nlary (<100 words) that maps categories to domain-\nspeciﬁc words, and use the following procedure to\ngenerate passages.\nWe sample 3-6 templates with replacement, and\ninstantiate them one-by-one (the bottom part of\nFigure 3 illustrates instantiation). Each template\nis instantiated by uniformly sampling values from\nthe vocabulary with probability 1 −p and from\npreviously generated sentences with probability p.\nTo avoid a collection of unrelated sentences, we set\nthe probability of using previously used values to\np= 0.7. An example passage is shown in Table 3.\nQuestion generation After generating a passage,\nthe world state holds information about all contain-\ners in the passage and the number of entities they\nhold. In Table 3, the state will include the number\nof families and rebels of different nationalities in\neach container (the commander, the householder,\nand the countries). Based on this world state, nu-\nmerical reasoning questions can be asked.\nTo create questions, we craft 13 question tem-\nplates that are instantiated with objects from the\nworld state. The questions teach the model to track\nevents and perform numeric and discrete operations.\n951\nOperation Template Example instantiation\nsigned ﬂoat combination s1 f1 s2 f2 s3 f3 s4 f4 517.4 - 17484 - 10071.75 + 1013.21\nmin/max/avg o(f1, f2, f3, f4) largest(13.42, 115.5, 72.76)\narg max, arg min arg(w1 f1, w2 f2, w3 f3, w4 f4) arg min(highish 137.1, sightliness 43.2)\ndate min/max dsup(d1, d2, d3, d4) oldest(June 04, 959; 01 May 959)\ndate difference diff in prd(d1, d2) diff in days(05 April 112; June 01, 112)\npercentage pcent w:: w1 p1%, w2 p2%, w3 p3%, w4 p4% percent not sunbird :: sunbird 33.2%, defector\n60.77%, molehill 6.03%\nTable 2: Templates for generating synthetic numerical examples and the numerical operations required to answer them.\nDomains (deﬁned in App. A.1): si ∈{−, +}, fi ∈R+, o ∈O : superlative words like “longest”, arg ∈{arg min, arg max},\nwi ∈W : words from NTLK Words Corpus, di ∈D: dates until Sep 2019, dsup ∈DSUP : superlative words like “latest”,\nprd ∈{“days”, “months”, “years”}, pi ∈(0, 100), pcent ∈{“percent”, “percent not”}.\nP: The commander recruited 1949 Polish families in Spain.\nThe householder recruited 1996 Japanese families in Spain.\nThere were 10913 white rebels and 77 Chinese families\nin Spain. 6641 British soldiers, 476 asian rebels, and 338\nGermans families were recruited in Russia.\nQ: How many Japanese families were in Spain?\nA: 1996\nQ: How many more Japanese families were in Spain than\nPolish families?\nA: 47 (1996-1949)\nQ: How many families of Spain were not Polish families?\nA: 2073 (4022-1949)\nTable 3: An example synthetic passage (P) and questions.\nQuestions (Q) were generated from templates and answers (A)\nwere calculated based on the world state.\nExamples for generated questions are shown in Ta-\nble 3, where answers are computed from the world\nstate. Overall, we create 13 question templates for\n7 different “skills\", provided in §A.2.\n4.3 Training G ENBERT on Synthetic Data\nFor pre-training on ND, we generated 1M exam-\nples for training and 10K for validation. For TD,\nwe generated 2.5M examples for training and 10K\nfor validation. For both synthetic datasets, we used\nthe GENBERT model loss, Lmodel, from §3. To\nensure that the model does not lose its language\nunderstanding abilities, we employ a multi-task\nsetup, and include a standard masked LM objective\nfrom BERT. Speciﬁcally, given a masked token\nsequence ⟨m⟩, we compute the contextualized rep-\nresentations, Lenc and pass them through a feed-\nforward network FFmlm. For each masked index\ni, it outputs the probability p(ai |i,⟨m⟩) of the\noriginal token ai. The MLM loss is computed as\nLmlm(⟨m⟩) = meani∈masked−log(p(ai |i,⟨m⟩)).\nDetails about the MLM data are in §A.3.\nDuring training, we sample mini-batches from\nthe respective datasets, and minimize the weighted\nsum of the losses. Concretely, while pre-training\non ND and TD, we sample mini-batchesXND, XTD\nand XMLM and optimize the objective\nLmodel(XND) +Lmodel(XTD) +λ·Lmlm(XMLM).\n0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80\ntraining step (thousands)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0evaluation accuracy\ndata (setting)\nND (ND)\nTD (TD)\nND (ND-LM)\nND (ND-LM-RS)\nND (ND-LM-DT)\nTD (ND+TD)\nFigure 4: Progression of eval accuracy (EM) of GENBERT,\nfor different pre-training settings listed in §5.1.\n5 Experimental Evaluation\nWe now evaluate our two pre-training steps and\ntheir applicability for numerical reasoning tasks.\nWe consider the following variants, aiming to in-\nvestigate the contributions of ND and TD, the im-\nportance of MLM loss, and techniques like DT\nand RS. In all cases, we initialize GENBERT with\nBERT-base-uncased, use DT and RS, and include\nthe MLM loss, except where noted:\n•GENBERT +ND : trained on numerical data.\n•GENBERT +ND-LM : trained on ND without the\nadditional MLM loss.\n•GENBERT +ND-LM-DT : trained on ND using\nwordpiece tokenization, without the MLM loss.\n•GENBERT +ND-LM-RS : trained on ND without\nMLM loss and random shift (RS).\n•GENBERT +TD : trained on textual data (TD).\n•GENBERT +ND+TD : GENBERT +ND trained on\nboth ND and TD.\n5.1 Pre-training Performance\nWe ﬁrst ask whether the pre-training procedure al-\nlows GENBERT to absorb the intended numerical\nskills. We observe that across various settings (ND,\nTD, ND+TD), GENBERT consistently achieves\nmore than 96% accuracy in predicting the correct\nsolution for both ND and TD. Thus, we conclude\nthat indeed a pre-trained LM can learn the designed\nskills from generated data.\n952\nFigure 4 shows the learning curves of GEN-\nBERT for the different variants. Note that in ND-\nLM-DT the model does not learn to solve the nu-\nmerical data task. This demonstrates the utility\nof using DT over conventional wordpieces. The\nlower sample complexity in the case of ND+TD\ncompared to the only-TD can be attributed to the\nfact that ND and TD share some numeric skills and\nhence a model already trained on ND converges\nfaster on TD compared to GENBERT.\n5.2 Numerical Reasoning Performance\nAfter successfully injecting GENBERT with nu-\nmeric skills, we test GENBERT guided by the fol-\nlowing questions:\n(a) Are the injected skills robust and generalize to\nNRoT datasets like DROP?\n(b) Are the new skills learned at the expense of\nthe model’s ability to understand language?\n(c) Can the pre-trained weights be used with ar-\nchitectures other than GENBERT?\nFor (a), we ﬁne-tune GENBERT on DROP and\nfurther evaluate on MWP in a zero-shot setup . For\n(b), we evaluate GENBERT on a RC task that does\nnot involve numerical reasoning, namely, SQUAD\n(Rajpurkar et al., 2016). For (c), we useGENBERT\nencoder as a drop-in replacement forBERT on two\nother architectures.\nResults on DROP We report results of GEN-\nBERT initialized by BERT-base and leave pre-\ntraining a larger model for future work. We com-\npare GENBERT to MTMSN (Hu et al., 2019) ini-\ntialized with BERT-base, as MTMSN initialized\nwith BERT-large is a state-of-the-art model on\nDROP. 1\nTable 4 presents ﬁne-tuning results on DROP .\nWithout pre-training, GENBERT performs poorly\ncompared to current state of the art models like\nMTMSN , reporting an EM of only 46.1. Pre-\ntraining on each of the numerical data (ND) and\ntextual data (TD) improves performance dramati-\ncally to 64.7 EM and 64.4 EM, respectively. More-\nover, pre-training on both ND and TD leads to a\nperformance of 68.8 EM, on par with MTMSN’s\n68.2 EM. This demonstrates that the skills that\nGENBERT learns from ND and TD are com-\nplementary. In addition, the lower performance\nof GENBERT +ND-LM and GENBERT +ND-LM-RS\n1 Per ACL policy, we compare to models that were made\npublic 3 months prior to submission.\nDevelopment Test\nEM F1 EM F1\nGENBERT 46.1 49.3 - -\nGENBERT +ND-LM-RS 61.5 65.4 - -\nGENBERT +ND-LM 63.8 67.2 - -\nGENBERT +ND 64.7 68.2 - -\nGENBERT +TD 64.4 67.8 - -\nGENBERT +ND+TD 68.8 72.3 68.6 72.4\nNABERT+ 63.0 66.0 61.6 65.1\nMTMSN BASE 68.2 72.8 - -\nTable 4: Performance of GENBERT and comparable models\non the development and test sets of DROP.\nnumber span date spans\nGENBERT 42.3 67.3 47.5 21.1\nGENBERT +ND 70.5 71.0 54.5 24.2\nGENBERT +TD 69.2 72.6 55.2 22.0\nGENBERT +ND+TD 75.2 74.5 56.4 24.2\nNABERT+ 67.8 69.2 39.8 22.4\nMTMSN BASE 75.0 71.3 44.2 53.4\nTable 5: F1 scores on DROP development per answer type.\nshows the importance of including the MLM loss\nand the utility of RS for short inputs.\nBreaking down performance by answer type (Ta-\nble 5) highlights several points. First, pre-training\non ND and TD improves performance mostly due\nto number answer types, as expected. Second,\nGENBERT +ND+TD outperforms MTMSN BASE on\nquestions whose answer isa span. We argue a prob-\nable cause for this are span questions that require\nperforming a numerical computation internally, as\nexplained in §2. Third, MTMSN BASE substan-\ntially outperforms GENBERT on questions whose\nanswer is a list of non-contiguous spans. This is\nexpected, as MTMSN has a specialized head and\nprocedure for handling such questions, while build\non a simpler and more standard RC architecture.\nGeneralization to MWP (zero-shot) The\nMAWPS repository is a collection of math word\nproblem (MWP) datasets (Koncel-Kedziorski\net al., 2016). To test the models on skills they were\ntrained on, we picked datasets with addition and\nsubtraction problems, and ﬁltered out examples\nwith other operations (e.g., multiplication and\ndivision). All models that were ﬁne-tuned on\nDROP were evaluated in a zero-shot setup on 395\nexamples from ADDSUB (Hosseini et al., 2014),\n321 from SOP (Roy et al., 2015), and 305 from\nSEQ (Koncel-Kedziorski et al., 2015).\nResults are shown in Table 6. Overall,\nGENBERT +ND+TD dramatically improves perfor-\nmance compared to GENBERT. GENBERT +ND\nperforms much better than GENBERT +TD , demon-\nstrating the utility of ND when the context is short.\n953\nADDSUB SOP SEQ\nGENBERT 2 1.2 1.3\nGENBERT +ND 22.8 26.5 23\nGENBERT +TD 10.4 21.5 12.1\nGENBERT +ND+TD 22.8 28.3 22.3\nNABERT+ 19.2 19.6 17.4\nMTMSN BASE 32.2 28 32.5\nTable 6: EM on MWP datasets.\n2 3 4 5\n# terms\n0\n10\n20\n30EM\nGenBERT\nGenBERT+ND\nGenBERT+TD\nGenBERT+ND+TD\nNABERT+\nMTMSN\nFigure 5: Breakdown of model accuracy (EM) by the number\nof terms in the arithmetic expression, for the MWP datasets\nADDSUB, SOP and SE Q.\nLast, MTMSN outperforms GENBERT +ND+TD .\nHowever, MTMSN uses a specialized architecture\nfor addition and subtraction, suitable when calcu-\nlations are done outside of the model. GENBERT,\non the other hand, is a general-purpose generative\nmodel, that can also return span answers when the\ncomputation is done internally.\nNext, we break down performance by the num-\nber of terms in the arithmetic expression (Figure 5).\nThe plot shows that all models struggle to gener-\nalize to more complex problems, and completely\nfail when the calculation involves more than 3\nterms. Interestingly, the drop in performance of\nGENBERT +ND+TD between 2 and 3 terms is sig-\nniﬁcantly smaller than that of GENBERT +ND and\nGENBERT +TD . This suggests that both ND and\nTD are useful for improving robustness.\nError analysis To understand the limitations\nof our method, we analyze the errors of\nGENBERT +ND+TD on the development set of\nDROP , excluding questions with a multi-span\nanswer which are not supported by the model.\nWe sample 100 random examples for which\nGENBERT +ND+TD fails to predict the correct an-\nswer and manually analyze the types of questions\nand mistakes done by the model.\nWe ﬁnd that in almost half of the cases (43%),\nthe example requires reasoning skills that are either\nnot covered by the pre-training tasks (e.g. sorting),\nor not numerical. Another common case (23%) is\ninaccurate predictions, such as spans that are too\nEM F1\nBERT 81.1 88.6\nGENBERT +ND-LM 78.1 85.8\nGENBERT +ND 80.7 88.1\nGENBERT +TD 80.7 88.2\nGENBERT +ND+TD 81.3 88.6\nTable 7: Performance on SQuAD v1 development set. Scores\nfor BERT are using wordpiece tokenization.\nlong and numbers with partial digit match to the\ngold answer. We note that many of these errors can\nbe addressed by extending the pre-training tasks\nto cover additional numerical skills and a larger\nnumber range. We leave such extensions for future\nwork. Further details and example failure cases are\nprovided in §A.5.\n5.3 Reading Comprehension Performance\nHaving shown that our models successfully learned\nto perform NRoT, we investigate if this improve-\nment comes at the expense of performance on RC\ndatasets. We initialize the RC model from Devlin\net al. (2019) with GENBERT weights (encoder\nonly) and ﬁne-tune it on SQUAD v1. As shown in\nTable 7, the performance of GENBERT +ND+TD is\nalmost identical to the original BERT. Moreover,\nGENBERT +ND-LM reported a loss of 3 EM points\nhighlighting the importance of using the MLM loss.\n5.4 G ENBERT With Other Architectures\nTo further establish the utility of GENBERT, we\nused the weights of GENBERT +ND+TD to initialize\nthe encoder of NABERT+ and MS-TAG , a recent\nmulti-span tagging model of Efrat et al. (2019).\nFine-tuning on DROP shows an improvement of\n∼2 EM points compared to the originally reported\nperformance: 63.0 →65.1 EM for NABERT+ ,\nand 67.3 →69.3 EM for MS-TAG . This shows\nthat GENBERT can be used as a drop-in replace-\nment for BERT, when numerical reasoning is\nneeded.\nTo summarize, we have empirically shown that\none can inject numerical reasoning skills into a\npre-trained LM, resulting in good performance on\nDROP , generalization to MWP, while maintaining\nhigh performance on standard RC datasets. More-\nover, the resulting weights can be used for initializ-\ning numerical reasoning models.\n6 Related Work\nMost NRoT models designed for DROP are ex-\ntractive QA models augmented with specialized\nmodules (§2). Two recent work (Andor et al., 2019;\n954\nChen et al., 2020) take a more symbolic approach\nand output a symbolic program augmented with\noperations over text. In our work, numerical com-\nputations are latent and performed internally by the\nmodel.\nA related line of work has been analyzing the\nmathematical reasoning abilities of neural mod-\nels over text (Wallace et al., 2019; Rozen et al.,\n2019; Ravichander et al., 2019), and on arithmetic\nproblems (Saxton et al., 2019; Amini et al., 2019;\nLample and Charton, 2020).\nDesigning pre-training tasks to teach LMs ad-\nditional skills has been applied by Huang et al.\n(2019), who designed cross-lingual pre-training\ntasks to teach better mappings between languages,\nand Lee et al. (2019), who introduced the Inverse\nCloze Task to pre-train an information retriever.\n7 Conclusions\nLarge pre-trained LMs lack high-level skills such as\nnumerical reasoning. Consequently, current mod-\nels that perform numerical reasoning over a pre-\ntrained LM resorted to customized modules with\nlimited ﬂexibility. In this work, we propose a gen-\neral method for injecting additional skills into LMs,\nassuming automatic data generation is possible. We\napply our approach to the task of numerical reason-\ning over text, using a general-purpose model called\nGENBERT, and a simple framework for generating\nlarge amounts of synthetic examples. Our experi-\nments demonstrate the effectiveness of our method,\nshowing that GENBERT successfully learns the\nnumerical skills, and performs on par with state-of-\nthe-art NRoT models of the same size.\nAcknowledgments\nWe thank Daniel Andor and Thang Luong for help-\nful discussions, and Shimi Salant for constructive\nsuggestions. This research was partially supported\nby The Israel Science Foundation grant 942/16,\nThe Yandex Initiative for Machine Learning, and\nthe European Research Council (ERC) under the\nEuropean Union Horizons 2020 research and inno-\nvation programme (grant ERC DELPHI 802800).\nReferences\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. MathQA: Towards interpretable math\nword problem solving with operation-based for-\nmalisms. In North American Association for Com-\nputational Linguistics (NAACL) , pages 2357–2367,\nMinneapolis, Minnesota.\nDaniel Andor, Emily Pitler, Kenton Lee, and Luheng\nHe. 2019. Giving bert a calculator: Finding op-\nerations and arguments with reading comprehen-\nsion. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nXinyun Chen, Chen Liang, Adams Wei Yu, Denny\nZhou, Dawn Song, and Quoc V . Le. 2020. Neural\nsymbolic reader: Scalable integration of distributed\nand symbolic representations for reading compre-\nhension. In International Conference on Learning\nRepresentations (ICLR).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL) , pages 4171–4186,\nMinneapolis, Minnesota.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In North\nAmerican Association for Computational Linguistics\n(NAACL).\nAvia Efrat, Elad Segal, and Mor Shoham. 2019. Tag-\nbased multi-span extraction in reading comprehen-\nsion. arXiv preprint arXiv:1909.13375.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In North American Association for Compu-\ntational Linguistics (NAACL), pages 4129–4138.\nM. J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kush-\nman. 2014. Learning to solve arithmetic word\nproblems with verb categorization. In Empirical\nMethods in Natural Language Processing (EMNLP),\npages 523–533.\nMinghao Hu, Yuxing Peng, Zhen Huang, and Dong-\nsheng Li. 2019. A multi-type multi-span network\nfor reading comprehension that requires discrete rea-\nsoning. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjun Shou, Daxin Jiang, and Ming Zhou. 2019.\nUnicoder: A universal language encoder by pre-\ntraining with multiple cross-lingual tasks. In Pro-\nceedings of the Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\n955\nAishwarya Kamath and Rajarshi Das. 2019. A survey\non semantic parsing. Automated Knowledge Base\nConstruction (AKBC).\nJambay Kinley and Raymond Lin. 2019. NABERT+:\nImproving Numerical Reasoning in Reading Com-\nprehension.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences ,\n114(13):3521–3526.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\n2015. Parsing algebraic word problems into equa-\ntions. In Transactions of the Association for Compu-\ntational Linguistics (TACL) , volume 3, pages 585–\n597. MIT Press.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini,\nNate Kushman, and Hannaneh Hajishirzi. 2016.\nMAWPS: A math word problem repository. InAsso-\nciation for Computational Linguistics (ACL) , pages\n1152–1157, San Diego, California.\nGuillaume Lample and FranÃ ˘gois Charton. 2020.\nDeep learning for symbolic mathematics. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Association for Com-\nputational Linguistics (ACL).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nM. E. Peters, M. Neumann, M. Iyyer, M. Gard-\nner, C. Clark, K. Lee, and L. Zettlemoyer. 2018.\nDeep contextualized word representations. In North\nAmerican Association for Computational Linguistics\n(NAACL).\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases? In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016.\nSQuAD: 100,000+ questions for machine compre-\nhension of text. In Empirical Methods in Natural\nLanguage Processing (EMNLP).\nAbhilasha Ravichander, Aakanksha Naik, Carolyn\nRose, and Eduard Hovy. 2019. EQUATE: A bench-\nmark evaluation framework for quantitative reason-\ning in natural language inference. In Proceedings\nof the 23rd Conference on Computational Natural\nLanguage Learning (CoNLL), pages 349–361, Hong\nKong, China. Association for Computational Lin-\nguistics.\nS. Roy, T. Vieira, and D. Roth. 2015. Reasoning about\nquantities in natural language. Transactions of the\nAssociation for Computational Linguistics (TACL) ,\n1.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1743–\n1752.\nOhad Rozen, Vered Shwartz, Roee Aharoni, and Ido\nDagan. 2019. Diversify your datasets: Analyzing\ngeneralization via controlled variance in adversar-\nial datasets. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 196–205, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nDavid Saxton, Edward Grefenstette, Felix Hill, and\nPushmeet Kohli. 2019. Analysing mathematical rea-\nsoning abilities of neural models. In International\nConference on Learning Representations (ICLR).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems (NIPS), pages 5998–6008.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do nlp models know num-\nbers? probing numeracy in embeddings. In Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\n956\nA Supplemental Material\nA.1 Synthetic Numerical Data Generation\nWe brieﬂy describe the numerical templates, pro-\nviding the details missing from Table 2. In all cases,\nintegers are sampled from {0,..., 20K}, and split\ninto disjoint train and development sets to assure\ngeneralization.\n•signed ﬂoat combination : Random signed com-\nbinations of up to 4 ﬂoats. Floats are sampled\nfrom the set of ﬂoats with two decimal places.\n•min/max/avg : We sample 2-4 ﬂoats and apply a\nmin, max, avg, operation by sampling a word\nfrom the set O= {“longest”, “last”, “highest”,\n“largest”, “most”, “shortest”, “ﬁrst”, “smallest”,\n“lowest”, “least”, “average”}.\n•arg max, arg min: We sample word-ﬂoat pairs,\nwhere words are sampled from W: words in the\nNLTK Words Corpus2 having at most 2 word-\npieces, and ﬂoats are sampled as above.\n•date max/min : Same as min/max/avg above, but\nfor dates. Dates are sampled from D: the set of\ndates until Sep 2019. The operator word is sam-\npled from DSUP = {“last”, “latest”, “most re-\ncent”, “youngest”, “ﬁrst”, “earliest”, “oldest”,\n“least recent”} and mapped to min or max.\n•date difference : This teaches our model to per-\nform date arithmetic in days, months and years.\n•percentage : We teach our model to perform\n100 −xoperations in the context of percentages.\nGiven a number of arguments, we sample a per-\ncentage split using a ﬂat Dirichlet distribution.\nA.2 Synthetic Textual Data Generation\nA.2.1 Sentence template extraction\nTo extract sentence templates, we abstract the text\nof math word problems from the corpus published\nby Hosseini et al. (2014). Going over examples, we\nsplit the problem text into sentences3, and abstract\nthe tokens of each sentence independently. To-\nkens are abstracted according to the framework into\nnumbers (NUM), verb categories ( VERB), entities\n(ENT), containers (CONT) and attributes (ATTR).\nTo have a better control over the generation pro-\ncess, we extend the framework of Hosseini et al.\n(2014) to support two container types - agent (AGT)\nand environment (ENV). Agents are objects which\nactively collect and drop entities, for example a per-\nson or an organization. Environments are passive\ncontainers, such as places or time periods. In addi-\n2 https://www.nltk.org/\n3 Using the Spacy library http://spacy.io/\ntion, we introduce two-level containers to express\ninclusion relation between containers. For instance,\nif 3 submarines anchor near the city of Devonport,\nthen they also anchor near the country of England.\nThe 12 most common extracted sentence tem-\nplates, which were used for generating synthetic\ndata, are provided in Table 8.\nA.2.2 Template instantiation\nSentence templates are instantiated with a small\nvocabulary, that map categories into words. In\nthis work, we construct two domain-speciﬁc small-\nworld vocabularies, about history and the National\nFootball League. The vocabularies are available\nin a json format in https://github.com/\nag1988/injecting_numeracy.\nA.2.3 Question templates\nThe 13 question templates for 7 different skills are\nprovided in Table 9.\nA.3 Data for Masked LM task\nFor creating the training data for the masked\nLM task (§ 5.1) we took the pages from English\nWikipedia whose lowercased title containing a\nstring in { season, economy, demographics, con-\nquest, war, battle, uprising, rebellion, insurgency,\nconﬂict, crisis, revolution, military history, mutiny,\nregiment, revolt, geography, raids, insurrection,\ninvasion, feud, siege, campaign, expedition, suc-\ncession, coup, university}. This resulted in 156K\nfull pages. In the remaining pages, paras with < 15\nnumbers were discarded. Pages were tokenized us-\ning DT (§ 3) and chunked into 512-token sequences.\nFollowing Devlin et al. (2019), each token was\nmasked with probability 0.15 with no more than 65\nmasks per sample. This gave us 0.7M samples.\nA.4 Experimental Setup\nFor all our experiments, we used an older version of\nHugging Face’s Transformers library (Wolf et al.,\n2019) and provide our training hyperparameters in\nTable 10.\nA.5 G ENBERT +ND+TD Error Analysis\nTable 11 summarizes the main failure types ofGEN-\nBERT +ND+TD on 100 random examples from the\ndevelopment set of DROP , excluding questions\nwith a multi-span answer.\n957\nTemplate\nCONT-1-AGT VERB-1-* NUM-1 ATTR-1 ENT-1.\nCONT-1-AGT VERB-1-POS NUM-1 ATTR-1 ENT-1and CONT-2-AGT VERB-1-POS NUM-2 ATTR-1 ENT-1.\nCONT-1-AGT VERB-1-POS NUM-1 ATTR-1 ENT-1and NUM-2 ATTR-2 ENT-2.\nCONT-1-AGT VERB-1-POS NUM-1 ATTR-1 ENT-1, but VERB-2-NEG NUM-2 ATTR-2 ENT-2.\nCONT-1-AGT VERB-1-POS NUM-1 ATTR-1 ENT-1in ATTR-2 CONT-2-ENV.\nCONT-1-AGT VERB-1-NEG NUM-1of the ATTR-1 ENT-1.\nCONT-1-AGT had NUM-1 ATTR-1 ENT-1, CONT-2-AGT had NUM-2 ATTR-1 ENT-1, and CONT-3-AGT had\nNUM-3 ATTR-1 ENT-1.\nNUM-1 ATTR-1 ENT-1, NUM-2 ATTR-2 ENT-2, and NUM-3 ATTR-3 ENT-3were VERB-1-POS in ATTR-4\nCONT-1-ENV .\nThere were NUM-1 ATTR-1 ENT-1and NUM-2 ATTR-2 ENT-2in ATTR-3 CONT-1-ENV.\nThere were NUM-1 ATTR-1 ENT-1in ATTR-2 CONT-1-ENV.\nCONT-1-AGT VERB-1-NEGTRN NUM-1 ATTR-1 ENT-1to CONT-2-AGT .\nCONT-1-AGT VERB-1-POSTRN NUM-1 ATTR-1 ENT-1from CONT-2-AGT .\nTable 8: Sentence templates for synthetic textual examples.\nReasoning Templates\nSelection How many ATTR-1 ENT-1were in CONT-1-ENV?\nHow many ATTR-1 ENT-1did CONT-1-AGT VERB-POS?\nIntra-entity difference How many more ATTR-1 ENT-1were in CONT-1-ENV than ATTR-2 ENT-2?\nHow many more ATTR-1 ENT-1did CONT-1-AGT have than ATTR-2 ENT-2?\nIntra-entity subset How many ENT-1 of CONT-1 were ATTR-1 ENT-1?\nHow many ENT-1 of CONT-1 were not ATTR-1 ENT-1?\nInter-entity comparison Were there {more | less} ATTR-1 ENT-1in CONT-1-ENV or in CONT-2-ENV ?\nWho had {more | less} ATTR-1 ENT-1, CONT-1-AGT or CONT-2-AGT ?\nInter-entity superlative Who had the {highest | lowest} number of ATTR-1 ENT-1in total ?\nIntra-entity superlative What was the {highest | lowest} number of ATTR-1 ENT-1 VERB-POS in\nCONT-1-ENV ?\nWhat is the {highest | lowest} number of ATTR-1 ENT-1 CONT-1-AGT VERB-POS?\nInter-entity sum How many ATTR-1 ENT-1 were in CONT-1-ENV (, CONT-*-ENV) and\nCONT-2-ENV {in total | combined} ?\nHow many ATTR-1 ENT-1did CONT-1-ENV (, CONT-*-ENV) and CONT-2-ENV\nhave {in total | combined} ?\nTable 9: Templates for questions about generated synthetic passages, testing for numerical reasoning. The template placeholders\nare ﬁlled-in with values from the world state obtained after generating the synthetic passage.\npre-training ﬁnetuning\nlr bsz epochs lr bsz\nGENBERT - - - 3e-5 16\nGENBERT +ND 6e-5 800 60 3e-5 16\nGENBERT +ND-LM 4e-5 440 60 3e-5 16\nGENBERT +ND-LM-DT 4e-5 440 60 - -\nGENBERT +ND-LM-RS 4e-5 440 60 3e-5 16\nGENBERT +TD 1e-5 240 5 3e-5 14\nGENBERT +ND+TD 1e-5 240 5 3e-5 14\nTable 10: Hyperparameters used for pre-training GENBERT\nand ﬁnetuning it on DROP . lr=leaning rate, bsz=train batch\nsize. Common params: seed=42, optimizer=Bert-Adam,\nlinear-lr-warm-up=0.1, num epochs for ﬁnetuning=30, weight-\ndecay=0.01, max-grad-norm=1.0.\n958\nError category Example\nCounting q: How many people were the heads of the business?\nSorting q: Which nationality was the fourth largest?\nComplex calculation q: How many percent of people were either Black Hispanic, of Sub-Saharan African origin, or\nof West Indian or Afro-Caribbean American origin?\nComplex semantics q: By how many points did the Pistons lose their closest game?\nNot numerical q: Who defeated the Kievan Rus at the Battle of the Alta River?\nLonger span\nq: Where there more people in the peninsula pre-war or at the time of the ﬁrst census?\na: pre-war\np: pre-war population\nShorter span\nq: Was the life expectancy in 2015 higher for males or females?\na: females\np: female\nImprecise number\nprediction\nq: How many more estimated Chinese Americans lived in California compared to Massachusetts?\na: 1130100\np: 110100\nTable 11: Error categories of GENBERT +ND+TD on the development set of DROP , based on a manual error analysis of 85\nrandom examples. The upper part shows categories which are not not covered by our pre-training tasks or do not require\nnumerical skills. The lower part shows categories of inaccurate model predictions. The letters q, a and p denote the question,\ngold answer and model prediction, respectively."
}