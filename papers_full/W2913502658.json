{
  "title": "Transfer Learning for British Sign Language Modelling",
  "url": "https://openalex.org/W2913502658",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2743613575",
      "name": "Mocialov, Boris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2344299208",
      "name": "Turner, Graham",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227383775",
      "name": "Hastie, Helen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W46193368",
    "https://openalex.org/W2291817131",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2585754816",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2747236259",
    "https://openalex.org/W2129574239",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W1490721004",
    "https://openalex.org/W2516310076",
    "https://openalex.org/W1922655562",
    "https://openalex.org/W2250689755",
    "https://openalex.org/W2759336060",
    "https://openalex.org/W2963993537",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2047687344",
    "https://openalex.org/W2588820066",
    "https://openalex.org/W2077566853",
    "https://openalex.org/W1942925989",
    "https://openalex.org/W2550893117",
    "https://openalex.org/W2149933564",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W2538552469",
    "https://openalex.org/W2513079830",
    "https://openalex.org/W2076094076",
    "https://openalex.org/W2090291326",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W1931184598",
    "https://openalex.org/W1920292565",
    "https://openalex.org/W92271087",
    "https://openalex.org/W2252218224",
    "https://openalex.org/W1512014753",
    "https://openalex.org/W2091636093",
    "https://openalex.org/W1511991230"
  ],
  "abstract": "Automatic speech recognition and spoken dialogue systems have made great advances through the use of deep machine learning methods. This is partly due to greater computing power but also through the large amount of data available in common languages, such as English. Conversely, research in minority languages, including sign languages, is hampered by the severe lack of data. This has led to work on transfer learning methods, whereby a model developed for one language is reused as the starting point for a model on a second language, which is less resourced. In this paper, we examine two transfer learning techniques of fine-tuning and layer substitution for language modelling of British Sign Language. Our results show improvement in perplexity when using transfer learning with standard stacked LSTM models, trained initially using a large corpus for standard English from the Penn Treebank corpus",
  "full_text": "Transfer Learning for British Sign Language Modelling\nBoris Mocialov\nSchool of Mathematical\nand Computer Sciences and\nEdinburgh Centre for Robotics\nHeriot-Watt University\nEdinburgh, UK\nbm4@hw.ac.uk\nGraham Turner\nSchool of Social Sciences and\nCentre for Translation &\nInterpreting Studies in Scotland\nHeriot-Watt University\nEdinburgh, UK\ng.h.turner@hw.ac.uk\nHelen Hastie\nSchool of Mathematical\nand Computer Sciences and\nEdinburgh Centre for Robotics\nHeriot-Watt University\nEdinburgh, UK\nh.hastie@hw.ac.uk\nAbstract\nAutomatic speech recognition and spoken dialogue systems have made great advances through\nthe use of deep machine learning methods. This is partly due to greater computing power but also\nthrough the large amount of data available in common languages, such as English. Conversely,\nresearch in minority languages, including sign languages, is hampered by the severe lack of data.\nThis has led to work on transfer learning methods, whereby a model developed for one language\nis reused as the starting point for a model on a second language, which is less resourced. In\nthis paper, we examine two transfer learning techniques of ﬁne-tuning and layer substitution for\nlanguage modelling of British Sign Language. Our results show improvement in perplexity when\nusing transfer learning with standard stacked LSTM models, trained initially using a large corpus\nfor standard English from the Penn Treebank corpus.\n1 Introduction\nSpoken dialogue systems and voice assistants have been developed to facilitate natural conversation\nbetween machines and humans. They provide services through devices such as Amazon Echo Show\nand smartphones to help the user do tasks (McTear, 2004) and, more recently, for open domain chitchat\n(Serban et al., 2016), all through voice. Recent advances have been facilitated by the huge amounts of\ndata collected through such devices and have resulted in the recent success of deep machine methods,\nproviding signiﬁcant improvements in performance. However, not all languages are able to beneﬁt from\nthese advances, particularly those that are under-resourced. These include sign languages and it means\nthat those who sign are not able to leverage such interactive systems nor the beneﬁts that automatic\ntranscription and translating of signing would afford.\nHere, we advance the state of the art with respect to transcribing British Sign Language (BSL). Our aim\nis for automated transcription of the BSL into English leveraging video recognition technologies. BSL\nenables communication of meaning through parameters such as hand shape, position, hand orientation,\nmotion, and non-manual signals (Sutton-Spence and Woll, 1999). BSL has no standard notation for\nwriting the signs, as with letters and words in English. Analogous to the International Phonetic Alphabet\n(IPA), highly detailed mapping of visual indicators to written form are available, such as HamNoSys\n(Hanke, 2004). Despite the expressiveness of the HamNoSys writing system, its practical uses are limited\nand only a handful of experts know how to use it. Recent methods for automatic speech recognition\n(ASR) use deep neural models to bypass the need for phoneme dictionaries (Hannun et al., 2014), which\nare then combined with language models. Previous work (Mocialov et al., 2016; Mocialov et al., 2017)\nhas shown that we can use visual features to automatically predict individual signs. This work follows\non in that these individual signs are to be used with a language model to take into account context and\ntherefore increase accuracy of the transcriber, which outputs a string of word-like tokens. These tokens\nare called glosses (Sutton-Spence and Woll, 1999; Cormier et al., 2015). Although glosses are translated\nBSL signs, they also convey some grammatical information about BSL. This makes glosses useful in\nThis work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/\narXiv:2006.02144v1  [cs.CL]  3 Jun 2020\ntheir own right without the videos of the BSL signs and sheds some light into the syntax and semantics\nof the BSL.\nThis paper focuses on language modelling, a common technique in the ﬁeld of ASR and Natural\nLanguage Processing to model the likelihood of certain words following each other in a sequence. We\nimprove modelling of the BSL glosses by proposing to use transfer learning approaches, such as ﬁne-\ntuning and layer substitution. The use of transfer learning technique can overcome the data sparsity issue\nin statistical modelling for scarce resource languages by using similar resources that can be found in\nlarge quantities and then further training the models on a speciﬁc low resource data.\nWe show that a model, pre-trained on the Penn Treebank (PTB) dataset 1 and ﬁne-tuned on the BSL\nmonolingual corpus2 can yield better results. This is in contrast to the same architecture that is trained\ndirectly on the BSL dataset without pre-training. This is a somewhat surprising result as there are marked\ndifferences between the two languages, particularly with the respect to the syntax (Sutton-Spence and\nWoll, 1999).\nThe paper begins with presenting methods for modelling languages and how they can be utilised in the\nBSL modelling. Section 2.2 gives an overview of how transfer learning can be achieved as well as the\nuse of transfer learning in sign languages. Section 3 gives an overview of the datasets that are used in this\npaper, their statistics, and pre-processing steps to create two monolingual corpora for statistical model\ntraining. Section 4 describes in detail the setup for the experiments in this paper. Section 5 presents the\nresults of the models employed for this research and discusses these results and the limitations of the\napproach taken in terms of the data used in Section 5.3. The paper is then concluded and future work is\nproposed.\n2 Related Work\n2.1 Sign Language Modelling\nDespite the availability of many alternatives for language modelling, such as count-based n-grams and\ntheir variations (Chen and Goodman, 1999; Rosenfeld, 2000; MacCartney, 2005; Bulyko et al., 2007;\nGuthrie et al., 2006), hidden Markov models (Dreuw and Ney, 2008; Dreuw et al., 2008), decision trees\nand decision forests (Filimonov, 2011), and neural networks (Deena et al., 2016; Mikolov et al., 2010),\nresearch in sign language modelling predominantly employs simple n-gram models, such as in Cate and\nHussain (2017), Forster et al. (2012), and Mass´o and Badia (2010).\nThe reason for the wide-spread use of n-grams in sign language modelling is the simplicity of the\nmethod. However, there is a disconnect between n-grams and sign language in that signing is embodied\nand perceived visually, while the n-grams are commonly applied to text sequence modelling. For this\nreason, the authors in Stein et al. (2007), Zhao et al. (2000), Dreuw et al. (2008), Mass ´o and Badia\n(2010), and Forster et al. (2013) model glosses, such as the ones shown on Figure 2, which are obtained\nfrom the transcribed sign languages, in a similar way to how language modelling is applied to automatic\ntranscribed words from speech.\nGlosses model the meaning of a sign in a written language, but not the execution (i.e. facial expres-\nsions, hand movement). Therefore, the more detailed meaning of what was signed may get lost when\nworking with the higher-level glosses. To overcome this issue and to incorporate valuable information\ninto sign language modelling, additional features are added in similar research, such as non-manual fea-\ntures (e.g facial expressions) (San-Segundo et al., 2009; Mass´o and Badia, 2010; Zhao et al., 2000; Stein\net al., 2007).\nIn this work we use glosses because we want to model BSL purely at the gloss level without any\nadditional information (e.g. facial expressions).\n2.2 Transfer Learning\nWhile transfer learning is a more general machine learning term, cross-domain adaptation of language\nmodels is used in the language modelling literature (Deena et al., 2016; Ma et al., 2017). Models are\n1https://catalog.ldc.upenn.edu/ldc99t42\n2http://www.bslcorpusproject.org/\nusually trained on some speciﬁc domain that consists of a speciﬁc topic, genre, and similar features that\ncan be identiﬁed by an expert. For example, a restaurant domain when a new type of a restaurant is\ncreated then the system needs to be able to adapt and be able to understand and discuss this new type\nof the restaurant. Unfortunately, it is nearly impossible to train a model for all possible conﬁguration of\ncurrent or future features. Commonly, a set of features are extracted from the raw data. When features\nchange, re-training is required. Useful features can also be extracted without expert knowledge with\nsuch techniques as Latent Dirichlet Allocation (LDA). These features usually take the form of words that\nrepresent topics in the data (Deena et al., 2016). Best practice tries to avoid re-training the models every\ntime one of the features changes as the domain changes due to the overhead involved.\nModel-based adaptation to the new domains, on the other hand, is achieved by either ﬁne-tuning or\nthe introduction of adaptation layer(s) (Yosinski et al., 2014). Fine-tuning involves further training the\nalready pre-trained model using the data from the new domain. The intuition behind the ﬁne-tuning is\nthat it is much quicker to learn new information with related knowledge. The adaptation layer approach\nincorporates new knowledge by re-training only the adaptation layer, whereas the rest of the model\nremains exactly the same as if it was used in the original domain and acts as a feature extractor for the\nnew domain (Deena et al., 2016).\nTransfer learning has been applied to sign languages in computing for various purposes to demonstrate\nthat the method is suitable for the task due to the lack of substantial domain-speciﬁc sign language data.\nTransfer learning has been successfully applied to static pose estimation, transferring the knowledge\nfrom pose estimation to the sign language pose estimation (Gattupalli et al., 2016) and classiﬁcation\nof ﬁngerspelled letters in American Sign Language (Garcia and Viesca, 2016; Karthick Arya, 2017;\nChaudhary, 2017; Muskan Dhiman, 2017). In particular, most of the transfer learning in sign language\nhas been applied to static image recognition to recognise the hand shape in an image using convolutional\nneural networks.\nWe apply transfer learning to the language modelling task as this is a key challenge in successfully\ntranscribing BSL.\n3 Corpora\nThe BSL corpus and the preprocessed Penn Treebank (PTB) corpus were chosen for this research. The\nmonolingual PTB dataset consists of telephone speech, newswire, microphone speech, and transcribed\nspeech. The dataset is preprocessed to eliminate letters, numbers, or punctuation and was used by\nMikolov (2010). The BSL corpus contains video conversations among deaf native, near-native and ﬂuent\nsigners across the United Kingdom. Almost all of the approximately one hundred recorded conversations\nare annotated for thirty seconds each at the gloss level using ELAN 3 annotation tool (Schembri et al.,\n2013).\nFigure 1: The BSL Corpus Project Sample Video Snippets4\n3https://tla.mpi.nl/tools/tla-tools/elan/\nAll recordings of the signers were made using up to four standard video cameras with a plain backdrop\nto provide full body view of the individuals, as well as, views from above of their use of signing space.\nThe conversations between the signers included signing personal experience anecdotes, spontaneous\nconversations (Schembri et al., 2013).\nThe BSL data that we focused on was narratives between two participants, where one person had to\nthink of a topic to sign about to another participant during the elicitation.\nRH-IDglossPT:PRO1SGEXPLAINABOUTPT:POSS1SGFS:PUPPYDSEW(FLAT)-BE:ANIMALPT:POSS1SGW ANTFAMILYAT-LASTHA VEDSEW(FLAT)-BE:ANIMAL?LAST-WEEKGOODLH-IDgloss EXPLAINABOUT FS:PUPPYDSEW(FLAT)-BE:ANIMALAT-LASTHA VEDSEW(FLAT)-BE:ANIMALGOODFree Translation I want to tell you about my puppy My family got a puppy last year\nModel Input GlossEXPLAINABOUT PUPPYANIMAL W ANTFAMILYAT-LASTHA VEANIMAL LAST-WEEKGOOD\nFigure 2: a) The BSL Corpus Annotation in ELAN; b) Table shows full text of the annotated glosses for\nthe two ﬁrst sentences from the ELAN annotation; c) Glosses that are used for the BSL modelling\nThe corpus is annotated with glosses, taken from the BSL SignBank in ELAN as shown in Figure 2a.\nFigure 2b shows all the glosses of the ﬁrst sentence. As mentioned above, gloss is an identiﬁer of\na unique sign, written in English and should represent its phonological and morphological meaning\n(Schembri et al., 2013). In the corpus, the glosses are identiﬁed throughout the videos for both left and\nright hands as sometimes different signs can be signed at the same time. Apart from the glossing, the\nannotations include the corresponding free English written translation of the meaning of the signing split\ninto sentences (see the Free Translation in the Figure 2). Figure 2c shows which glosses are considered\nfor the BSL modelling and which are ignored. This is done to match the vocabulary of the PTB corpus\nfor the transfer learning purposes.\n3.1 Data Pre-processing\nFor the BSL corpus, we ignore the free translation and extract English text from the glosses, preserving\nthe order of the signs executed. For example, in Figure 2, right-hand glosses identify the following\norder of the signs: good, explain, about, puppy, etc. excluding glosses, such as PT:PRO for pointing\nsigns or PT:POSS for possessives and others (Figure 2c), which are explained in more detail in Fenlon et\nal. (2014). Since the gloss annotation does not include explicit punctuation, it is impossible to tell where\na signed sentence begins and where it stops. To overcome this limitation of the gloss annotation, we use\nthe Free Translation annotation, which gives the boundaries of sentences in videos. Later, we split the\nextracted glosses into sentences using these sentence boundaries. By the end of the pre-processing stage,\nwe have glosses (excluding special glosses for pointing signs, posessives or other non-lexical glosses) in\nthe order that the corresponding signs were executed in the video, split into sentences. As a result, we\nextracted 810 nominal sentences from the BSL corpus with an average length of the sentence being 4.31\nglossed signs, minimum and maximum lengths of 1 and 13 glossed signs respectively. A monolingual\ndataset has been created with the extracted sentences. As obtained from the PTB dataset (Merity et al.,\n2017), the English language corpus has 23.09 words on average per sentence with minimum being 3 and\nmaximum 84 words per sentence. The pre-processed BSL corpus has a vocabulary of 666 words, while\nthe PTB dataset has a vocabulary of 10,002 words. From this point on in this paper, we will use the\nterm ‘words’ to refer to both glosses in the BSL and words in the PTB datasets because we aim to use a\ncommon vocabulary for training our models.\n4http://www.bslcorpusproject.org/cava/\na)\nb)\nc)\nBoth monolingual datasets were split into training, validation, and testing sets as required for training\nand evaluation of the statistical models. Both datasets were split using ratio 85:15. The smaller subset,\nin turn, was split 50:50 for validation and testing for the two datasets.\n4 Language Modelling Methodology\n4.1 Statistical Language Models\nPerplexity measure has been used for evaluation and comparison purposes of different models. We used\nthe following formula to calculate the perplexity values: eCross−Entropy as used in Bengio et al. (2003),\nwhich approximates geometric average of the predicted words probabilities on the test set. We have\nexplicitly modelled out-of-vocabulary (OOV), such as < unk >placeholder in all the experiments.\n4.1.1 Neural Models\nFor comparison, we use two methods: 1) stacked LSTM and 2) Feed-Forward (FFNN) architectures to\ncreate the BSL language models. All models are implemented in PyTorch 5 with weight-drop recurrent\nregularisation scheme for the LSTMs, which is important for overcoming commonly known LSTM\nmodel generalisation issues (Merity et al., 2017; Merity et al., 2018). The feed-forward model, on the\nother hand, had no regularisations as it is less susceptible to overﬁtting due to the much smaller number\nof parameters.\nThe parameters that were modiﬁed to achieve the lowest perplexity were input size of the overall input\nsequence for the recurrent neural network (back-propagation through time, BPTT), batch size, learning\nrate, and the optimizer. The parameters were selected using the grid search approach using perplexity\nmetric. As a result, for the stacked LSTMs, bptt was set to 5, batch size was set to 16, discounted learning\nrate was set to 30, and the optimizer was set to stochastic gradient descent. In case of the feed-forward\nnetwork, input was set to 5 words, batch size was set to 16, discounted learning rate was set to 30, and\nthe optimizer was set to stochastic gradient descent. All the neural models were trained for 100 epochs.\nIn the case of the neural networks, the sequences of words were tokenised (i.e. turned into integers)\nand the tokenisation was stored to ensure the same tokenisation during the transfer learning phase. The\ninput, therefore, consisted of a set of tokens, while the outputs (i.e. predicted words) were turned into a\none-hot vectors.\n(a)\nStacked LSTMs model\n(b)\nFeed-Forward model\nFigure 3: The two types of neural models used to test transfer methods for sign language modelling\n5http://pytorch.org/\n4.1.1.1 Stacked LSTMs\nFigure 3a shows the architecture of the stacked LSTM model. The model consists of an embedding layer\nof 400 nodes, which, together with the tokenisation, turns string of words into a vector of real numbers.\nSecondly, three LSTM layers with 1150 nodes each are stacked vertically for deeper feature extraction.\nThirdly, the linear layer downsizes the stacked LSTMs output to the vocabulary size and applies linear\ntransformation with softmax normalisation. The weights of the embedding and the linear layers are tied.\nThis means that the two layers share the same weights, which reduces the number of parameters of the\nnetwork and makes the convergence during training faster. The same architecture was used in Merity\net al. (2017) to model PTB dataset, reporting 57.3 perplexity, utilising cache in the model from recent\npredictions.\n4.1.1.2 FFNN\nFigure 3b shows the Feed-forward model architecture. The model does not have the stacked LSTMs lay-\ners. Instead, the stacked LSTMs are substituted with one hidden fully-connected rectiﬁer layer, which is\nknown to overcome the vanishing gradient problem. The weights of the embedding and the outputs lay-\ners are not tied together. Similar architectures have been used for language modelling in Le et al. (2013),\nMikolov et al. (2009), and de Br ´ebisson et al. (2015) with the hidden layer having different activation\nfunctions with the PTB dataset being used in Audhkhasi et al. (2014), reporting 137.32 perplexity.\n4.1.2 Training the Models\nTransfer learning was achieved with both ﬁne-tuning and substitution. Both FFNN and LSTM were\ntrained on the PTB dataset and then either ﬁne-tuned or the last layer was substituted with the new\nadaptation layer, freezing the rest of the weights, and further training on the BSL dataset.\nTo achieve ﬁne-tuning, ﬁrst the best model is saved after the training of both the FFNN and the stacked\nLSTMs on the PTB dataset. Then the training is restarted on the BSL corpus, having initialised the model\nwith the weights, trained on the PTB dataset.\nTo perform layer substitution as a transfer learning approach, the same ﬁrst step as with the ﬁne-tuning\nis repeated and the model, trained on the PTB, is saved. When the training is restarted on the BSL dataset,\nthe saved model is loaded and the last linear layer is substituted with a layer that has as many nodes as\nthe BSL vocabulary. Later, all the weights of the network are locked and will not be modiﬁed during\nthe optimisation. Only the weights of the last substituted layer will be modiﬁed. This method uses the\npretrained network as a feature extractor and only modiﬁes the last layer weights to train the model for\nthe BSL dataset.\n5 Results\nThis section is split into two subsections. We ﬁrstly present results without transfer learning, namely\nboth the FFNN and the stacked LSTMs models trained and tested on the PTB dataset or trained and\ntested on the BSL. Later we present results with the transfer learning, with both FFNN and the stacked\nLSTMs models trained on the PTB dataset and then ﬁne-tuned and tested on the BSL.\nTo show that the two languages are different, as discussed in Section 3.1, we applied the model trained\non one language to the other language and vice versa. As a result, the model trained on English language\nand applied to the BSL scored 1051.91 in perplexity using SRILM toolkit (Stolcke, 2002). Conversely,\na model trained on the BSL has been applied to the English language and scored 1447.23 in perplexity.\nAs expected, the perplexity is high in both cases, which means that the probability distribution over the\nnext word in one language is far from the true distribution of words in the other language.\n5.1 Without Transfer Learning\nTable 1 shows perplexities on the two datasets with two statistical models. From the table, we can infer\nthat the trained models on the PTB dataset have lower perplexity than the same architectures trained on\nthe BSL dataset. This can be explained by the fact that the PTB dataset has more data than the BSL\nMethod\nPenn Treebank\n(PTB)\nThe BSL\nCorpus Project\nFFNN 190.46 258.1\nStacked LSTMs 65.91 274.03\nOOV 6.09% 25.18%\nTable 1: Perplexities on either the PTB or the BSL test sets using models trained and tested on the same\ncorpus (i.e. PTB and BSL)\ndataset and, therefore, statistical models can generalise better. Furthermore, the amount of data is further\nreduced in the BSL case as the OOV covers a quarter of the overall dataset.\n5.2 With Transfer Learning\nTable 2 shows perplexities on the two datasets with two statistical models, applying transfer learning.\nFrom this table, it can be seen that the substitution approach gives very similar results independent of the\nwhether FFNN or stacked LSTMs model is used (123.92 versus 125.32). The best result is achieved with\nthe ﬁne-tuning approach on the stacked LSTMs model, while the higher perplexity result is on the FFNN\nmodel with the ﬁne-tuning approach. Similar results have been reported in Irie et al. (2016), where ﬁne-\ntuned GRU performed worse than ﬁne-tuned LSTM model. In addition, the OOV count differs from that\nof the Table 1 due to the fact that a subset of the vocabulary, observed in the PTB dataset during training\nis then identiﬁed in the BSL dataset during testing.\nMethod Fine-tuning Substitution\nFFNN 179.3 123.92\nStacked LSTMs 121.46 125.32\nOOV 12.71%\nTable 2: Perplexities on the BSL test set after applying the transfer learning on FFNN and LSTMs\n5.3 Discussion\nThe salient idea of this paper is whether transfer learning is a legitimate method for modelling one\nlanguage with the knowledge of another, assuming the languages are different, but share some common\nproperties, such as vocabulary. This theory is intuitive and has been discussed in linguistics for spoken\nlanguages (Kaivapalu and Martin, 2007). In our case, PTB corpus covers most of the vocabulary found\nin the BSL corpus (12.71% OOV) by the virtue of the gloss annotation of the BSL corpus (Schembri\net al., 2013). However, the languages are assumed to be different as they evolved independently of one\nanother (Brennan, 1992).\nThe results obtained are different from reported in similar research. For example, for the FFNN model,\nAudhkhasi et al. (2014) report 137.32 versus our achieved 190.46 perplexity and for the stacked LSTMs\nmodel, Merity et al. (2017) report 57.3 versus our achieved 65.91 perplexity. This can be explained by\nthe fact that not all the regularisation techniques had been used in this research as in the past research and\nthe model training had been restricted to 100 epochs. Further training may further reduce the perplexity\nto that reported in Merity et al. (2017).\nFrom the results, we can see that the transfer learning leads to superior models than the models trained\non the BSL directly (258.1 and 274.03 against 123.92 and 125.32). Since the quality of the trained models\nusing either of the approaches is similar in case of the stacked LSTMs model (121.46 and 125.32), the\nchoice between the ﬁne-tuning and substitution can be guided based on the convergence speed. During\nthe substitution, only one layer of the network is replaced with a new one and the rest of the weights\nin the network are locked, therefore, one set of weights will be optimized. This is in contrast to the\nﬁne-tuning method, which optimizes all of the weights, which may, in turn, require more interactions,\ndepending on how different the new data is.\n6 Conclusion\nThis paper shows how transfer learning techniques can be used to improve language modelling for the\nBSL language at the gloss level. Statistical modelling techniques are used to generate language models\nand to evaluate them using a perplexity measure.\nThe choice of the transfer learning technique is guided by the scarcity of available resources of the\nBSL language and the availability of the English language dataset that shares similar language modelling\nvocabulary with the annotated BSL. Feed-forward and recurrent neural models have been used to evaluate\nand compare generated language models. The results show that transfer learning can achieve superior\nquality of the generated language models. However, our pre-processed BSL corpus lacks constructs that\nare essential for a sign language, such as classiﬁer signs and others. Nevertheless, transfer learning for\nmodelling the BSL shows promising results and should be investigated further.\n6.1 Future Work\nAlthough this paper discusses the use of a model initially trained on English and presents promising\npreliminary results, the annotation of the BSL, used in this paper, is limited as this paper serves as a\nproof of concept. In particular, the annotation used is missing some of the grammatical aspects of the\nBSL, such as classiﬁer signs and others. Inclusion of these into the BSL language modelling would\nincrease the OOV count as the English language does not have equivalent language constructs. This\nraises a question whether a sign language can be modelled using other languages that may have these\nconstructs. More generally, is it possible to model a language with transfer learning using other less-\nrelated languages? Similar questions have been partly answered for the written languages in the ﬁeld of\nmachine translation (Gu et al., 2018) by bringing words of different languages close to each other in the\nlatent space. However, nothing similar has been done for the sign languages.\nFrom the methodological side of the modelling, additional advanced state of the art techniques should\nbe experimented with to achieve greater quality of the generated models, such as attention mechanism\nfor the recurrent neural networks. Finally, this paper focuses on key techniques for sign processing,\nwhich could be part of a larger conversational system whereby signers could interact with computers\nand home devices through their natural communication medium of sign. Research in such end-to-end\nsystems would include vision processing, segmentation, classiﬁcation, and language modelling as well\nas language understanding and dialogue modelling, all tuned to sign language.\nReferences\n[Audhkhasi et al.2014] Kartik Audhkhasi, Abhinav Sethy, and Bhuvana Ramabhadran. 2014. Diverse embedding\nneural network language models. arXiv preprint arXiv:1412.7063, abs/1412.7063.\n[Bengio et al.2003] Yoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural\nprobabilistic language model. Journal of machine learning research, 3(Feb):1137–1155.\n[Brennan1992] Mary Brennan. 1992. The visual world of BSL: An introduction. Faber and Faber. In David Brien\n(Ed.), Dictionary of British Sign Language/English.\n[Bulyko et al.2007] Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long Nguyen, and John Makhoul. 2007.\nLanguage model adaptation in machine translation from speech. In Proceedings of the Acoustics, Speech and\nSignal Processing, 2007. ICASSP 2007. IEEE International Conference on, volume 4, pages IV–117. IEEE.\n[Cate and Hussain2017] Hardie Cate and Zeshan Hussain. 2017. Bidirectional american sign language to english\ntranslation. arXiv preprint arXiv:1701.02795, abs/1701.02795.\n[Chaudhary2017] Belal Chaudhary. 2017. Real-time translation of sign language into text. Data Science Retreat.\nhttps://github.com/BelalC/sign2text, apr.\n[Chen and Goodman1999] Stanley F Chen and Joshua Goodman. 1999. An empirical study of smoothing tech-\nniques for language modeling. Computer Speech & Language, 13(4):359–394.\n[Cormier et al.2015] Kearsy Cormier, Jordan Fenlon, Sannah Gulamani, and Sandra Smith. 2015. Bsl corpus\nannotation conventions.\n[de Br´ebisson et al.2015] Alexandre de Br ´ebisson, ´Etienne Simon, Alex Auvolat, Pascal Vincent, and Yoshua Ben-\ngio. 2015. Artiﬁcial neural networks applied to taxi destination prediction. arXiv preprint arXiv:1508.00021,\nabs/1508.00021.\n[Deena et al.2016] Salil Deena, Madina Hasan, Mortaza Doulaty, Oscar Saz, and Thomas Hain. 2016. Combining\nfeature and model-based adaptation of rnnlms for multi-genre broadcast speech recognition. In Proceedings of\nthe Annual Conference of the International Speech Communication Association, INTERSPEECH, pages 2343–\n2347. Shefﬁeld.\n[Dreuw and Ney2008] Philippe Dreuw and Hermann Ney. 2008. Visual modeling and feature adaptation in sign\nlanguage recognition. In Voice Communication (SprachKommunikation), 2008 ITG Conference on, pages 1–4.\nVDE.\n[Dreuw et al.2008] Philippe Dreuw, Carol Neidle, Vassilis Athitsos, Stan Sclaroff, and Hermann Ney. 2008.\nBenchmark databases for video-based automatic sign language recognition. In LREC.\n[Fenlon et al.2014] Jordan Fenlon, Adam Schembri, Ramas Rentelis, David Vinson, and Kearsy Cormier. 2014.\nUsing conversational data to determine lexical frequency in British Sign Language: The inﬂuence of text type.\nLingua, 143:187–202.\n[Filimonov2011] Denis Filimonov. 2011. Decision tree-based syntactic language modeling. University of Mary-\nland, College Park.\n[Forster et al.2012] Jens Forster, Christoph Schmidt, Thomas Hoyoux, Oscar Koller, Uwe Zelle, Justus H Piater,\nand Hermann Ney. 2012. Rwth-phoenix-weather: A large vocabulary sign language recognition and translation\ncorpus. In Proceedings of the 8th International Conference on Language Resources and Evaluation, LREC.\n[Forster et al.2013] Jens Forster, Oscar Koller, Christian Oberd ¨orfer, Yannick Gweth, and Hermann Ney. 2013.\nImproving continuous sign language recognition: Speech recognition techniques and system design. In Pro-\nceedings of the Fourth Workshop on Speech and Language Processing for Assistive Technologies, pages 41–46.\n[Garcia and Viesca2016] Brandon Garcia and Sigberto Alarcon Viesca. 2016. Real-time american sign language\nrecognition with convolutional neural networks. InIn Proceedings of Machine Learning Research, pp. 225-232.\n[Gattupalli et al.2016] Srujana Gattupalli, Amir Ghaderi, and Vassilis Athitsos. 2016. Evaluation of deep learning\nbased pose estimation for sign language. arXiv preprint arXiv:1602.09065, abs/1602.09065.\n[Gu et al.2018] Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor OK Li. 2018. Universal neural machine transla-\ntion for extremely low resource languages. arXiv preprint arXiv:1802.05368.\n[Guthrie et al.2006] David Guthrie, Ben Allison, Wei Liu, Louise Guthrie, and Yorick Wilks. 2006. A closer\nlook at skip-gram modelling. In Proceedings of the 5th international Conference on Language Resources and\nEvaluation (LREC-2006).\n[Hanke2004] Thomas Hanke. 2004. Hamnosys-representing sign language data in language resources and lan-\nguage processing contexts.\n[Hannun et al.2014] Awni Y . Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan\nPrenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and Andrew Y . Ng. 2014. Deep speech: Scaling\nup end-to-end speech recognition. CoRR, abs/1412.5567.\n[Irie et al.2016] Kazuki Irie, Zoltan Tuske, Tamer Alkhouli, Ralf Schluter, and Hermann Ney. 2016. LSTM, gru,\nhighway and a bit of attention: an empirical overview for language modeling in speech recognition. Technical\nreport, RWTH Aachen University Aachen Germany.\n[Kaivapalu and Martin2007] Annekatrin Kaivapalu and Maisa Martin. 2007. Morphology in Transition: Plural\nInﬂection of Finnish nouns by Estonian and Russian Learners. Acta Linguistica Hungarica, 54(2):129–156.\n[Karthick Arya2017] Jayesh Kudase Karthick Arya. 2017. Convolutional neural networks based sign language\nrecognition. International Journal of Innovative Research in Computer and Communication Engineering, 5(10),\noct.\n[Le et al.2013] Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc Gauvain, and Franc ¸ois Yvon. 2013. Struc-\ntured output layer neural network language models for speech recognition.IEEE Transactions on Audio, Speech,\nand Language Processing, 21(1):197–206.\n[Ma et al.2017] Min Ma, Michael Nirschl, Fadi Biadsy, and Shankar Kumar. 2017. Approaches for neural-network\nlanguage model adaptation. Proc. Interspeech 2017, pages 259–263.\n[MacCartney2005] Bill MacCartney. 2005. NLP lunch tutorial: Smoothing.\n[Mass´o and Badia2010] Guillem Mass ´o and Toni Badia. 2010. Dealing with sign language morphemes in statisti-\ncal machine translation. In 4th workshop on the representation and processing of sign languages: corpora and\nsign language technologies.\n[McTear2004] Mike McTear. 2004. Spoken Dialogue Technology: Toward the Conversational User Interface.\nSpringer, London.\n[Merity et al.2017] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and Optimizing\nLSTM Language Models. arXiv preprint arXiv:1708.02182.\n[Merity et al.2018] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. An Analysis of Neural\nLanguage Modeling at Multiple Scales. arXiv preprint arXiv:1803.08240.\n[Mikolov et al.2009] T. Mikolov, J. Kopecky, L. Burget, O. Glembek, and J. Cernocky. 2009. Neural network\nbased language models for highly inﬂective languages. In 2009 IEEE International Conference on Acoustics,\nSpeech and Signal Processing, pages 4725–4728, April.\n[Mikolov et al.2010] Tom ´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan ˇCernock`y, and Sanjeev Khudanpur. 2010.\nRecurrent neural network based language model. In Eleventh Annual Conference of the International Speech\nCommunication Association.\n[Mocialov et al.2016] Boris Mocialov, Patricia A Vargas, and Micael S Couceiro. 2016. Towards the evolution of\nindirect communication for social robots. In Computational Intelligence (SSCI), 2016 IEEE Symposium Series\non, pages 1–8. IEEE.\n[Mocialov et al.2017] Boris Mocialov, Graham Turner, Katrin Lohan, and Helen Hastie. 2017. Towards continuous\nsign language recognition with deep learning. In Proc. of the Workshop on the Creating Meaning With Robot\nAssistants: The Gap Left by Smart Devices.\n[Muskan Dhiman2017] Dr G.N. Rathna Muskan Dhiman. 2017. Sign language recognition.\nhttps://edu.authorcafe.com/academies/6813/sign-language-recognition.\n[Rosenfeld2000] Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from\nhere? Proceedings of the IEEE, 88(8):1270–1278.\n[San-Segundo et al.2009] Rub ´en San-Segundo, Jos´e Manuel Pardo, Javier Ferreiros, Valent´ın Sama, Roberto Barra-\nChicote, Juan Manuel Lucas, D S ´anchez, and Antonio Garc ´ıa. 2009. Spoken spanish generation from sign\nlanguage. Interacting with Computers, 22(2):123–139.\n[Schembri et al.2013] Adam Schembri, Jordan Fenlon, Ramas Rentelis, Sally Reynolds, and Kearsy Cormier.\n2013. Building the British Sign Language corpus. Language Documentation and Conservation 7.\n[Serban et al.2016] Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, and Joelle Pineau. 2016. Generative deep\nneural networks for dialogue: A short review. arXiv preprint arXiv:1611.06216.\n[Stein et al.2007] Daniel Stein, Philippe Dreuw, Hermann Ney, Sara Morrissey, and Andy Way. 2007. Hand in\nhand: automatic sign language to english translation.\n[Stolcke2002] Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of the\nSeventh international conference on spoken language processing.\n[Sutton-Spence and Woll1999] R. Sutton-Spence and B. Woll. 1999. The Linguistics of British Sign Language: An\nIntroduction. The Linguistics of British Sign Language: An Introduction. Cambridge University Press.\n[Yosinski et al.2014] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are\nfeatures in deep neural networks? In Advances in neural information processing systems, pages 3320–3328.\n[Zhao et al.2000] Liwei Zhao, Karin Kipper, William Schuler, Christian V ogler, Norman Badler, and Martha\nPalmer. 2000. A machine translation system from english to american sign language. In Conference of the\nAssociation for Machine Translation in the Americas, pages 54–67. Springer.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8844332695007324
    },
    {
      "name": "Computer science",
      "score": 0.868389368057251
    },
    {
      "name": "Treebank",
      "score": 0.8619883060455322
    },
    {
      "name": "Natural language processing",
      "score": 0.7024635076522827
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6668015122413635
    },
    {
      "name": "Transfer of learning",
      "score": 0.6533917188644409
    },
    {
      "name": "Language model",
      "score": 0.5394312739372253
    },
    {
      "name": "Deep learning",
      "score": 0.4505987763404846
    },
    {
      "name": "Point (geometry)",
      "score": 0.4501764476299286
    },
    {
      "name": "Sign language",
      "score": 0.43012794852256775
    },
    {
      "name": "American Sign Language",
      "score": 0.4207736849784851
    },
    {
      "name": "Linguistics",
      "score": 0.217555969953537
    },
    {
      "name": "Annotation",
      "score": 0.1061648428440094
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}