{
    "title": "Beware of Data Leakage from Protein LLM Pretraining",
    "url": "https://openalex.org/W4400943186",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5105036233",
            "name": "Leon Hermann",
            "affiliations": [
                "Hasso Plattner Institute",
                "University of Potsdam"
            ]
        },
        {
            "id": "https://openalex.org/A5016240737",
            "name": "Tobias Fiedler",
            "affiliations": [
                "Hasso Plattner Institute",
                "University of Potsdam"
            ]
        },
        {
            "id": "https://openalex.org/A5105036234",
            "name": "Hoang An Nguyen",
            "affiliations": [
                "Hasso Plattner Institute",
                "University of Potsdam"
            ]
        },
        {
            "id": "https://openalex.org/A5033634402",
            "name": "Melania Nowicka",
            "affiliations": [
                "Hasso Plattner Institute",
                "University of Potsdam",
                "Massachusetts Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5039532268",
            "name": "Jakub M. Bartoszewicz",
            "affiliations": [
                "Hasso Plattner Institute",
                "University of Potsdam",
                "Massachusetts Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4394906324",
        "https://openalex.org/W4393785684",
        "https://openalex.org/W3213545574",
        "https://openalex.org/W4392686654",
        "https://openalex.org/W4287724045",
        "https://openalex.org/W4288066876",
        "https://openalex.org/W4391316987",
        "https://openalex.org/W2170747616",
        "https://openalex.org/W4327909864",
        "https://openalex.org/W4280491725",
        "https://openalex.org/W4223581484",
        "https://openalex.org/W3015921770",
        "https://openalex.org/W4366392978",
        "https://openalex.org/W4394891885",
        "https://openalex.org/W1988336536",
        "https://openalex.org/W4286500588",
        "https://openalex.org/W4318071656",
        "https://openalex.org/W4396675754",
        "https://openalex.org/W4283733033",
        "https://openalex.org/W4389486923",
        "https://openalex.org/W2528586407",
        "https://openalex.org/W2102461176",
        "https://openalex.org/W2787587553",
        "https://openalex.org/W3217314753",
        "https://openalex.org/W4391203107"
    ],
    "abstract": "Abstract Pretrained protein language models are becoming increasingly popular as a backbone for protein property inference tasks such as structure prediction or function annotation, accelerating biological research. However, related research oftentimes does not consider the effects of data leakage from pretraining on the actual downstream task, resulting in potentially unrealistic performance estimates. Reported generalization might not necessarily be reproducible for proteins highly dissimilar from the pretraining set. In this work, we measure the effects of data leakage from protein language model pretraining in the domain of protein thermostability prediction. Specifically, we compare two different dataset split strategies: a pretraining-aware split, designed to avoid similarity between pretraining data and the held-out test sets, and a commonly-used naive split, relying on clustering the training data for a downstream task without taking the pretraining data into account. Our experiments suggest that data leakage from language model pretraining shows consistent effects on melting point prediction across all experiments, distorting the measured performance. The source code and our dataset splits are available at https://github.com/tfiedlerdev/pretraining-aware-hotprot .",
    "full_text": null
}