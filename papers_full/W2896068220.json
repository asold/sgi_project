{
  "title": "Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language Modeling",
  "url": "https://openalex.org/W2896068220",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4223199573",
      "name": "Winata, Genta Indra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222272762",
      "name": "Madotto, Andrea",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222053679",
      "name": "Wu, Chien-Sheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743188307",
      "name": "Fung, Pascale",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W761725120",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2805459554",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2149778059",
    "https://openalex.org/W2100578792",
    "https://openalex.org/W2798779216",
    "https://openalex.org/W2807157666",
    "https://openalex.org/W2314691132",
    "https://openalex.org/W2061272101",
    "https://openalex.org/W2949335953",
    "https://openalex.org/W1996430422",
    "https://openalex.org/W2474824677",
    "https://openalex.org/W44716798"
  ],
  "abstract": "Building large-scale datasets for training code-switching language models is challenging and very expensive. To alleviate this problem using parallel corpus has been a major workaround. However, existing solutions use linguistic constraints which may not capture the real data distribution. In this work, we propose a novel method for learning how to generate code-switching sentences from parallel corpora. Our model uses a Seq2Seq model in combination with pointer networks to align and choose words from the monolingual sentences and form a grammatical code-switching sentence. In our experiment, we show that by training a language model using the augmented sentences we improve the perplexity score by 10% compared to the LSTM baseline.",
  "full_text": "LEARN TO CODE-SWITCH: DATA AUGMENTATION USING COPY MECHANISM\nON LANGUAGE MODELING\nGenta Indra Winata, Andrea Madotto, Chien-Sheng Wu, Pascale Fung\nCenter for Artiﬁcial Intelligence Research (CAiRE)\nDepartment of Electronic and Computer Engineering\nHong Kong University of Science and Technology, Clear Water Bay, Hong Kong\n{giwinata, amadotto, cwuak}@connect.ust.hk, pascale@ece.ust.hk\nABSTRACT\nBuilding large-scale datasets for training code-switching lan-\nguage models is challenging and very expensive. To alle-\nviate this problem using parallel corpus has been a major\nworkaround. However, existing solutions use linguistic con-\nstraints which may not capture the real data distribution.\nIn this work, we propose a novel method for learning how to\ngenerate code-switching sentences from parallel corpora. Our\nmodel uses a Seq2Seq model in combination with pointer net-\nworks to align and choose words from the monolingual sen-\ntences and form a grammatical code-switching sentence. In\nour experiment, we show that by training a language model\nusing the augmented sentences we improve the perplexity\nscore by 10% compared to the LSTM baseline.\nIndex Terms— code-switch, bilingual, copy attention,\nlanguage modeling, data augmentation\n1. INTRODUCTION\nLanguage mixing has been a common phenomenon in mul-\ntilingual communities. It is motivated in response to social\nfactors as a way of communication in a multicultural so-\nciety. From a sociolinguistic perspective, individuals do\ncode-switching in order to construct an optimal interaction\nby accomplishing the conceptual, relational-interpersonal,\nand discourse-presentational meaning of conversation [1]. In\nits practice, the variation of code-switching will vary due to\nthe traditions, beliefs, and normative values in the respective\ncommunities. A number of studies [2, 3, 4, 5] found that\ncode-switching is not produced indiscriminately, but follows\nsyntactic constraints. Many linguists formulated various con-\nstraints to deﬁne a general rule for code-switching [2, 4, 5].\nHowever, the constraints are not enough to make a good\ngeneralization of real code-switching constraints, and they\nhave not been tested in large-scale corpora for many language\npairs.\nThis work is partially funded by ITS/319/16FP of the Innovation Tech-\nnology Commission, HKUST 16214415 & 16248016 of Hong Kong Re-\nsearch Grants Council, and RDC 1718050-0 of EMOS.AI.\nOne of the biggest problem in code-switching is collect-\ning large scale corpora. Speech data have to be collected from\na spontaneous speech by bilingual speakers and the code-\nswitching has to be triggered naturally during the conversa-\ntion. In order to solve the data scarcity issue, code-switching\ndata generation is useful to increase the volume and variance.\nA linguistics constraint-driven generation approach such as\nequivalent constraint [6, 7] is not restrictive to languages with\ndistinctive grammar structure.\nIn this paper, we propose a novel language-agnostic\nmethod to learn how to generate code-switching sentences\nby using a pointer-generator network [8]. The model is\ntrained from concatenated sequences of parallel sentences\nto generate code-switching sentences, constrained by code-\nswitching texts. The pointer network copies words from\nboth languages and pastes them into the output, generating\ncode switching sentences in matrix language to embedded\nlanguage and vice versa. The attention mechanism helps the\ndecoder to generate meaningful and grammatical sentences\nwithout needing any sequence alignment. This idea is also\nin line with code-mixing by borrowing words from the em-\nbedded language [9] and intuitively, the copying mechanism\ncan be seen as an end-to-end approach to translate, align, and\nreorder the given words into a grammatical code-switching\nsentence. This approach is the uniﬁcation of all components\nin the work of [6] into a single computational model. A\ncode-switching language model learned in this way is able\nto capture the patterns and constraints of the switches and\nmitigate the out-of-vocabulary (OOV) issue during sequence\ngeneration. By adding the generated sentences and incorpo-\nrating syntactic information to the training data, we achieve\nbetter performance by 10% compared to an LSTM baseline\n[10] and 5% to the equivalent constraint.\n2. RELATED WORK\nThe synthetic code-switching generation approach was in-\ntroduced by adapting equivalence constraint on monolingual\nsentence pairs during the decoding step on an automatic\narXiv:1810.10254v2  [cs.CL]  30 Oct 2018\nAttention\nDistribution \ni 'm\nEN words\ntogoing check 要\nZH words\n检去 查我 <SOS>我 要\nVocabulary\nDistribution \n× pgen\nFinal distribution\ncheck\npgen\ncontext\nvector \n× (1− )pgen\n去\nInput\nI 'm going to check 我  要  去  检 查\n \nGenerated Sentences \n我  要  去  check\ni 'm going to check\n我  们  要  去  check\nFig. 1. Pointer Generator Networks [8]. The ﬁgure shows the example of input and 3-best generated sentences.\nspeech recognition (ASR) model [6]. [11] explored Func-\ntional Head Constraint, which was found to be more re-\nstrictive than the Equivalence Constraint, but complex to be\nimplemented, by using a lattice parser with a weighted ﬁnite-\nstate transducer. [12] extended the RNN by adding POS\ninformation to the input layer and factorized output layer\nwith a language identiﬁer. Then, Factorized RNN networks\nwere combined with an n-gram backoff model using linear\ninterpolation [13]. [14] added syntactic and semantic features\nto the Factorized RNN networks. [15] adapted an effective\ncurriculum learning by training a network with monolingual\ncorpora of both languages, and subsequently train on code-\nswitched data. A further investigation of Equivalence Con-\nstraint and Curriculum Learning showed an improvement in\nlanguage modeling [7]. A multi-task learning approach was\nintroduced to train the syntax representation of languages by\nconstraining the language generator [10].\nA copy mechanism was proposed to copy words directly\nfrom the input to the output using an attention mechanism\n[16]. This mechanism has proven to be effective in several\nNLP tasks including text summarization [8], and dialog sys-\ntems [17]. The common characteristic of these tasks is parts\nof the output are exactly the same as the input source. For ex-\nample, in dialog systems the responses most of the time have\nappeared in the previous dialog steps.\n3. METHODOLOGY\nWe use a sequence to sequence (Seq2Seq) model in com-\nbination with pointer and copy networks [8] to align and\nchoose words from the monolingual sentences and gener-\nate a code-switching sentence. The models’ input is the\nconcatenation of the two monolingual sentences, denoted\nas [wℓ1\n1 ,...,w ℓ1\nn ,wℓ2\n1 ,...,w ℓ2\nm], and the output is a code-\nswitched sentence, denoted as ycs\n1 ,...,y cs\nk . The main as-\nsumption is that almost all, the token present in the code-\nswitching sentence are also present in the source monolingual\nsentences. Our model leverages this property by copying in-\nput tokens, instead of generating vocabulary words. This ap-\nproach has two major advantages: (1) the learning complexity\ndecreases since it relies on copying instead of generating; (2)\nimprovement in generalization, the copy mechanism could\nproduce words from the input that are not present in the\nvocabulary.\n3.1. Pointer-generator Network\nInstead of generating words from a large vocabulary space\nusing a Seq2Seq model with attention [18], pointer-generator\nnetwork [8] is proposed to copy words from the input to the\noutput using an attention mechanism and generate the output\nsequence using decoders. The network is depicted in Figure\n1. For each decoder step, a generation probability pgen ∈\n[0,1] is calculated, which weights the probability of generat-\ning words from the vocabulary, and copying words from the\nsource text. pgen is a soft gating probability to decide whether\ngenerating the next token from the decoder or copying the\nword from the input instead. The attention distribution at is\na standard attention with general scoring [18]. It considers\nall encoder hidden states to derive the context vector. The\nvocabulary distribution Pvocab(w) is calculated by concate-\nnating the decoder state st and the context vector h∗\nt .\npgen = σ(wT\nh∗ h∗\nt + wT\ns st + wT\nx xt + bptr) (1)\nwhere wh∗ ,ws,wx are trainable parameters and bptr is the\nscalar bias. The vocabulary distribution Pvocab(w) and the\nattention distribution at are weighted and summed to obtain\nthe ﬁnal distribution P(w). The ﬁnal distribution is calculated\nas follows:\nP(w) =pgenPvocab(w) + (1−pgen)\n∑\ni:wi=w\nat\ni (2)\nTable 1. Data Statistics of SEAME Phase II and Generated\nSequences using Pointer-generator Network [10].\nSEAME Phase II Generated Seqs\nTrain Dev Test 1-best 3-best\n# Speakers 138 8 8 - -\n# Utterances 78,815 4,764 3,933 78,815 236,445\n# Tokens 1.2M 65K 60K - -\n# Tokens\nPreprocessed 978K 53K 48K 945K 2.8M\nAvg. segment 4.21 3.59 3.99 4.77 4.31\nAvg. switches 2.94 3.12 3.07 2.51 2.79\nTable 2. Code-Switching Sentence Generation Results.\nHigher BLEU and lower perplexity (PPL) is better.\nDev Test\nBLEU PPL BLEU\nseq2seq with attention 53.71 5.89 56.10\npointer-generator 55.19 4.61 59.68\nWe use a beam search to select N-best code-switching sen-\ntences and concatenate the generated sentence with the train-\ning set to form a larger dataset. The result of the gener-\nated code-switching sentences is showed in Table 1. As our\nbaseline, we compare our proposed method with three other\nmodels: (1) We use Seq2Seq with attention; (2) We gener-\nate sequences that satisfy Equivalence Constraint [6]. The\nconstraint doesn’t allow any switch within a crossing of two\nword alignments. We use FastAlign [19] as the word aligner1;\n(3) We also form sentences using the alignments without any\nconstraint. The number of the generated sentences are equiv-\nalent to 3-best data from the pointer-generator model. To in-\ncrease the generation variance, we randomly permute each\nalignment to form a new sequence.\n3.2. Language Modeling\nThe quality of the generated code-switching sentences is eval-\nuated using a language modeling task. Indeed, if the perplex-\nity in this task drops consistently we can assume that the gen-\nerated sentences are well-formed. Hence, we use an LSTM\nlanguage model with weight tying [20] that can capture an\nunbounded number of context words to approximate the prob-\nability of the next word. Syntactic information such as Part-\nof-speech (POS) [p1,...,p T ] is added to further improve the\nperformance. The POS tags are generated phrase-wise using\npretrained English and Chinese Stanford POS Tagger [21] by\nadding a word at a time in a unidirectional way to avoid any\nintervention from future information. The word and syntax\nunit are represented as a vector xw and xp respectively. Next,\nwe concatenate both vectors and use it as an input [xw|xp] to\nan LSTM layer similar to [10].\n1The code can be found at https://github.com/clab/fast align\nTable 3. Language Modeling Results (in perplexity).\nw/o syntax with syntax\nDev Test Dev Test\nRNNLM [10] 178.35 171.27 - -\nLSTM [10] 150.65 153.06 147.44 148.38\nwith augmentation\nrandom switch 166.70 158.87 153.46 151.08\nequivalent-constraint 149.72 147.03 147.48 145.05\npointer-generator 1-best 145.05 144.26 143.39 140.96\npointer-generator 3-best 144.69 143.84 142.84 138.91\n4. EXPERIMENT\n4.1. Corpus\nIn our experiment, we use a conversational Mandarin-English\ncode-switching speech corpus called SEAME Phase II (South\nEast Asia Mandarin-English). The data are collected from\nspontaneously spoken interviews and conversations in Singa-\npore and Malaysia by bilinguals [22]. As the data preprocess-\ning, words are tokenized using Stanford NLP toolkit [23] and\nall hesitations and punctuations were removed except apos-\ntrophe. The split of the dataset is identical to [10] and it is\nshowed in Table 1.\n4.2. Training Setup\nIn this section, we present the experimental settings for\npointer-generator network and language model. Our ex-\nperiment, our pointer-generator model has 500-dimensional\nhidden states and word embeddings. We use 50k words\nas our vocabulary for source and target. We evaluate our\npointer-generator performance using BLEU 2 score. We take\nthe best model as our generator and during the decoding\nstage, we generate 1-best and 3-best using beam search with\na beam size of 5. For the input, we build a parallel mono-\nlingual corpus by translating the mixed language sequence\nusing Google NMT 3 to English ( wℓ1 ) and Mandarin ( wℓ2 )\nsequences. Then, we concatenate the translated English and\nMandarin sequences and assign code-switching sequences as\nthe labels (ycs).\nThe baseline language model is trained using RNNLM\n[24]4. Then, we train our 2-layer LSTM models with a hid-\nden size of 500 and unrolled for 35 steps. The embedding\nsize is equal to the LSTM hidden size for weight tying. We\noptimize our model using SGD with initial learning rates of\n{10,20}. If there is no improvement during the evaluation,\nwe reduce the learning rate by a factor of 0.75. In each time\nstep, we apply dropout to both embedding layer and recur-\nrent network. The gradient is clipped to a maximum of 0.25.\nPerplexity measure is used in the evaluation.\n2BLEU is computed using multi bleu.perl from MOSES package\n3Google NMT Translate API\n4RNNLM toolkit from http://www.ﬁt.vutbr.cz/∼imikolov/rnnlm/\n0 2 4 6 8\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6values\ncs\n1-best\n3-best\n0 1 2 3 4 5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nproposal\n country\ncs\n1-best\n3-best\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nlog term frequencies\n0\n2\n4\n6\n8values\nreconstruct\n building\ncs\n1-best\n3-best\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nlog term frequencies\n0\n5\n10\n15\nchristmas /uni00000003/uni00000003\ncs\n1-best\n3-best\nFig. 2. Univariate data distribution for unigram (top-\nleft), bigram (top-right), trigram (bottom-left), and fourgram\n(bottom-right). The showed n-grams are sampled from 3-best\ndata pointer-generator model.\n5. RESULTS\nThe pointer-generator signiﬁcantly outperforms the Seq2Seq\nwith attention model by 3.58 BLEU points on the test set as\nshown in Table 2. Our language modeling result is given in\nTable 3. Based on the empirical result, adding generated sam-\nples consistently improve the performance of all models with\na moderate margin around 10% in perplexity. After all, our\nproposed method still slightly outperforms the heuristic from\nlinguistic constraint. In addition, we get a crucial gain on per-\nformance by adding syntax representation of the sequences.\nChange in data distribution: To further analyze the\ngenerated result, we observed the distribution of real code-\nswitching data and the generated code-switching data. From\nFigure 2, we can see that 1-best and real code-switching\ndata have almost identical distributions. The distributions are\nleft-skewed where the overall mean is less than the median.\nInterestingly, the distribution of the 3-best data is less skewed\nand generates a new set of n-grams such as “ 那个 (that)\nproposal” which was learned from other code-switching se-\nquences. As a result, generating more samples effects the\nperformance positively.\nImportance of Linguistic Constraint: The result in Ta-\nble 3 emphasizes that linguistic constraints have some signif-\nicance in replicating the real code-switching patterns, speciﬁ-\ncally the equivalence constraint. There is a slight reduction in\nperplexity around 6 points on the test set. In addition, when\nwe ignore the constraint, we lose performance because it still\nallows switches in the inversion grammar cases.\nDoes the pointer-generator learn how to switch? We\nfound that our pointer-generator model generates sentences\nthat have not been seen before. The example in Figure 1\nshows that our model is able to construct a new well-formed\nsentence such as “ 我 们 要 去 (We want to) check”. It is\nalso shown that the pointer-generator model has the capability\nto learn the characteristics of the linguistic constraints from\ndata without any word alignment between the matrix and em-\nbedded languages. On the other hand, training using 3-best\ndata obtains better performance compared to 1-best data. We\nfound a positive correlation from Table 1, where 3-best data\nis more similar to the test set in terms of segment length and\nnumber of switches compared to 1-best data. Adding more\nsamples N may improve the performance, but it will be sat-\nurated at a certain point. One way to solve this is by using\nmore parallel samples.\n6. CONCLUSION\nWe introduce a new learning method for code-switching sen-\ntence generation using a parallel monolingual corpus that\nis applicable to any language pair. Our experimental result\nshows that adding generated sentences to the training data,\neffectively improves our model performance. Combining the\ngenerated samples with code-switching dataset reduces per-\nplexity. We get further performance gain after using syntactic\ninformation of the input. In future work, we plan to explore\nreinforcement learning for sequence generation and employ\nmore parallel corpora.\n7. REFERENCES\n[1] Rakesh M Bhatt and Agnes Bolonyai, “Code-switching\nand the optimal grammar of bilingual language use,”\nBilingualism: Language and Cognition, vol. 14, no. 4,\npp. 522–546, 2011.\n[2] Shana Poplack, Syntactic structure and social function\nof code-switching, vol. 2, Centro de Estudios Puertor-\nrique˜nos,[City University of New York], 1978.\n[3] Carol W Pfaff, “Constraints on language mixing: in-\ntrasentential code-switching and borrowing in span-\nish/english,” Language, pp. 291–318, 1979.\n[4] Shana Poplack, “Sometimes i’ll start a sentence in span-\nish y termino en espanol: toward a typology of code-\nswitching1,” Linguistics, vol. 18, no. 7-8, pp. 581–618,\n1980.\n[5] Hedi M Belazi, Edward J Rubin, and Almeida Jacque-\nline Toribio, “Code switching and x-bar theory: The\nfunctional head constraint,”Linguistic inquiry, pp. 221–\n237, 1994.\n[6] Ying Li and Pascale Fung, “Code-switch language\nmodel with inversion constraints for mixed language\nspeech recognition,” Proceedings of COLING 2012, pp.\n1671–1680, 2012.\n[7] Adithya Pratapa, Gayatri Bhat, Monojit Choudhury,\nSunayana Sitaram, Sandipan Dandapat, and Kalika Bali,\n“Language modeling for code-mixing: The role of lin-\nguistic theory based synthetic data,” in Proceedings of\nthe 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), 2018,\nvol. 1, pp. 1543–1553.\n[8] Abigail See, Peter J. Liu, and Christopher D. Manning,\n“Get to the point: Summarization with pointer-generator\nnetworks,” in Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers). 2017, pp. 1073–1083, Associa-\ntion for Computational Linguistics.\n[9] John M Lipski, “Code-switching or borrowing? no s´e so\nno puedo decir, you know,” in Selected proceedings of\nthe second workshop on Spanish sociolinguistics. Cas-\ncadilla Proceedings Project Somerville, MA, 2005, pp.\n1–15.\n[10] Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu,\nand Pascale Fung, “Code-switching language modeling\nusing syntax-aware multi-task learning,” inProceedings\nof the Third Workshop on Computational Approaches to\nLinguistic Code-Switching. 2018, pp. 62–67, Associa-\ntion for Computational Linguistics.\n[11] LI Ying and Pascale Fung, “Language modeling with\nfunctional head constraint for code switching speech\nrecognition,” in Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing\n(EMNLP), 2014, pp. 907–916.\n[12] Heike Adel, Ngoc Thang Vu, Franziska Kraus, Tim\nSchlippe, Haizhou Li, and Tanja Schultz, “Recurrent\nneural network language modeling for code switching\nconversational speech,” inAcoustics, Speech and Signal\nProcessing (ICASSP), 2013 IEEE International Confer-\nence on. IEEE, 2013, pp. 8411–8415.\n[13] Heike Adel, Ngoc Thang Vu, and Tanja Schultz, “Com-\nbination of recurrent neural networks and factored lan-\nguage models for code-switching language modeling,”\nin Proceedings of the 51st Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 2: Short\nPapers), 2013, vol. 2, pp. 206–211.\n[14] Heike Adel, Ngoc Thang Vu, Katrin Kirchhoff, Do-\nminic Telaar, and Tanja Schultz, “Syntactic and seman-\ntic features for code-switching factored language mod-\nels,” IEEE Transactions on Audio, Speech, and Lan-\nguage Processing, vol. 23, no. 3, pp. 431–440, 2015.\n[15] Ashutosh Baheti, Sunayana Sitaram, Monojit Choud-\nhury, and Kalika Bali, “Curriculum design for code-\nswitching: Experiments with language identiﬁcation\nand language modeling with deep neural networks,”\nProceedings of ICON, pp. 65–74, 2017.\n[16] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly,\n“Pointer networks,” in Advances in Neural Information\nProcessing Systems, 2015, pp. 2692–2700.\n[17] Andrea Madotto, Chien-Sheng Wu, and Pascale Fung,\n“Mem2seq: Effectively incorporating knowledge bases\ninto end-to-end task-oriented dialog systems,” in Pro-\nceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers).\n2018, pp. 1468–1478, Association for Computational\nLinguistics.\n[18] Thang Luong, Hieu Pham, and Christopher D Manning,\n“Effective approaches to attention-based neural machine\ntranslation,” in Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Processing,\n2015, pp. 1412–1421.\n[19] Chris Dyer, Victor Chahuneau, and Noah A. Smith, “A\nsimple, fast, and effective reparameterization of ibm\nmodel 2,” in Proceedings of the 2013 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies.\n2013, pp. 644–648, Association for Computational Lin-\nguistics.\n[20] Oﬁr Press and Lior Wolf, “Using the output embedding\nto improve language models,” inProceedings of the 15th\nConference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Papers.\n2017, pp. 157–163, Association for Computational Lin-\nguistics.\n[21] Kristina Toutanova, Dan Klein, Christopher D Manning,\nand Yoram Singer, “Feature-rich part-of-speech tagging\nwith a cyclic dependency network,” in Proceedings of\nthe 2003 Conference of the North American Chapter of\nthe Association for Computational Linguistics on Hu-\nman Language Technology-Volume 1. Association for\nComputational Linguistics, 2003, pp. 173–180.\n[22] Universiti Sains Malaysia Nanyang Technological Uni-\nversity, “Mandarin-english code-switching in south-east\nasia ldc2015s04. web download. philadelphia: Linguis-\ntic data consortium,” 2015.\n[23] Christopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven J. Bethard, and David McClosky,\n“The Stanford CoreNLP natural language processing\ntoolkit,” in Association for Computational Linguistics\n(ACL) System Demonstrations, 2014, pp. 55–60.\n[24] Tomas Mikolov, Stefan Kombrink, Anoop Deoras,\nLukar Burget, and Jan Cernocky, “Rnnlm-recurrent neu-\nral network language modeling toolkit,” in Proc. of the\n2011 ASRU Workshop, 2011, pp. 196–201.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9437399506568909
    },
    {
      "name": "Computer science",
      "score": 0.8531080484390259
    },
    {
      "name": "Pointer (user interface)",
      "score": 0.6813705563545227
    },
    {
      "name": "Language model",
      "score": 0.6743342876434326
    },
    {
      "name": "Workaround",
      "score": 0.664118766784668
    },
    {
      "name": "Sentence",
      "score": 0.5889161229133606
    },
    {
      "name": "Natural language processing",
      "score": 0.5881785154342651
    },
    {
      "name": "Code (set theory)",
      "score": 0.5691879391670227
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5553786754608154
    },
    {
      "name": "Code-switching",
      "score": 0.5046645402908325
    },
    {
      "name": "Programming language",
      "score": 0.28399544954299927
    },
    {
      "name": "Linguistics",
      "score": 0.10666859149932861
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 21
}