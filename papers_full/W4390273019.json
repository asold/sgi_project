{
  "title": "An Investigation of Applying Large Language Models to Spoken Language Learning",
  "url": "https://openalex.org/W4390273019",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2343654326",
      "name": "Yingming Gao",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5092715834",
      "name": "Baorian Nuchged",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2096860276",
      "name": "Ya Li",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2944918952",
      "name": "Linkai Peng",
      "affiliations": [
        "NetEase (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2343654326",
      "name": "Yingming Gao",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5092715834",
      "name": "Baorian Nuchged",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2096860276",
      "name": "Ya Li",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2944918952",
      "name": "Linkai Peng",
      "affiliations": [
        "NetEase (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2016114400",
    "https://openalex.org/W3123665557",
    "https://openalex.org/W2888781318",
    "https://openalex.org/W2103651905",
    "https://openalex.org/W343636949",
    "https://openalex.org/W6780218876",
    "https://openalex.org/W3034861927",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4392669753",
    "https://openalex.org/W4327564965",
    "https://openalex.org/W4382240547",
    "https://openalex.org/W4387162877",
    "https://openalex.org/W2619383789",
    "https://openalex.org/W4376226279",
    "https://openalex.org/W2896348597",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4381786045",
    "https://openalex.org/W6846648116",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2130623574",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W4385572162",
    "https://openalex.org/W6631362777",
    "https://openalex.org/W3177100526",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W4205452697",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W4281557260"
  ],
  "abstract": "People have long desired intelligent conversational systems that can provide assistance in practical scenarios. The latest advancements in large language models (LLMs) are making significant strides toward turning this aspiration into a tangible reality. LLMs are believed to hold the most potential and value in education, especially in the creation of AI-driven virtual teachers that facilitate language learning. This study focuses on assessing the effectiveness of LLMs within the educational domain, specifically in the areas of spoken language learning, which encompass phonetics, phonology, and second language acquisition. To this end, we first introduced a new multiple-choice question dataset to evaluate the effectiveness of LLMs in the aforementioned scenarios, including the understanding and application of spoken language knowledge. Moreover, we investigated the influence of various prompting techniques such as zero- and few-shot methods (prepending the question with question-answer exemplars), chain-of-thought (CoT) prompting, in-domain exemplars, and external tools. We conducted a comprehensive evaluation of popular LLMs (20 distinct models) using these methods. The experimental results showed that the task of extracting conceptual knowledge posed few challenges for these LLMs, whereas the task of application questions was relatively difficult. In addition, some widely proven effective prompting methods combined with domain-specific examples resulted in significant performance improvements compared to the zero-shot baselines. Additionally, some other preliminary experiments also demonstrated the strengths and weaknesses of different LLMs. The findings of this study can shed light on the application of LLMs to spoken language learning.",
  "full_text": null,
  "topic": "Spoken language",
  "concepts": [
    {
      "name": "Spoken language",
      "score": 0.739301860332489
    },
    {
      "name": "Computer science",
      "score": 0.5458254814147949
    },
    {
      "name": "Task (project management)",
      "score": 0.5295363664627075
    },
    {
      "name": "Written language",
      "score": 0.42866265773773193
    },
    {
      "name": "Psychology",
      "score": 0.3220939636230469
    },
    {
      "name": "Artificial intelligence",
      "score": 0.30574291944503784
    },
    {
      "name": "Linguistics",
      "score": 0.27416688203811646
    },
    {
      "name": "Engineering",
      "score": 0.1066322922706604
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}