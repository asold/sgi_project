{
    "title": "Vision Transformers For Weeds and Crops Classification Of High Resolution UAV Images",
    "url": "https://openalex.org/W3197076153",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287086389",
            "name": "Reedha, Reenul",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287086390",
            "name": "Dericquebourg, Eric",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287086391",
            "name": "Canals, Raphael",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3193443207",
            "name": "Hafiane Adel",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3195223586",
        "https://openalex.org/W2969262604",
        "https://openalex.org/W2154579312",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W169539560",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2869138134",
        "https://openalex.org/W3039926407",
        "https://openalex.org/W3119997354",
        "https://openalex.org/W2994866182",
        "https://openalex.org/W2267186426",
        "https://openalex.org/W3146366485",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2804078698",
        "https://openalex.org/W3177138146",
        "https://openalex.org/W3049361076",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3136950817",
        "https://openalex.org/W3083841824",
        "https://openalex.org/W2767767563",
        "https://openalex.org/W2165698076",
        "https://openalex.org/W2091142597",
        "https://openalex.org/W2886554959",
        "https://openalex.org/W2914490130",
        "https://openalex.org/W2800002789",
        "https://openalex.org/W2600445444",
        "https://openalex.org/W2913227116",
        "https://openalex.org/W3005863531",
        "https://openalex.org/W2755766995",
        "https://openalex.org/W3004479823",
        "https://openalex.org/W2898498213",
        "https://openalex.org/W3022695253",
        "https://openalex.org/W2995187556",
        "https://openalex.org/W3047050685",
        "https://openalex.org/W2900646070",
        "https://openalex.org/W3035682985",
        "https://openalex.org/W2394911398",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W3000652771"
    ],
    "abstract": "Crop and weed monitoring is an important challenge for agriculture and food production nowadays. Thanks to recent advances in data acquisition and computation technologies, agriculture is evolving to a more smart and precision farming to meet with the high yield and high quality crop production. Classification and recognition in Unmanned Aerial Vehicles (UAV) images are important phases for crop monitoring. Advances in deep learning models relying on Convolutional Neural Network (CNN) have achieved high performances in image classification in the agricultural domain. Despite the success of this architecture, CNN still faces many challenges such as high computation cost, the need of large labelled datasets, ... Natural language processing's transformer architecture can be an alternative approach to deal with CNN's limitations. Making use of the self-attention paradigm, Vision Transformer (ViT) models can achieve competitive or better results without applying any convolution operations. In this paper, we adopt the self-attention mechanism via the ViT models for plant classification of weeds and crops: red beet, off-type beet (green leaves), parsley and spinach. Our experiments show that with small set of labelled training data, ViT models perform better compared to state-of-the-art CNN-based models EfficientNet and ResNet, with a top accuracy of 99.8\\% achieved by the ViT model.",
    "full_text": "Vision Transformers For Weeds and Crops\nClassiﬁcation Of High Resolution UA V Images\nReenul Reedha, Eric Dericquebourg, Raphael Canals, Adel Haﬁane\nINSA CVL, University of Orleans, PRISME EA 4229, Bourges, 18022,France\nAbstract\nCrop and weed monitoring is an important challenge for agriculture and food production nowadays. Thanks to recent advances\nin data acquisition and computation technologies, agriculture is evolving to a more smart and precision farming to meet with\nthe high yield and high quality crop production. Classiﬁcation and recognition in Unmanned Aerial Vehicles (UA V) images are\nimportant phases for crop monitoring. Advances in deep learning models relying on Convolutional Neural Network (CNN) have\nachieved high performances in image classiﬁcation in the agricultural domain. Despite the success of this architecture, CNN\nstill faces many challenges such as high computation cost, the need of large labelled datasets, ... Natural language processing’s\ntransformer architecture can be an alternative approach to deal with CNN’s limitations. Making use of the self-attention paradigm,\nVision Transformer (ViT) models can achieve competitive or better results without applying any convolution operations. In this\npaper, we adopt the self-attention mechanism via the ViT models for plant classiﬁcation of weeds and crops: red beet, off-type\nbeet (green leaves), parsley and spinach. Our experiments show that with small set of labelled training data, ViT models perform\nbetter compared to state-of-the-art CNN-based models EfﬁcientNet and ResNet, with a top accuracy of 99.8% achieved by the\nViT model.\nKeywords— Computer vision, deep learning, self-attention, UA V , classiﬁcation, agriculture\nI. I NTRODUCTION\nAgriculture is at the heart of scientiﬁc evolution and innovation\nto face major challenges for achieving high yield produc-\ntion while protecting plants growth and quality to meet the\nanticipated demands on the market [1]. However, a major\nproblem arising in modern agriculture is the excessive use\nof chemicals to boost the production yield and to get rid of\nunwanted plants such as weeds from the ﬁeld [2]. Weeds are\ngenerally considered harmful to agricultural production [3].\nThey compete directly with crop plants for water, nutrients\nand sunlight [4]. Herbicides are often used in large quantities\nby spraying all over agricultural ﬁelds which has however\nshown various concerns like air, water and soil pollution\nand promoting weed resistance to such chemicals [2]. If the\nrate of usage of herbicides remains the same, in the near\nfuture, weeds will become fully resistant to these products and\neventually destroy the harvest [5]. This is why weed and crop\ncontrol management is becoming an essential ﬁeld of research\nnowadays [6].\nAutomated crop monitoring system is a practical solution that\ncan be beneﬁcial both economically and environmentally. Such\nsystem can reduce labour costs by making use of robots to\nremove weeds and hence minimising the use of herbicides\n[7]. The foremost step to an automatic weed control system is\nthe detection and mapping of weeds on the ﬁeld which can be\na challenging part as weeds and crop plants often have similar\ncolours, textures, and shapes [4]. The use of Unmanned Aerial\nVehicles (UA Vs) has proved signiﬁcant results for mapping\nweed density across a ﬁeld by collecting RGB images ([8],\n[9], [10], [11], [12]) or multispectral images ([13], [14], [15],\n[16], [17]) covering the whole ﬁeld. As UA Vs ﬂy over the\nﬁeld at an elevated altitude, the images captured cover a large\nground surface area and these large images can be split into\nsmaller tiles to facilitate their processing ([18], [19], [20])\nbefore feeding them to learning algorithms to identify and\nclassify a weed from a crop plant.\nIn the agricultural domain, the main approach to plant detec-\ntion is to ﬁrst extract vegetation from the image background\nusing segmentation and then distinguish crops from the weeds\n[21]. Common segmentation approaches use multispectral in-\nformation to separate the vegetation from the background (soil\nand residuals) [22]. However, weeds and crops are difﬁcult\nto distinguish from one another even while using spectral\ninformation because of their strong similarities [23]. This point\nhas also been highlighted in [6], in which the authors reported\nthe importance of using both spectral and spatial features to\nidentify weeds in crops. Recently, deep learning (DL) became\nan essential approach in image classiﬁcation, object detection\nand recognition [24], [25] notably in the agricultural domain\n[26]. DL models with CNN-like architectures, ruling in com-\nputer vision tasks so far, have been the standard in image\nclassiﬁcation and object detection [27], [28], [29]. CNN uses\nconvolutional ﬁlters on an image to extract important features\nto understand the object of interest in an image with the help\nof convolutional operations covering key properties such as\nlocal connection, parameters (weight) sharing and translation\nequivariance [30], [24]. Most of the papers covering weed\ndetection or classiﬁcation make use of CNN-based model\nstructures [31], [32], [33] such as AlexNet [28], VGG-19,\nVGG-16 [34], GoogLeNet [35], ResNet-50, ResNet-101 [29]\nand Inception-v3 [36].\nOn the other hand, attention mechanism has seen a rapid devel-\nopment particularly in natural language processing (NLP) [37]\nand has shown spectacular performance gains when compared\nto previous generation of state-of-the-art models [38]. In vision\narXiv:2109.02716v2  [cs.CV]  22 Oct 2021\napplications the use of attention mechanism has been much\nmore limited, due to the high computational cost as the number\nof pixels in an image is much larger than the number of units\nof words in NLP applications. This makes it impossible to\napply standard attention models to images. A recent survey of\napplications of transformer networks in computer vision can\nbe found in [39]. The recently-proposed vision transformer\n(ViT) appears to be a major step towards adopting transformer-\nattention models for computer vision tasks [40]. Considering\nimage patches as units of information for training is a ground-\nbreaking method compared to CNN-based models considering\nimage pixels. ViT incorporates image patches into a shared\nspace and learns the relation between these patches using self-\nattention modules. Given massive amounts of training data and\ncomputational resources, ViT was shown to surpass CNNs in\nimage classiﬁcation accuracy [40]. Vision transformer models\nhave not been used yet for weeds and crops classiﬁcation of\nhigh resolution UA V images.\nIn this paper, we adopt the self-attention paradigm via vision\ntransformers for the classiﬁcation of images of weeds and\ndifferent crops: red leaves beet, green leaves beet, parsley and\nspinach, taken by a high resolution digital camera mounted on\na UA V . We make use of the self-attention mechanism on small,\nlabelled plant images using the convolutional-free ViT model,\nshowing its outperformance compared to current state-of-the-\nart CNN-based models ResNet and EfﬁcientNet. Furthermore,\nwe show that when training with fewer number of labelled\nimages, the ViT model performs better than the CNN-based\nmodels. The rest of the paper is organised as follows. Section\n2 presents the materials and methods used as well as a brief\ndescription of the self-attention mechanism and the vision\ntransformer model architecture. The experimental results and\nanalysis are presented in Section 3. Section 4 reﬂects the\nconclusion and work perspectives.\nII. M ATERIALS AND METHODS\nThis part outlines the acquisition, preparation and manual\nlabelling of the dataset acquired from a high resolution camera\nmounted on a UA V . It also presents a brief description of\nthe self-attention paradigm and the vision transformer model\narchitecture.\nA. Image collection and annotation\nThe study area is located at the agricultural ﬁelds of beet,\nparsley and spinach present in the region of Centre Val de\nLoire, in France. It is also a region with many pedo-climatic\nadvantages: the region has limited rainfall and clay-limestone\nsoils with good ﬁltering capacity. Irrigation is also offered on\n95% of the plots.\nTo survey the study areas, a \" Starfury\", Pilgrim technologies\nUA V was equipped with a Sony ILCE-7R , 36 mega pixels\ncamera as shown in Figure 1. The camera is mounted on a 3-\naxis stabilised brushless gimbal on the drone in order to keep\nthe camera axis stable even during strong winds. The drone\nwas ﬂown at an altitude of 30 m over the beet ﬁeld in a light\nmorning fog and at an altitude of 20 m in a sunny weather for\nthe parsley and spinach ﬁelds. The drone followed a speciﬁc\nﬂight plan and the camera captured RGB images at regular\nintervals as shown in Figure 2. The images captured have\nrespectively a minimum longitudinal and lateral overlapping of\n70% and 50-60% depending on the ﬁelds vegetation coverage\nand homogeneity, assuring a better and complete coverage of\nthe whole ﬁeld of 4ha (40 000 m²) and improving the accuracy\nof the ﬁeld orthophoto generation.\n(a) Starfury Drone\n (b) Sony ILCE-7R Camera\nFig. 1: Apparatus used for data acquisition.\nFig. 2: Overlay of the orthophoto on google earth of the\nspinach plot (left) and the ﬂight plan (right) across a spinach\nﬁeld (the images are taken along the yellow lines at regular\nintervals to ensure sufﬁcient overlapping).\nFig. 3: Example of image captured from a spinach study site.\nThe data was then manually labelled using bounding boxes\nwith the tool LabelImg (https://github .com/tzutalin/labelImg)\nand then divided into 5 classes as shown in Figure 4 and\nTable I below.\n2\n(a) Weeds (b) Beet\n(c) Off-type green leaves beet (d) Parsley\n(e) Spinach\nFig. 4: This overview shows sample images patches of all 5\nclasses of our custom dataset. The images measure 64 x 64\npixels. Each class contains 3200 to 4000 images.\nClass Number\nWeed 4 000\nBeet 4 000\nOff-type Beet 3 265\nParsley 4 000\nSpinach 4 000\nTABLE I: Class Distribution\nThe labelled dataset has been classiﬁed in 5 folders for each\nclass label and each containing an equal number of images\nas presented in Table I. We have 16.9% off-type beet plants\n(obtained by data augmentation - ﬂips and rotations from 765\nlabelled data) and equally 20.8% images for the four other\nclasses. For a total of 19 265 images of size 64x64. The dataset\nis then divided into training, validation and testing sets as\nshown in Figure 11.\nB. Image preprocessing\nDue to the huge labour cost of the whole process of super-\nvision training, artiﬁcial data augmentation was used in the\nexperiment to generate additional new images and increase the\namount of data to solve the problem of insufﬁcient agricultural\nimage dataset of weeds and crops. Image data augmentation is\nnot only used to expand the training dataset and attempting to\ncover real-world scenarios but also to improve the performance\nand ability of the model to generalise on agricultural images\nas they can vary a lot depending on the soil, environment,\nseasons and climate conditions.\nAs a data preprocessing method, data augmentation plays an\nimportant role in deep learning [41]. In general, effective\ndata augmentation can better improve the robustness of the\nmodel and obtain stronger generalisation capabilities. We\nthus employed data augmentation strategies, which have been\nwidely used in practice, so as to enrich the datasets. After\nnormalising each image, the following steps have been applied:\nrandom rotation, random resized crop, random horizontal ﬂip,\ncolour jitters and rand augment (Cubuk et al.). This technique\nis implemented using keras ImageDataGenerator, generating\naugmented images on the ﬂy while the model is still in the\ntraining stage.\nC. Self attention for weeds detection\nAttention mechanism is becoming a key concept in the deep\nlearning ﬁeld [43]. Attention was inspired by the human\nperception process where the human tends to focus on parts of\ninformation, ignoring other perceptible parts of information at\nthe same time. The attention mechanism has had a profound\nimpact on the ﬁeld of natural language processing, where the\ngoal was to focus on a subset of important words. The self-\nattention paradigm has emerged from the concepts attention\nshowing improvement in the performance of deep networks\n[38].\nLets denote a sequence of n entities (x1, x2, ...,xn) by X ∈Rn×d,\nwhere d is the embedding dimension to represent each en-\ntity. The goal of self-attention is to capture the interaction\namongst all n entities by encoding each entity in terms of\nthe global contextual information. This is done by deﬁning\nthree learnable weight matrices, Queries ( WQ ∈Rn×dq ), Keys\n(WK ∈Rn×dk ) and Values ( WV ∈Rn×dv ). The input sequence\nX is ﬁrst projected onto these weight matrices to get Q =\nXW Q, K = XW KandV = XW V .\nThe attention matrix A ∈Rn×dv indicates a score between N\nqueries Q and KT keys representing which part of the input\nsequence to focus on.\nA(Q, K) =σ(QKT ) (1)\nwhere σ is an activation function, usually so f tmax(). To\ncapture the relations among the input sequence, the values\nV are weighted by the scores from Equation 1. Resulting in\n[40],\nSel f Attention(Q, K,V ) =A(Q, K) ·V\n⇒Sel f Attention(Q, k,V ) =so f tmax( QKT\n√dk\n) ·V (2)\nwhere dk is dimension of the input queries.\nIf each pixel in a feature map is regarded as a random variable\nand the paring covariances are calculated, the value of each\npredicted pixel can be enhanced or weakened based on its\nsimilarity to other pixels in the image. The mechanism of\nemploying similar pixels in training and prediction and ignor-\ning dissimilar pixels is called the self-attention mechanism.\nIt helps to relate different positions of a single sequence of\nimage patches in order to gain a more vivid representation of\nthe whole image [44].\n3\nThe transformer network is an extension of the attention mech-\nanism from Equation 2 based on the Multi-Head Attention\noperation. It is based on running k self-attention operations,\ncalled “heads”, in parallel, and project their concatenated\noutputs [38]. This helps the transformer jointly attend to\ndifferent information derived from each head. The output\nmatrix is obtained by the concatenation of each attention heads\nand a dot product with the weight WO. Hence, generating\nthe output of the multi-headed attention layer. The overall\noperation is summarised by the equations below [38].\nMultiHead (Q, K,V ) =Concat(head1, ...,headh)WO\nwhereheadi = Attention (QWQ\ni , KW K\ni ,VW V\ni )\n(3)\nwhere WQ\ni ,WK\ni ,WV\ni are weight matrices for queries, keys and\nvalues respectively and WO ∈Rhdv×dmodel .\nBy using the self-attention mechanism, global reference can\nbe realised during the training and prediction of models.\nThis helps in reducing by a considerable amount training\ntime of the model to achieve high accuracy [40]. The self-\nattention mechanism is an integral component of transformers,\nwhich explicitly models the interactions between all entities\nof a sequence for structured prediction tasks. Basically, a\nself-attention layer updates each component of a sequence\nby aggregating global information from the complete input\nsequence. While, the convolution layers’ receptive ﬁeld is a\nﬁxed K ×K neighbourhood grid, the self-attention’s receptive\nﬁeld is the full image. The self-attention mechanism increases\nthe receptive ﬁeld compared to the CNN without adding\ncomputational cost associated with very large kernel sizes\n[45]. Furthermore, self-attention is invariant to permutations\nand changes in the number of input points. As a result, it\ncan easily operate on irregular inputs as opposed to standard\nconvolution that requires grid structures [39].\nFig. 5: Attention mechanism on an image patch (left) contain-\ning weeds (in green) and beet plant (in red). With the original\nimage on the left and the attention map on the right obtained\nwith ViT-B16 model. The attention map shows the model’s\nattention on the different plants: with a dark blue and purple\ncolour pixel representing the attention on the weeds and a light\nblue colour pixel representing the beet plant.\nFig. 6: Attention map generated from layers 7 to 12 of the\nViT-B16 model on an image of a weed.\nAverage attention weights of all heads mean heads across\nlayers and the head in the same layer. Basically, the area has\nevery attention in the transformer which is called attention\npattern or attention matrix. When the patch of the weed image\nis passed through the transformer, it will generate the attention\nweight matrix for the image patches. For example, when\npatch 1 is passed through the transformer, self-attention will\ncalculate how much attention should pay to others (patch 2,\npatch 3, ...). And every head will have one attention pattern as\nshown in Figure 6 and ﬁnally, they will sum up all attention\npatterns (all heads). We can observe that the model tries to\nidentify the object (weed) on the image and tries to focus its\nattention on it (as it stands out from the background).\nAn attention mechanism is applied to selectively give more\nimportance to some of the locations of the image compared to\nothers, for generating caption(s) corresponding to the image.\nAnd consequently, this helps to focus on the main differences\nbetween weeds and crops in an images and improves the\nlearning of the model to identify the contrasts between these\nplants. This mechanism also helps the model to learn features\nfaster, and eventually decreases the training cost [40].\nD. Vision Transformers\nTransformer models were major headway in natural language\nprocessing (NLP). They became the standard for modern NLP\ntasks and they brought spectacular performance yields when\ncompared to the previous generation of state-of-the-art models\n[38]. Recently, it was reviewed and introduced to computer\nvision and image classiﬁcation aiming to show that this\nreliance on CNNs is not necessary anymore in object detection\nor image classiﬁcation and a pure transformer applied directly\nto sequences of image patches can perform very well on image\nclassiﬁcation tasks [40].\nFigure 7 presents the architecture of the vision transformer\nused in this paper for weed and crop classiﬁcation. It is\nbased on the ﬁrst developed ViT model by Dosovitskiy et al.\n[40]. The model architecture consists of 7 main steps. Firstly,\n4\nFig. 7: ViT model architecture based on original ViT model [40].\nFig. 8: Positional embeddings as vector representations.\nthe input image is split into smaller ﬁxed-size patches. Then\neach patch is ﬂattened into a 1-D vector. The input sequence\nconsists of the ﬂattened vector (2D to 1D) of pixel values from\na patch of size 16×16.\nFor an input image,\n(x) ∈RH×W×C (4)\nand patch size P, N image patches are created\n(x)P ∈RN×P×P×C (5)\nwith\nN = HW\nP ×P (6)\nwhere N is the sequence length (token) similar to the words\nof a sentence, (H,W) is the resolution of the original image\nand C is the number of channels [40].\nAfterwards, each ﬂattened element is then fed into a linear\nprojection layer that will produce what is called the “patch\nembedding”. There is one single matrix, represented as ‘E’\n(embedding) used for the linear projection. A single patch\nis taken and ﬁrst unrolled into a linear vector as shown in\nFigure 8. This vector is then multiplied with the embedding\nmatrix E. The ﬁnal result is then fed to the transformer,\nalong with the positional embedding. In the 4 th phase, the\nposition embeddings are linearly added to the sequence of\nimage patches so that the images can retain their positional\ninformation. It injects information about the relative or ab-\nsolute position of the image patches in the sequence. The\nnext step is to attach an extra learnable (class) embedding to\nthe sequence according to the position of the image patch.\nThis class embedding is used to predict the class of the\ninput image after being updated by self-attention. Finally,\nthe classiﬁcation is performed by just stacking a multilayer\nperceptron (MLP) head on top of the transformer, at the\nposition of the extra learnable embedding that has been added\nto the sequence.\n5\nIII. P ERFORMANCE EVALUATION\nWe made use of recent implementations of ViT-B32 and ViT-\nB16 models as well as EfﬁcientNet and ResNet models. The\nalgorithms were built on top of a Tensorﬂow 2.4.1 and Keras\n2.4.3 frameworks using Python 3.6.9. To run and evaluate our\nmethods, we used the following hardware; an Intel Xeon(R)\nCPU E5-1620 v4 3.50 GHz x 8 processor (CPU) with 16\nGB of RAM, and a graphics processing unit (GPU) NVIDIA\nQuadro M2000 with an internal RAM of 4 GB under the Linux\noperating system Ubuntu 18.04 LTS (64 bits).\nAll models were trained using the same parameters in order\nto have an unbiased and reliable comparison between their\nperformance. The initial learning rate was set to 0.0001 with\na reducing factor of 0.2. The batch size was set to 8 and the\nmodels were trained for 100 epochs with an early stopping\nafter a wait of 10 epochs without better scores. The models\nused, ViT-B16, ViT-B32, EfﬁcientNet B0, EfﬁcientNet B1 and\nResNet 50 were loaded from the keras library with pre-trained\nweights of \"ImageNet\".\nA. Cross Validation\nThe experiments have been carried out using the cross-\nvalidation technique to ensure the integrity and accuracy of\nthe models. We used stratiﬁed K-Fold with the variation of\nthe proportion of training and validation folders. Therefore,\nthe dataset was divided into k folders using certain percentage\nof the data as a validation set and the rest as a training\nset. Stratiﬁed is to ensure that each fold contains the same\nproportion of each label. Thus stratiﬁed K-Fold is one of the\nbest approaches in ensuring that the data is well balanced and\nshufﬂed in each folds before splitting them into validation and\ntraining sets.\nFig. 9: Stratiﬁed ﬁve-folds cross validation, leaving one out\nfor validation and the remaining 4 folds are used for training.\nDark blue representing validation folds, light blue colour folds\nare used as training set and yellow colour folds are used as\ntesting set containing unprocessed images. This generates 5\ntrained models.\nThe data was ﬁrst shufﬂed randomly and divided equally into\n5 folders, each containing an equal number of classes and\nthe performances of the tested models were evaluated using\nthe stratiﬁed ﬁve-folds cross validation leaving k folds as\nvalidation set (where 1 ≤k ≤4). Figure 10 shows how the\ndata was splitted and divided into 5 folders each containing\nand equal number of classes. Using Equation 7 (with n = 5 and\nk = 2), this results in 10 training models. Increasing the value\nof k (number of validation folders), decreases the number of\ntraining folders and thus forces the model to train on a smaller\ndataset. This helps to evaluate how well the models perform on\nreduced training datasets and their capacities to extract features\nfrom a few images. The number of combinations (splits) of\nthe train-validation is as follows:\nnCk = n!\nk!(n −k)! (7)\nwhere n is the number of folders and k is the number of\nvalidation folds.\nFig. 10: Stratiﬁed ﬁve-folds cross validation and leaving two\nout as validation set and the rest are used for training resulting\nin 10 different models. Dark blue representing validation folds\nand light blue colour folds are used as training set.\nThe labelled dataset is divided into training, validation and\ntesting sets for three sets of experiments. The number of\ntesting images is increased for each experiments, consequently\ndecreasing the number of training and validation images.\nExperiment 1 contains 12 844 training images, Experiment\n2 contains half the dataset for testing (9633 images) and 7706\ntraining images and Experiment 3 contains only 4535 training\nimages for 13 596 testing images. Each set of experiments is\nthen trained using the cross validation technique leaving one\nfold as validation set.\nFig. 11: Variation of training/validation set and testing image\nset for 3 sets of experiments. The training/validation set is used\nfor the cross validation as shown in Figure 9 and Figure 10.\nB. Evaluation metrics\nIn the collected dataset, each image has been manually classi-\nﬁed into one of the categories: weeds, off-type beet (green\n6\nleaves beet), beet (red leaves), parsley or spinach, called\nground-truth data. By running the classiﬁers on a test set,\nwe obtained a label for each testing image, resulting in the\npredicted classes. The classiﬁcation performance is measured\nby evaluating the relevance between the ground-truth labels\nand the predicted ones resulting in classiﬁcation probabilities\nof true positives (TP), false positives (FP) and false negatives\n(FN). We then calculate a recall measure representing how\nwell a model correctly predicts all the ground-truth classes\nand a precision representing the ratio of how many of the\npositive predictions were correct relative to all the positive\npredictions.\nPrecision = T P\nT P+ FP\nRecall = T P\nT P+ FN\n(8)\nThe metrics used in the evaluation procedure were the pre-\ncision, recall and F1-Score. The latter being the weighted\naverage of precision and recall, hence considering both false\npositives and false negatives into account to evaluate the\nperformance of the model.\nF1 −Score = 2 ×(Recall ×Precision)\n(Recall + Precision) (9)\nSince we used cross validation techniques to evaluate the\nperformance of each model, we calculated the mean ( µ) and\nstandard deviation ( σ) of the F1-scores of the model in order\nto have an average overview of its performance. The equations\nused are presented below:\nµF1−Score = ∑N\ni=1(F1 −Scorei)\nN\nσF1−Score =\n√\n∑N\ni=1(F1 −Scorei −µF1−Score)2\nN\n(10)\nwhere N is the number of splits generated from the cross\nvalidation procedure. For instance, one leave out generates ﬁve\nsplits (N = 5) using Equation 7 as shown in Figure 9.\nAs for the loss metrics, we used the cross-entropy loss function\nbetween the true classes and predicted classes.\nIV. R ESULTS AND DISCUSSION\nState-of-the-art CNN-based architectures, ResNet and Efﬁ-\ncientNet were trained along the ViT-B16 and ViT-B32 in\norder to compare their performances on our custom dataset\ncomprising of 5 classes (weeds, beet, off-type beet, parsley\nand spinach). All models have been trained using the ﬁve-folds\ncross validation leaving one out technique. The accuracies and\nlosses of the models tend to be ﬂat after the 30th epoch.\nThe average F1-Scores and losses obtained with 3211 testing\nimages (original and unprocessed images except for off-type\nbeet images) are reported in Table II below.\nModel µF1−Score Loss\nViT B-16 0.998 ±0.002 0.656\nViT B-32 0.996 ±0.002 0.672\nEfﬁcientNet B0 0.987 ±0.005 0.735\nEfﬁcientNet B1 0.989 ±0.005 0.720\nResNet 50 0.995 ±0.005 0.716\nTABLE II: Comparison table between state-of-the-art CNN-\nbased models and vision transformer models on agricultural\nimage classiﬁcation. The F1-Score has been calculated using\nEquation 10 with N = 5.\nFrom these experimental results, we notice the outperformance\nof the ViT models compared to the CNN-based models with\na best F1-Score of 99.8% for the ViT B-16 model although\nthe ViT B-32 model’s performance is very close behind at\n99.6% with a minimum loss of 0.656 for the ViT B-16 model.\nThe EfﬁcientNet and ResNet models fall behind compared to\nViT models but with high scores nevertheless, having been\ntrained on a large dataset (12 844 training images). These\nexperimental results conﬁrm vision transformers high perfor-\nmance compared to current state-of-the-art models ResNet and\nEfﬁcientNet as presented by Dosovitskiy et al.. Although all\nnetwork families obtain high accuracy and precision, the clas-\nsiﬁcation of crops and weed images using vision transformer\nyields the best prediction performance.\nA. Inﬂuence of the training set size\nIn the next stage we tried to answer the question of which\nnetwork family yields the best performance with a smaller\ntraining dataset. We did so by carrying out a ﬁve-folds cross\nvalidation leaving k out, where k is a varying parameter from\n1 to 4 while keeping the testing set to 3211 images to evaluate\nthe performance of the models.\nVarying the number of training images has a direct inﬂuence\non the performance of the trained ViT model, as shown in\nTable III. The results obtained with the ﬁve-folds cross vali-\ndation, leaving two out as validation set (k=2) are promising,\nwith a mean F1-Score of 99.7% and a standard deviation of\n0.1% showing a very small difference between the scores of\nthe 10 generated models. We notice a very small decrease\nin performance of the ViT B-16 model while reducing the\nnumber of training images. We note a very light decrease\nof 0.1% in the accuracy of the ViT B-16 model while\ntraining only with 2/5 of the dataset (6422 images, k=3)\nand validating on the remaining 3/5. With k=4, the ViT B-\n16 model was trained with a smaller dataset of 3211 images\n(75% reduction), and its performance decreased as expected\nbut by a small margin of only 0.44% for an overall accuracy\nof 99.63%. These experimental results shows how well the\nvision transformer models perform with small datasets. The\nViT B-16 model makes use of the self-attention mechanism\nto learn recognisable patterns from few images in order to\nachieve such high accuracy.\n7\nClasses\nk-Folds k=1 k=2 k=3 k=4\nPrec RecµF1−Score Prec RecµF1−Score Prec RecµF1−Score Prec RecµF1−Score\nWeeds 1.000 1.000 1.000±0.0001.000 0.989 0.994±0.0021.000 0.987 0.993±0.0020.993 0.976 0.985±0.003\nOff-Type Beet 0.992 1.000 0.996±0.0010.988 1.00 0.994±0.0020.985 0.999 0.992±0.0020.974 0.992 0.983±0.004\nBeet 1.000 1.000 1.000±0.0000.999 1.000 0.999±0.0010.999 1.000 0.999±0.0010.998 0.999 0.999±0.001\nParsley 1.000 1.000 1.000±0.0001.000 1.000 1.000±0.0011.000 1.000 1.000±0.0000.999 1.000 0.999±0.000\nSpinach 1.000 1.000 1.000±0.0001.000 1.000 1.000±0.0001.000 1.000 1.000±0.0000.999 1.000 0.999±0.001\nTABLE III: Comparison of classiﬁcation reports generated from 5-Fold cross validation leaving k folds out (where k-folds\nstands for the number of validation folds) with 1 ≤k ≤4. k = 1 represents the most number of training images (12 844) and k\n= 4 represents the lowest number of training images (3 211). The average precision (Prec), recall (Rec) and F1-Score, obtained\nusing Equation 10 are reported for each class obtained with the ViT B-16 model.\nFig. 12: Comparison between ViT B-16, EfﬁcientNet B0,\nEfﬁcientNet B1 and ResNet50 on their respective performance\nwith different number of training images.\nWe then compared the performance of the ViT B-16 model\nto state-of-the-art CNN-based models ResNet and EfﬁcientNet\nwith a decreasing number of training images. The experimental\nresults of their F1-Scores are reported in the Figure 12. We\nnotice a decrease in the F1-Scores of the ResNet50, Efﬁcient-\nNet B0 and EfﬁcientNet B1 with a reduction in the number\nof training images. In contrast, the ViT B-16 model keeps its\nhigh performance in this set of experiments, specially with the\nsmallest number of training images, achieving an F1-Score of\n99.63%. On the other hand, RestNet50 scores an accuracy of\n97.54%, EfﬁcientNet B0 scores 96.53% and EfﬁcientNet B1\nwith the worst score of 95.91%. EfﬁcientNet B1 has the worst\ndecrease in performance of 3.07% (from 98.98% - with 12 844\ntraining images to 95.91% - with 3211 training images). Even\nthough EfﬁcientNet B1 achieves better results with the largest\ndataset (98.98% accuracy) than EfﬁcientNet B0 (98.78%),\nits performance falls off the most with the smallest training\ndataset. While the F1-Scores of ResNet and EfﬁcientNet B0\nand B1 declines with a reduction of training images by 25%\n(from 12 844 images to 9 633 images), the ViT B-16 model\nstill achieves a high performance of 99.75% (a slight decrease\nfrom 99.80%). These experimental results show the outper-\nformance of vision transformer models over current CNN-\nbased models ResNet and EfﬁcientNet in agricultural image\nclassiﬁcation when dealing with small training datasets.\nNumber of testing imagesViT B-16 EfﬁcientNetB0 EfﬁcientNetB1 ResNet50\n3211 99.80% 98.78% 98.98% 99.50%\n9633 99.61% 96.60% 98.59% 98.59%\n13596 99.14% 97.40% 94.10% 95.32%\nTABLE IV: Comparison of performances of ViT B-16, Efﬁ-\ncientNet B0, EfﬁcientNet B1 and ResNet50 with a decreasing\nnumber of training images while using a 5-Fold cross valida-\ntion technique leaving one fold out as validation. The average\nF1-Scores for each model, obtained using Equation 10 are\nreported for three set of experiments with a varying number\nof testing images.\nFurthermore, we compared the performance of the models\nwith a variation in the number of testing images while using\na 5-Folds leaving one fold out cross validation technique. The\nF1-Scores are reported in Table IV. As shown in Figure 13,\nthere is a notable decrease in the F1-Scores of the four models\nwhile testing with 9633 and 13596 images and training with\nonly 50% and 30% of the labelled dataset. On the third set\nof experiments, the models were trained on only 4535 images\nand validating on 1134 images, which explains the decrease in\ntheir performances. Even though all models have a decrease in\ntheir F1-Scores with an increasing number of testing images,\nthe ViT B-16 model still achieves higher performance than the\nstate-of-the art CNN-based models, EfﬁcientNetB0, Efﬁcient-\nNetB1 and ResNet50. The ViT B-16 model had the smallest\ndecrease in performance from 99.80% (for 3211 testing images\n8\nFig. 13: Comparison between ViT B-16, EfﬁcientNet B0,\nEfﬁcientNet B1 and ResNet50 on their respective performance\nwith different number of testing set while keeping 5-Fold cross\nvalidation leaving one fold as validation set.\nand 12 844 training images) to 99.14% (for 13 596 testing\nimages and 4535 training images).\nV. C ONCLUSION\nIn this study, we used the self-attention paradigm via the\nvision transformer models to learn and classify custom crops\nand weeds images acquired by UA V in beet, parsley and\nspinach ﬁelds. The results achieved with these datasets indicate\na promising direction in the use of vision transformers in\nagricultural problems. Outperforming current state-of-the-art\nCNN-based models like ResNet and EfﬁcientNet, the base\nViT model is to be preferred over the other models for its\nhigh accuracy and its low calculation cost. Furthermore, the\nViT B-16 model has proven better with its high performance\nspecially with small training datasets where other models\nfailed to achieve such high accuracy. This shows how well\nthe convolutional-free, ViT model interprets an image as a\nsequence of patches and processes it by a standard transformer\nencoder, using the self-attention mechanism, to learn patterns\nbetween weeds and crops images. In this respect, we come\nto conclusion that the application of vision transformer could\nchange the way to tackle vision tasks in agricultural applica-\ntions for image classiﬁcation by bypassing classic CNN-based\nmodels. In future works, we plan to use vision transformer\nclassiﬁer as a backbone in an object detection architecture\nto locate and identify weeds and plants on UA V orthopho-\ntos.\nACKNOWLEDGEMENT\nThis work was carried out as a part of DESHERBROB\n(2020-2024) project funded by the Region Centre-Val de\nLoire (France). We gratefully acknowledge Region Centre-Val\nde Loire for its support. We thank the company FRASEM,\npartner of this project for their valuable provision of plots and\ndata.\nREFERENCES\n[1] P. Radoglou-Grammatikis, P. Sarigiannidis, T. Lagkas,\nand I. Moscholios, “A compilation of UA V\napplications for precision agriculture,” Computer\nNetworks, 2020. [Online]. Available: https://doi .org/\n10.1016/j.comnet.2020.107148\n[2] U. T, A. S. M, and A. K, “Effect of Herbicides on Living\nOrganisms in The Ecosystem and Available Alternative\nControl Methods,”International Journal of Scientiﬁc and\nResearch Publications (IJSRP) , 2020.\n[3] D. D. Patel and B. A. Kumbhar, “Weed and its man-\nagement: A major threats to crop economy,” Journal\nof. Pharmaceutical Sciences and Bioscientiﬁc Research ,\n2016.\n[4] N. Iqbal, S. Manalil, B. Chauhan, and S. Adkins, “In-\nvestigation of alternate herbicides for effective weed\nmanagement in glyphosate-tolerant cotton,” Archives of\nAgronomy and Soil Science , 02 2019.\n[5] S. Vrbni ˇcanin, D. Pavlovi ´c, and D. Boži ´c, “Weed Resis-\ntance to Herbicides,” Herbicide Resistance in Weeds and\nCrops, 2017.\n[6] W. Z. A. Wang and X. Wei, “A review on weed detection\nusing ground-based machine vision and image processing\ntechniques,” Computers and electronics in agriculture ,\n2019.\n[7] X. Wu, S. Aravecchia, P. Lottes, C. Stachniss, and\nC. Pradalier, “Robotic weed control using automated\nweed and crop classiﬁcation,” Journal of Field Robotics,\n2020.\n[8] C. Donmez, O. Villi, S. Berberoglu, and A. Cilek,\n“Computer vision-based citrus tree detection in a\ncultivated environment using UA V imagery,”Computers\nand Electronics in Agriculture, 2021. [Online]. Available:\nhttps://doi.org/10.1016/j.compag.2021.106273\n[9] M. D. Bah, A. Haﬁane, and R. Canals, “CRowNet: Deep\nNetwork for Crop Row Detection in UA V Images,”IEEE\nAccess, 2020.\n[10] H. Huang, J. Deng, Y . Lan, A. Yang, X. Deng, and\nL. Zhang, “A fully convolutional network for weed\nmapping of unmanned aerial vehicle (UA V) imagery,”\nPLoS ONE, 2018.\n[11] H. Huang, Y . Lan, A. Yang, Y . Zhang,\nS. Wen, and J. Deng, “Deep learning versus\nObject-based Image Analysis (OBIA) in weed\nmapping of UA V imagery,” International Journal\nof Remote Sensing , 2020. [Online]. Available:\nhttps://doi.org/10.1080/01431161.2019.1706112\n[12] L. Petrich, G. Lohrmann, M. Neumann, F. Martin,\nA. Frey, A. Stoll, and V . Schmidt, “Detection of\nColchicum autumnale in drone images, using a machine-\nlearning approach,” Precision Agriculture , 2020.\n[Online]. Available: https://doi.org/10.1007/s11119-020-\n09721-7\n[13] A. Puerto, C. Pedraza, D. A. Jamaica-Tenjo, and A. Os-\norio Delgado, “A deep learning approach for weed\n9\ndetection in lettuce crops using multispectral images,”\nAgriEngineering, 2020.\n[14] W. Ramirez, P. Achanccaray Diaz, L. Mendoza, and\nM. Pacheco, “Deep convolutional neural networks for\nweed detection in agricultural crops using optical aerial\nimages,” ISPRS - International Archives of the Pho-\ntogrammetry, Remote Sensing and Spatial Information\nSciences, 2020.\n[15] S. Patidar, U. Singh, S. Sharma, and Himanshu, “Weed\nseedling detection using mask regional convolutional\nneural network,” 2020 International Conference on\nElectronics and Sustainable Communication Systems\n(ICESC), 2020.\n[16] I. Sa, Z. Chen, M. Popovic, R. Khanna, F. Liebisch,\nJ. Nieto, and R. Siegwart, “Weednet: Dense semantic\nweed classiﬁcation using multispectral images and mav\nfor smart farming,” IEEE Robotics and Automation Let-\nters, 2017.\n[17] I. Sa, M. Popovic, R. Khanna, Z. Chen, P. Lottes,\nF. Liebisch, J. Nieto, C. Stachniss, A. Walter, and\nR. Siegwart, “Weedmap: A large-scale semantic weed\nmapping framework using aerial multispectral imaging\nand deep neural network for precision farming,” Remote\nSensing, 2018.\n[18] A. dos Santos Ferreira, D. Matte Freitas, G. Gonçalves\nda Silva, H. Pistori, and M. Theophilo Folhes, “Weed\ndetection in soybean crops using convnets,” Computers\nand Electronics in Agriculture , 2017. [Online].\nAvailable: https://www.sciencedirect.com/science/article/\npii/S0168169917301977\n[19] A. Milioto, P. Lottes, and C. Stachniss, “Real-time blob-\nwise sugar beets vs weeds classiﬁcation for monitoring\nﬁelds using convolutional neural networks,” ISPRS An-\nnals of the Photogrammetry, Remote Sensing and Spatial\nInformation Sciences, 2017.\n[20] A. N. V . Sivakumar, J. Li, S. Scott, E. Psota, A. J.\nJhala, J. D. Luck, and Y . Shi, “Comparison of object\ndetection and patch-based classiﬁcation deep learning\nmodels on mid-to late-season weed detection in UA V\nimagery,” Remote Sensing, 2020.\n[21] J. Kang, L. Liu, F. Zhang, C. Shen, N. Wang, and\nL. Shao, “Semantic segmentation model of cotton roots\nin-situ image based on attention mechanism,” Computers\nand Electronics in Agriculture, 2021. [Online]. Available:\nhttps://doi.org/10.1016/j.compag.2021.106370\n[22] M. Kerkech, A. Haﬁane, and R. Canals, “Vine disease\ndetection in uav multispectral images with deep learning\nsegmentation approach,” arXiv, 2019.\n[23] E. Hamuda, M. Glavin, and E. Jones, “A survey of\nimage processing techniques for plant extraction and\nsegmentation in the ﬁeld,” Computers and Electronics in\nAgriculture, 2016. [Online]. Available: http://dx .doi.org/\n10.1016/j.compag.2016.04.024\n[24] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,”\nNature, 2015.\n[25] S. Hochreiter and J. Schmidhuber, “Long short-term\nmemory,” Neural computation, 1997.\n[26] A. S. Hasan, F. Sohel, D. Diepeveen, H. Laga, and M. G.\nJones, “A survey of deep learning techniques for weed\ndetection from images,” Computers and Electronics in\nAgriculture, 2021.\n[27] Y . Lecun, “Generalization and network design strategies,”\nTechnical Report, 1989.\n[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet\nclassiﬁcation with deep convolutional neural networks,”\nCurran Associates, Inc. , 2012.\n[29] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image recognition,” CoRR, 2015. [Online].\nAvailable: http://arxiv.org/abs/1512.03385\n[30] Y . Lecun, B. Boser, J. Denker, D. Henderson, R. Howard,\nW. Hubbard, and L. Jackel, “Handwritten digit recogni-\ntion with a back-propagation network,” Neural networks,\ncurrent applications, 1989.\n[31] D. Nkemelu, D. Omeiza, and N. Lubalo, “Deep convolu-\ntional neural network for plant seedlings classiﬁcation,”\nComputer Vision and Pattern Recognition , 2018.\n[32] H. K. Suh, J. IJsselmuiden, J. W. Hofstee, and E. J.\nvan Henten, “Transfer learning for the classiﬁcation of\nsugar beet and volunteer potato under ﬁeld conditions,”\nBiosystems Engineering , 2018. [Online]. Available:\nhttps://doi.org/10.1016/j.biosystemseng.2018.06.017\n[33] M. Dian Bah, A. Haﬁane, and R. Canals, “Deep learning\nwith unsupervised data labeling for weed detection in line\ncrops in UA V images,”Remote Sensing, 2018.\n[34] K. Simonyan and A. Zisserman, “Very deep convo-\nlutional networks for large-scale image recognition,”\nCoRR, 2014.\n[35] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. E.\nReed, D. Anguelov, D. Erhan, V . Vanhoucke, and\nA. Rabinovich, “Going deeper with convolutions,”CoRR,\n2014. [Online]. Available: http://arxiv.org/abs/1409.4842\n[36] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and\nZ. Wojna, “Rethinking the inception architecture for\ncomputer vision,” CoRR, 2015. [Online]. Available:\nhttp://arxiv.org/abs/1512.00567\n[37] D. Hu, “An introductory survey on attention mechanisms\nin nlp problems,” IntelliSys, 2019.\n[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” Advances in Neural Infor-\nmation Processing Systems , 2017.\n[39] S. Khan, M. Naseer, M. Hayat, S. W. Zamir,\nF. S. Khan, and M. Shah, “Transformers in Vision:\nA Survey,” arXiv, 2021. [Online]. Available: http:\n//arxiv.org/abs/2101.01169\n[40] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and\nN. Houlsby, “An Image is Worth 16x16 Words: Trans-\nformers for Image Recognition at Scale,” CoRR, 2020.\n[Online]. Available: http://arxiv.org/abs/2010.11929\n[41] S. J. Pan and Q. Yang, “A survey on transfer learning,”\n10\nIEEE Transactions on Knowledge and Data Engineering,\n2010.\n[42] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V . Le, “Randaug-\nment: Practical automated data augmentation with a re-\nduced search space,” IEEE Computer Society Conference\non Computer Vision and Pattern Recognition Workshops,\n2020.\n[43] Z. Niu, G. Zhong, and H. Yu, “A review on the attention\nmechanism of deep learning,” Neurocomputing, 2021.\n[Online]. Available: https://www .sciencedirect.com/\nscience/article/pii/S092523122100477X\n[44] J. Cheng, L. Dong, and M. Lapata, “Long short-term\nmemory-networks for machine reading,” CoRR, 2016.\n[Online]. Available: http://arxiv.org/abs/1601.06733\n[45] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena,\n“Self-attention generative adversarial networks,” 36th\nInternational Conference on Machine Learning, ICML\n2019, 2019.\n11"
}