{
  "title": "Neural Data Augmentation for Legal Overruling Task: Small Deep Learning Models vs. Large Language Models",
  "url": "https://openalex.org/W4393113494",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2594221561",
      "name": "Reshma Sheik",
      "affiliations": [
        "National Institute of Technology Tiruchirappalli"
      ]
    },
    {
      "id": "https://openalex.org/A5094228538",
      "name": "K. P. Siva Sundara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2145729279",
      "name": "S. Jaya Nirmala",
      "affiliations": [
        "National Institute of Technology Tiruchirappalli"
      ]
    },
    {
      "id": "https://openalex.org/A2594221561",
      "name": "Reshma Sheik",
      "affiliations": [
        "National Institute of Technology Tiruchirappalli"
      ]
    },
    {
      "id": "https://openalex.org/A5094228538",
      "name": "K. P. Siva Sundara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2145729279",
      "name": "S. Jaya Nirmala",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3174828871",
    "https://openalex.org/W2251658415",
    "https://openalex.org/W2610850660",
    "https://openalex.org/W2963545917",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2786273134",
    "https://openalex.org/W2395579298",
    "https://openalex.org/W3186492090",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2151975921",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2905266130",
    "https://openalex.org/W2963149098",
    "https://openalex.org/W6600493712",
    "https://openalex.org/W6837901829",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3035565536",
    "https://openalex.org/W6631442365",
    "https://openalex.org/W2998184481",
    "https://openalex.org/W3153451655",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4285285618",
    "https://openalex.org/W3176923149",
    "https://openalex.org/W3180181113",
    "https://openalex.org/W3186332848",
    "https://openalex.org/W2991266149",
    "https://openalex.org/W3042881361",
    "https://openalex.org/W6602599378",
    "https://openalex.org/W4206699105",
    "https://openalex.org/W3048797457",
    "https://openalex.org/W3103291112",
    "https://openalex.org/W3202026671",
    "https://openalex.org/W3098341425"
  ],
  "abstract": "Abstract Deep learning models produce impressive results in any natural language processing applications when given a better learning strategy and trained with large labeled datasets. However, the annotation of massive training data is far too expensive, especially in the legal domain, due to the need for trained legal professionals. Data augmentation solves the problem of learning without labeled big data. In this paper, we employ pre-trained language models and prompt engineering to generate large-scale pseudo-labeled data for the legal overruling task using 100 data samples. We train small recurrent and convolutional deep-learning models using this data and fine-tune a few other transformer models. We then evaluate the effectiveness of the models, both with and without data augmentation, using the benchmark dataset and analyze the results. We also test the performance of these models with the state-of-the-art GPT-3 model under few-shot setting. Our experimental findings demonstrate that data augmentation results in better model performance in the legal overruling task than models trained without augmentation. Furthermore, our best-performing deep learning model trained on augmented data outperforms the few-shot GPT-3 by 18% in the F1-score. Additionally, our results highlight that the small neural networks trained with augmented data achieve outcomes comparable to those of other large language models.",
  "full_text": "Neural Processing Letters (2024) 56:121\nhttps://doi.org/10.1007/s11063-024-11574-4\nNeural Data Augmentation for Legal Overruling Task: Small\nDeep Learning Models vs. Large Language Models\nReshma Sheik 1 · K. P. Siva Sundara 2 · S. Jaya Nirmala 1\nAccepted: 15 February 2024 / Published online: 23 March 2024\n© The Author(s) 2024\nAbstract\nDeep learning models produce impressive results in any natural language processing applica-\ntions when given a better learning strategy and trained with large labeled datasets. However,\nthe annotation of massive training data is far too expensive, especially in the legal domain,\ndue to the need for trained legal professionals. Data augmentation solves the problem of\nlearning without labeled big data. In this paper, we employ pre-trained language models and\nprompt engineering to generate large-scale pseudo-labeled data for the legal overruling task\nusing 100 data samples. We train small recurrent and convolutional deep-learning models\nusing this data and ﬁne-tune a few other transformer models. We then evaluate the effective-\nness of the models, both with and without data augmentation, using the benchmark dataset\nand analyze the results. We also test the performance of these models with the state-of-the-\nart GPT-3 model under few-shot setting. Our experimental ﬁndings demonstrate that data\naugmentation results in better model performance in the legal overruling task than models\ntrained without augmentation. Furthermore, our best-performing deep learning model trained\non augmented data outperforms the few-shot GPT-3 by 18% in the F1-score. Additionally,\nour results highlight that the small neural networks trained with augmented data achieve\noutcomes comparable to those of other large language models.\nKeywords Deep learning · Natural language processing · Data augmentation · Legal\noverruling task · Transformer · Few-shot · GPT-3 · Large language models\nB Reshma Sheik\nrezmasheik@gmail.com\nK. P . Siva Sundara\nsivaparthi1989@gmail.com\nS. Jaya Nirmala\nsjaya@nitt.edu\n1 Department of Computer Science and Engineering, National Institute of Technology, Tiruchirapalli\n620015, India\n2 Department of Electronics and Communication Engineering, Coimbatore Institute of Technology,\nCoimbatore 641013, India\n123\n121 Page 2 of 21 R. Sheik et al.\n1 Introduction\nDeep neural network models have signiﬁcantly progressed in several Natural Language\nProcessing (NLP) applications. The biggest concern of deep learning is preventing overﬁtting\nand obtaining a better-generalized model. The more training data we have, the better the\nperformance of deep learning models in any supervised learning method. Due to the wide\nvariety of NLP tasks, majority of the task-speciﬁc datasets only contain a few thousand to a few\nhundred thousand manually-annotated training examples. In many deep learning techniques,\nthe dataset inﬂuences the problem to be solved. Creating new labeled data is slow, costly,\nand requires trained personnel. Additionally, the labeling process is susceptible to inevitable\nhuman error. Researchers are looking for alternate ways to produce data without manual\nannotation. Data augmentation (DA) is motivated to extend the range of training examples\nwithout explicitly collecting annotated data. Data augmentation can avoid overﬁtting and\nresolve class-imbalance issues by oversampling the unrepresented label with enough training\nsamples. DA is explored well in image processing, and it is simple to accomplish using\ntechniques like ﬂipping, rotating, decolorizing, enhancing the edges of images, etc. However,\nit is considerably more difﬁcult with textual data because it would change the syntactic and\nsemantic construct of the augmented data. It has recently drawn more interest in NLP because\nof the growth in work in low-resource areas [ 1]. In NLP , this could be achieved using deleting\nor adding words, synonym swaps, or various text generation approaches. The process of data\naugmentation in NLP can be addressed via Thesaurus [ 2], word embeddings [ 3–5] back\ntranslation [6], text generation [7], etc., There have been many recent advancements in pre-\ntrained language models and its application in prompting downstream NLP tasks. These\ndevelopments in generative models are revolutionizing the process of data augmentation. In\nthese Large Language models(LLMs), this issue is solved by adding novel learning strategies\nlike few-shot learning and transfer learning [8].\nTransformer models like Generative Pre-trained Transformer 3 (GPT-3) [ 9] with hun-\ndreds of billions of parameters can achieve high-level task-speciﬁc few-shot performance\ncomparable to state-of-the-art models. These models are based on a self-attention mecha-\nnism that allows them to process long text sequences effectively, making them ideal for tasks\nthat require understanding and generating complex language. However, one of the major\nchallenges of using large-scale transformer models is the massive computational resources\nrequired for training and inference, leaving a large carbon footprint and posing a challenge\nfor academics and practitioners to use them.\nThus, we formulate interesting research questions in this direction.\nKey Research Questions\n• RQ1: Can we effectively reduce human annotation effort to train models with fair per-\nformance using augmented data?\n• RQ2: Are student models trained on this generated data more efﬁcient than ﬁne-tuned\nlarge language models?\n• RQ3: Will student models exhibit better scores than the state-of-the-art GPT-3 models\ntested under few-shot setting?\nMotivated by this, the paper focuses on neural data augmentation using LLMs to address\nLegal Overruling Task with limited-sized datasets of 2400 samples from the Casetext law\ncorpus.\n1 The overruling task is a binary classiﬁcation task, with positive examples repre-\nsenting overruling sentences from the law and negative ones representing non-overruling\n1 https://casetext.com/blog/a-benchmark/ .\n123\nNeural Data Augmentation for Legal Overruling Task: Small Deep… Page 3 of 21 121\nTable 1 Example sentences for overruled“1” and not overruled“0” labels from the dataset\nSentence Label\nWe disapprove of the line of cases from the courts of appeal allowing attorney’s\nfees in the case of executory process\n1\nFor these reasons we believe that board of education v. chattin, supra, should be\noverruled to the extent that it conﬂicts with the views herein set out\n1\nChanges in custody or parenting time may be modiﬁed only if the moving party\ndemonstrates that modiﬁcation is justiﬁed by proper cause or because of a\nchange of circumstances\n0\nDATA AUGMENTATION\nLEGAL OVERRULING TASK\nDataset Augmented \nTraining Data\nPredictive Model\nLanguage \nModels\nGenerator\nPrompts\nScore\nDeep Learning \nClassifier\nTest Data Training Data\nAugment\nFig. 1 Framework illustrates the process of data augmentation for legal overruling task\nsentences, as shown in Table 1. An overruling sentence is a declaration that invalidates a\nprevious case decision. The Overruling task holds great signiﬁcance for lawyers as it ensures\nthe validity of legal arguments and prevents overruled cases [ 10]. Identifying and labeling\noverruled sentences requires trained legal professionals, which is costly and time-consuming.\nOur primary objective is to create synthetic data using language models with only 100\ntraining samples. This is because deep learning models require a signiﬁcant amount of data\nto perform well, which may not be readily available for the task of legal overruling. This is\nparticularly relevant in real-world scenarios, where obtaining large gold-standard datasets in\nthe legal domain can be challenging. As a result, our task is both challenging and pertinent to\nreal-world applications. As shown in Fig. 1, we propose a data augmentation approach that\nutilizes a small sample of the available dataset to generate a larger set of training data via the\naugmentation process. We also aim to design a simple deep-learning model architecture that\ncan attain comparable performance to large language models.\nMain Contributions\nThe contributions of this paper can be summed up as follows:\n• We generate training samples for legal overruling task using pre-trained language models\n(GPT-3, GPT-2 [ 11], Bidirectional Encoder Representation from Transformers (BERT)\n[12], a distilled version of BERT (DistilBERT) [ 13], and Robustly Optimized BERT\n(RoBERTa) [14]).\n123\n121 Page 4 of 21 R. Sheik et al.\n• Using this synthetic data, we train student models like Bidirectional Long Short Term\nMemory (BiLSTM), Bidirectional Gated Recurrent Unit (BiGRU), and Convolutional\nBiLSTM (ConvBiLSTM) and ﬁne-tune a few transformer models like BERT, RoBERTa,\nand DistilBERT. Compared the performance of these models with and without augmen-\ntation\n• Our work also explores the performance of large language model GPT-3 under few-shot\nsetting for legal overruling task.\n• Finally, we evaluate and compare all models based on accuracy, F1-score, model size,\nand inference time. We also compare with other neural text augmentation approaches to\npresent the effectiveness of our approach. As a result, we arrived at a simple and efﬁcient\ndeep-learning model for the legal overruling task.\nThe rest of this paper is organized as follows. Section 2 presents the background study of\ntext data augmentation, followed by neural data augmentation, and ends with current works\nin the legal domain. Section 3 covers the techniques used for data augmentation in legal\noverruling task. The model architecture and additional experimental setups for training and\nﬁne-tuning deep learning models are provided in Sect. 4. Section 5 addresses the research\nquestions and analyses the performance of the models. Finally, Sect. 6 concludes this paper\nwith future works.\n2 Background\n2.1 Text Data Augmentation\nData augmentation has recently drawn more attention in NLP due to the growth of work\nin low-resource domains, the emergence of new tasks, and the popularity of deep neural\nnetworks that need lots of training data. One simple and practical approach to performing\ntext augmentation is to substitute words or phrases with their synonyms by utilizing an\nexisting thesaurus to generate a large amount of data quickly. The paper [ 2] chooses a word\nand swaps out its synonyms based on the geometric distribution. In other methods, a related\nword for augmentation is identiﬁed using similarity searches such as k-Nearest Neighbor(k-\nNN), cosine similarity [ 3], and pre-trained traditional word embeddings like word2vec [15],\nGloVe [16], and fasttext [17]. By selecting words from the topic-word and document-topic\ndistributions [ 18], used probabilistic topic models to produce more training instances. To\ndevelop a sequence generation model [ 19], proposed a data augmentation method that uses\ntransformation operations given by technical experts, like a word swap. In the study, [ 5] ﬁnds a\nreplacement for a target word by choosing from its context using a bi-directional deep learning\narchitecture. The model based on Recurrent Neural Networks (RNN) and Convolutional\nNeural Networks (CNN) showed improved classiﬁer performance when applied to various\ntext classiﬁcation tasks. Another work by [ 20] uses deep learning architectures (RNN, CNN)\nto perform multiple transformations on the text, including synonym replacement, random\ninsertion, deletion, and swapping, demonstrating gains in performance when tested on ﬁve\nnatural language processing tasks. Kaﬂe et al. [ 21] created fresh samples for visual question\nanswering using a template-based technique and Long Short Term Memory (LSTM)-based\n[22] strategy. The accuracy of the CNN and LSTM sentence classiﬁcation models improved\nwhen interpolation-based augmentation techniques were adopted at the word and sentence\nembedding levels [ 23].\n123\nNeural Data Augmentation for Legal Overruling Task: Small Deep… Page 5 of 21 121\nIn contrast to the preceding methods, Kaﬂe et al. [ 7] uses techniques to generate the\nentire phrase rather than just one or a few words. In [ 6], the concept of back translation is\nemployed to translate the text information into another language and then translate it back\ninto the original language, which makes it easier to create textual data while maintaining the\ncontext of the text data. Recent data augmentation uses counterfactual examples [ 24], which\nintroduces negations or numeric alterations to ﬂip the sample’s label.\n2.2 Data Augmentation Using Transformers\nThe success of pre-trained transformer models in many NLP applications led to conquering\nthe data augmentation ﬁeld. Earlier transfer learning approaches [ 25] used was to ﬁne-tune the\nBERT model with an extra label-conditional constraint to enhance contextual augmentation.\nThe examples generated by this model can also be used to perform style transfer, where\nthey reverse the original label of the sentence, and the model outputs a new label-compatible\nsentence. However, they depend heavily on the amount of training data, leading to a low\nvariance of training samples in the collection. Elsahar et al. [ 26] uses zero-shot learning\nfor generating questions from knowledge graphs using an encoder-decoder neural network\narchitecture with attention vectors. The study uses a large in-domain training dataset and a\ntransfer learning setting for unknown knowledge base types based on existing ones. However,\nin-domain pretraining seems insufﬁcient; novel pretraining and few-shot learning strategies\nare required in domain-speciﬁc NLP applications.\nRecent improvements in text generation models, such as GPT and its later versions have\nsparked the creation of novel augmentation techniques that produce extra training data from\noriginal samples rather than performing local changes to the text. A similar study [ 27]\nuses GPT-2 models to generate large labeled training sets to train a BERT-based classi-\nﬁer for improving the performance of relation extraction tasks. Another framework [ 28]t h a t\nmakes use of a pre-trained GPT-2 model to produce label-invariant modiﬁcation of the input\ntexts to augment the existing training data for multi-label classiﬁcation showed considerable\ngains over baseline models. In [ 29], two powerful transformer language models, GPT-3 and\nBioBERT [ 30] were tested in few-shot scenarios on several biomedical NLP applications.\nThe study hypothesizes that language models may gain from domain-speciﬁc pretraining in\ntask-speciﬁc few-shot learning. Therefore in [ 31], few-shot learning strategies are used for\nnatural language generation to create a multi-domain table-to-text dataset using only 200\ndata samples. The same few-shot learning approach to natural language generation is used\nin another study [ 32] for data augmentation using GPT-2 models. They adopted a few seed\nselection strategies for extracting the learning samples. Another work language model-based\ndata augmentation (LAMBADA) [ 33] uses GPT-2 to generate new sentences for the class by\nﬁne-tuning them for a particular text classiﬁcation problem. The phrases generated by this\nmethod were ﬁltered using a classiﬁer trained on the original data to produce high-quality\ndata. This method offers an alternative to semi-supervised techniques when unlabelled data\ndoes not exist. Another work [ 34] uses GPT-3 models were used to perform text augmenta-\ntion via prompts and improve the robustness of pre-trained transformer models. In addition\nto the above generative models, in [ 35] demonstrated an effective method for preparing the\npre-trained models for data augmentation is to preﬁx the class labels to text sequences. They\nshowed that the text-to-text BART [ 36] model performs better than other transformer-based\npre-trained models in a low-resource setting. A recent work [ 37] also uses ﬁne-tuned seq2seq\nlanguage models: T5 [ 38] and BART to build new samples for performance improvement in\nvarious classiﬁcation tasks.\n123\n121 Page 6 of 21 R. Sheik et al.\nThe studies and surveys [ 1, 39–41] indicate that data augmentation techniques can improve\nthe performance of models trained on textual data, but the effectiveness of these techniques\ndepends on the speciﬁc task and size of the dataset. However, it is important to note that\nthe unique features of legal text, such as lengthy unstructured documents and legal jargon,\nmake it challenging to apply many of the techniques commonly used in the context of normal\ntext. As a result, additional research and development are necessary to identify effective data\naugmentation strategies for legal text datasets.\n2.3 Data Augmentation for Legal Text\nLegal documents use a separate vocabulary known as “Legalese language\" with words with\nspeciﬁc meanings concerning legal context. Therefore, certain words designated as protected\nwords cannot be changed or altered. For instance, although theft and fraud are similar words\nin general English, they are two distinct categories of crime in legal texts [ 42]. These differ-\nences necessitate careful consideration while choosing an appropriate augmentation strategy.\nAnother work [ 43] aims to tackle the crime prediction problem by applying three different\ntechniques random scramble, random delete, and random insert at the sentence level, gained\nan 11% increase in F1-score when training data is 10,000 documents and performed sig-\nniﬁcantly less when fed with 10–15 times more training data. Another technique applied\nto Chinese legal cases [ 44] performs a two-step augmentation which includes stop word\ndeletion, back translation, and synonym replacement. As in [ 42], semantic similarity aug-\nmentation can be applied by assuming that words occurring in the same/similar contexts have\nsimilar meanings. The current solutions are applied on short texts, but it requires more run-\ntime on long texts. The authors of [ 45] investigate using transformer-based decoder models\nto generate U.S. Court options. The trained models produced judicial opinions, which are\nvery similar to actual opinions, and legal experts ﬁnd it difﬁcult to distinguish between them.\nThese results point to a potential application of transformer models in the ﬁeld of law. V arious\ntypes of research are underway to study the impact of pre-trained models in legal domain\ntasks. In [ 46], the authors researched to explore the issues that affect pre-trained models in\nthe legal domain.\nThus in our research, we focus on the combined way of generating samples using prompts\nas the initial step and further generating more samples using other neural models. We\nparticularly focus on the legal data because a legal expert’s annotation is expensive and\ntime-consuming.\n3 Methodology\n3.1 Data Augmentation Process\nThis section presents the methodology used to augment data for the legal overruling task.\nTo address the issue of limited data, we adopted various techniques described in the follow-\ning subsections. The original overruling benchmark dataset contained only 2,400 examples,\nwhich were manually annotated by attorneys for positive overruling samples, while negative\nsamples were randomly selected from the Casetext law corpus. However, this open dataset is\ninsufﬁcient to train a deep-learning network. To generate more training samples, we randomly\nselected a portion of this dataset, typically 100 samples, ensuring an equal split between 50\npositive and 50 negative samples. Using these samples as few-shot prompts, we utilized GPT\n123\nNeural Data Augmentation for Legal Overruling Task: Small Deep… Page 7 of 21 121\nmodels to generate more positive and negative samples as described in Sect. 3.1.1.F u r t h e r ,\nwe employed a random subset of the data samples generated by these generative models\nas input for the BERT-based models (Sect. 3.1.2) and the Word2V ec model (Sect. 3.1.3)t o\ngenerate more artiﬁcial samples. The algorithm 1 shows the entire neural data augmentation\nprocess for artiﬁcial sample generation by the neural network models.\nAlgorithm 1 Neural Data Augmentation: Artiﬁcial Samples Generation\nInput: Training Data Samples: Dtrain , Language Models: G\nOutput: Augmented Data: Daug\n1: Dsub −train ⇐ RandomSubset (Dtrain )\n2: Daug ⇐ NULL\n3: Synthesize sentences D∗ using generative language models G by providing k few-shot samples from\nDsub −train\n4: Remove duplicates and NULL in D∗\n5: Daug = Daug ∪ D∗\n6: Dsub ∗1 ⇐ RandomSubset (Daug )\n7: Dsub ∗2 ⇐ RandomSubset (Daug )\n8: i ⇐ 1\n9: while i < range (Dsub ∗1 ) do\n10: di ⇐ Dsub ∗1 [i]\n11: Synthesize samples d∗\ni1 and d∗\ni2 from di using BERT language models\n12: Daug = Daug ∪{ d∗\ni1, d∗\ni2}\n13: i ⇐ i + 1\n14: end while\n15: j ⇐ 1\n16: while j < range (Dsub ∗2 ) do\n17: d j ⇐ Dsub ∗2 [j]\n18: Synthesize d∗\nj from d j using Word2V ec model\n19: Daug = Daug ∪{ d∗\nj }\n20: j ⇐ j + 1\n21: end while\n22: return Daug\n3.1.1 Using Generative Pre-trained Transformer Models\nGenerative models are auto-regressive models developed by OpenAI, which are trained on\nhuge data and use neural networks to predict a probable future output based on an input.\nTo generate results, they use a unique type of neural network that processes data through\nmulti-headed attention modules to produce results. GPT-3, 116 times larger than GPT-2 with\nmore than 175 billion parameters, can generate human-like text and do tasks like question\nanswering, summarization, translation, and developing codes. We used both models, which\nfunction as a generator for the augmentation process. A portion of the data, typically 100\nsamples, is used as few-shot examples for GPT models to generate synthetic data. Using the\nprompt-based generation technique, the GPT-3 model generates overruled and non-overruled\nsentences from the OpenAI API platform.\n2 In GPT2, the text generator API from DeepAI 3\nis used for the augmentation process. Later, data is cleaned to remove duplicates and missing\nvalues. Table 8 in appendix shows the sample augmented sentences generated by the models.\n2 https://beta.openai.com.\n3 https://api.deepai.org/api/text-generator .\n123\n121 Page 8 of 21 R. Sheik et al.\nTable 2 Legal overruling example augmented using Word2V ec model\nOriginal sentence Augmented sentence\nThe court overruled the objection and found that the\nevidence was signiﬁcant\nThe court overruled the objection and found that the\nevidence was admissible\nTable 3 Shows the number of sentences generated by each model and the time taken\nModels Number of sentences Time taken for each sentence (s)\nGPT-3 21,605 0.56\nGPT-2 9269 0.192\nBERT 2800 2.89\nRoBERTa 2800 3.2\nDistilBERT 2800 2.57\nWord2vec 2100 0.144\n3.1.2 Using Pre-trained BERT-Based Models\nAdditionally, synthetic data was produced using encoder-based transformer models, which\ninclude the BERT, RoBERTa, and DistilBERT models. NLPaug framework 4 is utilized to\ndo the data augmentation process. A simple substitution mechanism is adopted for every\nmodel to augment data. We use 1400 samples of data generated by the generative models.\nFor each example text, we augment two sentences using different BERT architectures. Table\n9 in appendix shows the data augmented using BERT Architectures.\n3.1.3 Using Pre-trained Word2Vec Model\nFor the augmentation process using the Word2V ec language model, we used a random sample\nof 2100 data generated by the generative models. We started the augmentation process with\nthe simple synonym replacement strategy based on contextual word embedding (Word2V ec)\n[47]. Table 2 shows the sample legal sentence generated using the Word2V ec neural model.\nWe used a single-word replacement strategy for the augmentation process using the NLPaug\nframework at the word level.\nWe combined all data produced by the neural models into a single ﬁle to proceed with\nthe binary classiﬁcation of the legal overruling task. Table 3 shows the number of artiﬁcial\nsamples produced by each model and the time taken to augment each sentence. We obtained\na highly balanced data set of 20,854 overruled samples and 20,520 non-overruled examples,\nresulting in a total of 41,374 instances. The word distribution of the complete augmented\ndata is shown in Fig. 2.\n3.2 Model Architecture\nTo evaluate the performance of the legal overruling task across different deep learning models,\na diverse set of student models was trained using artiﬁcial data. The architectural details of\n4 https://github.com/GEM-benchmark/NL-Augmenter .\n123\nNeural Data Augmentation for Legal Overruling Task: Small Deep… Page 9 of 21 121\nFig. 2 Word frequency distribution of the augmented data\nthese models were detailed in Sect. 3.2.1. We also ﬁne-tuned BERT-based large language\nmodels and presented them in 3.2.2.\n3.2.1 Deep Learning Models\nThis section discusses the architecture of the various student models used for the legal over-\nruling task.\nBidirectional LSTM: Bidirectional long-short-term memory (BiLSTM) is the deep learn-\ning technique to process sequence data in both directions, forward and backward. In our\nexperiment, the BiLSTM model was deﬁned to include a 300-dimensional embedding layer.\nIt was then followed by three layers of BiLSTM with sizes of 128, 64, and 32 each, followed\nby a dropout layer. The dense layer for classiﬁcation was equipped with one hidden layer of\nsize 32 with ReLU activation [ 48], later fed to the output layer with sigmoid activation.\nConvolutional BiLSTM: This architecture uses a convolutional layer as a feature extractor\nlayer in the BiLSTM framework. The architecture starts with 500 dimensional embedding\nlayer. This was followed by a convolutional layer with a kernel size of three and 32 ﬁlters,\nsubsequently followed by a max-pooling layer. Then in sequence, two layers of BiLSTM\nof 128 and 64 hidden sizes followed by the dropout layer. The ﬁnal output layer is a fully\nconnected dense layer with sigmoid activation.\nBidirectional GRU: The architecture in which our Bidirectional Gated Recurrent Unit\n(BiGRU) was deﬁned is similar to that of BiLSTM. The architecture starts with an embedding\nlayer of size 500, the subsequent components comprised three BiGRU layers of 256, 128,\nand 64 sizes. In sequence, each BIGRU layer is followed by a dropout layer. The ﬁnal layer\nis a fully connected dense layer with a hidden layercomprising 32 neurons activated by ReLu\nand an output layer with sigmoid activation.\n3.2.2 Large Language Models\nWe ﬁne-tuned the following bert-base language models as these models were used for the\ndata augmentation process in Sect. 3.1.2.\nBERT: BERT is a model pre-trained on a large corpus of English data in a self-supervised\nfashion. This implies that an automatic method to create inputs and labels from those texts\nwas pre-trained on the raw texts without human labeling effort [ 12].\n123\n121 Page 10 of 21 R. Sheik et al.\nRoBERTa:R o B E R T a[14] is an optimized version of BERT trained on a much larger\ndataset with an effective training procedure.\nDistilBERT: DistilBERT is a compact, fast, and cheap transformer model trained by a\ndistilling BERT base. It runs 60% faster and uses 40% fewer parameters than bert-base-\nuncased while retaining over 95% of BERT’s performance on language understanding tasks\n[13].\nFine-tuning LLMs, like BERT, RoBERTa, and DistilBERT, requires the input data to be\nin a speciﬁc format, so the data must be processed to convert it into this format. This involves\ntokenizing the text, creating attention masks, and converting the labels into numerical values.\nThe default tokenizer class, AutoTokenizer, sourced from the Hugging Face Transformer\nlibrary,\n5 is used for data processing and subsequent ﬁne-tuning of the models.\n4 Experimental Details\nThis section provides an overview of our experimental setup for data augmentation, training\nparameters for model classiﬁcation, and few-shot parameter tuning. The last subsection out-\nlines the evaluation setups used in our work to assess the performance of the trained classiﬁers.\nThe whole experiment was carried out in the Google colaboratory notebook pro-environment\nand implemented using Python.\n4.1 Setup for Data Augmentation\nData augmentation with generative models comes with certain limitations and difﬁculties.\nOne of the main difﬁculties is the usage policies and usage limits set by OpenAI. These poli-\ncies are designed to ensure the model’s responsible usage and prevent misuse. For example,\nthere are limits on the number of API calls that can be made per month, making it challenging\nfor researchers to collect large amounts of data for their experiments. Additionally, GPT-3\nis restricted for certain types of usage, such as using the model for illegal or unethical pur-\nposes. This can limit the scope of data samples generated with the model. For data generation\nusing GPT-3, we used the completion function from the OpenAI library and the davinci-002\nmodel.\n6 The model will produce the best results for applications that require a high level of\nunderstanding of the content, such as creative content generation. More computing resources\nare required to support these enhanced capabilities. We have set the temperature to 0.45. The\ntemperature parameter controls the randomness in the model’s behavior. Lowering the value\nresults in fewer random completions. As prompt engineering, the model uses a short descrip-\ntion of the task and an example sentence to generate new synthetic data. We used DeepAI’s\ntext generator API to access the GPT-2 model to produce synthetic data. We adopted the\ndefault setting for model parameters in GPT-2, and text was generated using the prompt\nwith examples as input. We sourced the BERT-based models for text augmentation from the\nHugging Face pre-trained transformer library.\n5 https://huggingface.co/models.\n6 https://beta.openai.com/docs/models/gpt-3 .\n123\nNeural Data Augmentation for Legal Overruling Task: Small Deep… Page 11 of 21 121\nTable 4 Models and its attributes for training\nModel Model size #Parameters #Epochs Training time/ epoch\nBiLSTM 93 mb 91,73,665 17 58\nBiGRU 121 mb 1,38,86,865 12 55\nConvBiLSTM 76 mb 80,71,433 15 19\nBERT 367 mb 10,83,11,810 3 5520\nRoBERT 475 mb 12,46,47,170 3 5586\nDistilBERT 255 mb 6,69,55,010 3 2820\nFig. 3 The optimal K value, which represents the number of few-shot samples fed to the model to be determined\nto test the best performance of the GPT-3 model\n4.2 Training Parameters for Model Classification\nFor training the deep neural network framework (BiLSTM, BiGRU, and ConvBiLSTM), the\noptimizer was adam [ 49] with binary cross entropy as the loss function. A dropout of 0.2\nwas used after every hidden layer. These models underwent 20 epochs of training and used\nearly stopping based on validation accuracy. Hyperparameter tuning was performed using a\nHyperband tuner from the Keras library\n7 to determine the appropriate learning rate and the\noptimal number of neurons for each layer. We have adopted a learning rate of 0.001 from\nchoices [1e-2, 1e-3, 1e-4, 1e-5], and neurons were selected in the range of 32 to 512 with a\nstep size of 32. For the transformer models, the base version of pre-trained LLMs from the\nhugging face library was loaded for the sequence classiﬁcation task. The models were then\nﬁne-tuned on the augmented data for three epochs with a learning rate of 0.00001. Table 4\nshows the model size and the optimal number of epochs with training time.\n4.3 Parameter Setting for Few-Shot GPT-3\nTo evaluate the performance of the GPT-3 model under the few-shot setting, multiple values\nfor K (which speciﬁes the number of examples fed to the model as prompts before testing)\nwere experimented with, and the F1-score was recorded, revealed in Fig. 3 that the best\noutcome was achieved when K was equal to 10, with the F1-score decreasing when K was\nset to 15.\nThus, the GPT-3 model was tested on the original overruling dataset of 2300 samples,\nwith a K value of 10.\n7 https://keras.io/api/keras_tuner/tuners/hyperband/ .\n123\n121 Page 12 of 21 R. Sheik et al.\nArtificial Samples \nGeneration\nEvaluation Setup 1\nEvaluation Setup 2\nRandom \nSelect\nTest Set\nAugmented \nClassifier\nNon-Augmented \nClassifier\nTrain Set\nNeural \nModelsTrain\nEvaluation Setup 3\nNeural \nModels\nTest\nAugment\nTrain\nTest\nTest\nTest\nTrain\nScore\nScore\nScore\nScore\nFig. 4 The various evaluation strategies used for the augmented classiﬁers and non-augmented neural classi-\nﬁers\n4.4 Evaluation Setup\nTo assess the performance of trained classiﬁers, we use three evaluation setups. In the ﬁrst\napproach, we compare the performance of the augmented and non-augmented classiﬁers. To\nestablish a strong baseline for comparison, the non-augmented classiﬁer undergoes training\non the original 80% of human-labeled data (train-set), while the augmented classiﬁer is\ntrained on artiﬁcially generated samples using a random subset of 100 samples from the\ntrain-set. Both these models were tested on the remaining 20% test-set for comparison. In\nthe second setup, the augmented classiﬁer trained with artiﬁcial samples was tested on the\nremaining gold-standard data to to evaluate the performance of various deep learning models,\nincluding the few-shot GPT-3. The third evaluation setup involves training neural models by\nincorporating the artiﬁcially generated samples into the original train-set. This allows us to\ncompare their performance against various neural data augmentation techniques serving as\nbaselines. Figure 4 outlines the various evaluation strategies used for both classiﬁers.\n5 Results and Discussion\nThe results of our experiments corresponding to evaluation setup 1, 2, and 3 are presented\nin Sects. 5.1, 5.2,a n d 5.3, respectively. Finally, we address and discuss the key research\nquestions in Sect. 5.4.\n5.1 Performance of the Models With and Without Augmentation\nIn this study, we compare the classiﬁer’s performance with and without augmentation, based\non the existing dataset comprising 2,400 samples. Training the non-augmented classiﬁers\ninvolved allocating 80% of the dataset, while the remaining 20% was reserved for testing both\naugmented and non-augmented models for comparative analysis. We also used a statistical\nsigniﬁcance test to evaluate the difference in performance.\n123\nNeural Data Augmentation for Legal Overruling Task: Small Deep… Page 13 of 21 121\nTable 5 Models classiﬁcation\nperformance with and without\naugmentation\nModel Without augmentation With augmentation\nAccuracy F1-score Accuracy F1-score\nBiLSTM 0.75 0.75 0.84 0.87\nBiGRU 0.79 0.8 0.83 0.86\nConvBiLSTM 0.8 0.79 0.86 0.85\nBERT 0.81 0.78 0.87 0.82\nRoBERTa 0.86 0.81 0.92 0.88\nDistilBERT 0.77 0.73 0.83 0.81\nTable 5 shows the performance of the models with and without augmentation based on\naccuracy and F1-score. It is observed that the RoBERTa models had a better score in accuracy\nand F1-score than the other deep learning models. Among the student models, BiLSTM had a\nrelative increase in accuracy and F1-score compared to other models trained with augmented\ndata. The Receiver Operating Characteristic (ROC) curve is used to further analyze classiﬁer\nperformance. Figure 5 shows that the area under the ROC curve is larger for models trained\nwith augmentation than without augmentation.\nThe paired t-test [50], which compares the means of two groups, was used as the sta-\ntistical test to assess further the impact of augmentation on the classiﬁer’s performance. It\nis used to establish whether or not the mean difference between the pairs of observation is\nzero. It is commonly used to determine whether two groups vary or whether a process or\ntreatment signiﬁcantly affects the population of interest. We use this test to measure whether\nthe augmented training samples signiﬁcantly improve the classiﬁer’s performance compared\nto the non-augmented classiﬁer. We set the hypothesis as follows:-\nNull Hypothesis H\no: The performance of classiﬁers does not increase due to augmented\ndata.\nAlternate Hypothesis Ha: The performance of classiﬁers has increased due to augmented\ndata.\nAs a threshold, we use α as 0.05. The following equation 1 represents the test statistic t.\nt =\n¯X − μ\ns/√n (1)\nwhere μ denotes the hypothesized population mean with ¯X as sample mean and s as\nstandard deviation with n denotes the number of classiﬁer models. On evaluation, it results in\na test statistic value of −9.428473 and obtains a p − value for one_tailed_test as 0.000113,\na value that is signiﬁcantly lower than α value. Thus t value falls into the critical region,\nthereby rejecting the null hypothesis and concluding that augmented techniques improve\nclassiﬁer performance compared to non-augmented classiﬁers.\n5.2 Comparison Between Models\nIn this research, we performed extensive experiments, studying different conﬁgurations to\ncompare and contrast the deep learning models. This study used the following criteria to\ndetermine the best model.\n• Relatively high F1-score\n• Relatively small model size\n123\n121 Page 14 of 21 R. Sheik et al.\nFig. 5 ROC curves for the models trained with and without augmentation\n• Relatively low inference time\nOur study assessed the efﬁcacy of several deep learning models trained on the artiﬁcial\nsamples for the legal overruling task by evaluating their performance on 2300 unseen test\nsamples. We compared the models based on three key metrics: F1-score, model size, and\ninference time. While GPT-3 has been shown to outperform many state-of-the-art models in\nnatural language processing applications, it requires signiﬁcant computational resources to\nachieve optimal results. To establish a baseline for comparison, we analyzed the performance\nof GPT-3 under a ten-shot setting. Table 6 shows the metrics for the classiﬁcation performance\nof all the deep learning models. Our ﬁndings indicate that the ten-shot GPT-3 model achieves\nan F1-score of 0.69 for the legal overruling task. However, the ﬁne-tuned encoder-based\ntransformer models performed better than the few-shot GPT-3 model. Speciﬁcally, RoBERTa\nbase had the highest F1-score of 0.9, while the BiGRU student model led with an accuracy\nof 0.96 highlighted in bold. The table also presents ﬁndings related to the inference time\n123\nNeural Data Augmentation for Legal Overruling Task: Small Deep… Page 15 of 21 121\nTable 6 Model classiﬁcation performance\nModel Accuracy Precision Recall F1-score Inference time (s)\nBiLSTM 0.9 0.85 0.85 0.85 8\nBiGRU 0.96 0.87 0.87 0.87 20\nConvBiLSTM 0.87 0.83 0.84 0.83 11\nBERT 0.92 0.88 0.86 0.87 110\nRoBERTa 0.93 0.9 0.91 0.9 146\nDistilBERT 0.86 0.89 0.87 0.88 70\nGPT-3\n1 0.74 0.86 0.57 0.69 1029\n1 The GPT-3 model is tested under few-shot scenario\nBold values indicates the metrics values of best performing models\nTable 7 Models classiﬁcation performance with and without augmentation\nData augmentation approach/ Model for classiﬁcation BiGRU RoBERTa\nAccuracy F1-score Accuracy F1-Score\nUsing samples generated by Word2V ec 0.94 0.94 0.95 0.96\nUsing samples generated by BERT 0.93 0.94 0.96 0.97\nUsing samples generated by GPT2 0.92 0.93 0.93 0.93\nUsing samples generated by GPT3 0.90 0.90 0.92 0.91\nProposed approach 0.96 0 .96 0 .97 0 .97\nBold values indicates the metrics values of best performing models\nin seconds for each individual model. It is observed that the inference time of ﬁne-tuned\ntransformer models was several times higher than the student deep-learning models.\n5.3 Comparison to Other Neural Data Augmentation Techniques\nIn this section, we individually compare the various neural text data augmentation techniques\nwith our proposed hybrid approach. The samples are generated using neural models ranging\nfrom the traditional Word2V ec model [ 15] to transformer models[ 25, 35], including gener-\native pre-trained models such as GPT-2 [ 28, 31]a n dG P T - 3[34]. The samples produced by\nthese techniques were augmented to the existing training data for classiﬁcation. We have per-\nformed this classiﬁcation deploying the best-performed small deep learning model, BiGRU,\nand the best-performed LLM, RoBERTA, based on the result of the experiment conducted in\nSect. 5.2.T a b l e7 shows that our proposed approach leads in accuracy and F1-score for both\nmodels.\n5.4 Discussion\nGPT-3 is a large and complex model requiring signiﬁcant computational resources, making it\ndifﬁcult for researchers with limited computational resources to experiment with the model.\nFurther, a signiﬁcant issue with text generation models is the possibility of noise generation,\nwhich lowers classiﬁcation model performance instead of improving it [ 51]. From Table 6,\nthe RoBERTa base model produces F1-scores that outperform those of the GPT-3 model,\n123\n121 Page 16 of 21 R. Sheik et al.\nindicating that the RoBERTa model is a more efﬁcient option for the legal overruling task.\nIn fact, it produces results that are the best among all models evaluated. However, it is\nnoteworthy that despite the RoBERTa base model being smaller than the GPT-3 model and\nhaving fewer trainable parameters, the student models exhibit similar performance levels.\nMore parameters aid the RoBERTa model in performing better, but its larger size results in\nhuge training time. On the other hand, the student models’ training time is merely a fraction\nof the ﬁne-tuning time of transformer models, making them a more practical and efﬁcient\nsolution.\nAfter analyzing all the information presented, the BiGRU model trained with augmented\ndata stands out as the best option for the legal overruling task. Despite being one of the smallest\nmodels, the BiGRU model requires less training time and has fewer trainable parameters\nwhile still exhibiting high accuracy. In fact, it outperforms the GPT-3 model under few-shot\nconditions and competes well with other large language models in terms of the F1 score.\nSimple RNNs like BiGRU are preferred to RoBERTa because of their simpler architecture,\nsmaller size, and faster runtime. Since the positive and negative overruling examples are\ntaken from the casetext law corpus and the terminology used in the samples is speciﬁc to\nthe legal domain, overruling has moderate to high domain speciﬁcity [ 52]. LLMs like GPT-3\nare trained on a vast English corpus different from the legal corpora, which could be the\nreason for the lower performance of GPT-3 under few-shot setting. These limitations should\nbe considered when planning and conducting research with GPT-3. Overall, the evidence\nsuggests that small deep learning models trained on domain-speciﬁc data may offer a more\nefﬁcient and effective approach to domain-speciﬁc NLP tasks like legal overruling. While\nlarger pre-trained models like GPT-3 and RoBERTa are impressive in their ability to handle\na wide range of tasks, they may not always be the best option for speciﬁc domain-speciﬁc\ntasks. By leveraging domain-speciﬁc data and designing models that are optimized for speciﬁc\ntasks, researchers can potentially achieve better accuracy and faster inference times while\nusing fewer computational resources.\nAnswers to Research Questions\nRQ1: Can we effectively reduce human annotation effort to train models with fair perfor-\nmance using augmented data ?Yes, all the models trained with the augmented data performed\nbetter in the test data (Table 5) compared to the results achieved with non-augmented data.\nFurther, the outcomes of the paired t-test and ROC curves (Fig. 5) strengthened the same ﬁnd-\ning. Hence, it signiﬁcantly reduces expensive human annotation in training models for the\nlegal overruling task, thereby improving the generalization power of deep learning models.\nRQ2: Are student models trained on this generated data more efﬁcient than ﬁne-tuned\nlarge language models? Yes, our results indicate that it is indeed possible to achieve com-\nparable performance to large ﬁne-tuned language models using small deep-learning models\ntrained with augmented data, even with limited computing power and memory resources, as\ndemonstrated in Tables 6 and 7.\nRQ3: Will student models exhibit better scores than the state-of-the-art GPT-3 models\ntested under few-shot setting?Yes, according to our ﬁndings on the legal overruling task, our\nbest-performed deep learning model outperforms GPT-3 models tested in few-shot settings\nby over 18%. Additionally, it is much faster to run these student models, which leads to a\nbetter inference score that is way higher than GPT-3 models (Table 6). Thus dealing with\nstate-of-the-art models suffers from economic costs, latency in usage, and high memory\nfootprints.\n123\nNeural Data Augmentation for Legal Overruling Task: Small Deep… Page 17 of 21 121\n6 Conclusions and Future Work\nThe research utilizes neural models for text data augmentation to enhance performance in\nlow-resourced tasks. We augmented 41,374 data samples with 100 training examples for the\nlegal overruling task using neural models. Our study proposed three simple neural network\narchitectures: BiLSTM, BiGRU, and ConvBiLSTM. The result showed that data augmenta-\ntion strengthens the classiﬁcation process leading to robust classiﬁers. Our experiments also\nuse the state-of-the-art GPT-3 models tested under a few-shot scenario for the legal overrul-\ning task. As GPT-3 was trained on a generic domain, it failed to produce results with high\naccuracy for the domain-speciﬁc task. Instead, small deep-learning neural networks trained\non a large amount of augmented data have results that outperform GPT-3 and are nearly equal\nto other ﬁne-tuned transformer models. After testing all models, results show that the BiGRU\nmodel being smaller in size and with less inference time, outperforms the GPT-3 model and\ncompetes with ﬁne-tuned RoBERTa model, reducing the carbon footprint caused by these\nLLMs. As a result, our work demonstrated the deduction of expensive manual data anno-\ntation through data augmentation for such domain-speciﬁc tasks, which has great potential\nfor facilitating the training of small, robust deep learning models. Furthermore, despite their\nsmaller size, the student models still deliver nearly equal performance to the larger models,\nmaking them a compelling option for researchers looking to develop domain-speciﬁc NLP\nmodels with limited computational resources.\nHandling protected words in legal texts is another challenge in augmentation, which can\nbe addressed using a knowledge base of such words. Since pre-training using a general corpus\nis not viable in the legal domain, data augmentation based on domain knowledge, such as\nsubstitution based on a list of words that can be replaced, may be effective. In the future,\nwe plan to use domain-speciﬁc transformer models in the augmentation process, which can\nproduce more realistic samples. Additionally, the class imbalance issue can be resolved by\nusing a few seed selection strategies for extracting different samples from the training data for\nthe augmentation process. Domain specialists can make the process more efﬁcient; therefore,\nin the future, legal NLP research could consider modeling active learning approaches to take\nthese data variations into account.\nData availability and ethical concerns: The Overruling task dataset used in this study was provided by Casetext,\na company specializing in legal research, and is open for research. The augmented version of this dataset\ngenerated as part of this work is made available upon request to the authors. The code used in this study for\naugmentation and modeling was developed using open-source tools and libraries, and the authors are willing\nto make the code accessible upon request. This would enable other researchers to reproduce the experiments\nand build upon the research ﬁndings, potentially leading to new insights.\nDeclarations\nConﬂict of interest All authors certify that they have no involvement in any ﬁrm or entity with any ﬁnancial\nor non-ﬁnancial interest in the materials covered in this document.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is\nnot included in the article’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\n123\n121 Page 18 of 21 R. Sheik et al.\nAppendix A Results of Data Augmentation\nHere, we present the results of the augmentation process by using large language models.\nTable8 shows the sample sentences by GPT models. Table 9 presents the sentences augmented\nby the BERT architectures.\nTable 8 Augmented sentences using generative models\nAugmented sentence Label\nThe fact that the defendant had access to the victim’s house at the time of the\ncrime was of no effect in the decision, and insofar as the decision is inconsistent\ntherewith, it must be overruled\n1\nThe court’s decision in Langley is hereby overruled to the extent that it suggests\nthat a defendant’s demeanor in court may give rise to a reasonable inference of\nguilt\n1\nThe court may not review the Attorney General’s decision to deny a request for a\nstay of deportation\n0\nThe admissions ofﬁce also undertakes regular “reliability analyses” to ensure that\nits admissions decisions are made in accordance with the law\n0\nTable 9 Augmented examples generated by the Encoder-based transformer models\nModel Original sentence Augmented sentence Label\nBERT to the extent that this dictum is\ninconsistent with our holding here,\nwe expressly overrule it\non the fact that his dictum is\ninconsistent with any authority\nhere, we therefore overrule it\n1\nbut detective kabler’s report states\nthat while detective baker\nconducted the initial interview with\ndefendant, both detectives\nultimately interviewed him together\nbut steve lyons’ofﬁcial report states\nthat while detective robinson\nconducted that initial interview\nagainst defendant, said detectives\nultimately interviewed prosecutors\ntogether\n0\nRoBERTa to the extent that these decisions are\ninconsistent with the views\nhereinafter expressed, they are\ndisapproved\nto cause appearance that these\ndecisions constitute unlawful or the\nviews hereinafter expressed, and be\ndisapproved\n1\na subsequent search of the vehicle\nrevealed the presence of an\nadditional syringe that had been\nhidden inside a purse located on the\npassenger side of the vehicle\na physical search of third vehicle\nrevealed the presence of the\nadditional syringe that may been\nhidden inside two compartments\nlocated around the passenger side\ninside each vehicle\n0\nDistilBERT as discussed elsewhere, the absence\nof that language in the mlssa\nsupports overruling boutte\nas observed elsewhere, that absence\nof formal language deﬁning the\ndeﬁnition supports overruling\ndeﬁnitions\n1\nbradley fails to show that a rational\njury could not have found that the\nevidence, viewed in a light most\nfavorable to the prosecution,\nsupports her conviction beyond\nreasonable doubt\nbradley sought to show whether a\nrational jury could not truly proved\nthat the evidence, viewed in a light\nmost favorable in the defendants,\ndenied false conviction beyond\nfalse suspicion\n0\n123\nNeural Data Augmentation for Legal Overruling Task: Small Deep… Page 19 of 21 121\nReferences\n1. Feng SY , Gangal V , Wei J, Chandar S, V osoughi S, Mitamura T, Hovy E (2021) A survey of data\naugmentation approaches for nlp. In: Findings of the association for computational linguistics: ACL-\nIJCNLP 2021, pp 968–988\n2. Zhang X, Zhao J, LeCun Y (2015) Character-level convolutional networks for text classiﬁcation. Adv\nNeural Inf Process Syst 28\n3. Wang WY , Y ang D (2015) That’s so annoying!!!: A lexical and frame-semantic embedding based data\naugmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets. In:\nProceedings of the 2015 conference on empirical methods in natural language processing, pp. 2557–2563.\nAssociation for Computational Linguistics, Lisbon, Portugal (2015)\n4. Fadaee M, Bisazza A, Monz C (2017) Data augmentation for low-resource neural machine translation.\nIn: Proceedings of the 55th annual meeting of the association for computational linguistics, vol 2, pp\n567–573. Association for Computational Linguistics, V ancouver, Canada\n5. Kobayashi S (2018) Contextual augmentation: Data augmentation by words with paradigmatic relations.\nIn: Proceedings of the 2018 conference of the North American chapter of the association for computational\nlinguistics: human language technologies, vol 2, pp 452–457. Association for Computational Linguistics,\nNew Orleans, Louisiana\n6. Sennrich R, Haddow B, Birch A (2016) Improving neural machine translation models with monolingual\ndata. In: Proceedings of the 54th annual meeting of the association for computational linguistics, vol 1,\npp 86–96. Association for Computational Linguistics, Berlin, Germany\n7. Kaﬂe K, Y ousefhussien M, Kanan C (2017) Data augmentation for visual question answering. In: Pro-\nceedings of the 10th international conference on natural language generation, pp 198–202. Association\nfor Computational Linguistics, Santiago de Compostela, Spain\n8. Weiss K, Khoshgoftaar TM, Wang D (2016) A survey of transfer learning. J Big Data 3(1):1–40\n9. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P , Neelakantan A, Shyam P , Sastry G,\nAskell A et al (2020) Language models are few-shot learners. Adv Neural Inf Process Syst 33:1877–1901\n10. Zheng L, Guha N, Anderson BR, Henderson P , Ho DE (2021) When does pretraining help? assessing\nself-supervised learning for law and the casehold dataset of 53,000+ legal holdings. In: Proceedings of\nthe eighteenth international conference on artiﬁcial intelligence and law, pp 159–168\n11. Radford A, Wu J, Amodei D, Amodei D, Clark J, Brundage M, Sutskever I (2019) Better language models\nand their implications. OpenAI blog 1:2\n12. Kenton JDMWC, Toutanova LK (2019) Bert: Pre-training of deep bidirectional transformers for language\nunderstanding. In: Proceedings of NAACL-HLT, pp 4171–4186\n13. Sanh V , Debut L, Chaumond J, Wolf T (2019) Distilbert, a distilled version of bert: smaller, faster, cheaper\nand lighter. arXiv preprint arXiv:1910.01108\n14. Liu Y , Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V (2019)\nRoberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n15. Mikolov T, Chen K, Corrado G, Dean J (2013) Efﬁcient estimation of word representations in vector\nspace. arXiv preprint arXiv:1301.3781\n16. Pennington J, Socher R, Manning CD (2014) Glove: global vectors for word representation. In: Pro-\nceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp\n1532–1543\n17. Bojanowski P , Grave E, Joulin A, Mikolov T (2017) Enriching word vectors with subword information.\nTrans Assoc Comput Linguist 5:135–146\n18. Chen E, Lin Y , Xiong H, Luo Q, Ma H (2011) Exploiting probabilistic topic models to improve text\ncategorization under class imbalance. Inf Process Manag 47(2):202–214\n19. Ratner AJ, De Sa CM, Wu S, Selsam D, Ré C (2016) Data programming: Creating large training sets,\nquickly. Adv Neural Inf Process Syst 29\n20. Wei J, Zou K (2019) EDA: Easy data augmentation techniques for boosting performance on text classiﬁ-\ncation tasks. In: Proceedings of the 2019 conference on empirical methods in natural language processing\nand the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pp 6382–\n6388. Association for Computational Linguistics, Hong Kong, China (2019)\n21. Kaﬂe K, Y ousefhussien M, Kanan C (2017) Data augmentation for visual question answering. In: Pro-\nceedings of the 10th international conference on natural language generation, pp 198–202\n22. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9(8):1735–1780\n23. Guo H, Mao Y , Zhang R (2019) Augmenting data with mixup for sentence classiﬁcation: an empirical\nstudy. arXiv\n24. Kaushik D, Hovy E, Lipton ZC (2019) Learning the difference that makes a difference with\ncounterfactually-augmented data. arXiv preprint arXiv:1909.12434 (2019)\n123\n121 Page 20 of 21 R. Sheik et al.\n25. Wu X, Lv S, Zang L, Han J, Hu S (2019) Conditional bert contextual augmentation. In: International\nconference on computational science, pp 84–95. Springer\n26. Elsahar H, Gravier C, Laforest F (2018) Zero-shot question generation from knowledge graphs for unseen\npredicates and entity types. arXiv preprint arXiv:1802.06842\n27. Papanikolaou Y , Pierleoni A (2020) Dare: data augmented relation extraction with gpt-2. arXiv preprint\narXiv:2004.13845\n28. Zhang D, Li T, Zhang H, Yin B (2020) On data augmentation for extreme multi-label classiﬁcation. arXiv\npreprint arXiv:2009.10778\n29. Moradi M, Blagec K, Haberl F, Samwald M (2021) Gpt-3 models are poor few-shot learners in the\nbiomedical domain. arXiv preprint arXiv:2109.02555\n30. Lee J, Y oon W, Kim S, Kim D, Kim S, So CH, Kang J (2020) Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinformatics 36(4):1234–1240\n31. Chen Z, Eavani H, Chen W, Liu Y , Wang WY (2019) Few-shot nlg with pre-trained language model.\narXiv preprint arXiv:1904.09521\n32. Edwards A, Ushio A, Camacho-Collados J, de Ribaupierre H, Preece A (2021) Guiding generative\nlanguage models for data augmentation in few-shot text classiﬁcation. arXiv preprint arXiv:2111.09064\n33. Anaby-Tavor A, Carmeli B, Goldbraich E, Kantor A, Kour G, Shlomov S, Tepper N, Zwerdling N (2020)\nDo not have enough data? Deep learning to the rescue! In: Proceedings of the AAAI conference on\nartiﬁcial intelligence, vol 34, pp 7383–7390\n34. Y oo KM, Park D, Kang J, Lee SW, Park W (2021) Gpt3mix: Leveraging large-scale language models\nfor text augmentation. In: Findings of the association for computational linguistics: EMNLP 2021, pp\n2225–2239\n35. Kumar V , Choudhary A, Cho E (2020) Data augmentation using pre-trained transformer models. In: Pro-\nceedings of the 2nd workshop on life-long learning for spoken language systems, pp 18–26. Association\nfor Computational Linguistics, Suzhou, China (2020)\n36. Lewis M, Liu Y , Goyal N, Ghazvininejad M, Mohamed A, Levy O, Stoyanov V , Zettlemoyer L (2019)\nBart: denoising sequence-to-sequence pre-training for natural language generation, translation, and com-\nprehension. arXiv preprint arXiv:1910.13461\n37. Chen Y , Liu Y (2022) Rethinking data augmentation in text-to-text paradigm. In: Proceedings of the\n29th international conference on computational linguistics, pp 1157–1162. International Committee on\nComputational Linguistics, Gyeongju, Republic of Korea\n38. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y , Li W, Liu PJ (2020) Exploring the\nlimits of transfer learning with a uniﬁed text-to-text transformer\n39. Okimura I, Reid M, Kawano M, Matsuo Y (2022) On the impact of data augmentation on downstream\nperformance in natural language processing. In: Proceedings of the third workshop on insights from\nnegative results in NLP , pp 88–93\n40. Shorten C, Khoshgoftaar TM, Furht B (2021) Text data augmentation for deep learning. J Big Data\n8(1):1–34\n41. Bayer M, Kaufhold MA, Reuter C (2021) A survey on data augmentation for text classiﬁcation. ACM\nComput Surv (2021)\n42. Csányi G, Orosz T (2022) Comparison of data augmentation methods for legal document classiﬁcation.\nActa Technica Jaurinensis 15(1):15–21\n43. Y an G, Li Y , Zhang S, Chen Z (2019) Data augmentation for deep learning of judgment documents. In:\nInternational conference on intelligent science and big data engineering, Springer, pp 232–242\n44. Guo Z, Liu J, He T, Li Z, Zhangzhu P (2020) Taujud: test augmentation of machine learning in judicial\ndocuments. In: Proceedings of the 29th ACM SIGSOFT international symposium on software testing and\nanalysis, pp 549–552\n45. Peric L, Mijic S, Stammbach D, Ash E (2020) Legal language modeling with transformers. In: Proceedings\nof the fourth workshop on automated semantic analysis of information in legal text (ASAIL 2020) held\nonline in conjunction with te 33rd international conference on legal knowledge and information systems\n(JURIX 2020) December 9, 2020, vol 2764 (2020). CEUR-WS\n46. Nguyen HT, Nguyen LM (2021) Sublanguage: A serious issue affects pretrained models in legal domain.\narXiv preprint arXiv:2104.07782\n47. Bonthu S, Dayal A, Lakshmi M, Rama Sree S (2022) Effective text augmentation strategy for nlp models.\nIn: Proceedings of third international conference on sustainable computing, pp 521–531. Springer\n48. Agarap AF (2018) Deep learning using rectiﬁed linear units (relu). arXiv preprint arXiv:1803.08375\n49. Kingma DP , Ba J (2015) Adam: a method for stochastic optimization. In: Bengio Y , LeCun Y (eds) 3rd\ninternational conference on learning representations, ICLR 2015, San Diego, CA, USA, May 7–9, 2015,\nConference Track Proceedings (2015)\n50. Hsu H, Lachenbruch PA (2014) Paired t test. Wiley StatsRef: statistics reference online\n123\nNeural Data Augmentation for Legal Overruling Task: Small Deep… Page 21 of 21 121\n51. Y ang Y , Malaviya C, Fernandez J, Swayamdipta S, Le Bras R, Wang JP , Bhagavatula C, Choi Y , Downey\nD (2020) Generative data augmentation for commonsense reasoning. In: Findings of the association for\ncomputational linguistics: EMNLP 2020, pp 1008–1025. Association for Computational Linguistics\n52. Chalkidis I, Jana A, Hartung D, Bommarito M, Androutsopoulos I, Katz DM, Aletras N (2021) Lexglue:\nabenchmark dataset for legal language understanding in english. arXiv preprint arXiv:2110.00976\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional afﬁliations.\n123",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.7241219878196716
    },
    {
      "name": "Computational intelligence",
      "score": 0.6877455115318298
    },
    {
      "name": "Computer science",
      "score": 0.6446514129638672
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5339905619621277
    },
    {
      "name": "Deep learning",
      "score": 0.4838339388370514
    },
    {
      "name": "Artificial neural network",
      "score": 0.46933048963546753
    },
    {
      "name": "Language model",
      "score": 0.42854610085487366
    },
    {
      "name": "Multi-task learning",
      "score": 0.42787641286849976
    },
    {
      "name": "Natural language processing",
      "score": 0.4114249050617218
    },
    {
      "name": "Machine learning",
      "score": 0.4087558388710022
    },
    {
      "name": "Cognitive psychology",
      "score": 0.32511842250823975
    },
    {
      "name": "Psychology",
      "score": 0.2894962430000305
    },
    {
      "name": "Engineering",
      "score": 0.08849379420280457
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I122964287",
      "name": "National Institute of Technology Tiruchirappalli",
      "country": "IN"
    }
  ],
  "cited_by": 4
}