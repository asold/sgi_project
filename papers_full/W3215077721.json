{
    "title": "Predicting Retrosynthetic Reaction using Self-Corrected Transformer Neural Networks",
    "url": "https://openalex.org/W3215077721",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A5075817762",
            "name": "Shuangjia Zheng",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A5015329746",
            "name": "Jiahua Rao",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A5056181358",
            "name": "Zhongyue Zhang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A5101956796",
            "name": "Jun Xu",
            "affiliations": [
                "Sun Yat-sen University",
                "Wuyi University"
            ]
        },
        {
            "id": "https://openalex.org/A5023539493",
            "name": "Yuedong Yang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3210732293",
        "https://openalex.org/W2152488335",
        "https://openalex.org/W2062601468",
        "https://openalex.org/W4244424278",
        "https://openalex.org/W2747592475",
        "https://openalex.org/W1997974358",
        "https://openalex.org/W2551217916",
        "https://openalex.org/W2328708083",
        "https://openalex.org/W1979641548",
        "https://openalex.org/W1973166400",
        "https://openalex.org/W2093371352",
        "https://openalex.org/W29374554",
        "https://openalex.org/W2001496974",
        "https://openalex.org/W4300465858",
        "https://openalex.org/W3100545487",
        "https://openalex.org/W2056701057",
        "https://openalex.org/W2015928143",
        "https://openalex.org/W2580919858",
        "https://openalex.org/W2621742623",
        "https://openalex.org/W2100233978",
        "https://openalex.org/W4240234659",
        "https://openalex.org/W2125031621",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2060531713",
        "https://openalex.org/W2020996919",
        "https://openalex.org/W2571050567",
        "https://openalex.org/W2324964582",
        "https://openalex.org/W2769756736",
        "https://openalex.org/W2098297786",
        "https://openalex.org/W2060586571",
        "https://openalex.org/W2929387033",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2799620402"
    ],
    "abstract": "Synthesis planning is the process of recursively decomposing target molecules into available precursors. Computer-aided retrosynthesis can potentially assist chemists in designing synthetic routes, but at present it is cumbersome and provides results of dissatisfactory quality. In this study, we develop a template-free self-corrected retrosynthesis predictor (SCROP) to perform a retrosynthesis prediction task trained by using the Transformer neural network architecture. In the method, the retrosynthesis planning is converted as a machine translation problem between molecular linear notations of reactants and the products. Coupled with a neural network-based syntax corrector, our method achieves an accuracy of 59.0% on a standard benchmark dataset, which increases &gt;21% over other deep learning methods, and &gt;6% over template-based methods. More importantly, our method shows an accuracy 1.7 times higher than other state-of-the-art methods for compounds not appearing in the training set.",
    "full_text": "Predicting Retrosynthetic Reaction using Self-Corrected Transformer \nNeural Networks \nShuangjia Zheng1,2#, Jiahua Rao2#, Zhongyue Zhang2, Jun Xu*1,3, and Yuedong \nYang*2,4 \n1Research Center for Drug Discovery, School of Pharmaceutical Sciences, Sun Yat-sen \nUniversity, 132 East Circle at University City, Guangzhou 510006, China \n2School of Data and Computer Science , Sun Yat-sen University, Guangzhou 510006, \nChina \n3School of Computer Science & Technology, Wuyi University, 99 Yingbin Road, \nJiangmen 529020, China \n4Key Laboratory of Machine Intel ligence and Advanced Computing, Sun Yat -sen \nUniversity, Ministry of Education, Guangzhou 510000, China \n \n \nAbstract \nSynthesis planning  is the process of recursively decomposing target molecules into \navailable precursors. Computer-aided retrosynthesis can potentially assist chemists  in \ndesigning synthetic routes, but at present it is cumbersome and provides results of \ndissatisfactory quality.  In this study, w e develop a template-free self-corrected \nretrosynthesis predictor (SCROP) to perform a retrosynthesis prediction task trained by \nusing the Transformer neural network architecture. In the method, the  retrosynthesis \nplanning is converted as a machine translation problem between  molecular linear \nnotations of reactants and the products. Coupled with a neural network -based syntax \ncorrector, our method achieves an accuracy of 59.0% on a standard benchmark dataset, \nwhich increases >21% over other deep learning methods, and >6% over template-based \nmethods. More importantly, our method shows an accuracy 1.7 times higher than other \nstate-of-the-art methods for compounds not appearing in the training set.   \nIntroduction \nOrganic synthesis is one of the fundamental pillars of modern chemical society, as it \nprovides a wide range of compounds from medicines to materials. The synthetic route \nto a desired organic compound is widely constructed by recursively decomposing it into \na set of available reaction building blocks .1 This analysis mode was formalized as \nretrosynthesis by E. J. Corey2-3 who ultimately got the Nobel Prize in 1990.4 \n Planning synthesis requires chemists to predict functional group s reacting with a \ngiven reactant and their reacting poses. Since molecules may have many possible ways \nto decompose, the retrosynthetic analysis of a target compound usually leads to a large \nnumber of possible synthetic routes. It is challenging to select an appropriate synthesis \nroute because the differences between routes are subtle that often depend on the global \nstructures. Therefore, it remains  challenging even for the best ch emists to plan a \nretrosynthetic route for a complex molecule.5-6 \nTo this end, many in silico methods have been developed to assist in designing \nsynthetic routes for novel molecules, among which most are dependent on hand-coded \nreaction templates.7-13 Based on these templates, synthesis routes were then built \naccording to generalized reaction rule s. Therefore, the accuracy of these methods \ndepends on the availability of both templates and reaction rules. The rule-based systems \nrequired an extensive and up-to-date rule-base to cover the majority of known synthetic \napproaches, but such system could cover only a small fraction of chemistâ€™s knowledge \nbase due to  the constant increase in the number of  new reactions.14 Additionally, a \nsimple template is generally not enough to reliably predict reactions, because it only \nidentifies reaction centers and their neighboring atoms  without considering the global \ninformation of the target molecule. \nRecently, the  so-called focused template -based methods were designed through  \nautomatically extracting templates from the reaction databases and applying the rules \nto selected relevant templates. It is critical to select appropriate templates. Segler and \nWaller employed neural network s to score template relevance  based on molecular \nfingerprints.15-16 Later, they showed that the Monte Carlo tree combined with deep \nneural networks  could prioritize templates  and pre-select the most promising  \nretrosynthetic steps. 5 Coley and co -workers also demonstrated an approach for \nautomated retrosynthesis based on analogy to known reactions .17 Albeit sturdy, these \nmethods rely heavily on pre -defined atom mapping to map atoms in the reactants to \natoms in the product, which is still a nontrivial problem.18-19 Besides, commonly used \ntools to identify the atom-mapping are themselves based on databases of expert rules \nand templates, which seems to get stuck in an infinite  loop.20 At any rate,  template-\nbased models have the limitation that they cannot infer reactions outside the chemical \nspace covered by the  templateâ€™s libraries, and thus  are restrained from discovering \nnovel chemistry.16 \nTo overcome the problem, template-free alternatives have emerged over recent years. \nThe key idea is to use a text representation of the reactants and products (like SMILES), \nand thus convert retrosynthesis prediction as machine translation from one language \n(reactants) to the other  language (products). Nam and Kim first described a neural \nsequence-to-sequence (seq2seq) model for the forward reaction prediction task.21 Later, \nLiu and co-workers reported a similar seq2seq model that had comparable performance \nto a rule-based expert system.22 This seq2seq model can be trained in a fully end-to-end \nfashion that does not require atom-mapped reaction examples for training. However, it \ndoesnâ€™t show significant improvement in accuracy compared to the rule-based system \nand produces a great number of chemically invalid outputs. Though the invalid outputs \ncan be easily identif ied according to chemical rules, there are no effective ways to \ncorrect the mistakes. \nMore recently, Transformer architecture shows advantages in machine translation.23 \nIt removes the traditional recurrent units and is based entirely on self-attention \nmechanism, allowing extracting both the local and global features  without regard to \ntheir distance  between the input and output sequences . Schwaller and co -workers \nemployed this model to predict the products of chemical reactions and reached state-\nof-the-art results.24 This indicates that Transformer architecture has the potential to \nperform the retrosynthetic reaction  prediction task , though it  is more challenging \ncompared to the forward one. \nIn this study, we propose a novel template-free self-corrected retrosynthesis predictor \n(SCROP) built on the m ulti-head attention Transformer architecture . Our model \nachieves 59.0% top-1 accuracy on a standard benchmark dataset , which outperforms \nall the state-of-the-art template-free and template-based algorithms. At the same time, \nthe rates of invalid candidate precursors could reduce from 12.1% to 0.7% by coupling \nwith a novel neural network -based syntax checker. When excluding similar reactant s \nfrom the training set, our method achieves an accuracy of 47.6% that is 1.7 times higher \nthan other methods.  More importantly, this model requires no handcrafted templates \nand atom mappings, and can accurately predict subtle chemical disconnections.  \nOverview  \nDataset. We have used the same reaction data as first parsed by Lowe. 25 The dataset \nwas derived from USPTO granted patents that includes 50, 000 reactions that was later \nclassified into 10 reaction classes by Schneider et al, 26 namely USPTO-50K. Figure 1 \nshows the distribution of each reaction class within the USPTO-50K. This dataset was \nalso employed by Liu et al . and Coley et al . for the same task. 17, 22 We follow their \nrandom split strategy, with 40K, 5K, and 5K for training, validating, and testing.  \n \n \nFigure 1. Distribution of Reaction Classes within the USPTO-50K. \n \nThe random split of the dataset that may separate similar reactants into the training \nand test set made the results over-estimated. In order to  accurately estimate the \n\nperformance for unseen compounds (compounds not covered by the chemical space of \ntraining data), we re-constructed a more challenging dataset using cluster split strategy \nas also used in the previous study,27 where the compounds belonging to the same cluster \nwere put to the same subse t during the splitting . Here, we used 2-D similarity \nfingerprints to measure the topological  similarities between target products , and \nemployed the typically  used Bemisâˆ’Murcko atomic frameworks28 to cluster the \nproducts with a similarity threshold of 0.6. The same proportion (80%/10%/10%) was \nused to split t he reactions into training, validation, and test set, namely USPTO-50K-\ncluster. The splitting makes the retrosynthesis prediction task significantly harder, as \nthe model has to determine the reaction center for a target molecule outside its training \n(with a low similarity). \nProblem Definition. Given an input of a target molecule and its specified reaction type, \nour task is to predict likely reactants that can react in the specified reaction type to form \nthe target product. In this study, a reaction is desc ribed by a variable-length string \ncontaining one pair of SMILES notations representing the reactant and target compound. \nFollowing the process of Liu et al, 22 each reaction is split  into a source sequence and \ntarget sequence for model training. The source sequence is the product of the reaction \nwith a reaction type token prepended to the  sequence, and the target sequence is the \nreactant set.  For example, a protection reaction can be described as \nâ€œNCc1ccoc1.S=(Cl)Cl>>[RX_5]S=C=NCc1ccoc1â€, where â€œ[RX_5]S=C=NCc1ccoc1â€ \nis the source sequence, and â€œNCc1ccoc1.S=(Cl)Clâ€ is the target sequence. The â€œ[RX_5]â€ \ntoken denotes the reaction class 5 (protections). \nThese sequences are then encoded into one-hot matrices with a token vocabulary (in \nour case, it has  a totally 50 unique tokens retrieved from  the dataset). In the one -hot \nencoding approach, each sequence is represented by a set of token vectors. All token \nvectors have the same number of components. Each component in a vector is set to zero \nexcept the one at the tokenâ€™s index position. To make the training procedure more stable \nand efficient, the input one-hot matrices are compressed to information-enriched word \nembedding vectors following the previous work.29-30 As a result, each input sequence \nis finally represented in a molecular embedding: \n                        ğ‘š = (ğ‘¡1, ğ‘¡2, â€¦ ğ‘¡ğ‘›)                   (1) \nwhere ğ‘¡ğ‘– is a vector standing for a d dimensional token embedding for the i-th token in \na molecule consisted of n tokens. \n \nFigure 2. Overview of the architecture and training procedure of the Transformer-based \nretrosynthetic reaction predictor. â€˜RX_5â€™ token indicates that the target should be \ndecomposed through a protection reaction. \n \nModel \nOur model predicts the synthetic route for a target molecule in a two-step manner: (1) \napplying a fully-trained retrosynthetic reaction predictor to infer a set of raw candidate \nreactants, and (2) fixing their syntax errors to make more reasonable predictions using \nmolecular syntax corrector. \nRetrosynthesis predictor. In the first stage, we adapted the Transformer architecture \nto map the sequence of products to the sequence of the reactants.23 As shown in Figure \n2, the architecture of the Transformer system follows the so-called encoder-decoder \nparadigm, trained in an end -to-end fashion. The encoder layers attend the source \nmolecular embedding ğ‘šğ‘  = (ğ‘¡1, â€¦ , ğ‘¡ğ‘›)  and i teratively transform it into a latent \n\nrepresentation ğ’ = (ğ‘™1, â€¦ , ğ‘™ğ‘›) . After finishing the e ncoder phasing, each step in the \ndecoding phase outputs a token based on the latent information l until the ending token \nâ€˜</s>â€™ is reached indicating that the transformer decoder has completed its output. The \npredicted output ğ‘¦ğ‘ = (ğ‘¦1, â€¦ , ğ‘¦ğ‘š)  is then used to compare with target reactants \nsequence ğ‘šğ‘Ÿ = (ğ‘¡1, â€¦ , ğ‘¡ğ‘˜). The training goal is to minimize the gap between ğ‘¦ğ‘ and \nğ‘šğ‘Ÿ so that the model can finally infer accurate reactions.  \nSeveral identical layers are stacked for the encoding phase. Each layer comprises a \ncombination of a multi -head self -attention sub -layer and a positional feedforward \nnetwork (FFN) sub -layer. A residual  connection and layer normalization were \nemployed to integrate the two sub-layers.31  \nDifferent from the encoder, the decoder is composed of two types of attention multi-\nhead attention layers: i) a decoder self -attention and ii) an encoder -decoder attention. \nThe decoder self-attention focuses on the previous predictions of reactants made step \nby step, masked by one position. The encoder-decoder attention builds the connection \nbetween the final encoder  representation and the decoder representation.  It integrates \nthe information of the source molecular embeddings with the reactants strings that have \nbeen predicted so far, which helps the decoder focus on appropriate places in the input \nsequence.  \nA multi -head attention unit itself comprised several scaled -dot attention layers  \nperforming the attention mechanism in parallel, which are then concatenated  and \nprojected to the final values . The scaled -dot attention layers take three matrices: the \nqueries Q, the keys K, and the values V. The query, key, and value matrices are created \nby multiplying the input molecular embedding M by three weight matrices that were \nalso trained during the training process. We then compute the attention weight for each \ntoken within a SMILES string as follows: \n ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„, ğ¾, ğ‘‰) = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (\nğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜\n) ğ‘‰               (2) \nThe dot -product of the keys and the queries computes how closely the keys are \ncorrelated with the queries. If the query and key are aligned well, their dot-product will \nbe large. Each key has a corresponding value vector, which is multiplied with the output \nof the softmax. ğ‘‘ğ‘˜ denotes a scaling factor depending on the weight matrices size. By \nthis procedure, the encoder extracts pivotal features from the source sequence, which \nare then queried by the decoder depending on its preceding outputs. Thus, the model \ncan learn the global level information from the input molecular embeddings and build \na semantic connection between encoder and decoder. \nAs the recurrent unit i s removed from the transformer architecture, the model lacks \na way to account for the order of words in the input SMILES st rings. To address this, \nwe used the position encoding as proposed in the previous study,23 which adopted the \nsine and cosine functions to identify the position of different tokens in the sequence: \n      ğ‘ƒğ¸(ğ‘ğ‘œğ‘ ,2ğ‘–) = ğ‘ ğ‘–ğ‘› (\nğ‘ğ‘œğ‘ \nğ‘¡ğ‘–ğ‘šğ‘’ğ‘ ğ‘ğ‘ğ‘™ğ‘’2ğ‘–/ğ‘‘ğ‘’ğ‘šğ‘),  ğ‘ƒğ¸(ğ‘ğ‘œğ‘ ,2ğ‘–+1) = ğ‘ğ‘œğ‘  (\nğ‘ğ‘œğ‘ \nğ‘¡ğ‘–ğ‘šğ‘’ğ‘ ğ‘ğ‘ğ‘™ğ‘’2ğ‘–/ğ‘‘ğ‘’ğ‘šğ‘)  (3) \nwhere pos is the position, i denotes the dimensional index of position encodings. The \noutputs of positional encodings have the same dimension demb as the token embeddings. \nThe timescale is set to10000 to form a geometric progression from 2Ï€ to 10000 Â· 2Ï€.  \n  For a particular source  sequence, the training objective is to minimize the  cross-\nentropy loss function: \nâ„’(ğ‘¦, ğ‘š) =  âˆ’ âˆ‘ ğ‘¦ğ‘–log(ğ‘šğ‘–)ğ¾\nğ‘–=1                     (4) \nwhere ğ‘¦ denotes the predicted sequence and ğ‘š is the target molecular sequence. \nOur best-performance model was trained for 12 hours on one GPU (Nvidia 1080TI) \non the training set, saving one checkpoint every 20, 000 steps and averaging the last ten \ncheckpoints. More detailed hyperparameter settings of the model are shown in Table \nS1. The hyperparameters were chosen according to performances on the validating set. \nA beam search procedure32 is then used to infer multiple reactant candidates on the test \nset. We used the best-performance model to infer the reactant candidates with a beam \nwidth of 10. Therefore, the top ten candidate sequences ranked by overall probability \nare retained.  \n \nMolecular syntax corrector . It is important to note  that syntactically plausible \nmolecular strings are not guaranteed to be semantically valid. For instance, â€˜c1ccocâ€™ \ncannot be deduced to a valid structure because it misses the token â€˜1â€™ representing the \nclosing of the heterocycle. Previous works entirely relied on the raw outputs obtained \nfrom the default beam  search.22, 33 This procedure enumerates the top N predictions \nbased on the joint probability of generated tokens withou t consideration of chemical \nfeasibility. Thus, in the second stage, we build a Transformer-based syntax corrector to \nautomatically correct the syntax of unreasonab le SMILES strings  for improving the \nmodel performance. The design of the neural network is motivated by the grammatical \nerror correction tool widely utilized in natural language processing  tasks.34 Figure 3 \nillustrates the procedure of our syntax correction system. The syntax corrector takes the \nunreasonable predictions generated from the retrosynthetic reaction predictor and fixes \ntheir syntax errors to increase the quality of predictions. \nThe system does this by taking ground truth reactants and invalid reactants generated \nfrom the retrosynthesis predictor to produce input-output pairs (where the output is the \nground truth reactants), which are then used to train a sequence -to-sequence \ntransformer model. Concretely, we first use a fully trained model to generate the top ten \ncandidate precursors given a set of target compounds in the training set. Second, we \nfilter the candidate reactant sets  by removing the ground truth  reactant set s \ncorresponding to the target molecules. Third, we construct a training library that \nconsists of a set of input-output pairs, where the inputs are predicted invalid reactants, \nand the outputs are the ground truth reactants. Given such a syntax correction dataset \nwith input -output pairs, we can train a new sequence -to-sequence model using the \nTransformer architecture introduced above and hook it up to the retrosynthetic reaction \npredictor. Once trained, we can input unsatisfactory SMILES strings generated from \nthe reaction predictor and fix their syntax errors to make more reasonable predictions. \nNote that we only retained the top-1 candidate produced by syntax corrector and replace \nthe original one.  \n \nFigure 3. Example SMILES syntax correction for two invalid predictions generated by \ntransformer-based retrosynthesis predictor. The syntax corrector fixes the syntax errors \nand produces the ground truth reactants.  \n \nAfter correction, we canonicalize all predicted sequences by reordering the token s \nwith fixed rules and compared the predicted candidates with the ground truth reactants. \nAll scripts were written in Python (version 3. 6), and RDKit  was used for data \npreprocessing and  molecule canonicalization.35 The Transformer model was \nimplemented using OpenNMT.36 \nExperiments and results \nWe evaluated the self-corrected retrosynthesis predictor (SCROP) on USPTO-50K data \nsets with two split methods (random and clustering) and compared the performance \nwith other state -of-the-art results (we have repeated their result s as reported in their \noriginal papers ). Table 1 shows the quantitative performance of the model s on the \nUSPTO-50K dataset when the reaction type is known and unknown, respectively. When \ninferring reaction within a specific reaction class, the SCROP outperforms all baselines \nin the top-1 recommendation, achieving an exactly matching accuracy of 59.0%. The \npercentages of correctly predicted reactants by top-3, top -5, and top -10 are 74.8%, \n78.1%, and 81.1%, respectively. These results are much better than the results reported \nin the previous seq2seq model (37.4% of top-1 and 61.7% of top-10).  \nNote that the template -based model of Coley et al.  (similarity) predict ed 100 \ncandidates and picked the top-10 as a result. As the SCROP does not rank candidates \n\nbut was trained on accurately predicting the top -1 outcome and only predicted ten \ncandidates, it is not surprising that the similarity-based method has higher top -5 and \ntop-10 accuracies. Even so, the SCROP improves the similarity-based method by a \nmargin of 6.1% in top-1 accuracy. \nBy comparing with the results of the original retrosynthesis predictor (SCROP-\nnoSC), we find that the syntax corrector  leads to an increase  from top -1 to top -10 \naccuracies, ranging from 0.2% to 1.0%. The growth of top-1 and top-3 accuracy are not \napparent because only a small part of invalid predictions was generated in these two \nstages. \nWithout prior knowledge of the reaction class (removing the reaction type tokens in \nthe training procedure) , the SCROP improves upon the similarity-based method by a \nmargin of 6.4%, 5.7% in top -1, top-3 accuracies, and achieves a comparable result in \ntop-5 and top-10 suggestions.  \n \nTable 1. Comparison of Top -N accuracies between the  baselines and SCROP on \nUSPTO-50K. \nData model top-n accuracy (%), n =  \n1  3  5  10  \nWith reaction class \nLiu-baseline 35.4  52.3  59.1  65.1  \nLiu-seq2seq 37.4  52.4  57.0  61.7  \nsimilarity 52.9  73.8  81.2  88.1  \nSCROP-noSC 58.8 74.4 77.5 80.1 \nSCROP 59.0  74.8  78.1  81.1  \nWithout reaction class \nsimilarity 37.3 54.7 63.3 74.1 \nSCROP-noSC 43.3 59.1 64.0 67.0 \nSCROP 43.7 60.0  65.2 68.7 \n \nGeneralization Estimation. To compare the generalization ability of these approaches, \nwe further evaluated the models on USPTO-50K-cluster. We retrained both the seq2seq \nand similarit y-based methods on the USPTO -50K-cluster with the same parameter \nsettings as in the original papers, except the generated candidates in the similarity-based \nmethod was set to 10 rather than 100 for a fair comparison. Table 2 shows the modelsâ€™ \nperformance aggregated across all classes on the USPTO-50K-cluster dataset. By this \ntable, we observe that the SCROP achieved an order-of-magnitude improvement over \nbaselines within or without reaction class from top-1 to top-10 predictions. Besides, we \nfind that our model performs more stable compared to the template-based method. The \ntop-1 accuracy of our model decreases only by 7. 5% compared to 1 6.6% of the \nsimilarity method when planning retrosynthesis for clustered dataset without the \nknowledge of reaction class. This result demonstrates that our template-free method has \nbetter generalization ability than the template-based one when the training dataset has \nno similar compound to the target compounds. \n \nTable 2. Comparison of top-N accuracies between the  baselines and SCROP on \nUSPTO-50K-cluster.  \nData model top-n accuracy (%), n =  \n1 3 5 10 \nWith reaction class \nseq2seq 25.5  38.7  43.6  49.0  \nsimilarity 36.7  58.0  61.4  67.2  \nSCROP 47.6  63.9 68.1  71.1  \nWithout reaction class \nseq2seq 16.5  28.8  34.0  40.6  \nsimilarity 20.7  36.3  43.2  46.9  \nSCROP 36.2 52.0  57.1  60.9 \n \nThe detailed top-10 results on USPTO-50K-cluster dataset for the baseline models and \nour model broken down by the reaction classes are shown  in Table 3. The SCP OR \nperforms significantly better than the seq2seq model  in reaction class 4 (heterocycle \nformation). The common feature of this reaction type is the formation of cyclic \nstructures, resulting in a significant difference between the reactant SMILES string and \nthe target product SMILES string. This result shows that the SCPOR is able to induce \nbetter syntactic relationships and capture global chemical information from the reaction \ndata. The SCPOR also outperforms the seq2seq and similarity models by a large margin \nin class 1 (heteroatom alkylation and arylation). The key feature of  this reaction class \nis that the reactions are possibly happened with many different functional groups within \nthe target molecules. It is hard to identify the accurate reaction site when the structure \nof target molecules is not similar to the one in the knowledge base. As a result, t he \nSCPOR shows better g eneralization ability in this reaction type and  infers reactants \ncorrectly.  \\  \n \nTable 3. Model top-10 accuracy within each class on USPTO-50K-cluster when the \nreaction type is known.  \nmodel top-10 accuracy (%), reaction class= \n1  2  3  4  5  6  7  8  9  10  \nseq2seq 43.9  58.7  29.8  11.9  63.5  54.4  66.0  54.4  43.3  45.8  \nsimilarity 56.4  77.0  50.8  52.4  86.2  77.5  73.6  86.2  58.8  85.3  \nSCROP 71.8  78.9  57.2  56.2  85.1  68.5  79.9  69.6  68.0  68.8  \n \nEvaluation of syntax corrector . In Liu â€™s work, 7 incorrect predictions  were \nsummarized as three types: grammatically invalid outputs, grammatically valid but \nchemically unreasonable, and chemically plausible but do not match to the ground truth. \nIn the above comparison, we only used the syntax corrector  to fix the grammatically \ninvalid outputs  since the ide ntification of the other two types of errors requir es the \nknowledge of ground truth reactants. As shown in Figure 4, for SCROP, only 0.7% of \nthe top -1 and 2.3% of the top -10 predictions are grammatically invalid, which is \nsignificantly better than the retrosynthesis predictor (SCROP-noSC) and seq2seq model. \nFigure 5 shows an example of retrosynthetic predictions  coupling with the syntax \ncorrector. The model successfully proposes the ground truth by correcting the original \nrank 2 prediction, which is a n invalid molecule string.  Besides, the syntax corrector \nmakes the rank 3 prediction more reasonable, even if it is not the correct answer. This \nresult suggests that the syntax corrector can be used to optimize the invalid predictions. \nIt provides a new strategy to optimize the synthesis route when chemists find that the \nreaction is going to a wrong way. \n \nFigure 4. Comparison of invalid rates among seq2seq, retrosynthesis predictor  \n(SCROP-noSC) and self-corrected retrosynthesis predictor (SCROP) for different beam \nsizes.  \n \n \nFigure 5. Example of retrosynthetic predictions coupling with the syntax corrector. The \nmodel successfully proposes the ground truth  by correcting the original rank 2 \nprediction, which is an invalid molecule string. Red color denotes incorrect grammar. \n \n\nAttention analysis. To investigate what the model has learned, we further analyzed the \nattention weights in our model . The attention weights provide clues on tokens in the \ninput string that were considered to be more critical when a particular symbol in the \noutput string was generated.  Figure 6, for example, shows the t op-1 candidateâ€™ s \nattention maps of an acylation reaction extracted from the SCROP (accurately predicted) \nand seq2seq (wrong predicted) models. We observe that strong weights trend diagonally \nin SCROPâ€™s attention map, which constantly align SMILES substrings that are shared \nbetween the input and output. Besides, when inferring the unseen reaction sites â€˜OHâ€™ \nand â€˜.â€™, the model can simultaneously take several non-continuous tokens into account. \nThis suggests that the SCROP tries to extract both the local and global information of \nthe source sequence . In comparison, the attention map of the seq2seq model fails to \nalign the substrings of the input-output pair and results in a wrong prediction. \n \n \nFigure 6. Top-1 candidateâ€™s attention maps of an acylation reaction extracted from the \nSCROP (accurately predicted) and seq2seq (wrong predicted) models. \n \nLimitation. A distinct disadvantage is that the model scores candidates only taking \naccount of the overall probability of the predicted string s. Practically, there are many \n\nadditional considerations in synthetic route design, such as process complexity, expense, \nproductivity, safety, and so on. This is because the public data sets do not contain \nadditional technical information except for the reaction strings itself. We would explore \nthe scoring system by incorporating additional considerations  like similarity, reaction \ncomplexity, cost, and reaction yield to optimize the present evaluation metric. \nAnother limitation of the model is the multi-step reaction. A possible option would \nbe to recursively decompose the target compound using the transformer prediction \nsystem until the commercially available building block is obtained. Monte Carlo tree \nsearch could be employed to score the outputs in each single reaction step.5 \nBesides, f inding the best -performing set of hyperparameters for a deep  neural \nnetwork as well as the inference procedure  is computationally  expensive, as many \nhyperparameter settings take tens of hours to train using one GeForce GTX1080Ti \ngraphics card. For this reason, we use a rough grid search to identify the final settings \nin training and inferring procedure. Better optimization techniques with su fficient \nequipment may result in a further increase in accuracy.  \nConclusions \nIn this work, we have proposed a novel template -free deep learning method for \nchemical retrosynthetic prediction . Instead of generating candidate precursors by \nreaction templates , we employ Transformer neural networks  to generate potential \ncandidates by learning the sequential representation of reactant -product pairs . Our \nmodel achieves 59.0% top -1 accuracy on a standard benchmark dataset , which \noutperforms all the state-of-the-art template-free and template-based algorithms. At the \nsame time, the rates of invalid candidate precursors could reduce from 12.1% to 0.7% \nby coupling with a novel neural network-based syntax checker. When excluding similar \nreactants from the training set,  our method achieves an  accuracy of 47.6% that is 1.7 \ntimes higher than other methods. More importantly, our method is trained in an end-to-\nend fashion and is free of chemical rules, and the accuracy will improve automatically \nwith the increase of the training samples.  \n \nAcknowledgment \nThe work was supported in part by the National Key R&D Program of China \n(2018YFC0910500), GD Frontier & Key Tech Innovation Program (2019B020228001), \nthe National Natural Science Foundation of China (61772566, U1611261 and \n81801132), and the  program for Guangdong Introducing Innovative and \nEntrepreneurial Teams (2016ZT06D211).  \n \nAuthor information \n*To whom correspondence should be addressed. \nYuedong Yang: yangyd25@mail.sysu.edu.cn \nJun Xu: junxu@biochemomes.com \n \nCompeting interests \nThe authors declare that they have no competing interests \n \nAuthorsâ€™ contributions \nSZ, JR, and YY contributed concept and implementation. SZ and JR co-designed \nexperiments. SZ and JR were responsible for programming. All authors contributed to \nthe interpretation of results. SZ and YY wrote the manuscript. All authors reviewed \nand approved the final manuscript. \n \n \nReference \n1. Robinson, R., LXIII.â€”A synthesis of tropinone. Journal of the Chemical Society, \nTransactions 1917, 111, 762-768. \n2. Corey, E. J.; Wipke, W. T., Computer-assisted design of complex organic syntheses. \nScience 1969, 166, 178-192. \n3. Corey, E. J., General methods for the construction of complex molecules. Pure and \nApplied chemistry 1967, 14, 19-38. \n4. Corey, E. J.; Long, A. K.; Rubenstein, S. D., Computer-assisted analysis in organic \nsynthesis. Science 1985, 228, 408-418. \n5. Segler, M. H. S.; Preuss, M.; Waller, M. P., Planning chemical syntheses with deep \nneural networks and symbolic AI. Nature 2018, 555, 604-610. \n6. Collins, K. D.; Glorius, F., A robustness screen for the rapid assessment of chemical \nreactions. Nature chemistry 2013, 5, 597. \n7. Christ, C. D.; Zentgraf, M.; Kriegl, J. M., Mining electronic laboratory notebooks: \nanalysis, retrosynthesis, and reaction based enumeration. J Chem Inf Model 2012, 52, \n1745-1756. \n8. Szymkuc, S.; Gajewska, E. P.; Klucznik, T.; Molga, K.; Dittwald, P.; Startek, M.; \nBajczyk, M.; Grzybowski, B. A., Computer -Assisted Synthetic Planning: The End of \nthe Beginning. Angew Chem Int Ed Engl 2016, 55, 5904-5937. \n9. Cook, A.; Johnson, A. P.; Law, J.; Mirzazadeh, M.; Ravitz, O.; Simon, A., \nComputerâ€aided synthesis design: 40 years on. Wiley Interdisciplinary Reviews: \nComputational Molecular Science 2012, 2, 79-107. \n10. Ihlenfeldt, W. D.; Gasteiger, J., Computerâ€assisted planning of organic syntheses: \nthe second generation of programs. Angew Chem Int Ed Engl 1996, 34, 2613-2633. \n11. Todd, M. H., Computer-aided organic synthesis. Chemical Society Reviews 2005, \n34, 247-266. \n12. Kayala, M. A.; Azencott, C.-A.; Chen, J. H.; Baldi, P., Learning to predict chemical \nreactions. J Chem Inf Model 2011, 51, 2209-2222. \n13. BÃ¸gevig, A.; Federsel, H.-J. r.; Huerta, F.; Hutchings, M. G.; Kraut, H.; Langer, T.; \nLoÌˆ w, P.; Oppawsky, C.; Rein, T.; Saller, H., Route design in the 21st century: The IC \nSYNTH software tool as an idea generator for synthesis prediction. Organic Process \nResearch & Development 2015, 19, 357-368. \n14. Coley, C. W.; Green, W. H.; Jensen, K. F., Machine Learning in Computer-Aided \nSynthesis Planning. Acc Chem Res 2018, 51, 1281-1289. \n15. Segler, M. H. S.; Waller, M. P., Neural -Symbolic Machine Learning for \nRetrosynthesis and Reaction Prediction. Chemistry 2017, 23, 5966-5971. \n16. Segler, M. H. S.; Waller, M. P., Model ling Chemical Reasoning to Predict and \nInvent Reactions. Chemistry 2017, 23, 6118-6128. \n17. Coley, C. W.; Rogers, L.; Green, W. H.; Jensen, K. F., Computer -Assisted \nRetrosynthesis Based on Molecular Similarity. ACS Cent Sci 2017, 3, 1237-1245. \n18. Jaworski, W.; Szymkuc, S.; Mikulak -Klucznik, B.; Piecuch, K.; Klucznik, T.; \nKazmierowski, M.; Rydzewski, J.; Gambin, A.; Grzybowski, B. A., Automatic mapping \nof atoms across both simple and complex chemical reactions. Nat Commun 2019, 10, \n1434. \n19. Chen, W. L.; Chen, D. Z.; Taylor, K. T., Automatic reaction mapping and reaction \ncenter detection. Wiley Interdisciplinary Reviews: Computational Molecular Science \n2013, 3, 560-593. \n20. Schneider, N.; Lowe, D. M.; Sayle, R. A.; Landrum, G. A., Development of a novel \nfingerprint for chemical reactions and its application to large -scale reaction \nclassification and similarity. J Chem Inf Model 2015, 55, 39-53. \n21. Nam, J.; Kim, J. Linking the Neural Machine Translation and the Prediction of \nOrganic Chemistry Reactions. arXiv preprint arXiv:1612.09529 2016. \n22. Liu, B.; Ramsundar, B.; Kawthekar, P.; Shi, J.; Gomes, J.; Luu Nguyen, Q.; Ho, S.; \nSloane, J.; Wender, P.; Pande, V ., Retrosynthetic Reaction Prediction Using Neural \nSequence-to-Sequence Models. ACS Cent Sci 2017, 3, 1103-1113. \n23. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \nL.; Polosukhin, I. Attention Is All You Need. arXiv preprint arXiv:1706.03762 2017. \n24. Schwaller, P.; Laino, T.; Gaudin, T.; Bolgar, P.; Bekas, C.; Lee, A. A., Molecular \ntransformer for chemical reaction prediction and uncertainty estimation. arXiv preprint \narXiv:1811.02633 2018. \n25. Lowe, D. M., Extraction of chemical structures and reactions from the literature; \nUniversity of Cambridge 2012. \n26. Schneider, N.; Stiefl, N.; Landrum, G. A., What's What: The (Nearly) Definitive \nGuide to Reaction Role Assignment. J Chem Inf Model 2016, 56, 2336-2346. \n27. Hawkins, D. M.; Basak, S. C.; Mills, D., Assessing model fit by cross -validation. \nJournal of chemical information and computer sciences 2003, 43, 579-586. \n28. Bemis, G. W.; Murcko, M. A., The properties of known drugs. 1. Molecular \nframeworks. Journal of medicinal chemistry 1996, 39, 2887-2893. \n29. Levy, O.; Goldberg, Y . In Neural word embedding as implicit matrix factorization, \nAdvances in neural information processing systems, 2014; pp 2177-2185. \n30. Zheng, S.; Yan, X.; Yang, Y .; Xu, J., Identifying Structure-Property Relationships \nthrough SMILES Syntax Analysis with Self -Attention Mechanism. J Chem Inf Model \n2019, 59, 914-923. \n31. He, K.; Zhang, X.; Ren, S.; Sun, J. Deep Residual Learning for Image Recognition. \narXiv preprint arXiv:1512.03385 2015. \n32. Ow, P. S.; Morton, T. E., Filtered beam search in scheduling. The International \nJournal Of Production Research 1988, 26, 35-62. \n33. Schwaller, P.; Gaudin, T.; Lanyi, D.; Bekas, C.; Laino, T., \"Found in Translation\": \npredicting outcomes of complex organic chemistry reactions using neural sequence-to-\nsequence models. Chem Sci 2018, 9, 6091-6098. \n34. Ng, H. T.; Wu, S. M.; Briscoe, T.; Hadiwinoto, C.; Susanto, R. H.; Bryant, C. In \nThe CoNLL -2014 shared task on grammatical error correction , Proceedings of the \nEighteenth Conference on Computational Natural Language Learning: Shared Task, \n2014; pp 1-14. \n35. RDKit: Open-source cheminformatics, Version: 2018-09-3. http://www.rdkit.org. \n36. Klein, G.; Kim, Y .; Deng, Y .; Senellart, J.; Rush, A. M. OpenNMT: Open-Source \nToolkit for Neural Machine Translation. arXiv preprint arXiv:1701.02810 2017. \n "
}