{
  "title": "Enhancing Cross-lingual Biomedical Concept Normalization Using Deep Neural Network Pretrained Language Models",
  "url": "https://openalex.org/W4286491780",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2571537279",
      "name": "Ying-Chi Lin",
      "affiliations": [
        "Leipzig University"
      ]
    },
    {
      "id": "https://openalex.org/A2412839884",
      "name": "Phillip Hoffmann",
      "affiliations": [
        "Leipzig University"
      ]
    },
    {
      "id": "https://openalex.org/A2158742306",
      "name": "Erhard Rahm",
      "affiliations": [
        "Leipzig University"
      ]
    },
    {
      "id": "https://openalex.org/A2571537279",
      "name": "Ying-Chi Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2412839884",
      "name": "Phillip Hoffmann",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2158742306",
      "name": "Erhard Rahm",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3130491114",
    "https://openalex.org/W2043705607",
    "https://openalex.org/W2116817969",
    "https://openalex.org/W2033442056",
    "https://openalex.org/W2169677418",
    "https://openalex.org/W3012501985",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2294370754",
    "https://openalex.org/W2997006708",
    "https://openalex.org/W3034229721",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3042631625",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3116423158",
    "https://openalex.org/W3164540570",
    "https://openalex.org/W3166593409",
    "https://openalex.org/W2964271799",
    "https://openalex.org/W3100806282",
    "https://openalex.org/W1588434295",
    "https://openalex.org/W2316878322",
    "https://openalex.org/W2113021202",
    "https://openalex.org/W2522781938",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W2765765241",
    "https://openalex.org/W3172806051",
    "https://openalex.org/W3104033643"
  ],
  "abstract": "Abstract In this study, we propose a new approach for cross-lingual biomedical concept normalization, the process of mapping text in non-English documents to English concepts of a knowledge base. The resulting mappings, named as semantic annotations, enhance data integration and interoperability of documents in different languages. The US FDA (Food and Drug Administration), therefore, requires all submitted medical forms to be semantically annotated. These standardized medical forms are used in health care practice and biomedical research and are translated/adapted into various languages. Mapping them to the same concepts (normally in English) facilitates the comparison of multiple medical studies even cross-lingually. However, the translation and adaptation of these forms can cause them to deviate from its original text syntactically and in wording. This leads the conventional string matching methods to produce low-quality annotation results. Therefore, our new approach incorporates semantics into the cross-lingual concept normalization process. This is done using sentence embeddings generated by BERT-based pretrained language models. We evaluate the new approach by annotating entire questions of German medical forms with concepts in English, as required by the FDA. The new approach achieves an improvement of 136% in recall, 52% in precision and 66% in F-measure compared to the conventional string matching methods.",
  "full_text": "Vol.:(0123456789)\nSN Computer Science (2022) 3:387 \nhttps://doi.org/10.1007/s42979-022-01295-7\nSN Computer Science\nORIGINAL RESEARCH\nEnhancing Cross‚Äëlingual Biomedical Concept Normalization Using \nDeep Neural Network Pretrained Language Models\nYing‚ÄëChi¬†Lin1 ¬†¬∑ Phillip¬†Hoffmann1¬†¬∑ Erhard¬†Rahm1\nReceived: 13 October 2021 / Accepted: 30 June 2022 / Published online: 21 July 2022 \n¬© The Author(s) 2022, corrected publication 2022\nAbstract\nIn this study, we propose a new approach for cross-lingual biomedical concept normalization, the process of mapping \ntext in non-English documents to English concepts of a knowledge base. The resulting mappings, named as semantic \nannotations, enhance data integration and interoperability of documents in different languages. The US FDA (Food and \nDrug Administration), therefore, requires all submitted medical forms to be semantically annotated. These standardized \nmedical forms are used in health care practice and biomedical research and are translated/adapted into various languages. \nMapping them to the same concepts (normally in English) facilitates the comparison of multiple medical studies even \ncross-lingually. However, the translation and adaptation of these forms can cause them to deviate from its original text \nsyntactically and in wording. This leads the conventional string matching methods to produce low-quality annotation \nresults. Therefore, our new approach incorporates semantics into the cross-lingual concept normalization process. This is \ndone using sentence embeddings generated by BERT-based pretrained language models. We evaluate the new approach by \nannotating entire questions of German medical forms with concepts in English, as required by the FDA. The new approach \nachieves an improvement of 136% in recall, 52% in precision and 66% in F-measure compared to the conventional string \nmatching methods.\nKeywords Biomedical concept normalization¬†¬∑ BERT¬†¬∑ Sentence embedding¬†¬∑ Cross-lingual¬†¬∑ UMLS\nIntroduction\nConcept normalization, also named as semantic annotation \nor entity linking, aims to map a sequence of text to a concept \nof a given knowledge base, such as an ontology, taxonomy \nor thesaurus. Those mappings or annotations have been \napplied to enhance search engines, data integration or drug \ndiscovery. For example, the MEDLINE database contains \njournal citations and abstracts for biomedical literature. The \ndata in MEDLINE are annotated using the MeSH (Medical \nSubject Headings) vocabulary. PubMED,1 the search engine \naccessing the MEDLINE database, uses these annotations to \nimprove retrieval speed and quality.\nSemantic annotations enhance interoperability of the doc-\numents and facilitates data integration. The CDISC Stand-\nards, jointly developed by the US Food and Drug Adminis-\ntration (FDA) and the Clinical Data Interchange Standards \nConsortium (CDISC), define the baselines of the interchange \nformat of medical research data. Since 2016, regulatory sub-\nmissions to the FDA such as new drug applications have \nto comply with those standards, that incorporate semantic \nannotation of any submitted medical form. These study data \nstandards ensure the FDA to process the submissions more \nefficiently. Furthermore, they also facilitate the FDA to solve \nresearch questions that need to integrate data from multiple \nstudies. The vocabulary used for the annotations are defined \nin the Study Data Tabulation Model Controlled Terminology \nThis article is part of the topical collection ‚ÄúBiomedical \nEngineering Systems and Technologies‚Äù guest edited by Hugo \nGamboa and Ana Fred.\n * Ying-Chi Lin \n lin@informatik.uni-leipzig.de\n Phillip Hoffmann \n ph30gabo@studserv.uni-leipzig.de\n Erhard Rahm \n rahm@informatik.uni-leipzig.de\n1 ScaDS.AI/Department of¬†Computer Science, Leipzig \nUniversity, Augustusplatz 10, 04109¬†Leipzig, Germany 1 PubMed https:// www. ncbi. nlm. nih. gov/ pubmed.\n SN Computer Science (2022) 3:387\n387 Page 2 of 20\nSN Computer Science\n(SDTM-CT), which is maintained and distributed as part \nof the NCI Thesaurus. This terminology covers a large set \nof medical forms, clinical studies and questionnaires, for \ninstance, the Epworth Sleepiness Scale (ESS) Questionnaire \nand the Hamilton Depression Rating Scale (HAMD). Here, \nan entire question is assigned to a unique corresponding con-\ncept of the ontology. In this study, we focus on such type of \nannotatiossns.\nCross-lingual concept normalization denotes the process \nof annotating non-English documents using English con-\ncepts. This process is needed because the portion of English \nconcepts still dominates most of the knowledge bases. For \ninstance, one of the largest biomedical ontology sources, \nthe Unified Medical Language System (UMLS) Metathe-\nsaurus,2 contains more than 16.1 million terms in its current \nversion, 2021AA. Thereof, 71% are in English, followed by \n10% in Spanish and only 3% of all terms are in French or \nPortuguese, respectively. To augment the interoperability of \nnon-English documents, cross-lingual concept normalization \nis indispensable. It is especially a necessity for finding the \ncorresponding concepts of entire question as such concepts \nare not available in non-English languages.\nIt is common that medical forms are translated into \nother languages for the application in non-English speak -\ning regions, such as for clinical or epidemiological studies. \nAnnotating these non-English forms using the same English \nconcepts is not only a requirement of the FDA but also ena-\nbles the comparison between multiple studies carried out \nin different languages. Figure¬† 1 presents examples of such \ncross-lingual semantic annotations. The same standardized \nforms in various languages shall retain conceptually equiva-\nlent meaning. Hence, many of these forms have not only \nbeen translated into a new language but also gone through \nsome cultural adaptation and validation processes. For exam-\nple, the GAD-7 (Generalized Anxiety Disorder-7) form was \nfirst published in English in 2006 [2]. It has been translated \nand adapted/validated into Portuguese [3 ], into German [4] \nand into Spanish [5]. The adaptation might result in further \nmodifications on the question text, which can complicate the \ncross-lingual concept normalization process.\nMapping questions to concepts in the same language \n(normally in English) is a trivial task as the concepts are \nmostly syntactically identical to the question, since the \nconcepts are derived from standardized forms. In fact, our \nprevious studies [ 1, 6] show that the conventional string \nmatching methods can already deliver good results. On \nthe contrary, such methods perform poorly in a cross-\nlingual context due to text deviation caused by translation \nand adaptation. However, no matter cultural adaption or \n(machine/manual) translation, the semantics of the questions \nshall still be preserved. As a consequence, we proposed \nthe idea of using deep neural network models to generate \nsentence embeddings as semantic representations of the \nquestions and the concepts [1 ]. We achieved a substantial \nimprovement of the annotation quality and proved that \nsemantic embedding methods are superior to string matching \nbased methods in a cross-lingual setting.\nIn this work, we expand our previous work [1 ] and aim \nto further improve the annotation quality by three means: \n(1) by applying new encoders (2) by injecting UMLS \ninto new models and (3) by refining the post-processing \nthrough re-ranking annotation candidates. This study has \nthe following main contributions: \n(1) We refine the workflows of using deep network \nsentence encoders for cross-lingual biomedical concept \nnormalization.\n(2) We investigate the annotation quality using Biomedical \nPretrained Language Models (BPLMs) as encoders.\n(3) We include more state-of-the-art (SOTA) Sentence \nBERT (SBERT) encoders.\n(4) We perform UMLS injection into the SBERT encoders \nand evaluate their performance.\n(5) We apply candidate re-ranking using Cross-\nEncoder and test its impact on the annotation quality.\nFig. 1  Cross-lingual annotation examples of two questions of medi-\ncal forms. On the left, the original English questions (OE), their Ger -\nman version (DE) and their translations using Google Translate (GO) \nare listed. On the right, the mapped UMLS concepts are shown. In \nUMLS, each concept is assigned with a CUI (Concept Unique Identi-\nfier). (adapted from [1])\n2 https:// www. nlm. nih. gov/ resea rch/ umls/ knowl edge_ sourc es/ metat \nhesau rus/ relea se/ stati stics. html.\nSN Computer Science (2022) 3:387 \nPage 3 of 20 387\nSN Computer Science\n(6) We further enhance performance by combining single \nmodel results with set operations.\nBackground and¬†Related Work\nIn this section, we briefly describe the recent development \nof the pretrained language models with the main focus \non BERT (Bidirectional Encoder Representations from \nTransformers, [7 ]) and its derivatives. The BERT-models \nhave achieved many SOTA results in various natural \nlanguage processing tasks (examples see GLUE 3 and \nSuperGLUE 4 benchmarks). This also motivates us to \nintegrate some of these models in our workflows for solving \nthe concept normalization problem.\nBERT consists of multi-layer bidirectional Transformer \nencoder based on the Transformer implementation in [8]. It \nwas released as two sizes: BERTbase consists of 12 Transformer \nlayers and BERTlarge has 24 layers. BERT is trained using two \nunsupervised tasks: (1) masked language model (MLM) objec-\ntive and (2) next sentence prediction (NSP). With the MLM, \na certain percentage (usually 15%) of the input tokens are \nmasked at random and BERT learns to predict those masked \ntokens. With the NSP task, BERT is trained to understand the \nrelationship between sentences such as in Question Answering \n(QA) and Natural Language Inference (NLI) tasks. The initial \nBERT is pretrained on the BooksCorpus (800 M words) [9] \nand the English Wikipedia (2500 M words).\nLiu et¬†al. [10] modified the BERT‚Äôs pretraining approach \nand proposed RoBERTa (robustly optimized BERT \napproach). Here, they remove the NSP objective, use dynamic \nmasking for the MLM and increase the mini-batch size. In \naddition, RoBERTa is trained for more steps and with much \nmore data (use 160 GB instead of 13 GB). These approaches \nhave advanced BERT to a better performing model.\nSince BERT is relatively resource intensive to apply, Sanh \net¬†al. [11] developed a light-weighted version of BERT, the \nDistilBERT. The model is compressed using the so-called \nknowledge distillation [ 12, 13], where a compact model‚Äî\nthe student‚Äîis trained to reproduce the behavior of a more \ncomplex model‚Äîthe teacher‚Äîby minimizing the differ -\nences between the model features. The DistilBERT com-\nprises only 6 Transformer layers and has 40% fewer param-\neters. Nevertheless, it is 60% faster and still retains roughly \n97% of BERT‚Äôs performance on the GLUE benchmark.\nMiniLM [14] is another light-weighted variant of BERT. \nThe compression method of MiniLM, termed as deep self-\nattention distillation, is also based on knowledge distilla-\ntion principles but with some modifications. The approach \ndistills the self-attention distribution and self-attention value \nrelation of the last Transformer layer of the teacher model. \nIn addition, it also incorporates an intermediate-size stu-\ndent model, named as teacher assistant [15]. The teacher \nassistant distills the teacher model first and is subsequently \nused as the teacher to guide the training of the final student \nmodel. MiniLM outperforms DistilBERT in the majority of \nGLUE benchmark tasks and achieves a slightly lower aver-\nage GLUE score compared to BERTbase [14].\nMPNet (masked and permuted language modeling  [16]) \nwas proposed to overcome two problems. The first problem \nis that the MLM of BERT ignores a potential dependency \nof the masked tokens. To address this disadvantage, XLNet \n[17] was introduced that uses permuted language modeling \n(PLM) as pretraining method. PLM inherits the benefits of \nautoregressive modeling but also allows the model to be \ntrained in a bidirectional manner. However, it suffers from \nposition discrepancy between pretraining and fine-tuning, \nwhich evokes the second problem. With MLM, BERT cap-\ntures the position information and sees 85% of the input \n(if 15% of the tokens are masked). On the other hand, \nPLM does not have any position information, as the input \nsequence is presented in a permuted manner and the model \nonly sees the preceding tokens of the to-predict token. \nThis leads inevitably to the above-mentioned discrepancy \nbetween pretraining and fine-tuning of downstream tasks, \nwhere the model can see the entire input sequence. Conse-\nquently, MPNet introduces position compensation to PLM \nand alleviates the previously mentioned issues [16].\nSentence-BERT (SBERT) and SBERT-WK In this \nstudy, we incorporate many pretrained SBERT models \n[18] as our sentence encoders. Our concept normalization \ntask involves finding the most similar pair of sentences in \na large dataset. Using BERT for such type of comparison \nis computationally expensive as it requires each sentence \npair to be input into the network separately. For a compari-\nson of 10,000 sentences, BERT needs 50 million inference \ncomputations ( ‚àº 65 h, [18]). It is infeasible for us as the \nontologies we use contain over 1 million entries. Hence, \nReimers et¬†al. [18] proposed SBERT to overcome such inef-\nficiency. SBERT uses the above-mentioned BERT variants \nas backbone and adds a pooling operation (generally the \nmean pooling) to generate a fixed-sized sentence embedding. \nThe models are trained using Siamese or triplet networks. \nThe generated embeddings can be compared using similarity \nmeasures such as cosine similarity.\nThe SBERT-WK 5 [ 19] aims to refine the sentence \nembeddings generated by SBERT. It modifies the SBERT \nword embeddings based on how informative/important \nthe word is. The importance of a word is defined by its \n3 General Language Understanding Evaluation https:// glueb enchm \nark. com.\n4 SuperGLUE https:// super. glueb enchm ark. com/ leade rboard. 5 WK stands for the initials of the two authors.\n SN Computer Science (2022) 3:387\n387 Page 4 of 20\nSN Computer Science\nneighboring words of the same layer and the changes of \nits cosine similarities through layers. When a word aligns \nwell with its neighboring word vectors, it is less informa-\ntive. Similarly, a word which evolves faster across layers \n(larger variance of the pair-wise cosine similarity), it is \nmore important. Since this pooling strategy only alters \nthe already generated embeddings, no further training is \nneeded.\nBERT-based biomedical pretrained language mod-\nels (BPLMs) Since the publication of the BERT model in \n2018 [7], various efforts have been made to adapt it for the \nbiomedical domain. We name these as BERT-based Bio-\nmedical Pretrained Language Models (BPLMs). The earli-\nest BPLM is BioBERT [20]. It uses the original pretrained \nBERT (pretrained on BooksCorpus and English Wikipedia) \nas base model and is further trained with PubMed abstracts \nand PubMed Central full-text articles (PMC). A few months \nlater, Alsentzer et¬†al. [21] published Clinical BERT. One of \nits best performing variants uses BioBERT as base model \nand is trained with approximately 2 million MIMIC-III \nv1.4 clinical notes [22]. The BlueBERT [23] can be under -\nstood as a combination of BioBERT and Clinical BERT. \nIt has four variants depending on base model size (either \nBERTbase or BERTlarge ) and used training corpus (trained on \nthe PubMed corpus solely or additional with the MIMIC-\nIII corpus). Interestingly, the large-models do not perform \nbetter than the base-models. The BERTbase-variant trained \nsolely on the PubMed corpus is analogous to BioBERT, \nyet the BlueBERT variant is trained for more steps (5 M \nsteps instead of 0.2 M steps). Experiments on various NLP \ntasks show that this increase in training steps does improve \nthe results.\nThe above-mentioned BPLM models are all derivatives \nof BERT which is already pretrained on BooksCorpus and \nEnglish Wikipedia. Gu et¬†al. [24] challenge such contin-\nual pretraining and argue that training BERT from scratch \nusing domain-specific corpora is more beneficial when deal-\ning with domain-specific tasks. They pretrained the BERT \nmodel from scratch using the PubMed corpus and name their \nmodel as PubMedBERT. In addition to PubMedBERT, the \nauthors also create a new benchmark, the Biomedical Lan-\nguage Understanding and Reasoning Benchmark (BLURB), \nwhich comprises biomedical NLP tasks focusing on Pub -\nMed-based applications. PubMedBERT outperforms the \nabove-mentioned models in almost every BLURB task \n(only BioBERT is better in 2 of the 13 tasks). Hence, they \nconclude that to solve domain-specific tasks, it is better to \nuse models entirely pretrained on domain-specific corpora \nthan to use models that have already been trained with¬†out-\ndomain corpora.\nUMLS injected BPLMs Various studies have shown that \nfor named-entity recognition or concept normalization tasks \nextra training of the language models on a given knowledge \nbase is beneficial [16, 25‚Äì28]. Thus, we incorporate two such \nmodels in our workflows: CODER [27] and SapBERT [26]. \nBoth approaches propose pretraining using UMLS syno-\nnyms, referred to as UMLS injection. In addition, CODER \nalso embeds the relationships between the concepts into the \nvector representation. CODER has two versions. The English \nversion, CODERENG , uses PubMedBERT as base model and \nthe multilingual version, CODERALL , uses multilingual BERT \nas base model. Both versions differ also in training corpus: \nCODERENG utilizes only the English concepts in the UMLS \nwhile CODERALL is trained on concepts of all languages. \nCODER applies contrastive learning. Here, term representa-\ntions are learnt by maximizing cosine similarity between posi-\ntive term-term pairs (i.e., between synonyms of a given concept) \nand term-relation-term pairs.\nThe SapBERT achieves many SOTA results on the medi-\ncal entity linking (MEL) benchmark. The Self-Alignment \nPretraining (SAP) is a procedure that learns to self-align \nsynonyms in the UMLS and can also be used for fine-tuning \non task-specific datasets. During pretraining, an online hard \ntriplet mining is necessary to locate the most informative \ntraining examples. With each mini-batch, all possible triplets \nfor all terms are constructed. A triplet (xa,xp ,xn ) contains an \nanchor xa , an arbitrary term in the mini-batch and the xp and \nxn denote each either a positive or a negative match of the xa . \nOnly the triplets are retained for pretraining if they satisfy \nthe following constraint :\nwhere f is modeled by a BERT model and /u1D706 is a predefined \nmargin. In other words, only triplets with negative samples \nthat are very similar (in the paper they use cosine similarity) \nto the positive sample by a margin of /u1D706 are kept for \npretraining. They use Multi-Similarity loss function [29] as \nlearning objective that leverages the similarities among and \nbetween positive and negative pairs.\nMultilingual pretrained language models In the current \nstudy we apply several multilingual pretrained language \nmodels. We choose the pretrained models developed by \nthe same authors of SBERT. Further, we also include the \nmultilingual versions of CODER (described previously) and \nSapBERT in our workflows.\nReimers et¬†al. [30] proposed multilingual knowledge \ndistillation that seeks to reinforce better alignment of \nthe multilingual sentence embeddings, i.e., the sentence \nembeddings of different languages shall be mapped to \nthe same vector space if they are semantically equivalent. \nThrough the distillation, the student model ÃÇM , generally (but \nnot restricted to) a smaller multilingual pretrained model, \nlearns the behavior of the teacher model M , generally an \nintensively trained monolingual (English) model. The \npretraining requires a set of parallel (translated) sentences \n‚Äñf(xa)‚àíf (xp)‚Äñ2 < ‚Äñf(xa)‚àí f(xn)‚Äñ2 + ùúÜ\nSN Computer Science (2022) 3:387 \nPage 5 of 20 387\nSN Computer Science\n((s1 ,t1 ),‚Ä¶ ,(sn ,tn )) where ti is the translation of si . The \nlearning objective is to minimize the mean squared loss so \nthat ÃÇM (si)‚âàM (si) and ÃÇM (ti)‚âàM (si).\nThe multilingual version of SapBERT, later referred to as \nSapBERT-XLMR, differs from the English version in two \nfolds. Firstly, it is trained with UMLS terms of all languages. \nSecondly, the pretraining also incorporates general-domain \ntranslation data, including ‚Äúmuse‚Äù word translations [31] \nand parallel Wikipedia article titles. The original and the \ntranslated sequences are considered as synonyms for the \nSAP training process.\nMethods\nCorpus and¬†Ontology\nThis study uses the same 21 German medical forms and \nthe 497 questions as in [1 ]. Many of the forms are utilized \nin the LIFE 6 Adult Study [32], a large scale cohort study \ninvestigating the factors leading to civilization diseases, such \nas vascular disease, heart function, allergies and depression. \nExamples of the included medical forms are the Patient \nHealth Questionnaire (PHQ, [33]) and the GAD-7.\nThe UMLS Metathesaurus is one of the largest biomedical \nontology sources by far. We consequently choose UMLS so \nthat we can maximize the semantic interoperability for our \ncorpus. Since some of the pretrained models that are applied \nin this study (namely CODER and SapBERT) use the UMLS \nversion 2020AA for concept injection, we also limited \nourselves to the same version for our annotation task for a \nfair comparison. The UMLS version 2020AA integrates 214 \nsource vocabularies and contains approximately 4.28 million \nconcepts. To improve annotation efficiency and since not \nall ontologies in the UMLS are relevant, we selected three \nsource ontologies from the UMLS that still cover 99.1% of \nthe GSC annotations [ 1]. The selected subset contains all \nconcepts from (1) the NCI Thesaurus, (2) the LOINC, and \n(3) the Consumer Health Vocabulary. In total, the subset \nincludes 1,115,090 terms belonging to 399,758 concepts.\nIn order to evaluate the annotation quality, we manually \nannotated the medical forms using the selected UMLS \nsubset and built a Gold Standard Corpus (GSC) [1]. Overall, \nwe identified 1105 GSC annotations. Their frequency \ndistribution of number of annotations per question is shown \nin Fig.¬† 2. In the GSC, most of the questions have up to 2 \nannotations and about 10% of the questions have 3 or 4 \nannotations. There are only a few questions being mapped \nto more than 5 UMLS concepts. From our observations, the \nduplication is mainly due to (1) same question of a form \nmight be given multiple CUIs in the UMLS or (2) the same \nquestion occurs in different forms and hence has different \nCUIs. Figure¬†1 shows such examples.\nAnnotation Workflows\nWe design two workflows: (1) Workflow-Multi and (2) Work-\nflow-MT to tackle the cross-lingual concept normalization \nproblem (Fig.¬† 3). In Workflow-Multi we input the German \nforms directly into a given multilingual sentence encoder to \ngenerate sentence embeddings. We use the same encoder \nto encode the embeddings for the English concepts in the \nUMLS (Fig.¬†3a). In Workflow-MT (MT stands for Machine \nTranslation), we first translate the German forms into Eng-\nlish using three machine translators (DeepL, 7 Microsoft \nTranslator8 and Google Translate9) (Fig.¬†3b). We then gener-\nate the embeddings of the translated questions and the Eng-\nlish UMLS concepts using a given sentence encoder. The \nsentence encoders we used in Workflow-MT are not limited \nto English encoders but also include multilingual ones. In \na preliminary study we observed that multilingual encoders \nwe selected to generate English sentence embeddings can \nalso achieve good annotation quality. After the encoding \nprocess, cosine similarity is computed between each pair of \nquestion and a candidate concept embeddings. These map-\npings are ranked and the Top k results are retained for evalu-\nation, where k ‚àà{ 1, 2, 3, 5} . We apply the metrics precision, \nrecall and F-measure to evaluate our results. We also use \nWorkflow-MT to annotate the original English corpus for the \nreference comparison.\nThere are four optional components in the Workflow-\nMT, which are presented in dashed lines in Fig.¬† 3. First, \nthe UMLS injection indicates that we train the sentence \nencoders using concepts in the UMLS to refine the sentence \n121\n293\n28 27 8 6 5 1 3 2 1 20\n100\n200\n300\n12345678 91 11 21 3\nNumber of GSC annotations of a question\nFrequency\nFig. 2  Frequency distribution of number of annotations of a question \nin the GSC (adapted from [1])\n6 LIFE stands for Leipzig Research Center for Civilization Diseases \nhttps:// life. uni- leipz ig. de/ en/ life_ health_ study. html.\n7 https:// www. deepl. com/ trans lator.\n8 https:// www. micro soft. com/ en- us/ trans lator/.\n9 https:// trans late. google. com.\n SN Computer Science (2022) 3:387\n387 Page 6 of 20\nSN Computer Science\nencoders. The methods and encoders used for training are \ndetailed in \"UMLS Injected SBERTv2 Models  (MGSapFull \nand  MGSapSubset)\". Second, we incorporate the SBERT-WK \n[19] as we observed that applying SBERT-WK to the Eng-\nlish embeddings generated by SBERT models does improve \nthe annotation quality significantly [1 ]. The third and forth \noptional components in the Workflow-MT are extra post-\nprocessing steps. The Cross-Encoder is used to rerank \nthe candidates. In the combination step, set operations are \napplied to the result sets generated by different translated \ncorpora. See \"Post-processing\" for more details about these \npost-processing methods.\nBaseline: AnnoMap\nAnnoMap [34, 35] is a conventional string matcher that \ngenerates candidates using three string similarity functions: \nTF/IDF, Trigram and LCS (longest common substring). \nAfter candidate generation, an optional group-based selec-\ntion can be applied to improve precision. AnnoMap retains \ncandidates whose similarity scores are above a given thresh-\nold /u1D6FF . We set two thresholds /u1D6FF‚àà{ 0.6, 0.7} that generally \ngenerate the best F-measures. We also retain the same result \nsizes k as in workflows using language pretrained models, i.e., \nk ‚àà{ 1, 2, 3, 5} . To be able to obtain the desired result set sizes, \nwe did not apply group-based selection in this study because it \nmight return fewer candidates than a given k. We annotate the \nsame three translated corpora as in the Workflow-MT and also \nthe original English corpus as reference.\nModel‚ÄëGroups\nIn total, we applied 53 English and multilingual BERT-\nbased pretrained language models that are grouped into \nfive Model-Groups: MGSBERTv1 , MGBPLM , MGSBERTv2 , \nMGSapFull and MGSapSubset . First group, MGSBERTv1 , includes \nthe ten English SBERT models we used in our previous \nstudy [1 ] as reference. These models are selected from \nFig. 3  Two workflows to generate cross-lingual annotations using sentence encoders\nTable 1  Selected models from SentenceTransformers-v1 ( MGSBERTv1)\nMean pooling was applied in the SBERT training phase of all models\nModel name Base model Size Training\nBERTbase-N BERT Base NLI\nBERT base-NS BERT Base NLI + STSb\nBERTlarge -N BERT Large NLI\nBERT large -NS BERT Large NLI + STSb\nRoBERTabase-N RoBERTa Base NLI\nRoBERTabase-NS RoBERTa Base NLI + STSb\nRoBERTalarge -N RoBERTa Large NLI\nRoBERTalarge -NS RoBERTa Large NLI + STSb\nDistilBERT-N DistilBERT 6 layers NLI\nDistilBERT-NS DistilBERT 6 layers NLI + STSb\nSN Computer Science (2022) 3:387 \nPage 7 of 20 387\nSN Computer Science\nSentenceTransformers-v1 and are listed in Table¬†1. Be noted \nthat since we use different UMLS versions in both studies \n(2019AB in [1 ] and 2020AA in current work), the results \ndiffer. The description of other Model-Groups are detailed \nas follows.\nBiomedical Pretrained Language Models ( MGBPLM)\nWe select nine BERT-based BPLMs from BioBERT, Pub-\nMedBERT, SapBERT and CODER (Table¬† 2). Six of them \nare English encoders, which are trained using English text \nand the other three are multilingual encoders. PubMedBERT \ndiffers from BioBERT in that it is trained from scratch using \nPubMed corpus while BioBERT was also pretrained with \nEnglish Wikipedia and BookCorpus. There are two ver -\nsions of PubMedBERT, one version is pretrained only with \nabstracts of PubMed (named PubMedBERTabstract  ) whereas \nthe other one is also trained on full text of PubMed articles \n( PubMedBERTf ull ). Both SapBERT and CODER use Pub -\nMedBERT as base-model and are injected with the UMLS \n2020AA full version. Pretrained SapBERT models are \navailable in two versions: one with [CLS] representation \n( SapBERTCLS ) and one with mean-token ( SapBERTmean ). \nWe applied both the English and the multilingual encoders \nin MGBPLM for the Workflow-MT and eventually only the \nmultilingual encoders for Workflow-Multi.\nSBERTv2 Models ( MGSBERTv2)\nSince the publication of our previous study [1], various new \nmodels are trained into Sentence-BERT and are included in \nthe SentenceTransformers-v2. 10 Among them, we selected \nten English models and four multilingual models that \nhave shown to yield good results in NLP tasks and vary in \nefficiency (Table¬†3).\nEnglish encoders The selected ten English SBERTv2 \nmodels are mainly derived from MPNet, RoBERTa, \nDistilRoBERTa and MiniLM. They are fine-tuned on three \ndifferent training sets: NLI + STSb, paraphrase and ALL. \nThe NLI + STSb training set includes the NLI datasets 11 \nand the STSb dataset [38]. Additional corpora12 are used to \ntrain the models using the paraphrase training set. The ALL \ncorpus further expands the paraphrase training set into a \ndataset including one billion sentence pairs from various \nsources. 13 We also apply the optional SBERT-WK to the \nnew SBERTv2 English encoders.\nMultilingual encoders We choose three new multilin-\ngual models from the SentenceTransformers-v2, which are \nlisted as M2-M4 in Table¬† 3. We retain the best performing \nmodel in our previous study (M1) for comparison. Unlike \nthe models in MGBPLM , we apply only the English encoders \nin MGSBERTv2 for Workflow-MT as, according to our pre-\nliminary study, the SBERTv2 multilingual encoders do not \ngenerate good annotation results using Workflow-MT.\nTable 2  Selected English and multilingual BPLM models\nAll models consists of 12 transformer layers as BERTbase except SapBERT-XLMRlarge is based on BERTlarge\nPubMed: PubMed abstracts, PMC: PubMed Central full-text articles\nModel name Base model Representation Pretraining UMLS injection\n(a) English encoders\nBioBERT BERT (Wikipedia, \nBookCorpus)\n[CLS] PubMed ‚Äì\nPubMedBERTabstract BERT from scratch [CLS] PubMed ‚Äì\nPubMedBERTf ull BERT from scratch [CLS] PubMed, PMC ‚Äì\nSapBERTCLS PubMedBERTf ull [CLS] PubMed, PMC 2020AA English\nSapBERTmean PubMedBERTf ull Mean-token PubMed, PMC 2020AA English\nCODERENG PubMedBERTabstract [CLS] PubMed 2020AA English\n(b) Multilingual encoders\nSapBERT-XLMRbase XLMRbase [CLS] Multilingual CommonCrawl 2020AB multilingual\nSapBERT-XLMRlarge XLMRlarge [CLS] Multilingual CommonCrawl 2020AB multilingual\nCODERALL mBERT [CLS] Multilingual Wikipedia 2020AA multilingual\n10 https:// www. sbert. net/ index. html.\n11 Containing the Stanford Natural Language Inference dataset [36] \nand the Multi-Genre NLI dataset [37].\n12 Sentence-compression, SimpleWiki, altlex, msmarco-triplets, \nquora-duplicates, coco-captions, flickr30k-captions, yahoo-answers-\ntitle-question, S2ORC-citation-pairs, stackexchange-duplicate-ques-\ntions, wiki-atomic-edits.\n13 https:// huggi ngface. co/ sente nce- trans forme rs/ all- mpnet- base- v2.\n SN Computer Science (2022) 3:387\n387 Page 8 of 20\nSN Computer Science\nUMLS Injected SBERTv2 Models ( MGSapFull and¬†MGSapSubset)\nAmong the BPLM models, CODER and SapBERT are both \nwith UMLS injection. Based on our research results, Sap-\nBERT models perform significantly better than CODER \n(see Table¬† 8). Consequently, we use the method proposed \nin SapBERT [26]14 to inject UMLS 2020AA into the English \nmodels of MGSBERTv2 . For the injection, we use either the \nfull version of the UMLS ( MGSapFull ) or the selected subset \nof the UMLS ( MGSapSubset , subset selection see \"Corpus and \nOntology\"). Since these UMLS injected models are SBERT-\nbased, we are also able to apply SBERT-WK to them.\nIn the Workflow-MT, a configuration, denoted as config \nin the following text, is determined by a given model, with \nor without SBERT-WK, different translated corpora and the \nvarious result sizes. Table¬†4 shows the number of models of \neach Model-Group and the corresponding number of con-\nfigs used to generate annotation results.\nPost‚Äëprocessing\nCombination using set operations Our previous studies \n[1, 6, 39] show that combining annotation results using set \noperations can further improve annotation quality. We also \nconclude that combining result sets of the three different \ntranslated corpora deliver the best quality [1]. Hence, within \nTable 3  Selected models from SentenceTransformers-v2\nMean pooling was applied in the SBERT training phase of all models\n(a) English encoders\nModel name Base model Size Training\nMPNet-STSb MPNet Base NLI + STSb\nRoBERTa-STSb RoBERTa Base NLI + STSb\nDistilRoBERTa-STSb DistilRoBERTa 6 layers NLI + STSb\nMPNet-Paraphrase MPNet Base Paraphrase\nMiniLM(L12)-Paraphrase MiniLM Base Paraphrase\nMiniLM(L6)-Paraphrase MiniLM 6 layers Paraphrase\nMPNet-ALL MPNet Base All\nDistilRoBERTa-ALL DistilRoBERTa Base All\nMiniLM(L12)-ALL MiniLM Base All\nMiniLM(L6)-ALL MiniLM 6 layers All\n(b) Multilingual encoders\nModel code Teacher model Student model Languages\nM1 mUSE DistilmBERT 15 languages\nM2 mUSE DistilmBERT 50+ languages\nM3 MPNet-Paraphrase XLM-R 50+ languages\nM4 MiniLM(L12)-Paraphrase XLM-R 50+ languages\nTable 4  Number of models and configurations used in Workflow-MT \nModel-Group Models SBERT-WK Corpus Result size config\nMGSBERTv1 10 with or w/o\nGO\nDL\nMS\nk ‚àà\n{1, 2, 3, 5}\n192\nMGBPLM 9n .a. 108\nMGSBERTv2 10\nwith or w/o\n160\nMGSapFull 10 160\nMGSapSubset 10 160\nIn column SBERT-WK, ‚Äún.a.‚Äù indicates that SBERT-WK is not applicable to BPLM models\nGO: Google Translate, DL: DeepL, MS: Microsoft Translator\n14 https:// github. com/ cambr idgel tl/ sapbe rt.\nSN Computer Science (2022) 3:387 \nPage 9 of 20 387\nSN Computer Science\neach Model-Group, we combine the annotation candidates \ngenerated by the single configs of the three different \ntranslated corpora (Google Translate, DeepL and Microsoft \nTranslator) using intersection, union and 2-vote-agreement \n(an annotation is considered as correct by at least two of the \nthree configs) in this study.\nCross-Encoders For finding the most similar sentence \npair, if two sentences are passed into the encoder-net-\nwork simultaneously, such a network is named Cross-\nEncoder [ 40]. Thakur et¬†al. [41] show that a fine-tuned \nCross-Encoder (BERT) delivers better results for the \nSTS Benchmark than a fine-tuned Bi-Encoder (SBERT). \nHowever, since the sentences are passed to the network in \npairs, using Cross-Encoder for finding most similar sen-\ntences is computationally expensive as it demands quadratic \ntime complexity. To overcome this inefficiency and still tak-\ning the advantage of the better result quality of the Cross-\nEncoder, we apply the Cross-Encoder on a limited \ncandidate list. We first reduce the search space by generating \na short list of candidates using the standard cosine similarity \nranking. We then use the Cross-Encoder to rerank these \ncandidates and evaluate the Top k results accordingly. We \nutilize the implementation of Cross-Encoder in the Sen-\ntenceTransformers-v2.15 We select the Cross-Encoder  \nmodel stsb-roberta-large as it delivers the best results for \nthe STS benchmark. The Cross-Encoder returns a score \nfor each given sentence pair. Given an encoder config, a \nsentence pair comprises a question of the given translated \ncorpus (the reference sentence) and one of the candidates \nfound by that config. For each single config, we retain \nthe best 50 candidates for reranking. For each combination, \nwe first rerank the 50 candidates generated by each single \nconfig and then apply the set operations to combine the \nreranked candidates to obtain the final annotation results.\nEvaluation\nIn this section, we first present the annotation quality of \nAnnoMap, the conventional string matching method. We then \nreport the results of the proposed workflows: Workflow-Multi \nand Workflow-MT. In addition to annotation quality, we also \ninvestigate the computation efficiency of the models used in \nWorkflow-MT and the combination results. Further, we give a \nreflection on the relationship between recall and result size. At \nthe end of the section, we summarize our main findings.\nBaseline: AnnoMap\nThe best results of the conventional string matching method, \nAnnoMap, are shown in Table¬†5. When using the original Eng-\nlish corpus, AnnoMap obtain the best precision of 93.6%, best \nrecall of 86% and best F-measure of 79.49%. However, when \nannotating translated corpora, the AnnoMap performs far less \nwell: 32% reduction in precision, 50.4% in recall and 41.13% in \nF-measure. The large drop in recall indicates that the paraphrase \nof the questions after translation/cultural adaptation prevents the \nconventional string matching method from finding the correct \nannotations, especially in recall. The results of AnnoMap also \nshow that the most suitable machine translator is Google Trans-\nlate while the annotation quality using DeepL and Microsoft \nTranslator are worse.\nTable 5  The best precision, recall and F-measure obtained from \nAnnoMap with thresholds /u1D6FF‚àà{ 0.6, 0.7} and of result sizes \nk ‚àà{ 1, 2, 3, 5}\nCorpus Œ¥ Result size Precision Recall F-measure\nPrecision\nGO 0.7 Top1 61.60 13.942 2.73\nOE 0.7 Top1 93.60 41.00 57.02\nRecall\nGO 0.6 Top5 35.50 36.113 5.80\nOE 0.6 Top5 50.91 86.15 64.00\nF-measure\nGO 0.6 Top2 53.04 30.053 8.36\nOE 0.6 Top2 86.32 73.67 79.49\nThe last row of each metric in grey is the best corresponding result \nobtained using the original English corpus (OE)\nGO: Google Translate\nTable 6  Averaged annotation quality using Workflow-Multi \nFor the model names refer to Table¬† 2 and Table¬†3. Models are ranked \nby F-measure within each Model-Group\nModel name MPrecision MRecall MF-measure\nMGBPLM\n¬†¬†¬†SapBERT-XLMRlarge 48.22 50.59 45.85\n¬†¬†¬†SapBERT-XLMRbase 40.48 42.15 38.34\n¬†¬†¬†CODERALL 14.48 16.13 14.13\nMGSBERTv2\n¬†¬†¬†M3 44.18 46.76 42.11\n¬†¬†¬†M1 43.88 45.95 41.63\n¬†¬†¬†M2 41.40 43.46 39.32\n¬†¬†¬†M4 37.55 39.30 35.62\n15 https:// www. sbert. net/ examp les/ appli catio ns/ cross- encod er/ \nREADME. html.\n SN Computer Science (2022) 3:387\n387 Page 10 of 20\nSN Computer Science\nAnnotation Using Pretrained Language Models\nWorkflow‚ÄëMulti\nThe Workflow-Multi has the advantage of not requiring \nmachine translators but uses the German questions as \ninput. We applied the three multilingual encoders in the \nMGBPLM (Table¬† 2) and the four multilingual encoders in \nthe MGSBERTv2 (Table¬†3) for this workflow. Table¬†6 presents \nthe averaged annotation quality of these multilingual \nmodels. Among the MGBPLM models, SapBERT models \nperform significantly better than the CODERALL . Actually, \nCODERALL is the worst performing multilingual model \namong all. Among the MGSBERTv2 models, M3 performs \nbest and is better than the best model we tested in our \nprevious study [1 ]. The best multilingual encoder is the \nSapBERT-XLMRlarge . It gains approximately 8% more than \nits base model (  SapBERT-XLMRbase ) in every averaged \nmetric and also outperforms the best MGSBERTv2 multilingual \nmodel (M3). When comparing results of single configs, the \nsuperiority of the model over other multilingual encoders \ncan be seen again (Table¬† 7). Using Workflow-Multi , \nSapBERT-XLMRlarge generates 60.16% as the best precision, \n68.69% as best recall and 52.33% as best F-measure.\nWorkflow‚ÄëMT\nThe following presents the results of Workflow-MT. We \ntest if integrating machine translators into cross-lingual \nbiomedical concept normalization workflow improves \nthe annotation quality. For this workflow, we also use \nmultilingual encoders in MGBPLM to encode English corpora \n(original English and the three translated English corpora).\nBPLM models Table¬†8 presents the performance of the \nnine models in the MGBPLM , including 6 English encod-\ners and 3 multilingual encoders. Interestingly, the best two \nmodels are the multilingual models ( SapBERT-XLMRlarge \nand SapBERT-XLMRbase ). The models with UMLS injec-\ntion (SapBERT and CODER models) outperform the \nmodels without UMLS injection (PubMedBERT and \nBioBERT) remarkably. All SapBERT models exceed \nCODER models. Notably, the two multilingual SapBERT \nmodels ( SapBERT-XLMRlarge and SapBERT-XLMRbase ) \nachieve better results than the two English SapBERT models \n( SapBERTmean and SapBERTCLS ). Similarly, the multilin-\ngual CODER (  CODERALL ) is also better than the English \nCODER ( CODERENG).\nWhen comparing the averaged results of the same \nmultilingual models but using different workflows \n(Table¬† 6 and Table¬† 8), we observe that the multilingual \nmodels deliver better results using Workflow-MT than \nWorkflow-Multi. The SapBERT-XLMRlarge improves \n3.37% in F-measure using Workflow-MT (from 45.85 to \n49.22%) and the SapBERT-XLMRbase gains an even larger \nincrease of 8.38% in F-measure (from 38.34 to 46.72%). \nThe CODERALL performs dramatically different on using \ndifferent workflows, in Workflow-Multi it only reaches an \naveraged F-measure of 14.13% while using Workflow-MT it \nachieves an averaged F-measure of 38.10%.\nUMLS injection of SBERTv2 models Table¬†9 presents the \naveraged annotation quality of models without UMLS injec-\ntion ( MGSBERTv2 ), those injected with 2020AA UMLS full \nversion ( MGSapFull ) and those injected with the selected subset \n( MGSapSubset ). UMLS injection is beneficial for 8 of the 10 mod-\nels (except MiniLM(L12)-ALL and MiniLM(L6)-ALL). We \nalso observe that UMLS injection improves different models in \nTable 7  Best performing \nconfig for each metric using \nWorkflow-Multi in MGBPLM and \nMGSBERTv2\nMetric Model name Result size Precision Recall F-measure\nMGBPLM\n¬†¬†¬†Precision SapBERT-XLMRlarge 1 60.16 27.06 37.33\n¬†¬†¬†Recall SapBERT-XLMRlarge 5 31.74 68.69 43.42\n¬†¬†¬†F-measure SapBERT-XLMRlarge 2 56.44 48.78 52.33\nMGSBERTv2\n¬†¬†¬†Precision M1 1 56.74 25.52 35.21\n¬†¬†¬†Recall M3 5 28.98 63.62 39.82\n¬†¬†¬†F-measure M1 2 51.69 45.70 48.51\nTable 8  Averaged annotation quality of BPLM models\nFor the model details refer to Table¬† 2. Models are ranked by \nF-measure\nModel name MPrecision MRecall MF-measure\nSapBERT-XLMRlarge 51.94 54.22 49.22\nSapBERT-XLMRbase 49.28 51.48 46.74\nSapBERTmean 43.80 45.85 41.55\nSapBERTCLS 43.16 45.17 40.98\nCODERALL 39.92 42.35 38.10\nCODERENG 39.10 41.85 37.49\nPubMedBERTabstract 25.51 27.66 24.59\nPubMedBERTf ull 20.22 21.63 19.36\nBioBERT 15.99 17.34 15.43\nSN Computer Science (2022) 3:387 \nPage 11 of 20 387\nSN Computer Science\nvarious magnitudes. The most significant improvement is seen \nby DistilRoBERTa-ALL. Before UMLS injection, RoBERTa-\nSTSb delivers the best averaged annotation quality (colored in \nblue). After UMLS injection (with either full version or subset), \nDistilRoBERTa-ALL becomes the best model. The second best \nmodel, MPNet-ALL, can also outperform RoBERTa-STSb after \nUMLS injection.\nWe conduct pairwise t-test to compare the annotation \nmetrics of the same model between different Model-Groups \nto test the effect of UMLS injection statistically. Each com-\nparison is done between the identical configs of the same \nmodel between two Model-Groups: MGSBERTv2 against \nMGSapFull , MGSBERTv2 against MGSapSubset and MGSapFull \nagainst MGSapSubset . The results are shown as superscripts \nin Table¬†9. Only one model, MiniLM(L6)-ALL, performs \nbetter without UMLS injection statistically (p -value < \n0.01). Among the eight models that benefit from the UMLS \ninjection, five (of MGSapFull ) and six (of MGSapSubset ) of \nTable 9  Averaged annotation \nquality of MGSBERTv2 , MGSapFull \nand MGSapSubset models\nModeln ame MPr ecision MRecall MF‚àímeasur e\nMGSBERTv2\nMiniLM(L12)-ALL 49.74 51.72 47.08\nMiniLM(L6)-ALL 48.88‚àó 50.90‚àó 46.30‚àó\nDistilRoBERTa-ALL 48.77 50.88 46.21\nMPNet-ALL 49.47 51.64 46.90\nMiniLM(L12)-Paraphrase 45.63 47.97 43.37\nMiniLM(L6)-Paraphrase 44.20 46.59 42.09\nMPNet-Paraphrase 50.26 52.10+ 47.50\nDistilRoBERTa-STSb4 8.52 51.14 46.15\nMPNet-STSb5 0.48 52.62 47.80‚àó\nRoBERTa-STSb 50.58 53.22 48.08\nMGSapFull\nMiniLM(L12)-ALL 47.80 49.71 45.24\nMiniLM(L6)-ALL 45.48 47.48 43.12\nDistilRoBERTa-ALL 52.65‚àó‚àó 54.34‚àó‚àó 49.65‚àó‚àó\nMPNet-ALL 52.28‚àó‚àó 54.09‚àó‚àó 49.40‚àó‚àó\nMiniLM(L12)-Paraphrase 48.47‚àó‚àó 50.44‚àó‚àó 45.87‚àó‚àó\nMiniLM(L6)-Paraphrase 44.39 46.46 42.10\nMPNet-Paraphrase 50.78 52.48 47.95\nDistilRoBERTa-STSb5 0.81‚àó‚àó 53.19‚àó‚àó 48.19‚àó‚àó\nMPNet-STSb5 0.58 52.27 47.76\nRoBERTa-STSb5 2.00‚àó‚àó 54.21‚àó‚àó 49.24‚àó‚àó\nMGSapSubset\nMiniLM(L12)-ALL 47.79 49.72 45.24\nMiniLM(L6)-ALL 45.43 47.43 43.08\nDistilRoBERTa-ALL 52.58‚àó‚àó 54.34‚àó‚àó 49.61‚àó‚àó\nMPNet-ALL 52.24‚àó‚àó 54.04‚àó‚àó 49.36‚àó‚àó\nMiniLM(L12)-Paraphrase 48.63‚àó‚àó 50.62‚àó‚àó 46.03‚àó‚àó\nMiniLM(L6)-Paraphrase 45.59‚àó‚àó 47.82‚àó‚àó 43.32‚àó‚àó\nMPNet-Paraphrase 50.19 51.49 47.25\nDistilRoBERTa-STSb4 9.98‚àó‚àó 52.32‚àó‚àó 47.39‚àó‚àó\nMPNet-STSb5 0.80 52.41 47.92\nRoBERTa-STSb5 1.95‚àó‚àó 54.11‚àó‚àó 49.17‚àó‚àó\nFor the model details refer to Table¬† 3. The best models of each Model-Group are \nin blue. The * indicates the models of MGSBERTv2 ¬† are significantly better than \nmodels in both MGSapFull and MGSapSubset using pairwise t-tests (p-value < 0.01). \nContrastingly, ** shows the models of MGSapFull and MGSapSubset are statistically \nbetter than those of MGSBERTv2 . + specifies the models of MGSBERTv2 are better \nthan those of MGSapSubset . The better models are in bold when comparing the \nmetrics between MGSapFull and MGSapSubset\n SN Computer Science (2022) 3:387\n387 Page 12 of 20\nSN Computer Science\nthem perform significantly better than uninjected models \n(denoted with ** in the table). When comparing the results \nbetween injection using full version ( MGSapFull ) or selected \nsubset ( MGSapSubset ), the differences are mostly insignifi-\ncant: merely two models are better using selected subset \n(MiniLM(L12)-Paraphrase and MiniLM(L6)-Paraphrase, \nin bold), while one model (MPNet-Paraphrase) is better \nusing full version. Hence, we can conclude that UMLS \ninjection into SBERTv2 models is generally beneficial for \nour biomedical concept normalization task, though various \nTable 10  Best single configs  in precision within each Model-Group using only cosine similarity (Standard) for ranking or additionally \nreranked with Cross-Encoder \nModel SBERT-\nWK\nCorpus Result\nsize\nStandard Cross-Encoder\nPR FP RF\nMGSBERTv1\nDistilBERT-NS with GO Top1 66.80 30.05 41.45 70.62 31.76 43.82\nBERTlarge-NS w/oG OT op16 6.60 29.95 41.32 70.62 31.76 43.82\nRoBERTabase-N with GO Top1 65.792 9.59 40.82 70.02 31.49 43.45\nBERTbase-N with GO Top1 65.59 29.50 40.70 70.02 31.49 43.45\nRoBERTalarge-NS w/o GO Top1 65.39 29.41 40.57 69.82 31.404 3.32\nBERTlarge-NS w/o OE Top1 94.77 42.62 58.80 92.35 41.54 57.30\nMGBPLM\nSapBERT-XLMRlarge n.a. GO Top1 67.61 30.41 41.956 9.22 31.13 42.95\nSapBERT-XLMRlarge n.a. DL Top1 66.40 29.86 41.20 70.023 1.49 43.45\nSapBERT-XLMRlarge n.a. MS Top1 63.98 28.78 39.706 9.01 31.04 42.82\nSapBERT-XLMRbase n.a. GO Top1 63.98 28.78 39.706 5.19 29.32 40.45\nSapBERT-XLMRlarge n.a. GO Top2 62.75 54.57 58.376 3.38 57.01 60.03\nSapBERT-XLMRbase n.a. OE Top1 94.57 42.53 58.68 91.95 41.36 57.05\nMGSBERTv2\nRoBERTa-STSb with GO Top1 67.81 30.50 42.077 0.22 31.58 43.57\nMPNet-Paraphrase withD LT op16 6.60 29.95 41.32 70.623 1.76 43.82\nMPNet-STSb with DL Top1 66.40 29.86 41.207 0.22 31.58 43.57\nMiniLM(L12)-ALL with DL Top1 66.40 29.864 1.20 71.23 32.04 44.19\nMPNet-STSb with GO Top1 66.00 29.68 40.95 69.42 31.22 43.07\nMiniLM(L12)-ALL with OE Top1 95.17 42.81 59.05 92.76 41.72 57.55\nMGSapFull\nDistilRoBERTa-ALL with MS Top1 70.42 31.67 43.706 7.00 30.14 41.57\nDistilRoBERTa-ALL with GO Top1 70.22 31.58 43.57 69.22 31.13 42.95\nDistilRoBERTa-ALL with DL Top1 68.81 30.95 42.70 69.82 31.40 43.32\nRoBERTa-STSb with GO Top1 68.61 30.86 42.577 0.02 31.49 43.45\nMPNet-ALL with GO Top1 68.21 30.684 2.32 69.22 31.134 2.95\nMiniLM(L6)-Paraphrase w/o OE Top1 95.98 43.17 59.55 93.16 41.90 57.80\nMGSapSubset\nDistilRoBERTa-ALL with GO Top1 71.23 32.04 44.19 69.62 31.31 43.20\nRoBERTa-STSb with GO Top1 69.22 31.13 42.957 0.42 31.67 43.70\nDistilRoBERTa-ALL with MS Top1 69.22 31.13 42.956 6.00 29.68 40.95\nDistilRoBERTa-ALL with DL Top1 69.01 31.04 42.82 69.82 31.40 43.32\nMPNet-ALL with GO Top1 68.01 30.594 2.20 69.01 31.044 2.82\nDistilRoBERTa-ALL w/o OE Top1 95.98 43.17 59.55 91.95 41.36 57.05\nIn column SBERT-WK, ‚Äún.a.‚Äù indicates that SBERT-WK is not applicable to BPLM models. The last row of each metric in gray is the best \ncorresponding result obtained using the original English corpus (OE). The best precision of all configs of translated corpora and OE are in bold\nGO: Google Translate, DL: DeepL, MS: Microsoft Translator, P: precision, R: recall, F: F-measure\nSN Computer Science (2022) 3:387 \nPage 13 of 20 387\nSN Computer Science\neffectiveness is observed. Moreover, injection using a rel-\nevant subset is sufficient and also more efficient than inject-\ning the full version of the UMLS.\nBest configs of Workflow-MT The best performing sin-\ngle configs in precision, recall and F-measure are shown in \nTables¬†10, 11, 12, respectively. We present the best 5 results \nwithin each Model-Group. The last row in gray of each model \nTable 11  Best single configs in recall within each Model-Group using only cosine similarity (Standard) for ranking or additionally reranked \nwith Cross-Encoder \nModel SBERT-\nWK\nCorpus Result\nsize\nStandard Cross-Encoder\nPR FP RF\nMGSBERTv1\nRoBERTabase-NS with GO Top5 31.73 70.41 43.74 32.64 73.39 45.18\nRoBERTalarge-NS w/oG OT op5 31.86 70.32 43.85 31.67 71.22 43.84\nRoBERTabase-N with GO Top5 31.35 69.59 43.23 31.79 71.49 44.01\nBERTbase-N with GO Top5 31.15 69.14 42.95 32.35 72.76 44.79\nRoBERTabase-NS with DL Top5 30.91 68.51 42.60 32.39 72.85 44.85\nBERTbase-N with OE Top5 41.27 91.95 56.97 40.24 90.50 55.71\nMGBPLM\nSapBERT-XLMRlarge n.a. GO Top5 33.66 73.67 46.21 32.43 72.94 44.90\nSapBERT-XLMRlarge n.a. DL Top5 33.17 72.49 45.51 32.56 73.21 45.07\nSapBERT-XLMRlarge n.a. MS Top5 32.38 70.86 44.45 31.15 70.05 43.12\nSapBERT-XLMRbase n.a. GO Top5 31.91 69.59 43.76 32.27 72.58 44.68\nSapBERT-XLMRbase n.a. MS Top5 31.10 68.05 42.69 31.10 68.05 42.69\nSapBERT-XLMRlarge n.a. OE Top5 41.89 91.58 57.48 39.96 89.86 55.32\nMGSBERTv2\nRoBERTa-STSb with GO Top5 32.98 73.30 45.49 32.39 72.85 44.85\nRoBERTa-STSb with DL Top5 32.03 70.95 44.13 32.39 72.85 44.85\nMPNet-STSb with GO Top5 32.04 70.86 44.13 31.95 71.86 44.23\nDistilRoBERTa-STSb w/o GO Top5 31.70 70.32 43.70 32.56 73.21 45.07\nMPNet-Paraphrasew /o GO Top5 32.24 70.23 44.19 32.60 73.30 45.13\nMPNet-STSb with OE Top5 41.23 91.49 56.85 40.00 89.95 55.38\nMGSapFull\nRoBERTa-STSb with GO Top5 33.09 73.57 45.65 33.16 74.57 45.91\nMPNet-Paraphrasew /o GO Top5 33.71 73.03 46.13 32.72 73.57 45.29\nMPNet-STSb w/oG OT op5 33.72 73.03 46.14 32.80 73.76 45.40\nDistilRoBERTa-ALL with GO Top5 33.02 72.85 45.44 32.52 73.12 45.01\nMPNet-ALL w/oD LT op5 33.40 72.22 45.68 32.23 72.49 44.62\nRoBERTa-STSb w/o OE Top5 42.18 92.04 57.85 40.08 90.14 55.49\nMGSapSubset\nRoBERTa-STSb with GO Top5 33.05 73.48 45.59 33.28 74.84 46.07\nDistilRoBERTa-ALL with GO Top5 33.14 73.12 45.61 32.56 73.21 45.07\nMPNet-ALL w/oG OT op5 33.66 72.49 45.97 32.72 73.57 45.29\nMPNet-ALL with GO Top5 33.42 72.04 45.66 32.27 72.58 44.68\nMPNet-ALL w/oD LT op5 33.28 71.95 45.51 32.15 72.31 44.51\nDistilRoBERTa-STSb w/o OE Top5 41.71 91.76 57.35 40.04 90.05 55.43\nIn column SBERT-WK, ‚Äún.a.‚Äù indicates that SBERT-WK is not applicable to BPLM models. The last row of each metric in grey is the best \ncorresponding result obtained using the original English corpus (OE). The best recall of all configs of translated corpora and OE are in bold\nGO: Google Translate, DL: DeepL, MS: Microsoft Translator, P: precision, R: recall, F: F-measure\n SN Computer Science (2022) 3:387\n387 Page 14 of 20\nSN Computer Science\ngroup is the best results using original English corpus as an \nindication of upper bound. The first three metric columns are \nthe results using standard workflow, i.e., the candidates are \nranked using the cosine similarities of the mappings. The last \nthree metric columns show the results that are reranked using \nCross-Encoder. Overall, we can exceed our previous \nresults in [1] (comparable results shown as MGSBERTv1 models \nin Tables¬†10, 11, 12) in all metrics. The best annotation quality \ngenerated by Workflow-MT on annotating original English corpus \nexceeds that of conventional string matching. We can push the \nTable 12  Best single configs  in F-measure within each Model-Group using only cosine similarity (Standard) for ranking or additionally \nreranked with Cross-Encoder \nModel SBERT-\nWK\nCorpus Result\nsize\nStandard Cross-Encoder\nPR FP RF\nMGSBERTv1\nRoBERTabase-N with GO Top2 61.51 53.94 57.47 63.48 57.10 60.12\nDistilBERT-NS with GO Top2 61.09 53.85 57.24 63.38 57.01 60.03\nRoBERTalarge-NS w/o GO Top2 60.74 53.48 56.88 62.98 56.65 59.65\nRoBERTabase-NS with GO Top2 60.78 53.30 56.80 63.58 57.19 60.22\nBERTbase-N with GO Top2 60.49 53.485 6.77 62.98 56.65 59.65\nRoBERTabase-NS with OE Top2 86.36 75.66 80.66 81.59 73.39 77.27\nMGBPLM\nSapBERT-XLMRlarge n.a. GO Top2 62.75 54.57 58.376 3.385 7.01 60.03\nSapBERT-XLMRlarge n.a. DL Top2 61.86 53.57 57.42 63.18 56.835 9.84\nSapBERT-XLMRlarge n.a. MS Top2 59.92 51.95 55.65 59.96 53.94 56.79\nSapBERT-XLMRbase n.a. GO Top2 59.42 51.67 55.286 2.685 6.38 59.36\nSapBERT-XLMRlarge n.a. GO Top3 48.02 62.62 54.364 7.486 4.07 54.55\nSapBERT-XLMRbase n.a. OE Top2 87.29 75.84 81.16 81.79 73.57 77.47\nMGSBERTv2\nRoBERTa-STSb with GO Top2 62.54 55.29 58.696 3.78 57.38 60.41\nMPNet-STSb with DL Top2 61.65 54.125 7.646 3.58 57.19 60.22\nDistilRoBERTa-STSb w/o GO Top2 61.48 54.03 57.51 63.38 57.01 60.03\nRoBERTa-STSb with DL Top2 61.13 53.94 57.31 63.58 57.196 0.22\nMiniLM(L12)-ALL with DL Top2 61.215 3.855 7.29 63.48 57.106 0.12\nMiniLM(L6)-ALL w/o OE Top2 86.95 75.38 80.76 81.59 73.39 77.27\nMGSapFull\nDistilRoBERTa-ALL with GO Top2 65.94 57.47 61.41 63.18 56.83 59.84\nDistilRoBERTa-ALL with MS Top2 65.63 57.19 61.126 1.07 54.93 57.84\nRoBERTa-STSb with GO Top2 64.33 56.47 60.146 4.19 57.74 60.79\nDistilRoBERTa-ALL with DL Top2 63.73 55.66 59.42 63.48 57.10 60.12\nMPNet-STSb w/o GO Top2 63.73 55.02 59.06 63.38 57.01 60.03\nMiniLM(L6)-Paraphrase w/o OE Top2 87.83 75.75 81.34 81.49 73.30 77.18\nMGSapSubset\nDistilRoBERTa-ALL with GO Top2 66.46 57.92 61.90 63.78 57.38 60.41\nDistilRoBERTa-ALL with MS Top2 64.80 56.47 60.356 0.36 54.30 57.17\nRoBERTa-STSb with GO Top2 64.43 56.56 60.246 4.39 57.92 60.98\nDistilRoBERTa-ALL with DL Top2 63.21 55.20 58.94 63.18 56.83 59.84\nMPNet-ALLw /o DL Top2 63.56 54.93 58.93 63.18 56.83 59.84\nMiniLM(L6)-Paraphrase w/o OE Top2 87.55 75.75 81.22 81.29 73.12 76.99\nIn column SBERT-WK, ‚Äún.a.‚Äù indicates that SBERT-WK is not applicable to BPLM models. The last row of each metric in grey is the best corresponding \nresult obtained using the original English corpus (OE). The best F-measure of all configs of translated corpora and OE are in bold\nGO: Google Translate, DL: DeepL, MS: Microsoft Translator, P: precision, R: recall, F: F-measure\nSN Computer Science (2022) 3:387 \nPage 15 of 20 387\nSN Computer Science\nupper limit for a further 6.04% in recall (92.04 vs 86%) and 2% \nin the best precision and F-measure (precision: 95.98 vs. 93.6%, \nF-measure: 81.34 vs 79.49%).\nWe achieve the best precision of 71.23% with both standard \nranking and reranking with Cross-Encoder (Table¬† 10). \nReranking using Cross-Encoder can improve the pre -\ncision results for almost all the 25 configs (only 4 cases in \nMGSapFull and MGSapSubset are exceptions). The best recall \nof standard ranking is 73.67% by the best BPLM model \n( SapBERT-XLMRlarge with Google Translate corpus, \nTable 13  Computing time per \nembedding of models in each \nModel-Group\nModel w/o SBERT-WK (ms) withS BERT-WK (ms)\nMGSBERTv1\nBERTbase 0.994 6.47\nBERTlarge 1.48n .a.\nRoBERTabase 0.793 5.92\nRoBERTalarge 1.37n .a.\nDistilBERT 0.551 2.78\nMGBPLM\nBioBERT 19.54 n.a.\nPubMedBERT 19.43 n.a.\nSapBERT 19.64n .a.\nCODER 19.79n .a.\nMGSBERTv2\nMPNet 0.51 34.17\nRoBERTa 0.51 33.24\nDistilRoBERTa 0.30 22.59\nMiniLM(L12) 0.463 3.78\nMiniLM(L6) 0.272 2.55\n‚Äún.a.‚Äù indicates that SBERT-WK is not applied to the large models and the BPLM \nmodels\nTable 14  The best combination results in precision by combining three different corpora within each Model-Group using intersection\nModel SBERT-\nWK\nCorpus Result\nsize\nStandard Cross-Encoder\nPR FP RF\nMGSBERTv1\nDistilBERT-Nw ithM S Top1\n(Top2) 93.46 9.05 16.508 2.80 44.43 57.83RoBERTabase-NS with GO\nRoBERTalarge-N w/o DL\nMGBPLM\nCODERALL n.a. MS Top1\n(Top2) 92.94 7.151 3.28 87.32 38.645 3.58SapBERT-XLMRlarge n.a. DL\nSapBERTmean n.a. GO\nMGSBERTv2\nMPNet-STSbw ith GO Top1\n(Top2) 93.00 8.421 5.448 6.30 39.915 4.58DistilRoBERTa-STSbw ith MS\nMiniLM(L12)-Paraphrase w/o DL\nMGSapFull\nMPNet-STSb w/o DL Top1\n(Top2) 90.621 0.50 18.828 5.35 30.59 45.04RoBERTa-STSbw ithG O\nMPNet-STSbw ith MS\nMGSapSubset\nRoBERTa-STSbw ithG O Top1\n(Top2) 91.26 8.511 5.568 4.18 29.864 4.09MiniLM(L6)-Paraphrasew ith DL\nDistilRoBERTa-STSbw ith MS\nIn column SBERT-WK, ‚Äún.a.‚Äù indicates that SBERT-WK is not applicable to BPLM models. The result size of Cross-Encoder results are in \nbrackets\nGO: Google Translate, DL: DeepL, MS: Microsoft Translator, P: precision, R: recall, F: F-measure\n SN Computer Science (2022) 3:387\n387 Page 16 of 20\nSN Computer Science\nTable¬†11). Using Cross-Encoder for reranking, we can \nimprove the best recall further to 74.84% with RoBERTa-STSb \nusing Google Translate and with SBERT-WK. Actually, this \nconfig delivers the best recalls within each of the SBERTv2 \nModel-Groups. Similar to the best precision results, reranking \nusing Cross-Encoder improves most best recall results \nexcept from three configs (two configs in MGBPLM and \none config in MGSBERTv2 ). The best F-measure, 61.90%, is \ndelivered by DistilRoBERTa-ALL using Google Translate with \nSBERT-WK in MGSapSubset (Table¬†12). On the other hand, the \nbest F-measure using Cross-Encoder does not exceed this \nresult. But in general, reranking using Cross-Encoder is \nalso beneficial for F-measure results. The F-measures of only \n4 configs (two in MGSapFull and two in MGSapSubset ) are not \nimproved by reranking.\nThe best performing model in MGBPLM is the \nSapBERT-XLMRlarge . It achieves the best 3 results in every \nmetric using the 3 different translated corpora in the order of \nGoogle Translate, DeepL and Microsoft Translator (Tables¬†10, \n11, 12). An interesting observation is, since the model is a mul-\ntilingual encoder, it is also applied in the Workflow-Multi. Com-\nparing its best results using Workflow-Multi (Table¬†7) and those \nusing Workflow-MT, using translated corpora can deliver even \nbetter results. When inputing the German forms directly into \nSapBERT-XLMRlarge , the best precision, recall and F-measure \nare 60.16, 68.69 and 52.33%. On the other hand, using Google \nTranslate translated corpus as input, it achieves 67.61% in preci-\nsion, 73.69% in recall and 58.37% in F-measure. This implies \nthat the alignment of multilingual sentences of the model is still \nnot as good as aligning solely the English sentences.\nNotably, DistilRoBERTa-ALL of MGSapSubset with the set-\nting of including SBERT-WK and using Google Translate \ncorpus delivers the best precision and F-measure of all single \nconfigs. Without UMLS injection ( MGSBERTv2 ), RoBERTa-\nSTSb using Google Translate and with SBERT-WK deliv -\ners the best precision, recall and F-measure. However, after \nUMLS injection, DistilRoBERTa-ALL is able to outperform \nRoBERTa-STSb in best precision and F-measure. RoBERTa-\nSTSb delivers not only the best recalls among all SBERTv2 \nmodels, with the help of Cross-Encoder, it achieves the \nbest recall of all models compared in this study. All the best \nmetrics of the SBERT-based models (models in MGSBERTv1 , \nMGSBERTv2 , MGSapFull and MGSapSubset ) are produced by \nconfigs with SBERT-WK. This consents to our previous \nobservation that adding SBERT-WK does improve annota-\ntion quality [1]. However, we also observe that many of the \nTable 15  The best combination results in recall by combining three different corpora within each Model-Group using union\nModel SBERT-\nWK\nCorpus Result\nsize\nStandard Cross-Encoder\nPR FP RF\nMGSBERTv1\nRoBERTabase-NS with DL Top5\n(Top5) 19.94 84.16 32.24 26.01 84.16 39.74RoBERTalarge-NS w/oM S\nDistilBERT-NS with GO\nMGBPLM\nSapBERT-XLMRbase n.a. MS Top5\n(Top5) 20.43 83.98 32.86 23.75 82.26 36.86SapBERT-XLMRlarge n.a. DL\nSapBERTCLS n.a. GO\nMGSBERTv2\nMiniLM(L6)-ALL w/oD L Top5\n(Top5) 20.21 84.89 32.65 23.86 81.99 36.96RoBERTa-STSbw ithG O\nMPNet-STSbw /o MS\nMGSapFull\nMPNet-Paraphrase w/oG O Top5\n(Top5) 22.59 85.16 35.71 23.66 82.81 36.80RoBERTa-STSbw ithD L\nMPNet-ALL w/oM S\nMGSapSubset\nRoBERTa-STSbw ithG O Top5\n(Top5) 22.43 85.25 35.51 23.20 81.63 36.13MPNet-ALL w/oD L\nMPNet-STSbw /o MS\nIn column SBERT-WK, ‚Äún.a.‚Äù indicates that SBERT-WK is not applicable to BPLM models. The result size of Cross-Encoder results are in \nbrackets\nGO: Google Translate, DL: DeepL, MS: Microsoft Translator, P: precision, R: recall, F: F-measure\nSN Computer Science (2022) 3:387 \nPage 17 of 20 387\nSN Computer Science\nconfigs without SBERT-WK also perform well in recall and \nF-measure (Tables¬†11, 12). Furthermore, these best configs \nall achieve the best results using Google Translate corpora with \nonly one exception (best precision of MGSapFull ). Consent to \nthe AnnoMap results, the Google Translate is the most suit-\nable machine translator also in the pretrained language model \nworkflow.\nThe best precisions are delivered with configs of result size \nas Top1 and as expected, the best recalls of result size as Top5. \nThe best F-measures are generated by configs with Top2 as \nresult size. Only 2 of the 75 configs in Tables¬† 10 - 12 are \nexceptions. They are the config SapBERT-XLMRlarge using \nGoogle Translate ranked 5th in both the precision table (result \nsize = 2 instead of 1) and the F-measure table (result size = \n3 instead of 2). These exceptions are mainly due to that the \nSapBERT-XLMRlarge performs the best in the MGBPLM and \ntherefore, even with sub-optimal result size, it still outperforms \nother configs. The configs with Top2 result size generates \nthe best F-measure can be explained by that in our corpus most \nof the questions have 2 GSC annotations (as shown in Fig.¬†2). \nComputation efficiency We used two NVIDIA V100 \nTensor Cores as GPUs to encode the questions and the \nUMLS concepts. Table¬†13 presents the computation time of \nthe models in MGSBERTv1 , MGBPLM and MGSBERTv2 . Since \nthe UMLS injection using SAP does not change the model \nstructure, the encoding time of the models in MGSapFull and \nMGSapSubset remains the same as those in MGSBERTv2 . Simi-\nlarly, the same model but pretrained on different corpora \n(e.g., MPNet-STSb and MPNet-ALL) also have the same \ncomputation time and therefore are not shown separately in \nthe table. We can conclude that the newly selected SBERTv2 \nmodels not only outperform the SBERTv1 models in annota-\ntion quality, they are also more efficient. MiniLM(L6) and \nDistilRoBERTa are the fastest models. Applying SBERT-\nWK drastically increases the computation time because it \nrelies on CPU to operate. Owing to the fact that all BPLM \nmodels are direct derivatives of the initial BERT, their effi-\nciency are alike. They are approximately 10% faster than the \nfastest SBERTv2 models with SBERT-WK.\nCombination of results In our previous study [1 ] we \nshowed that combining using set operations on the result sets of \ndifferent translated corpora can improve annotation quality fur-\nther. Therefore, we applied the combinations and obtained the best \nprecisions by intersecting the result sets (Table¬†14), the best recalls \nby union (Table¬†15) and the best F-measures by 2-vote-agree-\nments (Table¬†16). In each table, we present the best combination \nTable 16  The best combination results in F-measure by combining three different corpora within each Model-Group using 2-vote-agreement\nModel SBERT-\nWK\nCorpus Result\nsize\nStandard Cross-Encoder\nPR FP RF\nMGSBERTv1\nBERTbase-N with MS Top2\n(Top2) 77.35 52.85 62.80 70.49 53.39 60.76RoBERTalarge-N w/oD L\nDistilBERT-NS with GO\nMGBPLM\nSapBERT-XLMRlarge n.a. GO Top2\n(Top2) 68.78 53.03 59.89 69.36 53.67 60.51SapBERT-XLMRlarge n.a. DL\nSapBERT-XLMRbase n.a. MS\nMGSBERTv2\nMPNet-Paraphrase with MS Top2\n(Top3) 74.97 54.75 63.28 68.20 56.47 61.78MPNet-STSbw ithD L\nRoBERTa-STSbw ithG O\nMGSapFull\nDistilRoBERTa-ALL with MS Top2\n(Top5) 74.97 54.75 63.28 58.67 57.56 58.11MiniLM(L12)-ALL with GO\nMPNet-ALL w/oD L\nMGSapSubset\nMiniLM(L12)-ALL w/oM S Top2\n(Top3) 74.40 55.75 63.74 68.32 57.38 62.37RoBERTa-STSbw ithD L\nDistilRoBERTa-ALL with GO\nIn column SBERT-WK, ‚Äún.a.‚Äù indicates that SBERT-WK is not applicable to BPLM models. The result size of Cross-Encoder results are in \nbrackets\nGO: Google Translate, DL: DeepL, MS: Microsoft Translator, P: precision, R: recall, F: F-measure\n SN Computer Science (2022) 3:387\n387 Page 18 of 20\nSN Computer Science\nresult of each Model-Group on the given metric. The last three \ncolumns of these tables also show the results of reranking the \ncandidates using Cross-Encoder before combination.\nOverall, we are able to improve the best precision of using \ntranslated corpora to 93.46% by combining the results of \nthe MGSBERTv1 models (Table¬† 14). This is an improvement \nof 22.23% compared to best single config result (71.23%, \nTable¬†10). Combining three models in MGSapSubset achieves \nthe best recall of 85.25% (Table¬†15), an increase of 11.58% \ncompared to the best single config (73.67%, Table¬†11). On \nthe other hand, combination only raises the best F-measure of \n1.84% compared to single config (from 61.90 to 63.74%). Again, \nthis best F-measure result is delivered by combining models in \nMGSapSubset as in best recall result. Unlike the enhancement we \ncould see in the single config results, reranking using Cross-\nEncoder can not improve the combination results further but \nrather worsens them.\nRecall vs result size It is clear, that applying union to three \nconfig result sets achieves a higher recall than a single con-\nfig, as after union the result size increases by a factor of three \nat the most. Hence, we ask, given the same result size, which \ncan deliver better recall: the union of three configs or a single \nconfig? To answer this question, we plot the change of recall \nover increasing result size up to 150. We only consider the best \nmodel regarding the metric recall of each Model-Group (Fig.¬†4). \nWe observe that the increase of recalls flattens at a result size of \napproximately 13 when annotating the original English corpus \n(Fig.¬†4a). On the other hand, when annotating a translated cor-\npus, the recalls keep increasing even until a result size of 140, \nthough the increasing rates mostly reaches a saturation at the \nresult size between 55 and 75 (Fig.¬†4b). Moreover, the single \nconfigs deliver higher recalls than combination when the result \nsizes smaller than approximately 30. However, with larger result \nsizes, the recalls of combination overtake those of single con-\nfigs. This shows combination does raise the overall recall limit \ncompared to single configs. These plots also reveal the poten-\ntial maximum recalls can be reached when retaining the best 150 \ncandidates. By combining the MGSapSubset models, it is possible \nto reach a recall of 94.48% and with single config using the best \nBPLM model (i.e., SapBERT-XLMRlarge ), a recall of 93.48% \nis attainable.\nResult Summary\nWe compile the best results generated by each approach and \nshow them in Fig.¬†5. Notably, if the task is not cross-lingual but \nto annotate the original English forms, the proposed Workflow-\nMT still outperforms the traditional string matching method \neven the questions and the corresponding concepts are syntacti-\ncally identical. Using the sentence encoder workflows, we gain \na large improvement in recall and F-measure. This indicates \nthat the use of sentence embeddings as semantic representation \ndoes help to find many more correct concepts. Incorporating \nmachine translators into the workflow (Workflow-MT) pro-\nduces better results than using original German forms as input \n(Workflow-Multi). This observation still holds true when the \n(a)\n(b)\nFig. 4  Increase of recall against result size. a The result of the best \nsingle config or combination in recall of each Model-Group starts \nfrom result size of 1, including the results of annotating original Eng-\nlish (OE) corpus. b The same results without OE corpus and starts \nfrom result size of 15\n0\n25\n50\n75\n100\nPrecision Recall F‚àímeasure\n%\nAnnoMap_MT\nWorkflow‚àíMulti\nWorkflow‚àíMT\nCombinatoin\nAnnoMap_OE\nWorkflow‚àíMT_OE\nFig. 5  The best results of each approach. OE: original English corpus\nSN Computer Science (2022) 3:387 \nPage 19 of 20 387\nSN Computer Science\nsame encoder is applied in different workflows, as we have seen \non SapBERT-XLMRlarge . Hence, we can conclude that using \nmachine translator is still inevitable before better aligned multi-\nlingual sentence encoders are available.\nThe best annotation quality we achieve using single config is \n71.23% in precision, 74.84% in recall and 61.90% in F-measure. \nAll these best results are generated using UMLS subset injected \nSBERTv2 models, i.e., DistilRoBERTa-ALL for precision and \nF-measure and RoBERTa-STSb for recall. Further, these two \nmodels are both enhanced by SBERT-WK and used Google \nTranslate corpus as input. In addition, RoBERTa-STSb obtains \nthe best recall with reranking of Cross-Encoder.\nFigure¬†5 also shows that among the three metrics, precision \nbenefits most from combination. The best precision, 93.46%, on \ntranslated corpora using combination, is almost equivalent to the \nresult of using AnnoMap to annotate the original English corpus \n(93.6%). Similarly, we also achieve a best recall of 85.25% that is \nalso amounting to that of the AnnoMap on original English cor-\npus (86%). However, there is still space for improvement in terms \nof F-measure (the best F-measure 63.74%). Overall, we achieved \nan improvement of 136% in recall (from 36.11% of AnnoMap to \n85.25% of combination), 52% in precision (61.60% of AnnoMap \nto 93.46% of combination) and 66% in F-measure (AnnoMap: \n38.36%, combination: 63.74%). We set our maximum result size \nas Top5 so that the system can provide a reasonably short list of \ncandidates for further manual verification (semi-automatic anno-\ntation). In this case, with the best recall of 85.25% and a precision \nof 100%, a F-measure of 92.04% is plausible.\nConclusion\nIn this study, we apply BERT-based pretrained language models \nto generate sentence embeddings to solve cross-lingual biomedi-\ncal concept normalization problem. We show that the annotation \nquality can be improved significantly compared to the conven-\ntional string matching tool. For the future work, we aim to apply \nsuch techniques onto other types of annotations (e.g., biomedical \nname entities) or in other domains.\nWe select current SOTA models that are specifically pre-\ntrained on biomedical corpus (the BPLM models) or only pre-\ntrained on plain English text (the SBERT models without UMLS \ninjection). The results show that the best performance of these \ntwo types of models is similar. This can be due to that many of \nthe questions in our medical forms are in colloquial language as \nthey are designed to interview general public. Therefore, extra \nbiomedical corpus pretraining does not benefit the annotation \nresults. Furthermore, we show that we can further enhance the \nannotation quality of SBERTv2 models using UMLS injection \nand outperform the best BPLM model (which is already UMLS-\ninjected). We also discover that UMLS injection using only the \nrelevant subset is sufficient to produce comparable (or even \nslightly better) results than using the full version of the UMLS. \nThis observation is similar to the idea of the PubMedBERT [24] \nthat more pretraining using out-domain corpora is not necessarily \nbeneficial for solving domain-specific tasks.\nWe tested two post-processing strategies in this study. Com-\nbination can improve annotation quality significantly and also \nraise the recall upper bound compared to single config. The \nreranking of Cross-Encoder benefits the results of configs \nbut does not improve combination result further. However, as \nFig.¬†4 shows, with the result size of 150, we have the potential \nof finding up to 94.48% of the correct annotations. Hence, we \nplan to develop better post-processing approaches that can rerank \nthe candidates so that the correct annotations are included in the \nTop5 result sets.\nAcknowledgements This work is funded by the German Research Foun-\ndation (DFG) (grant RA 497/22-1, ‚ÄúELISA - Evolution of Semantic \nAnnotations‚Äù) and by the ScaDS.AI. We also thank Leipzig Univer -\nsity Computing Centre for providing the computation resources and \nsupport.\nFunding Open Access funding enabled and organized by Projekt \nDEAL.\nConflict of interest On behalf of all the authors, the corresponding au-\nthor states that there is no conflict of interest.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Lin Y-C, Hoffmann P, Rahm E. Enhancing cross-lingual semantic \nannotations using deep network sentence embeddings. In: Pro -\nceedings of the 14th International Joint Conference on Biomedical \nEngineering Systems and Technologies (BIOSTEC 2021), vol. 5. \nHEALTHINF; 2021. p. 188‚Äì99.\n 2. Spitzer RL, Kroenke K, Williams JB, L√∂we B. A brief measure for \nassessing Generalized Anxiety Disorder: the GAD-7. Arch Intern \nMed. 2006;166(10):1092‚Äì7.\n 3. Sousa TV, Viveiros V, Chai MV, Vicente FL, Jesus G, Carnot \nMJ, Gordo AC, Ferreira PL. Reliability and validity of the Por -\ntuguese version of the Generalized Anxiety Disorder (GAD-7) \nscale. Health Qual Life Outcomes. 2015;13(1):50.\n 4. L√∂we B, Decker O, M√ºller S, Br√§hler E, Schellberg D, Herzog W, \nHerzberg PY. Validation and standardization of the Generalized \nAnxiety Disorder Screener (GAD-7) in the general population. Med \nCare. 2008;46(3):266‚Äì74.\n 5. Garc√≠a-Campayo J, Zamorano E, Ruiz MA, Pardo A, P√©rez-P√°ramo \nM, L√≥pez-G√≥mez V, Freire O, Rejas J. Cultural adaptation into \n SN Computer Science (2022) 3:387\n387 Page 20 of 20\nSN Computer Science\nSpanish of the Generalized Anxiety Disorder-7 (GAD-7) scale as a \nscreening tool. Health Qual Life Outcomes. 2010;8(1):8.\n 6. Lin Y-C, Christen V, Gro√ü A, Kirsten T, Cardoso SD, Pruski C, \nDa¬†Silveira M, Rahm E. Evaluating cross-lingual semantic annota-\ntion for medical forms. In: HEALTHINF; 2020. p. 145‚Äì55.\n 7. Devlin J, Chang M-W, Lee K, Toutanova K. BERT: pre-training of \ndeep bidirectional transformers for language understanding. 2018. \narXiv preprint arXiv: 1810. 04805.\n 8. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, \nKaiser ≈Å, Polosukhin I. Attention is all you need. In: Advances in \nneural information processing systems. 2017. pp. 5998‚Äì6008.\n 9. Zhu Y, Kiros R, Zemel R, Salakhutdinov R, Urtasun R, Torralba A, \nFidler S. Aligning books and movies: towards story-like visual expla-\nnations by watching movies and reading books. In: Proceedings of the \nIEEE International Conference on Computer Vision, 2015. pp. 19‚Äì27.\n 10. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, \nZettlemoyer L, Stoyanov V. RoBERTa: a robustly optimized BERT \npretraining approach. 2019. arXiv preprint arXiv: 1907. 11692.\n 11. Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, a distilled \nversion of BERT: smaller, faster, cheaper and lighter. 2019. arXiv \npreprint arXiv: 1910. 01108.\n 12. Bucilu«é C, Caruana R, Niculescu-Mizil A. Model compression. In: \nProceedings of the 12th ACM SIGKDD International Conference \non Knowledge Discovery and Data Mining, 2006. pp. 535‚Äì41.\n 13. Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural \nnetwork. 2015. arXiv preprint arXiv: 1503. 02531.\n 14. Wang W, Wei F, Dong L, Bao H, Yang N, Zhou M. MiniLM: Deep \nself-attention distillation for task-agnostic compression of pre-\ntrained transformers. In: NIPS'20: Proceedings of the 34th Interna-\ntional Conference on Neural Information Processing Systems, 2020; \np.5776‚Äì5788\n 15. Mirzadeh SI, Farajtabar M, Li A, Levine N, Matsukawa A, \nGhasemzadeh H. Improved knowledge distillation via teacher assis-\ntant. In: Proceedings of the AAAI Conference on Artificial Intel-\nligence, 2020;34:5191‚Äì98.\n 16. Sung M, Jeon H, Lee J, Kang J. Biomedical entity representations \nwith synonym marginalization. 2020. arXiv preprint arXiv: 2005. \n00239.\n 17. Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov RR, Le QV. \nXLNet: generalized autoregressive pretraining for language under-\nstanding. In: NIPS'19: Proceedings of the 33rd International Con-\nference on Neural Information Processing Systems, 2019. pp. \n5753‚Äì5763\n 18. Reimers N, Gurevych I. Sentence-BERT: sentence embeddings using \nsiamese BERT-networks. 2019. arXiv preprint arXiv: 1908. 10084.\n 19. Wang B, Kuo C-CJ. SBERT-WK: a sentence embedding method by \ndissecting BERT-based word models. 2020. arXiv preprint arXiv: \n2002. 06652.\n 20. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, Kang J. BioBERT: a \npre-trained biomedical language representation model for biomedi-\ncal text mining. Bioinformatics. 2020;36(4):1234‚Äì40.\n 21. Alsentzer E, Murphy JR, Boag W, Weng W-H, Jin D, Naumann T, \nMcDermott M. Publicly available clinical BERT embeddings. 2019. \narXiv preprint arXiv: 1904. 03323.\n 22. Johnson AE, Pollard TJ, Shen L, Li-Wei HL, Feng M, Ghassemi \nM, Moody B, Szolovits P, Celi LA, Mark RG. MIMIC-III, a freely \naccessible critical care database. Sci Data. 2016;3(1):1‚Äì9.\n 23. Peng Y, Yan S, Lu Z. Transfer learning in biomedical natural lan-\nguage processing: an evaluation of BERT and ELMO on ten bench-\nmarking datasets. 2019. arXiv preprint arXiv: 1906. 05474.\n 24. Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, Naumann \nT, Gao J, Poon H. Domain-specific language model pretraining for \nbiomedical natural language processing. 2020. arXiv preprint arXiv: \n2007. 15779.\n 25. Hao B, Zhu H, Paschalidis I. Enhancing clinical BERT embedding \nusing a biomedical knowledge base. In: Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics, 2020. pp. 657‚Äì61.\n 26. Liu F, Shareghi E, Meng Z, Basaldella M, Collier N. Self-alignment \npretraining for biomedical entity representations. In: Proceedings of \nthe 2021 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technolo-\ngies, p. 4228‚Äì38. Association for Computational Linguistics, Online \n(2021). https:// doi. org/ 10. 18653/ v1/ 2021. naacl- main. 334. https:// \naclan tholo gy. org/ 2021. naacl- main. 334.\n 27. Yuan Z, Zhao Z, Yu S. CODER: knowledge infused cross-lingual \nmedical term embedding for term normalization. 2020. arXiv preprint \narXiv: 2011. 02947.\n 28. Yuan Z, Liu Y, Tan C, Huang S, Huang F. Improving biomedical pre-\ntrained language models with knowledge. 2021. arXiv preprint arXiv: \n2104. 10344.\n 29. Wang X, Han X, Huang W, Dong D, Scott MR. Multi-similarity loss \nwith general pair weighting for deep metric learning. In: Proceedings \nof the IEEE/CVF Conference on Computer Vision and Pattern Rec-\nognition, 2019. pp. 5022‚Äì30.\n 30. Reimers N, Gurevych I. Making monolingual sentence embeddings \nmultilingual using knowledge distillation. 2020. arXiv preprint arXiv: \n2004. 09813.\n 31. Conneau A, Lample G, Ranzato M, Denoyer L, J√©gou H. Word trans-\nlation without parallel data. 2018. arXiv preprint arXiv: 1710. 04087.\n 32. Loeffler M, Engel C, Ahnert P, Alfermann D, Arelin K, Baber R, \nBeutner F, Binder H, Br√§hler E, Burkhardt R, et¬†al. The LIFE-Adult-\nStudy: objectives and design of a population-based cohort study with \n10,000 deeply phenotyped adults in Germany. BMC Public Health. \n2015;15(1):691.\n 33. Kroenke K, Spitzer RL, Williams JBW. The PHQ-15: validity of a new \nmeasure for evaluating the severity of somatic symptoms. Psychosom \nMed. 2002;64(2):258‚Äì66.\n 34. Christen V, Gro√ü A, Varghese J, Dugas M, Rahm E. Annotating medi-\ncal forms using UMLS. In: International Conference on Data Integra-\ntion in the Life Sciences. Springer; 2015. p. 55‚Äì69.\n 35. Christen V, Gro√ü A, Rahm E. A reuse-based annotation approach \nfor medical documents. In: International Semantic Web Conference. \nSpringer; 2016. p. 135‚Äì50.\n 36. Bowman SR, Angeli G, Potts C, Manning CD. A large annotated \ncorpus for learning natural language inference. 2015. arXiv preprint \narXiv: 1508. 05326.\n 37. Williams A, Nangia N, Bowman SR. A broad-coverage challenge \ncorpus for sentence understanding through inference. 2017. arXiv \npreprint arXiv: 1704. 05426.\n 38. Cer D, Diab M, Agirre E, Lopez-Gazpio I, Specia L. Semeval-2017 \ntask 1: semantic textual similarity-multilingual and cross-lingual \nfocused evaluation. 2017. arXiv preprint arXiv: 1708. 00055.\n 39. Lin Y-C, Christen V, Gro√ü A, Cardoso SD, Pruski C, Da¬†Silveira \nM, Rahm E. Evaluating and improving annotation tools for medical \nforms. In: Proc. Data Integration in the Life Science (DILS 2017). \nSpringer; 2017. pp. 1‚Äì16.\n 40. Humeau S, Shuster K, Lachaux M-A, Weston J. Poly-encoders: trans-\nformer architectures and pre-training strategies for fast and accurate \nmulti-sentence scoring. 2019. arXiv preprint arXiv: 1905. 01969.\n 41. Thakur N, Reimers N, Daxenberger J, Gurevych I. Augmented SBERT: \ndata augmentation method for improving bi-encoders for pairwise sen-\ntence scoring tasks. 2020. arXiv preprint arXiv: 2010. 08240.\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8126525282859802
    },
    {
      "name": "Natural language processing",
      "score": 0.6937082409858704
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6311215162277222
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.5830155611038208
    },
    {
      "name": "Sentence",
      "score": 0.5709811449050903
    },
    {
      "name": "Unified Medical Language System",
      "score": 0.5672088861465454
    },
    {
      "name": "Parsing",
      "score": 0.44012072682380676
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I926574661",
      "name": "Leipzig University",
      "country": "DE"
    }
  ]
}