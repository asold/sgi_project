{
  "title": "Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach",
  "url": "https://openalex.org/W4385573421",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2099349455",
      "name": "Miao Chen",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2105521496",
      "name": "XinJiang Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101240275",
      "name": "Tong Xu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2098156456",
      "name": "Yanyan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2347713146",
      "name": "Zhou Jing-bo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2040419331",
      "name": "Dejing Dou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1991795496",
      "name": "Hui Xiong",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong",
        "Guangzhou HKUST Fok Ying Tung Research Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962905474",
    "https://openalex.org/W3116465435",
    "https://openalex.org/W2997776495",
    "https://openalex.org/W3174887376",
    "https://openalex.org/W2532807140",
    "https://openalex.org/W3176395279",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2898972706",
    "https://openalex.org/W3116342879",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W3176356945",
    "https://openalex.org/W2739046565",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3098495697",
    "https://openalex.org/W3176585753",
    "https://openalex.org/W3034606970",
    "https://openalex.org/W2963541420",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W4288091035",
    "https://openalex.org/W3099873751",
    "https://openalex.org/W2963091658",
    "https://openalex.org/W2983105506",
    "https://openalex.org/W3026997957",
    "https://openalex.org/W2752047430",
    "https://openalex.org/W3156392851",
    "https://openalex.org/W3123091710",
    "https://openalex.org/W2963592583",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3174679944",
    "https://openalex.org/W2971160629",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2741690481"
  ],
  "abstract": "Although remarkable progress on the neural table-to-text methods has been made, the generalization issues hinder the applicability of these models due to the limited source tables. Large-scale pretrained language models sound like a promising solution to tackle such issues. However, how to effectively bridge the gap between the structured table and the text input by fully leveraging table information to fuel the pretrained model is still not well explored. Besides, another challenge of integrating the deliberation mechanism into the text-to-text pretrained model for solving the table-to-text task remains seldom studied. In this paper, to implement the table-to-text generation with pretrained language model, we propose a table structure understanding and text deliberating approach, namely TASD. To be specific, we devise a three-layered multi-head attention network to realize the table-structureaware text generation model with the help of the pretrained language model. Furthermore, a multi-pass decoder framework is adopted to enhance the capability of polishing generated text for table descriptions. The empirical studies, as well as human evaluation, on two public datasets, validate that our approach can generate faithful and fluent descriptive texts for different types of tables.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8199–8210\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nTowards Table-to-Text Generation with Pretrained Language Model: A\nTable Structure Understanding and Text Deliberating Approach\nMiao Chen§∗, Xinjiang Lu¶\u0000 , Tong Xu§, Yanyan Li¶, Jingbo Zhou¶, Dejing Dou¶, Hui Xiong†‡\n¶BIL, Baidu Research, §University of Science and Technology of China\n†Hong Kong University of Science and Technology (Guangzhou)\n‡Guangzhou HKUST Fok Ying Tung Research Institute\ncmer@mail.ustc.edu.cn, {luxinjiang,liyanyanliyanyan,zhoujingbo}@baidu.com,\ntongxu@ustc.edu.cn, doudejing@baidu.com, xionghui@ust.hk\nAbstract\nAlthough remarkable progress on the neural\ntable-to-text methods has been made, the gen-\neralization issues hinder the applicability of\nthese models due to the limited source tables.\nLarge-scale pretrained language models sound\nlike a promising solution to tackle such issues.\nHowever, how to e ffectively bridge the gap\nbetween the structured table and the text in-\nput by fully leveraging table information to\nfuel the pretrained model is still not well ex-\nplored. Besides, another challenge of integrat-\ning the deliberation mechanism into the text-\nto-text pretrained model for solving the table-\nto-text task remains seldom studied. In this\npaper, to implement the table-to-text genera-\ntion with pretrained language model, we pro-\npose a table structure understanding and text\ndeliberating approach, namely TASD. To be\nspecific, we devise a three-layered multi-head\nattention network to realize the table-structure-\naware text generation model with the help of\nthe pretrained language model. Furthermore,\na multi-pass decoder framework is adopted to\nenhance the capability of polishing generated\ntext for table descriptions. The empirical stud-\nies, as well as human evaluation, on two public\ndatasets, validate that our approach can gen-\nerate faithful and fluent descriptive texts for\ndifferent types of tables.\n1 Introduction\nThe task of learning to generate natural language\ndescriptions from non-linguistic input, which is\nreferred to as data-to-text, is important for many\napplications, such as weather forecast genera-\ntion (Mei et al., 2016), sports news writing (Wise-\nman et al., 2017), biography writing (Lebret et al.,\n2016), market comments writing (Murakami et al.,\n2017) and automatic question-answering (Li et al.,\n2021b). The input data can be in various forms\n∗This work was done when the first author was an intern\nat Baidu Research under the supervision of the second author.\nfor data-to-text though, here we focus on the text\ngeneration task that takes the table as input.\nInspired by neural machine translation models,\nprevious studies on table-to-text tasks mainly adopt\ntraditional seq2seq methods to generate table de-\nscriptions (Lebret et al., 2016; Wiseman et al.,\n2017; Liu et al., 2018; Gong et al., 2019b; Wang\net al., 2020; Li et al., 2021a). Despite generating\ntext with high fluency, lacking numerous source\ntables leads to lower generalizability of the table-\nto-text model. Recent progress in the pretrained\nlanguage model (Devlin et al., 2019; Radford et al.,\n2019) shows remarkable performance in solving\nnatural language processing tasks. The model pre-\ntrained on large-scale data possesses rich knowl-\nedge, which inspires us with the potential for solv-\ning generalization issues of the text generation task.\nTo exploit the expressive power of the pretrained\nmodel for the table-to-text task, it is necessary to\nserialize the input table effectively. Several works\nhave put efforts to bridge this gap, such as serial-\nizing the table into a token sequence (Zhang et al.,\n2020; Suadaa et al., 2021; Xing and Wan, 2021),\nor introducing an extra task to control the table\nrepresentation (Gong et al., 2020). However, none\nof these leveraged the table structure information\neffectively. Furthermore, the text-to-text pretrained\nmodel decodes and generates a sequence in a one-\npass forward process, which means it cannot per-\nceive the future words in advance on the target side.\nRecently, the deliberation mechanism (Niehues\net al., 2016; Geng et al., 2018) implemented by\nthe multi-pass decoder is proposed to tackle this\nproblem. However, how to adapt this approach for\ntext-to-text pretraining, which can be further ap-\nplied to the table-to-text task, is another challenge.\nTo this end, we propose a table structure under-\nstanding and text deliberating approach, namely\nTASD, to solve the table-to-text task with the pre-\ntrained language model enhanced by the deliber-\nation mechanism. Specifically, we first serialize\n8199\nthe table input with customized templates which\ndo not acquire the target cells to be labeled. Then,\nwe employ the multi-head attention in a hierarchi-\ncal way to learn the table representation that is\naware of table structure and apply it to guide the\nfine-tuning of the text-to-text pretrained model. Af-\nterward, we adopt the multi-pass decoder to realize\ntext deliberation. More specifically, we treat the\nabove table-structure-aware fine-tuned model as\nthe first-pass decoder and adopt another pretrained\nmodel as the second-pass decoder to further polish\nthe descriptive text. In the second-pass decoding\nphase, the table representation can be conveniently\nleveraged as the “original text” in the text deliber-\nation mechanism. The main contributions of this\nwork can be summarized as follows:\n• We propose a novel table-to-text generation\napproach (i.e., TASD) to assimilating the com-\nplete table information with the help of table\nstructure distillation, the pretrained language\nmodel, and the text deliberation.\n• We devise a table-structure-aware text gen-\neration model ( TASATG) via the hierarchi-\ncal multi-head attention network, which can\nrealize the content selection automatically.\nAnd we develop an effective text deliberation\nmethod dedicated to the table-to-text task.\n• Extensive experiments conducted on two dif-\nferent datasets demonstrate that TASD out-\nperforms comparable baselines in terms of\nvarious metrics.\n2 Related Work\n2.1 Table-to-Text Generation\nEncouraged by the success of seq2seq methods\nin machine translation and text summarization, re-\nsearchers proposed to formulate the input table as a\nsequence of records (Lebret et al., 2016; Wiseman\net al., 2017), and further improve the performance\nof table-to-text methods based on seq2seq by mod-\neling table representation (Liu et al., 2018; Gong\net al., 2019a). Introducing auxiliary tasks to enrich\nthe table representation (Tian et al., 2019; Li et al.,\n2021a) is another promising paradigm to address\nthe table-to-text problem. Moreover, there have\nbeen studies focusing on how to disaggregate the\ntable-to-text pipeline effectively to generate more\nfaithful and fluent text, e.g. leveraging content\nselection and planning (Puduppully et al., 2019;\nTrisedya et al., 2020; Bai et al., 2021), combin-\ning autoregressive and non-autoregressive meth-\nods (Wang et al., 2021). In addition, recent Trans-\nformers were also applied to solve the table-to-text\ntask (Gong et al., 2019b; Wang et al., 2020; Obeid\nand Hoque, 2020). However, current table-to-text\nmethods may fail to tackle the overfitting problem\naroused by the lack of diversity in small datasets.\nFine-tuning the model pretrained in a large cor-\npus and adapting to a specific task is an e ffective\napproach to tackling the generation issues disturbed\nby small data and large parameters (Radford et al.,\n2019). (Kale and Rastogi, 2020) explored the feasi-\nbility of applying the text-to-text pretrained model\nto the table-to-text task, (Gong et al., 2020) applied\nmulti-task learning to solve the table-to-text task\nwith pretrained language model, and (Suadaa et al.,\n2021) leveraged pretrained language model for fact\ninference in numerical table contents. However,\nthese approaches seldom perceived and integrated\nthe complete table information into the fine-tuning\nof the pretrained model. A table-to-text pretrained\nmodel (Xing and Wan, 2021) was proposed though,\nthe large and diversified table corpus is often un-\navailable. In addition, recent works on fact verifica-\ntion taking tabular as input (Yin et al., 2020; Dong\nand Smith, 2021) have suggested the effectiveness\nof the table-structure-aware pretrained model.\n2.2 Text Deliberation\nThe encoder-decoder framework has been widely\napplied to neural machine translation, while the\nsubsequent words are often invisible on the target\nside when decoding a sequence. To alleviate this, re-\nsearchers proposed to decode and refine the output\nsequence in multiple passes, like human cognitive\nbehavior when polishing an article. Studies have\nbeen made on text deliberation, such as the solu-\ntion with two separate stages (i.e., generating and\npolishing) (Niehues et al., 2016), combining two\nseparate stages as one framework (Xia et al., 2017),\nand deliberating generated text in multiple passes\nadaptively via reinforcement learning (Geng et al.,\n2018) or customized evaluating architecture (Li\nand Yao, 2021). To the best of our knowledge, we\nare the first to apply the deliberation mechanism to\nthe table-to-text problem.\n3 Preliminaries\n3.1 Problem Formulation\nOur table-to-text problem takes a table as input,\nand we formulate a table as a sequence of records:\nT = {τ1,1,τ1,2,··· ,τi,j,··· ,τm,n}, where m and n\ndenote the number of rows and columns of T, re-\n8200\nFigure 1: The framework overview of TASD.\nspectively. Then, we aim to generate a document Y\ncontaining words Y = y1y2 ··· yl that can describe\nthe content of T precisely, where l is the document\nlength. Formally, given a table T, the table-to-text\nmodel is excepted to generate a descriptive docu-\nment Y in an auto-regressive way\nyi = arg maxP(yi |T,y1y2 ··· yi−1; Θ), i = 1,··· ,l\nwhere Θ is the set of model parameters.\n3.2 Data\nNumericNLG Dataset. The numericNLG dataset\nwas released by (Suadaa et al., 2021). In this\ndataset, the tables demonstrate experimental re-\nsults in research papers, thus, most of the table\ncontents are numerical values. We use this dataset\nto evaluate the accuracy and smoothness of the\ngenerated descriptions for the table with numerical\ncontent. In particular, for each table of numer-\nicNLG, <table_id> acts as the pronoun of the\ntable, and <caption> is the descriptive text of the\ntable. Moreover, for each cell of a table, there are\n<metric>, (row and column) <header>, and\n<value> as different views of a cell.\nTotto Dataset. The Totto dataset (Parikh\net al., 2020) is an open-domain table-to-text\ndataset collected from Wikipedia. The table\ncontents are mainly in text form. The metadata\nof the Totto dataset includes <page_title>,\n<section_title> and <section_text>. In\ndetail, each cell of a table has corresponding\n<header> and <value>. Unlike numericNLG,\ntextual content in our Totto dataset accounts for\n62.4%, which can evaluate the text generation\neffectiveness for the tables with textual records.\n4 Methodology\nIn this section, we introduce the proposed frame-\nwork in detail. As shown in Fig. 1, our framework\nmainly consists of three components, i.e., template-\nbased table serialization , table-structure-aware\nfine-tuning, and text deliberation. Specifically, we\nfirst produce a sequence describing the table con-\ntents with customized templates. The templates we\nadopted do not require the target cells to be labeled.\nThen, to generate informative text, we adopt full ta-\nble representation learning to guide the description\ngeneration, such that the outcome text is capable\nof emphasizing and delineating the facts in the ta-\nble from a macroscopic perspective. Finally, we\nemploy and adapt the multi-pass decoder to our\ndata-to-text problem, which can further fine-tune\nthe generated table description. Technical details\nfor all three modules will be introduced separately\nin the following subsections.\n4.1 Template-based Table Serialization\nTo well harness the expressive power of the text-to-\ntext pretrained model for the input table, it is nec-\nessary to serialize the raw table first. The template-\nbased representation offers us a simple yet effective\nlinearization approach to generating descriptive\ntexts which can reflect the facts in a table without\nyielding an intractable downstream model.\nIn particular, the templates we adopted in this\nwork are devised to mention all the available facts\nin the table without knowing the emphasized cells\nin advance, which is different from (Suadaa et al.,\n2021). The template for describing facts consists\nof two parts:\n1. The title or descriptive text that comes with\nthe table.\n2. A series of expressions, in which each one\ndescribes the content of a cell.\nMore specifically, for the numericNLG dataset,\nwe apply the following template:\n<table_id> shows <caption>. <metric1,1\n> of <header1,1> is <value1,1>, ··· , <me\ntrici,j> of <headeri,j> is <valuei,j>, ··· .\nFor the Totto dataset, we apply another template:\nAs <page_title> <section_title>, <se\nction_text>. <header1,1> is <value1,1>,\n··· , <headeri,j> is <valuei,j>, ··· .\nThe second part of the template enumerates all the\ncells in the table. This preliminary table represen-\ntation, denoted by TS , covers all the available facts\nin a raw table. Note that, the templates we adopt\nmay encounter the content selection problem. In\ntable-to-text applications, target cells in the input\ntable are often not highlighted and the generated\ntable description should emphasize certain cells.\n8201\nFigure 2: The architecture of table-structure-aware text\ngeneration model (i.e., TASATG).\n4.2 Table-Structure-Aware Text Generation\nA text-to-text pretrained model can take the large-\nscale corpus as input to possess vast knowledge\nand generate texts in an unsupervised way so that\nit has been widely applied to text-generation tasks.\nWhen handling a specific text generation task, it is\neffective to fine-tune the pretrained model on new\ndata. However, for the table-to-text task, some hid-\nden information, like table structure, is most likely\nto be overlooked, though the drafted TS mentions\nall the available facts in the table. Thus, we pro-\npose to exploit table structure information to guide\nfine-tuning of the text-to-text pretrained model.\nAs shown in Fig. 2, we first encode the table\ncontent in a multi-view fashion. To be specific,\ngiven a cell τi,j in a table T, it can be viewed from\ndifferent perspectives, such as the value of τi,j, the\nrow header of τi,j, and the column header of τi,j,\netc. Then, we treat the k-th view of τi,j as a to-\nken sequence which is denoted by x(k)\ni,j . Afterward,\nwe pad x(k)\ni,j with placeholders (if necessary) and\nconcatenate these token sequences as follows:\nxi,j = x(1)\ni,j ⊛ x(2)\ni,j ⊛ ··· , (1)\nwhere ⊛ denotes the concatenation operator, and\nthe multi-viewed representation of a table T is de-\nnoted as X = [x1,1,··· ,xi,j,··· ,xm,n]. Each to-\nken of x(k)\ni,j can be encoded as a d-dimensional em-\nbedding by looking up the text-to-text pretrained\nmodel and updated accordingly when fine-tuning\nthe pretrained model. In this way, we can obtain\nthe semantic representation of table T, which is\ndenoted by E(0) ∈Rm×n×s×d, where s is the length\nof concatenated sequence xi,j.\nTo realize TASATG for table-to-text, we pro-\npose to employ multi-head attention (Vaswani et al.,\n2017) to guide fine-tuning of the text-to-text pre-\ntrained model. In particular, we adopt three multi-\nhead attention (MHA) layers to interactively extract\nthe information in the table in a hierarchical way.\nSpecifically, the MHA layer is defined as:\nQi = QWQ\ni , Ki = KW K\ni , Vi = VW V\ni\nhead i = Attention (Qi,Ki,Vi) = softmax\n(QiK⊤\ni√\nd\n)\nVi,\nMHA(Q,K,V) = [ head 1,··· , head h] WO,\nwhere Q, K, V represent the query, key and value\nin the attention mechanism, respectively.\nAs illustrated in Fig. 2, in the first MHA layer,\nwe add a cell text position embedding ( E(ctpe) ∈\nRs×d) to each cell of the aforementioned E(0), and\nfeed it to the multi-head attention to implement cell\ntext self-attention,\n˜E0 = E(0) ⊕E(ctpe),\nE(1) = MHA(˜E(0),˜E(0),˜E(0)),\nE(1) = 1\ns\ns∑\ni=1\n(E(1)[:,:,i,:]) ,\n(2)\nwhere ⊕denotes the element-wise addition opera-\ntion. Consequently, E(1) ∈Rm×n×d can be deemed\nas an initial aggregated table representation. Next,\nin the second MHA layer, we add a table position\nembedding (E(tpe) ∈Rm×n×d) to E(1) to implement\ntable structure self-attention,\n˜E(1) = E(1) ⊕E(tpe),\nE(2) = MHA(˜E(1),˜E(1),˜E(1)).\n(3)\nE(2) ∈Rm×n×d is the table-structure-aware represen-\ntation. Moreover, in the third MHA layer, we ap-\nply a multi-head cross-attention to take the hidden\nstate of the text-to-text pretrained model (denoted\nby H ∈Rs×d) as the attention query, such that we\ncan focus on the important cells of the table,\n˜H = MHA(H,E(2),E(2)) ⊕H. (4)\nThis new hidden state ˜H guided by the table repre-\nsentation will replace the original hidden state H\nin the text-to-text pretrained model to generate the\nprobability of the next word.\nNote that, the cross attention weights on differ-\nent table cells based on the previous words can\nrealize the content selection automatically. In ad-\ndition, we implement the text-to-text pretrained\nmodel with GPT2 (Radford et al., 2019), which\nadopts a decoder-only Transformer architecture.\n8202\n(a) Training.\n (b) First and second fine-tuning of TASATGwith vali-\ndation data.\n(c) Testing.\nFigure 3: Training, validation and testing procedures of the proposed TASD approach.\n4.3 Text Deliberation\nThe encoder-decoder framework applied in many\nsequence generation tasks often adopts a one-pass\nprocess while decoding a sequence. Though e ffi-\ncient, the one-pass decoder cannot perceive future\ncontext for further text deliberation. Multi-pass de-\ncoder extends the capability of generating more\nrefined text by exploring global information in the\nsequence (Niehues et al., 2016; Xia et al., 2017).\nFor the text-to-text pretrained model, due to the\nhuge amount of parameters of the pretrained lan-\nguage model, it is unwise to directly combine the\nmodels in different passes. A common solution is\nto concatenate the original serialized table content\nand the text generated in the previous pass to fine-\ntune the pretrained model in the next-pass decoding.\nHowever, in this way, the length of input text prob-\nably exceeds the limit of the text-to-text pretrained\nmodel, and the time complexity is too high.\nTo effectively implement the fine-tuning of the\ntext-to-text pretrained model in multiple passes,\nas shown in Figs. 3a and 3b, we take the table\nrepresentation as the “original text” and feed the\ntext generated in the first-pass fine-tuning plus\nthe table representation to the second-pass fine-\ntuning. Note that, as shown in Fig. 3a, we sep-\narately fine-tune the table-to-text generation task\nand the text-to-text deliberation task with two inde-\npendent TASATGmodels, and each of them takes\na text-to-text pretrained model as the backbone.\n5 Experiments\n5.1 Experimental Settings\nData. We conducted experiments on the aforemen-\ntioned datasets, i.e., numericNLG and Totto. The\nstatistics of the numericNLG dataset can be found\nin (Suadaa et al., 2021). Besides, the size of the\noriginal Totto dataset is 120K, which is much larger\nthan the numericNLG dataset. To evaluate differ-\nent methods for table-to-text with comparable data\nsize, for the Totto dataset, we filtered out the tables\nwith fewer rows and columns, i.e., #rows <8 and\n#columns <8, such that the filtered Totto dataset\ncontains 1.8K tables. Then, we randomly selected\n1.2K1 tables to generate the new Totto dataset.\nEvaluation Metrics.We calculated BLEU (from\ngram-1 to gram-4) (Papineni et al., 2002), ROUGE-\nL (Lin, 2004) and METEOR (Denkowski and\nLavie, 2014) to evaluate the quality of the gen-\nerated text. The BLEU-n with a small value of n\nmeasures the accuracy of the word level, and the\nBLEU-n with a large n can measure the fluency\nof the sentence. The ROUGE-L measures the re-\ncall rate based on the longest common sequence\nbetween source and target texts. The METEOR is\nbased on the harmonic mean of unigram precision\nand recall, with recall weighted higher than preci-\nsion. These metrics are widely used to measure the\naccuracy and fluency of the generated sentence.\nBaselines. We compare TASD with the following\nbaselines.\n• Template-based Table Serialization. We use\nthe template designed for table serialization\nas a baseline. Note that, the token sequence\ngenerated by the template-based method is\ndenoted as TS .\n• Pointer Generator(See et al., 2017). This\nis a seq2seq model with the attention and\ncopy mechanism. We take TS as input for\nthe pointer generator model.\n• TRM. We implemented a simplified version\nof the proposed TASD that omits the pos-\nsessed knowledge in the pretrained language\nmodel and removes text deliberation for focus-\ning on table representation modeling, namely\nTRM. In particular, TRM adopts the architec-\nture of GPT2 but initializes the parameters\nrandomly and trains 100 epochs at most for\nfine-tuning. Besides, TRM takes TS plus the\ntable structure representation as input and is\nfed with TS in the inference phase.\n1The size of numericNLG data is 1.3K.\n8203\nTable 1: Performance comparisons of the automatic evaluation on the numericNLG dataset.\nMethod BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L\nTemplate-based Method 10.28 5.52 2.83 1.14 11.31 11.49\nPointer Generator 5 .10±0.59 2.71±0.19 1.16±0.17 0.56±0.04 7.82±0.15 15.21±0.14\nTRM 14 .16±0.97 6.05±0.50 2.11±0.13 0.80±0.12 9.72±0.94 12.72±0.80\nFine-tuned GPT2 16 .13±0.56 9.02±0.31 4.68±0.22 2.20±0.22 10.14±0.32 17.48±0.36\nTableGPT 18 .69±0.39 8.21±0.24 3.31±0.19 1.51±0.14 11.06±0.18 16.90±0.27\nTASD w/o TAS 18.20±2.40 9.74±1.01 4.38±0.31 1.98±0.39 10.64±0.86 19.29±1.77\nTASD w/o D 18.02±0.50 10.06±0.25 5.20±0.13 2.47±0.20 10.99±0.29 18.57±0.27\nTASD w/o 1st-TAS 20.07±1.94 10.35±0.69 4.67±0.35 2.05±0.34 11.52±0.80 20.10±0.62\nTASD 21.81±1.13 11.03±0.11 4.92±0.22 2.15±0.39 11.87±0.40 20.40±0.80\nTable 2: Performance comparisons of the automatic evaluation on the Totto dataset.\nMethod BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L\nTemplate-based Method 0.84 0.43 0.23 0.09 4.59 1.51\nPointer Generator 11 .34±1.57 2.05±0.83 0.45±0.27 0.35±0.13 5.38±0.78 14.46±1.46\nTRM 10 .21±1.79 3.44±0.88 1.21±0.48 0.54±0.25 9.30±1.16 11.52±2.03\nFine-tuned GPT2 9 .53±0.51 3.65±0.34 1.18±0.37 0.40±0.26 9.89±0.39 10.69±0.27\nTableGPT 6 .80±0.26 3.51±0.22 1.33±0.21 0.76±0.12 11.10±0.42 11.73±0.44\nTASD w/o TAS 13.70±0.90 4.44±0.69 1.28±0.47 0.65±0.35 10.79±0.83 14.47±1.11\nTASD w/o D 10.03±0.39 4.42±0.29 1.64±0.36 0.71±0.38 10.29±0.49 10.67±0.34\nTASD w/o 1st-TAS 13.90±0.60 5.07±0.61 1.68±0.52 0.79±0.25 10.98±0.40 14.88±0.71\nTASD 14.19±1.08 5.17±0.38 1.71±0.32 0.78±0.21 11.65±0.71 14.96±1.10\n• Fine-tuned GPT2(Radford et al., 2019). We\ntake the concatenation of TS and Y as the in-\nput for fine-tuning. In the inference phase,\nwe only feed TS to the model to generate Y\nstarting after the last token of TS .\n• TableGPT (Gong et al., 2020). TableGPT is\na state-of-the-art table-to-text method. To im-\nprove the text fidelity and exploit the struc-\ntural information at the same time, TableGPT\nemploys a multi-task learning paradigm con-\nsisting of two auxiliary tasks, that is, one task\nreconstructs the table structure from represen-\ntations of GPT2, and the other aligns the tables\nand the information in the generated text.\nImplementation Details. The split settings for\ntraining, validation and, testing were 1084:136:135\n2 for the numericNLG dataset and 960:120:120\nfor the Totto dataset, respectively. Regarding auto-\nmatic evaluation, all results of deep models were\nobtained by conducting experiments on a Linux\nmachine with Nvidia A100 GPU, and the averaged\nresults of 5 runs were reported. Besides, an Adam\n2This setting follows the experiments of (Suadaa et al.,\n2021).\noptimizer was utilized (with an initial learning rate\nof 3e-5) for GPT2 fine-tuning, and the training was\niterated in 20 epochs at most. A beam search algo-\nrithm was adopted when decoding a sequence and\nthe beam width was set to 5 3.\n5.2 Automatic Evaluation\nThe comparisons of automatic evaluation results\nbetween TASD and other baselines can be found\nin Tables 1 and 2. In general, TASD outperforms\nthe baselines for all the metrics on two datasets. In\nparticular, compared to the reported best result of\nall the baselines, TASD achieves improvements of\n3.12 for BLEU-1 (18.69 →21.81), 2.01 for BLEU-\n2 (9.02 →11.03), 0.24 for BLEU-3 (4.68 →4.92),\n0.56 for METEOR (11.31 →11.87), and 2.92 for\nROUGE-L (17.48 →20.40) on the numericNLG\ndataset, and 2.85 for BLEU-1 (11.34 →14.19),\n1.52 for BLEU-2 (3.65 →5.17), 0.38 for BLEU-3\n(1.33 →1.71), 0.02 for BLEU-4 (0.76 →0.78),\n0.55 for METEOR (11.10 →11.65), and 0.50 for\nROUGE-L (14.46 →14.96) on the Totto dataset.\n3Our implementation is available at https://github.\ncom/ramber1836/TASD.\n8204\nIn other words, for different types of source tables,\nTASD generates better descriptive texts w.r.t. accu-\nracy at the word level, recall of the sequence, and\nfluency of sentences.\nBesides, we have the following observations: 1)\nThe template-based method performs much bet-\nter on the numericNLG dataset compared to the\nTotto dataset, since the referenced table descrip-\ntions in numericNLG were collected from scientific\npapers, however, the table summaries in the Totto\ndataset are more diverse. 2) In the Totto dadaset,\nthe pointer generator model tends to cover more\nwords in descriptive text and generate more fluent\nsentences than the template-based method, as the\ncontents in source tables of the Totto dataset are\nmostly linguistic. This can also explain why the\npointer generator performs worse than the template-\nbased method on the numericNLG dataset w.r.t.\nBLEU and METEOR. 3) Fine-tuned GPT2 can\ngenerate more faithful and fluent text than other\nbaselines (refer to Tables 1 and 2) most of the time,\nwhich validates the effectiveness of the pretrained\nlanguage model. 4) In general, TableGPT performs\nbetter, and even the best, among all the baselines.\nIn the numericNLG dataset, the headers of the\ninput tables (a.k.a. the attributes of records for\nTableGPT) are more diverse, which may explain\nwhy the performance of TableGPT is not promising\nas expected on the numericNLG dataset. 5) TRM\ncan generate comparable, or even better descriptive\ntext as fined-tuned GPT2, which further suggests\nthe effectiveness of table structure understanding.\n5.3 Ablation Analysis\nMoreover, to verify the effectiveness of different\nmodules, we compare TASD with its variants.\n• After generating text with fine-tuned GPT2,\nwe fed the generated text concatenated with\nTS to another fine-tuned GPT2 to realize the\nsecond-pass decoder without table structure\nrepresentation.\n• We implemented TASD without deliberating\non the outcome text, which means that we\nrealized TASATGbased on GPT2 in a one-\npass forward process.\n• TASD w/o 1st-TAS. We removed table struc-\nture modeling in the first-pass decoding from\nTASD, which was implemented by taking the\nfine-tuned GPT2 as the first-pass decoder and\nthe table-structure-aware fine-tuned GPT2 as\nthe second-pass decoder.\nAs can be seen in Tables 1 and 2, TASD w/o TAS\nperforms worse than TASD under all metrics, since\nthe table structure modeling can benefit the fine-\ntuning of GPT2. This can also be validated by com-\nparing fine-tuned GPT2 to TASD w/o D. Besides,\nthe effectiveness of deliberating text can be proven\nby comparing TASD w/o D to TASD (this can also\nbe validated by comparing fine-tuned GPT2 to\nTASD w/o TAS). While text deliberation may harm\nsentence fluency as depicted by the results of these\nmethods w.r.t. BLEU-3 & 4 in Table 1. In addition,\nTASD w/o 1st-TAS outperforms TASD w/o TAS under\nall metrics suggesting that taking the table repre-\nsentation as the “original text” in the deliberation\nmechanism is also effective.\n5.4 Qualitative Analysis\nFigs. 4(a) and (b) show two selected source ta-\nbles and corresponding descriptive texts (i.e., cap-\ntion and section_text) in numericNLG and Totto\ndatasets. Fig. 4(c) demonstrates the generated de-\nscriptions by different methods. The text that cor-\nrectly reflects the facts of the source table is in\ngreen, the erroneous text is in red, and the con-\nfusing text is in blue. We can see that, there are\nmany grammatical errors in the text produced by\nthe pointer generator. Fine-tuned GPT2 tends to\nrepeat phrases and sentences due to the limited\nknowledge about the input table, which can also\nexplain why the fine-tuned GPT2 can obtain a false\nhigh score in BLEU-n as n grows. Thanks to the\nsemantic knowledge brought by pretraining, fine-\ntuned GPT2 can generate more natural descriptions,\nin which, however, perplexing factual errors ex-\nist. Compared to fine-tuned GPT2, the description\ngenerated by TASD is more relevant to the table\ncontents. Since the target cells are not known in\nadvance, the generated text may miss the empha-\nsized points described in the reference. The text\ngenerated by TableGPT is also fluent, though coun-\nterfactual descriptions may exist.\n5.5 Human Evaluation\nWe randomly selected 30 samples from the test set\nin numericNLG and Totto datasets, respectively,\nand invited 10 volunteers to evaluate the quality of\nthe outcome text by considering three criteria, i.e.,\ngrammar, coherence & concise, and factual per-\nspective (correct and relevant). Each criterion has\nscores of five degrees, ranging from 1 (the worst) to\n5 (the best). The averaged scores were reported in\nTable 3, which show that TASD can generate more\n8205\nFigure 4: Two examples of the generated table descriptions.\nTable 3: Result of Human Evaluation\nDataset Method Grammar Coherence\n& Concise\nFactual per-\nspective\nnumericNLG\nPointer Genera-\ntor 3.16±0.99 2.73±1.20 1.54±0.69\nFine-tuned\nGPT2 3.42±0.56 3.11±0.58 2.51±0.45\nTASD w/o D 3.72±0.61 3.48±0.55 2.82±0.45\nTASD 4.17±0.72 3.98±0.64 3.15±0.73\nTotto\nPointer Genera-\ntor 2.03±0.71 1.89±0.82 1.56±0.55\nFine-tuned\nGPT2 2.60±0.55 2.36±0.64 1.85±0.46\nTASD w/o D 2.63±0.52 2.46±0.60 1.89±0.46\nTASD 3.4±0.66 3.18±0.70 2.25±0.69\nreadable and coherent texts, and describe more\ncorrect facts. Moreover, the pretrained models con-\nsistently achieve better scores than the pointer gen-\nerator on grammar and coherence because of the\nexpressive power learned from the large-scale cor-\npus. In the Totto dataset, the improvement of the\ntable structure modeling is smaller than that of the\npolishing mechanism, which is consistent with the\nautomatic evaluation results in Table 2.\n6 Discussion\nIn our work, we devised a two-pass decoder frame-\nwork dedicated to the table-to-text task with the\nhelp of the table-structure-aware text generation\nmodel (i.e., TASATG). However, the effectiveness\nof the text deliberation for the table-to-text task\nshould be further explored and integrated into the\ntable-structure-aware modeling in a more harmonic\nFigure 5: Table reconstruction for table-structure-aware\nmodeling enhancement.\nmanner. To discuss the limitation of the text de-\nliberation of TASD, we additionally developed a\ntable content reconstruction loss and integrate it\ninto TASD in a multi-task learning fashion.\nSpecifically, given the table-structure-aware em-\nbedding E(2) generated with Eq. (3), we randomly\nmask certain cells of the input table and yield a\npartially corrupted embedding of the input table,\ndenoted by ˆE(2). Then, a two-layer MLP (i.e., multi-\nlayer perceptron) is adopted to restore the table-\nstructure-aware embedding. Afterward, an MSE\n(i.e., mean square error) loss is adopted to mea-\nsure the effectiveness of table reconstruction and\nfurther integrated into the TASD framework in the\nmulti-task learning paradigm. The process of table\nreconstruction is demonstrated in Fig. 5.\nWe carried out a series of experiments to evalu-\nate the performance of TASD w/ and w/o the help\nof table reconstruction loss (i.e., TRLoss) on nu-\nmericNLG and Totto datasets in terms of BLEU-n\n(1 to 4), METEOR, and ROUGE-L. The results can\nbe found in Tables 4 and 5.\n8206\nTable 4: The performances of TASD w/ and w/o the table reconstruction on the numericNLG dataset.\nMethod BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L\nTASD w/o D 18.02±0.50 10.06±0.25 5.20±0.13 2.47±0.20 10.99±0.29 18.57±0.27\nTASD w/o D w/ TRLoss 20.56±0.25 11.57±0.21 5.90±0.23 2.98±0.17 12.00±0.48 20.50±0.39\nTASD w/ TRLoss 19.29±0.38 10.12±0.24 5.32±0.25 2.62±0.22 12.18±0.90 18.95±0.69\nTASD w/ TRLoss in 1st pass 18.23±0.68 9.39±0.52 4.64±0.26 2.36±0.24 11.51±0.78 18.13±0.45\nTASD w/ TRLoss in 2nd pass 19.38±2.21 10.33±1.34 5.11±0.73 2.40±0.38 11.35±0.92 18.69±1.05\nTASD 21.81±1.13 11.03±0.11 4.92±0.22 2.15±0.39 11.87±0.40 20.40±0.80\nTable 5: The performances of TASD w/ and w/o the table reconstruction on the Totto dataset.\nMethod BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L\nTASD w/o D 10.03±0.39 4.42±0.29 1.64±0.36 0.71±0.38 10.29±0.49 10.67±0.34\nTASD w/o D w/TRLoss 9.94±0.43 4.35±0.31 1.63±0.31 0.75±0.13 10.37±0.22 10.62±0.60\nTASD w/ TRLoss 14.57±0.87 5.22±0.42 1.70±0.49 0.89±0.38 11.79±0.77 15.28±0.86\nTASD w/ TRLoss in 1st pass 14.00±0.82 5.31±0.27 1.72±0.25 0.75±0.13 11.02±0.77 14.74±0.51\nTASD w/ TRLoss in 2nd pass 13.89±0.58 4.78±0.61 1.47±0.14 0.52±0.20 11.07±0.66 14.73±0.79\nTASD 14 .19±1.08 5.17±0.38 1.71±0.32 0.78±0.21 11.65±0.71 14.96±1.10\nAccording to the results reported on the nu-\nmericNLG dataset, the TRLoss is helpful in en-\nhancing the capability of table comprehension\nthough, the best performance is achieved by TASD\nw/o D w/TRLoss. It seems that the performance im-\nprovement gained by the table comprehension en-\nhancement is sacrificed after the text deliberation\nis adopted. Meanwhile, on the Totto dataset, TASD\nwith the table reconstruction (i.e., TASD w/TRLoss)\ndoes achieve the best performance in terms of\nBLEU-1, BLEU-2, METEOR, and ROUGE-L,\nthough the improvement is not remarkable. The\ncontents of the input tables are mainly linguistic\nand the table structures are not too diverse might\nbe able to explain the performance improvement\nof TASD w/TRLoss on the Totto dataset. With the\nabove comparisons, we can conclude that, for the\ninput tables with diverse structures, the limitation\nof the current text deliberation mechanism cannot\nbe neglected if one aims to enhance the capability\nof table comprehension for the table-to-text task.\nMoreover, this also suggests that the generalization\ncapability of text deliberation of TASD should be\nimproved in the future.\nLimitations. In this work, long tables in the\nTotto dataset are removed since the efficiency and\nperformance of TASD on large tables could be low-\nered. In the future, the capability of handling long\ntables for table-to-text models should be further ex-\nplored. Besides, a large-scale and more exhaustive\nhuman evaluation is necessary. We plan to recruit\nmore volunteers to conduct the human annotation.\n7 Conclusion\nIn this paper, to realize table-to-text with the pre-\ntrained language model, we proposed a table struc-\nture understanding and text deliberating approach,\nnamely TASD. The table structure understanding\nwas realized by developing a hierarchical multi-\nhead attention network, which can benefit the fine-\ntuning of the text-to-text pretrained model. The\nfully represented table information benefits not\nonly the pretrained language model but also the\ntext deliberation process since the structure infor-\nmation with rich semantics could be fed into the\nsecond-pass decoding naturally. We carried out ex-\ntensive experiments on two public datasets with\ndifferent table types. Automatic and human-based\nevaluations, as well as qualitative analysis, vali-\ndate the effectiveness of our approach to generating\nfaithful and fluent table descriptions. In the future,\nwe will improve text deliberation by devising a\nunified framework to integrate the multi-pass de-\ncoder and refine the descriptive text paying more\nattention to sentence fluency.\nAcknowledgements\nThis work is supported in part by Foshan\nHKUST Projects (FSUST21-FYTRI01A, FSUST\n21-FYTRI02A).\n8207\nReferences\nYang Bai, Ziran Li, Ning Ding, Ying Shen, and Hai-\nTao Zheng. 2021. Infobox-to-text generation with\ntree-like planning based attention network. In IJCAI,\npages 3773–3779.\nMichael Denkowski and Alon Lavie. 2014. Meteor uni-\nversal: Language specific translation evaluation for\nany target language. In 9th workshop on statistical\nmachine translation, pages 376–380.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT, pages 4171–4186.\nRui Dong and David A Smith. 2021. Structural en-\ncoding and pre-training matter: Adapting bert for\ntable-based fact verification. In EACL, pages 2366–\n2375.\nXinwei Geng, Xiaocheng Feng, Bing Qin, and Ting\nLiu. 2018. Adaptive multi-pass decoder for neural\nmachine translation. In EMNLP, pages 523–532.\nHeng Gong, Xiaocheng Feng, Bing Qin, and Ting Liu.\n2019a. Table-to-text generation with e ffective hier-\narchical encoder on three dimensions (row, column\nand time). In EMNLP-IJCNLP, pages 3143–3152.\nHeng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin,\nWei Bi, Xiaojiang Liu, and Ting Liu. 2020. Tablegpt:\nFew-shot table-to-text generation with table structure\nreconstruction and content matching. In COLING,\npages 1978–1988.\nLi Gong, Josep M Crego, and Jean Senellart. 2019b.\nEnhanced transformer model for data-to-text genera-\ntion. In Proceedings of the 3rd Workshop on Neural\nGeneration and Translation, pages 148–156.\nMihir Kale and Abhinav Rastogi. 2020. Text-to-text\npre-training for data-to-text tasks. In INLG, pages\n97–102.\nRémi Lebret, David Grangier, and Michael Auli. 2016.\nNeural text generation from structured data with ap-\nplication to the biography domain. In EMNLP, pages\n1203–1213.\nLiang Li, Can Ma, Yinliang Yue, and Dayong Hu. 2021a.\nImproving encoder by auxiliary supervision tasks\nfor table-to-text generation. In ACL-IJCNLP, pages\n5979–5989.\nXiao Li, Yawei Sun, and Gong Cheng. 2021b. Tsqa:\nTabular scenario based question answering. In AAAI,\nvolume 35, pages 13297–13305.\nYangming Li and Kaisheng Yao. 2021. Rewriter-\nevaluator architecture for neural machine translation.\nIn ACL-IJCNLP, pages 5701–5710.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nTianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,\nand Zhifang Sui. 2018. Table-to-text generation by\nstructure-aware seq2seq learning. In AAAI.\nHongyuan Mei, Mohit Bansal, and Matthew R Walter.\n2016. What to talk about and how? selective gener-\nation using lstms with coarse-to-fine alignment. In\nNAACL-HLT, pages 720–730.\nSoichiro Murakami, Akihiko Watanabe, Akira\nMiyazawa, Keiichi Goshima, Toshihiko Yanase, Hi-\nroya Takamura, and Yusuke Miyao. 2017. Learning\nto generate market comments from stock prices. In\nACL, pages 1374–1384.\nJan Niehues, Eunah Cho, Thanh-Le Ha, and Alex\nWaibel. 2016. Pre-translation for neural machine\ntranslation. In COLING, pages 1828–1836.\nJason Obeid and Enamul Hoque. 2020. Chart-to-text:\nGenerating natural language descriptions for charts\nby adapting the transformer model. In INLG, pages\n138–147.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalua-\ntion of machine translation. In ACL, pages 311–318.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Di-\npanjan Das. 2020. Totto: A controlled table-to-text\ngeneration dataset. In EMNLP, pages 1173–1186.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\nData-to-text generation with content selection and\nplanning. In AAAI, volume 33, pages 6908–6915.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In ACL, pages 1073–1083.\nLya Hulliyyatus Suadaa, Hidetaka Kamigaito, Kotaro\nFunakoshi, Manabu Okumura, and Hiroya Takamura.\n2021. Towards table-to-text generation with numeri-\ncal reasoning. In ACL-IJCNLP, pages 1451–1465.\nRan Tian, Shashi Narayan, Thibault Sellam, and\nAnkur P Parikh. 2019. Sticking to the facts: Con-\nfident decoding for faithful data-to-text generation.\narXiv.\nBayu Trisedya, Jianzhong Qi, and Rui Zhang. 2020.\nSentence generation for entity description with\ncontent-plan attention. In AAAI, volume 34, pages\n9057–9064.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS, pages 5998–6008.\n8208\nPeng Wang, Junyang Lin, An Yang, Chang Zhou,\nYichang Zhang, Jingren Zhou, and Hongxia Yang.\n2021. Sketch and refine: Towards faithful and in-\nformative table-to-text generation. In ACL-IJCNLP,\npages 4831–4843.\nZhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and\nChangyou Chen. 2020. Towards faithful neural table-\nto-text generation with content-matching constraints.\nIn ACL, pages 1072–1086.\nSam Wiseman, Stuart M Shieber, and Alexander M\nRush. 2017. Challenges in data-to-document genera-\ntion. In EMNLP, pages 2253–2263.\nYingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,\nNenghai Yu, and Tie-Yan Liu. 2017. Deliberation\nnetworks: Sequence generation beyond one-pass de-\ncoding. NeurIPS, 30:1784–1794.\nXinyu Xing and Xiaojun Wan. 2021. Structure-aware\npre-training for table-to-text generation. In ACL-\nIJCNLP, pages 2273–2278.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\nbastian Riedel. 2020. Tabert: Pretraining for joint\nunderstanding of textual and tabular data. In ACL,\npages 8413–8426.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In EMNLP, pages 1624–1629.\nA Human Evaluation Settings\nThe criteria adopted in our human-based evaluation\nare (1) Grammar (e.g., is this paragraph grammat-\nical?), (2) Coherence & Concise (e.g., is this para-\ngraph coherent and contextually consistent? does\nit repeat redundant information?), and (3) Factual\nperspective (e.g., are the facts that this paragraph\ndescribes correct? are these facts related to refer-\nences and tables?). More specifically, we list the\ndetailed justifications on how to score the generated\ntext in each criterion as follows.\nGrammar\n•1 It is more like garbled code than a paragraph.\n•2 There are many obvious grammatical mis-\ntakes.\n•3 There are a few obvious grammatical mistakes.\n•4 There are few grammatical mistakes.\n•5 There are no grammatical mistakes.\nCoherence & Concise\n•1 The logic of text expression is chaotic and\nnonsense.\n•2 There are a lot of logical inconsistencies or\nredundant information.\n•3 There are some logical inconsistencies or re-\ndundant information.\n•4 There are a few logical inconsistencies or\nredundant information, but it does not a ffect\nbrowsing.\n•5 The logic of the text is smooth without redun-\ndant information.\nFactual Perspective\n•1 The paragraph does not coincide with the ref-\nerence or table, and it is full of information\ninconsistent with the facts.\n•2 The paragraph describes the facts incorrectly\nand has a low correlation with reference, but is\nrelated to the information in the table.\n•3 The paragraph description is incorrect, but it\nis highly coincident with the reference.\n•4 The paragraph description is basically correct,\nand the coincidence with the reference is low,\nbut it also describes the information in the table.\n•5 The paragraph description is correct and\nhighly coincident with the reference.\nB Illustrative Examples of Generated\nDescriptions\nWe additionally selected another two examples of\nthe generated table descriptions from the numeric-\nNLG and Totto datasets, respectively. The results\nare shown in Figs. 6 and 7. From these four ex-\namples, we can see that TASD can generate more\naccurate and fluent descriptive texts. While incor-\nrect descriptions can be found in the outcome texts\ngenerated by different models for cases D and F,\nwhich suggests that generating faithful descriptions\nfor open-domain tables is much more challenging\nand requires more powerful and, thus larger, pre-\ntrained language models.\nC Extra Implementation Details\nThe learning rate of GPT2 was searched from{3e −\n4,3e −5,3e −6}. In the evaluation of discussing\nthe limitation of text deliberation (see Section 6),\na trade-off parameter for balancing the GPT2 fine-\ntuning loss and the TRLoss was adopted, then the\ntrade-off parameter was searched from {1e −1,5e −\n2,1e −2,5e −3,1e −3}, and 1e-2 was selected for\nthe reported performance. Besides, the reported\nresults in Tables 4 and 5 were averaged in 3 runs.\n8209\nFigure 6: Generated table descriptions on cases C and D.\nFigure 7: Generated table descriptions on cases E and F.\n8210",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8585261106491089
    },
    {
      "name": "Table (database)",
      "score": 0.7592445611953735
    },
    {
      "name": "Language model",
      "score": 0.6585538983345032
    },
    {
      "name": "Natural language processing",
      "score": 0.5893741250038147
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5726767778396606
    },
    {
      "name": "Generalization",
      "score": 0.5140329599380493
    },
    {
      "name": "Task (project management)",
      "score": 0.42473655939102173
    },
    {
      "name": "Data mining",
      "score": 0.18250399827957153
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}