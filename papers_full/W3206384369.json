{
    "title": "Unifying Multimodal Transformer for Bi-directional Image and Text Generation",
    "url": "https://openalex.org/W3206384369",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2937823523",
            "name": "Yupan Huang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2114466394",
            "name": "Hongwei Xue",
            "affiliations": [
                "University of Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2071563452",
            "name": "Bei Liu",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2122795086",
            "name": "Yutong Lu",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2506483933",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W3104152799",
        "https://openalex.org/W3034655362",
        "https://openalex.org/W2988975212",
        "https://openalex.org/W2965597639",
        "https://openalex.org/W2982450728",
        "https://openalex.org/W2986670728",
        "https://openalex.org/W3184784418",
        "https://openalex.org/W3035497460",
        "https://openalex.org/W2331128040",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W2990818246",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2795151422",
        "https://openalex.org/W2557449848",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2962845008",
        "https://openalex.org/W2963084599",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W2963966654",
        "https://openalex.org/W2963101956",
        "https://openalex.org/W2990069284",
        "https://openalex.org/W2890531016",
        "https://openalex.org/W2964024144",
        "https://openalex.org/W2997591391",
        "https://openalex.org/W2966792645",
        "https://openalex.org/W3103651098",
        "https://openalex.org/W2099471712",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2951140085",
        "https://openalex.org/W2952342379",
        "https://openalex.org/W2564591810",
        "https://openalex.org/W3014611590",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W2563399268",
        "https://openalex.org/W639708223",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W4296979096",
        "https://openalex.org/W2921737854",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2432004435",
        "https://openalex.org/W3000433013",
        "https://openalex.org/W2803775694",
        "https://openalex.org/W2123301721",
        "https://openalex.org/W2963248296",
        "https://openalex.org/W4301206121",
        "https://openalex.org/W2782980316",
        "https://openalex.org/W2176263492",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W3129576130",
        "https://openalex.org/W2969862959",
        "https://openalex.org/W2950761309",
        "https://openalex.org/W2963373786",
        "https://openalex.org/W2931831966",
        "https://openalex.org/W2423557781",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2600463316",
        "https://openalex.org/W3089089004",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W2892181857",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2949999304",
        "https://openalex.org/W2953106684",
        "https://openalex.org/W3153469116",
        "https://openalex.org/W3165647589",
        "https://openalex.org/W2405756170",
        "https://openalex.org/W4320013936",
        "https://openalex.org/W2963981733",
        "https://openalex.org/W4289542422",
        "https://openalex.org/W3016923549",
        "https://openalex.org/W2962968665",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2968549119"
    ],
    "abstract": "We study the joint learning of image-to-text and text-to-image generations,\\nwhich are naturally bi-directional tasks. Typical existing works design two\\nseparate task-specific models for each task, which impose expensive design\\nefforts. In this work, we propose a unified image-and-text generative framework\\nbased on a single multimodal model to jointly study the bi-directional tasks.\\nWe adopt Transformer as our unified architecture for its strong performance and\\ntask-agnostic design. Specifically, we formulate both tasks as sequence\\ngeneration tasks, where we represent images and text as unified sequences of\\ntokens, and the Transformer learns multimodal interactions to generate\\nsequences. We further propose two-level granularity feature representations and\\nsequence-level training to improve the Transformer-based unified framework.\\nExperiments show that our approach significantly improves previous\\nTransformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for\\ntext-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for\\nfine-tuned image-to-text generation on the MS-COCO dataset. Our code is\\navailable online.\\n",
    "full_text": "Unifying Multimodal Transformer\nfor Bi-directional Image and Text Generation\nYupan Huang‚àó\nSun Yat-sen University\nhuangyp28@mail2.sysu.edu.cn\nHongwei Xue‚àó\nUniversity of Science and Technology of China\ngh051120@mail.ustc.edu.cn\nBei Liu\nMicrosoft Research Asia\nBei.Liu@microsoft.com\nYutong Lu‚Ä†\nSun Yat-sen University\nluyutong@mail.sysu.edu.cn\nABSTRACT\nWe study the joint learning of image-to-text and text-to-image\ngenerations, which are naturally bi-directional tasks. Typical exist-\ning works design two separate task-specific models for each task,\nwhich impose expensive design efforts. In this work, we propose\na unified image-and-text generative framework based on a single\nmultimodal model to jointly study the bi-directional tasks. We adopt\nTransformer as our unified architecture for its strong performance\nand task-agnostic design. Specifically, we formulate both tasks as\nsequence generation tasks, where we represent images and text\nas unified sequences of tokens, and the Transformer learns mul-\ntimodal interactions to generate sequences. We further propose\ntwo-level granularity feature representations and sequence-level\ntraining to improve the Transformer-based unified framework. Ex-\nperiments show that our approach significantly improves previous\nTransformer-based model X-LXMERT‚Äôs FID from 37.0 to 29.9 (lower\nis better) for text-to-image generation, and improves CIDEr-D score\nfrom 100.9% to 122.6% for fine-tuned image-to-text generation on\nthe MS-COCO dataset. Our code is available online.\nCCS CONCEPTS\n‚Ä¢ Computing methodologies ‚ÜíNatural language generation ; Im-\nage processing .\nKEYWORDS\ncross-modal generation; image captioning; text-to-image synthesis\nACM Reference Format:\nYupan Huang, Hongwei Xue, Bei Liu, and Yutong Lu. 2021. Unifying Multi-\nmodal Transformer for Bi-directional Image and Text Generation. In Pro-\nceedings of the 29th ACM International Conference on Multimedia (MM ‚Äô21),\nOctober 20‚Äì24, 2021, Virtual Event, China. ACM, New York, NY, USA, 10 pages.\nhttps://doi.org/10.1145/3474085.3481540\n‚àóWork done during an internship at Microsoft Research Asia.\n‚Ä†Corresponding Author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nMM ‚Äô21, October 20‚Äì24, 2021, Virtual Event, China\n¬© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8651-7/21/10. . . $15.00\nhttps://doi.org/10.1145/3474085.3481540\n(1) Task-specific uni-directional architectures\n(2) Task-agnostic bi-directional architectures\nI LSTM\nDecoder T\n(a) I ‚Üí T (e.g., BUTD) (b) T ‚Üí I (e.g., DM-GAN)\n(c) I ‚Üê‚Üí T (e.g., X-LXMERT-FT) (d) I ‚Üê‚Üí T (Ours)\nCNN\nEncoder T CNN\nGenerator ILTSM \nEncoder\nI Transformer\nEncoder T\nT ITransformer\nEncoder\nTransformer\nEncoder CNN\nGenerator\nCNN\nGenerator\nI\nT\nT\nI\nFigure 1: Comparison with existing works [2, 6, 53] on\nbi-directional image and text generation tasks. Our task-\nagnostic bi-directional architecture as show in (2) releases\ndesign efforts of task-specific architectures in (1). When\ncomparing with other Transformer-based models in a pre-\ntrain and fine-tune fashion as shown in (c), our model uni-\nfies two tasks in a Transformer model. ‚ÄúX-LXMERT-FT‚Äù de-\nnotes fine-tuning X-LXEMRT [6] for two tasks. ‚ÄúI‚Äù and ‚ÄúT‚Äù\ndenote ‚Äúimage‚Äù and ‚Äútext‚Äù respectively.\n1 INTRODUCTION\nAs a result of developing multimodal processing technology, net-\nworks, and devices, the number of existing multimodal industrial\napplications (especially mobile applications) is rapidly increasing.\nThis trend has encouraged the development of techniques to sup-\nport multimodal interaction in a unified system. Bi-directional im-\nage and text generation is a technique that automatically translates\nacross real scene images and natural language descriptions. It has\na broad range of industrial applications. For example, a product\ndescription-and-picture generation system is an important appli-\ncation for consumers to search and preview products. Specifically,\nconsumers could easily get a text query when the system receives\na picture of one or several products. Meanwhile, the users could\nsay ‚Äúshow me the picture of a room with wooden furniture and\npurple curtains. ‚Äù and receive an illustrating picture by the system\nto preview this collocation. As these applications are equipped in\nmobile devices, unifying multimodal interactions in a single model\nwill be a better choice to optimize storage utilization compared to\ntwo separate models.\narXiv:2110.09753v1  [cs.CV]  19 Oct 2021\nDespite the great benefit of a unified framework for image and\ntext generation, the bi-directional generation tasks are conducted\nseparately with task-specific architectures traditionally. As shown\nin Figure 1 (1), a typical image-to-text generator consists of a visual\nencoder (e.g., CNN) to embed visual information, and a caption\ndecoder (e.g., LSTM) to generate captions [2]; while predominant\ntext-to-image generators adopt Generative Adversarial Nets (GANs)\n[11] framework based on CNN architectures [53]. To support mul-\ntimodal interaction, Huang et al. jointly trains an LSTM-based\nimage-to-text generator and a GAN-based text-to-image generator\nin a framework [17]. However, task-specific architectures are still\nneeded, which introduces expensive design efforts.\nTo alleviate above hassles, in this paper, we propose to unify\nimage-to-text and text-to-image generation tasks in one framework.\nIn this framework, we adopt Transformer-based architecture since\nit supports simple and task-agnostic designs, and exhibits strong\nperformance in image or text generative models [27, 36]. We formu-\nlate both tasks as sequence generation tasks, where an image and a\ntext are represented as sequences of tokens, and the model learns\nto predict target tokens conditioned on other ground-truth tokens\nwith a cross-entropy loss training. Existing Transformer-based text-\nto-image generation works [6, 9, 36] can be extended to support\nimage-to-text generation by exchanging the order of text and im-\nage tokens in their input sequences. While they have shown some\ninitial promise, these approaches still exhibit two major challenges\nfor bi-directional generations: information loss caused by the fea-\nture discretization process, and error accumulation caused by the\ncross-entropy loss training. Specifically, first, Transformer-based\napproaches enable image generation by clustering dense image\nfeatures into discrete indices as the labels of image tokens [6, 9, 36],\nHowever, this discretization process is harmful to image-to-text\ngeneration due to information loss. Second, the cross-entropy loss\ntraining in a ‚Äúteacher-forcing‚Äù manner creates a mismatch between\ntraining and testing, as the model is only exposed to the training\ndata distribution instead of its own prediction. This ‚Äúexposure bias‚Äù\nresults in error accumulation at test time [37]. Due to these chal-\nlenges, the typical Transformer-based approach, X-LXMERT [6],\ngenerates captions weakly correlated with their source images, and\neven its text-to-image generation results on automatic metric are\nworse than its comparing GAN-based approach [53].\nWe address these challenges with two major designs, i.e., two-\nlevel granularity feature representations and sequence-level train-\ning. First, we introduce two-level granularity feature representa-\ntions, in which we use dense features to reduce information loss for\nimage-to-text generation, and discrete features to enable text-to-\nimage generation. Second, we propose a training strategy that opti-\nmizes our model based on its sequence-level prediction instead of\ntoken-level predictions to bridge the gap between training and test-\ning. Based on this strategy, we particularly introduce a CLIP-based\nimage-level loss for text-to-image generation, which improves the\nconsistency between generated images and the source text by lever-\naging a large-scale pre-trained multimodal model CLIP [35]. More-\nover, because CLIP learns from a vast amount of image-text pairs on\nthe internet and releases task-specific crowd-sourced labeling, we\nnaturally propose a CLIP-based metric for text-to-image evaluation.\nWe share most of the Transformer networks for image and text\ngeneration tasks and train them iteratively as depicted in Figure 1\n(2). This paradigm facilitates image-and-text shared embedding\nlearning, which improves the performance with a half of model\nsize when compared with two separate Transformer models. Com-\npared with a previous Transformer-based approach, X-LXMERT,\nour approach significantly improves CLIPScore from 72.9 to 77.2\nfor text-to-image generation, and improves CIDEr-D score from\n100.9% to 122.6% for image-to-text generation on the MS-COCO\ndataset. In summary, our contributions are three-fold:\n‚Ä¢We present a unified image-and-text generative framework\nbased on a Transformer model with two proposals: (1) two-\nlevel granularity feature representation to avoid information\nloss ; (2) sequence-level training to mitigate the gap between\ntraining and testing.\n‚Ä¢We leverage the powerful pre-trained model CLIP to improve\ntext-to-image generation consistency, and to facilitate its\nevaluation without extra crowd-sourced labeling.\n‚Ä¢We conduct automatic and human evaluations that demon-\nstrates our approach significantly improves the quality of\nboth tasks over prior approaches on the MS-COCO dataset.\n2 RELATED WORK\n2.1 Image-to-Text Generation\nThe task of image captioning has attracted increasing attention[2,\n7, 12, 16, 26, 27, 31, 40, 48‚Äì50, 52]. The dominant image captioning\nmodels adopt encoder-decoder architectures, which embed images\nwith a CNN encoder, and generate captions with an RNN [2, 40] or\nTransformer decoder [7, 45]. By connecting the encoder and decoder\nwith attention mechanisms, the decoder can selectively integrate\nrelevant information for word generation. Transformer has shown\nup prominently thanks to its capability in parallel training and\nmodeling global dependency relying entirely on an attention mech-\nanism [45]. Transformer-based large-scale vision-and-language pre-\ntraining works over numerous image-text pairs have shown great\nadvances for image captioning [27, 52].\n2.2 Text-to-Image Generation\nSynthesizing images from text descriptions continues to be challeng-\ning. Deep generative models based on pixelCNN [44], approximate\nLangevin sampling [32], variational autoencoders [24], and Gener-\native Adversarial Nets (GANs) [11] have been proposed. Dominant\nworks adopt the GAN framework, which usually consists of a gener-\nator and a discriminator to solve a min-max optimization problem,\nand has shown effectiveness in image generations [38, 47, 51, 53].\nTypical works generate images with multiple stages, that they firstly\nsample low-resolution images, then they gradually upsample and\nimprove images in later stages [38, 47, 51, 53]. Recent large-scale\nTransformer-based pre-trained works impressed us with their high\nfidelity images [9, 36]. DALL-E and CogView greatly advance the\nquality of text-to-image generation by training transformers with\n12-billion and 4-billion parameters on 250 million and 30 million\nimage-text pairs respectively.\n2.3 Bi-directional Image-and-Text Generation\nImage-to-text and text-to-image generation are bi-directional tasks.\nHuang et al. [17] propose to jointly train an image-to-text generator\n‚Äúa kite is flying in \nthe air at a park.‚Äù\nDiscrete \nfeature\nMultimodal \nTransformer\n(b) Unified Model(a) Image and Text Representations\nTokenization\nT      I \nI      TClustering CIDEr-D \nreward \noptimization\nCLIP-based loss\n(c.2) Sequence-level training\nImage \nGenerator\nDense \nfeature\n‚Äúa group of people \nflying a kite in a field.‚Äù\nInput \nImage\nInput Text\nInput \nText\nInput \nText\n(c.1) Token-level training\nCE \nloss\nTarget \nimage \ntokens\nI      T\nT      I \nTarget \ntext \ntokens\nCE \nloss\nT      I \nI      T\nGenerated Text\nGenerated \nImage\nFigure 2: Overview of our framework. We formulate image and text generation tasks as sequence generation tasks. (a) Image-\nand-text pairs are represented as unified token sequences. Specifically, images are represented as two-level granularity features\nwhere we use dense features to reduce information loss for I->T and discrete features to enable image tokenization for T->I.\n(b) The unified Transformer learns multimodal interactions to generate sequences. (c) In addition to the standard token-level\ntraining, we introduce sequence-level training strategy to mitigate the mismatch between training and testing. ‚ÄúI‚Äù and ‚ÄúT‚Äù\ndenote ‚Äúimage‚Äù and ‚Äútext‚Äù.\nbased on an LSTM network and a text-to-image generator based\non a GAN network to boost both generators by enforcing cycle-\nconsistency in turbo learning. MirrorGAN [34] focus on the text-to-\nimage generation task and uses an off-the-shelf captioning model to\nregularize the redescription of the generated images to be semantic\nconsistent with the original text. X-LXMERT [6] is a pre-trained\nmodel based on Transformer for text-to-image generation, visual\nquestion answering, and visual reasoning. Different from these\nworks that use separate bi-directional models, in this paper, we use\nthe same model for image and text generation tasks.\n3 MODEL\nIn this section, we introduce our unified multimodal framework and\nthe design of two-level granularity image representation. Figure 2\n(a, b) gives an overview of the model.\n3.1 Unified Multimodal Framework\nOur model mainly consists of a unified multimodal Transformer [45],\nwhich has a multi-layer architecture and each layer mainly consists\nof multi-head self-attention and position-wise fully connected feed-\nforward networks. We adopt LXMERT [ 43] as our Transformer-\nbased architecture following X-LXMERT [6] for a direct comparison,\nsince our proposal is mainly based on feature representation and\ntraining mechanism but not on the specific model. LXMERT is a\ncross-modality Transformer consisting of an object-relation en-\ncoder, a language encoder and a cross-modality encoder. We omit\nan exhaustive background description of the model architecture\nand refer readers to [6, 43, 45] for additional details.\nTo enable both image-to-text and text-to-image generation tasks,\nwe formulate both tasks as sequence generation tasks. Specifically,\nwe firstly pre-process image-and-text pairs into sequences of image\ntokens and text tokens. Then the Transformer accepts sequences\nwith masked tokens as input, and maps the input to a contextualized\nrepresentation via attention networks. Lastly, an image or text linear\ntransformation classifier projects the contextualized representation\nto predicted target tokens. In our model, we share the parameters\nof most Transformer modules, except for the last classifiers since\nimage and text vocabularies have different sizes and semantics. For\ntext-to-image generation, we use an additional GAN-based image\ngenerator to convert the size of 8 √ó8 image token predictions to a\n256 √ó256 resolution visible image.\nText representation is a combination of the position embedding\nand the word embedding, where the position refers to the index of\nthe word within the caption, and the word embedding is initialized\nfrom pre-trained models like BERT [8] or LXMERT [43].\n3.2 Two-level Granularity Image\nRepresentations\nImage representation is a combination of image features and po-\nsition features. For image features, we split an image into a se-\nquence of uniform grid-level patches, which are effective to learn\nvisual representations for vision-and-language tasks [6, 18, 19, 21].\nWe propose to use two-level granularity image features.(1) Fine-\ngrained dense features: We extract the grid features with a Faster\nR-CNN [39] object detector pre-trained on the Visual Genome\ndataset [25]. The dense grid features are used as visual inputs for\nimage-to-text generation to reduce the loss of image information.\n(2) Coarse-grained discrete features: We use discrete clustering\nfeatures of the dense features to construct the ground-truth labels\nof visual tokens following X-LXMERT [6]. The discretization pro-\ncess is critical in Transformer-based image generation methods\nsince it is hard to construct a large visual vocabulary for the diverse\nimage pixels or image features, while discretization helps reduce\nthe feature noise and vocabulary size [6, 36]. Specifically, we firstly\ncreate a visual vocabulary using K-mean clustering, approximate\nthe target visual features via a nearest neighbor search, and then\nobtain the cluster indices and discrete grid features for each image.\nNaturally, the position feature of each grid is a 2-D embedding of\nthe grid-level bounding box positions.\n4 TRAINING\nWe formulate the bi-directional image and text generation tasks\nas sequence generation tasks and iteratively train the Transformer.\nSequence models are usually trained using the cross-entropy loss\nin a ‚ÄúTeacher-Forcing‚Äù manner, where ground truth tokens are\ngiven in each step of training. Since the model is only exposed\nto the training data distribution instead of its own predictions, a\nmismatch between training and testing named ‚Äúexposure bias‚Äù is\ncreated and results in error accumulation at test time [37]. To tackle\nthis problem, we design a two-stage training strategy.\n‚Ä¢Stage 1. Token-level Training. This stage aims to gener-\nate fluent captions and realistic images by ‚Äúteacher-forcing‚Äù\ntraining on word level or grid level in each step for image-\nto-text or text-to-image generation respectively.\n‚Ä¢Stage 2. Sequence-level Training. This stage further op-\ntimizes the model with generated text or image sequence\nto bridge the gap between training and testing.\nFigure 2 (c) gives an overview of the training process.\n4.1 Image-to-Text Generation\n4.1.1 Stage 1. Word-level Training.In the first stage, we aim to train\nan image caption generator in a \"Teacher-Forcing\" manner similar\nto training a uni-directional autoregressive transformer. The model\nis trained to maximize the likelihood of the next ground-truth word\ngiven the previous ground-truth words and visual context using\nback-propagation. We denote parameters of Transformer model\nwith ùúÉ and minimize the following cross-entropy loss:\nùêøùëá\nùëáùëúùëò (ùúÉ)= ‚àí\nùêø‚àëÔ∏Å\nùë°=1\nlog ùëùùúÉ(yùëô |y1:ùëô‚àí1, X), (1)\nwhere X = x1:ùëÄ and Y = y1:ùêø are the sequences of ground truth\nimage and text tokens respectively.ùëÄ and ùêø are the sequence length\nfor image and text sequences respectively.\n4.1.2 Stage 2. Sentence-level Training. To alleviate the exposure\nbias, we adopt the reinforcement learning algorithm of Self-Critical\nSequence Training (SCST) [40] to directly optimize non-differentiable\nsentence-level metrics (e.g., CIDEr-D [46]). We minimize the nega-\ntive expected score function ùëü of CIDEr-D metric:\nùêøùëá\nùëÜùëíùëû(ùúÉ)= ‚àíEÀÜy1:ùêø‚àºùëùùúÉ [ùëü (ÀÜy1:ùêø)], (2)\nwhere ÀÜY = ÀÜy1:ùêø denotes the sampled text tokens.\n4.2 Text-to-Image Generation\n4.2.1 Stage 1. Grid-level Training. The training target is to max-\nimize the likelihood of the target visual tokens given the other\nground-truth visual tokens and text context, which is similar to\nthe training target of the first stage of image-to-text generation.\nHowever, text-to-image direction aims to predict multiple tokens\ninstead of one token at each step, by minimizing the cross-entropy\nloss overall ùëÄ‚Ä≤masked tokens:\nùêøùêº\nùëáùëúùëò (ùúÉ)= ‚àí\nùëÄ‚Ä≤\n‚àëÔ∏Å\nùëö=1\nlog ùëùùúÉ(xùëö |x\\ùëÄ‚Ä≤, Y), (3)\nwhere x\\ùëÄ‚Ä≤ denotes the sequence of tokens excluding the masked\ntokens. We follow X-LXMERT to use this training strategy to en-\nables a non-autoregressive sampling strategy, i.e., mask-predict-k\nstrategy [10]. In this way, only a few sampling steps (e.g., ùëò = 4)\nare needed to generate all visual tokens of an image, which enjoys\na faster inference speed for broader industrial applications.\n4.2.2 Stage 2. Image-level Training. Although the cross-entropy\nloss in grid-level training has shown initial promise, there remain\ntwo major issues. First, the loss imposes restrictive supervision on\neach generated image to regard one reference image as a ‚Äúgold\nlabel‚Äù. This violates the one-to-many property of text-to-image\nmapping, where a caption can correspond to many feasible images.\nSecond, the loss is based on image grid indices or features, which\nignores the relations across grids.\nTo tackle these issues, we propose to directly optimize the model\ntowards generating an image that is more semantic consistent with\nthe source caption instead of one reference image. To achieve this,\nwe leverage a large-scale pre-trained multimodal model CLIP [35]\nto score the image-text consistency. This is desirable since CLIP\nis a general model pre-trained on 400M image-text pairs from the\nweb and exhibits unparalleled zero-shot transferable ability in a\ngreat variety of classification benchmarks. DALLE has used CLIP to\nre-rank its generated images with CLIP-based scores as an offline\npost-process [36], while we are the first to leverage a CLIP-based\nloss to directly regularize the training for text-to-image generation.\nSpecifically, we extract image and text embedding from the CLIP\nand calculate their cosine similarity to obtain CLIP-based loss:\nùêøùêº\nùëêùëôùëñùëù(ùúÉ)= ‚àímax(ùëêùëúùë† (I(ÀÜX), T(Y)), 0), (4)\nwhere I(¬∑) and T(¬∑) are the image and text embedding extrac-\ntion networks of CLIP. Note that the image embedding is also a\nGumbel-Softmax approximation [20] to support model optimization\nwith back-propagation. Learning from CLIP has several potential\nstrengths. It releases the crowd-sourced labeling, and naturally\nconforms to the one-to-many property of text-to-image mapping\nthanks to learning from the vast amount of image-text pairs on\nthe internet. Moreover, it connects text with the whole images in-\nstead of image grids to consider the relations across grids, thus it\nencourages higher semantic consistency between images and text.\nWe also have experimented with a grid feature similarity loss, a\npixel-wise loss and a perceptual loss [22], but we do not observe\nmuch improvement in our preliminary experiments. Due to the\nhigh variance property of sampling results of non-autoregressive\nsampling strategy, we conduct this second stage training accompa-\nnying with the first stage training iteratively, since the first stage of\n‚Äúteacher-forcing‚Äù training can encourage the sampling coherency.\n5 EXPERIMENTS\n5.1 Experimental Setup\nMS-COCO Dataset [29]. We evaluate our proposed method on the\npopular MS-COCO dataset. It is collected using Amazon Mechani-\ncal Turk (AMT) with five annotated captions for each image. MS-\nCOCO dataset‚Äôs official splits include 82,783/40,504/40,775 images\nfor training/validation/testing set respectively. We train our model\non the train split and evaluate our model on the validation split\nby randomly sampling 30,000 images following most text-to-image\ngeneration works [6, 53]. For image-to-text generation (image cap-\ntioning), we follow most captioning works to evaluate our model\non the Karpathy test split, which is a subset of the validation set\nconsisting of 5,000 images. Our results on image captioning are not\nTable 1: Ablations on image-to-text (I2T) and text-to-image (T2I) tasks on MSCOCO test set. (1) Our unified single Transformer\nmodel exhibits comparable performance to task-specific Transformers with half of model parameters. (2) Our two-level gran-\nularity image representation inherits the advantages from both discrete features for T2I, and dense features for I2T. (3) Our\nproposed sequence-level training significantly improves both tasks. (4) Our proposed CLIP-based loss improves T2I by improv-\ning the consistency between generated images and the source text.\nModel Parameters ‚Üì\nImage-to-Text Generation Text-to-Image Generation\nB@4‚Üë M‚Üë R‚Üë C‚Üë S‚Üë CLIPScore‚Üë FID‚Üì R-precision (hard/easy)\nViLBERT‚Üë CLIP‚Üë\nOurs 228M 37.3 28.2 57.9 122.6 21.9 77.2 29.9 37.7 /59.2 40.7/69.9\nw/o unified architecture 456M 37.4 28.2 58.0 122.3 22.0 76.5 30.2 37.0/ 59.6 40.3/68.9\nw/o two-level features\nDense feature 228M 37.2 28.2 57.9 122.2 22.0 75.7 34.9 32.9/51.2 38.6/61.9\nDiscrete feature 228M 34.7 27.0 56.0 114.3 20.7 76.9 30.2 37.0/59.1 40.7/69.3\nw/o sequence-level training 228M 32.2 26.9 54.8 107.9 20.2 73.4 33.5 33.3/51.1 35.5/63.0\nw/o CLIP-based loss 228M 37.6 28.3 58.1 122.5 22.0 72.5 40.1 30.7/47.6 34.2/59.0\ndirectly comparable to other image captioning models since they\nare trained with a larger split (113,287 vs. 82,783) and are expected\nto score higher. Moreover, we use grid-based 8x8 features for a fair\ncomparison with X-LXMERT, while this feature is weaker than the\n100 region-based feature used by standard image captioning works.\nImplementation Details. Our code is available online1. For de-\ntails on model architecture , we initialize our model from the\npre-trained X-LXMERT model [6] for a direct comparison, which\nadopts the architecture of LXMERT [ 43] and is pre-trained with\nMS-COCO Captions [29], Visual Genome [25] and VQA [3] datasets.\nWe adopt an image generator consisting of convolutional layers and\ntrained with Generative Adversarial Networks (GAN) [11] method\nfollowing X-LXMERT. Also, we directly use the pre-trained image\ngenerator provided by X-LXMERT for a fair comparison2. We limit\nthe length of a text caption to ùêø = 17 tokens, and the grid size of\neach image is ùëÄ = 8 √ó8. We use a vocabulary of 30,522 tokens for\ntext words, and a vocabulary of 10,000 tokens for image grids.\nFor details on training, we use the AdamW [23] optimizer with\nbeta coefficients of 0.9 and 0.999, and a weight decay of 1e-2 fol-\nlowing X-LXMERT. Both image-to-text or text-to-image generation\ntasks take 100,000 iterations in the first or second stage training.\nFor the first stage training, we linearly warm up the learning rate\nfrom 0 to 5e-5 over the first 5% iterations, and cosine decay it in\nthe rest of training steps following X-LXMERT. Since the second\nstage of training is initialized from the first stage, we use a fixed\nsmaller learning rate of 1e-6. We train the first stage with a batch\nsize of 256, and the second stage of 160 empirically. We use a label\nsmoothing of 0.2 [42], and a gradient clipping threshold of 1.0. We\nadopt mixed-precision training to reduce memory cost and speed\nup the training procedure.\n5.2 Evaluation Criteria\nImage-to-Text Generation. We report five commonly-used auto-\nmatic evaluation metrics for the image captioning task: BLEU@N [33]\n(N=1,4), ROUGE-L [28], MEREOR [4], CIDEr-D [46] and SPICE [1],\nwhich are denoted by B@N, M, R, C, and S for abbreviation.\n1https://github.com/researchmm/generate-it\n2https://github.com/allenai/x-lxmert\nText-to-Image Generation. We evaluate text-to-image genera-\ntion from three aspects: (1)Fr√©chet Inception Distance (FID) [14]\ncompares the distribution of generated images with the distribu-\ntion of real images. A lower FID implies a closer distance between\ngenerated image distribution and real-world image distribution. (2)\nR-precision [47] ranks retrieval results to evaluate whether the\ngenerated image is well conditioned on the given text. We follow\nX-LXMERT to use two variants of R-precision to fully evaluate our\nmodel. The easy variant randomly samples negatives among the\ntest caption set. The hard variant swaps a word in a caption with\nanother word within the same category. To compute the image\nand text similarity for ranking, X-LXMERT uses ViLBERT-MT [30],\nan off-the-shelf multimodal network based on object-level and\nword-level representations. We propose to use CLIP-based repre-\nsentations, as a complementary R-precision evaluation metric from\nglobal image and text levels. (3) CLIPScore. Since the R-precision\ncannot directly reflect the individual image-and-text consistency,\nwe propose to use a CLIP-based score, which calculates the co-\nsine similarity between the image and text representations from\nCLIP [35], as a complementary metric for text-to-image evaluation.\nCLIP is a powerful multimodal pre-trained model to evaluate the\nimage-text consistency, where a CLIP-based metric (i.e., CLIPScore)\nhas been proposed by Jack et al. to evaluate image captioning mod-\nels without reference captions [13]. Instead, we extend CLIPScore\nto evaluate image generation models.\nWe do not use Inception score (IS) [41], since it overfits within\nthe context of text-to-image generation and can be manipulated to\nachieve much higher scores using simple tricks [5, 15].\n5.3 Ablation Studies\nWe conduct ablation studies to evaluate our designs. Except for\nthe ablated component, we use the same model, dataset and other\nimplementation details. Results are shown in Table 1.\nWhat is the benefit of unified architecture? We evaluate the im-\npact of training a bi-directional unified Transformer by comparing\nour model to two separate Transformer models for image-to-text\nand text-to-image generations. For text-to-image generation task,\nour unified model outperforms the separate model on most metrics.\nTable 2: Comparisons with existing approaches on MS-COCO test set. We generate images (or captions) for DM-GAN [53],\nX-LXMERT [6] with their released codes and models, and evaluate the images (or captions) in the same way as ours and X-\nLXMERT-FT for more direct comparison. For BUTD [2] and Turbo-RL [17], we report the numbers recorded in their published\npapers. ‚Äú-‚Äù indicates the detail is not reported. ‚ÄúN/A‚Äù is the abbreviation of ‚Äúnot applicable‚Äù. ‚ÄúI‚Äù, ‚ÄúT‚Äù, ‚ÄúArch‚Äù and ‚ÄúTF‚Äù denote\n‚Äúimage‚Äù, ‚Äútext‚Äù, ‚Äúarchitecture‚Äù and ‚ÄúTransformer‚Äù respectively.\nDirection Model\nImage-to-Text Generation Text-to-Image Generation\nArch B@4 ‚Üë M‚Üë R‚Üë C‚Üë S‚Üë Arch CLIPScore ‚Üë FID‚Üì R-precision (hard/easy)\nViLBERT‚Üë CLIP‚Üë\nT->I DM-GAN N/A CNN 72.3 35.9 32.7/58.4 34.4/69.1\nI->T BUTD ‚Ä† RNN 36.3 27.7 56.9 120.1 21.4 N/A\nI<‚Äì>T\nTurbo-RL‚Ä† RNN 31.6 21.9 49.8 74.8 17.5 CNN - - - -\nX-LXMERT TF 15.2 22.7 42.2 41.0 16.6 TF 72.9 37.0 33.1/49.8 36.3/60.1\nX-LXMERT-FT TF 30.2 25.8 53.3 100.9 19.1 TF 73.3 33.9 33.1/50.7 35.1/62.3\nOurs TF 37.3 28.2 57.9 122.6 21.9 TF 77.2 29.9 37.7/59.2 40.7/69.9\nNote that the models denoted with ‚Ä†are trained on 113,287 images, and are expected to score higher on image-to-text generation than other models trained on 82,783 images.\nThey exhibit comparable performance on image-to-text genera-\ntion task. Moreover, the parameter size of our unified model is a\nhalf of two separate models‚Äô parameter size. We expect smaller\nTransformer could further decrease the model size. Thus our model\nsignificantly benefits industrial application towards optimizing stor-\nage utilization.\nDo two-level granularity image features help? As we have in-\ntroduced in Section 3.2, the common practice for image-to-text\nor text-to-image generation is using images‚Äô original dense fea-\nture or their discrete form respectively. To conduct bi-directional\ngenerations in a unified model, we separately train three models\nwith inputs of dense feature, discrete feature and our two-level\ngranularity feature. Note that when taking dense feature as input,\nthe model still learns to predict discrete image tokens instead of\ndense features for text-to-image generation, otherwise the model\nwould fail to generate normal images validated by X-LXMERT [6].\nHowever, this creates a mismatch between training and testing.\nResults show that dense feature performs much worse than our\ntwo-level feature in text-to-image generation, despite its good per-\nformance in image-to-text generation. Contrarily, using only the\ndiscrete feature performs much better than dense feature in text-to-\nimage generation, however, not in image-to-text generation. Our\ndesign of two-level granularity feature inherits the advantages of\ndense features on image-to-text generation, and discrete features\non text-to-image generation, and thus performs well on both tasks.\nWhat is the impact of the sequence-level training? We show\nthat removing the sequence-level training and leaving a single\nstage of token-level training hurts performance significantly on\nboth tasks. For text-to-image generation, the CIDEr-D score drops\nfrom 122.6% to 107.9%; for image-to-text generation, the CLIPScore\ndrops from 77.2 to 73.4. The results illustrate the effectiveness of\nsequence-level training to mitigate the ‚Äúexposure bias‚Äù mismatch\nbetween training and testing.\nDo we need the CLIP-based loss? To verify the effect of our\nproposed CLIP-based loss for text-to-image generation in the image-\nlevel training, we replace the CLIP-based loss with a mean squared\nerror (squared L2 norm) loss based on grid feature similarity. From\nthe results, we see that it hurts performance significantly on all text-\nto-image evaluation metrics. The results highlight the effectiveness\nof CLIP-based loss to improve the semantic consistency between\ngenerated images and source text. The performance of image-to-\ntext generation is not influenced, demonstrating the robustness of\nour model.\n5.4 Quantitative Results\nWe compare our approach with typical published uni-directional ap-\nproaches with task-specific designs, and bi-directional approaches\nwith or without task-specific designs as followed. We choose models\nthat are mainly trained with MS-COCO dataset as ours.\n‚Ä¢DM-GAN [53] is a typical text-to-image generation model\nwith a GAN-based framework. It progressively generates\nimages from low-resolution to high-resolution synthesis,\nand adaptively refines images with a memory module.\n‚Ä¢BUTD [2] is a image-to-text generation model with an encoder-\ndecoder architecture based on CNN-LSTM networks. It uses\na combined bottom-up and top-down attention mechanism\nto enable attention on objects.\n‚Ä¢Turbo-RL [17] is a bi-directional image and text generation\nmodel. It is jointly trained with an LSTM network and a GAN\nnetwork to boost both generators by enforcing cycle con-\nsistency in turbo learning. Since the authors did not report\ntext-to-image generation results relevant to our metric, and\nwe cannot find released codes, their results are not reported.\n‚Ä¢X-LXMERT [6] is a vision-and-language pre-trained model\nfor text-to-image generation, visual question answering, and\nvisual reasoning tasks. X-LXMERT remains image captioning\ncapability to sample sentences by masking and predicting\nword tokens. As the authors did not report their image-to-\ntext results, we generate captions with a prefix word ‚ÄúA‚Äù as\nthey suggested by their released code and model.\n‚Ä¢X-LXMERT-FT is a model separately fine-tuned on image-\nto-text or text-to-image generation task from the released\npre-trained X-LXMERT model with standard cross-entropy\nloss training. This provides a direct comparison with our\nmodel on feature representation and training strategy.\n73%\n27%\n60%\n40%\n67%\n33%\n65%\n35%\n59%\n41%\n62%\n38%\nFidelity\nSemantic\nOurs DM-GAN X-LXMERT              w/o CLIP\nFigure 3: Human evaluation on text-to-image generation be-\ntween our model, DM-GAN [53], X-LXMERT [6] and our\nablated model trained without CLIP-based loss (denoted by\n‚Äúw/o CLIP‚Äù). Our model clearly outperforms others in both\naspects of fidelity and semantic of images.\nTable 2 provides comprehensive results. Across two tasks, our\nmodel achieves significant performance gains over comparing meth-\nods in all metrics. For task-specific uni-directional models, our\nmodel outperforms the typical image-to-text model BUTD by 2.5%\nCIDEr-D score, despite being trained with much fewer images. Our\nmodel also outperforms typical text-to-image model DM-GAN by\n4.9 and 6.0 on CLIPScore and FID respectively. The task-specific\nbi-directional model Turbo-RL achieves 74.8% CIDEr-D score on\nMSCOCO Karpathy test set, which is far from satisfactory. The\ninferior results achieved by task-specific works may indicate the\nlimitation of RNN caption decoder or CNN image generator.\nFor Transformer-based approaches, the pre-trainedX-LXMERT\nmodel achieves 41.0% in CIDEr-D score and 15.2% in BLEU@4, in-\ndicating the original model is not able to generate very accurate\nand fluent captions for MS-COCO dataset. This large gap may be\ndue to the mismatch between X-LXMERT‚Äôs pre-training dataset\n(MS-COCO [29], VG [25] and VQA [3]) and downstream dataset\n(MS-COCO), so we fine-tune X-LXMERT with the standard cross-\nentropy loss training denoted by X-LXMERT-FT for fair compar-\nisons. X-LXMERT-FT do obtain large gains from 41.0% to 100.9% in\nCIDEr-D metric for image-to-text generation task, and from 37.0\nto 33.9 in FID score for text-to-image generation task. However,\nthere is still a gap between X-LXMERT-FT and the task-specific\nmodels, and there is little gain over semantic consistency-aware\nmetrics CLIPScore and R-precision. Our model mitigates this gap\nby significantly improving CIDEr-D score from 100.9% to 122.6%.\nMoreover, our model surpasses X-LXMERT-FT by 3.9 points in\nCLIPScore, 4.6/8.5 points in VilBERT-based R-precision, and 5.6/7.6\npoints in CLIP-based R-precision. The results confirm the superi-\nority of our model, and validates that a unified Transformer could\nresult in similar breakthroughs in both image-to-text and text-to-\nimage generations.\n5.5 Human Evaluation\nTo better evaluate the quality of generated images, we conduct a\nhuman evaluation to compare our method with existing works and\nvisualize results in Figure 3. Specifically, we choose DMGAN [53]\nand X-LXMERT [6] for comparisons, which are the best-performing\nGAN-based and Transformer-based published works with released\nmodels, respectively. We also compare with our ablated model\nwithout CLIP-based loss (denoted by ‚Äúw/o CLIP‚Äù). We randomly\nsample 300 captions from MSCOCO test set and generate images\nfrom each caption by different models. During the evaluation, we\nprovide a caption and an image pair generated by our model and\nother models in random order. We invite ten volunteers who are\nproficient in English with over ten years of learning experiences\nand ask to select the one that (1) shows higher fidelity and (2) better\nmatches the source caption semantically. As we can see from Figure\n3, our model significantly surpasses DM-GAN, X-LXMERT and\n‚Äúw/o CLIP‚Äù by about 37%, 19% and 29% on an average of fidelity and\nsemantic scores. The results validate the superiority of our model.\n5.6 Qualitative Examples\nVisual inspection of generated images and captions in Figure 4\nconvincingly shows the large quality improvement. For text gener-\nation, our model performs comparable with BUTD, which is uni-\ndirectional approach. Compared with X-LXMERT, our model gener-\nates more accurate captions with better rationality. Some made-up\nobjects like ‚Äúzebra‚Äù and ‚Äúred shirt‚Äù do not appear in our generated\ncaptions. For image generation, our model generates images with\nmore realistic objects and higher consistency with texts. Our model\noutperforms DM-GAN by a large margin on fidelity. Compared with\nX-LXMERT and w/o CLIP, our model learns better alignment with\ntexts from CLIP-based loss (notice people on the beach, ship in the\nwater, kite in the air.). Compared with the discrete-feature-based\nmodel, our model generates more realistic and smooth images.\n6 CONCLUSION\nIn this work, we propose a unified multimodal Transformer for\nbi-directional image-and-text generation tasks. Our proposal al-\nleviates the expensive design efforts of task-specific models, and\noptimizes storage utilization compared to the design of two sep-\narate models for bi-directional tasks. To tackle the challenges of\nTransformer-based image-and-text generative models, we design a\ntwo-level granularity feature representation and a sequence-level\ntraining strategy. The two-level granularity feature representation\naddresses the information loss issue caused by feature discretiza-\ntion process. The sequence-level training strategy address the error\naccumulation in test time caused by the cross-entropy training.\nSufficient qualitative and quantitative experiments have shown the\neffectiveness of our approach. With these benefits, our model could\nfacilitate industrial applications for multimodal interactions with\ndesirable performance.\n7 ACKNOWLEDGEMENT\nThis work was supported by the NSFC (U1811461), the Guangdong\nNatural Science Foundation (2018B030312002), and the Program\nfor Guangdong Introducing Innovative and Entrepreneurial Teams\nunder Grant NO.2016ZT06D211.\nWe would like to acknowledge Yiheng Xu and Zhicheng Huang\nfor the helpful discussions.\nGT (human)\nsome people\nwho are walking \non the beach.\na ship in the \nwater sailing \npast the city in \nthe background.\na tall massive \nclock tower\ntowering over \na city.\na vegetarian\npizza is half \neaten on a pizza \nholder.\nwoman cross \ncountry skiing \nalone on a trail \nin the woods.\na kite is flying \nin the air at a \npark.\na man with a \nsnowboard \nnext to a \nman with a \nmask.\nImage-\nto-text\na man sitting on \na beach with a \nbottle of beer.\na boat is sitting \nin the water in a \ncity.\na black and \nwhite photo of \na building with \na clock tower.\na pizza sitting \non top of a \ncutting board.\na woman\nriding skis \ndown a snow \ncovered slope.\na man flying a \nkite in a field.\na man and a \nwoman \nholding a \nsnowboard.\nText-to-Image\nDM-GANOurs w/o CLIP\nText-to-Image and Image-to-Text\nX-LXMERTOurs-DenseOurs\na man with a \nbird on his arm \nand a bird on \nthe beach and a \nzebra.\na large city\nwith many \nbuildings in the \nbackground \nand a large city \nwith a sailboat.\na clock tower\nwith a clock \non it and a \nbuilding on \nthe side of the \nstreet.\na pizza with a \ngreen pepper \non it and a \npepperoni on \ntop of a table.\na person is \nskiing down a \nhill in the \nsnow while a\nskier is on the \nsnow.\na man in a red\nshirt is \nstanding in the \ngrass with a \nkite in the air.\na man in a \nblack jacket \nholding a \nsurfboard while \na woman stands \nnext to him.\na man sitting \nunder a tree \nnext to a \nsurfboard.\na boat in a large \nbody of water\nwith tall \nbuildings.\na large \nbuilding with \na clock tower\non top of it.\na pizza sitting \non top of a \nwooden plate.\na woman\nriding skis \ndown a snow \ncovered slope.\na person flying \na kite in a field.\na man and a \nwoman \nholding a \nsnowboard.\na man sitting on \na beach next to a \nbird.\na boat in the \nmiddle of a \nbody of water.\na black and \nwhite photo of \na clock tower\nin a city.\ntwo slices of \npizza on a \nwooden cutting \nboard.\na person riding \nskis down a \nsnow covered \nslope.\na group of \npeople flying a \nkite in a field.\na man and a \nwoman \nholding a \nsnowboard.\nBUTD\nSource image and text\nFigure 4: Examples of text-to-image and image-to-text generation results generated by different models from human written\ncaptions or real images (human ground-truths in blue box). Right and wrong expressions in captions are highlighted in green\nand red respectively. Our model generates the most accurate and real images and text when compared with others.\nREFERENCES\n[1] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016.\nSpice: Semantic propositional image caption evaluation. In European Conference\non Computer Vision . 382‚Äì398.\n[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson,\nStephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In Proceedings of the IEEE\nconference on computer vision and pattern recognition . 6077‚Äì6086.\n[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,\nC Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In\nProceedings of the IEEE international conference on computer vision . 2425‚Äì2433.\n[4] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for\nMT evaluation with improved correlation with human judgments. In Proceedings\nof the acl workshop on intrinsic and extrinsic evaluation measures for machine\ntranslation and/or summarization . 65‚Äì72.\n[5] Shane Barratt and Rishi Sharma. 2018. A note on the inception score. ICML 2018\nWorkshop on Theoretical Foundations and Applications of Deep Generative Models\n(2018).\n[6] Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, and Aniruddha\nKembhavi. 2020. X-LXMERT: Paint, Caption and Answer Questions with Multi-\nModal Transformers. In EMNLP. 8785‚Äì8805.\n[7] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2020. M\n2: Meshed-Memory Transformer for Image Captioning. In CVPR.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers) . 4171‚Äì4186.\n[9] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin,\nJunyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. 2021. CogView:\nMastering Text-to-Image Generation via Transformers.\n[10] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019.\nMask-Predict: Parallel Decoding of Conditional Masked Language Models. In\nEMNLP. 6114‚Äì6123.\n[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial\nNets. In Advances in Neural Information Processing Systems , Vol. 27.\n[12] Longteng Guo, Jing Liu, Jinhui Tang, Jiangwei Li, Wei Luo, and Hanqing Lu. 2019.\nAligning Linguistic Words and Visual Semantic Units for Image Captioning. In\nProceedings of the 27th ACM International Conference on Multimedia . 765‚Äì773.\n[13] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021.\nCLIPScore: A Reference-free Evaluation Metric for Image Captioning. arXiv\npreprint arXiv:2104.08718 (2021).\n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and\nSepp Hochreiter. 2017. GANs Trained by a Two Time-Scale Update Rule Converge\nto a Local Nash Equilibrium. InAdvances in Neural Information Processing Systems ,\nVol. 30.\n[15] Tobias Hinz, Stefan Heinrich, and Stefan Wermter. 2020. Semantic Object Ac-\ncuracy for Generative Text-to-Image Synthesis. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (2020).\n[16] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. 2019. Attention on\nattention for image captioning. InProceedings of the IEEE International Conference\non Computer Vision . 4634‚Äì4643.\n[17] Qiuyuan Huang, Pengchuan Zhang, Dapeng Wu, and Lei Zhang. 2018. Turbo\nlearning for CaptionBot and DrawingBot. In Proceedings of the 32nd International\nConference on Neural Information Processing Systems . 6456‚Äì6466.\n[18] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and\nJianlong Fu. 2021. Seeing Out of tHe bOx: End-to-End Pre-training for Vision-\nLanguage Representation Learning. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) .\n[19] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. 2020.\nPixel-bert: Aligning image pixels with text by deep multi-modal transformers.\narXiv preprint arXiv:2004.00849 (2020).\n[20] Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization\nwith gumbel-softmax. arXiv preprint arXiv:1611.01144 (2016).\n[21] Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei\nChen. 2020. In defense of grid features for visual question answering. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n10267‚Äì10276.\n[22] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual losses for real-\ntime style transfer and super-resolution. In European conference on computer\nvision. 694‚Äì711.\n[23] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-\nmization. In International Conference on Learning Representations .\n[24] Diederik P Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In\nICLR.\n[25] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua\nKravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al .\n2017. Visual genome: Connecting language and vision using crowdsourced dense\nimage annotations. International Journal of Computer Vision 123, 1 (2017), 32‚Äì73.\n[26] Guang Li, Linchao Zhu, Ping Liu, and Yi Yang. 2019. Entangled Transformer\nfor Image Captioning. In Proceedings of the IEEE International Conference on\nComputer Vision . 8928‚Äì8937.\n[27] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Li-\njuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020. Oscar: Object-semantics\naligned pre-training for vision-language tasks. In European Conference on Com-\nputer Vision . Springer, 121‚Äì137.\n[28] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In\nText Summarization Branches Out: Proceedings of the Association for Computational\nLinguistics Workshop. 74‚Äì81.\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr Doll√°r, and C Lawrence Zitnick. 2014. Microsoft coco: Common\nobjects in context. In European conference on computer vision . Springer, 740‚Äì755.\n[30] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining\ntask-agnostic visiolinguistic representations for vision-and-language tasks. In\nAdvances in Neural Information Processing Systems . 13‚Äì23.\n[31] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2018. Neural baby talk.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition .\n7219‚Äì7228.\n[32] Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski.\n2017. Plug & play generative networks: Conditional iterative generation of\nimages in latent space. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition . 4467‚Äì4477.\n[33] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a\nmethod for automatic evaluation of machine translation. In Proceedings of the\n40th annual meeting on association for computational linguistics . 311‚Äì318.\n[34] Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao. 2019. Mirror-\ngan: Learning text-to-image generation by redescription. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 1505‚Äì1514.\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from natural language supervision.\narXiv preprint arXiv:2103.00020 (2021).\n[36] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec\nRadford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation.\narXiv preprint arXiv:2102.12092 (2021).\n[37] Marc‚ÄôAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016.\nSequence Level Training with Recurrent Neural Networks. In ICLR.\n[38] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele,\nand Honglak Lee. 2016. Generative adversarial text to image synthesis. In Inter-\nnational Conference on Machine Learning . PMLR, 1060‚Äì1069.\n[39] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:\nTowards real-time object detection with region proposal networks. In Advances\nin neural information processing systems . 91‚Äì99.\n[40] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava\nGoel. 2017. Self-critical sequence training for image captioning. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition . 7008‚Äì7024.\n[41] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi\nChen, and Xi Chen. 2016. Improved Techniques for Training GANs. In Advances\nin Neural Information Processing Systems , Vol. 29.\n[42] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew\nWojna. 2016. Rethinking the inception architecture for computer vision. In\nProceedings of the IEEE conference on computer vision and pattern recognition .\n2818‚Äì2826.\n[43] Hao Tan and Mohit Bansal. 2019. Lxmert: Learning cross-modality encoder\nrepresentations from transformers. In EMNLP.\n[44] A√§ron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu, Oriol\nVinyals, and Alex Graves. 2016. Conditional Image Generation with PixelCNN\nDecoders. In NIPS.\n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998‚Äì6008.\n[46] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider:\nConsensus-based image description evaluation. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition . 4566‚Äì4575.\n[47] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang,\nand Xiaodong He. 2018. Attngan: Fine-grained text to image generation with\nattentional generative adversarial networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition . 1316‚Äì1324.\n[48] Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai. 2019. Auto-encoding\nscene graphs for image captioning. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition . 10685‚Äì10694.\n[49] Xu Yang, Hanwang Zhang, and Jianfei Cai. 2019. Learning to collocate neural\nmodules for image captioning. In Proceedings of the IEEE International Conference\non Computer Vision . 4250‚Äì4260.\n[50] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. 2018. Exploring visual relationship\nfor image captioning. InProceedings of the European conference on computer vision\n(ECCV). 684‚Äì699.\n[51] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei\nHuang, and Dimitris N Metaxas. 2017. Stackgan: Text to photo-realistic image\nsynthesis with stacked generative adversarial networks. In Proceedings of the\nIEEE international conference on computer vision . 5907‚Äì5915.\n[52] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng\nGao. 2020. Unified vision-language pre-training for image captioning and vqa. In\nProceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 13041‚Äì13049.\n[53] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. 2019. Dm-gan: Dynamic mem-\nory generative adversarial networks for text-to-image synthesis. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition ."
}