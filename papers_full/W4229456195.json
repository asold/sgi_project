{
  "title": "A Data Cartography based MixUp for Pre-trained Language Models",
  "url": "https://openalex.org/W4229456195",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5054327489",
      "name": "Seo Yeon Park",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5089085275",
      "name": "Cornelia Caragea",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2626967530",
    "https://openalex.org/W2970121940",
    "https://openalex.org/W4300833946",
    "https://openalex.org/W3175116353",
    "https://openalex.org/W3115242847",
    "https://openalex.org/W3099142828",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2889326796",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4301183982",
    "https://openalex.org/W3035542229",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963508788",
    "https://openalex.org/W3103649165"
  ],
  "abstract": "MixUp is a data augmentation strategy where additional samples are generated during training by combining random pairs of training samples and their labels. However, selecting random pairs is not potentially an optimal choice. In this work, we propose TDMixUp, a novel MixUp strategy that leverages Training Dynamics and allows more informative samples to be combined for generating new data samples. Our proposed TDMixUp first measures confidence, variability, (Swayamdipta et al., 2020), and Area Under the Margin (AUM) (Pleiss et al., 2020) to identify the characteristics of training samples (e.g., as easy-to-learn or ambiguous samples), and then interpolates these characterized samples. We empirically validate that our method not only achieves competitive performance using a smaller subset of the training data compared with strong baselines, but also yields lower expected calibration error on the pre-trained language model, BERT, on both in-domain and out-of-domain settings in a wide range of NLP tasks. We publicly release our code.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4244 - 4250\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nA Data Cartography based MixUp for Pre-trained Language Models\nSeo Yeon Park and Cornelia Caragea\nComputer Science\nUniversity of Illinois Chicago\nspark313@uic.edu cornelia@uic.edu\nAbstract\nMixUp is a data augmentation strategy where\nadditional samples are generated during train-\ning by combining random pairs of training\nsamples and their labels. However, select-\ning random pairs is not potentially an opti-\nmal choice. In this work, we propose TD-\nMixUp, a novel MixUp strategy that lever-\nages Training Dynamics and allows more in-\nformative samples to be combined for gener-\nating new data samples. Our proposed TD-\nMixUp first measures confidence, variability,\n(Swayamdipta et al., 2020), and Area Under\nthe Margin (AUM) (Pleiss et al., 2020) to iden-\ntify the characteristics of training samples (e.g.,\nas easy-to-learn or ambiguous samples), and\nthen interpolates these characterized samples.\nWe empirically validate that our method not\nonly achieves competitive performance using\na smaller subset of the training data compared\nwith strong baselines, but also yields lower ex-\npected calibration error on the pre-trained lan-\nguage model, BERT, on both in-domain and\nout-of-domain settings in a wide range of NLP\ntasks. We publicly release our code.1\n1 Introduction\nMixUp (Zhang et al., 2018) is a simple data aug-\nmentation strategy in which additional samples are\ngenerated during training by combining random\npairs of training samples and their labels. While\nsimple to implement, MixUp has been shown to\nimprove both predictive performance and model\ncalibration (i.e., avoiding over-confident predic-\ntions) (Guo et al., 2017) due to its regularization\neffect through data augmentation (Thulasidasan\net al., 2019). However, selecting random pairsin\nMixUp might not necessarily be optimal.\nDespite this, MixUp has been explored for NLP\ntasks with substantial success using hidden state\nrepresentations (Verma et al., 2019). For instance,\nSun et al. (2020) explored MixUp, which uses the\n1https://github.com/seoyeon-p/TDMixUp\nhidden representation of BERT (Devlin et al., 2019)\nto synthesize additional samples from randomly se-\nlected pairs. Yin et al. (2021) proposed MixUp,\nwhich uses the hidden representation of RoBERTa\n(Liu et al., 2019) to interpolate all samples in the\nsame mini-batch to better cover the feature space.\nTo date, only a few prior works have focused on se-\nlecting informative samples for MixUp. For exam-\nple, Chen et al. (2020) proposed semi-supervised\nlearning, which interpolates labeled and unlabeled\ndata based on entropy. Kong et al. (2020) explored\nBERT calibration with MixUp, which generates\nnew samples by exploiting the distance between\nsamples in the feature space.\nRecently, Swayamdipta et al. (2020) introduced\ndata maps, which allow evaluating data quality by\nusing training dynamics (i.e., the behavior of a\nmodel as training progresses). Specifically, they\nconsider the mean and standard deviation of the\ngold label probabilities, predicted for each sample\nacross training epochs (i.e., confidence and vari-\nability), and characterize data into three different\ncategories: (1) samples that the model predicts cor-\nrectly and consistently (i.e., easy-to-learn); (2)\nsamples where true class probabilities vary fre-\nquently during training (i.e., ambiguous); and (3)\nsamples that are potentially mis-labeled or erro-\nneous (i.e., hard-to-learn). The author revealed\nthat the easy-to-learn samples are useful for model\noptimization (parameter estimation) and without\nsuch samples the training could potentially fail to\nconverge, while the ambiguous samples are those\non which the model struggles the most and push the\nmodel to become more robust, hence, these ambigu-\nous samples are the most beneficial for learning\nsince they are the most challenging for the model.\nInspired by these observations, we propose a\nnovel MixUp strategy which we call TDMixUp\nthat monitors training dynamics and interpolates\neasy-to-learn samples with ambiguous samples in\nthe feature space. That is, we pair one sample from\n4244\nthe easy-to-learn set with another sample from the\nambiguous set to allow more informative samples\nto be combined for MixUp. Accordingly, we gen-\nerate new samples that share the characteristics\nof both easy-to-learn and ambiguous data samples\nand are hence more beneficial for learning. How-\never, the easy-to-learn and the ambiguous sets can\ncontain mis-labeled samples that can degrade the\nmodel performance. Consequently, we measure\nanother training dynamic, Area Under the Margin\n(AUM) (Pleiss et al., 2020), to filter out possibly\nmis-labeled samples in each set. We validate our\nproposed method on a wide range of natural lan-\nguage understanding tasks including textual entail-\nment, paraphrase detection, and commonsense rea-\nsoning tasks. We achieve competitive accuracy and\nlow expected calibration error (Guo et al., 2017) on\nboth in-domain and out-of-domain settings for the\npre-trained language model BERT (Devlin et al.,\n2019), without using the full training data.\n2 Proposed Approach: TDMixUp\nWe introduce our proposed TDMixUp, which gen-\nerates additional samples based on the characteris-\ntics of the data samples. We first reveal the char-\nacteristics of each data sample by using training\ndynamics, i.e., confidence, variability, and Area\nUnder the Margin (AUM). We then describe our\nMixUp operation that combines training samples\nbased on the above data characteristics that are\nmeasured during training.\n2.1 Data Samples Characterization\nWe first introduce confidence and variability, that\nare used to evaluate the characteristics of each indi-\nvidual sample (Swayamdipta et al., 2020). These\nstatistics are calculated for each sample (xi,yi)\nover Etraining epochs.\nConfidence We define confidence as the mean\nmodel probability of the true labelyi across epochs:\nˆµi = 1\nE\nE∑\ne=1\npθ(e) (yi|xi) (1)\nwhere pθ(e) denotes the model’s probability with\nparameter θ(e) at the end of eth epoch.\nVariability We define variability as the standard\ndeviation of pθ(e) across epochs E:\nˆσi =\n ∑E\ne=1(pθ(e) (yi|xi) −ˆµi)2\nE (2)\nGiven these statistics per sample, we identify the\ntop 33% easy-to-learn samples, i.e., those samples\nthat the model predicts correctly and consistently\nacross epochs (high-confidence, low-variability),\nand the top 33% ambiguous samples, i.e., those\nsamples whose true class probabilities have a high\nvariance during training (high-variability).\nArea Under the Margin (AUM) As another\nmeasure of data quality, we monitor training dy-\nnamics using the Area Under the Margin (AUM)\n(Pleiss et al., 2020). AUM measures how different\na true label for a sample is compared to a model’s\nbelief at each epoch and is calculated as the aver-\nage difference between the logit values for a sam-\nple’s assigned class (gold label) and its highest non-\nassigned class across training epochs. Formally,\ngiven a sample (xi,yi), we compute AUM(xi,yi)\nas the area under the margin averaged across all\ntraining epochs E. Specifically, at some epoch\ne∈E, the margin is defined as:\nMe(xi,yi) =zyi −maxyi!=k(zk) (3)\nwhere Me(xi,yi) is the margin of sample xi with\ntrue label yi, zyi is the logit corresponding to the\ntrue label yi, and maxyi!=k(zk) is the largest other\nlogit corresponding to label knot equal to yi. The\nAUM of (xi,yi) across all epochs is:\nAUM(xi,yi) = 1\nE\nE∑\ne=1\nMe(xi,yi) (4)\nIntuitively, while both AUM and confidence mea-\nsure training dynamics, confidence simply mea-\nsures the probability output of the gold label and\nhow much it fluctuates over the training epochs. In\ncontrast, AUM measures the probability output of\nthe gold label with respect to the model’s belief in\nwhat the label for a sample should be according\nto its generalization capability (derived by observ-\ning other similar samples during training). More\nprecisely, AUM considers each logit value and mea-\nsures how much the gold label assigned logit value\ndiffers from the other largestlogit value, which\nallows identifying mis-labeled samples.\nTo identify possibly mis-labeled data in each set\n(i.e., the set of easy-to-learn and the set of ambigu-\nous samples that are categorized by confidence and\nvariability as described above), we first fine-tune\na model on each set, respectively, with inserting\nfake data (i.e., threshold samples). Data with simi-\nlar or worse AUMs than threshold samples can be\n4245\nassumed to be mis-labeled (Pleiss et al., 2020). We\nconstruct threshold samples by taking a subset of\nthe training data and re-assigning their labels ran-\ndomly, including a class that does not really exist.\nSpecifically, given N training samples that belong\nto cclasses, we randomly select N/(c+1) samples\nper class and re-assign their labels to classes that\nare different from the original class. We then train a\nmodel on training samples including threshold sam-\nples and measure the AUMs of all training data. We\nidentify possible mis-labeled data by computing a\nthreshold value (i.e., the kth percentile threshold\nsample AUMs wherekis a hyper-parameter chosen\non the validation set). At last, we filter out samples\nthat have lower AUM than the threshold value.\n2.2 MixUp\nMixUp training generates vicinity training samples\naccording to the rule introduced in Zhang et al.\n(2018):\n˜x= λxi + (1−λ)xj\n˜y= λyi + (1−λ)yj\n(5)\nwhere xi and xj are two randomly sampled input\npoints, yi and yj are their associated one-hot en-\ncoded labels, and λis a mixing ratio sampled from\na Beta(α, α) distribution with a hyper-parameter\nα. In standard MixUp, training data is augmented\nby linearly interpolating random training samples\nin the input space. In contrast, our proposed TD-\nMixUp interpolates one easy-to-learn sample with\none ambiguous sample after applying AUM to filter\npotential erroneous samples that harm performance.\nOur current implementation uses easy-to-learn and\nambiguous data loaders respectively and then ap-\nplies MixUp to a randomly sampled mini-batch of\neach loader. We train a model on the generated TD-\nMixUp samples in addition to the easy-to-learn and\nambiguous samples using the cross entropy-loss.\n3 Experiments and Results\n3.1 Tasks and Datasets\nWe evaluate our TDMixUp on three natural lan-\nguage understanding tasks. We describe our in-\ndomain and out-of-domain sets as follows.\nNatural Language Inference (NLI) Stanford\nNatural Language Inference (SNLI) is a task to\npredict if the relation between a hypothesis and\na premise is entailment, contradiction,or neutral\n(Bowman et al., 2015). Multi-Genre Natural Lan-\nguage Inference (MNLI) captures NLI with diverse\ndomains (Williams et al., 2018).\nParaphrase Detection Quora Question Pairs\n(QQP) is a paraphrase detection task to test if two\nquestions are semantically equivalent (Iyer et al.,\n2017). TwitterPPDB (TPPDB) is a dataset built\nto determine whether sentence pairs from Twitter\nconvey similar semantics when they share URLs\n(Lan et al., 2017).\nCommonsense Reasoning Situations With Ad-\nversarial Generations (SWAG) is a commonsense\nreasoning task to choose the most plausible contin-\nuation of a sentence among four candidates (Zellers\net al., 2018). HellaSWAG is a dataset built using\nadversarial filtering to generate challenging out-of-\ndomain samples.\n3.2 Experimental Setup\nWe use BERT (Devlin et al., 2019) based classi-\nfication model and pass the resulting [CLS] rep-\nresentation through a fully connected layer and\nsoftmax to predict the label distribution. We follow\nthe published train/validation/test datasets splits as\ndescribed in Desai and Durrett (2020). To iden-\ntify mis-labeled samples in the top 33% easy-to-\nlearn samples, we set threshold values k as: the\n80th/80th/50th percentile threshold sample AUMs\non SNLI/QQP/SW AG, respectively. More training\ndetails and hyper-parameter settings can be found\nin the Appendix. We evaluate the capability of\nour TDMixUp strategy to improve both predictive\nperformance and model calibration due to its regu-\nlarization effect through data augmentation. Hence,\nwe use two metrics: (1) accuracy, and (2) expected\ncalibration error (ECE) (Guo et al., 2017; Desai\nand Durrett, 2020). We report results averaged\nacross 5 fine-tuning runs with random restarts for\nall experiments.\n3.3 Baseline Methods\nBERT (Devlin et al., 2019) is the pre-trained\nbase BERT model fine-tuned on each downstream\ntask.\nBack Translation Data Augmentation (Edunov\net al., 2018) generates augmented samples by using\npre-trained translation models2 which can generate\ndiverse paraphrases while preserving the semantics\n2We use FairSeq and set the random sampling temperature\nas 0.9.\n4246\nSNLI QQP SW AG\nAcc ECE Acc ECE Acc ECE\n100% train 90.040.3 2.540.8 90.270.3 2.710.5 79.400.4 2.491.8\n33% train, Easy-to-learn 82.780.6 16.220.7 63.160.1 36.880.1 75.390.2 17.510.1\n24% train, Easy-to-learn with AUM 83.030.9 15.050.9 66.430.6 33.930.8 75.560.1 15.810.7\n33% train, Ambiguous 89.710.5 0.740.1 87.510.5 1.710.4 75.910.6 1.840.7\n24% train, Ambiguous with AUM 87.880.7 7.090.8 88.630.5 6.360.6 71.740.4 7.551.1\n66% train, Easy-to-learn & Ambiguous 89.650.2 2.640.5 90.230.7 1.350.4 78.780.5 2.510.8\nMNLI TwitterPPDB HellaSW AG\nAcc ECE Acc ECE Acc ECE\n100% train 73.520.3 7.092.1 87.630.4 8.510.6 34.480.2 12.622.8\n33% train, Easy-to-learn 61.410.8 36.681.9 81.070.8 18.920.7 33.591.1 29.382.1\n24% train, Easy-to-learn with AUM 62.971.5 32.482.9 82.160.7 17.461.0 33.671.4 16.892.6\n33% train, Ambiguous 72.521.2 10.731.0 86.620.6 6.011.1 34.290.9 8.401.3\n24% train, Ambiguous with AUM 70.870.9 17.231.6 86.590.8 7.310.8 33.811.0 3.762.3\n66% train, Easy-to-learn & Ambiguous 73.890.6 3.461.9 87.290.3 8.040.7 34.430.2 9.681.1\nTable 1: The comparison of accuracy and expected calibration error (ECE) in percentage on in-domain (top) and out-of-domain\n(bottom) for BERT. We compare the results of fine-tuning on subsets of train samples (i.e., easy-to-learn and ambiguous samples)\nand fine-tuning on the entire 100% of training samples. Lower ECE implies better calibrated models. We report the mean\naccuracy across five training runs with the standard deviation shown in subscript (e.g., 90.040.3 indicates 90.04 ± 0.3).\nof the original sentences. In experiments, we trans-\nlate original sentences from English to German and\nthen translate them back to English to obtain the\nparaphrases.\nMixUp (Zhang et al., 2018) generates augmented\nsamples by interpolating random training samples\nin the input space (obtained from the first layer of\nthe BERT pre-trained language model).\nManifold MixUp (M-MixUp) (Verma et al.,\n2019) generates additional samples by interpolat-\ning random training samples in the feature space\n(obtained from the task-specific layer on top of the\nBERT pre-trained language model).\nMixUp for Calibration (Kong et al., 2020) gen-\nerates augmented samples by utilizing the cosine\ndistance between samples in the feature space.\nNote that the above baselines use 100% training\ndata while our proposed method focuses on partic-\nular subsets of the training data.\n3.4 Results\nFine-tuning on Subsets of Training Data To\nexplore the effect of different subsets of the data\nthat are characterized using training dynamics, we\ncompare the result of BERT fine-tuned on 100%\ntraining data with the results of BERT when fine-\ntuned on these subsets, and show the comparison\nin Table 1. Note that, for each task, we train the\nmodel on in-domain training set, and evaluate on\nin-domain and out-of-domain test sets. We make\nthe following observations: First, we observe that\naccuracy and ECE improve when we filter out pos-\nsibly mis-labeled samples in the top 33% easy-to-\nlearn samples by using AUM in all cases (for both\nin-domain and out-of-domain test sets). Specifi-\ncally, using 24% train, Easy-to-learn with AUM3\nreturns better accuracy and lower ECE than 33%\ntrain, easy-to-learn, showing that there are some\npotentially erroneous samples that harm the per-\nformance in the top 33% easy-to-learn samples.\nWe manually investigate filtered samples on the\ntop 33% easy-to-learn samples and observe that\nthe top 33% easy-to-learn samples indeed include\nmis-labeled samples. For example, in SNLI, we ob-\nserve that the relation between the following pairs\nof sentences in the top 33% easy-to-learn samples\nis contradiction when it should be neutral: <Two\nopposing wrestlers competing to pin one another.;\n‘Two women are shopping in a boutique. ’>and <‘A\nperson dressed in a colorful costume is holding\nsome papers. ’; ‘the cat jumps on the dog. ’>. In\ncontrast, we observe that in many cases accuracy\nand ECE worsen when we filter out possibly mis-\nlabeled samples in the 33% ambiguous samples by\nusing AUM, suggesting that all ambiguous samples\nare useful for learning and generalization (which\nis consistent with Swayamdipta et al. (2020)). Sec-\n3Although we write 24% in Table 1 for simplicity, the\npercentage of training samples on SNLI/QQP/SWAG after\nAUM filtering are 20%/24%/25%, respectively\n4247\nSNLI QQP SW AG\nAcc ECE Acc ECE Acc ECE\n100% train 90.040.3 2.540.8 90.270.3 2.710.5 79.400.4 2.491.8\n100% train, MixUp (Zhang et al., 2018) 88.820.2 7.731.1 89.120.5 9.040.8 74.982.3 7.081.0\n100% train, M-MixUp (Verma et al., 2019) 89.450.9 1.510.8 89.930.6 3.021.0 78.260.4 4.120.6\n100% train, MixUp for Calibration (Kong et al., 2020)89.250.5 2.160.5 90.240.3 5.220.6 79.440.6 1.100.4\n100% train, Back Translation Data Augmentation (Edunov et al., 2018)89.220.5 1.980.6 89.180.6 5.010.3 76.220.9 1.240.2\n66% train, TDMixUp, Easy-to-learn + Ambiguous 89.730.1 2.390.8 89.770.2 1.890.4 78.380.3 4.210.3\n57% train, TDMixUp, Easy-to-lean with AUM + Ambiguous (Ours)90.310.2 1.220.4 90.420.2 1.530.9 79.590.3 2.160.4\nMNLI TwitterPPDB HellaSW AG\nAcc ECE Acc ECE Acc ECE\n100% train 73.520.3 7.092.1 87.630.4 8.510.6 34.480.2 12.622.8\n100% train, MixUp (Zhang et al., 2018) 69.190.8 19.512.1 87.450.3 11.701.6 33.220.4 10.932.0\n100% train, M-MixUp (Verma et al., 2019) 73.220.6 8.061.2 87.580.7 7.681.3 34.860.9 13.561.6\n100% train, MixUp for Calibration (Kong et al., 2020)64.900.5 17.751.8 74.511.1 11.831.0 32.510.8 31.612.3\n100% train, Back Translation Data Augmentation (Edunov et al., 2018)73.150.7 8.461.3 86.820.7 8.830.6 34.970.4 22.683.3\n66% train, TDMixUp, Easy-to-learn + Ambiguous 72.831.1 5.841.9 87.630.2 6.480.7 34.110.1 10.541.6\n57% train, TDMixUp, Easy-to-learn with AUM + Ambiguous (Ours)74.280.6 2.911.4 87.890.3 6.080.4 35.210.6 9.451.3\nTable 2: Accuracy (in percentage) and expected calibration error (ECE) on in-domain (top) and out-of-domain (bottom) for\nBERT when comparing our proposed TDMixUp with baseline methods. Bold text shows the best performance and calibration.\nFor 100% trainresults, we use reported results by Desai and Durrett (2020). We report the mean accuracy across five training\nruns with the standard deviation shown in subscript (e.g., 90.040.3 indicates 90.04 ± 0.3).\nond, we observe that fine-tuning BERT on both\nthe easy-to-learn and the ambiguous samples (66%\ntrain, Easy-to-learn & Ambiguous) achieves simi-\nlar performance and ECE as 100% train.\nMain Results Table 2 shows the result of the\ncomparison of our proposed TDMixUp and base-\nline methods. We observe that our proposed\nmethod generally achieves higher accuracy and\nlower ECE on both in-domain and out-of-domain\nsettings compared to any baseline using the full\n100% training data showing the effectiveness of\nour TDMixUp strategy.\n3.5 Ablation Study\nTo compare the impact of the MixUp operation on\nsamples generated by random pairingand on sam-\nples generated by informative pairing, we conduct\nan ablation study. Specifically, we compare the\nresults of MixUp on 66% train set (i.e., conduct the\nMixUp operation between randomly selected sam-\nples on 66% train set, which is the union of the top\n33% easy-to-learn and the top 33% ambiguous sam-\nples) and our proposed TDMixUp (i.e., conduct the\nMixUp operation between the easy-to-learn filtered\nby AUM and the ambiguous samples). As shown in\nTable 3, we observe that our proposed TDMixUp\nwhich selects informative samples to combine per-\nforms better with respect to accuracy and ECE than\nvanilla MixUp that selects random samples, in all\ncases (in-domain and out-of-domain).\nAcc ECE Acc ECE Acc ECE\nSNLI QQP SW AG\nRandom 89.59 1.70 89.87 3.06 79.15 4.51\nOurs 90.31 1.22 90.42 1.53 79.59 2.16\nMNLI TwitterPPDB HellaSW AG\nRandom 73.22 6.89 87.23 6.53 34.43 15.87\nOurs 74.28 2.91 87.89 6.08 35.21 9.45\nTable 3: The results comparison of MixUp selecting random\nsamples on the union of the top 33% easy-to-learn and the top\n33% ambiguous samples (i.e., Random) and our method.\n4 Conclusion\nIn this work, we propose a novel MixUp that lever-\nages training dynamics (confidence, variability, and\nArea Under the Margin) to allow more informative\nsamples to be combined in generating augmented\nsamples. We empirically validate that our method\nnot only achieves competitive accuracy but also\ncalibrates BERT model on various NLP tasks, both\non in-domain and out-of-domain settings.\nAcknowledgements\nThis research is supported in part by NSF CAREER\naward #1802358 and NSF CRI award #1823292.\nAny opinions, findings, and conclusions expressed\nhere are those of the authors and do not necessarily\nreflect the views of NSF. We thank AWS for com-\nputing resources. We also thank our anonymous\nreviewers for their constructive feedback.\n4248\nReferences\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mix-\nText: Linguistically-informed interpolation of hid-\nden space for semi-supervised text classification. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2147–\n2157, Online. Association for Computational Lin-\nguistics.\nShrey Desai and Greg Durrett. 2020. Calibration of\npre-trained transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 295–302, Online.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 489–500, Brussels, Belgium. Association for\nComputational Linguistics.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In International Conference on Machine\nLearning, pages 1321–1330. PMLR.\nShankar Iyer, Nikhil Dandekar, and Kornél Csernai.\n2017. First quora dataset release: Question pairs. In\nQuora.\nLingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie\nLyu, Tuo Zhao, and Chao Zhang. 2020. Cali-\nbrated language model fine-tuning for in- and out-\nof-distribution data. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1326–1340, Online. As-\nsociation for Computational Linguistics.\nWuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017.\nA continuously growing dataset of sentential para-\nphrases. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1224–1234, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nGeoff Pleiss, Tianyi Zhang, Ethan R. Elenberg, and\nKilian Q. Weinberger. 2020. Identifying mislabeled\ndata using the area under the margin ranking. In\nAdvances in Neural Information Processing Systems,\nvolume 33.\nLichao Sun, Congying Xia, Wenpeng Yin, Tingting\nLiang, Philip Yu, and Lifang He. 2020. Mixup-\ntransformer: Dynamic data augmentation for NLP\ntasks. In Proceedings of the 28th International Con-\nference on Computational Linguistics, pages 3436–\n3440, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A. Smith,\nand Yejin Choi. 2020. Dataset cartography: Mapping\nand diagnosing datasets with training dynamics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9275–9293, Online. Association for Computa-\ntional Linguistics.\nSunil Thulasidasan, Gopinath Chennupati, Jeff A\nBilmes, Tanmoy Bhattacharya, and Sarah Michalak.\n2019. On mixup training: Improved calibration and\npredictive uncertainty for deep neural networks. In\nAdvances in Neural Information Processing Systems,\nvolume 32.\nVikas Verma, Alex Lamb, Christopher Beckham, Amir\nNajafi, Ioannis Mitliagkas, David Lopez-Paz, and\nYoshua Bengio. 2019. Manifold mixup: Better rep-\nresentations by interpolating hidden states. In In-\nternational Conference on Machine Learning, pages\n6438–6447. PMLR.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nWenpeng Yin, Huan Wang, Jin Qu, and Caiming Xiong.\n2021. BatchMixup: Improving training by interpo-\nlating hidden states of the entire mini-batch. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 4908–4912, Online.\nAssociation for Computational Linguistics.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. SW AG: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 93–104, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\n4249\nHongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and\nDavid Lopez-Paz. 2018. mixup: Beyond empirical\nrisk minimization. In Proceedings of the 6th Inter-\nnational Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada.\nA Supplementary Materials\nA.1 Training Details\nIn our experiments, we use bert-base-uncased clas-\nsification model on top of a task-specific fully-\nconnected layer. The model is fine-tuned with a\nmaximum of 3 epochs, batch size of 16 for SNLI\nand QQP, batch size 4 for SW AG, a learning rate of\n1e-5, gradient clip of 1.0, and no weight decay. We\nuse the hyper-parameter of MixUp αas 0.4. All\nhyper-parameters are estimated on the validation\nset of each task. For all results, we report aver-\naged results across 5 fine-tuning runs with random\nstarts. Finally, all experiments are conducted on\na single NVIDIA RTX A6000 48G GPU with the\ntotal time for fine-tuning all models being under\n24 hours. For each dataset, we follow the pub-\nlished train/validation/test split by Desai and Dur-\nrett (2020) and show the statistics of the datasets in\nTable 4.\nDataset Train Dev Test\nSNLI 549,368 4,922 4,923\nMNLI 392,702 4,908 4,907\nQQP 363,871 20,216 20,217\nTwitterPPDB 46,667 5,060 5,060\nSW AG 73,547 10,004 10,004\nHellaSW AG 39,905 5,021 5,021\nTable 4: The statistics of all used datasets.\nA.2 Data Maps\nIn this section, we provide data maps\n(Swayamdipta et al., 2020) of our in-domain\ndatasets on bert-base-uncased model in Figure\n1. These data maps are used to identify the\ncharacteristics of each training sample (i.e.,\neasy-to-learn, ambiguous, and hard-to-learn).\nFigure 1: Data Maps of SNLI, QQP and SW AG onbert-\nbert-uncased model.\n4250",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8054284453392029
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.7424951791763306
    },
    {
      "name": "Language model",
      "score": 0.6607686281204224
    },
    {
      "name": "Training set",
      "score": 0.569107711315155
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5665560960769653
    },
    {
      "name": "Calibration",
      "score": 0.5637964010238647
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.5618152022361755
    },
    {
      "name": "Code (set theory)",
      "score": 0.5528523325920105
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5367459058761597
    },
    {
      "name": "Random forest",
      "score": 0.5214033126831055
    },
    {
      "name": "Machine learning",
      "score": 0.4587417542934418
    },
    {
      "name": "Labeled data",
      "score": 0.44420427083969116
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3289417028427124
    },
    {
      "name": "Data mining",
      "score": 0.32713741064071655
    },
    {
      "name": "Natural language processing",
      "score": 0.32628631591796875
    },
    {
      "name": "Statistics",
      "score": 0.17712345719337463
    },
    {
      "name": "Mathematics",
      "score": 0.08521541953086853
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    }
  ]
}