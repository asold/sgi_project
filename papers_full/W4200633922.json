{
    "title": "Geometry-Contrastive Transformer for Generalized 3D Pose Transfer",
    "url": "https://openalex.org/W4200633922",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5100763975",
            "name": "Haoyu Chen",
            "affiliations": [
                "University of Oulu"
            ]
        },
        {
            "id": "https://openalex.org/A5050748634",
            "name": "Hao Tang",
            "affiliations": [
                "ETH Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A5062522283",
            "name": "Zitong Yu",
            "affiliations": [
                "University of Oulu"
            ]
        },
        {
            "id": "https://openalex.org/A5027171279",
            "name": "Nicu Sebe",
            "affiliations": [
                "University of Trento"
            ]
        },
        {
            "id": "https://openalex.org/A5082301986",
            "name": "Guoying Zhao",
            "affiliations": [
                "University of Oulu"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2968928303",
        "https://openalex.org/W2967247499",
        "https://openalex.org/W2483862638",
        "https://openalex.org/W6646626034",
        "https://openalex.org/W6743033446",
        "https://openalex.org/W2956286548",
        "https://openalex.org/W3204862792",
        "https://openalex.org/W3114458132",
        "https://openalex.org/W1981784948",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W6753343845",
        "https://openalex.org/W2603777577",
        "https://openalex.org/W3112160422",
        "https://openalex.org/W6760869523",
        "https://openalex.org/W6774645763",
        "https://openalex.org/W2920879895",
        "https://openalex.org/W2939665722",
        "https://openalex.org/W3115372334",
        "https://openalex.org/W2950642167",
        "https://openalex.org/W2624503621",
        "https://openalex.org/W6910365999",
        "https://openalex.org/W3088900258",
        "https://openalex.org/W6693397905",
        "https://openalex.org/W6678447759",
        "https://openalex.org/W6743741991",
        "https://openalex.org/W3048409285",
        "https://openalex.org/W6780698268",
        "https://openalex.org/W3011304133",
        "https://openalex.org/W2796312544",
        "https://openalex.org/W6746034047",
        "https://openalex.org/W3188511781",
        "https://openalex.org/W3094861582",
        "https://openalex.org/W3009920034",
        "https://openalex.org/W3081152303",
        "https://openalex.org/W3044960031",
        "https://openalex.org/W3034157075",
        "https://openalex.org/W6730160451",
        "https://openalex.org/W3035459165",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W1985907520",
        "https://openalex.org/W4289549306",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3128358041",
        "https://openalex.org/W2555859288",
        "https://openalex.org/W3034948195",
        "https://openalex.org/W4287545189",
        "https://openalex.org/W4312249545",
        "https://openalex.org/W3080084217",
        "https://openalex.org/W4236124550",
        "https://openalex.org/W2962974533",
        "https://openalex.org/W2267893120",
        "https://openalex.org/W2962778872",
        "https://openalex.org/W2992956318",
        "https://openalex.org/W3008758407",
        "https://openalex.org/W2963121255",
        "https://openalex.org/W4252995899",
        "https://openalex.org/W2560609797",
        "https://openalex.org/W4287690777",
        "https://openalex.org/W3106811106",
        "https://openalex.org/W2962928839",
        "https://openalex.org/W2979283733",
        "https://openalex.org/W2502312327",
        "https://openalex.org/W3175199633",
        "https://openalex.org/W2746892480",
        "https://openalex.org/W2978956737",
        "https://openalex.org/W2928521819"
    ],
    "abstract": "We present a customized 3D mesh Transformer model for the pose transfer task. As the 3D pose transfer essentially is a deformation procedure dependent on the given meshes, the intuition of this work is to perceive the geometric inconsistency between the given meshes with the powerful self-attention mechanism. Specifically, we propose a novel geometry-contrastive Transformer that has an efficient 3D structured perceiving ability to the global geometric inconsistencies across the given meshes. Moreover, locally, a simple yet efficient central geodesic contrastive loss is further proposed to improve the regional geometric-inconsistency learning. At last, we present a latent isometric regularization module together with a novel semi-synthesized dataset for the cross-dataset 3D pose transfer task towards unknown spaces. The massive experimental results prove the efficacy of our approach by showing state-of-the-art quantitative performances on SMPL-NPT, FAUST and our new proposed dataset SMG-3D datasets, as well as promising qualitative results on MG-cloth and SMAL datasets. It's demonstrated that our method can achieve robust 3D pose transfer and be generalized to challenging meshes from unknown spaces on cross-dataset tasks. The code and dataset are made available. Code is available: https://github.com/mikecheninoulu/CGT.",
    "full_text": "Geometry-Contrastive Transformer for Generalized 3D Pose Transfer\nHaoyu Chen1 Hao Tang2 Zitong Yu1 Nicu Sebe3 Guoying Zhao*1\n1 CMVS, University of Oulu\n2 Computer Vision Lab, ETH Zurich\n3 DISI, University of Trento\nchen.haoyu@oulu.ﬁ, hao.tang@vision.ee.ethz.ch, zitong.yu@oulu.ﬁ, nicu.sebe@unitn.it, guoying.zhao@oulu.ﬁ\nAbstract\nWe present a customized 3D mesh Transformer model for\nthe pose transfer task. As the 3D pose transfer essentially\nis a deformation procedure dependent on the given meshes,\nthe intuition of this work is to perceive the geometric in-\nconsistency between the given meshes with the powerful\nself-attention mechanism. Speciﬁcally, we propose a novel\ngeometry-contrastive Transformer that has an efﬁcient 3D\nstructured perceiving ability to the global geometric inconsis-\ntencies across the given meshes. Moreover, locally, a simple\nyet efﬁcient central geodesic contrastive loss is further pro-\nposed to improve the regional geometric-inconsistency learn-\ning. At last, we present a latent isometric regularization mod-\nule together with a novel semi-synthesized dataset for the\ncross-dataset 3D pose transfer task towards unknown spaces.\nThe massive experimental results prove the efﬁcacy of our ap-\nproach by showing state-of-the-art quantitative performances\non SMPL-NPT, FAUST and our new proposed dataset SMG-\n3D datasets, as well as promising qualitative results on MG-\ncloth and SMAL datasets. It’s demonstrated that our method\ncan achieve robust 3D pose transfer and be generalized to\nchallenging meshes from unknown spaces on cross-dataset\ntasks. The code and dataset are made available. Code is avail-\nable: https://github.com/mikecheninoulu/CGT.\nIntroduction\nPose transfer, applying the desired pose of a source mesh to\na target mesh, is a promising and challenging task in 3D\ncomputer vision, which can be widely applied to various\nindustrial ﬁelds. However, existing methods (Wang et al.\n2020; Cosmo et al. 2020; Zhou, Bhatnagar, and Pons-Moll\n2020; Chen et al. 2021b) can only perform well within given\ndatasets of synthesized/known pose and shape space, and\nfail to be generalized to other unknown spaces with robust\nperformances, which severely limits the further real-world\nimplementations.\nTo achieve robust performances on unknown latent spaces\nand other domains as shown in Fig. 1, we propose a novel\nTransformer network targeting generalized 3D mesh pose\ntransfer. Speciﬁcally, a novel geometry-contrastive Trans-\nformer with geometrically structured encoders is designed\nthat aims to enhance the identity mesh representation un-\n*Corresponding Author.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nder the guidance of the pose mesh with their global geomet-\nric contrasts. Locally, we introduce a novel central geodesic\ncontrastive loss to improve the geometric representation by\nconsidering the regional contrast of all the geodesic direc-\ntions of each vertex as back-propagation gradients. Further-\nmore, we present a latent isometric regularization module\nto stabilize the unreliable performance of cross-dataset pose\ntransfer problems.\nMoreover, we present a new 3D mesh dataset, i.e., SMG-\n3D, for quantitatively evaluating the 3D pose transfer with\nunknown spaces. The SMG-3D is based on daily sponta-\nneously performed body gestures with more plausible and\nchallenging body movements and different than those well-\nperformed poses (Mahmood et al. 2019; Bogo et al. 2017).\nWe use a semi-synthesis way to build the dataset to provide\nnecessary GT meshes for training and validating. Our SMG-\n3D dataset can be jointly combined with other existing body\nmesh datasets for cross-dataset qualitative analysis.\nA natural question to ask is: why not simply use purely\nsynthesized meshes to train and evaluate the model? The\nshort answer is that models trained on purely synthesized\nmeshes cannot cover the whole latent space and will fail\nin the cross-dataset task. Indeed, using mesh synthesizing\nmodels like the SMPL series (Bogo et al. 2016; Zufﬁ et al.\n2017; Pavlakos et al. 2019) can synthesize unlimited poses\nthat can cover the whole latent space, or a large-scale dataset\nAMASS (Mahmood et al. 2019) to eliminate the inconsis-\ntencies with unknown dataset space. However, in practice,\neven for a small dataset FAUST with only 10 pose cate-\ngories, it takes more than 26 hours to train a model (Cosmo\net al. 2020) to fully learn the latent space. Thus, due to the\nstaggering variability of poses and movements, it’s not fea-\nsible to train the model with synthesized samples covering\nthe whole pose space. It’s desirable that a model can be di-\nrectly generalized to unknown latent spaces in a more efﬁ-\ncient way. To this end, we propose the SMG-3D dataset to\ntackle the cross-dataset learning issue. It can provide chal-\nlenging latent distribution allocates on natural and plausi-\nble body poses with occlusions and self-contacts instead\nof well-posed body moves like AMASS (Mahmood et al.\n2019), which could advance the research to real-world sce-\nnarios one step further.\nTo summarize, our contributions are as follows:\n• A novel geometry-contrastive Transformer of positional\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n258\nFigure 1: Examples of pose transfer results by our 3D GC-Transformer. Blue, pink, and purple colors stand for identity, pose,\nand result meshes, respectively. The left part shows the human pose transfer results. The identity meshes are from FAUST\n(Bogo et al. 2014), MG-cloth (Bhatnagar et al. 2019), SMPL-NPT (Wang et al. 2020), and our new SMG-3D dataset. The right\npart shows animal pose transfer results on the SMAL dataset (Zufﬁ et al. 2017). Our method can be generalized to different\nspaces and even real-world scenarios and animals. More experimental results can be found in supplementary materials.\nembedding free architectures with state-of-the-art perfor-\nmances on the challenging 3D pose transfer task.\n• A simple and efﬁcient central geodesic contrastive loss\nthat can further improve the geometric learning via pre-\nserving the direction gradient of the 3D vertices.\n• A challenging 3D human body mesh dataset (i.e., SMG-\n3D) providing unknown space of naturally plausible body\nposes with challenging occlusions and self-contacts for\nthe cross-dataset qualitative evaluation.\n• A new latent isometric regularization module for adapting\nto challenging unknown spaces on cross-dataset tasks.\nRelated Work\n3D Mesh Deformation Transfer. Deformation transfer\naims to generate a new 3D shape with a given pair of\nsource poses and target shapes. Even though existing meth-\nods (Groueix et al. 2018; Sumner and Popovi ´c 2004) could\nbring impressive deformation results, the superb perfor-\nmances largely rely on the given correspondences of the\nsource and target meshes, which limits their generalization\nability. Some disentanglement-based methods like (Zhou,\nBhatnagar, and Pons-Moll 2020; Cosmo et al. 2020; Chen\net al. 2021a) tried to decompose meshes into shape and pose\nfactors and achieve pose transfer as a natural consequence.\nHowever, extra constraints on the datasets are still needed.\nModel Verte\nx\noperator\nVertex\ntopology Mesh size Mesh type\nVanilla\nMLP Damaged - -\nMETRO Positional\nembedding\nPreserv\ned,\nhigh cost\nDown-sampled\nfrom 6890 to 431\nPost-\nprocess\nPolyGen Pointer\nembedding\nPreserved,\nhigh\ncost\nFilter meshes larger\nthan 800 vertices Real mesh\nGCT\n(ours)\nDepth-wise\n1D Con\nv\nPreserved,\nno cost\nOriginal size\nsuch as 6890 Real mesh\nTable 1: A comparison of our GC-Transformer with other\n3D Transformer variants.\nDeep Learning for Geometric Representation. Point-\nNet (Qi et al. 2017a) and PointNet++ (Qi et al. 2017b) have\nbecome common-use frameworks that can work directly on\nsparse and unorganized point clouds. After that, mesh varia-\ntional autoencoders (Aumentado et al. 2019; Tan et al. 2018)\nwere also proposed to learn mesh embedding for shape syn-\nthesis but they are under a strong condition that the shape of\ntarget objects should be given as prior. On the other hand,\nthere is a trend to utilize to the self-attention mechanism\nof Transformers for structural geometric information learn-\ning. However, as shown in Table 1, those preliminary works\n(Lin, Wang, and Liu 2021; Nash et al. 2020; Engel, Bela-\ngiannis, and Dietmayer 2020) tried to directly encode the\nvertex topological structures with computationally demand-\ning embeddings, thus can only handle small-size meshes. In\nthis work, our GC-Transformer is completely different and\nimplements depth-wise 1D Convolution instead of any com-\nputational embedding to preserve vertex topological struc-\ntures thus freely handles LARGE meshes with ﬁne-grained\ndetails at no cost, which could boost efﬁcient implementa-\ntions of Transformer frameworks in 3D ﬁelds.\nCross-Dataset 3D Pose Transfer. There is few 3D mesh\ndataset suitable for the pose transfer task. Though many\ntechniques and body models have been developed for 3D\ndata analysis such as SMPL series (Bogo et al. 2016;\nRomero, Tzionas, and Black 2017; Pavlakos et al. 2019;\nZufﬁ et al. 2017), as well as various 3D human body datasets\n(Bogo et al. 2014, 2017; Bhatnagar et al. 2019; Pavlakos\net al. 2019; Mahmood et al. 2019), they are all originally\ndesigned for other tasks such as scan registration, recog-\nnition, or shape retrieval. Thus, the poses in those datasets\nare all exaggerated and perfectly posed actions, for instance,\nto ensure the quality of the scan registration. However, the\nlatent space distribution of real ones with occlusion and\nself-contacts can differ widely. Besides, few of the existing\ndatasets can be parameterized and manipulated in the latent\nspace towards desired poses, thus no standard GT is avail-\nable for the training and the quantitative evaluation. Existing\nmethods (Cosmo et al. 2020) could merely use approxima-\ntions such as geodesic preservation as substitutes.\nMethodology\nWe deﬁne a 3D parametric mesh as M(\u000b;\f), where \u000b, \f\ndenote the parameters of identity (i.e., shape) and pose. Let\n259\nFigure 2: An overlook of our GC-Transformer. The left part is the whole architecture of the GC-Transformer. The right part\nillustrates the architecture details of one GC-Transformer decoder. The GC-Transformer borrows the idea from the work of\n(Dosovitskiy et al. 2021) but is extensively extended to 3D data processing tasks for both the encoders and decoders.\nM1(\u000bpose;\fpose) be the mesh with the desired pose for\nstyle transfer and M2(\u000bid;\fid) be the mesh with its iden-\ntity to preserve. Then the polygon mesh M′(\u000bid;\fpose) is\nthe target to generate. The goal of pose transfer is to learn a\ndeformation function f which takes a pair M1 and M2 and\nproduces a new mesh M′, so that the geodesic preservation\nof the resulting mesh M′is identical to the source one M2\nand the pose style is identical to M1.\nf(M1(\u000bid;\fid);M2(\u000bpose;\fpose)) = M′(\u000bid;\fpose):\n(1)\nBelow, we will ﬁrst introduce how to use the Transformer\narchitecture-based model, called Geometry-Contrastive\nTransformer (GC-Transformer) for learning the deformation\nfunction f, then the Central Geodesic Contrastive (CGC)\nloss for detailed geometric learning, and at last, the La-\ntent Isometric Regularization (LIR) module for robust pose\ntransfer on cross-dataset tasks.\nGeometry-Contrastive Transformer\nAn overview of the GC-Transformer is depicted in Fig. 2.\nOur GC-Transformer consists of two key components, one\nis a structured 3D mesh feature encoder and the other one is\na Transformer decoder.\nStructured 3D Encoder. As mentioned, existing 3D Trans-\nformers needs computationally demanding embeddings to\nencode vertex positions, thus in practice can only process\n‘toy’ meshes. Inspired by NeuralBody (Peng et al. 2021) that\nuses structured latent codes to preserve the vertex topology,\nwe modify the conventional PointNet (Qi et al. 2017a) into\nstructured 3D encoders to capture the vertex topology by\nimplementing depth-wise 1D convolution instead of redun-\ndant positional embeddings commonly used in conventional\nTransformers. Meanwhile, we replace the batch normaliza-\ntion layers into Instance Normalization (Ulyanov, Vedaldi,\nand Lempitsky 2016) layers to preserve the instance style\nwhich is widely used on style transfer tasks (Huang and Be-\nlongie 2017; Park et al. 2019). The resulting latent embed-\nding vector Zwith dimension Nlatent from the encoder will\nbe dimensionally reduced with 1D convolution and fed into\nthe following GC-Transformer decoder. In this way, LARGE\nmeshes with ﬁne-grained details can be handled freely at\nno cost by our GC-Transformer while preserving the vertex\nstructures.\nGC-Transformer Decoder. We encourage readers to refer\nto the ViT (Dosovitskiy et al. 2021) for a standard Trans-\nformer structure, which achieve state-of-the-art results on\nmany tasks (Li et al. 2021; Yang et al. 2021). We propose the\nGC-Transformer decoder that inherits the classical structure\nwith customized designs for 3D meshes. The structure of the\nGC-Transformer decoder is shown in Fig. 2.\nThe core difference between the GC-Transformer and a\nstandard Transformer is the design of the multihead self-\nattention. To learn the correlations between the given meshes\nfor geometric deformation, the model should be able to per-\nceive the geometric information from the two meshes. Thus,\nwe make the inputs of a GC-Transformer as the latent em-\nbedding vectors of two meshesinstead of a single input like\nthe classical Transformer. Besides, as it’s a style transfer\ntask, we utilize the Instance Norm (Huang and Belongie\n2017) as our normalization layers. At last, to preserve the\nstructural information of 3D data, the MLP layers are re-\nplaced with 1D Convolutional layers.\nWe denote the latent embedding vectors of the pose mesh\nand identity mesh from the encoders as Zpose and Zid re-\nspectively. We feed the two embedding vectors into differ-\nent 1D convolution layers to generate the representations\nqkv for the standard multihead self-attention (Vaswani et al.\n2017). The query q is from Zpose, and the value v and key\nk are from Zid. Then, the attention weights Ai;j based on\nthe geometric pairwise similarity between two elements of\nq and k is given with the following formula:\nAi;j = exp(qikj)\nPn\ni=1 exp(qikj): (2)\nAfter this, a matrix multiplication between v and the trans-\npose of A is conducted to perceive the geometric inconsis-\ntency between meshes. Finally, we weigh the result with a\nscale parameter \r and conduct an element-wise sum opera-\ntion with the original latent embedding Zpose to obtain the\nreﬁned latent embedding Z′\npose,\nZ′\npose = \r\nnX\ni=1\n(Ai;jvi) + Zpose; (3)\nwhere \r is initialized as 0 and updated gradually during the\ntraining with gradients. The obtained Z′\npose is followed by\n260\nFigure 3: Left: an overlook of our semi-synthesized 3D mesh body gesture dataset SMG-3D. It is a 3D dataset with a pose\nspace that ﬁts the real-world dataset pose distribution, including naturally and spontaneously performed body movements\nin daily communication with challenging occlusions and self-contacts. Right: the architecture of proposed Latent Isometric\nRegularization (LIR) module for unknown latent space learning.\ntypical Transformer operators as introduced above Fig. 2\nwith a convolutional layer and Tanh activation, generating\nthe ﬁnal output M′. Please refer to the supplementary mate-\nrials for more implementing details.\nIn such a crossing way, the geometric-perceived feature\ncode can consistently be rectiﬁed by the original identity\nmesh and its latent embedding representations. Note that,\ndifferent than previous attention-based modules (Wang et al.\n2018b; Tang et al. 2020b; Huang and Belongie 2017; Tang\net al. 2020a), our GC-Transformer could not only com-\npute the pair-wise correlations and contrasts in a crossing-\nmesh way but also could fully preserve the local geometric\ndetails with the residual layer. Most importantly, our GC-\nTransformer is designed for 3D mesh processing which has\nnever been attempted in these works. Note that input mesh\nvertices are all shufﬂed randomly to ensure the network is\nvertex-order invariant.\nCentral Geodesic Contrastive Loss\nMost of the existing 3D mesh representation learning losses,\nsuch as triangle regularization loss, edge loss, Chamfer loss\nand Laplacian loss (Wang et al. 2018a, 2020; Groueix et al.\n2018; Sorkine 2005; Zhou et al. 2020) all repeal the gradi-\nent of the direction information of 3D vertices. They only\ncompare the scalar (or weak vector) differences of the mesh\nvertices such as one-ring geodesic lengths to construct the\nobjective function, while the convexity of the mesh surface\ncontaining rich directional gradient information is not uti-\nlized. To this end, inspired by the superb performances of\ncentral difference convolution (Yu et al. 2020, 2021a,b) that\nconsiders the directional difference of depth space, we sug-\ngest to compare the vector differences of the vertex topol-\nogy by proposing a simple yet efﬁcient central geodesic con-\ntrastive loss as below:\nLcontra = 1\nV\nX\np\nX\nu∈Γ(p)\nq\nu2\nM0 + u2\nM −2uM0 uM ·cos(\u0012);\n(4)\nwhere Γ(p) denotes the neighbor edges of vertex p and V\nis the total vertex number of the mesh. uM denotes the edge\nof mesh M and \u0012denote the included angle of the edges of\nuM and uM0 . In practice, Lcontra can be easily calculated\nby taking the vector difference of uM and uM0 within the\ncoordinate of each vertex pand divided by the total vertex\nnumber as a global normalization.\nOur CGC loss has three improvements compared to ex-\nisting losses: 1) the full inconsistencies of vertex vectors are\ncalculated to preserve the direction gradient; 2) each direc-\ntion of the vertex is separately considered instead of a simple\nsum-up; 3) the sampling methods of the neighbor vertices of\np in Eq. (4) is different: the CGC loss samples all the ver-\ntices connected to p resulting in a ﬂexible N neighbor ver-\ntices while others (Wang et al. 2018a; Groueix et al. 2018)\nare within the mesh triangle of vertexp and ﬁxed to 3. Please\nrefer to Fig. 4 for a better understanding. A point-wise L2\nreconstruction loss of mesh vertices can only capture the ab-\nsolute distance in the coordinate space. Contrastively, our\nCGC loss captures the inconsistencies of all the geodesic\ndirections at each vertex, so that direction gradients can be\npreserved in the back-propagation. Note that our CGC loss\nis similar to Laplacian loss but can preserve full vector dif-\nferences without Laplacian normalization, thus is not only\nlimited to smooth surfaces. As shown in Fig. 4, our CGC\nloss could offer additional strong supervision especially in\ntightening the output mesh surface.\nOverall Objective Function. With our proposed CGC loss,\nwe deﬁne the full objective function as below:\nLfull = \u0015recLrec + \u0015edgeLedge + \u0015contraLcontra; (5)\nwhere Lrec, Ledge and Lcontra are the three losses used\nas our full optimization objective, including reconstruction\nloss, edge loss and our newly proposed CGC loss. \u0015is the\ncorresponding weight of each loss. In Eq. (5), reconstruc-\ntion loss Lrec is the point-wise L2 distance and the edge\nloss (Groueix et al. 2018) is an edge-wise regularization be-\ntween the GT meshes and predicted meshes.\n261\nFigure 4: A comparison of different losses for both the neighbor vertex sampling strategy and the local inconsistency. Our CGC\nloss considers the inconsistencies of all the geodesic directions at each vertex, so that direction gradients can be preserved in\nthe back-propagation. Results show that CGC loss can make the local details more tight and realistic.\nCross-Dataset Pose Transfer\nAlthough existing pose transfer methods can deal with fully\nsynthesized/known pose space, they fail to have a robust per-\nformance on the pose space that is different from the training\none. To facilitate the 3D analysis of human behaviors to real-\nworld implementations, we propose a new SMG-3D dataset\nas well as a LIR module towards the cross-dataset issue.\nA New SMG-3D Dataset. The main contribution of the\nSMG-3D dataset is providing an alternative benchmark to-\nwards cross-dataset tasks by providing standard GTs under a\nchallenging latent pose distribution (unlike perfectly synthe-\nsized/performed known distributions). As shown in Fig. 3,\nSMG-3D is derived from an existing 2D body pose dataset\ncalled SMG dataset (Chen et al. 2019) that consists of spon-\ntaneously performed body movements with challenging oc-\nclusions and self-contacts. Speciﬁcally, we ﬁrst adopt the\n3D mesh estimation model STRAPS (Sengupta, Budvytis,\nand Cipolla 2020) to generate the 3D mesh estimations\nfrom the original 2D images of SMG. Then, we select 200\nposes and 40 identities as templates to form the potential\npose space and optimize them by Vposer (Pavlakos et al.\n2019). At last, the generated 3D meshes are decomposed\ninto numerical registrations as latent parameters which are\npaired to synthesize the resulting 8,000 body meshes via the\nSMPL model (Bogo et al. 2016), each with 6,890 vertices.\nCompared to synthesized/well-performed meshes, our in-\nthe-wild 3D body meshes are more practical and challeng-\ning with the large diversity and tricky occlusions for provid-\ning the unknown latent space. Please check more about our\ndataset in the supplementary materials.\nLatent Isometric Regularization Module. When the poses\nand shapes are from unknown latent spaces, existing meth-\nods suffer from degeneracy in varying degrees (see Fig. 5).\nWe address this issue by introducing the LIR module as\nshown in Fig. 3 right part, that can aggregate the data dis-\ntribution of target set and source set. The LIR can bestacked\nto existing standard models to enhance the cross-dataset\nperformance. Speciﬁcally, the difference between the two\ndatasets is obtained by comparing the latent pose codes zM\nand zM0 of the shape mesh M′from the target set and the\npose mesh Mfrom the source dataset. The target shape mesh\nwill be fed into GC-Transformer along with another ran-\ndomly sampled mesh from the target set to obtain a newly\ngenerated mesh M′. This will be iteratively executed until\nthe latent pose code difference zM0 and zM converges to\nless than \u0012, resulting in a normalized target set. In this way,\nPMD#\n(\u000210−4)\nSeen Unseen\nNPT-MP\nNPT GCT (ours) NPT-MP NPT GCT (ours)\nSMG-3D 70.3\n62.1 30.7 120.3 94.6 52.8\nSMPL-NPT 2.1 1.1 0.6 12.7 9.3 4.0\nTable 2: Intra-dataset performances on SMG-3D and SMPL-\nNPT datasets. “NPT MP” stands for NPT model with max\npooling layers. Note that the “unseen” setting is still within\nthe same dataset with similar data distributions.\nthe latent pose distribution of the target set will be regulated\nwhile its isometric information can still be preserved. Essen-\ntially, our LIR module serves as a domain adaptive normal-\nization to warm-up the unknown target set to better ﬁt the\nmodel trained on the source pose space.\nExperiments\nDatasets\nSMPL-NPT (Wang et al. 2020) dataset contains 24,000 syn-\nthesized body meshes with the SMPL model (Bogo et al.\n2016) by sampling in the parameter space. For training, 16\ndifferent identities and 400 different poses are randomly se-\nlected and made into pairs as GTs. For testing, 14 new iden-\ntities are paired with those 400 poses and 200 new poses\nas “seen” and “unseen” sets. Note that the “unseen” poses\nare sampled within the same parameter distributionas the\n“seen” poses, thus still in thesame/known latent pose space.\nSMG-3D (Chen et al. 2019) dataset contains 8,000 pairs of\nnaturally plausible body meshes of 40 identities and 200\nposes, 35 identities and 180 poses are used as the training\nset. The rest 5 identities with the 180 poses and the other\n20 poses are used for “seen” and “unseen” testing. Note that\nboth SMPL-NPT and SMG-3D provide GT meshes so that\nthey can be used for cross-dataset quantitative evaluation.\nFAUST (Bogo et al. 2014) dataset consists of 10 differ-\nent human subjects, each captured in 10 poses. The FAUST\nmesh structure is similar to SMPL with 6,890 vertices.\nMG-Cloth (Bhatnagar et al. 2019) dataset contain 96\ndressed identity meshes with different poses and clothes.\nThe MG-cloth meshes contain way more vertices (above\n27,000), which is more challenging for more ﬁne-grained\ngeometry details. Note that meshes in FAUST and MG-cloth\nare not parameterized SMPL models so geodesic-based ap-\nproximations (Crane, Weischedel, and Wardetzky 2013) is\nalways used for evaluation in previous works.\n262\nFigure 5: Intra- and cross-dataset qualitative results on the SMPL-NPT, FAUST, MG-cloth, SMG-3D and SMAL datasets.\nSMAL (Zufﬁ et al. 2017) animal dataset is based on a\nparametric articulated quadrupedal animal model and we\nadopted it to synthesize the training and testing datasets.\nIntra-Dataset Pose Transfer Evaluation\nFirstly, we evaluate the intra-dataset pose transfer perfor-\nmance of our GC-Transformer on the SMPL-NPT and\nSMG-3D. Given the GT meshes, we follow (Wang et al.\n2020) to adopt Point-wise Mesh Euclidean Distance (PMD)\nas the evaluation metric:\nPMD = 1\n|V|\nX\nv\n∥Mv −M′\nv∥2\n2 : (6)\nwhere Mv and M′\nv are the point pairs from the GT mesh\nM and generated one M′. The ﬁnal experimental results\ncan be found in Table 2. For both settings of the SMPL-\nNPT: “seen” and “unseen pose”, our GC-Transformer signif-\nicantly outperforms compared SOTA methods by more than\n45% and 55% with PMD (× 10−4) of: 0.6 and 4.0 vs. 1.1\nand 9.3. We denote PMD (×10−4) as PMD for simplicity in\nthe following. On our SMG-3D dataset, our network again\nyields the best performance among other methods with PMD\nof (30.7 and 52.8). As shown, the SMG-3D is more chal-\nlenging than the SMPL-NPT dataset with way higher PMD\nvalues for all the models. Compared to the fully synthesized\ndataset SMPL-NPT, the poses in SMG-3D are more realis-\ntic as they contain many occlusions and self-contacts. The\ndistribution of the poses in the latent space is signiﬁcantly\nuneven and discontinuous while the poses synthesized in the\nSMPL-NPT dataset are way easier with less noise.\nGeneralized Pose Transfer Evaluation\nCross-Dataset Pose Transfer with Same Pose Space. We\nextent the setting to cross-datasets by training the model on\nSMPL-NPT dataset and directly conduct the pose transfer on\nthe unseen meshes from FAUST and MG-cloth datasets. As\nDisentnaglement Error\nV AE LIMP-Euc LIMP-Geo GCT (ours)\n7.16 4.04 3.48 0.11\nTable 3: Cross-dataset performances on FAUST dataset. Be-\ncause we use the raw meshes of FAUST and there is no GT,\ngeometric approximations are used for evaluation.\nCross-dataset PMD\n#(\u000210−4)\nTraining\nset Testing set NPT-MP NPT GCT (ours)\nSMPL-NPT\nSMPL-NPT 12.7\n9.3 4.0\nSMG-3D w\no/LIR 321.4 240.1 178.7\nSMG-3D w/LIR 132.3 121.4 79.2\nSMG-3D SMG-3D\n120.3 94.6 52.8\nTable 4: Cross-Dataset performances with standard GTs as\nmetrics. Our LIR module can be stacked to existing models\nand robustly improve the performances on unknown spaces.\nshown in Fig. 5 second/third line, NPT might fail when the\ntarget pose is not within the training latent space while our\nmethod can still perform well. Since there is no GT available\nhere, we adopt the disentanglement error of the pose transfer\ntask illustrated in the work of LIMP (Cosmo et al. 2020) as\nthe metrics, see LIMP (Cosmo et al. 2020) for more details.\nIn Table 3, we report the performances of GC-Transformer\nand state-of-the-art models on FAUST. Compared to LIMP\ntrained with the preservation of geodesic distances, ours sig-\nniﬁcantly outperforms it: 0.23 vs. 3.48. As expected, the\npreservation of geodesic distances, geodesic distances can\nonly serve as the approximation of GTs.\nCross-Dataset Pose Transfer with Different Pose Space .\nIn this part, we quantitatively analyze the cross-dataset per-\nformance between different latent spaces of SMPL-NPT and\n263\nFigure 6: Ablation study by progressively enabling each component. The rightmost mesh is from the full GC-Transformer.\nPose Source PMD\n#(\u000210−4)\n1 block\n2 blocks 3 blocks 4 blocks\nSeen-pose 1.4\n1.0 0.9 0.8\nUnseen-pose 7.3 4.9 4.9 4.2\nTable 5: Effect of GC-Transformer. We evaluate the GP-\ntransformer by varying its multihead-attention block number\nwith the rest of the model untouched.\nSMG-3D datasets by using GTs as metrics. As shown in\nTable 4. We directly use the model trained on SMPL-NPT\nto conduct the pose transfer on the meshes from SMG-3D.\nThe performance of the GC-Transformer (PMD 79.2 and\n178.7) keeps outperforming compared methods (PMD 121.4\nand 240.1) as presented in Table 4. It can be seen that, by\nadopting our LIR module, all the models can effectively im-\nprove the performances which proves its efﬁciency, which\nalso proves that the inconsistency of the latent pose space\naffects the generalization of the pose transfer.\nEfﬁcacy of SMG-3D Dataset. From Table 4, we observe\nthat models trained on the synthesized SMPL-NPT dataset\ncan perform well within the same pose space (ﬁrst row of the\ntable). However, when directly transferring the model to a\nunknown space like SMG-3D, the PMD dramatically drops\ndown. This proves that a model trained with purely synthe-\nsized datasets cannot ﬁt the space distribution of challeng-\ning real-world poses. In contrast, by introducing SMG-3D\ndataset, we can train the model with semi-synthesized data\nto better ﬁt the pose space of the real-world one, as shown\nin the last line (PMD improved from 321.4 to 120.3 for NPT\nand 178.7 to 52.8 for our GC-Transformer). As indicated, a\nmodel that works on whole latent pose space is challenging\nwhich proves the necessity of our SMG-3D dataset.\nPose Transfer on Different Domain. In the end, we show\nthe robust performance of GC-Transformer on animal pose\ntransfer in Fig. 5. Our model can be directly trained on\nSMAL dataset without further modiﬁcation to adapt the non-\nhuman meshes, showing a strong generalizing ability.\nAblation Study\nExperiments are conducted to present the effectiveness of\neach proposed component on the SMPL-NPT dataset.\nEffect of GC-Transformer. We vary the number of the\nmulti-head attention blocks to show the effect brought\nby GC-Transformer in Table 5. We observe that the\nproposed GC-Transformer with four multi-head attention\nblocks works the best. However, increasing the number of\nPose Source PMD\n#(\u000210−4)\n\u0015constra=0 0.0005\n0.001 0.005 0.05\nSeen-pose 0.83 0.64 0.84\n0.92 1.13\nUnseen-pose 4.21 3.98 4.27 4.55 4.71\nTable 6: Effect of CGC loss. We validate the contribution of\nCGC loss by varying the weight of the CGC loss. As we can\nsee, the CGC loss evidently improves the geometry learning\nby more than 20%.\nblocks further requires large computational consumption\nand reaches the GPU memory limits. Thus, we adopt four\nblocks as default in our experiments.\nEffect of CGC Loss. We also validate the effect of CGC\nloss with different \u0015constra settings, as shown in Table 6. It\nshows that it gains the best performance when\u0015constra is set\nas 0.0005, which proves that our CGC loss could effectively\nimprove the geometric reconstruction results.\nLastly, we visually present the contributions made from\neach component in the GC-Transformer in Fig. 6. We dis-\nable all the key components as a Vanilla model and enable\neach step by step. Compared to the Vanilla model, the GC-\nTransformer, LIR module and CGC loss can consistently\nimprove geometric representation learning. All components\ncan be easily stacked to other existing models.\nConclusion\nWe introduce the novel GC-Transformer, as well as the CGC\nloss that can freely conduct robust pose transfer on LARGE\nmeshes at no cost which could be a boost to Transformers in\n3D ﬁelds. Besides, the SMG-3D dataset together with LIR\nmodule can tackle the problem of unstable transferring per-\nformance as the cross-dataset benchmark. New SOTA re-\nsults proves our framework’s efﬁciency in robust and gener-\nalized pose transfer. The proposed components can be easily\nextended to other 3D data processing models.\nAcknowledgements\nThis work was supported by the Academy of Finland\nfor Academy Professor project EmotionAI (grant 336116,\n345122) and project MiGA (grant 316765), EU H2020\nAI4Media (No. 951911) and Infotech Oulu projects. Au-\nthors also would like to thank the CSC-IT Center for Sci-\nence, Finland, for their computational resources.\n264\nReferences\nAumentado, T., Armstrong; Tsogkas, S.; Jepson, A.; and\nDickinson, S. 2019. Geometric Disentanglement for Gen-\nerative Latent Shape Models. In ICCV.\nBhatnagar, B. L.; Tiwari, G.; Theobalt, C.; and Pons-Moll,\nG. 2019. Multi-Garment Net: Learning to Dress 3D People\nfrom Images. In ICCV.\nBogo, F.; Kanazawa, A.; Lassner, C.; Gehler, P.; Romero, J.;\nand Black, M. J. 2016. Keep it SMPL: Automatic estimation\nof 3D human pose and shape from a single image. InECCV.\nBogo, F.; Romero, J.; Loper, M.; and Black, M. J. 2014.\nFAUST: Dataset and evaluation for 3D mesh registration. In\nCVPR.\nBogo, F.; Romero, J.; Pons-Moll, G.; and Black, M. J. 2017.\nDynamic FAUST: Registering human bodies in motion. In\nCVPR.\nChen, H.; Liu, X.; Li, X.; Shi, H.; and Zhao, G. 2019.\nAnalyze Spontaneous Gestures for Emotional Stress State\nRecognition: A Micro-gesture Dataset and Analysis with\nDeep Learning. In FG.\nChen, H.; Tang, H.; Henglin, S.; Peng, W.; Sebe, N.; and\nZhao, G. 2021a. Intrinsic-Extrinsic Preserved GANs for Un-\nsupervised 3D Pose Transfer. In ICCV.\nChen, H.; Tang, H.; Sebe, N.; and Zhao, G. 2021b. An-\niFormer: Data-driven 3D Animation withTransformer. In\nBMVC.\nCosmo, L.; Norelli, A.; Halimi, O.; Kimmel, R.; and Rodol`a,\nE. 2020. LIMP: Learning Latent Shape Representations with\nMetric Preservation Priors. ECCV.\nCrane, K.; Weischedel, C.; and Wardetzky, M. 2013.\nGeodesics in Heat: A New Approach to Computing Distance\nBased on Heat Flow. ACM TOG, 32.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In ICLR.\nEngel, N.; Belagiannis, V .; and Dietmayer, K. 2020. Point\nTransformer. arXiv preprint arXiv:2011.00931.\nGroueix, T.; Fisher, M.; Kim, V . G.; Russell, B. C.; and\nAubry, M. 2018. 3d-coded: 3d correspondences by deep de-\nformation. In ECCV.\nHuang, X.; and Belongie, S. 2017. Arbitrary Style Trans-\nfer in Real-Time with Adaptive Instance Normalization. In\nICCV.\nLi, W.; Liu, H.; Tang, H.; Wang, P.; and Van Gool, L. 2021.\nMHFormer: Multi-Hypothesis Transformer for 3D Human\nPose Estimation. arXiv preprint arXiv:2111.12707.\nLin, K.; Wang, L.; and Liu, Z. 2021. End-to-end human pose\nand mesh reconstruction with transformers. In CVPR.\nMahmood, N.; Ghorbani, N.; F. Troje, N.; Pons-Moll, G.;\nand Black, M. J. 2019. AMASS: Archive of Motion Capture\nas Surface Shapes. In ICCV.\nNash, C.; Ganin, Y .; Eslami, S. A.; and Battaglia, P. 2020.\nPolygen: An autoregressive generative model of 3d meshes.\nIn ICML.\nPark, T.; Liu, M.-Y .; Wang, T.-C.; and Zhu, J.-Y . 2019. Se-\nmantic image synthesis with spatially-adaptive normaliza-\ntion. In CVPR.\nPavlakos, G.; Choutas, V .; Ghorbani, N.; Bolkart, T.; Os-\nman, A. A. A.; Tzionas, D.; and Black, M. J. 2019. Expres-\nsive Body Capture: 3D Hands, Face, and Body from a Single\nImage. In CVPR.\nPeng, S.; Zhang, Y .; Xu, Y .; Wang, Q.; Shuai, Q.; Bao, H.;\nand Zhou, X. 2021. Neural body: Implicit neural represen-\ntations with structured latent codes for novel view synthesis\nof dynamic humans. In CVPR, 9054–9063.\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017a. Pointnet:\nDeep learning on point sets for 3d classiﬁcation and segmen-\ntation. In CVPR.\nQi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017b. Point-\nnet++: Deep hierarchical feature learning on point sets in a\nmetric space. In NeurIPS.\nRomero, J.; Tzionas, D.; and Black, M. J. 2017. Embod-\nied Hands: Modeling and Capturing Hands and Bodies To-\ngether. ACM TOG, 36.\nSengupta, A.; Budvytis, I.; and Cipolla, R. 2020. Synthetic\nTraining for Accurate 3D Human Pose and Shape Estima-\ntion in the Wild. In BMVC.\nSorkine, O. 2005. Laplacian mesh processing. In Euro-\ngraphics (State of the Art Reports).\nSumner, R. W.; and Popovi´c, J. 2004. Deformation transfer\nfor triangle meshes. ACM TOG, 23(3): 399–405.\nTan, Q.; Gao, L.; Lai, Y .-K.; and Xia, S. 2018. Variational\nautoencoders for deforming 3d mesh models. In CVPR.\nTang, H.; Bai, S.; Torr, P. H.; and Sebe, N. 2020a. Bipar-\ntite graph reasoning gans for person image generation. In\nBMVC.\nTang, H.; Bai, S.; Zhang, L.; Torr, P. H.; and Sebe, N. 2020b.\nXingGAN for Person Image Generation. In ECCV.\nUlyanov, D.; Vedaldi, A.; and Lempitsky, V . 2016. Instance\nnormalization: The missing ingredient for fast stylization.\narXiv preprint arXiv:1607.08022.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, u.; and Polosukhin, I. 2017. At-\ntention is All You Need. In NeurIPS.\nWang, J.; Wen, C.; Fu, Y .; Lin, H.; Zou, T.; Xue, X.; and\nZhang, Y . 2020. Neural Pose Transfer by Spatially Adaptive\nInstance Normalization. In CVPR.\nWang, N.; Zhang, Y .; Li, Z.; Fu, Y .; Liu, W.; and Jiang, Y .-G.\n2018a. Pixel2mesh: Generating 3d mesh models from single\nrgb images. In ECCV.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018b. Non-\nlocal neural networks. In CVPR.\nYang, G.; Tang, H.; Ding, M.; Sebe, N.; and Ricci, E. 2021.\nTransformer-based attention networks for continuous pixel-\nwise prediction. In ICCV, 16269–16279.\nYu, Z.; Wan, J.; Qin, Y .; Li, X.; Li, S. Z.; and Zhao, G. 2021a.\nNas-fas: Static-dynamic central difference network search\nfor face anti-spooﬁng. IEEE TPAMI.\n265\nYu, Z.; Zhao, C.; Wang, Z.; Qin, Y .; Su, Z.; Li, X.; Zhou,\nF.; and Zhao, G. 2020. Searching central difference convo-\nlutional networks for face anti-spooﬁng. In CVPR, 5295–\n5305.\nYu, Z.; Zhou, B.; Wan, J.; Wang, P.; Chen, H.; Liu, X.; Li,\nS. Z.; and Zhao, G. 2021b. Searching multi-rate and multi-\nmodal temporal enhanced networks for gesture recognition.\nIEEE Transactions on Image Processing.\nZhou, K.; Bhatnagar, B. L.; and Pons-Moll, G. 2020. Unsu-\npervised Shape and Pose Disentanglement for 3D Meshes.\nIn ECCV.\nZhou, Y .; Wu, C.; Li, Z.; Cao, C.; Ye, Y .; Saragih, J.; Li, H.;\nand Sheikh, Y . 2020. Fully convolutional mesh autoencoder\nusing efﬁcient spatially varying kernels. In NerIPS.\nZufﬁ, S.; Kanazawa, A.; Jacobs, D. W.; and Black, M. J.\n2017. 3D menagerie: Modeling the 3D shape and pose of\nanimals. In CVPR.\n266"
}