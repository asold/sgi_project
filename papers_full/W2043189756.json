{
  "title": "Immersive Performance Training Tools Using Motion Capture Technology",
  "url": "https://openalex.org/W2043189756",
  "year": 2007,
  "authors": [
    {
      "id": "https://openalex.org/A5020846028",
      "name": "Jacky C. P. Chan",
      "affiliations": [
        "City University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5001617092",
      "name": "Howard Leung",
      "affiliations": [
        "City University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5011214641",
      "name": "Kai Tai Tang",
      "affiliations": [
        "City University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5014324841",
      "name": "Taku Komura",
      "affiliations": [
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1989373513",
    "https://openalex.org/W2087496489",
    "https://openalex.org/W2086032654",
    "https://openalex.org/W2065181375",
    "https://openalex.org/W2145475103",
    "https://openalex.org/W2044309259",
    "https://openalex.org/W2113594156",
    "https://openalex.org/W1939757736",
    "https://openalex.org/W2100018893",
    "https://openalex.org/W2070276364",
    "https://openalex.org/W4238646661",
    "https://openalex.org/W2106459977"
  ],
  "abstract": "Traditionally, students can undergo performance training by imitating teachers’ motions and improving their moves by following teachers’ advice. However, teachers are not always available and there is a lack of effective self-training tools. In this paper, we propose an immersive performance trainin",
  "full_text": "Immersive Performance Training Tools Using Motion \nCapture Technology \nJacky Chan \nDepartment of Computer \nScience \nCity University of Hong Kong \n83 Tat Chee Avenue,  \nKowloon, Hong Kong \n+852-21942925 \njcpchan@cityu.edu.hk \nHoward Leung \nDepartment of Computer \nScience \nCity University of Hong Kong \n83 Tat Chee Avenue,  \nKowloon, Hong Kong \n+852-27887234 \nhoward@cityu.edu.hk \n Kai Tai Tang \nDepartment of Computer \nScience \nCity University of Hong Kong \n83 Tat Chee Avenue,  \nKowloon, Hong Kong \n+852-21942837 \nitjeff@cityu.edu.hk\n Taku Komura \nSchool of Informatics \nThe University of Edinburgh \nIPAB JCMB, Kings Buildings, \nMayfield Rd, Edinburgh \nEH9 3JZ, U.K.  \n+44 (0)131 651 3445   \ntkomura@inf.ed.ac.uk\n \nABSTRACT \nTraditionally, students can undergo performance training by \nimitating teachers’ motions and improving their moves by \nfollowing teachers’ advice. However, teachers are not always \navailable and there is a lack of effective self-training tools. In this \npaper, we propose an immersive performance training system with \nmotion capture technology. Our aim is to provide a virtual training \nenvironment for performance traini ng. In the environment, the \nstudent’s performance is analyzed by our system and a virtual \nteacher will give feedback for his/her improvement. As a result, \nthe student can self-train and get feedback the same way as in a \ntraditional performance training lesson. We will present our \nprototype implementation of a dance education system according \nto our design of immersive performance training system with \nmotion capture technology. \nCategories and Subject Descriptors \nJ.5 [Performing Arts], K.3.1 [Computer Uses in Education] \nGeneral Terms \nDesign, Human Factors, Measurement, Performance \nKeywords \nImmersive VR application, Huma n Computer Interaction, 3D \nHuman Motion Analysis, Performance Training. \n1. INTRODUCTION \nWith the recent advance in computer graphic and motion capture \ntechnology, immersive virtual reality (VR) applications gain much \nattention in both academics and industry. Virtual environment is \nbecoming more and more realistic. Through an immersive VR \napplication, people may experience a real-world-like or an \nimaginative virtual world. Diffe rent kinds of immersive VR \napplications have been developed for entertainment, martial arts, \netc [1][2].  \nAn immersive training system is potentially a complement to real-\nworld training. It can simulate an environment that may be \nunreachable or unrealizable in the real world. Trainee/learner can \ngain some experiences before they actually start training or \nlearning in the real world, especially for trainings that may be \ndangerous or impossible to carry out in the real world such as \nmilitary training. US army had invented an immersive VR \napplication that their soldiers could learn performing checkpoint \nduty [3]. Chou et al.  applied virtual reality in civil engineering \neducation that provides an alterative for students to learn \nstructural analysis as these c oncepts are difficult to understand \nwhen they are visualized by 2D wire-frame diagrams [4]. \nSome immersive VR applications target on motion training. With \nmotion capture technology, user can interact with the computer by \nbody movement. Chua et. al. proposed an immersive VR motion \ntraining system using motion capture technology for practicing \nTai Chi [5]. The avatar of the Tai Chi learner and his/her \ninstructor are put together with  in a virtual environment. The \nstudent learns by mimicking th e instructor’s motion and knows \nwhat is wrong with their motion by observing his/her own \nmovement from the avatar. Hachimura et. al.  proposed a dance \ntraining system with mixed reality [6] that merge motion capture \nand virtual reality techniques. The head mounted display shows \nthe expert’s dance motion in front of a trainee like a “Ghost”. The \nlearner can observe the 3D animation demonstrated by the expert \nand move his/her body until perfectly overlapped with the expert’s \n3D avatar. In other words, a learner should follow the postures of \nthe expert. In these two app lications, although learners can \nobserve their own movement by obs erving the virtual avatar, they \nmay not have enough experience to identify whether they have \nperformed wrongly. In our design, learners can concentrate on \ntheir training while our system will be an observer and provide \nfeedback to them. \nThe paper is organized as fo llows. The proposed performance \ntraining system is described in section 2. A showcase of our dance \neducation system is presented as an example performance training \napplication in section 3. Conclusions and future work are provided \nin section 4. \n2. PROPOSED SYSTEM DESCRIPTION \nThe objective of our system is to provide a virtual environment for \nperformance training. In this paper, we aim to teach people to \ndance properly. We use a motion capture system to capture the 3D \nmotion of the student. Meanwhile, an animated avatar is rendered \nin a virtual classroom. There is also a virtual teacher demonstrates \nthe moves to the student and gives feedback after analyzing the \nstudent’s performance. The student  is immersed in the virtual \nenvironment and feels like a real training lesson. \nFigure 1 shows how a user interacts with our proposed system. \nOur system consists of three part s: 1) Training Room, 2) Virtual \nClassroom and 3) Data S erver. Training room  is  the area where \ntraining really takes place. It contains  all neces sary equipm ents \nsuch as  m otion capture s ystem, s peakers, and s creen, etc. The \nequipments provide a sense of immersion to the student and by the \nway collect his/her motion data. Virtual classroom is a virtual \nenvironment where dis plays the virtual teacher and the s tudent’s \nanimated avatar. It renders  both pre-s tored and real-tim e \nanimations. Data server stores the data of the perform ance lesson \nincludes the m otion templates, performance names, and \nperformance types, etc. \n \nFigure 1. The user interaction diagram of our proposed \nperformance training tool \n2.1 Training Room \nFigure 2 shows the detailed user in teraction in the training room \nenvironment. In a training room , various equipments support the \ninteraction between the student and the virtual teacher. During the \nlesson, the student’s motions are captured as input data, and hence \nrendered by  an anim ated avatar. Meanwhile, the student could \nobserve the teacher’s moves through the screen and listen to the \ninstruction through the speakers. By  the way , the student receives \nfeedback. \n \nFigure 2. The user interaction diagram in the classroom \nenvironment \n2.1.1 Real Time Motion Capture \nIn our sy stem, we use optical motion capture system. Figure 3 \nshows our helper who wore a suit with markers attached and \nsurrounded by  motion capture cameras. Those cameras are \nmounted on surrounding walls of the training room. The student \nhas to wear a s uit attached with  reflective m arkers. W hen the \nmotion capture sy stem is running, the cam era will em it light and \ntrack the positions of the markers through the reflected light. \nCombining different views from a ll cam eras, the 3D coordinates  \nof m arkers are obtained. W hen the training starts, the system \ncaptures the 3D motion data of the student in real time manner and \nthe data is continuously sent to our system for processing. \n \nFigure 3. An optical motion capture system with cameras and \na suit with markers attached \n2.1.2 Display \nThe sy stem renders the virtual classroom and display s on the \nscreen. The virtual teacher dem onstrates the s tandard m oves and \nprovides feedback to the user about how good he/she had \nperformed. M oreover, the s tudent’s anim ated avatar is rendered \naccording to the captured motion data in real-tim e manner. As the \nstudent can control his/her own avatar in the virtual classroom, the \nstudent will feel him /herself im mersed into the virtual \nenvironment. \n2.1.3 Sound \nSpeakers are s etup in the training room  as  some performances \ninvolve music. During each practi ce or demonstration session, the \ncorresponding music can be played in the training room. \n2.2 Virtual Classroom \nVirtual classroom is a place where the virtual teacher and s tudents \ninteract. In the virtual classroom , the virtual teacher will react \naccording to the student’s action and the student will follow the \nsuggested training plan.  \n2.2.1 Setup \n3D motions of real teachers were captured and pre-s tored in the \nserver to form template lessons. At the beginning, the student can \nsee the name and ty pe of av ailable performance lessons and \nchoose one of them to perform trai ning. After selecting the lesson, \nthe corresponding data will be retrieved from  the data server. At \nthe same time, the sy stem will check whether the motion capture \nsystem is online and ready  to capture the s tudent’s motion. When \neverything is ready, the virtual classroom will be established. \n2.2.2 Interaction \nInteraction between the s tudent and the virtual teacher is  one of \nthe important features that makes the student immersed into our \nsystem and helps him/her to achieve better perform ance. The \nvirtual teacher will give suggesti ons about the student’s training \nbased on his /her current perform ance. The virtual teacher m ay \neither suggest a training plan or specify  particular moves for the \nstudent to practice. \n2.2.3 Personal Training Plan \nThe teacher in a real perform ance lesson will suggest a personal \ntraining plan for students. Student s will practice basic m oves first. \nAfter they  get fam iliar with thes e basic m oves, the teacher will \nteach them to com bine the learned m oves. S tudents m ay \ndemonstrate different talents when performing various moves. For \nexample, some students may  ha ve difficulty  in moving legs \nquickly so they should focus more on practicing the leg motions.  \nThe trainings in our s ystem are designed in a sense that follows \nthe normal flow in a real perform ance training lesson. There are \nthree stages of trainings: 1) m otion prim itives training, 2) \nintermediate training and 3) whole motion sequence training. \nAfter analyzing the s tudent’s motion, our s ystem can identify  the \nweaknesses of the student and s uggest him/her to train weak \nmotions more often. \n \nFigure 4. Flowchart of Stage 1: motion primitives training \n2.2.3.1 Stage 1: Motion Primitives Training \nIn the first stage of each lesson, the virtual teacher will \ndemonstrate the basic moves of a performance. The bas ic moves, \nor motion primitives, are ordered according to the difficulty  level \nthat depends on the difficulty of body movement.  Figure 4 shows \nthe flow of m otion prim itives training. After the student had \ncompleted learning one move, the virtual teacher will dem onstrate \nnext m ove and the student should follow the way  the teacher \nperformed. If the student does not perform well, the virtual teacher \nwill tell the student how to im prove until he/she can m aster the \nmove. This stage will continue until the student can do well in all \nmotion primitives. \n2.2.3.2 Stage 2: Intermediate Training \nIn the second stage, the student is required to practice longer \nmotion segments. These motion segm ents are parts  of the whole \nmotion sequence and they  are formed by combining motion \nprimitives. By  performing training on m otion segments, students \nwill be able to link several m otion prim itives together. If the \nstudent fails to perform particul ar motion segment, our sy stem \nwill point out the erroneous m otion segm ent and break it down \ninto smaller sub-segments that aids the student to practice. If the \nstudent still performs the sub-segment motion poorly, the sy stem \nthen prompts the student to go back to the first stage for motion \nprimitives training again.  \n2.2.3.3 Stage 3: Whole Motion Sequence Training \nIn the last stage, the student attem pts to perform the whole motion \nsequence. Figure 5 shows the flowchart. The student may  perform \nthe motion incorrectly in several way s such as forgetting som e \nmoves, mixing the order of prim itive motions or having difficulty  \nin linking up the m otion segments. The sy stem will identify and \nlocate the error when the s tudent perform s the whole motion \nsequence. The virtual teacher will suggest the student to go back \nto the previous  s tage and practice the m otion s egments that \ncontain significant errors. On the other hand, if there are no \nsignificant erroneous m otion segm ents, then the student will \nreceive an evaluation about his/her overall performance. The \nstudent can keep practicing in this stage to improve the \nsmoothness of his/her movement. Th e training is com plete when \nthe system satisfies with the student’s performance. \n \nFigure 5. Flowchart of Stage 3: whole motion sequence \ntraining \n2.2.4 Feedback \nThroughout the training, motions of student are captured and \nevaluated. The sy stem will identify  erroneous joints and provide \nsuggestion in the training plan. The feedback is provided in the \nform of text dialogue. This will give students the impression that \nthe virtual teacher is  hum an-like. The text feedback like “ Your \nmovement on LEFT HAND is not correct” is more descriptive and \ndirect than a sim ple similarity score. From this kind of feedback, \nthe student can realize their errors immediately  and know how to \ncorrect the errors . F igure 6 s hows the feedback from virtual \nteacher on the dialog box.  \n \nFigure 6. An example of feedback dialogue by virtual teacher \n2.3 Data Server \nTwo types of data are stored in the server for defining a lesson: 1) \nlesson data and 2) 3D avatar models. Before the training, our \nsystem retrieves the required data from the server. \n2.3.1 Lesson \nEach tem plate lesson contains motion data, name, performance \ntype and background music. The lesson data is pre-stored. Motion \ndata is prepared through capturing the motions performed by  \nprofessionals. The m otion data will be analy zed and divided into \nmotion segments and m otion primitives that are important moves \nand repetitive in the whole m otion sequence. Motion data, motion \nprimitives and motion segments will be stored as template motion. \n2.3.2 Avatar Model \nDifferent kinds of outfit are su itable for different types of \nperformance. To make our virtual teachers look more professional \nand include more varieties, the sy stem offers different kinds of \navatars to s uit different perform ances. F or example, a clown \navatar would be suitable for performing funny  motion as shown in \nFigure 6.  \n3. APPLICATION SHOWCASE: DANCE \nEDUCATION SYSTEM \nWe have implemented a dance edu cation system as a prototy pe of \nthe proposed immersive performance training sy stem. We choose \ndance education for our application becaus e dance is  one of the \nmost popular performances and whole body  is involved in the \nmovement. The virtual teacher dem onstrates the standard way of \ndancing (the template motion) and then the student is instructed to \nperform likewise. The 3D m otion of student is captured by  the \noptical motion capture sy stem. Figure 7 shows a student learning \ndance through our dance education sy stem. The screen shows a \nvirtual teacher on the left hand s ide and the student’s own virtual \navatar on the right hand side. The student is trying to imitate the \nteacher’s motion. The sy stem keeps capturing and updating the \nstudent’s poses that drives  the avatar to m ove accordingly . The \nmotion data will be analyzed and the virtual teacher will feedback \nthe student what is wrong with his/her performance. \n \nFigure 7. An example showing a student undergoes training \nwith our Dance Education System \n3.1 Rendering \nThe 3D animation is rendered by  OpenGL, which is a popular 3D \ngraphic toolbox that supports most platforms like C++. The \nrendering result is projected onto the screen. \n3.2 Demonstration of Moves \nTemplate lessons are pre-s et by the real teacher. The lesson data \ncontains the lesson name, description, dance sty le and template \ndance m otion perform ed by  the teacher. In a dance motion \nsequence, some m oves are often repeated and we m ark them  as  \nmotion primitives. Tem plate dance m otions are divided into \ndifferent motion primitives. The dance m otion data are sorted by \ndifficulty that determ ines the frequency and range of movement. \nFigure 8 shows a 3D virtual teacher demonstrating certain moves. \n \nFigure 8. Dance demonstration by a 3D virtual teacher \n3.3 Analysis and Feedback \nNakamura et. al  proposed a dance training tools with a timing-\nvibro device and a robotic screen [7]. These two devices  are us ed \nfor showing the timing informati on and the amount of translation \nin a dance perform ance. S tudents’ m otions are anim ated in real-\ntime on the screen and they  get feedback on their movement \npattern. This application is sim ilar to our proposed design in the \nsense that we als o provide real-tim e animation and feedback for \nthe captured motion.  \nHachimura et. al  proposed to use weight, space and time for \nanalysis and evaluation of danci ng movement [8] . Weight relates \nto the kinetic energy  of each part of the body . Space refers to the \ndirection of movement of body  as a whole. Time incorporates the \nrate of movement, i.e., the acceleration of each part of the body . \nBy extracting these features from the two motions to be compared, \nthe time instants at which the two motions differ can be found. \nThis approach performs a global matching between the postures \nbut does not localize the mismatches. \nIn recent researches, angles between joints or different parts of the \nbody are used for analy zing specific dance motions. For the \nJapanese dance Furi, four s patial indices  are calculated for \nquantifying the characteristic features  [9] . Thes e four indices  \nshow significant differences betw een an expert’s motion and a \nbeginner’s motion. A gesture recognition engine in a dance system \nalso presents similar design [10]. It matches two gestures by  \nconsidering the joint angles of 10 body parts including head, torso, \nupper arms, forearms, upper legs and lower legs but ignoring \nhands and feet. The similarity  between two ges tures is calculated \nbased on the Mahalanobis distance. Kwon et. al.  designed a \nmotion training system using visual and body sensors [1]. In their \napproach, the body sensor is attached to the forearm for measuring \ntwo angular values, pitch and roll, in a motion. They compare \nsimilarity of two m otions by using the Euclidean distance metric. \nParticular joint angles are considered in above researches for \nanalysis of s pecific dance m otions. W e would like to consider \nmore joint angles of the whole body  and develop a matching \nalgorithm for handling more general dance motions. \nIn our approach, the m otion capture s ystem is used to collect the \nmotion data whose joint angles are compared in a frame-by -frame \nanalysis. A frame contains information about a set of joints \nspecifying a pos ture. The m otion of the virtual teacher, or the \ntemplate motion, is displayed on the screen whereas the m otion of \nthe student is captured in real tim e. Since the student is try ing to \nimitate the virtual teacher’s  m otion, each frame of the student \nmotion in terms of joint angles should be compared with the \ncorresponding fram e of the virtual teacher’s motion being \ndisplayed. After analy sis, feedback will be given by  the virtual \nteacher to the s tudent. Different follow-up actions may be \ntriggered depending on the analysis result at each stage. \n3.3.1 Comparison of Joint Angle \nEach frame consisted of joint a ngles of the student’s motion is \ncompared with the corresponding fram e of the virtual teacher’s. A \nframe is indeed a representation for a posture. Posture matching is \nperformed for each pair of joint angle sets contained in the frames. \nMany joints are considered in the m atching process. For example, \ntorso is taken into account since the rotation along the body’s \nprinciple axis is important in dance perform ance s uch as  ballet. \nFor comparing the template motion and the student’s captured \nmotion, in total 33 pairs of joint angles are used. The relative \nchange Rij for the i-th joint angle between the template motion and \nthe student’s motion at frame j is computed by equation (1): \nijijijij AABR /|| −=   (1)  \nwhere Bij is the i-th joint angle of the j-th frame of the student’s \nmotion, Aij is the i-th joint angle of j-th fram e of the template \nmotion, and i = 1,2,3,…,33.  \nA validity index matrix V is computed by the set of R values and a \nsatisfactory index S. The elem ents of the validity  index m atrix Vij \nfor the i-th joint at frame j is computed by equation (2): \n⎩\n⎨\n⎧\n>\n<\n= SR\nSR\nV\nij\nij\nij         0\n          1\n  (2)  \nThe satisfactory index S is set to be 0.2. The validity is set to be 1 \nif the relative change is sm aller than the satisfactory  index \nmeaning that the move of the partic ular body part at the particular \ntime instant is valid. In other words, the satisfactory  index \nspecifies the error tolerance when the matching is not perfect. \nThere are two reas ons for the im perfect m atching res ults. F irst, \neven when two people try  to perform the sam e posture, there will \nbe some differences due to th eir unequal body  shapes. Second, \nthere may be som e delay fluctuations when a student im itates the \nvirtual teacher’s moves shown on the screen. The value of 0 for Vij \nindicates that the i-th joint at frame j is poorly matched.  \n3.3.2 Follow-up Actions at Each Stage \nIn the firs t stage of training, the s tudent practices on each m otion \nprimitive. There are two possible follow-up actions based on the \nanalysis result. The first kind of actions is to give advice on \nimproving the particular m otion primitive. After the analy sis, we \ncan identify  the validity  Vij that is equal to 0 m eaning that the \nmove is invalid. The subscript i specifies which joint m oves \nwrongly so the erroneous limb can be identified. The second kind \nof actions is to dem onstrate the next m otion prim itive. W hen a \nmotion primitives are marked as correct, i.e., when all elem ents of \nthe validity matrix V are equal to 1, the student is considered to be \nfamiliar with that m otion prim itive. The teacher will thus \ndemonstrate a new move or start the second stage. \nIn the second stage, the student  is asked to perform a motion \nsegment by linking several motion primitives. Following a similar \nprocess as  in the firs t s tage, we locate thos e fram es that contain \nerrors by checking which elem ent in the validity  m atrix Vij is \nequal to 0. The student will be no tified about his/her errors and be \nasked to perform the same moti on segment again. If the student \ndemonstrates recurring errors while performing motion primitive \nX, then the follow-up action is to bring the student back to the first \nstage and focus on training for the motion primitive X. \nIn the third stage, the student is required to perform the whole \nmotion sequence. Sim ilar to the previous two stages, we first \nlocate those frames that contain errors  by checking for the values \nof 0 in the validity  m atrix V. Then we identify  the m otion \nsegments that contain thos e fram es. In this  case, the follow-up \naction is to ask the student to go back to the second stage and \nwork on the identified motion segments. On the other hand, if no \nerrors are reflected by the validity matrix, then the student can get \nan evaluation about his/her overall performance. The follow-up \naction in this case is to let th e student practice the whole motion \nsequence again to improve the smoothness. The training is \ncomplete when the student is satisfied with the performance. \n4. Experiment \nTo test the usability  of our prototy pe system, we have invited 9 \nsubjects to try  out our sy stem and fill in the questionnaires \nafterward. Some of the questions are listed below at Table 1. \nSubjects rated for each ques tion in order to s how if they  \nagree/disagree the proposition. For the graphs shown in Figure 9, \nthe x-axis corresponds to the user score and the y-axis corresponds \nto the number of users giving the score. It can be seen in Figure 9 \nthat all the question got score 4 or above showing that all the \nsubjects are satisfied with our system. \nTable 1. Some example questions in the survey \nNo. Questions \n3 The system can help y ou know what and wher e mistakes ar e in \nyour dancing. \n5 The virtual teacher anim ations are interesting and they can \nstimulate your interest to use th is sy stem and to obser ve their  \nmotions. \n6 You prefer lear ning dancing with the sy stem to tr aditional \n“watching dance video” approach. \n9 The interaction part in the system assisted you in dancing training \nprocess. \n \nQuestion 3\n0\n2\n4\n6\n8\n12 34 5 6\n \nQuestion 5\n0\n2\n4\n6\n8\n123 456\nQuestion 6\n0\n2\n4\n6\n8\n12 34 5 6\n \nQuestion 9\n0\n2\n4\n6\n8\n1 234 5 6\nFigure 9. Subjects’ response for survey’s questions \n5. CONCLUSIONS AND FUTURE WORK \nIn this paper, we have pr oposed an immersive performance \ntraining tool using the motion captu re system. It provides a virtual \nenvironment for students to undergo performance training. With \nthe motion capture system, a student can control his /her animated \navatar in the virtual classroom and interact with the virtual teacher \nwho will assist him /her in the training process. Based on the \nanalysis of the captured 3D motion of the student, the virtual \nteacher will give feedback for the student to make improvement. \nThe virtual teacher will point out m istakes made in the student’s \nmove, identify the student’s weakness and suggest him/her to pay  \nattention on particular limbs for particular moves. \nAs future work, the template lesson generation will be autom ated \nby considering pattern recogniti on techniques. Moreover, we will \nexplore various ways to improve the matching algorithm. We will \ntake into account the fluctuations in delay s when the student is \nimitating the virtual teacher’s m otion. The analy sis result will be \nfurther exploited to provide even more meaningful feedback. For \nexample, when the student perfo rms a wrong move at the knee, \nour system will be able to identify if his/her leg is required to bend \nmore or less. \n6. ACKNOWLEDGMENTS \nThe work described in this paper was fully  supported by a grant \nfrom the Research Grants Councils of the Hong Kong Special \nAdministration Region, China (Project No. CityU 1167/05E). \n7. REFERENCES \n[1] Ronald Sidharta and Carolina Cruz-Neira, “Cy clone \nUppercut, A Boxing Game for an Immersive Environment”, \nProc. of the 2005 ACM SIGCHI Intl. Conf. on Advances in \ncomputer entertainment technology, pp.363-364, 2005. \n[2] Zhenyu Yang, Bin Yu, Ross Diankov, Wanmin Wu and \nRuzena Bajscy , “ Collaborative Dancing in Tele-immersive \nEnvironment”, Proceedings of the 14th annual ACM intl. \nConf. on Multimedia, pp.732-726, 2006. \n[3] R.Bowen Loftin, M ark W .Scerbo, F rederic D. M chKenzie \nand J ean M . Catanzaro, “ Training in P eacekeeping \nOperations Using Virtual Environments”,  IEEE Computer \nGraphic and Applications, Vol. 24, pp.18-21, July 2004. \n[4] Chien Chou, Hsieh-Lung Hsu and Yu-Seng Yao, \n“Construction of a Virtual Reality Learning Environm ent for \nTeaching S tructural Analy sis”, Computers in Industry, V ol. \n51, pp.31-40,  May 2003. \n[5] Philo Tan Chua, Rebecca Crivella, Bo Daly , Ning Hu, Russ \nSchaaf, David Ventura, Todd Camill, Jessica Hodgins and \nRandy Pausch, “Tai Chi: Training for Phy sical Tasks in \nVirtual Environments”, Virtual Reality, 2003. Proc. IEEE , \npp.87-94, 2003. \n[6] Koaburo Hachimura, Hiromu Kato and Hideyuki Tamura, “A \nPrototype Dance Training Support Sy stem with Motion \nCapture and Mixed Reality  Technologies”, Proc. of the 2004 \nIEEE Intl. Workshop on Robot and Human Interactive \nCommunication, pp.217-222, 2004. \n[7] Akio Nakamura, Sou Tabata, Tomoy a Ueda, Shinichiro \nKiyofuji and Yoshinori Kuno, “Dance Training Sy stem with \nActive Vibro-Devices  and a M obile Image Display”, 2005 \nIEEE/RSJ Intl. Conf. on Intelligent Robots and Systems, 2005 \n(IROS 2005), pp.3075-3080, Aug 2005. \n[8] Kozahuro Hachimura, Kats umi Takashina and Mitsu \nYoshimura, “Analysis and Evaluation of Dancing Movement \nBased on LMA”, 2005 IEEE Intl. Workshop on Robots and \nHuman Interactive Communication, pp.294-299, 2005. \n[9] Mitsu Yoshimura, Norio Mine, Tamiko Kai and Lsao \nYoshimura, “Quantification of Characteris tic F eatures of \nJapanese Dance for Individuality  Recognition”, Proceedings. \n10th IEEE Intl. Workshop on Robot and Human Interactive \nCommunication, pp.193-199, Sept. 2001. \n[10] Gang Qian, Feng Guo, Todd Ingalls, Loren Olson, Jody \nJames and Thanasis Rikakis, “ A Gesture-Driven Multim odal \nInteractive Dance Sy stem”,  2004 IEEE Intl. Conf. on \nMultimedia and Expo ( ICME 2004) , pp.1579-1582, June \n2004. \n[11] Doo Young Kwon and Markus Cross, “Combining Body  \nSensors and Visual Sensors for Motion Training”, Proc. of \nthe 2005 ACM SIGCHI Intl. Conf. on Advances in computer \nentertainment technology, pp.94-101, 2005. \n ",
  "topic": "Training (meteorology)",
  "concepts": [
    {
      "name": "Training (meteorology)",
      "score": 0.7629201412200928
    },
    {
      "name": "Computer science",
      "score": 0.7427201271057129
    },
    {
      "name": "Motion capture",
      "score": 0.6000919938087463
    },
    {
      "name": "Motion (physics)",
      "score": 0.5774720907211304
    },
    {
      "name": "Multimedia",
      "score": 0.5323750376701355
    },
    {
      "name": "Human–computer interaction",
      "score": 0.514507532119751
    },
    {
      "name": "Virtual reality",
      "score": 0.43132007122039795
    },
    {
      "name": "Artificial intelligence",
      "score": 0.25125065445899963
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I168719708",
      "name": "City University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    }
  ],
  "cited_by": 8
}