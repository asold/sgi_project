{
  "title": "FedET: A Communication-Efficient Federated Class-Incremental Learning Framework Based on Enhanced Transformer",
  "url": "https://openalex.org/W4385767986",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2100941586",
      "name": "Cheng-Hao Liu",
      "affiliations": [
        "Tsinghua University",
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2226081338",
      "name": "Qu Xiaoyang",
      "affiliations": [
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2170577868",
      "name": "Jianzong Wang",
      "affiliations": [
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2047540789",
      "name": "Jing Xiao",
      "affiliations": [
        "Ping An (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3100156920",
    "https://openalex.org/W3168511142",
    "https://openalex.org/W4288336773",
    "https://openalex.org/W2134797427",
    "https://openalex.org/W3171057731",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4287285787",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W4287082897",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3030364939",
    "https://openalex.org/W4206178588",
    "https://openalex.org/W3035542229",
    "https://openalex.org/W2265846598",
    "https://openalex.org/W3175516982",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4312769405",
    "https://openalex.org/W3013325675",
    "https://openalex.org/W2911681509",
    "https://openalex.org/W4388820220",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W4287936108",
    "https://openalex.org/W2962966271",
    "https://openalex.org/W3157503981",
    "https://openalex.org/W4214924370",
    "https://openalex.org/W2964189064",
    "https://openalex.org/W2954929116",
    "https://openalex.org/W4239072543"
  ],
  "abstract": "Federated Learning (FL) has been widely concerned for it enables decentralized learning while ensuring data privacy. However, most existing methods unrealistically assume that the classes encountered by local clients are fixed over time. After learning new classes, this impractical assumption will make the model's catastrophic forgetting of old classes significantly severe. Moreover, due to the limitation of communication cost, it is challenging to use large-scale models in FL, which will affect the prediction accuracy. To address these challenges, we propose a novel framework, Federated Enhanced Transformer (FedET), which simultaneously achieves high accuracy and low communication cost. Specifically, FedET uses Enhancer, a tiny module, to absorb and communicate new knowledge, and applies pre-trained Transformers combined with different Enhancers to ensure high precision on various tasks. To address local forgetting caused by new classes of new tasks and global forgetting brought by non-i.i.d class imbalance across different local clients, we proposed an Enhancer distillation method to modify the imbalance between old and new knowledge and repair the non-i.i.d. problem. Experimental results demonstrate that FedET's average accuracy on a representative benchmark dataset is 14.1% higher than the state-of-the-art method, while FedET saves 90% of the communication cost compared to the previous method.",
  "full_text": "FedET: A Communication-Efficient Federated Class-Incremental Learning\nFramework Based on Enhanced Transformer\nChenghao Liu1,2 , Xiaoyang Qu1 , Jianzong Wang1‚àó and Jing Xiao1\n1Ping An Technology (Shenzhen) Co., Ltd., Shenzhen, China\n2The Shenzhen International Graduate School, Tsinghua University, China\nliucheng21@mails.tsinghua.com.cn, {quxiaoyang343, wangjianzong347, xiaojing661}@pingan.com.cn\nAbstract\nFederated Learning (FL) has been widely con-\ncerned for it enables decentralized learning while\nensuring data privacy. However, most existing\nmethods unrealistically assume that the classes en-\ncountered by local clients are fixed over time. Af-\nter learning new classes, this assumption will make\nthe model‚Äôs catastrophic forgetting of old classes\nsignificantly severe. Moreover, due to the limi-\ntation of communication cost, it is challenging to\nuse large-scale models in FL, which will affect\nthe prediction accuracy. To address these chal-\nlenges, we propose a novel framework, Federated\nEnhanced Transformer (FedET), which simultane-\nously achieves high accuracy and low communica-\ntion cost. Specifically, FedET uses Enhancer, a tiny\nmodule, to absorb and communicate new knowl-\nedge, and applies pre-trained Transformers com-\nbined with different Enhancers to ensure high pre-\ncision on various tasks. To address local forgetting\ncaused by new classes of new tasks and global for-\ngetting brought by non-i.i.d (non-independent and\nidentically distributed) class imbalance across dif-\nferent local clients, we proposed an Enhancer dis-\ntillation method to modify the imbalance between\nold and new knowledge and repair the non-i.i.d.\nproblem. Experimental results demonstrate that\nFedET‚Äôs average accuracy on representative bench-\nmark datasets is 14.1% higher than the state-of-the-\nart method, while FedET saves 90% of the commu-\nnication cost compared to the previous method.\n1 Introduction\nFederated learning (FL) enables each participating local\nclient to benefit from other clients‚Äô data while ensuring\nclient‚Äôs data does not leave the local [Yang et al., 2019;\nDong et al., 2023]. On the premise of ensuring the data pri-\nvacy of all clients, the problem of data silos has been success-\nfully solved [Hong et al., 2021; Qu et al., 2020 ]. However,\nmost existing FL methods are modelled in static scenarios,\n‚àóCorresponding author: Jianzong Wang, jzwang@188.com\nmeaning the models‚Äô classes are preset and fixed, which un-\ndoubtedly reduces the model‚Äôs generality. Therefore, Feder-\nated Class-Incremental Learning (FCIL) is proposed. FCIL\nsolves the problem that FL needs to retrain the entire model\nwhen meeting the new classes, saving time and computing\ncosts. For FCIL, how to deal with catastrophic forgetting,\nseek the plasticity-stability balance of the model and ensure\nthe cooperation of multiple parties are the keys to the prob-\nlem.\nTo date, less work has been done on FCIL studies. The re-\nsearch conducted by [Hendryx et al., 2021] focuses on global\nIL by facilitating knowledge sharing among diverse clients.\nHowever, the author overlooks the non-i.i.d distribution of\nclasses across these distinct clients. The paper [Dong et al.,\n2022] draw on the regularization methods used in Incremen-\ntal Learning (IL) and proposes two loss functions. One for\naddressing the issue of forgetting old classes after IL, and the\nother is concentrate on the global forgetting caused by the\nnon-i.i.d (non-independent and identically distributed) dis-\ntribution of classes among different clients. However, this\nmethod needs a proxy server to achieve its best performance,\nleading to high communication costs and some privacy is-\nsues. To raise the accuracy of the model in FCIL settings, a\nnatural idea is to choose a more powerful backbone model.\nWe note that there is still no work to apply transformers to\nFCIL, and the biggest obstacle is that the communication cost\nis extremely high and cannot be reduced, which makes this\napplication unrealistic. From another perspective, the accu-\nracy and application scope will be significantly improved if\nwe solve the communication and non-i.i.d. problem of class\ndistribution between different clients.\nDriven by these ideas, we propose a new Fed\nerated\nEnhanced Transformer (FedET) framework. Compared with\nother existing FCIL methods, FedET has better prediction\nperformance, lower communication volume, and more uni-\nversality. It has achieved excellent performance in both\nComputer Vision (CV) and Natural Language Process (NLP)\nfields, also it is more efficient when dealing with catastrophic\nforgetting. FedET consists of four main components: Pre-\ntrained Transformer Blocks, Enhancer Select Module, En-\nhancer Pool and Sample Memory Module (only the local\nclients have the Sample Memory Module). FedET first di-\nvides the entire label space into multiple domains, each with\nits corresponding Enhancer Group. When new classes need to\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3984\nlearn, Enhancer Select Module will determine which domain\nthe new classes belong to and train a temporary Enhancer\nGroup. The new Enhancer Group is obtained by performing\ndistillation between the temporary Group and the correspond-\ning old one. In this way, not only can the local clients have the\ncapability of IL, but large-scale models (such as MAE [He et\nal., 2022]) can also be used. At the same time, because only\nthe parameters of the chosen Enhancer Group need to be up-\ndated, the communication cost is significantly reduced.\nWe make the following contributions:\n‚Ä¢ We introduce FedET in order to address the FCIL prob-\nlem, which mitigates the issue of catastrophic forgetting\nin both local and global models and effectively reduces\ncommunication overhead. According to our knowledge,\nit is the first effort to explore the FCIL problem in a\nlarge-scale model.\n‚Ä¢ We propose the first FCIL framework used in both CV\nand NLP fields. Using different transformers as back-\nbones, FedET can handle problems in multiple fields.\nCompared with baseline models, FedET improves the\naverage accuracy of image classification by 3% and text\nclassification by 1.6%.\n‚Ä¢ We develop a new loss to handle global catastrophic for-\ngetting named entropy-aware multiple distillation. This\nis the first time an FCIL model incorporating entropy as\na factor when setting the loss function.\n‚Ä¢ We combine the IL problem of text classification with\nFL for the first time. By discussing the experimental\ndesign method and baseline selection, we think it is a\nnew challenge for both NLP and FCIL fields.\n2 Preliminary\nIn standard IL [Rebuffi et al. , 2017; Simon et al. , 2021;\nShmelkov et al., 2017 ], the streaming task sequence is de-\nfined by T = {T t}T\nt=1, in which T represents the task order,\nthe first t tasks T t = {xt\ni, yt\ni}Nt\ni=1 contains Nt pairs the sam-\nple xt\ni and the corresponding one-hot encoded label yt\ni ‚àà Yt.\nYt represents the label space of the t-th task, which includes\nthe new classes Mt = SB\nb=1 mt\nb that have not appeared in\nthe previous t ‚àí 1 tasks, and B represents the number of new\nclasses. At this time, the set of all classes that the model can\njudge is MA = St\ni=1 Mi. Inspired by [Ermis et al., 2022;\nLiu et al., 2020], based on the unique architecture of FL, we\nconstruct a Sample Memory Module S located on every lo-\ncal client to store |S|\nMA exemplars of each class at local, and it\nsatisfies |S|\nMA ‚â™ Nt\nMt .\nFor FCIL, we give the initial setting under the FL frame-\nwork [Yoonet al., 2021]: we set K local clients C = {Ck}K\nk=1\nand a global server CG, the model structures on all clients and\nserver are the same, from the perspective of parameters, in-\ncluding the frozen parameter Œ¶ (that is, the parameters of the\nselected pre-trained backbone model) and the variable param-\neter Œ∏. When a clients (a < K) send applications to server\nfor Class-Incremental Learning (CIL), they will access the t-\nth task, updated Œ∏, and select some samples {xt\ni, yt\ni} put into\nSample Memory Module.\n¬∑¬∑¬∑l-thClient m-thClient\nServer\nn-thClient\nNew Task 1\nAnEnhancer Group\nFrozenTransformerModel\nNew Task 2\n¬∑¬∑¬∑\nP!\"&\nP!#& P!\"\nP!#& \nAuxiliary Data\nEnhancerPool\nP!={P(ùë¶\"),P(ùë¶#),‚Ä¶,P(ùë¶$)}\nFigure 1: Simple FedET scenario when performing incremental\nlearning. Local clients upload the weight of the selected Enhancer\nGroup and the label distribution (Py) of their private training data to\nthe server after updating the group with new tasks. Then the server\nuses Py to construct auxiliary data, use auxiliary data to distil upload\ngroups, and send the updated group to all local clients.\n3 Methodology\nWhile CIL and FCIL share similarities, the key distinction\nbetween them is that FCIL involves tackling two types of for-\ngetting: local and global. FedET addresses local forgetting\nthrough a dual distillation loss and mitigates global forget-\nting through auxiliary data construction and an entropy-aware\nmultiple distillation loss. Figure 1 shows the general outline\nof the FedET approach.\n3.1 Solution of Local Forgetting\nIn FedET, a local model mainly includes four parts: Pre-\ntrained Transformer Blocks, Enhancer Select Module, En-\nhancer Pool and Sample Memory Module. We show the local\nmodel‚Äôs specific structure and predicting process in Figure 2.\nEnhancer Pool and Enhancer Group\nEnhancer is the core of FedET, so it is introduced here first.\nAn Enhancer Group contains some Enhancers and a pre-\ndiction head. And the number of Enhancers is decided by\nthe frozen Pre-trained Transformer Blocks. The mainstream\nmethods of IL fall into three categories [Lange et al., 2022]:\nplayback, regularization, and parameter isolation. In FedET,\nwe use a combination of three approaches: for each client,\nwe set up an Enhancer Pool, which contains multiple En-\nhancer Groups H = {Hj}J\nj=1, each group is dedicated to\nbeing proficient in part of all existing classes. That is, for an\nEnhancer Group, the class it is responsible for is MHj\n, andSJ\nj=1 MHj\n= MA. Setting the parameters of an Enhancer\nGroup and a frozen Pre-trained transformer model asŒ∏ and Œ¶\nrespectively, An Enhancer Group includes many Enhancers\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3985\nMulti-HeadAttention\nFeedForward\nAdd & Norm\nAdd &Norm\n: Prediction HeadStep1 Result\nStep2¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑Enhancer Pool\nG1 GnG2\n¬∑¬∑¬∑+Pre-trainedTransformerBlocks\nG7 =¬∑¬∑¬∑\nùë•\nEnhancedTransformerwithPrediction Headùë•\nStep3: Embedding Block\n¬∑¬∑¬∑¬∑¬∑¬∑\nEnhancerSelect Enhancer\nFFUpReLUFFDown\nScaling\n:Trainable:Frozen\n¬∑¬∑¬∑\nSelect\nFigure 2: The workflow of making predictions by the local model in FedET. The input will first be processed by the Enhancer Select Module\nto decide the right Enhancer Group. Then this Group will insert into the pre-trained backbone and develop the prediction model. It should be\nnoted that only the parameters of Enhancers are trainable, and the seventh group is used as an example in the figure.\nŒ∏E and a prediction head Œ∏H. During training, only Œ∏ is mod-\nified, Œ¶ is still frozen. Thereby greatly reducing the number\nof parameters that need to be adjusted without dropping ac-\ncuracy. An Enhancer is comprised of three components: a\ndown-projection with Wdown ‚àà Rn√óm, an activation func-\ntion f(¬∑), and an up-projection with Wup ‚àà Rm√ón. Since the\nencoder structures of transformers are almost the same, after\ncompleting FedET‚Äôs experiments in the NLP and CV fields,\nwe believe this framework can be used for most of the cur-\nrently known transformers.\nWhy choose ‚ÄùEnhancer + Freeze the backbone model‚Äù in-\nstead of Freeze the underlying encoder to adjust the upper\nencoder? We draw two perspectives from experiments and\nthe literature [R¬®uckl¬¥e et al., 2021]. First, Enhancer is freely\npluggable, and its internal structure can keep the input of\nthe original encoder, so it can retain the maximum amount\nof the knowledge that the backbone model has learned dur-\ning the pre-training stage. Meanwhile, through the design of\nthe Enhancer bottleneck structure and the freezing of the pre-\ntraining model, the entire model can learn the downstream\ntasks better, while the number of parameters that need to be\nadjusted is significantly reduced. Second, we note that di-\nrect fine-tuning can easily lead to overfitting during training\non downstream tasks, whereas inserting the Enhancer mod-\nule performs much better. Although it can be compensated\nby carefully tuning hyperparameters such as learning rate\nand batch size, it is undoubtedly time-consuming and labour-\nintensive.\nEnhancer Select Module and Sample Memory Module\nAfter a sample is preprocessed, it will be input to the En-\nhancer Select Module Gs(x). The Enhancer Select Module\nis a pre-trained frozen classifier. The function of this mod-\nule is to select a suitable Enhancer Group to handle the input\nsample. The output of this classifier tells FedET which group\nis the right group to call up. In t-th task T t, the Enhancer Se-\nlect Module will first select an Enhancer Group (e.g.j-th) Hj\naccording to the judgement that the new class Mt is the most\nsimilar to the class MHj\n, then Hj will participate in distilla-\ntion as Hj\nold. There will be a randomly initialized temporary\nEnhancer Group Ht aiming to study Mt. After the study is\ncompleted, Ht will perform distillation with Hj\nold to obtain a\nnew Enhancer Group Hj\nnew which covers Hj\nold, and the spe-\ncialized class of the group change to MHj\nnew = MHj\n‚à™ Mt.\nThe judgment methods of the Enhancer Select Module cor-\nresponding to different task fields are also different. For CV\nfields, we design the Enhancer Select Module as an Efficient-\nNet [Tan and Le, 2019 ], and for NLP fields, we use text-\nRCNN [Lai et al., 2015]. During the distillation process, the\nrequired old class samplesS\nMHj\nold\nare provided by the Sample\nMemory Module. After the new class is learned, this module\nwill also store one typical sample of each new type SMt, get\nnew SMHj\nnew\n= SMHj ‚à™ SMt, to ensure that the subsequent\ndistillation can proceed smoothly.\nAfter selection, the parameters of the chosen Enhancer\nGroup Œ∏old are linked with the frozen pre-trained model pa-\nrameters Œ¶. The Œ∏old contains two parts: Œ∏old\nE , the parameters\nof the Enhancers, and Œ∏old\nH , the parameters of the prediction\nhead.\nDistillation of Enhancers\nFor a new task T t, the new classes need to learn is Mt. After\nthe temporary Enhancer Group Ht finish studying, it will be\nlinked with the frozen transformer model as the temporary\nmodel ft, which contains Œ¶ and Œ∏t. The frozen transformer\nmodel with Hold is called the old model fold. We use the\nfollowing objective to distill fold and ft:\nfnew(x; Œ∏new, Œ¶) =\nÔ£±\nÔ£≤\nÔ£≥\nfold(x; Œ∏old, Œ¶)[i] 1 ‚â§ i ‚â§ m\nft(x; Œ∏t, Œ¶)[i] m < i‚â§ c\n(1)\nwe set\nc = m + n (2)\nwhere m and n is the number of Mold and Mt respectively.\nTo ensure that the consolidated model‚Äôs output f(x; Œ∏new, Œ¶)\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3986\napproximates the combination of outputs fromft and fold, we\nutilize the output of ft and fold as supervisory signals during\nthe joint training of the consolidated parameters Œ∏new.\nTo achieve this goal, we employ the double distillation loss\nproposed by [Zhang et al., 2020] to train fnew. The distillation\nprocess is as follows: ft and fold are frozen, and run a feed-\nforward pass with every sample in training set to collect the\nlogits of ft and fold:\nÀÜyold = {ÀÜy1, ¬∑¬∑¬∑ , ÀÜym}, ÀÜyt = {ÀÜym+1, ¬∑¬∑¬∑ , ÀÜym+n}\nrespectively, where the superscript is the class label. Then\nmain requirements is to reduce the gap between the logits\ngenerated by fnew and the logits generated by ft and fold.\nBased on prior work[Zhang et al., 2020], We choose L2 loss\n[Ba and Caruana, 2014 ] as the distance metric. Specifically,\nthe training objective for consolidation is:\nmin\nŒ∏new\n1\n|U|\nX\nxi‚ààU\nLdd(y, Àôy) (3)\nwhere U denotes the training samples from Sample Memory\nModule used for distillation. And Ldd is the double distilla-\ntion loss:\nLdd(y, Àôy) = 1\nm + n\nm+nX\ni=1\n(yi ‚àí Àôyi)2 (4)\nin which yi are the logits produced by fnew for the t-th task,\nand\nÀôyi =\nÔ£±\nÔ£¥Ô£¥\nÔ£¥\nÔ£¥\nÔ£≤\nÔ£¥Ô£¥Ô£¥Ô£¥\nÔ£≥\nÀÜyi ‚àí 1\nm\nmP\nj=1\nÀÜyj 1 ‚â§ i ‚â§ m\nÀÜyi ‚àí 1\nn\nm+nP\nj=m+1\nÀÜyj m < i‚â§ c\n(5)\nwhere ÀÜy is the concatenation of ÀÜyold and ÀÜynew. After the con-\nsolidation, the Enhancers parameters Œ∏new are used for fold in\nthe next round. The pseudo code for local forgetting solution\nis shown in Algorithm 1.\n3.2 Solution of Global Forgetting\nGlobal catastrophic forgetting primarily arises from the het-\nerogeneity forgetting among local clients participating in\nincremental learning. Which means the non-i.i.d. class-\nimbalanced distributions across local clients lead to catas-\ntrophic forgetting of old classes on a global scale, further ex-\nacerbating local catastrophic forgetting. Therefore, it is nec-\nessary to solve the heterogeneity forgetting problem across\nclients in global perspective. To ensure precision and speed,\nFedET handles this problem with double distillation loss and\nthe difference of average entropy across different clients.\nDistillation of Enhancers of Different Clients\nFedET changed the stereotype of having to queue up for up-\ndates and proposed a new way to update the model. The\nnew method is more scientific, reasonable, and time-effective.\nWhen a single client uploads the new Enhancer Group ob-\ntained after distillation to the server, all the server needs\nto do is update the parameters of the corresponding group.\nAlgorithm 1 Local ICL\nInput: Enhancer Select Module Gs(x)\nInput: Enhancer Pool H = {Hj}J\nj=1\nInput: Sample Memory Module S\nInput: Parameters of pre-trained transformer model Œ¶\nInput: t-th task data T t = {xt\ni, yt\ni}Nt\ni=1\n1: for i = 1 ‚Üí Nt do\n2: Group number j ‚Üê Gs(xt\ni)\n3: Put {xt\ni, yt\ni} with the same j into a list Lj\n4: end for\n5: for every selected j do\n6: while Non-convergence do\n7: Randomly initialize temporary group Ht\n8: train ft(x; Œ∏t, Œ¶) with Lj\n9: Sample from Lj and add to S\n10: fnew(x; Œ∏new, Œ¶) = DISTILLATION(ft, fold, S)\n11: Œ∏j ‚Üê Œ∏new\n12: Communicate Hj with Server to get Global best j-\nth group in this turn\n13: end while\n14: end for\n15:\n15: function DISTILLATION(ft, fold, S)\n16: Get ÀÜyold from fold and S\n17: Get ÀÜyt from ft and S\n18: Compute loss function as in Eq.4 and train fnew\n19: return fnew\nWhen many clients upload the same Enhancer simultane-\nously, queuing is unscientific because only the last client‚Äôs up-\ndate is critical, and this is how global catastrophic forgetting\nhappens. FedET sets a server waiting time limitation. Within\na specific time, multiple schemes for an Enhancer Group will\nbe aggregated by the server to perform global model distilla-\ntion.\nFor global distillation, the server will distill some En-\nhancer Group at same time, which means there will be\nf1\nt , f2\nt , ¬∑¬∑¬∑ , fq\nt (q < the number of clinets) and a fold distill\ntogether. Note that the new classes are learned by all uploaded\ngroups. Suppose the class which the distilled Enhancer Group\nmajor in is Mt. For every group-uploaded client, they also\nupload the information entropy H(Mt) of Mt to the server.\nThe server uses H(Mt) to judge the importance of each ft,\nin detail, the consolidated model of global distillation is:\nfnew(x; Œ∏new, Œ¶) =\nÔ£±\nÔ£¥Ô£¥\nÔ£≤\nÔ£¥Ô£¥\nÔ£≥\nfold(x; Œ∏old, Œ¶)[i] 1 ‚â§ i ‚â§ m\nqP\nk=1\nHk\nHsum\nfk\nt (x; Œ∏t, Œ¶)[i] m < i‚â§ c\n(6)\nwhere Hsum is the sum of information entropy H(Mt) of\nall uploaded clients. Noted that all output of f here are log-\nits, not hard-label. To get Œ∏new, the entropy-aware multiple\ndistillation loss Lemd is:\nLemd(y, ¬®y) = 1\nm + n\nm+nX\ni=1\n(yi ‚àí ¬®yi)2 (7)\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3987\nModel Method Updated\nParas.\nTraining\nTime\nBERT Fine-tuning 110.01 √ó 106 1.92 sec\nEnhancer 1.76 √ó 106 1.19 sec\nViT-Base Fine-tuning 75.99 √ó 106 0.94 sec\nEnhancer 1.19 √ó 106 0.59 sec\nTable 1: The communication cost and computation cost difference\nbetween whether inserting Enhancer or not. Here Updated Paras.\nrefers to the number of updated parameters\nin which ¬®y is:\n¬®yi =\nÔ£±\nÔ£¥Ô£¥\nÔ£¥Ô£¥Ô£≤\nÔ£¥Ô£¥\nÔ£¥\nÔ£¥Ô£≥\nÀÜyi ‚àí 1\nm\nmP\nj=1\nÀÜyj 1 ‚â§ i ‚â§ m\nÀÜyi ‚àí 1\nnHsum\nm+nP\nj=m+1\nqP\nk=1\nHk ÀÜyk\nj m < i‚â§ c\n(8)\nBecause of the nature of FL, we cannot rely solely on the\nsampled data to consolidate the updated Enhancers. There-\nfore, auxiliary data must be used. During local Enhancer dis-\ntillation, we generate U using the Sample Memory Module,\nwhich stores one representative sample per class and utilizes\ndata augmentation to create the dataset. For global distilla-\ntion, we construct an equivalent dataset to approximate the\ntraining samples. After the local clients send the label distri-\nbution Py to the server, the server can construct the auxiliary\ndatasets using available data of a similar domain. Notably,\nthese auxiliary datasets are dynamically fetched and inputted\nin mini-batches, reducing the storage burden, and discarded\nafter distillation is complete.\nCommunication Cost Analysis\nThe parameter quantity of a single Enhancer is2mn+n+m.\nFor a single local model, if there are D Encoder modules in\none Enhancer Group, after adding a group of Enhancers, the\nincreased parameter quantity is:\nD √ó (2mn + n + m) + n √ó labels (9)\nOther parameters are frozen except for the Enhancer Group\nand prediction head in the model. As shown in Table 1, the\nnetwork parameters that need to be transmitted are reduced\nby more than 70% compared with the various FCIL models\npreviously proposed.\nComputation Cost Analysis\nThe computation FLOPs for each Enhancer in the forward\npass are 2 √ó m √ó n √ó sequence length (normalized to a sin-\ngle data sample). The overhead incurred in this way is neg-\nligible compared to the original model complexity, e.g., less\nthan 1% on BERT. In the meantime, since all other parame-\nters are fixed during the training period, the computation dur-\ning backpropagation is reduced by skipping the gradient that\ncomputes most of the weights. As shown in Table 1, the use\nof Enhancers reduces the training time by about 40%.\n4 Experiments\nAs discussed in Section 1, since transformers are widely used\nin both NLP and CV fields, we test the performance of FedET\nDataset Class Type Train / Test\nAGnews 4 News 8000 / 2000\nYelp 5 Sentiment 10000 / 2500\nAmazon 5 Sentiment 10000 / 2500\nDBpedia 14 Wikipedia Article 28000 / 7000\nYahoo 10 Q&A 20000 / 5000\nTable 2: The text classification dataset we used includes statistics on\nvarious domains of classification tasks.\nOrder Task Sequence\n1 AGnews‚ÜíYelp‚ÜíYahoo\n2 Yelp‚ÜíYahoo‚ÜíAGnews\n3 Yahoo‚ÜíAGnews‚ÜíYelp\n4 AG‚ÜíYelp‚ÜíAmazon‚ÜíYahoo‚ÜíDBpedia\n5 Yelp‚ÜíYahoo‚ÜíAmazon‚ÜíDBpedia‚ÜíAGnews\n6 DBpedia‚ÜíYahoo‚ÜíAGnews‚ÜíAmazon‚ÜíYelp\nTable 3: Six different dataset sequences for NLP experiments\non image and text classification tasks. The complete setup\nwill be described in the following subsections.\n4.1 Natural Language Processing (NLP)\nDatasets and Baselines\nOwing to the limited label space of a single dataset, we inte-\ngrated five text classification datasets [Chen et al., 2020] to\nevaluate FedET. Table 2 displays the details of the dataset.\nConsidering the domain similarity of Yelp and Amazon, we\nmerge their label spaces for a total of 33 classes. We followed\nspecific task sequences as outlined in Table 3 during training.\nTo alleviate the impact of sequence length and task order on\nexperiment results, we test task sequences of length-3 and\nlength-5 in different orders. The first three tasks of length-\n3 sequences are a cyclic shift of AGnews‚ÜíYelp‚ÜíYahoo,\nwhich belong to three distinct domains (news classification,\nsentiment analysis, Q&A classification). The remaining three\ntask sequences of length-5 follow the experimental design\nproposed by [de Masson d‚ÄôAutume et al., 2019]. During val-\nidation, the validation set comprise all classes.\nCurrently, there is no text classification in the FCIL field,\nso we choose the baseline of text classification in the Class-\nIncremental Learning (CIL) field and federate it to form the\nbaseline of this experiment. We compare FedET with five\nbaselines:\n‚Ä¢ Finetune [Yogatama et al., 2019] + FL: Only new tasks\nare used to fine-tune the BERT model in turn.\n‚Ä¢ Replay [de Masson d‚ÄôAutume et al., 2019 ] + FL: Re-\nplay some old tasks examples during new-tasks-learning\nto Finetune the model.\n‚Ä¢ Regularization + Replay + FL: On the foundation of\nReplay, add an L2 regularization term to the hidden\nstate of the classifier following BERT.\n‚Ä¢ IDBR [Huang et al., 2021] + FL: On the basis of Regu-\nlarization + Replay + FL, replace the L2 regularization\nterm with an information disentanglement-based regu-\nlarization term.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3988\nModel Length-3 Task Sequences Length-5 Task Sequences\nOrder 1 2 3 Average 4 5 6 Average\nFinetune + FL 25.79 36.56 41.01 34.45 32.37 32.22 26.44 30.34\nReplay + FL 69.32 70.25 71.31 70.29 68.25 70.52 70.24 69.67\nRegularization + Replay + FL 71.50 70.88 72.93 71.77 72.28 73.03 72.92 72.74\nIDBR + FL 71.80 72.72 73.08 72.53 72.63 73.72 73.23 73.19\nFedET 73.12 73.57 74.28 73.66 73.83 74.23 73.18 73.75\nMTL + FL 74.16 74.16 74.16 74.16 75.09 75.09 75.09 75.09\nTable 4: Performance comparisons between FedET and other incremental text classification baseline methods\nMethods 10 20 30 40 50 60 70 80 90 100 Avg. Communication\nCost per Task\niCaRL + FL 73.5 61.3 55.7 45.9 45.0 39.7 36.7 33.9 32.2 31.8 46.5 10.82 √ó 106\nBiC + FL 74.3 63.0 57.7 51.3 48.3 46.0 42.7 37.7 35.3 34.0 49.0 10.82 √ó 106\nPODNet + FL 74.3 64.0 59.0 56.7 52.7 50.3 47.0 43.3 40.0 38.3 52.6 10.82 √ó 106\nSS-IL + FL 69.7 60.0 50.3 45.7 41.7 44.3 39.0 38.3 38.0 37.3 46.4 10.82 √ó 106\nDDE + iCaRL + FL 76.0 57.7 58.0 56.3 53.3 50.7 47.3 44.0 40.7 39.0 52.3 10.82 √ó 106\nGLFC 73.0 69.3 68.0 61.0 58.3 54.0 51.3 48.0 44.3 42.7 57.0 10.82 √ó 106\nFedET(J = 10) 83.2 75.7 72.0 69.4 67.9 65.8 63.4 62.1 61.0 60.6 68.1 1.19 √ó 106\nTable 5: Comparison of FedET‚Äôs performance with other CV baselines in ten incremental tasks on ImageNet-Subset. During the experiment,\nFedET only communicates the parameter of the changed Enhancer Group, and other methods update the entire model(ResNet18).\n‚Ä¢ Multi-task Learning (MTL): Train the model with all\nclass in one task. This approach represents an upper\nbound on the performance achievable through incremen-\ntal learning.\nImplementation Details\nIn this NLP experiment, we set J = 3 Enhancer Groups and\nK = 10 local clients. The prediction head for each group is\na linear layer with a Softmax activation function. For sam-\nple collection (i.e. experience replay), we stored 3% (store\nratio Œ≥ = 0 .03) of observed examples in the Sample Mem-\nory Module, which is used for local Enhancer distillation.\nWe choose the pre-trained Bert-Base-Uncased from Hugging-\nFace Transformers[Wolfet al., 2020] as our backbone model.\nAll experiments utilized a batch size of 16 and a maximum se-\nquence length of 256. During training, ADAMW[Loshchilov\nand Hutter, 2019] is used as the optimizer, with a learning rate\nlr = 3e‚àí5 and a weight decay 0.01 for all parameters. For\neach round of global training, three clients are randomly se-\nlected for ten epochs of local training. Selected clients are\nrandomly given 60% of the classes from the label space of its\nseen tasks.\nResults\nAs shown in Table 1 and Table 4, we can directly see the im-\nportance of experience replay for FCIL in NLP. Moreover,\nthe simple regularization approach based on experience re-\nplay consistently improves results across all six orders. In\nmost cases, FedET achieves higher performance in incremen-\ntal learning compared to other baseline methods, while signif-\nicantly reducing the communication cost. Specifically, com-\npared to IDBR+FL, FedET‚Äôs Enhancer structure adds a seg-\nmentation step to the regularisation and empirical replay, fur-\nther improving the performance of the model.\n4.2 Computer Version(CV)\nDatasets and Baselines\nWe use ImageNet-Subset[Deng et al., 2009] and CIFAR-100\n[Krizhevsky et al., 2009] to evaluate our method. We follow\nthe same protocol as iCaRL [Rebuffi et al., 2017 ] to set in-\ncremental tasks and we use the same order generated from\niCaRL for a fair comparison. In detail, we compare FedET\nwith the following baselines in the FL scenario: iCaRL, BiC\n[Wu et al., 2019 ], PODNet [Douillard et al., 2020 ], SS-\nIL [Ahn et al., 2021 ], DDE+ iCaRL [Hu et al., 2021 ] and\nGLFC [Dong et al., 2022].\nImplementation Details\nIn this CV experiment, we set J = 10 for Enhancer Groups\nand K = 30 for local. The prediction head for each group is a\nlinear layer with a Softmax activation function. We collected\nsamples at a store ratio of Œ≥ = 0.01. In the CIL baselines, we\nchoose ResNet18 [He et al., 2016] to be the backbone with\ncross-entropy as the classification loss. On the other hand,\nFedET uses frozen pre-trained ViT-Base [He et al., 2022 ]\nas the backbone. All experiments have a batch size of 64.\nThe training of the Enhancer used an SGD optimizer with\nminimum learning rate lrmin = 1 e‚àí5 and a base learning\nrate lrb = 0.1. In each round of global training, ten clients\nare randomly selected for ten local-training epochs. Selected\nclients are randomly given 60% of the classes from the label\nspace of its seen tasks.\nResults\nTable 6 and Table 5 show that FedET consistently outper-\nform all the baselines by 3.3% ‚àº 10.5% in terms of aver-\nage accuracy and reduces the communication cost to 11.0%\nof baseline models. These results demonstrate that FedET\ncan cooperatively train a global class-incremental model in\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3989\nMethods 10 20 30 40 50 60 70 80 90 100 Avg. Communication\nCost per Task\niCaRL + FL 89.0 55.0 57.0 52.3 50.3 49.3 46.3 41.7 40.3 36.7 51.8 10.82 √ó 106\nBiC + FL 88.7 63.3 61.3 56.7 53.0 51.7 48.0 44.0 42.7 40.7 55.0 10.82 √ó 106\nPODNet + FL 89.0 71.3 69.0 63.3 59.0 55.3 50.7 48.7 45.3 45.0 59.7 10.82 √ó 106\nSS-IL + FL 88.3 66.3 54.0 54.0 44.7 54.7 50.0 47.7 45.3 44.0 54.9 10.82 √ó 106\nDDE + iCaRL + FL 88.0 70.0 67.3 62.0 57.3 54.7 50.3 48.3 45.7 44.3 58.8 10.82 √ó 106\nGLFC 90.0 82.3 77.0 72.3 65.0 66.3 59.7 56.3 50.3 50.0 66.9 10.82 √ó 106\nFedET(J = 1) 89.0 61.0 62.0 55.4 51.7 49.1 45.8 43.1 40.2 37.9 53.2 1.19 √ó 106\nFedET(J = 5) 89.6 82.3 77.0 72.9 64.8 61.0 59.9 56.3 50.7 49.8 66.4 1.19 √ó 106\nFedET(J = 10) 93.1 84.2 82.4 79.3 77.4 74.7 71.4 68.7 66.5 66.0 76.4 1.19 √ó 106\nTable 6: Comparison of FedET‚Äôs performance with other CV baselines in ten incremental tasks on CIFAR-100. During the experiment,\nFedET only communicates the parameter of the changed Enhancer Group, and other methods update the entire model(ResNet18).\n(a) Different incremental tasks with different number of Enhancer Groups ùêΩ when Incremental size equals to\t5\t(left), 10\t\n(middle) and 20 (right).\n(b) Same incremental tasks with different \nnumber of clients ùêæon CIFAR-100\nFigure 3: Ablation study\nconjunction with local clients more efficiently. Furthermore,\nfor all incremental tasks, FedET has steady performance im-\nprovement over other methods, validating the effectiveness in\naddressing the forgetting problem in FCIL.\nAblation Studies\nTable 6 and Figure 3 illustrate the results of our ablation ex-\nperiments on the number of Enhancers and local clients.\nThe number of Enhancers (J ) Figure 3 (a) shows the\nmodel‚Äôs performance in four cases, J = 1, J = 5, J =\n10, J= 15, respectively. Compared with J = 10, the per-\nformance of J = 1 and J = 5 is worse but J = 15 is better.\nSince the Enhancer Select Module is frozen, in FedET, we\ncannot set J = the number of classes. It has been verified\nthat a suitable value of J will make FedET powerful and ef-\nficient. We observed that increasing the value of J makes\nFedET perform better at the beginning of incremental learn-\ning but degrades faster as learning progresses. We believe\nthat the reasons may be: (1) With the increase of J, the re-\nquirements for the Enhancer Select Module are higher. When\nthe Enhancer Select Module cannot precisely perform rough\nclassification, the model accuracy is bound to decrease. (2)\nAs J increases, the learning cost on a single Enhancer Group\nis lower, which means that as long as the Enhancer Select\nModule selects the correct group, the possibility of accurate\njudgment will be significantly improved.\nThe number of clients (K ) We tested the performance of\nFedET with client numbers of 30, 50, and 100, as depicted\nin Figure 3(b). It is clear that FedET‚Äôs capacity declines as\nK increases. The performance decrease is most noticeable\nwhen K = 100. We believe that with an increase in K, the\ncentral server needs to perform distillation on more enhancers\nsimultaneously, which causes the model to not fully converge\nwithin the specified number of iterations, resulting in a de-\ncrease in model performance.\n5 Conclusion\nFedET is an FCIL framework that can be used in many\nfields. Based on previous work, it introduces transformers\nto improve the accuracy of FCIL and increase the application\nfield of the framework. To reduce communication costs and\nstreamline training, only the Enhancer and its related com-\nponents are designated as trainable parameters. Our exper-\niments on datasets in NLP and CV demonstrate that FedET\noutperforms existing methods in FCIL while decreasing com-\nmunication information by up to 90%.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3990\nAcknowledgements\nThis paper is supported by the Key Research and De-\nvelopment Program of Guangdong Province under grant\nNo.2021B0101400003. Corresponding author is Jianzong\nWang from Ping An Technology (Shenzhen) Co., Ltd.\nReferences\n[Ahn et al., 2021] Hongjoon Ahn, Jihwan Kwak, Subin Lim,\nHyeonsu Bang, Hyojun Kim, and Taesup Moon. SS-\nIL: separated softmax for incremental learning. In 2021\nIEEE/CVF International Conference on Computer Vision,\nICCV 2021, Montreal, QC, Canada, October 10-17, 2021,\npages 824‚Äì833, 2021.\n[Ba and Caruana, 2014] Jimmy Ba and Rich Caruana. Do\ndeep nets really need to be deep? In Advances in Neural\nInformation Processing Systems 27: Annual Conference\non Neural Information Processing Systems 2014, Decem-\nber 8-13 2014, Montreal, Quebec, Canada, pages 2654‚Äì\n2662, 2014.\n[Chen et al., 2020] Jiaao Chen, Zichao Yang, and Diyi Yang.\nMixtext: Linguistically-informed interpolation of hidden\nspace for semi-supervised text classification. In Proceed-\nings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 2147‚Äì2157, 2020.\n[de Masson d‚ÄôAutume et al., 2019] Cyprien de Mas-\nson d‚ÄôAutume, Sebastian Ruder, Lingpeng Kong, and\nDani Yogatama. Episodic memory in lifelong language\nlearning. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, December 8-14,\n2019, Vancouver, BC, Canada, pages 13122‚Äì13131, 2019.\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-\nJia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE Computer So-\nciety Conference on Computer Vision and Pattern Recog-\nnition (CVPR 2009), 20-25 June 2009, Miami, Florida,\nUSA, pages 248‚Äì255, 2009.\n[Dong et al., 2022] Jiahua Dong, Lixu Wang, Zhen Fang,\nGan Sun, Shichao Xu, Xiao Wang, and Qi Zhu. Feder-\nated class-incremental learning. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2022,\nNew Orleans, LA, USA, June 18-24, 2022, pages 10154‚Äì\n10163, 2022.\n[Dong et al., 2023] Jiahua Dong, Yang Cong, Gan Sun, Yu-\nlun Zhang, Bernt Schiele, and Dengxin Dai. No one left\nbehind: Real-world federated class-incremental learning.\nabs/2302.00903, 2023.\n[Douillard et al., 2020] Arthur Douillard, Matthieu Cord,\nCharles Ollion, Thomas Robert, and Eduardo Valle. Pod-\nnet: Pooled outputs distillation for small-tasks incremental\nlearning. In Computer Vision - ECCV 2020 - 16th Euro-\npean Conference, Glasgow, UK, August 23-28, 2020, Pro-\nceedings, Part XX, pages 86‚Äì102, 2020.\n[Ermis et al., 2022] Beyza Ermis, Giovanni Zappella, Mar-\ntin Wistuba, and C ¬¥edric Archambeau. Memory ef-\nficient continual learning for neural text classification.\nabs/2203.04640, 2022.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2016, Las Vegas, NV , USA,\nJune 27-30, 2016, pages 770‚Äì778, 2016.\n[He et al., 2022] Kaiming He, Xinlei Chen, Saining Xie,\nYanghao Li, Piotr Doll ¬¥ar, and Ross B. Girshick. Masked\nautoencoders are scalable vision learners. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\nCVPR 2022, New Orleans, LA, USA, June 18-24, 2022,\npages 15979‚Äì15988, 2022.\n[Hendryx et al., 2021] Sean M. Hendryx, Dharma Raj KC,\nBradley Walls, and Clayton T. Morrison. Federated recon-\nnaissance: Efficient, distributed, class-incremental learn-\ning. abs/2109.00150, 2021.\n[Hong et al., 2021] Zhenhou Hong, Jianzong Wang, Xi-\naoyang Qu, Jie Liu, Chendong Zhao, and Jing Xiao.\nFederated learning with dynamic transformer for text to\nspeech. In Interspeech 2021, 22nd Annual Conference\nof the International Speech Communication Association,\nBrno, Czechia, 30 August - 3 September 2021, pages\n3590‚Äì3594, 2021.\n[Hu et al., 2021] Xinting Hu, Kaihua Tang, Chunyan Miao,\nXian-Sheng Hua, and Hanwang Zhang. Distilling causal\neffect of data in class-incremental learning. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2021, virtual, June 19-25, 2021, pages 3957‚Äì3966,\n2021.\n[Huang et al., 2021] Yufan Huang, Yanzhe Zhang, Jiaao\nChen, Xuezhi Wang, and Diyi Yang. Continual learning\nfor text classification with information disentanglement\nbased regularization. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2021, Online, June 6-11, 2021, pages\n2736‚Äì2746, 2021.\n[Krizhevsky et al., 2009] Alex Krizhevsky, Geoffrey Hinton,\net al. Learning multiple layers of features from tiny im-\nages. Citeseer, 2009.\n[Lai et al., 2015] Siwei Lai, Liheng Xu, Kang Liu, and\nJun Zhao. Recurrent convolutional neural networks for\ntext classification. In Proceedings of the Twenty-Ninth\nAAAI Conference on Artificial Intelligence, January 25-\n30, 2015, Austin, Texas, USA, pages 2267‚Äì2273, 2015.\n[Lange et al., 2022] Matthias De Lange, Rahaf Aljundi,\nMarc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gre-\ngory G. Slabaugh, and Tinne Tuytelaars. A continual\nlearning survey: Defying forgetting in classification tasks.\n44(7):3366‚Äì3385, 2022.\n[Liu et al., 2020] Yaoyao Liu, Yuting Su, An-An Liu, Bernt\nSchiele, and Qianru Sun. Mnemonics training: Multi-\nclass incremental learning without forgetting. In 2020\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3991\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2020, Seattle, WA, USA, June 13-19,\n2020, pages 12242‚Äì12251, 2020.\n[Loshchilov and Hutter, 2019] Ilya Loshchilov and Frank\nHutter. Decoupled weight decay regularization. In 7th\nInternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.\n[Qu et al., 2020] Xiaoyang Qu, Jianzong Wang, and Jing\nXiao. Quantization and knowledge distillation for efficient\nfederated learning on edge devices. In 22nd IEEE Inter-\nnational Conference on High Performance Computing and\nCommunications, HPCC 2020, Yanuca Island, December\n14-16, 2020, pages 967‚Äì972, 2020.\n[Rebuffi et al., 2017] Sylvestre-Alvise Rebuffi, Alexander\nKolesnikov, Georg Sperl, and Christoph H. Lampert. icarl:\nIncremental classifier and representation learning. In 2017\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017,\npages 5533‚Äì5542, 2017.\n[R¬®uckl¬¥e et al., 2021] Andreas R ¬®uckl¬¥e, Gregor Geigle, Max\nGlockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers,\nand Iryna Gurevych. Adapterdrop: On the efficiency\nof adapters in transformers. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 7-11 November, 2021, pages 7930‚Äì\n7946, 2021.\n[Shmelkov et al., 2017] Konstantin Shmelkov, Cordelia\nSchmid, and Karteek Alahari. Incremental learning\nof object detectors without catastrophic forgetting. In\nIEEE International Conference on Computer Vision,\nICCV 2017, Venice, Italy, October 22-29, 2017, pages\n3420‚Äì3429, 2017.\n[Simon et al., 2021] Christian Simon, Piotr Koniusz, and\nMehrtash Harandi. On learning the geodesic path for in-\ncremental learning. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2021, virtual, June\n19-25, 2021, pages 1591‚Äì1600, 2021.\n[Tan and Le, 2019] Mingxing Tan and Quoc V . Le. Efficient-\nnet: Rethinking model scaling for convolutional neural\nnetworks. In Proceedings of the 36th International Confer-\nence on Machine Learning, ICML 2019, 9-15 June 2019,\nLong Beach, California, USA, pages 6105‚Äì6114, 2019.\n[Wolf et al., 2020] Thomas Wolf, Lysandre Debut, Victor\nSanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R ¬¥emi Louf, Morgan Fun-\ntowicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. Transformers: State-of-\nthe-art natural language processing. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, EMNLP 2020\n- Demos, Online, November 16-20, 2020, pages 38‚Äì45,\n2020.\n[Wu et al., 2019] Yue Wu, Yinpeng Chen, Lijuan Wang,\nYuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu.\nLarge scale incremental learning. In IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2019,\nLong Beach, CA, USA, June 16-20, 2019, pages 374‚Äì382,\n2019.\n[Yang et al., 2019] Qiang Yang, Yang Liu, Yong Cheng, Yan\nKang, Tianjian Chen, and Han Yu. Federated Learning.\nSynthesis Lectures on Artificial Intelligence and Machine\nLearning. Morgan & Claypool Publishers, 2019.\n[Yogatama et al., 2019] Dani Yogatama, Cyprien de Mas-\nson d‚ÄôAutume, Jerome T. Connor, Tom ¬¥as Kocisk ¬¥y, Mike\nChrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang\nLing, Lei Yu, Chris Dyer, and Phil Blunsom. Learning and\nevaluating general linguistic intelligence. abs/1901.11373,\n2019.\n[Yoon et al., 2021] Jaehong Yoon, Wonyong Jeong, Gi-\nwoong Lee, Eunho Yang, and Sung Ju Hwang. Feder-\nated continual learning with weighted inter-client transfer.\nIn Proceedings of the 38th International Conference on\nMachine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent, pages 12073‚Äì12086, 2021.\n[Zhang et al., 2020] Junting Zhang, Jie Zhang, Shalini\nGhosh, Dawei Li, Serafettin Tasci, Larry P. Heck, Hem-\ning Zhang, and C.-C. Jay Kuo. Class-incremental learning\nvia deep model consolidation. In IEEE Winter Conference\non Applications of Computer Vision, WACV 2020, Snow-\nmass Village, CO, USA, March 1-5, 2020, pages 1120‚Äì\n1129, 2020.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3992",
  "topic": "Forgetting",
  "concepts": [
    {
      "name": "Forgetting",
      "score": 0.8698520660400391
    },
    {
      "name": "Computer science",
      "score": 0.8104199171066284
    },
    {
      "name": "Transformer",
      "score": 0.610909640789032
    },
    {
      "name": "Federated learning",
      "score": 0.5463129281997681
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4826703369617462
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48205265402793884
    },
    {
      "name": "Machine learning",
      "score": 0.46557754278182983
    },
    {
      "name": "Class (philosophy)",
      "score": 0.44432342052459717
    },
    {
      "name": "Distributed computing",
      "score": 0.417377233505249
    },
    {
      "name": "Engineering",
      "score": 0.09672468900680542
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210105511",
      "name": "Beacon Tech (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I4401726822",
      "name": "Ping An (China)",
      "country": null
    }
  ]
}