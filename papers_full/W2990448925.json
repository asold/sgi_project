{
  "title": "TreeGen: A Tree-Based Transformer Architecture for Code Generation",
  "url": "https://openalex.org/W2990448925",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2188591270",
      "name": "Sun, Zeyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2625063975",
      "name": "Zhu Qihao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2354132175",
      "name": "Xiong, Yingfei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221967978",
      "name": "Sun, Yican",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2576690091",
      "name": "Mou, Lili",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1989596241",
      "name": "Zhang Lu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1496189301",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2962728167",
    "https://openalex.org/W2304240348",
    "https://openalex.org/W2797328513",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963371736",
    "https://openalex.org/W2605887895",
    "https://openalex.org/W2951790836",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2953001633",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2896392119",
    "https://openalex.org/W2224454470",
    "https://openalex.org/W2963868406",
    "https://openalex.org/W2949734169",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2949384567",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W2111742432",
    "https://openalex.org/W2251673953",
    "https://openalex.org/W2227250678"
  ],
  "abstract": "A code generation system generates programming language code based on an input natural language description. State-of-the-art approaches rely on neural networks for code generation. However, these code generators suffer from two problems. One is the long dependency problem, where a code element often depends on another far-away code element. A variable reference, for example, depends on its definition, which may appear quite a few lines before. The other problem is structure modeling, as programs contain rich structural information. In this paper, we propose a novel tree-based neural architecture, TreeGen, for code generation. TreeGen uses the attention mechanism of Transformers to alleviate the long-dependency problem, and introduces a novel AST reader (encoder) to incorporate grammar rules and AST structures into the network. We evaluated TreeGen on a Python benchmark, HearthStone, and two semantic parsing benchmarks, ATIS and GEO. TreeGen outperformed the previous state-of-the-art approach by 4.5 percentage points on HearthStone, and achieved the best accuracy among neural network-based approaches on ATIS (89.1%) and GEO (89.6%). We also conducted an ablation test to better understand each component of our model.",
  "full_text": "TreeGen: A Tree-Based Transformer Architecture for Code Generation\nZeyu Sun† Qihao Zhu† Yingfei Xiong∗† Yican Sun† Lili Mou‡ Lu Zhang†\n†Key Laboratory of High Conﬁdence Software Technologies (Peking University), MoE;\nSoftware Institute, Peking University, 100871, P. R. China\n{szy , zhuqh, xiongyf, sycpku, zhanglucs}@pku.edu.cn\n‡University of Alberta, Edmonton, AB, Canada\ndoublepower.mou@gmail.com\nAbstract\nA code generation system generates programming language\ncode based on an input natural language description. State-of-\nthe-art approaches rely on neural networks for code genera-\ntion. However, these code generators suffer from two prob-\nlems. One is the long dependency problem, where a code\nelement often depends on another far-away code element.\nA variable reference, for example, depends on its deﬁnition,\nwhich may appear quite a few lines before. The other problem\nis structure modeling, as programs contain rich structural in-\nformation. In this paper, we propose a novel tree-based neural\narchitecture, TreeGen, for code generation. TreeGen uses the\nattention mechanism of Transformers to alleviate the long-\ndependency problem, and introduces a novel AST reader (en-\ncoder) to incorporate grammar rules and AST structures into\nthe network. We evaluated TreeGen on a Python benchmark,\nHearthStone, and two semantic parsing benchmarks, ATIS\nand GEO. TreeGen outperformed the previous state-of-the-\nart approach by 4.5 percentage points on HearthStone, and\nachieved the best accuracy among neural network-based ap-\nproaches on ATIS (89.1%) and GEO (89.6%). We also con-\nducted an ablation test to better understand each component\nof our model.\nIntroduction\nCode generation is an important artiﬁcial intelligence prob-\nlem that has the potential to signiﬁcantly boost the pro-\nductivity of programmers. Given a speciﬁcation written in\nnatural language, a code generation system translates the\nspeciﬁcation into an executable program. For example, if a\npython programmer gives an instruction “initialize a dictio-\nnary, Dict”, the code generator is expected to automatically\ngenerates “Dict={}”.\nWith the development deep learning techniques, re-\nsearchers have applied various neural architectures to this\nproblem, such as sequence-to-sequence (Seq2Seq) models\nor sequence-to-tree (Seq2Tree) models (Sutskever, Vinyals,\nand Le 2014; Ling et al. 2016; Yin and Neubig 2017;\nRabinovich, Stern, and Klein 2017; Hayati et al. 2018;\n∗Yingfei Xiong is the corresponding author. The code is avail-\nable at https://github.com/zysszy/TreeGen\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nSun et al. 2019). Especially, state-of-the-art approaches gen-\nerate code by predicting a sequence of grammar rules (Yin\nand Neubig 2017; Rabinovich, Stern, and Klein 2017; Sun\net al. 2019). That is to say, the system keeps a partial ab-\nstract syntax tree (AST) of the already-generated code, and\npredicts the grammar rule to be used to expand a particular\nnode.\nThe classiﬁcation of grammar rules faces two main chal-\nlenges. The ﬁrst challenge is the long-dependency prob-\nlem (Bengio, Simard, and Frasconi 1994). A code element\nmay depend on another far-away element. For example, a\nvariable reference statement “ if len(a) < Max Length:”\nat line 100 may depend on a variable deﬁnition statement\n“Max Length = 100” at line 10. The second challenge\nis the representation of code structures. It is pointed out\nthat the tree-structural information is crucial for modeling\ncode (Mou et al. 2016; Yin and Neubig 2017; Rabinovich,\nStern, and Klein 2017; Sun et al. 2019). However, a “ﬂat”\nneural architecture, such as an RNN, cannot capture struc-\nture information well.\nIn this paper, we propose a novel neural architecture,\nTreeGen, for the code generation. To address the ﬁrst chal-\nlenge, TreeGen adopts the recently proposed Transformer\narchitecture (Vaswani et al. 2017), which is capable of\ncapturing long dependencies. However, the original Trans-\nformer architecture is not designed for programs, and can-\nnot utilize tree structures, i.e., the second above mentioned\nchallenge. A standard way of utilizing structural informa-\ntion, as in graph- and tree-based convolutional neural net-\nworks, is to combine the vector representations of a node\nand its structural neighbors as the output of a structural con-\nvolution sub-layer. However, a standard Transformer archi-\ntecture does not have such structural convolution sub-layers,\nand it is not clear where to add them.\nIt is tempting to add structural convolution sub-layers in\nall the Transformer blocks. Our core conjecture is that when\nconvolving a node and its structural neighbors, the vector\nrepresentation should mainly contain the information from\nthe original node. As the vector representation of the nodes\nis processed by more blocks in the decoder of the Trans-\nformer, they gradually mix in more information from other\nnodes and lose their original information. Therefore, we add\narXiv:1911.09983v2  [cs.LG]  28 Nov 2019\nroot\nModule\nbody\nAssign\ntargets\nName\nid\nlength\nNum\nn\n10\nvalue\n1: root -> Module\n2: Module -> body\n3: body -> Assign\n10: Assign -> \ntargets value\n11: targets -> Name\n13: Name -> id\n64: id -> length\n18: value -> Num\n19: Num -> n\n8: n -> 10\nRule Sequence:\n1 2 3 10 11 13 64 18 19 8\nFigure 1: A Python AST for code “length = 10”\nthe structural convolution sub-layer only to the ﬁrst several\nTransformer decoder blocks but not all.\nGenerally speaking, the TreeGen architecture consists of\nthree parts: (1) a natural language (NL) reader (encoder)\nencodes the text description; (2) an AST reader (the ﬁrst\nseveral Transformer decoder blocks) encodes the previously\ngenerated partial code with the structural convolution sub-\nlayers; (3) a decoder (the rest Transformer decoder blocks)\ncombines the query (the node to be expanded in AST) and\nthe previous two encoders to predict the next grammar rule.\nWe evaluated our model on an established benchmark\ndataset for Python code generation, HearthStone (Ling et\nal. 2016), which is a Python implementation of a card\ngame HearthStone. The results show that our model sig-\nniﬁcantly outperforms previous models by 4.5 percentage\npoints. We further evaluated our model on two semantic\nparsing datasets, ATIS and GEO, which translate natural lan-\nguage sentences into lambda calculus logical forms. The re-\nsults show that our model has the best accuracy among pre-\nvious neural models, with 89.1% and 89.6% accuracy, re-\nspectively. Our evaluation also shows that adding the struc-\ntural convolution sub-layer to the ﬁrst several Transformer\nblocks signiﬁcantly outperforms a Transformer with struc-\ntural convolution in all blocks.\nOur Model\nWe generate code by predicting the grammar rules of the\nprogramming language. Figure 2 shows the overall picture\nof our model, which comprises three parts: an NL reader, an\nAST reader, and decoder. We introduce them in detail in the\nfollowing subsections.\nGrammar Rule Prediction\nIn this section, we introduce how to model code genera-\ntion as a series of classiﬁcation problems of grammar rules.\nThe programs can be decomposed into several context-free\ngrammar rules and parsed as an AST. For example, Figure 1\nshows a Python AST for the code “ length = 10”, where\ndotted boxes are terminal symbols and solid boxes are non-\nterminal symbols.\nAST-based code generation could be thought of as ex-\npanding a non-terminal node by a grammar rule. This pro-\ncess is repeated until all leaf nodes are terminal. In Figure 1,\n“1: root - > Module” is an example of the grammar rules,\nwhere the preceding number is the ID of the rules. Follow-\ning the pre-order traverse, we could obtain the sequence of\nrules that generate the AST shown in the top right corner.\nFormally, the probability can be factorized as the proba-\nbilities of the rules generating the code following the order.\np(code) =\n∏P\ni=1\np(ri |NL input,ri,··· ,ri−1) (1)\nwhere ri is the ith rule in the rule sequence. In this way,\nour task is to train a model to calculate p(ri |NL input,pi),\ni.e., given the natural language description and the currently\ngenerated partial AST the model calculates the probabilities\nof the rules to expand this node.\nNL Reader\nThe input description determines the functionality of the\ncode. It can be a semi-structural description as in the Hearth-\nStone dataset, or a natural language as in ATIS and GEO\nsemantic parsing datasets.\nFor an input description, we ﬁrst tokenize it into to-\nkens n1,n2,··· ,nL, where L denotes the length of\nthe input. Each token ni is then split to characters\nc(ni)\n1 ,c(ni)\n2 ,··· ,c(ni)\nS , where S is the number of characters\nin ni. All the tokens and characters are represented as real-\nvalued vectors n1,n2,··· ,nL and c(ni)\n1 ,c(ni)\n2 ,··· ,c(ni)\nS\nby embeddings.\nInput Text Representation\nCharacter Embedding. It often happens that similar words\nhave similar characters (e.g., “program” and “programs”).\nTo utilize this property, we represent a token by character\nembeddings with a fully-connected layer\nn(c)\ni = W(c)[c(ni)\n1 ; ··· ; c(ni)\nM ] (2)\nwhere W(c) are the weights and the character sequence is\npadded to a pre-deﬁned maximum lengthM. After the fully-\nconnected layer, we also apply layer normalization (Lei Ba,\nKiros, and Hinton 2016). These vectors are then fed to the\nNL reader, and are integrated with the word embeddings by\na gating sub-layer.\nNeural Structure of NL Reader. The NL reader is com-\nposed of a stack of blocks ( Nd blocks in total). Each block\ncontains three different sub-layers (namely, self-attention,\ngating mechanism, and word convolution) to extract fea-\ntures, which we introduce in detail in the following subsec-\ntions. Between two sub-layers, we employ a residual con-\nnection (He et al. 2016) followed by a layer normalization.\nSelf-Attention. The self-attention sub-layer follows the\nTransformer’s architecture (Vaswani et al. 2017), and uses\nmulti-head attention to capture long dependency informa-\ntion.\nFor a sequence of input tokens n1,n2,··· ,nL, we rep-\nresent them as an embedding n1,n2,··· ,nL by a look-up\nConv\nGating\nTree Conv\nNL Attention\nSelf attention\nCharacter\nEmbedding Gating\nRule\nDefinition\nEncoding\nDense\nNL Attention\nAST Attention\nSoftmax & Pointer\nNatural Language Description\n(Input)\n+Position\nEmbedding\nSelf Attention\n+Position\nEmbedding\nRule Sequence\n(Generated Code)\nTree Path\n(Query)\nNd x N1 x N2 x\nNL Reader AST Reader Decoder\nDepth\nEmbedding\nFigure 2: Overview of the TreeGen.\ntable. We also use position embeddings to encode the infor-\nmation of word positions. In particular, we adopt the variant\nin Dehghani et al. (2018), and compute the position embed-\nding for the ith word in the bth Transformer block as\npb,i[2j] = sin((i+ b)/(100002j/d)) (3)\npb,i[2j+ 1] = sin((i+ b)/(100002j/d)) (4)\nwhere pi,b[·] indexes a dimension of the vector pi,b, and dis\nthe number of dimensions (i.e., embedding size).\nA Transformer block learns non-linear features by\nmulti-head attention, which yields a matrix Y(self)\nb =\n[y(self)\nb,1 ,y(self)\nb,2 ,··· ,y(self)\nb,L ]⊤, where Y(self)\nb ∈RL×d. For no-\ntational simplicity, we omit the subscript b. The multi-head\nlayer is computed by\nY(self) = concat(head1,··· ,headH)Wh (5)\nwhere Hdenotes the number of heads andWh is the weight.\nAn attention layer is applied in each head headt, computed\nby\nheadt = softmax(QK⊤\n√dk\n)V (6)\nwhere dk = d/Hdenotes the length of each features vector.\nQ, Kand V are computed by\n[Q,K,V ] = [x1,··· ,xL]⊤[WQ,WK,WV ] (7)\nwhere WQ,WK,WV ∈ Rd×dk are model parameters. xi\nis the input of this Transformer block. For the ﬁrst block,\nit is the vector sum of the table-lookup embedding and the\nposition embedding, i.e., ni + p1,i; for other blocks, it is the\nvector sum of the lower Transformer block’s output and the\nposition embedding that corresponds to this block.\nGating Mechanism. After the features are computed by\nself-attention, we further incorporate with the information\nof character embeddings. This is given by a gating mech-\nanism based on softmax. For the ith word, we compute a\ncontrol vector qi from y(self)\ni by a linear transformation. The\nsoftmax weight k(c)\ni for character embedding is given by a\nlinear transformation from n(c)\ni in Equation 2. The softmax\nweight k(y)\ni for Transformer’s output is given by another lin-\near transformation from y(self)\ni . Then, the gate is computed\nby\n[α(y)\ni,t,α(c)\ni,t] = softmax{q⊤\ni k(y)\ni ,q⊤\ni k(c)\ni } (8)\nThey are used to weigh the feature of the Transformer’s\nlayer v(y)\ni and the feature of character embeddings v(c)\ni , lin-\near transformed from y(self)\ni and n(c)\ni , respectively.\nhi,t = [α(y)\ni,tv(y)\ni + α(c)\ni,tv(c)\ni ] (9)\nSimilar to Equation 5, the output of our gating mechanism\nis Y(gate) = (hi,t)i,t, where (·)i,t represents a block matrix\nwith the elements being hi,t.\nWord Convolution. Finally, two convolutional layers\nare applied to the output of the gating mechanism\ny(gate)\n1 ,··· ,y(gate)\nL and to extract the local features around\neach token y(conv,l)\n1 ,··· ,y(conv,l)\nL , where l denotes the layer\nof convolutional layers. The y(conv,l)\ni is computed by\ny(conv,l)\ni = W(conv,l)[y(conv,l−1)\ni−w ; ··· ; y(conv,l−1)\ni+w ] (10)\nwhere W(conv,l) are the convolution weights,w= (k−1)/2,\nand k denotes the window size. In particular, y(conv,0)\ni de-\nnotes the output of gating mechanism y(gate)\ni . In these layers,\nseparable convolution (Chollet 2017) is used. The reason\nis separable convolution has fewer parameters that is easy\nfor training. For the ﬁrst and the last words, we add zero\npadding. Between these layers, we use theGELU activation\nfunction (Hendrycks and Gimpel 2016).\nIn summary, the NL reader has a few Transformer blocks\nof self-attention, the gating mechanism, and word convolu-\ntion. The natural language description is encoded as features\ny(NL)\n1 ,y(NL)\n2 ,··· ,y(NL)\nL .\nAST Reader\nWe design an AST reader to model the structure of the par-\ntial AST that has generated. Although our programs are gen-\nerated by predicting the sequence of grammar rules, these\nrules alone lack a concrete picture of the program and are\ninsufﬁcient for predicting the next rule. Therefore, our AST\nreader considers heterogeneous information, including the\npredicted rules and the tree structures.\nTo incorporate such program-speciﬁc information, we\nﬁrst represent the code as a sequence of rules, then encode\nthe rules with attention mechanism, and ﬁnally use a tree\nconvolution layer to combine the encoded representation of\neach node with its ancestors.\nAST Representation\nRule Sequence Embedding. To encode the rule informa-\ntion, we use the ID of the rules. Suppose we have a sequence\nof rules r1,r2,··· ,rP that are been used to generate the par-\ntial AST in a decoding step, where P denotes the length of\nthe sequence. We represent these rules as real-valued vectors\nr1,r2,··· ,rP by table-lookup embeddings.\nRule Deﬁnition Encoding. The above table-lookup em-\nbedding treats a grammar rule as an atomic token, and loses\nthe information of the rule’s content.\nTo alleviate this problem, we enhance the representation\nof a rule with the encoding of rule deﬁnition.\nFor a grammar rule i : α →β1 ···βK, where α is the\nparent node and β1 ···βK are child nodes. They can be ei-\nther terminal or non-terminal symbols. The index iis the ID\nof the rule.\nSimilar to Equation 2, we encode the rule content as a vec-\ntor r(c) by a fully-connected layer with input being the table-\nlookup embeddings α,β1,··· ,βK of respective symbols.\nIt is noted that the sequence is also padded to a maximum\nlength.\nThen the rule deﬁnition features y(rule)\n1 ,··· ,y(rule)\nP are\ncomputed by another fully-connected layer as\ny(rule)\ni = W(rule)[ri; r(c); α] (11)\nwhere ri is the table-lookup embedding of the ruleri, r(c)\ni is\nthe content-encoding rule representation, and we emphasize\nthe parent node αagain. After that, a layer normalization is\nfollowed.\nPosition and Depth Embeddings. Since our AST reader\nwould use self-attention mechanisms, we need to represent\nthe position where a grammar rule is used.\nWe ﬁrst adopt the position embedding as in Equation 4,\nrepresenting when a rule is used in the sequencer1,··· ,rP .\nThe position embeddings are denoted by p(r)\n1 ··· ,p(r)\nP\nHowever, such position embedding does not capture the\nposition of a rule in the AST. We further encode such in-\nformation by a depth embedding . If we expand a symbol\nα by the rule r : α →β1 ···βK, we represent the depth\nof the rule by its parent node, i.e., α. In this way, we\nassociate another sequence of table-lookup depth embed-\ndings d1,··· ,dP to the sequence of used grammar rules\nr1,··· ,rP .\nNeural Structure of AST Reader. The AST reader is\nalso composed of a stack of blocks ( N1 blocks in total).\nEach block is decomposed into four sub-layers (namely,\nself-attention, a gating mechanism, NL attention, and tree\nconvolution). We employ a residual connection around each\nsub-layer except the layer of tree convolution. After each\nsub-layer, we apply a layer normalization.\nSelf-Attention. To capture the information of AST, we\nbuild a Transformer-like self-attention layer, where the in-\nput is sum of the rule embedding, position embedding, and\ndepth embedding, i.e., ri + di + p(r)\ni . The self-attention sub-\nlayer extract featuresy(ast-self)\n1 ,y(ast-self)\n2 ,··· ,y(ast-self)\nP of AST\ninput, using the same mechanism as Equations 4, 5, 6 with\ndifferent weights but add an additional depth embedding to\np(r)\ni .\nGating Mechanism. We would like to incorporate\nthe content-encoding rule representation y(rule)\ni into the\nTransformer-extracted features. We adopt a gating mecha-\nnism as in Equations 8, 9, and the fused features becomes\ny(ast-g)\n1 ,y(ast-g)\n2 ,··· ,y(ast-g)\nP after this sub-layer.\nNL Attention. During the decoding step, we should be in-\nformed of the input NL description. This is given by a multi-\nhead NL attention, similar to the Transformer decoder’s at-\ntention to its encoder (Vaswani et al. 2017). The extracted\nfeatures are denoted byy(ast-nl)\n1 ,y(ast-nl)\n2 ,··· ,y(ast-nl)\nP .\nTree Convolution. Should we consider only the above\nsub-layers, it would be hard for the reader to combine the\ninformation of a node with its ancestors. A node can be far\naway from its ancestors in the rule sequence but is close\nin structure. Therefore, it is difﬁcult for a traditional Trans-\nformer to extract such structural features.\nWe integrate the features of a node with those of its an-\ncestors. We treat the AST as a graph and use an adjacency\nmatrix M to represent the directed graph. If a node αi is the\nparent of αj, then Mji = 1. Suppose all the nodes are pre-\nsented by features f1,··· ,fn, their parents’ features can be\ngiven by the multiplication with the adjacency matrix:\n[f(par)\n1 ,··· ,f(par)\nn ] = [f1,··· ,fn]M (12)\nwhere f(par)\ni denotes the parent of theith node. For the father\nof the root node, we pad it with the feature vector of the root\nnode itself.\nThe tree-based convolution window, applied to the current\nsub-tree, is given by\nY(tconv, l) = f(W(tconv, l)[Y(tconv, l−1);\nY(tconv, l−1)M; ··· ; Y(tconv, l−1)Mkt−1])\n(13)\nwhere W(tconv, l) is the weights of the convolutional layer,\nkt denotes the window size (we set to 3 in our experi-\nments), lis the layer of these convolutional layers. In partic-\nular, Y(tconv, 0) = [y(att)\n1 ,y(att)\n2 ,··· ,y(att)\nP ], where Y(tconv, 0) ∈\nRd×P . For the last layer of the AST reader, we add addi-\ntional two convoluation layers. In the equation,f is the acti-\nvation function and GELU is applied between these layers.\nIn summary, the AST reader has N1 blocks of these four\nsub-layers, and yields the features y(ast)\n1 ,y(ast)\n2 ,··· ,y(ast)\nP .\nDecoder\nOur ﬁnal component is a decoder that integrates the infor-\nmation of the generated code with the NL description, and\npredicts the next grammar rule. Similar to the AST reader,\na stack of blocks (N2 blocks in total) each with several sub-\nlayers is used in the decoder as follows. A residual connec-\ntion is also employed around each sub-layer followed by a\nlayer normalization.\nThe decoder takes the non-terminal node to be expanded\nas a query. Inspired by a previous approach (Sun et al. 2019),\nthe querying node is represented as a path from the root to\nthe node to be expanded. For example, if we are going to\nexpand node “Assign” in Figure 1, the path should be root,\nModule, body, Assign. We represent the nodes in this path as\nreal-valued vectors. Then we apply a fully-connected layer\nlike Equation 2 to these vectors and the output of the path\n(querying node) is q(path)\ni .\nWe then apply two attention layers to integrate the outputs\nof the AST reader and the NL reader.\nWe ﬁrst apply an AST attention layer over the out-\nput of the AST reader with queries and extract features\nf(tree)\n1 ,··· ,f(tree)\nP . In this layer, Qis computed from queries\nq(path)\n1 ,··· ,q(path)\nP ; K and V are computed from the code\nfeatures y(ast)\n1 ,··· ,y(ast)\nP . We further integrate the features\nfrom the input description. This integration is also imple-\nmented with an NL attention, where Qis computed by fea-\nture f(tree)\n1 ,··· ,f(tree)\nP ; and K and V are computed by the\ninput description y(NL)\n1 ,··· ,y(NL)\nL .\nFinally, a set of two fully-connected layers, where the ﬁrst\nlayer has a GELU activation function, are followed to ex-\ntract features for prediction.\nTraining and Inference\nWe predict the next grammar rule, among all possible candi-\ndates, by softmax based on the decoder’s last layer features.\nWe also introduce the pointer network (See, Liu, and\nManning 2017) (essentially, an attention) that can directly\ncopy a token afrom the NL description. In this case, the re-\nsulting grammar rule is α →a, where αis a non-terminal\nsymbol to be expanded and a is a terminal symbol. Such\npointer mechanism is helpful for user-deﬁned identiﬁers\n(e.g., variable and function names).\nThe choice between softmax rule prediction and the\npointer network is given by another gating mechanism pg,\nalso computed from the decoder’s last feature. The overall\npredicted probability of the next grammar rule is\np(ri|·) =\n{pg p(ri|·) if i∈D\n(1 −pg) Pr{copy word tat step i|·} if i∈C\n(14)\nwhere idenotes the ID of the rule, D is the set of predeﬁned\nrules, and C denotes the set of rules in the form of α →a,\nwhere ais a terminal token that occurs in the NL description.\npg is the probability of using the type of predeﬁned rules,\nand the p(ri|·) (the probability of each predeﬁned rules) are\ncomputed by two single-layer perceptrons with the sigmoid\nand softmax activation functions, respectively, and the input\nof these layers are the features h(dec).\nNAME: Darkscale Healer\nATK: 4\nDEF: 5\nCOST: 5\nDUR: -1\nTYPE: Minion\nPLAYER: Neutral\nRACE: NIL\nRARITY: Common\nDESCRIPTION: \n<b>Battlecry:</b> Restore 2 Health to all \nfriendly characters.\nFigure 3: A example of the implement of HearthStone.\nThe pointer network is computed by\nξt = vT tanh(W1h(dec) + W2y(NL)\nt )\nPr{copy word tat step i|·}= exp {ξt}∑L\nj=1 exp {ξj}\n(15)\nwhere h(dec) denotes the decoder’s last feature. The model\nis optimized by maximizing negative log likelihood loss\nagainst the reference program.\nThe inference starts with a start rule, start : snode −→\nroot, expanding a special symbol snode to the root symbol.\nThe recursive prediction terminates if every leaf node in the\npredicted AST is a terminal. During prediction, we use beam\nsearch with a size of 5. Invalid rules are excluded during\nbeam search.\nEvaluation\nWe evaluated our approach on two types of benchmarks: (1)\na Python code generation benchmark, HearthStone, and (2)\ntwo semantic parsing benchmarks, ATIS and GEO.\nExperiment: HearthStone\nDataset. We ﬁrst evaluated our approach on the Hearth-\nStone benchmark (Ling et al. 2016). The benchmark con-\ntains Python code that implements 665 different cards of\nHearthStone. Each card is composed of a semi-structural\ndescription and a groundtruth Python program. The Python\nprograms have a length of 84 tokens on average. The de-\nscription comes with several attributes such as card name,\ncard type, as well as a natural language description for\nthe functionality of the card. A Python program is mainly\ndecided by the natural language description where the at-\ntributes decide the constants or identiﬁer names. A sample\ndescription and its corresponding Python program are shown\nin Figure 3. When preprocessing the card description into to-\nken sequences, existing approaches consider two methods.\nThe ﬁrst (Yin and Neubig 2017; Hayati et al. 2018) (called\nplain preprocessing) treats the whole description as plain\ntext and delimit the tokens by standard separators such as\nspace or periods. The second (Rabinovich, Stern, and Klein\n2017) (called structural preprocessing) treats the descrip-\ntions as semi-structural and always treat an attribute as one\ntoken. In this experiment, we consider both methods and de-\nnote the results corresponding to the plain preprocessing as\nTreeGen-A and that corresponding to the structural prepro-\ncessing as TreeGen-B. We followed the train-dev-test split\nin Ling et al. (2016), and the statistic is listed in Table 2.\nMetrics. We measured the performance following the\nmetrics in Sun et al. (2019). We computed the StrAcc, which\nis the percentage of programs that has exactly the same token\nsequence as the ground truth; the BLEU score, which is used\nto measure the similarity between the generated code and\nthe reference code at the token level; and the Acc+, which\nis evaluated manually, allows variable renaming on top of\nStrAcc, for every test case.\nSettings. For neural networks, we set the number of NL\nreader layers Nd = 6, and N1 = N2 = 5 for the AST\nreader as well as the decoder. The size of all embedding is\n256. The hidden sizes were all set to the 256 except each\nfully-connected layers, except the ﬁrst layer was 1024 di-\nmensions. We applied dropout after each layer (including at-\ntention layers, gating mechanism layers, convolutional lay-\ners, and fully-connected layers, where the drop rate is 0.15).\nThe model is optimized by Adafactor (Shazeer and Stern\n2018) with default parameters.\nOverall Results. We show the results in Table 1. In this\ntable, the structural preprocessing has a better performance\ncompared with the plain preprocessing.\nAs shown, our model achieves 6 percentage points ac-\ncuracy improvement with plain preprocessing and 4.5 per-\ncentage points accuracy improvement with structural pre-\nprocessing. For the BLEU score, our model also achieves\nthe best results. These boosts in performance indicate that\nTreeGen successfully alleviates the long dependency prob-\nlem and effectively encodes the structural information in the\ncode generation.\nTime Efﬁciency. We further evaluated the complexity of\nour model on the HearthStone, and the result shows that our\nmodel is faster than the previous ones. It takes 18s for an\nepoch on a single Nvidia Titan XP, whereas 180s for the\nCNN (Sun et al. 2019) and 49s for the RNN (Yin and Neubig\n2017).\nLocation of Structural Convolution Sub-layer. One of\nthe keys of our approach is to add the structural convolution\nsub-layers only to part of the Transformer blocks in the de-\ncoder. To evaluate whether this design decision is effective,\nwe evaluate four competing settings: 1) adding the struc-\ntural convolution sub-layers to all Transformer blocks (i.e.,\nN1 = 10); 2) adding the structural convolution sub-layers to\nthe ﬁrst 7 blocks in AST reader (i.e.,N1 = 10(7)); 3) adding\nthe structural convolution sub-layers to the ﬁrst 8 blocks in\nAST reader (i.e., N1 = 10(8)); 4) the other adds to none\n(i.e., N1 = 0). As we can see, from Table 1 our approach\nadding the sub-layer to all transformer blocks ( N1 = 10)\nsigniﬁcantly outperforms the last setting ( N1 = 0), but\nslightly worse than the other two settings.\nAblation Test. We ablated our model (TreeGen-B was\nused) to analyze the contribution of each component, results\nalso shown in Table 1. First, we compared our model with\nthe traditional Transformer, which is a Transformer with-\nout effective structure modeling. We achieved 21 percentage\npoints higher accuracy (p-value is less than 0.001) and 12\nhigher BLEU score. This result provides strong evidence of\nthe effectiveness of the AST reader in our model and the im-\nportance of the structural information. Next, we replaced the\ntree convolutional layers in the AST Reader with two layers\nof fully-connected layers, and we removed the char embed-\nding, rule deﬁnition encoding, self-attention layers in turn.\nThe experimental results show the identiﬁers-encoding, alle-\nviating long-dependency and structural information signiﬁ-\ncantly inﬂuence the accuracy. Please note that in some cases\nBLEU increases while StrAcc and Acc+ decrease. Here we\nconsider StrAcc and Acc+ more important as they guarantee\nthe correctness of the generated programs and correctness is\nusually crucial in code generation.\nExperiment II: Semantic Parsing\nDataset. We further evaluated our approach on the seman-\ntic parsing tasks. Our experiment was conducted on two se-\nmantic parsing datasets, ATIS and GEO. The input of these\ndatasets is a natural language description, while the output\nis a short piece of code in lambda calculus. We followed the\nstandard train-dev-test split of these datasets, and the statis-\ntics are listed in Table 2.\nMetrics and Settings. In this task, we follow the evalua-\ntion of the previous approaches (Dong and Lapata 2016) and\nuse accuracy as the metric, where the tree exact match was\nconsidered to avoid spurious errors. In other words, the or-\nder of the children can be changed within conjunction nodes.\nWe followed all the settings in the HS experiment except that\nwe changed the embedding size and the hidden sizes to 128\ncompared with the setting of the HS experiment.\nResults. Table 3 shows the performance of our TreeGen.\nAs seen, the accuracy of our approach is sightly worse than\nthe traditional approach WKZ14 (Wang, Kwiatkowski, and\nZettlemoyer 2014), which is based on the CCG parser and\nuses a large number of templates. This traditional approach\nis hard to generalize new datasets. However, our model\nwas directly adopted from the HS dataset, and achieved the\nhighest accuracy, among all neural models (Dong and La-\npata 2016; Rabinovich, Stern, and Klein 2017; Dong and\nLapata 2018; Chen, Sun, and Han 2018; Xu et al. 2018;\nModel StrAcc Acc+ BLEU\nPlain\nLPN (Ling et al. 2016) 6.1 – 67.1SEQ2TREE (Dong and Lapata 2016) 1.5 – 53.4YN17 (Yin and Neubig 2017) 16.2 ∼18.2 75.8ASN (Rabinovich, Stern, and Klein 2017) 18.2 – 77.6ReCode (Hayati et al. 2018) 19.6 – 78.4\nTreeGen-A 25.8 25.8 79.3\nStructural\nASN+SUPATT (Rabinovich, Stern, and Klein 2017) 22.7 – 79.2SZM19 (Sun et al. 2019) 27.3 30.3 79.6\nTreeGen-B 31.8 33.3 80.8\nLocation of Structural Convolutional Sub-layerN1 = 10,N2 = 0 25.8 27.3 80.4N1 = 10(7),N2 = 0 27.3 28.8 78.5N1 = 10(8),N2 = 0 25.8 28.8 78.5N1 = 0,N2 = 10 21.2 22.7 79.6\nAblation testBaseline: Transformer 10.6 ( p= 0.015) 12.1 68.0- Tree Convolution 27.3 ( p= 0.015) 27.3 80.9- Rule Deﬁnition Encoding 27.3 ( p<0.001) 28.8 81.8- Char Embedding 15.2 ( p<0.001) 18.2 72.9- Self-Attention 28.8 ( p<0.001) 28.8 81.0\nTable 1: Performance of our model in comparison with previous state-of-the-art results.\nExp II\nStatistics HS ATIS GEO\n# Train 533 4,434 600\n# Dev 66 491 -\n# Test 66 448 280\nAvg. tokens in description 35.0 10.6 7.4\nMax. tokens in description 76.0 48 23\nAvg. tokens in code 83.2 33.9 28.3\nMax. tokens in code 403 113 144\nTable 2: Statistics of the datasets we used.\nMethod ATIS GEO\nTraditional\nZC07 (Zettlemoyer and Collins 2007) 84.6 86.1FUBL (Kwiatkowski et al. 2011) 82.8 88.6KCAZ13 (Kwiatkowski et al. 2013) - 89.0WKZ14 (Wang, Kwiatkowski, and Zettlemoyer 2014)91.3 90.4\nNeural Networks\nSEQ2SEQ (Dong and Lapata 2016) 84.2 84.6SEQ2TREE (Dong and Lapata 2016) 84.6 87.1ASN (Rabinovich, Stern, and Klein 2017) 85.3 85.7ASN+SUPATT (Rabinovich, Stern, and Klein 2017) 85.9 87.1COARSE2FINE (Dong and Lapata 2018) 87.7 88.2TRANX (Yin and Neubig 2018) 86.2 88.2Seq2Act (Chen, Sun, and Han 2018) 85.5 88.9Graph2Seq (Xu et al. 2018) 85.9 88.1SZM19 (Sun et al. 2019) 85.0 -\nTreeGen 89.1 89.6\nTable 3: Accuracy in semantic parsing (in percentage).\nSun et al. 2019). This experiment shows the effectiveness\nand generalizability of TreeGen.\nRelated Work\nCode generation achieves signiﬁcant progress in recent\nyears. The early approaches are mainly based on templates\n(Zettlemoyer and Collins 2007; Zettlemoyer and Collins\n2005; Kushman and Barzilay 2013; Wang, Kwiatkowski,\nand Zettlemoyer 2014). With the prosperity of deep learn-\ning, the sequence-to-sequence framework has shown to be\neffective in various tasks (Sutskever, Vinyals, and Le 2014).\nLing et al. (2016) applied this framework to generate code\nbased on tokens. Unlike natural languages, it is shown\nthat the code contains much more structural information.\nThus, the abstract syntax tree (AST) was used in more re-\ncent works (Dong and Lapata 2016; Yin and Neubig 2017;\nRabinovich, Stern, and Klein 2017; Hayati et al. 2018;\nYin and Neubig 2018). However, these studies mainly use\nrecurrent neural networks (RNNs) from the long depen-\ndency problem (Bengio, Simard, and Frasconi 1994). Sun\net al. (2019) proposed to use the convolutional neural net-\nwork (CNN) to handle the long dependency problem. Our\napproach addresses this problem by Transformer’s intensive\nattention mechanism (Vaswani et al. 2017). To incorporate\nthe structural information and the idea of self-attention, we\npropose a tree-based Transformer architecture for code gen-\neration.\nConclusion\nIn this work, we propose TreeGen for program generation.\nTreeGen uses the attention mechanism of Transformers to\nalleviate the long-dependency problem and introduces the\nAST reader to combine the grammar rules and the AST\nstructure.\nThe evaluation was conducted on a Python dataset,\nHearthStone, and two semantic parsing datasets, ATIS and\nGEO. The experimental results show that our model signiﬁ-\ncantly outperforms existing approaches. We also conducted\nin-depth ablation tests, which suggests that each component\nin our model plays a signiﬁcant role.\nAcknowledgments\nThis work is sponsored by the National Key Re-\nsearch and Development Program of China under Grant\nNo. 2017YFB1001803, and National Natural Science Foun-\ndation of China under Grant Nos. 61672045, 61529201, and\n61922003. Lili Mou is an Amii Fellow; he is supported by\nthe CCAI Chair Program; and he also thanks AltaML for\nsupport.\nReferences\n[Bengio, Simard, and Frasconi 1994] Bengio, Y .; Simard, P.;\nand Frasconi, P. 1994. Learning long-term dependencies\nwith gradient descent is difﬁcult. IEEE Trans. Neural Net-\nworks 5(2):157–166.\n[Chen, Sun, and Han 2018] Chen, B.; Sun, L.; and Han, X.\n2018. Sequence-to-Action: End-to-End Semantic Graph\nGeneration for Semantic Parsing. In ACL, 766–777.\n[Chollet 2017] Chollet, F. 2017. Xception: Deep learning\nwith depthwise separable convolutions. In CVPR, 1251–\n1258.\n[Dehghani et al. 2018] Dehghani, M.; Gouws, S.; Vinyals,\nO.; Uszkoreit, J.; and Kaiser, Ł. 2018. Universal transform-\ners. arXiv preprint arXiv:1807.03819.\n[Dong and Lapata 2016] Dong, L., and Lapata, M. 2016.\nLanguage to Logical Form with Neural Attention. In ACL,\n33–43.\n[Dong and Lapata 2018] Dong, L., and Lapata, M. 2018.\nCoarse-to-Fine Decoding for Neural Semantic Parsing. In\nACL, 731–742.\n[Hayati et al. 2018] Hayati, S. A.; Olivier, R.; Avvaru, P.;\nYin, P.; Tomasic, A.; and Neubig, G. 2018. Retrieval-Based\nNeural Code Generation. In EMNLP, 925–930.\n[He et al. 2016] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.\nDeep residual learning for image recognition. In CVPR,\n770–778.\n[Hendrycks and Gimpel 2016] Hendrycks, D., and Gimpel,\nK. 2016. Bridging Nonlinearities and Stochastic Regu-\nlarizers with Gaussian Error Linear Units. arXiv preprint\narXiv:1606.08415.\n[Kushman and Barzilay 2013] Kushman, N., and Barzilay,\nR. 2013. Using semantic uniﬁcation to generate regular\nexpressions from natural language. In NAACL, 826–836.\n[Kwiatkowski et al. 2011] Kwiatkowski, T.; Zettlemoyer, L.;\nGoldwater, S.; and Steedman, M. 2011. Lexical general-\nization in CCG grammar induction for semantic parsing. In\nEMNLP, 1512–1523.\n[Kwiatkowski et al. 2013] Kwiatkowski, T.; Choi, E.; Artzi,\nY .; and Zettlemoyer, L. 2013. Scaling semantic parsers with\non-the-ﬂy ontology matching. In EMNLP, 1545–1556.\n[Lei Ba, Kiros, and Hinton 2016] Lei Ba, J.; Kiros, J. R.; and\nHinton, G. E. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\n[Ling et al. 2016] Ling, W.; Blunsom, P.; Grefenstette, E.;\nHermann, K. M.; Ko ˇcisk`y, T.; Wang, F.; and Senior, A.\n2016. Latent Predictor Networks for Code Generation. In\nACL, 599–609.\n[Mou et al. 2016] Mou, L.; Li, G.; Zhang, L.; Wang, T.; and\nJin, Z. 2016. Convolutional neural networks over tree struc-\ntures for programming language processing. InAAAI, 1287–\n1293.\n[Rabinovich, Stern, and Klein 2017] Rabinovich, M.; Stern,\nM.; and Klein, D. 2017. Abstract Syntax Networks for Code\nGeneration and Semantic Parsing. In ACL, 1139–1149.\n[See, Liu, and Manning 2017] See, A.; Liu, P. J.; and Man-\nning, C. D. 2017. Get to the point: Summarization with\npointer-generator networks. In ACL, 1073–1083.\n[Shazeer and Stern 2018] Shazeer, N., and Stern, M. 2018.\nAdafactor: Adaptive learning rates with sublinear memory\ncost. arXiv preprint arXiv:1804.04235.\n[Sun et al. 2019] Sun, Z.; Zhu, Q.; Mou, L.; Xiong, Y .; Li,\nG.; and Zhang, L. 2019. A grammar-based structural cnn\ndecoder for code generation. In AAAI, volume 33, 7055–\n7062.\n[Sutskever, Vinyals, and Le 2014] Sutskever, I.; Vinyals, O.;\nand Le, Q. V . 2014. Sequence to sequence learning with\nneural networks. In NIPS, 3104–3112.\n[Vaswani et al. 2017] Vaswani, A.; Shazeer, N.; Parmar, N.;\nUszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polo-\nsukhin, I. 2017. Attention is all you need. In NIPS, 6000–\n6010.\n[Wang, Kwiatkowski, and Zettlemoyer 2014] Wang, A.;\nKwiatkowski, T.; and Zettlemoyer, L. 2014. Morpho-\nsyntactic lexical generalization for CCG semantic parsing.\nIn EMNLP, 1284–1295.\n[Xu et al. 2018] Xu, K.; Wu, L.; Wang, Z.; Yu, M.; Chen, L.;\nand Sheinin, V . 2018. Exploiting Rich Syntactic Information\nfor Semantic Parsing with Graph-to-Sequence Model. In\nACL, 918–924.\n[Yin and Neubig 2017] Yin, P., and Neubig, G. 2017. A Syn-\ntactic Neural Model for General-Purpose Code Generation.\nIn ACL, 440–450.\n[Yin and Neubig 2018] Yin, P., and Neubig, G. 2018.\nTRANX: A transition-based neural abstract syntax parser\nfor semantic parsing and code generation. In EMNLP, 7–\n12.\n[Zettlemoyer and Collins 2005] Zettlemoyer, L. S., and\nCollins, M. 2005. Learning to map sentences to logical\nform: structured classiﬁcation with probabilistic categorial\ngrammars. In UAI, 658–666.\n[Zettlemoyer and Collins 2007] Zettlemoyer, L., and\nCollins, M. 2007. Online learning of relaxed CCG\ngrammars for parsing to logical form. In EMNLP-CoNLL,\n678–687.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8320634365081787
    },
    {
      "name": "Code generation",
      "score": 0.6475081443786621
    },
    {
      "name": "Python (programming language)",
      "score": 0.64580237865448
    },
    {
      "name": "Parsing",
      "score": 0.575635552406311
    },
    {
      "name": "Programming language",
      "score": 0.5722618103027344
    },
    {
      "name": "Parse tree",
      "score": 0.4860554039478302
    },
    {
      "name": "Redundant code",
      "score": 0.4513406753540039
    },
    {
      "name": "Encoder",
      "score": 0.4486826956272125
    },
    {
      "name": "Source code",
      "score": 0.4441743791103363
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4163912534713745
    },
    {
      "name": "Transformer",
      "score": 0.41278648376464844
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3309856355190277
    },
    {
      "name": "Operating system",
      "score": 0.09883806109428406
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}