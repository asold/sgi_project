{
  "title": "Improved Hierarchical Patient Classification with Language Model Pretraining over Clinical Notes",
  "url": "https://openalex.org/W2983359628",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4306442253",
      "name": "Kemp, Jonas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4302632707",
      "name": "Rajkomar, Alvin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224379245",
      "name": "Dai, Andrew M.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963022149",
    "https://openalex.org/W2518582440",
    "https://openalex.org/W2893892260",
    "https://openalex.org/W2783824366",
    "https://openalex.org/W2757643185",
    "https://openalex.org/W3011483410",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2963354094",
    "https://openalex.org/W2510842514",
    "https://openalex.org/W2395172628",
    "https://openalex.org/W2897815444",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2404901863",
    "https://openalex.org/W2887004720",
    "https://openalex.org/W2511950764",
    "https://openalex.org/W2964010366",
    "https://openalex.org/W2514798530",
    "https://openalex.org/W2409027918",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2950035161",
    "https://openalex.org/W2141173017",
    "https://openalex.org/W2885828205",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2094791001",
    "https://openalex.org/W2605266158",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2488984245",
    "https://openalex.org/W2534858547",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W2072647517",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2963576560",
    "https://openalex.org/W2768377508",
    "https://openalex.org/W3098949126",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2617980815",
    "https://openalex.org/W2789244308",
    "https://openalex.org/W2481271618",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2616619952",
    "https://openalex.org/W2906103632",
    "https://openalex.org/W2985962305",
    "https://openalex.org/W2964142373",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2625625371",
    "https://openalex.org/W2963618814",
    "https://openalex.org/W2892109811"
  ],
  "abstract": "Clinical notes in electronic health records contain highly heterogeneous writing styles, including non-standard terminology or abbreviations. Using these notes in predictive modeling has traditionally required preprocessing (e.g. taking frequent terms or topic modeling) that removes much of the richness of the source data. We propose a pretrained hierarchical recurrent neural network model that parses minimally processed clinical notes in an intuitive fashion, and show that it improves performance for discharge diagnosis classification tasks on the Medical Information Mart for Intensive Care III (MIMIC-III) dataset, compared to models that treat the notes as an unordered collection of terms or that conduct no pretraining. We also apply an attribution technique to examples to identify the words that the model uses to make its prediction, and show the importance of the words' nearby context.",
  "full_text": "Improved hierarchical patient classiﬁcation with\nlanguage model pretraining over clinical notes\nJonas Kemp∗ Alvin Rajkomar Andrew M. Dai\nGoogle Health\n{jonasbkemp,alvinrajkomar,adai}@google.com\nAbstract\nClinical notes in electronic health records contain highly heterogeneous writing\nstyles, including non-standard terminology or abbreviations. Using these notes in\npredictive modeling has traditionally required preprocessing (e.g. taking frequent\nterms or topic modeling) that removes much of the richness of the source data.\nWe propose a pretrained hierarchical recurrent neural network model that parses\nminimally processed clinical notes in an intuitive fashion, and show that it improves\nperformance for discharge diagnosis classiﬁcation tasks on the Medical Information\nMart for Intensive Care III (MIMIC-III) dataset, compared to models that treat the\nnotes as an unordered collection of terms or that conduct no pretraining. We also\napply an attribution technique to examples to identify the words that the model\nuses to make its prediction, and show the importance of the words’ nearby context.\n1 Introduction\nWith the rapid deployment of electronic health records (EHRs) in the US, clinicians routinely\nenter patient data electronically, mostly in unstructured, free-text clinical notes. Because clinicians\nfrequently employ non-standard, ambiguous shorthand phrases or organize their notes in unpredictable\nways, automated parsing for downstream use can be quite challenging. Traditional natural language\nprocessing (NLP) techniques relying on hand-crafted rules [1] or feature engineering can be difﬁcult\nto apply in this context. In practice, machine learning models tend to make more use of structured\nﬁelds such as medications and diagnoses that can be straightforwardly extracted from the EHR [2],\nand clinical notes are often ignored outright [3–11]. Models that do use notes frequently reduce them\nto an unordered set of words [12–14] or topics [15, 16], which ignores many subtleties of language\nand context and can therefore obscure the meaning of the note.\nRecent advances in deep learning have led to major improvements in a wide variety of NLP applica-\ntions [17, 18]. Building on this work, we propose a model employing sequential, hierarchical, and\npretraining (SHiP) techniques from deep NLP to improve EHR predictive models by automatically\nlearning to extract relevant information from clinical notes. Speciﬁcally, our model employs a hierar-\nchical attention network [19], augmented with a language model pretraining objective [20], to read\nnotes with minimal asssumptions about the text. We evaluate our model on standard classiﬁcation\ntasks for EHRs, and compare performance against existing state-of-the-art baselines [14]. We also\nevaluate the sensitivity of the model’s outputs to different phrases in the text using deep learning\nattribution methods [21]. To our knowledge, the effectiveness of language model pretraining has not\nbeen previously demonstrated for hierarchical classiﬁcation models.\n∗Work completed in part during the Google AI Residency.\nMachine Learning for Health (ML4H) at NeurIPS 2019 - extended abstract.\narXiv:1909.03039v3  [cs.LG]  15 Nov 2019\n2 Methods\n2.1 Dataset and Prediction Tasks\nWe developed our models using critical care data from the Medical Information Mart for Intensive\nCare (MIMIC-III) [22, 23]. We represented patients’ medical histories as a time series according to\nthe Fast Healthcare Interoperability Resources (FHIR) speciﬁcation, as described in previous work\n[14]. The study cohort included all patients in MIMIC-III hospitalized for at least 24 hours. (See\nsupplementary table 2 for cohort summary statistics.) From these records, we extracted basic en-\ncounter information (admission type, status, and source), diagnosis and procedure codes, medication\norders, quantitative observations (lab results and vital signs), and free-text clinical notes. For each\ncontinuous feature, we standardized values to Z-scores using training set statistics, with any outliers\nmore than 10 standard deviations from the mean capped to a score of ±10. For each hospitalization,\nwe developed models for the following classiﬁcation tasks, using the patient’s full history up to the\nspeciﬁed time in the current admission (including all past hospitalizations):\n• Inpatient mortality prediction (predicted 24 hours after admission).\n• Primary CCS [24] discharge diagnosis code (predicted at the moment of discharge).\n• All ICD-9 [25] discharge diagnosis codes (predicted at the moment of discharge).\n2.2 Model Architecture\nWe built on a core embedding scheme and top-level LSTM architecture described in previous work\n[14]. In this framework, we embedded discrete features from the patient record (e.g. diagnosis codes)\nand trained these jointly with the model. To reduce sequence length, we grouped observations into\nﬁxed-length timesteps, or “bags,” and averaged all embeddings or continuous values for observations\nof the same feature within the same bag; additionally, we collapsed all observations occurring prior\nto the most recent ttimesteps into a single bag (with bag duration and ttuned as hyperparameters).\nFinally, we concatenated the bagged embeddings or values for all features into a single representation\nof each timestep in the patient history, and we fed this embedded sequence into a long short-term\nmemory (LSTM) network [26], generating predictions from the ﬁnal hidden state.\nIn the standard bag-of-words (BOW) approach to these models, notes are treated just as any other\ndiscrete feature, with individual words embedded and aggregated without regard to ordering. Our\nSHiP models augmented this approach in two ways. First, we maintained the sequential order\nof embeddings within each note and fed these to a second LSTM to generate a context-sensitive\nrepresentation for each word. We computed the ﬁnal output vector for each note by applying\nhierarchical dot product attention [ 19] over this output sequence, placing higher weight on the\nportions of the notes most important for downstream prediction. Second, we used unsupervised\nlanguage model pretraining [20] to pretrain the notes LSTM: before optimizing the prediction loss,\nwe trained an auxiliary objective such that, for each word in the note, the LSTM learned to predict\nthe next word (and, if bidirectional, the previous word).\nIn addition to the core BOW and SHiP models, we also compared several variants of the above,\nincluding: a model without notes; models using only notes; a BOW model with unigram and bigram\nembeddings; and hierarchical attention models without pretraining.\n2.3 Attribution Methods\nTo compute attribution scores over the text of notes, we used the path-integrated gradients technique\n[21]. For clarity in these attributions, we ran a notes-only model over only the selected note, omitting\nthe rest of the notes in the patient’s record. We computed attribution scores with respect to each word\nembedding, relative to a zero-vector baseline, using m= 20steps to approximate the path integral.\n3 Results\n3.1 Training and Evaluation Approach\nWe split our cohort by patient ID into 80% train, 10% validation, and 10% test splits. Models\nwere optimized using Adam [ 27], and regularized using dropout [ 28, 29] and Zoneout [ 30]. We\n2\nused a Gaussian process bandit optimization algorithm [31] to select hyperparameters maximizing\nperformance for each task on the validation set. (See supplementary material B, particularly Table 3,\nfor additional details.) Following hyperparameter tuning, we report mean (standard deviation) test set\nmetrics over ﬁve runs from random initialization. Where reported, we also compute the statistical\nsigniﬁcance of pairwise differences in models’ performance using a two-tailed Welch’s t-test.\n3.2 Model Performance\nTable 1: Model performance results on the tasks of interest. Best values for each metric are bolded.\nModel Mortality Primary CCS All ICD-9\nAUPRC AUROC Top-1\nRecall\nTop-5\nRecall\nAUPRC AUROC,\nweighted\nNo notes - 0.449\n(0.006)\n0.869\n(0.001)\n0.526\n(0.006)\n0.796\n(0.003)\n0.305\n(0.001)\n0.873\n(<0.001)\nBag-of-words Unigrams\n(notes only)\n0.383\n(0.004)\n0.832\n(0.003)\n0.591\n(0.004)\n0.849\n(0.002)\n0.328\n(0.002)\n0.880\n(0.001)\nUnigrams (all\nfeatures)\n0.479\n(0.008)\n0.880\n(0.001)\n0.592\n(0.003)\n0.842\n(0.003)\n0.331\n(0.001)\n0.883\n(0.001)\nUnigrams\nand bigrams\n(all features)\n0.460\n(0.005)\n0.872\n(0.002)\n0.587\n(0.008)\n0.829\n(0.005)\n0.325\n(0.002)\n0.881\n(<0.001)\nHierarchical\n(without\npretraining)\nNotes only 0.351\n(0.003)\n0.825\n(0.003)\n0.606\n(0.003)\n0.850\n(0.001)\n0.345\n(0.005)\n0.887\n(0.002)\nAll features 0.471\n(0.006)\n0.876\n(0.003)\n0.591\n(0.008)\n0.833\n(0.006)\n0.301\n(0.004)\n0.868\n(0.001)\nSHiP Notes only 0.353\n(0.005)\n0.825\n(0.004)\n0.667\n(0.006)\n0.897*†\n(0.003)\n0.352†\n(0.001)\n0.891†\n(0.001)\nAll features 0.479\n(0.007)\n0.882\n(0.001)\n0.671*†\n(0.004)\n0.890\n(0.001)\n0.345\n(0.005)\n0.889\n(0.002)\n* p< 0.001 for difference compared to corresponding hierarchical model without pretraining.\n†p< 0.001 for difference compared to best bag-of-words model.\nTable 1 compares the performance of all model variants. The SHiP models signiﬁcantly improved\nover the BOW baselines on the two diagnosis tasks ( p <0.001 under Welch’s t-test): for CCS\nprediction, the best SHiP models improved top-1 recall by 7.9 percentage points and top-5 recall\nby 4.8 percentage points, respectively, over the best BOW models; for ICD-9 prediction, area under\nthe precision-recall curve (AUPRC) increased by 2.1 percentage points and weighted area under the\nROC curve (AUROC) increased by 0.8 percentage points. For mortality prediction, we saw negligible\nbeneﬁt from the SHiP architecture.\nThe SHiP models also improved over the corresponding hierarchical models without pretraining. For\nmortality, pretraining the all-features model increased AUPRC by 0.8 percentage points (p= 0.06)\nand AUROC by 0.6 percentage points ( p = 0.004); for primary CCS, pretraining the all-feature\nmodel increased top-1 recall by 8.0 percentage points (p< 0.001), while pretraining the notes-only\nmodel increased top-5 recall by 4.7 percentage points ( p <0.001); for all ICD-9, pretraining the\nnotes-only model increased AUPRC by 0.7 percentage points (p= 0.03) and weighted AUROC by\n0.4 percentage points (p= 0.01).\n3.3 Qualitative Analysis\nFigure 1 shows examples of path-integrated gradients attribution from CCS prediction models, over\ndischarge summaries from different patients. We observe that the SHiP model frequently concentrates\n3\non just one or a few important phrases, even in very long notes. The choice of phrase is often informed\nby the nearby context: for example, we can see that the SHiP model is consistently most sensitive to\nthe clinically-relevant words following the phrase “discharge diagnoses.” In fact, in each sample here,\nthe patient’s diagnosis is restated elsewhere in the text in a less relevant context (e.g. stating that the\npatient has “no family history” of diabetes), but the model is sensitive only to the instance where the\ndischarge context is made explicit. The bag-of-words model, by contrast, is incapable of making such\ncontextual distinctions, and is generally more sensitive to key words and phrases throughout the text.\nFigure 1: Visualization of integrated gradients attribution over excerpts from patient discharge\nsummaries (primary diagnosis shown at left). For each excerpt, the left column shows attribution\nfrom the BOW baseline, and the right column shows attribution from the SHiP model. Below each\nword is the value of the attribution computed for that word, where a higher absolute value indicates\ngreater importance. Red boxes highlight the patient’s stated diagnosis in the text, while blue boxes\nindicate relevant pieces of nearby context.\n4 Conclusion\nWe demonstrate that SHiP, a novel combination of hierarchical modeling of clinical notes and\nlanguage model pretraining, can improve discharge diagnosis classiﬁcation over previous state-of-the-\nart models, with only minimal preprocessing of text. Our work builds on a substantial recent literature\non applying deep learning techniques to analysis of electronic health records data [ 32], including\nmany clinical NLP studies using more standard convolutional or recurrent architectures [33–37], or\nemploying hierarchical models with limited or no pretraining [38–42]. Drawing on the respective\nsuccesses of hierarchical attention networks [19, 43–46] and pretraining methods [18, 20, 47] in a\nwide variety of general NLP applications, we show the utility of these methods applied jointly, and\nspeciﬁcally within a clinical context.\n4\nAcknowledgments\nWe thank Nissan Hajaj and Xiaobing Liu for developing the core framework used to implement our\nmodels. We thank Gerardo Flores, Kathryn Rough, and Kun Zhang for providing assistance with our\ndata processing and evaluation pipelines. We thank Kai Chen, Michael Howell, and Denny Zhou for\ntheir comments and feedback on this manuscript.\nCode Availability\nA code sample illustrating our approach is available at https://github.com/google-health/records-\nresearch/tree/master/clinical-notes-prediction.\nReferences\n[1] Maxwell Taggart, Wendy W Chapman, Benjamin A Steinberg, Shane Ruckel, Arianna\nPregenzer-Wenzler, Yishuai Du, Jeffrey Ferraro, Brian T Bucher, Donald M Lloyd-Jones,\nMatthew T Rondina, and Rashmee U Shah. Comparison of 2 natural language processing\nmethods for identiﬁcation of bleeding among critically ill patients. JAMA Netw Open, 1(6):\ne183451–e183451, October 2018.\n[2] Michael J Pencina, Benjamin A Goldstein, Ann Marie Navar, and John P A Ioannidis.\nOpportunities and challenges in developing risk prediction models with electronic health\nrecords data: a systematic review. Journal of the American Medical Informatics Asso-\nciation, 24(1):198–208, 05 2016. ISSN 1067-5027. doi: 10.1093/jamia/ocw042. URL\nhttps://doi.org/10.1093/jamia/ocw042.\n[3] Zachary C. Lipton, David C. Kale, Charles Elkan, and Randall Wetzel. Learning to Diagnose\nwith LSTM Recurrent Neural Networks. arXiv e-prints, art. arXiv:1511.03677, Nov 2015.\n[4] Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F Stewart, and Jimeng Sun.\nDoctor AI: Predicting clinical events via recurrent neural networks. In Machine Learning for\nHealthcare Conference, pages 301–318, December 2016.\n[5] Cristobal Esteban, Oliver Staeck, Stephan Baier, Yinchong Yang, and V olker Tresp. Predicting\nclinical events by combining static and dynamic information using recurrent neural networks.\nIn 2016 IEEE International Conference on Healthcare Informatics (ICHI), 2016.\n[6] Paul Nickerson, Patrick Tighe, Benjamin Shickel, and Parisa Rashidi. Deep neural network\narchitectures for forecasting analgesic response. Conf. Proc. IEEE Eng. Med. Biol. Soc., 2016:\n2966–2969, August 2016.\n[7] Trang Pham, Truyen Tran, Dinh Phung, and Svetha Venkatesh. DeepCare: A deep dynamic\nmemory model for predictive medicine. In Advances in Knowledge Discovery and Data Mining,\npages 30–41. Springer, Cham, April 2016.\n[8] Edward Choi, Andy Schuetz, Walter F Stewart, and Jimeng Sun. Using recurrent neural network\nmodels for early detection of heart failure onset. J. Am. Med. Inform. Assoc., 24(2):361–370,\nMarch 2017.\n[9] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent\nneural networks for multivariate time series with missing values. Sci. Rep., 8(1), 2018.\n[10] Yu Cheng, Fei Wang, Ping Zhang, and Jianying Hu. Risk prediction with electronic health\nrecords: A deep learning approach. In Proceedings of the 2016 SIAM International Conference\non Data Mining, 2016.\n[11] P. Nguyen, T. Tran, N. Wickramasinghe, and S. Venkatesh. Deepr: A convolutional net for\nmedical records. IEEE Journal of Biomedical and Health Informatics, 21(1):22–30, Jan 2017.\nISSN 2168-2194. doi: 10.1109/JBHI.2016.2633963.\n5\n[12] Ben J Maraﬁno, Miran Park, Jason M Davies, Robert Thombley, Harold S Luft, David C\nSing, Dhruv S Kazi, Colette DeJong, W John Boscardin, Mitzi L Dean, and R Adams Dudley.\nValidation of prediction models for critical care outcomes using natural language processing of\nelectronic health record data. JAMA Netw Open, 1(8):e185097–e185097, December 2018.\n[13] Olof Jacobson and Hercules Dalianis. Applying deep learning on electronic health records in\nswedish to predict healthcare-associated infections. In Proceedings of the 15th Workshop on\nBiomedical Natural Language Processing, 2016.\n[14] Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M Dai, Nissan Hajaj, Michaela Hardt, Pe-\nter J Liu, Xiaobing Liu, Jake Marcus, Mimi Sun, Patrik Sundberg, Hector Yee, Kun Zhang,\nYi Zhang, Gerardo Flores, Gavin E Duggan, Jamie Irvine, Quoc Le, Kurt Litsch, Alexander\nMossin, Justin Tansuwan, De Wang, James Wexler, Jimbo Wilson, Dana Ludwig, Samuel L\nV olchenboum, Katherine Chou, Michael Pearson, Srinivasan Madabushi, Nigam H Shah, Atul J\nButte, Michael D Howell, Claire Cui, Greg S Corrado, and Jeffrey Dean. Scalable and accurate\ndeep learning with electronic health records. npj Digital Medicine, 1(1):18, May 2018.\n[15] Riccardo Miotto, Li Li, Brian A Kidd, and Joel T Dudley. Deep patient: An unsupervised\nrepresentation to predict the future of patients from the electronic health records. Sci. Rep., 6:\n26094, May 2016.\n[16] Harini Suresh, Nathan Hunt, Alistair Johnson, Leo Anthony Celi, Peter Szolovits, and Marzyeh\nGhassemi. Clinical intervention prediction and understanding with deep neural networks. In\nMachine Learning for Healthcare Conference, pages 322–337, November 2017.\n[17] Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen,\nNikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and\nJeffrey Dean. Google’s multilingual neural machine translation system: Enabling Zero-Shot\ntranslation. Transactions of the Association for Computational Linguistics, 5:339–351, 2017.\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding. arXiv e-prints , art.\narXiv:1810.04805, Oct 2018.\n[19] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. Hierarchical\nattention networks for document classiﬁcation. In Proceedings of the 2016 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, 2016.\n[20] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Neural Information\nProcessing Systems (NIPS), November 2015.\n[21] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks.\nIn Proceedings of the 34th International Conference on Machine Learning - Volume 70 ,\nICML’17, pages 3319–3328. JMLR.org, 2017. URLhttp://dl.acm.org/citation.cfm?\nid=3305890.3306024.\n[22] Alistair E W Johnson, Tom J Pollard, Lu Shen, Li-Wei H Lehman, Mengling Feng, Mohammad\nGhassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. MIMIC-III,\na freely accessible critical care database. Sci Data, 3:160035, May 2016.\n[23] T J Pollard and A E W Johnson. The MIMIC-III clinical database. http://dx.doi.org/10.\n13026/C2XW26, 2016. Accessed: 2018-12-10.\n[24] Anne Elixhauser. Clinical Classiﬁcations for Health Policy Research, Version 2: Hospital\nInpatient Statistics. 1996.\n[25] Vergil N Slee. The international classiﬁcation of diseases: Ninth revision (ICD-9). Ann. Intern.\nMed., 88(3):424, 1978.\n[26] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):\n1735–1780, December 1997.\n6\n[27] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd\nInternational Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May\n7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\n[28] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: A simple way to prevent neural networks from overﬁtting. J. Mach. Learn. Res., 15:\n1929–1958, 2014.\n[29] Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent\nneural networks. In Advances in Neural Information Processing Systems, pages 1019–1027,\n2016.\n[30] David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rose-\nmary Ke, Anirudh Goyal, Yoshua Bengio, Aaron Courville, and Chris Pal. Zoneout: Regulariz-\ning RNNs by Randomly Preserving Hidden Activations. arXiv e-prints, art. arXiv:1606.01305,\nJun 2016.\n[31] Thomas Desautels, Andreas Krause, and Joel W. Burdick. Parallelizing exploration-exploitation\ntradeoffs in gaussian process bandit optimization. Journal of Machine Learning Research, 15:\n4053–4103, 2014. URL http://jmlr.org/papers/v15/desautels14a.html.\n[32] Benjamin Shickel, Patrick James Tighe, Azra Bihorac, and Parisa Rashidi. Deep EHR: A survey\nof recent advances in deep learning techniques for electronic health record (EHR) analysis.\nIEEE Journal of Biomedical and Health Informatics, pages 1–1, 2017.\n[33] Abhyuday Jagannatha and Hong Yu. Structured prediction models for RNN based sequence\nlabeling in clinical text. In Proceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing, 2016.\n[34] Ankit Vani, Yacine Jernite, and David Sontag. Grounded Recurrent Neural Networks. arXiv\ne-prints, art. arXiv:1705.08557, May 2017.\n[35] James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, and Jacob Eisenstein. Explainable\nprediction of medical codes from clinical text. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers) , pages 1101–1111, New Orleans, Louisiana, June\n2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1100. URL https:\n//www.aclweb.org/anthology/N18-1100.\n[36] Sebastian Gehrmann, Franck Dernoncourt, Yeran Li, Eric T. Carlson, Joy T. Wu, Jonathan\nWelt, John Foote, Jr., Edward T. Moseley, David W. Grant, Patrick D. Tyler, and Leo A. Celi.\nComparing deep learning and concept extraction based methods for patient phenotyping from\nclinical narratives. PLOS ONE, 13(2):1–19, 02 2018. doi: 10.1371/journal.pone.0192360. URL\nhttps://doi.org/10.1371/journal.pone.0192360.\n[37] Thanat Chokwijitkul, Anthony Nguyen, Hamed Hassanzadeh, and Siegfried Perez. Identi-\nfying risk factors for heart disease in electronic medical records: A deep learning approach.\nProceedings of the BioNLP 2018 workshop, pages 18–27, 2018.\n[38] Shang Gao, Michael T Young, John X Qiu, Hong-Jun Yoon, James B Christian, Paul A Fearn,\nGeorgia D Tourassi, and Arvind Ramanthan. Hierarchical attention networks for information\nextraction from cancer pathology reports. J. Am. Med. Inform. Assoc., November 2017.\n[39] Tal Baumel, Jumana Nassour-Kassis, Raphael Cohen, Michael Elhadad, and No‘emie Elhadad.\nMulti-Label Classiﬁcation of Patient Notes a Case Study on ICD Code Assignment. arXiv\ne-prints, art. arXiv:1709.09587, Sep 2017.\n[40] Mary Jane C Samonte, Bobby D Gerardo, Arnel C Fajardo, and Ruji P Medina. ICD-9 tagging\nof clinical notes using topical word embedding. In Proceedings of the 2018 International\nConference on Internet and e-Business - ICIEB ’18, 2018.\n[41] Jingshu Liu, Zachariah Zhang, and Narges Razavian. Deep EHR: Chronic Disease Prediction\nUsing Medical Notes. arXiv e-prints, art. arXiv:1808.04928, Aug 2018.\n7\n[42] Denis Newman-Grifﬁs and Ayah Zirikly. Embedding transfer for Low-Resource medical named\nentity recognition: A case study on patient mobility. Proceedings of the BioNLP 2018 workshop,\npages 1–11, 2018.\n[43] Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical Multiscale Recurrent Neural\nNetworks. arXiv e-prints, art. arXiv:1609.01704, Sep 2016.\n[44] K. Hwang and W. Sung. Character-level language modeling with hierarchical recurrent neural\nnetworks. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 5720–5724, March 2017. doi: 10.1109/ICASSP.2017.7953252.\n[45] Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei Xu. Video paragraph captioning\nusing hierarchical recurrent neural networks. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2016.\n[46] Zhao Meng, Lili Mou, and Zhi Jin. Hierarchical RNN with static Sentence-Level attention\nfor Text-Based speaker change detection. In Proceedings of the 2017 ACM on Conference on\nInformation and Knowledge Management - CIKM ’17, 2017.\n[47] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V . Le.\nXLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv e-prints,\nart. arXiv:1906.08237, Jun 2019.\n[48] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,\nGreg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\nAndrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,\nManjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray,\nChris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul\nTucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden,\nMartin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-Scale\nMachine Learning on Heterogeneous Distributed Systems.arXiv e-prints, art. arXiv:1603.04467,\nMar 2016.\n[49] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,\nOlivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vander-\nplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard\nDuchesnay. Scikit-learn: Machine learning in python. J. Mach. Learn. Res., 12(Oct):2825–2830,\n2011.\n8\nSupplementary Material\nA Additional details of patient cohort\nTable 2: Descriptive statistics for patient cohort.\nTrain & validation Test\nNumber of patients 40,511 4,439\nNumber of hospital admissions* 51,081 5,598\nGender, n(%)\nFemale 22,468 (44.0) 2,548 (45.5)\nMale 28,613 (56.0) 3,050 (54.5)\nAge, median (IQR) 62 (32) 62 (33)\nHospital discharge service, n(%)\nGeneral medicine 21,350 (41.8) 2,354 (42.1)\nCardiovascular 10,965 (21.5) 1,175 (21.0)\nObstetrics 7,123 (13.9) 803 (14.3)\nCardiopulmonary 4,459 (8.7) 519 (9.3)\nNeurology 4,282 (8.4) 457 (8.2)\nCancer 2,217 (4.3) 223 (4.0)\nPsychiatric 28 (0.1) 4 (0.1)\nOther 657 (1.3) 63 (1.1)\nDischarge location, n(%)\nHome 28,991 (56.8) 3,095 (55.3)\nSkilled nursing facility 6,878 (13.5) 794 (14.2)\nRehab 5,757 (11.3) 653 (11.7)\nOther healthcare facility 3,830 (7.5) 448 (8.0)\nExpired 4,420 (8.7) 462 (8.3)\nOther 1,205 (2.4) 146 (2.6)\nPrevious hospitalizations, n(%)\nNone 40,362 (79.0) 4,415 (78.9)\nOne 6,427 (12.6) 721 (12.9)\nTwo to ﬁve 3,681 (7.2) 397 (7.1)\nSix or more 611 (1.2) 65 (1.2)\nNumber of discharge ICD-9, median (IQR)** 9 (8) 9 (8)\n* For primary CCS prediction, 1.3% of these admissions were excluded, where the primary diagnosis\ncorresponded to a non-billable ICD-9 code.\n** Includes only billable ICD-9 codes.\n9\nB Additional details of model training\nFor memory and performance reasons, in all hierarchical models we restricted the maximum amount\nof text used in the notes LSTM, keeping the most recent N tokens per record (across all notes) and\ndiscarding any additional leading tokens. We tuned the level of truncation on the validation set, and\nfound N = 1000to be sufﬁcient for training mortality models, but increased to N = 2500for both\ndiagnosis tasks and for pretraining. All models were implemented in Tensorﬂow 1.12 [ 48], and\ntrained on Nvidia Tesla P100 GPUs.\nMetrics for model selection included AUROC for mortality and ICD9, and top-5 recall for CCS. For\nmultilabel ICD9 prediction, we computed a weighted AUROC, where the AUROC for each label is\naveraged according to the label’s prevalence. Evaluation metrics and statistical tests were calculated\nusing scikit-learn 0.20 [49].\n10\nTable 3: Model hyperparameters. For the same task, all non-hierarchical models shared the BOW\nhyperparameters, and all hierarchical models shared the SHiP hyperparameters, except where noted.\nFor the SHiP models, dropout was applied during both pretraining and training. All models were\ntrained using the Adam optimizer with default constant values: β1 = 0.9,β2 = 0.999,ϵ = 1× 10−8.\nHyperparameters Mortality Primary CCS All ICD-9\nBOW SHiP BOW SHiP BOW SHiP\nTraining Learning rate 0.00015 0.00011 0.00369 0.00067 0.00369 0.00048\nBatch size 128 16 128 16 128 16\nPretraining steps – 30,000 – 30,000 – 40,000\nGradient clip norm 37.5 37.5 0.125 0.125 0.125 0.125\nVariational\nvocabulary\ndropout*\n0.001 0.229 0.273 0.396 0.273 0.273\nBag length, hours\nNotes only 1 1 1 1 1 1\nAll features 1 1 8 8 8 8\nMaximum\ntimesteps\nNotes only 1000 1000 1000 1000 1000 1000\nAll features 1000 1000 200 200 200 200\nRecord\nLSTM\nHidden units 379 518 518\nInput dropout 0.466 0.246 0.246\nHidden dropout 0.045 0.136 0.136\nVariational input\ndropout\n0.034 0.071 0.071\nVariational hidden\ndropout\n0.090 0.122 0.122\nZoneout 0.268 0.437 0.437\nNotes\nLSTM\nBidirectional? – Yes – Yes – No\nHidden units – 350 – 325 – 780\nInput dropout – 0.052 – 0.019 – 0.340\nHidden dropout – 0.175 – 0.391 – 0.238\nVariational input\ndropout\n– 0.176 – 0.291 – 0.156\nVariational hidden\ndropout\n– 0.061 – 0.085 – 0.103\nZoneout – 0.312 – 0.336 – 0.387\n* The variational vocabulary dropout rate is shared across all features. For baseline models with\nbigrams, we increased the dropout rate on the notes vocabulary only to 0.75.\n11\nC Additional experimental results\nTable 4: Comparison of different bagging lengths for all-feature hierarchical CCS and ICD9 models.\nReporting mean (standard deviation) test set results over ﬁve runs from random initialization.\nPrimary CCS All ICD-9\nTop-1\nRecall\nTop-5\nRecall\nAUPRC AUROC,\nweighted\nNo pretraining 1-hour bagging,\nt= 1000\n0.555\n(0.020)\n0.812\n(0.014)\n0.291\n(0.010)\n0.869\n(0.004)\n8-hour bagging,\nt= 200\n0.591\n(0.008)\n0.833\n(0.006)\n0.301\n(0.004)\n0.868\n(0.001)\nSHiP 1-hour bagging,\nt= 1000\n0.660\n(0.004)\n0.887\n(0.003)\n0.332\n(0.016)\n0.889\n(0.002)\n8-hour bagging,\nt= 200\n0.671\n(0.004)\n0.890\n(0.001)\n0.345\n(0.005)\n0.889\n(0.002)\nTable 5: Comparison of unidirectional vs. bidirectional notes LSTMs for SHiP models. Reporting\nvalidation set results for a single run.\nMortality Primary CCS All ICD-9\nAUPRC AUROC Top-1\nRecall\nTop-5\nRecall\nAUPRC AUROC,\nweighted\nUnidirectional 0.490 0.895 0.651 0.888 0.342 0.887\nBidirectional 0.497 0.896 0.663 0.896 0.326 0.878\nTable 6: Comparison of pretraining time thresholds for all-feature SHiP mortality models. Reporting\nmean (standard deviation) test set results over ﬁve runs from random initialization.\nAUPRC AUROC\nPretrained to 24 hours 0.478 (0.005) 0.881 (0.001)\nPretrained to discharge 0.479 (0.007) 0.882 (0.001)\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7103304862976074
    },
    {
      "name": "Terminology",
      "score": 0.6189461946487427
    },
    {
      "name": "Preprocessor",
      "score": 0.6170755624771118
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5836738348007202
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5719524621963501
    },
    {
      "name": "Natural language processing",
      "score": 0.5684815645217896
    },
    {
      "name": "Language model",
      "score": 0.5187082886695862
    },
    {
      "name": "Artificial neural network",
      "score": 0.49723556637763977
    },
    {
      "name": "Attribution",
      "score": 0.45369935035705566
    },
    {
      "name": "Hierarchical database model",
      "score": 0.4406013488769531
    },
    {
      "name": "Machine learning",
      "score": 0.4259689748287201
    },
    {
      "name": "Information retrieval",
      "score": 0.320221483707428
    },
    {
      "name": "Data mining",
      "score": 0.2753450870513916
    },
    {
      "name": "Linguistics",
      "score": 0.18362021446228027
    },
    {
      "name": "Psychology",
      "score": 0.17550188302993774
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}