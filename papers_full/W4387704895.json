{
  "title": "Employing Large Language Models for Surgical Education: An In-depth Analysis of ChatGPT-4",
  "url": "https://openalex.org/W4387704895",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3118824731",
      "name": "Adrian Hang Yue Siu",
      "affiliations": [
        "Royal Prince Alfred Hospital",
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2096768699",
      "name": "Damien Gibson",
      "affiliations": [
        "St George Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2023481138",
      "name": "Xin Mu",
      "affiliations": [
        "Peninsula Health"
      ]
    },
    {
      "id": "https://openalex.org/A2969639752",
      "name": "Ishith Seth",
      "affiliations": [
        "Peninsula Health",
        "Bendigo Health"
      ]
    },
    {
      "id": "https://openalex.org/A5102568189",
      "name": "Alexander Chi Wang Siu",
      "affiliations": [
        "Royal Prince Alfred Hospital",
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A349823751",
      "name": "Dilshad Dooreemeah",
      "affiliations": [
        "Bendigo Health"
      ]
    },
    {
      "id": "https://openalex.org/A2982280510",
      "name": "Angus Lee",
      "affiliations": [
        "Bendigo Health"
      ]
    },
    {
      "id": "https://openalex.org/A3118824731",
      "name": "Adrian Hang Yue Siu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096768699",
      "name": "Damien Gibson",
      "affiliations": [
        "St George Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2023481138",
      "name": "Xin Mu",
      "affiliations": [
        "Peninsula Health"
      ]
    },
    {
      "id": "https://openalex.org/A2969639752",
      "name": "Ishith Seth",
      "affiliations": [
        "Bendigo Health",
        "Peninsula Health"
      ]
    },
    {
      "id": "https://openalex.org/A5102568189",
      "name": "Alexander Chi Wang Siu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A349823751",
      "name": "Dilshad Dooreemeah",
      "affiliations": [
        "Bendigo Health"
      ]
    },
    {
      "id": "https://openalex.org/A2982280510",
      "name": "Angus Lee",
      "affiliations": [
        "Bendigo Health"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4367051110",
    "https://openalex.org/W4380319827",
    "https://openalex.org/W4378471319",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4324130227",
    "https://openalex.org/W4362641141",
    "https://openalex.org/W4385827730",
    "https://openalex.org/W4386596026",
    "https://openalex.org/W4382765948",
    "https://openalex.org/W635486979",
    "https://openalex.org/W2123109647",
    "https://openalex.org/W2043565216",
    "https://openalex.org/W2048587526",
    "https://openalex.org/W2014981322",
    "https://openalex.org/W2337345116",
    "https://openalex.org/W2621009456",
    "https://openalex.org/W3028974348",
    "https://openalex.org/W4379378720",
    "https://openalex.org/W2047082476",
    "https://openalex.org/W2587592251",
    "https://openalex.org/W2102876475",
    "https://openalex.org/W2997407656",
    "https://openalex.org/W4233283344",
    "https://openalex.org/W2886860777",
    "https://openalex.org/W3096409088",
    "https://openalex.org/W3156298796",
    "https://openalex.org/W2916492225",
    "https://openalex.org/W4375955709",
    "https://openalex.org/W4366823098",
    "https://openalex.org/W4361204578",
    "https://openalex.org/W3039746272",
    "https://openalex.org/W2951405082",
    "https://openalex.org/W1588786163",
    "https://openalex.org/W2056330418"
  ],
  "abstract": "Background: The growing interest in artificial intelligence (AI) has spurred an increase in the availability of Large Language Models (LLMs) in surgical education. These LLMs hold the potential to augment medical curricula for future healthcare professionals, facilitating engagement in remote learning experiences, and assisting in personalised student feedback. Objectives: To evaluate the ability of LLMs to assist junior doctors in providing advice for common ward-based surgical scenarios with increasing complexity. Methods: Utilising an instrumental case study approach, this study explored the potential of LLMs by comparing the responses of the ChatGPT-4, BingAI and BARD. LLMs were prompted by 3 common ward-based surgical scenarios and tasked with assisting junior doctors in clinical decision-making. The outputs were assessed by a panel of two senior surgeons with extensive experience in AI and education, qualitatively utilising a Likert scale on their accuracy, safety, and effectiveness to determine their viability as a synergistic tool in surgical education. A quantitative assessment of their reliability and readability was conducted using the DISCERN score and a set of reading scores, including the Flesch Reading Ease Score, Flesch-Kincaid Grade Level, and Coleman-Liau index. Results: BARD proved superior in readability, with Flesch Reading Ease Score 50.13 (± 5.00), Flesch-Kincaid Grade Level 9.33 (± 0.76), and Coleman-Liau index 11.67 (± 0.58). ChatGPT-4 outperformed BARD and BingAI, with the highest DISCERN score of 71.7 (± 2.52). Using a Likert scale-based framework, the surgical expert panel further affirmed that the advice provided by the ChatGPT-4 was suitable and safe for first-year interns and residents. A t-test showed statistical significance in reliability among all three AIs (P &lt; 0.05) and readability only between the ChatGPT-4 and BARD. This study underscores the potential for LLM integration in surgical education, particularly ChatGPT, in the provision of reliable and accurate information. Conclusions: This study highlighted the potential of LLM, specifically ChatGPT-4, as a valuable educational resource for junior doctors. The findings are limited by the potential of non-generalizability of the use of junior doctors' simulated scenarios. Future work should aim to optimise learning experiences and better support surgical trainees. Particular attention should be paid to addressing the longitudinal impact of LLMs, refining AI models, validating AI content, and exploring technological amalgamations for improved outcomes.",
  "full_text": "J Med Edu. 2023 December; 22(1):e137753.\nPublished online 2023 October 17.\nhttps://doi.org/10.5812/jme-137753.\nResearch Article\nEmploying Large Language Models for Surgical Education: An In-depth\nAnalysis of ChatGPT-4\nAdrian Hang Yue Siu\n 1, 2, *, Damien Gibson 3, Xin Mu 4, Ishith Seth 4, 5, Alexander Chi Wang Siu6,\nDilshad Dooreemeah 5 and Angus Lee 5\n1 Surgical Outcomes Research Centre, Royal Prince Alfred Hospital, Sydney , Australia\n2 Faculty of Medicine and Health, Central Clinical School, The University of Sydney , Sydney , New South Wales, Australia\n3 Department of Surgery , St George Hospital, Sydney , Australia\n4 Department of Surgery , Peninsula Health, Victoria, Australia\n5 Department of Surgery , Bendigo Health, Victoria, Australia\n6 School of Medical Sciences, University of Sydney , Sydney , Australia\n* Corresponding author: Research Aﬃliate, Surgical Outcomes Research Centre, Royal Prince Alfred Hospital, Sydney , Australia. Email: adriansiu7@hotmail.com\nReceived 2023 May 22; Revised 2023 September 13; Accepted 2023 September 17.\nAbstract\nBackground: The growing interest in artiﬁcial intelligence (AI) has spurred an increase in the availability of Large Language Models\n(LLMs) in surgical education. These LLMs hold the potential to augment medical curricula for future healthcare professionals,\nfacilitating engagement in remote learning experiences, and assisting in personalised student feedback.\nObjectives: To evaluate the ability of LLMs to assist junior doctors in providing advice for common ward-based surgical scenarios\nwith increasing complexity .\nMethods: Utilising an instrumental case study approach, this study explored the potential of LLMs by comparing the responses of\nthe ChatGPT-4, BingAI and BARD. LLMs were prompted by 3 common ward-based surgical scenarios and tasked with assisting junior\ndoctors in clinical decision-making. The outputs were assessed by a panel of two senior surgeons with extensive experience in AI and\neducation, qualitatively utilising a Likert scale on their accuracy , safety , and eﬀectiveness to determine their viability as a synergistic\ntool in surgical education. A quantitative assessment of their reliability and readability was conducted using the DISCERN score and\na set of reading scores, including the Flesch Reading Ease Score, Flesch-Kincaid Grade Level, and Coleman-Liau index.\nResults: BARD proved superior in readability , with Flesch Reading Ease Score 50.13 (± 5.00), Flesch-Kincaid Grade Level 9.33 (± 0.76),\nand Coleman-Liau index 11.67 ( ± 0.58). ChatGPT-4 outperformed BARD and BingAI, with the highest DISCERN score of 71.7 ( ± 2.52).\nUsing a Likert scale-based framework, the surgical expert panel further aﬃrmed that the advice provided by the ChatGPT-4 was\nsuitable and safe for ﬁrst-year interns and residents. A t-test showed statistical signiﬁcance in reliability among all three AIs (P <\n0.05) and readability only between the ChatGPT-4 and BARD. This study underscores the potential for LLM integration in surgical\neducation, particularly ChatGPT, in the provision of reliable and accurate information.\nConclusions: This study highlighted the potential of LLM, speciﬁcally ChatGPT-4, as a valuable educational resource for junior\ndoctors. The ﬁndings are limited by the potential of non-generalizability of the use of junior doctors’ simulated scenarios. Future\nwork should aim to optimise learning experiences and better support surgical trainees. Particular attention should be paid to\naddressing the longitudinal impact of LLMs, reﬁning AI models, validating AI content, and exploring technological amalgamations\nfor improved outcomes.\nKeywords: Surgical Education, Medical Education, Large Language Models, Artiﬁcial Intelligence, COVID-19 Pandemic, Decision\nAids\n1. Background\nChat Generative Pre-Trained Transformer 4 (ChatGPT-4)\n(Open AI), BARD (Google), and BingAI (Microsoft) are\nstate-of-the-art large language models (LLM), that generate\nhuman-like language to answer questions and complete\ntext (1). Currently , there is a growing prevalence of\nchatbots and artiﬁcial intelligence (AI) in daily life, from\nassisting with school homework to fooling researchers\nwith phony abstracts (2) While discussions surrounding\nownership, authorship, and potential misuse continue\nto be debated (3, 4), there has been a growing trend of\nartiﬁcial intelligence in medical education as a disruptive\nCopyright © 2023, Siu et al. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License (CC BY 4.0)\n(https://creativecommons.org/licenses/by/4.0/) which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nSiu AHY et al.\ntechnology (5-7).\nTrained by a vast corpus of medical literature,\nthese LLMs can provide students with detailed and\nrelevant information on any chosen subject matter\n(8). ChatGPT-3.5 has demonstrated performance near\nthe passing threshold of 60% accuracy in the USMLE\nSteps 1, 2 CK, and 3 exams, comparable to a ﬁrst-year\npostgraduate doctor seeking licensure as an unsupervised\nphysician in the United States of America (USA) (5).\nFollowing the March 2023 release of ChatGPT-4, it has\nbeen claimed that the updated version of this LLM\nhas enhanced clinical reasoning and test-answering\ncapabilities compared to previous iterations (9, 10).\nThis has been further demonstrated with ChatGPT-4\nsigniﬁcantly outperforming ChatGPT-3.5 on American\nneurosurgical written board examinations (8).\nConcurrently , the COVID-19 pandemic has\nfundamentally reshaped medical education owing to\npublic health concerns and stay-at-home mandates in\nAustralia, leading to reduced face-to-face teaching and\nclinical exposure for medical students and junior doctors.\nThis has triggered concerns regarding the potential\nlong-term eﬀects of medical training. However, the\npandemic has also spurred innovations in medical\neducation, particularly in virtual simulation and\ntelehealth (5, 10, 11). While most universities resume\nface-to-face training, the growing prevalence of AI and\nLLMs cannot be ignored, as they have emerged as possible\ntools to aid informed diagnosis and make safer treatment\nplans. The unique ability of LLMs to process vast amounts\nof clinical data and current information positions them\nas theoretical adjuncts for medical students and junior\ndoctors. However, the challenge therefore, is ensuring that\ntheir everyday use in medical education is not at the cost of\ncritical thinking and clinical acumen. Additionally , given\nthe prevalence of LLMs in the post-pandemic context, it\nmust also be considered whether they have a role in virtual\nsimulation and remote learning.\nThis study aimed to assess the potential of LLMs,\nwith a focus on ChatGPT-4, BingAI, and BARD, to aid\nsurgical education and oﬀer reliable advice to junior\ndoctors (post-graduate years one or two). Comparison\nof these LLMs will be conducted through comprehensive\nquantitative and qualitative assessment, which will\nprovide valuable insights into the potential use of LLMs\nin surgical education. In particular, the limitations of\nAI and LLMs are also brieﬂy explored to understand the\nboundaries of this technology . Ultimately , by exploring\nthese issues in-depth, this study will help reshape the\ntraditional medical education curriculum.\nUsing a case study approach, each LLM will be\nprompted in three routine ward-based surgical situations\nto aid clinical decision-making. These scenarios were\nformulated based on real-life clinical scenarios and\ntextbook case studies (11, 12), with a ﬁnal review by expert\ngeneral surgeons. Responses were qualitatively evaluated\nfor accuracy , safety , and eﬀectiveness using a Likert\nscale (13). Their validity , reliability , and readability were\nquantitatively assessed using the DISCERN score (14) and\nvarious readability metrics, including the Flesch Reading\nEase (FRE) Score (15), Flesch-Kincaid Grade Level (FKGL)\n(16), and Coleman-Liau index (CLI) (17). It is hypothesised\nthat their outputs may exhibit disparities in reliability and\nreadability owing to diﬀering training data.\n2. Methods\n2.1. Study Design\nTo address the primary research aim, this study\nadopted an instrumental case study approach (18). This\nresearch method is often used to understand and gain\ninsight into a phenomenon in a context, which in our case,\nis the use of AI LLMs (ChatGPT-4, BARD, and BingAI) in\nmedical education.\n2.2. Methodology\nA series of three increasingly complex clinical\nscenarios were posed to AI LLMs (ChatGPT-4, BARD, and\nBingAI). These scenarios were common ward-based\nsurgical reviews performed by a junior doctor. These\nscenarios were formulated and designed from real-life\nclinical scenarios that the authors had encountered as\nward-based junior doctors. The scenarios were then\nvalidated for accuracy and relevance with textbook\nanalysis of case studies (11, 12). These case studies were\nevaluated by a panel of two board-certiﬁed surgeons\nindependently (AL and DD) with over 20 years of\nexperience. Responses were qualitatively evaluated\nfor accuracy , safety , and eﬀectiveness using a Likert scale\n(13). If any diﬀerences in the Likert scale or reliability tools\narose, these were discussed until consensus was achieved.\nThe responses from each scenario then underwent\nqualitative analysis for accuracy , appropriateness, and\npatient safety . The quantitative assessment standards\ncomprised of two aspects: Reliability , which was\ndetermined using the DISCERN score (14), and readability ,\nwhich was evaluated through three widely recognized\nscoring systems: FRE score (15), FKGL (16), and CLI (17).\nDue to diﬀering training data, it is hypothesised that\ntheir outputs may exhibit disparities in reliability and\nreadability .\n2 J Med Edu. 2023; 22(1):e137753.\nSiu AHY et al.\n2.3. Data Collection Tools\nThe Likert scale (13) used in this study , is a 5-point\nglobal scale to qualitatively evaluate the accuracy , safety\nand eﬀectiveness of the three LLMs. The 5-point scale\nconsisted of two utmost poles (‘Strong Agree’ and\n‘Strongly disagree’) and neutral option (‘Neither agree\nnor disagree’), linked with intermediate answer options\n(‘Agree’ and ‘Disagree’).\nThe DISCERN questionnaire (14) was used to\nquantitatively assess the reliability of written information\nfrom the LLM outputs, and is considered a valid and\nreliable score for evaluating consumer health information\n(19). According to the literature (20), DISCERN scores may\nbe categorised as follows: ‘excellent’ for scores of 63 to 75\npoints, ‘good’ for scores of 51 to 62 points, ‘fair’ for scores\nof 39 to 50 points, ‘poor’ for scores of 27 to 38 points, and\n‘very poor’ for scores of 16 to 26 points.\nIn this study , readability was assessed using three\nrecognised scoring systems: Flesch Reading Ease Score\n(15), Flesch-Kincaid Grade Level (16), and Coleman-Liau\nindex (17). The FRE score and FKGL are both calculated\nusing the average sentence length (i.e., number of words\ndivided by the number of sentences) and the average\nsyllables per word (i.e., number of syllables divided by the\nnumber of words) (21). The CLI (17) is calculated using the\naverage number of letters per 100 words, and the average\nsentence length. Scoring of the FRE is through a 100-point\nscale with higher scores indicating higher readability and\neasier to understand text. Alternatively , FKGL and CLI,\nindicate the USA academic grade level (number of years of\neducation) necessary to comprehend the written material.\nThese readability tests were selected due to their wide and\nvalidated use in previous studies (21, 22).\n2.4. Expert Review Framework\nTo evaluate the AI outputs, a Likert scale-based\nframework was employed (Table 1). Each LLM output\nwas assessed by a panel of expert General Surgeons\nusing this framework. The panel of board-certiﬁed\nsurgeons conceptualised the research idea and both\nexpert surgeons were recruited due to their experience\nin AI, research, and education. The surgeons’ credentials\nextended beyond medical degrees to include specialised\nsurgical training, aﬃliations with professional medical\nbodies, and leading roles at esteemed medical institutions.\nTheir proﬁciency in evaluating AI outputs, understanding\nof its implications in medical education, and previous\nexperience with AI research projects substantiated their\nselection for the panel. Experts were asked to rate the\naccuracy , reliability , proﬁciency , comprehensiveness,\nrelevancy , general knowledge, errors, citations, and\nreferences of AI-generated responses.\n2.5. Statistical Analysis\nBetween the three LLM, a student’s t-test (23) was\nconducted to determine the statistical signiﬁcance of the\nreliability and readability scores. Further commentary and\ncritique of the answers were provided by two specialist\ngeneral surgeons with extensive clinical experience, who\nprovided an expert review framework on the subject\nmatter. Statistical analyses were then performed on the\ncollected data to determine the AI’s performance across\ndiﬀerent dimensions, with a focus on identifying areas of\nstrength and potential improvement.\n2.6. Selection of LLMs\nDue to the probabilistic algorithm and\nrandom-sampling method of LLMs, answers may vary\nslightly even if the same question is asked. For this study ,\nthree scenarios were placed into ChatGPT-4, BARD, and\nBingAI, and the ﬁrst responses from each prompt were\nrecorded. These three LLMs were selected as the three\nmost readily available and widely used LLMs in medical\nresearch (24). Extreme care was taken to craft each prompt,\nto ensure that there were no grammatical errors or points\nof contention. Subsequent clariﬁcation of answers or\ncorrections was also not utilised. To preserve the integrity\nof the original response and mirror the conditions of a\njunior doctor, the function to regenerate answers or to\nalter previous responses was not utilised.\nAll prompts were inputted on the same day on a single\naccount of ChatGPT plus (owned by one of the authors,\nIS), which provided access to ChatGPT-4. Access to BARD\nand BingAI required no additional paywall. No inclusion\nor exclusion criteria were placed on the answers from\nChatGPT-4, BARD and BingAI. Institutional ethics were not\nrequired as no human participants were involved in the\nstudy .\n3. Results\n3.1. Quantitative Analysis\nThrough a comprehensive quantitative analysis of\nChatGPT, BARD, and BingAI as shown in Table 2, signiﬁcant\nvariability was observed in the mean readability , as\nassessed by three standard grading scales. BARD’s\nresponses displayed the greatest readability , scoring\n50.13 (± 5.00) in the FRE, 9.33 (± 0.76) in the FKGL, and 11.67\n(± 0.58) in the CLI.\nRegarding the accuracy of the information, ChatGPT\nsurpassed others by providing medical advice closely\naligned with current clinical guidelines and up-to-date\nreferences. It is evidenced in the highest DISCERN score of\n71.7 (± 2.52) compared to BingAI’s 64.3 (± 2.08) and BARD’s\nJ Med Edu. 2023; 22(1):e137753. 3\nSiu AHY et al.\nTable 1. Evaluation of Large Language Model Platforms’ Responses\nCriteria ChatGPT Bing’s AI Google’s BARD\nThe large language model provides\naccurate answers to questions.;\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [x] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [x] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[x] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\nThe large language model is\nreliable when generating factual\ninformation.\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [x] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[x] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\n[x] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\nThe large language model is\nproﬁcient at understanding\ncomplex questions and providing\nappropriate answers.\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [x] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[x] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\n[x] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\nThe large language model provides\ncomprehensive information when\nanswering questions.\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [x] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[x] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [x] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\nThe large language model\ngenerates content that covers all\nrelevant aspects of a subject.\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [x] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[x] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\n[x] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\nThe large language model is able to\nprovide in-depth information for a\nwide range of topics.\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [x] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [x] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\n[x] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\nThe large language model is a\nvaluable source of general\nknowledge.\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [x] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [x] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [x] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\nThe large language model is\nwell-versed in a variety of subjects.\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [x] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [x] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [x] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\nThe large language model can\nprovide useful insights and\nperspectives on various topics.\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [x] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [x] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [x] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\nThe large language model rarely\nmakes errors when referencing\nsources.\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[x] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [x] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [x] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\nThe large language model is\nconsistent in providing accurate\ncitations.\n[ ] 1 – Strongly Disagree; [ ] 2 – Disagree;\n[x] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [x] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\n[ ] 1 – Strongly Disagree; [x] 2 – Disagree;\n[ ] 3 – Neither Agree or Disagree; [ ] 4 –\nAgree; [ ] 5 – Strongly Agree\nTable 2. Quantitative Analysis of ChatGPT, BARD, and BingAI\nReadability Reliability\nFRE FKGL CLI DISCERN\nChatGPT 28.20 ± 1.59 13.80 ± 2.18 14.37 ± 1.10 71.70 ± 2.52\nBARD 50.13 ± 5.00 9.33 ± 0.76 11.67 ± 0.58 56.70 ± 2.52\nBingAI 22.80 ± 15.16 18.33 ± 4.58 12.67 ± 1.53 64.30 ± 2.08\nAbbreviations: FRE, Flesch Reading Ease Score; FKGL, Flesch-Kincaid Grade Level; CLI, Coleman-Liau index; DISCERN, DISCERN questionnaire.\n56.7 (± 2.52). A t-test comparing all three AIs demonstrated\nstatistical signiﬁcance in the reliability tests among them,\nwith a P-value < 0.05. Among the readability tests, only the\ncomparison between ChatGPT and BARD was statistically\nsigniﬁcant (<0.05).\n3.2. Qualitative Analysis\nScenario A illustrates a patient who is two-days\npost-haemorrhoidectomy that begins to deteriorate\non the ward (Appendix 1 in the Supplementary File). The\nguidance that is oﬀered by ChatGPT-4 is to start with a\nfocused history , physical examination, then to review\ninvestigations. This dynamic and formulaic process of\nhistory , examination, and investigation forms the crux of\npatient assessment and is a distinguishing characteristic\nof a master clinician (25). From the onset, there is\nalso general advice to escalate to either a supervising\nphysician or surgeon, which demonstrates an awareness\n4 J Med Edu. 2023; 22(1):e137753.\nSiu AHY et al.\nof limitations and a high level of patient safety . While\nthe scenario is deliberately broad, the complaints of\nsuprapubic pain could have been triggered by a urinary\ntract infection. The answer may have been improved by\nfurther investigation with urine microscopy , culture, and\nsensitivity .\nIn our scenario, the patient becomes acutely unwell\nafter spiking a temperature and experiencing mild\ntachycardia. The recommendation from ChatGPT-4 is to\ninvolve a supervising physician or specialist, which is\nconsidered safe and appropriate. While the patient has\na fever and warrants further investigations, the answer\ncould have been improved by mentioning the importance\nof conducting a basic septic screen (chest x-ray , urine\nculture, and wound/blood cultures). As the patient\nbegins to deteriorate and becomes haemodynamically\nunstable, the situation becomes highly concerning for\nnecrotising fasciitis. While a formal diagnosis is not\nmentioned, the recommendations for investigations,\nfurther imaging, empiric antibiotic therapy , and ﬂuid\nresuscitation are all appropriate. Additionally , due to the\nrapidly progressive nature of necrotising fasciitis (26),\nthere is also a recommendation for close monitoring\nand a low threshold for escalation to intensive care.\nOnce a clinical diagnosis is established, urgent surgical\ndebridement, antibiotic therapy , and ﬂuid resuscitation\nbecome the cornerstone of management (27), which are\nall recommended by ChatGPT-4. The recommendations\nfrom this scenario showcase a safe and practical approach\nto managing a deteriorating surgical patient on the ward.\nScenario B describes a post-operative patient who is\ntachycardic and hypotensive on the ward (Appendix 2 in\nthe Supplementary File). Again, the recommendation of\nfurther history , physical examination, and investigations\nare used to assess this patient. Early consultation with\na supervising physician or surgeon and appropriate\ntreatment with intravenous ﬂuids is also recommended,\ndemonstrating a high level of patient safety . As the\npatient continues to deteriorate, important diﬀerentials\nof bowel obstruction or post-operative ileus are proposed\nfor consideration, which normally occur 24 - 48 hours\npost-operatively (28). As the scenario progresses with no\nrecordable drain outputs and persistent pain, ChatGPT-4’s\nresponse is to reassess the patient, re-evaluate analgesia,\nand consult with a senior. While the response is adequate\nand safe, the answer may have been improved by further\nelucidating options for post-operative analgesia – given\nits persistent nature (29). Additionally , while there is low\nevidence for the use of abdominal drainage after open\nappendectomy (30), the lack of drain outputs may also be\nmisleading, and focus should have been directed toward\nascertaining the accuracy of drain measurements.\nThe progression of the scenario ﬁnds a twisted knot\nin the drain, which likely would have prevented any\nknowledge of a leak, bleed, or collection. Once a cause is\nestablished, the advice to involve the surgical team, ﬂuid\nresuscitation, and blood transfusion are safe principles of\nmanagement. In ChatGPT-4’s response, there is also the\nsuggestion that this patient may require further surgical\nre-exploration or haemostasis if ongoing bleeding. This\nanswer therefore may have been improved by ensuring\nthe patient had a valid group and save and was alerted\nabout the possibility of further surgery . Nevertheless, in\nall answers from ChatGPT-4, safe management principles\nwere complemented with a suggestion to involve a senior\nmedical colleague, highlighting its safe-guarding and\npractical approach to managing a post-operative patient.\nScenario C describes a challenging patient who has\nrecurrent small bowel obstructions in the emergency\ndepartment (Appendix 3 in the Supplementary File).\nThe initial approach to diagnosis and management\nof the patient’s condition is congruent with current\nguidelines (31) – including bowel decompression with\nnasogastric tube insertion, pain management, and\nkeeping the patient fasted. It is noted that ChatGPT-4 also\nadvises involving the surgical team early in this scenario,\ndemonstrating a strong predisposition towards patient\nsafety . As the patient’s electrolytes become deranged,\nfurther management of re-checking laboratory values and\nelectrolyte replacement are correctly suggested (32).\nThe scenario progresses with the patient becoming\nangry , removing his nasogastric tube, and threatening to\ndischarge him against medical advice. During this ethical\ndilemma, there is a careful balance between respecting\npatient wishes and ensuring the best medical practice.\nThe response by ChatGPT-4 highlights the importance of\nstaying calm, educating the patient, oﬀering alternatives,\ncareful documentation, and safety-netting with a\ndischarge plan – highlighting its awareness of patient\nsafety and medico-legal risk. The concept of capacity\nis also proposed, where junior doctors must ascertain\nwhether a patient fully comprehends the implications\nof their actions. While the response from ChatGPT-4 was\nsafe and explored issues around capacity , this response\nmay have been improved by listing complications of small\nbowel obstruction – pain, bowel necrosis and perforation,\nintra-abdominal abscess, and aspiration (33, 34).\nThe expert review framework revealed distinct\ndiﬀerences in the performance of the three LLMs.\nChatGPT-4 outperformed Bing’s AI and Google’s BARD\nin providing accurate answers, generating factual\ninformation, understanding complex questions,\nand oﬀering comprehensive, relevant, and in-depth\ninformation across various topics. ChatGPT-4 also excelled\nJ Med Edu. 2023; 22(1):e137753. 5\nSiu AHY et al.\nin general knowledge and providing useful insights on\ndiﬀerent subjects. Divergences in the readability and\ncomprehensibility of LLMs are also noted in previous\nliterature (24). It is potentially attributable to varying\ntraining data, data pre-processing strategies, and inherent\ndata structures. Such variations could impact each model’s\nproﬁciency in managing unique terminologies and\nabbreviations. However, all three models demonstrated\nroom for improvement in referencing sources and\nproviding accurate citations, with none of them scoring\nabove \"Neither Agree or Disagree\" in those categories.\n4. Discussion\nThis study evaluated and compared the performance\nof the three most popular LLMs – ChatGPT-4, BingAI, and\nBARD in providing precise and reliable advice for junior\ndoctors in diﬀerent post-operative scenarios. Overall,\nChatGPT-4 demonstrated a strong foundation in clinical\nassessment by recommending a structured approach\nconsisting of a focused history , and physical examination,\nfollowed by investigations. It consistently recommends\njunior doctors escalate or involve senior surgeons at an\nearly stage, showcasing an appropriate level of patient\nsafety and awareness of junior doctors’ limitations in\nhandling surgical emergencies. ChatGPT-4 was able\nto generate appropriate diﬀerential diagnoses in all 3\nscenarios, indicating a comprehensive understanding of\nthe clinical context. It also provided safe and practical\nguidance on patient management in line with established\nclinical guidelines while considering the ethical and\nmedico-legal aspects of patient care, including respecting\npatient autonomy and addressing capacity . However,\nsome ChatGPT-4 responses lacked speciﬁcity and\ncomprehensiveness, as shown in scenario B where its\nanswer can be improved by ensuring the patient had\na valid group and save for potential re-exploration,\nreﬂecting its weakness in anticipating complications.\nIn contrast, the responses generated by BingAI\nand BARD are notably less speciﬁc in oﬀering distinct\nrecommendations for pathology and imaging tests.\nNevertheless, it is important to acknowledge that BARD\nis the only model to make a preliminary diagnosis\nof necrotising fasciitis based on haemodynamic\ninstability and provide relevant risk factors to aid\njunior doctors in formulating diﬀerential diagnoses.\nThis capability may indicate BARD’s potential as a\nvaluable complementary diagnostic aid (10). Despite\nthese strengths, the performance of BingAI and\nBARD in providing management advice for handling\npostoperative emergencies is markedly inferior to that\nof ChatGPT-4. This deﬁciency is not only attributable to\ntheir responses’ lack of structure and comprehensiveness\nbut also to the occasional dissemination of misleading\ninformation. For instance, BARD entirely overlooked the\ninclusion of nasogastric tube (NGT) decompression\nin the context of bowel obstruction, while BingAI\nneglected to mention ﬂuid resuscitation in instances\nof hemodynamic instability . Both oversights could lead\nto delayed treatment and potentially life-threatening\nconsequences for patients. Nonetheless, both BingAI\nand BARD consistently emphasised the importance of\nescalating care throughout their responses. This emphasis\nis integral to ensuring safe practice for junior doctors in\nthe clinical setting.\nThe COVID-19 pandemic has substantially disrupted\nsurgical education, posing unique challenges for junior\ndoctors and medical students. The suspension of clinical\nrotations and postponement of elective surgeries has\nrestricted trainees’ clinical exposure. While online\nlearning oﬀers ﬂexibility and convenience, this shift\nhas limited hands-on learning opportunities, especially\nconcerning procedural and physical examination skills.\nWhile most universities have resumed face-to-face\ntraining, it is also imperative for surgical training\nprograms to consider adapting their curricula for\ninnovative educational strategies, including hybrid\nlearning model and AI-assisted technologies (35).\nHowever, the real question posed to medical educators,\nis how to incorporate this new educational tool without\ncompromising clinical acumen and patient safety .\nLLMs particularly have considerable potential to help\naddress the challenges in surgical education due to its\ninteractive nature and rapid information retrieval capacity\n(36, 37). While they cannot entirely replace in-person\ntraining or clinical exposure, they can be instrumental in\nsupplementing and enhancing the educational experience\nfor junior surgeons. Integrating ChatGPT-4 into clinical\nsettings could oﬀer several beneﬁts. Firstly , it can provide\nreal-time assistance to trainees in understanding complex\nclinical scenarios, interpreting medical data, and making\ninformed decisions. This on-demand support may reduce\nthe cognitive burden on junior doctors and help them\nreﬁne their clinical reasoning skills. ChatGPT-4 can\npotentially ﬁll the gaps left by traditional teaching\nmethods, which might be constrained by time, resources,\nand the availability of experienced faculty . If used as an\nindependent bedside tutor, it can deliver personalised,\ntailored learning experiences that address the speciﬁc\nneeds and knowledge gaps of each trainee. Providing\nadvice on relevant and important questions during\nhistory taking and critical components to examination for\neach presenting complaint. This can facilitate self-directed\nclinical training and accelerate learning, improving the\n6 J Med Edu. 2023; 22(1):e137753.\nSiu AHY et al.\noverall quality of medical education. AI-generated clinical\nsimulations, although not yet developed, hold great\npotential as a research area. They could be tailored\nto simulate various patient scenarios and conditions,\noﬀering a safe and controlled learning environment for\nstudents to hone their skills.\nAlthough utilising LLMs (especially ChatGPT-4) in\nassisting surgical education is promising, several ethical\nconcerns and potential challenges must be addressed\nto ensure the responsible and eﬀective integration of\nsuch models into medical training. LLMs can generate\ninformation based on patterns in the data they have been\ntrained on, which might not be accurate or up to date as\ndemonstrated in some responses from BARD and BingAI.\nRelying on potentially incorrect information in medical\neducation can have serious consequences for patient\ncare. Establishing responsibility for the consequences of\nAI-generated advice is unclear. When a trainee follows a\nLLM’s guidance resulting in negative patient outcomes,\ndetermining accountability and liability may also prove\nchallenging. Therefore, eﬀective and ethical use of\nLLMs in medical education requires adherence to key\nguidelines. Trainees should view LLMs as advisory tools,\nverifying their output against other trusted resources\nsince these AI models lack real-world clinical judgement.\nData privacy and conﬁdentiality must be prioritised,\ngiven that interactions with public LLMs may be stored\nfor future model training, making it essential to avoid\nsharing personally identiﬁable or conﬁdential patient\ninformation. In addition, there is a risk that surgical\ntrainees may become overly reliant on AI-generated advice,\npotentially undermining their critical thinking and\ndecision-making abilities (38). Striking a balance between\nusing LLMs as a supplementary tool and developing\nindependent clinical judgment is essential. While AI can\nprovide valuable input, it cannot clinically assess the\npatient, take a detailed history , or complete an eﬀective\nexamination, all fundamental skills integral to medical\ntraining. Thus, supervision by medical professionals is\nessential, particularly in the early stages, to ensure the\naccurate interpretation and application of LLM-generated\nadvice, and that surgical trainees continue to develop\nclinical and communication skills, as well as empathy for\ntheir patients.\n4.1. Limitations and Future Directions\nThe primary limitation of this study lies in the fact that\nthe inquiries posed to LLMs are derived from simulated\nscenarios constructed by a limited group of junior surgical\ndoctors. Consequently , this approach may result in\nﬁndings that are less generalisable and applicable to a\nbroader context. Nonetheless, the study oﬀers insights\ninto the potential integration of LLMs within surgical\neducation, thereby contributing to the ongoing discourse\non artiﬁcial intelligence in medical training. Large-scale\nlongitudinal studies should be conducted to continuously\nassess the impact of this innovative teaching approach\non surgical trainees’ knowledge, skill development,\nand overall educational outcomes, with a focus on\ncomparisons with traditional educational methods to\nidentify areas of improvement or potential drawbacks.\nFuture research on expanding the AI model’s training\ndata is also worthwhile to reﬁne the accuracy of their\nresponses. This may be achieved by including more\nhigh-quality and up-to-date resources, such as surgical\ntextbooks, guidelines, and research articles speciﬁcally\ncovering a wide variety of clinical scenarios, thus\nproviding more accurate and contextually relevant\nrecommendations (39). Collaboration with medical\nprofessionals and educators should be encouraged to\nvalidate, review , and curate AI-generated content aligning\nwith expert consensus and best medical practices.\nOther strategies such as the integration of LLMs with\nexisting technologies including virtual reality and\nsurgical simulators, in the future, may further enhance\nthe learning experience. Additionally , amalgamating\nLLMs into virtual and robotic trainers may also provide\nmore comprehensive and context-aware guidance to the\nsurgical trainee, leading to improved delivery of feedback,\nand ultimately improving patient outcomes (40).\n4.2. Conclusions\nThis study illustrates the potential of using AI\ntechnologies to aid junior doctors by providing accurate\nand pertinent guidance in common ward-based surgical\nscenarios. The ﬁndings suggest LLMs, particularly\nChatGPT-4, hold promise as valuable educational resources\nin medical training in certain scenarios. However,\nwhile these results are promising, ethical concerns\nand challenges limit the routine use of LLM in medical\neducation. Further investigations are warranted to\nexamine the applicability of LLM in diverse medical\nspecialties, as well as its impact on patient outcomes\nand building clinical acumen in junior doctors. By\ncomprehending the advantages and constraints of AI\nlanguage models in medical education, we may devise\ninnovative approaches to instructing future generations\nof healthcare professionals.\nSupplementary Material\nSupplementary material(s) is available here [To read\nsupplementary materials, please refer to the journal\nwebsite and open PDF/HTML].\nJ Med Edu. 2023; 22(1):e137753. 7\nSiu AHY et al.\nAcknowledgments\nWe would like to acknowledge the research staﬀ of\nDr A.L.’s research group and Surgical Outcomes Research\nCentre (SOuRCe).\nFootnotes\nAuthors’ Contribution: AHYS: Study concept and design,\nacquisition/analysis/interpretation of data, drafted the\nmanuscript, critically revised the manuscript. DG: Student\nconcept and design, acquisition/analysis/interpretation\nof data, drafted the manuscript, critically revised the\nmanuscript. XM: Drafted the manuscript, critically revised\nthe manuscript. IS: Drafted the manuscript, critically\nrevised the manuscript. ACWS: Drafted the manuscript,\ncritically revised the manuscript. DD: Study supervision,\nanalysis of data, drafted the manuscript, critically revised\nthe manuscript. AL: Study supervision, analysis of data,\ndrafted the manuscript, critically revised the manuscript.\nConﬂict of Interests: AHYS and ACWS would like to\ndeclare they are brothers. However, both have contributed\nsigniﬁcantly to the work of this paper. AHYS would like to\ndeclare that he is a member of the AMA (NSW).\nData Reproducibility: The dataset presented in the study\nis available on request from the corresponding author\nduring submission or after publication.\nEthical Approval: Institutional ethics were not required\nas no human participants, people, medical records, or\nanimal studies were involved in this study .\nFunding/Support: There was no funding/support for this\nmanuscript.\nReferences\n1. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, et al.\nLanguage models are few-shot learners. Adv Neural Inf Process Syst.\n2020;33:1877–901.\n2. Gao CA, Howard FM, Markov NS, Dyer EC, Ramesh S, Luo Y, et al.\nComparing scientiﬁc abstracts generated by ChatGPT to original\nabstracts using an artiﬁcial intelligence output detector, plagiarism\ndetector, and blinded human reviewers. npj Dig Med. 2022; 6(75\n(2023)). https://doi.org/10.1038/s41746-023-00819-6.\n3. Hacker P, Engel A, Mauer M. Regulating ChatGPT and other Large\nGenerative AI Models. arXiv preprint arXiv:230202337. 2023:1112–23.\nhttps://doi.org/10.1145/3593013.3594067.\n4. Seth I, Sinkjaer Kenney P, Bulloch G, Hunter-Smith DJ, Bo Thomsen\nJ, Rozen WM. Artiﬁcial or Augmented Authorship? A Conversation\nwith a Chatbot on Base of Thumb Arthritis. Plast Reconstr Surg Glob\nOpen. 2023;11(5). e4999. [PubMed ID: 37250832]. [PubMed Central ID:\nPMC10219695]. https://doi.org/10.1097/GOX.0000000000004999.\n5. Kung TH, Cheatham M, Medenilla A, Sillos C, De Leon L, Elepano C, et al.\nPerformance of ChatGPT on USMLE: Potential for AI-assisted medical\neducation using large language models. PLOS Digit Health. 2023;2(2).\ne0000198. [PubMed ID: 36812645]. [PubMed Central ID: PMC9931230].\nhttps://doi.org/10.1371/journal.pdig.0000198.\n6. Lee H. The rise of ChatGPT: Exploring its potential in medical\neducation. Anat Sci Educ . 2023. [PubMed ID: 36916887].\nhttps://doi.org/10.1002/ase.2270.\n7. Chow JCL, Sanders L, Li K. Impact of ChatGPT on medical chatbots\nas a disruptive technology . Front Artif Intell . 2023; 6:1166014.\n[PubMed ID: 37091303]. [PubMed Central ID: PMC10113434].\nhttps://doi.org/10.3389/frai.2023.1166014.\n8. Ali R, Tang OY, Connolly ID, Zadnik Sullivan PL, Shin JH, Fridley JS,\net al. Performance of ChatGPT and GPT-4 on Neurosurgery Written\nBoard Examinations. Neurosurgery. 2023. [PubMed ID: 37581444].\nhttps://doi.org/10.1227/neu.0000000000002632.\n9. Teebagy S, Colwell L, Wood E, Yaghy A, Faustina M. Improved\nPerformance of ChatGPT-4 on the OKAP Examination: A Comparative\nStudy with ChatGPT-3.5. J Acad Ophthalmol (2017). 2023; 15(2):e184–7.\n[PubMed ID: 37701862]. [PubMed Central ID: PMC10495224].\nhttps://doi.org/10.1055/s-0043-1774399.\n10. Seth I, Bulloch G, Rozen WM. Applications of Artiﬁcial Intelligence\nand Large Language Models to Plastic Surgery Research.\nAesthet Surg J . 2023; 43(10):NP809–10. [PubMed ID: 37392428].\nhttps://doi.org/10.1093/asj/sjad210.\n11. Smith JA, Kaye AH, Christophi C, Brown WA. Textbook of surgery. John\nWiley & Sons; 2020.\n12. Farne H, Norris-Cervetto E, Warbrick-Smith J. Oxford cases in medicine\nand surgery. Oxford University Press; 2015.\n13. Nemoto T, Beglar D. Developing Likert-scale questionnaires.In: Sonda\nN, Krause A, editors.JALT 2013 conference proceedings. Tokyo: JALT. 2014.\np. 1–8.\n14. Charnock D, Shepperd S, Needham G, Gann R. DISCERN: an\ninstrument for judging the quality of written consumer health\ninformation on treatment choices. J Epidemiol Community Health.\n1999;53(2):105–11. [PubMed ID: 10396471]. [PubMed Central ID:\nPMC1756830]. https://doi.org/10.1136/jech.53.2.105.\n15. England GW, Thomas M, Paterson DG. Reliability of the original\nand the simpliﬁed Flesch reading ease formulas. J Appl Psychol.\n1953;37(2):111–3. https://doi.org/10.1037/h0055346.\n16. Flesch R. Flesch-Kincaid readability test. Retrieved October .\n2007;26(3):2007.\n17. Coleman M, Liau TL. A computer readability formula\ndesigned for machine scoring. J Appl Psychol . 1975; 60(2):283–4.\nhttps://doi.org/10.1037/h0076540.\n18. Stake RE. The art of case study research. SAGE; 1995.\n19. Rees CE, Ford JE, Sheard CE. Evaluating the reliability of DISCERN:\na tool for assessing the quality of written patient information on\ntreatment choices. Patient Educ Couns. 2002;47(3):273–5. [PubMed ID:\n12088606]. https://doi.org/10.1016/s0738-3991(01)00225-7.\n20. Cassidy JT, Baker JF. Orthopaedic Patient Information on the World\nWide Web: An Essential Review .J Bone Joint Surg Am. 2016;98(4):325–38.\n[PubMed ID: 26888683]. https://doi.org/10.2106/JBJS.N.01189.\n21. Kher A, Johnson S, Griﬃth R. Readability Assessment of Online\nPatient Education Material on Congestive Heart Failure. Adv Prev\nMed. 2017; 2017:9780317. [PubMed ID: 28656111]. [PubMed Central ID:\nPMC5471568]. https://doi.org/10.1155/2017/9780317.\n22. Szmuda T, Ozdemir C, Ali S, Singh A, Syed MT, Sloniewski P. Readability\nof online patient education material for the novel coronavirus\ndisease (COVID-19): a cross-sectional health literacy study . Public\nHealth. 2020; 185:21–5. [PubMed ID: 32516624]. [PubMed Central ID:\nPMC7260546]. https://doi.org/10.1016/j.puhe.2020.05.041.\n23. Wadhwa RR, Marappa-Ganeshan R. Disclosure: Raghavendra\nMarappa-Ganeshan declares no relevant ﬁnancial relationships\nwith ineligible companies. T Test. Treasure Island, USA: StatPearls\nPublishing; 2023.\n24. Doshi R, Amin K, Khosla P, Bajaj S, Chheang S, Forman HP. Utilizing\nLarge Language Models to Simplify Radiology Reports: a comparative\nanalysis of ChatGPT3.5, ChatGPT4.0, Google Bard, and Microsoft Bing.\nmedRxiv. 2023. https://doi.org/10.1101/2023.06.04.23290786.\n8 J Med Edu. 2023; 22(1):e137753.\nSiu AHY et al.\n25. Davis JL, Murray JF. History and physical examination. Murray and\nNadel’s Textbook of Respiratory Medicine. 263. Elsevier; 2016.\n26. Randhawa S, Bhullar JS, Rana G, Bhullar A, Mittal VK, Goriel Y.\nNecrotizing fasciitis–a sinister complication of hemorrhoidectomy .\nInt J Colorectal Dis . 2015; 30(6):851–2. [PubMed ID: 25367181].\nhttps://doi.org/10.1007/s00384-014-2050-4.\n27. Misiakos EP, Bagias G, Papadopoulos I, Danias N, Patapis P,\nMachairas N, et al. Early Diagnosis and Surgical Treatment for\nNecrotizing Fasciitis: A Multicenter Study . Front Surg . 2017; 4:5.\n[PubMed ID: 28224127]. [PubMed Central ID: PMC5293831].\nhttps://doi.org/10.3389/fsurg.2017.00005.\n28. Luckey A, Livingston E, Tache Y. Mechanisms and treatment of\npostoperative ileus. Arch Surg . 2003; 138(2):206–14. [PubMed ID:\n12578422]. https://doi.org/10.1001/archsurg.138.2.206.\n29. Small C, Laycock H. Acute postoperative pain management.\nBr J Surg . 2020; 107(2):e70–80. [PubMed ID: 31903595].\nhttps://doi.org/10.1002/bjs.11477.\n30. Li Z, Li Z, Zhao L, Cheng Y, Cheng N, Deng Y. Abdominal drainage\nto prevent intra-peritoneal abscess after appendectomy for\ncomplicated appendicitis. Cochrane Database Syst Rev . 2021; 8(8).\nCD010168. [PubMed ID: 34402522]. [PubMed Central ID: PMC8407456].\nhttps://doi.org/10.1002/14651858.CD010168.pub4.\n31. Bower KL, Lollar DI, Williams SL, Adkins FC, Luyimbazi DT, Bower\nCE. Small Bowel Obstruction. Surg Clin North Am. 2018; 98(5):945–71.\n[PubMed ID: 30243455]. https://doi.org/10.1016/j.suc.2018.05.007.\n32. Tong JWV, Lingam P, Shelat VG. Adhesive small bowel obstruction\n- an update. Acute Med Surg. 2020; 7(1). e587. [PubMed ID: 33173587].\n[PubMed Central ID: PMC7642618]. https://doi.org/10.1002/ams2.587.\n33. Naidu K. Small bowel obstruction. Emergency Surgery for Low Resource\nRegions. Springer; 2021. p. 135–40.\n34. Schick MA, Kashyap S, Meseeha M. Small Bowel Obstruction. Treasure\nIsland, USA: StatPearls Publishing; 2017.\n35. Wartman SA, Combs CD. Reimagining Medical Education in the\nAge of AI. AMA J Ethics. 2019; 21(2):E146–52. [PubMed ID: 30794124].\nhttps://doi.org/10.1001/amajethics.2019.146.\n36. Seth I, Cox A, Xie Y, Bulloch G, Hunter-Smith DJ, Rozen WM, et\nal. Evaluating Chatbot Eﬃcacy for Answering Frequently Asked\nQuestions in Plastic Surgery: A ChatGPT Case Study Focused on\nBreast Augmentation. Aesthet Surg J. 2023;43(10):1126–35. [PubMed ID:\n37158147]. https://doi.org/10.1093/asj/sjad140.\n37. Xie Y, Seth I, Hunter-Smith DJ, Rozen WM, Ross R, Lee M. Aesthetic\nSurgery Advice and Counseling from Artiﬁcial Intelligence: A\nRhinoplasty Consultation with ChatGPT. Aesthetic Plast Surg. 2023.\n[PubMed ID: 37095384]. https://doi.org/10.1007/s00266-023-03338-7.\n38. Farrokhnia M, Banihashem SK, Noroozi O, Wals A. A\nSWOT analysis of ChatGPT: Implications for educational\npractice and research. Innov Educ Teach Int . 2023:1–15.\nhttps://doi.org/10.1080/14703297.2023.2195846.\n39. Sapci AH, Sapci HA. Artiﬁcial Intelligence Education and Tools for\nMedical and Health Informatics Students: Systematic Review . JMIR\nMed Educ. 2020;6(1). e19285. [PubMed ID: 32602844]. [PubMed Central\nID: PMC7367541]. https://doi.org/10.2196/19285.\n40. Winkler-Schwartz A, Bissonnette V, Mirchi N, Ponnudurai N, Yilmaz\nR, Ledwos N, et al. Artiﬁcial Intelligence in Medical Education: Best\nPractices Using Machine Learning to Assess Surgical Expertise in\nVirtual Reality Simulation. J Surg Educ. 2019; 76(6):1681–90. [PubMed\nID: 31202633]. https://doi.org/10.1016/j.jsurg.2019.05.015.\nJ Med Edu. 2023; 22(1):e137753. 9",
  "topic": "Readability",
  "concepts": [
    {
      "name": "Readability",
      "score": 0.9309704303741455
    },
    {
      "name": "Likert scale",
      "score": 0.7604601383209229
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5939193367958069
    },
    {
      "name": "Reading (process)",
      "score": 0.5924911499023438
    },
    {
      "name": "Curriculum",
      "score": 0.5779869556427002
    },
    {
      "name": "Medical education",
      "score": 0.5600014925003052
    },
    {
      "name": "Index (typography)",
      "score": 0.4890192449092865
    },
    {
      "name": "Psychology",
      "score": 0.4404485821723938
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.42487984895706177
    },
    {
      "name": "Medicine",
      "score": 0.38411226868629456
    },
    {
      "name": "Computer science",
      "score": 0.21933132410049438
    },
    {
      "name": "Pedagogy",
      "score": 0.15088927745819092
    },
    {
      "name": "Political science",
      "score": 0.11066368222236633
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    }
  ]
}