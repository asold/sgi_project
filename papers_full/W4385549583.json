{
  "title": "Structure-Informed Protein Language Models are Robust Predictors for Variant Effects",
  "url": "https://openalex.org/W4385549583",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2555066594",
      "name": "Yuanfei Sun",
      "affiliations": [
        "Texas A&M University",
        "Mitchell Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2097850282",
      "name": "Yang Shen",
      "affiliations": [
        "Texas A&M University",
        "Mitchell Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3104285666",
    "https://openalex.org/W2883972171",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W2943495267",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W3133458480",
    "https://openalex.org/W4286500588",
    "https://openalex.org/W4281648132",
    "https://openalex.org/W4310886000",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W3186179742",
    "https://openalex.org/W6600005967",
    "https://openalex.org/W6828894009",
    "https://openalex.org/W2612196455",
    "https://openalex.org/W4319071809",
    "https://openalex.org/W2064795263",
    "https://openalex.org/W2995514860",
    "https://openalex.org/W3144239152",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W1563940013",
    "https://openalex.org/W2245592118",
    "https://openalex.org/W2971227267",
    "https://openalex.org/W3209435229",
    "https://openalex.org/W2987965949",
    "https://openalex.org/W4324311892",
    "https://openalex.org/W3215086693",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W4388024559",
    "https://openalex.org/W2143238378",
    "https://openalex.org/W4296060337",
    "https://openalex.org/W3037888463",
    "https://openalex.org/W3111174583",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W4206940890",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4311943761",
    "https://openalex.org/W3179485843",
    "https://openalex.org/W3154275519",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W4225868104",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2890223884",
    "https://openalex.org/W4234609530",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W4366163632"
  ],
  "abstract": null,
  "full_text": "Structure-Informed Protein Language Models are\nRobust Predictors for Variant Effects\nYuanfei Sun \nTexas A&M University\nYang Shen \nTexas A&M University\nResearch Article\nKeywords: Variant effect prediction, protein sequences, protein structures, protein language models,\nmultimodal machine learning.\nPosted Date: August 3rd, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3219092/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nVersion of Record: A version of this preprint was published at Human Genetics on August 8th, 2024. See\nthe published version at https://doi.org/10.1007/s00439-024-02695-w.\nStructure-Informed Protein Language Models are\nRobust Predictors for Variant Eﬀects\nYuanfei Sun1 and Yang Shen1,2,3*\n1Department of Electrical and Computer Engineering, Texas A&M\nUniversity, College Station, 77843, Texas, USA.\n2Department of Computer Science and Engineering, Texas A&M\nUniversity, College Station, 77843, Texas, USA.\n3Institute of Biosciences and Technology and Department of Translational\nMedical Sciences, Texas A&M University, Houston, 77030, Texas, USA.\n*Corresponding author(s). E-mail(s): yshen@tamu.edu;\nContributing authors: sunyuanfei@tamu.edu;\nAbstract\nPredicting protein variant eﬀects through machine learning is oft en challenged\nby the scarcity of experimentally measured eﬀect labels. Recen tly, protein lan-\nguage models (pLMs) emerge as zero-shot predictors without the n eed of eﬀect\nlabels, by modeling the evolutionary distribution of function al protein sequences.\nHowever, biological contexts important to variant eﬀects are im plicitly modeled\nand eﬀectively marginalized. By assessing the sequence aware ness and the struc-\nture awareness of pLMs, we ﬁnd that their improvements often correl ate with\nbetter variant eﬀect prediction but their tradeoﬀ can present a b arrier as observed\nin over-ﬁnetuning to speciﬁc family sequences. We introduce a fra mework of\nstructure-informed pLMs (SI-pLMs) to inject protein structural con texts pur-\nposely and controllably, by extending masked sequence denoi sing in conventional\npLMs to cross-modality denoising. Our SI-pLMs are applicable to revising any\nsequence-only pLMs through model architecture and training obje ctives. They do\nnot require structure data as model inputs for variant eﬀect predict ion and only\nuse structures as context provider and model regularizer during tra ining. Numeri-\ncal results over deep mutagenesis scanning benchmarks show th at our SI-pLMs,\ndespite relatively compact sizes, are robustly top performers agai nst competing\nmethods including other pLMs, regardless of the target protein fa mily’s evolution-\nary information content or the tendency to overﬁtting / over-ﬁnet uning. Learned\ndistributions in structural contexts could enhance sequence d istributions in pre-\ndicting variant eﬀects. Ablation studies reveal major contribu ting factors and\n1\nanalyses of sequence embeddings provide further insights. The data and scripts\nare available at https://github.com/Stephen2526/Structure-informed PLM.git.\nKeywords: Variant eﬀect prediction, protein sequences, protein stru ctures, protein\nlanguage models, multimodal machine learning.\n1 Introduction\nThe workhorse molecule of life, proteins play a central role in cellu lar functions. Their\nvariations in humans and pathogens often lead to genetic diseases and therap eutic\nresistance, respectively. The ability to decipher the associ ation between protein varia-\ntions and resulting eﬀects could facilitate prognostics and therapeu tics for diseases.\nAlthough multiplexed assays of variant eﬀects (MAVE), such as deep mut ational scan-\nning (DMS) and massively parallel reporter assay (MPRA), are increasi ngly generating\ndata of variant eﬀects ranging from protein stability to cell viability [1, 2], their speed\nand applicability can be dwarfed by the amount of variants and eﬀects to ch aracterize.\nTherefore, there is a pressing demand to develop high-throughput an d high-accuracy\ncomputational tools for variant eﬀect prediction, ideally with mechani stic explainability.\nThe task of predicting functional eﬀects of genetic variations has been recently\npursued by machine learning methods, supervised and unsupervi sed. Experimental\nmeasurements of eﬀects ranging from protein properties to clinical s igniﬁcance have\nbeen curated in public databases and provided labels for supervised machine learning.\nTo predict such labels, input features for supervised machine le arning models are often\nmanually engineered, such as physicochemical properties of sequen ce and structure,\nevolutionary proﬁles and conservation, or interaction features, as seen i n representative\nmethods such as DEOGEN2 [ 3], PolyPhen-2 [ 4] and MutPred2 [ 5]. The “features” can\nalso be representations learned from sequence data using neural ne tworks, as explored\nin PrimateAI [ 6], UniRep [ 7] and TAPE [ 8].\nCompared to supervised predictors, unsupervised ones do not rely on labels that\ntend to be scarce and rather exploit the abundance of unlabeled sequen ce data. They\noften estimate variant eﬀects with evolutionary landscapes inferred from sequence data\n(especially homologous sequences of the target protein). SIFT [ 9] initiated this line of\nwork with a ﬁrst-order or site-independent model using position- speciﬁc-scoring-matrix\n(PSSM). EVmutation [ 10] extended it by considering pairwise interactions in a 20-\nstate Potts model. DeepSequence [ 11] further introduced higher-order interactions with\na latent variable model (variational autoencoder), and scored variants b y sampling\nmultiple times on the ELBO (evidence lower bound) of the log likeli hood for the wild\ntype and any given variant sequence. EVE [ 12] subsequently enhanced the DeepSequence\narchitecture by factorizing decoder weights with the Gaussian dis tribution. Multiple\nsequence alignments (MSAs) are required for the unsupervised mod els described above,\nwhich poses challenges to certain cases without suﬃcient homologous seq uences to reveal\nevolutionary information (such as orphan proteins and designed proteins) . WaveNet [ 13]\neliminated this requirement, thus alignment-free, by training an autoregressive model\nusing the task of predicting the next residue.\n2\nA recent wave of unsupervised variant eﬀect predictors is instigat ed by protein\nlanguage models (pLMs). Unlike aforementioned unsupervised models, they learn\nthe evolutionary information across families of homologous proteins, using l arge-scale\ncorpora of protein sequences (such as Pfam, UniRef and BFD) and deep-le arning\nadvances in natural language processing (such as LSTM and Transformers). Un iRep [ 7]\nand SeqVec [ 14] started with LSTM models. TAPE [ 8] benchmarked ResNet, LSTM,\nand transformers and showed the superiority of transformers. Since t hen, a series of\ntransformer-based pLMs have been developed with increasing size an d complexity, from\n108 to 10 10 parameters, represented by ESM-1b [ 15], ProGen [ 16], ProtTrans [ 17], MSA-\ntransformer [ 18], ESM-1v [ 19] and ESM-2 [ 20]. Importantly, self-supervised pretraining\nof these pLMs learned contexualized representations, attentions, an d distributions\nof amino acids from functionally-ﬁt sequences, which is informative in evolutionary,\nstructural and functional contexts and beneﬁcial to various downstre am tasks (including\nvariant eﬀect prediction [ 19, 21, 22], protein function prediction [ 23, 24], supervised few-\nshot protein engineering [ 25], and protein design [ 16, 26].) A summary of comparisons\namong these pLMs can be found in Table 6.\nBeing a state of the art for variant eﬀect prediction, especially in t he zero-shot\nscenario where no labels of variant eﬀects are used, pLMs still have m ajor limitations\nto overcome [ 27]. Although variations originate in genetic and protein sequences, their\neﬀects are often manifested in biological contexts and may not be adequate ly captured\nin a model with sequence information alone. Moreover large-scale pLMs ar e prone\nto overﬁtting to training sequences [ 19], which hurts the robustness of variant eﬀect\npredictors.\nTo address the aforementioned limitations of pLMs for variant eﬀect pred iction, we\nhave developed a framework to introduce protein structures, an add itional modality to\nprotein sequences, to pLMs as a context provider and model regulariz er. Compared to\nthe state-of-the-art pLMs, our models also start with pre-training o ver representative\ndomain sequences for global albeit coarse-grained evolutionary distribut ions. But they\nare structurally informed during ﬁne-tuning over target protein f amily’s sequences and\nstructures for local and ﬁne-grained evolutionary distributions (Fi g. 1).\nIn developing such a structure-informed pLM, there are a few chall enges in data\nand in machine learning formulation. First, the two modalities of prot ein data, 1D\nsequences and 3D structures, are imbalanced. Compared to that of seque nces, the\namount of experimentally determined structures in the Protein Dat a Bank is orders of\nmagnitude less. The recent breakthroughs in protein structure pre diction (AF2 [ 28],\nRoseTTAFold [29] and ESMFold [ 20]) reduce but do not remove the modality imbalance.\nSecond, variant structures are mostly unavailable (and impossible to enumerate)\nexperimentally; whereas predicting conformational changes upon sequ ence variation is\nstill challenging to the best structure predictors [ 30]. These led to the often unavailability\nof structures as model inputs for variant eﬀect predictors. Last, des pite recent works\nincorporating structures into pLMs through multi-task learning [ 31], serial encoding [ 32]\nand conditioning [ 33], some of which demand protein structures as part of model\ninputs, it remains largely unanswered how to rationally introduce s tructure data into\nthe formulation of masked language modeling in pLMs and how to practically e nhance\n3\nFig. 1 Conceptual diﬀerences among competing methods and our stru cture-informed protein language\nmodels (SI-pLMs) from the data perspective. Whereas many fam ily-speciﬁc models are trained over\naligned (blue) or unaligned sequences (orange) in a protein family, pLMs are often pre-trained over\nunaligned (green) or aligned sequences (red) in the protein universe and some of them can be ﬁne-\ntuned over family sequences. In contrast, our SI-pLMs after pretraining are ﬁnetuned with both family\ndatasets of sequences and structures.\nrather than dilute the sequence-derived evolutionary information w ith structures for\nthe granularity needed for variant eﬀect prediction.\nTo address the challenges above, we have introduced the formulation of cross-\nmodality masked modeling with the task decomposition of conventional s equence-\nonly masked modeling and newly introduced sequence-to-structu re masked modeling\n(Sec. 3.2). In this way we reach a ﬂexible framework of SI-pLM where any convent ional\nsequence-only pLMs can be appended with structural decoders in ne ural network\narchitectures and regularized by self-supervised masked sequen ce-to-structure tasks\n(illustrated in Fig. 2 and detailed in Sec. 3.3). With a controllable hyperparameter\nto adjust the level of injecting structure awareness, this frame work is more generic\ncompared to conventional sequence-only pLMs as it reduces to the latte r when the\nhyperparameter is set at zero. Furthermore, during inference, our framework does not\nrequire any structure inputs; and during training, it does not req uire variant structures\nand can utilize both paired sequence–structure data and unpaired seq uences (with\nstructures absent).\nUnder the framework of SI-pLMs, we ﬁrst deﬁne and evaluate sequence aw areness\nand structure awareness in conventional sequence-only pLMs, which unravels pLMs’\neﬀectiveness in variant eﬀect prediction and motivates our approach to explicitly\ninjecting structure awareness in a controllable fashion (Sec. 2.1). With our curated\nstructures from experiments (PDB) and predictions (AlphaFold DB) , we then assessed\nour SI-pLMs against competing methods over 35 DMS datasets of variant ﬁtne ss\n(Sec. 2.3). We found their ranking performances among the best compared to mode ls of\n2–12 times more parameters and robust across families of various sequence in formation\n4\nFig. 2 Model architecture of our structure-informed protein lang uage models based on cross-modal\nsequence and structure denoising. Our model only takes sequ ential input of noised amino acid types\n(gray ones are masked), which is easily applicable to varian t sequences without structures during\ninference. Protein LM module is warm-started from a pretrai ned in-house protein BERT model and\ngenerates features for denoising modules. Embeddings afte r the last layer in pLM, after an outer\nproduct moduel, and attention matrices from all heads are co ncatenated as features for individual\namino acids and their pairs, respectively, for denoising se quences and decoding structures. Speciﬁcally,\nthe masked amino acids are recovered by amino acid (AA) predi ction head. And three structural\nproperties: secondary structure(SS), relative solvent ac cessibility (RSA) and contact map (ContMap),\nare classiﬁed with their corresponding prediction heads (d ecoders) in a dense manner.\n(measured by MSA depth). Importantly, we found that ﬁne-tuned fami ly-speciﬁc\npLMs can perform worse than pre-trained meta pLMs in family-speciﬁc v ariant eﬀect\nprediction (Sec. 2.2) and such over-ﬁnetuning or “overﬁt” can be mitigated by structural\ninformation incorporated in our SI-pLMs (which enhances another sense of robustness).\nCompared to using paired sequence–structure data only, SI-pLMs per formances in\nvariant eﬀect prediction are further improved when unpaired sequ ences (with structures\nabsent) are additionally considered (Sec. 2.3), regardless of the extent of structure\navailability in individual families. Learning distributions in st ructural contexts not\nonly regularized learning sequence distributions but could also im prove the latter’s\nvariant scoring capability (Sec. 2.4). Lastly, we reveal the main contributing factors of\nour SI-pLMs in ablation studies (Sec. 2.5) and assess how SI-pLMs aﬀect the latent\nembeddings of variant sequences toward variant eﬀect prediction (Se c. 2.6).\n2 Results\n2.1 Fitness sensitivity in pLMs is supported by the underlying\nsequence and structure awareness\nProtein sequence modeling, especially through transformer-based p rotein language\nmodels (pLMs), has been shown eﬀective for variant ﬁtness predict ion without using\nﬁtness labels during training. Trained over protein corpora of natural sequences, pLMs\ncan learn the distribution of amino acid compositions underlying func tional sequences\n5\nand predict variant eﬀects with estimated log odds (the ratios betwe en variant and\nwild-type likelihoods).\nTo understand the knowledge transfer from sequence modeling in p LMs to the down-\nstream variant eﬀect prediction, we followed the protein sequence -structure-function\nparadigm and examined sequence and structure awareness of pLMs in assoc iation\nwith their ﬁtness sensitivity. Sequence awareness measures a model’s ability to recover\nmasked amino acids, aligned with the goal of sequence modeling. Similarl y we deﬁned\nstructure awareness as a model’s ability to predict three struc tural properties: sec-\nondary structure (SS), relative solvent accessibility (RSA) and Cβ distance map (DM).\nAs conventional pLMs only produce distribution over amino acids, a sim ple multilayer\nperceptron (MLP) was trained for each property using amino acid embedd ings at the\nﬁnal layer of the trained sequence models. Continuous property labe ls (RSA and DM)\nwere discretized to make each task classiﬁcation. The MLPs were train ed over the same\nfamily-speciﬁc structure property sets, utilized to develop s tructure-informed pLMs in\nSec. 2.3.\nWe pre-trained ﬁve bidirectional transformer encoder (BERT) pLMs over combina-\ntions of four architectures and two Pfam representative proteomes domai n sequences.\nThe four architecture variations of BERT were utilized where from B4 to B1 more\nself-attention heads, layers, and neurons in the position speciﬁc f eed-forward layer\nare included (See details in Sec. 3.3 and architectures in Table 5). The two sequence\ndatasets were RP15 and RP75 at at 15% and 75% co-membership thresholds, respe c-\ntively. We selected 12 ﬁtness sets from a benchmark set [ 11] and evaluated the ﬁve\nencoders’ sequence/structure awareness (towards target protein s) and ﬁtness sensitiv-\nity (Table 1). We also ﬁne-tuned the ﬁve encoders for each of the 12 protein famili es’\nsequences and performed the same evaluation.\nNumerical results showed strong correlation between sequence/str ucture awareness\nand ﬁtness sensitivity. Among pre-trained models, the best ﬁtne ss predictor, with the\naverage Spearman’s ρ at 0.302, exhibited the highest sequence awareness of 0.411. We\nalso found awareness supporting for the second best predictor, proc essing the highest\noverall sequence+structure awareness ( ASRD value 0.485), and the third one having\nthe best overall structure awareness ( SRD value 0.557). After ﬁne-tuning on family\nsequences, these models showed signiﬁcant improvement in ﬁtn ess prediction, with the\naverage ρ increasing from ∼0.25 to 0.55 and the standard deviation reducing almost by\nhalf. It is noteworthy that such improvements in ﬁtness predicti on were accompanied\nwith those in sequence awareness ( AA increasing from 0.4 to nearly 0.9) and structure\nawareness ( SRD increasing from 0.55 to 0.60). The best ﬁtness predictor among the\nﬁne-tuned models was also the one with the highest structure and st ructure awareness.\nThese results indicate that the amount of biological contexts learned in pLMs is\ncorrelated with their abilities for variant eﬀect prediction, whic h motivates our approach\nto purposed injection of structure awareness in pLMs for variant eﬀec t prediction.\n2.2 Structure information abates overﬁtting in sequence\nmodeling\nAlthough ﬁne-tuning with family sequences is an eﬀective way to adap t pre-trained\nmeta pLMs for the local and ﬁne-grained evolutionary landscapes of target p roteins\n6\nTable 1 Five in-house pLMs’ variant ﬁtness sensitivity (Spearman’ s rank correlation between model\nscoring and experiment ﬁtness measurements) versus the awa reness of target protein’s sequence and\nstructure (clarity scores detailed next) evaluated over 12 ﬁtness datasets. The normalized clarity score,\nderived from perplexity, represents models’ awareness tow ards sequences or structures of target\nproteins. Integers in the parenthesis are class numbers for corresponding properties. SRD, as a\nsummarized score for structure awareness, takes geometric mean of Secondary Structure( SS), Relative\nSolvent Accessibility(R SA) and Distance Map(D M). ASRD additionally includes sequence awareness\non Amino Acids(A A) in averaging. All values are reported in percentage forma t and values with\nsmaller font sizes are standard deviations for the precedin g quantities. Superscripts refer to ranking\namong the group and the maximum clarity values are underscor ed for each property.\nEncoder Spearman’s ρ ↑ Normalized Clarity( × 1e-2) on WT proteins ↑\n(× 1e-2) A A(20) S S(3) R SA(2) D M(32) SRD ASRD\nSequence-Pretrained Meta pLMs\nRP75 B1 18.97 14.38 28.78 22.40 55.98 8.23 35.34 7.91 73.82 18.08 52.52 10.46 43.03 13.51\nRP15 B1 24.893 14.83 35.92 25.03 58.37 5.82 37.93 6.25 78.22 14.38 55.66 8.12 47.79 13.02\nRP15 B2 25.912 16.40 38.40 24.89 58.89 5.67 37.32 6.21 76.73 15.84 55.13 8.34 48.47 13.29\nRP15 B3 30.231 18.10 41.14 29.12 56.25 7.63 35.72 7.90 71.68 19.10 52.25 10.61 46.66 16.97\nRP15 B4 24.03 17.32 36.28 26.65 55.87 7.78 35.32 7.80 71.24 18.66 51.85 10.51 45.30 15.98\nSequence-Finetuned Family-speciﬁc pLMs\nRP75 B1 55.12 10.23 83.83 10.16 61.31 3.33 41.24 3.38 80.18 9.23 58.67 4.25 63.99 3.92\nRP15 B1 55.791 9.58 87.26 3.61 61.86 2.70 42.08 3.13 81.84 8.93 59.67 4.06 65.59 3.62\nRP15 B2 55.49 8.28 85.65 4.14 61.83 2.70 41.53 3.82 80.72 9.68 59.11 4.58 64.83 4.28\nRP15 B3 55.54 7.60 83.93 5.29 59.41 4.45 39.70 4.68 75.02 15.01 55.98 6.88 61.88 6.32\nRP15 B4 55.24 8.07 80.46 6.78 58.83 5.35 38.62 5.86 74.95 15.27 55.30 7.93 60.64 7.23\n(as demonstrated in the average performances in Table 1), overﬁtting may occur and\ncause poor performances in predicting variant ﬁtness, especially f or over-parameterized\npLMs. In fact, ESM-1v has experienced rapid overﬁtting when naivel y ﬁne-tuned on\nfamily sequences and had to resort to a new strategy called spiked ﬁne -tuning [ 19].\nTo examine the overﬁtting issue during ﬁne-tuning, we selecte d the encoder RP15 B2\n(50M parameters) and examined its ﬁtness prediction performances for e ach of the 12\ndatasets (families) before and after ﬁne-tuning. Fig. shows analyses o ver the 12 families,\nordered in decreasing diﬀerence in Spearman’s correlation between pre-trained (gray\nsquare) and ﬁne-tuned (black square). We found that one third (the 4 t o the right)\nof the cases had the overﬁt issue, indicated by worse ﬁtness ranking (up to 0.1) after\nﬁne-tuning.\nTo provide a possible remedy to overﬁt, we examined the sequenc e and structure\nawareness of ﬁne-tuned RP15 B2 in each case in comparison to those of the other four\nﬁne-tuned models. Supplemental Fig. A1 shows that, once reaching the local landscape\nwith ﬁne-tuning, there might be a trade-oﬀ between sequence and st ructure awareness\nand simultaneous improvements in both may no longer be feasible. In t he overﬁtting\ncases ( DLG4, PABP, and YAP1), better ﬁtness predictors than RP15 B2 existed with\nfamily-speciﬁc trade-oﬀs tilted more toward structure awareness .\n7\nFig. 3 Fitness prediction performance declines after ﬁne-tuning when overﬁtting happens in sequence\nmodeling. The meta pre-trained model (gray square) was ﬁne- tuned (black square) over family\nsequences to enhance evolutionary awareness speciﬁcally f or the target protein. This ﬁne-tuning\nprocess aimed to improve ﬁtness prediction, as demonstrate d by the ﬁtness sets on the left side.\nHowever, as we moved towards the right side, we noticed dimin ishing improvement and even increasing\ndeterioration in Spearman’s rank correlation, indicating the occurrence of overﬁtting over the sequence\nmodeling task. Such over-ﬁnetuning or overﬁtting was evide nt in the last four sets on the right. Our\ninitial experiments indicate that by incorporating struct ure information through model regularization,\nwe can enhance the robustness and eﬀectiveness of ﬁne-tunin g for ﬁtness prediction. The weighting\nhyper-parameter λ, which balanced the contribution of structure tasks within the overall objective\nfunction, was tested over a grid of values from 0.0 to 20.0, as illustrated in the legend.\nThese results indicate that the ability to adjust the trade-oﬀ betw een sequence\nand structure awareness during ﬁne-tuning could mitigate the ove rﬁtting issue, which\nprovides another motivation to our approach to controllable injection of st ructure\nawareness in pLMs for variant eﬀect prediction.\n2.3 Structure-informed pLMs predict variant ﬁtness robustly\nMotivated to inject structure awareness into pLM purposely (Sec. 2.1) and controllably\n(Sec. 2.2) for robust variant eﬀect prediction, we propose to extend masked langu age\nmodeling in pLMs and introduce structure-informed (SI)-pLMs th rough cross-modal\nmasked modeling (denoising) (Sec. 3.2). This SI-pLM framework consists of two\nsimultaneous learning tasks: intra-modal sequence denoising as in c onventional pLMs\nand cross-modal sequence-to-structure denoising as auxiliary tasks . Speciﬁcally, the\nauxiliary tasks include predictions from masked sequences to thre e structure properties\nas in evaluating structure awareness: two 1D properties (relative s olvent accessibility or\nRSA and secondary structure or SS, per residue) and one 2D property (di stograms for\nresidue pairs). Accordingly, additional decoders are appended to a tr ansformer-based\nlanguage model; and additional losses are averaged and weighted by a controllab le\n8\nhyperparameter λ, then added to the loss of masked language modeling, so as to train\nSI-pLMs. Our SI-pLMs are illustrated in Fig. 2 and detailed in Sec. 3.3. Compared to\nconventional sequence-only pLMs, SI-pLMs are more general and its speci al case when\nλ = 0 is equivalent to conventional pLMs.\nOur most advanced version of SI-pLMs uses both sequence-structure p airs and\nunpaired sequences as training data and a small ﬁtness-labeled subs et as validation to\ntune λ over a grid 0, 0.5, 2, 20 (Sec. 3.4). We assessed its performance based on the\nencoder RP15 B2 (50M parameters) over a benchmark dataset for ﬁtness prediction.\nEach protein labeled with variant eﬀects in this dataset comes with an M SA ﬁle queried\nover the UniRef100 database. We further curated crystal or predicted s tructures for\neach family sequence available from Protein Data Bank and AlphaFold Struc ture\nDatabase (AFDB), respectively. Apart from uploaded structures in AFDB , we didn’t\nadditionally predict structures using the AlphaFold2 software, so n ot necessarily every\nsequence has a paired structure. As more than 99% sequences in viral fam ilies have\nno structures, they are excluded from this study. The ﬁnal dataset contains 35 DMS\nsets (12 of which were used in the motivational studies in Sec. 2.1 and Sec. 2.2). More\ndetails about data preparation can be found in Sec. 3.4.\nWe compared our SI-pLM in ranking (Spearman’s ρ) and classiﬁcation (AUROC\nand AUPRC) to 10 competing methods including alignment-based PSSM, EVMutation,\nDeepSequence, and MSA-transformer as well as alignment-free UniRep, WaveNet,\nTAPE, ESM-1b, and ESM-1v (pretrained and ﬁne-tuned). For ranking per formances\nwe also split the families by sequence information (measured by MSA depth ( Neﬀ /L\nwith cutoﬀs at 10 and 60). Table 2 shows that our alignment-free SI-pLM, using both\nstructure-paired and unpaired sequences and label validation (for λ), outperformed\nall other alignment-based or alignment-free methods in overall rankin g. As SI-pLM is\nﬁne-tuned over family sequences, its ranking performances were impacted by the level\nof sequence information but still among top 2 and 3 for families of medium and low\nMSA depth, respectively. Even for ranking in the low MSA-depth c ases, it only trailed\nalignment-based MSA-transformer with twice amount of parameters and ﬁne -tuned\nESM-1v with 13-times more parameters. As to classiﬁcation performances, SI-pLM\nalso ranked top 2 and 3 in AUROC and AUPRC, respectively, whereas DeepS equence\ndid the best. By comparing our SI-pLM and each other method through the W ilcoxon\nsigned-rank test (a non-parametric version of the paired t-test), we showed that its\nperformance gains over 7 of the 10 competing methods were statisticall y signiﬁcant\np < 0.05. The performance comparison for each individual variant set was reported in\nFig. 4 and showed that our SI-pLM was top 1 or 2 among the ﬁve best methods for 20\nof the 35 variant sets. Additional performance split over mutation depth was reported\nin Supplemental Table A1.\nThese results show that our SI-pLM, although much smaller compared to c ompeting\npLMs such as ESM-1v and MSA-transformer, has competitive performance rob ustly\nacross ranking and classiﬁcation tasks and across protein families of various sequence\ninformation. Furthermore, we found that it is capable of mitigating over ﬁtting for\naﬀected families often by tuning up λ for more structure awareness (Fig. 3), which\nprovides robustness across protein families that may or may not exper ience overﬁt in\npLM ﬁne-tuning.\n9\nTable 2 Spearman’s rank correlation, AUROC and AUPRC between model scores and experimental\nmeasurements over the ﬁtness benchmark set. Number of param eters is also shown for each model. The top three\nmodels in each column are boldfaced with ranks in superscrip ts. †: sequence pre-trained; ⋆: ﬁne-tuned over target\nprotein’s family sequences; ⋄: ﬁne-tuned over target protein’s family sequences and stru ctures (if available).\nModel type Model name Spearman’s ρ by MSA depth ↑ auroc ↑ auprc ↑ p-value #paramsLow Medium High All All All\nAlign.-based\nPSSM (site-indep) .382 .453 .449 .442 .752 .752 <1e-6 -\nEVMutation (Potts) .445 .514 .520 .505 .782 .798 3e-6 -\nDeepSequence .459 .5473 .5583 .5372 .8041 .8151 0.055 -\nMSA Transformer † .4791 .539 .5622 .5353 .8012 .8112 0.136 100M\nAlign.-free\nUniRep† -.111 -.122 -.202 -.139 .414 .512 <1e-6 18.2M\nWaveNet .460 .527 .551 .523 .792 .805 0.0008 -\nTAPE† .101 .219 .018 .156 .587 .629 <1e-6 38M\nESM-1b† .431 .506 .457 .484 .771 .784 2.1e-5 650M\nESM-1v† .433 .539 .481 .511 .788 .799 0.004 650M\nProtBert-BFD† .378 .479 .487 .466 .763 .777 <1e-6 420M\nESM-1v⋆ .4672 .5561 .497 .530 .797 .8093 0.188 650M\nOur SI-pLM ⋄ .4653 .5502 .5861 .5461 .8012 .8093 - 51M\nFig. 4 Mutation eﬀect prediction on DMS benchmark sets\n2.4 Learned distributions in structural properties enhance\nvariant eﬀect scoring\nOur SI-pLMs not only regularize learning the distributions in protei n sequence (amino\nacid types), but also learn the distributions in protein structur e (structural properties in\nthis study). We assessed the potential of using the structure dis tributions (additionally)\nto rank or classify variant eﬀects (Table 3). Using the learned distributions in structural\nproperties alone, individually or together, did not rank or classify variant eﬀects\nbetter than using the learned distributions in sequence (with s tructural regularization).\n10\nTable 3 Comparison among sequence-based, structure-based and hyb rid\nscoring, using learned log odds in corresponding variable( s), for variant eﬀects\nin Spearman’s ρ, AUROC, and AUPRC. Compared to conventional pLMs\nthat only use learned distributions in sequence (amino acid s or AA), our\nstructure-informed pLMs here could use learned distributi ons in sequence,\nstructural properties (secondary structures or SS, relati ve solvent accessibility\nor RSA, and contact map or CM), and both. Whereas only mutation\npositions are considered by default, versions with subscri pts ‘env’ use all\nneighbor positions forming local environment of mutant pos itions. Boldfaced\nare the best performances.\nType Variable(s) Spearman’s ρ ↑ auroc ↑ auprc ↑\nSequence AA .546 .800 .806\nStructure\n(single\nproperty)\nSS .090 .540 .615\nSSenv .095 .551 .609\nRSA .081 .545 .611\nRSAenv .084 .546 .599\nCM .169 .592 .599\nStructure\n(multi)\nSS+RSA+CM .158 .587 .590\nSSenv+RSAenv+CM .144 .574 .591\nSequence +\nStructure\nAA+SS+RSA+CM .552 .803 .792\nAA+CM .556 .802 .794\nAA+SSenv+RSAenv+CM .554 .799 .791\nHowever, using the learned distributions in structural properti es in addition tothose\nin sequence did improve ranking (in Spearman’s ρ) and classiﬁcation (in AUROC).\nDistributions in contact map (residue–residue edge features) was th e best performer\namong single structural properties (including two residue node f eatures) and also\nenhanced sequence distributions the most in ranking.\n2.5 Ablation study\nTo quantify the contributions of various components of our SI-pLM, we st arted with\nthe Pfam RP15 pre-trained pLM RP15-B2 and incrementally included the following\nsymbolized components in order:\n†: sequence pre-trained only\n⋆: ﬁne-tuned over target protein’s family sequences including ˆS, the subset of sequences\nwith experimental or predicted structures available, and S, the set of all sequences\n(training-set split).\n⋄: ﬁne-tuned over target protein’s family sequences ( ˆS or S) and structures ( ˆT , the\nstructure set corresponding to ˆS.\nLS: select the model with the best zero-shot ﬁtness ranking perform ances based on\nthe label validation set.\nWe reported these ablated versions’ performances again in ranking and c lassiﬁcation\nover the 35 DMS variant sets. We also split the ranking performances i nto families\nof low to high MSA depths ( Neﬀ /L cutoﬀs at 10 and 60) as well as low to high\nstructure availability ( | ˆS|/|S|) ranging from 27% to 95% with cutoﬀs at 50% and\n11\n90%). Table 4 shows increasing performances as model components were increment ally\nincluded, indicating the positive contribution of each. Compared to the pre-trained\npLM† (pfam), ﬁne-tuning (pLM ⋆) greatly improved ranking performances ( ρ increased\nfrom 0.43 to 0.50), especially for families of low to medium MSA depth, as w ell as\nclassiﬁcation performances (AUROC increased from 0.75 to 0.78). Fine-tun ing with\nmore sequences ( S versus ˆS) also helped, especially for families of low to medium\nMSA depth. When structure information was ﬁrst introduced and pair ed sequences\nand structures ( ˆS and ˆT ) were used, structure-informed SI-pLM ⋄ ( ˆS+ ˆT ) improved\nsigniﬁcantly compared to the ﬁne-tuned counterpart without struct ures (pLM ⋆ ( ˆS)),\nwith overall ρ increased from 0.508 to 0.526 and AUROC increased from 0.778 to 0.794.\nThe structure-boosted ranking improvements were more pronounce d when structure\navailability was higher. Additionally using structure-unpaired se quences in SI-pLM ⋄\n(S+ ˆT ) especially helped the families with low structure availabilit y ( ρ increased from\n0.530 to 0.542) but can help the families with high structure availabilit y as well. Lastly,\ncompared to self-supervised validation loss without the need of ﬁtne ss labels, using\nthe supervised ﬁtness validation ( LS) further improved the overall ranking (overall\nρ increased from 0.530 to 0.546, although the improvement was diminished wit hout\nunpaired sequences). This result suggests that self-supervise d learning (ﬁne-tuned) and\nthe downstream variant ﬁtness prediction are not perfectly aligned in objectives and a\nsmall labeled dataset, if available, could help reduce the alignment gap and improving\nthe downstream variant eﬀect prediction. Taken together, other than ﬁ ne-tuning, the\nbiggest contributor was injecting structure awareness and another bo ost was from the\ncombination of supervised label validation and unpaired sequences.\nTable 4 Ablation study of in-house pLMs from pretrained meta model, family sequence\nﬁne-tuned models to structure-informed ﬁne-tuned models. Spearman’s rank correlation\ncoeﬃcient at diﬀerent MSA depth are reported along with over all AUROC and AUPRC values.\nSpearman’s ρ ↑ auroc ↑ auprc ↑\nModel conﬁg. Split by MSA depth Split by struct. % All All All\nLow Medium High Low Medium High\nOur pLM † (pfam) .262 .433 .539 .359 .442 .516 .433 .747 .747\nOur pLM ⋆ ( ˆS) .435 .506 .505 .518 .492 .474 .496 .778 .791\nOur pLM ⋆ (S) .453 .524 .497 .546 .507 .456 .508 .785 .798\nOur SI-pLM ⋄ ( ˆS+ ˆT ) .445 .535 .551 .530 .523 .528 .526 .794 .803\nOur SI-pLM ⋄ (S+ ˆT ) .459 .541 .545 .542 .518 .545 .530 .794 .800\nOur SI-pLM ⋄ ( ˆS+ ˆT + LS) .435 .541 .566 .529 .527 .548 .531 .801 .806\nOur SI-pLM ⋄ (S+ ˆT + LS) .465 .550 .586 .553 .533 .571 .546 .801 .809\n2.6 Understanding the impact of structure regularization on\nsequence embedding\nSince transformer-based pLMs have exhibited capabilities of learni ng informative\nrepresentations for proteins in terms of structures and functions [ 15, 34, 35], we\nare driven to answer the following two questions through examining t he embedding\n12\nmanifolds in pLMs: 1) how ﬁtness landscapes reﬂect in the latent embe dding space\nof pLM, and 2) after injecting structure information, how embedding m anifolds are\naltered relative to sequence-only ones.\nTo answer these questions, we select three protein sets ( UBEX MOUSE and\ntwo BLAT ECOLX studies), where SI-pLMs have obvious gains in ﬁtness ranking\nand classiﬁcation after injecting structure information, and probe th e embedding of\nsequence variants at the last self-attention layer. We investigate t hree models: domain\nsequence pre-trained pLM (pLM †(pfam)), family sequence ﬁne-tuned pLM (pLM ⋆(S))\nand structure-informed ﬁne-tuned pLM (SI-pLM ⋄(S+ ˆT + LS)). The two-dimension\nembedding manifold generated by UMAP are shown in Fig. 5, where each row is a\nprotein case and each column is the set of three models or an individual m odel. In\neach ﬁgure, each dot represents the position of one mutation in the embed ding space\nand its color transparency indicates experimental ﬁtness values (dar ker colors for more\nﬁt variants). As our pLMs were only trained with the residue-level tas k, which is\ndenoising (or recovering) masked residues, we tried two averaging approaches to obtain\nthe sequence-level embedding for each mutant sequence: all-posi tion averaging and\nmutant-position averaging. We ﬁnd that the manifold of mutant-position av eraged\nembeddings better capture target protein’s ﬁtness landscape (Fig. 5) relative to all-\nposition averaged embeddings (data not shown). Our interpretation is that the small\nperturbation in embedding space induced by amino acid substituti ons at few positions\n(most mutations in our DMS datasets are single-site) is largely washed out if their\nembeddings are averaged along with those of all other unchanged positions. R ather,\naveraging residue-wise embeddings only over mutated positions (as i n Fig. 5) provides\nmore sensitivity to our analyses.\nFig. 5 shows that all three types of models lead to certain separation betwee n\nsequence variants of high versus low ﬁtness, which echoes the pre viously observed\neﬀectiveness of pLMs for zero-shot variant eﬀect prediction. Fine-t uning over target\nfamily sequences or sequences & structures led to better separat ion, as quantiﬁed by the\nhigher silhouette coeﬃcients (more compactness within high/low-ﬁ tness clusters and\nmore separation across high/low-ﬁtness clusters). Such better separ ation of embedding\nmanifolds in the latent space makes corresponding model more ready for ﬁ tness\nprediction. Importantly, compared to sequence-only ﬁne-tuned mo dels, our SI-pLMs\nﬁnetuned over target family sequences and structures, had even be tter cluster separation\nwith higher silhouette coeﬃcients, making them ready to better d istinguish variants of\nlow versus high ﬁtness values. Four more protein sets were examined in the Supplemental\nFig. A2 where similar observations were made. These results demonstrate th e impact\nof structure information as a context provider and model regularizer.\n3 Methods\n3.1 Preliminaries\nProteins possess multimodal attributes in the forms of 1D sequences , 2D inter-residue\ndistograms/anglegrams and 3D structures. Amino acids act as the fundamental u nits,\nfollowing sequential constraints to form functional proteins, whic h subsequently fold\n13\nFig. 5 Low-dimension manifolds of variant embeddings by various p LMs (columns) for three rep-\nresentative proteins (rows) indicate that structure infor mation led to better separation of high and\nlow-ﬁtness clusters. Whereas the ﬁrst column is the union of t hree models: sequence pre-trained\nmodel (pLM †(pfam), blue), sequence ﬁne-tuned model (pLM ⋆(S), green), structure-informed model\n(SI-pLM⋄(S + ˆT + LS), red), each of the last three columns corresponds to one of the thr ee models.\nIn each ﬁgure, each point represents a variant, is located at the averaged embedding over mutant posi-\ntions, and is colored according to the experimental ﬁtness v alues (darker for higher ﬁtness, continuous\nin the ﬁrst column and binarized relative to the wild type in t he other three columns).\ninto particular 3D structures. In this paper, we denote protein pri mary sequences as\nSseq and structures as Sstruct.\nSequence-based protein language modeling (pLM) estimates the likel ihood of protein\nsequences ( p(Sseq)) with models trained over large corpora of natural sequences such\nas UniRef and BFD. Masked language modeling (MLM) is one of the major self-\nsupervision frameworks for pLM. It takes a noised sequence with random positions\nmasked and recovers amino acid types at those masked positions, thus mo deling\nthe pseudo log-likelihood (pll) of sequence:\npll(Sseq) = log ∏\n{m} p(S{m}\nseq |S\\{m}\nseq ) =\n∑\n{m} log p(S{m}\nseq |S\\{m}\nseq , where {m} denotes a set of masked positions and \\{m}\ndenotes its complement. Another pLM framework, next token prediction (NTP) recovers\nthe whole sequence token by token through modeling exact likelihoo d ( p(Sseq) =∏ N\ni=1 p(ai|a1..i−1)) following the causal order (left-to-right or right-to-left).\n3.2 Cross-modal masked learning (denoising) framework\nExtending beyond sequence pseudo log-likelihoods, we propose to model the following\ncross-modal pseudo log-likelihood (CMPL):\nCMPL(Sseq, Sstruct) = log p(S{m}\nseq |S\\{m}\nseq ) + log p(Sstruct|S\\{m}\nseq ) (1)\n14\nThe ﬁrst term log p(S{m}\nseq |S\\{m}\nseq ) is exactly aligned with MLM in conventional\nsequence-only protein language models. And the second term, which can be multiplied\nby a weighting hyperparameter λ, recovers protein structures from masked sequences.\nWe represent Sseq by a string of amino-acid type ai over residue i. We used twenty\nnine choices for amino acid type ai, including twenty standard amino acids, two\nspecial amino acids (U and O), three ambiguity letters (X, B, and Z), and four special\ntokens used in language models. And we represent Sstruct by a residue contact graph\nG = {V, E } where nodes in V are amino acids and edges in E represent interactions\namong amino acids (if pairwise Cβ distances are within 8 ˚A). Therefore, structures are\nrepresented by nodes and edges, in other words, structural propert ies. We selected\nsecondary structure si (3 classes) and relative solvent accessibility ri (binarized with\nthe cutoﬀ of 0.25) for nodes, and pairwise Cβ distance dij for edges, which are essential\nattributes for protein function and can be quickly acquired from stru cture ﬁles.\nThe second term can now be factorized over nodes and edges while assum ing their\nindependence:\nlog p(Sstruct|S\\{m}\nseq\n≈ log\n(\np(V Sstruct |a\\{m})p(ESstruct |a\\{m})\n)\n≈ log p(VSS|a\\{m}) + log p(VRSA|a\\{m}) + log p(EDistMap|a\\{m})\n=\nL∑\ni=1\nlog p(si|a\\{m}) +\nL∑\ni=1\nlog p(ri|a\\{m}) +\nL∑\ni=1\nL∑\nj>i\nlog p(dij |a\\{m}))\n(2)\nThe resulting expression provides the foundation for the training losses (objective\nfunctions) in our structure-informed protein language models, whic h is detailed as\nfollows.\n3.3 Structure-informed protein language models\nNext, we describe the neural network parameterization for the CMPL fr amework and\nobjective functions to train the model, as well as the variant scoring method under\nzero-shot transfer setting.\n3.3.1 Model architecture\nThe diagram of our model is shown in Fig. 2. Built upon a BERT-based protein LM,\nwe developed simple decoders to predict variables in two modali ties employing amino\nacid embeddings and attention scores from pLM. We accommodated the dim ensionality\nrequirement in the 2D contact-map prediction with a outer product m odule.\nProtein LM\nWe applied a BERT-based architecture and pre-trained the model ov er domain sequences\nfrom Pfam. BERT makes use of the transformer encoder, an attention mech anism\nthat learns contextual relations between entities in input data. To b e more speciﬁc\non protein sequence inputs, attention enables each amino acid select ively attending\nto other positions for tailored information aggregation to fulﬁl prediction t asks. Since\n15\nattentions are conducted within positions of a single protein, this is further called\nself-attention, to be distinguished from cross-attention. Multip le self-attention blocks\n(termed heads) are utilized to assemble a self-attention layer, an d multiple self-attention\nlayers forming the whole model. Position-wise feed-forward modul e is attached after\nmulti-head self-attention module in each layer to let embeddings being updated before\nentering the next layer. At layer i, the multi-head self-attention is:\nMultiHeadSelfAttention(Xq, Xk, Xv) = [head 1| . . . |headH ]Wo\nwhere head h = Attention(XqWq\nh, XkWk\nh, XvWv\nh)\n= softmax( (XqWq\nh)(XkWk\nh)T\n√dk\n)(XvWv\nh)\n(3)\nwhere Xq = Xk = Xv = Xi ∈ RL×s in self-attention, and [ .|.] is a concatenation oper-\nation. Wq\nh, Wk\nh ∈ Rs×dk/H , Wv\nh ∈ Rs×dv /H are weight matrices to transform input\nfeatures X into query, key and value matrices. Wo ∈ Rdv ×s linearly transforms concate-\nnated output features from multi-head self-attention. An ”add & norm” ope ration is\napplied afterwards Xa = LayerNorm(MultiHeadSelfAttention(Xq, Xk, Xv)+X; γa, βa).\nThe position-wise feed-forward module is:\nXf = GELU( XaW1)W2\nXi+1 = LN( Xf + Xs; γf , βf ) (4)\nThe Gaussian Error Linear Units (GELU) [ 36] activation function and layer\nnormalization (LN) [ 37] are used to reduce overﬁtting.\nWe have tested ﬁve BERT-based encoders with increasing trainin g data and model\ncomplexity as follows:\nTable 5 Architecture diﬀerences among ﬁve BERT-based encoders\nEncoder name Pre-training Database\n(# examples)\n# Layers # Heads Self-atten. hidden Feed-forward hidden\nRP15 B4 pfam rp15 (12M) 4 8 768 1024\nRP15 B3 ... 4 8 768 3072\nRP15 B2 ... 6 12 768 3072\nRP15 B1 ... 12 12 768 3072\nRP75 B1 pfam rp75 (68M) 12 12 768 3072\nOuter product module\nIn order to apply amino acid features from pLM for 2D contact-map prediction , we\ndesigned an outer product module to convert positional embeddings to pairwise ones.\nfor any pair of amino acid i, j and their embeddings from the last self-attention layer\nin pLM xL\ni ,\nxL\nj ∈ Rs\n, the embedding dimension is ﬁrstly reduced for left and right\n16\nvectors, then take the outer product: Xij = xL\ni\nWl ⊗ xL\nj\nWr, where Wl, Wr ∈ Rs×dm\nare left and right linear transformation matrices. The outer product mat rix Xij ∈\nRdm×dm is then ﬂattened and linearly transformed: xpair\nij\n= ﬂatten(Xij )Ws, where\nWs ∈ R(dm)2×ds , xpair\nij ∈ Rds\n. This pairwise feature map is later concatenated with\nattention matrices from all self-attention heads along the feature dime nsion.\nLabel prediction heads\nSince transformer encoder-based protein language models have been d emonstrated\neﬀective to learn informative protein representations, we only cons idered very simple\ndecoders to predict sequence and structure self-supervision labels. The positional feature\n( ˜Xpos ∈ RL×s) or pairwise feature ˜Xpair ∈ RL×L×(da+ds) pass through individual\ntwo-layer MLPs to generate class logits for each label: ˆy = LN(GELU( ˜XWD\n1 ))WD\n2 ,\nwhere sizes of WD\n1 and WD\n2 are label-dependent, and ˆy ∈ RC , C as number of\nclasses for each label. We note that pairwise feature map is symmetrize d ( ˜Xpair =\n1\n2 (Xpair + Xpair⊤\n)) before feed into the MLP, relieving the model from learning the\nsymmetry in contact-maps.\n3.3.2 Training objectives\nThe softmax function is applied on class logits to generate a probability d istribution p\nover classes. Following Eq. 1 and 2, the overall training objective is L = Laa + λ(Lss +\nLrsa + Lcm), where λ is the hyper-parameter regulating the strength of structure tasks.\nLaa = EDseq\n[\n1\n|{m}|\n∑\nˆa∈a{ m}\n−log(paa(ˆa))\n]\nLss = EDstruct\n[\n1\nL\n∑ L\ni=1 −log(pss(si))\n]\nLrsa = EDstruct\n[\n1\nL\n∑ L\ni=1 −log(prsa(ri))\n]\nLcm = EDstruct\n[\n1\n|{ij}|\n∑ L\ni=1\n∑ L\nj>i −log(pcm(dij ))\n]\n(5)\nAs each term is calculated over randomly sampled data points in each batch , there may\nbe batches which only contain structure-absent sequences when augm ented sequence\ndata is used. For these cases where size of Dstruct is zero, all three structure-property\nloss terms are set to 0 in such a batch.\n3.3.3 Variant eﬀect scoring\nSequence-based methods is built on the modeling goal that a well trai ned network over\nnatural sequences in target MSA should learn evolutionary constraints am ong residues\nunderpinning function-favored sequences. In other words, compar ed to the probability\nthat a model assigns to the wild-type amino acid, higher probability s hould be given\nto amino acids with better functional propensity than wild-types an d vice versa. We\nmask all variant positions and score each single-site variant ( M = {i}) by the log odds\nbetween the variant and wild-type amino acids, which is equivalent to log\np(amut\ni |a\\ M )\np(awt\ni |a\\ M )\n.\n17\nFor multi-site mutations ( |M| > 1), the additive assumption is used and the above\nscore is summed over i ∈ M.\nOur structure-informed pLMs provide more options to score variant eﬀ ects. First,\nwe could use the same expression of log odds in amino-acid (AA) types but n ote\nthat the learned odds are regularized by structural information. Secon d, we could use\nadditional log odds in structural properties, including secondary s tructure (SS) classes,\nrelative solvent accessibility (RSA) classes, and contact-map (CM) d istance bins, or\nthe sum of the log odds in all three structural properties. We also con sider the log\nodds not only for the mutation site but for its local environment (any re sidues within\n8˚A in Cβ distances. Lastly, we could use the sum of log odds in both amino acid typ es\n(sequence) and structural properties.\n3.4 Datasets\nSequence dataset\nThe sequence data to pre-train pLMs are non-redundant domain sequenc es of represen-\ntative proteomes (RP) downloaded from Pfam. Representative proteomes are groupings\nof similar proteomes, whereby a single proteome is chosen to best repr esent the set of\ngrouped proteomes in terms of both sequence and annotation information [ 38]. The\ngrouping redundancy is controlled by the co-membership threshold ( at four levels, 75,\n55, 35 and 15%) that lower value produces larger groupings, hence resulting in less\nredundant sequence sets. We used the RP15 set as pretraining corpu s since it is the\nmost cost-eﬃcient one. We also trained a version of our largest pLM with th e RP75 set.\nThe alignments were removed from original downloaded MSA ﬁles to train on primary\nsequences.\nThe family ﬁne-tuning sequence dataset was downloaded from Wavenet [13]. Each\nprotein comes with a MSA ﬁle containing homologous sequences queried f rom UniRef100\ndatabase. An identity-based weighting score is attached to each sequen ce that sequences\nwith lower identity among homolog population have higher values. We dir ect readers to\nDeepSequence [ 11] for details of weight calculation. The ﬁne-tuning was also conducted\nover unaligned primary sequences. Diﬀerent from pre-training, tr aining samples were\nre-weighted according to their weighting scores.\nStructure dataset\nAs we are dealing with protein domain sequences, a Uniprot accession num ber together\nwith a pair of start and end indices can uniquely deﬁne one sequence d ata in our\ndataset. We queried RCSB Protein Data Bank (PDB) and AlphaFold Protein S tructure\nDatabase (AlphaFold DB) for available structures to family sequences. To collect\ncrystal structures from RCSB PDB, we ﬁrst curated a set of 100% non-red undant\nprotein polymer entities for Uniprot accession number candidate set. As one protein\npolymer entity may have multiple instances (e.g. chains in homopoly mer proteins), we\nkept the polymer instance with the longest coverage and least unobserv ed residues. At\nthis point, we ended up with a set of structure instances with no i dentical sequences to\neach other. Then we excluded structures having no overlapping w ith target sequence by\ninspecting start and end indices. This pipeline was conducted ov er each family to collect\nits non-redundant structure dataset. We note that partial sequences have no structure\n18\ndata since we didn’t further run AlphaFold2 inference if no struc tures can be found\non AFDB. The structure ﬁles in mmCIF format were downloaded for calcul ation of\nsecondary structure and relative solvent accessibility using Biop ython’s DSSP module.\nCβ distance-maps were manually generated and transformed to contact-maps with the\ncutoﬀ of 8 ˚\nA.\nMutation eﬀect datasetsThe wildly used mutation ﬁtness benchmark set was downloaded from De epSe-\nquence [ 11] with one tRNA set excluded as we are handling protein mutations. We\nonly considered missense mutations in all experiments. Since str ucture availability is\nextremely low over viral families, ﬁtness sets of viral proteins w ere also not consid-\nered in this study. The predicted mutation ﬁtness scores of all comp eting methods we\ncompared to were acquired from ESM-1v’s github repo.\nTo further mitigate the discrepancy between self-supervision t asks and ﬁtness\nprediction, we hope to directly use label information to select the most ﬁtness-sensitive\nmodel for inference. A small yet representative mutation subset i s selected that one\nmutation, including multi-site mutations, is randomly picked at eac h mutant position\nwithout replacement.\n3.5 Competing methods\nWe focus on the main sequence modeling approaches described in the introduction,\nincluding a number of protein-speciﬁc methods: PSSM (site-in dependent) model,\nEVmutation (Potts model) [ 10], DeepSequence [ 11], which were all trained on aligned\nsequences, and Wavenet [ 13], trained over unaligned sequences with autoregressive\ngenerative models. We also include representative protein language m odels trained\nacross protein families that leverage alignments during training, such as the MSA\nTransformer [18] or that are alignment-free, such as UniRep [ 7], TAPE [ 8], ESM-1b [ 15],\nESM-1v [ 19] and ProtBert-BFD [ 17]. Our SI-pLM is alignment-free with additional\nstructure modeling (see comparison with other pLMs as follows).\nTable 6 Comparison between our protein language models and existin g representative ones\nModel name Network (# params) Input Pre-training Database ( # examples) Tasks\nUniRep multiplicative LSTM (18.2M) Sequence UniRef50 (24M ) NTP\nTAPE Transformer (38M) ResNet; LSTM Sequence Pfam (32M) NTP, MLM\nESM-1b Transformer (650M) Sequence UniRef50 2018 03 (27M) MLM\nProtBERT-BFD Transformer (420M) Sequence BFD (2122M) MLM\nMSA-transformer Axial transformer (100M) MSA UniRef50\n2018 03 (26M MSAs) MLM\nESM-1v Transformer (650M) Sequence UniRef90 2020 03 (98M) MLM\nTranception k-mer Transformer (700M) Sequence UniRef100 ( 249M) NTP\nOur pLM Transformer (B1:92M B2:50M\nB3:36M B4:23M)\nSequence Pfam rp15 (12M); Pfam rp75 (68M) MLM\n19\n4 Conclusion\nIn this study, we take the perspective of assessing and injecting structural contexts\ninto protein language models toward variant eﬀect predictors. We found that, although\ncommonly-used sequence ﬁne-tuning may improve sequence and st ructure awareness\ntoward better variant eﬀect prediction, over-ﬁnetuning could oc cur; and the balance\nbetween sequence and structure awareness needs to be purposefu l and controllable.\nExtending the sequence-only masked language modeling, we introduc e a framework\nof cross-modality masked learning for purposeful and controllable inje ction of structure\nawareness into protein language models. This framework is agnostic to pr otein language\nmodels (pLMs) in the sense that it can modify the architecture of any existing\ntransformer-based pLMs with structural decoders and the training losses of any existing\ntransformer-based pLMs through auxiliary sequence-to-structure de noising tasks. This\nframework does not demand protein structure data as additional inputs t o protein\nsequences during inference (and variant eﬀect prediction), whi le utilizing both sequences\npaired with available structures and unpaired sequences without s tructures (no multiple\nsequence alignments are needed either).\nNumerical results over benchmarks for variant eﬀect prediction ind icate that,\nwhereas our SI-pLMs are compact in model size compared to competing langu age\nmodels, they are consistently top performers regardless of the prote in family being\nevolutionary information-rich/poor or being prone to over-ﬁnetuning. Learned distri-\nbutions in structural contexts not only regularize those in sequence but could also\nenhance the latter’s variant scoring performances. Ablation studies revealed major\ncontributors of the numerical performances, whereas visualization of t he latent embed-\ndings showed that structure information led to better separation of lo w/high-ﬁtness\nsequence clusters and better readiness for zero-shot variant eﬀect prediction.\nAcknowledgments. This project was in part supported by the National Institute\nof General Medical Sciences (R35GM124952 to Y.S.). Portions of this research were\nconducted with the advanced computing resources provided by Texas A&M High\nPerformance Research Computing.\nReferences\n[1] Esposito, D., Weile, J., Shendure, J., Starita, L.M., Papenfuss , A.T., Roth, F.P.,\nFowler, D.M., Rubin, A.F.: Mavedb: an open-source platform to dist ribute and\ninterpret data from multiplexed assays of variant eﬀect. Genome biol ogy 20(1),\n1–11 (2019)\n[2] Rubin, A.F., Min, J.K., Rollins, N.J., Da, E.Y., Esposito, D., Harringt on, M., Stone,\nJ., Bianchi, A.H., Dias, M., Frazer, J., et al.: Mavedb v2: a curated com munity\ndatabase with over three million variant eﬀects from multiplexed fu nctional assays.\nbioRxiv, 2021–11 (2021)\n[3] Raimondi, D., Tanyalcin, I., Fert´ e, J., Gazzo, A., Orlando, G., Lenaert s, T.,\nRooman, M., Vranken, W.: Deogen2: prediction and interactive visualiz ation of\n20\nsingle amino acid variant deleteriousness in human proteins. Nucleic acids research\n45(W1), 201–206 (2017)\n[4] Adzhubei, I., Jordan, D.M., Sunyaev, S.R.: Predicting functional eﬀect of human\nmissense mutations using polyphen-2. Current protocols in human gen etics 76(1),\n7–20 (2013)\n[5] Pejaver, V., Urresti, J., Lugo-Martinez, J., Pagel, K.A., Lin, G.N., Nam, H. -\nJ., Mort, M., Cooper, D.N., Sebat, J., Iakoucheva, L.M., et al.: Inferring the\nmolecular and phenotypic impact of amino acid variants with mutpred2. Natu re\ncommunications 11(1), 5918 (2020)\n[6] Sundaram, L., Gao, H., Padigepati, S.R., McRae, J.F., Li, Y., Kosmicki, J.A. ,\nFritzilas, N., Hakenberg, J., Dutta, A., Shon, J., et al.: Predicting the clinical\nimpact of human mutation with deep neural networks. Nature genetics 50(8),\n1161–1170 (2018)\n[7] Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., Church, G. M.: Uniﬁed\nrational protein engineering with sequence-based deep represent ation learning.\nNature methods 16(12), 1315–1322 (2019)\n[8] Rao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen, P., Canny, J., Abbeel,\nP., Song, Y.: Evaluating protein transfer learning with tape. Advances i n neural\ninformation processing systems 32 (2019)\n[9] Ng, P.C., Henikoﬀ, S.: Sift: Predicting amino acid changes that aﬀect pr otein\nfunction. Nucleic acids research 31(13), 3812–3814 (2003)\n[10] Hopf, T.A., Ingraham, J.B., Poelwijk, F.J., Sch¨ arfe, C.P., Springe r, M., Sander,\nC., Marks, D.S.: Mutation eﬀects predicted from sequence co-variat ion. Nature\nbiotechnology 35(2), 128–135 (2017)\n[11] Riesselman, A.J., Ingraham, J.B., Marks, D.S.: Deep generative mode ls of genetic\nvariation capture the eﬀects of mutations. Nature methods 15(10), 816–822 (2018)\n[12] Frazer, J., Notin, P., Dias, M., Gomez, A., Min, J.K., Brock, K., Gal, Y., Marks,\nD.S.: Disease variant prediction with deep generative models of evol utionary data.\nNature 599(7883), 91–95 (2021)\n[13] Shin, J.-E., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E ., Sander, C.,\nManglik, A., Kruse, A.C., Marks, D.S.: Protein design and variant predi ction using\nautoregressive generative models. Nature communications 12(1), 2403 (2021)\n[14] Heinzinger, M., Elnaggar, A., Wang, Y., Dallago, C., Nechaev, D., Matthes, F.,\nRost, B.: Modeling aspects of the language of life through transfer-learn ing protein\nsequences. BMC bioinformatics 20(1), 1–17 (2019)\n21\n[15] Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D. , Ott, M.,\nZitnick, C.L., Ma, J., Fergus, R.: Biological structure and function e merge from\nscaling unsupervised learning to 250 million protein sequences. b ioRxiv (2019)\nhttps://doi.org/10.1101/622803\n[16] Madani, A., Krause, B., Greene, E.R., Subramanian, S., Mohr, B.P., Holt on, J.M.,\nOlmos Jr, J.L., Xiong, C., Sun, Z.Z., Socher, R., et al.: Large language mode ls\ngenerate functional protein sequences across diverse families. Natur e Biotechnology,\n1–8 (2023)\n[17] Elnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G., Wang, Y., Jones, L., Gib bs,\nT., Feher, T., Angerer, C., Steinegger, M., et al.: Prottrans: Toward understanding\nthe language of life through self-supervised learning. IEEE transaction s on pattern\nanalysis and machine intelligence 44(10), 7112–7127 (2021)\n[18] Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., S ercu, T., Rives,\nA.: Msa transformer. bioRxiv (2021) https://doi.org/10.1101/2021.02.12.430858\n[19] Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., Rives, A.: Language models enable\nzero-shot prediction of the eﬀects of mutations on protein function. Adv ances in\nNeural Information Processing Systems 34, 29287–29303 (2021)\n[20] Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkui l, R.,\nKabeli, O., Shmueli, Y., et al.: Evolutionary-scale prediction of atomic-level protein\nstructure with a language model. Science 379(6637), 1123–1130 (2023)\n[21] Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A., Marks, D .S.,\nGal, Y.: Tranception: protein ﬁtness prediction with autoregressive transformers\nand inference-time retrieval (2022) https://doi.org/10.48550/arxiv.2205.13760\n[22] Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Erckert, K., B ernhofer,\nM., Nechaev, D., Rost, B.: Embeddings from protein language models pred ict\nconservation and variant eﬀects. Human genetics 141(10), 1629–1647 (2022)\n[23] Wang, S., You, R., Liu, Y., Xiong, Y., Zhu, S.: Netgo 3.0: Protein language model\nimproves large-scale functional annotations. bioRxiv, 2022–1205519073 (2022)\nhttps://doi.org/10.1101/2022.12.05.519073\n[24] Brandes, N., Ofer, D., Peleg, Y., Rappoport, N., Linial, M.: ProteinBERT : A\nUniversal Deep-learning Model of Protein Sequence and Function. https://www.\nbiorxiv.org/content/10.1101/2021.05.24.445464v1\n[25] Biswas, S., Khimulya, G., Alley, E.C., Esvelt, K.M., Church, G. M.: Low-n protein\nengineering with data-eﬃcient deep learning. Nature methods 18(4), 389–396\n(2021)\n[26] Nijkamp, E., Ruﬀolo, J., Weinstein, E.N., Naik, N., Madani, A.: Progen2: exp loring\n22\nthe boundaries of protein language models. arXiv preprint arXiv:2206.13517 (2022)\n[27] Shen, Y.: Predicting protein structure from single sequences. Nat ure Computa-\ntional Science 2(12), 775–776 (2022)\n[28] Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronn eberger, O.,\nTunyasuvunakool, K., Bates, R., ˇZ´ ıdek, A., Potapenko, A.,et al.: Highly accurate\nprotein structure prediction with alphafold. Nature 596(7873), 583–589 (2021)\n[29] Baek, M., DiMaio, F., Anishchenko, I., Dauparas, J., Ovchinnikov, S. , Lee, G.R.,\nWang, J., Cong, Q., Kinch, L.N., Schaeﬀer, R.D., et al.: Accurate prediction of\nprotein structures and interactions using a three-track neural ne twork. Science\n373(6557), 871–876 (2021)\n[30] Buel, G.R., Walters, K.J.: Can alphafold2 predict the impact of miss ense mutations\non structure? Nature Structural & Molecular Biology 29(1), 1–2 (2022)\n[31] Bepler, T., Berger, B.: Learning the protein language: Evolution, str ucture, and\nfunction. Cell systems 12(6), 654–669 (2021)\n[32] Zhang, Z., Xu, M., Chenthamarakshan, V., Lozano, A., Das, P., Tang, J.: Enhanc-\ning protein language models with structure-based encoder and pre-t raining. arXiv\npreprint arXiv:2303.06275 (2023)\n[33] Zheng, Z., Deng, Y., Xue, D., Zhou, Y., Ye, F., Gu, Q.: Structure-inf ormed\nlanguage models are protein designers. bioRxiv, 2023–02 (2023)\n[34] Vig, J., Madani, A., Varshney, L.R., Xiong, C., Socher, R., Rajani, N.F.: Bertology\nmeets biology: interpreting attention in protein language models. arXiv preprint\narXiv:2006.15222 (2020)\n[35] Rao, R., Meier, J., Sercu, T., Ovchinnikov, S., Rives, A.: Transf ormer protein\nlanguage models are unsupervised structure learners. Biorxiv, 2020–12 (2020)\n[36] Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXi v preprint\narXiv:1606.08415 (2016)\n[37] Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint\narXiv:1607.06450 (2016)\n[38] Chen, C., Natale, D.A., Finn, R.D., Huang, H., Zhang, J., Wu, C.H., Mazumde r,\nR.: Representative proteomes: a stable, scalable and unbiased proteom e set for\nsequence analysis and functional annotation. PloS one 6(4), 18910 (2011)\n23\nAppendix A Supplemental results\nA.1\nExtended results for ﬁtness-awareness correlation analysis\nFig. A1 The ﬁve BERT-based models, when applied to diﬀerent protein s, exhibit diﬀerent trade-oﬀs\nbetween sequence semantics and structure awareness for the ir sensitivity towards ﬁtness prediction.\n24\nA.2 Results grouped by mutation depth\nTable A1 Spearman’s rank correlation, AUROC and AUPRC between model scores and experimental\nmeasurements grouped by mutation depth (1 to 28) for HIS7 YEAST. Top-3 performances are with the rank\nin superscripts 1–3.\nModel type Model name Spearman’s ρ by mutation depth ↑ auroc ↑ auprc ↑\n1 2 3 4 5 6-10 10+ All All All\nAlign based\nmodels\nPSSM (site-indep) .059 .197 .295 .349 .375 .448 .642 .460 .75 9 .841\nEVMutation (Potts) .084 .187 .331 .414 .481 .5981 .752 .5861 .84182 .91212\nDeepSequence .116 .160 .293 .367 .421 .537 .7603 .540 .810 .888\nWaveNet .109 .208 .3772 .4462 .4932 .567 .748 .5737 .832 .908\nMSA Transformer † .111 .2222 .365 .432 .468 .394 .505 .302 .685 .811\nNon-align\nbased\nmodels\nUniRep† -.055 .062 .020 .002 -.047 -.205 -.190 -.182 .382 .586\nTAPE† .083 .013 .027 .006 .005 .110 .410 .101 .561 .718\nESM-1b† .026 .209 .328 .380 .398 .486 .750 .500 .788 .888\nESM-1v† .096 .206 .323 .380 .417 .518 .7771 .529 .807 .896\nProtBert-BFD† .046 .194 .300 .378 .435 .554 .752 .553 .811 .892\nESM-1v⋆ .084 .2203 .350 .417 .457 .554 .7692 .560 .832 .9103\nOur pLM † .1471 .160 .231 .271 .296 .381 .641 .391 .717 .824\nOur pLM ⋆ ( ˆS) .102 .210 .368 .4393 .4823 .531 .675 .541 .823 .903\nOur pLM ⋆ (S) .1352 .218 .3791 .4571 .5091 .571 .665 .57443 .84221 .91251\nOur SI-pLM ⋄ ( ˆS+ ˆT ) .111 .216 .352 .422 .471 .5773 .674 .5736 .8403 .908\nOur SI-pLM ⋄ (S+ ˆT ) .1273 .219 .357 .432 .480 .5812 .698 .5802 .838 .909\nOur SI-pLM ⋄ (S+ ˆT ,LS) .104 .2481 .3693 .432 .464 .559 .720 .561 .836 .908\n25\nA.3 Embedding analysis\nFig. A2 Low-dimension manifold of mutant embedding space for four p roteins (correspondng to\nfour rows). Each point represents the averaged embedding ov er mutated positions in one variant\nsequence, and the color transparency indicates experiment al ﬁtness values that darker points are more\nﬁt mutations. The original embedding dimension is 768 and, r educed to 2 by UMAP. Embeddings\nfrom outputs of the last self-attention layer are probed her e, and three models (corresponding to the\nlast 3 columns) are compared together: sequence pre-traine d model (pLM †(pfam), blue), sequence\nﬁne-tuned model (pLM ⋆(S), green), and structure-informed model (SI-pLM ⋄(S + ˆT + LS), red)\n26",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.7604872584342957
    },
    {
      "name": "Computer science",
      "score": 0.6454232931137085
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5412447452545166
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5404215455055237
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5091611742973328
    },
    {
      "name": "Machine learning",
      "score": 0.4714570641517639
    },
    {
      "name": "Language model",
      "score": 0.4115701913833618
    },
    {
      "name": "Artificial neural network",
      "score": 0.195257306098938
    },
    {
      "name": "Biology",
      "score": 0.1428205668926239
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I91045830",
      "name": "Texas A&M University",
      "country": "US"
    }
  ]
}