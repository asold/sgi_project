{
  "title": "LLMs to the Moon? Reddit Market Sentiment Analysis with Large Language Models",
  "url": "https://openalex.org/W4367309853",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2101262251",
      "name": "Xiang Deng",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2222889463",
      "name": "Vasilisa Bashlovkina",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2037704425",
      "name": "Feng Han",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2123232829",
      "name": "Simon Baumg√§rtner",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2112702096",
      "name": "Michael Bendersky",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2962788902",
    "https://openalex.org/W3034368386",
    "https://openalex.org/W2616763096",
    "https://openalex.org/W4281975731",
    "https://openalex.org/W3035101152",
    "https://openalex.org/W3125952890",
    "https://openalex.org/W2625464253",
    "https://openalex.org/W2798658104",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4206908526",
    "https://openalex.org/W3138154797",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Market sentiment analysis on social media content requires knowledge of both financial markets and social media jargon, which makes it a challenging task for human raters. The resulting lack of high-quality labeled data stands in the way of conventional supervised learning methods. In this work, we conduct a case study approaching this problem with semi-supervised learning using a large language model (LLM). We select Reddit as the target social media platform due to its broad coverage of topics and content types. Our pipeline first generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production. We find that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels, while training the student model using a regression loss further improves distillation quality. With only a handful of prompts, the final model performs on par with existing supervised models. Though production applications of our model are limited by ethical considerations, the model's competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5613172054290771
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5317199230194092
    },
    {
      "name": "Astrobiology",
      "score": 0.4077087342739105
    },
    {
      "name": "Data science",
      "score": 0.35965317487716675
    },
    {
      "name": "Natural language processing",
      "score": 0.27994978427886963
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}