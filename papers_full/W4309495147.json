{
  "title": "TC-Net: Dual coding network of Transformer and CNN for skin lesion segmentation",
  "url": "https://openalex.org/W4309495147",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5073948811",
      "name": "Yuying Dong",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5081489939",
      "name": "Liejun Wang",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5100427622",
      "name": "Yongming Li",
      "affiliations": [
        "Xinjiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3202285299",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W2148743296",
    "https://openalex.org/W2607363228",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2118386984",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W3081752372",
    "https://openalex.org/W6600007113",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W3208002538",
    "https://openalex.org/W3171031465",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6600213211",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W2074317748",
    "https://openalex.org/W2963946669",
    "https://openalex.org/W2794825826",
    "https://openalex.org/W2905338897",
    "https://openalex.org/W2921406441",
    "https://openalex.org/W3161081823",
    "https://openalex.org/W6608687280",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3173693036",
    "https://openalex.org/W4226462882",
    "https://openalex.org/W3090920939",
    "https://openalex.org/W3102785203",
    "https://openalex.org/W3103010481",
    "https://openalex.org/W2618530766"
  ],
  "abstract": "Skin lesion segmentation has become an essential recent direction in machine learning for medical applications. In a deep learning segmentation network, the convolutional neural network (CNN) uses convolution to capture local information for modeling. However, it ignores the relationship between pixels and still can not meet the precise segmentation requirements of some complex low contrast datasets. Transformer performs well in modeling global feature information, but their ability to extract fine-grained local feature patterns is weak. In this work, The dual coding fusion network architecture Transformer and CNN (TC-Net), as an architecture that can more accurately combine local feature information and global feature information, can improve the segmentation performance of skin images. The results of this work demonstrate that the combination of CNN and Transformer brings very significant improvement in global segmentation performance and allows outperformance as compared to the pure single network model. The experimental results and visual analysis of these three datasets quantitatively and qualitatively illustrate the robustness of TC-Net. Compared with Swin UNet, on the ISIC2018 dataset, it has increased by 2.46% in the dice index and about 4% in the JA index. On the ISBI2017 dataset, the dice and JA indices rose by about 4%.",
  "full_text": "RESEA RCH ARTICL E\nTC-Net: Dual coding network of Transformer\nand CNN for skin lesion segmentation\nYuying Dong, Liejun Wang\nID\n*, Yongming Li\nCollege of Informatio n Science and Engineerin g, Xinjiang University, Urumqi, China\n* wljxju@ xju.edu.cn\nAbstract\nSkin lesion segmentation has become an essential recent direction in machine learning for\nmedical applications. In a deep learning segmentation network, the convolutional neural net-\nwork (CNN) uses convolution to capture local information for modeling. However, it ignores\nthe relationship between pixels and still can not meet the precise segmentation require-\nments of some complex low contrast datasets. Transformer performs well in modeling global\nfeature information, but their ability to extract fine-grained local feature patterns is weak. In\nthis work, The dual coding fusion network architecture Transformer and CNN (TC-Net), as\nan architecture that can more accurately combine local feature information and global fea-\nture information, can improve the segmentation performance of skin images. The results\nof this work demonstrate that the combination of CNN and Transformer brings very signifi-\ncant improvement in global segmentation performanc e and allows outperformance as com-\npared to the pure single network model. The experimental results and visual analysis of\nthese three datasets quantitatively and qualitatively illustrate the robustness of TC-Net.\nCompared with Swin UNet, on the ISIC2018 dataset, it has increased by 2.46% in the dice\nindex and about 4% in the JA index. On the ISBI2017 dataset, the dice and JA indices rose\nby about 4%.\nIntroduction\nWith the rapid development of AI, approaches that integrate AI with the medical field are also\nflowering everywhere in the medical field. Among them, the combination of medical imaging\ndiagnosis and deep learning is not only a newer branch of intelligent medical diagnosis, but\nalso a hot spot in the digital medical industry. Medical imaging contains massive amounts of\ndata, and even experienced physicians sometimes appear disadvantageous. Artificial diagnosis\nof medical images requires long-term professional experience and relatively long professional\ntraining. At the same time, AI can do more rapidly than expert physicians in both detection\nefficiency and precision of images, and it can also reduce the false positive rate of human\nmanipulation.\nMedical image segmentation occupies a key position in the intelligent diagnosis and analy-\nsis of medical images. It plays a vital role in computer-aided clinical diagnostic systems. Its\nfunction is to segment essential parts of medical images (such as lesion parts or organ parts)\nPLOS ONE\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 1 / 18\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Dong Y, Wang L, Li Y (2022) TC-Net:\nDual coding network of Transfo rmer and CNN for\nskin lesion segmentati on. PLoS ONE 17(11):\ne0277578. https://d oi.org/10.1371/j ournal.\npone.027757 8\nEditor: Jyotismita Chaki, Vellore Institute of\nTechnology : VIT University, INDIA\nReceived: June 15, 2022\nAccepted: October 29, 2022\nPublished: November 21, 2022\nCopyright: © 2022 Dong et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: We used two\nclassical dermosc opy datasets to evaluate the\nproposed segmentatio n network. They are\nrespective ly ISIC2018 dataset , ISBI2017 dataset\nand ISBI2016 dataset . The url of these datasets is\nhttps://chall enge.isic-arc hive.com/d ata.\nFunding: This research was funded by the National\nScience Foundation of China under Grant\nU1903213, and the Natural Science Foundation of\nXinjiang Uygur Autonomous Region grant number\n2022D01C82 . The funders had no role in study\nthrough in-depth learning supervision or unsupervised. To provide a reliable basis and help\nfor doctors in clinical medical diagnosis. With the gradual popularization and application of\nintelligence, medical image diagnosis also faces the transformation to intelligent medicine.\nTherefore, improving the accuracy of medical image processing will become an essential direc-\ntion of the development of medical image processing. Medical image segmentation is an\nimportant and challenging stage for clinical medical diagnosis. Common medical image seg-\nmentation includes polyp segmentation [1], lesion segmentation [2], cell segmentation [3], etc.\nThis paper mainly studies the segmentation of skin lesions [4].\nBecause skin lesion images have variable resolution and an uneven proportion of skin\nlesions included in the images, critical information on the location of skin lesions is difficult to\nobtain. Skin lesions images are usually rarely directly processed and often require pre-process-\ning of images (cropping, spinning, normalization). Nowadays, it has become an urgent need\nfor AI medical diagnosis to continuously improve the segmentation accuracy of lesion parts in\nmedical diagnosis. But the excessive waste of computer resources can be generated in process-\ning and training, hindering the application of smart medical in real life. Therefore, the research\nfocus of this study is to comprehensively utilize the model’s global and local full-type features\nto improve the model’s feature extraction ability without preprocessing the datasets.\nThis paper investigates the application potential of transformer network in dermatological\nfocus segmentation. Interestingly, when this paper is tested on skin lesion datasets using the\ntransformer model, which has achieved significant results so far, The results show that the\npure transformer network model can not obtain satisfactory results in the field of skin lesions\nsegmentation. Because during the process of entering dermatological image pictures into the\ntransformer network in the transformer network coding phase, these images were compressed\ninto one-dimensional sequences. The operation of batch sequence processing damages the\nstructure information in the picture, and can not make good use of its complete structure\ninformation in the decoding stage. It will eventually lead to a less satisfactory network model\nsegmentation. Inspired by the CNN network, in this paper, we capture the feature context\ninformation and spatial feature information of images at various stages through series opera-\ntions such as convolution. Then try to seek an algorithm that can fuse the local feature infor-\nmation of feature maps and the global feature information of networks.\nTherefore, this paper proposes a skin disease segmentation model TC-Net. TC-Net adopts\nthe architecture of Swin Transformer combined with CNN. TC-Net combines Swin Trans-\nformer with CNN using a double coding structure. The Swin Transformer branch mainly takes\na self-attention approach and adds a sliding window form to acquire feature information. CNN\nbranches operate detailed local information through convolutional series. In this paper, the\nbackbone of the CNN branch network adopts Resnet34 with the pre-training model, and the\ntransformer branch network selects Swin Transform architecture. They work together to obtain\nthe feature information of skin lesions images with different feature degrees. Meanwhile, the\nstructural design of dual encoder fusion enables the model to obtain more extensive feature\ninformation. As TC-Net models continue to be profoundly and widely acquired, the perceptual\ndomain of the models also increases. There are some main contribution points as follows:\n1. Firstly, a U-shaped network framework TC-Net with a dual encoding structure is designed.\nTC-Net uses Swin Transformer and CNN as two encoder branches. The combination of\nthe two encoders enables the simultaneous acquisition of global and local information of\nthe input image, while richer feature information is input from the encoder part to the\ndecoder part.\n2. Secondly, TC-Net proposes a fusion module of CNN and Transformer, which is used to\nfuse the local information obtained from the CNN encoding part and the global\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 2 / 18\ndesign, data collection and analysis, decision to\npublish, or preparation of the manuscript.\nCompeting interests : The authors have declared\nthat no competing interests exist.\ninformation obtained from the Transformer encoding part. At the same time, the fused fea-\nture information is transformed with the corresponding corresponding patch in order to be\ninput to the Transformer decoder part, and the acquired information features are recovered\nby up-sampling. The experimental results show that the dual-coding structure improves the\nperformance of the model and the utilisation of feature information at each level.\nThe following chapters are arranged as follows: the second chapter introduces the relevant\nresearch on skin lesions segmentation methods. The third chapter presents the methods pro-\nposed in this paper and the experimental settings and parameter settings in detail. The fourth\nchapter mainly shows the quantitative and qualitative analysis of the methods proposed in this\npaper and other networks, as well as the ablation analysis of the proposed innovation. Finally,\nthe fifth chapter summarizes and makes a simple arrangement for the future work.\nRelated work\nThis section summarizes the related progress of medical image segmentation in computer\nvision research.The first part is the convolutional neural network-based medical image seg-\nmentation progress, and the second part is transformer-based medical image segmentation\nprogress.\nOver the past few decades, the field of computer vision has flourished with the wave of deep\nlearning. Recently, the research on the CNN has not decreased, and transformer architecture\nhas become a new research direction of computer vision. Both of them have good performance\nin the field of computer vision. Here, we briefly review the traditional segmentation methods\nbased on CNN and the recently proposed Transformer network for segmentation.\nMedical image segmentation based on CNN\nYann et al. Proposed the first standard CNN [5], which is for handwritten character recogni-\ntion tasks. In the past few decades, many powerful networks have achieved unprecedented suc-\ncess in image segmentation tasks [6]. Alexnet [7] and Vggnet [8] show that increasing the\ndepth of the network by stacking convolution and aggregation layers in the network architec-\nture can obtain rich feature information. Google-Net [9] and Inception-Net [10] proposed to\nadd multiple paths for feature information transmission and proved their effectiveness. Resnet\n[11] in order to better improve its generalization ability, it is proposed to add fast connections\nin every two layers of the main network. To optimize the problem of limited acceptance\ndomain in previous studies, some studies regard the attention mechanism as the operator of\ninter-mode adaptation. Senet [12] and Genet [13] improved the performance of the network\nby establishing the model of interdependence between channels and adaptively recalibrating\nthe channel characteristic response. The summary is shown in the following table.\nU-Net [14] creatively proposed a U-shaped network based on encoding/decoding, which\nhas an irreplaceable position in medical image segmentation. UNet + + [15] designed multi-\nsegment nested and dense jump paths in jump connection to narrow the semantic gap. Atten-\ntion U-Net [16] enables the model to focus on targets of different shapes and sizes by propos-\ning a new attention gate mechanism. A new structure system is proposed, which uses\nincomplete and over-complete features to improve the segmentation of small anatomical\nstructures. Double-net [17] adopts the order of two u-nets and adopts spatial cone pool\n(ASPP) [18]. UNet3+ [19] uses deep monitoring and full-scale skip connection, and combines\nthe mask of the previous era with the feature mapping of the current period in the training\nprocess. Abayomi-Alli, et al [20]. propose a new data enhancement technique based on the\ncovariant synthetic minority oversampling technique (SMOTE) to address data scarcity and\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 3 / 18\nclass imbalance. Kadry, et al [21]. used the VGG SegNet protocol to automate the acquisition\nof dermatologic lesion sections from digital Dermospy images. The Table 1 shows the related\npapers and contribution, this table details the contributions of relevant research literature to\nthe field of deep learning.\nMedical image segmentation based on transformer\nMotivated by the achievement of the transformer [22] in various NLP [23] tasks, with the\nmigration of researchers in different fields, Increasingly transformer-based methodologies are\nappearing in computer vision tasks. In the current development process of the computer vision\nfield, Vit [24] adopted the network architecture of a pure transformer for the first time and\nrealized the SOTA performance of image recognition by pre-training a large number of data-\nsets. Deit [25] solves the limitation that the transformer needs many datasets in training by\nintroducing an efficient data training strategy and knowledge extraction algorithm. Swin\ntransformer [26] innovatively proposed the powerful mechanism of self-attention based on a\nmobile window, which has linear computational complexity and refreshes the best results in\nthe fields of image recognition, target detection, and semantic segmentation. It breaks through\nthe limitations of most previous models based on the transformer. Swin transformer adopts a\nlayered architecture, which improves the flexibility of its network architecture. Trans-UNet\n[27] introduced transformer architecture into the field of medical image segmentation and\nproved its powerful coding performance. PVT [28] imitates the pyramid structure in CNN\nand introduces it into Vit to realize various pixel-level intensive prediction tasks by generating\nmulti-scale feature maps. CPVT [29] and CVT [30] are most relevant to our work on conven-\ntional transformer groups using convolution projection.Under this line of research, we also try\nto get better results by investigating different components, the combination of other modules,\nmaking up for the deficiencies of the existing transformer, and taking advantage of the current\nadvantages. Although many investigators have successfully applied converters to visual tasks,\nthere are still many aspects that have not shown satisfactory results. Compared with the more\nestablished CNNs in the visual field, transformer network architecture still has a lot to develop,\nTable 1. The contribut ions of CNN networks.\nInternet contributio n\nCNN [5] The first standard recognition task for handwritte n characters\nAlexnet [7] First success ful applicati on ReLU[U+3 001]Dropo ut\nVggnet [8] Increase network depth\nGoogle- Net [9] Proposed convolutio nal reaggregati on at multiple dimensi ons\nInception -Net [10] Add multiple paths for feature informa tion transmissi on\nResnet [11] Add fast connections at every two layers of the main network\nSenet [12] Using simple low-level feature aggregation metho ds\nGenet [13] Aggregate s the neuron responses in a given spatial range\nU-Net [14] Creativel y proposed a coding/d ecoding based u-type network\nUNet + + [15] Multiple nested and dense jump paths in the jump connection\nAttention U-Net [16] A new attentio n gate mechanism is proposed\nDouble-ne t [17] Second- order u-shaped network architecture is used\nASPP [18] Free multi-scale feature extraction\nUNet3+ [19] Use deep monitoring and full-size jump connections\nAbayomi-A lli, et al. [20] A new data enhanc ement technique\nVGG SegNet [21] Automat ed acquisition of dermatolog ical lesion sections\nhttps://d oi.org/10.1371/j ournal.pon e.0277578.t00 1\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 4 / 18\nespecially in acquiring local feature information. Inspired by these methods, we propose an\ninput method combining transformer and CNN. We believe that the unified architecture of\ntransformer-based encoder and decoder can provide robust performance in medical image\nsegmentation.\nMethodology\nIn this part, we first introduce our research motivation, then describe the overall architecture\nof the proposed TC-Net network, and finally introduce the bilateral code structure and bilat-\neral code fusion module in detail.\nMotivation\nTransformer shows excellent potential in computer vision tasks, which makes this paper\nexplore the solutions based on the transformer. At the same time, the feasibility of applying\ntransformer-based network architecture to dermatological image segmentation tasks is also\ninvestigated. In the field of computer vision, it is recognized that the use of transformer net-\nwork architecture mainly needs to use a large number of data sets or load pre-training models.\nIn view of the small amount of medical image data, the simple transformer network has not\nmade significant progress in the medical field. In the research of skin disease segmentation,\nthe characteristics of CNN convolution operation make CNN unable to correlate and model\nthe global information of the input picture. In recent years, researchers have continuously pro-\nposed the information extraction module to strengthen the acquisition of the input picture\ninformation by the CNN network, which has been dramatically improved. It is found that the\nself-attention model in the transformer network model applied in the field of natural language\nprocessing can model the global semantic information. Therefore, this paper will use the\nadvantages of CNN to extract local information and transformer to extract global information\nto reasonably achieve more accurate skin disease segmentation results in the case of limited\ndatasets.\nNetwork architecture of TC-Net\nAs shown in Fig 1, in this paper, the double coding structure is adopted in the coding part, the\nlesion image is input into the two coding structures at the same time, the global feature and\nlocal feature are extracted, respectively, and the output results of the coding part are fused and\ninput to the decoding part. In the CNN branch, the image of skin lesions is mainly used to pro-\ncess the whole image to extract feature information. The transformer divides the picture into a\nseries of patch sequences, encodes the position and then inputs it into the transformer branch\narchitecture. The feature is extracted through the Swin transformer module and patch merging\nstructure. TC-Net architecture design the coding output of transformer architecture is adopted\nto enforce decoding global information of each layer by jumping connection operation, and\nthe captured global information is input to the decoding part. Finally, the global feature infor-\nmation of the output segmentation images is enhanced.\nDouble coding structure\nThe structure of the TC-Net network model proposed in this paper is based on double coding\narchitecture, which is mainly composed of the Resnet module and Swin Transformer module.\nThe coding part on the left is composed of the Resnet module and Swin Transformer module,\nwhile the decoding part on the right is only composed of the Swin Transformer module and\nskip connection module.\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 5 / 18\nThe framework of its network model is shown in Fig 2. The first coding channel adopts\nresidual structure and convolution structure. By calculating the residual convolution module,\nrich local feature information is obtained from the input skin disease image. At the same time,\nthe input skin disease image is divided into equal-sized image blocks. The relative position\ninformation is added and input into the Swin Transformer module to obtain the global feature\ninformation of the input image. The CNN encoder’s local information-dominated feature\ninformation output is fused to the global information-dominated feature information output\nby the transformer encoder via the CTF fusion module.\nFig 1. The overall architectur e of TC-Net. The image to be processed and input it to the dual coding channel at the same time, then fuse at the bottom\nof the coding to decode and output the split image.\nhttps://doi.o rg/10.1371/j ournal.pone .0277578.g001\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 6 / 18\nThe module of CTF\nSince the transformer and CNN employ different image feature extraction methods, in order\nto better fuse the different types of feature information extracted by the two encoding parts,\nthis paper proposes a module that combines these two, as shown in Fig 3. Firstly, this paper\nfurther obtains the essential information under different receptive fields through two different\nconvolution operations. Then, through a series of operations such as flattening, variable\ndimension, connection and regularization, the image form processed by CNN is transformed\ninto the same form as that processed by the transformer. Then, the local information is\nstrengthened, summarized, and fused with global information.\nThe specific operations are as follows: for the CTF module architecture, we have processed\ntwo branches according to the picture features from CNN branches. The two branches use the\nconvolution operation of different convolution kernels to obtain various ranges of characteris-\ntic information through different sizes of receptive fields. Then, the two are effectively fused\nand combined with the branch feature information from the transformer through the above\noperations.\nDatasets and metrics\nThe datasets\nIn the experiment provided in this paper, three famous public skin lesions image datasets\nISBI2016 [31], ISBI2017 [32], and ISIC2018 [33] are used to train the network proposed in this\npaper. These three data sets come from the public data sets of the ISIC challenge competition.\nConsidering the requirements of computer hardware configuration in the natural clinical\nmedical environment, in order to make the algorithm network better applied in real life, this\npaper does not process the datasets and only adjusts all images and labels to the resolution of\n224 × 224 at the same time, In order to test the effect of the network proposed in this paper in\nthe end-to-end natural clinical medical environment,we performed multiple verifications. We\nselected a total of three datasets, each dataset is divided into three parts: train, valid and test.\nFig 2. The architectur e of feature fusion block.\nhttps://d oi.org/10.1371/j ournal.pon e.0277578.g0 02\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 7 / 18\nThe Table 2 shows the data distribution of the three datasets, and from the table, we can see\nthat this study divides the three data sets in a certain proportion. So that we can have better\nrobustness in training.\nMetrics\nTo quantitatively evaluate the segmentation performance of TC-Net, we used the following\nwidely recognized evaluation indexes. Accuracy(ACC), sensitivity(SE), specificity(SP), preci-\nsion (PC), Jaccard index(JA) and dice index(DC) were included. All metrics are closer to\n100%, with better segmentation.\nFig 3. The architectur e of CNN and transform er fusion blocks.\nhttps://d oi.org/10.1371/j ournal.pon e.0277578.g0 03\nTable 2. The introduct ion of the public datasets.\nDataset Train valid Test\nISBI2016 900 79 300\nISBI2017 2000 150 600\nISIC2018 1815 259 520\nhttps://d oi.org/10.1371/j ournal.pon e.0277578.t00 2\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 8 / 18\nResults and analysis\nExperimental setup\nExperimental parameters for this paper were set as follows: for mini-batch training, 12 is the\nfixed value set for the batch size. The network loss function is the Bce loss function and Dice\nloss function. The network uses Adam optimizer and Kaiming’s initialization method for opti-\nmization and training. The initialization of the network parameters was optimized and trained\nby Adam optimizer according to the method of the Kaiming et al. The number of iterations of\nthe network is equal to 200, and the initial learning rate is equal to 0.0001, The experiments in\nthis chapter are completed under the Linux system. The deep learning architecture adopted is\nthe PyTorch framework, and the hardware server is NVIDIA Tesla V100.\nAblation experiment\nThis paper verifies the effectiveness of the dual encoder network TC-Net and the feasibility\nand effectiveness of the proposed CNN and transformer fusion module CTF by setting four\ngroups of experiments: CE-Net, Swin-UNet, direct addition of dual encoders, and dual\nencoder + CTF. This paper is tested on the same dataset (ISIC2018 skin lesions dataset). This\npaper compares the results of different networks after segmenting the same kind of skin lesions\nto compare the effectiveness of other modules tested in the ablation experiment. Table 3 shows\nthe performance of the main indicators in each experiment.\nIn this paper, four experiments are set as ablation experiments to verify the effectiveness of\nTC-Net, and the values tested on the dataset are compared through statistical evaluation\nindexes. It shown in 3, this paper can clearly see that the network method of double encoder\naddition is better than the traditional pure transformer architecture Swin UNet on the skin\ndisease data set, which reflects the effectiveness of the double coding network structure pro-\nposed in this paper. At the same time, it can be clearly observed that the dual encoder with\nCTF is better than the ordinary dual encoder, which verifies the effectiveness of the CTF pro-\nposed in this paper. By comprehensive comparison, the method proposed in this paper is not\nonly superior to the pure transformer architecture Swin-UNet, but also superior to the pure\nCNN network CE-Net network architecture. Finally, through the evaluation indicators men-\ntioned above, it can be proved that the hybrid dual encoder architecture TC-Net of hybrid\nCNN and transformer proposed in this paper is effective in skin lesions segmentation.\nIn the table, the performance of Swin-UNet in each skin disease image segmentation index is\nlower than that of CE-Net. Compared with Swin-UNet, double coding + add has a correspond-\ning improvement in each index, with an increase of 0.6% on ACC, 2.3% on SE, 1.2% on PC,\n2.5% on JA and 1.6% on DC. After joining CTF, the indicators have also improved accordingly.\nVisualization results of ablation experiment\nIn order to illustrate the effectiveness of dual encoder fusion, this paper selects the visual output\nimages of ISIC2018, ISBI2017 and ISBI2016 datasets ablation experiments. This paper verifies\nTable 3. The ablation experime nt based on ISIC2018.\nMethods ACC(%) SE(%) SP(%) PC(%) JA(%) DC(%)\nCE-Net 95.81 88.11 97.88 91.64 81.58 89.71\nSwin UNet 95.40 86.81 97.73 90.85 79.62 88.46\nDouble coding+add 96.03 88.51 98.02 92.07 82.14 90.02\nDouble coding+CTF 96.31 90.59 97.86 91.57 83.55 90.80\nhttps://do i.org/10.1371/j ournal.pone .0277578.t003\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 9 / 18\nthe efficacy of the double encoder network TC net proposed in this paper and the feasibility\nand effectiveness of the proposed CNN transformer fusion module CTF by showing four\ngroups of experiments: CE-Net, Swin-UNet, direct addition of dual encoders, and dual encoder\n+ CTF, This paper compares the results of different networks after segmenting the same kind\nof skin lesions to compare the effectiveness of other modules tested in the ablation experiment.\nFigs 4 and 5 show the segmentation effect of skin diseases in four groups of experiments.\nAs shown in the Figs 4 and 5, the feature information extracted by Swin Transformer per-\nforms well globally. After adding the CNN encoder, the local edge information extracted by\nthe network is better. The segmentation image obtained after adding the CTF module is finally\nappropriate to the label image. In the comparison diagrams of the ablation experiment, this\npaper can clearly observe the images of the segmentation effect of adding different modules on\nskin lesions, and intuitively prove the effectiveness of TC-Net and each module.\nComparative experiment\nThis paper evaluates TC-Net in the ISBI2016 test dataset, ISBI2017 dataset and ISIC2018 data-\nset, respectively, and compares the equivalence of ACC, SE, SP, PC, JS and DC, respectively.\nWe compared it with the mature segmentation network, including U-Net, R2U-Net [34],\nCE-Net [35], SA-UNet [36], UNet3+ and Swin-UNet [37]. Meanwhile, we conducted experi-\nments under identical parameter settings and computational environments to ensure fairness\nin experimental comparisons. The performance of TC-Net in each index is the best among all\nFig 4. Visual analysi s of ablation experimen t on ISBI2016. (a) Origina l. (b) Gray. (c) GT. (d) CE-Net. (e) Swin-Unet. (f) Double coding+add. (g)\nTC-Net.\nhttps://doi.o rg/10.1371/j ournal.pone .0277578.g004\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 10 / 18\nnetworks, which can be proved from Tables 4–6. Compared with the pure transformer net-\nwork architecture of Swin UNet, and the pure network architecture of CE-Net, TC-net is sig-\nnificantly improved in these three test datasets.\nIn the ISIC2018 dataset, by careful comparison with Table 4, it can be concluded that com-\npared with U-Net network architecture, TC-Net has increased by 1.4% on ACC, 7% on SE,\n0.2% on SP, 1.8% on PC, 6.2% on JA and 4.5% on DC. Compared with the Swin-UNet, the net-\nwork of TC-Net has increased by 0.99% in the ACC index, about 3.7% in the SE index, about\n0.7% in the PC index, about 4% in JA index and 2.4% in DC index. Then compared with the\nCE-Net network, the network proposed in this paper has increased by 0.5% in the ACC index,\nabout 2.4% in the SE index, and about 2% in the JA index and 1.1% in DC index.\nFig 5. Visual analysi s of ablation experimen t on ISBI2016. (a) Origina l. (b) Gray. (c) GT. (d) CE-Net. (e) Swin-Unet. (f) Double coding+add. (g)\nTC-Net.\nhttps://doi.o rg/10.1371/j ournal.pone .0277578.g005\nTable 4. Comparativ e experiment s based on ISIC2018 dataset.\nMethods Year ACC(%) SE(%) SP(%) PC(%) JA(%) DC(%)\nU-Net 2015 94.66 86.03 97.10 88.72 77.43 87.13\nR2U-Net 2018 95.09 86.58 97.51 90.00 78.85 88.05\nCE-Net 2019 95.81 88.11 97.88 91.64 81.58 89.71\nU-Net3+ 2020 94.97 85.20 97.77 90.86 78.30 87.71\nSA-UNet 2021 94.78 84.87 97.59 90.29 77.63 87.25\nSwin UNet 2021 95.40 86.81 97.73 90.85 79.62 88.46\nTC-Net 96.31 90.59 97.86 91.57 83.55 90.80\nhttps://do i.org/10.1371/j ournal.pone .0277578.t004\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 11 / 18\nIn the ISBI2017 dataset, by careful comparison with Table 5, Compared with U-Net net-\nwork architecture, TC-net has increased by 1.7% on ACC, 4.5% on SE, 0.7% on SP, 2.8% on\nPC, 6.1% on JA and 3.7% on DC. Compared with the Swin-UNet, tthe network of TC-Net has\nincreased by 1.4% in ACC index, about 2.4% in SE index, about 5.1% in PC index, about 4.7%\nin JA index and 3.3% in DC index. On the ISBI2017 dataset, compared with the CE-Net net-\nwork, the network proposed in this paper has increased by 0.2% in the ACC index, about 1%\nin the SE index, about 1.5% in the PC index, about 0.8% in JA index and 0.8% in DC index.\nSimilarly, in the ISBI2016 dataset, by careful comparison with Table 6, it can be concluded\nthat compared with U-Net network architecture, TC-Net has increased by 1.4% on ACC, 1.8%\non SE, 1.1% on SP, 3.3% on PC, 4.5% on JA and 2.7% on DC. Compared with the Swin-UNet\nnetwork, the network of TC-Net has increased by 1% in the ACC index, about 1.3% in SE\nindex, about 2.3% in PC index, about 3% in JA index and 1.7% in DC index. On the ISBI2016\ndataset, compared with the CE-Net network, the network proposed in this paper has increased\nby 1% in the JA index and 0.5% in the DC index.\nIn order to better verify the effectiveness of the proposed method, we directly compare it\nwith State-of-the-Art Methods. As shown in Table 7, in the absence of data enhancement,the\nmethod we proposed still has corresponding improvement compared with other networks.\nTable 5. Comparativ e experiment s based on ISBI2017 dataset.\nMethods Year ACC(%) SE(%) SP(%) PC(%) JA(%) DC(%)\nU-Net 2015 92.21 74.38 97.58 89.58 68.30 80.70\nR2U-Net 2018 92.28 75.37 97.45 89.38 69.04 81.17\nCE-Net 2019 93.49 80.51 97.33 89.92 73.83 84.55\nU-Net3+ 2020 92.08 72.95 97.87 90.69 67.79 80.29\nSA-UNet 2021 92.08 76.93 96.66 86.74 68.76 81.06\nSwin UNet 2021 92.26 79.01 96.30 86.23 69.86 81.95\nTC-Net 93.68 81.45 97.79 91.38 74.55 85.20\nhttps://do i.org/10.1371/j ournal.pone .0277578.t005\nTable 7. Comparativ e experiences with state-of-the- art methods on fused networks .\nMethods Year ACC(%) SP(%) JA(%) DC(%)\nMedT [38] 2021 - - 77.8 85.9\nTransUNet [39] 2021 - - 82.2 89.4\nMCTans [40] 2021 - - - 90.3\nR.Ali et.al [41] 2022 95.4 97.1 - -\nH.Wu et.al [42] 2022 95.2 97 - -\nTC-Net - 96.31 97.86 83.55 90.80\nhttps://do i.org/10.1371/j ournal.pone .0277578.t007\nTable 6. Comparativ e experiment s based on ISBI2016 dataset.\nMethods Year ACC(%) SE(%) SP(%) PC(%) JA(%) DC(%)\nU-Net 2015 94.69 91.30 96.01 89.32 82.18 90.12\nR2U-Net 2018 94.43 87.68 97.06 91.49 80.95 89.38\nCE-Net 2019 95.94 92.80 97.10 92.06 85.85 92.31\nU-Net3+ 2020 94.94 90.26 96.74 91.12 82.87 90.54\nSA-UNet 2021 94.11 89.46 95.90 88.82 80.14 88.82\nSwin UNet 2021 95.02 91.87 96.21 90.39 83.70 91.01\nTC-Net 96.06 93.17 97.12 92.62 86.68 92.82\nhttps://do i.org/10.1371/j ournal.pone .0277578.t006\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 12 / 18\nThe above contents mainly show and analyze the results of TC-Net and the comparative\nexperiments taken in this paper. Through the above introduction, we can easily understand the\neffectiveness and universality of TC-Net in the task of skin lesions segmentation. At the same\ntime, it also proves the practical significance of this work in the segmentation of skin lesions.\nThe visualization results of the comparison algorithm\nIn order to illustrate the effectiveness of the TC-Net algorithm and other network structures in\nthe task of skin disease segmentation, this paper selects some segmentation results from three\ndatasets: ISBI2016, ISBI2017 and ISIC2018, to display and explain. As shown in the visualiza-\ntion results, although the location of skin diseases presents different sizes and shapes. The\nTC-Net architecture integrated by CNN and transformer is better than the pure CNN network\narchitecture and the pure transformer network architecture.\nFig 6. The example on ISIC2018 dataset, (a) original image; (b) Gray image; (c) GT label image; (d) Segmentation image of CE-Net; (e)\nSegmentation image of Swin-UNet; (f) Segmentat ion image of TC-Net.\nhttps://doi.o rg/10.1371/j ournal.pone .0277578.g006\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 13 / 18\nAs shown in the Figs 6 and 7, the visual image of the CE-Net does not perform well in the\noverall connection of the overall edge part, and there is a large difference between the edge and\nthe label image. On the contrary, the Swin-UNet is more in line with the real label value at the\nsegmented edge, but the local situation at the edge is fuzzy. TC-Net combines the advantages\nof the two networks to achieve the segmentation result more in line with the label value.\nAs shown in Fig 8, we pick out two dermatological pictures and their segmentation maps\nunder different networks for more exhaustive analysis, from left to right are respectively the\noriginal diagram, the label map, the segmentation map of CE-Net, the segmentation map of\nSwin UNet and the segmentation map of TC-Net. The excellent performance of the dual cod-\ning fusion network TC-Net for skin disease segmentation was excellently demonstrated in\nterms of the margins of the segmentation map and the fit to the GT map.\nFig 7. The example on ISIC2018 dataset, (a) original image; (b) Gray image; (c) GT label image; (d) Segmentation image of CE-Net; (e)\nSegmentation image of Swin-UNet; (f) Segmentat ion image of TC-Net.\nhttps://doi.o rg/10.1371/j ournal.pone .0277578.g007\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 14 / 18\nTo analyze the detailed performance of TC-Net on the skin disease segmentation map, we\nanalyze its performance on the segmentation map in more detail. As shown in the Fig 8, we\nmark the subtle segmentation edges with a red box. From the mark, we can find that TC-Net\ncan also obtain the characteristic information of the lesion in modest places to get a more real-\nistic partition map.\nConclusion\nNowadays, in the convolutional neural network, it is mainly through adding innovative feature\nextraction modules to enrich the critical information in the convolutional neural network. In\nthe transformer network, the advantage of extracting global feature information in the net-\nwork has also attracted a large number of researchers to explore the field of computer vision.\nHowever, the effect of skin disease segmentation using the transformer network alone is not\ngood. In the face of complex and diverse feature information of skin disease focus pictures,\nthis paper proposes a dual encoder segmentation algorithm TC-Net mixed with CNN and\ntransformer. TC-Net is mainly composed of the Resnet module and Swin Transformer\nFig 8. The detail analysi s on ISBI2017 dataset, (a) original image; (b) GT label image; (c) Segmentation image of CE-Net; (d) Segmentation image\nof Swin-UNet; (e) Segmentation image of TC-Net. The red box indicates the segmenta tion edge information at the same position.\nhttps://doi.o rg/10.1371/j ournal.pone .0277578.g008\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 15 / 18\nmodule. The coding part on the left is composed of the Resnet module and Swin Transformer\nmodule. Local information and global information are convoluted with a transformer fusion\nmodule CTF to generate coded output information with rich global information and local\ninformation. We selected three publicly available datasets for test validation and performed\nstatistical analysis of the experimental results. Compared to Swin-UNet, it increased the dice\nindex by 2.46% and the JA index by approximately 4% on the ISIC2018 dataset. On the\nISBI2017 dataset, both the dice and JA indices increased by approximately 4%. The statistical\nresults show that the proposed network has excellent segmentation performance. However,\nthe current method proposed in this paper only achieves a simple fusion of the two methods,\nTransformer and CNN, in terms of acquiring information features, and to some extent opti-\nmises the segmentation performance of the network TC-Net in segmenting dermatological\nlesion areas, but the network has no significant advantages in terms of operational speed and\nnetwork complexity. The next task is therefore to perform a simpler and more effective fusion\nof network features within the two networks, with some optimisation not only in terms of seg-\nmentation effectiveness, but also in terms of overall network performance.\nAuthor Contributions\nConceptualization: Yuying Dong.\nFormal analysis: Liejun Wang, Yongming Li.\nFunding acquisition: Liejun Wang.\nSoftware: Yuying Dong.\nSupervision: Liejun Wang.\nValidation: Yongming Li.\nWriting – original draft: Yuying Dong.\nWriting – review & editing: Liejun Wang, Yongming Li.\nReferences\n1. G. Ji, Y. Chou, D. Fan, and L. Shao, Progressive ly Normalized Self-Attentio n Networ k for Video Polyp\nSegmen tation, in Medical Image Computing and Computer Assisted Interventi on - MICCAI 2021—24th\nInternational Conferen ce, Strasbourg, France, Septemb er 27—Oc tober 1, 2021, Proceedings , Part I,\nvolume 12901, pages 142–152, Springer, 2021.\n2. Li X., Chen H., Qi X., Dou Q., and Heng P., H-Dens eUNet: Hybrid Densely Connec ted UNet for Liver\nand Tumor Segmentat ion From CT Volumes, IEEE Trans. Medical Imaging 37, 2663–267 4 (2018).\nhttps://doi.or g/10.110 9/TMI.2018.2 845918\n3. Meijering E., Cell Segment ation: 50 Years Down the Road [Life Sciences], IEEE Signal Process. Mag.\n29, 140–145 (2012). https:// doi.org/10.11 09/MSP .2012.2204190\n4. Yuan Y., Chao M., and Lo Y., Automatic Skin Lesion Segment ation Using Deep Fully Convolution al Net-\nworks With Jaccard Distance, IEEE Trans. Medical Imaging 36, 1876–1 886 (2017). https://doi.or g/10.\n1109/TMI.20 17.2695 227\n5. LeCun Y., Bottou L., Bengio Y., and Haffner P., Gradient-b ased learning applied to document recogni -\ntion, Proc. IEEE 86, 2278–2324 (1998). https://doi.or g/10.110 9/5.726791\n6. Pham D. L., Xu C., and Prince J. L., Current methods in medical image segmentatio n., Annual Review\nof Biomed ical Engineeri ng 2, 315–337 (2000). https://doi.or g/10.114 6/annurev. bioeng.2.1.315 PMID:\n11701515\n7. Krizhevsky A., Sutskever I., and Hinton G. E., ImageN et classification with deep convolutio nal neural\nnetworks, Commun. ACM 60, 84–90 (2017). https://doi.or g/10.1145/ 3065386\n8. K. Simonyan and A. Zisserman, Very Deep Convolut ional Networ ks for Large-Sc ale Image Recogni-\ntion, in 3rd Internation al Conferen ce on Learning Representa tions, ICLR 2015, San Diego, CA, USA,\nMay 7-9, 2015, Conferen ce Track Proceedings , edited by Y. Bengio and Y. LeCun, 2015.\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 16 / 18\n9. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, and A. Rabinovich, Going deeper with convolutio ns, in IEEE\nConferen ce on Comput er Vision and Pattern Recognition , CVPR 2015, Boston, MA, USA, June 7-12,\n2015, pages 1–9, IEEE Computer Society, 2015.\n10. C. Szegedy, V. Vanhouck e, S. Ioffe, and Z. Wojna, Rethink ing the Inception Architecture for Comput er\nVision, in 2016 IEEE Conferen ce on Computer Vision and Pattern Recognition , CVPR 2016, Las\nVegas, NV, USA, June 27-30, 2016, pages 2818–2826, IEEE Comp uter Society, 2016.\n11. K. He, X. Zhang, and J. Sun, Deep Residual Learning for Image Recogn ition, in 2016 IEEE Conferen ce\non Computer Vision and Pattern Recognition , CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016,\npages 770–77 8, IEEE Computer Society, 2016.\n12. J. Hu, L. Shen, and G. Sun, Squeeze-and -Excitatio n Networks, in 2018 IEEE Conferen ce on Comput er\nVision and Pattern Recog nition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 7132–\n7141, Comput er Vision Foundation / IEEE Compu ter Society, 2018.\n13. J. Hu, L. Shen, and A. Vedaldi, Gather-Exci te: Exploiti ng Feature Context in Convo lutional Neural Net-\nworks, in Advances in Neural Informatio n Processin g Systems 31: Annua l Conferen ce on Neural Infor-\nmation Processin g Systems 2018, NeurIPS 2018, Decemb er 3-8, 2018, Montre ´ al, Canada, pages\n9423–9433, 2018.\n14. O. Ronneber ger, P. Fischer, and T. Brox, U-Net: Convolution al Networks for Biomedic al Image Seg-\nmentation , in Medical Image Computing and Computer- Assisted Interventi on - MICCAI 2015—18th\nInternational Conferen ce Munich, Germany, October 5—9, 2015, Proceedings , Part III, volume 9351,\npages 234–24 1, Springer, 2015.\n15. Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, UNet++: A Nested U-Net Architectu re for\nMedical Image Segmentation, in Deep Learning in Medical Image Analysis— and—Mult imodal Learning\nfor Clinical Decision Support—4 th Interna tional Workshop, DLMIA 2018, and 8th International Work-\nshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, Septemb er 20, 2018,\nProceedings , volume 11045, pages 3–11, Springer, 2018.\n16. O. Oktay, J. Schlempe r, L. L. Folgoc, M. C. H. Lee, and D. Ruecke rt, Attention U-Net: Learning Where\nto Look for the Pancreas, CoRR abs/1804 .03999 (2018).\n17. D. Jha, M. A. Riegler, D. Johansen, P. Halvorsen , and H. D. Johansen, DoubleU -Net: A Deep Convolu-\ntional Neural Network for Medical Image Segmentat ion, in 33rd IEEE Internat ional Symposium on Com-\nputer-Based Medical Systems, CBMS 2020, Roches ter, MN, USA, July 28-30, 2020, pages 558–564,\nIEEE, 2020.\n18. L. Chen, G. Papandreo u, F. Schroff, and H. Adam, Rethink ing Atrous Convolution for Semantic Image\nSegmen tation, CoRR abs/1706 .05587 (2017).\n19. H. Huang, L. Lin, R. Tong, H. Hu, Y. Chen, and J. Wu, UNet 3+: A Full-Scale Connected UNet for Medi-\ncal Image Segmen tation, in 2020 IEEE Internation al Conferen ce on Acoustics, Speech and Signal Pro-\ncessing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pages 1055–1059 , IEEE, 2020.\n20. Abayomi- Alli OO., Damaev iius R., Misra S., Maskeliūnas R., and Abayomi- Alli A., “Malignant skin mela-\nnoma detection using image augmenta tion by oversampli ng in nonlinear lower-dimens ional embedding\nmanifold,” Turkish Journal of Electrical Engineeri ng and Comput er Sciences , vol. 2021, no. 29, p.\n2600–2614, 2021.\n21. S. Kadry, D. Taniar, R. Damase vicius, V. Rajinikanth , and I. A. Lawal, “Extracti on of abnormal skin\nlesion from dermoscopy image using vgg-segn et,” in 2021 Seventh International conferen ce on Bio Sig-\nnals, Images , and Instrumenta tion (ICBSII) , 2021.\n22. A. Vaswani, N. Shazeer, N. Parmar, L. Kaiser, and I. Polosukh in, Attention is All you Need, in Advances\nin Neural Informatio n Processin g System s 30: Annual Conferen ce on Neural Informatio n Processing\nSystems 2017, Decemb er 4-9, 2017, Long Beach, CA, USA, pages 5998–6008, 2017.\n23. Badrinara yanan V., Kenda ll A., and Cipolla R., SegNet: A Deep Convo lutional Encoder-Dec oder Archi-\ntecture for Image Segmen tation, IEEE Trans. Pattern Anal. Mach. Intell. 39, 2481–2495 (2017). https://\ndoi.org/10.11 09/TPAMI .2016.2644615\n24. A. Dosovitskiy , L. Beyer, A. Kolesniko v, D. Weissenbor n, and N. Houlsby , An Image is Worth 16x16\nWords: Transforme rs for Image Recognition at Scale, in 9th International Conferen ce on Learning Rep-\nresentati ons, ICLR 2021, Virtual Event, Austria , May 3-7, 2021, OpenRevi ew.net, 2021.\n25. H. Touvron, M. Cord, A. Sablayrolle s, and H. Je ´ gou, Training data-effic ient image transfor mers & distil-\nlation through attention , in Proceedings of the 38th Internat ional Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, volume 139, pages 10347–10357 , PMLR, 2021.\n26. Z. Liu, Y. Lin, Y. Cao, H. Hu, and B. Guo, Swin Transforme r: Hierarchica l Vision Transforme r using\nShifted Windows , in 2021 IEEE/CVF International Conferen ce on Comput er Vision, ICCV 2021, Mon-\ntreal, QC, Canada, October 10-17, 2021, pages 9992–10002, IEEE, 2021.\n27. J. Chen, Y. Lu, Q. Yu, A. L. Yuille, and Y. Zhou, TransUN et: Transfor mers Make Strong Encode rs for\nMedical Image Segmentation, CoRR abs/2102 .04306 (2021).\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 17 / 18\n28. W. Wang, E. Xie, X. Li, and L. Shao, Pyramid Vision Transfor mer: A Versati le Backbone for Dense Pre-\ndiction without Convolut ions, in 2021 IEEE/CV F Internationa l Conferen ce on Computer Vision, ICCV\n2021, Montreal , QC, Canada, October 10-17, 2021, pages 548–558, IEEE, 2021.\n29. X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, et al. Condition al positiona l encodings for vision\ntransforme rs, arXiv preprint arXiv:210 2.10882 (2021).\n30. H. Wu, B. Xiao, N. Codella, L. Yuan, and L. Zhang, CvT: Introduci ng Convolution s to Vision Transform-\ners, in 2021 IEEE/CV F Interna tional Conference on Computer Vision, ICCV 2021, Montreal, QC, Can-\nada, October 10-17, 2021, pages 22–31, IEEE, 2021.\n31. Ma Z. and Tavares J. M. R. S., A Novel Approach to Segmen t Skin Lesions in Dermosc opic Images\nBased on a Deform able Model, IEEE J. Biomed. Health Informatics 20, 615–623 (2016). https://doi.\norg/10.1109/ JBHI.2015.23 90032 PMID: 25585429\n32. N. C. F. Codella, D. A. Gutman , H. Kittler, and A. Halpern, Skin lesion analysis toward melanoma detec-\ntion: A challenge at the 2017 Interna tional symposiu m on biomedi cal imaging (ISBI), hosted by the inter-\nnational skin imaging collabor ation (ISIC), in 15th IEEE Interna tional Symposium on Biomedical\nImaging, ISBI 2018, Washingto n, DC, USA, April 4-7, 2018, pages 168–172, IEEE, 2018.\n33. P. Tschandl, C. Rosendahl, and H. Kittler, The HAM100 00 Dataset: A Large Collection of Multi-Sou rce\nDermatos copic Images of Common Pigmen ted Skin Lesions, CoRR abs/1803 .10417 (2018).\n34. M. Z. Alom, M. Hasan, C. Yakopcic, T. M. Taha, and V. K. Asari, Recurren t Residual Convolution al Neu-\nral Network based on U-Net (R2U-N et) for Medical Image Segmen tation, CoRR abs/1802 .06955\n(2018).\n35. Gu Z., Cheng J., Fu H., Zhou K., Gao S., and Liu J., CE-Net: Contex t Encoder Network for 2D Medical\nImage Segmentat ion, IEEE Trans. Medical Imaging 38, 2281–2292 (2019). https://do i.org/10.1109 /\nTMI.2019.29 03562\n36. C. Guo, M. Szemeny ei, Y. Yi, W. Wang, B. Chen, and C. Fan, SA-UNet: Spatial Attentio n U-Net for Ret-\ninal Vessel Segmentation , in 25th International Conferen ce on Patter n Recognition , ICPR 2020, Virtual\nEvent / Milan, Italy, January 10-15, 2021, pages 1236–1 242, IEEE, 2020.\n37. H. Cao, Y. Wang, J. Chen, D. Jiang, and X. Zhang, Swin-Unet : Unet-like Pure Transfor mer for Medical\nImage Segmentat ion, CoRR abs/2105. 05537 (2021).\n38. J. M. J. Valanarasu, P. Oza, I. Hacihalilog lu, and V. M. Patel, “Medical transfor mer: Gated axial-att en-\ntion for medical image segment ation,” in Medical Image Comput ing and Computer Assisted Interven-\ntion—MICC AI 2021—24t h International Conferen ce, Strasbo urg, France, Septemb er 27 - October 1,\n2021, Procee dings, Part I, ser. Lecture Notes in Compute r Science, M. de Bruijne, P. C. Cattin, S.\nCotin, N. Padoy, S. Speidel, Y. Zheng, and C. Essert, Eds., vol. 12901. Springer, 2021, pp. 36–46.\n39. J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, et al. “Transunet: Transfor mers make strong encoders\nfor medical image segmenta tion,” CoRR, vol. abs/2102 .04306, 2021. [Online]. Available: https://arx iv.\norg/abs/2 102.04306\n40. Y. Ji, R. Zhang, H. Wang, Z. Li, L. Wu, S. Zhang, et al. “Multi-comp ound transfor mer for accurate bio-\nmedical image segmentatio n,” in Medical Image Computing and Computer Assisted Interventi on—MIC-\nCAI 2021—24th Internat ional Conferen ce, Strasbourg, France, Septemb er 27—Oc tober 1, 2021,\nProceedings , Part I, ser. Lecture Notes in Computer Science, M. de Bruijne, P. C. Cattin, S. Cotin, N.\nPadoy, S. Speidel, Y. Zheng, and C. Essert, Eds., vol. 12901. Springer, 2021, pp. 326–33 6.\n41. R. Ali and H. K. Ragb, “Skin lesion segmentatio n and classificat ion using deep learning and handcra fted\nfeatures,” CoRR, vol. abs/2112 .10307, 2021. [Online]. Available: https://arx iv.org/abs/2 112.10307\n42. Wu H., Pan J., Li Z., Wen Z., and Qin J., “Automated skin lesion segment ation via an adaptive dual\nattention module,” IEEE transaction s on medical imaging , vol. 40, no. 1, pp. 357–370.\nPLOS ONE\nTC-Net: Dual coding network of Transforme r and CNN for skin lesion segmenta tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02775 78 November 21, 2022 18 / 18",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.7693573832511902
    },
    {
      "name": "Computer science",
      "score": 0.7303503751754761
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6981998682022095
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6616028547286987
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.6014739871025085
    },
    {
      "name": "Deep learning",
      "score": 0.5778601765632629
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.45144766569137573
    },
    {
      "name": "Dice",
      "score": 0.4410821199417114
    },
    {
      "name": "Transformer",
      "score": 0.4203764796257019
    },
    {
      "name": "Mathematics",
      "score": 0.10198408365249634
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I96908189",
      "name": "Xinjiang University",
      "country": "CN"
    }
  ],
  "cited_by": 22
}