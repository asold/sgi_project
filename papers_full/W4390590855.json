{
    "title": "A Survey of Text Classification With Transformers: How Wide? How Large? How Long? How Accurate? How Expensive? How Safe?",
    "url": "https://openalex.org/W4390590855",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A1963803131",
            "name": "John Fields",
            "affiliations": [
                "Concordia University Wisconsin",
                "Concordia University Ann Arbor"
            ]
        },
        {
            "id": "https://openalex.org/A2258527278",
            "name": "Kevin Chovanec",
            "affiliations": [
                "Marquette University"
            ]
        },
        {
            "id": "https://openalex.org/A1891645282",
            "name": "Praveen Madiraju",
            "affiliations": [
                "Marquette University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3156333129",
        "https://openalex.org/W4382246105",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2009190245",
        "https://openalex.org/W3122336031",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W6781051561",
        "https://openalex.org/W4212926655",
        "https://openalex.org/W3171654528",
        "https://openalex.org/W4385988359",
        "https://openalex.org/W2997049449",
        "https://openalex.org/W2895547478",
        "https://openalex.org/W6778575327",
        "https://openalex.org/W6797565217",
        "https://openalex.org/W4221153690",
        "https://openalex.org/W2977526300",
        "https://openalex.org/W3166583983",
        "https://openalex.org/W6848004557",
        "https://openalex.org/W4320921249",
        "https://openalex.org/W2970217403",
        "https://openalex.org/W3037422790",
        "https://openalex.org/W2937423263",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W6766673545",
        "https://openalex.org/W6763701032",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4365152619",
        "https://openalex.org/W4387860528",
        "https://openalex.org/W4225095674",
        "https://openalex.org/W2945830819",
        "https://openalex.org/W2619383789",
        "https://openalex.org/W4385573848",
        "https://openalex.org/W6838454321",
        "https://openalex.org/W3199015608",
        "https://openalex.org/W6755754581",
        "https://openalex.org/W4379391218",
        "https://openalex.org/W4379929801",
        "https://openalex.org/W4310390625",
        "https://openalex.org/W3202428668",
        "https://openalex.org/W6797867632",
        "https://openalex.org/W3216660278",
        "https://openalex.org/W3170989320",
        "https://openalex.org/W6772383348",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W4313045155",
        "https://openalex.org/W4229658977",
        "https://openalex.org/W6853349582",
        "https://openalex.org/W6851579256",
        "https://openalex.org/W3145602566",
        "https://openalex.org/W6852353527",
        "https://openalex.org/W6781533629",
        "https://openalex.org/W6776048684",
        "https://openalex.org/W6851077998",
        "https://openalex.org/W2962785754",
        "https://openalex.org/W4385573298",
        "https://openalex.org/W4327519588",
        "https://openalex.org/W3161947065",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W3190572136",
        "https://openalex.org/W6810787498",
        "https://openalex.org/W6810220367",
        "https://openalex.org/W6810730852",
        "https://openalex.org/W6787335730",
        "https://openalex.org/W6810332117",
        "https://openalex.org/W3213241618",
        "https://openalex.org/W4285107714",
        "https://openalex.org/W6810463509",
        "https://openalex.org/W4385573569",
        "https://openalex.org/W4385573004",
        "https://openalex.org/W3165327186",
        "https://openalex.org/W3188505388",
        "https://openalex.org/W6802709103",
        "https://openalex.org/W6802279528",
        "https://openalex.org/W6801292351",
        "https://openalex.org/W3167675683",
        "https://openalex.org/W4283172211",
        "https://openalex.org/W6794978363",
        "https://openalex.org/W3128232076",
        "https://openalex.org/W3166961312",
        "https://openalex.org/W3186964694",
        "https://openalex.org/W3164886736",
        "https://openalex.org/W3049565363",
        "https://openalex.org/W3202038256",
        "https://openalex.org/W7027083346",
        "https://openalex.org/W2972663293",
        "https://openalex.org/W3156200279",
        "https://openalex.org/W3176890131",
        "https://openalex.org/W3154654049",
        "https://openalex.org/W6791866086",
        "https://openalex.org/W3168771811",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W3099695344",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W3105042180",
        "https://openalex.org/W4384261711",
        "https://openalex.org/W4389519044",
        "https://openalex.org/W3212368439",
        "https://openalex.org/W6768583413",
        "https://openalex.org/W3021934057",
        "https://openalex.org/W4200048988",
        "https://openalex.org/W3204889936",
        "https://openalex.org/W4385648800",
        "https://openalex.org/W4361008252",
        "https://openalex.org/W4362680847",
        "https://openalex.org/W6850299220",
        "https://openalex.org/W4385570792",
        "https://openalex.org/W6760568010",
        "https://openalex.org/W2998436975",
        "https://openalex.org/W6780266381",
        "https://openalex.org/W2802022891",
        "https://openalex.org/W6638665372",
        "https://openalex.org/W6761748628",
        "https://openalex.org/W3029142491",
        "https://openalex.org/W3136407507",
        "https://openalex.org/W6763147337",
        "https://openalex.org/W6796525963",
        "https://openalex.org/W3011296786",
        "https://openalex.org/W3133665323",
        "https://openalex.org/W6784025715",
        "https://openalex.org/W4281488715",
        "https://openalex.org/W4280600998",
        "https://openalex.org/W6846086705",
        "https://openalex.org/W6784682006",
        "https://openalex.org/W3105639882",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W6810081322",
        "https://openalex.org/W3183138634",
        "https://openalex.org/W4378464434",
        "https://openalex.org/W6800875267",
        "https://openalex.org/W4388132131",
        "https://openalex.org/W4281931487",
        "https://openalex.org/W4385574262",
        "https://openalex.org/W6846076640",
        "https://openalex.org/W6852800892",
        "https://openalex.org/W3176390686",
        "https://openalex.org/W3196783364",
        "https://openalex.org/W6785427054",
        "https://openalex.org/W6811479858",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W6774526564",
        "https://openalex.org/W2952370363",
        "https://openalex.org/W2964573145",
        "https://openalex.org/W4386576703",
        "https://openalex.org/W6765939562",
        "https://openalex.org/W2963119602",
        "https://openalex.org/W6767182473",
        "https://openalex.org/W3042602466",
        "https://openalex.org/W6768419744",
        "https://openalex.org/W6611482171",
        "https://openalex.org/W2970193165",
        "https://openalex.org/W3027173706",
        "https://openalex.org/W2923978210",
        "https://openalex.org/W4226174367",
        "https://openalex.org/W4353007316",
        "https://openalex.org/W4313447114",
        "https://openalex.org/W4287025617",
        "https://openalex.org/W2972991257",
        "https://openalex.org/W4221159672",
        "https://openalex.org/W3207429447",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W2979949198",
        "https://openalex.org/W3206066344",
        "https://openalex.org/W2948740140",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W3177049011",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3167136668",
        "https://openalex.org/W3168323035",
        "https://openalex.org/W339896394",
        "https://openalex.org/W3088409176",
        "https://openalex.org/W4300485781",
        "https://openalex.org/W3208372599",
        "https://openalex.org/W3008110149",
        "https://openalex.org/W3123651166",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W4299568369",
        "https://openalex.org/W3173075577",
        "https://openalex.org/W4362655426",
        "https://openalex.org/W3046441874",
        "https://openalex.org/W4389523706",
        "https://openalex.org/W3166727371",
        "https://openalex.org/W4280568465",
        "https://openalex.org/W3033561598",
        "https://openalex.org/W2939507640",
        "https://openalex.org/W3105625590",
        "https://openalex.org/W4392240262",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3102058420",
        "https://openalex.org/W3105538385",
        "https://openalex.org/W4287888099",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4401042146",
        "https://openalex.org/W3102503200",
        "https://openalex.org/W4378505278",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W4385567093",
        "https://openalex.org/W4307315542",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4287278290"
    ],
    "abstract": "Text classification in natural language processing (NLP) is evolving rapidly, particularly with the surge in transformer-based models, including large language models (LLM). This paper presents an in-depth survey of text classification techniques across diverse benchmarks, addressing applications from sentiment analysis to chatbot-driven question-answering. Methodologically, it utilizes NLP-facilitated approaches such as co-citation and bibliographic coupling alongside traditional research techniques. Because new use cases continue to emerge in this dynamic field, the study proposes an expanded taxonomy of text classification applications, extending the focus beyond unimodal (text-only) inputs to explore the emerging field of multimodal classification. While offering a comprehensive review of text classification with LLMs, this review highlights novel questions that arise when approaching the task with transformers: It evaluates the use of multimodal data, including text, numeric, and columnar data, and discusses the evolution of text input lengths (tokens) for long text classification; it covers the historical development of transformer-based models, emphasizing recent advancements in LLMs; it evaluates model accuracy on 358 datasets across 20 applications, with results challenging the assumption that LLMs are universally superior, revealing unexpected findings related to accuracy, cost, and safety; and it explores issues related to cost and access as models become increasingly expensive. Finally, the survey discusses new social and ethical implications raised when using LLMs for text classification, including bias and copyright. Throughout, the review emphasizes the importance of a nuanced understanding of model performance and a holistic approach to deploying transformer-based models in real-world applications.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nA Survey of Text Classification with\nTransformers: How wide? How large? How long?\nHow accurate? How expensive? How safe?\nJohn Fields1,2, Kevin Chovanec2, and Praveen Madiraju2\n1Concordia University Wisconsin-Ann Arbor, Mequon, WI 53097 USA (e-mail: john.fields@cuw.edu)\n2Department of Computer Science, Marquette University, Milwaukee, WI 53233 USA (e-mail: kevin.chovanec@marquette.edu,\npraveen.madiraju@marquette.edu)\nCorresponding author: John Fields (e-mail: john.fields@cuw.edu).\nABSTRACT Text classification is a basic task in natural language processing (NLP) with applications\nfrom sentiment analysis to question-answering with chat bots. In recent years, transformer-based models\nhave emerged as the prevailing framework in NLP, demonstrating excellent results across many bench-\nmarks. This paper recommends an expanded taxonomy of applications and provides a review of the\nperformance of different models across these applications. The use of traditional research techniques\nplus co-citation and bibliographic coupling provides a comprehensive view of the current and past\nresearch in this area. The study begins by providing an overview of the history of transformer-based\nmodels with an emphasis on recent large language models (LLM). Next, uni-modal (text only) inputs\nand the emerging area of multi-modal classification are discussed to provide a comparison of current and\nemerging research in this area. Gaps are highlighted in the use of multi-modal text/numeric/columnar data\nand recommendations for future research are provided. Finally, the length of text input variables (tokens)\nis reviewed to explore the evolution from short-text to longer document applications. Furthermore, the\naccuracy on 358 datasets across 20 applications is reviewed and unexpected results emerge which show\nthat LLMs are not always the most accurate or least expensive option. In addition to model performance,\nthe safety implications of transformer-based models are reviewed, and a summary of issues related to\nethics, bias, social implications, and copyright are explored.\nINDEX TERMS NLP, text classification, transformers, survey.\nI. INTRODUCTION\nI\nN the past five years, large language models have revo-\nlutionized natural language processing (NLP), achieving\nstate-of-the-art across several classic NLP tasks. One of these\ntasks, text classification, is a diverse and growing set of\naims in academia and industry related to categorizing and\norganizing text. In text classification, the goal is to assign\nsome label, category, or tag to a body of text (sentence,\nparagraph, document). Traditionally, text classification, like\nclassification tasks more generally, can be divided into three\ntypes:\n• Binary classification: classifying texts into one of two\nmutually exclusive categories (for example, Spam or Not\nSpam)\n• Multiclass classification: dividing texts into one of three\nor more mutually exclusive categories (for example,\nclassifying a text’s genre)\n• Multilabel classification: labeling texts with three or\nmore potentially overlapping labels, in which each text\ncan receive multiple labels (such as offensive comments\nlabeling, in which a comment might be marked for both\nviolence and hate speech)\nHowever, as automated text classification has expanded,\ncommon aims and data sources have reappeared often. For\nexample, researchers often work with social media, surveys,\nscraped web data, emails, user reviews or comments, and they\noften attempt similar kinds of classification: sentiment anal-\nysis, news classification, topic labeling, emotion detection,\noffensive language labeling, etc.\nText data offers rich information, but it has historically\nbeen difficult and expensive to process. LLMs, especially\nopen-source LLMs such as BERT, offer generalized models\nthat can greatly facilitate processing this data. An enormous\namount of research over the last half decade has thus been\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nFIGURE 1. Timeline of major transformer-based large language model introductions.\ninvested in deploying, fine-tuning, and adapting LLMs to\ntext classification tasks. In this literature survey, we aim to\nprovide an overview of the various ways LLMs have been\ndeployed for text-classification, along with a new taxonomy\nof the common subtypes and an explanation of the central\nmethods that have appeared in the research.\nOur paper builds on two recent, related literature surveys.\n[1] have published a survey of deep learning text classifi-\ncation methods in 2021, covering research through 2020. In\nthe same year, [2] published a detailed account of pre-trained\nlanguage models used in NLP tasks. We extend the work of\nthese earlier papers, contributing to research in three primary\nways:\n1) First, we fill in the last two years, which, in this rapidly-\ndeveloping field, has seen several new models, ap-\nproaches, and benchmarks.\n2) Next, our survey focuses specifically on text classifi-\ncation and LLMs, which allows us to offer a compre-\nhensive overview of the work done in this area. [1], for\nexample, offers only a relatively short section on LLMs,\nand [2] devote their survey to all uses of LLMs, only\nbriefly discussing classification. Moreover, [2] offer an\nexcellent technical overview of LLMs, covering model\narchitecture and fine-tuning in detail, while our survey\nleans more toward the novel practical and ethical ques-\ntions that researchers have addressed when deploying\nLLMs for text classification.\n3) Finally, based on the articles we survey, we propose an\nexpansion of existing taxonomies of text classification.\nWhile still rooted in traditional tasks such as sentiment\nanalysis, news classification, or topic labeling, recent\napproaches, propelled by the success of LLMs, have\nseen increasingly granular subtasks that appear often\nenough in the literature to merit space in the taxon-\nomy. As multi-modal approaches are perhaps the most\nexciting subfield of text classification, this survey also\ngoes beyond previous surveys to include classification\nmethods that pair text data with other kinds of data.\nBy large language model, we refer to the pre-trained,\ntransformer-based architectures that have been widely\nadopted since [3] seminal work on attention. These language\nmodels, beginning with GPT [4] and BERT [5] in 2018,\nemploy a neural network using a parallel multi-head attention\nmechanism, with either an encoder or encoder-decoder struc-\nture, to transform tokens into embeddings, or word vectors,\nwith billions or even trillions of parameters, which can then\nbe used in downstream tasks. While many of the most famous\ngains from these LLMs have been in classic question-answer\nproblems, they have also been adopted widely for text classifi-\ncation. As Figure 1 shows, LLMs have also recently exploded\nafter the popularity of ChatGPT (GPT-3.5/4), offering new\nopportunities for integrating these recently released models\ninto research on text classification.\nOur survey is organized around the central questions raised\nby the use of transformer-based LLMs for text classification.\nMany of these relate to the training or fine-tuning of the model\nitself, while others relate to the social and ethical questions. In\nthe next section, we present our methodology, which blends\ntraditional survey methods with an approach using novel NLP\ntools. Then, in the third and central section of the work, we\noffer our novel taxonomy of text-classification.\nThe rest of the survey’s body is then organized around\nquestions related to text classification with LLMs as com-\npared to other methods. First, we consider the type of data\nused in the model or the ‘width’ of the model; next, we\nexplore questions related to the size of the model ( in ‘How\nLarge’); then, in ‘How Long,’ we consider the length of\nthe documents being classified; ‘How Accurate’ summarizes\nbenchmarking across text classification subtasks; and ‘How\nSafe’ considers the ethical and legal issues related to text\nclassification with LLM.\nFinally, we offer a conclusion that summarizes our findings\nand suggests directions for future research in this rapidly-\nevolving field.\nII. METHODOLOGY\nWe follow the guidelines for comprehensive literature reviews\nproposed by [6]. We first formulated the research questions\nabove, and then systematically searched for primary studies\nthrough keyword searches focusing on text classification and\nlarge language models, including all LLMs listed in table 3:\nfor example, we searched “BERT” and “Text Classification”;\n“Llama” and “Text Classification”; “GPT” and “Text Clas-\nsification,” etc. Using IEEE Explore and the ACM database,\nthis returned 277 results, after removing duplicates. Initially,\nGoogle Scholar was used as a third database, but the results\nwere almost all either redundant or outside the scope of our\nsurvey, and we therefore narrowed the search to include only\nsources from the IEEE and ACM databases.\nWe then reviewed the primary studies for their quality and\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nrelevance to our topic, and extracted information about the\nmethods, area, and focus of the work. We excluded works not\nprimarily focused on text classification and works not using\na transformer-based, pre-trained large language model. We\nalso excluded works published before 2020, since previous\nsurveys cover deep learning techniques through 2020. After\nremoving articles using our exclusion criteria, we found 231\narticles relevant to the topic. However, many of these papers\noverlapped significantly in focus and methodology, as the last\nthree years have seen a proliferation of work using LLMs for\ntext classification; to avoid redundancy, we chose the most\nrepresentative and most cited works from each category.\nFinally, we supplemented this traditional survey method-\nology in two ways. First, most simply, we used backward\nsnowballing to identify frequently cited papers missed in\nour original search. We also integrated new NLP tools to\nenhance our search, namely, Connected Papers, a visual net-\nworking tool that explores interrelations between research\npapers [6]. Connected Papers uses similarity metrics (rather\nthan citations alone) to link papers together, and for any\npaper searched, it returns a few dozens of the most frequently\nsighted similar papers. As in other studies that have employed\nthis tool for the purposes of a literature review, we chose seed\narticles from our initial search for each section in our paper\nand fed these articles into Connected Papers. An example of\nthe Connected Papers graph and prior/derivative works for [1]\ncan be accessed at [7].\nIII. TAXONOMY OF TEXT CLASSIFICATION\nClassification philosophies have existed for thousands of\nyears to provide structure and order to the world around us [8].\nOnly in the 1950’s and 1960’s did we begin to utilize comput-\ners to aid in the classification of text-based documents, which\nwas a time-consuming and laborious manually task [9]. These\nsystems also relied on handcrafted, rule-based systems that\nhad limited effectiveness and scalability. The developments\nfrom the 1960’s to 1990’s progressed slowly during the “AI\nwinter” and significant progress was not made until the 1990’s\nwith new machine learning techniques and faster computers\n[10]. In recent years, new transformer-based algorithms were\napplied to classification problems in the late 2010’s with great\nsuccess and this expanded the potential use cases for text\nclassification as shown in Table 1.\nThe taxonomy in Table 1 has been sufficient for the early\nperiod of transformer-based text classification but there is a\nneed for improved test datasets and an expanded taxonomy\nof applications [14]. With the advent of ChatGPT and other\nLLMs, the text classification capabilities and applications\nhave expanded, and we propose evolving the above taxonomy\nas shown in Table 2 [15].\nSome additional classification applications that are emerg-\ning are fine-grained sentiment analysis, aspect-based senti-\nment analysis, offensive language detection, intent recogni-\ntion, document classifiers, fake news detection, cross-lingual\nclassification, stance detection, emotion/mental health detec-\ntion, malicious software detection, cause and effect classi-\nTABLE 1. Current taxonomy of text classification\nType Definition\nSentiment analysis Classification based on the polarity\n(positive, negative, or neutral) or\nemotional tone of text.\nNews classification Categorize news articles based on\ncontent and subject matter.\nTopic labeling Identify the main subject or topic\nin text.\nQuestion-answering Provide accurate answers to user\nqueries.\nNatural language\ninference\nDetermine the logical relationship\n(entailment, contradiction, or\nneutral) between sentences.\nDialog act classifica-\ntion\nIdentification of text intention\nsuch as requesting, questioning, or\ninforming.\nNamed entity\nrecognition\nCategorize named entities such\nas people, organizations, and\nlocations.\nSyntactic parsing Analyze the grammatical\nstructure of text to understand\nthe relationship between words and\nphrases.\nSources: [11]–[13]\nfiers, sentence classification, multi-label, multi-modal, and\nother applications. Some of these new areas, like emo-\ntion/mental health detection involve more complex social and\nethical issues that will be described more fully in Section 9.\nHowever, as we develop systems with more “human like” ca-\npabilities, we should consider how these fit into the taxonomy\nand propose rules and guidelines for how we implement these\nsafely, or not at all.\nIV. HOW WIDE?\nIn addition to the applications described in Section 3, we also\nconsider “How wide?” in reference to the different data types\nand methods used for analysis. Text classification tasks with\ntransformers can be categorized as uni-modal or multi-modal.\nA. UNI-MODAL\nUni-modal text classification uses only textual information\nand applies the transformer model to make a classification\nprediction. Many survey papers have focused on traditional\nNLP methods and newer transformer-based methods applied\nto text classification [1], [13], [27], [28]. Since the launch of\nChatGPT in late 2022, there has been a significant increase\nin the research and investment in LLMs. The primary appli-\ncation for these models is typically question-answer chat bots\nwhich is explored more in Section 7. Table 3 below provides\na summary of the major releases of LLMs that can perform\nuni-modal text classification.\nThe primary differentiator for these LLMs is the size\nof the training data and this aspect of LLMs is covered\nin detail in Section 5. The technical differences between\nencoder-decoder/encoder only (BERT-method), or decoder-\nonly (GPT-method) is covered in detail in the papers by [12]\nand [13].\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nTABLE 2. Additional taxonomy categories for text\nclassification\nType Definition Source\nFine-grained\nsentiment analysis\nCapture more nuance\nof sentiment and\nwork better with\nmulticlass problems.\n[16]\nAspect-based\nsentiment analysis\nMore detailed\nsentiment (e.g., not\njust like or dislike\na product but by\nfeature/aspect).\n[17]\nOffensive language\ndetection\nIdentify text that\ncontains abusive,\noffensive, or\ninappropriate content.\n[18]\nIntent recognition Train on dialog to\nunderstand user\nintent.\n[19]\nDocument classifier Use new techniques\nfor longer text\ndocuments.\n[20]\nFake news\nclassification\nIdentify patterns\nto predict true/fake\nnews.\n[21]\nCross-lingual\nclassification\nWork with multiple\nlanguages for a more\nglobal context.\n[22]\nStance detection Determine the\nperspective (support,\nopposition, or\nneutral) toward a\npiece of text.\n[23]\nEmotion/mental\nhealth detection\nAssess the mental\nhealth status of\na person from\nanalyzing text.\n[24]\nSentence classifica-\ntion\nDetermine the class\nof an entire sentence.\n[25]\nMulti-label Assign multiple\nlabels to a single\ninput text.\n[26]\nMulti-modal Utilizing multiple\ntypes of input data\nsuch as text, video,\naudio, etc.\n1\n1 See Section 4.2.\nOne variation is the BloombergGPT model which is trained\non financial data for internal use by the company. This is\nan area for future research to determine if the investment\nin proprietary, domain specific LLM could be a competitive\nadvantage for the companies who make these investments.\nMulti-method approaches for uni-modal text classification\nare also achieving best-in-class results for some applications\nsuch as aspect-based sentiment analysis. The combination of\ngraph database methods with transformer-based text classifi-\ncation is another emerging area in NLP. Table 4 below shows\ntwo recent uses of this technique.\nB. MULTI-MODAL\nMulti-modal classification uses text, video, signal, image,\naudio, and columnar data for classification. A taxonomy for\nmulti-modal machine learning proposed by [43] included\nRepresentation, Translation, Alignment, Fusion, and Co-\nTABLE 3. Large language model examples and release dates\nName Organization Date Source\nBERT Google Oct 2018 [5]\nRoBERTa Meta Jul 2019 [29]\nXL Net Google Dec 2019 [30]\nGPT-3 OpenAI Jun 2020 [31]\nLlama Meta Feb 2023 [32]\nBard(LaMBDA) Bloomberg Mar 2023 [33]\nGPT-4 OpenAI Mar 2023 [34]\nBloombergGPT Bloomberg Mar 2023 [35]\nDolly 2 Databricks Apr 2023 [36]\nStableLM Stability AI Apr 2023 [37]\nTitan Amazon Apr 2023 [38]\nBing Chat Microsoft Apr 2023 [39]\nLlama 2 Meta Jul 2023 [40]\nTABLE 4. Multi-method examples in aspect-based sentiment analysis\nModel Paper\nHGCN+BERT Learn from Structural Scope: Improv-\ning Aspect-Level Sentiment Analysis\nwith Hybrid Graph Convolutional\nNetworks [41]\nHAABSA A Hybrid Approach for Aspect-Based\nSentiment Analysis Using a Lexicalized\nDomain Ontology and Attentional\nNeural Models [42]\nlearning.\nThis is an emerging area of investment by large technology\ncompanies as evidenced by Elon Musk’s goal of creating\nan “everything app” from his new company X.ai [44] and\nGoogle’s refocusing their AI teams on “...a series of power-\nful, multi-modal AI models” [45]. Some multi-modal models,\nsuch as UniMSE [46] for sentiment analysis and SEMI-FND\n[47] for Fake News Detection, are already emerging as best-\nin-class as shown in Table 10 below.\nAs we consider multi-modal classification for text, the\nprior research is primarily focused on the use of image data\nand text [48]. Our hypothesis is that the primary types of\ndata in most companies and organizations is text and nu-\nmeric/columnar data. We were unable to find academic re-\nsearch on the breakdown of data types most common in a\nbusiness context, so we utilized ChatGPT [49] and Bard [50]\nto provide the following estimates.\nTABLE 5. Estimated data types in businesses and other organizations\nType Proportion\nText ≈ 60 − 80%\nNumeric ≈ 15%\nVideo ≈ 5 − 20%\nV oice ≈ 1 − 5%\nNote: ChatGPT would not provide the sources for the\nestimates in Table 5. Bard provided the following sources:\n• The Data & Analytics Association (D&AA) 2022 State\nof Data & Analytics report\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\n• Gartner’s 2022 Magic Quadrant for Data Integration\nTools\n• IDC’s 2022 Worldwide DataSphere Market Forecast\n• Statista’s 2022 statistics on corporate data storage\nIf these estimates are accurate, it would indicate that the\nresearch on multi-modal classification is too focused on\nvideo/voice and not enough on numeric or tabular data. For\nexample, the paper by [51] includes a summary of 46 papers\non Multi-modal Classification with Deep Learning. However,\nonly 1 of these papers by [52] focuses on the use of text and\ntabular data which is commonly found in a variety of business\napplications such as health care and risk classification [53].\nThe more recent 2023 paper by [54] includes a similar list of\n31 multi-modal datasets with a mix of images, text, video, and\naudio but none with numeric/columnar data.\nThere are several recent surveys by [55]–[58] on the use\nof deep learning with columnar data but these focus on cat-\negorical/numeric data and not text. Since there is a gap in\nthe research on multi-modal text and columnar data appli-\ncations, we will provide additional detail on this important\nbut overlooked area of text classification. One example of a\nsolution using text and tabular data is the Multi-modal Toolkit\npackage for Python that was developed by Georgian.io [59].\nAs mentioned previously, the focus on text and video may\nbe of interest to academics due to the availability of datasets\nbut corporations and organizations have access to text and\ncolumnar data with information that could provide valuable\ninsights. However, the benefits of solutions like Multimodal\nToolkit have only shown limited gains by using transformers\non text and numeric data [51]. Since this is only one example,\nwe hope this paper will highlight the need to look at the\nemerging research on using transformers for columnar data\nand the opportunities to synthesize this research with the\nextensive effort and money that is focused on multi-modal\nresearch by technology companies as described above.\nV. HOW LARGE?\n\"Our results strongly suggest that larger models\nwill continue to perform better, and will also be\nmuch more sample efficient than has been previ-\nously appreciated\" [60].\nThe use of transformers for text classification began with\nBERT and GPT in 2018. Data scientists with an interest in\nNLP began to see the types of exponential improvements\nthat the field of vision experienced from neural networks in\n2012 [61]. The success of “small” transformers like BERT\nwith 110-340 million parameters 1 and RoBERTa with 123-\n354 million parameters established many new benchmarks in\ntext classification since 2018.\nThe subsequent creation of the HuggingFace platform\nprovided a platform for data scientists to utilize these\ntransformer-based models [62]. The subsequent launch of\nGoogle Colab and other cloud-based solutions provided sim-\n1Parameters are the weights and biases that are adjusted during training\nand used to make predictions.\npler and more affordable access to Graphical Processing Units\n(GPU) running these compute intensive applications [63].\nThe general public did not have extensive direct exposure\nto transformer-based NLP until the release of ChatGPT in late\n2022. This application achieved a new record by reaching 100\nmillion customers two months after the application launch\n[64]. The ensuing press, investment, and hype over ChatGPT\nhas increased the interest in question-answer based NLP ap-\nplications. However, the use of this technology for other text\nclassification tasks has not received the same attention but\nshould benefit from the increased focus. This paper will also\nsurvey the recent developments and explain how these can be\nleveraged to improve text classification using transformers.\nTable 6 below adds a size element to Table 3 to show the rapid\nincrease in the scale of these LLMs.\nTABLE 6. Size of selected large language models released since ChatGPT\n(Nov 2022)\nName Organization Date Size\nLlama Meta Feb 2023 7B-65B\nBard(LaMBDA) Bloomberg Mar 2023 137B\nGPT-4 OpenAI Mar 2023 ≈ 1T\nBloombergGPT Bloomberg Mar 2023 50B\nDolly 2 Databricks Apr 2023 12B\nStableLM Stability\nAI\nApr 2023 13B\nTitan Amazon Apr 2023 ≈ 45B\nBing Chat Microsoft Apr 2023 Unknown\nLlama 2 Meta Jul 2023 7B-70B\nAlthough the size of the various LLMs continues to grow,\nthere are several other strategic approaches that should be\nconsidered when choosing the best option for text classifi-\ncation. Some of the considerations include:\n• Amount of training data.\n• Privacy and security.\n• Complexity and uniqueness of the task.\n• Scalability.\n• Speed.\n• Compute resources available.\nAs the limiting factor to the growth of LLMs could be the\namount of data available on the internet, these other factors\nmay become more important as we continue to improve these\nmodels [65]. Additional research efforts, like the Pythia suite,\nare providing new tools to analyze LLMs and address this\nissue [66]. Other recent survey papers such as [67], seek to\naddress the issue of how to apply more efficient transformer\nmethods to NLP tasks. Approaches are grouped together by\nsparse, factorized attention, and architectural change. How-\never, [67] concludes there are,\n“...no simple and universal guidelines regarding\nthe current Transformer alternatives.”\nThe cost to develop LLMs is another limiting factor where\nlarge technology companies and research institutions have the\nresources to afford investments in the millions of US dollars\n[68]. The ability to predict the gains in performance could be\nbeneficial to better understand the value of these investments.\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nHowever, the reality is that the “democratization” of this\ntechnology will not occur in the near future until Graphical\nProcessing Units (GPU) are more accessible and affordable\n[69].\nVI. HOW LONG?\nAnother factor to consider for text classification is the length\nof the input text. The original BERT model had a limit of 512\ntokens or ≈ 400 words. Extensions of BERT such as Big\nBird [70] and Longformer [71] extended the token limit to\n4096 to handle longer text such as essays. Most of the new\nLLM’s have implemented tokens ≤ 2048 as shown in Table\n7. However, GPT-4 is the exception with a limit of 8,192\ntokens. This new capability is already being tested in text\nclassification challenges in health care [72] and will likely\nexpand to many other domains. Additional research into hi-\nerarchical attention [73] and long-document summarization\n[74] techniques are emerging as there are many practical\napplications that would benefit from going beyond words and\nsentences to long documents.\nTABLE 7. Token length of selected large language models released since\nChatGPT (Nov 2022)\nName Organization Date Size\nLlama Meta Feb 2023 2,048\nBard(LaMBDA) Bloomberg Mar 2023 ≈ 1000\nGPT-4 OpenAI Mar 2023 8,192\nBloombergGPT Bloomberg Mar 2023 Unknown\nDolly 2 Databricks Apr 2023 2,048\nStableLM Stability AI Apr 2023 75\nTitan Amazon Apr 2023 Unknown\nBing Chat Microsoft Apr 2023 Unknown\nLlama 2 Meta Jul 2023 4,096\nOne of the sensational headlines and subsequent journal\narticle titles was “GPT-4 Passes the Bar Exam” [75]. One\nmajor factor attributed to the improved results from GPT-3 to\nGPT-4 was the increased context window allowing for more\naccurate processing of long sequences of text.\nAnother extension of long text capability is in multi-modal\napplications. The significant research in multi-modal appli-\ncations should also lead to new longer document capabilities\nfor this application. One of the authors of this paper recently\nmodified the Multi-modal Transformers package to add the\ncapability to utilize Longformer for text classification with\ntext up to 4096 tokens for numeric/categorical data [76]. This\nresulted in a .9% increase in the F1 score on the Women’s E-\nCommerce Clothing Review data set compared to using only\n512 tokens. The authors plan an additional paper to publish\nthese results.\nVII. HOW ACCURATE?\nBy utilizing attention mechanisms and self-attention lay-\ners, transformers have demonstrated state-of-the-art accu-\nracy across a variety of text classification tasks. Pre-trained\ntransformers like BERT and GPT have become the standard\napproach for many general text classification tasks such as\nsentiment analysis, document classification, and named en-\ntity recognition. Transfer learning with transformers has also\npushed the boundaries of accuracy by using pre-trained mod-\nels that can be fine-tuned on smaller datasets with excellent\nresults.\nTo understand the accuracy of different models on text clas-\nsification tasks, a review of Papers With Code [77] provided\na summary of the best models from 358 datasets used in 20\ndifferent NLP classification tasks (as shown in Table 1,2).\nPapers With Code was chosen since it is the most complete\nsource to date on benchmarks in NLP and other machine\nlearning tasks, although it should be noted that this source\nis not comprehensive as described by [78]. The methodology\nto determine if the model is \"transformer-based\" was to re-\nview the abstracts and search for the keywords \"transformer\",\n\"attention\", \"BERT\", or \"GPT\". If a model used attention\nor was \"transformer-like\", it was classified as a transformer-\nbased model in Table 10 below. Overall, the transformer based\nmodels are ≈ 68% of the \"best models\".\nTo simplify the resulting analysis, any data set that had\nonly 1 Best Model by application category was not included\nin the results shown in Table 10. This reduced the number\nof datasets from 358 to 151 and applications from 20 to\n15. One unexpected observation is that the new LLM’s are\nalmost exclusively the top models in the question-answer\napplication. For the other 14 applications, many of the best\nmodels are transformer-based but the results also include non-\ntransformer models like XGBoost and Spark NLP. This could\nbe because the new LLM’s work best on question-answer\ntasks or because they are so new that tests have not been\nconducted on a wide range of applications.\nHere are some guidelines for choosing the best model for\ntext classification based on accuracy:\n1) Review the list in Table 10 by application and use this\nas a guide for models to consider.\n2) If your application is not question-answer, BERT (and\nthe derivative models) work well across a variety of\napplications.\n3) Try the new family of LLM’s on your task as these will\nlikely continue to improve with the intense funding and\nresearch.\n4) If you have the resources available, develop a custom\nfine-tuned model or build your own LLM (more on this\nin the next section).\nVIII. HOW EXPENSIVE?\nThe expense of transformers used for text classification varies\ndepending on several factors such as hardware, software, cost\nof personnel, and the type of model. The cloud computing\ncost estimates for early transformer models like BERT and\nGPT-2 were $2074 to $43,008 [79]. The cost for newer LLMs\nis an open research question. When OpenAI’s ChatGPT was\nasked the cost of GPT-3, the response was “... it is widely\nbelieved that the training cost for GPT-3 is in the range of\ntens of millions of dollars.” Google’s Bard was asked a similar\nquestion and the response was “The exact cost to train Bard\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nis not publicly known, but it is estimated to be in the millions\nof dollars.”\nThe cost to train these LLMs and the economic/environmental\nconcerns has emerged as a significant issue in recent years.\nHowever, most of the research to alleviate this issue has been\nfocused on the efficiency of these models as illustrated in\nTable 8.\nTABLE 8. Current research on the efficiency of large language models\nAuthor Date Paper\nSubramanyam et al 2021 AMMUS: A Survey of\nTransformer-based\nPretrained Models\nin Natural Language\nProcessing [80]\nRae et al 2021 Scaling Language\nModels: Methods,\nAnalysis & Insights from\nTraining Gopher [81]\nArtetxe et al 2021 Efficient Large Scale\nLanguage Modeling with\nMixtures of Experts [82]\nHoffman et al 2022 Training Compute-\nOptimal Large Language\nModels [83]\nBorgeaud et al 2022 Improving Language\nModels by Retrieving from\nTrillions of Tokens [84]\nCreative solutions to the economic and environmental is-\nsues remains an open challenge for organizations and re-\nsearchers. It is our hope that more attention will be focused\non how to improve the access and sustainability.\nIX. HOW SAFE?\nThe increasing popularity of LLM across several NLP tasks\nhas also raised novel questions about the safety and ethics of\nuse, including issues related to privacy and security, fairness\nand equity, and copyright.\nA primary concern of research into the security of large\nlanguage models has been memorization, or the model’s abil-\nity to repeat content from training verbatim [85]. [86] have\nshown memorization becomes more frequent as the size of\nthe model increases, or with a higher number of duplicates,\nand the project memorization will become more prevalent as\nmodels continue to expand.\nA technical concern stemming from memorization is\ndownstream data contamination, or when the pretrained cor-\npus contains some of the material from the test set. Multiple\nstudies have shown this to be the case in major training\ndatasets, meaning LLM performance on benchmarking tasks\nmay be misleading [31], [87]. [88] attempt to measure the\nimpact of this contamination on classification tasks.\nArguably more problematically, memorization can also\nopen these models to adversarial attacks. [89] have shown the\nuse of web-scraped data has led to mining private information\nfrom LLMs, with duplication in the training data making\nthe model more vulnerable. [85] demonstrated GPT-2’s vul-\nnerability to adversarial attack, noting the public, personal\ninformation, including names, phone numbers, and email\naddresses can be extracted verbatim from the language model,\nand also finding that larger models were more vulnerable\nthan smaller models. [90] similarly found LLMs risked leak-\ning personal information, though they distinguish between\nmemorization and association, arguing that models do not\npose significant risk because of their low association, since\nprivate information will only be leaked randomly rather than\nextracted. As these models are frequently used in email, text,\nand code auto-completion, they also risk revealing personal\ndata when fine-tuned for sensitive tasks. [91] have explored\nhow fine-tuning a model impact the risks to privacy, observing\nthat fine-tuning the head of the model leaves it most suscep-\ntible to extraction attacks.\nOn the other hand, [92] attempted an adversarial attack on\nBERT to mine patient names and conditions from clinical\nnotes in pre-training and found that the model did not mean-\ningfully associate names and conditions.\nEven if the risk of specific data leakage is low, several\nscholars have begun calling for privacy-preserving LLMs,\neither by adapting the training to ensure privacy or removing\nsensitive information from the training text [93]–[98].\nThe memorization of demographic features also presents\nissues in terms of potential bias in downstream tasks. Other\nscholars, interested in fairness and equity rather than the\nprivacy and security of these LLMs, have explored the extent\nto which LLMs have learned protected demographic features\nin training [99]–[101] and, relatedly, how this learned bias\nmight impact downstream classification tasks. Perhaps most\nresearch in this area has been devoted to inequities in how the\nlanguage models handle gender. For example, [101] found\ngender bias impacted classification results when working\nwith medical text. Similarly, several scholars have shown\nthat large language models tend to classify text written by\nwomen as more emotional [99], [102]–[104] . In addition\nto gender, other scholarship notes bias when dealing with\nrace [105]. [106] recently attempted to quantify the extent\nto which BERT has incorporated demographic information,\nusing BERT outputs in a logistic regression model to predict\nsensitive features.\nBecause of these known biases embedded in many pre-\ntrained language models, scholars have also explored ways\nof reducing or mitigating this bias. Generally, this takes place\nduring fine-tuning. [103], for example, developed a method\nfor identifying and removing semantic features which con-\ntained sensitive information. [101] incorporated loss during\ntraining to minimize bias learned during fine tuning. Others\nattempted to modify the data used for fine tuning, [107]–\n[109], removing traits that indicate gender [110], [111] , or\nrace [104]. A third method uses ensemble methods [105],\nand, most recently, [106] use active sampling of protected-\nattribute-uninformative data, which was then used to fine-\ntune the model, also reducing downstream inequalities.\nThis bias stemmed in part from the kind of data used when\nbuilding the language model [110], [112], [113] and research\nhas recently highlighted the fact that almost all training data\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nfor these large language models remains closed as another\nsecurity concern. When training data is known, as [87] have\nshown with the C4 dataset, the filtering applied often dis-\nproportionately removes texts from and about minoritized\ngroups. [114] demonstrate that GPT-2 was more likely to\ndetect negative sentiment in texts written in African Amer-\nican English; [115], investigating toxic content produced by\nLLMs, argue that the cause of this toxic content can be found\nin the training datasets, exploring two corpora used to train\nLLM including GPT-2, which both contain toxic content.\n[116] note that these learned biases also impact downstream\nQuestion and Answering tasks.\nAs language models have grown larger and more prof-\nitable, they have also increasingly become proprietary, mak-\ning potential issues in the data regarding personal privacy of\ngeneral equity more difficult to explore. Many of even the\n‘open’ large language models, shown in Table 9, have not\nreleased information about their training. [117] have recently\nshown the lack of openness and transparency in ChatGPT,\nLlama, and other large language models, providing structure\nfor discussing degrees of openness, noting for example that\neven if a model can be used, many crucial aspects of the\nmodel remain closed, such as the training data, processes for\ninstruction tuning, and code used to train the model.\nBecause full details on training are not available, some have\nalso accused these models of incorporating copyright materi-\nals, potentially violating intellectual property laws. [118], for\nexample, have very recently shown that both BERT and GPT-\n4 know a wide range of copyright material. Further, [119]\nhave argued that training data must be made open to assess\nsources of bias, thus dovetailing the concerns over fairness\nand equity with concerns over intellectual property.\nRelating to the broader concerns over bias and fairness,\nsignificant research has gone into incorporating eXplainable\nArtificial Intelligence (XAI) methos into LLMs, especially\nwhen use in downstream classification tasks.\nFor example, there have been various approaches to mod-\nifying or highlighting BERT’s architecture to make outputs\nexplainable. ExBERT, for example, offers a dashboard that\noverviews the model’s attention and internal representation\n[120]. [121] similarly developed VisBERT, which tracks to-\nkens as they are processed by BERT. They extract hidden\nstates from each transformer block and apply Principal Com-\nponent Analysis to map tokens to 2d state where distance rep-\nresents semantic similarity. Transformers Interpret, a Python\npackage, uses Integrated Gradients to determine and visualize\nthe significance of words in any task done with pre-trained\nlanguage models [122].\nAnother approach is to use post hoc, model agnostic ex-\nplainability methods in classification. [123] use LIME an\nAnchors to explain “Fake News” classifications made with\nBERT, highlighting words that have the highest contribution\nto the classification result. [124] apply “explanations-by-\nexample” to a BERT-based model, using the twin-systems\napproach (that is, pairing a white-box model with the black-\nbox BERT model). They use a Case-based reasoning (CBR)\nmodel.\nFinally, several researchers have incorporated XAI into\ntext classification task, even if the main focus of their work\nwas not on explainability approaches. These approaches have\nbeen deployed in almost every category of the novel tax-\nonomy we propose above. For example, [125] apply gra-\ndientSHAP to a multimodal model using BERT for emo-\ntion classification. In news classification, [126] have added\nLIME to a BERT-based classifier detecting misinformation\nabout COVID-19, which show the users how the decision\nwas reached which data sources were used to make the clas-\nsification, extracting sentences from relative news articles\nto explain the classification. Similar approaches have been\nemployed in the medical field [127] and analysis of online\nreviews [128].\nTABLE 9. Transparency of selected large language models released since\nChatGPT (Nov 2022)\nName Organization Date Open/Closed\nLlama Meta Feb 2023 Closed\nBard(LaMBDA) Bloomberg Mar 2023 Closed\nGPT-4 OpenAI Mar 2023 Closed\nBloombergGPT Bloomberg Mar 2023 Closed\nDolly 2 Databricks Apr 2023 Open\nStableLM Stability\nAI\nApr 2023 Open (but uses\nLlama)\nTitan Amazon Apr 2023 Closed\nBing Chat Microsoft Apr 2023 Closed\nLlama 2 Meta Jul 2023 Open\nX. CONCLUSION AND FURTHER RESEARCH\nIn conclusion, this paper has provided a unique exploration\ninto the use of text classification with transformer-based mod-\nels. The transformative impact of transformers on NLP tasks,\nparticularly text classification, is evident through their ability\nto capture complex contextual relationships and semantic nu-\nances. Throughout this study, we have examined the questions\nof How wide? How large? How long? How accurate? How\nexpensive? and How safe? as it applies to this new technology.\nThe comparative analysis conducted on various trans-\nformer models across diverse datasets underscores their re-\nmarkable performance and versatility across many but not all\ntext classification tasks. The current hype related to the use of\nLLMs for question-answering chat bots is understandable as\nthis capability moves AI closer to human-level performance\non this task. However, this is only 1 of 20 applications re-\nviewed in this paper and similar gains are likely possible\nin other areas. It is unclear at this point if the transformer\ntechnologies will be applied to these other applications or if\nnew techniques will emerge.\nChallenges such as computational requirements, model\nsize, and potential biases present in pre-trained represen-\ntations call for continued research and innovation. Efforts\nto address these challenges, alongside emerging techniques\nfor model interpret-ability and explain-ability, pave the way\nfor more responsible and ethically sound applications of\ntransformer-based text classification.\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nAs researchers and practitioners alike, we must strive to\nharness the full potential of transformers while staying vigi-\nlant to ethical considerations and the broader societal impact\nof our work. The insights gained from this paper serve as\na foundation for future advancements in the field, guiding\nus towards a deeper understanding of transformer-based text\nclassification and its implications for the ever-evolving land-\nscape of natural language processing.\nIn the future, potential areas for further research related to\ntransformer-based text classification include:\n• Multimodal classification using text and colum-\nnar/numeric data.\n• Multiclass classification applications.\n• Changes over time (drift).\n• Cost and access issues.\n• Legal issues related to copyrighted data used in training.\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nTABLE 10. Best models by application on 151 datasets with Count of Best Model≥ 2 for text classification tasks from Papers with Code (24 Jul 2023)\nApplication Model Count Transformer-\nBased\nAspect-Based Sentiment Analysis InstructABSA [129] 4 N\nHGCN+BERT [41] 4 Y\nMvP(multi-task) [130] 3 Y\nBERT-pair-QA-B [131] 2 Y\nDocument Classification MPAD-path [132] 3 Y\nApproxRepSet [133] 3 N\nRMDL(30 RDLs) [134] 2 N\nBilBOWA [135] 2 N\nKD-LSTMreg [136] 2 Y\nMulti-Label Text Classification XGBoost [137] 3 N\nLAHA [138] 3 Y\nBERT [139], [140] 2 Y\nMAGNET [141] 2 Y\nMulti-Modal Sentiment Analysis UniMSE [46] 2 Y\nNamed Entity Recognition Spark NLP [142] 4 N\nACE + document-context [143] 4 Y\nBLSTM-CNN-Char(SparkNLP) [142] 3 N\nDeepStruct multi-task w/finetune [144] 3 Y\nHGN [145] 2 Y\nConNER [146] 2 N\nOurs: cross-sentence ALB [147] 2 Y\nNatural Language Inference Human Benchmark [148] 3 Y\nERNIE 2.0 Large [149] 2 Y\nPaLM 540B(finetuned) [150] 2 Y\nNeuralLog [151] 2 N\nQuestion-Answer Bing Chat [152] 8 Y\nFLAN 137B zero-shot [153] 4 Y\nChatGPT [152], [154] 4 Y\nmonoT5-3B [155] 3 Y\nHuman Benchmark [156] 3 Y\nPaLM 540B(Self) [157] 2 Y\nPaLM 2-L(one-shot) [150] [158] 2 Y\nBioLinkBERT(large) [159] 2 Y\nXLNet(single-model) [30] 2 Y\nPaLM 540B (finetuned) [150] 2 Y\nBigBird-etc [70] 2 Y\nUnitedQA [160] 2 Y\nMa et al.-ELECTRA [161] 2 Y\nFast Weight Memory [162] 2 N\nLongformer [163] 2 Y\nSentence Classification SciBERT(SciV ocab [164] 2 Y\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nTABLE 11. Table 10 continued\nApplication Model Count Transformer-\nBased\nSentiment Analysis XLNet [30] 3 Y\nAraBERTv1 [165] 3 Y\nRoBERTA-wwm-ext-large [166] 2 Y\nCNN-LSTM [167] 2 N\nxlmindic-base-uniscript [168] 2 Y\nBERT large [169] 2 Y\nLSTMs+CNNs ensemble [170] 2 N\nFinBERT [171] 2 Y\nSyntactic Parsing ACE [143] 4 Y\nTopic Labeling JoSH [172] 2 N\nFake News Detection SEMI-FND [47] 2 Y\nCross-Lingual Document Classification XLMft UDA [173] 5 N\nBiinclusion [174] 2 N\nMultiFiT, pseudo [175] 2 N\nStance Detection MUSE + UMAP [176] 2 N\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nREFERENCES\n[1] Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad,\nMeysam Chenaghlu, and Jianfeng Gao. Deep learning–based text classi-\nfication: A comprehensive review. ACM Comput. Surv., 54(3):1–40, April\n2021.\n[2] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh,\nThien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan\nRoth. Recent advances in natural language processing via large pre-\ntrained language models: A survey. ACM Comput. Surv. , 56(2):1–40,\nSeptember 2023.\n[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is\nall you need. Adv. Neural Inf. Process. Syst. , 30, 2017.\n[4] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. https://www.cs.ubc.ca/ amuham01/LING530\n/papers/radford2018improving.pdf. Accessed: 2023-6-29.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-training of deep bidirectional transformers for language un-\nderstanding. October 2018.\n[6] Barbara Kitchenham and Stuart Charters. Guidelines for performing\nsystematic literature reviews in software engineering. 2, 01 2007.\n[7] Connected papers. https://www.connectedpapers.com. Accessed: 2023-\n9-18.\n[8] Paul Studtmann. Aristotle’s categories. In Edward N Zalta, editor, The\nStanford Encyclopedia of Philosophy . Stanford University - Metaphysics\nResearch Lab, Stanford, California, USA, 2021.\n[9] M E Maron. Automatic indexing: An experimental inquiry. J. ACM ,\n8(3):404–417, July 1961.\n[10] Thierry Poibeau. The 1966 ALPAC report and its consequences. In Ma-\nchine Translation, pages 75–89. MIT Press, Cambridge, Massachusetts,\nUSA, 2017.\n[11] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement\nDelangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan\nFuntowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma,\nYacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander Rush. Transformers:\nState-of-the-Art natural language processing. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online, October 2020. Association\nfor Computational Linguistics.\n[12] Qian Li, Hao Peng, Jianxin Li, Congying Xia, Renyu Yang, Lichao Sun,\nPhilip S Yu, and Lifang He. A survey on text classification: From shallow\nto deep learning. August 2020.\n[13] Andrea Gasparetto, Matteo Marcuzzo, Alessandro Zangari, and Andrea\nAlbarelli. A survey on text classification algorithms: From text to\npredictions. Information, 13(2):83, February 2022.\n[14] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,\nZhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik\nRingshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem,\nPontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina\nWilliams. Dynabench: Rethinking benchmarking in NLP. April 2021.\n[15] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang,\nJiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao\nWu, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu,\nand Bao Ge. Summary of ChatGPT/GPT-4 research and perspective\ntowards the future of large language models. April 2023.\n[16] Manish Munikar, Sushil Shakya, and Aakash Shrestha. Fine-grained\nsentiment classification using BERT. October 2019.\n[17] Hai Ha Do, P W C Prasad, Angelika Maag, and Abeer Alsadoon. Deep\nlearning for Aspect-Based sentiment analysis: A comparative review.\nExpert Syst. Appl. , 118:272–299, March 2019.\n[18] Abdelrahim Elmadany, Chiyu Zhang, Muhammad Abdul-Mageed, and\nAzadeh Hashemi. Leveraging affective bidirectional transformers for\noffensive language detection. May 2020.\n[19] Jianguo Zhang, Kazuma Hashimoto, Yao Wan, Zhiwei Liu, Ye Liu,\nCaiming Xiong, and Philip S Yu. Are pretrained transformers robust in\nintent classification? a missing ingredient in evaluation of Out-of-Scope\nintent detection. June 2021.\n[20] Michihiro Yasunaga, Jure Leskovec, and Percy Liang. LinkBERT: Pre-\ntraining language models with document links. March 2022.\n[21] Heejung Jwa, Dongsuk Oh, Kinam Park, Jang Mook Kang, and Heuiseok\nLim. exBAKE: Automatic fake news detection model based on bidirec-\ntional encoder representations from transformers (BERT). NATO Adv. Sci.\nInst. Ser. E Appl. Sci. , 9(19):4062, September 2019.\n[22] Cindy Wang and Michele Banko. Practical transformer-based multilin-\ngual text classification. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies: Industry Papers , pages 121–129,\nOnline, June 2021. Association for Computational Linguistics.\n[23] Bowen Zhang, Daijun Ding, and Liwen Jing. How would stance detection\ntechniques evolve after the launch of ChatGPT? December 2022.\n[24] Candida M Greco, Andrea Simeri, Andrea Tagarelli, and Ester Zumpano.\nTransformer-based language models for mental health issues: A survey.\nPattern Recognit. Lett. , 167:204–211, March 2023.\n[25] Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, and Daniel S\nWeld. Pretrained language models for sequential sentence classification.\nSeptember 2019.\n[26] Wei-Cheng Chang, Hsiang-Fu Yu, Kai Zhong, Yiming Yang, and In-\nderjit S Dhillon. Taming pretrained transformers for extreme multi-\nlabel text classification. In Proceedings of the 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining , KDD\n’20, pages 3163–3171, New York, NY , USA, August 2020. Association\nfor Computing Machinery.\n[27] Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana\nMendu, Laura Barnes, and Donald Brown. Text classification algorithms:\nA survey. Information, 10(4):150, April 2019.\n[28] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and\nXuanjing Huang. Pre-trained models for natural language processing:\nA survey. Sci. China Tech. Sci. , 63(10):1872–1897, 2020.\n[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\nRoBERTa: A robustly optimized BERT pretraining approach. July 2019.\n[30] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R\nSalakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pre-\ntraining for language understanding. Adv. Neural Inf. Process. Syst. , 32,\n2019.\n[31] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, and Others. Language models are few-shot\nlearners. Adv. Neural Inf. Process. Syst. , 33:1877–1901, 2020.\n[32] Introducing LLaMA: A foundational, 65-billion-parameter language\nmodel. https://ai.facebook.com/blog/large-language-model-llama-meta-\nai/. Accessed: 2023-7-5.\n[33] Sundar Pichai. An important next step on our AI journey.\nhttps://blog.google/technology/ai/bard-google-ai-search-updates/, Febru-\nary 2023. Accessed: 2023-7-5.\n[34] OpenAI. GPT-4 technical report. March 2023.\n[35] Introducing BloombergGPT, bloomberg’s 50-billion parameter\nlarge language model, purpose-built from scratch for finance.\nhttps://www.bloomberg.com/company/ press/bloomberggpt-50-billion-\nparameter-llm-tuned-finance/, March 2023. Accessed: 2023-7-5.\n[36] Free dolly: Introducing the world’s first truly open instruction-tuned\nLLM. https://www.databricks.com/blog/2023/ 04/12/dolly-first-open-\ncommercially-viable-instruction-tuned-llm, April 2023. Accessed: 2023-\n7-5.\n[37] StableLM: StableLM: Stability AI language models.\n[38] Amazon titan. https://aws.amazon.com/bedrock/titan/. Accessed: 2023-\n7-5.\n[39] Bing chat. https://www.microsoft.com/en-us/edge/features/bing-\nchat?form=MT00D8. Accessed: 2023-7-5.\n[40] Llama 2. https://ai.meta.com/llama/. Accessed: 2023-7-25.\n[41] Lvxiaowei Xu, Xiaoxuan Pang, Jianwang Wu, Ming Cai, and Jiawei Peng.\nLearn from structural scope: Improving aspect-level sentiment analysis\nwith hybrid graph convolutional networks. Neurocomputing, 518:373–\n383, January 2023.\n[42] Olaf Wallaart and Flavius Frasincar. A Hybrid Approach for Aspect-Based\nSentiment Analysis Using a Lexicalized Domain Ontology and Attentional\nNeural Models. Springer International Publishing, New York, NY , USA,\n2019.\n[43] Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Mul-\ntimodal machine learning: A survey and taxonomy. IEEE Trans. Pattern\nAnal. Mach. Intell. , 41(2):423–443, February 2019.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\n[44] Berber Jin and Deepa Seetharaman. Elon musk creates new artificial\nintelligence company x.ai. Wall St. J. (East Ed) , April 2023.\n[45] Sundar Pichai. Google DeepMind: Bringing together two world-class\nAI teams. https://blog.google/technology/ai/april-ai-update/, April 2023.\nAccessed: 2023-7-6.\n[46] Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan Wu, and\nYongbin Li. UniMSE: Towards unified multimodal sentiment analysis\nand emotion recognition. November 2022.\n[47] Prabhav Singh, Ridam Srivastava, K P S Rana, and Vineet Kumar. SEMI-\nFND: Stacked ensemble based multimodal inference for faster fake news\ndetection. May 2022.\n[48] Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Mul-\ntimodal machine learning: A survey and taxonomy. IEEE Trans. Pattern\nAnal. Mach. Intell. , 41(2):423–443, February 2019.\n[49] ChatGPT. https://chat.openai.com. Accessed: 2023-7-31.\n[50] Try bard, an AI experiment by google. https://bard.google.com. Ac-\ncessed: 2023-7-31.\n[51] William C Sleeman, IV , Rishabh Kapoor, and Preetam Ghosh. Multi-\nmodal classification: Current landscape, taxonomy and future directions.\nSeptember 2021.\n[52] Keyang Xu, Mike Lam, Jingzhi Pang, Xin Gao, Charlotte Band, Piyush\nMathur, Frank Papay, Ashish K Khanna, Jacek B Cywinski, Kamal Ma-\nheshwari, Pengtao Xie, and Eric P Xing. Multimodal machine learning\nfor automated ICD coding. In Finale Doshi-Velez, Jim Fackler, Ken Jung,\nDavid Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens, editors,\nProceedings of the 4th Machine Learning for Healthcare Conference ,\nvolume 106 of Proceedings of Machine Learning Research , pages 197–\n215, Maastricht, Netherlands, 2019. PMLR.\n[53] Niklas Holtz and Jorge Marx Gomez. Multimodal transformer for risk\nclassification: Analyzing the impact of different data modalities. In\nNatural Language Processing and Machine Learning , Zurich, Switzer-\nland, May 2023. Academy and Industry Research Collaboration Center\n(AIRCC).\n[54] Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-\nYong Wei, Yaowei Wang, Yonghong Tian, and Wen Gao. Large-scale\nmulti-modal pre-trained models: A comprehensive survey. Machine\nIntelligence Research, June 2023.\n[55] G Badaro, M Saeed, and P Papotti. Transformers for tabular data repre-\nsentation: A survey of models and applications. Trans. Assoc. Comput.\nLinguist.\n[56] Vadim Borisov, Tobias Leemann, Kathrin Sessler, Johannes Haug, Martin\nPawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data:\nA survey. IEEE Trans Neural Netw Learn Syst , PP, December 2022.\n[57] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\nRevisiting deep learning models for tabular data. Adv. Neural Inf. Process.\nSyst., 34:18932–18943, 2021.\n[58] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not\nall you need. Inf. Fusion, 81:84–90, May 2022.\n[59] Ken Gu and Akshay Budhkar. A package for learning on tabular and\ntext data with transformers. In Proceedings of the Third Workshop on\nMultimodal Artificial Intelligence , pages 69–73, Mexico City, Mexico,\nJune 2021. Association for Computational Linguistics.\n[60] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin\nChess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. Scaling laws for neural language models. January 2020.\n[61] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet\nclassification with deep convolutional neural networks. Commun. ACM,\n60(6):84–90, May 2017.\n[62] Shashank Mohan Jain. Introduction to Transformers for NLP: With the\nHugging Face Library and Models to Solve Problems . Apress, New York,\nNew York, USA, October 2022.\n[63] Ekaba Bisong. Google colaboratory. In Building Machine Learning and\nDeep Learning Models on Google Cloud Platform , pages 59–64. Apress,\nBerkeley, CA, 2019.\n[64] Dan Milmo. ChatGPT reaches 100 million users two months after launch.\nThe Guardian, February 2023.\n[65] Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao,\nAleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and\nColin Raffel. Scaling Data-Constrained language models. May 2023.\n[66] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley,\nKyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu\nPurohit, Usvsn Sai Prashanth, Edward Raff, Aviya Skowron, Lintang\nSutawika, and Oskar van der Wal. Pythia: A suite for analyzing large\nlanguage models across training and scaling. April 2023.\n[67] Quentin Fournier, Gaétan Marceau Caron, and Daniel Aloise. A practical\nsurvey on faster and lighter transformers. ACM Comput. Surv., 55(14s):1–\n40, December 2023.\n[68] Chuan Li. OpenAI’s GPT-3 language model: A technical overview.\nhttps://lambdalabs.com/blog/demystifying-gpt-3, June 2020. Accessed:\n2023-7-17.\n[69] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng,\nHaoming Jiang, Bing Yin, and Xia Hu. Harnessing the power of LLMs\nin practice: A survey on ChatGPT and beyond. April 2023.\n[70] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris\nAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. Big bird: Transformers for longer sequences.\nJuly 2020.\n[71] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The Long-\nDocument transformer. April 2020.\n[72] Zhengliang Liu, Xiaowei Yu, Lu Zhang, Zihao Wu, Chao Cao, Haixing\nDai, Lin Zhao, Wei Liu, Dinggang Shen, Quanzheng Li, Tianming Liu,\nDajiang Zhu, and Xiang Li. DeID-GPT: Zero-shot medical text De-\nIdentification by GPT-4. March 2023.\n[73] X Zhang, F Wei, and M Zhou. HIBERT: Document level pre-training\nof hierarchical bidirectional transformers for document summarization.\narXiv preprint arXiv:1905.06566 , 2019.\n[74] Xiang Dai, Ilias Chalkidis, Sune Darkner, and Desmond Elliott. Revis-\niting transformer-based models for long document classification. April\n2022.\n[75] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo\nArredondo. GPT-4 passes the bar exam. March 2023.\n[76] Multimodal-Toolkit: Multimodal model for text and tabular data with\nHuggingFace transformers as building block for text data.\n[77] Papers with code - the latest in machine learning.\nhttp://paperswithcode.com. Accessed: 2023-7-27.\n[78] Fernando Martínez-Plumed, Pablo Barredo, Seán Ó hÉigeartaigh, and\nJosé Hernández-Orallo. Research community dynamics behind popular\nAI benchmarks. Nature Machine Intelligence , 3(7):581–589, May 2021.\n[79] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and\npolicy considerations for deep learning in NLP. June 2019.\n[80] Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivanesan\nSangeetha. AMMUS : A survey of transformer-based pretrained models\nin natural language processing. arXiv e-prints , page arXiv:2108.05542,\nAugust 2021.\n[81] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan\nHoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring,\nSusannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Al-\nbin Cassirer, Richard Powell, George van den Driessche, Lisa Anne\nHendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes\nWelbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor,\nIrina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Suther-\nland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gri-\nbovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-\nBaptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz,\nThibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel\nToyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir\nMikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura\nWeidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero,\nLaura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway,\nLorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey\nIrving. Scaling language models: Methods, analysis & insights from\ntraining gopher. December 2021.\n[82] Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott,\nSam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth\nPasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Man-\ndeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo,\nJeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves\nStoyanov. Efficient large scale language modeling with mixtures of\nexperts. December 2021.\n[83] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena\nBuchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas,\nLisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan,\nEric Noland, Katie Millican, George van den Driessche, Bogdan Damoc,\nAurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nRae, Oriol Vinyals, and Laurent Sifre. Training Compute-Optimal large\nlanguage models. March 2022.\n[84] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza\nRutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren\nMaggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini,\nGeoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack\nRae, Erich Elsen, and Laurent Sifre. Improving language models by\nretrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie\nJegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,\nProceedings of the 39th International Conference on Machine Learning ,\nvolume 162 of Proceedings of Machine Learning Research , pages 2206–\n2240, Maastricht, Netherlands, 2022. PMLR.\n[85] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel\nHerbert-V oss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song,\nÚlfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training\ndata from large language models. In 30th USENIX Security Symposium\n(USENIX Security 21) , pages 2633–2650, 2021.\n[86] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee,\nFlorian Tramer, and Chiyuan Zhang. Quantifying memorization across\nneural language models. February 2022.\n[87] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel\nIlharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Docu-\nmenting large webtext corpora: A case study on the colossal clean crawled\ncorpus. April 2021.\n[88] Inbal Magar and Roy Schwartz. Data contamination: From memorization\nto exploitation. March 2022.\n[89] Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training\ndata mitigates privacy risks in language models. In Kamalika Chaud-\nhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan\nSabato, editors, Proceedings of the 39th International Conference on\nMachine Learning , volume 162 of Proceedings of Machine Learning\nResearch, pages 10697–10707, Maastricht, Netherlands, 2022. PMLR.\n[90] Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large Pre-\nTrained language models leaking your personal information? May 2022.\n[91] Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David\nEvans, and Taylor Berg-Kirkpatrick. An empirical analysis of memoriza-\ntion in fine-tuned autoregressive language models. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing ,\npages 1816–1826, Abu Dhabi, United Arab Emirates, December 2022.\nAssociation for Computational Linguistics.\n[92] Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, and Byron C\nWallace. Does BERT pretrained on clinical notes reveal sensitive data?\nApril 2021.\n[93] Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manu-\nrangsi. Large-Scale differentially private BERT. August 2021.\n[94] Xuechen Li, Florian Tramèr, Percy Liang, and Tatsunori Hashimoto.\nLarge language models can be strong differentially private learners. Oc-\ntober 2021.\n[95] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan,\nGautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas\nWutschitz, Sergey Yekhanin, and Huishuai Zhang. Differentially private\nfine-tuning of language models. October 2021.\n[96] Weiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, and Zhou Yu. Selective\ndifferential privacy for language modeling. August 2021.\n[97] Shlomo Hoory, Amir Feder, Avichai Tendler, Sofia Erell, Alon Peled-\nCohen, Itay Laish, Hootan Nakhost, Uri Stemmer, Ayelet Benjamini,\nAvinatan Hassidim, and Yossi Matias. Learning and evaluating a differen-\ntially private pre-trained language model. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021 , pages 1178–1189, Punta\nCana, Dominican Republic, November 2021. Association for Computa-\ntional Linguistics.\n[98] Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza\nShokri, and Florian Tramèr. What does it mean for a language model to\npreserve privacy? In Proceedings of the 2022 ACM Conference on Fair-\nness, Accountability, and Transparency , FAccT ’22, pages 2280–2292,\nNew York, NY , USA, June 2022. Association for Computing Machinery.\n[99] Xisen Jin, Francesco Barbieri, Brendan Kennedy, Aida Mostafazadeh\nDavani, Leonardo Neves, and Xiang Ren. On transferability of bias\nmitigation effects in language model Fine-Tuning. October 2020.\n[100] Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anu-\npam Datta. Gender bias in neural natural language processing. In Vivek\nNigam, Tajana Ban Kirigin, Carolyn Talcott, Joshua Guttman, Stepan\nKuznetsov, Boon Thau Loo, and Mitsuhiro Okada, editors, Logic, Lan-\nguage, and Security: Essays Dedicated to Andre Scedrov on the Occasion\nof His 65th Birthday , pages 189–202. Springer International Publishing,\nCham, 2020.\n[101] Andrew Silva, Pradyumna Tambwekar, and Matthew Gombolay. Towards\na comprehensive understanding and accurate evaluation of societal biases\nin Pre-Trained transformers. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages 2383–2389, Online,\nJune 2021. Association for Computational Linguistics.\n[102] Samia Touileb, Lilja Øvrelid, and Erik Velldal. Using gender- and\nPolarity-Informed models to investigate bias. In Proceedings of the 3rd\nWorkshop on Gender Bias in Natural Language Processing , pages 66–74,\nOnline, August 2021. Association for Computational Linguistics.\n[103] Rishabh Bhardwaj, Navonil Majumder, and Soujanya Poria. Investigating\ngender bias in BERT. Cognit. Comput., 13(4):1008–1018, July 2021.\n[104] Marzieh Mozafari, Reza Farahbakhsh, and Noël Crespi. Hate speech\ndetection and racial bias mitigation in social media based on BERT model.\nPLoS One, 15(8):e0237861, August 2020.\n[105] Matan Halevy, Camille Harris, Amy Bruckman, Diyi Yang, and Ayanna\nHoward. Mitigating racial biases in toxic language detection with an\nEquity-Based ensemble framework. In Equity and Access in Algorithms,\nMechanisms, and Optimization , number Article 7 in EAAMO ’21, pages\n1–11, New York, NY , USA, November 2021. Association for Computing\nMachinery.\n[106] Lele Sha, Yuheng Li, Dragan Gasevic, and Guanliang Chen. Bigger\ndata or fairer data? augmenting BERT via active sampling for educational\ntext classification. In Proceedings of the 29th International Conference\non Computational Linguistics , pages 1275–1285, Gyeongju, Republic of\nKorea, October 2022. International Committee on Computational Lin-\nguistics.\n[107] Flavien Prost, Nithum Thain, and Tolga Bolukbasi. Debiasing embed-\ndings for reduced gender bias in text classification. August 2019.\n[108] Rashidul Islam, Kamrun Naher Keya, Ziqian Zeng, Shimei Pan, and\nJames Foulds. Debiasing career recommendations with neural fair col-\nlaborative filtering. In Proceedings of the Web Conference 2021 , WWW\n’21, pages 3779–3790, New York, NY , USA, June 2021. Association for\nComputing Machinery.\n[109] Yada Pruksachatkun, Satyapriya Krishna, Jwala Dhamala, Rahul Gupta,\nand Kai-Wei Chang. Does robustness improve fairness? approaching\nfairness with word substitution robustness methods for text classification.\nJune 2021.\n[110] Daniel de Vassimon Manela, David Errington, Thomas Fisher, Boris van\nBreugel, and Pasquale Minervini. Stereotype and skew: Quantifying\ngender bias in pre-trained and fine-tuned language models. In Proceed-\nings of the 16th Conference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume , pages 2232–2242, Online,\nApril 2021. Association for Computational Linguistics.\n[111] Joshua R Minot, Nicholas Cheney, Marc Maier, Danne C Elbers, Christo-\npher M Danforth, and Peter Sheridan Dodds. Interpretable bias mitigation\nfor textual data: Reducing gender bias in patient notes while maintaining\nclassification performance. March 2021.\n[112] Li Lucy and David Bamman. Gender and representation bias in GPT-3\ngenerated stories. In Proceedings of the Third Workshop on Narrative\nUnderstanding, pages 48–55, Virtual, June 2021. Association for Com-\nputational Linguistics.\n[113] Moin Nadeem, Anna Bethke, and Siva Reddy. StereoSet: Measuring\nstereotypical bias in pretrained language models. April 2020.\n[114] Sophie Groenwold, Lily Ou, Aesha Parekh, Samhita Honnavalli, Sharon\nLevy, Diba Mirza, and William Yang Wang. Investigating African-\nAmerican vernacular english in Transformer-Based text generation. Oc-\ntober 2020.\n[115] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and\nNoah A Smith. RealToxicityPrompts: Evaluating neural toxic degener-\nation in language models. September 2020.\n[116] Tao Li, Tushar Khot, Daniel Khashabi, Ashish Sabharwal, and Vivek\nSrikumar. UnQovering stereotyping biases via underspecified questions.\nOctober 2020.\n[117] Andreas Liesenfeld, Alianda Lopez, and Mark Dingemanse. Opening\nup ChatGPT: Tracking openness, transparency, and accountability in\ninstruction-tuned text generators. In Proceedings of the 5th International\nConference on Conversational User Interfaces , number Article 47 in\nCUI ’23, pages 1–6, New York, NY , USA, July 2023. Association for\nComputing Machinery.\n14 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\n[118] Kent K Chang, Mackenzie Cramer, Sandeep Soni, and David Bamman.\nSpeak, memory: An archaeology of books known to ChatGPT/GPT-4.\nApril 2023.\n[119] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. Datasheets\nfor datasets. Commun. ACM, 64(12):86–92, November 2021.\n[120] Benjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exBERT:\nA visual analysis tool to explore learned representations in transformers\nmodels. October 2019.\n[121] Betty van Aken, Benjamin Winter, Alexander Löser, and Felix A Gers.\nVisBERT: Hidden-State visualizations for transformers. In Companion\nProceedings of the Web Conference 2020 , WWW ’20, pages 207–211,\nNew York, NY , USA, April 2020. Association for Computing Machinery.\n[122] Charles Pierse. Introducing transformers interpret — explainable AI for\ntransformers. https://towardsdatascience.com/introducing-transformers-\ninterpret-explainable-ai-for-transformers-890a403a9470, February 2021.\nAccessed: 2023-8-24.\n[123] Mateusz Szczepański, Marek Pawlicki, Rafał Kozik, and Michał Choraś.\nNew explainability method for BERT-based model in fake news detection.\nSci. Rep., 11(1):23705, December 2021.\n[124] Eoin M Kenny and Mark T Keane. Explaining deep learning using\nexamples: Optimal feature weighting methods for twin systems using\npost-hoc, explanation-by-example in XAI. Knowledge-Based Systems ,\n233:107530, December 2021.\n[125] T Shaikh, A Khalane, R Makwana, and A Ullah. Evaluating significant\nfeatures in Context-Aware multimodal emotion recognition with XAI\nmethods. Authorea Preprints, 2023.\n[126] Lwin Moe, Arghya Kundu, and Uyen Trang Nguyen. A BERT-\nbased explainable system for COVID-19 misinformation identification.\nhttps://workshop-proceedings.icwsm.org/pdf/202346.pdf . Accessed :\n2023 − 10 − 5.\n[127] Max Tigo Rietberg, Van Bach Nguyen, Jeroen Geerdink, Onno Vijlbrief,\nand Christin Seifert. Accurate and reliable classification of unstructured\nreports on their diagnostic goal using BERT models. Diagnostics (Basel),\n13(7), March 2023.\n[128] Farzad Ahmed, Samiha Sultana, Md Tanzim Reza, Sajib Kumar Saha Joy,\nand Md Golam Rabiul Alam. Interpretable movie review analysis using\nmachine learning and transformer models leveraging XAI. In 2022 IEEE\nAsia-Pacific Conference on Computer Science and Data Engineering\n(CSDE), pages 1–6, December 2022.\n[129] Kevin Scaria, Himanshu Gupta, Siddharth Goyal, Saurabh Arjun Sawant,\nSwaroop Mishra, and Chitta Baral. InstructABSA: Instruction learning\nfor aspect based sentiment analysis. February 2023.\n[130] Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. MVP: Multi-\ntask supervised pre-training for natural language generation. June 2022.\n[131] Chi Sun, Luyao Huang, and Xipeng Qiu. Utilizing BERT for Aspect-\nBased sentiment analysis via constructing auxiliary sentence. March\n2019.\n[132] Giannis Nikolentzos, Antoine J. -P. Tixier, Michalis Vazirgiannis. Mes-\nsage passing attention networks for document understanding. Proceed-\nings of the AAAI Conference on Artificial Intelligence , 2020.\n[133] Konstantinos Skianis, Giannis Nikolentzos, Stratis Limnios, and Michalis\nVazirgiannis. Rep the set: Neural networks for learning set representa-\ntions. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of\nthe Twenty Third International Conference on Artificial Intelligence and\nStatistics, volume 108 of Proceedings of Machine Learning Research ,\npages 1410–1420, Maastricht, Netherlands, 2020. PMLR.\n[134] Kamran Kowsari, Mojtaba Heidarysafa, Donald E Brown, Kiana Jafari\nMeimandi, and Laura E Barnes. RMDL: Random multimodel deep learn-\ning for classification. In Proceedings of the 2nd International Conference\non Information System and Data Mining , ICISDM ’18, pages 19–28, New\nYork, NY , USA, April 2018. Association for Computing Machinery.\n[135] Stephan Gouws, Yoshua Bengio, and Greg Corrado. BilBOWA: Fast\nbilingual distributed representations without word alignments. In Francis\nBach and David Blei, editors, Proceedings of the 32nd International\nConference on Machine Learning , volume 37 of Proceedings of Machine\nLearning Research, pages 748–756, Lille, France, 2015. PMLR.\n[136] Ashutosh Adhikari, Achyudh Ram, Raphael Tang, and Jimmy Lin.\nDocBERT: BERT for document classification. April 2019.\n[137] Pedro Henrique Luz de Araujo, Teófilo Emídio de Campos, Fabricio\nAtaides Braz, and Nilton Correia da Silva. VICTOR: a dataset for\nBrazilian legal documents classification. In Proceedings of the Twelfth\nLanguage Resources and Evaluation Conference , pages 1449–1458, Mar-\nseille, France, May 2020. European Language Resources Association.\n[138] Xin Huang, Boli Chen, Lin Xiao, Jian Yu, and Liping Jing. Label-aware\ndocument representation via hybrid attention for extreme multi-label text\nclassification. Neural Process. Letters , 54(5):3601–3617, October 2022.\n[139] Jieh-Sheng Lee and Jieh Hsiang. PatentBERT: Patent classification with\nFine-Tuning a pre-trained BERT model. May 2019.\n[140] Rong-Ching Chang, Chun-Ming Lai, Kai-Lai Chang, and Chu-Hsing Lin.\nDataset of propaganda techniques of the State-Sponsored information\noperation of the people’s republic of china. June 2021.\n[141] Ankit Pal, Muru Selvakumar, and Malaikannan Sankarasubbu. Multi-\nLabel text classification using attention-based graph neural network.\nMarch 2020.\n[142] Veysel Kocaman and David Talby. Biomedical named entity recognition\nat scale. In Pattern Recognition. ICPR International Workshops and\nChallenges, pages 635–646, New York, New York, USA, 2021. Springer\nInternational Publishing.\n[143] Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang,\nFei Huang, and Kewei Tu. Automated concatenation of embeddings for\nstructured prediction. October 2020.\n[144] Chenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong, Jie Tang, and\nDawn Song. DeepStruct: Pretraining of language models for structure\nprediction. May 2022.\n[145] Jinpeng Hu, Yaling Shen, Yang Liu, Xiang Wan, and Tsung-Hui Chang.\nHero-Gang neural model for named entity recognition. May 2022.\n[146] Minbyul Jeong and Jaewoo Kang. Enhancing label consistency on\ndocument-level named entity recognition. October 2022.\n[147] Zexuan Zhong and Danqi Chen. A frustratingly easy approach for entity\nand relation extraction. October 2020.\n[148] Tatiana Shavrina, Alena Fenogenova, Anton Emelyanov, Denis Shevelev,\nEkaterina Artemova, Valentin Malykh, Vladislav Mikhailov, Maria\nTikhonova, Andrey Chertok, and Andrey Evlampiev. RussianSuper-\nGLUE: A russian language understanding evaluation benchmark. October\n2020.\n[149] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu,\nand Haifeng Wang. ERNIE 2.0: A continual Pre-Training framework for\nlanguage understanding. AAAI, 34(05):8968–8975, April 2020.\n[150] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,\nGaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,\nNoam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard,\nGuy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay\nGhemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant\nMisra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\nDavid Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan\nSepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M Dai,\nThanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\nZhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele\nCatasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,\nSlav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with\npathways. April 2022.\n[151] Zeming Chen, Qiyue Gao, and Lawrence S Moss. NeuralLog: Natural\nlanguage inference with joint neural and logical reasoning. May 2021.\n[152] Xuan-Quy Dao, Ngoc-Bich Le, The-Duy V o, Xuan-Dung Phan, Bac-\nBien Ngo, Van-Tien Nguyen, Thi-My-Thanh Nguyen, and Hong-Phuoc\nNguyen. VNHSGE: VietNamese high school graduation examination\ndataset for large language models. May 2023.\n[153] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei\nYu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned\nlanguage models are Zero-Shot learners. September 2021.\n[154] Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and\nGuilin Qi. Can ChatGPT replace traditional KBQA models? an in-depth\nanalysis of GPT family LLMs’ question answering performance. March\n2023.\n[155] Guilherme Moraes Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo\nAbonizio, Marzieh Fadaee, Roberto Lotufo, and Rodrigo Nogueira. No\nparameter left behind: How distillation and model size affect Zero-Shot\nretrieval. June 2022.\n[156] Ekaterina Taktasheva, Tatiana Shavrina, Alena Fenogenova, Denis\nShevelev, Nadezhda Katricheva, Maria Tikhonova, Albina Akhmetga-\nreeva, Oleg Zinkevich, Anastasiia Bashmakova, Svetlana Iordanskaia,\nAlena Spiridonova, Valentina Kurenshchikova, Ekaterina Artemova, and\nVOLUME 11, 2023 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nVladislav Mikhailov. TAPE: Assessing few-shot russian language under-\nstanding. October 2022.\n[157] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang,\nHongkun Yu, and Jiawei Han. Large language models can Self-Improve.\nOctober 2022.\n[158] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry\nLepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige\nBailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey,\nYanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira,\nMark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan\nAhn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha\nBrahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry,\nChristopher A Choquette-Choo, Aakanksha Chowdhery, Clément Crepy,\nShachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber,\nMarkus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez,\nGuy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland,\nAndrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah,\nMatthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha\nKudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li,\nWei Li, Yaguang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez,\nVedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric\nNi, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex\nPolozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker\nRiley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel,\nRenee Shelby, Ambrose Slone, Daniel Smilkov, David R So, Daniel\nSohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran V odrahalli,\nXuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting,\nYuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui\nYu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou,\nSlav Petrov, and Yonghui Wu. PaLM 2 technical report. May 2023.\n[159] Michihiro Yasunaga, Jure Leskovec, and Percy Liang. LinkBERT: Pre-\ntraining language models with document links. March 2022.\n[160] Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen,\nand Jianfeng Gao. UnitedQA: A hybrid approach for open domain\nquestion answering. January 2021.\n[161] Xinbei Ma, Zhuosheng Zhang, and Hai Zhao. Enhanced Speaker-Aware\nMulti-Party Multi-Turn dialogue comprehension. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing , 31:2410–2423, 2023.\n[162] Imanol Schlag, Tsendsuren Munkhdalai, and Jürgen Schmidhuber. Learn-\ning associative inference using fast weight memory. November 2020.\n[163] G Thomas Hudson and Noura Al Moubayed. MuLD: The multitask long\ndocument benchmark. February 2022.\n[164] Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language\nmodel for scientific text. March 2019.\n[165] Wissam Antoun, Fady Baly, and Hazem Hajj. AraBERT: Transformer-\nbased model for arabic language understanding. February 2020.\n[166] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang. Pre-\nTraining with whole word masking for chinese BERT. IEEE/ACM\nTransactions on Audio, Speech, and Language Processing , 29:3504–\n3514, 2021.\n[167] Ibrahim Abu Farha and Walid Magdy. Mazajak: An online arabic senti-\nment analyser. In Proceedings of the Fourth Arabic Natural Language\nProcessing Workshop, pages 192–198, Stroudsburg, PA, USA, August\n2019. Association for Computational Linguistics.\n[168] Ibraheem Muhammad Moosa, Mahmud Elahi Akhter, and Ashfia Binte\nHabib. Does transliteration help multilingual language modeling? January\n2022.\n[169] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le.\nUnsupervised data augmentation for consistency training. Adv. Neural\nInf. Process. Syst. , 33:6256–6268, 2020.\n[170] Mathieu Cliche. BB_twtr at SemEval-2017 task 4: Twitter sentiment\nanalysis with CNNs and LSTMs. April 2017.\n[171] Dogu Araci. FinBERT: Financial sentiment analysis with pre-trained\nlanguage models. August 2019.\n[172] Yu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang, Chao Zhang, and\nJiawei Han. Hierarchical topic mining via joint spherical tree and text\nembedding. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining , KDD ’20, pages\n1908–1917, New York, NY , USA, August 2020. Association for Com-\nputing Machinery.\n[173] Guokun Lai, Barlas Oguz, Yiming Yang, and Veselin Stoyanov. Bridging\nthe domain gap in cross-lingual document classification. September 2019.\n[174] Hubert Soyer, Pontus Stenetorp, and Akiko Aizawa. Leveraging monolin-\ngual data for crosslingual compositional word representations. December\n2014.\n[175] Julian Martin Eisenschlos, Sebastian Ruder, Piotr Czapla, Marcin Kardas,\nSylvain Gugger, and Jeremy Howard. MultiFiT: Efficient multi-lingual\nlanguage model fine-tuning. September 2019.\n[176] Ammar Rashed, Mucahid Kutlu, Kareem Darwish, Tamer Elsayed, and\nCansın Bayrak. Embeddings-Based clustering for target specific stances:\nThe case of a polarized turkey. ICWSM, 15:537–548, May 2021.\nJOHN FIELDS holds the position of Assistant\nProfessor of Business Analytics at Concordia Uni-\nversity Wisconsin-Ann Arbor. He received his B.S.\ndegree in engineering (industrial distribution) from\nTexas A&M University, a M.S. degree in applied\ndata science from Syracuse University. Currently,\nhe is actively pursuing a PhD in computer science\nat Marquette University, Milwaukee, WI, USA.\nHis research spans various domains, with a pri-\nmary focus on the integration of artificial intelli-\ngence (AI) in education and its impact on student success. Specific areas of\ninterest include text classification (including multimodal approaches), graph\ndatabases, and addressing AI bias and fairness. He has collaborated as a co-\nauthor on an upcoming paper scheduled for presentation at the IEEE Big Data\n2023 conference, delving into the application of natural language processing\n(NLP) in the realm of education.\nMr. Fields is a co-inventor on a pending patent (62/935,928) that explores\nthe utilization of machine learning and AI for higher education applications.\nHe is also the recipient of the 2015 SAP IGgie award for information\ngovernance.\nKEVIN CHOVANECis a data scientist in the Office\nof Institutional Research at Marquette University\nin Milwaukee, WI, USA. He received his B.S. in\nMath and English from Marquette, an M.A. from\nthe University of Chicago, and a Ph. D. in English\nand Comparative Literature from the University of\nNorth Carolina. Currently, he is pursuing a Ph. D.\nin Computer Science at Marquette. His research\nembraces an interdisciplinary approach, focusing\non the digital humanities, editing, natural language\nprocessing, fairness and accountability in AI, and educational analytics,\nand his work has appeared in journals and conference proceedings across\nacademic disciplines, including Digital Humanities Quarterly, Renaissance\nDrama, and a forthcoming paper in IEEE Big Data 2023.\n16 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFields et al.: A Survey of Text Classification with Transformers for IEEE Access\nPRAVEEN MADIRAJU is a Professor in the De-\npartment of Computer Science at Marquette Uni-\nversity, Milwaukee, WI, USA. He received his PhD\nin Computer Science from Georgia State Univer-\nsity. He directs the Data Science and Text Analyt-\nics Lab. The DATA Lab focuses on solving real-\nworld problems by applying techniques from the\nbroad area of data science and data analytics on\nboth structured and unstructured data. The lab also\nconducts research on applying machine learning\ntechniques to analyze textual and social media data. He is also the Graduate\nProgram Chair for Computer Science program at Marquette.\nDr. Madiraju’s research is in the areas of Data Science, Healthcare In-\nformatics, Text Analytics, and Databases. He has published over 50 peer-\nreviewed articles and has organized workshops on Middleware Systems in\nconjunction with ACM SAC and IEEE COMPSAC. He regularly serves on\nNSF Panels.\nVOLUME 11, 2023 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3349952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}