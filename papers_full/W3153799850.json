{
  "title": "Discourse Probing of Pretrained Language Models",
  "url": "https://openalex.org/W3153799850",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4223199574",
      "name": "Koto, Fajri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747998458",
      "name": "Lau, Jey Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750522270",
      "name": "Baldwin, Timothy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3099772776",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2970474271",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3116295307",
    "https://openalex.org/W3101717721",
    "https://openalex.org/W2952750383",
    "https://openalex.org/W2091714641",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2986341143",
    "https://openalex.org/W2924120895",
    "https://openalex.org/W2251500379",
    "https://openalex.org/W3101190870",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W3028965759",
    "https://openalex.org/W2140676672",
    "https://openalex.org/W3100308117",
    "https://openalex.org/W2885227423",
    "https://openalex.org/W2758815496",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2962932447",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2044599851",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2982399380"
  ],
  "abstract": "Existing work on probing of pretrained language models (LMs) has predominantly focused on sentence-level syntactic tasks. In this paper, we introduce document-level discourse probing to evaluate the ability of pretrained LMs to capture document-level relations. We experiment with 7 pretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be overall the best model at capturing discourse -- but only in its encoder, with BERT performing surprisingly well as the baseline model. Across the different models, there are substantial differences in which layers best capture discourse information, and large disparities between models.",
  "full_text": "Discourse Probing of Pretrained Language Models\nFajri Koto Jey Han Lau Timothy Baldwin\nSchool of Computing and Information Systems\nThe University of Melbourne\nffajri@student.unimelb.edu.au, jeyhan.lau@gmail.com, tbaldwin@unimelb.edu.au\nAbstract\nExisting work on probing of pretrained lan-\nguage models (LMs) has predominantly fo-\ncused on sentence-level syntactic tasks. In\nthis paper, we introduce document-level dis-\ncourse probing to evaluate the ability of pre-\ntrained LMs to capture document-level rela-\ntions. We experiment with 7 pretrained LMs,\n4 languages, and 7 discourse probing tasks,\nand ﬁnd BART to be overall the best model at\ncapturing discourse — but only in its encoder,\nwith BERT performing surprisingly well as\nthe baseline model. Across the different mod-\nels, there are substantial differences in which\nlayers best capture discourse information, and\nlarge disparities between models.\n1 Introduction\nThe remarkable development of pretrained lan-\nguage models (Devlin et al., 2019; Lewis et al.,\n2020; Lan et al., 2020) has raised questions about\nwhat precise aspects of language these models do\nand do not capture. Probing tasks offer a means to\nperform ﬁne-grained analysis of the capabilities of\nsuch models, but most existing work has focused\non sentence-level analysis such as syntax (Hewitt\nand Manning, 2019; Jawahar et al., 2019; de Vries\net al., 2020), entities/relations (Papanikolaou et al.,\n2019), and ontological knowledge (Michael et al.,\n2020). Less is known about how well such models\ncapture broader discourse in documents.\nRhetorical Structure Theory is a framework for\ncapturing how sentences are connected and describ-\ning the overall structure of a document (Mann and\nThompson, 1986). A number of studies have used\npretrained models to classify discourse markers\n(Sileo et al., 2019) and discourse relations (Nie\net al., 2019; Shi and Demberg, 2019), but few (Koto\net al., to appear) have systematically investigated\nthe ability of pretrained models to model discourse\nstructure. Furthermore, existing work relating to\ndiscourse probing has typically focused exclusively\nModel Type #Param #Data Objective\nBERT\nEnc\n110M 16GB MLM+NSP\nRoBERTa 110M 160GB MLM\nALBERT 12M 16GB MLM+SOP\nELECTRA 110M 16GB MLM+DISC\nGPT-2 Dec 117M 40GB LM\nBART Enc+Dec121M 160GB DAE\nT5 110M 750GB DAE\nTable 1: Summary of all English pretrained language\nmodels used in this work. “MLM” = masked language\nmodel, “NSP” = next sentence prediction, “SOP” =\nsentence order prediction, “LM” = language model,\n“DISC” = discriminator, and “DAE” = denoising au-\ntoencoder.\non the BERT-base model, leaving open the ques-\ntion of how well these ﬁndings generalize to other\nmodels with different pretraining objectives, for\ndifferent languages, and different model sizes.\nOur research question in this paper is:How much\ndiscourse structure do layers of different pretrained\nlanguage models capture, and do the ﬁndings gen-\neralize across languages?\nThere are two contemporaneous related studies\nthat have examined discourse modelling in pre-\ntrained language models. Upadhye et al. (2020)\nanalyzed how well two pretrained models capture\nreferential biases of different classes of English\nverbs. Zhu et al. (2020) applied the model of Feng\nand Hirst (2014) to parse IMDB documents (Maas\net al., 2011) into discourse trees. Using this (po-\ntentially noisy) data, probing tasks were conducted\nby mapping attention layers into single vectors of\ndocument-level rhetorical features. These features,\nhowever, are unlikely to capture all the intrica-\ncies of inter-sentential abstraction as their input\nis formed based on discourse relations1 and aggre-\ngate statistics on the distribution of discourse units.\n1For example, they only consider discourse relation labels\nand ignore nuclearity.\narXiv:2104.05882v1  [cs.CL]  13 Apr 2021\nProbing Task English Chinese German Spanish\n(1) 4-way NSP\n(2) Sentence Ordering\nXSUM articles\n(Narayan et al., 2018)\nSplit: 8K/1K/1K\nWikipedia (ZH)\nSplit: 8K/1K/1K\nWikipedia (DE)\nSplit: 8K/1K/1K\nWikipedia (ES)\nSplit: 8K/1K/1K\n(3) Discourse Connective\nSampled DisSent dataset\n(Nie et al., 2019)\n#Labels: 15\nSplit: 10K/1K/1K\nCDTB (Li et al., 2014)\n#Labels: 22\nSplit: 1539/76/168\nPotsdam Commentary\n(Bourgonje and Stede, 2020)\n#Labels: 15\nSplit: 900/148/159\nN/A\n(4) RST Nuclearity\n(5) RST Relation\nRST-DT\n(Carlson et al., 2001)\n#Labels (nuc/rel): 3/18\nSplit: 16903/1943/2308\nCDTB (Li et al., 2014)\n#Labels (nuc/rel): 3/4\nSplit: 6159/353/809\nPotsdam Commentary\n(Bourgonje and Stede, 2020)\n#Labels (nuc/rel): 3/31\nSplit: 1892/289/355\nRST-Spanish Treebank\n(da Cunha et al., 2011)\n#Labels (nuc/rel): 3/29\nSplit: 2042/307/421\n(6) RST EDU Segmentation\nRST-DT\n(Carlson et al., 2001)\nSplit: 312/35/38 docs\nCDTB (Li et al., 2014)\nSplit: 2135/105/241 p’graphs\nPotsdam Commentary\n(Bourgonje and Stede, 2020)\nSplit: 131/20/25 docs\nRST-Spanish Treebank\n(da Cunha et al., 2011)\nSplit: 200/34/30 docs\n(7) Cloze Story Test (Mostafazadeh et al., 2016)\nSplit: 1683/188/1871N/A N/A N/A\nTable 2: A summary of probing tasks and datasets for each of the four languages. “Split” indicates the number of\ntrain/development/test instances.\nEDU1 EDU2 \nEDU3 elab \ncause Nuclearity\tand\tRelation\tprediction: \ntext1\t|\ttext2\t ⇨ \tnuclearity,\trelation \nEDU1\t|\tEDU2\t ⇨ \tSN,\telab \nEDU1\tEDU2\t|\tEDU3  ⇨ \tNS,\tcause \nEDU\tsegmentation: \nEDU1\tEDU2\tEDU3  ⇨ \tEDU1\t|\tEDU2\t|\tEDU3 \n(N) (S) \n(S) (N) \nFigure 1: Illustration of the RST discourse probing\ntasks (Tasks 4–6).\nTo summarize, we introduce 7 discourse-related\nprobing tasks, which we use to analyze 7 pretrained\nlanguage models over 4 languages: English, Man-\ndarin Chinese, German, and Spanish. Code and\npublic-domain data associated with this research\nis available at https://github.com/fajri91/discourse_\nprobing.\n2 Pretrained Language Models\nWe outline the 7 pretrained models in Table 1. They\ncomprise 4 encoder-only models: BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2019), AL-\nBERT (Lan et al., 2020), and ELECTRA (Clark\net al., 2020); 1 decoder-only model: GPT-2 (Rad-\nford et al., 2019); and 2 encoder–decoder mod-\nels: BART (Lewis et al., 2020) and T5 (Raffel\net al., 2019). To reduce the confound of model size,\nwe use pretrained models of similar size (∼110m\nmodel parameters), with the exception of ALBERT\nwhich is designed to be lighter weight. All mod-\nels have 12 transformer layers in total; for BART\nand T5, this means their encoder and decoder have\n6 layers each. Further details of the models are\nprovided in the Supplementary Material.\n3 Probing Tasks for Discourse Coherence\nWe experiment with a total of seven probing tasks,\nas detailed below. Tasks 4–6 are component tasks\nof discourse parsing based on rhetorical structure\ntheory (RST; Mann and Thompson (1986)). In an\nRST discourse tree, EDUs are typically clauses or\nsentences, and are hierarchically connected with\ndiscourse labels denoting: (1) nuclearity = nucleus\n(N) vs. satellite (S); 2 and (2) discourse relations\n(e.g. elaborate). An example of a binarized RST\ndiscourse tree is given in Figure 1.\n1. Next sentence prediction. Similar to the next\nsentence prediction (NSP) objective in BERT\npretraining, but here we frame it as a 4-way\nclassiﬁcation task, with one positive and 3\nnegative candidates for the next sentence. The\npreceding context takes the form of between 2\nand 8 sentences, but the candidates are always\nsingle sentences.\n2. Sentence ordering. We shufﬂe 3–7 sentences\nand attempt to reproduce the original order.\nThis task is based on Barzilay and Lapata\n(2008) and Koto et al. (2020), and is assessed\nbased on rank correlation relative to the origi-\nnal order.\n3. Discourse connective prediction. Given two\nsentences/clauses, the task is to identify an\nappropriate discourse marker, such as while,\n2The satellite is a supporting EDU for the nucleus.\n2 4 6 8 10 12\nLayer\n0.4\n0.6\n0.8\n1.0Accuracy\nNSP/uni00A0with/uni00A04/uni00A0MC\n2 4 6 8 10 12\nLayer\n0.2\n0.3\n0.4Spearman/uni00A0Rank\nSent./uni00A0Ordering\n2 4 6 8 10 12\nLayer\n0.3\n0.4\n0.5\n0.6Accuracy\nDiscourse/uni00A0Connective\n2 4 6 8 10 12\nLayer\n0.65\n0.70\n0.75\n0.80Accuracy\nNuclearity\n2 4 6 8 10 12\nLayer\n0.4\n0.5\n0.6Accuracy\nRelation\n2 4 6 8 10 12\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8F1/uni00ADmacro\nEDU/uni00A0segmentation\n2 4 6 8 10 12\nLayer\n0.6\n0.7\n0.8\n0.9Accuracy\nCloze\n2 4 6 8 10 12\nLayer\n0.2\n0.4\n0.6\n0.8Score\nAverage/uni00A0of/uni00A0Normalized/uni00A0Scores\nBERT\nRoBERTa\nALBERT\nGPT/uni00AD2\nELECTRA\nBART\nT5\nFigure 2: Probing task performance on English for each of the seven tasks, plus the average across all tasks. For\nBART and T5, layers 7–12 are the decoder layers. All results are averaged over three runs, and the vertical line for\neach data point denotes the standard deviation (noting that most results have low s.d., meaning the bar is often not\nvisible).\nor, or although (Nie et al., 2019), represent-\ning the conceptual relation between the sen-\ntences/clauses.\n4. RST nuclearity prediction. For a given or-\ndered pairing of (potentially complex) EDUs\nwhich are connected by an unspeciﬁed rela-\ntion, predict the nucleus/satellite status of each\n(see Figure 1).\n5. RST relation prediction. For a given or-\ndered pairing of (potentially complex) EDUs\nwhich are connected by an unspeciﬁed rela-\ntion, predict the relation that holds between\nthem (see Figure 1).\n6. RST elementary discourse unit (EDU) seg-\nmentation. Chunk a concatenated sequence\nof EDUs into its component EDUs.\n7. Cloze story test. Given a 4-sentence story\ncontext, pick the best ending from two pos-\nsible options (Mostafazadeh et al., 2016;\nSharma et al., 2018). This task is harder than\nNSP, as it requires an understanding of com-\nmonsense and storytelling (Chaturvedi et al.,\n2017; Liu et al., 2018).\n4 Experimental Setup\nWe summarize all data (sources, number of labels,\nand data split) in Table 2. This includes English,\nChinese, German, and Spanish for each probing\ntask. For NSP and sentence ordering, we generate\ndata from news articles and Wikipedia. For the\nRST tasks, we use discourse treebanks for each of\nthe four languages.\nWe formulate all probing tasks except sentence\nordering and EDU segmentation as a classiﬁca-\ntion problem, and evaluate using accuracy. During\nﬁne-tuning, we add an MLP layer on top of the pre-\ntrained model for classiﬁcation, and only update\nthe MLP parameters (all other layers are frozen).\nWe use the [CLS] embedding for BERT and AL-\nBERT following standard practice, while for other\nmodels we perform average pooling to obtain a\nvector for each sentence, and concatenate them as\nthe input to the MLP.3\n3BERT and ALBERT performance with average pooling\nFor sentence ordering, we follow Koto et al.\n(2020) and frame it as a sentence-level sequence\nlabelling task, where the goal is to estimate P(r|s),\nwhere r is the rank position and s the sentence.\nThe task has 7 classes, as we have 3–7 sentences\n(see Section 3). At test time, we choose the label\nsequence that maximizes the sequence probability.\nSentence embeddings are obtained by average pool-\ning. The EDU segmentation task is also framed as\na binary sequence labelling task (segment boundary\nor not) at the (sub)word level. We use Spearman\nrank correlation and macro-averaged F1 score to\nevaluate sentence ordering and EDU segmentation,\nrespectively.\nWe use a learning rate 1e−3, warm-up of 10%\nof total steps, and the development set for early\nstopping in all experiments. All presented results\nare averaged over three runs.4\n5 Results and Analysis\nIn Figure 2, we present the probing task perfor-\nmance on English for all models based on a repre-\nsentation generated from each of the 12 layers of\nthe model. First, we observe that most performance\nﬂuctuates (non-monotonic) across layers except for\nsome models in the NSP task and some ALBERT\nresults in the other probing tasks. We also found\nthat most models except ALBERT tend to have a\nvery low standard deviation based on three runs\nwith different random seeds.\nWe discover that all models except T5 and early\nlayers of BERT and ALBERT perform well over\nthe NSP task, with accuracy ≥0.8, implying it is a\nsimple task. However, they all struggle at sentence\nordering (topping out at ρ∼0.4), suggesting that\nthey are ineffective at modelling discourse over\nmultiple sentences; this is borne out in Figure 4,\nwhere performance degrades as the number of sen-\ntences to re-order increases.\nInterestingly, for Discourse Connectives, RST\nNuclearity, and RST Relation Prediction, the mod-\nels produce similar patterns, even though the dis-\ncourse connective data is derived from a different\ndataset and theoretically divorced from RST. BART\noutperforms most other models in layers 1–6 for\nthese tasks (a similar observation is found for NSP\nand Sentence Ordering) with BERT and ALBERT\nstruggling particularly in the earlier layers. For\nis in included in the Appendix.\n4More details of the training conﬁguration are given in the\nAppendix.\nEDU segmentation, RoBERTa and again the ﬁrst\nfew layers of BART perform best. For the Cloze\nStory Test, all models seem to improve as we go\ndeeper into the layers, suggesting that high-level\nstory understanding is captured deeper in the mod-\nels.\nWe summarize the overall performance by cal-\nculating the averaged normalized scores in the last\nplot in Figure 2.5 RoBERTa and BART appear to\nbe the best overall models at capturing discourse\ninformation, but only in the encoder layers (the\nﬁrst 6 layers) for BART. We hypothesize that the\nBART decoder focuses on sequence generation,\nand as such is less adept at language understanding.\nThis is supported by a similar trend for T5, also\na denoising autoencoder. BERT does surprisingly\nwell (given that it’s the baseline model), but mostly\nin the deeper layers (7–10), while ELECTRA per-\nforms best at the three last layers.\nIn terms of the inﬂuence of training data, we see\nmixed results. BART and RoBERTa are the two\nbest models, and both are trained with more data\nthan most models (an order of magnitude more; see\nTable 1). But T5 (and to a certain extent GPT-2) are\nalso trained with more data (in fact T5 has the most\ntraining data), but their discourse modelling per-\nformance is underwhelming. In terms of training\nobjectives, it appears that a pure decoder with an\nLM objective (GPT-2) is less effective at capturing\ndiscourse structure. ALBERT, the smallest model\n(an order of magnitude less parameters than most),\nperforms surprisingly well (with high standard de-\nviation), but only at its last layer, suggesting that\ndiscourse knowledge is concentrated deep inside\nthe model.\nLastly, we explore whether these trends hold if\nwe use a larger model (BERT-base vs. BERT-large)\nand for different languages (again based on mono-\nlingual BERT models for the respective languages).\nResults are presented in Figure 3. For model size\n(“English (large)” vs. “English”), the overall pat-\ntern is remarkably similar, with a slight uplift in\nabsolute results with the larger model. Between the\n4 different languages (English, Chinese, German,\nand Spanish), performance varies for all tasks ex-\ncept for NSP (e.g. EDU segmentation appears to\nbe easiest in Chinese, and relation prediction is the\nhardest in German), but the shape of the lines is\nlargely the same, indicating the optimal layers for\n5Given a task, we perform min–max normalization for all\nmodel-layer scores (7×12 scores in total), and then compute\nthe average over all tasks for each model’s layer.\n0 5 10 15 20 25\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Score\nEnglish/uni00A0(large)\n2 4 6 8 10 12\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Score\nEnglish\n2 4 6 8 10 12\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Score\nChinese\n2 4 6 8 10 12\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Score\nGerman\n2 4 6 8 10 12\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Score\nSpanish\nNSP/uni00A0with/uni00A04/uni00A0MC\nSent./uni00A0Ordering\nDiscourse/uni00A0Connective\nNuclearity\nRelation\nEDU/uni00A0segmentation\nFigure 3: Discourse performance of BERT across different languages. All results are averaged over three runs, and\na vertical line is used to denote the standard deviation for each data point (most of which are not visible, due to the\nlow standard deviation).\n3 4 5 6 7\nNumber/uni00A0of/uni00A0sentences\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5Spearman/uni00A0Correlation\nALBERT\nBART\nBERT\nGPT/uni00AD2\nRoBERTa\nT5\nELECTRA\nFigure 4: Sentence ordering task breakdown based on\nthe best layer of each model.\na particular task are consistent across languages.\n6 Conclusion\nWe perform probing on 7 pretrained language mod-\nels across 4 languages to investigate what discourse\neffects they capture. We ﬁnd that BART’s encoder\nand RoBERTa perform best, while pure language\nmodels (GPT-2) struggle. Interestingly, we see a\nconsistent pattern across different languages and\nmodel sizes, suggesting that the trends we found\nare robust across these dimensions.\nAcknowledgements\nWe are grateful to the anonymous reviewers for\ntheir helpful feedback and suggestions. The ﬁrst\nauthor is supported by the Australia Awards Schol-\narship (AAS), funded by the Department of Foreign\nAffairs and Trade (DFAT), Australia. This research\nwas undertaken using the LIEF HPC-GPGPU Fa-\ncility hosted at The University of Melbourne. This\nfacility was established with the assistance of LIEF\nGrant LE170100200.\nReferences\nRegina Barzilay and Mirella Lapata. 2008. Modeling\nlocal coherence: An entity-based approach. Compu-\ntational Linguistics, 34(1):1–34.\nPeter Bourgonje and Manfred Stede. 2020. The Pots-\ndam commentary corpus 2.2: Extending annotations\nfor shallow discourse parsing. In Proceedings of\nthe 12th Language Resources and Evaluation Con-\nference, pages 1061–1066, Marseille, France. Euro-\npean Language Resources Association.\nLynn Carlson, Daniel Marcu, and Mary Ellen\nOkurovsky. 2001. Building a discourse-tagged cor-\npus in the framework of Rhetorical Structure Theory.\nIn Proceedings of the Second SIGdial Workshop on\nDiscourse and Dialogue.\nSnigdha Chaturvedi, Haoruo Peng, and Dan Roth.\n2017. Story comprehension for predicting what hap-\npens next. In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1603–1614, Copenhagen, Denmark. As-\nsociation for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR 2020: Eighth International\nConference on Learning Representations.\nIria da Cunha, Juan-Manuel Torres-Moreno, and Ger-\nardo Sierra. 2011. On the development of the RST\nSpanish Treebank. In Proceedings of the 5th Lin-\nguistic Annotation Workshop, pages 1–10.\nWietse de Vries, Andreas van Cranenburgh, and Malv-\nina Nissim. 2020. What’s so special about BERT’s\nlayers? A closer look at the NLP pipeline in mono-\nlingual and multilingual models. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 4339–4350, Online. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nVanessa Wei Feng and Graeme Hirst. 2014. A linear-\ntime bottom-up discourse parser with constraints\nand post-editing. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 511–\n521, Baltimore, Maryland. Association for Compu-\ntational Linguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nFajri Koto, Jey Han Lau, and Timothy Baldwin. to ap-\npear. Top-down discourse parsing via sequence la-\nbelling. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Com-\nputational Linguistics.\nFajri Koto, Afshin Rahimi, Jey Han Lau, and Timothy\nBaldwin. 2020. IndoLEM and IndoBERT: A bench-\nmark dataset and pre-trained language model for In-\ndonesian NLP. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics\n(COLING 2020).\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In ICLR 2020:\nEighth International Conference on Learning Repre-\nsentations.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nYancui Li, Wenhe Feng, Jing Sun, Fang Kong, and\nGuodong Zhou. 2014. Building Chinese discourse\ncorpus with connective-driven dependency tree\nstructure. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 2105–2114, Doha, Qatar. Associa-\ntion for Computational Linguistics.\nFei Liu, Trevor Cohn, and Timothy Baldwin. 2018.\nNarrative modeling with memory chains and seman-\ntic supervision. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) , pages 278–\n284, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nWilliam C. Mann and Sandra A. Thompson. 1986. As-\nsertions from discourse structure. In Strategic Com-\nputing - Natural Language Workshop: Proceedings\nof a Workshop Held at Marina del Rey, California,\nMay 1-2, 1986.\nJulian Michael, Jan A. Botha, and Ian Tenney. 2020.\nAsking without telling: Exploring latent ontologies\nin contextual representations. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6792–6812,\nOnline. Association for Computational Linguistics.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A cor-\npus and cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807, Brussels, Bel-\ngium. Association for Computational Linguistics.\nAllen Nie, Erin Bennett, and Noah Goodman. 2019.\nDisSent: Learning sentence representations from ex-\nplicit discourse relations. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4497–4510, Florence,\nItaly. Association for Computational Linguistics.\nYannis Papanikolaou, Ian Roberts, and Andrea Pier-\nleoni. 2019. Deep bidirectional transformers for re-\nlation extraction without supervision. In Proceed-\nings of the 2nd Workshop on Deep Learning Ap-\nproaches for Low-Resource NLP (DeepLo 2019) ,\npages 67–75, Hong Kong, China. Association for\nComputational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nRishi Sharma, James Allen, Omid Bakhshandeh, and\nNasrin Mostafazadeh. 2018. Tackling the story end-\ning biases in the story cloze test. In Proceedings of\nthe 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers) ,\npages 752–757, Melbourne, Australia. Association\nfor Computational Linguistics.\nWei Shi and Vera Demberg. 2019. Next sentence pre-\ndiction helps implicit discourse relation classiﬁca-\ntion within and across domains. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5790–5796, Hong Kong,\nChina. Association for Computational Linguistics.\nDamien Sileo, Tim Van De Cruys, Camille Pradel,\nand Philippe Muller. 2019. Mining discourse mark-\ners for unsupervised sentence representation learn-\ning. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n3477–3486, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nShiva Upadhye, Leon Bergen, and Andrew Kehler.\n2020. Predicting reference: What do language mod-\nels learn about discourse models? In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 977–982,\nOnline. Association for Computational Linguistics.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\nChina. Association for Computational Linguistics.\nZining Zhu, Chuer Pan, Mohamed Abdalla, and Frank\nRudzicz. 2020. Examining the rhetorical capacities\nof neural language models. In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP , pages 16–32,\nOnline. Association for Computational Linguistics.\nA Pretrained Language Models\nThe pretrained models are sourced from Hugging-\nface (https://huggingface.co/), as detailed in Ta-\nbles 3 and 4.\nModel Huggingface model\nBERT bert-base-uncased\nBERT (large) bert-large-uncased\nRoBERTa roberta-base\nALBERT albert-base-v2\nELECTRA electra-base-discriminator\nGPT-2 gpt2\nBART bart-base\nT5 t5-small\nTable 3: List of English pretrained language models\nLanguage Huggingface model\nChinese bert-base-chinese\nGerman bert-base-german-dbmdz-uncased\nSpanish bert-base-spanish-wwm-uncased\nTable 4: List of non-English BERT models.\nB Data Construction, Examples, and\nTraining Conﬁguration\nB.1 Next Sentence Prediction\nWe use spaCy (https://spacy.io/) to perform sen-\ntence tokenization, and ensure that the distractor\noptions in the training set do not overlap with the\ntest set. For all languages and models, the training\nconﬁgurations are similar: the maximum tokens in\nthe context and the next sentence are 450 and 50,\nrespectively. If the token lengths are more than this,\nwe truncate the context from the beginning of the\nsequence, and truncate the next sentence at the end\nof the sequence. We concatenate context with each\noption, and perform binary classiﬁcation.\nOther training conﬁguration details: learning\nrate = 1e-3, Adam epsilon =1e-8, maximum gradi-\nent norm = 1.0, maximum epochs = 20, warmup\n= 10% of the training steps, and patience for early\nstopping = 5 epochs.\nB.2 Sentence Ordering\nIn generating sentence ordering data, we once again\nuse spaCy (https://spacy.io/) to perform sentence\ntokenization. For all languages and models, the\n#Sentence (context) Total\n2 2500\n4 2500\n6 2500\n8 2500\nTotal 10000\nTable 5: NSP data based on the number of sentences.\nContext\ns1:The Eastern Star, mostly carrying elderly\ntourists, capsized on 1 June near Jianli in\nHubei province.\ns2:Just 14 of the 456 passengers and crew are\nknown to have survived.\nOptions\n0: The channel recently said its signal was\ncarried by 22 satellites\n0:That step has become a huge challenge for\nopposition candidates\n0: Six men were convicted and then acquit-\nted of the atrocity and no-one has since been\nconvicted of involvement in the bombing\n1:A search is continuing for eight people who\nremain missing.\nTable 6: Example of English NSP data with 2-sentence\ncontext. 1 indicates the correct next sentence.\n#Sentence Total\n3 2000\n4 2000\n5 2000\n6 2000\n7 2000\nTotal 10000\nTable 7: Sentence ordering data based on number of\nsentence.\ntraining conﬁgurations are similar, with the max-\nimum tokens in each sentence = 50, learning rate\n= 1e-3, Adam epsilon = 1e-8, maximum gradient\nnorm = 1.0, training epochs = 20, warmup = 10% of\nthe training steps, and patience for early stopping\n= 10 epochs.\nB.3 Discourse Connective Prediction\nAs our Chinese and German data is extracted from\ndiscourse treebanks, the number of distinct con-\nnective words varies. For instance, in the Chinese\ndiscourse treebank, we ﬁnd 246 unique connective\nContext\ns0:West Mercia Police said the police do not\nencourage members of the public to pursue\ntheir own investigations.\ns1:David John Poole, from Hereford, poses\nonline as a 14-year-old girl and says he has\nbeen sent hundreds of explicit messages.\ns2:He says his work has led to two arrests in\nfour weeks.\nCorrect order:2–0–1\nTable 8: Example of English sentence ordering data\nwords. To simplify this, we set the connective word\nto OTHER if its word frequency is less than 12.\nFor all languages and models, the training con-\nﬁgurations are: maximum token length of each\nsentence = 50, learning rate = 1e-3, Adam epsilon\n= 1e-8, maximum gradient norm = 1.0, maximum\nepochs = 20, warmup = 10% of the training steps,\nand patience for early stopping = 10 epochs.\nB.4 RST-related Tasks\nIn Figures 7 and 8, we present the distribution of\nthe nuclearity and relation labels for the 4 different\ndiscourse treebanks. The English treebank is signif-\nicantly larger, with a strong preference for the NS\n(nuclear–satellite) relationship. Unlike other lan-\nguages, the proportion of NN (nuclear–nuclear)\nrelationships in the Chinese discourse treebank\n(CDTB) is the highest. We also notice that the\nrelation label set in CDTB is the simplest, with\nonly 4 labels.\nMost of the training details for nuclearity and\nrelation prediction are the same as for the NSP\ntask, except we set the maximum token length of\neach sentence to 250. Particularly for EDU seg-\nmentation, we set the maximum token length in a\ndocument to 512.\nB.5 Cloze Story Test\nAs discussed in Table 2, we use cloze story test\nversion-1 (Mostafazadeh et al., 2016). Although\nversion-2 (Sharma et al., 2018) is better in terms\nof story biases, the gold labels for the test set are\nnot publicly available, which limited our ability to\nexplore different layers of a broad range of pre-\ntrained language models (due to rate limiting of\ntest evaluation).\nFor the data split, we followed previous work\n(Liu et al., 2018) in splitting the development set\ninto a training and validation set. We perform bi-\nENGLISH \nT rain : \nbut:\t2237,\tand:\t2190,\tas:\t1547,\twhen:\t1085,\tif:\t993, \nbefore:\t462,\twhile:\t358,\tbecause:\t335,\tthough:\t229, \nafter:\t196,\tso:\t180,\talthough:\t84,\tstill:\t38,\tthen:\t35,\t \nalso:\t31 \nDevelopment : \nbut:\t222,\tand:\t192,\tas:\t181,\twhen:\t1 16,\tif:\t104,\t \nbecause:\t44,\tbefore:\t40,\twhile:\t28,\tthough:\t28,\tso:\t16, \nafter:\t16,\talso:\t5,\talthough:\t5,\tthen:\t2,\tstill:\t1 \nT est : \nand:\t21 1,\tbut:\t202,\tas:\t153,\twhen:\t129,\tif:\t97,\t \nbefore:\t48,\twhile:\t44,\tbecause:\t41,\tafter:\t20,\tthough:\t19, \nalthough:\t1 1,\tso:\t9,\tstill:\t7,\talso:\t5,\tthen:\t4 \nCHINESE \nT rain : \nother :\t520,\t 并 :\t182,\t 其中 :\t131,\t 也 :\t1 18,\t 但 :\t60,\t ⽽ :\t60, \n还 :\t55,\t 以 :\t47,\t 使 :\t43,\t 后 :\t42,\t 为 :\t41,\t 同时 :\t37,\t \n由于 :\t34,\t 因此 :\t28,\t 如 :\t26,\t 又 :\t20,\t 为了 :\t19,\t 如果 :\t17, \n⽽且 :\t16,\t 但是 :\t15,\t 因为 :\t15,\t 虽然 - 但 :\t13 \nDevelopment : \nother :\t22,\t 并 :\t7,\t 也 :\t6,\t ⽽ :\t6,\t 其中 :\t4,\t 但 :\t4,\t 因为 :\t4,\t 为 : \n3,\t 还 :\t3,\t ⽽且 :\t3,\t 又 :\t2,\t 如果 :\t2,\t 同时 :\t2,\t 使 :\t2,\t 后 :\t2, \n如 :\t1,\t 由于 :\t1,\t 虽然 - 但 :\t1,\t 为了 :\t1 \nT est : \nother :\t60,\t 其中 :\t18,\t 并 :\t18,\t 也 :\t10,\t 使 :\t10,\t 还 :\t9,\t \n同时 :\t8,\t ⽽ :\t6,\t 以 :\t5,\t 但 :\t5,\t 为 :\t4,\t 又 :\t3,\t 因为 :\t2,\t \n虽然 - 但 :\t2,\t 由于 :\t2,\t 为了 :\t2,\t 因此 :\t2,\t ⽽且 :\t1,\t 如 :\t1 \nGERMAN \nT rain : \nother :\t336,\tund:\t191,\tdoch:\t62,\twenn:\t56,\taber:\t56, \ndenn:\t36,\tdann:\t23,\tauch:\t23,\tsondern:\t19,\toder:\t19,\t \nso:\t18,\talso:\t17,\tdeshalb:\t16,\tweil:\t15,\tals:\t13 \nDevelopment : \nother :\t50,\tund:\t32,\tdoch:\t12,\twenn:\t1 1,\taber:\t9,\tdenn:\t6, \ndann:\t5,\tso:\t5,\tauch:\t5,\toder:\t4,\tdeshalb:\t3,\tweil:\t3, \nsondern:\t2,\tals:\t1 \nT est : \nother :\t69,\tund:\t23,\tdoch:\t1 1,\taber:\t10,\tdenn:\t9,\twenn:\t8, \ndann:\t8,\tso:\t4,\tweil:\t4,\tauch:\t4,\tsondern:\t3,\tdeshalb:\t3, \noder:\t2,\tals:\t1 \nFigure 5: Discourse connective word distribution.\nnary classiﬁcation similar to the NSP task, by ﬁrst\nmerging all 4-sentence stories into a single text\n(context). We limit the context to a maximum of\n450 tokens, and each candidate sentence (as the\nstory ending) is limited to 50 tokens. Other train-\ning details are the same as for the NSP task.\nS1:\t ⽬前，约有⼗五万家外商投资企业在中国银⾏开\n⽴帐户，\nS2:\t ⼆万多家获得中国银⾏的贷款⽀持。\nConnective\tword:\t \t 其中\nCHINESE \nGERMAN \nS1:\t der\tmann\tbezahlte\tviele\thandwerker\tnicht\t\nS2: \twurde\tvoriges\tjahr\tzu\teiner\tmehrjährigen\thaftstrafe\nverurteilt\nConnective\tword:\t und\nENGLISH \nS1:\t Two\tmen\tnudged\tthe\tdoor\topen.\nS2: \t\tSlipped\tinto\tthe\troom\twith\thim.\nConnective\tword:\t and\nFigure 6: Discourse connective: data examples\nENGLISH \nT rain:\t NS:\t10348,\tNN:\t3853,\tSN:\t2702 \nDevelopment:\t NS:\t1 195,\tNN:\t407,\tSN:\t341 \nT est:\t NS:\t1373,\tNN:\t507,\tSN:\t428 \nCHINESE \nT rain:\t NN:\t3133,\tNS:\t1784,\tSN:\t1242 \nDevelopment:\t NN:\t432,\tNS:\t219,\tSN:\t158 \nT est:\t NN:\t188,\tNS:\t107,\tSN:\t58 \nGERMAN \nT rain:\t SN:\t752,\tNS:\t733,\tNN:\t407 \nDevelopment:\t NS:\t1 16,\tSN:\t106,\tNN:\t67 \nT est:\t NS:\t150,\tSN:\t133,\tNN:\t72 \nSP ANISH \nT rain:\t NS:\t101 1,\tSN:\t570,\tNN:\t461 \nDevelopment:\t NS:\t163,\tSN:\t73,\tNN:\t71 \nT est:\tN S:\t21 1,\tSN:\t121,\tNN:\t89 \nFigure 7: Nuclearity label distribution.\nENGLISH \nelab:\t7830,\tattr:\t3041,\tlist:\t1957,\tsame:\t1390,\tcont: \n1 108,\tevid:\t967,\tback:\t931,\tcause:\t685,\teval:\t588,\tpurp: \n560,\ttemp:\t526,\tcond:\t326,\tcomp:\t299,\tmann:\t225, \nsumm:\t222,\ttopic:\t204,\tprob:\t153,\ttext:\t142 \nCHINESE \n并列类 :\t4144,\t 解说类 :\t1630,\t 因果类 :\t1333,\t \n转折类 :\t214 \nSP ANISH \nelaboración:\t625,\tpreparación:\t370,\tlista:\t257,\t \nfondo:\t178,\tunión:\t168,\tmedio:\t135,\tresultado:\t134, \ncircunstancia:\t122,\tpropósito:\t1 15,\tsecuencia:\t79, \ninterpretación:\t77,\tantítesis:\t67,\tcontraste:\t61,\tcausa:\t57, \nevidencia:\t49,\tcondición:\t47,\tconcesión:\t44, \njustificación:\t39,\tsame-unit:\t33,\tsolución:\t26, \nmotivación:\t21,\treformulación:\t16,\tconjunción:\t14, \ndisyunción:\t9,\tevaluación:\t9,\tresumen:\t8,\t \ncapacitación:\t5,\talternativa:\t3,\tunless:\t2 \nGERMAN \nreason:\t267,\tinterpretation:\t232,\telaboration:\t204,\t \njoint:\t203,\tbackground:\t163,\tlist:\t138,\tconcession:\t125, \nantithesis:\t123,\tconjunction:\t1 17,\tcondition:\t1 16, \ncircumstance:\t1 13,\te-elaboration:\t1 1 1,\tcause:\t101, \nevidence:\t99,\tpreparation:\t87,\tevaluation-s:\t80,\t \ncontrast:\t49,\tresult:\t46,\tevaluation-n:\t38,\tpurpose:\t30, \nsequence:\t29,\trestatement:\t17,\tmeans:\t1 1,\t \ndisjunction:\t10,\tsummary:\t9,\tsolutionhood:\t7,\tjustify:\t4, \notherwise:\t3,\tenablement:\t2,\tunless:\t1,\tmotivation:\t1 \nFigure 8: Relation label distribution.\nC Full Experimental Results\nLayer NSP Sent.\nOrd.\nDiscourse\nConn.\nNuclearity Relation EDU\nsegment.\nCloze\nST.\nBERT (English); std = 0.00 – 0.02\n1 0.36 0.28 0.28 0.62 0.38 0.30 0.58\n2 0.45 0.28 0.31 0.64 0.41 0.42 0.61\n3 0.79 0.28 0.30 0.62 0.40 0.49 0.63\n4 0.95 0.32 0.35 0.65 0.44 0.52 0.60\n5 0.97 0.31 0.44 0.75 0.54 0.55 0.66\n6 0.97 0.35 0.53 0.78 0.60 0.56 0.72\n7 0.96 0.33 0.57 0.80 0.65 0.56 0.72\n8 0.96 0.32 0.57 0.81 0.65 0.54 0.73\n9 0.96 0.34 0.59 0.80 0.65 0.52 0.72\n10 0.97 0.33 0.58 0.80 0.64 0.47 0.75\n11 0.97 0.31 0.59 0.79 0.63 0.44 0.76\n12 0.99 0.32 0.56 0.76 0.59 0.39 0.76\nRoBERTa (English); std = 0.00 – 0.02\n1 0.78 0.29 0.46 0.72 0.55 0.68 0.72\n2 0.86 0.31 0.48 0.73 0.56 0.92 0.73\n3 0.88 0.30 0.49 0.75 0.58 0.90 0.74\n4 0.95 0.34 0.51 0.77 0.59 0.88 0.75\n5 0.96 0.37 0.51 0.79 0.60 0.91 0.78\n6 0.96 0.37 0.52 0.79 0.61 0.86 0.78\n7 0.96 0.39 0.52 0.78 0.61 0.85 0.83\n8 0.95 0.37 0.54 0.78 0.61 0.87 0.86\n9 0.94 0.37 0.54 0.79 0.61 0.87 0.86\n10 0.94 0.36 0.54 0.78 0.61 0.88 0.86\n11 0.93 0.35 0.53 0.77 0.59 0.87 0.85\n12 0.90 0.31 0.48 0.75 0.56 0.73 0.82\nALBERT (English); std = 0.00 – 0.03\n1 0.34 0.29 0.29 0.63 0.40 0.47 0.56\n2 0.85 0.33 0.33 0.63 0.44 0.54 0.66\n3 0.91 0.30 0.32 0.64 0.45 0.53 0.68\n4 0.93 0.30 0.35 0.67 0.46 0.51 0.69\n5 0.96 0.30 0.35 0.67 0.47 0.47 0.70\n6 0.97 0.29 0.37 0.67 0.48 0.44 0.71\n7 0.97 0.29 0.40 0.68 0.48 0.40 0.73\n8 0.98 0.26 0.40 0.68 0.49 0.34 0.73\n9 0.97 0.23 0.40 0.68 0.49 0.32 0.75\n10 0.98 0.21 0.41 0.70 0.49 0.25 0.76\n11 0.98 0.17 0.43 0.73 0.52 0.18 0.77\n12 0.99 0.13 0.53 0.79 0.63 0.11 0.85\nELECTRA (English); std = 0.00 – 0.02\n1 0.86 0.27 0.47 0.72 0.54 0.42 0.72\n2 0.90 0.31 0.48 0.72 0.55 0.47 0.74\n3 0.90 0.31 0.49 0.73 0.55 0.45 0.74\n4 0.94 0.31 0.51 0.74 0.57 0.50 0.75\n5 0.96 0.35 0.52 0.77 0.59 0.54 0.76\n6 0.96 0.36 0.53 0.78 0.60 0.57 0.78\n7 0.96 0.37 0.53 0.79 0.61 0.54 0.78\n8 0.97 0.39 0.55 0.80 0.63 0.51 0.82\n9 0.97 0.41 0.56 0.80 0.63 0.48 0.86\n10 0.97 0.43 0.58 0.80 0.63 0.49 0.89\n11 0.97 0.42 0.57 0.80 0.64 0.52 0.89\n12 0.96 0.40 0.57 0.79 0.60 0.48 0.88\nTable 9: Full results for BERT, RoBERTa, ALBERT, and ELECTRA over English.\nLayer NSP Sent.\nOrd.\nDiscourse\nConn.\nNuclearity Relation EDU\nsegment.\nCloze\nST.\nGPT-2 (English); std = 0.00 – 0.02\n1 0.86 0.26 0.47 0.72 0.55 0.35 0.73\n2 0.87 0.26 0.48 0.73 0.56 0.37 0.73\n3 0.88 0.28 0.48 0.73 0.56 0.40 0.74\n4 0.90 0.30 0.51 0.75 0.57 0.40 0.76\n5 0.91 0.32 0.51 0.75 0.57 0.41 0.75\n6 0.93 0.33 0.52 0.77 0.59 0.42 0.76\n7 0.93 0.33 0.52 0.76 0.60 0.42 0.77\n8 0.92 0.34 0.51 0.77 0.59 0.41 0.77\n9 0.92 0.33 0.50 0.76 0.59 0.41 0.77\n10 0.91 0.31 0.49 0.75 0.58 0.41 0.77\n11 0.91 0.30 0.49 0.74 0.57 0.42 0.75\n12 0.85 0.28 0.47 0.73 0.55 0.38 0.72\nBART (English); Layers 7–12 are the decoder; std = 0.00 – 0.01.\n1 0.86 0.30 0.48 0.73 0.55 0.79 0.73\n2 0.92 0.34 0.49 0.76 0.58 0.88 0.76\n3 0.95 0.35 0.51 0.76 0.58 0.89 0.76\n4 0.96 0.38 0.52 0.78 0.60 0.86 0.78\n5 0.97 0.39 0.53 0.78 0.62 0.82 0.79\n6 0.96 0.41 0.52 0.80 0.62 0.62 0.78\n7 0.94 0.32 0.51 0.77 0.59 0.10 0.76\n8 0.95 0.39 0.54 0.79 0.61 0.23 0.77\n9 0.95 0.40 0.54 0.79 0.62 0.32 0.78\n10 0.95 0.40 0.54 0.80 0.62 0.31 0.81\n11 0.96 0.38 0.52 0.78 0.60 0.34 0.80\n12 0.95 0.36 0.52 0.77 0.59 0.47 0.82\nT5 (English); Layers 7–12 are the decoder; std = 0.00 – 0.03.\n1 0.77 0.27 0.39 0.71 0.50 0.26 0.71\n2 0.80 0.30 0.43 0.74 0.54 0.38 0.70\n3 0.82 0.32 0.45 0.75 0.55 0.40 0.73\n4 0.84 0.33 0.46 0.76 0.57 0.37 0.74\n5 0.87 0.33 0.45 0.76 0.57 0.33 0.72\n6 0.86 0.35 0.46 0.76 0.58 0.28 0.72\n7 0.77 0.28 0.41 0.73 0.54 0.24 0.71\n8 0.77 0.26 0.44 0.74 0.55 0.27 0.71\n9 0.77 0.24 0.46 0.75 0.56 0.27 0.71\n10 0.74 0.22 0.45 0.75 0.55 0.20 0.72\n11 0.70 0.22 0.44 0.73 0.54 0.11 0.72\n12 0.68 0.20 0.42 0.73 0.52 0.00 0.72\nTable 10: Full results for GPT-2, BART, and T5 over English.\nLayer NSP Sent.\nOrd.\nDiscourse\nConn.\nNuclearity Relation EDU\nsegment.\nChinese; std = 0.00 – 0.02.\n1 0.30 0.38 0.36 0.53 0.58 0.70\n2 0.47 0.38 0.36 0.53 0.58 0.75\n3 0.47 0.42 0.36 0.53 0.58 0.79\n4 0.83 0.42 0.36 0.53 0.58 0.79\n5 0.90 0.44 0.35 0.59 0.60 0.81\n6 0.93 0.44 0.39 0.60 0.64 0.83\n7 0.94 0.45 0.42 0.64 0.67 0.83\n8 0.94 0.44 0.43 0.64 0.66 0.83\n9 0.94 0.43 0.46 0.63 0.68 0.83\n10 0.96 0.43 0.44 0.64 0.68 0.83\n11 0.96 0.42 0.46 0.61 0.66 0.81\n12 0.98 0.40 0.44 0.62 0.66 0.78\nGerman; std = 0.00 – 0.07.\n1 0.43 0.58 0.44 0.48 0.16 0.55\n2 0.60 0.58 0.43 0.49 0.18 0.56\n3 0.77 0.59 0.43 0.47 0.17 0.67\n4 0.76 0.61 0.43 0.44 0.21 0.71\n5 0.98 0.63 0.38 0.48 0.26 0.73\n6 0.99 0.65 0.43 0.54 0.29 0.74\n7 1.00 0.65 0.43 0.58 0.31 0.76\n8 0.99 0.64 0.43 0.60 0.35 0.75\n9 1.00 0.64 0.44 0.59 0.33 0.69\n10 0.99 0.64 0.45 0.58 0.35 0.65\n11 1.00 0.63 0.43 0.58 0.33 0.58\n12 1.00 0.63 0.38 0.58 0.33 0.59\nSpanish; std = 0.00 – 0.02.\n1 0.39 0.49 — 0.50 0.29 0.43\n2 0.55 0.52 — 0.56 0.31 0.50\n3 0.56 0.53 — 0.58 0.31 0.52\n4 0.96 0.55 — 0.62 0.37 0.57\n5 0.98 0.56 — 0.64 0.41 0.59\n6 0.99 0.56 — 0.68 0.45 0.62\n7 1.00 0.57 — 0.68 0.47 0.64\n8 1.00 0.58 — 0.75 0.49 0.69\n9 1.00 0.58 — 0.74 0.51 0.66\n10 1.00 0.58 — 0.77 0.56 0.62\n11 1.00 0.57 — 0.77 0.55 0.59\n12 1.00 0.56 — 0.76 0.54 0.50\nTable 11: Full results for the BERT monolingual models over Chinese, German, and Spanish.\nLayer NSP Sent.\nOrd.\nDiscourse\nConn.\nNuclearity Relation EDU\nsegment.\nCloze\nST.\nBERT-Large (English); std = 0.00 – 0.02.\n1 0.34 0.26 0.29 0.60 0.37 0.23 0.61\n2 0.53 0.25 0.33 0.62 0.40 0.28 0.67\n3 0.57 0.26 0.32 0.63 0.42 0.29 0.66\n4 0.60 0.29 0.32 0.63 0.42 0.40 0.66\n5 0.64 0.30 0.35 0.65 0.43 0.43 0.69\n6 0.82 0.31 0.37 0.66 0.44 0.45 0.68\n7 0.87 0.31 0.39 0.66 0.45 0.44 0.69\n8 0.95 0.30 0.39 0.66 0.45 0.44 0.72\n9 0.96 0.32 0.40 0.67 0.47 0.43 0.70\n10 0.96 0.32 0.40 0.67 0.46 0.49 0.71\n11 0.97 0.32 0.39 0.68 0.47 0.53 0.70\n12 0.97 0.33 0.47 0.77 0.57 0.58 0.71\n13 0.97 0.34 0.52 0.78 0.61 0.56 0.71\n14 0.97 0.34 0.57 0.81 0.65 0.59 0.73\n15 0.97 0.34 0.61 0.82 0.67 0.58 0.75\n16 0.97 0.35 0.60 0.83 0.67 0.55 0.75\n17 0.97 0.34 0.62 0.82 0.68 0.53 0.82\n18 0.98 0.36 0.63 0.82 0.68 0.54 0.82\n19 0.99 0.37 0.63 0.82 0.67 0.50 0.83\n20 0.99 0.34 0.63 0.81 0.67 0.48 0.84\n21 0.99 0.35 0.63 0.81 0.65 0.41 0.83\n22 0.99 0.35 0.61 0.81 0.65 0.37 0.84\n23 0.99 0.34 0.59 0.80 0.63 0.36 0.82\n24 0.99 0.33 0.57 0.77 0.58 0.31 0.81\nTable 12: Full results of English BERT-large.\nD Frozen vs. Fine-tuned BERT Layers\n2 4 6 8 10 12\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Score\nFrozen/uni00A0Layers\n2 4 6 8 10 12\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Score\nFine/uni00ADtuned/uni00A0Layers\nNSP/uni00A0with/uni00A04/uni00A0MC\nSent./uni00A0Ordering\nDiscourse/uni00A0Connective\nNuclearity\nRelation\nEDU/uni00A0segmentation\nCloze\nFigure 9: A comparison of BERT with frozen vs. ﬁne-tuned layers.\nE Full Results of Models with Average Pooling\n2 4 6 8 10 12\nLayer\n0.4\n0.6\n0.8\n1.0Accuracy\nNSP/uni00A0with/uni00A04/uni00A0MC\n2 4 6 8 10 12\nLayer\n0.2\n0.3\n0.4Spearman/uni00A0Rank\nSent./uni00A0Ordering\n2 4 6 8 10 12\nLayer\n0.3\n0.4\n0.5Accuracy\nDiscourse/uni00A0Connective\n2 4 6 8 10 12\nLayer\n0.65\n0.70\n0.75\n0.80Accuracy\nNuclearity\n2 4 6 8 10 12\nLayer\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65Accuracy\nRelation\n2 4 6 8 10 12\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8F1/uni00ADmacro\nEDU/uni00A0segmentation\n2 4 6 8 10 12\nLayer\n0.6\n0.7\n0.8\n0.9Accuracy\nCloze\n2 4 6 8 10 12\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8Score\nAverage/uni00A0of/uni00A0Normalized/uni00A0Scores\nBERT\nRoBERTa\nALBERT\nGPT/uni00AD2\nELECTRA\nBART\nT5\nFigure 10: Full results of all models over English with average pooling on all tasks except in EDU segmentation\n(with the only differences over Figure 2 being for BERT and ALBERT, where we originally used [CLS] embed-\ndings on two-text classiﬁcation probing tasks).\nF [CLS] vs. Average Pooling in English BERT-base Model\nAverage pooling generally performs worse than [CLS] embeddings in the last layers of BERT.\n2 4 6 8 10 12\nLayer\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Score\n[CLS]/uni00A0vs/uni00A0Average/uni00A0Pooling/uni00A0in/uni00A0BERT/uni00A0model\nNSP/uni00A0with/uni00A04/uni00A0MC/uni00A0[CLS]\nDiscourse/uni00A0Connective/uni00A0[CLS]\nNuclearity/uni00A0[CLS]\nRelation/uni00A0[CLS]\nCloze/uni00A0[CLS]\nNSP/uni00A0with/uni00A04/uni00A0MC/uni00A0(Avg./uni00A0Pooling)\nDiscourse/uni00A0Connective/uni00A0(Avg./uni00A0Pooling)\nNuclearity/uni00A0(Avg./uni00A0Pooling)\nRelation/uni00A0(Avg./uni00A0Pooling)\nCloze/uni00A0(Avg./uni00A0Pooling)\nFigure 11: Comparison of [CLS] vs. average pooling embeddings for BERT-base across the ﬁve tasks for English.\nPlease note that sentence ordering and EDU segmentation are always performed with average pooling embeddings\nand sequence labelling at the (sub)word level, respectively.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.781022846698761
    },
    {
      "name": "Encoder",
      "score": 0.6877809166908264
    },
    {
      "name": "Language model",
      "score": 0.6820546388626099
    },
    {
      "name": "Baseline (sea)",
      "score": 0.6730974316596985
    },
    {
      "name": "Natural language processing",
      "score": 0.6501460671424866
    },
    {
      "name": "Sentence",
      "score": 0.6316273212432861
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5760210156440735
    },
    {
      "name": "Ranging",
      "score": 0.4225670099258423
    },
    {
      "name": "Linguistics",
      "score": 0.3437650203704834
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165779595",
      "name": "The University of Melbourne",
      "country": "AU"
    }
  ],
  "cited_by": 4
}