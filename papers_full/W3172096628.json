{
  "title": "UniDrop: A Simple yet Effective Technique to Improve Transformer without Extra Cost",
  "url": "https://openalex.org/W3172096628",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5055397380",
      "name": "Zhen Wu",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A5102750692",
      "name": "Lijun Wu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5101704281",
      "name": "Qi Meng",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5021772140",
      "name": "Yingce Xia",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5088666942",
      "name": "Shufang Xie",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5020025718",
      "name": "Tao Qin",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5102994315",
      "name": "Xinyu Dai",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A5115592065",
      "name": "Tie‚ÄêYan Liu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4297798436",
    "https://openalex.org/W3034857244",
    "https://openalex.org/W3035302202",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2946068894",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2989499211",
    "https://openalex.org/W3098985395",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2890166761",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3007325053",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962821226",
    "https://openalex.org/W3006381853",
    "https://openalex.org/W3036939249",
    "https://openalex.org/W2409027918",
    "https://openalex.org/W3043859333",
    "https://openalex.org/W2903810591",
    "https://openalex.org/W2963975324",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2904614653",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3125700282",
    "https://openalex.org/W2953173959",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W3106031848",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W4293714597",
    "https://openalex.org/W2767989436",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2740721704",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W4297803870",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2964582580",
    "https://openalex.org/W4288333985",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2581377246",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2948981900",
    "https://openalex.org/W2963000090",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W2994928925",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W3087346608",
    "https://openalex.org/W2946232455",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2952634764",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2250473257",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W2408279554",
    "https://openalex.org/W2144513243"
  ],
  "abstract": "Zhen Wu, Lijun Wu, Qi Meng, Yingce Xia, Shufang Xie, Tao Qin, Xinyu Dai, Tie-Yan Liu. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 3865‚Äì3878\nJune 6‚Äì11, 2021. ¬©2021 Association for Computational Linguistics\n3865\nUniDrop: A Simple yet Effective Technique to Improve Transformer\nwithout Extra Cost\nZhen Wu1‚àó Lijun Wu2 Qi Meng2 Yingce Xia2 Shufang Xie2\nTao Qin2 Xinyu Dai1 Tie-Yan Liu2\n1National Key Laboratory for Novel Software Technology, Nanjing University\n2Microsoft Research Asia\nwuz@smail.nju.edu.cn, daixinyu@nju.edu.cn\n{Lijun.Wu,meq,yingce.xia,shufxi,taoqin,tyliu}@microsoft.com\nAbstract\nTransformer architecture achieves great suc-\ncess in abundant natural language processing\ntasks. The over-parameterization of the Trans-\nformer model has motivated plenty of works\nto alleviate its overÔ¨Åtting for superior perfor-\nmances. With some explorations, we Ô¨Ånd sim-\nple techniques such as dropout, can greatly\nboost model performance with a careful de-\nsign. Therefore, in this paper, we integrate dif-\nferent dropout techniques into the training of\nTransformer models. SpeciÔ¨Åcally, we propose\nan approach named UniDrop to unite three\ndifferent dropout techniques from Ô¨Åne-grain\nto coarse-grain, i.e., feature dropout, structure\ndropout, and data dropout. Theoretically, we\ndemonstrate that these three dropouts play dif-\nferent roles from regularization perspectives.\nEmpirically, we conduct experiments on both\nneural machine translation and text classiÔ¨Åca-\ntion benchmark datasets. Extensive results\nindicate that Transformer with UniDrop can\nachieve around 1.5 BLEU improvement on\nIWSLT14 translation tasks, and better accu-\nracy for the classiÔ¨Åcation even using strong\npre-trained RoBERTa as backbone.\n1 Introduction\nIn recent years, Transformer (Vaswani et al., 2017)\nhas been the dominant structure in natural language\nprocessing (NLP), such as neural machine transla-\ntion (Vaswani et al., 2017), language modeling (Dai\net al., 2019) and text classiÔ¨Åcation (Devlin et al.,\n2019; Liu et al., 2019). To further improve the\nmodel performance, there has been much effort in\ndesigning better architectures or introducing exter-\nnal knowledge into Transformer models (Wu et al.,\n2019; Lu et al., 2019; Kitaev et al., 2020; Ahmed\net al., 2017; Hashemi et al., 2020), which increases\ncomputational costs or requires extra resources.\nDespite the effectiveness of above strategies,\nthe over-parameterization and overÔ¨Åtting is still\n‚àóThis work was done when Zhen Wu was a research\nintern at Microsoft Research Asia.\na crucial problem for Transformer. Regularization\nmethods such as weight decay (Krogh and Hertz,\n1992), data augmentation (Sennrich et al., 2016a),\ndropout (Srivastava et al., 2014), parameter shar-\ning (Dehghani et al., 2018; Xia et al., 2019) are\nall widely adopted to address overÔ¨Åtting. Among\nthese regularization approaches, dropout (Srivas-\ntava et al., 2014), which randomly drops out some\nhidden units during training, is the most popular\none and various dropout techniques have been pro-\nposed for Transformer. For example, Fan et al.\n(2020a) propose LayerDrop, a random structured\ndropout, to drop certain layers of Transformer dur-\ning training. Zhou et al. (2020) alternatively pro-\npose DropHead as a structured dropout method for\nregularizing the multi-head attention mechanism.\nBoth of them achieved promising performances.\nOne great advantage of dropout is that it is free\nof additional computational costs and resource re-\nquirements. Hence we ask one question: can we\nachieve stronger or even state-of-the-art (SOTA)\nresults only relying on various dropout techniques\ninstead of extra model architecture design or knowl-\nedge enhancement?\nTo this end, in this paper, we proposeUniDrop\nto integrate three different-level dropout techniques\nfrom Ô¨Åne-grain to coarse-grain, feature dropout,\nstructure dropout, and data dropout, into Trans-\nformer models. Feature dropout is the conventional\ndropout (Srivastava et al., 2014) that we introduced\nbefore, which is widely applied on hidden rep-\nresentations of networks. Structure dropout is a\ncoarse-grained control and aims to randomly drop\nsome entire substructures or components from the\nwhole model. In this work, we adopt the afore-\nmentioned LayerDrop (Fan et al., 2020a) as our\nstructure dropout. Different from the previous two\ndropout methods, data dropout (Iyyer et al., 2015)\nis performed on the input data level, which serves\nas a data augmentation method by randomly drop-\nping out some tokens in an input sequence.\n3866\nFeed \nForward\nInput \nProjection\nN√ó\nPositional \nEncoding\nMulti-head\nAttention\nAdd & Norm\nAdd & Norm\ndropout\ndropout\n(a) Transformer architecture.\nMatMul\nScale\nSoftmax\nMatMul\nQ K V\nSoftmax\nOutput \nProjection\nH\nLinear\nActivation\nLinear\nH\n(b1) Attention (b2) FeedForward (b3) Output Prediction\nFD-2\nFD-1\nFD-3\nFD-4 (b) Structure and overview of feature dropout.\nFigure 1: Transformer structure and feature dropout applied in different Transformer components.\nWe Ô¨Årst theoretically analyze different regular-\nization roles played by the three dropout tech-\nniques, and we show they can improve the gen-\neralization ability from different aspects. Then,\nwe provide empirical evaluations of the UniDrop\napproach. We conduct experiments on neural ma-\nchine translation with 8 translation datasets, and\ntext classiÔ¨Åcation task with 8 benchmark datasets.\nOn both sequence generation and classiÔ¨Åcation\ntasks, experimental results show that the three\ndropouts in UniDrop can jointly improve the per-\nformance of Transformer.\nThe contributions of this paper can be summa-\nrized as follows:\n‚Ä¢We introduce UniDrop, which unites three\ndifferent dropout techniques into a robust one\nfor Transformer, to jointly improve the per-\nformance of Transformer without additional\ncomputational cost and prior knowledge.\n‚Ä¢We theoretically demonstrate that the three\ndropouts, i.e., feature dropout, structure\ndropout, and data dropout play different roles\nin preventing Transformer from overÔ¨Åtting\nand improving the robustness of the model.\n‚Ä¢Extensive results indicate that Transformer\nmodels with UniDrop can achieve strong or\neven SOTA performances on sequence gen-\neration and classiÔ¨Åcation tasks. SpeciÔ¨Åcally,\naround 1.5 BLEU improvement on IWSLT14\ntranslation tasks, and better accuracy for clas-\nsiÔ¨Åcation even using strong pre-trained model\nRoBERTa as backbone.\n2 Background\nFeature dropout (FD) and structure dropout (SD)\nare highly coupled with model architecture. There-\nfore, we brieÔ¨Çy recap Transformer and refer the\nreaders to Vaswani et al. (2017) for details.\nAs shown in Figure 1a, Transformer is stacked\nby several identical blocks, and each block con-\ntains two sub-layers, which are multi-head self-\nattention layer and position-wise fully connected\nfeed-forward layer. Each sub-layer is followed by\nan AddNorm operation that is a residual connection\nAdd (He et al., 2016) and a layer normalization\nLN (Ba et al., 2016).\nMulti-head Attention sub-layer consists of multi-\nple parallel attention heads, and each head maps\nthe query Q and a set of key-value pairs K,V to\nan output through a scale dot-product attention:\nAttn(Q,K,V) = softmax(QK‚ä§\n‚àödk\n)V, (1)\nwhere dk is the dimension of query and key, and\n1‚àödk\nis a scaling factor. The outputs of these heads\nare then concatenated and projected again to result\nin the Ô¨Ånal values.\nPosition-wise Feed-Forward sub-layer ap-\nplies two linear transformations with an inner\nReLU (Nair and Hinton, 2010) activation:\nFFN(x) = max(0,xW1 + b1)W2 + b2, (2)\nwhere W and b are parameters.\nThe output of each sub-layer is then followed\nwith AddNorm: AddNorm(x) =LN(Add(x)).\n3867\n3 UniDrop\nIn this section, we Ô¨Årst introduce the details of\nthe three different levels of dropout techniques we\nstudy, feature dropout, structure dropout and data\ndropout. Then we provide the theoretical analy-\nsis of these dropout methods on the regularization\nperspectives. Finally, we present our proposed\nUniDrop approach for training Transformer.\n3.1 Feature Dropout\nThe feature dropout (FD), as a well-known regu-\nlarization method, is proposed by Srivastava et al.\n(2014), which is to randomly suppress neurons of\nneural networks during training by setting them to\n0 with a pre-deÔ¨Åned probability p.\nIn practice, dropout is applied to the output of\neach sub-layer by default. Besides, Transformer\nalso contains two speciÔ¨Åc feature dropouts for\nmulti-head attention and activation layer of feed-\nforward network. In this work, we also explore\ntheir effects on the performance of Transformer.\n‚Ä¢FD-1 (attention dropout): according to Equa-\ntion (1), we can obtain attention weight matrix\nA = QK‚ä§towards value sequence V. Our\nFD-1 is applied to the attention weight A.\n‚Ä¢FD-2 (activation dropout): FD-2 is employed\nafter the activation function between the two\nlinear transformations of FFN sub-layer.\nIn addition to the above FDs for Transformer, we\nstill Ô¨Ånd the risk of overÔ¨Åtting in pre-experiments.\nTherefore, we further introduce another two feature\ndropouts into the model architecture:\n‚Ä¢FD-3 (query, key, value dropout): FD-1 is\nused to improve generalization of multi-head\nattention. However, it is directly applied to the\nattention weights A, where drop value A(i,j)\nmeans ignore the relation between token iand\ntoken j, thus a larger FD-1 means a larger\nrisk of losing some critical information from\nsequence positions. To alleviate this potential\nrisk, we add dropout to query, key, and value\nbefore the calculation of attention.\n‚Ä¢FD-4 (output dropout): we also apply dropout\nto the output features before linear transforma-\ntion for softmax classiÔ¨Åcation. SpeciÔ¨Åcally,\nwhen dealing with sequence-to-sequence\ntasks such as machine translation, we add FD-\n4 to the output features of the last layer in the\nTransformer decoder, otherwise the last layer\nof the Transformer encoder.\nThe positions of each feature dropout applied in\nTransformer1 are shown in Figure 1b.\n3.2 Structure Dropout\nThere are three structure dropouts, respectively\nLayerDrop (Fan et al., 2020a), DropHead (Zhou\net al., 2020) and HeadMask (Sun et al., 2020),\nwhich are speciÔ¨Åcally designed for Transformer.\nSome recent studies (V oita et al., 2019; Michel\net al., 2019) show multi-head attention mecha-\nnism is dominated by a small portion of attention\nheads. To prevent domination and excessive co-\nadaptation between different attention heads, Zhou\net al. (2020) and Sun et al. (2020) respectively pro-\npose structured DropHead and HeadMask that drop\ncertain entire heads during training. In contrast,\nLayerDrop (Fan et al., 2020a) is a higher-level and\ncoarser-grained structure dropout. It drops some\nentire layers at training time and directly reduces\nthe Transformer model size.\nIn this work, we adopt LayerDrop as the struc-\nture dropout to incorporate it into our UniDrop.\n3.3 Data Dropout\nData dropout aims to randomly remove some words\nin the sentence with a pre-deÔ¨Åned probability. It is\noften used as a data augmentation technique (Wei\nand Zou, 2019; Xie et al., 2020). However, di-\nrectly applying vanilla data dropout is hard to keep\nthe original sequence for training, which leads\nto the risk of losing high-quality training sam-\nples. To address this issue, we propose a two-\nstage data dropout strategy . SpeciÔ¨Åcally, given\na sequence, with probability pk (a hyperparame-\nter lies in (0,1)), we keep the original sequence\nand do not apply data dropout. If data dropout is\napplied, for each token, with another probability\np(another hyperparameter lies in (0,1)), we will\ndrop the token.\n3.4 Theoretical Analysis\nIn this section, we provide theoretical analysis for\nfeature dropout, structure dropout and data dropout,\nto show their different regularization effects. We\nÔ¨Årst re-formulate the three dropout methods. For\nsome probability pand layer representation h‚ààRd\n(i.e., his the vector of outputs of some layer), we\n1We also explored other positions for feature dropout, but\ntheir performances are not so good (see Appendix A.3).\n3868\nrandomly sample a scaling vector Œæ ‚ààRd with\neach independent coordinate as follows:\nŒæi =\n{\n‚àí1 with probability p\np\n1 ‚àíp with probability 1-p. (3)\nHere, i indexes a coordinate of Œæ, i ‚àà[1,...,d ].\nThen feature dropout can be applied by computing\nhfd = (1 + Œæ) ‚äôh,\nwhere ‚äôdenotes element-wised product and 1 =\n(1,1,¬∑¬∑¬∑ ,1)‚Ä≤.\nWe use F(hfd(x)) to denote the output of a\nmodel after dropping feature from a hidden layer\nand Lto denote the loss function. Similar to Wei\net al. (2020), we apply Taylor expansion to Land\ntake expectation to Œæ:\nEŒæL(F(hfd(x))) =EŒæL(F((1 + Œæ) ‚äôh(x)))\n‚âàL(F(h(x)) +1\n2EŒæ(Œæ‚äôh(x))TD2\nhL(x)(Œæ‚äôh(x))\n= L(F(h(x)) + p\n2(1 ‚àíp)\nd‚àë\nj=1\nD2\nhj ,hj L(x) ¬∑hj(x)2, (4)\nwhere D2\nhLis the Hessian matrix of loss with re-\nspect to hidden output h and D2\nhj,hjL(x) is the\nj-th diagonal element of D2\nhL. Expect the orig-\ninal loss L(F(h(x))), the above formula shows\nthat feature dropout implicitly regularize the term‚àëd\nj=1 D2\nhj,hjL(x) ¬∑hj(x)2, which relates to the\ntrace of the Hessian.\nFor structure dropout, we use a 1-dim random\nscalar Œ∑ ‚ààR whose distribution is: Œ∑ = ‚àí1 with\nprobability p, and Œ∑= 0with probability 1‚àíp. The\nstructure dropout is similarly applied by computing\nhsd = (1 +Œ∑) ¬∑h.\nFor input data x‚ààRm, here xis a sequence of\ntokens and mis the sequence length, we sample a\nrandom scaling vector Œ≤ ‚ààRm with independent\nrandom coordinates where each coordinate is iden-\ntically distributed as Œ∑. The input data after drop\ndata becomes xdd = (1 + Œ≤) ‚äôx.\nSimilar to feature dropout, we can obtain that\ndata dropout implicitly optimizes the regularized\nloss as follows: L(F(h(x))) ‚àíp ¬∑xT‚àáxL(x) + p ¬∑‚àëm\nj=1 D2\nxj ,xj L(x) ¬∑x2\nj, and structure dropout implic-\nitly optimizes the regularized loss: L(F(h(x))) ‚àí\np¬∑h(x)T‚àáhL(x) +p¬∑‚àëm\ni,j=1 D2\nhi,hj L(x) ¬∑hi(x)hj(x),\nwhere D2\nhi,hjL(x) is the (i,j)-th element in Hes-\nsian matrix D2\nhL.\nInterpretation From the above analysis, we can\nconclude that feature dropout, structure dropout\nand data dropout regularize different terms of the\nData\nDropout\nLayer\nDropout\nFeature\nDropout\nùë°ùëñ‚àí1 ùë°ùëñ ùë°ùëñ+1 ùë°ùëñ+2\nFigure 2: Different dropout components in UniDrop.\nThe gray positions denote applying the corresponding\ndropout.\nmodel, and they can not be replaced by each other.\n(1) Because the hidden output will be normalized\nby layer normalization, the term h(x)T‚àáhL(x)\nequals to zero according to Lemma 2.4 in Arora\net al. (2019). Therefore, structure dropout im-\nplicitly regularizes the term ‚àëm\ni,j=1 D2\nhi,hjL(x).\nHence, structure dropout can regularize the whole\nelements of Hessian of the model with respect to\nhidden output, while feature dropout only regular-\nizes the diagonal elements of the Hessian. Thus,\nintegrating structure dropout and feature dropout\ncan regularize every component of Hessian with\nemphasizing the diagonal elements of the Hessian.\n(2) Since xis also normalized, the term xT‚àáxL(x)\nequals to zero according to Lemma 2.4 in Arora\net al. (2019). Different from feature dropout and\nstructure dropout, data dropout regularizes Hessian\nof loss with respect to input data.\nRegularizing Hessian matrix with respect to both\ninput and hidden output can improve model robust-\nness and hence the generalization ability. We put\nmore details in Appendix A.1.\n3.5 UniDrop Integration\nFrom the above theoretical analysis, the three\ndropout techniques are performed in different ways\nto regularize the training of Transformer, each with\nunique property to improve the model generaliza-\ntion. Therefore, we introduce UniDrop to take\nthe most of each dropout into Transformer. The\noverview of UniDrop is presented in Figure 2.\nTo better view each dropout in a model forward\npass, we only show a three layers of architecture in\nFigure 2, and each layer with one speciÔ¨Åc dropout\ntechnique. The data dropout is applied in the input\nlayer by dropping out some word embeddings (e.g.,\nembedding of word ti is dropped). In the middle\nlayer, the feature dropout randomly drops several\nneurons in each word representations (e.g., the third\nneurons of word ti‚àí1 is dropped). The last layer is\n3869\ndirectly dropped out through layer dropout2.\n4 Experiments\nWe conduct experiments on both sequence gener-\nation and classiÔ¨Åcation tasks, speciÔ¨Åcally, neural\nmachine translation and text classiÔ¨Åcation, to vali-\ndate the effectiveness of UniDrop for Transformer.\n4.1 Neural Machine Translation\nIn this section, we introduce the detailed settings\nfor the neural machine translation tasks and report\nthe experimental results.\n4.1.1 Datasets\nWe adopt the widely acknowledged IWSLT14\ndatasets3 with multiple language pairs, including\nEnglish‚ÜîGerman (En‚ÜîDe), English‚ÜîRomanian\n(En‚ÜîRo), English ‚ÜîDutch (En ‚ÜîNl), and\nEnglish‚ÜîPortuguese-Brazil (En‚ÜîPt-br), a total\nnumber of 8 translation tasks. Each dataset\ncontains about 170k‚àº190k translation data pairs.\nThe datasets are processed by Moses toolkit4 and\nbyte-pair-encoding (BPE) (Sennrich et al., 2016b)\nis applied to obtain subword units. The detailed\nstatistics of datasets are shown in Appendix A.2.\n4.1.2 Model\nWe use thetransformer_iwslt_de_en con-\nÔ¨Åguration5 for all Transformer models. SpeciÔ¨Å-\ncally, the encoder and decoder both consist of 6\nblocks. The source and target word embeddings are\nshared for each language pair. The dimensions of\nembedding and feed-forward sub-layer are respec-\ntively set to 512 and 1024, the number of attention\nheads is 4. The default dropout (not our four fea-\nture dropout) rate is0.3 and weight decay is0.0001.\nAll models are optimized with Adam (Kingma and\nBa, 2015) and the learning rate schedule is same\nas in Vaswani et al. (2017). The weight of label\nsmoothing (Pereyra et al., 2017) is set to 0.1.\nFor the Transformer models with our UniDrop,\nwe set all feature dropout rates to0.1. The structure\ndropout LayerDrop is only applied to the decoder\nwith rate 0.1. For the data dropout, the sequence\n2Except the data dropout is only applied in the input layer,\nfeature/structure dropout can be applied in each layer.\n3https://wit3.fbk.eu/mt.php?release=\n2014-01\n4https://github.com/moses-smt/\nmosesdecoder/tree/master/scripts\n5https://github.com/pytorch/fairseq\nkeep rate pk and token dropout rate pare respec-\ntively 0.5 and 0.2. The other settings are the same\nas the conÔ¨Åguration of the baseline Transformer.\nTo evaluate the model performance, we use beam\nsearch (Sutskever et al., 2014) algorithm to gener-\nate the translation results. The beam width is 5\nand the length penalty is 1.0. The evaluation met-\nric is the tokenized BLEU (Papineni et al., 2002)\nscore with multi-bleu.perl script6. We re-\npeat each experiment three times with different\nseeds and report the average BLEU.\n4.1.3 Results\nTable 1 shows the BLEU results of the Transformer\nbaselines and models with different dropouts. Com-\npared with baselines, we can see that the dropouts\nFD, SD, or DD all bring some improvements 7.\nThis observation veriÔ¨Åes the existence of overÔ¨Åt-\nting in the Transformer. In contrast, our model\nTransformer+UniDrop achieves the most improve-\nments across all translation tasks, which demon-\nstrates the effectiveness of UniDrop for the Trans-\nformer architecture. To further explore the ef-\nfects of the three different grained dropouts in\nUniDrop, we conduct ablation studies and re-\nspectively remove the FD, SD, and DD from\nTransformer+UniDrop. The results in Table 1\nshow that three ablated models obtain lower BLEU\nscores compared to the full model. This observa-\ntion validates the necessity of them for UniDrop.\nAmong all ablation versions, the Transformer-\nUniDrop w/o FD obtains the least improvements.\nIt is reasonable because FD actually contains four\nfeature dropouts on different positions, which can\neffectively prevent Transformer from overÔ¨Åtting.\nTo show the superiority of UniDrop, we also\ncompare the Transformer+UniDrop with several\nexisting works on the widely acknowledged bench-\nmark IWSLT14 De‚ÜíEn translation. These works\nimprove machine translation from different as-\npects, such as the training algorithm design (Wang\net al., 2019b), model architecture design (Lu et al.,\n2019; Wu et al., 2019) and data augmentation (Gao\net al., 2019). The detailed results are shown in Ta-\nble 2. We can see that the Transformer model with\nour UniDrop outperforms all previous works and\nachieve state-of-the-art performance, with 36.88\n6https://github.com/moses-smt/\nmosesdecoder/blob/master/scripts/\ngeneric/multi-bleu.perl\n7The dropout rates of model Transformer+FD, Trans-\nformer+SD, Transformer+DD are tuned with IWSLT14\nDe‚ÜíEn dev set and respectively set to 0.2, 0.2, 0.3.\n3870\nEn‚ÜíDe De ‚ÜíEn En ‚ÜíRo Ro ‚ÜíEn En ‚ÜíNl Nl ‚ÜíEn Nn ‚ÜíPt-br Pt-br‚ÜíEn Avg. ‚ñ≥\nTransformer 28.67 34.84 24.74 32.14 29.64 33.28 39.08 43.63 33.25 -\n+FD 29.61 36.08 25.45 33.12 30.37 34.50 40.10 44.74 34.24 +0.99\n+SD 29.03 35.09 25.03 32.69 29.97 33.94 39.78 44.02 33.69 +0.44\n+DD 28.83 35.26 24.98 32.76 29.72 34.00 39.50 43.71 33.59 +0.34\n+UniDrop 29.99 36.88 25.77 33.49 31.01 34.80 40.62 45.62 34.77 +1.52\nw/o FD 29.24 35.68 25.18 33.17 30.16 33.90 39.97 44.81 34.01 +0.76\nw/o SD 29.92 36.70 25.59 33.26 30.55 34.75 40.45 45.60 34.60 +1.35\nw/o DD 29.76 36.38 25.44 33.26 30.86 34.55 40.37 45.27 34.49 +1.24\nTable 1: Machine translation results of the standard Transformer and our models on various IWSLT14 translation\ndatasets. The ‚Äú+FD‚Äù, ‚Äú+SD‚Äù, ‚Äú+DD‚Äù, and ‚Äú+UniDrop‚Äù denotes applying the feature dropout, structure dropout,\ndata dropout, or UniDrop to the standard Transformer. The ‚Äúw/o FD‚Äù, ‚Äúw/o SD‚Äù and ‚Äúw/o DD‚Äù respectively indi-\ncate the removal of the feature dropout, structure dropout, or data dropout from the model Transformer+UniDrop.\nAvg. and ‚ñ≥denote the average results of the 8 translation tasks and improvements compared with the standard\nTransformer. Best results are in bold.\nApproaches BLEU\nAdversarial MLE (Wang et al., 2019b) 35.18\nDynamicConv (Wu et al., 2019) 35.20\nMacaron (Lu et al., 2019) 35.40\nIOT (Zhu et al., 2021) 35.62\nSoft Contextual Data Aug (Gao et al., 2019) 35.78\nBERT-fused NMT (Zhu et al., 2020) 36.11\nMAT (Fan et al., 2020b) 36.22\nMixReps+co-teaching (Wu et al., 2020) 36.41\nTransformer 34.84\n+UniDrop 36.88\nTable 2: Comparison with existing works on IWSLT-\n2014 De‚ÜíEn translation task.\nApproaches En‚ÜíDe Ro ‚ÜíEn Nl ‚ÜíEn\nMAT (Fan et al., 2020b) 29.90 - -\nMixReps+co-teaching (Wu et al., 2020)29.93 33.12 34.45\nTransformer 28.67 32.14 33.38\n+UniDrop 29.99 33.49 34.80\nTable 3: Comparison with existing works on IWSLT-\n2014 En‚ÜíDe, Ro‚ÜíEn, and Nl‚ÜíEn translation tasks.\nBLEU score. Especially, it surpasses the BERT-\nfused NMT model (Zhu et al., 2020), which incor-\nporates the pre-trained language model BERT, by\na non-trivial margin. We also show some compar-\nisons on IWSLT14 En‚ÜíDe, Ro‚ÜíEn, and Nl‚ÜíEn\ntranslations, the results are shown in Table 3.\nAccording to the above results, UniDrop suc-\ncessfully unites the FD, SD, and DD, and Ô¨Ånally im-\nproves the performance of Transformer on neural\nmachine translation tasks, without any additional\ncomputation costs and resource requirements.\n4.2 Text ClassiÔ¨Åcation\nWe also conduct experiments on text classiÔ¨Åcation\ntasks to further demonstrate the effectiveness of\nUniDrop for the Transformer models.\n4.2.1 Datasets\nWe evaluate different methods on the text classiÔ¨Åca-\ntion task based on 8 widely-studied datasets, which\ncan be divided into two groups. The Ô¨Årst group is\nfrom GLUE tasks (Wang et al., 2019a), and they\nare usually used to evaluate the performance of\nthe large-scale pre-trained language models after\nÔ¨Åne-tuning. The second group is some typical text\nclassiÔ¨Åcation datasets that are widely used in previ-\nous works (V oorhees and Tice, 1999; Maas et al.,\n2011; Zhang et al., 2015). The statistics of all\ndatasets are shown in Appendix A.2.\n4.2.2 Model\nWe employRoBERTaBASE (Liu et al., 2019) as the\nstrong baseline and Ô¨Åne-tune it on the text classiÔ¨Å-\ncation datasets. Different from BERTBASE (Devlin\net al., 2019), RoBERTaBASE is pre-trained with dy-\nnamic masking, full-sentences without NSP loss\nand a larger mini-batches. It has 12 blocks, and\nthe dimensions of embedding and FFN are 768 and\n3072, the number of attention heads is 12. When\nÔ¨Åne-tuning, we set the batch size to 32 and the max\nepoch to 30. Adam is applied to optimize the mod-\nels with a learning rate of 1e-5 and a warm-up step\nratio of 0.1. We employ the polynomial decay strat-\negy to adjust the learning rate. The default dropout\nand weight decay are both set to 0.1.\nWhen adding UniDrop to RoBERTaBASE,\nwe empirically set feature dropout rate and\nLayerDrop rate to 0.1. For data dropout, the se-\nquence keep rate pk and token dropout rate pare\nrespectively 0.5 and 0.1. The other settings are the\nsame as in the baselineRoBERTaBASE. We use the\nstandard accuracy to evaluate different methods on\ntext classiÔ¨Åcation tasks.\n3871\nMNLI QNLI SST-2 MRPC\nBiLSTM+Attn, CoVe 67.9 72.5 89.2 72.8\nBiLSTM+Attn, ELMo 72.4 75.2 91.5 71.1\nBERTBASE 84.4 88.4 92.9 86.7\nBERTLARGE 86.6 92.3 93.2 88.0\nRoBERTaBASE 87.1 92.7 94.7 89.0\n+UniDrop 87.8 93.2 95.5 90.4\nTable 4: Accuracy on GLUE tasks (dev set). The mod-\nels BiLSTM+Attn, CoVe and BiLSTM+Attn, ELMo\nare from Wang et al. (2019a). Best results are in bold.\nIMDB Yelp AG TREC\nChar-level CNN - 62.05 90.49 -\nVDCNN - 64.72 91.33 -\nDPCNN - 69.42 93.13 -\nULMFiT 95.40 - 94.99 96.40\nBERTBASE 94.60 69.94 94.75 97.20\nRoBERTaBASE 95.7 70.9 95.1 97.6\n+UniDrop 96.0 71.4 95.5 98.0\nTable 5: Accuracy on the typical text classiÔ¨Åcation\ndatasets. Char-level CNN and VDCNN are from Zhang\net al. (2015) and Conneau et al. (2017), DPCNN\nand ULMFiT are from Johnson and Zhang (2017)\nand Howard and Ruder (2018). Best results are in bold.\n4.2.3 Results\nTable 4 and Table 5 respectively show the accuracy\nof different models on GLUE tasks and typical text\nclassiÔ¨Åcation datasets.\nCompared with the conventional BiLSTM\nand CNN based models, we can observe the\npre-trained models, including ULMFiT, BERT,\nRoBERTa, achieve obvious improvements on most\ndatasets. BeneÔ¨Åting from better training strategy,\nRoBERTaBASE outperforms BERTBASE and even\nBERTLARGE on GLUE tasks.\nWe can see our proposed UniDrop further im-\nprove the performance RoBERTaBASE on both\nsmall-scale and large-scale datasets. SpeciÔ¨Åcally,\nUniDrop brings about 0.4 improvements of accu-\nracy on the typical text classiÔ¨Åcation datasets from\nTable 5. In contrast, RoBERTaBASE+UniDrop\nachieves more improvements on GLUE tasks. The\nexperimental results on the 8 text classiÔ¨Åcation\nbenchmark datasets consistently demonstrate the\nfacilitation of UniDrop for Transformer. We show\nmore results and ablation study on text classiÔ¨Åca-\ntion task in Appendix A.5.\n5 Analysis\nIn this section, we use IWSLT14 De‚ÜíEn transla-\ntion as the analysis task to investigate the capability\nof UniDrop to avoid overÔ¨Åtting, as well as the ef-\nfects of different dropout components and dropout\nFigure 3: The dev loss of different models on IWSLT14\nDe‚ÜíEn translation task.\nrates on UniDrop.\n5.1 OverÔ¨Åtting\nTo show the superiority of UniDrop to pre-\nvent Transformer from overÔ¨Åtting, we com-\npare the dev loss during training of Trans-\nformer, Transformer with each dropout technique,\nTransformer+UniDrop, and ablated models of\nTransformer+UniDrop. Figure 3 shows loss curves\nof different models.\nWe can observe that the standard Transformer\nis quickly overÔ¨Åtted during training, though it\nis equipped with a default dropout. In contrast,\nthe feature dropout, structure dropout, and data\ndropout, as well as the combinations of any two\ndropouts (i.e., ablated models), greatly reduce the\nrisk of overÔ¨Åtting to some extent. Among all com-\npared models, our Transformer+UniDrop achieves\nthe lowest dev loss and shows great advantage to\nprevent Transformer from overÔ¨Åtting. Besides, we\nalso Ô¨Ånd that the dev loss of Transformer+UniDrop\ncontinuously falls until the end of the training. We\nstop it to keep training epochs of all models same\nfor a fair comparison.\nIn Appendix A.4, we also plot the curves of train-\ning loss for the above models, together with the dev\nloss, to make a better understanding of the regular-\nization effects from these dropout techniques.\n5.2 Ablation Study\nIn Table 1, we have presented some important ab-\nlation studies by removing FD, SD, or DD from\nUniDrop. The consistent decline of BLEU scores\ndemonstrates their effectiveness. Besides, we fur-\nther investigate the effects of the two existing\nfeature dropouts FD-1, FD-2, two new feature\ndropouts FD-3, FD-4, and our proposed two-stage\n3872\nË°®Ê†º 1\n0 0.1 0.2 0.3 0.4\nFD dev 36.94 38.00 37.30 35.29\n35.68 36.88 36.33 34.71\nSD dev 37.91 38.00 37.72 36.70\n36.70 36.88 36.59 35.65\nDD dev 37.52 37.80 38.00 37.87 37.69\n36.38 36.74 36.88 36.75 36.55\nBLEU score\n34.5\n35.5\n36.5\n37.5\n38.5\n0 0.1 0.2 0.3\nDev\n Test\nBLEU score\n34.5\n35.5\n36.5\n37.5\n38.5\n0 0.1 0.2 0.3\nDev\n Test\nBLEU score\n34.5\n35.5\n36.5\n37.5\n38.5\n0 0.1 0.2 0.3 0.4\nDev\n Test\n(a). Varying FD rate. (b). Varying SD rate. (c). Varying DD rate.\n1\nFigure 4: The BLEU scores of Transformer+UniDrop on IWSLT14 De‚ÜíEn translation dev set and test test, with\nvarying the rates of FD, SD and DD respectively.\nDe‚ÜíEn En ‚ÜíDe Ro ‚ÜíEn\nTransformer 34.84 28.67 32.14\n+UniDrop 36.88 29.99 33.49\nw/o FD-1 36.72 29.84 33.33\nw/o FD-2 36.57 29.76 33.28\nw/o FD-3 36.59 29.83 33.31\nw/o FD-4 36.65 29.59 33.24\nw/o 2-stage DD 36.61 29.78 33.12\nTable 6: Ablation study of data dropout and different\nfeature dropouts on IWSLT14 De ‚ÜíEn, En ‚ÜíDe, and\nRo‚ÜíEn translation tasks.\ndata dropout strategy on Transformer models. The\nexperimental results are shown in Table 6.\nFrom Table 6, we can see the four ablation mod-\nels removing FDs underperform the full model\nTransformer+UniDrop, which means they can\nwork together to prevent Transformer from over-\nÔ¨Åtting. In multi-head attention module, FD-3\nbrings more BLUE improvement than FD-1. This\ncomparison shows the insufÔ¨Åciency of only apply-\ning FD-1 for the Transformer architecture. The\nTransformer+UniDrop w/o 2-stage DD means we\ndirectly apply conventional data dropout to the se-\nquence instead of our proposed 2-stage strategy.\nCompared with the full model, its performance also\ndecreases. This shows the necessity of keeping the\noriginal sequence for data dropout.\n5.3 Effects of Different Dropout Rates\nTo investigate the effects of FD, SD, and DD\ndropout rates on the UniDrop, we respectively\nvary them based on the setting (FD=0.1, SD=0.1,\nDD=0.2). When varying one dropout component,\nwe keep other dropout rates unchanged. Figure 4\nshows the corresponding results.\nWe can observe that the performance of each\ndropout for Transformer+UniDrop Ô¨Årst increases\nthen decreases when varying the dropout rates from\nsmall to large. Especially, varying the rate for FD\ndropout makes a more signiÔ¨Åcant impact on the\nmodel performance since FD contains four feature\ndropout positions. In contrast, the DD is least sen-\nsitive to the dropout rate change, but it still plays a\nrole in the model regularization.\n6 Related Work\n6.1 Dropout\nDropout is a popular regularization method for\nneural networks by randomly dropping some neu-\nrons during training (Srivastava et al., 2014). Fol-\nlowing the idea, there are abundant subsequent\nworks designing speciÔ¨Åc dropout for speciÔ¨Åc ar-\nchitecture, such as StochasticDepth (Huang et al.,\n2016), DropPath (Larsson et al., 2017), Drop-\nBlock (Ghiasi et al., 2018) for convolutional neural\nnetworks, Variational Dropout (Gal and Ghahra-\nmani, 2016), ZoneOut (Krueger et al., 2017), and\nWord Embedding Dropout (Gal and Ghahramani,\n2016) for recurrent neural networks. Recently,\nthe Transformer architecture achieves great suc-\ncess in a variety of tasks. To improve generaliza-\ntion of Transformer, some recent works propose\nLayerDrop (Fan et al., 2020a), DropHead (Zhou\net al., 2020) and HeadMask (Sun et al., 2020) as\nstructured regularizations, and obtain better perfor-\nmance than standard Transformer. Instead of de-\nsigning a speciÔ¨Åc dropout for Transformer, in this\nwork, we focus on integrating the existing dropouts\ninto one UniDrop to further improve generalization\nof Transformer without any additional cost.\n6.2 Data Augmentation\nData augmentation aims at creating realistic-\nlooking training data by applying a transforma-\ntion to a sample, without changing its label (Xie\net al., 2020). In NLP tasks, data augmentation often\nrefers to back-translation (Sennrich et al., 2016a),\nword replacing/inserting/swapping/dropout (Wei\n3873\nand Zou, 2019; Xie et al., 2020), etc. In this work,\nwe adopt simple but effective word dropout as data\nlevel dropout in our UniDrop. We, additionally,\ndesign a two-stage data dropout strategy.\n7 Conclusion\nIn this paper, we present an integrated dropout\napproach, UniDrop, to speciÔ¨Åcally regularize the\nTransformer architecture. The proposed UniDrop\nunites three different level dropout techniques from\nÔ¨Åne-grain to coarse-grain, feature dropout, struc-\nture dropout, and data dropout respectively. We\nprovide a theoretical justiÔ¨Åcation that the three\ndropouts play different roles in regularizing Trans-\nformer. Extensive results on neural machine trans-\nlation and text classiÔ¨Åcation datasets show that our\nTransformer+UniDrop outperforms the standard\nTransformer and various ablation versions. Further\nanalysis also validates the effectiveness of differ-\nent dropout components and our two-stage data\ndropout strategy. In conclusion, the UniDrop im-\nproves the performance and generalization of the\nTransformer without additional computational cost\nand resource requirement.\nAcknowledgments\nThe authors would like to thank the anonymous\nreviewers for their valuable comments. Xinyu Dai\nand Lijun Wu are the corresponding authors. This\nwork was partially supported by the NSFC (No.\n61976114,61936012) and National Key R&D Pro-\ngram of China (No. 2018YFB1005102).\nReferences\nKarim Ahmed, Nitish Shirish Keskar, and Richard\nSocher. 2017. Weighted transformer net-\nwork for machine translation. arXiv preprint\narXiv:1711.02132.\nSanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. 2019.\nTheoretical analysis of auto rate-tuning by batch\nnormalization. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo√Øc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2017, Copen-\nhagen, Denmark, September 9-11, 2017, pages 670‚Äì\n680.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na Ô¨Åxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978‚Äì2988.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and ≈Åukasz Kaiser. 2018. Univer-\nsal transformers. arXiv preprint arXiv:1807.03819.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186.\nAngela Fan, Edouard Grave, and Armand Joulin.\n2020a. Reducing transformer depth on demand with\nstructured dropout. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Ad-\ndis Ababa, Ethiopia, April 26-30, 2020.\nYang Fan, Shufang Xie, Yingce Xia, Lijun Wu,\nTao Qin, Xiang-Yang Li, and Tie-Yan Liu.\n2020b. Multi-branch attentive transformer. CoRR,\nabs/2006.10270.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Advances in Neural Informa-\ntion Processing Systems 29: Annual Conference on\nNeural Information Processing Systems 2016, De-\ncember 5-10, 2016, Barcelona, Spain , pages 1019‚Äì\n1027.\nFei Gao, Jinhua Zhu, Lijun Wu, Yingce Xia, Tao\nQin, Xueqi Cheng, Wengang Zhou, and Tie-Yan Liu.\n2019. Soft contextual data augmentation for neural\nmachine translation. In Proceedings of the 57th Con-\nference of the Association for Computational Lin-\nguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, Volume 1: Long Papers, pages 5539‚Äì5544.\nGolnaz Ghiasi, Tsung-Yi Lin, and Quoc V . Le. 2018.\nDropblock: A regularization method for convolu-\ntional networks. In Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neu-\nral Information Processing Systems 2018, NeurIPS\n2018, 3-8 December 2018, Montr√©al, Canada, pages\n10750‚Äì10760.\nHelia Hashemi, Hamed Zamani, and W Bruce Croft.\n2020. Guided transformer: Leveraging multiple ex-\nternal sources for representation learning in conver-\nsational search. In Proceedings of the 43rd Inter-\nnational ACM SIGIR Conference on Research and\n3874\nDevelopment in Information Retrieval, pages 1131‚Äì\n1140.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770‚Äì\n778.\nJudy Hoffman, Daniel A Roberts, and Sho Yaida. 2019.\nRobust learning with jacobian regularization. arXiv\npreprint arXiv:1908.02729.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model Ô¨Åne-tuning for text classiÔ¨Åcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 1:\nLong Papers, pages 328‚Äì339.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and\nKilian Q. Weinberger. 2016. Deep networks with\nstochastic depth. In Computer Vision - ECCV 2016 -\n14th European Conference, Amsterdam, The Nether-\nlands, October 11-14, 2016, Proceedings, Part IV ,\nvolume 9908 of Lecture Notes in Computer Science,\npages 646‚Äì661. Springer.\nMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,\nand Hal Daum√© III. 2015. Deep unordered compo-\nsition rivals syntactic methods for text classiÔ¨Åcation.\nIn Proceedings of the 53rd annual meeting of the as-\nsociation for computational linguistics and the 7th\ninternational joint conference on natural language\nprocessing (volume 1: Long papers) , pages 1681‚Äì\n1691.\nDaniel Jakubovitz and Raja Giryes. 2018. Improv-\ning dnn robustness to adversarial attacks using jaco-\nbian regularization. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 514‚Äì\n529.\nRie Johnson and Tong Zhang. 2017. Deep pyramid\nconvolutional neural networks for text categoriza-\ntion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2017, Vancouver, Canada, July 30 - August 4, Vol-\nume 1: Long Papers, pages 562‚Äì570.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efÔ¨Åcient transformer. In Inter-\nnational Conference on Learning Representations.\nAnders Krogh and John A Hertz. 1992. A simple\nweight decay can improve generalization. In Ad-\nvances in neural information processing systems ,\npages 950‚Äì957.\nDavid Krueger, Tegan Maharaj, J√°nos Kram√°r, Moham-\nmad Pezeshki, Nicolas Ballas, Nan Rosemary Ke,\nAnirudh Goyal, Yoshua Bengio, Aaron C. Courville,\nand Christopher J. Pal. 2017. Zoneout: Regulariz-\ning rnns by randomly preserving hidden activations.\nIn 5th International Conference on Learning Rep-\nresentations, ICLR 2017, Toulon, France, April 24-\n26, 2017, Conference Track Proceedings . OpenRe-\nview.net.\nGustav Larsson, Michael Maire, and Gregory\nShakhnarovich. 2017. Fractalnet: Ultra-deep\nneural networks without residuals. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nYiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin\nDong, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2019.\nUnderstanding and improving transformer from a\nmulti-particle dynamic system point of view. arXiv\npreprint arXiv:1906.02762.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn The 49th Annual Meeting of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Proceedings of the Conference, 19-24 June,\n2011, Portland, Oregon, USA, pages 142‚Äì150. The\nAssociation for Computer Linguistics.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, 8-14 Decem-\nber 2019, Vancouver, BC, Canada , pages 14014‚Äì\n14024.\nVinod Nair and Geoffrey E Hinton. 2010. RectiÔ¨Åed\nlinear units improve restricted boltzmann machines.\nIn ICML.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA, pages 311‚Äì318. ACL.\nGabriel Pereyra, George Tucker, Jan Chorowski,\nLukasz Kaiser, and Geoffrey E. Hinton. 2017. Regu-\nlarizing neural networks by penalizing conÔ¨Ådent out-\nput distributions. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Workshop Track Proceed-\nings. OpenReview.net.\n3875\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86‚Äì96.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2016, August 7-12, 2016, Berlin,\nGermany, Volume 1: Long Papers. The Association\nfor Computer Linguistics.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overÔ¨Åtting. The journal of machine learning\nresearch, 15(1):1929‚Äì1958.\nZewei Sun, Shujian Huang, Xinyu Dai, and Jiajun\nChen. 2020. Alleviating the inequality of atten-\ntion heads for neural machine translation. CoRR,\nabs/2009.09672.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104‚Äì3112.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998‚Äì6008.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Conference of the Association for Computa-\ntional Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers , pages\n5797‚Äì5808. Association for Computational Linguis-\ntics.\nEllen M. V oorhees and Dawn M. Tice. 1999. The\nTREC-8 question answering track evaluation. In\nProceedings of The Eighth Text REtrieval Confer-\nence, TREC 1999, Gaithersburg, Maryland, USA,\nNovember 17-19, 1999 , volume 500-246 of NIST\nSpecial Publication. National Institute of Standards\nand Technology (NIST).\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019a.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nDilin Wang, ChengYue Gong, and Qiang Liu. 2019b.\nImproving neural language modeling via adversarial\ntraining. In Proceedings of the 36th International\nConference on Machine Learning, ICML 2019, 9-\n15 June 2019, Long Beach, California, USA , vol-\nume 97 of Proceedings of Machine Learning Re-\nsearch, pages 6555‚Äì6565. PMLR.\nColin Wei, Sham Kakade, and Tengyu Ma. 2020.\nThe implicit and explicit regularization effects of\ndropout. arXiv preprint arXiv:2002.12915.\nJason W. Wei and Kai Zou. 2019. EDA: easy data\naugmentation techniques for boosting performance\non text classiÔ¨Åcation tasks. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019, pages 6381‚Äì6387.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2019. Pay less attention with\nlightweight and dynamic convolutions. In Interna-\ntional Conference on Learning Representations.\nLijun Wu, Shufang Xie, Yingce Xia, Yang Fan, Tao\nQin, Jianhuang Lai, and Tie-Yan Liu. 2020. Se-\nquence generation with mixed representations. In\nProceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event.\nYingce Xia, Tianyu He, Xu Tan, Fei Tian, Di He, and\nTao Qin. 2019. Tied transformers: Neural machine\ntranslation with shared encoder and decoder. In Pro-\nceedings of the AAAI Conference on ArtiÔ¨Åcial Intel-\nligence, volume 33, pages 5466‚Äì5473.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,\nand Quoc Le. 2020. Unsupervised data augmenta-\ntion for consistency training. Advances in Neural\nInformation Processing Systems, 33.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiÔ¨Åcation. In Advances in Neural Information Pro-\ncessing Systems 28: Annual Conference on Neural\nInformation Processing Systems 2015 , pages 649‚Äì\n657.\nWangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou,\nand Ke Xu. 2020. Scheduled drophead: A regular-\nization method for transformer models. In Findings,\nEMNLP 2020, Online Event, 16-20 November 2020,\npages 1971‚Äì1980.\nJinhua Zhu, Lijun Wu, Yingce Xia, Shufang Xie, Tao\nQin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu.\n2021. {IOT}: Instance-wise layer reordering for\ntransformer structures. In International Conference\non Learning Representations.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tie-Yan Liu.\n2020. Incorporating BERT into neural machine\ntranslation. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\n3876\nA Appendix\nA.1 Supplementary materials for theoretical\nanalysis\nIn this section, we explain why regularizing Hes-\nsian matrix with respect to input or hidden output\ncan improve model robustness and generalization.\nWe use DŒ±\nfLto denote the Œ±-order derivatives\nof loss Lwith respect to f. If the hidden output is\nperturbed by œµ, i.e., Àúh= h+ œµ, the k-th output Fk\nshifts to\nFk(h+ œµ) =Fk(h) +œµTJFk,h\n+ 1\n2œµT(D2\nhFk(h))œµ+ o(œµ2), (5)\nwhere JFk,h(x) is the Jacobian between hidden\noutput hand Ô¨Ånal output Fk.\nStructure dropout regularizes all elements in\nHessian matrix D2\nhL. For Hessian matrix of loss\nfunction, we have D2\nhL = JT\nF,h(D2\nFL)JF,h +‚àë\nk(DFkL)(D2\nhFk(h)). Thus, regularizing all el-\nements in D2\nhLmeans regularizing both JF,h and\nD2\nhFk(h). As shown in Eq.5, regularizing this two\nterms can make |Fk(h+œµ)‚àíFk(h)|smaller. There-\nfore, the robustness of the model is improved and\nthe generalization ability of the model can also be\nimproved (Hoffman et al., 2019; Jakubovitz and\nGiryes, 2018).\nFeature dropout regularizes diagonal element\nof D2\nhL. Using the approximation D2\nhL ‚âà\nJT\nF,h(D2\nFL)JF,h(Wei et al., 2020), regularizing di-\nagonal elements D2\nhLequals to regularizing norm\nof Jacobian, i.e., ||JF,h||2 if D2\nFLis roughly a di-\nagonal matrix. For cross-entropy loss, D2\nFL=\ndiag(z) ‚àízzT, where z is the probability vector\npredicted by the model encoding the distribution\nover output class labels, the matrix D2\nFLcan be\napproximated by a diagonal matrix. Thus, feature\ndropout mainly regularizes the Ô¨Årst-order coefÔ¨Å-\ncient JFk,h in Taylor expansion in Eq.5, which is\ndifferent from structure dropout. Since Jacobian\nis an essential quantity for the generalization (Wei\net al., 2020; Hoffman et al., 2019), emphasising\nthis term is necessary for generalization although\nstructure dropout can also regularize it.\nSimilar analysis can be applied to data dropout\nand we only need to replace hidden output hto the\ninput x.\nA.2 Statistics of Datasets\nTable 7 and Table 8 respectively show the statistics\nof machine translation and text classiÔ¨Åcation bench-\nDatasets Train Dev Test\nEn‚ÜîDe 160k 7k 7k\nEn‚ÜîRo 180k 4.7k 1.1k\nEn‚ÜîNl 170k 4.5k 1.1k\nEn‚ÜîPt-br 175k 4.5k 1.2k\nTable 7: Statistics for machine translation datasets.\nDatasets Classes Train Dev\nMNLI 3 393k 20k\nQNLI 2 105k 5.5k\nSST-2 2 67k 0.9k\nMRPC 2 3.7k 0.4k\nDatasets Classes Train Test\nIMDB 2 25k 25k\nYelp 5 650k 50k\nAG‚Äôs News 4 120k 76k\nTREC 6 5.4k 0.5k\nTable 8: Statistics for text classiÔ¨Åcation datasets.\nmark datasets we used to evaluate the UniDrop for\nTransformer.\nFor machine translation tasks, the four language\npairs all contain around 170k‚àº190k training pairs.\nText classiÔ¨Åcation experiments are conducted in\nGLUE tasks (Wang et al., 2019a) and typical text\nclassiÔ¨Åcation benchmarks datasets (V oorhees and\nTice, 1999; Maas et al., 2011; Zhang et al., 2015).\nFor GLUE tasks, we adopt the four datasets MNLI,\nQNLI, SST-2 and MRPC. They are used to evaluate\nthe ability of models on language inference, sen-\ntiment classiÔ¨Åcation and paraphrase detection. In\ntypical text classiÔ¨Åcation datasets, IMDB is binary\nÔ¨Ålm review classiÔ¨Åcation task (Maas et al., 2011).\nYelp and AG‚Äôs News datasets are built by (Zhang\net al., 2015), respectively for sentiment classiÔ¨Å-\ncation and topic classiÔ¨Åcation. TREC is a ques-\ntion classiÔ¨Åcation dataset consisting of 6 question\ntypes (V oorhees and Tice, 1999).\nA.3 Dropout Attempts\nBesides the different dropout methods introduced\nin Section 3, we also tried some other dropouts.\nWe Ô¨Årst introduce their settings. The ‚ÄòQKV_proj‚Äô\napplies dropout to query, key, and value after lin-\near projection. In contrast, FD-3 is to add dropout\nto query, key, and value before projection. Sim-\nilarly, ‚ÄòLogitsDrop‚Äô means that we use dropout\nafter obtaining output logits from output projec-\ntion layer. Compared to LogitsDrop, FD-4 directly\napplies dropout before the output projection layer.\n3877\nBLEU\nTransformer 34.84\n+FD-1, FD-2 35.46\n+FD-1, FD-2, FD-3 36.10\n+FD-1, FD-2, QKV_proj 35.75\n+FD-1, FD-2, FD-4 36.15\n+FD-1, FD-2, LogitsDrop 36.00\n+FD-1, FD-2, FD-3, LogitsDrop 36.06\n+FD-1, FD-2, FD-3, FD-4 36.48\n+FD-1, FD-2, Encoder LayerDrop 35.24\n+FD-1, FD-2, Decoder LayerDrop 35.99\n+FD-1, FD-2, Encoder&Decoder LayerDrop 35.74\n+FD-1, FD-2, EncoderDrop 35.64\n+FD-1, FD-2, DD 36.09\n+FD-1, FD-2, FD-3, FD-4, Decoder LayerDrop 36.61\n+UniDrop 36.88\nTable 9: The results of different dropouts on IWSLT14 De‚ÜíEn translation task.\n‚ÄòEncoderDrop‚Äô means that we randomly drop the\nwhole information of Transformer encoder with a\nprobability and only use previous outputs to gener-\nate the next token during training. Obviously, it is a\nlanguage modeling task when dropping the encoder.\n‚ÄòEncoder LayerDrop‚Äô is that we applyLayerDrop\nonly on the Transformer encoder. Table 9 shows\nthe BLEU scores of different models on IWSLT-\n2014 De‚ÜíEn translation task. All dropout rates\nare tuned within [0.1,0.2,0.3,0.4] according to the\nperformance of the dev set.\nFD-1 and FD-2 are two existing feature dropouts\nfor Transformer. We Ô¨Årst use them and achieve\nbetter BLUE scores than the standard Transformer,\nwhich demonstrates the existence of serious over-\nÔ¨Åtting in Transformer model. On this basis, we\ntry to add further feature dropout to prevent Trans-\nformer from overÔ¨Åtting. However, we can see that\nQKK_proj achieves fewer improvements compared\nwith FD-3. Similarly, LogitsDrop also underper-\nforms FD-4. Therefore, we Ô¨Ånally use FD-3 and\nFD-4 as our feature dropout components together\nwith FD-1 and FD-2.\nAmong all structure dropout models, decoder\nLayerDrop outperforms all compared methods. In\ncontrast, EncoderDrop only brings small improve-\nments. Surprisingly, we can see that here the en-\ncoder LayerDrop actually has a negative effect on\nTransformer. Thus we integrate the promising de-\ncoder LayerDrop as structured dropout component\ninto UniDrop.\nMNLI QNLI SST-2 MRPC\nRoBERTaBASE 87.1 92.7 94.7 89.0\n+UniDrop 87.8 93.2 95.5 90.4\nw/o FD 87.3 92.9 94.8 90.1\nw/o SD 87.5 93.1 95.1 89.5\nw/o DD 87.7 93.1 95.0 89.5\nRoBERTaLAEGE 89.8 94.3 96.3 90.4\n+UniDrop 90.2 94.8 96.6 91.4\nw/o FD 89.9 94.6 96.2 90.4\nw/o SD 90.0 94.6 96.3 90.7\nw/o DD 90.2 94.7 95.2 90.7\nTable 10: Ablation Study on GLUE tasks (dev set).\nThe ‚Äúw/o FD‚Äù, ‚Äúw/o SD‚Äù, ‚Äúw/o DD‚Äù indicate re-\nspectively removing feature dropout, structure dropout,\nand data dropout from RoBERTa BASE+UniDrop or\nRoBERTaLARGE+UniDrop.\nA.4 Loss Curves\nFigure 5 shows the loss curves of different mod-\nels during training. Overall, we can see that our\nTransfomer+UniDrop obtains the minimal gap of\ntraining loss and dev loss compared with other\ndropout models and the standard Transformer. This\nobservation shows the better capability ofUniDrop\nto prevent Transformer from overÔ¨Åtting. Bene-\nÔ¨Åtting from the advantage, Transfomer+UniDrop\nachieves the best generalization and dev loss on\nIWSLT14 De‚ÜíEn translation task.\nA.5 Ablation Study on Text ClassiÔ¨Åcation\nTable 10 show the accuracy of standard\nRoBERTaBASE and RoBERTaLARGE, the\nmodels with UniDrop and corresponding ablated\nmodels on GLUE tasks. Compared the base\n3878\nFigure 5: The training and dev loss of different models on IWSLT14 De‚ÜíEn translation task.\nmodels RoBERTaBASE and RoBERTaLARGE, we\ncan observe that UniDrop further improves their\nperformance on text classiÔ¨Åcation tasks. After\nremoving FD, SD, or DD from UniDrop, the\ncorresponding accuracy has decreased more or\nless. The consistent declines again demonstrate the\nnecessity of the feature dropout, structure dropout\nand data dropout for UniDrop.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6032155752182007
    },
    {
      "name": "Computer science",
      "score": 0.5343626737594604
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.4422786831855774
    },
    {
      "name": "Engineering",
      "score": 0.2780260741710663
    },
    {
      "name": "Philosophy",
      "score": 0.23674896359443665
    },
    {
      "name": "Electrical engineering",
      "score": 0.17158815264701843
    },
    {
      "name": "Epistemology",
      "score": 0.07339441776275635
    },
    {
      "name": "Voltage",
      "score": 0.06852132081985474
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I881766915",
      "name": "Nanjing University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ],
  "cited_by": 22
}