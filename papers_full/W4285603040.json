{
  "title": "Transformer-based Objective-reinforced Generative Adversarial Network to Generate Desired Molecules",
  "url": "https://openalex.org/W4285603040",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2095600869",
      "name": "Chen Li",
      "affiliations": [
        "Kyushu Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3139625098",
      "name": "Chikashige Yamanaka",
      "affiliations": [
        "Kyushu Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2947044828",
      "name": "Kazuma Kaitoh",
      "affiliations": [
        "Kyushu Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2029139975",
      "name": "Yoshihiro Yamanishi",
      "affiliations": [
        "Kyushu Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2080635178",
    "https://openalex.org/W3109493217",
    "https://openalex.org/W4297951436",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2114704115",
    "https://openalex.org/W2994860160",
    "https://openalex.org/W2786722833",
    "https://openalex.org/W2618625858",
    "https://openalex.org/W1983478747",
    "https://openalex.org/W4230007416",
    "https://openalex.org/W2925830236",
    "https://openalex.org/W2155027007",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W2610148085",
    "https://openalex.org/W2604296437",
    "https://openalex.org/W3014154196",
    "https://openalex.org/W2963676163",
    "https://openalex.org/W1975147762"
  ],
  "abstract": "Deep generative models of sequence-structure data have attracted widespread attention in drug discovery. However, such models cannot fully extract the semantic features of molecules from sequential representations. Moreover, mode collapse reduces the diversity of the generated molecules. This paper proposes a transformer-based objective-reinforced generative adversarial network (TransORGAN) to generate molecules. TransORGAN leverages a transformer architecture as a generator and uses a stochastic policy gradient for reinforcement learning to generate plausible molecules with rich semantic features. The discriminator grants rewards that guide the policy update of the generator, while an objective-reinforced penalty encourages the generation of diverse molecules. Experiments were performed using the ZINC chemical dataset, and the results demonstrated the usefulness of TransORGAN in terms of uniqueness, novelty, and diversity of the generated molecules.",
  "full_text": "Transformer-Based Objective-Reinforced Generative Adversarial Network to\nGenerate Desired Molecules\nChen Li\u0003 , Chikashige Yamanaka, Kazuma Kaitoh and Yoshihiro Yamanishi\nDepartment of Bioscience and Bioinformatics, Faculty of Computer Science and Systems Engineering,\nKyushu Institute of Technology, Iizuka, Japan\nli260@bio.kyutech.ac.jp, yamanaka.chikashige215@mail.kyutech.jp, {kaito168,\nyamani}@bio.kyutech.ac.jp\nAbstract\nDeep generative models of sequence-structure data\nhave attracted widespread attention in drug dis-\ncovery. However, such models cannot fully ex-\ntract the semantic features of molecules from se-\nquential representations. Moreover, mode collapse\nreduces the diversity of the generated molecules.\nThis paper proposes a transformer-based objective-\nreinforced generative adversarial network (Tran-\nsORGAN) to generate molecules. TransORGAN\nleverages a transformer architecture as a generator\nand uses a stochastic policy gradient for reinforce-\nment learning to generate plausible molecules with\nrich semantic features. The discriminator grants re-\nwards that guide the policy update of the genera-\ntor, while an objective-reinforced penalty encour-\nages the generation of diverse molecules. Exper-\niments were performed using the ZINC chemical\ndataset, and the results demonstrated the usefulness\nof TransORGAN in terms of uniqueness, novelty,\nand diversity of the generated molecules.\n1 Introduction\nThe goal of drug discovery is to produce new compounds for\ntreating diseases. However, the path to drug approval has be-\ncome increasingly complicated and expensive. Over the past\ndecade, the development cost of a new prescription medicine\nthat gains market approval has risen by 145% to 2.6 billion\nUSD, and the average development time is 10 years. To ac-\ncelerate this process, medical scientists have begun using ar-\ntiﬁcial intelligence (AI) and deep learning for drug discovery\n[Olivecrona et al., 2017 ]. Some pharmaceutical companies\nare now using AI and deep learning to perform tasks that once\ndepended on human intelligence. By using advanced tech-\nniques, medical researchers at the forefront of drug develop-\nment can gain timely and actionable insights from stacks of\nunstructured data.\nVarious deep generative models for de novomolecular\ngeneration have been explored recently, including variational\n\u0003Corresponding author\n1Supplementary materials accompany this paper at http://labo.\nbio.kyutech.ac.jp/\u0018yamani/ijcai2022/.\nauto-encoders (V AEs)[Kusner et al., 2017; Dai et al., 2018;\nJin et al., 2018] and generative adversarial networks (GANs)\n[Guimaraes et al., 2017; De Cao and Kipf, 2018 ]. A GAN\n[Goodfellow et al., 2014 ] is an unsupervised deep learning\napproach for generative modeling and has two main compo-\nnents: a generator and discriminator. The generator attempts\nto generate realistic fake data, and the discriminator aims to\ndistinguish synthetic data from the original data. For molecu-\nlar generation, the simpliﬁed molecular-input line-entry sys-\ntem (SMILES) is commonly used to represent molecules\nas string-based sequences derived from molecular graphs\n[Weininger, 1988 ]. For example, the molecular structure\n“Ic1cnccn1” begins with the iodine atom “I” followed by\nthe ring structure “c1cnccn1.” Therefore, using a GAN with\nSMILES to generate new molecules seems to be a reason-\nable approach. Unfortunately, there are two problems with\nsequence generation by a GAN. First, a GAN reliably gener-\nates real-valued continuous data, but it is unsuitable for indi-\nrect sequence generation of discrete tokens such as SMILES\nstrings. Second, a GAN can only give the score/loss of an en-\ntire string. Balancing the current score of a partially generated\nsubsequence with the future score of the complete sequence\nis a nontrivial task.\nSeqGAN [Yu et al., 2017 ] and objective-reinforced GAN\n(ORGAN) [Guimaraes et al., 2017 ] were developed to ad-\ndress the above problems. SeqGAN uses a reinforcement\nlearning (RL) approach [Sutton et al., 2000 ] to facilitate\nthe generation of discrete data. ORGAN is a SeqGAN-\nbased model that accounts for chemical properties such as\nthe solubility and drug-likeness of generated molecules. Both\nSeqGAN and ORGAN employ recurrent neural networks\n(RNNs), especially long short-term memory (LSTM), as the\ngenerator. However, RNNs have certain limitations when\ngenerating molecules with SMILES representations. First,\nRNNs have difﬁculty with designing a molecule with com-\nplex rings [Ar´us-Pous et al., 2019]. In general, highly cyclic\nmolecules have long sequence representations and a stricter\nsyntax than acyclic molecules. Slight changes in syntax may\nresult in the generation of invalid molecules or molecules\nwith very different chemical properties from those intended.\nSecond, RNNs cannot work on GPU versions because the\ncurrent iteration must be computed after the previous time\nstep, which is not conducive to exploring the near-inﬁnite\nchemical spaces of big data.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3884\nA transformer is a self-attention-based neural network ar-\nchitecture that provides state-of-the-art performance for neu-\nral machine translation tasks [Vaswani et al., 2017]. A trans-\nformer can effectively solve the shortcomings of traditional\nRNNs in iterative calculations and uses the self-attention\nmechanism to capture useful syntax information. However,\ntransformer-based GANs face several problems with molec-\nular generation. First, because the input and output are the\nsame, a transformer is likely to remember only that the to-\nkens at the corresponding positions are the same, which re-\nsults in insufﬁcient training. In addition, simply applying RL\napproaches, such as a policy gradient to train the transformer\nmay lead to instability.\nTo overcome these drawbacks, we propose a transformer-\nbased ORGAN (TransORGAN) model. The generator is a\ntransformer that outputs likely molecules as SMILES strings.\nIn addition, the same molecule can have various SMILES rep-\nresentations, called variant SMILES, which are used by the\nmodel to sufﬁciently learn the syntactic and semantic rules\nof highly cyclic molecules. The discriminator evaluates the\ngenerated data and feedbacks the rewards that guide the gen-\nerator according to the RL method. The main contributions\nare summarized below:\n• A transformer is employed as the generator. A trans-\nformer architecture can be parallelized, and the self-\nattention mechanism can capture the rich semantic in-\nformation of molecules represented by SMILES strings.\n• Variant SMILES allows the transformer to sufﬁciently\nlearn the syntax rules with different strings and increase\nthe diversity of generated molecules.\n• The teacher-forcing improves the stability of the policy\ngradient when it is guiding the generation of molecules\nwith the desired chemical properties during training.\n2 Related Works\nDeep generative models have attracted widespread attention,\nbecause they can learn coherent latent representations of con-\ntinuous data such as image and video data. However, gen-\nerating molecular structures and other discrete data remains\na challenging task. Molecules generated from discrete data\nare often invalid. Two kinds of V AEs have been proposed\nfor generating molecules: Character-V AE and Grammar-V AE\n[Kusner et al., 2017]. Character-V AE can generate molecules\nfrom characters without constraints, while Grammar-V AE\napplies a parse tree to constrain the grammar of SMILES.\nGrammar-V AE can generate syntactically valid molecules,\nbut it cannot generate semantics. A syntax-direct V AE (SD-\nV AE) model was proposed to generate syntactic and semantic\nvalid molecules [Dai et al., 2018]. A junction tree-based V AE\n(JT-V AE) can generate molecular graphs by using a graph\nmessage-passing network [Jin et al., 2018 ]. A V AE and its\nvariants attempt to extract features of the encoder into a ﬁxed-\nsize latent space, which limits the ability of decoders to de-\ncode the latent features. Graph-AF is a ﬂow-based autore-\ngressive neural network, that can generate molecules from\nmolecular graphs [Shi et al., 2019]. Although graphical rep-\nresentations of molecules contain rich semantic and syntactic\ninformation, graph-based models are generally more compli-\ncated than SMILES strings.\nA GAN alternately conducts adversarial training to im-\nprove the generator’s ability to generate data and the dis-\ncriminator’s ability to distinguish between synthetic and real\ndata. Although GANs are not limited to a ﬁxed-size latent\nspace, they have difﬁculties with generating discrete data se-\nquences. SeqGAN considers the sequence generation pro-\ncedure as a sequential decision-making process [Yu et al.,\n2017]. The generator is an agent of RL, the state comprises\nthe tokens generated thus far, and the action is the next token.\nThe discriminator evaluates the sequence and provides feed-\nback to guide the learning of the generator. However, GANs\ncannot back propagate gradients to the generator when the\noutput is discrete. In SeqGAN, the generator is a stochas-\ntic parametrized policy. The policy gradients approximate\nstate-action values using a Monte Carlo search process. To\nconsider desired chemical properties, ORGAN extended the\ntraining process of SeqGAN to include not only the discrimi-\nnator reward but also domain-speciﬁc objectives [Guimaraes\net al., 2017 ]. Speciﬁcally, ORGAN weights the molecular\nobjectives by a factor \u0015. When \u0015 = 0, ORGAN becomes a\nNa¨ıve RL that aims only to generate valid molecules. Alter-\nnatively, when \u0015 = 1, ORGAN becomes a simple SeqGAN\nthat does not consider the molecular properties. MolGAN\nis an implicit and likelihood-free graph-structure-based GAN\nfor generating small molecular structures [De Cao and Kipf,\n2018]. However, although MolGAN improved the validity of\nthe generated molecules, mode collapse reduced the unique-\nness of the generated molecules to less than 3.2%.\nIn most of the above works, the GAN generator was an\nRNN, which cannot sufﬁciently extract the latent features\nfrom a long sequence with rich semantic information such\nas SMILES strings. A transformer [Vaswani et al., 2017 ]\ncan capture semantic features in sentences by using a self-\nattention mechanism alone without needing an RNN or con-\nvolution. Molecular generation can be regarded as a spe-\ncial type of natural language processing that treats SMILES\nstrings as sentences. In this case, a transformer can be ap-\nplied to generate molecules. A transformer-based neural net-\nwork has been proposed that uses SMILES data to generate\nmolecules with a desirable balance of properties [He et al.,\n2021]. A novel architecture based on a transformer and spa-\ntial graph convolutional networks (GNNs) has also been pro-\nposed that enhances the attention mechanism of a transformer\nbased on the interatomic distances and graph structures of\nmolecules [Danel et al., 2020 ]. However, few works have\ncombined a transformer with a GAN to improve the qual-\nity of generation. In addition, mode collapse during the ad-\nversarial training process is a common cause of low unique-\nness [De Cao and Kipf, 2018]. Our proposed TransORGAN,\nwhich enhances the semantic feature extraction for molec-\nular generation and imposes additional objective-reinforced\npenalties with RL to alleviate mode collapse.\n3 TransORGAN\nFigure 1 shows the architecture of TransORGAN, which con-\nsists of two main components: a generator and discrimina-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3885\nFigure 1: Architecture of the TransORGAN.\ntor. The generator and discriminator act as two players in a\nminimaxgame. The generator tries to generate high-quality\nmolecules represented as SMILES strings under the guidance\nof the discriminator through RL. Meanwhile, the discrimi-\nnator tries to distinguish the generated fake data from real\nmolecules and to avoid being fooled by the generator. Note\nthat the generator and discriminator are trained in alternation.\nFormally, let G\u0012 and D\u001e represent the generator and dis-\ncriminator, respectively, where \u0012 and \u001eare parameters. The\nminimaxgame can then be expressed as follows:\nmin\n\u0012\nmax\n\u001e\nV(G\u0012;D\u001e) =Ex∼pdata(x)[log D\u001e(x)]\n+Ez∼pz(z)[log(1 −D\u001e(G\u0012(z)))]:\n(1)\n3.1 Variant SMILES\nIn general, the input of a transformer differs from the out-\nput, which facilitates model learning. However, TransOR-\nGAN uses only SMILES data, which are not paired. If the\nsame SMILES string is used as the input and output, the gen-\nerator of TransORGAN can only learn a one-to-one corre-\nspondence between the input and output, so the training is\ninsufﬁcient. Insufﬁcient learning by the generator may lead\nto mode collapse. We considered assigning different atomic\norders as training labels to each SMILES string. Fortu-\nnately, a molecule can be represented by various strings[Bjer-\nrum, 2017]. For example, “CC(=O)Oc1ccccc1C(=O)O” and\n“c1cc(c(cc1)C(O)=O)OC(C)=O” are different strings repre-\nsenting the same molecule. Figure A.1 and Algorithm A.1 in\nthe Appendix describe how to produce variants for SMILES.\n3.2 Generator Training Process\nA transformer can be represented by an encoder-decoder\nparadigm and trained in an end-to-end fashion. An encoder\nuses self-attention to extract features of the input sequence in\nparallel. These features are used as the input of the decoder.\nIn the decoder, sequence masking is used in the masked multi-\nhead attention layer to avoidexposure to future information.\nFor each token within a SMILES string, the transformer com-\nputes the attention weight as follows:\nAttention(Q;K)V = softmax(QKT\n√\ndk\n)V; (2)\nwhere Q, K, and V are the queries, keys, and values, and\nhave the corresponding dimensions dk, dk, and dv. As the\nmulti-head attention projects the queries, keys, and values h\ntimes, the transformer jointly attends to features from differ-\nent strings:\nMultiHead(Q;K;V ) = [Head1;:::;Head h]W; (3)\nHeadi = Attention(QWQ;KWK;VW V); (4)\nwhere W, WQ, WK, and WV are the weight matrices.\n3.3 Discriminator Training Process\nLet {x1;x2;:::; xT}and {y1;y2;:::; yT}represent a real\nmolecule and generated sequence, respectively, of the ﬁxed\nlength T, where xt;yt ∈Rk are k-dimensional token em-\nbedding vectors. The source matrix X1:T and target matrix\nY1:T are then expressed as\nX1:T = x1 ⊕:::⊕xT; Y1:T = y1 ⊕:::⊕yT; (5)\nwhere ⊕ is the concatenation operator and X1:T;Y1:T ∈\nRT×k. By applying a convolutional operation with a window\nsize of ttokens, we then obtain a new feature map:\ncij = ReLu(!j ⊙Xi:i+t−1 + b); (6)\nwhere !j is the j-th kernel and !j ∈Rt×k, the ⊙operator\nrepresents element-wise summation of the production, and b\nis the bias of the convolutional layer. Note that kernels with\nvarious window sizes extract various latent features. Finally,\nthe features are selected by the max-pooling technique:\necj = max{cj1;:::;cjT−t+1 }: (7)\nAfter the max-pooling operation, the performance is en-\nhanced by highway neural networks. Finally, the fully con-\nnected layer outputs the probability that the input matrix of\nthe CNNs is real.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3886\n3.4 Policy Gradient Training Process\nThe sampling process of molecules represented by SMILES\nstrings is not differentiable. To solve this problem, we\nadopted an RL approach. Speciﬁcally, we can deﬁne\nG\u0012(Y1:T|X1:T) as the stochastic policy of the agent and let\ns0 and a represent the initial state and action, respectively.\nUnder the guidance of the policy gradient, the generator gen-\nerates sequences by maximizing the expected reward score:\nJ(\u0012) =\nX\nY1:T\nG\u0012(Y1:T|X1:T) ·R((X1:T;Y1:T−1);yT); (8)\nwhere the action-value function R(s0;a) denotes the ex-\npected reward of starting from state (X1:T;Y1:T−1) and tak-\ning the action yT. The action-value function is calculated\nin three main parts: the discriminator probability of the cur-\nrent input sequence, which is determined from the original\nSMILES dataset; the value of the desired chemical property\nof the current input sequence O(Y1:T); and the penalty func-\ntion for a generated sequence Y1:T with low diversity. Note\nthat both O(Y1:T) and P(Y1:T) are valued between 0 and 1\ninclusive. O(Y1:T) refers to objective scores of the generated\nSMILES strings, such as the quantitative estimate of drug-\nlikeness (QED) scores. P(Y1:T) refers to the penalty for gen-\nerating non-unique SMILES strings and is calculated by\nP(Y1:T) = # of unique SMILES\n# of SMILES×# of repeated Y1:T\n: (9)\nWhen the score is zero, the generated sequence represents\nan invalid molecule or already exists. Otherwise, the se-\nquence describes a valid molecule with the desired chemical\nproperty. The reward function is then given by\nR((X1:T;Y1:T−1);yT) =\u0015·D\u001e(Y1:T)\n+(1 −\u0015) ·O(Y1:T) ·P(Y1:T); (10)\nwhere \u0015 is a hyperparameter that adjusts the learning ratio\nof Na¨ıve RL and SeqGAN. However, the above function can\nonly provide a reward score of the entire string. When the\ngenerated string Y1:T is incomplete, the values of D\u001e(Y1:T),\nO(Y1:T), and P(Y1:T) are nonsensical. The action-value\nfunction of the subsequence generated at each intermediate\ntime step is calculated by using a Monte Carlo (MC) search\nunder the policy G\u0012. The search is continued until the se-\nquence length reaches T. To generalize the MC search, we\ncan repeat the MC search N times for each partial sequence:\nMCG\u0012((X1:T;Y1:t);N) ={Y1\n1:T;Y 2\n1:T;:::;Y N\n1:T;}; (11)\nwhere Yn\n1:t represents the current partial sequence and Yn\n1:t =\nY1:t. Yn\nt+1:T is sampled via the policy G\u0012 . For an N-times\nMC search, we obtain N samples from the discriminator. Fi-\nnally, the N rewards for the partial sequence are averaged as\nRn((X1:T;Y1:t\n−1);yt) = 1\nN\nNX\nn=1\nR((X1:T;Y n\n1:T−1);\nyT)\nYn\n1:T ∈MCG\u0012(Y1:t; N) if t<T;\n(12)\nand\nRn((X1:T;Y1:t−1);\nyt) =R((X1:T;Y1:T−1);yT) if t= T: (13)\nAlgorithm 1:MC search under policy G\u0012\nInput: Rollout times: N, an generated sample:\nY1:T, Generator: G\u0012, Discriminator: D\u001e\n1 for n= 1to N do\n2 reward = [];\n3 for t= 1to T do\n// Sample the subsequence\n4 Y1:T = G\u0012:sample(X1:T;Y1:t);\n5 p= D\u001e(Y1:T);\n6 Compute the property score and penalty of\nY1:T: O(Y1:T);P(Y1:T);\n7 Use Eq. (10) to compute\nRn((X1:T;Y1:T−1);yT);\n8 if n== 1then\n9 reward.append(Rn((X1:T;Y1:T−1);yT))\n10 else\n11 reward += Rn((X1:T;Y1:T−1);yT)\n12 Rn((X1:T;Y1:T−1);yT) = reward / N;\n13 Use Eq.(15) to update the gradient of G\u0012\nAlgorithm 1 shows the MC search process with the stochastic\nrollout policy G\u0012.\nBy using the discriminator as a reward function to guide\nthe update of the generator, we can boost the number of\nhigh-quality sequences. We can then retrain the discriminator\nby using realistic generated sequences and the real molecule\ndataset. The loss function of the discriminator is given by\nmin −Ex∼pdata(x)[log D\u001e(x)] −Ey∼G\u0012[log(1 −D\u001e(y))]: (14)\nFollo\nwing the training of the discriminator, we retrain the\ngenerator. The gradient of J(\u0012) is derived as\n∇\u0012J(\u0012) ≃ 1\nT\nTX\nt=1\nX\nyt\n[Rn((X1:T;Y1:t−1);yt)\n·∇\u0012log G\u0012(yt|X1:T;Y1:t−1)]:\n(15)\nAlgorithm 2 outlines the proposed TransORGAN model.\nFirst, G\u0012 is pre-trained on real SMILES strings by using\nmaximum-likelihood estimation. Next, D\u001e is pre-trained by\nusing binary cross-entropy. Finally, G\u0012 and D\u001e are alter-\nnately trained by using the policy gradient method. The train-\ning phase also uses the teacher-forcing technique to ensure\nthe stability of training.\n4 Experiments\nWe conducted experiments to compare the performance of\nour proposed TransORGAN with those of RNN-based molec-\nular generation models. The test data were a subset of the\nZINC chemical dataset [Ramakrishnan et al., 2014 ], which\ncontains 134,000 molecules represented by SMILES strings.\nOur subset comprised 5,000 randomly selected molecules\nwith up to nine heavy atoms (e.g., carbon (C), oxygen (O),\nnitrogen (N), and ﬂuorine (F)) within the GDB-17 universe\nof 166.4 billion molecules [Ruddigkeit et al., 2012 ]. The\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3887\nAlgorithm 2:Pre/training for TransORGAN\n// Pre-train the Generator\n1 for i= 1to Pre Gen do\n2 Update G\u0012 based on maximum likelihood.\n// Pre-train the Discriminator\n3 for i= 1to Pre Dis do\n4 Update D\u001e based on binary cross entropy.\n// Adversarial training\n5 for i= 1to Adversarial epochs do\n6 for j = 1to Generator epochs do\n// Generate samples\n7 Y1:T = G\u0012:sample(X1:T);\n8 Calculate the expected reward: R(Y1:T);\n// Unsupervised training\n9 Update the gradient of G\u0012 using R(Y1:T);\n// Produce variant SMILES.\n10 (X; eX) = Variant(X);\n// Teacher-forcing training\n11 Update the gradient of G\u0012 using\n(X1:T; eX1:T);\n12 for k= 1to Discriminator epochs do\n13 Update D\u001e based on ( eX1:T;Y1:T)\nvocabulary size was 45. Note that the maximum length of\nmolecules in [Guimaraes et al., 2017 ] was set to 51; in this\npaper, this value is set to 75. We compared TransORGAN\nagainst the following models: Random Sampler[Gao and Co-\nley, 2020], SMILES LSTM [Guimaraes et al., 2017], JT-V AE\n[Jin et al., 2018], Graph-AF [Shi et al., 2019], Character-V AE\n[Kusner et al., 2017 ], Grammar-V AE[Kusner et al., 2017 ],\nNa¨ıve RL, and ORGAN[Guimaraes et al., 2017].\n4.1 Desired Chemical Properties\nThe chemical properties are scored in a range from zero (un-\nfavorable) to one (very favorable). Solubility describes the\ndegree of hydrophilicity of a molecule. The log octanol-water\npartition coefﬁcient (logP) is deﬁned as the logarithm of the\nratio of concentrations of a substance in a mixture of two sol-\nvents, octanol and water. Drug-likeness uses QED [Bicker-\nton et al., 2012] to describe the likelihood of a compound be-\ning a drug. Synthesizability uses the synthetic accessibility\nscore [Ertl and Schuffenhauer, 2009] to measure the difﬁculty\nof molecular synthesis.\n4.2 Hyperparameter Settings\nThe generator of TransORGAN is a transformer architecture.\nWe set the dimension of the word embedding to 16 and the\ndropout rate to 0.2. The encoder and decoder each had four\nheads and two stacked layers. The generator was pre-trained\nover 100 epochs by maximum likelihood estimation (MLE).\nThe dimension of the word embedding was 32 for the dis-\ncriminator. We set the number of kernels as 1, 3, 5, 7, and\n9; the kernel size as 20, 30, 40, 50, and 60; and the dropout\nrate to 0.75. In the pre-training phase, the discriminator was\npre-trained over ten epochs. In addition, we set the tradeoff\nbetween maintaining the likelihood and RL as \u0015 = 0:5. The\nMC search time N was set to 16. All experiments were per-\nformed by using Pytorch version 1.8.1.\n4.3 Evaluation Measures\nValidity was deﬁned the ratio of chemically valid molecules\namong all generated molecules. Uniqueness was deﬁned as\nthe number ratio of unique molecules among chemically valid\nsamples. Novelty was deﬁned as the ratio of valid molecules\nthat were absent in the training dataset and the set of chem-\nically valid molecules. Diversity was used to quantify the\nchemical diversity of the generated molecules. The similarity\nof chemical structures between molecules was calculated ac-\ncording to the Tanimoto coefﬁcient [Tanimoto, 1968] based\non MorganFingerprint [Cereto-Massagu´e et al., 2015]. All of\nthese statistics had a range from zero to one. A larger value\nindicated a better performance.\n4.4 Performance Evaluation\nObjective-reinforced methods aim to create compound struc-\ntures with desired chemical properties, which are critical in\ndrug discovery. Table 1 demonstrates the comparative re-\nsults of the objective-reinforced methods. In the case of sol-\nubility, Na¨ıve RL scored the highest validity (84.82%), but\nthe lowest uniqueness (46.55%) and diversity (0.339) among\nthe three models. Although TransORGAN had lower validity\n(74.03%) than Na¨ıve RL, it scored much higher than the other\ntwo models in terms of uniqueness, novelty, and diversity.\nThis result is reasonable because Na ¨ıve RL seeks only valid\nmolecules without considering other factors. Similar to Na¨ıve\nRL, ORGAN tries to optimize the tradeoff between validity,\nuniqueness, and diversity, so it generates valid molecules at\nthe expense of uniqueness and diversity. Repetitive molecules\nare not desired in practice. Moreover, Na¨ıve RL and ORGAN\ngenerated molecules containing many carbon atoms but few\nother atoms. This is why Na ¨ıve RL and ORGAN achieved\nhigher validity than TransORGAN but lower scores for the\nother three measures. In the case of drug-likeness and synthe-\nsizability, ORGAN scored the highest validity (81.59% and\n86.60%, respectively) among the three models but the lowest\nuniqueness (46.58% and 33.36%, respectively).\nIn drug discovery, chemists hope to generate molecules\nthat are not only valid but also novel. If the generated\nmolecules already exist in the original dataset, the generation\nis meaningless. TransORGAN achieved the highest unique-\nness, novelty, and diversity while maintaining relatively high\nvalidity. TransORGAN generated 3,327, 3,326, and 3,367\nnovel molecules with the desired solubility, drug-likeness,\nand synthesizability. Compared with Na ¨ıve RL, TransOR-\nGAN improved the solubility, drug-likeness, and synthesiz-\nability scores by 69.66%, 10.90%, and 115.70%, respectively.\nCompared with ORGAN, it improved the scores by 40.91%,\n75.33%, and 133.50%, respectively.\nTo evaluate the drug-likeness of generated molecules, we\nmeasured the QED scores. Figure 2 shows the QED distri-\nbutions when the objective was drug-likeness. Na¨ıve RL and\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3888\nObjective Method Validity Uniqueness Novelty Diversity\nSolubility\nNa¨ıve RL 4,234 (84.82%) 1,971 (46.55%) 1,961 (99.49%) 0.339\nORGAN 3,343 (66.97%) 2,381 (71.22%) 2,361 (99.16%) 0.358\nTransORGAN 3,654 (74.03%) 3,327 (91.05%) 3,327 (100.0%) 0.650\nDrug-likeness\nNa¨ıve RL 3,986 (79.85%) 3,005 (75.39%) 2,999 (99.80%) 0.293\nORGAN 4,073 (81.59%) 1,897 (46.58%) 1,897 (100.0%) 0.338\nTransORGAN 3,589 (72.71%) 3,326 (92.67%) 3,326 (100.0%) 0.624\nSynthesizability\nNa¨ıve RL 3,874 (77.60%) 1,561 (40.29%) 1,561 (100.0%) 0.365\nORGAN 4,323 (86.60%) 1,442 (33.36%) 1,442 (100.0%) 0.391\nTransORGAN 3,668 (74.31%) 3,367 (91.79%) 3,367 (100.0%) 0.656\nBold values indicate the maximum values in the objective-reinforced methods.\nTable 1: Evaluation results of generated molecules with objective reinforcement.\nFigure 2: Distributions of the QED scores for generated molecules.\nORGAN had mean QED values \u0016 of the 0:55 and 0:50, re-\nspectively. This result is intuitive because Na ¨ıve RL focuses\nonly on maximizing the rewards, whereas ORGAN main-\ntains the tradeoff between RL and GAN. Therefore, Na ¨ıve\nRL should have a larger \u0016than ORGAN. TransORGAN had\na larger \u0016(0:62) than the other models, which suggests that it\nis likely to generate more drug-like molecules than the other\nmodels. As an illustration, the top-12 novel molecules gen-\nerated by TransORGAN, Na¨ıve RL and ORGAN with drug-\nlikeness as an objective function are shown in Fig. A.2 in\nAppendix A. For example, the molecules generated by Tran-\nsORGAN include substructures that are often contained in\nmany approved drugs (e.g., 2-aminopyridine and amide sub-\nstructures). On the other hand, the molecules generated by\nORGAN and Na¨ıve RL include unnatural and unstable sub-\nstructures (e.g., an excessive number of halogen atoms on sul-\nfur atoms). These results suggest that TransORGAN can gen-\nerate more drug-like molecules than the two other models.\nTable 2 shows the comparative results of baseline meth-\nods. Random Sampler and JT-V AE had validity scores of\n100.0%, but they had the lowest novelty (0.0%) and unique-\nness (13.94%) among the baseline models. SMILES LSTM\nhad a higher uniqueness (98.41%), but it had the lowest va-\nlidity (42.81%) among the baseline methods. TransORGAN\nscored better than Graph-AF1 and was comparable to Graph-\nAF10, Character-V AE, and Grammar-V AE without objective\nreinforcement. However, the diversity scores of Character-\nV AE (0.392) and Grammar-V AE (0.451) were much lower\nMethod Validity Uniqueness Novelty Diversity\nRandom Sampler 100.0% 61.54% 0.0% 0.638\nSMILES LSTM 42.81% 98.41% 98.48% 0.423\nJT-V AE 100.0% 13.94% 99.43% 0.607\nGraph-AF1∗ 72.26% 84.80% 100.0% 0.786\nGraph-AF10∗ 67.27% 99.44% 100.0% 0.691\nCharacter-V AE 73.34% 99.18% 100.0% 0.392\nGrammar-V AE 76.36% 99.55% 100.0% 0.451\nTransORGAN 75.52% 94.64% 100.0% 0.682\nThe superscripts 1 and 10 for Graph-AF indicate the mini-\nmum length of the SMILES strings.\nTable 2: Comparison of TransORGAN with baseline methods.\nthan TransORGAN. These results suggest that TransORGAN\nhas high molecular generation abilities even though Tran-\nsORGAN is an objective-reinforced method. In summary,\nTransORGAN had high uniqueness, novelty, diversity while\nmaintaining relatively high validity.\n5 Conclusions\nWe proposed TransORGAN with the aim of generating\nhigh-quality molecules represented as SMILES strings in an\nobjective-reinforced manner. By leveraging the joint train-\ning of a sequence-based GAN model and an RL objective,\nthe proposed model generates molecules with high degrees\nof uniqueness, novelty, and diversity.\nOne limitation of TransORGAN is the difﬁculty of opti-\nmizing the values for the hyperparameters \u0015 and N. In fu-\nture work, we will investigate the effects of changing \u0015and\nN. Because \u0015 controls the tradeoff between MLE and RL,\nit may also affect the performance of the molecular gener-\nation. When \u0015 is small, TransORGAN strongly believes in\nthe ability of the discriminator. With increasing \u0015, TransOR-\nGAN becomes more focused on generating valid molecules.\nThe number of sample times N in the MC search may also\ninﬂuence the performance. Intuitively, a small N may lead to\nwrongly generated intermediate tokens, whereas a largeNin-\ncreases the computational time. We believe that the proposed\nmodels will help chemists produce meaningful new drugs.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3889\nAcknowledgements\nThis research was supported by AMED under Grant Num-\nber JP21nk0101111 and JSPS KAKENHI [grant number\n20H05797].\nReferences\n[Ar´us-Pous et al., 2019] Josep Ar ´us-Pous, Thomas\nBlaschke, Silas Ulander, Jean-Louis Reymond, Hong-\nming Chen, and Ola Engkvist. Exploring the gdb-13\nchemical space using deep generative models. Journal of\ncheminformatics, 11(1):1–14, 2019.\n[Bickerton et al., 2012] G Richard Bickerton, Gaia V\nPaolini, J ´er´emy Besnard, Sorel Muresan, and Andrew L\nHopkins. Quantifying the chemical beauty of drugs.\nNature chemistry, 4(2):90–98, 2012.\n[Bjerrum, 2017] Esben Jannik Bjerrum. Smiles enumera-\ntion as data augmentation for neural network modeling of\nmolecules. arXiv preprint arXiv:1703.07076, 2017.\n[Cereto-Massagu´e et al., 2015] Adri`a Cereto-Massagu ´e,\nMar´ıa Jos´e Ojeda, Cristina Valls, Miquel Mulero, San-\ntiago Garcia-Vallv ´e, and Gerard Pujadas. Molecular\nﬁngerprint similarity search in virtual screening. Methods,\n71:58–63, 2015.\n[Dai et al., 2018] Hanjun Dai, Yingtao Tian, Bo Dai, Steven\nSkiena, and Le Song. Syntax-directed variational autoen-\ncoder for molecule generation. In Proceedings of the in-\nternational conference on learning representations, 2018.\n[Danel et al., 2020] Tomasz Danel, Przemysław Spurek,\nJacek Tabor, Marek ´Smieja, Łukasz Struski, Agnieszka\nSłowik, and Łukasz Maziarka. Spatial graph convolutional\nnetworks. In International Conference on Neural Informa-\ntion Processing, pages 668–675. Springer, 2020.\n[De Cao and Kipf, 2018] Nicola De Cao and Thomas Kipf.\nMolgan: An implicit generative model for small molecular\ngraphs. arXiv preprint arXiv:1805.11973, 2018.\n[Ertl and Schuffenhauer, 2009] Peter Ertl and Ansgar Schuf-\nfenhauer. Estimation of synthetic accessibility score\nof drug-like molecules based on molecular complexity\nand fragment contributions. Journal of cheminformatics,\n1(1):1–11, 2009.\n[Gao and Coley, 2020] Wenhao Gao and Connor W Coley.\nThe synthesizability of molecules proposed by generative\nmodels. Journal of chemical information and modeling,\n60(12):5714–5723, 2020.\n[Goodfellow et al., 2014] Ian Goodfellow, Jean Pouget-\nAbadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. Gen-\nerative adversarial nets. Advances in neural information\nprocessing systems, 27, 2014.\n[Guimaraes et al., 2017] Gabriel Lima Guimaraes, Ben-\njamin Sanchez-Lengeling, Carlos Outeiral, Pedro\nLuis Cunha Farias, and Al ´an Aspuru-Guzik. Objective-\nreinforced generative adversarial networks (organ)\nfor sequence generation models. arXiv preprint\narXiv:1705.10843, 2017.\n[He et al., 2021] Jiazhen He, Huifang You, Emil Sandstr¨om,\nEva Nittinger, Esben Jannik Bjerrum, Christian Tyrchan,\nWerngard Czechtizky, and Ola Engkvist. Molecular opti-\nmization by capturing chemist’s intuition using deep neu-\nral networks. Journal of cheminformatics, 13(1):1–17,\n2021.\n[Jin et al., 2018] Wengong Jin, Regina Barzilay, and Tommi\nJaakkola. Junction tree variational autoencoder for molec-\nular graph generation. In International conference on ma-\nchine learning, pages 2323–2332. PMLR, 2018.\n[Kusner et al., 2017] Matt J Kusner, Brooks Paige, and\nJos´e Miguel Hern´andez-Lobato. Grammar variational au-\ntoencoder. InInternational Conference on Machine Learn-\ning, pages 1945–1954. PMLR, 2017.\n[Olivecrona et al., 2017] Marcus Olivecrona, Thomas\nBlaschke, Ola Engkvist, and Hongming Chen. Molecular\nde-novo design through deep reinforcement learning.\nJournal of cheminformatics, 9(1):1–14, 2017.\n[Ramakrishnan et al., 2014] Raghunathan Ramakrishnan,\nPavlo O Dral, Matthias Rupp, and O Anatole V on Lilien-\nfeld. Quantum chemistry structures and properties of 134\nkilo molecules. Scientiﬁc data, 1(1):1–7, 2014.\n[Ruddigkeit et al., 2012] Lars Ruddigkeit, Ruud\nVan Deursen, Lorenz C Blum, and Jean-Louis Reymond.\nEnumeration of 166 billion organic small molecules in the\nchemical universe database gdb-17. Journal of chemical\ninformation and modeling, 52(11):2864–2875, 2012.\n[Shi et al., 2019] Chence Shi, Minkai Xu, Zhaocheng Zhu,\nWeinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a\nﬂow-based autoregressive model for molecular graph gen-\neration. In International Conference on Learning Repre-\nsentations, 2019.\n[Sutton et al., 2000] Richard S Sutton, David A McAllester,\nSatinder P Singh, and Yishay Mansour. Policy gradient\nmethods for reinforcement learning with function approxi-\nmation. In Advances in neural information processing sys-\ntems, pages 1057–1063, 2000.\n[Tanimoto, 1968] Taffee T Tanimoto. An elementary mathe-\nmatical theory of classiﬁcation and prediction, ibm report\n(november, 1958). Automatic Information Organization\nand Retrieval, 1968.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing sys-\ntems, pages 5998–6008, 2017.\n[Weininger, 1988] David Weininger. Smiles, a chemical lan-\nguage and information system. 1. introduction to method-\nology and encoding rules.Journal of chemical information\nand computer sciences, 28(1):31–36, 1988.\n[Yu et al., 2017] Lantao Yu, Weinan Zhang, Jun Wang, and\nYong Yu. Seqgan: Sequence generative adversarial nets\nwith policy gradient. In Proceedings of the AAAI confer-\nence on artiﬁcial intelligence, volume 31, 2017.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3890",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.7491213083267212
    },
    {
      "name": "Transformer",
      "score": 0.7236756086349487
    },
    {
      "name": "Generative adversarial network",
      "score": 0.6364014148712158
    },
    {
      "name": "Computer science",
      "score": 0.5881574749946594
    },
    {
      "name": "Generative grammar",
      "score": 0.5364535450935364
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4133871793746948
    },
    {
      "name": "Machine learning",
      "score": 0.36112314462661743
    },
    {
      "name": "Engineering",
      "score": 0.2216060757637024
    },
    {
      "name": "Electrical engineering",
      "score": 0.17551210522651672
    },
    {
      "name": "Deep learning",
      "score": 0.08605268597602844
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I207014233",
      "name": "Kyushu Institute of Technology",
      "country": "JP"
    }
  ]
}