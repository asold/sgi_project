{
  "title": "EEG Conformer: Convolutional Transformer for EEG Decoding and Visualization",
  "url": "https://openalex.org/W4312597583",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2480043917",
      "name": "Yonghao Song",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1985865479",
      "name": "Qingqing Zheng",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shenzhen Institutes of Advanced Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2105708824",
      "name": "Bingchuan Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2004916865",
      "name": "Xiaorong Gao",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4200285824",
    "https://openalex.org/W3200688861",
    "https://openalex.org/W4210532851",
    "https://openalex.org/W2997591134",
    "https://openalex.org/W2505613605",
    "https://openalex.org/W3040978719",
    "https://openalex.org/W4283070704",
    "https://openalex.org/W1969878365",
    "https://openalex.org/W2143183535",
    "https://openalex.org/W2996722826",
    "https://openalex.org/W2792295722",
    "https://openalex.org/W2568407436",
    "https://openalex.org/W3014163061",
    "https://openalex.org/W2802685589",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2741907166",
    "https://openalex.org/W2559463885",
    "https://openalex.org/W3030231317",
    "https://openalex.org/W3215950319",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4289538860",
    "https://openalex.org/W4229068725",
    "https://openalex.org/W4281642485",
    "https://openalex.org/W2794345050",
    "https://openalex.org/W4200406972",
    "https://openalex.org/W3089752722",
    "https://openalex.org/W2792724009",
    "https://openalex.org/W4283460703",
    "https://openalex.org/W3131225866",
    "https://openalex.org/W3201991715",
    "https://openalex.org/W4206927580",
    "https://openalex.org/W3177342940",
    "https://openalex.org/W1947251450",
    "https://openalex.org/W1610928686",
    "https://openalex.org/W3047208690",
    "https://openalex.org/W2962905870",
    "https://openalex.org/W2790404832",
    "https://openalex.org/W2950162539",
    "https://openalex.org/W3024961463",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2062887494",
    "https://openalex.org/W3148095804",
    "https://openalex.org/W4210597087",
    "https://openalex.org/W3205821217",
    "https://openalex.org/W3102455230",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3173912422"
  ],
  "abstract": "Due to the limited perceptual field, convolutional neural networks (CNN) only extract local temporal features and may fail to capture long-term dependencies for EEG decoding. In this paper, we propose a compact Convolutional Transformer, named EEG Conformer, to encapsulate local and global features in a unified EEG classification framework. Specifically, the convolution module learns the low-level local features throughout the one-dimensional temporal and spatial convolution layers. The self-attention module is straightforwardly connected to extract the global correlation within the local temporal features. Subsequently, the simple classifier module based on fully-connected layers is followed to predict the categories for EEG signals. To enhance interpretability, we also devise a visualization strategy to project the class activation mapping onto the brain topography. Finally, we have conducted extensive experiments to evaluate our method on three public datasets in EEG-based motor imagery and emotion recognition paradigms. The experimental results show that our method achieves state-of-the-art performance and has great potential to be a new baseline for general EEG decoding. The code has been released in https://github.com/eeyhsong/EEG-Conformer.",
  "full_text": "710 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nEEG Conformer: Convolutional T ransformer\nfor EEG Decoding and Visualization\nY onghao Song, Graduate Student Member, IEEE, Qingqing Zheng , Member, IEEE,\nBingchuan Liu , Student Member, IEEE, and Xiaorong Gao , Member, IEEE\nAbstract — Due to the limited perceptual ﬁeld, convolu-\ntional neural networks (CNN) only extract local temporal\nfeatures and may fail to capture long-term dependencies for\nEEG decoding. In this paper, we propose a compact Convo-\nlutional Transformer, named EEG Conformer, to encapsulate\nlocal and global features in a uniﬁed EEG classiﬁcation\nframework. Speciﬁcally, the convolution module learns the\nlow-level local features throughout the one-dimensional\ntemporal and spatial convolution layers. The self-attention\nmodule is straightforwardly connected to extract the\nglobal correlation within the local temporal features.\nSubsequently, the simple classiﬁer module based on fully-\nconnected layers is followed to predict the categories for\nEEG signals. To enhance interpretability, we also devise\na visualization strategy to project the class activation\nmapping onto the brain topography. Finally, we have\nconducted extensive experiments to evaluate our method\non three public datasets in EEG-based motor imagery\nand emotion recognition paradigms. The experimental\nresults show that our method achieves state-of-the-art\nperformance and has great potential to be a new baseline\nfor general EEG decoding. The code has been released in\nhttps://github.com/eeyhsong/EEG-Conformer.\nIndex Terms — EEG classiﬁcation, self-attention, trans-\nformer, brain-computer interface (BCI), motor imagery.\nI. I NTRODUCTION\nB\nRAIN-COMPUTER interface (BCI) is an emerging\ntechnology in recent decades, which establishes a direct\npathway between external devices and the brain. BCI has\nbrought many new applicati ons in motor rehabilitation,\nemotion recognition, human-machine interaction, etc [1], [2],\n[3]. Among various non-invasive techniques, electroencephalo-\ngraph (EEG) is widely employed to detect neural activities,\nManuscript received 20 September 2022; revised 21 November 2022;\naccepted 11 December 2022. Date of publication 16 December 2022;\ndate of current version 2 February 2023. This work was supported\nin part by the National Natural Science Foundation of China under\nGrant U2241208, Grant 62206270, and Grant 62171473; in part by\nthe Key Research and Development Program of Ningxia under Grant\n2022CMG02026; in part by the GuangDong Basic and Applied Basic\nResearch Foundation under Grant 2021A1515110598; and in part by\nthe Doctoral Brain+X Seed Grant Program of Tsinghua University.\n(Corresponding author: Xiaorong Gao.)\nY onghao Song, Bingchuan Liu, and Xiaorong Gao are with the\nDepartment of Biomedical Engineering, School of Medicine, Tsinghua\nUniversity, Beijing 100084, China (e-mail: gxr-dea@tsinghua.edu.cn).\nQingqing Zheng is with the Guangdong Provincial Key Laboratory\nof Computer Vision and Virtual Reality Technology, Shenzhen Institute\nof Advanced Technology, Chinese Academy of Sciences, Shenzhen\n518055, China.\nDigital Object Identiﬁer 10.1109/TNSRE.2022.3230250\nusing a cap with multiple electrodes to capture changes in\npotential on the scalp. With collected EEG signals, people\ncan decode them into movement, vision, and other intentions,\nthen use the results to control external devices such as\ncomputers, wheelchairs, and robots [4], [5], [6]. Although\nEEG is convenient and low-cost, EEG decoding is still very\nchallenging due to many artifacts caused by impedance and\nother physiological signals [7].\nVarious pattern recognition methods have been developed\nto decode useful information from noisy EEG signals.\nThese methods extract features and perform classiﬁcation for\ndifferent tasks. For example, common spatial pattern (CSP)\nis used to enhance spatial features for motor imagery (MI)\ntasks [8]. The ﬁlter bank is further embedded for frequency\nrhythms in MI and steady-state visually evoked potential\n(SSVEP) classiﬁcation [9]. Continuous wavelet transform\n(CWT) is utilized to extract time-frequency features from\nEEG signals for detecting dementia [10]. Empirical wavelet\ntransform (EWT) is applied to obtain improved time-frequency\nfeatures from EEG with good performance for seizure\ndetection [11], [12]. With these representative features, we can\neffectively achieve EEG decoding just by following a classiﬁer,\nsuch as support vector machine (SVM) and multi-layer\nperceptron (MLP) [13], [14]. However, most traditional feature\nextraction methods are task-dependent, meaning that features\nare obtained with speciﬁc prior knowledge for different BCI\nparadigms and of limited generalization. Moreover, optimizing\nfeature extraction and classiﬁer separately may also lead to\nimperfect global optimization.\nResearchers further attempt to decode EEG with end-to-\nend convolutional neural network (CNN), which has shown\nexcellent representation capability in computer vision tasks\n[15]. As expected, the modiﬁed CNN model, ConvNet [16],\nachieves comparable performance to traditional algorithms\non EEG classiﬁcation tasks, learning discriminative features\nin convolutional layers. Similarly, the compact EEGNet [17]\ndemonstrates remarkable te mporal feature perception and\nshows good generalization across multiple BCI paradigms.\nNevertheless, due to the limited kernel size, CNNs learn\nfeatures with local receptive ﬁelds, but fail to acquire long-\nterm dependencies that are crucial for time series. Recurrent\nneural networks (RNN) and long short-term memory (LSTM)\nare further proposed to capture temporal features for EEG\nclassiﬁcation [18], [19]. However, such models cannot be\ntrained in parallel, and the dependency inﬂuence computed\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nSONG et al.: EEG CONFORMER: CONVOLUTIONAL TRANSFORMER FOR EEG DECODING AND VISUALIZATION 711\nby the hidden states is quickly lost after a few time\nsteps.\nLately, attention-based Transformer models have made\nwaves in natural language and image processing due to the\ninherent perception of global dependencies [20]. Transformers\nalso emerge in EEG decoding and achieve good performance,\nby leveraging long-term temporal relationships [21], [22].\nHowever, such models ignore learning local features, which\nare also necessary for EEG decoding. In that case, extra feature\nextraction processing, such as activity map and spatial ﬁlter,\nhas to be added for compensation [23], [24]. And there is no\ndetailed analysis and visualization to clarify how Transformer\nworks for EEG decoding. Therefore, Transformer models\nremain explored in the EEG domain and not yet capable of\nserving as end-to-end backbones for raw EEG classiﬁcation.\nTo tackle the above issues, we propose a Convolu-\ntional Transformer framework, named EEG Conformer,\nto comprehensively exploit the advantages of both CNN\nand Transformer. The overall framework consists of three\ncomponents in series, namely, the convolution module, the\nself-attention module and the classiﬁer. In the convolution\nmodule, we ﬁrst employ temporal and spatial convolutions\nto capture local temporal and spa tial features, respectively.\nAn average pooling layer is followed to slice temporal feature\nsegments, which not only reduces the model complexity\nbut also removes redundant information. Then, we treat all\nconvolutional channels at each point in the time dimension\nas a token and feed them into the self-attention module,\nwhich further learns the global temporal dependencies with\nself-attention layers. Finally, simple fully-connected layers are\nused to obtain the decoding results. Detailed comparative\nexperiments are performed on several EEG datasets of\ndifferent paradigms to revealthe remarkable performance of\nEEG Conformer.\nThe contributions are summarized as follows:\n• We propose a concise network named Convolutional\nTransformer (EEG Conformer) to couple local features\nand global features of EEG signals. It achieves state-of-\nthe-art results on three public datasets, with the potential\nto be a new backbone for EEG decoding.\n• We conduct extensive experiments to investigate the effect\nof the Transformer module and attention parameters.\nThe results show that our model is insensitive to the\ndepth and head number of the self-attention module while\nprocessing EEG data.\n• We design a novel visualization based on class activation\nmapping and topography to illustrate how the model\nlearns essential features from a global perspective.\nThe rest of this paper is organized as follows. See Section II\nfor the related works. A detailed description of the method\nis given in Section III. We present experiments and results\nin Section IV. After then, there is a careful discussion in\nSection V. Finally, we draw a conclusion in Section VI.\nII. R\nELA TEDWORKS\nA. EEG Decoding With Machine Learning\nAdvances in machine learning have facilitated the devel-\nopment of EEG classiﬁcation [25], [26], [27]. In recent\nyears, end-to-end deep learning methods have been widely\nadopted to process EEG signals and show good generalization.\nSchirrmeister et al. [16] proposed a shallow ConvNet\nwith temporal and spatial convolutional layers to decode\ntask-related information from raw EEG signals. Similarly,\nLawhern et al. [17] developed a compact EEGNet with\nconvolution along the temporal dimension and depthwise\nconvolution along the spatial dimension, respectively. These\ntwo robust EEG-based CNN backbones soon inspired many\nexcellent studies. Sakhavi et al. [28] used CNN to learn\ntemporal information from the ﬁlter bank CSP features and\nselect architecture parameters for each subject. Shan et al. [29]\nleveraged the cross-channel topological connectivity by\nintroducing graphs to spatial-temporal CNN. Hong et al. [30]\nextracted subject-invariant features via CNN in an adversarial\nlearning-driven domain adaptation framework. There are also\nworks that proposed some tricks to enhance the performance\nof CNN for EEG-based motor imagery tasks [31], [32].\nB. Attention-Based Transformer Network\nAttention-based Transformers derived from machine trans-\nlation have attracted much attention. The attention mechanism\nhas the intrinsic ability to evaluate global dependencies on\nvery long sequences [20]. Dosovitskiy et al. [33] applied\npure Transformer on image patches and achieved good results\ncompared with CNN-based methods. Transformers are brought\ninto EEG processing because the global interaction is non-\nnegligible in task-related EEG trials. Kostas et al. [34]\ndesigned a pre-training and ﬁne-tuning approach using\nTransformer for EEG classiﬁcation tasks. Song et al.\nperformed feature learning from the spatial and temporal\ndomains, where the EEG signal was sliced along the time\ndimension [22]. A similar framework was given by Liu et al.\n[35] to deal with differential entropy features of EEG.\nBagchi et al. [23] converted EEG to multi-frame activity maps,\nthen used a CNN-based module as well as combined CNN and\nTransformer modules to capture useful information. However,\nfeature extraction reduces the information in raw data and\noften tends to depend on speciﬁc tasks. And previous studies\nusually focused on how to improve EEG decoding accuracy,\nwhile neglecting to interpret the role of global features with\nlong-term dependencies visually. Therefore, inspired by the\nworks above, we propose the EEG Conformer as an efﬁcient\nbackbone with novel visualization.\nIII. M\nETHODS\nA. Overview\nAs an emerging neural network, Transformer is good at\ncapturing global dependencies, but how to effectively apply\nit in EEG decoding remains to be explored. In this paper,\nwe propose a novel framework, called EEG Conformer,\nto combine CNN and Transformer straightforwardly for end-\nto-end EEG classiﬁcation. Borrowing ideas from CNN and\nTransformer, the Conformer uses convolution to learn local\ntemporal and spatial features and then adopts self-attention to\nencapsulate global temporal features.\n712 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nFig. 1. The framework of Convolutional Transformer (Conformer), includinga convolution module, a self-attention module, and a classiﬁer module.\nThe overall framework is depicted in Fig. 1. The archi-\ntecture comprises three components: a convolution module,\na self-attention module, and a fully-connected classiﬁer. In the\nconvolution module, taking the raw two-dimensional EEG\ntrials as the input, temporal and spatial convolutional layers\nare applied along the time dimension and electrode channel\ndimensions, respectively. Then, an average pooling layer\nis utilized to suppress noise in terference while improving\ngeneralization. Secondly, the spatial-temporal representation\nobtained by the convolution module is fed into the self-\nattention module. The self-attention module further extracts\nthe long-term temporal features by measuring the global\ncorrelations between different time positions in the feature\nmaps. Finally, a compact classiﬁer consisting of several fully-\nconnected layers is adopted to output the decoding results.\nB. Preprocessing\nThe raw EEG trials are of size ch × sp,w h e r e\nch represents electrode channels and sp denotes time\nsamples. Without introducing additional task-dependent prior\nknowledge, we only use a few steps to pre-process the raw\nEEG data. First, band-pass ﬁltering is employed to ﬁlter out\nextraneous high and low-frequency noise. Here, we use a\n6-order Chebyshev ﬁlter to preserve task-relevant rhythms.\nThen, a Z-score standardization is performed to reduce the\nﬂuctuation and nonstationarity as\nx\no = xi − μ\n√\nσ2\n, (1)\nwhere xi and xo denote band-pass ﬁltered data and the output\nof standardization, respectively.μ and σ2 represent the mean\nand variance, calculated with the training data and used\ndirectly for the test data.\nC. Network Architecture\nAs shown inFig. 1, EEG Conformer consists of three steps\nin the end-to-end process: convolution module, self-attention\nmodule, and fully-connected classiﬁer. The input is a batch of\npre-processed EEG trials with channel and sample dimensions,\nexpanded by one dimension as the convolution channel. The\noutput is the probability of different EEG categories.\nTABLE I\nNETWORK ARCHITECTURE OF THE CONVOLUTION MODULE\n1) Convolution Module: Inspired by [16] and [17], we design\nthe convolution module by separating the two-dimensional\nconvolution operator into two one-dimensional temporal and\nspatial convolution layers. The ﬁrst layer hask kernels of size\n(1,25) with a stride of(1,1), which means the convolution is\nperformed over the time dimension. The second layer keepsk\nkernels of size(ch,1) with a stride of(1,1),w h e r ech equals\nthe number of electrode channels of EEG data. This layer acts\nas a spatial ﬁlter to learn the representation of the interactions\nbetween different electrode ch annels. Subsequently, batch\nnormalization is adopted to boost the training process and\nalleviate overﬁtting. We use exponential linear units (ELUs)\nas the activation function for nonlinearity following [17]. The\nthird layer is an average pooling along time dimension with\nthe kernel size of(1,75) and a stride of(1,15). This pooling\nlayer smooths the temporal features, which not only avoids\noverﬁtting, but also reduces the computational complexity.\nAs shown inTable I, the hyper-parameterk is set to 40. In the\nend, we rearrange the feature maps of the convolution module,\nsqueeze the electrode channel dimension, and transpose the\nconvolution channel dimension with the time dimension.\nIn this way, we feed all feature channels of each temporal\npoint as a token into the next module.\n2) Self-Attention Module: We assume that the context-\ndependent representation within the low-level temporal-spatial\nfeatures would beneﬁt the EEGdecoding, because the neural\nactivities are coherent. In this module, we use self-attention\nto learn global temporal dependencies of EEG features,\ncomplementing the limited receptive ﬁeld in the convolution\nmodule. The arranged tokens from the previous module are\nlinearly transformed into equal-shaped triplicates, called query\n(Q), key (K), and value (V). Dot product is employed over Q\nand K to evaluate the correlation between different tokens.\nA scaling factor is designed to avoid vanishing gradients,\nSONG et al.: EEG CONFORMER: CONVOLUTIONAL TRANSFORMER FOR EEG DECODING AND VISUALIZATION 713\nthus ensuring stable training. The result is passed through a\nSof tmax function to obtain the weighting matrix, namely the\nattention score. Then the attention score is weighted on V with\na dot product [20]. This process can be formulated as\nAttention(Q, K, V ) = Sof tmax ( QK T\n√\nk\n)V, (2)\nwhere k denotes the length of a token. Besides, two\nfully-connected feed-forward layers are connected behind to\nenhance the ﬁtting ability. The input and output sizes of this\nprocess remain the same. The entire attention computation is\nrepeated N times in the self-attention module.\nWe also employ the multi-head strategy to further improve\nrepresentation diversity. The tokens are equally divided into\nh segments and fed into the self-attention module separately,\nand the results are concatenated as the module output [20].\nThe process can be expressed as\nMHA(Q, K, V ) =[ head\n0;···; head h−1],\nhead l = Attention(Ql , Kl , Vl ) (3)\nwhere MHA stands for multi-head attention, Ql , Kl , Vl ∈\nRm×k/h denote the query, key, and value obtained by linear\ntransformation of divided token in thel-th head, respectively.\n3) Classiﬁer Module : Finally, we adopt two fully-\nconnected layers as the classiﬁer module, which outputs an\nM-dimensional vector afterSof tmax function. Cross-entropy\nis used as the loss function of the whole framework as\nL =− 1\nNb\nNb∑\ni=1\nM∑\nc=1\ny log(ˆy). (4)\nwhere M represents the number of EEG categories, y and\nˆy are the ground truth and predicted label, respectively. Nb\ndenotes the number of trials in a batch.\nTo sum up, the band-pass ﬁltered and standardized EEG data\nare fed into the model ﬁrstly. Then the data are sequentially\npassed through the temporal and spatial convolution layers and\narranged into tokens by the pooling layer. After that,N self-\nattention layers are used, followed by fully-connected layers\nto output the classiﬁcation results.\nIV . E\nXPERIMENTS AND RESULTS\nIn this section, we conduct experiments to verify the\nproposed network on three public EEG datasets, including\npopular motor imagery and emotion recognition paradigms.\nWe not only compare our method with different state-of-\nthe-art approaches, but also demonstrate the improvements\nby introducing the attention-based Transformer through\nablation studies. We also present detailed comparative\nexperiments to show the inﬂuence of attention parameters on\noverall performance. Finally, we design different visualization\nmethods for interpretability.\nA. Datasets\nWe evaluate our method on three widely used EEG\ndatasets, including BCI competition IV dataset 2a, 1 BCI\n1https://www.bbci.de/competition/iv/desc_2a.pdf\ncompetition IV dataset 2b,2 SEED3 [36] These EEG datasets\nwere collected with different acquisition devices, paradigms,\nnumber of subjects, and sample size, thus fairly validating the\ngeneralization of our method.\n1) Dataset I: BCI Competition IV Dataset 2a provided by\nGraz University of Technology consists of EEG data from\n9 subjects. There were four motor imagery tasks, covering\nthe imagination of moving left hand, right hand, both feet,\nand tongue. Two sessions on different days were collected\nwith twenty-two Ag/AgCl electrodes at a sampling rate of\n250 Hz. One session contained 288 EEG trials, i.e., 72 trials\nper task. We used[2,6] seconds of each trial and ﬁltered the\nEEG data to[4,40] Hz with a band-passed ﬁlter as [8] in our\nexperiments. The ﬁrst session was used for training and the\nsecond session for test.\n2) Dataset II: BCI Competition IV Dataset 2b provided by\nGraz University of Technology consists of EEG data from\n9 subjects. There were two motor imagery tasks, covering the\nimagination of moving left and right hand. Five sessions were\ncollected with three bipolar electrodes (C3, Cz, and C4) at a\nsampling rate of 250 Hz and each session contained 120 trials.\nWe used the [3,7] seconds of each trial in the experiments.\nWe also performed band-pass ﬁltering between[4,40] Hz to\nreduce high and low-frequencynoise. The ﬁrst three sessions\nwere training set, and the last two sessions were test set.\n3) Dataset III: SEED dataset provided by Shanghai Jiao\nTong University consists of emotion-based EEG signals from\n15 subjects. There were three emotions, including positive,\nneutral, and negative, stimulated by ﬁfteen ﬁlm clips. The data\ncollection process was repeated three times on each subject\nat approximately weekly intervals. The EEG signals were\ncaptured with 62 electrodes at a sample rate of 1000 Hz\nand subsequently downsampled to 200 Hz. Each sample was\nsegmented with a non-overlapped one-second time window,\nresulting in a total of 3394 trials from one session. We also\nperformed band-pass ﬁltering of[4,47] Hz on the data. Five-\nfold cross-validation was used in the SEED dataset.\nB. Data Augmentation\nEEG acquisition is time-consuming, which results in\nsmall datasets that are prone to overﬁtting. Some methods\nemploy data augmentation to feed enough samples into the\nmodels [16]. However, the conventional strategies of adding\nGaussian noise or cropping may further lower the signal-\nto-noise ratio or destroy the original coherence. Therefore,\nwe employ segmentation and reconstruction (S&R) in the time\ndomain to generate new data. Follow [37], the training samples\nof the same category are equally divided into N\ns segments,\nthen randomly concatenated while maintaining the original\ntime order. We generate the augmented data of the same size\nas the batch in each iteration.\nC. Experiment Details\nOur method is implemented with PyTorch library in Python\n3.10 with a Geforce 3090 GPU. We train the model using\n2https://www.bbci.de/competition/iv/desc_2b.pdf\n3https://bcmi.sjtu.edu.cn/home/seed/seed.html\n714 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nAdam optimizer with the learning rate,β1 and β2 of 0.0002,\n0.5, and 0.999, respectively. We set the execution timesN of\nself-attention to 6, the number of headsh to 10, and the Ns\nin S&R to 8. The classiﬁcation accuracy and kappa are used\nas evaluation metrics for the overall performance. Kappa can\nbe calculated with\nkappa = po − pe\n1 − pe\n, (5)\nwhere po represents the average accuracy of all the trials and\npe denotes the accuracy of random guesses. Wilcoxon Signed-\nRank Test is employed to analyze the statistical signiﬁcance.\nD. Baseline Comparison\nWe conduct extensive subject-dependent experiments and\ncompare our method with some state-of-the-art approaches on\nthree public datasets.\nDatasets I is currently the most widely used multi-class\nmotor imagery dataset. We compare many representative\nmethods, which have achieved impressive performance on\nthis dataset. For example, FBCSP [8], the winner of BCI\nCompetition IV using hand-crafted spatial features; Con-\nvNet [16] and EEGNet [17], which have shown remarkable\nresults on many EEG datasets with CNN-based end-to-end\nframeworks; C2CM [28], which inputs the FBCSP features\nto the CNN model, combining the advantages of traditional\nfeature extraction and deep learning methods; FBCNet [38],\nextracting spectro-spatial features by spatial ﬁltering multi-\nview data. We even compare with deep representation-based\ndomain adaptation (DRDA) [39] that utilizes data from other\nsubjects for enhancement with adversarial learning.\nThe classiﬁcation performance of each subject and the\naverage results on Dataset I are presented in Table II.\nWe can observe that our Conformer signiﬁcantly improves the\naccuracy by 10.91% over FBCSP (p < 0.01), which depends\non traditional feature extraction. The results also show that\nother deep learning methods, such as ConvNet and EEGNet,\noutperform FBCSP , indicating that the CNN-based methods\nhave strong feature representation capability. However, these\nCNN-based methods only focus on local features due to the\nlimited perceptual ﬁeld, and ignore the global correlation,\nwhich may compromise the decoding accuracy for coherent\nEEG series. Differently, our method encapsulates both the\nlocal and global dependencies by integrating Transformer\narchitecture on the basis of the original CNN. Thus, Conformer\nobtains better results on most subjects and achieves signiﬁcant\nupgrades on average accuracy (p < 0.05) and kappa. C2CM\nand FBCNet effectively combine the idea of hand-crafted\nfeatures and deep models, but still cannot beat ours except for\nsubject 5 (p < 0.05), although C2CM ﬁne-tuned the model\nparameters for each subject. DRDA brings in data from other\nsubjects with the distribution aligned to the target subject,\nwhich is still inferior to ours just using the data of target\nsubject (p < 0.05), once again demonstrating the effectiveness\nof leveraging both local and global features.\nThen we present the comparison with several state-of-the-\nart methods on Dataset II in Table III. We can see that the\nbinary classiﬁcation results show similar trends as in Dataset I.\nFig. 2. Loss and accuracy during training of EEG Conformer.\nConformer promotes the overall performance signiﬁcantly\ncompared with FBCSP (p < 0.05), with even an increasing\naccuracy of 12.5% on subject 1. There is an obvious boost\nby contrast with other end-to-end methods using just CNN\narchitecture, with improvements of 5.25% and 4.15% for\nConvNet (p < 0.05) and EEGNet (p < 0.01). The average\naccuracy and kappa of our method still precede DRDA on\nalmost all the subjects, which further validates the efﬁcacy of\nour method.\nWe also comprehensively evaluate our method on Dataset III\nof multi-category EEG emotion data. We compare with\nmachine learning methods like SVM [36], which ﬁrst achieved\nnotable results on this dataset; graph regularized extreme\nlearning machine (GELM) [40] with a single feed-forward\nlayer to learn discriminative features, and regions to global\nspatial-temporal neural network (R2G-STNN) [42] that adopts\nthe bidirectional long short term memory to learn spatial\nand temporal features of emotion EEG signals. Besides,\ngraph-based neural networks learning the intrinsic relationship\namong different EEG channels such as dynamical graph\nconvolutional neural network(DGCNN) [41] and regularized\ngraph neural network (RGNN) [43] are also included for\ncomparison. The results are presented in Table IV. It can\nbe seen that Conformer is still competitive on Dataset III\ncompared with other state-of-the-art methods. In this way,\nour method achieves impressive performance on both motor\nimagery and emotion recognition paradigms, illustrating that\nour method has good generalization.\nE. Training Process\nIn image processing, Transformer models often need a\nlarge amount of data for pre-training to achieve good results\nin downstream tasks. However, pre-training is not used in\nEEG Conformer, due to the limited data for calibration.\nWe demonstrate the trend of loss and accuracy during training\nin Fig. 2. The process is stable under the lightweight use of\nthe self-attention module. It can be noticed that the model\nconverges quickly around the 250\nth epoch. Moreover, our\nmethod is also efﬁcient. We train the Conformer model\ncontinuously with the ﬁrst subject in Dataset I for 2000 epochs\nSONG et al.: EEG CONFORMER: CONVOLUTIONAL TRANSFORMER FOR EEG DECODING AND VISUALIZATION 715\nTABLE II\nCOMPARISONS WITH STATE-OF -THE -ART METHODS ON DATASETS I\nTABLE III\nCOMPARISONS WITH STATE-OF -THE -ART METHODS ON DATASETS II\nTABLE IV\nCOMPARISONS WITH STATE-OF -THE -ART METHODS ON DATASETS III\non a single GPU, obtaining an average time of 0.27 seconds\nper epoch.\nF . Ablation Study\nThe key improvement of EEG Conformer over the\nCNN-based approach is the addition of the attention-based\nTransformer module for learning global representations.\nAs well, data augmentation may have contributed to the ﬁnal\nresults. Therefore, We conduct an ablation study on Dataset I,\nas shown in Fig. 3, where the self-attention module and the\nS&R data augmentation is removed separately. It can be\nseen that when the Transformer part is removed, there is a\nsubstantial decrease in the result on each subject. Subject 6\nreduces the most by 8.68%, and subject 3 reduces the\nleast by 3.12%. The average accuracy drops signiﬁcantly by\n6.02% (p < 0.01). Similar to ConvNet [16], the experimental\nresults in Fig. 3 also show the data augmentation strategy\ncan help improve the performance of our model. The overall\nperformance improves by an average accuracy of 3.75%\n(p < 0.01) compared with the one without data augmentation.\nInterestingly, the improvement is only 1 .04% for subject\n1 with better discrimination, while for subject 5 and 6, who\nperform originally poor, the improvements are more signiﬁcant\nand reach 4 .86% and 5 .56%, respectively. Therefore, the\nintroduction of data augmentation in the training process\nenhances the robustness of Conformer.\nFig. 3. Ablation study on the self-attention module and data\naugmentation.\nG. Parameter Sensitivity\nIn this section, we evaluate in detail the impact of\nseveral important parameters in the self-attention module on\nperformance. These include the depth N of self-attention\nlayers, the numberh of attention heads, and the design of the\npooling kernel, which constructs the input for learning global\nfeatures.\nDepth is usually a crucial factor affecting the ﬁtting ability\nof end-to-end models, such as CNN and Transformer. As in\nFig. 4, we explore the effect of depth on EEG Conformer\nby gradually increasing the layers of self-attention module\nfrom 0 to 15. It can be seen that for Dataset I, there is\na signiﬁcant improvement in accuracy when the depth goes\nfrom 0 to 1 ( p < 0.01). It illustrates the introduction of\nTransformer does help EEG decoding once again. For the\nother depths, the highest accuracy is only 1.24% higher than\nthe lowest. And the difference is not signiﬁcant (p > 0.05).\nHowever, as shown in the parameter curves, the number\n716 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nFig. 4. The inﬂuence of the depth of the self-attention module\n(from 0 to 15) on the accuracy and the amounts of parameters for\nDataset I and II.\nFig. 5. The inﬂuence of the number of attention heads on the accuracy\nfor different datasets.\nof parameters increases proportionally with depth, which\nmakes the model less cost-effective. The same evaluation on\nDataset II also shows the insensitivity of Conformer to self-\nattention depth.\nHead is an important parameter of common Transformer\nmodels based on the multi-head attention. It is reported that\nit can help to learn different aspects of features. We also\ncompare the impact of different head selections on the model,\nas shown in Fig. 5, choosing eight head numbers between\n1 and 40. From the box, there is no clear pattern for the\neffect of different head numbers on the results. The distribution\nof different subjects has no obvious difference. The average\naccuracy maintains a mild ﬂuctuation, where the range is just\n1.43% on Dataset I and 1.02% on Dataset II. The performance\nhas a slight upward trend as the head number increases\nbut then declines. The average accuracy is 0.82% higher in\nDataset I and 0.50% higher in Dataset II (p > 0.05), when\nthe number of heads is taken as 10 than when it is taken as 1.\nOverall, changes in the number of heads have not yet shown\na signiﬁcant effect in prompting feature learning.\nFig. 6. The inﬂuence of different kernel sizes in the pooling layer, namely,\nthe token size of the self-attention module.\nFig. 7. t-SNE visualization illustrates the signiﬁcance of introducing\nTransformer for feature learning. Different colors represent different\ncategories.\nThe token determined by the pooling kernel, is also a critical\nfactor for the self-attention module. If the kernel size is too\nlarge, the temporal features would be too smoothed and lose\nuseful details. Thus, it is difﬁcult for the model to perceive\nthe global relationship between details. In contrast, if the\nkernel is too small, the performance may be easily affected\nby local noise. We compare the effect of different pooling\nkernel choices on model performance as inFig. 6.T h ek e r n e l\nsize is taken from 15 to 135 with an interval of 10. It is clear\nto see a substantial upgrade in the average accuracy when the\nkernel size starts to grow. A gain of 13.08% (p < 0.01) is\nobtained on Dataset I by increasing the kernel from 15 to 45.\nAfter that, the results ﬂatten out and do not rise observably\nwith increasing kernel size. The experiments demonstrate that\napplying self-attention to sufﬁciently large slices does make\nsense for EEG with a low signal-to-noise ratio.\nSONG et al.: EEG CONFORMER: CONVOLUTIONAL TRANSFORMER FOR EEG DECODING AND VISUALIZATION 717\nFig. 8. Raw EEG topography averaged over all trials of each subject, Class Activation Mapping (CAM) of the Transformer module on the input\nEEG, Class Activation Topography (CA T) we designed to show CAM-weighted EEG. Raw shows that many regions are activated throughout the\ntrial. CAM shows that our model pays different attention to different ranges in the time domain. CA T shows our model focus on areas of the motor\ncortex in motor imagery data.\nFig. 9. CA T shows the ERD/ERS phenomena on both the data of imagining left and right hand movements, compared to the irregular\npatterns in raw EEG topography. Contralateral activation and ipsilateral inhibition can be clearly observed in the CA T of several subjects, such\nas S1, S7, and S8.\nH. Visualization\nWe visualize two perspectives to show the interpretability\nof EEG Conformer, including deep features by t-SNE [44] and\nspatial-temporal features reﬂected on topography.\n1) Feature Distribution: t-distributed stochastic neighbor\nembedding (t-SNE) is a popular statistical dimension reduction\nand visualization method. The feature distribution of Subject 1\nin Dataset I after adequate training with and without\nTransformer is shown inFig. 7. We can see that for training\ndata, the features of different categories are relatively close\nwithout the help of Transformer. After adding Transformer, the\ninter-category distance becomes larger, and the intra-category\ndistance becomes smaller, as inFig. 7(b). On the other hand,\nthe aliasing between categories is evident in the absence of\nTransformer, which sharpens category boundaries inFig. 7(d).\n2) Global Representation: Transformer is introduced to\nlearn global temporal dependencies in EEG data, which\nmeans locating more important information for decoding tasks\nfrom time series. We use topography and Gradient-weighted\nClass Activation Mapping (CAM) [45] to show the global\nrepresentation learned by our model with motor imagery\nDataset I inFig. 8. The ﬁrst row in the ﬁgure denotes that all\ntraining trials of each subject are averaged for the topography.\nThere are no apparent clues of active brain regions among\ndifferent subjects. CAM is adopted to monitor the time period\nthat the self-attention module pays attention to on the EEG\nfeatures, as shown in the second row ofFig. 8. EEG data is\ndrawn as a circle, clockwise from the top during the motor\nimagery process. Different activation is presented at different\ntime. As expected, data of all subjects are attenuated at the\nbeginning of trials, which may indicate a latency for movement\nintention.\nWe further propose a new visualization method applied\nto EEG named Class Activation Topography (CAT). EEG\nTopography is drawn on the normalized data multiplied by\nthe normalized CAM. From the third row ofFig. 8,m o s to f\nthe EEG data weighted by CAM focus on the area of the motor\ncortex, consistent with the paradigm of motor imagery [46].\nFurthermore, the raw EEG and CAT of imagining left-hand\nmovement and right-hand movement are plotted in Fig. 9.\nWe are surprised to ﬁnd event-related desynchronization\n(ERD) and event-related synchronization (ERS) phenomenon.\nObvious contralateral activation and ipsilateral inhibition are\nobserved in the CAT of several subjects, such as the ﬁrst\nand eighth one, compared with the irregular raw EEG\ntopography.\n718 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nFig. 10. CAM and CA T of the model with only 1 head in the self-attention module. The activation is close to that inFig. 8with 10 heads.\nV. DISCUSSION\nThe practicality of BCI systems depends on the performance\nof the decoding method. We propose a very concise but\neffective method named Conformer to combine the advantages\nof CNN and Transformer networks. Conformer is a lightweight\nsolution for EEG decoding without pre-training. It only\nemploys a few steps for preprocessing, including band-pass\nﬁltering and standardization, without depending heavily on\nspeciﬁc tasks. The convolution module with both temporal\nand spatial convolution layers pays attention to the low-level\nrepresentation, considering the local temporal features, while\nthe self-attention module further focuses on the long-term\ndependencies and captures the global temporal correlation.\nThus, the proposed method is capable of learning more\ndiscriminative representation compared with the existing\nCNN-based models.\nIn experiments, we can see that EEG Conformer achieves\nstate-of-the-art results on three datasets with different\nparadigms and data acquisitions. The ablation study presents\nthat Transformer module contributes signiﬁcantly to the\noverall model, and data augmentation helps improve training\nperformance. We also explore the effect of several key\nparameters on the model. The results show that the model\nis not sensitive to the depth and head number of the\nself-attention module. However, the kernel size of the\npooling layer reveals a noticeable effect, which suggests\nthat a large unit to apply attention can help to avoid the\ninterference of local noise. Detailed visualizations are used for\ninterpretability illustrations. The Transformer module provides\nbetter discrimination capability as the feature distribution\nshown by t-SNE. We also design a new visualization approach\nname CAT to discover the function of a layer in a model\nby combining EEG topography and CAM. The results\ndemonstrate that our model focuses on changes near the motor\ncortex with motor imagery data. Besides, ERD/ERS produced\nby the imagery of left and right hands is also clearly perceived.\nThe role of multi-heads in theself-attention module remains\nunclear, so we train the model with only 1 head for Dataset I,\nand plot CAM and CAT in Fig. 10. We can see that the\nactivation of the self-attention module is close to that inFig. 8\nwith 10 heads. The comparison indicates that both cases learn\nsimilar global features, resulting in similar decoding accuracy.\nThe slight difference in activation still needs to be addressed.\nThere are several more limitations. Firstly, we mainly\nvalidate oscillatory EEG data such as motor imagery and\nemotion, which lack stationary patterns as event-related\npotential (ERP) EEG data. Secondly, the parameter scale of\nthe current model is not small. For Dataset I, the parameters\nof the Conformer increase by 17.6% compared to removing\nthe self-attention module. These additional costs arise from the\nlinear transformation and feed-forward layer used to calculate\nglobal dependencies. Although we have conﬁrmed that the\ntime cost to train the model is acceptable for actual use,\nit is still an issue that can be improved. Besides, the fully-\nconnected classiﬁer contributes a large number of parameters.\nGlobal average pooling may be used as an alternative with\nlittle performance degradation. Third, the proposed method is\ntrained and validated on each individual, and cannot utilize\nuseful information from other subjects. We will apply this\nmodel in ERP and subject-independent tasks in the future.\nVI. C\nONCLUSION\nThis paper proposes a concise and efﬁcient EEG decoding\nmethod called Conformer. Transformer is incorporated into\nCNN to learn global dependencies in the temporal domain.\nRemarkable results are achieved on different EEG datasets\nwith detailed comparative experiments. The visualization also\nshows that our model locates key information that conforms to\nthe principles of the paradigm on a global level. Overall, our\nmodel yields good performance in promoting EEG decoding.\nR\nEFERENCES\n[1] J. Jin et al., “A novel classiﬁcation framework using the graph\nrepresentations of electroencephalogram for motor imagery based brain–\ncomputer interface,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 30,\npp. 20–29, 2022.\n[2] B. Liu, X. Chen, N. Shi, Y . Wang, S. Gao, and X. Gao, “Improving the\nperformance of individually calibrated SSVEP-BCI by task-discriminant\ncomponent analysis,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 29,\npp. 1998–2007, 2021.\n[3] J. W. Li et al., “Single-channel selection for EEG-based emotion\nrecognition using brain rhythm sequencing,” IEEE J. Biomed. Health\nInformat., vol. 26, no. 6, pp. 2493–2503, Jun. 2022.\n[4] S. He et al., “EEG- and EOG-based asynchronous hybrid BCI: A system\nintegrating a speller, a web browser, an E-mail client, and a ﬁle explorer,”\nIEEE Trans. Neural Syst. Rehabil. Eng., vol. 28, no. 2, pp. 519–530,\nFeb. 2020.\n[5] K.-T. Kim, H.-I. Suk, and S.-W. Lee, “Commanding a brain-controlled\nwheelchair using steady-state somatosensory evoked potentials,”IEEE\nTrans. Neural Syst. Rehabil. Eng. , vol. 26, no. 3, pp. 654–665,\nMar. 2018.\n[6] Y . Song, S. Cai, L. Yang, G. Li, W. Wu, and L. Xie, “A practical EEG-\nbased human-machine interface to online control an upper-limb assist\nrobot,” Frontiers Neurorobot., vol. 14, p. 32, Jul. 2020.\n[7] B.-Y . Tsai, S. V . S. Diddi, L.-W. Ko, S.-J. Wang, C.-Y . Chang, and\nT.-P. Jung, “Development of an adaptive artifact subspace reconstruction\nbased on Hebbian/anti-Hebbian learning networks for enhancing BCI\nperformance,” IEEE Trans. Neural Netw. Learn. Syst., early access,\nJun. 17, 2022, doi:10.1109/TNNLS.2022.3174528.\n[8] K. K. Ang, Z. Y . Chin, C. Wang, C. Guan, and H. Zhang, “Filter bank\ncommon spatial pattern algorithm on BCI competition IV datasets 2a\nand 2b,”Frontiers Neurosci., vol. 6, no. 1, p. 39, 2012.\nSONG et al.: EEG CONFORMER: CONVOLUTIONAL TRANSFORMER FOR EEG DECODING AND VISUALIZATION 719\n[9] X. Chen, Y . Wang, S. Gao, T.-P. Jung, and X. Gao, “Filter bank canonical\ncorrelation analysis for implementing a high-speed SSVEP-based brain–\ncomputer interface,” J. Neural Eng. , vol. 12, no. 4, Aug. 2015,\nArt. no. 046008.\n[10] C. Ieracitano, N. Mammone, A. Hussain, and F. C. Morabito,\n“A novel multi-modal machine learning based approach for automatic\nclassiﬁcation of EEG recordings in dementia,”Neural Netw., vol. 123,\npp. 176–190, Mar. 2020.\n[11] A. Bhattacharyya, L. Singh, and R. B. Pachori, “Fourier–Bessel\nseries expansion based empirical wavelet transform for analysis of\nnon-stationary signals,” Digit. Signal Process., vol. 78, pp. 185–196,\nJul. 2018.\n[12] A. Bhattacharyya and R. B. Pachori, “A multivariate approach\nfor patient-speciﬁc EEG seizure detection using empirical wavelet\ntransform,” IEEE Trans. Biomed. Eng., vol. 64, no. 9, pp. 2003–2015,\nSep. 2017.\n[13] X. Wu, W.-L. Zheng, Z. Li, and B.-L. Lu, “Investigating EEG-based\nfunctional connectivity patterns for multimodal emotion recognition,”\nJ. Neural Eng., vol. 19, no. 1, Feb. 2022, Art. no. 016012.\n[14] H. Göksu, “BCI oriented EEG analysis using log energy entropy of\nwavelet packets,”Biomed. Signal Process. Control, vol. 44, pp. 101–109,\nJul. 2018.\n[15] K. He, X. Zhang, S. Ren, and J . Sun, “Deep residual learning for\nimage recognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 770–778.\n[16] R. T. Schirrmeister et al., “Deep learning with convolutional neural\nnetworks for EEG decoding and visua lization: Convolutional neural\nnetworks in EEG analysis,” Hum. Brain Mapping, vol. 38, no. 11,\npp. 5391–5420, Nov. 2017.\n[17] V . J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung,\nand B. J. Lance, “EEGNet: A compact convolutional neural network for\nEEG-based brain–computer interfaces,” J. Neural Eng., vol. 15, no. 5,\nOct. 2018, Art. no. 056013.\n[18] S. Tortora, S. Ghidoni, C. Chi sari, S. Micera, and F. Artoni,\n“Deep learning-based BCI for gait decoding from EEG with LSTM\nrecurrent neural network,” J. Neural Eng., vol. 17, no. 4, Jul. 2020,\nArt. no. 046011.\n[19] A. Shoeibi et al., “Automatic diagnosis of schizophrenia in EEG signals\nusing CNN-LSTM models,”Frontiers Neuroinform., vol. 15, Nov. 2021,\nArt. no. 777977.\n[20] A. Vaswani et al., “Attention is all you need,” inProc. Adv. Neural Inf.\nProcess. Syst., vol. 30. Red Hook, NY , USA: Curran Associates, 2017,\npp. 1–11.\n[21] J. Xie et al., “A transformer-based approach combining deep learning\nnetwork and spatial–temporal information for raw EEG classiﬁcation,”\nIEEE Trans. Neural Syst. Rehabil. Eng., vol. 30, pp. 2126–2136, 2022.\n[22] Y . Song, X. Jia, L. Yang, and L. Xie, “Transformer-based spatial–\ntemporal feature learning for EEG decoding,” Jun. 2021. [Online].\nAvailable: https://arxiv.org/abs/2106.11170\n[23] S. Bagchi and D. R. Bathula, “EEG-ConvTransformer for single-trial\nEEG-based visual stimulus classiﬁcation,” Pattern Recognit., vol. 129,\nSep. 2022, Art. no. 108757.\n[24] Y . Zheng, X. Zhao, and L. Yao, “Copula-based transformer in EEG to\nassess visual discomfort induced by stereoscopic 3D,”Biomed. Signal\nProcess. Control, vol. 77, Aug. 2022, Art. no. 103803.\n[25] F. Lotte et al., “A review of classiﬁcation algorithms for EEG-based\nbrain–computer interfaces: A 10 year update,”J. Neural Eng., vol. 15,\nno. 3, Jun. 2018, Art. no. 031005.\n[26] P. V . and A. Bhattacharyya, “Human emotion recognition based on time–\nfrequency analysis of multivariate EEG signal,” Knowl.-Based Syst.,\nvol. 238, Feb. 2022, Art. no. 107867.\n[27] A. Bhattacharyya, R. K. Tripathy, L. Garg, and R. B. Pachori,\n“A novel multivariate-multiscale approach for computing EEG spectral\nand temporal complexity for human emotion recognition,”IEEE Sensors\nJ., vol. 21, no. 3, pp. 3579–3591, Feb. 2021.\n[28] S. Sakhavi, C. Guan, and S. Yan, “Learning temporal information for\nbrain–computer interface using convolutional neural networks,” IEEE\nTrans. Neural Netw. Learn. Syst. , vol. 29, no. 11, pp. 5619–5629,\nNov. 2018.\n[29] X. Shan, J. Cao, S. Huo, L. Chen, P. G. Sarrigiannis, and Y .\nZhao, “Spatial–temporal graph convolutional network for Alzheimer\nclassiﬁcation based on brain functional connectivity imaging of\nelectroencephalogram,” Hum. Brain Mapping , vol. 43, no. 17,\npp. 5194–5209, Jun. 2022.\n[30] X. Hong et al., “Dynamic joint domain adaptation network for motor\nimagery classiﬁcation,”IEEE Trans. Neural Syst. Rehabil. Eng., vol. 29,\npp. 556–565, 2021.\n[31] W. Huang, W. Chang, G. Yan, Z. Yang, H. Luo, and H. Pei, “EEG-based\nmotor imagery classiﬁcation using convolutional neural networks with\nlocal reparameterization trick,”Expert Syst. Appl., vol. 187, Jan. 2022,\nArt. no. 115968.\n[32] A. M. Roy, “An efﬁcient multi-scale CNN model with intrinsic\nfeature integration for motor imagery EEG subject classiﬁcation in\nbrain-machine interfaces,” Biomed. Signal Process. Control, vol. 74,\nApr. 2022, Art. no. 103496.\n[33] A. Dosovitskiy et al., “An image is worth 16×16 words: Transformers\nfor image recognition at scale,” inProc. Int. Conf. Learn. Represent.,\nMar. 2022, pp. 1–21.\n[34] D. Kostas, S. Aroca-Ouellette, and F. Rudzicz, “BENDR: Using\ntransformers and a contrastive self-supervised learning task to learn\nfrom massive amounts of EEG data,”Frontiers Hum. Neurosci., vol. 15,\np. 253, Jun. 2021.\n[35] J. Liu, L. Zhang, H. Wu, and H. Zhao, “Transformers for EEG emotion\nrecognition,” Oct. 2021. [Online]. Available: https://arxiv.org/abs/\n2110.06553\n[36] W.-L. Zheng and B.-L. Lu, “Investigating critical frequency bands\nand channels for EEG-based emotion recognition with deep neural\nnetworks,” IEEE Trans. Auton. Mental Develop. , vol. 7, no. 3,\npp. 162–175, Sep. 2015.\n[37] F. Lotte, “Signal processing approaches to minimize or suppress\ncalibration time in oscillatory activity-based brain–computer interfaces,”\nProc. IEEE, vol. 103, no. 6, pp. 871–890, Jun. 2015.\n[38] R. Mane et al., “FBCNet: A multi-view convolutional neural\nnetwork for brain–computer interface,” Mar. 2021. [Online]. Available:\nhttps://arxiv.org/abs/2104.01233\n[39] H. Zhao, Q. Zheng, K. Ma, H. Li, and Y . Zheng, “Deep representation-\nbased domain adaptation for nonstationary EEG classiﬁcation,” IEEE\nTrans. Neural Netw. Learn. Syst., vol. 32, no. 2, pp. 535–545, Feb. 2021.\n[40] W.-L. Zheng, J.-Y . Zhu, and B.-L. Lu, “Identifying stable patterns over\ntime for emotion recognition from EEG,”IEEE Trans. Affect. Comput.,\nvol. 10, no. 3, pp. 417–429, Jul. 2019.\n[41] T. Song, W. Zheng, P. Song, and Z. Cui, “EEG emotion recognition using\ndynamical graph convolutional neural networks,” IEEE Trans. Affect.\nComput., vol. 11, no. 3, pp. 532–541, Jul. 2020.\n[42] Y . Li, W. Zheng, L. Wang, Y . Zong, and Z. Cui, “From regional to global\nbrain: A novel hierarchical spatial–temporal neural network model for\nEEG emotion recognition,”IEEE Trans. Affect. Comput., vol. 13, no. 2,\npp. 568–578, Apr. 2022.\n[43] P. Zhong, D. Wang, and C. Miao, “EEG-based emotion recognition\nusing regularized graph neural networks,”IEEE Trans. Affect. Comput.,\nvol. 13, no. 3, pp. 1290–1301, Jul. 2022.\n[44] L. van der Maaten and G. Hinton, “Visualizing data using t-SNE,”\nJ. Mach. Learn. Res., vol. 9, pp. 2579–2605, Nov. 2008.\n[45] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\nD. Batra, “Grad-CAM: Visual explanations from deep networks via\ngradient-based localization,” in Proc. IEEE Int. Conf. Comput. Vis.\n(ICCV), Oct. 2017, pp. 618–626.\n[46] A. Schnitzler, S. Salenius, R. Salmelin, V . Jousmäki, and\nR. Hari, “Involvement of primary motor cortex in motor imagery:\nA neuromagnetic study,” NeuroImage, vol. 6, no. 3, pp. 201–208,\nOct. 1997.",
  "topic": "Electroencephalography",
  "concepts": [
    {
      "name": "Electroencephalography",
      "score": 0.805077314376831
    },
    {
      "name": "Decoding methods",
      "score": 0.618667483329773
    },
    {
      "name": "Transformer",
      "score": 0.5464545488357544
    },
    {
      "name": "Computer science",
      "score": 0.4918239116668701
    },
    {
      "name": "Visualization",
      "score": 0.4504741132259369
    },
    {
      "name": "Speech recognition",
      "score": 0.3977460563182831
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36454445123672485
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.346312016248703
    },
    {
      "name": "Psychology",
      "score": 0.23823004961013794
    },
    {
      "name": "Neuroscience",
      "score": 0.20869094133377075
    },
    {
      "name": "Engineering",
      "score": 0.14244845509529114
    },
    {
      "name": "Electrical engineering",
      "score": 0.10655850172042847
    },
    {
      "name": "Algorithm",
      "score": 0.0969061553478241
    },
    {
      "name": "Voltage",
      "score": 0.07065567374229431
    }
  ]
}