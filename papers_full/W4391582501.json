{
    "title": "Lightweight SAR Ship Detection Network Based on Transformer and Feature Enhancement",
    "url": "https://openalex.org/W4391582501",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5031780181",
            "name": "Shichuang Zhou",
            "affiliations": [
                "Inner Mongolia University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A1994445760",
            "name": "Ming Zhang",
            "affiliations": [
                "Inner Mongolia University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2100092354",
            "name": "Liang Wu",
            "affiliations": [
                "Shandong First Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A2558872377",
            "name": "Dahua Yu",
            "affiliations": [
                "Inner Mongolia University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2102724350",
            "name": "Jianjun Li",
            "affiliations": [
                "Inner Mongolia University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2024423577",
            "name": "Fei Fan",
            "affiliations": [
                "Inner Mongolia University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2115714320",
            "name": "Liyun Zhang",
            "affiliations": [
                "Inner Mongolia University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A1983143503",
            "name": "Yang Liu",
            "affiliations": [
                "Inner Mongolia University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4214897468",
        "https://openalex.org/W4221161331",
        "https://openalex.org/W3130337729",
        "https://openalex.org/W4313886592",
        "https://openalex.org/W4387623853",
        "https://openalex.org/W4378714546",
        "https://openalex.org/W4367281842",
        "https://openalex.org/W1982095312",
        "https://openalex.org/W2044531722",
        "https://openalex.org/W3135487395",
        "https://openalex.org/W2058777458",
        "https://openalex.org/W6620707391",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W2102605133",
        "https://openalex.org/W2193145675",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W3034971973",
        "https://openalex.org/W3180954604",
        "https://openalex.org/W4313413662",
        "https://openalex.org/W4323823273",
        "https://openalex.org/W3032837604",
        "https://openalex.org/W3129289673",
        "https://openalex.org/W3136239039",
        "https://openalex.org/W3206302652",
        "https://openalex.org/W4283795270",
        "https://openalex.org/W4285265737",
        "https://openalex.org/W4379983825",
        "https://openalex.org/W4214571488",
        "https://openalex.org/W4212781487",
        "https://openalex.org/W6798838024",
        "https://openalex.org/W3035414587",
        "https://openalex.org/W3034552520",
        "https://openalex.org/W2990763144",
        "https://openalex.org/W4362500735",
        "https://openalex.org/W6838547440",
        "https://openalex.org/W2928007866",
        "https://openalex.org/W4386076325",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W6777046832",
        "https://openalex.org/W4311080910",
        "https://openalex.org/W3106250896",
        "https://openalex.org/W4281790833"
    ],
    "abstract": "With the rapid development of synthetic aperture radar (SAR) technology, SAR remote sensing has a wide range of applications in fields, such as marine surveillance and sea rescue. Currently, the SAR ship detection model based on deep learning suffers from the problems of low detection in real time and low detection accuracy. In order to solve the abovementioned problems, this article proposes a lightweight SAR ship detection network (EGTB-Net) based on transformer and feature enhancement. First, we design a novel Ghost-ECA model as the backbone network of EGTB-Net, which reduces the number of parameters of the model and enhances the ability to identify key feature information at the same time. Then, we incorporate the transformer block in the backbone network to capture long-range dependencies, enrich contextual information, and improve the network&#x0027;s ability to capture different types of local information. Finally, we adopt a new SIoU loss function, which is used to solve the direction problem of mismatch between the real frame and the predicted frame and improve the network&#x0027;s ability to localize ship targets. The experimental results on the SAR-ship-dataset show that the mean average precision of the method is 94.83&#x0025;, the detection speed is 61 frames per second, and the model size is only 5.94 M, while the model has excellent anti-interference ability.",
    "full_text": "JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 1\nLightweight SAR Ship Detection Network Based on\nTransformer and Feature Enhancement\nShichuang Zhou, Ming Zhang, Liang Wu, Dahua Yu, Jianjun Li, Fei Fan, Liyun Zhang and Yang Liu\nAbstract—With the rapid development of synthetic aperture\nradar (SAR) technology, SAR remote sensing has a wide range\nof applications in fields such as marine surveillance and sea\nrescue. Currently, the SAR ship detection model based on\ndeep learning suffers from the problems of low detection in\nreal-time and low detection accuracy. In order to solve the\nabove problems, this paper proposes a lightweight SAR ship\ndetection network (EGTB-Net) based on transformer and feature\nenhancement. Firstly, we design a novel Ghost-ECA model as the\nbackbone network of EGTB-Net, which reduces the number of\nparameters of the model and enhances the ability to identify key\nfeature information at the same time. Then, we incorporate the\ntransformer block in the backbone network to capture long-range\ndependencies, enrich contextual information, and improve the\nnetwork’s ability to capture different types of local information.\nFinally, we adopt a new SIoU loss function, which is used to solve\nthe direction problem of mismatch between the real frame and\nthe predicted frame and improve the network’s ability to localize\nship targets. The experimental results on the SAR-Ship-Dataset\nshow that the mean average precision (mAP) of the method is\n94.83%, the detection speed is 61 FPS, and the model size is only\n5.94 M, while the model has excellent anti-interference ability.\nIndex Terms—synthetic aperture radar (SAR), ship detection,\nfeature enhancement, complex background, lightweight network.\nI. I NTRODUCTION\nS\nAR is an all-day, all-weather active microwave remote\nsensing imaging radar with certain surface penetration\ncapabilities. SAR is not limited by the variability of ocean\nclimate and can provide all-round real-time monitoring of mili-\ntary targets at sea. In recent years, with the rapid development\nof satellite-based SAR technology, SAR remote sensing has\nbeen widely used in geological exploration, natural disaster\nmonitoring, sea ice classification, marine ship monitoring [1]–\n[5], etc. Accurate identification of ship targets is beneficial\nto commanders in obtaining military intelligence, adjusting\nfirepower deployment, and improving naval defense early\nwarning capability [6], [7].\nThis work was supported in part by the National Natural Science Foun-\ndation of China under Grant 62066036 and Grant 82260359, in part by the\nNatural Science Foundation of Inner Mongolia Autonomous Region under\nGrant 2022LHMS06005, in part by the Central Government Guides Local\nScience and Technology Development Fund Project of China under Grant\n2023ZY0017, and in part by the Fundamental Research Funds for Inner\nMongolia University of Science & Technology under Grant 2023QNJS195.\n(Corresponding author: Ming Zhang)\nShichuang Zhou, Ming Zhang, Dahua Yu, Jianjun Li, Fei Fan, Liyun Zhang\nand Yang Liu are with the School of Digital and Intelligence Industry, Inner\nMongolia University of Science and Technology, Baotou 014010, China. (e-\nmail: nkd zm@imust.edu.cn)\nLiang Wu is with the School of Radiology, Shandong First Medical\nUniversity & Shandong Academy of Medical Sciences, Tai’ an 271016, China.\nThe detection and recognition of maritime ship targets\nbased on SAR images has been one of the most challenging\nproblems in the field of remote sensing imagery. At present,\nSAR image ship detection mainly includes two categories: the\ntraditional detection method, represented by the constant false\nalarm rate (CFAR), and the detection algorithm based on deep\nlearning. The CFAR [8] algorithm uses the background units\naround the target, selects a constant false alarm probability to\ndetermine the size of the threshold, and finally detects based\non the threshold. These algorithms are easy to implement,\nhave low complexity, and are widely used as they have better\ndetection results under simple background clutter. Hou et al.\n[9] proposed a novel high-resolution synthetic aperture radar\nimage ship detection method based on multilayer CFAR that\npreserves more information about ship target features by using\nlognormal distribution and sliding windows. Li et al. [10] pro-\nposed a super pixel-level CFAR detector with the core idea of\nusing a non-local super pixel topology method, which has good\ndetection performance in dense areas of ships. Zhao et al. [11]\nproposed an adaptive CFAR detector that solves the problem\nof difficult detection of local k-distribution CFAR detectors\nunder interference conditions. However, these aforementioned\nCFAR improvement algorithms do not make full use of the\nfeature information in the image, have low detection accuracy,\npoor anti-interference capability, and are hardly adaptable to\nthe complex and changing marine environment.\nWith the rise of artificial intelligence and big data, convo-\nlutional neural networks have gradually replaced traditional\ndetection methods by virtue of their powerful feature ex-\ntraction capability and unique advantages. The existing deep\nlearning-based ship detection methods mainly include two-\nstage detection networks represented by Faster R-CNN [12],\nMask R-CNN [13], and R-CNN [14]. The input image for\nthis class of algorithms first goes through a candidate box\ngeneration network. Then it goes through the classification\nnetwork. Finally, the target classification and regression are\nperformed on the region within the candidate box. They\nhave high detection accuracy, but their network structure is\ncomplex and has a slow detection speed, which cannot meet\nthe task requirements of ship detection. In addition, there\nare one-stage detection networks, mainly represented by SSD\n[15], RetinaNet [16], EfficientDet [17], and YOLO series\nalgorithms. The algorithm directly extracts features from the\nnetwork to predict the object category, location information,\nand output detection results. Therefore, this type of algorithm\noperates efficiently but has low detection accuracy.\nWith the increasing resolution of SAR images, a large\nnumber of SAR ship detection methods based on deep learning\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 2\nhave emerged. Zhang et al. [18] proposed a novel four-feature\npyramid network (Quad-FPN), which solves the problem of\nfeature discrepancy in multi-scale ships with superior detection\nperformance by designing four unique FPN structures. Lv et\nal. [19] proposed an anchorless detection algorithm (SRDet)\nfor SAR ship targets based on deep saliency representation.\nThe method designs a new lightweight backbone network\n(LWBackbone) and a new hybrid-domain attentional mecha-\nnism (CNAM), which better balances the model’s detection\naccuracy and operational efficiency in complex scenarios.\nZhang et al. [20] proposed a ship detection network combining\nYOLOv4 and RFB (CY-RFB), which achieved good detec-\ntion accuracy for SAR ship detection in complex scenarios.\nZhao et al. [21] proposed a novel attention perception field\npyramid network (ARPN), which utilized the hybrid attention\nmechanism module (CBAM) and the receptive field module\n(RFB) to improve the detection performance of multi-scale\nships. Wang et al. [22] proposed a CenterNet-based docked\nship target detection method (SGE-CenterNet), which extracts\nmore semantic features for docked SAR ship target detection\nby incorporating the SGE attention module. Li et al. [23] pro-\nposed a novel multidimensional domain deep learning network\nfor SAR ship detection using the polar Fourier transform. The\nrotation-invariant features of SAR ship targets were obtained\nin the frequency domain using a polar Fourier transform\napproach, which enhanced the extraction capability of multi-\nscale and rotating ship targets in complex backgrounds.\nThe detection performance of the aforementioned fully\nsupervised methods depends heavily on the labeling of ship\ntargets in the dataset, and the labeling cost is again expensive.\nIn recent years, the use of semi-supervised learning (SSL)\nmethods for ship detection in SAR images has received much\nattention. Wang et al. [24] proposed a semi-supervised target\ndetection framework that effectively improves the detection\nperformance of the network by using consistency enhancement\nand labeling propagation methods. El Rai et al. [25] proposed a\nsemi-supervised synthetic aperture radar image segmentation\nalgorithm based on GSP (SegSAR), which solves the prob-\nlems of high acquisition cost and low accuracy in labeled\nscenes. Chen et al. [26] propose a novel unsupervised domain-\nadaptive YOLOv5 detection model that effectively solves\nthe problem of domain drift. Zhou et al. [27] enhance the\nmodel’s generalization ability by introducing an interference\nconsistency learning mechanism. In addition, they designed a\npseudo-tagged calibration network to solve the problem of low\ndetection accuracy in complex near-shore scenes. The above\nmethods reduce the reliance on labeled data and save economic\ncosts. However, using a large amount of unlabeled data reduces\nthe detection performance of the network and increases the\ncomplexity of the model structure.\nAlthough the above-improved algorithms improve the de-\ntection performance of ship targets to a certain extent, these\nnetwork models have a more complex structure and a larger\nnumber of parameters, which are easily limited by the arith-\nmetic power and storage space of the sea surface detection\nequipment. At the same time, the existing detection models are\nsusceptible to the effects of sea clutter and coherent speckle\nnoise in complex environments, with poor detection accuracy\nand a high false alarm probability. Therefore, balancing the\ndetection accuracy of the model and the model size becomes\nthe main problem currently faced. Liu et al. [28] proposed\na light ship detection network based on the YOLOv4-LITE\nmodel, using MobileNetv2 as the backbone feature extraction\nnetwork of the model. By incorporating the RFB module, the\ndetection accuracy of multi-scale ship targets was improved.\nThe improved lightweight network has a parametric count of\n49.34 M and high detection accuracy and speed. Xu et al. [29]\nproposed a lightweight SAR ship detector (Lite-YOLOv5). It\ndesigns a lightweight L-CSP module to reduce computation.\nBy combining the channel and spatial attention (CSA) models\nto focus on the region of interest, it improves the feature\nextraction capability of the network on the target.\nBased on the above analyses, ship detection in complex\nenvironments is susceptible to interference from external infor-\nmation such as islands and noise. Meanwhile, the limited stor-\nage space of the detection equipment is considered. Therefore,\nbased on YOLOX-S [30], this paper proposes a lightweight\nSAR ship detection network by combining the lightweight\nfeature extraction module Ghost-ECA, the transformer block,\nand the novel SIoU loss function, which ensures the detection\naccuracy and realizes the lightweight network at the same time.\nThe main contributions of this paper are as follows:\n1) A lightweight feature extraction module called Ghost-\nECA is designed, which is conducive to the efficient\nextraction of local features of ships and enhances the\nnetwork’s ability to represent ship targets. Among them,\nthe Ghost module utilizes inexpensive linear operations\nto deal with the redundant information of the feature\nmap, which effectively reduces the complexity of the\nmodel computation. The ECA module effectively en-\nhances the network’s ability to focus on the important\nfeatures of the ship target by adaptively adjusting the\nchannel weights of the input features.\n2) The transformer block is introduced to effectively reduce\nthe interference caused by land background and coherent\nspeckle noise in the detection process. The module\ncaptures global and contextual information about the\ntarget and enhances the network’s potential to explore\nthe target’s feature representation, helping to reduce\nfalse alarms and improve the accuracy of ship target\ndetection.\n3) The SIoU loss function is introduced to effectively solve\nthe direction problem of mismatch between the true and\npredicted frames. This loss function takes into account\nthe angle of the vectors between the desired regressions,\nredefines the angle penalty metric so that the regression\ndirection of the prediction frame is closer to the real\nframe, and improves the network’s localization accuracy\nfor ship targets.\nThe rest of the paper is structured as follows: Section\nII describes in detail the overall structure of the proposed\nmodel. Section III describes the implementation details of\nthe experiment. In Section IV, the paper presents a detailed\ndiscussion based on all the experimental results. Section V\nsummarizes the whole paper.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 3\nFig. 1. The overall structure of the EGTB-Net model. It mainly includes three parts: 1) the feature extraction part with Focus, Ghost-ECA, and transformer\nblocks as the backbone; 2) the neck part composed of a feature fusion network; and 3) the detection head.\nII. M ETHODOLOGY\nIn this section, in order to better balance the relationship\nbetween detection accuracy and detection speed of the network\nin complex scenes, we proposed a lightweight SAR ship\ndetection model (EGTB-Net). First, we introduce the overall\nstructure of the EGTB-Net model, and then we describe in\ndetail the main structures such as the Ghost-ECA model,\nGhost module, ECA module, and transformer block. Finally,\nwe describe the loss functions used in this paper.\nA. The overall structure of EGTB-Net\nThe network structure of EGTB-Net is shown in Fig.\n1. In order to better save computer resources and memory\nconsumption while achieving efficient extraction of feature\nmaps by the network. Therefore, we chose the lightweight\nnetwork GhostNet [31] as the backbone network of EGTB-\nNet, which generates more feature maps using some inexpen-\nsive operations to obtain better detection results with fewer\nparameters and faster speed. In addition to this, we add the\nECA channel attention module [32] to the GhostNet bottleneck\nstructure, which improves the detection accuracy of the model\nby enabling cross-channel information interaction. We then\nfuse the transformer block after the high-level feature map\noutput by the backbone network, which is used to capture\nlong-range dependencies and enhance the feature representa-\ntion of the ship target. Next, the three feature layers output\nby the backbone network are sent to the neck network for\nfurther feature fusion. Finally, the regression and classification\ntasks are completed by the decoupled head to output the final\ndetection results. In the meantime, a novel SIoU loss function\nis used instead of the CIoU loss function [33] to improve the\naccuracy of the model in locating the ship targets.\nB. Ghost-ECA Model\nDue to the large number of parameters and computational\neffort of the YOLOX-S backbone network model, as well as\nthe poor feature extraction for ship targets, we have designed\na lightweight feature extraction module (Ghost-ECA). The\nstructure of the Ghost-ECA module is shown in Fig. 2. The\nmodule combines the advantages of both the Ghost bottleneck\nand the ECA channel attention module. Ghost Bottleneck is a\nlightweight plug-and-play module that consists of two Ghost\nmodules, where the Ghost module utilizes inexpensive linear\noperations to handle redundant information in the feature map,\nreducing the number of parameters and computation of the\nmodel. The ECA module is a simple and efficient channel\nattention module that avoids channel dimensionality reduc-\ntion and effectively captures information interactions across\nchannels. We place the ECA module in the middle of two\nGhost modules to both ensure high detection performance of\nthe network model and significantly reduce the computational\ncost of the model. Fig. 2(a) shows the structure of the Ghost-\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 4\nFig. 2. Structure of the Ghost-ECA.\nECA module with step size 1, and Fig. 2(b) shows the structure\nof the Ghost-ECA module with step size 2.\nC. Ghost module\nThe core idea of the Ghost module is to generate these\nredundant feature maps using less computationally intensive\n(cheap operations) operations. The structure of the Ghost\nmodule is illustrated in Fig. 3. Firstly, a smaller number of\nconvolutional kernels are used to generate the output feature\nmap, then a deep separable convolution is used to obtain a\nsimilar feature map, and finally the two are fused to produce\nthe final feature map. This not only significantly reduces the\nnumber of parameters and the number of operations in the\nmodel but also improves the detection speed of the network.\nNext, the normal convolution and the Ghost module are\ncompared in terms of the number of parameters and computa-\ntional effort. Suppose the size of the input image is H × W ×\nC, the size of the convolution kernel is k × k, and the size of\nthe output image is H′ × W′ × C′. The input image is obtained\nafter an ordinary convolution. The number of channels in the\nfeature layer is m, and the size of the convolution kernel for\nlinear operations is d × d, yielding n similar feature layers.\nWhere k × k and d × d are of the same size, and n<<C.\nThe equations for calculating the number of parameters\nusing the normal convolution and Ghost module are as follows:\nPconv = C ×C′ ×k ×k (1)\nPGhost = C ×m ×k ×k +m ×n ×d ×d (2)\nThe equations for the amount of computation required using\nnormal convolution and the Ghost module are as follows:\nGconv = C ×k ×k ×C′ ×H′ ×W′ (3)\nGGhost = H′ ×W′ ×C ×k ×k ×m +H′ ×W′ ×m ×d ×d ×n\n(4)\nFig. 3. Structure of the normal convolution and Ghost modules.\nTherefore, the equation for the ratio of the number of normal\nconvolution and Ghost module parameters is as follows:\nRp = C ×C′ ×k ×k\nC ×m ×k ×k +m ×n ×d ×d ≈ C′\nm (5)\nThus, the equation for the ratio of the computational effort\nof the normal convolution and Ghost modules is as follows:\nRG = C ×k ×k ×C′ ×H′ ×W′\nH′ ×W′ ×C ×k ×k ×m +H′ ×W′ ×m ×d ×d ×n ≈ C′\nm(6)\nFrom Eqs. (5) and (6), the ratio of the number of parameters\nand computational effort of the ordinary convolution to the\nGhost module is both C′/m. It can be concluded that the Ghost\nmodule has fewer parameters and computational effort com-\npared to the ordinary convolution. This also fully demonstrates\nthat the Ghost module is a more lightweight structure.\nD. ECA module\nIn SAR images, due to the small scale of ship targets and the\ninterference of land background and noise, the detection model\nstill cannot achieve satisfactory performance in the detection\nprocess. In this paper, although we adopt the GhostNet as\nthe backbone network to significantly reduce the number of\nparameters in the network, the detection performance of the\nmodel is still poor. Therefore, we add the ECA module to the\nGhost module to enhance the network’s ability to express the\nfeatures of ship targets.\nThe ECA is a lightweight channel attention module. The\nmodule proposes a local cross-channel interaction strategy that\ndoes not require dimensionality reduction, effectively avoiding\nthe impact of dimensionality reduction on learning channel\nattention. In addition, proper cross-channel interaction can\nsignificantly reduce the complexity of the model while main-\ntaining performance. In order for the network to learn more\nfeature information about ship targets, we add the ECA module\nbetween the two Ghost modules. It enhances the key features\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 5\nFig. 4. Structure of the ECA module.\nof ship targets by adaptively weighting the features in the\nchannel dimension, suppresses the interference of irrelevant\ninformation, and makes the feature map contain more effective\ninformation, thus improving the detection accuracy of SAR\nship targets.\nThe structure of the ECA module is shown in Fig. 4. Firstly,\nthe module performs feature extraction on the input image by\nglobal average pooling and 1D convolution to achieve cross-\nchannel information interaction. Then, the weights of each\nfeature point is obtained after the Sigmoid activation function\nand multiplied with the input feature layer to generate a new\nfeature map. Where GAP denotes global average pooling, σ\ndenotes the sigmoid activation function, ⊗ denotes the bit-by-\nbit dot product, and k denotes the convolution kernel size.\nThe ECA module avoids channel dimension reduction\nthrough the change of adaptive function k. It establishes a pos-\nitive proportional mapping relationship between the channel\ndimension C and the convolution kernel size k, realizes cross-\nchannel information interaction, and improves the feature\nextraction capability. k is calculated as shown in Eq. (7):\nk = Ψ(C) =\n\f\f\f\f\nlog2 (C)\nγ + b\nγ\n\f\f\f\f\nodd\n(7)\nWhere |t|odd denotes the odd number closest to t. The values\nof γ and b are 2 and 1, respectively.\nE. Transformer block\nThe complexity and variability of the SAR image scene, as\nwell as the interference of coherent speckle noise around the\nship hull, lead to the low detection accuracy of the network\nfor ship targets. In order to further improve the feature extrac-\ntion capability of the backbone network, this paper adds the\ntransformer block to suppress the interference of background\nclutter and effectively improve the detection accuracy of ship\ntargets. This module is proposed by Vision Transformer [34] to\nutilize the self-attention mechanism for feature extraction. The\nstructure of the module is shown in Fig. 5. The transformer\nblock mainly includes Layer Normalization (LN), multi-head\nattention (MSA), and feed-forward neural networks (MLP).\nLN helps the network converge better and prevents the network\nfrom overfitting. MSA not only captures the dependencies\nbetween neighboring elements but also acquires the global\nfeatures of the image. MLP is a fully connected layer used\nfor image classification. The transformer block is computed\nas follows:\nz0 =\nh\nxclass,x1\npE;x2\npE;··· xN\np E\ni\n+Epos (8)\nFig. 5. Structure of the Transformer block.\nzl′ = MSA(LN(zl−1))+ zl−1, l = 1...L (9)\nzl = MLP\n\u0000\nLN\n\u0000\nz′\nl\n\u0001\u0001\n+z′\nl, l = 1...L (10)\ny = LN\n\u0000\nz0\nL\n\u0001\n(11)\nWhere z0 denotes the process of patch embedding and\npositional encoding, zl′ denotes the process of input feature\nmaps going through MSA and LN, repeated L times,zl denotes\nthe process of input feature maps going through MLP and LN,\nrepeated L times, and y denotes the final output.\nMSA is an extension structure of self-attention (SA) specific\nto the transformer block, which mainly uses the mechanism of\nself-attention to obtain the global features of the image. The\ncomputation process for MSA is as follows:\nMSA(z) = [SA1 (z);SA2 (z);··· ;SAk (z)]Umsa (12)\nSA uses the query vector to query the features of the\nsequence to obtain the importance of each part of the sequence,\nand then re-imposes the importance of each part of the\nsequence on the sequence. SA is calculated as follows:\n[q,k,v] =zUqkv Uqkv ∈ RD×3Dh (13)\nA = softmax\n\u0010\nqkT/\np\nDh\n\u0011\nA ∈ RN×N (14)\nSA(z) =Av (15)\nIn order to give full play to the advantages of both the vision\ntransformer and YOLOX and further improve the detection\nperformance of the network model in complex environments,\nwe add the transformer block at the end of the backbone\nnetwork. Because the output feature map size at the end of the\nbackbone network is 13 × 13, adding the block can reduce\ncertain computational costs and memory consumption [35].\nThe transformer block is able to capture long-range depen-\ndencies and enrich contextual information to help the network\nbetter focus on the important features of ship targets and\nreduce the influence of unfavorable factors such as background\nclutter. It also utilizes the self-attention mechanism to enhance\nthe potential of the network to explore the feature representa-\ntion of the target, effectively extract more feature information\nabout ship targets, and improve the detection accuracy of SAR\nship targets.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 6\nFig. 6. Schematic diagram of the angle cost.\nF . SIoU loss\nAs the CIoU loss function does not take into account the\ndirectional problem of mismatch between the real box and the\npredicted box, it leads to a slow convergence of the network.\nTherefore, this paper uses a new loss function, SIoU [36],\ninstead of CIoU loss. SIoU not only considers the angle cost\nbut also refines the distance cost and shape cost on the basis of\nCIoU, improving the accuracy of detection and training speed.\nThe angular cost is shown in Fig. 6.\nWhere B is the prediction box. BGT is the true box. It\nconverges to α if the angle between the two is less than π\n4 ;\notherwise, it converges to β. The angle cost is calculated as\nfollows:\n∧ = 1 −2 ∗sin2\n\u0010\narcsin(x)− π\n4\n\u0011\n(16)\nWhere:\nx = Ch\nσ = sin(α) (17)\nσ =\nq\u0000\nbgt\ncx −bcx\n\u00012\n+\n\u0000\nbgt\ncy −bcy\n\u00012\n(18)\nCh = max\n\u0010\nbgt\ncy ,bcy\n\u0011\n−min\n\u0010\nbgt\ncy ,bcy\n\u0011\n(19)\nDue to the introduction of the angle cost, the distance cost\nneeds to be redefined. Therefore, the distance cost is calculated\nas:\n∆ = ∑\nt=x,y\n\u0000\n1 −e−γρt\n\u0001\n(20)\nWhere:\nρx =\n\u0012bgt\ncx −bcx\ncw\n\u00132\n,ρy =\n \nbgt\ncy −bcy\nch\n!2\n(21)\nγ = 2 −∧ (22)\nWhere cw and ch are the width and height of the minimum\nbounding rectangle of the real and predicted boxes, respec-\ntively.\nThe shape cost is calculated using the formula:\nΩ = ∑\nt=w,h\n\u0000\n1 −e−ωt\n\u0001θ (23)\nFig. 7. Target scale distribution of SAR-Ship-Dataset. (a) distribution of the\nsize of the annotated boxes. (b) distribution of the aspect ratio of the annotated\nboxes.\nWhere:\nωw = |w −wgt|\nmax(w,wgt),ωh = |h −hgt|\nmax(h,hgt) (24)\nThe value of θ is a very significant term in the equation and\ncontrols the degree of attention paid to the shape cost. Finally,\nthe expression for the SIoU loss function is given by:\nLbox = 1 −IoU + ∆+Ω\n2 (25)\nWhere:\nIoU =\n\f\fB ∩BGT \f\f\n|B ∪BGT | (26)\nIII. E XPERIMENTS\nIn this section, we evaluate the detection performance of\nEGTB-Net using the SAR-Ship-Dataset. First, the experimen-\ntal dataset and environment configuration are described. Then,\nthe experimental evaluation metrics are described in detail.\nNext, the effectiveness of each of the proposed modules is\ndemonstrated through a series of ablation experiments. Finally,\nEGTB-Net is compared with some mainstream target detection\nalgorithms to demonstrate the significant advantages of our\napproach. Moreover, we tested the anti-interference capability\nof the model in different scenes.\nA. Dataset description\nThe SAR-Ship-Dataset [37] consists of 102 domestic\nGaofen-3 images and 108 Sentinel-1 images. The dataset\ncurrently contains 43819 ship slices, all of which are 256 ×\n256 in size. SAR ship images in the dataset have a resolution of\n3-25 m and polarization methods including HH, HV , VH, and\nVV . The distribution of the size of the annotated boxes in the\ndataset is shown in Fig. 7(a), and the distribution of the aspect\nratio of the annotated boxes and the number of corresponding\nships in the dataset is shown in Fig. 7(b). As can be seen\nfrom Fig. 7, the ship targets in the dataset show a multi-\nscale distribution, indicating that the SAR-Ship-Dataset is well\nsuited for evaluating the performance of SAR ship detection\nmodels. The experimental data in this paper are divided in the\nratio of 7:2:1 according to the training set, validation set, and\ntest set.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 7\nB. Experimental Setup\nAll experiments were conducted on the Windows 10 oper-\nating system using the Python 3.6 programming environment\nand TensorFlow 1.13.2 framework. The basic server config-\nuration was: Intel(R) Xeon(R) CPU E5-2650 v2 (2.60 GHz),\ngraphics card model NVDIA Tesla K40m with 12 GB of video\nmemory, and training was accelerated using CUDA10.1 and\nCUDNN.\nThe training process used the SGD optimizer, 200 epochs\nwere trained, batch size was set to 16, learning rate was\nadjusted by cosine annealing, initial learning rate was 0.001,\nweight decay was 0.0001, momentum size was 0.937, and IoU\nthreshold was set to 0.5. The size of the images before the\ninput network was uniformly adjusted to 416 × 416. Mosaic\ndata enhancement was used in the training.\nC. Evaluation Metrics\nIn order to better reflect the detection ability of the model,\nthis paper adopts seven indicators to objectively evaluate the\nmodel, mainly including precision (P), recall (R), F1, mean\naverage precision (mAP), parameters, GFLOPs, and frames\nper second (FPS).\nPrecision indicates the percentage of ships that were cor-\nrectly detected in all detection results. The calculation method\nis expressed as Eq. (27):\nPrecision = TP/(TP +FP) (27)\nWhere TP (true positive) indicates the number of ship\ntargets detected correctly and FP (false positive) indicates the\nnumber of ship targets detected incorrectly.\nRecall indicates the percentage of ships that were correctly\ndetected in the sample of all real ships. The calculation method\nis expressed as Eq. (28):\nRecall = TP/(TP +FN) (28)\nWhere FN (false negative) indicates the number of missed\nship targets.\nF1 is a combined evaluation metric of precision and recall\nthat integrates the detection capability of the model. The\ncalculation method is expressed as Eq. (29):\nF1 = 2 ×Precision ×Recall\nPrecision +Recall (29)\nThe mAP indicates the area enclosed by the P-R curve and\nthe coordinate axis; the larger the enclosed area, the better the\ndetection performance of the model. The calculation method\nis expressed as Eq. (30):\nmAP =\nZ 1\n0\nP(R)dR (30)\nFPS is used to indicate the speed at which the model detects\nimages. The calculation method is expressed as Eq. (31):\nFPS = 1/T (31)\nWhere T indicates the time taken by the network to detect\na SAR image.\nParams denote all the parameters contained in the network\nmodel. GFLOPs denotes the number of floating-point opera-\ntions of the model, which can be used as a measure of the\ncomplexity of the model.\nD. Effect of Ghost-ECA model\nTo verify the usefulness of the Ghost-ECA module, we\ndesigned a set of ablation experiments using the YOLOX-S\nnetwork as the baseline method, and the experimental results\nare shown in Table I. Method (a) indicates the baseline method.\nMethod (b) indicates the replacement of YOLOX’s backbone\nnetwork with the GhostNet network. Method (c) indicates\nthe addition of the ECA module to method (b). From the\nexperimental results in Table I, it can be seen that replacing the\nbackbone network of YOLOX with the GhostNet network can\nmake the model more lightweight. Compared with the baseline\nmethod, the number of parameters in the model is reduced by\n37.1%, and the operation efficiency is improved by 12 FPS. At\nthe same time, the accuracy metric mAP of the model shows\na small improvement compared with the baseline method. To\nfurther improve the detection accuracy of the model without\nadding additional parameters and complexity, we added the\nlightweight channel-focused ECA modules. Compared to the\nmethod (b) model, the mAP improved by 0.62% and the F1\nscore improved by 1%. However, the parameters of the model\nbarely increased significantly. The results show that replacing\nthe GhostNet network can significantly reduce the complexity\nof the model. The ECA module effectively improves the\ndetection model’s ability to extract features from the ship\ntargets, ultimately improving the model’s accuracy. The best\ndetection results were achieved by combining the GhostNet\nand ECA modules together. The effectiveness of the Ghost-\nECA module is thus demonstrated.\nE. Ablation Study\nIn order to further evaluate the effectiveness of each module\nproposed in this paper on the model, we designed a series\nof ablation experiments using method (a) (YOLOX-S) as the\nbenchmark model, and the experimental results are shown in\nTable II. In Table II, “-” indicates that the module is not\nadded to the benchmark model, and “ ✓” indicates that the\nmodule is added to the benchmark model. GEM denotes the\nGhost-ECA module, TFB denotes the transformer block, and\nSIoU denotes the SIoU loss function. From Table II, it can\nbe found that when only the Ghost-ECA module is added,\nthe parameters and the running efficiency of the model are\n5.64 M and 63 FPS, respectively, which drastically reduces\nthe number of parameters and the computation amount of\nthe model and has the best running efficiency. Meanwhile,\nthe mAP of the model was improved by 0.75%. In order\nto further improve the detection accuracy of method (b), the\ntransformer block is integrated into the backbone network,\nand the mean average precision (mAP) of method (c) is\nimproved by 0.59%, which illustrates that the module can be\nable to capture long-distance dependencies, enrich contextual\ninformation, and enhance the network’s ability to express the\nfeatures of the ship target. After replacing CIoU loss with\nthe SIoU loss function, the mAP value is improved by 0.13%,\nwhich indicates that the SIoU loss function is more accurate in\nlocating the ship target, thus improving the detection accuracy.\nThe mAP for method (d) was 94.83%, an improvement of\n1.47% over the baseline model. The experimental results show\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 8\nTABLE I\nVERIFYING THE USEFULNESS OF THE GHOST -ECA M ODULE , THE BEST\nRESULTS ARE HIGHLIGHTED IN BOLD\nMethod P(%) R(%) F1(%) mAP(%) Params(M) FPS\n(a) 91.65 91.25 91.00 93.36 8.96 51\n(b) 92.04 89.75 91.00 93.49 5.64 63\n(c) 91.54 91.88 92.00 94.11 5.64 63\nTABLE II\nRESULTS OF ABLATION EXPERIMENTS\nMethod GEM TFB SIoU mAP(%) Params(M) FPS\n(a) - - - 93.36 8.96 51\n(b) ✓ - - 94.11 5.64 63\n(c) ✓ ✓ - 94.70 5.94 61\n(d) ✓ ✓ ✓ 94.83 5.94 61\nTABLE III\nPERFORMANCE COMPARISON OF EGTB-N ET WITH YOLOX\nMethod P(%) R(%) F1(%) mAP(%) Params(M) FPS\nYOLOX 91.65 91.25 91.00 93.36 8.96 51\nEGTB-Net 92.78 91.84 92.00 94.83 5.94 61\nthat the proposed method substantially reduces the number of\nparameters and computation of the model while maintaining\nhigher accuracy.\nFurthermore, we visualized the P-R curves for the four\ngroups of ablation experimental results, and the visualization\nresults are shown in Fig. 8. From Fig. 8, it can be clearly seen\nthat the area enclosed by the P-R curve and the coordinate\naxes is also getting bigger when each module proposed in this\npaper is stacked in turn, among which the area enclosed by\nthe P-R curve is the biggest when three modules are stacked.\nIt can be concluded that each of the modules proposed in this\npaper can improve the detection accuracy of the model. It is\nalso verified that the detection performance of the model is\noptimal when three modules are added to the network at the\nsame time.\nF . Comparison with the latest SAR Ship Detection Methods\nTable III shows the quantitative results of the EGTB-Net\nmodel and the YOLOX-S model on the SAR-Ship-Dataset. In\nTable III, we can draw the following conclusions: On the one\nhand, from the analysis of model size and complexity metrics,\ncompared to the original YOLOX-S model, the EGTB-Net\nmodel has 33.7% fewer parameters and improves detection\nspeed by 10 FPS. Thus, it demonstrates that the EGTB-\nNet proposed in this paper is a lightweight model with high\ndetection efficiency. On the other hand, from the analysis of\ndetection accuracy metrics, the EGTB-Net improved precision\nby 1.13%, recall by 0.59%, F1 by 1%, and the most important\nmetric, mAP, by 1.47%. The effectiveness of the Ghost-ECA,\ntransformer block, and SIoU loss proposed in this paper is\nfully demonstrated. A comprehensive analysis of the above\nFig. 8. P-R curves of different methods.\nFig. 9. Comparison of heat map visualization results.\nshows that the EGTB-Net proposed in this paper not only\nensures the lightweightness of the model but also has high\ndetection accuracy.\nIn order to further show the superiority of the method in this\npaper, we selected four different scenarios in the SAR-Ship-\nDataset for comparing the heat map visualization results. As\nshown in Fig. 9, the first row represents the ground truth, the\nsecond row is the heat map visualization result of the YOLOX-\nS model, and the third row is the heat map visualization\nresult of the EGTB-Net model. From the analysis of the\ncomparison results, it can be seen that the YOLOX-S model\nhas a small number of missed detections and false alarms,\nand the network is weak in expressing information about the\nship’s target characteristics. However, the EGTB-Net model\nwas able to correctly detect ship targets in different scenarios\nwhile effectively enhancing the focus on ship target feature\ninformation. Analyzing the specific reasons, it can be seen that\nthe Ghost-ECA and transformer blocks effectively suppress\nthe interference of sea clutter and coherent speckle noise on\nthe detection process and enhance the expression of effective\nfeatures. Meanwhile, the SIoU loss function is used in the\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 9\nTABLE IV\nCOMPARISON WITH MAINSTREAM TARGET DETECTION MODELS\nMethod P(%) R(%) F1(%) mAP(%) Params(M) GFLOPs(G) FPS\nQuad-FPN [18] 77.55 96.10 - 94.39 - - 23\nRetinaNet [16] 91.78 83.11 87.00 92.90 36.38 68.81 29\nCY-RFB [20] 93.60 89.90 - 94.50 - - 51\nYOLOv7 [38] 94.20 84.83 89.00 93.81 37.24 104.76 29\nDETR [39] 72.69 93.19 82.00 92.04 36.74 114.22 23\nYOLOv4 [40] 90.45 67.92 78.00 86.52 64.00 59.76 35\nYOLOX-S [30] 91.65 91.25 91.00 93.36 8.96 26.64 51\nEGTB-Net 92.78 91.84 92.00 94.83 5.94 18.53 61\nmodel to enhance the network’s localization effect on ship\ntargets.\nIn order to evaluate the detection performance of the\nmethod in this paper, Table IV shows the experimental results\nof comparing the EGTB-Net model with seven mainstream\ntarget detection models on the SAR-Ship-Dataset, and we\nmainly selected Quad-FPN [18], RetinaNet [16], CY-RFB\n[20], YOLOv7 [38], DETR [39], YOLOv4 [40], and YOLOX-\nS [30] for comparison. Table IV shows that the most important\nmetrics of the EGTB-Net model, the mAP value of 94.83%\nand the F1 value of 92%, are higher than the next-best CY-\nRFB model. The number of parameters and computation of our\nmethod are 5.94 M and 18.53 G, which are 33.7% and 30.4%\nlower than YOLOX-S. Meanwhile, the detection efficiency\nof our method is 61 FPS, which has the highest detection\nefficiency compared with the other seven methods. Although\nQuad-FPN has the highest recall, it has the lowest detection\nefficiency of 23 FPS, and the rest of the metrics are lower\nthan the EGTB-Net model. YOLOv7, the latest version of\nthe YOLO series, has the highest precision of all detection\nmethods. Our method improves recall, F1, and mAP by 7%,\n3%, and 1%, respectively, compared to YOLOv7. In addition,\nYOLOv7 has a more complex network structure, and its\nnumber of parameters and computation amount are 37.24 M\nand 104.76 G, which are significantly higher than those of\nRetinaNet, YOLOX-S, and EGTB-Net. After comprehensively\ncomparing the different detection models, the proposed EGTB-\nNet model has the smallest number of parameters and compu-\ntation amount and the highest detection accuracy and detection\nefficiency. It is fully demonstrated that the EGTB-Net model\nhas excellent real-time detection performance.\nTo visualize the superiority of the method in this paper,\nwe trained RetinaNet, YOLOv7, YOLOX-S, and EGTB-Net\non the SAR-Ship-Dataset and then randomly selected several\nimages from the dataset for testing. The test results under dif-\nferent scenarios are shown in Fig. 10. Fig. 10(a)-(e) represent\nGround truth, RetinaNet, YOLOv7, YOLOX-S, and EGTB-\nNet, respectively. Where green rectangular boxes represent\ncorrectly detected targets, red ellipses represent incorrectly\ndetected targets, and yellow ellipses represent missed targets.\nFrom the analysis in Fig. 10(b)-(d), it can be seen that in\nthe near-coast scenario, the detection model is susceptible\nto interference from islands, land, and multiple noises when\nidentifying targets. In the far-coast scenario, due to the small\nsize of the ship target, some of the islands and the ship\ntarget features are relatively similar, and the detection model\nis unable to make accurate judgments, resulting in missed and\nwrong detections by the RetinaNet, YOLOv7, and YOLOX-S\nalgorithms. From the analysis in Fig. 10(e), it can be seen that\nEGTB-Net effectively suppresses the interference of complex\nenvironments and noise by adding Ghost-ECA and transformer\nblocks, enhances the network’s ability to represent the features\nof ship targets, and reduces the number of missed and false\nalarm targets in near and far coast cases. At the same time,\nby using the SIoU bounding box regression loss function, the\nnetwork improves the localization accuracy of ship targets.\nIn summary, the EGTB-Net detection model proposed in\nthis paper is able to correctly detect ship targets in near-\ncoast, strong interference conditions, and far-coast scenarios,\nindicating that the method in this paper has very excellent\ndetection performance.\nIn addition to subjective evaluation metrics, the P-R curve is\nalso an effective way of evaluating. If the area surrounded by\nthe curve and the coordinate axis is larger, then the detection\nperformance of the model is better. In order to verify the\neffectiveness of the proposed model, the EGTB-Net model and\nfive comparison models are visualized to show the results, and\nthe experimental results are shown in Fig. 11. From Fig. 11,\nit can be seen that the P-R curve of the EGTB-Net model is\nhigher than all the other curves, which indicates that the model\nin this paper has better extraction and discrimination abilities\nin ship detection and recognition.\nG. Evaluation in Different Scenes\nTable V shows the experimental results of this paper’s\nmethod and the four comparative methods on the near and far\ncoasts. In order to better reflect the anti-interference capability\nof the model, three more stringent metrics, mAP 50, mAP 75,\nand mAP 0.5:0.95, are used in this paper. mAP 50 denotes the\nmean average precision when IoU=0.5. mAP 75 denotes the\nmean average precision when IoU=0.75. mAP 0.5:0.95 denotes\nthe mean average precision when IoU=0.50:0.05:0.95. From\nthe experimental results in Table V, it can be seen that the\nYOLOv4 and DETR algorithms perform poorly for mAP 75\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 10\nFig. 10. Detection results of different detection models in typical scenes. (a) Ground truth. (b) RetinaNet. (c) YOLOv7. (d) YOLOX-S. (e) EGTB-Net.\nTABLE V\nEXPERIMENTAL RESULTS OF DIFFERENT DETECTION METHODS ON THE NEAR AND FAR COASTS\nMethod\nInshore Scenes Offshore Scenes\nmAP 50 mAP 75 mAP 0 . 5:0 . 95 mAP 50 mAP 75 mAP 0 . 5:0 . 95\nYOLOv4 [40] 0.656 0.253 0.298 0.918 0.424 0.455\nRetinaNet [16] 0.791 0.495 0.447 0.929 0.615 0.542\nYOLOX [30] 0.847 0.514 0.477 0.938 0.626 0.552\nDETR [39] 0.776 0.362 0.398 0.921 0.552 0.510\nOurs 0.898 0.689 0.570 0.970 0.794 0.659\nand mAP0.5:0.95 in the near-coast scenario due to the presence\nof land and harbor interference. However, the proposed method\nachieves 68.9% and 57.0% for mAP75 and mAP0.5:0.95, respec-\ntively, which are much higher than the other four algorithms.\nIn the far-coast scenario, although the mAP 50 of YOLOv4,\nRetinaNet, YOLOX, and DETR algorithms are all above 90%,\nthere is still a gap compared with the proposed method.\nComparing with several other algorithms, our method’s mAP75\nand mAP 0.5:0.95 also achieved the best results. In general,\nour method achieves the highest accuracy values for both the\nmore stringent metrics of mAP75 and mAP0.5:0.95 in both near-\ncoastal complex scenarios and far-coastal simple scenarios,\nwhich proves that our method has excellent detection perfor-\nmance in complex scenarios.\nIn order to intuitively illustrate that the EGTB-Net model\nhas a significant advantage in anti-interference capability in\ndifferent complex scenarios, we selected SAR images of three\ntypical scenarios for testing, namely, the strong interference\n Fig. 11. Visualization of P-R curve results for different detection models.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 11\nFig. 12. Detection results of the EGTB-Net model under different complex scenarios.\ncondition, the near-coastal complex scenario, and the far-\ncoastal multi-target scenario, and the detection results are\nshown in Fig. 12. From the detection results, it can be seen\nthat under strong interference conditions, the detection model\nmay have difficulty obtaining effective ship target feature\ninformation due to the effect of coherent speckle noise and\nthe scattering phenomenon occurring around the ship.\nHowever, our method still accurately detects all the ship\ntargets under the strongly interfering scenario in Fig. 12(a). In\nthe near-coastal complex scene 2 in Fig. 12(b), this paper’s\nmethod has one missed and incorrectly detected small ship\neach. Analyzing the specific reasons, it can be concluded that\nthe detection model may be difficult to distinguish between the\ntarget and the surrounding background, resulting in a certain\nnumber of missed detections and false alarms because the\nsurrounding islands and land background have similar features\nto the ship targets. For the far-coast multi-target scenario,\nour method can accurately locate all the ship targets. Taken\ntogether, the proposed method can show good adaptability\nand superior detection performance in a variety of complex\nmaritime scenarios.\nIV. D ISCUSSION\nIn complex and changing marine environments, ship de-\ntection near the coast has been a great challenge. The vi-\nsualization results in Fig. 12 show that our method has a\nsmall number of false alarms and missed targets when the\ncomplexity of the near-shore scene is high.\nThe reason for the high miss detection rate may be due to\nthe fact that near-shore ships are in close proximity to each\nother and the targets have multi-angle rotational behaviors,\nand the detection model is unable to correctly distinguish such\nships, thus resulting in a certain number of miss detections. In\norder to mitigate this problem, on one hand, it is necessary\nto strengthen the attention to the important areas of the\nship target, such as the contour of the hull and the detail\ninformation of the edges. On the other hand, information about\nthe rotation angle of the target needs to be introduced during\nthe training process to better localize the ship target.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 12\nThe reason for the high false alarm rate may lie in the\nfact that there are extremely similar features between the\nship target and the surrounding islands and buildings, and at\nthe same time, the scattering interference is easily generated\naround the ship hull, and this phenomenon will prevent the\ndetection model from effectively obtaining the target feature\ninformation, which will lead to a certain number of false\nalarms. In order to solve this problem, the global features and\nlocal context information of the target need to be introduced\nto better distinguish between ships and false targets.\nV. C ONCLUSION\nIn this paper, a lightweight SAR ship detection network\nbased on transformers and feature enhancement, namely\nEGTB-Net, is proposed. Firstly, in order to obtain a\nlightweight feature extraction network, a novel Ghost-ECA\nmodel is designed in this paper. This model not only reduces\nthe number of parameters and computation of the model\nbut also enables cross-channel information interaction and\nimproves the feature extraction capability of the network.\nThen, this paper incorporates a transformer block in the\nbackbone network, which can suppress the interference of\ncomplex environments and multiple noises, capture the local\nand global information of the ship target, and enhance the\nability of the network to identify the important features of\nthe target. Finally, a new SIoU loss function is used in this\npaper. It refines the distance cost and shape cost on the\nbasis of angle cost to improve detection accuracy and training\nspeed. Experimental results on the SAR-Ship-Dataset show\nthat the EGTB-Net detection accuracy reaches 94.83%, and\nthe number of parameters and computation are only 5.94 M\nand 18.53 G. Compared with some state-of-the-art methods,\nthe proposed model has the highest detection accuracy as\nwell as the least computational complexity. The lightweight\nship detection model proposed plays an important role in sea\ndefense construction and military surveillance and better meets\nthe real-time demand for maritime SAR ship detection.\nAlthough the effectiveness of the proposed model is verified\non the SAR-Ship-Dataset in this paper, this paper adopts\nvertical rectangular frame detection, and for some ship targets\nthat are adjacent to some near coasts and have multiple angles,\nthe method in this paper still fails to achieve satisfactory\nresults. The reason is that the vertical rectangular frame cannot\nobtain the azimuth estimation information of the ship target,\nand the rectangular frame makes it easy to introduce a large\namount of background noise that is not related to the pixels\nof the ship. The use of a rotating frame can not only obtain\nthe position coordinates of the ship but also its rotation angle\ninformation [41]. It is beneficial for the detection model\nto completely separate the target to be detected from the\nbackground pixels and locate the target to be detected more\naccurately. Therefore, how to obtain the angle information of\nthe rotating frame of the ship target to further improve the\nnear-coastal detection accuracy is the direction of subsequent\nresearch.\nREFERENCES\n[1] S. M. Idicula and B. Paul, “A novel sarnede method for real-time ship\ndetection from synthetic aperture radar image,” Multimedia Tools and\nApplications, vol. 81, no. 12, pp. 16 921–16 944, 2022.\n[2] Y . Deng, D. Guan, Y . Chen, W. Yuan, J. Ji, and M. Wei, “Sar-Shipnet:\nSar-ship detection neural network via bidirectional coordinate atten-\ntion and multi-resolution feature fusion,” in ICASSP 2022-2022 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2022, pp. 3973–3977.\n[3] S. Hou, X. Ma, X. Wang, Z. Fu, J. Wang, and H. Wang, “SAR image\nship detection based on scene interpretation,” in IGARSS 2020-2020\nIEEE International Geoscience and Remote Sensing Symposium. IEEE,\n2020, pp. 2863–2866.\n[4] L. Bai, C. Yao, Z. Ye, D. Xue, X. Lin, and M. Hui, “Feature en-\nhancement pyramid and shallow feature reconstruction network for SAR\nship detection,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , vol. 16, pp. 1042–1056, 2023, doi:\n10.1109/JSTARS.2022.3230859.\n[5] S. Liu, D. Li, R. Jiang, Q. Liu, J. Wan, X. Yang, and H. Liu,\n“A mixed-scale self-distillation network for accurate ship detection\nin SAR images,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , vol. 16, pp. 9843–9857, 2023, doi:\n10.1109/JSTARS.2023.3324496.\n[6] J. Zhang, W. Sheng, H. Zhu, S. Guo, and Y . Han, “MLBR-YOLOX:\nAn efficient SAR ship detection network with multilevel background\nremoving modules,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , vol. 16, pp. 5331–5343, 2023, doi:\n10.1109/JSTARS.2023.3280741.\n[7] Z. He, W. Chen, Y . Yang, D. Weng, and N. Cao, “Maritime ship target\nimaging with GNSS-based passive multistatic radar,” IEEE Transactions\non Geoscience and Remote Sensing , vol. 61, pp. 1–18, 2023, doi:\n10.1109/TGRS.2023.3270182.\n[8] J. Schou, H. Skriver, A. Nielsen, and K. Conradsen, “CFAR edge\ndetector for polarimetric SAR images,” IEEE Transactions on Geo-\nscience and Remote Sensing , vol. 41, no. 1, pp. 20–32, 2003, doi:\n10.1109/TGRS.2002.808063.\n[9] B. Hou, X. Chen, and L. Jiao, “Multilayer CFAR detection of ship targets\nin very high resolution SAR images,” IEEE Geoscience and Remote\nSensing Letters, vol. 12, no. 4, pp. 811–815, 2014.\n[10] M.-D. Li, X.-C. Cui, and S.-W. Chen, “Adaptive superpixel-level CFAR\ndetector for SAR inshore dense ship detection,” IEEE Geoscience and\nRemote Sensing Letters , vol. 19, pp. 1–5, 2021.\n[11] Z. Zhao, K. Ji, X. Xing, and H. Zou, “Adaptive CFAR detection of ship\ntargets in high resolution SAR imagery,” in MIPPR 2013: Multispectral\nImage Acquisition, Processing, and Analysis , vol. 8917. SPIE, 2013,\npp. 133–140.\n[12] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-\ntime object detection with region proposal networks,” in Advances in\nNeural Information Processing Systems, C. Cortes, N. Lawrence, D. Lee,\nM. Sugiyama, and R. Garnett, Eds., vol. 28. Curran Associates, Inc.,\n2015.\n[13] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick, “Mask R-CNN,” in\nProceedings of the IEEE international conference on computer vision ,\n2017, pp. 2961–2969.\n[14] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\nhierarchies for accurate object detection and semantic segmentation,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2014, pp. 580–587.\n[15] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and\nA. C. Berg, “SSD: Single shot multibox detector,” in Computer Vision–\nECCV 2016: 14th European Conference, Amsterdam, The Netherlands,\nOctober 11–14, 2016, Proceedings, Part I 14 . Springer, 2016, pp.\n21–37.\n[16] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar, “Focal loss\nfor dense object detection,” in Proceedings of the IEEE international\nconference on computer vision , 2017, pp. 2980–2988.\n[17] M. Tan, R. Pang, and Q. V . Le, “EfficientDet: Scalable and efficient\nobject detection,” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , 2020, pp. 10 781–10 790.\n[18] T. Zhang, X. Zhang, and X. Ke, “Quad-FPN: A novel quad feature\npyramid network for SAR ship detection,” Remote Sensing , vol. 13,\nno. 14, p. 2771, 2021.\n[19] J. Lv, J. Chen, Z. Huang, H. Wan, C. Zhou, D. Wang, B. Wu, and\nL. Sun, “An anchor-free detection algorithm for SAR ship targets with\ndeep saliency representation,” Remote Sensing , vol. 15, no. 1, p. 103,\n2022.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 13\n[20] M. Zhang, Y . Chen, X. Lv, L. Yang, D. Yu, J. Li, and B. Zhang,\n“Synthetic aperture radar ship detection in complex scenes based on mul-\ntifeature fusion network,” Journal of Applied Remote Sensing , vol. 17,\nno. 1, pp. 016 511–016 511, 2023.\n[21] Y . Zhao, L. Zhao, B. Xiong, and G. Kuang, “Attention receptive pyramid\nnetwork for ship detection in SAR images,” IEEE Journal of Selected\nTopics in Applied Earth Observations and Remote Sensing , vol. 13, pp.\n2738–2756, 2020, doi:10.1109/JSTARS.2020.2997081.\n[22] X. Wang, Z. Cui, Z. Cao, and S. Dang, “Dense docked ship detection via\nspatial group-wise enhance attention in SAR images,” in IGARSS 2020-\n2020 IEEE International Geoscience and Remote Sensing Symposium .\nIEEE, 2020, pp. 1244–1247.\n[23] D. Li, Q. Liang, H. Liu, Q. Liu, H. Liu, and G. Liao, “A novel mul-\ntidimensional domain deep learning network for SAR ship detection,”\nIEEE Transactions on Geoscience and Remote Sensing , vol. 60, pp. 1–\n13, 2021, doi:10.1109/TGRS.2021.3062038.\n[24] C. Wang, J. Shi, Z. Zou, W. Wang, Y . Zhou, and X. Yang, “A\nsemi-supervised Sar ship detection framework via label propaga-\ntion and consistent augmentation,” in 2021 IEEE International Geo-\nscience and Remote Sensing Symposium IGARSS , 2021, pp. 4884–4887,\ndoi:10.1109/IGARSS47720.2021.9553060.\n[25] M. Chendeb El Rai, J. H. Giraldo, M. Al-Saad, M. Darweech, and\nT. Bouwmans, “SemiSegSAR: A semi-supervised segmentation algo-\nrithm for ship SAR images,” IEEE Geoscience and Remote Sensing\nLetters, vol. 19, pp. 1–5, 2022, doi:10.1109/LGRS.2022.3185306.\n[26] S. Chen, R. Zhan, W. Wang, and J. Zhang, “Domain adapta-\ntion for semi-supervised ship detection in SAR images,” IEEE\nGeoscience and Remote Sensing Letters , vol. 19, pp. 1–5, 2022,\ndoi:10.1109/LGRS.2022.3171789.\n[27] Y . Zhou, X. Jiang, Z. Chen, L. Chen, and X. Liu, “A semisupervised\narbitrary-oriented SAR ship detection network based on interference\nconsistency learning and pseudolabel calibration,” IEEE Journal of\nSelected Topics in Applied Earth Observations and Remote Sensing ,\nvol. 16, pp. 5893–5904, 2023, doi:10.1109/JSTARS.2023.3284667.\n[28] S. Liu, W. Kong, X. Chen, M. Xu, M. Yasir, L. Zhao, and J. Li, “Multi-\nscale ship detection algorithm based on a lightweight neural network\nfor spaceborne SAR images,” Remote Sensing, vol. 14, 2022.\n[29] X. Xu, X. Zhang, and T. Zhang, “Lite-YOLOv5: A lightweight deep\nlearning detector for on-board ship detection in large-scene sentinel-1\nSAR images,” Remote Sensing, vol. 14, no. 4, p. 1018, 2022.\n[30] Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “YOLOX: Exceeding YOLO\nseries in 2021,” arXiv preprint arXiv:2107.08430 , 2021.\n[31] K. Han, Y . Wang, Q. Tian, J. Guo, C. Xu, and C. Xu, “GhostNet:\nMore features from cheap operations,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 2020, pp. 1580–\n1589.\n[32] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, “ECA-Net:\nEfficient channel attention for deep convolutional neural networks,”\nin Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2020, pp. 11 534–11 542.\n[33] Z. Zheng, P. Wang, W. Liu, J. Li, R. Ye, and D. Ren, “Distance-IoU loss:\nFaster and better learning for bounding box regression,” in Proceedings\nof the AAAI conference on artificial intelligence , vol. 34, no. 07, 2020,\npp. 12 993–13 000.\n[34] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale. arxiv 2020,” arXiv preprint arXiv:2010.11929 , 2010.\n[35] Y . Zhou, H. Liu, F. Ma, Z. Pan, and F. Zhang, “A sidelobe-aware\nsmall ship detection network for synthetic aperture radar imagery,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 61, pp. 1–16,\n2023, doi:10.1109/TGRS.2023.3264231.\n[36] Z. Gevorgyan, “SIoU loss: More powerful learning for bounding box\nregression,” arXiv preprint arXiv:2205.12740 , 2022.\n[37] Y . Wang, C. Wang, H. Zhang, Y . Dong, and S. Wei, “A SAR dataset of\nship detection for deep learning under complex backgrounds,” remote\nsensing, vol. 11, no. 7, p. 765, 2019.\n[38] C.-Y . Wang, A. Bochkovskiy, and H.-Y . M. Liao, “YOLOv7: Trainable\nbag-of-freebies sets new state-of-the-art for real-time object detectors,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023, pp. 7464–7475.\n[39] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean conference on computer vision . Springer, 2020, pp. 213–\n229.\n[40] A. Bochkovskiy, C.-Y . Wang, and H.-Y . M. Liao, “YOLOv4: Op-\ntimal speed and accuracy of object detection,” arXiv preprint\narXiv:2004.10934, 2020.\n[41] Y . Zhou, X. Jiang, G. Xu, X. Yang, X. Liu, and Z. Li, “PVT-\nSAR: An arbitrarily oriented SAR ship detector with pyramid vi-\nsion transformer,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , vol. 16, pp. 291–305, 2023,\ndoi:10.1109/JSTARS.2022.3221784.\nShichuang Zhou received the B.S. degree in com-\nmunication engineering from Nanjing University of\nScience and Technology ZiJin College, Nanjing,\nChina, in 2021, and the M.S. degree in information\nand communication engineering from Inner Mongo-\nlia University of Science and Technology, Baotou,\nChina.\nHis research interests include remote sensing im-\nage processing, synthetic aperture radar (SAR) ob-\nject detection, and computer vision.\nMing Zhang received the B.S. degree in electronic\ninformation engineering from Dalian Ocean Univer-\nsity, Dalian, China, in 2008, the M.S. degree in\ncontrol theory and control engineering from Inner\nMongolia University of Science and Technology,\nBaotou, in 2011, and the Ph.D. degree in computer\napplication technology from Dalian Maritime Uni-\nversity, Dalian, in 2023. He is currently an assistant\nprofessor with the School of Digital and Intelligence\nIndustry, Inner Mongolia University of Science and\nTechnology, Baotou.\nHis research interests include remote sensing image processing, image\nregistration, pattern recognition, and computer vision.\nLiang Wu received his PhD from Shandong Uni-\nversity. He is currently an assistant professor at\nShandong First Medical University.\nHis research interests include medical image pro-\ncessing, image segmentation, image registration, and\npattern recognition.\nDahua Yu received his PhD from Xidian Univer-\nsity. He is currently a professor at Inner Mongolia\nUniversity of Science and Technology.\nHis research interests include medical image and\nsignal processing.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 14\nJianjun Li received his PhD from Beijing University\nof Aeronautics and Astronautics. He is currently a\nprofessor at Inner Mongolia University of Science\nand Technology.\nHis research interests include intelligent infor-\nmation processing, pattern recognizing, and image\nprocessing.\nFei Fan received his MS degree from Inner Mon-\ngolia University of Science and Technology.\nHis research interests include remote sensing im-\nage processing and computer vision.\nLiyun Zhang received her MS degree from Inner\nMongolia University of Science and Technology.\nHer research interests include remote sensing im-\nage super-resolution reconstruction and computer\nvision.\nYang Liu received his MS degree from Inner Mon-\ngolia University of Science and Technology.\nHis research interests include infrared image\npedestrian detection and computer vision.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3362954\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}