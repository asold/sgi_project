{
  "title": "Federated Learning of Gboard Language Models with Differential Privacy",
  "url": "https://openalex.org/W4385572321",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099980193",
      "name": "Zheng Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108548640",
      "name": "Yanxiang Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1991036657",
      "name": "Galen Andrew",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5092596348",
      "name": "Christopher Choquette",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A18301080",
      "name": "Peter Kairouz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2336634010",
      "name": "Brendan McMahan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5106415747",
      "name": "Jesse Rosenstock",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150497912",
      "name": "Yuanbo Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2900120080",
    "https://openalex.org/W2962790997",
    "https://openalex.org/W4294106961",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4385187849",
    "https://openalex.org/W3172323480",
    "https://openalex.org/W4225150360",
    "https://openalex.org/W3207429447",
    "https://openalex.org/W4221164659",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W2767079719",
    "https://openalex.org/W3178336997",
    "https://openalex.org/W2784621220",
    "https://openalex.org/W2027595342",
    "https://openalex.org/W4287663285",
    "https://openalex.org/W2904190483",
    "https://openalex.org/W4298221930",
    "https://openalex.org/W4318619660",
    "https://openalex.org/W3214167602",
    "https://openalex.org/W3038028469",
    "https://openalex.org/W4287723974",
    "https://openalex.org/W4295358171",
    "https://openalex.org/W4309129280",
    "https://openalex.org/W4386075835",
    "https://openalex.org/W4377866448",
    "https://openalex.org/W2963699739",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W4319653418"
  ],
  "abstract": "Zheng Xu, Yanxiang Zhang, Galen Andrew, Christopher Choquette, Peter Kairouz, Brendan Mcmahan, Jesse Rosenstock, Yuanbo Zhang. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 5: Industry Track, pages 629–639\nJuly 10-12, 2023 ©2023 Association for Computational Linguistics\nFederated Learning of Gboard Language Models with Differential Privacy\nZheng Xu∗, Yanxiang Zhang ∗, Galen Andrew, Christopher A. Choquette-Choo,\nPeter Kairouz, H. Brendan McMahan, Jesse Rosenstock, Yuanbo Zhang\nGoogle\nAbstract\nWe train language models (LMs) with fed-\nerated learning (FL) and differential privacy\n(DP) in the Google Keyboard (Gboard). We\napply the DP-Follow-the-Regularized-Leader\n(DP-FTRL) (Kairouz et al., 2021b) algorithm\nto achieve meaningfully formal DP guarantees\nwithout requiring uniform sampling of client\ndevices. To provide favorable privacy-utility\ntrade-offs, we introduce a new client partici-\npation criterion and discuss the implication of\nits conﬁguration in large scale systems. We\nshow how quantile-based clip estimation (An-\ndrew et al., 2021) can be combined with DP-\nFTRL to adaptively choose the clip norm dur-\ning training or reduce the hyperparameter tun-\ning in preparation for training. With the help of\npretraining on public data, we train and deploy\nmore than twenty Gboard LMs that achieve\nhigh utility and ρ−zCDP privacy guarantees\nwith ρ ∈(0.2,2), with two models addition-\nally trained with secure aggregation (Bonawitz\net al., 2017). We are happy to announce that all\nthe next word prediction neural network LMs\nin Gboard now have DP guarantees, and all fu-\nture launches of Gboard neural network LMs\nwill require DP guarantees. We summarize our\nexperience and provide concrete suggestions\non DP training for practitioners.\n1 Introduction\nFL and Gboard LMs. In cross-device federated\nlearning (FL), client devices collaboratively train a\nmodel without directly exchanging their local data\n(Kairouz et al., 2019). Google Keyboard (Gboard)\nwas an early adopter of FL to train models that im-\nprove the user experience, following data minimiza-\ntion principles (Bonawitz et al., 2021) to protect\nusers’ privacy from some risks. Language mod-\nels (LMs) are trained with FL to support various\nfeatures in Gboard, including Next Word Predic-\ntion (NWP), Smart Compose (SC), and On-The-\n∗Equal contribution, alphabetical order. Correspondence\nto {xuzheng,zhangyx}@google.com.\nNext Word Prediction (NWP) Smart Compose (SC)\nOn-The-Fly Rescoring(OTF)\nFigure 1: Gboard features supported by language mod-\nels: NWP for next word, SC for inline suggestion, and\nOTF for candidates re-ranking.\nFly rescoring (OTF). As illustrated in Fig. 1, NWP\n(Hard et al., 2018) uses an LM to suggest a word,\nwhich is triggered after a previous word is com-\nmitted; SC provides longer inline suggestions to\naccelerate typing, which can be triggered per char-\nacter when the conﬁdence is high; OTF is used\nto re-rank the candidate words generated during\ntyping before a word is committed.\nModels, metrics and tasks. We train LMs\nwith the same neural network (NN) architecture\ndescribed in (Hard et al., 2018): a one-layer\nLSTM/CIFG of 670 hidden neurons, with in-\nput and output word-based embeddings of dimen-\nsion 96. OTF LMs use a larger vocabulary ( ∼\n30K words) compared to NWP LMs ( ∼10–20K\nwords); the number of parameters for models with\na 10K/20K/30K vocabulary is 2.4M/4.4M/6.4M,\nrespectively. SC is a downstream task that reuses\nNWP LMs without any retraining from data. We\ntrain NWP LMs and OTF LMs from populations of\ndevices categorized by language and location. For\nexample, en-US NWP denotes the task of training\nNWP model on data generated by devices using\nEnglish in the United States.\nFederated Averaging (FedAvg) (McMahan et al.,\n2017) and variants (Wang et al., 2021) are pop-\nular FL training algorithms in practice. In each\ncommunication round, the server will orchestrate\na small subset of client devices for training and\n629\n+\n∑\nDP release\n0. (Optional) initialize model \nwith public pre-training\n7. Launch final model \nafter all training rounds\nPopulation of \nclient devices 1. A subset of clients can \nparticipate in a training \nround when local criteria \nare met; client participation \ncontrol is applied\n2. Broadcast model\n3. Clip after local \ntraining to limit a \nuser’s contribution\n4. (Securely) aggregate\n updates from at least \nreport goal number of \nclients\n5. Add stateful noise\n6. Apply privatized \nupdate to model \nFigure 2: System overview of federated learning of Gboard language models with differential privacy and secure\naggregation.\naggregate the resulting model deltas to update the\nglobal model. In a successful round, the system\nguarantees the number of clients participating in\ntraining is at least as large as the conﬁgured report\ngoal (Bonawitz et al., 2019). A model is typically\ntested and deployed after training for several thou-\nsands of rounds. Top-1 in-vocab accuracy is used\nto track the utility during training and additional\nmetrics for A/B testing are introduced in Sec. 3.\nDP and DP-FTRL. Differential privacy (DP) can\nbe combined with FL to provide a formal guar-\nantee that the trained model will not memorize\nspeciﬁc users’ data, which provides stronger pri-\nvacy protection by executing data anonymization\nprinciples (Bonawitz et al., 2021; Wang et al.,\n2021). Ramaswamy et al. (2020) applied DP-\nFedAvg (McMahan et al., 2018; Geyer et al.,\n2017), a variant of DP-SGD (Abadi et al., 2016)\nfor user/client-level DP, to train production LMs\nin FL. Ramaswamy et al. (2020) demonstrated\nanonymization via empirical auditing techniques\nby Carlini et al. (2019) but did not provide a for-\nmal DP guarantee. Achieving a strong formal DP\nguarantee for DP-FedAvg would require privacy\nampliﬁcation-by-sampling, which necessitates sam-\npling clients uniformly at random on each round.\nHowever, a cross-device FL system has limited\ncontrol over client sampling as devices have to\nsatisfy local criteria such as being charging and\nconnected to an unmetered network to be eligible\nfor participation (Bonawitz et al., 2019; Balle et al.,\n2020). In contrast, we deploy a recent algorithm,\nDP-FTRL (Kairouz et al., 2021b), allowing us to\nachieve strong privacy and utility for production\nmodels without uniform sampling assumptions.\nContributions. We discuss our strategy and expe-\nrience of training Gboard LMs with FL and DP. We\nintroduce an algorithm that enables adaptive clip-\nping (Andrew et al., 2021) in DP-FTRL (Kairouz\net al., 2021b) (Sec. 2.1), which can reliably esti-\nmate the clip norm to reduce hyperparameter tun-\ning. We discuss the impact of scaling up computa-\ntion and limiting client participation (Sec. 2.2), and\nidentify the algorithm and system conﬁgurations\nfor the regime of strong privacy and utility. We\nalso successfully apply pre-training (Sec. 2.3) to\nimprove privacy and utility, which is (to the best of\nour knowledge) the ﬁrst time pretraining is applied\nto training a DP model directly from users’ data.\nWe combine DP-FTRL with secure aggrega-\ntion (SecAgg) to further strengthen the data min-\nimization properties of our approach (Sec. 2.4).\nFig. 2 provides a system overview of the techniques\nfor training Gboard language models with feder-\nated learning and differential privacy. Finally, we\nsummarize concrete suggestions for practitioners\ntraining differentially private models to deploy in\nproduction in (Sec. 2.5), and present and analyze\ntwenty Gboard LMs trained with formal DP guar-\nantees (Sec. 3). We are happy to announce that\nall the next word prediction neural network LMs\nin Gboard now have DP guarantees, and all future\nlaunches of Gboard neural network LMs will re-\nquire DP guarantees.\n2 DP FL in Practice\n2.1 DP-FTRL and adaptive clipping\nAs described in Alg. 1, we apply DP-FTRL in FL\nby modifying the FedAvg algorithm: clip the model\nupdate ∆, and add noise when updating the global\n630\nAlgorithm 1 Federated DP-FTRL with adaptive clipping\ninput : report goal m, learning rate for model weights on client ηc and on server ηs, momen-\ntum β = 0 .9, noise multiplier for model delta z∆, total number of rounds T, restart rounds\nR = {128 + 1024i,i = 0 ,1,... }, quantile based norm estimation C0 , target quantile γ = 0.5 ,\nlearning rate for norm ηγ = 0.2 , noise stddev for clip estimation σb = m/20\nInitialize model θ0, momentum buffer ¯∆0 = 0,\nclip norm Cθ = C0\nInitialize tree Tθ with z∆, Cθ, and Tb with σb\nfor each round t= 0,1,2,...,T do\nQt ←(at least musers for this round)\nfor each user i∈Qt in parallel do\n(∆t\ni,bt\ni) ←ClientUpdate(i,θt,ηc,Cθ,Ct)\n//Update model weights with noise addition\n˜∆t = 1\nmPrivateSum\n(\nTθ, ∑\ni∈Qk ∆k\ni, k∈[0,t]\n)\n¯∆t = β¯∆t−1 + ˜∆t, θt+1 ←θ0 + ηs¯∆t\n//Estimate quantile-based norm\n˜bt = 1\nmPrivateSum\n(\nTb, ∑\ni∈Qk bk\ni, k∈[0,t]\n)\nCt+1 ←C0 ·exp\n(\n−ηγ(˜bt −tγ)\n)\n//Restart and adjust clip norm\nif t∈R then\nCθ ←Ct+1\nRestart tree Tθ and Tb with updated Cθ\nfunction ClientUpdate(i, θ0, η, Cθ, C)\nθ←θ0\nG← (user i’s local data split into batches)\nfor batch g∈G do\nθ←θ−η∇ℓ(θ; g)\n∆ ←θ−θ0\nb←I||∆||≤C\n∆′←∆ ·min\n(\n1, Cθ\n||∆||\n)\n//Clipping\nreturn (∆′,b)\n0 500 1000 1500 2000 2500\nRounds\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16Eval Acc\nAdaptive\nFixed\nPretrain\nAcc-0.16\n(a) Evaluation accuracy\n0 500 1000 1500 2000 2500\nRounds\n1\n2\n3\n4\n5\n6\n7\n8Clip Norm\nAdaptive\nEstimated\nFixed (b) Clip norm\nFigure 3: DP training of the en-GB NWP model. Adap-\ntive clipping performs similar to ﬁxed clipping, while\nachieves slightly weaker guarantees. Pre-training sig-\nniﬁcantly reduces the number of rounds to reach the\nutility target, and achieves stronger guarantees.\nmodel. Two additional hyperparameters are intro-\nduced for DP: the clip norm C, which bounds the\nnorm of ∆, and the noise multiplier z, which de-\ntermines the standard deviation zC for the added\nGaussian noise. We discuss clip norm in this sec-\ntion and defer the discussion of noise multiplier and\nother privacy related hyperparameters to Sec. 2.2.\nAndrew et al. (2021) introduced an adaptive clip-\nping method that automatically adjusts the clip\nnorm each round by privately estimating the norm\nof the model delta at a targeted quantile. How-\never, adaptive clipping cannot be directly applied\nto DP-FTRL as the tree-based noise addition in DP-\nFTRL assumes a ﬁxed clip norm across rounds. We\nintegrate adaptive clipping in DP-FTRL through\nrestarts, where the quantile estimate Ct is continu-\nally tracked but only becomes an active clip norm\nCθ upon tree restarting. As both the aggregated\nmodel delta ˜∆t and the quantile ˜bt use tree-based\nnoise, we can directly use the privacy accounting\nin (Kairouz et al., 2021b) by applying the noise\ntransformation in Thm. 1 in App. A.\nIn practice, Alg. 1 slightly inﬂates the noise for\nthe model from zC to z∆C and requires restarts\nthat complicate the privacy accounting for DP-\nFTRL. Moreover, we ﬁnd that a ﬁxed clip norm can\nachieve comparable or slightly better model utility,\nand is more robust in experiments with large report\ngoal. For example, adaptive clipping for the de-DE\nNWP model experiences catastrophic failure and\nmakes no progress in the ﬁrst 1000 rounds.\nNevertheless, adaptive clipping can reduce hy-\nperparameter tuning for many tasks when privacy\nbudget allows. Fig. 3 shows the evaluation accuracy\nand corresponding clip norm for DP training the en-\nGB NWP model with report goal 6500 and noise\nmultiplier 7. The adaptive clip curve starts from a\nsmall initial clip norm to avoid catastrophic failure\ndue to large initial noise and eventually catches up\non accuracy. The estimated clip norm (quantile\nγ = 0.5) stabilizes and we can ﬁx the clip norm\nto 5 based on the estimated value. The clip norm\nis relatively insensitive, especially when tuning to-\n631\ngether with the server learning rate. However, clip\nnorm can have a wide tuning range across tasks\nand models, and quantile-based estimation is still\nuseful for estimating a clip norm to be ﬁxed.\n2.2 DP parameters and system conﬁguration\nThe privacy guarantees of DP-FTRL (Kairouz et al.,\n2021b) are affected by several factors: noise multi-\nplier z, number of total roundsT, max participation\n(MaxP) of a client, and min separation (MinS) of\nrounds between the participation of the same client.\nThe noise multiplier is a conventional parameter\nfor controlling privacy-utility trade-off: large noise\nachieves strong privacy guarantees but can poten-\ntially hurt the utility. Achieving the same utility\nwith smaller rounds T can signiﬁcantly improve\nthe privacy guarantees. Next, we discuss the ef-\nfect of MaxP and MinS, and the privacy-utility-\ncomputation trade-off for system conﬁguration.\nClient participation. DP-FTRL achieves strong\nprivacy if each client only participates once dur-\ning training, or the number of client participation\nis limited when a client can participate multiple\ntimes. Two parameters are introduced to charac-\nterize client participation for DP-FTRL: the maxi-\nmum participations (MaxP) of a client in all train-\ning rounds and the minimum round separation\n(MinS) between any single client’s two participa-\ntions. MaxP and MinS are correlated as MaxP\nis upper bounded by rounds T divided by MinS.\nIn general, for ﬁxed rounds T, decreasing MaxP\nand increasing MinS can lead to stronger privacy\nguarantees without changing utility. In addition,\nCho et al. (2023) suggests potential advantage of\nincreasing MinS for utility.\nWhen using the worst-case MaxP estimated by\nrounds T divided by MinS, Fig. 4c shows increas-\ning MinS can achieve stronger privacy measured\nby smaller zCDP values. However, the maximum\nMinS is limited by the population size divided by\nthe number of clients per round lower bounded by\nthe report goal. For example, when the report goal\nis 6500 for small population of around 106, MinS\nhas to be smaller than153 rounds, so strong privacy\nguarantees are difﬁcult to achieve when training for\n3000 rounds. While we cannot measure the precise\npopulation size in the FL system due to client dy-\nnamics, we estimate the population size of various\nGboard tasks as ranging from 0.8 million to 16.6\nmillion in Tab. 1.\nReport goal. We study report goal for privacy-\ncomputation trade-off based on a hypothesis used\nin (McMahan et al., 2018; Kairouz et al., 2021b;\nXu et al., 2022): for sufﬁciently large data, the\nutility is approximately non-decreasing if the noise\nmultiplier and clients per round (lower bounded\nby report goal) proportionally increase. We pro-\nvide empirical justiﬁcation to this hypothesis by\ncomparing the evaluation accuracy of two training\nruns: one with a report goal of 500 and noise mul-\ntiplier of 0.54, versus another of report goal 6500\nand noise multiplier 7. On more than three Gboard\nlanguage tasks, we observed that the ﬁnal utility\nremains similar, or slightly better for larger report\ngoals. Moreover, using a larger report goal speeds\nup learning at the beginning of training. Based on\nthe hypothesis, we plot Figs. 4a and 4b by linearly\nincreasing report goal and noise multiplier, and\nassuming the MinS is set to the maximum possi-\nble value (population divided by report goal) for\nstrong privacy. Though a large report goal can\nlimit the MinS, it generally leads to stronger pri-\nvacy guarantees for reasonable population size and\ntotal rounds.\nSystem conﬁguration. According to Figs. 4a\nand 4b, we choose a large report goal of 6500 sup-\nported by the large scale FL systems and aim for\nmaximum MinS for DP-FTRL. To control MinS in\npractice, a timer is introduced on clients in the FL\nsystem so that a client will only become eligible\nto participate in training (again) after a certain pe-\nriod of time has passed. McMahan and Thakurta\n(2022) used a timer period of 24 hours to train\nthe es-ES NWP model, which led to an observed\nMinS of 313. The MinS of es-ES is upper bounded\nby 4.21M/6500 ∼647 and can be potentially im-\nproved by increasing the timer period. We increase\nthe timer period in the unit of 24 hours due to the\nuneven diurnal participation pattern (Yang et al.,\n2018; Zhu et al., 2022), and generally observe that\nMinS can proportionally increase with the timer pe-\nriod before reaching the possible maximum. How-\never, there are many factors in the FL system that\nmay affect the wall clock training speed, which\nmakes it challenging to optimize the timer period\nto maximize MinS.\n2.3 Public pretraining\nWe explore pretraining on public data for produc-\ntion models, which were shown to substantially\nimprove model utility in DP simulations (Li et al.,\n2021; De et al., 2022; Xu et al., 2022; Wang et al.,\n632\n4000 5000 6000 7000 8000 9000 10000\nReport goal\n0\n1\n2\n3\n4\n5\n6zCDP\nrounds_500\nrounds_1000\nrounds_2000\nrounds_3000\nzCDP-2.61\n(a) Population 1M\n4000 5000 6000 7000 8000 9000 10000\nReport goal\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6zCDP\nrounds_500\nrounds_1000\nrounds_2000\nrounds_3000\nzCDP-2.61 (b) Population 3M\n200 400 600 800 1000120014001600\nMin separation\n2−4\n2−3\n2−2\n2−1\n20\n21\n22\n23\n24\n25\nzCDP\nrounds_500\nrounds_1000\nrounds_2000\nrounds_3000\nzCDP-2.61\npop-1M\npop-3M\npop-10M (c) Report goal 6500\nFigure 4: The effect of population size, number of rounds, report goals, and min separation on DP-FTRL privacy\nguarantees. For a ﬁxed number of rounds to achieve utility target, increasing report goal and min separation can\nachieve stronger guarantees measured by smaller zCDP.\n2023). We pretrain a model for each Gboard lan-\nguage task using the multi-lingual C4 dataset (Raf-\nfel et al., 2019; Xue et al., 2020) collected from\npublic web pages. Fig. 3a shows that pretraining\ncan reduce ∼1000 rounds to reach a given utility\nthreshold under the same noise multiplier, which\ncan signiﬁcantly improve the privacy guarantees as\nshown in Fig. 4.\nWe additionally observe that: (1) it is challeng-\ning to ﬁne-tune from a pretrained model when the\nword embeddings are shared for input and output\nto reduce the parameter size of LMs for on-device\ndeployment; (2) the accuracy may decrease in the\nﬁrst a few rounds of ﬁne-tuning; (3) pretraining\nhelps with diminishing marginal returns: at some\npoint further pretraining does not necessarily im-\nprove the ﬁnal performance. Therefore, we use\nmodels with separate input and output embeddings\nand pretrain with half of the C4 dataset for Gboard\nLMs.\n2.4 Combining with secure aggregation\nSecure aggregation (SecAgg) (Bonawitz et al.,\n2017) ensures that the central server can only ac-\ncess the aggregated update from a large set of\nclients, preventing inspection of individual client\nupdates. We combine SecAgg and DP-FTRL to\nprovide strong data minimization and anonymiza-\ntion protection (Bonawitz et al., 2021). To avoid\nthe suboptimal privacy cost from the ℓ2 norm in-\ncrease of the discretized vector in SecAgg, we\nfollow the protocol of (Kairouz et al., 2021a) for\ndiscretizing, ﬂattening, and modularly clipping 1\nthe client model updates—this introduces mini-\nmal norm inﬂation later accounted in DP-FTRL.\nThe large report goal requirement for strong DP\n1In our current implementation, there is a very small\nchance that modular operator in SecAgg will inﬂate the sensi-\ntivity. The problem will be ﬁxed by an additional element-wise\nclipping of the ﬂattened vector.\nguarantees is challenging for SecAgg in practice,\nwhich requires a slightly different system conﬁg-\nuration. The SecAgg training speeds we observe\nare still notably slower, and we leave for future\nwork potential improvements such as compression\nfor communication efﬁciency (Chen et al., 2022),\nnew DP methods to reduce report goal (Choquette-\nChoo et al., 2022), and embedding compression to\nreduce round time (Shu and Nakayama, 2017).\n2.5 Recommended strategies and practices\nWe summarize our strategy for training Gboard\nLMs with DP. (1) Pre-train the model on public\ndatasets if possible. (2) Choose the maximum noise\nmultiplier that meets the utility target based on\nsmall report goal simulation experiments on pub-\nlic datasets that is similar to the production task.\n(3) Based on the target number of rounds and esti-\nmated population, linearly increase the report goal\nand noise multiplier to meet the privacy target, and\nchoose a large report goal supported by the system.\nIf the privacy target is unachievable, ﬁx the report\ngoal to maximum, and increase the noise multi-\nplier to target on a model with suboptimal utility.\n(4) Estimate the possible maximum MinS based on\nchosen report goal and estimated population, and\nconﬁgure the timer period to approach the MinS;\nuse previous experience of model training speed if\napplicable. (5) If the hyperparameters (e.g., learn-\ning rates) are known from previous experiments\nor simulation on public datasets, apply DP-FTRL\nwith adaptive clipping (Alg. 1) without manual tun-\ning to try meet the privacy and utility goals. Note\nthat Alg. 1 needs to account the noise inﬂation and\nrestart for privacy guarantees. (6) If Alg. 1 fails or\nstronger privacy and utility are desirable, we can\nrun a few small report goal experiments with Alg. 1\nthat tune quantile γand server learning rate ηs, se-\nlect the best learning rate, and ﬁx the clip norm\n633\nbased on the estimation; and run DP-FTRL with\nlarge report goals. (7) SecAgg can be used for all\nexperiments, and precise MaxP and MinS are com-\nputed by post-processing for privacy accounting.\n3 Deploying DP LMs\nA/B test metrics. We introduce metrics in A/B test\nto measure the utility of Gboard LMs. (1) Picked\nRate (PRate): the ratio of picked candidates among\nthe NWP predictions; or SC predictions when it is\ntriggered. (2) Accuracy (Acc): the ratio of candi-\ndates matching the ﬁnal committed words among\nthe NWP model predictions. (3) Trigger Rate: the\nratio of words with SC triggered among all com-\nmitted words, which is an important metric when\nPRate is ﬁxed. (4) Word Modiﬁed Ratio (WMR):\nthe ratio of words being modiﬁed during typing or\nafter committed; improvement is shown by reduc-\ntion. (5) Word Per Minute (WPM): the number of\ncommitted words per minute.\nPrivacy guarantees. Same as (McMahan and\nThakurta, 2022), the zero-one device neighbor-\ning relationship ((Kairouz et al., 2021b, deﬁnition\n1.1)) is adopted for DP. For user’s with a single\ndevice, device-level DP corresponds directly to\nuser-level DP. Our privacy guarantee holds for all\nwell-behaved clients during training, and we do\nnot account for privacy cost of modest amount of\nhyperparameter tuning. DP is measured by the\nzero-Concentrated DP (zCDP) (Bun and Steinke,\n2016) guarantee that has been used by US cen-\nsus bureau (US Census Bureau, 2021), and can be\neasily converted to (ϵ,δ)-DP. We use the privacy\naccounting in (Kairouz et al., 2021b, appendix D)\nimplemented in Tensorﬂow Privacy (TFP Authors,\n2022), and follow the guidelines outlined in (Pono-\nmareva et al., 2023, Sec. 5.3) to report detailed\nnarratives of privacy guarantees in App. C.\nExperimental setup. We use the implementation\nin App. B, and apply the strategy in Sec. 2.5 to\ntrain Gboard LMs with DP. We present NWP re-\nsults in Tab. 1, and OTF results in Tab. 2. As\nSmart Compose (SC) reuses NWP LMs, SC has the\nsame DP guarantees as NWP models by the post-\nprocessing property (Dwork et al., 2014). Follow-\ning es-ES NWP model in (McMahan and Thakurta,\n2022), we choose noise multiplier 7 and report\ngoal 6500 based on simulation in (Kairouz et al.,\n2021b) on public StackOverﬂow dataset (TFF Au-\nthors, 2022b). We pretrain the models on public\ndatasets and conﬁgure the timer period to control\nclient participation, separately for different tasks.\nWe use DP-FTRL with adaptive clipping and small\nreport goal 500 to tune server learning rate and\nestimate the clip norm. Interestingly, we observe\nthe learning rate and clip norm to be consistent for\nvarious Gboard LMs, and tuning seems to be unnec-\nessary. DP-FTRL with ﬁxed clip and large report\ngoal is used to run the ﬁnal model for deployment.\nResult analysis. All NWP and OTF models in\nTabs. 1 and 2 are trained with stronger guarantees\n(smaller zCDP) compared to zCDP >2.6 used by\nUS Census Bureau (US Census Bureau, 2021). For\nﬁve NWP models in Europe (DE, GB, FR, IT, PT),\nthe DP NN models signiﬁcantly improve the utility\ncompared to previous N-gram models. On en-US,\npt-BR and en-IN, DP NN models also achieve com-\nparable, or slightly better utility compared to their\nnon-private versions as the strong models. SecAgg\nis successfully applied to en-US and es-ES, and\ncan achieve good privacy-utility trade-off with a\nsmaller number of rounds, likely due to the system\nconﬁguration that results in more clients per round.\nHowever, SecAgg is also notably slower. There is a\ngeneral positive correlation between the estimated\npopulation size and privacy guarantees.\nHowever, only a few tasks approach the pos-\nsible maximum MinS for strong privacy guaran-\ntees, which highlights the challenge of both estimat-\ning population and controlling client participation.\nLonger training rounds are often used for NWP\n(compared to OTF) as the non-private NN baselines\nare strong, and to improve the downstream SC per-\nformance. As an example, we train es-ES NWP\nfor 1900 rounds with a pretrained model, while the\nprevious models (McMahan and Thakurta, 2022)\nis trained for 2000 rounds without pretraining. Our\nes-ES NWP model slightly improves the utility\nmeasured by PRate and Acc, and improves the\nzCDP bound from 0.81 to 0.35 due to the larger\nMinS by timer conﬁguration. We highlight that\nour es-ES model at round 1240 already achieves\nsimilar NWP utility and a strong privacy guarantee,\nbut the utility of SC keeps improving with train-\ning. Compared to the previous model in (McMahan\nand Thakurta, 2022), our model improves the SC\ntrigger rate by 4.23% at round 1240, and 9.51% at\nround 1900.\n4 Concluding remarks\nWe discuss our experience and summarize our strat-\negy for training production Gboard LMs with FL\n634\nNWP Rounds Utility Privacy Est.\nPop. (M) BaseModelPRate(+%) Acc(+%) MinS/MaxP/Timer zCDP\nde-DE 930 8.28 12.49 212 / 4/ 48h 0.48 3.24\nN-gram\nen-GB 980 3.26 7.72 226 / 4 / 72h 0.48 2.38\nfr-FR 1280 3.78 8.50 180 / 5 / 72h 0.89 2.79\nit-IT 1620 3.98 9.86 303 / 5 / 72h 0.71 3.32\npt-PT 530 3.99 7.82 54 / 8 / 48h 1.86 0.83\nes-ES 1900 0.29 0.48 526 / 3 / 144h 0.35 4.21 zCDP 0.81es-ES* 1750 0.32 0.56 349 / 4 / 144h 0.52\nen-US 2800 -0.39 0.11 371 / 7 / 48h 1.31 13\nNo-DP NNen-US* 1360 -0.30 0.15 622 / 2 / 144h 0.25\npt-BR 3600 0.18 0.29 909 / 3 / 144h 0.45 16.6\nen-IN 1290 0.19 0.40 170 / 6 / 96h 1.14 7.72\nes-MX 1980 -0.15 0.29 343 / 5 / 96h 0.64 9.96\nes-AR 640 0.25 3.50 90 / 5 / 96h 0.84 4.09 Mix\nTable 1: Live A/B tests of DP NWP models. Utility shows the improvement from previously deployed models;\nprivacy shows the key parameters and corresponding device-level zCDP; all models are trained by DP-FTRL with\nreport goal of 6500 and noise multiplier of 7; en-US*/es-ES* are trained with SecAgg in addition to DP; the base\nmodel in AR is a mix of N-gram and No-DP NN models.\nOTF Rounds Utility Privacy\nWMR(-%) WPM(+%) MinS/MaxP/Timer zCDP DP-ϵ(δ= 10−10)\nde-DE 1170 1.01 0.59 206 / 5 / 48h 0.89 9.01\nen-GB 1220 1.99 0.38 206 / 5 / 72h 0.89 9.01\nes-ES 1280 1.03 0.60 197 / 5 / 48h 0.89 9.01\nfr-FR 1300 1.83 0.67 290 / 4 / 72h 0.61 7.31\nit-IT 1360 1.39 0.80 188 / 5 / 48h 0.89 9.01\nru-RU 870 0.72 0.34 327 / 3 / 48h 0.32 5.13\npt-PT 430 1.71 0.32 54 / 7 / 48h 0.99 9.56\nTable 2: Live A/B tests of DP OTF models. Utility shows the WMR decrease and WPM increase; privacy shows\nthe key parameters and corresponding zCDP bound; all models are trained with DP-FTRL with report goal of 6500\nand noise multiplier of 7; estimated population for ru-RU is 6.63M and other tasks can be found in Tab. 1.\nand DP. We propose an algorithm applying adap-\ntive clipping (Andrew et al., 2021) in DP-FTRL\n(Kairouz et al., 2021b) to reduce the hyperparamter\ntuning. We discuss the impact on privacy and utility\nof several important factors: the clip norm, report\ngoal, client participation, and pre-training. Our\nstudy highlights the importance of system and algo-\nrithm co-design for differential privacy in practice,\nthe challenges of tuning in FL systems, and op-\nportunities to improve the scalability and stability\nof FL with DP and/or SecAgg. More than twenty\nLMs with formal DP guarantees are trained and\nlaunched to support Gboard NWP, SC, and OTF\nfeatures, including en-US and es-ES NWP models\nadditionally with SecAgg. Our experience demon-\nstrates the possibility of training DP models for\npractical applications when a large scale system is\navailable for large scale data. Therefore, Gboard is\nintroducing and enforcing a new policy: DP has to\nbe applied in all future training and launching of\nGboard LMs.\nAcknowledgement\nThe authors would like to thank Stanislav Chik-\nnavaryan, Adria Gascon, Zachary Garrett, and Ti-\nmon Van Overveldt for infrastructure conﬁgura-\ntion support; Swaroop Ramaswamy, Om Thakkar,\nAbhradeep Thakurta for early discussion on models\nand algorithms; Jeremy Gillula for internal review\nprocess; Xu Liu, Shumin Zhai, and Daniel Ramage\nfor leadership support.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\n635\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC con-\nference on computer and communications security ,\npages 308–318.\nGalen Andrew, Om Thakkar, H Brendan McMahan,\nand Swaroop Ramaswamy. 2021. Differentially pri-\nvate learning with adaptive clipping. Conference on\nNeural Information Processing Systems (NeurIPS).\nBorja Balle, Peter Kairouz, Brendan McMahan,\nOm Thakkar, and Abhradeep Guha Thakurta. 2020.\nPrivacy ampliﬁcation via random check-ins. Ad-\nvances in Neural Information Processing Systems ,\n33:4623–4634.\nKallista Bonawitz, Peter Kairouz, Brendan McMahan,\nand Daniel Ramage. 2021. Federated learning and\nprivacy: Building privacy-preserving systems for\nmachine learning and data science on decentralized\ndata. Queue, 19(5):87–114.\nKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp,\nDzmitry Huba, Alex Ingerman, Vladimir Ivanov,\nChloe Kiddon, Jakub Kone ˇcn`y, Stefano Mazzocchi,\nBrendan McMahan, et al. 2019. Towards federated\nlearning at scale: System design. Proceedings of\nmachine learning and systems, 1:374–388.\nKeith Bonawitz, Vladimir Ivanov, Ben Kreuter, Anto-\nnio Marcedone, H Brendan McMahan, Sarvar Patel,\nDaniel Ramage, Aaron Segal, and Karn Seth. 2017.\nPractical secure aggregation for privacy-preserving\nmachine learning. In proceedings of the 2017 ACM\nSIGSAC Conference on Computer and Communica-\ntions Security, pages 1175–1191.\nMark Bun and Thomas Steinke. 2016. Concentrated\ndifferential privacy: Simpliﬁcations, extensions, and\nlower bounds. In Theory of Cryptography Confer-\nence, pages 635–658. Springer.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neu-\nral networks. In 28th USENIX Security Symposium\n(USENIX Security 19), pages 267–284.\nWei-Ning Chen, Christopher A Choquette Choo, Peter\nKairouz, and Ananda Theertha Suresh. 2022. The\nfundamental price of secure aggregation in differ-\nentially private federated learning. In International\nConference on Machine Learning, pages 3056–3089.\nPMLR.\nYae Jee Cho, Pranay Sharma, Gauri Joshi, Zheng Xu,\nSatyen Kale, and Tong Zhang. 2023. On the conver-\ngence of federated averaging with cyclic client par-\nticipation. arXiv preprint arXiv:2302.03109.\nChristopher A Choquette-Choo, H Brendan McMa-\nhan, Keith Rush, and Abhradeep Thakurta. 2022.\nMulti-epoch matrix factorization mechanisms\nfor private machine learning. arXiv preprint\narXiv:2211.06530.\nSoham De, Leonard Berrada, Jamie Hayes, Samuel L\nSmith, and Borja Balle. 2022. Unlocking high-\naccuracy differentially private image classiﬁcation\nthrough scale. arXiv preprint arXiv:2204.13650.\nDP Team. 2022. Google’s differential privacy\nlibraries. https://github.com/google/\ndifferential-privacy.\nCynthia Dwork, Aaron Roth, et al. 2014. The algorith-\nmic foundations of differential privacy.Foundations\nand Trends® in Theoretical Computer Science, 9(3–\n4):211–407.\nRobin C Geyer, Tassilo Klein, and Moin Nabi. 2017.\nDifferentially private federated learning: A client\nlevel perspective. arXiv preprint arXiv:1712.07557.\nAndrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop\nRamaswamy, Françoise Beaufays, Sean Augenstein,\nHubert Eichner, Chloé Kiddon, and Daniel Ramage.\n2018. Federated learning for mobile keyboard pre-\ndiction. arXiv preprint arXiv:1811.03604.\nPeter Kairouz, Ziyu Liu, and Thomas Steinke. 2021a.\nThe distributed discrete gaussian mechanism for fed-\nerated learning with secure aggregation. In Inter-\nnational Conference on Machine Learning , pages\n5201–5212. PMLR.\nPeter Kairouz, Brendan Mcmahan, Shuang Song,\nOm Thakkar, Abhradeep Thakurta, and Zheng Xu.\n2021b. Practical and private (deep) learning without\nsampling or shufﬂing. In International Conference\non Machine Learning (ICML), pages 5213–5225.\nPeter Kairouz, H. Brendan McMahan, Brendan Avent,\nAurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,\nKaylee Bonawitz, Zachary Charles, Graham Cor-\nmode, Rachel Cummings, Rafael G. L. D’Oliveira,\nSalim El Rouayheb, David Evans, Josh Gard-\nner, Zachary Garrett, Adrià Gascón, Badih Ghazi,\nPhillip B. Gibbons, Marco Gruteser, Zaïd Har-\nchaoui, Chaoyang He, Lie He, Zhouyuan Huo,\nBen Hutchinson, Justin Hsu, Martin Jaggi, Tara Ja-\nvidi, Gauri Joshi, Mikhail Khodak, Jakub Konecný,\nAleksandra Korolova, Farinaz Koushanfar, Sanmi\nKoyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal,\nMehryar Mohri, Richard Nock, Ayfer Özgür, Ras-\nmus Pagh, Mariana Raykova, Hang Qi, Daniel Ra-\nmage, Ramesh Raskar, Dawn Song, Weikang Song,\nSebastian U. Stich, Ziteng Sun, Ananda Theertha\nSuresh, Florian Tramèr, Praneeth Vepakomma,\nJianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Fe-\nlix X. Yu, Han Yu, and Sen Zhao. 2019. Advances\nand open problems in federated learning. CoRR,\nabs/1912.04977.\nXuechen Li, Florian Tramer, Percy Liang, and Tat-\nsunori Hashimoto. 2021. Large language models\ncan be strong differentially private learners. arXiv\npreprint arXiv:2110.05679.\nBrendan McMahan, Eider Moore, Daniel Ramage,\nSeth Hampson, and Blaise Aguera y Arcas. 2017.\n636\nCommunication-efﬁcient learning of deep networks\nfrom decentralized data. In AISTATS, pages 1273–\n1282. PMLR.\nBrendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2018. Learning differentially private\nrecurrent language models. In International Confer-\nence on Learning Representations (ICLR).\nBrendan McMahan and Abhradeep Thakurta. 2022.\nFederated learning with formal differential privacy\nguarantees.\nNatalia Ponomareva, Hussein Hazimeh, Alex Kurakin,\nZheng Xu, Carson Denison, H. Brendan McMahan,\nSergei Vassilvitskii, Steve Chien, and Abhradeep\nThakurta. 2023. How to dp-fy ml: A practical guide\nto machine learning with differential privacy.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research.\nSwaroop Ramaswamy, Om Thakkar, Rajiv Mathews,\nGalen Andrew, H Brendan McMahan, and Françoise\nBeaufays. 2020. Training production language mod-\nels without memorizing user data. arXiv preprint\narXiv:2009.10031.\nRaphael Shu and Hideki Nakayama. 2017. Compress-\ning word embeddings via deep compositional code\nlearning. arXiv preprint arXiv:1711.01068.\nTFF Authors. 2022a. TensorFlow Federated. https:\n//github.com/tensorflow/federated.\nTFF Authors. 2022b. TensorFlow Federated Stack-\nOverﬂow dataset. https://www.tensorflow.org/\nfederated/api_docs/python/tff/simulation/\ndatasets/stackoverflow.\nTFP Authors. 2022. TensorFlow Privacy. https://\ngithub.com/tensorflow/privacy.\nUS Census Bureau. 2021. Disclosure avoidance for the\n2020 census: An introduction.\nBoxin Wang, Yibo Jacky Zhang, Yuan Cao, Bo Li,\nH Brendan McMahan, Sewoong Oh, Zheng Xu, and\nManzil Zaheer. 2023. Can public large language\nmodels help private cross-device federated learning?\narXiv preprint arXiv:2305.12132.\nJianyu Wang, Zachary Charles, Zheng Xu, Gauri\nJoshi, H Brendan McMahan, Blaise Aguera y Arcas,\nMaruan Al-Shedivat, Galen Andrew, Salman Aves-\ntimehr, Katharine Daly, et al. 2021. A ﬁeld guide to\nfederated optimization. arXiv:2107.06917.\nZheng Xu, Maxwell Collins, Yuxiao Wang, Liviu\nPanait, Sewoong Oh, Sean Augenstein, Ting\nLiu, Florian Schroff, and H Brendan McMahan.\n2022. Learning to generate image embeddings\nwith user-level differential privacy. arXiv preprint\narXiv:2211.10844.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nTimothy Yang, Galen Andrew, Hubert Eichner,\nHaicheng Sun, Wei Li, Nicholas Kong, Daniel Ra-\nmage, and Françoise Beaufays. 2018. Applied fed-\nerated learning: Improving google keyboard query\nsuggestions. arXiv preprint arXiv:1812.02903.\nChen Zhu, Zheng Xu, Mingqing Chen, Jakub Koneˇcn`y,\nAndrew Hard, and Tom Goldstein. 2022. Diurnal or\nnocturnal? federated learning of multi-branch net-\nworks from periodically shifting distributions. In\nInternational Conference on Learning Representa-\ntions.\n637\nA Privacy accounting for adaptive clipping\nTheorem 1 (Privacy Accounting for Adaptive Clipping (Andrew et al., 2021)). One step of DP-FTRL with\nadaptive clipping using σb noise standard deviation on the clipped counts ∑bt\ni and z∆ noise multiplier\non the vector sums ∑∆t\ni is equivalent to one step of non-adaptive DP-FTRL with noise multiplier zif we\nset z∆ =\n(\nz−2 −(2σb)−2)−1/2\n.\nB Implementation.\nWe use the open source implementation of DP-FTRL in Tensorﬂow Privacy (TFP Authors, 2022) integrated\nwith Tensorﬂow Federated (TFF Authors, 2022a) as a DP aggregator for federated learning. Conceptually,\nDP-FTRL adds noise to the summation of updates across rounds, i.e., PrivateSum in Alg. 1. Instead\nof tracking the noise and summation separately, PrivateSum is implemented to only track the noise and\nupdates ˜θt−1 by adding the residual of noise between round tand round t−1. This design makes it easy\nto integrate with various optimizer choices, for example, momentum that is important for utility; and also\nallows ephemeral access of model deltas without directly storing unnoised states.\nC Reporting privacy guarantees\nThis section clariﬁes the nuances of the reported DP guarantees following the guidelines outlined in\n(Ponomareva et al., 2023, Sec. 5.3)\n1. DP setting. This a central DP guarantee where the service provider is trusted to correctly implement\nthe mechanism.\n2. Instantiating the DP Deﬁnition\n(a) Data accesses covered: The DP guarantee applies to all well-behaved clients2 in a single training\nrun. We do not account for hyperparameter tuning in our guarantees. Public multilingual C4\ndata (Raffel et al., 2019; Xue et al., 2020) is used for pre-training.\n(b) Final mechanism output: Only the ﬁnal model checkpoint is released for production launches,\nhowever the mechanism’s output is technically the full sequence of privatized gradients, and\nso the guarantee also applies at this level, and hence all intermediate models are protected\n(including those sent to devices participating in federated learning).\n(c) Unit of privacy. Device-level DP is considered, i.e., the notion of adjacency is with respect\nto arbitrary training datasets on each client device, and the device might have an arbitrarily\nlarge local dataset containing arbitrary training examples. For user’s with a single device, this\ncorresponds directly to user-level DP; for devices shared with multiple users, this provides\na stronger notion of DP than user-level; for a user with multiple devices that happen to both\nparticipate in training the model, the notion is weaker, but group privacy can be used to obtain a\nuser-level guarantee.\n(d) Adjacency deﬁnition for “neigbouring” datasets : We use the zero-out deﬁnition (Kairouz et al.,\n2021b). This is a a special form of the add-or-remove deﬁnition, where neighboring data sets\ndiffer by addition/removal of a single client. In the absence of a client at any training step, we\nassume that the client’s model update gets replaced with the all zeros vector. This assumption\nenforces a subtle modiﬁcation to the traditional deﬁnition of the add/remove notion of DP which\nallows neighboring data sets to have the same number of records.\n3. Privacy accounting details\n(a) Type of accounting used: Both ρ−zCDP (Bun and Steinke, 2016) accounting, and PLD account-\ning (DP Team, 2022) for (ϵ,δ)−DP are used.\n(b) Accounting assumptions : Each client only participates limited times during the training, and\nthere are at least a min-separation number of rounds between two consecutive participation of a\n2Clients that faithfully follow the algorithm including participation limits. Due to the design of the algorithm, a mis-behaved\nclient does not adversely affect the DP guarantee of any well-behaved clients.\n638\nclient, i.e., MaxP and MinS as discussed in Sec. 2.2. Client participation is enforced by a timer\non clients in the cross-device FL system.\n(c) The formal DP statement : The launched Gboard LMs have ρ−zCDP range in (0.2, 2). We\nalso transform zCDP to (ϵ,δ)−DP by PLD accounting (DP Team, 2022): given δ= 10−10, the\nsmallest zCDP ρ= 0.25 corresponds to DP ϵ= 4.49; the largest zCDP ρ= 1.86 corresponds\nto DP ϵ= 13.69.\n(d) Transparency and veriﬁability: We open sourced our core implementation code in TensorFlow\nFederated and Tensorﬂow Privacy. Key portions of the cross-device FL system are also open\nsourced.\n639",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.8408920764923096
    },
    {
      "name": "Computer science",
      "score": 0.5371279120445251
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4673234522342682
    },
    {
      "name": "Differential privacy",
      "score": 0.4349173307418823
    },
    {
      "name": "Differential (mechanical device)",
      "score": 0.4161502718925476
    },
    {
      "name": "Philosophy",
      "score": 0.34024107456207275
    },
    {
      "name": "Engineering",
      "score": 0.1743445098400116
    },
    {
      "name": "Law",
      "score": 0.16198375821113586
    },
    {
      "name": "Political science",
      "score": 0.15597981214523315
    },
    {
      "name": "China",
      "score": 0.11996397376060486
    },
    {
      "name": "Data mining",
      "score": 0.11073830723762512
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 29
}