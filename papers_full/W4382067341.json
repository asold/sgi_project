{
    "title": "A Coarse-to-Fine Feature Match Network Using Transformers for Remote Sensing Image Registration",
    "url": "https://openalex.org/W4382067341",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5078148930",
            "name": "Chenbin Liang",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Shaanxi Normal University",
                "Shandong Institute of Automation",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5076092683",
            "name": "Yunyun Dong",
            "affiliations": [
                "Shaanxi Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A5032228681",
            "name": "Changjun Zhao",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A5064560838",
            "name": "Zengguo Sun",
            "affiliations": [
                "Shaanxi Normal University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3047057232",
        "https://openalex.org/W3159453912",
        "https://openalex.org/W2110375187",
        "https://openalex.org/W2034432063",
        "https://openalex.org/W3010777628",
        "https://openalex.org/W2151103935",
        "https://openalex.org/W2468641423",
        "https://openalex.org/W2996444841",
        "https://openalex.org/W2593936530",
        "https://openalex.org/W3087985223",
        "https://openalex.org/W4362654402",
        "https://openalex.org/W6676770471",
        "https://openalex.org/W6694954494",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W2320444803",
        "https://openalex.org/W6752208963",
        "https://openalex.org/W2991115635",
        "https://openalex.org/W6764386301",
        "https://openalex.org/W2737260104",
        "https://openalex.org/W1869500417",
        "https://openalex.org/W2618039218",
        "https://openalex.org/W2963284197",
        "https://openalex.org/W2558625610",
        "https://openalex.org/W2897093986",
        "https://openalex.org/W3035477606",
        "https://openalex.org/W6754984195",
        "https://openalex.org/W6783308292",
        "https://openalex.org/W3043075211",
        "https://openalex.org/W2963748588",
        "https://openalex.org/W4292183569",
        "https://openalex.org/W3089211228",
        "https://openalex.org/W2781542913",
        "https://openalex.org/W3200870516",
        "https://openalex.org/W4213200979",
        "https://openalex.org/W3166285241",
        "https://openalex.org/W6781533629",
        "https://openalex.org/W3034573343",
        "https://openalex.org/W2964700958",
        "https://openalex.org/W3107240050",
        "https://openalex.org/W2116710882",
        "https://openalex.org/W3034275286",
        "https://openalex.org/W3025573667",
        "https://openalex.org/W4287666213",
        "https://openalex.org/W4287704453"
    ],
    "abstract": "Feature matching is a core step in multi-source remote sensing image registration approaches based on feature. However, for existing methods, whether traditional classical SIFT algorithm or deep learning-based methods, they essentially rely on generating descriptors from local regions of feature points, which can lead to low matching success rates due to various challenges, including gray-scale changes, content changes, local similarity, and occlusions between images. Inspired by the human approach of finding rough corresponding regions globally and then carefully comparing local regions, and the excellent global attention property of transformers, the proposed feature matching network adopts a coarse-to-fine matching strategy that utilizes both global and local information between images to predict corresponding feature points. Importantly, the network has great flexibility of matching corresponding points for any feature points and can be effectively trained without strong supervised signals of corresponding feature points and only require the true geometric transformation between images. The qualitative experiment illustrate the effectiveness of the proposed network by matching feature points extracted by SIFT or sampled uniformly. In the quantitative experiments, we used feature points extracted by SIFT, SuperPoint, and LoFTR as the keypoints to be matched. We then calculated the mean match success ratio (MSR) and mean reprojection error (MRE) of each method at different thresholds in the test dataset. Additionally, boxplot graphs were plotted to visualize the distributions. By comparing the MSR and MRE values as well as their distributions with other methods, we can conclude that the proposed method consistently outperforms the comparison methods in terms of MSR at different thresholds. Moreover, the MSR of the proposed method remains within a reasonable range compared to the MRE of other methods.",
    "full_text": null
}