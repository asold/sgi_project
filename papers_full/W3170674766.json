{
    "title": "Video Instance Segmentation using Inter-Frame Communication Transformers",
    "url": "https://openalex.org/W3170674766",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4225608478",
            "name": "Hwang, Sukjun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4281993909",
            "name": "Heo, Miran",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3091520060",
            "name": "Oh, Seoung Wug",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2755910718",
            "name": "Kim, Seon Joo",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2599765304",
        "https://openalex.org/W2743473392",
        "https://openalex.org/W2950477723",
        "https://openalex.org/W2997277830",
        "https://openalex.org/W3111250322",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W3126721948",
        "https://openalex.org/W3030520226",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2772955562",
        "https://openalex.org/W3137206544",
        "https://openalex.org/W2432481613",
        "https://openalex.org/W2999219213",
        "https://openalex.org/W2963155035",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3136635488",
        "https://openalex.org/W3011350702",
        "https://openalex.org/W2928502868",
        "https://openalex.org/W3034499084",
        "https://openalex.org/W3011199263",
        "https://openalex.org/W3118212025",
        "https://openalex.org/W3139267983",
        "https://openalex.org/W607748843",
        "https://openalex.org/W3108995912",
        "https://openalex.org/W3045495919",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3168649818",
        "https://openalex.org/W3012524094",
        "https://openalex.org/W2944644794",
        "https://openalex.org/W2908510526"
    ],
    "abstract": "We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. Recently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. However, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality. In this work, we propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip. Specifically, we propose to utilize concise memory tokens as a mean of conveying information as well as summarizing each frame scene. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens. We validate our method on the latest benchmark sets and achieved the state-of-the-art performance (AP 44.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). Our method can also be applied to near-online inference for processing a video in real-time with only a small delay. The code will be made available.",
    "full_text": "Video Instance Segmentation using\nInter-Frame Communication Transformers\nSukjun Hwang1 Miran Heo1 Seoung Wug Oh2 Seon Joo Kim1\n1Yonsei University 2Adobe Research\n{sj.hwang, miran, seonjookim}@yonsei.ac.kr seoh@adobe.com\nAbstract\nWe propose a novel end-to-end solution for video instance segmentation (VIS)\nbased on transformers. Recently, the per-clip pipeline shows superior performance\nover per-frame methods leveraging richer information from multiple frames. How-\never, previous per-clip models require heavy computation and memory usage to\nachieve frame-to-frame communications, limiting practicality. In this work, we pro-\npose Inter-frame Communication Transformers (IFC), which signiÔ¨Åcantly reduces\nthe overhead for information-passing between frames by efÔ¨Åciently encoding the\ncontext within the input clip. SpeciÔ¨Åcally, we propose to utilize concise memory\ntokens as a mean of conveying information as well as summarizing each frame\nscene. The features of each frame are enriched and correlated with other frames\nthrough exchange of information between the precisely encoded memory tokens.\nWe validate our method on the latest benchmark sets and achieved the state-of-the-\nart performance (AP 44.6 on YouTube-VIS 2019 val set using the ofÔ¨Çine inference)\nwhile having a considerably fast runtime (89.4 FPS). Our method can also be\napplied to near-online inference for processing a video in real-time with only a\nsmall delay. The code will be made available.\n1 Introduction\nWith the growing interest toward the video domain in computer vision, the task of video instance\nsegmentation (VIS) is emerging [1]. Most of the current approaches [1, 2, 3, 4] extend image instance\nsegmentation models [5, 6, 7, 8] and take frame-wise inputs. These per-frame methods extend the\nconcept of temporal tracking by matching frame-wise predictions of high similarities. The models\ncan be easily customized to real-world applications as they run in an online [9] fashion, but they show\nlimitation in dealing with occlusions and motion blur that are common in videos.\nOn the contrary, per-clip models are designed to overcome such challenges by incorporating multiple\nframes while sacriÔ¨Åcing the efÔ¨Åciency. Previous per-clip approaches [10, 11, 12] aggregate informa-\ntion within a clip to generate instance-speciÔ¨Åc features. As the features are generated per instance,\nthe number of instances in addition to the number of frames has a signiÔ¨Åcant impact on the overall\ncomputation. Recently proposed VisTR [11] adapted DETR [13] to the VIS task and reduced the\ninference time by inserting the entire video, not a clip, to its ofÔ¨Çine end-to-end network. However, its\nfull self-attention transformers [14] over the space-time inputs involve explosive computations and\nmemories. In this work, we raise the following question: can a per-clip method be efÔ¨Åcient while\nattaining great accuracy?\nTo achieve our goal, we introduce Inter-frame Communication Transformers (IFC) to greatly reduce\nthe computations of the full space-time transformers. Similar to recent works [15, 16] that alleviate\nthe explosive computational growth inherent in attention-based models [ 17, 14], IFC takes a de-\ncomposition strategy utilizing two transformers. The Ô¨Årst transformer (Encode-Receive, E) encodes\nPreprint. Under review.\narXiv:2106.03299v1  [cs.CV]  7 Jun 2021\neach frame independently. To exchange the information between frames, the second transformer\n(Gather-Communicate, G) executes attention between a small number of memory tokens that hold\nconcise information of the clip. The memory tokens are utilized to store the overall context of the clip,\nfor example ‚Äúa hand over a lizard‚Äù in Fig. 1. The concise information assists detecting the lizard\nthat is largely occluded by the hand in the Ô¨Årst frame, without employing an expensive pixel-level\nattention over space and time. The memory tokens are only in charge of the communications between\nframes, and the features of each frame are enriched and correlated through the memory tokens.\nWe further reduce overheads while taking advantages of per-clip pipelines by concisely representing\neach instance with a unique convolutional weight [7]. Despite the changes of appearances at different\nframes, the instances of the same identity share commonalities because the frames are originated from\nthe same source video. Therefore, we can effectively capture instance-speciÔ¨Åc characteristics in a clip\nwith dynamically generated convolutional weights. In companion with the segmentation, we track\ninstances by uniformly applying the weights to all frames in a clip. Moreover, all executions of our\nspatial decoder are instance-agnostic except for the Ô¨Ånal layer which applies instance-speciÔ¨Åc weights.\nAccordingly, our model is highly efÔ¨Åcient and also suitable for scenes with numerous instances.\nIn addition to the efÔ¨Åcient modeling, we provide optimizations and an instance tracking algorithm\nthat are designed to be VIS-centric. By the deÔ¨Ånition of APVIS, the VIS task [1] aims to maximize\nthe objective similarity: space-time mask IoU. Inspired by previous works [13, 18, 19], our model is\noptimized to maximize the similarity between bipartitely matched pairs of ground truth masks and\npredicted masks. Furthermore, we again adopt the similarity maximization for tracking instances of\nsame identities, which effectively links predicted space-time masks using bipartite matching. As both\nof our training and inference algorithm are fundamentally designed to address the key challenge of\nVIS task, our method attains an outstanding accuracy.\nFrom these improvements, IFC sets the new state-of-the-art by using ResNet-50: 42.8% AP and more\nsurprisingly, in 107.1 fps. Furthermore, our model also shows great speed-accuracy balance under\nnear-online setting, which leads to a huge practicality. We believe that our model can be a powerful\nbaseline for video instance segmentation approaches that follow the per-clip execution.\n2 Related Work\nVideo instance segmentation The VIS task [1] extends the concept of tracking to the image in-\nstance segmentation task. The early solutions [1, 2] follow per-frame pipeline, which utilize additional\ntracking head to the models that are mainly designed to solve image instance segmentation. More\nadvanced algorithms that are recently proposed [4, 3] take video characteristics into consideration,\nwhich result in improved performance.\nPer-clip models [ 12, 10, 11] dedicate computations to extract information from multiple frames\nfor higher accuracy. By exploiting multiple frames, per-clip models can effectively handle typical\nchallenges in video, i.e., motion blurs and occlusions. Our model is designed to be highly efÔ¨Åcient\nwhile following the per-clip pipeline, which leads to fast and accurate predictions.\nTransformers Recently, transformers [14] are greatly impacting many tasks in computer vision.\nAfter the huge success of DETR [13], which has brought a new paradigm to the object detection task,\nnumerous vision tasks are incorporating transformers [20, 21] in place of CNNs. For classiÔ¨Åcation\ntasks in both NLP and computer vision, many adopt an extra classiÔ¨Åcation token to the input of\ntransformers [22, 20]. All the input tokens affect each other as the encoders are mainly composed of\nthe self-attention, thus the classiÔ¨Åcation token can be used to determine the class of the overall input.\nMaX-DeepLab [19] adopted the concept of memory and proposed a novel dual-path transformer\nfor the panoptic segmentation task [23]. By making use of numerous memory tokens similar to the\nprevious classiÔ¨Åcation token, MaX-DeepLab integrates the transformer and the CNN by making both\nfeedback itself and the other.\nWe further utilize the concept of the memory tokens to the videos. Using Inter-frame Communication\nTransformers, each frame runs independently while sharing their information with interim communi-\ncations. The communications lead to higher accuracy while the execution independence between\nframes accelerates the inference.\n2\n‚Ä¶\nTransformerDecoder\nEncoder output\n‚úïND\n‚àÖ‚àÖ\n‚Ä¶\nP\n‚Ä¶ ‚Ä¶\nCNN PCNN PCNN\n‚Ä¶ ‚Ä¶\n‚Ä¶‚Ä¶\nObject queries\nTransformerEncoderTransformerEncoderTransformerEncoder\n‚úïNE\nP2D positional encodingMemory tokenFrame token\n‚àó ‚àó ‚àó\n‚Ä¶‚Ä¶\nTransformerEncoder‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶‚Ä¶‚Ä¶\nùëì!\" ‚Ä¶ ‚Ä¶ùëì#\"\n‚Ñ∞\nùí¢\nùëì!\"\nùëì$\"\nùëì!! ùëì#!\nùëö!\"\n#ùëö!! #ùëö#!\nùëö!\nùëì$\" ùëö$\" ùëì#\" ùëö#\"\nùëì$! #ùëö$!\nùëö$%ùê∏ùëì!%ùê∏ ùëö!%ùê∏ ùëö#%ùê∏ùëì$%ùê∏ ùëì#%ùê∏\nùëì%,ùëö%ùê∏ ùê∏\nSpatialDecoder\nFigure 1: Overview of IFC framework. Our transformer encoder block has two components: 1)\nEncode-Receive (E) simultaneously encodes frame tokens and memory tokens. 2) Only memory\ntokens pass Gather-Communicate (G) to perform communications between frames. The outputs from\nthe stack of NE encoder blocks goes into two modules, spatial decoder and transformer decoder, to\ngenerate segmentation masks.\n3 Method\nThe proposed method follows a per-clip pipeline which takes a video clip as input and outputs\nclip-level results. We also introduce Inter-frame Communication Transformers, which can effectively\nshare frame-wise information within a clip with a high efÔ¨Åciency.\n3.1 Model architecture\nInspired by DETR [13], our network consists of a CNN backbone and transformer encoder-decoder\nlayers (Fig. 1). The input clip is Ô¨Årst independently embedded into a feature map through the backbone.\nThen, the embedded clip passes through our inter-frame communication encoder blocks that enrich\nthe feature map by allowing information exchange between frames. Next, a set of transformer decoder\nlayers that take the encoder outputs and object queries as inputs predict unique convolutional weights\nfor each instance in the clip. Finally, the masks for each instance across the clip are computed in one\nshot by convolving the encoded feature map with the unique convolutional weight.\nBackbone Given an input clip {xi}T\ni=1 ‚ààRT√óH0√óW0√ó3, composed of T frames with 3 color\nchannels, the CNN backbone processes the input clip frame-by-frame. As the result, the clip is\nencoded into a set of low-resolution features, {f0\ni }T\ni=1 ‚ààRT√óH√óW√óC, where C is the number of\nchannels and H,W = H0\n32 ,W0\n32 .\nInter-Frame Communication Encoder Given an image, humans can effortlessly summarize the\nscene with only a few words. Also, frames from a same video share a lot of commonalities, the\ndifference between them is sufÔ¨Åciently summarized and communicated even with a small bandwidth.\nBased on this hypothesis, we propose an inter-frame communication encoder to make the computation\nto be mostly frame-wise independent with some communications between frames. SpeciÔ¨Åcally, we\nadopt memory tokens for both summarizing per-frame scenes and the means of communications.\nOur encoder blocks are composed of two phases of separate transformers: Encode-Receive ( E)\nand Gather-Communicate (G). Both Encode-Receive and Gather-Communicate follow the typical\ntransformer encoder architecture [14], which consists of an addition of Ô¨Åxed positional encoding, a\nmulti-head self-attention module, and a feed forward network.\nEncode-Receive operates in a per-frame manner, taking a frame-level feature map and corresponding\nmemory tokens. Passing through Encode-Receive, we expect two functionalities: (1) image features\nencode per-frame information to the memory tokens, and (2) image features receive information of\ndifferent frames that are gathered in the memory tokens. Gather-Communicate operates across frames\n3\nTable 1: Complexity comparison. Various transformer encoders for space-time input. As the overall\nFLOPs can vary by the number of detected instances, listed values are measured only at the encoders.\nCommunication Type Complexity per Layer\nFLOPs (G)1\n360 √ó640 720 √ó1280\nT=5 T=36 T=5 T=36\nNo Comm O(C2THW + CT (HW )2) 5.17 37.23 24.62 177.29\nFull THW O(C2THW + C(THW )2) 6.94 148.70 50.63 1815.38\nDecompose T-HW O(C2THW + CT (HW )2 + CT 2HW ) 8.33 60.24 36.73 265.50\nIFC (M = 8) O(C2THW + CT (HW )2) 5.52 39.73 25.05 180.39\nto form a clip-level knowledge. It takes the memory tokens from each frame as inputs and performs\ncommunications between frames. Alternating two phases through multiple layers, the encoder can\nefÔ¨Åciently learn consensus representations across frames.\nIn more detail, given the frame embedding {f0\ni }T\ni=1, we spatially Ô¨Çatten each feature RH√óW√óC ‚Üí\nRHW√óC. The initial memory tokens m0 of size M are copied per frame and concatenated to each\nframe feature as follows:\n[f0\nt,m0\nt] ‚ààR(HW+M)√óC, t ‚àà{1,2,¬∑¬∑¬∑,T}, (1)\nwhere [¬∑,¬∑] indicates a concatenation of two feature vectors. Note that the initial memory tokens m0\nare trainable parameters learnt during training.\nThe Ô¨Årst phase of IFC is Encode-Receive, which processes frames individually as follows:\n[fl\nt, ÀÜml\nt] =El([fl‚àí1\nt ,ml‚àí1\nt ]), (2)\nwhere El denotes the l-th Encode-Receive layer. With a self-attention computed over the frame pixel\nlocations and the memory tokens, the information of each frame can be passed to the memory tokens\nand vise-versa.\nThe outputs of Encode-Receive are grouped by memory indices and formulate the inputs for Gather-\nCommunicate layer. The grouping can be understood as a decomposition of memory tokens, and\nbecomes computationally beneÔ¨Åcial when the total size of gathered memory tokens increases.\n[ml\n1(i),ml\n2(i),¬∑¬∑¬∑,ml\nT(i)] =Gl([ ÀÜml\n1(i), ÀÜml\n2(i),¬∑¬∑¬∑, ÀÜml\nT(i)]), i ‚àà{1,2,¬∑¬∑¬∑,M}, (3)\nwhere Gl denotes the l-th Gather-Communicate layer. The processed outputs are redistributed by\nthe originated frame and get concatenated as mt = [mt(1),mt(2),¬∑¬∑¬∑,mt(M)]. Unlike Encode-\nReceive, Gather-Communicate utilizes the attention mechanism to convey the information from\ndifferent frames over the input clip.\nDeÔ¨Åning a the l-th inter-frame encoder block (IFCl) as El followed by Gl, the stack of NE encoder\nblocks can be inductively formulated as:\n[fl\nt,ml\nt] =IFCl([fl‚àí1\nt ,ml‚àí1\nt ]), 1 ‚â§l‚â§NE, (4)\nwhere [fNE\nt ,mNE\nt ] is the Ô¨Ånal result. The stacking of multiple encoders brings communications\nbetween frames, thus each frame can have coincidence to the other, specifying the identities of\ninstances in a given clip.\nComplexity comparison. In Table 1, we analyze the computational complexity of transformer\nencoder variants applied for video input in terms of the Big-O complexity and FLOPs. The complexity\nof the original transformer encoder layer [14] is O(C2N + CN2), where N is the number of inputs.\nWithout any communication between frames,No Comm, it shows the smallest amount of computation\n(O(C2THW + CT(HW)2)). As indicated as Full THW in Table 1, the complexity of VisTR [11]\nthat performs a full space-time self-attention is O(C2(THW ) +C(THW )2) thus either a higher\nresolution or an increase of number of input frames leads to a massive increase in computations. VisTR\nbypasses the problem by highly reducing the input resolution and utilizing GPUs with tremendous\n1Measured using flop_count function of fvcore==0.1.5.\n4\nmemory capacity. However, as such solutions cannot resolve the fundamental issues, it is impractical\nto real-world videos. Moreover, VisTR remains as a complete ofÔ¨Çine strategy because it takes the\nentire video as an input.\nAn intriguing improvement for the na√Øve full self-attention would be the decomposition of the attention\ninto space and time axis [24, 16]. In Decompose T-HW, we decompose attention computation into\nspatial and temporal attention. The complexity of the separation of space-time leads to the sum of the\ntwo transformer encoder: O(T(C2(HW) +C(HW)2)) and O(HW(C2T+ CT2)). In comparison\nto the full self-attention, the decomposition lowers the computational growth relative to the number\nof frames.\nOur encoder, IFC, that communicates between frames using the memory tokens leads to a huge\nbeneÔ¨Åt to the total computations adding only a small amount of computation over No Comm while\nproviding sufÔ¨Åcient channels for communication. The complexity of each phase in our proposed\nencoder is: O(C2T(HW + M) +CT(HW + M)2) for Encode-Receive and O(C2TM + CT2M)\nfor Gather-Communicate respectively. Assuming that M is kept small (e.g., 8), the computation\nneeded for Gather-Communicate can be neglected, while the complexity of Encode-Receive can be\napproximated to O(C2THW +CT(HW)2) as shown in Table 1. Finally, with respect to the number\nof frames of the input, we can expect approximate linear increase rather than the high increase of\ncomputation occurred in VisTR.\nDecoders and output headsAs depicted in Fig. 1, the transformer decoder of our model is stacked\nwith ND layers [14]. Contrary to VisTR, where the number of object queries increases proportionally\nto the number of frames, our model receives learnt encodings of Ô¨Åxed sizeNq for object queries. Also,\nby utilizing these encodings throughout the entire frames, our model can effectively deal with clips\nof various lengths. A set of projection matrices are applied to {fNE\nt ,mNE\nt }T\nt=1 for the generation of\nkeys and values. The object queries turn into output embeddings by the transformer decoder, and the\nembeddings are eventually used as an input to the output heads.\nThere are two output heads on top of the transformer decoder, a class head and a segmentation head,\neach composed of two fully-connected layers. The output embeddings from the transformer decoder\nare independently inserted to the heads, resulting in Nq predictions per a clip. The class head outputs\na class probability distribution of instances ÀÜp(c) ‚ààRNq√ó|C|. Note that the possible classes C ‚àãc\ninclude no object ‚àÖ class in addition to the given classes of a dataset.\nThe segmentation head generates Nq conditional convolutional weights w ‚ààRNq√óC in a manner\nsimilar to [19, 7]. For the conditional convolution, the output feature of the encoder {fNE\nt }T\nt=1 is\nreused by undoing the Ô¨Çatten operation. For the upsampling, the encoder feature passes through\nfpn-style [25] spatial decoder without temporal connections resulting in T feature maps that are 1/8\nof the input resolution. Finally, the resulting feature maps are convolved with each convolutional\nweights to generate segmentation mask as follows:\nÀÜsi = {f‚Ä≤\nt ‚ó¶wi}T\nt=1, (5)\nwhere wi is i-th convolutional weight, ‚ó¶indicate 1 √ó1 spatial convolution operation, and the result ÀÜsi\nis a spatial-temporal object mask in shape of RT√óH‚Ä≤√óW‚Ä≤\nwhere H‚Ä≤= H0\n8 , W‚Ä≤= W0\n8 . Note that, for\nan instance, a common weight is applied throughout the video clip. Our spatial decoder is of instance-\nagnostic design which gets highly efÔ¨Åcient than decoders of instance-speciÔ¨Åc designs [13, 11, 12, 10]\nas the number of detected instances increases. Meanwhile, thanks to our segmentation head which\nspeciÔ¨Åes and captures the characteristics of an instance, IFC can conduct both segmentation and\ntracking at once within a clip.\n3.2 Instance matching and loss\nTo train our network, we Ô¨Årst assign the ground truth for each instance estimation and then a set of\nloss function between each the ground truth and prediction pair. For a given input clip, our model\ngenerate a Ô¨Åxed-size set of class-labeled masks {ÀÜyi}Nq\ni=1 = {(ÀÜpi(c),ÀÜsi)}Nq\ni=1. The ground truth set of\nthe clip can be represented as yi = (ci,si); ci is the target class label including ‚àÖ, and si is the target\nmask which is down-sampled to the size of the prediction masks for efÔ¨Åcient similarity calculation.\nOne-to-one bipartite matching between the prediction set {ÀÜyi}Nq\ni=1 and the ground truth set {yi}K\ni=1 is\nperformed to Ô¨Ånd the best assignment of a prediction to a ground truth. The objective can be formally\n5\ndescribed as:\nÀÜœÉ= arg max\nœÉ‚ààSNq\nK‚àë\ni=1\nsim(yi,ÀÜyœÉ(i)), (6)\nwhere sim(yi,ÀÜyœÉ(i)) refers a pair-wise similarity over a permutation of œÉ ‚ààSNq . Following prior\nwork [ 26, 13, 19], the bipartite matching is efÔ¨Åciently computed using Hungarian algorithm [ 18].\nWe Ô¨Ånd that box-based similarity measurement as used in DETR [13] shows weaknesses in matching\ninstances in video clip due to the case of occlusion and disappear-and-reappear. Therefore, we deÔ¨Åne\nsim(yi,ÀÜyœÉ(i)) to be mask-based term as1{ciÃ∏=‚àÖ}[ÀÜpœÉ(i)(ci)+ Œª0DICE (si,ÀÜsœÉ(i))], where DICE denotes\ndice coefÔ¨Åcients [27].\nGiven the optimal assignment ÀÜœÉ, we refer to the Kmatched predictions and (Nq ‚àíK) non-matched\npredictions as positive and negative pairs respectively. The positive pairs aim to predict the ground\ntruth masks and classes while the negative pairs are optimized to predict the ‚àÖ class. The Ô¨Ånal loss is\na sum of the losses from positive pairs and negative pairs where each can be computed as follows:\nLpos =\nK‚àë\ni=1\n[‚àílog ÀÜpÀÜœÉ(i)(ci)Ó¥ô Ó¥òÓ¥ó Ó¥ö\nCross-entropy loss\n+Œª1(1 ‚àíDICE (si,ÀÜsÀÜœÉ(i))Ó¥ô Ó¥òÓ¥ó Ó¥ö\nDice loss [27]\n) +Œª2 FOCAL (si,ÀÜsÀÜœÉ(i))Ó¥ô Ó¥òÓ¥ó Ó¥ö\nSigmoid-focal loss [28]\n],\nLneg =\nNq‚àë\ni=k+1\n[‚àílog ÀÜpÀÜœÉ(i)(‚àÖ)].\n(7)\nAs (Nq ‚àíK) is likely to be much greater than K, we down-weight Lneg by a factor of 10 to resolve\nthe imbalance, following prior work [13]. The goal of video instance segmentation [1] is to maximize\nthe space-time IoU between a prediction and a ground truth mask. Therefore, our mask-related losses\n(Dice loss and Sigmoid-focal loss) are spatio-temporally calculated over an entire clip, rather than\naveraging the losses that are accumulated frame-by-frame.\n3.3 Clip-level instance tracking\nTo infer a video input that is longer than the clip length, we match instances using the predicted\nmasks of overlapping frames. Let YI and YA be the result sets of clip I and Aexcluding the ‚àÖ\nclass. The goal is to perform matching of same identities between pre-collected instance set YI and\nYA. We Ô¨Årst calculate the matching scores which are space-time soft IoU at intersecting frames\nbetween YI and YA. Then, we Ô¨Ånd optimal paired indices ÀÜœÉSusing Hungarian algorithm [18] to\nthe gathered matching score S‚àà [0,1]|YI|√ó|YA|. We update YI(i) by concatenating YA(ÀÜœÉS(i)) if\nS(i,ÀÜœÉS(i)) is above a certain threshold, and add non-matched prediction sets to YI as new instances.\nNote that a previous per-clip model (MaskProp [ 10]) also utilizes soft IoU for tracking instances,\nbut the matching scores are computed per-frame and averaged for intersecting frames. Different\nfrom MaskProp, using space-time soft IoU leads to an accurate tracking as it can better represent\nthe deÔ¨Ånition of mask similarities between clips which brings at most 2% AP increase. The overall\ntracking pipeline can be effectively implemented in GPU-friendly manner.\n4 Experiments\nIn this section, we evaluate the proposed method using YouTube-VIS 2019 and 2021 [ 1]. We\ndemonstrate the effectiveness of our model regarding both accuracy and speed. We further examine\nhow different settings affect the overall performance and efÔ¨Åciency of IFC encoder. Unless speciÔ¨Åed,\nall models for measurements used NE = 3,ND = 3, stride of 1, and ResNet-50.\n4.1 Implementation Details\nWe used detectron2 [32] for our code basis, and hyper-parameters mostly follow the settings of\nDETR [13] unless speciÔ¨Åed. We used AdamW [33] optimizer with initial learning rate of 10‚àí4 for\ntransformers, and 10‚àí5 for backbone. We Ô¨Årst pre-train the model for image instance segmentation\non COCO [34] by setting our model to T = 1. The pre-train procedure follows the shortened training\nschedule of DETR [13], which runs 300 epochs with a decay of the learning rate by a factor of 10 at\n200 epochs. Using the pre-trained weights, the models are trained on targeted dataset using the batch\n6\nTable 2: Evaluations on various settings.\n(a) AP and FPS on YouTube-VIS 2019val set. For fairness, FPS is measured on a same machine, using a single\nRTX 2080Ti GPU. We used the ofÔ¨Åcial codes and checkpoints provided by the authors for the measurements.\nWe report the clip settings of [11, 10]. T: window size.\nMethod (Settings) Backbone [29] FPS 2 AP AP 50 AP75 AR1 AR10\nMaskTrack R-CNN [1] ResNet-50 26.1 30.3 51.1 32.6 31.0 35.5\nMaskTrack R-CNN [1] ResNet-101 - 31.8 53.0 33.6 33.2 37.6\nSipMask [2] ResNet-50 35.5 33.7 54.1 35.8 35.4 40.1\nSG-Net [4] ResNet-50 - 34.8 56.1 36.8 35.8 40.8\nSG-Net [4] ResNet-101 - 36.3 57.1 39.6 35.9 43.0\nCrossVIS [3] ResNet-50 - 36.3 56.8 38.9 35.6 40.7\nCrossVIS [3] ResNet-101 - 36.6 57.3 39.7 36.0 42.0\nSTEm-Seg [30] ResNet-101 3.0 34.6 55.8 37.9 34.4 41.6\nCompFeat [31] ResNet-50 - 35.3 56.0 38.6 33.1 40.3\nVisTR [11] ( T=36) ResNet-50 51.1 36.2 59.8 36.9 37.2 42.4\nVisTR [11] ( T=36) ResNet-101 43.5 40.1 64.0 45.0 38.3 44.9\nMaskProp [10] ( T=13) ResNet-50 - 40.0 - 42.9 - -\nMaskProp [10] ( T=13) ResNet-101 - 42.5 - 45.6 - -\nOursnear-online (T=5) ResNet-50 46.5 41.0 62.1 45.4 43.5 52.7\nOursofÔ¨Çine (T=36) ResNet-50 107.1 42.8 65.8 46.8 43.8 51.2\nOursofÔ¨Çine (T=36) ResNet-101 89.4 44.6 69.2 49.5 44.0 52.1\n(b) Accuracy on YTVIS 2021 val set\nAP AP 50 AP75\nMaskTrack-RCNN 28.6 48.9 29.6\nSipMask 31.7 52.5 34.0\nCrossVIS 34.2 54.4 37.9\nOurs 36.6 57.9 39.3\n(c) Bipartite matching\nAP\nBox-based 37.2\nMask-based 39.4\n(d) Effect of strides\nAP AP 75 FPS\nT = 5 S = 3 40.9 45.0 72.7\nT = 10 S = 5 41.1 44.5 83.0\nT = 15 S = 8 42.0 45.9 92.5\nT = 20 S = 10 42.4 46.9 95.7\nsize of 16, each clip composed of T = 5frames downscaled to either 360p or 480p. The models are\ntrained for 8 epochs, and decays the learning rate by 10 at 6th epoch. For the evaluation, the input\nvideos are downscaled to 360p, which follows MaskTrack R-CNN [1].\n4.2 Main Results\nYouTube-VIS 2019 evaluation results. We compare our proposed IFC to the state-of-the-art\nmodels in the video instance segmentation task on YouTube-VIS 2019val in Table 2 (a). We measure\nthe accuracy by AP and our model sets the highest score among all online, near-online, and ofÔ¨Çine\nmodels while presenting the fastest runtime. As mentioned earlier, IFC is highly efÔ¨Åcient during\nthe inference thanks to three advantages: (1) memory token-based decomposition for transformer\nencoder (2) instance-agnostic spatial decoder (3) GPU-friendly instance matching. Moreover, our\nmodel does not make use of any heavy modules such as deformable convolutions [35] or cascading\nnetworks [36]. Thanks to these advantages, IFC achieves an outstanding runtime, which is faster\nspeed than online models [1, 2].\nDuring the inference, our method is able to freely adjust the length of the clip (T) as needed. If the\ninput clip length is set to contain entire video frames, our method becomes an ofÔ¨Çine method (like\nVisTR [11]) that processes the entire video in one shot. As the ofÔ¨Çine inference can skip matching\nbetween clips and maximize the GPU utilization, our method represents surprisingly fast runtime\n(107.1 FPS). On the other hand, if the application requires instant outputs given a video stream, we\ncan reduce the clip length to make our method near-online. In the near-online scenario with T = 5,\nour system is still able to process a video in real-time (46.5 FPS) with only a small delay.\nYouTube-VIS 2021 evaluation results.The recently introduced dataset YouTube-VIS 2021 is an\nimproved version of YouTube-VIS 2019. The newly added videos in the dataset include higher\n2We follow detectron2 [32] for measuring FPS.\n7\nTable 3: Encoder variations. We show how different encoders affect the overall performance.\n(a) Various encoders taking clips of different lengths (see Table 1)\nT=5 T=10 T=15 T=20\nAP AP 75 FPS AP AP 75 FPS AP AP 75 FPS AP AP 75 FPS\nNo Comm 37.4 39.9 38.1 38.8 41.6 40.8 39.3 41.7 46.7 39.6 41.9 52.9\nFull THW 37.2 40.0 37.6 38.8 41.2 35.5 39.8 42.6 32.9 39.7 42.8 34.8\nDecomp T-HW 37.2 39.8 35.7 38.3 40.9 37.9 38.5 41.5 42.6 39.0 41.9 49.4\nIFC 39.0 42.7 36.3 39.6 43.0 38.9 39.8 43.0 43.7 40.4 43.4 50.2\n(b) Image instance segmenta-\ntion on COCO val set\nAPCOCO APCOCO\n50\nw/o mem 35.0 56.6\nw/ mem 35.1 56.5\n(c) Number of memory tokens (AP)\nT=5 T=10 T=15 T=20\nM=1 37.6 39.2 39.4 39.4\nM=2 37.9 39.2 39.6 39.8\nM=4 38.0 39.5 39.7 39.9\nM=8 39.0 39.6 39.8 40.4\nM=16 38.1 39.1 39.7 39.9\n(d) Index-wise memory decomposition\nT=5 T=10 T=15 T=20\nUniÔ¨Åed 38.1 38.9 39.7 39.9\nDecomp 39.0 39.6 39.8 40.4\nnumber of instances and frames. In Table 2 (b), we refer the results reported in [3], which evaluated\n[1, 2] using ofÔ¨Åcial implementations. Again, our model achieves the best performance.\n4.3 Ablation Study\nIn this section, we provide ablation studies and discuss how different settings impact the overall\nperformance. The experiments are conducted using YouTube-VIS 2019 val set. For every ablation\nstudies, we report the mean of Ô¨Åve runs as the results may vary by each run due to the insufÔ¨Åcient\nnumber of training and testing set of YouTube-VIS dataset.\nBox-based and mask-based bipartite matching.We observe how the different policies for bipar-\ntite matching affect the performance. As our model does is a box-free method, we adjust our model\nto predict bounding boxes similar to VisTR [11] and conduct bipartite matching [18, 13] using the\npredicted boxes. The change of optimization from mask-based to box-based brings a noticeable\nperformance drop as shown in Table 2 (c). With the VIS-centric design, the mask-based optimization\nshows more robustness than box-based optimizations under typical video circumstances such as\ninstances with heavy overlaps and partial occlusions.\nDiffering window strides. In addition to the clip length ( T), we further optimize our runtime\nplacing a stride Sbetween clips, as shown in Table 2 (d). IFC can be used in a near-online manner,\nwhich takes clips that are consecutively extracted from a video. The placement of a larger stride\nreduces temporal intersections, which lessens computational overheads but also causes difÔ¨Åculty in\nmatching instances. By enlarging the stride from S = 1to S = 3, IFC accomplishes approximately\n150% speed improvement with only 0.1% AP drop. The tendency of high speed gain and low accuracy\ndrop persists under various conditions. Therefore, our model can be applied to conditions where\nthe enlargement of strides is necessary, i.e., using devices that are not powerful enough but has to\nmaintain high inference speed.\nVarious decomposition strategies of encoders.In Table 1, we observed the computational gaps\nderived from the decomposition of the encoder layers. Extending Table 1, we now investigate the\nhow the decomposition strategies affect the accuracy in Table 3.\nThe models are evaluated with variety of window sizes (T = 5,10,15,20) as an increase of window\nsize T has pros and cons. When matching predictions from different clips, greater T is advantageous\ndue to an enlargement of temporal intersections between clips. On the contrary, frames in longer clips\nare likely to be composed of diverse appearances, which disrupt tracking and segmenting instances\nwithin a clip. Therefore, the key to the performance enhancement is to cope with the appearance\nchanges by precisely encoding and correlating space-time inputs.\nAs shown in Table 3 (a), the full self-attention [11] surpasses the encoder without communications as\nthe length of clips increase. However, the enlargement of the window size highly slows down the\n8\nFigure 2: Visualizations of results and attention maps of memory tokens.\ninference speed, and the improvements are marginal that the tremendous computation and memory\nusage cannot be compensated. The decomposition of space-time maintains comparable speed even if\nthe window is large, but fails to achieve high accuracy.\nOur model is shows fast inference as the only additional computations of IFC are from utilizing a\nsmall number of memory tokens. Furthermore, by effectively encoding the space-time inputs with\nthe communications between frames, IFC can take advantages of enlarging the window size, and\nsurpasses other encoders.\nMemory tokens. We also study the effects of utilizing memory tokens. As mentioned, the motiva-\ntion of using the memory tokens is to build communications between frames. Different from the video\ninstance segmentation task, the image segmentation task is consisted of a single frame. Therefore,\nthe use of the memory tokens does not lead to improvements to the image instance segmentation\ntask as mutual communications cannot be solely made (see Table 3 (b)). Meanwhile, the utilization\nof the memory tokens achieves great improvements by effectively passing the information between\nframes. Results in Table 3 (a, c) demonstrate that the use of memory tokens achieves higher accuracy\nthan the encoder without any communications ( No comm), which emphasizes the importance of\nthe communications. We evaluate how the size of the memory tokens affect the overall accuracy\nin Table 3 (c) and set the default size of the tokens M to be 8.\nIn Section 3.1, we demonstrated the formulation of the inputs for Gather-Communicate layer, which\ngroups the outputs of Encode-Receive by memory indices. As aforementioned, the formulation can\nbe considered as a decomposition of memory tokens: insertion to the Gather-Communicate layer by\nseparate M groups each consisting of T tokens. In Table 3 (d), we investigate the impact of inserting\nthe uniÔ¨Åed MT tokens as a whole. Compared to the uniÔ¨Åed insertion, the decomposition brings better\naccuracy as the memories of same indices have more correspondences, which ease the encoders to\nbuild attentions in between.\nWe choose a memory index attending foreground instances and visualize the attention map in Fig. 2.\nAs shown in the results of the upper clip, we Ô¨Ånd that the memory token has more interests to\ninstances that are relatively difÔ¨Åcult to detect; it more attends the heavily occluded car at the rear.\nThe clip at the bottom is composed of frames with huge motion blurs and appearance changes. With\nthe communications of memory tokens, IFC successfully tracks and segments the rabbit.\n5 Conclusion\nIn this paper, we have proposed a novel video instance segmentation network using Inter-frame\nCommunication Transformers (IFC), which alleviates full space-time attention and successfully\nbuilds communications between frames. Finally, our network presents a rapid inference and sets the\nnew state-of-the-art on the YouTube-VIS dataset. For the future work, we plan to integrate temporal\ninformation, which indeed would take a step further to the human video understanding.\n9\nBroader Impact\nOur framework is designed for the VIS task, which targets to classify and segment foreground\ninstances of predeÔ¨Åned classes. Recently, while investigating the capabilities of transformers, many\ndisregard the importance of efÔ¨Åciency and take inputs of tremendous sizes. In comparison, IFC\nfocuses on reducing the overall computation while improving the performance. We believe our\nnetwork can positively impact many industrial Ô¨Åelds that require high accuracy and speed, i.e., alert\nsystem, autonomous driving, robotics. We want to note that for the community to move in the right\ndirection, the studies on VIS should be aware of potential misuses which violates personal privacy.\nCOCO [34], YouTube-VIS [1],detectron2 [32] license: CC-4.0, CC-4.0, Apache-2.0\nReferences\n[1] Yang, L., Y . Fan, N. Xu. Video instance segmentation. In ICCV. 2019.\n[2] Cao, J., R. M. Anwer, H. Cholakkal, et al. Sipmask: Spatial information preservation for fast\nimage and video instance segmentation. In ECCV. 2020.\n[3] Yang, S., Y . Fang, X. Wang, et al. Crossover learning for fast online video instance segmentation.\narXiv preprint arXiv:2104.05970, 2021.\n[4] Liu, D., Y . Cui, W. Tan, et al. Sg-net: Spatial granularity network for one-stage video instance\nsegmentation. In CVPR. 2021.\n[5] He, K., G. Gkioxari, P. Dollar, et al. Mask r-cnn. In ICCV. 2017.\n[6] Bolya, D., C. Zhou, F. Xiao, et al. Yolact: Real-time instance segmentation. In ICCV. 2019.\n[7] Tian, Z., C. Shen, H. Chen. Conditional convolutions for instance segmentation. In ECCV.\n2020.\n[8] Chen, H., K. Sun, Z. Tian, et al. Blendmask: Top-down meets bottom-up for instance segmenta-\ntion. In CVPR. 2020.\n[9] Luo, W., J. Xing, A. Milan, et al. Multiple object tracking: A literature review. ArtiÔ¨Åcial\nIntelligence, 2020.\n[10] Bertasius, G., L. Torresani. Classifying, segmenting, and tracking object instances in video with\nmask propagation. In CVPR. 2020.\n[11] Wang, Y ., Z. Xu, X. Wang, et al. End-to-end video instance segmentation with transformers. In\nCVPR. 2020.\n[12] Lin, H., R. Wu, S. Liu, et al. Video instance segmentation with a propose-reduce paradigm.\narXiv preprint arXiv:2103.13746, 2021.\n[13] Carion, N., F. Massa, G. Synnaeve, et al. End-to-end object detection with transformers. In\nECCV. 2020.\n[14] Vaswani, A., N. Shazeer, N. Parmar, et al. Attention is all you need. In NeurIPS. 2017.\n[15] Wang, H., Y . Zhu, B. Green, et al. Axial-deeplab: Stand-alone axial-attention for panoptic\nsegmentation. In ECCV. 2020.\n[16] Bertasius, G., H. Wang, L. Torresani. Is space-time attention all you need for video understand-\ning? arXiv preprint arXiv:2102.05095, 2021.\n[17] Wang, X., R. Girshick, A. Gupta, et al. Non-local neural networks. In CVPR. 2018.\n[18] Kuhn, H. W. The hungarian method for the assignment problem. In Naval research logistics\nquarterly. 1955.\n[19] Wang, H., Y . Zhu, H. Adam, et al. Max-deeplab: End-to-end panoptic segmentation with mask\ntransformers. In CVPR. 2021.\n[20] Dosovitskiy, A., L. Beyer, A. Kolesnikov, et al. An image is worth 16x16 words: Transformers\nfor image recognition at scale. In ICLR. 2021.\n[21] Ranftl, R., A. Bochkovskiy, V . Koltun. Vision transformers for dense prediction.arXiv preprint\narXiv:2103.13413, 2021.\n10\n[22] Devlin, J., M.-W. Chang, K. Lee, et al. Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding. In NAACL. 2019.\n[23] Kirillov, A., K. He, R. Girshick, et al. Panoptic segmentation. In CVPR. 2019.\n[24] Tran, D., H. Wang, L. Torresani, et al. A closer look at spatiotemporal convolutions for action\nrecognition. In CVPR. 2018.\n[25] Lin, T.-Y ., P. Dollar, R. Girshick, et al. Feature pyramid networks for object detection. InCVPR.\n2017.\n[26] Stewart, R., M. Andriluka, A. Y . Ng. End-to-end people detection in crowded scenes. InCVPR.\n2016.\n[27] Milletari, F., N. Navab, S.-A. Ahmadi. V-net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In 3DV. 2016.\n[28] Lin, T.-Y ., P. Goyal, R. Girshick, et al. Focal loss for dense object detection. In ICCV. 2017.\n[29] He, K., X. Zhang, S. Ren, et al. Deep residual learning for image recognition. In CVPR. 2016.\n[30] Athar, A., S. Mahadevan, A. O≈°ep, et al. Stem-seg: Spatio-temporal embeddings for instance\nsegmentation in videos. In ECCV. 2020.\n[31] Fu, Y ., L. Yang, D. Liu, et al. Compfeat: Comprehensive feature aggregation for video instance\nsegmentation. arXiv preprint arXiv:2012.03400, 2020.\n[32] Wu, Y ., A. Kirillov, F. Massa, et al. Detectron2.https://github.com/facebookresearch/\ndetectron2, 2019.\n[33] Loshchilov, I., F. Hutter. Decoupled weight decay regularization. In ICLR. 2019.\n[34] Lin, T.-Y ., M. Maire, S. Belongie, et al. Microsoft coco: Common objects in context. InECCV.\n2014.\n[35] Dai, J., H. Qi, Y . Xiong, et al. Deformable convolutional networks. In ICCV. 2017.\n[36] Cai, Z., N. Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In CVPR.\n2018.\n11"
}