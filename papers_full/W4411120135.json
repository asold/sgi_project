{
  "title": "Adaptation of Large Language Models",
  "url": "https://openalex.org/W4411120135",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2798848506",
      "name": "Zixuan Ke",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2808460989",
      "name": "Yifei Ming",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1208744602",
      "name": "Shafiq Joty",
      "affiliations": [
        "Salesforce (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2554863749"
  ],
  "abstract": null,
  "full_text": "Proceedings of the 2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics:\nHuman Language Technologies: Tutorial Abstracts, pages 30–37\nMay 1, 2025 ©2025 Association for Computational Linguistics\nAdaptation of Large Language Models\nZixuan Ke Yifei Ming Shafiq Joty\nSalesforce AI Research\n{zixuan.ke,yifei.ming,sjoty}@salesforce.com\n1 Brief Description\nThis tutorial on adaptation of Large Language Mod-\nels (LLMs) is designed to address the growing de-\nmand for models that go beyond the static capabili-\nties of generic LLMs by providing an overview of\ndynamic, domain-specific, and task-adaptive LLM\nadaptation techniques. While general LLMs have\ndemonstrated strong generalization across a variety\nof tasks, they often struggle to perform well in spe-\ncialized domains such as finance, healthcare, and\ncode generation for underrepresented languages.\nAdditionally, their static nature limits their abil-\nity to evolve with the changing world, and they\nare often extremely large in size, making them\nimpractical and costly to deploy at scale. As a\nresult, the adaptation of LLMs has drawn much\nattention since the birth of LLMs and is of core\nimportance, both for industry, which focuses on\nserving its targeted users, and academia, which can\ngreatly benefit from small but powerful LLMs.\nTo our knowledge, there has been no similar tuto-\nrial on this topic in the past few years1. To address\nthis gap, this tutorial aims to provide an overview\nof the LLM adaptation techniques. We start with\nan introduction to LLM adaptation, from both the\ndata perspective, which has been widely accepted\nas one of the most important ingredients in LLM\ntraining, and the model perspective, which focuses\non the training strategies to adapt the LLMs.\nIn Session 2, we emphasize how the evaluation\nmetrics and benchmarks are different from other\ntechniques. There is a growing recognition that\nevaluating adapted LLMs should not focus solely\non their performance within the target domains or\ntasks. While this remains important, an emerging\nconsensus is that adaptation should not be at the\ncost of sacrificing what it has already learned. An\nideal evaluation protocol should therefore assess\n1https://www.aclweb.org/adminwiki/index.php/\nPast_tutorials\nSlot Session\nSession 1: Introduction and Motivation\n14:00 - 14:05Tutorial presenters introduction\n14:05 - 14:10Challenges with general-purpose LLMs\n14:10 - 14:20Overview and use cases of LLM adaptation\n14:20 - 14:30Key concepts\n14:30 - 14:40Research questions in data and model\nSession 2: Evaluation and benchmark\n14:40 - 14:50General and specialized knowledge\n14:50 - 15:00Evaluation metrics\nSession 3: Parametric Knowledge Adaptation\n15:00 - 15:15Domain-adaptive Pre-training\n15:15 - 15:30Instruction Tuning\n15:30 - 16:00Coffee Break\n16:00 - 16:15Preference Learning\n16:15 - 16:30Model Editing\nSession 4: Semi-Parametric Knowledge Adaptation\n16:30 - 16:45Retrieval-augmented Generation\n16:45 - 17:00Agent-based Integration\n17:00 - 17:30Summary, Discussion & QA\nTable 1: Example tutorial schedule.\nnot only specialized task performance but also the\npreservation of general knowledge. Furthermore,\nas we will present in this tutorial, adapting an LLM\nusually involves multiple training stages, where\neach stage contributes to the final outcome. This\nmulti-stage process requires more comprehensive\nevaluation methods, as conventional metrics may\nnot capture the cumulative impact of these stages.\nAs such, researchers are increasingly focused on\ndesigning evaluation metrics and benchmarks that\nassess both the specialized knowledge gained and\nthe general knowledge retained.\nAfter establishing the problems in the aforemen-\ntioned sessions, we explore various adaptation\ntechniques. When powerful general LLMs first\nemerged, many believed that prompt engineering\nalone, without any actual model training (i.e., up-\ndating the model’s parameters), was sufficient to\nadapt them for new tasks. However, recently we\nhave seen more researchers begin finetuning model\nwights. This shift is understandable not only be-\ncause training typically leads to better results on\nspecialized tasks, but also because training has be-\ncome more affordable due to the growing number\n30\nof techniques that enable efficient training. For ex-\nample, the training cost increased from $900 for\nthe original Transformer to over $4 million to train\nthe GPT-3 (davinci), but it has recently dropped to\naround $0.8 million to train a GPT-3 on-par model\nlike Phi3.5 (Maslej et al., 2024). In light of this\ntrend, this tutorial will focus on increasingly popu-\nlar methods that adapt LLMs through training.\nWe categorize adaptation techniques into two\nmain families. The first is parametric knowledge\nadaptation, which focuses on updating the para-\nmetric knowledge within LLMs, including meth-\nods like Domain-Adaptive Pre-Training (DAPT),\nInstructional Tuning (IT), and Preference Learning\n(PL) via human or model feedback. Additionally,\nwe will discuss real-time adaptation techniques, in-\ncluding model editing, which allows LLMs to be\nupdated dynamically in production environments.\nThe second kind of adaptation is semi-parametric\nknowledge adaptation, where the goal is to up-\ndate LLM parameters to better leverage external\nknowledge or tools (e.g., documents or functions)\nthrough techniques like retrieval-augmented gener-\nation (RAG) and agent-based systems.\n2 Detailed Outline\nThis will be a three-hour tutorial devoted to the\ncutting-edge topic of the adaptation of LLMs. Ta-\nble 1 gives an overview.\n1. Introduction and Motivation\n1.1. Challenges with General-purpose LLMs.\nWhile LLMs have demonstrated strong gen-\neralization capabilities, they are general and\nstatic. They often fall short in highly special-\nized domains or tasks. These LLMs struggle\nto adapt to evolving requirements and exhibit\nhigh computational overhead due to their fixed\nnature. This section will highlight key limi-\ntations of them, with detailed examples and\nstatistics to illustrate why adaptation is nec-\nessary for handling domain-specific or task-\nspecific demands.\n1.2. Overview and Use Cases of LLM Adapta-\ntion. LLM Adaptation adapts a pre-trained\nLLM to specialized use cases. It offers prac-\ntical solutions for domain-specific applica-\ntions, such as in finance and healthcare, as\nwell as task-specific applications like retrieval-\naugmented generation and code generation.\nPersonalized systems also benefit from adap-\ntive models, which can fine-tune responses\nbased on individual user needs. This section\nwill showcase real-world use cases that demon-\nstrate the critical need for LLM adaptability to\naddress specialized scenarios.\n1.3. Key Concepts. Before exploring detailed\nmethodologies, we will introduce foundational\nconcepts essential to understanding adaptive\nLLMs. These include distinctions between\ndomain vs. task adaptation, continual learn-\ning vs. LLM adaptation, Parameter-Efficient\nFine-Tuning (PEFT), different stages of post-\ntraining (e.g., Domain-Adaptive Pre-Training\nand Supervised Fine-Tuning), long-context\nLLMs, retrieval-augmented generation (RAG),\nagentic approach. These concepts will set the\nstage for the following technical sections.\n1.4. Research Questions in Data and Model Per-\nspective. In this tutorial, we aim to address\nseveral critical research questions. In data per-\nspective, we focus on what gives a good data\nmixture and how to obtain high-quality data.\nIn model perspective, we ask what constitutes\nan effective training recipe for adaptive LLMs,\nhow can we properly evaluate adaptive mod-\nels across different domains and tasks, what\nstrategies can be used to prevent data leakage\nduring post-training, and how can we scale up\npost-training efficiently. These questions will\nguide the subsequent discussions on evalua-\ntion and training.\n2. Evaluation and Benchmarks. Evaluating adap-\ntive LLMs involves a multidimensional ap-\nproach. We will cover methods for assessing\ngeneralization, domain- and task-specific perfor-\nmance, and real-time effectiveness.\n2.1. General and Specialized Knowledge. When\nadapting a model to specialize in specific\nknowledge, it is critical not to lose already\nlearned general knowledge. A robust evalua-\ntion protocol should account for both. We will\nintroduce methods to evaluate both types.\n2.2. Evaluation Metrics . Numerous metrics have\nbeen discussed in the literature, such as per-\nplexity, needle-in-haystack evaluations, and\ndownstream task performance. We will com-\npare and analyze the effectiveness of these\nmetrics in evaluating adaptive LLMs.\n3. Parametric Knowledge Adaptation. Multiple\nstages are undertaken to refine and specialize\ntheir knowledge for specific tasks and domains.\n31\n3.1. Domain-Adaptive Pre-Training (DAPT).\nDiscuss the data and objective of DAPT that\nlearn domain-specific background knowledge\nusing raw text with next token prediction.\n3.2. Instruction Tuning (IT).Discuss the data and\nobjective of IT, also known as Supervised Fine-\nTuning (SFT). It can be full model fine-tuning\nor parameter-efficient fine-tuning, with focus\non instruction following ability.\n3.3. Preference Learning (PL) Discuss the PL,\nwhere extra supervision on negative cases and\ndelayed reward are introduced in this stage.\nThe preference feedback can come from hu-\nmans, AI models, or programmatic metrics. It\ncan be done in online or offline (direct prefer-\nence optimization; DPO) manner.\n3.4. Model Editing. Discuss the techniques that\nallow for targeted updates in real-time, improv-\ning adaptability in production environments\nwithout the need for expensive retraining.\n4. Semi-parametric Knowledge Integration\n4.1. Retrieval-augmented Generation (RAG)\nHow RAG allows models to dynamically re-\ntrieve external knowledge, and significantly\nenhance performance on specialized tasks or\nwhen up-to-date information is needed.\n4.2. Agent-based Integration. This focuses more\non the agentic aspect, highlighting the model\nas an agent that performs multi-step reasoning\nand interacts with external APIs and dynamic\nenvironments. It draws attention to the sys-\ntem’s autonomous decision-making capacity\nand its ability to operate as an agent in com-\nplex environments.\n5. Summary, Discussion & QA\n3 Proportion of Other Researchers’\nWork, Reading list and Prerequisite\nLLM adaptation involved almost all domains and\ntasks, making it become the main focus of industry\nand academic research. Our tutorial is designed to\nprovide a comprehensive overview of this broad\nand rapidly evolving field, ensuring that the content\nreflects a wide range of techniques and contribu-\ntions. As a result, the tutorial naturally includes a\nsubstantial amount of work from other researchers.\nThe prerequisite includes familiarity with basic\nknowledge of machine learning, NLP and LLM.\nKnowledge of continual learning is a plus. Here are\na few papers that lay a foundation for this area and\nwe encourage attendees to read the papers before\nthe tutorial to familiarize themselves with founda-\ntional concepts and approaches:\n• Continual pre-training of language models (Ke\net al., 2023)\n• Locating and editing factual associations in GPT\n(Meng et al., 2023a)\n• Retrieval augmented language model pre-\ntraining (Guu et al., 2020)\nBelow, we provide a detailed reference to the\nworks we plan to cover. While we will give pointers\nto dozens of relevant papers over the course of\nthe tutorial, we plan to cover around 7-8 research\npapers in close detail. Only 1-2 of the “deep dive”\npapers will come from the presenter team.\nContinual Learning learns a sequence of tasks\nsequentially without forgetting (Chen and Liu,\n2018; McCloskey and Cohen, 1989; Van de Ven\nand Tolias, 2019; Mai et al., 2022; Aljundi et al.,\n2019; Ke and Liu, 2022), which is closely re-\nlated to LLM adaptation. Typical approaches in-\nclude regularization-based methods that regularize\nparameter updates to preserve important parame-\nters (Kirkpatrick et al., 2016; Seff et al., 2017);\nmodular-based methods that dynamically modify\nthe architecture (Serrà et al., 2018; Wortsman et al.,\n2020); and replay-based method that recall previ-\nous experiences (Rebuffi et al., 2017; Wang et al.,\n2020).\nParametric Knowledge Adaptation adapts\nLLMs by updating its parametric knowledge. It\nhas been widely adopted across board domains,\nsuch as code (Nijkamp et al., 2022), medical (Luo\net al., 2023), law (Colombo et al., 2024), mathemat-\nics (Azerbayev et al., 2023), multi-lingual (Chen\net al., 2024a) and finance (Ke et al., 2025; Xie\net al., 2023a; Writer, 2024) and tasks such as\nfunction calling (Zhang et al., 2024b). Some fo-\ncus on domain-specific or task-specific data cura-\ntion (Yang et al., 2024b), mixture ratio (Que et al.,\n2024), data-efficiency (Xie et al., 2023b), hyper-\nparameters (Parmar et al., 2024) or training recipe\n(Jiang et al., 2024; Yu et al., 2024; Gao et al., 2024).\nSemi-Parametric Knowledge Adaptation aug-\nments LLMs with relevant information retrieved\nfrom various knowledge sources, i.e., RAG or ex-\nternal functions, i.e., agent-based intergation.\nRetrieval-Augmented Generation (RAG) is\nproven effective across numerous NLP tasks, in-\ncluding language modeling (Borgeaud et al., 2022;\nKhandelwal et al., 2020; Shi et al., 2023), question\n32\nanswering (Ke et al., 2024; Lewis et al., 2020; Izac-\nard et al., 2022; de Jong et al., 2023; De Jong et al.,\n2023; Shi et al., 2023; Guu et al., 2020; Izacard\nand Grave, 2020; Xu et al., 2023), fact versifica-\ntion (Lewis et al., 2020) and text generation (Lewis\net al., 2020). Specifically, RAG utilizes input as\na query and comprises two main components: a\nretriever retrieves a set of items from a side corpus,\nand a LLM incorporates the retrieved items, as addi-\ntional information in the input context, and makes\nfinal predictions. Depending on which components\nare subject to updates, typical approaches can be\ncategorized into three families: adapting retrievers\nand LLMs jointly, which is the most widely used\nsetting of RAG (Izacard et al., 2022; Khandelwal\net al., 2020; Wu et al., 2022; Guu et al., 2020; Lewis\net al., 2020); adapting LLMs only, which avoid the\ncostly updates of retrievers and document index\n(Izacard and Grave, 2020; De Jong et al., 2023;\nde Jong et al., 2023); and adapting retrievers only,\nwhich compatible with black-box LLMs (Shi et al.,\n2023; Xu et al., 2023).\nModel Editing aims to adapt LLM in real-time\nwithout the need for expensive training or even\nwithout training (Zhang et al., 2024c; Wang et al.,\n2024). Typical approaches include locate-and-edit\nthat first locate the position of the knowledge in\nLLMs and edit the KV cache (Meng et al., 2023a,b;\nYang et al., 2024a; Gupta et al., 2024a,b), meta-\nlearning that employ hyper-network to learn the\nediting (Tan et al., 2024; Cao et al., 2021; Mitchell\net al., 2022) and memory-based that use memory\nelements to store and manipulate information dur-\ning editing (Zhu et al., 2020; Ni et al., 2024; Zheng\net al., 2023). As of now, editing is still in its infancy\nand suffers from serious forgetting, in particular\nwhen editing a sequence of samples.\nAgent-based Integration augments the LLMs\nwith external functions. Typical approaches include\nfine-tuning the LLMs for better function calling\ncapacities (Qin et al., 2023; Chen et al., 2023; Patil\net al., 2023; Zeng et al., 2023; Zhang et al., 2024a;\nYin et al., 2024; Chen et al., 2024b; Li et al., 2023;\nTang et al., 2023); and prompting techniques (Wei\net al., 2023; Yao et al., 2022; Shinn et al., 2023)\n4 Target Audience and Estimated Size\nResearchers, graduate students, and practitioners\nwho are interested in NLP and machine learning.\nThe tutorial will is particularly beneficial to peo-\nple who intend to develop machine learning and\nNLP techniques and applications that can adapt the\nLLM to specialized domains or tasks while pre-\nserving the original learned knowledge in the LLM.\nThis tutorial has not been given before. Due to the\nincreasing popularity of LLM and its adpatation,\nwe estimate that there will be 100 to 150 people\nattending the tutorial.\n5 Diversity Consideration\nThe topic of LLM adaptation will be inclusive for\na broad range of communities. It will be of interest\nto industrial practitioners as they aim to deploy the\nbest adaptive models for their targeted customers.\nAdditionally, academic researchers will find value\nin this topic, as it essentially addresses a contin-\nual learning problem, enabling LLMs to adapt to\nnew environments without forgetting—an essential\nstep toward advancing Artificial General Intelli-\ngence. This topic also makes it easier for users\nto engage with LLMs, as it involves techniques\nto train smaller models that can be deployed effi-\nciently. It naturally spans a wide range of domains\nand tasks, making it relevant to participants inter-\nested in various specific areas.\nOur presenters include both junior and senior re-\nsearchers from industry and academia, and we also\nbring geographical diversity, with instructors from\nboth North America and Asia. This diverse group\nof instructors will help attract a wide audience and\nencourage participation from various backgrounds.\nWe will make our tutorial materials digitally ac-\ncessible to all participants. All materials will be\npublicly available before the tutorial. We will pro-\nvide slides and discussion questions before the tu-\ntorial to the audience. During the tutorial sessions,\nwe will work with student volunteers to encour-\nage open dialogue and promote active listening,\nallowing participants to share their thoughts and\nexperiences without fear of judgment. After the\ntutorial, we will actively collect feedback to iden-\ntify areas for improvement related to diversity and\ninclusion and share it with future tutorial presen-\nters. Our presenter team will share our tutorial with\na worldwide audience by promoting it on social\nmedia, and to diverse research communities. We\nwill work with ACL/NAACL/EMNLP D&I teams,\nand consult resources such as the BIG directory to\ndiversify our audience participation.\n6 Ethics Statement\nThe tutorial will present general problems and al-\ngorithms related to LLM adaptation in NLP. All\n33\nevaluation datasets are public domain benchmarks.\nThus, the tutorial does not have any ethical issues.\n7 Presenter Biography\nZixuan Ke is a research scientist at Salesforce AI\nResearch, where he works with Dr. Shafiq Joty\non continual pre-training of LLMs. Prior to that,\nhe earned his Ph.D. at the University of Illinois,\nChicago, where he worked with Dr. Bing Liu on\ncontinual learning. His research studies how to\nadapt the foundation models, particularly LLMs,\nfor an ever-changing world characterized by emerg-\ning domains, events, topics, or information. He\nhas published 25+ papers in top-tier conferences\nsuch as ICLR, ICML, NeurIPS, NAACL, ACL,\nand EMNLP with around 1.5K citations. He has\nserved as an Area Chair and PC member for pres-\ntigious conferences such as ICLR, NeurIPS, ACL,\nEMNLP, and as a reviewer for leading journals such\nas Neural Networks, Neurocomputing, Artificial\nIntelligence, TKDE, and TPAMI, since 2021. He\nhas given an invited short PhD course on continual\nlearning at Aalborg University in the summer of\n2022 and has also given several oral presentations\nat leading conferences. Further information about\nhim, including past talks and slides, can be found\nat https://vincent950129.github.io/.\nYifei Ming is a Research Scientist at Salesforce\nAI Research. He received his Ph.D. in Computer\nScience from the University of Wisconsin-Madison\nin 2024. His research spans both empirical and\ntheoretical studies, with a focus on understanding,\nbenchmarking, and developing novel algorithms\ntowards trustworthy foundation models (LLM and\nVLM). He has a prolific publication record with\n20+ papers in top-tier AI conferences and journals.\nHe co-organized several workshops and has served\non the program committees of leading machine\nlearning conferences and journals since 2021, in-\ncluding NeurIPS, ICLR, ICML, AAAI, EMNLP,\nIJCV , TPAMI, and NN. Further information can be\nfound at https://alvinmingsf.github.io/.\nShafiq Joty is a research director at Salesforce\nResearch and an Associate Professor at the Com-\nputer Science Department of the Nanyang Techno-\nlogical University. His research has contributed to\nover 30+ patents and 150+ papers in top-tier NLP\nand ML conferences and journals. He has served\nas a Program Chair of SIGDIAL’23, a member of\nthe best paper award committees for ICLR’23 and\nNAACL’22, and in the capacity of a (senior) area\nchair for many leading NLP and ML conferences\n(e.g. NeurIPS, EMNLP, and ACL). He previously\ngave tutorials at EMNLP’23, IEEEVis’22, ACL’19,\nCOLING’18 and ICDM’18. Further information\nabout him, including past talks and slides, can be\nfound at https://raihanjoty.github.io/.\nReferences\nRahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua\nBengio. 2019. Gradient based sample selection for\nonline continual learning. In NeurIPS.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster,\nMarco Dos Santos, Stephen McAleer, Albert Q Jiang,\nJia Deng, Stella Biderman, and Sean Welleck. 2023.\nLlemma: An open language model for mathematics.\narXiv preprint arXiv:2310.10631.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning, pages 2206–2240. PMLR.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models.\nBaian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier,\nKarthik Narasimhan, and Shunyu Yao. 2023. Fireact:\nToward language agent fine-tuning.\nJie Chen, Zhipeng Chen, Jiapeng Wang, Kun Zhou, Yu-\ntao Zhu, Jinhao Jiang, Yingqian Min, Wayne Xin\nZhao, Zhicheng Dou, Jiaxin Mao, Yankai Lin, Rui-\nhua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei,\nDi Hu, Wenbing Huang, and Ji-Rong Wen. 2024a.\nTowards effective and efficient continual pre-training\nof large language models.\nZehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei\nZhang, Jiangning Liu, Dahua Lin, Kai Chen, and\nFeng Zhao. 2024b. Agent-flan: Designing data and\nmethods of effective agent tuning for large language\nmodels.\nZhiyuan Chen and Bing Liu. 2018. Lifelong machine\nlearning. Synthesis Lectures on Artificial Intelligence\nand Machine Learning, 12(3).\nPierre Colombo, Telmo Pessoa Pires, Malik Boudiaf,\nDominic Culver, Rui Melo, Caio Corro, Andre FT\nMartins, Fabrizio Esposito, Vera Lúcia Raposo, Sofia\nMorgado, et al. 2024. Saullm-7b: A pioneer-\ning large language model for law. arXiv preprint\narXiv:2403.03883.\nMichiel De Jong, Yury Zemlyanskiy, Nicholas FitzGer-\nald, Joshua Ainslie, Sumit Sanghai, Fei Sha, and\nWilliam W Cohen. 2023. Pre-computed memory or\non-the-fly encoding? a hybrid approach to retrieval\n34\naugmentation makes the most of your compute. In In-\nternational Conference on Machine Learning, pages\n7329–7342. PMLR.\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\nald, Sumit Sanghai, William W Cohen, and Joshua\nAinslie. 2023. Glimmer: generalized late-interaction\nmemory reranker. arXiv preprint arXiv:2306.10231.\nTianyu Gao, Alexander Wettig, Howard Yen, and Danqi\nChen. 2024. How to train long-context language\nmodels (effectively).\nAkshat Gupta, Sidharth Baskaran, and Gopala Anu-\nmanchipalli. 2024a. Rebuilding rome : Resolving\nmodel collapse during sequential model editing.\nAkshat Gupta, Dev Sajnani, and Gopala Anu-\nmanchipalli. 2024b. A unified framework for model\nediting.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nGautier Izacard and Edouard Grave. 2020. Leverag-\ning passage retrieval with generative models for\nopen domain question answering. arXiv preprint\narXiv:2007.01282.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nZhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Ro-\ndriguez, Chunting Zhou, Graham Neubig, Xi Vic-\ntoria Lin, Wen-tau Yih, and Srinivasan Iyer. 2024.\nInstruction-tuned language models are better knowl-\nedge learners. arXiv preprint arXiv:2402.12847.\nZixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang,\nQiaozhu Mei, and Michael Bendersky. 2024. Bridg-\ning the preference gap between retrievers and llms.\narXiv preprint arXiv:2401.06954.\nZixuan Ke and Bing Liu. 2022. Continual learning\nof natural language processing tasks: A survey. In\nArkiv.\nZixuan Ke, Yifei Ming, Xuan-Phi Nguyen, Caiming\nXiong, and Shafiq Joty. 2025. Demystifying domain-\nadaptive post-training for financial llms. arXiv\npreprint arXiv:2501.04961.\nZixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Kon-\nishi, Gyuhak Kim, and Bing Liu. 2023. Contin-\nual pre-training of language models. arXiv preprint\narXiv:2302.03241.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nJames Kirkpatrick, Razvan Pascanu, Neil C. Rabi-\nnowitz, Joel Veness, Guillaume Desjardins, Andrei A.\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2016. Overcoming catastrophic forgetting in neural\nnetworks. CoRR.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nMinghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song,\nHangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. 2023. API-bank: A comprehensive\nbenchmark for tool-augmented LLMs. In Proceed-\nings of the 2023 Conference on Empirical Methods\nin Natural Language Processing.\nYizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang,\nYushuai Wu, Mu Qiao, and Zaiqing Nie. 2023.\nBiomedgpt: Open multimodal generative pre-trained\ntransformer for biomedicine. arXiv preprint\narXiv:2308.09442.\nZheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe,\nHyunwoo Kim, and Scott Sanner. 2022. Online con-\ntinual learning in image classification: An empirical\nsurvey. Neurocomputing, 469.\nNestor Maslej, Loredana Fattorini, Raymond Perrault,\nVanessa Parli, Anka Reuel, Erik Brynjolfsson, John\nEtchemendy, Katrina Ligett, Terah Lyons, James\nManyika, Juan Carlos Niebles, Yoav Shoham, Rus-\nsell Wald, and Jack Clark. 2024. The ai index 2024\nannual report,.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learn-\ning and motivation.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2023a. Locating and editing factual associ-\nations in gpt.\nKevin Meng, Arnab Sen Sharma, Alex Andonian,\nYonatan Belinkov, and David Bau. 2023b. Mass-\nediting memory in a transformer.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D. Manning. 2022. Fast model\nediting at scale.\nShiwen Ni, Dingwei Chen, Chengming Li, Xiping Hu,\nRuifeng Xu, and Min Yang. 2024. Forgetting before\nlearning: Utilizing parametric arithmetic for knowl-\nedge updating in large language models.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474.\n35\nJupinder Parmar, Sanjev Satheesh, Mostofa Patwary,\nMohammad Shoeybi, and Bryan Catanzaro. 2024.\nReuse, don’t retrain: A recipe for continued pre-\ntraining of language models. arXiv preprint\narXiv:2407.07263.\nShishir G. Patil, Tianjun Zhang, Xin Wang, and\nJoseph E. Gonzalez. 2023. Gorilla: Large language\nmodel connected with massive apis.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,\nRuobing Xie, Jie Zhou, Mark Gerstein, Dahai Li,\nZhiyuan Liu, and Maosong Sun. 2023. Toolllm: Fa-\ncilitating large language models to master 16000+\nreal-world apis.\nHaoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang,\nXingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Ji-\nakai Wang, Yuanxing Zhang, Xu Tan, Jie Fu, Wenbo\nSu, Jiamang Wang, Lin Qu, and Bo Zheng. 2024.\nD-cpt law: Domain-specific continual pre-training\nscaling law for large language models.\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H. Lampert. 2017. icarl: In-\ncremental classifier and representation learning. In\nCVPR.\nAri Seff, Alex Beatson, Daniel Suo, and Han Liu. 2017.\nContinual learning in generative adversarial nets.\nCoRR, abs/1705.08395.\nJoan Serrà, Didac Suris, Marius Miron, and Alexan-\ndros Karatzoglou. 2018. Overcoming catastrophic\nforgetting with hard attention to the task. In ICML.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nNoah Shinn, Beck Labash, and Ashwin Gopinath.\n2023. Reflexion: an autonomous agent with dy-\nnamic memory and self-reflection. arXiv preprint\narXiv:2303.11366.\nChenmien Tan, Ge Zhang, and Jie Fu. 2024. Massive\nediting for large language models via meta learning.\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,\nQiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca:\nGeneralized tool learning for language models with\n3000 simulated cases.\nGido M Van de Ven and Andreas S Tolias. 2019. Three\nscenarios for continual learning. arXiv preprint\narXiv:1904.07734.\nSong Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng,\nChen Chen, and Jundong Li. 2024. Knowledge edit-\ning for large language models: A survey.\nZirui Wang, Sanket Vaibhav Mehta, Barnabás Póczos,\nand Jaime Carbonell. 2020. Efficient meta lifelong-\nlearning with limited memory. In EMNLP.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nMitchell Wortsman, Vivek Ramanujan, Rosanne Liu,\nAniruddha Kembhavi, Mohammad Rastegari, Jason\nYosinski, and Ali Farhadi. 2020. Supermasks in su-\nperposition. In NeurIPS.\nWriter. 2024. Palmyra-Fin-70B-32k: a powerful LLM\ndesigned for Finance. https://dev.writer.com.\nYuhuai Wu, Markus N Rabe, DeLesley Hutchins, and\nChristian Szegedy. 2022. Memorizing transformers.\narXiv preprint arXiv:2203.08913.\nQianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao\nLai, Min Peng, Alejandro Lopez-Lira, and Jimin\nHuang. 2023a. Pixiu: A large language model, in-\nstruction data and evaluation benchmark for finance.\nYong Xie, Karan Aggarwal, and Aitzaz Ahmad. 2023b.\nEfficient continual pre-training for building domain\nspecific large language models.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Re-\ncomp: Improving retrieval-augmented lms with com-\npression and selective augmentation. arXiv preprint\narXiv:2310.04408.\nWanli Yang, Fei Sun, Xinyu Ma, Xun Liu, Dawei Yin,\nand Xueqi Cheng. 2024a. The butterfly effect of\nmodel editing: Few edits can trigger large language\nmodels collapse.\nZitong Yang, Neil Band, Shuangping Li, Emmanuel\nCandès, and Tatsunori Hashimoto. 2024b. Syn-\nthetic continued pretraining. arXiv preprint\narXiv:2409.07431.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nDa Yin, Faeze Brahman, Abhilasha Ravichander, Khy-\nathi Chandu, Kai-Wei Chang, Yejin Choi, and\nBill Yuchen Lin. 2024. Agent lumos: Unified and\nmodular training for open-source language agents.\nXiao Yu, Qingyang Wu, Yu Li, and Zhou Yu. 2024.\nLions: An empirically optimized approach to align\nlanguage models.\nAohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao\nLiu, Yuxiao Dong, and Jie Tang. 2023. Agenttuning:\nEnabling generalized agent abilities for llms.\nJianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei\nLiu, Weiran Yao, Juntao Tan, Thai Hoang, Liang-\nwei Yang, Yihao Feng, Zuxin Liu, Tulika Awal-\ngaonkar, Juan Carlos Niebles, Silvio Savarese,\n36\nShelby Heinecke, Huan Wang, and Caiming Xiong.\n2024a. Agentohana: Design unified data and training\npipeline for effective agent learning.\nJianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai\nHoang, Shirley Kokane, Weiran Yao, Juntao Tan,\nAkshara Prabhakar, Haolin Chen, et al. 2024b. xlam:\nA family of large action models to empower ai agent\nsystems. arXiv preprint arXiv:2409.03215.\nNingyu Zhang, Yunzhi Yao, Bozhong Tian, Peng\nWang, Shumin Deng, Mengru Wang, Zekun Xi,\nShengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan\nCheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang,\nPengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang,\nXiaowei Zhu, Jun Zhou, and Huajun Chen. 2024c. A\ncomprehensive study of knowledge editing for large\nlanguage models.\nCe Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong\nWu, Jingjing Xu, and Baobao Chang. 2023. Can we\nedit factual knowledge by in-context learning?\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.\n2020. Modifying memories in transformer models.\n37",
  "topic": "Adaptation (eye)",
  "concepts": [
    {
      "name": "Adaptation (eye)",
      "score": 0.7350952625274658
    },
    {
      "name": "Computer science",
      "score": 0.6617550849914551
    },
    {
      "name": "Natural language processing",
      "score": 0.3363964259624481
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3290185332298279
    },
    {
      "name": "Psychology",
      "score": 0.12690919637680054
    },
    {
      "name": "Neuroscience",
      "score": 0.06481653451919556
    }
  ],
  "institutions": [],
  "cited_by": 1
}