{
    "title": "‘Indicatements’ that character language models learn English morpho-syntactic units and regularities",
    "url": "https://openalex.org/W2891343966",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2623738526",
            "name": "Yova Kementchedjhieva",
            "affiliations": [
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A2250623146",
            "name": "Adam Lopez",
            "affiliations": [
                "University of Edinburgh"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2828202920",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W2311921240",
        "https://openalex.org/W2963251942",
        "https://openalex.org/W2531207078",
        "https://openalex.org/W2560864221",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2292919134",
        "https://openalex.org/W2251670681",
        "https://openalex.org/W2950290711",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2056654723",
        "https://openalex.org/W2950613790",
        "https://openalex.org/W2963099225",
        "https://openalex.org/W2101609803",
        "https://openalex.org/W2514307064",
        "https://openalex.org/W196214544"
    ],
    "abstract": "Character language models have access to surface morphological patterns, but it is not clear whether or how they learn abstract morphological regularities. We instrument a character language model with several probes, finding that it can develop a specific unit to identify word boundaries and, by extension, morpheme boundaries, which allows it to capture linguistic properties and regularities of these units. Our language model proves surprisingly good at identifying the selectional restrictions of English derivational morphemes, a task that requires both morphological and syntactic awareness. Thus we conclude that, when morphemes overlap extensively with the words of a language, a character language model can perform morphological abstraction.",
    "full_text": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 145–153\nBrussels, Belgium, November 1, 2018.c⃝2018 Association for Computational Linguistics\n145\nIndicatements that character language models learn English\nmorpho-syntactic units and regularities\nYova Kementchedjhieva\nUniversity of Copenhagen\nyova@di.ku.dk\nAdam Lopez\nUniversity of Edinburgh\nalopez@inf.ed.ac.uk\nAbstract\nCharacter language models have access to sur-\nface morphological patterns, but it is not clear\nwhether or how they learn abstract morpho-\nlogical regularities. We instrument a charac-\nter language model with several probes, ﬁnd-\ning that it can develop a speciﬁc unit to iden-\ntify word boundaries and, by extension, mor-\npheme boundaries, which allows it to capture\nlinguistic properties and regularities of these\nunits. Our language model proves surpris-\ningly good at identifying the selectional re-\nstrictions of English derivational morphemes,\na task that requires both morphological and\nsyntactic awareness. Thus we conclude that,\nwhen morphemes overlap extensively with the\nwords of a language, a character language\nmodel can perform morphological abstraction.\n1 Introduction\nCharacter-level language models (Sutskever et al.,\n2011) are appealing because they enable open-\nvocabulary generation of language, and condi-\ntional character language models have now been\nconvincingly used in speech recognition (Chan\net al., 2016) and machine translation (Weiss et al.,\n2017; Lee et al., 2016; Chung et al., 2016). They\nsucceed due to parameter-sharing between fre-\nquent, rare, and even unobserved training words,\nprompting claims that they learn morphosyntac-\ntic properties of words. For example, Chung\net al. (2016) claim that character language mod-\nels yield “better modelling [of] rare morpholog-\nical variants” while Kim et al. (2016) claim that\n“Character-level models obviate the need for mor-\nphological tagging or manual feature engineer-\ning.” But these claims of morphological awareness\nare backed more by intuition than direct empirical\nevidence. What do these models really learn about\nmorphology? And, to the extent that they learn\nabout morphology, how do they learn it?\nOur goal is to shed light on these questions, and\nto that end, we study the behavior of a character-\nlevel language model (hereafter LM) applied to\nEnglish. We observe that, when generating text,\nthe LM applies certain morphological processes\nof English productively, i.e. in novel contexts\n(§3). This rather surprising ﬁnding suggests that\nthe model can identify the morphemes relevant to\nthese processes. An analysis of the LM’s hidden\nunits presents a possible explanation: there ap-\npears to be one particular unit that ﬁres at mor-\npheme and word boundaries ( §4). Further experi-\nments reveal that the LM learns morpheme bound-\naries through extrapolation from word boundaries\n(§5). In addition to morphology, the LM appears\nto encode syntactic information about words, i.e.\ntheir part of speech ( §6). With access to both\nmorphology and syntax, the model should also be\nable to learn linguistic phenomena at the intersec-\ntion of the two domains, which we indeed ﬁnd to\nbe the case: the LM captures the (syntactic) se-\nlectional restrictions of English derivational mor-\nphemes, albeit with some incorrect generalizations\n(§7). The conclusions of this work can thus be\nsummarized in two main points—a character-level\nlanguage model can:\n1. learn to identify linguistic units of higher or-\nder, such as morphemes and words.\n2. learn some underlying linguistic properties\nand regularities of said units.\n2 Language Modeling\nThe LM explored in this work is a ‘wordless’ char-\nacter RNN with LSTM units (Karpathy, 2015). 1\n1Karpathy (2015) is a blog post that discusses exactly this\nmodel. We are unaware of scholarly publications that use\nthis model in isolation, though it is used in several conditional\nmodels (Chan et al., 2016; Weiss et al., 2017; Lee et al., 2016;\nChung et al., 2016), and it is similar to the character RNN\n146\nIt is ‘wordless’ in the sense that input is not seg-\nmented into words, and spaces are treated just like\nany other character. This architecture allows for\nexperiments on a subword, i.e. morphological\nlevel: we can feed a partial word and ask the model\nto complete it or record the probability the model\nassigns to an ending of our choice.\n2.1 Formulation\nAt each timestep t, character ct is projected into a\nhigh-dimensional space by a character embedding\nmatrix E ∈R|V |×d: xct = ET vct , where |V |is the\nvocabulary of characters encountered in the train-\ning data, d is the dimension of the character em-\nbeddings and vct ∈R|V |is a one-hot vector with\nctth element set to 1 and all other elements set to\nzero.\nThe hidden state of the neural network is ob-\ntained as: ht = LSTM(xct ; ht−1). This hidden\nstate is followed by a linear transformation and a\nsoftmax function over all elements ofV , which re-\nsults in a probability distribution.\np(ct+1 = c |ht) = softmax(Woht+bo)i ∀c ∈V\nwhere i is the index of c in V .\n2.2 Training\nThe model was trained on a continuous stream\nof the ﬁrst 7M character tokens from the English\nWikipedia corpus. Data was not lowercased and\nit was randomly split into training (90%) and de-\nvelopment (10%). Following a grid search over\nhyperparameters on a subset of the training data,\nwe chose to use one layer with a hidden unit size\nof 256, a learning rate of 0.003, minibatch size of\n50, and dropout rate of 0.2, applied to the input of\nthe hidden layer.\n3 The English dialect of a character LM\nIn an initial analysis of the learned model we stud-\nied text generated with the LM and found that it\nclosely resembled English on the word level and,\nto some degree, on the level of syntax.\n3.1 Words\nWhen sampled, the LM generates real English\nwords most of the time, and only about 1 in ev-\nery 20 tokens is a nonce word. 2 Regular morpho-\nof Sutskever et al. (2011), which uses a multiplicative RNN\nrather than an LSTM unit.\n2As measured by checking whether the word appeared in\nthe training data or in the pyenchant UK or US English dic-\ntionaries.\nsinding, fatities, complessed\nbreaked, indicatement\napplie, therapie\nknwotator, mindt, ouctromor\nTable 1: Nonce words generated with the LM through\nsampling.\nThe novel regarded the modern Laboratory has a\nweaken-little director and many of them in 2012\nto defeat in 1973 - or eviven of Artitagements.\nTable 2: A sentence generated with the LM through\nsampling.\nlogical patterns can be observed within some of\nthese nonce words (Table 1). The words sinding,\nfatities and complessed all seem like well-formed\ninﬂected variants of English-looking words. The\nforms breaked and indicatement show productive\nmorphological patterns of inﬂection and deriva-\ntion being applied to bases of the correct syntactic\nclass, namely verbs. It happens that break is an ir-\nregular verb and indicate forms a noun with sufﬁx\n-ion rather than -ment, but these are lexical rules\nthat block the more regular inﬂectional and deriva-\ntional rules the LM has applied. In addition to\ncomposing morphologically complex words, the\nLM also attempts to decompose them, as can be\nseen with the forms therapie and applie. Here\nthe inﬂectional sufﬁx -s has been dropped, but\nthe orthographic change associated with it has not\nbeen successfully reversed. Not all nonce words\ngenerated by the LM can be explained in terms\nof morphological productivity, however: knwota-\ntor, mindt, and ouctromor don’t resemble any real\nmorphemes and don’t follow English phonotac-\ntics. These forms may be highly improbable ac-\ncidents of the sampling process.\n3.2 Sentences\nConsider the sentence in Table 2 generated with\nthe LM through sampling. The sentence could not\nbe considered ﬂuent or grammatical: there are no\nclear dependencies between verbs, subjects, and\nobjects; it contains the nonce word eviven and the\nnovel and unlikely compound weaken-little. Yet,\nsome short-distance syntactic regularities can be\nobserved. Articles precede adjectives and nouns\nbut not verbs, prepositions precede nouns, and par-\nticle to precedes a verb. On an even larger scale,\nthe clause the novel regarded the modern Labora-\n147\nUnit t−13 . . . t\nPunctuation r ( 1936-1939)\nchool in 1921.\nil 13 , 1813 .\nified in 1901.\n( 1993-1998 )\nWord ’s predictions\nral relativism\ncontributions\nwere contract\nat connections\nLatinate sufﬁx ered in inform\nthe concentra\nultural recrea\nwas accommoda\nReyes introdu\nTable 3: Top 5 Contexts for Three Units in the Network\nof the LM. The last character in each string marks the\npeak in activation.\ntory has a weaken-little director is grammatical in\nterms of the order between parts of speech 3. The\nsentence appears unnatural due to its odd seman-\ntics, but consider the following alternative choice\nof words for the same syntactic structure: the man\nthought the modern laboratory has a weaken-little\ndirector. This sentence sounds only marginally\nanomalous.\nThe predominantly well-formed output of the\nLM suggests that it is appropriate to further study\nthe linguistic regularities learned by it.\n4 Meaningful hidden units in the LM\nThe hidden units of the LM were analyzed by\nfeeding the training data back into the system and\ntracking unit activations on each timestep, i.e. af-\nter every character. For each unit, the ﬁve inputs\nwhich triggered highest activation (highest abso-\nlute value) were recorded (Kadar et al., 2016).\nAbout 40 units exhibited patterns of activation that\ncould be identiﬁed as meaningful with the human\neye. We selected three of the more interesting\nunits to brieﬂy discuss here. Table 3 shows a list\nof the top ﬁve triggers for each of these units, to-\ngether with up to 13 characters that preceded them,\nto put them in context. One unit, which we’ll dub\nthe punctuation unit, seems to respond to closing\npunctuation marks. Another, dubbed the Latinate\nsufﬁx unit, appears to recognize contexts that are\nlikely to precede sufﬁx-ion and its variants,-ation,\n-ction and -tion.\n3That is, assuming that novel is a noun in this context and\nweaken-little is an adjective, by analogy with its second base.\nFigure 1: Activation of the word unit. Query: its daily\npaper) grew accordingly\n4.1 The word unit\nThe most interesting unit, the word unit, appears\nto recognize complete words and sub-word units\nwithin them. Figure 1 shows the activation pat-\ntern of the word unit over a partial sentence from\nthe training data. The dotted lines, which mark\nthe end of tokens, often coincide with the peaks\nin activation. The unit also recognizes the base it\nwithin its and according within accordingly. The\nbehavior of the unit could be explained either as\na signal for the end of a familiar, repeated pat-\ntern, or as a predictor of a following space. The\nfact that we don’t see a peak in activation at the\nright bracket symbol (which should be a cue for a\nfollowing space) suggests that the former explana-\ntion is more plausible. Support for this idea comes\nfrom the low correlation coefﬁcient between the\nactivation of the unit and the probability the model\nassigns to a following space: only 0.08 across the\nentire training set. It appears that the LM knows\nthat linguistic units need not occur on their own,\ni.e. that in a lot of cases a sufﬁx is very likely to\nfollow.\n5 Morphemes encoded by the LM\nMorphological segmentation aims to identify the\nboundaries in morphologically complex words,\ni.e. words consisting of multiple morphemes. A\nmorpheme boundary could separate a base from an\ninﬂectional morpheme, e.g. like+s and carrie+s,\na base from a derivational morpheme, e.g. con-\nsider+able, or two bases, e.g. air+plane. One ap-\nproach to morphological segmentation is to see it\nas a sequence-labeling task, where words are pro-\ncessed one character at a time and every between-\ncharacter position is considered a potential bound-\nary. RNNs are particularly suitable for sequence\n148\nlabeling and recent work on supervised morpho-\nlogical segmentation with RNNs shows promising\nresults (Wang et al., 2016).\nIn this experiment we probe the LM using a\nmodel for morphological segmentation to test the\nextent to which the LM captures morphological\nregularities.\n5.1 Formulation\nAt each timestep t, character ct is projected into\nhigh-dimensional space:\nxct = ET vi E ∈R|Vchar|×dchar\nThe hidden state of the encoder is obtained as be-\nfore: henc\nt = LSTM enc(xct ; henc\nt−1). The hidden\nstate of the decoder is then obtained as: hdec\nt =\nLSTM dec(henc\nt ; hdec\nt−1) and followed by a linear\ntransformation and a softmax function over all el-\nements in Vlab, which results in a probability dis-\ntribution over labels.\np(lt = l |c, lword) = softmax(Wdec\no hdec\nt + bdec\no )i\n∀ l ∈Vlab\nwhere lword refers to all previous labels for the cur-\nrent word and i refers to the index of l in Vlab.\nThe embedding matrix, E and LSTM enc\nweights are taken from the LM. Decoder weights\nand bias terms are learned during training.\n5.2 Data\nThe model (hereafter referred to as C2M,\ncharacter-to-morpheme) was trained on a com-\nbined set of gold standard (GS) segmentations\nfrom MorphoChallenge 2010 and Hutmegs 1.0\n(free data). The data consisted of 2275 word\nforms; 90% were used for training and 10% for\ntesting. For the purposes of meaningful LM em-\nbeddings, which are highly contextual, a past con-\ntext is necessary. Since GS segmentations are\navailable for words in isolation only, we extract\ncontexts for every word from the Wiki data, tak-\ning the 15 word tokens that preceded the word on\nup to 15 of its appearances in the dump. The oc-\ncurrence of a word in each of its contexts was then\ntreated as a separate training instance.\n5.3 Performance\nThe system achieved a rather low F1 score: 53.3.\nCompared to Ruokolainen et al. (2013), who ob-\ntain F1 score 86.5 with a bidirectional CRF model,\nC2M is clearly inferior. This is not particularly\nModel Precision Recall F1\nC2M - WE 76.6 62.6 68.9\nC2M - ¬WE 23.1 34.2 27.6\nC2M - EOW 98.5 84.4 90.90\nC2M - ¬PREF 53.6 59.2 56.3\nTable 4: C2M Performance. WE stand for word edge,\nEOW for end of word, and PREF for preﬁx.\nsurprising given that the CRF makes predictions\nconditioned on past and future context, while C2M\nonly has access to the past context. Recall also that\nthe encoder of C2M shares the weights of the LM\nand is not ﬁne-tuned for morphological segmenta-\ntion. But taken as a probe of the LM’s encoding,\nthe F1 score of 53.3 suggests that this encoding\nstill contains some information about morpheme\nboundaries. A breakdown of morphemic bound-\naries by type provides insights into the source of\nperformance and limitations of C2M.\nPotential word endings as cues for morpheme\nboundaries The results labeled C2M - WE and\nC2M - ¬WE in Table 4 refer to two types of mor-\npheme boundaries: boundaries that could also be\na word ending (WE), e.g. drink+ing, agree+ment,\nand boundaries that could not be a word ending\n(¬WE), e.g. dis+like, intens+ify. It becomes ap-\nparent that a large portion of the correct segmen-\ntations produced by C2M can be attributed to an\nability to recognize word endings. Earlier ﬁndings\nrelating to the word unit of the LM (section 4.1)\nalign with this line of argument: the unit indeed\ndetects words, and those morphemes that resem-\nble words. The sample segmentations in A and C\nof Table 5 can be straightforwardly explained in\nterms of transfer knowledge on word endings:act,\naction and ant are all words the LM has encoun-\ntered during training. Notice that the morpheme\nant has not been observed by C2M, i.e. it is not in\nthe training data, but its status as a word is encoded\nby the LM.\nActual word endings An interesting result\nemerges when C2M’s performance is tested on\nword-ﬁnal characters, which by default should\nall be labeled as a morpheme boundary ( C2M -\nEOW in Table 4). Recall that the rest of the\nresults exclude these predictions, since morpho-\nlogical segmentation concerns word-internal mor-\npheme boundaries. C2M performs extremely well\nat identifying actual word endings. The margin be-\ntween C2M - WEresults and C2M - EOWresults is\n149\nInput True Segmentation Predicted Segmentation Correct\nA. actions act+ion+s act+ion+s ✓\nB. acquisition acquisit+ion acquisit+ion ✓\nC. antenna antenna ant+enna\nD. included in+clud+ed in+clude+d ✓\nE. intensely in+tense+ly intense+ly\nF. misunderstanding mis+under+stand+ing misunder+stand+ing\nG. woodwork wood+work wood+work ✓\nTable 5: Sample predictions of morphological segmentations.\nsubstantial, even though both look at units of the\nsame type, namely words. The higher accuracy\nin the EOW setting shows that the LM prefers to\nends word where they actually end, rather than at\nearlier points that would have also allowed it. The\nLM thus appears to take into consideration context\nand what words would syntactically ﬁt in it. Con-\nsider example E in Table 5. This instance of the\nword in+tense+ly occurred in the context of the\nHimalayan regions of. In this context C2M fails\nto predict a morpheme boundary, even though in\nis a very frequent word on its own. The LM may\nbe aware that preposition in would not ﬁt syntacti-\ncally in the context, i.e. that the sequence regions\nof in is ungrammatical. It thus waits to see a longer\nsequence that would better ﬁt the context, such as\nintense or intensely. Example D shows that C2M\nis indeed capable of segmenting preﬁx in in other\ncontexts. The word included is preceded by the\ncontext the English term propaganda . This con-\ntext allows a following preposition: the English\nterm propaganda in, so the LM predicted that the\nword may end after just in, which allowed C2M to\ncorrectly predict a boundary after the preﬁx.\n6 Parts of speech encoded by the LM\nPart-of-speech (POS) tagging is also seen as a se-\nquence labeling task because words can take on\ndifferent parts of speech in different contexts. Ac-\ncess to the subword level can be highly beneﬁcial\nto POS tagging, since the shape of words often re-\nveals their syntax: words ending in -ed, for exam-\nple, are much more often verbs or adjectives than\nnouns. Recent studies in the area of POS tagging\ndemonstrate that processing input on a subword-\nlevel indeed boosts the performance of such sys-\ntems (dos Santos and Zadrozny, 2014). Here we\nprobe the LM with a POS-tagging model, here-\nafter C2T (character-to-tag). Its formulation is\nidentical to that of C2M.\n6.1 Data\nWe used the English UD corpus (with UD POS\ntags) with an 80-10-10 train-dev-test split. Train-\ning data for character-level prediction was created\nby pairing each character of a word with the POS\ntag of that word, e.g. ’likeVERB’ was labeled as as\n⟨VERB VERB VERB VERB ⟩. UD doesn’t specify\na POS tag for the space character, so we used the\ngeneric X tag for it. Similarly to C2M, encodings\nfor C2T were obtained for words in context.\n6.2 Performance\nC2T obtained an accuracy score of 78.85% on\nthe character level and 87.06% on the word level,\nwhere word-level accuracy was measured by com-\nparing the tag predicted for the last character of a\nword to the gold standard tag. The per-character\nscore is naturally lower by a large margin, as pre-\ndictions early on in the word are based on very lit-\ntle information about the identity of the word. No-\ntice that the per-word score for C2T falls short of\nthe state-of-the-art in POS tagging due a structural\nlimitation: the tagger assigns tags based on just\npast and present information. The high accuracy\nof C2T in spite of this limitation suggests that the\nmajority of the information concerning the POS\ntag of a word is contained within that word and its\npast context, and that the LM is particularly good\nat encoding this information.\nEvolution of Tag Predictions over Time Fig-\nure 6.2 illustrates the evolution of POS tag pre-\ndictions over the string and I have already over-\nheard youngsters (extract from the UD data) as\nprocessed by C2T. Early into the wordalready, for\nexample, C2T identiﬁes the word, recognizes it as\nan adverb and maintains this hypothesis through-\nout. With respect to the morphologically complex\nword youngsters we see C2T making reasonable\npredictions, predicting PRON for you-, ADJ for\nthe next two characters and NOUN for youngster-\nand youngsters.\n150\nFigure 2: C2T Evolution of POS Tag Predictions. Red rectangles point to the correct tags of words.\n7 Selectional restrictions in the LM\nEnglish derivational sufﬁxes have selectional re-\nstrictions with respect to the syntactic category of\nthe base they would attach to. Sufﬁxes -al, -ment\nand -ance, for example, only attach to verbs, e.g.\nbetrayal, annoyance, containment , while -hood,\n-ous, and -ic only attach to nouns, as in nation-\nhood, spacious and metallic.4 The former are thus\nknown as deverbal, and the latter as denominal.\nCertain sufﬁxes are members of more than one\nclass, e.g. -ful attaches to both verbs and nouns, as\nin forgetful and peaceful, respectively. Since our\nLM appears to encode information about (some)\nmorphological units and part of speech, it is natu-\nral to wonder whether it also encodes information\nabout selectional restrictions of derivational suf-\nﬁxes. If it does, then the probability of a deverbal\nsufﬁx should be signiﬁcantly higher after a verbal\nbase than after other bases, likewise with denom-\ninal sufﬁxes and nominal bases. Our next experi-\nment tests whether this is so.\n7.1 Method\nOur experiment measures and compares the proba-\nbility of sufﬁxes with different selectional restric-\ntions across subsets of nominal, verbal, and ad-\njectival bases, as processed by the LM. We use\ncarefully chosen nonce words as bases in order\nto abstract away from previously seen base-sufﬁx\ncombinations, which the model may have simply\nmemorized.\nProbability We compute the probability of a\nsufﬁx given a base as the joint probability of its\ncharacters. For example, the probability of sufﬁx\n4All examples are from Fabb (1988).\n-ion attaching to base edit is:\np(ion |edit) =p(i |edit)×p(o |editi)×p(n |editio)\nSince the LM is a wordless language model,\np(·|base) is approximated from p(·|c) where c is\nthe entire past. The probability of a sufﬁx in the\ncontext of a particular syntactic category was com-\nputed as the average probability over all bases be-\nlonging to that category.\nNonce Bases Nonce bases were obtained by\nsampling complete words from the LM—that is,\nsequences delimited by a space or punctuation\non both sides. We discarded all words that ap-\npeared in an English dictionary, and imposed sev-\neral restrictions on the remaining candidates: their\nprobability had to be at most one standard devi-\nation below the mean for real words (to ensure\nthey weren’t highly unlikely accidents of the sam-\npling procedure), and the probability of a follow-\ning space character had to be at most one stan-\ndard deviation below the mean for real words (to\navoid prematurely ﬁnished words, such as measu\nand experimen). In addition, nonce words had\nto be composed entirely of lowercase characters\nand couldn’t end in a sufﬁx (as certain sufﬁxes\nincluded in the experiment only attach to base\nstems). The candidates that met these conditions\nwere labeled for POS using C2T. The ﬁnal nonce\nbases used were the ones whose POS tag con-\nﬁdence was at most one standard deviation be-\nlow the mean conﬁdence with which tags of real\nwords were assigned. Some examples of nonce\nwords from the ﬁnal selection are shown in Table\n6. An embedding was recorded for every nonce\nword that met these conditions by taking the hid-\n151\nNoun crystale, algoritum, cosmony, landlough\nVerb underspire, restruct, actrace\nAdjective nucleent, transplet, orthouble\nTable 6: Sample Nonce Bases\nNoun -ous, -an, -ic, -ate, -ary, -hood, -less, -ishVerb -ance, -ment, -ant, -ory, -ive, -ion, -able, -ablyAdjective -ness, -ity, -en\nTable 7: Syntactically unambiguous derivational suf-\nﬁxes\nden state of the language model at the end of the\nword in context.\nSufﬁxes The sufﬁxes included in this experi-\nment (listed in Table 7) were taken from Fabb\n(1988), one of the most extensive studies of the\nselectional restrictions of English derivational suf-\nﬁxes. Fabb discussed 43 sufﬁxes, many of which\nattach to a base of two out of three available syn-\ntactic categories, e.g. -ize attaches to both nouns,\nas in symbolize, and adjectives, as in specialize.\nThe analysis of such syntactically ambiguous suf-\nﬁxes is complex since the frequency with which\nthey attach to each base type should be taken into\nconsideration, but such statistics are not readily\navailable and require morphological parsing. For\nthe purposes of the present study ambiguous suf-\nﬁxes were thus excluded and only the remaining\nnineteen sufﬁxes were used.\n7.2 Results\nFigure 3 shows the results from the experiment.\nEleven out of nineteen sufﬁxes exhibit the ex-\npected behavior: sufﬁxes -ment, -ive, -able, -ably,\n-an, -ic, -ary, -hood, -less, ness and -ity are more\nprobable in the context of their corresponding syn-\ntactic base than in other contexts. Sufﬁx-ment, for\ninstance, is more than twice as probable in the con-\ntext of a verbal base than in the context of a nomi-\nnal or an adjectival base. The fact that almost 70%\nof sufﬁxes ‘select’ their correct bases, points to a\nlinguistic awareness within the LM with respect to\nthe selectional restrictions of sufﬁxes.\nDespite the overall success of the LM in this\nrespect, some sufﬁxes show a deﬁnitive prefer-\nence for the wrong base. A further analysis of\nsome of these cases shows that they don’t neces-\nsarily counter the evidence for syntactic awareness\nwithin the LM.\nFigure 3: Sufﬁx Probability Following Nominal (blue),\nVerbal (green) and Adjectival (red) Bases. Sufﬁxes are\ngrouped according to their selectional restrictions: (a)\ndeverbal, (b) denominal and (c) deadjectival. Eleven\nout of nineteen sufﬁxes obtained highest probability\nfollowing the syntactic category that matched their se-\nlectional restrictions.\n7.3 Sufﬁx -ion\nDeverbal sufﬁx -ion should be most probable\nfollowing verbs, but prefers nominal bases.\nNotice that -ion is a noun-forming sufﬁx:\ncommunicateV ERB → communicationNOUN ,\nregressV ERB→regressionNOUN . It appears that\nfrom the perspective of the LM, the syntactic\ncategory of such morphologically complex forms\nextends to their bases, e.g. the LM perceives\nthe populat substring in population as nominal.\nThis observation can be explained precisely with\nreference to the high frequency of sufﬁx -ion:\nthe sufﬁx itself occurred in 18,945 words in the\ndataset, while the frequency of its various bases\nin isolation, e.g. of populate, regress, etc., was\nestimated to be only 9,7555. This shows that bases\nthat can take sufﬁx -ion were seen more often\nwith it than without it. As a consequence, the LM\nis biased to expect a noun when seeing one of\nthese character sequences and may thus perceive\nthe base itself as a nominal one.\nThe tag prediction evolution over population\nand renewable in Figure 4, show that this is indeed\nthe case, by comparing a base sufﬁxed with-ion to\na base sufﬁxed with -able (whose selectional re-\nstrictions, we know, were learned correctly). For\nboth words C2T starts off predicting NOUN. For\n5To estimate this, we removed the sufﬁx of each word and\nthen searched for the remainder in isolation, followed by e\nand followed by s/ es—e.g. for population we counted the\noccurrences of populat, populate, populates and populated.\n152\nFigure 4: C2T Evolution: (a) population and (b) renewable. The red square points to the syntactic category of the\nbase.\nrenew it switches to VERB, which is the correct\ntag for this base, and only upon seeing the sufﬁx,\nprogresses to the conclusive ADJ tag for renew-\nable. For population, in contrast the prediction\nremains constant at NOUN, which is indeed the\ncategory of the word, as determined by the sufﬁx.\n8 Conclusion\nThis work presented an exploratory analysis of\na ‘wordless’ character language model, aiming\nto identify the morpho-syntactic regularities cap-\ntured by the model.\nThe ﬁrst conclusion of this work is that mor-\npheme boundaries are mainly learned by the LM\nthrough analogy with syntactic boundaries. Find-\nings relating to the extremely frequent sufﬁx -ion\nillustrate that the LM was able to learn to identify\npurely morphological boundaries through general-\nization. But a prerequisite for this generalization\nis that a morpho-syntactic boundary was also seen\nin the relevant position during training.\nThe second conclusion is that having recog-\nnized certain boundaries and by extension, the\nunits that lie between them, the model could also\nlearn the regularities that concern these units, e.g.\nthe selectional restrictions of most derivational\nsufﬁxes included in the study could be captured\naccurately.\n9 Implications for future research\nThe above conclusions have strong implications\nwith respect to the use of character-level LMs for\nlanguages other than English.\nEnglish is the perfect candidate for character-\nlevel language modeling, due to its fairly poor in-\nﬂectional morphology. The nature of English is\nsuch that the boundary between a base and a suf-\nﬁx is often also a potential word boundary, which\nmakes sufﬁxes easily segmentable. This is not\nthe case for many languages with richer and more\ncomplex morphology. Without access to the units\nof verbal morphology, it is less clear how the\nmodel would learn these types of regularities. This\nshortcoming should hold not just for the LM but\nfor any character-level language model that pro-\ncesses input as a stream of characters without seg-\nmentation on the subword level.\nThis implication is in line with the results of\nVania and Lopez (2017) showing that for many\nlanguages, language modeling accuracy improves\nwhen the model is provided with explicit morpho-\nlogical annotations during training, with English\nshowing relatively small improvements. Our anal-\nysis might explain why this is so; we expect anal-\nyses of other languages to yield further insight.\nFinally, we should point out that it may not\nbe the case that a single, highly-speciﬁed word\nunit should exist in every character-level LM. Qian\net al. (2016) ﬁnd that different levels of linguistic\nknowledge are encoded with different model ar-\nchitectures, and K ´ad´ar et al. (2018) ﬁnd that even\na different initialization of an otherwise identi-\ncal model can results in very different hierarchi-\ncal processing of the input. We consider ourselves\nlucky for coming across this particular setup that\nproduced a model with very interpretable behav-\nior, but we also acknowledge the importance of\nevaluating the reliability of the word unit ﬁnding\nin future work.\n153\nAcknowledgements\nWe thank Sorcha Gilroy, Joana Ribeiro, Clara Va-\nnia, and the anonymous reviewers for comments\non previous drafts of this paper.\nReferences\nWilliam Chan, Navdeep Jaitly, Quoc Le, and Oriol\nVinyals. 2016. Listen, attend and spell: A neural\nnetwork for large vocabulary conversational speech\nrecognition. In Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2016 IEEE International Confer-\nence on, pages 4960–4964. IEEE.\nJunyoung Chung, Kyunghyun Cho, and Yoshua Ben-\ngio. 2016. A character-level decoder without ex-\nplicit segmentation for neural machine translation.\narXiv preprint arXiv:1603.06147.\nNigel Fabb. 1988. English sufﬁxation is constrained\nonly by selectional restrictions. Natural Language\nand Linguistic Theory, 6(4):527–539.\nAkos Kadar, Grzegorz Chrupala, and Afra Alishahi.\n2016. Representation of linguistic form and func-\ntion in recurrent neural networks.\n´Akos K ´ad´ar, Marc-Alexandre C ˆot´e, Grzegorz\nChrupała, and Afra Alishahi. 2018. Revisiting\nthe hierarchical multiscale lstm. arXiv preprint\narXiv:1807.03595.\nAndrej Karpathy. 2015. The unreasonable effective-\nness of recurrent neural networks. Andrej Karpathy\nblog.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In Proceedings of AAAI, pages 2741–2749.\nJason Lee, Kyunghyun Cho, and Thomas Hofmann.\n2016. Fully Character-Level Neural Machine Trans-\nlation without Explicit Segmentation. Acl-2016,\npages 1693–1703.\nPeng Qian, Xipeng Qiu, and Xuanjing Huang. 2016.\nAnalyzing linguistic knowledge in sequential model\nof sentence. In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 826–835.\nTeemu Ruokolainen, Oskar Kohonen, Sami Virpioja,\nand Mikko Kurimo. 2013. Supervised morphologi-\ncal segmentation in a low-resource learning setting\nusing conditional random ﬁelds. In CoNLL, pages\n29–37.\nCicero Nogueira dos Santos and Bianca Zadrozny.\n2014. Learning Character-level Representations\nfor Part-of-Speech Tagging. Proceedings of the\n31st International Conference on Machine Learn-\ning, ICML-14(2011):1818–1826.\nIlya Sutskever, James Martens, and Geoffrey E Hin-\nton. 2011. Generating text with recurrent neural\nnetworks. In Proceedings of the 28th International\nConference on Machine Learning (ICML) , pages\n1017–1024.\nClara Vania and Adam Lopez. 2017. From Characters\nto Words to in Between : Do We Capture Morphol-\nogy ? To appear in Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics.\nLinlin Wang, Zhu Cao, Yu Xia, and Gerard de Melo.\n2016. Morphological segmentation with window\nlstm neural networks. In AAAI, pages 2842–2848.\nRon J Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui\nWu, and Zhifeng Chen. 2017. Sequence-to-\nsequence models can directly translate foreign\nspeech. arXiv preprint arXiv:1703.08581."
}