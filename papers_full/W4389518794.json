{
  "title": "Investigating Efficiently Extending Transformers for Long Input Summarization",
  "url": "https://openalex.org/W4389518794",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2891256592",
      "name": "Jason Phang",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2096674145",
      "name": "Yao Zhao",
      "affiliations": [
        "Google (United States)",
        "Brain (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2095838526",
      "name": "Peter Liu",
      "affiliations": [
        "Brain (Germany)",
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4306672396",
    "https://openalex.org/W3163073193",
    "https://openalex.org/W4221148258",
    "https://openalex.org/W4385573254",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3171639395",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W4221153884",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4287019748",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W4226275767",
    "https://openalex.org/W4226174367",
    "https://openalex.org/W3170490008",
    "https://openalex.org/W4353089798",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4297899311",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3201357245",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4289598175",
    "https://openalex.org/W4320087074",
    "https://openalex.org/W4286750685",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W4385573804",
    "https://openalex.org/W3015468748"
  ],
  "abstract": "While large pretrained Transformer models have proven highly capable at tackling natural language tasks, handling long sequence inputs still poses a significant challenge. One such task is long input summarization, where inputs are longer than the maximum input context of most models. Through an extensive set of experiments, we investigate what model architectural changes and pretraining paradigms most efficiently adapt a pretrained Transformer for long input summarization. We find that a staggered, block-local Transformer with global encoder tokens strikes a good balance of performance and efficiency, and that an additional pretraining phase on long sequences meaningfully improves downstream summarization performance. Based on our findings, we introduce PEGASUS-X, an extension of the PEGASUS model with additional long input pretraining to handle inputs of up to 16K tokens, which achieves strong performance on long input summarization tasks comparable with much larger models.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3946–3961\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nInvestigating Efﬁciently Extending Transformers\nfor Long Input Summarization\nJason Phang 1∗ Yao Zhao2 Peter J. Liu2\n1New York University, 2Google Research, Brain Team\njasonphang@nyu.edu\n{yaozhaoyz, peterjliu}@google.com\nAbstract\nWhile large pretrained Transformer models\nhave proven highly capable at tackling natural\nlanguage tasks, handling long sequence inputs\nstill poses a signiﬁcant challenge. One such\ntask is long input summarization, where inputs\nare longer than the maximum input context of\nmost models. Through an extensive set of ex-\nperiments, we investigate what model architec-\ntural changes and pretraining paradigms most\nefﬁciently adapt a pretrained Transformer for\nlong input summarization. We ﬁnd that a stag-\ngered, block-local Transformer with global en-\ncoder tokens strikes a good balance of perfor-\nmance and efﬁciency, and that an additional\npretraining phase on long sequences mean-\ningfully improves downstream summarization\nperformance. Based on our ﬁndings, we in-\ntroduce PEGASUS-X, an extension of the PE-\nGASUS model with additional long input pre-\ntraining to handle inputs of up to 16K tokens,\nwhich achieves strong performance on long\ninput summarization tasks comparable with\nmuch larger models.\n1 Introduction\nLarge pretrained Transformer models have proven\nto be extremely capable at tackling natural lan-\nguage tasks (Devlin et al., 2018; Brown et al.,\n2020). However, handling long textual sequences\ncontinues to be a signiﬁcant challenge for these\nmodels. Training models to handle long sequences\nis expensive in both computation and memory, and\nmoreover requires training and evaluating on long\nsequence data, which is rarer and more costly to\ncollect. Given the broad success of Transformer\nmodels on short-sequence language tasks, our goal\nis to investigate the best way to extend these models\nto handle longer input sequences.\nIn this work, we focus on the task of long in-\nput summarization: summarizing long input doc-\numents into shorter text sequences. The inputs\n∗ Work done while at Google.\n200M 400M 800M 1600M 3200M\n# Params\n22.5\n23.0\n23.5\n24.0\n24.5\n25.0\n25.5SCROLLS Summ. Score\nPEGASUS-Xbase\nPEGASUS-X\nLongT5base\nLongT5large\nLongT5XL\nBARTbase-SLED\nBARTlarge-SLED\nBARTbase-LED\nFigure 1: Performance on SCROLLS (Shaham et al.,\n2022) summarization tasks. All models evaluated on\nup to 16K input tokens. PEGASUS-X outperforms\nother models at comparable model sizes. Scores (as of\n08/08/22) shown are the average of the geometric mean\nof ROUGE-1/2/L.\nof such tasks are often signiﬁcantly longer than\nthe maximum input lengths of most standard Trans-\nformer models, and hence warrant both architecture\nmodiﬁcations as well as new training regimes. For\ninstance, to avoid the quadratic growth in memory\nconsumption of attention in Transformers, many\nmemory-efﬁcient Transformer variants have been\nproposed (Tay et al., 2020, 2021). However, the\nmanner in which these changes are incorporated\ninto models has been inconsistent and ad-hoc, and\nthere are few established best practices. For in-\nstance, some works directly ﬁne-tune on long-input\nsummarization tasks (Zaheer et al., 2020; Pang\net al., 2022), while others ﬁrst perform additional\npretraining (Beltagy et al., 2020). Because of the\nhigh cost of training these models, there has yet to\nbe a systematic study of how best to adapt models\nfor long input sequences. Hence, it has been difﬁ-\ncult to establish which model and training changes\nare necessary or complementary.\nTo answer these questions, we conduct an ex-\ntensive empirical investigation into the architec-\ntural changes, model conﬁgurations and pretraining\n3946\nschemes to identify better approaches to training\nTransformer models for long input summarization.\nWe evaluate a set of efﬁcient Transformer variants,\nand propose a simple block-wise local Transformer\narchitecture with staggered blocks and global to-\nkens that strikes a good balance of performance and\nmemory efﬁciency. We show that given a ﬁxed to-\nken budget, pretraining on short sequences and then\npre-adapting the model to an efﬁcient Transformer\narchitecture by training on longer sequences leads\nto better performance than only long input pretrain-\ning or no adaptation at all. We also investigate\nmodel design choices such as position encoding\nschemes, encoder-decoder layer distributions, and\nthe impact of discrepancies between pretraining\nand ﬁne-tuning architecture hyperparameters.\nBased on the ﬁndings from our empirical inves-\ntigation, we adapt the pretrained PEGASUS Large\nmodel (Zhang et al., 2020) to tackle long input\nsummarization on up to 16K input tokens. The re-\nsulting model, which we call PEGASUS-X, attains\ntop scores on long summarization tasks, outper-\nforming much larger models like LongT5 (Guo\net al., 2021). Moreover, impact on short input sum-\nmarization performance is minimal. A smaller ver-\nsion which we call PEGASUS-XBase attains simi-\nlar scores with much fewer parameters. Beyond\nsummarization, we believe that many of our ﬁnd-\nings will be useful to the community for efﬁciently\nadapting Transformer models to handle ever longer\ninput sequences for other tasks.\nIn summary, our contributions are:\n1. We evaluate a series of proposed efﬁcient\nTransformer architectures as well as other\nmodel modiﬁcations, and report their efﬁcacy\nand computational trade-offs when applied to\nlong input summarization tasks.\n2. Based on our ﬁndings, we propose a recipe\nfor adapting a short-context, pretrained Trans-\nformer encoder-decoder to longer inputs, and\napply it to PEGASUS to greatly improve its\nlong-document summarization performance,\nwith comparable short-input performance.\n2 Experimental Setup\nSimilar to Zhang et al. (2020), we perform the ma-\njority of our experiments with a PEGASUS Base-\nsized model, before applying our ﬁndings to\nPEGASUSLarge-sized model.\n2.1 Pretraining\nWe generally follow the recipe from PEGASUS\n(Zhang et al., 2020) for pretraining PEGASUSBase-\nsized models. All experiments in our ablation study\nperformed pretraining with C4 (Raffel et al., 2020)\nfor 500k steps with 512 input tokens and 256 output\ntokens and a masking ratio of 45%, unless other-\nwise stated. For long input pretraining we extend\nthe input length to 4096 tokens, and adjust the\nmasking ratio from 45% to 5.625%, reducing the\nratio by a factor of 8 to account for the 8x increase\nin input sequence length. We also ﬁlter for only\ndocuments longer than 10000 characters.\n2.2 Fine-tuning\nWe evaluate models by ﬁne-tuning on the arXiv\n(Cohan et al., 2018) and GovReport (Huang et al.,\n2021) long input summarization tasks. Where\nrelevant, we also ﬁne-tune on the shorter-context\nXSUM and CNN/DailyMail tasks. For each ex-\nperiment, we report the best validation set scores\nbased on the geometric average (RG) of ROUGE-1,\nROUGE-2 and ROUGE-L scores (Lin, 2004) based\non the rouge-score package.1 Fine-tuning hy-\nperparameters can be found in Appendix E. Unless\notherwise stated, we directly switch to the efﬁcient\nTransformer architectures between pretraining (on\nshorter context) and ﬁne-tuning (on longer con-\ntexts), with no adaptation phase in between.\n3 Ablation Experiments\n3.1 Encoder architectures\nWe ﬁrst investigate whether using an efﬁcient\nTransformer encoder allows models to incorpo-\nrate longer input sequences while consuming rea-\nsonable amounts of device memory. We consider\ntwo encoder architectures that exemplify different\napproaches to efﬁcient attention. Big Bird (Za-\nheer et al., 2020) uses sparse attention computation,\ncombining sliding-window and random attention,\nand a set of global-attention tokens. Conversely,\nPerformer (Choromanski et al., 2021) factorizes\nattention matrices via orthogonal random features.\nBoth model also performed well on the LRA tasks\n(Tay et al., 2021). For this experiment, we perform\nboth pretraining and ﬁne-tuning with the same en-\ncoder architecture to avoid the issue of mismatch\nbetween pretraining and ﬁne-tuning architectures.\n1https://github.com/google-research/\ngoogle-research/tree/master/rouge\n3947\nXSUM CNN/DM arXiv GovReport\nEncoder R1 / R2 / RL RG R1 / R2 / RL RG R1 / R2 / RL RG R1 / R2 / RL RG Steps/s Mem\nTransformer 40.0/ 16.9/ 32.0 27.9 39.5 / 19.0/ 28.6 27.8 - / - / - - - / - / - - - -\nBigBird 39.6 / 16.7 / 31.7 27.6 39.3 / 18.2 / 28.1 27.2 46.8 / 19.6 / 28.0 29.5 60.5 / 28.5 / 30.1 37.3 0.31 1.88\nPerformer 36.5 / 14.0 / 28.7 24.5 37.4 / 17.4 / 26.9 26.0 39.0 / 13.2 / 23.8 23.1 55.8 / 20.2 / 24.7 30.3 0.96 1.12\nLocal 38.5 / 15.7 / 30.6 26.4 39.0 / 18.4 / 28.1 27.2 46.5 / 19.7 / 27.9 29.5 60.2 / 28.3 / 30.0 37.1 1.00 1.00\nGlobal-Local 38.7 / 16.2 / 31.2 26.9 39.0 / 18.6 / 28.2 27.3 47.6/ 20.2/ 28.5 30.1 61.4 / 29.3/ 30.6 38.0 0.87 1.08\nTable 1: Comparison of different encoder architectures on short (XSUM, CNN/DM) and long (arXiv, GovReport)\nsummarization tasks. Training steps per second and memory are computed based on arXiv, and normalized to\nLocal Transformer performance.\nStagger\nLocal Blocks\nUse Global\nIn Decoder\narXiv GovReport\nEncoder R1 / R2 / RL RG R1 / R2 / RL RG\nGlobal-Local /enc-33 /enc-3348.1 / 20.3 / 28.5 30.3 60.5 / 28.8 / 30.5 37.6\nGlobal-Local /enc-3347.0 / 19.5 / 27.9 29.5 60.9 / 28.9 / 30.2 37.6\nGlobal-Local /enc-33 47.7 / 20.4 / 28.6 30.3 61.3 / 29.4 / 30.8 38.1\nGlobal-Local 46.7 / 19.5 / 27.9 29.4 59.5 / 27.8 / 29.4 36.5\nLocal /enc-33 - 46.8 / 19.7 / 28.0 29.6 59.2 / 27.9 / 30.0 36.7\nLocal - 46.5 / 19.2 / 27.5 29.1 58.8 / 27.5 / 28.9 36.0\nTable 2: Comparison of architectural tweaks to Local and GlobalLocal encoder. Staggering local blocks uses\ndifferent blocks boundaries for different layers in block-local attention. Global information is incorporated in the\ndecoder via an additional cross-attention before cross-attention over the encoded input.\nIn addition, we also introduce two simple vari-\nants of local attention Transformer encoders. First,\nwe use a simple block-local Transformer (Local),\nwhere encoder input tokens are divided into non-\noverlapping blocks, and tokens can only attend to\nother tokens within the block. Second, we extend\nthe local Transformer by adding a set of global to-\nkens with learned embeddings, that can attend to\nand be attended from every encoder token (Global-\nLocal). These components are similar to the slid-\ning window attention and global token attention\nof Big Bird, ETC (Ainslie et al., 2020) and Long-\nformer (Beltagy et al., 2020). However, we opt for\nthe simpler block-local attention rather than sliding\nwindow attention, and compensate for the lack of\noverlapping blocks by staggering the local atten-\ntion blocks, which we elaborate on in Section 3.2.\nAs we show below, the performance is highly com-\npetitive despite its simplicity.\nResults on short and long summarization tasks\nare shown in Table 1, with the relative training\nsteps per second and memory consumed per device\nfor ﬁne-tuning on arXiv shown in the right-most\ncolumns. Among the short tasks, the full-attention\nTransformer performs best, followed by BigBird.\nOn the long tasks, Big Bird and Global-Local mod-\nels perform best, but Big Bird consumes signiﬁ-\ncantly more memory and trains much more slowly\nthan the other architectures. Conversely, although\nPerformer has relatively low memory consumption\nand trains efﬁciently, it performs worst among the\narchitectures tested by a noticeable margin.\nOn the other hand, Local and Global-Local en-\ncoders strike a good balance of performance and\nefﬁciency. The simple local attention encoder,\nwhich uses block-local attention, attains perfor-\nmance close to that of Big Bird while being much\nfaster and using much less memory. Global-Local\ntrades off a small amount of speed and memory for\nbetter performance, outperforming Big Bird.\nTakeaways: Local attention is a strong baseline,\nand adding global tokens signiﬁcantly improves\nperformance. Both models are resource-efﬁcient.\n3.2 Local and Global-Local conﬁgurations\nGiven the good performance of both Local and\nGlobal-Local encoder variants, we next consider\nfurther architectural tweaks to these models.\nFirst, we introduce staggering of local attention\nblocks. In block-local attention, tokens can only\nattend to other tokens within the same block. If the\ninput tokens are divided up into the same blocks\nin every layer, this means that no information is\nexchanged across blocks through the entire encoder.\nTo address this pitfall, we stagger attention blocks\nby shifting the block boundaries by half a block\nevery other layer. We show an example of this in\nFigure 2. In practice, we implement this by padding\n3948\n(a) Block-local attention\n (b) Block-local attention with staggered blocks\nFigure 2: In block-local attention (a), the same block boundaries are used across all layers, preventing information\nfrom being shared across blocks. Staggering the block boundaries (b) be shifting the boundaries every other layer\nallows for cross-block interactions with minimal additional computational cost or complexity.\nthe hidden representations on either side by half a\nblock and masking accordingly.\nSecondly, in the Global-Local model, the de-\ncoder only attends to the encoded token representa-\ntions, and not the global token representations. We\nconsider a variant where we supply the global to-\nken representations to the decoder and introduce a\nsecond cross-attention layer that attends only to the\nglobal tokens. Our goal is to allow the decoder to\nincorporate global information before performing\ncross-attention over the encoded sequence.\nResults are shown in Table 2. We ﬁnd that\nstaggering local blocks noticably improves perfor-\nmance in both Local and Global-Local models. Per-\nformance improves even with Global-Local mod-\nels, which already allow for cross-block interac-\ntions via global tokens, indicating that both model\nimprovements are complementary. Conversely, in-\ncorporating global token information in the decoder\ndid not lead to much performance improvement,\nparticularly once staggered local blocks were used.\nTakeaways: Staggering local attention blocks\nsigniﬁcantly improves performance, and is com-\nplementary to global tokens.\n3.3 Global-Local: Block Size and Number of\nGlobal Tokens\nNext, we vary the block size and number of global\ntokens for the Global-Local encoder, with results\nshown in Table 3.2 Broadly, we ﬁnd that increas-\ning either block size or global tokens leads to im-\nproved performance, with a corresponding increase\nin memory consumption and computation time.\nHowever, the effect size from going to larger block\nsizes is not large, and saturates with larger block\nsizes or number of global tokens. As such, in-\ncreasing either of these hyperparameters is ideal if\n2Experiments with very small block sizes or number global\ntokens ran into memory issues, because TPUs pad small di-\nmensions of arrays to certain minimum lengths, leading to\nlarger than expected memory consumption.\nresources allow, but is not a high priority compared\nto other model improvements. For the remainder\nof the ablation experiments, we use a block size of\n64 and 32 global tokens for consistency.\nTakeaways: Larger block sizes and/or number\nof global tokens leads to improved performance,\nalthough the effect saturates.\n3.4 Other Architecture Modiﬁcations\nWe further investigate a of series architectural mod-\niﬁcations to the encoder-decoder model, including\nthe position encoding scheme (Table 8), scaling the\nencoder and decoder layers (Table 10) and using\ncross-attention in only a fraction of the decoder lay-\ners (Table 12). We ﬁnd that the sinusoidal position\nencoding provide a good balance of performance\nand efﬁciency, and that a balanced encoder-decoder\nwith full cross-attention generally performs the\nbest. More details are provided in Appendix B.\n3.5 Pretraining vs Fine-tuning Architectures\nPrevious works using efﬁcient Transformer en-\ncoders have generally taken the parameters of a\nfull-attention Transformer pretrained on a shorter\nsequences and adapted them to efﬁcient architec-\ntures, either directly during ﬁne-tuning (Zaheer\net al., 2020) or with an intermediate stage of ad-\nditional pretraining (Beltagy et al., 2020). In this\nsection, we investigate if such an approach is opti-\nmal, or if models beneﬁt from being pretrained with\nefﬁcient encoders from the beginning. Note that\nwe still perform pretraining on a short sequences\n(512 tokens), even with an efﬁcient encoder.\nWe consider both pretraining with a Transformer\nand pretraining with the efﬁcient architecture for\nboth Local and Global-Local models. We also\nvary the block size, as the main difference be-\ntween a Transformer and Local Transformer is\nthe block size (aside from staggering, a Local\nmodel with block size 512 is equivalent to a dense\n3949\narXiv GovReport\nBlock Size Global Tokens R1 / R2 / RL RG R1 / R2 / RL RG Steps/s Mem\n16 32 47.1 / 20.0 / 28.3 29.9 59.7 / 27.8 / 29.2 36.5 0.92 1.15\n64 46.8 / 19.7 / 28.0 29.6 60.8 / 28.6 / 30.0 37.4 0.75 1.54\n128 47.7 / 20.0 / 28.2 30.0 60.7 / 28.8 / 30.2 37.5 0.58 1.70\n64 32 47.7 / 20.3 / 28.5 30.2 61.0 / 29.3 / 30.8 38.0 0.47 1.07\n64 47.4 / 20.2 / 28.5 30.1 60.9 / 29.1 / 30.7 37.9 0.94 1.10\n128 47.8 / 20.4 / 28.6 30.3 60.9 / 29.0 / 30.3 37.7 0.85 1.26\n128 32 46.9 / 19.7 / 28.0 29.6 60.9 / 28.7 / 30.1 37.5 1.00 1.00\n64 47.4 / 20.2 / 28.4 30.1 60.9 / 28.9 / 30.8 37.8 0.96 1.05\n128 47.1 / 20.0 / 28.3 29.9 61.0 / 28.9 / 30.6 37.8 0.90 1.15\n256 32 47.3 / 20.2 / 28.3 30.0 61.6 / 29.4 / 30.7 38.2 0.92 1.11\n64 47.2 / 20.2 / 28.4 30.0 59.2 / 28.6 / 30.5 37.2 0.88 1.16\n128 48.1 / 20.5 / 28.6 30.4 61.7 / 29.3 / 30.8 38.2 0.83 1.26\nTable 3: Varying the block size and number of global tokens in Global-Local encoders. Training steps per second\nand memory are computed based on arXiv, and normalized to the run with Block Size=128 and Global Tokens=32.\nTransformer), and hence the difference in block\nsize also corresponds to the extent to which the\nmodel needs to adapt between architectures. When\nadapting from a pretrained Transformer encoder to\na Global-Local architecture, because the Global-\nLocal model relies on newly introduced global to-\nken embeddings, we initialize them by randomly\nsampling tokens from the vocabulary embeddings.\nResults are shown in Table 11. For Local mod-\nels, pretraining with local attention using small\nblock sizes tends to hurt performance, but at mod-\nerate block sizes (e.g. 64) there is little differ-\nence between the two approaches. In contrast, for\nGlobal-Local pretraining with the efﬁcient archi-\ntecture tends to perform better. We hypothesize\nthat this difference arises because of the learned\nglobal embedding tokens, which are randomly ini-\ntialized when adapting from a pretrained Trans-\nformer and hence may beneﬁt from pretraining and\nbeing jointly trained with the local attention.\nTakeaways: For moderate block sizes, either pre-\ntraining or adapting to a Local encoder performs\nabout equally well, but pretraining with a Global-\nLocal encoder performs slightly better.\n3.6 Pretraining Schemes\nUp to this point, we have only considered pretrain-\ning with short sequences. We might expect that\npretraining with longer sequences ought to improve\nperformance on downstream long input summariza-\ntion. However, pretraining only on long sequences\nis computationally expensive and requires a large\ncollection of long input documents, which are rel-\natively rarer. Long documents may also contain\ndifferent information from short documents, hence\nlimiting training to only long inputs mae reduce\nthe diversity of training data. Different long con-\ntext Transformers have taken different approaches\nto pretraining on long inputs. For instance, Long-\nformer (Beltagy et al., 2020) performed several\nadditional stages of increasingly longer-sequence\npretraining to adapt the initial RoBERTa to long\nsequence inputs. On the other hand, LongT5 (Guo\net al., 2021) is pretrained exclusively with long in-\nput sequences. Others (Zaheer et al., 2020; Ivgi\net al., 2022) perform no long input pretraining at all.\nIn this section, we investigate how the balance of\nshort and long pretraining impact downstream per-\nformance, and try to ﬁnd the best trade-off between\npretraining cost and downstream performance.\nWe consider two setups for pretraining: short-\ninput pretraining, with 512 input tokens and 256\noutput tokens, and long-input pretraining, with\n4096 input tokens and 256 output tokens. We de-\nscribe the corresponding differences in data pre-\nprocessing in Section 2.1. We ﬁx the number of\ninput tokens seen during training, and vary conﬁg-\nurations subject to this constraint. This constraint\nroughly proxies for the amount of compute con-\nsumed and corresponds to the number of input to-\nkens seen during pretraining.3\nWe set our total input token budget at 131 billion\ntokens, which corresponds to 1 million steps with\n512 input tokens, compared to the 500k steps in the\nabove experiments. This larger budget ensures that\nwhen we only do long-input pretraining, the model\n3If we instead ﬁxed the number of training steps, long-\ninput pretraining would consume far more compute for the\nsame number of steps.\n3950\nXSUM CNN/DM arXiv GovReport\nPretraining Scheme Encoder R1 / R2 / RL RG R1 / R2 / RL RG R1 / R2 / RL RG R1 / R2 / RL RG\nShort (50%) Local 38.4 / 15.8 / 30.6 26.5 39.2 / 18.1 / 27.9 27.1 46.8 / 19.7 / 28.0 29.6 60.1 / 28.3 / 29.8 37.0\nGlobal-Local 39.4 / 16.5 / 31.5 27.4 39.1 / 18.6 / 28.3 27.4 47.7 / 20.4 / 28.6 30.3 61.9 / 29.6 / 30.8 38.4\nShort (100%) Local 39.2 / 16.3 / 31.3 27.1 39.2 / 18.6 / 28.3 27.4 46.9 / 19.7 / 28.0 29.6 60.1 / 28.3 / 29.8 37.0\nGlobal-Local 39.9/17.0/ 31.9 27.9 39.8 / 18.6 / 28.3 27.6 48.1 / 20.5 / 28.7 30.5 61.9 / 29.6 / 30.8 38.4\nShort (75%)→Long (25%) Local 38.8 / 15.9 / 30.7 26.7 39.1 / 18.2 / 28.0 27.1 47.5 / 20.1 / 28.2 30.0 60.6 / 28.9 / 30.6 37.7\nGlobal-Local 39.6 / 16.8 / 31.7 27.639.8/18.8/ 28.5 27.7 48.4 / 20.7 / 28.8 30.7 61.8 / 29.8 / 31.1 38.5\nShort (50%)→Long (50%) Local 38.4 / 15.7 / 30.5 26.4 39.4 / 18.1 / 27.9 27.1 47.7 / 20.2 / 28.3 30.1 60.9 / 29.1 / 30.7 37.9\nGlobal-Local 39.3 / 16.4 / 31.4 27.3 39.4 / 18.3 / 28.1 27.348.4/20.9/ 29.1 30.9 61.7 /30.0/ 31.2 38.7\nLong (100%) Local 36.0 / 14.0 / 28.6 24.3 38.4 / 17.7 / 27.4 26.5 46.7 / 19.5 / 27.7 29.3 59.8 / 28.0 / 29.5 36.7\nGlobal-Local 36.4 / 14.3 / 28.9 24.7 38.5 / 17.8 / 27.5 26.6 47.3 / 19.9 / 28.1 29.8 61.1 / 29.1 / 30.7 37.9\nTable 4: Comparison of different pretraining formats, given a input token budget of 131B tokens, which corre-\nsponds to 1M steps with 512 input tokens. Short pretraining uses 512 input tokens, whereas long pretraining uses\n4096 input tokens.\nis still pretrained for a reasonable number of steps.\nWe consider four pretraining conﬁgurations:\n• Short-input for 100% of tokens (1M steps)\n• Short-input for 75% of tokens (98.3B, 750k\nsteps), then long-input for 25% of tokens\n(32.8B, 31.25k steps)\n• Short-input for 50% of tokens (62.5B, 500k\nsteps), then long-input for 50% of tokens\n(62.5B, 62.5k steps)\n• Long-input for 100% of tokens (125k steps)\nWe compare the performance of the different\npretraining scehemes in Table 4. We also include\nshort-input pretraining for 500k steps for compar-\nison. First, comparing short-input pretraining for\n500k and 1M steps, we ﬁnd that more pretraining\nstill improves performance, indicating that our base\nmodels may still be undertrained at 500k steps. Sec-\nond, long-input pretraining performs consistently\nworse than the other variants, which we attribute\nhaving fewer training steps, again highlighting the\nissue of potential undertraining. For the middle\nthree conﬁgurations, on the long tasks, all three\nnon-long-only variants atttain similar scores, with\nmore long-input pretraining having slightly better\nperformance, particularly on the ROUGE-2 and\nROUGE-L scores. While the small absolute differ-\nences in scores make it hard to draw strong conclu-\nsions, we lean towards the conclusion that adding a\nshort phase of long input pretraining can improve\nperformance on long input summarization tasks.\nTakeaways: Given a ﬁxed compute budget, allo-\ncating some training steps to long-input training\ncan improve performance, although the optimal al-\nlocation is difﬁcult to determine. Exclusively long\npretraining results in worse performance.\n4 PEGASUS-X\nBased on our ﬁndings, we settle on the follow-\ning recipe for adapting PEGASUS models (Zhang\net al., 2020) to long sequence summarization.\n• We use a Global-Local architecture with block\nstaggering, a large number of global tokens,\nand large block sizes during pretraining.\n• We perform additional long input pretraining\non 4096 token inputs for 300k steps.\n• We extend input sequences up to 16384 input\ntokens in ﬁne-tuning, depending on the task.\nWe experiment with two model sizes:\nPEGASUS-X (PEGASUS e Xtended) based\non PEGASUSLarge, and PEGASUS-XBase based on\na newly trained PEGASUS Base model which we\ncall PEGASUSBase+.4\nWe initialize the weights of PEGASUS-X and\nPEGASUS-XBase with the pretrained weights of\nPEGASUSLarge and PEGASUSBase+ respectively.\nOnly two new sets of parameters are introduced:\nglobal token embeddings, and a new LayerNorm\nfor the global input representations in each Trans-\nformer layer. This is ∼1M more parameters for\nPEGASUS-XBase and 2M more for PEGASUS-X.\nWe initialize the global token embeddings by ran-\ndomly sampling tokens from the input embeddings,\nand we initialize the LayerNorm weights with the\nregular input LayerNorm weights.\nThe task- and model-speciﬁc hyperparameters\nfor ﬁne-tuning can be found in Appendix 15. For\nthis section, we report ROUGE-Lsum5 rather than\nROUGE-L for consistency with the metrics re-\nported in other papers and leaderboards.\n4See Appendix C.\n5https://github.com/google-research/\n3951\nPEGASUS-XBase PEGASUS-X\n# Parameters 272M 568M\n# Global Tokens 128 128\nBlock Size 512 512\nBatch Size 512 1024\nAdditional\nPretraining 300K steps 300K steps\nTable 5: Hyperparameters of PEGASUS-X Models\n4.1 Results on Summarization Tasks\nLong summarization tasks In Table 6, we com-\npare the performance of PEGASUS models to those\nof PEGASUS-X on three long-input summarization\ntasks: arXiv, Big Patent and PubMed. In all three\ntasks, we see signiﬁcant improvements in perfor-\nmance of PEGASUS-XBase over PEGASUSBase+,\nand PEGASUS-X over PEGASUS Large. To iso-\nlate the impact of additional long input pretraining\ncompared to only switching the architecture dur-\ning ﬁne-tuning, we also include evaluation on the\nPEGASUS models using the Global-Local archi-\ntecture with no further pretraining, which we list in\nthe table as PEGASUSBase+ + Global-Local.\nWe also compare to reported results of Big\nBird-PEGASUS6 (Zaheer et al., 2020), LED (Belt-\nagy et al., 2020), Top-Down Transformer (Pang\net al., 2022) with both Average-Pool (AvgP) and\nAdaptive-Pool (AdaP) variants, BART-LS (Xiong\net al., 2022a), LongT5-Large and XL, and SLED\n(Ivgi et al., 2022). LED, Top-Down and SLED\nare initialized with BART Large weights with no\nadditional pretraining on long input sequences.\nBART-LS is concurrent work that also incorpo-\nrates staggered block-local attention and addition\nlong-sequence pretraining, in addition to pooling\nlayers and different pretraining data.\nPEGASUS-X outperforms Big Bird-PEGASUS\non all tasks, and Top-Down-AvgP on both com-\npared tasks. Although Top-Down-AdaP outper-\nforms PEGASUS-X, it uses a much more complex\nﬁne-tuning setup, using an importance tagger on\nreference summaries to construct token pooling\nweights, whereas PEGASUS-X only uses standard\nﬁne-tuning. Even so, PEGASUS-X still outper-\nforms Top-Down-AdaP on PubMed. PEGASUS-X\noutperforms BART-LS on PubMed and slightly\nunderperforms on arXiv; as mentioned above,\ngoogle-research/blob/master/rouge/README.\nmd#two-flavors-of-rouge-l\n6Big Bird-PEGASUS only has a context of 3072 tokens,\nlikely due to the larger memory consumption of Big Bird.\nPEGASUS-X and BART-LS share many similari-\nties, and we see the strong performance of BART-\nLS as conﬁrmation of the efﬁcacy of parts of our\nrecipe for longer sequence models. PEGASUS-\nX also outperforms LongT5 on both arXiv and\nPubMed, despite both compared LongT5 models\nhaving more parameters. However, we ﬁnd that\nLongT5 performs much better on BigPatent, which\nis a largely extractive summarization task. We hy-\npothesize that a larger hidden size may improve\nextraction over very long sequences.\nShort summarization tasks We show in Ta-\nble 14 the performance of PEGASUS and\nPEGASUS-X models on shorter summarization\ntasks, where there is a slight regression in perfor-\nmance of both PEGASUS-X models compared to\ntheir PEGASUS equivalents. We hypothesize that\nlong input pretraining might negatively impact the\nperformance on shorter input tasks because of the\ndata ﬁltering for long documents, resulting in a\npotentially less diverse training data distribution.\n4.2 SCROLLS Summarization Tasks\nWe report the performance of the PEGASUS-\nX models on the summarization tasks in the re-\ncently introduced SCROLLS benchmark in Table 7.\nThis includes GovReport (Huang et al., 2021), the\nForeverDreaming subset of SummScreen (Chen\net al., 2022), and QMSum (Zhong et al., 2021).\nPEGASUS-X outperforms all other models\non GovReport, setting the state of the art on\nthe dataset. 7 It also performs comparably to\nboth LongT5Large and Top-Down-AvgP on Summ-\nScreen/FD, although it underperforms LongT5\nmodels and BART-LS on QMSum. Moreover,\nPEGASUS-XBase also performs competitively, out-\nperforming both LongT5 models on GovReport,\nand only a small margin behind PEGASUS-X on\nall three tasks. PEGASUS-XBase also outperforms\nBARTLarge-SLED, a larger model with a similar\n16K input length.\n5 Pertinent Related Work\nMany works such as Zaheer et al. (2020), Beltagy\net al. (2020), Ivgi et al. (2022) have investigated\nextending short input models to longer sequences\nusing efﬁcient attention mechanisms. In closely\ncomparable work, Guo et al. (2021) pretrained a\nT5 model on long sequences from scratch, incor-\n7As of 08/08/2022\n3952\narXiv Big Patent PubMed\nModel #Params R1 / R2 / RLs RG R1 / R2 / RLs RG R1 / R2 / RLs RG\nPEGASUSBase 271M 34.8 / 10.2 / 22.5* 20.0* 43.5 / 20.4 / 31.8* 30.5* 40.0 / 15.2 / 25.2* 24.8*\nPEGASUSBase+ 271M 42.2 / 15.8 / 37.3 29.2 51.2 / 32.6 / 41.0 40.9 44.1 / 18.3 / 40.1 31.9\nPEGASUSBase+ + Global-Local 272M 47.6 / 20.2 / 42.4 34.4 58.1 / 39.5 / 47.2 47.7 47.3 / 21.4 / 43.0 35.2\nPEGASUS-XBase 272M 49.4 / 21.6 / 44.0 36.1 61.3 / 42.6 / 50.1 50.8 49.6 / 23.6 / 45.2 37.5\nPEGASUSLarge 567M 44.7 / 17.2 / 25.7* 27.0* 53.4 / 32.9 / 42.1* 42.0* 45.1 / 19.6 / 27.4* 28.9*\nPEGASUS-X 568M 50.0 / 21.8 / 44.6 36.5 64.8 / 47.5 / 54.3 55.1 51.0/ 24.7/ 46.6 38.9\nBART-LS 460M 50.2 / 22.1 / 45.4 36.9 –.- / –.- / –.- –.- 50.3 / 24.3 / 46.3 38.4\nLongformer Encoder-Decoder 464M 46.6 / 19.6 / 41.8 33.7 –.- / –.- / –.- –.- –.- / –.- / –.- –.-\nTop-Down (AvgP) 464M 48.7 / 20.7 / 43.9 35.4 –.- / –.- / –.- –.- 48.3 / 21.4 / 44.2 35.7\nTop-Down (AdaP) 464M 51.0/ 21.9/ 45.6 37.1 –.- / –.- / –.- –.- 51.1 / 23.3 / 46.5 38.1\nBig Bird-Pegasus 567M 46.6 / 19.0 / 41.8 33.3 60.6 / 42.5 / 50.1 50.5 46.3 / 20.7 / 42.3 34.4\nLongT5Large 770M 48.3 / 21.6 / 44.1 35.8 70.4 / 56.8 / 62.7 63.1 50.0 / 24.7 / 46.5 38.6\nLongT5XL 3B 48.4 / 21.9 / 44.3 36.1 76.9/ 66.1/ 70.8 71.1 50.2 / 24.8 / 46.7 38.7\nTable 6: Comparison on long summarization tasks (Test sets). Results for other models are taken from their\nrespective papers. *: PEGASUS (Zhang et al., 2020) only reports ROUGE-L and not ROUGE-LSum.\nGovReport SummScreen/FD QMSum\nModel #Params R1 / R2 / RL RG R1 / R2 / RL RG R1 / R2 / RL RG\nPEGASUS-XBase 272M 59.3 / 29.3 / 30.9 37.7 35.0 / 8.9 / 20.4 18.5 32.9 / 9.8 / 21.4 19.0\nPEGASUS-X 568M 60.3 / 30.0 / 31.5 38.5 35.7 / 9.1 / 20.6 18.8 33.2 / 9.6 / 21.6 19.0\nBARTLarge-SLED 406M 58.0 / 26.9 / 27.6 35.1 33.8 / 8.0 / 18.5 17.1 32.1 / 10.2 / 21.0 19.0\nBART-LS 460M 59.4 / 29.8 / 30.8 37.9 37.7 / 10.2 / 21.5 20.2 35.1 / 12.0 / 23.3 21.4\nTop-Down-AvgP 464M –.- / –.- / –.- –.- 35.8 / 8.9 /30.6* 21.4* –.- / –.- / –.- –.-\nTop-Down-AdaP 464M –.- / –.- / –.- –.- 36.8 / 9.2 /31.1* 21.9* –.- / –.- / –.- –.-\nLongT5Large 770M 54.2 / 27.8 / 29.8 35.5 35.6 / 9.2 / 21.2 19.1 35.1 / 12.0 / 23.3 21.4\nLongT5XL 3B 54.7 / 28.2 / 30.2 36.0 35.8 / 9.6 / 21.1 19.4 34.9 / 11.8 / 23.5 21.3\nUL2 20B 53.6 / 26.1 / 28.8 34.3 32.9 / 7.8 / 19.4 17.1 31.1 / 8.5 / 20.4 17.5\nTable 7: Comparison on SCROLLS benchmark (Summarization tasks, Test sets). Results for SLED, BART-LS,\nLongT5 and UL2 models are taken from the SCROLLS benchmark leaderboard. *: Top-Down (Pang et al., 2022)\nreports much higher scores for ROUGE-L on SummScreen/FD than any other model, and may have been computed\nwith a variant of ROUGE-L that involves splitting on sentences rather than newlines.\nporating sliding window attention and global rep-\nresentations. However, pretraining only on long\nsequences signiﬁcantly increases the pretraining\ntime, and as we show in Section 3.6, pretraining\nﬁrst on short inputs and then subsequently on long\ninputs is much more cost efﬁcient.\nIn concurrent work released shortly before this\nsubmission deadline, Xiong et al. (2022a) also in-\nvestigated extending short input Transformer mod-\nels for long input tasks. While they focus on BART\nrather than PEGASUS, they similarly ﬁnd that\nglobal tokens, staggered block-local attention, and\nextended pretraining greatly improve performance,\nlending further support to our ﬁndings. Their ﬁ-\nnal model also incorporates pooling layers and is\ntrained on different data.\nA broader treatment of related work can be found\nin Appendix A.\n6 Conclusion\nIn this work, we investigate a range of proposed\nimprovements to Transformer models to effectively\nand economically handle long inputs in summariza-\ntion tasks. Through extensive ablation experiments,\nwe ﬁnd a simple but effective recipe for extend-\ning short-input models to tackle long-input sum-\nmarization. Based on our ﬁndings, we introduce\nPEGASUS-X, an extended version of PEGASUS\nwith a modiﬁed architecture and additional long-\nsequence pretraining. We show that PEGASUS-X\nsets the state of the art on two long input summa-\nrization tasks (GovReport and PubMed) and per-\nforms competitively on many others, even despite\nbeing much smaller than some compared models.\nOur ﬁndings can be extended to models in other do-\nmains beyond summarization, both for pretraining\nlong input models from scratch as well as extending\nalready pretrained short sequence models.\n3953\nLimitations\nChallenges of Evaluating Long-Document\nSummarization Models\nOne limitation of our work is that evaluation\nof long-document summarization models is chal-\nlenging, and while we evaluate on the widely\nused benchmarks for long-document summariza-\ntion models, we highlight here the difﬁculties of\nmeasuring the capabilities of such models. In addi-\ntion to the widely accepted issues with automatic\nevaluation of model-generated summaries with met-\nrics such as ROUGE, long-document summariza-\ntion brings about new challenges. In particular,\nthere are relatively fewer long-document summa-\nrization tasks available to evaluate models on, and\nmany of them (e.g. arXiv, Pub Med, SummScreen)\nare constructed by repurposing existing data and\nproxies for summaries (e.g. abstracts) rather than\nexplicitly written summaries. As such, the avail-\nable datasets for summarization reﬂect the data that\nis easy to repurpose into summarization rather than\npractical downstream summarization settings; in\nother words, the available evaluation datasets may\nnot match the distribution of data or settings where\nsuch models are realistically used.\nOn scoring generations, human evaluation\nshould ideally be conducted to measure the qual-\nity of model-generated summaries. However, the\nmuch longer input texts also means that human\nevaluation of summaries becomes much more ex-\npensive and onerous, as raters would need to read\nthe whole input before judging the quality of the\nsummary.\nMore discussion on the challenges of evaluat-\ning long-document summarization models can be\nfound in Wang et al. (2022).\nFindings May Not Generalize to Other Tasks\nWe have conﬁned our study to summarization tasks,\nas it matches our goal of investigating the ability\nfor models to process large input contexts, with\nless focus on generating long outputs. We acknowl-\nedge that our ablation studies and experiments are\nfocused solely on summarization tasks, and that\nour ﬁndings may not directly apply or extend to\nother long-input language tasks.\nReferences\nJoshua Ainslie, Santiago Ontañón, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. Etc: Encoding long and structured data in\ntransformers. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP 2020).\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nJames Bradbury, Roy Frostig, Peter Hawkins,\nMatthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake\nVanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. 2018. JAX: composable transformations of\nPython+NumPy programs.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin\nGimpel. 2022. SummScreen: A dataset for ab-\nstractive screenplay summarization. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8602–8615, Dublin, Ireland. Association for\nComputational Linguistics.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamás Sar-\nlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Belanger, Lucy Colwell, and\nAdrian Weller. 2021. Rethinking attention with per-\nformers. In International Conference on Learning\nRepresentations, ICLR 2021.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Na-\nzli Goharian. 2018. A discourse-aware attention\nmodel for abstractive summarization of long docu-\nments. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 615–621,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nBehrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur\nBapna, Maxim Krikun, Xavier Garcia, Ciprian\n3954\nChelba, and Colin Cherry. 2021. Scaling laws\nfor neural machine translation. arXiv preprint\narXiv:2109.07740.\nMandy Guo, Joshua Ainslie, David Uthus, Santiago\nOntanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei\nYang. 2021. Longt5: Efﬁcient text-to-text trans-\nformer for long sequences.\nCurtis Hawthorne, Andrew Jaegle, C ˘at˘alina Cangea,\nSebastian Borgeaud, Charlie Nash, Mateusz Mali-\nnowski, Sander Dieleman, Oriol Vinyals, Matthew\nBotvinick, Ian Simon, Hannah Sheahan, Neil Zeghi-\ndour, Jean-Baptiste Alayrac, João Carreira, and\nJesse Engel. 2022. General-purpose, long-context\nautoregressive modeling with perceiver ar.\nJonathan Heek, Anselm Levskaya, Avital Oliver, Mar-\nvin Ritter, Bertrand Rondepierre, Andreas Steiner,\nand Marc van Zee. 2020. Flax: A neural network\nlibrary and ecosystem for JAX.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models. CoRR, abs/2203.15556.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\nJi, and Lu Wang. 2021. Efﬁcient attentions for long\ndocument summarization. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1419–1436, On-\nline. Association for Computational Linguistics.\nGeorge Hudson and Noura Al Moubayed. 2022. Muld:\nThe multitask long document benchmark. In Pro-\nceedings of the Language Resources and Evaluation\nConference, pages 3675–3685, Marseille, France.\nEuropean Language Resources Association.\nMaor Ivgi, Uri Shaham, and Jonathan Berant. 2022. Ef-\nﬁcient long-text understanding with short-text mod-\nels.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. Scaling laws for neural language\nmodels. CoRR, abs/2001.08361.\nWojciech Kry´sci´nski, Nazneen Rajani, Divyansh Agar-\nwal, Caiming Xiong, and Dragomir Radev. 2021.\nBooksum: A collection of datasets for long-form\nnarrative summarization.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nBo Pang, Erik Nijkamp, Wojciech Kry ´sci´nski, Silvio\nSavarese, Yingbo Zhou, and Caiming Xiong. 2022.\nLong document summarization with top-down and\nbottom-up inference.\nOﬁr Press, Noah Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In International Confer-\nence on Learning Representations.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori\nYoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,\nMor Geva, Jonathan Berant, and Omer Levy. 2022.\nScrolls: Standardized comparison over long lan-\nguage sequences.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yun-\nfeng Liu. 2021. Roformer: Enhanced transformer\nwith rotary position embedding.\nYi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won\nChung, William Fedus, Jinfeng Rao, Sharan Narang,\nVinh Q. Tran, Dani Yogatama, and Donald Metzler.\n2022. Scaling laws vs model architectures: How\ndoes inductive bias inﬂuence scaling?\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang\nShen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler. 2021.\nLong range arena : A benchmark for efﬁcient trans-\nformers. In International Conference on Learning\nRepresentations.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient transformers: A survey.\nCoRR, abs/2009.06732.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nAlex Wang, Richard Yuanzhe Pang, Angelica Chen, Ja-\nson Phang, and Samuel R. Bowman. 2022. SQuAL-\nITY: Building a long-document summarization\ndataset the hard way. arXiv preprint 2205.11465.\nWenhan Xiong, Anchit Gupta, Shubham Toshniwal,\nYashar Mehdad, and Wen-tau Yih. 2022a. Adapt-\ning pretrained text-to-text models for long text se-\nquences.\nWenhan Xiong, Barlas Oguz, Anchit Gupta, Xilun\nChen, Diana Liskovich, Omer Levy, Scott Yih, and\nYashar Mehdad. 2022b. Simple local attentions re-\nmain competitive for long-context tasks. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\n3955\nLinguistics: Human Language Technologies, pages\n1975–1986, Seattle, United States. Association for\nComputational Linguistics.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33.\nBiao Zhang, Behrooz Ghorbani, Ankur Bapna, Yong\nCheng, Xavier Garcia, Jonathan Shen, and Orhan\nFirat. 2022. Examining scaling and transfer of lan-\nguage model architectures for machine translation.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and\nPeter Liu. 2020. PEGASUS: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn Proceedings of the 37th International Conference\non Machine Learning, volume 119 of Proceedings\nof Machine Learning Research, pages 11328–11339.\nPMLR.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir\nRadev. 2021. QMSum: A new benchmark for query-\nbased multi-domain meeting summarization. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5905–5921, Online. Association for Computational\nLinguistics.\n3956\nA Full Related Work\nLong Document Summarization Several new\nlong input summarization datasets and benchmarks\nhave been recently introduced, providing better\nmeasures of long input summarization capabil-\nity as well as prompting new interest in this re-\nsearch direction. The BookSum dataset (Kry ´s-\nci´nski et al., 2021) consists of paragraph, chapter,\nand full summaries of books on Project Gutenberg\nbased on web-scraped educational website. (Chen\net al., 2022) consists of television show transcripts\nand episode summaries based on web-scraped fan-\nwritten summaries. The SCROLLS benchmark\n(Shaham et al., 2022) and the MuLD benchmark\n(Hudson and Al Moubayed, 2022) consist of multi-\nple natural language tasks with long inputs, includ-\ning long input summarization. The SQuALITY\ndataset (Wang et al., 2022) consists of question-\nfocused summaries of Project Gutenberg stories,\nwhere annotators write summaries based on dif-\nferent questions that cover different aspects of the\nsame story.\nEfﬁcient Transformers Many efﬁcient Trans-\nformer variants have been introduced in recent\nyears (Tay et al., 2020), and we discuss here the\nworks more relevant to this manuscript. (Beltagy\net al., 2020) use global tokens as well as a sliding\nwindow local attention, implemented using custom\nCUDA kernels. The ETC model (Ainslie et al.,\n2020) uses both global tokens and block-wise slid-\ning window local attention, although the global\nattention is incorporated based on the ﬁrst few to-\nkens of a sequence, rather than separately learned\nglobal tokens. Zaheer et al. (2020) extend ETC\nby adding random attention blocks, but we found\nthat this signiﬁcantly increases code complexity\nand computational cost. Guo et al. (2021) sim-\nilarly extend ETC’s block-wise sliding window\nattention, but computes transient “global token”\nrepresentations by pooling over blocks of tokens.\nPang et al. (2022) propose to augment the Long-\nformer encoder-decoder with additional pooling\nlayers to improve long-sequence summarization\nperformance. Ivgi et al. (2022) propose an alter-\nnative approach to sparse attention via encoding\noverlapping chunks and fusing information across\nchunks int he decoder. We highlight that while the\nﬁnal Global-Local model architecture that we set-\ntle on shares similarity with several other proposed\nefﬁcient Transformer architectures, our key con-\ntribution lies in our extensive ablation study that\nidentiﬁes architectural tweaks that improve and,\njust as importantly, do not improve downstream\nperformance.\nAmong the listed model architectures for long\ninput summarization, LongT5 (Guo et al., 2021) is\nthe most similar to PEGASUS-X, sharing a similar\nencoder-decoder architecture, a similar training ob-\njective in generating masked sentences, and a mix\nof local attention and global information sharing\nfor the encoder. We brieﬂy highlight the key dif-\nferences between the two models. Firstly, LongT5\ntrains from scratch on long sequences, whereas\nwe initialize our model weights with PEGASUS\nweights (which is trained on short sequences) be-\nfore doing additional pretraining on long input se-\nquences. This signiﬁcantly reduces the overall pre-\ntraining cost, as short sequence pretraining and\nbe performed much more economically. LongT5\nalso uses the T5 relative position biases whereas\nPEGASUS-X uses sinusoidal position embeddings–\nas shown in Section B.1, T5 relative position biases\nperform slightly better but are signiﬁcantly slower.\nThe efﬁcient encoder architecture between the two\nmodels is also different: LongT5 uses a transient\nglobal representations based on pooling chunks of\ntokens, whereas PEGASUS-X uses learned global\ntoken embeddings. LongT5 also uses a sliding win-\ndow local attention based on ETC (Ainslie et al.,\n2020), whereas we use a simpler block-local at-\ntention with staggered blocks. Lastly, the largest\nLongT5 model is 3B parameters, more than 5× the\nsize of PEGASUS-X.\nMore broadly, Tay et al. (2021) compare a vari-\nety of efﬁcient Transformer architectures on a set of\ntasks designed to probe long-sequence processing\ncapability, evaluating the different models on both\nperformance as well as computation requirements.\nTay et al. (2022) further evaluate the scaling proper-\nties of novel Transformer architectures, ﬁnding that\ndeviating from full attention tends to hurt down-\nstream performance. Xiong et al. (2022b) showed\nthat simple local attention variants can be highly\ncompetitive with more complex sparse attention\nschemes, consistent with our ﬁndings.\nB Details of Architecture Modiﬁcation\nExperiments\nB.1 Position Encoding Schemes\nNew position encoding schemes encoding schemes\nsuch as RoPE (Su et al., 2021) and ALiBi (Press\n3957\nXSUM CNN/DM arXiv GovReport\nPosition Encoding R1 / R2 / RL RG R1 / R2 / RL RG R1 / R2 / RL RG R1 / R2 / RL RG Step/s\nNone 34.3 / 12.5 / 26.8 22.6 25.6 / 7.8 / 17.7 15.2 36.1 / 9.8 / 22.0 19.8 38.3 / 13.2 / 18.7 21.1 0.96\nSinusoidal 39.8 / 16.9 / 31.8 27.8 40.0/ 18.6/ 28.4 27.6 44.5 / 17.6 / 26.7 27.6 40.0 / 18.8 / 22.3 25.6 0.96\nT5 40.1/ 17.1/ 32.0 28.0 39.8 / 18.8 / 28.6 27.8 44.9/ 17.9/ 26.8 27.8 40.2 / 19.5/ 22.9 26.2 0.53\nRoPE 39.8 / 16.9 / 31.8 27.8 39.2 / 18.7 / 28.5 27.5 43.5 / 17.2 / 26.5 27.1 40.0 / 19.1 / 22.6 25.8 0.85\nAbsolute 39.1 / 16.4 / 31.3 27.2 39.7 / 18.7 / 28.5 27.7 44.3 / 17.5 / 26.5 27.4 38.6 / 17.5 / 21.1 24.2 1.00\nTable 8: Comparison of position encodings schemes for a Transformer encoder-decoder. Training steps per sec-\nond are computed based on arXiv summarization. Absolute position embeddings are replicated to longer input\nsequences, following Beltagy et al. (2020). Training steps per second is computed based on arXiv, and normalized\nto the run with absolute position embeddings.\narXiv GovReport\nPosition Encoding R1 / R2 / RL RG R1 / R2 / RL RG\nFactor=10000 48.1 / 20.4 / 28.6 30.4 60.9 / 29.3 / 30.8 38.0\nFactor=50000 48.1 / 20.4 / 28.6 30.4 61.4 / 29.5 / 30.9 38.3\nTable 9: Comparison of different scaling constants in sinusoidal position encodings.\net al., 2022) have garnered recent attention, show-\ning improved performance on downstream evalua-\ntions. As input sequence lengths have gotten much\nlonger, and in particular longer than the dimensions\nof hidden representations, previous choices of posi-\ntion encoding may no longer be optimal. Moreover,\nrelative position encodings such as RoPE, T5 and\nALiBi may be better suited for adapting models\nto different input lengths between pretraining and\nﬁne-tuning. Hence, this is a good opportunity to\nrevisit the choice of positioning encoding schemes\nin encoder models.\nBecause of the more complex interaction be-\ntween local attention blocks and relative position\nencoding implementations, we conduct a prelimi-\nnary investigation with a full-attention Transformer.\nWe pretrain with an input length of 512, and ﬁne-\ntune with an input length of 2048 for the long\nsequence tasks – this experiment also tests the\npropensity for position encodings to be adapted\nto longer sequences downstream. In addition to\nthe sinusoidal position encoding used in PEGA-\nSUS and Vaswani et al. (2017), we also consider\nthe bucket-based relative position encoding scheme\nof T5, RoPE, absolute position embeddings, and\nno position encoding as a baseline. For absolute\nposition embeddings, we follow the recipe of Belt-\nagy et al. (2020) and duplicate the learned position\nembeddings to handle longer sequences before ﬁne-\ntuning. The chosen position encoding scheme is\napplied to all parts of the model, including both the\nencoder and the decoder. We do not experiment\nwith ALiBi, as we found no natural way to adapt\nALiBi to cross-attention.\nOur results are shown in Table 8. We ﬁnd that al-\nthough T5 performs the best, it is also almost twice\nas slow as the other position encoding schemes,\nwhich is consistent with the ﬁndings of Press et al.\n(2022). Sinusoidal position encodings and RoPE\nperform only slightly worse than T5 with much bet-\nter efﬁciency, making them more desirable choices.\nGiven the much simpler implementation of sinu-\nsoidal position encodings, we opt to stick with them\nfor the remainder of the experiments.\nTakeaways: Sinusoidal position encodings still\nremain a good choice for long input Transformers.\nB.2 Scaling Encoder and Decoder Layers\nScaling laws (Kaplan et al., 2020; Ghorbani et al.,\n2021; Zhang et al., 2022) that describe the em-\npirical relationship between model sizes and per-\nformance have proven surprisingly consistent and\ngotten signiﬁcant attention in recent years. We\npresent in this section a small set of scaling experi-\nments, exploring the distribution of layers between\nencoder and decoder.\nOur results are shown in Table 10. In the top half,\nwe ﬁx the total number of layers to 24, and con-\nsider both encoder-heavy and decoder-heavy distri-\nbutions, for both Local and Global-Local models.\nWe observe that impact of distribution of encoder\nand decoder layers on performance is relatively\nsmall. For Local models, we see a slight boost from\ndecoder-heavy models. For Global-Local models,\nwe observe that a balanced encoder-decoder outper-\nforms encoder- and decoder-heavy models, both of\n3958\nXSUM CNN/DM arXiv GovReport\nArchitecture Enc Dec R1 / R2 / RL RG R1 / R2 / RL RG R1 / R2 / RL RG R1 / R2 / RL RG\nLocal 18 6 37.4 / 15.0 / 29.7 25.5 39.0 / 18.2 / 27.9 27.0 46.0 / 19.4 / 27.6 29.1 58.9 / 27.4 / 29.1 36.1\n12 12 37.5 / 14.9 / 29.7 25.5 38.5 / 18.0 / 27.6 26.7 45.4 / 18.9 / 27.3 28.6 59.2 / 27.6 / 29.3 36.3\n6 18 37.7 / 15.1 / 29.9 25.7 38.5 / 18.1 / 27.7 26.9 46.3 / 19.3 / 27.6 29.1 59.4 / 27.8 / 29.5 36.5\nGlobal-Local 18 6 38.6/ 15.9/ 30.9 26.7 39.2 / 18.5 / 28.2 27.3 47.3 / 20.1 / 28.3 30.0 60.2 / 28.7 / 30.6 37.5\n12 12 38.6 / 15.9 / 30.7 26.6 40.0/ 18.6/ 28.3 27.6 47.5 / 20.1 / 28.3 30.0 61.1/ 29.3/ 30.7 38.1\n6 18 37.7 / 15.1 / 29.9 25.7 38.5 / 18.1 / 27.7 26.9 46.4 / 19.5 / 27.9 29.3 60.3 / 28.6 / 30.0 37.2\nGlobal-Local 18 12 38.5 / 15.7 / 30.6 26.4 38.7 / 18.4 / 28.1 27.1 47.3 / 20.0 / 28.3 29.9 60.2 / 29.2 / 31.0 37.9\n12 18 38.6 / 15.8 / 30.5 26.5 38.6 / 18.3 / 28.0 27.0 47.5/ 20.3/ 28.5 30.2 60.9 / 29.0 / 30.4 37.7\nTable 10: Varying the distribution of encoder/decoder layers)\nwhich perform about comparably.\nWe also consider cases where we further increase\nthe size of either the encoder or decoder to 18 lay-\ners, shown in the second half of Table 10. We\nobserve no improvement in performance over the\n12/12-layer encoder-decoder, and suspect that other\nhyperparameters (e.g. hidden size) might be the\nbottleneck rather than the number of layers.\nWe highlight here that because of the asymmetry\nof the input and output lengths, there are different\ncomputational trade-offs to different balances of\nencoder and decoder layers. Encoder-heavy mod-\nels require more memory because of the long input\nsequences, whereas decoder-heavy models are rel-\native slower at inference because of the autoregres-\nsive nature of decoding. Given the relatively small\ndifference in the margin of performance, memory\nor computational constraints may outweigh the per-\nformance differences in practical scenarios.\nTakeaways: A balanced encoder-decoder per-\nforms best, but the difference in performance may\nbe outweighed by other resource considerations.\nB.3 Partial Cross Attention\nGiven the use of an efﬁcient attention architec-\nture, which has memory consumption scale lin-\nearly rather than quadratically in input sequence\nlength, another major memory bottleneck is the\nencoder-decoder cross-attention. Because each de-\ncoder layer attends separately to the long encoder\nrepresentations, and the attention is dense, this is a\nlarge contiguous chunk of memory that we could\nseek to reduce.\nPerceiver AR (Hawthorne et al., 2022) demon-\nstrated strong performance by using only a single\ncross-attention at the bottom layer of an autoregres-\nsive language model. Based on these results, we in-\nvestigate the impact of only having cross-attention\non a subset of decoder layers. In Table 12, we show\nthe results of pretraining and ﬁne-tuning Global-\nLocal models with cross-attention only on speciﬁc\nlayers on a variety of conﬁgurations. We ﬁnd that\nreducing the number of cross-attention layers leads\nto a drop in performance, but the impact on per-\nformance is smaller than expected. For instance,\nwith only cross-attention on the ﬁrst and sixth layer,\nthe Global-Local model still outperforms a Local\nmodel. The reduction of cross-attention layers also\nleads to a corresponding improvement in training\nstep and reduction in memory consumption.\nGiven the small drop in performance from using\nfewer decoder layers with cross-attention, we con-\nsider the viability of dropping cross-attention layers\nafter pretraining. In other words, we take a Global-\nLocal model pretrained with full cross-attention,\ndrop the cross-attention for a subset of layers, and\nﬁne-tune directly. Our results are shown in Ta-\nble 13. We ﬁnd that dropping the cross-attention\nafter pretraining again only leads to a small (ad-\nditional) dip in performance. This indicates that\ndropping cross-attention may be a viable strategy\nfor further reducing memory requirements for an\nexisting pretrained model with a small performance\ntrade-off, and pretraining a separate model from\nscratch is not necessary.\nTakeaways: Dropping cross-attention for a frac-\ntion of decoder layers can reduce memory con-\nsumption at the cost of slight performance regres-\nsion. Cross-attention can be dropped after pretrain-\ning, with an associated performance trade-off.\nB.4 Comparison on short summarization\ntasks\nC PEGASUS Base+\nIn a similar ﬁnding as Hoffmann et al. (2022), we\nfound that PEGASUS Base beneﬁts from training\non signiﬁcantly more tokens. As such, we trained\na PEGASUSBase for a much larger number of to-\n3959\narXiv GovReport\nPretraining →Fine-tuning Block Size R1 / R2 / RL RG R1 / R2 / RL RG\nTransformer →Local 16 46.4 / 19.6 / 27.9 29.4 59.6 / 28.2 / 29.9 36.9\n64 46.5 / 19.5 / 27.8 29.3 59.5 / 28.0 / 29.6 36.7\n256 46.8 / 19.7 / 28.0 29.6 59.8 / 28.0 / 29.8 36.8\nLocal →Local 16 45.9 / 19.1 / 27.5 28.9 59.0 / 27.5 / 29.3 36.2\n64 46.5 / 19.5 / 27.8 29.3 59.7 / 28.1 / 29.8 36.8\n256 47.1 / 19.9 / 28.1 29.8 59.7 / 28.5 / 30.3 37.2\nTransformer →Global-Local 16 46.0 / 19.2 / 27.5 29.0 60.3 / 28.2 / 29.8 37.0\n64 47.0 / 20.0 / 28.2 29.8 60.8 / 28.7 / 30.1 37.4\n256 47.6 / 20.3 / 28.4 30.2 60.8 / 28.7 / 30.0 37.4\nGlobal-Local →Global-Local 16 47.1 / 20.0 / 28.3 29.9 59.7 / 27.8 / 29.2 36.5\n64 47.7 / 20.3 / 28.5 30.2 61.0 / 29.3 / 30.8 38.0\n256 47.3 / 20.2 / 28.3 30.0 61.6 / 29.4 / 30.7 38.2\nTable 11: Comparison of adapting models architectures between pretraining and ﬁne-tuning.\nXSUM CNN/DM arXiv GovReport\nCross-Attention R1 / R2 / RL RG R1 / R2 / RL RG R1 / R2 / RL RG R1 / R2 / RL RG Step/s Mem\nFull 38.8/16.0/ 31.0 26.8 39.5 / 18.6 / 28.4 27.5 47.7 / 20.4 / 28.6 30.3 61.3/29.4/ 30.8 38.1 1.00 1.00\nCross[0,2,4,6,8,10] 38.3 / 15.6 / 30.5 26.3 39.8/18.8/ 28.5 27.7 48.1 /20.4/ 28.6 30.4 61.0 / 29.0 / 30.7 37.9 1.10 0.90\nCross[0,3,6,9,11] 38.0 / 15.3 / 30.2 26.0 38.8 / 18.4 / 28.1 27.2 46.9 / 19.9 / 28.2 29.7 60.1 / 28.6 / 30.2 37.3 1.15 0.88\nCross[0,4,8,11] 37.8 / 15.3 / 30.1 25.9 38.5 / 18.1 / 27.9 26.9 47.6 / 20.2 / 28.4 30.1 60.9 / 28.9 / 30.3 37.6 1.15 0.86\nCross[0,6,11] 37.4 / 14.8 / 29.7 25.4 38.8 / 18.1 / 27.9 27.0 46.9 / 19.7 / 28.1 29.6 60.3 / 28.5 / 30.2 37.3 1.18 0.87\nCross[0,6] 37.5 / 14.9 / 29.7 25.5 38.3 / 18.0 / 27.8 26.8 47.1 / 19.8 / 28.1 29.7 60.4 / 28.1 / 29.7 36.9 1.21 0.85\nTable 12: Comparison of models with cross-attention only in a subset of the 12 decoder layers. Training steps per\nsecond and memory are computed based on arXiv, and normalized to the Cross[0,6] run.\nkens (the same as PEGASUSLarge), which achieves\nmuch better performance than the previously re-\nleased PEGASUSBase model.\nD Encoder Architecture\nHyperparameters\nFor experiments in Section 3.1, BigBird, Local and\nGlobal-Local all use a block size of 64. BigBird\nand Global-Local also use 32 global tokens. Per-\nformer uses 256 random features.\nE Fine-tuning Hyperparameters\nFor arXiv, we ﬁne-tune with an input length of up\nto 16384 tokens and 256 output tokens, while for\nGovReport we use an input length of 10240 in-\nput tokens and 1024 output tokens given the longer\nsummaries for the task. For XSUM and CNN/Daily\nMail, with use an input length of 512, and output\nlengths of 64 and 128 respectively, following PE-\nGASUS hyperparameters. The full set of hyper-\nparameters for ﬁne-tuning models are shown in\nTable 15.\nF Engineering Details\nThe original PEGASUS model was trained using a\ncodebase based on TensorFlow. The experiments\nin this paper were run using a new codebase written\nwith JAX (Bradbury et al., 2018) and Flax (Heek\net al., 2020). PEGASUS-X Baseand PEGASUS-\nXwere trained by converting the weights from the\nTensorFlow checkpoint to a Flax checkpoint for-\nmat, and then continuing with long input training.\n3960\narXiv GovReport\nCross-Attention Model R1 / R2 / RL RG R1 / R2 / RL RG\nPretrained Full 47.7 / 20.4 / 28.6 30.3 61.3 / 29.4 / 30.8 38.1\nCross[0,2,4,6,8,10] 48.1 / 20.4 / 28.6 30.4 61.0 / 29.0 / 30.7 37.9\nCross[0,6] 47.1 / 19.8 / 28.1 29.7 60.4 / 28.1 / 29.7 36.9\nConverted Cross[0,2,4,6,8,10] 46.4 / 19.7 / 28.1 29.5 60.2 / 28.8 / 30.3 37.4\nCross[0,6] 46.2 / 19.7 / 28.1 29.5 60.2 / 28.1 / 29.8 36.9\nTable 13: Comparison of models pretrained with cross-attention for a subset of layers, and adapting a pretrained\nmodel by dropping cross-attention layers only during ﬁne-tuning\nCNN/DailyMail XSum\nModel R1 / R2 / RLs RG R1 / R2 / RLs RG\nPEGASUSBase 41.8 / 18.8 / 38.9 31.3 39.8 / 16.6 / 31.7 27.6\nPEGASUSBase+ 42.5 / 20.1 / 39.6 32.4 43.8 / 21.2 / 36.0 32.2\nPEGASUS-XBase 42.5 / 20.1 / 39.6 32.4 42.9 / 20.1 / 35.0 31.2\nPEGASUSLarge 44.2 / 21.5 / 41.1 33.9 47.2 / 24.6 / 39.2 35.7\nPEGASUS-X 43.4 / 21.2 / 40.6 33.5 45.8 / 22.8 / 37.6 34.0\nTable 14: Comparison on short summarization tasks (Test sets)\nDataset Batch\nSize\nLearning\nRate\nNum\nSteps\nMax Input\nTokens\nMax Output\nTokens\nBeam\nSize\nBeam\nAlpha\nPEGASUS-XBase\nXSum 64 8e-4 97.5K 1024 128 4 0.8\nCNN/DailyMail 64 8e-4 410K 1024 128 4 0.8\narXiv 64 8e-4 92.5K 16384 256 1 1\nBig Patent 64 8e-4 272.5K 16384 256 1 1\nPubMed 64 8e-4 85K 8096 256 1 1\nGovReport 64 8e-4 40K 12288 1024 2 1\nSummScreen 64 8e-4 90K 16384 256 1 1\nQMSum 64 8e-4 7.5K 16384 256 1 1\nPEGASUS-X\nXSum 64 8e-4 5k 1024 128 4 0.8\nCNN/DailyMail 64 8e-4 7.5k 1024 128 4 0.8\narXiv 64 8e-4 85k 16384 256 1 1\nBig Patent 64 8e-4 390k 12192 256 1 1\nPubMed 64 8e-4 47.5k 12192 256 1 1\nGovReport 64 8e-4 75K 12288 1024 1 1\nSummScreen 64 8e-4 40K 12192 256 1 1\nQMSum 64 8e-4 35K 12192 256 1 1\nTable 15: Hyperparameters for ﬁne-tuning models\n3961",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9425030946731567
    },
    {
      "name": "Transformer",
      "score": 0.8133832216262817
    },
    {
      "name": "Computer science",
      "score": 0.7958190441131592
    },
    {
      "name": "Encoder",
      "score": 0.6727292537689209
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5184456706047058
    },
    {
      "name": "Language model",
      "score": 0.4336514472961426
    },
    {
      "name": "Decoding methods",
      "score": 0.42624229192733765
    },
    {
      "name": "Natural language processing",
      "score": 0.3450024724006653
    },
    {
      "name": "Machine learning",
      "score": 0.32217079401016235
    },
    {
      "name": "Voltage",
      "score": 0.2097170650959015
    },
    {
      "name": "Algorithm",
      "score": 0.1373235583305359
    },
    {
      "name": "Engineering",
      "score": 0.1242213249206543
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210113520",
      "name": "Brain (Germany)",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}