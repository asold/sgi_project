{
  "title": "Encoding Syntactic Information into Transformers for Aspect-Based Sentiment Triplet Extraction",
  "url": "https://openalex.org/W4383503791",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1999606766",
      "name": "Li Yuan",
      "affiliations": [
        "Yunnan University"
      ]
    },
    {
      "id": "https://openalex.org/A2105468542",
      "name": "Jin Wang",
      "affiliations": [
        "Yunnan University"
      ]
    },
    {
      "id": "https://openalex.org/A4373347396",
      "name": "Liang-Chih Yu",
      "affiliations": [
        "Yuan Ze University"
      ]
    },
    {
      "id": "https://openalex.org/A2115527613",
      "name": "XueJie Zhang",
      "affiliations": [
        "Yunnan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2253519362",
    "https://openalex.org/W3003963580",
    "https://openalex.org/W3098363382",
    "https://openalex.org/W3098042509",
    "https://openalex.org/W2753840835",
    "https://openalex.org/W2780698117",
    "https://openalex.org/W2766718178",
    "https://openalex.org/W2751820697",
    "https://openalex.org/W2998446468",
    "https://openalex.org/W2962808042",
    "https://openalex.org/W2903110172",
    "https://openalex.org/W2965510113",
    "https://openalex.org/W2949660355",
    "https://openalex.org/W2970895602",
    "https://openalex.org/W3077780682",
    "https://openalex.org/W2604205681",
    "https://openalex.org/W2963063806",
    "https://openalex.org/W3101602207",
    "https://openalex.org/W3138389337",
    "https://openalex.org/W3176920001",
    "https://openalex.org/W4289525790",
    "https://openalex.org/W4291910423",
    "https://openalex.org/W3105293920",
    "https://openalex.org/W3105083537",
    "https://openalex.org/W3174860526",
    "https://openalex.org/W4294791692",
    "https://openalex.org/W2741252866",
    "https://openalex.org/W2963264961",
    "https://openalex.org/W3196474414",
    "https://openalex.org/W2950601686",
    "https://openalex.org/W2997194369",
    "https://openalex.org/W2946015932",
    "https://openalex.org/W1980867644",
    "https://openalex.org/W2165855670",
    "https://openalex.org/W6676723433",
    "https://openalex.org/W6727807531",
    "https://openalex.org/W2562607067",
    "https://openalex.org/W2757541972",
    "https://openalex.org/W3093816678",
    "https://openalex.org/W3099409556",
    "https://openalex.org/W3167287584",
    "https://openalex.org/W3167830325",
    "https://openalex.org/W3210828003",
    "https://openalex.org/W3081228062",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2969743835",
    "https://openalex.org/W2251648804",
    "https://openalex.org/W2251294039",
    "https://openalex.org/W2465978385",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6729856301",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W4367353919",
    "https://openalex.org/W4294982692",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Aspect-based sentiment triplet extraction</i> (ASTE) aims to extract triplets consisting of aspect terms and their associated opinion terms and sentiment polarities from sentences, a relatively new and challenging subtask of aspect-based sentiment analysis (ABSA). Previous studies have used either pipeline models or unified tagging schema models. These models ignore the syntactic relationships between the aspect and its corresponding opinion words, which leads them to mistakenly focus on syntactically unrelated words. One feasible option is to use a graph convolution network (GCN) to exploit syntactic information by propagating the representation from the opinion words to the aspect. However, such a method considers all syntactic dependencies to be of the same type and thus may still incorrectly associate unrelated words to the target aspect through the iterations of graph convolutional propagation. Herein, a syntax-aware transformer (SA-Transformer) is proposed to extend the GCN strategy by fully exploiting the dependency types of edges to block inappropriate propagation. The proposed approach can obtain different representations and weights even for edges with the same dependency type according to their adjacent dependency type of edges. Instead of using a GCN layer, we used an <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">L</i> -layer SA transformer to encode syntactic information in the word-pair representation to improve performance. Experimental results on four benchmark datasets show that the proposed model outperforms various previous models for ASTE.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nEncoding Syntactic Information into\nTransformers for Aspect-Based Sentiment Triplet\nExtraction\nLi Yuan, Jin Wang, Member, IEEE,Liang-Chih Yu, Member, IEEE,Xuejie Zhang, Member, IEEE\nAbstract— Aspect-based sentiment triplet extraction(ASTE) aims to extract triplets consisting of aspect terms and their associated\nopinion terms and sentiment polarities from sentences, a relatively new and challenging subtask of aspect-based sentiment analysis\n(ABSA). Previous studies have used either pipeline models or uniﬁed tagging schema models. These models ignore the syntactic\nrelationships between the aspect and its corresponding opinion words, which leads them to mistakenly focus on syntactically unrelated\nwords. One feasible option is to use a graph convolution network (GCN) to exploit syntactic information by propagating the\nrepresentation from the opinion words to the aspect. However, such a method considers all syntactic dependencies to be of the same\ntype and thus may still incorrectly associate unrelated words to the target aspect through the iterations of graph convolutional\npropagation. Herein, a syntax-aware transformer (SA-Transformer) is proposed to extend the GCN strategy by fully exploiting the\ndependency types of edges to block inappropriate propagation. The proposed approach can obtain different representations and\nweights even for edges with the same dependency type according to their adjacent dependency type of edges. Instead of using a GCN\nlayer, we used an L-layer SA transformer to encode syntactic information in the word-pair representation to improve performance.\nExperimental results on four benchmark datasets show that the proposed model outperforms various previous models for ASTE.\nIndex Terms—Aspect Sentiment Triplet Extraction, Sentiment Analysis, Syntactic Information, Transformers\n!\n1 I NTRODUCTION\nA\nSPECT -based sentiment analysis (ABSA) [1] aims to\nrecognize the sentiment polarity and opinion of tar-\ngeted aspects in a given sentence [1], [2], [3], which is a\nuseful technique for various sentiment applications [4], [5],\n[6], [7], [8]. ABSA is composed of several related subtasks,\nsuch as aspect term extraction (ATE), opinion term extrac-\ntion (OTE), and aspect sentiment classiﬁcation (ASC). Here,\nATE indicates what aspect is being discussed, ASC shows\nhow the sentiment polarity impacts the aspect, and OTE\nexplains why the polarity is associated [9].\nPrevious works have attempted to either solve the above\nsubtasks individually or solve two of the subtasks jointly,\nsuch as ATE and ASC [10], [11], [12], [13], [14], [15] or ATE\nand OTE [16], [17]. To further integrate the tree subtasks,\nPeng et al. [9] pioneered a uniﬁed task, namely, aspect-based\nsentiment triplet extraction (ASTE), which aims to provide\na complete analysis of a user-generated text by producing\nall triplets ( aspect term , opinion term , and corresponding\nsentiment polarity) from sentences. Fig. 1 shows an example\nreview. The ASTE task requires a model to generate three\ntriplets: (staff, very courteous, Pos), (staff, great, Pos), and (food,\nterrible, Neg), where staff and food are aspect terms; very\ncourteous, great, and terrible are corresponding opinion terms;\nCorresponding authors: Jin Wang and Liang-Chih Yu.\n• L. Yuan, J. Wang and X. Zhang are with the School of Information\nScience and Engineering, Yunnan University, Kunming 650000, China\nE-mail: yuanli@mail.ynu.edu.cn; wangjin@ynu.edu.cn;\nxjzhang@ynu.edu.cn\n• L.-C. Yu is with the Department of Information Management, Yuan Ze\nUniversity, Taoyuan 32003, Taiwan\nE-mail: lcyu@saturn.yzu.edu.tw\nand Pos and Neg denote their sentiment polarity.\nPrevious studies have typically accomplished ASTE\ntasks by using a two-stage pipeline approach with sequence\nlabeling models [9]. This approach ﬁrst identiﬁes the aspect\nterms with their sentiment, as well as the opinion terms.\nThe extracted aspect terms are then matched with each\nopinion term to determine their consistency. Unfortunately,\nthe pipeline approach ignores the relationships between\nelements and is prone to error propagation. Alternatively,\nanother viable option is to apply a multitask strategy to\nintegrate both stages into a joint framework [18], [19], [20],\n[21], [22]. The main limitation of the joint approach is that\nit cannot efﬁciently handle scenarios in which a review\ncontains multiple relational triplets that overlap with each\nother; e.g., in the previous example sentence, both opinion\nterms very courteous and great should be associated with the\nsame aspect term, staff.\nSeveral recent works have studied the overlapping\ntriplet problem by applying a grid tagging scheme (GTS)\n[23], [24]. Therefore, the ASTE task is converted to predict\nthe relation tags of word pairs, as shown in the lower part of\nFig. 1. The tags A and O denote that the word pair represents\nthe same aspect term and opinion term, the tag N denotes\nno relation between the word pair, and Pos, Neg and Neu\nare the sentiment labels. For example, the polarities between\nword pairs ( staff, courteous) and ( staff, great) are both posi-\ntive. However, the equivalence classiﬁcation between word\npairs may lead to an inappropriate association between the\naspect terms and opinion terms. For example, great could be\nsimultaneously associated with both aspects terms staff and\nfood.\nTo address the limitations of the above models, graph-\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nThe very courteousand great food was terrible\ncc\nstaff was but\nThe staff is courteous and great food was terrible\nN N N N N N N\nbut\nN\nN N\nN\nN N N N\nN N N N N N N\nN N N N N N\nN N N\nN\nN\nN\nN\nN\nN\nN\nN\nN\nN\nN\nN\nN\nN\nvery\nN\nN N N N N N\nis\nand\ngreat\nfood\nwas\nbut\nterrible\nvery\nThe\nstaff\ncourteous\nN N\nA\nO O\nO\nPos Pos\nN\nPos\nA Neg\nO\nO\nPos Pos Neg\ndet nsubj\nconjccacompamod conj nsubj acomp\nFig. 1. : Dependency parsing and grid tagging of a given sentence. N, A, O, Pos, Neg, and Neu respectively denote the word-pair tags of none,\naspect opinion, positive, negative, and neutral.\nbased methods have been proposed to introduce syntactic\ndependencies to model the relationship between words [25],\n[26]. By parsing the text into a dependency tree, a special\ntype of graph is constructed based on the adjacency matrix.\nGraph convolution networks (GCNs) can then propagate\nthe representations through the edges from opinion words\nto the corresponding aspects. However, these models con-\nsider all syntactic dependencies to be of the same type and\nassign an equal weight to each edge. The inappropriate\nassociation of less important words may still occur through\nmultiple iterations of graph convolution propagation. In the\nexample shown in Fig. 1, the representation of courteous can\nbe correctly propagated to staff through the path of edges\ncourteous-acomp-was-nsubj-staff, but it can also be incorrectly\npropagated to food through courteous-acomp-was-conj-was-\nnsubj-food.\nDependency types are useful features to model word\nrelationships from the syntactic aspect, and different de-\npendency types should be assigned different weights. For\ninstance, the dependency types nusbj and acomp indicate\na subject-object relation, and increasing their weights can\nhelp accomplish correct propagation (e.g., from courteous\nto staff and terrible to food). On the other hand, even the\nsame dependency type may necessitate different weights.\nFor instance, the example sentence in Fig. 1 contains two\nedges with conj. The conj between was and was should be\nassigned a lower weight to block inappropriate propagation\n(e.g., from courteous to food and terrible to staff ), but the conj\nbetween courteous and great should be assigned a higher\nweight to help propagation from great to staff.\nBased on this notion, this study proposes a syntax-aware\ntransformer (SA-Transformer) to incorporate the knowledge\nof dependency types into graph neural networks for triplet\nextraction. The proposed method extends graph neural\nnetworks in three aspects. First, it can distinguish not only\nbetween edges with different dependency types but also\nthose with the same dependency type to achieve more\naccurate graph propagation. This is accomplished by de-\nveloping an adjacent edge attention (AEA) mechanism to\nlearn the edge representation for each edge according to the\ndependency types of its adjacent edges. That is, the edges\nthat have adjacent edges with different dependency types\nmay have different representations and weights. Second,\nthe edge representations are encoded into contextual word\nrepresentations to learn the syntactic and positional relation-\nships between the words to enhance word pair representa-\ntions. Third, given that a multiword aspect/opinion term\n(e.g., very courteous) is divided by multiple consecutive word\npairs for prediction, this study devises an adjacency infer-\nence strategy to improve triplet extraction for multiword\naspect/opinion terms. This strategy can iteratively predict\nthe tag of each word pair according to the predicted results\nof its adjacent word pairs instead of predicting each word\npair independently. The proposed SA-Transformer model is\nevaluated with respect to four benchmark datasets. Experi-\nmental results show that the proposed method outperforms\nvarious previous models for ASTE.\nThe main contributions of this study are summarized as\nfollows.\n• We propose the SA-Transformer, which incorporates\nthe knowledge of dependency types to extend graph\nneural networks for the ASTE task.\n• We design the AEA mechanism that can learn differ-\nent representations and weights for different edges,\neven for those with the same dependency type, thus\nachieving more accurate graph propagation.\n• Experiments conducted on four benchmark datasets\nshow that the proposed method outperforms existing\nmethods for ASTE.\nThe rest of this paper is organized as follows. Sec-\ntion 2 brieﬂy reviews existing methods for ASTE. Sec-\ntion 3 presents a detailed description of the proposed SA-\nTransformer model. Section 4 summarizes the implementa-\ntion details and experimental results. Conclusions are ﬁnally\ndrawn in Section 5.\n2 R ELATED WORKS\nPrevious ABSA works can be broadly divided into three\nindependent extraction subtasks (ATE, OTE and ASC), the\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nTABLE 1\nDifferent subtasks and corresponding methods for ABSA.\nTasks Methods Example Outputs\nIndependent Extraction\nATE Attention [27], [28], [29]; Seq2Seq [30] Staff; Food\nOTE LSTM [31]; Attention [32] Courteous; Great; Terrible\nASC\nMachine learning [33], [34], [35];\nLSTM [36], [37], [38], [39];\nGraph-based [40], [41], [42], [43], [44]\nPositive; Negative\nJoint Pair Extraction ATE+ASC Uniﬁed Tagging [11], [12], [13], [14], [15] (Staff, Positive); (Food, Negative)\nATE+OTE Attention [16]; Graph-based [17] (Staff, Courteous); (Staff, Great); (Food, Terrible)\nTriplet Extraction ASTE\nPipeline [9];\nMultitask [18], [19], [20], [21], [22];\nWord-Pair [23], [24], [25], [26]\n(Staff, Courteous, Positive);\n(Staff, Great, Positive);\n(Food, Terrible, Negative)\njoint pair extraction subtask, and ASTE subtask. This section\nbrieﬂy reviews different methods for these subtasks, which\nare summarized in Table 1.\n2.1 Independent Extraction\nATE [27], [28], [29], [30] and OTE [31], [32] tasks are usually\naddressed by using a named entity recognition (NER) model\nto extract the target terms. In contrast to ATE and OTE\ntasks, ASC tasks aim to predict the sentiment polarity for a\ngiven aspect [33]. Most prior methods for ASC tasks employ\nstatistical machine learning models [33], [34], [35]. However,\nobtaining features through these models is time- and labor-\nintensive. Later, after the rapid development of NLP tasks,\nvarious methods based on neural networks were proposed\nfor ASC [36], [37], [38], [39].\nRecent methods for ASC tasks mostly use graph-based\nmodels [40], [41], [42], [43], [44], which encode syntactic\ninformation to block the inappropriate propagation of un-\nrelated contextual information to the aspect. For example,\nTian et al. [41] proposed a type-aware graph convolutional\nnetwork to capture the syntactic relation between context\nand target aspect. Xiao et al. [44] presented a syntactic\nedge-enhanced network with interactive attention, which\nleverages the edge information of a dependency parsing\ntree to interactively learn the representations of aspect terms\nwith context.\n2.2 Joint Pair Extraction\nRecently, many researchers have focused on designing ef-\nfective models to jointly extract aspect terms and sentiment\npolarity [10], [11], [12], [13], [14], [15], [16], [17]. For example,\nLi et al. [11] designed a multigranularity alignment network\nto decrease the false alignment of features in ASC and ATE\ntasks. Li et al. [12] designed a two-layer stacked LSTM\nmodel in which the lower-layer network guides the upper-\nlayer network to improve performance on ATE and ASC\ntasks. Hu et al. [13] proposed a span-based model that\noutperforms joint and collapsed models.\nTo efﬁciently align the features of aspect granularity and\ndomains, Wang et al. [16] and Dai et al. [17] attempted\nto coextract both aspect and opinion terms. Wang et al.\n[16] proposed a coupled multilayer attention network that\nuses a couple of attentions in each layer to extract aspect\nand opinion terms. The multilayer structure can capture\nboth direct and indirect relations between words to achieve\nmore precise extraction. Dai et al. [17] developed a weakly\nsupervised method to extract aspect and opinion terms. It\nﬁrst mined the extraction rules based on the dependencies\nbetween words and then used the mined rules to expand\nthe training data for neural model training.\n2.3 Triplet Extraction\nAspect sentiment triplet extraction aims to jointly extract\naspect terms, opinion terms, and their corresponding sen-\ntiment polarity, presenting a greater challenge than the\nindependent subtasks. Previous works can be separated into\nthe pipeline, multitask, and word-pair methods\nPeng et al. [9] proposed a pipeline model for ASTE. It\nﬁrst extracted aspect terms, opinion terms, and sentiment\npolarities using the mutual inﬂuence between aspect and\nopinion terms and then employed a classiﬁer to pair the\nextracted terms to obtain the ﬁnal triplets. Peng et al. [9]\nalso extended several joint pair extraction models [12], [16],\n[17] as a pipeline model.\nSeveral studies have proposed multitask frameworks to\njointly extract triplets [18], [19], [20], [21], [22]. Zhang et al.\n[18] used a sequence tagging strategy to extract aspect and\nopinion terms and predicted sentiment polarities using a\ntable ﬁlling method. Chen et al. [19] converted the ASTE\ntask into a machine reading comprehension (MRC) task and\nproposed a bidirectional MRC framework to gather infor-\nmation useful for triplet extraction from both the aspect-to-\nopinion and opinion-to-aspect directions. Xu et al. [20] de-\nsigned a span-level model that can capture the span-to-span\ninteractions instead of word-to-word interactions between\nthe aspects and opinions for ASTE. Dai et al. [21] presented\na bidirectional sentiment-dependence detector with double\nembeddings to obtain better sentence representations and\ngather information from both the aspect-to-opinion and\nopinion-to-aspect directions. Zhang et al. [22] proposed a\ndual decoder with a span copy mechanism that can extract\nmultiple and overlapped triplets based on multitype infor-\nmation.\nWu et al. [23] designed a grid tagging schema to formal-\nize the ASTE task into a word-pair task where classiﬁcations\nare applied between word pairs. Moreover, Xu et al. [24]\nused a model with a position-aware tagging scheme to\njointly extract triplets. However, these methods may asso-\nciate unrelated opinion terms with the target aspect, even if\nthey are syntactically irrelevant. To address this limitation,\nChen et al. [25] proposed the S3E2 model based on a GCN\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\n2x\n3x\n1nx −\nnx\n……\n……\n…\n…\n……\n…\n…\n…\n…\n…\n…\nSyntax-Aware Transformer \nAdjacency Inference \nSoftmax\n…\n…\n…\n1h\n3h\n2h\n…\n…\nMatMul\nScale\nMask\nSoftmax\nMatMul (c) SA-Transformer \nAdjacent Edge \nAttention (AEA)\nContext Encoder\nvery courteousand great food was terrible\nnsubjdet acompamod conjcc\nconj\nnsubj acomp\nstaff was but\ncc\nThe\nSyntactic Relative Distance Adjacency Matrix (A) Relationship Matrix (R) Dependency Type\n(b) Dependency structure representation\nQ\nK\nV\nE\n\n1,1o\n1,2o\n1, -1no\n1,\n1\nn\nn\no −\n−\n1,nno −\n3,no\n,nno\n1,3o\n1,no\n2,2o\n2,3o\n2, -1no\n2,no\n3,3o\n3, -1no\n1nh −\nnh\nScaled Dot-Product\nAttention\n\nSyntax-Enhanced Word Representation \nWord Pair  Representation \nContextual Word  Representation \n(a) SA-Transformer architecture \n: self\n: det\n: cc\n7r\n: nsubj\n: acomp\n: conj\n: amod\n6r\n5r\n4r\n3r\n2r\n1r\n12\n2 1 3\n3 1 4 5\n16\n4 6 1\n13\n5 3 1 4\n41\n0 0 0 0 0 0\n0 0 0 0 0\n0 0 0 0\n0 0 0 0 0 0\n0 0 0 0 0\n0 0 0 0 0 0\n0 0 0 0\n0 0 0 0 0 0\nrr\nr r r\nr r r r\nrr\nr r r\nrr\nr r r r\nrr\n1 1 0 0 0 0 0 0\n1 1 1 0 0 0 0 0\n0 1 1 0 1 0 1 0\n0 0 0 1 1 0 0 0\n0 0 1 1 1 0 0 0\n0 0 0 0 0 1 1 0\n0 0 1 0 0 1 1 1\n0 0 0 0 0 0 1 1\n1x\nFig. 2. The overall framework of the proposed SA-Transformer.\nto learn dependency information. However, this model only\nconsiders the semantic information of syntactic adjacent\ncontexts and ignores edge attributes. Zhao et al. [26] also\ndeveloped a pointer-speciﬁc tagging method to integrate\ndependency information into GCN for ASTE. A triplet\nalignment scheme was then proposed to extract triplets\nby aligning the corresponding positions of the aspect and\nopinion terms.\n3 S YNTAX -AWARE TRAMSFORER NETWORK\nGiven an input sentence X = {x1,x2,··· ,xn} with n\ntokens, the goal is to extract a set of opinion triplets\n{(a,o,s )ω}Ω\nω=1, where (a,o,s )ω is the ω-th opinion triplet,\nwhich consists of an aspect term of length aω = {xla, an\nopinion term oω = {xlo,··· ,xro}of length ro −lo + 1, and\nthe corresponding sentiment polarity s ∈{Pos,Neg,Neu}.\nTable 2 lists the notations used throughout the paper.\nFig. 2(a) shows the overall architecture of the proposed\nmethod, which is composed of four parts: a context encoder,\nSA-Transformer, syntactic relative distance, and adjacent\ninference strategy. The context encoder is used to produce\nthe contextual word representations for an input sentence.\nSA-Transformer then uses dependency parsing to obtain\nthe dependency structure of the sentence and represents it\nusing an adjacency matrix and relationship matrix to record\nwhether an edge exists between two words and the kind of\ndependency type, as shown in Fig. 2(b). Both matrices are\nused by the AEA to learn the edge representations ( E) for\neach edge based on the dependency types of its adjacent\nedges, as shown in Fig. 2(c). The edge representations ( E)\nTABLE 2\nNotations used in the paper and their descriptions.\nNotations Descriptions\nW The trainable weight matrix.\nb The trainable biases.\nxi The input of the ith word.\nwi The embedding of the ith word.\nhi The hidden state of the ith word.\nA The adjacency matrix of the input sentence.\nR The relationship matrix of the input sentence.\nri,j The dependency type between the words xi and xj.\nzi,j The initial edge embedding of words i to j.\nei,j The ﬁnal edge representation of the words xi and xj.\nDg\ni The ith word representation in the gth attention head\n˜ei\ni,j\nThe edge representation of ei,j learned from its\nadjacent of the ei.\nUi,m\ni,j The edge representation of the mth attention head in AEA.\nS(l)\ni The ith word representation in the lth SA-Transformer layer.\nfd(i,j) The representation of the syntactic relative distance\nbetween words i and j.\noi,j The ﬁnal representation of the word pair xi to xj.\npT\ni,j\nThe ﬁnal tag probability distribution of the word pair\nxi to xj.\nare then added into the key ( K) and value ( V) of a scaled\ndot-product attention to be integrated into the contextual\nword representations. The syntax-enhanced representations\nof any two words can then be used to constitute a word-\npair representation. In addition, the distance between the\ntwo words is calculated by their syntactic relative distance\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nScaled Dot-Product\nAttention\nvery courteous and great food was terrible\nnsubjdet\nacompamod conjcc nsubj acomp\nstaff was but\ncc\nThe\n3V\n3,10Q\nself acompnsubj cc conj\n3K\n+ conj self acomp\nScaled Dot-Product\nAttention\n10V\n10\n3,10e\n10K\n+\n3\n3,10e\n[:]\n. .\n\n+\n3,10e\n3,10z\nconj nsubj\n1 −conj\n3 3,2 3,3 3,5 3,8 3,10{ ,  ,  ,  ,  }e e e e e e=\n3 1 4 7 5[0, , ,0, ,0,0, ,0, ,0]r r r r r\n5 3 1 4[0,0, ,0,0,0,0,0, , , ]r r r r\n[0,0, ,0,0,0 ]11 ,0,0, 1,1,\n[0, , ,0, ,0,0, ,0, ,0]1 1 1 1 1\n3R =\n3A =\n10A =\n10R =\n3 3,2 3,3 3,5 3,8 3,10{ ,  ,  ,  ,  }z z z z z z=\n3,10 conje\n10 3,10 9,10 10,10 11,10{ ,  ,  ,  }z z z z z=\n10 3,10 9,10 10,10 11,10{ ,  ,  ,  }e e e e e=\n3,10z\n3,10Q\nFig. 3. Illustrative example of AEA to learn edge representations with de-\npendency types. The dependency type “self” denotes the edge between\na word itself.\nin a dependency tree and encoded into the word-pair repre-\nsentation as an extra feature. Finally, the adjacent inference\nstrategy is used to iteratively predict the tag of each word\npair from those of its adjacent word pairs. The details of\neach component are described as follows.\n3.1 Context Encoder\nTo obtain the contextual word representations for each\nsentence, 300-dimensional GloVe [45] vectors are used as\nthe initial word embeddings {w1,w2,··· ,wN}, where wi\ndenotes the word vector of word xi. A bidirectional LSTM\n(BiLSTM) model [46] is then used as a context encoder to\nproduce the hidden representations of the word vectors,\ndeﬁned as\n(\n⇀\nhi,\n⇀\nci) = LSTM(wi,\n⇀\nhi−1,\n⇀\nci−1)\n(\n↼\nhi,\n↼\nci) = LSTM(wi,\n↼\nhi+1,\n↼\nci+1) (1)\nwhere\n⇀\nhi ∈ Rdh/2 and\n↼\nhi ∈ Rdh/2 respectively denote\nthe forward and backward hidden representations of wi,\n⇀\nci and\n↼\nci respectively denote the forward and backward\nLSTM unit states, and dh denotes the dimensionality of the\nhidden representations. The forward and backward hidden\nrepresentations are then concatenated to comprise the ﬁnal\nhidden representations, deﬁned as\nhi = [\n⇀\nhi :\n↼\nhi] (2)\nwhere hi ∈Rdh denotes the ﬁnal hidden representations of\nwi, and [:] denotes a concatenation operation.\n3.2 SA-Transformer\nOnce we obtain the contextual hidden representations of\neach word, SA-Transformer encodes syntactic dependency\ninformation into them in three steps: representation of the\ndependency structure, learning of edge representations with\ndependency types using AEA, and injection of edge rep-\nresentations into contextual representations. The details of\neach step are described as follows.\nDependency Structure Representation. A given sentence\nis ﬁrst parsed as a dependency tree. Each dependency is\nrepresented as a tuple (xi,xj,ri,j), where ri,j denotes the\ndependency type between the words xi and xj. The de-\npendency structure can then be represented as an adjacency\nmatrix A and relationship matrix R, where A = {ai,j ∈\n{0,1}} ∈Rn×n records whether an edge exists between\ntwo words, and R= {ri,j}∈ Rn×n records the dependency\ntype of each edge. Both A and R are symmetric matrices.\nAdjacent Edge Attention (AEA). Both adjacency matrix\nA = {ai,j}∈ Rn×n and relationship matrix R = {ri,j}∈\nRn×n are taken as input to learn the edge representations\nE = {ei,j} ∈Rn×n×d, where ei,j ∈ Rd denotes the\nrepresentation of the edge between words xi and xj, and\nd is the dimensionality of the edge representations. To\naccomplish this goal, an embedding layer is ﬁrst applied\nto map R = {ri,j} ∈Rn×n to obtain the initial edge\nembeddings Z = {zi,j} ∈Rn×n×dz , where zi,j ∈ Rdz\ndenotes the initial edge embedding of ei,j ∈ Rd, and dz\nis the dimensionality of the initial edge representations.\nEach edge representation ei,j is determined based on the\ndependency types of the edges adjacent to xi and xj. In the\nexample shown in Fig. 3, to learn the edge representation\ne3,10 = conj, the AEA ﬁrst looks up the matrices A= {ai,j}\nand R= {ri,j}to identify the edge representations adjacent\nto x3 = was, i.e., e3 = {e3,2,e3,3,e3,5,e3,8,e3,10} with\ndependency types {nsubj, self, acomp, cc, conj }, and those\nadjacent to x10 = was, i.e., e10 = {e3,10,e9,10,e10,10,e11,10}\nwith dependency types {conj, nsubj, self, acomp}. AEA then\ntakes the initial edge embeddings z3,10, z3 and z10 as inputs,\nuses scaled dot-product attention to learn the hidden edge\nrepresentation ˜e3\n3,10 based on e3 and ˜e10\n3,10 based on e10, and\nﬁnally uses a gate function to combine ˜e3\n3,10 and ˜e10\n3,10 as the\nﬁnal representation of e3,10. By considering the dependency\ntypes of the adjacent edges, even two edges with the same\ndependency type can have different representations and\nweights.\nThe formal description of AEA is presented as follows.\nLet zi,j and zi be the initial edge embeddings of ei,j and ei,\nrespectively (i.e., the adjacent edge representations of xi);\nthus, the AEA learns the hidden representation ˜ei\ni,j as\n˜ei\ni,j = AEA(zi,j,zi) = [Ui,1\ni,j,Ui,2\ni,j,··· ,Ui,M\ni,j ]Wz (3)\nwhere ˜ei\ni,j ∈Rdz denotes the hidden representation of ei,j\nlearned from its adjacent edge representations ei, Wz ∈\nRdz×dz is a trainable weight matrix, and Ui,m\ni,j ∈ R1×de\nin [Ui,1\ni,j,Ui,2\ni,j,··· ,Ui,M\ni,j ] denotes the edge representation\nlearned by the m-th attention head of scaled dot-product\nattention, deﬁned as\nUm\ni,j = softmax(Ai ·\n˜Qm\ni,j( ˜Km\ni )\nT\n√de\n)˜Vm\ni (4)\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\n˜Qm\ni,j = W˜Qzi,j\n˜Km\ni = W˜Kzi (5)\n˜Vm\ni = W˜Vzi\nwhere ˜Qm\ni,j ∈R1×de denotes a query regarding the current\nedge representation zi,j, ˜Km\ni ∈Rn×de and ˜Vm\ni ∈Rn×de\nrespectively denote the key and value both regarding the\nadjacent edge representations zi of zi,j, W˜Q ∈ Rde×dz ,\nW˜K ∈ Rde×dz , and W˜V ∈ Rde×dz are trainable weight\nmatrices, de = dz/M is the dimensionality of the edge\nrepresentations in each head, Ai ∈Rn×1 denotes a mask\nvector used to help ˜Qm\ni,j query the key ˜Km\ni to identify the\nedges connected to xi in the value ˜Vm\ni , and softmax( ·)\ndenotes the attention weights for these adjacent edges of\nxi, which can be obtained in the training process according\nto their contribution to learning the current edge represen-\ntation. The attention weight is then used to aggregate the\nadjacent edge representations of xi in ˜Vm\ni to generate the\nedge representation of the m-th attention head Ui,m\ni,j . By\nconcatenating the edge representations of all attention heads\nusing Eq. (3), the hidden representation ˜ei\ni,j can be obtained.\nSimilarly, the hidden representation ˜ej\ni,j can be learned\nfrom ej (i.e., the adjacent edge representations of xj) using\nEqs. (3)-(5) with zi,j and zj as inputs. Once ˜ei\ni,j and ˜ej\ni,j are\nobtained, the AEA uses a gate function to combine the two\nhidden edge representations as the ﬁnal representation of\nei,j. That is,\nei,j = α˜ei\ni,j + (1 −α)˜ej\ni,j (6)\nα= σ(Wr([˜ei\ni,j : ˜ej\ni,j]) + br) (7)\nwhere α is a combination coefﬁcient, σ is the sigmoid\nactivation function, [:] is a concatenation operation, and\nWr ∈R1×2d and br ∈R1 are the trainable weight and bias,\nrespectively.\nSA-Transformer. Once the edge representations E =\n{ei,j} are learned according to the dependency types,\nSA-Transformer adds them into contextual word repre-\nsentations. SA-Transformer is composed of L similar lay-\ners, i.e., S = [ S\n(1)\n,S\n(2)\n,··· ,S\n(L)\n]. Each layer S(l) =\n[S\n(l)\n1 ,S\n(l)\n2 ,··· ,S\n(l)\nn ] represents the contextual representa-\ntions of the words in the sentence, which are computed by\ncombining the hidden representations of the current layer\n˜S(l) and the output of the previous layer S(l−1) using layer\nnormalization. That is,\nS(l) = LayerNorm(˜S(l) + S(l−1)) (8)\nNote that S0 = his the hidden word representation output\nby the context encoder. The hidden representations of each\nlayer ˜S(l) are generated by injecting the edge representa-\ntions E = {ei,j}into the output of the previous layerS(l−1),\ndeﬁned as\n˜S(l) = SA −Transformer (S(l−1),E)\n= [D1,··· ,DG]Ws\n(9)\nwhere Dg = [Dg\n1,Dg\n2,··· ,Dg\nn] ∈Rn×ds denotes the word\nrepresentations of all words learned by the g-th attention\nhead, and Ws ∈R(G·ds)×dh denotes the output linear pro-\njection. Each word representation Dg\ni in Dg is injected with\nthe edge representations by a scaled dot-product attention,\ndeﬁned as\nDg\ni = softmax(Ai ·Qg\ni(Kg\ni)T\n√ds\n)Vg\ni (10)\nQg\ni = WQS(l−1)\ni (11)\nKg\ni = WKS(l−1) + βWe,kei (12)\nVg\ni = WVS(l−1) + βWe,vei (13)\nwhere Qg\ni ∈R1×ds denotes a query regarding S(l−1)\ni , which\nrepresents the current word representation of xi in the (l-1)-\nth layer; Kg\ni ∈Rn×ds and Vg\ni ∈Rn×ds denote the key and\nvalue, respectively, regardingS(l−1) and ei, which represent\nthe word representations of all words in the ( l-1)-th layer\nand adjacent edge representations of xi, respectively; β\nis a balance coefﬁcient; WQ ∈ Rds×dh, WK ∈ Rds×dh,\nWV ∈ Rds×dh, We,k ∈ Rds×d, and We,v ∈ Rds×d are\ntrainable weight matrices; ds = dh/Gis the dimensionality\nof the word representations in each head; and Ai ∈Rn×1\ndenotes a mask vector used to help Qg\ni query Kg\ni identify\nboth the words and edges connected to xi in Vg\ni . For\neach word connected to xi, the contextual representation\nin S(l−1) is enhanced by combining its corresponding edge\nrepresentation in ei using β. These syntax-enhanced word\nrepresentations are then aggregated using the attention\nweight softmax( ·) to generate the word representation of\nxi in the g-th attention head Dg\ni.\n3.3 Syntactic Relative Distance\nOnce the dependency types are incorporated into the con-\ntextual word representations, the syntactic relative distance\nbetween words [47] is further introduced as an extra feature\nto enhance word pair representations. The syntactic relative\ndistance between two words, denoted as dist(xi,xj), is\ncalculated by the number of hops in the path from word\nxi to xj in a dependency tree. As the example shows\nin Fig. 2, there are 4 hops between great to food, i.e.,\ndist(great,food ) = 4.\nThe representation of the syntactic relative distance be-\ntween words, denoted as fd(i,j) ∈Rdd, is generated using\nan embedding layer with dist(xi,xj) as input, where dd is\nthe dimensionality of the syntactic relative distance repre-\nsentation. The representation of a word pair (xi,xj) is gen-\nerated based on the representations of the two words output\nby SA-Transformer, i.e.,S\n(L)\ni and S\n(L)\nj . The syntactic relative\ndistance representation is then concatenated with the word\npair representation to generate the ﬁnal representation of the\nword pair, denoted as S\n(L)\ni and S\n(L)\nj . The syntactic relative\ndistance representation is then concatenated with the word\npair representation to generate the ﬁnal representation of\nthe word pair, denoted as\noi,j = [S(L)\ni : S(L)\nj : fd(i,j)] (14)\nwhere oi,j ∈ R2dh+dd denotes the ﬁnal representation of\nthe word pair (xi,xj), and [:] denotes a concatenation\noperation.\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nThe\nsalad\nis\nwell\nand\ndone\nfresh\nvegetable A A\nA\nO\nO\nOO\nPos Pos\nPos N\n,1ijp\n1, 1ijp\n1,ijp\n,ijp\nFig. 4. Illustrative example of the adjacency inference strategy.\n3.4 Adjacency Inference\nThe last step is to predict the relation tag of each word pair\nin the sentence as one of the six classes: aspect term (A),\nopinion term (O), positive (Pos), negative (Neg), neutral\n(Neu), and no relation (N). Generally, each word pair is\npredicted independently without considering other word\npairs. In fact, other word pairs, such as adjacent word pairs,\nalso contribute to tag prediction, especially for multiword\naspect/opinion terms. Considering the example sentence in\nFig. 4, both the aspect term vegetable salad and opinion term\nwell done consist of two words. Each element in the matrix\ndenotes a word pair representation, and the red rectangle\ncontains the word pair representations for the two-word as-\npect and opinion terms, i.e., (vegetable, well), (vegetable, done),\n(salad, well), and ( salad, done). Any of the four word pairs\ncan be predicted using the information provided by the\nother three. For instance, suppose that the model correctly\npredicts the ﬁrst three word pairs as ( vegetable, well, Pos),\n(vegetable, done, Pos), and ( salad, well, Pos) but incorrectly\npredicts the last one as no relation (salad, done, N). The three\ncorrectly predicted adjacent word pairs provide useful in-\nformation that done is highly likely to be an opinion term of\nsalad with a positive sentiment. The model can thus correct\nthe prediction of ( salad, done, N) as ( salad, done, Pos) in the\nnext prediction iteration.\nBased on this notion, we devise an adjacency inference\nstrategy that can predict the tag of each word pair by\nleveraging the predicted results of its adjacent word pairs to\neffectively extract the triplets for multiword aspect/opinion\nterms. Given a word pair (xi,xj), the adjacency inference\ncalculates its tag probability distribution of the six classes\n{A, O, Pos, Neg, Neu, N}using an iterative process, deﬁned\nas\npt\ni,j = γtct\ni,j + (1 −γt)(˜ct\ni,j)\nγt=σ(Wp[ct\ni,j : ˜ct\ni,j]) (15)\nwhere pt\ni,j denotes the ﬁnal tag probability distribution of\n(xi,xj) in the t-th iteration, which is calculated by combin-\ning its current tag probability distribution ct\ni,j and that of\nits adjacent word pairs ˜ct\ni,j using a balance coefﬁcient γt, σ\ndenotes a sigmod function, Wp ∈R1×2dy denotes a trainable\nTABLE 3\nStatistics of datasets (#S, #T, #Pos, #Neu, #Neg, Mean, and Max\nrespectively denote the numbers of sentences, triplets, positive triplets,\nneutral triplets, negative triplets, mean length and max length.)\nDatasets #S #T #Pos #Neu #Neg Mean Max\nRes14\nTrain 1259 2356 1693 172 491 17 79\nDev 315 580 427 46 107 17 66\nTest 493 1008 784 68 156 16 70\nLap14\nTrain 899 1452 808 111 533 19 78\nDev 225 383 199 48 136 19 83\nTest 332 547 364 67 336 16 71\nRes15\nTrain 603 1038 799 29 210 15 68\nDev 151 239 181 9 49 15 39\nTest 325 493 324 25 144 16 63\nRes16\nTrain 863 1421 1036 55 330 15 68\nDev 216 348 263 8 77 15 56\nTest 328 525 416 30 79 15 78\nweight matrix, and [:] denotes a concatenation operation.\nThe adjacent tag probability distribution ˜ct\ni,j is calculated as\n˜ct\ni,j = Wo[ct−1\ni−1,j : ct−1\ni−1,j : ct−1\ni−1,j−1 ] (16)\nwhere ct−1\ni−1,j ,ct−1\ni−1,j ,ct−1\ni−1,j−1 denotes the three adjacent tag\nprobability distributions of (xi,xj) in the ( t-1)-th iteration,\nWo ∈Rdy×3dy is a trainable weight matrix and dy = 6 is the\nnumber of tags. The current tag probability distribution ct\ni,j\nis calculated as\nct\ni,j = softmax(Wc˜ot\ni,j + bc)\n˜ot\ni,j = W˜o[˜ot−1\ni,j : pt−1\ni,j ] + b˜o (17)\nwhere ˜ot\ni,j denotes the hidden representation of (xi,xj)\nwhich is initialized by its word pair representation oi,j, i.e.,\n˜o0\ni,j = oi,j, Wc ∈Rdy×dh, and W˜o ∈R(2dh+dd)×(2dh+dd+dy)\nare trainable weight matrices, and bc ∈ Rdy and b˜o ∈\nR2dh+dd are trainable biases. After T iterations, the ﬁnal\ntag probability distribution of all word pairs is denoted as\npT = [pT\n1,1,pT\n1,2,··· ,pT\nn,1,··· ,pT\nn,n].\n3.5 Training\nThe training objective is to minimize the cross-entropy error\nof the ground-truth distribution Yi,j ∈Y and the predicted\ntag distribution pT\ni,j of all word pairs:\nL(θ) =\nΦ∑\nφ=1\nn∑\ni=1\nn∑\nj=1\nY(φ)\ni,j log(p(φ),T\ni,j |θ) (18)\nwhere Φ and θ respectively denote the number of training\nsamples and all trainable parameters.\n4 EXPERIMENTS\n4.1 Datasets and Evaluation Metrics\nTo evaluate the proposed SA-Transformer, four ASTE bench-\nmark datasets were used, including Rest14, Lap14, Rest15,\nand Rest16, which mainly contain consumer reviews of\nlaptop computers and restaurants. These datasets have\nbeen used for SemEval-2014 [48], SemEval-2015 [49] and\nSemEval-2016 [50]. The statistics of the datasets are pre-\nsented in Table 3.\nThe precision (P), recall (R), and micro F1-score (F1) are\nused as evaluation metrics for triplet extraction. Compared\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nTABLE 4\nHyperparameter settings.\nParameter Name Value\nMaximum sequence Length 100\nBatch Size 16\nInitial Learning Rate 1e-3\nDimension of hidden state dh 200\nDimension of dd 100\nDimension of syntactic features dz 200\nThe keep dropout rate 0.2\nwith precision and recall, F1 is a more appropriate metric\nbecause it considers both precision and recall. A triplet\nis regarded as correctly predicted only if the predicted\naspect term, opinion term, and sentiment polarity match the\nground-truth aspect term, opinion term, and corresponding\npolarity, respectively.\n4.2 Baselines\nThe baseline models used for comparison include the\npipeline, multitask, and word-pair methods. The implemen-\ntation details of each method are described as follows.\nPipeline Methods\n• CLMA+ is the extended version of CLMA [16],\nwhich proposes a coupled multilayer attention net-\nwork that can capture both direct and indirect rela-\ntions between words to coextract aspect and opin-\nion terms. Peng et al. [9] modiﬁed this method\nas CLMA+ by using CLMA in the ﬁrst stage, fol-\nlowed by pairing the extracted aspect and opinion\nterms and identifying sentiment polarities to gener-\nate triplets.\n• RINATE+ is an extended version of RINATE [17]\nthat uses a weakly supervised method to extract\naspect and opinion terms. It uses a set of extraction\nrules mined based on the dependencies between\nwords to expand the training data for neural model\ntraining. Peng et al. [9] modiﬁed this method as\nRINATE+ using the same method as CLMA+.\n• Li-uniﬁed-R+ is the extended version of Li-uniﬁed\n[12], a uniﬁed method that implements a two-layer\nstacked LSTM model to extract the aspect terms and\ntheir sentiment polarities. Peng et al. [9] modiﬁed\nthis method as Li-uniﬁed-R+ by additionally extract-\ning the opinion terms in the ﬁrst stage and pairing\nthe extracted terms to generate triplets in the second\nstage.\n• TSF [9] is a two-stage pipeline model for ASTE. In\nthe ﬁrst stage, it extracts aspect terms, opinion terms\nand sentiment polarities using the mutual inﬂuence\nbetween aspect and opinion terms. A classiﬁer is then\nused to pair the extracted terms to generate triplets\nin the second stage.\nMultitask Methods\n• OTE-MTL [18] uses a multitask learning framework\nto jointly extract aspect terms, opinion terms and\nsentiment polarities. It ﬁrst uses a sequence tagging\nstrategy to extract aspect and opinion terms, then\npredicts the sentiment polarities using a table ﬁlling\nmethod, and ﬁnally applies a decoding process to\ngenerate triplets based on heuristic rules.\n• BMRC [19] proposes a bidirectional machine reading\ncomprehension framework with multiturn queries\nthat are designed to gather information useful for\nextracting the aspect terms, opinion terms and sen-\ntiment polarities. The bidirectional structure can fur-\nther ensure that information can be gathered from\nboth the aspect-to-opinion and opinion-to-aspect di-\nrections.\n• Span-ASTE [20] proposes a span-level model that\ncan capture the span-to-span interactions instead of\nword-to-word interactions between the aspects and\nopinions for ASTE. It ﬁrst enumerates all possible\naspect and opinion spans, then uses a dual-channel\nspan pruning strategy to ﬁlter out the invalid spans,\nand ﬁnally determines the sentiment relations be-\ntween each valid aspect span and opinion span.\n• DE-OTE-BISDD [21] presents a method based on\ndouble embeddings and bidirectional sentiment-\ndependence detection. The double embeddings fuse\ncharacter- and word-level embeddings to obtain sen-\ntence representations. Multitask learning is then ap-\nplied to extract aspect and opinion terms, using\nthe bidirectional sentiment-dependence detector to\ndetermine the sentiment polarities by leveraging in-\nformation gathering from both aspect-to-opinion and\nopinion-to-aspect directions.\n• CopyMTL [22] presents a method to extract multiple\nand overlapped triplets using a span copy mecha-\nnism and a dual decoder. The span copy mechanism\ncan capture the multitoken aspect and opinion words\nthrough multihead attention. The dual decoder is\nused to generate aspect and opinion words sepa-\nrately based on multitype information.\nWord-Pair Methods\n• GTS [23] pioneered the use of a grid tagging scheme\nfor ASTE. It ﬁrst enumerates all possible word pairs\nin a sentence and represents them as a grid. A clas-\nsiﬁer is then used to classify the relation tags of the\nword pairs to generate the triplets.\n• JET [24] converts the ASET task into a structured\nprediction problem with a position-aware tagging\nscheme to jointly extract triplets. It develops a joint\nextraction model based on conditional random ﬁeld\n(CRF) and semi-Markov CRF, which can effectively\ncapture the interactions among aspect terms, opinion\nterms and sentiment polarities based on factorized\nfeatures.\n• S3E2 [25] proposes a graph neural network to\nexploit the semantic and syntactic information for\nASTE. It ﬁrst uses BiLSTM to learn the contextual se-\nmantics of sentences and then encodes the syntactic\ndependencies between words into graph representa-\ntions to jointly extract the triplets.\n• MAS [26] integrates syntactic dependencies into\ngraph neural networks for ASTE. It proposes a\npointer-speciﬁc tagging method to identify the rela-\ntionships between the aspect and opinion terms. A\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nTABLE 5\nExperimental results for triplet extraction. Each model was run ﬁve times to report the average result. The best scores are in bold and the second\nbest are underlined.\nModel Rest14 Lap14 Rest15 Rest16\nP R F1 P R F1 P R F1 P R F1\nPipline Models\nRINANTE+ 31.07 37.63 34.03 23.10 17.60 20.00 29.40 26.90 28.00 27.10 20.50 23.30\nCMLA+ 40.11 46.63 43.12 31.40 34.60 32.90 34.40 37.60 35.90 43.60 39.80 41.60\nLi-uniﬁed-R+ 41.44 68.79 51.68 42.25 42.78 42.47 43.34 50.73 46.69 38.19 53.47 44.51\nTSF 44.18 62.99 51.89 40.40 47.24 43.50 49.97 54.68 46.79 46.76 62.97 53.62\nMultitask Methods\nOTE-MTL 64.54 55.57 59.67 54.18 45.20 48.97 58.16 54.02 55.83 48.17 42.43 45.05\nBMRC - - 63.32 - - 48.15 - - 53.77 - - 63.16\nSpan-ASTE 72.52 62.43 67.08 59.85 45.67 51.80 64.29 52.12 57.56 67.25 61.75 64.37\nDE-OTE-BISDD 68.57 59.17 63.53 56.17 46.20 50.70 61.54 48.43 54.21 65.20 61.34 63.21\nCopyMTL 64.25 63.85 64.05 48.55 47.72 48.13 54.81 55.49 55.14 64.33 65.61 64.96\nWord-Pair Methods\nGTS 67.28 61.91 64.48 59.42 45.13 51.30 63.26 50.71 56.29 66.07 65.05 65.56\nJET 61.50 55.13 58.14 53.03 33.89 41.35 64.37 44.33 52.50 70.94 57.00 63.21\nMAS 70.70 64.20 67.30 60.50 47.10 53.00 64.70 53.70 58.70 67.40 63.30 65.30\nS3E2 69.08 64.55 66.74 59.43 46.23 52.01 61.06 56.44 58.66 71.08 63.13 66.87\nSA-Transformer 70.76 65.85 68.22 61.28 48.98 54.44 62.82 58.31 60.48 72.01 62.87 67.13\nTABLE 6\nAblation study results of the proposed method. Each model was run ﬁve times to report its average result.\nModel Rest14 Lap14 Rest15 Rest16 Training\nTime(s)\nTest\nTime(s)P R F1 P R F1 P R F1 P R F1\nProposed Model 70.76 65.85 68.22 61.28 48.98 54.44 62.82 58.31 60.48 72.01 62.87 67.13 4.28 0.85\nGTS 67.28 61.91 64.48 59.42 45.13 51.30 63.26 50.71 56.29 66.07 65.05 65.56 2.58 0.51\nProposed Model w/o SA-Trans 68.01 62.32 65.03 59.32 45.77 51.67 63.01 50.59 56.12 66.33 64.74 65.53 2.79 0.54\nProposed Model w/o SA 68.56 62.78 65.54 59.18 46.14 51.85 62.35 53.13 57.37 67.81 64.21 65.96 3.62 0.71\nProposed Model w/o AEA 69.72 64.03 66.75 60.12 46.97 52.74 61.79 56.13 58.82 70.52 62.78 66.42 3.82 0.78\nProposed Model w/o SRD 70.12 64.58 67.23 60.77 47.69 53.44 62.05 57.96 59.94 71.01 62.32 66.38 4.12 0.83\nProposed Model w/o AF 70.25 65.12 67.59 60.29 48.19 53.57 60.79 58.21 59.47 71.29 62.32 66.48 3.74 0.75\ntriplet alignment scheme is then designed to extract\ntriplets by aligning the corresponding positions of\nthe aspect and opinion terms.\n4.3 Implementation Details\nThe 300-dimensional GloVe [45] vectors were used as the\ninitial word embedding, and the uniform distribution of U\n(-0.25, 0.25) was initialized to words that do not appear\nin the GloVe vectors. The dimension dh of the hidden\nstate was set to 200. The spaCy 1 with the en core web trf\nversion was used to parse each given sentence into a de-\npendency tree and then build both a relationship matrix\nand an adjacency matrix from the dependency tree. Adam\n[51] was used as the optimizer with a maximum learning\nrate of 1e-3 and a decay factor of 0.5. The dimensions of\nthe syntactic relative distance embedding dd and syntactic\ndependency features dz in AEA were 100 and 200, respec-\ntively, which were initialized by the uniform distribution\nof U(-0.5, 0.5). The grid search strategy was implemented\nto select the optimal values for the model hyperparameters.\nWe ran each model ﬁve times and report the average results.\nTable 4 summarizes the hyperparameter settings of the\nproposed method. The code of this paper is available at:\nhttps://github.com/YuanLi95/SA-Transformer-for-ASTE.\n4.4 Comparative Results\nTable 5 summarizes the comparative results of the proposed\nmodel and previous methods in terms of P , R, and F1. For\n1. https://spacy.io/models\nF1, both the multitask (OTE-MTL, BMRC, Span-ASTE, DE-\nOTE-BISDD, and CopyMTL) and word-pair models (GTS,\nJET, S3E2, and MAS) notably outperformed the pipeline\nmodels (RINANTE+, CMLA+, Li-uniﬁed-R+, and TSF) for\nall datasets since the joint prediction of subtasks can signiﬁ-\ncantly address the error propagation in the pipeline models.\nIn addition, the word-pair models outperformed the mul-\ntitask models for most datasets, indicating that word-pair\nclassiﬁcation can effectively extract the relationships for the\nnested labels.\nThe proposed SA-Transformer outperformed the previ-\nous methods with respect to F1 for all datasets. There are\nthree possible reasons to explain this. First, SA-Transformer\nincorporates the knowledge of dependency types into con-\ntextual word representations and thus can effectively reduce\nthe number of syntactically irrelevant word pairs. Second,\nAEA enables the model to learn an appropriate representa-\ntion for the dependency type of each edge, and the syntactic\nrelative distance further learns the syntactic and positional\ninformation between words. Third, the adjacent inference\ncan iteratively reﬁne the predicted tag distribution of each\nword pair according to those of its adjacent word pairs and\nthus can more effectively extract the triplets for multiword\naspect/opinion terms.\n4.5 Ablation Studies\nAblation studies were conducted to investigate the effec-\ntiveness of each component in the proposed model: SA-\nTransformer, adjacent edge attention (AEA), syntactic rel-\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\n(a) Layer number L of SA-Transformer.\n (b) Number of inferences T.\n (c) Balance coefﬁcient β.\nFig. 5. The effect of different parameters on different datasets.\nTABLE 7\nEffects of using different parsing toolkits for triplet extraction.\nParsing Toolkits English PTB F1\nRest14 Lap14 Rest15 Rest16\nRandom - 57.92 46.06 49.17 55.45\nDeep biafﬁne 95.74 67.75 54.01 60.85 66.57\nEn core web sm 92.01 66.48 53.49 59.36 66.26\nEn core web trf 95.1 68.22 54.44 60.48 67.13\native distance (SRD), and adjacent inference (AF). Table\n6 shows the ablation results with the GTS model as the\nbaseline. The various ablation models produced different\ndegrees of performance decline, indicating that each com-\nponent makes its own unique contribution to the proposed\nmodel. By removing the entire SA-Transformer (w/o SA-\nTrans), i.e., removing both the syntactic dependency module\nand transformer architecture, the proposed method was\ndegraded to become similar to the GTS model, which thus\ncaused the largest performance decline. Instead of removing\nthe entire SA-Transformer, we replaced the SA-Transformer\nwith the vanilla transformer [52] to retain the transformer\narchitecture while removing the syntactic dependency mod-\nule (w/o SA). This also resulted in a sharp performance\ndecline, indicating that encoding the dependency type in-\nformation into the weight and distribution can improve\nthe model’s ability to learn the relationships between word\npairs.\nIn addition, the removal of AEA (w/o AEA) also re-\nsulted in a decline in performance because AEA can learn\nappropriate edge representations to achieve more accu-\nrate graph propagation. Although the model can work\nproperly without SRD and AF, the performance still de-\ncreased because SRD can further capture syntactic and\npositional information, and AF can better handle multiword\naspect/opinion terms.\nTo further investigate the computational cost of each\ncomponent, the last two columns in Table 6 show the\naverage training and test times per epoch across all datasets\nfor each component. For w/o SA-Trans, the computational\ncost was reduced by 46% (1.49 seconds) because of removal\nof the entire SA-Transformer, which thus required a lower\ncomputational cost similar to that of GTS. For w/o SA,\nthe computational cost was reduced by 15% (0.66 second)\nbecause of removal of the syntactic dependency module,\nnamely, both the adjacency matrix and relationship matrix\nused for dependency structure representation and their\nrelated operations. For w/o AEA, the computational cost\nwas reduced by 11% (0.46 second) which is lower than\nthat of w/o SA because only the adjacency matrix and its\nrelated operations were removed. Once the adjacency matrix\nis removed, the weight and representation of each edge\ncan only be learned using the relationship matrix according\nto its contribution to the prediction and cannot be learned\nfrom its adjacent edges. For w/o SRD, it produced the least\nreduction in computational cost of 4% (0.16 second) among\nall components, indicating that calculating the syntactic\nrelative distance to enhance the word pair representation\nis efﬁcient. For w/o AF, it reduced computational cost by\n13% (0.54 second) because it removed the inference strategy\nthat can predict from the adjacent word pairs.\n4.6 Effects of Dependency Parsing\nTo investigate the effects of using different depen-\ndency parsing toolkits for triplet extraction, we se-\nlected three dependency parsers including Deep biafﬁne\n[53], En core web sm and En core web trf. Deep bi-\nafﬁne uses biafﬁne attention to predict the dependencies\nand their types between words. En core web sm and\nEn core web trf represent different versions of the Spacy\ntoolkit, which respectively use the token2vector and a trans-\nformer such as RoBERTa [54] as the context encoder. A\nrandom parser was also implemented to randomly generate\na dependency tree for each sentence. SinceEn core web trf\nwas used in the previous experiments, this experiment\nreplaced it with the other three parsers to rerun the exper-\niment on triplet extraction. Table 7 shows the comparative\nresults. The parsing performance on the English Penn Tree-\nbank (PTB) is also provided for reference. The results show\nthat the three parsers Deep biafﬁne , En core web sm,\nand En core web trf achieved comparable results, and\nall signiﬁcantly outperformed Random. This indicates that\nrandomly generated dependencies and their types contain\nmany errors that degrade the extraction performance, while\nthe parsed results of the other three parsers can still main-\ntain the extraction performance at a certain level.\n4.7 Effects of Parameters\nSince we used L layers in SA-Transformer, we investigate\nthe effect of the number of layers on the performance of the\nproposed model, as presented in Fig. 5(a). As indicated, the\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\n(a) Rest14\n (b) Lap14\n(c) Rest15\n (d) Rest16\nFig. 6. Performance change of removing different dependency types.\nbest performance was achieved at L=2 on Rest14, Lap14,\nand Rest15, and L=3 on Rest16.\nFurthermore, we investigate the effect of the number of\ninferences T over the range of 0 to 3 for all datasets. As\npresented in Fig. 5(b), the best performance was achieved\nat T=2 on Rest14, Lap14, and Rest15 and T=3 on Rest16.\nT=0 means that AF is not used and the proposed model\nwill degenerate to SA-Transformer w/o AF, thus performing\nworst for all datasets.\nIn addition, Fig. 5(c) presents the inﬂuence of the balance\ncoefﬁcient β in Eqs.(12) and (13). β = 0 means that none of\nthe syntactic dependency information is incorporated into\nthe contextual word representations, thus performing worst\nfor all datasets. The best performance was achieved at β =\n0.5 on Lap14 and Rest16 and β = 1 on Rest14 and Res15.\n4.8 Effects of Dependency Types\nDifferent dependency types may yield different contribu-\ntions to prediction performance. To investigate their effects,\nwe removed one dependency type at a time to examine the\nperformance change. Fig. 6 shows the change in F1-scores\nafter removing the top 12 most frequently occurring depen-\ndency types in the datasets. The results show that most de-\npendency types (e.g., nsubj, acomp, conj etc.) yield a positive\ncontribution because removing them led to a certain degree\nof performance decline. Only selected dependency types\n(e.g., cc) caused a negative contribution to performance.\nFor example, the dependency types nsubj and acomp are\nhighly useful features because they can capture the subject-\nobject relation between the aspect and opinion words (e.g.,\n(staff, courteous) and (food, terrible) in Fig. 1). Conversely, the\ndependency type cc typically captures redundant relations\n(e.g., (was, but) and (courteous, and) in Fig. 1).\n4.9 Case Study\nTo further explain the effectiveness of the proposed SA-\nTransformer, three test examples were selected from Rest14\nand Rest15 for the case study. Table 8 shows the golden\ntriplets, the predicted triplets of GTS, S3E2 and our model,\nand the dependency structure of the three test examples.\nIn the ﬁrst example, GTS correctly extracts the aspect term\nfood with the opinion term cold and the corresponding sen-\ntiment polarity. However, the word pair ( food, soggy) is not\nconsidered a triplet because the distance between soggy and\nfood is too far. It is difﬁcult for GTS to effectively learn the\npotential relationship between them. S3E2 and our model\ncorrectly predict all the triplets because both incorporate\nsyntactic dependencies and thus can effectively aggregate\nsyntax-related information during prediction.\nIn the second example, the irrelevant context word great\nis equally close to the aspect term ambiance as it is to the\nopinion term good. GTS regards both great and good as\nopinion terms for ambiance, thus producing the incorrect\ntriplet ( ambiance, great, Pos). The same situation occurs for\nS3E2. After several iterations, GTS associates the aspect\nterm ambiance with the irrelevant word reﬁned, thus mis-\ntakenly predicting ( ambiance, reﬁned, Pos) as a triplet. The\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nTABLE 8\nCase study. The aspect and opinion terms are respectively highlighted in orange and blue. The red line and the blue dotted line respectively\nindicate the predicted triplets and missed correct triplets.\nGTS\nThe arrived 20 minutes after I called cold and soggyfood\nNeg\n×\nNeg\nNeg\nThe arrived 20 minutes after I called cold and soggyfood\nS3E2\nNeg Neg\nThe arrived 20 minutes after I called cold and soggyfood\nSA-Transformer\ndet nsubj cc\nconjadvcl\nadvcl\nThe arrived 20 minutes after I called cold and soggyfood\nSyntax Dependency\n(a): The Golden Triplets of ﬁrst sentence is [( food, cold, Neg), (food, soggy, Neg)].\nGTS\nPos\nPos\n× Pos\n×\nservice is and the ambiance is goodrefined great for a dateThe\nS3E2\nPos\nPos\n× Pos\nPos\nservice is and the is goodrefined great for a dateThe ambiance\nSA-Transformer\nPos\nPos\nPos\nservice is and the is goodrefined great for a dateThe ambiance\ndet nsubj\nconj\nacomp conj det nsubj acomp pobj\nservice is and the is goodrefined great for a dateThe ambiance\nSyntax Dependency\ncc prep det\n(b): The Golden Triplets of the second sentence is [( service, reﬁned, Pos), (service, great, Pos), (ambiance, good, Pos)].\nNeg\nPos\n×\nis and food is notrotten the greatold\nNeg\nDecor\nGTS\nNeg\nNegNeg\nNeg\n×\nis and food is notrotten the greatoldDecor\nS3E2\nNeg\nNeg\nNeg\nNeg\nis and food is notrotten the greatoldDecor\nSA-Transformer\nnsubj\nconj\nacomp conj det nsubj acompneg\nis and food is notrotten the greatoldDecor\nSyntax Dependency\ncc\n(c): The Golden Triplets of the third sentence is [( Decor, old, Neg), (Decor, rotten, Neg), (food, not great, Neg)].\nFig. 7. Visualization of the attention of a given sentence.\nproposed SA-Transformer model can avoid the generation\nof the incorrect triplet ( ambiance, reﬁned, Pos) because it\ncan assign different weights to different edges even if they\nhave the same dependency type. For instance, it can assign\na lower weight to the conj between is and is to block\ninappropriate propagation from reﬁned to ambiance. It can\nalso assign a higher weight to the conj between is and great\nto successfully aggregate information between service and\ngreat.\nIn the third example, the opinion term not great consists\nof multiple words. Both GTS and S3E2 fail to completely\nextract the opinion term. SA-Transformer can do this be-\ncause it applies adjacent inference to deal with multiword\naspect/opinion terms. For this case, the sentiment polarity\nof the word pair ( food, great) is correctly predicted as neg-\native by learning the predicted result of its adjacent word\npair (food, not).\n4.10 Visualization\nTo further demonstrate how SA-Transformer improves the\nASET task, we select the second example in Table 8 to\nvisualize the attention weights of its words, as shown in\nFig. 7. For the example sentence, SA-Transformer correctly\npredicts the tag of (service, reﬁned) as positive because it\nassigns a higher weight to nsubj and acomp (black lines)\nand thus can effectively align service with reﬁned through\ntwo graph propagation iterations. The above situation also\noccurs in the case of ( ambiance, good). On the other hand,\nalthough the two edges conj between is and is and conj\nbetween reﬁned and great have the same dependency type,\nthey are assigned different weights (respectively lower and\nhigher). This example demonstrates that learning edge rep-\nresentations for each edge by querying its adjacency edges\ncan obtain more appropriate weights and representations\nand thereby result in more accurate graph propagation.\n5 CONCLUSIONS\nThis study proposes a syntax-aware transformer that can\nencode dependency type information into both edge and\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\nword representations to improve graph neural networks\nfor ASTE. By encoding the dependency types into edge\nrepresentations, the proposed method can learn different\nrepresentations and weights for different edges, even for\nthose with the same dependency type, thus achieving more\naccurate graph propagation. Incorporating edge represen-\ntations into contextual word representations can further\nlearn syntactic and positional relationships between words\nto enhance word pair representations. To effectively extract\ntriplets for multiword aspect/opinion terms, an adjacency\ninference strategy is developed to iteratively predict the tag\nof each word pair from the predicted results of its adja-\ncent word pairs. Experiments on four benchmark datasets\ndemonstrate the effectiveness of the proposed method. A\nseries of experiments was also conducted for in-depth anal-\nysis, including an ablation study that showed that each\ncomponent contributes to triplet extraction; a dependency\nparsing experiment that examined the effects of using differ-\nent dependency parsing toolkits on extraction performance;\na case study that presented several missed and correctly\nextracted triplets to discuss the effectiveness and limitations\nof different methods; and a visualization experiment that il-\nlustrated the attention weights of each dependency type for\nan example sentence to explain how the proposed method\ncan accomplish proper graph propagation through weight\nassignment.\nFuture work will focus on incorporating other useful\nexternal knowledge to improve graph propagation and con-\nsider long-range information between word pairs to extend\nthe adjacent inference strategy. Another direction is to in-\nvestigate recent advancements in ABSA tasks such as large\nlanguage models (LLMs) [55], prompt-based methods [56]\nand neurosymbolic AI frameworks [57] to improve ASTE.\nACKNOWLEDGMENTS\nThis work was supported by the National Natural Science\nFoundation of China (NSFC) under grant no. 61966038, in\npart by the Ministry of Science and Technology, Taiwan,\nROC, under grant no. MOST110-2628-E-155-002. The au-\nthors would like to thank the anonymous reviewers for their\nconstructive comments.\nREFERENCES\n[1] K. Schouten and F. Frasincar, “Survey on Aspect-Level Sentiment\nAnalysis,” IEEE Transactions on Knowledge and Data Engineering ,\nvol. 28, no. 3, pp. 813–830, 2016.\n[2] A. Nazir, Y. Rao, L. Wu, and L. Sun, “Issues and challenges of\naspect-based sentiment analysis: A comprehensive survey,” IEEE\nTransactions on Affective Computing, vol. 13, no. 2, pp. 845–863, 2022.\n[3] H. Liu, I. Chatterjee, M. Zhou, X. S. Lu, and A. Abusorrah,\n“Aspect-based sentiment analysis: A survey of deep learning\nmethods,” IEEE Transactions on Computational Social Systems, vol. 7,\nno. 6, pp. 1358–1375, 2020.\n[4] S. Poria, D. Hazarika, N. Majumder, and R. Mihalcea, “Beneath the\ntip of the iceberg: Current challenges and new directions in senti-\nment analysis research,” IEEE Transactions on Affective Computing ,\nvol. 14, no. 1, pp. 108–132, 2023.\n[5] M. Soleymani, D. Garcia, B. Jou, B. Schuller, S. F. Chang, and\nM. Pantic, “A survey of multimodal sentiment analysis,” Image\nand Vision Computing, vol. 65, pp. 3–14, 2017.\n[6] I. Chaturvedi, E. Cambria, R. E. Welsch, and F. Herrera, “Dis-\ntinguishing between facts and opinions for sentiment analysis:\nSurvey and challenges,” Information Fusion , vol. 44, pp. 65–77,\n2018.\n[7] F. Z. Xing, E. Cambria, and R. E. Welsch, “Natural language based\nﬁnancial forecasting: a survey,”Artiﬁcial Intelligence Review, vol. 50,\nno. 1, pp. 49–73, 2018.\n[8] G. Y. Zhou and J. X. Huang, “Modeling and mining domain\nshared knowledge for sentiment analysis,” ACM Transactions on\nInformation Systems, vol. 36, no. 2, pp. 1–36, 2017.\n[9] H. Peng, L. Xu, L. Bing, F. Huang, W. Lu, and L. Si, “Knowing\nWhat, How and Why: A Near Complete Solution for Aspect-based\nSentiment Analysis,” in Proceedings of the AAAI 2019 , 2019, pp.\n8600–8607.\n[10] W. Xue and T. Li, “Aspect based sentiment analysis with gated\nconvolutional networks,” in Proceedings of the ACL 2018 , 2018, pp.\n2514–2523.\n[11] Z. Li, Y. Wei, Y. Zhang, X. Zhang, and X. Li, “Exploiting coarse-\nto-ﬁne task transfer for aspect-level sentiment classiﬁcation,” in\nProceedings of the AAAI 2019, 2019, pp. 4253–4260.\n[12] X. Li, L. Bing, P . Li, and W. Lam, “A uniﬁed model for opinion\ntarget extraction and target sentiment prediction,” in Proceedings\nof the AAAI 2019, 2019, pp. 6714–6721.\n[13] M. Hu, Y. Peng, Z. Huang, D. Li, and Y. Lv, “Open-domain\ntargeted sentiment analysis via span-based extraction and clas-\nsiﬁcation,” in Proceedings of the ACL 2020, 2020, pp. 537–546.\n[14] H. Li and W. Lu, “Learning explicit and implicit structures for\ntargeted sentiment analysis,” in Proceedings of the EMNLP 2020 ,\n2020, pp. 5478–5488.\n[15] N. Majumder, R. Bhardwaj, S. Poria, A. Gelbukh, and A. Hussain,\n“Improving aspect-level sentiment analysis with aspect extrac-\ntion,” Neural Computing and Applications , vol. 34, pp. 8333–8343,\n2022.\n[16] W. Wang, S. J. Pan, D. Dahlmeier, and X. Xiao, “Coupled multi-\nlayer attentions for co-extraction of aspect and opinion terms,” in\nProceedings of the AAAI 2017, 2017, pp. 3316–3322.\n[17] H. Dai and Y. Song, “Neural aspect and opinion term extraction\nwith mined rules as weak supervision,” in Proceedings of the ACL\n2020, 2020, pp. 5268–5277.\n[18] C. Zhang, Q. Li, D. Song, and B. Wang, “A multi-task learn-\ning framework for opinion triplet extraction,” in Findings of the\nEMNLP 2020, 2020, pp. 819–828.\n[19] S. Chen, Y. Wang, J. Liu, and Y. Wang, “Bidirectional machine\nreading comprehension for aspect sentiment triplet extraction,”\nin Proceedings of the AAAI 21, 2021, pp. 12 666–12 674.\n[20] L. Xu, Y. K. Chia, and L. Bing, “Learning span-level interactions\nfor aspect sentiment triplet extraction,” in Proceedings of the ACL\n2021, 2021, pp. 4755–4766.\n[21] D. Dai, T. Chen, S. Xia, G. Wang, and Z. Chen, “Double embedding\nand bidirectional sentiment dependence detector for aspect sen-\ntiment triplet extraction,” Knowledge-Based Systems , vol. 253, pp.\n109 506–109 514, 2022.\n[22] J. Zhang, Zhihao and Zuo, Yuan and Wu, “Aspect Sentiment\nTriplet Extraction: A Seq2Seq Approach With Span Copy En-\nhanced Dual Decoder,”IEEE/ACM transactions on audio, speech, and\nlanguage processing, vol. 30, pp. 2729–2742, 2022.\n[23] Z. Wu, C. Ying, F. Zhao, Z. Fan, X. Dai, and R. Xia, “Grid tagging\nscheme for aspect-oriented ﬁne-grained opinion extraction,” in\nFindings of the EMNLP 2020, 2020, pp. 2576–2585.\n[24] L. Xu, H. Li, W. Lu, and L. Bing, “Position-aware tagging for aspect\nsentiment triplet extraction,” in Proceedings of the EMNLP 2020 ,\n2020, pp. 2339–2349.\n[25] Z. Chen, H. Huang, B. Liu, X. Shi, and H. Jin, “Semantic and\nsyntactic enhanced aspect sentiment triplet extraction,” inFindings\nof the ACL 2021, 2021, pp. 1474–1483.\n[26] Z. Zhao, Y. Liu, H. Wu, Z. Yue, and J. Li, “Multi-task Alignment\nScheme for Span-level Aspect Sentiment Triplet Extraction,” in\nProceedings of the ICANN 2022, 2022, pp. 282–293.\n[27] R. He, W. S. Lee, H. T. Ng, and D. Dahlmeier, “An unsupervised\nneural attention model for aspect extraction,” in Proceedings of the\nACL 2017, 2017, pp. 388–397.\n[28] X. Li, L. Bing, P . Li, W. Lam, and Z. Yang, “Aspect term extraction\nwith history attention and selective transformation,” inProceedings\nof the IJCAI 2018, 2018, pp. 4194–4200.\n[29] A. Kumar, A. S. Veerubhotla, V . T. Narapareddy, V . Aruru, L. B. M.\nNeti, and A. Malapati, “Aspect term extraction for opinion mining\nusing aa Hierarchical Self-Attention Network,” Neurocomputing,\nvol. 465, pp. 195–204, 2021.\n[30] D. Ma, S. Li, F. Wu, X. Xie, and H. Wang, “Exploring sequence-to-\nsequence learning in aspect term extraction,” in Proceedings of the\nACL 2020, 2020, pp. 3538–3547.\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\n[31] Z. Wu, F. Zhao, X. Y. Dai, S. Huang, and J. Chen, “Latent opinions\ntransfer network for target-oriented opinion words extraction,” in\nProceedings of the AAAI 2020, 2020, pp. 9298–9305.\n[32] Z. Fan, Z. Wu, X. Y. Dai, S. Huang, and J. Chen, “Target-oriented\nopinion words extraction with target-fused neural sequence label-\ning,” in Proceedings of the NAACL 2019, 2019, pp. 2509–2518.\n[33] Y. Zhang, R. Jin, and Z. H. Zhou, “Understanding bag-of-words\nmodel: A statistical framework,” International Journal of Machine\nLearning and Cybernetics, vol. 1, pp. 43–52, 2010.\n[34] J. Wagner, P . Arora, S. Cortes, U. Barman, D. Bogdanova, J. Foster,\nand L. Tounsi, “DCU: Aspect-based polarity classiﬁcation for\nsemeval task 4,” in Proceedings of the SemEval 2014 , 2014, pp. 223–\n229.\n[35] L. Jiang, M. Yu, M. Zhou, X. Liu, and T. Zhao, “Target-dependent\ntwitter sentiment classiﬁcation,” in Proceedings of the ACL 2011 ,\n2011, pp. 151–160.\n[36] D. Tang, B. Qin, X. Feng, and T. Liu, “Effective lstms for target-\ndependent sentiment classiﬁcation,” in Proceedings of the COLING\n2016, 2016, pp. 3298–3307.\n[37] Y. Wang, M. Huang, X. Zhu, and L. Zhao, “Attention-based\nlstm for aspect-level sentiment classiﬁcation,” in Proceedings of the\nEMNLP 2016, 2016, pp. 606–615.\n[38] P . Chen, Z. Sun, L. Bing, and W. Yang, “Recurrent attention\nnetwork on memory for aspect sentiment analysis,” in Proceedings\nof the EMNLP 2017, 2017, pp. 452–461.\n[39] B. Liang, R. Yin, L. Gui, J. Du, Y. He, and R. Xu, “Aspect-invariant\nsentiment features learning: Adversarial multi-task learning for\naspect-based sentiment analysis,” in Proceedings of the CIKM 2020 ,\n2020, pp. 825–834.\n[40] A. Pouran Ben Veyseh, N. Nouri, F. Dernoncourt, Q. H. Tran,\nD. Dou, and T. H. Nguyen, “Improving aspect-based sentiment\nanalysis with gated graph convolutional networks and syntax-\nbased regulation,” in Findings of the EMNLP 2020 , 2020, pp. 4543–\n4548.\n[41] Y. Tian, G. Chen, and Y. Song, “Aspect-based sentiment analysis\nwith type-aware graph convolutional networks and layer ensem-\nble,” in Proceedings of the NAACL 2021, 2021, pp. 2910–2922.\n[42] X. Hou, J. Huang, G. Wang, P . Qi, X. He, and B. Zhou, “Selective\nattention based graph convolutional networks for aspect-level sen-\ntiment classiﬁcation,” in Proceedings of the Workshop on TextGraphs-\n15, 2021, pp. 83–93.\n[43] B. Liang, H. Su, L. Gui, E. Cambria, and R. Xu, “Aspect-based sen-\ntiment analysis via affective knowledge enhanced graph convolu-\ntional networks,” Knowledge-Based Systems, vol. 235, pp. 107 643–\n107 653, 2022.\n[44] Y. Xiao and G. Zhou, “Syntactic edge-enhanced graph convo-\nlutional networks for aspect-level sentiment classiﬁcation with\ninteractive attention,” IEEE Access , vol. 8, pp. 157 068–157 080,\n2020.\n[45] C. D. M. Jeffrey Pennington, Richard Socher, “GloVe: Global\nvectors for word representation,” inProceedings of the EMNLP 2014,\n2014, pp. 1532–1543.\n[46] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,”\nNeural Computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[47] B. Zeng, H. Yang, R. Xu, W. Zhou, and X. Han, “LCF: A Local Con-\ntext Focus Mechanism for Aspect-Based Sentiment Classiﬁcation,”\nApplied Sciences, vol. 9, no. 16, pp. 3389–3410, 2019.\n[48] M. Pontiki, D. Galanis, J. Pavlopoulos, H. Papageorgiou, I. An-\ndroutsopoulos, and S. Manandhar, “SemEval-2014 task 4: aspect\nbased sentiment analysis,” in Proceedings of the SemEval 2014, 2014,\npp. 27–35.\n[49] M. Pontiki, D. Galanis, H. Papageorgiou, S. Manandhar, and I. An-\ndroutsopoulos, “SemEval-2015 task 12: aspect based sentiment\nanalysis,” in Proceedings of the SemEval 2015, 2015, pp. 486–495.\n[50] M. Pontiki, S. Galanis, Dimitrios Papageorgiou, Haris Androut-\nsopoulos, Ion Manandhar, M. Al-Smadi, M. Al-Ayyoub, Y. Zhao,\nand B. Qin, “SemEval-2016 task 5: aspect based sentiment analy-\nsis,” in Proceedings of the SemEval 2016, 2016, pp. 342–349.\n[51] D. P . Kingma and J. L. Ba, “Adam: A method for stochastic\noptimization,” in Proceedings of the ICLR 2015, 2015, pp. 1–15.\n[52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you Need,”\nin Proceedings of the NIPS 2017, 2017, pp. 5998–6008.\n[53] T. Dozat and C. D. Manning, “Deep biafﬁne attention for neural\ndependency parsing,” in Proceedings of the ICLR 2017 , 2017, pp.\n1–8.\n[54] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A ro-\nbustly optimized bert pretraining approach,” arXiv preprint\narXiv:1907.11692, 2019.\n[55] M. M. Amin, E. Cambria, B. W. Schuller, and E. Cambria, “Will\naffective computing emerge from foundation models and general\nartiﬁcial intelligence? a ﬁrst evaluation of chatgpt,” IEEE Intelligent\nSystems, vol. 38, no. 2, p. 15–23, 2023.\n[56] R. Mao, Q. Liu, K. He, W. Li, and E. Cambria, “The biases of pre-\ntrained language models: An empirical study on prompt-based\nsentiment analysis and emotion detection,” IEEE Transactions on\nAffective Computing, pp. 1–11, 2022 (early access).\n[57] E. Cambria, Q. Liu, S. Decherchi, F. Xing, and K. Kwok, “Sentic-\nNet 7: A commonsense-based neurosymbolic AI framework for\nexplainable sentiment analysis,” in Proceedings of the IREC 2022 ,\n2022, pp. 3829–3839.\nLi Yuanis currently pursuing his master’s degree\nin the School of Information Science and Engi-\nneering, Yunnan University, China. He received\na bachelor’s degree in Information Management\nand Information Systems from Tianjin University\nof Technology, China. His research interests in-\nclude natural language processing, text mining,\nand machine learning.\nJin Wang is an associate professor in the\nSchool of Information Science and Engineering,\nYunnan University, China. He holds one Ph.D. in\nComputer Science and Engineering from Yuan\nZe University, Taoyuan, Taiwan and one in Com-\nmunication and Information Systems from Yun-\nnan University, Kunming, China. His research\ninterests include natural language processing,\ntext mining, and machine learning.\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15\nLiang-Chih Yuis a professor in the Department\nof Information Management at Yuan Ze Univer-\nsity in Taiwan, R.O.C. He received his Ph.D. in\nComputer Science and Information Engineering\nfrom National Cheng Kung University in Taiwan,\nR.O.C. He was a visiting scholar at the Natural\nLanguage Group, Information Sciences Institute,\nUniversity of Southern California (USC/ISI) from\n2007 to 2008, and at DOCOMO Innovations for\nthree months in 2018. He is currently Board\nMember and Convener of SIGCALL of the Asso-\nciation for Computational Linguistics and Chinese Language Processing\n(ACLCLP), and serves as an editorial board member of International\nJournal of Computational Linguistics and Chinese Language Process-\ning. His research interests include natural language processing, sen-\ntiment analysis, computer-assisted language learning. His team has\ndeveloped systems that ranked ﬁrst in IJCNLP 2017 Task 4: Customer\nFeedback Analysis, and second in the recentSemEval and BEA shared\ntask competitions.\nXuejie Zhang is a professor in the School of\nInformation Science and Engineering, and Di-\nrector of High-Performance Computing Center,\nYunnan University, China. He received his Ph.D.\nin Computer Science and Engineering from Chi-\nnese University of Hong Kong in 1998. His re-\nsearch interests include high performance com-\nputing, cloud computing, and big data analytics.\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3291730\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7337746620178223
    },
    {
      "name": "Sentiment analysis",
      "score": 0.6194220185279846
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5712892413139343
    },
    {
      "name": "Transformer",
      "score": 0.564795732498169
    },
    {
      "name": "Natural language processing",
      "score": 0.5578041076660156
    },
    {
      "name": "Dependency graph",
      "score": 0.5230394601821899
    },
    {
      "name": "Dependency (UML)",
      "score": 0.44483891129493713
    },
    {
      "name": "Dependency grammar",
      "score": 0.4341374933719635
    },
    {
      "name": "Syntax",
      "score": 0.41957271099090576
    },
    {
      "name": "Graph",
      "score": 0.3440468907356262
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2886274456977844
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I189210763",
      "name": "Yunnan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99908691",
      "name": "Yuan Ze University",
      "country": "TW"
    }
  ]
}