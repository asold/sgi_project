{
  "title": "DEMix Layers: Disentangling Domains for Modular Language Modeling",
  "url": "https://openalex.org/W3190540921",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2790629656",
      "name": "Suchin Gururangan",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1978197916",
      "name": "Mike Lewis",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2785787486",
      "name": "Ari Holtzman",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2166589550",
      "name": "Noah Smith",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A334758317",
      "name": "Luke Zettlemoyer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2970789589",
    "https://openalex.org/W3093233911",
    "https://openalex.org/W3170796112",
    "https://openalex.org/W2913946806",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3099531031",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W3176618728",
    "https://openalex.org/W2971196067",
    "https://openalex.org/W3156785025",
    "https://openalex.org/W1969486090",
    "https://openalex.org/W3103727211",
    "https://openalex.org/W2891691791",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4300858592",
    "https://openalex.org/W4287083037",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2963149635",
    "https://openalex.org/W2140842551",
    "https://openalex.org/W2762009853",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W2150884987",
    "https://openalex.org/W3102438316",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3020786614",
    "https://openalex.org/W3187255235",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W4287692509",
    "https://openalex.org/W2963205619",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3147874613",
    "https://openalex.org/W3169432135",
    "https://openalex.org/W3166707765",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2892244498",
    "https://openalex.org/W3130846764",
    "https://openalex.org/W2970172215",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W3127622310",
    "https://openalex.org/W3126553126",
    "https://openalex.org/W3093871477",
    "https://openalex.org/W3135190223",
    "https://openalex.org/W4319988532",
    "https://openalex.org/W1979839410",
    "https://openalex.org/W2133622676",
    "https://openalex.org/W2133568543",
    "https://openalex.org/W3104783994",
    "https://openalex.org/W2885965553",
    "https://openalex.org/W3112486745",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3213460052",
    "https://openalex.org/W2508920525",
    "https://openalex.org/W3002800333",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3183115158",
    "https://openalex.org/W3034640977",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3002330681",
    "https://openalex.org/W1947594277",
    "https://openalex.org/W3161820423",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2951328679",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W3037528277",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W3114610051",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W2948974578",
    "https://openalex.org/W3130523865",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W1205016396",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2734314755"
  ],
  "abstract": "Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah Smith, Luke Zettlemoyer. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5557 - 5576\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nDEM IX Layers: Disentangling Domains for Modular Language Modeling\nSuchin Gururangan†♢ Mike Lewis♢ Ari Holtzman†\nNoah A. Smith†♠ Luke Zettlemoyer†♢\n†Paul G. Allen School of Computer Science & Engineering, University of Washington\n♠Allen Institute for AI\n♢Meta AI\nSeattle, W A, USA\nsg01@cs.washington.edu\nAbstract\nWe introduce a new domain expert mixture\n(DEM IX) layer that enables conditioning a lan-\nguage model (LM) on the domain of the input\ntext. A DEM IX layer includes a collection of\nexpert feedforward networks, each specialized\nto a domain, that makes the LM modular: ex-\nperts can be mixed, added, or removed after\ninitial training. Extensive experiments with\nautoregressive transformer LMs (up to 1.3B pa-\nrameters) show that DEM IX layers reduce test-\ntime perplexity (especially for out-of-domain\ndata), increase training efficiency, and enable\nrapid adaptation. Mixing experts during infer-\nence, using a parameter-free weighted ensem-\nble, enables better generalization to heteroge-\nneous or unseen domains. We also show it is\npossible to add experts to adapt to new domains\nwithout forgetting older ones, and remove ex-\nperts to restrict access to unwanted domains.\nOverall, these results demonstrate benefits of\ndomain modularity in language models.\n1 Introduction\nMost language models (LMs) are trained with data\nhomogeneity: all parameters are updated to min-\nimize the loss on all of the data. We refer to this\nas dense training. Dense training leaves variation\nin the data, or domains, to be implicitly discov-\nered (Aharoni and Goldberg, 2020), assuming that\nmodels will be able to fit all domains equally well.\nWhile dense training is convenient, and densely\ntrained LMs achieve impressive results (Brown\net al., 2020), the approach has drawbacks with re-\nspect to generalization, efficiency, and flexibility.\nEven if training data is sourced from many do-\nmains, dense training can in practice emphasize\nsubsets of the data in proportion to their ease of\naccess (Oren et al., 2019; Fan et al., 2020), limiting\ngeneralization to less prevalent domains. Updat-\ning all parameters of the network gets substantially\nmore expensive as model size grows (Strubell et al.,\n2019), making fine-tuning or domain adaptation\nx0\nh0\nDEmix \nLayer\nSelf \nAttn\nGithub  \nCode\nFFN 1\n FFN 2 FFN 3 FFN 4 FFN 5\nMedical \nPapers\nU.S. Court \nOpinions\nTraining \nCOVID-19 \nPapers\nFFN 1\n FFN 2 FFN 3 FFN 4\nInference \nFigure 1: Illustration of a DEM IX layer in a single\ntransformer block. During training, expert feedforward\nnetworks are conditionally activated based on the do-\nmain (here, document provenance) of the input sequence\n(i.e., scientific papers or court opinions). At inference\ntime, the language model has new modular functions:\ndomain experts can be mixed to handle heterogeneous\ndomains (e.g., COVID-19 papers), added to adapt to\nnovel domains (e.g., Github code), or removed to re-\nduce the influence of unwanted domains (e.g., social\nmedia). Image attribution in §A.1.\nhard to perform with smaller computational bud-\ngets. It is also difficult to adapt to new domains\nwithout forgetting the original data (McCloskey\nand Cohen, 1989; Aghajanyan et al., 2021) or to\nrestrict access to certain domains the LM has been\nexposed to during training (e.g., those that contain\nhatespeech; Bender et al. 2021), leading to risks of\nunwanted behavior (Gehman et al., 2020).\nTo address these limitations of dense training, we\nargue that LMs should be designed withmodularity.\nWe propose a modular LM that has components\nspecialized to distinct domains in the training data,\nand can be customized at inference-time by mixing,\n5557\nadding, or removing these separated components\nas needed. This design principle emphasizes the\nability to rapidly adapt the LM after training, a\nneed that has been broadly advocated for language\nsystems (Dinan et al., 2021; Lazaridou et al., 2021).\nWe introduce modularity into an LM with a new\ndomain expert (DEM IX) layer that explicitly condi-\ntions the LM on the domain of the input text (when\nit is known), or estimates the input domain during\ninference (when it is not known). A DEM IX layer\nis a drop-in substitute for a feedforward layer in\na transformer LM (e.g., GPT-3 ), creating a spe-\ncialized version of the layer (or expert) per domain\n(see Figure 1; §3).\nThis is an example of conditional computation\n(Fedus et al., 2021; Lepikhin et al., 2020; Lewis\net al., 2021; Roller et al., 2021). Unlike dense train-\ning, conditional computation activates different pa-\nrameters for different inputs. Instead of learning\nhow to route data to experts, theDEM IX layer rout-\ning mechanism follows from a natural, observable\nsegmentation of the data.1\nWe identify domains using coarse provenance\ncategories (e.g., whether a document is a medi-\ncal research paper or a Reddit post; §2). Train-\ning on data from eight different domains, we find\nthat replacing every feedforward layer in the trans-\nformer with a DEM IX layer consistently improves\nin-domain performance (§4). To improve perfor-\nmance in settings in which the target data does\nnot clearly align with a single domain, we intro-\nduce a parameter-free probabilistic approach to dy-\nnamically estimate a weighted mixtureof domains\nduring inference (§5). We observe that expert mix-\ning provides especially strong performance gains\non novel test-time domains, as well as consistent\nperformance improvements on test data from the\ntraining domains, which may themselves be het-\nerogeneous.\nOur results suggest that DEM IX consistently\nimproves model generalization, especially out-of-\ndomain, while enabling many new modular capabil-\nities. Because DEM IX forces experts to specialize\nto domains, the overall model can be (partially)\ndisentangled after training. Beyond mixing, we\ncan add (§6) or remove (§7) domain experts, pre-\ndictably changing model behavior at inference time.\nAdding experts allows for model adaptation with-\nout updating all parameters (hence avoiding forget-\n1We perform a detailed comparison of learned andDEM IX\nrouting in §A.5.\nting), and removing experts allows for simulating\nthe removal of training domains without additional\ntraining. These results, in aggregate, demonstrate\nthe considerable benefits of moving away from\ntreating data homogeneously during language mod-\neling. Our code is publicly available.2\n2 Multi-Domain Corpus\nTo better measure domain modularity, we introduce\na new multi-domain corpus constructed with do-\nmain provenancethat records the original dataset\neach document appeared in (Table 1). Defining\ndomains in this way is intuitive and conveys a great\ndeal about the type of language that can be expected\nin each document. Other accounts of domains (e.g.,\nLucy and Bamman, 2021; Gururangan et al., 2020)\nmay be studied in future work. While other multi-\ndomain corpora (Koh et al., 2021; Gao et al., 2020)\ncover many more domains, our corpus is restricted\nto datasets with more permissive licensing to sup-\nport reproducibility.\nWe divide our data into training and test domains.\nThe training domains text from eight English cor-\npora (top of Table 1), each of which varies in com-\nplexity and coverage, totaling 73.8B whitespace-\nseparated tokens. Our test (or novel) domains in-\nclude eight collections of English text (bottom of\nTable 1), which may or may not align with the train-\ning domains. The novel domains allow us to mea-\nsure how models generalize to a more challenging\ndata distribution shift, where domain boundaries\nmay be less clear.\n§A.2 has more details on how these data were\ncollected. For larger domains, we use an additional\n10M tokens for the validation and test sets each.\nSmaller domains have 1M tokens in each. To sup-\nport future work with the data, we also release an\nAPI to download and preprocess it into a format\ncompatible with Fairseq (Ott et al., 2019).3\n3 DEM IX Layer\n3.1 Background: Mixture-of-Experts\nTransformers\nThe transformer architecture interleaves multi-head\nself-attention, layer-norms, and feedforward net-\nworks (Vaswani et al., 2017). Our focus is on the\nfeedforward component:\nht,ℓ = FFN(ht,ℓ−1), (1)\n2github.com/kernelmachine/demix\n3github.com/kernelmachine/demix-data\n5558\nDomain Corpus # Train (Eval.) Tokens\nTRAINING\n1B 30M NewsWire sentences (Chelba et al., 2014) 700M (10M)\nCS 1.89M full-text CS papers from S2ORC (Lo et al., 2020) 4.5B (10M)\nLEGAL 2.22M U.S. court opinions, 1658 to 2018 (Caselaw Access Project) 10.5B (10M)\nMED 3.2M full-text medical papers from S2ORC (Lo et al., 2020) 9.5B (10M)\nWEBTEXT † 8M Web documents (Gokaslan and Cohen, 2019) 6.5B (10M)\nREAL NEWS † 35M articles from REAL NEWS (Zellers et al., 2019) 15B (10M)\nREDDIT Reddit comments from pushshift.io (Baumgartner et al., 2020) 25B (10M)\nREVIEWS † 30M Amazon product reviews (Ni et al., 2019) 2.1B (10M)\nTotal 73.8B (80M)\nDomain Corpus # Train (Eval.) Tokens\nNOVEL\nACL PAPERS 1.5K NLP papers from ACL (Dasigi et al., 2021) 1M (1M)\nBREAKING NEWS † 20K latest articles from 400 English news sites (Baly et al., 2018) 11M (1M)\nCONTRACTS † 500 commercial legal contracts (Hendrycks et al., 2021) 1.5M (1M)\nCORD-19 400K excerpts from COVID-19 research papers (Wang et al., 2020) 60M (10M)\nGITHUB 230K public Github repository contents (Github Archive Project) 200M (10M)\nGUTENBERG 3.2M copyright-expired books (Project Gutenberg) 3B (10M)\nTWEETS † 1M English tweets from 2013-2018 8M (1M)\nYELP REVIEWS † 6M Yelp restaurant reviews (Yelp Reviews) 600M (10M)\nTable 1: Domains that make up our multi-domain training corpus, including the size of our training and evaluation\n(i.e. validation and test) data, in whitespace-separated tokens. †indicates datasets that we (partially) anonymize\n(§2). See Appendix §A.2 for more details on how these data were collected.\nwhere ht,ℓ is the vector for the tth token produced\nby layer ℓ.\nShazeer et al. (2017) propose to replace dense\nfeedforward layers with an ensemble of nexperts\nFFN1,..., FFNn, assigned weights respectively\nby functions g1,...,g n:\nFFN(ht,ℓ−1) =\nn∑\nj=1\ngj(ht,ℓ−1) ·FFNj(ht,ℓ−1)\n(2)\nThe gfunction routes tokens to different experts,\nusually each a separate dense feedforward network.\nIf g routes to a single expert, then the computa-\ntional cost (in floating-point operations; FLOPs)\nwill be same as a corresponding dense network,\neven though it has more than ntimes as many pa-\nrameters.\n3.2 DEM IX Routing\nPrevious approaches learn the weighting functions\ng at a token-level, and either assign at most one\n(Fedus et al., 2021) or two (Lepikhin et al., 2020)\nexperts per token. This requires careful load bal-\nancing to encourage the model to use all experts,\nmotivating work on explicit balancing mechanisms\n(Lewis et al., 2021).\nInstead of learning g, we use domain metadata\nto route data to experts at the document (i.e., se-\nquence) level. During training, every token in an\ninput text is assigned to the same expert based on\nthe domain label.\nLet Ddenote the set of domain labels (i.e., the\neight labels in Table 1). If we index the experts by\nDand d ∈D is the domain label for the current\ntraining instance, then\ngj(ht,ℓ) =\n{1 if j = d\n0 otherwise (3)\nWe assume that each training document is asso-\nciated with a single domain label. However, we\nrelax this requirement at inference time (§5), to\nmodel unseen or heterogeneous domains.\nWe perform a detailed comparison of DEM IX\nrouting with GSHARD (Lepikhin et al., 2020), a\nmixture-of-experts transformer LM with learned\ntoken-level routing, in §A.5. Our results suggest\nthat learned token-level routing does not enable\nmodularity, underperforms DEM IX at similar com-\nputational budgets (especially on novel domains),\nand is much less efficient to train and evaluate.\n3.3 DEM IX Architecture\nOur design results in one expert in a DEM IX layer\nper domain (i.e., eight experts for eight training\ndomains in our multi-domain corpus).\nWe replace every feedforward layer in the trans-\nformer with a separate DEM IX layer, in contrast to\nprevious work (Fedus et al., 2021; Lepikhin et al.,\n2020) that interleaves shared and expert layers.\n5559\nParameters per GPU\n125M 350M 760M 1.3B\nDENSE\nGPUs 32 64 128 128\nTotal Experts 0 0 0 0\nGPUs/expert 0 0 0 0\nTotal params 125M 350M 760M 1.3B\nTFLOPs/update 556 3279 13,637 23,250\nTFLOPs/GPU 31 37 45 51\nDEM IX\nGPUs 32 64 128 128\nTotal Experts 8 8 8 8\nGPUs/expert 4 8 16 16\nTotal params 512M 1.8B 3.8B 7.0B\nTFLOPs/update 556 3279 13,637 23,250\nTFLOPs/GPU 31 37 48 55\nTable 2: Our specifications for training DENSE and\nDEM IX LMs. All models are trained for about 48 hours\non V100 GPUs. DEM IX layers increase the total pa-\nrameters of the LM while maintaining (or increasing)\nthroughput, measured in TFLOPs/GPU. We use the for-\nmula described in Narayanan et al. (2021) to calculate\nthese metrics. See §A.4 for more details.\nPreliminary experiments showed that interleaving\nled to worse in-domain performance (see §A.6 for\nmore details). Future work may comprehensively\ncompare different architectural choices.\nEach expert FFNj is a two-layer MLP with the\nsame dimensions as the original FFN layer of the\ntransformer. This means that the effective number\nof parameters in the overall DEM IX LM increases\n(Table 2), without increasing inference runtime.\n3.4 DEM IX Training\nTo train an LM with DEM IX layers, we partition\nthe GPUs among the domains, so that each GPU\nis assigned a single domain (along with its cor-\nresponding expert). Each mini-batch contains k\nsequences from a particular domain, and we send\neach mini-batch to its dedicated expert. We use\nlarger batch sizes with distributed data parallel be-\ntween expert parameters on GPUs assigned to the\nsame domain; we assign n/8 GPUs to each domain\n(Table 2).\nCompared to dense LMs, DEM IX layers achieve\nthe same or slightly higher throughput (measured\nin TFLOPs/GPU) for the same total FLOPs per up-\ndate, despite adding significantly more parameters\n(Table 2). DEM IX achieves higher throughput be-\ncause we while we sync shared parameters across\nall GPUs, we only sync expert parameters allocated\nto the same domain. Dense models sync all param-\neters across all GPUs. As we increase model size,\nDEMix reduces latency costs between GPUs, and\nhence, leads to faster training.\n3.5 Comparison with Adapters\nDEM IX experts are related to adapters (Bapna and\nFirat, 2019), which add a small feedforward net-\nwork into a frozen pretrained LM to enable param-\neter efficient finetuning. In contrast, our focus is on\nefficiently training all of the parameters of a mod-\nular LM from scratch, and as such is not directly\ncomparable to existing adapter schemes. Adapters\ncould enable more fine-grained control over which\nparts of the LM are domain-specific, and may cir-\ncumvent the need to train domain-aware LMs from\nscratch. However, the shared parameters in the\nfrozen pretrained LM may limit modularity. We\nleave exploring such architectural variants and their\ntradeoffs to future work.\n4 In-Domain Performance\nOur first set of experiments measure in-domain\nperformance when replacing the feedforward layers\nin a transformer LM with DEM IX layers.\n4.1 Experimental Setup\nArchitecture, Input and Hyperparameters We\nfollow the GPT-3 (Brown et al., 2020) architecture\n(small, medium, large, and XL) implemented in\nFairseq (Ott et al., 2019). We use the GPT-2 (Rad-\nford et al., 2019) vocabulary of 50,264 BPE types,\nand train with 1,024-token sequences. See §A.7 for\ntraining hyperparameters.\nEvaluation We follow previous work by report-\ning performance for a given computational budget\n(Lewis et al., 2021). For each model, we report test\nperplexity after a single run of 48 hours of train-\ning on differing numbers of NVIDIA V100 32GB\nGPUs (Table 2).\n4.2 Models\nDENSE The first baseline is a dense LM, imple-\nmented with distributed data parallel (Li, 2021).\nThere is no explicit conditioning on domain.\nDENSE (balanced) We train dense models on an\nequal amount of data from each domain. While\nthere is still no explicit conditioning on domain,\nthe gradient updates during training average across\nall domains represented in a batch.\n+DOMAIN -TOKEN This model is trained identi-\ncally to DENSE (balanced), but we prepend a token\nto every sequence indicating its domain (during\ntraining and test time). We ignore the domain to-\nken when computing perplexity during evaluation.\n5560\nParameters per GPU\n125M 350M 760M 1.3B\nDENSE 20.6 16.5 14.5 13.8\nDENSE (balanced) 19.9 15.8 14.3 13.6\n+DOMAIN -TOKEN 19.2 15.9 14.3 13.4\nDEM IX (naive) 18.4 15.5 14.2 13.8\nDEM IX (cached; §5.4) 17.8 14.7 13.9 13.4\nTable 3: Average in-domain test-set perplexity across\nthe 8 domains in the training data. We discuss the last\nrow in §5.4. See §A.9 for per-domain results.\nDEM IX (naive) We replace every feedforward\nlayer in the transformer with a DEM IX layer, and\nuse DEM IX training (§3). Under this naive setting,\nthe test data domain is known (e.g., the CS expert\nis used for CS test data). We relax this assumption\nin the next section.\n4.3 Results\nTable 3 shows test perplexities, averaged across\nthe eight training domains. Domain balancing is\nconsistently helpful for dense training. Additional\ndomain information always helps (i.e., domain to-\nkens or DEM IX layers), but the effects are largest\nfor the smaller models. Overall, domain informa-\ntion enables the model to better specialize to dif-\nferent training domains. However, as the model\nsize grows, the dense baseline improves, catching\nup to the DEM IX (naive) model, at least when\nconsidering the average perplexity across domains.\n4.4 Domain Hetereogeneity\nHowever, a more complete view of the experiments\nwith the largest model is shown in Table 4. We see\nthat even at scale, most training domains benefit\nfrom DEM IX layers in a naive setting (where the\ndomain label is revealed at test time), but some do\nnot; WEBTEXT , REAL NEWS , and REDDIT fare\nworse than the dense baseline. We hypothesize that\ndense training is advantageous for hetereogenous\ndomains. Heterogeneous domains have a higher\noverlap with other training domains, and therefore,\nbenefit from parameter sharing.\nIndeed, we observe that experts perform best on\ntheir assigned domain, and the experts assigned to\ndomains that benefit from dense training perform\nrelatively well on many training domains (§A.8).\nThese findings suggest overall that a discrete notion\nof domain is too rigid. In the next section, we\nsoften Equation 3 into a mixture of experts.\n1.3B parameters per GPU\nDomain D ENSE DEM IX DEM IX\n(naive) (cached prior; §5.4)\n1B 11.8 11.5 11.3\nCS 13.5 12.2 12.1\nLEGAL 6.8 6.7 6.7\nMED 9.5 9.2 9.1\nWEBTEXT 13.8 14.6 14.3\nREAL NEWS 12.5 13.3 13.1\nREDDIT 28.4 30.6 28.1\nREVIEWS 14.0 12.6 12.5\nAverage 13.8 13.8 13.4\nTable 4: Test perplexity by domain for the largest mod-\nels. We discuss the last column in §5.4.\n5 Mixing Experts at Inference Time\nThe previous section established that incorporating\nDEM IX layers improves LM performance on test\ndata from known training domains. In practice,\nhowever, text may not come with a domain label,\nmay straddle multiple domains, or may not belong\nto any of the domains constructed at training time.\nIn these cases, rather than a hard choice among\nexperts (Equation 3), we propose to treatg1,...,g n\nas mixture coefficients, transforming the domain\nmembership of an input text into a matter of proba-\nbilistic belief. Unlike previously proposed mixture-\nof-experts formulations (Shazeer et al., 2017; Lep-\nikhin et al., 2020), this approach is parameter-free\nand computed only at test time.\n5.1 Dynamic Domain Mixtures\nConsider the probabilistic view of language model-\ning, where we estimate p(xt |x<t). We introduce\na domain variable, Dt, alongside each word. We\nassume that this hidden variable depends on the\nhistory, x<t, so that:\np(xt |x<t)=\nn∑\nj=1\np(xt |x<t, Dt = j) ·p(Dt = j |x<t)  \ngj\n(4)\nThis model is reminiscent of class-based n-gram\nLMs (Brown et al., 1992; Saul and Pereira, 1997).\nWe have already designed the DEM IX LM to\ncondition on a domain label, giving a form for\np(Xt | x<t,Dt = j). We now further treat\ng1,...,g n as a posterior probability over domains,\ncalculated either globally or at each timestep.\n5561\nx<t\n“ The COVID-19 pandemic is \ncaused by severe acute  \nrespiratory syndrome \ncoronavirus-2 (SARS-CoV-2)  \nand has spread worldwide…”\nxt\nP(Dt | x<t)\nDt\nFFN 2 FFN 3 FFN 4FFN 1\nFigure 2: Illustration of inference with domain expert\nmixing. For a given input text x<t from CORD-19 , we\nestimate a posterior domain probabilities p(Dt |x<t),\ninformed by a prior that is either iteratively updated\nduring inference, or is precomputed and cached on held-\nout data. In this example, the model assigns highest\ndomain probabilities to the medical and news domains.\nWe use these probabilities in a weighted mixture of\nexpert outputs to compute the output xt.\nTo do this, we apply Bayes’ rule:\np(Dt = j |x<t)= p(x<t |Dt = j) ·p(Dt = j)\np(x<t) (5)\n= p(x<t |Dt = j) ·p(Dt = j)∑n\nj′=1 p(x<t |Dt = j′) ·p(Dt = j′)\n(6)\nThe conditional probabilities of word sequences\ngiven a domain label, as noted above, are already\ndefined by the DEM IX LM. For the prior over\ndomain labels, we consider three alternatives:\nUniform Set a uniform prior across domains.\nUpdating Set the prior at timestep t to be an\nexponentially weighted moving average of the pos-\nteriors from previous timesteps:\np(Dt = j) ∝\nt−1∑\nt′=1\nλt−t′\n·p(Dt′ = j |x<t′) (7)\nDuring evaluation, this moving average is calcu-\nlated over the posterior at the end of each sequence.\nThe decay factor avoids putting too much weight\non calculations made early in the dataset, when\nposterior calculations are noisier (§A.10). We per-\nformed a small grid search to set the value λ, and\nfound that λ= 0.3 worked well.\n1B\nCS\nLegal\nMed\nWebtext\nRealnews\nReddit\nReviews\nx< 102, 400\n1B\nCS\nLegal\nMed\nWebtext\nRealnews\nReddit\nReviews\nD102, 400\nTraining Domains\nCORD-19\nGithub\nGutenberg\nBreaking News\nContracts\nACL\nTweets\nYelp\nx< 102, 400\n1B\nCS\nLegal\nMed\nWebtext\nRealnews\nReddit\nReviews\nD102, 400\nNovel Domains\n0.0\n0.5\n1.0\nP(Dt|x< t)\n0.0\n0.5\n1.0\nP(Dt|x< t)\nFigure 3: Estimates of posteriors p(Dt | x<t) with\na DEM IX LM (1.3B parameters per GPU), after 100\nsequences (i.e., 102,400 tokens) of data in training (top\nheatmap) and novel domains (bottom heatmap).\nCached We calculate the posterior over domain\nlabels from additional data from the test distribu-\ntion, and fix the prior to that estimate. We use\n100 sequences from the validation set to estimate\nthe prior, which we found to result in stable poste-\nrior probabilities. See §A.10 for more details, and\nFigure 2 for an illustration of expert mixing.\n5.2 Visualizing Domain Membership\nIn Figure 3, we plot domain posteriors calculated\nusing the largest DEM IX LM from §4 and the up-\ndating prior, after 100 sequences of validation data.\nFor training domains, the associated domain label\nhas the highest probability, but some of the do-\nmains are more hetereogeneous than we assumed.\nMore variation is observed for the novel domains.\nHowever, generally we find the domain posterior\ndistribution to be sparse; suggesting that after esti-\nmating the domain posterior, not all experts need\nto be active for test evaluation.\n5.3 Experimental Setup\nHere, we experiment with the corpus of novel do-\nmains (Table 1). We evaluate the three mixture\ntreatments of DEM IX layers (§5.1) against five\n5562\nParameters per GPU\n125M 350M 760M 1.3B\nDENSE 25.9 21.4 18.4 17.8\nDENSE (balanced) 25.3 19.6 18.3 17.1\n+DOMAIN -TOKEN 24.8 20.4 18.4 18.0\nDEM IX (naive) 28.8 23.8 21.8 21.1\nDEM IX (average) 27.2 22.4 21.5 20.1\nDEM IX (uniform) 24.5 20.5 19.6 18.7\nDEM IX (updating) 21.9 18.7 17.6 17.1\nDEM IX (cached) 21.4 18.3 17.4 17.0\nTable 5: Average perplexity on novel domains. Mixing\ndomain experts with a prior estimated using a small\namount of data in the target domain outperforms all\nother baselines. See §A.9 for per-domain results.\nbaselines. No new models are trained for this ex-\nperiment beyond those used in §4.\nDENSE and DENSE (balanced) These are the\nbasic baselines from §4.\n+DOMAIN -TOKEN Here test data is evaluated\nusing each domain label token, and we choose the\nlowest among these perplexity values per test set.\nDEM IX (naive) Similar to +DOMAIN -TOKEN ,\nwe evaluate the data separately with each of the\neight experts, and report the lowest among these\nperplexity values per test set.\nDEM IX (average) At every timestep, we take a\nsimple average of the eight experts’ predictions.\n5.4 Results\nNovel Domain Performance Ensembling\nDEM IX experts outperforms dense baselines\nand using experts individually (i.e., the “naive”\nbaseline), and caching a prior before evaluation\nresults in the best average performance (Table 5).\nEnsembling DEM IX experts with a cached prior\nallows smaller models to match or outperform\nmuch larger dense models. Weighted ensembling\noutperforms simple averaging and mixing with\na uniform prior, confirming the importance of\nsparsity in the expert mixture. These results\ndemonstrate that modularity need not come at a\ncost to generalization to new domains.4\nIn-Domain Performance We can also apply the\nexpert mixture variant of inference (using a cached\n4We have separately observed that with expert mixing,\nour largest DEM IX LM closely approaches the performance\nof GPT-3 Da-Vinci (Brown et al., 2020) on another novel\ndomain, the LM benchmark PTB (Marcus et al., 1993). See\n§A.11 for more details.\nprior) to the training domains; see the last line of\nTable 3. We see performance improvements across\nall training domains for every scale, though the\nlargest gains come from hetereogeneous domains\n(Table 4 and §A.9; across all model sizes, RED-\nDIT improves on average 10.7%, WEBTEXT 2.4%,\nREAL NEWS 1.9%), confirming that domain labels\nmay not align with the most effective boundaries.\n5.5 Summary\nAs opposed to other token-level routing mecha-\nnisms (e.g., Lepikhin et al. 2020), expert mixing in\nDEM IX is introduced at test-time and is parameter-\nfree; it instead makes use of Bayesian inference\nwith specialized experts to improve generalization.\nExpert mixing dynamically increases model capac-\nity at test-time, while avoiding the need to learn\ntoken-level routing patterns during training, which\nis expensive and breaks modularity (§A.5).\n6 Domain Adaptation with New Experts\nDomain adaptation is an important technique to\nimprove LM performance in new domains that are\nrare or unseen during training. A popular technique\nfor adapting LMs is domain-adaptive pretraining\n(DAPT ; Gururangan et al. 2020), which involves\ncontinued dense training of the LM on the target\ndomain. However, DAPT with dense training (or\nDENSE -DAPT ) is expensive (Strubell et al., 2019)\nand may entail forgetting domains learned during\nearlier training phases (Aghajanyan et al., 2021),\nsince it updates all parameters of the LM towards\nthe target domain. These issues make adapting\nlarge LMs less feasible, especially in domains that\nchange frequently over time (Luu et al., 2021).\nDEM IX layers allow for cheap adaptation with-\nout forgetting through a technique we call DEM IX-\nDAPT (Figure 4). To adapt to a new domain, we\ninitialize a new expert in each DEM IX layer us-\ning the parameters of the nearest pretrained expert,\nwhich we identify using domain posteriors from §5.\nWe then train the added expert on target data,updat-\ning only the new expert parameters. For inference,\nwe mix experts with a cached prior (§5).\n6.1 Experimental Setup\nWe compare DEM IX-DAPT to DENSE -DAPT on\nthe novel domains. We report test perplexity after\nadapting to each domain for 1 hour with 8 NVIDIA\nV100 32GB GPUs, tracking validation perplexity\nevery 10 minutes for early stopping. We adapt to\n5563\n3. Adapt new expert, freezing all other parameters\nx<t\n1. Calculate Domain Posteriors\n2. Copy “closest” expert\nFFN 1 FFN 2 FFN 3 FFN 4 FFN 5\nFFN 1 FFN 2 FFN 3 FFN 4 FFN 5\nP(Dt | x<t)\nDt\nCOVID-19 \nPapers\nCOVID-19 \nPapers\nFigure 4: Illustration of DEM IX-DAPT . First, we es-\ntimate domain posteriors on a held out sample of the\ntarget domain (e.g., CORD-19 ). We then initialize a\nnew expert with the parameters of the most likely ex-\npert under the domain posterior distribution. Finally,\nwe adapt the parameters of the new expert to the target\ndomain, keeping all other parameters in the LM frozen.\neach novel domain with the same hyperparame-\nters as §4, except with a 10x smaller learning rate.\nDEM IX-DAPT updates about 10% of the total pa-\nrameters in the DEM IX LM, while DENSE -DAPT\nupdates all parameters of the dense LM.\n6.2 Results\nAdding One Expert We display examples of\nDEM IX-DAPT and DENSE -DAPT on a single do-\nmain in Figure 5. As DENSE -DAPT proceeds, its\nperformance on the training domains progressively\nworsens (see §A.12 for results with larger LMs). In\ncontrast, DEM IX-DAPT reduces perplexity on the\nnovel domain without forgetting.\nAdding Eight Experts We find that adding all\neight experts adapted to novel domains to the\nDEM IX model from §4 significantly reduces per-\nplexity on novel and previously seen domains (Ta-\nble 6) while also helping in-domain for smaller\n10\n20\n30\nDense-DAPT\nPPL\nCORD-19\n20\n30\n40\n Gutenberg\n0 30 60\n# Minutes of DAPT\n10\n20\n30\nDEMix-DAPT\nPPL\n0 30 60\n# Minutes of DAPT\n20\n30\n40\nTraining Domains Target Domain\nFigure 5: Adapting an LM (125M parameters per GPU)\nto CORD-19 or G UTENBERG . Top row: with DENSE -\nDAPT , average perplexity on all training domains de-\ngrades. Bottom row: DEM IX-DAPT avoids forgetting\nwhile achieving close (for GUTENBERG ) or better (for\nCORD-19) performance on the target domain.\nParameters per GPU\nDomains # Experts 125M 350M 760M 1.3B\nTRAINING 8 17.8 14.7 13.9 13.4\n16 17.7 14.6 13.7 13.4\nNOVEL 8 21.4 18.3 17.4 17.0\n16 16.0 14.0 13.5 12.5\nTable 6: Average perplexity in training and novel do-\nmains before and after adding 8 experts adapted to the\nnovel domains (via DEM IX-DAPT ). Adding experts\nreduces perplexity on novel and training domains.\nmodels (perhaps surprisingly, given the fact that\ntheir domain experts are frozen). For example,\nacross all model sizes, on average, we see a 2.4%\nreduction on MED, 1.8% reduction on REAL NEWS ,\nand 2% reduction on REDDIT (see §A.9 for details).\n7 Removing Experts\nDense LMs are also prone to unexpected behavior\nwhen deployed. For example, they may generate\nhatespeech (Gehman et al., 2020), which is unde-\nsirable for user-facing tasks (Xu et al., 2020).\nWe argue that dense training contributes to un-\nexpected model behavior, as domains are learned\ndiffusely over the parameter space, and it is diffi-\ncult to restrict the model’s access to certain training\ndomains during inference. Some mechanisms have\nbeen introduced to steer a dense model towards\n(Keskar et al., 2019; Dathathri et al., 2020) and\naway (Welleck et al., 2019) from certain behaviors,\nbut they tend to be expensive or require retrain-\ning the model with a different objective, which\n5564\n125M Parameters per GPU\nDomain +E XPERT –EXPERT –DOMAIN\n1B 13.7 25.5 30.4\nCS 15.7 22.4 25.4\nLEGAL 8.9 20.9 22.7\nMED 12.4 18.6 21.9\nWEBTEXT 20.9 27.3 25.4\nREAL NEWS 18.9 26.7 25.0\nREDDIT 34.4 47.8 51.3\nREVIEWS 20.5 39.0 43.0\nAverage 18.2 28.5 30.6\nTable 7: Removing a domain expert ( –EXPERT ) de-\ngrades perplexity on the corresponding domain, ap-\nproaching the performance of an LM that has not been\nexposed to that domain (–DOMAIN ). Here we bold the\nworst performing model for each domain.\nbecomes less feasible as the LM grows in size.\nDEM IX layers offer a simple mechanism for\ncheap, lightweight control of large LMs: since do-\nmain experts specialize (§A.8), experts assigned to\nunwanted domains can be disabled at test-time.5\n7.1 Experimental Setup\nDoes disabling an expert simulate a model that has\nnot been exposed to a particular training domain?\nTo answer this question, we compare three settings:\n+EXPERT , a DEM IX LM with all experts active,\n–EXPERT , a DEM IX LM with a domain expert\ndeactivated, and –DOMAIN , a DEM IX LM trained\nfrom scratch without a particular domain.6\nFor all settings, we use a DEM IX LM (125M\nparameters per GPU) from §4 and expert mixing\nwith a cached prior (§5) for inference.\n7.2 Results\nRemoving a domain expert harms model perfor-\nmance on the associated domain, in most cases ap-\nproaching the performance of a model that has not\nbeen exposed to data from that domain (Table 7).\nIn some cases (e.g., WEBTEXT and REAL NEWS ),\n–EXPERT even underperforms –DOMAIN . This\nleads us to conjecture that most domain-specific\nlearning happens within the DEM IX layer.\nOur preliminary analysis here suggests that\nDEM IX enables LMs with removable parts, for\nquick adaptation to situations in which a particu-\nlar training domain is unwanted for inference. We\n5Removing an expert offers no guarantee of having fully\nforgotten content from the removed domain, since there are\nshared parameters in the model.\n6We replace the removed domain withGUTENBERG , since\nour cluster allocates training jobs via 8-GPU nodes.\nleave further exploration of this mechanism and its\npotential for LM control to future work.\n8 Related Work\nDocument metadata has been used to improve topic\nmodels (Mimno and McCallum, 2012), adapt RNN-\nbased LMs (Jaech and Ostendorf, 2018), learn doc-\nument representations (Card et al., 2018), and im-\nprove text generation control (Zellers et al., 2019;\nKeskar et al., 2019). Other inference-time methods\n(Dathathri et al., 2020; Liu et al., 2021) may be\nused to steer text generation with DEM IX experts.\nFuture work may explore applying DEM IX to\nmultilingual settings, as multilingual models ben-\nefit from language-specific parameters (Fan et al.,\n2020; Pfeiffer et al., 2020; Chau et al., 2020).\nDEM IX-DAPT is related to model expansion\ntechniques in reinforcement learning or vision\n(Rusu et al., 2016; Draelos et al., 2017) and\nadapters for pretrained LMs (Houlsby et al., 2019;\nPfeiffer et al., 2020).\nMulti-domain models have been studied in ma-\nchine translation (Pham et al., 2021) and supervised\nsettings (Wright and Augenstein, 2020), and with\nsmaller dense LMs (Maronikolakis and Schütze,\n2021). Previous studies have shown the importance\nof considering domains when adapting LMs (Ram-\nponi and Plank, 2020; Gururangan et al., 2020).\nOur study establishes the importance of consider-\ning domains when training LMs from scratch.\n9 Conclusion\nWe introduce DEM IX layers, which provide modu-\nlarity to an LM at inference time, addressing lim-\nitations of dense training by providing a rapidly\nadaptable system. DEM IX layers experts can be\nmixed to handle heterogeneous or unseen domains,\nadded to iteratively incorporate new domains, and\nremoved to restrict unwanted domains. Future\nwork may combine domain and token-level routing,\ndiscover domains automatically with unsupervised\nlearning, or scale the number of training domains.\nAcknowledgments\nThe authors thank Shruti Bhosale, Tim Dettmers,\nEmily Dinan, Doug Downey, Margaret Li, Myle\nOtt, Ofir Press, and Swabha Swayamdipta, for help-\nful comments. At UW, this work was partially sup-\nported by NSF grant 1562364, the Office of Naval\nResearch under MURI grant N00014-18-1-2670,\nand an Amazon research award.\n5565\nEthical Considerations\nWhile DEM IX offers new opportunities to reduce\nthe influence of unwanted training domains (e.g.,\nthose that contain hatespeech) at inference time,\nshared parameters in the LM may prevent the\nmodel from fully forgetting the unwanted domain\nafter expert removal. Therefore, DEM IX LMs may\nstill be prone to producing harmful generations\nwhen deployed, and further research is required to\nunderstand the bounds on the probability of toxic\ndegeneration after expert removal.\nWhile we partially anonymize our corpus with\nsimple regexes, it is difficult to guarantee that sen-\nsitive information is not exposed in large datasets.\nTo protect data authors and subjects, we do not\npublicly release our models or data, although we\nprovide instructions and code to replicate them to\nsupport reproducibility.\nReferences\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta,\nNaman Goyal, Luke Zettlemoyer, and Sonal Gupta.\n2021. Better fine-tuning by reducing representational\ncollapse. In 9th International Conference on Learn-\ning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7747–\n7763, Online. Association for Computational Lin-\nguistics.\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov,\nJames Glass, and Preslav Nakov. 2018. Predict-\ning factuality of reporting and bias of news media\nsources. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3528–3539, Brussels, Belgium. Association\nfor Computational Linguistics.\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 1921–1931, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\nMegan Squire, and Jeremy Blackburn. 2020. The\npushshift reddit dataset.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nPeter F. Brown, Vincent J. Della Pietra, Peter V . deS-\nouza, Jenifer C. Lai, and Robert L. Mercer. 1992.\nClass-based n-gram models of natural language.\nComputational Linguistics, 18(4):467–480.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nDallas Card, Chenhao Tan, and Noah A. Smith. 2018.\nNeural models for documents with metadata. Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers).\nCaselaw Access Project. Caselaw access project.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with multilingual BERT, a small corpus, and\na small treebank. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1324–1334, Online. Association for Computational\nLinguistics.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robinson.\n2014. One billion word benchmark for measuring\nprogress in statistical language modeling.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A. Smith, and Matt Gardner. 2021. A dataset\nof information-seeking questions and answers an-\nchored in research papers. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4599–4610, On-\nline. Association for Computational Linguistics.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. In\n8th International Conference on Learning Represen-\ntations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nEmily Dinan, Gavin Abercrombie, A. Stevie Bergman,\nShannon Spruit, Dirk Hovy, Y-Lan Boureau, and\nVerena Rieser. 2021. Anticipating safety issues in\ne2e conversational ai: Framework and tooling.\n5566\nT. Draelos, N. Miner, Christopher C. Lamb, Jonathan A.\nCox, Craig M. Vineyard, Kristofor D. Carlson,\nWilliam M. Severa, C. James, and J. Aimone. 2017.\nNeurogenesis deep learning: Extending deep net-\nworks to accommodate new classes. 2017 Interna-\ntional Joint Conference on Neural Networks (IJCNN),\npages 526–533.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Man-\ndeep Baines, Onur Celebi, Guillaume Wenzek,\nVishrav Chaudhary, Naman Goyal, Tom Birch, Vi-\ntaliy Liptchinsky, Sergey Edunov, Edouard Grave,\nMichael Auli, and Armand Joulin. 2020. Beyond\nenglish-centric multilingual machine translation.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The pile: An\n800gb dataset of diverse text for language modeling.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nGithub Archive Project. Github archive project.\nAaron Gokaslan and Vanya Cohen. 2019. Openwebtext\ncorpus.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360, Online. Association for Computational\nLinguistics.\nDan Hendrycks, Collin Burns, Anya Chen, and Spencer\nBall. 2021. Cuad: An expert-annotated nlp dataset\nfor legal contract review.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp.\nAaron Jaech and Mari Ostendorf. 2018. Low-rank rnn\nadaptation for context-aware language modeling.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nPang Wei Koh, Shiori Sagawa, Henrik Mark-\nlund, Sang Michael Xie, Marvin Zhang, Akshay\nBalsubramani, Weihua Hu, Michihiro Yasunaga,\nRichard Lanas Phillips, Irena Gao, Tony Lee, Etienne\nDavid, Ian Stavness, Wei Guo, Berton Earnshaw, Im-\nran Haque, Sara M Beery, Jure Leskovec, Anshul\nKundaje, Emma Pierson, Sergey Levine, Chelsea\nFinn, and Percy Liang. 2021. WILDS: A benchmark\nof in-the-wild distribution shifts. In Proceedings of\nthe 38th International Conference on Machine Learn-\ning, volume 139 ofProceedings of Machine Learning\nResearch, pages 5637–5664. PMLR.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\nSebastian Ruder, Dani Yogatama, Kris Cao, Tomas\nKocisky, Susannah Young, and Phil Blunsom. 2021.\nPitfalls of static language modelling.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\nGoyal, and Luke Zettlemoyer. 2021. Base layers:\nSimplifying training of large, sparse models. In\nProceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pages 6265–6274.\nPMLR.\nShen Li. 2021. Getting started with distributed data\nparallel.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021. Dexperts: Decoding-time con-\ntrolled text generation with experts and anti-experts.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4969–4983, Online. Asso-\nciation for Computational Linguistics.\nLi Lucy and David Bamman. 2021. Characterizing\nEnglish Variation across Social Media Communities\nwith BERT. Transactions of the Association for Com-\nputational Linguistics, 9:538–556.\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar-\nishma Mandyam, and Noah A. Smith. 2021. Time\nwaits for no one! analysis and challenges of temporal\nmisalignment. In Proc. of NAACL.\n5567\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated cor-\npus of English: The Penn Treebank. Computational\nLinguistics, 19(2):313–330.\nAntonios Maronikolakis and Hinrich Schütze. 2021.\nMultidomain pretrained language models for green\nNLP. In Proceedings of the Second Workshop on Do-\nmain Adaptation for NLP, pages 1–8, Kyiv, Ukraine.\nAssociation for Computational Linguistics.\nM. McCloskey and N. Cohen. 1989. Catastrophic in-\nterference in connectionist networks: The sequential\nlearning problem. Psychology of Learning and Moti-\nvation, 24:109–165.\nDavid Mimno and Andrew McCallum. 2012. Topic\nmodels conditioned on arbitrary features with\ndirichlet-multinomial regression.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper,\nPatrick LeGresley, Mostofa Patwary, Vijay Anand\nKorthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\nJulie Bernauer, Bryan Catanzaro, Amar Phanishayee,\nand Matei Zaharia. 2021. Efficient large-scale lan-\nguage model training on gpu clusters using megatron-\nlm.\nJianmo Ni, Jiacheng Li, and Julian McAuley. 2019.\nJustifying recommendations using distantly-labeled\nreviews and fine-grained aspects. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 188–197, Hong\nKong, China. Association for Computational Lin-\nguistics.\nYonatan Oren, Shiori Sagawa, Tatsunori Hashimoto, and\nPercy Liang. 2019. Distributionally robust language\nmodeling. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4227–4237, Hong Kong, China. Association for Com-\nputational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 48–53, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nMinhQuang Pham, Josep Maria Crego, and François\nYvon. 2021. Revisiting multi-domain machine trans-\nlation. Transactions of the Association for Computa-\ntional Linguistics, 9:17–35.\nProject Gutenberg. Project gutenberg.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAlan Ramponi and Barbara Plank. 2020. Neural unsu-\npervised domain adaptation in NLP—A survey. In\nProceedings of the 28th International Conference\non Computational Linguistics, pages 6838–6855,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam,\nand Jason Weston. 2021. Hash layers for large sparse\nmodels.\nAndrei A. Rusu, Neil C. Rabinowitz, Guillaume Des-\njardins, Hubert Soyer, James Kirkpatrick, Koray\nKavukcuoglu, Razvan Pascanu, and Raia Hadsell.\n2016. Progressive neural networks.\nLawrence Saul and Fernando Pereira. 1997. Aggre-\ngate and mixed-order Markov models for statistical\nlanguage processing. In Second Conference on Em-\npirical Methods in Natural Language Processing.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc V . Le, Geoffrey E. Hinton, and\nJeff Dean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,\nRussell Reas, Jiangjiang Yang, Doug Burdick, Dar-\nrin Eide, Kathryn Funk, Yannis Katsis, Rodney Kin-\nney, Yunyao Li, Ziyang Liu, William Merrill, Paul\nMooney, Dewey Murdick, Devvret Rishi, Jerry Shee-\nhan, Zhihong Shen, Brandon Stilson, Alex Wade,\nKuansan Wang, Nancy Xin Ru Wang, Chris Wilhelm,\nBoya Xie, Douglas Raymond, Daniel S. Weld, Oren\nEtzioni, and Sebastian Kohlmeier. 2020. Cord-19:\nThe covid-19 open research dataset.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2019. Neu-\nral text generation with unlikelihood training.\n5568\nDustin Wright and Isabelle Augenstein. 2020. Trans-\nformer based multi-source domain adaptation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7963–7974, Online. Association for Computa-\ntional Linguistics.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason\nWeston, and Emily Dinan. 2020. Recipes for safety\nin open-domain chatbots.\nYelp Reviews. Yelp reviews.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In NeurIPS.\n5569\nA Appendix\nA.1 Image Attribution\nImages retrieved from emojipedia.org or\nistockphoto.com.\nA.2 Collecting Domains\nAll datasets are fair use for research purposes ac-\ncording to their original licenses. For most do-\nmains, we use the associated sources, listed in Ta-\nble 1, without modification. REDDIT was extracted\nand obtained by a third party and made available on\npushshift.io, and was anonymized by Xu et al.\n(2020); we use their version. For GUTENBERG ,\nwe use the scraping tool provided in https://\ngithub.com/aparrish/gutenberg-dammit. For\nBREAKING NEWS , we identify a list of fac-\ntually reliable English news sources, using the\nlist curated by Baly et al. (2018). Specifi-\ncally, we filter on \"high\" factuality in the data\nprovided in this repository: https://github.\ncom/ramybaly/News-Media-Reliability. We\nthen use Newspaper3K ( https://newspaper.\nreadthedocs.io/en/latest/) to scrape the lat-\nest 1000 articles from each site. After drop-\nping duplicates, we arrive at about 20K articles\nfrom 400 news sources. We provide download-\ning links and general instructions at github.com/\nkernelmachine/demix-data.\nA.3 Dataset Anonymization\nTo anonymize certain datasets, we apply a suite\nof regexes that aim to identify common patterns\nof user-identifiable data and substitute them with\ndummy tokens. We display anonymization regexes\nand associated dummy tokens in Table 8.\nA.4 Calculating TFLOPs/GPU\nWe use the formula presented in Narayanan\net al. (2021) to calculate TFLOPs/GPU\nand TFLOPs/update. The spreadsheet\nthat contains the calculations and for-\nmula can be accessed here: https:\n//docs.google.com/spreadsheets/d/1NO-Lz_\nVqZGF2fpJTFxtXyjhmaoYi6qnz50Xr8W8hgGw/\nedit?usp=sharing.\nA.5 Gshard Comparison\nHere we describe empirical comparisons between\nDEM IX and GSHARD , the token-level mixture\nof experts transformer proposed by Lepikhin et al.\n(2020). As opposed to DEM IX, which uses domain\nlabels to route data to experts, GSHARD learns\na token-level routing mechanism during training.\nEach token in every other layer is sent to two of k\nexperts, and this routing is updated via backpropa-\ngation.\nAs GSHARD is emblematic of an learned routing\nprocedure, we are generally interested if GSHARD\nnaturally learns to specialize experts to domains,\nwhether its experts are modular, and howGSHARD\nLM generally performs compared to DEM IX and\nDENSE models on our multi-domain corpus.\nExperimental Setup We aim to make minimal\nchanges to the overall architecture of the model,\nto focus on the differences afforded by token-level\nrouting (vs. DEM IX routing). As such, we keep all\narchitecture and computational budgets the same\nas our DEM IX and DENSE baselines (we gener-\nally display results for the 125M, 350M, and 760M\nparameter LMs). We only add the GSHARD rout-\ning procedure to every other layer, which involves\nrouting each token to the top-2 experts of that layer.\nThis additionally necessitates a load balancing loss\nto prevent only a minority of experts from being\nused (Lepikhin et al., 2020). All GSHARD experts\nare of the same size as our DEM IX experts, i.e.,\neach expert is a two layer MLP with the same di-\nmensions as the original feedforward layer of the\ntransformer. We display hyperparameters used to\ntrain GSHARD in §A.7.\nModel Scale In DEM IX, we always add the same\nnumber of experts as the number of training do-\nmains (in our case — eight experts), and use extra\ncomputation to increase the batch size for each ex-\npert. Our GSHARD implementation, on the other\nhand, allocates one expert per GPU. This means\nthat GSHARD adds many more experts to the sys-\ntem, which results in a substantially larger increase\nin model size (Table 9). Unlike DEM IX, GSHARD\nresults in an increase in FLOP count relative to\nthe DENSE model, due to a variety of additional\ncomputation during training, like load balancing\nand routing to two experts for every token, which\nDEM IX does not need.\nTraining efficiency However, unlike DEM IX,\nwhich increases model size while maintaining or\nimproving GPU throughput, GSHARD in fact re-\nduces GPU throughput during training (Table 9).\nThis is due to the necessity of expensive all-to-all\noperations in GSHARD which mediate communica-\ntion between experts on different GPUs that are ac-\n5570\nCategory Link to Regex Dummy Token\nEmail https://regex101.com/r/ZqsF9x/1 <EMAIL>\nDART https://regex101.com/r/0tQ6EN/1 <DART>\nFB User ID https://regex101.com/r/GZl5EZ/1 <FB_USERID>\nPhone Number https://regex101.com/r/YrDpPD/1 <PHONE_NUMBER>\nCredit Card Number https://regex101.com/r/9NTO6W/1 <CREDIT_CARD_NUMBER>\nSocial Security Number https://regex101.com/r/V5GPNL/1 <SSN>\nUser handles https://regex101.com/r/vpey04/1 <USER>\nTable 8: Anonymization schema. We anonymize text using the regexes provided in the above links for the categories\nlisted.\nParameters per GPU\n125M 350M 760M 1.3B\nDENSE\nGPUs 32 64 128 128\nTotal Experts 0 0 0 0\nGPUs/expert 0 0 0 0\nTotal params 125M 350M 760M 1.3B\nTFLOPs/update 556 3279 13,637 23,250\nTFLOPs/GPU 31 37 45 51\nDEM IX\nGPUs 32 64 128 128\nTotal Experts 8 8 8 8\nGPUs/expert 4 8 16 16\nTotal params 512M 1.8B 3.8B 7.0B\nTFLOPs/update 556 3279 13,637 23,250\nTFLOPs/GPU 31 37 48 55\nGSHARD\nGPUs 32 64 128 128\nTotal Experts 32 64 128 128\nGPUs/expert 1 1 1 1\nTotal params 1B 6.7B 29.5B 52.5B\nTFLOPs/update 675 4120 17,400 30,000\nTFLOPs/GPU 15 16 19 13\nTable 9: Our specifications for trainingDENSE , DEM IX,\nand GSHARD LMs. All models are trained for about 48\nhours on V100 GPUs. DEM IX layers increase the total\nparameters of the LM while maintaining (or increasing)\nthroughput, measured in TFLOPs/GPU. We use the for-\nmula described in Narayanan et al. (2021) to calculate\nthese metrics. See §A.4 for more details.\ntivated for different tokens of the same document.7\nThese all-to-all operations are bottlenecked by the\nquality of GPU communication channels on the\ncluster. We also found that additional inefficiencies\nare introduced via GSHARD ’s load balancing, since\nsome experts are not used at test time. DEM IX has\nno load balancing or all-to-all communication. It\nuses all experts to maximum efficiency, because\nwe simply assign GPUs to domains for our routing\nprotocol.\nEvaluation efficiency Another benefit to\nDEM IX is that its experts specialize to their\ndomain, and only a sparse subset of them are\n7https://images.nvidia.com/events/\nsc15/pdfs/NCCL-Woolley.pdf\nParameters per GPU\n125M 350M 760M 1.3B\nDENSE (balanced) 19.9 15.8 14.3 13.6\nDEM IX 17.8 14.7 13.9 13.4\nGSHARD 17.2 14.3 14.2 12.7\nTable 10: Average in-domain test-set perplexity across\nthe 8 domains in the training data. We discuss the last\nrow in §5.4. See §A.9 for per-domain results.\nParameters per GPU\n125M 350M 760M 1.3B\nDENSE (balanced) 25.9 21.4 18.4 17.8\nDEM IX 21.4 18.3 17.4 17.0\nGSHARD 24.0 19.5 18.9 17.2\nTable 11: Average perplexity on novel domains. Mixing\ndomain experts with a prior estimated using a small\namount of data in the target domain outperforms all\nother baselines. See §A.9 for per-domain results.\nactivated at test time. Does token-level routing\nvia GSHARD also result in a modular model? We\nexplore this question by computing the average\ngating probabilities in the GSHARD router across\nall experts for all test data in each domain. We\ngenerally find that gating probabilities in GSHARD\nhave high entropy across experts regardless of\ndomain, suggesting that the token-level routing\nprocedure does not in fact result in modularity\nout-of-the-box and all experts are needed for all\ninput texts (Figure 6). As we increase computa-\ntional budget, this issue is exacerbated; we need\n128 GPUs to evaluate on the test data for the final\nmodel. Whereas with DEM IX, we only need 8\nGPUs to compute the domain posterior on a subset\nof the validation data. Moreover, because the\ndomain posterior is usually sparse, one can use an\neven smaller number of GPUs for evaluating on\ntest data, loading only those experts with non-zero\nprobabilities.\n5571\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\n30\nLayer 1 Layer 3 Layer 5\n1B\nCS\nLegal\nMed\nWebtext\nRealnews\nReddit\nReviews\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27\n30\nLayer 7\n1B\nCS\nLegal\nMed\nWebtext\nRealnews\nReddit\nReviews\nLayer 9\n1B\nCS\nLegal\nMed\nWebtext\nRealnews\nReddit\nReviews\nLayer 11\n0.00\n0.02\n0.04\n0.06\n0.00\n0.02\n0.04\n0.06\n0.00\n0.02\n0.04\n0.06\n0.00\n0.02\n0.04\n0.06\n0.00\n0.02\n0.04\n0.06\n0.00\n0.02\n0.04\n0.06\nFigure 6: Average gating probabilities across domains (x-axis) for each expert (y-axis) in the expert layers of a\nGSHARD LM with 125M parameters per GPU. We observe high entropy of gating probabilities across experts and\ndomains in each expert layer, with similar results in larger models.\nModel performance As noted earlier, our\nGShard implementation substantially increases the\neffective parameter count of the model relative to\nDEM IX (Table 9). While this expansion of model\nsize by GShard translates to better in-domain per-\nformance than DEMix for the 32 and 64 GPU set-\ntings, we observe the DEMix LMs consistently out-\nperform GShard on the novel domains regardless\nof computational budget (Table 11). Surprisingly,\nGSHARD underperforms DEM IX even in-domain\nfor the 760M parameter model (Table 10), despite\nbeing 4x larger in effective parameter count (Ta-\nble 9). This suggests that domain-modularity is an\nimportant mechanism to improve model general-\nization, in addition to model size. We believe there\nis a rich area of future work to investigate how to\ncombine token- and domain-level routing, to real-\nize the benefits of increasing parameter count while\nmaintaining domain modularity at scale.\nSummary Our results suggest that while\nGSHARD is an effective method for substantially in-\ncreasing model size under a fixed budget, it comes\nwith large costs to training and evaluation effi-\nciency, does not result in a modular LM. The lack\nof modularity also implies that GSHARD suffers\nfrom similar downstream issues as DENSE mod-\nels, e.g., forgetting after adaptation and lack of\nlightweight controllability, though we leave a close\nexploration of those phenomena to future work.\nOverall, DEM IX LMs are substantially simpler and\nmore efficient for training and evaluation, and even\noutperform GSHARD (especially out of domain)\ndespite being substantially smaller, suggesting the\nimportance of domain modularity as an alternative\nmechanism to model scaling for improving gener-\nalization in LMs.\nA.6 Interleaving Experiments\nWe hypothesize that shared layers may serve as a\nbottleneck to find shared features between domains,\nand may impact performance adversely when train-\ning domains are highly different from one another.\nIndeed, preliminary experiments suggest that in-\nterleaving expert layers causes large performance\nhits in the most distinct domains, i.e., those with\nlower vocabulary overlap with other domains in the\ncorpus.\nA.7 Hyperparameter Assignments\nWe display hyperparameter assignments for LM\npretraining in Tables 14, 15, 16, and 17. We set\nthe total number of training steps based on this al-\nlocated runtime, set 8% of these steps to be warm-\nup, and use the Adam optimizer (Kingma and Ba,\n2015) with a polynomial learning rate decay. Learn-\ning rates are tuned for each model separately over\n{0.0001, 0.0003, 0.0005}, taking the fastest learn-\ning rate that avoids divergence. Each worker pro-\ncesses two sequences of length 1,024, and gradients\nare accumulated over 8 updates. We clip gradients\nif their L2 norm exceeds 0.1. These settings are\ninspired by Lewis et al. (2021).\n5572\n1B\nLegal\nCS\nMed\nWebtext\nRealnews\nReddit\nReviews\nDomain\n1B\nLegal\nCS\nMed\nWebtext\nRealnews\nReddit\nReviews\nExpert\n1x\n2x\n4x\n6x\n8x\nperplexity increase\nFigure 7: Heatmap of expert performance ratios, using\nthe largest DEM IX LM (1.3B parameters per GPU).\nThe diagonal indicates that expert specialization to their\nown domain. While some experts (e.g., 1B, MED) do\nnot transfer well to most domains in the training corpus,\nWEBTEXT and REAL NEWS experts transfer much bet-\nter, confirming the heterogeneity of those domains.\nDENSE (1.3B params per GPU) 29.4\nDEM IX (cached; 1.3B params per GPU) 21.8\nGPT-3 Da-Vinci 20.5\nTable 12: Zero-shot perplexity on the Penn TreeBank\nCorpus (Marcus et al., 1993), comparing our largest\nDENSE and DEM IX baselines with GPT-3 Da-Vinci,\nthe largest Brown et al. (2020). Our largest DEM IX LM\ngains a large boost in performance overDENSE baseline,\napproaching the performance of GPT-3 Da-Vinci with\na fraction of the compute budget.\nA.8 Expert Performance Ratios\nWe display a heatmap of expert performance ratios,\nusing the largest DEM IX LM (1.3B parameters\nper GPU) in Figure 7. These results suggest that\nexperts specialize to their domain, and that lever-\naging the outputs of multiple experts (especially\nthose specialized to hetereogeneous domains) at\ntest time would lead to better language modeling\nperformance.\nA.9 Per-Domain Results\nWe display the rest of the per-domain test re-\nsults in the spreadsheets at the following link:\nhttps://docs.google.com/spreadsheets/d/\n1yNMZGSPAvhTi3JttLamiCULaOIGTJ4QGEOajO3b5kt8/\nedit?usp=sharing\nA.10 Domain Posterior Calculations\nWe track calculated domain posteriors over se-\nquences of development data in Figure 8 (training\ndomains) and Figure 9 (novel domains). The do-\nmain posteriors are noisier for earlier sequences,\nParameters\n125M 350M 760M 1.3B\nDENSE -\nDAPT\nT +70.1% +21.4% +16.7% +20.6%\nN –55.1% –46.6% –38.3% –44.4%\nTable 13: Average change in perplexity in training (T)\nand novel (N) domains after DENSE -DAPT . Negative\nvalues indicate better performance relative to the origi-\nnal DENSE LM. While average perplexity in the novel\ndomains decreases more for DENSE -DAPT , this comes\nat the cost of a significant deterioration in performance\nin training domains.\nstabilizing usually after around 50 sequences. For\nall experiments, we conservatively use 100 se-\nquences of data to compute the domain posterior,\nthough one may be able to accurately calcuate the\ndomain posterior for some domains with less data.\nA.11 GPT-3 Da-Vinci Comparison\nWe conduct an experiment comparing our largest\nDEM IX LM with GPT-3 Da-Vinci from Brown\net al. (2020), using the zero-shot language model-\ning evaluation they report: Penn TreeBank (Marcus\net al. 1993; Table 12). We observe that the largest\nDEM IX LM achieves competitive results with the\nGPT-3 Da-Vinci result with a fraction of the com-\nputation, and gives large performance boosts on\nthis benchmark over our other DENSE baselines.\nThese results further suggest the importance of do-\nmain modularity as a mechanism to improve gener-\nalization performance, in addition to model scaling.\nA.12 Perplexity Changes after DENSE -DAPT\nIn Table 13, we display the average perplexity\nchange after performing DENSE -DAPT on a new\ndomain. We observe that across all model sizes,\nDENSE -DAPT improves performance in the novel\ndomain, at the cost of a large performance hit in\nthe training domains.\n5573\n0.00\n0.25\n0.50\n0.75\n1.00P(D | X)\nMed\n RealNews\n Reddit\n OpenWebText\n0 25 50 75 100\nnumber of blocks\n0.00\n0.25\n0.50\n0.75\n1.00P(D | X)\nReviews\n0 25 50 75 100\nnumber of blocks\nCS\n0 25 50 75 100\nnumber of blocks\nLegal\n0 25 50 75 100\nnumber of blocks\n1B\n1b openwebtext realnews reviews cs legal med reddit\nFigure 8: Calculated domain posteriors for 8 training domains.\n0.00\n0.25\n0.50\n0.75\n1.00P(D | X)\nCORD-19\n Github\n Gutenberg\n Breaking News\n0 25 50 75 100\nnumber of blocks\n0.00\n0.25\n0.50\n0.75\n1.00P(D | X)\nLegal Contracts\n0 25 50 75 100\nnumber of blocks\nACL Papers\n0 25 50 75 100\nnumber of blocks\nTweets\n0 25 50 75 100\nnumber of blocks\nYelp Reviews\n1b openwebtext realnews reviews cs legal med reddit\nFigure 9: Calculated domain posteriors for 8 novel domains.\n5574\nComputing Infrastructure 32 V olta 32GB GPUs\nHyperparameter Assignment\narchitecture GPT-3 small\ntokens per sample 1024\nbatch size 2\nnumber of workers 2\nlearning rate [5e–4, 3e–4, 1e–4]\nclip norm 0.1\ngradient acculumation steps 8\nnumber of steps 300,000\nsave interval updates 6,000\nvalidation interval 3,000\nnumber of warmup steps 24,000\nlearning rate scheduler polynomial decay\nlearning rate optimizer Adam\nAdam beta weights (0.9, 0.95)\nAdam epsilon 10e-8\nweight decay 0.1\nTable 14: Hyperparameters for pretraining the LM with 125M parameters per GPU. All hyperparameters are the\nsame for DEM IX and DENSE training.\nComputing Infrastructure 64 V olta 32GB GPUs\nHyperparameter Assignment\narchitecture GPT-3 medium\ntokens per sample 1024\nbatch size 2\nnumber of workers 2\nlearning rate [5e–4, 3e–4, 1e–4]\nclip norm 0.1\ngradient acculumation steps 8\nnumber of steps 120,000\nsave interval updates 3,000\nvalidation interval 2,000\nnumber of warmup steps 9,600\nlearning rate scheduler polynomial decay\nlearning rate optimizer Adam\nAdam beta weights (0.9, 0.95)\nAdam epsilon 10e-8\nweight decay 0.1\nTable 15: Hyperparameters for pretraining the LM with 350M parameters per GPU. All hyperparameters are the\nsame for DEM IX and DENSE training.\n5575\nComputing Infrastructure 128 V olta 32GB GPUs\nHyperparameter Assignment\narchitecture GPT-3 large\ntokens per sample 1024\nbatch size 2\nnumber of workers 2\nlearning rate [5e–4, 3e–4, 1e–4]\nclip norm 0.1\ngradient acculumation steps 8\nnumber of steps 65,000\nsave interval updates 2,000\nvalidation interval 1,000\nnumber of warmup steps 5,200\nlearning rate scheduler polynomial decay\nlearning rate optimizer Adam\nAdam beta weights (0.9, 0.95)\nAdam epsilon 10e-8\nweight decay 0.1\nTable 16: Hyperparameters for pretraining the LM with 760M parameters per GPU. All hyperparameters are the\nsame for DEM IX and DENSE training.\nComputing Infrastructure 128 V olta 32GB GPUs\nHyperparameter Assignment\narchitecture GPT-3 XL\ntokens per sample 1024\nbatch size 2\nnumber of workers 2\nlearning rate [5e–4, 3e–4, 1e–4]\nclip norm 0.1\ngradient acculumation steps 8\nnumber of steps 50000\nsave interval updates 2,000\nvalidation interval 500\nnumber of warmup steps 4000\nlearning rate scheduler polynomial decay\nlearning rate optimizer Adam\nAdam beta weights (0.9, 0.95)\nAdam epsilon 10e-8\nweight decay 0.1\nTable 17: Hyperparameters for pretraining the LM with 1.3B parameters per GPU. All hyperparameters are the\nsame for DEM IX and DENSE training.\n5576",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6812891960144043
    },
    {
      "name": "Modular design",
      "score": 0.6249171495437622
    },
    {
      "name": "Computational linguistics",
      "score": 0.5390782356262207
    },
    {
      "name": "Programming language",
      "score": 0.4903569221496582
    },
    {
      "name": "Linguistics",
      "score": 0.45681819319725037
    },
    {
      "name": "Modeling language",
      "score": 0.4399670362472534
    },
    {
      "name": "Natural language processing",
      "score": 0.421923965215683
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3698832392692566
    },
    {
      "name": "Cognitive science",
      "score": 0.32403838634490967
    },
    {
      "name": "Philosophy",
      "score": 0.17571991682052612
    },
    {
      "name": "Psychology",
      "score": 0.10620194673538208
    },
    {
      "name": "Software",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ]
}