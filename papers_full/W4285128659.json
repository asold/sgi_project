{
  "title": "Developmental Negation Processing in Transformer Language Models",
  "url": "https://openalex.org/W4285128659",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4320548982",
      "name": "Antonio Laverghetta Jr.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A343778178",
      "name": "John Licato",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2038721957",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2395519003",
    "https://openalex.org/W3213014097",
    "https://openalex.org/W3186095502",
    "https://openalex.org/W3213468647",
    "https://openalex.org/W3098613713",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2346220646",
    "https://openalex.org/W592361793",
    "https://openalex.org/W2139021992",
    "https://openalex.org/W3099843385",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2402909549",
    "https://openalex.org/W3111239699",
    "https://openalex.org/W2801575510",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W343411360",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W1992119553",
    "https://openalex.org/W3127444392",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2937780504",
    "https://openalex.org/W2953316918",
    "https://openalex.org/W3034255912",
    "https://openalex.org/W3183369009",
    "https://openalex.org/W590123328",
    "https://openalex.org/W1586623992",
    "https://openalex.org/W2168880251",
    "https://openalex.org/W2941156205",
    "https://openalex.org/W2525450212",
    "https://openalex.org/W2109553965"
  ],
  "abstract": "Reasoning using negation is known to be difficult for transformer-based language models. While previous studies have used the tools of psycholinguistics to probe a transformer’s ability to reason over negation, none have focused on the types of negation studied in developmental psychology. We explore how well transformers can process such categories of negation, by framing the problem as a natural language inference (NLI) task. We curate a set of diagnostic questions for our target categories from popular NLI datasets and evaluate how well a suite of models reason over them. We find that models perform consistently better only on certain categories, suggesting clear distinctions in how they are processed.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 545 - 551\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nDevelopmental Negation Processing in Transformer Language Models\nAntonio Laverghetta Jr.and John Licato\nAdvancing Machine and Human Reasoning (AMHR) Lab\nDepartment of Computer Science and Engineering\nUniversity of South Florida\nTampa, FL, USA\n{alaverghett,licato}@usf.edu\nAbstract\nReasoning using negation is known to be dif-\nﬁcult for transformer-based language models.\nWhile previous studies have used the tools of\npsycholinguistics to probe a transformer’s abil-\nity to reason over negation, none have focused\non the types of negation studied in develop-\nmental psychology. We explore how well trans-\nformers can process such categories of nega-\ntion, by framing the problem as a natural lan-\nguage inference (NLI) task. We curate a set of\ndiagnostic questions for our target categories\nfrom popular NLI datasets and evaluate how\nwell a suite of models reason over them. We\nﬁnd that models perform consistently better\nonly on certain categories, suggesting clear dis-\ntinctions in how they are processed.1\n1 Introduction\nNegation is an important construct in language for\nreasoning over the truth of propositions (Heine-\nmann, 2015), garnering interest from philosophy\n(Horn, 1989), psycholinguistics (Zwaan, 2012),\nand natural language processing (NLP) (Morante\nand Blanco, 2020). While transformer language\nmodels (TLMs) (Vaswani et al., 2017) have\nachieved impressive performance across many\nNLP tasks, a great deal of recent work has found\nthat they do not process negation well, and often\nmake predictions that would be trivially false in\nthe eyes of a human (Rogers et al., 2020; Ettinger,\n2020; Laverghetta Jr. et al., 2021).\nIn developmental psychology, there has likewise\nbeen a great deal of interest in how a child’s abil-\nity to comprehend negation emerges in the early\nyears of life (Nordmeyer and Frank, 2013, 2018b;\nReuter et al., 2018; Grigoroglou et al., 2019). Un-\nlike in NLP, which typically treats negation as rep-\nresenting a single monolithic competency, this re-\nsearch has long understood that there are many\n1Code and data to reproduce our experiments can be found\non Github: https://github.com/Advancing-Machine-Human-\nReasoning-Lab/negation-processing-ACL-2022\nkinds of negation used in everyday interactions\n(Bloom, 1970; Pea, 1982). This ranges from using\nnegation to express a child’s rejection of something\nto clarifying a child’s knowledge. These “devel-\nopmental” categories of negation do not emerge\nsimultaneously; children tend to start using certain\nkinds before others (Nordmeyer and Frank, 2018a).\nGiven that these categories represent some of\nthe earliest uses of negation among humans, un-\nderstanding how well TLMs can master them is\nimportant for building more human-like models of\nlanguage processing. Understanding how well mod-\nels perform on different categories will indicate\nwhether they have mastery of some forms of nega-\ntion, while also helping to identify failure points.\nAnother interesting question is whether the proﬁ-\nciency of TLMs on these categories is at all related\nto competencies in human children (e.g., is the cat-\negory which models consistently perform the best\non the same that children most frequently employ?).\nHowever, to our knowledge, no prior work in NLP\nhas focused on how well models perform on the\nforms of negation of interest to developmental psy-\nchology.\nIn this short paper, we investigate how well a\nsuite of TLMs can process developmental nega-\ntion,2 by framing the problem as a natural lan-\nguage inference (NLI) task. We develop a rule-\nbased parser to extract problems from existing NLI\ndatasets, and evaluate our models on each cate-\ngory, in order to determine (i) whether certain cat-\negories are more solvable by our models than oth-\ners, and (ii) what relationships exist among the\ncategories. We ﬁnd that models can consistently\nachieve stronger performance only on certain cat-\negories, and that training on combinations or se-\nquences of these categories does not substantially\nimprove a model’s downstream performance.\n2By which we mean the forms of negation studied in de-\nvelopment psychology.\n545\n2 Related Work\nNegation is known to be frequently used in every-\nday conversation. While this includes its logical\nform, we primarily focus on negation’s psycholin-\nguistic forms, especially those that have been stud-\nied in the context of developmental psychology.\nNegation emerges early in child development, with\n‘no’ sometimes being a child’s ﬁrst word (Schnei-\nder et al., 2015), and even infants appear to under-\nstand forms of negation (Piaget, 1980; Hochmann\nand Toro, 2021). Preschool children use at least\nthree different kinds of negation (Bloom, 1970),\nbut possibly as many as nine (Choi, 1988). As\nnoted by Nordmeyer and Frank (2018a), one of\nthe ﬁrst categories children use is rejection, where\na child rejects an object or activity. This is later\nfollowed by existence, where a child might ex-\npress the lack of an object, and later still denial,\nwhich a child uses to deny the truth of a claim.\nLarger scale studies of child-directed speech have\nfound that truth-functional kinds of negation tend\nto emerge later (Liu and Jasbi, 2021), but individual\nchildren do vary in their speciﬁc order of acquisi-\ntion (Nordmeyer and Frank, 2018a). It is unknown\nwhether this ordering reﬂects any deeper depen-\ndencies among the different categories, or whether\nthe ordering is reﬂected in how artiﬁcial language\nmodels (LMs) learn negation.\nIn NLP, methods from psycholinguistics have\nbeen used to probe the reasoning capabilities of\nLMs. Results from some studies have indicated\nthat TLMs are not human-like in their processing\nof negation (Ettinger, 2020; Kassner and Schütze,\n2020). A similar line of work has used the NLI\ntask to probe a model’s ability to process negation\nand found that TLMs will often alter their predic-\ntions when negation is inserted or removed, even\nwhen the negation does not alter the entailment re-\nlationship (Hossain et al., 2020; Hartmann et al.,\n2021). As argued by Kruszewski et al. (2016), part\nof the challenge of modeling purely logical nega-\ntion is that a predicate often occurs in very similar\ncontexts regardless of whether it is being negated.\nThey argue that we should view negation as be-\ning a “graded similarity function”, and show that\ndistributional models can predict human plausibil-\nity judgments quite well, even in the presence of\nnegation. These works show that it is unclear how\nwell distributional models, especially TLMs, are\nactually processing negation. We contribute to this\nliterature from a new perspective, by studying how\nCategory # Train # Test\nPossession (PO) 1053 520\nExistence (EX) 5528 2723\nLabeling (L) 2241 1104\nProhibition (PR) 814 400\nInability (I) 1384 682\nEpistemic (EP ) 1903 936\nRejection (R) 1737 856\nTable 1: Summary statistics for the curated dataset.\nwell models can reason over forms of negation\ncommon in developmental psychology.\n3 The Developmental Negation Corpus\nWe use the NLI task to study the negation reasoning\ncapabilities of our models. NLI problems consist\nof two sentences: a premise ( p) and hypothesis\n(h), and solving such a problem involves assessing\nwhether p textually entails h. The generic structure\nof the NLI task makes it suitable for studying a va-\nriety of underlying reasoning skills, including nega-\ntion. We speciﬁcally use the SNLI (Bowman et al.,\n2015) and MNLI (Williams et al., 2018) datasets.\nTo automatically identify questions that contain\na speciﬁc kind of negation, we rely on the work\nby Liu and Jasbi (2021) which studied how fre-\nquently different kinds of developmental negation\noccur in child-directed speech, using the data from\nthe CHILDES corpus (MacWhinney, 2014). To do\nthis, they created a simple rule-based parser to au-\ntomatically tag each sentence in CHILDES with\nthe type of negation it contained (if any). We re-\nimplement their parser, in some cases tweaking\nthe rules slightly to better suit the structure of the\nNLI task. For each example across all the splits of\nboth datasets, we ﬁrst obtain a dependency parse\nof both p and h using the diaparser package (Wang\net al., 2019), and check if either contains an explicit\nnegation marker (“no”, “not”, or “n’t”). If one span\ncontains negation, we check if the syntactic struc-\nture obeys the rules of any of our categories. If the\nspan falls into a category, we mark it as belonging\nto that category. We use these questions as the diag-\nnostic set for our experiments, splitting out 1/3 of\nthe questions in each category as a diagnostic test\nset, and leaving the remainder as a diagnostic train\nset (and we will refer to them as such). We place\nthe remaining NLI questions containing no nega-\ntion in a separate NLItrain set, giving us about\n730,000 examples we use to ﬁnetune our models\non the NLI task. We split out 9,000 questions from\nthis train set at random to use as a NLIdev set, bal-\n546\nCategory Premise Hypothesis\nPO yeah you probably don’t have the right temperatures... You probably have ideal temperatures...\nEX This analysis pooled estimates... The analysis proves that there is no link...\nL Not orders, no. It is not orders.\nPR Two people are sitting against a building near shopping carts. Run that way but don’t run into the...\nI His manner was unfortunate, I observed thoughtfully. I could not pick out what kind of manner he...\nEP yeah i don’t know why I know why\nR I lowered my voice... I didn’t want to be overheard.\nTable 2: NLI examples extracted from each category, long examples have been trimmed to ﬁt on one line.\nanced for each label. In the following, we describe\nthe precise rules used to determine which category\na negated example should be assigned to:\nPossession (PO ) We require that the lemma of\nthe root be have, has, or had, and that the root is\ndirectly modiﬁed by both the negation and the verb\ndo.\nExistence (EX) We require that there occur in\nthe text and precede the negative marker and that\nthe negative marker directly modiﬁes a noun phrase,\ndeterminer, or an adverb.\nLabeling (L) We require that the sentence be-\ngin with either That or It, and that the root of the\nsentence is a noun which is modiﬁed by is or ’s.\nProhibition (PR) We require that the sentence\nnot contain a subject and that the negation is im-\nmediately preceded by do. To not conﬂate this cat-\negory with others, we ﬁlter out cases where the\nroot contains one of the explicit markers of another\ncategory (e.g., like or want in the case of rejection).\nInability (I) We require that the negation di-\nrectly modify the root of the sentence, and that\nthe word immediately before the negation is either\ncan or could (e.g., can not do). Prior literature has\ntypically viewed inability from an egocentric per-\nspective. However, we found that allowing only the\nﬁrst person severely restricted the number of ex-\namples extracted, and therefore chose to also allow\nthe second and third person.\nEpistemic (EP ) We require that the root be re-\nmember, know, or think, and that the root be directly\nmodiﬁed by the verb do.\nRejection (R) We require that the lemma of the\nroot word be either like or want, and that the root\nis modiﬁed by the negative marker.\nAfter performing extraction, categories L and\nPR contained fewer than 1000 examples, which\nwe deemed was insufﬁcient to split into separate\ntrain and test sets. To address this, we developed\na simple data augmentation approach that utilized\nthe Wordnet database (Miller, 1998). From the de-\npendency parse of both p and h, we check if the\nroot of either parse occurs in both spans. If it does,\nwe obtain all synonyms of the word in Wordnet and\nreplace the root in both spans with the synonym\n(doing this for every synonym). We found this sim-\nple approach increased the number of examples for\nboth L and PR to at least 1500. Note that we per-\nformed no augmentation for the other categories, as\nour parser extracted at least 1500 examples for all\nother cases. Table 1 shows statistics for the dataset\nafter augmentation.\nTable 2 shows extracted examples, along with\ntheir category assignment. We generally found that\nthe extracted examples matched up with the pro-\ntotypical category quite well, although in some\ncases their semantics differed slightly. For instance,\nconsider a PR example with p = don’t miss hav-\ning a ﬂick through the albums and h = The pic-\ntures of old Madeira show a more interesting city\nthan now, which is an MNLI example originally\nextracted from a travel guide. Although this tech-\nnically counts as PR, it does not have quite the\nsame semantics as an actual command. Unfortu-\nnately, these ambiguities are not easily resolved,\ngiven that negation takes on many forms and may\noccur at any location within a sentence. We, there-\nfore, opted to focus on forms of negation that can\nbe easily extracted, and leave improvements to our\ndataset creation protocol for future work.\n4 Experiments\nUsing the curated dataset, we performed a series of\nexploratory experiments to help us understand how\nwell TLMs process each of the negation categories.\nWe use BERT (Devlin et al., 2019), and RoBERTa\n(Liu et al., 2019), two popular transformer LMs\nthat have demonstrated impressive results on a\nvariety of language understanding tasks. We also\nexamine MiniBERTa (Warstadt et al., 2020) and\nBabyBERTa (Huebner et al., 2021), which are both\n547\nbased on the RoBERTa architecture but were pre-\ntrained on a much smaller number of tokens (10\nmillion and 5 million respectively), which is more\nrealistic to the amount of language a child is ex-\nposed to in the ﬁrst few years of life. We use the\nHuggingface implementation of all models (Wolf\net al., 2020), and use both the base and large ver-\nsion of BERT and RoBERTa, which differ only in\nthe number of trainable parameters.\nExperiment 1: We began by investigating\nwhether TLMs would master certain negation cate-\ngories sooner than others over the course of train-\ning. We train our models on NLItrain for 10\nepochs, using a learning rate of 1e − 5, a weight\ndecay of 0.01, a batch size of 16, and a maximum\nsequence length 175.3 We selected these hyperpa-\nrameters to be similar to those which were previ-\nously reported to yield strong results when train-\ning on NLI datasets (Laverghetta Jr. et al., 2021).\nWe additionally evaluated the models onNLIdev,\nand found that they all achieved a Matthews Cor-\nrelation of at least 0.6 (Matthews, 1975), and thus\nconcluded that these hyperparameters were suit-\nable. For every end of epoch checkpoint across all\nmodels, we obtained evaluation results on each di-\nagnostic test set. Importantly, the models are not\nﬁnetuned on any negated NLI questions for this ex-\nperiment, meaning that all knowledge of negation\ncomes from pre-training. Results are shown in Fig-\nure 1. We see that the categories have similar rank-\nings in terms of accuracy. For example, L and PO\nare among the top two best-performing categories,\nwhile R is generally one of the worst-performing\nones, indicating clear distinctions in how LMs pro-\ncess the categories. BabyBERTa, unlike other mod-\nels, also shows stronger similarities to how children\nacquire negation. For instance, while R is thought\nto be one of the ﬁrst categories children acquire,\nBabyBERTa is the only model where R is one of\nthe highest-ranking categories in terms of accuracy.\nExperiment 2: One might expect that children\ndevelop a more abstract understanding of negation\nas they are exposed to different categories. This\nwas suggested by Pea (1978) who argued that more\nabstract forms of negation develop from less ab-\nstract ones, suggesting that mastering one form of\nnegation can lead to positive transfer on others. In\nExperiment 2, we examined how much positive\n3We set the maximum sequence length for BabyBERTa to\n128, which is the longest that the model supports.\nFigure 1: Performance of models ﬁnetuned on\nNLItrain for each diagnostic test set. We refer to\nMiniBERTa using its Huggingface model ID ( roberta-\nbase-10M-2).\ntransfer could be obtained from training on one\nof the negation categories, and then testing on the\nothers. We adopt a similar methodology to Pruk-\nsachatkun et al. (2020), who explored the condi-\ntions that affect intermediate task transfer learning.\nUsing the models trained in Experiment 1, we fur-\nther ﬁnetune these models for 25 epochs on each\ndiagnostic train set separately. We then evaluate the\nﬁnetuned models on each diagnostic test set, which\nallows us to examine all possible pairwise interac-\ntions among categories. Figure 2 shows the results\nfor all combinations of diagnostic categories for\ntraining and testing. Surprisingly, we ﬁnd that posi-\ntive transfer generally only occurs when a model is\ntrained on the same category it is being tested on.\nTraining on a different category has little to no ef-\nfect on the target category. BabyBERTa is again an\nexception, as we do see positive transfer for most\npairs, suggesting the model is generalizing across\ncategories\nExperiment 3: Building on Experiment 2, we\nexamined how the performance of our models is\naffected when trained on all diagnostic categories\nin sequence. Assuming that no positive transfer\nexists among the categories, we would expect to\nsee a model’s performance on a particular cate-\ngory improve only after it has been trained on that\nsame category, and even training on multiple other\ncategories should not substantially improve perfor-\n548\nFigure 2: Accuracy of each model on every diagnostic\ntest set, after being ﬁnetuned on every diagnostic train\nset. Plots are color-coded based on the target category.\nmance on the target. Using the models from Ex-\nperiment 1, we ﬁnetune each model for 10 epochs\non every diagnostic train set, using the sequence of\ncategories shown in the x-axis of Figure 3. Addi-\ntionally, we under-sample all diagnostic train sets to\nhave the same number of questions as PR, so that\nall categories contribute the same amount of data.\nFigure 3 shows the results. For some categories,\nsuch as L and PR, we see the expected trend. The\nlargest accuracy gain for these categories occurs\nwhenever the model is trained on the same cate-\ngory it is being tested on, and performance drops\nslightly after being trained on others. However, for\ncategories such as R, the best performance gain\nis not always after being trained on the same cat-\negory. We sometimes see the model continue to\nimprove on R after being trained on R, and in\nsome cases, training on R causes performance on\nR to decrease.\n5 Discussion and Conclusion\nIn this paper, we have explored how well trans-\nformers process categories of developmental nega-\ntion. We ﬁnd that performance rankings across cat-\negories are generally consistent, but that the cate-\ngories seem to test for orthogonal skills in the ma-\njority of LMs. In BabyBERTa, we see signiﬁcant\nsimilarities with the order of negation acquisition\nin children. Two of the best performing categories\nare R and L, while two of the worst are EX and\nFigure 3: Results from Experiment 3. The x-axis shows\nthe sequence of categories on which all models were\ntrained, while the y-axis shows the accuracy obtained\nafter being trained on a category.\nPR, which aligns quite well to the order observed\nby Liu and Jasbi (2021). It thus seems that TLMs\ndo at least partially reﬂect the order of negation\nacquisition observed in children, although more\nexperiments would be needed to understand the\nextent of this correlation. That we found category\nrankings to generally be consistent across LMs may\nhave interesting implications, and understanding\nwhy LMs struggle with certain categories may help\nto improve the ability of LMs to process negation.\nFuture work can build on these experiments in\nseveral ways. In Experiments 2 and 3, we modeled\ninteractions among the negation categories in either\na pairwise or sequential fashion, which is unlikely\nto reﬂect how children are exposed to negation.\nMore experiments, mixing all of the categories at\nonce in various proportions, might yield a more\nrealistic model of cognitive development. Our ap-\nproach also requires that each category ﬁts into a\nspeciﬁc structure, which limits the amount of exam-\nples that can be extracted. Future work will need\nto expand our ruleset to include more variations\nin the negated utterances covered. Finally, while\nwe primarily focus on ﬁnetuning, pre-training is\nlikely to impact the proﬁciency of our models on\nthe categories as well. Future work should precisely\ncontrol the prevalence of each category in the pre-\ntraining corpus, to observe what effect this has on\ndownstream performance.\n549\nReferences\nLois Bloom. 1970. Language development: Form and\nfunction in emerging grammars. mit research mono-\ngraph, no 59.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nSoonja Choi. 1988. The semantic development of nega-\ntion: a cross-linguistic longitudinal study.Journal of\nchild language, 15(3):517–531.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nMyrto Grigoroglou, Sharon Chan, and Patricia A\nGanea. 2019. Toddlers’ understanding and use of\nverbal negation in inferential reasoning search tasks.\nJournal of experimental child psychology, 183:222–\n241.\nMareike Hartmann, Miryam de Lhoneux, Daniel Her-\nshcovich, Yova Kementchedjhieva, Lukas Nielsen,\nChen Qiu, and Anders Søgaard. 2021. A multi-\nlingual benchmark for probing negation-awareness\nwith minimal pairs. In Proceedings of the 25th Con-\nference on Computational Natural Language Learn-\ning, pages 244–257, Online. Association for Compu-\ntational Linguistics.\nF. H. Heinemann. 2015. VIII.—The Meaning of\nNegation. Proceedings of the Aristotelian Society ,\n44(1):127–152.\nJean-Rémy Hochmann and Juan M. Toro. 2021. Neg-\native mental representations in infancy. Cognition,\n213:104599. Special Issue in Honour of Jacques\nMehler, Cognition’s founding editor.\nLaurence Horn. 1989. A Natural History of Negation .\nUniversity of Chicago Press.\nMd Mosharaf Hossain, Venelin Kovatchev, Pranoy\nDutta, Tiffany Kao, Elizabeth Wei, and Eduardo\nBlanco. 2020. An analysis of natural language in-\nference benchmarks through the lens of negation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9106–9118, Online. Association for Computa-\ntional Linguistics.\nPhilip A. Huebner, Elior Sulem, Fisher Cynthia, and\nDan Roth. 2021. BabyBERTa: Learning More\nGrammar With Small-Scale Child-Directed Lan-\nguage. In Proceedings of the 25th Conference on\nComputational Natural Language Learning , pages\n624–646, Online. Association for Computational\nLinguistics.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot ﬂy. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. As-\nsociation for Computational Linguistics.\nGermán Kruszewski, Denis Paperno, Raffaella\nBernardi, and Marco Baroni. 2016. There is no\nlogical negation here, but there are alternatives:\nModeling conversational negation with distri-\nbutional semantics. Computational Linguistics ,\n42(4):637–660.\nAntonio Laverghetta Jr., Animesh Nighojkar, Jamshid-\nbek Mirzakhalov, and John Licato. 2021. Can trans-\nformer language models predict psychometric prop-\nerties? In Proceedings of *SEM 2021: The Tenth\nJoint Conference on Lexical and Computational Se-\nmantics, pages 12–25, Online. Association for Com-\nputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZoey Liu and Masoud Jasbi. 2021. English negative\nconstructions and communicative functions in child\nlanguage. In Proceedings of the Annual Meeting of\nthe Cognitive Science Society, volume 43.\nBrian MacWhinney. 2014. The CHILDES project:\nTools for analyzing talk, Volume II: The database .\nPsychology Press.\nBrian W Matthews. 1975. Comparison of the pre-\ndicted and observed secondary structure of t4 phage\nlysozyme. Biochimica et Biophysica Acta (BBA)-\nProtein Structure, 405(2):442–451.\nGeorge A Miller. 1998. WordNet: An electronic lexical\ndatabase. MIT press.\nRoser Morante and Eduardo Blanco. 2020. Recent ad-\nvances in processing negation. Natural Language\nEngineering, pages 1–10.\nAnn Nordmeyer and Michael Frank. 2013. Measur-\ning the comprehension of negation in 2-to 4-year-old\nchildren. In Proceedings of the Annual Meeting of\nthe Cognitive Science Society, volume 35.\n550\nAnn Nordmeyer and Michael C Frank. 2018a. Individ-\nual variation in children’s early production of nega-\ntion. In CogSci.\nAnn E Nordmeyer and Michael C Frank. 2018b. Early\nunderstanding of pragmatic principles in children’s\njudgments of negative sentences. Language Learn-\ning and Development, 14(4):262–278.\nRoy D Pea. 1978. The development of negation in early\nchild language. Ph.D. thesis, University of Oxford.\nRoy D Pea. 1982. Origins of verbal logic: Spontaneous\ndenials by two-and three-year olds. Journal of child\nlanguage, 9(3):597–626.\nJean Piaget. 1980. Experiments in Contradiction. Uni-\nversity of Chicago Press.\nYada Pruksachatkun, Jason Phang, Haokun Liu,\nPhu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe\nPang, Clara Vania, Katharina Kann, and Samuel R.\nBowman. 2020. Intermediate-task transfer learning\nwith pretrained language models: When and why\ndoes it work? In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5231–5247, Online. Association for\nComputational Linguistics.\nTracy Reuter, Roman Feiman, and Jesse Snedeker.\n2018. Getting to no: Pragmatic and semantic factors\nin two-and three-year-olds’ understanding of nega-\ntion. Child development, 89(4):e364–e381.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nRose M Schneider, Daniel Yurovsky, and Mike Frank.\n2015. Large-scale investigations of variability in\nchildren’s ﬁrst words. In CogSci, pages 2110–2115.\nCiteseer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nXinyu Wang, Jingxian Huang, and Kewei Tu. 2019.\nSecond-order semantic dependency parsing with\nend-to-end neural networks. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4609–4618, Florence,\nItaly. Association for Computational Linguistics.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun\nLiu, and Samuel R. Bowman. 2020. Learning\nWhich Features Matter: RoBERTa Acquires a Prefer-\nence for Linguistic Generalizations (Eventually). In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 217–235, Online. Association for Computa-\ntional Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nRolf A Zwaan. 2012. The experiential view of lan-\nguage comprehension: How is negation represented.\nHigher level language processes in the brain: Infer-\nence and comprehension processes, page 255.\n551",
  "topic": "Negation",
  "concepts": [
    {
      "name": "Negation",
      "score": 0.8127371072769165
    },
    {
      "name": "Computer science",
      "score": 0.6565356850624084
    },
    {
      "name": "Transformer",
      "score": 0.5871820449829102
    },
    {
      "name": "Natural language processing",
      "score": 0.5522779822349548
    },
    {
      "name": "Psycholinguistics",
      "score": 0.5325075387954712
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5158316493034363
    },
    {
      "name": "Inference",
      "score": 0.4344305992126465
    },
    {
      "name": "Linguistics",
      "score": 0.3879142701625824
    },
    {
      "name": "Cognitive science",
      "score": 0.32682448625564575
    },
    {
      "name": "Psychology",
      "score": 0.2438482940196991
    },
    {
      "name": "Cognition",
      "score": 0.23688599467277527
    },
    {
      "name": "Programming language",
      "score": 0.20939025282859802
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2613432",
      "name": "University of South Florida",
      "country": "US"
    }
  ],
  "cited_by": 4
}