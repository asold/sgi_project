{
  "title": "Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight",
  "url": "https://openalex.org/W3169745417",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2104888217",
      "name": "Qi Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3089424620",
      "name": "Zejia Fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114787941",
      "name": "Qi Dai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106065951",
      "name": "Lei Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2246491098",
      "name": "Ming-Ming Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118204509",
      "name": "Jiaying Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124874746",
      "name": "Jingdong Wang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3159337199",
    "https://openalex.org/W3164208409",
    "https://openalex.org/W3158846111",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W2778955544",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2986324132",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W3172624760",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2989676862",
    "https://openalex.org/W2965853874",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3162276117",
    "https://openalex.org/W3138796575",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W3145444543",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W3165150763",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3163465952",
    "https://openalex.org/W2963984455",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2414711238",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2964169985",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3165088525",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W3171398643",
    "https://openalex.org/W2979861340",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2963993763",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W3047517563",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W3177349073",
    "https://openalex.org/W3034363135",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2964258058",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3127856697"
  ],
  "abstract": "Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as weight computation. Sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. Weight sharing: the connection weights for one position are shared across channels or within each group of channels. Dynamic weight: the connection weights are dynamically predicted according to each image instance. We point out that local attention resembles depth-wise convolution and its dynamic version in sparse connectivity. The main difference lies in weight sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions. We empirically observe that the models based on depth-wise convolution and the dynamic variant with lower computation complexity perform on-par with or sometimes slightly better than Swin Transformer, an instance of Local Vision Transformer, for ImageNet classification, COCO object detection and ADE semantic segmentation. These observations suggest that Local Vision Transformer takes advantage of two regularization forms and dynamic weight to increase the network capacity.",
  "full_text": "Published as a conference paper at ICLR 2022\nON THE CONNECTION BETWEEN LOCAL ATTENTION\nAND DYNAMIC DEPTH -WISE CONVOLUTION\nQi Han1∗ Zejia Fan2∗ Qi Dai3† Lei Sun3 Ming-Ming Cheng1 Jiaying Liu2\nJingdong Wang4†\nTKLNDST, CS, Nankai Univerisy1, Peking University2, Microsoft Research Asia3, Baidu Inc.4\nABSTRACT\nVision Transformer (ViT) attains state-of-the-art performance in visual recognition,\nand the variant, Local Vision Transformer, makes further improvements. The major\ncomponent in Local Vision Transformer, local attention, performs the attention\nseparately over small local windows. We rephrase local attention as a channel-wise\nlocally-connected layer and analyze it from two network regularization manners,\nsparse connectivity and weight sharing, as well as dynamic weight computation.\nWe point out that local attention resembles depth-wise convolution and its dynamic\nvariants in sparse connectivity: there is no connection across channels, and each\nposition is connected to the positions within a small local window. The main\ndifferences lie in (i) weight sharing - depth-wise convolution shares connection\nweights (kernel weights) across spatial positions and attention shares the connection\nweights across channels, and (ii) dynamic weight computation manners - local\nattention is based on dot-products between pairwise positions in the local window,\nand dynamic convolution is based on linear projections conducted on the center\nrepresentation or the globally pooled representation.\nThe connection between local attention and dynamic depth-wise convolution is\nempirically veriﬁed by the ablation study about weight sharing and dynamic weight\ncomputation in Local Vision Transformer and (dynamic) depth-wise convolu-\ntion based network, namely (dynamic) DWNet. We empirically observe that\nthe depth-wise convolution based DWNet and its dynamic variants with lower\ncomputation complexity perform on-par with or slightly better than Swin Trans-\nformer, an instance of Local Vision Transformer, for ImageNet classiﬁcation,\nCOCO object detection and ADE semantic segmentation. Code is available at\nhttps://github.com/Atten4Vis/DemystifyLocalViT.\n1 I NTRODUCTION\nVision Transformer (Chu et al., 2021b; d’Ascoli et al., 2021; Dosovitskiy et al., 2021; Guo et al.,\n2021; Han et al., 2020; Khan et al., 2021; Touvron et al., 2020; Wang et al., 2021b; Wu et al., 2021;\nXu et al., 2021; Yuan et al., 2021b) has shown promising performance in ImageNet classiﬁcation.\nThe improved variants, Local Vision Transformer (Chu et al., 2021a; Liu et al., 2021b; Vaswani et al.,\n2021), adopt the local attention mechanism, which partitions the image space into a set of small\nwindows, and conducts the attention over the windows simultaneously. Local attention leads to great\nimprovement in memory and computation efﬁciency and makes the extension to downstream tasks\neasier and more efﬁcient, such as object detection and semantic segmentation.\nWe exploit the network regularization schemes (Goodfellow et al., 2016), sparse connectivity that\ncontrols the model complexity, and weight sharing that relaxes the requirement of increasing the\ntraining data scale and reduces the model parameters, as well as dynamic weight prediction that\nincreases the model capability, to study the local attention mechanism. We rephrase local attention\nas a channel-wise spatially-locally connected layer with dynamic connection weights. The main\nproperties are summarized as follows. (i) Sparse connectivity: there is no connection across channels,\nand each output position is only connected to the input positions within a local window. (ii) Weight\n∗Equal contribution\n†Corresponding author. wangjingdong@outlook.com\n1\narXiv:2106.04263v5  [cs.CV]  4 Aug 2022\nPublished as a conference paper at ICLR 2022\nsharing: the connection weights are shared across channels or within each group of channels. (iii)\nDynamic weight: the connection weights are dynamically predicted according to each image instance.\nWe connect local attention with depth-wise convolution (Chollet, 2017; Howard et al., 2017) and its\ndynamic variants that are also a channel-wise spatially-locally connected layer with optional dynamic\nconnection weights. They are similar in sparse connectivity. The main differences lie in (i) weight\nsharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions\nand attention shares the connection weights across channels, and (ii) dynamic weight computation\nmanners - local attention is based on dot-products between pairwise positions in the local window,\nand dynamic convolution is based on linear projections conducted on the center representation or the\nglobally pooled representation.\nWe further present the empirical veriﬁcation for the connection. We take the recently-developed Local\nVision Transformer, Swin Transformer (Liu et al., 2021b), as an example, and study the empirical\nperformance of local attention and (dynamic) depth-wise convolution in the same training settings as\nSwin Transformer. We replace the local attention layer with the (dynamic) depth-wise convolution\nlayer, keeping the overall structure unchanged. The constructed model is named DWNet.\nThe results show that the (dynamic) depth-wise convolution-based DWNet achieves comparable or\nslightly higher performance for ImageNet classiﬁcation and two downstream tasks, COCO object\ndetection and ADE semantic segmentation, and (dynamic) DWNet takes lower computation com-\nplexity. The ablation studies imply that weight sharing and dynamic weight improves the model\ncapability. Speciﬁcally, (i) for Swin Transformer, weight sharing across channels is beneﬁcial mainly\nfor reducing the parameter (attention weight) complexity, and the attention-based dynamic weight\nscheme is advantageous in learning instance-speciﬁc weights and block-translation equivalent repre-\nsentations; (ii) for depth-wise convolution, weight sharing across positions is beneﬁcial for reducing\nthe parameter complexity as well as learning translation equivalent representations, and the linear\nprojection-based dynamic weight scheme learns instance-speciﬁc weights.\n2 C ONNECTING LOCAL ATTENTION AND DEPTH -WISE CONVOLUTION\n2.1 L OCAL ATTENTION\nVision Transformer (Dosovitskiy et al., 2021) forms a network by repeating the attention layer and\nthe subsequent point-wise MLP (point-wise convolution). The local Vision Transformer, such as\nSwin Transformer (Liu et al., 2021b) and HaloNet (Vaswani et al., 2021), adopts the local attention\nlayer, which partitions the space into a set of small windows and performs the attention operation\nwithin each window simultaneously, to improve the memory and computation efﬁciency.\nThe local attention mechanism forms the keys and values in a window that the query lies in. The\nattention output for the query xi ∈RD at the position iis the aggregation of the corresponding\nvalues in the local window,{xi1,xi2,..., xiNk}, weighted by the corresponding attention weights\n{ai1,ai2,...,a iNk}1:\nyi =\n∑Nk\nj=1\naijxij, (1)\nwhere Nk = Kw ×Kh is the size of the local window. The attention weight aij is computed as the\nsoftmax normalization of the dot-product between the query xi and the key xij:\naij = e\n1√\nD x⊤\ni xij\nZi\nwhere Zi =\n∑Nk\nj=1\ne\n1√\nD x⊤\ni xij\n. (2)\nThe multi-head version partitions the D-dimensional query, key and value vectors into M subvectors\n(each with D\nM dimensions), and conducts the attention process M times, each over the corresponding\nsubvector. The whole output is the concatenation of M outputs, yi = [y⊤\ni1 y⊤\ni2 ... y⊤\niM ]⊤. The mth\noutput yim is calculated by\nyim =\n∑Nk\nj=1\naijmxijm, (3)\nwhere xijm is the mth value subvector and aijm is the attention weight computed from the mth head\nin the same way as Equation 2.\n1For presentation convenience, we ignore the linear projections conducted to the queries, the keys and the\nvalues. In vision applications, the value and the corresponding key are from the same feature possibly with\ndifferent linear projections, and we denote them using the same symbol xij.\n2\nPublished as a conference paper at ICLR 2022\n(a)\nSpatial\nChannel\n(b)\nSpatial\nChannel\n(c)\nSpatial\nChannel\n(d)\nSpatial\nChannel\n(e)\nSpatial\nChannel\nFigure 1: Illustration of connectivity for (a) convolution, (b) global attention and spatial mixing MLP, (c) local\nattention and depth-wise convolution, (d) point-wise MLP or 1 × 1 convolution, and (e) MLP (fully-connected\nlayer). In the spatial dimension, we use 1D to illustrate the local-connectivity pattern for clarity.\n2.2 S PARSE CONNECTIVITY , WEIGHT SHARING , AND DYNAMIC WEIGHT\nWe give a brief introduction of two regularization forms, sparse connectivity and weight sharing, and\ndynamic weight, and their beneﬁts. We will use the three forms to analyze local attention and connect\nit to dynamic depth-wise convolution.\nSparse connectivitymeans that there are no connections between some output neurons (variables)\nand some input neurons in a layer. It reduces the model complexity without decreasing the number of\nneurons, e.g., the size of the (hidden) representations.\nWeight sharingindicates that some connection weights are equal. It lowers the number of model\nparameters and increases the network size without requiring a corresponding increase in training\ndata (Goodfellow et al., 2016).\nDynamic weightrefers to learning specialized connection weights for each instance. It generally\naims to increase the model capacity. If regarding the learned connection weights as hidden variables,\ndynamic weight can be viewed as introducing second-order operations that increase the capability of\nthe network. The connection to Hopﬁeld networks is discussed in (Ramsauer et al., 2020).\n2.3 A NALYZING LOCAL ATTENTION\nWe show that local attention is a channel-wise spatially-locally connected layer with dynamic weight\ncomputation, and discuss its properties. Figure 1 (c) illustrates the connectivity pattern.\nThe aggregation processes (Equation 1 and Equation 3) for local attention can be rewritten equivalently\nin a form of element-wise multiplication:\nyi =\n∑Nk\nj=1\nwij ⊙xij, (4)\nwhere ⊙is the element-wise multiplication operator, and wij ∈RD is the weight vector formed\nfrom the attention weight aij or {aij1,aij2,...,a ijM }.\nSparse connectivity.The local attention layer is spatially sparse: each position is connected to the Nk\npositions in a small local window. There are also no connections across channels. The element-wise\nmultiplication in Equation 4 indicates that given the attention weights, each output element, e.g., yid\n(the ith position for the dth channel), is only dependent on the corresponding input elements from the\nsame channel in the window, {xi1d,xi2d,...,x iNkd}, and not related to other channels.\nWeight sharing.The weights are shared with respect to channels. In the single-head attention case, all\nthe elements {wij1,wij2,...,w ijD}in the weight vector wij are the same: wijd = aij, 1 ⩽ d⩽ D.\nIn the multi-head attention case, the weight vector wij is group-wise same: wij is partitioned to M\nsubvectors each corresponding to one attention head, {wij1,wij2,..., wijM }, and the elements in\neach subvector wijm are the same and are equal to the mth attention weight, aijm.\nDynamic weight.The weights, {wi1,wi2,..., wiNk}, are dynamically predicted from the query xi\nand the keys {xi1,xi2,..., xiNk}in the local window as shown in Equation 2. We rewrite it as:\n{wi1,wi2,..., wiNk}= f(xi; xi1,xi2,..., xiNk). (5)\nEach weight may obtain the information across all the channels in one head, and serves as a bridge to\ndeliver the across-channel information to each output channel.\nTranslation equivalence.Different from convolution which satisﬁes translation equivalence through\nsharing weights across positions, the equivalence to translation for local attention, depends if the\nkeys/values are changed, i.e., the attention weights are changed, when the feature map is translated.\n3\nPublished as a conference paper at ICLR 2022\nIn the case of sparsely-sampled window (for run-time efﬁciency), e.g., (Hu et al., 2019; Liu et al.,\n2021b; Ramachandran et al., 2019; Vaswani et al., 2021), local attention is equivalent to block-wise\ntranslation, i.e., the translation is a block or multiple blocks with the block size same as the window\nsize Kw ×Kh, and otherwise not equivalent (as keys/values are changed). In the case that the\nwindows are densely sampled (e.g., (Zhao et al., 2020)), local attention is equivalent to translation.\nSet representation. The keys/values for one query are collected as a set with the spatial-order\ninformation lost. This leads to that the spatial correspondence between the keys/values across\nwindows is not exploited. The order information loss is partially remedied by encoding the positions\nas embeddings (Dosovitskiy et al., 2021; Touvron et al., 2020), or learning a so-called relative\nposition embedding (e.g., (Liu et al., 2021b)) in which the spatial-order information is preserved as\nthe keys/values in a local window are collected as a vector.\n2.4 C ONNECTION TO DYNAMIC DEPTH -WISE CONVOLUTION\nDepth-wise convolution is a type of convolution that applies a single convolutional ﬁlter for each\nchannel: ¯Xd = Cd ⊗Xd, where Xd and ¯Xd are the dth input and output channel maps, Cd ∈RNk\nis the corresponding kernel weight, and ⊗is the convolution operation. It can be equivalently written\nin the form of element-wise multiplication for each position:\nyi =\n∑Nk\nj=1\nwoﬀset(i,j) ⊙xij. (6)\nHere, oﬀset(i,j) is the relative offset, from the 2D coordinate of the position jto the 2D coordinate\nof the central position i. The weights {woﬀset(i,j) ∈RD; j = 1 ,2,...,N k}are reshaped from\nC1,C2,..., CD. The Nk weight vectors are model parameters and shared for all the positions.\nWe also consider two dynamic variants of depth-wise convolution: homogeneous and inhomoge-\nneous2. The homogeneous dynamic variant predicts the convolution weights using linear projections\nfrom a feature vector that is obtained by globally-pooling the feature maps:\n{w1,w2,..., wNk}= g(GAP(x1,x2,..., xN )). (7)\nHere, {x1,x2,..., xN }are the image responses. GAP() is the global average pooling operator. g()\nis a function based on linear projection: a linear projection layer to reduce the channel dimension\nwith BN and ReLU, followed by another linear projection to generate the connection weights.\nThe inhomogeneous dynamic variant predicts the convolution weights separately for each position\nfrom the feature vector xi at the position (the center of the window):\n{wi1 ,wi2 ,..., wiNk\n}= g(xi). (8)\nThis means that the weights are not shared across positions. We share the weights across the channels\nin a way similar to the multi-head attention mechanism to reduce the complexity.\nWe describe the similarities and differences between (dynamic) depth-wise convolution and local\nattention. Figure 1 (c) illustrates the connectivity patterns and Table 1 shows the properties between\nlocal attention and depth-wise convolution , and various other modules.\nSimilarity. Depth-wise convolution resembles local attention in sparse connectivity. There are no\nconnections across channels. Each position is only connected to the positions in a small local window\nfor each channel.\nDifference. One main difference lies in weight sharing: depth-wise convolution shares the connection\nweights across spatial positions, while local attention shares the weights across channels or within\neach group of channels. Local attention uses proper weight sharing across channels to get better\nperformance. Depth-wise convolution beneﬁts from the weight sharing across positions to reduce the\nparameter complexity and increase the network capability.\nThe second difference is that the connection weights for depth-wise convolution arestatic and learned\nas model parameters, while the connection weights for local attention aredynamic and predicted from\neach instance. The dynamic variants of depth-wise convolution also beneﬁt from the dynamic weight.\n2The homogeneous version follows and applies dynamic convolution to depth-wise convolution. The\ninhomogeneous version is close to involution (Li et al., 2021) and lightweight depth-wise convolution (Wu et al.,\n2019).\n4\nPublished as a conference paper at ICLR 2022\nTable 1: The comparison of attention, local MLP (non-dynamic version of local attention, the attention weights\nare learned as static model parameters), local attention, convolution, depth-wise convolution (DW-Conv.) and\nthe dynamic variant (D-DW-Conv.) in terms of the patterns of sparse connectivity, weight sharing, and dynamic\nweight. Please refer to Figure 1 for the connectivity pattern illustration.\nSparse between positions Sparse between Weight sharing across Dynamic\nnon-local full channels position channel weight\nLocal MLP \u0013 \u0013 \u0013♭\nLocal attention \u0013 \u0013 \u0013♭ \u0013\nDW-Conv. \u0013 \u0013 \u0013\nD-DW-Conv. \u0013 \u0013 \u0013 \u0013\nConv. \u0013 \u0013\nOne more difference lies in window representation. Local attention represents the positions in\na window by utilizing a set form with spatial-order information lost. It explores the spatial-order\ninformation implicitly using the positional embedding or explicitly using the learned so-called relative\npositional embedding. Depth-wise convolution exploits a vector form: aggregate the representations\nwithin a local window with the weights indexed by the relative position (see Equation 6); keep\nspatial correspondence between the positions for different windows, thus exploring the spatial-order\ninformation explicitly.\n3 E XPERIMENTAL STUDY\nWe conduct empirical comparisons between local attention and depth-wise convolutions on three\nvisual recognition tasks: ImageNet classiﬁcation, COCO object detection, and ADE semantic\nsegmentation. We follow the structure of Swin Transformer to build the depth-wise convolution-\nbased networks. We apply the same training and evaluation settings from Swin Transformer to our\nmodels. In addition, we study the effects of weight sharing and dynamic weight in the two structures.\nThe results for large scale pre-training are given in the appendix.\n3.1 A RCHITECTURES\nWe use the recently-developed Swin Transformer as the example of local attention-based networks\nand study the performance over the tiny and base networks: Swin-T and Swin-B, provided by the\nauthors (Liu et al., 2021b) We follow the tiny and base networks to build two depth-wise convolution-\nbased networks, DWNet-T and DWNet-B, so that the overall architectures are the same, making\nthe comparison fair. We also build two dynamic versions, dynamic DWNet and i-dynamic DWNet,\nby predicting the dynamic weights as described in Section 2.4. We simply replace local attention\nin Swin Transformer by depth-wise convolution of the same window size, where the pre- and post-\nlinear projections over the values are replaced by 1 ×1 convolutions. We adopt the convolutional\nnetwork design pattern to append BN (Ioffe & Szegedy, 2015) and ReLU (Nair & Hinton, 2010) to\nthe convolution. The details are available in the Appendix. In terms of parameter and computation\ncomplexity, the depth-wise convolution-based networks are lower (Table 2) because there are linear\nprojections for keys and values in local attention.\n3.2 D ATASETS AND IMPLEMENTATION DETAILS\nImageNet classiﬁcation. The ImageNet-1K recognition dataset (Deng et al., 2009) contains 1.28M\ntraining images and 50K validation images with totally 1,000 classes. We use the exactly-same\ntraining setting as Swin Transformer (Liu et al., 2021b). The AdamW (Loshchilov & Hutter, 2019)\noptimizer for 300 epochs is adopted, with a cosine decay learning rate scheduler and 20 epochs of\nlinear warm-up. The weight decay is 0.05, and the initial learning rate is 0.001. The augmentation\nand regularization strategies include RandAugment (Cubuk et al., 2020), Mixup (Zhang et al., 2018a),\nCutMix (Yun et al., 2019), stochastic depth (Huang et al., 2016), etc.\nCOCO object detection. The COCO 2017 dataset (Lin et al., 2014) contains 118K training and 5K\nvalidation images. We follow Swin Transformer to adopt Cascade Mask R-CNN (Cai & Vasconcelos,\n2019) for comparing backbones. We use the training and test settings from Swin Transformer:\nmulti-scale training - resizing the input such that the shorter side is between 480 and 800 and the\nlonger side is at most 1333; AdamW optimizer with the initial learning rate 0.0001; weight decay -\n0.05; batch size - 16; and epochs - 36.\nADE semantic segmentation. The ADE20K (Zhou et al., 2017) dataset contains 25K images, 20K\nfor training, 2K for validation, and 3K for testing, with 150 semantic categories. The same setting\n5\nPublished as a conference paper at ICLR 2022\nTable 2: ImageNet classiﬁcation comparison for ResNet, Mixer and ResMLP, ViT and DeiT, Swin (Swin\nTransformer), DWNet, dynamic DWNet and i-dynamic DWNet.\nmethod img. size #param. FLOPs throughput (img. / s) top-1 acc. real acc.\nBottleneck: convolution with low rank\nResNet-50 (He et al., 2016) 2242 26M 4.1G 1128.3 76.2 82.5\nResNet-101 (He et al., 2016) 2242 45M 7.9G 652.0 77.4 83.7\nResNet-152 (He et al., 2016) 2242 60M 11.6G 456.7 78.3 84.1\nChannel and spatial separable MLP , spatial separable MLP = point-wise1 × 1 convolution\nMixer-B/16 (Tolstikhin et al., 2021) 2242 46M - - 76.4 82.4\nMixer-L/16 (Tolstikhin et al., 2021) 2242 189M - - 71.8 77.1\nResMLP-12 (Touvron et al., 2021) 2242 15M 3.0G - 76.6 83.3\nResMLP-24 (Touvron et al., 2021) 2242 30M 6.0G - 79.4 85.3\nResMLP-36 (Touvron et al., 2021) 2242 45M 8.9G - 79.7 85.6\nGlobal attention: dynamic channel separable MLP + spatial separable MLP\nViT-B/16 (Dosovitskiy et al., 2021) 3842 86M 55.4G 83.4 77.9 83.6\nViT-L/16 (Dosovitskiy et al., 2021) 3842 307M 190.7G 26.5 76.5 82.2\nDeiT-S (Touvron et al., 2020) 2242 22M 4.6G 947.3 79.8 85.7\nDeiT-B (Touvron et al., 2020) 2242 86M 17.5G 298.2 81.8 86.7\nDeiT-B (Touvron et al., 2020) 3842 86M 55.4G 82.7 83.1 87.7\nLocal MLP: perform static separable MLP in local small windows\nSwin-Local MLP-T 2242 26M 3.8G 861.0 80.3 86.1\nSwin-Local MLP-B 2242 79M 12.9G 321.2 82.2 86.9\nLocal attention: perform attention in local small windows\nSwin-T (Liu et al., 2021b) 2242 28M 4.5G 713.5 81.3 86.6\nSwin-B (Liu et al., 2021b) 2242 88M 15.4G 263.0 83.3 87.9\nDepth-wise convolution + point-wise1 × 1 convolution\nDWNet-T 2242 24M 3.8G 928.7 81.3 86.8\nDWNet-B 2242 74M 12.9G 327.6 83.2 87.9\ndynamic DWNet-T 2242 51M 3.8G 897.0 81.9 87.3\ndynamic DWNet-B 2242 162M 13.0G 322.4 83.2 87.9\ni-dynamic DWNet-T 2242 26M 4.4G 685.3 81.8 87.1\ni-dynamic DWNet-B 2242 80M 14.3G 244.9 83.4 88.0\nas Swin Transformer (Liu et al., 2021b) is adopted. UPerNet (Xiao et al., 2018) is used as the\nsegmentation framework. Details are provided in the Appendix.\n3.3 R ESULTS\nImageNet classiﬁcation. The comparison for ImageNet classiﬁcation is given in Table 2. One can\nsee that the local attention-based networks, Swin Transformer, and the depth-wise convolution-based\nnetworks, DWNets, perform on par (with a slight difference of0.1) in terms of top-1 accuracy and real\naccuracy (Beyer et al., 2020) for both tiny and base models. In the tiny model case, the two dynamic\nDWNets perform higher. In particular, the depth-wise convolution-based networks are more efﬁcient\nin parameters and computation complexities. In the tiny model case, the parameters and computation\ncomplexities are reduced by 14.2% and 15.5%, respectively. Similarly, in the base model case, the\ntwo costs are reduced by 15.9% and 16.2%, respectively. The homogeneous dynamic variant takes\nmore parameters but with almost the same complexity efﬁciency, and the inhomogeneous dynamic\nvariant take advantage of weight sharing across channels that reduce the model parameters.\nCOCO object detection. The comparisons between local attention (Swin Transformer), depth-\nwise convolution (DWNet), and two versions of dynamic variants are shown in Table 3. Depth-wise\nconvolution performs a little lower than local attention, and dynamic depth-wise convolution performs\nbetter than the static version and on par with local attention.\nADE semantic Segmentation. The comparisons of single scale testing on ADE semantic segmen-\ntation are shown in Table 3. In the tiny model case, dynamic DWNet is ~ 1.0% higher than local\nattention. In the base model case, the performances are similar3.\n3We conducted an additional experiment by changing the ending learning rate from0 to 1e − 6. The base\nmodel with depth-wise convolutions achieves a higher mIoU score: 48.9.\n6\nPublished as a conference paper at ICLR 2022\nTable 3: Comparison results on COCO object detection and ADE semantic segmentation.\nCOCO Object Detection ADE20K Semantic Segmentation\n#param. FLOPs APbox APbox\n50 APbox\n75 APmask #param. FLOPs mIoU\nSwin-T 86M 747G 50.5 69.3 54.9 43.7 60M 947G 44.5\nDWNet-T 82M 730G 49.9 68.6 54.3 43.4 56M 928G 45.5\ndynamic DWNet-T 108M 730G 50.5 69.5 54.6 43.7 83M 928G 45.7\ni-dynamic DWNet-T 84M 741G 50.8 69.5 55.3 44.0 58M 939G 46.2\nSwin-B 145M 986G 51.9 70.9 56.5 45.0 121M 1192G 48.1\nDWNet-B 132M 924G 51.1 69.6 55.4 44.2 108M 1129G 48.3\ndynamic DWNet-B 219M 924G 51.2 70.0 55.4 44.4 195M 1129G 48.0\ni-dynamic DWNet-B 137M 948G 51.8 70.3 56.1 44.8 114M 1153G 47.8\nTable 4: Effects of weight sharing across channels and positions. The results are reported on the\nImageNet top-1 accuracy. SC = Sharing across channels. SP = sharing across positions.\nSC SP Acc. #param. SC SP Acc. #param.\nLocal MLP\n\u0017 \u0013 80.2 35.3M\nDWNet\n\u0017 \u0013 81.3 24.2M\n\u0013 \u0017 80.3 26.2M \u0013 \u0017 80.3 26.2M\n\u0013 \u0013 80.3 24.3M \u0013 \u0013 81.1 23.9M\n3.4 E MPIRICAL ANALYSIS\nLocal and channel-separable connection has been shown to be helpful for visual recognition. The\nempirical results in Table 2, e.g., local attention performs better than global attention (local connection)\nand depth-wise convolution performs better than convolution (channel-separable connection), also\nverify it. In the following, we present empirical results for weight sharing and dynamic weight by\ntaking the tiny models as examples.\nWeight sharing. We study how the performance is affected by the number of channels in each group\nacross which the weights are shared (the numbers of attention heads at each stage are accordingly\nchanged) for local attention and local MLP (learn the weights in each window as model parameters\nand not shared across windows). Figure 2 shows the effect for (a) local MLP - static weights, and\n(b) local attention - dynamic weights. One can see that for local attention, too many channels and\ntoo few channels in each group perform similarly, but do not lead to the best. For local MLP, weight\nsharing signiﬁcantly reduces model parameters. These indicate proper weight sharing across channels\nis helpful for both local attention and local MLP.\nWe further study the effect of combining the weight sharing pattern for local MLP and DWNet. For\nlocal MLP, Weight sharing across positions means the connection weight is shared for different spatial\nblocks in local MLP. For convolution, the scheme of sharing weights across channels is similar to the\nmulti-head manner in local attention. The results in Table 4 suggest that: (i) for local MLP, sharing\nweight across channels reduces the model parameters and sharing across spatial blocks do not have\nbig impact; (ii) For depth-wise convolution, sharing weight across channels does not have big impact,\nbut sharing weight across positions signiﬁcantly increase the performance.\nThe window sampling scheme for local MLP and DWNet is different: local MLP sparsely samples the\nwindows using the way in Swin Transformer, for reducing the high memory cost, and DWNet densely\nsamples the windows. Weight sharing across positions in local MLP is insufﬁcient for learning\ntranslation-equivalent representation, explaining why local MLP with weight sharing across both\nchannels and positions performs lower than depth-wise convolution with additional weight sharing\nacross channels.\nDynamic weight. We study how dynamic weight in local attention affects performance. As seen\nfrom Table 2, local MLP achieves, the static version, 80.3% and 82.2% for tiny and base models,\nlower than Swin, the dynamic version,81.3% and 83.3%. This implies that dynamic weight is helpful.\nThe improvements from dynamic weight are also observed for depth-wise convolution (Table 2).\nWe further study the effects of the attention scheme and the linear-projection scheme for dynamic\nweight computation. The observations from in Table 5 include: the attention mechanism for shifted\nand sliding window sampling performs similarly; the inhomogeneous dynamic weight computa-\ntion way is better than the attention mechanism ( 81.8 vs 81.4). We think that the reasons for the\nlatter observation include: for the attention mechanism the representation is only block translation\n7\nPublished as a conference paper at ICLR 2022\n(a) 1 / 95M2 / 59M6 / 36M16 / 28M32 / 26M48 / 25M96 / 25M\n79.7\n80\n80.5\n(b) 6 / 28M16 / 28M32 / 28M48 / 28M96 / 28M\n80\n80.5\n81\n81.5\nFigure 2: Effect of #channels sharing the weights on ImageNet classiﬁcation. X-axis: #channels within each\ngroup / #param. Y-axis: ImageNet classiﬁcation accuracy. (a) Local MLP: the static version of Swin transformer.\n(b) Local attention: Swin transformer. Results is reported for tiny model on ImageNet dataset.\nTable 5: Comparison of different dynamic weight manners. The results are reported on the ImageNet top-1\naccuracy. Shifted window sampling (Win. samp.) means the way in Swin Transformer and sliding means\nthe densely-sampling manner. The result of Sliding local MLP is from (Liu et al., 2021b). homo. dyna. =\nhomogeneous dynamic weight. inhomo. dyna. = inhomogeneous dynamic weight.\nWin. samp. #param. FLOPs Acc. Win. samp. #param. FLOPs Acc.\nLocal MLP shifted 26M 3.8G 80.3 DWNet sliding 24M 3.8G 81.3\nw/ attention shifted 28M 4.5G 81.3 w/ homo. dyna. sliding 51M 3.8G 81.9\nw/ attention sliding 28M 4.5G 81.4 w/ inhomo. dyna. sliding 26M 4.4G 81.8\nequivalent other than any translation equivalent; the linear projection-based dynamic weight scheme\n(vector representation for the window) learns better weights than the attention-based scheme (set\nrepresentation for the window). We also observe that such inﬂuence is eliminated for large models\nand detection tasks.\nSet representation. Local attention represents the positions in a window as a set with the spatial-\norder information lost. Swin Transformer learns relative positional embeddings where the positions\nin a window are actually described as a vector keeping the spatial-order information. It is reported\nin (Liu et al., 2021b) that removing the relative positional embeddings leads to a 1.2% accuracy drop,\nindicating the spatial-order information is important.\nConcurrent works. We give the comparison between inhomogeneous dynamic depth-wise convolu-\ntion (i-dynamic DWNet) and concurrent local attention-based works (Chu et al., 2021a; Wang et al.,\n2021a; Huang et al., 2021; Xu et al., 2021) in Table 6. We follow Shufﬂe Transformer and add an\nextra DW Conv. before FFN in i-dynamic DWNet, and the performance is improved by 0.5. The\nperformance is on par with these concurrent works except the Twins-SVT (81.9%, 2.9G) which uses\ninterleaved attention and additional depth-wise convolutions.\n4 R ELATED WORK\nSparse connectivity. Sparse connection across channels is widely explored for removing redun-\ndancy in the channel domain. The typical schemes are depth-wise convolution adopted by Mo-\nbileNet (Howard et al., 2017; Sandler et al., 2018), ShufﬂeNetV2 (Ma et al., 2018) and IGCv3 (Sun\net al., 2018), and group convolution adopted by ResNeXt (Xie et al., 2017), merge-and-run (Zhao\net al., 2018), ShufﬂeNetV1 (Zhang et al., 2018b), and IGC (Zhang et al., 2017).\nThe self-attention unit4 in Vision Transformer, its variants (Chen et al., 2020; Chu et al., 2021b;\nDosovitskiy et al., 2021; Han et al., 2021; Heo et al., 2021; Li et al., 2021; Liu et al., 2021b; Pan\net al., 2021; Touvron et al., 2020; Vaswani et al., 2021; Wang et al., 2021b; Wu et al., 2021; Yuan\net al., 2021a;b; Zhang et al., 2021; Zhao et al., 2020; Zhou et al., 2021), and the spatial information\nfusion unit (e.g., token-mixer in MLP-Mixer (Tolstikhin et al., 2021) and ResMLP (Touvron et al.,\n2021)) have no connections across channels.\n1 ×1 (point-wise) convolution (in ShufﬂeNetV2 (Ma et al., 2018), MobileNet (Howard et al., 2017;\nSandler et al., 2018), IGC (Zhang et al., 2017), ViT (Dosovitskiy et al., 2021), local ViT (Liu et al.,\n2021b; Vaswani et al., 2021), MLP-Mixer (Tolstikhin et al., 2021), ResMLP (Touvron et al., 2021))\nhas no connections across spatial positions. The convolutions with other kernel sizes and local\nattention (Zhao et al., 2020; Liu et al., 2021b; Vaswani et al., 2021) have connections between each\nposition and the positions within a small local window, respectively.\n4The pre- and post- linear projections for values can be regarded as1 × 1 convolutions. The attention weights\ngenerated from keys and values with linear projections in some sense mix the information across channels.\n8\nPublished as a conference paper at ICLR 2022\nTable 6: Comparison with concurrent works on ImageNet classiﬁcation with tiny models.\n#param. FLOPs top-1 acc.\nTwins-PCPVT (Chu et al., 2021a) 24M 3.8G 81.2\nTwins-SVT (Chu et al., 2021a) 24M 2.9G 81.7\nCoaT-Lite (Xu et al., 2021) 20M 4.0G 81.9\nCoaT (Xu et al., 2021) 22M 12.6G 82.1\nPVT-v2 (Wang et al., 2021a) 25M 4.0G 82.0\nShufﬂe Transformer (Huang et al., 2021) 29M 4.6G 82.5\ni-dynamic DWNet 26M 4.4G 81.8\ni-dynamic DWNet + DW 27M 4.4G 82.3\nWeight sharing. Weight sharing across spatial positions is mainly used in convolution, including\nnormal convolution, depth-wise convolution and point-wise convolution. Weight sharing across\nchannels is adopted in the attention unit (Vaswani et al., 2017), its variants (Chu et al., 2021a;b;\nDosovitskiy et al., 2021; Li et al., 2021; Liu et al., 2021b; Touvron et al., 2020; Vaswani et al.,\n2021; Wang et al., 2021b; Wu et al., 2021; Yuan et al., 2021b), and token-mixer MLP in MLP-\nmixer (Tolstikhin et al., 2021) and ResMLP (Touvron et al., 2021).\nDynamic weight. Predicting the connection weights is widely studied in convolutional networks.\nThere are basically two types. One is to learn homogeneous connection weights, e.g., SENet (Hu\net al., 2018b), dynamic convolution (Jia et al., 2016). The other is to learn the weights for each region\nor each position (GENet (Hu et al., 2018a), Lite-HRNet (Yu et al., 2021), Involution (Li et al., 2021)).\nThe attention unit in ViT or local ViT learns dynamic connection weights for each position.\nNetworks built with depth-wise separable convolutions. There are many networks built upon\ndepth-wise separable convolution or its variants, such as MobileNet (Howard et al., 2017; Sandler\net al., 2018), ShufﬂeNet (Ma et al., 2018), IGC (Zhang et al., 2017), Xception (Chollet, 2017), and\nEfﬁcientNet (Tan & Le, 2019; 2021). In this paper, our goal is to connect dynamic depth-wise\nconvolution with local attention.\nConvolution vs Transformer.The study in (Cordonnier et al., 2020) shows that a multi-head self-\nattention layer can simulate a convolutional layer by developing additional carefully-designed relative\npositional embeddings with the attention part dropped. Differently, we connect (dynamic) depth-wise\nconvolution and local self-attention by connecting the attention weights for self-attention and the\ndynamic weights for convolution(as well as studying weight sharing). In (Andreoli, 2019), the\nmathematical connection (in terms of the tensor form) between convolution and attention is presented.\nThe opinion that convolution and attention are essentially about the model complexity control is\nsimilar to ours, and we make the detailed analysis and report empirical studies.\nThe concurrently-developed work in NLP (Tay et al., 2021) empirically compares lightweight depth-\nwise convolution (Wu et al., 2019) to Transformer for NLP tasks, and reaches a conclusion similar to\nours for vision tasks: convolution and Transformer obtain on-par results. Differently, we attempt to\nunderstand why they perform on par from three perspectives: sparse connectivity, weight sharing and\ndynamic weight, and discuss their similarities and differences.\n5 C ONCLUSION\nThe connections between local attention and dynamic depth-wise convolution are summarized as\nfollows. (i) Same with dynamic depth-wise convolution, local attention beneﬁts from two sparse\nconnectivity forms: local connection and no connection across channels. (ii) Weight sharing across\nchannels in local attention is helpful for reducing the parameter (attention weight) complexity and\nslightly boosting the performance, and weight sharing across positions in depth-wise convolution is\nhelpful for reducing the parameter complexity and learning translation-equivalent representations\nand thus boosting the performance. (iii) The attention-based dynamic weight computation for\nlocal attention is beneﬁcial for learning image-dependent weights and block-translation equivalent\nrepresentations, and the linear projection-based dynamic weight computation for (in)homogeneous\ndynamic depth-wise convolution is beneﬁcial for learning image-dependent weights. The constructed\ni-dynamic DWNet is superior over Swin transformer for ImageNet classiﬁcation and segmentation\nin the case of tiny models, and on par for larger models and detection tasks. In addition, the better\ndownstream performance for local attention and depth-wise convolution stems from the larger kernel\nsize (7 ×7 vs 3 ×3), which is also observed in Yuan et al. (2021d).\n9\nPublished as a conference paper at ICLR 2022\nREFERENCES\nJean-Marc Andreoli. Convolution, attention and structure embedding. arXiv preprint\narXiv:1905.01289, 2019.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nLucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are\nwe done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\nZhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: high quality object detection and instance\nsegmentation. IEEE Trans. Pattern Anal. Mach. Intell., 2019.\nHanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chun-\njing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. arXiv preprint\narXiv:2012.00364, 2020.\nFrançois Chollet. Xception: Deep learning with depthwise separable convolutions. In IEEE Conf.\nComput. Vis. Pattern Recog., pp. 1251–1258, 2017.\nXiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. Twins: Revisiting spatial attention design in vision transformers. arXiv preprint\narXiv:2104.13840, 2021a.\nXiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. Do we really need explicit\nposition encodings for vision transformers? arXiv preprint arXiv:2102.10882, 2021b.\nMMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and\nbenchmark. https://github.com/open-mmlab/mmsegmentation, 2020.\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\nattention and convolutional layers. In Int. Conf. Learn. Represent., 2020.\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated\ndata augmentation with a reduced search space. In IEEE Conf. Comput. Vis. Pattern Recog., pp.\n702–703, 2020.\nStéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun.\nConvit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint\narXiv:2103.10697, 2021.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 248–255. Ieee, 2009.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.\nIn Int. Conf. Learn. Represent., 2021.\nShang-Hua Gao, Qi Han, Duo Li, Pai Peng, Ming-Ming Cheng, and Pai Peng. Representative batch\nnormalization with feature calibration. In IEEE Conf. Comput. Vis. Pattern Recog., 2021.\nIan Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.\nMIT press Cambridge, 2016.\nMeng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, and Shi-Min Hu. Beyond self-attention: External\nattention using two linear layers for visual tasks. arXiv preprint arXiv:2105.02358, 2021.\nKai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang,\nAn Xiao, Chunjing Xu, Yixing Xu, et al. A survey on visual transformer. arXiv preprint\narXiv:2012.12556, 2020.\nKai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in\ntransformer. arXiv preprint arXiv:2103.00112, 2021.\n10\nPublished as a conference paper at ICLR 2022\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 770–778, 2016.\nByeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh.\nRethinking spatial dimensions of vision transformers. arXiv preprint arXiv:2103.16302, 2021.\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,\nMarco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for\nmobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\nIn Int. Conf. Comput. Vis., pp. 3464–3473, 2019.\nJie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea Vedaldi. Gather-excite: Exploiting feature\ncontext in convolutional neural networks. In Adv. Neural Inform. Process. Syst., 2018a.\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In IEEE Conf. Comput. Vis.\nPattern Recog., pp. 7132–7141, 2018b.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\nstochastic depth. In Eur. Conf. Comput. Vis., pp. 646–661. Springer, 2016.\nLang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. Interlaced\nsparse self-attention for semantic segmentation. CoRR, abs/1907.12273, 2019a.\nZilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\nCriss-cross attention for semantic segmentation. In Int. Conf. Comput. Vis., pp. 603–612, 2019b.\nZilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu. Shufﬂe transformer:\nRethinking spatial shufﬂe for vision transformer. arXiv preprint arXiv:2106.03650, 2021.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In Int. Conf. Mach. Learn., pp. 448–456. PMLR, 2015.\nXu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc Van Gool. Dynamic ﬁlter networks. In Adv.\nNeural Inform. Process. Syst., 2016.\nSalman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and\nMubarak Shah. Transformers in vision: A survey. arXiv preprint arXiv:2101.01169, 2021.\nDuo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei Zhu, Tong Zhang, and Qifeng Chen.\nInvolution: Inverting the inherence of convolution for visual recognition. In IEEE Conf. Comput.\nVis. Pattern Recog., 2021.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Eur. Conf.\nComput. Vis., pp. 740–755. Springer, 2014.\nHanxiao Liu, Zihang Dai, David R So, and Quoc V Le. Pay attention to mlps. arXiv preprint\narXiv:2105.08050, 2021a.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021b.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Int. Conf. Learn.\nRepresent. OpenReview.net, 2019.\nNingning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufﬂenet v2: Practical guidelines for\nefﬁcient cnn architecture design. In Eur. Conf. Comput. Vis., pp. 116–131, 2018.\nVinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In\nInt. Conf. Mach. Learn., 2010.\n11\nPublished as a conference paper at ICLR 2022\nZizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jianfei Cai. Scalable visual transformers with\nhierarchical pooling. arXiv preprint arXiv:2103.10619, 2021.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon\nShlens. Stand-alone self-attention in vision models. In Adv. Neural Inform. Process. Syst., pp.\n68–80, 2019.\nHubert Ramsauer, Bernhard Schäﬂ, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler,\nLukas Gruber, Markus Holzleitner, Milena Pavlovi´c, Geir Kjetil Sandve, et al. Hopﬁeld networks\nis all you need. arXiv preprint arXiv:2008.02217, 2020.\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-\nbilenetv2: Inverted residuals and linear bottlenecks. In IEEE Conf. Comput. Vis. Pattern Recog.,\npp. 4510–4520, 2018.\nKe Sun, Mingjie Li, Dong Liu, and Jingdong Wang. Igcv3: Interleaved low-rank group convolutions\nfor efﬁcient deep neural networks. In Brit. Mach. Vis. Conf., 2018.\nKe Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for\nhuman pose estimation. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 5693–5703, 2019.\nMingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.\nIn Int. Conf. Mach. Learn., pp. 6105–6114. PMLR, 2019.\nMingxing Tan and Quoc V Le. Efﬁcientnetv2: Smaller models and faster training. arXiv preprint\narXiv:2104.00298, 2021.\nYi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen Qin, and Donald Metzler. Are\npre-trained convolutions better than pre-trained transformers? arXiv preprint arXiv:2105.03322,\n2021.\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-\nterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An\nall-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\nHugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard\nGrave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Hervé Jégou. Resmlp: Feedforward\nnetworks for image classiﬁcation with data-efﬁcient training. arXiv preprint arXiv:2105.03404,\n2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Adv. Neural Inform. Process. Syst., pp.\n5998–6008, 2017.\nAshish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon\nShlens. Scaling local self-attention for parameter efﬁcient visual backbones. InIEEE Conf. Comput.\nVis. Pattern Recog., 2021.\nJingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong\nMu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high-resolution representation\nlearning for visual recognition. IEEE Trans. Pattern Anal. Mach. Intell., 2020.\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint\narXiv:2106.13797, 2021a.\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122, 2021b.\n12\nPublished as a conference paper at ICLR 2022\nFelix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, and Michael Auli. Pay less attention with\nlightweight and dynamic convolutions. In Int. Conf. Learn. Represent., 2019.\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual parsing for\nscene understanding. In Eur. Conf. Comput. Vis., pp. 418–434, 2018.\nSaining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual\ntransformations for deep neural networks. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 1492–\n1500, 2017.\nWeijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers.\narXiv preprint arXiv:2104.06399, 2021.\nChangqian Yu, Bin Xiao, Changxin Gao, Lu Yuan, Lei Zhang, Nong Sang, and Jingdong Wang.\nLite-hrnet: A lightweight high-resolution network. In IEEE Conf. Comput. Vis. Pattern Recog.,\n2021.\nKun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating\nconvolution designs into visual transformers. arXiv preprint arXiv:2103.11816, 2021a.\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and\nShuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet.arXiv\npreprint arXiv:2101.11986, 2021b.\nLi Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. V olo: Vision outlooker for visual\nrecognition, 2021c.\nYuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and Jingdong Wang.\nHrformer: High-resolution transformer for dense prediction. Adv. Neural Inform. Process. Syst.,\n34, 2021d.\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classiﬁers with localizable features. In Int. Conf.\nComput. Vis., pp. 6023–6032, 2019.\nHongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical\nrisk minimization. In Int. Conf. Learn. Represent., 2018a.\nPengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao.\nMulti-scale vision longformer: A new vision transformer for high-resolution image encoding.\narXiv preprint arXiv:2103.15358, 2021.\nQinglong Zhang and Yubin Yang. Rest: An efﬁcient transformer for visual recognition. arXiv\npreprint arXiv:2105.13677, 2021.\nTing Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. Interleaved group convolutions. InInt. Conf.\nComput. Vis., pp. 4373–4382, 2017.\nXiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufﬂenet: An extremely efﬁcient\nconvolutional neural network for mobile devices. In IEEE Conf. Comput. Vis. Pattern Recog., pp.\n6848–6856, 2018b.\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\nIEEE Conf. Comput. Vis. Pattern Recog., June 2020.\nLiming Zhao, Mingjie Li, Depu Meng, Xi Li, Zhaoxiang Zhang, Yueting Zhuang, Zhuowen Tu, and\nJingdong Wang. Deep convolutional neural networks with merge-and-run mappings. In Jérôme\nLang (ed.), IJCAI, pp. 3170–3176, 2018.\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmenta-\ntion. In Assoc. Adv. Artif. Intell., volume 34, pp. 13001–13008, 2020.\n13\nPublished as a conference paper at ICLR 2022\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene\nparsing through ade20k dataset. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 633–641, 2017.\nDaquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi Feng.\nDeepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.\n14\nPublished as a conference paper at ICLR 2022\nAPPENDIX\nA R ELATION GRAPH\nWe present a relation graph in Figure 3 to describe the relation between convolution, depth-wise\nseparable convolution (depth-wise convolution + 1 ×1 convolution), Vision Transformer, Local\nVision Transformer, as well as multilayer perceptron (MLP), Separable MLP in terms of sparse\nconnectivity, weight sharing, and dynamic weight. Table 7\nMultilayer perceptron (MLP) is a fully-connected layer: each neuron (an element at each position\nand each channel) in one layer is connected with all the neurons in the previous layer5. Convolution\nand separable MLP are sparse versions of MLP. The connection weights can be formulated as a\ntensor (e.g., 3D tensor, two dimension for space and one dimension for channel) and the low-rank\napproximation of the tensor can be used to regularize the MLP.\nConvolution is a locally-connected layer, formed by connecting each neuron to the neurons in a small\nlocal window with the weights shared across the spatial positions. Depth-wise separable convolution\nis formed by decomposing the convolution into two components: one is point-wise 1 ×1 convolution,\nmixing the information across channels, and the other is depth-wise convolution, mixing the spatial\ninformation. Other variants of convolution, such as bottleneck, multi-scale convolution or pyramid,\ncan be regarded as low-rank variants.\nSeparable MLP (e.g., MLP-Mixer and ResMLP) reshapes the 3D tensor into a 2D format with the\nspatial dimension and channel dimension. Separable MLP consists of two sparse MLP along the two\ndimensions separately, which are formed by separating the input neurons into groups. Regarding\nchannel sparsity, the neurons in the same channel form a group, and an MLP is performed over each\ngroup with the MLP parameters shared across groups, forming the ﬁrst sparse MLP (spatial/token\nmixing). A similar process is done by viewing the neurons at the same position into a group, forming\nthe second sparse MLP (channel mixing).\nVision Transformer is a dynamic version of separable MLP. The weights in the ﬁrst sparse MLP\n(spatial/token mixing) are dynamically predicted from each instance. Local Vision Transformer is a\nspatially-sparser version of Vision Transformer: each output neuron is connected to the input neurons\nin a local window. PVT (Wang et al., 2021b) is a pyramid (spatial sampling/ low-rank) variant of\nVision Transformer.\nDepth-wise separable convolution can also be regarded as a spatially-sparser version of separable\nMLP. In the ﬁrst sparse MLP (spatial/token mixing), each output neuron is only dependent on the\ninput neurons in a local window, forming depth-wise convolution. In addition, the connection weights\nare shared across spatial positions, instead of across channels.\nB M ATRIX FORM EXPLANATION\nWe use the matrix form to explain sparsity connectivity in various layers and how they are obtained\nby modifying the MLP.\nMLP. The term MLP, Multilayer Perceptron, is used ambiguously, sometimes loosely to any feedfor-\nward neural network. We adopt one of the common deﬁnitions, and use it to refer to fully-connected\nlayers. Our discussion is based on a single fully-connected layer, and can be easily generalized to\ntwo or more fully-connected layers. One major component, except the nonlinear units and others, is a\nlinear transformation:\ny = Wx, (9)\nwhere x represents the input neurons, y represents the output neurons, and W represents the\nconnection weights, e.g., W ∈RNC ×NC , where N is the number of positions, and Cis the number\nof channels.\nConvolution. Considering the 1D case with a single channel (the 2D case is similar), the connection\nweight matrix W ∈RN×N is in the following sparse form, also known as the Toeplitz matrix (We\n5We use the widely-used deﬁnition for the term MLP: fully-connected layer. There might be other deﬁnitions.\n15\nPublished as a conference paper at ICLR 2022\nregularization\nPyramid\nMS Conv.\nLocality Sep.\nDim. Sep.\nLocality Sep.\nDim. Sep.\nSpatial\nSpatial LR\nChannel LR\nViT\nMLP\nConv.\nSparse Connection\nDynamic Weight\nLow-Rank\nBottleneck\nDW-S Conv.\nLocal ViT\nSep. MLP\nLR MLP\nDim. LR\nDynamic\nPVT\nLocality\nSep.\nSpatial LR\nDynamic\nLocal-S MLP\nInhomogeneous\nVec. Atten.\nDynamic\nDW-S Conv.\nVec. Atten.\nFigure 3: Relation graph for convolution (Conv.), depth-wise separable convolution (DW-S Conv.), Vision\nTransformer (ViT) building block, local ViT building block, Sep. MLP (e.g., MLP-Mixer and ResMLP),\ndynamic depth-wise separable convolution (Dynamic DW-S Conv.), as well as dynamic local separable MLP\n( e.g., involution (Li et al., 2021) and inhomogeneous dynamic depth-wise convolution) in terms of sparse\nconnectivity and dynamic weight. Dim. = dimension including spatial and channel, Sep. = separable, LR = low\nrank, MS Conv. = multi-scale convolution, PVT = pyramid vision transformer.\nTable 7: The comparison of attention, local MLP (non-dynamic version of local attention, the attention weights\nare learned as static model parameters), local attention, convolution, depth-wise convolution (DW-Conv.) and the\ndynamic variant (D-DW-Conv.), as well as MLP and MLP variants in terms of the patterns of sparse connectivity,\nweight sharing, and dynamic weight. †Spatial-mixing MLP (channel-separable MLP) corresponds to token-mixer\nMLP. ‡1 × 1 Conv. is also called point-wise (spatial-separable) MLP. ♭The weights might be shared within each\ngroup of channels. Please refer to Figure 1 for the connectivity pattern illustration.\nSparse between positions Sparse between Weight sharing across Dynamic\nnon-local full channels position channel weight\nLocal MLP \u0013 \u0013 \u0013♭\nLocal attention \u0013 \u0013 \u0013♭ \u0013\nDW-Conv. \u0013 \u0013 \u0013\nD-DW-Conv. \u0013 \u0013 \u0013 \u0013\nConv. \u0013 \u0013\nMLP\nAttention \u0013 \u0013♭ \u0013\nSpatial-mixing MLP† \u0013 \u0013\n1 × 1 Conv.‡ \u0013 \u0013\nuse the window size 3 as an example):\nW =\n\n\na2 a3 0 0 ··· 0 a1\na1 a2 a3 0 ··· 0 0\n... ... ... ... ... ... ...\na3 0 0 0 ··· a1 a2\n\n\n. (10)\nFor the C-channel case, we organize the input into a vector channel by channel: [x⊤\n1 x⊤\n2 ... x⊤\nC]⊤,\nand accordingly the connection weight matrix channel by channel for the coth output channel,\nWco = [Wco1 Wco2 ... WcoC] (the form of Wcoi is the same as Equation 10). The whole form\ncould be written as\n\n\ny1\ny2\n...\nyC\n\n\n=\n\n\nW1\nW2\n...\nWC\n\n\n\n\nx1\nx2\n...\nxC\n\n\n. (11)\n16\nPublished as a conference paper at ICLR 2022\nSep. MLP. Sep. MLP, e.g., ResMLP and MLP-Mixer, is formed with two kinds of block-sparse\nmatrices: one for channel-mixing and the other for spatial-mixing. In the case that the input is\norganized channel by channel (the neurons in each channel form a group), x = [x⊤\n1 x⊤\n2 ... x⊤\nC]⊤,\nthe connection weight matrix is in a block-sparse form:\nW =\n\n\nWc 0 ··· 0 0\n0 W c ··· 0 0\n... ... ... ... ...\n0 0 ··· 0 W c\n\n\n, (12)\nwhere the block matrices Wc ∈RN×N are shared across all the channels, and the sharing pattern\ncan be modiﬁed to share weights within each group of channels.\nThe input can be reshaped position by position (the neurons at each position forms a group): x =\n[x⊤\n1 x⊤\n2 ... x⊤\nN ]⊤, and similarly one more connection weight matrix can be formulated in a block-\nsparse form (it is essentially a 1 ×1 convolution, Wp ∈RC×C):\nW′=\n\n\nWp 0 ··· 0 0\n0 W p ··· 0 0\n... ... ... ... ...\n0 0 ··· 0 W p\n\n\n. (13)\nThe forms of block-sparsity are studied in interleaved group convolutions (Zhang et al., 2017) without\nsharing the weights across groups.\nSep. MLP can also be regarded as using Kronecker product to approximate the connection matrix,\nWx = vec(A mat(x)B). (14)\nHere, W = B⊤⊗A = W⊤\nc ⊗Wp. and ⊗is the Kronecker product operator. mat(x) reshapes\nthe vector x in a 2D matrix form, while vec(x) reshapes the 2D matrix into a vector form. In Sep.\nMLP, the 2D matrix, mat(x) ∈RC×N , is organized so that each row corresponds to one channel\nand each column corresponds to one spatial position. CCNet (Huang et al., 2019b) and interlaced\nself-attention (Huang et al., 2019a) use Kronecker product to approximate the spatial connection: the\nformer reshapes the vector in a 2D matrix form along the xand yaxes, and the latter reshapes the\nvector windows by windows.\nVision Transformer (ViT).The matrix form is similar to Sep. MLP. The difference is that the matrix\nWc is predicted from each image instance. The weight prediction manner in ViT has a beneﬁt: handle\nan arbitrary number of input neurons.\nDepth-wise separable convolution. There are two basic components: depth-wise convolution, and\n1 ×1 convolution that is the same as channel-mixing MLP in Sep. MLP. Depth-wise convolution can\nbe written in the matrix form:\n\n\ny1\ny2\n...\nyC\n\n\n=\n\n\nW11 0 ··· 0\n0 W 22 ··· 0\n... ... ... ...\n0 0 ··· WCC\n\n\n\n\nx1\nx2\n...\nxC\n\n\n, (15)\nwhere the form of Wcc is the same as Equation 10.\nLocal ViT. In the non-overlapping window partition case, local ViT simply repeats ViT over each\nwindow separately with the linear projections, applied to keys, values, and queries, shared across\nwindows. In the overlapping case, the form is a little complicated, but the intuition is the same. In the\n17\nPublished as a conference paper at ICLR 2022\nextreme case, the partition is the same as convolution, and the form is as the following:\n\n\ny1\ny2\n...\nyC\n\n\n=\n\n\nWd 0 ··· 0\n0 W d ··· 0\n... ... ... ...\n0 0 ··· Wd\n\n\n\n\nx1\nx2\n...\nxC\n\n\n, (16)\nwhere the dynamic weight matrix Wd is like the form below:\nWd =\n\n\na12 a13 0 0 ··· 0 a11\na21 a22 a23 0 ··· 0 0\n... ... ... ... ... ... ...\naN3 0 0 0 ··· aN1 aN2\n\n\n. (17)\nLow-rank MLP. Low-rank MLP approximates the connection weight matrix W ∈RDo×Di in\nEquation 9 using the product of two low-rank matrix:\nW ←WDorWrDi, (18)\nwhere ris a number smaller than Di and Do\nPyramid. The downsampling process in the pyramid networks can be regarded as spatial low\nrank: W(∈ RNC ×NC ) → W′(∈ RN′C×N′C), where N′ is equal to N\n4 in the case that the\nresolution is reduced by 1\n2 . If the numbers of input and output channels are different, it becomes\nW(∈RNC ′×NC ) →W′(∈RN′C′×N′C).\nMulti-scale parallel convolution. Multi-scale parallel convolution used in HRNet (Wang et al.,\n2020; Sun et al., 2019) can also be regarded as spatial low rank. Consider the case with four scales,\nmulti-scale parallel convolution can be formed as as the following,\nW →\n\n\nW1 ∈RNC1\nW2 ∈RNC2\nW3 ∈RNC3\nW4 ∈RNC4\n\n →\n\n\nW′\n1 ∈RNC1\nW′\n2 ∈R\nN\n4 C2\nW′\n3 ∈R\nN\n16 C3\nW′\n4 ∈R\nN\n64 C4\n\n\n, (19)\nwhere C1,C2,C3, and C4 are the numbers of the channels in four resolutions.\nC L OCAL ATTENTION VS CONVOLUTION : DYNAMIC WEIGHTS\nWe take the 1D case with the window size 2K+ 1 as an example to illustrate the dynamic weight\nprediction manner. Let {xi−K,..., xi,..., xi+k}correspond to the (2K+ 1) positions in the ith\nwindow, and {wi−K,...,w i,...,w i+K}be the corresponding dynamic weights for updating the\nrepresentation of the ith (center) position. The discussion can be easily extended to multiple weights\nfor each positions, like the M-head attention and updating the representations for other positions.\nInhomogeneous dynamic convolution. We use the case using only a single linear projection to\nillustrate inhomogeneous dynamic convolution. The properties we will discuss are similar for more\nlinear projections. The dynamic weights are predicted as the following:\n\n\nwi−K\n...\nwi\n...\nwi+K\n\n\n= Θxi =\n\n\nθ⊤\n−K\n...\nθ⊤\n0\n...\nθ⊤\nK\n\n\nxi. (20)\n18\nPublished as a conference paper at ICLR 2022\nIt can be seen that dynamic convolution learns the weights for each position through the parameters\nthat are different for different positions, e.g., θk corresponds to wi+k. It regards the positions in the\nwindow as the vector form, keeping the spatial order information.\nDot-product attention. The dot-product attention mechanism in the single-head case predicts the\nweights as the following6:\n\n\nwi−K\n...\nwi\n...\nwi+K\n\n\n=\n\n\n(xi−K)⊤\n...\n(xi)⊤\n...\n(xi+K)⊤\n\n\nP⊤\nk Pqxi. (21)\nDot-product attention uses the same parameters P⊤\nk Pq for all the positions. The weight depends\non the features at the same position, e.g., wi−k corresponds to xi−k. It in some sense regards the\npositions in the window as a set form, losing the spatial order information.\nWe rewrite it as the following\nΘd =\n\n\n(xi−K)⊤\n...\n(xi)⊤\n...\n(xi+K)⊤\n\n\nP⊤\nk Pq, (22)\nfrom which we can see that the parameters Θd is dynamically predicted. In other words, dot-product\nattention can be regarded as a two-level dynamic scheme.\nRelative position embeddings is equivalent to adding static weights that keeps the spatial order\ninformation:\n\n\nwi−K\n...\nwi\n...\nwi+K\n\n\n= Θdxi +\n\n\nβ−K\n...\nβ0\n...\nβK\n\n\n. (23)\nA straightforward variant is a combination of the static Θ and the dynamic Θd:\n\n\nwi−K\n...\nwi\n...\nwi+K\n\n\n= (Θd + Θ)xi. (24)\nConvolutional attention. We introduce a convolutional attention framework so that it enjoys the\nbeneﬁts of dynamic convolution and dot-product attention: keep the spatial order information and\ntwo-level dynamic weight prediction.\n6For presentation clarity, we omit the softmax normalization and the scale in dot-product. What we discuss\nstill holds if softmax and scale are included.\n19\nPublished as a conference paper at ICLR 2022\nThe post-convolutional attention mechanism left-multiplies a matrix (with the kernel size being 3):\nΘd =\n\n\na2 a3 0 0 ··· 0 a1\na1 a2 a3 0 ··· 0 0\n... ... ... ... ... ... ...\na3 0 0 0 ··· a1 a2\n\n\n\n\n(xi−K)⊤\n...\n(xi)⊤\n...\n(xi+K)⊤\n\n\nP⊤\nk Pq. (25)\nThis can be reviewed as a variant of relative positional embeddings (Equation 23). In the simpliﬁed\ncase that the left matrix is diagonal, it can be regarded as the product version of relative positional\nembeddings (Equation 23 is an addition version).\nWe can perform a convolution with the kernel size being 3, the kernel weights shared across channels\n(it is also ﬁne not to share weights), and then do dot-product attention. This is called pre-convolutional\nattention: perform convolutions on the representations. The two processes are can be written as\nfollows (omit BN and ReLU that follow the convolution),\n\n\nwi−K\n...\nwi\n...\nwi+K\n\n\n=\n\n\na1 a2 a3 ··· 0 0 0\n0 a1 a1 ··· 0 0 0\n... ... ... ... ... ... ...\n0 0 0 ··· a2 a3 0\n0 0 0 ··· a1 a2 a3\n\n\n\n\n(xi−K−1)⊤\n(xi−K)⊤\n...\n(xi)⊤\n...\n(xi+K)⊤\n(xi+K+1)⊤\n\n\nP⊤\nk Pq [xi−1 xi xi+1]\n\n\na1\na2\na3\n\n.\n(26)\nIt can be generalized to using normal convolution:\n\n\nwi−K\n...\nwi\n...\nwi+K\n\n\n= C′\n\n\nxi−K−1 xi−K−1 ··· xi−K−1\nxi−K xi−K ··· xi−K\n... ... ... ...\nxi xi ··· xi\n... ... ... ...\nxi+K xi+K ··· xi+K\nxi+K+1 xi+K+1 ··· xi+K+1\n\n\nP⊤\nk PqC3\n\n\nxi−1\nxi\nxi+1\n\n. (27)\nHere, C’ is a (2K + 1)-row matrix and can be easily derived from the convolutional kernel C3.\nThe (2K+ 1) weights, {wi−1,wi,wi+1}, correspond to the (2K+ 1) rows in C, respectively. This\nmeans that the three positions are differentiated and the same position in each window corresponds to\nthe same row. This explains why the positional embeddings are not necessary when convolutions are\nadopted (Wu et al., 2021). Using different pairs (Wq,Wk) leads to more weights for each position,\ne.g., M pairs correspond to M-head attention.\nD A RCHITECTURE DETAILS\nOverall structures. Following local vision transformer, Swin Transformer (Liu et al., 2021b), we\nbuild two depth-wise convolution-based networks, namely DWNet-T and DWNet-B. The corre-\nsponding dynamic versions are dynamic DWNet-T, dynamic DWNet-B, i-dynamic DWNet-T, and\ni-dynamic DWNet-B. The depth-wise convolution-based networks follow the overall structure of\nSwin Transformer. We replace local self attention by depth-wise convolution with the same window\nsize. We use batch normalization (Ioffe & Szegedy, 2015) and ReLU (Nair & Hinton, 2010) instead\nof layer normalization (Ba et al., 2016) in the convolution blocks.\n20\nPublished as a conference paper at ICLR 2022\nTable 8: Architectures details of Swin Transformer and DWNet for the tiny model. The architectures\nfor the base model can be easily obtained.\ndownsp. rate\n(output size) Swin DWNet\nstage 1 4×\n(56×56)\nconcat 4×4, linear 96-d, LN concat 4×4, linear 96-d, LN\n\nLN, linear 96x3-d\nlocal sa. 7×7, head 3\nlinear 96-d\nLN, linear 384-d\nGELU, linear 96-d\n\n\n× 2\n\n\nlinear 96-d, BN, ReLU\ndepthwise conv. 7×7, BN, ReLU\nlinear 96-d, BN\nBN, linear 384-d\nGELU, linear 96-d\n\n\n× 2\nstage 2 8×\n(28×28)\nconcat 2×2, linear 192-d , LN concat 2×2, linear 192-d , LN\n\nLN, linear 192x3-d\nlocal sa. 7×7, head 6\nlinear 192-d\nLN, linear 768-d\nGELU, linear 192-d\n\n\n× 2\n\n\nlinear 192-d, BN, ReLU\ndepthwise conv. 7×7, BN, ReLU\nlinear 192-d, BN\nBN, linear 768-d\nGELU, linear 192-d\n\n\n× 2\nstage 3 16×\n(14×14)\nconcat 2×2, linear 384-d , LN concat 2×2, linear 384-d , LN\n\nLN, linear 384x3-d\nlocal sa. 7×7, head 12\nlinear 384-d\nLN, linear 1536-d\nGELU, linear 384-d\n\n\n× 6\n\n\nlinear 384-d, BN, ReLU\ndepthwise conv. 7×7, BN, ReLU\nlinear 384-d, BN\nBN, linear 1536-d\nGELU, linear 384-d\n\n\n× 6\nstage 4 32×\n(7×7)\nconcat 2×2, linear 768-d , LN concat 2×2, linear 768-d , LN\n\nLN, linear 768x3-d\nlocal sa. 7×7, head 24\nlinear 768-d\nLN, linear 3072-d\nGELU, linear 768-d\n\n\n× 2\n\n\nlinear 768-d, BN, ReLU\ndepthwise conv. 7×7, BN, ReLU\nlinear 768-d, BN\nBN, linear 3072-d\nGELU, linear 768-d\n\n\n× 2\nstage 4 1×1 LN, AvgPool. 1×1 LN, AvgPool. 1×1\nlinear classiﬁer linear classiﬁer\nTable 8 shows the architecture details of Swin Transformer and DWNet for the tiny model. Nor-\nmalizations are performed within the residual block, same as Swin Transformer. The base model is\nsimilarly built by following Swin Transformer to change the number of channels and the depth of the\nthird stage.\nDynamic depth-wise convolution. Dynamic depth-wise convolution generates the connection\nweights according to the instance. As described in Section 2.4, for the homogeneous version, we\nconduct the global average pooling operation to get a vector, and adopt two linear projections: the ﬁrst\none reduces the dimension by 1/4, followed by BN and ReLU, and then generate the kernel weights\nand shared for all spatial positions. Unlike SENet (Hu et al., 2018b), we currently do not use the\nSigmoid activation function for generating the weights. For the inhomogeneous version, we generate\nunshared dynamic weight for each spatial position using the corresponding feature. The connection\nweights are shared across channels to reduce the model parameters and computation complexity.\nSpeciﬁcally, we share 3 and 4 channels in each group of channels for tiny and base models. Thus the\nnumber of model parameters and computation complexity are similar to Swin Transformer.\n21\nPublished as a conference paper at ICLR 2022\nTable 9: ImageNet classiﬁcation comparison for ResNet, HRNet, Mixer and ResMLP and gMLP, ViT and DeiT,\nSwin (Swin Transformer), DWNet, dynamic DWNet and i-dynamic DWNet. † means that ResNet is built by\nusing two 3 × 3 convolutions to form the residual units. Table 7 presents the comparison for representative\nmodules in terms of spare connectivity, weight sharing and dynamic weight.\nmethod img. size #param. FLOPs throughput (img. / s) top-1 acc. real acc.\nConvolution: local connection\nResNet-38 † (Wang et al., 2020) 2242 28M 3.8G 2123.7 75.4 -\nResNet-72 † (Wang et al., 2020) 2242 48M 7.5G 623.0 76.7 -\nResNet-106 † (Wang et al., 2020) 2242 65M 11.1G 452.8 77.3 -\nBottleneck: convolution with low rank\nResNet-50 (He et al., 2016) 2242 26M 4.1G 1128.3 76.2 82.5\nResNet-101 (He et al., 2016) 2242 45M 7.9G 652.0 77.4 83.7\nResNet-152 (He et al., 2016) 2242 60M 11.6G 456.7 78.3 84.1\nPyramid: convolution with pyramid (spatial low rank) features.\nHRNet-W18 (Wang et al., 2020) 2242 21M 4.0G - 76.8 -\nHRNet-W32 (Wang et al., 2020) 2242 41M 8.3G - 78.5 -\nHRNet-W48 (Wang et al., 2020) 2242 78M 16.1G - 79.3 -\nChannel and spatial separable MLP , spatial separable MLP = point-wise1 × 1 convolution\nMixer-B/16 (Tolstikhin et al., 2021) 2242 46M - - 76.4 82.4\nMixer-L/16 (Tolstikhin et al., 2021) 2242 189M - - 71.8 77.1\nResMLP-12 (Touvron et al., 2021) 2242 15M 3.0G - 76.6 83.3\nResMLP-24 (Touvron et al., 2021) 2242 30M 6.0G - 79.4 85.3\nResMLP-36 (Touvron et al., 2021) 2242 45M 8.9G - 79.7 85.6\ngMLP-Ti (Liu et al., 2021a) 2242 6M 1.4G - 72.0 -\ngMLP-S (Liu et al., 2021a) 2242 20M 4.5G - 79.4 -\ngMLP-B (Liu et al., 2021a) 2242 73M 15.8G - 81.6 -\nGlobal attention: dynamic channel separable MLP + spatial separable MLP\nViT-B/16 (Dosovitskiy et al., 2021) 3842 86M 55.4G 83.4 77.9 83.6\nViT-L/16 (Dosovitskiy et al., 2021) 3842 307M 190.7G 26.5 76.5 82.2\nDeiT-S (Touvron et al., 2020) 2242 22M 4.6G 947.3 79.8 85.7\nDeiT-B (Touvron et al., 2020) 2242 86M 17.5G 298.2 81.8 86.7\nDeiT-B (Touvron et al., 2020) 3842 86M 55.4G 82.7 83.1 87.7\nPyramid attention: perform attention with spatial low rank\nPVT-S (Wang et al., 2021b) 2242 25M 3.8G - 79.8 -\nPVT-M (Wang et al., 2021b) 2242 44M 6.7G - 81.2 -\nPVT-L (Wang et al., 2021b) 2242 61M 9.8G - 81.7 -\nLocal MLP: perform static separable MLP in local small windows\nSwin-Local MLP-T 2242 26M 3.8G 861.0 80.3 86.1\nSwin-Local MLP-B 2242 79M 12.9G 321.2 82.2 86.9\nLocal attention: perform attention in local small windows\nSwin-T (Liu et al., 2021b) 2242 28M 4.5G 713.5 81.3 86.6\nSwin-B (Liu et al., 2021b) 2242 88M 15.4G 263.0 83.3 87.9\nDepth-wise convolution + point-wise1 × 1 convolution\nDWNet-T 2242 24M 3.8G 928.7 81.3 86.8\nDWNet-B 2242 74M 12.9G 327.6 83.2 87.9\ndynamic DWNet-T 2242 51M 3.8G 897.0 81.9 87.3\ndynamic DWNet-B 2242 162M 13.0G 322.4 83.2 87.9\ni-dynamic DWNet-T 2242 26M 4.4G 685.3 81.8 87.1\ni-dynamic DWNet-B 2242 80M 14.3G 244.9 83.4 88.0\nE S ETTING DETAILS\nImageNet pretraining. We use the identical training setting with Swin Transformer in ImageNet pre-\ntraining for fair comparison. The default input size is 224 ×224. The AdamW optimizer (Loshchilov\n& Hutter, 2019), with the initial learning rate0.001 and the weight decay 0.05, is used for 300 epochs.\nThe learning rate is scheduled by a cosine decay schema and warm-up with linear schema for the\n22\nPublished as a conference paper at ICLR 2022\nﬁrst 20 epochs. We train the model on 8 GPUs with the total batch size 1024. The augmentation and\nregularization strategies are same as Swin Transformer, which includes RandAugment (Cubuk et al.,\n2020), Mixup (Zhang et al., 2018a), CutMix (Yun et al., 2019), random erasing (Zhong et al., 2020)\nand stochastic depth (Huang et al., 2016). The stochastic depth rate is employed as 0.2 and 0.5 for\nthe tiny and base models, respectively, the same as Swin Transformer.\nCOCO object detection. We follow Swin Transformer to adopt Cascade Mask R-CNN (Cai &\nVasconcelos, 2019) for comparing backbones. We use the training and test settings from Swin\nTransformer: multi-scale training - resizing the input such that the shorter side is between 480 and\n800 and the longer side is at most 1333; AdamW optimizer with the initial learning rate 0.0001;\nweight decay - 0.05; batch size - 16; and epochs - 36.\nADE semantic segmentation. Following Swin Transformer, we use UPerNet (Xiao et al., 2018)\nas the segmentation framework. We use the same setting as the Swin Transformer: the AdamW\noptimizer with initial learning rate 0.00006; weight decay 0.01; linear learning rate decay; 160,000\niterations with warm-up for 1500 iterations; 8 GPUs with mini-batch 2 per GPU. We use the same\ndata augmentation as Swin Transformer based on MMSegmentation (Contributors, 2020). The\nexperimental results are reported as single scale testing.\nStatic version of Swin Transformer - Local MLP. We remove the linear projections applied to\nkeys and queries, accordingly dot production and softmax normalization. The connection weights\n(corresponding to attention weights in the dynamic version) are set as static model parameters which\nare learnt during the training and shared for all the images.\nRetraining on 384 ×384. We retrain the depth-wise convolution-based network on the ImageNet\ndataset with 384 ×384 input images from the model trained with 224 ×224 images. We use learning\nrate 10−5, weight decay 10−8 and stochastic depth ratio 0.1 for 30 epochs for both 7 ×7 and 12 ×12\nwindows.\nF A DDITIONAL EXPERIMENTS AND ANALYSIS\nMore results on ImageNet classiﬁcation. We give more experimental results with different sparse\nconnection strategies, as shown in Table 9. These results also verify that locality-based sparsity pattern\n(adopted in depth-wise convolution and local attention) besides sparsity between channels/spatial\npositions still facilitates the network training for ImageNet-1K.\nResults on large scale pre-training. Transformers (Liu et al., 2021b; Dosovitskiy et al., 2021)\nshow higher performance compared with the previous convolutional networks with large scale pre-\ntraining. We further study the performance on ImageNet-22K pre-training. We ﬁrst train the model\non ImageNet-22K dataset which has about 14.2 million images, and then ﬁne-tune the model on\nImageNet-1K classiﬁcation, downstream detection and segmentation tasks. The same training settings\nwith Swin transformer are used in all tasks. The ﬁne-tuning results in Table 10 and Table 11 indicate\nthe dynamic convolution based DWNets could get the performance comparable to Swin transformer\nwith large scale pre-training.\nCooperating with different normalization functions. Transformers usually use the layer normal-\nization to stabilize the training, while convolutional architectures adopt batch normalization. We\nverify different combinations of backbones (Swin and DWNet) and normalization functions. The\npopular used layer normalization (LN), batch normalization (BN), and the dynamic version of batch\nTable 10: Comparison on ImageNet-1K classiﬁcation with ImageNet-22K pre-training.\nImageNet-1K ﬁne-tuning\n#param. FLOPs top-1 acc.\nSwin-B 88M 15.4G 85.2\nDWNet-B 74M 12.9G 84.8\ndynamic DWNet-B 162M 13.0G 85.0\ni-dynamic DWNet-B 80M 14.3G 85.2\n23\nPublished as a conference paper at ICLR 2022\nTable 11: Comparison results on COCO object detection and ADE semantic segmentation with\nImageNet-22k pre-training.\nCOCO ﬁne-tuning ADE20K ﬁne-tuning\n#param. FLOPs APbox APbox\n50 APbox\n75 APmask #param. FLOPs mIoU\nSwin-B 145M 986G 53.4 72.1 58.1 46.1 121M 1192G 49.4\nDWNet-B 132M 924G 52.0 70.4 56.3 45.0 108M 1129G 50.1\ndynamic DWNet-B 219M 924G 51.9 70.7 56.2 45.0 195M 1129G 49.6\ni-dynamic DWNet-B 137M 948G 52.9 71.2 57.2 45.8 114M 1153G 51.3\nTable 12: Exploring normalization schemes of Swin Transformer and depth-wise convolution based\nnetworks (DWNet) for the tiny model. The results are reported on the ImageNet top-1 accuracy.\nLayer Norm. Batch Norm. Centering calibrated Batch Norm. Top-1 Acc.\nSwin \u0013 81.3\nSwin \u0013 80.9\nSwin \u0013 81.2\nDWNet \u0013 81.2\nDWNet \u0013 81.3\nDWNet \u0013 81.7\nnormalization - centering calibrated batch normalization (Gao et al., 2021) (CC. BN) are veriﬁed in\nthe experiments. Table 12 shows the results on ImageNet classiﬁcation.\nDepth-wise convolution with other architectures.We conduct experiments on other local attention\ndesigns, such as SVT (Chu et al., 2021a) and VOLO (Yuan et al., 2021c) whose implementations\nare publicly available. SVT uses local self attention as a basic spatial feature fusion operation, while\nVOLO proposes a new attention module named Vision Outlooker. We replace the local self attention\nwith depth-wise convolution in SVT same as the paper, and replace Vision Outlooker with7 ×7 local\nself attention and 7 ×7 depth-wise convolution, respectively. The remaining structures are unchanged\nand the same training setting is used as the original papers. The experimental results are shown in\nTab 13 and the observations are the same as the Swin Transformer design.\nRetraining on 384 ×384 images. Similar to (Liu et al., 2021b), we study the performance of ﬁne-\ntuning the models: ﬁrst learn with 224 ×224 images, then ﬁne-tune on large images of 384 ×384.\nWe study two cases: (1) keep the window size 7 ×7 unchanged; and (2) upsample the kernel weights\nfrom 7×7 to 12×12 as done in (Liu et al., 2021b) for upsampling the relative positional embeddings.\nThe results are in Table 14 7. In the case of keeping the window size 7 ×7 unchanged, DWNet\nperforms better. When using a larger window size 12 ×12, depth-wise convolution performs worse\nthan 7 ×7. We suspect that this is because upsampling the kernel weights is not a good starting for\nﬁne-tuning. In Swin Transformer, using a larger window size improves the performance. We believe\nthat this is because the local attention mechanism is suitable for variable window sizes.\nCooperating with SE. Squeeze-and-excitation (Hu et al., 2018b) (SE) is a parameter- and\ncomputation-efﬁcient dynamic module, initially designed for improving the ResNet performance.\nThe results in Table 15 show that DWNet, a static module, beneﬁts from the SE module, while Swin\nTransformer, already a dynamic module, does not beneﬁt from dynamic module SE. The reason is\nstill unclear, and might lie in the optimization.\nG P OTENTIAL STUDIES\nComplexity balance between point-wise ( 1 ×1) convolution and depth-wise (spatial) convolu-\ntion. Depth-wise convolution takes only about 2% computation in the depth-wise convolution-based\narchitecture. The major computation complexity comes from 1 ×1 convolutions. The solutions to\nthis issue could be: group 1 ×1 convolution studied in IGC (Zhang et al., 2017; Sun et al., 2018), and\n7Swin Transformer takes slightly higher FLOPs for 7 × 7 than 12 × 12. The higher computation cost comes\nfrom larger padding than 12 × 12.\n24\nPublished as a conference paper at ICLR 2022\nTable 13: Comparison between local attention and depth-wise convolution in VOLO (Yuan et al.,\n2021c) and SVT (Chu et al., 2021a) architecture. Results are reported on ImageNet classiﬁcation\nwith tiny model.\n#param. FLOPs top-1 acc.\nVOLO-d1 (Yuan et al., 2021c) 27M 7.0G 84.1\nVOLO (Local SA)-d1 27M 7.2G 84.2\nDW Conv.-d1 26M 6.9G 84.2\nSVT-S (Chu et al., 2021a) 24M 2.8G 81.7\nDW Conv.-S 22M 2.7G 81.9\nTable 14: Retrain on larger images.\nmodel ws. #param. FLOPs Acc.\nSwin 7×7 28M 14.4G 81.8\n12×12 28M 14.2G 82.4\nDWNet 7×7 24M 11.1G 82.2\n12×12 25M 11.5G 82.1\nTable 15: Cooperate with SE.\nmodel SE #param. FLOPs Acc.\nSwin 28M 4.5G 81.3\n\u0013 29M 4.5G 81.2\nDWNet 24M 3.8G 81.3\n\u0013 24M 3.8G 81.7\nchannel-wise weighting (like SENet) studied in Lite-HRNet (Yu et al., 2021) and EfﬁcientNet (Tan &\nLe, 2019; 2021), or simply add more depth-wise (spatial) convolutions.\nAttention weights as channel maps. Attention weights in attention can be regarded as channel maps.\nThe operations, such as convolution or simple weighting, can be applied to the attention weights. The\nresT approach (Zhang & Yang, 2021) performs 1 ×1 convolutions over the attention weight maps.\nDynamic weights. In Swin Transformer and our developed dynamic depth-wise convolution net-\nworks, only the spatial part, attention and depth-wise convolution, explores dynamic weights. Lite-\nHRNet instead studies dynamic weight for point-wise (1 ×1) convolution. It is interesting to explore\ndynamic weight for both parts.\nConvolution-style MLP weights. The weights of the spatial-mixing MLP in MLP-Mixer and\nResMLP could be modiﬁed in the convolution-like style with more weights (some like the relative\nposition embeddings used in local attention, larger than the image window size) so that it could be\nextended to larger images and downstream tasks with different image sizes.\n25",
  "topic": "Computation",
  "concepts": [
    {
      "name": "Computation",
      "score": 0.5766404271125793
    },
    {
      "name": "Computer science",
      "score": 0.5371361970901489
    },
    {
      "name": "Transformer",
      "score": 0.5343697667121887
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4989805221557617
    },
    {
      "name": "Segmentation",
      "score": 0.4116480350494385
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3954218626022339
    },
    {
      "name": "Algorithm",
      "score": 0.31821197271347046
    },
    {
      "name": "Engineering",
      "score": 0.13998180627822876
    },
    {
      "name": "Voltage",
      "score": 0.10074540972709656
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}