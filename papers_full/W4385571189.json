{
  "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
  "url": "https://openalex.org/W4385571189",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2127969602",
      "name": "Jiang Dongfu",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2563984569",
      "name": "Bill Yuchen Lin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3102187933",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4300537273",
    "https://openalex.org/W3173210704",
    "https://openalex.org/W4385573512",
    "https://openalex.org/W4389524372",
    "https://openalex.org/W2760656271",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2143331230",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W4285222512",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2789758093",
    "https://openalex.org/W2999791086",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2115584760",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2557503240",
    "https://openalex.org/W1600537614",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3082928416",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2936695845"
  ],
  "abstract": "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 14165‚Äì14178\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nLLM-BLENDER : Ensembling Large Language Models\nwith Pairwise Ranking and Generative Fusion\nDongfu Jiang‚àë Xiang Ren‚à´œÄ Bill Yuchen LinœÄ\ndongfu@zju.edu.cn, xiangren@usc.edu, yuchenl@allenai.org\nœÄAllen Institute for ArtiÔ¨Åcial Intelligence\n‚à´University of Southern California ‚àëZhejiang University\nAbstract\nWe present LLM-BLENDER , an ensembling\nframework designed to attain consistently su-\nperior performance by leveraging the diverse\nstrengths of multiple open-source large lan-\nguage models (LLMs). Our framework con-\nsists of two modules:PAIR RANKER and GEN-\nFUSER , addressing the observation that opti-\nmal LLMs for different examples can signif-\nicantly vary. PAIR RANKER employs a spe-\ncialized pairwise comparison method to dis-\ntinguish subtle differences between candidate\noutputs. It jointly encodes the input text and\na pair of candidates, using cross-attention en-\ncoders to determine the superior one. Our re-\nsults demonstrate thatPAIR RANKER exhibits\nthe highest correlation with ChatGPT-based\nranking. Then,GENFUSER aims to merge the\ntop-ranked candidates, generating an improved\noutput by capitalizing on their strengths and\nmitigating their weaknesses. To facilitate large-\nscale evaluation, we introduce a benchmark\ndataset, MixInstruct, which is a mixture\nof multiple instruction datasets featuring oracle\npairwise comparisons. Our LLM-BLENDER\nsigniÔ¨Åcantly outperform individual LLMs and\nbaseline methods across various metrics, estab-\nlishing a substantial performance gap.1 2\n1 Introduction\nLarge language models (LLMs) have shown im-\npressive performance in diverse tasks, primarily\ndue to their capacity to follow instructions and ac-\ncess extensive, high-quality data, showing a promis-\ning future for artiÔ¨Åcial general intelligence (Bubeck\net al., 2023). However, prominent LLMs such as\nGPT-4 and PaLM (Chowdhery et al., 2022) are\nclosed-source, restricting insights into their archi-\ntectures and training data. Open-source LLMs like\n1https://yuchenlin.xyz/LLM-Blender\n2The experiments on summarization, translation, and con-\nstrained generation tasks in the prior version have been moved\nto the appendix. Instead, we mainly present our work in the\ncontext of instruction-following data and LLMs in this version.\nOpen Assistant12.61%Koala6.71%Alpaca11.61%Baize11.61%StableLM1.90%FLAN-T50.80%Vicuna21.22%\nDolly V24.50%\nMOSS12.91%\nChatGLM8.51%MPT7.61%\nPercentageof Examples Where Each Model Ranks First\nWhich LLM should I useformyinput?All! I can ensemble!Figure 1:Motivation of ensembling LLMs.Based on\nthis pie chart about the percentage of examples where\neach LLM ranks 1st, we can see that optimal LLMs for\ndifferent examples can signiÔ¨Åcantly vary.\nPythia (Biderman et al., 2023), LLaMA (Touvron\net al., 2023), and Flan-T5 (Chung et al., 2022) of-\nfer a chance to Ô¨Åne-tune these models on custom\ninstruction datasets, enabling the development of\nsmaller yet efÔ¨Åcient LLMs, such as Alpaca, Vi-\ncuna (Chiang et al., 2023), OpenAssistant (LAION-\nAI, 2023), and MPT (MosaicML, 2023).\nThe open-source LLMs exhibit diverse strengths\nand weaknesses due to variations in data, archi-\ntectures, and hyperparameters, making them com-\nplementary to each other. Figure1 illustrates the\ndistribution of best LLMs on 5,000 instructions that\nwe collected. More ranking details can be found\nin Sec.5.1. Although Vicuna achieves the highest\npercentage, it ranks Ô¨Årst in only 21.22% of the ex-\namples. Furthermore, the pie chart suggests that\nthe optimal LLMs for different examples can sig-\nniÔ¨Åcantly vary and there is no open-source LLM\n14165\nthat dominates the competition. Therefore, it is\nimportant to dynamically ensemble these LLMs\nto generate consistently better responses for each\ninput. Considering the diverse strengths and weak-\nnesses of LLMs, it is crucial to develop an ensem-\nbling method that harnesses their complementary\npotentials, leading to improved robustness, gener-\nalization, and accuracy. By combining their unique\ncontributions, we can alleviate biases, errors, and\nuncertainties in individual LLMs, resulting in out-\nputs better aligned with human preferences.\nWe introduce\n LLM-BLENDER , an ensem-\nbling framework designed to achieve consistently\nsuperior performance by mixing the outputs of\nmultiple LLMs. LLM-BLENDER comprises two\nmodules: PAIR RANKER and GENFUSER . Ini-\ntially, PAIR RANKER compares the outputs from\nN LLMs, whichGENFUSER then fuses to gener-\nate the Ô¨Ånal output from the topK ranked outputs.\nExisting approaches (Ravaut et al., 2022a; Liu\nand Liu, 2021), including the reward model within\nInstructGPT (Ouyang et al., 2022), for ranking out-\nputs {y1,...,y N } from language models (LMs) on\na given inputx have mostly focused onindividually\nscoring each yi based onx, employing encoding\nmodules in the form ofsi = f\u0000(x, yi). Although\nthis list-wise ranking objective can be powerful\nand efÔ¨Åcient when candidate differences are appar-\nent, it may not be as effective when ensembling\nLLMs. Among the output candidates from LLMs,\ncandidate differences can be quitesubtle, as they\nare all produced by very sophisticated models and\none may only be marginally better than another.\nEven for humans, it can be challenging to gauge\ncandidate quality without direct comparison.\nAs a result, we propose a specializedpairwise\ncomparison method, PAIR RANKER (Sec. 3), to\neffectively discern subtle differences between can-\ndidate outputs and enhance ranking performance.\nIn particular, we Ô¨Årst gather the outputs fromN\nmodels (e.g., theN = 11 models in Fig.1) for each\ninput and subsequently create theN(N \u0000 1)/2\npairs of their outputs. We jointly encode the input\nx and the two candidate outputsyi and yj as input\nto a cross-attention encoder (e.g., RoBERTa (Liu\net al., 2019)), in the form off\u0000(x, yi,y j), to learn\nand determine which candidate is better.\nDuring the inference stage, we compute a ma-\ntrix containing logits representing pairwise com-\nparison results. Given this matrix, we can infer\na ranking of theN outputs for the given inputx.\nSubsequently, we can employ the top-ranked can-\ndidate from PAIR RANKER for each input as the\nÔ¨Ånal result. Hence, this approach does not rely\non a single model for all examples; instead,PAIR -\nRANKER selects the best model for each example\nby comprehensively comparing all candidate pairs.\nNonetheless, this approach may constrain the\npotential to generate even better outputs than the\nexisting candidates. To investigate this possibility,\nwe introduce theGENFUSER (Sec. 4) module to\nfuse the topK of theN ranked candidates and gen-\nerate an improved output for end-users. Our goal is\nto capitalize on the strengths of the topK selected\ncandidates while mitigating their weaknesses.\nTo assess the effectiveness of LLM ensembling\nmethods, we introduce a benchmark dataset called\nMixInstruct (Sec. 2.2). In this dataset, we\nuse N=11 popular open-source LLMs to generate\nN candidates for each input across various exist-\ning instruction-following tasks formatted as self-\ninstruct (Wang et al., 2022). The dataset comprises\n100k training examples and 5k validation examples\nfor training a candidate ranking module like our\nPAIR RANKER , and 5k test examples with oracle\ncomparisons for automatic evaluation.\nIn Section 5, our empirical results on\nthe MixInstruct benchmark reveal that the\nLLM-BLENDER framework signiÔ¨Åcantly boosts\noverall performance by ensembling LLMs. The\nselections made byPAIR RANKER outperform any\nÔ¨Åxed individual LLM models, as indicated by su-\nperior performance in both reference-based met-\nrics and GPT-Rank. By leveraging the top selec-\ntions fromPAIR RANKER , GENFUSER further en-\nhances response quality through effective fusion\ninto the Ô¨Ånal output. LLM-BLENDER achieves\nthe highest scores in terms of both conventional\nmetrics (i.e., BERTScore, BARTScore, BLUERT)\nand ChatGPT-based ranking. The average rank\nof LLM-BLENDER stands at 3.2 among the 12\nmethods, which is considerably better than the best\nLLM‚Äôs rank of 3.90. Moreover,LLM-BLENDER ‚Äôs\noutput ranks in the top 3 for 68.59% of examples,\nwhile Viccuna only reaches 52.88%. We believe\nLLM-BLENDER and our Ô¨Åndings would beneÔ¨Åt\nboth practitioners and researchers for deploying\nand studying LLMs with ensemble learning.\n2 Preliminaries\nWe Ô¨Årst provide the problem formulation and two\ncommon types of ensembling methods. Next, we\n14166\nComparison Result\nLLM N\nLLM 1\nLLM 2.‚Ä¶Input: ! \"!\"\"\n\"#\n!+\"!+\"\"\n!+\"#+\"#$!\n!+\"!+\"%\n.‚Ä¶!+\"!+\"#.‚Ä¶.‚Ä¶\nPairRankerCandidate PairsCandidates !+#(\"#$)+#&'(+#()*()\nInput + Top K Cand.\nOutput: $\"\nGenFuser\nModels\n 1234561,1\nN,1\n2,3\nN,Nrankfuse\nLLM-Blender\nFigure 2: The LLM-BLENDER framework. For each inputx from users, we employN different LLMs to get\noutput candidates. Then, we pair all candidates and concatenate them with the input before feeding them to\nPAIR RANKER , producing a matrix as comparison results. By aggregating the results in the matrix, we can then rank\nall candidates and take the topK of them for generative fusion. TheGENFUSER module concatenates the inputx\nwith theK top-ranked candidates as input and generate the Ô¨Ånal outputÀÜy.\npresent the dataset MixInstruct created for\ntraining and evaluation purposes. Finally, we give\nan overview of our framework.\n2.1 Problem Setup\nGiven an inputx and N models, {M1,..., MN },\nwe can generateN candidate outputs by processing\nx with each model. We denote the candidates as\nY = {y1,...,y N }. In the training data, we assume\nthere is a ground truth output,y, while it remains\nhidden during evaluation at test time.\nIn practice, one might choose a Ô¨Åxed model, such\nas M9, to infer all unseen examples (i.e., always\nusing y9 as the Ô¨Ånal output forx). This can be\nreasonable ifM9 demonstrates signiÔ¨Åcantly better\noverall performance on certain observed examples.\nHowever, relying on a pre-selected model may re-\nsult in sub-optimal performance, as theN models\nlikely possess different strengths and weaknesses in\nvarious situations, meaning that the optimal selec-\ntion for differentx values may not always originate\nfrom the same model.\nOur objective is to develop an ensemble learning\nmethod that produces an outputÀÜy for the inputx,\nmaximizing the similarityQ(ÀÜy,y ; x). TheQ func-\ntion can be implemented in various ways, which we\nwill discuss later. We anticipate that this method\nwill yield better overall performance than using a\nÔ¨Åxed model or randomly selecting a model forx.\nSpeciÔ¨Åcally, given a test setDtest = {(x(i),y (i))},\nwe aim to maximize<i Q(ÀÜy(i),y (i); x(i)).\nThere are two primary approaches for ensem-\nbling LLMs:selection-based and generation-based\nmethods. Selection-based methods compare can-\ndidates in the setY, selecting the top-ranked can-\nSources #Examples Source I/O Tokens\nAlpaca-GPT4 22,862 GPT-4 22 / 48\nDolly-15K 7,584 Human 24 / 53\nGPT4All-LAION 76,552 ChatGPT 18 / 72\nShareGPT 3,002 ChatGPT 36 / 63\nTotal 110K Mix 20 / 66\nTable 1: Statistics of MixInstruct. It contains\n110K examples and we randomly split the dataset into\ntrain/dev/test in 100K/5K/5K sizes.\ndidate as the Ô¨Ånal outputÀÜy, which implies that\nÀÜy \" Y. Due to the inherent nature of selec-\ntion and the limited solution space, the perfor-\nmance of selection-based methods is bounded by\nthe N candidates being considered. Conversely,\ngeneration-based methods focus on fusingK can-\ndidates (1 < K & N) fromY to produce an unseen\nresponse as the Ô¨Ånal outputÀÜy.\n2.2\n MixInstruct: A New Benchmark\nWe introduce a new dataset, MixInstruct,\nto benchmark ensemble models for LLMs in\ninstruction-following tasks. We collect a large-\nscale set of instruction examples primarily from\nfour sources, as shown in Table1. After curating\nand processing this open-source data, we sample\n100k examples for training, 5k for validation, and\n5k for testing. We then runN = 11 popular open-\nsource LLMs, including Vicuna, OpenAssistant,\nAlpaca, MPT, and others (see Table2 and Figure1),\non these 110k examples.\nTo obtain the oracle ranking of candidates, we\ndesign comparative prompts for ChatGPT to evalu-\nate all candidate pairs. SpeciÔ¨Åcally, for each exam-\n14167\nple, we prepare 55 pairs of candidates (11 ‚úì 10/2).\nFor each pair, we ask ChatGPT to judge the better\ncandidate (or declare a tie). The prompt template\ncan be found in the appendix. For the training and\nvalidation sets, we provide the results based on con-\nventional metrics like BERTScore, BLEURT, and\nBARTScore. In that case, we use functionQ(yi,y )\nto estimate a candidateyi‚Äôs quality according to its\nsimilarity to the ground truthy.\n2.3\n LLM-BLENDER : A Novel Framework\nWe propose a rank-and-fuse pipeline framework,\nLLM-BLENDER , for ensembling LLMs, as illus-\ntrated in Figure 2. This framework consists of\ntwo main components: a pairwise ranking module,\nPAIR RANKER (Section 3), and a fusion module,\nGENFUSER (Section 4). ThePAIR RANKER mod-\nule learns to compare all pairs of candidates for\neach input and subsequently rank the list of can-\ndidates. We then select the topK = 3 ranked\ncandidates, concatenate them with the inputx, and\nconstruct the input sequence for theGENFUSER\nmodule. TheGENFUSER module, a seq2seq LM,\nultimately generates the Ô¨Ånal output to serve users.\n3P AIR RANKER : Pairwise Ranking\nIn this section, we introduce three baseline methods\nfor ranking the candidates inY in Sec. 3.1 and\npresent the proposed PAIR RANKER method.\n3.1 Baseline Methods\nPrevious reranking methods primarily focus on\ncomputing the scoresi = f\u0000(x, yi) for each can-\ndidate yi \" Y independently, where si is solely\ndetermined byyi. Notably, the reward model in in-\nstruction tuning for GPT-3.5 (Ouyang et al., 2022)\nalso belongs to this category. Figure3 illustrates\nthese baseline methods, which are further detailed\nin the following paragraphs.\nMLM-Scoring (Salazar et al., 2020) assesses the\nquality of a candidate by calculating its pseudo-log-\nlikelihood, which is obtained by masking tokens\none by one and computing the log-likelihood for\nthe masked token using masked LMs (e.g., BERT).\nGiven a candidateyi as a sequence of wordsW =\n{w1,. . . ,w‚àÇW‚àÇ}, the pseudo-log-likelihood is:si =\n<\n‚àÇW‚àÇ\nt=1 log P(wt‚àÇW\\t). This unsupervised method\nis effective for reranking outputs in NLG tasks such\nas machine translation and speech recognition.\nSimCLS (Liu and Liu, 2021) encodes the in-\nput x and each generated candidateyi \" Y us-\ning the same encoderH, resulting inH(x) and\nH(yi). The cosine similarity between them,si =\ncos (H(x),H (yi)), serves as the predicted score,\nas H(x) and H(yi) share the same embedding\nspace induced by the language encoder. In training,\nmarginal ranking loss is used to optimizeH.\nSummaReranker (Ravaut et al., 2022a) con-\ncatenates the inputx and each candidateyi, using\na cross-attention encoder to learn ranking. SpeciÔ¨Å-\ncally, they employH([x; yi]) to predict the score\nsi, whereH is a Transformer model. In the training\nstage, binary cross-entropy (BCE) loss is employed\nto differentiate the best candidate from the others.\nLimitations. Despite using contrastive loss in\ntraining, these methods rely on individual scoring\nfor inference. The encoders have not been exposed\nto pairs of candidates for direct comparison learn-\ning. We argue that such pointwise ranking methods\nmay be insufÔ¨Åcient for selecting the best candidates\nin the context of LLMs and instruction-following\ntasks. One reason is that the quality of LLM out-\nputs is generally high when the chosen LLMs are\npopular and competitive. Moreover, the responses\nfor instruction tasks can be quite open-ended, un-\nlike summarization tasks. Therefore, merely ex-\namining individual candidates may not yield a re-\nliable score. This issue becomes more prominent\nfor shorter responses, where sequences may dif-\nfer by only a few words but vary signiÔ¨Åcantly in\nhelpfulness, harmfulness, and fairness. Given these\nlimitations, we contend that individual scoring ap-\nproaches may fail to capture crucial nuances.\n3.2 Pairwise Comparisons\nIn order to address the limitations of pointwise\nranking, we aim to train a rankerf with parameter\n\u0000 that can compare a pair of output candidates by\nencoding them together with the input text. Our\nranker module should focus on learning to capture\nthe differences between the two candidates and\nprefer the ones of higher quality.\nGiven a pair of candidatesyi,y j, we obtain their\npair-speciÔ¨Åc scores:si\n(i,j) and sj\n(i,j). We denote the\nmodel‚Äôs conÔ¨Ådence in thinkingyi is better thanyj\nas sij = si\n(i,j) \u0000 sj\n(i,j). We can use these scores for\nall pairs induced fromY to infer the Ô¨Ånal ranking.\nTo learn this ability, we concatenate the inputx and\nthe two candidates to form a sequence[x; yi; yj]\nand feed it into a cross-attention Transformer to get\nthe features:f\u0000([x; yi; yj]) for modelingsij.\nWe assume multipleQ functions to optimize\n14168\nPairRanker\nùíáùíáùë∑ùë∑ùë∑ùë∑\nùíöùíöùüèùüè ùíöùíöùíéùíé\nùíôùíô ùíöùíö ùíöùíö\nMLM-Scoring\nùíáùíáùíéùíéùíéùíéùíéùíé\nùíöùíö ùíöùíöùíéùíé\nyùíäùíä\nSimCLS\nùíáùíáùë∫ùë∫ùë∫ùë∫\nùíöùíö ùíöùíöùíéùíé\nùíôùíô ùíöùíöùíäùíä\nSummaRerank\ner\nùíáùíáùë∫ùë∫ùë∑ùë∑\nùíôùíô ùíöùíöùíäùíä\nùíôùíô ùíôùíô\nùíöùíöùüèùüè ùíöùíöùíéùíéùíôùíô\nùíîùíîùüèùüè ùíîùíîùíéùíéùíîùíîùíäùíä\ncosine\nùíîùíîùüèùüè ùíîùíîùíéùíéùíîùíîùíäùíä\nranking loss\nùíîùíî(ùíäùíä,ùíãùíã)ùíäùíä ùíîùíî(ùíäùíä,ùíãùíã)\nùíãùíã\nBCE\nloss\nùíîùíîùüèùüè ùíîùíîùíéùíéùíîùíîùíäùíä\nùíîùíîùíÉùíÉùíÉùíÉùíîùíîùíÉùíÉ ùíîùíîùíêùíêùíÉùíÉùíêùíêùíÉùíÉùíêùíêùíîùíî\nBCE\nloss\nshuffle\nunsupervised\nFigure 3:The architectures of typical reranking methods.x is an input andci is a certain candidate, and its score\nis si. MLM-Scoring is an unsupervised method that uses an external masked LM to score a candidate; SimCLS\nuses the same encoder to encodex and each candidateci; SummaReranker instead employs a cross-encoder to\nencode bothx and ci at the same time; Our proposedPAIR RANKER encodes a pair of candidates at the same time\nfor pairwisely scoring them, and the Ô¨Ånal score of each candidate is produced as shown in Fig.4.\nfor, such as BERTScore, BARTScore, etc., and\nconsider the learning problem as a multi-task clas-\nsiÔ¨Åcation problem:\nLQ = \u0000zi log \u0000(si\n(i,j)) \u0000 (1 \u0000 zj) log \u0000(sj\n(i,j)),\nwhere \u0000 denotes the sigmoid function and\n(zi,z j) = w(1, 0),Q (yi,y ) ' Q(yj,y )\n(0, 1),Q (yi,y ) < Q(yj,y ) .\nFor optimizing towards multipleQ, we take the av-\nerage as the Ô¨Ånal multi-objective loss:L = <LQ.\n3.3 P AIR RANKER Architecture\nWe discuss the concrete designs for thePAIR -\nRANKER module in this subsection.\nEncoding. We employ Transformer layers to\nencode an input and a pair of candidates, en-\nabling the attentions to capture the difference be-\ntween candidates in the context of the input. We\nconcatenate the three segments sequentially and\nform a single input sequence with special tokens\nas separators: <source>, <candidate1>,\nand <candidate2>. The resulting input se-\nquences to Transformers are in the form of\n‚Äú<s><source> x </s> <candidate1> yi\n</s> <candidate2> yj </s>‚Äù, where x is\nthe text of a source input andyi and yj are the\ntext of two output candidates. The embeddings of\nspecial tokens<source>, <candidate1>, and\n<candidate2> are used as the representations\nof x, yi, andyj respectively, denoted asx, yi, yj.\nTraining. To determine the scores for the two\ncandidates, we concatenate the embeddings ofx\nwith yi and yj respectively, and pass them through\na single-head layer, which is a multi-layer percep-\ntron with the Ô¨Ånal layer‚Äôs dimension equal to the\nnumber ofQ functions to be optimized. Each value\nwithin this dimension represents a computedQ\nscore for a speciÔ¨ÅcQ function. We derive the Ô¨Ånal\nscore si\n(i,j) or sj\n(i,j) for the candidate by averaging\nthese Q scores. Since there areO(N2) unique pair\ncombinations, we apply an effective sub-sampling\nstrategy during the training stage to ensure learning\nefÔ¨Åciency.\nDuring training, we randomly select some com-\nbinations from the candidate poolY2, instead of\nall theN(N \u0000 1)/2 pairs. We also compare the\ntarget text with other candidates by extending the\ncandidate pool by mixing the ground truthy into Y.\nIn practice, we found that using 5 pairs per input is\nsufÔ¨Åcient for obtaining decent results.\nDue to the position embeddings of the lan-\nguage model, the order of the candidates in a\npair (x, yi,y j) matters, as the comparison result of\n14169\n(x, yi,y j) and (x, yj,y i) might not be consistent.\nThus, we shufÔ¨Çe the order of candidates within\neach training pair so that the model learns to be\nconsistent with itself.\nInference. During the inference stage, we obtain\nscores sij for each pair of candidates(yi,y j) \" Y2.\nAfter N(N \u0000 1) iterations, we obtain a matrixM,\nwhere Mj\ni = sij represents theconÔ¨Ådence that yi is\nbetter thanyj. To identify the best candidate based\non M, we introduce three aggregation functions\nfor determining the Ô¨Ånal ranking ofY.\nWe propose two scoring methods,MaxLogits\nand MaxWins, which utilize all elements in the\nmatrix. LetM√≤\ni and Mj\n√≤ denote thei-th row and\nj-th column of the matrix as vectors. For each\ncandidate yi, itsMaxLogits score is deÔ¨Åned as\nsi = <(M√≤\ni \u0000 Mi\n√≤), while itsMaxWins score is\ndeÔ¨Åned assi = ‚àÇ{sij \" M√≤\ni ‚àÇsij > 0}‚àÇ + ‚àÇ{sji \"\nMi\n√≤‚àÇsji < 0}‚àÇ, where‚àÇ‚àÇ denotes the set size.\nIn essence, MaxLogits computes the conÔ¨Å-\ndence that yi is superior to all other candidates,\nwhereas MaxWins calculates the number of victo-\nries in comparisons with other candidates.\nHowever, these two methods necessitateO(N2)\niterations forN candidates, which can be compu-\ntationally burdensome. Thus, we propose a more\nefÔ¨Åcient aggregation method, performinga single\nbubble sort runwith pairwise comparisons to se-\nlect the best candidate. We Ô¨Årst shufÔ¨Çe the order of\ncandidates inY to obtain a default order, and initial-\nize the best candidate indexk to 1. We iteratively\nupdate the best candidate index as follows:\nk = v\nk, M\ni\nk \u0000 M\nk\ni > 0\ni, M\nk\ni \u0000 M\ni\nk > 0\n.\nAfter N \u0000 1 comparisons, we selectyk as the best\ncandidate. This method reduces the inference time\ncomplexity fromO(N2) to O(N), aligning with\nprevious pointwise methods.\nRegardless of the aggregation method, we can\nrank all candidates inY. Our experiments (shown\nin the appendix) reveal thatMaxLogits yields\nthe best performance, so we useMaxLogits as\nthe default aggregator for PAIR RANKER .\n4G ENFUSER : Generative Fusion\nThe effectiveness ofPAIR RANKER is constrained\nby the quality of selections from the candidate\npool Y. We hypothesize that by merging multi-\nple top-ranked candidates, we can overcome this\n0.00 1.27 1.28 -3.93 -4.79\n-1.32 0.00 -1.69 -4.14 -4.74\n-1.40 0.12 0.00 -4.18 -4.74\n2.58 3.83 3.82 0.00 0.57\n3.53 4.36 4.33 -1.07 0.00\nùëÄùëÄ‚àó1 ùëÄùëÄ‚àó2 ùëÄùëÄ‚àó3 ùëÄùëÄ‚àó4 ùëÄùëÄ‚àó5\nùëÄùëÄ5‚àó ùëÄùëÄ4‚àó ùëÄùëÄ3‚àó ùëÄùëÄ2‚àó ùëÄùëÄ1‚àó\nùëÄùëÄ12 ùëÄùëÄ13 ùëÄùëÄ14\nùëÄùëÄ45\nùíáùíáùë∑ùë∑ùë∑ùë∑\nùíîùíîùíäùíäùíäùíä=ùíîùíî(ùíäùíä,ùíäùíä)ùíäùíä ‚àíùíîùíî(ùíäùíä,ùíäùíä)ùíäùíä\nPairRanker\n‚àë(ùëÄùëÄ4‚àó‚àíùëÄùëÄ‚àó4)=ùë†ùë†4\nùë•ùë•‚ààùëÄùëÄ4‚àóùë•ùë•>0 1+ ùë•ùë•‚ààùëÄùëÄ‚àó4ùë•ùë•<0 1=ùë†ùë†4\n{ùëÄùëÄ12,ùëÄùëÄ13,ùëÄùëÄ14,ùëÄùëÄ45}‚Üíùë†ùë†2<ùë†ùë†3<ùë†ùë†1<ùë†ùë†5<ùë†ùë†4\nMax logits\nMax wins\nBubble Sort\nthree scoring functions for PR\nFigure 4:Aggregation methods for PAIR RANKER .\nconstraint. As these top candidates often show-\ncase complementary strengths and weaknesses, it\nis plausible to generate a superior response by com-\nbining their advantages while mitigating their short-\ncomings. Our objective is to devise a generative\nmodel that takes inputx and K top-ranked candi-\ndates {y1,. . . ,yK} L Y (e.g., K = 3) and produces\nan improved outputÀÜy as the Ô¨Ånal response.\nTo accomplish this, we presentGENFUSER ,a\nseq2seq approach for fusing a set of candidates\nconditioned on the input instruction to generate an\nenhanced output. SpeciÔ¨Åcally, we concatenate the\ninput andK candidates sequentially using separa-\ntor tokens, such as<extra_id_i>, and Ô¨Åne-tune\na T5-like model to learn to generatey. In practice,\nwe employ Flan-T5-XL (Chung et al., 2022), which\nhas 3b parameters, due to its superior performance\nand relatively smaller size.\n5 Evaluation\n5.1 Setup\nWe useMixInstruct (Sec. 2.2) to conduct eval-\nuation, and more results are in the appendix.\nNLG metrics. We employ two types of eval-\nuation metrics (i.e., Q ). The Ô¨Årst group is\nconventional automatic metrics for NLG tasks:\nBERTScore (Zhang et al., 2020b), BLEURT (Sel-\nlam et al., 2020), and BARTScore (Yuan et al.,\n2021).\nGPT-Rank. The second is based on prompting\nChatGPT for pairwise comparisions on all candi-\ndates and decide their rank by the number of wins\n14170\nCategory Methods BERTScore \u0000 BARTScore\u0000 BLEURT\u0000 GPT-Rank‚á§ 'Vic(%)\u0000 'OA(%)\u0000 Top-3(%)\u0000\nLLMs\nOpenAssistant (LAION-AI,2023) 74.68 -3.45 -0.39 3.90 62.78 N/A 51.98Vicuna (Chiang et al.,2023) 69.60 -3.44 -0.61 4.13 N/A 64.77 52.88Alpaca (Taori et al.,2023) 71.46 -3.57 -0.53 4.62 56.70 61.35 44.46Baize (Xu et al.,2023) 65.57 -3.53 -0.66 4.86 52.76 56.40 38.80MOSS (Sun and Qiu,2023) 64.85 -3.65 -0.73 5.09 51.62 51.79 38.27ChatGLM (Du et al.,2022) 70.38 -3.52 -0.62 5.63 44.04 45.67 28.78Koala (Geng et al.,2023) 63.96 -3.85 -0.84 6.76 39.93 39.01 22.55Dolly V2 (Conover et al.,2023) 62.26 -3.83 -0.87 6.90 33.33 31.44 16.45Mosaic MPT (MosaicML,2023) 63.21 -3.72 -0.82 7.19 30.87 30.16 16.24StableLM (Stability-AI,2023) 62.47 -4.12 -0.98 8.71 21.55 19.87 7.96Flan-T5 (Chung et al.,2022) 64.92 -4.57 -1.23 8.81 23.89 19.93 5.32\nAnalysis\nOracle (BERTScore) 77.67 -3.17 -0.27 3.88 54.41 38.84 53.49Oracle (BLEURT) 75.02 -3.15 -0.15 3.77 55.61 45.80 55.36Oracle (BARTScore) 73.23 -2.87 -0.38 3.69 50.32 57.01 57.33Oracle (GPT-Rank) 70.32 -3.33 -0.51 1.00 100.00 100.00 100.00\nRankers\nRandom 66.36 -3.76 -0.77 6.14 37.75 36.91 29.05MLM-Scoring 64.77 -4.03 -0.88 7.00 33.87 30.39 21.46SimCLS 73.14 -3.22 -0.38 3.50 52.11 49.93 60.72SummaReranker 71.60 -3.25 -0.41 3.66 55.63 48.46 57.54PairRanker 72.97 -3.14 -0.37 3.20 54.76 57.79 65.12\nLLM-BLENDER PR (K=3) + GF 79.09 -3.02 -0.17 3.01 70.73 77.72 68.59\nTable 2: Empirical results onMixInstruct. GPT-Rank are the most important metric.\n(i.e., MaxWins aggregation). We name this GPT-\nbased ranking metric withGPT-Rank.\nModel training. We use the DeBERTa (He et al.,\n2021) (400m) as the backbone forPAIR RANKER ,\nand GENFUSER is based on Flan-T5-XL (3b).\nAccording to our ablation studies, we choose to\nuse BARTScore for its superior correlation with\nGPT-Rank as shown in5.2.\n5.2 Main results\nIn Table 2, we present the overall performance\nof N=11 LLMs as well as other methods on\nMixInstruct. In addition to the three auto met-\nrics and GPT-Rank, we also show the percentage of\nexamples where each method can produce outputs\nthat arebetter than or same goodas the two top\nLLMs, namely OpenAssistant ('OA) and Vicuna\n('Vic), in terms ofGPT-Rank.\nLLMs have diverse strengths and weakness.\nThe table presents the LLMs in a sorted order\nbased on their average rank as determined by Chat-\nGPT (GPT-Rank). Among these models, Open\nAssistant, Vicuna, and Alpaca are the top-3 per-\nformers. Following them, three renowned LLMs,\nnamely Baize, Moss, and ChatGLM, which have\nbeen Ô¨Åne-tuned using both Chinese and English in-\nstruction data, also exhibit impressive performance\non MixInstruct. Conversely, Mosaic MPT, Sta-\nbleLM, and Flan-T5 rank at the bottom-3 in the\nevaluation. Nevertheless, the averageGPT-Rank\nof top/bottom models maintain a noticeable dis-\ntance from the Ô¨Årst/last position (1 or 11), high-\nlighting the importance of ensembling LLMs.\nTop LLMs are not always good.It is evident\nthat although OA and Vic perform remarkably well,\nthere is still a substantial percentage of examples\nwhere other LLMs are considered to outperform\nthem. For instance, despite Koala having an av-\nerage GPT-Rank of 6.76, approximately40% of\nthe examples demonstrate that Koala produces re-\nsponses that are better or equally as good as both\nOA and Vic. This further emphasizes the signif-\nicance of employing ourLLM-BLENDER frame-\nwork for ranking and fusion purposes.\nNLG Metrics. Moreover, we conduct a com-\nprehensive analysis of the performance of ora-\ncle (top-1) selections based on each of the met-\nrics themselves. The Ô¨Åndings demonstrate that\nthese selections also exhibit favorable performance\nacross other metrics as well. For example, the or-\nacle selections derived fromGPT-Rank achieve\na BARTScore of\u00003.33, surpassing that of OA\n(\u00003.45). Conversely, the oracle selections of\nBARTScore yield3.69 in GPT-Rank, also signiÔ¨Å-\ncantly outperforming OA (3.90). This observation\nsubstantiates the rationality of using BARTScore\nto provide supervision forPAIR RANKER , which is\nalso suggested by Table3.\nPAIR RANKER outperforms other rankers.\nMLM-Scoring fails to outperform even random\nselection, highlighting the limitations of its un-\nsupervised paradigm. On the contrary, SimCLS,\nSummaReranker, and PAIR RANKER exhibit su-\n14171\nRanking MethodsPearson\nCorrelation\u0000\nSpearman‚Äôs\nCorrelation\u0000\nSpearman‚Äôs\nFootrule‚á§\nRandom 0.00 0.00 48.27\nBLEU 28.70 26.92 33.57\nRouge2 29.17 27.77 32.96\nBERTScore 32.25 30.33 33.34\nBLEURT 34.14 32.31 32.17\nBARTScore 38.49 36.76 30.93\nMLM-Scoring -0.02 -0.01 47.16\nSimCLS 39.89 38.13 29.32\nSummaReranker 41.13 39.10 29.69\nPairRanker 46.98 44.98 27.52\nTable 3: The correlation between each ranking method\nand oracle ranking (GPT-Rank).\nperior performance compared to the best model\n(OA) across BARTScore andGPT-Rank. No-\ntably, the average GPT-rank of the responses\nselected byPAIR RANKER (3.20) signiÔ¨Åcantly out-\nperforms the best model by0.70 (a 18% relative\nperformance gain) and also all other rankers. More-\nover, it achieves impressive results in metrics such\nas BARTScore (\u00003.14) with a substantial advan-\ntage. PAIR RANKER ‚Äôs selections are better than or\nequal to Vic/OA on54.76%/57.79% examples re-\nspectively, and ranks in top 3 for 65.12% examples.\nLLM-BLENDER is the best. We use top-3 selec-\ntions from the PAIR RANKER and feed them as\ncandidates for GENFUSER . Based on this inte-\ngration, LLM-BLENDER demonstrates remarkable\ncapabilities as expected. In terms ofGPT-Rank,\nit achieves3.01, surpassing both the best model\nOA (3.90) by a signiÔ¨Åcant margin. The scores\nfor BERTScore (79.09), BARTScore (\u00003.02), and\nBELURT (\u00000.17) all exceed the best model by\n4.41, 0.43, and0.22 respectively, showcasing sub-\nstantial advantages. Moreover, LLM-BLENDER\nalso performs well in surpassing the top two mod-\nels, Vic (70.73) and OA (77.72), thereby comple-\nmenting the weaknesses of PAIR RANKER .\nRanking correlation. In addition to focusing\nsolely on the top-1 selection of each ranker, we\npresent a comprehensive analysis of the overall\nrank correlation among all the candidates with\nGPT-Rank (see Table3). The correlation metrics\nused here include the Pearson Correlation Coef-\nÔ¨Åcient, Spearman‚Äôs Correlation, and Spearman‚Äôs\nFootrule distance(Diaconis and Graham, 1977).\nIt turns our that BARTScore gets the highest\ncorrelation withGPT-Rank against other metrics,\nwhich suggests we use BARTScore to provide su-\npervision for training. For rankers, MLM-Scoring\nstill falls short of outperforming random permuta-\ntions. On the other side, SummaReranker demon-\nstrates better correlation in terms of the Pearson\nCorrelation (41.13) and Spearman‚Äôs Correlation\n(39.10), while SimCLS gets a better Spearman‚Äôs\nFootrule distance (29.32) Notably,PAIR RANKER\nachieves the highest correlation withGPT-Rank\nacross all correlation types, which is even way bet-\nter than the BARTScore.\nMore analysis. We leave many other ablation\nstudies and analyses in Appendix, where we ap-\nply PAIR RANKER to the three typical natural lan-\nguage generation (NLG) tasks: summarization\n(CNN/DM), machine translation (WMT18-zh-en),\nand constrained text generation (CommonGen).\nWe Ô¨Ånd that PAIR RANKER still outperforms other\nmethods by a large margin in the context of us-\ning a single same base model to decodeN candi-\ndates (with different algorithms). We also show\nthat MaxLogits is much better thanMaxWins\nand the bubble sort method is very cost-effective if\nthe inference efÔ¨Åciency is a big concern.\n6 Related Work\nLLM evaluation As open-source large language\nmodels (LLMs) continue to Ô¨Çourish and demon-\nstrate remarkable competitiveness across various\nnatural language generation (NLG) tasks, assessing\nthe capabilities of LLMs has become an exceed-\ningly challenging endeavor. To address this issue,\nZheng et al.(2023) pioneered the creation of a chat-\nbot arena, enabling users to provide pairwise eval-\nuations of responses generated by two randomly\nselected LLMs. Based on these evaluations, they\nestablished an LLM Elo rating leaderboard. In a\nsimilar vein,Cabrera and Neubig(2023) conducted\nan evaluation study on a customer service dataset,\nleveraging automated metrics such as BERTScore\nand ChrF. This approach yielded similar LLM rank-\ning results. Not content with relying solely on hu-\nman evaluation, (Yidong et al., 2023) developed\na Ô¨Åne-tuned model called PandaLM to compare\nresponses generated by different LLMs. Impres-\nsively, this model achieved a accuracy of 94% when\ncompared against ChatGPT-based comparisons.\nPairwise ranking Pairwise ranking, known for\nits long-standing effectiveness, has demonstrated\nexceptional performance across a wide array of\nNLP tasks (Jamieson and Nowak, 2011). No-\ntably, Ranknet (Burges et al., 2005) and Lamb-\n14172\ndaRank (Burges, 2010) have emerged as pow-\nerful techniques for various ranking problems.\nFurthermore, within the renowned RLHF proce-\ndure(Ouyang et al., 2022), these methods incorpo-\nrate pairwise training of their reward model based\non OPT. However, these approaches still compute\nscores individually and solely undergo pairwise\ntraining at the loss level. In contrast, our proposed\nPAIR RERANKER not only employs pairwise train-\ning but also utilizes the attention mechanism for\npairwise inference during the inference stage. We\nposit that this approach better captures the sub-\ntleties between candidates and yields superior re-\nsults, as demonstrated in Section5.2.\nEnsemble learning Ensemble learning is a\nwidely employed technique to enhance a model‚Äôs\ncapabilities by leveraging multiple weaker mod-\nels (Sagi and Rokach, 2018; Anio≈Ç and Pietro¬¥n,\n2019; Wang et al., 2016). Typically, ensembling is\nperformed either by considering model weights or\nby combining diverse outputs. Recently,Izacard\nand Grave(2021) introduced a novel framework\nnamed Fusion-in-Decoder (FiD) to improve the\nquality of question answering by fusing retrieved\ntext. Building upon FiD, Ravaut et al.(2022b)\nfurther investigated the effectiveness of fusion in\nthe context of text summarization. However, they\nneglected to incorporate a selection process prior\nto feeding the candidates into the fuser, resulting\nin only moderate improvements. In contrast, our\nproposed approach, referred to asLLM-BLENDER ,\ninitially utilizes thePAIR RANKER algorithm to Ô¨Ål-\nter out candidates of poor quality. Subsequently,\nfusion is performed exclusively on the top-ranked\ncandidates, leading to superior performance.\n7 Conclusion & Future Directions\nIn this paper, we formulated the motivation to\nexploit the diverse strengths and weaknesses of\nopen-source large language models (LLMs), aim-\ning to create an ensembling framework that lever-\nages their complementary capabilities to generate\nconsistently superior results on various instruction-\nfollowing tasks. By dynamically ensembling\nLLMs, we aimed to reduce biases, errors, and un-\ncertainties in individual models, yielding outputs\nbetter aligned with human feedback.\nOur major contributions are as follows:\n‚Ä¢ A new framework:\n LLM-BLENDER is a\npost-hoc ensemble learning method for rank-\ning and fusing the outputs from multiple\nLLMs. It is composed of two modules:\nPAIR RANKER and GENFUSER , and both are\nstraightforward yet effective.\n‚Ä¢ A new dataset:\n MixInstruct is a\nbenchmark dataset, created for training and\nevaluating LLM ensembling methods on\ninstruction-following tasks.\n‚Ä¢ Promising results:We show that our method\ncan signiÔ¨Åcantly improve the overall results\non various metrics, and our Ô¨Åndings indicates\nthat this direction is promising for both re-\nsearch community and practitioners.\n‚Ä¢ Toolkit: By open-sourcing our framework,\nwe aim to make it easier for others to lever-\nage our approach, enabling the development\nof more advanced AI systems that achieve\nrobustness, generalization, and enhanced ac-\ncuracy in a wide variety of tasks.\nFuture directions. Potential future directions in-\nclude extending theLLM-BLENDER framework to\nmore types of models or even non-text modalities,\ndeveloping more sophisticated ranking and fusion\ntechniques, and investigating the transferability of\nour ensembling approach to other domains and\ntasks. Additionally, exploring ways to minimize\ncomputational overhead and incorporating active\nlearning strategies for rapid adaptation to new spe-\ncialized domains and data sources represent fruit-\nful areas for further research. Overall, our work\nunderscores the value of combining the unique con-\ntributions of multiple models.\n*Limitations\nEfÔ¨Åciency. To get the optimal performance from\nPAIR RANKER , one may need to call the model\nO(n2) times for getting the full matrix, thus result-\ning in a much less efÔ¨Åcient solution. We attempted\nto resolve this limitation by proposing to use mul-\ntiple rounds of bubble sort methods to reduce the\nnumber of inferences needed, and we Ô¨Ånd it works\npretty well. We also want to argue that although\nthe number of inferences can be large for obtaining\nthe best performance withPAIR RANKER , those in-\nferences can be executed in parallel because they\nare totally independent.\nHuman evaluation. We agree that automatic\nmetrics have limitations. Human evaluation could\n14173\nprovide us with more reliable and comprehensive\nevaluation results. However, due to the number\nof models as well as the amounts of generation\ncandidates, we cannot afford large-scale human\nevaluation. We argue that our use of ChatGPT for\nevaluation is a good alternative, according to recent\nstudies. Also, we would like to highlight that we\nshow the ground truths when using ChatGPT to do\npairwise comparisions, which is quite informative\nthan the common practice.\n*Ethical Statement\nThis work fully complies with the ACL Ethics Pol-\nicy. We declare that there are no ethical issues in\nthis paper, to the best of our knowledge.\nAcknowledgements\nWe thank members of the INK lab at USC and the\nMosaic team at AI2 for valuable feedback on this\nproject. Xiang is supported in part by the OfÔ¨Åce\nof the Director of National Intelligence (ODNI),\nIntelligence Advanced Research Projects Activ-\nity (IARPA), via the HIATUS Program contract\n#2022-22072200006, the DARPA MCS program\nunder Contract No. N660011924033, the Defense\nAdvanced Research Projects Agency with award\nW911NF-19-20271, NSF IIS 2048211, and gift\nawards from Google and Amazon. Yuchen‚Äôs re-\nsearch was also supported by the Allen Institute\nfor AI (AI2). The views and conclusions contained\nherein are those of the authors and should not be in-\nterpreted as necessarily representing the ofÔ¨Åcial\npolicies, either expressed or implied, of ODNI,\nIARPA, or the U.S. Government.\nReferences\nAnna Anio≈Ç and Marcin Pietro¬¥n. 2019. Ensemble ap-\nproach for natural language question answering prob-\nlem. 2019 Seventh International Symposium on\nComputing and Networking Workshops (CANDARW),\npages 180‚Äì183.\nStella Rose Biderman, Hailey Schoelkopf, Quentin G.\nAnthony, Herbie Bradley, Kyle O‚ÄôBrien, Eric Hal-\nlahan, Mohammad AÔ¨Çah Khan, Shivanshu Puro-\nhit, USVSN Sai Prashanth, Edward Raff, Aviya\nSkowron, Lintang Sutawika, and Oskar van der Wal.\n2023. Pythia: A suite for analyzing large language\nmodels across training and scaling. ArXiv preprint,\nabs/2304.01373.\nOndÀárej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara\nLogacheva, Christof Monz, Matteo Negri, Matt Post,\nRaphael Rubino, Lucia Specia, and Marco Turchi.\n2017. Findings of the 2017 conference on machine\ntranslation (WMT17). InProceedings of the Second\nConference on Machine Translation, pages 169‚Äì214,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nS√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan,\nJohn A. Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,\nYin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Har-\nsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and\nYi Zhang. 2023.Sparks of artiÔ¨Åcial general intelli-\ngence: Early experiments with gpt-4. ArXiv preprint,\nabs/2303.12712.\nChristopher J. C. Burges. 2010. From ranknet to lamb-\ndarank to lambdamart: An overview.\nChristopher J. C. Burges, Tal Shaked, Erin Renshaw,\nAri Lazier, Matt Deeds, Nicole Hamilton, and Gre-\ngory N. Hullender. 2005.Learning to rank using gra-\ndient descent. InMachine Learning, Proceedings of\nthe Twenty-Second International Conference (ICML\n2005), Bonn, Germany, August 7-11, 2005, volume\n119 of ACM International Conference Proceeding\nSeries, pages 89‚Äì96. ACM.\nAlex Cabrera and Graham Neubig. 2023.Zeno chatbot\nreport. Blog post.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023.Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinod-\nkumar Prabhakaran, Emily Reif, Nan Du, Benton C.\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garc√≠a,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Mor-\neira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark D√≠az, Orhan Firat, Michele Catasta, Jason Wei,\nKathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean,\nSlav Petrov, and Noah Fiedel. 2022.Palm: Scaling\nlanguage modeling with pathways. ArXiv preprint,\nabs/2204.02311.\nHyung Won Chung, Le Hou, S. Longpre, Barret Zoph,\nYi Tay, William Fedus, Eric Li, Xuezhi Wang,\n14174\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Dasha\nValter, Sharan Narang, Gaurav Mishra, Adams Wei\nYu, Vincent Zhao, Yanping Huang, Andrew M.\nDai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi,\nJeff Dean, Jacob Devlin, Adam Roberts, Denny\nZhou, Quoc V . Le, and Jason Wei. 2022.Scal-\ning instruction-Ô¨Ånetuned language models. ArXiv\npreprint, abs/2210.11416.\nMike Conover, Matt Hayes, Ankit Mathur, Xiangrui\nMeng, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi,\nPatrick Wendell, Matei Zaharia, and Reynold Xin.\n2023. Free dolly: Introducing the world‚Äôs Ô¨Årst truly\nopen instruction-tuned llm.\nPersi Diaconis and Ron Graham. 1977. Spearman‚Äôs\nfootrule as a measure of disarray.Journal of the royal\nstatistical society series b-methodological, 39:262‚Äì\n268.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022.GLM:\nGeneral language model pretraining with autoregres-\nsive blank inÔ¨Ålling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320‚Äì335,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-\nlace, Pieter Abbeel, Sergey Levine, and Dawn Song.\n2023. Koala: A dialogue model for academic re-\nsearch. Blog post.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. ArXiv preprint, abs/2111.09543.\nKarl Moritz Hermann, Tom√°s Kocisk√Ω, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015.Teaching machines to read\nand comprehend. InAdvances in Neural Information\nProcessing Systems 28: Annual Conference on Neu-\nral Information Processing Systems 2015, December\n7-12, 2015, Montreal, Quebec, Canada, pages 1693‚Äì\n1701.\nGautier Izacard and Edouard Grave. 2021.Leveraging\npassage retrieval with generative models for open do-\nmain question answering. InProceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874‚Äì880, Online. Association for Computa-\ntional Linguistics.\nKevin G. Jamieson and Robert D. Nowak. 2011.Active\nranking using pairwise comparisons. InAdvances in\nNeural Information Processing Systems 24: 25th An-\nnual Conference on Neural Information Processing\nSystems 2011. Proceedings of a meeting held 12-14\nDecember 2011, Granada, Spain, pages 2240‚Äì2248.\nLAION-AI. 2023. Open assistant. https://\ngithub.com/LAION-AI/Open-Assistant.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. InProceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871‚Äì7880, Online. Association for Computa-\ntional Linguistics.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. InFindings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823‚Äì1840,\nOnline. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv preprint, abs/1907.11692.\nYixin Liu and Pengfei Liu. 2021.SimCLS: A sim-\nple framework for contrastive learning of abstractive\nsummarization. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 2: Short\nPapers), pages 1065‚Äì1072, Online. Association for\nComputational Linguistics.\nNLP Team MosaicML. 2023.Introducing mpt-7b: A\nnew standard for open-source, ly usable llms. Ac-\ncessed: 2023-05-23.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\n√áa Àòglar Gul√ßehre, and Bing Xiang. 2016.Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of the 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280‚Äì290, Berlin, Germany.\nAssociation for Computational Linguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke E.\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul Francis Christiano, Jan Leike, and Ryan J.\nLowe. 2022. Training language models to follow\ninstructions with human feedback. ArXiv preprint,\nabs/2203.02155.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020.Exploring the limits\nof transfer learning with a uniÔ¨Åed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67.\n14175\nMathieu Ravaut, ShaÔ¨Åq Joty, and Nancy Chen. 2022a.\nSummaReranker: A multi-task mixture-of-experts\nre-ranking framework for abstractive summarization.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 4504‚Äì4524, Dublin, Ireland.\nAssociation for Computational Linguistics.\nMathieu Ravaut, ShaÔ¨Åq Joty, and Nancy Chen. 2022b.\nTowards summary candidates fusion. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 8488‚Äì8504,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nOmer Sagi and Lior Rokach. 2018. Ensemble learning:\nA survey. Wiley Interdisciplinary Reviews: Data\nMining and Knowledge Discovery, 8.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020.Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2699‚Äì2712, Online. Association for Computational\nLinguistics.\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\nBLEURT: Learning robust metrics for text genera-\ntion. InProceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7881‚Äì7892, Online. Association for Computational\nLinguistics.\nNoam Shazeer and Mitchell Stern. 2018.Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsm√§s-\nsan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages\n4603‚Äì4611. PMLR.\nStability-AI. 2023. Stablelm: Stability ai lan-\nguage models. https://github.com/\nstability-AI/stableLM.\nTianxiang Sun and Xipeng Qiu. 2023. Moss.https:\n//github.com/OpenLMLab/MOSS.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\nford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nJ√∂rg Tiedemann and Santhosh Thottingal. 2020a.\nOPUS-MT ‚Äì building open translation services for\nthe world. InProceedings of the 22nd Annual Confer-\nence of the European Association for Machine Trans-\nlation, pages 479‚Äì480, Lisboa, Portugal. European\nAssociation for Machine Translation.\nJ√∂rg Tiedemann and Santhosh Thottingal. 2020b.\nOPUS-MT ‚Äì building open translation services for\nthe world. InProceedings of the 22nd Annual Confer-\nence of the European Association for Machine Trans-\nlation, pages 479‚Äì480, Lisboa, Portugal. European\nAssociation for Machine Translation.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur‚Äôelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023.Llama: Open\nand efÔ¨Åcient foundation language models. ArXiv\npreprint, abs/2302.13971.\nBenyou Wang, Jiabin Niu, Liqun Ma, Yuhua Zhang,\nLipeng Zhang, Jingfei Li, Peng Zhang, and Dawei\nSong. 2016. A chinese question answering approach\nintegrating count-based and embedding-based fea-\ntures. InNLPCC/ICCPOL.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022.Self-instruct: Aligning lan-\nguage model with self generated instructions. ArXiv\npreprint, abs/2212.10560.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023. Baize: An open-source chat model with\nparameter-efÔ¨Åcient tuning on self-chat data. ArXiv\npreprint, abs/2304.01196.\nWang Yidong, Yu Zhuohao, Zeng Zhengran, Yang\nLinyi, Heng Qiang, Wang Cunxiang, Chen Hao,\nJiang Chaoya, Xie Rui, Wang Jindong, Xie Xing,\nYe Wei, Zhang Shikun, and Zhang Yue. 2023.\nPandalm: Reproducible and automated language\nmodel assessment. https://github.com/\nWeOpenML/PandaLM.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. InAdvances in Neural Information Processing\nSystems 34: Annual Conference on Neural Informa-\ntion Processing Systems 2021, NeurIPS 2021, De-\ncember 6-14, 2021, virtual, pages 27263‚Äì27277.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2020a.PEGASUS: pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 ofProceedings of Machine\nLearning Research, pages 11328‚Äì11339. PMLR.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020b.Bertscore: Eval-\nuating text generation with BERT. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nLianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao\nZhang, Joseph E. Gonzalez, and Ion Stoica. 2023.\nChatbot arena: Benchmarking llms in the wild with\nelo ratings. Blog post.\n14176\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nLimitations\n‚ñ°\u0013 A2. Did you discuss any potential risks of your work?\nLimitations\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nAbstract and Introduction and Conclusion\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\nSection 2\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\nmainly in Sec 4\n‚ñ° B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 2\n‚ñ° B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nin Section 2\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nSection 2 Table 1\nC ‚ñ°\u0013 Did you run computational experiments?\n4\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix A\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n14177\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nappendix E Metrics\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n14178",
  "topic": "Pairwise comparison",
  "concepts": [
    {
      "name": "Pairwise comparison",
      "score": 0.7861384153366089
    },
    {
      "name": "Computer science",
      "score": 0.7236389517784119
    },
    {
      "name": "Oracle",
      "score": 0.7163591384887695
    },
    {
      "name": "Merge (version control)",
      "score": 0.6790737509727478
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.598288357257843
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5457299947738647
    },
    {
      "name": "Machine learning",
      "score": 0.5266554355621338
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5137676000595093
    },
    {
      "name": "Generative model",
      "score": 0.48111027479171753
    },
    {
      "name": "Data mining",
      "score": 0.387803316116333
    },
    {
      "name": "Generative grammar",
      "score": 0.34951841831207275
    },
    {
      "name": "Information retrieval",
      "score": 0.25270920991897583
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Software engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ]
}