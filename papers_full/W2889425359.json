{
  "title": "A Quantum Many-body Wave Function Inspired Language Modeling Approach",
  "url": "https://openalex.org/W2889425359",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A1800355547",
      "name": "Zhang Peng",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2104040837",
      "name": "Su Zhan",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A1547405171",
      "name": "Zhang Lipeng",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2356302948",
      "name": "Wang Benyou",
      "affiliations": [
        "University of Padua"
      ]
    },
    {
      "id": "https://openalex.org/A1924050475",
      "name": "Song Da-wei",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2756668352",
    "https://openalex.org/W1915024344",
    "https://openalex.org/W2419175238",
    "https://openalex.org/W2741924718",
    "https://openalex.org/W2610935556",
    "https://openalex.org/W2536015822",
    "https://openalex.org/W1508593203",
    "https://openalex.org/W2258054274",
    "https://openalex.org/W2059059956",
    "https://openalex.org/W2055467685",
    "https://openalex.org/W2070740689",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2087746031",
    "https://openalex.org/W1966443646",
    "https://openalex.org/W2099437808",
    "https://openalex.org/W2128739123",
    "https://openalex.org/W2185337446",
    "https://openalex.org/W2160416736",
    "https://openalex.org/W2737434030",
    "https://openalex.org/W4211080241",
    "https://openalex.org/W2120735855",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W2102306213"
  ],
  "abstract": "The recently proposed quantum language model (QLM) aimed at a principled approach to modeling term dependency by applying the quantum probability theory. The latest development for a more effective QLM has adopted word embeddings as a kind of global dependency information and integrated the quantum-inspired idea in a neural network architecture. While these quantum-inspired LMs are theoretically more general and also practically effective, they have two major limitations. First, they have not taken into account the interaction among words with multiple meanings, which is common and important in understanding natural language text. Second, the integration of the quantum-inspired LM with the neural network was mainly for effective training of parameters, yet lacking a theoretical foundation accounting for such integration. To address these two issues, in this paper, we propose a Quantum Many-body Wave Function (QMWF) inspired language modeling approach. The QMWF inspired LM can adopt the tensor product to model the aforesaid interaction among words. It also enables us to reveal the inherent necessity of using Convolutional Neural Network (CNN) in QMWF language modeling. Furthermore, our approach delivers a simple algorithm to represent and match text/sentence pairs. Systematic evaluation shows the effectiveness of the proposed QMWF-LM algorithm, in comparison with the state of the art quantum-inspired LMs and a couple of CNN-based methods, on three typical Question Answering (QA) datasets.",
  "full_text": "A Quantum Many-body Wave Function Inspired\nLanguage Modeling Approach\nPeng Zhang1, Zhan Su1, Lipeng Zhang2, Benyou Wang3, Dawei Song4 âˆ—\n1 School of Computer Science and Technology, Tianjin University, Tianjin, China\n2 School of Computer Software, Tianjin University, Tianjin, China\n3 Department of Information Engineering, University of Padova, Padova, Italy\n4 School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China\n{pzhang,suzhan,lpzhang}@tju.edu.cn\nwang@dei.unipd.it;dawei.song2010@gmail.com\nABSTRACT\nThe recently proposed quantum language model (QLM) aimed at\na principled approach to modeling term dependency by applying\nthe quantum probability theory. The latest development for a more\neffective QLM has adopted word embeddings as a kind of global\ndependency information and integrated the quantum-inspired idea\nin a neural network architecture. While these quantum-inspired\nLMs are theoretically more general and also practically effective,\nthey have two major limitations. First, they have not taken into ac-\ncount the interaction among words with multiple meanings, which\nis common and important in understanding natural language text.\nSecond, the integration of the quantum-inspired LM with the neu-\nral network was mainly for effective training of parameters, yet\nlacking a theoretical foundation accounting for such integration.\nTo address these two issues, in this paper, we propose a Quantum\nMany-body Wave Function (QMWF) inspired language modeling\napproach. The QMWF inspired LM can adopt the tensor product\nto model the aforesaid interaction among words. It also enables\nus to reveal the inherent necessity of using Convolutional Neu-\nral Network (CNN) in QMWF language modeling. Furthermore,\nour approach delivers a simple algorithm to represent and match\ntext/sentence pairs. Systematic evaluation shows the effectiveness\nof the proposed QMWF-LM algorithm, in comparison with the\nstate of the art quantum-inspired LMs and a couple of CNN-based\nmethods, on three typical Question Answering (QA) datasets.\nKEYWORDS\nLanguage modeling, quantum many-body wave function, convolu-\ntional neural network\nACM Reference Format:\nPeng Zhang1, Zhan Su1, Lipeng Zhang 2, Benyou Wang3, Dawei Song4 .\n2018. A Quantum Many-body Wave Function Inspired, Language Modeling\nApproach. In The 27th ACM International Conference on Information and\nâˆ—Corresponding authors: P. Zhang and D. Song.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCIKM â€™18, October 22â€“26, 2018, Torino, Italy\nÂ© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-6014-2/18/10. . . $15.00\nhttps://doi.org/10.1145/3269206.3271723\nKnowledge Management (CIKM â€™18), October 22â€“26, 2018, Torino, Italy. ACM,\nNew York, NY, USA, 10 pages. https://doi.org/10.1145/3269206.3271723\n1 INTRODUCTION\nIt is essential to model and represent a sequence of words for many\nInformation Retrieval (IR) or Natural Language Processing (NLP)\ntasks. In general, Language Modeling (LM) approaches utilize proba-\nbilistic models to measure theuncertainty of a text (e.g., a document,\na sentence, or some keywords). Based on different probability mea-\nsures, there are roughly two different categories of LM approaches,\nnamely traditional LMs [39] based on the classical probability the-\nory, and quantum-inspired LMs [31, 40] motivated by the quantum\nprobability theory, which can be considered as a generalization of\nthe classical one [20, 30].\nRecently, Sordoni, Nie and Bengio proposed a Quantum Lan-\nguage Modeling (QLM) approach, which aims to model the term\ndependency in a more principled manner [31]. In traditional LMs,\nmodeling word dependency will increase the number of parameters\nto be estimated for compound dependencies (e.g., n-gram LM for\nIR) [28]), or involve computing additional scores from matching\ncompound dependencies in the final ranking function (e.g., Markov\nRandom Field based LM [21]). To solve these problems, QLM es-\ntimates a density matrix, which has a fixed dimensionality and\nencodes the probability measurement for both single words and\ncompound words. In addition to its theoretical benefits, QLM has\nbeen applied to ad-hoc information retrieval task and achieved\neffective performance.\nIn order to further improve the practicality of the quantum lan-\nguage models, a Neural Network based Quantum-like Language\nModel (NNQLM) was proposed [40]. NNQLM utilizes word embed-\nding vectors [22] as the state vectors, based on which a density\nmatrix can be directly derived and integrated into an end-to-end\nNeural Network (NN) structure. NNQLM has been effectively ap-\nplied in a Question Answering (QA) task. In NNQLM, a joint repre-\nsentation based on the density matrices can encode the similarity\ninformation of each question-answer pair. A Convolutional Neural\nNetwork (CNN) architecture is adopted to extract useful similarity\nfeatures from such a joint representation and shows a significant\nimprovement over the original QLM on the QA task.\nDespite the progress in the quantum-inspired LMs from both\ntheoretical and practical perspectives, there are still two major\nlimitations, in terms of the representation capacity and seamless in-\ntegration with neural networks. First, both QLM and NNQLM have\nnot modeled the complex interaction among words with multiple\narXiv:1808.09891v3  [cs.CL]  3 Sep 2018\nmeanings. For example, suppose we have two polysemous words A\nand B, in the sense that A has two meanings A1 and A2, while B\nhas two meanings B1 and B2. If we put them together and form a\ncompound word, this compound word will have four possible states\n(A1B1, A1B2, A2B1, A2B2), each corresponding to a combination of\nspecific meanings of different words. If we have more words, such\nan interaction will become more complex. However, in QLM and\nNNQLM, a compound word is modeled as a direct addition of the\nrepresentation vectors or subspaces of the single words involved.\nTherefore, it is challenging to build a language modeling mecha-\nnism which has the representation capacity towards the complex\ninteractions among words as described above.\nSecond, although in NNQLM the neural network structure can\nhelp quantum-inspired LM with effective training, the fundamen-\ntal connection between the quantum-inspired LM and the neural\nnetwork remains unclear. In other words, the integration of NN\nand QLM so far has not been in a principled manner. Hence, we\nneed to investigate and explain the intrinsic rationality of neural\nnetwork in quantum-inspired LM. It is challenging, yet important\nto bridge quantum-inspired idea, language modeling, and neural\nnetwork structure together, and develop a novel LM approach with\nboth theoretical soundness and practical effectiveness.\nIn order to address the above two challenges, we propose a\nnew language modeling framework inspired by Quantum Many-\nbody Wave Function (QMWF). In quantum mechanics, the wave\nfunction can model the interaction among many spinful particles\n(or electrons), where each particle is laying on multiple states si-\nmultaneously, and each state corresponds to a basis vector [6, 23].\nTherefore, by considering a word as a particle, different meanings\n(or latent/embedded concepts) as different basis vectors, the interac-\ntion among words can be modeled by the tensor product of different\nbasis vectors for different words. It is then natural to use such a\nQMWF formalism to represent the complex interaction system for\na sequence of natural language words.\nIn addition, we show that the convolutional neural network ar-\nchitecture can be mathematically derived in our quantum-inspired\nlanguage modeling approach. Since the tensor product is performed\nin QMWF based LM, the dimensionality of the tensor will be expo-\nnentially increased, yielding a quantum many-body problem. To\nsolve this problem, the tensor decomposition can be used to solve\na high-dimensional tensor [ 18]. With the help of tensor decom-\nposition, the projection of the global representation to the local\nrepresentation of a word sequence can result in a convolutional\nneural network architecture. In turn, for the convolutional neural\nnetwork, it can be interpreted as a mapping form a global semantic\nspace to the local semantic space.\nHence, our QMWF inspired Language Modeling (QMWF-LM)\napproach also delivers a feasible and simple algorithm to represent\na text or a sentence and match the text/sentence pairs, in both\nword-lever and character-level. We implement our approach in\nthe Question Answering task. The experiments have shown that\nthe proposed QMWF-LM can not only outperform its quantum\nLM counterparts (i.e., QLM and NNQLM), but also achieve better\nperformance in comparison with typical CNN-based approaches\non three QA datasets.\nOur main contributions can be summarized as follows:\n(1) We propose a Quantum Many-body Wave Function based\nLanguage Modeling (QMWF-LM) approach, which is able to\nrepresent complex interaction among words, each with multi-\nple semantic basis vectors (for multiple meanings/concepts).\n(2) We show a fundamental connection between QMWF based\nlanguage modeling approach and the convolutional neural\nnetwork architecture, in terms of the projection between the\nglobal representation to the local one of a word sequence.\n(3) The proposed QMWF-LM delivers an efficient algorithm to\nrepresent and match the text/sentence pairs, in both word-\nlever and character-level, as well as achieves effective per-\nformance on a number of QA datasets.\n2 QUANTUM PRELIMINARIES\nIn this section, we first describe some basis notations and concepts\nof the quantum probability theory. Then, we briefly explain the\nquantum many-body wave function.\n2.1 Basic Notations and Concepts\nThe formalism of quantum theory is actually based on vector spaces\nusing Dirac notations. In line with previous studies on the quantum-\ninspired language models [29, 31, 40], we restrict our problem to\nvectors spaces over real numbers in R.\nA wave function in quantum theory is a mathematical descrip-\ntion of the quantum state of a system. A state vector is denoted\nby a unit vector |ÏˆâŸ©(called as a ket ), which can be considered as a\ncolumn vector Â®Ïˆ âˆˆRn (for better understanding). The transpose of\n|ÏˆâŸ©is denoted as âŸ¨Ïˆ|(called as bra ), which is a row vector.\nThe state vector can be considered as arayin a Hilbert space (i.e.,\n|ÏˆâŸ©âˆˆH ), which has a set of orthonormal basis vectors |ei âŸ©(i =\n1, . . . ,n). A state vector |ÏˆâŸ©can be a superposition of the basis\nvectors:\n|ÏˆâŸ©=\nnÃ•\ni=1\nai |ei âŸ© (1)\nwhere ai is a probability amplitude and Ã\ni a2\ni = 1, since a2\ni repre-\nsents a probability of the sum to 1.\nFor example, suppose we have a two basis vectors |0âŸ©and |1âŸ©,\nwhich can be considered as (1, 0)T and (0, 1)T , respectively. Then,\nwe have a state vector\n|ÏˆâŸ©= a1 |0âŸ©+ a2 |1âŸ©\nIt means that the corresponding quantum system|ÏˆâŸ©is a superposed\nstate, i.e., it is in the two states |0âŸ©and |1âŸ©simultaneously. In the\nnatural language processing tasks, such a superposed state can be\nused to model the multiple semantic meanings of a word [3].\nIn Eq. 1, the probability amplitude ai can be calculated by the\ninner product âŸ¨ei |ÏˆâŸ©.\nai = âŸ¨ei |ÏˆâŸ©\nThe inner product âŸ¨ei |ÏˆâŸ©is a projection of |ÏˆâŸ©onto |ei âŸ©. As illus-\ntrated in Fig.1, the projection measurement can be formulated as\np(ei |Ïˆ)= a2\ni = âŸ¨ei |ÏˆâŸ©2 (2)\nwhere p(ei |Ïˆ)denotes the probability of the quantum elementary\nevent |ei âŸ©1 given the system |ÏˆâŸ©.\n1More strictly, the outer product of |ei âŸ©is called as the quantum elementary event.\n| âŸ©ğ‘’ğ‘’1\n| âŸ©ğ‘’ğ‘’2\nğ‘ğ‘2\nğ‘ğ‘1\n| âŸ©ğœ“ğœ“\nFigure 1: Projection of |ÏˆâŸ©on its basis\nIt turns out that the projection measurement based on inner\nproduct plays an essential role in the probability measurement, We\nwill further illustrate such a concept in our quantum many-body\nwave function inspired LM approach. Note that, in a broad sense,\nthe wave function is a state vector |ÏˆâŸ©. In a narrow sense, the wave\nfunction is a projection on a basis, e.g., Ïˆ(x)= âŸ¨x |ÏˆâŸ©, where x can\nbe a basis ei , and Ïˆ(x)is the probability amplitude. In this paper,\nwe will use the description of wave function in the broad sense.\n2.2 Quantum Many-Body Wave Functions\nWhat we mentioned above is a single system which corresponds to\na single particle in a Hilbert space. A quantum many-body system\nconsists of N particles, each one with a wave function residing\nin a finite dimensional Hilbert space Hi for i âˆˆ[N ]:= {1 . . .N }.\nWe set the dimensions of each Hilbert space Hi for all i, i.e., âˆ€i :\ndim(Hi )= M and the orthonormal basis of the Hilbert space as\n{|ehâŸ©}M\nh=1. The Hilbert space of a many-body system is a tensor\nproduct of the spaces: H:= âŠ—N\ni=1Hi , and the corresponding state\nvector |ÏˆâŸ©âˆˆH is\n|ÏˆâŸ©=\nMÃ•\nh1, . . .,hN =1\nAh1 . . .hN |eh1 âŸ©âŠ—Â·Â·Â·âŠ—| ehN âŸ© (3)\nwhere |eh1 âŸ©âŠ—Â·Â·Â·âŠ—| ehN âŸ©is a basis vector of the MN dimensional\nHilbert space H, and Ah1 . . .hN is a specific entry in a tensorAhold-\ning all the probability amplitude. The tensor Acan be considered\nas N -dimensional array Aâˆˆ RMÃ—Â·Â·Â·Ã—M .\nFor example, a system includes two spinful particles, which are\nqubit states superposed as two basis vectors |e1âŸ©= |0âŸ©= (1, 0)T\nand |e2âŸ©= |1âŸ©= (0, 1)T . Therefore, we can get four basis vectors\n|e1âŸ©âŠ—| e1âŸ©= |00âŸ©(abbreviation of |0âŸ©âŠ—| 0âŸ©= (1, 0, 0, 0)T ), |01âŸ©=\n(0, 1, 0, 0)T , |10âŸ©= (0, 0, 1, 0)T and |11âŸ©= (0, 0, 0, 1)T , and the state\nÏˆ of this system can be represented as\n|ÏˆâŸ©=\n2Ã•\ni, j=1\naij |ei âŸ©âŠ—| ej âŸ©\n= a11 |00âŸ©+ a12 |01âŸ©+ a21 |10âŸ©+ a22 |11âŸ©\n(4)\nwhere aij is a probability amplitude and Ã\nij a2\nij = 1. Each aij can\nbe considered as a specific entry in a tensor Aâˆˆ R2Ã—2.\n3 QUANTUM MANY-BODY WAVE FUNCTION\nINSPIRED LANGUAGE MODELING\n3.1 Basic Intuitions and Architecture\nIn Physics, Quantum Many-body Wave Function (QMWF) can\nmodel the interaction among many particles and the associated\nbasis vectors. In the language scenario, by considering a word as\na particle, different meanings (or latent/embedded concepts) as\ndifferent basis vectors, the interaction among words (or word\nmeanings) can be modeled by the tensor product of basis vectors,\nvia the many-body wave function. A tensor product of different\nbasis vectors generates a compound meaning for a compound word.\nBased on such an analogy, QMWF representation can model\nthe probability distribution of compound meanings in natural\nlanguage. Such a representation depends on basis vectors . The\nchoices of basis vectors can be one-hot vectors (representing single\nwords), or embedding vectors (representing latent meanings or\nconcepts). Theprobabilities are encoded in atensor, as we can see\nfrom Eq. 3. Each entry in a tensor is the probability amplitude of the\ncompound meaning, or can be considered as a coefficient/weight.\nAs shown in Fig. 2, given a word sequence and a set of basis\nvectors, there are local and global representations (see details\nin Section 3.2). Intuitively, the local representation is constructed\nby the current word sequence (e.g., a sentence), and the global\nrepresentation corresponds to the information of a large corpora\n(e.g., a collection of data). In classical language modeling approaches,\nthere are also local and global information, as well as the interplay\nbetween them. For example, in n-grams, the probability/statistics\nof each term can be estimated from the current piece of text (as\nlocal information), and also be smoothed with the statistics from a\nlarge corpora (as global information).\nBased on QMWF representation, in Section 3.2.3, we describe the\nprojection from the global representation to the local one. Such\nprojection can model the interplay between the local information\nand the global one, and enable us to focus on the high-dimensional\ntensors, which encode the probability distribution of the compound\nmeanings. In Fig. 2, we can observe that, the high-dimensional\ntensor Tcan be reduced by the tensor decomposition, the tensors\nA(for probabilities in local representation) andT(for probabilities\nin global representation) are kept. Therefore, the projection can\nalso be considered as an interplay between global and local tensors\n(see Section 3.2.3 for details).\nThe high-dimensional tensor Tcan be reduced by the tensor\ndecomposition. With the help of the tensor decomposition, the\nabove projection from the global representation (as a global se-\nmantic space) to the local one can be realized by a convolutional\nneural network architecture(see Section 3.3). Intuitively, each de-\ncomposed subspace of the high-dimensional tensor corresponds to a\nconvolutional channel. Together with a product pooling technique,\na CNN architecture can be constructed. Then, an algorithm based\non a CNN architecture is revealed based on the above intuitions.\n3.2 Language Representation and Projection\nvia Many-body Wave Function\n3.2.1 Local Representation by Product State. Suppose we have a\nword sequenceS (e.g., a sentence) with the lengthN : S = [x1, x2, ...,xN ].\nàµ¿|Î¨ğ‘ \nğ‘ğ‘ \nQMWF\nRepresentation\nText \nSequence\n Projection\n Tensor Decomposition\nProduct \nPooling\nCNN\nX\nConvolutional Neural Network   \nLocal Representation\nÛ§|Î¨ğ‘ \nğ’¯ = à·\nğ‘Ÿ=1\nğ‘…\nğ‘¡ğ‘Ÿ âˆ™ğ’†ğ’“,ğŸâ¨‚ğ’†ğ’“,ğŸâ€¦â¨‚ğ’†ğ’“,ğ‘µ\nGlobal Representation\nÎ¨ğ‘ \nğ‘ğ‘  Î¨ğ‘ \nConv\nConv\nConv\nğ’†ğ’“,ğŸ\nğ’†ğ’“,ğŸ\nğ’†ğ’“,ğ‘µ\nâ€¦\n= à·ğ’¯â„1,â€¦,â„ğ‘ Ã—ğ’œâ„1,â€¦,â„ğ‘\nFigure 2: Outline of quantum many-body wave function inspired language modeling approach\nFor each word xi in S, based on Eq. 1, we define its state vector as:\n|xi âŸ©=\nMÃ•\nhi =1\nÎ±i, hi |Ï•hi âŸ© (5)\nwhere each basis vector |Ï•hi âŸ©(hi = 1, . . . ,M)is corresponding\nto a specific semantic meaning (or a latent concept), and Î±i, hi is\nits associated probability amplitude. Different from the notation\nÎ±i in Eq. 1, the notation Î±i, hi in Eq.5 is for the convenience to be\nrepresented in a tensor depicted latter. For a better understanding, as\nan example, the state vectors for wordsx1 and x2 can be represented\nby\n\u001a |x1âŸ©= Î±1, 1 |Ï•1âŸ©+ Î±1, 2 |Ï•2âŸ©+ Î±1, 3 |Ï•3âŸ©\n|x2âŸ©= Î±2, 1 |Ï•1âŸ©+ Î±2, 2 |Ï•2âŸ©+ Î±2, 3 |Ï•3âŸ© (6)\nwhere N = 2, M = 3 and hi is from 1 to 3, i.e., three basis vectors\n|Ï•hi âŸ©are involved, each corresponding to a word meaning.\nFor the basis vectors |Ï•hi âŸ©, there can be different choices, e.g.,\none-hot vectors or embedded vectors. Different basis vectors yield\ndifferent interpretations for the semantic meanings. We will adopt\nthe embedding space when we instantiate this framework in the\nquestion answering task (see Section 4). If we use such a space, the\nprobability amplitudeÎ±i, hi is the feature value (after normalization)\nof the word xi on the hi th dimension of the embedding space.\nNext, we show how to use the tensor product to model the inter-\naction among word meanings. For a sentence S = [x1, x2, ...,xN ],\nits wave function can be represented as:\n|Ïˆps\nS âŸ©= |x1âŸ©âŠ— . . .âŠ—|xN âŸ© (7)\nwhere |Ïˆps\nS âŸ©is the product state of the QMWF representation of a\nsentence. We can expand the product state |Ïˆps\nS âŸ©as follows:\n|Ïˆps\nS âŸ©=\nMÃ•\nh1, . . .,hN =1\nAh1 . . .hN |Ï•h1 âŸ©âŠ— . . .âŠ—|Ï•hN âŸ© (8)\nwhere |Ï•h1 âŸ©âŠ—. . .âŠ—|Ï•hN âŸ©is the new basis vectors withMN dimen-\nsion, and each new basis vector corresponds a compound meaning\nby the tensor product of the word meanings |Ï•hi âŸ©. Ais a MN di-\nmensional tensor and each entry Ah1 . . .hN (= ÃN\ni=1 Î±i, hi ) encodes\nthe probability of the corresponding compound meaning.\nEq. 8 can represent the interaction among words as we discussed\nin Introduction. For example, for two words x1 and x2 in Eq. 6,\nsuppose x1 only has two meanings corresponding to the basis vec-\ntors |Ï•1âŸ©and |Ï•2âŸ©, while x2 has two meanings corresponding to\n|Ï•2âŸ©and |Ï•3âŸ©. Then, A1, 3(= Î±1, 1Î±2, 3)represents the probability\nwith the basis vector |Ï•1âŸ©âŠ—| Ï•3âŸ©. Intuitively, this implies that the\nunderlying meaning (|Ï•1âŸ©) of word x1 and the meaning (|Ï•3âŸ©) of x2\nis interacted and form a compound meaning |Ï•1âŸ©âŠ—| Ï•3âŸ©.\nNow, we can see that this product state representation is actu-\nally a local representation for a word sequence. In other words,\ngiven the basis vectors, the probability amplitudes can be estimated\nfrom the current word sequence. In fact, Ais a rank-1 N -order\ntensor, which includes only M Ã—N free parameters Î±i, hi , rather\nthan MN parameters to be estimated. In addition, given only a\nword sequence, the valid compound meanings are not too many. In\nsummary, this rank-1 tensor actually encodes the local distributions\nof these compound meanings for the given sentence.\n3.2.2 Global Representation for All Possible Compound Meanings.\nAs aforementioned in Section 3.1 , we need a global distribution of\nall the possible compound meanings, given a set of basis vectors.\nIntuitively, a global distribution is useful in both classical LM and\nquantum-inspired LM, since we often have unseen words, word\nmeanings or the compound meanings, in a text.\nTo represent such a global distribution of state vectors, we define\na quantum many-body wave function as follows:\n|ÏˆS âŸ©=\nMÃ•\nh1, . . .,hN =1\nTh1 . . .hN |Ï•h1 âŸ©âŠ— . . .âŠ—|Ï•hN âŸ© (9)\nwhere |Ï•h1 âŸ©âŠ— . . .âŠ—|Ï•hN âŸ©is the basis state (corresponding to a\ncompound meaning) with MN dimension, and Th1 . . .hN is the cor-\nresponding probability amplitude. This wave function represents\na semantic meaning space with a sequence of N uncertain words,\nwhich does not rely on a specific sentence showed in Eq. 8. The\nprobability amplitudes in tensorTcan be trained in a large corpora.\nThe difference between |Ïˆps\nS âŸ©in Eq. 8 and |ÏˆS âŸ©in Eq. 9 is the\ndifferent tensors Aand T. Aencodes the local distribution of\ncompound meanings (for the current sentence) and Tencodes the\nglobal distribution (for a large corpora). Moreover, Ais essentially\nrank-1, whileThas a higher rank. In fact, solvingTis an intractable\nproblem which is referred as a quantum many-body problem.\n3.2.3 Projection from Global to Local Representation. Section 2.1\nhas emphasized the role of projection in the probability measure-\nment. Now, we show the projection of the global semantic repre-\nsentation |ÏˆS âŸ©on its product state |Ïˆps\nS âŸ©as a local representation\nfor the given sentence, to calculate the probability amplitudes in\nthe tensor.\nSuch a projection can be modeled by the inner productâŸ¨Ïˆps\nS |ÏˆS âŸ©.\nInspired by a recent work [18], this projection will eliminate the\nhigh-dimensional basis vectors of the wave function:\nâŸ¨Ïˆps\nS |ÏˆS âŸ©= âŸ¨x1 . . .xN |\nMÃ•\nh1, . . .,hN =1\nTh1 . . .hN |Ï•h1 . . .Ï•hN âŸ©âŸ©\n=\nMÃ•\nh1, . . .,hN =1\nTh1 . . .hN\nNÃ–\ni=1\nâŸ¨xi |Ï•hi âŸ©i\n=\nMÃ•\nh1, . . .,hN =1\nTh1 . . .hN\nNÃ–\ni=1\nÎ±i, hi\n=\nMÃ•\nh1, . . .,hN =1\nTh1 . . .hN Ã—Ah1 . . .hN\n(10)\nwhich reveals the interplay between the global tensor Tand local\ntensor A. This is similar to the idea in the classical LM, where the\nlocal statistics in a text will be smoothed by collection statistics.\n3.3 Projection Realized by Convolutional\nNeural Network\nAs shown in Eq. 10, the high-dimensional tensor Tis still an un-\nsolved issue. Now, we first describe the tensor decomposition to\nsolve this high-dimensional tensor. Then, with the decomposed\nvectors, we will show that the convolutional neural network can\nbe considered as a projection or a mapping process from the global\nsemantics to local ones.\n3.3.1 Tensor Decomposition. In general, Tensor decomposition can\nbe regarded as a generalization of Singular Value Decomposition\n+ â‹¯+\nğ´ğ´ğ‘šğ‘šÃ—ğ‘›ğ‘› ğ‘‰ğ‘‰ğ‘˜ğ‘˜Ã—ğ‘›ğ‘›ğ‘ˆğ‘ˆğ‘šğ‘šÃ—ğ‘˜ğ‘˜\n= =\n= = ï¿½\nğ‘–ğ‘–=1\nğ‘˜ğ‘˜\nğœğœğ‘–ğ‘–ğ’–ğ’–ğ‘–ğ‘–â¨‚ğ’—ğ’—ğ‘–ğ‘–\n+ â‹¯+= +\nğ’¯ğ’¯ = ï¿½\nğ‘Ÿğ‘Ÿ=1\nğ‘…ğ‘…\nğ‘¡ğ‘¡ğ‘Ÿğ‘Ÿğ’†ğ’†ğ‘Ÿğ‘Ÿ,1â¨‚ğ’†ğ’†ğ‘Ÿğ‘Ÿ,2â¨‚ğ’†ğ’†ğ‘Ÿğ‘Ÿ,3\n(ğ’‚ğ’‚)\n(ğ’ƒğ’ƒ)\nFigure 3: An illustration of the singular value decomposition\nof a matrix with dimension M Ã—N in (a) and the CP decom-\nposition of a three order tensor in (b).\n(SVD) from matrices to tensors and can help to solve high-dimensional\nproblems (see Fig. 3). There are many methods to decompose a high-\ndimensional tensor, such as Canonical Polyadic Decomposition (CP\ndecomposition [13]), Tucker Decomposition, etc. The CP decompo-\nsition with weights is:\nT=\nRÃ•\nr =1\ntr Â·er, 1 âŠ—er, 2 âŠ—. . .âŠ—er, N (11)\nwhere tr is the weight coefficient for each rank-1 tensor and er, i =\n(er, i, 1, . . . ,er, i, M )T (i = 1, . . . ,N ) is a unit vector withM-dimension.\nR is the rank ofT, which is defined as the smallest number of rank-1\ntensors that generate Tas their sum.\nThe decomposed vector er, i with a low dimension will play a\nkey role in the later derivation. A set of vectors er, i (i = 1, . . . ,N )\ncan be a subspace of the high-dimensional tensor T.\n3.3.2 Towards Convolutional Neural Network. We will show that\nthe projection from the global representation |ÏˆS âŸ©to the local one\n|Ïˆps\nS âŸ©can be realized by a Convolutional Neural Network (CNN)\nwith product pooling [7]. To see this, we can put the CP decompo-\nsition Eq. 11 of Tin Eq. 10, and obtain:\nâŸ¨Ïˆps\nS |ÏˆS âŸ©=\nRÃ•\nr =1\ntr\nNÃ–\ni=1\n(\nMÃ•\nhi =1\ner, i, hi Â·Î±i, hi ) (12)\nThe above equation provides a connection between the quantum-\ninspired LM and the CNN design. The CNN interpretations of Eq. 12\nare summarized in Table 1 and also illustrated in Fig. 4.\nGiven a sequence withN words, each is represented by an vector\nxi = (Î±i, h1 , . . . ,Î±i, hM )T . The convolution function isÃM\nhi =1 er, i, hi Â·\nÎ±i, hi , which is the inner product < xi , er, i > between xi and er, i .\nThe input vector xi is a kind of local information and its entries\nÎ±i, hi actually are the values in the local tensorA. The entries in the\nvector er, i decomposed from the global tensor T, are now parame-\nters to be trained in the convolutional layer. Such an inner product,\ncan be considered as a mapping from the global information er, i to\nthe local representationxi . After that, the product pooling layer (i.e.,\nğ’†ğ‘¹,ğŸ,ğ’‰ğŸğ’†ğ‘¹,ğŸ,ğ’‰ğŸâ€¦ ğ’†ğ‘¹,ğŸ,ğ’‰ğ‘´\nğ’†ğ’“,ğŸ,ğ’‰ğŸ ğ’†ğ’“,ğŸ,ğ’‰ğŸ\nğ’†ğŸ,ğŸ,ğ’‰ğŸ ğ’†ğŸ,ğŸ,ğ’‰ğŸ â€¦ ğ’†ğŸ,ğŸ,ğ’‰ğ‘´\nğ’†ğŸ,ğŸ,ğ’‰ğŸ ğ’†ğŸ,ğŸ,ğ’‰ğŸğ›¼1,â„1 ğ›¼1,â„2 â€¦ ğ›¼1,â„ğ‘€\nğ›¼2,â„1 ğ›¼2,â„2\nâ€¦ ğ›¼2,â„ğ‘´\nâ€¦ â€¦ â€¦ â€¦\nğ›¼ğ‘,â„1 ğ›¼ğ‘,â„ğŸ\nâ€¦ ğ›¼ğ‘,â„ğ‘´\nğ’†ğŸ,ğŸ,ğ’‰ğŸ ğ’†ğŸ,ğŸ,ğ’‰ğŸ â€¦ ğ’†ğŸ,ğŸ,ğ’‰ğ‘´\nğ’†ğŸ,ğŸ,ğ’‰ğŸ ğ’†ğŸ,ğŸ,ğ’‰ğŸ\nâ€¦ ğ’†ğŸ,ğŸ,ğ’‰ğ‘´\nğ‘¥1\nğ‘¥2\nğ‘¥ğ‘\nâ€¦\nKernel weights\n .\nRepresentation Matrix\nChannel  1â€¦R\nâ€¦\nğ‘£ğ‘…\nâ€¦\nğ‘£1 ğ‘£2 â€¦\nâ€¦\nProduct \npooling\nOutput\nFigure 4: Realization of QMWF-LM via convolution neural network with product pooling\nTable 1: CNN Interpretation of Projection\nInput xi = (Î±i, h1 , . . . ,Î±i, hM )T\nConvolution Î£r, i = ÃM\nhi =1 er, i, hi Â·Î±i, hi\nProduct pooling Î r = ÃN\ni=1 Î£r, i\nOutput ÃR\nr =1 tr Â·Î r\nÃ\nr , see Table 1) multiplies all the mapping resultsÎ£r, i =<xi , er, i >\nfor all the N words.\nAs mentioned above, a set ofN decomposed vectors corresponds\nto a subspace of the high-dimensional tensor. The rank R is the\nnumber of decomposed subspace, and this number corresponds\nto the number of convolution channels. In Fig. 4, different color\nmeans different channels of convolution. Following the input layer,\na convolution layer withR channels calculates weighted sums of the\nrepresentation vectors xi and the vectors er, i (as kernel weights).\nIn Eq. 12, one can sum R products Î r with weights tr to obtain the\nprojection âŸ¨Ïˆps\nS |ÏˆS âŸ©. Then, a vector v = (v1, . . . ,vR )T can be used\nto represent a sequence of words, wherevr = tr Â·Î r (r = 1, . . . ,R).\n3.3.3 Algorithm. Based on the above ideas, a practical algorithm\ncan be obtained with four parts as follows, also shown in Fig. 4:\nâ€¢Input layer\nThe input to our model, a sequence of words S, is composed\nof N words or patches [x1, ...,xN ]. Each word xi will be\nrepresented by a vector(Î±i, h1 , ...,Î±i, hM )T . Then, we will get\na representation matrix S âˆˆRN Ã—M .\nâ€¢Convolution layer For each word or patch, the convolution\nis computed as follows: Î£r, i = ÃM\nhi =1 er, i, hi Â·Î±i, hi (r =\n1, . . . ,R), where R is the number of convolution channels.\nâ€¢Product pooling layer We apply the product pooling on\nthe results of the convolution. It multiplies a number (N ) of\nÎ£r, i to get the Î r = ÃN\ni=1 Î£r, i , where Î r âˆˆR.\nâ€¢Output Finally, we represent a sequence S using a vector\nv = (v1, . . . ,vR )T âˆˆRR .\nThe above algorithm can model the sentence representation\nand can be applied in natural language processing tasks such as\nclassification or matching between sentence pairs. It is worth noting\nthat the decomposition of a symmetric tensor can make the unit\nvectors er as same as each order, which means for a convolutional\nkernel, er, 1 = er, 2 = . . .= er, N . In this way, we will get a property\nabout convolutional neural networks, i.e., the weight sharing.\n4 APPLICATIONS\nQuestion Answering (QA) tasks aim to rank the answers from a\ncandidate answer pool, given a question. The ranking is based on\nthe matching scores between question and answer sentences. The\nkey points are to build effective representations of the question\nand answer sentences and measure their matching score over such\nrepresentations. In this paper, we model the question and answer\nsentences with quantum many-body wave functions and apply the\nalgorithm in Section 3.3.3 to obtain question and answer sentences\nand match pairs of them.\nCompared with the ad-hoc retrieval task, the question in the QA\ntask is usually a piece of fluent natural language instead of a phrase\nor multiple keywords. The candidate answers are also shorter than\nthe documents in ad-hoc retrieval. There is often less number of\noverlapping words between the question and answer sentences in\nthe QA task, where semantic matching via neural network is widely\nused. In this paper, we apply the proposed QMWF based LM with\nneural network implementation in QA tasks. It should be noted\nthat our model is also applicable to other LM based ranking tasks.\n4.1 Quantum Many-Body Representation\nAs introduced in Section 3.2, each word vector |xâŸ©locates at the\nM dimensional Hilbert space. The product state representation of\na specific sentence is represented by the wave function in Eq. 7,\nand the global representation of an arbitrary sentence with the\nsame length is using the wave function in Eq. 9. As introduced in\nSection 3.2.3, the projection onto the product state of a sentence is\nformulated in Eq.10. The process of projection can be implemented\nby a convolution and a product pooling which has been introduced\nin Section 3.3.2. As a text has different granularity, we can utilize\ntwo kinds of methods to obtain our input matrix S.\nWord-level Based Input. We can utilize the expressive ability\nof pre-trained embedding. We think each dimension of word em-\nbedding is corresponding to a latent concept and a basis vector.\nLet Î±i, hi be the coefficient with respect to the corresponding basis\nvector, reflecting that the words can reside in specific concept with\na certain probability. Given a sentence S = [x1, . . . ,xN ], where xi\nis a single word represented by a vector (Î±i, 1, . . . ,Î±i, M )T . Then,\nthis sentence can be represented as a matrix in S âˆˆRN Ã—M .\nCharacter-level Based Input. Inspired by the work [ 16] that\nusing character-level convolution. A sentence is represented by a\nsequence of characters: Sc = [x1, ...,xN ]. The first part is a look-up\ntable. We utilize the CNN with max pooling to obtain the representa-\ntion in word level. We define the matrixZ= [z1, ...,zN ]as a input\nmatrix where each column contains a vector zm âˆˆRdk that is the\nconcatenation of a sequence of k char embeddings. N = N âˆ’k + 1,\nd is the dimension of char embedding and k is the window size\nof convolution. Each output of convolution is (Î±i, h1 , . . . ,Î±i, hM )T .\nThen, we can obtain the input matrix S âˆˆRN Ã—M .\nBased on word-level and character-level, a representation ma-\ntrix S can be obtained. Then, a sentence is represented by v =\n(v1, . . . ,vR )T âˆˆRR based on the algorithm in Section 3.3.3.\n4.2 Matching via Many-body Wave Function\nRepresentation on QA\nLet Q = [q1, q2 . . .qNQ ]and A = [a1, a2 . . .aNA ]be the sentence\nof the question and answer, respectively. As introduced above, we\nhave:\nvq\ni = tr Â·\nNÃ–\ni=1\n(\nMÃ•\nhi =1\ner, i, hi Â·Î±q\ni, hi\n) (13)\nva\ni = tr Â·\nNÃ–\ni=1\n(\nMÃ•\nhi =1\ner, i, hi Â·Î±a\ni, hi\n) (14)\nThen, we havevq = (vq\n1 , . . . ,vq\nR )T and va = (va\n1 , . . . ,va\nR )T for the\nvector representation of the question and answer, respectively. The\nmatching score is defined as the projection from the answer state to\nthe question state, which is an inner product âŸ¨vq,vaâŸ©.\n5 LITERATURE REVIEW\nQuantum theory is one of the most remarkable discoveries in Sci-\nence. It not only can explain the behavior and movement of mi-\ncroscopic particles or electrons, but also have been widely applied\nin the macro-world problems. For instance, the quantum theory\nhas been applied in social science and economics [12], cognition\nand decision making [4, 5], language model [1], natural language\nprocessing [1, 3], and information retrieval [ 31, 33, 40].These re-\nsearch directions are making use of the mathematical formulation\nand non-classical probability measurement of quantum theory, not\nnecessarily for quantum computation.\nIn Information Retrieval (IR), van Rijsbergen for the first time\nproposed to adopt the mathematical formalism of quantum the-\nory to unify various IR formal models and provide a theoretical\nfoundation for developing new models [ 33]. Later, a number of\nresearch efforts have been made to build quantum-like IR mod-\nels [25, 31, 41, 42]. The main inspiration is rooted on the quantum\ntheory as a principled framework for manipulating vector spaces\nand measuring probability in Hilbert space [20]. Piwowarski et\nal. [25] proposed a quantum IR framework, which represents the\nqueries and documents as density matrices and subspaces, respec-\ntively. The information need space can be constructed by the tensor\nproduct of term vectors, based on a so-called multi-part system,\nwhich corresponds to a product state (see Section 3) of the quantum\nmany-body wave function. However, the issue of the probability\namplitudes (forming a high dimensional tensor) have not been ad-\ndressed systematically. In addition, this framework has not shown\nthe effect of tensor decomposition and its connection with the\nneural network design.\nSome quantum-inspired retrieval models are based on the analo-\ngies between IR problems and quantum phenomena. For instance, by\nconsidering the inter-document dependency as a kind of quantum\ninterference phenomena, a Quantum Probability Ranking Princi-\nple was proposed [42]. In addition, a quantum-inspired re-ranking\nmethod was developed by considering the ranking problem as a\nfiltering process in the photon polarization [41]. These models are\nnovel in terms of their quantum-inspired intuitions. In practice,\nthey adopted the relevance scores of classical retrieval models as\nthe input probabilities, without actually performing a quantum\nprobability measurement (e.g., the projection measurement).\nRecently, Sordoni et al. [31] proposed a principled Quantum Lan-\nguage Model (QLM), which generalizes the traditional statistical\nLM with the quantum probability theory. In QLM, the probability\nuncertainties of both single and compound words are measured by\nthe projection measurement in Hilbert space. For a text (a query or a\ndocument), a density matrix is then estimated based on a Maximal\nLikelihood Estimation (MLE) solution. Practically, QLM shows an\neffective performance on the ad-hoc retrieval task. Extending QLM\nwith the idea of quantum entropy minimization, Sordoni et al. pro-\nposed to learn latent concept embeddings for query expansion in a\nsupervised way [29]. In addition, to capture the dynamic informa-\ntion need in search sessions, an adaptive QLM was built with an\nevolution process of the density matrix [19]. More recently, an End-\nto-End Quantum-like Language Model (named as NNQLM) [ 40]\nhas been proposed, which built a quantum-like LM into a neural\nnetwork architecture and showed a good performance on QA tasks.\nIn this paper, we aim to tackle these challenging problems (as\nidentified in the Introduction) of the existing quantum-inspired\nLMs. Our work is inspired by the recent multidisciplinary research\nfindings across quantum mechanics and machine learning [2] (espe-\ncially neural network [6, 18]). The two different disciplines, while\nseemingly to have huge gaps at the first glance, can benefit each\nother based on rigorous mathematical analysis and proofs. For in-\nstance, the neural network can help yield a more efficient solution\nfor the quantum many-body problem [6]. The quantum many-body\nsystem, on the other hand, can help better explain the mechanism\nbehind the neural network [2, 18]. The neural network based ap-\nproaches have been shown effective in both the neural IR [8, 9, 11]\nand QA fields [14, 15, 37]. In this paper, we propose a novel quantum-\ninspired language modeling approach and apply it in ranking-based\nQA tasks. We expect that our attempt would potentially open a\ndoor for the consequent research across the fields of information\nretrieval, neural network, and quantum mechanics.\n6 EXPERIMENTS\n6.1 Datasets\nWe conduct our evaluation on three widely used datasets (summa-\nrized in Table 2) for the question answering task.\nâ€¢TRECQA is a benchmark dataset used in the Text Retrieval\nConference (TREC)â€™s QA track(8-13) [ 35]. It contains two\ntraining sets, namely TRAIN and TRAIN-ALL. We use TRAIN-\nALL, which is a larger and contains more noise, in our ex-\nperiment, in order to verify the robustness of the proposed\nmodel.\nâ€¢WIKIQA is an open domain question-answering dataset\nreleased by Microsoft Research [36]. We remove all questions\nwith no correct candidate answers.\nâ€¢YahooQA, collected from Yahoo Answers, is a benchmark\ndataset for community-based question answering. It contains\n142627 questions and answers. The answers are generally\nlonger than those in TRECQA and WIKIQA. As introduced\nin [32], we select the QA pairs containing questions and\nthe best answers of length 5-50 tokens after removing non-\nalphanumeric characters. For each question, we construct\nnegative examples by randomly sampling 4 answers from\nthe set of answer sentences.\n6.2 Algorithms for Comparison\nQMWF-LM is a quantum inspired language model. The closest\napproaches to our QMWF-LM are QLM [31] and NNQLM [40]. We\ntreat them as our baselines.\nâ€¢QLM. The question and answer sentences are represented\nby the density matrices Ïq and Ïa, respectively. Then the\nscore function is based on the negative Von-Neumann (VN)\nDivergence between Ïq and Ïa .\nâ€¢NNQLM-II. This model is an end-to-end quantum language\nmodel. We actually compare our model with NNQLM-II,\nwhich performs much better than NNQLM-I [40]. The ques-\ntion and answer sentences are also encoded in the density\nmatrix, but with the embedding vector as the input. The den-\nsity matrix Ï is trained by a neural network. The matching\nscore is computed by the convolutional neural network over\nthe joint representation of two density matrices Ïq and Ïa.\nâ€¢QMWF-LM. QMWF-LM is the model introduced in this pa-\nper. It is inspired by the quantum many-body wave function.\nQMWF-LM-word is the model whose input matrix is the\nword embedding. QMWF-LM-char is the model whose input\nmatrix is based on char embedding.\nSince we utilize the CNN with product pooling to implement\nthe QMWF based LM, we compare our model with a range of\nCNN-based QA models [10, 26, 27]. Additional CNN-based models\ninclude QA-CNN [10], and AP-CNN which is the attentive pooling\nnetwork. Our focus is to show the potential of the QMWF-inspired\nLM and its connection with CNN, rather than a comprehensive\ncomparison with all the recent CNN based QA models. Therefore,\nwe just pick up a couple of basic and typical CNN-based QA models\nfor comparison.\n6.3 Evaluation Metrics\nFor experiments on TRECQA and WIKIQA, we use the same ma-\ntrix as in [ 26], namely the MAP (mean average precision) and\nMRR (mean reciprocal rank). For experiments on YahooQA dataset,\nwe use the same metrics as in [ 34], namely Precision@1 (P@1)\nand MRR. P@1 is defined by 1\nN\nÃN\n1 [rank (Aâˆ—)= 1]where [Â·] is the\nindicator function and Aâˆ—is the ground truth.\nAccording to Wilcoxon signed-rank test, the symbols Î± and Î²\ndenote the statistical significance (with p < 0.05) over QLM and\nNNQLM-II, respectively, in experimental table.\n6.4 Implementation Details and\nHyperparameters\nFor QLM and NNQLM, we use the same parameters introduced\nin [40]. The model is implemented by Tensorflow and the exper-\nimental machine is based on TITAN X GPU. We train our model\nfor 50 epochs and use the best model obtained in the dev dataset\nfor evaluation in the test set. We utilize the Adam [17] optimizer\nwith learning rate [0.001,0.0001,0.00001]. The batch size is tuned\namong [80,100,120,140]. The L2 regularization is tuned among\n[0.0001,0.00001,0.000001]. For QMWF-LM-word, we initialize the\ninput layer with 300-dimensional Glove vectors [24]. For QMWF-\nLM-char, the initial char embedding is a one-hot vector. In QMWF\nalgorithm, we use the logarithm value for the product pooling, and\nuse two or three words in a patch to capture the phrase information.\n6.5 Experimental Results\nTable 3 reports the results on the TRECQA dataset. The first group\nshows a comparison of three quantum inspired language models.\nQMWF-LM-word significantly outperforms QLM by 10.91% on\nMAP and 12.12% on MRR, respectively. The result of QMWF-LM-\nword is comparable with that of NNQLM-II. In the second group,\nwe compare our model with a range of CNN-based models against\ntheir results reported in the corresponding original papers. We can\nsee that the QMWF-LM-word improves the CNN model in [38] by\n5.77% on MAP, and 3.69% on MRR, respectively.\nTable 4 reports the results on WIKIQA. QMWF-LM-word sig-\nnificantly outperforms QLM by 35.74% on MAP, and 37.86% on\nMRR, as well as NNQLM-II by 6.92% on MAP, and 7.74% on MRR.\nIn comparison with CNN models, QMWF-LM-word outperforms\nQA-CNN and AP-CNN by (1%âˆ¼2%) on both MAP and MRR, based\non their reported results.\nThe experimental results on YahooQA are shown in Table 5. Our\nQMWF-LM-word achieves a significant improvement over QLM\nby 45.57% on P@1 and 23.34% on MRR, respectively. It also out-\nperforms NNQLM-II on P@1 by 23.39% and on MRR by 10.70%,\nrespectively. Compared with the results of other CNN models on\nYahooQA dataset as reported in [32], QMWF-LM-word shows im-\nprovements over AP-CNN by about 2.67% on P@1 and 2.61% on\nMRR, respectively. Note that the data preprocessing of YahooQA\ndataset in our experiments is a little different, as we randomly\nsample four negative examples from the answers sentence set.\nTable 2: Statistics of Datasets\nTREC-QA WIKIQA YahooQA\nTRAIN DEV TEST TRAIN DEV TEST TRAIN DEV TEST\n#Question 1229 82 100 873 126 243 56432 7082 7092\n#Pairs 53417 1148 1517 8672 1130 2351 287015 35880 35880\n%Correct 12.0 19.3 18.7 12.0 12.4 12.5 20 20 20\nTable 3: Experimental Result on TRECQA (raw). Î± denotes\nsignificant improvement over QLM.\nModel MAP MRR\nQLM 0.678 0.726\nNNQLM-II 0.759 0.825\nCNN (Yu et al.) [38] 0.711 0.785\nCNN (Severyn) [27] 0.746 0.808\naNMM (Yang et al.) [37] 0.749 0.811\nQMWF-LM-char 0.715 Î± 0.758 Î±\nQMWF-LM-word 0.752 Î± 0.814 Î±\nTable 4: Experimental Result on WIKIQA. Î± and Î² denote\nsignificant improvement over QLM and NNQLM-II, respec-\ntively.\nModel MAP MRR\nQLM 0.512 0.515\nNNQLM-II 0.650 0.659\nQA-CNN (Santos et al.) [10] 0.670 0.682\nAP-CNN (Santos et al.) [10] 0.688 0.696\nQMWF-LM-char 0.657 Î± 0.679 Î±\nQMWF-LM-word 0.695 Î± Î² 0.710 Î± Î²\nTable 5: Experimental Result on YahooQA. Î± and Î² denote\nsignificant improvement over QLM and NNQLM-II, respec-\ntively.\nModel P@1 MRR\nRandom guess 0.200 0.457\nQLM 0.395 0.604\nNNQLM-II 0.466 0.673\nQA-CNN (Santos et al.)[10] 0.564 0.727\nAP-CNN (Santos et al.)[10] 0.560 0.726\nQMWF-LM-char 0.513 Î± Î² 0.696 Î± Î²\nQMWF-LM-word 0.575 Î± Î² 0.745 Î± Î²\nAs we can see from the results, the performance of QMWF-LM-\nchar is not as good as that of QMWF-LM-word. However, compared\nwith NNQLM-II, QMWF-LM-char has better results on WIKIQA\nand YahooQA datasets. We will give a further analysis of this phe-\nnomenon in Section 6.6.2.\n6.6 Discussion and Analysis\n6.6.1 The Result Analysis. The experimental results show that our\nproposed model, namely QMWF-LM, has achieved a significant\nimprovement over QLM on three QA datasets, and outperforms\nNNQLM-II on both WIKIQA and YahooQA datasets. Especially on\nYahooQA, which is the largest among the three datasets, QMWF-LM\nsignificantly outperforms the other two quantum-inspired LM ap-\nproaches. Note that the original QLM is trained in an unsupervised\nmanner. Therefore, unsurprisingly it under-performs the other two\nsupervised models (i.e., NNQLM and QMWF-LM). NNQLM adopts\nthe embedding vector as its input and uses the convolutional neu-\nral network to train the density matrix. However, the interaction\namong words is not taken into account in NNQLM. The experiment\nalso shows that the proposed model can achieve a comparable and\neven better performance over a couple of CNN-based QA mod-\nels. In summary, our proposed model reveals the analogy between\nthe quantum many-body system and the language modeling, and\nfurther effectively bridge the quantum many-body wave function\ninspired LM with the neural network design.\n6.6.2 The Comparison between Word embedding and Char embed-\nding. In our experiment, the input layer is based on word embedding\nand char embedding. For char embedding, we treat the text as a\nkind of raw signal which has been proved effective in modeling sen-\ntence [16]. As char embedding is initialized by one-hot vector, the\nsemantic information is only based on training dataset. In QA tasks,\nthe semantic matching requires a relatively large data for training\nthe embeddings. Therefore, compared with char embedding, pre-\ntrained word embedding trained by an external large corpus (rather\nthan training data only) is more effective.\n6.6.3 Influence of Channels in Convolution. As introduced in Sec-\ntion 3, the number of convolution channels is corresponding to\nR which is the rank of a tensor. In our problem, it is not straight-\nforward to determine the rank of a concerned tensor. We select\nthe optimal R in a range [20, 200]with increment 5. For different\ndatasets, we set a suitable number of channels to obtain the best\nperformance. The number of channels is set to 150 for TRECQA\nand WIKIQA dataset, and 200 For YahooQA dataset.\n6.6.4 Efficiency Analysis. As we utilize convolution neural net-\nwork to implement the operation of tensor decomposition. The\nefficiency relies on the convolution neural network. In our experi-\nment, for QMWF-LM-char, the training epoch is set to be 200, while\nfor QMWF-LM-word, after training 20 epochs, we will obtain the\nresults.\n7 CONCLUSIONS AND FUTURE WORK\nIn this paper, we propose a Quantum Many-body Wave Function\n(QMWF) inspired Language Modeling (QMWF-LM) framework. We\nhave shown that the incorporation of QMWF has enhanced the rep-\nresentation space of quantum-inspired LM approaches, in the sense\nthat QMWF-LM can model the complex interactions among words\nwith multiple meanings. In addition, inspired by the recent progress\non solving the quantum many-body problem and its connection\nto the neural network, we bridge the gap between the quantum-\ninspired language modeling and the convolutional neural network.\nSpecifically, a series of derivations (based on projection and ten-\nsor decomposition) show that the quantum many-body language\nmodeling representation and matching process can be implemented\nby the convolutional neural network (CNN) with product pooling.\nThis result simplifies the estimation of the probability amplitudes\nin QMWF-LM. Based on this idea, we provide a simple algorithm\nin a basic CNN architecture.\nWe implement our approach on the question answering task.\nExperiments on three QA datasets have demonstrated the effective-\nness of our proposed QMWF based LM. It achieves a significant\nimprovement over its quantum-like counterparts, i.e., QLM and\nNNQLM. It can also achieve effective performance compared with\nseveral convolutional neural network based approaches. Further-\nmore, based on the analytical and empirical evidence presented in\nthis paper, we can conclude that the proposed approach has made\nthe first step to bridge the quantum-inspired formalism, language\nmodeling and neural network structure in a principled manner.\nIn the future, the quantum many-body inspired language model\nshould be investigated in more depth from both theoretical and\nempirical perspectives. Theoretically, a more unified framework to\nexplain another widely-adopted neural network architecture, i.e.,\nRecurrent Neural Network (RNN), can be explored based on the\nmechanism of quantum many-body language modeling. Practically,\nwe will apply and evaluate QMWF-LM on other IR or NLP tasks\nwith larger scale datasets.\n8 ACKNOWLEDGMENTS\nThis work is supported in part by the state key development pro-\ngram of China (grant No. 2017YFE0111900), Natural Science Foun-\ndation of China (grant No. U1636203, 61772363), and the European\nUnionâ€™s Horizon 2020 research and innovation programme under\nthe Marie SkÅ‚odowska-Curie grant agreement No. 721321.\nREFERENCES\n[1] Ivano Basile and Fabio Tamburini. 2017. Towards Quantum Language Models. In\nProc. of EMNLP . 1840 â€“1849.\n[2] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe,\nand Seth Lloyd. 2016. Quantum machine learning.arXiv preprint arXiv:1611.09347\n(2016).\n[3] William Blacoe, Elham Kashefi, and Mirella Lapata. 2013. A Quantum-Theoretic\nApproach to Distributional Semantics.. In Proc. of HLT-NAACL . 847â€“857.\n[4] Peter D. Bruza, Zheng Wang, and Jerome R. Busemeyer. 2015. Quantum cognition:\na new theoretical approach to psychology. Trends in Cognitive Sciences 19, 7\n(2015), 383 â€“ 393.\n[5] Jerome R. Busemeyer and Peter D. Bruza. 2013. Quantum Models of Cognition\nand Decision . Cambridge University Press.\n[6] G. Carleo and M. Troyer. 2017. Solving the quantum many-body problem with\nartificial neural networks. Science 355 (2017), 602â€“606.\n[7] Nadav Cohen, Or Sharir, and Amnon Shashua. 2016. On the Expressive Power of\nDeep Learning: A Tensor Analysis. Computer Science (2016).\n[8] Nick Craswell, W. Bruce Croft, Maarten de Rijke, Jiafeng Guo, and Bhaskar Mitra.\n2017. SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IRâ€™17). In Proc.\nof SIGIR . 1431â€“1432.\n[9] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce\nCroft. 2017. Neural Ranking Models with Weak Supervision. In Proc. of SIGIR .\n65â€“74.\n[10] CÄ±cero Nogueira dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. 2016.\nAttentive pooling networks. CoRR, abs/1602.03609 (2016).\n[11] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance\nMatching Model for Ad-hoc Retrieval. In Proc. of CIKM . 55â€“64.\n[12] E. Haven and A. Khrennikov. 2013.Quantum Social Science . Cambridge University\nPress.\n[13] Frank L Hitchcock. 1927. The expression of a tensor or a polyadic as a sum of\nproducts. Studies in Applied Mathematics 6, 1-4 (1927), 164â€“189.\n[14] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional\nneural network architectures for matching natural language sentences. In Proc.\nof NIPS . 2042â€“2050.\n[15] Yoon Kim. 2014. Convolutional neural networks for sentence classification.arXiv\npreprint arXiv:1408.5882 (2014).\n[16] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2016. Character-\nAware Neural Language Models.. In Proc. of AAAI . 2741â€“2749.\n[17] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[18] Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. 2017. Deep Learn-\ning and Quantum Entanglement: Fundamental Connections with Implications to\nNetwork Design. CoRR abs/1704.01552 (2017). http://arxiv.org/abs/1704.01552\n[19] Qiuchi Li, Jingfei Li, Peng Zhang, and Dawei Song. 2015. Modeling multi-query\nretrieval tasks using density matrix transformation. In Proc. of SIGIR . ACM, 871â€“\n874.\n[20] Massimo Melucci and Keith van Rijsbergen. 2011. Quantum Mechanics and\nInformation Retrieval . Springer Berlin Heidelberg, Berlin, Heidelberg, 125â€“155.\n[21] Donald Metzler and W. Bruce Croft. 2005. A Markov random field model for\nterm dependencies. In Proc. of SIGIR . 472â€“479.\n[22] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.\nDistributed representations of words and phrases and their compositionality. In\nProc. of NIPS . 3111â€“3119.\n[23] Michael A. Nielsen and Isaac L. Chuang. 2011. Quantum Computation and\nQuantum Information: 10th Anniversary Edition (10th ed.). Cambridge University\nPress, New York, NY, USA.\n[24] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:\nGlobal vectors for word representation. In Proc. of EMNLP . 1532â€“1543.\n[25] Benjamin Piwowarski, Ingo Frommholz, Mounia Lalmas, and Keith van Rijsber-\ngen. 2010. What can quantum theory bring to information retrieval. In Proc. of\nCIKM. 59â€“68.\n[26] Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to rank short text\npairs with convolutional deep neural networks. In Proc. of SIGIR . ACM, 373â€“382.\n[27] Aliaksei Severyn and Alessandro Moschitti. 2016. Modeling relational informa-\ntion in question-answer pairs with convolutional neural networks.arXiv preprint\narXiv:1604.01178 (2016).\n[28] Fei Song and W. Bruce Croft. 1999. A General Language Model for Information\nRetrieval (poster abstract). In Proc. of SIGIR . 279â€“280.\n[29] Alessandro Sordoni, Yoshua Bengio, and Jian-Yun Nie. 2014. Learning Concept\nEmbeddings for Query Expansion by Quantum Entropy Minimization.. In Proc.\nof AAAI , Vol. 14. 1586â€“1592.\n[30] Alessandro Sordoni and Jian-Yun Nie. 2013. Looking at vector space and language\nmodels for ir using density matrices. In Proc. of QI . Springer, 147â€“159.\n[31] Alessandro Sordoni, Jian-Yun Nie, and Yoshua Bengio. 2013. Modeling term\ndependencies with quantum language models for IR. In Proc. of SIGIR . ACM,\n653â€“662.\n[32] Yi Tay, Minh C Phan, Luu Anh Tuan, and Siu Cheung Hui. 2017. Learning to\nRank Question Answer Pairs with Holographic Dual LSTM Architecture. In Proc.\nof SIGIR . ACM, 695â€“704.\n[33] Cornelis Joost Van Rijsbergen. 2004. The geometry of information retrieval . Cam-\nbridge University Press.\n[34] Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi Cheng.\n2016. A Deep Architecture for Semantic Matching with Multiple Positional\nSentence Representations.. In Proc. of AAAI . 2835â€“2841.\n[35] Mengqiu Wang, Noah A Smith, and Teruko Mitamura. 2007. What is the Jeopardy\nModel? A Quasi-Synchronous Grammar for QA.. InProc. of EMNLP-CoNLL , Vol. 7.\n22â€“32.\n[36] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A Challenge Dataset\nfor Open-Domain Question Answering.. In Proc. of EMNLP . Citeseer, 2013â€“2018.\n[37] Wenpeng Yin, Hinrich SchÃ¼tze, Bing Xiang, and Bowen Zhou. 2015. Abcnn:\nAttention-based convolutional neural network for modeling sentence pairs.arXiv\npreprint arXiv:1512.05193 (2015).\n[38] Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep\nlearning for answer sentence selection. arXiv preprint arXiv:1412.1632 (2014).\n[39] ChengXiang Zhai. 2008. Statistical Language Models for Information Retrieval .\nMorgan & Claypool Publishers.\n[40] Peng Zhang, Jiabin Niu, Zhan Su, Benyou Wang, Liqun Ma, and Dawei Song.\n2018. End-to-End Quantum-like Language Models with Application to Question\nAnswering. In Proc. of AAAI . 5666â€“5673.\n[41] Xiaozhao Zhao, Peng Zhang, Dawei Song, and Yuexian Hou. 2011. A novel re-\nranking approach inspired by quantum measurement. In Proc. of ECIR . 721â€“724.\n[42] Guido Zuccon and Leif Azzopardi. 2010. Using the Quantum Probability Ranking\nPrinciple to Rank Interdependent Documents. In Proc. of ECIR . 357â€“369.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I162868743",
      "name": "Tianjin University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I138689650",
      "name": "University of Padua",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I125839683",
      "name": "Beijing Institute of Technology",
      "country": "CN"
    }
  ]
}