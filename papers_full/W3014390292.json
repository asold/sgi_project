{
  "title": "Unsupervised Domain Clusters in Pretrained Language Models",
  "url": "https://openalex.org/W3014390292",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4223699700",
      "name": "Aharoni, Roee",
      "affiliations": [
        "Laboratoire d'Informatique de Paris-Nord",
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A4221504774",
      "name": "Goldberg, Yoav",
      "affiliations": [
        "Laboratoire d'Informatique de Paris-Nord",
        "Bar-Ilan University",
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2970279348",
    "https://openalex.org/W3015504467",
    "https://openalex.org/W2950940239",
    "https://openalex.org/W2970295111",
    "https://openalex.org/W2946379889",
    "https://openalex.org/W2740743644",
    "https://openalex.org/W2953958347",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2962899377",
    "https://openalex.org/W2758334418",
    "https://openalex.org/W2251784502",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2147262247",
    "https://openalex.org/W2953337107",
    "https://openalex.org/W1794756993",
    "https://openalex.org/W2117045850",
    "https://openalex.org/W2982407184",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963149635",
    "https://openalex.org/W2988249555",
    "https://openalex.org/W2105410942",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2751885040",
    "https://openalex.org/W2810533336",
    "https://openalex.org/W2987154291",
    "https://openalex.org/W2757592053",
    "https://openalex.org/W2218630638",
    "https://openalex.org/W2403717966",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970678329",
    "https://openalex.org/W2511717148",
    "https://openalex.org/W2952474700",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W3004726085",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2988849053",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2750588180",
    "https://openalex.org/W2902689991",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2577063019",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2964125718",
    "https://openalex.org/W3018564137",
    "https://openalex.org/W1532325895",
    "https://openalex.org/W1905522558",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2772572269",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2963897095",
    "https://openalex.org/W2939335894",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2982393719",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963281280",
    "https://openalex.org/W2909737760",
    "https://openalex.org/W2983829373",
    "https://openalex.org/W2117278770",
    "https://openalex.org/W1483007503"
  ],
  "abstract": "The notion of \"in-domain data\" in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision -- suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and by precision and recall of sentence selection with respect to an oracle.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7747–7763\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n7747\nUnsupervised Domain Clusters in Pretrained Language Models\nRoee Aharoni1 & Yoav Goldberg1,2\n1 Computer Science Department, Bar Ilan University\n2 Allen Institute for Artiﬁcial Intelligence\nfirst.last@gmail.com\nAbstract\nThe notion of “in-domain data” in NLP is of-\nten over-simplistic and vague, as textual data\nvaries in many nuanced linguistic aspects such\nas topic, style or level of formality. In addi-\ntion, domain labels are many times unavail-\nable, making it challenging to build domain-\nspeciﬁc systems. We show that massive pre-\ntrained language models implicitly learn sen-\ntence representations that cluster by domains\nwithout supervision – suggesting a simple data-\ndriven deﬁnition of domains in textual data.\nWe harness this property and propose domain\ndata selection methods based on such models,\nwhich require only a small set of in-domain\nmonolingual data. We evaluate our data se-\nlection methods for neural machine translation\nacross ﬁve diverse domains, where they outper-\nform an established approach as measured by\nboth BLEU and by precision and recall of sen-\ntence selection with respect to an oracle.\n1 Introduction\nIt is common knowledge in modern NLP that us-\ning large amounts of high-quality training data is a\nkey aspect in building successful machine-learning\nbased systems. For this reason, a major challenge\nwhen building such systems is obtaining data in\nthe domain of interest. But what deﬁnes a do-\nmain? Natural language varies greatly across top-\nics, styles, levels of formality, genres and many\nother linguistic nuances (van der Wees et al., 2015;\nvan der Wees, 2017; Niu et al., 2017). This over-\nwhelming diversity of language makes it hard to\nﬁnd the right data for the task, as it is nearly im-\npossible to well-deﬁne the exact requirements from\nsuch data with respect to all the aforementioned\naspects. On top of that, domain labels are usually\nunavailable – e.g. in large-scale web-crawled data\nlike Common Crawl1 which was recently used to\n1https://commoncrawl.org/\nit\nkoran\nsubtitles\nmedical\nlaw\nbert-base-uncased\nFigure 1: A 2D visualization of average-pooled BERT\nhidden-state sentence representations using PCA. The\ncolors represent the domain for each sentence.\ntrain state-of-the-art pretrained language models\nfor various tasks (Raffel et al., 2019).\nDomain data selectionis the task of selecting the\nmost appropriate data for a domain from a large cor-\npus given a smaller set of in-domain data (Moore\nand Lewis, 2010; Axelrod et al., 2011; Duh et al.,\n2013; Silva et al., 2018). In this work, we propose\nto use the recent, highly successful self-supervised\npre-trained language models, e.g. Devlin et al.\n(2019); Liu et al. (2019) for domain data selec-\ntion. As pretrained LMs demonstrate state-of-the-\nart performance across many NLP tasks after being\ntrained on massive amounts of data, we hypothe-\nsize that the robust representations they learn can\nbe useful for mapping sentences to domains in an\nunsupervised, data-driven approach. We show that\nthese models indeed learn to cluster sentence repre-\nsentations to domains without further supervision\n(e.g. Figure 1), and quantify this phenomenon by\nﬁtting Gaussian Mixture Models (GMMs) to the\nlearned representations and measuring the purity of\nthe resulting unsupervised clustering. We then pro-\n7748\npose methods to leverage these emergent domain\nclusters for domain data selection in two ways:\n• Via distance-based retrieval in the sentence\nembedding space induced by the pretrained\nlanguage model.\n• By ﬁne-tuning the pretrained language model\nfor binary classiﬁcation, where positive exam-\nples are from the domain of interest.\nOur methods enable to select relevant data for\nthe task while requiring only a small set of mono-\nlingual in-domain data. As they are based solely\non the representations learned by self-supervised\nLMs, they do not require additional domain la-\nbels which are usually vague and over-simplify\nthe notion of domain in textual data. We evaluate\nour method on data selection for neural machine\ntranslation (NMT) using the multi-domain German-\nEnglish parallel corpus composed by Koehn and\nKnowles (2017). Our data selection methods en-\nable to train NMT models that outperform those\ntrained using the well-established cross-entropy dif-\nference method of Moore and Lewis (2010) across\nﬁve diverse domains, achieving a recall of more\nthan 95% in all cases with respect to an oracle that\nselects the “true” in-domain data.\nOur contributions in this work are as follows.\nFirst, we show that pre-trained language models\nare highly capable of clustering textual data to do-\nmains with high accuracy in a purely unsupervised\nmanner. Second, we propose methods to select\nin-domain data based on this property using vector-\nspace retrieval and positive-unlabeled ﬁne-tuning\nof pretrained language models for binary classiﬁca-\ntion. Third, we show the applicability of our pro-\nposed data selection methods on a popular bench-\nmark for domain adaptation in machine translation.\nAn additional contribution is a new, improved data\nsplit we create for this benchmark, as we point on\nissues with previous splits used in the literature.\nThe code and data for this work is publicly avail-\nable.2 We hope this work will encourage more re-\nsearch on understanding the data landscape in NLP,\nenabling to “ﬁnd the right data for the task” in the\nage of massive models and diverse data sources.\n2https://github.com/roeeaharoni/\nunsupervised-domain-clusters\n2 Emerging Domain Clusters in\nPretrained Language Models\n2.1 Motivation\nThe proliferation of massive pretrained neural lan-\nguage models such as ELMo (Peters et al., 2018),\nBERT (Devlin et al., 2019) or RoBERTa (Liu et al.,\n2019) has enabled great progress on many NLP\nbenchmarks (Wang et al., 2018, 2019a). Larger\nand larger models trained on billions of tokens of\nraw text are released in an ever-increasing pace\n(Raffel et al., 2019), enabling the NLP community\nto ﬁne-tune them for the task of interest. While\nmany works tried to “probe” those models for the\nmorphological, syntactic and semantic information\nthey capture (Tenney et al., 2019; Goldberg, 2019;\nClark et al., 2019), an important aspect of language\nremained overlooked in this context – the domain\nthe data comes from, often referred to as the “data\ndistribution”.\nThe deﬁnition of domain is many times vague\nand over-simplistic (e.g. “medical text” may be\nused for biomedical research papers and for clin-\nical conversations between doctors and patients,\nalthough the two vary greatly in topic, formality\netc.). A common deﬁnition treats a domain as a\ndata source: “a domain is deﬁned by a corpus from\na speciﬁc source, and may differ from other do-\nmains in topic, genre, style, level of formality, etc.”\n(Koehn and Knowles, 2017). We claim that a more\ndata-driven deﬁnition should take place, as differ-\nent data sources may have sentences with similar\ntraits and vice versa - a single massive web-crawled\ncorpus contains texts in numerous styles, topics and\nregisters. Our analysis in Section 2 shows examples\nfor such cases, e.g. a sentence discussing “Viruses\nand virus-like organisms” in a legal corpus.\nWe hypothesize that massive pretrained LMs\ncan learn representations that cluster to domains,\nas texts from similar domains will appear in similar\ncontexts. We test this hypothesis across several\nlarge, publicly-available pretrained LMs; we ex-\nplore both masked-language-models (MLMs) and\nauto-regressive LMs.\n2.2 Method\nWe encode multi-domain data at the sentence level\ninto vector representations. We then cluster these\nvector representations for each model using a Gaus-\nsian Mixture Model (GMM) with k pre-deﬁned\nclusters. We chose GMM as our clustering ap-\nproach as it allows soft assignments (vs. hard as-\n7749\nk=5 k=10 k=15\nRandom 15.08 (±0.0) 16.77 (±0.0) 17.78 (±0.0)\nLDA 24.31 (±0.99) 26.73 (±2.19) 30.79 (±2.97)\nwith PCA (n=50) without PCA\nk=5 k=10 k=15 k=5 k=10 k=15\nword2vec 53.65 (±0.79) 68.14 (±2.58) 73.44 (±0.68) 45.93 65.80 76.26\nBERT-base 87.66 (±0.24) 88.02 (±1.10) 88.37 (±0.66) 85.74 85.08 86.37\nBERT-large 85.64 (±6.13) 87.61 (±0.26) 89.07 (±0.53) 68.56 86.53 86.99\nDistillBERT 83.68 (±7.14) 86.31 (±0.86) 87.53 (±0.85) 79.00 86.42 88.14\nRoBERTa-base 79.05 (±0.10) 86.39 (±0.90) 86.51 (±0.28) 70.21 80.35 81.49\nRoBERTa-large 80.61 (±0.33) 89.04 (±0.15) 89.94 (±0.23) 69.88 81.07 85.91\nGPT-2 70.30 (±0.05) 84.76 (±0.30) 82.56 (±1.29) 37.82 39.02 41.45\nXLNet 55.72 (±0.69) 68.17 (±3.93) 72.65 (±1.92) 30.36 32.96 48.55\nTable 1: Unsupervised domain clustering as measured by purity for the different models. Best results are marked\nin bold for each setting.\nsignments as in e.g. K-means) which we think ﬁts\nthe task better (as a sentence can be seen as drawn\nfrom a mixture of several domain). 3 In all cases,\nto create a sentence representation we perform av-\nerage pooling of the last hidden state (before the\nsoftmax layer) for each token in the sentence.4 To\naccelerate the clustering process and enable visual-\nization we also experiment with performing dimen-\nsionality reduction with PCA over the sentence vec-\ntors before clustering them. We experiment with k\nin 5, 10 and 15 to test how adding ﬂexibility would\nimprove the domain clustering accuracy.\n2.3 Models and Baselines\nFor MLM-based models we use BERT (Devlin\net al., 2019), DistilBERT (Sanh et al., 2019) and\nRoBERTa (Liu et al., 2019) (in both the base and\nlarge versions). For autoregressive models we use\nGPT-2 (Radford et al., 2018) and XLNet (Yang\net al., 2019). In all cases we use the implementa-\ntions from the HuggingFace Transformers toolkit\n(Wolf et al., 2019). We also evaluated three addi-\ntional, simpler baselines. The ﬁrst is using repre-\nsentations from word2vec (Mikolov et al., 2013),\nwhere we average-pooled the word vectors for the\ntokens that were present in the model vocabulary.\nThe second is using Latent Dirichlet Allocation\n(LDA, Blei et al., 2003), which is a classic ap-\nproach to unsupervised clustering of text.5 We also\n3See further discussion comparing GMMs and K-means\nin Daume (2009).\n4Using the penultimate layer or others may result in better\nperformance; we leave this for future work.\n5We used the LDA implementation provided in the Gensim\ntoolkit: https://radimrehurek.com/gensim/\nreport results for a baseline which assigns sentences\nby sampling randomly from a uniform distribution\nover the clusters.\n2.4 Evaluation\nTo evaluate the unsupervised domain clustering we\nused the multi-domain corpus proposed by Koehn\nand Knowles (2017) which includes textual data in\nﬁve diverse domains: subtitles6, medical text (PDF\ndocuments from the European Medicines Agency),\nlegal text (legislative text of the European Union),\ntranslations of the Koran, and IT-related text (man-\nuals and localization ﬁles of open-source software).\nThis dataset includes parallel sentences in English\nand German; for this experiment we used the En-\nglish portion of the data. See more details on the\ndataset in Section 3.1. We used 2000 distinct sen-\ntences from each domain. To evaluate whether the\nresulting clusters indeed capture the domains the\ndata was drawn from we measure the clustering\npurity, which is a well-known metric for evaluat-\ning clustering (Manning et al., 2008). To measure\nthe clustering purity, we assign each unsupervised\ncluster with the most common “true” domain in the\nsentences assigned to that cluster, and then com-\npute the accuracy according to this majority-based\ncluster-domain assignment (note that in this case\nseveral unsupervised clusters can be assigned to\nthe same domain). In cases where randomness is\ninvolved we run each experiment ﬁve times with\ndifferent initializations and report the mean and\nvariance of the purity metric for each model.\n6From http://www.opensubtitles.org/\n7750\nit\nkoran\nsubtitlesmedical\nlaw\nPredicted label\nit\nkoran\nsubtitles\nmedical\nlaw\nTrue label\n1927 0 55 16 2\n4 1767 225 0 4\n47 21 1918 9 5\n340 0 82 1413 165\n206 0 10 58 1726\nFigure 2: A confusion matrix for clustering with k=5\nusing BERT-base.\n2.5 Results and Discussion\nAs can be seen in Table 1, pre-trained language\nmodels are indeed highly capable of generating\nsentence representations that cluster by domains,\nresulting in up to 87.66%, 89.04% and 89.94% ac-\ncuracy when using k=5, k=10 and k=15 clusters,\nrespectively, across 10,000 sentences in 5 domains.\nWe ﬁnd these scores remarkably high given our\nstraight-forward average-pooling strategy and that\nno domain-supervision was involved in the process\nof learning the pre-trained representations. Figure\n3 also demonstrates the quality of the obtained clus-\nters in 2D using the BERT-base model, where the\nellipses describe the mean and variance parameters\nlearned for each cluster by the GMM with k = 5.7\nWe note that some classes of models did better\nthan others: while all vector-based models did far\nbetter than the random and LDA baselines 8, the\nMLM-based models dominated in all cases over\nword2vec and the auto-regressive models. This\nmay be explained by the fact that the MLM-based\nmodels use the entire sentence context when gen-\nerating the representations for each token, while\nthe auto-regressive models only use the past con-\ntext, and word2vec uses a limited window context.\nUsing PCA improved performance in most cases\nand especially for the auto-regressive models, al-\nthough the results for the MLMs remain high in\n7Similar visualizations for additional models are available\nin the supplementary material.\n8Note that the LDA models were trained using the multi-\ndomain data alone, and did not utilize additional pretraining\nas in the other, more successful models. This may explain\ntheir relatively weak performance.\nboth cases – suggesting that these models encode\nthe information very differently.\n2.6 Analysis\nAs can be seen in Figure 3, in some areas the do-\nmains are somewhat overlapping in the embedding\nspace, which may lead to outlier cases where ex-\namples from one domain are assigned to a cluster\nof a another domain. We plot a confusion matrix\n(Figure 2) to analyze this further based on the clus-\ntering with BERT-base and k=5. We ﬁrst note that\nthe outlier sentences are much shorter than the av-\nerage sentence length in the corpus (11.62 tokens\non average for outliers vs. 20.5 tokens on average\nin general). This makes sense as shorter sentences\ncontain less information, making it harder to assign\nthem to an appropriate cluster. Table 2 shows ex-\namples of outlier sentences, assigned to clusters of\ndomains different from their originating domain.\nWe can see that in many cases the assignments are\nsensible – for example for sentences originating\nfrom the subtitles corpus, a sentence that mentions\n“great priest” is assigned to the Koran cluster, a\nsentence that mentions “The International Criminal\nCourt in The Hague” is assigned to the Law cluster,\na sentence that mentions “the virus” is assigned to\nthe Medical cluster and so on. This strengthens our\nclaim that deﬁning domains based on the corpus\nthey originated from is over-simplistic, and using\na data-driven approach may enable to ﬁnd better\ndomain assignments across different corpora.\nThe domain that attracted the largest number\nof outliers is the IT domain cluster, with 597 sen-\ntences assigned to it from other domains. Looking\nit\nkoran\nsubtitles\nmedical\nlaw\nbert-base-uncased\nFigure 3: A 2D visualization of the unsupervised GMM\nclustering for the same sentences as in Figure 1.\n7751\nSubtitles assigned to Koran Subtitles assigned to Medical\nI am Spa’am, high priest of the boars. Oxygen supply at 50%.\nJoseph, go in peace, and the Lord be with you. Or it can help her walk again if the virus is kept in check\nwith this.\nSubtitles assigned to IT Subtitles assigned to Law\nPush it up to the front of the screen. Statutes, transcripts, redacted immunity agreements.\nPolyalloy requires programming to take permanent The Security Council therefore must press for his immediate\nform. referral to the International Criminal Court in The Hague.\nLaw assigned to Medical Law assigned to IT\n- Viruses and virus-like organisms ”INFORMATION SOCIETY STATISTICS\nwhere the glucose content is equal to or less than This document must be attached to the certiﬁcate and ﬁeld\nthe fructose content. with it, except where there is a computerised checking system.\nMedical assigned to Law Medical assigned to IT\nThis will be introduced by a Regulation adopted by the An updated and improved version of the CD-ROM was issued\nEuropean Commission. to all subscribers during the ﬁrst half of the year.\nThe marketing authorisation was renewed on 22 May - All tables will be based on generic and not product-speciﬁc\n2002 and 22 May 2007. data.\nIT assigned to Medical IT assigned to Subtitles\nR65: Harmful: may cause lung damage if swallowed At the end we say good bye.\nAutomatic Red-Eye Removal What would you like to do for your next shot?\nTable 2: Sentences from one domain which were assigned to another domain by the BERT-based clustering, k=5.\nmore closely we ﬁnd that more than half of these\nsentences (340 out of 597) included numbers (e.g.\n“34% 25% 34%” (from medical), “(b) reference\nnumber 20 is deleted;” (from law), “(Command of\nProstration # 1)” (from Koran) or “The message,\nR2.” (from subtitles)). As numbers appear in many\ndifferent contexts, they may be harder to assign to\na speciﬁc domain by the context-aware language\nmodels in such short sentences. The second largest\nattractor of outliers is the Subtitles cluster, with\n372 sentences assigned to it from other domains.\nWe ﬁnd that most of these sentences contain per-\nsonal pronouns or question marks (228 out of 372,\n61.2%) while the ratio of such sentences in the en-\ntire corpus is only 40%. Examples include “Why\ndid you choose the name & amarok;?” (from IT),\nor “What is Avonex?” (from Medical). This may\nbe expected as the subtitles corpus mainly includes\ntranscriptions of spoken, conversational language,\nand “conversation tends to have more verbs, more\npersonal pronouns, and more questions” (Conrad\nand Biber, 2005). Another possible reason for the\nsubtitles domain to attract outliers is the fact that\nthis is the least-topical cluster: movies and TV\nseries may discuss diverse topics, unlike medical,\nreligious, legal and technical texts that may have a\nmore cohesive topic.\n3 Neural Machine Translation in a\nMulti-Domain Scenario\nAs we showed that pre-trained language models\nare indeed very useful in clustering sentence repre-\nsentations by domains in an unsupervised manner,\nwe now seek to harness this property for a down-\nstream task – domain data selection for machine\ntranslation. Domain data selection is the task of\nselecting examples from a large corpus which are\nas close as possible to the domain of interest, given\na smaller set of in-domain examples. The selected\nexamples can be used to either (1) train a domain-\nspeciﬁc model from scratch (Axelrod et al., 2011),\n(2) ﬁne-tune a pre-trained general-domain model\n(Sajjad et al., 2017; Silva et al., 2018), or (3) prior-\nitize data for annotation as in an Active-Learning\nframework, if only monolingual data is available\n(Haffari et al., 2009). To demonstrate the need for\ndomain data selection and set the stage for our data\nselection experiments, we perform preliminary ex-\nperiments with NMT in a multi-domain scenario.\n3.1 Multi-Domain Dataset\nTo simulate a diverse multi-domain setting we use\nthe dataset proposed in Koehn and Knowles (2017),\nas it was recently adopted for domain adaptation\nresearch in NMT (Hu et al., 2019; M ¨uller et al.,\n2019; Dou et al., 2019a,b). The dataset includes\nparallel text in German and English from ﬁve di-\nverse domains (Medical, Law, Koran, IT, Subtitles;\nas discussed in Section 2), available via OPUS\n(Tiedemann, 2012; Aulamo and Tiedemann, 2019).\nIn a preliminary analysis of the data we found\nthat in both the original train/dev/test split by\nKoehn and Knowles (2017) and in the more re-\ncent split by M¨uller et al. (2019) there was overlap\nbetween the training data and the dev/test data. 9\nFixing these issues is important, as it may affect\nthe conclusions one draws from experiments with\n9More details are available in the supplementary material.\n7752\nOriginal New Split\nMedical 1,104,752 248,099\nLaw 715,372 467,309\nIT 378,477 222,927\nKoran 533,128 17,982\nSubtitles 22,508,639 14,458,058\nTable 3: Number of training examples for each domain\nin the original split (M¨uller et al., 2019) and in our split.\nthis dataset. For example, as overlapping devel-\nopment sets favor memorization of the training\nset, one may choose checkpoints and report results\non over-ﬁtting models. This is especially relevant\nwith neural sequence-to-sequence models, as they\nare highly susceptible to memorization (Aharoni\nand Goldberg, 2018) and hallucination (Lee et al.,\n2018), as conﬁrmed by M ¨uller et al. (2019).\nTo create a better experimental setting to test\ngeneralization within and across domains, we cre-\nate a new data split where we ensure that no such\noverlap between the training, development and test\nsets occur. We started from the split of M ¨uller\net al. (2019) as it included newer versions of some\nof the datasets.10 Furthermore, we did not allow\nmore than one translation of a given source or tar-\nget sentence, as such cases were very frequent in\nthe dataset and usually stand for duplicate sentence\npairs (See Table 3). For example, applying this\nﬁltering reduced the size of the Koran corpus from\n533,128 sentence pairs to only 17,982. Finally,\nfollowing M ¨uller et al. (2019) we cap the subti-\ntles corpus to 500,000 sentence pairs as it is much\nlarger than the rest. We make the new split pub-\nlicly available and hope it will enable better future\nexperimentation on this important subject.11\n3.2 Cross-Domain Experiments\nExperimental SetupWe follow Hu et al. (2019)\nand train domain-speciﬁc models for all domains.\nWe then evaluate each model across the different\ndomain test sets, enabling us to understand the ef-\nfect of different domains on the downstream MT\nperformance and to set up strong baselines for data\nselection experiments. We also train a general-\ndomain model using the available data from all\ndomains, as it is also a common approach in multi-\ndomain scenarios (M ¨uller et al., 2019). In all ex-\nperiments we use a similar Transformer (Vaswani\net al., 2017) model, and only control for the train-\n10Their dataset is available in: https://github.com/\nZurichNLP/domain-robustness\n11https://github.com/roeeaharoni/\nunsupervised-domain-clusters\nMedical Law Koran IT Subtitles\nMedical 56.5 18.3 1.9 11.4 4.3\nLaw 21.7 59 2.7 13.1 5.4\nKoran 0.1 0.2 15.9 0.2 0.5\nIT 14.9 9.6 2.8 43 8.6\nSubtitles 7.9 5.5 6.4 8.5 27.3\nAll 53.3 57.2 20.9 42.1 27.6\nTable 4: SacreBLEU (Post, 2018) scores of our base-\nline systems on the test sets of the new data split. Each\nrow represents the results from one model on each test\nset. The best result in each column is marked in bold.\ning data. More details on the exact training and\nhyperparameter settings for the NMT models are\navailable in the supplementary material.\nResults The results for the cross-domain evalua-\ntion are available in Table 4. In most cases, the best\nresults for each domain are obtained by training on\nthe in-domain data. Training on all the available\ndata helped mostly for the Koran test set. This is\nexpected as the training data for this domain is con-\nsiderably smaller than the training data for rest of\nthe domains (Table 3). We can also see that more\ndata is not necessarily better (Gasc´o et al., 2012):\nwhile the subtitles corpus is the largest of all 5 and\nincludes 500,000 sentence pairs, it is second to last\nin performance as measured by the average BLEU\nacross all test sets.\nCross-Domain BLEU vs. Cluster Proximity\nAn interesting observation can be made with re-\nspect to the visual analysis of the domain clusters\nas depicted in Figure 3: as the Medical cluster\n(in Yellow), Law cluster (in Purple) and IT cluster\n(in Red) are close to each other in the embedding\nspace, their cross-domain BLEU scores are also\nhigher. For example, note how in the results for the\nMedical domain-speciﬁc model (ﬁrst row in Table\n4), the BLEU scores on the Law and IT test sets are\nmuch higher in comparison to those on the Koran\nand Subtitles test sets, which clusters are farther\naway in the visualized embedding space. Similarly,\nas the Subtitles cluster (Blue) is closer to the Koran\ncluster (Green), the highest cross-domain BLEU\nscore on the Koran test set is from the Subtitles\nmodel. To further quantify this phenomenon, we\nplot and measure Pearson’s correlation between the\ncosine similarity of the centroids for the English\nBERT-based dev sentence representations for each\ndomain pair, and the cross-domain BLEU score for\nthis domain pair. This is shown in Figure 4. We can\nsee the general trend where the closer the domain\ncentroids are (with a similarity of 1 for training\nand evaluating on the same domain), the higher\nthe cross-domain BLEU is between those domains,\n7753\nFigure 4: The cosine similarity between the centroids\nof the BERT representations for each domain pair vs.\nthe corresponding cross-domain BLEU.\nresulting in a Pearson’s correlation of 0.81 (strong\ncorrelation). This suggests that such preliminary\nvisual analysis can be a useful tool for understand-\ning the relationship between diverse datasets, and\nmotivates the use of pre-trained language model\nrepresentations for domain data selection in MT.\n4 Domain Data Selection with Pretrained\nLanguage Models\nAs shown in the previous section, using the right\ndata is critical for achieving good performance on\nan in-domain test set, and more data is not neces-\nsarily better. However, in real-world scenarios, the\navailability of data labeled by domain is limited,\ne.g. when working with large scale, web-crawled\ndata. In this section we focus on a data-selection\nscenario where only a very small number of in-\ndomain sentences are used to select data from a\nlarger unlabeled parallel corpus. An established\nmethod for data selection was proposed by Moore\nand Lewis (2010), which was also used in training\nthe winning systems in WMT 2019 (Ng et al., 2019;\nBarrault et al., 2019). This method compares the\ncross-entropy, according to domain-speciﬁc and\nnon-domain-speciﬁc language models, for each\ncandidate sentence for selection. The sentences\nare then ranked by the cross-entropy difference,\nand only the top sentences are selected for training.\nWhile the method by Moore and Lewis (2010)\nis tried-and-true, it is based on simple n-gram lan-\nguage models which cannot generalize beyond the\nn-grams that are seen in the in-domain set. In ad-\ndition, it is restricted to the in-domain and general-\ndomain datasets it is trained on, which are usually\nsmall. On the contrary, pre-trained language mod-\nels are trained on massive amounts of text, and, as\nwe showed through unsupervised clustering, learn\nrepresentations with domain-relevant information.\nIn the following sections, we investigate whether\nthis property of pretrained language models makes\nthem useful for domain data selection.\n4.1 Methods\nWe propose two methods for domain data selection\nwith pretrained language models.\nDomain-Cosine In this method we ﬁrst compute\na query vector, which is the element-wise average\nover the vector representations of the sentences in\nthe small in-domain set. We use the same sentence-\nlevel average-pooling approach as described in Sec-\ntion 2 to obtain sentence representations. We then\nretrieve the most relevant sentences in the train-\ning set by computing the cosine similarity of each\nsentence with this query vector and ranking the\nsentences accordingly.\nDomain-Finetune It is now common knowl-\nedge that pretrained language models are especially\nuseful when ﬁne-tuned for the task of interest in\nan end-to-end manner (Ruder et al., 2019). In this\nmethod we ﬁne-tune the pretrained LM for binary\nclassiﬁcation, where we use the in-domain sen-\ntences as positive examples, and randomly sam-\npled general-domain sentences as negative exam-\nples. We then apply this classiﬁer on the general-\ndomain data and pick the sentences that are classi-\nﬁed as positive as in-domain, or choose the top-k\nsentences as ranked by the classiﬁer output distri-\nbution. This can be seen as an instance of positive-\nunlabeled learning for document-set expansion; see\nJacovi et al. (2019) for a recent discussion and\nmethodology for this task.\nNegative Sampling with Pre-ranking One\nproblem that may rise when randomly sampling\nnegative examples is that unlabeled in-domain sen-\ntences from the general-domain data may be sam-\npled as negative examples – deteriorating the clas-\nsiﬁer performance. To alleviate this issue, we\nperform a biased sampling of negative examples.\nWe ﬁrst rank the general-domain data using the\nwithout pre-ranking with pre-ranking\np r F1 p r F1\nSubtitles 0.722 0.984 0.833 0.964 0.978 0.971\nLaw 0.761 0.94 0.841 0.944 0.94 0.942\nMedical 0.821 0.916 0.866 0.929 0.92 0.925\nIT 0.848 0.956 0.898 0.955 0.98 0.967\nKoran 0.966 0.958 0.962 0.994 0.974 0.984\nTable 5: Ablation analysis showing precision (p) recall\n(r) and F1 for the binary classiﬁcation accuracy on a\nheld-out set, with and without pre-ranking.\n7754\nMedical Law Koran IT Subtitles Average\nRandom-500k 49.8 53.3 18.5 37.5 25.5 36.92\nMoore-Lewis-Top-500k 55 58 21.4 42.7 27.3 40.88\nDomain-Cosine-Top-500k 52.7 58 22 42.5 27.1 40.46\nDomain-Finetune-Top-500k 54.8 58.8 21.8 43.5 27.4 41.26\nDomain-Finetune-Positive 55.3 58.7 19.2 42.5 27 40.54\nOracle 56.5 59 15.9 43 27.3 40.34\nAll 53.3 57.2 20.9 42.1 27.6 40.22\nTable 6: SacreBLEU scores for the data selection experiments. Highest scores per column are marked in bold.\nDomain-Cosine method, and then sample negative\nexamples under a certain threshold in the ranking\n(in our experiments we sampled from the bottom\ntwo-thirds). Table 5 shows an ablation for such\npre-ranking, measuring precision, recall and F1\nfor binary classiﬁcation on a held-out set for each\ndomain. When not using pre-ranking, as the train-\ning data for the domain is larger, the precision is\nlower – since more in-domain examples are drawn\nas negative samples. Using pre-ranking indeed al-\nleviates this issue, achieving higher F1 scores in all\ncases. Given the results in Table 5 we always use\npre-ranking in the following experiments.\n4.2 Experimental Setup\nWe perform data selection experiments for each do-\nmain in the multi-domain dataset. As the small set\nof monolingual in-domain data we take the 2000\ndevelopment sentences from each domain. For the\ngeneral-domain corpus we concatenate the training\ndata from all domains, resulting in 1,456,317 sen-\ntences. To enable faster experimentation we used\nDistilBERT (Sanh et al., 2019) for the Domain-\nCosine and Domain-Finetune methods. More tech-\nnical details are available in the supplementary ma-\nterial. We compare our methods to four approaches:\n(1) The established method by Moore and Lewis\n(2010), (2) a random selection baseline, (3) an ora-\ncle which is trained on all the available in-domain\ndata, and (4) the model we train on all the domains\nconcatenated. We select the top 500k examples to\ncover the size of every speciﬁc in-domain dataset.\nWe train Transformer NMT models on the selected\ndata with a similar conﬁguration to the ones trained\nin the cross-domain evaluation.\n4.3 Results\nThe results are available in Table 6. We can see\nthat all selection methods performed much bet-\nter in terms of BLEU than random selection. It\nis also nice to see that all selection methods per-\nformed better than using all the available data or\nthe oracle-selected data when averaged across all\nMoore-Lewis D-Cosine D-Finetune\np r p r p r\nMedical 0.476 0.955 0.391 0.788 0.485 0.975\nLaw 0.836 0.894 0.841 0.899 0.902 0.965\nKoran 0.35 0.985 0.36 0.989 0.36 0.998\nIT 0.441 0.985 0.382 0.857 0.447 0.998\nSubtitles 0.899 0.899 0.916 0.916 0.957 0.957\nAverage 0.6 0.944 0.578 0.89 0.63 0.979\nTable 7: Precision (p) and recall (r) for data selection\nof 500k sentences with respect to the oracle selection.\ndomains, showing again that more data is not nec-\nessarily better in multi-domain scenarios and that\ndata selection is a useful approach. Regarding a\ncomparison of the data selection methods, Moore-\nLewis performed better than Domain-Cosine, while\nDomain-Finetune performed best, showing the ben-\neﬁt of ﬁne-tuning large pretrained models for the\ndata selection task. Using the positively-labeled\nexamples alone (Domain-Finetune-Positive) per-\nformed worse than using the top 500k examples\nbut better than Domain-Cosine, while not requiring\nto determine the number of selected sentences.\n4.4 Analysis\nWe perform an analysis on the selected datasets,\nwhere we measure the precision and recall of sen-\ntence selection with respect to the oracle selection.\nThe results are available in Table 7. As also re-\nﬂected in the BLEU scores, the Domain-Finetune\nmethod resulted in the highest domain recall with a\nminimum of 97.5, while Moore-Lewis and Domain-\nCosine scored 89.4 and 78.8 respectively. We ﬁnd\nthese results very appealing given that only 2000\nin-domain sentences were used for selection for\neach domain out of 1.45 million sentences. Also\nnote that we used DistilBERT in these experiments:\nwe believe that using larger, non-distilled models\nmay result in even better selection performance\n(although at the price of larger computational re-\nquirements).\n5 Related Work\nPrevious works used n-gram LMs for data selection\n(Moore and Lewis, 2010; Axelrod et al., 2011) or\n7755\nother count-based methods (Axelrod, 2017; Ponce-\nlas et al., 2018; Parcheta et al., 2018; Santamar´ıa\nand Axelrod, 2019). While such methods work\nwell in practice, they cannot generalize beyond the\nN-grams observed in the in-domain datasets, which\nare usually small.\nDuh et al. (2013) proposed to replace n-gram\nmodels with RNN-based LMs with notable im-\nprovements. However, such methods do not cap-\nture the rich sentence-level global context as in the\nrecent self-attention-based MLMs; as we showed\nin the clustering experiments, autoregressive neural\nLMs were inferior to masked LMs in clustering the\ndata by domain. In addition, training large LMs\nmay be prohibitive without relying on pre-training.\nRegarding domain clustering for MT, Hasler\net al. (2014) discovered topics using LDA instead\nof using domain labels. Cuong et al. (2016) in-\nduced latent subdomains from the training data\nusing a dedicated probabilistic model.\nMany works used vector-based retrieval for data\nselection; Ruder and Plank (2017) learn to select\ndata using Bayesian optimization, and explored\nword2vec for that purpose. Duma and Menzel\n(2016) create paragraph vectors for data selection\nin the context of SMT. Wang et al. (2017) use in-\nternal representations from the NMT model to per-\nform data selection. Bapna and Firat (2019) pro-\npose a mechanism for incorporating retrieved sen-\ntences for each instance for domain adaptation in\nNMT, using representations extracted from a pre-\ntrained NMT model. Farajian et al. (2017) explored\ninstance-based data selection in a multi-domain sce-\nnario using information retrieval methods.\nOther related works on domain adaptation in-\nclude Dou et al. (2019a) that adapts multi-domain\nNMT models with domain-aware feature embed-\ndings, which are learned via an auxiliary language\nmodeling task. Peris et al. (2017) proposed neural-\nnetwork based classiﬁers for data selection in SMT.\nFor more related work on data selection and domain\nadaptation in the context of MT, see the surveys by\nEetemadi et al. (2015) for SMT and more recently\nChu and Wang (2018) for NMT.\nUnrelated to MT, Ma et al. (2019) used BERT\nto select data for tasks from the GLUE benchmark\n(Wang et al., 2018). However, they assumed su-\npervision for all the different tasks/domains, while\nwe propose an unsupervised method requiring only\na small set of in-domain data. Also in the con-\ntext of pretrained language models, Gururangan\net al. (2020) show the importance of additional pre-\ntraining with in-domain data to improve the down-\nstream task-speciﬁc performance.\nWhile previous work made important contribu-\ntions to domain data selection, our work is the ﬁrst\nto explore massive pretrained language models for\nboth unsupervised domain clustering and for data\nselection in NMT.\n6 Conclusions and Future Work\nWe showed that massive pre-trained language mod-\nels are highly effective in mapping data to domains\nin a fully-unsupervised manner using average-\npooled sentence representations and GMM-based\nclustering. We suggest that such clusters are a more\nappropriate, data driven approach to domains in nat-\nural language than simplistic labels (e.g. “medical\ntext”), and that it will improve over time as better\nand larger pretrained LMs will become available.\nWe proposed new methods to harness this prop-\nerty for domain data selection using distance-based\nranking in vector space and pretrained LM ﬁne-\ntuning, requiring only a small set of in-domain data.\nWe demonstrated the effectiveness of our methods\non a new, improved data split we created for a pre-\nviously studied multi-domain machine translation\nbenchmark. Our methods perform similarly or bet-\nter than an established data selection method and\noracle in-domain training across all ﬁve domains\nin the benchmark.\nThis work just scratches the surface with what\ncan be done on the subject; possible avenues for\nfuture work include extending this with multilin-\ngual data selection and multilingual LMs (Conneau\nand Lample, 2019; Conneau et al., 2019; Wu et al.,\n2019; Hu et al., 2020), using such selection meth-\nods with domain-curriculum training (Zhang et al.,\n2019; Wang et al., 2019b), applying them on noisy,\nweb-crawled data (Junczys-Dowmunt, 2018) or for\nadditional tasks (Gururangan et al., 2020). Another\ninteresting avenue is applying this to unsupervised\nNMT, which is highly sensitive to domain mis-\nmatch (Marchisio et al., 2020; Kim et al., 2020).\nWe hope this work will encourage more research\non ﬁnding the right data for the task, towards more\nefﬁcient and robust NLP.\nAcknowledgements\nWe thank Wei Wang for early discussions on do-\nmain adaptation and data selection that inspired this\nwork during Roee’s internship in Google Translate.\n7756\nReferences\nRoee Aharoni and Yoav Goldberg. 2018. Split and\nrephrase: Better evaluation and stronger baselines.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n2: Short Papers), pages 719–724, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nMikko Aulamo and J¨org Tiedemann. 2019. The OPUS\nresource repository: An open package for creating\nparallel corpora and machine translation services. In\nProceedings of the 22nd Nordic Conference on Com-\nputational Linguistics, pages 389–394, Turku, Fin-\nland. Link¨oping University Electronic Press.\nAmittai Axelrod. 2017. Cynical selection of lan-\nguage model training data. arXiv preprint\narXiv:1709.02279.\nAmittai Axelrod, Xiaodong He, and Jianfeng Gao.\n2011. Domain adaptation via pseudo in-domain data\nselection. In Proceedings of the 2011 Conference on\nEmpirical Methods in Natural Language Processing,\npages 355–362, Edinburgh, Scotland, UK. Associa-\ntion for Computational Linguistics.\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1921–1931,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nLo¨ıc Barrault, Ond ˇrej Bojar, Marta R. Costa-juss `a,\nChristian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\nShervin Malmasi, Christof Monz, Mathias M ¨uller,\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\nFindings of the 2019 conference on machine transla-\ntion (WMT19). In Proceedings of the Fourth Con-\nference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1), pages 1–61, Florence, Italy. As-\nsociation for Computational Linguistics.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of ma-\nchine Learning research, 3(Jan):993–1022.\nChenhui Chu and Rui Wang. 2018. A survey of do-\nmain adaptation for neural machine translation. In\nProceedings of the 27th International Conference on\nComputational Linguistics, pages 1304–1319, Santa\nFe, New Mexico, USA. Association for Computa-\ntional Linguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. arXiv\npreprint arXiv:1906.04341.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems, pages\n7057–7067.\nSusan M Conrad and Douglas Biber. 2005. The fre-\nquency and use of lexical bundles in conversation\nand academic prose. Lexicographica.\nHoang Cuong, Khalil Sima’an, and Ivan Titov. 2016.\nAdapting to all domains at once: Rewarding domain\ninvariance in SMT. Transactions of the Association\nfor Computational Linguistics, 4:99–112.\nHal Daume. 2009. K-means vs GMM, sum-product vs\nmax-product.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nZi-Yi Dou, Junjie Hu, Antonios Anastasopoulos, and\nGraham Neubig. 2019a. Unsupervised domain adap-\ntation for neural machine translation with domain-\naware feature embeddings. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1417–1422, Hong Kong,\nChina. Association for Computational Linguistics.\nZi-Yi Dou, Xinyi Wang, Junjie Hu, and Graham Neu-\nbig. 2019b. Domain differential adaptation for neu-\nral machine translation. In Proceedings of the 3rd\nWorkshop on Neural Generation and Translation,\nHong Kong. Association for Computational Linguis-\ntics.\nKevin Duh, Graham Neubig, Katsuhito Sudoh, and Ha-\njime Tsukada. 2013. Adaptation data selection us-\ning neural language models: Experiments in ma-\nchine translation. In Proceedings of the 51st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 678–683,\nSoﬁa, Bulgaria. Association for Computational Lin-\nguistics.\nMirela-Stefania Duma and Wolfgang Menzel. 2016.\nData selection for IT texts using paragraph vector.\nIn Proceedings of the First Conference on Machine\nTranslation: Volume 2, Shared Task Papers, pages\n428–434, Berlin, Germany. Association for Compu-\ntational Linguistics.\n7757\nSauleh Eetemadi, William Lewis, Kristina Toutanova,\nand Hayder Radha. 2015. Survey of data-selection\nmethods in statistical machine translation. Machine\nTranslation, 29(3-4):189–223.\nM. Amin Farajian, Marco Turchi, Matteo Negri, and\nMarcello Federico. 2017. Multi-domain neural\nmachine translation through unsupervised adapta-\ntion. In Proceedings of the Second Conference on\nMachine Translation, pages 127–137, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nGuillem Gasc ´o, Martha-Alicia Rocha, Germ ´an\nSanchis-Trilles, Jes´us Andr´es-Ferrer, and Francisco\nCasacuberta. 2012. Does more data always yield\nbetter translations? In Proceedings of the 13th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 152–161,\nAvignon, France. Association for Computational\nLinguistics.\nYoav Goldberg. 2019. Assessing BERT’s syntactic\nabilities. arXiv preprint arXiv:1901.05287.\nSuchin Gururangan, Ana Marasovi, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. ACL.\nGholamreza Haffari, Maxim Roy, and Anoop Sarkar.\n2009. Active learning for statistical phrase-based\nmachine translation. In Proceedings of Human\nLanguage Technologies: The 2009 Annual Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics, pages 415–423,\nBoulder, Colorado. Association for Computational\nLinguistics.\nEva Hasler, Phil Blunsom, Philipp Koehn, and Barry\nHaddow. 2014. Dynamic topic adaptation for\nphrase-based MT. In Proceedings of the 14th\nConference of the European Chapter of the Asso-\nciation for Computational Linguistics, pages 328–\n337, Gothenburg, Sweden. Association for Compu-\ntational Linguistics.\nKenneth Heaﬁeld. 2011. KenLM: Faster and smaller\nlanguage model queries. In Proceedings of the\nSixth Workshop on Statistical Machine Translation,\npages 187–197, Edinburgh, Scotland. Association\nfor Computational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. arXiv preprint arXiv:2003.11080.\nJunjie Hu, Mengzhou Xia, Graham Neubig, and Jaime\nCarbonell. 2019. Domain adaptation of neural ma-\nchine translation by lexicon induction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAlon Jacovi, Gang Niu, Yoav Goldberg, and Masashi\nSugiyama. 2019. Scalable evaluation and im-\nprovement of document set expansion via neu-\nral positive-unlabeled learning. arXiv preprint\narXiv:1910.13339.\nMarcin Junczys-Dowmunt. 2018. Dual conditional\ncross-entropy ﬁltering of noisy parallel corpora. In\nProceedings of the Third Conference on Machine\nTranslation: Shared Task Papers, pages 888–895,\nBelgium, Brussels. Association for Computational\nLinguistics.\nYunsu Kim, Miguel Grac ¸a, and Hermann Ney. 2020.\nWhen and why is unsupervised neural machine trans-\nlation useless? arXiv preprint arXiv:2004.10581.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProceedings of the 45th Annual Meeting of the As-\nsociation for Computational Linguistics Companion\nVolume Proceedings of the Demo and Poster Ses-\nsions, pages 177–180, Prague, Czech Republic. As-\nsociation for Computational Linguistics.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceed-\nings of the First Workshop on Neural Machine Trans-\nlation, pages 28–39, Vancouver. Association for\nComputational Linguistics.\nKatherine Lee, Orhan Firat, Ashish Agarwal, Clara\nFannjiang, and David Sussillo. 2018. Hallucinations\nin neural machine translation.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nXiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nalla-\npati, and Bing Xiang. 2019. Domain adaptation\nwith BERT-based domain classiﬁcation and data se-\nlection. In Proceedings of the 2nd Workshop on\nDeep Learning Approaches for Low-Resource NLP\n(DeepLo 2019), pages 76–83, Hong Kong, China.\nAssociation for Computational Linguistics.\nChristopher D Manning, Prabhakar Raghavan, and Hin-\nrich Sch ¨utze. 2008. Introduction to information re-\ntrieval. Cambridge university press.\nKelly Marchisio, Kevin Duh, and Philipp Koehn. 2020.\nWhen does unsupervised machine translation work?\narXiv preprint arXiv:2004.05516.\n7758\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nRobert C. Moore and William Lewis. 2010. Intelligent\nselection of language model training data. In Pro-\nceedings of the ACL 2010 Conference Short Papers,\npages 220–224, Uppsala, Sweden. Association for\nComputational Linguistics.\nMathias M ¨uller, Annette Rios, and Rico Sennrich.\n2019. Domain robustness in neural machine trans-\nlation.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\nMichael Auli, and Sergey Edunov. 2019. Facebook\nFAIR’s WMT19 news translation task submission.\nIn Proceedings of the Fourth Conference on Ma-\nchine Translation (Volume 2: Shared Task Papers,\nDay 1), pages 314–319, Florence, Italy. Association\nfor Computational Linguistics.\nXing Niu, Marianna Martindale, and Marine Carpuat.\n2017. A study of style in machine translation: Con-\ntrolling the formality of machine translation output.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2814–2819, Copenhagen, Denmark. Association for\nComputational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nZuzanna Parcheta, Germ ´an Sanchis-Trilles, and Fran-\ncisco Casacuberta. 2018. Data selection for NMT\nusing infrequent n-gram recovery. EAMT 2018.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research,\n12:2825–2830.\n´Alvaro Peris, Mara Chinea-R ´ıos, and Francisco\nCasacuberta. 2017. Neural networks classiﬁer\nfor data selection in statistical machine translation.\nThe Prague Bulletin of Mathematical Linguistics,\n108(1):283–294.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlberto Poncelas, Gideon Maillette de Buy Wenniger,\nand Andy Way. 2018. Data selection with feature\ndecay algorithms using an approximated target side.\narXiv preprint arXiv:1811.03039.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pages 157–163, Valencia,\nSpain. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. OpenAI blog.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer.\nSebastian Ruder, Matthew E. Peters, Swabha\nSwayamdipta, and Thomas Wolf. 2019. Trans-\nfer learning in natural language processing. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Com-\nputational Linguistics: Tutorials , pages 15–18,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nSebastian Ruder and Barbara Plank. 2017. Learning to\nselect data for transfer learning with Bayesian opti-\nmization. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 372–382, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nHassan Sajjad, Nadir Durrani, Fahim Dalvi, Yonatan\nBelinkov, and Stephan V ogel. 2017. Neural ma-\nchine translation training in a multi-domain scenario.\narXiv preprint arXiv:1708.08712.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nLuc´ıa Santamar´ıa and Amittai Axelrod. 2019. Data\nselection with cluster-based language difference\nmodels and cynical selection. arXiv preprint\narXiv:1904.04900.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\n7759\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nCatarina Cruz Silva, Chao-Hong Liu, Alberto Poncelas,\nand Andy Way. 2018. Extracting in-domain training\ncorpora for neural machine translation using data se-\nlection methods. In Proceedings of the Third Con-\nference on Machine Translation: Research Papers,\npages 224–231, Belgium, Brussels. Association for\nComputational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nJ¨org Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation (LREC-2012), pages 2214–2218, Istan-\nbul, Turkey. European Languages Resources Associ-\nation (ELRA).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. 2019a. Super-\nglue: A stickier benchmark for general-purpose\nlanguage understanding systems. arXiv preprint\narXiv:1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nRui Wang, Andrew Finch, Masao Utiyama, and Ei-\nichiro Sumita. 2017. Sentence embedding for neural\nmachine translation domain adaptation. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 560–566, Vancouver, Canada. Associa-\ntion for Computational Linguistics.\nWei Wang, Isaac Caswell, and Ciprian Chelba. 2019b.\nDynamically composing domain-data selection with\nclean-data selection by “co-curricular learning” for\nneural machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1282–1292, Florence,\nItaly. Association for Computational Linguistics.\nMarlies van der Wees. 2017. What’s in a Domain? To-\nwards Fine-Grained Adaptation for Machine Trans-\nlation. Ph.D. thesis, University of Amsterdam.\nMarlies van der Wees, Arianna Bisazza, Wouter\nWeerkamp, and Christof Monz. 2015. What’s in\na domain? analyzing genre and topic differences\nin statistical machine translation. In Proceedings\nof the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 560–566, Beijing,\nChina. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nShijie Wu, Alexis Conneau, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Emerging\ncross-lingual structure in pretrained language mod-\nels. arXiv preprint arXiv:1911.01464.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. XLNet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nXuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul\nMcNamee, Marine Carpuat, and Kevin Duh. 2019.\nCurriculum learning for domain adaptation in neu-\nral machine translation. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 1903–1915, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\n7760\nA Appendix\nA.1 NMT Training\nFigure 5 details the hyperparameter conﬁguration\nwe used to train the NMT models. We use Trans-\nformer models (Vaswani et al., 2017) in the Base\nconﬁguration using the implementation provided\nin Fairseq (Ott et al., 2019). For all models we\nuse a joint BPE vocabulary (Sennrich et al., 2016)\nlearned with 32k merge operations over the con-\ncatenated corpus in both languages, enabling to tie\nall the embedding layers (Press and Wolf, 2017).12\nWe perform early stopping if the BLEU score on\nthe domain-speciﬁc development set did not im-\nprove in 10 consequent checkpoints. We use the\nADAM (Kingma and Ba, 2014) optimizer with an\ninitial learning rate of 5 · 10−4 and a maximum\nof 4096 tokens per batch. We trained all models\non a single NVIDIA GPU. We decode using beam\nsearch with a beam size of 5. For pre-processing\nwe used the Moses (Koehn et al., 2007) pipeline in-\ncluding tokenization, normalize-punctuation, non-\nprinting character removal, truecasing and cleaning.\nWe removed examples with sequences longer than\n100 tokens from the training data (before subword\nsegmentation).\nA.2 Data Split\nTable 8 shows details about the overlap between the\ntraining, development and test sets for the different\ndata splits of the multi-domain dataset. The overlap\nwas computed using the English part of the corpus.\nA.3 GMM Clustering\nWe learn GMMs with full covariance matrices, i.e.\nwithout constraints on covariance matrices that de-\ntermine the shape of each component in the mix-\nture, as implemented in scikit-learn (Pedregosa\net al., 2011). We train the models until conver-\ngence or for a maximum of 150 EM iterations.\nA.4 Language Model Finetuning\nWe ﬁne-tune the binary classiﬁcation head for 5\nepochs. We use the ADAM (Kingma and Ba, 2014)\noptimizer with an initial learning rate of 2 · 10−5.\nWe train the model using 4 NVIDIA GPUs with\n256 sentences per batch (64 per GPU).\n12We used the implementation in https://github.\ncom/rsennrich/subword-nmt\nCUDA_VISIBLE_DEVICES=0 \\\npython $FAIRSEQ_PATH/train.py ${BINARIZED_DATA_DIR} \\\n--arch transformer_wmt_en_de \\\n--share-all-embeddings \\\n--optimizer adam \\\n--adam-betas ’(0.9, 0.98)’ \\\n--clip-norm 1.0 \\\n--lr 0.0005 \\\n--lr-scheduler inverse_sqrt \\\n--warmup-updates 4000 \\\n--warmup-init-lr 1e-07 \\\n--dropout 0.2 \\\n--weight-decay 0.0 \\\n--criterion label_smoothed_cross_entropy \\\n--label-smoothing 0.1 \\\n--max-tokens 4096 \\\n--update-freq 5 \\\n--attention-dropout 0.2 \\\n--activation-dropout 0.2 \\\n--max-epoch 200 \\\n--seed 17 \\\n-s $src \\\n-t $tgt \\\n--save-dir $MODEL_PATH \\\n--save-interval-updates 10000 \\\n--validate-interval 1\nFigure 5: The hyperparameter conﬁguration we used\nfor NMT model training using Fairseq (Ott et al.,\n2019).\nA.5 Moore-Lewis Implementation\nWe used the implementation of Moore and\nLewis (2010) by Pamela Shapiro, as avail-\nable in: https://github.com/pamelashapiro/\nmoore-lewis. This implementation uses the\nKenLM N-Gram language model toolkit (Heaﬁeld,\n2011).\nA.6 Additional Visualizations\nFigure 6 shows visualizations of the multi-domain\ndataset from additional pre-trained masked lan-\nguage models (BERT large and RoBERTa), and\nFigure 7 shows the same visualization for autore-\ngressive models (XLNet and GPT2).\n7761\nKoehn and Knowles (2017) M¨uller et al. (2019) New Split\n% dev\nin train\nMedical 1090/2000 (54.5%) 1204/2000 (60.2%) 0/2000\nKoran 0/2000 1926/2000 (96.3) 0/2000\nSubtitles 1183/5000 (23.66%) 638/2000 (31.9%) 0/2000\nLaw 595/2000 (29.75%) 1000/2000 (50%) 0/2000\nIT 2496/2526 (98.81%) 783/2000 (39.15%) 0/2000\n% test\nin train\nMedical 571/2000 (28.55%) 516/1691 (30.51%) 0/2000\nKoran 0/2000 1949/2000 (97.45%) 0/2000\nSubtitles 451/5000 (9.02%) 478/2000 (23.9%) 0/2000\nLaw 649/2000 (32.45%) 966/2000 (48.3%) 0/2000\nIT 945/1856 (50.92%) 1036/2000 (51.8%) 0/2000\nTable 8: Details about the different data splits for the multi-domain corpus.\n7762\nit\nkoran\nsubtitles\nmedical\nlaw\nbert-large-cased\nit\nkoran\nsubtitles\nmedical\nlaw\nroberta-large\nFigure 6: 2D visualizations of the unsupervised GMM-based clustering for different pretrained MLMs.\n7763\nit\nkoran\nsubtitles\nmedical\nlaw\nxlnet-base-cased\nit\nkoran\nsubtitles\nmedical\nlaw\ngpt2\nFigure 7: 2D visualizations of the unsupervised GMM-based clustering for different pretrained auto-regressive\nLMs.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8282796144485474
    },
    {
      "name": "Formality",
      "score": 0.7241030931472778
    },
    {
      "name": "Natural language processing",
      "score": 0.7200538516044617
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6906908750534058
    },
    {
      "name": "Sentence",
      "score": 0.6696486473083496
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.6526170372962952
    },
    {
      "name": "Oracle",
      "score": 0.6270442008972168
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6016929745674133
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5453353524208069
    },
    {
      "name": "Machine translation",
      "score": 0.5205612778663635
    },
    {
      "name": "Language model",
      "score": 0.5034322142601013
    },
    {
      "name": "Data set",
      "score": 0.41557711362838745
    },
    {
      "name": "Linguistics",
      "score": 0.18198388814926147
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Software engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I13955877",
      "name": "Bar-Ilan University",
      "country": "IL"
    }
  ],
  "cited_by": 13
}