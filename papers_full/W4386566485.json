{
    "title": "Selective-LAMA: Selective Prediction for Confidence-Aware Evaluation of Language Models",
    "url": "https://openalex.org/W4386566485",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5056006362",
            "name": "Hiyori Yoshikawa",
            "affiliations": [
                "Fujitsu (Japan)",
                "Tokyo Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5066940046",
            "name": "Naoaki Okazaki",
            "affiliations": [
                "Tokyo Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2962735233",
        "https://openalex.org/W3099142828",
        "https://openalex.org/W2971044268",
        "https://openalex.org/W3034775979",
        "https://openalex.org/W582134693",
        "https://openalex.org/W2171278097",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3199958362",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W2988249555",
        "https://openalex.org/W3169920290",
        "https://openalex.org/W3211777899",
        "https://openalex.org/W2618169590",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2101946573",
        "https://openalex.org/W2946930197",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2768957049",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2950339735",
        "https://openalex.org/W3104939451",
        "https://openalex.org/W3173673636",
        "https://openalex.org/W2250770256",
        "https://openalex.org/W3118017018",
        "https://openalex.org/W3035807844",
        "https://openalex.org/W3035441651",
        "https://openalex.org/W3173699110"
    ],
    "abstract": "Recent studies have suggested that neural language models learn and store a large amount of facts and commonsense knowledge from training data. The ability of language models to restore such knowledge is often evaluated via zero-shot cloze-style QA tasks. However, such evaluations rely only on prediction accuracy without punishing the systems for their mistakes, e.g., simply guessing or hallucinating likely answers. Selective prediction is a more informative evaluation framework that takes the confidence of predictions into account. Under the selective prediction setting, a model is evaluated not only by the number of correct predictions, but also by the ability to filter out dubious predictions by estimating the confidence of individual predictions. Such confidence-aware evaluation is crucial for determining whether to trust zero-shot predictions of language models. In this paper, we apply the selective prediction setting to an existing benchmark, LAMA probe, and conduct extensive experiments with recent neural language models and different confidence functions. We empirically show that our Selective-LAMA evaluation is more robust to the effect of simple guesses than the conventional accuracy-based evaluation.Our evaluation reveals the importance of the choice of confidence functions by showing that simply relying on token probabilities is not always the best choice.Further analysis shows that various confidence functions exhibit different preferences over predicted tokens for a given context.",
    "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 2017‚Äì2028\nMay 2-6, 2023 ¬©2023 Association for Computational Linguistics\nSelective-LAMA: Selective Prediction for Confidence-Aware Evaluation of\nLanguage Models\nHiyori Yoshikawa‚Ä†‚Ä° Naoaki Okazaki‚Ä†\n‚Ä†Tokyo Institute of Technology, Japan ‚Ä°Fujitsu Limited, Japan\n{hiyori.yoshikawa@nlp., okazaki@}c.titech.ac.jp\nAbstract\nRecent studies have suggested that neural lan-\nguage models learn and store a large amount of\nfacts and commonsense knowledge from train-\ning data. The ability of language models to\nrestore such knowledge is often evaluated via\nzero-shot cloze-style QA tasks. However, such\nevaluations rely only on prediction accuracy\nwithout punishing the systems for their mis-\ntakes, e.g., simply guessing or hallucinating\nlikely answers. Selective prediction is a more\ninformative evaluation framework that takes the\nconfidence of predictions into account. Under\nthe selective prediction setting, a model is eval-\nuated not only by the number of correct predic-\ntions, but also by the ability to filter out dubious\npredictions by estimating the confidence of in-\ndividual predictions. Such confidence-aware\nevaluation is crucial for determining whether to\ntrust zero-shot predictions of language models.\nIn this paper, we apply the selective predic-\ntion setting to an existing benchmark, LAMA\nprobe, and conduct extensive experiments with\nrecent neural language models and different\nconfidence functions. We empirically show\nthat our Selective-LAMA evaluation is more\nrobust to the effect of simple guesses than the\nconventional accuracy-based evaluation. Our\nevaluation reveals the importance of the choice\nof confidence functions by showing that sim-\nply relying on token probabilities is not always\nthe best choice. Further analysis shows that\nvarious confidence functions exhibit different\npreferences over predicted tokens for a given\ncontext.\n1 Introduction\nRecently, knowledge stored in pre-trained lan-\nguage models has been intensively investigated.\nMany studies have suggested that language mod-\nels trained on a large amount of textual corpora,\nsuch as BERT (Devlin et al., 2019) and GPT (Rad-\nford et al., 2019; Brown et al., 2020), store both\nlinguistic knowledge (Warstadt et al., 2019; Mi-\naschi et al., 2020) and factual and commonsense\nknowledge (Bosselut et al., 2019; Roberts et al.,\n2020) during training. However, this knowledge\nis embedded in the parameters of these language\nmodels and thus is difficult to interpret, in contrast\nto symbolic knowledge bases, which allows us to\ninspect and edit stored facts explicitly.\nPetroni et al. (2019) proposed a benchmark\ntask, the LAMA probe, that aims at evaluating the\namount of relational knowledge, such as common-\nsense knowledge and facts, which is stored in a\nlanguage model. In LAMA probe, a relational fact\nis converted into a cloze statement (query) and then\ngiven to a language model as a fill-in-the-blank\nquestion. If the language model fills in the blank\nwith the correct answer, the model is considered\nto possess ‚Äúknowledge‚Äù of the relation. According\nto Petroni et al.‚Äôs experiments, the BERT language\nmodel (Devlin et al., 2019) has a comparable perfor-\nmance to a supervised relation extraction baseline,\nwith precision ranging from 10.5 to 32.3 depending\non the dataset type.\nHowever, in many applications, we are con-\ncerned not only with the amount of the knowledge\nextracted from a language model, but also with\nits reliability. This is because large pre-trained\nlanguage models are known to fluently generate\n‚Äúfacts‚Äù that they have never seen (Cao et al., 2018;\nRohrbach et al., 2018; M√ºller et al., 2020). There-\nfore, it is crucial to know when we can trust the\noutput of a language model. The LAMA probe\nframework does not cover this issue, as it always\nforces the model to output an answer for all in-\nstances, regardless of whether the model really\n‚Äúknows‚Äù the answer to a query. This means that\nit implicitly trusts all outputs of a language model\nto the same degree.\nFigure 1 shows an example suggesting that a\npre-trained language model is not always using\nits knowledge for prediction. The figure shows\nthe distribution of predicted tokens for a particular\nrelation in the original LAMA probe benchmark\n2017\n(place-of-birth). We can see that three tokens\naccount for more than half of the wrong predictions.\nThis indicates that the model has a bias which it\nacquired during training, probably due to the input\ntemplate used, rather than using actual question-\nspecific knowledge about individual facts.\nTo address this issue, we apply the selective pre-\ndiction (El-Yaniv and Wiener, 2010; Geifman and\nEl-Yaniv, 2017) setting to the LAMA probe and\npropose a new evaluation framework, Selective-\nLAMA, to evaluate both the amount of knowledge\nin a pre-trained language model and the model‚Äôs\nability to estimate the reliability of its prediction.\nSelective prediction is a framework by which a sys-\ntem can choose whether to output the individual\npredictions of a model based on the prediction re-\nsults. Specifically, we consider the selection with\nguaranteed risk control setting (Geifman and El-\nYaniv, 2017), where the system computes confi-\ndence scores of individual predictions to determine\nwhether it outputs the predictions. A system is\nevaluated by the number of predictions it can make\nwhile maintaining a risk of error below a certain\nlevel. To achieve high performance, a system is\nrequired not only to answer many questions cor-\nrectly, but also to accurately estimate the model‚Äôs\nconfidence about individual facts and determine\nwhen the system should not answer a question.\nIn this paper, we focus on masked language mod-\nels and address the following research questions:\n(1) whether the pre-trained language model has the\nability to estimate the confidence of individual pre-\ndictions and (2) how various confidence metrics\naffect the ability of a system to do that. With our\nproposed Selective-LAMA framework, we exam-\nine several basic confidence functions that can be\ncomputed using only language model predictions\nand do not require additional datasets or external\nknowledge sources. We empirically verify that\nthe selective prediction evaluation is less likely to\noverestimate predictions with template-related bi-\nases than the conventional accuracy-based evalua-\ntion. The results of the experiments suggest that the\nchoice of confidence functions also influences the\nresults, showing that simply using token probability\nis a strong baseline but not always the best choice,\nand that the optimal confidence function depends\non both the model and the dataset. We hope that the\nselective prediction framework facilitates an under-\nexplored research direction of utilizing predictions\nof language models in a more reliable way.\nDataset: Google-RE, Model: BERT-base\nRelation: place-of-birth\nInput:‚ÄúX (Subject) was born in [MASK] .‚Äù\nCorrect(total: 439)\nWrong(total: 2,498)\nPredicted tokens\nFigure 1: Composition of predicted tokens in each of the\ncorrect (top) and wrong (bottom) predictions by BERT-\nbase for theplace-of-birth relation in the Google-RE\ndataset (size: 2,937). Just three tokens account for more\nthan half of the wrong predictions, implying that the\nmodel has a template-dependent bias.\n2 Selective Prediction\nUnder the selective prediction setting (El-Yaniv\nand Wiener, 2010; Geifman and El-Yaniv, 2017),\na selective classifier determines whether a system\nshould output the prediction of the model. We\nconsider a classification problem from an input\nspace Xto a set of labels Y. A selective classifier\n(f,g) consists of an original classification model\nf : X ‚ÜíY and a selection function g : X ‚Üí\n{0,1}. Given an input example x‚ààX, a selection\nfunction determines whether the system outputs the\nprediction f(x) ‚ààY:\n(f,g)(x) :=\n{\nf(x) if g(x) = 1\ndon‚Äôt know if g(x) = 0 . (1)\nGeifman and El-Yaniv (2017) introduced the se-\nlection with guaranteed risk (SGR) setting, which\nuses a confidence-based selection function:\ng(x) =\n{\n1 if œï(x) ‚â•Œ≤\n0 if œï(x) <Œ≤ , (2)\n2018\nwhere œï(x) : X‚Üí R is the confidence score func-\ntion of f. The system outputs the prediction if\nthe confidence score exceeds the threshold Œ≤ ‚ààR.\nThis setting allows a user to adjust the error risk\ngenerated by the system by appropriately setting\nthe value of Œ≤. Specifically, increasing Œ≤decreases\nthe number of cases predicted by the system while\nreducing the risk of making a wrong prediction.\nUnder the SGR setting, there is a risk-coverage\ntrade-off between the risk (Npred ‚àíNcorr)/Npred\nthat a selective classifier will make a wrong predic-\ntion and the coverage Npred/Nof the predictions\nmade by the system. Here, N,Npred, and Ncorr\ndenote the number of all examples, predicted exam-\nples, and correct predictions, respectively. The per-\nformance of a selective classifier is evaluated based\non the AUC of the risk-coverage curve (RC-AUC)\nobtained by changing Œ≤ in the selection function\n(2). A smaller RC-AUC value indicates a lower\nrisk of making a wrong prediction. In practice, the\nthreshold will be determined by the level of risk\nacceptable to the users.\n3 Selective-LAMA\n3.1 LAMA Probe and Model Prediction\nIn the original LAMA probe, a relational fact is con-\nverted into a natural sentence using templates and\ninput into the language model. For example, when\nquerying about an entity that has a relationship of\nborn-in with ‚ÄúDante,‚Äù the input to the language\nmodel will be ‚ÄúDante was born in[MASK].,‚Äù where\n[MASK] is a special token that represents the mask\ntoken. The model output for the masked position is\nconsidered the answer to the query.1 The templates\nare manually designed for each relation type.\nFollowing the original study (Petroni et al.,\n2019), we focus on bi-directional language mod-\nels. Given the input sentence with a mask\ntoken at the t-th position x = W\\t :=\n(w1,...,w t‚àí1,[MASK],wt+1,...,w |W|), the lan-\nguage model predicts the probability distribution of\nthe t-th token PLM(wt|W\\t). The model prediction\nis the token w‚Ä≤with the highest probability:\nf(x) = w‚Ä≤:= arg max\nwt\nPLM(wt|W\\t). (3)\nWe denote the sentence in which the masked posi-\ntion is filled with w‚Ä≤by W‚Ä≤.\n1For simplicity, the target is limited to entities comprising\na single word.\n3.2 Confidence Functions\nAs the task is to evaluate the knowledge present in\na pre-trained language model, we select confidence\nfunctions that use only the prediction of the lan-\nguage model and do not require additional training\nor external knowledge sources. The following is\nthe list of confidence functions that we investigate.\nToken (T) The simplest confidence function is to\nuse the log probability of the predicted token w‚Ä≤\n(3) directly:\nœïT(x) = log PLM(w‚Ä≤|W\\t). (4)\nSent (S) Sentence-level likelihood is widely\nused in the context of sentence acceptability and\nfact-checking (Lau et al., 2020; Lee et al., 2021).\nThis reflects how natural the entire sentence is\nwhen the predicted token is substituted into the\nmask position. Here, we adopt the pseudo-log like-\nlihood (Salazar et al., 2020) for masked language\nmodels normalized by sentence length:\nœïS(x) = 1\n|W‚Ä≤|\n|W‚Ä≤|‚àë\nu=1\nlog PLM(wu|W‚Ä≤\n\\u). (5)\nGap (G) Let w‚Ä≤‚Ä≤ be the token with the second-\nlargest probability by the model. The confidence\nscore is then calculated as follows:\nœïG(x) = log PLM(w‚Ä≤|W\\t) ‚àílog PLM(w‚Ä≤‚Ä≤|W\\t). (6)\nThis function is based on the assumption that a\nmodel makes a confident prediction when the prob-\nability of the predicted token is significantly larger\nthan that of other tokens.\nReranking (R) The following function is based\non the assumption that, if the confidence of the\nprediction is high, the score for the prediction is\nconsistently higher than those of other candidates\neven when different metrics are used. First, we\nobtain top-Kpredictions Wbased on the token log\nprobability (3). Then, we re-rank those candidates\nusing another score function œà. Let rankœà(w‚Ä≤) be\nthe rank of w‚Ä≤after the reranking. The confidence\nscore is subsequently computed as follows:\nœïR(x) = log2\nK\nrankœà(w‚Ä≤) = log2 K‚àílog2 rankœà(w‚Ä≤).\n(7)\nThe above score function is essentially a measure\nbased only on the new rank after the reranking and\nhas been used to assess the risk of language models\nto memorize privacy information (Carlini et al.,\n2019). In the experiments, we apply K = 100 and\nuse the Sent score œïS(x) for œà.\n2019\nDropoutMean (DM) Dropout-based metrics have\nbeen widely used to estimate uncertainty of deep\nneural network models (Gal and Ghahramani,\n2016). The basic concept is to use dropout to\nsample slightly different model parameters that\nyield different predictions and to use stochastic\ninformation to estimate the model uncertainty.\nFollowing (Kamath et al., 2020), we adopt two\ndropout-based measures. We apply M differ-\nent dropout masks to the language model‚Äôs lay-\ners and obtain different probability distributions.\nLet P(m)\nLM (w‚Ä≤|W\\t) denote the m-th output ( m ‚àà\n{1,...,M }). DropoutMean takes the mean of the\nM outputs:\nœïDM(x) = 1\nM\nM‚àë\nm=1\nP(m)\nLM (w‚Ä≤|W\\t), (8)\nwhich can be considered an ensemble of the M\nmodel predictions.\nDropoutVar (DV) Similarly, DropoutVar uti-\nlizes the variance of the outputs. As large vari-\nance implies high model uncertainty, we take the\nnegative variance of the outputs:\nœïDV(x) = ‚àí1\nM\nM‚àë\nm=1\n(P(m)\nLM (w‚Ä≤|W\\t) ‚àíœïDM(x))2. (9)\nIn our experiments, we apply M = 30 differ-\nent dropout masks for each input, using the same\ndropout ratios as those used to train the models.\nTemplateDiff (TD) A large portion of the\nLAMA probe benchmark consists of instances\nbased on subject-relation-object triples. These in-\nstances share relation-specific templates, such as\n‚Äú<subj> was born in [MASK].‚Äù, where the subject\nof each triple is substituted for <subj>. Cao et al.\n(2021) found that predictions of language models\nare highly biased by templates and the impact of\nsubject entities are limited. Inspired by this ob-\nservation, we define a confidence measure that as-\nsesses the impact of subject entities to predictions.\nLet Wtemp be a template-only input sentence where\nthe subject of the inputW\\t is replaced by the mask\ntoken, e.g. ‚Äú[MASK] was born in [MASK].‚Äù Then,\nwe calculate the confidence by comparing the log\nprobabilities of the prediction with and without the\nsubject entity mention:\nœïTD(x) = PLM(w‚Ä≤|W\\t) ‚àíPLM(w‚Ä≤|Wtemp).\n(10)\n4 Experiments\nThe proposed Selective-LAMA framework allows\nus to evaluate the ability of language models to\nrecognize questions for which they do not know\nthe answer. To see how the proposed framework\naffects the evaluation of language models, in Sec-\ntion 4.2, we first compare the evaluation based\non the Selective-LAMA framework with the con-\nventional accuracy-based evaluation, focusing on\nthe sensitivity to biased predictions. Then, in Sec-\ntion 4.3, we present a comprehensive study of the\nperformance of three masked language models on\ndifferent datasets using the confidence functions\nintroduced in Section 3.2.\n4.1 Experimental Settings\nWe used the same data set as the original LAMA\nbenchmark for our experiment and evaluated it\nwith our proposed Selective-LAMA framework.\nThe benchmark consists of four datasets: Google-\nRE, T-REx, ConceptNet, and SQuAD. The Google-\nRE and T-REx datasets contain relational facts ex-\ntracted from Wikipedia. The ConceptNet dataset\ncontains relational knowledge about commonsense\nextracted from the ConceptNet dataset (Speer and\nHavasi, 2012). The SQuAD dataset (Rajpurkar\net al., 2016) is based on a question answering\ndataset of the same name, but the questions are\nrewritten in cloze style. As all these datasets, ex-\ncept for ConceptNet, use Wikipedia as the knowl-\nedge source, evidence for the correct answer should\nbe found in Wikipedia. For language models, we\nuse BERT-base (110 M parameters), BERT-large\n(340 M parameters), and RoBERTa-base (Liu et al.,\n2019). Because these models are trained using\nWikipedia, it is expected that the models have seen\nthe correct answers for the queries during training.\n4.2 Template Bias Robustness\nIn the selective prediction framework, the perfor-\nmance of language models is evaluated by RC-\nAUC (Section 2), while the original LAMA bench-\nmark uses the accuracy of the top-1 predictions as\nthe evaluation metric. A disadvantage of accuracy-\nbased evaluation is that the amount of knowledge\nof a language model can be overestimated by count-\ning lucky guesses. Such lucky guesses can affect\nthe evaluation results, especially in cases where the\nmodel‚Äôs predictions are biased by relation-specific\ntemplates (Figure 1).\nWe investigate how these evaluation metrics are\n2020\nBERT-base BERT-large RoBERTa-base\nCovA CovP CovA CovP CovA CovP\nAccuracy 0.387 -0.247 0.469 -0.244 0.512 -0.224\nRC-AUC Token 0.344 ‚Üì -0.316 ‚Üì 0.438 ‚Üì -0.292 ‚Üì 0.484 ‚Üì -0.277 ‚Üì\n(negative) Sent 0.355 ‚Üì -0.290 ‚Üì 0.441 ‚Üì -0.285 ‚Üì 0.499 ‚Üì -0.249 ‚Üì\nGap 0.351 ‚Üì -0.314 ‚Üì 0.430 ‚Üì -0.294 ‚Üì 0.474 ‚Üì -0.285 ‚Üì\nReranking 0.350 ‚Üì -0.286 ‚Üì 0.452 ‚Üì -0.283 ‚Üì 0.498 ‚Üì -0.266 ‚Üì\nDropoutMean 0.338 ‚Üì -0.319 ‚Üì 0.433 ‚Üì -0.293 ‚Üì 0.486 ‚Üì -0.280 ‚Üì\nDropoutVar 0.419 ‚Üë -0.125 ‚Üë 0.470 ‚Üë -0.124 ‚Üë 0.456 ‚Üì -0.166 ‚Üë\nTemplateDiff 0.349 ‚Üì -0.317 ‚Üì 0.427 ‚Üì -0.299 ‚Üì 0.486 ‚Üì -0.271 ‚Üì\nTable 1: Correlation between evaluation metrics and template bias metrics: answer coverage (CovA) and prediction\ncoverage (CovP) on the T-REx dataset. Here, we use the sign-reversed RC-AUC values for easier interpretation.\naffected by template-related biases using the T-REx\nsubset of the LAMA benchmark, which contains\n34k facts about 41 different relations with their cor-\nresponding templates. To quantify template-related\nbiases, we introduce two indicators: prediction cov-\nerage and answer coverage.\nPrediction coverage quantifies biases in model\npredictions for a given template. If a model often\npredicts the same answers for a template, it is likely\nthat the predictions are heavily influenced by the\ntemplate, rather than using knowledge of individ-\nual subject entities. Let Dr = ( {(si,oi)}Nr\ni=1,tr)\ndenote a relation subset containing Nr fact triples\n(si,r,oi) of relation rand a template tr. We rep-\nresent the input sentence corresponding to the i-th\nfact by tr(si). For each relation subset Dr, we\nfirst identify five most frequent tokens Wfreq(r)\npredicted by a model. Prediction coverage is the\nproportion of predicted tokens covered by these\ntokens:\nCovP(r) = |{i|f(tr(si)) ‚ààWfreq(r)}|\nNr\n. (11)\nAnswer coverage quantifies biases in a relation\nsubset in the test set. If the distribution of the cor-\nrect answers for a relation subset is skewed towards\na few particular entities, the subset can be easily\nanswered by exploiting the bias without using the\nknowledge of individual subject entities. Answer\ncoverage is calculated as the proportion of gold an-\nswers covered by the frequently predicted tokens:\nCovA(r) = |{i|oi ‚ààWfreq(r)}|\nNr\n. (12)\nTable 1 shows the correlation between the bias\nindicators and the evaluation metrics including ac-\ncuracy and (negative) RC-AUC calculated with\ndifferent confidence functions. Compared to the\nconventional accuracy metric, all RC-AUC met-\nrics except DropoutVar show a weaker positive\ncorrelation with answer coverage and a stronger\nnegative correlation with prediction coverage, in-\ndicating that the RC-AUC metrics are less likely\nto overestimate template-biased predictions and re-\nsults from intrinsically biased test examples.\nFigure 2 shows the output of the BERT-base\nmodel for two relation subsets P36 and P1412. Al-\nthough the accuracy scores for both subsets are\naround 0.6, for P1412, both the prediction and\nanswer distributions are biased towards a small\nnumber of entities, leading to high prediction and\nanswer coverage. The Token confidence scoring\nfails to discriminate between correct and incorrect\npredictions in this subset, resulting in high risk\nat a low coverage point. Evaluation based on the\nRC-AUC score successfully captures the difference\nbetween these two cases and avoids overestimating\nthe results from biased predictions.\n4.3 Selective-LAMA Evaluation and Analysis\nOverall performance on different datasets\nTable 2 shows the RC-AUC scores achieved by dif-\nferent confidence functions on various datasets. We\nalso calculate the performance lower bound based\non an oracle confidence function that gives 1 to all\ncorrect predictions and 0 to incorrect ones. While\nthe simple Token metric constantly performs well,\nthe best confidence function depends on the model\nand dataset. Notably, Gap and TemplateDiff per-\nform better on the datasets of Wikipedia fact triples,\nGoogle-RE and T-REx, than on ConceptNet and\nSQuAD, outperforming the Token metric in some\ncases. The breakdown of the results on the T-REx\ndataset indicates that the performance of confi-\ndence functions also depend on relation templates.\nWe further investigate this phenomenon below.\n2021\nSubject\nGold\nPredict\nùúôùëá\nAdrianus\nValerius\nDutch\nLatin\n-\n0.490\nMuhammad Ali\nEnglish\nArabic\n-\n0.575\nGloria Estefan\nSpanish\nSpanish\n-\n0.587\nImre\nNagy\nHungarian\nHungarian\n-\n0.610\nSextus\nPompeius Festus\nLatin\nLatin\n-\n0.619\nHieronymus\nFabricius\nLatin\nLatin\n-\n0.635\nInfante Juan, Count of Barcelona\nSpanish\nSpanish\n-\n0.637\nRamon\nLlull\nCatalan\nSpanish\n-\n0.665\nLau Kar\n-leung\nChinese\nCantonese\n-\n0.724\nJuan Bautista Villalpando\nSpanish\nSpanish\n-\n0.749\nr=P1412 (‚ÄúX (Subject) used to communicate in [MASK] .‚Äù)Accuracy = 0.650, RC -AUC = 0.278\nùí≤freq ùëü: English (38.6%), French (15.9%), Spanish (10.0%), Italian (9.4%), Russian (4.5%)\nPrediction coverage: 0.784, Answer coverage: 0.687\nr=P36 (‚ÄúThe capital ofX (Subject) is [MASK] .‚Äù)Accuracy = 0.621, RC -AUC = 0.121\nùí≤freq ùëü: Rome (1.9%), Baghdad (1.7%), Paris (1.7%), Bangor (1.7%), Kabul (1.4%)\nPrediction coverage: 0.084, Answer coverage: 0.047\nCoverage\nRisk\nRisk\nCoverage\nSubject\nGold\nPredict\nùúôùëá\nSri Lanka\nColombo\nColombo\n-\n0.001\nBratislava Region\nBratislava\nBratislava\n-\n0.001\nAlbania\nTirana\nTirana\n-\n0.002\nTirana District\nTirana\nTirana\n-\n0.002\nHiroshima Prefecture\nHiroshima\nHiroshima\n-\n0.002\nBrest Region\nBrest\nBrest\n-\n0.003\nSouth Korea\nSeoul\nSeoul\n-\n0.003\nAfghanistan\nKabul\nKabul\n-\n0.003\nBosnia and Herzegovina\nSarajevo\nSarajevo\n-\n0.003\nDemocratic Republic of Afghanistan\nKabul\nKabul\n-\n0.003\nFigure 2: BERT-base results for relation subsets r= P36 and r= P1412. While the model performance is similar\nin terms of accuracy, the RC-AUC scores exhibit a large difference. Left: Risk-coverage curves ofToken and the\nOracle confidence scores. Right: Top 20 predictions sorted by the Token confidence score œïT. The gray-shaded\nrows indicate incorrect predictions. Many incorrect predictions for P1412 indicate that the model suffers from high\nrisk even at a low coverage point.\nWhen does a confidence function beat another?\nFor the T-REx dataset in Table 2, Gap and\nTemplateDiff outperform the Token metric for\nBERT-base and RoBERTa-base, respectively. We\nchoose these two cases and perform a pairwise\ncomparison for each relation type to identify the\nproperties that determine the preference for one\nconfidence function over the other. The results in\nTable 3 show that Gap is preferred over Token for\neasier relations with high accuracy and low RC-\nAUC for BERT-base, whereas TemplateDiff is\npreferred over Token for more difficult relations\nfor RoBERTa-base. The subset where Gap is pre-\nferred over Token also shows lower prediction cov-\nerage, which might be because the Gap function is\nnot good at handling overconfident predictions by\ndefinition.\nConfidence functions and relation templates\nTo understand whether and how different confi-\ndence functions prioritize one relation over another,\nwe visualize in Figure 3 the composition of the\nrelation types of input examples sorted by the con-\n0\n20\n40\n60\n80\n100\nToken\n0\n20\n40\n60\n80\n100\nSent\n0\n20\n40\n60\n80\n100\nGap\n0\n20\n40\n60\n80\n100\nReranking\n0\n20\n40\n60\n80\n100\nDropoutMean\n0\n20\n40\n60\n80\n100\nDropoutVar\n0\n20\n40\n60\n80\n100\nTemplateDiff\ndate_of_birth\nplace_of_birth\nplace_of_death\nFigure 3: Breakdown of relation types of BERT-base\npredictions on Google-RE, sorted by confidence scores\n(left is the largest).\nfidence scores in the Google-RE dataset predicted\nby the BERT-base model. The Google-RE dataset\ncontains three relation types, date-of-birth,\nplace-of-birth, and place-of-death. Evi-\ndently, the BERT-base language model tend to pro-\n2022\nModel Conf. Google-RE T-REx ConceptNet SQuAD All\n1-1 N-1 N-M All\nToken .775 .118 .434 .611 .478 .686 .755 .545\nSent .834 .163 .549 .776 .594 .797 .815 .652\nGap .798 .133 .422 .604 .470 .714 .794 .548\nBERT-base Reranking .835 .248 .580 .623 .597 .834 .798 .633\nDropoutMean .775 .123 .425 .609 .473 .690 .762 .543\nDropoutVar .962 .525 .834 .883 .850 .918 .912 .886\nTemplateDiff .778 .119 .427 .603 .472 .782 - -\nOracle .663 .070 .301 .456 .344 .551 .583 .413\nToken .763 .085 .409 .575 .445 .616 .669 .506\nSent .815 .119 .520 .740 .560 .738 .768 .614\nGap .801 .092 .412 .597 .456 .650 .712 .525\nBERT-large Reranking .826 .170 .552 .610 .576 .792 .785 .609\nDropoutMean .762 .086 .402 .572 .441 .616 .670 .504\nDropoutVar .960 .370 .775 .894 .817 .881 .907 .858\nTemplateDiff .763 .084 .406 .574 .444 .730 - -\nOracle .648 .048 .277 .459 .327 .489 .522 .388\nToken .818 .191 .540 .635 .562 .618 .741 .599\nSent .876 .267 .631 .761 .657 .754 .780 .716\nGap .827 .197 .545 .632 .565 .657 .782 .610\nRoBERTa-base Reranking .865 .276 .637 .627 .636 .804 .828 .669\nDropoutMean .815 .201 .536 .633 .562 .615 .744 .599\nDropoutVar .979 .643 .924 .920 .920 .896 .907 .923\nTemplateDiff .813 .189 .537 .626 .558 .744 - -\nOracle .730 .106 .416 .492 .432 .503 .571 .474\nTable 2: RC-AUC calculated on each dataset (lower is better). For T-REx, the results on three splits divided\nby the property of the relations are also provided: one-to-one relations ( 1-1), many-to-one relations ( N-1) and\nmany-to-many relations (N-M). ‚ÄúOracle‚Äù represents the best possible performance that could be achieved by an\noracle confidence function that gives 1 to all correct predictions and 0 to incorrect ones. TemplateDiff cannot be\ncalculated for SQuAD as the instances do not contain subject entities.\nduce high probability outputs for a certain rela-\ntion type, namely, place-of-birth. While Gap,\nDropoutMean, and TemplateDiff follow the same\ntrend as that of Token, Sent and Reranking are\nless sensitive to relation types. DropoutVar shows\nthe opposite trend. While the Token metric is ef-\nfective in many cases, one should be aware of the\npotential bias this confidence function may intro-\nduce.\nTable 4 compares the most frequent predic-\ntions of BERT-base on the Google-RE dataset\nranked top by two different metrics: Token and\nReranking. We can observe similar distributions\nfor the date-of-birth relation type. This indi-\ncates that the model is strongly biased toward a\nlimited vocabulary for this particular relation type.\nFor the other two relation types, the frequent words\nin the top predictions are clearly different between\nToken and Reranking. However, while the over-\nlap of the top-ranked predictions between them\nare small, both results have strong preference to-\nward a few particular tokens for each relation type.\nFor place-of-birth, five tokens account for more\nthan 50% of the top-ranked predictions for both\nToken and Reranking. In place-of-death, just\none token occupies around 40% of the top predic-\ntions. The results indicate that these confidence\nfunctions produce different template biases rather\nthan that one is more robust to template biases than\nthe other.\nUsing confidence functions for prediction\nIn the experiments above, model predictions are\nalways determined by the token log probability as\nin (3). However, some of the confidence functions\nintroduced in Section 3.2 can also be used directly\nto determine the prediction as an alternative to (3).\nTherefore, we investigate whether effective con-\nfidence functions are also effective in improving\nprediction accuracy (P @ 1) when used directly for\ntoken prediction. For Gap, we extend the original\ndefinition (6) so that we can apply the function to\ntoken candidates that are not ranked first in terms\nof token probability. Let w(k) denote the k-th best\nprediction based on the model‚Äôs predicted token\nprobability. Then, the extended Gap function is\n2023\nBERT-base\nAll Token-win Gap-win ‚àÜ\nAccuracy 0.311 0.283 0.413 -0.130\nRC-AUC Token 0.558 0.577 0.466 0.111\nRC-AUC Gap 0.566 0.597 0.443 0.154\nAnswer Cov. 0.285 0.276 0.334 -0.058\nPrediction Cov. 0.547 0.579 0.464 0.115\nRoBERTa-base\nAll Token-win TD-win ‚àÜ\nAccuracy 0.242 0.315 0.231 0.085\nRC-AUC Token 0.643 0.545 0.657 -0.112\nRC-AUC TD 0.638 0.546 0.650 -0.103\nAnswer Cov. 0.237 0.285 0.235 0.050\nPrediction Cov. 0.562 0.586 0.541 0.045\nTable 3: Comparison of two confidence functions on the\nT-REx dataset (Token‚ÄìGap for BERT-base andToken‚Äì\nTemplateDiff (TD) for RoBERTa-base). The average\nvalue of each metric is displayed for the entire T-REx\ndataset (All) and the subset for which the confidence\nfunction X outperforms the other (X-win). ‚àÜ stands for\nthe difference between the two subsets.\ndefined as follows:\nœïG(x) = 1\nk(log PLM(w(k)|W\\t)‚àílog PLM(w(k+1)|W\\t)).\n(13)\nThe Gap score for the lowest ranked prediction is\ndefined as zero. The computation of the Sent score\nrequires O(|W‚Ä≤|¬∑V) forward computations for each\ninstance, where V is the vocabulary size. To save\ncomputational cost, we approximate the prediction\nresults by limiting the token candidates to the top\n100 results based on the Token score (3).\nTable 5 shows the results. For all models,\nthe best performance on all data is achieved by\nDropoutMean. However, all functions, except\nfor DropoutVar, show a quite competitive perfor-\nmance in terms of precision. Unlike for confidence\nestimation, no advantage is observed for Gap and\nTemplateDiff on the T-REx dataset. Overall, the\nperformance of these confidence functions is flat\nwhen they are used directly for token prediction.\nFurthermore, there is no strong correlation between\nthe performance of each confidence function as a\npredictor and a confidence estimator. The results\nsuggest that effective metrics for inference and con-\nfidence estimation should be designed based on\ndifferent strategies.\n5 Related Work\nIn NLP, the reliability of the model responses has\nbeen discussed mainly in the field of question an-\nswering. Estimating the confidence of an answer\nis critical in quiz competitions, such as Jeopardy,\nsince the system has to decide when to answer the\nquestions (Ferrucci et al., 2010). Kamath et al.\n(2020) recently introduced a selective prediction\nsetting to question answering tasks and then eval-\nuated the performance of the models on out-of-\ndomain questions. Jiang et al. (2021) addressed\na similar problem, but focused on a calibration of\nthe model prediction on QA tasks. While they fo-\ncused on extractive or multiple-choice QA tasks\nwhere a limited number of candidate answers are\navailable, our focus is on the knowledge probing\nof language models where the candidate answer is\nthe entire vocabulary and, thus, false positives are\nmore frequent.\nSeveral studies have addressed the reliability is-\nsue of pre-trained language models as a calibration\nproblem; the goal of these studies is to train a well-\ncalibrated language model that makes accurate con-\nfidence estimation. Desai and Durrett (2020) inves-\ntigate the calibration level of pre-trained language\nmodels, focusing on BERT (Devlin et al., 2019)\nand RoBERTa (Liu et al., 2019). They evaluate\nthe ‚Äúout-of-the-box‚Äù performance of these mod-\nels without post-processing, as well as the perfor-\nmance of post-hoc calibration methods (e.g., tem-\nperature scaling and label smoothing). Kong et al.\n(2020) proposed regularization methods to better\ncalibrate pre-trained language models. Both stud-\nies assume access to (at least in-domain) training\ndata of the target tasks on which parameterized\ncalibration models can be trained. In contrast, our\nstudy primarily aims to explore better signals in\npre-trained language models to estimate the knowl-\nedge they store. Thus, we focus on methods that do\nnot require additional training data or an external\nknowledge source. Although training-based meth-\nods (e.g., temperature scaling) have the potential to\nachieve better performance in terms of calibration,\noptimal parameters vary depending on models and\ntasks, especially when evaluated in out-of-domain\ndatasets (Desai and Durrett, 2020).\nIn our experiments, all queries have at least one\ncorrect answer. Therefore, when a model cannot\nanswer a question correctly, this implies that it\ndid not acquire the correct knowledge during train-\ning or that its knowledge was not elicited by the\nnatural language query because of a sub-optimal\nprompt (Jiang et al., 2020). However, there are\nalso cases where the question is essentially impos-\nsible to answer due to ambiguity (Zhang and Choi,\n2021) or false presupposition (Kim et al., 2021).\n2024\nRelation Confidence Top predictions\ndate-of-birth Token 1979 (47), 1944 (33), 1988 (10), 1990 (8)\nReranking 1979 (44), 1944 (32), 1953 (13), 1970 (3), 1949 (2)\nplace-of-birth Token Budapest (18), Prague (10), Istanbul (8), Athens (8), Paris (7), Moscow (7), Helsinki (6),\nBucharest (6), Tehran (5), Stockholm (4)\nReranking London (30), Dublin (12), Paris (12), Moscow (5), Madrid (4), Philadelphia (4), Chicago\n(4), Warsaw (3), Tehran (3), Berlin (2)\nplace-of-death Token Paris (38), Rome (32), Moscow (6), Madrid (4), infancy (4), office (3), Athens (2), Helsinki\n(2), Warsaw (2), Amsterdam (2)\nReranking London (46), Paris (14), Rome (7), office (6), Moscow (4), Munich (3), Amsterdam (3),\ninfancy (2), prison (2), Stockholm (2)\nTable 4: Comparison of the most frequent tokens among the top-100 predictions based on different confidence\nscores. Based on the results on the Google-RE dataset with the BERT-base model. The numbers in parentheses\nrepresent the frequency of the predictions.\nModel Pred. GRE TREx CNet SQ All\nT 10.3 29.6 15.8 14.1 24.3\nS 10.5 29.6 14.6 14.4 24.1\nBERT-base G 9.7 28.6 15.3 15.1 23.5\nDM 10.3 29.8 15.4 14.1 24.4\nDV 0.2 0.1 0.1 0.0 0.1\nTD 9.6 29.4 14.2 - -\nT 11.0 31.0 19.3 17.4 26.1\nS 11.2 31.5 17.6 15.7 26.1\nBERT-large G 10.4 29.6 18.6 17.4 25.0\nDM 10.9 31.7 19.6 17.7 26.7\nDV 0.2 0.0 0.0 0.0 0.1\nTD 10.6 30.5 17.0 - -\nT 7.5 23.0 18.5 14.7 20.2\nS 8.2 24.3 17.0 12.2 20.7\nRoBERTa-base G 7.6 22.0 17.4 14.7 19.3\nDM 8.0 24.4 18.3 15.7 21.1\nDV 0.1 0.1 0.1 0.0 0.1\nTD 7.5 23.2 16.4 - -\nTable 5: P@1 based on different prediction scores\nfor each dataset. Bb: BERT-base, Bl: BERT-large,\nT: Token, S: Sent, G: Gap, DM: DropoutMean, DV:\nDropoutVar, TD: TemplateDiff, GRE: Google-RE,\nCNet: ConceptNet, SQ: SQuAD. We omit the result of\nusing the Reranking score because the results are the\nsame as those of Sent by definition.\nAn investigation of such cases remains a direction\nof future research.\n6 Conclusion\nIn this paper, we introduced the selective prediction\nsetting to the LAMA probe benchmark to evaluate\nboth the amount of relational knowledge stored in\na language model and the ability of the models to\neffectively filter out unconfident predictions. We\ncompared different confidence functions that can\nbe calculated using only the model parameters and\nthe output information. The experimental results\nare summarized as follows:\n‚Ä¢ The selective prediction evaluation is more\nrobust to template-related biases than the con-\nventional accuracy-based evaluation (Table 1).\n‚Ä¢ The token log probability is not always the\nbest choice, and the best confidence func-\ntion depends on the language model and the\ndataset (Table 2).\n‚Ä¢ Different confidence functions have different\npreferences over relation types and predicted\ntokens, even though all functions are based\nsolely on the model output (Figure 3, Table 4).\n‚Ä¢ There is no strong correlation between the\nperformance of each confidence function as a\npredictor and a confidence estimator (Table 5).\nFuture studies will include a detailed analysis of the\nrelationship between tasks, models, and confidence\nscores. Moreover, more sophisticated methods will\nbe explored to ensure the reliability of language\nmodel predictions under various tasks. The code\nfor our work is attached as supplementary material.\nLimitations\nIn this paper, we focused on evaluating the predic-\ntions of masked language models on the LAMA\nprobe bencmhark. Although our proposed frame-\nwork is easily applicable to other kinds of lan-\nguage model with small adjustments, some of\nthe confidence functions we investigated require\nproperties specific to particular language models\nand datasets. For instance, Token and Gap func-\ntions require the prediction to be a single token,\nand TemplateDiff requires templates for subject-\nrelation-object triples.\n2025\nEthics Statement\nData and code\nIn our experiments, we use the original LAMA\nbenchmark dataset from Petroni et al. (2019) as\nis. All data are based on publicly available data\nsources and data statistics can be found in the orig-\ninal paper. Parts of the code are based on LAMA2.\nThe license of the code can be found in the supple-\nmentary material.\nDetails of experiments\nThe experiments were conducted using a 2.4GHz\nCPU and an NVIDIA TESLA P100 GPU. Infer-\nence time was 1‚Äì1.5 s per instance for BERT-base\nand 2‚Äì3 s per instance for BERT-large.\nPotential risks\nThis study evaluates the knowledge stored in lan-\nguage models considering the reliability of model\npredictions. However, it should be emphasized that\nthe outputs of the selective classifier constructed by\nthe proposed method do not guarantee the correct-\nness of the model predictions. For the validation\nof each fact, this method should only be used as an\naid, and the final decision should be made by the\nuser.\nAcknowledgements\nWe thank Prof. Simone Teufel, Marco Cognetta\nand anonymous reviewers for their valuable feed-\nback. This study was carried out using the TSUB-\nAME3.0 supercomputer at Tokyo Institute of Tech-\nnology. This work was partially supported by JSPS\nKAKENHI Grant Number 19H01118.\nReferences\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for auto-\nmatic knowledge graph construction. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4762‚Äì4779, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\n2https://github.com/facebookresearch/LAMA\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877‚Äì1901. Curran Associates,\nInc.\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.\nKnowledgeable or educated guess? revisiting lan-\nguage models as knowledge bases. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1860‚Äì1874, Online.\nAssociation for Computational Linguistics.\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.\nFaithful to the original: Fact-aware neural abstrac-\ntive summarization. In Proceedings of the Thirty-\nSecond AAAI Conference on Artificial Intelligence\nand Thirtieth Innovative Applications of Artificial In-\ntelligence Conference and Eighth AAAI Symposium\non Educational Advances in Artificial Intelligence ,\nAAAI‚Äô18/IAAI‚Äô18/EAAI‚Äô18. AAAI Press.\nNicholas Carlini, Chang Liu, √ölfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neu-\nral networks. In Proceedings of the 28th USENIX\nConference on Security Symposium, SEC‚Äô19, pages\n267‚Äì284, USA. USENIX Association.\nShrey Desai and Greg Durrett. 2020. Calibration of\npre-trained transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 295‚Äì302, Online.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRan El-Yaniv and Yair Wiener. 2010. On the Founda-\ntions of Noise-free Selective Classification. Journal\nof Machine Learning Research, 11(53):1605‚Äì1641.\nDavid Ferrucci, Eric Brown, Jennifer Chu-Carroll,\nJames Fan, David Gondek, Aditya A. Kalyanpur,\nAdam Lally, J. William Murdock, Eric Nyberg, John\nPrager, Nico Schlaefer, and Chris Welty. 2010. Build-\ning watson: An overview of the deepqa project. AI\nMagazine, 31(3):59‚Äì79.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as a\nbayesian approximation: Representing model uncer-\ntainty in deep learning. In Proceedings of the 33rd\n2026\nInternational Conference on International Confer-\nence on Machine Learning - Volume 48, ICML‚Äô16,\npage 1050‚Äì1059. JMLR.org.\nYonatan Geifman and Ran El-Yaniv. 2017. Selective\nClassification for Deep Neural Networks. In Pro-\nceedings of the 31st International Conference on\nNeural Information Processing Systems , NIPS‚Äô17,\npages 4885‚Äì4894. Curran Associates Inc.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How can we know when language\nmodels know? on the calibration of language models\nfor question answering. Transactions of the Associa-\ntion for Computational Linguistics, 9:962‚Äì977.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423‚Äì438.\nAmita Kamath, Robin Jia, and Percy Liang. 2020. Se-\nlective question answering under domain shift. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5684‚Äì\n5696, Online. Association for Computational Lin-\nguistics.\nNajoung Kim, Ellie Pavlick, Burcu Karagol Ayan, and\nDeepak Ramachandran. 2021. Which linguist in-\nvented the lightbulb? presupposition verification for\nquestion-answering. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3932‚Äì3945, Online. Association\nfor Computational Linguistics.\nLingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie\nLyu, Tuo Zhao, and Chao Zhang. 2020. Cali-\nbrated language model fine-tuning for in- and out-\nof-distribution data. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1326‚Äì1340, Online. As-\nsociation for Computational Linguistics.\nJey Han Lau, Carlos Armendariz, Shalom Lappin,\nMatthew Purver, and Chang Shu. 2020. How fu-\nriously can colorless green ideas sleep? sentence\nacceptability in context. Transactions of the Associa-\ntion for Computational Linguistics, 8:296‚Äì310.\nNayeon Lee, Yejin Bang, Andrea Madotto, and Pascale\nFung. 2021. Towards few-shot fact-checking via\nperplexity. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 1971‚Äì1981, Online. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nAlessio Miaschi, Dominique Brunato, Felice\nDell‚ÄôOrletta, and Giulia Venturi. 2020. Lin-\nguistic profiling of a neural language model. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 745‚Äì756,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nMathias M√ºller, Annette Rios, and Rico Sennrich. 2020.\nDomain robustness in neural machine translation. In\nProceedings of the 14th Conference of the Associa-\ntion for Machine Translation in the Americas (Volume\n1: Research Track), pages 151‚Äì164, Virtual. Associa-\ntion for Machine Translation in the Americas.\nFabio Petroni, Tim Rockt√§schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463‚Äì2473, Hong Kong, China. Association\nfor Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383‚Äì2392, Austin,\nTexas. Association for Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418‚Äì5426,\nOnline. Association for Computational Linguistics.\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,\nTrevor Darrell, and Kate Saenko. 2018. Object hallu-\ncination in image captioning. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4035‚Äì4045, Brussels,\nBelgium. Association for Computational Linguistics.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2699‚Äì2712, Online. Association for Computational\nLinguistics.\nRobyn Speer and Catherine Havasi. 2012. Representing\ngeneral relational knowledge in ConceptNet 5. In\nProceedings of the Eighth International Conference\non Language Resources and Evaluation (LREC‚Äô12),\npages 3679‚Äì3686, Istanbul, Turkey. European Lan-\nguage Resources Association (ELRA).\n2027\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason\nPhang, Anhad Mohananey, Phu Mon Htut, Paloma\nJeretic, and Samuel R. Bowman. 2019. Investigating\nBERT‚Äôs knowledge of language: Five analysis meth-\nods with NPIs. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2877‚Äì2887, Hong Kong, China. Association\nfor Computational Linguistics.\nMichael Zhang and Eunsol Choi. 2021. SituatedQA: In-\ncorporating extra-linguistic contexts into QA. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 7371‚Äì\n7387, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\n2028"
}