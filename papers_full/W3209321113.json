{
    "title": "3D Object Tracking with Transformer",
    "url": "https://openalex.org/W3209321113",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2131075666",
            "name": "Cui Yu-bo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2061772064",
            "name": "Fang Zheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4212387993",
            "name": "Shan Jiayao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4302096223",
            "name": "Gu, Zuoxu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2244971966",
            "name": "Zhou, Sifan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3129373298",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3119686997",
        "https://openalex.org/W3036853234",
        "https://openalex.org/W2966759264",
        "https://openalex.org/W2962922818",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W3003618643",
        "https://openalex.org/W2470394683",
        "https://openalex.org/W2435777129",
        "https://openalex.org/W2964253307",
        "https://openalex.org/W3115390238",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W3034314779",
        "https://openalex.org/W2897529137",
        "https://openalex.org/W2963727135",
        "https://openalex.org/W2150066425",
        "https://openalex.org/W2963534981",
        "https://openalex.org/W2799058067",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3138486308",
        "https://openalex.org/W2912429050"
    ],
    "abstract": "Feature fusion and similarity computation are two core problems in 3D object tracking, especially for object tracking using sparse and disordered point clouds. Feature fusion could make similarity computing more efficient by including target object information. However, most existing LiDAR-based approaches directly use the extracted point cloud feature to compute similarity while ignoring the attention changes of object regions during tracking. In this paper, we propose a feature fusion network based on transformer architecture. Benefiting from the self-attention mechanism, the transformer encoder captures the inter- and intra- relations among different regions of the point cloud. By using cross-attention, the transformer decoder fuses features and includes more target cues into the current point cloud feature to compute the region attentions, which makes the similarity computing more efficient. Based on this feature fusion network, we propose an end-to-end point cloud object tracking framework, a simple yet effective method for 3D object tracking using point clouds. Comprehensive experimental results on the KITTI dataset show that our method achieves new state-of-the-art performance. Code is available at: https://github.com/3bobo/lttr.",
    "full_text": "CUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER 1\n3D Object Tracking with Transformer\nYubo Cui1,2\nybcui21@stumail.neu.edu.cn\nZheng Fang1\nfangzheng@mail.neu.edu.cn\nJiayao Shan1\nshanjiayao97@stumail.neu.edu.cn\nZuoxu Gu1\nguzuoxu@stumail.neu.edu.cn\nSifan Zhou1\nzhousifan@stumail.neu.edu.cn\n1 Northeastern University\nShenyang, China\n2 Science and Technology on\nNear-Surface Detection Laboratory\nWuxi, China\nAbstract\nFeature fusion and similarity computation are two core problems in 3D object track-\ning, especially for object tracking using sparse and disordered point clouds. Feature\nfusion could make similarity computing more efﬁcient by including target object in-\nformation. However, most existing LiDAR-based approaches directly use the extracted\npoint cloud feature to compute similarity while ignoring the attention changes of object\nregions during tracking. In this paper, we propose a feature fusion network based on\ntransformer architecture. Beneﬁting from the self-attention mechanism, the transformer\nencoder captures the inter- and intra- relations among different regions of the point cloud.\nBy using cross-attention, the transformer decoder fuses features and includes more target\ncues into the current point cloud feature to compute the region attentions, which makes\nthe similarity computing more efﬁcient. Based on this feature fusion network, we pro-\npose an end-to-end point cloud object tracking framework, a simple yet effective method\nfor 3D object tracking using point clouds. Comprehensive experimental results on the\nKITTI dataset show that our method achieves new state-of-the-art performance. Code is\navailable at: https://github.com/3bobo/lttr.\n1 Introduction\nRecently, LiDAR-based 3D object tracking has been received more and more attention. Ben-\neﬁting from the development of visual tracking [1, 7, 13, 15, 16], most 3D tracking meth-\nods [11, 20, 30] also use the Siamese-like tracking pipeline. The pipeline ﬁrst inputs template\npoint clouds of the target object and search point clouds of the current frame to its top and\nbottom branches respectively, then fuses the two-branch features based on similarity. Finally,\nthe fused features are used to localize the position of the object to be tracked. However, com-\npared with visual tracking, LiDAR-based tracking has more challenges due to the sparsity\nand disorder of the point clouds. For example, the point clouds will become much sparser\nwith the increasing distance of the object, which hinders the feature extraction. Meanwhile,\n© 2021. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\narXiv:2110.14921v1  [cs.CV]  28 Oct 2021\n2 CUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER\nthe disorder of the point clouds also makes it hard to compute the similarity between the two\nbranches.\nPrevious works use shape completion [11], image prior [30], or feature augmentation [20]\nto deal with the above problems. Although they achieve better tracking performance, they\nusually ignore the attention changes in different regions of the object during tracking. How-\never, the tracking method should pay more attention to regions with salient features when\nprocessing dense point cloud, while it should focus on regions with more points when pro-\ncessing sparse point cloud. Therefore, in the tracking process, different regions in the point\ncloud should have different attentions depending on the situation, even the same region also\nshould have different attentions in different periods.\nInspired by [8, 12], in this work, we introduce transformer architecture [24] into LiDAR-\nbased 3D object tracking. First, the point cloud is divided into several non-overlapping\nlocal regions. Then, based on the self-attention mechanism of the transformer encoder, the\nrepresentation of each region is constructed by capturing the structural information of the\nlocal points, and the feature of the point cloud is reconstructed by considering the global\nrelation among regions. Finally, in the decoding process, through propagating the template\nfeature to the current search feature, the feature of the target object becomes more prominent\nand includes more target cues. Furthermore, following [27], we propose a LiDAR-based\n3D Object Tracking with TRansformer framework (LTTR), which is simple but efﬁcient.\nExperiments on KITTI [10] dataset show that LTTR has outstanding tracking performance\nand achieves new state-of-the-art performance.\nIn summary, our contributions are as follows:\n• We propose a transformer architecture that explores not only the inter- and intra- re-\nlations among different regions within the point cloud but also the relations between\ndifferent point clouds.\n• We propose a new 3D object tracking framework based on the transformer architec-\nture, which is simple but efﬁcient.\n• Extensive experimental results on KITTI dataset show that the proposed method achieves\noutstanding tracking performances.\n2 Related Work\n2.1 3D Object Tracking\n3D object tracking aims to localize the object in successive frames in 3D space given the\ninitial position. Previous works usually focus on RGB-D data [2, 14], which heavily depend\non visual features. Recently, with the development of 3D vision methods, there are many\nLiDAR-based 3D object tracking works [11, 20, 30]. For example, Giancola et al. [11]\nused point clouds to track object in LiDAR space based on computing the cosine similarity\nbetween template and search branch. However, they ignored the characteristics of the point\nclouds. Zou et al. [30] leveraged RGB image feature to generate 3D search space, and\nused point clouds feature to track. Based on [11], Qi et al. [20] proposed a feature fusion\nmodule to augment search point features and achieved state-of-the-art tracking performance.\nIn this paper, we explore the inter- and intra- relations among different regions and propagate\nfeatures between branches to compute region attentions by leveraging the transformer.\nCUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER 3\nEncoder\nEncoder\nData Processing\nSearch\nPoint Cloud\nTemplate\nPoint Cloud\nFeature Extraction Feature Fusion Prediction\nVoxelization\nVoxelization To BEV\nCenter\nHeatmap\nOffset \nMap\nZ-Value \nMap\nOrientation \nMap\nTo BEV\nDecoder\n3D Backbone\n2D Backbone Template \nFeature\nSearch \nFeature\nSearch Region \nFeature\nTemplate Region \nFeature\nSimilarity \nFeature\nFigure 1: An overview of our LiDAR-based 3D Object Tracking with Transformer frame-\nwork (LTTR). ⊙is the cross-correlation operation, ⊗represents multiplication operation.\n2.2 Vision Transformer\nDue to the great success of Transformer [24] in natural language processing, recent works\nstart to apply it to vision tasks. Dosovitskiy et al. [8] proposed ViT to apply a pure trans-\nformer in image classiﬁcation. They split an image into a series of ﬂattened patches and pro-\ncesses the patches by vanilla transformer block to get image cls token. Furthermore, Han et\nal. [12] explored the intrinsic structure information inside each patch and achieved higher\naccuracy than ViT. Chu et al. [6] explored the position embedding for ViT and proposed a\nconditional positional encoding scheme. Liu et al. [18] proposed a shifted windows-based\nattention and a pure hierarchical backbone which could be used in dense vision tasks.\nCarion et al. [3] proposed DETR which is the ﬁrst work to apply the transformer into\ndense prediction tasks. They applied the transformer architecture into object detection and\nfound the best match between the encoded image embeddings and object queries via the\nattention module. However, DETR suffers from heavy computation and slow convergence.\nZhu et al. [29] proposed deformable attention to reduce the complexity and speed up con-\nvergence, yielding higher performance. There are also other works applying transformers to\nother tasks, such as visual tracking [4, 25], multi-object tracking [19, 22].\n3 Method\nIn this section, we present the proposed framework, named LTTR. As shown in Figure 1, the\nframework consists of data processing, feature extraction, feature fusion, and prediction. We\nwill introduce the details of LTTR in the following subsections.\n3.1 Overall Architecture\nData Processing. We adopt the Siamese-like tracking pipeline which inputs template and\nsearch point cloud to top and bottom branches respectively. By reading the label, we obtain\nthe 3D box of the target object and transform the whole scene point clouds into the local\ncoordinate system whose origin is set as the center of the box. After that, we randomly shift\nthe (x,y) of the center of the 3D box to get the training label value in the search branch, then\nnormalize the points into the x-axis of the box in the template branch. Finally, we apply the\nsame 3D range to both branches to get the input pair. The point cloud in the 3D range in the\nsearch branch is the search point cloud, and the point cloud in the 3D box in the template\n4 CUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER\nbranch is the template point cloud. For both branches, we divide the points into regular\nvoxels with a spatial resolution of W ×L ×H and get input I ∈RW×L×H.\nFeature Extraction. We use the 3D sparse convolution network and 2D convolution\nnetwork as the backbone network to extract features for both branches. Through 3D sparse\nconvolution, the voxels are converted into feature volumes with 8 ×downsampled sizes.\nBy converting the 8×downsampled 3D feature volumes into BEV representation, the ﬁnal\nfeature map M ∈R\nW\n8 ×L\n8 ×F is generated following the 2D backbone network, where F is the\nfeature channels. The weights are sharing between two branches.\nFeature Fusion. Subsequently, we update and fuse the search feature Ms and the tem-\nplate feature Mt in the feature fusion network. As shown in Figure 1, Ms and Mt are ﬁrst\nfed into the encoder respectively, and then sent into the decoder together. Following [12],\nthe transformer encoder receives M ∈R\nW\n8 ×L\n8 ×F and outputs region feature G ∈RN×D of\nchannel D with N regions. The transformer decoder propagates information from template\nregions Gt to search regions Gs and decodes a fused Gs ∈RN×D through cross-attention.\nMoreover, we project the region feature G ∈RN×D to G ∈RN×1 as an attention weight by\na fully-connected layer in both branches, and unfold the original feature M to the size of\nW×L×F\n64×N ×N to multiply with G. The feature is recovered back to the size of W\n8 ×L\n8 ×F ﬁ-\nnally. The details of transformer architecture will be described in Section 3.2. Following the\ndepthwise cross-correlation, the similarity feature with size 1 ×1 ×F is computed between\nMs and Mt . Finally, we multiply the similarity feature with Ms to recover feature size for\ndense prediction.\nPrediction. Following [9, 27], we use a center-based regression to predict several object\nproperties. The regression consists of four heads, including the center heatmap head, local\noffset head, z-axis location head, and orientation head. Since our aim is to track the target\nobject, we follow the assumption in [20] that the 3D object size is known. The heads produce\na center heatmap ˆH ∈R\nW\n8 ×L\n8 ×C, a local offset regression map ˆO ∈R\nW\n8 ×L\n8 ×2, a z-value map\nˆZ ∈R\nW\n8 ×L\n8 ×1 and an orientation map ˆΘ ∈R\nW\n8 ×L\n8 ×2 respectively, where C is the number of\nclasses (1 in our tracking task) and orientation includes sin(θ) and cos(θ). We follow [9] to\nset heatmap value for every point (x, y) in the downsampling feature map as:\nHx,y,c =\n\n\n\n1, if d = 0\n0.8, if d = 1\n1\nd , otherwise\n(1)\nwhere d is the Euclidean distance calculated between the object center and the point location\nin the downsample BEV map. A prediction ˆHx,y,c = 1 corresponds to the object center and\nˆHx,y,c = 0 corresponds to background. We train the heatmap with focal loss [17]:\nLheat = −1\nN ∑\nx,y,c\n\n\n\n(\n1 −ˆHx,y,c\n)α log\n(ˆHx,y,c\n)\nif Hx,y,c = 1\n(1 −Hx,y,c)β (ˆHx,y,c\n)α\notherwiselog\n(\n1 −ˆHx,y,c\n)\n,\n(2)\nFor other heads, we use L1 loss:\nLv = 1\nN\nN\n∑\nk=1\n⏐⏐⏐ˆVp(k) −v(k)\n⏐⏐⏐. (3)\nCUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER 5\n Attention Block\nLinear\nAttention Block\n \nSplit & Unfold\n P2 PN \nPoint  Embeddings\nP3  \nE1\npoint  E2\npoint EN point\nPoint  Positional Embeddings\nE3\npoint  \nP1\nInput Feature\nRegion Embeddings (N+1, D)\nRegion Tensor (N, S)\nGclass G1 G2\nGN \nOutput Region Embeddings\n \nSelf-Attention\nFeed-Forward\nLayer Norm\nLayer Norm\nOutput\nInput\nJ × \nGclass G1 G2\nGN \nRegion Embedding Memories\nEclass\nregion E1\nregion E2\nregion EN\nregion \nRegion Positional Embeddings\nLinear\n(a)\nEncoder\n Encoder\nCross-Attention\nFeed-Forward\nLayer Norm\nSearch Region\nEmbeddings\nK V Q\nJ × \nSearch\nFeature\nTemplate\nFeature\nSelf-Attention\nLayer Norm\n (b)\nFigure 2: (a) The transformer encoder. (b)An overview of the proposed transformer archi-\ntecture.\nwhere Lv ∈(Lo f f, Lz, Lori), ˆV is the true value andv(k) is the predicted value for these heads.\nTherefore, the overall training loss is\nL= Lheat +λoff Loff +λzLz +λoriLori (4)\nwhere λ is the regularization parameter for each head.\n3.2 Transformer Architecture\nMulti-head Attention. Attention function is the core of the transformer, thus we ﬁrst brieﬂy\nreview the principle of attention. Given query matrix Q, key matrix K and value matrix V,\nattention function computes the similarity matrix between query and key, then multiplies\nvalue with normalized similarity, deﬁned as:\nAttention(Q, K,V) =softmax(QKT\n√dk\n)V (5)\nwhere dk is the dimension of key. Meanwhile, multiple heads are usually utilized in the\nattention function. Multi-head attention (MHA) projects query, key, and value into different\nfeature spaces h times, where h is the number of heads, and computes the attention in parallel\nfor every of these projected queries, keys, and values. The results from different heads are\nconcatenated and projected to the ﬁnal value. Following [24], the deﬁne of MHA is:\nMHA(Q, K,V) =Concat(head1, ...,headh)WO (6)\nwhere headi = Attention(QWQ\ni , KWK\ni ,VW V\ni ) and WO ∈Rhdv×dmodel , dv is the dimension of\nvalue and dmodel is the dimension of a single head attention.\nTransformer Encoder. The transformer encoder takes BEV point cloud feature M ∈\nR\nW\n8 ×L\n8 ×F as its input. Following [12], we ﬁrst split M into N non-overlapping regions of\nresolution (R, R) and reshape them to M′∈RN×(R×R×F), where N = W\n8R ×L\n8R . Meanwhile,\nwe also transform each region into the target size (R′, R′) with point unfold. After applying\na linear projection, the sequence of regions can be formed as:\nU0 = [U1\n0 ,U2\n0 , ···,UN\n0 ] ∈RN×(R′×R′×S) (7)\n6 CUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER\nwhere Ui\n0 ∈RR′×R′×S, i = 1, 2, ···, N, and S is the number of channels. Furthermore, each\nregion tensor can also be viewed as a sequence of point tensors:\nUi\n0 = [Pi,1\n0 , Pi,2\n0 , ···, Pi,N′\n0 ] (8)\nwhere N′= R′2. By utilizing multi-head self-attention, we could explore the intra-relation\namong regions:\nˆUi\nj = Ui\nj−1 +MHA(Ui\nj−1,Ui\nj−1,Ui\nj−1), (9)\nUi\nj = ˆUi\nj +FFN( ˆUi\nj). (10)\nwhere j = 1, 2, ···, J is the index of the j-th layer, J is the total number of layers, and FFN\nmeans the feed-forward network, which is a 2-layer MLP module. The point-level MHA\nbuilds the local relations among points within one region and produces the region tensor.\nAdditionally, similar to previous vision transformer works [3, 8, 12], we create a set of\nlearnable parameters called region embedding memories G0 for the region tensors and take\nthem into output as the region representations. Specially, the region embedding memories\nare added with the region tensors in each layer:\nG0 = [Gclass, G1\n0, G2\n0, ···, GN\n0 ] ∈R(N+1)×D (11)\nGi\nj−1 = Gi\nj−1 +Φ(Ui\nj−1), (12)\nwhere Gclass is the global point cloud embedding, Gi\nj−1 ∈RD, Φ is the projection function,\nwhich is fully-connected layer in our implementation. We random initialize all of the region\nembedding memories. Meanwhile, we utilize the MHA once again for region embeddings.\nThe mechanism can be summarized as:\nˆGi\nj = Gi\nj−1 +MHA(Gi\nj−1, Gi\nj−1, Gi\nl−1), (13)\nGi\nj = ˆGi\nj +FFN( ˆGi\nj). (14)\nThe region-level MHA explores the inter-relation among regions, building the global in-\nformation of the point cloud. Therefore, the region embedding memories learn the region\nrepresentation by adding to region tensors and being sent into the MHA during training.\nMeanwhile, although Gclass does not have a corresponding region tensor to add, it can also\ncapture the global information by exchanging information with the other region embeddings\nthrough the region-level MHA. Furthermore, we use standard learnable 1D position embed-\ndings to add to embeddings as follows:\nT = T +E (15)\nwhere T ∈(G,U), E ∈(Eregion, Epoint), Eregion ∈R(N+1)×D and Epoint ∈RN′×S. Both the\nregion and point position embeddings are added to the corresponding embeddings before\nMHA and are shared across the same data level, thus the local and global spatial information\ncan be maintained. The whole process is shown in Figure 2(a).\nBy processing both points and regions, the encoder explores the local information across\npoints within regions and global relations across regions, producing G ∈R(N+1)×Dfor each\npoint cloud feature M. We take G = [G1\n0, G2\n0, ···, GN\n0 ] ∈RN×D as the input of the decoder.\nTransformer Decoder.The above encoder processes template and search features sepa-\nrately, thus the information only ﬂows within the point cloud itself. To build the inter-relation\nCUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER 7\nMethod Reference LiDAR RGB Success Precision FPS\nSC3D [11] CVPR2019 √ 41.3 57.9 1.8\nF-Siamese [30] IROS2020 √ √ 37.1 50.6 -\nP2B [20] CVPR2020 √ 56.2 72.8 45.5\nLTTR(Ours) - √ 65.0 77.1 22.6\nTable 1: Comprehensive comparison with state-of-the-art trackers on Car category.\nbetween point clouds and exchange information across branches, we further utilize a trans-\nformer decoder to fuse features. The decoder fuses features by propagating template region\nfeature Gt to search region feature Gs. The decoder ﬁrst updates the search region feature Gs\nby self-attention mechanism, then computes the similarity among regions from search and\ntemplate point clouds based on the cross-attention mechanism. Specially, the decoder takes\nGs as the query and Gt as key and value through the cross-attention, the fused search region\nfeature Gs is generated following a feed-forward layer. The decoder is shown in Figure 2(b)\nand can be summarized as:\nˆGs = Gs +MHA(Gs, Gs, Gs), (16)\n˜Gs = ˆGs +MHA( ˆGs, Gt , Gt ), (17)\nGs = ˜Gs +FFN( ˜Gs). (18)\nThrough the decoder, the search and template region features exchange region information,\nwhich makes the search region feature include much more information of the target object\nand computes the region attention. To have clear representations, the layer norm operation\nis not represented in the above equations.\n4 Experiments\n4.1 Datasets and Evaluation\nWe use KITTI tracking dataset [10] as the benchmark and follow [20] in data split. We also\nuse One Pass Evaluation (OPE) as evaluation metric, including Success and Precision.\n4.2 Implementation Details\nIn data processing, we set point cloud range as [-3.2m, 3.2m], [-3.2m, 3.2m], [-3m, 1m]\nalong x, y, z axis, and set voxel size as [0.025m, 0.025m, 0.05m]. The template and search\npoints are voxelized following [28]. A maximum of ﬁve points are randomly sampled from\neach voxel. Meanwhile, we use the same backbone as [26, 28]. In regression, each head\nconsists of four convolution layers to predict and the heatmap head is followed by a sigmoid\nfunction to generate the ﬁnal score. Following the training setting of the popular codebase\nOpenPCDet [23], we train the network end-to-end with 80 epochs and 36 batch. In loss\nsetting, we set α = 2, β = 4 in Equation 2, and set λz = 1.5, λo f f= λori = 1 in Equation 4.\n4.3 State-of-the-art Comparisons\nWe compare our LTTR with previous state-of-the-art methods on KITTI dataset. As shown\nin Table 1, our approach surpasses the previous methods by +8.8% Success and +4.3% Preci-\nsion respectively in the Car category. Additionally, LTTR achieves a real-time running speed.\n8 CUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER\nMethod Car Pedestrian Van Cyclist Mean\nFrame Number 6424 6088 1248 308 14068\nSuccess\nSC3D [11] 41.3 18.2 40.4 41.5 31.2\nF-Siamese [30] 37.1 16.2 - 47.0 -\nP2B [20] 56.2 28.7 40.8 32.1 42.4\nLTTR(Ours) 65.0 33.2 35.8 66.2 48.7\nPrecision\nSC3D [11] 57.9 37.8 47.0 70.4 48.5\nF-Siamese [30] 50.6 32.2 - 77.2 -\nP2B [20] 72.8 49.6 48.4 44.7 60.0\nLTTR(Ours) 77.1 56.8 45.6 89.9 65.8\nTable 2: Extensive comparisons with state-of-the-art trackers on multiple categories.\nFigure 3: The inﬂuence of the number of points on the ﬁrst frame car.\nWe also report multiple categories tracking results on KITTI dataset, including Pedestrian,\nVan, and Cyclist. As shown in Table 2, our method outperforms P2B [20] by 5% on average.\nIn particular, LTTR shows its advantages on objects with a small size, e.g. Pedestrian and\nCyclist, surpassing previous methods by a large margin. Considering the difference among\nthese categories, our method is a general and efﬁcient method for different categories.\nWe also report the inﬂuence of the number of the ﬁrst frame point in the Car category.\nAs shown in Figure 3, with more points, LTTR has a higher performance. We believe that\nmore points in the ﬁrst frame give the network enough information about the target to track.\n4.4 Ablation Study\nDifferent Network Version Success Precision\nBaseline 57.2 70.9\nEncoder (w/o Decoder) 60.6 3.4%↑ 71.91.0%↑\nEncoder + Decoder (Max) 64.2 7.0%↑ 77.36.4%↑\nEncoder + Decoder 65.07.8%↑ 77.16.2%↑\nTable 3: Ablative study of our transformer\narchitecture.\nIn this section, we ablate the proposed method\non the Car category of KITTI dataset. We\nﬁrst ablate the transformer network to ver-\nify the inﬂuence of the encoder and decoder.\nWe introduce a baseline version and a Max-\nDecoder version. The baseline version does\nnot have any transformer component, and the\nMax-Decoder version inputs Gclass instead of G = [G1\n0, G2\n0, ···, GN\n0 ] in the template branch to\nthe decoder. Moreover, we compare the different numbers of heads, layers and region sizes\nin the transformer to validate our design choices. Finally, we compare different backbones\nand regression heads to explore their inﬂuence on the proposed method.\nEffect of Encoder. As shown in Table 3, with the transformer encoder, the performance\nhas +3.4% and +1.0% gains on Success and Precision respectively. The result indicates the\nCUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER 9\neffectiveness of our encoder to build the local and global relations of point cloud. It is worth\nnoting that even without any transformer component, our baseline version has a competitive\nperformance with the state-of-the-art methods.\nEffect of Decoder. We further add the transformer decoder. Specially, we evaluate\ntwo decoder versions different in template input. As shown in Table 3, both of them bring\nsigniﬁcant performance improvements. However, compared to Max-version, our version has\na balanced result between Success and Precision. We believe that the Max-version loses the\ninter-relation among regions of the point cloud due to its single global input.\nSuccess Precision\nHead Number\n1 61.0 73.7\n2 61.6 74.6\n4 62.1 75.3\n8 65.0 77.1\n12 63.8 78.3\nLayer Number\n1 65.0 77.1\n2 61.9 74.3\n4 62.7 75.7\n6 61.1 73.8\n8 60.2 73.2\nRegion Size\n1 60.5 73.3\n4 63.6 76.7\n16 65.0 77.1\nTable 4: Ablative study of our transformer\narchitecture.\nStructure Modiﬁcations. We also dis-\ncuss the details of our transformer structure\nas shown in Table 4, including the number of\nheads, number of encoder/decoder layers and\nthe region size. All experimental networks\nhave a complete encoder and decoder compo-\nnent. For the number of heads, we observe that\nheads=8 achieves the best performance, while\nincrease heads to 12 results in a decrease in\nSuccess but an increase in Precision. The re-\nsults indicate that MHA is efﬁcient in our trans-\nformer architecture, as discussed in Section\n3.2, but too many heads may result in degen-\neration in orientation prediction. Meanwhile,\nstacking more layers does not bring in performance improvement but has more parameters\nand lower speed. We speculate that more layers may divide the template and search features\ninto different feature subspaces. Different from detection task, the tracking task has two\ninput branches and tracks the object based on their similarity. Therefore, tracking method\nrequires the template and search features to be in the same feature spaces to have a better\nsimilarity computation. Additionally, with a small region size, the performance of the net-\nwork degenerates to the encoder version. We believe that the smaller size generates more\nregions and leads to the decoder not being able to exchange global information effectively.\nThus, we use the max non-overlapping size R = 16 for higher performance.\nBackbone. We also make modiﬁcations to the backbone to explore whether the per-\nformance could be further improved by increasing the parameters in the backbone. Our\nbackbone follows Second [26], a backbone baseline in 3D vision for its simple architecture\nand wide use [5, 21, 28]. The baseline includes 3D and 2D backbones to process voxels and\nBEV features respectively. The 3D backbone is termed as BaseV oxel and the 2D backbone is\ntermed as BaseBEV . In this comparison experiment, we use the resnet-manner version of Ba-\nseV oxel in OpenPCDet [23] and termed it as ResV oxel, which adds a residual path in every\nsparse block of BaseV oxel. Meanwhile, we add one convolution block to BaseBEV and term\nit as DeepBEV . Therefore, the ResV oxel has more parameters than BaseV oxel, and DeepBEV\nis deeper than BaseBEV . However, as Table 5 shows, with the network going deeper and the\ntotal parameters becoming larger, the performance does not have improved but decreased.\nWe speculate that more parameters in the backbone may hinder the transformer to capture\nuseful information, thus our baseline backbone could achieve better performance with fewer\nparameters comparing to these modiﬁcations.\nRegression Head. We compare our center-based regression head with an anchor-based\ncounterpart. For anchor-based regression, we follow the setting of Second [26]. Specially,\nfor every location, we set two anchors with 0 degrees and 90 degrees, and the thresholds for\n10 CUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER\n3D Backbone 2D Backbone 3D Params 2D Params Success Precision\nBaseV oxel BaseBEV 1.280K 8.266M 65.0 77.1\nDeepBEV 1.280K 12.988M 62.4 76.4\nResV oxel BaseBEV 2.656K 8.266M 62.6 75.4\nDeepBEV 2.656K 12.988M 60.2 73.8\nTable 5: Ablative study of different 3D and 2D backbones.\nFigure 4: Visualization results. There are template point cloud, search point cloud, heatmap\nwithout transformer, point-level attention, region-level attention, heatmap with transformer\nand the predicted boxes from left to right.\nSuccess Precision\nCenter-based 65.0 77.1\nAnchor-based 65.3 75.1\nTable 6: Comparison of different regres-\nsion heads.\npositive and negative are 0.6 and 0.45 respec-\ntively. As shown in Table 6, the anchor-based\nregression improves the Success 0.3 points\nwhile reduces the Precision 2.0 points. The\nresult shows that the center-based regression\nhas better prediction results in location but is\nweaker in rotation regression compared to anchor-based regression. We believe this is be-\ncause of the pre-deﬁned anchor rotation degree in anchor-based regression. Meanwhile,\nthe anchor-based regression needs more hyper-parameters in the anchor setting which needs\nﬁne-tuning. Therefore, to have fewer hyper-parameters and more balanced tracking results,\nwe adopt the center-based regression.\n4.5 Qualitative Visualization\nTo explore the effect of the transformer network, we visualized the predicted heatmap and\nthe transformer point and region weights, as shown in Figure 4. Compared to the heatmap\nwithout transformer, the heatmap with transformer accurately ﬁnds the target position with\nthe help of point and region attentions, avoiding the false track. The results verify the effec-\ntiveness of the proposed transformer network, even for target object with sparse points.\n5 Conclusions\nIn this paper, we present LTTR, a novel tracking framework based on the transformer.\nThrough the transformer network, LTTR builds local information and global relation within\nthe point cloud, explores the inter-relation between point clouds, and predicts the 3D bound-\ning box of the target object by a center-based regression. Comprehensive experiments on\nKITTI dataset demonstrate that our method achieves new state-of-the-art performance. In\nthe future, we will investigate how to integrate temporal information into our method.\nCUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER 11\nAcknowledgments This work was supported by National Natural Science Foundation\nof China (62073066, U20A20197), Science and Technology on Near-Surface Detection\nLaboratory (6142414200208), the Fundamental Research Funds for the Central Universi-\nties (N182608003), Major Special Science and Technology Project of Liaoning Province\n(No.2019JH1/10100026), and Aeronautical Science Foundation of China (No.201941050001).\nReferences\n[1] Luca Bertinetto, Jack Valmadre, João F. Henriques, Andrea Vedaldi, and Philip H. S.\nTorr. Fully-convolutional siamese networks for object tracking. In Computer Vision –\nECCV 2016 Workshops, pages 850–865, Cham, 2016. Springer International Publish-\ning. ISBN 978-3-319-48881-3.\n[2] Adel Bibi, Tianzhu Zhang, and Bernard Ghanem. 3d part-based sparse tracker with\nautomatic synchronization and registration. In 2016 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 1439–1448, 2016. doi: 10.1109/CVPR.\n2016.160.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kir-\nillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Com-\nputer Vision – ECCV 2020, pages 213–229, Cham, 2020. Springer International Pub-\nlishing. ISBN 978-3-030-58452-8.\n[4] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu. Trans-\nformer tracking. 2021. URL https://arxiv.org/abs/2103.15436.\n[5] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast point r-cnn. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.\n[6] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. Conditional positional encodings for vision transformers. 2021. URL\nhttps://arxiv.org/abs/2102.10882.\n[7] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Atom:\nAccurate tracking by overlap maximization. In 2019 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 4655–4664, 2019. doi:\n10.1109/CVPR.2019.00479.\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. 2020. URL https://arxiv.org/abs/\n2010.11929.\n[9] Runzhou Ge, Zhuangzhuang Ding, Yihan Hu, Yu Wang, Sijia Chen, Li Huang, and\nYuan Li. Afdet: Anchor free one stage 3d object detection. 2020. URL https:\n//arxiv.org/abs/2006.12671.\n[10] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driv-\ning? the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision\n12 CUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER\nand Pattern Recognition (CVPR), pages 3354–3361, 2012. doi: 10.1109/CVPR.2012.\n6248074.\n[11] Silvio Giancola, Jesus Zarzar, and Bernard Ghanem. Leveraging shape completion\nfor 3d siamese tracking. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE, June 2019. ISBN 9781728132938. doi:\n10.1109/cvpr.2019.00145. URL http://dx.doi.org/10.1109/CVPR.2019.\n00145.\n[12] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Trans-\nformer in transformer. 2021. URL https://arxiv.org/abs/2103.00112.\n[13] David Held, Sebastian Thrun, and Silvio Savarese. Learning to track at 100 fps with\ndeep regression networks. In Computer Vision – ECCV 2016, pages 749–765, Cham,\n2016. Springer International Publishing. ISBN 978-3-319-46448-0.\n[14] U ˘gur Kart, Joni-Kristian Kämäräinen, and Ji ˇrí Matas. How to make an rgbd tracker?\nIn Computer Vision – ECCV 2018 Workshops, pages 148–161, Cham, 2019. Springer\nInternational Publishing. ISBN 978-3-030-11009-3.\n[15] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance visual\ntracking with siamese region proposal network. In 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 8971–8980, 2018. doi: 10.\n1109/CVPR.2018.00935.\n[16] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan.\nSiamrpn++: Evolution of siamese visual tracking with very deep networks. In 2019\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\n4277–4286, 2019. doi: 10.1109/CVPR.2019.00441.\n[17] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss\nfor dense object detection. In 2017 IEEE International Conference on Computer Vision\n(ICCV). IEEE, Oct 2017. ISBN 9781538610329. doi: 10.1109/iccv.2017.324. URL\nhttp://dx.doi.org/10.1109/iccv.2017.324.\n[18] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and\nBaining Guo. Swin transformer: Hierarchical vision transformer using shifted win-\ndows, 2021. URL https://arxiv.org/abs/2103.14030.\n[19] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer.\nTrackformer: Multi-object tracking with transformers. 2021. URL https://\narxiv.org/abs/2101.02702.\n[20] Haozhe Qi, Chen Feng, Zhiguo Cao, Feng Zhao, and Yang Xiao. P2b: Point-to-\nbox network for 3d object tracking in point clouds. In 2020 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR). IEEE, Jun 2020. ISBN\n9781728171685. doi: 10.1109/cvpr42600.2020.00636. URL http://dx.doi.\norg/10.1109/CVPR42600.2020.00636.\n[21] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and\nHongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection.\nIn IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\n2020.\nCUI ET AL.: 3D OBJECT TRACKING WITH TRANSFORMER 13\n[22] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Tao Kong,\nZehuan Yuan, Changhu Wang, and Ping Luo. Transtrack: Multiple-object tracking\nwith transformer. 2020. URL https://arxiv.org/abs/2012.15460.\n[23] OpenPCDet Development Team. Openpcdet: An open-source toolbox for 3d object de-\ntection from point clouds. https://github.com/open-mmlab/OpenPCDet,\n2020.\n[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Ad-\nvances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,\n2017. URL https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n[25] Ning Wang, Wengang Zhou, Jie Wang, and Houqaing Li. Transformer meets tracker:\nExploiting temporal context for robust visual tracking. 2021. URL https://\narxiv.org/abs/2103.11681.\n[26] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection.\nIn Sensors, volume 18, 2018. doi: 10.3390/s18103337. URL https://www.mdpi.\ncom/1424-8220/18/10/3337.\n[27] Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points. 2019. URL\nhttps://arxiv.org/abs/1904.07850.\n[28] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning for point cloud based 3d\nobject detection. In 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR). IEEE, Jun 2018. ISBN 9781538664209. doi: 10.1109/cvpr.\n2018.00472. URL http://dx.doi.org/10.1109/CVPR.2018.00472.\n[29] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable\ndetr: Deformable transformers for end-to-end object detection. In International Con-\nference on Learning Representations (ICLR), 2021. URL https://openreview.\nnet/forum?id=gZ9hCDWe6ke.\n[30] Hao Zou, Jinhao Cui, Xin Kong, Chujuan Zhang, Yong Liu, Feng Wen, and Wan-\nlong Li. F-siamese tracker: A frustum-based double siamese network for 3d single\nobject tracking. In 2020 IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS). IEEE, Oct 2020. ISBN 9781728162126. doi: 10.1109/\niros45743.2020.9341120. URL http://dx.doi.org/10.1109/IROS45743.\n2020.9341120."
}