{
  "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances",
  "url": "https://openalex.org/W4389520806",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2126310749",
      "name": "Zihan Zhang",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2152013896",
      "name": "Meng Fang",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A2058929874",
      "name": "Ling Chen",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2915641214",
      "name": "Mohammad Reza Namazi Rad",
      "affiliations": [
        "University of Wollongong"
      ]
    },
    {
      "id": "https://openalex.org/A754385464",
      "name": "Jun Wang",
      "affiliations": [
        "University College London"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W4385565472",
    "https://openalex.org/W4225909425",
    "https://openalex.org/W4288725442",
    "https://openalex.org/W4386875188",
    "https://openalex.org/W4303441863",
    "https://openalex.org/W4287204036",
    "https://openalex.org/W4394743141",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4385574183",
    "https://openalex.org/W4353113046",
    "https://openalex.org/W4281629162",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W4230262515",
    "https://openalex.org/W4221145545",
    "https://openalex.org/W4385714663",
    "https://openalex.org/W3118999024",
    "https://openalex.org/W4384811608",
    "https://openalex.org/W4385569780",
    "https://openalex.org/W4362598605",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W4313483736",
    "https://openalex.org/W4282980384",
    "https://openalex.org/W4385572666",
    "https://openalex.org/W4377372007",
    "https://openalex.org/W4389518771",
    "https://openalex.org/W4319049323",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4309953708",
    "https://openalex.org/W3153269634",
    "https://openalex.org/W4363675974",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3207024370",
    "https://openalex.org/W4378942569",
    "https://openalex.org/W4285293425",
    "https://openalex.org/W4385571289",
    "https://openalex.org/W4223974161",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W4389520468",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W4385570365",
    "https://openalex.org/W4378508793",
    "https://openalex.org/W3106976604",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W4385569933",
    "https://openalex.org/W4221155916",
    "https://openalex.org/W4319653544",
    "https://openalex.org/W4378508578",
    "https://openalex.org/W4205694376",
    "https://openalex.org/W4281483318",
    "https://openalex.org/W3207976211",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3152884768",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4377864080",
    "https://openalex.org/W4385570481",
    "https://openalex.org/W4389524270",
    "https://openalex.org/W4281485769",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4389520370",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W4386566752",
    "https://openalex.org/W4387225582",
    "https://openalex.org/W4388778348",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W4366566341",
    "https://openalex.org/W3112170794",
    "https://openalex.org/W4306313145",
    "https://openalex.org/W4287890137",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4221164017",
    "https://openalex.org/W4221141423",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4385570086",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3190540921",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4285166440",
    "https://openalex.org/W4385573164",
    "https://openalex.org/W3104215796",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W4385571271",
    "https://openalex.org/W4327487374",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4385574174",
    "https://openalex.org/W4386554558",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4389524066",
    "https://openalex.org/W4389518797",
    "https://openalex.org/W3202099651",
    "https://openalex.org/W4389520380",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4290771878",
    "https://openalex.org/W4386044411",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W4377865309",
    "https://openalex.org/W4385569882",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4315881234",
    "https://openalex.org/W4389523756",
    "https://openalex.org/W4386080925",
    "https://openalex.org/W4385570777",
    "https://openalex.org/W4385572464",
    "https://openalex.org/W4385570864",
    "https://openalex.org/W3003265726",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4389519118",
    "https://openalex.org/W4385572901",
    "https://openalex.org/W4313304841",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W3156476125",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4366328015",
    "https://openalex.org/W4287888899",
    "https://openalex.org/W4389518954",
    "https://openalex.org/W4318142410",
    "https://openalex.org/W3213460052",
    "https://openalex.org/W4281679115",
    "https://openalex.org/W4385573021",
    "https://openalex.org/W4287887895",
    "https://openalex.org/W4320561779",
    "https://openalex.org/W4385571512",
    "https://openalex.org/W4389519586",
    "https://openalex.org/W4380994551",
    "https://openalex.org/W4389518671",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W4389523675",
    "https://openalex.org/W4327810433",
    "https://openalex.org/W4385571050",
    "https://openalex.org/W4378474184",
    "https://openalex.org/W4306808680",
    "https://openalex.org/W4390692489",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4221152111",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4285169833",
    "https://openalex.org/W4388626886",
    "https://openalex.org/W4385567084",
    "https://openalex.org/W4377142519",
    "https://openalex.org/W4385573753",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W4385574113",
    "https://openalex.org/W4323076530",
    "https://openalex.org/W4378711594",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4386235056",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4322825541",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W4287820586",
    "https://openalex.org/W3092288641",
    "https://openalex.org/W4376167603",
    "https://openalex.org/W4385567201",
    "https://openalex.org/W4385570326"
  ],
  "abstract": "Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning deployed LLMs with the ever-changing world knowledge. We categorize research works systemically and provide in-depth comparisons and discussions. We also discuss existing challenges and highlight future directions to facilitate research in this field.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8289–8311\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nHow Do Large Language Models Capture the Ever-changing World\nKnowledge? A Review of Recent Advances\nZihan Zhang1∗, Meng Fang2∗, Ling Chen1, Mohammad-Reza Namazi-Rad3, Jun Wang4\n1University of Technology Sydney 2University of Liverpool\n3University of Wollongong 4University College London\nZihan.Zhang-5@student.uts.edu.au, Meng.Fang@liverpool.ac.uk\nLing.Chen@uts.edu.au, mrad@uow.edu.au, junwang@cs.ucl.ac.uk\nAbstract\nAlthough large language models (LLMs) are\nimpressive in solving various tasks, they can\nquickly be outdated after deployment. Main-\ntaining their up-to-date status is a pressing con-\ncern in the current era. This paper provides\na comprehensive review of recent advances in\naligning LLMs with the ever-changing world\nknowledge without re-training from scratch.\nWe categorize research works systemically and\nprovide in-depth comparisons and discussion.\nWe also discuss existing challenges and high-\nlight future directions to facilitate research in\nthis field 1.\n1 Introduction\nLarge language models (LLMs) (Brown et al.,\n2020; Ouyang et al., 2022; Chowdhery et al.,\n2022; Zhang et al., 2022; OpenAI, 2023b; Tou-\nvron et al., 2023; Anil et al., 2023) trained on mas-\nsive corpora from various sources (e.g., Wikipedia,\nBooks, Github) implicitly store enormous amounts\nof world knowledge in their parameters (Petroni\net al., 2019; Roberts et al., 2020; Jiang et al., 2020),\nenabling them to act as versatile foundation models\nfor performing various natural language process-\ning (NLP) tasks directly through in-context learn-\ning (Liu et al., 2023b; OpenAI, 2023b; Bubeck\net al., 2023; Kamalloo et al., 2023) or for further\nfine-tuning for domain-specific uses (Singhal et al.,\n2022; Google, 2023; Liu and Low, 2023).\nDespite their impressive performance, LLMs are\nstatic after deployment, and there is no mechanism\nto update themselves or adapt to a changing envi-\nronment (Kasai et al., 2022; Bubeck et al., 2023).\nOur world, however, is dynamic and constantly\nevolving. As shown in Fig.1, the static nature of\ntrained LLMs makes the memorized knowledge\n*Equal contribution\n1We release the paper list at https://github.com/\nhyintell/awesome-refreshing-llms and will periodically\nupdate it.\nWho won the World \nCup in Qatar?\n France\nArgentina\nHow?\nWho won the World \nCup in Qatar?\ntime 2021.9 2022⦿\nLLM\nFigure 1: A trained LLM is static and can be out-\ndated (e.g., ChatGPT; OpenAI 2022). How can LLMs\nbe aligned to the ever-changing world knowledge effi-\nciently and effectively?\nquickly obsolete, which often causes hallucinations,\nrendering them unreliable for knowledge-intensive\ntasks (Lazaridou et al., 2022; Luu et al., 2022; Ji\net al., 2023; Si et al., 2023). In the era of LLMs,\nensuring their alignment with the ever-changing\nworld knowledge and maintaining their up-to-date\nstatus after deployment is a pressing concern be-\ncause many users and downstream applications rely\non them. Unfortunately, simply re-training LLMs\nwith the latest information is infeasible due to pro-\nhibitive costs (Patterson et al., 2021).\nIntuitively, to update an LLM, one can either\nreplace the obsolete knowledge stored implicitly\nin the model with new ones by modifying its pa-\nrameters, or override the outdated model outputs\nusing new information explicitly retrieved from the\nworld. Tremendous work has been proposed in\nthe literature to implicitly or explicitly refresh de-\nployed LLMs; however, these approaches, scat-\ntered among various tasks, have not been systemat-\nically reviewed and analyzed.\nIn this review, we survey the recent compelling\nadvances in aligning deployed LLMs with the ever-\nchanging world knowledge. We categorize research\nworks systemically and highlight representative\napproaches in each category (§2) and provide in-\ndepth comparison with discussion for insights (§3).\nLastly, we discuss potential future directions to\n8289\nLLMs align with\never-changing\nworld knowledge\nImplicit\n(§2.1)\nKnowledge\nEditing\nMeta-learning Editable Training (Sinitsin et al., 2020), RECKONING (Chen et al., 2023c)\nHypernetwork\nEditor\nKnowledgeEditor (De Cao et al., 2021), MEND (Mitchell et al., 2022a), SLAG (Hase et al.,\n2023b), REMEDI (Hernandez et al., 2023), Distillation (Padmanabhan et al., 2023)\nLocate\nand edit\nKnowledge Neurons (Dai et al., 2022), ROME (Meng et al., 2022a), MEMIT (Meng et al., 2023),\nMEMITCSK (Gupta et al., 2023a), PMET (Li et al., 2023b), Chen et al. (2023b), Geva et al.\n(2023), KLoB (Ju and Zhang, 2023)\nOther Eva-KELLM (Wu et al., 2023), RippleEdits (Cohen et al., 2023), Wang et al. (2023a), Xu et al.\n(2023b), IKE (Zheng et al., 2023)\nContinual\nLearning\nContinual\nPre-training\nRegularization-\nbased RecAdam (Chen et al., 2020), DSA (Ke et al., 2023)\nReplay-based ELLE (Qin et al., 2022), CT0 (Scialom et al., 2022)\nArchitectural-\nbased\nK-Adapter (Wang et al., 2021), ELLE (Qin et al., 2022), CKL (Jang et al.,\n2022b), CPT (Ke et al., 2022), Lifelong-MoE (Chen et al., 2023a), Module-\nFormer (Shen et al., 2023)\nOther\nLifelong Pre-training (Jin et al., 2022), CKL (Jang et al., 2022b), KILM (Xu\net al., 2023a), SeMem (Peng et al., 2023b), CaMeLS (Hu et al., 2023), Yu\nand Ji (2023), Gupta et al. (2023b)\nContinual\nKnowledge\nEditing\nCMR (Lin et al., 2022), CL-plugin (Lee et al., 2022a), Transformer-Patcher (Huang et al., 2023),\nGRACE (Hartvigsen et al., 2023)\nExplicit\n(§2.2)\nMemory-\nenhanced\nCorpus or\nDocuments\nkNN-LM (Khandelwal et al., 2020), AdaptRet (He et al., 2021a), RetoMaton (Alon et al., 2022),\nBhardwaj et al. (2022), kNN-prompt (Shi et al., 2022), SeMem (Peng et al., 2023b)\nFeedback or\nCorrections\nFBNet (Tandon et al., 2022), MemPrompt (Madaan et al., 2022), TeachMe (Dalvi Mishra et al.,\n2022), SERAC (Mitchell et al., 2022b), MeLLo (Zhong et al., 2023)\nRetrieval-\nenhanced\nSingle-Stage IC-Retrieval (Si et al., 2023), IC-RALM (Ram et al., 2023), AAR (Yu et al., 2023b), IKE (Zheng\net al., 2023), Adaptive Retrieval (Mallen et al., 2023), RePlug (Shi et al., 2023b)\nMulti-Stage\nIRCoT (Trivedi et al., 2022), RARR (Gao et al., 2023), Self-Ask (Press et al., 2023), DecomP\n(Khot et al., 2023), ReAct (Yao et al., 2023a), ART (Paranjape et al., 2023), LLM-Augmenter\n(Peng et al., 2023a), DSP (Khattab et al., 2023), Iter-RetGen (Shao et al., 2023), Knowledge\nSolver (Feng et al., 2023)\nInternet-\nenhanced\nInternet-Fewshot (Lazaridou et al., 2022), LLM-URL (Ziems et al., 2023), TaskMatrix.AI (Liang et al., 2023), MM-\nREACT (Yang et al., 2023), Chameleon (Lu et al., 2023), ChatGPT Plugins (OpenAI, 2023a)\nFigure 2: Taxonomy of methods to align LLMs with the ever-changing world knowledge (due to space limitation,\nplease refer to Appendix A.2 for a complete review). Implicit means the approaches seek to directly alter the\nknowledge stored in LLMs ( e.g., parameters) (§2.1), while Explicit means more often incorporating external\nresources to override internal knowledge (e.g., search engine) (§2.2).\nfacilitate research in this field (§4).\nComparison with Related Work To the best of\nour knowledge, surveys on this topic are scarce.\nClosest to our work, AlKhamissi et al. (2022) re-\nview pre-trained language models (LMs) as knowl-\nedge bases (KBs) and review a set of aspects that\na LM should have to fully act as a KB; Cao et al.\n(2023) further divide the life cycle of knowledge in\nLLMs into five periods and survey how knowledge\ncirculates; Yao et al. (2023b) conduct an empiri-\ncal analysis of existing knowledge editing meth-\nods. Despite partially overlapping with our dis-\ncussion of knowledge editing in §2.1.1, they only\ntouch a subset of the scope that our survey stud-\nies and ignore other potentials in aligning LLMs\nwith the world knowledge. Mialon et al. (2023);\nWang et al. (2023b); Qin et al. (2023b) study aug-\nmented, interactive, and tool learning of LLMs\nrespectively, which share different goals from ours.\nPrevious knowledge-enhanced LMs surveys (Zhu\net al., 2021; Wei et al., 2021; Yu et al., 2022; Yin\net al., 2022; Zhen et al., 2022) focus on injecting\nknowledge into LMs, typically requiring modify-\ning the model’s architecture or re-training. Instead,\nwe focus on the potential of how deployed LLMs\ncapture the ever-changing world knowledge effec-\ntively and efficiently without re-training. Wang\net al. (2023c) provide a comprehensive review of\nforgetting in deep learning that is not limited to\ncontinual learning. Pan et al. (2023) review poten-\ntial approaches that unify knowledge graphs (KGs)\nand LLMs. While structural knowledge, such as\nKGs, can broadly be categorised as explicit knowl-\nedge and augmented to LLMs for new knowledge,\nKG is static after creation and can still be outdated\n(Ji et al., 2022). New information or discoveries\nnot yet incorporated into KGs may lead to outdated\nknowledge. However, how to efficiently update\nKGs is out of the scope of this survey.\n2 Taxonomy of Methods\nBased on whether the method tends to directly al-\nter the knowledge stored implicitly in LLMs, or\nleverage external resources to override the outdated\nknowledge, we roughly categorize them as implicit\n(§2.1) or explicit (§2.2) approaches. Fig.2 provides\na summary of representative works from each cate-\n8290\ngory (See Fig.6 in Appendix for a complete review).\nDetailed descriptions of the methods can be found\nin Appendix A.1.\n2.1 Implicitly Align LLMs with World\nKnowledge\nPrevious studies have shown that LLMs can implic-\nitly memorize knowledge in their large number of\nparameters after being pre-trained on massive cor-\npora (Petroni et al., 2019; Roberts et al., 2020; Jiang\net al., 2020; Singhal et al., 2022). To keep LLMs\nup-to-date and align with the current world knowl-\nedge, the straightforward way is to alter the model’s\nbehaviour from themselves to generate desired out-\nputs. Naively, one can regularly re-train the model\nfrom scratch or fine-tune the model with the lat-\nest corpora to align with current world knowledge.\nHowever, re-training is expensive and environmen-\ntally unfriendly (Patterson et al., 2021), especially\nin the era of LLMs with billions of parameters.\nFine-tuning without constraints may have a \"but-\nterfly effect\" and affect other knowledge or skills\npresent in the model (Kirkpatrick et al., 2017; Li\net al., 2022; AlKhamissi et al., 2022). To cope with\nthis issue, this line of work aims to design better\nstrategies to modify the internal states of LLMs in\na more controllable and efficient way, which can\nbe categorized into knowledge editing (§2.1.1) and\ncontinual learning (§2.1.2).\n2.1.1 Knowledge Editing\nSince tuning LLMs to learn new knowledge can be\nprohibitively expensive (Patterson et al., 2021), re-\nsearchers seek efficient methods to directly update\nmore specific, localized, or fine-grained knowledge\nthat is preserved in LLMs (Mitchell et al., 2022a).\nKnowledge editing (KE) is an arising and promis-\ning research area that aims to alter the parameters\nof some specific knowledge stored in pre-trained\nmodels so that the model can make new predictions\non those revised instances while keeping other irrel-\nevant knowledge unchanged (Sinitsin et al., 2020;\nDe Cao et al., 2021; Mitchell et al., 2022a; Meng\net al., 2022a; Hase et al., 2023b; Meng et al., 2023).\nIn this section, we categorize existing methods into\nmeta-learning, hypernetwork, and locate-and-edit\n-based methods.\nMeta-learning. This line of work generally fo-\ncuses on the intrinsic editability of the model itself,\naiming to modify the model parameters so that they\ncan be easily updated during inference (De Cao\net al., 2021; Mitchell et al., 2022a). Sinitsin et al.\n(2020) propose a model-agnostic meta-learning-\nbased (Finn et al., 2017) method that trains neural\nnetworks in a way that the trained parameters can\nbe easily edited afterwards. Chen et al. (2023c) in-\ntroduce a two-loop framework. In the inner training\nloop, they employ a few gradient updates to enable\na pre-trained GPT-2 model (Radford et al., 2019)\nto efficiently memorize external knowledge. Sub-\nsequently, in the outer loop, the model parameters\nare dynamically adjusted through optimal meta-\nparameter learning to incorporate additional knowl-\nedge that aids reasoning tasks.\nHypernetwork Editor. In contrast to pre-\nmodifying the pre-trained language model, an al-\nternative approach in the field involves training\nextrinsic editors that update knowledge during test\ntime, thereby avoiding any modifications to the\nbase model. De Cao et al. (2021) reframe editing\nthe knowledge of a model as a learning-to-update\nproblem. Specifically, given a single data instance\nthat needs to be updated, their trained hypernet-\nwork (Ha et al., 2017) predicts a shift ∆θ such that\nθ′ = θ + ∆θ, where θ is the original pre-trained\nLM weights and θ′is the updated weights. To keep\nediting effective while being easy to scale to larger\nLMs with billions of parameters, Mitchell et al.\n(2022a) decompose weight updates into low-rank\ncomponents (Hu et al., 2022), thus making it possi-\nble to scale to LLMs. Orthogonal to Mitchell et al.\n(2022a), Hase et al. (2023b) introduce a new train-\ning objective considering sequential, local, and gen-\neralizing model updates. Although scaled beyond\na single edit, their edit success rate significantly\ndegrades when performing larger edits simultane-\nously. Unlike the above methods that operate on\nthe model’s weight, Hernandez et al. (2023) per-\nform edits on the representation level. Padmanab-\nhan et al. (2023) employ knowledge distillation to\ntransfer knowledge generated from a teacher model\nto a student model.\nLocate and Edit. Generally, this line of work\nadopts the locate and edit pattern: they first iden-\ntify the location of specific knowledge stored in the\nmodel via different assumptions, then directly mod-\nify the weights or representations to update knowl-\nedge. Inspired by the findings that feed-forward\nnetworks (FFN) in Transformer (Vaswani et al.,\n2017) are key-value memories (Geva et al., 2021),\nDai et al. (2022) introduce the knowledge neurons\n8291\ntime 2021\n(Deployed Date) 2022\n⦿ ⦿ ⦿\n2023\nLLM\n(UK PM, is, Liz Truss) \n(US President, is, Joe Biden)\n \nLLM\n(UK PM, is, Rishi Sunak) \n(World Cup, win, Argentina)\n \nLLM\nLLM\n LLM\n LLM\nWho won the World \nCup in Qatar?\nLLM\nArgentina\nWho won the World \nCup in Qatar?\nLLM\n Argentina\nLLM\n LLM\nWho won the World \nCup in Qatar?\nArgentina\nKnowledge \nEditing \nContinual \nLearning\nRetrieval-\nbased\nFigure 3: A high-level comparison of different approaches.\nconcept and propose a gradient-based knowledge\nattribution method to identify these knowledge neu-\nrons in FFNs. Further, without fine-tuning, they\ndirectly modify the corresponding value slots (e.g.,\nembeddings) in the located knowledge neurons and\nsuccessfully update or delete knowledge, demon-\nstrating a preliminary potential to edit knowledge\nin LMs.\nDifferent from Geva et al. (2021)’s per-neuron\nview, Meng et al. (2022a) conduct casual tracing\nanalysis on GPT-2 and hypothesize that the Trans-\nformer MLP can be viewed as a linear associa-\ntive memory. They verify their hypothesis by di-\nrectly updating the middle-layer MLP weights with\na rank-one update (Bau et al., 2020). Following\nMeng et al. (2022a)’s work, Meng et al. (2023)\npropose a scalable multi-layer method to update an\nLLM with thousands of facts simultaneously, sig-\nnificantly improves editing efficiency while main-\ntaining generalization and specificity. Gupta et al.\n(2023a) further adapt it to fix commonsense mis-\ntakes. Li et al. (2023b) find that Multi-Head Self-\nAttention (MHSA) weights do not require updating\nwhen introducing new knowledge. Based on this,\nthey propose precisely updating FFN weights by\nsimultaneously optimizing the Transformer compo-\nnent hidden states of MHSA and FFN to memorize\ntarget knowledge. Chen et al. (2023b) propose an\narchitecture-adapted multilingual integrated gradi-\nents method to localize knowledge neurons pre-\ncisely across multiple architectures and languages.\nGeva et al. (2023) analyze the internal recall pro-\ncess of factual associations in auto-regressive LMs,\nopening new research directions for knowledge lo-\ncalization and model editing.\nOther. Wu et al. (2023) propose an evaluation\nframework and dataset for measuring the effective-\nness of knowledge editing of LLMs, as well as the\nability to reason with the altered knowledge and\ncross-lingual knowledge transfer. Similarly, Cohen\net al. (2023) evaluate the implications of an edit on\nrelated facts and show that existing methods fail to\nintroduce consistent changes in the model’s knowl-\nedge. Ju and Zhang (2023) propose an evaluation\nbenchmark for locate-and-edit-based methods, aim-\ning to reassess the validity of the locality hypothesis\nof factual knowledge. Wang et al. (2023a) and Xu\net al. (2023b) take multilingual into account and\nextend existing knowledge editing methods into\ncross-lingual scenarios.\n2.1.2 Continual Learning\nContinual learning (CL) aims to enable a model to\nlearn from a continuous data stream across time\nwhile reducing catastrophic forgetting of previously\nacquired knowledge (Biesialska et al., 2020). With\nCL, a deployed LLM has the potential to adapt to\nthe changing world without costly re-training from\nscratch (Bubeck et al., 2023). In this section, we\nintroduce approaches that employ CL for aligning\nLLMs with the current world knowledge, including\ncontinual pre-training and continual knowledge\nediting.\n8292\nContinual Pre-training. Unlike traditional con-\ntinual learning, which sequentially fine-tunes a\npre-trained LM on some specific downstream\ntasks (e.g., QA, text classification), continual pre-\ntraining is used to further pre-train an LM to ac-\nquire new knowledge, where the data corpus is\nusually unsupervised (Gururangan et al., 2020; Ke\nand Liu, 2023). Since our target is the versatile\nfoundation LLMs (e.g., GPT-4) that can be applied\nto many different use cases rather than a fine-tuned\nmodel designed for a specific task, we focus on the\nliterature on continual pre-training.\nEarly works (Gururangan et al., 2020; Röttger\nand Pierrehumbert, 2021; Lazaridou et al., 2021;\nDhingra et al., 2022) empirically analyze continu-\ning LM pre-training on emerging domain or tem-\nporal data, showing the potential to update the\nbase LM with new knowledge. Jang et al. (2022b)\nexplicitly categorize world knowledge as time-\ninvariant, outdated, and new knowledge, which\nshould be retained, acquired, and updated respec-\ntively by an LM when learning continually. Jin\net al. (2022); Jang et al. (2022a,b) additionally im-\nplement traditional CL methods to alleviate catas-\ntrophic forgetting, a phenomenon in which previ-\nously learned knowledge or abilities are degraded\ndue to overwritten parameters (Kirkpatrick et al.,\n2017). Among the literature, CL methods can be\nmainly categorized into 1 Regularization, 2 Re-\nplay, and 3 Architectural -based methods.\n1 Regularization. To mitigate forgetting,\nregularization-based methods apply regulations to\npenalize the changes of the critical parameters\nlearned from previous data. Chen et al. (2020)\nimprove the traditional EWC (Kirkpatrick et al.,\n2017) by recalling previously learned knowledge\nthrough the pre-trained parameters, and the method\ncontinually learns new information using a multi-\ntask learning objective. Ke et al. (2023) compute\nthe importance of each unit (i.e., attention head and\nneuron) to the general knowledge in the LM us-\ning a proxy based on model robustness to preserve\nlearned knowledge. When continually learning new\ndomains, the approach prevents catastrophic for-\ngetting of the general and domain knowledge and\nencourages knowledge transfer via soft-masking\nand contrastive loss.\n2 Replay. These methods generally reduce\nforgetting by replaying previous training data when\nlearning new data. Assuming that the initial pre-\ntraining corpus is available, He et al. (2021b) use\na gradual decay mix-ratio to adjust the quantity\nof the pre-training corpus mixed in the new data\nwhen learning sequentially. ELLE (Qin et al., 2022)\nand CT0 (Scialom et al., 2022) also mix the old\ndata while learning new data. However, ELLE\nstarts the pre-training from a newly initialized and\nrelatively small BERT (Devlin et al., 2019) and\nGPT (Radford et al., 2018), while CT0 continues\nlearning from T0-3B (Sanh et al., 2022), a pre-\ntrained and instruction-tuned model.\n3 Architectural. These methods typically al-\nleviate forgetting by using different subsets of pa-\nrameters for distinct tasks or domains. Wang et al.\n(2021); Hu et al. (2022); Ke et al. (2022) freeze\nthe original parameters of the LM to preserve the\nlearned knowledge and add lightweight tunable pa-\nrameters for continually learning new knowledge.\nWang et al. (2021) add separate adapters (Houlsby\net al., 2019) for each new task, while Ke et al.\n(2022) let all domains share adapters and employ\ntask masks to protect critical neurons from being\nupdated. DEMix-DAPT (Gururangan et al., 2022)\nreplaces every FFN layer in Transformer with a\nseparate domain expert mixture layer, containing\none expert per domain. When learning new knowl-\nedge, they only train the newly added expert in\neach DEMix layer while fixing all other experts.\nSimilarly, Lifelong-MoE (Chen et al., 2023a) pro-\ngressively expands experts to increase model ca-\npacity for learning new knowledge, and mitigates\nforgetting by freezing previously trained experts\nand gatings with output-level regularization. Qin\net al. (2022) enlarge the model’s width and depth\nto attain learning efficiency and employ memory\nreplay to reduce forgetting.\n4 Other Methods. Hu et al. (2023) meta-trains\nan importance-weighting model to reweight the per-\ntoken loss of the continual data stream, intending to\nquickly adapt the base LM to new knowledge. Peng\net al. (2023b) apply kNN-LM (Khandelwal et al.,\n2020) to continual learning from streaming data\nand selectively store hard cases in a non-parametric\nmemory, significantly improving the data-wise and\nmodel-wise scalability. Yu and Ji (2023) assess\nself-information-update in LLMs via CL and miti-\ngate exposure bias by incorporating the selection\nof relevant facts into training losses.\nContinual Knowledge Editing. Lin et al. (2022);\nLee et al. (2022a); Huang et al. (2023) and\nHartvigsen et al. (2023) propose a more realistic\nsetting that a deployed LM should be constantly\n8293\nCategory Representative Method Base LM LM Params Augmentation No\nTraining\nBlack\n-box\nMEND (Mitchell et al., 2022a) T5 (11B) /snowflakeauxiliary model ✗ ✗\nROME (Meng et al., 2022a) GPT-J (6B) ὒ5 – ✔ ✗\nCaliNET (Dong et al., 2022) T5 (0.7B) /snowflake+params ✗ ✗\nMEMIT (Meng et al., 2023) GPT-NeoX (20B) ὒ5 – ✔ ✗\nK-Adapter (Wang et al., 2021) RoBERTa (0.3B) /snowflake+params ✗ ✗\nCT0 (Scialom et al., 2022) T0 (3B) ὒ5 memory ✗ ✗\nDSA (Ke et al., 2023) RoBERTa (0.1B) ὒ5 – ✗ ✗\nMemPrompt (Madaan et al., 2022) GPT-3 (175B) /snowflakememory+retriever ✔ ✔\nSERAC (Mitchell et al., 2022b) T5 (0.7B) /snowflakememory\n+auxiliary model ✗ ✔\nMeLLo (Zhong et al., 2023) GPT-3.5 (175B) /snowflakememory+retriever ✔ ✔\nIRCoT (Trivedi et al., 2022) GPT-3.5 (175B) /snowflakeretriever ✔ ✔\nRARR (Gao et al., 2023) PaLM (540B) /snowflakesearch engine\n+auxiliary model ✔ ✔\nDecomP (Khot et al., 2023) GPT-3 (175B) /snowflakeretriever ✔ ✔\nReAct (Yao et al., 2023a) PaLM (540B) /snowflakesearch engine ✔ ✔\nFLARE (Jiang et al., 2023) GPT-3.5 (175B) /snowflakeretriever/search engine ✔ ✔\nLazaridou et al. (2022) Gopher (280B) /snowflakesearch engine ✔ ✔\nCRITIC (Gou et al., 2023) GPT-3.5 (175B) /snowflakevarious tools ✔ ✔\nChameleon (Lu et al., 2023) GPT-4 (?B) /snowflakevarious tools ✔ ✔\nKnowledge Editing\nContinual Learning\nMemory-enhanced\nRetrieval-enhanced\nInternet-enhanced\nTable 1: Comparison between representative methods (refer to Appendix A.2 for a complete review). ὒ5means the\nparameters of the original LM are modified, while /snowflakemeans they are unchanged; Augmentation means additional\ncomponents used; No Training indicates the method does not require additional training; Black-box refers to\nwhether the method suits non-publicly available models (e.g., no model architecture, parameters, activations, or\ngradients are available).\ncorrected to fix its prediction errors, showing the\npotential to align the model with the latest world\nknowledge. Lin et al. (2022) benchmark the con-\ntinual model refinement problem by implementing\ntraditional CL methods. Lee et al. (2022a) and\nHartvigsen et al. (2023) freeze the LM’s original\nparameters and continually introduce trainable neu-\nrons to the FFN layer to rectify problematic model\nbehaviors. In contrast, Hartvigsen et al. (2023)\nlearn to cache a chosen layer’s activations in a\nkey-value-based codebook and retrieve activations\nwhen previous similar edits have been performed.\nWithout influencing unrelated inputs, it can effi-\nciently edit the model thousands of times in a row\nwhile generalizing edits to previously unseen in-\nputs.\n2.2 Explicitly Align LLMs with World\nKnowledge\nAlthough altering the knowledge implicitly stored\nin LLMs has shown to be effective (Jang et al.,\n2022b; Meng et al., 2023), it remains unclear\nwhether it will affect the models’ general abilities\ndue to the complexity of neural networks. In con-\ntrast, explicitly augmenting LLMs with the latest\ninformation retrieved from various sources can ef-\nfectively adapt the models to new world knowledge\nwithout affecting the original LLMs (Mialon et al.,\n2023). However, previous retrieval-augmented\nmethods (Karpukhin et al., 2020; Guu et al., 2020;\nLewis et al., 2020; Izacard et al., 2022; Borgeaud\net al., 2022; Jiang et al., 2022; Kaur et al., 2022)\nusually jointly train a retriever and an LM in an end-\nto-end fashion, making it challenging to apply to a\ndeployed LLM (e.g., GPT-3). Recently, researchers\nhave focused on equipping a fixed LLM with ex-\nternal memory (memory-enhanced; §2.2.1), an off-\nthe-shelf retriever (retrieval-enhanced; §2.2.2), or\nInternet (Internet-enhanced; §2.2.3) to cope with\nthis issue.\n2.2.1 Memory-enhanced Methods\nPairing a static LLM with a growing non-\nparametric memory enables it to capture informa-\ntion beyond its memorized knowledge during infer-\nence (Wu et al., 2022). The external memory can\nstore a recent corpus or feedback that contains new\ninformation to guide the model generation.\nStoring Corpus or Documents. kNN-LM\n(Khandelwal et al., 2020) stores every <context,\ntoken> as key-value pairs from a corpus in mem-\nory. During inference, it calculates the probabil-\nity of the next token by interpolating a fixed LM\nwith a distribution retrieved from the k nearest to-\nkens in the memory. Following this vein, He et al.\n(2021a); Drozdov et al. (2022); Alon et al. (2022)\nimprove the efficiency of kNN-LM by skipping\n8294\nWho won the World \nCup in Qatar?\nArgentina\nHow many champions does \nthe winner of the Qatar \nWorld Cup have in total \nthroughout history?\n3 times\nArgentina is the winner \nof the Qatar World \nCup...How many times…\nSingle-Stage Multi-Stage\nLLM\nLLM\nLLM\nFigure 4: Single-Stage (left) typically retrieves once,\nwhile Multi-Stage (right) involves multiple retrievals\nor revisions to solve complex questions (§2.2.2).\nunnecessary retrieval. Meng et al. (2022b) build\nan additional graph neural network to aggregate\ninformation from the retrieved context for better\ngeneration. Peng et al. (2023b) improve the scal-\nability of kNN-LM for continual learning, while\nShi et al. (2022) apply it for zero-shot inference on\ndownstream tasks.\nStoring Feedback or Corrections. Inspired by\nthe fact that humans can learn from past mistakes,\nthis line of work stores user feedback in memory to\nfix the model’s problematic predictions and avoids\nsimilar errors in the future. By querying the mem-\nory, the base LLM gains editability to update its\noutdated knowledge. Kassner et al. (2021); Tandon\net al. (2022) train an auxiliary corrector to apply\nfeedback to repair the model output. Dalvi Mishra\net al. (2022) allow users to interact with the system\nto check its facts and reasoning and correct it when\nit is wrong. Similarly, Madaan et al. (2022) equip\nGPT-3 with a growing memory, where the key is\na misunderstanding question, and the value is the\ncorrective feedback. Instead of storing user feed-\nback, Mitchell et al. (2022b); Zhong et al. (2023)\nexplicitly preserve updated knowledge in memory.\nGiven an input, Mitchell et al. (2022b) first apply a\nclassifier to determine if a relevant edit exists in the\nmemory and perform knowledge updating through\na counterfactual model. Conversely, Zhong et al.\n(2023) decompose complex questions and ask the\nbase model to generate a temporary answer. They\nrevise the model output when the generated answer\ncontradicts the retrieved facts from memory.\n2.2.2 Retrieval-enhanced Methods\nLeveraging an off-the-shelf retriever and the in-\ncontext learning ability of LLMs (Brown et al.,\n2020), this line of work designs better retrieval\nstrategies to incorporate world knowledge into a\nfixed LLM through prompting, which can be di-\nvided into single-stage and multi-stage (Fig.4).\nSingle-Stage. To ground the model with external\nknowledge during generation, Ram et al. (2023); Si\net al. (2023) adopt zero-shot and few-shot retrieval\nrespectively and directly prepend the retrieved doc-\numents to the input without changing the base LLM.\nZheng et al. (2023) retrieve similar edit demon-\nstrations for each input and perform in-context\nknowledge editing. Compared with gradient-based\nknowledge editing (§2.1.1), they have competitive\nediting performance with fewer side effects. Ar-\nguing that the general-purpose retrievers could be\nsub-optimal, Yu et al. (2023b) adopt a small source\nLM to provide LM-preferred signals to train an\nadaptive retriever. Mallen et al. (2023) employ a\nheuristic based on entity popularity and only re-\ntrieve relevant context when the input questions are\nless popular, which improves performance and re-\nduces inference costs. Unlike above, to address the\nlimited model’s context length, Shi et al. (2023b)\nprepend each retrieved document separately to an\nLLM and then ensemble output probabilities from\ndifferent passes.\nMulti-Stage. When solving complex questions,\nretrieving information only once based on the in-\nput is often inadequate. This branch of work aims\nto transform single-stage retrieval into multi-stage\nretrieval in order to solve complex tasks, usually\nby leveraging reasoning. Trivedi et al. (2022) inter-\nleave knowledge retrieval with chain-of-thoughts\n(CoT; Wei et al. 2022) generation to solve complex\nmulti-step reasoning questions. Similarly, Press\net al. (2023); Khot et al. (2023); Yao et al. (2023a);\nJiang et al. (2023); Shao et al. (2023) decompose\nquestions into sub-questions to provide a specific\ncontext for retrieval with model generation. Paran-\njape et al. (2023); Chen et al. (2023d); Inaba et al.\n(2023) further enable the usage of different tools\nto solve various tasks. Unlike the simple retrieve-\nthen-read paradigm, Khattab et al. (2023) pass in-\ntermediate messages between an LLM and a re-\ntriever; Gao et al. (2023); He et al. (2022); Zhao\net al. (2023); Yu et al. (2023a) retrieve after gen-\neration and perform post-edit revisions for more\nfaithful outputs. Peng et al. (2023a) iteratively\nrevise ChatGPT to improve model responses us-\ning feedback and external knowledge. Feng et al.\n8295\n(2023) teach LLMs themselves to search for knowl-\nedge from external knowledge graphs (KGs) via\nprompting and simplify searching into a multi-hop\ndecision sequence, allowing explainable decision-\nmaking of the processes.\n2.2.3 Internet-enhanced Methods\nPrior retrieval-augmented work relies on static or\noffline knowledge sources (e.g., Wikipedia dump),\nwhich may not be sufficiently up-to-date or com-\nplete for tasks that require the latest knowledge\n(Kasai et al., 2022; Zhang et al., 2023; Li et al.,\n2023a). A recent trend uses the whole web as\nthe knowledge source and equips LLMs with the\nInternet to support real-time information seeking\n(Nakano et al., 2022; Menick et al., 2022; Komeili\net al., 2022; Shuster et al., 2022; Qin et al., 2023a;\nLiu et al., 2023a). Lazaridou et al. (2022) augment\nfew-shot QA prompting with the context retrieved\nfrom Google search. Press et al. (2023); Jiang et al.\n(2023) interleave reasoning with web search. Re-\ncently, tools such as LangChain (Chase, 2022) and\nChatGPT Plugins (OpenAI, 2023a) connect a de-\nployed LLM to the Internet without training, mak-\ning them more powerful for solving knowledge-\nintensive tasks. Beyond search engines, Yao et al.\n(2023a); Liang et al. (2023); Paranjape et al. (2023);\nYang et al. (2023); Gou et al. (2023); Lu et al.\n(2023) treat LLMs as central planners and compose\nvarious plug-and-play tools for solving complex\nquestions.\n3 Comparison and Discussion\nWe present the comparison of different methods\nin Table 1 and in Fig.3, and the characteristics of\ndifferent methods in Table 2 in Appendix.\nDiscussion of Implicit Methods (§2.1). Com-\npared to naive re-training or fine-tuning, KE and\nCL can effectively update obsolete knowledge in\nLLMs while minimizing interference on irrelevant\nones. We identify their major differences: 1\nScale. Existing KE methods focus on updating\nsmall-scale and localized knowledge, typically on\nsynthetic fact pairs (Mitchell et al., 2022a; Meng\net al., 2022a). While one can perform thousands of\nedits simultaneously (Meng et al., 2023), updating\nenormous knowledge in LLMs may be cumber-\nsome. In contrast, CL enhances models’ adaptabil-\nity via tuning larger-scale parameters, thus updat-\ning more knowledge at scale (Jang et al., 2022b).\nHowever, KE provides fine-grained controllability\nwhen specific knowledge needs to be altered, which\nis unachievable by CL; 2 Forgetting. Applying\nKE methods on LLMs frequently in response to the\never-changing world is sub-optimal due to catas-\ntrophic forgetting (Huang et al., 2023; Hartvigsen\net al., 2023); CL mitigates this issue when learning\nnew knowledge; 3 Cost. CL is generally more\ncomputationally expensive than KE due to larger-\nscale weight updating.\nDiscussion of Explicit Methods (§2.2). Explicit\nmethods use new knowledge retrieved from the\nworld to override old knowledge in an LLM dur-\ning generation. Despite being effective, memory-\nand retrieval-enhanced methods must periodically\nmaintain the external memory and the knowledge\nsources in response to the ever-changing world (Ka-\nsai et al., 2022). Conversely, Internet-enhanced\nmethods enable real-time knowledge seeking, al-\nthough potentially suffering from noisy and low-\nquality web content (Li et al., 2023a; Luo et al.,\n2023). Compared to single-stage retrieval, multi-\nstage retrieval can solve more complex problems.\nNevertheless, they may interrupt the generation\nwith multiple retrievals or revisions, leading to con-\nsiderable inference overheads (Shao et al., 2023).\nUpdating LLMs Implicitly or Explicitly? We\nobserve an increasing trend of explicitly aligning\nLLMs with world knowledge while keeping the\nmodel untouched (Table 3 in Appendix). Com-\npared to explicit approaches: 1 Applicability. Im-\nplicit methods usually require modifying LLM’s\nparameters or gradients, making it challenging to\nupdate closed-source models; 2 Side Effects. Al-\nthough constraints have been added to avoid edit-\ning irrelevant knowledge (Mitchell et al., 2022a;\nMeng et al., 2023) or forgetting general knowl-\nedge (Jang et al., 2022b), modifying the LLM’s\nparameters inevitably has side effects that may hurt\nthe performance, which is hard to estimate due to\nthe complexity of neural networks (Brown et al.,\n2023); 3 Efficiency. Implicit methods typically\nrequire training, while most explicit methods lever-\nage a fixed LLM and an off-the-shelf retriever, eras-\ning the necessity of training. However, explicit\nmethods do not directly modify the intrinsic knowl-\nedge within LLMs, but instead rely on on-the-fly\nretrieval during inference, resulting in a notable\nincrease in the computational cost of inference.\n8296\n4 Challenges and Future Directions\nRobust and Efficient Knowledge Editing. KE\noffers fine-grained knowledge updating, which is\ndesirable in some scenarios. Despite promising,\nKE is still in its infancy stage. 1 Various knowl-\nedge. It is challenging to renew the internal knowl-\nedge stored in the parameters of LLMs, and ex-\nisting efforts have only explored updating rela-\ntional knowledge while overlooking other knowl-\nedge (Meng et al., 2023); 2 Edit dataset. Current\nKE methods assume edited knowledge pairs exist,\nwhich must be annotated beforehand. In reality,\nhow do LLMs know what knowledge is outdated\nand thus needs to be updated (Zhang and Choi,\n2023; Yin et al., 2023)? 3 Memorization mech-\nanism. Hase et al. (2023a) argue that the localiza-\ntion of specific knowledge via casual tracing may\nnot be reliable, calling for a better understanding\nof the internal memorization of LLMs (Tirumala\net al., 2022; Carlini et al., 2023); 4 Generaliza-\ntion. Recent studies (Onoe et al., 2023; Zhong\net al., 2023) find that existing KE methods show\nlittle propagation of edited knowledge, meaning\nthe LLM cannot make further reasoning based on\nthe newly acquired knowledge; 5 Effectiveness\nand efficiency. Although early efforts have been\nmade (Hernandez et al., 2023; Huang et al., 2023;\nHartvigsen et al., 2023), methods to effectively, ef-\nficiently, and continually renew the knowledge of\nLLMs at scale have yet to be thoroughly explored.\nEfficient Continual Learning of LLMs. A con-\ntinual pre-trained LLM can update its internal\nknowledge and adapt to the changing world, but\nmaintaining the general knowledge required for\ndownstream tasks without forgetting is challeng-\ning (Ke and Liu, 2023). Moreover, existing meth-\nods are limited to small-scale LMs, leaving CL of\nLLMs rarely studied. While parameter-efficient\ntuning (Ding et al., 2022) may be beneficial, it re-\nmains under-explored to align an LLM with the\ndynamic world via CL.\nSolving Knowledge Conflicts. Replacing old\nknowledge with new ones can cause knowledge\nconflicts regardless of using implicit or explicit\nmethods. For implicit methods, these side effects\nare only evaluated in specific settings, and there\nis no idea of how the general skills of LLMs are\nimpacted (Brown et al., 2023). For retrieval-based\nmethods, knowledge retrieved from the world can\ncontradict the knowledge memorized inside LLMs,\nand LLMs sometimes favour their internal knowl-\nedge rather than the provided context during gen-\neration (an example in Fig.5; Neeman et al. 2022;\nLi et al. 2022; Chen et al. 2022). While initial at-\ntempts have been made (Mallen et al., 2023; Zhou\net al., 2023; Xie et al., 2023), they are still limited.\nRobust and Efficient Retrieval. Interacting with\nexternal resources can cause interruptions during\ngeneration, significantly increasing inference over-\nheads, especially for multi-stage methods that in-\nvolve multiple retrievals or revisions. Potential\nremedies may be efficient memory management\n(Peng et al., 2023b; Kang et al., 2023; Cheng et al.,\n2023) or selective retrieval that only consults exter-\nnal resources when necessary (Mallen et al., 2023).\nOn the other hand, the retrieved context can be ir-\nrelevant and noisy, which may distract LLMs (Shi\net al., 2023a; Luo et al., 2023), or too long, which\nexceeds the input limits and renders high cost (Shi\net al., 2023b).\nComprehensive Evaluation and Benchmarks.\nAlthough approaches of different categories can\nalign the trained LLMs with the changing world\nwithout re-training, their effectiveness is primarily\nevaluated on synthetic datasets in specific settings,\nwhich might not be comprehensive (Jang et al.,\n2022a,b; Hoelscher-Obermaier et al., 2023). More-\nover, although efforts have been made to evaluate\nKE (Wu et al., 2023; Cohen et al., 2023; Ju and\nZhang, 2023), there is no quantitative comparison\nof methods of different categories (i.e., comparing\nKE vs. CL vs. retrieval-based methods), hinder-\ning their application in different scenarios. Lastly,\nexisting benchmarks are too static to measure the\ndynamic world, which calls for real-time evalua-\ntion benchmarks (Liška et al., 2022; Kasai et al.,\n2022).\n5 Conclusion\nIn this paper, we systematically review recent ad-\nvances in aligning LLMs with the ever-changing\nworld knowledge without re-training. We sum-\nmarize existing approaches and categorize them\nbased on whether they tend to directly alter the\nknowledge stored implicitly in LLMs, or leverage\nexternal resources to override the outdated knowl-\nedge. We comprehensively compare methods of\ndifferent categories and point out challenges and\nfuture directions to facilitate research in this area.\n8297\nLimitations\nIn this paper, we systematically review recent ad-\nvances in aligning LLMs with the ever-changing\nworld knowledge without re-training. We compare\nour work with the related surveys in §1 and will pe-\nriodically add related approaches. Despite our best\nefforts, there exist some limitations in this paper:\nScope. In this survey, we do not review\nknowledge-enhanced approaches that require re-\ntraining because we focus on the already trained\n(deployed) models and how to keep them up-to-\ndate. We refer interested readers to the relevant\nknowledge-enhanced LMs surveys (Zhu et al.,\n2021; Wei et al., 2021; Yu et al., 2022; Yin et al.,\n2022; Zhen et al., 2022). Second, in terms of world\nknowledge, we focus on text-based knowledge and\nleave other kinds of knowledge, such as images,\nvideo, audio, etc., and structural knowledge, such\nas knowledge graphs (KGs) and databases, for fu-\nture work. Third, we mainly review the cutting-\nedge approaches within three years (mostly in 2022\nand 2023) in §2, mainly from the ACL, EMNLP,\nNAACL, TACL, NeurIPS, ICML, ICLR, arXiv. De-\nspite our best efforts, by no means the surveyed\nmethods are complete, and we may miss some im-\nportant references. Lastly, we cannot afford all\nthe technical details due to page limitations and\nmay only provide brief introductions. We provide\nadditional discussion of approaches in Appendix\nA.1.\nTaxonomy. It should be noted that some ap-\nproaches are hybrid and can be categorized into\ndifferent branches. We mainly categorize them\nbased on their main components or mechanism.\nFor instance, all methods in §2.2 require retrieving\nfrom external resources. Memory-enhanced meth-\nods (§2.2.1) pay more attention to the design of\nexternal memory, while paying little attention to\nretrieval strategies.\nEmpirical Comparison. We provide detailed\ncomparisons and discussions in §3 and potential\nfuture directions in §4. All the conclusions are\nproposed based on empirical summarization of ex-\nisting works. However, as the field evolves fast,\nthese empirical conclusions might be inapplicable.\nWe will update the latest opinions timely. In addi-\ntion, we do not provide quantitative comparisons\nthrough experiments since there is no unified eval-\nuation benchmarks of different categories. Quan-\ntitative evaluation (benchmarks) is a challenging\nand interesting future direction to fairly compare\nmethods of different categories to align LLMs with\nupdated world knowledge (§4). We will leave quan-\ntitative comparisons and analysis as future work.\nAcknowledgements\nThis work is supported by TPG Telecom. We\nwould like to thank anonymous reviewers for their\nvaluable comments.\nReferences\nBadr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona\nDiab, and Marjan Ghazvininejad. 2022. A review on\nlanguage models as knowledge bases.\nUri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan\nRoth, and Graham Neubig. 2022. Neuro-symbolic\nlanguage modeling with automaton-augmented re-\ntrieval. In Proceedings of the 39th International\nConference on Machine Learning , volume 162 of\nProceedings of Machine Learning Research, pages\n468–485. PMLR.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\n8298\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\nreport.\nAlexei Baevski and Michael Auli. 2019. Adaptive input\nrepresentations for neural language modeling. In In-\nternational Conference on Learning Representations.\nDavid Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu,\nand Antonio Torralba. 2020. Rewriting a deep gen-\nerative model. In Computer Vision – ECCV 2020 ,\npages 351–369, Cham. Springer International Pub-\nlishing.\nRishabh Bhardwaj, George Polovets, and Monica\nSunkara. 2022. Adaptation approaches for nearest\nneighbor language models.\nMagdalena Biesialska, Katarzyna Biesialska, and\nMarta R. Costa-jussà. 2020. Continual lifelong learn-\ning in natural language processing: A survey. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 6523–6541,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n2022. Improving language models by retrieving from\ntrillions of tokens.\nDavis Brown, Charles Godfrey, Cody Nizinski,\nJonathan Tu, and Henry Kvinge. 2023. Edit at your\nown risk: evaluating the robustness of edited models\nto distribution shifts.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-\nter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,\nHarsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nand Yi Zhang. 2023. Sparks of artificial general in-\ntelligence: Early experiments with gpt-4.\nBoxi Cao, Hongyu Lin, Xianpei Han, and Le Sun. 2023.\nThe life cycle of knowledge in big language models:\nA survey.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2023. Quantifying memorization across neural lan-\nguage models. In The Eleventh International Confer-\nence on Learning Representations.\nHarrison Chase. 2022. Langchain.\nHung-Ting Chen, Michael Zhang, and Eunsol Choi.\n2022. Rich knowledge sources bring complex knowl-\nedge conflicts: Recalibrating models to reflect con-\nflicting evidence. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2292–2307, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7870–7881, Online. As-\nsociation for Computational Linguistics.\nWuyang Chen, Yanqi Zhou, Nan Du, Yanping Huang,\nJames Laudon, Zhifeng Chen, and Claire Cui. 2023a.\nLifelong language pretraining with distribution-\nspecialized experts. In Proceedings of the 40th Inter-\nnational Conference on Machine Learning, volume\n202 of Proceedings of Machine Learning Research,\npages 5383–5395. PMLR.\nYuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, and\nJun Zhao. 2023b. Journey to the center of the knowl-\nedge neurons: Discoveries of language-independent\nknowledge neurons and degenerate knowledge neu-\nrons.\nZeming Chen, Gail Weiss, Eric Mitchell, Asli Celiky-\nilmaz, and Antoine Bosselut. 2023c. Reckoning:\nReasoning through dynamic knowledge encoding.\nZhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong,\nWayne Xin Zhao, and Ji-Rong Wen. 2023d. Chatcot:\nTool-augmented chain-of-thought reasoning on chat-\nbased large language models.\nXin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao,\nand Rui Yan. 2023. Decouple knowledge from\nparamters for plug-and-play language modeling.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\n8299\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nRoi Cohen, Eden Biran, Ori Yoran, Amir Globerson,\nand Mor Geva. 2023. Evaluating the ripple effects of\nknowledge editing in language models.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8493–\n8502, Dublin, Ireland. Association for Computational\nLinguistics.\nBhavana Dalvi Mishra, Oyvind Tafjord, and Peter Clark.\n2022. Towards teachable reasoning systems: Using a\ndynamic memory of user feedback for continual sys-\ntem improvement. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 9465–9480, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6491–\n6506, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-aware language mod-\nels as temporal knowledge bases. Transactions of the\nAssociation for Computational Linguistics, 10:257–\n273.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei\nChen, Yang Liu, Jie Tang, Juanzi Li, and Maosong\nSun. 2022. Delta tuning: A comprehensive study of\nparameter efficient methods for pre-trained language\nmodels.\nQingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu,\nZhifang Sui, and Lei Li. 2022. Calibrating factual\nknowledge in pretrained language models. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2022, pages 5937–5947, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nAndrew Drozdov, Shufan Wang, Razieh Rahimi, An-\ndrew McCallum, Hamed Zamani, and Mohit Iyyer.\n2022. You can’t pick your neighbors, or can you?\nwhen and how to rely on retrieval in the kNN-LM.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 2997–3007, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nChao Feng, Xinyu Zhang, and Zichu Fei. 2023. Knowl-\nedge solver: Teaching llms to search for domain\nknowledge from knowledge graphs.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In Proceedings of the 34th Interna-\ntional Conference on Machine Learning - Volume 70,\nICML’17, page 1126–1135. JMLR.org.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-\ncent Y . Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,\nand Kelvin Guu. 2023. Rarr: Researching and re-\nvising what language models say, using language\nmodels.\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir\nGloberson. 2023. Dissecting recall of factual associ-\nations in auto-regressive language models.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5484–5495, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nGoogle. 2023. Med-palm 2.\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,\nYujiu Yang, Nan Duan, and Weizhu Chen. 2023.\nCritic: Large language models can self-correct with\ntool-interactive critiquing.\nAnshita Gupta, Debanjan Mondal, Akshay Krishna She-\nshadri, Wenlong Zhao, Xiang Lorraine Li, Sarah\nWiegreffe, and Niket Tandon. 2023a. Editing com-\nmonsense knowledge in gpt.\nKshitij Gupta, Benjamin Thérien, Adam Ibrahim,\nMats L. Richter, Quentin Anthony, Eugene\nBelilovsky, Irina Rish, and Timothée Lesort. 2023b.\nContinual pre-training of large language models:\nHow to (re)warm your model?\n8300\nSuchin Gururangan, Mike Lewis, Ari Holtzman,\nNoah A. Smith, and Luke Zettlemoyer. 2022. DEMix\nlayers: Disentangling domains for modular language\nmodeling. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5557–5576, Seattle, United States.\nAssociation for Computational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nDavid Ha, Andrew M. Dai, and Quoc V . Le. 2017. Hy-\npernetworks. In International Conference on Learn-\ning Representations.\nThomas Hartvigsen, Swami Sankaranarayanan, Hamid\nPalangi, Yoon Kim, and Marzyeh Ghassemi. 2023.\nAging with grace: Lifelong model editing with dis-\ncrete key-value adaptors.\nPeter Hase, Mohit Bansal, Been Kim, and Asma Ghan-\ndeharioun. 2023a. Does localization inform editing?\nsurprising differences in causality-based localization\nvs. knowledge editing in language models. arXiv\npreprint arXiv:2301.04213.\nPeter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zor-\nnitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and\nSrinivasan Iyer. 2023b. Methods for measuring, up-\ndating, and visualizing factual beliefs in language\nmodels. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 2714–2731, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nHangfeng He, Hongming Zhang, and Dan Roth. 2022.\nRethinking with retrieval: Faithful large language\nmodel inference.\nJunxian He, Graham Neubig, and Taylor Berg-\nKirkpatrick. 2021a. Efficient nearest neighbor lan-\nguage models. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5703–5714, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nTianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing\nLiu, James Glass, and Fuchun Peng. 2021b. Ana-\nlyzing the forgetting problem in pretrain-finetuning\nof open-domain dialogue response models. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 1121–1133, Online.\nAssociation for Computational Linguistics.\nEvan Hernandez, Belinda Z. Li, and Jacob Andreas.\n2023. Inspecting and editing knowledge representa-\ntions in language models.\nJason Hoelscher-Obermaier, Julia Persson, Esben Kran,\nIoannis Konstas, and Fazl Barez. 2023. Detecting\nedit failures in large language models: An improved\nspecificity benchmark.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nNathan Hu, Eric Mitchell, Christopher D. Manning, and\nChelsea Finn. 2023. Meta-learning online adaptation\nof language models.\nZeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou,\nWenge Rong, and Zhang Xiong. 2023. Transformer-\npatcher: One mistake worth one neuron. In The\nEleventh International Conference on Learning Rep-\nresentations.\nTatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and\nSadao Kurohashi. 2023. Multitool-cot: Gpt-3 can use\nmultiple external tools with chain of thought prompt-\ning.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Atlas: Few-shot learning with retrieval\naugmented language models.\nJoel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,\nJoongbo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. 2022a. TemporalWiki: A lifelong\nbenchmark for training and evaluating ever-evolving\nlanguage models. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6237–6250, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,\nJanghoon Han, Gyeonghun KIM, Stanley Jungkyu\nChoi, and Minjoon Seo. 2022b. Towards continual\nknowledge learning of language models. In Interna-\ntional Conference on Learning Representations.\nShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Martti-\nnen, and Philip S. Yu. 2022. A survey on knowledge\ngraphs: Representation, acquisition, and applications.\n8301\nIEEE Transactions on Neural Networks and Learning\nSystems, 33(2):494–514.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv., 55(12).\nZhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki,\nHaibo Ding, Jamie Callan, and Graham Neubig.\n2022. Retrieval as attention: End-to-end learning\nof retrieval and reading within a single transformer.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2336–2349, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023. Active retrieval\naugmented generation.\nXisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao,\nShang-Wen Li, Xiaokai Wei, Andrew Arnold, and\nXiang Ren. 2022. Lifelong pretraining: Continually\nadapting language models to emerging corpora. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4764–4780, Seattle, United States. Association\nfor Computational Linguistics.\nYiming Ju and Zheng Zhang. 2023. Klob: a bench-\nmark for assessing knowledge locating methods in\nlanguage models.\nEhsan Kamalloo, Nouha Dziri, Charles L. A. Clarke,\nand Davood Rafiei. 2023. Evaluating open-domain\nquestion answering in the era of large language mod-\nels.\nJikun Kang, Romain Laroche, Xindi Yuan, Adam\nTrischler, Xue Liu, and Jie Fu. 2023. Think before\nyou act: Decision transformers with internal working\nmemory.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,\nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A. Smith, Yejin Choi, and Kentaro Inui.\n2022. Realtime qa: What’s the answer right now?\nNora Kassner, Oyvind Tafjord, Hinrich Schütze, and\nPeter Clark. 2021. BeliefBank: Adding memory to a\npre-trained language model for a systematic notion\nof belief. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 8849–8861, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nJivat Kaur, Sumit Bhatia, Milan Aggarwal, Rachit\nBansal, and Balaji Krishnamurthy. 2022. LM-CORE:\nLanguage models with contextually relevant external\nknowledge. In Findings of the Association for Com-\nputational Linguistics: NAACL 2022, pages 750–769,\nSeattle, United States. Association for Computational\nLinguistics.\nZixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu,\nand Bing Liu. 2022. Continual training of language\nmodels for few-shot learning. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 10205–10216, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nZixuan Ke and Bing Liu. 2023. Continual learning of\nnatural language processing tasks: A survey.\nZixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi,\nGyuhak Kim, and Bing Liu. 2023. Continual pre-\ntraining of language models. In The Eleventh Inter-\nnational Conference on Learning Representations.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\nDavid Hall, Percy Liang, Christopher Potts, and\nMatei Zaharia. 2023. Demonstrate-search-predict:\nComposing retrieval and language models for\nknowledge-intensive nlp.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2023. Decomposed prompting: A modular\napproach for solving complex tasks. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A. Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2017. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the National Academy of\nSciences, 114(13):3521–3526.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\nInternet-augmented dialogue generation. In Proceed-\nings of the 60th Annual Meeting of the Association\n8302\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8460–8478, Dublin, Ireland. Association\nfor Computational Linguistics.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\nTomáš Koˇciský, Sebastian Ruder, Dani Yogatama,\nKris Cao, Susannah Young, and Phil Blunsom. 2021.\nMind the gap: Assessing temporal generalization\nin neural language models. In Advances in Neural\nInformation Processing Systems.\nKyungjae Lee, Wookje Han, Seung-won Hwang,\nHwaran Lee, Joonsuk Park, and Sang-Woo Lee.\n2022a. Plug-and-play adaptation for continuously-\nupdated QA. In Findings of the Association for Com-\nputational Linguistics: ACL 2022 , pages 438–447,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nNayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pas-\ncale N Fung, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2022b. Factuality enhanced language mod-\nels for open-ended text generation. In Advances in\nNeural Information Processing Systems, volume 35,\npages 34586–34599. Curran Associates, Inc.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin\nWang, Michal Lukasik, Andreas Veit, Felix Yu, and\nSanjiv Kumar. 2022. Large language models with\ncontrollable working memory.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang,\nJian-Yun Nie, and Ji-Rong Wen. 2023a. The web can\nbe your oyster for improving large language models.\nXiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun\nMa, and Jie Yu. 2023b. Pmet: Precise model editing\nin a transformer.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, Yun Wang, Linjun Shou, Ming\nGong, and Nan Duan. 2023. Taskmatrix.ai: Com-\npleting tasks by connecting foundation models with\nmillions of apis.\nBill Yuchen Lin, Sida Wang, Xi Lin, Robin Jia, Lin\nXiao, Xiang Ren, and Scott Yih. 2022. On continual\nmodel refinement in out-of-distribution data streams.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3128–3139, Dublin, Ireland.\nAssociation for Computational Linguistics.\nNelson F. Liu, Tianyi Zhang, and Percy Liang. 2023a.\nEvaluating verifiability in generative search engines.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023b. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9).\nTiedong Liu and Bryan Kian Hsiang Low. 2023. Goat:\nFine-tuned llama outperforms gpt-4 on arithmetic\ntasks.\nAdam Liška, Tomáš Koˇciský, Elena Gribovskaya, Tay-\nfun Terzi, Eren Sezener, Devang Agrawal, Cyprien\nde Masson d’Autume, Tim Scholtes, Manzil Zaheer,\nSusannah Young, Ellen Gilsenan-McMahon, Sophia\nAustin, Phil Blunsom, and Angeliki Lazaridou. 2022.\nStreamingqa: A benchmark for adaptation to new\nknowledge over time in question answering models.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and\nJianfeng Gao. 2023. Chameleon: Plug-and-play com-\npositional reasoning with large language models.\nHongyin Luo, Yung-Sung Chuang, Yuan Gong, Tian-\nhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, He-\nlen Meng, and James Glass. 2023. Sail: Search-\naugmented instruction learning.\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar-\nishma Mandyam, and Noah A. Smith. 2022. Time\nwaits for no one! analysis and challenges of tem-\nporal misalignment. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 5944–5958, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023. Query rewriting for retrieval-\naugmented large language models.\nAman Madaan, Niket Tandon, Peter Clark, and Yim-\ning Yang. 2022. Memory-assisted prompt editing\nto improve GPT-3 after deployment. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 2833–2861,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022a. Locating and editing factual asso-\nciations in gpt. In Advances in Neural Information\nProcessing Systems, volume 35, pages 17359–17372.\nCurran Associates, Inc.\n8303\nKevin Meng, Arnab Sen Sharma, Alex J Andonian,\nYonatan Belinkov, and David Bau. 2023. Mass-\nediting memory in a transformer. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nYuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tian-\nwei Zhang, Fei Wu, and Jiwei Li. 2022b. GNN-\nLM: Language modeling based on global contexts\nvia GNN. In International Conference on Learning\nRepresentations.\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, and Nat McAleese.\n2022. Teaching language models to support answers\nwith verified quotes.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, Edouard Grave, Yann LeCun, and\nThomas Scialom. 2023. Augmented language mod-\nels: a survey.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2022a. Fast model\nediting at scale. In International Conference on\nLearning Representations.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npher D Manning, and Chelsea Finn. 2022b. Memory-\nbased model editing at scale. In Proceedings of the\n39th International Conference on Machine Learning,\nvolume 162 of Proceedings of Machine Learning\nResearch, pages 15817–15831. PMLR.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2022. Webgpt: Browser-\nassisted question-answering with human feedback.\nElla Neeman, Roee Aharoni, Or Honovich, Leshem\nChoshen, Idan Szpektor, and Omri Abend. 2022.\nDisentqa: Disentangling parametric and contextual\nknowledge with counterfactual question answering.\nYasumasa Onoe, Michael J. Q. Zhang, Shankar Padman-\nabhan, Greg Durrett, and Eunsol Choi. 2023. Can\nlms learn new entities from descriptions? challenges\nin propagating injected knowledge.\nOpenAI. 2022. Introducing chatgpt.\nOpenAI. 2023a. chatgpt plugins.\nOpenAI. 2023b. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nShankar Padmanabhan, Yasumasa Onoe, Michael J. Q.\nZhang, Greg Durrett, and Eunsol Choi. 2023. Propa-\ngating knowledge updates to lms through distillation.\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Ji-\napu Wang, and Xindong Wu. 2023. Unifying large\nlanguage models and knowledge graphs: A roadmap.\nBhargavi Paranjape, Scott Lundberg, Sameer Singh,\nHannaneh Hajishirzi, Luke Zettlemoyer, and\nMarco Tulio Ribeiro. 2023. Art: Automatic multi-\nstep reasoning and tool-use for large language mod-\nels.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. 2021. Carbon\nemissions and large neural network training.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, and Jianfeng Gao. 2023a. Check\nyour facts and try again: Improving large language\nmodels with external knowledge and automated feed-\nback.\nGuangyue Peng, Tao Ge, Si-Qing Chen, Furu Wei, and\nHoufeng Wang. 2023b. Semiparametric language\nmodels are scalable continual learners.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2023. Measuring\nand narrowing the compositionality gap in language\nmodels.\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao\nLiang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,\nHuadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan\nLiu, Maosong Sun, and Jie Zhou. 2023a. Webcpm:\nInteractive web search for chinese long-form ques-\ntion answering.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,\nHuadong Wang, Cheng Qian, Runchu Tian, Kunlun\nZhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen\nZhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,\nYuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,\n8304\nYaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,\nXu Han, Xian Sun, Dahai Li, Jason Phang, Cheng\nYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. 2023b. Tool learning with foundation\nmodels.\nYujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2022. ELLE: Ef-\nficient lifelong pre-training for emerging data. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2022, pages 2789–2810, Dublin, Ire-\nland. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1–\n67.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nPaul Röttger and Janet Pierrehumbert. 2021. Temporal\nadaptation of BERT and performance on downstream\ndocument classification: Insights from social media.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2400–2412, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nThomas Scialom, Tuhin Chakrabarty, and Smaranda\nMuresan. 2022. Fine-tuned language models are\ncontinual learners. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6107–6122, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nSina J. Semnani, Violet Z. Yao, Heidi C. Zhang, and\nMonica S. Lam. 2023. Wikichat: A few-shot llm-\nbased chatbot grounded with wikipedia.\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie\nHuang, Nan Duan, and Weizhu Chen. 2023. En-\nhancing retrieval-augmented large language models\nwith iterative retrieval-generation synergy.\nYikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan,\nZhenfang Chen, and Chuang Gan. 2023. Mod-\nuleformer: Modularity emerges from mixture-of-\nexperts.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed Chi, Nathanael Schärli, and\nDenny Zhou. 2023a. Large language models can be\neasily distracted by irrelevant context.\nWeijia Shi, Julian Michael, Suchin Gururangan, and\nLuke Zettlemoyer. 2022. Nearest neighbor zero-shot\ninference. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3254–3265, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen tau Yih. 2023b. Replug: Retrieval-augmented\nblack-box language models.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, Morteza\nBehrooz, William Ngan, Spencer Poff, Naman Goyal,\nArthur Szlam, Y-Lan Boureau, Melanie Kambadur,\nand Jason Weston. 2022. Blenderbot 3: a deployed\nconversational agent that continually learns to respon-\nsibly engage.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang\nWang, Jianfeng Wang, Jordan Lee Boyd-Graber, and\nLijuan Wang. 2023. Prompting GPT-3 to be reli-\nable. In The Eleventh International Conference on\nLearning Representations.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\nPerry Payne, Martin Seneviratne, Paul Gamble, Chris\nKelly, Nathaneal Scharli, Aakanksha Chowdhery,\nPhilip Mansfield, Blaise Aguera y Arcas, Dale Web-\nster, Greg S. Corrado, Yossi Matias, Katherine Chou,\nJuraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Ra-\njkomar, Joelle Barral, Christopher Semturs, Alan\nKarthikesalingam, and Vivek Natarajan. 2022. Large\nlanguage models encode clinical knowledge.\n8305\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin,\nSergei Popov, and Artem Babenko. 2020. Editable\nneural networks. In International Conference on\nLearning Representations.\nNiket Tandon, Aman Madaan, Peter Clark, and Yiming\nYang. 2022. Learning to repair: Repairing model out-\nput errors after deployment using a dynamic memory\nof feedback. In Findings of the Association for Com-\nputational Linguistics: NAACL 2022, pages 339–352,\nSeattle, United States. Association for Computational\nLinguistics.\nKushal Tirumala, Aram Markosyan, Luke Zettlemoyer,\nand Armen Aghajanyan. 2022. Memorization with-\nout overfitting: Analyzing the training dynamics of\nlarge language models. In Advances in Neural Infor-\nmation Processing Systems, volume 35, pages 38274–\n38290. Curran Associates, Inc.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-\nintensive multi-step questions.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nJiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao,\nand Jiarong Xu. 2023a. Cross-lingual knowledge\nediting in large language models.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021. K-Adapter: Infusing\nKnowledge into Pre-Trained Models with Adapters.\nIn Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pages 1405–1418,\nOnline. Association for Computational Linguistics.\nZekun Wang, Ge Zhang, Kexin Yang, Ning Shi,\nWangchunshu Zhou, Shaochun Hao, Guangzheng\nXiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen,\nQingqing Zhu, Zhenzhu Yang, Adam Nik, Qi Liu,\nChenghua Lin, Shi Wang, Ruibo Liu, Wenhu Chen,\nKe Xu, Dayiheng Liu, Yike Guo, and Jie Fu. 2023b.\nInteractive natural language processing.\nZhenyi Wang, Enneng Yang, Li Shen, and Heng Huang.\n2023c. A comprehensive survey of forgetting in deep\nlearning beyond continual learning.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nXiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bha-\ntia, and Andrew Arnold. 2021. Knowledge enhanced\npretrained language models: A compreshensive sur-\nvey.\nSuhang Wu, Minlong Peng, Yue Chen, Jinsong Su, and\nMingming Sun. 2023. Eva-kellm: A new benchmark\nfor evaluating knowledge editing of llms.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,\nand Christian Szegedy. 2022. Memorizing transform-\ners. In International Conference on Learning Repre-\nsentations.\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and\nYu Su. 2023. Adaptive chameleon or stubborn sloth:\nUnraveling the behavior of large language models in\nknowledge clashes.\nYan Xu, Mahdi Namazifar, Devamanyu Hazarika, Aish-\nwarya Padmakumar, Yang Liu, and Dilek Hakkani-\nTür. 2023a. Kilm: Knowledge injection into encoder-\ndecoder language models.\nYang Xu, Yutai Hou, Wanxiang Che, and Min Zhang.\n2023b. Language anisotropic cross-lingual model\nediting.\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin\nLin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-\nreact: Prompting chatgpt for multimodal reasoning\nand action.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao.\n2023a. React: Synergizing reasoning and acting\nin language models. In The Eleventh International\nConference on Learning Representations.\nYunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng,\nZhoubo Li, Shumin Deng, Huajun Chen, and Ningyu\nZhang. 2023b. Editing large language models: Prob-\nlems, methods, and opportunities.\nDa Yin, Li Dong, Hao Cheng, Xiaodong Liu, Kai-Wei\nChang, Furu Wei, and Jianfeng Gao. 2022. A survey\nof knowledge-intensive nlp with pre-trained language\nmodels.\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,\nXipeng Qiu, and Xuanjing Huang. 2023. Do large\nlanguage models know what they don’t know?\nPengfei Yu and Heng Ji. 2023. Self information up-\ndate for large language models through mitigating\nexposure bias.\nWenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng\nJiang, and Ashish Sabharwal. 2023a. Improving lan-\nguage models via plug-and-play retrieval feedback.\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\nQingyun Wang, Heng Ji, and Meng Jiang. 2022. A\nsurvey of knowledge-enhanced text generation. ACM\nComput. Surv., 54(11s).\n8306\nZichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu.\n2023b. Augmentation-adapted retriever improves\ngeneralization of language models as generic plug-\nin.\nMichael J. Q. Zhang and Eunsol Choi. 2023. Mitigating\ntemporal misalignment by discarding outdated facts.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei\nFang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,\nDanny Fox, Helen Meng, and James Glass. 2023.\nInterpretable unified language checking.\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei\nQin, and Lidong Bing. 2023. Verify-and-edit: A\nknowledge-enhanced chain-of-thought framework.\nChaoqi Zhen, Yanlei Shang, Xiangyu Liu, Yifei Li,\nYong Chen, and Dell Zhang. 2022. A survey on\nknowledge-enhanced pre-trained language models.\nCe Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong\nWu, Jingjing Xu, and Baobao Chang. 2023. Can we\nedit factual knowledge by in-context learning?\nZexuan Zhong, Zhengxuan Wu, Christopher D. Man-\nning, Christopher Potts, and Danqi Chen. 2023.\nMquake: Assessing knowledge editing in language\nmodels via multi-hop questions.\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and\nMuhao Chen. 2023. Context-faithful prompting for\nlarge language models.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.\n2020. Modifying memories in transformer models.\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming\nZheng, Soujanya Poria, and Tat-Seng Chua. 2021.\nRetrieving and reading: A comprehensive survey on\nopen-domain question answering.\nXinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, Jian-\nGuang Lou, and Yujiu Yang. 2023. Question an-\nswering as programming for solving time-sensitive\nquestions.\nNoah Ziems, Wenhao Yu, Zhihan Zhang, and Meng\nJiang. 2023. Large language models are built-in au-\ntoregressive search engines.\nA Appendix\nA.1 Additional Description of Approaches\nA.1.1 Naive Approaches\nAlthough more advanced approaches have been\nproposed, we introduce naive solutions for com-\npleteness in this section.\nRe-training. Intuitively, one can regularly re-\ntrain the model from scratch with the latest corpora\nto align with current world knowledge. However,\nthis naive solution has clear downsides: (1) Re-\ntraining is both time and money expensive and\nenvironmentally unfriendly (Patterson et al., 2021),\nespecially in the era of LLMs with billions of pa-\nrameters. For instance, LLaMA-65B was trained\nfor about one million GPU-hours and emitted more\nthan a hundred tons of carbon (Touvron et al.,\n2023); (2) It is unrealistic to frequently re-training\nan LLM in response to the constantly changing\nworld.\nFine-tuning. Another simple approach is to pe-\nriodically curate a small-scale dataset containing\ndesired knowledge we wish the model to add, up-\ndate, or delete, then fine-tune the model on the\ndataset. Despite being computationally cheaper\nthan re-training, it still falls short in that, with-\nout constraints, directly fine-tuning the model may\nhave a \"butterfly effect\" and affect other knowl-\nedge or skills present in the model (Li et al., 2022),\ncausing degraded generalization (Mitchell et al.,\n2022a), catastrophic forgetting (Kirkpatrick et al.,\n2017; Zhu et al., 2020; AlKhamissi et al., 2022), or\nknowledge conflicts (Neeman et al., 2022).\nConstrained Fine-tuning. To solve part of above\nmentioned issues, Zhu et al. (2020) propose to only\nfine-tune the model on the small-scale modified\nfacts set and add explicit constraints on the model\nweights so that the model learns to answer the\nmodified facts while keeping the remaining knowl-\nedge intact. Specifically, they use various norms\n(L0, L2, and L∞) to prevent the parameters of the\nfine-tuned model θ′from drifting too far from the\noriginal model parameters θ. They further find\nthat fine-tuning only the first and last layers of the\nTransformer model (Vaswani et al., 2017) results\nin better adaptation to the modified facts and bet-\nter preservation of performance on the unmodified\nfacts. However, the norm-based constraint on pa-\nrameters ignores the highly non-linear nature of\nLMs and how parameters determine the outputs of\nthe model, making their method potentially unre-\nliable (De Cao et al., 2021). In addition, Mitchell\net al. (2022a) confirm that constrained fine-tuning\ngenerally does not consistently provide edit gener-\nality.\n8307\nFigure 5: An example of knowledge conflict of Chat-\nGPT (OpenAI, 2022). Even if the correct context is\nprovided, ChatGPT still favours its internally memo-\nrized knowledge. The screenshot was taken in May\n2023 for GPT-3.5 without web browsing.\nA.1.2 Knowledge Editing\nTo facilitate the development of this area, De Cao\net al. (2021) formulate three desiderata that an\nideal editing method should follow: 1 Gener-\nality: the method should be capable of alter-\ning the knowledge of any LM that is not specif-\nically trained to be editable ( e.g., PaLM, GPT-\n4, LLaMA); 2 Reliability: the method should\nonly update the targeted knowledge without influ-\nencing the rest of the knowledge in the LM. For\ninstance, the answer to \"Who is the current\nPrime Minister of Australia?\" has changed\nfrom \"Scott Morrison\"to \"Anthony Albanese\"\nsince 2022, updating the knowledge from \"Scott\nMorrison\" to \"Anthony Albanese\" should not\nchange the knowledge\"Argentina won the 2022\nWorld Cup\" ; 3 Consistency (Generalization):\nafter updates, the model predictions should be con-\nsistent across semantically equivalent inputs (e.g.,\ncorrectly predicts \"Anthony Albanese\" to \"Who\nis the AU PM?\" ). Beyond updating outdated\nknowledge, knowledge editing can also delete sen-\nsitive information for privacy issues or eliminate\nbiases in the pre-training corpora.\nHowever, not until recently, Onoe et al. (2023);\nZhong et al. (2023) show that, after performing\nknowledge editing, the LLM does not really \"learn\"\nthe updated knowledge and thus cannot propagate\nthe new knowledge and make further inferences\nbased on them. For instance, after learning that\n\"the current PM of Australia is Anthony\nAlbanese\", the model might not able to make pre-\ndictions of \"Who is the spouse of the current\nPM of Australia?\".\nMeta-learning. Sinitsin et al. (2020), by con-\nstraining the training objective, encodes editability\ninto the parameters of the model itself so that the\nmodel is \"prepared\" for incoming edits. While be-\ning effective and no new parameters are required,\nit does not conform to generality as it requires spe-\ncialized training of the original model (De Cao\net al., 2021). Moreover, to enforce the constraint\nthat the editable model agrees with the original pre-\ntrained model’s predictions, Sinitsin et al. (2020)’s\nmethod needs to retain a copy of the original model,\nwhich significantly consumes computation memory\n(Mitchell et al., 2022a). Chen et al. (2023c) also\nrequires training of the original LM, which could\nbe computationally expensive for larger LMs. In\naddition, whether it will influence other irrelevant\nknowledge in the model remains unknown, making\nthe method potentially unreliable.\nHypernetwork Editor. De Cao et al. (2021) can\nbe more efficient than Sinitsin et al. (2020), as it\ndoes not retain the copy of the original model nor\ncompute higher-order gradients. However, it can\nonly update a single fact rather than multiple facts\nin a row and fail to edit large models, leading to\npoor scalability (Mitchell et al., 2022a; Hase et al.,\n2023b). Mitchell et al. (2022a) improve De Cao\net al. (2021)’s work and is stable to edit LMs from\nBERT-base (110M) (Devlin et al., 2019) to T5-\nXXL (11B) (Raffel et al., 2020). However, when\nediting multiple knowledge simultaneously, their\nedit success rate significantly degrades.\nLocate and Edit. While simple, Dai et al. (2022)\ndo not ensure reliability on other irrelevant knowl-\nedge and generalization on semantically equivalent\ninputs. Despite showing both generalization and\nspecificity, Meng et al. (2022a) only edits a single\nfact at a time, making it impractical for large-scale\nknowledge updating in LLMs. Through casual\ntracing, Meng et al. (2023) identify and update the\ncritical MLP layers in one go. However, Hase et al.\n(2023a) argue that the relation between localization\nand editing may be misleading as they can edit fac-\ntual knowledge in different locations that are not\nsuggested by casual tracing.\nA.1.3 Continual Learning\nWhile knowledge editing provides a fine-grained\ncontrol to update specific knowledge in LLMs, it\noften requires large amounts of supervised train-\ning data to make edits, which is non-trivial to cre-\nate (Hartvigsen et al., 2023). In addition, when\nan LLM needs to quickly acquire new domain\nknowledge (e.g., legal or medical), such small-scale\nmodel edits may not be efficient. Moreover, after\nmultiple parameter patches to a deployed model,\nits internal knowledge may conflict, leading to un-\npredictable behaviors (Mitchell et al., 2022a).\n8308\nSharing a related goal, continual learning (CL)\naims to enable a model to learn from a continu-\nous data stream across time while reducing catas-\ntrophic forgetting of previously acquired knowl-\nedge (Biesialska et al., 2020). In contrast to knowl-\nedge editing, CL generally updates models on a\nlarger scale and works in long learning sequences\nwith minimal memory overheads (Mitchell et al.,\n2022a). Hence, CL can also be used for deployed\nmodels to update their knowledge.\nA.2 The Complete Taxonomy of Methods\nWe list the complete taxonomy of methods to align\nLLMs with the ever-changing world knowledge in\nFig.6 and the complete comparison of methods in\nTable 3. We also compare the characteristics of\ndifferent methods in Table 2.\nCategory Large\nScale\nNo Side\nEffects Persistent\nKnowledge Editing\n(§2.1.1) ✗ ✗ ✔\nContinual Learning (§2.1.2) ✔ ✗ ✔\nRetrieval-based (§2.2) ✗ ✔ ✗\nTable 2: High-level comparison of characteristics of\ndifferent approaches.\n8309\nCategory Representative Method Base LM LM Params Augmentation No\nTraining\nBlack\n-box\nRe-training – ὒ5 – ✗ ✗\nFine-tuning – ὒ5 – ✗ ✗\nDe Cao et al. (2021) BERT (0.1B) /snowflakeauxiliary model ✗ ✗\nMEND (Mitchell et al., 2022a) T5 (11B) /snowflakeauxiliary model ✗ ✗\nSLAG (Hase et al., 2023b) BERT (0.1B) /snowflakeauxiliary model ✗ ✗\nRECKONING (Chen et al., 2023c) GPT-2 (0.1B) ὒ5 – ✗ ✗\nROME (Meng et al., 2022a) GPT-J (6B) ὒ5 – ✔ ✗\nKnowledge Neurons (Dai et al., 2022) BERT (0.1B) ὒ5 – ✔ ✗\nMEMIT (Meng et al., 2023) GPT-NeoX (20B) ὒ5 – ✔ ✗\nCaliNET (Dong et al., 2022) T5 (0.7B) /snowflake+params ✗ ✗\nREMEDI (Hernandez et al., 2023) GPT-J (6B) ὒ5 auxiliary model ✗ ✗\nDSA (Ke et al., 2023) RoBERTa (0.1B) ὒ5 – ✗ ✗\nELLE (Qin et al., 2022) BERT (0.1B) ὒ5 memory+params ✗ ✗\nCT0 (Scialom et al., 2022) T0 (3B) ὒ5 memory ✗ ✗\nK-Adapter (Wang et al., 2021) RoBERTa (0.3B) /snowflake+params ✗ ✗\nGururangan et al. (2022) GPT-2 (0.7B) /snowflake+params ✗ ✗\nCPT (Ke et al., 2022) RoBERTa (0.1B) /snowflake+params ✗ ✗\nKILM (Xu et al., 2023a) BART (0.4B) ὒ5 – ✗ ✗\nCaMeLS (Hu et al., 2023) GPT-2 (1.5B) ὒ5 auxiliary model ✗ ✗\nSeMem (Peng et al., 2023b) GPT-2 (0.7B) /snowflakememory\n+auxiliary model ✗ ✗\nCL-plugin (Lee et al., 2022a) T5 (0.7B) /snowflake+params ✗ ✗\nHuang et al. (2023) BERT (0.1B) /snowflake+params ✗ ✗\nGRACE (Hartvigsen et al., 2023) T5 (0.06B) /snowflakememory ✗ ✗\nkNN-LM (Khandelwal et al., 2020)\nADP\n(Baevski and Auli, 2019)\n(0.2B)\n/snowflakememory ✔ ✗\nAdaptRet (He et al., 2021a) ADP (0.2B) /snowflakememory\n+auxiliary model ✗ ✗\nRetoMaton (Alon et al., 2022) ADP (0.2B) /snowflakememory\n+auxiliary graph ✗ ✗\nkNN-prompt (Shi et al., 2022) GPT-2 (0.8B) /snowflakememory ✔ ✔\nBelief Bank (Kassner et al., 2021) T5 (0.7B) /snowflakememory\n+constraint solver ✔ ✔\nFBNet (Tandon et al., 2022) T5 (11B) /snowflakememory\n+auxiliary model ✗ ✔\nMemPrompt (Madaan et al., 2022) GPT-3 (175B) /snowflakememory+retriever ✔ ✔\nTeachMe (Dalvi Mishra et al., 2022) T5 (11B) /snowflakememory+retriever ✔ ✔\nSERAC (Mitchell et al., 2022b) T5 (0.7B) /snowflakememory\n+auxiliary model ✗ ✔\nMeLLo (Zhong et al., 2023) GPT-3.5 (175B) /snowflakememory+retriever ✔ ✔\nIC-Retrieval (Si et al., 2023) GPT-3.5 (175B) /snowflakeretriever ✔ ✔\nIC-RALM (Ram et al., 2023) OPT (66B) /snowflakeretriever+reranker ✗ ✔\nIKE (Zheng et al., 2023) OPT (175B) /snowflakeretriever ✔ ✔\nAAR (Yu et al., 2023b) GPT-3.5 (175B) /snowflakeretriever ✗ ✔\nRePlug (Shi et al., 2023b) GPT-3 (175B) /snowflakeretriever ✗/ ✔ ✔\nIRCoT (Trivedi et al., 2022) GPT-3.5 (175B) /snowflakeretriever ✔ ✔\nRARR (Gao et al., 2023) PaLM (540B) /snowflakesearch engine\n+auxiliary model ✔ ✔\nRR (He et al., 2022) GPT-3.5 (175B) /snowflakeretriever\n+auxiliary model ✔ ✔\nReFeed (Yu et al., 2023a) GPT-3.5 (175B) /snowflakeretriever ✔ ✔\nDecomP (Khot et al., 2023) GPT-3.5 (175B) /snowflakeretriever ✔ ✔\nReAct (Yao et al., 2023a) PaLM (540B) /snowflakesearch engine ✔ ✔\nSelf-Ask (Press et al., 2023) GPT-3 (175B) /snowflakesearch engine ✔ ✔\nFLARE (Jiang et al., 2023) GPT-3.5 (175B) /snowflakeretriever/search engine ✔ ✔\nDSP (Khattab et al., 2023) GPT-3.5 (175B) /snowflakeretriever ✔ ✔\nART (Paranjape et al., 2023) GPT-3.5 (175B) /snowflakevarious tools ✔ ✔\nIter-RetGen (Shao et al., 2023) GPT-3.5 (175B) /snowflakeretriever ✔ ✔\nVerify-and-Edit (Zhao et al., 2023) GPT-3.5 (175B) /snowflakeretriever/search engine ✔ ✔\nLazaridou et al. (2022) Gopher (280B) /snowflakesearch engine ✔ ✔\nCRITIC (Gou et al., 2023) GPT-3.5 (175B) /snowflakevarious tools ✔ ✔\nLLM Rewriter (Ma et al., 2023) GPT-3.5 (175B) /snowflakesearch engine ✗ ✔\nChameleon (Lu et al., 2023) GPT-4 (?B) /snowflakevarious tools ✔ ✔\nChatGPT Plugins (OpenAI, 2023a) GPT-3.5 (175B) /snowflakevarious tools ✔ ✔\nNaive\nKnowledge\nEditing\nContinual\nLearning\nMemory\n-enhanced\nRetrieval\n-enhanced\nInternet\n-enhanced\nTable 3: Comparison between representative methods. ὒ5means the parameters of the original LM are modified,\nwhile /snowflakemeans they are unchanged; Augmentation means additional components used; No Training indicates the\nmethod does not require additional training; Black-box refers to whether the method suits non-publicly available\nmodels (e.g., no model architecture, parameters, activations, or gradients are available). Note that we only list the\nlargest size model used in the paper due to space limitations.\n8310\nLLMs align with\never-changing\nworld knowledge\nImplicit\n(§2.1)\nNaive\nRe-training\nFine-tuning\nKnowledge\nEditing\nMeta-learning Editable Training (Sinitsin et al., 2020), RECKONING (Chen et al., 2023c)\nHypernetwork\nEditor\nKnowledgeEditor (De Cao et al., 2021), MEND (Mitchell et al., 2022a), SLAG (Hase et al.,\n2023b), REMEDI (Hernandez et al., 2023), Distillation (Padmanabhan et al., 2023)\nLocate\nand edit\nKnowledge Neurons (Dai et al., 2022), ROME (Meng et al., 2022a), MEMIT (Meng et al.,\n2023), MEMITCSK (Gupta et al., 2023a), PMET (Li et al., 2023b), Chen et al. (2023b),\nGeva et al. (2023), KLoB (Ju and Zhang, 2023)\nOther Eva-KELLM (Wu et al., 2023), RippleEdits (Cohen et al., 2023), Wang et al. (2023a), Xu\net al. (2023b), IKE (Zheng et al., 2023)\nContinual\nLearning\nContinual\nPre-training\nRegularization-\nbased RecAdam (Chen et al., 2020), DSA (Ke et al., 2023)\nReplay-based Mix-Review (He et al., 2021b), ELLE (Qin et al., 2022), CT0 (Scialom\net al., 2022)\nArchitectural-\nbased\nK-Adapter (Wang et al., 2021), LoRA (Hu et al., 2022), ELLE (Qin\net al., 2022), DEMix-DAPT (Gururangan et al., 2022), CPT (Ke et al.,\n2022), Lifelong-MoE (Chen et al., 2023a), ModuleFormer (Shen et al.,\n2023)\nOther\nTemporal-LM (Dhingra et al., 2022), Lifelong Pre-training (Jin et al.,\n2022), CKL (Jang et al., 2022b), TemporalWiKi (Jang et al., 2022a),\nTopicPrefix (Lee et al., 2022b), KILM (Xu et al., 2023a), SeMem\n(Peng et al., 2023b), CaMeLS (Hu et al., 2023), Yu and Ji (2023),\nGupta et al. (2023b)\nContinual\nKnowledge\nEditing\nCMR (Lin et al., 2022), CL-plugin (Lee et al., 2022a), Transformer-Patcher (Huang et al.,\n2023), GRACE (Hartvigsen et al., 2023)\nExplicit\n(§2.2)\nMemory-\nenhanced\nCorpus or\nDocuments\nkNN-LM (Khandelwal et al., 2020), AdaptRet (He et al., 2021a), AdaptCoef (Drozdov\net al., 2022), RetoMaton (Alon et al., 2022), Bhardwaj et al. (2022), kNN-prompt (Shi\net al., 2022), SeMem (Peng et al., 2023b)\nFeedback or\nCorrections\nBelief Bank (Kassner et al., 2021), FBNet (Tandon et al., 2022), MemPrompt (Madaan\net al., 2022), TeachMe (Dalvi Mishra et al., 2022), SERAC (Mitchell et al., 2022b), MeLLo\n(Zhong et al., 2023)\nRetrieval-\nenhanced\nSingle-Stage IC-Retrieval (Si et al., 2023), IC-RALM (Ram et al., 2023), AAR (Yu et al., 2023b), IKE\n(Zheng et al., 2023), Adaptive Retrieval (Mallen et al., 2023), RePlug (Shi et al., 2023b)\nMulti-Stage\nIRCoT (Trivedi et al., 2022), RARR (Gao et al., 2023), RR (He et al., 2022), ReFeed (Yu\net al., 2023a), Self-Ask (Press et al., 2023), DecomP (Khot et al., 2023), ReAct (Yao et al.,\n2023a), ART (Paranjape et al., 2023), ChatCoT (Chen et al., 2023d), MultiTool-CoT (In-\naba et al., 2023), LLM-Augmenter (Peng et al., 2023a), QAaP (Zhu et al., 2023), FLARE\n(Jiang et al., 2023), DSP (Khattab et al., 2023), Iter-RetGen (Shao et al., 2023), Verify-and-\nEdit (Zhao et al., 2023), CRITIC (Gou et al., 2023), WikiChat (Semnani et al., 2023), LLM\nRewriter (Ma et al., 2023), Knowledge Solver (Feng et al., 2023)\nInternet-\nenhanced\nInternet-Fewshot (Lazaridou et al., 2022), LLM-URL (Ziems et al., 2023), ReAct (Yao et al., 2023a), Self-Ask\n(Press et al., 2023), ART (Paranjape et al., 2023), RARR (Gao et al., 2023), TaskMatrix.AI (Liang et al., 2023),\nMM-REACT (Yang et al., 2023), Chameleon (Lu et al., 2023), FLARE (Jiang et al., 2023), CRITIC (Gou et al.,\n2023), LLM Rewriter (Ma et al., 2023), LangChain (Chase, 2022), ChatGPT Plugins (OpenAI, 2023a)\nFigure 6: Taxonomy of methods to align LLMs with the ever-changing world knowledge. Implicit means the\napproaches seek to directly alter the knowledge stored in LLMs (e.g., parameters) (§2.1), while Explicit means\nmore often incorporating external resources to override internal knowledge (e.g., search engine) (§2.2).\n8311",
  "topic": "Software deployment",
  "concepts": [
    {
      "name": "Software deployment",
      "score": 0.6975576281547546
    },
    {
      "name": "Computer science",
      "score": 0.5697647333145142
    },
    {
      "name": "Data science",
      "score": 0.5506033301353455
    },
    {
      "name": "Categorization",
      "score": 0.5485471487045288
    },
    {
      "name": "Field (mathematics)",
      "score": 0.5292066335678101
    },
    {
      "name": "Knowledge management",
      "score": 0.36524927616119385
    },
    {
      "name": "Engineering ethics",
      "score": 0.3423376977443695
    },
    {
      "name": "Management science",
      "score": 0.3393198847770691
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.3213478922843933
    },
    {
      "name": "Artificial intelligence",
      "score": 0.19798675179481506
    },
    {
      "name": "Engineering",
      "score": 0.164653480052948
    },
    {
      "name": "Business",
      "score": 0.13248273730278015
    },
    {
      "name": "Software engineering",
      "score": 0.11919200420379639
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114017466",
      "name": "University of Technology Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I146655781",
      "name": "University of Liverpool",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I204824540",
      "name": "University of Wollongong",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    }
  ]
}