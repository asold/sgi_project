{
  "title": "The Zero Resource Speech Benchmark 2021: Metrics and baselines for\\n unsupervised spoken language modeling",
  "url": "https://openalex.org/W3110458199",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2653674194",
      "name": "Nguyễn Tú Anh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226814786",
      "name": "de Seyssel, Maureen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287403536",
      "name": "Roze, Patricia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226319351",
      "name": "Riviere, Morgane",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227681708",
      "name": "Kharitonov, Evgeny",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221724799",
      "name": "Baevski, Alexei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282597096",
      "name": "Dunbar, Ewan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202141647",
      "name": "Dupoux, Emmanuel",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2889947987",
    "https://openalex.org/W2103318667",
    "https://openalex.org/W2170682101",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2973049979",
    "https://openalex.org/W2395899413",
    "https://openalex.org/W3016011332",
    "https://openalex.org/W2251025892",
    "https://openalex.org/W2963620343",
    "https://openalex.org/W2963425185",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W3102342027",
    "https://openalex.org/W2809981375",
    "https://openalex.org/W2988736778",
    "https://openalex.org/W2142625445",
    "https://openalex.org/W2176085882",
    "https://openalex.org/W3129289122",
    "https://openalex.org/W3003875258",
    "https://openalex.org/W2996383576",
    "https://openalex.org/W2132631284",
    "https://openalex.org/W3093096176",
    "https://openalex.org/W2973026522",
    "https://openalex.org/W2014307400",
    "https://openalex.org/W2346964103",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2963419157",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W3005511757",
    "https://openalex.org/W2137735870",
    "https://openalex.org/W2963366649",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2741692265",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2972447203",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W2080100102",
    "https://openalex.org/W2593779438",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W2995181338",
    "https://openalex.org/W2252211741",
    "https://openalex.org/W2026487812",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "We introduce a new unsupervised task, spoken language modeling: the learning\\nof linguistic representations from raw audio signals without any labels, along\\nwith the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot\\nmetrics probing for the quality of the learned models at 4 linguistic levels:\\nphonetics, lexicon, syntax and semantics. We present the results and analyses\\nof a composite baseline made of the concatenation of three unsupervised\\nsystems: self-supervised contrastive representation learning (CPC), clustering\\n(k-means) and language modeling (LSTM or BERT). The language models learn on\\nthe basis of the pseudo-text derived from clustering the learned\\nrepresentations. This simple pipeline shows better than chance performance on\\nall four metrics, demonstrating the feasibility of spoken language modeling\\nfrom raw speech. It also yields worse performance compared to text-based\\n'topline' systems trained on the same data, delineating the space to be\\nexplored by more sophisticated end-to-end models.\\n",
  "full_text": "The Zero Resource Speech Benchmark 2021:\nMetrics and baselines for unsupervised\nspoken language modeling\nTu Anh Nguyen∗\nFacebook AI Research & EHESS,\nENS-PSL Univ., CNRS, INRIA, France\nnguyentuanh208@gmail.com\nMaureen de Seyssel∗\nEHESS, ENS-PSL Univ., CNRS, INRIA\n& U. Paris, France\nmaureen.deseyssel@gmail.com\nPatricia Rozé\nENS-PSL Univ., CNRS\nFrance\npatricia.roze@ens.fr\nMorgane Rivière\nFacebook AI Research\nFrance\nmriviere@fb.com\nEvgeny Kharitonov\nFacebook AI Research\nFrance\nkharitonov@fb.com\nAlexei Baevski\nFacebook AI Research\nFrance\nabaevski@fb.com\nEwan Dunbar†\nU. Paris Diderot, France\n& U. Toronto, Canada\newan.dunbar@utoronto.ca\nEmmanuel Dupoux†\nFacebook AI Research & EHESS,\nENS-PSL, CNRS, INRIA, France\nemmanuel.dupoux@gmail.com\nAbstract\nWe introduce a new unsupervised task, spoken language modeling: the learning\nof linguistic representations from raw audio signals without any labels, along\nwith the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-\nshot metrics probing for the quality of the learned models at 4 linguistic levels:\nphonetics, lexicon, syntax and semantics. We present the results and analyses of\na composite baseline made of the concatenation of three unsupervised systems:\nself-supervised contrastive representation learning (CPC), clustering (k-means) and\nlanguage modeling (LSTM or BERT). The language models learn on the basis of\nthe pseudo-text derived from clustering the learned representations. This simple\npipeline shows better than chance performance on all four metrics, demonstrating\nthe feasibility of spoken language modeling from raw speech. It also yields worse\nperformance compared to text-based ‘topline’ systems trained on the same data,\ndelineating the space to be explored by more sophisticated end-to-end models.\n1 Introduction\nIn recent work, self-supervised techniques from vision and NLP have been applied to large datasets of\nraw audio, giving rise to very effective methods of pretraining for downstream ASR tasks, particularly\nin the low resource scenario (Schneider et al., 2019; Baevski et al., 2019; Chung and Glass, 2019;\nBaevski et al., 2020b; Rivière et al., 2020; Kawakami et al., 2020; Wang et al., 2020). The approaches\nbased on transformers and masking objectives, strikingly similar to the models used to train language\nmodels, are especially intriguing. The fact that these approaches yield excellent ASR performance\n(less than 10% WER) with as little as 10 minutes of labels plus a language model (LM), or with\n10 hours of labels but no LM (Baevski et al., 2020b), suggests that these systems may actually go\nbeyond acoustic modeling, learning their own LM from raw audio. Such work therefore connects with\n∗Equal contribution as ﬁrst authors. †Equal contributions as last authors.\nPreprint from the Self-Supervised Learning for Speech and Audio Processing Workshop @ NeurIPS 2020.\narXiv:2011.11588v2  [cs.CL]  1 Dec 2020\nTable 1: Summary description of the four Zero Resouce Benchmark 2021 metrics. The metrics\nin light blue use a pseudo-distance dbetween embeddings (dh being from human judgments), the\nmetrics in light orange use a pseudo-probability pcomputed over the entire input sequence.\nLinguistic\nlevel\nMetrics Dataset Task Example\nacoustic-\nphonetic\nABX Libri-light d(a,x) < d(b,x)?\na∈A,b ∈B,\nx̸= a∈A\nwithin-speaker:\n(apas1 ,abas1 ,apas1 )\nacross-speaker:\n(apas1 ,abas1 ,apas2 )\nlexicon spot-the-\nword\nsWUGGY p(a) >p(b)? (brick, blick)\n(squalled, squilled)\nlexical\nsemantics\nsimilarity\njudgement\nsSIMI d(a,b) ∝dh(a,b)? (abduct, kidnap) : 8.63\n(abduct, tap) : 0.5\nsyntax acceptability\njudgment\nsBLIMP p(a) >p(b)? (dogs eat meat, dogs eats meat)\n(the boy can’t help himself, the\nboy can’t help herself)\nresearch into the zero resourcesetting, which aims at learning linguistic representations from scratch\nfor language with little or no textual resources. However, up to now, there exists no established\nbenchmark to analyse the representations learned by such models beyond the acoustic/phonetic level.\nTypically, language models trained from text are evaluated using scores like perplexity. Unfortunately,\nthis simple approach cannot be used here, since perplexity scores computed from learned discrete\nunits vary according to granularity, making model comparison impossible. This is why we chose to\nfollow a black-box NLP strategy: our metrics do require expert linguistic labels for the dev and test\nsets, but are zero-shot in that they do not require training a classiﬁer, they use simple tasksenabling\ndirect human/machine comparison, and they giveinterpretable scores at each linguistic level. As seen\nin Table 1, they can be divided into two types: distance-based and probability-based metrics. Distance-\nbased metrics require models to provide a pseudo-distance computed over pairs of embeddings. The\nABX score (Schatz et al., 2013), already used for the evaluation of acoustic/phonetic representations,\nfalls in this category and provides a measure of how well separated phonetic categories are in a\ngiven embedding space. Here, we use the ABX score developed in Libri-light (Kahn et al., 2020).\nDistance-based methods can also be used to evaluate the semantic representation of words, by\ncomputing the correlation between these distances and human semantic similarity judgements (see\nSchnabel et al., 2015; Faruqui et al., 2016). Chung and Glass (2018) adapted this metric to speech,\nwhich we compiled into our sSIMI dataset. Probability-based metrics require models to compute\na pseudo-probability for a given test input (non-normalized non-negative number for a given input\nwaveform). The pseudo-probabilities are computed over pairs of inputs, one of which is acceptable in\nthe tested language and the other not. Such methods have been used in NLP to evaluate the syntactic\nabilities of language models, by comparing the probabilities of grammatical versus ungrammatical\nsentences(Warstadt et al., 2019), and we built the sBLIMP dataset upon this work. Finally, in our\nsWUGGY dataset, we extend this logic to the lexical level by comparing the pseudo-probability\nassociated to words and nonwords. The four metrics are presented in more details in Section 3.2.\nNext, we apply these metrics to a simple baseline system (Section 3.3), built on contrastive pretraining\n(Contrastive Predictive Coding, CPC, van den Oord et al., 2018; Rivière et al., 2020), followed by\nk-means clustering, which we use to decode a speech dataset (LibriSpeech, Panayotov et al., 2015)\ninto pseudo-text. This pseudo-text is used to train a language model varying in compute budget:\nan LSTM (smaller budget) or BERT (larger budget) model. We show (Section 4) that such simple\nbaseline models give better than chance performance on all 4 metrics, demonstrating that it has\nlearned representations at the four corresponding linguistic levels. However, comparison with a\ntext-based BERT topline system trained on the phonetic transcription of the same training data shows\nthat the speech input raises challenges for the LM component of the model that need to be addressed\nin further work. Datasets and baselines will be open sourced to encourage bridging the gap between\nspeech and text-based systems.\n2 Related work\nZero Resource Speech Challenge Series.Previous work (Versteegh et al., 2016; Dunbar et al.,\n2017, 2019, 2020) has focused on establishing benchmarks for unsupervised learning of an entire\ndialogue system, but has so far remained at a rather low level (acoustic, lexical). Acoustic modeling\n2\nhas used two metrics: ABX, a distance-based metric to be discussed later, and opinion scores on\nTTS output (whereby the discovered units are used to resynthesize speech). As for the lexical level,\npast work has focused on using the NLP metrics developed for word segmentation (Ludusan et al.,\n2014). However, these metrics assume that the models should discover words explicitly. The success\nof character-based language models suggests that it is possible to learn high-level linguistic concepts\nwithout explicitly segmenting words (see Hahn and Baroni, 2019).\nBlack box NLP. Among the variety of black-box linguistic tasks, psycholinguistically-inspired\nones enable direct comparison of models and humans. Grammaticality judgments for recurrent\nnetworks have been investigated since Allen and Seidenberg (1999), who use closely matched pairs of\nsentences to investigate grammatical correctness. This approach has recently been adopted to assess\nthe abilities of RNNs, and LSTMs in particular, in capturing syntactic structures. For instance, Linzen\net al. (2016) and Gulordava et al. (2018) use word probes in minimally different pairs of English\nsentences to study number agreement. To discriminate grammatical sentences from ungrammatical\nones, they retrieve the probabilities of the possible morphological forms of a target word, given the\nprobability of the previous words in the sentence. Practically, in the sentence “the boyis sleeping”,\nthey assume the network has detected number agreement if P(w = is) > P(w = are). This\nmethodology has also been adapted by Goldberg (2019) to models trained with a masked language-\nmodeling objective. Similarly, Ravfogel et al. (2018) use word probes to examine whether LSTMs\nunderstand Basque agreement and Godais et al. (2017) to test the lexical level in character-based LM.\n3 Methods\n3.1 Training set\nWe used as a training set the LibriSpeech 960h dataset (Panayotov et al., 2015). We also included in\nthis work the clean-6k version of the Libri-light dataset (Kahn et al., 2020) which is a huge collection\nof speech for unsupervised learning. A phonetic transcription of the LibriSpeech dataset was also\nemployed. To obtain this, we used the original LibriSpeech lexicon, as well as the G2P-seq2seq\ntoolkit2 to generate the phonetic transcriptions of words lacking from the lexicon. We generated\na forced-alignment version of Librispeech using the abkhazia library3. This enabled us to provide\ncomparative text-based topline systems along with the speech baseline.\n3.2 Metrics\nWe set up four metrics with their accompanying datasets, to evaluate the sLMs at four levels: phonetic\n(the Libri-light ABX metrics), lexical (the sWUGGY spot-the-word metrics), syntactic (the sBLIMP\nacceptability metrics) and semantic (the sSIMI similarity metric). The 4 datasets are composed\nof speech sounds extracted from LibriSpeech (sSIMI), or synthetic stimuli constructed with the\nGoogle API4 using 4 different voices, two males and two females (sWUGGY , sBLIMP, sSIMI)5.\nWhen synthetic, the stimuli were subsequently force-aligned to retrieve the phonetic boundaries. The\ndatasets containing words or sentences were ﬁltered to only contain the LibriSpeech vocabulary, and\nare split into dev and test sets.\nPhonetics: Libri-light ABX metrics.The ABX metric consists in computing, for a given contrast\nbetween two speech categories Aand B(e.g., the contrast between triphones ‘aba’ and ‘apa’), the\nprobability that two sounds belonging to the same category are closer to one another than two sounds\nthat belong to different categories. Formally, we compute an asymmetric score, withaand x, different\ntokens belonging to category A(of cardinality nA) and bbelonging to B(nB), respectively:\nˆe(A,B) := 1\nnA(nA −1)nB\n∑\na,x∈A\nx̸=a\n∑\nb∈B\n[\n1d(b,x)<d(a,x) + 1\n21d(b,x)=d(a,x)\n]\n(1)\n2https://github.com/cmusphinx/g2p-seq2seq\n3https://github.com/bootphon/abkhazia\n4https://cloud.google.com/text-to-speech\n5We use WaveNet voices A, C, D and F. All dev set stimuli are synthesised in all four voices. Stimuli in the\nsSIMI and sBLIMP test sets are split evenly among the four different voices, and sWUGGY uses all four for\neach test set stimulus.\n3\nThe score is symmetrized and aggregated across all minimal pairs of triphones like ‘aba’, ‘apa’,\nwhere the change only occurs in the middle phoneme. This score can be computed within speaker (in\nwhich case, all stimuli a, band xare uttered by the same speaker) or across speaker (aand bare from\nthe same speaker, and xfrom a different speaker). This score requires a pseudo-distance between\nacoustic tokens computed by averaging along a dynamic time warping path a framewise distance\n(KL or angular distance). This metric is agnostic to the dimensionality of the embeddings, can work\nwith discrete or continuous codes, and has been used to compare ASR speech features (Schatz, 2016).\nHere, we run this metric on the pre-existing Libri-light dev and test sets, which has been already used\nto evaluate several self-supervised models (Kahn et al., 2020; Rivière et al., 2020).\nLexicon: sWUGGY spot-the-word metrics. We built on Godais et al. (2017) which used the\n‘spot-the-word’ task. In this task, networks are presented with a pair of items, an existing word and a\nmatching nonword, and are evaluated on their capacity to attribute a higher probability to the existing\nword. The spot-the-word metric corresponds to the average accuracy of classifying the words and\nnonwords correctly across each pair.\nThe nonwords are produced with WUGGY(Keuleers and Brysbaert, 2010), which generates for a\ngiven word, a list of candidate nonwords best matched in phonotactics and syllabic structure. Because\nwe were aiming at speech stimuli, we needed additional constraints to ensure that (i) the audio\nsynthesis of the pairs would be of good quality, and (ii) that the pairs would have matching unigram\nand bigram scores relative to their phonemes. On a sample of 100 word/nonword pairs, and with\nfeedback from a native English speaker informant, we designed a synthesis-quality rule. The rule\nconsists of testing whether the original phonetic transcription matches the output of a back-to-back\nphoneme-to-grapheme (p2g) and grapheme-to-phoneme encoding (g2p).6 Only pairs where both the\nwords and nonwords passed this test were kept. We added additional constraints using a stochastic\nsampler to also match unigram and bigram phoneme frequencies (see Supplementary Material A).\nThe ﬁnal sWUGGY test and development sets consists of 20,000 and 5,000 pairs respectively, with\nthe existing words being part of the LibriSpeech train vocabulary. We also prepared additional\nOOV-sWUGGY test and development sets consisting of 20,000 and 5,000 pairs respectively, with\nexisting words which do not appear in the LibriSpeech training set.\nThe spot-the-word accuracy is the average of the indicator function 1PP (wordk)>PP (nonwordk) over\nthe set of pairs (wordk,nonwordk), where PP is a pseudo-probability (a possibly non-normalized\nnon-negative number) assigned to each input ﬁle by the model.\nSyntax: sBLIMP acceptability metrics. This part of the benchmark is adapted from BLIMP\n(Warstadt et al., 2019), a dataset of linguistic minimal sentence pairs of matched grammatical and\nungrammatical sentences. Similarly to the preceding test, the task is to decide which of the two\nmembers of the pair is grammatical based on the probability of the sentence. We adapted the code used\nto generate the BLIMP dataset (Warstadt et al., 2019) in order to create sBLIMP, speciﬁcally tailored\nfor speech purposes. In BLIMP, sentences are divided into twelve broad categories of syntactic\nparadigms. These categories are themselves divided into 68 speciﬁc paradigms containing 1000\nsentence pairs each, automatically generated using an expert hand-crafted grammar (this includes an\nadditional subcategory which was added to the code subsequent to Warstadt et al. (2019).\nTo make this dataset ‘speech-ready,’ we discarded ﬁve subcategories and slightly modiﬁed the\ngrammar for nine additional subcategories in order to ensure sentences had appropriate prosodic\ncontours. We also removed from the vocabulary all words absent from the LibriSpeech train set\n(Panayotov et al., 2015), as well as compound words and homophones that could cause further\ncomprehension issues once synthesised. 5000 sentence pairs were then generated for each of the 63\nremaining subcategories. We sampled sentence pairs from the generated pool to create a development\nand a test set, ensuring that the larger linguistic categories were sampled so as to balance the n-gram\nlanguage model scores (see Supplementary Material A). The test and development sets contain\n63,000 and 6,300 sentence pairs respectively, with no overlap in sentence pairs. Stimuli were then\nsynthesized and force-aligned as described at the beginning of the section.\nSimilar to the spot-the-word metric, the acceptability judgment metric requires a pseudo-probability\nfor each given input ﬁle. The sentence acceptability accuracy is reported similarly to the spot-the-word\naccuracy with the pairs of grammatical and ungrammatical sentences in the sBLIMP dataset.\n6We used the G2P-seq2seq toolkit.\n4\nLexical semantics: sSIMI similarity metrics.Here, the task is to compute the similarity of the\nrepresentations of pairs of words and compare it to human similarity judgements. Based on previous\nwork (Chung and Glass, 2018), we used a set of 13 existing semantic similarity and relatedness tests\nto construct our similarity benchmark. The similarity-based datasets include WordSim-353 (Yang\nand Powers, 2006), WordSim-353-SIM (Agirre et al., 2009), mc-30 (Miller and Charles, 1991), rg-65\nRubenstein and Goodenough (1965), Rare-Word (or rw) (Luong et al., 2013), simLex999 (Hill et al.,\n2015), simverb-3500 (Gerz et al., 2016), verb-143 (Baker et al., 2014) , YP-130 Yang and Powers\n(2006) and the relatedness-based datasets include MEN (Bruni et al., 2012), Wordsim-353-REL\n(Agirre et al., 2009), mturk-287 (Radinsky et al., 2011), mturk-771 (Halawi et al., 2012). All scores\nwere normalised on a 0-10 scale, and pairs within the same dataset containing the same pair of words\nbut in the opposite order were averaged. Pairs containing a word not in the LibriSpeech train set\nPanayotov et al. (2015) were discarded.\nWe selected as a development set the mturk-771 dataset, which was, in preliminary study using\ncharacter- and word-based LMs, both highly correlated with all other datasets and was large enough\nto be used as a development set. It was also ensured that no pair from the development set was\npresent in any of the test sets. All other twelve datasets were used as test sets. We then created two\nsubsets of audio ﬁles, one synthetic, one natural. For the ﬁrst, we followed the synthesis and forced\nalignment procedures described at the beginning of the section. For the second, we retrieved the audio\nextracts from LibriSpeech corresponding to each word, following the process presented in (Chung\nand Glass, 2018). The natural subset is therefore smaller than its synthesized counterpart as we had\nto discard pairs from the test and dev sets which were not present in the LibriSpeech test and dev sets\nrespectively. However, in this natural subset, each word may appear in multiple tokens, providing\nphonetic diversity; duplicated scores are averaged in the analysis step. The synthesised subset is\ncomposed of 9744 and 705 word pairs for the test and dev sets respectively, and the LibriSpeech\nsubset is composed of 3753 and 309 pairs for the test and dev sets.\nThe semantic similarity score is reported as the Spearman’s rank correlation coefﬁcientρbetween the\nsemantic distance scores given by the model and the true human scores in the dataset. Note that in\nthis work all the semantic similarity scores are multiplied by 100 for clarity.\n3.3 Models\nBaseline models. Our baseline models are a composite of three components: an acoustic model\n(CPC), a clustering module (k-means) and a language model (LSTM or BERT) varying in size.\nThe acoustic model is built upon Contrastive Predictive Coding (CPC, van den Oord et al. (2018)),\nwhere the representation of the audio is learned by predicting the future through an autoregressive\nmodel. In more detail, given an input signal x, the CPC model embeds x to a sequence of embeddings\nz = ( z1,...,z T ) at a given rate through a non-linear encoder genc. At each time step t, the\nautoregressive model gar takes as input the available embeddings z1,...,z t and produces a context\nlatent representation ct = gar (z1,...,z t). Given the context ct, the CPC model tries to predict the K\nnext future embeddings {zt+k}1≤k≤K by minimizing the following constrastive loss:\nLt = −1\nK\nK∑\nk=1\nlog\n[\nexp\n(\nz⊤\nt+kWkct\n)\n∑\n˜z∈Nt exp (˜z⊤Wkct)\n]\n(2)\nwhere Nt is a random subset of negative embedding samples, and Wk is a linear classiﬁer used to\npredict the future k-step observation. We used a PyTorch implementation of CPC7 (Rivière et al.,\n2020), which is a modiﬁed version of the CPC model that stabilizes the CPC training by replacing\nbatch normalization with a channel-wise normalization and improves the CPC model by replacing the\nlinear classiﬁer Wk in equation (2) with a 1-layer Transformer network (Vaswani et al., 2017). The\nencoder genc is a 5-layer 1D-convolutional network with kernel sizes of 10,8,4,4,4 and stride sizes of\n5,4,2,2,2 respectively, resulting in a downsampling factor of 160, meaning that the embeddings have\na rate of 100Hz. The autoregressive model gar is a multi-layer LSTM network, with the same hidden\ndimension as the encoder. For this baseline, we trained two different versions of CPC: CPC-small\nand CPC-big. Details are given in Table 2.\nAfter training the CPC model, we then train a k-means clustering module on the outputs of either the\nﬁnal layer or a hidden layer of the autoregressive model. The clustering is done on the collection of\n7https://github.com/facebookresearch/CPC_audio\n5\nTable 2: Characteristics of the baseline acoustic CPC models. We took the last LSTM layer of\nCPC-small and the second LSTM hidden layer of CPC-big as inputs to the clustering as they give the\nbest ABX scores (Supplementary Table S1).\nCPC conﬁguration Training data Input to kmeansModel autoregressive hidden units\nCPC-small 2-layer LSTM 256 LibriSpeech clean-100 LSTM level 2\nCPC-big 4-layer LSTM 512 Libri-light clean-6k LSTM level 2\nTable 3: Characteristics of the baseline LMs.L refers to the number of hidden layers; ED, HD\nand FFD refer to the dimension of the embedding layer, hidden layer, and feed-forward output layer\nrespectively; H refers to the number of attention heads in the BERT case.\nArchitecture nb Train Compute\nModel L ED HD FFD H parameters data Budget\nBERT 12 768 768 3072 12 90M LS960 48h - 32 GPUs\nBERT-small 8 512 512 2048 8 28M LS960 60h - 1GPU\nLSTM 3 200 1024 200 - 22M LS960 60h- 1GPU\nall the output features at every time step of all the audio ﬁles in a given training set. After training\nthe k-means clustering, each feature is then assigned to a cluster, and each audio ﬁle can then be\ndiscretized to a sequence of discrete units corresponding to the assigned clusters. The k-means\ntraining was done on the subset of LibriSpeech containing 100 hours of clean speech.\nFinally, with the discretized version of the audio ﬁles, we train language models on the discretized\nunits. We establish two ‘low budget’ and two ‘high budget’ baselines, based on the number of\nparameters and the compute resources necessary to train them. The high budget used a BERT-based\narchitecture (Devlin et al., 2019) trained either on CPC-small or CPC-big plus k-means-50 pretrained\nunits. The low budget architectures were a two-layer LSTM and a small BERT architecture (see Table\n3 for details); they both used the units from the CPC-big pretraining. Following Baevski et al. (2020a),\nwe trained the BERT models with only the masked token prediction objective. We also followed\nBaevski et al. (2020a) by masking a span of tokens in the input sequence instead of a single token\n(otherwise the prediction would be trivial to the model as discretized units tend to replicate). We\nmasked M consecutive tokens for each span, where M ∼N(10,10), with a total masking coverage\nof roughly half of the input tokens (spans may overlap). All models were trained on LibriSpeech\n960h. The BERT models were trained with a total batch size of 524k tokens, and the LSTM model\nwas trained with a total batch size of 163k tokens. The learning rate was warmed up to a peak value\nof 1 ×10−5. All the implementation was done via fairseq (Ott et al., 2019).\nThe Topline models. For topline comparison, we trained a BERT model on force-aligned\nphonemes using the gold transcription of the LibriSpeech dataset. We also employed the span\nmasking similarly to the baseline model. In addition to the BERT trained on forced alignments, we\nalso included a BERT model trained on the gold phonetic transcription of the LibriSpeech dataset,\nwith the difference that we only mask one token instead of a span of tokens. For an absolute topline\ncomparison, we used the pretrained RoBERTa large model (Liu et al., 2019), which was trained on\n50K subword units on a huge dataset of total 160GB, 3000 times bigger than the transcription of the\nLibriSpeech 960h dataset.\n4 Results\n4.1 Libri-light ABX\nComputing distances. We used the average angular distance (arccos of the normalized dot product)\nof the representations along the DTW-realigned path, as used by default in previous challenges\n(Versteegh et al., 2016; Dunbar et al., 2017, 2019). For our baseline models, we computed the ABX\nscores over one-hot representations of discretized units of the audio ﬁles.\nResults. We ﬁrst ran experiments varying the number of clusters. As seen in Supplementary Table\nS2, too few or too many clusters gives rise to worse ABX performance, with a sweet spot at 50\nclusters, which is the number we retain for the remainder of the paper. In Table 4, we present the\nresult of the ABX for our two models (CPC-small and CPC-big), before and after clustering. One can\nsee that the CPC-big model yields better performance than the CPC-small model (we retain the big\n6\nTable 4: Within and Across Speaker ABX error(lower is better) on Libri-light dev-clean and\n-other for two unsupervised models, before and after clustering (1-hot representations).\nwithin across\nEmbedding dev-clean dev-other dev-clean dev-other\nMFCC 10.95 13.55 20.94 29.4\nCPC-small 6.24 8.48 8.17 13.55\n+kmeans-50 10.26 14.24 14.17 21.26\nCPC-big 3.41 4.85 4.18 7.64\n+kmeans-50 6.38 10.22 8.26 14.86\nmodel for the rest of the experiments), and the clustering step yields an increase in error of between\n60-100%. Still, the performances are better than for an MFCC representation, with a much more\ncompact code.\n4.2 sWUGGY spot-the-word\nComputing the pseudo-probability. Given an audio ﬁle x, we ﬁrst discretize xinto a sequence of\ndiscretized units q1...qT . Then, following Salazar et al. (2020), we propose the following pseudo-\nprobability score for our BERT models trained with a span-masked token prediction objective:\nspan-PPMd,∆t(q1..qT ) =\n∏\ni=1+j∆t\n⌊(T−1)/∆t⌋≥j≥0\nP(qi..qi+Md |q1..qi−1qi+Md+1..qT ),\nwhere Md is a chosen decoding span size, and ∆tis a temporal sliding size. For the LSTM model,\nwe computed the probability of the discretized sequence with the classic left-to-right scoring style\nobtained by the chain rule: P(q1..qT ) = ∏T\ni=1 P(qi|q1..qi−1).\nResults. We determined the optimal masking (Supplementary Table S3) to be∆t= 5 and Md = 15.\nWe kept this setting for all other experiments involving pseudo-probabilities. Table 5 presents the\naverage of the four baseline systems and in Figure S1, the detailed performances of the baseline\ncompared to n-gram controls and toplines. The performance of all four baselines is consistently better\nthan chance and n-gram controls.\n4.3 sBLIMP acceptability\nComputing the pseudo-probability. We computed the pseudo-probability as in Section 4.2.\nResults. The aggregate results are shown in Table 5 and the detailed ones on the best system in\nTable S4. The results of this test, while above chance are considerably lower than the text-based\ntoplines.\n4.4 sSIMI semantic similarity\nComputing the distance. We computed the semantic distance between two audio ﬁles xand yas\nthe similarity between the two corresponding discretized sequences qx\n1 ...qx\nT and qy\n1 ...qy\nS. To obtain\nthis, we extracted outputs from a hidden layer of the LM to the two discretized sequences, aggregating\nthem with a pooling function to produce a ﬁxed-length representation vector for each sequence, and\ncomputed the cosine similarity between the two representation vectors:\ndSEM (x,y) = sim\n(\nfpool\n(\nh(i)(qx\n1 ...qx\nT )\n)\n,fpool\n(\nh(i)(qy\n1 ...qy\nS)\n))\n,\nwhere fpool is the pooling function and h(i)(·) is the output of the ith hidden layer of the LM.\nAs each word consists of possibly several voices, we averaged the similarity distance over pairs of\nthe same voice for the synthetic subset, and all possible pairs for the LibriSpeech subset.\nResults. For each model, we chose the pooling function and the hidden level that give the best\nscore on the dev set, and computed the score on the corresponding test set. The aggregate results are\nin Table 5, and a detailed layer-by-layer analysis in Table S5. The scores for semantic similarity are\noverall modest, compared to BERT systems trained on larger units (BPE). However, one can observe\nthat the best layers for semantic similarity occur towards the ﬁrst third of the transformer, and that\nmax pooling seems to be best. This contrasts with the best layers for acoustic similarity (as indexed\nby ABX), which occur at the extremities.\n7\n4.5 Model comparison\nThe overall results are in Table 5. They show that the four baseline models are above chance in the\nfour tasks, even low budget ones, although there is substantial variation between tasks. While task\nat the lexical level is substantially above chance, the syntactic and semantic tasks show room for\nimprovement compared to text-based toplines trained on similar amounts of data.\n5 Discussion\nTable 5: Overall performance of our baseline and topline models on dev and test sets on our four\nzero-shot metrics. For baseline models, the k-means training (k=50) was performed on LibriSpeech\nclean-100h, and the LSTM/BERT models was trained on discretized units of LibriSpeech 960h. For\ntopline comparisons, we included a BERT model trained on the forced aligned frames of LibriSpeech\n960h, a BERT model trained on the gold phonetic transcription of LibriSpeech 960h, and a RoBERTa\nlarge model pretrained on a text dataset 3000 times bigger than the transcription of LibriSpeech 960h.\nABX within ABX across sWUGGY sBLIMP sSIMI\nSystem Set clean other clean other synth. libri.\nLow budget baseline systems\nCPC-big+km50+BERT-smalldev 6.38 10.22 8.26 14.86 65.81 52.91 3.88 5.56\ntest 6.71 10.62 8.41 15.06 65.94 53.02 3.02 0.06\nCPC-big+km50+LSTM dev 6.38 10.22 8.26 14.86 66.13 53.32 4.42 7.56\ntest 6.71 10.62 8.41 15.06 66.22 52.89 7.35 6.66\nHigh budget baseline systems\nCPC-small+km50+BERT dev 10.26 14.24 14.17 21.26 70.69 54.26 2.99 6.68\ntest 10.07 14.71 13.45 22.42 70.50 54.61 8.96 -1.55\nCPC-big+km50+BERT dev 6.38 10.22 8.26 14.86 75.56 56.14 6.25 8.72\ntest 6.71 10.62 8.41 15.06 75.51 56.16 5.17 1.75\nTopline systems\nForced align BERT dev 0.00 0.00 0.00 0.00 92.19 63.72 7.92 4.54\ntest 0.00 0.00 0.00 0.00 91.88 63.16 8.52 2.41\nPhone BERT dev - - - - 97.90 66.78 9.86 16.11\ntest - - - - 97.67 66.91 12.23 20.16\nRoBERTa large dev - - - - 96.58 81.56 32.28 28.96\ntest - - - - 96.25 82.11 33.16 27.82\nWe introduced the new Zero Resource Speech Benchmark 2021 for spoken language models. It is\ncomposed of 4 zero-shot tests probing 4 linguistic levels: acoustic, lexical, syntactic and semantic.\nWe showed that a simple CPC+clustering+LM trained on LibriSpeech can perform above chance on\nall of these tests, outperforming n-gram models, while being worse than text-based models trained\non the same data. This shows both that the spoken LM task is feasible, and that there is room for\nimprovement.\nObvious directions for research include improving the representation learning component, the\nclustering methods, and the transformer, which have not been particularly tuned for this benchmark.\nThere are also end-to-end models like wav2vec (Baevski et al., 2020b) and other masking systems\n(Wang et al., 2020) that could be tried in this context. The performance gap between the RoBERTa\nlarge system and our toplines trained on LibriSpeech suggest that much is to be gained by increasing\nthe size of the training set, which can be obtained by large unlabelled audio datasets like LibriV ox.\nFinally, even though this benchmark is intended for developing speech technology for low resource\nlanguages, signiﬁcant resources are still required to construct the test sets and metrics (phonetic\ndictionary, aligned speech, grammar, TTS or trained speakers to make the stimuli). More work is\nneeded to reduce this footprint and scale up this benchmark to languages other than English.\nBroader Impact\nThe metrics developed here may help improve interpretability of unsupervised systems. Research\nwithin the Zero Resource setting may help for developing speech technology for low resourced\nlanguages, or for languages with no textual resources, which cannot be addressed in the supervised\nsetting. Even for high resource languages, learning a language model from raw speech would help\naddress dialect variation, including minorities, making speech technology more inclusive. Broadening\nthe reach of speech technology might be used to increase the economic dominance of already-large\nactors if developed with proprietary resources. To minimize this, we engage the community through\nan open source benchmark.\n8\nAcknowledgments\nThe work for MS, PR and for EDupoux and TAN in their EHESS role was supported by the Agence\nNationale de la Recherche (ANR-17-EURE-0017 Frontcog, ANR-10-IDEX-0001-02 PSL*, ANR-\n19-P3IA-0001 PRAIRIE 3IA Institute) and grants from CIFAR (Learning in Minds and Brains) and\nFacebook AI Research (Research Grant). The work for EDunbar was supported by a Google Faculty\nResearch Award and by the Agence Nationale de la Recherche (ANR-17-CE28-0009 GEOMPHON,\nANR-18-IDEX-0001 U de Paris, ANR-10-LABX-0083 EFL).\nReferences\nEneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009.\nA study on similarity and relatedness using distributional and wordnet-based approaches.\nJoseph Allen and Mark S Seidenberg. 1999. The emergence of grammaticality in connectionist\nnetworks. The emergence of language, pages 115–151.\nAlexei Baevski, Michael Auli, and Abdelrahman Mohamed. 2019. Effectiveness of self-supervised\npre-training for speech recognition. arXiv preprint arXiv:1911.03912.\nAlexei Baevski, Steffen Schneider, and Michael Auli. 2020a. vq-wav2vec: Self-supervised learning\nof discrete speech representations. In International Conference on Learning Representations.\nAlexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020b. wav2vec 2.0: A\nframework for self-supervised learning of speech representations.arXiv preprint arXiv:2006.11477.\nSimon Baker, Roi Reichart, and Anna Korhonen. 2014. An unsupervised model for instance level\nsubcategorization acquisition. In Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 278–289.\nElia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran. 2012. Distributional semantics\nin technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 136–145.\nYu-An Chung and James Glass. 2018. Speech2vec: A sequence-to-sequence framework for learning\nword embeddings from speech. arXiv preprint arXiv:1803.08976.\nYu-An Chung and James Glass. 2019. Generative pre-training for speech with autoregressive\npredictive coding. arXiv preprint arXiv:1910.12607.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. NAACL.\nEwan Dunbar, Robin Algayres, Julien Karadayi, Mathieu Bernard, Juan Benjumea, Xuan-Nga Cao,\nLucie Miskic, Charlotte Dugrain, Lucas Ondel, Alan W. Black, Laurent Besacier, Sakriani Sakti,\nand Emmanuel Dupoux. 2019. The zero resource speech challenge 2019: Tts without t.\nEwan Dunbar, Xuan Nga Cao, Juan Benjumea, Julien Karadayi, Mathieu Bernard, Laurent Besacier,\nXavier Anguera, and Emmanuel Dupoux. 2017. The zero resource speech challenge 2017.\nEwan Dunbar, Julien Karadayi, Mathieu Bernard, Xuan-Nga Cao, Robin Algayres, Lucas\nOndel, Laurent Besacier, Sakti Sakriani, and Emmanuel Dupoux. 2020. The zero resource\nspeech challenge 2020: Discovering discrete subword and word units. In INTERSPEECH,\nperception;bootstrapping/modeling;clustering/bootphon.\nManaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, and Chris Dyer. 2016. Problems with evaluation\nof word embeddings using word similarity tasks. arXiv preprint arXiv:1605.02276.\nDaniela Gerz, Ivan Vuli´c, Felix Hill, Roi Reichart, and Anna Korhonen. 2016. Simverb-3500: A\nlarge-scale evaluation set of verb similarity. arXiv preprint arXiv:1608.00869.\nGaël Godais, Tal Linzen, and Emmanuel Dupoux. 2017. Comparing character-level neural language\nmodels using a lexical decision task. pages 125–130.\n9\nYoav Goldberg. 2019. Assessing bert’s syntactic abilities.arXiv preprint 1901.05287.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically.\nMichael Hahn and Marco Baroni. 2019. Tabula nearly rasa: Probing the linguistic knowledge\nof character-level neural language models trained on unsegmented text. Transactions of the\nAssociation for Computational Linguistics (Accepted).\nGuy Halawi, Gideon Dror, Evgeniy Gabrilovich, and Yehuda Koren. 2012. Large-scale learning\nof word relatedness with constraints. In Proceedings of the 18th ACM SIGKDD international\nconference on Knowledge discovery and data mining, pages 1406–1414.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with\n(genuine) similarity estimation. Computational Linguistics, 41(4):665–695.\nJ. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P.E. Mazare, J. Karadayi, V . Liptchinsky,\nR. Collobert, C. Fuegen, and et al. 2020. Libri-light: A benchmark for asr with limited or no\nsupervision. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nK. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. van den Oord. 2020. Learning robust and\nmultilingual speech representations.\nEmmanuel Keuleers and Marc Brysbaert. 2010. Wuggy: A multilingual pseudoword generator.\nBehavior research methods, 42(3):627–633.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. TACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT\npretraining approach. CoRR, abs/1907.11692.\nBogdan Ludusan, Maarten Versteegh, Aren Jansen, Guillaume Gravier, Xuan-Nga Cao, Mark Johnson,\nand Emmanuel Dupoux. 2014. Bridging the gap between speech technology and natural language\nprocessing: an evaluation toolbox for term discovery systems. In Proceedings of LREC, pages\n560–567.\nMinh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations\nwith recursive neural networks for morphology. In Proceedings of the Seventeenth Conference on\nComputational Natural Language Learning, pages 104–113.\nGeorge A Miller and Walter G Charles. 1991. Contextual correlates of semantic similarity. Language\nand cognitive processes, 6(1):1–28.\nAäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive\npredictive coding. CoRR, abs/1807.03748.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nV . Panayotov, G. Chen, D. Povey, and S. Khudanpur. 2015. Librispeech: An asr corpus based on\npublic domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 5206–5210.\nKira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A word at a\ntime: computing word relatedness using temporal semantic analysis. In Proceedings of the 20th\ninternational conference on World wide web, pages 337–346.\nShauli Ravfogel, Francis M Tyers, and Yoav Goldberg. 2018. Can LSTM learn to capture agreement?\nthe case of basque. arXiv preprint 1809.04022.\n10\nMorgane Rivière, Armand Joulin, Pierre-Emmanuel Mazaré, and Emmanuel Dupoux. 2020.\nUnsupervised pretraining transfers well across languages.\nHerbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy.\nCommunications of the ACM, 8(10):627–633.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. 2020. Masked language\nmodel scoring. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 2699–2712, Online. Association for Computational Linguistics.\nT. Schatz, V . Peddinti, F. Bach, A. Jansen, H. Hermansky, and E. Dupoux. 2013. Evaluating speech\nfeatures with the minimal-pair abx task: Analysis of the classical mfc/plp pipeline.INTERSPEECH.\nThomas Schatz. 2016. ABX-discriminability measures and applications. Ph.D. thesis, Paris 6.\nTobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims. 2015. Evaluation methods for\nunsupervised word embeddings. In Proceedings of the 2015 conference on empirical methods in\nnatural language processing, pages 298–307.\nS. Schneider, A. Baevski, R. Collobert, and M. Auli. 2019. wav2vec: Unsupervised pre-training for\nspeech recognition. arXiv:1904.05862.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all you need. CoRR, abs/1706.03762.\nMaarten Versteegh, Xavier Anguera, Aren Jansen, and Emmanuel Dupoux. 2016. The zero resource\nspeech challenge 2015: Proposed approaches and results. Procedia Computer Science, 81:67–72.\nWeiran Wang, Qingming Tang, and Karen Livescu. 2020. Unsupervised pre-training of bidirectional\nspeech encoders via masked reconstruction. In ICASSP 2020-2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pages 6889–6893. IEEE.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and\nSamuel R Bowman. 2019. Blimp: A benchmark of linguistic minimal pairs for english. arXiv\npreprint arXiv:1912.00582.\nDongqiang Yang and David Martin Powers. 2006. Verb similarity on the taxonomy of WordNet.\nMasaryk University.\n11\nSupplementary Materials\nA Sampling method to balance ngram scores\nWe describe here our sampling method to balance ngram scores for sWUGGY and sBLIMP datasets.\nWe ﬁrst show the algorithm that we applied to sWUGGY , then we just modify slightly the algorithm\nfor the sBLIMP dataset.\nFor sWUGGY , let’s assume that we haveN words w1,...,w N ; and for each word wi, we have a\nlist of Kmatching nonword candidates nw1\ni ,...,nw K\ni . We also assume that each word or nonword\nwhas M scores s1(w),...,s M (w) (this might be unigram/bigram char/phone scores). We aim to\nchoose, for each word wi, a matching nonword nw∗\ni such that the proportion of the pairs where the\nscore of the word is higher than the score of nonword is close to 50% as possible, for each of M\nscores.\nIn other words, we want to build a list of word-nonword pairs L= {(w1,nw∗\n1),..., (wN ,nw∗\nN )}\nsuch that the objective function\nobj(L) =\nM∑\nm=1\n|accuracy_of_score_m(L)-0.5| (S1)\nis as close to zero as possible.\nWe thus deduce a simple sampling method as follows: We ﬁrst initialize a listLof chosen pairs of\nword and nonword. At each iteration, we randomly choose an unchosen word. Then we sample a\nnonword candidate in the list of matching nonword candidates, update the list with the new pair, and\ncompute the objective function of the new list as given in S1. If the objective increases, we remove\nthis newly added element, and resample a new nonword from the list of candidates. If we encounter\nall the nonword candidates but cannot ﬁnd a new pair, we randomly choose a nonword from the list\nof candidates. We then continue to the next word until all the words are chosen.\nWe found afterwards that if we sample all the words at the same time, we can obtain an overall score\nvery close to 50%, but then words with high frequency or with short length tended to have higher\naccuracy than others. We then decided to divide the words into sub-categories by frequency and word\nlength, and then do the sampling on each of the sub-categories, which gives a more balanced score on\nall the length and frequency levels.\nFor sBLIMP, the candidates are slightly different. We now have a list ofN pairs of grammatical and\nnon-grammatical sentences and we want to choose Kpairs among them such that the accuracy of\nthe chosen pairs is as close to 50% as possible as for sWUGGY . We can then use the same sampling\nmethod as described above, with the exception that instead of choosing a word and sampling the\nnonword candidates at each iteration, we sample an unchosen pair in the list of candidates, and add\nthat pair to the chosen list if we succeed to decrease the objective function.\nAs we also found that there is a huge difference in the accuracy scores of linguistic paradigms, we\ntried to do the sampling by each sub-paradigm. However, there were still some paradigms for which\nwe were not able to perfectly balance the score.\nB Supplementary ABX methods and results\nGiven two sounds x and y with two sequences of representations rx = rx\n1 ,...,r x\nT and ry =\nry\n1,...,r y\nS respectively, the ABX distance between xand yis computed as follows:\ndABX (x,y) = 1\n|pathDTW(rx,ry)|\n∑\n(i,j)∈pathDTW(rx,ry)\nsim(rx\ni ,ry\nj ). (S2)\nwhere sim(x,y) is the arc cosine of the normalized dot product between the embeddings xand y.\nTable S1 shows the ABX error on Libri-light dev-clean as a function of different hidden layer of the\nautoregressive network. We found that as long as we have a big autoregressive network, it is generally\nnot the last layer that brings the best phonetic information of the audio ﬁle.\n1\nTable S1: Within and Across Speaker ABX error (lower is better) on Libri-light dev-clean at different\nlevel of the autoregressive network of CPC-small and CPC-big models. Best layer for each model in\nbold.\nCPC-small CPC-big\nLSTM layer 1 2 1 2 3 4\nwithin 10.26 6.24 9.62 3.41 4.65 9.50\nacross 14.17 8.17 14.73 4.18 5.40 9.95\nTable S2 reports the ABX scores for different number of clusters, we also included multiple-group\nclustering in our experiences as similar to Baevski et al. (2020a). We found that the best score is\nobtained with 50 clusters. Using multiple groups do not further improve the quality of the discretized\nunits, this may be due to the fact that we only used one-hot information of the multiple groups (for\nexample, the two codes 26-20 and 26-10 represent two different one-hot units without any correlation).\nTable S2: Within and Across-Speaker ABX error rate (lower is better) on the LibriSpeech dev-\nclean dataset for CPC-small+kmeans (one-hot vectors embeddings) with different number of units\n(clusterings). Optimal number of clusters in bold.\nnunits 20 50 200 500 2000 50 x 2gr 320 x 2gr\nwithin 11.3 10.3 12.5 13.4 17.0 12.6 18.3\nacross 14.5 14.2 16.8 19.9 27.2 17.7 27.7\nC Supplementary spot-the-word results\nTable S3: Spot-the-word accuracy(higher is better) on sWUGGY dev as a function of the masking\nparameters to compute the pseudo-probabilities. The runtime is estimated based on the evaluation\ntime with the base parametersMd = ∆t= 10. In bold the compromise we selected between accuracy\nand speed.\nMd 5 10 15 20\n∆t 5 1 10 5 1 15 5 1 20 5 1\nscores 59.14 62.59 64.59 68.23 70.85 66.45 70.69 72.52 64.38 69.04 71.33\nruntime (est.) ×2 ×10 ×1 ×2 ×10 ×0.66 ×2 ×10 ×0.5 ×2 ×10\nTable S3 investigates the effect of the masking parametersMd and ∆tto the spot-the-word metrics.\nWe found that the way of computing log-probability can greatly inﬂuence the evaluation scores. We\nsee that as long as we overlap the masking spans more, the performance is better. In addition, given\nthat we masked spans of M ∼N(10,10) tokens during training, the best decoding masking size was\nfound to be 15. Considering the evaluation time, it is theoretically inversely proportional to ∆t, and\nwe thus decided to choose Md = 15 and ∆t= 5 for an accuracy and speed trade-off.\nFigure S1 shows the performance of the CPC-big system on the BERT-large architecture: they are\nworse than the toplines but well above chance. We reproduce the frequency effects (more frequent\nwords giving rise to better accuracies) and the length effect (longer words giving rise to better\naccuracies). This may be due to the fact that the phonetic space is sparser for long than for short\nwords. As a consequence, a short nonword like \"tup\" could be continued as a real word in multiple\nways (\"tuple\", \"tupperware\", etc.). In contrast, a long nonword can rarely be salvaged into a word (eg,\n’rhanoceros’ is a nonword very early on).\nD Supplementary grammaticality results\nTable S4 shows the detailed results on the various subsets of sBLIMP of our best model. Almost all\nof the subsets show better than chance scores (11/12), and of the phoneme ngrams controls (11/12),\n2\nFigure S1: Spot-the-word accuracy(sWUGGY dev set, higher is better, chance level at 50%) for our\nbest CPC+clustering+BERT model (blue), compared to phone ngram baselines (gray) and text-based\ntransformer toplines (orange). Left, word frequency effect. Right, word length effect.\nand most are better than the word ngrams controls (9/12 for unigram models, and 10/12 for bigram\nmodels).\nTable S4: Sentence acceptability accuracy(sBLIMP dev set, higher is better, chance level at\n50%) for our best CPC+kmeans 50+BERT model, compared to phone ngram baselines, text-based\ntransformer toplines, and human scores (from Warstadt et al., 2019).\nOverallAna. Agr.Agr. Str.BindingCtrl. Rais.D-N Agr.EllipsisFill. Gap.IrregularIslandNPI Li.QuantiﬁersS-V Arg.\nPhone Unigram48.29 50.00 50.00 52.90 50.00 50.00 50.00 50.00 45.50 50.00 38.36 39.33 50.00\nPhone Bigram50.20 50.50 50.11 52.40 49.80 50.12 50.00 49.88 50.00 49.93 50.00 50.00 50.00\nWord Unigram54.40 50.50 50.06 65.20 49.90 50.06 49.50 75.00 51.00 50.00 49.79 50.00 49.92\nWord Bigram51.64 50.00 50.06 66.50 50.00 50.06 49.00 50.00 50.00 50.07 50.00 57.00 49.92\nCPC-big+km50 BERT56.14 61.50 51.10 62.30 51.62 60.66 74.75 59.91 55.44 56.64 48.29 63.25 51.62\nForced phone BERT63.72 72.62 56.40 63.80 54.90 80.47 69.00 66.34 79.94 58.71 54.29 61.00 65.12\nPhone BERT66.78 72.50 59.89 54.40 62.20 92.25 75.00 63.75 82.50 57.71 54.57 81.67 70.17\nRoBERTa large81.56 98.50 74.33 80.40 78.20 95.88 99.00 73.62 89.50 68.71 80.71 90.67 87.83\nHuman (on BLIMP original)88.60 97.50 90.00 87.30 83.90 92.20 85.00 86.90 97.00 84.90 88.10 86.60 90.90\nE Supplementary semantic similarity results\nTable S5 shows the detailed sSIMI results, layer by layer of the best BERT model together with the\ndetailed ABX results on the same layers. This shows a complementarity of these two metrics (the\nbest layers for acoustics/phonetics are the worst for semantics and vice versa).\nTable S5: Comparison ofSemantic similarity scores(Spearman’s correlation with human judgement,\nhigher is better) on the sSIMI synthetic dev set and ABX scoreson Libri-light dev-clean on different\nembedding levels of our CPC-big+kmeans50+BERT model. CPC refers to the outputs of the second\nLSTM hidden layer of the CPC-big model, kmeans and outs refers to 1-hot representations before and\nafter the BERT model repsectively. The semantic similarity scores are also evaluated with different\npooling function (mean, max, min). Higher error rates than MFCC baseline in ABX and negative\nSIMI scores are in red. Note that all the semantic similarity scores are multiplied by 100.\nScore CPC kmeans BERT Layer\n0 1 2 3 4 5 6 7 8 9 10 11 12 logits outs\nABX\nwithin3.41 6.38 11.8221.9735.0242.5447.4044.4643.7141.7333.7619.6715.9115.933.30 3.655.65\nacross4.18 8.26 13.7724.5936.9543.9047.9445.5244.7643.1236.2923.1318.9218.844.11 4.597.32\nsSIMI\nmean - - -0.58-1.97-1.54 0 1.47 -0.381.04 2.26 1.71 2.26 1.47 2.96-0.57 - -\nmax - - -1.790.25 0.51 5.02 6.25 4.03 2.61 1.86 1.69 0.83 1.78 1.78 0.09 - -\nmin - - -3.3 -1.12-0.930.86 6.21 1.9 0.96 0.12 3.53 5.03 0.71 3.41 -0.9 - -\n3",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.8020639419555664
    },
    {
      "name": "Computer science",
      "score": 0.7768169641494751
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.6489848494529724
    },
    {
      "name": "Spoken language",
      "score": 0.5436614155769348
    },
    {
      "name": "Speech recognition",
      "score": 0.5350345969200134
    },
    {
      "name": "Language model",
      "score": 0.4834824204444885
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.482048362493515
    },
    {
      "name": "Natural language processing",
      "score": 0.480753630399704
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4197002053260803
    },
    {
      "name": "Linguistics",
      "score": 0.11112165451049805
    },
    {
      "name": "Computer network",
      "score": 0.05172106623649597
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210151031",
      "name": "Laboratoire de Sciences Cognitives et Psycholinguistique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    }
  ],
  "cited_by": 29
}