{
  "title": "Grimoire is All You Need for Enhancing Large Language Models",
  "url": "https://openalex.org/W4390781316",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2106511970",
      "name": "Ding Chen",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2110569463",
      "name": "Shichao Song",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2899466292",
      "name": "Qingchen Yu",
      "affiliations": [
        "Computer Algorithms for Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2103787844",
      "name": "Zhiyu Li",
      "affiliations": [
        "Computer Algorithms for Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2106414604",
      "name": "Wenjin Wang",
      "affiliations": [
        "Computer Algorithms for Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2538679894",
      "name": "Feiyu Xiong",
      "affiliations": [
        "Computer Algorithms for Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A1950145876",
      "name": "Bo Tang",
      "affiliations": [
        "Computer Algorithms for Medicine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4323697341",
    "https://openalex.org/W1970409510",
    "https://openalex.org/W4385571076",
    "https://openalex.org/W4389524585",
    "https://openalex.org/W3035309367",
    "https://openalex.org/W4387634446",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4320516905",
    "https://openalex.org/W4386942538",
    "https://openalex.org/W4385573504",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2956090150",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4286769130",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W4385573261",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2949380545",
    "https://openalex.org/W4225603598",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4281481998",
    "https://openalex.org/W4387687253",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W4387724290",
    "https://openalex.org/W4308900200"
  ],
  "abstract": "Abstract In-context Learning (ICL) is one of the key methods for enhancing the performance of large language models on specific tasks by providing a set of few-shot question and answer examples. However, the ICL capability of different types of models shows significant variation due to factors such as model architecture, volume of learning data, and the size of parameters. Generally, the larger the model's parameter size and the more extensive the learning data, the stronger its ICL capability. In this paper, we propose a method SLEICL (Strong LLM Enhanced ICL) that involves learning from examples using strong language models and then summarizing and transferring these learned skills to weak language models for inference and application. This ensures the stability and effectiveness of ICL. Compared to directly enabling weak language models to learn from prompt examples, SLEICL reduces the difficulty of ICL for these models. Our experiments, conducted on up to eight datasets with five language models, demonstrate that weak language models achieve consistent improvement over their own zero-shot or few-shot capabilities using the SLEICL method. Some weak language models even surpass the performance of GPT4-1106-preview (zero-shot) with the aid of SLEICL.",
  "full_text": "Grimoire is All You Need for Enhancing Large\nLanguage Models\nDing Chen \nBeihang University\nShichao Song \nRenmin University of China\nQingchen Yu \nInstitute for Advanced Algorithms Research, Shanghai\nZhiyu Li  (  lizy@iaar.ac.cn )\nInstitute for Advanced Algorithms Research, Shanghai\nWenjin Wang \nInstitute for Advanced Algorithms Research, Shanghai\nFeiyu Xiong \nInstitute for Advanced Algorithms Research, Shanghai\nBo Tang \nInstitute for Advanced Algorithms Research, Shanghai\nResearch Article\nKeywords: In-context Learning, large language models\nPosted Date: January 9th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3845612/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: The authors declare no competing interests.\nGrimoire is All Y ou Need for Enhancing Large Language Models\nDing Chen 1,∗ , Shichao Song 2,∗ , Qingchen Y u3 , Zhiyu Li 3,† , W enjin W ang3 , Feiyu\nXiong3 and Bo T ang3\n1Beihang University\n2Renmin University of China\n3Institute for Advanced Algorithms Research, Shanghai\nAbstract\nIn-context learning (ICL) is one of the key meth-1\nods for enhancing the performance of large lan-2\nguage models on speciﬁc tasks by providing a set3\nof few-shot question and answer examples. How-4\never, the ICL capability of different types of mod-5\nels shows signiﬁcant variation due to factors such6\nas model architecture, volume of learning data, and7\nthe size of parameters. Generally , the larger the8\nmodel’s parameter size and the more extensive the9\nlearning data, the stronger its ICL capability . In10\nthis paper, we propose a method S L EIC L (Strong11\nLLM Enhanced ICL) that involves learning from12\nexamples using strong language models and then13\nsummarizing and transferring these learned skills14\nto weak language models for inference and applica-15\ntion. This ensures the stability and effectiveness of16\nICL. Compared to directly enabling weak language17\nmodels to learn from prompt examples, S L EIC L re-18\nduces the difﬁculty of ICL for these models. Our19\nexperiments, conducted on up to eight datasets with20\nﬁve language models, demonstrate that weak lan-21\nguage models achieve consistent improvement over22\ntheir own zero-shot or few-shot capabilities using23\nthe S L EIC L method. Some weak language mod-24\nels even surpass the performance of GPT4-1106-25\npreview (zero-shot) with the aid of S L EIC L. The26\nsource code is available at GitHub 1 .27\n1 Introduction28\nThe continuous evolution of large language models has posi-29\ntioned In-context learning (ICL) prominently in foundational30\ncapabilities, attracting signiﬁcant research interest [Dong et31\nal., 2023 ]. ICL improves model performance in novel tasks32\nand enhances task-speciﬁc outcomes by providing a con-33\ncise set of prompt examples within a given task context. In34\ncontrast to parameter-based optimization methods like su-35\npervised ﬁne-tuning, ICL eliminates the need for extensive36\npre-use training or updates to the original large Language37\n∗ Equal contribution.\n† Corresponding author:lizy@iaar.ac.cn\n1 https://github.com/IAAR-Shanghai/Grimoire\nRegular In-context Learning\nthis is a stunning ﬁlm , a one-of-a-kind tour de force . subjective\nkatie is a young girl who loves to climb . objective\n—>\n—>\nit's a mystery how the movie could be released in this condition . —>\nClassify a sentence as subjective or objective: \nWeak Language Model\nsubjective\nobjective\nStrong LLM Enhanced In-context Learning\nHere are some Tips for Sentiment Classiﬁcation:\\n\\nTo classify a sentence as \nsubjective or objective, consider the presence of personal opinions, emotions, or \njudgments versus factual statements or descriptions...\nClassify a sentence as subjective or objective. \nit's a mystery how the movie could be released in this condition . —>\nWeak Language Model\nHere are some Tips…\nLearning from Examples Summary of Skills \nfor Answering \nQuestions\nStrong Language Model\n1\n2\n3 Learn from skills and \nanswer questions.\nGrimoire\nFigure 1: Compared to having a language model directly engage in\nRegular In-Context Learning (Regular ICL), Strong LLM Enhanced\nIn-Context Learning (S L EIC L) involves having a strong language\nmodel initially learn and summarize techniques based on represen-\ntative samples. Subsequently , the generated techniques (grimoire)\nare incorporated as part of the prompt to guide the weak language\nmodels in their responses.\nmodels, thus offering broader applicability . In a range of 38\nnatural language processing tasks—such as sentiment clas- 39\nsiﬁcation [\nSocher et al. , 2013 ], topic categorization [Zhang 40\net al. , 2015 ], and natural language inference [Dagan et al. , 41\n2005], the provision of well-crafted demonstration examples 42\nto less powerful models (e.g., those with 7B or 13B parame- 43\nters) often results in performance that matches or surpasses 44\nmore advanced models (such as GPT -4) [Lu et al. , 2022 ; 45\nQin et al. , 2023 ]. This phenomenon signiﬁcantly fuels re- 46\nsearchers’ interest in exploring the potential of ICL.47\nPrior research on ICL has emphasized the importance of48\nselecting and utilizing demonstration examples [Liu et al. ,49\n2022]. Studies suggest that the number, quality , and rele-50\nvance of these demonstration examples, along with their se-51\nquential ordering in the prompt queue, markedly affect the52\nefﬁcacy of ICL in LLMs [Brown et al. , 2020 ; Milios et al. ,53\n2023]. This phenomenon stems from the diverse learning and54\nknowledge transfer capabilities inherent in models of varied55\nscales and architectures. For instance, given identical input56\nexample pairs (x1, y1),(x2, y2),(x3, y3), weak models fre-57\nquently demonstrate a reduced proﬁciency in approximating58\ny compared to their more advanced counterparts. Conse-59\nquently , rather than relying on weak models to directly assim-60\nilate these examples, a more efﬁcacious strategy may involve61\nharnessing the learning prowess of stronger models. This can62\nbe achieved by isolating the process of ﬁtting demonstration63\nexamples and utilizing the empirical function f(x) → y, in-64\nferred from more stronger models, to direct the learning in65\nweaker models. Ultimately , this approach serves to diminish66\nthe complexity involved in direct learning from demonstra-67\ntions, thus bolstering the performance of weak models.68\nAs depicted in Figure 1, this paper presents a new paradigm69\nfor augmenting ICL, denominated as S L EIC L (Strong LLM70\nEnhanced ICL). This method exploits the superior capabili-71\nties of strong LLM to assimilate functions or problem-solving72\nskills from demonstration examples, metaphorically termed73\nas a Grimoire. Once the grimoire is generated by the strong74\nLLM, it serves as a substitute for the original prompt exam-75\nples, thus streamlining the learning process for weak models.76\nIn conventional ICL methodologies, LLMs generally must77\nnavigate through a collection of demonstration examples for78\neach query [Rubin et al. , 2022 ], choosing the most suitable79\nexamples to optimize performance. Conversely , S L EIC L ne-80\ncessitates only a solitary instance of example selection for a81\ndesignated task. Once the grimoire is produced, it can direct82\nweak models in addressing queries within that task, yielding83\nresults that exceed those achievable through regular ICL.84\n2 Related W orks85\n2.1 In-context Learning of Large Language Model86\nICL has emerged as a novel learning paradigm, initially pro-87\nposed and applied in the pre-training of GPT -3 [Brown et al. ,88\n2020]. ICL can efﬁciently learn tasks with a small number89\nof prompt examples, without the parameter updates. Why90\nICL is effective has sparked widespread discussion. [Chan91\net al. , 2022 ] suggests that the ICL capability of models is92\ndriven by the distributional characteristics of the data itself,93\nemphasizing the importance of data structure. [Xie et al. ,94\n2022] suggests that contextual learning takes place as the lan-95\nguage model infers shared latent concepts among examples96\nwithin prompts. [Garg et al. , 2022 ] indicates that models can97\nlearn speciﬁc functions based on encoded prompt samples,98\nachieving performance comparable to speciﬁc task learning99\nalgorithms. Combining the above works, we ﬁnd that the100\nICL capabilities of models are more derived from learning101\nthe distributional features of example samples or underlying102\nrules, rather than necessarily relying on the speciﬁc exam-103\nples. Moreover, the greater the parameter size of large lan- 104\nguage models, the more robust their corresponding ICL ca- 105\npabilities [Brown et al. , 2020 ; Y oo et al. , 2022 ], establishing 106\nthe theoretical foundation for our work. 107\n2.2 Prompt Engineering of Demo Examples 108\nExtensive research indicates that the construction of demon- 109\nstration examples is crucial for the performance of ICL [Y oo 110\net al. , 2022 ; Qin et al. , 2023 ]. Furthermore, recent research 111\nhas enhanced ICL performance by optimizing both the char- 112\nacteristics of demonstration examples and the order and se- 113\nlection strategies of example samples. 114\nCharacteristics of Demonstration Examples . The study 115\nby [Min et al. , 2022 ] highlights that the inﬂuence of prompt 116\nsamples on ICL performance is attributable to four princi- 117\npal elements: input-label mappings, label space, and sam- 118\nple distribution. Nonetheless, it is notable that opinions di- 119\nverge concerning the effect of input-label mappings relation- 120\nships—namely , label accuracy—on ICL performance. Cer- 121\ntain studies propose that input-label mappings relationships 122\npotentially inﬂuence ICL performance, with [Pawelczyk et 123\nal., 2023 ] observing that label inversion within contextual ex- 124\namples markedly impacts the outputs of the Bloom model. 125\nIn contrast, ﬁndings by [W ei et al. , 2023 ] demonstrate that 126\nlarger models generally exhibit more robust ICL capabilities 127\nand excel in deciphering label mapping relationships from 128\ncontextual examples relative to their smaller counterparts. 129\nOrdering of Demonstration Examples . The ordering of 130\nsamples represents a critical aspect of prompt sample con- 131\nstruction, with various sorting methods leading to differences 132\nin ICL performance [Zhao et al. , 2021 ]. The study by [Liu 133\net al. , 2022 ] introduced KA TE, a system that selects demon- 134\nstration examples based on semantic similarity . Expanding 135\non this, they examined various ordering methods and found 136\nthat their impact on KA TE’s performance was minimal. Re- 137\nsearch from [Lu et al. , 2022 ] employed GlobalE and LocalE 138\nto sequence demonstration examples and uncovered a posi- 139\ntive relationship between information entropy and ICL per- 140\nformance. As the parameter scale increases, models become 141\nmore proﬁcient at ICL, and their sensitivity to sample order- 142\ning diminishes accordingly [Milios et al. , 2023 ]. 143\nSelecting Demonstration Examples . Selection of demon- 144\nstration examples is a pivotal stage in crafting prompt sam- 145\nples, with a substantial effect on ICL performance [Liu et 146\nal., 2022 ]. Presently , selection of demonstration examples 147\nmethodologies are primarily categorized into three types: se- 148\nmantic similarity [Liu et al. , 2022 ; Su et al. , 2022 ], cluster- 149\ning [Zhang et al. , 2023 ], and information entropy [Liu and 150\nW ang, 2023]. Moreover, certain studies have introduced spe- 151\ncialized models to score demonstration examples, aiming to 152\nselect representative demonstration examples [Rubin et al. , 153\n2022; Wu et al. , 2023 ; Li and Qiu, 2023 ]. Previous re- 154\nsearch on the selection of demonstration examples can be 155\ncategorized based on the granularity of selection. One strat- 156\negy involves obtaining instance-level examples, where re- 157\ntrieval is performed for every test query [Liu et al. , 2022 ; 158\nSu et al. , 2022 ; Rubin et al. , 2022 ; Wu et al. , 2023 ]. Al- 159\nternatively , task-level example retrieval is utilized as prompts 160\nfor all test samples [Zhang et al. , 2022 ; Liu and W ang, 2023 ; 161\nthis is a stunning ﬁlm , a one-of-a-kind tour de force . subjective\nkatie is a young girl who loves to climb . objective\n—>\n—>\nthe movie's biggest oﬀense is its complete and utter lack of tension . —> subjective\nhe tells her to keep ﬁghting for her goals but still to compromise . —> objective\nshaken , shae goes to check on her young daughter , sophie . —> objective\n…… —> ……\nRepresentative Samples Selection\nthe movie's … tension . —> subjective\nshaken , … , sophie . —> objective\nthe movie's … tension . —> subjective\nkatie … to climb . —> objective\nthis is… de force . —> subjective\nkatie … climb . —> objective\nhe … compromise . —> objective\nkatie … to climb . —> objective\nKCS\nKCS\nHCS\nHSS\nRSS\nStrong\nLLM\nSimple\nProfound\nKCS-PG\nQuestion N\nRanking\nKCS-SG\nHCS-PG\nHCS-SG\nHSS-PG\nHSS-SG\nRSS-PG\nRSS-SG\nHSS-SG\nAnswer\nQuestion List\nQuestion 1\nQuestion 2\nQuestion N\nQuestion N\nGrimoire Generation\nGrimoire Ranking\n1\n2\n3\n4\n5\nWeak LLM\nZero-Shot -PG\nZero-Shot -SG\nFigure 2: Framework of proposed S L EIC L method. First, multiple sets of representative samples are obtained using different sample selection\nmethods (KCS, HCS, HSS, RSS), with each set sampled in a stratiﬁed manner based on labels. Subsequently , corresponding profound\ngrimoires (PG) and simple grimoires (SG) are generated based on each sample set. Additionally , zero-shot-PG and zero-shot-SG, generated\nwithout samples, are included. Finally , all grimoires are ranked based on given test samples, and the optimal grimoire is handed over to the\nweak LLM for response.\nLi and Qiu, 2023 ], which is less resource-intensive compared162\nto instance-level retrieval. However, such selected samples163\nmay not be sufﬁciently representative or could be substan-164\ndard, potentially resulting in modest improvements in ICL165\nperformance or reduced stability . Researchers and practition-166\ners must deliberate these aspects when advancing and imple-167\nmenting ICL methodologies to ensure the most effective de-168\nployment of demonstration examples.169\n3 Enhancing LLMs with Grimoire170\n3.1 Problem Formulation171\nIn the ICL scenario, the key is to ﬁnd suitable demonstration172\nexamples to achieve higher prediction accuracy . Conversely ,173\nin the S L EIC L scenario, the focus is on ﬁnding an appropriate174\ngrimoire. Speciﬁcally , given a task T that needs to be solved175\nby a weak language model W, along with a training set TT176\nand a validation set TD, our objective is to identify the opti-177\nmal grimoire gi ∈ G produced by the strong language model178\nL that enhances the performance of W on TD compared to179\nICL prompting W with n-shot examples, S n ∈ TT :180\n\n\n\ng∗ = arg maxgi∈G SL EIC L (WTD |gi)\nS∗\nn = arg maxSn∈ TT ICL (WTD |Sn)\nSL EIC L (WTD |g∗ ) > ICL (WTD |S∗\nn)\n(1)\nIn Equation\n1, S L EIC L(·) denotes prompting with gri-181\nmoire, and ICL (·) denotes ICL prompting. Figure 2 shows182\nthe enhancing framework based on strong language model183\nL. Initially , we select a representative set of examples S n184\nfrom the training set TT provided by the task. In this185\nstep, we designed four different selection methods S n ∈186\n{KCS(n),HCS(n),HSS(n),RSS(n)} to ﬁnd better demon-187\nstration examples. Subsequently , even among weak language188\nmodels, there are variations in learning capabilities. Hence,189\nwe have speciﬁcally designed two distinct paradigms for gen-190\nerating grimoires, namely the Profound Grimoire (PG) and191\nthe Simple Grimoire (SG). Ultimately , by combining four ex- 192\nample selection methods with the two grimoire generation 193\nstrategies, we are able to create a candidate set comprising 194\neight different grimoires for a speciﬁc task. In addition, we 195\nalso added PG and SG generated without samples(zero-shot). 196\nIn the ﬁnal stage of task evaluation, we implement a grimoire 197\nranking algorithm. This algorithm is designed to select the 198\npotentially optimal grimoire for enhancement, corresponding 199\nto different problems, thereby further improving the perfor- 200\nmance of the weak language models. 201\n3.2 Representative Samples Selection 202\nThis process involves choosing a subset of data that captures 203\nthe underlying patterns and complexities of the entire sam- 204\nples, aiming to improve the efﬁciency and effectiveness of 205\ngrimoire generation. V arious methods have been developed 206\nto tackle this challenge, each with its own methodology and 207\nfocus. 208\nK-means Clustering Selection (KCS) 209\nK-means Clustering Selection refers to the process of using 210\nthe k-means algorithm to cluster the semantic representations 211\nof a sample set and selecting the nearest n samples to each of 212\nthe k cluster centers as representative samples. Consequently , 213\na collection of n ∗ k exemplary sample sets is obtained. W e 214\nbelieve that a diverse set of representative samples may po- 215\ntentially enhance the performance of large models [Zhang et 216\nal., 2023 ], and is more conducive to allowing strong models 217\nto generalize answer skills on a holistic level. This, in turn, 218\nincreases the universality of the ﬁnal generated grimoire with- 219\nout resulting in localized optimization. In KCS, the hyperpa- 220\nrameters include the number of clusters k, and the number of 221\nsamples n in each cluster. 222\nHierarchical Clustering Selection (HCS) 223\nThe Hierarchical Clustering Selection method employs the 224\nhierarchical clustering algorithm to perform a detailed hier- 225\narchical clustering analysis of the sample set. HCS selects 226\nrepresentative samples from the cluster centers identiﬁed at227\nvarious levels in a dendrogram, aiming to capture and dis-228\nplay the rich hierarchical semantic features within the sam-229\nples. HCS not only reveals subtle associations between sam-230\nples, but also has the advantage of not requiring a predeﬁned231\nnumber of clusters, making HCS more advantageous when232\ndealing with datasets that have complex semantic structures.233\nMoreover, by providing multi-level semantic feature repre-234\nsentations, HCS adds a more diversiﬁed basis for choices to235\nthe subsequent grimoire ranking algorithms, thus allowing a236\nmore precise reﬂection of the true semantic relationships be-237\ntween samples, and further optimizing the effectiveness and238\nquality of ranking.239\nHard Samples Selection (HSS)240\nHard Samples Selection refers to selecting samples that are241\neasily mispredicted by weak models as representative sam-242\nples. Hard examples may contain information and knowledge243\nthat are either lacking or only partially understood by weak244\nmodels in solving a given task. Thus, we aim to effectively245\ncompensate for the deﬁciencies and insufﬁciencies of weak246\nlanguage models in addressing speciﬁc problems by using247\nstrong language models to extract and reﬁne the skills needed248\nto solve these hard examples. Our ﬁrst step involves con-249\nducting zero-shot testing on the training sets using the weak250\nmodel, and recording the predicted labels for each sample.251\nWhen we perform hard sample selection, we choose a ﬁxed252\nproportion of hard samples based on a given ratio. For in-253\nstance, when the ratio is set to 0.3, it means selecting 30% of254\nsamples from the mispredicted examples and the remaining255\n70% from the correctly predicted ones.256\nRandom Samples Selection (RSS)257\nRandom Samples Selection is a method that selects represen-258\ntative samples from a dataset in an non-discriminatory man-259\nner. Samples are picked entirely at random. This approach260\nis beneﬁcial for maintaining an impartial sample distribution,261\nparticularly when little is known about the datasets structure262\nor when seeking a baseline method for comparison with more263\ncomplex selection methods like KCS, HCS, and HSS. RSS’s264\nsimplicity makes it efﬁcient for large datasets and useful for265\npreliminary explorations or alongside other selection meth-266\nods.267\n3.3 Grimoire Generation268\nUpon completing the selection of representative examples,269\nit becomes imperative to employ strong language models to270\ngenerate content aimed at guiding weak language models in271\nresponding. This generated content has been termed ”Gri-272\nmoire”. However, given the substantial variations in ICL273\ncapabilities among language models of different parameter274\nsizes, employing a singular grimoire generation approach275\nfor all weak language models presents a challenge. Conse-276\nquently , we have devised two fundamental types of grimoire277\ngeneration paradigms: Profound Grimoire Generation and278\nSimple Grimoire Generation. Our hypothesis posits that for279\nlarger models with enhanced reasoning and comprehension280\nskills, Profound grimoire tends to yield superior outcomes281\nthrough the use of detailed skill explanations and the gen-282\neration of diverse answers based on reference samples. In283\nPlease summarize rules (or skills) for solving this task based \non the task description and examples below.\nNote that the output should satisfy:\n1. The rules can be explained with examples if necessary.\n2. The method of applying the rule needs to be given.\n3. Don't be too long.\nTask:\n{task_description}\nExamples:\n{examples}\nProfound Grimoire\nSimple Grimoire\n1\nPlease abbreviate the following content in simpler and more \nstraightforward language, and try to retain important \ninformation.\nNote that the output should satisfy:\n1. Provide no more than three general rules.\n2. Try to be as brief as possible.\nContent:\n{                                    }\n2\nProfound Grimoire\nStrong LLM\nStrong LLM\nPrompt 1\nPrompt 2\nFigure 3: W orkﬂow for grimoire generation.\ncontrast, weake language models are more likely to attain im- 284\nproved results with more concise grimoires featuring straight- 285\nforward examples. 286\nProfound Grimoire (PG) 287\nThe profound grimoire is characterized by an abundance of 288\ninformation and details, necessitating that the guided weak 289\nlanguage model possesses robust ICL abilities. Consequently , 290\na specialized prompt template for generating PG has been de- 291\nveloped, as illustrated in Figure 3. The essence of this prompt 292\ntemplate involves compelling the strong language model to 293\nsynthesize the skills essential for solving the given task, draw- 294\ning upon the provided task description and representative 295\nsamples. W e anticipate utilizing the strong language model’s 296\nexceptional summarization and abstraction capabilities to dis- 297\ntill universal solutions for speciﬁc tasks, providing explana- 298\ntions alongside the given samples, as well as methods for skill 299\napplication, mirroring the instructional approach of human 300\nteachers. This facilitates the weak language model’s compre- 301\nhension of task requirements and accelerates its learning of 302\nthe necessary skills for task resolution. The ultimate stipula- 303\ntion in the template is to constrain the length of the strong lan- 304\nguage model’s output, thereby preventing the grimoire from 305\nsurpassing the weak language model’s context capacity , as 306\nan excessively extended grimoire could yield detrimental out- 307\ncomes. 308\nSimple Grimoire (SG) 309\nThe simple grimoire represents a simpliﬁed version of the PG. 310\nCrafted for greater conciseness and clarity , it retains critical 311\ninformation from the PG, thereby enabling language mod-312\nels with lesser ICL capability to comprehend the grimoire313\nand distill the essential information and methodologies for314\naddressing speciﬁc tasks (T emplate illustrated in Figure 3).315\nThe prompt template for generating SG was not reconﬁgured316\nto maintain the uniformity in content and skill articulation317\nbetween SG and PG. Employing a simpliﬁed methodology318\nguarantees robust consistency between these two grimoires319\nfor the identical task. The beneﬁt of this consistency lies in320\nthe fact that when comparing the impacts of both grimoires321\non the same weak language model, it becomes clear that any322\nefﬁcacy disparities are primarily attributed to the complexity323\nof the articulation, rather than variances in content and mean-324\ning. This approach aids users in selecting the more appropri-325\nate grimoire when the suitability for a speciﬁc weak language326\nmodel’s comprehension capabilities is ambiguous.327\n3.4 Grimoire Ranking328\nBy integrating four representative samples selection methods329\n(KCS, HCS, HSS and RSS) and zero-shot method with two330\ngrimoire generation paradigms (PG and SG), for each task,331\nwe are able to obtain ten types of grimoires:332\ngi ∈ G = {KCS,HCS,HSS,RSS,Zero-Shot}×{ PG,SG} (2)\nAmong these, the best grimoire will be chosen to serve as333\nthe prompt for our S L EIC L method. Hence, the formulated334\nobjective is to identify the optimal grimoire g∗ from the can-335\ndidate set G for a given task qj in the test dataset TD:336\n∀qj ∈ TD, g∗ = arg max\ngi∈G\nu (qj , gi|W) (3)\nIn Equation 3, u (qj , gi|W) denotes the utility of grimoire337\ngi for query qj on a speciﬁc weak language model W. There-338\nfore, the crux of the problem shifts to how to deﬁne this utility339\nfunction. Here, we propose two approaches, similarity-based340\nmethod and classiﬁer-based method.341\nSimilarity-based method . The simplest way to evaluate342\nutility is to calculate the similarity between query and gri-343\nmoire. Therefore, we employ an embedding model from the344\nMPNet[Song et al. , 2020 ] to embed both the query and the345\ngrimoire. Subsequently , the utility is determined by calculat-346\ning the cosine similarity as shown in Equation 4. Certainly ,347\nthis approach solely relies on similarity , disregarding critical348\nfeatures like task type and grimoire type. Consequently , it349\ncannot effectively model the utility function. Hence, we opt350\nto pre-train a deep neural network.351\nusim (·) ≜ cos(embedgi,embedqj ) (4)\nDual-tower deep neural network classiﬁer method . This352\nmethod is composed of multiple layers of neural networks, as353\nillustrated in simpliﬁed form by Equation 5. The dense (·)354\nportion consists of a two-layer densely connected neural net-355\nwork with residual connections. Then, a self-attention mech-356\nanism transforms the concatenated query and grimoire, and357\nthe resulting joint representation is fed into a classiﬁcation358\nhead with four layers. For detailed feature engineering, neu-359\nral network architecture and training process, please refer to360\nthe appendix.361\n{ joint = dense(gi) ⊗ dense(qj )\nutower(·) ≜ classiﬁer-head (self-attn(joint)) (5)\n4 Experiments 362\n4.1 Datasets 363\nOur evaluation encompassed eight datasets across four task 364\ncategories: Sentiment Analysis (SST5 [Socher et al. , 2013 ] 365\nand Subj [Pang and Lee, 2004 ]), T opic Classiﬁcation (Ag- 366\nNews [Zhang et al. , 2015 ] and TREC [Li and Roth, 2002 ; 367\nHovy et al. , 2001 ]), Natural Language Inference (R TE [Da- 368\ngan et al. , 2005 ] and QNLI [Rajpurkar et al. , 2016 ]), and 369\nHate Speech Detection (hate sp18 [de Gibert et al. , 2018 ] 370\nand ethos [Mollas et al. , 2020 ]). A series of preprocess- 371\ning activities were conducted on the original data from these 372\ndatasets to transform all samples into a consistent and uni- 373\nﬁed format, thus enabling smooth subsequent testing. Ini- 374\ntially , samples from the original dataset that are excessively 375\nlengthy were excluded to prevent the issue of prompts ex- 376\nceeding the context window limits of smaller models during 377\nfew-shot testing. Subsequently , the original dataset was parti- 378\ntioned into a training set and a test set (unless pre-segmented), 379\nwith the training set comprising 2000 samples and the test set 380\n1000 samples. In cases where the total number of samples in 381\nthe original dataset fell below 3000, a distribution ratio ex- 382\nceeding 2:1 was maintained between the training and test sets 383\nto ensure sufﬁcient samples for subsequent example instance 384\nselection in the prompts. 385\n4.2 Conﬁguration and Metrics 386\nLarge Language models . W e evaluate 6 language mod- 387\nels, including GPT3.5-Turbo (175B 2 )3 , LLaMA2 Chat (70B, 388\n13B) [T ouvron et al. , 2023 ], Baichuan2 (7B) [Y ang et al. , 389\n2023], GPT4-1106-preview 4 , Phi-2 (2.7B) 5 . Among them, 390\nGPT3.5-Turbo and GPT4-1106-review are two important 391\nLLMs developed by OpenAI. The LLaMA2 Chat series 392\nmodel is an open-source chat model generated by Meta, and 393\nwe plan to evaluate the 70B and 13B chat models in this 394\nseries. Baichuan2 is an open-source LLM developed by 395\nBaichuan Inc. , and we plan to evaluate the 7B chat model in 396\nthis series. Phi-2 is a small language model (SLM) developed 397\nby Microsoft. Among these models, GPT4-1106-review will 398\nbe used as a strong language model for generating grimoires. 399\nBaseline. W e designed the following two types baselines 400\nto compare with our method: zero-shot and few-shot (n=4). 401\nSingle Grimoire . W e evaluated the ten types of grimoires 402\nin Equation 2, where representative samples are stratiﬁed by 403\nlabel ( k samples per label). 404\nSL EIC L. W e evaluated the two types of methods proposed 405\nin Section 3.4: similarity-based method and classiﬁer-based 406\nmethod. 407\n2 175B is an estimated value.\n3 https://openai.com\n4 https://openai.com/blog/new-models-and-developer-products-\nannounced-at-devday\n5 https://www .microsoft.com/en-us/research/blog/phi-2-the-\nsurprising-power-of-small-language-models/\nT able 1: Detailed evaluation results of GPT3.5-Turbo enhanced by grimoire.\nModel Submodel n/k-shot\nSentiment Analysis T opic Classiﬁcation Natural Language\nInference Hate Speech Detection\nA VG\nSST5 Subj AgNews TREC R TE QNLI hate sp18 ethos\nGPT3.5-Turbo Zero-Shot - 53.87% 39.00% 83.82% 81.69% 77.67% 67.40% 78.76% 81.20% 70.43%\nFew-Shot n=4 54.07% 43.67% 85.75% 77.76% 79.07% 74.80% 68.31% 76.87% 70.04%\nSingle Grimoire (GPT3.5-Turbo)\nKCS-PG k=4 54.04% 60.60% 81.87% 76.81% 80.20% 75.50% 78.24% 84.20% 73.93%\nKCS-SG k=4 50.23% 50.80% 84.00% 74.47% 79.93% 73.60% 75.23% 84.00% 71.53%\nHCS-PG k=4 50.90% 43.87% 79.53% 73.20% 74.20% 76.27% 83.67% 81.80% 70.43%\nHCS-SG k=4 50.20% 45.20% 85.13% 80.49% 80.80%\n75.20% 78.89% 83.00% 72.36%\nHSS-PG k=4 (r=1.0) 51.60% 65.00% 84.80% 83.01% 75.80% 74.67% 82.98% 83.33% 75.15%\nHSS-SG k=4 (r=1.0) 55.27% 48.80% 83.80% 82.43% 80.13% 73.07% 77.87% 84.07% 73.18%\nHSS-PG k=4 (r=0.5) 50.97% 46.93% 85.31% 79.67% 78.07% 68.60% 86.27% 84.07% 72.49%\nHSS-SG k=4 (r=0.5) 53.10% 47.80% 84.27% 72.54% 81.47% 73.33% 79.01% 85.33% 72.11%\nRSS-PG k=4 51.57% 52.67% 82.47% 72.03% 80.40% 79.40% 85.34% 84.13% 73.50%\nRSS-SG k=4 54.30% 51.13% 82.53% 75.53% 78.47% 77.00% 75.50% 83.20% 72.21%\nZero-Shot-PG - 49.50% 57.13% 82.31% 66.69% 76.20% 72.33% 81.93% 84.13% 71.28%\nZero-Shot-SG - 50.67% 53.47% 82.38% 75.10% 77.13% 74.87% 78.77% 84.47%\n72.11%\nSL EIC L (GPT3.5-Turbo) Similarity-based - 52.97% 58.53% 82.64% 78.34% 76.60% 73.53% 79.21% 83.87% 73.21%\nClassiﬁer-based - 52.23% 59.13% 83.06% 79.03% 79.07% 74.32% 79.93% 83.80% 73.82%\nNote: - indicates that this hyper-parameter is invalid for the current test; n-shot indicates that n samples will be provided for each prediction; k-shot provides a selection of k samples under each\nlabel to generate grimoire; r represents the sampling ratio of hard samples. The best performance in each column will be bolded, and the second-best performance will be underlined.\n4.3 Performance Analysis408\nIn T able 1, we present in detail the test results of GPT3.5-409\nTurbo on three types of methods: baseline, single grimoire,410\nand S L EIC L. Overall, the single grimoire and S L EIC L have411\nbetter average accuracy than baseline on all task datasets,412\nwith HSS-PG (r=0.5) and KCS-PG having the highest aver-413\nage accuracy , exceeding baseline by 4.72% and 3.5%, respec-414\ntively . This indicates that the grimoire generated by strong415\nlanguage model can effectively improve the performance of416\nweak language model on various tasks. And the best perform-417\ning single grimoire is HSS-PG, and the maximum accuracy418\non each task dataset is not obviously concentrated on a single419\ngrimoire. This indicates that representative samples used to420\ngenerate grimoires can indeed effectively affect the ﬁnal per-421\nformance of grimoires; on the other hand, it indicates that the422\noptimal grimoire for different tasks is not the same. That is,423\nwe can optimize the grimoire at the task level and select the424\ngrimoire auxiliary weak language model for different tasks.425\nSL EIC L proposed by us is further based on this, that is, gri-426\nmoire optimization at the sample level. However, although427\nthe similarity-based method and classiﬁer-based method we428\nhave implemented have some improvement compared to the429\nbaseline, they still cannot comprehensively surpass single gri-430\nmoire. The possible reason is that similarity-based method is431\nlimited to ﬁltering using semantic similarity , while the neural432\nnetwork structure constructed by the classiﬁer-based method433\nis still relatively simple, and its training data is currently lim-434\nited.435\nAt the task dataset level, in sentiment analysis and topic436\nclassiﬁcation, the grimoire method (single grimoire and S L E-437\nIC L) has a small performance improvement over the baseline.438\nIn natural language inference and hate speech detection, the439\nperformance of grimoire method improved signiﬁcantly and440\nwas more stable compared with the baseline, indicating that441\nthe current capacity of weak language model is sufﬁcient for442\nrelatively simple classiﬁcation tasks. For more difﬁcult tasks443\n(such as natural language inference requiring deep semantics444\nand simple reasoning), grimoire method can effectively make445\nup for the shortcomings of weak language model.446\nIn addition, we found that four PGs outperformed their 447\ncorresponding SG and two SGs outperformed their corre- 448\nsponding PG in 12 categories of single grimoire, suggesting 449\nthat GPT3.5-Turbo may favor more complex and detailed gri- 450\nmoire. However, in other small models tested (see appendix 451\nfor detailed results), we do not ﬁnd that small models are 452\nsigniﬁcantly inclined to SG, which indicates that we cannot 453\nsimply select the best grimoire for a language model, and fur- 454\nther demonstrates the necessity of exploring grimoire ranking 455\nmethods. 456\nIn T able 2, we provide a detailed presentation of the dif- 457\nferences between the best performance of all single gri- 458\nmoires (Max(single grimoire)) and baseline performance on 459\neach task dataset, as well as the differences between S L EIC L 460\n(classiﬁer-based) and baseline performance. Among all lan- 461\nguage models, the Max(single grimoire) almost comprehen- 462\nsively exceeds baseline, and it can be found that the smaller 463\nthe language model, the greater the performance improve- 464\nment compared with baseline under the support of grimoire. 465\nFor example, Baichuan2-7B has improved by 30.11% and 466\n15.16% relative to zero-shot and few-shot, respectively , while 467\nGPT3.5-Turbo has improved by 7.21% and 7.60% relative 468\nto zero-shot and few-shot, respectively . On the other hand, 469\nthis also indicates that for weak language model, the gri- 470\nmoire method can signiﬁcantly outperform the performance 471\nof few-shot (n=4) and has a wide range of applicability (weak 472\nlanguage model from 175B to 7B have signiﬁcant improve- 473\nments). At the same time, we can once again observe that on 474\nall weak language models, the grimoire method performs bet- 475\nter in natural language interference and hate speech detection 476\ntasks, which further demonstrates that the grimoire method 477\ncan effectively enhance the capability of weak language mod- 478\nels. 479\nIn all weak language models, although the average perfor- 480\nmance of the S L EIC L(classiﬁer-based) has improved com- 481\npared to the baseline, it still cannot surpass the baseline 482\non some datasets. This indicates that although the S L E- 483\nIC L(classiﬁer-based) has improved compared to the S L E- 484\nIC L(similarity-based), it still has not achieved the ideal per- 485\nT able 2: Prediction accuracy difference of grimoire method relative to baseline\nDIFF LLM\nSentiment Analysis T opic Classiﬁcation Natural Language\nInference Hate Speech Detection\nA VG\nSST5 Subj AgNews TREC R TE QNLI hate sp18 ethos\nMax(Single Grimoire)\n&\nZero-Shot\nGPT -3.5-Turbo 1.40% 26.00% 1.49% 1.32% 3.80% 12.00% 7.51% 4.13% 7.21%\nLLaMA2-70B-Chat -1.03% 16.16% 5.69% 20.11% 1.60% 14.53% 9.67% 11.40% 9.77%\nLLaMA2-13B-Chat 3.29% 32.34% 19.06% 16.82% 9.95% 10.44% 40.91% 14.68% 18.44%\nBaichuan2-7B-Chat 10.82% 28.66% 37.88% 52.23% 11.40% 7.01% 63.61% 29.30% 30.11%\nPhi-2 5.34% 37.04% 32.51% NaN 9.76% NaN 39.15% 16.05% 23.31%\nMax(Single Grimoire)\n&\nFew-Shot\nGPT -3.5-Turbo 1.20% 21.33% -0.44% 5.25% 2.40% 4.60% 17.96% 8.47% 7.60%\nLLaMA2-70B-Chat 1.40% 2.05% 5.20% 16.20% 0.20% 10.13% 25.74% 25.32% 10.78%\nLLaMA2-13B-Chat -5.60% 17.52% 28.75% 11.57% 6.83% 5.58% 10.96% 2.40% 9.75%\nBaichuan2-7B-Chat 14.87% 1.24% 39.50% 30.39% 4.87% 4.33% 14.78% 11.30% 15.16%\nPhi-2 -1.04% 8.57% NaN 19.22% NaN NaN 10.08% -1.16% 7.13%\nSL EIC L (Classiﬁer-based)\n&\nZero-Shot\nGPT -3.5-Turbo -1.63% 20.13% -0.77% -2.66% 1.40% 6.92% 1.17% 2.60% 3.39%\nLLaMA2-70B-Chat -3.33% 7.10% -0.11% 6.00% -2.33% 6.33% 1.99% 8.52% 3.02%\nLLaMA2-13B-Chat 12.64% 1.88% 13.66% 24.07% -8.94% -5.72% 34.35% -1.69% 8.78%\nBaichuan2-7B-Chat 3.58% 42.80% 30.82% 37.85% 6.93% -1.11% 50.18% 25.42% 24.56%\nPhi-2 3.19% 42.89% 24.34% NaN NaN NaN 32.85% 13.26% 23.31%\nSL EIC L (Classiﬁer-based)\n&\nFew-Shot\nGPT -3.5-Turbo -1.83% 15.47% -2.70% 1.27% 0.00% -0.48% 11.62% 6.93% 3.78%\nLLaMA2-70B-Chat -0.90% -7.01% -0.60% 2.09% -3.73% 1.93% 18.06% 22.44% 4.04%\nLLaMA2-13B-Chat 3.75% -12.94% 23.35% 18.82% -12.06% -10.58% 4.40% -13.97% 0.10%\nBaichuan2-7B-Chat 7.63% 15.38% 32.44% 16.01% 0.40% -3.79% 1.35% 7.42% 9.61%\nPhi-2 -3.19% 14.42% NaN 6.08% NaN NaN 3.78% -3.95% 3.43%\nNote: Max(Single Grimoire) indicates the best performance among all Single Grimoire methods; Positive differences will be highlighted in green, with darker colors being\ngreater: <5% , 5% ∼ 10% , 10% ∼ 20% , 20% ∼ 30% , >30% ; The negative difference will be highlighted in red, and the smaller the difference, the darker the color:\n>-5% , -5% ∼ -10% , <-10% . NaN indicates that the number of valid experimental data is too small to give a reliable accuracy rate.\nSubj\nSubj\nAgNews\nTREC\nRTE\nQNLI\nhate_sp18\nethos\nSubj\n0.20.30.40.50.60.70.80.9\nGPT4\nGPT3.5-T urbo\nLLaMA2-70B-Chat\nLLaMA2-13B-Chat\nBaichuan2-7B\nPHI2\nFigure 4: Radar Chart comparing GPT -4 results in zero-shot prompt-\ning with other models’ results in Max(Single Grimoire) setting.\nformance. In addition, we can still observe the pattern that486\nsmaller language models can achieve more signiﬁcant im-487\nprovements. For example, on almost all datasets, Baichuan2-488\n7B has a signiﬁcant improvement compared to baseline in the489\nSL EIC L(classiﬁer-based), while other larger language models490\ndo not perform so well. Furthermore, we can observe from491\nFigure\n4 that with the use of grimoires, weak language mod-492\nels have the potential to surpass GPT4-1106-preview under493\nzero-shot settings. Even on some datasets, the performance494\nof weak language models exceeds that of larger-scale lan-495\nguage models. For instance, on the TREC and Subj datasets,496\nthe PHI2 model with only 2.7 billion parameters outperforms497\nGPT4-1106-preview .498\n5 Conclusion 499\nIn this paper, we introduce a method named S L EIC L, predi- 500\ncated on utilizing strong language models to learn from repre- 501\nsentative samples and distill skills for solving speciﬁc tasks, 502\nthereby enhancing the proﬁciency of weak language mod- 503\nels in these tasks. The synthesized skills within this frame- 504\nwork are termed grimoire. T o diversify the grimoire cate- 505\ngories and comprehensively examine their impacts, we de- 506\nveloped four distinct representative sample selection methods 507\n(KCS, HCS, HSS, RSS) and a zero-shot approach, alongside 508\ntwo grimoire generation templates (PG and SG), culminat- 509\ning in the creation of 10 types of single grimoires. Build- 510\ning on this, we formulated a grimoire ranking method, aimed 511\nat automating the selection of the most suitable grimoire for 512\nvarious models and tasks at the sample level. Ultimately , 513\nwe evaluated 5 models across 8 datasets under 4 task types, 514\ndemonstrating that S L EIC L can substantially enhance the per- 515\nformance of weak language models with varying parameter 516\nsizes on diverse tasks, with smaller models exhibiting more 517\npronounced improvements. Remarkably , on certain datasets, 518\nweak language models, with the aid of our method, outper- 519\nformed GPT4-1106-preview in zero-shot scenarios. How- 520\never, while our grimoire ranking method showed some im- 521\nprovements over Zero-shot and Few-shot approaches, it did 522\nnot surpass the performance of the best single grimoire re- 523\nsults, suggesting that the classiﬁer-based method has poten- 524\ntial for further optimization. Additionally , the representative 525\nsample selection method presents an avenue for further explo- 526\nration to expand the variety of grimoires available for weak 527\nlanguage models across diverse tasks. 528\nReferences529\n[Brown et al. , 2020 ] T om Brown, Benjamin Mann, Nick Ry-530\nder, et al. Language models are few-shot learners. In531\nAdvances in Neural Information Processing Systems , vol-532\nume 33, pages 1877–1901. Curran Associates, Inc., 2020.533\n[Chan et al. , 2022 ] Stephanie Chan, Adam Santoro, Andrew534\nLampinen, et al. Data distributional properties drive emer-535\ngent in-context learning in transformers. In Advances in536\nNeural Information Processing Systems , volume 35, pages537\n18878–18891, 2022.538\n[Dagan et al. , 2005 ] Ido Dagan, Oren Glickman, and539\nBernardo Magnini. The pascal recognising textual entail-540\nment challenge. In Proceedings of the First International541\nConference on Machine Learning Challenges: Evaluating542\nPredictive Uncertainty V isual Object Classiﬁcation,543\nand Recognizing T extual Entailment , MLCW’05, page544\n177–190, Berlin, Heidelberg, 2005. Springer-V erlag.545\n[de Gibert et al. , 2018 ] Ona de Gibert, Naiara Perez, Aitor546\nGarc´ıa-Pablos, et al. Hate Speech Dataset from a White547\nSupremacy Forum. In Proceedings of the 2nd W orkshop548\non Abusive Language Online (ALW2) , pages 11–20, Brus-549\nsels, Belgium, October 2018. Association for Computa-550\ntional Linguistics.551\n[Dong et al. , 2023 ] Qingxiu Dong, Lei Li, Damai Dai,552\net al. A survey on in-context learning. arXiv preprint553\narXiv:2301.00234, 2023.554\n[Garg et al. , 2022 ] Shivam Garg, Dimitris Tsipras, Percy S555\nLiang, et al. What can transformers learn in-context? a556\ncase study of simple function classes. In Advances in557\nNeural Information Processing Systems , volume 35, pages558\n30583–30598, 2022.559\n[Hovy et al. , 2001 ] Eduard Hovy , Laurie Gerber, Ulf Herm-560\njakob, et al. T oward semantics-based answer pinpointing.561\nIn Proceedings of the First International Conference on562\nHuman Language T echnology Research , 2001.563\n[Li and Qiu, 2023 ] Xiaonan Li and Xipeng Qiu. Finding564\nsupport examples for in-context learning. In Findings of565\nthe Association for Computational Linguistics: EMNLP566\n2023, pages 6219–6235, 2023.567\n[Li and Roth, 2002 ] Xin Li and Dan Roth. Learning ques-568\ntion classiﬁers. In COLING 2002: The 19th International569\nConference on Computational Linguistics , 2002.570\n[Liu and W ang, 2023 ] Hongfu Liu and Y e W ang. T o-571\nwards informative few-shot prompt with maximum in-572\nformation gain for in-context learning. arXiv preprint573\narXiv:2310.08923, 2023.574\n[Liu et al. , 2022 ] Jiachang Liu, Dinghan Shen, Yizhe Zhang,575\net al. What makes good in-context examples for GPT-576\n3? In Proceedings of Deep Learning Inside Out (DeeLIO577\n2022): The 3rd W orkshop on Knowledge Extraction and578\nIntegration for Deep Learning Architectures , pages 100–579\n114, Dublin, Ireland and Online, May 2022. Association580\nfor Computational Linguistics.581\n[Lu et al. , 2022 ] Y ao Lu, Max Bartolo, Alastair Moore, et al.582\nFantastically ordered prompts and where to ﬁnd them:583\nOvercoming few-shot prompt order sensitivity . In Pro- 584\nceedings of the 60th Annual Meeting of the Association 585\nfor Computational Linguistics (V olume 1: Long P apers) , 586\npages 8086–8098, Dublin, Ireland, May 2022. Association 587\nfor Computational Linguistics. 588\n[Milios et al. , 2023 ] Aristides Milios, Siva Reddy , and 589\nDzmitry Bahdanau. In-context learning for text classiﬁ- 590\ncation with many labels. In Proceedings of the 1st Gen- 591\nBench W orkshop on (Benchmarking) Generalisation in 592\nNLP, pages 173–184, Singapore, December 2023. Asso- 593\nciation for Computational Linguistics. 594\n[Min et al. , 2022 ] Sewon Min, Xinxi Lyu, Ari Holtzman, 595\net al. Rethinking the role of demonstrations: What makes 596\nin-context learning work? In Y oav Goldberg, Zornitsa 597\nKozareva, and Y ue Zhang, editors, Proceedings of the 598\n2022 Conference on Empirical Methods in Natural Lan- 599\nguage Processing, EMNLP 2022, Abu Dhabi, United Arab 600\nEmirates, December 7-11, 2022 , pages 11048–11064. As- 601\nsociation for Computational Linguistics, 2022. 602\n[Mollas et al. , 2020 ] Ioannis Mollas, Zoe Chrysopoulou, 603\nStamatis Karlos, et al. Ethos: an online hate speech de- 604\ntection dataset, 2020. 605\n[Pang and Lee, 2004 ] Bo Pang and Lillian Lee. A sentimen- 606\ntal education: Sentiment analysis using subjectivity sum- 607\nmarization based on minimum cuts. In Proceedings of 608\nthe 42nd Annual Meeting of the Association for Compu- 609\ntational Linguistics (ACL-04) , pages 271–278, Barcelona, 610\nSpain, July 2004. 611\n[Pawelczyk et al. , 2023 ] Martin Pawelczyk, Seth Neel, and 612\nHimabindu Lakkaraju. In-context unlearning: Lan- 613\nguage models as few shot unlearners. arXiv preprint 614\narXiv:2310.07579, 2023. 615\n[Qin et al. , 2023 ] Chengwei Qin, Aston Zhang, Anirudh Da- 616\ngar, et al. In-context learning with iterative demonstration 617\nselection. arXiv preprint arXiv:2310.09881 , 2023. 618\n[Rajpurkar et al. , 2016 ] Pranav Rajpurkar, Jian Zhang, Kon- 619\nstantin Lopyrev , et al. SQuAD: 100,000+ questions for 620\nmachine comprehension of text. In Jian Su, Kevin Duh, 621\nand Xavier Carreras, editors, Proceedings of the 2016 622\nConference on Empirical Methods in Natural Language 623\nProcessing, pages 2383–2392, Austin, T exas, November 624\n2016. Association for Computational Linguistics. 625\n[Rubin et al. , 2022 ] Ohad Rubin, Jonathan Herzig, and 626\nJonathan Berant. Learning to retrieve prompts for in- 627\ncontext learning. arXiv preprint arXiv:2112.08633 , 2022. 628\n[Socher et al. , 2013 ] Richard Socher, Alex Perelygin, Jean 629\nWu, et al. Recursive deep models for semantic composi- 630\ntionality over a sentiment treebank. In Proceedings of the 631\n2013 Conference on Empirical Methods in Natural Lan- 632\nguage Processing , pages 1631–1642, Seattle, W ashington, 633\nUSA, October 2013. Association for Computational Lin- 634\nguistics. 635\n[Song et al. , 2020 ] Kaitao Song, Xu T an, T ao Qin, et al. Mp- 636\nnet: Masked and permuted pre-training for language un- 637\nderstanding. In Proceedings of the 34th International 638\nConference on Neural Information Processing Systems ,639\nNIPS’20, Red Hook, NY , USA, 2020. Curran Associates640\nInc.641\n[Su et al. , 2022 ] Hongjin Su, Jungo Kasai, Chen Henry Wu,642\net al. Selective annotation makes language models better643\nfew-shot learners. arXiv preprint arXiv:2209.01975 , 2022.644\n[T ouvron et al. , 2023 ] Hugo T ouvron, Louis Martin, Kevin645\nStone, et al. Llama 2: Open foundation and ﬁne-tuned646\nchat models. arXiv preprint arXiv:2307.09288 , 2023.647\n[W ei et al. , 2023 ] Jerry W ei, Jason W ei, Yi T ay , et al. Larger648\nlanguage models do in-context learning differently . arXiv649\npreprint arXiv:2303.03846 , 2023.650\n[Wu et al. , 2023 ] Zhiyong Wu, Y aoxiang W ang, Jiacheng651\nY e, et al. Self-adaptive in-context learning: An informa-652\ntion compression perspective for in-context example selec-653\ntion and ordering. In Proceedings of the 61st Annual Meet-654\ning of the Association for Computational Linguistics (V ol-655\nume 1: Long P apers) , pages 1423–1436, T oronto, Canada,656\nJuly 2023. Association for Computational Linguistics.657\n[Xie et al. , 2022 ] Sang Michael Xie, Aditi Raghunathan,658\nPercy Liang, et al. An explanation of in-context learn-659\ning as implicit bayesian inference. arXiv preprint660\narXiv:2111.02080, 2022.661\n[Y ang et al. , 2023 ] Aiyuan Y ang, Bin Xiao, Bingning W ang,662\net al. Baichuan 2: Open large-scale language models.663\narXiv preprint arXiv:2309.10305 , 2023.664\n[Y oo et al. , 2022 ] Kang Min Y oo, Junyeob Kim,665\nHyuhng Joon Kim, et al. Ground-truth labels mat-666\nter: A deeper look into input-label demonstrations. In667\nY oav Goldberg, Zornitsa Kozareva, and Y ue Zhang,668\neditors, Proceedings of the 2022 Conference on Empirical669\nMethods in Natural Language Processing, EMNLP 2022,670\nAbu Dhabi, United Arab Emirates, December 7-11,671\n2022, pages 2422–2437. Association for Computational672\nLinguistics, 2022.673\n[Zhang et al. , 2015 ] Xiang Zhang, Junbo Zhao, and Y ann674\nLeCun. Character-level convolutional networks for text675\nclassiﬁcation. In Proceedings of the 28th International676\nConference on Neural Information Processing Systems677\n- V olume 1 , NIPS’15, page 649–657, Cambridge, MA,678\nUSA, 2015. MIT Press.679\n[Zhang et al. , 2022 ] Yiming Zhang, Shi Feng, and Chenhao680\nT an. Active example selection for in-context learning. In681\nProceedings of the 2022 Conference on Empirical Meth-682\nods in Natural Language Processing , pages 9134–9148,683\nAbu Dhabi, United Arab Emirates, December 2022. Asso-684\nciation for Computational Linguistics.685\n[Zhang et al. , 2023 ] Zhuosheng Zhang, Aston Zhang,686\nMu Li, et al. Automatic chain of thought prompting in687\nlarge language models. In The Eleventh International688\nConference on Learning Representations , 2023.689\n[Zhao et al. , 2021 ] Zihao Zhao, Eric W allace, Shi Feng, et al.690\nCalibrate before use: Improving few-shot performance of691\nlanguage models. In Marina Meila and T ong Zhang, edi-692\ntors, Proceedings of the 38th International Conference on693\nMachine Learning , volume 139 of Proceedings of Ma- 694\nchine Learning Research , pages 12697–12706. PMLR, 695\n18–24 Jul 2021. 696\nAppendix697\nA Datasets and models698\nSome additional details about all the datasets used in this pa-699\nper are shown in T able 3, including the number of dataset700\nsample labels (#Class), the number of training set samples,701\nand the number of test set samples.702\nT able 3: Dataset information\nT ask Dataset #Class #Train #Eval\nSentiment Analysis SST5 5 8544 1101\nSubj 2 10000 -\nT opic Classiﬁcation AgNews 4 120000 7600\nTREC 6 5452 500\nNatural Language\nInference\nR TE 2 2490 277\nQNLI 2 104743 5463\nHate Speech\nDetection\nhate\nsp18 2 10944 -\nethos 2 998 -\nNote: - indicates that the dataset is not pre-divided into the training set\nand the evaluation set.\nAll of the language models used in our evaluation are703\nshown in T able 4, sorted by release date.704\nT able 4: Models Sorted by Release Date\nModel Parm. T ype Publisher Release\nGPT3.5-Turbo 175B ∗ Chat OpenAI 2023.06 ∗\nLLaMA2 70B Chat Meta 2023.07\nLLaMA2 13B Chat Meta 2023.07\nBaichuan2 7B Chat Baichuan Inc. 2023.09\nGPT4-1106-preview NaN Chat OpenAI 2023.11\nPHI2 2.7B Base Microsoft 2023.12\nNote: * indicates an estimated value, NaN signiﬁes the absence of publicly\navailable data, and 175B represents 175 billion.\nB Dual-tower deep neural network classiﬁer705\nB.1 Feature engineering706\nIn order to train the neural network constructed in the707\nclassiﬁer-based method, we primarily designed four cate-708\ngories of features to identify language models, tasks, test709\nsamples, and grimoires respectively . Among them, we only710\nselected the parameter scale as the feature to identify lan-711\nguage models, as the performance of large models is mainly712\ndirectly related to their parameter scale. Other information713\nabout language models is either strongly correlated with their714\nparameter scale or difﬁcult to obtain (such as closed-source715\nlanguage models). For tasks, we selected three features re-716\nlated to task category and task description. For test samples,717\nwe chose the text length of the sample and its embedding718\nvector as corresponding features. Regarding grimoires, we719\nselected their type, text length, embedding vector, and repre-720\nsentative sample selection method as features. The detailed721\ninformation about the features can be found in T able 5.722\nIn the processing of raw features, we utilize MPNet [Song723\net al. , 2020 ] to embed task descriptions, test questions, and724\ngrimoire, generating corresponding 768-dimensional dense 725\nvectors. For categorical features, we uniformly apply one-hot 726\nencoding. As for features related to text length, we initially 727\napply data binning and subsequently employ one-hot encod- 728\ning based on this binned data. 729\nB.2 Classiﬁer Architecture 730\ntodo 731\nB.3 T raining Process 732\ntodo 733\nC Detailed evaluation results of other 734\nlanguage models 735\nDetailed evaluation results of LLaMA2-70B-Chat, LLaMA2- 736\n13B-Chat, Baichuan2-7B-Chat and Phi-2 are supplemented 737\nin T able 6-9. Due to the poor instruction following ability of 738\nPhi-2, there are less than 500 valid experimental data in mul- 739\ntiple evaluation items, which cannot guarantee the reliability 740\nof accuracy rate, so some evaluation results are missing. 741\nOverall, the best performance on various datasets is mostly 742\nachieved by the grimoire method, which aligns with our 743\npreviously drawn conclusion that grimoire can effectively 744\nenhance the performance of weak language models across 745\ndifferent tasks. Secondly , for these four language models, 746\nthe best performance on individual datasets is not concen- 747\ntrated on a speciﬁc grimoire method, reafﬁrming that it is 748\nnot possible to simply identify a universally optimal gri- 749\nmoire for all tasks and language models. Therefore, the gri- 750\nmoire ranking method is necessary . Furthermore, although 751\non Baichuan2-7B-Chat and Phi-2, the average performance of 752\nSL EIC L(classiﬁer-based) is the best, S L EIC L still cannot con- 753\nsistently outperform all single grimoire methods on all weak 754\nlanguage models. Therefore, further optimization of the gri- 755\nmoire ranking method is still needed in the future. Finally , we 756\nstill observe the pattern that smaller language models tend to 757\nexhibit a higher improvement over the baseline under the gri- 758\nmoire method. Additionally , we ﬁnd that the average perfor- 759\nmance of LLaMA2-13B-Chat on RSS-PG signiﬁcantly sur- 760\npasses that of LLaMA2-70B-Chat in the same series but with 761\na larger parameter size, indicating that the beneﬁts of the gri- 762\nmoire method for weak language models are quite signiﬁcant. 763\nT able 5: Dataset information\nFeature type Feature Description Original data type Embedding method\nLLM-based llm param cnt type LLM parameter scale type Enum One-hot\nT ask-based\ntask type T ask type Enum One-hot\ntask desc len type T ypes of the task description length Int One-hot\ntask desc emb Embedding of the task description Str Dense\nQuestion-based question len type T ypes of question text length Int One-hot\nquestion emb Embedding of the question Str Dense\nGrimoire-based\ngrimoire type Grimoire type Enum One-hot\ngrimoire len type T ypes of grimoire length Int One-hot\ngrimoire sample method Sample selection methods of grimoire Enum One-hot\ngrimoire emb Embedding of the grimoire Str Dense\nT able 6: Detailed evaluation results of LLaMA2-70B-Chat enhanced by grimoire.\nModel Submodel n/k-shot\nSentiment analysis T opic classiﬁcation Natural language\ninference Hate speech detection\nA vg\nSST5 Subj AgNews TREC R TE QNLI hate sp18 ethos\nBaseline Zero-Shot - 53.40% 34.15% 74.38% 63.69% 80.53% 59.60% 60.88% 71.59% 62.28%\nFew-Shot n=4 50.97% 48.26% 74.87% 67.60% 81.93% 64.00% 44.81% 57.67% 61.26%\nSingle Grimoire\nKCS-PG k=4 52.20% 50.31% 77.52% 68.45% 73.00% 58.93% 55.23% 82.99% 64.83%\nKCS-SG k=4 52.37% 36.72% 75.12% 67.28% 81.73% 64.47% 55.77% 73.16% 63.33%\nHCS-PG k=4 50.71% 37.38% 73.68% 73.80% 77.67% 65.80% 60.51% 73.27% 64.10%\nHCS-SG k=4 48.20% 37.35% 74.72% 74.88% 79.27% 64.93% 68.35%\n73.24% 65.12%\nHSS-PG k=4 (r=1.0) 49.55% 39.67% 76.80% 60.81% 82.13% 58.20% 70.55% 80.82% 64.82%\nHSS-SG k=4 (r=1.0) 49.39% 39.07% 74.93% 57.39% 81.80% 66.80% 61.95% 71.66% 62.87%\nHSS-PG k=4 (r=0.5) 47.95% 42.90% 80.07% 83.80% 76.80% 66.93%\n66.51% 79.03% 68.00%\nHSS-SG k=4 (r=0.5) 49.59% 36.44% 73.33% 78.03% 79.07% 63.47% 55.00% 75.39% 63.79%\nRSS-PG k=4 50.53% 37.24% 71.36% 66.40% 79.13% 74.13% 64.66% 77.81% 65.16%\nRSS-SG k=4 46.51% 41.36% 69.49% 69.33% 80.60% 63.20% 57.66% 68.79% 62.12%\nZero-Shot-PG - 46.86% 43.32% 76.40% 60.49% 75.73% 58.53% 65.81% 76.35% 62.94%\nZero-Shot-SG - 49.52% 34.99% 73.27% 52.53% 79.80% 55.80% 62.90% 74.55% 60.42%\nSL EIC L Similarity-based - 50.90% 39.93% 74.67% 74.37% 79.33% 64.27% 61.69% 70.30% 64.43%\nClassiﬁer-based - 50.07% 41.25% 74.27% 69.69% 78.20% 65.93% 62.87% 80.11% 65.30%\nNote: - indicates that this hyper-parameter is invalid for the current test; n-shot indicates that n samples will be provided for each prediction; k-shot provides a selection of\nk samples under each label to generate grimoire; r represents the sampling ratio of hard samples. The best performance in each column will be bolded, and the second-best\nperformance will be underlined.\nT able 7: Detailed evaluation results of LLaMA2-13B-Chat enhanced by grimoire.\nModel Submodel n/k-shot\nSentiment analysis T opic classiﬁcation Natural language\ninference Hate speech detection\nA vg\nSST5 Subj AgNews TREC R TE QNLI hate sp18 ethos\nBaseline Zero-Shot - 34.57% 38.85% 57.98% 52.60% 68.31% 57.92% 45.40% 63.36% 52.37%\nFew-Shot n=4 43.46% 53.67% 48.29% 57.85% 71.43% 62.78% 75.35% 75.64% 61.06%\nSingle Grimoire\nKCS-PG k=4 35.53% 69.19% 75.33% 69.02% 48.07% 64.15% 72.67% 76.06% 63.75%\nKCS-SG k=4 36.79% 47.37% 74.62% 51.88% 69.39% 60.40% 83.40% 68.30% 61.52%\nHCS-PG k=4 35.14% 64.88% 74.13% 62.20% 56.07% 59.06% 80.52% 76.55% 63.57%\nHCS-SG k=4 35.53% 40.88% 72.18% 49.10% 63.80% 64.53% 86.31% 78.04% 61.30%\nHSS-PG k=4 (r=1.0) 37.86% 71.19% 73.73% 69.42%\n59.07% 67.20% 56.31% 72.05% 63.35%\nHSS-SG k=4 (r=1.0) 34.85% 47.20% 71.91% 43.73% 78.26% 58.60% 55.15% 60.85% 56.32%\nHSS-PG k=4 (r=0.5) 36.26% 67.91% 74.25% 51.97% 73.27% 68.36% 72.95% 74.68% 64.96%\nHSS-SG k=4 (r=0.5) 36.17% 38.53% 34.47% 42.76% 71.27% 63.00% 60.40% 75.19% 52.72%\nRSS-PG k=4 32.97% 69.53% 77.04% 62.68% 68.13% 66.69% 70.86% 75.13% 65.38%\nRSS-SG k=4 35.46% 53.41% 61.32% 51.75% 72.55% 62.13% 34.96% 67.00% 54.82%\nZero-Shot-PG - 35.01% 69.79% 52.00% 46.72% 52.87% 60.75% 78.16% 70.21% 58.19%\nZero-Shot-SG - 34.96% 53.06% 49.83% 47.34% 59.89% 57.37% 76.47% 77.54% 57.06%\nSL EIC L Similarity-based - 33.74% 50.33% 66.38% 52.58% 58.49% 62.60% 70.56% 73.05% 58.47%\nClassiﬁer-based - 47.21% 40.73% 71.64% 76.67% 59.37% 52.20% 79.75% 61.67% 61.16%\nNote: - indicates that this hyper-parameter is invalid for the current test; n-shot indicates that n samples will be provided for each prediction; k-shot provides a selection of\nk samples under each label to generate grimoire; r represents the sampling ratio of hard samples. The best performance in each column will be bolded, and the second-best\nperformance will be underlined.\nT able 8: Detailed evaluation results of Baichuan2-7B-Chat enhanced by grimoire.\nModel Submodel n/k-shot\nSentiment analysis T opic classiﬁcation Natural language\ninference Hate speech detection\nA vg\nSST5 Subj AgNews TREC R TE QNLI hate sp18 ethos\nBaseline Zero-Shot - 33.91% 38.07% 39.96% 16.05% 66.27% 52.04% 13.70% 45.60% 38.20%\nFew-Shot n=4 29.86% 65.49% 38.34% 37.89% 72.80% 54.72% 62.53% 63.60% 53.15%\nSingle Grimoire\nKCS-PG k=4 39.56% 66.07% 60.57% 54.78% 77.67% 57.04% 46.03% 68.21% 58.74%\nKCS-SG k=4 39.93% 43.33% 42.32% 19.09% 72.00% 52.35% 75.92% 53.67% 49.83%\nHCS-PG k=4 33.67% 64.87% 44.30% 66.07% 77.00% 58.87% 54.39% 68.89% 58.51%\nHCS-SG k=4 39.07% 42.47% 44.33% 34.97% 71.40% 59.05% 77.31% 54.85% 52.93%\nHSS-PG k=4 (r=1.0) 42.39% 63.00% 36.47% 32.28% 74.33% 57.60% 50.26% 65.47% 52.73%\nHSS-SG k=4 (r=1.0) 42.02% 66.73% 48.99% 28.37% 70.07% 52.60% 21.54% 47.36% 47.21%\nHSS-PG k=4 (r=0.5) 29.30% 65.67% 70.01% 68.28% 73.87% 53.47% 41.05% 74.90% 59.57%\nHSS-SG k=4 (r=0.5) 44.73% 47.07% 77.84% 45.66% 69.53% 57.18% 17.60% 61.47% 52.64%\nRSS-PG k=4 34.40% 63.40% 49.13% 48.71% 73.60% 56.09% 42.28% 67.68% 54.41%\nRSS-SG k=4 40.32% 36.67% 45.64% 26.98% 68.80% 57.79% 15.80% 46.66% 42.33%\nZero-Shot-PG - 33.22% 35.93% 39.09% 28.75% 76.13% 51.60% 49.16% 60.92% 46.85%\nZero-Shot-SG - 41.13% 40.07% 57.35% 22.89% 77.40%\n54.18% 28.13% 64.25% 48.18%\nSL EIC L Similarity-based - 36.49% 61.53% 56.84% 35.35% 77.25% 57.95% 45.12% 55.59% 53.26%\nClassiﬁer-based - 37.49% 80.87% 70.78% 53.90% 73.20% 50.93% 63.88% 71.02% 62.76%\nNote: - indicates that this hyper-parameter is invalid for the current test; n-shot indicates that n samples will be provided for each prediction; k-shot provides a selection of\nk samples under each label to generate grimoire; r represents the sampling ratio of hard samples. The best performance in each column will be bolded, and the second-best\nperformance will be underlined.\nT able 9: Detailed evaluation results of Phi-2 enhanced by grimoire.\nModel Submodel n/k-shot\nSentiment analysis T opic classiﬁcation Natural language\ninference Hate speech detection\nA vg\nSST5 Subj AgNews TREC R TE QNLI hate sp18 ethos\nBaseline Zero-Shot - 39.18% 34.15% 44.38% NaN 58.50% NaN 19.37% 48.68% 40.71%\nFew-Shot n=4 45.56% 62.62% NaN 66.88% NaN NaN 48.44% 65.89% 57.88%\nSingle Grimoire\nKCS-PG k=4 40.96% 39.31% 73.19% 52.78% NaN NaN 44.21% 62.49% 52.16%\nKCS-SG k=4 43.67% 34.49% 53.22% NaN 66.71%\nNaN 52.19% 52.47% 50.46%\nHCS-PG k=4 44.52% 42.00% 76.06% 86.10% NaN 63.07% 36.10% 64.73% 58.94%\nHCS-SG k=4 42.01% 41.01% 64.97% 72.33% 68.26% 61.49% 58.52% 60.55% 58.64%\nHSS-PG k=4 (r=1.0) 40.89% 52.60% 76.38% 55.37% NaN NaN 37.54% 56.11% 53.15%\nHSS-SG k=4 (r=1.0) 38.52% 37.97% 61.15% NaN 63.92% 52.22% 24.01% 58.88% 48.10%\nHSS-PG k=4 (r=0.5) 41.71% 71.19%\n34.40% 47.51% NaN NaN 29.01% 55.32% 46.52%\nHSS-SG k=4 (r=0.5) 42.13% 36.62% 37.16% 64.48% 61.47% 57.02% 47.03% 58.44% 50.54%\nRSS-PG k=4 38.80% 51.99% 76.89% 51.12% NaN NaN 52.65% 63.02% 55.75%\nRSS-SG k=4 41.81% 36.21% 65.06% 62.13% NaN NaN 23.91% 48.29% 46.24%\nZero-Shot-PG - 39.59% 36.46% 49.13% 52.53% NaN NaN 42.07% 53.21% 45.50%\nZero-Shot-SG - 41.38% 33.84% 43.51% NaN 58.51% NaN 36.55% 59.94% 45.62%\nSL EIC L Similarity-based - 41.26% 50.52% 59.35% NaN 64.96% NaN 37.73% 55.33% 51.53%\nClassiﬁer-based - 42.37% 77.04% 68.72% 72.96% NaN NaN 52.22% 61.94% 62.54%\nNote: - indicates that this hyper-parameter is invalid for the current test; n-shot indicates that n samples will be provided for each prediction; k-shot provides a selection of\nk samples under each label to generate grimoire; r represents the sampling ratio of hard samples. The best performance in each column will be bolded, and the second-best\nperformance will be underlined. NaN indicates that the number of valid experimental data is too small to give a reliable accuracy rate.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7952847480773926
    },
    {
      "name": "Language model",
      "score": 0.7189290523529053
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.548448920249939
    },
    {
      "name": "Inference",
      "score": 0.5444766283035278
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5442748665809631
    },
    {
      "name": "Variation (astronomy)",
      "score": 0.5291918516159058
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5236240029335022
    },
    {
      "name": "Stability (learning theory)",
      "score": 0.4906660318374634
    },
    {
      "name": "Natural language processing",
      "score": 0.4470660984516144
    },
    {
      "name": "Key (lock)",
      "score": 0.41162121295928955
    },
    {
      "name": "Machine learning",
      "score": 0.4014779031276703
    },
    {
      "name": "Programming language",
      "score": 0.06147688627243042
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Astrophysics",
      "score": 0.0
    }
  ]
}