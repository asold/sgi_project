{
  "title": "Evaluating Saliency Methods for Neural Language Models",
  "url": "https://openalex.org/W3153800458",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Ding, Shuoyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222096228",
      "name": "Koehn, Philipp",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2161185676",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W2971296520",
    "https://openalex.org/W2562979205",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2962790223",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W2626639386",
    "https://openalex.org/W3101155149",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W3034324324",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2964178496",
    "https://openalex.org/W2948099658",
    "https://openalex.org/W3105115233",
    "https://openalex.org/W2964045325",
    "https://openalex.org/W2971201880",
    "https://openalex.org/W2986889180",
    "https://openalex.org/W2891612330",
    "https://openalex.org/W2970447476",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3104153978",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2962843949",
    "https://openalex.org/W2962776659",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3092292656",
    "https://openalex.org/W2553397501",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2972376180",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2970658946",
    "https://openalex.org/W2963615251",
    "https://openalex.org/W3035503910",
    "https://openalex.org/W2798698226",
    "https://openalex.org/W3035577668",
    "https://openalex.org/W2941666437",
    "https://openalex.org/W2155069789",
    "https://openalex.org/W1902674502",
    "https://openalex.org/W2964110616"
  ],
  "abstract": "Saliency methods are widely used to interpret neural network predictions, but different variants of saliency methods often disagree even on the interpretations of the same prediction made by the same model. In these cases, how do we identify when are these interpretations trustworthy enough to be used in analyses? To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models. We evaluate the quality of prediction interpretations from two perspectives that each represents a desirable property of these interpretations: plausibility and faithfulness. Our evaluation is conducted on four different datasets constructed from the existing human annotation of syntactic and semantic agreements, on both sentence-level and document-level. Through our evaluation, we identified various ways saliency methods could yield interpretations of low quality. We recommend that future work deploying such methods to neural language models should carefully validate their interpretations before drawing insights.",
  "full_text": "Evaluating Saliency Methods for Neural Language Models\nShuoyang Ding Philipp Koehn\nCenter for Language and Speech Processing\nJohns Hopkins University\n{dings, phi}@jhu.edu\nAbstract\nSaliency methods are widely used to interpret\nneural network predictions, but different vari-\nants of saliency methods often disagree even\non the interpretations of the same prediction\nmade by the same model. In these cases, how\ndo we identify when are these interpretations\ntrustworthy enough to be used in analyses? To\naddress this question, we conduct a compre-\nhensive and quantitative evaluation of saliency\nmethods on a fundamental category of NLP\nmodels: neural language models. We evaluate\nthe quality of prediction interpretations from\ntwo perspectives that each represents a desir-\nable property of these interpretations: plausi-\nbility and faithfulness. Our evaluation is con-\nducted on four different datasets constructed\nfrom the existing human annotation of syntac-\ntic and semantic agreements, on both sentence-\nlevel and document-level. Through our evalua-\ntion, we identiﬁed various ways saliency meth-\nods could yield interpretations of low qual-\nity. We recommend that future work deploy-\ning such methods to neural language models\nshould carefully validate their interpretations\nbefore drawing insights.\n1 Introduction\nWhile neural network models for Natural Language\nProcessing (NLP) have recently become popular,\na general complaint is that their internal decision\nmechanisms are hard to understand. To alleviate\nthis problem, recent work has deployed interpreta-\ntion methods on top of the neural network models.\nAmong them, there is a category of interpretation\nmethods called saliency method that is especially\nwidely adopted (Li et al., 2016a,b; Arras et al.,\n2016, 2017; Mudrakarta et al., 2018; Ding et al.,\n2019). At a very high level, these methods assign\nan importance score to each feature in the input fea-\nture set F, regarding a speciﬁc prediction ymade\nby a neural network model M. Such feature impor-\ntance scores can hopefully shed light on the neural\nnetwork models’ internal decision mechanism.\nV U.S. companieswantingto expandin Europe\nSG U.S. companieswantingto expandin Europe\nIG U.S. companieswantingto expandin Europe\nTable 1: An example from our evaluation where dif-\nferent saliency methods assign different importance\nscores for the same model (Transformer language\nmodel) and the same next word prediction (are). V , SG\nand IG are different saliency methods (see Section 2).\nThe tints of green and yellow mark the magnitude of\npositive and negative importance scores, respectively.\nWhile analyzing saliency interpretations uncov-\ners useful insights for their respective task of in-\nterest, different saliency methods often give differ-\nent interpretations even when the internal decision\nmechanism remains the same (with F, y and M\nheld constant), as exempliﬁed in Table 1. Even\nso, most existing work that deploys these meth-\nods often makes an ungrounded assumption that a\nspeciﬁc saliency method can reliably uncover the\ninternal model decision mechanism or, at most, re-\nlies merely on qualitative inspection to determine\ntheir applicability. Such practice has been pointed\nout in Adebayo et al. (2018); Lipton (2018); Be-\nlinkov and Glass (2019) to be potentially problem-\natic for model interpretation studies – it can lead\nto misleading conclusions about the deep learning\nmodel’s reasoning process. On the other hand, in\nthe context of NLP, thequantitative evaluation of\nsaliency interpretations largely remains an open\nproblem (Belinkov and Glass, 2019).\nIn this paper, we address this problem by build-\ning a comprehensive quantitative benchmark to\nevaluate saliency methods. Our benchmark focuses\non a fundamental category of NLP models: neural\nlanguage models. Following the concepts proposed\nby Jacovi and Goldberg (2020), our benchmark\nevaluates the credibility of saliency interpretations\nfrom two aspects: plausibility and faithfulness. In\nshort, plausibility measures how much these inter-\npretations align with basic human intuitions about\narXiv:2104.05824v1  [cs.CL]  12 Apr 2021\nthe model decision mechanism, while faithfulness\nmeasures how consistent the interpretations are re-\ngarding perturbations that are supposed to preserve\nthe same model decision mechanism on either the\ninput feature F or the model M.\nWith these concepts in mind, our main contribu-\ntion is materializing these tests’ procedure in the\ncontext of neural language modeling and building\nfour test sets from existing linguistic annotations\nto conduct these tests. Our study covering SOTA-\nlevel models on three different network architec-\ntures reveals that saliency methods’ applicability\ndepends heavily on speciﬁc choices of saliency\nmethods, model architectures, and model conﬁgura-\ntions. We suggest that future work deploying these\nmethods to NLP models should carefully validate\ntheir interpretations before drawing conclusions.\nThis paper is organized as follows: Section 2\nbrieﬂy introduces saliency methods; Section 3 de-\nscribes the plausibility and faithfulness tests in our\nevaluation; Section 4 presents the datasets we built\nfor the evaluation; Section 5 presents our experi-\nment setup and results; Section 6 discusses some\nlimitations and implications of the evaluation; Sec-\ntion 7 concludes the paper.\n2 Saliency\nThe notion of saliency discussed in this paper is a\ncategory of neural network interpretation methods\nthat interpret a speciﬁc prediction ymade by a neu-\nral network model M, by assigning a distribution\nof importance Ψ(F) over the input feature set F of\nthe original neural network model.\nThe most basic and widely used method is to\nassign importance by the gradient (Simonyan et al.,\n2013), which we refer to as vanilla gradient method\n(V). For each x∈F, ψ(x) = ∂py\n∂x , while py is the\nscore of prediction y generated by M. We also\nexamine two improved version of gradient-based\nsaliency: SmoothGrad (SG) (Smilkov et al., 2017)\nand Integrated Gradients (IG) (Sundararajan et al.,\n2017). SmoothGrad reduces the noise in vanilla\ngradient-based scores by constructing several cor-\nrupted instances of the original input by adding\nGaussian noise, followed by averaging the scores.\nIntegrated Gradients computes feature importance\nby computing a line integral of the vanilla saliency\nfrom a “baseline” pointF0 to the input F in the fea-\nture space. We refer the readers to the cited papers\nfor details of these saliency methods.\nThere is a slight complication in the meaning of\nF when applying these methods in the context of\nNLP: all the methods above will generate one im-\nportance score for each dimension of the word em-\nbedding, but most applications of saliency to NLP\nwant a word-level importance score. Hence, we\nneed composition schemes to combine scores over\nword embedding dimensions into a single score for\neach word. In the rest of this paper, we assume the\n“features” in the feature setF are input words to the\nlanguage model, and word-level importance scores\nare composed using the gradient ·input scheme\n(Denil et al., 2014; Ding et al., 2019).1\n3 Evaluation Paradigm\nIn this section, we ﬁrst introduce the notion of\nplausibility and faithfulness in the context of neu-\nral network interpretations (following Jacovi and\nGoldberg (2020)), and then, respectively, introduce\nthe test we adopt to evaluate them.\n3.1 Plausibility\nConcept An interpretation is plausible if it aligns\nwith human intuitions about how a speciﬁc neural\nmodel makes decisions. For example, intuitively,\nan image classiﬁer can identify the object in the\nimage because it can capture some features of the\nmain object in the image. Hence, a plausible inter-\npretation would assign high importance to the area\noccupied by the main object. This idea of compari-\nson with human-annotated ground-truth (often as\n“bounding-boxes” signaling the main object’s area)\nis used by various early studies in computer vi-\nsion to evaluate saliency methods’ reliability (Jiang\net al., 2013, inter alia). However, the critical chal-\nlenge of such evaluations for neural language mod-\nels is the lack of such ground-truth annotations.\nTest To overcome this challenge, we follow Po-\nerner et al. (2018) to construct ground-truth annota-\ntions from existing lexical agreement annotations.\nConsider, for example, the case of morphological\nnumber agreement. Intuitively, when the language\nmodel predicts a verb with a singular morphologi-\ncal number, the singular nouns in the preﬁx should\nbe considered important features, and vice versa.\nBased on this intuition, we divide the nouns in the\npreﬁx into two different sets: the cue set C, which\nshares the same morphological number as the verb\nin the sentence; and the attractor set A, which has\n1We also experimented with the vector norm Li et al.\n(2016a) scheme in our preliminary study, and we ﬁnd it per-\nforming much worse. See details in Appendix B.1.\na different morphological number than the verb in\nthe sentence.\nThen, according to the prediction ymade by the\nmodel M, the test will be conducted under one of\nthe two following scenarios:\n• Expected: when y is the verb with the cor-\nrect number, the interpretation passes the test\nif maxw∈C ψ(w) >maxw∈A ψ(w)\n• Alternative: when y is the verb with the incor-\nrect number, the interpretation passes the test if\nmaxw∈C ψ(w) <maxw∈A ψ(w)\nHowever, this test has a ﬂaw: while the evalua-\ntion criteria focus on a speciﬁc category of lexical\nagreement, the prediction of a word could depend\non multiple lexical agreements simultaneously. To\nillustrate this point, consider the verb prediction\nfollowing the preﬁx “At the polling station peo-\nple ... ”. Suppose the model M predicts the verb\nvote. One could argue that people is more impor-\ntant than polling station because it needs the sub-\nject to determine the morphological number of the\nverb. However, the semantic relation between vote\nand polling station is also important because that\nis what makes vote more likely than other random\nverbs, e.g. sing.\nTo minimize such discrepancy and constrain the\nscope of agreements used to make predictions, we\ndraw inspiration from the previous work on rep-\nresentation probing and make adjustment to the\nmodel M we are evaluating on (Tenney et al.,\n2019a,b; Kim et al., 2019; Conneau et al., 2018;\nAdi et al., 2017; Shi et al., 2016). The idea is to take\na language model that is trained to predict words\n(e.g., vote in the example above) and substitute the\noriginal ﬁnal linear layer with a new linear layer\n(which we refer to as a probe) ﬁne-tuned to pre-\ndict a binary lexical agreement tag (e.g., PLURAL)\ncorresponding to the word choice. By making this\nadjustment, the ﬁnal layer extracts a subspace in\nthe representation that is relevant to the prediction\nof particular lexical agreement during the forward\ncomputation, and reversely, ﬁlters out gradients\nthat are irrelevant to the agreement prediction in\nthe backward pass, creating an interpretation that\nis only subject to the same agreement constraints\nas to when the annotation for the test set is done.\nApart from the adjustment made on the model\nM above, we also extend Poerner et al. (2018) in\nthe other two aspects: (1) we evaluate on one more\nlexical agreement: gender agreements between pro-\nnouns and referenced entities, and on both natural\nand synthetic datasets; (2) instead of evaluating\non small models, we evaluate on large SOTA-level\nmodels for each architecture. We also show that\nevaluation results obtained on smaller models can-\nnot be trivially extended to larger models.\n3.2 Faithfulness\nConcept An interpretation is faithful if the fea-\nture importance it assigns is consistent with the\ninternal decision mechanism of a model. However,\nas Jacovi and Goldberg (2020) pointed out, the\nnotion of “decision mechanism” lacks a standard\ndeﬁnition and a practical way to make comparisons.\nHence, as a proxy, we follow the working deﬁnition\nof faithfulness as proposed in their work, which\nstates that an interpretation is faithful if the fea-\nture importance it assigns remains consistent with\nchanges that should not change the internal model\ndecision mechanism. Among the three relevant fac-\ntors for saliency methods (prediction y, model M,\nand input feature set F), we focus on consistency\nupon changes in model M (model consistency) and\ninput feature set F (input consistency).2 Note that\nthese two consistencies respectively correspond to\nassumptions 1 and 2 in the discussion of faithful-\nness evaluation in Jacovi and Goldberg (2020).\nModel Consistency Test To measure model con-\nsistency, we propose to measure the consistency\nbetween feature importance ΨM (F) and ΨM′ (F),\nwhich is respectively generated from the original\nmodel M and a smaller model M′ that is trained\nby distilling knowledge from M. In this way, al-\nthough M and M′ have different architectures, M′\nis trained to mimic the behavior of M to the ex-\ntent possible, and thus having similar underlying\ndecision mechanisms.\nInput Consistency Test To measure input con-\nsistency, we perform substitutions in the input and\nmeasure the consistency between feature impor-\ntance Ψ(F) and Ψ(F′), where F and F′ are input\nfeatures sets before/after the substitution. For ex-\nample, the following preﬁx-prediction pairs should\nhave the same feature importance distribution:\n• The nun bought the son a gift because (she...)\n• The woman bought the boy a gift because (she...)\nWe measure consistency by Pearson correlation\nbetween pairs of importance score over the input\n2Although evaluating interpretation consistency over simi-\nlar predictions y is also possible, it is not of interest as most\napplications expect different interpretations for different pre-\ndictions.\nfeature set F for both tests. Also, note that although\nwe can theoretically conduct faithfulness tests with\nany model M and any dataset, for the simplicity\nof analysis and data creation, we will use the same\nmodel M (with lexical agreement probes) and the\nsame dataset as plausibility tests.\n4 Data 3\nFollowing the formulation in Section 3, we con-\nstructed four novel datasets for our benchmark, as\nexempliﬁed in Table 2. Two of the datasets are con-\ncerned with number agreement of a verb with its\nsubject. The other two are concerned with gender\nagreement of a pronoun with its anteceding entity\nmentions. For each lexical agreement type, we\nhave one synthetic dataset and one natural dataset.\nBoth synthetic datasets ensure there is only one cue\nand one attractor for each test instance, while for\nnatural datasets, there are often more than one.\nFor number agreement, our synthetic dataset is\nconstructed from selected sections of Syneval, a\ntargeted language model evaluation dataset from\nMarvin and Linzen (2018), where the verbs and\nthe subjects could be easily induced with heuristics.\nWe only use the most challenging sections where\nstrongly interceding attractors are involved. Our\nnatural dataset for this task is ﬁltered from Penn\nTreebank (Marcus et al., 1993, PTB), including\ntraining, development, and test. We choose PTB\nbecause it offers not only human-annotated POS-\ntags necessary for benchmark construction but also\ndependent subjects of verbs for further analysis.\nFor gender agreement, our synthetic dataset\ncomes from the unambiguous Winobias corefer-\nence resolution dataset used in Jumelet et al. (2019),\nand we only use the 1000-example subset where\nthere is respectively one male and one female an-\ntecedent. Because this dataset is intentionally de-\nsigned such that most humans will ﬁnd pronouns of\neither gender equally likely to follow the preﬁx, no\nsuch pronoun gender is considered to be “correct”.\nHence, without loss of generality, we assign the fe-\nmale pronoun to be the expected case.4 Our natural\ndataset for this task is ﬁltered from CoNLL-2012\nshared task dataset for coreference resolution (Prad-\nhan et al., 2012, also including training, develop-\n3More details on data ﬁltering are in Appendix A.\n4Note that this assumption will not change the interpreta-\ntions we generate or the benchmark test conducted for inter-\npretations, as we always interpret the argmax decision of the\nmodel, which is not affected by this assumption. It will only\naffect the breakdown of the result we report.\nment, and test). The preﬁx of each test example\ncovers a document-level context, which usually\nspans several hundred words.\nPlausibility Test For number agreement, the cue\nset Cis the set of all nouns that have the same\nmorphological number as the verb. In contrast, the\nattractor set Ais the set of all nouns with a different\nmorphological number. For gender agreement, the\ncue set Cis the set of all nouns with the same\ngender as the pronoun, while the attractor set Ais\nthe set of all nouns with a different gender.\nModel Consistency Test No special treatment\nto data is needed for this test. We conduct model\nconsistency tests on all datasets we built.\nInput Consistency Test We recognize that gen-\nerating interpretation-preserving input perturba-\ntions for natural datasets is quite tricky. Hence, un-\nlike the model consistency test, we focus on the two\nsynthetic datasets for faithfulness tests because they\nare generated from templates. As can be seen from\nthe examples, when the nouns in the cue/attractor\nset are substituted while maintaining the lexical\nagreement, the underlying model decision mech-\nanism should be left unchanged; hence they can\nbe viewed as interpretation-preserving perturba-\ntions. We identiﬁed 24 and 254 such interpretation-\npreserving templates from our Syneval and Wino-\nbias dataset and generated perturbations pairs by\ncombining the ﬁrst example of each template with\nother examples generated from the same template.\n5 Experiments\n5.1 Setup\nInterpretation Methods For SmoothGrad (SG),\nwe set sample size N = 30 and sample variance σ2\nto be 0.15 times the L2-norm of word embedding\nmatrix; for Integrated Gradients (IG), we use step\nsize N = 100. These choices are made empirically\nand veriﬁed on a small held-out development set.\nInterpreted Model Our benchmark covers three\ndifferent neural language model architectures,\nnamely LSTM (Hochreiter and Schmidhuber,\n1997), QRNN (Bradbury et al., 2017) and Trans-\nformer (Vaswani et al., 2017; Baevski and Auli,\n2019; Dai et al., 2019). All language models are\ntrained on WikiText-103 dataset (Merity et al.,\n2017). For the ﬁrst two architectures, we use the\nimplementation as in awd-lstm-lm toolkit (Merity\net al., 2018). For Transformer, we use the imple-\nPTB U.S. Trade RepresentativeCarla Hills said the first dispute-settlementpanel set up\nunder the U.S.-Canadian“ free trade ” agreementhas ruled that Canada’s restrictions\non exportsof Pacificsalmonand herring(PLURAL...)\nSyneval the consultantthat loves the parents(SINGULAR...)\nCoNLL IsraeliPrime MinisterEhud Barak says he is freezingtens of millionsof dollarsin\ntax paymentsto the PalestinianAuthority. Mr. Barak says he is withholdingthe money\nuntil the Palestiniansabide by cease - fire agreements. EarlierThursdayMr. Barak\nruled out an early resumptionof peace talks , even with the UnitedStatesactingas\nintermediary. Eve Conettereportsfrom Jerusalem. Defendingwhat (MASCULINE...)\nWinobias The bride examinedthe son for injuriesbecause(FEMININE...)\nTable 2: Examples preﬁxes from the four evaluation datasets, followed by the probing tag prediction under the\nexpected scenario. The cue and attractor sets are marked with solid Green and yellow, respectively.\nNumber Agreement Gender Agreement\nPTB Syneval CoNLL Winobias\nall exp. alt. all exp. alt. all exp. alt. all exp. alt.\nRandom - 0.546 0.454 - 0.500 0.500 - 0.519 0.481 - 0.500 0.500\nNearest - 0.502 0.498 - 0.140 0.860 - 0.00 1.00 - 0.500 0.500\nLSTM (0.858) (0.142) (0.596) (0.404) (0.730) (0.270) (0.584) (0.416)\nV 0.452 0.484 0.259 0.304 0.371 0.206 0.288 0.266 0.348 0.403 0.440 0.351\nSG 0.780 0.805 0.629 0.950 0.951 0.949 0.799 0.767 0.880 0.984 0.981 0.988\nIG 0.816 0.856 0.571 0.888 0.941 0.811 0.585 0.561 0.652 0.881 0.853 0.921\nQRNN (0.818) (0.182) (0.558) (0.442) (0.712) (0.288) (0.715) (0.285)\nV 0.463 0.501 0.289 0.511 0.536 0.480 0.669 0.638 0.546 0.242 0.269 0.175\nSG 0.575 0.599 0.468 0.707 0.692 0.726 0.503 0.436 0.669 0.790 0.801 0.761\nIG 0.697 0.728 0.555 0.797 0.764 0.838 0.737 0.700 0.828 0.768 0.730 0.863\nTransformer (0.919) (0.081) (0.594) (0.406) (0.761) (0.239) (0.219) (0.781)\nV 0.551 0.551 0.551 0.723 0.785 0.632 0.674 0.693 0.614 0.781 0.799 0.766\nSG 0.842 0.851 0.737 0.895 0.879 0.920 0.956 0.951 0.971 0.994 1.00 0.992\nIG 0.734 0.741 0.652 0.849 0.786 0.940 0.829 0.843 0.786 0.806 0.865 0.775\nTable 3: Plausibility benchmark result. Each number is the fraction of cases the interpretation passes the bench-\nmark test, while the numbers in brackets for each architecture are the fraction of times these scenarios occur for\npredictions generated by the corresponding model. Results from the best interpretation method for each architec-\nture are boldfaced. The exp. and alt. columns are breakdown of evaluation results into expected scenarios and\nalternative scenarios as deﬁned in Section 3. V , SG, IG stands for the vanilla saliency, SmoothGrad, and Integrated\nGradients, respectively.\nmentation in fairseq tookit (Ott et al., 2019).\nFor all the task-speciﬁc “probes”, the ﬁne-tuning\nis performed on examples extracted from Wiki-\nText-2 training data. A tuning example consists\nof an input preﬁx and a gold tag for the lexical\nagreement in both cases. For number agreement,\nwe ﬁrst run Stanford POS Tagger (Toutanova et al.,\n2003) on the data, and an example is extracted\nfor each present tense verb and each instance of\nwas or were. For gender agreement, an example is\nextracted for each gendered pronoun. During ﬁne-\ntuning, we ﬁx all the other parameters except the\nﬁnal linear layer. The ﬁnal layer is tuned to mini-\nmize cross-entropy, with Adam optimizer (Kingma\nand Ba, 2015) and initial learning rate of1e−3 with\nReduceLROnPlateau scheduler.\nWe follow the setup for DistillBERT (Sanh et al.,\n2019) for the distillation process involved during\nthe model consistency test, which reduces the depth\nof models but not the width. For our LSTM (3 lay-\ners) and QRNN model (4 layers), the M′ we distill\nis one layer shallower than the original model M.\nFor our transformer model (16 layers), we distill a\n4-layer M′ largely due to memory constraints.\n5.2 Main Results\nPlausibility According to our plausibility evalua-\ntion result, summarized in Table 3, both SG and IG\nconsistently perform better than the vanilla saliency\nmethod regardless of different benchmark datasets\nand interpreted models. However, the comparison\nbetween SG and IG interpretations varies depend-\ning on the model architecture and test sets.\nAcross different architectures, Transformer lan-\nguage model achieves the best plausibility except\non the Syneval dataset. LSTM closely follows\nTransformer for most benchmarks, while the plau-\nsibility of the interpretation from QRNN is much\nworse. Another trend worth noting is that the gap\nbetween Transformer and the other two architec-\ntures is much larger on the CoNLL benchmark,\nwhich is the only test that involves interpreting\ndocument-level contexts. However, these architec-\ntures’ prediction accuracy is similar, meaning that\nthere is no signiﬁcant modeling power difference\nfor gender agreements in this dataset. We hence\nconjecture that the recurrent structure of LSTM\nand QRNN might diminish gradient signals with\nincreasing time steps, which causes the deterio-\nration of interpretation quality for long-distance\nagreements – a problem that Transformer is exempt\nfrom, thanks to the self-attention structure.\nFaithfulness Table 4a shows the input consis-\ntency benchmark result. Firstly, it can be seen\nthat the interpretations of LSTM and Transformer\nare more resilient to input perturbations than that\nof QRNN. This is the same trend as we observed\nfor plausibility benchmark on these datasets. When\ncomparing different saliency methods, we see that\nSG consistently outperforms for Transformer, but\nfails for the other two architectures, especially for\nQRNN. Also, note that achieving higher plausi-\nbility does not necessarily imply higher faithful-\nness. For example, compared to the vanilla saliency\nmethod, SG and IG almost always signiﬁcantly im-\nprove plausibility but do not always improve faith-\nfulness. This lack of improvement is different from\nthe ﬁndings in computer vision (Yeh et al., 2019),\nwhere they show both SG and IG improve input\nconsistency. Also, for LSTM, although SG works\nslightly better than IG in terms of plausibility, IG\noutperforms SG in terms of input consistency by a\nlarge margin.\nTable 4b shows the model consistency bench-\nmark result. One should ﬁrst notice that model\nconsistency numbers are lower than input consis-\ntency across the board, and the drop is more sig-\nniﬁcant for LSTM and QRNN even though their\nstudent model is not as different as the Transformer\nmodel (<20% parameter reduction vs. 61%). As\na result, there is a signiﬁcant performance gap in\nterms of best model consistency results between\nLSTM/QRNN and Transformer. Note that, like in\nplausibility results, such gap is most notable on the\nCoNLL dataset. On the other hand, when compar-\ning between saliency methods, we again see that\nSG outperforms for Transformer while failing most\nof the times for QRNN and LSTM.\n5.3 Analysis\nPlausibility vs. Faithfulness A natural question\nfor our evaluation is how the property of plausibil-\nity and faithfulness interact with each other. Table 5\nillustrates such interaction with qualitative exam-\nples. Among them, 1 and 2 are two cases where\nthe plausibility and input faithfulness evaluation\nresults do not correlate. In general, the interpre-\ntations in both cases are of low quality, but they\nalso fail in different ways. In case 1, the interpre-\ntation assigns the correct relative ranking for the\ncue words and attractor words, but the importance\nof the words outside the cue/attractor set varies\nupon perturbation. On the other hand, in case 2,\nthe importance ranking among features is roughly\nmaintained upon perturbation, but the importance\nscore assigned for both examples do not agree with\nthe prediction interpreted (FEMININE tag) and thus\ncan hardly be understood by humans. It should\nbe noted that these defects can only be revealed\nwhen both plausibility and faithfulness tests for\ninterpretations are deployed.\nCase 3 shows a scenario where the saliency\nmethod yields very different interpretations for the\nsame input/prediction pair, indicating that inter-\npretations from this architecture/saliency method\ncombination are subject to changes upon changes\nin the architecture conﬁgurations. Finally, in case\n4, we see that an architecture/saliency method com-\nbination performing well in all tests yields stable\ninterpretations that humans can easily understand.\nSensitivity to Model Conﬁgurations Our\nmodel faithfulness evaluation shows that variations\nin the model conﬁgurations (number of layers)\ncould drastically change the model interpretation\nin many cases. Hence, we want to answer two\nanalysis questions: (1) are these interpretations\nchanging for the better or worse quality-wise\nwith the distilled smaller models? (2) are there\nany patterns for such changes? Due to space\nconstraints, we only show some analysis results for\nquestion (1) in Table 6. Overall, compared to the\ncorresponding results in Table 3 (for plausibility)\nSyneval Winobias\nexp. alt. exp. alt.\nLSTM\nV 0.532 0.533 0.447 0.447\nSG 0.481 0.491 0.560 0.404\nIG 0.736 0.695 0.735 0.795\nQRNN\nV 0.226 0.223 0.566 0.566\nSG 0.166 0.239 0.184 0.239\nIG 0.448 0.387 0.499 0.622\nTransformer\nV 0.367 0.375 0.545 0.545\nSG 0.604 0.627 0.775 0.752\nIG 0.521 0.480 0.542 0.494\nNumber Agreement Gender Agreement\nPTB Syneval CoNLL Winobias\nexp. alt. exp. alt. exp. alt. exp. alt.\nLSTM\nV 0.325 0.324 0.370 0.370 0.301 0.301 0.082 0.082\nSG 0.242 0.294 0.453 0.394 0.190 0.235 0.071 0.138\nIG 0.548 0.487 0.439 0.513 0.256 0.275 0.435 0.252\nQRNN\nV 0.208 0.207 0.228 0.229 0.147 0.147 0.212 0.212\nSG 0.043 0.044 0.144 0.131 0.010 0.016 0.063 0.070\nIG 0.259 0.387 0.316 0.350 0.305 0.375 0.303 0.285\nTransformer\nV 0.160 0.160 0.219 0.219 0.289 0.289 0.104 0.104\nSG 0.584 0.584 0.598 0.570 0.688 0.693 0.656 0.581\nIG 0.239 0.294 0.450 0.413 0.219 0.277 0.310 0.291\n(a) Input Consistency (b) Model Consistency\nTable 4: Faithfulness Benchmark Result. Each number is the average Pearson correlation computed on the cor-\nresponding dataset. Results from the best interpretation method for each architecture are boldfaced. Refer to the\ncaption of Table 3 for other notations.\n1a QRNN+SG The [grandmother]examinedthe (grandson)for injuriesbecause\n1b QRNN+SG The [sister]examinedthe (groom)for injuriesbecause\n2a QRNN+V The [grandmother]examinedthe (grandson)for injuriesbecause\n2b QRNN+V The [aunt]examinedthe (groom)for injuriesbecause\n3a QRNN+SG The [grandmother]examinedthe (grandson)for injuriesbecause\n3b QRNN_distilled+SG The [grandmother]examinedthe (grandson)for injuriesbecause\n4a Transformer+SG The [grandmother]examinedthe (grandson)for injuriesbecause\n4b Transformer+SG The [aunt]examinedthe (groom)for injuriesbecause\n4c Transformer_distilled+SGThe [grandmother]examinedthe (grandson)for injuriesbecause\nTable 5: Examples from Winobias dataset for qualitative analysis. Cue words are marked with [] while attractor\nwords are marked with (). The tints of green and yellow mark the magnitude of positive and negative importance\nscores, respectively. For all examples, the prediction interpreted is the FEMININE tag. 1 is a case with high plausi-\nbility and low input faithfulness; 2 is a case with low plausibility and high input faithfulness; 3 is a case with low\nmodel faithfulness; 4 is a case with high plausibility and high input/model faithfulness.\nand Table 4a (for input faithfulness), the saliency\nmethods we evaluated perform better with the\nsmaller distilled models. Most remarkably, we see\na drastic performance improvement for QRNN,\nboth in plausibility and faithfulness. For LSTM\nand Transformer, we observe an improvement for\ninput faithfulness on Winobias and roughly the\nsame performance for other tests.\nAs for the second question, we build smaller\nTransformer language models with various depth,\nnumber of heads, embedding size, and feed-\nforward layer width settings, while keeping other\nhyperparameters unchanged. Unfortunately, the\ntrends are quite noisy and also heavily depends on\nthe chosen saliency methods.5 Hence, it is highly\n5Detailed discussion of these analyses is in Appendix B.2.\nrecommended that evaluation of saliency methods\nbe conducted on the speciﬁc model conﬁgurations\nof interest, and trends of interpretation quality on\na speciﬁc model conﬁguration should not be over-\ngeneralized to other conﬁgurations.\nSaliency vs. Probing Our evaluation incorpo-\nrates probing to focus only on speciﬁc lexical agree-\nments of interest. It should be pointed out that in\nthe literature of representation probing, the method\nhas always been working under the following as-\nsumption: when the model makes an expected-\nscenario (\"correct\") prediction, it is always refer-\nring to a grammatical cue, for example, the subject\nof the verb in the number agreement case. However,\nin our evaluation, we also observe some interest-\ning phenomena in the interpretation of saliency\nSyneval Winobias\nall exp. alt. all exp. alt.\nbest plausibility\nLSTM (SG) 0.945 0.922 0.973 0.948 0.950 0.904\nQRNN (IG) 0.981 0.964 0.998 0.974 0.974 1.00\nTransformer (SG)0.917 0.908 0.929 0.997 1.00 0.996\nbest (input)\nfaithfulness\nLSTM (IG) – 0.628 0.739 – 0.820 0.769\nQRNN (IG) – 0.733 0.831 – 0.891 0.841\nTransformer (SG)– 0.569 0.581 – 0.932 0.912\nTable 6: Plausibility & input faithfulness on synthetic\ndatasets with distilled models. Only results for the in-\nterpretation method with best performance are shown.\nRefer to the caption of Table 3 for other notations.\nV “ The [fact]that this happenedtwo\n(years)ago and therewas a [recovery]\nSG “ The [fact]that this happenedtwo\n(years)ago and therewas a [recovery]\nIG “ The [fact]that this happenedtwo\n(years)ago and therewas a [recovery]\nTable 7: A number agreement test case where the dis-\ntilled Transformer model makes the correct prediction\n(singular) but all interpretation methods unanimously\npoint to a singular noun that is not grammatical subject\nas the most salient cue for this prediction.\nmethods that breaks the assumption, which is ex-\nempliﬁed in Table 7. This calls for future work\nthat aims to better understand language model be-\nhaviors by examining other possible cues used for\npredictions made in representation probing under\nthe validated cases where saliency methods could\nbe reliably applied.\n6 Discussion\nMost existing work on evaluating saliency methods\nfocuses only on computer vision models (Adebayo\net al., 2020; Hooker et al., 2019; Adebayo et al.,\n2018; Heo et al., 2019; Ghorbani et al., 2019, inter\nalia). In the context of NLP, Poerner et al. (2018) is\nthe ﬁrst work to conduct such evaluations for NLP\nand the only prior work that conducts such evalu-\nations for neural language models but has several\nlimitations as we have already pointed out in Sec-\ntion 3. Arras et al. (2019); Atanasova et al. (2020);\nHao (2020) conducted similar evaluations based on\nspeciﬁcally designed diagnostic toy tasks and/or\ntext classiﬁcation, while Bastings and Filippova\n(2020) casted doubt on whether these conclusions\ncould be generalized to sequence generation tasks.\nLi et al. (2020) evaluated various interpretation\nmethods for neural machine translation models by\nbuilding proxy models on only the top-kimportant\ninput words as determined by the interpretation\nmethods, but such evaluation requires generating\ninterpretations for a large training set and hence\nis intractable for even mildly computationally-\nexpensive methods such as SmoothGrad and In-\ntegrated Gradients. On a slightly different line,\nDeYoung et al. (2020) built a benchmark to evalu-\nate a speciﬁc category of NLP models that generate\nrationales during predictions, which is a different\npath towards building explainable NLP models.\nOur evaluation is not without its limitations. The\nﬁrst limitation, inherited from earlier work by Po-\nerner et al. (2018), is that our plausibility test only\nconcerns the words in cue/attractor sets rather than\nother words in the input preﬁx. Such limitation\nis inevitable because the annotations from which\nwe build our ground-truth interpretations are only\nconcerned with a speciﬁc lexical agreement. This\nlimitation can be mitigated by combining plausibil-\nity tests with faithfulness tests, which concern all\nthe input preﬁx words.\nThe second limitation is that the test sets used in\nthese benchmarks need to be constructed in a case-\nto-case manner, according to the chosen lexical\nagreements and the input perturbations. While it is\nhard to create plausibility test sets without human\ninterference, future work could explore automatic\ninput consistency tests by utilizing adversarial in-\nput generation techniques in NLP (Alzantot et al.,\n2018; Cheng et al., 2019, 2020).\nIt should also be noted that while our work fo-\ncuses on evaluating a speciﬁc category of interpre-\ntation methods for neural language models, our\nevaluation paradigm can be easily extended to eval-\nuating other interpretation methods such as atten-\ntion mechanism, and with other sequence mod-\nels such as masked language models (e.g., BERT).\nWe would also like to extend these evaluations be-\nyond English datasets, especially to languages with\nricher morphological inﬂections.\n7 Conclusion\nWe conduct a quantitative evaluation of saliency\nmethods on neural language models based on the\nperspective of plausibility and faithfulness. Our\nevaluation shows that a model interpretation can ei-\nther fail due to a lack of plausibility or faithfulness,\nand the interpretations are trustworthy only when\nthey do well with both tests. We also noticed that\nthe performance of saliency interpretations are gen-\nerally sensitive to even minor model conﬁguration\nchanges. Hence, trends of interpretation quality\non a speciﬁc model conﬁguration should not be\nover-generalized to other conﬁgurations.\nWe want the community to be aware that saliency\nmethods, like many other post-hoc interpretation\nmethods, still do not generate trustworthy inter-\npretations all the time. Hence, we recommend\nthat adopting any model interpretation method as\na source of knowledge about NLP models’ rea-\nsoning process should only happen after simi-\nlar quantitative checks as presented in this paper\nare performed. We also hope our proposed test\nparadigm and accompanied test sets provide use-\nful guidance to future work on evaluations of in-\nterpretation methods. Our evaluation dataset and\ncode to reproduce the analysis are available at\nhttps://github.com/shuoyangd/tarsius.\nAcknowledgements\nThe authors would like to thank colleagues at CLSP\nand anonymous reviewers for feedback at various\nstages of the draft. This material is based upon\nwork supported by the United States Air Force\nunder Contract No. FA8750-19-C-0098. Any opin-\nions, ﬁndings, and conclusions or recommenda-\ntions expressed in this material are those of the\nauthor(s) and do not necessarily reﬂect the views\nof the United States Air Force and DARPA.\nReferences\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian J.\nGoodfellow, Moritz Hardt, and Been Kim. 2018.\nSanity checks for saliency maps. In Advances\nin Neural Information Processing Systems 31: An-\nnual Conference on Neural Information Processing\nSystems 2018, NeurIPS 2018, 3-8 December 2018,\nMontréal, Canada, pages 9525–9536.\nJulius Adebayo, Michael Muelly, Ilaria Liccardi, and\nBeen Kim. 2020. Debugging tests for model expla-\nnations. CoRR, abs/2011.05429.\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2017. Fine-grained anal-\nysis of sentence embeddings using auxiliary pre-\ndiction tasks. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Pro-\nceedings.\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary,\nBo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.\n2018. Generating natural language adversarial ex-\namples. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2890–2896, Brussels, Belgium. Association\nfor Computational Linguistics.\nLeila Arras, Franziska Horn, Grégoire Montavon,\nKlaus-Robert Müller, and Wojciech Samek. 2016.\nExplaining predictions of non-linear classiﬁers in\nNLP. In Proceedings of the 1st Workshop on Repre-\nsentation Learning for NLP, pages 1–7, Berlin, Ger-\nmany. Association for Computational Linguistics.\nLeila Arras, Grégoire Montavon, Klaus-Robert Müller,\nand Wojciech Samek. 2017. Explaining recurrent\nneural network predictions in sentiment analysis. In\nProceedings of the 8th Workshop on Computational\nApproaches to Subjectivity, Sentiment and Social\nMedia Analysis, pages 159–168, Copenhagen, Den-\nmark. Association for Computational Linguistics.\nLeila Arras, Ahmed Osman, Klaus-Robert Müller, and\nWojciech Samek. 2019. Evaluating recurrent neural\nnetwork explanations. In Proceedings of the 2019\nACL Workshop BlackboxNLP: Analyzing and Inter-\npreting Neural Networks for NLP , pages 113–126,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2020. A diagnostic\nstudy of explainability techniques for text classiﬁ-\ncation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP) , pages 3256–3274, Online. Associa-\ntion for Computational Linguistics.\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\n7th International Conference on Learning Represen-\ntations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019.\nJasmijn Bastings and Katja Filippova. 2020. The ele-\nphant in the interpretability room: Why use atten-\ntion as explanation when we have saliency methods?\nIn Proceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for\nNLP, pages 149–155, Online. Association for Com-\nputational Linguistics.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2017. Quasi-recurrent neural net-\nworks. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings.\nYong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.\nRobust neural machine translation with doubly ad-\nversarial inputs. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4324–4333, Florence, Italy. Associa-\ntion for Computational Linguistics.\nYong Cheng, Lu Jiang, Wolfgang Macherey, and Ja-\ncob Eisenstein. 2020. AdvAug: Robust adversar-\nial augmentation for neural machine translation. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5961–\n5970, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nMisha Denil, Alban Demiraj, and Nando de Freitas.\n2014. Extraction of salient sentences from labelled\ndocuments. CoRR, abs/1412.6815.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,\nEric Lehman, Caiming Xiong, Richard Socher, and\nByron C. Wallace. 2020. ERASER: A benchmark to\nevaluate rationalized NLP models. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 4443–4458, On-\nline. Association for Computational Linguistics.\nShuoyang Ding, Hainan Xu, and Philipp Koehn. 2019.\nSaliency-driven word alignment interpretation for\nneural machine translation. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n1: Research Papers), pages 1–12, Florence, Italy. As-\nsociation for Computational Linguistics.\nAmirata Ghorbani, Abubakar Abid, and James Y . Zou.\n2019. Interpretation of neural networks is fragile.\nIn The Thirty-Third AAAI Conference on Artiﬁcial\nIntelligence, AAAI 2019, The Thirty-First Innova-\ntive Applications of Artiﬁcial Intelligence Confer-\nence, IAAI 2019, The Ninth AAAI Symposium on Ed-\nucational Advances in Artiﬁcial Intelligence, EAAI\n2019, Honolulu, Hawaii, USA, January 27 - Febru-\nary 1, 2019, pages 3681–3688. AAAI Press.\nYiding Hao. 2020. Evaluating attribution methods us-\ning white-box LSTMs. In Proceedings of the Third\nBlackboxNLP Workshop on Analyzing and Interpret-\ning Neural Networks for NLP , pages 300–313, On-\nline. Association for Computational Linguistics.\nJuyeon Heo, Sunghwan Joo, and Taesup Moon. 2019.\nFooling neural network interpretations via adversar-\nial model manipulation. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, 8-14 December 2019, Vancou-\nver, BC, Canada, pages 2921–2932.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nSara Hooker, Dumitru Erhan, Pieter-Jan Kindermans,\nand Been Kim. 2019. A benchmark for interpretabil-\nity methods in deep neural networks. In Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, 8-14 December 2019,\nVancouver, BC, Canada, pages 9734–9745.\nAlon Jacovi and Yoav Goldberg. 2020. Towards faith-\nfully interpretable NLP systems: How should we de-\nﬁne and evaluate faithfulness? In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4198–4205, Online. As-\nsociation for Computational Linguistics.\nHuaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu,\nNanning Zheng, and Shipeng Li. 2013. Salient ob-\nject detection: A discriminative regional feature in-\ntegration approach. In 2013 IEEE Conference on\nComputer Vision and Pattern Recognition, Portland,\nOR, USA, June 23-28, 2013, pages 2083–2090.\nJaap Jumelet, Willem Zuidema, and Dieuwke Hupkes.\n2019. Analysing neural language models: Con-\ntextual decomposition reveals default reasoning in\nnumber and gender assignment. In Proceedings of\nthe 23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL), pages 1–11, Hong Kong,\nChina. Association for Computational Linguistics.\nNajoung Kim, Roma Patel, Adam Poliak, Patrick Xia,\nAlex Wang, Tom McCoy, Ian Tenney, Alexis Ross,\nTal Linzen, Benjamin Van Durme, Samuel R. Bow-\nman, and Ellie Pavlick. 2019. Probing what dif-\nferent NLP tasks teach machines about function\nword comprehension. In Proceedings of the Eighth\nJoint Conference on Lexical and Computational Se-\nmantics (*SEM 2019), pages 235–249, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nJierui Li, Lemao Liu, Huayang Li, Guanlin Li, Guop-\ning Huang, and Shuming Shi. 2020. Evaluating ex-\nplanation methods for neural machine translation. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 365–\n375, Online. Association for Computational Linguis-\ntics.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016a. Visualizing and understanding neural mod-\nels in NLP. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 681–691, San Diego, California.\nAssociation for Computational Linguistics.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Un-\nderstanding neural networks through representation\nerasure. CoRR, abs/1612.08220.\nZachary C. Lipton. 2018. The mythos of model inter-\npretability. Commun. ACM, 61(10):36–43.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of english: The penn treebank. Comput. Lin-\nguistics, 19(2):313–330.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing LSTM\nlanguage models. In 6th International Conference\non Learning Representations, ICLR 2018, Vancou-\nver, BC, Canada, April 30 - May 3, 2018, Confer-\nence Track Proceedings.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings.\nPramod Kaushik Mudrakarta, Ankur Taly, Mukund\nSundararajan, and Kedar Dhamdhere. 2018. Did\nthe model understand the question? In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1896–1906, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nNina Poerner, Hinrich Schütze, and Benjamin Roth.\n2018. Evaluating neural network explanation meth-\nods using hybrid documents and morphosyntactic\nagreement. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 340–350, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Yuchen Zhang. 2012. Conll-\n2012 shared task: Modeling multilingual unre-\nstricted coreference in ontonotes. In Joint Con-\nference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Lan-\nguage Learning - Proceedings of the Shared Task:\nModeling Multilingual Unrestricted Coreference in\nOntoNotes, EMNLP-CoNLL 2012, July 13, 2012,\nJeju Island, Korea, pages 1–40.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter.CoRR,\nabs/1910.01108.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural MT learn source syntax? In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1526–\n1534, Austin, Texas. Association for Computational\nLinguistics.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. 2013. Deep inside convolutional networks: Vi-\nsualising image classiﬁcation models and saliency\nmaps. CoRR, abs/1312.6034.\nDaniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B.\nViégas, and Martin Wattenberg. 2017. Smooth-\ngrad: removing noise by adding noise. CoRR,\nabs/1706.03825.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Pro-\nceedings of the 34th International Conference on\nMachine Learning, ICML 2017, Sydney, NSW, Aus-\ntralia, 6-11 August 2017, pages 3319–3328.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Conference of the Associ-\nation for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1:\nLong Papers, pages 4593–4601.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019b. What do you\nlearn from context? probing for sentence structure\nin contextualized word representations. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nKristina Toutanova, Dan Klein, Christopher D. Man-\nning, and Yoram Singer. 2003. Feature-rich part-of-\nspeech tagging with a cyclic dependency network.\nIn Proceedings of the 2003 Human Language Tech-\nnology Conference of the North American Chapter\nof the Association for Computational Linguistics ,\npages 252–259.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 5998–6008.\nChih-Kuan Yeh, Cheng-Yu Hsieh, Arun Suggala,\nDavid I Inouye, and Pradeep K Ravikumar. 2019.\nOn the (in) ﬁdelity and sensitivity of explanations.\nIn Advances in Neural Information Processing Sys-\ntems, pages 10965–10976.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages 15–20,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nA Data Filtering Details\nA.1 Penn Treebank (PTB)\nA potential candidate for a test case is extracted\nevery time a word with POS tag VBZ (Verb, 3rd\nperson singular present) or VBP (Verb, non-3rd\nperson singular present), or a copula that is among\nis, are, was, were, shows up. The candidate will\nthen be ﬁltered subjecting to the following criteria:\n1. The preﬁx has at least one attractor word (a\nnoun that has different morphological num-\nber as the verb that is predicted). This is to\nensure that evaluation could be conducted in\nalternative scenario.\n2. The verb cannot immediately follow its gram-\nmatical subject (note: it may still immediately\nfollow a cue word that is not a grammatical\nsubject). This is to ensure that the signal of\nthe subject is not overwhelmingly-strong com-\npared to the attractors.\n3. Not all attractors occur earlier 10 words than\nthe grammatical subject. Same reason as the\nprevious criteria.\nOverall, we obtained 1448 test cases out of\n49168 sentences in PTB (including train, dev and\ntest set). We lose a vast majority of sentences\nmostly because of the last two criteria.\nA.2 Syneval\nWe use the following sections of the original data\n(followed by their names in the data dump, Marvin\nand Linzen, 2018):\n• Agreement in a sentential complemenet:\nsent_comp\n• Agreement across a prepositional phrase:\nprep_anim and prep_inanim\n• Agreement across a subject relative clause:\nsubj_rel\n• Agreement across an object relative clause:\nobj_rel_across_anim,\nobj_rel_across_inanim,\nobj_rel_no_comp_across_anim,\nobj_rel_no_comp_across_inanim\n• Agreement within an object relative clause:\nobj_rel_within_anim,\nobj_rel_within_inanim,\nobj_rel_no_comp_within_anim,\nobj_rel_no_comp_within_inanim\nWe select these sections because they all have\nstrong interfering attractors or have cues that may\npotentially be mistaken as attractors. We obtained\nmuch less examples (6280) than the original data\n(249760) because lots of examples only differ in\nthe verb or the object they use, which become du-\nplicates when we extract preﬁx before the verb.\nThe original dataset does not come with\ncue/attractor annotations, but it can be easily in-\nferred because they are generated by simple heuris-\ntics.\nNote that most of these sections have only\naround 50% prediciton accuracy with RNNs in the\noriginal paper. Our results on large-scale language\nmodels corroborate the ﬁndings in the original pa-\nper.\nA.3 CoNLL\nWe use the dataset (Pradhan et al., 2012) with gold\nparses, entities mentions and mention boundaries.\nA potential candidate for a test case is extracted ev-\nery time a pronoun shows up. The male pronouns\nare he, him, himself, his, while the female pronouns\nare she, her, herself, hers. We don’t consider cases\nepicene pronouns like it, they, etc. because they\noften involve tricky cases like entity mentions cov-\nering a whole clause. We break preﬁxes according\nto the document boudaries as provided in the orig-\ninal dataset unless the preﬁx is longer than 512\nwords, in which case we instead break at the near-\nest sentence boundary.\nThe annotation for this dataset does not cover\nthe gender of entities. We are aware that the origi-\nnal shared task provides gender annotation, but to\nthis day, the documentation for the data is missing\nand hence we cannot make use of this annotation.\nHence, we instead used several heuristics to infer\nthe gender of an entity mention, in desending order:\n• If an entity mention and a pronoun have coref-\nerence relationship, they should share the same\ngender.\n• If an entity mention starts with “Mr.” or “Mrs.”\nor “Ms.”, we assign the corresponding gender.\n• If the entity mention has a length of two tokens,\nwe assume its a name and use gender inference\ntools6 to guess its gender. Note that the gender\n6https://github.com/lead-ratings/gender-guesser\nguesser may also indicate that it’s not able to\ninfer the gender, in that case we do not assign a\ngender.\n• If a mention is coreferenced with another men-\ntion that is not pronoun, they should also have\nthe same gender.\nManual inspection of the resulting data indicates\nthat the scheme above covers the gender of most\nentity mentions correctly. We hope that our dataset\ncould be further perfected by utilizing higher qual-\nity annotation on entity genders.\nSince each entity mention could span more than\none words, we add all words within the span into\ntheir corresponding cue/attractor set. A tricky case\nis where two entity mention spans are nested or\nintersected. For the ﬁrst case, we exclude smaller\nspan from the larger one to create two unintersected\nspans as the new span for cue/attracor set. For the\nsecond case, we exclude the intersecting parts from\nboth spans.\nFinally, all candidates are ﬁltered subjecting to\nthe following two criteria:\n1. The preﬁx should include one attractor entity.\n2. The entity mention that is cloest to the verb\nshould be an attractor.\nWe obtained 586 document segments from the\n2280 documents in the original data. As pointed\nout in Zhao et al. (2018), the CoNLL dataset is\nsigniﬁcantly biased towards male entity mentions.\nNevertheless, our ﬁltering scheme generated a rela-\ntively balanced test set: among the 586 test cases,\n258 are male pronouns, while 328 are female pro-\nnouns.\nA.4 Winobias\nWe used the same data as the unambiguous coref-\nerence resolution dataset in Jumelet et al. (2019),\nwhich is in turn generated by a script from Zhao\net al. (2018), except that we excluded the cases\nwhere both nouns in the sentence are of the same\ngender. Similar to Syneval dataset, the cue and\nattractors could easily be inferred with heuristics.\nB Additional Results\nWe leave some results that we cannot ﬁt into the\nmain paper here.\nB.1 Vector Norm (VN) Composition Scheme\nIn this section, we explain why we chose not to\ncover the vector norm composition scheme (men-\ntioned in 2) in our main evaluation results.\nWe would like to argue ﬁrst that even math-\nematically, VN is not a good ﬁt for our evalua-\ntion paradigm. Vector norm composition scheme\nwill only indicate the importance of a feature, but\nwill not indicate the polarity of the importance be-\ncause it cannot generate a negative word impor-\ntance score, which is important for our evaluation.\nThe reason why it is important is that our plausi-\nbility evaluation does distinguish between input\nwords that should have positive/negative impor-\ntance scores by placing them in cue and attractor\nsets, respectively. For example, in Table 1, the sin-\ngular proper noun U.S. and Europe are important\ninput words because they could potentially lead the\nmodel to make the alternative prediction is instead\nof the expected prediction are. Hence, they are\nplaced into the attractor set, and when interpret-\ning the next word prediction are, our plausibility\ntest expects that they should have large negative\nimportance scores.\nBesides, we did run the plausibility evaluation\nwith vector norm composition scheme under some\nsettings, as shown in Table 8. For the vanilla gradi-\nent saliency method, the VN composition scheme\nperforms on-par with the gradient ·input (GI)\nscheme (which is used for our main results). How-\never, with SmoothGrad, the plausibility result does\nnot signiﬁcantly change like the case with the gra-\ndient ·input (GI) scheme. This corroborates with\nthe results in (Ding et al., 2019), where they also\nshow that SmoothGrad does not improve the inter-\npretation quality with VN composition scheme.\nWith these theoretical and empirical evidence,\nwe decided to drop vector norm composition\nscheme for our evaluation.\nB.2 Patterns for Changes of Interpretation\nQuality with Varying Model\nConﬁgurations\nAs mentioned in Section 5.3, we would like to\nknow if there are any predictable patterns in how\ninterpretation quality changes with varying model\nconﬁgurations. To answer this question, we build\nsmaller Transformer language models with various\ndepth, number of heads, embedding size, and feed-\nforward layer width settings, while keeping other\nhyperparameters unchanged.\nNumber Agreement Gender Agreement\nPTB Syneval CoNLL Winobias\nall exp. alt. all exp. alt. all exp. alt. all exp. alt.\nLSTM (0.858) (0.142) (0.596) (0.404) (0.730) (0.270) (0.584) (0.416)\nV+VN 0.683 0.719 0.463 0.643 0.466 0.903 0.459 0.393 0.639 0.680 0.807 0.502\nSG+VN 0.543 0.540 0.561 0.549 0.271 0.959 0.394 0.234 0.829 0.625 0.587 0.678\nQRNN (0.818) (0.182) (0.558) (0.442) (0.712) (0.288) (0.715) (0.285)\nV+VN 0.630 0.673 0.437 0.579 0.456 0.735 0.427 0.309 0.716 0.526 0.650 0.214\nSG+VN 0.559 0.567 0.521 0.556 0.352 0.813 0.398 0.230 0.811 0.539 0.538 0.540\nTransformer (0.919) (0.081) (0.594) (0.406) (0.761) (0.239) (0.219) (0.781)\nV+VN 0.604 0.620 0.424 0.671 0.525 0.885 0.507 0.511 0.493 0.481 0.840 0.380\nSG+VN 0.592 0.596 0.542 0.654 0.504 0.872 0.529 0.538 0.500 0.437 0.836 0.325\nTable 8: Plausibility benchmark result for Vector Norm (VN) composition scheme. Refer to the caption of Table 3\nfor notations.\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00plausibility\n(a) PTB-SG\n2l 4l 8l\nlayers\n(b) PTB-IG\n2l 4l 8l\nlayers\n(c) CoNLL-SG\n2l 4l 8l\nlayers\n(d) CoNLL-IG\n2l 4l 8l\nlayers\n1024w_1024e_8h\n1024w_256e_8h\n1024w_512e_2h\n1024w_512e_4h\n1024w_512e_8h\n2048w_512e_8h\n512w_512e_8h\nconfig\n20\n40\n60\n80\nparams (M)\nFigure 1: Analysis of model conﬁguration vs. plausibility on PTB and CoNLL benchmark. Each model conﬁgu-\nration is color-coded, while the parameter size (in millions) is shown with circle size. l, w, e, h stands for model\ndepth, width of feed-forward layers after self-attention, embedding size, and the number of heads.\nWe show two different groups of comparison\nhere. Figure 1 shows our investigation on the in-\nteraction between model conﬁguration and inter-\npretation plausibility on PTB and CoNLL test sets.\nIn general, Integrated Gradients method works bet-\nter for deeper models, while SG works better for\nshallower models on the PTB test set, but remains\nroughly the same performance for all architectures\non the CoNLL test set. This indicates the noisiness\nof the trend we are investigating, as both inter-\npretability methods and evaluation dataset choice\ncan inﬂuence the trend. As for the other factors of\nthe model conﬁgurations, the trend is even noisier\n(note how much rankings of different conﬁgura-\ntions change moving from shallow to deep models)\nand do not show any clear patterns.\nFigure 2, on the other hand, focuses on one\nspeciﬁc dataset and investigate the trend on both\nthe plausibility and input faithfulness with varying\nmodel conﬁgurations. For plausibility results, we\nlargely see the same trend as on PTB dataset. For\nfaithfulness results, the trend for SG is largely the\nsame as plausibility. For IG, the variance across\nother factors of conﬁgurations tends to be different\non shallower models vs. deeper models, but overall\nstill shows higher numbers for deeper models like\nfor plausibility.\nOverall, these analyses further support our con-\nclusion in the main paper, that interpretation quali-\nties are sensitive to model conﬁguration changes,\nand we reiterate that evaluations of saliency meth-\nods should be conducted on the speciﬁc model con-\nﬁgurations of interest, and trends of interpretation\nquality on a speciﬁc model conﬁguration should\nnot be over-generalized to other conﬁgurations.\n1024w_1024e_8h\n1024w_256e_8h\n1024w_512e_2h\n1024w_512e_4h\n1024w_512e_8h\n2048w_512e_8h\n512w_512e_8h\narch\n20\n40\n60\n80\nparams\n2l\n4l\n8l\nlayers\n0.6\n0.7\n0.8\n0.9\n1.0plausibility\n1024w_1024e_8h\n1024w_256e_8h\n1024w_512e_2h\n1024w_512e_4h\n1024w_512e_8h\n2048w_512e_8h\n512w_512e_8h\narch\n20\n40\n60\n80\nparams\n2l\n4l\n8l\nlayers\n0.6\n0.7\n0.8\n0.9\n1.0plausibility\n(a) Plausibility benchmark with SmoothGrad (b) Plausibility benchmark with Integrated Gradients\n1024w_1024e_8h\n1024w_256e_8h\n1024w_512e_2h\n1024w_512e_4h\n1024w_512e_8h\n2048w_512e_8h\n512w_512e_8h\narch\n20\n40\n60\n80\nparams\n2l\n4l\n8l\nlayers\n0.2\n0.4\n0.6\n0.8\n1.0faithfulness\n1024w_1024e_8h\n1024w_256e_8h\n1024w_512e_2h\n1024w_512e_4h\n1024w_512e_8h\n2048w_512e_8h\n512w_512e_8h\narch\n20\n40\n60\n80\nparams\n2l\n4l\n8l\nlayers\n0.2\n0.4\n0.6\n0.8\n1.0faithfulness\n(c) Faithfulness benchmark with SmoothGrad (d) Faithfulness benchmark with Integrated Gradients\nFigure 2: Analysis of model conﬁguration vs. plausibility and faithfulness on Syneval benchmark. Each model\nconﬁguration is color-coded, while the parameter size (in millions) is shown with circle size. l, w, e, h stands for\nmodel depth, width of feed-forward layers after self-attention, embedding size, and the number of heads. Note that\nthe faithfulness numbers plotted here are the ones interpreted with expected scenario predictions.\nC Language Model Perplexities\nParameter size and perplexity on WikiText-103 dev\nset for all language models are shown in Table 9\nfor reference.\nBelow are the respective commands to reproduce\nthese results.\n• LSTM: python -u main.py -epochs 50\n-nlayers 3 -emsize 400 -nhid 2000\n-dropoute 0 -dropouth 0.01 -dropouti\n0.01 -dropout 0.4 -wdrop 0.2 -bptt\n140 -batch_size 60 -optimizer adam\n-lr 1e-3 -data data/wikitext-103 -save\nsave -when 25 35 -model LSTM\n• QRNN: python -u main.py -epochs 14\n-nlayers 4 -emsize 400 -nhid 2500\n-alpha 0 -beta 0 -dropoute 0 -dropouth\n0.1 -dropouti 0.1 -dropout 0.1 -wdrop\n0 -wdecay 0 -bptt 140 -batch_size\n40 -optimizer adam -lr 1e-3 -data\ndata/wikitext-103 -save save -when 12\n-model QRNN\n• Transformer: python train.py\n-task language_modeling\ndata-bin/wikitext-103\n-save-dir checkpoints -arch\ntransformer_lm_wiki103 -decoder-layers\n$layers -decoder-attention-heads\n$num_heads -decoder-embed-dim\n$emb -decoder-ffn-embed-dim $width\n-max-update 286000 -max-lr 1.0\n-t-mult 2 -lr-period-updates 270000\n-lr-scheduler cosine -lr-shrink 0.75\n-warmup-updates 16000 -warmup-init-lr\n1e-07 -min-lr 1e-09 -optimizer nag\n-lr 0.0001 -clip-norm 0.1 -criterion\nadaptive_loss -max-tokens 3072\n-update-freq 3 -tokens-per-sample\n3072 -seed 1 -sample-break-mode none\n-skip-invalid-size-inputs-valid-test\n-ddp-backend=no_c10d\nArchitectures Layers Conﬁg Params (M) dev ppl\nLSTM 3 - 162 37.65\n2 - 130 41.97\nQRNN 4 - 154 32.12\n3 - 135 36.54\nTransformer\n16 4096w_1024e_8h 247 17.97\n4 4096w_1024e_8h (Distill Student) 96.1 24.92\n4 4096w_1024e_8h 96.1 28.96\n8 1024w_512e_2h 39.3 32.63\n8 1024w_512e_4h 39.3 32.09\n8 1024w_512e_8h 39.3 31.38\n8 512w_512e_8h 35.1 33.99\n8 2048w_512e_8h 47.7 30.19\n8 1024w_256e_8h 17.4 41.56\n8 1024w_1024e_8h 96.1 27.01\n4 1024w_512e_2h 30.8 37.03\n4 1024w_512e_4h 30.8 35.67\n4 1024w_512e_8h 30.8 35.82\n4 512w_512e_8h 28.7 38.34\n4 2048w_512e_8h 35.0 33.70\n4 1024w_256e_8h 14.3 48.47\n4 1024w_1024e_8h 70.9 30.46\n2 1024w_512e_2h 26.6 44.45\n2 1024w_512e_4h 26.6 42.23\n2 1024w_512e_8h 26.6 41.86\n2 512w_512e_8h 25.6 44.97\n2 2048w_512e_8h 28.7 38.99\n2 1024w_256e_8h 12.7 59.16\n2 1024w_1024e_8h 58.3 36.06\nTable 9: Parameter size (in millions) and perplexity on WikiText-103 dev set for all language models we trained.\nD Additional Interpretation Examples\nWe show some additional interpretations generated\nby the state-of-the-art LSTM (Table 10), QRNN\n(Table 11) and Transformer (Table 12) models on\nPTB and CoNLL dataset, with their respective best-\nperforming interpretation method.\nPTB\n1- (U.S.)(Trade)(Representative)(Carla)(Hills)said the first\ndispute-settlement(panel)set up under the U.S.-Canadian“ free (trade)\n” (agreement)has ruled that (Canada)’s [restrictions]on [exports]of\n(Pacific)(salmon)and (herring) | PLURAL\n2- Individual[investors], (investment)[firms]and [arbitragers]who\nspeculatein the [stocks]of (takeover)[candidates]can suffer(liquidity)\nand (payment)[problems]when [stocks]dive ; those [investors]often | PLURAL\n3- (U.S.)[companies]wantingto expandin (Europe) | PLURAL\n4- CURBING[WAGE](BOOSTS)will get high [priority]again in 1990 collective\n[bargaining], a [Bureau]of [National][Affairs][survey]of 250 (companies)\nwith (pacts)expiringnext [year] | PLURAL\n5- TEMPORARY(WORKERS)have good (educations), the [National][Association]\nof [Temporary][Services] | SINGULAR\nCoNLL\n1- [Israeli][Prime][Minister][Ehud][Barak]says [he] is freezingtens of\nmillionsof dollarsin tax paymentsto the PalestinianAuthority. [Mr.]\n[Barak]says [he] is withholdingthe money until the Palestiniansabide\nby cease - fire agreements. EarlierThursday[Mr.] [Barak]ruled out an\nearly resumptionof peace talks , even with the UnitedStatesactingas\nintermediary. (Eve) (Conette)reportsfrom Jerusalem. Defendingwhat | MALE\n2- Once again there ’ll be two presidentialcandidatesmissingfrom the\ndebate. Pat Buchananhardlyregisterson the politicalradar this year .\nAnd Ralph Nader , who may make the differencebetweena [Gore]or [Bush]\nwin in severalplaces. (ABC) (’s) (Linda)(Douglas)was with | FEMALE\nTable 10: Addition interpretation examples with LSTM.\nPTB\n1- (U.S.)(Trade)(Representative)(Carla)(Hills)said the first\ndispute-settlement(panel)set up under the U.S.-Canadian“ free (trade)\n” (agreement)has ruled that (Canada)’s [restrictions]on [exports]of\n(Pacific)(salmon)and (herring) | PLURAL\n2- Individual[investors], (investment)[firms]and [arbitragers]who\nspeculatein the [stocks]of (takeover)[candidates]can suffer(liquidity)\nand (payment)[problems]when [stocks]dive ; those [investors]often | PLURAL\n3- (U.S.)[companies]wantingto expandin (Europe) | PLURAL\n4- CURBING[WAGE](BOOSTS)will get high [priority]again in 1990 collective\n[bargaining], a [Bureau]of [National][Affairs][survey]of 250 (companies)\nwith (pacts)expiringnext [year] | PLURAL\n5- TEMPORARY(WORKERS)have good (educations), the [National][Association]\nof [Temporary][Services] | PLURAL\nCoNLL\n1- [Israeli][Prime][Minister][Ehud][Barak]says [he] is freezingtens of\nmillionsof dollarsin tax paymentsto the PalestinianAuthority. [Mr.]\n[Barak]says [he] is withholdingthe money until the Palestiniansabide\nby cease - fire agreements. EarlierThursday[Mr.] [Barak]ruled out an\nearly resumptionof peace talks , even with the UnitedStatesactingas\nintermediary. (Eve) (Conette)reportsfrom Jerusalem. Defendingwhat |\nFEMALE\n2- Once again there ’ll be two presidentialcandidatesmissingfrom the\ndebate. Pat Buchananhardlyregisterson the politicalradar this year .\nAnd Ralph Nader , who may make the differencebetweena [Gore]or [Bush]\nwin in severalplaces. (ABC) (’s) (Linda)(Douglas)was with | FEMALE\nTable 11: Addition interpretation examples with QRNN.\nPTB\n1- (U.S.)(Trade)(Representative)(Carla)(Hills)said the first\ndispute-settlement(panel)set up under the U.S.-Canadian“ free (trade)\n” (agreement)has ruled that (Canada)’s [restrictions]on [exports]of\n(Pacific)(salmon)and (herring) | PLURAL\n2- Individual[investors], (investment)[firms]and [arbitragers]who\nspeculatein the [stocks]of (takeover)[candidates]can suffer(liquidity)\nand (payment)[problems]when [stocks]dive ; those [investors]often | PLURAL\n3- (U.S.)[companies]wantingto expandin (Europe) | PLURAL\n4- CURBING[WAGE](BOOSTS)will get high [priority]again in 1990 collective\n[bargaining], a [Bureau]of [National][Affairs][survey]of 250 (companies)\nwith (pacts)expiringnext [year] | PLURAL\n5- TEMPORARY(WORKERS)have good (educations), the [National][Association]\nof [Temporary][Services] | SINGULAR\nCoNLL\n1- [Israeli][Prime][Minister][Ehud][Barak]says [he] is freezingtens of\nmillionsof dollarsin tax paymentsto the PalestinianAuthority. [Mr.]\n[Barak]says [he] is withholdingthe money until the Palestiniansabide\nby cease - fire agreements. EarlierThursday[Mr.] [Barak]ruled out an\nearly resumptionof peace talks , even with the UnitedStatesactingas\nintermediary. (Eve) (Conette)reportsfrom Jerusalem. Defendingwhat |\nFEMALE\n2- Once again there ’ll be two presidentialcandidatesmissingfrom the\ndebate. Pat Buchananhardlyregisterson the politicalradar this year .\nAnd Ralph Nader , who may make the differencebetweena [Gore]or [Bush]\nwin in severalplaces. (ABC) (’s) (Linda)(Douglas)was with | MALE\nTable 12: Addition interpretation examples with Transformer.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7595261335372925
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6794861555099487
    },
    {
      "name": "Natural language processing",
      "score": 0.6122983694076538
    },
    {
      "name": "Sentence",
      "score": 0.5738734006881714
    },
    {
      "name": "Artificial neural network",
      "score": 0.5377116203308105
    },
    {
      "name": "Trustworthiness",
      "score": 0.5200269222259521
    },
    {
      "name": "Annotation",
      "score": 0.4911617934703827
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4858781695365906
    },
    {
      "name": "Property (philosophy)",
      "score": 0.44000041484832764
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.41881853342056274
    },
    {
      "name": "Machine learning",
      "score": 0.4024386703968048
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ],
  "cited_by": 6
}