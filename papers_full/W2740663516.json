{
  "title": "Dynamic Entity Representations in Neural Language Models",
  "url": "https://openalex.org/W2740663516",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222075082",
      "name": "Ji, Yangfeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2740243474",
      "name": "Tan, Chenhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4302647196",
      "name": "Martschat, Sebastian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2149953915",
      "name": "Choi, Yejin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221392677",
      "name": "Smith, Noah A.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2550448043",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2593847145",
    "https://openalex.org/W2336260055",
    "https://openalex.org/W2155069789",
    "https://openalex.org/W2563734883",
    "https://openalex.org/W2964036636",
    "https://openalex.org/W2180160918",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2341790067",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2151295812",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2577255746",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2053218206",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2962769558",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W1789782362"
  ],
  "abstract": "Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, EntityNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.",
  "full_text": "arXiv:1708.00781v1  [cs.CL]  2 Aug 2017\nDynamic Entity Representations in Neural Language Models\nY angfeng Ji∗ Chenhao T an∗ Sebastian Martschat† Y ejin Choi∗ Noah A. Smith∗\n∗Paul G. Allen School of Computer Science & Engineering, Univ ersity of W ashington\n†Department of Computational Linguistics, Heidelberg Univ ersity\n{yangfeng,chenhao,yejin,nasmith}@cs.washington.edu\nmartschat@cl.uni-heidelberg.de\nAbstract\nUnderstanding a long document requires\ntracking how entities are introduced and\nevolve over time. W e present a new type of\nlanguage model, E N TITY NLM, that can\nexplicitly model entities, dynamically up-\ndate their representations, and contextu-\nally generate their mentions. Our model is\ngenerative and ﬂexible; it can model an ar-\nbitrary number of entities in context while\ngenerating each entity mention at an arbi-\ntrary length. In addition, it can be used\nfor several different tasks such as language\nmodeling, coreference resolution, and en-\ntity prediction. Experimental results with\nall these tasks demonstrate that our model\nconsistently outperforms strong baselines\nand prior work.\n1 Introduction\nUnderstanding a narrative requires keeping track\nof its participants over a long-term context. As a\nstory unfolds, the information a reader associates\nwith each character in a story increases, and ex-\npectations about what will happen next change ac-\ncordingly . At present, models of natural language\ndo not explicitly track entities; indeed, in today’s\nlanguage models, entities are no more than the\nwords used to mention them.\nIn this paper, we endow a generative language\nmodel with the ability to build up a dynamic rep-\nresentation of each entity mentioned in the text.\nOur language model deﬁnes a probability distribu-\ntion over the whole text, with a distinct generative\nstory for entity mentions. It explicitly groups those\nmentions that corefer and associates with each en-\ntity a continuous representation that is updated by\nevery contextualized mention of the entity , and\nthat in turn affects the text that follows.\n[John]1 wanted to go to [ the coffee shop ]2 in\n[downtown Copenhagen]3. [ He]1 was told that\n[it]2 sold [ the best beans]4.\nFigure 1: E N TITY NLM explicitly tracks entities\nin a text, including coreferring relationships be-\ntween entities like [\nJohn]1 and [ He]1. As a lan-\nguage model, it is designed to predict that a coref-\nerent of [\nthe coffee shop]2 is likely to follow “ told\nthat, ” that the referring expression will be “ it”, and\nthat “ sold the best beans” is likely to come next, by\nusing entity information encoded in the dynamic\ndistributed representation.\nOur method builds on recent advances in repre-\nsentation learning, creating local probability dis-\ntributions from neural networks. It can be un-\nderstood as a recurrent neural network language\nmodel, augmented with random variables for en-\ntity mentions that capture coreference, and with\ndynamic representations of entities. W e estimate\nthe model’s parameters from data that is annotated\nwith entity mentions and coreference.\nBecause our model is generative, it can be\nqueried in different ways. Marginalizing every-\nthing except the words, it can play the role of a lan-\nguage model. In §\n5.1, we ﬁnd that it outperforms\nboth a strong n-gram language model and a strong\nrecurrent neural network language model on the\nEnglish test set of the CoNLL 2012 shared task\non coreference evaluation (\nPradhan et al. , 2012).\nThe model can also identify entity mentions and\ncoreference relationships among them. In §\n5.2,\nwe show that it can easily be used to add a per-\nformance boost to a strong coreference resolution\nsystem, by reranking a list of k-best candidate out-\nputs. On the CoNLL 2012 shared task test set, the\nreranked outputs are signiﬁcantly better than the\noriginal top choices from the same system. Fi-\nnally , the model can perform entity cloze tasks.\nAs presented in §\n5.3, it achieves state-of-the-art\nperformance on the InScript corpus ( Modi et al. ,\n2017).\n2 Model\nA language model deﬁnes a distribution over se-\nquences of word tokens; let Xt denote the random\nvariable for the tth word in the sequence, xt de-\nnote the value of Xt and xt the distributed repre-\nsentation (embedding) of this word. Our starting\npoint for language modeling is a recurrent neural\nnetwork (\nMikolov et al. , 2010), which deﬁnes\np(Xt | history) = softmax ( Whht−1 + b) (1)\nht−1 = LSTM (ht−2, xt−1) (2)\nwhere Wh and b are parameters of the\nmodel (along with word embeddings xt),\nLSTM is the widely used recurrent func-\ntion known as “long short-term memory”\n(\nHochreiter and Schmidhuber , 1997), and ht is\na LSTM hidden state encoding the history of the\nsequence up to the tth word.\nGreat success has been reported for this model\n(Zaremba et al. , 2015), which posits nothing ex-\nplicitly about the words appearing in the text se-\nquence. Its generative story is simple: the value\nof each Xt is randomly chosen conditioned on the\nvector ht−1 encoding its history .\n2.1 Additional random variables and\nrepresentations for entities\nT o introduce our model, we associate with each\nword an additional set of random variables. At po-\nsition t,\n• Rt is a binary random variable that indi-\ncates whether xt belongs to an entity men-\ntion ( Rt = 1 ) or not ( Rt = 0 ). Though not\nexplored here, this is easily generalized to a\ncategorial variable for the type of the entity\n(e.g., person, organization, etc.).\n• Lt ∈ { 1, . . . , ℓ max } is a categorical random\nvariable if Rt = 1 , which indicates the num-\nber of remaining words in this mention, in-\ncluding the current word (i.e., Lt = 1 for\nthe last word in any mention). ℓmax is a\npredeﬁned maximum length ﬁxed to be 25,\nwhich is an empirical value derived from the\ntraining corpora used in the experiments. If\nRt = 0 , then Lt = 1 . W e denote the value of\nLt by ℓt.\n• Et ∈ E t is the index of the entity referred to,\nif Rt = 1 . The set Et consists of {1, . . . , 1 +\nmaxt′<t et′ }, i.e., the indices of all previously\nmentioned entities plus an additional value\nfor a new entity . Thus Et starts as {1} and\ngrows monotonically with t, allowing for an\narbitrary number of entities to be mentioned.\nW e denote the value of Et by et. If Rt = 0 ,\nthen Et is ﬁxed to a special value ø.\nThe values of these random variables for our run-\nning example are shown in\nFigure 2 .\nIn addition to using symbolic variables to en-\ncode mentions and coreference relationships, we\nmaintain a vector representation of each entity that\nevolves over time. For the ith entity , let ei,t be\nits representation at time t. These vectors are\ndifferent from word vectors ( xt), in that they are\nnot parameters of the model. They are similar to\nhistory representations ( ht), in that they are de-\nrived through parameterized functions of the ran-\ndom variables’ values, which we will describe in\nthe next subsections.\n2.2 Generative story\nThe generative story for the word (and other\nvariables) at timestep t is as follows; forward-\nreferenced equations are in the detailed discussion\nthat follows.\n1. If ℓt−1 = 1 (i.e., xt is not continuing an\nalready-started entity mention):\n• Choose rt (\nEquation 3 ).\n• If rt = 0 , set ℓt = 1 and et = ø ; then go\nto step 3. Otherwise:\n– If there is no embedding for the\nnew candidate entity with index\n1 + max t′<t et′ , create one follow-\ning §\n2.4.\n– Select the entity et from {1, . . . , 1 +\nmaxt′<t et′ } (Equation 4 ).\n– Set ecurrent = eet,t−1, which is\nthe entity embedding of et before\ntimestep t.\n– Select the length of the mention, ℓt\n(Equation 5 ).\n2. Otherwise,\n• Set ℓt = ℓt−1 − 1, rt = rt−1, et = et−1.\nX1:12: John wanted to go to the coffee shop in downtown Copenhagen .\nR1:12: 1 0 0 0 0 1 1 1 0 1 1 0\nE1:12: 1 ø ø ø ø 2 2 2 ø 3 3 ø\nL1:12: 1 1 1 1 1 3 2 1 1 2 1 1\nX13:22: He was told that it sold the best beans .\nR13:22: 1 0 0 0 1 0 1 1 1 .\nE13:22: 1 ø ø ø 2 ø 4 4 4 ø\nL13:22: 1 1 1 1 1 1 3 2 1 0\nFigure 2: The random variable values in E N TITY NLM for the running example in Figure 1 .\n3. Sample xt from the word distribution given\nthe LSTM hidden state ht−1 and the current\n(or most recent) entity embedding ecurrent\n(Equation 6 ). (If rt = 0 , then ecurrent still\nrepresents the most recently mentioned en-\ntity .)\n4. Advance the RNN, i.e., feed it the word vec-\ntor xt to compute ht (\nEquation 2 ).\n5. If rt = 1 , update eet,t using eet,t−1 and ht,\nthen set ecurrent = eet,t. Details of the entity\nupdating are given in §2.4.\n6. For every entity eι ∈ E t \\ { et}, set eι,t =\neι,t−1 (i.e., no changes to other entities’ rep-\nresentations).\nNote that at any given time step t, ecurrent will al-\nways contain the most recent vector representation\nof the most recently mentioned entity .\nA generative model with a similar hierarchical\nstructure was used by\nHaghighi and Klein (2010)\nfor coreference resolution. Our approach differs\nin two important ways. First, our model de-\nﬁnes a joint distribution over all of the text, not\njust the entity mentions. Second, we use repre-\nsentation learning rather than Bayesian nonpara-\nmetrics, allowing natural integration with the lan-\nguage model.\n2.3 Probability distributions\nThe generative story above referenced several\nparametric distributions deﬁned based on vector\nrepresentations of histories and entities. These are\ndeﬁned as follows.\nFor r ∈ { 0, 1},\np(Rt = r | ht−1) ∝ exp(h⊤\nt−1Wrr), (3)\nwhere r is the parameterized embedding associ-\nated with r, which paves the way for exploring en-\ntity type representations in future work; Wr is a\nparameter matrix for the bilinear score for ht−1\nand r.\nT o give the possibility of predicting a new en-\ntity , we need an entity embedding beforehand\nwith index (1 + max t′<t et′ ), which is randomly\nsampled from\nEquation 7 . Then, for every e ∈\n{1, . . . , 1 + max t′<t et′ }:\np(Et = e | Rt = 1 , ht−1)\n∝ exp(h⊤\nt−1Wentity ee,t−1 + w⊤\ndist f(e)),\n(4)\nwhere ee,t−1 is the embedding of entity e at time\nstep t−1 and Wentity is the weight matrix for pre-\ndicting entities using their continuous representa-\ntions. The score above is normalized over values\n{1, . . . , 1 + max t′<t et′ }. f(e) represents a vector\nof distance features associated with e and the men-\ntions of the existing entities. Hence two informa-\ntion sources are used to predict the next entity: (i)\ncontextual information ht−1, and (ii) distance fea-\ntures f(e) from the current mention to the closest\nmention from each previously mentioned entity .\nf(e) = 0 if e is a new entity . This term can also\nbe extended to include other surface-form features\nfor coreference resolution (\nMartschat and Strube ,\n2015; Clark and Manning , 2016b).\nFor the chosen entity et from Equation 4 , the\ndistribution over its mention length is drawn ac-\ncording to\np(Lt = ℓ | ht−1, eet,t−1)\n∝ exp(W⊤\nlength,ℓ[ht−1; eet,t−1]),\n(5)\nwhere eet,t−1 is the most recent embedding of the\nentity et, not updated with ht. The intuition is that\neet,t−1 will help contextual information ht−1 to\nselect the residual length of entity et. Wlength\nis the weight matrix for length prediction, with\nℓmax = 25 rows.\nFinally , the probability of a word x as the next\ntoken is jointly modeled by ht−1 and the vector\nrepresentation of the most recently mentioned en-\ntity ecurrent:\np(Xt = x | ht−1, ecurrent)\n∝ C FSM (ht−1 + Weecurrent), (6)\nwhere We is a transformation matrix to ad-\njust the dimensionality of ecurrent. C FSM is\na class factorized softmax function ( Goodman,\n2001; Baltescu and Blunsom , 2015). It uses a two-\nstep prediction with predeﬁned word classes in-\nstead of direct prediction on the whole vocabulary ,\nand reduces the time complexity to the log of vo-\ncabulary size.\n2.4 Dynamic entity representations\nBefore predicting the entity at step t, we need an\nembedding for the new candidate entity with index\ne′ = 1 + max t′<t et′ if it does not exist. The new\nembedding is generated randomly , according to a\nnormal distribution, then projected onto the unit\nball:\nu ∼ N (r1, σ2I);\nee′,t−1 = u\n∥u∥2\n, (7)\nwhere σ = 0 .01. The time step t − 1 in ee′,t−1\nmeans the current embedding contains no infor-\nmation from step t, although it will be updated\nonce we have ht and if Et = e′. r1 is the pa-\nrameterized embedding for Rt = 1 , which will be\njointly optimized with other parameters and is ex-\npected to encode some generic information about\nentities. All the initial entity embeddings are cen-\ntered on the mean r1, which is used in\nEquation 3\nto determine whether the next token belongs to an\nentity mention. Another choice would be to ini-\ntialize with a zero vector, although our preliminary\nexperiments showed this did not work as well as\nrandom initialization in\nEquation 7 .\nAssume Rt = 1 and Et = et, which means xt\nis part of a mention of entity et. Then, we need\nto update eet,t−1 based on the new information we\nhave from ht. The new embedding eet,t is a con-\nvex combination of the old embedding ( eet,t−1)\nand current LSTM hidden state ( ht) with the in-\nterpolation ( δt) determined dynamically based on\na bilinear function:\nδt = σ(h⊤\nt Wδeet,t−1);\nu = δteet,t−1 + (1 − δt)ht;\neet,t = u\n∥u∥2\n, (8)\nThis updating scheme will be used to update et in\neach of all the following ℓt steps. The projection in\nthe last step keeps the magnitude of the entity em-\nbedding ﬁxed, avoiding numeric overﬂow . A simi-\nlar updating scheme has been used by\nHenaff et al.\n(2016) for the “memory blocks” in their recurrent\nentity network models. The difference is that their\nmodel updates all memory blocks in each time\nstep. Instead, our updating scheme in\nEquation 8\nonly applies to the selected entity et at time step t.\n2.5 T raining objective\nThe model is trained to maximize the log of the\njoint probability of R, E, L, and X:\nℓ(θ) = log P (R, E, L, X; θ)\n=\n∑\nt\nlog P (Rt, Et, Lt, Xt; θ), (9)\nwhere θ is the collection of all the parameters\nin this model. Based on the formulation in §\n2.3,\nEquation 9 can be decomposed as the sum of con-\nditional log-probabilities of each random variable\nat each time step.\nThis objective requires the training data anno-\ntated as in\nFigure 2 . W e do not assume that these\nvariables are observed at test time.\n3 Implementation Details\nOur model is implemented with DyNet\n(\nNeubig et al. , 2017) and available at\nhttps://github.com/jiyfeng/entitynlm.\nW e use AdaGrad ( Duchi et al. , 2011) with learn-\ning rate λ = 0 .1 and ADAM ( Kingma and Ba ,\n2014) with default learning rate λ = 0 .001\nas the candidate optimizers of our model. For\nall the parameters, we use the initialization\ntricks recommended by\nGlorot and Bengio\n(2010). T o avoid overﬁtting, we also em-\nploy dropout ( Srivastava et al. , 2014) with the\ncandidate rates as {0.2, 0.5}.\nIn addition, there are two tunable hyperpa-\nrameters of E N TITY NLM: the size of word em-\nbeddings and the dimension of LSTM hidden\nstates. For both of them, we consider the values\n{32, 48, 64, 128, 256}. W e also experiment with\nthe option to either use the pretrained GloV e word\nembeddings (\nPennington et al. , 2014) or randomly\ninitialized word embeddings (then updated during\ntraining). For all experiments, the best conﬁgura-\ntion of hyperparameters and optimizers is selected\nbased on the objective value on the development\ndata.\n4 Evaluation T asks and Datasets\nW e evaluate our model in diverse use scenarios:\n(i) language modeling, (ii) coreference resolution,\nand (iii) entity prediction. The evaluation on lan-\nguage modeling shows how the internal entity rep-\nresentation, when marginalized out, can improve\nthe perplexity of language models. The evaluation\non coreference resolution experiment shows how\nour new language model can improve a compet-\nitive coreference resolution system. Finally , we\nemploy an entity cloze task to demonstrate the\ngenerative performance of our model in predicting\nthe next entity given the previous context.\nW e use two datasets for the three evaluation\ntasks. For language modeling and coreference\nresolution, we use the English benchmark data\nfrom the CoNLL 2012 shared task on corefer-\nence resolution (\nPradhan et al. , 2012). W e employ\nthe standard training/development/test split, which\nincludes 2,802/343/348 documents with roughly\n1M/150K/150K tokens, respectively . W e follow\nthe coreference annotation in the CoNLL dataset\nto extract entities and ignore the singleton men-\ntions in texts.\nFor entity prediction, we employ the InScript\ncorpus created by\nModi et al. (2017). It consists of\n10 scenarios, including grocery shopping, taking a\nﬂight, etc. It includes 910 crowdsourced simple\nnarrative texts in total and 18 stories were ignored\ndue to labeling problems (\nModi et al. , 2017). On\naverage, each story has 12.4 sentences, 24.9 en-\ntities and 217.2 tokens. Each entity mention is\nlabeled with its entity index. W e use the same\ntraining/development/test split as in (\nModi et al. ,\n2017), which includes 619, 91, 182 texts, respec-\ntively .\nData preprocessing\nFor the CoNLL dataset, we lowercase all tokens\nand remove any token that only contains a punctu-\nation symbol unless it is in an entity mention. W e\nalso replace numbers in the documents with the\nspecial token N U M and low-frequency word types\nwith U N K. The vocabulary size of the CoNLL data\nafter preprocessing is 10K. For entity mention ex-\ntraction, in the CoNLL dataset, one entity men-\ntion could be embedded in another. For embed-\nded mentions, only the enclosing entity mention\nis kept. W e use the same preprocessed data for\nboth language modeling and coreference resolu-\ntion evaluation.\nFor the InScript corpus, we apply similar data\npreprocessing to lowercase all tokens, and we re-\nplace low-frequency word types with U N K. The\nvocabulary size after preprocessing is 1K.\n5 Experiments\nIn this section, we present the experimental results\non the three evaluation tasks.\n5.1 Language modeling\nT ask description. The goal of language model-\ning is to compute the marginal probability:\nP (X) =\n∑\nR,E,L\nP (X, R, E, L). (10)\nHowever, due to the long-range dependency in\nrecurrent neural networks, the search space of\nR, E, L during inference grows exponentially .\nW e thus use importance sampling to approxi-\nmate the marginal distribution of X. Speciﬁ-\ncally , with the samples from a proposal distri-\nbution Q(R, E, L|X), the approximated marginal\nprobability is deﬁned as\nP (X) =\n∑\nR,E,L\nP (X, R, E, L)\n=\n∑\nR,E,L\nQ(R, E, L | X) P (X, R, E, L)\nQ(R, E, L | X)\n≈ 1\nN\n∑\n{r(i),e(i),ℓ(i)}∼Q\nP (r(i), e(i), ℓ(i), x)\nQ(r(i), e(i), ℓ(i) | x)\n(11)\nA similar idea of using importance sampling for\nlanguage modeling evaluation has been used by\nDyer et al. (2016).\nFor language modeling evaluation, we train our\nmodel on the training set from the CoNLL 2012\ndataset with coreference annotation. On the test\ndata, we treat coreference structure as latent vari-\nables and use importance sampling to approximate\nthe marginal distribution of X. For each docu-\nment, the model randomly draws N = 100 sam-\nples from the proposal distribution, discussed next.\nProposal distribution. For implementation of\nQ, we use a discriminative variant of E N TI -\nTY NLM by taking the current word xt for predict-\ning the entity-related variables in the same time\nstep. Speciﬁcally , in the generative story described\nin §\n2.2, we delete step 3 (words are not gener-\nated, but rather conditioned upon), move step 4\nModel Perplexity\n1. 5-gram LM 138.37\n2. RNNLM 134.79\n3. E N TITY NLM 131.64\nT able 1: Language modeling evaluation on the test\nsets of the English section in the CoNLL 2012\nshared task. As mentioned in §\n4, the vocabulary\nsize is 10K. E N TITY NLM does not require any\ncoreference annotation on the test data.\nbefore step 1, and replace ht−1 with ht in the\nsteps for predicting entity type Rt, entity Et and\nmention length Lt. This model variant provides a\nconditional probability Q(Rt, Et, Lt | Xt) at each\ntimestep.\nBaselines. W e compare the language modeling\nperformance with two competitive baselines: 5-\ngram language model implemented in KenLM\n(\nHeaﬁeld et al. , 2013) and RNNLM with LSTM\nunits implemented in DyNet ( Neubig et al. , 2017).\nFor RNNLM, we use the same hyperparameters\ndescribed in §\n3 and grid search on the develop-\nment data to ﬁnd the best conﬁguration.\nResults. The results of E N TITY NLM and the\nbaselines on both development and test data are\nreported in\nT able 1. For E N TITY NLM, we use the\nvalue of 2− 1\nT\n∑ T\nt=1 log P (Xt,Rt,Et,Lt) on the devel-\nopment set with coreference annotation to select\nthe best model conﬁguration and report the best\nnumber. On the test data, we are able to calcu-\nlate perplexity by marginalizing all other random\nvariables using\nEquation 11 . T o compute the per-\nplexity numbers on the test data, our model only\ntakes account of log probabilities on word predic-\ntion. The difference is that coreference informa-\ntion is only used for training E N TITY NLM and\nnot for test. All three models reported in\nT able 1\nshare the same vocabulary , therefore the numbers\non the test data are directly comparable. As shown\nin\nT able 1, E N TITY NLM outperforms both the 5-\ngram language model and the RNNLM on the test\ndata. Better performance of E N TITY NLM on lan-\nguage modeling can be expected, if we also use the\nmarginalization method deﬁned in\nEquation 11 on\nthe development data to select the best conﬁgura-\ntion. However, we plan to use the same experi-\nmental setup for all experiments, instead of cus-\ntomizing our model for each individual task.\n5.2 Coreference reranking\nT ask description. W e show how E N TITY LM,\nwhich allows an efﬁcient computation of the\nprobability P (R, E, L, X), can be used as\na coreference reranker to improve a com-\npetitive coreference resolution system due to\nMartschat and Strube (2015). This task is analo-\ngous to the reranking approach used in machine\ntranslation (\nShen et al. , 2004). The speciﬁc for-\nmulation is as follows:\narg max\n{r(i),e(i),l(i)}∈K\nP (r(i), e(i), l(i), x) (12)\nwhere K is the k-best list for a given document.\nIn our experiments, k = 100 . T o the best of our\nknowledge, the problem of obtaining k-best out-\nputs of a coreference resolution system has not\nbeen studied before.\nApproximate k-best decoding. W e rerank the\noutput of a system that predicts an antecedent for\neach mention by relying on pairwise scores for\nmention pairs. This is the dominant approach\nfor coreference resolution (\nMartschat and Strube ,\n2015; Clark and Manning , 2016a). The predic-\ntions induce an antecedent tree, which represents\nantecedent decisions for all mentions in the doc-\nument. Coreference chains are obtained by tran-\nsitive closure over the antecedent decisions en-\ncoded in the tree. A mention also can have an\nempty mention as antecedent, which denotes that\nthe mention is non-anaphoric.\nFor extending Martschat and Strube’s greedy\ndecoding approach to k-best inference, we can-\nnot simply take the k highest scoring trees ac-\ncording to the sum of edge scores, because dif-\nferent trees may represent the same coreference\nchain. Instead, we use an heuristic that creates\nan approximate k-best list on candidate antecedent\ntrees. The idea is to generate trees from the orig-\ninal system output by considering suboptimal an-\ntecedent choices that lead to different coreference\nchains. For each mention pair (mj , mi), we com-\npute the difference of its score to the score of the\noptimal antecedent choice for mj. W e then sort\npairs in ascending order according to this differ-\nence and iterate through the list of pairs. For each\npair (mj, mi), we create a tree tj,i by replacing\nthe antecedent of mj in the original system output\nwith mi. If this yields a tree that encodes differ-\nent coreference chains from all chains encoded by\ntrees in the k-best list, we add ti,j to the k-best list.\nIn the case that we cannot generate a given num-\nber of trees (particularly for a short document with\na large k), we pad the list with the last item added\nto the list.\nEvaluation measures. For coreference\nresolution evaluation, we employ the\nCoNLL scorer (\nPradhan et al. , 2014). It\ncomputes three commonly used evalua-\ntion measures MUC (\nVilain et al. , 1995),\nB3 (Bagga and Baldwin , 1998), and CEAF e (Luo,\n2005). W e report the F1 score of each evaluation\nmeasure and their average as the CoNLL score.\nCompeting systems. W e employed C O RT\n1\n(Martschat and Strube , 2015) as our baseline\ncoreference resolution system. Here, we com-\npare with the original (one best) outputs of\nC O RT’s latent ranking model, which is the best-\nperforming model implemented in C O RT. W e\nconsider two rerankers based on E N TITY NLM.\nThe ﬁrst reranking method only uses the log\nprobability for E N TITY NLM to sort the candidate\nlist (\nEquation 12 ). The second method uses a\nlinear combination of both log probabilities from\nEN TITY NLM and the scores from C O RT, where\nthe coefﬁcients were found via grid search with\nthe CoNLL score on the development set.\nResults. The reranked results on the CoNLL\n2012 test set are reported in\nT able 2. The numbers\nof the baseline are higher than the results reported\nin\nMartschat and Strube (2015) since the feature\nset of C O RT was subsequently extended. Lines 2\nand 3 in T able 2 present the reranked best results.\nAs shown in this table, both reranked results give\nmore than 1% of CoNLL score improvement on\nthe test set over C O RT, which are signiﬁcant based\non an approximate randomization test\n2 .\nAdditional experiments also found that increas-\ning k from 100 to 500 had a minor effect. That is\nbecause the diversity of each k-best list is limited\nby (i) the number of entity mentions in the docu-\nment, (ii) the performance of the baseline corefer-\nence resolution system, and possibly (iii) the ap-\nproximate nature of our k-best inference proce-\ndure. W e suspect that a stronger baseline system\n(such as that of\nClark and Manning , 2016a) could\ngive greater improvements, if it can be adapted to\nprovide k-best lists. Future work might incorpo-\n1 https://github.com/smartschat/cort, we\nused version 0.2.4.5.\n2 https://github.com/smartschat/art\n[I]1 was about to ride [ my]1 [bicycle]2 to the\n[park]3 one day when [ I]1 noticed that the front\n[tire]4 was ﬂat . [ I]1 realized that [ I]1 would\nhave to repair [ it]4 . [ I]1 went into [ my]1\n[garage]5 to get some [ tools]5 . The ﬁrst thing\n[I]1 did was remove the xxxx\nFigure 3: A short story on bicycles from the\nInScript corpus (\nModi et al. , 2017). The entity\nprediction task requires predicting xxxx given\nthe preceding text either by choosing a previously\nmentioned entity or deciding that this is a “new en-\ntity”. In this example, the ground-truth prediction\nis [\ntire]4. For training, E N TITY NLM attempts to\npredict every entity . While, for testing, it predicts\na maximum of 30 entities after the ﬁrst three sen-\ntences, which is consistent with the experimental\nsetup suggested by\nModi et al. (2017).\nrate the techniques embedded in such systems into\nEN TITY NLM.\n5.3 Entity prediction\nT ask description. Based on\nModi et al. (2017),\nwe introduce a novel entity prediction task that\ntries to predict the next entity given the preced-\ning text. For a given text as in\nFigure 3 , this task\nmakes a forward prediction based on only the left\ncontext. This is different from coreference reso-\nlution, where both left and right contexts from a\ngiven entity mention are used in decoding. It is\nalso different from language modeling, since this\ntask only requires predicting entities. Since E N-\nTITY NLM is generative, it can be directly applied\nto this task. T o predict entities in test data, Rt is\nalways given and E N TITY NLM only needs to pre-\ndict Et when Rt = 1 .\nBaselines and human prediction. W e intro-\nduce two baselines in this task: (i) the always-new\nbaseline that always predicts “new entity”; (ii) a\nlinear classiﬁcation model using shallow features\nfrom\nModi et al. (2017), including the recency of\nan entity’s last mention and the frequency . W e also\ncompare with the model proposed by\nModi et al.\n(2017). Their work assumes that the model has\nprior knowledge of all the participant types, which\nare speciﬁc to each scenario and ﬁne-grained, e.g.,\nrider in the bicycle narrative, and predicts partic-\nipant types for new entities. This assumption is\nunrealistic for pure generative models like ours.\nTherefore, we remove this assumption and adapt\nMUC B 3 CEAFe\nModel CoNLL P R F1 P R F1 P R F1\n1. Baseline: C O RT’s one best 62.93 77.15 68.67 72.66 66.00 54.92 59.95 60.07 52.76 56.18\n2. Rerank: E N T I T Y NLM 64.00 77.90 69.45 73.44 66.84 56.12 61.01 61.73 53.90 57.55\n3. Rerank: E N T I T Y NLM + C O RT 64.04 77.93 69.49 73.47 67.08 55.99 61.04 61.76 53.98 57.61\nT able 2: Coreference resolution scores on the CoNLL 2012 tes t set. C O RT is the best-performing model\nof Martschat and Strube (2015) with greedy decoding.\nAccuracy (%)\n1. Baseline: always-new 31.08\n2. Baseline: shallow features 45.34\n3.\nModi et al. (2017) 62.65\n4. E N T I T Y NLM 74.23\n5. Human prediction 77.35\nT able 3: Entity prediction accuracy on the test set\nof the InScript corpus.\ntheir prediction results to our formulation by map-\nping all the predicted entities that have not been\nmentioned to “new entity”. W e also compare to\nthe adapted human prediction used in the In-\nScript corpus. For each entity slot,\nModi et al.\n(2017) acquired 20 human predictions, and the\nmajority vote was selected. More details about\nhuman predictions are discussed in (\nModi et al. ,\n2017).\nResults. T able 3 shows the prediction accura-\ncies. E N TITY NLM (line 4) signiﬁcantly outper-\nforms both baselines (line 1 and 2) and prior work\n(line 3) ( p ≪ 0.01, paired t-test). The compari-\nson between line 4 and 5 shows our model is even\nclose to the human prediction performance.\n6 Related W ork\nRich-context language models. The originally\nproposed recurrent neural network language mod-\nels only capture information within sentences.\nT o extend the capacity of RNNLMs, various re-\nsearchers have incorporated information beyond\nsentence boundaries. Previous work focuses\non contextual information from previous sen-\ntences (\nJi et al. , 2016a) or discourse relations be-\ntween adjacent sentences ( Ji et al. , 2016b), show-\ning improvements to language modeling and re-\nlated tasks like coherence evaluation and discourse\nrelation prediction. In this work, E N TITY NLM\nadds explicit entity information to the language\nmodel, which is another way of adding a memory\nnetwork for language modeling. Unlike the work\nby\nTran et al. (2016), where memory blocks are\nused to store general contextual information for\nlanguage modeling, E N TITY NLM assigns each\nmemory block speciﬁcally to only one entity .\nEntity-related models. T wo recent approaches\nto modeling entities in text are closely related\nto our model. The ﬁrst is the “reference-aware”\nlanguage models proposed by\nY ang et al. (2016),\nwhere the referred entities are from either a pre-\ndeﬁned item list, an external database, or the con-\ntext from the same document.\nY ang et al. (2016)\npresent three models, one for each case. For mod-\neling a document with entities, they use corefer-\nence links to recover entity clusters, though they\nonly model entity mentions as containing a single\nword (an inappropriate assumption, in our view).\nTheir entity updating method takes the latest hid-\nden state (similar to ht when Rt = 1 in our model)\nas the new representation of the current entity; no\nlong-term history of the entity is maintained, just\nthe current local context. In addition, their lan-\nguage model evaluation assumes that entity infor-\nmation is provided at test time (Y ang, personal\ncommunication), which makes a direct compari-\nson with our model impossible. Our entity updat-\ning scheme is similar to the “dynamic memory”\nmethod used by\nHenaff et al. (2016). Our entity\nrepresentations are dynamically allocated and up-\ndated only when an entity appears up, while the\nEntNet from\nHenaff et al. (2016) does not model\nentities and their relationships explicitly . In their\nmodel, entity memory blocks are pre-allocated\nand updated simultaneously in each timestep. So\nthere is no dedicated memory block for every en-\ntity and no distinction between entity mentions\nand non-mention words. As a consequence, it is\nnot clear how to use their model for coreference\nreranking and entity prediction.\nCoreference resolution. The hierarchical struc-\nture of our entity generation model is inspired by\nHaghighi and Klein (2010). They implemented\nthis idea as a probabillistic graphical model with\nthe distance-dependent Chinese Restaurant Pro-\ncess (\nPitman, 1995) for entity assignment, while\nour model is built on a recurrent neural network\narchitecture. The reranking method considered in\nour coreference resolution evaluation could also\nbe extended with samples from additional corefer-\nence resolution systems, to produce more variety\n(\nNg, 2005). The beneﬁt of such a system comes,\nwe believe, from the explicit tracking of each en-\ntity throughout the text, providing entity-speciﬁc\nrepresentations. In previous work, such informa-\ntion has been added as features (\nLuo et al. , 2004;\nBj ¨ orkelund and Kuhn, 2014) or by computing\ndistributed entity representations ( Wiseman et al. ,\n2016; Clark and Manning , 2016b). Our approach\ncomplements these previous methods.\nEntity prediction. The entity prediction task\ndiscussed in §\n5.3 is based on work by Modi et al.\n(2017). The main difference is that we do\nnot assume that all entities belong to a previ-\nously known set of entity types speciﬁed for\neach narrative scenario. This task is also\nclosely related to the “narrative cloze” task of\nChambers and Jurafsky (2008) and the “story\ncloze test” of Mostafazadeh et al. (2016). Those\nstudies aim to understand relationships between\nevents, while our task focuses on predicting up-\ncoming entity mentions.\n7 Conclusion\nW e have presented a neural language model, E N-\nTITY NLM, that deﬁnes a distribution over texts\nand the mentioned entities. It provides vector rep-\nresentations for the entities and updates them dy-\nnamically in context. The dynamic representations\nare further used to help generate speciﬁc entity\nmentions and the following text. This model out-\nperforms strong baselines and prior work on three\ntasks: language modeling, coreference resolution,\nand entity prediction.\nAcknowledgments\nW e thank anonymous reviewers for the helpful\nfeedback on this work. W e also thank the members\nof Noah’s ARK and XLab at University of W ash-\nington for their valuable comments, particularly\nEunsol Choi for pointing out the InScript corpus.\nThis research was supported in part by a Univer-\nsity of W ashington Innovation A ward, Samsung\nGRO, NSF grant IIS-1524371, the DARP A CwC\nprogram through ARO (W911NF-15-1-0543), and\ngifts by Google and Facebook.\nReferences\nAmit Bagga and Breck Baldwin. 1998. Algorithms for\nscoring coreference chains. In LREC W orkshop on\nLinguistic Coreference.\nPaul Baltescu and Phil Blunsom. 2015. Pragmatic neu-\nral language modelling in machine translation. In\nNAACL.\nAnders Bj ¨ orkelund and Jonas Kuhn. 2014. Learn-\ning structured perceptrons for coreference resolution\nwith latent antecedents and non-local features. In\nACL.\nNathanael Chambers and Daniel Jurafsky. 2008. Un-\nsupervised Learning of Narrative Event Chains. In\nACL.\nKevin Clark and Christopher D. Manning. 2016a.\nDeep reinforcement learning for mention-ranking\ncoreference models. In EMNLP.\nKevin Clark and Christopher D. Manning. 2016b. Im-\nproving coreference resolution by learning entity-\nlevel distributed representations. In ACL.\nJohn Duchi, Elad Hazan, and Y oram Singer. 2011.\nAdaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine\nLearning Research, 12:2121–2159.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In EMNLP.\nXavier Glorot and Y oshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In AISTATS, pages 249–256.\nJoshua Goodman. 2001. Classes for fast maximum en-\ntropy training. In ICASSP.\nAria Haghighi and Dan Klein. 2010. Coreference res-\nolution in a modular, entity-centered model. In\nNAACL.\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H.\nClark, and Philipp Koehn. 2013. Scalable modiﬁed\nKneser-Ney language model estimation. In ACL.\nMikael Henaff, Jason W eston, Arthur Szlam, An-\ntoine Bordes, and Y ann LeCun. 2016. Track-\ning the world state with recurrent entity networks.\narXiv:1612.03969.\nSepp Hochreiter and J ¨ urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nY angfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,\nand Jacob Eisenstein. 2016a. Document context lan-\nguage models. In ICLR (workshop track).\nY angfeng Ji, Gholamreza Haffari, and Jacob Eisen-\nstein. 2016b. A latent variable recurrent neural\nnetwork for discourse-driven language models. In\nNAACL-HLT.\nDiederik Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization.\narXiv:1412.6980.\nXiaoqiang Luo. 2005. On coreference resolution per-\nformance metrics. In HLT -EMNLP.\nXiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda\nKambhatla, and Salim Roukos. 2004. A mention-\nsynchronous coreference resolution algorithm based\non the Bell tree. In ACL.\nSebastian Martschat and Michael Strube. 2015. La-\ntent structures for coreference resolution. T ransac-\ntions of the Association for Computational Linguis-\ntics, 3:405–418.\nT omas Mikolov, Martin Karaﬁ´ at, Lukas Burget, Jan\nCernock ` y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH.\nAshutosh Modi, Ivan Titov, V era Demberg, Asad Say-\need, and Manfred Pinkal. 2017. Modeling seman-\ntic expectation: Using script knowledge for referent\nprediction. T ransactions of the Association of Com-\nputational Linguistics, 5:31–44.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy V anderwende,\nPushmeet Kohli, and James Allen. 2016. A corpus\nand evaluation framework for deeper understanding\nof commonsense stories. In NAACL.\nGraham Neubig, Chris Dyer, Y oav Goldberg, Austin\nMatthews, W aleed Ammar, Antonios Anastasopou-\nlos, Miguel Ballesteros, David Chiang, Daniel\nClothiaux, Trevor Cohn, et al. 2017. Dynet: The\ndynamic neural network toolkit. arXiv:1701.03980.\nV incent Ng. 2005. Machine learning for coreference\nresolution: From local classiﬁcation to global rank-\ning. In ACL.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In EMNLP.\nJim Pitman. 1995. Exchangeable and partially ex-\nchangeable random partitions. Probability Theory\nand Related Fields, 102(2):145–158.\nSameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-\nuard Hovy, V incent Ng, and Michael Strube. 2014.\nScoring coreference partitions of predicted men-\ntions: A reference implementation. In ACL.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Y uchen Zhang. 2012. CoNLL-\n2012 shared task: Modeling multilingual unre-\nstricted coreference in OntoNotes. In EMNLP-\nCoNLL.\nLibin Shen, Anoop Sarkar, and Franz Josef Och. 2004.\nDiscriminative reranking for machine translation. In\nNAACL.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch, 15(1):1929–1958.\nKe Tran, Arianna Bisazza, and Christof Monz. 2016.\nRecurrent memory networks for language modeling.\nIn NAACL-HLT.\nMarc V ilain, John Burger, John Aberdeen, Dennis Con-\nnolly, and Lynette Hirschman. 1995. A model-\ntheoretic coreference scoring scheme. In MUC.\nSam Wiseman, Alexander M. Rush, and Stuart M.\nShieber. 2016. Learning global features for coref-\nerence resolution. In NAACL.\nZichao Y ang, Phil Blunsom, Chris Dyer, and W ang\nLing. 2016. Reference-aware language models.\narXiv:1611.01628.\nW ojciech Zaremba, Ilya Sutskever, and Oriol V inyals.\n2015. Recurrent neural network regularization.\nICLR.",
  "topic": "Coreference",
  "concepts": [
    {
      "name": "Coreference",
      "score": 0.9017249345779419
    },
    {
      "name": "Computer science",
      "score": 0.8713755011558533
    },
    {
      "name": "Language model",
      "score": 0.7108749151229858
    },
    {
      "name": "Generative grammar",
      "score": 0.6673636436462402
    },
    {
      "name": "Natural language processing",
      "score": 0.6294726133346558
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6157546043395996
    },
    {
      "name": "Generative model",
      "score": 0.5896826982498169
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5822011232376099
    },
    {
      "name": "Resolution (logic)",
      "score": 0.5740426778793335
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": []
}