{
  "title": "Large language models for generating medical examinations: systematic review",
  "url": "https://openalex.org/W4390700361",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5093698157",
      "name": "Yaara Artsi",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A2809724045",
      "name": "Vera Sorin",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2155772135",
      "name": "Eli Konen",
      "affiliations": [
        "Tel Aviv University",
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A1498187152",
      "name": "Benjamin S Glicksberg",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2419298921",
      "name": "Girish Nadkarni",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1945837108",
      "name": "Eyal Klang",
      "affiliations": [
        "Sheba Medical Center",
        "Icahn School of Medicine at Mount Sinai",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A5093698157",
      "name": "Yaara Artsi",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A2809724045",
      "name": "Vera Sorin",
      "affiliations": [
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2155772135",
      "name": "Eli Konen",
      "affiliations": [
        "Tel Aviv University",
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A1498187152",
      "name": "Benjamin S Glicksberg",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2419298921",
      "name": "Girish Nadkarni",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1945837108",
      "name": "Eyal Klang",
      "affiliations": [
        "Sheba Medical Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4283578429",
    "https://openalex.org/W4281387590",
    "https://openalex.org/W3005496249",
    "https://openalex.org/W2623135331",
    "https://openalex.org/W4308767044",
    "https://openalex.org/W3046877299",
    "https://openalex.org/W4225281507",
    "https://openalex.org/W4295094102",
    "https://openalex.org/W2751579800",
    "https://openalex.org/W4323035111",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W3109948322",
    "https://openalex.org/W4387500346",
    "https://openalex.org/W4322761615",
    "https://openalex.org/W4387232979",
    "https://openalex.org/W4377115988",
    "https://openalex.org/W4324387439",
    "https://openalex.org/W4386249608",
    "https://openalex.org/W4387703131",
    "https://openalex.org/W4382020836",
    "https://openalex.org/W4385972264",
    "https://openalex.org/W4365503720",
    "https://openalex.org/W4321499657",
    "https://openalex.org/W4385850946",
    "https://openalex.org/W4375860725",
    "https://openalex.org/W2148623326",
    "https://openalex.org/W3121109690",
    "https://openalex.org/W4385409936",
    "https://openalex.org/W2168932951",
    "https://openalex.org/W1986908450",
    "https://openalex.org/W4381599219",
    "https://openalex.org/W2144619666",
    "https://openalex.org/W4385694104",
    "https://openalex.org/W4376108530",
    "https://openalex.org/W3130272805",
    "https://openalex.org/W1997866278",
    "https://openalex.org/W4364363895",
    "https://openalex.org/W4385266429",
    "https://openalex.org/W4366660674",
    "https://openalex.org/W4308869750",
    "https://openalex.org/W3208428226",
    "https://openalex.org/W4308681433",
    "https://openalex.org/W4386565997",
    "https://openalex.org/W4379377098",
    "https://openalex.org/W3093176888",
    "https://openalex.org/W4306913606",
    "https://openalex.org/W4376108512"
  ],
  "abstract": "Abstract Purpose Writing multiple choice questions (MCQs) for the purpose of medical exams is challenging. It requires extensive medical knowledge, time and effort from medical educators. This systematic review focuses on the application of large language models (LLMs) in generating medical MCQs. Methods The authors searched for studies published up to November 2023. Search terms focused on LLMs generated MCQs for medical examinations. MEDLINE was used as a search database. Results Overall, eight studies published between April 2023 and October 2023 were included. Six studies used Chat-GPT 3.5, while two employed GPT 4. Five studies showed that LLMs can produce competent questions valid for medical exams. Three studies used LLMs to write medical questions but did not evaluate the validity of the questions. One study conducted a comparative analysis of different models. One other study compared LLM-generated questions with those written by humans. All studies presented faulty questions that were deemed inappropriate for medical exams. Some questions required additional modifications in order to qualify. Conclusions LLMs can be used to write MCQs for medical examinations. However, their limitations cannot be ignored. Further study in this field is essential and more conclusive evidence is needed. Until then, LLMs may serve as a supplementary tool for writing medical examinations.",
  "full_text": "1 \n \n \nLarge language models for generating medical examinations: \nsystematic review \nYaara Artsi, BMedSC1; Vera Sorin, MD2-4; Eli Konen, MD2-3; Benjamin S. Glicksberg, \nPhD5; Girish Nadkarni, MD5-6; Eyal Klang, MD2-6 \n1Azrieli Faculty of Medicine, Bar-Ilan University, Zefat, Israel. \n2Department of Diagnostic Imaging, Chaim Sheba Medical Center, Israel \n3Tel-Aviv University School of Medicine, Israel \n4DeepVision Lab, Chaim Sheba Medical Center, Israel \n5Division of Data-Driven and Digital Medicine (D3M), Icahn School of Medicine at Mount \nSinai, New York, New York, USA \n6The Charles Bronfman Institute of Personalized Medicine, Icahn School of Medicine at \nMount Sinai, New York, New York, USA. \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n2 \n \n \nAbstract \nPurpose \nWriting multiple choice questions (MCQs) for the purpose of medical exams is challenging. \nIt requires extensive medical knowledge, time and effort from medical educators. This \nsystematic review focuses on the application of large language models (LLMs) in generating \nmedical MCQs. \nMethods \nThe authors searched for studies published up to November 2023. Search terms focused on \nLLMs generated MCQs for medical examinations. MEDLINE was used as a search database. \nResults \nOverall, eight studies published between April 2023 and October 2023 were included. Six \nstudies used Chat-GPT 3.5, while two employed GPT 4. Five studies showed that LLMs can \nproduce competent questions valid for medical exams. Three studies used LLMs to write \nmedical questions but did not evaluate the validity of the questions. One study conducted a \ncomparative analysis of different models. One other study compared LLM-generated \nquestions with those written by humans.  \nAll studies presented faulty questions that were deemed inappropriate for medical exams. \nSome questions required additional modifications in order to qualify. \nConclusions \nLLMs can be used to write MCQs for medical examinations. However, their limitations \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n3 \n \ncannot be ignored. Further study in this field is essential and more conclusive evidence is \nneeded. Until then, LLMs may serve as a supplementary tool for writing medical \nexaminations. \nKeywords \nLarge language models, GPT, Multiple choice questions, Medical education, Artificial \nintelligence, Medical examination \n \nIntroduction \nThere is a global shortage of clinical practitioners and increasing demand for medical \nprofessionals. This need presents significant challenges in the healthcare system [1, 2, 3]. In \nresponse, the number of medical schools and students has been rising worldwide [4, 5], \nleading to an increase in the demand for written tests.\n \nCreating multiple choice questions (MCQs) requires medical knowledge, conceptual \nintegration, and avoiding potential pitfalls. For example, repeat items in examinations from \nyear to year, or inherent imperfections called item-writing flaws (IWFs). Although at first \nsight IWFs may appear trivial, they can affect the way students understand and answer \nquestions [6, 7, 8, 9]. Producing MCQs is also time consuming. Any application capable of \nautomating this process could be highly valuable [10, 11]. \nAmidst these challenges, advancements in natural language processing (NLP) are constantly \ndiscussed and evaluated [12]. In particular, the introduction of OpenAI's state-of-the-art large \nlanguage models (LLMs) such as GPT-3.5 and GPT-4 [13, 14]. These models offer potential \nsolutions to healthcare education, due to their human-like text understanding and generation, \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n4 \n \nwhich includes clinical knowledge [15]. This could be pivotal in automating the creation of \nmedically precise MCQs. However, automating MCQs creation introduces potential risks, as \nthe accuracy and quality of AI generated content is still in question [16, 17].\n \nWe aimed to review the literature on LLMs’ ability to generate medical questions. We \nevaluated their clinical accuracy and suitability for medical examinations. \n \nMethods \nLiterature search \nOn November 2nd 2023 we conducted a search identifying studies describing LLMs’ \napplications in generating medical questions. We searched PubMed/MEDLINE for papers \nwith the following keywords, using Boolean operators AND/ OR: large language models; \nGPT; ChatGPT; medical questions; medical education; USMLE; MCCQE1; board exam; \nmedical exam.   \nWe also checked the references list of selected publications for more relevant papers. \nSections as ‘Similar Articles’ below articles (e.g., PubMed) were also inspected for possible \nadditional articles.  \nEthical approval was not required, this is a systematic review of previously published \nresearch, and does not include any individual participant information.  \nOur study followed the Preferred Reporting Items for Systematic Reviews and Meta-\nAnalyses (PRISMA) guidelines. \nThe study is registered with PROSPERO (CRD42023481851) \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n5 \n \nInclusion and exclusion process \nPublications resulting from the search were initially assessed by one author (YA) for relevant \ntitles and abstracts. Next, full-text papers underwent an independent evaluation by two \nauthors (EK and VS).  \nWe included full length studies describing LLMs generating medical questions. We excluded \npapers dating before 2023, non-English papers and non-original studies. Any study in \nquestion was discussed among all authors until reaching a unanimous agreement. (Figure 1.) \nRisk of bias and applicability were evaluated using the tailored QUADAS-2 tool. (Figure 2.) \n \nResults \nStudy selection and characteristics \nThe initial literature search resulted in 838 articles. Eight studies met our inclusion criteria \n(Figure 1). Most studies were retrospective: 6/8 (75%). One study is cross-sectional and one \nstudy is prospective. Most studies used Chat-GPT (3.5 or 4) as an AI model of choice, other \nmodels evaluated included Microsoft's Bing and Google's Bard. The MCQs were produced \nwith varying parameters (Table 1). Overall 5/8 (62.5%) studies demonstrated valid MCQs. \n6/8 (75%) of the studies utilized the latest version Chat-GPT 4 (Figure 3.) \nDescriptive summary of results \nCheung et al. [18] were the first, and so far, the only study to compare LLM to humans in \nMCQs writing. Chat-GPT 3.5 plus generated the MCQs. The reference for the prompt were \ntwo standard undergraduate medical textbooks: Harrison’s Principles of Internal Medicine the \n21th edition for medicine [19], and Bailey and Love’s Short Practice of Surgery 27th Edition \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n6 \n \nfor surgery [20]. Only four choices were given per question. Also, only text and knowledge-\nbased questions were generated. No modification to the MCQs was allowed after generation. \nChat-GPT 3.5 performed relatively well in the task. The overall time required for the AI to \ngenerate 50 MCQs was 21 minutes. This is about 10% of the total time human writing \nrequired (211 min). However, the questions written by humans were far better. Both in terms \nof quality and validity, outperforming the AI in a total score of 30 (60%) eligible MCQs \n(Table 2).\n \nKlang et al. [21] performed blind assessment of the generated questions. They did not \ndisclose to the evaluators whether the MCQs origin was AI. At first, they asked Chat-GPT 4 \nto create MCQs on the topic of internal medicine. They used as reference (few-shot learning) \na former exam of the same subject. The MCQs had four possible answers, with the correct \nanswer marked with an asterisk. At first, the generated MCQs were short with no clinical \nbackground. This required a second prompting of the AI model, specifically requesting the \nAI to create MCQs with clinical history. The study showed promising results, with the \nmajority of MCQs deemed valid as exam questions (Table 2). \nIn a cross-sectional study, Agarwal et.al [22] compared different LLMs. They compared \nChat-GPT 3.5/Bard/Bing in MCQs generating capability. They used as reference the 11-\nmodule curriculum for physiology, created by The Indian National Medical Commission \n(NMC). The authors requested in the prompt to Generate five difficult reasoning-based \nMCQs, fitting levels of Bachelor of Medicine, and Bachelor of Surgery (MBBS). Chat-GPT’s \ngenerated MCQs were significantly more valid than the other AI tools examined in the study. \nHowever, the difficulty level was lower compared to Bard and Bing (Table 2).\n \nAyub et al. [23] focused on medical board examination for Dermatology. They utilized Chat-\nPDF to upload entire PDF files into a ChatGPT 3.5 portal. The reference used was \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n7 \n \n“Continuing medical education” (CME) articles, taken from the Journal of the American \nAcademy of Dermatology (JAAD). This reference is considered high-yield review material \nfor the American Board of Dermatology Applied Exam (ABD-AE). This study's prompt was \nnot detailed in the paper. The three parameters to evaluate the MCQs were accuracy, \ncomplexity, and clarity. Only 16 (40%) of the generated questions were applicable (Table 2). \nThe rest were unclear 9 (22%), inaccurate 5 (13%) or had low complexity 10 (25%) (Table \n3). \nSevgi et al. [24] asked Chat-GPT 3.5 to prepare three questions with answers and \nexplanations at a level appropriate for a neurosurgery board exam. There was no independent \nevaluation of the MCQs. \nHan et al. [25] instructed Chat-GPT 3.5 to write three MCQs, each containing clinical \nbackground and lab values. Each time they requested Chat-GPT to rephrase the question. \nFirst, for a different correct answer and then for an increased level of difficulty. There was no \nindependent evaluation of the MCQs. \nTotlis et al. [26] asked Chat-GPT 4 to generate MCQs on the topic of anatomy. In the prompt \nthey requested increasing difficulty and matching correct pairs. There was no independent \nevaluation of the MCQs.  \nBiswas [27] requested in the prompt to prepare MCQs for USMLE step 1 exam. There was \nno independent evaluation of the MCQs. \nAll studies presented some faulty questions that were deemed inappropriate for medical \nexams. Some questions required additional modifications in order to qualify (Table 3). We \nincluded examples from each study, demonstrating valid MCQs as well as faulty MCQs for \nvarious reasons (Table 4.)  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n8 \n \n \nDiscussion \nIn this study we explored LLMs’ applicability in generating medical questions. Specifically, \nmultiple choice questions (MCQs) for medical examinations.  \nMCQs are an essential component of medical exams, used in almost every aspect of medical \neducation [8, 9]. Yet, they are time consuming and expensive to create [28]. The possibility \nof AI generated questions can provide an important opportunity for the medical community \nand transform the way written tests are generated. Using LLMs to support these tasks can \npotentially save time, money and reduce burnout. Especially in a system already sustaining \nitself on limited resources [29]. \nAI benefits \nPhysician burn-out, poor mental health and growing personal distress are constantly studied \n[30]. However, academic physicians experience a unique set of additional challenges, such as \nincreased administrative work, less time with patients and increased clinical responsibilities. \nAs a result, they have less time for traditional academic pursuits such as research and \neducation [31, 32, 33]. In the famous words of Albert Einstein: “Bureaucracy is the death of \nany achievement”.  \nAI can potentially relieve medical educators from tiresome bureaucracy and administrative \nwork, allowing them to focus on the areas that they view as most personally meaningful and \navoid career dissatisfaction [32, 34].  \nAccording to Bond et al. another possible application of AI in medical education is grading \npatients notes. This can provide additional formative feed-back for students in the face of \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n9 \n \nlimited faculty availability [35]. Moreover, AI can assist medical students by creating \npersonalized learning experience, while accessing current up-to-date information [36]. These \nare only a few examples, as every day new tasks improved by AI are discovered. \nAI drawbacks \nNowadays, AI continues to evolve, becoming more integrated in various medical fields [37]. \nThe probability of revolutionizing the healthcare world seems inevitable. AI performance is \nfast, efficient and with what seems like endless data resources [38]. In almost every study we \nreviewed, LLMs’ execution was more than satisfactory with the consensus that AI is capable \nof producing valid questions for medical exams. However, while these models show promise \nas an educational tool, their limitations must be acknowledged. \nOne notable limitation is a phenomenon known as “Hallucination” [39]. This occurs when \ngenerative AI misinterprets the given prompt, resulting in outputs that lack logical \nconsistency. When relying on AI for quality MCQs writing, this phenomenon is concerning. \nFurthermore, AI ability to integrate contextual and sensory information is still not fully \ndeveloped. Currently, AI cannot understand non-verbal cues or body language. Also, bias in \ndata and inaccuracy is troubling [40, 41].  \nAnother consideration is the logistics necessary to implement AI in healthcare and education. \nNew technologies require training, commitment and investment in order to be maintained and \nmanaged in a sustainable way. Such a process can take time and energy [42]. Moreover, \nimplementing new technology increases concern for privacy and data security [43, 44]. \nPatients' data is sensitive and often a target for cyber-attacks [45]. \nEqually important limitation of AI integration in healthcare is accountability. AI “black box” \nrefers to the “knowledge within the machine”. The internal workings of the system are \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n10 \n \ninvisible to the user. Healthcare staff use the AI, write the input and receive the output. But, \nthe system's code or logic cannot be questioned or explained [46].  \nAdditional aspect to consider is the longstanding concern of AI replacing human jobs [47]. \nThis could cause a dislike and resistance to AI integration. This notion is unlikely in the near \nfuture. But, distrust in AI technology is yet another challenge to its implementation [48].  \nBut maybe the biggest concern of AI application in medical education is impairing students' \ncritical thinking. According to Van de Ridder et al., self-reflection and criticism are crucial \nfor a medical student's learning process and professional growth. In a reality where a student \ncan delegate to Chat-GPT tasks such as writing personal reflection or learning experiences, \nthe students deny themselves of the opportunity to self-reflect and grow as physicians [49].  \nIt is imperative to take into consideration those significant shortcomings and challenges. AI \nshould be used wisely and responsibly while integrating it into medical education. \n \nMCQs creation \nFor each study we examined the process of crafting the MCQs. We noticed a wide range of \napproaches to writing the prompts. In some studies, additional modifications took place in \norder to improve the validity of the questions. This emphasizes the importance and sensitivity \nof prompts. Prompt-engineering may be a task that requires specific training, so that the \nprompt is phrased correctly and the MCQs quality is not impaired.   \nLimitations \nOur review has several limitations. Most of the studies are retrospective in nature. Due to \nheterogeneity in study design and data, a meta-analysis was not performed. None of the \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n11 \n \nquestions were image or graph based, which is an integral part of medical exams. Three \nstudies did not base their prompt on a valid medical reference, such as previous exams or \napproved syllabus. Also, the three studies did not evaluate the questions after they were \ngenerated. Two studies were at high risk of bias.    \nAdditional studies will be needed to further solidify the usefulness of AI tools, especially in \ngenerating competent medical exam questions. \nLastly, we limited our search to PubMed/MEDLINE. We did so due to its relevance in \nbiomedical research. We recognize this choice narrows our review's scope. This might \nexclude studies from other databases, possibly limiting diverse insights. \nConclusion \nAI-generated MCQs for medical exams are feasible. The process is fast and efficient, \ndemonstrating great promise in the future of medical education and exam preparation. \nHowever, their use warrants cautious and critical evaluation. Awareness of AI limitations is \nimperative in order to avoid misuse and deterioration of medical education quality. Currently, \nfurther research in this field is needed. Until more advancements are achieved, AI should be \nviewed as a powerful tool best utilized by experienced professionals. \nDisclosure statement \nDisclosure of interest \nThe authors report there are no competing interests to declare \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n12 \n \n \nAdditional information \nFunding \nThe author(s) reported there is no funding associated with the work featured in this article. \nAcknowledgments \nNone \nDisclaimer \nNone \n \n \n \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n13 \n \n \n \n \nReferences\n \n \n1. Boniol M, Kunjumen T, Nair TS, Siyam A, Campbell J, Diallo K. The global health \nworkforce stock and distribution in 2020 and 2030: a threat to equity and 'universal' \nhealth coverage? BMJ Glob Health. 2022 Jun;7(6):e009316. doi: 10.1136/bmjgh-\n2022-009316. PMID: 35760437; PMCID: PMC9237893. \n \n2. GBD 2019 Human Resources for Health Collaborators. Measuring the availability of \nhuman resources for health and its relationship to universal health coverage for 204 \ncountries and territories from 1990 to 2019: a systematic analysis for the Global \nBurden of Disease Study 2019. Lancet. 2022;399(10341):2129-2154. \ndoi:10.1016/S0140-6736(22)00532-3 \n \n3. Zhang X, Lin D, Pforsich H, Lin VW. Physician workforce in the United States of \nAmerica: forecasting nationwide shortages. Hum Resour Health. 2020;18(1):8. \nPublished 2020 Feb 6. doi:10.1186/s12960-020-0448-3 \n \n4. Rigby PG, Gururaja RP. World medical schools: The sum also rises. JRSM Open. \n2017;8(6):2054270417698631. Published 2017 Jun 5. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n14 \n \ndoi:10.1177/2054270417698631 \n \n5. Hashem F, Marchand C, Peckham S, Peckham A. What are the impacts of setting up \nnew medical schools? A narrative review. BMC Medical Education. 2022;22(1). \ndoi:10.1186/s12909-022-03835-4 \n \n6. Przymuszała, P., Piotrowska, K., Lipski, D., Marciniak, R., & Cerbin-Koczorowska, \nM. (2020). Guidelines on Writing Multiple Choice Questions: A Well-Received and \nEffective Faculty Development Intervention. SAGE Open, 10(3). \nhttps://doi.org/10.1177/2158244020947432 \n \n7. Pham H, Court-Kowalski S, Chan H, Devitt P. Writing Multiple Choice Questions-\nHas the Student Become the Master?. Teach Learn Med. 2023;35(3):356-367. \ndoi:10.1080/10401334.2022.2050240 \n \n8. Balaha MH, El-Ibiary MT, El-Dorf AA, El-Shewaikh SL, Balaha HM. Construction \nand Writing Flaws of the Multiple-Choice Questions in the Published Test Banks of \nObstetrics and Gynecology: Adoption, Caution, or Mitigation?. Avicenna J Med. \n2022;12(3):138-147. Published 2022 Aug 31. doi:10.1055/s-0042-1755332 \n \n9. Coughlin PA, Featherstone CR. How to Write a High Quality Multiple Choice \nQuestion (MCQ): A Guide for Clinicians. Eur J Vasc Endovasc Surg. \n2017;54(5):654-658. doi:10.1016/j.ejvs.2017.07.012 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n15 \n \n \n10. Homolak J. Opportunities and risks of ChatGPT in medicine, science, and academic \npublishing: a modern Promethean dilemma. Croat Med J. 2023;64(1):1-3. \ndoi:10.3325/cmj.2023.64.1 \n \n11. Gilardi F, Alizadeh M, Kubli M. ChatGPT outperforms crowd workers for text-\nannotation tasks. Proc Natl Acad Sci U S A. 2023;120(30):e2305016120. \ndoi:10.1073/pnas.2305016120 \n \n12. Sorin V, Barash Y, Konen E, Klang E. Deep-learning natural language processing for \noncological applications. Lancet Oncol. 2020;21(12):1553-1556. doi:10.1016/S1470-\n2045(20)30615-X \n \n \n13. Clusmann J, Kolbinger FR, Muti HS, et al. The future landscape of large language \nmodels in medicine. Commun Med (Lond). 2023;3(1):141. Published 2023 Oct 10. \ndoi:10.1038/s43856-023-00370-1 \n \n14. Eysenbach G. The Role of ChatGPT, Generative Language Models, and Artificial \nIntelligence in Medical Education: A Conversation With ChatGPT and a Call for \nPapers. JMIR Med Educ. 2023;9:e46885. Published 2023 Mar 6. doi:10.2196/46885 \n \n15. Brin D, Sorin V, Vaid A, et al. Comparing ChatGPT and GPT-4 performance in \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n16 \n \nUSMLE soft skill assessments. Sci Rep. 2023;13(1):16492. Published 2023 Oct 1. \ndoi:10.1038/s41598-023-43436-9 \n \n16. Bhattacharyya M, Miller VM, Bhattacharyya D, Miller LE. High Rates of Fabricated \nand Inaccurate References in ChatGPT-Generated Medical Content. Cureus. \n2023;15(5):e39238. Published 2023 May 19. doi:10.7759/cureus.39238 \n \n17. Vaishya R, Misra A, Vaish A. ChatGPT: Is this version good for healthcare and \nresearch?. Diabetes Metab Syndr. 2023;17(4):102744. doi:10.1016/j.dsx.2023.102744 \n \n18. Cheung BHH, Lau GKK, Wong GTC, et al. ChatGPT versus human in generating \nmedical graduate exam multiple choice questions-A multinational prospective study \n(Hong Kong S.A.R., Singapore, Ireland, and the United Kingdom). PLoS One. \n2023;18(8):e0290691. Published 2023 Aug 29. doi:10.1371/journal.pone.0290691 \n \n19. Harrison’s Principles of Internal Medicine, 21E | AccessMedicine | McGraw Hill \nMedical. https://accessmedicine.mhmedical.com/book.aspx?bookid=3095. \n \n20. Williams NS, O’Connell PR, McCaskie AW. Bailey & Love’s Short Practice of \nSurgery: Taylor & Francis Group; 2018. \n \n21. E K, S P, R G, et al. Advantages and pitfalls in utilizing artificial intelligence for \ncrafting medical examinations: a medical education pilot study with GPT-4. BMC \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n17 \n \nMed Educ. 2023;23(1):772. Published 2023 Oct 17. doi:10.1186/s12909-023-04752-\nw \n \n22. Agarwal M, Sharma P, Goswami A. Analysing the Applicability of ChatGPT, Bard, \nand Bing to Generate Reasoning-Based Multiple-Choice Questions in Medical \nPhysiology. Cureus. 2023;15(6):e40977. Published 2023 Jun 26. \ndoi:10.7759/cureus.40977 \n \n23. Ayub I, Hamann D, Hamann CR, Davis MJ. Exploring the Potential and Limitations \nof Chat Generative Pre-trained Transformer (ChatGPT) in Generating Board-Style \nDermatology Questions: A Qualitative Analysis. Cureus. 2023;15(8):e43717. \nPublished 2023 Aug 18. doi:10.7759/cureus.43717 \n \n24. Sevgi UT, Erol G, Doğ ruel Y, Sönmez OF, Tubbs RS, Güngör A. The role of an open \nartificial intelligence platform in modern neurosurgical education: a preliminary \nstudy. Neurosurgical Review. 2023;46(1). doi:10.1007/s10143-023-01998-2 \n \n25. Han Z, Battaglia F, Udaiyar A, Fooks A, Terlecky SR. An Explorative Assessment of \nChatGPT as an Aid in Medical Education: Use it with Caution. medRxiv (Cold Spring \nHarbor Laboratory). February 2023. doi:10.1101/2023.02.13.23285879 \n \n26. Totlis T, Natsis K, Filos D, et al. The potential role of ChatGPT and artificial \nintelligence in anatomy education: a conversation with ChatGPT. Surgical and \nRadiologic Anatomy. 2023;45(10):1321-1329. doi:10.1007/s00276-023-03229-1 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n18 \n \n \n27. Biswas S. Passing is Great: Can ChatGPT Conduct USMLE Exams?. Ann Biomed \nEng. 2023;51(9):1885-1886. doi:10.1007/s10439-023-03224-y \n \n28. Gierl MJ, Lai H, Turner SR. Using automatic item generation to create multiple-\nchoice test items. Med Educ. 2012;46(8):757-765. doi:10.1111/j.1365-\n2923.2012.04289.x \n \n29. Alhalaseh Y, Elshabrawy HA, Erashdi M, Shahait M, Abu-Humdan AM, \nAl/i2 Hussaini M. Allocation of the “Already” limited medical resources amid the \nCOVID-19 pandemic, an iterative ethical encounter including suggested solutions \nfrom a real life encounter. Frontiers in Medicine. 2021;7. \ndoi:10.3389/fmed.2020.616277 \n \n \n30. Khan, Rabia PhD, MSc1; Hodges, Brian David MD, PhD2; Martimianakis, Maria \nAthina PhD, MA3. Constructing “Burnout”: A Critical Discourse Analysis of Burnout \nin Postgraduate Medical Education. Academic Medicine 98(11S):p S116-S122, \nNovember 2023. | DOI: 10.1097/ACM.0000000000005358 \n \n31. Shanafelt TD, West CP, Sloan JA, et al. Career fit and burnout among academic \nfaculty. Arch Intern Med. 2009;169(10):990-995. doi:10.1001/archinternmed.2009.70 \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n19 \n \n32. Woolhandler S, Himmelstein DU. Administrative work consumes one-sixth of U.S. \nphysicians' working hours and lowers their career satisfaction. Int J Health Serv. \n2014;44(4):635-642. doi:10.2190/HS.44.4.a \n \n33. Szulewski, Adam MD, MHPE, PhD1; Braund, Heather PhD2; Dagnone, Damon J. \nMD, MSc, MMEd3; McEwen, Laura PhD4; Dalgarno, Nancy PhD5; Schultz, Karen \nW. MD6; Hall, Andrew K. MD, MMEd7. The Assessment Burden in Competency-\nBased Medical Education: How Programs Are Adapting. Academic Medicine \n98(11):p 1261-1267, November 2023. | DOI: 10.1097/ACM.0000000000005305 \n \n34. Lowenstein SR, Fernandez G, Crane LA. Medical school faculty discontent: \nprevalence and predictors of intent to leave academic careers. BMC Med Educ. \n2007;7:37. Published 2007 Oct 14. doi:10.1186/1472-6920-7-37 \n \n35. Bond, William F. MD, MS1; Zhou, Jianing MS2; Bhat, Suma PhD3; Park, Yoon Soo \nPhD4; Ebert-Allen, Rebecca A.5; Ruger, Rebecca L.6; Yudkowsky, Rachel MD, \nMHPE7. Automated Patient Note Grading: Examining Scoring Reliability and \nFeasibility. Academic Medicine 98(11S):p S90-S97, November 2023. | DOI: \n10.1097/ACM.0000000000005357 \n \n36. Feng, Songwei1; Shen, Yang MD, PhD2. ChatGPT and the Future of Medical \nEducation. Academic Medicine 98(8):p 867-868, August 2023. | DOI: \n10.1097/ACM.0000000000005242 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n20 \n \n \n37. Maassen O, Fritsch S, Palm J, et al. Future Medical Artificial Intelligence Application \nRequirements and Expectations of Physicians in German University Hospitals: Web-\nBased Survey. J Med Internet Res. 2021;23(3):e26646. Published 2021 Mar 5. \ndoi:10.2196/26646 \n \n38. Ramesh AN, Kambhampati C, Monson JR, Drew PJ. Artificial intelligence in \nmedicine. Ann R Coll Surg Engl. 2004;86(5):334-338. doi:10.1308/147870804290 \n \n39. Athaluri SA, Manthena SV, Kesapragada VSRKM, Yarlagadda V, Dave T, \nDuddumpudi RTS. Exploring the Boundaries of Reality: Investigating the \nPhenomenon of Artificial Intelligence Hallucination in Scientific Writing Through \nChatGPT References. Cureus. 2023;15(4):e37432. Published 2023 Apr 11. \ndoi:10.7759/cureus.37432 \n \n40. Safranek CW, Sidamon-Eristoff AE, Gilson A, Chartash D. The Role of Large \nLanguage Models in Medical Education: Applications and Implications. JMIR Med \nEduc. 2023;9:e50945. Published 2023 Aug 14. doi:10.2196/50945 \n \n41. Vorisek CN, Stellmach C, Mayer PJ, et al. Artificial Intelligence Bias in Health Care: \nWeb-Based Survey. J Med Internet Res. 2023;25:e41089. Published 2023 Jun 22. \ndoi:10.2196/41089 \n42. van Gemert-Pijnen JL. Implementation of health technology: Directions for research \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n21 \n \nand practice. Front Digit Health. 2022;4:1030194. Published 2022 Nov 10. \ndoi:10.3389/fdgth.2022.1030194 \n \n43. Raimundo R, Rosário A. The Impact of Artificial Intelligence on Data System \nSecurity: A Literature Review. Sensors (Basel). 2021;21(21):7029. Published 2021 \nOct 23. doi:10.3390/s21217029 \n \n44. Ignatovski M. Healthcare Breaches During COVID-19: The Effect of the Healthcare \nEntity Type on the Number of Impacted Individuals. Perspect Health Inf Manag. \n2022;19(4):1c. Published 2022 Oct 1. \n \n45. Sorin V, Soffer S, Glicksberg BS, Barash Y, Konen E, Klang E. Adversarial attacks in \nradiology - A systematic review. Eur J Radiol. 2023;167:111085. \ndoi:10.1016/j.ejrad.2023.111085 \n \n46. Chan B. Black-box assisted medical decisions: AI power vs. ethical physician care. \nMed Health Care Philos. 2023;26(3):285-292. doi:10.1007/s11019-023-10153-z \n \n47. Shuaib A, Arian H, Shuaib A. The Increasing Role of Artificial Intelligence in Health \nCare: Will Robots Replace Doctors in the Future?. Int J Gen Med. 2020;13:891-896. \nPublished 2020 Oct 19. doi:10.2147/IJGM.S268093 \n \n48. Starke G, Ienca M. Misplaced Trust and Distrust: How Not to Engage with Medical \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n22 \n \nArtificial Intelligence. Camb Q Healthc Ethics. Published online October 20, 2022. \ndoi:10.1017/S0963180122000445 \n \n49. van de Ridder, J.M. Monica PhD, MSc1; Shoja, Mohammadali M. MD2; Rajput, \nVijay MD, MACP3. Finding the Place of ChatGPT in Medical Education. Academic \nMedicine 98(8):p 867, August 2023. | DOI: 10.1097/ACM.0000000000005254 \n \n \nLegends: \nFigures \nFigure 1: Flow Diagram of the Inclusion Process. Flow diagram of the search and inclusion \nprocess based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses \n(PRISMA) guidelines, November 2023. \nFigure 2: QUADAS-2 table for potential bias and applicability. Risk of bias and applicability \nwere evaluated using the tailored QUADAS-2 tool, November 2023. \nFigure 3: Illustration of multiple-choice questions (MCQs) generation and summary of \npreliminary results. The images in the upper row illustrate the MCQs generation process via a \nlarge language model. The images in the bottom row showcase preliminary data results, \nNovember 2023. \nTables \nTable 1: Summary of the articles in the literature that applied AI for generating medical \nquestions, November 2023. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n23 \n \nTable 2: Summary of key parameters investigated in each study, November 2023. \nTable 3: Summary of faulty questions generated by the AI, November 2023 \nTable 4: Examples from studies of multiple-choice questions generated by AI. Both valid \nand faulty, November 2023. \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.06.24300920doi: medRxiv preprint ",
  "topic": "Medical literature",
  "concepts": [
    {
      "name": "Medical literature",
      "score": 0.5240516662597656
    },
    {
      "name": "Medical education",
      "score": 0.4933227598667145
    },
    {
      "name": "MEDLINE",
      "score": 0.47353169322013855
    },
    {
      "name": "English language",
      "score": 0.471597284078598
    },
    {
      "name": "Medicine",
      "score": 0.35889115929603577
    },
    {
      "name": "Psychology",
      "score": 0.31825998425483704
    },
    {
      "name": "Pathology",
      "score": 0.2772550582885742
    },
    {
      "name": "Mathematics education",
      "score": 0.1503276228904724
    },
    {
      "name": "Political science",
      "score": 0.10506173968315125
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}