{
  "title": "Learning the histone codes with large genomic windows and three-dimensional chromatin interactions using transformer",
  "url": "https://openalex.org/W4308239184",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2109779860",
      "name": "Do-Hoon Lee",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2461103973",
      "name": "Jee-Won Yang",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2104335212",
      "name": "Sun Kim",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2109779860",
      "name": "Do-Hoon Lee",
      "affiliations": [
        "Seoul National University",
        "Institute of Molecular Biology"
      ]
    },
    {
      "id": "https://openalex.org/A2461103973",
      "name": "Jee-Won Yang",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2104335212",
      "name": "Sun Kim",
      "affiliations": [
        "Seoul National University of Science and Technology",
        "Seoul National University",
        "New Generation University College",
        "Institute of Refrigeration"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2143658931",
    "https://openalex.org/W2479945688",
    "https://openalex.org/W1019830208",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2016015848",
    "https://openalex.org/W2082912595",
    "https://openalex.org/W2093229652",
    "https://openalex.org/W2090037139",
    "https://openalex.org/W4221000785",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W6791955017",
    "https://openalex.org/W2955994579",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W6735377749",
    "https://openalex.org/W2963582649",
    "https://openalex.org/W3091722278",
    "https://openalex.org/W2076154138",
    "https://openalex.org/W2259938310",
    "https://openalex.org/W2008809596",
    "https://openalex.org/W2142205294",
    "https://openalex.org/W1986974894",
    "https://openalex.org/W1972013835",
    "https://openalex.org/W3124680305",
    "https://openalex.org/W1839071933",
    "https://openalex.org/W3193227997",
    "https://openalex.org/W2043923025",
    "https://openalex.org/W3044355919",
    "https://openalex.org/W2146679006",
    "https://openalex.org/W3007150393",
    "https://openalex.org/W2513632107",
    "https://openalex.org/W2077874719",
    "https://openalex.org/W2991283206",
    "https://openalex.org/W2945038365",
    "https://openalex.org/W3203588026",
    "https://openalex.org/W2070021921",
    "https://openalex.org/W3120280701",
    "https://openalex.org/W2557097554",
    "https://openalex.org/W2128505445",
    "https://openalex.org/W2102619694",
    "https://openalex.org/W3122109253",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2345356016",
    "https://openalex.org/W2103441770",
    "https://openalex.org/W6968902705",
    "https://openalex.org/W4394460036",
    "https://openalex.org/W4210644165",
    "https://openalex.org/W2739174192",
    "https://openalex.org/W2963242855"
  ],
  "abstract": null,
  "full_text": "Article https://doi.org/10.1038/s41467-022-34152-5\nLearning the histone codes with large geno-\nmic windows and three-dimensional chro-\nmatin interactions using transformer\nDohoon Lee 1,2, Jeewon Yang 3 &S u nK i m3,4,5,6\nThe quantitative characterization of the transcriptional control by histone\nmodiﬁcations has been challenged by many computational studies, but most\nof them only focus on narrow and linear genomic regions around promoters,\nleaving a room for improvement. We present Chromoformer, a transformer-\nbased, three-dimensional chromatin conformation-aware deep learning\narchitecture that achievesthe state-of-the-art performance in the quantitative\ndeciphering of the histone codes in gene regulation. The core essence of\nChromoformer architecture lies in the three variants of attention operation,\neach specialized to model individual hierarchy of transcriptional regulation\ninvolving from core promoters to distal elements in contact with promoters\nthrough three-dimensional chromatin interactions. In-depth interpretation of\nChromoformer reveals that it adaptively utilizes the long-range dependencies\nbetween histone modiﬁcations associated with transcription initiation and\nelongation. We also show that the quantitative kinetics of transcription fac-\ntories and Polycomb group bodiescan be captured by Chromoformer.\nTogether, our study highlights the great advantage of attention-based deep\nmodeling of complex interactions in epigenomes.\nThe control of gene expression is carried out by diverse groups of\nregulators, including transcription factors, coactivators, corepressors\nalong with genomic sequence elements. However, the basic premise\nbehind the interplay among these factors is the appropriate conﬁg-\nuration of the covalent modiﬁcations of histone tails, or histone\nmodiﬁcations (HMs), at the relevant genomic regions since they play a\npivotal role in the regulation of chromatin accessibility. Thus, it can be\nconceived that an amount of HMs and their combinations encode the\nregulatory potential of the nearby genomic regions.\nThis notion is referred to as the‘histone code hypothesis’\n1.T h e r e\nhave been a number of computational and quantitative approaches to\ncrack the regulatory code of gene expression encoded by HMs. Most of\nthem are predictive models that utilize the levels of HMs at promoters\nsurrounding transcription start sites (TSSs) to predict the expression\nlevel of the corresponding gene. Notably, recent studies have shown\nthe superior performance of deep learning models compared to the\nconventional machine learning models in this task\n2,3.\nTo date, deep learning has been making remarkable break-\nthroughs in diverseﬁelds of computational biology, ranging from the\ncharacterization of binding the speciﬁcity of DNA- and RNA- binding\nproteins4 to the longstanding problem of the protein structure pre-\ndiction based on its amino acid sequence5. These successes of deep\nlearning in biology could not be achieved without the invention of\nnovel model architectures and also their clever applications for com-\nplex biological problems. In that sense, the high complexity of histone\ncode indeed made it a great target for deep learning, as shown in the\nexisting approaches, but they still pose two major limitations that\nmotivate the development of a new approach.\nReceived: 21 March 2022\nAccepted: 17 October 2022\nCheck for updates\n1Bioinformatics Institute, Seoul National University, Seoul 08826, Republic of Korea.2BK21 FOUR Intelligence Computing, Seoul National University, Seoul\n08826, Republic of Korea.3Interdisciplinary Program in Artiﬁcial Intelligence, Seoul National University, Seoul 08826, Republic of Korea.4Interdisciplinary\nProgram in Bioinformatics, Seoul National University, Seoul 08826, Republic of Korea.5Department of Computer Science and Engineering, Seoul National\nUniversity, Seoul 08826, Republic of Korea.6AIGENDRUG Co., Ltd., Seoul 08826, Republic of Korea.e-mail: sunkim.bioinfo@snu.ac.kr\nNature Communications|         (2022) 13:6678 1\n1234567890():,;\n1234567890():,;\nOne is that they could only use narrow genomic windows around\nTSSs. This is because the deep learning architectures that those\nmodels were based on, such as convolutional neural networks (CNNs)\nand recurrent neural networks (RNNs), were not effective in modeling\nthe dependencies within long sequences. CNNs are highly specialized\nfor learning local patterns of data,b u ti ti sc h a l l e n g i n gf o rt h e mt o\nlearn the distant dependencies between the patterns. Although\ndeveloped to model sequential data, RNN architectures also have dif-\nﬁculties in capturing the long-range dependencies clearly since the\ninformation embedded in a single position becomes gradually diluted\nand gets contaminated while the model computation travels along the\npositions between the two distant positions. Indeed, advanced forms\nof RNN cells such as Gated Recurrent Units\n6 or Long Short-Term\nMemory (LSTM)7 partially ameliorate this problem, but the intrinsic\ninefﬁciency in modeling long sequences due to the recurrence still\nremains.\nNext, a majority of the deep learning models do not account for\nthe distalcis-regulation mediated by three-dimensional (3D) chroma-\nt i nf o l d i n g ,e v e nt h o u g hi th a sb e e nw i d e l yk n o w nt h a tt h ep h y s i c a l\ninteractions between core promoters and distalcis-regulatory ele-\nments critically modulates the gene expression\n8,9. In other words, the\nregulatory information conveyed by the histone code is allowed to not\nonly propagate locally, but also jump between distant genomic loci\nthrough 3D chromatin interactions\n10. Fortunately, the recent\nadvancement of high-throughput measurement technologies such as\nHi-C\n11 succeeded in providing aﬁne-resolution view of 3D chromatin\ninteractions at kilobase-scale and offered us with unprecedented\nopportunities for exploiting such valuable information to model the\ncomprehensive view of gene regulation. There are few emerging stu-\ndies that explicitly take the 3D chromatin interactions into con-\nsideration to predict the gene expression. One such example is GC-\nMERGE\n12, a graph neural network (GNN) to propagate information\nbetween interacting genomic regions to predict the expression levels\nof genes. Although it is a proof-of-concept model that cannot be\napplied to genes without any chromatin interactions and only per-\nforms 10 kbp genomic bin-level predictions but not at gene-level, it still\nunderscores the promise of modeling epigenomic contexts of distal\ngenomic regions along with those of promoters.\nMeanwhile, a deep learning model architecture named transfor-\nmer, which was originally developed for natural language processing\n13,\nhas been exhibiting great potential for understanding the latent\ngrammar of DNA sequences\n14, amino acid sequences15, and even their\nalignments16. In particular, in this study, we noticed that the two main\nfunctionalities of the transformer architecture are highly suitable to\ntackle the two aforementioned challenges. First, transformers can\nprecisely model the long-range dependencies in sequential data. This\nis elegantly done by the additionof positional encodings to input\nsequences. These input features harboring positional information are\ntreated independently and fed into a subsequent self-attention module\nwhich calculates the all-pairwise dependencies between the input\nfeatures. Therefore, long-range dependencies can be captured without\nthe interference of features located between the pair. Secondly, the\ntransformer architecture can also be applied to model unordered sets\nof entities along with the interactions among them. Of note, this is not\nstraightforward for most of the deep learning architectures since the\noperations comprising them depend on input positions. On the other\nhand, the operations comprising the transformer are basically\npermutation-invariant. The interactions between input features are\nonly considered in self-attention operations, and all the other opera-\ntions are done in a position-wise manner, so they can be applied to a\nmodel an unordered set of features. Together, these two strengths of\nthe transformer architecture make it a promising choice for the\nquantitative modeling of histone codes by allowing us to utilize wider\ngenomic windows near TSSs and histone codes at multiple distal reg-\nulatory regions simultaneously.\nHere, we present a transformer-based deep learning architecture\nnamed Chromoformer to predict the gene expression levels based on\nthe HMs at the wide neighborhood of TSSs as well as the HMs placed at\nthe distal regulatory elements. Based on the model architecture con-\nsisting of three variants of self-attention operations to reﬂect the\nhierarchy of 3D gene regulation, Chromoformer achieves far better\npredictive power compared to the other deep learning models for\ngene expression prediction. Moreover, through the comprehensive\ninvestigation on how the use of transformer architecture contributed\nto the superior performance of the model, we demonstrate that the\nlong-range modeling of epigenetic context near TSS and simultaneous\nintegrative modeling of distal regulatory regions actually worked to\nimprove performances. Finally, we show that we could draw artiﬁcial\nintelligence-driven hypotheses for the quantitative effect ofcis-reg-\nulation by the two subdomains within nuclei, transcription factories,\nand silencing hubs, through the interpretation of the dynamics of\nlatent embeddings of the regulatory states learned by Chromoformer.\nResults\nChromoformer adopts three-level transformer architecture that\nreﬂects the hierarchy of 3D gene regulation\nThe core design principle of Chromoformer is twofold. One is to\nextract as much proximal regulatory information as possible from the\nHMs at the core promoters, and the other is to incorporate the distant\nhistone codes whose information is transmitted to the core promoter\nthrough 3D chromatin interactions. To fully utilize the transformer\narchitecture to model the complex dynamics ofcis-regulations invol-\nving multiple layers, we conceptually decomposed the gene regulation\ninto a three-layered hierarchy: (1)cis-regulation by core promoters, (2)\n3D pairwise interaction between a core promoter and a putativecis-\nregulatory regions (pCREs) and (3) a collective regulatory effect\nimposed by the set of 3D pairwise interactions. To computationally\nemulate this hierarchy, we introduced three transformer-based sub-\nmodules called Embedding, Pairwise Interaction, and Regulation\ntransformers that are specialized to learn the respective grammar of\ngene expression regulation in the order of increasing complexity.\nBefore illustrating the model architecture, we brieﬂyd e s c r i b et h e\ninput features used throughout this study. Chromoformer was trained\nusing read depth values from histone ChIP-seq experiments for seven\nmajor HMs (H3K4me1, H3K4me3, H3K9me3, H3K27me3, H3K36me3,\nH3K27ac, and H3K9ac) (Supplementary Fig. 1a). Read depths were\naveraged and log2-transformed forﬁxed-sized bins across 40 kbp\nregions ﬂanking TSSs (Fig.1a and Supplementary Fig. 1b). To account\nfor the distalcis-regulation, we additionally utilized the HM signals at\npCREs that are known to interact with the core promoter in the cor-\nresponding cell type (Supplementary Fig. 1c). For that, an experimen-\ntally validated set of pCREs for each core promoter was obtained using\na publicly available collection of promoter-capture Hi-C (pcHi-C)\ndata\n17. The 3D chromatin interactions were characterized at the reso-\nlution of HindIII restriction fragments. The interactions were char-\nacterized at adequately high-resolution, as the median and average\nlength of those fragments were 4797 bp and 5640 bp, respectively, and\nabout 95% of them were less than 10 kbp (Supplementary Fig. 2).\nThe full model architecture used in this study is illustrated in\nFig. 1b. At the highest level, it consists of three independent modules\neach of which accepts input features at different resolutions and in\nturn produces an embedding vector of the regulatory state at the core\npromoter. The resulting three regulatory embeddings are con-\ncatenated to form a multi-scale regulatory embedding which is sub-\nsequently fed into fully-connected layers to predict the expression\nlevel of the gene. The use of multi-scale regulatory embedding resulted\nin better performance than using any single-resolution regulatory\nembedding, and the combination of all three resolutions gave a\nrobustly higher performance increase than the combination of any\ntwo resolutions (Supplementary Fig. 3). Meanwhile, combining\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 2\nFig. 1 | Chromoformer model architecture. aInput features. To predict the\nexpression of a gene using levels of histone modiﬁcations (HMs), we extracted\nbinned average signals of HMs from both the core promoter and putativecis-\nregulatory regions (pCREs).b Chromoformer architecture. Three independent\nmodules were used to produce multi-scale representation of gene expression\nregulation. Each of the modules is fed with input HM features at different\nresolutions to produce an embedding vector reﬂecting the regulatory state of\nthe core promoter.c Embedding transformer architecture. Position-encoded\nHM signals of core promoter features are transformed into core promoter\nembeddings through self-attention.d Pairwise Interaction transformer archi-\ntecture. Position-encoded HM signals of pCREs are used to transform the core\npromoter embeddings into Pairwise Interaction embeddings through encoder-\ndecoder attention.e Regulation transformer architecture. Using the whole set of\nthe core promoter and Pairwise Interaction embeddings and gated self-atten-\ntion, the Regulation transformer learns how the pCREs collectively regulate the\ncore promoter. To guide the model to put greater attention to frequently\noccurring three-dimensional (3D) interactions, the normalized interaction fre-\nquency vector is added to self-attention afﬁnity matrices.\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 3\nregulatory embeddings at different resolutions using self-attention\noperation13,18– 20 did not seem to have signiﬁcant advantages over\nconcatenation, so we decided to concatenate regulatory embeddings\nfor the sake of the simplicity of the model (Supplementary Fig. 4). The\nEmbedding transformer (Fig.1c) learns the histone codes acting at the\ndirect vicinity of a TSS and produces aﬁxed-sized vector that sum-\nmarizes the epigenetic state of the region. This submodule alone works\nvery similar to the existing machine learning models for HM-based\ngene expression prediction, but we expected that the use of trans-\nformer architecture will allow the model to precisely identify relevant\nsignals within a wide-range view (up to 40 kbp) of core promoter HM\ncontexts without any performance degradation. Next, the resulting\ncore promoter embeddings are further updated by the Pairwise\nInteraction transformer (Fig.1d) in the context of pairwisecis-reg-\nulatory interactions between promoters and pCREs. Instead of using\ntypical self-attention layers as in the Embedding transformer, this\nmodule is built with encoder-decoder attention layers. Since the\nactivity of a promoter is modulated by the contact with pCREs, the\nencoder-decoder framework was chosen to reﬂect this by decoding\nthe promoter embeddings given the context of the pCRE features. We\ncall the resulting embedding vectors pairwise interaction embeddings\nas they carry the information of one-to-one relationship between\npromoters and pCREs. Finally, the Regulation transformer (Fig.1e)\naccepts a union set of the core promoter and pairwise interaction\nembeddings and ﬁnally produces a regulatory embedding by inte-\ngrating them. This module models the whole landscape ofcis-regula-\ntion using gated self-attention layers. The normalized interaction\nfrequencies (see Supplementary Method 1) are injected to self-\nattention score matrices to guide the model with the priorities of\ninteractions. Detailed explanation of the model is illustrated in Meth-\nods and Supplementary Note 1.\nChromoformer outperforms existing deep models for gene\nexpression prediction based on epigenetic features\nWe benchmarked the performance of Chromoformer with three\nbaseline deep-learning models using the optimal settings proposed in\nt h er e s p e c t i v es t u d i e s .W eﬁrst trained DeepChrome\n2, a convolutional\nneural network that learns the local combination of HMs through the\nweights of convolutionalﬁlters to predict gene expressions. We also\ntrained AttentiveChrome3 and DeepDiff21 models. The former com-\nbines LSTM and global attention mechanism to enhance the inter-\npretability of the model, and the latter extends it to predict the fold-\nchange of gene expression between a pair of cell types. Lastly, an HM-\nbased hybrid CNN-RNN model proposed by Kang et al.\n22 (HM-CRNN)\nwas also chosen for the comparison. It learns and captures the\nmeaningful local combination of HMs through CNN and comprehends\ntheir sequential dependencies through RNN.\nThe predictive performance of Chromoformer was comprehen-\nsively evaluated on three different gene expression prediction tasks:\nBinary gene expression state classiﬁcation, expression level regression,\nand expression fold-change regression. In the binary gene expression\nstate classiﬁcation task, model predicts whether an expression level of\na gene is above median or not. This problem formulation wasﬁrst\nproposed by Singh et al.\n2, and it has been so far widely adopted for\nmany studies, including the three aforementioned baseline studies. In\nthe expression level regression task, models were trained to predict\nlog2-transformed RPKM values, and the expression fold-change\nregression task evaluates the performance to predict the expression\nfold-change between the two cell types for each gene. For each task, we\ndesigned a tailored variant of Chromoformer models (Fig.2a), all of\nwhich are based on the multi-scale backbone of Chromoformer\n(Fig. 1b– e). Chromoformer-classiﬁer (Chromoformer-clf) was built for\nthe binary gene expression state classiﬁcation task. It has a classiﬁca-\ntion head with fully-connected layers that produces a two-dimensional\nprobability vector denoting the probability of high or low expression.\nChromoformer-regressor (Chromoformer-reg) was designed for the\nexpression level regression task using a regression head producing a\nsingle scalar. Chromoformer for fold-change regression (Chromo-\nformer-diff) adopts a Siamese neural network architecture to accept\nHM proﬁles from two different cell types (Supplementary Fig. 5a). The\ntwo Chromoformer backbone share their weights, so that the two HM\nproﬁles can be embedded in the same latent space and their differ-\nences can be nonlinearly translated into fold-change value by the\nsubsequent regression head.\nModel performances were evaluated for 11 cell types among the\n127 cell types proﬁled by Roadmap Epigenomics\n23 and ENCODE\nproject24. Those 11 cell types were chosen because all of the gene\nexpression proﬁles, ChIP-seq data for seven major HMs, and pcHi-C\ninteraction proﬁles were publicly available for each of the cell types. A\ntotal of 18,955 genes were split into four sets for 4-fold cross-validation\n(CV), each consisting of 5045, 4751, 4605, and 4554 genes. For every CV\nfold, each set became a held-out validation set, while the other three\nsets were used for model training. To avoid unwanted information\nleakage from the training to validation set through 3D chromatin\nfolding involving promoter-promoter interaction, we ensured that no\ntwo genes in different sets are located on the same chromosome.\nAs a result, our multi-scale Chromoformer model achieved sig-\nniﬁcant performance improvement over existing baseline deep learn-\ning models in all three tasks, suggesting that the proposed model\narchitecture was successful in modeling the regulatory hierarchy of\ngene expression (Fig.2b– f and Supplementary Figs. 5b, 6a, b and 7).\nThese results were consistently reproduced for all 11 cell types exam-\nined. In detail, for the binary gene expression state classiﬁcation task,\nChromoformer-clf achieved signiﬁcant performance improvement\nover existing baseline deep learning models in terms of area under\nreceiver operating characteristic curve (ROC-AUC) (Fig.2b), accuracy,\nand average precision (Supplementary Fig. 6a, b). Besides, we found\nthat the prediction probabilities produced by Chromoformer-clf\nshowed a very high positive correlation with the actual expression\nlevels (Supplementary Fig. 6c). These well-calibrated prediction\nprobabilities for the quantitative expression levels support the use of\nbinary classiﬁcation formulation for the quantitative modeling of HMs.\nChromoformer-clf also far outperformed GC-MERGE, a GNN using\nthree-dimensional chromatin interaction to predict gene expression\n(Fig. 2c). Importantly, GC-MERGE can only predict for genes involved\nin at least one chromatin interaction. Also, GC-MERGE can only predict\nthe gene expression in the unit of 10 kbp genomic bins, therefore it\ncannot produce gene-wise prediction when two or more genes are\npresent in the same bin. Therefore, Chromoformer was retrained from\nscratch for a subset of genes whose expression can be predicted by GC-\nMERGE for a fair comparison. Meanwhile, Chromoformer-reg out-\nperformed the regression variants of benchmark models in terms of\nPearson’s correlation coefﬁcient (Fig.2d) andR\n2 (Supplementary Fig.\n7). Chromoformer-diff also had signiﬁcantly better performance over\nthe state-of-the-art model DeepDiff (Fig.2e, f). Of note, it was also\nmuch better than the regression performance using the ratio of pre-\ndiction probabilities of classiﬁcation models as predicted fold-changes\n(Fig. 2e). Collectively, these results show the effectiveness of Chro-\nmoformer architecture in epigenetic gene regulation prediction.\nTraining with large window size andcis-regulatory interactions\ncontributed to the performance improvement in\nChromoformer\nTo dissect the performance of Chromoformer into the contributions of\nindividual factors, weﬁrst inspected for the effect of modeling wide-\nrange windows around the TSS up to 40 kbp. By gradually increasing\nthe window size around TSS from 2 kbp to 40 kbp, we observed a\nconsistent performance increase for our model, while other deep\nlearning models showed considerable performance degradation when\nthe window size was larger than 10 kbp (Fig.3a). Larger windows are\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 4\nFig. 2 | Chromoformer outperformed existing deep learning models in pre-\ndicting gene expression in various aspects. aThree variants of Chromoformer\nevaluated in this study. Chromoformer-classiﬁer (Chromoformer-clf) predicts\nbinary gene expression labels. Chromoformer-regressor (Chromoformer-reg)\npredicts the expression levels (in log2 RPKM) of genes. Chromoformer-diff predicts\nthe log2 fold-change of gene expression given the HM conﬁgurations of two dif-\nferent cell types for each gene.b Cross-validation (n = 4) performances of\nChromoformer-clf compared to the benchmark deep learning models that only\nutilize the core promoter features.c Comparison of cross-validation (n =4 )p e r -\nformances with GC-MERGE, a graph neural network model that utilizes 3Dcis-\nregulatory interactions. For fair comparisons, Chromoformer-clf models were\nretrained from scratch using only a subset of genes that GC-MERGE can predict.\nd Cross-validation (n = 4) performances of Chromoformer-reg compared to the\nbenchmark deep learning models. Prediction heads of benchmark models were\nmodiﬁed to produce single scalar values instead of binary labels.e Cross-validation\n(n=4) performances of Chromoformer-diff compared to the benchmark deep\nlearning models. Evaluation of the fold-change prediction based on the ratio of\nclassiﬁcation probabilities are shown as a reference (denoted as DeepChrome-clf,\nAttentiveChrome-clf, HM-CRNN-clf, and Chromoformer-clf).f Pairwise perfor-\nmance comparison between DeepDiff and Chromoformer-diff. Throughout\nb– e the center line denotes the median, the upper and lower box limits denote\nupper and lower quartiles, and the whiskers denote 1.5× interquartile range. AUC,\nArea under the receiver operating characteristic curve; CV, Cross-validation. Source\ndata are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 5\nmore likely to include TSSs of other genes, which jeopardizes the model\nperformance since the model will be more prone to the spurious\nattention towards those irrelevant TSSs. This is because the attention\nscore for each genomic position is computed against global context\nvectors without accounting for the absolute distance to the target TSS.\nIn the case of the transformer architecture, however, scaled-dot pro-\nduct attention with positional encoding allows the computation of\nattention scores among all pairs of positions without introducing a\nglobal context vector, thereby allowing us to query which position\nwithin the input window the speciﬁc TSS of our interest is attending on.\nThus, these results emphasize the strength of the transformer archi-\ntecture in pinpointing histone codesthat are relevant to the regulation\nof gene expression within a wide-range window around the TSS.\nWe next examined whether the incorporation of distalcis-reg-\nulatory interactions into modeling truly contributed to the perfor-\nmance improvement. To this end, we eliminated Pairwise Interaction\nand Regulation transformers from the Chromoformer model to see\nhow well it performs when trained without using distalcis-regulatory\ninteractions. We retrained the ablated Chromoformer models and\ncompared their performances to the intact Chromoformer model in\nterms of ROC-AUC (Fig.3b). This revealed that the Chromoformer in its\nintact form showed signiﬁcantly high performances in most of the\nanalyzed cell types (10 out of 11), implying that the inclusion of distal\npCREs and their interactions into deep modeling helped learning\nepigenetic factors that govern the expression of genes. To further\nsupport that this improvement is speciﬁcally due to the modeling of\nFig. 3 | Contributing factors to the superior performance of Chromoformer.\na Effect of input window size around TSS on the model performance. Chromo-\nformer and the other benchmark models were trained forﬁve different window\nsizes (2 kbp, 4 kbp, 10 kbp, 20 kbp, and 40 kbp), while all the other training pro-\ncedures were kept the same as previously. Bold lines denote the average validation\nAUC across 4-fold cross-validation for each window size, while the shades denote\nthe standard error of them.b Effect of taking distalcis-regulations by pCREs into\naccount. We trained ablated Chromoformer models which only have the Embed-\nding transformer and thus cannot incorporate thecis-regulatory information\nbetween the core promoter and pCREs. The resulting cross-validation (n =4 )\nperformances were compared with the intact Chromoformer model.c Comparison\nof the cross-validation (n = 4) performances for a subset of genes without or with\nknown chromatin interactions. ROC-AUC scores of Embedding transformer-only\nChromoformer and intact Chromoformer were computed only for a subset of\ngenes that do not have knowncis-regulatory interactions (Upper), and genes with at\nleast one knowncis-regulatory interactions.P-values from two-sided pairedt-tests\nare shown. In the boxplot, the center line denotes the median, the upper and lower\nbox limits denote upper and lower quartiles, and the whiskers denote 1.5× inter-\nquartile range. Source data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 6\ncis-regulatory interactions, but not due to the mere complexity of the\nmodel, we investigated the modeling performance separately for the\ngenes whose promoters do not interact with any pCREs. Since no\nbiologically meaningful information would be transferred to the core\npromoter embeddings of those genes through the Pairwise Interaction\nand Regulation transformers, we can discern whether the dominant\nfactor that contributed to the performance increase was the infor-\nmation transfer between genomic regions or the increased model\ncomplexity itself. As expected, we observed that Chromoformer\nshowed no signiﬁcant improvements for the majority of cell types\n(Fig. 3c). Besides, the performance increase was still signiﬁcant for the\ngenes having at least one interaction with pCREs (Fig.3c). Further-\nmore, the contribution of intra-TAD interactions to the performance\nwere shown to be greater than inter-TAD interactions, again support-\ning the biological relevance of distalcis-regulatory interactions in\nChromoformer training (Supplementary Fig. 8). Together, it indicates\nthat the contribution ofcis-regulatory modeling is far greater than that\nof increased modeling capacity arising from deeper layers.\nChromoformer learns to attend to the distant transcriptional\nelongation signals at gene bodies\nGiven the improved performance of the Chromoformer model com-\npared to the other deep learning models, as well as the success of the\nwide-range modeling of core promoter region, we then asked if there\nare any unique patterns of HMs captured by Embedding transformers.\nAs the self-attention layers inside the embedding transformers were\ndesigned to comprehend the dependencies of HMs at core promoters,\nwe postulated that any such dependencies that the Chromoformer\nm o d e li sa w a r eo fc a nb er e v e a l e dt h r o u g ht h es e l f - a t t e n t i o nm a pi t\nyields. Therefore, the attention weights produced by the Embedding\ntransformer of Chromoformer-clf during the prediction were visua-\nlized to analyze the internal behavior of the model. Figure4as h o w sa n\nexample snapshot of attention weights during the prediction of the\nexpression of anti-silencing function 1 A (ASF1A) histone chaperone in\nthe H1 cells (Epigenome identiﬁer E003). Interestingly, we observed\nthat a majority of attention heads were prominently giving strong\nattention to 4-6 kbp downstream the TSS than any other speciﬁc\npositions (Fig.4a, b). This was rather unexpected since most of the\nregulatory information delivered by HMs are known to be deposited\nnear the TSS, where the binding of transcription factors and tran-\nscriptional initiation mostly take place. In line with this notion, average\nsignals of HMs displayed their characteristic patterns mostly at the TSS\n(Fig. 4c). Speciﬁcally, methylations at H3K4 and histone acetylations\nwere associated with higher levels of gene expressions, whereas high\nlevels of H3K9me3 and H3K27me3 modiﬁcations were associated with\nlow expression of genes.\nIn striking contrast, the average H3K36me3 signal of the two\nclasses of genes (i.e., high/low expression) did not show any notable\ndifference at the TSS, but the difference was maximized at the 4– 6k b p\nregion downstream the TSS (Fig.4d). H3K36me3 is established by the\naddition of methyl groups at H3K36 by SETD2 histone methyl-\ntransferase, and SETD2 is known to be recruited to the C-terminal\ndomains of RNA polymerases in concert with the transcriptional\nelongation\n25. Thus, H3K36me3 is widely appreciated as a transcrip-\ntional elongation-associated HM that predominantly marks the bodies\nof actively-transcribed genes. The average attention weights imposed\nby the TSS-containing bin reached its maximum exactly at 4-6 kbp\nregion downstream the TSS (Fig.4e), suggesting that the model was\nwell optimized to focus on the most discriminative genomic region in\nterms of H3K36me3 signals. Intriguingly, the extent of the attention\ngiven at the 4– 6 kbp region downstream TSS was far greater for genes\nwith high expression than low expression (Fig.4e). In other words, the\nEmbedding transformer was trained to adaptively control the amount\nof attention given on the HMs at the gene body, based on the histone\ncontext at the direct vicinity of the TSS, as illustrated in Fig.4f. These\npatterns of self-attention weights were highly consistent across the cell\ntypes examined (Supplementary Fig. 9). One explanation for this\nbehavior of the Embedding transformer is that the model seeks for the\ncomplementary evidence that reinforces its conﬁdence for the initial\nguess on the gene expression, which is based on the HM states near the\nTSS. In this sense, H3K36me3 is a well-suited candidate for such a role\nsince its discriminative power resides where the other HMs do not\nshow large variabilities, hence being left as the only clear signal in\nthose regions. The importance of H3K36me3 at downstream gene\nbodies was further supported by the feature ablation experiment.\nWhen the H3K36me3 signals were excluded from model training and\nonly the other six HMs were used as input features, we observed a\nsigniﬁcant decrease in performance (Fig.4g). Moreover, we discovered\nthat the Embedding transformer predominantly lost speciﬁc attention\nto the 4– 6 kbp downstream region (Fig.4h).\nNotably, H3K36me3 ablation was the most detrimental in model\nperformance compared to the ablation of any other individual HMs\n(Supplementary Fig. 10a). This implies that the distribution of\nH3K36me3 may not be readily inferred by the other HMs, as shown by\nits low spatial correlation with them (Supplementary Fig. 10b). Fur-\nthermore, while ablating the combinations of other HMs correspond-\ning to the chromHMM-deﬁned 50 chromatin states generally resulted\nin poor performance, the effect of H3K36me3 almost seemed to be\nindependent (Supplementary Fig. 10c, d). We also noticed that abla-\ntion of the two enhancer marks, H3K4me1 and H3K27ac, was not solely\nsufﬁcient to signiﬁcantly degrade the performance of Chromoformer,\nwhile further ablating H3K4me3 and H3K9ac incurred drastic perfor-\nmance drop (Supplementary Fig. 10c). This implies the spatial corre-\nlation or the redundancy of active HMs (Supplementary Fig. 10b) were\neffectively compensating the absence of the regulatory information\nconveyed by other active HMs. In addition, it may be due the com-\npensation through promoter-promoter interactions between active\npromoters marked by H3K4me3 or H3K9ac that hint the existence of\nthe transcriptional hubs enriched with enhancers. Nevertheless, the\nablation of H3K4me1 and H3K27ac with additional HMs generally\nshowed high-performance degradation without H3K36me3 ablation,\nsuggesting the overall importance of HMs marking enhancers.\nIt seems that the importance of the feature representing tran-\nscriptional elongation was particularly important in this problem set-\nting because the model was trained to predict the steady-state levels of\nmRNA measured by RNA-seq. Steady-state levels of mRNA are deter-\nmined not only by the transcriptional initiation, but also by various\nfactors, including the rate of transcriptional elongation and mRNA\nstability. According to a study comparing different measurement\ntechnologies used for gene expression prediction tasks\n26, predicting\nthe expression levels measured by cap analysis of gene expression\n(CAGE) was shown to be easier than predicting RNA-seq-based\nexpression levels. This study also showed that H3K36me3 was pre-\ndictive for RNA-seq-based expression levels, while core promoter HMs\nincluding H3K4me3 were more useful for CAGE measurements. They\nimply that the hidden factors, including the efﬁcacy of transcriptional\nelongation, reside in RNA-seq measurements, and they cannot be\nreadily accounted for with core promoter features alone. Thus, we\nspeculate that the superior performance of Chromoformer model may\narise from the ability to model the rate of transcriptional elongation,\nwhich leaves its trace as H3K36me3 in the gene body. These results,\nbased on the great interpretability of the Embedding transformer,\ncollectively suggest that the Embedding transformer learned the dis-\ntant correlation between histone codes dictating active transcription\nnear TSS and high levels of H3K36me3 representing transcriptional\nelongation at gene body, especia l l ya tt h eg e n eb o d i e s4 - 6k b p\ndownstream the TSS. Moreover, this in part explains why the perfor-\nmance of Chromoformer showed consistent increase along the\nincrease of window size, as the model could collect additional evi-\ndence for gene expression also from the gene body.\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 7\nAnalyzingcis-regulome throughcis-regulatory impact predicted\nby Chromoformer\nWe then examined the effect of modelingcis-regulation by distal\npCREs in detail. As it has already been shown that the inclusion of\nPairwise Interaction and Regulation transformers results in better\noverall performance, we sought for a more detailed interpretation\nfor the gene-level effect of regulatory interaction modeling. To this\nend, we devised a gene-level measure to quantify the predicted\nimpact of the cis-regulation based on the latent representation\nlearned by Chromoformer. Speciﬁcally, we measured the Euclidean\ndistance between the interaction-aware regulatory embedding\nproduced by Pairwise Interaction and Regulation transformers, and\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 8\nthe original core promoter embedding resulting from Embedding\ntransformers (Fig.5a). For convenience, we termed this quantity as\n‘predicted cis-regulatory impact (PCRI)’.\nTo assure that PCRI truly reﬂects the dynamics of latent vector\nrepresentations as well as the effect of regulatory interactions, weﬁrst\nasked how it eventually affected the prediction outcome probabilities.\nThe fold-change in predicted gene expression between the interaction-\naware Chromoformer-reg and interaction-free Chromoformer-reg was\nmeasured for each gene to determine the amount of perturbation on\nthe prediction probability. As a result, fold-changes for the two groups\nof genes with above and below half PCRI were signiﬁcantly different\nwithin each of the low expression and high expression groups of genes\nin H1 cells (E003) (Fig.5b). Similarly, PCRI and fold-change for the\nhighly expressed genes were positively correlated (Spearman ’s\nr = 0.39,p <1 0\n−308) and PCRI and fold-change for the lowly expressed\ngenes were negatively correlated (Spearman’sr = −0.54, p <1 0−308)\n(Fig. 5c). In short, it can be summarized that high PCRI for highly-\nexpressed genes made those genes predicted with higher gene\nexpression, and high PCRI for lowly-expressed genes resulted in lower\npredicted expression.\nTo get deeper insights into how the Chromoformer model\ncould accurately discern pCREs associated with the activation or\nsuppression of gene expressions, we analyzed the characteristics\nof pCREs assigned for genes at the highest extreme in terms of\nPCRI, i.e., genes predicted to have greatest impact by distalcis-\nregulation. We collected 250 highly-expressed genes with the\nhighest PCRI values from each fold of the 4-fold CV and examined\nthe average signals of HMs near the pCREs associated with those\n1000 genes. As a result, pCREs for highly-expressed genes with\nhigh PCRI on average showed increased levels of HMs associated\nwith transcriptional activation compared to those for lowly-\nexpressed genes (Fig. 5d, shown for H1 cells). In particular, HMs\nrepresenting enhancers (H3K27ac, H3K9ac, and H3K4me1), active\npromoters (H3K4me3 and H3K9ac) and active gene bodies\n(H3K36me3) were enriched for those pCREs. This broad enrich-\nment of genomic elements associated with the greatest tran-\nscriptional activation implies that Chromoformer learned the\nexistence of transcription factories, on which the active genes\nand enhancers are gathered together for efﬁcient transcription\n27.\nBased on this observation, we sought for additional biological\nevidence by examining whether those genes clustered at putative\ntranscription factories show enrichment for particular biological\nfunctions. Interestingly, they were highly enriched for house-\nkeeping activities, including mRNA splicing, DNA replication,\nribosome biogenesis, and DNA damage response (Fig. 5e). We\nalso observed the enrichment of cell type-speciﬁc functions such\nas telomere maintenance in stem cells (E003 and E016) and\nimmortalized cell lines (E114, E116, and E118), cell morphogenesis\nand extracellular structure organization in mesenchymal stem\ncells (E006) or iron homeostasis in liver cells (E066) and hepa-\ntocellular carcinoma (HCC) (E118) (Supplementary Fig. 11). Taken\ntogether, it can be speculated that Chromoformer reﬂected the\ntendency of cells to ensure the robust expression of essential\ngenes for its function and survival through sequestering them\nwithin transcriptionally active subcompartments harboring mul-\ntiple enhancers\n28. On the other hand, pCREs for lowly-expressed\ngenes with high PCRI on average showed increased levels of\nrepressive marks such as H3K27me3 and H3K9me3 (Fig. 5f),\nimplying that Chromoformer also detected the transfer of the\nsuppressive regulatory information from the pCREs to the core\npromoter. We conjectured that those pCREs represent tran-\nscriptional silencers, since previous studies have shown the\npotential functionality of distal H3K27me3-rich regions in tran-\nscriptional repression\n29. The top 1000 genes which are predicted\nto have strong suppressive cis-regulation showed extreme\nenrichment towards developmental functions (Fig. 5g). One\nrepresentative example of the suppressive cis-regulatory inter-\nactions is shown in (Fig.5h) for Engrailed Homeobox 2 (EN2). As\nexpected, many of the pCREs showed high H3K27me3 signals.\nNotably, one of the pCREs was located 1.5 Mbp away fromEN2,\nand the pCRE spanned the core promoter of Motor Neuron and\nPancreas Homeobox 1 (MNX1), which is another homeobox tran-\nscription factor associated with development. The functional\nsimilarity between the two highly distant, but interacting genes\nimplies the existence of silencing hubs, where the developmental\ngenes and silencers are sequestered together through 3D chro-\nmatin folding.\nAssuming that the collection of the whole PCRI values can be\nregarded as a cis-regulome of each cell type, we further asked\nwhether we can detect differentialcis-regulation between cell types\nand unveil its underlying basis in terms of histone codes. We\nselected top 1000 genes having highest variances of normalized\nPCRI (Methods) across the 11 cell types used in this study (Supple-\nmentary Fig. 12) and performed hierarchical clustering with their\nPCRI values. As expected, similar cell types were clustered together\nand corresponding cell-type speciﬁc functions were highlighted by\nhigh PCRI values (Fig.6a). Notably, we observed that thecis-reg-\nulomes of healthy liver tissue and HepG2 HCC cells were tightly\nclustered, but also found a small subset of genes that were being\nsubjected to HepG2-speciﬁc cis-regulation (Fig. 6a, black box). We\ncould notﬁnd any biological terms signiﬁcantly enriched for these\ngenes unlike other clustered gene sets in the analysis, so it could be\nspeculated that they represent a consequence of cancer-speciﬁc\naberrant cis-regulation occurring in a stochastic manner. In support\nof this notion, we could identify four individual genes (GNA12,\nTRIB3, CCN2 and RBM39) tightly implicated in HCC\n30– 34, which can be\nthought as epigenetic hits by aberrant cis-regulation. The expres-\nsion of the four genes were 9.3-, 6.0-, 4.1-, and 3.7-fold higher in HCC\nthan in healthy liver cells, respectively, in accordance with the\ntendency of PCRIs (Supplementary Fig. 13). To interpret why the\nChromoformer predicted high PCRI values for those genes, we\nvisually inspected the histone modiﬁcation landscape surrounding\nthe genes. For example, Fig.6b shows histone modiﬁcation land-\nscapes aroundCCN2 in healthy liver and HCC cells. Comparing the\ntwo landscapes revealed a putative enhancer region that is only\nactive in HCC (Fig.6b, red arrow), which may explain higher PCRI as\nwell as higher expression ofCCN2 in HCC. It highlights that the in-\ndepth interpretation of Chromoformer model prediction in the\nFig. 4 | Analysis of self-attention weights learned by the Embedding transfor-\nmer. aRepresentative self-attention weight matrices for the prediction of the\nexpression of ASF1A in H1 cell. Each heatmap shows the attention weight for each\npair of genomic bins. The dotted lines indicate the 4– 6 kbp region downstream\nTSS.b Detailed description of the attention weights learned by the attention head 2\nfor 2 kbp-resolution Embedding transformer. Genome tracks representing the\nnormalized signals of the seven HMs are aligned with the attention weight matrix.\nThe dotted lines demarcate the regions 4– 6 kbp downstream the TSS. The red\narrow indicates the TSS, and the purple arrow indicates the exon located within the\nregion 4– 6 kbp downstream the TSS.c Average signals of the HMs other than\nH3K36me3. Signals were separately averaged according to their expression labels\n(High/Low expression).d Average H3K36me3 signal.e Average attention weights of\nthe second attention head for 100 bp resolution bins. The grey shade denotes the\n4– 6 kbp region downstream the TSS.f Schematic diagram illustrating the behavior\nof the Embedding transformer.g Cross-validation (n = 4) performances upon\nH3K36me3 feature ablation. The center line denotes the median, the upper and\nlower box limits denote upper and lower quartiles, and the whiskers denote 1.5×\ninterquartile range.h Attention weights for TSS bin. Red and grey lines indicate the\naverage attention weights for genes having above-median and below-median\nexpression, respectively. Source data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 9\nFig. 5 | Analysis of predictedcis-regulatory impact (PCRI). aSchematic diagram\nillustrating the computation of the PCRI.b, c Relationship between PCRI and the\nlog2 fold-change of predicted gene expression between the interaction-aware and\ninteraction-free Chromoformer-reg in H1 cells (Epigenome identiﬁer E003). Low\nand high PCRI groups were split by the median PCRI value. Inb, p-values from two-\nsided Wilcoxon rank-sum tests between genes with below-median PCRI (n =4 7 3 9\nfor both low and high expression groups) and above-median PCRI (n =4 7 3 8a n d\n4739 for low and high expression groups, respectively) are shown. In the boxes\nwithin the violinplot, the white point denotes the median and the upper and lower\nbox limits denote upper and lower quartiles.d Average HM signals near the pCREs\ninteracting with top 1000 genes with the highest PCRI among highly and lowly\nexpressed genes.e Functional enrichment analysis of the top 1000 genes with the\nhighest PCRI among highly expressed genes in H1 cells. Benjamini-Hochberg\nadjusted Fisher’se x a c tp-values are shown in negative logarithmic scale.f Average\nHM signals near the pCREs interacting with top 1000 genes with the highest PCRI\namong highly and lowly expressed genes.g Functional enrichment analysis of the\ntop 1000 genes with the highest PCRI among lowly expressed genes in H1 cells.\nBenjamini-Hochberg adjusted Fisher’s exactp-values are shown in a negative\nlogarithmic scale.h Representative genomic region showing suppressivecis-reg-\nulatory interactions forEN2. Black curved lines below the H3K27me3 signal track\nshows the 3D chromatin interactions centered at the core promoter ofEN2.N C B I\nRefSeq gene annotations are shown at the bottom. Source data are provided as a\nSource Dataﬁle.\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 10\nFig. 6 | Differentialcis-regulome analysis using PCRI values. aHierarchical\nclustering of top 1000 genes having the highest normalized PCRI variances across\ncell types. Representative GO terms enriched for the corresponding set of genes are\nshown on the right.b Histone modiﬁcation landscape around the transcription\nstart site of CCN2 and its pCREs in Liver (E066) and HepG2 (E118) cells. Red shades\ndenote promoter regions and blue shades denote pCRE regions interacting with the\npromoter. Red arrow represents a putative enhancer region that seems to be only\nactive in HepG2 cells. Source data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 11\nform of differential cis-regulome analysis can reveal an epigenomic\norigin of malignant gene expression.\nChromoformer learns the additive transcriptional activation at\ntranscription factories and switch-like suppression at\nPolycomb-bound silencing hubs\nTo assess the quantitative characteristics of the two regulatory hubs,\nnamely transcription factories, and silencing hubs, we analyzed the\ncorrelation between the levels of PCRIs and the number of pCREs\nassociated with transcriptional activation or suppression. First, to\ndeﬁne transcriptionally active regions across the genome, we utilized\nthe chromHMM-based chromatin state annotations available on the\nRoadmap Epigenomics Project. Among the 18 states, genomic intervals\nrepresenting active TSS, active gene bodies or enhancers were con-\nsidered as active regions, and we counted for each gene the number of\npCREs overlapping with the identiﬁed active regions. We found that\nthe number of the transcriptionally active pCREs and PCRI showed a\nmoderate but signiﬁcant positive correlation (Spearman’sr = 0 . 1 5 ,\np =1 . 3×1 0\n−42) in H1 cells (Fig.7a). As the expression levels of genes also\nincreased in accordance with the increasing number of tran-\nscriptionally active pCREs (Fig.7b), it implies that Chromoformer\nlearned the additive dynamics of pCREs in transcription factories for\ngene expression activation (Fig.7e).\nFor silencing hubs, we considered the pCREs associated with\nPolycomb group (PcG) proteins as functional silencers. Besides its\nenzymatic role as a histone methyltransferase targeting H3K27, it has\nbeen recently demonstrated that Polycomb repressive complex 2\n(PRC2) functions as a mediator of the repressivecis-regulatory inter-\naction between silencers and developmental promoters, promoting\nthe formation of PcG bodies\n35. To determine the number of PRC2-\nbound silencers, we utilized the ChIP-seq peaks for the two of the three\ncore subunits of PRC2, namely EZH2 and SUZ12, from ENCODE. As a\nresult, we found the logistic increase of PCRI upon the increasing\nnumber of pCREs overlapping with the EZH2 binding site in H1 cells\n(Fig. 7c, d), which indicates that the switch-like transcriptional sup-\npression (Fig. 7f) by PRC2-mediated silencing hub was learned by\nChromoformer. Similar results could be reproduced for SUZ12 binding\n(Supplementary Fig. 14a, b). Notably, this trend entirely disappeared\nwhen tested for the number of non-speciﬁc pCREs irrelevant to PRC2\n(Supplementary Fig. 14c), which is suggestive of the highly speciﬁc\nnature of PRC2-mediated silencing. We further asked in which cell\ntypes, other than H1, the switch-like dynamics of PRC2-mediated\nsilencing are observed. As a result, we found that such patterns in\nmesendoderm, neural progenitor cells and HUES64 cells (Supple-\nmentary Fig. 15). Interestingly, these cell types were already shown to\nhave close similarities when the various cell types were clustered based\non their H3K27me3 mark in repressed Polycomb chromatin states\n23.\nThus, our results highlight that the similarities in 1D epigenomic states,\nespecially Polycomb-associated H3K27me3 mark, can be recapitulated\nby the similarities in the functional impact ofcis-interactions. We also\nexamined if the pCREs associated with Polycomb repressive complex 1\n(PRC1) has different patterns of gene silencing and observed similar\npatterns (Supplementary Fig. 16). This may reﬂect the cooperativity of\nPRC1 and PRC2 in their localization at Polycomb response elements\n36,\nbut we cannot exclude the possibility that the model missed some\ngenes that were subjected to PRC1- or PRC2-speciﬁc cis-silencing.\nTo date, the collective effect of distalcis-regulatory elements on\ngene expression remains incompletely understood, but nevertheless,\nthe pioneering works exploiting modern technologies such as STARR-\nseq\n37 or CRISPRi-FlowFISH38 certainly provide us with deep insights\nabout their dynamics. Intriguingly, the observations drawn from the\ninterpretation of trained Chromoformer models, which are optimized\nto capture the quantitative characteristics ofcis-regulation, highly\nagree with the latest viewpoints from such studies. Our observations\non the additive transcriptional activation by active pCREs recapitulates\nthe results of a previous study on the quantitative characterization of\nenhancer activity inDrosophila. The underlying mechanism for this\nadditivity has been explained by either of the interaction hub or pro-\nmoter competition model\n39. The former assumes multi-way interac-\ntions between a promoter and several enhancers with independent\ncontributions, while the latter posits the one-to-one promoter-\nenhancer interactions and demonstrates that the probability of con-\ntact between a promoter and any enhancer increases as the number of\ncandidate enhancers increases. On the contrary, the quantitative nat-\nure of transcriptional silencing by PcG bodies with regard to the\nnumber of PcG-bound silencers is yet to be fully characterized.\nOur interpretation of Chromoformer leads to the hypothesis that\nthere exists a certain threshold of the local concentration of silencers\nfor PcG bodies to fully exert their suppressive function (Fig.7c). It may\nbe due to the synergy with other repressive epigenetic factors,\nincluding the DNA methylation induced by the HMs newly added by\nthose PcGs and other chromatin remodeling factors. In any case,\nexperimental validation of this hypothesis and further characterization\nof the biological factors that determine the tipping point of the PcG-\nmediated gene silencing will highly improve our understanding of\nprecise regulation of gene expression. Altogether, these results\ndemonstrate the utility of Chromoformer and, by extension, deep\nlearning models in the derivation of the new quantitative hypothesis in\nthe ﬁeld of computational biology that would ultimately facilitate\nexperimental validations and thus new scientiﬁc discoveries.\nDiscussion\nIn the present study, we proposed a transformer-based deep learning\narchitecture named Chromoformer to model the quantitative role of\nthe histone codes in the regulationof gene expression. Chromoformer\ngreatly improved the performance of gene expression prediction by\nmodeling the three-level hierarchy ofcis-regulation involving core\npromoters and pCREs. By the analyses of self-attention weights, latent\nembedding dynamics, and several feature ablation studies, we also\nprovided in-depth biological interpretations regarding the behavior of\nthe Chromoformer model. Thanks to the power of transformers for\ncomprehending distant dependencies in a sequence, Chromoformer\ncould successfully learn to focus on the speciﬁcr e g i o ni n s i d eg e n e\nbodies where the HMs associated with gene expression were the most\ndistinctive between highly expressed and lowly expressed genes.\nInterestingly, the amount of attention paid to the gene body was\ndependent on the epigenetic context of the TSS, implying that the\nChromoformer model captured the distant dependencies of the HMs\nplaced at TSS and gene body. On the other hand, by using transformers\nto model pairwise relationships within an unordered set of features,\nChromoformer could learn how the information mediated by histone\ncode is propagated from pCREs to core promoters through 3D chro-\nmatin folding to regulate gene expression. Analysis of the latent\nrepresentations of histone codes learned by the model highlighted\nthat the expression of housekeeping and cell-type speciﬁcg e n e sw e r e\nreinforced by the interaction with enhancers, whereas the expression\nof developmental genes were mainly repressed by the interaction with\nPRC2-bound silencers.\nWe explicitly used a pre-compiled knowledge of 3D chromatin\ninteractions to guide Chromoformer learning. Those experimentally\nmeasured interaction frequencies were used to prioritize the pCREs\nthat will participate in the model training by being explicitly injected\ninto the self-attention score matrices. However, it also seems possible\nto infer the interaction frequencies between pCREs and the core pro-\nmoters from genomic sequence information alone. This is because the\nspeciﬁcity of cis-regulatory interactions is largely governed by the\nrecognition of DNA sequence motifs by DNA-binding proteins\nincluding transcription factors or CCCTC-binding factors (CTCFs),\nwhich function as insulators that compartmentalize the 3D genome\nconformations. Therefore, those binding motifs embedded in the\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 12\ngenome may serve as hidden vocabularies that allow the inference of\nthe desired chromatin conformations solely based on the DNA\nsequence. Results from the recent model named Enformer strongly\nsupports that such de novo prioritization of pCREs are more effective\nwhen wider sequence information is used\n40, thereby suggesting the\nexciting possibilities for the fully data-driven modeling of gene\nexpression regulation through the integration of genomic and epige-\nnomic features using the transformer architecture. We leave this\ntransformer-based multi-omics integration as a further work.\nThe attention learned by the Embedding transformer that jumps\nfrom an active TSS to the gene body suggests that the HMs placed at\ngene bodies are indeed useful, if not the most critical, information\nwhen predicting the steady-state gene expression levels. From this\nresult, we can consider the possibility that using the entire landscape\nof histone codes distributed throughout a single gene may further\nimprove the predictive accuracy for steady-state mRNA levels. Fur-\nthermore, as H3K36me3 is far more enriched at exons than introns,\nutilizing the full-length gene annotation will be another effective gui-\ndance for model training. As gene lengths and exon-intron distribu-\ntions show great variability, we need some clever representation of\nsuch biological prior knowledge. Again, the transformer architecture\nwould be one of the most powerful choices because one canﬂexibly\napply masks to deal with variable-length inputs and also can extend\npositional encoding to form composite encoding that simultaneously\nharbors information for both genomic positions and annotations for\ngene structures.\nThe proposed training scheme for Chromoformer models is\nhighly expandable. For instance, we showed that Chromoformer\nmodels can be trained for cell types from species other than human,\nnamely mouse embryonic stem cells, using relevant histone ChIP-seq\n24\nand Hi-C proﬁles41, and the overall similarity between the grammars of\nhistone codes between the two species was demonstrated through\ncross-species prediction performances (Fig.8a– c). Also, cross-cell-\ntype prediction experiments showed that a Chromoformer model\ntrained in one cell type was still applicable to other cell types to some\ndegree (relative validation AUC > 92%), with the cross-prediction per-\nformances being higher for similar cell types (Fig.8d). This implies that\nChromoformer trained in cell type-speciﬁc manner not only learned\ncell type-speciﬁc features of gene regulation, but also still captured the\ngeneral rules that can be commonly applied to other cell types.\nChromoformer training can be extended to incorporate any additional\nepigenomic feature if it can be represented as an array of genomewide\nsignal values. Such features include transcription factor ChIP-seq sig-\nnals or the ﬁrst principal component (PC1) signals used for\nFig. 7 | Characteristics ofcis-regulation learned by Chromoformer. aAssociation\nbetween the number of transcriptionally active pCREs and PCRI.b Association\nbetween the number of transcriptionally active pCREs and gene expression level.\nc Association between the number of pCREs harboring EZH2 binding sites and\nPCRI. d Association between the number of pCREs harboring EZH2 binding sites\nand gene expression level. Throughout a-d, the number of genes having the cor-\nresponding number of pCREs are shown above the plot. In the boxplots, the center\nline denotes the median, the upper and lower box limits denote upper and lower\nquartiles, and the whiskers denote 1.5× interquartile range.e, f Illustrations for the\nproposed hypothetical models for regulatory dynamics ofe transcription factories\nand f silencing hubs are shown. Pol II, RNA polymerase II; PcG bodies, Polycomb\ngroup bodies; PRC2, Polycomb repressive complex 2. Source data are provided as a\nSource Dataﬁle.\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 13\ncompartment identiﬁcation. As mentioned above, CTCF binding is a\ncrucial determinant of 3D genome structure, and the promoter-\nproximal CTCF binding has been also highlighted in gene activation\nthrough distal enhancer-promoter interaction\n42, as recapitulated in\nSupplementary Fig. 17a. We could show that including CTCF gave\nmarginal but consistent increase in Chromoformer-clf performance\n(Supplementary Fig. 17b), and the increase was greater for Embedding\ntransformer-only Chromoformer-clf model (Supplementary Fig. 17c).\nOn the other hand, informing Chromoformer of cell-type speciﬁc\ngenomic compartmentalization state\n43 using PC1 value as an additional\nfeature did not result in signiﬁcant overall performance gain (Sup-\nplementary Fig. 18a). Even though the compartmentalization corre-\nlated with the levels of gene expression (Supplementary Fig. 18b, c), as\nt h ea b s o l u t el e v e lo ft h ea s s o c i a t i o n( P e a r s o n’sc o r r e l a t i o nc o e fﬁcient\n0.12– 0.19) is not great enough, we concluded that the predictive\npower of compartment-level features did not exceed that of gene-level\nHM features.\nIn summary, Chromoformer is another exemplary application\nemphasizing the great potential of transformer architecture in mod-\neling biological sequences. This study also underscores the impor-\ntance of developing specialized deep learning architectures effectively\nembedded with biological prior knowledge, not only for the\nimprovement of the performance in predictive tasks, but also quanti-\ntatively characterizing the complexrelationships between biological\nentities.\nMethods\nPromoter-centered 3D chromatin interactions\nExperimentally validated core promoter-pCRE interaction pair\ninformation is required to train a Chromoformer model. In this\nstudy, we used publicly available data deposited in the 3DIV\ndatabase\n17 for promoter-centered long-range chromatin interac-\ntions compiled by pcHi-C experiments comprehensively conducted\nfor various tissue types. Interactions were characterized at the\nresolution of HindIII restriction fragments with median and average\nlength of 4797 bp and 5640 bp, respectively. We could also obtain\nnormalized interaction frequencies between the DNA fragments\nfrom the database. The obtained frequencies were already pro-\ncessed by a two-step normalization that accounts for the captur-\nability and distance-dependent background signals. Although the\nsigniﬁcant interactions could be selected based on the estimated\nFDR values provided along with each interaction, we considered an\ninteraction as signiﬁcant if the normalized frequency was greater\nthan 1.5 in order to increase the sensitivity of chromatin interactions\nduring the Chromoformer training. Note that a normalized fre-\nquency of 1.5 denotes that the ratio between the interaction and\nbackground signal is 1.5.\nTraining data preparation\nConsolidated ChIP-seq read alignments for seven major HMs\n(H3K4me1, H3K4me3, H3K9me3, H3K27me3, H3K36me3, H3K27ac,\nFig. 8 | Cross-species and cross-cell type prediction performances of Chromo-\nformer. a, bCross-species prediction performances of Chromoformer-clf models.\na Cross-validation (n = 4) performances of Chromoformer-clf models, trained with\nES-Bruce4 mouse embryonic stem cells (mESC) or human embryonic stem cells\n(hESCs), are shown for the prediction of (a) mESC gene expression and (b) hESC\ngene expression. In the boxplot, the center line denotes the median, upper and\nlower box limits denote upper and lower quartiles, and whiskers denote 1.5×\ninterquartile range. c Log2-transformed histone modiﬁcation signals surrounding\nhumanSOX2TSS and mouseSox2TSS.d Cross-cell type gene expression prediction\nperformances. Colors represent the relative validation AUC compared to the\nmatched cell type Chromoformer-clf model (i.e., trained and evaluated for the\nsame cell type). Val, Validation. Source data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 14\nand H3K9ac) were obtained from the Roadmap Epigenomics\nProject23 in tagAlign format. This alignment data could make a\nhighly homogeneous training dataset, since the reads were trun-\ncated to 36 bp to reduce read length bias originating from the\ndifference in sequencing experiments, and also they were sub-\nsampled to a maximum of 30 million reads to homogenize for read\ndepths. After sorting and indexing the alignments with Sambamba\nv0.6.8\n44, read depths along the hg19 reference genome were\ncomputed for each base position usinggenomecov command of\nBedtools v2.23.045. For each given core promoter or pCREs, we\ncomputed log2-transformed binned average signals of seven HMs\nalong non-overlapping genomic windows of size 100 bp, 500 bp,\nand 2 kbp that fully covers the region and used those values as\ninput features for our model. Since the sizes of pCREs were not\nﬁxed in our setting, we zero-padded pCRE feature matrices to make\ntheir size agree with that of the core promoter feature matrices.\nSpeciﬁcally, we center-aligned the matrix and appended zero-\nmatrices of the appropriate size to the left and right side of the\ninput matrix. To determine prediction target labels, normalized\ngene expression levels (in RPKM) were also downloaded from the\nRoadmap Epigenomics Project. RefSeq annotation was used to\ndetermine the TSS for each gene. Total 18,955 genes that were\nappropriately annotated and had expression measurements were\nselected for model training and evaluation. The whole data col-\nlection and preprocessing pipeline is implemented in snakemake\nworkﬂow management system v6.5.3\n46.\nGene-wise prediction targets for the three Chromoformer variants\n(Chromoformer-clf, Chromoformer-reg and Chromoformer-diff) were\nderived as follows. Most simply, log2-transformed RPKM values were\nused for Chromoformer-reg training. For Chromoformer-diff training,\nwe used log2 fold-change of RPKM values between two cell types as\nprediction targets. To derive binary target labels for Chromoformer-clf\ntraining, the median expression values across all genes in each cell type\nwere used as threshold values to assign genes with one of the two\nlabels: highly (1) or lowly expressed (0). In other words, a gene whose\nexpression is above median was assigned with label“1”, and the others\nwere assigned with label“0”. This formulation of binary classiﬁcation\nof gene expression has been widely adopted for various machine-\nlearning approaches for gene expression modeling. Note that these\nlabels (“1” and “0”) do not have quantitative meanings, but just denote\nthe ordinal indices of binary classiﬁcation labels. That is,“0” indicates\nthat the gene is assigned to theﬁrst class, and“1” indicates that the\ngene is assigned to the second class.\nSelection of cell types for model training\nFor Chromoformer-clf and Chromoformer-reg model training, we only\nchose a subset of cell types analyzed in the Roadmap Epigenomics\nProject for which the whole proﬁles of gene expression, HMs, and 3D\nchromatin interactions were available. Since the 3D chromatin inter-\naction proﬁles we used are not the ofﬁcial results of Roadmap Epige-\nnomics but are obtained from an independent source, we manually\nmatched the epigenome IDs (EIDs) and cell type mnemonics from 3DIV\ndatabase\n17. As a result, the following 11 cell types were selected for\nChromoformer training: H1 cells (E003, H1), H1 BMP4 derived\nmesendoderm (E004, ME), H1 BMP4 derived trophoblast (E005, TB),\nH1 derived mesenchymal stem cells (E006, MSC), H1 derived neuronal\nprogenitor cultured cells (E007, NPC), HUES64 cells (E016, H1), Liver\n(E066, LI11), Pancreatic islets (E087, PA), A549 EtOH 0.02pct lung\ncarcinoma (E114, LG), GM12878 lymphoblastoid (E116, GM) and HepG2\nhepatocellular carcinoma (E118, LI11).\nChromoformer model architecture\nChromoformer consists of three modules based on transformer enco-\nders: Embedding, Pairwise Interaction and Regulation transformer.\nT h ee m b e d d i n gt r a n s f o r m e rh a sas i n g l ee n c o d e rl a y e r ,w h i c h\ntakes a binned average signal matrixX\ninput of seven HMs at a core\npromoter and summarizes it into a core promoter embedding matrix\nX\nemb that consists ofﬁxed-sized latent embedding vectors. Before\nXinput is fed into the module, seven-dimensional input features for each\nof then bins areﬁrst linearly projected into the dimension ofdemb\n( = 128), then a positional encoding matrixP of dimensionn × demb is\nadded to the input feature matrix of the same dimension in an\nelement-wise manner.P\nij is deﬁned as below.\nPij =\nsin i\n10000\n2k\ndemb\n/C18/C19\n,i f j =2 k\ncos i\n10000\n2k\ndemb\n/C18/C19\n,i f j =2 k +1\n8\n>>><\n>>\n>:\nð1Þ\nIt is worth noting that the inner product between any two-row\nvectors (e.g.,a-th andb-th rows) in the positional encoding matrixP,\nnamely P\na and Pb, only depends on the positional distance∣a /C0 b∣\nbetween the two vectors. Therefore, by adding a positional encoding\nmatrix to the input feature matrix, the relative distance between any\ntwo features can be recognized in the following multi-head attention\nlayers. A multi-head attention layer in Embedding transformer utilizes\na self-attention mechanism to capture inter-dependencies between\nHM conﬁgurations at different positions that contribute to the reg-\nulation of gene expression. Importantly, those operations are done\nseparately for multiple heads so that the model can capture different\naspects of inter-dependencies between input features. Self-attention\noperation in the transformer architecture is a special case of scaled\ndot-product attention where the query, key and value matrices origi-\nnate from the same sequence of features. Speciﬁcally, position-\nencoded input feature matrix of dimensionn × d\nemb is linearly pro-\njected to produce three matrixQemb, Kemb and Vemb of dimension\nn × d0\nemb, which semantically represents a query, key, and value matrix,\nrespectively.d0\nemb is set to 64. Then × n matrix produced by a multi-\nplication ofQemb and KT\nemb is called a pairwise afﬁnity matrix since each\nelement of the matrix is equivalent to a dot product between the\ncorresponding pair of vectors fromQ\nemb and KT\nemb. It denotes an\namount of afﬁnity between the two positions in the input sequence.\nThe pairwise afﬁnity matrix is divided by\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\nd0\nemb\nq\nand softmax function\nis applied to convert self-attention afﬁnities into a weight that sums to 1\nfor each row. The value matrixVemb is multiplied with the resulting\nattention weight matrix toﬁnally produce the output of self-attention\noperation. The whole process of scaled dot-product can be summar-\nized as below:\nAttention\nembðQemb,Kemb,VembÞ =s o f t m a xQembKT\nembﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\nd0\nemb\nq\n0\nB@\n1\nCAVemb ð2Þ\nIn the Embedding transformer, the self-attention operation above\nis separately done bymemb (= 2) heads and the resultingmemb vectors\nof dimension d0\nemb are concatenated to form a single\nd0\nemb × memb = demb (= 128) dimensional vector so that the dimension of\ninput feature is preserved. Subsequently, the input sequence of fea-\ntures right before the self-attention is added via residual connection\nand then they are layer-normalized. The result is then subjected to the\nlinear projection layer into the dimension ofδ\nemb (= 128), nonlinear\nactivation by rectiﬁed linear unit (ReLU) andﬁnal linear projection into\nthe dimension ofdemb.T h i ss e r i e so fo p e r a t i o n si n v o l v i n gl i n e a rp r o -\njection, nonlinear activation and another linear projection comprises a\nposition-wise feedforward layer.\nPositionwiseFeedForward xðÞ =L i n e a rðReLUðLinearðxÞÞÞ ð 3Þ\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 15\nAfter another residual connection and layer normalization, the\ncore promoter embedding matrixXemb is ﬁnally produced.\nPairwise Interaction transformer consists of two stacked layers to\nupdate a core promoter embedding based on its pairwise interaction\nwith each pCRE and produce pairwise interaction embedding matrix\nXpair. The difference between encoder-decoder attention and self-\nattention operation in the Embedding transformer is that encoder-\ndecoder attention separately builds query matrix and key-value\nmatrices. Speciﬁcally, query matrixQ\npair is derived from Xemb (or\nXpair resulting from theﬁrst layer), while key and value matricesKpair\nand Vpair are built from position-encoded pCRE HM featuresXHM.I n\nshort, the query, key and value matrices for the encoder-decoder\nattention can be summarized as below, where LinearNoBias denotes a\nlinear projection function without a bias.\nQ\npair =\nLinearNoBiasðXembÞ, first layer\nLinearNoBiasðXpairÞ, second layer\n(\nKpair = LinearNoBiasXHM\n/C0/C1\nVpair = LinearNoBiasXHM\n/C0/C1\nð4Þ\nThen scaled dot-product attention is conducted between core\npromoter queries and pCRE key-values as below, whered0\npair =6 4a n d\ndpair =1 2 8 :\nAttentionpairðQpair, Kpair,VpairÞ =s o f t m a x\nQpairKT\npairﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\nd0\npair\nq\n0\nB@\n1\nCAVpair ð5Þ\nThe rest of the operations, including position-wise feedforward\nlayer withδpair ( = 256), residual connections and layer normalizations\nare the same as the Embedding transformer,ﬁnally producing the\npairwise interaction embeddingXpair.\nTo avoid excessive computational load and to make the training\nbatch ﬁt in the memory of a single graphical processing unit (GPU)\nduring training, we only considered at mostimax ( = 8) pCREs for each\ncore promoter. To determine the set of pCREs participating in the\ntraining, all the candidate pCREs were prioritized according to their\nnormalized interaction frequencies with the core promoters since the\npCRE that is interacting more frequently is likely to be more infor-\nmative in predicting the expression of the corresponding gene.\nRegulation transformer consists of six stacked layers with gated\nself-attention mechanism. The key function of the Regulation trans-\nformer is to updateX\nemb along with the whole set ofXpair‘s at the same\ntime toﬁnally produce the regulatory embeddingXreg. To this end,\nindividual embedding vectors that exactly represent the genomic bin\nwhere the relevant TSS is located are extracted fromX\nemb and Xpair‘s.\nThen, they are concatenated side by side to form a composite input\nmatrix X\ncomp of dimension imax +1\n/C0/C1\n× demb (Recall thatdemb = dpair =\n128). Speciﬁcally, those are the vectors at the midpoint of the\nembedding matrices. Note that for genes having less thanimax cis-\nregulatory interactions, the rest ofXcomp was ﬁlled with dummy zero\nvectors. The Regulation transformer does not need a positional\nencoding since it does not assume any predeﬁned order among the\nembeddings. We onlyﬁx that theﬁrst row vector of the composite\ninput matrix is the core promoter embedding. This unordered set of\nembeddings is fed to a gated self-attention mechanism to allow the\nmodel to decide how much it will actively utilize the transformed\nembedding carrying the interaction information. In addition to the\nquery, key, and value matrices, gated self-attention introduces a gate\nmatrix G\nreg that learns the amount of information transfer. The four\nimax +1\n/C0/C1\n× d0\nreg matrices used for gated self-attention operation are\ncomputed as below, whered0\nreg = 32:\nQreg = LinearNoBiasXcomp\n/C16/C17\nKreg = LinearNoBiasðXcompÞ\nVreg = LinearNoBiasðXcompÞ\nGreg = LinearNoBiasðXcompÞ\nð6Þ\nMoreover, we added a vector of normalized interaction fre-\nquencies f between the corresponding core promoter-pCRE pair as a\nbias term to the self-attention matrix to inform the model with the\nrelative afﬁnities of the pairwise interactions. Ani\nmax +1\n/C0/C1\n× imax +1\n/C0/C1\nbias matrixB is introduced, whoseﬁrst row isﬁlled withf and all the\nother values are zero. Taken together, the attention operation used in\nRegulation transformer can be written as below:\nAttentionreg Qreg, Kreg, Vreg, Greg\n/C16/C17\n=s o f t m a x\nQregKT\nregﬃﬃﬃﬃﬃﬃﬃﬃﬃ\nd0\nreg\nq + γB\n0\nB@\n1\nCAVreg /C14 σðGregÞ ð7Þ\nwhere γ, σ and /C14 represent a learnable scalar coefﬁcient, the sigmoid\nfunction and the Hadamard product, respectively.\nWe concatenated threeXreg‘s resulting from independent mod-\nules learning from 100 bp, 500 bp, and 2 kbp-resolution inputs,\nrespectively. Only theﬁrst row of the concatenated matrix, which\ndenotes thecis-regulation-aware embedding vector of the core pro-\nmoter was extracted and fed into the fully-connected head. In all the\nthree variants of Chromoformer models (Chromoformer-clf,\nChromoformer-reg and Chromoformer-diff), the fully-connected head\nhad a single 128-dimension hidden layer with ReLU activation. The\nfully-connected head for Chromoformer-clf produces a two-\ndimensional output representing the two prediction logits for each\nbinary expression label, while that for Chromoformer-reg produces a\nsingle scalar representing the log2-transformed gene expression value.\nIn Chromoformer-diff, the fully-connected head is fed with a con-\ncatenated vector of two multi-scale regulatory embeddings from each\ncell type and produces a single scalar representing the log2 fold-\nchange of gene expression. Moreover, Chromoformer-diff adopts two\nauxiliary tasks predicting absolute levels of log2-transformed gene\nexpression for each cell type (Supplementary Fig. 5a). All of the\nChromoformer variants were implemented using PyTorch v1.9.0\n47.\nModel training and evaluation\nAll variants of Chromoformer models were trained for 10 epochs with\nAdamW optimizer\n48 and the model resulting from the last epoch was\nchosen as theﬁnal model. The initial learning rate was chosen as\n3×1 0−5 and was decreased by 13% after each epoch so that it can\napproximately shrink to half of its value after each of theﬁve epochs. In\nChromoformer-clf, cross-entropy between the predicted probability\nand one-hot encoded binary gene expression label was used as a loss\nfunction. In Chromoformer-reg and Chromoformer-diff, mean\nsquared error (MSE) between the predicted scalar and target value was\nused as a loss function. Batch size wasﬁxed to 64. All the imple-\nmentations for benchmark deep learning models were obtained from\nthe ofﬁcial code repositories provided by the respective authors. To\ntrain the benchmark models, we applied the optimal hyperparameters\nthat were previously identiﬁed for each benchmark model.\nFor GC-MERGE training, we needed to modify our input repre-\nsentations of HM signals and cis-regulatory interactions as per\nrequired. ChIP-seq read depths for each 10 kbp bin throughout the\ngenome were calculated usingmulticovcommand of Bedtools, and the\ninteraction frequencies between those 10 kbp genomic bins were\ndetermined using the pcHi-C experiment results. Since GC-MERGE\npredictions are made for each of the 10 kbp bins, but not for each gene,\nambiguity arises when there are two or more genes in the same bin.\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 16\nThe ambiguity is resolved by choosing a representative gene within\neach bin and assigning it with the most frequently occurring labels in\nthat bin. This is done at the cost of reduced number of predictable\ngenes. To perform as fair comparison as possible, we went through\n4-fold CV for both GC-MERGE and Chromoformer using the same gene\nset whose expression is predictable by GC-MERGE, by retraining\nChromoformer model.\nAnalysis of Embedding transformer self-attention\nEach Embedding transformer has two independent attention heads, so\nit produces two corresponding self-attention pairwise afﬁnity matrices\nQ\nembKT\nemb=\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\nd0\nemb\nq\nfor each input. Since the full model consists of the\nthree independent single-resolution modules, we can extract six self-\nattention weight matrices in total. We visualized the softmax-\nnormalized pairwise afﬁnities, i.e., self-attention weights, for Fig.4.\nNote that all the self-attention weights were obtained at the time of\ninference for genes in validation set.\nComputation of the predictedcis-regulatory impact, normal-\nization and clustering\nTo compute predicted cis-regulatory impact (PCRI), weﬁrst deﬁned a\nmulti-resolution core promoter embedding as the concatenation of\nindividual core promoter embeddings resulting from the three dif-\nferent input resolutions. Then, PCRI is deﬁned as the Euclidean dis-\ntance between the multi-resolution core promoter embedding and the\nmulti-resolution regulatory embedding for each gene. Importantly, we\nstandardized each embedding vector before calculating the Euclidean\ndistance to correct for the global shift arising from the transformation\nitself. Similar to the self-attention analysis, the entire PCRI values dis-\ncussed in the main text were calculated for genes in each validation set\nto ensure that the model did not explicitly learn the optimal trans-\nformation of latent embeddings re ﬂecting cis-regulations for\nthose genes.\nTo cluster cell types and genes based on PCRI values, the values\nwere standardized (i.e., Z-score normalized) beforehand. The resulting\nnormalized PCRI values in general had bell-shaped distribution around\nzero (Supplementary Fig. 12). Average linkage hierarchical clustering\nwith correlation similarity was conducted using top 1000 genes having\nhighest across-cell variances of PCRI values.\nFunctional enrichment analysis of genes with high PCRI\nFor each of the four validation folds, we identiﬁed top 250 genes with\nthe highest PCRI values separately for each binary label. Functional\nenrichment analysis of the union oft h ef o u rg e n es e t sw e r ed o n eu s i n g\nEnrichr\n49. Gene ontology biological process terms with Benjamini-\nHochberg adjusted p-values <0.05 were selected as signiﬁcantly\nenriched terms.\nDeﬁnition of polycomb-associated pCREs\nTo deﬁne pCREs associated with Polycomb-bound region, we used\nirreproducible discovery rate (IDR)-thresholded ChIP-seq peaks for\nPolycomb subunits determined in H1 cells that are publicly available in\nENCODE. Speciﬁcally, we downloaded EZH2 and SUZ12 ChIP-seq peaks\nfor PRC2, and RNF2 and CBX8 ChIP-seq peaks for PRC1 (Supplemen-\ntary Table 1).\nMouse embryonic stem cell data processing\nTo evaluate the utility of Chromoformer for species other than human,\nwe processed the ENCODE reference epigenome of ES-Bruce4 mouse\nembryonic stem cell (mESC) line from its raw histone ChIP-seq reads\n(Supplementary Table 2). To be consistent with human data, the pro-\ncessing pipeline followed that of Roadmap Epigenomics Project as\ndescribed below. After downloading FASTQﬁles, histone ChIP-seq\nreads wereﬁrst aligned to mm9 reference genome using bwa v0.7.17-\nr1188\n50. To normalize the effect of read length, each aligned read was\nthen truncated up to 36 bp. Also, the read depths were normalized by\nsubsampling the read alignment up to 3 million reads. Processed\nalignments were converted to genomewide read depth signals using\nbedtools genomecov. Besides, to determine the promoter-pCRE inter-\nactions that are used for Chromoformer training, we used the nor-\nmalized interaction frequencies from publicly available Hi-C\ninteraction matrices of mESC\n41. The bulk RNA-seq gene expression\nproﬁle for ES-Bruce4 cells was also obtained from ENCODE underﬁle\naccession ENCFF166EXS [ https://www.encodeproject.org/\nexperiments/ENCSR000CGU]. Gencode vM1 gene annotation was\nused to determine the transcription site and promoter region for\neach gene.\nCTCF ChIP-seq data processing\nTo examine the effect of including CTCF signal in model training, we\nobtained raw CTCF ChIP-seq reads from ENCODE. Onlyﬁve cell\ntypes had available CTCF ChIP-seq data in ENCODE: H1 cells (E003),\nH1 derived neuronal progenitor cultured cells (E007), A549 lung\ncarcinoma (E114), GM12878 lymphoblastoid (E116), and HepG2\nhepatocellular carcinoma cells (E118) (Supplementary Table 3).\nCTCF ChIP-seq reads were processed in an exact same way as mESC\nhistone ChIP-seq reads (see above), but using hg19 reference\ngenome.\nReporting summary\nFurther information on experimental design is available in the Nature\nResearch Reporting Summary linked to this paper.\nData availability\nHistone ChIP-seq read alignments, mRNA expression proﬁles and\nchromHMM chromatin states used in this study were downloaded\nfrom Roadmap Epigenomics Web Portal\n23 [https://egg2.wustl.edu/\nroadmap/web_portal/index.html]. Normalized interaction frequencies\nfor promoter-centered Hi-C experiments were obtained from hg19\npcHi-C data collection of 3DIV database [http://3div.kr/] under tissue\nmnemonics H1, ME, TB, MSC, NPC, LI11, PA, LG, and GM. TF ChIP-seq\nreads targeted for PRC1/2 subunits and CTCF were also downloaded\nfrom ENCODE\n24 under accession codes speciﬁed in Supplementary\nTables 1 and 3. TAD and genomic compartmentalization information\n(in PC1 values) were downloaded from the Supplementary Material of\nSchmitt et al.\n43 Raw histone ChIP-seq reads and the mRNA expression\nproﬁle for ES-Bruce4 mouse embryonic stem cell data were also\ndownloaded from ENCODE24 under accession codes speciﬁed in Sup-\nplementary Table 2. The accession code for the ES-Bruce4 mRNA\nexpression proﬁle was ENCFF166EXS [https://www.encodeproject.org/\nexperiments/ENCSR000CGU]. Hi-C interaction frequency matrices for\nES-Bruce4 cells were downloaded from the data repository of Dixon\net al.\n41 NCBI RefSeq gene annotations were downloaded from UCSC\nTable Browser [https://genome.ucsc.edu/cgi-bin/hgTables]. Gencode\nvM1 gene annotations were downloaded from GENCODE [https://www.\ngencodegenes.org/mouse/release_M1.html]. Source data are provided\nwith this paper.\nCode availability\nThe source code for Chromoformer model are available at the GitHub\nrepository [ https://github.com/dohlee/chromoformer] under\ndoi:10.5281/zenodo.7151966\n51. Pretrained weights for Chromoformer-\nclf models are available at Figshare under doi:10.6084/\nm9.ﬁgshare.19424807.v1\n52. Code implementations for benchmark\nmodels were downloaded from the respective code repositories:\nDeepChrome [ https://github.com/QData/DeepChrome], Attentive-\nChrome [ https://github.com/QData/AttentiveChrome], DeepDiff\n[https://github.com/QData/DeepDiffChrome], GC-MERGE [ https://\ngithub.com/rsinghlab/GC-MERGE], and HM-CRNN [ https://github.\ncom/pptnz/deeply-learning-regulatory-latent-space].\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 17\nReferences\n1. Jenuwein, T. & Allis, C. D. Translating the histone code.Science293,\n1074– 1080 (2001).\n2. Singh, R., Lanchantin, J., Robins, G. & Qi, Y. DeepChrome: deep-\nlearning for predicting gene expression from histone modiﬁcations.\nBioinformatics32,i 6 3 9– i648 (2016).\n3. Singh, R., Lanchantin, J., Sekhon, A. & Qi, Y. Attend and predict:\nunderstanding gene regulation by selective attention on chromatin.\nAdv. Neural Inf. Process Syst.30,6 7 8 5– 6795 (2017).\n4 . A l i p a n a h i ,B . ,D e l o n g ,A . ,W e i r a u c h ,M .T .&F r e y ,B .J .P r e d i c t i n gt h e\nsequence speciﬁcities of DNA- and RNA-binding proteins by deep\nlearning.Nat. Biotechnol.33,8 3 1– 838 (2015).\n5. Jumper, J. et al. Highly accurate protein structure prediction with\nAlphaFold.Nature 596,5 8 3– 589 (2021).\n6. Cho, K. et al. Learning phrase representations using RNN encoder-\ndecoder for statistical machine translation. InProceedings of the\n2014 Conference on Empirical Methods in Natural Language Pro-\ncessing.1 7 2 4– 1734 (2014).\n7. Hochreiter, S. & Schmidhuber, J. Long short-term memory.Neural\nComput. 9,1 7 3 5– 1780 (1997).\n8. Shlyueva, D., Stampfel, G. & Stark, A. Transcriptional enhancers:\nfrom properties to genome-wide predictions.Nat. Rev. Genet15,\n272– 286 (2014).\n9. Gorkin, D. U., Leung, D. & Ren, B. The 3D genome in transcriptional\nregulation and pluripotency.Cell Stem Cell14,7 6 2– 775 (2014).\n10. Harmston, N. & Lenhard, B. Chromatin and epigenetic features of\nlong-range gene regulation.Nucleic Acids Res.41,\n7185– 7199 (2013).\n11. Lieberman-Aiden, E. et al. Comprehensive mapping of long-range\ninteractions reveals folding principles of the human genome.Sci-\nence 326,2 8 9– 293 (2009).\n12. Bigness, J., Loinaz, X., Patel, S., Larschan, E. & Singh, R. Integrating\nlong-range regulatory interactions to predict gene expression using\ngraph convolutional networks.J. Computational Biol.29,\n409– 424 (2022).\n13. Vaswani, A. et al. Attention is all you need. InAdvances in neural\ninformation processing systems(2017).\n14. Ji, Y., Zhou, Z., Liu, H. & Davuluri, R. V. DNABERT: pre-trained bidir-\nectional encoder representations from transformers model for\nDNA-language in genome.Bioinformatics\n37, 2112– 2120 (2021).\n15. Rives, A. et al. Biological structure and function emerge from\nscaling unsupervised learning to 250 million protein sequences.\nP r o c .N a t lA c a d .S c i .U S A118, e2016239118 (2021).\n16. Rao, R. et al. MSA transformer. InInternational Conference on\nMachine Learning.8 8 4 4– 8856 (2021).\n17. Jung, I. et al. A compendium of promoter-centered long-range\nchromatin interactions in the human genome.Nat. Genet51,\n1442– 1449 (2019).\n18. Bahdanau, D., Cho, K. Bengio, Y. Neural machine translation by\njointly learning to align and translate. In3rd International Con-\nference on Learning Representations(2015).\n19. Luong, M.-T., Pham, H. Manning C. D. Effective approaches to\nattention-based neural machine translation. InProceedings of the\n2015 Conference on Empirical Methods in Natural Language Pro-\ncessing.1 4 1 2– 1421 (2015).\n20. Lin, Z. et al. A structured self-attentive sentence embedding. In5th\nInternational Conference on Learning Representations(2017).\n21. Sekhon, A., Singh, R. & Qi, Y. DeepDiff: DEEP-learning for predicting\nDIFFerential gene expression from histone modiﬁcations.Bioinfor-\nmatics 34,i 8 9 1– i900 (2018).\n22. Kang, M., Lee, S., Lee, D. & Kim, S. Learning cell-type-speciﬁcg e n e\nregulation mechanisms by multi-attention based deep learning\nwith regulatory latent space.Front Genet11, 869 (2020).\n23. Roadmap Epigenomics Consortium. et al. Integrative analysis of 111\nreference human epigenomes.Nature 518,3 1 7– 330 (2015).\n24. Encode Project Consortium. An integrated encyclopedia of DNA\nelements in the human genome.Nature 489,5 7– 74 (2012).\n25. Yoh, S. M., Lucas, J. S. & Jones, K. A. The Iws1:Spt6:CTD complex\ncontrols cotranscriptional mRNA biosynthesis and HYPB/Setd2-\nmediated histone H3K36 methylation.Genes Dev.22,\n3422– 3434 (2008).\n26. Dong, X. et al. Modeling gene expression using chromatin features\nin various cellular contexts.Genome Biol.13,1 – 17 (2012).\n27. Sutherland, H. & Bickmore, W. A. Transcription factories: gene\nexpression in unions?Nat. Rev. Genet10,4 5 7– 466 (2009).\n28. Frankel, N. et al. Phenotypic robustness conferred by apparently\nredundant transcriptional enhancers.Nature\n466,4 9 0– 493\n(2010).\n29. Cai, Y. et al. H3K27me3-rich genomic regions can function as\nsilencers to repress gene expression via chromatin interactions.\nNat. Commun.12, 719 (2021).\n30. Yang, Y. M. et al. Gα12 overexpressed in hepatocellular carcinoma\nreduces microRNA-122 expression via HNF4alpha inactivation,\nwhich causes c-Met induction.Oncotarget6,1 9 0 5 5– 19069\n(2015).\n31. Xu, C. et al. RNA-binding protein 39: a promising therapeutic target\nfor cancer.Cell Death Disco.7, 214 (2021).\n32. Jung, D. J., Na, S. Y., Na, D. S. & Lee, J. W. Molecular cloning and\ncharacterization of CAPER, a novelcoactivator of activating protein-\n1 and estrogen receptors.J. Biol. Chem.277, 1229– 1234 (2002).\n33. Wang, X. J., Li, F. F., Zhang, Y. J., Jiang, M. & Ren, W. H. TRIB3\npromotes hepatocellular carcinoma growth and predicts poor\nprognosis.Cancer Biomark.29,3 0 7– 315 (2020).\n34. Jia, Q., Dong, Q. & Qin, L. CCN: core regulatory proteins in the\nmicroenvironment that affect the metastasis of hepatocellular\ncarcinoma?Oncotarget7,1 2 0 3– 1214 (2016).\n35. Ngan, C. Y. et al. Chromatin interaction analyses elucidate the roles\nof PRC2-bound silencers in mouse development.Nat. Genet52,\n264– 272 (2020).\n36. Kahn, T. G. et al. Interdependence of PRC1 and PRC2 for recruitment\nto Polycomb Response Elements.Nucleic Acids Res44,\n10132– 10149 (2016).\n37. Arnold, C. D. et al. Genome-wide quantitative enhancer activity\nmaps identiﬁed by STARR-seq.Science 339,1 0 7 4– 1077 (2013).\n38. Fulco, C. P. et al. Activity-by-contact model of enhancer-promoter\nregulation from thousands of CRISPR perturbations.Nat. Genet51,\n1664– 1669 (2019).\n39. Schoenfelder, S. & Fraser, P. Long-range enhancer-promoter con-\ntacts in gene expression control.Nat. Rev. Genet20,\n437– 455 (2019).\n40. Avsec, Z. et al. Effective gene expression prediction from sequence\nby integrating long-range interactions.Nat. Methods18,\n1196– 1203 (2021).\n41. Dixon, J. R. et al. Topological domains in mammalian genomes\nidentiﬁ\ned by analysis of chromatin interactions.Nature 485,\n376– 380 (2012).\n42. Kubo, N. et al. Promoter-proximal CTCF binding promotes distal\nenhancer-dependent gene activation.Nat. Struct. Mol. Biol.28,\n152– 161 (2021).\n43. Schmitt, A. D. et al. A compendium of chromatin contact maps\nreveals spatially activeregions in the human genome.Cell Rep.17,\n2042– 2059 (2016).\n44. Tarasov, A., Vilella, A. J., Cuppen, E., Nijman, I. J. & Prins, P. Sam-\nbamba: fast processing of NGS alignment formats.Bioinformatics\n31,2 0 3 2– 2034 (2015).\n4 5 . Q u i n l a n ,A .R .&H a l l ,I .M .B E D T o o l s :aﬂexible suite of utilities\nfor comparing genomic features.Bioinformatics26,8 4 1– 842\n(2010).\n46. Molder, F. et al. Sustainable data analysis with Snakemake.\nF1000Res10, 33 (2021).\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 18\n47. Paszke, A. et al. PyTorch: An imperative style, high-performance\ndeep learning library. InAdvances in Neural Information Processing\nSystems (2019).\n48. Loshchilov, I. Hutter, F. Decoupled weight decay regularization. In\nInternational Conference on Learning Representations(2019).\n49. Kuleshov, M. V. et al. Enrichr: a comprehensive gene set enrichment\nanalysis web server 2016 update.Nucleic Acids Res44,\nW90– W97 (2016).\n50. Li, H. & Durbin, R. Fast and accurate short read alignment with\nBurrows-Wheeler transform.Bioinformatics25,1 7 5 4– 1760 (2009).\n51. Lee, D. Yang, J. Kim S. Chromoformer (Version 1.0) (Zenodo, 2022);\nhttps://doi.org/10.5281/zenodo.7151966.\n5 2 . L e e .D . ,Y a n g ,J .K i mS .P r e t r a i n e dChromoformer weights (Figshare,\n2022); https://doi.org/10.6084/m9.ﬁgshare.19424807.v1.\nAcknowledgements\nThis research was supported by the Collaborative Genome Program\nfor Fostering New Post-Genome Industry of the National Research\nFoundation (NRF), funded by the Ministry of Science and ICT (MSIT)\n(NRF-2014M3C9A3063541), Institute of Information & communica-\ntions Technology Planning & Evaluation (IITP) grant funded by the\nKorea government (MSIT) [NO.2021-0-01343, Artiﬁcial Intelligence\nGraduate School Program (Seoul National University)], and the Bio &\nMedical Technology Development Program of the National Research\nFoundation (NRF) funded by the Ministry of Science & ICT (NRF-\n2019M3E5D3073375 and NRF-2022M3E5F3085677) (to S. K.).\nAuthor contributions\nD.L., J.Y., and S.K. conceived the experiment(s), D.L. and J.Y. conducted\nthe experiment(s), D.L., J.Y., and S.K. analyzed the results. All authors\nreviewed the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-022-34152-5.\nCorrespondence and requestsfor materials should be addressed to\nSun Kim.\nPeer review informationNature Communicationsthanks Jim Clauwaert,\nRamana Davuluri and the other, anonymous, reviewer(s) for their con-\ntribution to the peer review of this work. Peer reviewer reports are\navailable.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons license and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2022\nArticle https://doi.org/10.1038/s41467-022-34152-5\nNature Communications|         (2022) 13:6678 19",
  "topic": "Chromatin",
  "concepts": [
    {
      "name": "Chromatin",
      "score": 0.7885372042655945
    },
    {
      "name": "Promoter",
      "score": 0.6350743770599365
    },
    {
      "name": "Computational biology",
      "score": 0.6238062381744385
    },
    {
      "name": "Histone",
      "score": 0.6011897921562195
    },
    {
      "name": "Histone code",
      "score": 0.5690160393714905
    },
    {
      "name": "Biology",
      "score": 0.44496914744377136
    },
    {
      "name": "Epigenomics",
      "score": 0.43536388874053955
    },
    {
      "name": "Computer science",
      "score": 0.4288191497325897
    },
    {
      "name": "Genetics",
      "score": 0.3901746869087219
    },
    {
      "name": "Gene",
      "score": 0.23922455310821533
    },
    {
      "name": "Nucleosome",
      "score": 0.1736639440059662
    },
    {
      "name": "Gene expression",
      "score": 0.1332375705242157
    },
    {
      "name": "DNA methylation",
      "score": 0.09518823027610779
    }
  ]
}