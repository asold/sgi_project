{
    "title": "A Distributed Knowledge Distillation Framework for Financial Fraud Detection Based on Transformer",
    "url": "https://openalex.org/W4394730807",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2288231282",
            "name": "Yuxuan Tang",
            "affiliations": [
                "Southwestern University of Finance and Economics"
            ]
        },
        {
            "id": "https://openalex.org/A2101297438",
            "name": "Zhanjun Liu",
            "affiliations": [
                "Chongqing University of Posts and Telecommunications"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6804746003",
        "https://openalex.org/W4362576462",
        "https://openalex.org/W4214576155",
        "https://openalex.org/W3186924568",
        "https://openalex.org/W3164897600",
        "https://openalex.org/W3159333674",
        "https://openalex.org/W4313307293",
        "https://openalex.org/W3125883326",
        "https://openalex.org/W4312816170",
        "https://openalex.org/W4321488527",
        "https://openalex.org/W2621388707",
        "https://openalex.org/W2786577118",
        "https://openalex.org/W3133885182",
        "https://openalex.org/W4285505260",
        "https://openalex.org/W4210269999",
        "https://openalex.org/W3217705725",
        "https://openalex.org/W2743289088",
        "https://openalex.org/W6784827398",
        "https://openalex.org/W3107622331",
        "https://openalex.org/W4386159907",
        "https://openalex.org/W3198730749",
        "https://openalex.org/W3025678142",
        "https://openalex.org/W3092415316",
        "https://openalex.org/W4391770543",
        "https://openalex.org/W6733260875",
        "https://openalex.org/W4319431571",
        "https://openalex.org/W2946712695",
        "https://openalex.org/W4379617205",
        "https://openalex.org/W1120269880",
        "https://openalex.org/W3096733902",
        "https://openalex.org/W2606665849",
        "https://openalex.org/W4352982040",
        "https://openalex.org/W4380761741",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2585476045",
        "https://openalex.org/W3217513252",
        "https://openalex.org/W3095368626"
    ],
    "abstract": "Financial fraud cases causing serious damage to the interests of investors are not uncommon. As a result, a wide range of intelligent detection techniques are put forth to support financial institutions&#x2019; decision-making. Currently, existing methods have problems such as poor detection accuracy, slow inference speed, and weak generalization ability. Therefore, we suggest a distributed knowledge distillation architecture for financial fraud detection based on Transformer. Firstly, the multi-attention mechanism is used to give weights to the features, followed by feed-forward neural networks to extract high-level features that include relevant information, and finally neural networks are used to categorize financial fraud. Secondly, for the problem of inconsistent financial data indicators and unbalanced data distribution focused on different industries, a distributed knowledge distillation algorithm is proposed. This algorithm combines the detection knowledge of the multi-teacher network and migrates the knowledge to the student network, which detects the financial data of different industries. The final experimental results show that the proposed method outperforms other methods in terms of F1 score (92.87%), accuracy (98.98%), precision (81.48%), recall (95.45%), and AUC score (96.73%) when compared to the traditional detection methods.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.1 120000\nA Distributed Knowledge Distillation Framework\nfor Financial Fraud Detection based on\nTransformer\nYUXUAN TANG1, ZHANJUN LIU2\n1School of Accounting, Southwestern University of Finance and Economics, Chengdu 611130, Sichuan, China (e-mail: tomyx@qq.com)\n2School of Communication and Information Engineering, Chongqing University of Posts and telecommunications, Chongqing 400065, China (e-mail:\nliuzj@cqupt.edu.cn)\nCorresponding author: Yuxuan Tang(e-mail: tomyx@qq.com).\nABSTRACT Financial fraud cases causing serious damage to the interests of investors are not uncommon.\nAs a result, a wide range of intelligent detection techniques are put forth to support financial institutions’\ndecision-making. currently, existing methods have problems such as poor detection accuracy, slow inference\nspeed, and weak generalization ability. Therefore, we suggest a distributed knowledge distillation architec-\nture based on Transformer. Firstly, the multi-attention mechanism is used to give weights to the features,\nfollowed by feed-forward neural networks to extract high-level features that include relevant information,\nand finally neural networks are used to categorize financial fraud. Secondly, for the problem of inconsistent\nfinancial data indicators and unbalanced data distribution focused on different industries, a distributed\nknowledge distillation algorithm is proposed. This algorithm combines the detection knowledge of the multi-\nteacher network and migrates the knowledge to the student network, which detects the financial data of\ndifferent industries. The final experimental results show that the proposed method outperforms other methods\nin terms of F1 score (92.87%), accuracy (98.98%), precision (81.48%), recall (95.45%), and AUC score\n(96.73%) when compared to the traditional detection methods.\nINDEX TERMS Transformer, Knowledge Distillation, Financial fraud detection\nI. INTRODUCTION\nT\nHE number of listed firms is increasing quickly due\nto the ongoing social economy development, and their\nplace in the global economy is vital. However, cases of finan-\ncial fraud are frequent and prohibited, causing great losses\nto the majority of investors and arousing discussions in all\nsectors of society. In China, the number of criminals involved\nin financial counterfeiting activities in 2019 exceeded 961,\nwith a total value of more than US 8 billion [1]. Numer-\nous investors’ faith has been damaged by these instances,\nwhich has had a detrimental impact on the capital markets\nand increased financial market volatility [2], [3]. In order\nto address these counterfeiting issues, the development of\nnew detection methods is imperative. Currently, there are two\nmain means of detecting counterfeiting by listed companies:\none is to audit and analyze the company’s financial data,\nand the other is to detect whether there is any suspicion\nof counterfeiting through big data-driven machine learning\nalgorithms [4]. Manual audits and reviews of publicly traded\ncorporations’ financial statements are examples of traditional\nfinancial analysis techniques, however they are expensive,\ntime-consuming, and prone to error [4]. These methods are\nnot absolute, and as the methods of financial fraud continue\nto evolve, it is difficult for practitioners to detect new patterns\nof fraud. Also, certain anomalies may be legitimate business\npractices, rendering such methods less feasible. Then, big\ndata-driven machine learning algorithms were used to detect\nfinancial fraud, an area where computers were more adept at\ndata analysis than people when dealing with large amounts of\ndata, particularly when it came to high-dimensional features.\nThe effectiveness of machine learning models in financial\nforgery detection has been demonstrated in the literature [5].\nBut forgers continue to innovate and adopt new concealment\nmethods, making it difficult for traditional machine learning\ndetection methods to detect and identify new forgery methods\nin a timely manner, and correlations between data features\nare difficult to be learned by the models. It is difficult for the\nmodel to extract the more critical information for the task at\nhand from the complex and large data features, resulting in\nthe performance of existing counterfeiting detection models\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nYuxuan Xuanet al.:A Distributed Knowledge Distillation Framework for Financial Fraud Detection based on Transformer\nbeing greatly limited.More significantly, by identifying the\nrelationships between features, the attention model can un-\ncover more concealed counterfeiting information and inves-\ntigate more counterfeiting patterns. For example, literature\n[6] proposes a two-level attention model that captures deep\nrepresentations of features from data sample level and feature\nlevel sets, respectively.\nExisting financial fraud detection methods are mostly\nbased on machine learning and deep learning algorithms [4].\nThese techniques pay less attention to the internal correla-\ntions within financial data and instead concentrate on mining\nthe fundamental features of the data. Additionally, different\nindustries may encounter varying challenges in financial data\nfraud, and the internal correlations of financial data features\ndiffer across industries. Furthermore, with the continuous\ngrowth in the scale of financial data, these models become\nincreasingly deep and complex, resulting in issues such as\nmodel bloat and slow inference speed. Therefore, how to\neffectively mine the internal correlation of financial data,\ncompress the model size, and enhance the model’s ability to\ndetect financial data falsification in different industries is a\nnew direction for researchers to explore. To address the above\nproblems, this research suggests a distributed knowledge dis-\ntillation architecture based on Transformer. The method uses\na multi-attention mechanism to extract the internal correlation\nof the data, and then the high-level features that contain the\ninformation related to the financial data are extracted through\na forward neural network, which is combined with the neural\nnetwork to classify the financial data fraud. Secondly, to ad-\ndress the problem of inconsistent financial data indicators and\nunbalanced data distribution focused on different industries,\nand to reduce the complexity of the financial fraud detection\nmodel and improve the accuracy of the model, this paper\nproposes a distributed knowledge distillation algorithm. The\nalgorithm migrates the detection knowledge of the multi-\nteacher network to the student network separately, and the\nstudent network detects the financial data of different indus-\ntries. The final experimental results show that the proposed\nmethod has better F1 score, accuracy, precision, recall, and\nAUC score compared to the traditional detection methods,\nwhich improves the accuracy of financial forgery detection.\nThe following are the primary contributions of our re-\nsearch:\n(1)For financial fraud detection, considering that Trans-\nformer has strong generalization and expressive ability, it is\neasier to adapt to diverse financial data. Therefore, we pro-\npose a financial fraud detection model based on Transformer,\nwhich utilizes the multi-head attention mechanism and feed-\nforward neural network to mine the high-level features that\nincorporate the relevant information of financial data, thus\nimproving the characterization of data relevance.\n(2)To address the problem of inconsistent financial data in-\ndicators and unbalanced data distribution focused on different\nindustries, and to reduce the complexity of the financial fraud\ndetection model and improve the accuracy of the model, this\npaper proposes a distributed knowledge distillation algorithm.\nThe algorithm migrates the detection knowledge of the multi-\nteacher network to the student network separately, and the stu-\ndent network detects the financial data of different industries.\n(3)The proposed distributed network was evaluated on\nthe dataset of the 9th \"TipDM Cup\" listed company finan-\ncial analysis competition. Experimental results demonstrate\nthat our proposed financial fraud detection method based\non Transformer with distributed knowledge distillation out-\nperforms traditional tree models and ensemble models in\nkey performance metrics on the dataset. This confirms the\nfeasibility and effectiveness of our proposed method.\nThe rest of the paper is structured as follows, the second\npart is a review of related research, the third part introduces\nour proposed model for financial fraud detection, the fourth\npart describes the distributed knowledge distillation frame-\nwork for detecting fraudulent data in different industries, and\nthe experimental results are discussed in the fifth part. Finally\nPart VI summarizes the conclusions of this study.\nII. BACKGROUND AND RELATED WORK\nA. TRADITIONAL FINANCIAL FRAUD DETECTION\nMETHODS\nFinancial fraud detection technology can lower investor\nlosses, preserve equity and justice in the trading market, and\nassist the China Securities Regulatory Commission (CSRC)\nin determining if listed businesses are suspected of fraud. Tra-\nditional approaches for determining a listed firm’s involve-\nment in fraudulent operations rely on analyzing financial data,\ninformation from listed firms, and third-party evidence. With\nthe continuous development of science and technology, detec-\ntion methods for fraud have also made significant progress.\nArtificial intelligence technologies driven by big data have\nbeen widely applied and have shown promising results in\nfraud detection. The core idea of artificial intelligence is to\ntrain a model with strong generalization capabilities, sup-\nported by big data, enabling the model to accurately detect\nthe likelihood of listed companies engaging in financial data\nfraud. According to whether the sample data is labeled, these\nmethods can be roughly divided into two categories: super-\nvised learning and unsupervised learning.\nIn a supervised learning approach, the model used for\nfinancial forgery detection can be viewed as a binary classi-\nfication task, i.e., whether the company is a forgery or not,\nand the result is often given in the form of a probability,\nwhere the higher the probability the more likely it is that the\ncompany is a forgery. Many classification algorithms have\nbeen proposed and have achieved good results in various\nindustries. Based on whether the distribution of observed vari-\nables is modeled, supervised learning models can be divided\ninto two categories: discriminative models and generative\nmodels. Generative models include Naive Bayes (NB), Re-\nstricted Boltzmann Machine (RBM), Hidden Markov Model\n(HMM). Discriminative models include Logistic Regression\n(LR), Multilayer Perceptron (MLP), Support Vector Machine\n(SVM), K-Nearest Neighbors (KNN), Maximum Entropy\nModel (ME), Conditional Random Field (CRF), Decision\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nYuxuan Xuanet al.:A Distributed Knowledge Distillation Framework for Financial Fraud Detection based on Transformer\nTree (DT), Random Forest (RF). Such as, in reference [7],\nthe accuracy of four machine learning algorithms—LR, RF,\nDT,CatBoost—is analyzed and compared as the subject of\nfinancial fraud detection is explored through the use of several\nalgorithms. Using a dataset of financial fraud, Liu et al. used\nthe RF technique and contrasted it with other algorithms like\nLR, KNN, DT, and SVM. They discovered that the RF al-\ngorithm had the best interpretability and maximum accuracy\n[8]. Unsupervised learning does not require labeling the data;\nit is similar in nature to a statistical tool that detects anoma-\nlous data to determine if samples that do not belong to the\nmain class are deceptive. Two common types of algorithms\nfor unsupervised learning are clustering and dimensionality\nreduction. The clustering algorithms are K-mean clustering,\nhierarchical clustering, etc., and the dimensionality reduc-\ntion algorithms are Principal Component Analysis(PCA) and\nSingular Value Decomposition(SVD). Such as, reference [9]\nproposed a model framework that separates clusters using\nthe K-means method and compared the performance with\ntwo of the most important financial fraud detection systems.\nReference [10] introduced an unsupervised learning approach\nthat combines Particle Swarm Optimization(PSO) and K-\nMeans clustering, demonstrating better performance in finan-\ncial fraud detection compared to K-Means.\nB. DEEP LEARNING COUNTERFEIT DETECTION METHODS\nClassical machine learning algorithms typically use shal-\nlow models, effective for linearly separable tasks or sim-\nple non-linear tasks. In contrast, deep learning algorithms\nare generally employed for deep models, providing stronger\nnon-linear modeling capabilities and better performance on\nreal-world complex tasks. For tasks with higher complexity\nand deeper concealment, such as financial data fraud detec-\ntion, deep learning algorithms generally outperform machine\nlearning algorithms [4]. For example, Rushin et al. compared\nthe performance of LR, gradient boosting trees, and deep\nlearning in detecting credit card fraud, indicating that deep\nlearning methods outperform the other two approaches [11].\nIn addition, deep learning algorithms can deeply explore\nthe potential connections between data, thereby uncovering\nmore methods for detecting financial fraud and enhancing\nthe effectiveness of detection. For example, the classification\nresults depend on features constructed from domain-specific\nknowledge, without considering other attributes of the data,\nsuch as temporal attribution. Jurgovsky et al. treated fraud\ndetection as a sequence classification task and utilized Long\nShort-Term Memory (LSTM) for predictions.Experimental\nresults show that LSTM effectively improves the accuracy\nof credit card fraud compared to random forest [12]. Zhou\net al. use a graph embedding algorithm to learn topological\nfeatures from financial network graphs and represent them\nas low-dimensional dense vectors. In this way, they utilize\ndeep neural networks to intelligently and efficiently classify\nand predict data samples from large-scale datasets [13]. The\nliterature [14], taking into account the homogeneity of the\ndata structure, proposes a graph learning algorithm capable of\nlearning topological features and transaction amount features\nin financial transaction network graphs. In literature [15], a\nnovel graph neural network (GNN) architecture with a time\nde-biasing constraint based on adversarial loss is proposed.\nThis architecture captures fraud patterns that exhibit fun-\ndamental consistency over time and performs well in fraud\ndetection tasks. In literature [16], a new credit card fraud\ndetection model named CCFD-Net is introduced, featuring a\nhybrid architecture combining 1D-Conv and Residual Neural\nNetwork (Res-net). This model demonstrates good effective-\nness and robustness in credit card fraud detection.\nC. MULTI-TEACHER KNOWLEDGE DISTILLATION\nMETHODS\nThe single-student-multi-teacher distillation paradigm has\nmade significant progress in converting complicated, multi-\nattribute instructor information into lightweight student net-\nworks. Multi-teacher distillation research focuses on design-\ning appropriate distillation strategies for use in instructing\nstudents. In 2017, You et al [17] proposed a framework for\nmulti-teacher distillation. This approach averages the soft\nlabels of logits produced from several teacher models and\nprovides them to student models for learning. Shi et al [18]\nused another way of directly splicing logits of multiple teach-\ners and then performing PCA dimensionality reduction on\nthe face recognition model. Shin [19] extended the multi-\ninstructor-single-student distillation architecture to a visual\nmulti-attribute recognition task of a target, where each in-\nstructor specialises in learning one attribute, and then syn-\nthesises the multi-instructor’s knowledge to transfer it to the\nstudent to achieve the student’s multi-attribute recognition\nlearning. Furthermore, in a recent study, Hailin [20] et al.\nproposed an adaptive multi-instructor knowledge distillation\nstrategy that allows diverse instructor knowledge to be jointly\nutilised to improve student performance. The multi-instructor\nknowledge distillation paradigm proposed in the literature\n[21] empowers students to integrate and capture a variety\nof knowledge from different sources. Although many stud-\nies have used a multi-teacher distillation framework, less\nattention has been paid to the uneven distribution of positive\nand negative samples. In this research, we employ a multi-\nteacher knowledge distillation strategy to aggregate various\ninstructors’ knowledge of financial fraud detection across\nindustries onto a lightweight student model. The goal is to\nenhance the model’s performance in detecting imbalances in\nthe distribution of positive and negative data using a sim-\nple and effective multi-teacher distillation architecture. One\ndistinction between our technique and other multi-teacher\napproaches is that our multi-teacher model learns about fi-\nnancial fraud in different industries separately, whereas our\nstudent network learns about financial fraud in each industry\nfrom all of the teacher models, allowing the model to be\ngeneralized efficiently in the presence of an imbalanced data\ndistribution.\nMachine learning techniques are heavily used in the field\nof financial fraud detection, and graph network-based ap-\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nYuxuan Xuanet al.:A Distributed Knowledge Distillation Framework for Financial Fraud Detection based on Transformer\nproaches have made significant progress in recent years [4].\nHowever, these methods only focus on the topological fea-\ntures and data features of the network, ignoring the dependen-\ncies between data features. Table 1 summarises the existing\nwork related to our problem, compared to other methods,\nthe method proposed in this paper exploits the dependencies\nbetween financial indicators for forgery detection, and uses\nmulti-instructor distributed knowledge distillation to improve\nthe speed of model inference and the generalisation of the\nmodel when the data is unbalanced. And these are not avail-\nable in other models.\nIII. FINANCIAL FRAUD DETECTION MODEL BASED ON\nTRANSFORMER\nA. FINANCIAL FRAUD DETECTION METHODS AND\nPROCESSES\nTransformer is an advanced deep learning model which was\nfirst proposed by Vaswani et al. in 2017 and was initially used\nfor natural language processing tasks [22]. However, due to\nits robust parallelism and expressive capabilities, it has been\nsuccessfully applied to other domains, including the fields of\nimage processing and classification.\nOne of Transformer’s basic features is the self-attention\nmechanism, which allows the model to process all points\nin the input sequence at once rather than step-by-step like\na recurrent neural network or convolutional neural network.\nThe self-attention mechanism enables the model to capture\ncorrelations by assigning different attentional weights to dif-\nferent sections of the input sequence. To better capture var-\nious sorts of relationships, the self-attention mechanism is\nexpanded to several attention heads, each capable of learning\nvaried attention weights. The structure of the Transformer\nencoder is shown in Figure 1. The encoder typically includes\na multi-head attention layer, a feed-forward neural network\nlayer, residual connectivity, and layer normalization. Trans-\nformer are usually made up of multiple encoders and decoders\nstacked on top of each other, and these stacked layers help the\nmodel learn complex feature representations.\nTo enhance the accuracy of data analysis and modeling, the\nfinancial dataset is first preprocessed. Subsequently, multiple\nattention scores are calculated for financial data to obtain\na representation of the correlation between features. These\nmultiple attention scores are then fed through a feedforward\nnetwork to extract higher-level features that integrate relevant\ninformation more comprehensively. Following this, a neural\nnetwork maps these higher-level features to the probability of\nfraud output. Finally, the cross-entropy loss of the samples\nis computed, and the model parameters are updated through\ngradient descent based on the loss value.\nB. MODEL ARCHITECTURE\nThe architecture of the financial fraud detection model based\non Transformer is illustrated in Figure 2. The model consists\nof three modules. The first module is a multi-industry data\nprocessing module. The second module is a Transformer\nEncode Block module, which includes a multi-head attention\nFIGURE 1. Transformer Encode Block.\nmodule and a fully connected feedforward neural network.\nThe feedforward network comprises a linear transformation,\nReLU non-linear activation function, along with a residual\nconnection and layer normalization operation. The third mod-\nule is the output neural network module, containing a linear\nneural network for output and a softmax function for result\nnormalization.\n1) Multi-Head Attention\nThe financial dataset is represented as D = {(Xn, Yn)}N\nn=1,\nwhere the matrix X = {x1, x2, . . . ,xm} represents fi-\nnancial data features. Here, xm is a vector of dimension\ndmodel ,Y = {y1, y2, . . . ,ym|ym ∈ [0, 1]}, where 0 indicates\nno fraud and 1 indicates fraud. For a single sample X,the\nfirst step involves computing the self-attention scores for\nits features. Here, we define three matrices for the scaled\ndot-product operation:Query( Q),Key(K),and Value( V ). Ad-\nditionally, three learnable weight matrices Wq,Wk ,Wv are in-\ntroduced to map each input feature to query, key, and value\nvectors:\nQ = XWq (1)\nK = XWk (2)\nV = XWv (3)\nwhere Q ∈ Rm×d ,K ∈ Rm×d ,V ∈ Rm×d ,Wq ∈ Rm×d ,Wk ∈\nRm×d ,Wv ∈ Rm×d .\nThen, for the query matrix Q, calculate its similarity score\nmatrix S with the key matrix K. To prevent excessively large\nscores that could lead to model gradient explosions, divide\neach score by\n√\nd:\nS = QKT\n√\nd (4)\nwhere S ∈ Rm×m the scores represent the correlation between\neach financial data feature and other features.\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nYuxuan Xuanet al.:A Distributed Knowledge Distillation Framework for Financial Fraud Detection based on Transformer\nTABLE 1. Comparative analysis of methods used to falsify financial statements.\nStudy Method(Acc(%)) Comparison\nAlgorithm (Acc(%))\nFast inference Problems solved Used metrics\nA. Byungdae & S.\nYongmoo(2020) [22]\nModified Random\nForest(MRF)(79.9)\nDT(74.8),RF(78.1),Bagging\nof DTs(78.0),LR(72.0),SVM\n(71,4),ANN(78.5)\nNO A high-performance\nclassification model\nthat can detect four\ntypes of FSFs was\ndeveloped\nAcc,Precision,F1-\nScore\nP. Craja, et al.(2020)\n[23]\nGPT-2+Attn(69.3) HAN(84.6),ANN(90.5),SVM\n(82.8),XGB(90.8),RF(87.4) NO Combined financial\nratios and\nmanagement\ncommentary\ninformation for\nfinancial statement\nfraud detection\nAcc,AUC,F1-\nScoreSensitivity\nW. Xiuguo & D.\nShengyong(2022) [3]\nRNN,CNN,LSTM(94.9),G-\nRU(94.6)\nSVM(91.0),XGB(90.0),ANN\n(90.3),CNN(91.65),LSTM(94.8),\nGRU(94.6),Transformer(94.4)\nNO Combines textual\ninformation and\nfinancial statement\ndata to perform testing\nAcc,AUC,F1-\nScoreSensitivity\nR. Li, et al.(2023) [24] A graph-learning\nalgorithm(TA-\nStruc2Vec)(AUC(82.7))\nDeepWalk(AUC(49.6)),Node-\n2vec(AUC(73.4),Struc2Vec(A-\nUC(80.1)),Line(AUC(57.3)),G-\nraphCosis(AUC(78.0)),CARE-\nGNN(AUC(79.5)),RioGNN(A-\nUC(81.9))\nNO Feature aggregation\nproblem for\nstructurally similar\ndistant nodes\nPrecision,Recall,F1-\nScore,AUC\nH. Zhou, et al.(2021)\n[13]\nGraph embedding\nalgorithm+DNN(Nod-\ne2Vec)(70)\nDeepWalk(60),SVM(30) NO Effective and realistic\nmining of topological\nfeatures of association\nnetwork graphs\nPrecision, Recall\n,F1-Score, F2-Score\nJ. Geng, et al.(2023)\n[25]\nBased on dual\nadversarial\nlearning(92.2)\nOCSVM(88.8),OCNN(90.6)\n,COPOD(89.4),DIF(65.6)\n,DSVDD(75.1),RCA(90.0)\nNO Utilizes intermediate\nfeatures and improves\ndetection performance\nwhen data are\nunbalanced\nAcc, Precision,\nRecall,\nF1-Score,MCC\nOurs Transformer+KD(98.9)\nLogReg(84.0),SVM linear(84.3),\nTree(98.4),RF(88.6),XG(92.1),\nAda(87.6)\nYes Detection based on\ninternal data\ncorrelation. Improved\nmodel performance in\nthe face of unequal\ndata distribution.\nAcc,Precision,F1-Sc-\nore,AUC,Recall,MA-\nE,RMSE,MCC\nANN-Artificial Neural Network, HAN-Heterogeneous graph attention network, XGB-eXtreme Gradient Boosting, GRU-Gated Recurrent Unit, OCSVM-\nOneClass SVM,Ada-Adaptive Boosting, MCC-Matthews Correlation Coefficient Acc-Accuracy,AUC-area under the ROC curveMAE-Mean Absolute E-\nrror,RMSE-Root Mean Square Error.\nFinally, normalize the scores using the softmax function\nand multiply the normalized correlation scores by the value\nmatrix V to obtain the self-attention scores O for financial\ndata features:\nO = softmax(S)V (5)\nwhere O ∈ Rm×d .\nThe self-attention scores for financial data features can be\nsummarized as formula (6):\nAttention (Q, K, V ) = softmax\n\u0012QKT\n√\nd\n\u0013\nV . (6)\nThe multi-head attention mechanism enables the model\nto capture richer correlations among financial data features,\nfacilitating a more in-depth exploration of patterns related to\ndata falsification. Multi-head attention involves performing\nthe self-attention mechanism multiple times, essentially hav-\ning n individuals focusing attention on different positions of\nfinancial data features. This approach increases the likelihood\nof detecting crucial information related to data falsification:\nMultiHeadAtt (Q, K, V ) = Concat\n(head1, head2, . . . ,headh) · Wo\n(7)\nwhere head = Attention (Qi, Ki, Vi),i ∈ {1, . . . ,h},Wo ∈\nRhd×d .\n2) Feedforward Neural Network\nThe multi-head attention scores obtained from formula (7)\nundergo a residual connection and layer normalization oper-\nation. The residual connection addresses the training issues\nof deep networks by adding the output to the original input,\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nYuxuan Xuanet al.:A Distributed Knowledge Distillation Framework for Financial Fraud Detection based on Transformer\nFIGURE 2. The architecture of the financial fraud detection model based on Transformer.\nenhancing the network’s representational capacity [26]. Layer\nnormalization normalizes all inputs to have a mean of 0 and\na standard deviation of 1. This helps alleviate the problem of\ninternal covariate shift in neural network training, providing\nmore stable and faster training:\nLayerNorm (X + MultiHeadAtt (Q, K, V )) . (8)\nSubsequently, the multi-head attention scores, after the resid-\nual connection and layer normalization, undergo further pro-\ncessing through two linear transformations and a ReLU acti-\nvation function. This step aims to extract higher-level features\nwith richer contextual information:\nFFN (X) = max (0, XW1 + b1) W2 + b2 (9)\nwhile the linear transformations at different positions in the\nencoder are the same, the parameters between layers are\ndistinct.\nIn order to prevent overfitting, we introduce dropout into\nthe output of each fully connected layer to ensure the model’s\ngeneralization. Dropout involves randomly discarding each\nneuron with a probability p. For the neurons that are not dis-\ncarded, their values are scaled by the reciprocal of the dropout\nprobability, maintaining the expected value of the data. By\ntraining different network structures in each iteration, dropout\nintroduces variability, eliminating and weakening the inter-\ndependence among neuron nodes, thereby enhancing the\nmodel’s ability to generalize internal correlations in financial\ndata. The dropout computation process is as follows formula\n(10).\ndroput (X) =\n\u001a 0 , p\nX\n1−p , 1 − p (10)\n3) Output Neural Network\nAfter the financial data goes through the stacked encoder,\nwe map and output the high-level features X, which are\nextracted by the last encoder and contain internal correlation\ninformation, through a linear layer. We normalize the output\nusing the softmax function. The normalization calculation is\nshown in formula (11):\nY pre = softmax\n\u0000\nW · XT + b\n\u0001\n(11)\nwhere Y pre ∈ R1×2 is the probability distribution vector, W\nis the neural network weight matrix, and b is the bias vector.\n4) Overall loss calculation\nThe financial dataset D = {(Xn, Yn)}N\nn=1 is passed into\nthe Transformer-based financial fraud detection model. After\nextracting high-level features related to the data, the model\nmaps the samples to predicted label values f (W , X). The true\nlabel values Yn and the predicted label values f (W , X) are\nthen used to calculate the cross-entropy loss through formula\n(12):\nfcls (W , Xn, Yn) = −[Yn · log (f (W , Xn))\n+ (1− Yn) · log (1− f (W , Xn))] (12)\nwhere W represents the model’s parameter matrix, f (W , X)\nrepresents the mapping of feature X through the model’s\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nYuxuan Xuanet al.:A Distributed Knowledge Distillation Framework for Financial Fraud Detection based on Transformer\nparameter matrix W , and its value is the probability of no\nfraud.\nThe model utilizes data samples for training to update the\nmodel parameters W . Here, we provide the general formula\nfor parameter updates:\nW = W + η · ∂Fcls (W )\n∂W (13)\nwhere η represents the learning rate, and Fcls (W ) represents\nthe total loss function of the financial dataset D. Its calculation\nformula is as follows:\nFcls (W ) = 1\nN\nX\nXn,Yn∈D\nfcls (W , Xn, Yn). (14)\nIV. DISTRIBUTED KNOWLEDGE DISTILLATION\nDETECTION FRAMEWORK\nOn the one hand, due to the presence of various challenges\nrelated to financial data manipulation in different industries,\nthere exist distinct characteristics and internal correlations\nin the financial data of different industries. Moreover, there\nare significant differences in the financial data indicators\nthat different industries focus on. Therefore, it is challenging\nto use a universal model to detect financial data with such\nsubstantial variations. On the other hand, traditional models\nsuffer from issues such as complex structures, deep model\ndepths, and slow inference speeds, making it difficult to\ndeploy them in practical application scenarios. Based on the\nabove problem considerations, this paper uses a distributed\narchitecture to train multiple teacher detection models for\nmultiple industries. And a distributed knowledge distillation\nalgorithm is proposed to migrate the detection knowledge\nfrom the multi-teacher network to the lightweight student\nnetwork separately. On the one hand, the detection model is\ncompressed to adapt to practical application scenarios, and on\nthe other hand, the generalisation ability of the model in the\ncase of unbalanced data distribution is improved.\nThe distributed knowledge distillation detection frame-\nwork, as shown in Figure 3, is illustrated as follows. Firstly,\ndatasets from various industries are prepared, and these\ndatasets are utilized to train teacher models. Subsequently,\nuntrained student models with simpler structures than the\nteacher models are prepared. A knowledge distillation algo-\nrithm is used so that the knowledge from the multi-teacher\nmodel is migrated separately to the student network, which\nfinally tests the financial data from different industries.\nA. MULTI-TEACHER MODEL\nThe knowledge distillation algorithm is a model compression\ntechnique. It involves transferring knowledge from a large\nmodel (usually referred to as the teacher model) to a smaller\nmodel (typically known as the student model), with the aim of\nretaining the performance of the teacher model on a relatively\nsmaller scale student model [27].\nThe teacher model adopts the Transformer-based financial\nfraud detection model mentioned in Section 3. The multi-\nindustry financial dataset is represented as the set I =\n{D1, D2, . . . ,Dm} where Dm = {(Xn, Yn)}N\nn=1 represents\nthe financial dataset of industries such as manufacturing and\ntransportation. The Multi-teacher model is trained using the\ncollection of multi-teacher financial datasets. The perfor-\nmance of the Multi-teacher model is further optimized by\nadjusting hyperparameters. The training of the Multi-teacher\nmodel is illustrated in Algorithm 1.\nB. STUDENT MODEL\nFor a classification task, the final output of the model is\nthe probabilities for each class, which are referred to as soft\ntargets. The true labels for each sample are called hard targets.\nThe difference with hard targets is that soft targets not only\ninform us about the most likely class for a sample but also\nprovide probabilities for other classes, indicating that soft\ntargets contain more information than hard targets. Therefore,\nwhen training the Multi-teacher model, we use hard targets.\nThe predictions obtained from training the teacher network on\na sample can convey more information to the student network.\nConsequently, we can use the soft targets from the teacher\nnetwork to guide the training of the student network.\nThe student network adopts a smaller Transformer-based\nfinancial fraud detection model with fewer parameters. For\nthe financial dataset D = {(Xn, Yn)}N\nn=1 let Z(t) ∈ RB×C\nand Z(s) ∈ RB×C represent the logits output by the teacher\nnetwork and student network, respectively, where B is the\nbatch size, and C is the number of categories. Y ∈ [0, 1]\nrepresents the hard targets for the samples. After applying\nthe softmax function to the outputs Z(t) ∈ RB×C and Z(t) ∈\nRB×C of the teacher and student networks, the probability\ndistributions range from 0 to 1. If we find that the relative\nsizes between the categories are not sufficiently distinct, we\nintroduce a distillation temperature T . A higher T makes\nthe relative sizes between the categories more pronounced.\nThe introduction of T involves dividing the original softmax\nvalues by T . In theory, as T increases,the distillation effect\nimproves, but excessively large T can cause the relative sizes\nbetween categories to disappear. Therefore, it’s necessary to\nchoose an appropriate value for T . The distillation process is\nrepresented as formula (15):\nsoftmax (Z/T ) = exp (zi/T )P\nj exp (zj/T ) (15)\nwhere Z = {z1, z2, . . . ,zn}.\nThe guidance of the teacher model in training the student\nmodel involves two steps. The first step is to compute the\ndistillation loss. This involves using the distillation formula\n(16) and formula (17)to calculate the soft targets P(t) and\nP(s) from the outputs Z(t) and Z(s) of the teacher and student\nnetworks, respectively. Then, the KL divergence loss between\nthese soft targets is calculated using formula (18):\nP(t) = softmax (Y pre\nT /T ) (16)\nP(s) = softmax (Y pre\nS /T ) (17)\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nYuxuan Xuanet al.:A Distributed Knowledge Distillation Framework for Financial Fraud Detection based on Transformer\nFIGURE 3. Distributed Knowledge Distillation Framework for Financial Data Detection.\nLKD = T 2\nB\nXB\ni=1\nXC\nj=1\nlog\n \np(s)\ni,j\np(t)\ni,j\n!\n. (18)\nThe second step is to compute the student loss. This involves\nusing a temperature softmax distiller (with T=1) on the output\nZ(s) of the student network to calculate soft targets P,and then\ncalculating the cross-entropy loss between P(t=1) and the hard\ntargets Yn from the financial data using formula (19):\nLcls = 1\nB\nXB\ni=1\nXC\nj=1\n−[Yi,j · log (Pi,j)\n+ (1− Yi,j) · log (1− Pi,j)].\n(19)\nThe final knowledge distillation loss is obtained by taking\nthe weighted sum of both the distillation loss and the student\nloss:\nLtr = α · Lcls + β · LKD. (20)\nwhere α and β are weight coefficients, determining the con-\ntribution of each loss term in the final knowledge distillation\nloss.\nThe model utilizes data samples for training to update\nmodel parameters W.The specific algorithm for model train-\ning is shown in Algorithm 2. Here, we provide the general\nformula for parameter updates:\nW = W+η · ∂Ltr\n∂W (21)\nwhere η represents the learning rate.\nV. EXPERIMENT\nIn this section, we first describe the structure of the dataset.\nSubsequently, we compare the performance metrics of the\nteacher model and the student model. We then compare the\nstudent model with other machine learning algorithms, fol-\nlowed by visualization and parameter analysis.\nA. DATASET DESCRIPTION\nThe dataset used in this experiment is from the 9th \"TipDM\nCup\" Financial Analysis Competition for Listed Companies.\nAll listed companies in the dataset come from 19 different\nindustries. Among them, manufacturing companies signif-\nicantly outnumber companies from other industries, with\n2,667 companies, while the distribution of companies in other\nindustries is relatively even, totaling only 1,496. Due to the\nuneven distribution of data across different industries, we\ndivide the entire dataset into two categories: manufacturing\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nYuxuan Xuanet al.:A Distributed Knowledge Distillation Framework for Financial Fraud Detection based on Transformer\nAlgorithm 1Multi-Teacher model training algorithm\nHyperparameters: Enter feature dimension d;Bulk\nattention nhead=6;Number of feedforward neurons\ndim=1024;Random dropout dropout=0.2;Encode layer\nnumber layers=2;Learning rate η =0.001;Number of\niterations T =100;Training data amount N1;Batch size\nn1=32;Optimizer=Adam.\nInput: Multi-industry financial data set collection I =\n{D1, D2, . . . ,Dm},where Dm = {(Xn, Yn)}N\nn=1.\nOutput: Teacher model convergence parameters W (t).\n1: Random initialization W (t) ← N (0, 1);\n2: Random sorting of different industries in the collection I;\n3: while t ≤ T do\n4: for n = 1 : N1/n1 do\n5: Select batch samples from data set I (Xn, Yn);\n6: for k = 1 : layers do\n7: for i = 1 : nhead do\n8: From the formula (1), (2), (3) calculate Qi, Ki,\nVi according to Xn;\n9: From the formula (6) calculate headi according\nto Qi, Ki, Vi;\n10: end for\n11: Calculate the multi-head attention score M based\non headi according to formula (7);\n12: Calculate the residual network and layer normal-\nization L based on X and M according to formula\n(8);\n13: Feed the feedforward neural network FFN (L)\nbased on formula (9), and apply random dropout\nto each fully connected layer according to for-\nmula (10);\n14: Calculate the residual network and layer normal-\nization to obtain the encoder output ¯X based on\nformula (8);\n15: Feed the output back to the input, and stack the\nencoder:X = ¯X;\n16: end for\n17: Apply the linear output layer to the output of the last\nencoder based on formula (11) to obtain the output\nresult Y pre;\n18: Calculate the cross-entropy loss for the dataset\nbased on formula (14);\n19: Update the model parameters W based on formula\n(13);\n20: end for\n21: end while\n22: return Output the convergence parameters W (t) of the\nteacher model.\nand other industries. We separately train student models for\nthe manufacturing industry and other industries. These two\nmodels serve as subsystems in a distributed framework. The\nexperiment involves training on 70% of the data, with the\nremaining 30% used as a validation set.\nAlgorithm 2Student Model Training Algorithm\nHyperparameters: Enter feature dimension d;Bulk\nattention nhead=2;Number of feedforward neurons\ndim=1024;Random dropout dropout=0.2;Encode layer\nnumber layers=2;Learning rate η=0.001;Distillation\ntemperature Tem=7;Number of iterations\nT =100;Training data amount N1;Batch size\nn1=32;Optimizer=Adam.\nInput: Multi-industry financial data set collection I =\n{D1, D2, . . . ,Dm},where Dm = {(Xn, Yn)}N\nn=1.\nOutput: Multi-industry student network convergence pa-\nrameters W (s) =\nn\nw(s)\n1 , w(s)\n2 , . . . ,w(s)\nn\no\n, where w(s)\nn Ex-\npress the network convergence parameters in a certain\nindustry.\n1: Random initialization W (s) ← N (0, 1);\n2: for i = 1 : m do\n3: Select the industry dataset Di from the collection I;\n4: Sorting the sample of the industry dataset Di randomly\nsort;\n5: while t ≤ T do\n6: for n = 1 : N1/n1 do\n7: Select batch samples from data set Di (Xn, Yn);\n8: Calculate the output Z(t)\nn of the teacher network\nbased on Xn and the teacher network parameters\nW (t) from algorithm 1;\n9: Calculate the output Z(s)\nn of the student network\nbased on Xn and the student network parameters\nw(s)\nn ;\n10: According to equations (16) and (17),distill the\nclassification results Z(t)\nn and Z(s)\nn through a\ndistillation process with distillation temperature\nTem = t , resulting in distilled outputs P(t)\nn and\nP(s)\nn ;\n11: According to equation (15),distill the classifica-\ntion result Z(s)\nn of the student network through a\ndistillation process with a distillation temperature\nTem = 1, obtaining the distilled output Pn;\n12: Calculate the final loss Ln for dataset Di based on\nformulas (18),(19) and (20);\n13: Finally, update the parameters w(s)\nn of the student\nnetwork based on the final loss Li using formula\n(21);\n14: end for\n15: end while\n16: end for\n17: return Multi-industry student network convergence pa-\nrameters W (s) =\nn\nw(s)\n1 , w(s)\n2 , . . . ,w(s)\nn\no\n, where w(s)\nn ex-\npress the network convergence parameters in a certain\nindustry.\nB. TEACHER MODEL AND STUDENT MODEL\nPERFORMANCE COMPARISON ANALYSIS\nOur experiment was conducted on the hardware platform\nof 13th Gen Intel(R) Core(TM) i9-13900KF 3.00 GHz and\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nYuxuan Xuanet al.:A Distributed Knowledge Distillation Framework for Financial Fraud Detection based on Transformer\nTABLE 2. Summary of the analyzed data sets.\nDataset No.of\nNegatives\nNo.of\nPositives\nNo.of\nFeatures\nOther\nindustries 6661 89 85\nManufacturing\nindustry 11219 91 85\nTABLE 3. Comparison of Evaluation Metrics Between Teacher and\nStudent Models.\nDataset Method Accura1 Recall Precision F1 score\nOther\nindustries\nTeacher\nModel\n0.9754 0.9251 0.6236 0.8142\nStudent\nModel\n0.9898 0.9545 0.8148 0.9287\nManufact-\nuring\nindustry\nTeacher\nModel\n0.9738 0.9012 0.5036 0.7213\nStudent\nModel\n0.9883 0.9270 0.6774 0.8765\nNVIDIA GeForce RTX3060 Ti. The primary configuration\nenvironment for the experiment includes Python 3.9.1, torch\n2.0.1, numpy 1.22.4, and pandas 2.1.1. All machine learning\nalgorithms were implemented using the third-party library\nScikit-Learn. The Transformer-based financial fraud detec-\ntion model was constructed using the PyTorch deep learning\nframework.\nFor the final detection criteria, we utilize the following\nmetrics:\nprecision = TP\nTP + FP (22)\nrecall = TP\nTP + FN (23)\nf 1_score = 2 · recall · precision\nrecall + precision (24)\naccuracy = TP + TN\nTP + FP + TN + FN (25)\nwhere TP,TN,FP and FN,represent true positives,true nega-\ntives,false positives, and false negatives, respectively.\nAfter training the proposed model on the training set, eval-\nuation was conducted using the test set to assess the detection\nperformance and speed of both the teacher model and the\nstudent model. As shown in Table 3, in terms of detection\nperformance, the student model that learned distillation had\naverage Accura values of 98.98% and 98.83% on the other in-\ndustries and manufacturing datasets, respectively, compared\nto only 97.54% and 97.38% for the instructor model, implying\nthat the student model outperformed the instructor model in\nterms of detection accuracy. The average Recall on the dataset\nOther Industries and Manufacturing was 92.51% and 90.12%\nfor the teacher model, and 95.45% and 92.70% for the student\nmodel, suggesting that the student model outperforms the\nteacher model at proper detection.\nTABLE 4. Inference Time Comparison Between Teacher and Student\nModels.\nDataset Method Avg.Time/µs\ncpu gpu\nOther\nindustries\nTeacher\nModel\n825.5 121.4\nStudent\nModel\n187.3 31.2\nManufact-\nuring\nindustry\nTeacher\nModel\n1048.7 262.3\nStudent\nModel\n256.4 40.7\nIn terms of model inference speed, as shown in Table 4,\non the dataset from other industries, the teacher model has\naverage inference times of 825.5 µs and 121.4 µs on CPU and\nGPU, respectively. In comparison, the student model has av-\nerage inference times of 187.3 µs and 31.2 µs, which are faster\nby 638.2 µs and 90.2 µs, respectively. On the manufacturing\nindustry dataset, the teacher model has average inference\ntimes of 1048.7 µs and 262.3 µs on CPU and GPU, while the\nstudent model has average inference times of 256.4 µs and\n40.7µs. The student model is faster by 792.3 µs and 221.3 µs,\nrespectively.From the table, it can be observed that the in-\nference speed of the student model is generally faster than\nthat of the teacher model. This is because the student model\nhas fewer parameters and a simpler structure than the teacher\nmodel, leading to faster inference speed. Additionally, the\ninference speed on GPU is faster compared to CPU, as GPUs\nare better suited for matrix operations. The experimental re-\nsults of comparing the performance of the teacher model and\nthe student model show that after multi-teacher distributed\nknowledge distillation, the student model improves detection\nperformance, generalization ability, and inference speed more\nthan the teacher network does.\nC. COMPARISON RESULTS OF STUDENT MODEL\nDETECTION PERFORMANCE WITH OTHER ALGORITHMS\nIn order to further evaluate the performance of the student\nmodel, we compared the proposed method with advanced\nmachine learning algorithms, including Log Reg [28], SVM\nlinear [29], DT [30], RF [31], XGBoost [32], and Adaboost\n[33].\nTo begin, this study uses the MAE and RMSE to assess\neach model’s error performance on the test data. Figures 4\nand 5 demonstrate a comparison examination of MAE and\nRMSE, with the findings indicating that our suggested model\nhas lower MAE and RMSE than the other models.\nSecond, MCC is used to evaluate the classification model’s\nperformance; the MCC can provide a more accurate per-\nformance assessment in unbalanced datasets. The closer the\nMCC metric is to 1 indicates better model classification\nperformance. The comparison study of MCC is displayed in\nFigure 6, and the findings reveal that our proposed model has\na higher MCC than the other models, implying that our model\nhas better classification performance in unbalanced datasets.\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nYuxuan Xuanet al.:A Distributed Knowledge Distillation Framework for Financial Fraud Detection based on Transformer\nFIGURE 4. Comparative analysis of MAE values of the proposed method\nwith other models.\nFIGURE 5. Comparative analysis of RMSE values of the proposed\nmethod with other models.\nFIGURE 6. Comparative analysis of MCC values of the proposed method\nwith other models.\nThe performance was assessed based on accuracy, pre-\ncision, recall, and F1 score. As indicated in Table 5, our\nproposed method achieved the highest accuracy of 98.98%\npercent on other sectors and 98.83% percent on manufactur-\nTABLE 5. Comparison of Evaluation Metrics Between Student Model\nand Machine Learning Model.\nDataset Method Accura1 Recall Precision F1 score\nOther\nindustries\nLogReg 0.8401 0.7297 0.1268 0.5605\nSVM\nlinear\n0.8430 0.7379 0.1311 0.5648\nTree 0.9840 0.9314 0.7241 0.8920\nRF 0.8860 0.6999 0.1525 0.5861\nXG 0.9217 0.8389 0.2727 0.6790\nAda 0.8763 0.8087 0.1827 0.6125\nStudent\nModel\n0.9898 0.9545 0.8148 0.9287\nManufact-\nuring\nindustry\nLogReg 0.8296 0.7119 0.0717 0.5167\nSVM\nlinear\n0.8147 0.7177 0.0688 0.5101\nTree 0.9880 0.9336 0.6666 0.8756\nRF 0.9293 0.7628 0.1679 0.6121\nXG 0.9072 0.8722 0.1657 0.6135\nAda 0.8836 0.8601 0.1358 0.5853\nStudent\nModel\n0.9883 0.9270 0.6774 0.8765\ning industries. Log Reg and linear SVM achieved the lowest\naccuracy in other industries and manufacturing, with values\nof 84.01% and 81.47%, respectively. Our proposed method\nachieved the highest recall in other industries at 95.45%,\nwhile the Tree algorithm slightly surpassed our model in\nmanufacturing with a recall of 93.36%. Our proposed method\nalso achieved the highest precision, with values of 81.48%\nand 67.74% for other industries and manufacturing, respec-\ntively. The Tree algorithm slightly lagged behind our pro-\nposed method, with precision values of 72.41% and 66.66%\nfor other industries and manufacturing. Furthermore, our pro-\nposed method obtained the highest F1 scores, with values of\n92.87% and 87.65% for other industries and manufacturing,\nrespectively. The F1 scores of the Tree algorithm were lower\nthan our proposed method, with values of 89.20% and 87.56%\nfor other industries and manufacturing.\nThe ROC curve is a measure of the model’s overall clas-\nsification performance, and the area under the ROC curve is\nthe AUC; the closer the AUC value is to one, the better the\nmodel’s correct classification performance, and the closer it\nis to zero, the worse the surface model’s correct classification\nperformance. Figures 7 and 8 display the ROC curves of the\nproposed method and other machine learning algorithms on\nother and manufacturing industry datasets. The ROC curves\nof the proposed method are positioned closest to the top-\nleft corner of the graphs, indicating superior performance of\nthe proposed fraud detection model on both datasets. These\nresults demonstrate the effectiveness of our proposed method.\nPrecision and recall are important metrics for comparing\nclassifier performance. Precision-recall (PR) curves can be\nplotted based on precision and recall, and the quality of a\nsystem can be judged based on these curves. The PR curve is\nplotted with recall on the x-axis and precision on the y-axis.\nFigures 9 and 10 clearly illustrate the PR curves of our pro-\nposed method and other machine learning algorithms. The PR\ncurve of the proposed method is positioned in the upper-right\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nYuxuan Xuanet al.:A Distributed Knowledge Distillation Framework for Financial Fraud Detection based on Transformer\nFIGURE 7. The proposed method and AUC curves compared to other\nmachine learning algorithms on datasets from various industries.\nFIGURE 8. The proposed method and AUC curves for a manufacturing\ndataset compared to other ML algorithms.\ncorner of the graphs, indicating good performance on both\ndatasets. Additionally, the PR curve of the proposed method\nis higher than the PR curves of other algorithms, suggesting\nthat, compared to other machine learning algorithms.\nVI. CONCLUSION\nThe detection of fraudulent financial data in listed companies\nis of significant importance for safeguarding the interests of\nshareholders and investors. This paper proposes a distributed\nknowledge distillation framework based on Transformer for\ndetecting fraudulent financial data in listed companies. Ex-\nperimental validation was conducted using the dataset from\nthe 9th \"TipDM Cup\" Financial Analysis Competition for\nListed Companies. The performance of the proposed method\nwas evaluated by comparing it with other advanced machine\nlearning algorithms, including logistic regression, linear sup-\nport vector machine, decision tree, random forest, XGBoost,\nFIGURE 9. The proposed method and precision-recall curves on\ndatasets from various industries compared to other ML algorithms.\nFIGURE 10. The proposed method and precision-recall curves on a\nmanufacturing dataset compared to other ML algorithms.\nand Adaboost. The experimental results demonstrate that the\nproposed method outperforms other machine learning algo-\nrithms, achieving the highest performance in terms of AUC,\naccuracy, precision, recall, and F1 score.\nREFERENCES\n[1] C. Defang and L. Baichi, ‘‘Svm model for financial fraud detection,’’\nNortheastern Univ., Natural Sci. , vol. 40, p. 295–299, 2019.\n[2] T. Shahana, V . Lavanya, and A. R. Bhat, ‘‘State of the art in financial\nstatement fraud detection: A systematic review,’’ Technol. Forecasting\nSocial Change, vol. 192, p. 122527, 2023.\n[3] W. Xiuguo and D. Shengyong, ‘‘An analysis on financial statement fraud\ndetection for chinese listed companies using deep learning,’’ IEEE Access,\nvol. 10, pp. 22516–22532, 2022.\n[4] M. N. Ashtiani and B. Raahemi, ‘‘Intelligent fraud detection in financial\nstatements using machine learning and data mining: A systematic literature\nreview,’’IEEE Access, vol. 10, pp. 72504–72525, 2022.\n[5] M. El-Bannany, A. H. Dehghan, and A. M. Khedr, ‘‘Prediction of finan-\ncial statement fraud using machine learning techniques in uae,’’ in 2021\n18th International Multi-Conference on Systems, Signals Devices (SSD) ,\npp. 649–654, 2021.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nYuxuan Xuanet al.:A Distributed Knowledge Distillation Framework for Financial Fraud Detection based on Transformer\n[6] R. Cao, G. Liu, Y . Xie, and C. Jiang, ‘‘Two-level attention model of\nrepresentation learning for fraud detection,’’ IEEE Trans. Comput. Social\nSyst., vol. 8, no. 6, pp. 1291–1301, 2021.\n[7] A. Singh, A. Singh, A. Aggarwal, and A. Chauhan, ‘‘Design and imple-\nmentation of different machine learning algorithms for credit card fraud\ndetection,’’ in 2022 International Conference on Electrical, Computer,\nCommunications and Mechatronics Engineering (ICECCME) , pp. 1–6,\n2022.\n[8] C. Liu, Y .-C. Chan, S. H. Alam, and H. Fu, ‘‘Financial fraud detection\nmodel: Based on random forest,’’ Econometrics: Econometric Model Con-\nstruction, 2015.\n[9] H. Shivraman, U. Garg, A. Panth, A. Kandpal, and A. Gupta, ‘‘A model\nframe work to segregate clusters through k-means method,’’ in 2022 Second\nInternational Conference on Computer Science, Engineering and Applica-\ntions (ICCSEA), pp. 1–6, 2022.\n[10] N. Sharma and V . Ranjan, ‘‘Credit card fraud detection: A hybrid of pso and\nk-means clustering unsupervised approach,’’ in 2023 13th International\nConference on Cloud Computing, Data Science Engineering (Confluence) ,\npp. 445–450, 2023.\n[11] G. Rushin, C. Stancil, M. Sun, S. Adams, and P. Beling, ‘‘Horse race\nanalysis in credit card fraud—deep learning, logistic regression, and gra-\ndient boosted tree,’’ in 2017 Systems and Information Engineering Design\nSymposium (SIEDS), pp. 117–121, 2017.\n[12] J. Jurgovsky, M. Granitzer, K. Ziegler, S. Calabretto, P.-E. Portier, L. He-\nGuelton, and O. Caelen, ‘‘Sequence classification for credit-card fraud\ndetection,’’Expert Syst. Appl. , vol. 100, pp. 234–245, 2018.\n[13] H. Zhou, G. Sun, S. Fu, L. Wang, J. Hu, and Y . Gao, ‘‘Internet financial\nfraud detection based on a distributed big data approach with node2vec,’’\nIEEE Access, vol. 9, pp. 43378–43386, 2021.\n[14] R. Li, Z. Liu, Y . Ma, D. Yang, and S. Sun, ‘‘Internet financial fraud\ndetection based on graph learning,’’ IEEE Trans. Comput. Social Syst. ,\nvol. 10, no. 3, pp. 1394–1401, 2023.\n[15] A. Singh, A. Gupta, H. Wadhwa, S. Asthana, and A. Arora, ‘‘Temporal\ndebiasing using adversarial loss based gnn architecture for crypto fraud\ndetection,’’ in 2021 20th IEEE International Conference on Machine\nLearning and Applications (ICMLA) , pp. 391–396, 2021.\n[16] X. Liu, K. Yan, L. Burak Kara, and Z. Nie, ‘‘Ccfd-net: A novel deep learn-\ning model for credit card fraud detection,’’ in 2021 IEEE 22nd International\nConference on Information Reuse and Integration for Data Science (IRI) ,\npp. 9–16, 2021.\n[17] S. You, C. Xu, C. Xu, and D. Tao, ‘‘Learning from multiple teacher net-\nworks,’’Proceedings of the 23rd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , pp. 1285–1294, 2017.\n[18] W. Shi, G. Ren, Y . Chen, and S. Yan, ‘‘Proxylesskd: Direct knowl-\nedge distillation with inherited classifier for face recognition,’’ ArXiv,\nvol. abs/2011.00265, 2020.\n[19] M. Shin, ‘‘Semi-supervised learning with a teacher-student network for\ngeneralized attribute prediction,’’ in European Conference on Computer\nVision, pp. 509–525, 2020.\n[20] H. Zhang, D. Chen, and C. Wang, ‘‘Adaptive multi-teacher knowledge\ndistillation with meta-learning,’’ 2023 IEEE International Conference on\nMultimedia and Expo (ICME) , pp. 1943–1948, 2023.\n[21] A. Amirkhani, A. Khosravian, M. Masih-Tehrani, and H. Kashiani, ‘‘Ro-\nbust semantic segmentation with multi-teacher knowledge distillation,’’\nIEEE Access, vol. 9, pp. 119049–119066, 2021.\n[22] B. An and Y . Suh, ‘‘Identifying financial statement fraud with decision\nrules obtained from modified random forest,’’ Data Technol. Appl., vol. 54,\npp. 235–255, 2020.\n[23] P. Craja, A. Kim, and S. Lessmann, ‘‘Deep learning for detecting financial\nstatement fraud,’’ Decis. Support Syst. , vol. 139, p. 113421, 2020.\n[24] R. Li, Z. Liu, Y . Ma, D. Yang, and S. Sun, ‘‘Internet financial fraud\ndetection based on graph learning,’’ IEEE Trans. Comput. Social Syst. ,\nvol. 10, no. 3, pp. 1394–1401, 2023.\n[25] J. Geng and B. Zhang, ‘‘Credit card fraud detection using adversarial learn-\ning,’’ in 2023 International Conference on Image Processing, Computer\nVision and Machine Learning (ICICML) , pp. 891–894, 2023.\n[26] E. Orhan, ‘‘Skip connections as effective symmetry-breaking,’’ ArXiv,\nvol. abs/1701.09175, 2017.\n[27] H. Hong and H. Kim, ‘‘Feature distribution-based knowledge distillation\nfor deep neural networks,’’ in 2022 19th International SoC Design Confer-\nence (ISOCC), pp. 75–76, 2022.\n[28] D. Varmedja, M. Karanovic, S. Sladojevic, M. Arsenovic, and A. Anderla,\n‘‘Credit card fraud detection - machine learning methods,’’ in 2019 18th\nInternational Symposium INFOTEH-JAHORINA (INFOTEH) , pp. 1–5,\n2019.\n[29] T. Priyaradhikadevi, S. Vanakovarayan, E. Praveena, V . Mathavan,\nS. Prasanna, and K. Madhan, ‘‘Credit card fraud detection using machine\nlearning based on support vector machine,’’ in 2023 Eighth International\nConference on Science Technology Engineering and Mathematics (ICON-\nSTEM), pp. 1–6, 2023.\n[30] C.-C. Lin, A.-A. Chiu, S. Y . Huang, and D. C. Yen, ‘‘Detecting the financial\nstatement fraud: The analysis of the differences between data mining\ntechniques and experts’ judgments,’’ Knowledge-Based Systems. , vol. 89,\npp. 459–470, 2015.\n[31] V . Arora, R. S. Leekha, K. Lee, and A. Kataria, ‘‘Facilitating user authoriza-\ntion from imbalanced data logs of credit cards using artificial intelligence,’’\nMob. Inf. Syst. , vol. 2020, pp. 8885269:1–8885269:13, 2020.\n[32] L. Torlay, M. Perrone-Bertolotti, E. Thomas, and M. Baciu, ‘‘Machine\nlearning–xgboost analysis of language networks to classify patients with\nepilepsy,’’Brain Inf., vol. 4, pp. 159 – 169, 2017.\n[33] P. Yu and X. Liu, ‘‘Construction and application of bid fraud prediction\nmodel based on adaboost algorithm,’’ in 2022 2nd International Confer-\nence on Electronic Information Engineering and Computer Technology\n(EIECT), pp. 292–295, 2022.\n[34] T. Zhang and S. Gao, ‘‘Graph attention network fraud detection based on\nfeature aggregation,’’ in 2022 4th International Conference on Intelligent\nInformation Processing (IIP) , pp. 272–275, 2022.\n[35] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Neural\nInformation Processing Systems , 2017.\nYUXUAN TANGis currently pursuing Bachelor’s\nDegree in Accounting from the School of Ac-\ncounting, Southwestern University of Finance and\nEconomics, Chengdu, Sichuan, China. Her current\nresearch interests include financial big data anal-\nysis, financial fraud detection, credit card fraud\ndetection, machine learning, deep learning.\nZHANJUN LIU received the Ph.D. degree in cir-\ncuits and systems from Chongqing University,\nChongqing, China, in 2018. He is currently a Pro-\nfessor with the School of Communication and In-\nformation Engineering, Chongqing University of\nPosts and Telecommunications, China. His cur rent\nresearch interests are network intelligence, big data\nanalysis, deep learning.\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387841\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}