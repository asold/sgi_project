{
    "title": "Frozen Pretrained Transformers as Universal Computation Engines",
    "url": "https://openalex.org/W4283794074",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2095776409",
            "name": "Kevin Lu",
            "affiliations": [
                "Meta (Israel)",
                "Berkeley College",
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A2008277149",
            "name": "Aditya Grover",
            "affiliations": [
                "Meta (Israel)",
                "UCLA Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A243981275",
            "name": "Pieter Abbeel",
            "affiliations": [
                "University of California, Berkeley",
                "Berkeley College"
            ]
        },
        {
            "id": "https://openalex.org/A710089344",
            "name": "Igor Mordatch",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2095776409",
            "name": "Kevin Lu",
            "affiliations": [
                "Berkeley College",
                "Meta (Israel)",
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A2008277149",
            "name": "Aditya Grover",
            "affiliations": [
                "Meta (Israel)",
                "UCLA Health"
            ]
        },
        {
            "id": "https://openalex.org/A243981275",
            "name": "Pieter Abbeel",
            "affiliations": [
                "Berkeley College",
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A710089344",
            "name": "Igor Mordatch",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6630631473",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W6769911694",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W6755752558",
        "https://openalex.org/W2161072217",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6666761814",
        "https://openalex.org/W2622049782",
        "https://openalex.org/W6759579507",
        "https://openalex.org/W2795851676",
        "https://openalex.org/W6767663227",
        "https://openalex.org/W6763868836",
        "https://openalex.org/W2154642048",
        "https://openalex.org/W3143253488",
        "https://openalex.org/W2999481648",
        "https://openalex.org/W2887258823",
        "https://openalex.org/W1511715654",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3103682594",
        "https://openalex.org/W4294568686",
        "https://openalex.org/W2971307358",
        "https://openalex.org/W3102060274",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W3007700590",
        "https://openalex.org/W3023252952",
        "https://openalex.org/W2951433247",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2970350231",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2766736793",
        "https://openalex.org/W4394664678",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W4287585714",
        "https://openalex.org/W2950735465",
        "https://openalex.org/W4297808394",
        "https://openalex.org/W2963168530",
        "https://openalex.org/W2963211188",
        "https://openalex.org/W2963281204",
        "https://openalex.org/W2963393721",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W2889498145",
        "https://openalex.org/W2626792426",
        "https://openalex.org/W2118706537",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3095583226",
        "https://openalex.org/W4288088856"
    ],
    "abstract": "We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.",
    "full_text": "Pretrained Transformers as Universal Computation Engines\nKevin Lu,1,2 Aditya Grover,2,3 Pieter Abbeel,1 Igor Mordatch4\n1 UC Berkeley, 2 Facebook AI Research, 3 UCLA, 4 Google Brain\nkzl@fb.com\nAbstract\nWe investigate the capability of a transformer pretrained on\nnatural language to generalize to other modalities with mini-\nmal ﬁnetuning – in particular, without ﬁnetuning of the self-\nattention and feedforward layers of the residual blocks. We\nconsider such a model, which we call a Frozen Pretrained\nTransformer (FPT), and study ﬁnetuning it on a variety of\nsequence classiﬁcation tasks spanning numerical computa-\ntion, vision, and protein fold prediction. In contrast to prior\nworks which investigate ﬁnetuning on the same modality as\nthe pretraining dataset, we show that pretraining on natural\nlanguage can improve performance and compute efﬁciency\non non-language downstream tasks. Additionally, we perform\nan analysis of the architecture, comparing the performance of\na random initialized transformer to a random LSTM. Com-\nbining the two insights, we ﬁnd language-pretrained trans-\nformers can obtain strong performance on a variety of non-\nlanguage tasks.\nBit Memory Bit XOR ListOps MNIST CIFAR-10 CIFAR-10 LRA Homology\nTest Accuracy\n100 100\n38\n98\n72\n39\n13\n100 100\n38\n99\n70\n42\n9\n61\n50\n17\n99.5\n74\n12 12\nPerformance on Multimodal Sequence Benchmarks\nFrozen Pretrained Transformer Full Transformer Full LSTM\nFigure 1: A frozen language-pretrained transformer (FPT)\n– without ﬁnetuning the self-attention and feedforward lay-\ners – can achieve strong performance compared to a trans-\nformer fully trained from scratch on a downstream modality\non literature benchmarks (Tay et al. 2020; Rao et al. 2019).\nWe show results on diverse classiﬁcation tasks (see Section\n2.1): numerical computation (Bit Memory/XOR, ListOps),\nimage classiﬁcation (MNIST, CIFAR-10, LRA), and pro-\ntein fold prediction (Homology). We also show results for\na fully-trained from-scratch LSTM as a baseline. Our code\nis available at: github.com/kzl/universal-computation\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1 Introduction\nThe transformer architecture (Vaswani et al. 2017) has\nshown broad successes in deep learning, serving as the back-\nbone of large models for tasks such as modeling natural lan-\nguage (Brown et al. 2020), images (Dosovitskiy et al. 2020),\nproteins (Jumper et al. 2021), and multimodal tasks compris-\ning of both images and text (Lu et al. 2019; Radford et al.\n2021). Inspired by these successes, we seek to explore the\ngeneralization capabilities of a transformer in transferring\nfrom one modality to another.\nClassical approaches to sequence processing used recur-\nrent neural network (RNN) approaches (Rumelhart, Hin-\nton, and Williams 1985; Hochreiter and Schmidhuber 1997).\nIn contrast, transformers utilize self-attention layers to ex-\ntract features across tokens of a sequence, such as words\n(Vaswani et al. 2017) or image patches (Dosovitskiy et al.\n2020). Furthermore, it has become common practice to train\nlarge models on unsupervised objectives before ﬁnetuning\nor evaluating zero-shot generalization on a downstream task.\nHowever, the downstream tasks that have been studied are\ngenerally restricted to the same modality as the original\ntraining set: for example, train GPT (Radford et al. 2018) on\na large language corpus, and ﬁnetune on a small task-speciﬁc\ndataset. Our goal in this work is to investigate ﬁnetuning on\nmodalities distinct from the training modality.\nWe hypothesize that transformers – namely the self-\nattention layers – can be pretrained on a data-rich modal-\nity (i.e. where data is plentiful, such as a language corpus)\nand identify feature representations that are useful for arbi-\ntrary data sequences, enabling downstream transfer to dif-\nferent modalities. In particular, we seek to investigate what\npretrained language models (LMs) are capable of in terms\nof generalizing to other modalities with sequential structure.\nTo investigate this hypothesis, we take a transformer\nmodel pretrained on natural language data, GPT-2 (Radford\net al. 2019), and ﬁnetune only the linear input and output\nlayers, as well as the positional embeddings and layer norm\nparameters. These decisions are made to highlight the pa-\nrameters already in the language model, and not for per-\nformance purposes. We call this model a Frozen Pretrained\nTransformer (FPT). On a range of tasks across a variety of\nmodalities – including numerical computation, image clas-\nsiﬁcation, and protein fold prediction – FPT displays com-\nparable performance to training the entire transformer from\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n7628\nscratch, matching reported benchmarks for these tasks (Fig-\nure 1). Additionally, we ﬁnd FPT models also converge\nfaster during training. Our results suggest the self-attention\nlayers learned by a language model may perform computa-\ntion that is universal across modalities. Through a series of\nexperiments, we investigate what contributes to cross-modal\ntransfer by isolating sub-components of these models.\n2 Methodology\n2.1 Tasks\nWe evaluate on a diverse set of classiﬁcation tasks represen-\ntative of different modalities. In particular, we are interested\nin if language models are inherently capable of universal\ncomputation, by which we mean the ability to learn repre-\nsentations for predictive learning across diverse modalities.\nBit Memory. Similar to the task proposed by (Miconi,\nStanley, and Clune 2018), we consider a bit memory task\nwhere the model is shown 5 bitstrings each of length 1000.\nAfterwards, the model is shown a masked version of one of\nthe bitstrings, where each bit is masked with probability0:5,\nand the model is tasked with producing the original bitstring.\nThe bitstrings are broken up into sequences of length 50, so\nthat the models are fed 120 tokens of dimension 50.\nBit XOR. Similar to the bit memory task, the model is\nshown 2 bitstrings of length 5, where the model must predict\nthe element-wise XOR. The bitstrings are shown 1 bit at a\ntime, so the models are fed 10 tokens of dimension 1.\nListOps. Taken from (Nangia and Bowman 2018; Tay\net al. 2020), the model is shown a sequence of list op-\nerations (ex. [ MAX 4 3 [ MIN 2 3 ] 1 0 ]) and\ntasked with predicting the resulting output digit (ex.4). This\ntask evaluates the ability of a model to parse mathematical\nexpressions and evaluate over a long context. The model is\nshown 1 token at a time, so the models are fed 512 tokens of\ndimension 15.\nMNIST. On MNIST, the model must classify a handwrit-\nten digit from a 32 \u0002 32 black-and-white image. The tokens\ngiven to the model are 4 \u0002 4 image patches, so the models\nare fed 64 tokens of dimension 16.\nCIFAR-10. On CIFAR-10 (Krizhevsky et al. 2009), the\nmodel will be given 4 \u0002 4 image patches, so the models are\nfed 64 tokens of dimension 16.\nCIFAR-10 LRA. This is a modiﬁed version of the above\ntask taken from the Long Range Arena (LRA) benchmark\nwhere the images are converted to grayscale and ﬂattened\nwith a token length of 1 (Tay et al. 2020). As a result, the\ninput sequence consists of 1024 tokens of dimension 1. This\ntask is more challenging than vanilla CIFAR-10 as the mod-\nels must learn patterns over a signiﬁcantly longer sequence\nlength and have minimal spatial inductive bias.\nRemote homology detection. In this task, we are inter-\nested in predicting the fold for a protein, represented as an\namino acid sequence. We use the datasets provided by TAPE\n(Rao et al. 2019; Fox, Brenner, and Chandonia 2013; Hou,\nAdhikari, and Cheng 2018), where the train/test split is gen-\nerated by holding out certain evolutionary groups. Note that\nwe do not pretrain on Pfam (El-Gebali et al. 2019), which\nis common in other works. There are 20 common and 5\nInput\nEmbedding\nMulti-Head\nAttention+ Feed\nForward\nOutput\nLayer\nPositional\nEmbeddings\nL frozen self-attention blocks\nAdd &\nLayer Norm\nAdd &\nLayer Norm\nx L\nFigure 2: Frozen Pretrained Transformer (FPT). The self-\nattention & feedforward layers are frozen.\nuncommon amino acids (25 different types of inputs), and\nthere are 1195 possible labels to predict. We only consider\nsequences of length less than 1024 for simplicity. The mod-\nels are thus fed up to 1024 tokens of dimension 25.\n2.2 Architecture\nThe architecture we use is summarized in Figure 2. We seek\nto highlight the internal computation already present in the\nlanguage model, and so freeze the main components of the\nmodel. Thus we consider ﬁnetuning the following parame-\nters of a pretrained GPT-2 model (Radford et al. 2019):\n• Output layer: it is crucial to ﬁnetune the output layer\nsince we transfer to a completely new task. We use the\nsimplest instantiation of an output network – a single lin-\near layer applied to the last output token output by the\ntransformer – in order to highlight that almost all the com-\nputation is being performed by the frozen transformer.\n• Input layer:it is important to reinitialize a new input layer\nsince we are reading in a new modality; in essence, we\nare learning how to query the transformer. This contrasts\nwith prior unsupervised embedding evaluation techniques,\nsuch as linear probing – due to the change in modality, we\nshould train the input layer as well, and evaluate if the\nfrozen intermediate transformer model performs effective\ncomputation. Again, we use a linear layer to minimize the\namount of computation outside the transformer.\n• Layer norm parameters: as common in other ﬁnetuning\nworks (Rebufﬁ, Bilen, and Vedaldi 2017; Houlsby et al.\n2019), we also ﬁnetune the layer norm afﬁne parameters,\nwhich adapt to the downstream task statistics.\n• Positional embeddings: we generally also see a small\nbeneﬁt to ﬁnetuning the positional embeddings, which are\nsimilar to the input layer parameters.\nGiven the cheap linear scaling of these parameters, the\nparameter counts of large transformer models are domi-\nnated by the self-attention and feedforward layers, which\ngrow quadratically with the sequence length and layer width.\nFor the base CIFAR-10 model with 124M parameters, these\ncome out to approximately 0:086% of the network. We fur-\nther ablate the importance of each parameter in Section 3.11.\nNote that, crucially, all communication between tokens in\nthe model are frozen. The data in each datapoint is chunked\ninto discrete tokens (bits, image patches, amino acids, etc.),\nand can only reference each other via the frozen attention\nconnections, which are not trained; additionally, neither the\noutput nor the input layers are connected to multiple tokens.\nOur key investigation is to analyze the computation that is\nalready inherent in the language model, and hence we do a\nminimal amount of computation that is learned on the down-\nstream modality.\n7629\nModel Bit Memory XOR ListOps MNIST C10 C10 LRA Homology\nFPT 100% 100% 38.4% 98.0% 68.2% 38.6% 12.7%\nRandom 75.8% 100% 34.3% 91.7% 61.7% 36.1% 9.3%\nBit 100% 100% 35.4% 97.8% 62.6% 36.7% 7.8%\nViT 100% 100% 37.4% 97.8% 72.5% 43.0% 7.5%\nTable 1: Investigation of pretraining modality. Test accuracy of language-pretrained (FPT) vs randomly initialized (Random) vs\nBit Memory pretraining (Bit) vs pretrained Vision Transformer (ViT) models. The transformer is frozen.\n3 Empirical Evaluations\nIn this section, we review the results demonstrating transfer\nfrom language to other modalities, and seek to better un-\nderstand why this occurs and what enables this transfer. All\nmodel sizes are the base model size (12 layers, 768 hidden\ndimension), unless stated otherwise; due to the number of\nexperiments, we use one seed for each reported accuracy.\n3.1 Can Pretrained Language Models Transfer to\nDifferent Modalities?\nWe investigate if the self-attention and feedforward layers\n– the main body – of a pretrained transformer can be ap-\nplied to a classiﬁcation problem in a different modality with-\nout ﬁnetuning. To do this, we apply our base procedure as\ndescribed above, where the input embedding layer, output\nreadout layer, and layer norm parameters are ﬁnetuned.\nWe minimally tune the FPT models, using the standard\npretrained GPT-2 and default PyTorch learning rate of10−3 .\nWe compare to fully training a transformer from scratch,\nwithout pretraining, using the same learning rate and batch\nsize; we swept over layer sizes of 3 or 12, as some of the\nfully trained models beneﬁted from smaller size due to op-\ntimization challenges. Additionally, we compare to state-of-\nthe-art from literature when available (full transformer on\nListOps, CIFAR-10 LRA, and Remote Homology; LSTM\non Remote Homology), as these works performed more in-\ndepth sweeps on hyperparameters.\nOur results are shown in Figure 1. We ﬁnd that across\nall seven tasks considered, FPT achieves comparable perfor-\nmance to the fully trained transformer benchmark results.\nWe believe these results support the idea that these mod-\nels are learning representations and performing computation\nthat is agnostic to the modality. We also note that both trans-\nformer variants signiﬁcantly outperform LSTMs on some\ntasks, particularly ListOps and CIFAR-10 LRA, which have\nlong sequence lengths of 512 and 1024, respectively.\nFurthermore, unlike some other works utilizing trans-\nformers for vision, we use minimal spatial bias to empha-\nsize the universal sequential aspect of the problem – for in-\nstance, we do not interleave self-attention and convolution\nlayers. Note that we also do not use 2D positional embed-\ndings (or other domain-speciﬁc techniques), hence provid-\ning very weak inductive prior to the model. Our reasoning\nfor these decisions is to evaluate the ability of transformers\nto work on arbitrary sequential tasks.\n3.2 What is the Importance of the Pretraining\nModality?\nWe now compare pretraining on language to other pretrain-\ning methods for base model sizes:\n• Random initialization (Random): initialization of the\nfrozen transformer parameters randomly using the default\ninitialization choices for GPT-2, i.e. without pretraining.\n• Bit memory pretraining (Bit): pretraining from scratch\non the Bit Memory task and then freezing the parameters\nbefore transferring. This allows the transformer to gain su-\npervision working with arbitrary bit strings and perform-\ning memory/denoising on independent inputs.\n• Image pretraining (ViT): Vision Transformer (Dosovit-\nskiy et al. 2020) pretrained on ImageNet-21k (Deng et al.\n2009). Note ViT does not have an autoregressive mask like\nGPT-2; however, we found FPT with a BERT (Devlin et al.\n2018) backbone performs very similarly to GPT-2, so this\ndistinction is not very important.\nThese experiments highlight the signiﬁcance of pretrain-\ning – as opposed to simply the transformer architecture –\nand compare language to other methods of supervision. Our\nresults are shown in Table 1. Although the random trans-\nformers can achieve surprisingly strong accuracies, there is\na considerable gap to using natural language pretraining,\nsuch as in MNIST, where random transformers achieve sim-\nilar performance to a linear classiﬁer on top of raw features\n(92%). Thus we believe that while the transformer architec-\nture might be naturally conducive to these evaluations, the\nattention mechanisms used to transfer may be nontrivial and\nnot fully speciﬁed by the architecture. We also ﬁnd that, in\naddition to performance beneﬁts, language pretraining im-\nproves convergence compared to the randomly initialized\ntransformer (see Section 3.4).\nPretraining on bit memory improves performance com-\npared to the random models, but still lags behind training on\nnatural language data. Furthermore, measured by gradient\nsteps, all models converge faster than the randomly initial-\nized transformers (more details in Section 3.4), indicating\nthat all modes of pretraining improve upon random initial-\nization even without considering accuracy.\nAdditionally, while freezing a vision transformer yields\nbetter improvements on CIFAR-10, pretraining on images is\nnot uniformly better; e.g., ViT is worse on protein classiﬁca-\ntion. One hypothesis is that protein sequences are structured\nlike language, in terms of discrete units of information with\na “grammar” (Saar et al. 2021), so transfer from language to\nproteins may be more natural.\n7630\nModel Bit Memory XOR ListOps MNIST CIFAR-10 C10 LRA Homology\nTrans. 75.8% 100% 34.3% 91.7% 61.7% 36.1% 9.3%\nLSTM 50.9% 50.0% 16.8% 70.9% 34.4% 10.4% 6.6%\nLSTM∗ 75.0% 50.0% 16.7% 92.5% 43.5% 10.6% 8.6%\nTable 2: Test accuracy of frozen randomly initialized transformers vs frozen randomly initialized LSTM models. Frozen LSTMs\nperform very poorly. LSTM∗ adds additional architecture improvements to match the transformers (see “+ Pos” in Table 4).\n3.3 How Important is the Transformer vs LSTM\nArchitecture?\nIn Section 3.2 we found the transformer architecture can al-\nready be fairly effective in this regime, even with only ran-\ndom parameters. In this section, we consider using a random\nLSTM architecture instead of the transformer, allowing us to\nconsider the raw effect of architecture and ablating pretrain-\ning. Like FPT, we ﬁnetune the input, output, and layernorm\nparameters for the LSTMs.\nOur results are shown in Table 2. “LSTM” refers to\na 3-layer “standard” LSTM with a hidden dimension of\n768, matching standard implementations of LSTMs, without\nresidual connections or positional embeddings. We include\nthis comparison to represent the traditional method more\nfaithfully, but add these additional architectural components\nbelow. This matches the width of the FPT models, but not\nthe depth or total parameter count (note that LSTMs also do\nnot have positional embeddings). In the same style of FPT\nand GPT-2, we do not use a bidirectional LSTM. We ﬁnd\nthat the self-attention architecture already serves as an ef-\nfective inductive bias for universal computation, improving\nsigniﬁcantly over the recurrent LSTM model and compris-\ning most of the improvement in test accuracy from random\nLSTM to FPT.\nTo better isolate the contribution of attention, we add ad-\nditional features to standard LSTMs. We compare the 3-\nlayer “standard” LSTM to a 12-layer “standard” LSTM,\nmatching the depth of the FPT networks. Under these model\nchoices, we report the performance of a frozen random 3-\nlayer vs 12-layer LSTM in Table 3. Naively, the 12-layer\nmodel is much worse than the 3-layer model, hinting that\nthere is some loss of information by repeated LSTM layers.\nLayers ListOps MNIST C10 C10 LRA\n12 16.2% 11.7% 10.8% 10.4%\n3 16.8% 70.9% 34.4% 10.4%\nTable 3: Test accuracy of frozen randomly initialized “stan-\ndard” LSTMs with a hidden dimension of 768. The 12-layer\nLSTM achieves only near-trivial performance.\nWe also experiment with ablating other architectural im-\nprovements included with the transformer architecture in Ta-\nble 4. Once residual connections (He et al. 2016) are added,\nthe 12-layer LSTM makes up a lot of the performance drops,\nhinting that residual connections could make up for loss of\ninformation from the LSTM layers which otherwise linearly\ncombine the features. We also add positional embeddings,\nwhich ﬁnishes bridging the gap between standard LSTM\nimplementations and the transformer. Even with these addi-\ntional beneﬁts, the LSTM still performs worse. Note that the\nﬁnal 12-layer LSTM has about the same number of trainable\nparameters as the transformer.\nModel ListOps MNIST CIFAR-10 C10 LRA\nLSTM 16.2% 11.7% 10.8% 10.4%\n+ Skip 16.8% 70.9% 34.4% 10.4%\n+ Pos 16.7% 92.5% 43.5% 10.6%\nTrans. 34.3% 91.7% 61.7% 36.1%\nTable 4: Test accuracy of 12-layer frozen randomly initial-\nized LSTMs with architecture modiﬁcations to match trans-\nformers: residual connections (Skip) and positional embed-\ndings (Pos). LSTM is worse in all settings.\n3.4 Does Language Pretraining Improve\nCompute Efﬁciency Over Random\nInitialization?\nWe investigate compute efﬁciency by considering the num-\nber of gradient steps to converge for FPT vs random trans-\nformer models, shown in Table 5. We generally ﬁnd FPT\nconverges faster, which indicates language pretraining can\nyield compute beneﬁts for non-language tasks. While ran-\ndom transformer models achieve decent test accuracies, in\nparticular when compared to random LSTMs, there is still a\nconsiderable gap in the compute efﬁciency compared to us-\ning pretraining. Note that bit memory pretraining introduced\nin Section 3.2 generally falls between the two models, and\nnotably is 6\u0002 slower than FPT on Bit XOR, which is signif-\nicantly better than random.\nModel Memory XOR ListOps C10 LRA\nFPT 1 \u0002 104 5 \u0002 102 2 \u0002 103 3 \u0002 105\nRandom 4 \u0002 104 2 \u0002 104 6 \u0002 103 6 \u0002 105\nSpeedup 4\u0002 40\u0002 3\u0002 2\u0002\nTable 5: Approximate gradient steps until convergence for\npretrained (FPT) vs randomly initialized (Random) models.\nLanguage pretraining converges faster in terms of gradient\nsteps (also translates to wall-clock time).\n7631\n3.5 Do the Frozen Attention Layers Attend to\nModality-Speciﬁc Tokens?\nWe investigate if FPT attends to semantically meaningful\npatterns in the data. We plot the attention weights (i.e. the\nvalues of the softmax of query-key dot product) from the\nﬁrst layer. We show the results in Figures 3 and 4 for the bit\ntasks. Note GPT-2 is autoregressive, so the upper right cor-\nner of the attention mask is zeroed out. On these tasks, FPT\nyields an interpretable attention pattern despite not training\nthe self-attention layers themselves. We did not ﬁnd easily\ninterpretable patterns on the other tasks.\nFigure 3: On Bit XOR, the model must produce the element-\nwise XOR of two bitstrings presented sequentially (bits 0-4\nare the ﬁrst bitstring, 5-9 are the second). FPT attends posi-\ntionally to the two bits that are XOR’ed by the output token.\nFigure 4: On Bit Memory, the model must return one of ﬁve\nstrings (inputs 0-99) given a masked version of one of the\nstrings (inputs 100-119). Each token is 50 bits. FPT learns\nto attend to the correct string based on ﬁnding similarity to\nthe inputs, not relying solely on position as in Bit XOR.\nWe also include the attention map for Bit XOR using a\nrandomly initialized transformer (which also solves the task)\nin Figure 5. This model also learns to exploit the diagonal\npattern, although the strength is a little weaker. This indi-\ncates that while the random transformer still learns to solve\nthe task, it learns a less semantically interpretable/strong at-\ntention pattern.\nFigure 5: A transformer with frozen randomly initialized\nself-attention layers also learns to correlate the two diago-\nnal elements on Bit XOR, although the magnitude of the\ndiagonals is lower.\n3.6 Does Performance Scale With Model Size?\nWe evaluate the efﬁcacy of adding more parameters to these\nmodels on CIFAR-10. Most of the additional parameters are\nin the transformer layers and are trained during the natural\nlanguage pretraining phase. Our results for pretrained and\nrandom models are in Table 6. Unlike fully training a trans-\nformer, which exhibits more overﬁtting and divergence dur-\ning training with larger models, increasing model size stably\nincreases the capacity of the models. This result indicates\nour observations and results are likely to scale as we move\ntowards larger models and higher-data regimes.\nModel Size Trained Params FPT Random\nSmall (Base) 106K 68.2% 61.7%\nMedium 190K 69.8% 64.0%\nLarge 300K 72.1% 65.7%\nTable 6: CIFAR-10 test accuracy of larger FPT models.\n3.7 Can Performance Be Attributed Simply to\nBetter Statistics for Initialization?\nIn this section, we ablate taking the layer-wise mean and\nstandard deviation from the pretrained model and using it to\ninitialize a random transformer, in order to ablate if a bet-\nter initialization scheme via an “oracle” standard deviation\ncan recover the performance of FPT. Note that the GPT-2\ninitialization scheme initializes parameters as Gaussian; tra-\nditionally, the standard deviation is 0:02 by default.\nWe show the results using this initialization scheme in Ta-\nble 7 (note that all of the weights, biases, layer norm, and\npositional embeddings are initialized – both mean and vari-\nance – in this fashion). This yields better results on most\ntasks (more in Supplementary Material), but does poorly on\nCIFAR-10. As a result, we believe the beneﬁts of language\npretraining cannot be recovered with a simple better initial-\nization scheme, although future work in transformer initial-\nization could yield different results.\nInit Memory ListOps C10 C10 LRA\nFPT 100% 38.4% 68.2% 38.6%\nStats 100% 37.4% 56.5% 33.1%\nDefault 75.8% 34.3% 61.7% 36.1%\nTable 7: Test accuracy when initializing parameters with\npretrained weights (FPT) vs randomly initializing accord-\ning to the mean and variance of the pretrained transformer\n(Stats) vs defaul random initialization (Default).\n3.8 Can We Train a Transformer by Only\nFinetuning the Output Layer?\nWe consider using FPT solely for naive feature extraction for\nlinear classiﬁcation, where we ﬁx a randomly initialized in-\nput layer and freeze all parts of the model except for the out-\nput. Note that this resembles resevoir computing/echo state\n7632\nnetworks (see Section 4 for discussion). The model evaluates\non every example in the training set once, caches the fea-\ntures, and then we train a linear output layer. This enables\nsubsequent epochs after the ﬁrst to run extremely quickly,\nbut does not easily handle dropout/data augmentations, and\nscales well in terms of number of epochs, but not in dataset\nsize. Note that this is mathematically equivalent to linear\nclassiﬁcation. Our results are shown in Table 8. Although we\nﬁnd speedups extremely signiﬁcant and they obtain nontriv-\nial performance, performance signiﬁcantly degrades and the\nmodels also exhibit overﬁtting (likely due to lack of regular-\nization; unlike the training of FPT, dropout is not applied).\nTask Speedup Output Only FPT\nListOps 500\u0002 32.8% 38.4%\nCIFAR-10 LRA 500\u0002 24.7% 38.6%\nTable 8: Training only the output layer as a linear regression\nproblem. Speedup refers to wall clock time per epoch.\n3.9 How Does Model Depth Affect Token Mixing?\nOne interesting question is the importance of the depth of\nthe transformer for generating representions which “mix” to-\nkens: for instance, if there is only one layer and the parame-\nters are random, it is unlikely for the tokens to be mixed well,\nwhereas if there are many layers, there are many chances for\nthe tokens to mix and form interesting representations use-\nful for downstream tasks. We investigate this on ListOps by\nconsidering pretrained vs random models, where we only\ntake the ﬁrst X layers of the 12-layer pretrained model (i.e.\nfor X=3, we use the ﬁrst 3 layers of the pretrained GPT-2\nmodel and perform classiﬁcation from those hidden states).\nAdditionally, to maximally highlight the importance of the\npretrained layers, we randomly initialize the input layer, and\ndo not train the input or positional parameters.\nWith ﬁnetuning layernorm. We ﬁrst investigate this\nquestion with ﬁnetuning the layernorm parameters (i.e. we\nﬁnetune only the output layer and the layernorm parame-\nters). Results are shown in Table 9. Both models are unable\nto do well with only one layer, but the pretrained model\nperforms signiﬁcantly better than the random model at 2\nlayers, indicating that while the difference in performance at\n12 layers is relatively small, there is a great beneﬁt to using\npretrained layers for when considering a small number of\nlayers in that the tokens are “mixed” faster.\nNumber of Layers Pretrained Random\n1 17% 17%\n2 36% 16%\n6 38% 35%\nTable 9: Test accuracy on Listops while varying model\ndepth and ﬁnetuning layernorm parameters. Pretrained lay-\ners “mix” tokens faster, performing better at low depths.\nWithout ﬁnetuning layernorm. We now investigate this\nquestion without ﬁnetuning the layernorm parameters, and\nonly ﬁnetuning the output parameters, as in the reservoir\ncomputing setup in Section 3.8. Note this is equivalent to lin-\near classiﬁcation. This setting is the most challenging since\nall processing that is able to mix tokens is done by either ran-\ndom or pretrained parameters, and we are only able to train\na linear layer on top of the output of the last token; as a re-\nsult, the only token mixing that is done is performed entirely\nby the pretrained self-attention layers. Results are shown in\nTable 10. The random model is poor even for a large number\nof layers, while the pretrained model can still do reasonably\nwell, even though it requires more layers than before.\nNumber of Layers Pretrained Random\n1 12% -\n3 18% -\n6 33% -\n12 33% 17%\n24 - 17%\nTable 10: Test accuracy on Listops while varying model\ndepth and only training output parameters. Even for a large\nnumber of layers, the random model performs poorly.\n3.10 Can Training More Parameters Improve\nPerformance?\nOur focus in this work was primarily to investigate if\nand how efﬁcient, general-purpose pretraining can trans-\nfer across modalities. However, for practical applications, it\nwould naturally be better to choose a more specialized ﬁne-\ntuning scheme or add more trainable parameters, to try and\nmaximize performance.\nOn CIFAR-10, we experiment with additionally ﬁnetun-\ning the last attention and all the feedforward layers, shown\nin Table 11. Generally these naive approaches can yield bet-\nter performance, we are optimistic about the possibility of\nsmarter universal training methods improving performance\nin future work, such as with adapters (Houlsby et al. 2019).\nTask Base (FPT) + All FF + Last Attn\nCIFAR-10 68.2% 76.6% 80.0%\nTable 11: Test accuracy on CIFAR-10 when ﬁnetuning ad-\nditional parameters. In addition to FPT, if we ﬁnetune the\nfeedforward layers and the last self-attention layer, we can\nachieve 80% accuracy.\n3.11 Which Parameters of the Model Are\nImportant To Finetune?\nWe now run ablations for only ﬁnetuning select parameters\nof the pretrained GPT-2 to see which parameters are most\nsensitive. Our results are in Table 12; full results are in the\nSupplementary Material. Similar to a study of random CNNs\n7633\nby (Frankle, Schwab, and Morcos 2020), we generally ﬁnd\nthe layer norm parameters to be most important.\nTask output only + LN + input + pos\nMemory 76% 94% 100% 100%\nXOR 56% 98% 98% 100%\nListOps 15% 36% 36% 38%\nMNIST 23% 96% 98% 98%\nCIFAR-10 25% 54% 60% 68%\nC10 LRA 17% 39% 39% 39%\nHomology 2% 9% 10% 13%\nTable 12: Ablation by successively adding certain parame-\nters to the list of ﬁnetuned parameters.\n4 Related Work and Discussion\nMultimodal transformers. Transformers (Vaswani et al.\n2017) were ﬁrst used successfully for natural language pro-\ncessing (Radford et al. 2018; Devlin et al. 2018; Rad-\nford et al. 2019; Brown et al. 2020). In recent years, they\nhave also been shown to be effective architectures for other\nmodalities. Work speciﬁcally tackling multimodal tasks in-\nclude (Kaiser et al. 2017), who showed a single model could\nlearn a variety of multimodal tasks with an attention archi-\ntecture. ViLBERT (Lu et al. 2019) and CLIP (Radford et al.\n2021) jointly learn over images and text, embedding each\nmodality with distinct transformers. Our work is most sim-\nilar to DALL-E (Ramesh et al. 2021), which uses a sin-\ngle transformer to embed both the image and text modali-\nties, which we consider to be generating a “universal latent\nspace” that projects any type of input into a single latent\nspace. Such a latent space would be useful for a model that\ncould learn from many sources of supervision.\nTransformers in transfer settings. There are also many\nworks looking at transformers speciﬁcally in the context\nof in-modality transfer, such as ViT for vision (Dosovit-\nskiy et al. 2020), T5 for language (Raffel et al. 2019), and\nUDSMProt for protein sequences (Strodthoff et al. 2020).\n(Hernandez et al. 2021) do a thorough investigation of trans-\nfer with language pretraining, notably showing transfer from\nEnglish to Python, which they consider to be reasonably\ndistanced from English; many works have also looked at\ntransferring from one langauge to another (Artetxe, Ruder,\nand Yogatama 2019; Ponti et al. 2019). Similar to our\nwork, (Papadimitriou and Jurafsky 2020) investigate trans-\nfer for LSTMs between modalities including code, differ-\nent languages, and music, ﬁnding that pretraining on “non-\nlinguistic data with latent structure” can transfer to language,\nﬁnding grammatical structure in a modality to be impor-\ntant, although we generally investigate the other direction,\nexplore more distanced modalities, and highlight the trans-\nformer architecture.\nGlobal workspace theory. Linear probing is a common\ntechnique for evaluating the embeddings learned by an un-\nsupervised model (Donahue, Kr¨ahenb¨uhl, and Darrell 2016;\nOord, Li, and Vinyals 2018; Chen et al. 2020), which is rea-\nsonable when you ﬁnetune on the same modality as the pre-\ntrained one. However, when ﬁnetuning on a different modal-\nity, as in our setting, we have to reframe this notion of gener-\nalizable embedding quality – instead of only ﬁnetuning the\noutput layer, we also want to ﬁnetune the input layer, and\ninstead evaluate the ability of the frozen intermediate model\nto perform generalizable computation. This is reminiscent\nof Global Workspace Theory (Baars 1993), which revolves\naround the notion that there is a “blackboard” that differ-\nent parts of the brain send data to; we might consider the\nfrozen language model as being a blackboard in this setting.\nLanguage might also be a natural choice of model for this\nblackboard, as there are hypotheses that language may serve\nas a good multipurpose high-level representation for cogni-\ntive behavior and conscious planning (Andreas, Klein, and\nLevine 2017; Goyal and Bengio 2020).\nReservoir computing. Similarly to the FPT setup and\nGlobal Workspace Theory, in reservoir computing (Tanaka\net al. 2019) and echo state networks (Jaeger 2001; Jaeger\nand Haas 2004), a random recurrent network is frozen and\nonly the output readout layer is trained. These models are\nvery fast to train, using a similar setup as in Section 3.8, be-\ncause the activations of the recurrent network can be cached\nand it is unnecessary to backpropagate over time. Somewhat\ndifferently to the FPT architecture, echo state networks are\nrecurrent and thus feed back into themselves, which allows\nthe outputs of the random frozen network to modulate future\ninputs. Unlike echo state networks, we also notably ﬁnetune\nthe input and positional embeddings, which allow the inputs\nto the frozen network to adapt to a particular modality/for\na query to the frozen network to be learned. Echo state net-\nworks are also similar to the perspective of self-attention ap-\nplying a data-dependent ﬁlter to the inputs, as opposed to 1D\nconvolutions, which are ﬁxed ﬁlters regardless of inputs.\n5 Conclusion\nWe proposed transferring a pretrained transformer language\nmodel for downstream tasks in non-language modalities. We\nshowed empirically that these models could achieve perfor-\nmance competitive with transformers fully trained on the\ndownstream task, relying solely on frozen parameters from\nthe language model to perform the bulk of the computation.\nWe believe this work can serve as the foundation for future\nwork investigating transfer between modalities, and training\nregimes for multimodal universal models.\nWe note a limitation of our analysis is that we analyze\nspeciﬁc models on a restricted set of tasks. More investiga-\ntion can highlight if similar behavior occurs for other models\non other tasks. As training regimes for these models evolve,\nperforming similar experiments may yield different results,\nand we are excited for more research in this direction.\nFor high stakes applications in the real-world, there are\npotential concerns with transfer of harmful biases from one\nmodality to one another using pretrained transformer mod-\nels (Sheng et al. 2019; Bender et al. 2021). Mitigating these\nbiases is an active area of research (Grover et al. 2019; Choi\net al. 2020). Conversely, there are also potential upsides\nwith FPT models being able to better exploit representative\ndatasets from one or more modalities, which merit future in-\nvestigation as well.\n7634\nAcknowledgments\nWe would like to thank Luke Metz, Kimin Lee, Fangchen\nLiu, Roshan Rao, Aravind Srinivas, Nikita Kitaev, Daniel\nFreeman, Marc’Aurelio Ranzato, Jacob Andreas, and\nAshish Vaswani for valuable feedback and discussions.\nReferences\nAndreas, J.; Klein, D.; and Levine, S. 2017. Learning with\nlatent language. arXiv preprint arXiv:1711.00482.\nArtetxe, M.; Ruder, S.; and Yogatama, D. 2019. On the\ncross-lingual transferability of monolingual representations.\narXiv preprint arXiv:1910.11856.\nBaars, B. J. 1993.A cognitive theory of consciousness. Cam-\nbridge University Press.\nBender, E. M.; Gebru, T.; McMillan-Major, A.; and\nShmitchell, S. 2021. On the dangers of stochastic parrots:\nCan language models be too big. In Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency;\nAssociation for Computing Machinery: New York, NY, USA.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020.\nA simple framework for contrastive learning of visual repre-\nsentations. In International conference on machine learning,\n1597–1607. PMLR.\nChoi, K.; Grover, A.; Singh, T.; Shu, R.; and Ermon, S.\n2020. Fair generative modeling via weak supervision. In In-\nternational Conference on Machine Learning, 1887–1898.\nPMLR.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDonahue, J.; Kr ¨ahenb¨uhl, P.; and Darrell, T. 2016. Adver-\nsarial feature learning. arXiv preprint arXiv:1605.09782.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nEl-Gebali, S.; Mistry, J.; Bateman, A.; Eddy, S. R.; Luciani,\nA.; Potter, S. C.; Qureshi, M.; Richardson, L. J.; Salazar,\nG. A.; Smart, A.; Sonnhammer, E. L. L.; Hirsh, L.; Paladin,\nL.; Piovesan, D.; Tosatto, S. C. E.; and Finn, R. D. 2019.\nThe Pfam protein families database in 2019. Nucleic Acids\nResearch, 47(D1): D427–D432.\nFox, N. K.; Brenner, S. E.; and Chandonia, J.-M. 2013.\nSCOPe: Structural Classiﬁcation of Proteins—extended, in-\ntegrating SCOP and ASTRAL data and classiﬁcation of new\nstructures. Nucleic acids research, 42(D1): D304–D309.\nFrankle, J.; Schwab, D. J.; and Morcos, A. S. 2020. Training\nbatchnorm and only batchnorm: On the expressive power of\nrandom features in cnns. arXiv preprint arXiv:2003.00152.\nGoyal, A.; and Bengio, Y . 2020. Inductive Biases for\nDeep Learning of Higher-Level Cognition. arXiv preprint\narXiv:2011.15091.\nGrover, A.; Song, J.; Agarwal, A.; Tran, K.; Kapoor, A.;\nHorvitz, E.; and Ermon, S. 2019. Bias correction of learned\ngenerative models using likelihood-free importance weight-\ning. arXiv preprint arXiv:1906.09531.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHernandez, D.; Kaplan, J.; Henighan, T.; and McCandlish,\nS. 2021. Scaling Laws for Transfer. arXiv preprint\narXiv:2102.01293.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation, 9(8): 1735–1780.\nHou, J.; Adhikari, B.; and Cheng, J. 2018. DeepSF: deep\nconvolutional neural network for mapping protein sequences\nto folds. Bioinformatics, 34(8): 1295–1303.\nHoulsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;\nDe Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and\nGelly, S. 2019. Parameter-efﬁcient transfer learning for\nNLP. In International Conference on Machine Learning,\n2790–2799. PMLR.\nJaeger, H. 2001. The “echo state” approach to analysing\nand training recurrent neural networks-with an erratum note.\nBonn, Germany: German National Research Center for In-\nformation Technology GMD Technical Report, 148(34): 13.\nJaeger, H.; and Haas, H. 2004. Harnessing nonlinearity: Pre-\ndicting chaotic systems and saving energy in wireless com-\nmunication. science, 304(5667): 78–80.\nJumper, J.; Evans, R.; Pritzel, A.; Green, T.; Figurnov, M.;\nTunyasuvunakool, K.; Ronneberger, O.; Bates, R.; ˇZ´ıdek,\nA.; Bridgland, A.; Meyer, C.; Kohl, S. A. A.; Potapenko,\nA.; Ballard, A. J.; Cowie, A.; Romera-Paredes, B.; Nikolov,\nS.; Jain, R.; Adler, J.; Back, T.; Petersen, S.; Reiman, D.;\nSteinegger, M.; Pacholska, M.; Silver, D.; Vinyals, O.; Se-\nnior, A. W.; Kavukcuoglu, K.; Kohli, P.; and Hassabis, D.\n2021. High Accuracy Protein Structure Prediction Using\nDeep Learning. Nature.\nKaiser, L.; Gomez, A. N.; Shazeer, N.; Vaswani, A.; Parmar,\nN.; Jones, L.; and Uszkoreit, J. 2017. One model to learn\nthem all. arXiv preprint arXiv:1706.05137.\nKrizhevsky, A.; et al. 2009. Learning multiple layers of fea-\ntures from tiny images. University of Toronto.\nLu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. Vil-\nbert: Pretraining task-agnostic visiolinguistic representa-\ntions for vision-and-language tasks. arXiv preprint\narXiv:1908.02265.\nMiconi, T.; Stanley, K.; and Clune, J. 2018. Differentiable\nplasticity: training plastic neural networks with backpropa-\ngation. In International Conference on Machine Learning,\n3559–3568. PMLR.\n7635\nNangia, N.; and Bowman, S. R. 2018. Listops: A di-\nagnostic dataset for latent tree learning. arXiv preprint\narXiv:1804.06028.\nOord, A. v. d.; Li, Y .; and Vinyals, O. 2018. Representation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748.\nPapadimitriou, I.; and Jurafsky, D. 2020. Pretraining on non-\nlinguistic structure as a tool for analyzing learning bias in\nlanguage models. arXiv preprint arXiv:2004.14601.\nPonti, E. M.; Vuli ´c, I.; Cotterell, R.; Reichart, R.; and Ko-\nrhonen, A. 2019. Towards zero-shot language modeling.\nIn Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2893–2903.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from natural\nlanguage supervision. Image, 2: T2.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training. OpenAI.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners. OpenAI.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2019. Explor-\ning the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683.\nRamesh, A.; Pavolv, M.; Goh, G.; Gray, S.; Chen, M.; Child,\nR.; Misra, V .; Mishkin, P.; Krueger, G.; Agarwal, S.; and\nSutskever, I. 2021. DALL·E: Creating Images from Text.\nOpenAI.\nRao, R.; Bhattacharya, N.; Thomas, N.; Duan, Y .; Chen, X.;\nCanny, J.; Abbeel, P.; and Song, Y . S. 2019. Evaluating Pro-\ntein Transfer Learning with TAPE. In Advances in Neural\nInformation Processing Systems.\nRebufﬁ, S.-A.; Bilen, H.; and Vedaldi, A. 2017. Learn-\ning multiple visual domains with residual adapters. arXiv\npreprint arXiv:1705.08045.\nRumelhart, D. E.; Hinton, G. E.; and Williams, R. J.\n1985. Learning internal representations by error propaga-\ntion. Technical report, California Univ San Diego La Jolla\nInst for Cognitive Science.\nSaar, K. L.; Morgunov, A. S.; Qi, R.; Arter, W. E.; Krainer,\nG.; Lee, A. A.; and Knowles, T. P. J. 2021. Learning the\nmolecular grammar of protein condensates from sequence\ndeterminants and embeddings. Proceedings of the National\nAcademy of Sciences, 118(15).\nSheng, E.; Chang, K.-W.; Natarajan, P.; and Peng, N. 2019.\nThe woman worked as a babysitter: On biases in language\ngeneration. arXiv preprint arXiv:1909.01326.\nStrodthoff, N.; Wagner, P.; Wenzel, M.; and Samek, W.\n2020. UDSMProt: universal deep sequence models for pro-\ntein classiﬁcation. Bioinformatics, 36(8): 2401–2409.\nTanaka, G.; Yamane, T.; H ´eroux, J. B.; Nakane, R.;\nKanazawa, N.; Takeda, S.; Numata, H.; Nakano, D.; and Hi-\nrose, A. 2019. Recent advances in physical reservoir com-\nputing: A review. Neural Networks, 115: 100–123.\nTay, Y .; Dehghani, M.; Abnar, S.; Shen, Y .; Bahri, D.; Pham,\nP.; Rao, J.; Yang, L.; Ruder, S.; and Metzler, D. 2020.\nLong Range Arena: A Benchmark for Efﬁcient Transform-\ners. arXiv preprint arXiv:2011.04006.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. arXiv preprint arXiv:1706.03762.\n7636"
}