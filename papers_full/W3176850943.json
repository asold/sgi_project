{
  "title": "EnsLM: Ensemble Language Model for Data Diversity by Semantic Clustering",
  "url": "https://openalex.org/W3176850943",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2438715126",
      "name": "Zhibin Duan",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2095945326",
      "name": "Hao Zhang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2098866077",
      "name": "Chaojie Wang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2803867291",
      "name": "Zhengjue Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2037222934",
      "name": "Bo Chen",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2129597680",
      "name": "Ming-Yuan Zhou",
      "affiliations": [
        "Xidian University",
        "The University of Texas at Austin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W3037753363",
    "https://openalex.org/W2163302275",
    "https://openalex.org/W2952723479",
    "https://openalex.org/W3035431747",
    "https://openalex.org/W2964074409",
    "https://openalex.org/W2103587173",
    "https://openalex.org/W2902689991",
    "https://openalex.org/W2944931850",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W1983719983",
    "https://openalex.org/W1905522558",
    "https://openalex.org/W3105692649",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3005861412",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2945542139",
    "https://openalex.org/W2962897020",
    "https://openalex.org/W2994797252",
    "https://openalex.org/W2950940239",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3034689796",
    "https://openalex.org/W2965491249",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3034771276",
    "https://openalex.org/W2951707031",
    "https://openalex.org/W2187625046",
    "https://openalex.org/W2097606805",
    "https://openalex.org/W2549476280",
    "https://openalex.org/W2148350549",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2963578424",
    "https://openalex.org/W2251784502",
    "https://openalex.org/W2147262247",
    "https://openalex.org/W2963921497",
    "https://openalex.org/W3034640977",
    "https://openalex.org/W3103817618",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2787803546",
    "https://openalex.org/W2533545350",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2962946486",
    "https://openalex.org/W3104783994",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2772572269",
    "https://openalex.org/W2117278770",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2216511711",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2758334418",
    "https://openalex.org/W2952478253",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2962966012"
  ],
  "abstract": "Zhibin Duan, Hao Zhang, Chaojie Wang, Zhengjue Wang, Bo Chen, Mingyuan Zhou. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 2954–2967\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2954\nEnsLM: Ensemble Language Model for Data Diversity\nby Semantic Clustering\nZhibin Duan*1, Hao Zhang*†1, Chaojie Wang1, Zhengjue Wang1,\nBo Chen†1, Mingyuan Zhou2\n1National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China\n2McCombs School of Business The University of Texas at Austin, Austin, TX 78712, USA\n{xd zhibin, zhanghao xidian}@163.com\nbchen@mail.xidian.edu.cn, Mingyuan.Zhou@mccombs.utexas.edu\nAbstract\nNatural language processing often faces the\nproblem of data diversity such as different\ndomains, themes, styles and so on. There-\nfore, a single language model (LM) is insufﬁ-\ncient to learn all knowledge from diverse sam-\nples. To solve this problem, we ﬁrstly pro-\npose an autoencoding topic model with mix-\nture prior (mATM) to perform clustering for\nthe data, where the clusters deﬁned in seman-\ntic space describe the data diversity. Having\nobtained the clustering assignment for each\nsample, we develop the ensemble LM (En-\nsLM) with the technique of weight modula-\ntion. Speciﬁcally, EnsLM contains a backbone\nwhich is adjusted by a few modulated weights\nto ﬁt for different sample clusters. As a re-\nsult, the backbone learns the shared knowledge\namong all clusters while modulated weights\nextract the cluster-speciﬁc features. EnsLM\ncan be trained jointly with mATM with ﬂexi-\nble LM backbone. We evaluate the effective-\nness of both mATM and EnsLM on different\nlanguage understanding and generative tasks.\n1 Introduction\nIt is common knowledge in modern natural lan-\nguage processing (NLP) that natural language\nvaries greatly across domains, themes, styles, gen-\nres and many other linguistic nuances (Van der\nWees et al., 2015; van der Wees, 2017; Niu et al.,\n2017). Generally, we call such nature of language\nas data diversity. Many existing works (Liu et al.,\n2017; Cai and Wan, 2019; Hu et al., 2019) have\nillustrated that data diversity will affect the perfor-\nmance of LMs if we just train a single LM over\nthe entire dataset, even though ﬁne-tuning a pre-\ntrained LM (that has been pre-training on a very\nlarge corpus) such as Bert (Devlin et al., 2019) on\ncurrent task (Aharoni and Goldberg, 2020).\n* Equal contribution. †Corresponding author.\n(a) LDA\n (b) mATM\nFigure 1: The distribution of samples on seman-\ntic space on 4 domains (different products) of Ama-\nzon dataset. The sample clustering characteristics of\nmATM can reﬂect the data diversity (domain in this ex-\nample) in the corpus.\nThe domain diversity in dataset is a very com-\nmon type of data diversity. In some cases, if we can\nobtain a well-deﬁned domain label for each sample,\nsome works (Jiang et al., 2020; Du et al., 2020;\nWright and Augenstein, 2020) try to consider the\nmulti-domain property of data in developing the\nLMs. However, these pre-deﬁned domain labels are\nnot always accurate or even available (Aharoni and\nGoldberg, 2020), especially for the wild datasets,\nin which data come from different sources, such as\ninternet news, product reviews, and daily conver-\nsation. To this end, we hope to develop a LM that\ncan explore the diversity from data automatically.\nData selection is a commonly used strategy to\nhandle diversity in data (Moore and Lewis, 2010;\nAxelrod et al., 2011; Duh et al., 2013; Silva et al.,\n2018; Aharoni and Goldberg, 2020). This kind\nof method is developed from an assumption that\nsamples belonging to the same cluster should own\nsimilar characteristics. According to the cluster-\ning assignment, models can select suitable data for\ntraining a LM for each cluster separately. Although,\nto some extend, data selection is an efﬁcient strat-\negy to alleviate the problem of data diversity, it\nmay bring two disadvantages as follows. Firstly,\nthe process of data selection is independent of the\nLM learning. In other words, the gradient signal\ngenerated by LM’s training loss can not affect the\n2955\ndata selection. Secondly, data selection only tells\nthe hard cluster belongings of samples, ignoring a\nfact that some samples may belong to more than\none clusters with soft (weighted) assignment.\nInspired by their works and to move beyond, in\nthis paper, we ﬁnd the semantics learned by topic\nmodeling (Blei et al., 2003; Srivastava and Sut-\nton, 2017) can infer sample clusters to a certain\nextent via K-means, but is not good enough, as\nshown in Fig. 1a . To jointly consider the clus-\ntering and topic modeling for better clustering (as\nshown in Fig. 1b) and for joint training with the\nfollowing LM, we ﬁrstly introduce an autoencod-\ning topic model with mixture priors (mATM). For\neach sample in the corpus, mATM can infer a soft\nclustering assignment. In order to jointly consider\nthe learning of mATM with various LMs, we em-\nploy the weight modulation methods (Cong et al.,\n2020; Wen et al., 2020). Speciﬁcally, as shown in\nFig. 3, given a LM as backbone, for each layer\n(convolutional or fully-connected), we introduce\nsome modulated parameters. Guided by clustering\nassignment inferred from mATM, these parameters\nmodulate the backbone single LM to multiple LMs,\ncorresponding to different clusters. Therefore, our\nproposed model can be seen as a type of ensemble\nlearning, and hence we call it ensemble language\nmodel (EnsLM).\nOur proposed mATM and EnsLM enjoy the fol-\nlowing distinguished properties:\n• The mATM learns the mixture-prior latent se-\nmantic space to deﬁne a soft clustering assign-\nment for each sample.\n• Guided by clustering assignments that de-\nscribe the data diversity, EnsLM learns both\nshared and cluster-speciﬁc knowledge by\nweight modulations.\n• Joint training of mATM and EnsLM improves\nthe performance of both on many NLP tasks.\n2 Related work\nFor NLP, topic modeling (TM) (Blei et al., 2003;\nZhou et al., 2012) and LMs are two common\nregimes with their own advantages. TM can dis-\ncover the interpretable global semantics that are\ntopics, while with pre-training on large corpus,\nLMs recently achieve the SOTA performance on\nmany NLP tasks with more focuses on local de-\npendencies. Therefore, some works consider to\ncombine them to obtain beneﬁts from both. Dieng\net al. (2016) and Wang et al. (2020) incorporate the\nTM with RNN-based model to capture the long-\nrange dependencies. To move beyond single-layer\nTM for RNNs, Guo et al. (2020) propose the re-\ncurrent hierarchical topic-guided RNN with the\nhelp of multi-layer TM (Zhou et al., 2015; Zhang\net al., 2018). To extract explicit document seman-\ntics for summarization, Wang et al. (2020) propose\nthree different modules to plug knowledge from\nTM into Transformer-based LMs (Vaswani et al.,\n2017; Devlin et al., 2018). Our work can be seen\nas a parallel work to combine their advantages to-\ngether but focuses on dealing with data diversity\nin NLP without the ground-truth information such\nas domain labels. Meanwhile, our work can be\napplied for different LMs including CNNs, RNNs,\nand Transformer-based models.\n3 Autoencoding topic model with\nmixture prior\nWe ﬁrstly describe one of the most popular topic\nmodels, latent Dirichlet allocation (LDA) (Blei\net al., 2003), and its autoencoding inference (Sri-\nvastava and Sutton, 2017). Inspired by them, in\norder to jointly consider topic learning and sam-\nple clustering, we propose the autoencoding topic\nmodel with mixture prior (mATM).\n3.1 LDA with autoencoding inference\nFor a document containing D words as w =\n{wd}D\nd=1, given K topics Φ = [ φ1,··· ,φK]\nwhere φk is a probability distribution over the vo-\ncabulary, LDA deﬁnes the generative process of w\nin Algorithm 1, where θ∈RK\n+ is the topic propor-\ntion with αas the prior parameter. After collapsing\nAlgorithm 1 Generative process of LDA\nfor each document wdo\nDraw topic proportion θ∼Dirichlet(α)\nfor each word at position ddo\nSample a topic id ∼Multinomial(1,θ)\nSample a word wd ∼Multinomial(1,φid)\nid, given θand Φ, we can represent the conditional\nlikelihood of wd as\nwd|Φ,θ∼Multinomial(1,Φθ). (1)\nGiven Φ, a popular approximation for efﬁcient\ninference of LDA is mean-ﬁeld variational infer-\nence, which tries to maximize the evidence lower\n2956\nbound (ELBO) of marginal data log likelihood as\nELBO = Eq(θ)[log p(w|θ,Φ)]−KL[q(θ)||p(θ)],\n(2)\nwhere q(θ) is the variational posterior. In particu-\nlar, Srivastava and Sutton (2017) propose the au-\ntoencoding variational inference (AEVB) (Kingma\nand Welling, 2013) for LDA by using Laplace ap-\nproximation (Hennig et al., 2012) for the Dirichlet\nprior, and building logistic-normal (LN) encoding\nposterior.\nAs shown in Fig. 1, we ﬁnd that running clus-\ntering method such as K-means on semantic space\nθcan not achieve satisfactory results. For jointly\nconsidering the learning of topics and sample clus-\ntering, we propose the mATM.\n3.2 Generative process of mATM\nSuppose the number of clusters is C, and the clus-\ntering prior parameter is π = [π1,··· ,πC] with∑C\nc=1 πc = 1, shown in Fig. 2a, mATM deﬁnes\nthe generative process of win Algorithm 2. Com-\nAlgorithm 2 Generative process of mATM\nfor each document wdo\nDraw cluster index z∼Categorical(π)\nDraw topic proportion θ∼Dirichlet(αz)\nfor each word at position ddo\nSample a topic id ∼Multinomial(1,θ)\nSample a word wd ∼Multinomial(1,φid)\npared with LDA, mATM has a mixture Dirichlet\nprior with parameters {αc}C\nc=1. In other words,\nmATM assumes that the θof different documents\nmay come from different clusters, which is the\nbasic thought to discover the data diversity from\ncorpus automatically.\n3.3 Variational encoder of mATM\nIn order to infer the parameters in mATM and\nfurther develop the EnsLM by mATM, we intro-\nduce AEVB for mATM, whose detailed structure\nis shown in Fig. 2b.\n3.3.1 Laplace approximation for mixture\nDirichlet prior\nAlthough Dirichlet prior of θis important to learn\ninterpretable topics (Wallach et al., 2009), it is dif-\nﬁcult to handle it within AEVB since AEVB needs\neffective reparameterization (RT) function for dis-\ntributions. Inspired by the success of the Laplace\napproximation for Dirichlet distribution, we pro-\npose the mixture LN (mLN) distribution as the\napproximation of mixture Dirichlet distribution.\nSpeciﬁcally, Srivastava and Sutton (2017) have\nproved that a Dirichlet distribution p(θ|α) can be\nwell approximated by LN distribution as\np(θ|µ,Σ) = LN(µ,Σ), (3)\nwhere the elements in mean vector µand diagonal\ncovariance matrix Σ are\nµk = log αk − 1\nK\nK∑\ni=1\nlog αi\nΣk = 1\nαk\n(\n1 − 2\nK\n)\n+ 1\nK2\nK∑\ni=1\n1\nαi\n. (4)\nTo go further, for inference of mATM, we construct\nthe mLN distribution as\np(θ|µ,Σ) =\nC∑\nc=1\nπcLN(µc,Σc)\nµc\nk = log αc\nk − 1\nK\nK∑\ni=1\nlog αc\ni\nΣc\nk = 1\nαc\nk\n(\n1 − 2\nK\n)\n+ 1\nK2\nK∑\ni=1\n1\nαc\ni\n, (5)\nwhich is used to approximate the mixture Dirichlet\nprior p(θ|{αc,πc}C\nc=1) in mATM. Therefore, for\neach document, the prior of θcan be written as∏C\nc=1 LN(µc,Σc)zc. In practice, we build the µc\nand Σc as\nµc = fWc\nµ(z),Σc = fWcσ(z), (6)\nwhere z= [z1,··· ,zC]. Next, we build variational\nposterior for latent variables with easy RT function.\n3.3.2 Variational encoding posterior\nAfter collapsing {id}D\nd=1 in mATM as (1) in LDA,\ngiven topics Φ, for document w, there are two\nlatent variables that need to be inferred: θand z.\nLN posterior forθ. We build the variational pos-\nterior of θas LN distribution q(θ) = LN(µ′,Σ′)\nwith µ′= fWθµ(x), Σ′= diag(fWθσ(x)), where\ndiagconverts a vector to a diagonal matrix,fWθµ(·)\nand fWθσ(·) are two encoding networks, and xis a\ntype of representation for documentwsuch as orig-\ninal words or bag of words (Bow) vector. Morevoer,\nLN distribution has easy RT function as Normal\ndistribution.\n2957\n(a) Generation in mATM\n (b) Inference in mATM\nFigure 2: Graphical model for the mATM, where the\ncircle with white color, the circle with gray color and\nthe rectangle denotes local latent variables, observa-\ntions, and global parameters in mATM.\nGumbel softmax (GS) posterior for z. As cate-\ngorical variable, z is difﬁcult to build variational\nposterior under AEVB with accurate RT function.\nInstead, we employ GS distribution (Jang et al.,\n2016) as the variational posterior of zfor efﬁcient\ngradient propagation.\nSpeciﬁcally, suppose the posterior of z is\nCategorical(π′), after obtaining C i.i.d samples\n{g1,··· ,gC}drawn from Gumbel(0,1), then z\ncan be sampled as\nz= arg max\nc\nexp ((log(π′\nc) + gc)/τ)∑O\no=1 exp ((log(π′o) + go)/τ)\n(7)\nwhere τ is the temperature parameter. In order\nto build encoder for π′, we let π′ = fWπ(θ,w).\nFor efﬁcient gradient propagation, rather than sam-\npling z from arg max as (7), we obtain the vari-\national posterior of soft assignment vector z =\n[z1,··· ,zC] as q(z):\n[q(z)]c = exp ((log(π′\nc) + gc)/τ)∑O\no=1 exp ((log(π′o) + go)/τ)\n. (8)\nBesides the beneﬁt of efﬁcient gradient back-\npropagation, the soft assignment in (8) provides\nclustering belonging weights. In the following En-\nsLM, this property is useful for some ambiguous\nsamples that may belong to different clusters.\n3.3.3 ELBO of mATM\nWe obtain the ELBO of mATM as\nELBO = Eq(θ)q(z)[log p(w|θ,Φ,z)]\n−KL[q(θ)||p(θ|z)] −KL[q(z)|p(z|π)]\n(9)\nSimilarly with Srivastava and Sutton (2017), in-\nstead of sampling Φ from Dirichlet posterior in\nLDA, we parameterize it as Φ = softmax(Wt),\nwhere Wt = [w1,··· ,wK] and softmax is op-\nerated for each topic {wk}K\nk=1 to ensure them\non a probability simplex. Therefore, as shown\nin Fig. 2, all the parameters of mATM are\nΘ1 = {Wθ\nµ,Wc\nµ,Wθ\nσ,Wc\nσ,Wπ,Wt}that can\nbe learned by maximizing the ELBO in (9).\n4 Ensemble language model\nRecently, various advanced LMs for language un-\nderstanding and generation have been introduced,\nmost of which do not consider the data diversities\nin the corpus. In this paper, having obtained the\nclustering assignment vector zfrom mATM, given\na single LM as backbone, we propose the ensemble\nLM (EnsLM) via z-guided weight modulation. In\nother words, the EnsLM can modulate the back-\nbone single LM to ﬁt for different clusters.\n4.1 Efﬁcient weight modulation\nAlthough LMs have many different types, basically,\nall of them build on convolutional (such as in CNN\n(Johnson and Zhang, 2015)) or fully-connected\n(such as in Transformer (Vaswani et al., 2017))\noperations (ignoring the bias) as\nConvolution : H2 = f(W ∗H1)\nFully-connection : H′\n2 = f(W′TH′\n1). (10)\nwhere, H1 ∈ RIx×Iy×Cin and H′\n1 ∈ RCin are\nthe input features, W ∈ Rkx×ky×Cin×Cout and\nW′ ∈RCin×Cout are the convolutional kernel or\nfull-connected weights1. Suppose the number of\nclusters (domains) in mATM isC, given a LM as\nbackbone, we introduce a few modulation parame-\nters to modulate the original parameters W or W′\nfor different clusters.\nSpeciﬁcally, shown in Fig. 3, for a convolutional\nor fully-connected layer in (10), suppose that there\nare two dictionaries of modulation parameters as:\nA = [α1,··· ,αC] ∈RCin×C\nB = [β1,··· ,βC] ∈RCout×C, (11)\nwhere {αc}C\nc=1 ∈RCin and {βc}C\nc=1 ∈RCout. For\na document wwhose feature at current layer is H1,\nafter archiving its domain assignment z ∈RC×1\n1Fully-connected layer can be also seen as a convo-\nlution layer where the convolutional kernel is W′ ∈\nR1×1×Cin×Cout (Ix = Iy = 1)\n2958\nBase\nparameters\nModulated\nparameters\nFigure 3: Illustration of weight modulation in EnsLM.\nfrom (8), we feed H1 into the modulated layer as\nConvolution : H2 = f((W ⊙Γ) ∗H1)\nFully-connection : H′\n2 = f((W′T ⊙Γ)H′\n1),\n(12)\nwhere Γ = αβT, α= Az∈RCin×1, β= Bz∈\nRCout×1, and ⊙denotes matrix element-wise prod-\nuct (with broadcasting for convolution).\nExplanation of (12). Intuitively, W and W′act\nas the backbone parameters in the original single\nLM, and Γ is the modulated parameters, which\nmoves the backbone to ﬁt different domains. If z\nis drawn from (7) that means zis a one-hot vec-\ntor, then it denotes that αand βare chosen from\nthe dictionaries A and B, correspondingly. If zis\ndrawn from (8) that means zis a soft assignment\nvector, then it denotes that αand βare weighted\nsummation of all elements inA and B, correspond-\ningly. In practice, we use the soft assignment vector\nsince i) it brings efﬁcient gradient propagation dur-\ning joint training of mATM and EnsLM, and ii)\nit considers the fact that there are some domain\nambiguous samples in the dataset.\nIt is interesting to note that although EnsLM is\ndeveloped for the problem that ground-truth priors\nof data diversity (such as domain label) is unavail-\nable, it can be also used when we know the priors.\nFor this scenario, rather than inferring the cluster-\ning assignment zfrom mATM via (8), we directly\nset zas the real one-hot assignment vector, which\nis illustrated in experiment in Sec. 5.2.\n4.2 Joint training of mATM and EnsLM\nDifferent from some strategies such as data selec-\ntion that separate the calculation of assignment and\nthe training of LM, our proposed mATM and En-\nsLM can be jointly trained in one framework.\nSpeciﬁcally, given a training set containing N\nsample {wn}N\nn=1, suppose that there is a label\n{yn}N\nn=1 for each sample. It should be noted that\nlabels {yn}N\nn=1 can be different for different tasks,\nsuch as labels for document classiﬁcation, golden\nsummarization for abstractive summarization, or\ndocument itself for generation. As a result, the\nloss for joint training of mATM and EnsLM can be\nwritten as\nL=\nN∑\nn=1\nEq(θn)q(zn)[log p(wn|θn,Φ,zn)]\n−Eq(zn)[LLM(wn,yn,zn)]\n−KL[q(θn)||p(θn)] −KL[q(zn)|p(zn)],\n(13)\nwhere, without loss of generality, LLM de-\nnotes the loss for LM. All learnable parame-\nters are i) parameters of mATM: ΘmATM =\n{Wθ\nµ,Wθ\nσ,Wu\nµ,Wu\nσ,Wπ}and ii) parameters of\nLM: ΘLM. These parameters can be jointly trained\nby stochastic gradient descend with low-variance\ngradient estimation since LN and GS distributions\nhave easy RT function.\n5 Experiments\nIn this section, we evaluate the effectiveness and ef-\nﬁciency of our proposed mATM and EnsLM on dif-\nferent NLP tasks including document clusters, text\nclassiﬁcation, language generation and abstractive\ndocument summarization. Our code is available at\nhttps://github.com/BoChenGroup/EnsLM\n5.1 Document clusters\nThe basic idea of mATM and EnsLM is that mATM\ncan automatically discover the sample clusters\nwhich describe the data diversity. Therefore, we\nﬁrstly evaluate the document clustering perfor-\nmance of mATM.\nDatasets Following Yao et al. (2019), we con-\nsider two widely used document clustering datasets,\n20News and R8 . This two datasets 2 can be\nfound in the open source code of Yao et al. (2019).\n2https://github.com/yao8839836/text gcn\n2959\n20News has 20 classes and consists of 18,846 docu-\nments with a vocabulary size of 61,188, partitioned\ninto a training set of 11,314 documents and a test\nset of 7,532 ones. R8 is a subset of the Reuters\n21578 dataset, which has 8 classes and was split\ninto 5,485 training and 2,189 test documents. For\nthese two datasets, we remove the stop words and\nuse the 2,000 most frequent terms as the vocabu-\nlary. For all methods, we set the number of clusters\nas the number of classes.\nComparison models and implementation de-\ntails To verify the effectiveness of mATM\nfor clustering, three types of document clus-\ntering models are compared. i) Raw+kmeans\nperforms K-means on raw BoW vectors, and\nPCA+kmeans uses PCA extract low-dimensional\nfeatures and then uses K-means for clustering;\nii) Train a topic model and then perform K-\nmeans for clustering on topic proportions, where\nwe consider LDA+kmeans (Blei et al., 2003),\nA VITM+kmeans(Srivastava and Sutton, 2017),\nand PFA+kmeans (Zhou et al., 2012); iii) Deep\nneural network based clustering methods, in-\ncluding Deep clustering (Xie et al., 2016), and\nDCN (Yang et al., 2017), which jointly consider\nthe feature extracting and clustering. Besides\nRaw+kmeans performing clustering on original in-\nputs, others are on a latent feature space (For topic\nmodeling, feature is the topic proportion). Fol-\nlowing (Xie et al., 2016; Yang et al., 2017), the\ndimension of feature space equals to the number of\nclusters.\nTable 1: Results of AC and NMI for document cluster-\ning task.\nModel 20News R8\nAC NMI AC NMI\nBase+kmeans 30.2 37.0 40.1 30.2\nPCA+kmeans 33.1 39.1 44.1 32.1\nLDA+kmeans 37.4 38.1 53.8 36.9\nPFA+kmeans 38.4 39.2 54.7 37.6\nA VITM+kmeans 40.2 41.2 56.3 38.3\nDeepCluster 42.2 43.5 58.23 41.02\nDCN 44.8 48.4 59.34 43.2\nmATM 46.44 49.86 62.15 48.12\nResults Following Yang et al. (2017), since we\nknow the ground-truth label and set the clustering\nnumber as the number of classes, we measure the\nclustering performance by accuracy (AC) and nor-\nmalized mutual information (NMI), both of which\nare the higher the better. The results are shown\nin Table 1. Compared with the Base+kmeans,\nPCA+kmeans performs better since it extracts ef-\nfective principal components. Beneﬁting from the\nlearning of semantics for documents, the second\ngroup including three types of topic modeling out-\nperforms PCA. Compared with the ﬁrst two groups,\nthe third group jointly considers the feature learn-\ning and clustering, thus achieving higher AC and\nNMI. Combined the advantages of topic modeling\nin extracting efﬁcient features from documents and\njoint learning of feature extractor and clustering,\nmATM gets the SOTA performance for document\nclustering tasks on these two datasets.\nThe clustering results support our motivation\nof using mATM to discover the data diversity. In\nthe following experiments, we evaluate the perfor-\nmance of both mATM and EnsLM on different\nlanguage understanding and generation tasks.\n5.2 Multi-domain sentiment classiﬁcation\nSentiment classiﬁcation (positive or negative) for\ndifferent products is a fundamental language un-\nderstanding task in NLP. For this task, the data di-\nversity mainly arises from different domains (prod-\nucts) (Blitzer et al., 2007), which brings the prob-\nlem that data from different domains may have\ndifferent distributions.\nDatasets To evaluate the performance of mATM\nand EnsLM in capturing the multi-domain property\nfor sentiment classiﬁcation, following Cai and Wan\n(2019), we perform experiments on the dataset re-\nleased by Liu et al. (2017), which consists of prod-\nuct and movie reviews in 16 different domains. The\ndata in each domain is randomly split into training\nset, development set and test set according to the\nproportion of 70%, 10%, 20%, whose statistics of\nthe 16 datasets are listed in Appendix A.1.\nComparison models and implementation de-\ntails Following (Cai and Wan, 2019), we ﬁrstly\nconsider three base models, BiLSTM (Adhikari\net al., 2019), TextCNN (Kim, 2014) and BERT\n(Devlin et al., 2019), which perform classiﬁca-\ntion on every domains separately. Secondly, com-\nbining data from different domains together, we\ntrain the above three models named as BiLSTM-\nmix, TextCNN-mix and DocBERT-mix. Hav-\ning obtained the ground-truth domain label, the\nprevious works regard the multi-domain problem\n2960\nas the multi-task learning (MTL) including DA-\nMTL (Zheng et al., 2018), ASP-MTL (Liu et al.,\n2017),and MDAE (Cai and Wan, 2019). All these\nworks are developed from BiLSTM model. For\nour proposed EnsLM, we use TextCNN, BiLSTM\nand DocBERT as the backbone of EnsLM. We\nperform experiments on two types of EnsLM: i)\nwith ground-truth (GT) domain label, we directly\nset zas the one-hot assignment vector (do not in-\nfer zfrom mATM), which is named as BiLSTM-\nEnsLM-GT, TextCNN-EnsLM-GT, and BERT-\nEnsLM-GT; ii) without GT domain label, we use\nmATM to infer z, which is named as BiLSTM-\nEnsLM-mATM, TextCNN-EnsLM-mATM, and\nBERT-EnsLM-mATM. For model using mATM,\nwe set the number of topics as 16. More detailed\nsettings and implementation details can be found\nin Appendix B.1.\nTable 2: Accuracy of sentiment classiﬁcation.\nModels ACC Models ACC\nTextCNN 84.3 TextCNN-Mix85.3\nBiLSTM 83.7 BiLSTM-Mix86.6\nBERT 88.1 BERT-Mix 91.3\nTextCNN-EnsLM-GT88.2 DA-MTL 88.2\nBiLSTM-EnsLM w-GT89.4 ASP-MTL 87.2\nBERT-EnsLM w-GT92.9 MDAE 90.1\nTextCNN-EnsLM-mATM88.8\nBiLSTM-EnsLM-mATM90.2 - -\nBERT-EnsLM-mATM93.5\nResults The results of averaged accuracy on all\ndomains are given in Table 2, where the results\nexcept ours are obtained from Cai and Wan (2019).\nComparing results on the ﬁrst row, we can see that\njoint training models on all domains outperform\nseparate training on each domain. Compared with\nBiLSTM-mix, having obtained the GT domain la-\nbel, DA-MTL, ASP-MTL and MDAE (all of them\nare developed based on BiLSTM) consider the real\ndomain knowledge in word embedding, feature\nextractor and attention layers, achieving higher ac-\ncuracy. Similarly, with GT domain label, three\nmodels equipped with our proposed EnsLM per-\nforms better than their basic counterparts with a\nlarge margin. Assuming that GT domain labels\nare unavailable, we use mATM to infer the clus-\ntering assignment to guide the learning of EnsLM,\nwhich obtains the SOTA performance on all three\nbasic models, even better than the models using GT\ndomain label. We attribute it to the fact that com-\nTable 3: Comparison of perplexity on four datasets.\nMethods APNEWS IMDB BNC COCO\nLSTM 60.13 65.16 95.73 21.34\nTransformer-XL 58.73 60.11 97.14 19.32\nTGV AE 48.73 57.11 87.86 -\nrGBN-RNN 42.71 51.36 79.13 -\nGPT-2 35.78 44.71 46.04 13.58\nGPT-2-EnsLM-mATM 23.67 35.48 40.79 12.45\npared with the hard GT domain label, mATM infers\nthe soft clustering assignment, which not only re-\nﬂect the domain characteristic of samples but also\ndescribe the samples having confused domain char-\nacteristics. For example samples from DVD may\nbe similar with the ones from Electronics.\n5.3 Language generation\nDatasets In order to verify the effectiveness of\nour model on datasets of different lengths, we con-\nsider four publicly available corpora: APNEWS,\nIMDB, BNC, and COCO. Following Lau et al.\n(2017), we tokenize words and sentences using\nStanford CoreNLP (Klein and Manning, 2003),\nlowercase all word tokens, and ﬁlter out word to-\nkens that occur less than 10 times. For the topic\nmodel, we additionally exclude stopwords. All\nthese corpora are partitioned into training, valida-\ntion, and testing sets, whose summary statistics are\nprovided in Appendix A.2.\nComparison models and implementation de-\ntails We consider the following baseline mod-\nels: LSTM, A standard LSTM language\nmodel (Hochreiter and Schmidhuber, 1997);\nTansnsformer-XL enables learning dependency\nbeyond a ﬁxed length by introducing a recurrence\nmechanism and a novel position encoding scheme\ninto the Transformer architecture (Dai et al., 2019);\nTGV AE(Wang et al., 2019), combines a varia-\ntional auto-encoder based natural sequence model\nwith a neural topic model; rGBN-RNN (Guo et al.,\n2020), extracts recurrent hierarchical semantic\nstructure via a dynamic deep topic model to guide\nnatural language generation; GPT-2(Radford et al.,\n2019) is a generative pre-training of a Transformer-\nbased LM on a diverse set of unlabeled text. For\nour proposed model, GPT-2-EnsLM-mATMﬁrst\nuses mATM to infer semantic clusters for each sam-\nple, and then introduce this diversity information to\npre-trained GPT2 by efﬁcient weight modulation\nnaturally. In the experiments, we use the Adam op-\ntimizer (Kingma and Ba, 2014) with learning rate\n10−6. The length of an input sample is limited to\n2961\nCluster # Representive topics Original sentences Generated sentences\n1\n['kite', 'flying', 'sky’, 'air’, 'holding’]\n['man', 'child', 'people', 'person', 'young’]\n['beach’, 'water', 'outside', 'near’, 'park']\nA child flying a pink kite on the beach.\nPerson flying a kite high over a sea inlet.\nThe bird is on a branch on the tree.\nA man in a yellow and white outfit flying a kite.\nA young child flying a kite with a frisbee in the air. \nA person flying a kite near the water in a body of water. \n2\n['cake', 'slice', 'piece', 'chocolate', 'cream’]\n['table', 'plate', 'fork', 'cup', 'eaten’]\n['white’, 'large', 'small’, 'blue', ‘red']\nA women receives a cake that is blue.\nA piece of a chocolate cake on a plate. \nA small bird perched on a thin branch.\nTwo cakes with frosting on top sit on a red plate.\nA sandwich on a platter with a pickle and some fruit.\nA cake that has various decorations on it.\n5\n['baseball', 'bat', 'player', 'ball', 'game’]\n['man', 'holding', 'batter', 'swinging', 'field’] \n['pitch', 'boy', 'plate', 'catcher', 'swing’]\nA baseball player stands with a baseball bat.\nA baseball player is holding a baseball bat.\nA baseball player is swinging a baseball bat.\nA man on a baseball field swinging a bat.\nA baseball player swinging a bat on a field.\nA batter is getting ready to hit the ball.\nFigure 4: Example topics and their segment clusters inferred by a mATM from the COCO corpus, and the generated\nsentences under segment cluster guidance. For each cluster, top topics are shown in the column 2 respectively,\noriginal sentence are shown in the column 3 , and generated sentences are shown in the column 4.\n1024. We set the mini-batch size as 8, the number\nof training epochs as 5. The clustering number of\nmATM is set to 64 for the ﬁrst three datasets, while\n80 for COCO dataset. More detailed settings and\nimplementation details can be found in Appendix\nB.2\nResults For fair comparison, we use standard\nlanguage model perplexity as the evaluation met-\nric. The results of all models on four datasets\nare given in Table 3, where the results of exist-\ning models are obtained from Guo et al. (2020).\nIn the ﬁrst group, Transformer-XL gets better re-\nsult, which shows that the transformer-based model\nhave better modeling capabilities. In terms of cap-\nturing the document global semantic information,\nthe second group can improve performance sig-\nniﬁcantly, which indicates that the topic model is\neffective in capturing document global information.\nPre-training on massive data, the GPT-2 can ob-\ntains better results compared with above models.\nAlthough GPT-2 gets a good result, the GPT-2-\nEnsLM-mATM can improve performance signif-\nicantly by capturing data diversity. It illustrates\nthat even pre-training on large scale of corpus, En-\nsLM can further improve the performance of pre-\ntrained LM via exploring data diversity. A similar\nphenomenon also appeared in the experiments con-\nducted by Gururangan et al. (2020)\nSentence generation of EnsLM Given the\nlearned GPT-2-EnsLM-mATM, we can sample the\nsentences conditioned on semantic clusters. Shown\nin the in Fig. 5, we select the top-3 topics to rep-\nresent this cluster, and select original sentences\naccording to the clustering results. we can see that\nmost of the generated sentences conditioned on a\nsemantic clusters are highly related to the given\ntopics in terms of their semantic meanings but not\nnecessarily in key words, indicating the LM is suc-\ncessfully guided by the cluster assignment. These\nTable 4: ROUGE scores on CNN/DM and Xsum test\nset, where the results are cited from Liu and Lapata\n(2019) and Wang et al. (2020)\n.\nModel CNN/DM XSUM\nR1 R2 RL R1 R2 RL\nPTGEN 36.4415.6633.4229.70 9.21 23.24\nPTGEN+Cov39.5317.2836.3828.10 8.02 21.72\nTransformer 40.2117.7637.0929.41 9.77 23.01\nBertSUM 42.1319.6039.1838.8116.5031.27\nBertSUM+TA43.0620.5839.6739.7717.3932.39\nBertSUM+EnsLM43.3420.7839.8340.0117.6232.57\nobservations suggest that GPT-2-EnsLM-mATM\nhas successfully captured syntax and global seman-\ntics simultaneously for natural language generation.\nSimilar to Fig. 5, we also provide other semantic\nclusters generated sentences in Appendix C.\n5.4 Abstractive summarization\nDatasets We evaluate the effectiveness and ef-\nﬁciency of proposed model on two benchmark\ndatasets, including the CNN/DailyMail (CNN/DM)\n(Hermann et al., 2015) and the XSum (Narayan\net al., 2018). The summary styles of these datasets\nvaries from highlights, composed of several sen-\ntences, to very brief one sentence. See more\ndetailed descriptions in Appendix A.3. We per-\nform data pre-processing following Liu and Lapata\n(2019).\nComparison models and implementation de-\ntails We consider some baseline models, in-\ncluding LSTM based models PTGEN and PT-\nGEN+Cov (See et al., 2017); Transformer based\nmodels Tansformer, BertSUM (Liu and Lapata,\n2019); and BertSUM+TA which combine pre-\ntrained model with topic model (Wang et al., 2020).\nWe combine EnsLM with BertSUM on the abstrac-\ntive summarization task. The clustering number of\nmATM is set to 64 for all datasets. Given BertSUM\n2962\ncheckpoints3 on CNN/DM and XSum provided by\nLiu and Lapata (2019), we further ﬁne-tune Bert-\nSUM+EnsLM. Besides, we adopt the settings in\nthe BertSUM. Following Liu and Lapata (2019), in\nthe test stage, we use beam search with size 5, se-\nlect the top-3 checkpoints based on their evaluation\nloss on the validation set, and report the averaged\nresults on the test set. More detailed settings and\nimplementation details can be found in Appendix\nB.3.\nResults ROUGE scores on CNN/DM, XSum\nhave been exhibited in Tables 4, respectively. Fo-\ncusing on the models without pre-training in the\nﬁrst group, Transformer achieves better perfor-\nmance compared with LSTM-based model, at-\ntributing to stronger sequence modeling capabil-\nities. Further, the outperformance of BertSUM\nillustrates the fact that the combination of a pre-\ntrained Bert encoder and a Transformer decoder is\na better choice of sequence-to-sequence structure.\nDespite owning the same structure as the BertSUM,\nthe BertSUM+TA employs a topic model to cap-\nture global document segment diversity, and achiev-\ning higher scores. Different from BertSUM+TA\nthat introduces document semantic diversity by\nadding topic information, BertSUM+mATM com-\nbines BertSUM with EnsLM model, result in a\nbetter performance. Compared with BertSUM+TA,\nthe performance improvement of our model is not\nenough promising is because they have been incor-\nporated the topical information into the BertSum\nmodel which considering the segment diversity and\ncontextual information. Note that the performance\nof our model improves signiﬁcantly compared with\nBertSum, which can prove the effectiveness of our\nmodel.\n6 Conclusion\nIn this paper, we ﬁrst propose mATM to infer latent\nsemantic clusters from raw text corpus, and then\ncombine it with LM with efﬁcient weight modula-\ntion, resulting in a more powerful EnsLM, which\ncan be naturally extended to other LMs. In the fu-\nture, we will study the effectiveness of EnsLM on\nother NLP tasks, such as the multi domain transla-\ntion, and investigate whether EnsLM can be applied\nto the pre-training stage of Transformer.\n3https://github.com/nlpyang/PreSumm\nAcknowledgments\nBo Chen acknowledges the support of NSFC\n(61771361), Shaanxi Youth Innovation Team\nProject, the 111 Project (No. B18039) and the\nProgram for Oversea Talent by Chinese Central\nGovernment. We acknowledge all the anonymous\nreviewers for their valuable comments and sugges-\ntions.\nReferences\nAshutosh Adhikari, Achyudh Ram, Raphael Tang, and\nJimmy Lin. 2019. Rethinking complex neural net-\nwork architectures for document classiﬁcation. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) , pages 4046–\n4051.\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7747–\n7763, Online. Association for Computational Lin-\nguistics.\nAmittai Axelrod, Xiaodong He, and Jianfeng Gao.\n2011. Domain adaptation via pseudo in-domain data\nselection. In Proceedings of the 2011 Conference on\nEmpirical Methods in Natural Language Processing,\npages 355–362.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of ma-\nchine Learning research, 3(Jan):993–1022.\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, bollywood, boom-boxes and blenders:\nDomain adaptation for sentiment classiﬁcation. In\nProceedings of the 45th annual meeting of the asso-\nciation of computational linguistics, pages 440–447.\nversion 3 (BNC XML Edition) British National Corpus.\n2007. Distributed by oxford university computing\nservices on behalf of the bnc consortium.\nYitao Cai and Xiaojun Wan. 2019. Multi-domain sen-\ntiment classiﬁcation based on domain-aware embed-\nding and attention. In IJCAI, pages 4904–4910.\nYulai Cong, Miaoyun Zhao, Jianqiao Li, Sijia Wang,\nand Lawrence Carin. 2020. Gan memory with no\nforgetting. arXiv preprint arXiv:2006.07543.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\n2963\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAdji B Dieng, Chong Wang, Jianfeng Gao, and John\nPaisley. 2016. Topicrnn: A recurrent neural net-\nwork with long-range semantic dependency. arXiv\npreprint arXiv:1611.01702.\nChunning Du, Haifeng Sun, Jingyu Wang, Qi Qi, and\nJianxin Liao. 2020. Adversarial and domain-aware\nbert for cross-domain sentiment analysis. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4019–\n4028.\nKevin Duh, Graham Neubig, Katsuhito Sudoh, and Ha-\njime Tsukada. 2013. Adaptation data selection us-\ning neural language models: Experiments in ma-\nchine translation. In Proceedings of the 51st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 678–683.\nDandan Guo, Bo Chen, Ruiying Lu, and Mingyuan\nZhou. 2020. Recurrent hierarchical topic-guided\nrnn for language generation. In International Con-\nference on Machine Learning , pages 3810–3821.\nPMLR.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nPhilipp Hennig, David Stern, Ralf Herbrich, and Thore\nGraepel. 2012. Kernel topic models. In Artiﬁcial\nIntelligence and Statistics, pages 511–519.\nKarl Moritz Hermann, Tom´aˇs Koˇcisk´y, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Proceedings of the 28th Inter-\nnational Conference on Neural Information Process-\ning Systems - Volume 1, NIPS’15, page 1693–1701,\nCambridge, MA, USA. MIT Press.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nJunjie Hu, Mengzhou Xia, Graham Neubig, and\nJaime G Carbonell. 2019. Domain adaptation of\nneural machine translation by lexicon induction. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 2989–\n3001.\nEric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-\nical reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144.\nHaoming Jiang, Chen Liang, Chong Wang, and Tuo\nZhao. 2020. Multi-domain neural machine trans-\nlation with word-level adaptive layer-wise domain\nmixing. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1823–1834, Online. Association for Computa-\ntional Linguistics.\nRie Johnson and Tong Zhang. 2015. Effective use of\nword order for text categorization with convolutional\nneural networks. In Proceedings of the 2015 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 103–112, Denver, Col-\norado. Association for Computational Linguistics.\nYoon Kim. 2014. Convolutional neural net-\nworks for sentence classiﬁcation. arXiv preprint\narXiv:1408.5882.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nDiederik P Kingma and Max Welling. 2013. Auto-\nencoding variational bayes. arXiv preprint\narXiv:1312.6114.\nDan Klein and Christopher D Manning. 2003. Accu-\nrate unlexicalized parsing. In Proceedings of the\n41st annual meeting of the association for compu-\ntational linguistics, pages 423–430.\nJey Han Lau, Timothy Baldwin, and Trevor Cohn.\n2017. Topically driven neural language model. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 355–365, Vancouver, Canada.\nAssociation for Computational Linguistics.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nPengfei Liu, Xipeng Qiu, and Xuan-Jing Huang. 2017.\nAdversarial multi-task learning for text classiﬁca-\ntion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1–10.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\n2964\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the as-\nsociation for computational linguistics: Human lan-\nguage technologies, pages 142–150.\nRobert C Moore and William Lewis. 2010. Intelligent\nselection of language model training data. In Pro-\nceedings of the ACL 2010 Conference Short Papers,\npages 220–224.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807, Brussels, Bel-\ngium. Association for Computational Linguistics.\nXing Niu, Marianna Martindale, and Marine Carpuat.\n2017. A study of style in machine translation: Con-\ntrolling the formality of machine translation output.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2814–2819.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nCatarina Cruz Silva, Chao-Hong Liu, Alberto Poncelas,\nand Andy Way. 2018. Extracting in-domain training\ncorpora for neural machine translation using data se-\nlection methods. In Proceedings of the Third Con-\nference on Machine Translation: Research Papers ,\npages 224–231.\nAkash Srivastava and Charles Sutton. 2017. Autoen-\ncoding variational inference for topic models. arXiv\npreprint arXiv:1703.01488.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nHanna Wallach, David Mimno, and Andrew McCallum.\n2009. Rethinking lda: Why priors matter. Advances\nin neural information processing systems , 22:1973–\n1981.\nWenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang,\nGuoyin Wang, Dinghan Shen, Changyou Chen, and\nLawrence Carin. 2019. Topic-guided variational\nauto-encoder for text generation. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers) , pages 166–177, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nZhengjue Wang, Zhibin Duan, Hao Zhang, Chaojie\nWang, Long Tian, Bo Chen, and Mingyuan Zhou.\n2020. Friendly topic assistant for transformer based\nabstractive summarization. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 485–497.\nvan der Wees. 2017. What’s in a domain?: To-\nwards ﬁne-grained adaptation for machine transla-\ntion. Ph.D. thesis, University of Amsterdam.\nMarlies Van der Wees, Arianna Bisazza, Wouter\nWeerkamp, and Christof Monz. 2015. What’s in a\ndomain? analyzing genre and topic differences in\nstatistical machine translation. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers), pages 560–566.\nYeming Wen, Dustin Tran, and Jimmy Ba. 2020.\nBatchensemble: an alternative approach to efﬁcient\nensemble and lifelong learning. arXiv preprint\narXiv:2002.06715.\nDustin Wright and Isabelle Augenstein. 2020. Trans-\nformer based multi-source domain adaptation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7963–7974, Online. Association for Computa-\ntional Linguistics.\nJunyuan Xie, Ross Girshick, and Ali Farhadi. 2016.\nUnsupervised deep embedding for clustering analy-\nsis. In International conference on machine learn-\ning, pages 478–487.\nBo Yang, Xiao Fu, Nicholas D Sidiropoulos, and\nMingyi Hong. 2017. Towards k-means-friendly\nspaces: Simultaneous deep learning and clustering.\nIn international conference on machine learning ,\npages 3861–3870. PMLR.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019.\nGraph convolutional networks for text classiﬁcation.\nIn Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 33, pages 7370–7377.\nHao Zhang, Bo Chen, Dandan Guo, and Mingyuan\nZhou. 2018. Whai: Weibull hybrid autoencoding\ninference for deep topic modeling. arXiv preprint\narXiv:1803.01328.\n2965\nRenjie Zheng, Junkun Chen, and Xipeng Qiu. 2018.\nSame representation, different attentions: Shareable\nsentence representation learning from multiple tasks.\nIn Proceedings of the 27th International Joint Con-\nference on Artiﬁcial Intelligence , IJCAI’18, page\n4616–4622. AAAI Press.\nMingyuan Zhou, Yulai Cong, and Bo Chen. 2015. The\npoisson gamma belief network. Advances in Neural\nInformation Processing Systems, 28:3043–3051.\nMingyuan Zhou, Lauren Hannah, David Dunson, and\nLawrence Carin. 2012. Beta-negative binomial pro-\ncess and poisson factor analysis. In Artiﬁcial Intelli-\ngence and Statistics, pages 1462–1471.\nAppendix\nA Dataset descriptions\nA.1 Multi-domain sentiment classiﬁcation\nDataset:\nwe perform experiments on the dataset 4 released\nby Liu et al. (2017), which consists of product\nand movie reviews in 16 different domains. The\ndata in each domain is randomly split into training\nset, development set and test set according to the\nproportion of 70%, 10%, 20%. Statistics of the 16\ndatasets is shown in Table. 5.\nA.2 Language Generation Datasets\nIn experiments, we evaluate the models on four\nbenchmark language generation datasets. They\nare the APNEWS, IMDB, BNC, and COCO Cap-\ntion. APNEWS is a collection of Associated Press\nnews articles from 2009 to 2016. IMDB is a set\nof movie reviews collected by Maas et al. (2011).\nBNC is the written portion of the British National\nCorpus (British National Corpus, 2007), which\ncontains documents from journals, books,letters,\nessays, memoranda, news and other types of text.\nCOCO Caption has 80 object categories, and there\nare caption to describe the scene of the image (Lin\net al., 2014). All these corpora are partitioned into\ntraining, validation, and testing sets, whose sum-\nmary statistics are provided in Table. 6. The AG-\nNEWS, IMDB and BNC datasets can be found in\nthe release code5 of ?. And for COCO dataset, we\nwill give processed dataset in our release code.\nA.3 Abstractive Summarization Dataset\nIn experiments, we evaluate the models on two\nbenchmark summarization datasets. The datasets6\n4https://github.com/FrankWork/fudan mtl reviews\n5https://github.com/jhlau/\ntopically-driven-language-model\n6https://github.com/nlpyang/PreSumm\ncan be fround in the release code of Liu and La-\npata (2019) They are the CNN/DailyMail news\n(CNN/DM) (Hermann et al., 2015) and XSum\n(Narayan et al., 2018).\nCNN/DM CNN/DM consists of news and asso-\nciated sentence highlights, that is a brief overview\ncomposed of a few sentences. Following the stan-\ndard training/validation/testing splits in Hermann\net al. (2015) without anonymizing entities, we per-\nform our experiments. We splits sentences using\nthe Stanford CoreNLP toolkit7 and pre-process the\ndataset following Liu and Lapata (2019). .\nXSum XSum includes 226,711 news ar-\nticles, each of which is associated with\na one-sentence summary. We use the\nstandard training/validation/testing splits\n(204,045/11,332/11,334) and follow the\npre-processing in Narayan et al. (2018). To satisfy\nthe maximum capacity of the encoder in the base\nmodel, such as 512 for BertSUM, we use truncated\ndocument as the encoder input. Statistics of\nsummarization datasets is shown in Table. 7.\nB Implementation Details\nB.1 Multi-domain sentiment classiﬁcation\nModels\nNote that we remove stop words to obtain the bag-\nof-word (BOW) vector for each document, and then\nuse the BOW vectors to infer the mATM model.\nCNN/BiLSTM-EnSLM-mATM: To reduce\nboth computation and storage costs, we introduce\na learnable key vector as W(t), which can\nbe combined with mATM by efﬁcient weight\nmodulation, leading to a CNN/BiLSTM-EnSLM-\nmATM. More speciﬁcally, we adopt 1-layer\nCNN/BiLSTMCNN with the channel/hidden size\nof 150 in CNN/BiLSTM-EnSLM-mATM equipped\nwith 300-dimensional word embedding vecotrs.\nFor optimization, the Adam optimizer is utilized\nhere (Kingma and Ba, 2014) with a learning rate of\n0.001. To avoid overﬁtting, we utilize the dropout\nand set its rate as 0.5. We set the size of minibatch\nas 50 in all experiments.\nBert-EnsLM-mATM: As a transformer-based\nmodel, the main component of Bert is query, key\nand value layer. And these component as MLP\nlayer, we can combine Bert with mATM by efﬁ-\ncient weight modulation easily. Specially, to re-\n7https://stanfordnlp.github.io/CoreNLP/\n2966\nTable 5: Statistics of the 16 datasets. The columns 2-4 denote the number of samples in training, development, and\ntest sets. The last two columns represent the average length and vocabulary size of corresponding dataset.\nDataset Train Dev. Test Avg.L V ocab Dataset Train Dev. Test Avg.L V ocab\nBooks 1400 200 400 159 62K Toys 1400 200 400 90 28K\nElec. 1398 200 400 101 30k Video 1400 200 400 156 57K\nDVD 1400 200 400 173 69K Baby 1300 200 400 104 26K\nKitchen 1400 200 400 89 28K Mag. 1370 200 400 117 30K\nApparel 1400 200 400 57 21K Soft. 1315 200 400 129 26K\nCamera 1397 200 400 130 26K Sports. 1400 200 400 94 30K\nHealth 1400 200 400 81 26K IMDB 1400 200 400 269 44K\nMusic 1400 200 400 136 60K MR 1400 200 400 21 12K\nTable 6: Statistics of data for language generation task.\nCollection Training Development Test\nDocs Tokens Docs Tokens Docs Tokens\nAGNEWS 50K 15M 2K 0.6M 2K 0.6M\nIMDB 75K 20M 12.5K 0.3M 12.5K 0.3M\nBNC 15K 18M 1K 1M 1K 1M\nCOCO 400K 4.1M 14K 0.2M 202K 2.1M\nTable 7: Statistics of summarization datasets.\nDatasets Train Dev. Test Doc Avg.L Sum.Avg.L\nCNN 90,266 1,220 1,093 760.50 45.70\nDM 196,961 12,148 10,396 8080.04 54.65\nXSUM 204,045 11,332 11,334 431.07 23.26\nduce the amount of new parameters, we only intro-\nduce segment diversity information to query layer.\nFor optimization, the Adam optimizer is utilized\nhere (Kingma and Ba, 2014) with a learning rate\nof 0.00001. To avoid overﬁtting, we utilize the\ndropout and set its rate as 0.3. We set the size of\nminibatch as 16 in all experiments.\nB.2 Language Generation Models\nFor language generation, we propose GPT-2-\nEnsLM-mATM which combine mATM with pre-\ntrained model GPT-2. And we introduce segment\ndiversity information to query, key and value for\neach layer. We use the Adam optimizer (Kingma\nand Ba, 2014) with learning rate 10−6. The length\nof an input sample is limited to 1024. We set the\nmini-batch size as 8, the number of training epochs\nas 5. The clustering number of mATM is set to\n64 for the ﬁrst three datasets, while 80 for COCO\ndataset.\nB.3 Abstractive Summarization Models:\nFor abstractive summarization, we combine Bert-\nSum with mATM, which include a pretrained en-\ncoder and a transformer decoder. Specially, we\nintroduce segment diversity information to query,\nkey and value for each layer. We set the hyper-\nparameters following the original papers and their\npublic codes, where BertSUM8 is referred to Liu\nand Lapata (2019). We ﬁne-tune all models in four\nNvidia GeForce RTX2080 TI GPUs. The experi-\nments are performed with mini-batch size including\n200 summary tokens with gradient accumulation\nevery six iterations. Model checkpoints were saved\nand evaluated on the validation set every1000 up-\ndates. Totally, we update the model 250,000 times.\nFollowing Liu and Lapata (2019), we select the top-\n3 checkpoints based on their evaluation loss on the\nvalidation set, and report the averaged results on\nthe test set. During decoding we used beam search\n8https://github.com/nlpyang/BertSUM\n2967\nFigure 5: Example topics and their segment clusters inferred by a mATM from the COCO corpus, and the generated\nsentences under segment cluster guidance. For each cluster, original sentence are shown in the column 2, and\ngenerated sentence are shown in the column 3.\nwith size 5, and tuned the αfor the length penalty\nbetween 0.6 and 1 on validation set. It is worth\nnoting that our decoder applies neither a copy nor\na coverage mechanism, despite their popularity in\nabstractive summarization.\nC More Generation Examples\nAs shown in Fig. 5, we provide semantic clusters\ngenerated sentences by GPT-2-EnsLM-mATM on\nthe coco corpus.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6716707944869995
    },
    {
      "name": "Chen",
      "score": 0.6664708852767944
    },
    {
      "name": "Natural language processing",
      "score": 0.6022197008132935
    },
    {
      "name": "Cluster analysis",
      "score": 0.5619222521781921
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5208224058151245
    },
    {
      "name": "Zhàng",
      "score": 0.500312328338623
    },
    {
      "name": "Computational linguistics",
      "score": 0.475448340177536
    },
    {
      "name": "Biomedical text mining",
      "score": 0.42524367570877075
    },
    {
      "name": "Diversity (politics)",
      "score": 0.4243428111076355
    },
    {
      "name": "Linguistics",
      "score": 0.35842522978782654
    },
    {
      "name": "Text mining",
      "score": 0.1351376175880432
    },
    {
      "name": "History",
      "score": 0.12587571144104004
    },
    {
      "name": "Philosophy",
      "score": 0.09997272491455078
    },
    {
      "name": "Sociology",
      "score": 0.08950898051261902
    },
    {
      "name": "Anthropology",
      "score": 0.08135837316513062
    },
    {
      "name": "China",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I149594827",
      "name": "Xidian University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    }
  ],
  "cited_by": 5
}