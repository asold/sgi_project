{
  "title": "Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing",
  "url": "https://openalex.org/W3208413651",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2113878559",
      "name": "jiawei Zhou",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2072731134",
      "name": "Tahira Naseem",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129506641",
      "name": "Ramón Fernandez Astudillo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100006581",
      "name": "Young-suk Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132510150",
      "name": "Radu Florian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2336572873",
      "name": "Salim Roukos",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963161698",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W3173625863",
    "https://openalex.org/W2963679213",
    "https://openalex.org/W2758846169",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2619249607",
    "https://openalex.org/W3093568530",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3174281149",
    "https://openalex.org/W2962947230",
    "https://openalex.org/W3173677700",
    "https://openalex.org/W2970796523",
    "https://openalex.org/W2949889664",
    "https://openalex.org/W3104681577",
    "https://openalex.org/W2987673972",
    "https://openalex.org/W2952768586",
    "https://openalex.org/W2963069010",
    "https://openalex.org/W3103469330",
    "https://openalex.org/W3167397433",
    "https://openalex.org/W4294990258",
    "https://openalex.org/W2953243654",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4293430466",
    "https://openalex.org/W2563976356",
    "https://openalex.org/W3104890489",
    "https://openalex.org/W1869752048",
    "https://openalex.org/W2964146228",
    "https://openalex.org/W3104713013",
    "https://openalex.org/W2966469393",
    "https://openalex.org/W2252123671",
    "https://openalex.org/W3035586188",
    "https://openalex.org/W2888912028",
    "https://openalex.org/W3116083993",
    "https://openalex.org/W2964164798",
    "https://openalex.org/W4287550997",
    "https://openalex.org/W2296308987",
    "https://openalex.org/W2760379373",
    "https://openalex.org/W2250375035",
    "https://openalex.org/W3106209546",
    "https://openalex.org/W655477013",
    "https://openalex.org/W2951391755",
    "https://openalex.org/W3177355445"
  ],
  "abstract": "Predicting linearized Abstract Meaning Representation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the integration of general pre-trained sequence-to-sequence language models and a structure-aware transition-based approach. We depart from a pointer-based transition system and propose a simplified transition set, designed to better exploit pre-trained language models for structured fine-tuning. We also explore modeling the parser state within the pre-trained encoder-decoder architecture and different vocabulary strategies for the same purpose. We provide a detailed comparison with recent progress in AMR parsing and show that the proposed parser retains the desirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6279–6290\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n6279\nStructure-aware Fine-tuning of Sequence-to-sequence Transformers for\nTransition-based AMR Parsing\nJiawei Zhoum Tahira Naseem3 Ramón Fernandez Astudillo3 Young-Suk Lee3\nRadu Florian3 Salim Roukos3\nmHarvard University 3IBM Research\nmjzhou02@g.harvard.edu 3ramon.astudillo@ibm.com\n3{tnaseem, ysuklee, raduf, roukos}@us.ibm.com\nAbstract\nPredicting linearized Abstract Meaning Rep-\nresentation (AMR) graphs using pre-trained\nsequence-to-sequence Transformer models\nhas recently led to large improvements on\nAMR parsing benchmarks. These parsers\nare simple and avoid explicit modeling of\nstructure but lack desirable properties such as\ngraph well-formedness guarantees or built-in\ngraph-sentence alignments. In this work we\nexplore the integration of general pre-trained\nsequence-to-sequence language models and\na structure-aware transition-based approach.\nWe depart from a pointer-based transition\nsystem and propose a simpliﬁed transition set,\ndesigned to better exploit pre-trained language\nmodels for structured ﬁne-tuning. We also\nexplore modeling the parser state within the\npre-trained encoder-decoder architecture and\ndifferent vocabulary strategies for the same\npurpose. We provide a detailed comparison\nwith recent progress in AMR parsing and\nshow that the proposed parser retains the de-\nsirable properties of previous transition-based\napproaches, while being simpler and reaching\nthe new parsing state of the art for AMR 2.0,\nwithout the need for graph re-categorization.\n1 Introduction\nThe task of Abstract Meaning Representation\n(AMR) parsing translates a natural sentence into\na rooted directed acyclic graph capturing the se-\nmantics of the sentence, with nodes representing\nconcepts and edges representing their relations (Ba-\nnarescu et al., 2013). Recent works utilizing pre-\ntrained encoder-decoder language models show\ngreat improvements in AMR parsing results (Xu\net al., 2020; Bevilacqua et al., 2021). These ap-\nproaches avoid explicit modeling of the graph struc-\nture. Instead, they directly predict the linearized\nAMR graph treated as free text. While the use\nof pre-trained Transformer encoders is widely ex-\ntended in AMR parsing, the use of pre-trained\nTransformer decoders is recent and has shown to be\nvery effective, maintaining current state-of-the-art\nresults (Bevilacqua et al., 2021).\nThese approaches however lack certain desirable\nproperties. There are no structural guarantees of\ngraph well-formedness, i.e. the model may predict\nstrings that can not be decoded into valid graphs,\nand post-processing is required. Furthermore, pre-\ndicting AMR linearizations ignores the implicit\nalignments between graph nodes and words, which\nprovide a strong inductive bias and are useful for\ndownstream AMR applications (Mitra and Baral,\n2016; Liu et al., 2018; Vlachos et al., 2018; Kapa-\nnipathi et al., 2021; Naseem et al., 2021).\nOn the other hand, transition-based AMR parsers\n(Wang et al., 2015; Ballesteros and Al-Onaizan,\n2017a; Astudillo et al., 2020; Zhou et al., 2021)\noperate over the tokens of the input sentence, gen-\nerating the graph incrementally. They implicitly\nmodel graph structural constraints through transi-\ntions and yield alignments by construction, thus\nguaranteeing graph well-formedness. 1 However,\nit remains unclear whether explicit modeling of\nstructure is still beneﬁcial for AMR parsing in the\npresence of powerful pre-trained language models\nand their strong free text generation abilities.\nIn this work, we integrate pre-trained sequence-\nto-sequence (seq-to-seq) language models with the\ntransition-based approach for AMR parsing, and\nexplore to what degree they are complementary. To\nfully utilize the generation power of the pre-trained\nlanguage models, we propose a transition system\nwith a small set of basic actions – a generaliza-\ntion of the action-pointer transition system of Zhou\net al. (2021). We use BART (Lewis et al., 2019) as\nour pre-trained language model, since it has shown\nsigniﬁcant improvements in linearized AMR gen-\neration (Bevilacqua et al., 2021). Unlike previous\napproaches that directly ﬁne-tune the model with\n1With the only exception being disconnected graphs,\nwhich happen infrequently in practice.\n6280\nlinearized graphs, we modify the model structure to\nwork with our transition system, and encode parser\nstates in BART’s attention mechanism (Astudillo\net al., 2020; Zhou et al., 2021). We also explore dif-\nferent vocabulary strategies for action generation.\nThese changes convert the pre-trained BART to a\ntransition-based parser where graph constraints and\nalignments are internalized.\nWe provide a detailed comparison with top-\nperforming AMR parsers and perform ablation ex-\nperiments showing that our proposed transition sys-\ntem and BART modiﬁcations are both necessary to\nachieve strong performance. Although BART has\ngreat language generation capacity, it still beneﬁts\nfrom parser state encoding with hard attention, and\ncan efﬁciently learn structural output. Our model\nestablishes a new state of the art for AMR 2.0 while\nmaintaining graph well-formedness guarantees and\nproducing built-in alignments.\n2 Intricacies of AMR Parsers\nA frequent complaint about AMR parsers is\nthat they involve combining many different tech-\nniques and hand-crafted rules, resulting in complex\npipelines that are hard to analyze and generalize\npoorly. This situation has notably improved in the\npast few years but there are still two main sources\nof complexity present in almost all recent parsers:\ngraph re-categorization and subgraph actions.\nGraph re-categorization (Wang and Xue, 2017;\nLyu and Titov, 2018; Zhang et al., 2019a) normal-\nizes the graph prior to learning. This includes\njoining certain subgraphs such as entities, dates\nand other constructs into single nodes, removing\nspecial types of nodes like polarity and normal-\nizing propbank names. An example of common\nnormalizations is displayed in Figure 1. Training\nand decoding of models using this technique hap-\npens in this re-categorized space. Re-categorized\ngraphs are expanded to normal valid AMR graphs\nin a post-processing stage. The type and number\nof subgraphs normalized vary across implementa-\ntions, but most high performing approaches (Cai\nand Lam, 2020; Bevilacqua et al., 2021) utilize\nthe re-categorization described in Appendix A.1 of\nZhang et al. (2019a). This version requires of an\nexternal Named Entity Recognition (NER) system\nto anonymize named entities, both at train time\nand test time. It also makes use of look-up tables\nfor nominalizations (e.g. English to England) and\nother hand-crafted rules. Graph re-categorization\nhave-03\n−\nRemoved polarity\nthing\nopine-01\nCollapsed verbalization\nteam\nname\nNew York Mets\nAnonimized named entity\nI\nFigure 1: AMR graph of the sentenceI have no opinion\non the New York Mets. Examples of subgraphs for en-\ntity anonymization, collapsing of verbalized nouns and\nremoval of the polarity node and edge.\nhas been criticised for its lack of generalization\nto new domains, such as biomedical domain or\neven the AMR 3.0 corpus (Bevilacqua et al., 2021).\nRecent top performing systems e.g. Cai and Lam\n(2020); Bevilacqua et al. (2021) also provide re-\nsults without re-categorization, but this is shown to\nhurt performance notably on the AMR 2.0 corpus.\nSubgraph actions (Ballesteros and Al-Onaizan,\n2017b) are used in transition-based systems and\nplay a role similar to re-categorization. Instead of\nnormalizing and reverting, transition-based parsers\napply a subgraph action that generates an entire\nsubgraph at once. This subgraph action coin-\ncides with many of the subgraphs collapsed in\nre-categorization. Subgraph actions bring how-\never fewer external dependencies, since the parser\nlearns to segment and identify subgraphs during\ntraining. They still suffer however from data spar-\nsity since some subgraphs appear very few times.\nAs in re-categorization, subgraph actions also make\nuse of lookup tables for nominalization and similar\nconstructs that hinder generalization. Furthermore,\nthey create the problem of unattachable nodes. This\nwas addressed in Zhou et al. (2021) by ignoring sub-\ngraphs for a set of heuristically determined cases.\nSubgraph actions have been used in all transition-\nbased AMR systems (Naseem et al., 2019; As-\ntudillo et al., 2020; Zhou et al., 2021).\nAside from NER, past AMR parsers have other\nexternal dependencies such as POS taggers (Zhang\net al., 2019a; Cai and Lam, 2020) and lemmatizers\n(Cai and Lam, 2020; Naseem et al., 2019; Astudillo\net al., 2020).\n6281\n1\nperson\nEmployees\nemploy-01\nperson\nARG 1-of\n2\nemploy-01\nEmployees\nRA(1, ARG 1-of)\nEmployees\nSHIFT\n5\nlike-01\nliked\nlike-01\nARG 0\nLA(1, ARG 0)\nROOT\nliked liked\nSHIFT\ntheir\nSHIFT\n10\ncity\nBoston\nname\nop1\ncity\nname\n11\nname\nBoston\nRA(10,name)\n13\nCOPY\nBoston\nRA(11,op1)\nBoston\nSHIFT\n16\ntrip-03\ntrip\ntrip-03\nARG 0 ARG 1\nARG 1\nRA(5, ARG 1)\nLA(1, ARG 0)\nLA(10, ARG 1)\nSHIFT\ntrip\nFigure 2: From top to bottom: graph (solid lines), sentence (source), addressable action positions and action\nsequence (target) for the sentence Employees liked their Boston trip, aligned (dotted lines) to its AMR graph. Arc-\ncreating actions are displayed vertically due to space constraints. Words are repeated in grey to indicate the word\nunder cursor for each action. The node Boston in dotted box is created by copying the token under cursor viaCOPY\naction at position 13. LA(1, ARG 0) creates a left arc with label ARG 0 from the top concept like-01 to the concept\nperson at position 1. For the concept trip-03, LA(1, ARG 0) is a co-reference (re-entrancy) to the concept person.\n3 A Simpliﬁed Transition System\nIn this section we propose a transition system for\nAMR parsing designed with two objectives: maxi-\nmize the use of strong pre-trained decoders such as\nBART, and minimize the complexity and dependen-\ncies of the transition system compared to previous\napproaches. Similarly to Zhou et al. (2021), we\nscan the sentence from left to right and use a to-\nken cursor to point to a source token at each step.\nParser actions can either shift the cursor one to-\nken forward or generate any number of nodes and\nedges while the cursor points to the same token.\nThe proposed set of actions is as follows:\nSHIFT moves token cursor one word to the right.\n<string> creates node of name <string>.\nCOPY creates node where the node name is the\ntoken under the current cursor position.\nLA(j,LBL ) creates an arc with label LBL from\nthe last generated node to the node generated at the\njth transition step.\nRA(j,LBL ) same as LA but with arc direction re-\nversed.\nROOT declares the last predicted node as the root.\nUnlike previous transition-based approaches, we\ndo not use a reserved action, such as PRED (Zhou\net al., 2021) or CONFIRM (Ballesteros and Al-\nOnaizan, 2017b), to predict nodes; 2 instead we\ndirectly use the node name <string> as the action\nsymbol generating that node. This opens the pos-\nsibility of utilizing BART’s target side pre-trained\nvocabulary. We avoid using any copy actions that\ninvolve copying from lemmatizer outputs or lookup\ntables. Our COPY action is limited to copying the\nlower cased word. We also eliminate the use of\nSUBGRAPH (Zhou et al., 2021) or ENTITY (Balles-\nteros and Al-Onaizan, 2017b) actions producing\nmultiple connected nodes simultaneously, as well\nas MERGE action creating spans of words. In previ-\nous approaches these actions were derived from\nalignments or hand-crafted. They thus did not\ncover all possible cases limiting the scalability of\nthe approach. Finally, we discard the REDUCE ac-\ntion previously used to delete a source token. The\neffect can be achieved by simply using SHIFT with-\nout performing any other action. Figure 2 shows an\nexample sentence with an action sequence and the\ncorresponding graph. This can be compared with\nthe handling of verbalization and named entities in\nFigure 1.\nTo train a parser with a transition system, we\nneed an action sequence for each training sentence\nthat will produce the gold graph when executed.\nThis action sequence then serves as the target for\nseq-to-seq models. A simple rule-based oracle al-\ngorithm creates these ground-truth sequences given\n2PRED wraps the node name inside the action as\nPRED (<string>), and CONFIRM calls a subroutine to predict\nthe AMR node and ﬁnd the right propbank sense.\n6282\na sentence, its AMR graph and node-to-word align-\nments. At each step, the oracle tries the following\npossibilities in the order of listing and performs the\nﬁrst one that applies:\n1. Create next gold arc between last created node\nand previously created nodes;\n2. Create next gold node aligned to token under\ncursor;\n3. If not at sentence end, SHIFT source cursor;\n4. Finish oracle.\nIf possible, nodes are generated by COPY and\notherwise with <string> actions. Arcs are gener-\nated with LA and RA, connecting the nodes closer\nto the current node before the ones that are farther\naway. Note that the arcs are created by pointing to\npositions in the action history, where a graph node\nis represented by the action that creates it, follow-\ning Zhou et al. (2021). Multiple nodes can be gen-\nerated at a single source word before the cursor is\nmoved by SHIFT . When multiple nodes are aligned\nto the same token, these nodes are generated in a\npredetermined topological order of graph traver-\nsal, interleaved by edge creation actions. ROOT is\napplied as soon as the root node is generated.\nThe above oracle circumvents the problem of\nunattachable nodes by avoiding the use of subgraph\nactions. This implies that the oracle will always\nproduce a unique action sequence that fully recov-\ners the gold graph as long as every node in the\ngraph is aligned to some token. To guarantee that\nall nodes are aligned, we improve upon the align-\nment system from Naseem et al. (2019); Astudillo\net al. (2020), which aligns a large majority, but\nnot all AMR nodes.3 In order to do this, we apply\na heuristic based on graph proximity to maintain\nlocal correspondences between graph nodes and\nsentence words. If a node is not aligned, we copy\nthe alignment from its ﬁrst child node, if existing,\nand otherwise the alignment from its ﬁrst parent\nnode. For example, in Figure 2 if the node person\nwas not provided with an alignment, our oracle\nwould have assigned it to the aligned token of its\nchild node employ-01. This is a recursive proce-\ndure – as long as there are some alignments to start\nwith and the ground-truth graph is connected, all\nthe nodes will get aligned.\n3Roughly 1% of graphs contain about 1-2 unaligned nodes.\nOur proposed transition system makes better use\nof BART pre-trained decoder compared to previous\ntransition-based approaches (see Section 6) while\ngreatly simplifying the transition set. It also natu-\nrally produces node-to-word alignments via source\ntoken cursor in the meantime.\n4 Parsing Model\nWe build our model on top of the pre-trained seq-\nto-seq Transformer, BART (Lewis et al., 2019).\nWe modify its architecture to incorporate a pointer\nnetwork and internalize parser states induced by\nour transition system, and ﬁne-tune for sentence-\nto-action generation.\n4.1 Structure-aware Architecture\nWe adopt similar modiﬁcations on the Transformer\narchitecture as in Zhou et al. (2021) since our tran-\nsition system is based on the same action-pointer\nmechanism. The modiﬁcations do not introduce\nnew modules or extra parameters, which naturally\nﬁt our need to adapt BART into a transition-based\nparser with internal graph well-formedness.\nIn particular, the target actions are factored into\ntwo parts: bare action symbols (containing labels\nwhen presented) and pointer values for edges. We\nuse the BART standard output for the former, and\na pointer network for the latter. As the pointing\nhappens on the actions history, essentially a self-\nattention mechanism, we re-purpose one decoder\nself-attention head as the pointer network. It is su-\npervised with additional cross entropy loss during\nﬁne-tuning and decoded for building graph edges\nat inference.\nWe encode the monotonic action-source align-\nments induced by the parser state with hard atten-\ntion, i.e. by masking some decoder cross-attention\nheads to only focus on aligned words. Since BART\nprocesses source sentences with subwords, we ap-\nply an additional average pooling layer on top of\nits encoder to return states of original source words,\nused for the decoder layers for our hard attention.\nAt last, as the possible valid actions are constrained\nwith transition rules and states at every step, we\nrestrict the decoder output space via hard masking\nof the BART ﬁnal softmax layer. For simplicity, we\ndo not incorporate the GNN-style (Li et al., 2019)\nstep-wise decoder graph embedding technique in\nZhou et al. (2021) as their gain was shown to be\nmodest.\n6283\n4.2 Action Generation\nAccording to how we treat the target-side vocabu-\nlary for action generation, we propose two vari-\nations of the model. The ﬁrst one is to use a\ncompletely separate vocabulary for target actions,\nwhere the decoder input side and output side use\nstand-alone embeddings for actions, separate from\nthe pre-trained BART subword embeddings.4 We\ndenote this setup as oursep-voc model (abbreviated\nas StructBART-S).\nHowever, this might not fully utilize the power of\nthe pre-trained BART since it is an encoder-decoder\nmodel with a single vocabulary and all embeddings\nshared. Although our generation targets are action\nsymbols, the node generating actions are closely re-\nlated to natural words in their surface forms, which\nare what BART was pre-trained on. Therefore, we\npropose a second variation where we use a joint\nvocabulary for both the source tokens and target\nactions. Naively relying on the original BART\nsubword vocabulary would end up splitting action\nsymbols blindly, which is not desired as the struc-\ntures such as alignments and edge pointers would\nbe disrupted. Inspired by Bevilacqua et al. (2021),\nwe add frequent node-creating actions to the vocab-\nulary, in order to capture common AMR concepts\nintact, and split the remaining concepts with BART\nsubword vocabulary. Non-node-creating actions\nsuch as SHIFT and COPY are added as-is to the\nBART vocabulary.\nIn this setup, a single node string can potentially\nbe generated with multiple steps; we modify the\narc transitions to always point to the beginning po-\nsition of a node string for attachment. With joint\nvocabulary setup, the model could learn to generate\nunseen nodes with BART’s subword vocabulary,\neliminating potential out-of-vocabulary problems.\nWe refer to this setup as our joint-voc model (ab-\nbreviated as StructBART-J).\n4.3 Training and Inference\nWe load the pre-trained BART parameters except\nfor the standalone vocabulary embeddings for sep-\nvoc model and the extended embeddings for the\njoint-voc model. We then ﬁne-tune the model\nwith the updated structure-aware architectures on\nsentence-action pairs with addition of pointer loss.\nFor decoding, we use similar constrained beam\n4In practice the separate embeddings are initialized with\nthe average subword embeddings from the original BART\nvocabulary, which gave small gains over random initialization.\nsearch algorithm as in Zhou et al. (2021), but with\nour own transition set and rules. We run a state\nmachine on the side to get parser states used by the\nmodel. Note that for our joint-voc model, we only\nallow subword split for node (<string>) actions. As\nour ﬁne-tuned model is already structure-aware,\nthe graph well-formedness is always guaranteed\nand no post-processing is needed to return valid\ngraphs, unlike Xu et al. (2020); Bevilacqua et al.\n(2021). The only post-processing we use is to add\nwikiﬁcation nodes as used in all previous parsers.5\n5 Experimental Setup\nDatasets We evaluate our models on 3\nAMR benchmark datasets, namely AMR 1.0\n(LDC2014T12), AMR 2.0 (LDC2017T10), and\nAMR 3.0 (LDC2020T02). They have around 10K,\n37K, and 56K sentence-AMR pairs for training,\nrespectively.6 Both AMR 2.0 and AMR 3.0 have\nwikiﬁcation nodes but AMR 1.0 does not.\nEvaluation We assess our models withSMATCH\n(F1) scores7 (Cai and Knight, 2013). We also re-\nport the ﬁne-grained evaluation metrics (Damonte\net al., 2016) to further investigate different aspects\nof parsing results, such as concept identiﬁcation,\nentity recognition, re-entrancies, etc.\nModel Conﬁguration We follow the original\nBART conﬁguration (Lewis et al., 2019) and code.\n8 We use the large model conﬁguration as default,\nand also the base model for ablation studies. The\npointer network is always tied with one head of\nthe decoder top layer, and the pointer loss is added\nto the model cross-entropy loss with 1:1 ratio for\ntraining. Transition alignments are used to mask\ncross-attention in 2 heads of all decoder layers.\nFor sep-voc model, we build separate embedding\nmatrices for target actions from the training data\nfor decoder input and output space. For joint-voc\nmodel, we add new embedding vectors for non-\nnode action symbols and node action strings with\na default minimum frequency of 5 (only accounts\nfor about one third of all nodes due to sparsity).\n5We also do light cleaning of the decoded AMR when they\nare printed to penman format, such as removing reserved char-\nacters in node concepts and printing disconnected subgraphs.\n6See Appendix A for detailed dataset sizes. Data source:\nhttps://amr.isi.edu/download.html.\n7https://github.com/snowblink14/\nsmatch/tree/v1.0.4.\n8https://github.com/pytorch/fairseq/\ntree/v0.10.2/examples/bart.\n6284\nID Model Pre-trained\nModel\nCollapse\nSubgraph\nExternal Dependency Extra\nData\nTrain\nAlign.\nSMATCHF1 (%)\nPOS NER Lemm. AMR 1.0 AMR 2.0 AMR 3.0\n1 Naseem et al. (2019) BERT S.A. \u0013 \u0013 - 75.5 -\n2 Zhang et al. (2019a) BERT G.R. \u0013 \u0013 70.2±0.1 76.3±0.1 -\n3 Zhang et al. (2019b) BERT G.R. \u0013 \u0013 71.3±0.1 77.0±0.1 -\n4 Cai and Lam (2020) BERT \u0013 \u0013 \u0013 74.0 78.7 -\n5 Cai and Lam (2020) BERT G.R. \u0013 \u0013 \u0013 75.4 80.2 -\n6 Astudillo et al. (2020)RoBERTa S.A. \u0013 \u0013 76.9±0.1 80.2±0.0 -\n7 Lyu et al. (2020) RoBERTa G.R. \u0013 \u0013 - - 75.8\n8 Bevilacqua et al. (2021)BART\u0006 - 83.8 83.0\n9 Bevilacqua et al. (2021)BART\u0006 G.R. \u0013 - 84.5 80.2\n10 Zhou et al. (2021) RoBERTa S.A. \u0013 \u0013 78.3±0.1 81.7±0.1 80.3±0.1\n11 Xu et al. (2020) Custom\u0006 4M - 81.4 -\n12 Lee et al. (2020) RoBERTa S.A. \u0013 85K \u0013 78.2±0.1 81.3±0.0 -\n13 Bevilacqua et al. (2021)BART\u0006 200K - 84.3 83.0\n14 Zhou et al. (2021) RoBERTa S.A. \u0013 70K \u0013 - 82.6 ±0.1 -\n15 StructBART-S BART\u0006 \u0013 81.6±0.1 84.0±0.1 82.3±0.0\n16 StructBART-J BART\u0006 \u0013 81.7±0.2 84.2±0.1 82.0±0.0\n17 StructBART-S BART\u0006 47K \u0013 - - 82.7 ±0.1\n18 StructBART-J BART\u0006 47K \u0013 - 84.7±0.1 82.6±0.1\n19 StructBART-S ensem.BART\u0006 47K \u0013 - - 83.1\n20 StructBART-J ensem.BART\u0006 47K \u0013 - 84.9 -\nTable 1: S MATCH (%) scores on AMR 1.0, 2.0, and 3.0 test data, associated with each model’s dependency on\nvarious resources. 1-10/11-14: previous models without/with extra data; 15-18: our models (-S/-J for separate/joint\nvocabularies for source and target); 19-20: our models with ensemble decoding. Symbols indicate: G.R. - graph\nre-categorization, S.A. - subgraph action used in transition-based parsers (both detailed in Section 2), POS - part\nof speech tagger, NER - named entity recognizer, Lemm. - lemmatizer, Align. - alignments (only used at training\ntime). \u0006 indicates ﬁne-tuning on top of pre-trained model. All models rely on a external wikiﬁcation method\n(ommited). Our results are average of 3 runs with different random seeds. We also report standard deviation and\nprovide ensemble results for the 3 seed combination.\nID Model SMATCH Unlabel NoWSD Concepts NER Negation Wiki. Re-entrancy SRL\n1 Naseem et al. (2019) 75.5 80 76 86 83 67 80 56 72\n4 Cai and Lam (2020) 78.7 81.5 79.2 88.1 87.1 66.1 81.3 63.8 74.5\n6 Astudillo et al. (2020) 80.2 84.2 80.7 88.1 87.5 64.5 78.8 70.3 78.2\n8 Bevilacqua et al. (2021)83.8 86.1 84.4 90.2 90.6 74.4 84.4 70.8 79.6\n10 Zhou et al. (2021) 81.8 85.5 82.3 88.7 88.5 69.7 78.8 71.1 80.8\n15 StructBART-S 84.1 87.5 84.4 90.4 92.2 71.0 79.6 73.9 83.0\n16 StructBART-J 84.3 87.9 84.7 90.6 92.1 72.5 80.8 74.3 83.4\nTable 2: Fine-grained F1 scores on the AMR 2.0 test set, among models that do not use extra silver data and graph\nre-categorization. The model IDs are matched with those in Table 1 for detailed model features. We report results\nwith our single best model (selected on development data) for fair comparison.\nImplementation Details Our models are trained\nwith Adam optimizer with batch size 2048 tokens\nand gradient accumulation of 4 steps. Learning rate\nis 1e−4 with 4000 warm-up steps using the inverse-\nsqrt scheduling scheme (Vaswani et al., 2017). The\nhyper-parameters are ﬁxed and not tuned for dif-\nferent models and datasets, as we found results are\nnot sensitive within small ranges. We train sep-voc\nmodels for 100 epochs and joint-voc models for\n40 epochs as the latter is found to converge faster.\nThe best 5 checkpoints based on development set\nSMATCH from greedy decoding are averaged, and\ndefault beam size of 10 is used for decoding for\nour ﬁnal parsing scores. We implement our model9\nwith the FAIRSEQ toolkit (Ott et al., 2019). More\ndetails can be found in the Appendix.\n6 Results\nMain Results We present parsing performances\nof our model (StructBART) in comparison with\nprevious approaches in Table 1. For each model,\nwe also list its features such as utilization of pre-\n9Code and model available athttps://github.com/\nIBM/transition-amr-parser.\n6285\nTransition System\nFeatures Model Results on AMR 2.0 Model Results on AMR 3.0\n#Base\nActions\nDistant\nEdges\nSpecial\nSubgraph\nAPT∗\n(Zhou et al., 2021)\nStructBART\nsep-voc\nAPT∗\n(Zhou et al., 2021)\nStructBART\nsep-voc\nAstudillo et al. (2020) 12 S WAP merge - - - -\nZhou et al. (2021) 10 pointer merge 81.5 ±0.1 83.4±0.1 79.8±0.1 81.6±0.0\nOurs 6 pointer no 81.6 ±0.1 84.0±0.1 79.6±0.0 82.3±0.1\nTable 3: Transition system comparison, including their effects on different parsing models. ∗ we adopt the cited\nmodel without graph structure embedding to compare and run on our proposed oracle.\nTransition System Avg. #actions Oracle SMATCH\nAstudillo et al. (2020) 76.2 98.0\nZhou et al. (2021) 41.6 98.9\nOurs 45.6 99.9\nTable 4: Average action sequence length and oracle\ncoverage on AMR 2.0 training data from different tran-\nsition systems. Average source sentence length is 18.9.\ntrained language models and graph simpliﬁcation\nmethods such as re-categorization. This gives a\ncomprehensive overview of how systems compare\nin terms of complexity aside from performance.\nAll recent systems rely on pre-trained language\nmodels, either as ﬁxed features or through ﬁne-\ntuning. The pre-trained BART is particularly bene-\nﬁcial due to its encoder-decoder structure. Among\nall the models, the graph linearization models (Xu\net al., 2020; Bevilacqua et al., 2021) have the\nleast number of extra dependencies when not us-\ning graph re-categorization. Our model only re-\nquires aligned training data, a trait common to all\ntransition-based approaches. This bears the advan-\ntage of producing reliable alignments at decoding\ntime, which are useful for downstream tasks and as\nexplanation of the graph constructing process.\nBoth our sep-voc and joint-voc model variations\nwork well on all datasets. Without using extra\nsilver data, our model achieves the SMATCH score\nof 84.2 ±0.1 on AMR 2.0, which is the same as\nthe previous best model (Bevilacqua et al., 2021)\nwith 200K silver data. With the input of only 47K\nsilver data (consisting of ∼20K example sentences\nof propbank frames and randomly selected ∼27K\nSQuAD-2.0 context sentences10), we achieve the\nhighest score of 84.7 ±0.1 for AMR 2.0. We also\nattain the high score of 81.7 ±0.2 on the smallest\nAMR 1.0 benchmark, and the second best score\nof 82.7 ±0.1 on the largest AMR 3.0 benchmark.\nEnsemble of the 3 models from the silver training\nfurther improves the performances to 84.9 for AMR\n10https://rajpurkar.github.io/SQuAD-explorer/.\n2.0 and 83.1 for AMR 3.0.\nFine-grained Results We further examine the\nﬁne-grained parsing results on AMR 2.0 in Ta-\nble 2. We compare models not relying on extra\ndata nor graph re-categorizationn since silver data\nsets differ across methods, and re-categorization\ncomes with limitations outlined in Section 2. Our\nmodels achieve the highest scores across most of\nthe categories, except for negation and wikiﬁcation.\nThe former may be due to alignment errors and\nthe latter is solved as a separate post-processing\nstep independent of the parser. Compared with\nthe closely related model from Bevilacqua et al.\n(2021) which also ﬁne-tunes BART but directly on\nlinearized graphs, we achieve signiﬁcant gains on\nre-entrancy and SRL ( :ARG -I arcs), proving our\nmodel generate AMR graphs more faithful to their\ntopological structures.\n7 Analysis\nTransition System Table 3 and Table 4 com-\npare different transition systems used by recent\ntransition-based AMR parsers with strong perfor-\nmances. Our proposed system has the smallest\nset of base actions, utilizes the action-side pointer\nmechanism for ﬂexible edge creation as in Zhou\net al. (2021), but does not rely on special treat-\nment of certain subgraphs such as named entities\nand dates. This results in slightly longer action\nsequences compared to Zhou et al. (2021), but with\nalmost 100% coverage11 (Table 4). Our transition\nsystem and oracle can always ﬁnd action sequences\nwith full recovery of the original AMR graph, re-\ngardless of graph topology and alignments.\nTo assess whether our proposed transition sys-\ntem helps integration with pre-trained BART, we\ntrain both the APT model from Zhou et al. (2021)\nand our sep-voc model on the transition system of\n11We can recover 100% of AMR 2.0 training graphs ex-\ncluding 4 with notation errors. Imperfect SMATCH is due to\nambiguities of our parser in recovering Penman notation.\n6286\nModel\nVariation\nSMATCH (%)\nsep-voc joint-voc\nOurs (hard attention) 84.0 ±0.1 84.2 ±0.1\nNo align. modeling 83.5 ±0.0 83.4 ±0.2\nAlign. soft supervision 82.9 ±0.0 83.0 ±0.0\nAlign. add src emb. 83.9 ±0.0 84.1 ±0.0\nNo COPY action 83.1 ±0.1 84.1 ±0.0\nTable 5: Ablation study of structure modeling with tran-\nsition alignments. Results are on AMR 2.0 test data.\nZhou et al. (2021) and the one introduced in this\nwork (Table 3 last 4 columns). The APT model,\nbased on ﬁxed RoBERTa features, does not bene-\nﬁt from the proposed transition system. However,\nour proposed model gains 0.6 on AMR 2.0 and 0.7\non AMR 3.0. This conﬁrms the hypothesis that\nthe proposed transitions are better able to exploit\nBART’s powerful language generation ability.\nStructural Alignment Modeling In Table 5, we\nevaluate the effects of our structural modeling\nof parser states within BART during ﬁne-tuning.\nAction-source alignments are natural byproduct of\nthe parser state, providing structural information of\nwhere and how to generate the next graph compo-\nnent. Our default use of hard attention to encode\nsuch alignments works the best. We explore two\nother strategies for modeling alignments. One is to\nsupervise cross-attention distributions for the same\nheads with inferred alignments during training, in-\nspired by Strubell et al. (2018). The other is to\ndirectly add the aligned source contextual embed-\ndings from the encoder top layer to the decoder\ninput at every generation step. The former hurts\nthe model performance, indicating the model is un-\nable to learn the underlying transition logic to infer\ncorrect alignments, while the latter does equally\nwell as our default model. These results justify the\nmodeling of structural constraints, even when ﬁne-\ntuning strong pre-trained models such as BART.\nWe also ablate the use of COPY action in our\ntransition system. The sep-voc model suffers but\nthe joint-voc model is not affected. Without COPY\naction, the joint-voc model would rely more on\nBART’s pre-trained subword embeddings to split\nnode concepts more frequently, while the sep-\nvocab model would need to learn to generate more\nrare concepts from scratch. This indicates that\nBART’s strong generation power is fully used to\ntackle concept sparsity problems with its subwords.\n1 5 10 100\nMinimum Node Frequency\n83.6\n83.8\n84.0\n84.2\n84.4Smatch (%)\n0\n2000\n4000\n6000\n8000\n10000\n12000\nNumber of Node Names\nFigure 3: Effect of special AMR node names added\nto the BART vocabulary in joint-voc model on AMR\n2.0 dataset. Remaining AMR concepts are split and\ngenerated with BART subwords and sense numbers.\n# Model Initialization S MATCH(%)\nsrc emb encoder decoder tgt emb Base Large\n1 71.2 72.7\n2 \u0013 \u0013 71.7 72.8\n3 \u0013 \u0013 \u0013 81.4 82.8\n4 \u0013 \u0013 \u0013 69.2 9.5 ∗\n5 \u0013 71.2 72.8\n6 \u0013 \u0013 80.9 82.5\n7 \u0013 \u0013 \u0013 82.2 83.9\n8 \u0013 \u0013 \u0013 \u0013 82.7 84.1\n9 freeze BART src emb 82.6 84.0\n10 freeze BART src emb + encoder 80.9 81.8\nTable 6: Effects of pre-trained BART parameters. Re-\nsults are with our sep-voc model on AMR 2.0 data. ∗\nfailed to converge with a range of hyper-parameters.\nSpecial Nodes in Joint-Voc In Figure 3, we\nshow the joint-voc model performance with dif-\nferent sized (joint) vocabularies. The vocabulary\nsize is controlled by specifying the minimum fre-\nquency of occurrence needed for an AMR concept\nto be added to the vocabulary. For instance, when\nthe minimum frequency is 1, all 12475 AMR con-\ncepts from the training data are added onto the\nBART vocabulary. The number of added concepts\ndecreases as we increase the minimum frequency\nthreshold. On model performance side, we only\nobserve ±0.2 score variations resulting from vo-\ncabulary expansion. More interestingly, the model\ncan work equally well when no special concepts\nare added to the BART vocabulary (minimum node\nfrequency is ∞) – where all the node names are\nsplit and generated with BART subword tokens.\nAlthough our default setup uses frequency thresh-\nold of 5 in joint-voc expansion, following Bevilac-\nqua et al. (2021), it seems unnecessary in terms\nof achieving good performance. This highlights\nthe efﬁcacy of utilizing the pre-trained BART’s lan-\n6287\nguage generation power for AMR concepts even\nwith relatively small annotated training datasets.\nPre-trained Parameters We study the contribu-\ntion of different pre-trained BART components in\nTable 6. With our sep-voc model, we decompose\nthe whole seq-to-seq Transformer into four com-\nponents for BART initialization, i.e. the source\nembedding (mapped with BART shared embed-\nding), encoder, decoder, and the separate target\nembedding (initialized with the average subword\nembeddings from BART shared embedding). We\nrun both the StructBART base model and the Struct-\nBART large model with different combinations of\nparameter initialization, on the top part of Table 6.\nWe can see that a randomly initialized model of the\nsame size (#1) performs badly. There is an accu-\nmulative effect of BART initialization in helping\nthe model performance, except that BART decoder\ncan not work alone well without its encoder (#4).\nThe encoder gives the largest performance gains\n(#3 vs. #2, #6 vs. #5) of about 10 points. Adding\nthe decoder further gives around 1.4 points on top\n(#7 vs. #6), justifying its importance as well.\nWe also experiment with freezing BART pa-\nrameters during training in the bottom part of Ta-\nble 6. Our results of freezing the BART encoder\nare on similar levels of previous best RoBERTa\nfeature based models, which is behind the full ﬁne-\ntuning. Overall, full initialization from BART with\nstructure-aware ﬁne-tuning (#8) works the best.\n8 Related Work\nUsing seq-to-seq to predict linearized graph se-\nquences for parsing was proposed in Vinyals et al.\n(2015) and is currently a very extended approach\n(Van Noord and Bos, 2017; Ge et al., 2019; Ron-\ngali et al., 2020). However, it is only recently with\nthe rise of pre-trained Transformer decoders, that\nthese techniques have become dominant in seman-\ntic parsing. Xu et al. (2020) proposed custom\nmulti-task pre-training and ﬁne-tuning approach\nfor conventional Transformer models (Vaswani\net al., 2017). The massively pre-trained transformer\nBART (Lewis et al., 2019) was used for executable\nsemantic parsing in Chen et al. (2020) and AMR\nparsing in Bevilacqua et al. (2021). The impor-\ntance of strongly pre-trained decoders seems also\njustiﬁed as BART gains popularity in various se-\nmantic generation tasks (Chen et al., 2020; Shi\net al., 2020). Our work aims at capitalizing on the\noutstanding performance shown by BART, while\nproviding a more structured approach that guaran-\ntees well-formed graphs and yields other desirable\nsub-products such as alignments. We show that\nthis is not only possible but also attains state-of-the\nart parsing results without graph re-categorization.\nOur analysis also shows that contrary to Xu et al.\n(2020), vocabulary sharing is not necessary for\nstrong performance for our structural ﬁne-tuning.\nEncoding of the parser state into neural parsers\nhas been undertaken in various works, including\nseq-to-seq RNN models (Liu and Zhang, 2017;\nZhang et al., 2017; Buys and Blunsom, 2017),\nencoder-only Transformers (Ahmad et al., 2019),\nseq-to-seq Transformers (Astudillo et al., 2020;\nZhou et al., 2021) and pre-trained language models\n(Qian et al., 2021). Here we explore the application\nof these approaches to pre-trained seq-to-seq Trans-\nformers. Borrowing ideas from Zhou et al. (2021),\nwe encode alignment states into the pre-trained\nBART attention mechanism, and re-purpose its self-\nattention as a pointer network. We also rely on a\nminimal set of actions targeted to utilize BART’s\ngeneration with desirable guarantees, such as no\nunattachable nodes and full recovery of all graphs.\nWe are the ﬁrst to explore transition-based parsing\napplied on ﬁne-tuning strongly pre-trained seq-to-\nseq models, and we demonstrate that parser state\nencoding is still important for performance, even\nwhen implemented inside of a powerful pre-trained\ndecoder such as BART.\n9 Conclusion\nWe explore the integration of pre-trained sequence-\nto-sequence language models and transition-based\napproaches for AMR parsing, with the purpose of\nretaining the high performance of the former and\nstructural advantages of the latter. We show that\nboth approaches are complementary, establishing\nthe new state of the art for AMR 2.0. Our results\nindicate that instead of simply converting the struc-\ntured data into unstructured sequences to ﬁt the\nneed of the pre-trained model, it is possible to ef-\nfectively re-purpose a generic pre-trained model\nto a structure-aware one achieving strong perfor-\nmance. Similar principles can be applied to adapt\nother powerful pre-trained models such as T5 (Raf-\nfel et al., 2019) and GPT-2 (Radford et al., 2019) for\nstructured data predictions. It is worth exploring\nthoroughly the pros and cons of introducing struc-\nture to the model compared to removing structure\nfrom the data (linearization) in various scenarios.\n6288\nReferences\nWasi Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard\nHovy, Kai-Wei Chang, and Nanyun Peng. 2019. On\ndifﬁculties of cross-lingual transfer with order differ-\nences: A case study on dependency parsing. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers) , pages 2440–2452,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nRamon Fernandez Astudillo, Miguel Ballesteros,\nTahira Naseem, Austin Blodgett, and Radu Flo-\nrian. 2020. Transition-based parsing with stack-\ntransformers. arXiv preprint arXiv:2010.10669.\nMiguel Ballesteros and Yaser Al-Onaizan. 2017a.\nAMR parsing using stack-LSTMs. In Proceedings\nof the 2017 Conference on Empirical Methods in\nNatural Language Processing , pages 1269–1275,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nMiguel Ballesteros and Yaser Al-Onaizan. 2017b.\nAmr parsing using stack-lstms. arXiv preprint\narXiv:1707.07755.\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina\nGeorgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin\nKnight, Philipp Koehn, Martha Palmer, and Nathan\nSchneider. 2013. Abstract meaning representation\nfor sembanking. In Proceedings of the 7th linguistic\nannotation workshop and interoperability with dis-\ncourse, pages 178–186.\nMichele Bevilacqua, Rexhina Blloshmi, and Roberto\nNavigli. 2021. One spring to rule them both: Sym-\nmetric amr semantic parsing and generation without\na complex pipeline.\nJan Buys and Phil Blunsom. 2017. Robust incremen-\ntal neural semantic graph parsing. arXiv preprint\narXiv:1704.07092.\nDeng Cai and Wai Lam. 2020. AMR parsing via graph-\nsequence iterative inference. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 1290–1301, Online. As-\nsociation for Computational Linguistics.\nShu Cai and Kevin Knight. 2013. Smatch: an evalua-\ntion metric for semantic feature structures. In Pro-\nceedings of the 51st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 748–752.\nXilun Chen, Asish Ghoshal, Yashar Mehdad, Luke\nZettlemoyer, and Sonal Gupta. 2020. Low-resource\ndomain adaptation for compositional task-oriented\nsemantic parsing. arXiv preprint arXiv:2010.03546.\nMarco Damonte, Shay B Cohen, and Giorgio Satta.\n2016. An incremental parser for abstract meaning\nrepresentation. arXiv preprint arXiv:1608.06111.\nDongLai Ge, Junhui Li, Muhua Zhu, and Shoushan Li.\n2019. Modeling source syntax and semantics for\nneural amr parsing. In IJCAI, pages 4975–4981.\nPavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Ravis-\nhankar, Salim Roukos, Alexander Gray, Ramón Fer-\nnandez Astudillo, Maria Chang, Cristina Cornelio,\nSaswati Dana, Achille Fokoue, Dinesh Garg, Alﬁo\nGliozzo, Sairam Gurajada, Hima Karanam, Naweed\nKhan, Dinesh Khandelwal, Young-Suk Lee, Yunyao\nLi, Francois Luus, Ndivhuwo Makondo, Nandana\nMihindukulasooriya, Tahira Naseem, Sumit Neelam,\nLucian Popa, Revanth Gangi Reddy, Ryan Riegel,\nGaetano Rossiello, Udit Sharma, G P Shrivatsa Bhar-\ngav, and Mo Yu. 2021. Leveraging Abstract Mean-\ning Representation for knowledge base question an-\nswering. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n3884–3894, Online. Association for Computational\nLinguistics.\nYoung-Suk Lee, Ramon Fernandez Astudillo, Tahira\nNaseem, Revanth Gangi Reddy, Radu Florian,\nand Salim Roukos. 2020. Pushing the limits of\namr parsing with self-learning. arXiv preprint\narXiv:2010.10673.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nMichael Lingzhi Li, Meng Dong, Jiawei Zhou, and\nAlexander M Rush. 2019. A hierarchy of graph neu-\nral networks based on learnable local features.arXiv\npreprint arXiv:1911.05256.\nFei Liu, Jeffrey Flanigan, Sam Thomson, Norman\nSadeh, and Noah A Smith. 2018. Toward abstrac-\ntive summarization using semantic representations.\narXiv preprint arXiv:1805.10399.\nJiangming Liu and Yue Zhang. 2017. Encoder-decoder\nshift-reduce syntactic parsing. In Proceedings of\nthe 15th International Conference on Parsing Tech-\nnologies, pages 105–114, Pisa, Italy. Association for\nComputational Linguistics.\nChunchuan Lyu, Shay B Cohen, and Ivan Titov. 2020.\nA differentiable relaxation of graph segmentation\nand alignment for amr parsing. arXiv preprint\narXiv:2010.12676.\nChunchuan Lyu and Ivan Titov. 2018. AMR parsing as\ngraph prediction with latent alignment. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 397–407, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nArindam Mitra and Chitta Baral. 2016. Addressing a\nquestion answering challenge by combining statis-\ntical methods with inductive rule learning and rea-\nsoning. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 30.\n6289\nTahira Naseem, Srinivas Ravishankar, Nandana Mihin-\ndukulasooriya, Ibrahim Abdelaziz, Young-Suk Lee,\nPavan Kapanipathi, Salim Roukos, Alﬁo Gliozzo,\nand Alexander Gray. 2021. A semantics-aware\ntransformer model of relation linking for knowledge\nbase question answering. ACL.\nTahira Naseem, Abhishek Shah, Hui Wan, Radu Flo-\nrian, Salim Roukos, and Miguel Ballesteros. 2019.\nRewarding Smatch: Transition-based AMR parsing\nwith reinforcement learning. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4586–4592, Florence,\nItaly. Association for Computational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensi-\nble toolkit for sequence modeling. arXiv preprint\narXiv:1904.01038.\nPeng Qian, Tahira Naseem, Roger Levy, and Ramón\nFernandez Astudillo. 2021. Structural guidance for\ntransformer language models. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 3735–3745, Online. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nSubendhu Rongali, Luca Soldaini, Emilio Monti, and\nWael Hamza. 2020. Don’t parse, generate! a se-\nquence to sequence architecture for task-oriented se-\nmantic parsing. In Proceedings of The Web Confer-\nence 2020, pages 2962–2968.\nPeng Shi, Patrick Ng, Zhiguo Wang, Henghui\nZhu, Alexander Hanbo Li, Jun Wang, Cicero\nNogueira dos Santos, and Bing Xiang. 2020. Learn-\ning contextual representations for semantic pars-\ning with generation-augmented pre-training. arXiv\npreprint arXiv:2012.10309.\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for semantic\nrole labeling. arXiv preprint arXiv:1804.08199.\nRik Van Noord and Johan Bos. 2017. Neural seman-\ntic parsing by character-based translation: Experi-\nments with abstract meaning representations. arXiv\npreprint arXiv:1705.09980.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nOriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,\nIlya Sutskever, and Geoffrey Hinton. 2015. Gram-\nmar as a foreign language. In Advances in neural\ninformation processing systems, pages 2773–2781.\nAndreas Vlachos et al. 2018. Guided neural lan-\nguage generation for abstractive summarization us-\ning abstract meaning representation. arXiv preprint\narXiv:1808.09160.\nChuan Wang and Nianwen Xue. 2017. Getting the\nmost out of amr parsing. In Proceedings of the\n2017 conference on empirical methods in natural\nlanguage processing, pages 1257–1268.\nChuan Wang, Nianwen Xue, and Sameer Pradhan.\n2015. A transition-based algorithm for amr parsing.\nIn Proceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 366–375.\nDongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, and\nGuodong Zhou. 2020. Improving amr parsing with\nsequence-to-sequence pre-training. arXiv preprint\narXiv:2010.01771.\nSheng Zhang, Xutai Ma, Kevin Duh, and Ben-\njamin Van Durme. 2019a. Amr parsing as\nsequence-to-graph transduction. arXiv preprint\narXiv:1905.08704.\nSheng Zhang, Xutai Ma, Kevin Duh, and Benjamin\nVan Durme. 2019b. Broad-coverage semantic pars-\ning as transduction. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 3784–3796, Hong Kong, China. As-\nsociation for Computational Linguistics.\nZhirui Zhang, Shujie Liu, Mu Li, Ming Zhou, and En-\nhong Chen. 2017. Stack-based multi-layer attention\nfor transition-based dependency parsing. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1677–\n1682, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nJiawei Zhou, Tahira Naseem, Ramón Fernandez As-\ntudillo, and Radu Florian. 2021. Amr parsing with\naction-pointer transformer. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 5585–5598, On-\nline. Association for Computational Linguistics.\n6290\nA Dataset Statistics\nWe list the dataset sizes of AMR benchmarks in\nTable 7. The sizes increase with the release version\nnumber. AMR 2.0 is the most used by far. AMR\n2.0 shares the same set of sentences for develop-\nment and test data with AMR 1.0, but with revised\nannotations and wikiﬁcation links. AMR 3.0 is\nreleased most recently, which is under-explored.\nOur silver data originate from two sources.\nFirst, we use ∼20K example sentences ( ∼386K\ntokens) from the propbank frames included in\nthe AMR 3.0 distribution. Second, we use\nrandomly selected ∼27K sentences ( ∼620K to-\nkens) from SQuAD 2.0 context sentences avail-\nable from https://rajpurkar.github.\nio/SQuAD-explorer/.\nData Split AMR 1.0 AMR 2.0 AMR 3.0\nTraining 10,312 36,521 55,635\nDevelopment 1,368 1,368 1,722\nTest 1.371 1.371 1,898\nTable 7: Number of sentence-AMR instances in the\nAMR benchmark datasets.\nB Details of Model Structures and\nNumber of Parameters\nIn Table 8, we list the detailed model conﬁguration\nand number of parameters of the ofﬁcial pre-trained\nBART models. Our ﬁne-tuned StructBART is with\ndifferent action vocabulary strategies which builds\nadditional embedding vectors for certain action\nsymbols. The numbers vary from training dataset.\nWe list the detailed number of parameters of our\nﬁne-tuned model in Table 9. The ﬁne-tuned model\nonly increases about 3%-8% more parameters for\nsep-voc model and 0.4%-1% more parameters for\njoint-voc model.\nC Implementation Details\nWe use the Adam optimizer with β1 = 0.9 and\nβ2 = 0.98. Batch size is set to 2048 maximum\nnumber of tokens, and gradient is accumulated\nover 4 steps. The the learning rate schedule is\nthe same as Vaswani et al. (2017), where we use\nthe maximum learning rate of 1e−4 with 4000\nwarm-up steps. Dropout of rate 0.2 and label\nsmoothing of rate 0.01 are used. These hyper-\nparameters are ﬁxed and not tuned for different\nmodels and datasets, as we found results are not\nConﬁguration BART Base BART Large\nEncoder layers 6 12\nHeads per layer 12 16\nHidden size 768 1024\nFFN size 3072 4096\nSize of vocab 51201 ∗ 50265\nSize of emb. matrix 39,322,368 51,471,360\n#parameters trained 140,139,266 406,291,458\nTable 8: Original model conﬁgurations of\npre-trained BART from FAIRSEQ (https:\n//github.com/pytorch/fairseq/tree/\nv0.10.2/examples/bart). The embeddings\nfor source, decoder input and output are all shared\nand thus the same (not counted as extra in training\nparameters). ∗vocabulary for the base model is larger\ndue to additional paddings at the end, but effective\nvocabulary symbols are the same as the large model.\nModel Param. AMR 1.0 AMR 2.0 AMR 3.0\nsep-voc\nSrc vocab size 50265 50265 50265\nTgt vocab size 6976 12752 16180\n#param. trained 420,578,304 432,407,552 439,436,288\njoint-vocjoint vocab size 51921 53487 54388\n#param. trained 407,987,200 409,590,784 410,517,504\nTable 9: Model parameters of our StructBART (large\nmodel). The sizes differ based on the target side vo-\ncabulary, which is dependent on different training data.\nAddition of silver training data adds only a fraction of\nthe parameters to the benchmark datasets.\nsensitive within small ranges. Without silver data,\nwe train sep-voc models for 100 (AMR 1.0 & 2.0)\nor 120 (AMR 3.0) epochs and joint-voc models\nfor 40 epochs as the latter is found to converge\nfaster. The best 5 (AMR 1.0 & 2.0) or 10 (AMR\n3.0) checkpoints among the last 40/30 epochs are\nselected based on development set SMATCH from\ngreedy decoding and averaged over the model pa-\nrameters as our ﬁnal model. With the 50K silver\ndata, we train both sep-voc and joint-voc models\nfor 20 epochs and select the best 10 checkpoints for\nmodel parameter averaging. We use a default beam\nsize of 10 for decoding for our ﬁnal parsing scores.\nOur models are implemented with the FAIRSEQ\ntoolkit (Ott et al., 2019), trained and tested on a\nsingle Nvidia Tesla V100 GPU with 32GB mem-\nory. We use fp16 mixed precision training when-\never possible, with which training a large model\non AMR 2.0 takes about 10 hours for sep-vocab\nmodels and 7 hours for joint-vocab models, and the\ntime varies proportionally with data size for other\ndatasets and with silver data.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8570258617401123
    },
    {
      "name": "Parsing",
      "score": 0.7160825133323669
    },
    {
      "name": "Transformer",
      "score": 0.5337294936180115
    },
    {
      "name": "Natural language processing",
      "score": 0.5337123870849609
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5301737785339355
    },
    {
      "name": "Sentence",
      "score": 0.4921697974205017
    },
    {
      "name": "Graph",
      "score": 0.48426052927970886
    },
    {
      "name": "Language model",
      "score": 0.46796590089797974
    },
    {
      "name": "Encoder",
      "score": 0.4478394687175751
    },
    {
      "name": "Exploit",
      "score": 0.44125866889953613
    },
    {
      "name": "Theoretical computer science",
      "score": 0.28881722688674927
    },
    {
      "name": "Voltage",
      "score": 0.09817010164260864
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    }
  ]
}