{
  "title": "DISCO: Distilling Counterfactuals with Large Language Models",
  "url": "https://openalex.org/W4385569878",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2360328110",
      "name": "Chen, Zeming",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A4202063421",
      "name": "Gao, Qiyue",
      "affiliations": [
        "Seattle University",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4227134328",
      "name": "Bosselut, Antoine",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A4224468494",
      "name": "Sabharwal, Ashish",
      "affiliations": [
        "Seattle University",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2560525202",
      "name": "Richardson, Kyle",
      "affiliations": [
        "Seattle University",
        "Allen Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962727366",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4310513087",
    "https://openalex.org/W2964044490",
    "https://openalex.org/W3105928338",
    "https://openalex.org/W3177575812",
    "https://openalex.org/W2798665661",
    "https://openalex.org/W2970019270",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W3101853775",
    "https://openalex.org/W3005343217",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3103649165",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W4225468040",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3035032873",
    "https://openalex.org/W2963542100",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3103873238",
    "https://openalex.org/W2977235550",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4293328703",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963394326",
    "https://openalex.org/W4312516176",
    "https://openalex.org/W2963120843",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2951674308",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W3009380369",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3174057701",
    "https://openalex.org/W2889663513",
    "https://openalex.org/W3035139434",
    "https://openalex.org/W3179534853",
    "https://openalex.org/W3105261549",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3175039711",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2970453125",
    "https://openalex.org/W4385574293",
    "https://openalex.org/W2785896739"
  ],
  "abstract": "Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce DISCO (DIStilled COunterfactual Data), a new method for automatically generating high-quality counterfactual data at scale. DISCO engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data. While task-agnostic, we apply our pipeline to the task of natural language inference (NLI) and find that on challenging evaluations such as the NLI stress test, comparatively smaller student models trained with DISCO-generated counterfactuals are more robust (6% absolute) and generalize better across distributions (2%) compared to models trained without data augmentation. Furthermore, DISCO-augmented models are 10% more consistent between counterfactual pairs on three evaluation sets, demonstrating that DISCO-augmentation enables models to more reliably learn causal representations. Our repository are available at: https://github.com/eric11eca/disco",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5514–5528\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDISCO: Distilling Counterfactuals with Large Language Models\nZeming Chen†∗ Qiyue Gao‡∗ Antoine Bosselut† Ashish Sabharwal‡ Kyle Richardson‡\n† Natural Language Processing Lab, EPFL, Lausanne, Switzerland\n{zeming.chen, antoine.bosselut}@epfl.ch\n‡ Allen Institute for AI, Seattle, U.S.A.\n{bertg, kyler, ashishs}@allenai.org\nAbstract\nModels trained with counterfactually aug-\nmented data learn representations of the causal\nstructure of tasks, enabling robust generaliza-\ntion. However, high-quality counterfactual data\nis scarce for most tasks and not easily generated\nat scale. When crowdsourced, such data is typi-\ncally limited in scale and diversity; when gen-\nerated using supervised methods, it is compu-\ntationally expensive to extend to new counter-\nfactual dimensions. In this work, we introduce\nDISCO (DIStilled COunterfactual Data), a\nnew method for automatically generating high-\nquality counterfactual data at scale. DISCO\nengineers prompts to generate phrasal pertur-\nbations with a large general language model.\nThen, a task-specific teacher model filters these\ngenerations to distill high-quality counterfac-\ntual data. While task-agnostic, we apply our\npipeline to the task of natural language infer-\nence (NLI) and find that on challenging evalua-\ntions such as the NLI stress test, comparatively\nsmaller student models trained with DISCO-\ngenerated counterfactuals are more robust (6%\nabsolute) and generalize better across distribu-\ntions (2%) compared to models trained with-\nout data augmentation. Furthermore, DISCO-\naugmented models are 10% more consistent be-\ntween counterfactual pairs on three evaluation\nsets, demonstrating that DISCOaugmentation\nenables models to more reliably learn causal\nrepresentations. Our repository are available at:\nhttps://github.com/eric11eca/disco\n1 Introduction\nDespite the tremendous progress made in NLP on\na wide range of reasoning tasks (Wang et al., 2018,\n2019a; Xu et al., 2020), dataset biases continue\nto be a formidable challenge for robust model de-\nvelopment (Gururangan et al., 2018; Poliak et al.,\n2018; Kaushik and Lipton, 2018; Tsuchiya, 2018;\nLiu et al., 2020b; Du et al., 2022). Counterfactual\n∗ Work done while at the Allen Institute for AI. Equal\ncontribution\nSpan  \nExtraction\nCounterfactual\nData\nEasy\nAmbigious\nHard\n \nCounterfactual \nOver-generation\nData  \nSelection\nSeed\nDataset\nOriginal: A young girl looks up \nas she rides on a merry-go-round.\nSpans: A young girl looks up \nas she rides on a merry-go-round.\nCounterfactual: A young girl\nlooks up at a very tall roller\ncoaster with an eager and excited\nlook on her face.\nGeneral LLM\n(GPT-3)\nSpecialized \nTeacher Model \nFiltering \nStudent Model \nData\nAugmentation\nFigure 1: Overview of our counterfactual data distilla-\ntion process (DISCO) using a large language model.\ndata augmentation (CAD) (Kaushik et al., 2019) is\none general approach to improve model robustness\nby training on edited instances that systematically\nalter the critical or causally salient parts of dataset\ninstances that contributes to the label assignment.\nTo date, two main approaches have been pursued as\npart of these efforts: human-centered approaches,\nwhere edits are obtained through direct human an-\nnotation and crowdsourcing (Kaushik et al., 2019;\nKhashabi et al., 2020; Gardner et al., 2020); and\nmodel-based approaches, where new examples are\ncollected through automatic text generation (Wu\net al., 2021; Madaan et al., 2021; Ross et al., 2022;\nWen et al., 2022, inter alia).\nHowever, crowd-sourcing counterfactual data\ncan be inefficient, costly, and difficult to scale. This\noften results in small counterfactual datasets, which\ncan hinder the diversity and coverage of the col-\nlected edits (e.g., in Kaushik et al. (2019), the train-\n5514\ning scenario for NLI involves 8.3k total instances\nwith augmentation). In contrast, supervised text\ngeneration methods are cheaper and easier to scale\n(e.g., Wu et al. (2022) use generation methods that\nexpand NLP datasets to include around a million\ntotal examples). However, such methods can only\ngenerate fixed perturbation types. They rely on a\nfixed inventory of perturbation types each requir-\ning new training sets. This is hard to scale up and\ncan limit the space of perturbation types learned by\nthe corresponding learned generation models. They\ncan also be expensive to extend to new perturbation\ntypes, given the need to retrain models.\nIn this paper, we focus on the Natural Lan-\nguage Inference (NLI) task, which has recently\nbeen shown to benefit from collaboration between\nhuman annotation and LLMs in the WANLI data\naugmentation system of Liu et al. (2022). Our pri-\nmary contribution is a counterfactual knowledge\ndistillation procedure called DISCO (DIStilled\nCOunterfactual Data), which works in the follow-\ning way (see Figure 1): First, task instances to be\nedited are selected and decomposed into spans us-\ning off-the-shelf linguistic processing tools. Then\nprompt engineering and in-context learning are ap-\nplied with a general LLM to overgenerate a di-\nverse set of perturbations for these instances. We\nthen employ a large teacher NLI model to con-\nservatively filter the over-generations as a fully-\nautomatic alternative to the human filtering used\nin WANLI. The distilled generations are finally\nused to train a much smaller and high-performance\nstudent model.\nWe show that DISCO, despite not relying\non explicit human annotation, yields high-quality\ndatasets. Manual annotation shows that, on average,\n83% of our counterfactual data correctly flips the\nsource labels, which is 1% higher than human per-\nformance. Additionally, compared to human CAD\nexamples (Kaushik et al., 2019), we find DISCO\ngenerated data to have much-improved perturba-\ntion and information richness. Through data aug-\nmentation experiments, we also find that training\non datasets built using DISCO obtains competi-\ntive and often improved performance across a wide\nrange of robustness and out-of-domain (OOD) NLI\ntests, despite having a significantly smaller size\nthan existing augmentation approaches (75k vs. 1\nmillion from Wu et al. (2022)). This includes con-\nsistent improvements (6% average) over WANLI\nand SNLI baselines on 7 NLI robustness tests.\nBuilding on the impressive results from Liu et al.\n(2022), this is significant as it shows the promis-\ning potential of data augmentation via LLMs, even\nwithout explicit human annotation. We find that\nmodels trained using our data exhibit 8% improved\ncounterfactual accuracy and 6% increased sensitiv-\nity to context differences between counterfactual\npairs than SNLI baselines. When augmenting on\ntop of WANLI, our method shows an 18% perfor-\nmance gain on counterfactual accuracy.\nContributions In summary, we presentDISCO,\na fully-automatic counterfactual knowledge distil-\nlation approach based on LLMs. To our knowledge,\nDISCO is the first to use LLMs such as GPT3 for\ncounterfactual data augmentation. We show that\nour approach helps produce more diverse counter-\nfactuals over existing crowd-sourcing approaches\nwhile showing higher quality than human-written\ndata. The distilled counterfactual data is more ef-\nfective than existing augmentation approaches for\nimproving NLI robustness, OOD generalization,\nand counterfactual consistency.\n2 Related Work\nMitigating Spurious Correlations for NLU\nThe augmentation methods described above are\npart of a large literature on model debiasing ap-\nproaches, which also includes work on dataset fil-\ntering (Bras et al., 2020), model ensembling (Clark\net al., 2019), feature removal, and other learning-\nbased techniques (Belinkov et al., 2019; Mahabadi\net al., 2020). Wu et al. (2022) also propose a new\ndebiasing method called Z-Aug that learns to gen-\nerate unbiased samples and filter out biased data\nusing a z-statistic filter. In contrast to the debiasing\nand data generation techniques already discussed,\nour approach is unique in exploiting the power of\nLLMs such as GPT3 (Brown et al., 2020) to cre-\nate more diverse augmented datasets as a way to\nmitigate biases and shortcuts.\nCounterfactual Data Augmentation Augment-\ning models with counterfactual data is a popular\nrecent approach for mitigating spurious correlation\nand improving model robustness. Kaushik et al.\n(2019) first recruits human workers to write coun-\nterfactual examples for augmentation. They find\nthat counterfactually augmented data can help mit-\nigate spurious patterns in the training data. As\nalready discussed, however, creating counterfac-\ntual data using humans requires a high cost, is\n5515\ntime-consuming, and can result in simple pertur-\nbations. Later, Wu et al. (2021) and Ross et al.\n(2022) proposed frameworks that use text genera-\ntion models to generate counterfactual data. These\nmodels require fine-tuning using pre-defined per-\nturbation types. Both methods have constraints: (1)\nthe generation is un-targeted, thus unlabeled, and\n(2) the perturbation types are limited. To acquire\nnew perturbation types, the models have to be re-\ntrained. Unlike the previous methods, our method\nuses LLMs to generate more diverse perturbation\ntypes cheaply and efficiently. Our method also\nimproves over un-targeted generations by using a\ntask-specific teacher model to verify the label.\nLarge Model Dataset CreationLeveraging the\npowerful generative ability of large language mod-\nels to create datasets automatically has recently\nattracted considerable attention. This method re-\nduces the cost of manually creating the dataset,\ncan collect more diverse phenomena to expand the\ndistribution, and can be adapted to a wide range\nof tasks in NLP. The most similar work to ours is\nW ANLI (Liu et al., 2022), an NLI dataset fully gen-\nerated by GPT-3 and annotated by human workers.\nThe idea is to elicit ambiguous NLI examples from\nGPT-3 to improve its performance on challenge\nevaluation benchmarks, which relies on the dataset\ncartography techniques from Swayamdipta et al.\n(2020) that we also use in our study for selecting\ninstances to edit. Our work also seeks to get diverse\ndata from GPT-3 to improve model robustness. Dif-\nferently, we only make local perturbations on the\npremise instead of generating a new example. We\ndid not label our training data using human work-\ners but leveraged an NLI model to filter out the\ncounterfactual examples.\n3 Counterfactual Distillation\nThe central idea of counterfactual data distilla-\ntion is to prompt a large language model through\nin-context learning to generate perturbations that\ncan flip the current label to a new one (ex.\nContradiction → Entailment). Once we se-\nlect a subset of a dataset (discussed in Section 5.1),\nwe first identify potential locations for perform-\ning counterfactual perturbations on the target in-\nstances. Then we prompt the GPT-3 (text-DaVinci-\n002) model to overgenerate perturbations (3.1). We\nuse a teacher language model specializing in the\nNLI task to filter the generated perturbations based\non the shift in model predictions from the orig-\ninal to the new label (3.2). Formally, given an\ninput premise-hypothesis pair <P,H >,l where\nl ∈{Entailment,Contradiction,Neutral }is\nthe ground-truth label. We want to get a counter-\nfactual input < P′,H >,l′ where we get P′ by\nperturbing parts of the premise P and l′is the new\nlabel corresponding to the new input.\n3.1 Prompting\nWe experiment with various prompting strategies\non GPT-3, detailed and illustrated in Figure 2. To\nmake local edits to a sentence following CAD\n(Kaushik et al., 2019)’s procedure, we use a neural\nsyntactic parser (Akbik et al., 2019) to split sen-\ntences to perturb into spans. Using this neural chun-\nker, we can get a set of spans S= {s: s∈P}de-\ncomposed from the premise P. These spans serve\nas the potential locations for making a perturbation.\nMasked Prompting. To prompt GPT-3 for coun-\nterfactual perturbations, we use a masked NLI for-\nmat to build the prompt. Let P and H be the\npremise and hypothesis pair we want to perturb,\nassociated with the current label l and the set of\nspans S. We select one span from Sand replace it\nin the premise with a mask token [blank]. Given\na new label l′we want to flip to, we ask the model\nto fill in the blank mask token with creative per-\nturbation s′to get a new premise P′that satisfies\nl′. Here the perturbation serves as an intervention\nin flipping the original label to the new label. Be-\ncause during the generation time, one can not know\nwhich span will flip the label after perturbation, we\novergenerate perturbations by iterating through all\nthe spans from a premise. Each span yields a new\nprompt and makes a new request to GPT-3.\nInsertion Mode. One of the key features of\nGPT-3 is its insertion mode, which allows users\nto insert a piece of text into the current con-\ntext and have the model generate text based on\nthe surrounding context. We can naturally con-\nvert the masked-NLI prompt into an insertion\nprompt format by providing the surrounding text\nof the mask token to the model. By forming\na natural sentence, we try to align the prompt\nto the pre-training objective of GPT-3 (e.g., ca-\nsual language modeling). We first map the label\nspace {Entailment,Contradiction,Neutral }\nto {true,false,possible }. Then we build the\nprompt: \"<Prefix> [insert] <Suffix>. It is <l′> that\n<H>\", where l′is the new label.\n5516\n \nComplete the context with creative content, \nso the conclusion is true based on the context.\nContext: \nA young girl looks up [blank].\nConclusion:\nThe little girl can't wait to ride the roller coaster.\n[blank] should be: \nInput\nPrompt\nInstruction\nOriginal Premise: \nA young girl looks up as she rides a merry-go-round.\nHypothesis: \nThe little girl can't wait to ride the roller coaster.\nLabel: Neutral                New Label: Entailment \nat a very tall roller coaster with an eager\nand excited look on her face\nPrompt\n A young girl looks up [insert]. It is true that \nthe little girl can't wait to ride the roller coaster.\nIn-context Examples\nFigure 2: Overview of the perturbation generation process on GPT-3 using the masked NLI prompt (right) and\nthe insertion NLI prompt (left-bottom). Here we are editing the input premise and hypothesis for the given NLI\nproblem to change the label from Neutral to Entailment. Thus we have \"conclusion is true“ in both prompts.\nThe advantage of using the insertion mode is that\nthe model considers both the prefix and suffix con-\ntext of the masked span. This solves a common is-\nsue in the completion mode where the model tends\nto finish a sentence when generating the perturba-\ntion without noticing the suffix context. Addition-\nally, the insertion mode does not require in-context\nlearning examples, which yields more diverse gen-\nerations at a much lower cost.\n3.2 Teacher Model Filtering\nUsing a combination of the prompting strategies\ndetailed in the last section, we then implement a\nfiltering system to select the most promising coun-\nterfactual examples, pruning out potential mistakes\nmade by GPT3. The filtering system first uses a\nheuristic-based automatic filter to remove any gen-\nerations that yield obvious signs of low quality,\nensuring that the remaining perturbations are more\nlikely to flip the label in the desired direction. Our\ncheck for several criteria, including:\n1. Does the perturbation contain parts from the\ninstruction or prompt?\n2. Does the perturbation copy parts from the in-\ncontext examples?\n3. Does the perturbation repeat parts from the\npremise or hypothesis?\nUsing a count of the lexical overlap rate between\nsentences and a pre-defined set of common\nnegation words, we also remove any perturbations\nwith clear data artifacts, such as excessive lexical\noverlap between premise and hypothesis or using\nnegation words as a shortcut to flip the label. After\nthe automatic filtering, we distill the remaining\ndata using a model-based teacher, which identifies\nthe perturbations that convert the original label\nto the target label. To verify if a perturbation\npotentially converts the original label in the\ndirection of the new label, a natural way would be\nto check if the prediction probability of the new\nlabel shifts by a large margin, given the new input\nand the original input. Specifically, we calculate\nthe distributional shift as follows:\n∆l′ = p(l′|P′,H) −p(l′|P,H), (1)\nwhich yields the change in prediction probability\nfrom the original input to the new input. We use a\nDeBERTa-v2 (He et al., 2020) model with SOTA\nperformance on NLI as the teacher model. Addi-\ntional details about the prompting parameters and\nteacher model can be found in Appendix A.\n4 Evaluate Counterfactual Quality\nLarge general language models like GPT-3 enable\nthe generation of counterfactual data at a large\nscale. The generation process is more efficient,\ncheap, and flexible than crowdsourcing. Here we\nevaluate the quality and diversity of DISCO data\nagainst counterfactually augmented data written\nby human workers (Human-CAD) (Kaushik et al.,\n2019) using automatic and human-based metrics.\n5517\nFlip Rate Score↑Data E2C E2N N2C N2E C2N C2E Avg.\nHuman-CAD86.37 82.36 86.08 84.34 73.42 82.28 82.55DISCO(ours)78.53 82.70 76.20 85.53 75.76 92.43 83.14\nSoft Flip Rate Score↑E2C E2N N2C N2E C2N C2E Avg.\nHuman-CAD94.32 83.33 88.61 86.75 82.28 94.94 88.24DISCO(ours)97.55 88.46 76.20 89.47 92.42 95.45 93.33\nSelf-BLEU Diversity Score↓E2C E2N N2C N2E C2N C2E Avg.\nHuman-CAD0.76 0.75 0.82 0.82 0.81 0.79 0.79DISCO(ours)0.23 0.26 0.26 0.18 0.25 0.21 0.23\nOTDD Dataset Distance↑E2C E2N N2C N2E C2N C2E Avg.\nHuman-CAD217 95 179 139 238 217 180DISCO(ours)250 199 254 165 275 301 240\nTable 1: Automatic and human evaluation results on a\nrandom subset (510 instances) of our counterfactual data\n(DISCO), compared with Human-CAD (Kaushik et al.,\n2019), counterfactual data written by human workers.\n4.1 Automatic Evaluation\nDiversity Measurement Following other work\non CAD (Wu et al., 2021), we use Self-BLEU (Zhu\net al., 2018) to measure the diversity of the gen-\nerated counterfactual examples. In Table 1, we\nlist the Self-BLEU score for each perturbation di-\nrection. Compared to human-written examples,\nGPT-3 generated examples have much lower Self-\nBLEU scores than human-written ones indicating\nthat GPT-3 can generate far more diverse examples.\nDataset Distance The Self-BLEU score mea-\nsures lexical and syntactic diversity only. To assess\nthe diversity of information in the data, we calcu-\nlate the dataset distance between the original exam-\nples and the new examples. Specifically, we mea-\nsure dataset distance via OTDD (optimal transport\ndataset distance) (Alvarez-Melis and Fusi, 2020), a\nmodel-agnostic distance metric that can operate on\ndatasets with disjoint label sets. OTDD can mea-\nsure how well the knowledge from one dataset can\ntransfer to another. We use OTDD to assess the\ndistributional difference between the original and\nnew examples. As Table 1 shows, our generated\nexamples have a higher distance from the original\nexamples than the human-written data, consistently\nin all directions. This trend demonstrates that our\ncounterfactual data provide more diverse informa-\ntion than human-written data.\n4.2 Human Evaluation\nLabel-Flip Score The label-flip score is an\naccuracy-based metric to check if the new exam-\nple after perturbation forms a counterfactual to the\noriginal example. We check the flip score in two\naspects. The Label Flip Rate (LFR) calculates the\npercentage of new examples that flip the original\nlabel to the target label. The Soft Label Flip Rate\n(SLFR) calculates the percentage of new examples\nwhose label differs from the original example’s la-\nbel. SLFR measures how often LLMs generate\nvalid counterfactuals independent of whether the\nnew label is right. Given the rigidness of LFR and\nthe fluidity of some NLI judgements (Pavlick and\nKwiatkowski, 2019), this last metric is meaningful\nfor checking if we still generate valid counterfac-\ntuals even when the exact label is not correct. The\nhigh SLFR suggests that many examples not ac-\ncepted by the filter could be valid counterfactuals\nmaking them useful for other types of learning (e.g.,\nleveraging signals from such data to train models\nto identify counterfactuals). For a dataset with K\nexamples, we calculate FLR and SFLR as follows:\nLFR = 1\nK\nK∑\nk=1\n1 (˜lk = l′\nk)\nSLFR = 1\nK\nK∑\nk=1\n1 (˜lk ̸= lk),\nwhere ˜lis the annotated label, l′is the target label,\nand lis the original label.\nWe use Amazon Mechanic Turk to conduct hu-\nman evaluations, asking annotators to label a ran-\ndom subset of our data following the standard an-\nnotation process for the NLI task. We assigned\nthree annotators for each example and did major-\nity voting on the annotated labels. We list more\ndetails on the instructions, interface, and annotator\nrequirements in Appendix B. We only give anno-\ntators the new sentence pairs to avoid bias from\nthe original example. Table 1 shows the human\nevaluation results in each perturbation direction.\nCompared to human-written examples, DISCO\nhas lower LFRs only on generating contradictions,\nshowing that GPT-3 generates better entailment\nand neutral examples rather than contradiction ex-\namples. We hypothesize that this is due to the\nambiguous boundary between contradiction and\nneutral examples. Moreover, generating contradic-\ntions while maintaining diversity is difficult. When\nasked to generate contradictions, they tend to gen-\nerate neutral examples by changing a sentence’s\nsemantics (i.e., adding diversified words). In the\ncase of Human-CAD, annotators tend to create con-\n5518\nDataset Focus Size\nPI-CD (a) Partial-input heuristics 3261\nPI-SP (b) Partial-input heuristics 371\nIS-CS (c) Inter-sentences Heuristics 656\nLI-LI (d,e) Logical Inference Ability 9927\nLI-TS (f,g) Logical Inference Ability 9832\nST (e) Stress (distraction & noise) test 93447\nHANS (h) Syntactic Heuristic 30000\nMNLI-hard-m Out-of-distribution 4573\nMNLI-hard-mm Out-of-distribution 4530\nQNLI Out-of-distribution 5266\nHuman-CAD Counterfactual consistency 1600\nSNLI-hard2→ Counterfactual consistency 3042\nW ANLI2→ Counterfactual consistency 4000\n(a) Gururangan et al. (2018) (b) Liu et al. (2020a)\n(c) Nie et al. (2019) (d) Glockner et al. (2018)\n(e) Naik et al. (2018) (f) Minervini and Riedel (2018)\n(g) Wang et al. (2019b) (h) McCoy et al. (2019)\nTable 2: Details about the evaluation datasets we used\nfor the experiments.\ntradictions using simple tricks like negation (Joshi\nand He, 2022). Although these tricks can produce\nabsolute contradiction examples, they can intro-\nduce strong data artifacts, leading to a model that\nis not robust. Overall, the human evaluation scores\nshow that our distilled counterfactual data exceeds\nhuman-written examples in correctly flipping the\nlabel, as shown by a higher average flip rate score.\n5 Experiments\n5.1 Counterfactual Data Augmentation\nWe next investigate how distilled GPT-3 counter-\nfactual data can improve model robustness and gen-\neralizability through data augmentation. Given\na set of original data D = {X,Y}, we gener-\nate a perturbation z for each example in a subset\nof D(Ds = {Xs,Ys}), and convert the original\none to a counterfactual example: Dc = {(xc =\nz(x),y′)|x ∈ Xs,y ∈ Ys}. Next, we augment\nthis subset by merging it with the counterfactual\nexamples: Da = Ds ∪Dc. For additional data\naugmentation, we also select a base set Db (a ran-\ndom subset from D), merge it with the augmenta-\ntion set Da and remove any duplicated examples:\nDtrain = Db ∪Da −Dd. We use models trained\non base sets Db alone as baselines and evaluate\nwhether augmenting the base sets using DISCO\ndata would improve the baselines’ performances\nfollowing Z-aug (Wu et al., 2022) and WANLI\n(Liu et al., 2022). We train a smaller student model,\nbased on RoBERTa-large (355 million parame-\nters) using the implementation from Wolf et al.\n(2020), on Dtrain and Da. Then, we evaluate the\nmodel on a set of test datasets for measuring ro-\nbustness and OOD generalizability.\nSource Datasets We select SNLI (Bowman et al.,\n2015) as the source dataset for generating DISCO\ndata and for data augmentation. SNLI is a widely-\nused NLI dataset employed in numerous research\nstudies. We apply data cartography (Swayamdipta\net al., 2020) to select the ambiguous part of SNLI.\nThe paper suggests that training on ambiguous data\nyields more robust models. Our intuition is that en-\nhancing the ambiguous set with counterfactual ex-\namples would benefit the model’s learning. We also\naugment DISCO on W ANLI (Liu et al., 2022) to\nanalyze the benefits of counterfactual data augmen-\ntation on a dataset constructed via human-GPT-3\ncollaboration.\nEvaluation Datasets We first evaluate how ro-\nbust model performance is under adversarial and\nstress tests. We select the adversarial datasets\nfrom Liu et al. (2020b)’s benchmark for debias-\ning strategies and NLI stress test suite from Naik\net al., 2018’s work. Next, we evaluate the model’s\ngeneralizability across different distributions. We\nselect two datasets with a different distribution\nfrom the SNLI dataset: MNLI-hard (matched and\nmismatched) (Mahabadi et al., 2020), and QNLI\n(Wang et al., 2018), a dataset adapted from the\nStanford Question Answering Dataset (Rajpurkar\net al., 2016). Details about the evaluation datasets\nare included in Table 2.\nComparisons For naive comparison, we evalu-\nate our models against baselines trained Db only\nwithout data augmentation. Then, we compare\nour models to prior augmentation methods, includ-\ning Tailor (Ross et al., 2022), WANLI (Liu et al.,\n2022), Z-aug (Wu et al., 2022), and Human-CAD\n(Kaushik et al., 2019). For W ANLI and Z-aug, we\nalso augment them on the full SNLI training set\nbecause of their large dataset sizes. In addition,\nwe fine-tune a model only on DISCO to compare\nwith all the models above (see Appendix A for\nmore details about training and hyper-parameters).\nResults Table 3 shows that our counterfactual\ndata augmentation significantly improves over the\nbaseline performance on most robustness datasets\nwhen augmenting the DISCO dataset on a subset\nof SNLI. Augmenting or training with DISCO\ndata achieves the highest accuracy on 7 evaluation\n5519\nModel Robustness OOD Generalization\nMethod Size PI-CD PI-SP IS-CS LI-LI LI-TS ST HANS MNLI 1 MNLI2 QNLI\nLarge-size augmentation on full SNLI\nSNLI 549,367 82.2 69.0 68.4 93.6 72.5 72.4 73.1 78.5 78.2 64.5\n+ W ANLI 652, 252 83.4 82.7 69.5 86.2 84.3 67.4 87.4 78.2 78.0 78.6\n+ Z-aug 1,142,475 84.1 72.5 72.6 93.9 87.1 75.4 68.3 80.0 80.7 75.0\nAugmentation on subset of SNLI\nSNLI-subset 100,000 82.0 71.7 65.1 85.5 83.9 69.5 65.8 78.0 79.1 73.4\n+ Tailor 192,457 79.5 52.0 55.8 84.6 80.1 62.7 55.8 64.1 65.7 71.4\n+ Human-CAD 108,330 82.8 77.8 69.2 90.7 87.1 71.3 65.5 79.0 79.0 72.8\n+DISCO(ours) 165,418 84.1 74.1 73.5 92.1 88.4 77.0 70.1 80.5 80.2 77.7\nAugmentation on WANLI\nW ANLI 102,885 65.6 81.3 65.9 65.6 82.7 56.5 89.4 76.1 76.3 81.1\n+DISCO(ours) 177,885 82.8 83.8 72.0 86.8 85.1 68.6 87.4 80.0 78.7 81.4\nTrained onDISCO(ours) data only\nDISCO(ours) 75,000 83.5 77.4 73.3 89.4 88.9 76.3 70.7 79.2 79.5 79.1\nTable 3: Results on Stress-tests, robust NLI test suites (Liu et al., 2020b), MNLI-hard, and QNLI. The bold numbers\nare the highest accuracy within a column, and the underlined numbers are the highest accuracy for each section.\nMNLI1 refers to MNLI-hard-match, and MNLI2 refers to MNLI-hard-mismatch.\nsets. When augmenting on W ANLI, the augmented\nmodel achieved better average performance (75.1)\non robustness than the baseline WANLI model\n(65.9). We list the average performance gain for\nrobustness and OOD generalization in Table 4. We\ncan see that DISCO-augmented models improve\nmodel robustness over baselines by a large margin\n(6.5 SNLI and 9.5 WANLI). These results show\nthe efficacy of our counterfactual data in helping\nmodels mitigate multiple types of NLI data bias\naltogether. On out-of-distribution (OOD) gener-\nalization, models trained on DISCO augmented\ndata achieve a positive performance gain of 2.7\n% over the SNLI subset baseline and 2.1% over\nthe W ANLI baseline. This suggests that augment-\ning with DISCO helps the model generalize to\ndatasets with distributional shifts. Compared to\nprior data augmentation methods, DISCO data\ncan more significantly improve model performance,\nshowing that our method yields high-quality and\neffective augmentation data.\nIn addition, DISCO is much smaller than other\naugmentation data like W ANLI and Z-aug. Interest-\ningly, training onDISCOdata yields better perfor-\nmance than these models trained on large datasets\n(on 7 datasets).\n5.2 Counterfactual Evaluation\nIn our second experiment, we investigate how\nDISCO data can enhance counterfactual reason-\ning ability of models on NLI problems. Coun-\nterfactual reasoning is the ability to predict how\nTest Metrics Original Augmented∆\nSNLI-SUB\nRobustness Avg. 71.0 77.5 6.5\nOOD Avg. 76.7 79.4 2.7\nAcc2→Avg. 47.1 55.2 8.1\nδs Avg. 58.6 64.9 6.3\nW ANLI\nRobustness Avg. 65.9 75.1 9.2\nOOD Avg. 78.0 80.1 2.1\nAcc2→Avg. 34.6 52.7 18.1\nδs Avg. 44.9 57.6 12.7\nTable 4: Performance gain of data augmentation using\nDISCO from baselines without augmentation (i.e., us-\ning the base sets in the first column).\nan alternative context, contrary to the present con-\ntext, might have resulted in different outcomes (Qin\net al., 2019). In the setting of NLI, we alter the cur-\nrent context with text perturbations sufficient to\nchange the current label to a different one while\nspuriously correlated features remain identical. A\nmodel that relies heavily on spurious features will\nlikely fail to predict both the original and counter-\nfactual examples correctly (Feder et al., 2022).\nEvaluation Datasets We first create two counter-\nfactual evaluation datasets using GPT-3 to generate\nthe perturbations. We recruit human workers on\nAmazon Mechanic Turk to annotate labels for the\ntwo datasets. SNLI-hard2→is constructed us-\ning a subset of the SNLI-hard (Gururangan et al.,\n2018) dataset. We pair each original example with\nthe generated counterfactual example, where hu-\nman annotators provide the gold label. In addi-\n5520\ntion, we want to construct a dataset different from\nDISCO’s distribution. Thus, we select a subset\nfrom the W ANLI test set and follow the same pro-\ncedure as SNLI-hard 2→to get a counterfactual\nevaluation set WANLI2→. We assign three human\nworkers to each problem to annotate the label. We\nlist more details on the instructions, interface, and\nannotator requirements in Appendix B. We also\ninclude the Human-CAD dataset as the examples\nwere written and labeled by human workers.\nMetrics We measure models’ counterfactual rea-\nsoning ability along two dimensions. First, we\nmeasure counterfactual sensitivityδs: how confi-\ndently a model differentiates the original and coun-\nterfactual examples. In other words, how confi-\ndently does it assign a different label when there\nis a causal change in the input. Specifically, we\ndefine δs ∈[0,1] as:\nδs = (p(ˆl′|x′) −p(ˆl′|x)) + (p(ˆl|x) −p(ˆl|x′))\n2 ,\nwhere x = (P,H) is the original input and x′is\nits perturbation. Intuitively, this metric quantifies\nthe amount of shift in model predictions between\nthe two related examples. Unchanged model pre-\ndiction results in a sensitivity of 0. When model\nprediction changes with extremely high confidence\n(i.e., assigning 100% on its predicted labels), δs is\nnormalized to 1. In binary classification, when the\npredicted label changes, the metric simplifies to:\nδs = p(ˆl′|x′) + p(ˆl|x) −1.\nδs here measures the model’s confidence in pre-\ndiction when the context changes, shown by the\nprobability it assigns to the predicted labels. In\ngeneral, the higher the δs, the more sensitive the\nmodel is to context changes in the input.\nNext, we measure the counterfactual accuracy\nAcc2→. Under this metric, a prediction is correct\nonly when the model correctly predicts the original\nand counterfactual examples. We use counterfac-\ntual accuracy to measure the consistency of model\nperformance on original and counterfactual exam-\nples. Acc2→is defined as:\n1\nK\nK∑\nk=1\n1\n(\n(ˆlk|Pk, Hk) =l∗\nk) ∧ (ˆl′\nk|P′\nk, Hk) =l′∗\nk )\n)\n,\nwhere Kis the number of examples evaluated, ˆl,ˆl′\nare model predictions for the original and coun-\nterfactual examples, and l∗, l′∗are the gold labels,\nHuman-CAD SNLI-hard2→ WANLI2→\nMethod δs Acc2→δs Acc2→ δs Acc2→\nSNLI-subset 62.8 59.1 66.1 51.1 51.3 39.3\n+ Tailor 58.8 55.6 60.6 55.6 33.9 23.7\n+ Human-CAD70.9 63.6 73.6 54.1 34.6 42.8\n+DISCO(ours) 69.4 64.1 74.3 60.3 55.9 47.7\nW ANLI 41.4 30.5 47.4 27.0 44.5 42.1\n+DISCO(ours) 65.6 64.9 68.5 59.2 46.1 42.8\nDISCO(ours) 65.7 66.5 71.2 63.1 41.9 48.3\nTable 5: Performance on the counterfactual sensitiv-\nity tests including three datasets: Human-CAD, SNLI-\nhard2→, AND W ANLI2→. The models used for evalu-\nation are from the first experiment directly.\nrespectively. This is similar in spirit to evalua-\ntions based on contrast sets from Gardner et al.\n(2020), perturbation clustersfrom Khashabi et al.\n(2020), and the grouped probe metricof Trivedi\net al. (2020).\nResults Table 5 shows models’ performance on\nthe three counterfactual evaluation sets. Models\naugmented or trained with DISCO consistently\noutperform the baseline models by a large margin.\nTraining with only DISCO achieves the highest\ncounter accuracy while augmenting DISCO on\nthe SNLI subset achieves the highest counterfac-\ntual sensitivity. This shows that our data helps\nincrease the model’s ability to differentiate the two\nexamples and improve its reasoning performance\non counterfactual data. Compared to other data\naugmentation methods, DISCO yields a perfor-\nmance gain on both metrics showing its benefit on\ncounterfactual reasoning.\nDISCO increases the WANLI baseline’s sen-\nsitivity and accuracy by more than 20% and\n30% respectively on both Human-CAD and SNLI-\nhard2→. However, the increase on WANLI2→is\nmarginal, which is likely because DISCO and the\nWANLI train set have very different distributions\n(OTDD distance 579). Although WANLI 2→is\nclose to the W ANLI train set (OTDD distance 270),\ntraining on it yields lower accuracy than DISCO,\nindicating that human-GPT-3 collaborated data con-\nstruction does not necessarily grant models the abil-\nity to reason on counterfactual data. Thus, we can\nconfirm that the distillation step on top of GPT-3\ngeneration is essential for improving the model’s\ncounterfactual reasoning ability.\n5521\n6 Conclusion\nIn this paper, we introduced the DISCO frame-\nwork for distilling high-quality counterfactual data\nfrom large language models (LLMs) using a task-\nspecific teacher model for NLI. Through automatic\nand human evaluations, we show that counterfac-\ntuals generated by LLMs have higher quality and\naccuracy than human-written examples while hav-\ning more diverse perturbations. Our evaluation\nresults suggest that training or augmenting with dis-\ntilled counterfactual data can help mitigate various\ntypes of distinct spurious patterns. Counterfactual\nexamples produced by DISCO significantly ben-\nefit model performance with improved robustness\nand out-of-distribution (OOD) generalizability. De-\nspite a smaller data size, DISCOdata help models\nachieve better performance on the evaluation sets\nthan baselines with extensive data. Furthermore,\ntraining on DISCOexamples improves model per-\nformance on counterfactual accuracy and helps the\nmodel be more sensitive to the context changes\nbetween counterfactual and original examples.\nFor future work, our method suggests several\ndirections. While our efforts are limited to NLI,\ngenerating counterfactual data using LLMs is more\ngeneral and, we believe, can be fruitfully ap-\nplied to a wider range of tasks. In specific, only\na task-specific filter model and modification to\nLLM prompts are needed to extend our genera-\ntion pipeline to other tasks or even other languages.\nAlso, while our approach takes inspiration from\nknowledge distillation (Hinton et al., 2015) ap-\nproaches and relies on a teacher filtering model,\nalternative strategies could be used to improve the\nquality. As a related direction, techniques for semi-\nsupervised learning over unfiltered LLM output\nshould also be investigated to help utilize the wide\nrange of data produced by LLMs.\n7 Limitations\nWhile we have argued that our approach to col-\nlecting counterfactual data via DISCO is agnostic\nto the particular task and language, we emphasize\nthat the experiments we report are limited to En-\nglish and the task of NLI. Given that English is a\nhigh-resource language, there could be additional\nchallenges (e.g., finding the tools needed for mak-\ning span selection) in re-creating our pipeline for\nother languages. We also emphasize that our data\ngeneration experiments were carried out using only\na single LLM, namely the publicly available GPT3\nmodel first reported in Brown et al. (2020).\nAs with the related studies we cite (e.g., Liu et al.\n(2022)), given the high costs associated with large-\nscale prompting, we are unable to ablate all parts\nof our data generation pipeline (e.g., the effect of\nsystematically alternating prompting styles at scale,\nalternative span extraction techniques). Similar to\nvirtually all experiments involving LLM prompting,\nsuch differences could affect the results and quality\nof the resulting augmentation datasets. Similarly,\ngiven the high costs of human annotation, we have\nlimited our human evaluation to around 500 ran-\ndom instances (each involving 3 annotators), which\nfollows other related studies.\nAcknowledgements\nWe thank the anonymous reviewers for their con-\nstructive and thoughtful comments. We also thank\nthe members of the Aristo team at AI2 for pro-\nviding helpful feedback on earlier versions of this\nwork. Thanks finally to the beaker.org team at AI2\nfor their assistance and help with experiments and\ncomputing infrastructure.\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\nFLAIR: An easy-to-use framework for state-of-the-\nart NLP. In Proceedings of NAACL.\nDavid Alvarez-Melis and Nicolò Fusi. 2020. Geometric\ndataset distances via optimal transport. In Proceed-\nings of NeurIPS.\nYonatan Belinkov, Adam Poliak, Stuart M Shieber, Ben-\njamin Van Durme, and Alexander M Rush. 2019.\nDon’t take the premise for granted: Mitigating arti-\nfacts in natural language inference. Proceedings of\nACL.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of EMNLP.\nRonan Le Bras, Swabha Swayamdipta, Chandra Bha-\ngavatula, Rowan Zellers, Matthew E. Peters, Ashish\nSabharwal, and Yejin Choi. 2020. Adversarial filters\nof dataset biases. Proceedings of ICML.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Proceedings of NeurIPS.\n5522\nChristopher Clark, Mark Yatskar, and Luke Zettlemoyer.\n2019. Don’t take the easy way out: Ensemble based\nmethods for avoiding known dataset biases. Proceed-\nings of EMNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL.\nMengnan Du, Fengxiang He, Na Zou, Dacheng Tao, and\nXia Hu. 2022. Shortcut learning of large language\nmodels in natural language understanding: A survey.\nAmir Feder, Katherine A. Keith, Emaad Manzoor, Reid\nPryzant, Dhanya Sridhar, Zach Wood-Doughty, Ja-\ncob Eisenstein, Justin Grimmer, Roi Reichart, Mar-\ngaret E. Roberts, Brandon M. Stewart, Victor Veitch,\nand Diyi Yang. 2022. Causal inference in natural lan-\nguage processing: Estimation, prediction, interpreta-\ntion and beyond. Transactions of the Association for\nComputational Linguistics, 10:1138–1158.\nMatt Gardner, Yoav Artzi, Victoria Basmova, Jonathan\nBerant, Ben Bogin, Sihao Chen, Pradeep Dasigi,\nDheeru Dua, Yanai Elazar, Ananth Gottumukkala,\net al. 2020. Evaluating models’ local decision bound-\naries via contrast sets. Findings of EMNLP.\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\n2018. Breaking nli systems with sentences that re-\nquire simple lexical inferences. Proceedings of ACL.\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy,\nRoy Schwartz, Samuel R Bowman, and Noah A\nSmith. 2018. Annotation artifacts in natural language\ninference data. Proceedings of NAACL.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\nNitish Joshi and He He. 2022. An investigation of the\n(in)effectiveness of counterfactually augmented data.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3668–3681, Dublin, Ireland.\nAssociation for Computational Linguistics.\nDivyansh Kaushik, Eduard Hovy, and Zachary C Lipton.\n2019. Learning the difference that makes a difference\nwith counterfactually-augmented data. Proceedings\nof ICLR.\nDivyansh Kaushik and Zachary C. Lipton. 2018. How\nmuch reading does reading comprehension require?\na critical investigation of popular benchmarks. In\nProceedings of EMNLP.\nDaniel Khashabi, Tushar Khot, and Ashish Sabharwal.\n2020. More bang for your buck: Natural perturba-\ntion for robust question answering. Proceedings of\nEMNLP.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. Proceedings of\nICLR.\nAlisa Liu, Swabha Swayamdipta, Noah A Smith, and\nYejin Choi. 2022. WANLI: Worker and AI collabo-\nration for natural language inference dataset creation.\nFindings of EMNLP.\nTianyu Liu, Zheng Xin, Baobao Chang, and Zhifang Sui.\n2020a. HypoNLI: Exploring the artificial patterns of\nhypothesis-only bias in natural language inference.\nIn Proceedings of LREC.\nTianyu Liu, Zheng Xin, Xiaoan Ding, Baobao Chang,\nand Zhifang Sui. 2020b. An empirical study on\nmodel-agnostic debiasing strategies for robust nat-\nural language inference. In Proceedings of CoNLL.\nNishtha Madaan, Inkit Padhi, Naveen Panwar, and Dip-\ntikalyan Saha. 2021. Generate your counterfactuals:\nTowards controlled counterfactual generation for text.\nIn Proceedings of AAAI.\nRabeeh Karimi Mahabadi, Yonatan Belinkov, and James\nHenderson. 2020. End-to-end bias mitigation by\nmodelling biases in corpora. Proceedings of ACL.\nR Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. Proceedings\nof ACL.\nPasquale Minervini and Sebastian Riedel. 2018. Adver-\nsarially regularising neural nli models to integrate log-\nical background knowledge. Proceedings of CoNLL.\nAakanksha Naik, Abhilasha Ravichander, Norman\nSadeh, Carolyn Rose, and Graham Neubig. 2018.\nStress test evaluation for natural language inference.\nProceedings of COLING.\nYixin Nie, Yicheng Wang, and Mohit Bansal. 2019.\nAnalyzing compositionality-sensitivity of nli models.\nProceedings of the AAAI.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of ACL.\nEllie Pavlick and Tom Kwiatkowski. 2019. Inherent\ndisagreements in human textual inferences. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:677–694.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language infer-\nence. In Proceedings of *SEM.\nLianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra\nBhagavatula, Elizabeth Clark, and Yejin Choi. 2019.\nCounterfactual story reasoning and generation. In\nProceedings of EMNLP.\n5523\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. Proceedings of\nEMNLP.\nAlexis Ross, Tongshuang Wu, Hao Peng, Matthew E Pe-\nters, and Matt Gardner. 2022. Tailor: Generating and\nperturbing text with semantic controls. Proceedings\nof ACL.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A Smith,\nand Yejin Choi. 2020. Dataset cartography: Map-\nping and diagnosing datasets with training dynamics.\nProceedings of EMNLP.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFever: a large-scale dataset for fact extraction and\nverification. Proceedings of NAACL.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2020. Is multihop QA in\nDiRe condition? Measuring and reducing discon-\nnected reasoning. In Proceedings of EMNLP.\nMasatoshi Tsuchiya. 2018. Performance impact caused\nby hidden bias of training data for recognizing textual\nentailment. Proceedings of LREC.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019a. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. Proceedings of NeurIPS, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. Proceedings of\nICLR.\nHaohan Wang, Da Sun, and Eric P. Xing. 2019b. What\nif we simply swap the two text fragments? a straight-\nforward yet effective way to test the robustness of\nmethods to confounding signals in nature language\ninference tasks.\nJiaxin Wen, Yeshuang Zhu, Jinchao Zhang, Jie Zhou,\nand Minlie Huang. 2022. Autocad: Automatically\ngenerating counterfactuals for mitigating shortcut\nlearning. Proceedings of EMNLP Findings.\nAdina Williams, Nikita Nangia, and Samuel R Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. Proceedings\nof NAACL.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of EMNLP.\nTongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and\nDaniel S Weld. 2021. Polyjuice: Generating coun-\nterfactuals for explaining, evaluating, and improving\nmodels. Proceedings of ACL.\nYuxiang Wu, Matt Gardner, Pontus Stenetorp, and\nPradeep Dasigi. 2022. Generating data to mitigate\nspurious correlations in natural language inference\ndatasets. Proceedings of ACL.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,\nYudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong\nYu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,\nYiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang,\nWeijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,\nYiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao,\nQipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang\nYang, Kyle Richardson, and Zhenzhong Lan. 2020.\nCLUE: A Chinese language understanding evaluation\nbenchmark. In Proceedings of COLING.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan\nZhang, Jun Wang, and Yong Yu. 2018. Texygen: A\nbenchmarking platform for text generation models.\n5524\nA Hyper-parameters and Implementation\nGPT3 and Teacher Model For perturbation\novergeneration, we use GPT-3 with the text-\nDaVinci-002 version. We set thetemperature to 0.8\nto encourage creative generations. For the penalties,\nwe set the frequency penaltyand presence penalty\nto 0.8 to lower the likelihood of sampling repeated\nwords. To mitigate error propagation from the filter-\ning step, we use a publicly available DeBERTa-v2\n(He et al., 2020) model checkpoint (containing 1.3\nbillion parameters) trained on a mixture of NLI\ndatasets, including SNLI (Bowman et al., 2015),\nMultiNLI (Williams et al., 2018), FEVER (Thorne\net al., 2018), ANLI (Nie et al., 2020), that achieves\nSOTA performance on these datasets.\nStudent Models and Training ProtocolFor all\nexperiments, we tuned Robert-large (containing\n345 million parameters) via a random search over\nkey hyper-parameters in the style of Devlin et al.\n(2019). We used ADAM (Kingma and Ba, 2015) as\nour optimizer. The key hyper-parameters include\nlearning rate(including 2e −5,3e −5,5e −5),\nbatch size(between 32,64), warmup ratio(in the\nrange of 0.08,0.1) and number of epochs(3 to\n5); weight decay was kept constant at 0.1 fol-\nlowing Liu et al. (2022), and early stopping was\nused with a patience of 2 epochs. We generally\nfound the following configuration to yield good per-\nformance: LR=3e −5, epochs=3, batch_size=64,\nwarmup_ration=0.1. Standardly, model selection\nwas performed by choosing the model with the\nhighest validation accuracy. In our main result\ntables (i.e., Tables 3-4) we report the best of 5 mod-\nels based on random restarts with different random\nseeds in all rows excluding the first 3. In the first 3\nrows, given the large size of the training sets and\nthe generally high cost of fine-tuning, we report the\nbest single run (and generally found these models\nto yield low variance across hyper-parameters).\nWhen comparing against other data augmenta-\ntion approaches, e.g., Z-aug (Wu et al., 2022), we\nused the exact code base compared with models\ntrained on DISCO to remove any differences in\nimplementation (our implementation is based on\nthe transformers library (Wolf et al., 2020)). All\nexperiments were performed on an NVIDIA RTX\nA6000 GPU.\nB Human Annotation Details\nWe recruit human annotators to evaluate our gener-\nated counterfactual data and to annotate two eval-\nuation sets for counterfactual consistency: SNLI-\nhard2→and WANLI2→. Here we discuss the de-\ntails of our annotation studies. Screenshots of the\ninstructions, guidelines, and annotation interface\nare shown in Fig 3 and Fig 4.\nAnnotators We recruit human workers on the\nAmazon Mechanical Turk 1 platform. We required\nMechanical Turk Masters to perform our tasks. An-\nnotators must have a HIT approval rate of 98%, a\ntotal of 1000 approved HITs, and be in the United\nStates. Throughout the data collection process,\nwe randomly select a subset of the annotations to\ncheck and correct any potentially controversial an-\nnotations. For each problem, we assign three an-\nnotators and use a majority vote to determine the\nfinal annotation. Workers were paid $0.3 for each\nAMT hit (consisting of 10 examples to annotate).\n1https://www.mturk.com/\n5525\nFigure 3: The annotated examples with explanations used on Amazon Mechanical Turk.\nFigure 4: Instructions provided to human annotators on Amazon Mechanical Turk and the annotation interface.\n5526\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 7\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 8\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract, Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3, 4, 5.1, 5.2\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3, 4, 5.1, 5.2\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nTable 2, 3\nC □\u0013 Did you run computational experiments?\nSection 5.1, 5.2\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix A\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5527\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 5.1, 5.2; Appendix A\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5.1, 5.2\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3.1\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 4.2, 5.2\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nAppendix B\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nAppendix B\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n5528",
  "topic": "Counterfactual thinking",
  "concepts": [
    {
      "name": "Counterfactual thinking",
      "score": 0.962661862373352
    },
    {
      "name": "Counterfactual conditional",
      "score": 0.7988564968109131
    },
    {
      "name": "Computer science",
      "score": 0.7366611957550049
    },
    {
      "name": "Task (project management)",
      "score": 0.6783144474029541
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5771867632865906
    },
    {
      "name": "Generalization",
      "score": 0.5457212328910828
    },
    {
      "name": "Machine learning",
      "score": 0.49819183349609375
    },
    {
      "name": "Pipeline (software)",
      "score": 0.491479754447937
    },
    {
      "name": "Scale (ratio)",
      "score": 0.48546627163887024
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.45937642455101013
    },
    {
      "name": "Natural language processing",
      "score": 0.41244256496429443
    },
    {
      "name": "Mathematics",
      "score": 0.1491837203502655
    },
    {
      "name": "Psychology",
      "score": 0.10776311159133911
    },
    {
      "name": "Engineering",
      "score": 0.07613569498062134
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}