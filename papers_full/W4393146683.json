{
    "title": "Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning",
    "url": "https://openalex.org/W4393146683",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2314529275",
            "name": "Zhongzhi Chen",
            "affiliations": [
                "Tencent (China)",
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2289511184",
            "name": "Xingwu Sun",
            "affiliations": [
                "Tencent (China)",
                "University of Macau"
            ]
        },
        {
            "id": "https://openalex.org/A2025561485",
            "name": "Xianfeng Jiao",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2624764249",
            "name": "Fengzong Lian",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2227302744",
            "name": "Zhanhui Kang",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2107274244",
            "name": "Di Wang",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2269845662",
            "name": "Chengzhong Xu",
            "affiliations": [
                "University of Macau"
            ]
        },
        {
            "id": "https://openalex.org/A2314529275",
            "name": "Zhongzhi Chen",
            "affiliations": [
                "Beihang University",
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2289511184",
            "name": "Xingwu Sun",
            "affiliations": [
                "Tencent (China)",
                "University of Macau"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4389519449",
        "https://openalex.org/W4282968607",
        "https://openalex.org/W4389519585",
        "https://openalex.org/W4319300158",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W3162926177",
        "https://openalex.org/W4389520749",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4307123345",
        "https://openalex.org/W4221164017",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4379928343",
        "https://openalex.org/W3093871960",
        "https://openalex.org/W4223908421",
        "https://openalex.org/W4378771713",
        "https://openalex.org/W4380353763",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W4310926773",
        "https://openalex.org/W2606347107",
        "https://openalex.org/W2973379954",
        "https://openalex.org/W4389518784"
    ],
    "abstract": "Despite the great success of large language models (LLMs) in various tasks, they suffer from generating hallucinations. We introduce Truth Forest, a method that enhances truthfulness in LLMs by uncovering hidden truth representations using multi-dimensional orthogonal probes. Specifically, it creates multiple orthogonal bases for modeling truth by incorporating orthogonal constraints into the probes. Moreover, we introduce Random Peek, a systematic technique considering an extended range of positions within the sequence, reducing the gap between discerning and generating truth features in LLMs. By employing this approach, we improved the truthfulness of Llama-2-7B from 40.8% to 74.5% on TruthfulQA. Likewise, significant improvements are observed in fine-tuned models. We conducted a thorough analysis of truth features using probes. Our visualization results show that orthogonal probes capture complementary truth-related features, forming well-defined clusters that reveal the inherent structure of the dataset.",
    "full_text": "Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models\nthrough Intervention without Tuning\nZhongzhi Chen1,2*, Xingwu Sun2,3*, Xianfeng Jiao2, Fengzong Lian2\nZhanhui Kang2, Di Wang2, Cheng-Zhong Xu3\n1Beihang University\n2Tencent Inc.\n3University of Macau\njongjyh@buaa.edu.cn, {sammsun,xfengjiao,faxonlian,kegokang,diwang}@tencent.com, czxu@um.edu.mo\nAbstract\nDespite the great success of large language models (LLMs)\nin various tasks, they suffer from generating hallucinations.\nWe introduce Truth Forest, a method that enhances truthful-\nness in LLMs by uncovering hidden truth representations us-\ning multi-dimensional orthogonal probes. Specifically, it cre-\nates multiple orthogonal bases for modeling truth by incor-\nporating orthogonal constraints into the probes. Moreover,\nwe introduce Random Peek, a systematic technique consid-\nering an extended range of positions within the sequence, re-\nducing the gap between discerning and generating truth fea-\ntures in LLMs. By employing this approach, we improved the\ntruthfulness of Llama-2-7B from 40.8% to 74.5% on Truth-\nfulQA. Likewise, significant improvements are observed in\nfine-tuned models. We conducted a thorough analysis of truth\nfeatures using probes. Our visualization results show that\northogonal probes capture complementary truth-related fea-\ntures, forming well-defined clusters that reveal the inherent\nstructure of the dataset.\n1 Introduction\nLarge language models are known to generate complex and\nunverifiable answers, often referred to as hallucinations.\nStudies show that advanced LLMs, like GPT-4, produce con-\nfusing statements without verification (Li et al. 2023a).\nIncorporating external knowledge can partially address\nhallucination issues (Li et al. 2023a), but methods like\nprompting or self-checking without additional knowledge\nalso yield improvements (Manakul, Liusie, and Gales 2023;\nSaunders et al. 2022). Research on extracting knowledge\nnetworks from LLMs (Wang, Liu, and Song 2020) reveals\nthat these models possess more knowledge than initially as-\nsumed.\nLLMs sometimes generate incorrect answers due to\nmisalignment between internal states and outputs, a phe-\nnomenon known as the Generating and Discerning Gap (G-\nD Gap) (Saunders et al. 2022). Studies indicate that super-\nvising internal states, rather than generating answers, en-\nhances recognition accuracy in classification tasks (Azaria\nand Mitchell 2023). Additional research on downstream\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n*Corresponding authors.\ntasks supports the G-D Gap’s impact on LLM performance\n(McKenna et al. 2023; Agrawal, Mackey, and Kalai 2023).\nThese studies suggest that hallucinations may partly stem\nfrom knowledge deficiency and misalignment between the\nmodel’s output and the desired truthful response, resulting\nfrom the model’s inability to properly access or utilize in-\nternal knowledge. Although generating factual statements\naligns with human preferences, this characteristic is not in-\nherently present in LLMs pre-trained on extensive, noisy\ndata. Reinforcement learning (RLHF) (Ziegler et al. 2020),\na method for introducing alignments, cannot fully address\nthe problem, as reward models may erroneously reward un-\nverifiable answers or prioritize versatility over truthfulness,\npotentially exacerbating the G-D Gap and hallucination is-\nsues.\nA more promising approach might involve focusing on\nthe concept of ’truth’ within LLMs, as recent studies have\nshown that LLMs can internally model truthfulness (Azaria\nand Mitchell 2023). By systematically analyzing the internal\nstates of LLMs and evaluating their propensity to generate\naccurate or inaccurate statements, insights have been gained\nfrom interventions designed to guide the model toward pro-\nducing more truthful outputs.\nInspired by existing work, we propose Truth Forest\n(TrFr), a method for exploring multi-dimensional truth fea-\ntures within LLMs. TrFr models complex truth features by\nemploying multiple orthogonal probes, effectively captur-\ning the intricate internal activities within LLMs. Truth For-\nest introduces a simple iterative algorithm with orthogonal\nconstraints to generate a series of orthogonal probes, which\nare merely direction vectors pointing towards some truth.\nThese direction vectors are weighted during the intervention\nto impose a preference for truthfulness. To mitigate the G-D\nGap, we incorporate Random Peek, a diversified sampling\nmethod that captures truth-related features from various po-\nsitions within the sequence, enhancing the model’s ability to\naccess and utilize its internal knowledge.\nWe conducted a systematic study of TrFr’s components.\nFor orthogonal directions, we explored various intervention\nintensities and data amounts, confirming the advantages of\nemploying multiple directions. A study on samples unveiled\nthe underlying logic of our approach. Through random peek,\nwe analyzed differences in intervention locations between\nour method and ITI. Our study reveals the first proof of the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20967\nv\nQ1:\nA1:\n…\nSample Z₁, Z₂, …, \nZn\nLayer 1\nLayer 2\nLayer l\n..\n.\nθ*1,1\nWhat\n subjects\n did\n Einstein\nflunk\n in\n school\n ?\nvery\ndid\nEinstein\nw̶e̶l̶l̶\n i̶n̶\n s̶c̶h̶o̶o̶l̶\n .̶\nQn: …… \nAn: …… \n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\nθ*2,1\nθ*l,1\nLM\nLM\n2. Rank  \n \nQ: Dear GPT, Are \nyou human？ \nθ*i,j\nNO!\nYes\nPositive Inputs\nLLM\nStep 1 Step 3\nEffective \nProbes\nHead \nFeature F \nZ₁ = 3\nZn= … \nSelected \nGroups\nStep 2\n- Intervened states\n - Unaffected states\nl\nh\nQ2:\nA2:\nAre\n you\n conscious\n ?\n.\nYes\nNegative Inputs\nZ₂ = 2\nF \n1.  Probe Training\n- Negative inputs\n- Positive inputs\n- Previous probe\n- Current probe\nF\nFigure 1: Framework of TrFr. TrFr involves three steps:(1) Feature Extraction. Extract key features from QA dataset using the\n’Random Peek’ technique.(2) Probe Training. Train orthogonal probing groups on these features, and then select the Top-K\neffective groups based on their identifying performance on a validation set. Then weight the directions within each group to\ndetermine the final truthful axis.(3) Intervention. For all effective groups’ regions, an adjustment based on the axis is performed\nto shift the LLM towards a truthful state.\nG-D Gap within the model, highlighting the importance of\ntackling this issue to improve the model’s performance.\nOur method, orthogonal to RLHF and Few-shot prompt-\ning(FSP), demonstrates consistent improvements in various\nLLMs. We conducted a detailed examination of Truth For-\nest on the TruthfulQA benchmark (Lin, Hilton, and Evans\n2022), raising the true rate of LLaMA-7B (Touvron et al.\n2023a) from 30.6% to 77.2% and the True*Info from 29.6%\nto 63.2%.\nOur contributions can be summarized as follows.\n• introducing a method that employs multiple orthogonal\nprobes to construct complex truth features within LLMs.\n• We introduce Random Peek, a technique that bridges the\ngap between generating and discerning truth features,\nleading to more responsible statement generation.\n• Our extensive analysis of multi-dimensional truth fea-\ntures demonstrated the effectiveness of TrFr.\n2 Related Work\nThe highly parameterized nature of LLMs often leads\nto black-box operations that are difficult to comprehend\n(Hu et al. 2021; Houlsby et al. 2019), resulting in lim-\nited intervention effects. While Contrast-Consistent Search\n(CCS) (Burns et al. 2022) has made progress in model-\ning truth within LLMs, it faces challenges due to its re-\nliance on a binary logic constraint for unsupervised truth-\nful directions. Similarly, Inference-Time Intervention (ITI)\n(Li et al. 2023b) has revealed the multi-dimensional truth-\nfulness within LLMs using supervised samples, but it suf-\nfers from high variance. These works employ the last token\nof a QA sequence to extract features for finding directions,\nwhich may lead to inconsistencies between generating and\ndiscerning truth for two reasons: (1) Using a fixed position\nfor feature extraction without special training can result in\nsuboptimal performance (Liu et al. 2019). (2) Since the an-\nswer is already given, the focus shifts from the question to\ndiscerning specific responses, which may limit the scope of\naddressing hallucinations.\nProbe-based Intervention. Recent work on modeling\ntruth within LLMs can be traced back to the Plug and Play\nLanguage Model (PPLM) series, which introduces a classi-\nfier P(a|x) and uses Markov Chain Monte Carlo (MCMC)\nsampling to obtain the posterior distribution P(x|a) ∝\nP(a|x)P(x). Typically, multiple backward and forward\npasses are required for intervention. These methods, consid-\nered activations editing, have been widely applied in style\ntransfer domains (Liu et al. 2022; Dhariwal and Nichol\n2021). Inspired by (Li et al. 2023b), TrFr simplifies the\nmulti-step intervention process and establishes a connection\nwith PPLM, serving as a low-order approximation of PPLM.\nWe follow ITI and further explore the multi-dimensional\ntruth property. We describe TrFr in the following sections.\n3 Truth Forest: Intervening from Multiple\nDirections for Enhanced Truthfulness\n3.1 Overview\nIn Figure 1, we illustrate the Training-Intervention Frame-\nwork for TrFr. TrFr is based on the idea that specific patterns\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20968\nin LLM’s attention mechanisms can indicate whether it is\nproviding false or true information (Li et al. 2023b; Burns\net al. 2022). These patterns are identifiable as points along a\naxis that separates truth from deception.\n3.2 Mitigating the G-D Gap With Random Peek\nA question-answer dataset with true and false responses(or\npositive & negative) is used to train probes to differentiate\ntruth from deception in an LLM. The Random Peek method\nis implemented through Algorithm 1.\nAlgorithm 1: Random Peek Method for Extracting Features\nInput: Question-answer Dataset D, LM, distribution Φ, LM’s lay-\ners L, LM’s Attention heads H\nOutput: MHA features F\n1: Initialize an L × H 2D-list F for storing features\n2: for each tuple (Qi, Ai, yi ∈ {0, 1}) in D, where yi indicates\ncorrectness of Ai do\n3: Sample cutoff index z ∼ Φ, ensuring 1 ≤ z ≤ |Ai|\n4: Si ← Concat(Qi,Ai[ :z ])\n5: Compute hidden states X ← LM(Si)\n6: for each layer l = 1 to L do\n7: for each head h = 1 to H do\n8: Extract last token’s features for head h at layer l: xl\nh\n9: Append (xl\nh, yi) to F[l][h]\n10: end for\n11: end for\n12: end for\n13: return F\nThe Random Peek solely truncates each answer at a po-\nsitional level. This approach is grounded in the assumption\nthat features sampled from different points in the answer se-\nquence can be more informative. In Section 5.2, we explore\nthe influence of Random Peek.\n3.3 Orthogonal Probes for Truthfulness\nRepresentation\nA single-layer sigmoid classifierpθ(x) = σ(⟨θ, x⟩) effective\nfor identifying truthful axis due to its interpretable parame-\nters. With the convention that 1 signifies truth, a smaller co-\nsine distance between attention state from positive inputsxP\nand the learnt parameter θ(normalized to unit length, seen\nas an axis) suggests a greater probability of the LLM being\ntruthful. Conversely, a closer angle with negative inputs xN\nsuggests a higher likelihood of being in a deceptive state.\nInspired by (Li et al. 2023b) we further explore the\nmulti-dimensionality of truthfulness. We introduce multiple\nprobes, i.e pθ(x), in each head for capturing multiple axis:\nΘ = {θ1, θ2, ..., θk}, θi ⊥ θj, i̸= j\nProbes in each orthogonal group are trained on the same\nfeature set Fl\nh to predict Si is positive or negative inputs\nfrom Algorithm 1 using a binary cross-entropy loss Lce.\nLce = − 1\nN\nNX\ni=1\n[yi ·log(pθ(xl\nh))+(1 −yi)·log(1−pθ(xl\nh))\nAfter training, the parameter θ aligns with the axis pointing\ntowards the majority of positive inputs, while its opposite\nangle gathers the majority of negative inputs. Intuitively, an\nadditive adjustment to attention states can be made to move\ncloser to that direction.\nTo avert model collapse, we enforce soft orthogonality\nconstraints, denoted as Lorth. To efficiently tackle the es-\ncalating optimization complexity for probes generated later,\nthe Limited-memory BFGS (L-BFGS) algorithm (Liu and\nNocedal 1989) is employed, owing to its proficiency in han-\ndling complex optimization challenges and ensuring stabil-\nity under augmented constraints.\nLorth =\nkX\ni=1\ni−1X\nj=1\n∥⟨θi, θj⟩∥1\nBy minimizing Lorth, we encourage the probes to remain\northogonal to each other, thus capturing different aspects of\nthe model’s internal representations of truthfulness.\nTo prevent overfiting, a weight decay regularizationL2 is\napplied to θ. The total loss for a probe incorporates three\ncomponents:\nLtotal = Lce + λLorth + µL2\nWe can control the trade-off between accuracy and orthogo-\nnality of probes by adjusting λ and µ.\n3.4 Implementing Truth Forest and Intervention\nProcess\nAfter training, we obtain multiple axis Θ pointing towards\ntruthfulness in each head. Note that during the training of\nthe probes, the K probes in each group are generated and\ntrained in sequence, which leads to decreased performance.\nIn each head, we perform weighting to balance disequilib-\nrium probes and obtain the final unit axis Θl,h.\nWe compute the final axis Θl,h using exponential decay\nweighting W:\nΘl,h =\nKX\nk=1\nwkθl,h,k, w k = e−k\nwhere wk is the weighting factor, and θl,h,k is the k-th axis\nat position (l, h).\nWe rank all the groups by each 1st probe and obtain the\neffective axis Θ∗\nl,h. To intervene in the MHA layer, we mod-\nify it as a constant:\nxl+1 = xl +\nHX\nh=1\nQh\nl\n\u0000\nAtth\nl\n\u0000\nPh\nl xl\n\u0001\n+ ασh\nl Θ∗\nl,h\n\u0001\nwhere xl and xl+1 represent layer l input and output, Qh\nl ,\nAtth\nl , and Ph\nl are MHA components, H is the number of\nheads, α is the intervention strength, Θ∗\nl,h is the unit axis,\nσh\nl is the standard deviation ensuring the effectiveness of\nthe intervention. We provide a theory perspective of TrFr in\nAppendix A.\nSince the additional term in each step is a constant, the\ntime complexity of TrFr when inference is O(1).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20969\nTrue*Info (%) True (%) MC acc. (%) CE KL\nFew-shot Setting\nBaseline 32.4 33.3 25.8 2.17 -\nSupervised Finetuning † 36.1 47.1 24.2 2.10 0.01\nFew-shot Prompting 45.9 47.5 33.3 2.17 -\nITI 40.2 45.0 26.7 2.40 0.24\nFew-shot Prompting + ITI 48.2 54.2 36.7 2.40 0.24\nTrFr 41.5 45.8 27.5 2.26 0.10\nFew-shot Prompting + TrFr 57.5 62.5 36.7 2.26 0.10\nFull Data\nBaseline 29.6 30.6 25.6 2.15 -\nRandom direction 30.5 31.6 25.5 2.21 0.02\nCCS † 33.4 34.7 26.2 2.21 0.06\nITI: Probe weight direction 34.1 35.4 26.8 2.20 0.06\nITI: Mass mean shift 42.1 45.4 29.0 2.41 0.28\nTrFr: Orthogonal directions 50.2 55.0 28.8 2.18 0.05\nTrFr: Single Mass 63.2 77.2 31.3 2.48 0.36\nTable 1: Comparison of model performance in few-shot and full data settings. We report the results for two variants of TrFr;\nThe Single Mass variant corresponds to using Random peek and directions directly obtained from the training samples, similar\nto ITI: Mass mean shift. Results are averaged over four runs. α and standard deviations are reported in Appendix B. † denotes\nresults reproduced from other authors.\n4 Experiments\nWe evaluate TrFr on the TruthfulQA (Lin, Hilton, and\nEvans 2022), a benchmark specifically designed to entice the\nmodel to produce hallucinatory answers. It comprises a di-\nverse set of questions targeting human misconceptions and\nrelated responses. We do not claim that TruthfulQA fully\nassesses the level of truthfulness of the model, as no dataset\ncan achieve this. The evaluation process involves two tracks:\nmultiple-choice and generation.\n4.1 Experimental Setup\nThis section provides an overview of the experimental setup,\norganized into four parts: Metrics, Models, Measuring, and\nHyperparameters.\nMetrics. For the multiple-choice track, the primary metric\nis MC1, based on the correct ranking of truthful answers. In\nthe generation track, the main metric is True*Informative\nrate, accounting for truthfulness and informativeness using\nGPT-judge. See Appendix F.1 for more details.\nModels. We assess a variety of open-source 7B models,\nincluding LLaMA, Llama 2(Touvron et al. 2023b), Alpaca\n(Taori et al. 2023), and Vicuna(Zheng et al. 2023). Our pri-\nmary focus is on utilizing LLaMA-7B for our experiments.\nMeasuring Intervention. Following (Li et al. 2023b), we\ncalibrate intervention strength using Cross Entropy (CE)\nand Kullback–Leibler divergence (KL) to measure devia-\ntion from the original generation distribution. Lower val-\nues indicate less change. We use a subset of Open Web\nText(Radford, Jozefowicz, and Sutskever 2017) for calcu-\nlations.\nHyperparameters. Details and used prompts are reported\nin Appendix E.\n4.2 Baseline Approaches\nWe compare several baseline approaches*:\nSupervised Fine-tuning (SFT): Alternates between su-\npervised training and pretraining for truthful answers.\nFew-shot Prompting (FSP): Improves truthfulness using\nin-distribution examples as prompts during inference.\nInstruction Fine-tuning (IFT): Enhances truthfulness by\nfine-tuning language models with task-specific instructions.\nFollowing (Li et al. 2023b), we evaluate SFT, FSP, and\nITI in few-shot scenarios with constraints on window size\nand compare CCS and ITI using 2-fold validation on the full\nTruthfulQA. See details of scenarios in Appendix F.\n4.3 Experimental Results\nIn Table 1, we compare TrFr with baseline in two different\nscenarios. In a few-shot setting, TrFr achieves better results\ndue to its compatibility with FSP. The CE and KL results\nindicate that we perform better with minimal intervention\nwhile maintaining informativeness.\nTable 2 compares the results of IFT and pre-trained mod-\nels using TrFr. We find that IFT effectively reduces halluci-\nnation issues. Results show that TrFr interventions are min-\nimal while significantly improving the True*Info % at any\nstage of the models. This also proves that TrFr is orthogo-\nnal to IFT and can enhance performance in conjunction with\nthem.\nIn Figure 2, we compare the performance of the Llama 2\nseries across 38 categories of TruthfulQA. We observe that\n*RLHF underperforms 50-shot in-distribution prompting for\nTruthfulQA as reported in (Bai et al. 2022). In both (Bai et al.\n2022; Menick et al. 2022), RLHF shows minimal improvement.\nTask-specific RLHF with 5% samples remains uncertain.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20970\nTrue*Info (%) True (%) MC acc. (%) CE KL\nPre-trained\nLLaMA 29.6 30.6 25.6 2.15 -\nLLaMA + TrFr 50.2 55.0 28.8 2.18 0.05\nLlama 2 37.5 40.8 28.5 2.07 -\nLlama 2 + TrFr 56.0 74.5 33.8 2.19 0.08\nFine-tuned\nAlpaca 40.7 40.8 26.2 2.51 -\nAlpaca + TrFr 70.5 77.6 30.8 2.74 0.50\nVicuna 55.4 59.1 33.3 2.59 -\nVicuna + TrFr 78.8 88.8 38.8 2.76 0.54\nLlama 2-Chat 58.6 63.0 33.7 2.46 -\nLlama 2-Chat + TrFr 76.7 84.9 39.3 2.59 0.22\nTable 2: Comparison of mainstream LLMs using 2-fold cross-validation. All models are 7B versions, and the results are aver-\naged over four independent runs.\nFigure 2: Category-wise performance of the Llama 2-7B series on the TruthfulQA dataset. Results for TrFr are combined from\nthe test sets of two folds with a random seed.\nTrFr improves Llama 2-Chat 7B in almost all categories.\nComplete intervention results are provided in Appendix G.\n5 Analysis\n5.1 Ablation Study of TrFr Components\nIn Table 3, we perform an ablation study on the compo-\nnents of TrFr. We find that both parts significantly improve\nLLaMA-7B, with Random Peek yielding the most consider-\nable improvement.\nMethod True*Info (%) True (%) MC\nBaseline 29.6 30.6 25.6\n+ Orthogonal directions 36.7 38.4 27.3\n+ Random peek 49.7 54.2 28.7\nTrFr 50.2 55.0 28.8\nTable 3: Ablation of TrFr Components. These experiments\nevaluate the individual components of TrFr, with the base-\nline being the unmodified LLaMA-7B.\n5.2 Analysis of Random Peek\nIn Table 4, we compare the last token and Random Peek\nby examining the overlap between the effective heads (i.e.,\nhigh-accuracy heads) generated by each method.\nWe find significant differences between the heads selected\nby R.P and EOS in both Top-48 and Top-96 scenarios. These\ndifferent heads significantly contribute to the differences in\ninterventions, reflecting the gap between generating and dis-\ncerning truth. Furthermore, the bottom table compares the\noverlap between directions within the method, showing that\nR.P. has better diversity.\nThe G-D gap emerges due to misalignments between\ngenerated answers and the model’s internal states. Super-\nvised learning aids in reconciling these misalignments by\nutilizing aligned data, while R.P.’s diversity ensures that the\nalignment can be effectively generalized to various positions\nwithin the sequence.\n5.3 Analysis of Number of Orthogonal Directions\nWe examine the orthogonal direction components from two\nperspectives: the amount of data and intervention strength.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20971\nFigure 3: Impact of the Number of Directions as Data Increases. In this study, we investigate the changes in fidelity preference\nas the volume of training data for probes increases (left) and the average results (right). On average, moderately increasing the\nnumber of directions helps improve performance.\nTop-48 acc. heads Top-96 acc. heads\nHeads Overlap between EOS and R.P .\n1st Dir. 39.58% 54.17%\n2nd Dir. 27.08% 34.38%\n3rd Dir. 22.92% 42.71%\nHeads Overlap between Directions\nR.P. 52.08% 58.33%\nEOS 59.57% 72.63%\nTable 4: Overlap comparison for various methods and di-\nrections. We denote EOS as the last token and R.P. as Ran-\ndom peek. The above table shows the overlap between EOS\nand R.P. for Top-K acc. heads. The bottom section compares\nthe overlap between the 1st and 2nd directions within the\nmethod.\nIn Figure 3, we assessed the impact of varying the number\nof orthogonal directions on True*Info % while training with\ndifferent feature data amounts. Our results indicate that us-\ning multiple directions improves the model’s performance,\nwith more probes enabling faster convergence, especially\nwhen data is limited.\nFurthermore, as shown in Table 5, our experiments reveal\nthat the optimal number of directions depends on the spe-\ncific intervention setting, with a moderate increase generally\nyielding better performance.\n5.4 Visualizing Orthogonal Directions\nTo explore the underlying principles of how Orthogonal Di-\nrections operate, we analyze the projections of True Positive\n(TP) samples in TruthfulQA onto different directions.\nIn Figure 4, we present the t-SNE results of sample\nprojections for each probe. Interestingly, we observe well-\ndefined clusters formed by the samples based on the clas-\nsifiers, suggesting that Orthogonal Directions may capture\ntruth-related features independently and combine them in a\ncomplementary manner.\nIn Figure 5, we investigate the relationship between the\nDir. Tr*In (%) True (%) MC (%) CE KL\n1 37.20 43.11 20.72 2.19 0.12\n2 38.38 51.28 22.00 2.37 0.31\n3 41.06 52.72 23.17 2.42 0.35\n4 38.63 51.17 23.72 2.45 0.39\n6 37.53 51.31 24.19 2.50 0.44\nTable 5: Impact of the Number of Directions in Different in-\ntervening Strengths. We experiment with scaling directions\non different intervened heads and strengths(α) to investigate\ntheir impact on the model’s fidelity.\nFigure 4: t-SNE visualization of samples projected onto or-\nthogonal probes, revealing complementary relationships and\nclustered patterns among the probes. Samples uniquely iden-\ntified by a single probe, while undetected by others, are\nmarked with distinct colors.\noverlap of TP data and orthogonal loss among different\nProbes. Classifiers with lower orthogonal loss generally\nhave a lower TP overlap rate.\n5.5 Generalization of TrFr\nTable 6 presents the generalization results for the Natu-\nral Questions dataset(Kwiatkowski et al. 2019), an out-of-\ndistribution test. We follow (Li et al. 2023b), using the con-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20972\nFigure 5: A Case Study on Highly Orthogonal Directions About Truth. We examine five orthogonal probes trained on the\n22nd layer’s 4th head and calculate their average Lorth (left), as well as the Jaccard similarity between their TP samples in\nTruthfulQA (right).\nfusing option generated by GPT-4. TrFr slightly improves\nover the baseline, demonstrating its potential to generalize\nto other datasets.\nNatural Questions\nBaseline 43.9\nTrFr 44.3\nTable 6: Generalization results on out-of-distribution\ndatasets. MC1 is reported.\n5.6 Balancing Veracity and Informativeness\nThis section investigates the optimal balance between inter-\nvention strength (α) and the number of intervened heads for\nachieving high Info %. Figure 6 shows the impact of inter-\nvention strength on LLaMA’s veracity. In contrast, Figure 7,\nwhich selects runs with an informative rate > 90%, empha-\nsizes the importance of balancing the number of intervened\nheads and intervention strength to ensure informative out-\nputs. We use the intervention settings sets from Section 5.3.\n6 Conclusions and Future Work\nIn this paper, we introduced Truth Forest, an innovative\nmethod that employs multiple orthogonal directions to en-\nhance the truthfulness of LLMs at inference time without\nadditional fine-tuning. Future research directions include ex-\nploring the applicability of TrFr to other tasks and domains\nand addressing other LLMs challenges, such as bias reduc-\ntion and controllability.\nFigure 6: Effect of Intervention Strength. Intervention in-\ntensity influences LLaMA’s veracity when limiting the\nnumber of intervened heads.\nFigure 7: Balancing Veracity and Informativeness. Achiev-\ning an optimal balance between the number of intervened\nheads and intervention strength is crucial for maintaining\ninformativeness.\nAcknowledgements\nThis paper is supported by the Science and Technology De-\nvelopment Fund of Macau SAR (File no. 0081/2022/A2,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20973\n0123/2022/AFJ, and 0015/2019/AKP), and GuangDong\nBasic and Applied Basic Research Foundation (No.\n2020B1515130004).\nReferences\nAgrawal, A.; Mackey, L.; and Kalai, A. T. 2023. Do Lan-\nguage Models Know When They’re Hallucinating Refer-\nences? arXiv:2305.18248.\nAzaria, A.; and Mitchell, T. 2023. The Internal State of an\nLLM Knows When its Lying. arXiv:2304.13734.\nBai, Y .; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; Das-\nSarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan,\nT.; Joseph, N.; Kadavath, S.; Kernion, J.; Conerly, T.; El-\nShowk, S.; Elhage, N.; Hatfield-Dodds, Z.; Hernandez, D.;\nHume, T.; Johnston, S.; Kravec, S.; Lovitt, L.; Nanda, N.;\nOlsson, C.; Amodei, D.; Brown, T.; Clark, J.; McCandlish,\nS.; Olah, C.; Mann, B.; and Kaplan, J. 2022. Training a\nHelpful and Harmless Assistant with Reinforcement Learn-\ning from Human Feedback. arXiv:2204.05862.\nBurns, C.; Ye, H.; Klein, D.; and Steinhardt, J. 2022. Dis-\ncovering Latent Knowledge in Language Models Without\nSupervision. arXiv:2212.03827.\nDhariwal, P.; and Nichol, A. 2021. Diffusion Models Beat\nGANs on Image Synthesis. arXiv:2105.05233.\nHoulsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;\nde Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly,\nS. 2019. Parameter-Efficient Transfer Learning for NLP.\narXiv:1902.00751.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2021. LoRA: Low-Rank Adap-\ntation of Large Language Models. arXiv:2106.09685.\nKwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.;\nParikh, A.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin,\nJ.; Lee, K.; Toutanova, K.; Jones, L.; Kelcey, M.; Chang, M.-\nW.; Dai, A. M.; Uszkoreit, J.; Le, Q.; and Petrov, S. 2019.\nNatural Questions: A Benchmark for Question Answering\nResearch. Transactions of the Association for Computa-\ntional Linguistics, 7: 452–466.\nLi, J.; Cheng, X.; Zhao, W. X.; Nie, J.-Y .; and Wen, J.-R.\n2023a. HaluEval: A Large-Scale Hallucination Evaluation\nBenchmark for Large Language Models. arXiv:2305.11747.\nLi, K.; Patel, O.; Vi´egas, F.; Pfister, H.; and Wattenberg, M.\n2023b. Inference-Time Intervention: Eliciting Truthful An-\nswers from a Language Model. arXiv:2306.03341.\nLin, S.; Hilton, J.; and Evans, O. 2022. Truth-\nfulQA: Measuring How Models Mimic Human Falsehoods.\narXiv:2109.07958.\nLiu, D.; and Nocedal, J. 1989. On the limited memory\nmethod for large scale optimization: Mathematical Program-\nming B.\nLiu, X.; Park, D. H.; Azadi, S.; Zhang, G.; Chopikyan, A.;\nHu, Y .; Shi, H.; Rohrbach, A.; and Darrell, T. 2022. More\nControl for Free! Image Synthesis with Semantic Diffusion\nGuidance. arXiv:2112.05744.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv:1907.11692.\nManakul, P.; Liusie, A.; and Gales, M. J. F. 2023.\nSelfCheckGPT: Zero-Resource Black-Box Hallucina-\ntion Detection for Generative Large Language Models.\narXiv:2303.08896.\nMcKenna, N.; Li, T.; Cheng, L.; Hosseini, M. J.; John-\nson, M.; and Steedman, M. 2023. Sources of Hallu-\ncination by Large Language Models on Inference Tasks.\narXiv:2305.14552.\nMenick, J.; Trebacz, M.; Mikulik, V .; Aslanides, J.; Song,\nF.; Chadwick, M.; Glaese, M.; Young, S.; Campbell-\nGillingham, L.; Irving, G.; and McAleese, N. 2022. Teach-\ning language models to support answers with verified\nquotes. arXiv:2203.11147.\nRadford, A.; Jozefowicz, R.; and Sutskever, I. 2017. Learn-\ning to generate reviews and discovering sentiment. arXiv\npreprint arXiv:1704.01444.\nSaunders, W.; Yeh, C.; Wu, J.; Bills, S.; Ouyang, L.; Ward,\nJ.; and Leike, J. 2022. Self-critiquing models for assisting\nhuman evaluators. arXiv:2206.05802.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford\nAlpaca: An Instruction-following LLaMA model. https:\n//github.com/tatsu-lab/stanford\nalpaca.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lam-\nple, G. 2023a. LLaMA: Open and Efficient Foundation Lan-\nguage Models. arXiv:2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucu-\nrull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.;\nGao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini,\nS.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa, M.;\nKloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.;\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poul-\nton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.;\nSilva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang,\nB.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.;\nZarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Ro-\ndriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b.\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\narXiv:2307.09288.\nWang, C.; Liu, X.; and Song, D. 2020. Language Models\nare Open Knowledge Graphs. arXiv:2010.11967.\nZheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu,\nZ.; Zhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.;\nZhang, H.; Gonzalez, J. E.; and Stoica, I. 2023. Judg-\ning LLM-as-a-judge with MT-Bench and Chatbot Arena.\narXiv:2306.05685.\nZiegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Rad-\nford, A.; Amodei, D.; Christiano, P.; and Irving, G. 2020.\nFine-Tuning Language Models from Human Preferences.\narXiv:1909.08593.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20974"
}