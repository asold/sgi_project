{
  "title": "A study of transformer-based end-to-end speech recognition system for Kazakh language",
  "url": "https://openalex.org/W4280556124",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2800168459",
      "name": "Mamyrbayev Orken",
      "affiliations": [
        "Institute of Information and Computational Technologies"
      ]
    },
    {
      "id": "https://openalex.org/A4280959146",
      "name": "Oralbekova Dina",
      "affiliations": [
        "Institute of Information and Computational Technologies",
        "Satbayev University"
      ]
    },
    {
      "id": "https://openalex.org/A3080257687",
      "name": "Alimhan Keylan",
      "affiliations": [
        "L. N. Gumilyov Eurasian National University",
        "Institute of Information and Computational Technologies"
      ]
    },
    {
      "id": "https://openalex.org/A4280959148",
      "name": "Turdalykyzy Tolganay",
      "affiliations": [
        "Institute of Information and Computational Technologies"
      ]
    },
    {
      "id": "https://openalex.org/A1994061794",
      "name": "Othman Mohamed",
      "affiliations": [
        "Universiti Putra Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A2800168459",
      "name": "Mamyrbayev Orken",
      "affiliations": [
        "Institute of Information and Computational Technologies"
      ]
    },
    {
      "id": "https://openalex.org/A4280959146",
      "name": "Oralbekova Dina",
      "affiliations": [
        "Satbayev University",
        "Institute of Information and Computational Technologies"
      ]
    },
    {
      "id": "https://openalex.org/A3080257687",
      "name": "Alimhan Keylan",
      "affiliations": [
        "Institute of Information and Computational Technologies",
        "L. N. Gumilyov Eurasian National University"
      ]
    },
    {
      "id": "https://openalex.org/A4280959148",
      "name": "Turdalykyzy Tolganay",
      "affiliations": [
        "Institute of Information and Computational Technologies"
      ]
    },
    {
      "id": "https://openalex.org/A1994061794",
      "name": "Othman Mohamed",
      "affiliations": [
        "University of Kuala Lumpur",
        "Universiti Putra Malaysia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2394932179",
    "https://openalex.org/W811578723",
    "https://openalex.org/W3089901025",
    "https://openalex.org/W2964539095",
    "https://openalex.org/W4243011756",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W423095831",
    "https://openalex.org/W2136466519",
    "https://openalex.org/W6600165366",
    "https://openalex.org/W2972389417",
    "https://openalex.org/W3015457435",
    "https://openalex.org/W3184165979",
    "https://openalex.org/W4220689448",
    "https://openalex.org/W2953343412",
    "https://openalex.org/W2010111780",
    "https://openalex.org/W2028558072",
    "https://openalex.org/W3015974384",
    "https://openalex.org/W3097075707",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2973048981",
    "https://openalex.org/W3096798607",
    "https://openalex.org/W2593463961",
    "https://openalex.org/W3015834770",
    "https://openalex.org/W3197964296",
    "https://openalex.org/W4238404964"
  ],
  "abstract": "Abstract Today, the Transformer model, which allows parallelization and also has its own internal attention, has been widely used in the field of speech recognition. The great advantage of this architecture is the fast learning speed, and the lack of sequential operation, as with recurrent neural networks. In this work, Transformer models and an end-to-end model based on connectionist temporal classification were considered to build a system for automatic recognition of Kazakh speech. It is known that Kazakh is part of a number of agglutinative languages and has limited data for implementing speech recognition systems. Some studies have shown that the Transformer model improves system performance for low-resource languages. Based on our experiments, it was revealed that the joint use of Transformer and connectionist temporal classification models contributed to improving the performance of the Kazakh speech recognition system and with an integrated language model it showed the best character error rate 3.7% on a clean dataset.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2022) 12:8337  | https://doi.org/10.1038/s41598-022-12260-y\nwww.nature.com/scientificreports\nA study of transformer‑based \nend‑to‑end speech recognition \nsystem for Kazakh language\nMamyrbayev Orken1, Oralbekova Dina1,2*, Alimhan Keylan1,3, Turdalykyzy Tolganay1 & \nOthman Mohamed4\nToday, the Transformer model, which allows parallelization and also has its own internal attention, \nhas been widely used in the field of speech recognition. The great advantage of this architecture is the \nfast learning speed, and the lack of sequential operation, as with recurrent neural networks. In this \nwork, Transformer models and an end‑to‑end model based on connectionist temporal classification \nwere considered to build a system for automatic recognition of Kazakh speech. It is known that \nKazakh is part of a number of agglutinative languages and has limited data for implementing speech \nrecognition systems. Some studies have shown that the Transformer model improves system \nperformance for low‑resource languages. Based on our experiments, it was revealed that the joint \nuse of Transformer and connectionist temporal classification models contributed to improving the \nperformance of the Kazakh speech recognition system and with an integrated language model it \nshowed the best character error rate 3.7% on a clean dataset.\nInnovative information and digital technologies are increasingly making their way into the life of a modern \nperson: this applies to deep learning systems like voice recognition, images, speech recognition and synthesis. \nNamely, speech technologies are widely used in communications, robotics and other areas of professional activity. \nSpeech recognition is a way to interact with technology. Speech recognition technology provides recognition of \nindividual words or text, with its further conversion into a sequence of words or commands. There are traditional \nspeech recognition systems that are based on acoustic, language models and lexicon. The acoustic model (AM) \nwas built based on hidden Markov models (HMM) with the Gaussian Mixture Model (GMM), and the language \nmodel (LM) was based on n-gram models. The components of these systems were trained separately, which made \nit difficult to manage and configure them, which led to a decrease in the efficiency of using these systems. With \nthe advent of deep learning, the performance of speech to text systems has improved. Artificial neural networks \nbegan to be used for acoustic modeling instead of GMM, which led to improved results that were obtained in \nmany research  works1–3. Thus, the HMM-DNN architecture has become one of the most common models for \ncontinuous speech recognition.\nCurrently, the end-to-end (E2E) model has become widespread. The E2E structure presents the system as a \nsingle neural network, unlike the traditional one, which has several independent  elements4,5. The E2E system \nprovides direct reflection of acoustic signals in the sequence of labels without intermediate states, without the \nneed to perform subsequent processing at the output, which makes it easy to implement. To increase the perfor-\nmance of E2E systems, it is necessary to solve the main tasks related to the definition of the model architecture, \nthe collection of a sufficiently large speech corpus with the appropriate transcription, and the availability of \nhigh-performance equipment. Solving these issues ensures the successful implementation of not only speech \nrecognition systems, but also other deep learning systems. In addition, E2E systems can significantly improve \nthe quality of recognition from learning large amounts of training data.\nModels based on the Connectionist temporal  classification6 (CTC), models based on the attention  mechanism7 \nare illustrative examples of end-to-end systems. In a CTC-based model, there is no need to align at the frame level \nbetween acoustics and transcription, since a special token is allocated, like an \"empty label\" which determines the \nbeginning and end of one  phoneme8. In the attention mechanism based encoder/decoder models, the encoder is \nan AM—converts input speech into a high-level representation, the attention mechanism is an alignment model, \nand determines encoded frames that are related to the creation of the current output, the decoder is similar to the \nAM—operates autoregressive, predicting each output token depending on previous  predictions9. The above E2E \nOPEN\n1Institute of Information and Computational Technologies CS MES RK, Almaty, Kazakhstan. 2Satbayev University, \nAlmaty, Kazakhstan. 3L.N. Gumilyov Eurasian National University, Nur -Sultan, Kazakhstan. 4Universiti Putra \nMalaysia, Kuala Lumpur, Malaysia. *email: dinaoral@mail.ru\n2\nVol:.(1234567890)Scientific Reports |         (2022) 12:8337  | https://doi.org/10.1038/s41598-022-12260-y\nwww.nature.com/scientificreports/\nmodels are based on convolutional and modified recurrent neural networks (RNNs). The models implemented \nusing RNN perform calculations on the character positions of the input and output data, thus generating a \nsequence of hidden states depending on the previous hidden state of the network. This sequential process does \nnot provide parallelization of learning in training examples, which is a problem with a longer sequence of input \ndata and takes much longer to train the network.  In10, another Transformer-based model was proposed, which \nallows parallelization of the learning process, and this model also removes repetitions and uses its internal atten-\ntion to find the dependencies between the received and resulting data. The big advantage of this architecture is \nthe fast-learning rate and the lack of sequential operation, as with RNN. In previous  studies11,12 it was revealed \nthat the combined use of Transformer models and an E2E model, like CTC, contributed to the improvement of \nthe quality of the English and Chinese speech recognition system.\nIt should be noted that the attention mechanism is a common method that greatly improves the quality of the \nsystem in machine translation and speech recognition. And the Transformer model uses this attention mecha -\nnism to increase the learning rate. This model has its own internal attention, which aligns all positions of the \ninput sequence to find a representation of the set, which does not require alignments. In addition, Transformer \ndoes not need to process the end of the text after processing its start.\nIn order to implement such models, a large amount of speech data are required for training, which is prob -\nlematic for languages with limited training data, namely for the Kazakh language, which is included in the group \nof agglutinative languages. To date, systems have been developed based on the CTC  model13,14 for recognizing \nKazakh speech with different sets of training data. The use of other methods and models to improve the accuracy \nof recognition of the Kazakh speech is a promising direction and can improve the performance of the recognition \nsystem with a small size of the training sample.\nThe main goal of our study is to improve the accuracy of the automatic recognition system for Kazakh \ncontinuous speech by increasing training data, as well as the use of models based on Transformer and CTC for \nrecognizing Kazakh speech.\nThe structure of the work is given in the following order: Sect.  2 presents traditional methods of speech \nrecognition, Sect. 3 provides an analytical review of the scientific direction. Section 4 describes the principles of \noperation of the Transformer-based model and the model we proposed. Further, in Sects.  5 and 6, our experi-\nmental data, corpus of speech, and equipment for the experiment are described, and the results obtained are \nanalyzed. The conclusions are given in the final section..\nTraditional speech recognition methods\nTraditional sequence recognition focused on estimating the maximum a posteriori probability. Formally, this \napproach is a transformation of a sequence of acoustic speech characteristics X into a sequence of words W . \nAcoustic characteristics are a sequence of feature vectors of length T: X = {xt ∈ RD | t = 1,…, T}, and the sequence \nof words is defined as W  = {wn ∈ V | n = 1,…, N}, having length N, where V is a vocabulary. The most probable \nword sequence W* can be estimated by maximizing P(W|X) for all possible word sequences V* (1)15. This process \ncan be represented by the following expression:\nTherefore, the main goal of the automatic speech recognition (ASR) is to find a suitable model that will \naccurately determine the posterior distribution P(W | X).\nThe process of automatic speech recognition consists of sequences of the following steps:\n• Extraction of features from the input signal.\n• Acoustic modeling (determines which phones were pronounced for subsequent recognition).\n• Language modeling (checks the correspondence of spoken words to the most likely sequences).\n• Decoding a sequence of words spoken by a person.\nThe most important parts of a speech recognition system are feature extraction methods and recognition \nmethods. Feature extraction is a process that allocates a small amount of data essential for solving a problem. To \nextract features, Mel-frequency cepstral coefficients (MFCC) and perceptual linear prediction (PLP) algorithms \nare commonly  used16–18. The popular one is MFCC.\nIn the speech recognition task, the original signal is converted into feature vectors, on the basis of which \nclassification will then be performed.\nAcoustic model. The acoustic model (AM) uses deep neural networks and hidden Markov models. Deep \nneural network, convolutional neural network (CNN), or long short-term memory, which is a variant of the \nrecurrent neural network is used to map the acoustic frame xt to the phonetic state of the subsequent ft at each \ninput time t (2):\nBefore this acoustic modeling procedure, the output targets of the neural network models, a sequence of pho-\nnetic states at the frame level f1:T, are generated by HMM and GMM in special training methods. GMM models \nthe acoustic element at the frame level x1:T, and HMM estimates the most probable sequence of phonetic states f1:T.\nThe acoustic model is optimized for the cross-entropy error, which is the phonetic classification error per \nframe.\n(1)W ∗= argmax P(W |X)\nW ∈ V ∗\n(2)P\n(\nft|xt\n)\n= AcousticModel(xt)\n3\nVol.:(0123456789)Scientific Reports |         (2022) 12:8337  | https://doi.org/10.1038/s41598-022-12260-y\nwww.nature.com/scientificreports/\nLanguage model. The language model p(w) models the most probable sequences of words regardless of \nacoustics (3):\nwhere w<u is the previous recognized word.\nCurrently, RNN or LSTM are commonly used extensively for language model architecture, as they can capture \nlong-term dependencies rather than traditional n-gram models, which are based on the Markov assumption and \nlimited to a certain n-range of word history.\nHidden Markov models. For a long time, a system based on hidden Markov models (HMM) was the main \nmodel for continuous speech recognition. The HMM mechanism can be used not only in acoustic modeling but \nalso in the language model. But in general, the use of the HMM model gives a greater advantage when modeling \nthe acoustic component.\nIn this HMM, the phone is the observation and the feature is the latent state. For an HMM that has a \nstate set {1,…, J}, the HMM-based model uses the Bayesian theorem and introduces the HMM state sequence \nS = {st ∈ {1,…, J} | t = 1,…, T} пo p (L|X) (4).\np(X|S), p(S|L), and p(L) in Eq. (4 ) correspond to the acoustic model, the pronunciation model and the lan-\nguage model, respectively.\nThe acoustic model P (X|S) indicates the probability of observing X from the hidden sequence S. Accord-\ning to the probability chain rule and the observation independence hypothesis in the HMM (observations at \nany time depend only on the hidden state at that time), P(X|S) can be decomposed into the following form (5):\nIn the acoustic model, p  (xt|st) is the probability of observation, which is usually represented by mixtures of \nGaussian distributions. The distribution of the posteriori probability of the hidden state p  (st|xt) can be calculated \nby the method of deep neural networks.\nTwo approaches, HMM-GMM and HMM-DNN, can be used to calculate p (X|S) in Eq. 5. The first approach \nHMM-GMM was for a long time the main method for building speech-to-text technology. With the develop -\nment of deep learning technology, DNN is introduced into speech recognition for acoustic modeling. The role \nof DNN is to calculate the posterior probability of the HMM state, which can be converted into probabilities, \nreplacing the usual GMM observation probability. Consequently, the transition of HMM-GMM to the hybrid \nmodel HMM-DNN has yielded excellent recognition results, and is becoming a popular ASR architecture.\nHybrid models have some important limitations. For example, ANN with more than two hidden levels were \nrarely used due to computational performance limitations, and the context-dependent model described above \ntakes into account numerous effective methods developed for GMM-HMM.\nThe learning process is complex and difficult for global optimization. Components of traditional models are \nusually trained on different datasets and methods.\nHybrid models based on DNN‑HMM. To calculate P(xt|st) directly, GMM was used, because this model \ngives the possibility to simulate the distribution for each state, allowing to obtain probability values of input \nsequences. However, in practice, these assumptions cannot always be modeled by GMM. DNNs have shown sig-\nnificant improvements over GMMs due to their ability to study nonlinear functions. DNN cannot directly pro-\nvide a conditional probability. The frame-by-frame posterior distribution is used to turn the probability model \nP(xt|st) into a classification problem P(st|xt) using a pseudo-likelihood trick as a joint probability approximation \n(6)15. The application this probability is referred to as a \"hybrid  architecture\".\nA numerator is a DNN classifier trained with a set of input functions as input x t and target state s t. The \ndenominator P(st) is the prior probability of the state s t. Frame-by-frame training requires frame-by-frame \nalignment with xt as input and st as target. This negotiation is usually achieved by using a weaker HMM/GMM \nnegotiation system or using human-made dictionaries. The quality and quantity of alignment labels are usually \nthe most significant limitations of the hybrid approach.\nEnd‑to‑end speech recognition models. E2E automatic speech recognition is a new technology in the \nfield of  ASR based on a neural network, which offers many advantages. E2E ASR is a single integrated approach \nwith a much simpler training approach with models that work at a low audio frame rate. This reduces learn-\ning time, decoding time, and allows joint optimization with subsequent processing, such as understanding the \nnatural language.\n(3)P(w u|w <u ) = LanguageModel(w <u )\n(4)argmax\nL∈γ ∗\np\n(\nL|X\n)\n≈ argmax\nL∈γ ∗\n∑\nS\np\n(\nX |S\n)\np\n(\nS|L\n)\np(L)\n(5)p(X |S) =\nT∏\nt=1\np(xt|x1 ,..., xt−1 ,S) ≈\nT∏\nt=1\np(xt|st) ∝\nT∏\nt=1\np(st|xt)\np(st)\n(6)\nT∏\nt=1\nP (xt|st) =\nT∏\nt=1\nP (xt|st)\np(st)\n4\nVol:.(1234567890)Scientific Reports |         (2022) 12:8337  | https://doi.org/10.1038/s41598-022-12260-y\nwww.nature.com/scientificreports/\nFor the global calculation of P(W | X) using E2E speech recognition models, the input can be represented as \na sequence of acoustic features X = (x1,…, xt), the sequence of target marks as y = (y1,…, yt), and the sequences \nwords in the form W = wm = (w1,…, wm).\nThus, the ANN finds probabilities P(∙|x 1),…,P(∙|xt), where the input probability parameters are some repre-\nsentations of a sequence of words, i.e. labels.\nThe basic principle of operation is that modern E2E models are trained on the basis of big data. From the \nabove, we can detect the main problem, it concerns the recognition of languages with limited training data, such \nas Kazakh, Kyrgyz, Turkish, etc. For such low-resource languages, there are no large corpuses of training data.\nRelated work/literature review\nThe Transformer model was first introduced  in8, in order to reduce sequential calculations and the number of \noperations for correlating input and output position signals. Experiments were conducted on machine translation \ntasks, from English to German and from English to French. As a result, the model was shown to have achieved \ngood performance compared to existing results. Moreover, Transformer works perfectly for other tasks with \nlarge and limited training data, and is very fruitful for all kinds of seq2seq tasks.\nThe use of Transformer for speech-to-text conversion also showed good results and was reflected in the fol-\nlowing research papers:\nTo implement a faster and more accurate ASR system, Transformer and ASR achievements based on RNN \nwere combined by Karita et al.11. To build the model, a Connectionist temporal classification (CTC) was  E2E with \nTransformer for co-learning and decoding. This approach speeds up learning and facilitates LM integration. The \nproposed ASR system implements significant improvements in various ASR tasks. For example, it lowered WER \nfrom 11.1% to 4.5% for the Wall Street Journal and from 16.1% to 11.6% for TED-LIUM, introducing CTC and \nLM integration into the Transformer baseline.\nMoritz et al.19 proposed a Transformer-based model for streaming speech recognition that requires an entire \nspeech utterance as input. Time-limited self-attention in the encoder and triggered attention for the encoder-\ndecoder with attention mechanism were applied to generate the output after the spoken word. The model archi-\ntecture achieved the best result in E2E streaming speech recognition − 2.8% and 7.3% WER for \"pure\" and \"other\" \nLibriSpeech test data.\nThe Weak-Attention Suppression (W AS) method was proposed by Y angyang Shi and  other20, which dynami-\ncally causes sparse attention probabilities. This method suppresses the attention of uncritical and redundant \ncontinuous acoustic frames and is more likely to suppress past frames than future ones. It was shown that the \nproposed method leads to a decrease in WER compared to the basic types of Transformer. In Test LibriSpeech, \nthe proposed W AS method reduced WER by 10% in cleanliness testing and by 5% in another test for streaming \nTransformers, which led to a new advanced level among streaming models.\nDong Linhao and co-authors21 presented a Speech-Transformer system using a 2D attention mechanism that \nco-processes the time and frequency axes of 2D speech inputs, thereby providing more expressive representa-\ntions for the Speech-Transformer. The Wall Street Journal (WSJ) corpus was used as training data. The results \nof the experiment showed that this model allows to reduce the training time and at the same time can provide \na competitive WER.\nGangi et al.22 suggested Transformer with SLT adaptation—an architecture for spoken language translation, \nfor processing long input sequences with low information density to solve ASR problems. The adaptation was \nbased on downsampling the input data using convolutional neural networks and modeling the two-dimensional \nnature of the audio spectrogram using 2D components. Experiments show that the SLT-adapted Transformer \noutperforms the RNN-based baseline in both translation quality and learning time, providing high performance \nin six language areas.\nTakaaki Hori et al. 23 advanced the Transformer architecture, on the basis of which a context window was \ndeveloped, which was trained in monologue and dialogue scenarios. Monologue tests on CSJ and TED-LIUM3 \nand dialog tests on SWITCHBOARD and HKUST were applied. As a result, results were obtained that surpass \nthe basic E2E ASR with one sound and with or without speaker i-vectors.\nIn the E2E system, the RNN-based encoder-decoder model was replaced by the Transformer architecture \nin Chang X. et al.  research24. And in order to use this model in the masking network of the neural beamformer \nin the multi-channel case, the self-attention component has been modified so that it is limited to a segment, \nrather than the entire sequence, in order to reduce the amount of computation. In addition to improvements to \nthe model architecture, preprocessing of external dereverberation, weighted prediction error (WPE), was also \nincluded, which allows the model to process reverberated signals. Experiments with the extended wsj1-2mix \ncorpus show that Transformer-based models achieve better results in echo-free conditions in single-channel and \nmulti-channel modes, respectively.\nTransformer architecture\nThe Transformer model was first created for machine translation, replacing recurrent neural networks (RNNs) \nin natural language processing (NLP) tasks. In this model, recurrence was completely eliminated, instead, for \neach statement, using the internal attention mechanism (self-attention mechanism), signs were built to identify \nthe significance of other sequences for this utterance. Therefore, the generated features for a given statement are \nthe result of linear transformations of sequence features that are significant.\nThe Transformer model consists of one large block, which in turn consists of blocks of encoders and decoders \n(Fig. 1). Here, the encoder takes as input the feature vectors from the audio signal X  = (x1,…,xT) and outputs a \nsequence of intermediate representations. Further, based on the received representations, the decoder reproduces \nthe output sequence W = wm = (w1,…,wM). Each stage of the model uses the previous symbols to output the next, \n5\nVol.:(0123456789)Scientific Reports |         (2022) 12:8337  | https://doi.org/10.1038/s41598-022-12260-y\nwww.nature.com/scientificreports/\nbecause it is autoregressive. The Transformer architecture uses several layers of self-attention in the encoder and \ndecoder blocks that are interconnected with each other. Consider each block individually.\nEncoder and decoder networks. Conventional E2E  encoder/decoder models for speech recognition \ntasks consist of a single encoder and decoder, an attention mechanism. The encoder converts the vector of acous-\ntic features into an alternative representation, and the decoder predicts a sequence of labels from the alternative \ninformation provided by the encoder, then attention highlights the significant parts of the frame for predicting \nthe output. In contrast to these models, the Transformer model can have several encoders and decoders, and \neach of them contains its own internal attention mechanism.\nAn encoder block consists of sets of encoders; as a study, 6 coders are usually taken, which are located one \nabove the other. The number of encoders is not fixed, it is possible to experiment with an arbitrary number of \nencoders in a block. All encoders have the same structure but different weights. The input of the encoder receives \nextracted feature vectors from the audio signal, obtained using Mel-frequency cepstral coefficients or convolu -\ntional neural networks. Then the first encoder transforms these data using self-attention into a set of vectors, and \nthrough the feed forward ANN transmits the received outputs to the next encoder. The last encoder processes \nthe vectors and transfers the data of the encoded functions to the decoder block.\nA decoder block is a set of decoders, and their number is usually identical to the number of encoders. Each \npart of the encoder can be divided into two sublayers: the input data entering the encoder first passes through \nthe multi-head attention layer, which helps the encoder look at other words in the incoming sentence during \nencoding of a particular word. The output of the inner multi-head attention layer is sent to the feed-forward \nneural network. The exact same network is independently applied to each word in the sentence.\nThe decoder also contains two of these layers, but there is an attention layer between them that helps the \ndecoder focus on significant parts of the incoming sentence, as is similar to the usual attention mechanism in \nseq2seq models. This component will take into account previous characters/words and, based on these data, \noutputs the posterior probabilities of the subsequent character/words.\nSelf‑attention mechanism. The Transformer model includes Scaled Dot-Product  Attention10. The advan-\ntage of self-attention is fast calculation and shortening of the path between words, as well as potential interpret-\nability. This attention includes 3 vectors: queries, keys and values, and scaling (7):\nThese parameters are considered useful for calculating attention. Multi-head attention combines several self-\nattention maps into general matrix calculations (8):\nHere sh = Attention(QWQ\nh , KW K\nh , VW V\nh ) . h is the amount of attention in the layer, QW Q\nh , KW K\nh , VW V\nh , sh \n– trained weight matrices.\nThe multi-head attention mechanism can be used as an optimization problem. Using this mechanism, you \ncan bypass problems associated with unsuccessful initialization, as well as improve the speed of training. In addi-\ntion, after training, you can exclude some parts of the heads of attention, since these changes will not affect the \nquality of decoding in any way. The number of heads in the model is designed to regulate attention mechanisms. \nIn addition, this mechanism helps the network to easily access any information, regardless of the length of the \nsequence, because this is done easily, regardless of the number of words in the set.\n(7)Attention\n(\nU Q , U K , U V )\n= softmax\n( Q , K , V√dattention\n)\nV\n(8)MultiHeadAttention(Q ,K ,V ) = Concat(s1 ,... ,sh)U head\nEncoder block \nDecoder block \nSequence of words \nFigure 1.  General scheme of the model.\n6\nVol:.(1234567890)Scientific Reports |         (2022) 12:8337  | https://doi.org/10.1038/s41598-022-12260-y\nwww.nature.com/scientificreports/\nIn the Transformer architecture, you can see the Normalize element, which is necessary to normalize feature \nvalues, since after using the attention mechanism, these values can have different values. As a normalization, the \nLayer Normalization method is usually used (Fig. 2).\nThe outputs of several heads can also be different, and in the final vector the spread of values can be large. \nTo prevent this, an approach has been  proposed11 where values at each position are converted with a two-layer \nperception. After applying the attention mechanism, the values are projected to a larger dimension using the \ntrained weights, where they are then transformed by the nonlinear activation function ReLU, and then these \nvalues are projected to the original dimension, after which the next normalization occurs.\nProposed model. Typically, Connectionist temporal classification (CTC) is used as a loss function to train \nrecurrent neural networks to recognize input speech without pre-aligning the input and output  data11. To achieve \nhigh performance from the CTC model, it is necessary to use an external language model, since direct decoding \nwill not work correctly. In addition, the Kazakh language has a rather diverse mechanism of word formation, \nwhich the use of language mode contributes to an increase in the quality of recognition of Kazakh speech.\nIn this work, we will jointly use the Transformer and CTC models with LM. The use of LM CTC in decoding \nresults in rapid model convergence, which reduces the amount of time to decode and improves system perfor -\nmance. The CTC function, after receiving the output from the encoder, finds the probability by formula 9  for \narbitrary alignment between the encoder output and the output symbol sequence.\nHere x is the output vector of the encoder, R is an additional operator for removing blank spaces and repeated \nsymbols,γ is a series of predicted symbols. This equation determines the sum of all alignments using dynamic \nprogramming, and helps to train the neural network on unlabeled data.\nThe general structure of the resulting model is shown in Fig. 3.\nDuring training, the multi-task loss method was used to bring the general formula for combining probabilities \naccording to the negative logarithm, as presented  in10.\nThus, the resulting model can be represented by the following expression (10):\n(9)PCTC (W |Encoder(x)) =\n∑\nγǫ R− 1(y)\np(γ |x)\n(10)P(w |x) = /afii9838PTransformer+ (1 − /afii9838)PCTC\n… ……\nLinear\n+ \nNormalize\nFeed Forward\nSelf-Attention\nNormalize\n+\nSelf-Attention\nNormalize\nNormalize\nFeed Forward\nENCODER 1 ENCODER N \nEnc-Dec Attention\nNormalize\nSoftmax\nDECODER N\nSelf -Attention\nNormalize\nNormalize\nFeed Forward\nDECODER 1 \nAcoustic features\n… ……\n… …… \nFigure 2.  Transformer Model.\n7\nVol.:(0123456789)Scientific Reports |         (2022) 12:8337  | https://doi.org/10.1038/s41598-022-12260-y\nwww.nature.com/scientificreports/\nwhere /afii9838—configurable parameter and satisfies the condition—0 ≤ /afii9838≤ 1.\nThe following additions have been included to improve model performance:\n(1) Using a character-level language model in feature extraction. Convolutional neural networks were used \nto extract features. To extract high-dimensional features from the audio data, we first wrap all the network \nparameters under the last hidden CNN layer. Softmax was used as an activation function. Next, a maxpooling \nlayer was added to eliminate noise signals and reduce noise with dimensionality reduction. This layer is needed \nto reduce the size of the collapsed element into a vector. Also it helps to reduce the processing power required \nfor data processing by reducing the dimensionality. And adaptation of training with character-level language \nmodel, without disturbing the structure of the neural network during training, allows us to preserve maximum \nnon-linearity for subsequent processing. Thus, our extracted features are already high-level, and there is no need \nto map these raw data to phonemes.\n(2) Application of a language model at the level of words and phrases when decoding together with CTC.\nTo measure the quality of the Kazakh speech recognition system, the following parameters were used: CER—\nthe number of incorrectly recognized characters, because characters are the most common and simple output \nunits for generating texts; and based on the word error rate (WER)25.\nExperiments and results\nDataset. To train the Transformer, Transformer + CTC models with LM and without LM, it was decided to \ndivide the corpus of 400 h of speech into two parts: 200 h of \"pure\" speech and 200 h of spontaneous telephone \nspeech. This corpus was assembled in the laboratory \"Computer Engineering of Intelligent Systems\" IICT MES \n RK13,26. When creating the corpus, various types of speech were taken into account: prepared (reading), spon-\ntaneous. In the corpus, sound files are divided into training and test parts, these are 90% and 10%, respectively.\nThe pure speech database consists of recordings of 380 speakers, native Kazakh speakers of different ages and \ngenders, as well as speech data from artistic audiobooks and audio data of news broadcasts. The voice acting \nand recording of each speaker took about 40–50 min. For the text, sentences with the richest phoneme of words \nwere selected. Text data was collected from news sites in the Kazakh language, and other materials were used in \nelectronic form in the Kazakh language.\nTo record the speakers, students, doctoral students and undergraduates were involved as a scientific practice, \nas well as employees of the institute, colleagues from different parts of the country, as well as acquaintances and \nrelatives. Recording the voice-overs took about a year, and experts in the field of linguists and linguistics were \ninvolved to evaluate and review the corpus in order to ensure high quality.\nLinear\n+ +\nSelf-Attention \nNormalize \nNormalize \nFeed Forward \nCTC RNN LM ENCODERS 1-6 \nMulti-head Attention \nNormalize \nSoftmax\nSelf -Attention \nNormalize \nNormalize \nFeed Forward \nDECODERS 1-6 \nJOINT DECODING\nAcoustic features\nFigure 3.  The structure of our model.\n8\nVol:.(1234567890)Scientific Reports |         (2022) 12:8337  | https://doi.org/10.1038/s41598-022-12260-y\nwww.nature.com/scientificreports/\nRecordings of telephone conversations were provided by the telecommunications company for scientific use \nonly. Transcribing of telephone conversations was carried out on the basis of the developed methodology for the \ncompilation of texts, since this speech is spontaneous, and may contain information in a foreign language, and \nalso speech may contain various kinds of speech noises, non-speech noises, such as a tone dial signal, a telephone \nbeep, sounds resembling a blow, a click, as well as sounds that serve to think about the next statement. In addi-\ntion, there may be slurred speech, overlapping of several speakers, etc. It should be noted that the selection of text \narrays with predetermined statistical requirements for the contextual use of phonemes is a very time-consuming \ntask and took quite a long time. This process took 5–6 months and is still ongoing. it was necessary to check not \nonly the speech data, but also the correctness of the transcription of the data.\nThe speech recognition system does not require the creation of a dictionary at the phoneme level, it is enough \nto have audio data with text data.\nAfter the above works, one of the important elements was created—a vocabulary base for the speech recogni-\ntion system (10,805 non-repeating words). All recorded texts are collected in one file and repeated words have \nbeen removed. Once sorted alphabetically.\nThe audio data were in .wav format. All audio data have been converted to a single channel. The PCM method \nwas used to convert the data into digital form. Discrete frequency 44.1 kHz, 16-bit.\nThe PyTorch toolkit was used for the Transformer models. The experiments were carried out on a server \nwith eight AMD Ryzen 9 GPUs with a GeForce RTX3090. The datasets were stored on 1000 GB SSD memory \nto allow faster data flow during training.\nResults. To optimize the model, a gradient descent optimizer based on  Adam27 was used with optimal \nparameters β1 = 0.8, β2 = 0.95 и ϵ =  10−4, which leads to an increase in the learning  rate10. To improve the model, \nparameter values during training that affect model learning qualities were configured. At the training stage, 3 \nregularization methods were used, as indicated  in10, these are residual dropout, level normalization and label \nsmoothing. Residual dropout is applied before data normalization with a factor equal to 0.3 and then nor -\nmalization is applied. The label smoothing method was applied during training with a parameter of 0.1. These \nregularization techniques improve the accuracy of the system’s metrics and keep training from overlapping the \ntraining set. The proposed  in28 method was used to initialize the Transformer weights and as a study, 6 encoders \nand 6 decoders were installed, which are located one above the other. All configurable parameters were set the \nsame for the two datasets. The packet size was fixed at 64. For the CTC, the interpolation weight was set to 0.2 \nand consists of a directional six-layer BLSTM with 256 cells in each layer. The beam research width at the decode \nstage is 15. The language model contains two 1024-unit LSTM layers and it was trained with crated vocabulary \nbase for speech recognition system. The model has been trained for 45 epochs.\nThe following tables (Tables  1,2) show the results for CER and WER of the built models on two databases \n(dataset). Experiments were carried out to recognize Kazakh speech using different models.\nThe model trained on a pure data set showed competitive results only with the use of an external language \nmodel. In Table 1, it can be seen that the Transformer model with CTC works well with and without the use of \nthe language model and achieved a CER of 6.2% and a WER of 13.5%. The integration of an external language \nmodel made the system heavier, but significantly reduced the CER and WER rates by 3.7 and 8.3%, respectively.\nAs can be seen from the tables, the Transformer + CTC LM model shows the best result on two databases. \nIn addition, the Transformer model with CTC learned faster and converged quickly compared to other models \nthan without it (Figs. 4,5).\nThe resulting model was also easier to integrate with LM. With the help of the CTC, the obtained data were \naligned.\nThe results obtained during the experiment prove the effectiveness of the joint use of the CTC with the  \nE2E language model and showed the best result on all datasets in Kazakh. In addition, adding a CTC to our \nTable 1.  Model results on database 1—Read speech.\nModel CER WER\nCTC LM 8.5 16.9\nTransformer 7.6 14.1\nTransformer + CTC decode 6.2 13.5\nTransformer + CTC LM 3.7 8.3\nTable 2.  Model results on database 2—Conversational telephone speech.\nModel CER WER\nCTC LM 11.2 18.5\nTransformer 15.9 21.2\nTransformer + CTC decode 10.5 17.4\nTransformer + CTC LM 9.6 15.8\n9\nVol.:(0123456789)Scientific Reports |         (2022) 12:8337  | https://doi.org/10.1038/s41598-022-12260-y\nwww.nature.com/scientificreports/\nmodel generally improves the performance of the system. In the future, it is necessary to expand our speech \ncorpus and improve CER and WER.\nDiscussion\nTo improve the performance of these metrics, a language model trained on the basis of RNN was integrated \ninto the model. This is the only way to achieve good results in our case. In addition, if you conduct additional \nexperiments with a corpus that has more volume, then this addition can also affect the quality of recognition. \nHowever, increasing the amount of data for training will probably not solve the problem just like that. There are \na large number of dialects and accents of Kazakh speech. It is not possible to collect enough data for all cases.\nSpeech recognition systems make many more errors as noise increases. This can be noticed on the basis of \nexperiments related to the recognition of conversational telephone speech (Table 2). The model cannot simulta-\nneously recognize 2 people who are talking at the same time, this leads to the overlap of voice data. The issues of \ndiarization and separation of sources and the indicators for determining semantic errors have not been resolved.\nTransformer takes into account the entire context and learns the language model better, and CTC helps the \nmodel learn to produce recognition that is optimally aligned in time with the recording. This architecture can \nbe further adapted for streaming speech recognition.\nConclusion\nIn this paper, the Transformer architecture for automatic recognition of Kazakh continuous speech was consid-\nered, which uses self-attention components. Despite the multiple model parameters that need to be tuned, the \ntraining process can be shortened by parallelizing the processes. The combined Transformer + CTC LM model \nshowed better results in Kazakh speech recognition in terms of character and word recognition accuracy, and \nreduced these figures by 3.7 and 8.3%, respectively, than using them separately. This proves that the implemented \nmodel can be applied to other low-resource languages.\nIn further research, it is planned to increase the speech corpus for the Kazakh language to conduct experi -\nments on the implemented model, and it is also necessary to make significant improvements to the Transformer \nmodel to reduce word and symbol errors in the recognition of Kazakh continuous speech.\nFigure 4.  Comparing curves when training a model on Read speech.\nFigure 5.  Comparative graph of Transformer + CTC and other models on the test set on Read speech.\n10\nVol:.(1234567890)Scientific Reports |         (2022) 12:8337  | https://doi.org/10.1038/s41598-022-12260-y\nwww.nature.com/scientificreports/\nData availability\nNot applicable.\nReceived: 28 December 2021; Accepted: 5 May 2022\nReferences\n 1. Seide, G. L., & Yu, D. Conversational Speech. Transcription Using Context-Dependent Deep Neural. Networks. Interspeech (2011).\n 2. Bourlard, H., & Morgan, N. Connectionist speech recognition: A hybrid approach. p. 352 (1993) https://  doi. org/ 10. 1007/ \n978-1- 4615- 3210-1.\n 3. Smit, P ., Virpioja, S. & Kurimo, M. Advances in subword-based HMM-DNN speech recognition across languages. Comput. Speech \nLang. 66, 1. https:// doi. org/ 10. 1016/j. csl. 2020. 101158 (2021).\n 4. Wang, D., Wang, X. & Lv, S. An overview of end-to-end automatic speech recognition. Symmetry 11, 1018. https:// doi. org/ 10. 3390/ \nsym11 081018 (2019).\n 5. Mamyrbayev, O. & Oralbekova, D. Modern trends in the development of speech recognition systems. News of the National academy \nof sciences of the republic of Kazakhstan 4(332), 42–51 (2020).\n 6. Graves, A., Fernandez, S., Gomez, F ., & Schmidhuber, J. Connectionist Temporal Classification: Labelling Unsegmented Sequence \nData with Recurrent Neural Networks. In ICML, Pittsburgh, USA, 2006\n 7. Chan, W ., Jaitly, N., Le, Q. V ., & Vinyals, O. Listen attend and spell. In IEEE International Conference on Acoustics, Speech and \nSignal Processing, ICASSP , 2016\n 8. Cui, X., & Gong, Y . Variable parameter Gaussian mixture hidden Markov modeling for speech recognition. 2003 IEEE International \nConference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP ’03)., 2003, pp. I-I. https:// doi. org/ 10. 1109/ \nICASSP . 2003. 11987 04.\n 9. Y an, Y ., Qi, W ., Gong, Y ., Liu, D., Duan, N., Chen, J., Zhang, R., & Zhou, M. ProphetNet: Predicting future N-gram for sequence-\nto-sequence pre-training. arXiv - CS - Computation and Language, 2020. arxiv-2001.04063.\n 10. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. Attention is all you need. In \nProceedings of the 31st International Conference on Neural Information Processing Systems (NIPS’17). Curran Associates Inc., \nRed Hook, NY , USA, 6000–6010 (2017).\n 11. Karita, S., Soplin, N. E. Y ., Watanabe, S., Delcroix, M., Ogawa, A., & Nakatani, T. (2019). Improving transformer-based end-to-\nend speech recognition with connectionist temporal classification and language model integration. Proceedings of the Annual \nConference of the International Speech Communication Association, INTERSPEECH, 2019-September, 1408–1412. https:// doi. \norg/ 10. 21437/ Inter speech. 2019- 1938.\n 12. Miao, H., Cheng, G., Gao, C., Zhang, P ., & Y an, Y . Transformer-Based Online CTC/Attention End-To-End Speech Recognition \nArchitecture. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. \n6084–6088 (2020). https:// doi. org/ 10. 1109/ ICASS P40776. 2020. 90531 65.\n 13. Mamyrbayev, O., Oralbekova, D., Kydyrbekova, A., Turdalykyzy, T., & Bekarystankyzy, A. End-to-End Model Based on RNN-T \nfor Kazakh Speech Recognition. In 2021 3rd International Conference on Computer Communication and the Internet (ICCCI), \n2021, pp. 163–167. https:// doi. org/ 10. 1109/ ICCCI 51764. 2021. 94868 11.\n 14. Mamyrbayev, O., Alimhan, K., Oralbekova, D., Bekarystankyzy, A. & Zhumazhanov, B. Identifying the influence of transfer learning \nmethod in developing an end-to-end automatic speech recognition system with a low data level. Eastern-Eur. J. Enterpris. Technol. \n19(115), 84–92 (2022).\n 15. Kamath, U., Liu, J. & Whitaker, J. Deep Learning for NLP and Speech Recognition (Springer, 2019).\n 16. El-Henawy, I. M., Khedr, W . I. & ELkomy OM, Abdalla A-ZMI,. Recognition of phonetic Arabic figures via wavelet-based Mel \nFrequency Cepstrum using HMMs. HBRC J. 10(1), 49–54 (2014).\n 17. Mohan, B. J. & Ramesh Babu, N. Speech recognition using MFCC and DTW . International Conference on Advances in Electrical \nEngineering (ICAEE) 1, 1–4. https:// doi. org/ 10. 1109/ ICAEE. 2014. 68385 64 (2014).\n 18. Dave, N. Feature extraction methods LPC, PLP and MFCC in speech recognition. Int. J. Adv. Res. Eng. Technol. 1, 1 (2013).\n 19. Moritz, N., Hori, T., & Le, J. Streaming Automatic Speech Recognition with the Transformer Model. In ICASSP 2020—2020 IEEE \nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 6074–6078. https:// doi. org/ 10. 1109/ \nICASS P40776. 2020. 90544 76.\n 20. Shi, Y ., Wang, Y ., Wu, C., Fuegen, C., Zhang, F ., Le, D., Y eh, C., & Seltzer, M. Weak-attention suppression for transformer-based \nspeech recognition. ArXiv abs/2005.09137 (2020).\n 21. Dong, L., Xu, S., & Xu, B. Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition. In 2018 \nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp 5884–5888 (2018).\n 22. Gangi, M. A. D., Negri, M., Cattoni, R., Dessì, R., & Turchi, M. Enhancing Transformer for End-to-end Speech-to-Text Translation. \nMTSummit (2019).\n 23. Hori, T., Moritz, N., Hori, C., & Roux, J. L. Transformer-based Long-context End-to-end Speech Recognition. INTERSPEECH \n2020, Shanghai, China (2020).\n 24. Chang, X., Zhang, W ., Qian, Y ., Le Roux, J., & Watanabe, S. End-to-end multi-speaker speech recognition with transformer. In \nICASSP 2020–2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\n 25. Levenshtein, V . I. Binary codes capable of correcting deletions, insertions, and reversals. Sov. Phys. Doklady 10, 707–710 (1996).\n 26. Mamyrbayev, O. et al. Development of security systems using DNN and i & x-vector classifiers. East.-Eur. J. Enterpris. Technol.  \n49(112), 32–45 (2021).\n 27. Kingma, D. P ., & Adam, B. J. A method for stochastic optimization. arXiv, 2014. http:// arxiv. org/ abs/ 1412. 6980 (data of request: \n18.04.2021).\n 28. LeCun, Y ., Bottou, L., Orr, G. B., & M¨uller, K.-R. Efficient backprop. In Neural Networks: Tricks of the Trade, This Book is an \nOutgrowth of a 1996 NIPS Workshop, 1998, pp. 9–50.\nAcknowledgements\nThis research has been funded by the Science Committee of the Ministry of Education and Science of the Republic \nKazakhstan (Grant No. AP08855743).\nAuthor contributions\nO.M. built a model and applied transfer learning to realized recognition model and participated in the prepara-\ntion of the manuscript, K.A. and M.O. carried out the analysis of literatures on the topic under study, D.O. built an \nend-to-end model based on Transformer, participated in the research and prepared the manuscript, T.T. prepared \ndata for training, D.O. helped in drawing up the program. All authors read and approved the final manuscript.\n11\nVol.:(0123456789)Scientific Reports |         (2022) 12:8337  | https://doi.org/10.1038/s41598-022-12260-y\nwww.nature.com/scientificreports/\nFunding\nThis study was funded by the Science Committee of the Ministry of Education and Science of the Republic \nKazakhstan (Grant No. AP08855743).\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to O.D.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8455120325088501
    },
    {
      "name": "Connectionism",
      "score": 0.6699863076210022
    },
    {
      "name": "Transformer",
      "score": 0.6321045160293579
    },
    {
      "name": "Speech recognition",
      "score": 0.5716969966888428
    },
    {
      "name": "Language model",
      "score": 0.5167862176895142
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4706769585609436
    },
    {
      "name": "End-to-end principle",
      "score": 0.4687894284725189
    },
    {
      "name": "Recurrent neural network",
      "score": 0.46563780307769775
    },
    {
      "name": "Kazakh",
      "score": 0.44283297657966614
    },
    {
      "name": "Artificial neural network",
      "score": 0.3924853205680847
    },
    {
      "name": "Natural language processing",
      "score": 0.3616533577442169
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}