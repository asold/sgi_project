{
  "title": "MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering",
  "url": "https://openalex.org/W3094798944",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287370308",
      "name": "Khan, Aisha Urooj",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744157328",
      "name": "Mazaheri, Amir",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288501369",
      "name": "Lobo, Niels da Vitoria",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743042772",
      "name": "Shah, Mubarak",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2202226326",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2963799607",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2968880719",
    "https://openalex.org/W2735159761",
    "https://openalex.org/W2963890755",
    "https://openalex.org/W2963150162",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2726160912",
    "https://openalex.org/W3010593057",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W2967593235",
    "https://openalex.org/W2963656855",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W2340874616",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2962749469",
    "https://openalex.org/W2950761309",
    "https://openalex.org/W2471094925",
    "https://openalex.org/W2964306924",
    "https://openalex.org/W2964306921",
    "https://openalex.org/W2174492417",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2978723076",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W2947962063",
    "https://openalex.org/W2963579811",
    "https://openalex.org/W2964322347",
    "https://openalex.org/W2783776165",
    "https://openalex.org/W2940906331",
    "https://openalex.org/W2964091467",
    "https://openalex.org/W2732016772",
    "https://openalex.org/W2969127500",
    "https://openalex.org/W2550936021",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2963890019",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963541336",
    "https://openalex.org/W2963223524",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2741631785"
  ],
  "abstract": "We present MMFT-BERT(MultiModal Fusion Transformer with BERT encodings), to solve Visual Question Answering (VQA) ensuring individual and combined processing of multiple input modalities. Our approach benefits from processing multimodal data (video and text) adopting the BERT encodings individually and using a novel transformer-based fusion method to fuse them together. Our method decomposes the different sources of modalities, into different BERT instances with similar architectures, but variable weights. This achieves SOTA results on the TVQA dataset. Additionally, we provide TVQA-Visual, an isolated diagnostic subset of TVQA, which strictly requires the knowledge of visual (V) modality based on a human annotator's judgment. This set of questions helps us to study the model's behavior and the challenges TVQA poses to prevent the achievement of super human performance. Extensive experiments show the effectiveness and superiority of our method.",
  "full_text": "MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings\nfor Visual Question Answering\nAisha Urooj Khan Amir Mazaheri Niels Da Vitoria Lobo Mubarak Shah\nCenter for Research in Computer Vision, University of Central Florida\naishaurooj@gmail.com, amirmazaheri@knights.ucf.edu\nshah@crcv.ucf.edu, niels@cs.ucf.edu\nAbstract\nWe present MMFT-BERT ( MultiModal\nFusion Transformer with BERT encodings),\nto solve Visual Question Answering (VQA)\nensuring individual and combined processing\nof multiple input modalities. Our approach\nbeneﬁts from processing multimodal data\n(video and text) adopting the BERT encodings\nindividually and using a novel transformer-\nbased fusion method to fuse them together.\nOur method decomposes the different sources\nof modalities, into different BERT instances\nwith similar architectures, but variable\nweights. This achieves SOTA results on the\nTVQA dataset. Additionally, we provide\nTVQA-Visual, an isolated diagnostic subset of\nTVQA, which strictly requires the knowledge\nof visual (V) modality based on a human\nannotator’s judgment. This set of questions\nhelps us to study the model’s behavior and\nthe challenges TVQA poses to prevent the\nachievement of super human performance.\nExtensive experiments show the effectiveness\nand superiority of our method1.\n1 Introduction\nIn the real world, acquiring knowledge requires\nprocessing multiple information sources such as\nvisual, sound, and natural language individually\nand collectively. As humans, we can capture ex-\nperience from each of these sources (like an iso-\nlated sound); however, we acquire the maximum\nknowledge when exposed to all sources concur-\nrently. Thus, it is crucial for an ideal Artiﬁcial\nIntelligence (AI) system to process modalities indi-\nvidually and jointly. One of the ways to understand\nand communicate with the world around us is by\nobserving the environment and using language (dia-\nlogue) to interact with it (Lei et al., 2018). A smart\n1Code will be available at https://github.com/\naurooj/MMFT-BERT\nFigure 1: MultiModal Fusion Transformer (MMFT):\nWe treat input modalities as a sequence. [FUSE] is\na trainable vector; hq0 j , hv0 j , and hs0 j are ﬁxed-length\nfeatures aggregated over question-answer (QA) pairs,\nvisual concepts, and subtitles. Using a transformer\nencoder block, [FUSE] attends all source vectors and\nassigns weights based on the importance of each in-\nput source. Training end to end for VQA enables the\nMMFT module to learn to aggregate input sources w.r.t.\nthe nature of the question. For illustration purposes,\nwe show that for a single head, MMFT collects more\nknowledge from the visual source hv0 j (green colored)\nthan from the QA and subtitles. Best viewed in color.\nsystem, therefore, should be able to process visual\ninformation to extract meaningful knowledge as\nwell as be able to use that knowledge to tell us\nwhat is happening. The story is incomplete if we\nisolate the visual domain from language. Now that\nadvancements in both computer vision and natural\nlanguage processing are substantial, solving prob-\nlems demanding multimodal understanding (their\nfusion) is the next step. Answering questions about\nwhat can be seen and heard lies somewhere along\nthis direction of investigation. In research towards\nthe pursuit of combining language and vision, vi-\nsual features are extracted using pre-trained neural\nnetworks for visual perception (He et al., 2016;\nRen et al., 2015), and word embeddings are ob-\ntained from pre-trained language models (Mikolov\net al., 2013b,a; Pennington et al., 2014; Devlin\net al., 2018) and these are merged to process mul-\ntiple modalities for various tasks: visual question\nanswering (VQA), visual reasoning, visual ground-\ning.\nTVQA (Lei et al., 2018), a video-based ques-\narXiv:2010.14095v1  [cs.CV]  27 Oct 2020\ntion answering dataset, is challenging as it pro-\nvides more realistic multimodal question answers\n(QA) compared to other existing datasets . To an-\nswer TVQA questions, the system needs an un-\nderstanding of both visual cues and language. In\ncontrast, some datasets are focused either visu-\nally: MovieFIB (Maharaj et al., 2017), Video Con-\ntext QA (Zhu et al., 2017), TGIF-QA (Jang et al.,\n2017); or by language: MovieQA (Tapaswi et al.,\n2016); or based on synthetic environments: Mari-\noQA (Mun et al., 2017) and PororoQA (Kim et al.,\n2017). We choose TVQA because of its challenges.\nThe introduction of transformers (Vaswani et al.,\n2017) has advanced research in visual question an-\nswering and shows promise in the ﬁeld of language\nand vision in general. Here, we adopt the pre-\ntrained language-based transformer model, BERT\n(Devlin et al., 2018) to solve the VQA task. The\nhuman brain has vast capabilities and probably con-\nducts processing concurrently. Like humans, an\nintelligent agent should also be able to process\neach input modality individually and collectively\nas needed. Our method starts with independent pro-\ncessing of modalities and the joint understanding\nhappens at a later stage. Therefore, our method\nis one step forward toward better joint understand-\ning of multiple modalities. We use separate BERT\nencoders to process each of the input modalities\nnamely Q-BERT, V-BERT and S-BERT to process\nquestion (Q), video (V), and subtitles (S) respec-\ntively. Each BERT encoder takes an input source\nwith question and candidate answer paired together.\nThis is important because we want each encoder\nto answer the questions targeted at its individual\nsource input. Thus, pairing up the question and\ncandidate answers enables each stream to attend\nto the relevant knowledge pertinent to the ques-\ntion by using a multi-head attention mechanism\nbetween question words and a source modality. We\nthen use a novel transformer based fusion mech-\nanism to jointly attend to aggregated knowledge\nfrom each input source, learning to obtain a joint\nencoding. In a sense, our approach is using two\nlevels of question-to-input attention: ﬁrst, inside\neach BERT encoder to select only relevant input;\nand second, at the fusion level, in order to fuse all\nsources to answer the common question. We show\nin our experiments that using Q-BERT, a separate\nBERT encoder for question and answer is helpful.\nOur contribution is three-fold:\nFirst, we propose a novel multi-stream end-to-end\ntrainable architecture which processes each input\nsource separately followed by feature fusion over\naggregated source features. Instead of combining\ninput sources before input to BERT, we propose to\nprocess them individually and deﬁne an objective\nfunction to optimize multiple BERTs jointly. Our\napproach achieves state-of-the-art results on the\nvideo-based question answering task.\nSecond, we propose a novel MultiModal Fusion\nTransformer (MMFT) module, repurposing trans-\nformers for fusion among multiple modalities. To\nthe best of our knowledge, we are the ﬁrst to use\ntransformers for fusion.\nThird, we isolate a subset of visual questions,\ncalled TVQA-Visual (questions which require only\nvisual information to answer them). Studying our\nmethod’s behavior on this small subset illustrates\nthe role each input stream is playing in improving\nthe overall performance. We also present detailed\nanalysis on this subset.\n2 Related Work\nImage-based Question Answering. Image-based\nVQA (Yu et al., 2015; Antol et al., 2015; Zhu et al.,\n2016; Jabri et al., 2016; Chao et al., 2018) has\nshown great progress recently. A key ingredient is\nattention (Ilievski et al., 2016; Chen et al., 2015;\nYu et al., 2017a,b; Xu and Saenko, 2016; Anderson\net al., 2018). Image based VQA can be divided\nbased on the objectives such as generic VQA on\nreal world images (Antol et al., 2015; Goyal et al.,\n2017), asking binary visual questions (Zhang et al.,\n2016) and reasoning based VQA collecting visual\ninformation recurrently (Kumar et al., 2016; Xiong\net al., 2016; Weston et al., 2014; Sukhbaatar et al.,\n2015; Hudson and Manning, 2018) to answer the\nquestion both in synthetic (Johnson et al., 2016;\nYang et al., 2018; Suhr et al., 2017) as well as real\nimage datasets (Hudson and Manning, 2019).\nVideo-based Question Answering. Video-based\nQA is more challenging as it requires spatiotem-\nporal reasoning to answer the question. (Lei et al.,\n2018) introduced a video-based QA dataset along\nwith a two-stream model processing both video\nand subtitles to pick the correct answer among can-\ndidate answers. Some studies are: grounding of\nspatiotemporal features to answer questions (Lei\net al., 2019); a video ﬁll in the blank version of\nVQA (Mazaheri et al., 2017); other examples in-\nclude (Kim et al., 2019b,a; Zadeh et al., 2019; Yi\net al., 2019; Mazaheri and Shah, 2018).\nFigure 2: Overview of the proposed approach. Q-BERT, V-BERT and S-BERT represent text encoder, visual en-\ncoder and subtitles encoder respectively. Ifhj=Q+Aj is jth hypothesis, then Q-BERT takeshj, V-BERT takes visual\nconcepts V+hj, and S-BERT takes subtitles S+hj as inputs respectively. The aggregated features from each BERT\nare concatenated with [FUSE], a special trainable vector, to form a sequence and input into the MMFT module\n(see section 3.2.4 for details). Outputs from the MMFT module for each answer choice are concatenated together\nand are input into a linear classiﬁer to obtain answer probabilities. We optimize individual BERTs along with\noptimizing the full model together. Losstotal denotes our objective function used to train the proposed architecture.\nAt inference time, we take features only from the MMFT module.\nRepresentation Learning. BERT has demon-\nstrated effective representation learning using self-\nsupervised tasks such as masked language model-\ning and next sentence prediction tasks. The pre-\ntrained model can then be ﬁnetuned for a variety\nof supervised tasks. QA is one such task. A single-\nstream approach takes visual input and text into a\nBERT-like transformer-based encoder; examples\nare: VisualBERT (Li et al., 2019b), VL-BERT\n(Su et al., 2019), Unicoder-VL (Li et al., 2019a)\nand B2T2 (Alberti et al., 2019). Two-stream ap-\nproaches need an additional fusion step; ViLBERT\n(Lu et al., 2019) and LXMERT (Tan and Bansal,\n2019) employ two modality-speciﬁc streams for\nimages. We take this a step further by employing\nthree streams. We use a separate BERT encoder\nfor the question-answer pair. We are speciﬁcally\ntargeting video QA and do not need any additional\npre-training except using pre-trained BERT.\n3 Approach\nOur approach permits each stream to take care of\nthe questions requiring only that input modality. As\nan embodiment of this idea, we introduce the Mul-\ntiModal Fusion Transformer with BERT encodings\n(MMFT-BERT) to solve VQA in videos. See ﬁg.\n1 for the proposed MMFT module and ﬁg. 2 for\nillustration of our full architecture.\n3.1 Problem Formulation\nIn this work, we assume that each data sample is a\ntuple (V, T, S, Q, A, l) comprised of the following:V:\ninput video; T: T = [tstart , tend], i.e., start and end\ntimestamps for answer localization in the video; S:\nsubtitles for the input video; Q: question about the\nvideo and/or subtitles; A: set of C answer choices;\nl: label for the correct answer choice.\nGiven a question with both subtitles and video\ninput, our goal is to pick the correct answer from\nC candidate answers. TVQA has 5 candidate an-\nswers for each question. Thus, it becomes a 5-way\nclassiﬁcation problem.\n3.2 MultiModal Fusion Transformer with\nBERT encodings (MMFT-BERT)\n3.2.1 Q-BERT:\nOur text encoder named Q-BERT takes only QA\npairs. The question is paired with each candidate\nanswer Aj, where, j = 0, 1, 2, 3, 4;|A|= C. BERT\nuses a special token [CLS] to obtain an aggregated\nfeature for the input sequence, and uses [SEP] to\ndeal with separate sentences. We, therefore, use the\noutput corresponding to the [CLS] token as the ag-\ngregated feature from Q-BERT and [SEP] is used\nto treat the question and the answer choice as sep-\nInput\nQ Q+V Q+S Q+V+S\nMethod Text Feat Vis. Feat w/o ts w/ ts w/o ts w/ ts w/o ts w/ ts w/o ts w/ ts\nLSTM(Q) Glove - 42.74 42.74 - - - - - -\nMTL (Kim et al., 2019a) Glove cpt - - - 43.45 - 64.36 - 66.22\nTwo-stream(Lei et al., 2018) Glove cpt 43.50 43.50 43.03 45.03 62.99 65.15 65.46 67.70\nPAMN (Kim et al., 2019b) Word2vec cpt - - - - - - 66.77 -\nSingle BERT BERT cpt - - - 48.95 - - - 72.20\nSTAGE (Lei et al., 2019) BERT reg - - - - - - 68.56 70.50\nW ACV20(Yang et al., 2020) BERT cpt 46.88 46.88 - 48.95 - 70.65 63.07 72.45\nOurs-SF BERT cpt 47.64 47.64 49.52 50.65 69.92* 70.33 65.55 73.10\nOurs-MMFT BERT cpt 47.64 47.64 49.32 51.36 69.98 * 70.79 66.10 73.55\nOurs-MMFT(ensemble) BERT cpt - - - 53.08 - - - 74.97\nTable 1: Comparison of our method with baseline methods on TVQA validation set. STAGE uses regional features for detected\nobjects in the video, all other models use visual concepts, ts= timestamp annotation, cpt=visual concepts, reg=regional features.\nOurs-SF represents proposed method with simple fusion, MMFT represents proposed multimodal fusion transformer, * indicates\nmodel trained with max seq len=512. The MMFT ensemble is 7x systems which use different training seeds.\narate sentences. The input I to the text encoder is\nformulated as:\nIqj = [CLS]+ Q +[SEP]+ Aj, (1)\nwhere, + is the concatenation operator, [CLS] and\n[SEP] are special tokens, Q denotes the question,\nand Aj denotes the answer choice j, Iqj is the input\nsequence which goes into Q-BERT and represents\nthe combination of question and the jth answer.\nWe initiate an instance of the pre-trained BERT to\nencode each of the Iqj sequences:\nhq0 j = Q-BERT (Iqj )[ 0], (2)\nwhere [0] denotes the index position of the aggre-\ngated sequence representation for only textual in-\nput. Note that, the [0] position of the input se-\nquence is [CLS].\n3.2.2 V-BERT:\nWe concatenate each QA pair with the video to\ninput to our visual encoder V-BERT. V-BERT is\nresponsible for taking care of the visual questions.\nPairing question and candidate answer with visual\nconcepts allows V-BERT to extract visual knowl-\nedge relevant to the question and paired answer\nchoice. Input to our visual encoder is thus formu-\nlated as follows:\nIvj = [CLS]+ V +”.”+ Q +[SEP]+ Aj, (3)\nwhere, V is the sequence of visual concepts 2, ”.”\nis used as a special input character, Ivj is the input\nsequence which goes into our visual encoder.\nhv0 j = V-BERT (Ivj )[ 0], (4)\nwhere, [0] denotes the index position of the aggre-\ngated sequence representation for visual input.\n2Visual concepts is a list of detected object labels using\nFasterRCNN (Ren et al., 2015) pre-trained on Visual Genome\ndataset. We use visual concepts provided by (Lei et al., 2018).\n3.2.3 S-BERT:\nThe S-BERT encoder applies attention between\neach QA pair and subtitles and results in an aggre-\ngated representation of subtitles and question for\neach answer choice. Similar to the visual encoder,\nwe concatenate the QA pair with subtitles as well;\nand the input is:\nIsj = [CLS]+ S +”.”+ Q +[SEP] +Aj, (5)\nwhere, S is the subtitles input, Isj is the resulting in-\nput sequence which goes into the S-BERT encoder.\nhs0 j = S-BERT (Isj )[ 0]. (6)\nwhere, [0] denotes the index position of the aggre-\ngated sequence representation for subtitles input.\n3.2.4 Fusion Methods\nLet Ii ∈Rd denote the feature vector for ith in-\nput modality with total n input modalities i.e.\nI1, I2, ...,In, d represents the input dimensionality.\nWe discuss two possible fusion methods:\nSimple Fusion: A simple fusion method is a\nHadamard product between all input modalities\nand given as follows:\nhFUSE = I1 ⊙I2 ⊙... ⊙In, (7)\nwhere, hFUSE is the resulting multimodal represen-\ntation which goes into the classiﬁer. Despite being\nextremely simple, this method is very effective in\nfusing multiple input modalities.\nMultiModal Fusion Tranformer (MMFT): The\nMMFT module is illustrated in ﬁg. 1. We treat Ii\nas a ﬁxed d-dimensional feature aggregated over\ninput for modality i. Inspired by BERT(Devlin\net al., 2018), we treat aggregated input features\nfrom multiple modalities as a sequence of features\nby concatenating them together. We concatenate a\nspecial trainable vector [FUSE ]3 as the ﬁrst feature\nvector of this sequence. The ﬁnal hidden state out-\nput corresponding to this feature vector is used as\nthe aggregated sequence representation over input\nfrom multiple modalities denoted as hFUSE .\nhFUSE = MMFT (I1 +I2 +... +In)[ 0], (8)\nwhere, + is the concatenation operator, [0] indi-\ncates the index position of the aggregated sequence\nrepresentation over all input modalities.\nIn our case, we have three input types: QA\npair, visual concepts and subtitles. For inputs i =\n{1, 2, 3}and answer index j = {0, 1, 2, 3, 4}, the\ninput to our MMFT module is I1 = hq0 j , I2 = hv0 j ,\nand I3 = hs0 j and the output is hFUSE denoting hid-\nden output corresponding to the [FUSE] vector.\nHere, hq0 j , hv0 j , and hs0 j are the aggregated outputs\nwe obtain from Q-BERT, V-BERT and S-BERT\nrespectively.\n3.2.5 Joint Classiﬁer\nAssuming a hypothesis for each tuple\n(V, T, S, Q, Aj), where Aj ∈A; j = 0, ..,4 denotes\nﬁve answer choices, our proposed Transformer Fu-\nsion module outputs hFUSE j ∈Rd. We concatenate\nthe aggregated feature representation for all the\nanswers together and send this to a joint classiﬁer\nto produce 5 answer scores, as follows:\nhf inal= hFUSE 0 +hFUSE 1 +... +hFUSE 4 , (9)\nscoresjoint = classi f ierjoint (hf inal), (10)\nwhere, hf inal∈RC·d and scoresjoint ∈RC, C denotes\nnumber of classes.\n3.3 Objective Function\nAlong with joint optimization, each of the Q-BERT,\nV-BERT and S-BERT are optimized with a single\nlayer classiﬁer using a dedicated loss function for\neach of them. Our objective function is thus com-\nposed of four loss terms: one each to optimize\neach of the input encoders Q-BERT, V-BERT and\nS-BERT, and a joint loss term over classiﬁcation\nusing the combined feature vector. The formulation\nof the ﬁnal objective function is as follows:\nLtotal = Lq +Lvid +Lsub +Ljoint , (11)\nwhere, Lq, Lvid, Lsub, and Ljoint denote loss func-\ntions for question-only, video, subtitles, and joint\nloss respectively; all loss terms are computed using\nsoftmax cross-entropy loss function using label l.\nThe model is trained end-to-end using Ltotal .\n3[FUSE ] is initialized as a d-dimensional zero vector.\nInput Model Acc (%)\nQ+V\nMTL (Kim et al., 2019a) 44.42\nTwo-stream (Lei et al., 2018) 45.44\nOurs - MMFT 51.83\nQ+V+S\nMTL (Kim et al., 2019a) 67.05\nTwo-stream (Lei et al., 2018) 68.48\nSTAGE (Lei et al., 2019) 70.23\nW ACV20 (Yang et al., 2020) 72.71\nOurs - MMFT model 72.89\nTable 2: Performance comparison of different models on\nTVQA testset-public with timestamp annotations. All models\nuse visual concepts except STAGE. We do not report numbers\nfor other comparisons (Q+S and w/o ts) because only limited\nattempts are allowed to the test server for evaluation.\nInp. Method Question family (Accuracy%)\nwhat who where why how others all\nQ+V\nTwo-stream 47.70 34.60 47.86 45.92 42.44 39.10 45.03\nW ACV20 51.31 41.14 52.86 48.45 46.24 36.86 48.95\nOurs-SF 52.76 42.52 52.36 51.42 46.75 41.61 50.65\nOurs-MMFT 52.97 43.58 54.00 53.00 46.97 44.16 51.36\nQ+V+S\nTwo-stream 66.05 67.99 61.46 71.53 78.77 74.09 67.70\nOurs-SF 71.52 72.10 68.93 76.99 82.25 83.2 73.10\nOurs-MMFT 72.22 72.39 69.89 76.92 81.74 82.48 73.55\nTable 3: Performance comparison for each question family.\nAll models are trained with localized input (w/ ts).\n4 Dataset\nIn TVQA, each question (Q) has 5 answer choices.\nIt consists of 152K QA pairs with 21.8K video\nclips. Each question-answer pair has been provided\nwith the localized video V to answer the question Q,\ni.e., start and end timestamps are annotated. Subti-\ntles S have also been provided for each video clip.\nSee supplementary work for a few examples.\n4.1 TVQA-Visual\nTo study the behavior of state-of-the-art models on\nquestions where only visual information is required\nto answer the question correctly, we selected 236\nsuch visual questions. Due to imperfections in the\nobject detection labels, only approximately 41%\nof these questions have the adequate visual input\navailable. We, therefore, refer to TVQA-Visual\nin two settings: TVQA-Visual (full):– full set of\n236 questions. A human annotator looked into the\nvideo carefully to ensure that the raw video is sufﬁ-\ncient to answer the question without using subtitles.\nTVQA-Visual (clean): This is the subset of 96\nquestions where the relevant input was available,\nyet the models perform poorly. For this subset, we\nrely on a human annotator’s judgement who veri-\nﬁed that either the directly related visual concept\nor the concepts hinting toward the correct answer\nare present in the list of detected visual concepts.\nFor instance, if the correct answer is “kitchen”, ei-\nFigure 3: Accuracy comparison with respect to the ques-\ntion family between Two-stream (Lei et al., 2018), W ACV20\n(Yang et al., 2020) and our method on the validation set of\nTVQA. Models were trained on Q+V . MMFT outperforms on\nall question types for Q+V .\nFigure 4: Testing accuracy curves on TVQA-Vis. (clean)\nand TVQA val set for different BERT streams during training.\nSolid lines: validation accuracy, dotted lines: visual set accu-\nracy. Although S-BERT is signiﬁcantly above V-BERT for\nfull validation set, for visual set, we can see that V-BERT is\nwell above Q-BERT and S-BERT. This shows that each BERT\ncontributes to the questions it is responsible for. Numbers are\nlog-scaled.\nther “kitchen” or related concepts (e.g. “stove”,\n“plate”, “glass”, etcetera) should be present in the\nlist of visual concepts. Thus, this easier subset is\ntermed as TVQA-Visual (clean). TVQA-visual,\nalthough small, is a diagnostic video dataset for\nsystematic evaluation of computational models on\nspatio-temporal question answering tasks and will\nhelp in looking for ways to make the V-stream con-\ntribution more effective. See supplementary mate-\nrial for the distribution of visual questions based on\nreasons for failure. If a model is correctly answer-\ning TVQA-visual questions which are not “clean”\n(the relevant concepts are missing from the visual\ninput), that is because of statistical bias in the data.\n5 Experiments and Results\n5.1 Baselines\nLSTM(Q): LSTM(Q) is a BiLSTM model to en-\ncode question and answer choices. The output from\nMethod TVQA-Vis. (clean) TVQA Vis. (full)\nTwo-stream 35.42 29.49\nW ACV20 42.71 40.00\nSF 42.71 34.37\nMMFT 46.88 39.57\nTable 4: Performance comparison on TVQA-Visual ques-\ntions for clean set and full set. Numbers reported for only\nQ+V (w/ts) model. Numbers are reported as percentage.\nLSTM for question and each answer choice is con-\ncatenated and is input to a 5-way classiﬁer to output\n5 answer probability scores.\nMTL: MTL (Kim et al., 2019a) uses two auxiliary\ntasks with the VQA task: temporal alignment and\nmodality alignment.\nTwo-stream: (Lei et al., 2018) uses two separate\nstreams for attention-based context matching each\ninput modality with question and candidate an-\nswers. A BiLSTM with max pooling over time\nis used to aggregate the resulting sequence.\nBERT: A single pre-trained BERT instance is ﬁne-\ntuned on QA pair along with visual concepts and\nsubtitles all together (Q+V+S).\nSTAGE: (Lei et al., 2019) uses moment localiza-\ntion and object grounding along with QA pair and\nsubtitles. STAGE uses BERT features to encode\ntext and spatio-temporal features for video.\nW ACV20:(Yang et al., 2020) concatenates subti-\ntles and visual concepts with QA pairs and input to\nBERT along with late fusion for Q+V and Q+S.\n5.2 MMFT-BERT\nFor video representation, we use detected attribute\nobject pairs as visual features provided by (Lei\net al., 2018). We follow (Lei et al., 2018) and\nonly unique attribute-object pairs are kept. Q-\nBERT, V-BERT and S-BERT are initialized with\nBERTbase pre-trained on lower-cased English text\nwith masked language modeling task. The MMFT\nmodule uses single transformer encoder layer (L=1)\nwith multi-head attention. We use 12 heads (H=12)\nfor multi-head attention in the MMFT module for\nour best model. We initialize the MMFT module\nwith random weights. A d-dimensional hidden fea-\nture output corresponding to [CLS] token is used as\nan aggregated source feature from each BERT. We\nconcatenate these aggregated features for each can-\ndidate answer together to acquire a feature of size\n5 ·d. A 5-way classiﬁer is then used to optimize\neach of Q-BERT, V-BERT and S-BERT indepen-\ndently. For joint optimization of the full model,\nwe treat the encoders’ output as a sequence of fea-\ntures with the order [[FUSE ], hq0 j , hv0 j , hs0 j ] and\ninput this into the MMFT module ( [FUSE] is a\nFigure 5: Visualization of multi-head attention (averaged over all heads) between different source features: Q, V\nand S for our best model. MMFT takes a sequence: [FUSE, Q, V , S] and uses multi-head attention for multimodal\nfusion. [FUSE] is the aggregated feature over Q, V , and S; column 1: representative image frame, column 2:\nlocalized subtitles, column 3: question with candidate answers (correct answer with corresponding attention map\nis in bold text and box respectively), columns 4-8 show attention for A0, A1, A2, A3, and A4 respectively. Top 2\nrows show attention weights for visual questions, next 2 rows are subtitles-based questions. Last row, depends on\nboth subtitles and visual information. See sec. 5.3.1 and supplementary work for details and insights.\ntrainable d-dimensional vector parameter). Output\ncorresponding to [FUSE] token is treated as an\naccumulated representation hFUSE j over all input\nmodalities for answer j. We concatenate hFUSE j\nfor each answer choice to obtain hf inalfor the joint\nclassiﬁcation. We learn four linear layers, one on\ntop of each of the three input encoders and the\nMMFT encoder respectively. Thus, each linear\nlayer takes a (5·d)-dimensional input and produces\n5 prediction scores.\nTraining Details. The entire architecture was\nimplemented using Pytorch (Paszke et al., 2019)\nframework. All the reported results were obtained\nusing the Adam optimizer (Kingma and Ba, 2014)\nwith a minibatch size of 8 and a learning rate of 2e-\n5. Weight decay is set to 1e-5. All the experiments\nwere performed under CUDA acceleration with\ntwo NVIDIA Turing (24GB of memory) GPUs. In\nall experiments, the recommended train / validation\n/ test split was strictly observed. We use the 4th\nlast layer from each BERT encoder for aggregated\nsource feature extraction. The training time varies\nbased on the input conﬁguration. It takes ∼4 hrs\nto train our model with Q+V and ∼8-9 hrs to train\non the full model for a single epoch. All models\nwere trained for 10 epochs. Our method achieves\nits best accuracy often within 5 epochs.\n5.3 Results\nAll results here use the following hyperparameters:\ninput sequence length max seq len=256, # heads\nH=12, # encoder layers L=1 for the MMFT module,\nand pre-trained BERTbase weights for Q-BERT, V-\nBERT and S-BERT unless speciﬁed explicitly.\nWith timestamp annotations (w/ ts). Columns\nwith “w/ ts” in table 1 show results for input with\ntimestamp localization. We get consistently bet-\nter results when using localized visual concepts\nand subtitles. We get 1.7% and 0.65% improve-\nment over WACV20 (Yang et al., 2020) with sim-\nple fusion for Q+V and Q+V+S inputs respectively.\nWhen using the MMFT for fusion, our method\nachieves SOTA performance with all three input\nsettings: Q+V (↑2.41%), Q+S (↑0.14) and Q+V+S\n(↑1.1%) (see table 1). Our fusion approach con-\ntributes to improved performance and gives best\nresults for localized input. See table 3 and ﬁg. 3\nfor results w.r.t. question family.\nWithout timestamp annotations (w/o ts). We\nModel Acc.(%)\n1 Single BERT 72.20\n2 Ours Simple Fusion, Single Loss 71.82\n3 Ours Simple Fusion, FO 73.10\n4 MMFT w/ BERT Encoder freezed 57.94\n5 Ours Simple Fusion, FO, +Img 71.82\n6 MMFT-BERT L=2, H=12 72.61\n7 MMFT-BERT L=2, H=12 w/ skip 72.62\n8 MMFT-BERT L=1, H=1 72.66\n9 MMFT-BERT L=1, H=12 73.55\nTable 5: Ablations over the design choices for the pro-\nposed architecture. L = no. of encoder layers in MMFT\nmodule, H = no. of heads in MMFT module, +Img =\nResnet101 features pooled over video frames. Rows 3-\n9 are trained with our full objective (FO). All models\nare trained for Q+V+S with timestamp annotations.\nalso train our model on full length visual features\nand subtitles. Our method with simple fusion and\nMMFT on Q+V input outperforms Two-stream\n(Lei et al., 2018) by absolute 6.49% and 5.59% with\nsimple fusion and MMFT respectively. We truncate\nthe input sequence if it exceeds max seq len. Sub-\ntitles without timestamps are very long sequences\n( 49% of subtitles are longer than length 256),\nhence QA pair might be truncated. Thus, we re-\narrange our input without timestamps as follows:\n“Q [SEP] Aj . V” and “Q [SEP] Aj . S” for V-BERT\nand S-BERT respectively. Models with Q+S in-\nput are trained with max seq len=512 and Q+V+S\nmodels are trained with max seq len=256 due to\nGPU memory constraints. For Q+S and Q+V+S,\nwe observe 69.92% and 65.55% with simple fu-\nsion, using MMFT produces 69.98% and 66.10%\nval. accuracy respectively.\nResults on test set. TVQA test-public set does\nnot provide answer labels and requires submission\nof the model’s predictions to the evaluation server.\nOnly limited attempts are permitted. The server’s\nevaluation results are shown in table 2. MMFT im-\nproves results by (↑6.39%) on Q+V . For Q+V+S,\nW ACV20 reported 73.57% accuracy with a differ-\nent input arrangement than MMFT. When com-\npared with the model with the same input, MMFT\nperforms slightly better (↑0.17%). Due to limited\nchances for submission to the test server for evalua-\ntion, the reported accuracy for Q+V+S is from one\nof our earlier models, not from our best model.\n5.3.1 Model Analysis\nPerformance analysis on TVQA-Visual. To\nstudy the models, we evaluate Two-stream (Lei\net al., 2018), W ACV20 (Yang et al., 2020) and our\nmethod on both TVQA-Visual (full) and TVQA-\nVisual (clean). See table 4 for full results.\nTVQA-Visual (full): Our method outperforms\nTwo-stream (Lei et al., 2018) by 10.08% but drops\nby 0.43% compared to WACV20 (Yang et al.,\n2020). TVQA-Visual (full) has approximately 59%\nof the questions with missing visual concept or\nrequire extra visual knowledge. All three models\nincluding ours were trained on visual concepts. In-\nadequate input, therefore, makes it difﬁcult for the\nmodels to attend the missing information.\nTVQA-Visual (clean): We observe (↑11.46%)\nand (↑4.17%) improvement for clean set com-\npared to Two-stream and W ACV20. TVQA-Visual\n(clean) has relevant visual concepts or related con-\ncepts to the answer present in the input. Yet, it is\nchallenging for existing methods (including ours)\nto perform well. Although our model observes sig-\nniﬁcant improvement (↑4-11%) over baselines for\nthis experiment, the take away message is that it\nis not enough. This subset of TVQA, therefore,\nserves as a good diagnostic benchmark to study\nthe progress of exploiting visual features for mul-\ntimodal QA tasks. Fig. 4 visualizes the test per-\nformance of each stream on TVQA-Visual clean\nduring training our Q+V model on TVQA.\nPerformance analysis w.r.t multimodal atten-\ntion. We study the behavior of the MMFT module\nfor aggregating multimodal source inputs (Q, V ,\nand S), we take our best model trained on all three\nsources, and evaluate it on questions which need\nknowledge about either the visual world, dialogue\nor both. We then visualize the average attention\nscore map over all heads inside MMFT module\n(H=12) for each candidate answer, see ﬁg. 5. Top\n2 rows show attention scores computed among all\n3 input sources and the [FUSE] vector for visual\nquestions. Since, [FUSE] is the aggregated output\nover all input modalities. For instance, visual part\nshould contribute more if the question is about the\nvisual world. We can see the attention map for the\ncorrect answer has high attention scores between V\nand [FUSE] vector. The incorrect answers attend\nto the wrong sources (either Q or S). Similar is\nthe behavior for rows 3-5, where the question is\nabout subtitles, and the correct answer gives most\nweight to the subtitles compared to the incorrect\nanswers. Heatmaps for incorrect answers are either\nfocused more on a wrong single input source or the\ncombination of them.\nPositional Encodings for V-BERT.Positional en-\ncoding is done internally in BERT. When ﬁnetuned,\nfor V-BERT, the positional encoding has no effect.\nThis has been veriﬁed by training our Q+V model\nwith simple fusion (Ours-SF), where the input to\nV-BERT is a shufﬂed sequence of objects; no dras-\ntic difference was observed (shufﬂed: 50.32% vs.\nnot shufﬂed: 50.65%).\n5.3.2 Ablations\nAll ablations were done with Q+V+S input. See\ntable 5 for complete results.\nSimple fusion vs. MMFT Though using simple\nfusion for combining multimodal inputs is very ef-\nfective and already outperforms all of the baselines,\nit lacks the basic functionality of explainability. Us-\ning MMFT instead, not only gives us an improve-\nment (↑0.71% for Q+V and ↑0.39% for Q+V+S)\nover simple fusion, but is also more explainable.\nSingle loss vs. multiple losses. A simple design\nchoice could be to use just joint loss instead of\nmultiple loss terms. However, through our exper-\niments, we ﬁnd that using single joint loss term\nhurts the performance (71.82%). Optimizing each\nBERT along with optimizing the full model jointly\ngives us best results (73.10%) even without using\nMMFT.\nSingle head vs. multi-head MMFT.In an attempt\nto know if simplicity (single head) has an advan-\ntage over using multi-head attention, we trained\nMMFT-BERT with H=1. Using single head at-\ntention for fusion consistently performed lower\nthan using multiple heads (72.66% vs. 73.55%)\n(we set H=12). Our hypothesis is that since pre-\ntrained BERTs have 12 heads, attention within each\nsource BERT was local ( dmodel /H). Using single\nhead attention over features which were attended\nin a multi-head fashion may be hurting the features\ncoming out of each modality encoder. Thus, it\nmakes more sense to keep the attention local inside\nMMFT if the input encoders use local attention to\nattend to input sequences.\nSingle layer fusion vs. stacked fusion. Another\ndesign parameter is #encoder layers (L) in MMFT.\nWe trained our full model with three settings: a)\nsingle encoder layer L=1, b) stacked encoder layer\nL=2, and c) stacked encoder with skip connection.\na) gives best results (73.55%), whereas both b)\nand c) fusion hurts (72.61% and 72.62%). Note\nthat all variants of our models are slightly better in\nperformance than our baseline methods.\nResnet features vs. visual concepts. To study\nif incorporating additional visual context is ad-\nvantageous, we experimented with Resnet101 fea-\ntures for visual information. We used Resnet101\nfeatures pooled over time along with visual con-\ncepts. We used question-words-to-region attention\nfor aggregating visual features; adding this aggre-\ngated visual feature to Ours-SF hurts the perfor-\nmance (71.82%); using object labels was consis-\ntently more useful than visual features in various\nother experimental settings.\n6 Conclusion\nOur method for VQA uses multiple BERT encod-\nings to process each input type separately with a\nnovel fusion mechanism to merge them together.\nWe repurpose transformers for using attention be-\ntween different input sources and aggregating the\ninformation relevant to the question being asked.\nOur method outperforms state-of-the-art methods\nby an absolute ∼2.41% on Q+V and ∼1.1% on\nQ+V+S on TVQA validation set. Our proposed\nfusion lays the groundwork to rethink transform-\ners for fusion of multimodal data in the feature\ndimension.\nAcknowledgments\nWe thank the reviewers for their helpful feedback.\nThis research is supported by the Army Research\nOfﬁce under Grant Number W911NF-19-1-0356.\nThe views and conclusions contained in this doc-\nument are those of the authors and should not be\ninterpreted as representing the ofﬁcial policies, ei-\nther expressed or implied, of the Army Research\nOfﬁce or the U.S. Government. The U.S. Gov-\nernment is authorized to reproduce and distribute\nreprints for Government purposes notwithstanding\nany copyright notation herein.\nReferences\nChris Alberti, Jeffrey Ling, Michael Collins, and\nDavid Reitter. 2019. Fusion of detected objects in\ntext for visual question answering. arXiv preprint\narXiv:1908.05054.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 6077–6086.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. 2015. Vqa: Visual question an-\nswering. In Proceedings of the IEEE international\nconference on computer vision, pages 2425–2433.\nWei-Lun Chao, Hexiang Hu, and F. Sha. 2018. Being\nnegative but constructively: Lessons learnt from cre-\nating better visual question answering datasets. In\nNAACL-HLT.\nKan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan\nGao, Wei Xu, and Ram Nevatia. 2015. Abc-\ncnn: An attention based convolutional neural net-\nwork for visual question answering. arXiv preprint\narXiv:1511.05960.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making the\nV in VQA matter: Elevating the role of image under-\nstanding in Visual Question Answering. In Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nDrew A Hudson and Christopher D Manning. 2018.\nCompositional attention networks for machine rea-\nsoning. International Conference on Learning Rep-\nresentations (ICLR).\nDrew A Hudson and Christopher D Manning. 2019.\nGqa: A new dataset for real-world visual reason-\ning and compositional question answering. Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nIlija Ilievski, Shuicheng Yan, and Jiashi Feng. 2016. A\nfocused dynamic attention model for visual question\nanswering. arXiv preprint arXiv:1604.01485.\nA. Jabri, Armand Joulin, and L. V . D. Maaten. 2016.\nRevisiting visual question answering baselines. In\nECCV.\nYunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim,\nand Gunhee Kim. 2017. Tgif-qa: Toward spatio-\ntemporal reasoning in visual question answering. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 2758–2766.\nJustin Johnson, Bharath Hariharan, Laurens van der\nMaaten, Li Fei-Fei, C. Lawrence Zitnick, and\nRoss B. Girshick. 2016. CLEVR: A diagnostic\ndataset for compositional language and elementary\nvisual reasoning. CoRR, abs/1612.06890.\nJunyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin\nKim, and Chang D Yoo. 2019a. Gaining ex-\ntra supervision via multi-task learning for multi-\nmodal video question answering. arXiv preprint\narXiv:1905.13540.\nJunyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin\nKim, and Chang D Yoo. 2019b. Progressive atten-\ntion memory network for movie story question an-\nswering. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n8337–8346.\nKyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and\nByoung-Tak Zhang. 2017. Deepstory: Video story\nqa by deep embedded memory networks. arXiv\npreprint arXiv:1707.00836.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,\nJames Bradbury, Ishaan Gulrajani, Victor Zhong,\nRomain Paulus, and Richard Socher. 2016. Ask me\nanything: Dynamic memory networks for natural\nlanguage processing. In International conference on\nmachine learning, pages 1378–1387.\nJie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.\n2018. Tvqa: Localized, compositional video ques-\ntion answering. In EMNLP.\nJie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal.\n2019. Tvqa+: Spatio-temporal grounding for video\nquestion answering. In Tech Report, arXiv.\nGen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and\nMing Zhou. 2019a. Unicoder-vl: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. arXiv preprint arXiv:1908.06066.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019b. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. arXiv preprint arXiv:1908.03557.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems, pages 13–23.\nTegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron\nCourville, and Christopher Pal. 2017. A dataset and\nexploration of models for understanding video data\nthrough ﬁll-in-the-blank question-answering. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 6884–6893.\nAmir Mazaheri and Mubarak Shah. 2018. Visual text\ncorrection. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), pages 155–171.\nAmir Mazaheri, Dong Zhang, and Mubarak Shah. 2017.\nVideo ﬁll in the blank using lr/rl lstms with spatial-\ntemporal attentions. In The IEEE International Con-\nference on Computer Vision (ICCV).\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013a. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013b. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nJonghwan Mun, Paul Hongsuck Seo, Ilchae Jung, and\nBohyung Han. 2017. Marioqa: Answering ques-\ntions by watching gameplay videos. In Proceedings\nof the IEEE International Conference on Computer\nVision, pages 2867–2875.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d Alch´e-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8026–8037. Curran Asso-\nciates, Inc.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster r-cnn: Towards real-time ob-\nject detection with region proposal networks. In\nAdvances in neural information processing systems,\npages 91–99.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2019. Vl-bert: Pre-\ntraining of generic visual-linguistic representations.\narXiv preprint arXiv:1908.08530.\nAlane Suhr, Mike Lewis, James Yeh, and Yoav Artzi.\n2017. A corpus of natural language for visual rea-\nsoning. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 217–223.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.\n2015. End-to-end memory networks. In Advances\nin neural information processing systems, pages\n2440–2448.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers. arXiv preprint arXiv:1908.07490.\nMakarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,\nAntonio Torralba, Raquel Urtasun, and Sanja Fidler.\n2016. Movieqa: Understanding stories in movies\nthrough question-answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 4631–4640.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJason Weston, Sumit Chopra, and Antoine Bor-\ndes. 2014. Memory networks. arXiv preprint\narXiv:1410.3916.\nCaiming Xiong, Stephen Merity, and Richard Socher.\n2016. Dynamic memory networks for visual and\ntextual question answering. In International confer-\nence on machine learning, pages 2397–2406.\nHuijuan Xu and Kate Saenko. 2016. Ask, attend and\nanswer: Exploring question-guided spatial attention\nfor visual question answering. In European Confer-\nence on Computer Vision, pages 451–466. Springer.\nGuangyu Robert Yang, Igor Ganichev, Xiao-Jing Wang,\nJonathon Shlens, and David Sussillo. 2018. A\ndataset and architecture for visual reasoning with a\nworking memory. In European Conference on Com-\nputer Vision, pages 729–745. Springer.\nZekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani,\nYuta Nakashima, and Haruo Takemura. 2020. Bert\nrepresentations for video question answering. In\nThe IEEE Winter Conference on Applications of\nComputer Vision (WACV).\nKexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli,\nJiajun Wu, Antonio Torralba, and Joshua B Tenen-\nbaum. 2019. Clevrer: Collision events for video\nrepresentation and reasoning. arXiv preprint\narXiv:1910.01442.\nDongfei Yu, Jianlong Fu, Tao Mei, and Yong Rui.\n2017a. Multi-level attention networks for visual\nquestion answering. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recog-\nnition, pages 4709–4717.\nLicheng Yu, Eunbyung Park, Alexander C. Berg, and\nTamara L. Berg. 2015. Visual madlibs: Fill in the\nblank description generation and question answer-\ning. In The IEEE International Conference on Com-\nputer Vision (ICCV).\nZhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao.\n2017b. Multi-modal factorized bilinear pooling with\nco-attention learning for visual question answering.\nIn Proceedings of the IEEE international conference\non computer vision, pages 1821–1830.\nAmir Zadeh, Michael Chan, Paul Pu Liang, Edmund\nTong, and Louis-Philippe Morency. 2019. Social-iq:\nA question answering benchmark for artiﬁcial social\nintelligence. In The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR).\nPeng Zhang, Yash Goyal, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2016. Yin and Yang:\nBalancing and answering binary visual questions. In\nConference on Computer Vision and Pattern Recog-\nnition (CVPR).\nLinchao Zhu, Zhongwen Xu, Yi Yang, and Alexan-\nder G Hauptmann. 2017. Uncovering the temporal\ncontext for video question answering. International\nJournal of Computer Vision, 124(3):409–421.\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-\nFei. 2016. Visual7w: Grounded question answering\nin images. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages\n4995–5004.\nA Supplementary Material\nA.1 Improved comprehension of submitted\npaper\nCertain parts of the submitted paper will be more\nclear to the reader if s/he is familiar with the con-\ncepts explained in (Vaswani et al., 2017) and (De-\nvlin et al., 2018). For instance, the attention mech-\nanism illustrated in the submitted paper’s ﬁgure\n1 needs understanding of transformers (Vaswani\net al., 2017).\nA.2 TVQA-Visual\nSee ﬁgure 7 for some statistics about TVQA-Visual\nset. Almost 59% of the questions have not enough\ninput available (plot shows results for 200 ques-\ntions, rest of the 35 questions are ”who” questions\nand need character recognition). We will make this\nlist of questions available to the community for\nfurther research.\nA.3 Experiments and Results\nEvaluation Metric. Multiple choice question an-\nswering accuracy is used to evaluate each model in\nthis work.\nFurther discussion about results without times-\ntamp annotations. For experiments without times-\ntamp annotations, for Q+V and Q+S, the only com-\npetitor whose results are available is Two-stream\n(Lei et al., 2018); in these categories, MMFT is\nmore than 6-7% better than the Two-stream, where,\nfor Q+S, we train MMFT with a sequence length\nof 512. For Q+V+S, MMFT achieves 66.10% with\nmax seq len=256. STAGE reports 2.46% higher\naccuracy. We had a GPU memory limitation and\ncould only train our model with input size of 256.\nHad we had access to at least 4 GPUs (24GB of\nmemory), we would have been able to train our full\nmodel with input size of 512, which would have\npresumably given us a similar boost we witnessed\nfor Q+S without timestamps (Q+S is ∼3% bet-\nter than Q+V+S). Therefore, we believe our model\nFigure 6: Validation accuracy curves for Q-BERT, V-\nBERT and full model when trained on Q+V input. Al-\nthough Q-BERT performs lower than V-BERT as ex-\npected, it helps when Q-BERT is kept as a separate\nstream. During initial training, Q-BERT trains quickly\nthan V-BERT. After ﬁrst epoch, V-BERT starts outper-\nforming Q-BERT as the model learns to leverage visual\nstream to answer the questions.\nwould perform better when provided with increased\ninput length.\nPerformance analysis on multimodal questions.\nFor true multimodal questions which cannot be an-\nswered without looking at both video and subtitles,\nthe aggregated feature should rely on both modali-\nties. The last row in ﬁgure 5 of the submitted paper\nis an attempt to study such questions. However, we\nobserved, that many of these type of such questions\nwhich apparently require both modalities, can in\npractice be answered by just one of them. Although\nthe question in last row is intended towards both\nvideo and dialogue (subtitles), the actual nature of\nthe question is visual. We don’t need to know what\nsomeone is saying to observe how they are dressed.\nTo the best of our knowledge, no such constraints\nwere imposed while collecting the original TVQA\ndataset. For instance, a true multimodal question\nabout a speciﬁc appearance would be asked if the\nperson appears multiple times with varying appear-\nance in a video. Referring to dialogue in that case\nto localize the visual input is a true multimodal\nquestion. For example, in row 5, the question is\n” What color is the shirt Marvin is wearing when\nhe say’s I could’nt see.?”, with the corresponding\nsubtitles, MMFT chooses to ignore subtitles yet\ngiving the correct answer. For the last row in ﬁgure\n5, examine the attention maps to see how MMFT\ngives more attention to V source than subtitles S\nfor the correct answer (which is in the last column).\nFusion Techniques: We also tried several other fu-\nsion methods including: a) gated fusion where each\nFigure 7: Few statistics for TVQA-Visual (full) set. Left) distribution of questions w.r.t question family, Center)\ndistribution of questions w.r.t TV show, Right) Distribution of questions w.r.t reason of failure.\nFigure 8: Qualitative results from validation set. Success and failure cases on visual and multimodal questions.\nBold text shows correct answer, prediction of each model is in parenthesis. Incorrect prediction is in red font.\nsource vector is gated w.r.t. every other source vec-\ntors before fusing them together. We merge the re-\nsulting gated source features with i) concatenation\nfollowed by a linear layer, ii) taking the product of\nthe gated source vectors, iii) concatenation of the\ngated fusion feature and the simple fusion feature.\nAll of them result in suboptimal performance than\nour simple fusion method with a performance drop\nof 1-2%.\nA.3.1 Qualitative Results\nSome of the qualitative results are shown in ﬁgure\n8 including both success and failure cases of our\nmethod and the baselines for Q+V+S input.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8254050016403198
    },
    {
      "name": "Computer science",
      "score": 0.7589067220687866
    },
    {
      "name": "Modalities",
      "score": 0.6428512334823608
    },
    {
      "name": "Question answering",
      "score": 0.6032132506370544
    },
    {
      "name": "Fuse (electrical)",
      "score": 0.5893875360488892
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5759751796722412
    },
    {
      "name": "Fusion",
      "score": 0.484464168548584
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.4403047561645508
    },
    {
      "name": "Natural language processing",
      "score": 0.4229852259159088
    },
    {
      "name": "Machine learning",
      "score": 0.3223649263381958
    },
    {
      "name": "Linguistics",
      "score": 0.0772089958190918
    },
    {
      "name": "Engineering",
      "score": 0.0625867247581482
    },
    {
      "name": "Voltage",
      "score": 0.053868889808654785
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ]
}