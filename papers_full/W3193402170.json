{
    "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
    "url": "https://openalex.org/W3193402170",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1510628783",
            "name": "Wang, Zirui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2552959557",
            "name": "Yu, Jiahui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3115150678",
            "name": "Yu, Adams Wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2352997497",
            "name": "Dai, Zihang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221698111",
            "name": "Tsvetkov, Yulia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1997485551",
            "name": "Cao Yuan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3126337491",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2986670728",
        "https://openalex.org/W3173909648",
        "https://openalex.org/W3129576130",
        "https://openalex.org/W3104279398",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W3126464137",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2912371042",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W3190043560",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W3173220247",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3171087525",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2928941594",
        "https://openalex.org/W3034655362",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W1889081078",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W3035688398",
        "https://openalex.org/W3135099046",
        "https://openalex.org/W2509282593",
        "https://openalex.org/W2560730294",
        "https://openalex.org/W3176824248",
        "https://openalex.org/W2606722458",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2963084599",
        "https://openalex.org/W3177224328",
        "https://openalex.org/W3159481202",
        "https://openalex.org/W3152532671",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3126792443",
        "https://openalex.org/W3118159310",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W3100859887",
        "https://openalex.org/W3184784418",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3034275286",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W2345720230"
    ],
    "abstract": "With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.",
    "full_text": "Published as a conference paper at ICLR 2022\nSIMVLM: S IMPLE VISUAL LANGUAGE MODEL PRE-\nTRAINING WITH WEAK SUPERVISION\nZirui Wang1,2∗, Jiahui Yu2, Adams Wei Yu2, Zihang Dai2, Yulia Tsvetkov3, Yuan Cao2\n1Carnegie Mellon University\n{ziruiw}@cs.cmu.edu\n2Google Research, Brain Team\n{jiahuiyu,adamsyuwei,zihangd,yuancao}@google.com\n3University of Washington\n{yuliats}@cs.washington.edu\nABSTRACT\nWith recent progress in joint modeling of visual and textual representations,\nVision-Language Pretraining (VLP) has achieved impressive performance on\nmany multimodal downstream tasks. However, the requirement for expensive\nannotations including clean image captions and regional labels limits the scal-\nability of existing approaches, and complicates the pretraining procedure with\nthe introduction of multiple dataset-speciﬁc objectives. In this work, we relax\nthese constraints and present a minimalist pretraining framework, named Simple\nVisual Language Model (SimVLM). Unlike prior work, SimVLM reduces the\ntraining complexity by exploiting large-scale weak supervision, and is trained\nend-to-end with a single preﬁx language modeling objective. Without utilizing\nextra data or task-speciﬁc customization, the resulting model signiﬁcantly outper-\nforms previous pretraining methods and achieves new state-of-the-art results on a\nwide range of discriminative and generative vision-language benchmarks, includ-\ning VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% ac-\ncuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore,\nwe demonstrate that SimVLM acquires strong generalization and transfer ability,\nenabling zero-shot behavior including open-ended visual question answering and\ncross-modality transfer.\n1 I NTRODUCTION\nSelf-supervised textual representation learning (Devlin et al., 2018; Radford et al., 2018; 2019; Liu\net al., 2019; Yang et al., 2019; Raffel et al., 2019; Brown et al., 2020) based on Transformers\n(Vaswani et al., 2017) has pushed the state of the art on a wide range of natural language pro-\ncessing (NLP) tasks (Rajpurkar et al., 2016; Wang et al., 2018; Sarlin et al., 2020). One successful\napproach is to ﬁrst pretrain the model (e.g. BERT) on large-scale unlabled text corpora using masked\nlanguage modeling (MLM) objective (Devlin et al., 2018), followed by ﬁnetuning on downstream\ntasks. While this pretraining-ﬁnetuning paradigm has been widely adopted, recent work on autore-\ngressive language models (LM) (Radford et al., 2019; Brown et al., 2020) such as GPT-3 has shown\nstrong performance without ﬁnetuning by utilizing few-shot prompts (Liu et al., 2021), suggesting\nthe text guided zero-shot generalization is a promising alternative.\nMotivated by the success of textual representation pretraining, various efforts have been made to\nbuild the multi-modal (visual and textual) counterpart. A line of work (Tan & Bansal, 2019; Lu\net al., 2019; Li et al., 2019; Chen et al., 2020b; Li et al., 2020; Su et al., 2020; Zhang et al., 2021)\nhas explored vision-language pretraining (VLP) that learns a joint representation of both modali-\nties to be ﬁnetuned on vision-language (VL) benchmarks, such as visual question answering (VQA)\n(Goyal et al., 2017). In order to capture the alignment between images and text, previous methods\nhave extensively exploited two types of human-labeled datasets from multiple sources, which typi-\ncally consist of the following steps. Firstly, object detection datasets are used to train a supervised\n∗This work was conducted at Google.\n1\narXiv:2108.10904v3  [cs.CV]  15 May 2022\nPublished as a conference paper at ICLR 2022\nobject detector (OD) which allows further extracting region-of-interest (ROI) features from images.\nNext, datasets with aligned image-text pairs are used for MLM pretraining of a fusion model that\nusually takes as input the concatenation of the extracted ROI features and the paired text. In addi-\ntion, due to the limited scale of human annotated data, various task-speciﬁc auxiliary losses have\nbeen introduced in order to improve performance. These design choices complicate the pretrain-\ning protocol of VLP, creating a bottleneck for further quality improvement. What is more, such\npretraining-ﬁnetuning based approaches usually lack the zero-shot capability, just like their lan-\nguage counterparts. In comparison, another line of work (Radford et al., 2021; Ramesh et al., 2021;\nJia et al., 2021) utilizes weakly labeled/aligned data crawled from the web to perform pretraining,\nachieving good performance and certain zero-shot learning capability on image classiﬁcation and\nimage-text retrieval. Nonetheless, these methods mainly focus on speciﬁc tasks of consideration\nand thus may not serve as a generic pretraining-ﬁnetuning representation for VL benchmarks.\nIn light of these disadvantages of the existing techniques, we are interested in building a VLP model\nthat: (1) can be seamlessly plugged into the pretraining-ﬁnetuning paradigm and achieve competitive\nperformance on standard VL benchmarks; (2) does not require a complicated pretraining protocol\nas in previous methods; and (3) has the potential towards text guided zero-shot generalization in\ncross-modal settings. To this end, we propose SimVLM, standing for Simple Visual Language\nModel, which signiﬁcantly simpliﬁes VLP by solely exploiting language modeling objectives on\nweakly aligned image-text pairs (Jia et al., 2021). In a nutshell, SimVLM consists of the following\ncomponents:\n• Objective. It is trained end-to-end from scratch with a single objective of Preﬁx Language\nModeling (PreﬁxLM), which can not only naturally perform text generation as GPT-3, but\nalso process contextual information in a bidirectional manner as BERT does.\n• Architecture. The framework employs ViT/CoAtNet (Dosovitskiy et al., 2021; Dai et al.,\n2021) and directly takes raw images as inputs. These models can also ﬁt the large-scale\ndata and are readily compatible with the PreﬁxLM objective.\n• Data. These setups relieve the requirement for object detection and allow the model to\nutilize the large-scale weakly labeled dataset, which has better potential towards zero-shot\ngeneralization.\nNot only is SimVLM simpler, requiring neither object detection pretraining nor auxiliary losses,\nbut it also obtains better performance than previous work. Empirically, SimVLM consistently out-\nperforms existing VLP models and achieves new state-of-the-art results on 6 VL benchmarks with-\nout additional data nor task-speciﬁc customization. Besides, it acquires stronger generalization in\nvisual-language understanding that empowers zero-shot image captioning and open-ended VQA. In\nparticular, SimVLM learns uniﬁed multimodal representation that enables zero-shot cross-modality\ntransfer, where the model is ﬁnetuned on text-only data and directly evaluated on image-and-text\ntest examples without further training. Our results suggest that generative VLP can not only match\nexisting MLM-based methods on VL tasks but also demonstrate promising zero-shot potential.\n2 R ELATED WORK\nRecent years have seen a rapid progress made in vision-language pretraining (Uppal et al., 2020;\nHan et al., 2021; Khan et al., 2021). While a variety of approaches have been proposed, a large\nportion of them require object detection for image region feature regression or tagging as part of\nthe pre-training objectives (Tan & Bansal, 2019; Su et al., 2020; Li et al., 2019; Chen et al., 2020b;\nGan et al., 2020; Li et al., 2020; Yu et al., 2021; Li et al., 2021; Zhang et al., 2021; Hu et al., 2021;\nCho et al., 2021). These methods rely on a strong object detection model like Fast(er) R-CNN (Ren\net al., 2015), which is often trained on human annotated data sets like Visual Genome (Krishna et al.,\n2016). Using such labeled training data as a prerequisite increases the cost of building the training\npipeline, and makes the approach less scalable. Some recent efforts have also explored VLP without\nobject detection module (Xu et al., 2021; Kim et al., 2021; Huang et al., 2021), but they only use\nclean pretraining data with small scales and thus their zero-shot capability is limited.\nOn the other hand, multiple cross-modality loss functions have been proposed as part of the training\nobjectives, for example image-text matching (Tan & Bansal, 2019; Lu et al., 2019; Xu et al., 2021),\nmasked region classiﬁcation/feature regression (Tan & Bansal, 2019; Chen et al., 2020b), object\n2\nPublished as a conference paper at ICLR 2022\nFigure 1: Illustration of the SimVLM model. This shows an example of training with PreﬁxLM\nof an image-text pair. For text-only corpora, it is straightforward to remove the image patches and\nutilize textual tokens only.\nattribute prediction (Xu et al., 2021), contrastive loss (Li et al., 2020; 2021), word-region alignment\n(Chen et al., 2020b) word-patch alignment (Kim et al., 2021). They are often mixed with other\nobjectives including image caption generation and masked language modeling to form compound\npre-training losses. This creates the challenge of balancing among different losses and datasets, and\nthus complicates the optimization procedure.\nOur work by contrast, follows a minimalist approach that takes raw image inputs and makes use\nof only the language modeling loss, without resorting to auxiliary models like faster R-CNN for\nimage region detection. Motivated by recent works (Radford et al., 2021; Ramesh et al., 2021; Jia\net al., 2021; Tsimpoukelli et al., 2021) that illustrate zero-shot learning in certain image-text tasks,\nwe train our model using large-scale weakly labeled data only. While concurrent work (Shen et al.,\n2021) has explored building on top of models pretrained with such dataset, we focus on pretraining\nfrom scratch to explore the limit of generative VLP.\n3 S IMVLM\n3.1 B ACKGROUND\nThe bidirectional Masked Language Modeling (MLM) has been one of the most popular self-\nsupervised training objectives for textual representation learning. As demonstrated by BERT (Devlin\net al., 2018), it is based on the idea of denoising autoencoder such that the model is trained to recover\nthe corrupted tokens in a document. Speciﬁcally, given a text sequence x, a subset of tokens xm are\nrandomly sampled and a corrupted sequence x\\m is constructed by replacing tokens in xm with\na special [MASK] token. The training objective is to reconstruct xm from the context x\\m by\nminimizing the negative log-likelihood:\nLMLM(θ) =−Ex∼D\n[\nlog Pθ(xm|x\\m)\n]\n, (1)\nwhere θis the trainable parameters of the model and Dis the pretraining data. This approach learns\ncontextualized representations that can be further ﬁnetuned for downstream tasks. The MLM-style\npretraining has been widely adopted in previous VLP models, whereby the input is an image-text\npair and the model needs to predict masked tokens by leveraging image ROI features.\nAlternatively, the unidirectional Language Modeling (LM) trains the model to directly maximize\nthe likelihood of the sequence x under the forward autoregressive factorization:\nLLM(θ) =−Ex∼D[log Pθ(x)] =−Ex∼D\n[ T∑\nt=1\nlog Pθ(xt|x<t)\n]\n. (2)\nCompared with MLM, the LM pretraining has also been shown to be highly effective for multiple\nNLP tasks (Radford et al., 2018). More importantly, it facilitates the model with strong generation\n3\nPublished as a conference paper at ICLR 2022\ncapability that enables text induced zero-shot generalization without ﬁnetuning (Brown et al., 2020).\nWhile MLM has become the de facto approach in VLP models reviewed above, the generative LM\nhas been understudied.\n3.2 P ROPOSED OBJECTIVE : P REFIX LANGUAGE MODELING\nMotivated by the zero-shot capability introduced by pre-training with LM loss, we propose to pretain\nvision-language representation using thePreﬁx Language Modeling (PreﬁxLM). PreﬁxLM differs\nfrom the standard LM such that it enables bi-directional attention on the preﬁx sequence (e.g. x<Tp\nin Eq. (3)), and only conducts autoregressive factorization on the remaining tokens (e.g.x≥Tp in Eq.\n(3)). During pretraining, a preﬁx sequence of tokens of (a randomly selected) length Tp is truncated\nfrom input sequence and the training objective becomes:\nLPreﬁxLM (θ) =−Ex∼D\n[\nlog Pθ(x≥Tp |x<Tp )\n]\n= −Ex∼D\n\n\nT∑\nt=Tp\nlog Pθ(xt|x[Tp,t],x<Tp )\n\n. (3)\nIntuitively, images can be considered as preﬁx for their textual descriptions as they often appear\nbefore text in a web document. Therefore, for a given image-text pair, we prepend image feature\nsequence of length Ti to the text sequence, and enforce the model to sample a preﬁx of length\nTp ≥Ti to calculate LM loss on text data only (an example is shown in Figure 1). Compared to\nprior MLM style VLP methods, our PreﬁxLM model under the sequence-to-sequence framework\nnot only enjoys the bidirectional contextualized representation as in MLM, but also can perform text\ngeneration similar to LM.\n3.3 A RCHITECTURE\nWe adopt Transformer as the backbone of our model due to its success for both language and vision\ntasks (Devlin et al., 2018; Dosovitskiy et al., 2021). Differently from standard LM, PreﬁxLM enables\nbidirectional attention within the preﬁx sequence, and thus it is applicable for both decoder-only\nand encoder-decoder sequence-to-sequence language models. In our preliminary experiments, we\nfound that the inductive bias introduced by encoder-decoder model which decouples encoding from\ngeneration is conducive to the improvement of downstream task.\nAn overview of our model architecture is depicted in Figure 1. For the visual modality, inspired by\nViT (Dosovitskiy et al., 2021) and CoAtNet (Dai et al., 2021), our model receives the raw image\nx ∈RH×W×C and maps it into ﬂattened 1D sequence of patches xp ∈RTi×D as input for the\ntransformer, where Dis the ﬁxed hidden size of the transformer layers andTi = HW\nP2 is the length of\nthe image tokens for a given patch sizeP. Following Dai et al. (2021), we use a convolution (Conv)\nstage consist of the ﬁrst three blocks of ResNet (He et al., 2016) to extract contextualized patches,\nwhich we ﬁnd advantageous over the naive linear projection (equivalent to 1×1 Conv layer) used in\nViT, consistent with the observation from (Xiao et al., 2021). For the textual modality, we follow the\nstandard practice to tokenize the input sentence into sub-word tokens (Kudo & Richardson, 2018),\nand the embeddings are learned for a ﬁxed vocabulary. To retain positional information, we add two\ntrainable 1D positional embeddings for image and text inputs separately, and we additionally add 2D\nrelative attention for the image patches within transformer layers (Dai et al., 2021). Notice that we\ndo not add extra modality type embeddings for which we found no improvement in our experiment.\nWe study the effects of various components of the model in Section 4.4.\n3.4 D ATASETS\nSince our approach does not rely on an object detection module and only operates with raw image\npatch inputs, we pretrain all model parameters from scratch using large-scale noisy image-text data,\nwhich has better potential for zero-shot generalization. Speciﬁcally, we use the image and alt-text\npairs introduced in Jia et al. (2021), which are crawled from the web with minimal post-processing.\nOn the other hand, our formulation of PreﬁxLM is modality-agnostic and thus we can additionally\ninclude text-only corpora to compensate for noisy text supervision in the alt-text data. As shown\nlater in our experiments, this uniﬁed PreﬁxLM formulation reduces the modality discrepancy and\nimproves the model quality.\n4\nPublished as a conference paper at ICLR 2022\nVQA NLVR2 SNLI-VE CoCo Caption NoCaps Multi30ktest-dev test-stddev test-P dev test B@4 M C S C S En-De\nBase-sized Models\nLXMERT 72.42 72.54 74.90 74.50 - - - - - - - - -VL-T5 - 70.30 74.6 73.6 - - - - 116.5 - - - 45.5SOHO 73.25 73.47 76.37 77.3285.00 84.95 - - - - - - -SimVLMbase 77.87 78.14 81.72 81.7784.20 84.1539.0 32.9 134.8 24.0 94.8 13.1 46.6\nLarge-sized Models\nUNITER 73.82 74.02 79.12 79.9879.39 79.38 - - - - - - -OSCAR 73.61 73.82 79.12 80.37 - - 41.7 30.6 140.0 24.5 80.9 11.3 -Villa 74.69 74.87 79.76 81.4780.18 80.02 - - - - - - -UNIMO 75.06 75.27 - - 81.11 80.6339.6 - 127.7 - - - -VinVL 76.56 76.60 82.67 83.98 - - 41.0 31.1 140.9 25.2 92.5 13.1 -SimVLMlarge 79.32 79.56 84.13 84.8485.68 85.6240.3 33.4 142.6 24.7 108.5 14.2 47.5\nHuge-sized Models\nSimVLMhuge 80.03 80.34 84.53 85.1586.21 86.3240.6 33.7 143.3 25.4110.3 14.5 47.6\nTable 1: Single model results for vision-language pretraining methods on popular VL banchmarks.\nWe report vqa-score for VQA, accuracy for NLVR2 and SNLI-VE, BLEU@4 for Multi30k and\nvarious metrics for image captioning (B@4: BLEU@4, M: METEOR, C: CIDEr, S: SPICE).\nCompared to prior VLP methods consisting of two pretraining stages and multiple auxiliary ob-\njectives, our model only requires one-pass pretraining using a single language modeling loss in an\nend-to-end manner, hence the name Simple Visual Language Model (SimVLM).\n4 E XPERIMENTS\nWe conduct systematic experiments on a diversiﬁed set of visual-linguistic benchmarks, including\nvisual question answering, image captioning, visual reasoning, visual entailment, and multimodal\ntranslation. We not only examine our model as a general-purpose VL representation learning in the\npretraining-ﬁnetuning paradigm, but also study its zero-shot generalization towards open-ended VL\nunderstanding.\n4.1 S ETUP\nOur models are implemented with the Lingvo framework (Shen et al., 2019). We follow the setup\nin ViT (Dosovitskiy et al., 2021) to explore 3 variants of SimVLM, namely “Base”, “Large”, and\n“Huge”, such that each variant follows the same setting as its corresponding ViT variant. All models\nare pretrained from scratch for about 1M steps on the training set of ALIGN (Jia et al., 2021) and\nthe Colossal Clean Crawled Corpus (C4) dataset presented in Raffel et al. (2019). We mix the two\npretraining datasets within each batch, which contains 4,096 image-text pairs (ALIGN) and 512\ntext-only documents (C4), sharded across 512 TPU v3 chips (Jouppi et al., 2017). More pretraining\nsettings are detailed in Appendix B.1.\nAfter pretrained, our model is ﬁnetuned and evaluated on six vision-language benchmarks, including\nthree discriminative tasks: VQA v2 (Goyal et al., 2017), SNLI-VE (Xie et al., 2019), and NLVR2\n(Suhr et al., 2018); as well as three generative tasks: CoCo captioning (Chen et al., 2015), NoCaps\n(Agrawal et al., 2019), and Multi30k (Elliott et al., 2016). We additionally examine its zero-shot\ngeneralization and performance on single-modality tasks. Details of tasks considered and the ﬁne-\ntuning process are outlined in Appendix B.2.\n4.2 C OMPARISON WITH EXISTING APPROACHES\nTo examine the quality of vision-language pretraining, we ﬁrst compare SimVLM on the popular\nmulti-modal tasks with state-of-the-art (SOTA) VLP methods including LXMERT (Tan & Bansal,\n2019), VL-T5 (Cho et al., 2021), UNITER (Chen et al., 2020b), OSCAR (Li et al., 2020), Villa (Gan\net al., 2020), SOHO (Huang et al., 2021), UNIMO (Li et al., 2021), and VinVL (Zhang et al., 2021).\nAs can be seen in Table 1, SimVLM outperforms all existing models and achieves new SOTA results\non all tasks considered, often by a signiﬁcant margin. This demonstrates our generative pretraining\napproach is competitive with MLM-based models and that simple framework with weak supervision\nis sufﬁcient to learn high-quality multi-modal representations.\n5\nPublished as a conference paper at ICLR 2022\nSetup CoCo Caption NoCaps\nB@4 M C S In Near Out Overall\nBUTDa†\nsupervised\n36.3 27.7 120.1 21.4 - - - -\nAoANetb† 39.5 29.3 129.3 23.2 - - - -\nM2 Transformerc† 39.1 29.2 131.2 22.6 81.2 - 69.4 75.0\nSimVLMbase\nzero-shot\n9.5 11.5 24.0 7.5 83.2 84.1 82.5 83.5\nSimVLMlarge 10.5 12.0 24.9 8.3 97.6 96.5 96.3 96.6\nSimVLMhuge 11.2 14.7 32.2 8.5 101.2 100.4 102.3 101.4\nSimVLMbase\nfew-shot\n34.7 29.2 118.7 21.9 95.0 91.9 98.5 93.7\nSimVLMlarge 35.4 30.2 124.1 22.7 102.5 100.9 106.0 102.2\nSimVLMhuge 36.8 31.5 131.3 24.0 111.8 110.6 111.0 110.4\nOSCAR†\npretrain-ﬁnetune\n41.7 30.6 140.0 24.5 85.4 84.0 80.3 83.4\nVinVL† 41.0 31.1 140.9 25.2 103.7 95.6 83.8 94.3\nSimVLMhuge 40.6 33.7 143.3 25.4 113.7 110.9 115.2 112.2\nTable 2: Image captioning results on CoCo Karpathy-test split and NoCaps validation split. For No-\nCaps, {In, Near, Out}refer to in-domain, near-domain and out-of-domain respectively. †indicates\nCider optimization. Model references: aAnderson et al. (2018) bHuang et al. (2019) cCornia et al.\n(2020).\nFor the discriminative tasks, the SimVLM base already outperforms all prior methods while using\nless capacity, and the SimVLM huge obtains almost 4 points absolute score improvement compared\nto the previous SOTA (VinVL), pushing the single model performance above 80% on VQA for\nthe ﬁrst time. In addition, SimVLM also consistently outperforms prior methods on NLVR2 and\nSNLI-VE, illustrating its capability of processing more complex visual-linguistic reasoning. For\nthe generation tasks including image captioning and image translation, SimVLM also shows large\nimprovements using naive ﬁnetuning techniques. Our model outperforms on 3 out of 4 metrics\non the public “Karpathy” 5k test split of CoCo captioning as well as the NoCaps benchmark than\nprior methods trained with more complex reinforcement learning approach of CIDEr optimization\n(Rennie et al., 2017). Finally, SimVLM is also effective for image translation of Multi30k from\nEnglish to German. These experiments demonstrate that our model can be seamlessly plugged into\nthe pretraining-ﬁnetuning paradigm with superior performance, utilizing minimalist pretraining and\nﬁnetuning procedures.\n4.3 Z ERO -SHOT GENERALIZATION\nA crucial beneﬁt of generative modeling and scaling with weak supervision is the potential of zero-\nshot generalization. Models (Brown et al., 2020; Radford et al., 2021; Jia et al., 2021) have been\nshown capable of performing few-shot or zero-shot transfer from pretrained models to downstream\ndatasets, even across language boundaries (Lample & Conneau, 2019). In this section, we show-\ncase three different settings of zero-shot applications less explored in prior VLP work, including\ntransferring to unseen tasks, modalities and/or testing instances.\n4.3.1 Z ERO -SHOT /FEW-SHOT IMAGE CAPTIONING\nThe pretraining procedure of SimVLM can be interpreted as a noisy image captioning objective\non real-world web corpus. Thus, it is natural to ask how well this caption ability generalizes to\nother datasets in a zero-shot/few-shot manner. To this end, we take the pretrained SimVLM model,\nand directly decode on image captioning benchmarks for the zero-shot setting while ﬁnetune on\n1% training data for 5 epochs for the few-shot setting. We also found that using a preﬁx prompt\n“A picture of” improves the quality of decoded captions, similar to the ﬁnding in Radford\net al. (2021).\nAs shown in Table 2, the zero-shot/few-shot performance (Appendix D) of SimVLM is competi-\ntive with fully supervised baselines on CoCo, and it also demonstrates strong generalization on the\nconcept-rich NoCaps benchmark by achieving better scores than pretrained models. Figure 2 (a)\nillustrates sample captions generated by our model (Appendix A). SimVLM is able to not only cap-\nture real-world concepts but also provide a detailed description of the visual input. For example, the\ndecoded samples are able to explain complex scenes with multiple objects (e.g. “people”, “table with\ndrinks”, “dark restaurant”). Besides, the model also shows understanding of ﬁne-grained abstraction\nsuch as speciﬁc car brand and model (e.g. “Aston Martin”, “Vantage”). SimVLM even performs\n6\nPublished as a conference paper at ICLR 2022\nSNLI-VE Multi30k\nSNLI-VE (T) SNLI MNLI Multi30k (T)\nAccdev/Acctest B@4 M\nFully Supervised Baseline\nEVE-Image 71.56 / 71.16 - -\nUNITER 78.59 / 78.28 - -\nSOHO 85.00 / 84.95 - -\nLIUMa - 23.8 35.1\nGroundedTransa - 15.8 31.2\nZero-Shot Cross-Modality Transfer\nSimVLMbase 71.35 / 71.02 72.65 / 72.24 64.37 / 63.98 15.0 24.8\nSimVLMlarge 72.85 / 72.44 73.62 / 73.23 66.97 / 66.31 17.7 30.1\nSimVLMhuge 73.56 / 73.08 74.24 / 73.86 67.45 / 66.97 18.2 32.6\nTable 3: Zero-shot cross-modality transfer results on SNLI-VE and Multi30k. For SNLI-VE, the\nzero-shot model is ﬁnetuned on three source datasets: text-only SNLI-VE (Xie et al., 2019), SNLI\n(Bowman et al., 2015), and MNLI (Williams et al., 2017). For Multi30k, the model is ﬁnetuned on\ntext-only Multi30k data. Model reference: a(Specia et al., 2016).\nrobustly on challenging images that could be tricky for human, such as abstract or dark pictures.\nThese all illustrate that our model learns a wide range of real-world concepts that generalize well in\na zero-shot manner.\n4.3.2 Z ERO -SHOT CROSS -MODALITY TRANSFER\nExisting pretraining methods have been shown to be successful in transferring knowledge across\nheterogeneous data spaces. For example, multilingual language models (Devlin et al., 2018; Lample\n& Conneau, 2019) enable zero-shot cross-lingual transfer such that the model is only ﬁnetuned\nusing training data from a source language (typically English) and evaluated on the target language\nwithout further training. Inspired by this setup, we explore a novel zero-shot cross-modality transfer\nparadigm of utilizing VLP models, and evaluate how well our model generalizes across modalities.\nSince text training data are usually cheaper to obtain compared to visual data, we ﬁnetune SimVLM\non text-only downstream data and then directly evaluate the zero-shot transfer on joint VL tasks.\nSpeciﬁcally, We utilize SNLI-VE and Multi30k to examine the zero-shot transfer performance. For\nSNLI-VE, we ﬁnetune on three text-only NLI datasets such that the premise sentence is used as the\nencoder’s input while the hypothesis is fed to the decoder, and a similar classiﬁer head is trained\non the embedding of the last token in the decoder. At inference, the ﬁnetuned model is evaluated\nby taking the premise image as the encoder input and the corresponding hypothesis sentence to the\ndecoder. As shown in Table 3, SimVLM performs competitively with fully supervised baselines\nincluding UNITER under the zero-shot setting. As a sanity check, we also mask out the image\nfeature to predict using the hypothesis only, and ﬁnd our models can only obtain results close to\nrandom guess (average scores of 34.31 / 34.62). This results in performance close to random guess\nhence demonstrating the effectiveness of SimVLM’s cross-modality transfer ability.\nIn addition, SimVLM is also capable of domain adaption by transferring from the MNLI dataset\nto SNLI-VE, whereby data comes not only from a different modality but also another domain. We\nalso ﬁnd it possible to transfer across different languages and modalities using SimVLM. Specif-\nically, we utilize the German image captioning task from WMT 2016 of Multi30k for evaluation,\nwhere our model is ﬁnetuned on English-German text-only translation data followed by decoding\nwith image-only input in the encoder. Table 3 shows that SimVLM is capable of transferring knowl-\nedge across modalities and languages in generative tasks, achieving comparable performance to\nsupervised baselines (decoded examples shown in Figure 2 (b)). These results suggest zero-shot\ncross-modality transfer emerges with the scaling of weakly labeled data.\n4.3.3 O PEN -ENDED VQA\nOn the VQA benchmark, the best performing models to date formulate the problem as a discrimina-\ntive task of multi-label classiﬁcation over a predeﬁned 3,129 answer candidates, often consisting of\nshort factual terms. In real-world applications, however, it is hard to deﬁne a closed set of candidate\nanswers that covering all possible scenarios, making the true open-ended VQA a challenging setup.\n7\nPublished as a conference paper at ICLR 2022\nDev Karpathy-test Partial Train\nIn-domain Out-domain OverallIn-domain Out-domain Overall\nDiscriminative\nUNITER - 74.4 10.0 70.5 - - -\nVL-T5 - 70.2 7.1 66.4 - - -\nVL-BART - 69.4 7.0 65.7 - - -\nSimVLMbase 73.8 79.0 16.7 75.3 78.4 10.3 70.5\nSimVLMlarge 76.0 80.4 17.3 76.7 79.5 11.0 71.8\nSimVLMhuge 76.5 81.0 17.5 77.2 80.2 11.1 72.2\nGenerative\nVL-T5 - 71.4 13.1 67.9 - - -\nVL-BART - 72.1 13.2 68.6 - - -\nSimVLMbase 73.2 78.3 25.8 75.2 77.1 27.1 71.3\nSimVLMlarge 75.2 79.5 29.6 76.5 78.7 28.4 72.5\nSimVLMhuge 75.5 79.9 30.3 77.0 79.1 28.8 73.0\nTable 4: Comparison of discriminative and generative VQA methods. “Dev” refers to standard\nvqa-score on the VQA validation split. “Karpathy-test” is the setup used in Cho et al. (2021) for\nevaluation on the Karpathy split with rare answers. “Partial Train” refers to train the model only on\npartial training data which contain subset of all candidate answers.\nGenerative models such as SimVLM provide an alternative solution towards this challenge by gen-\nerating free-form textual answers without being constrained to predeﬁned answers. To this end, we\nﬁnetune SimVLM using the PreﬁxLM loss described above where we treat the concatenation of the\nimage and the question as the preﬁx, and train the model to generate answers.\nMethod Acc@1\nSimCLRv2 (Chen et al., 2020a) 79.8\nDINO (Caron et al., 2021) 80.1\nCLIP (Radford et al., 2021) 85.4\nALIGN (Jia et al., 2021) 85.5\nSimVLMbase 80.6\nSimVLMlarge 82.3\nSimVLMhuge 83.6\nTable 5: Linear evaluation on ImageNet classiﬁ-\ncation, compared to state-of-the-art representation\nlearning methods.\nWe then compare the generative approach with\nclassiﬁcation methods in Table 4. Firstly,\nwe follow Cho et al. (2021) and evaluate\nmodel performance on questions with rare an-\nswers in the Karpathy-test split. Here, out-\nof-domain questions are deﬁned as those with\nbest-scoring answer not included in the 3,129\ncandidates. Results show that SimVLM outper-\nforms both discriminative and generative base-\nlines on all splits. More importantly, the gen-\nerative SimVLM signiﬁcantly improves on the\nout-of-domain split by over 17 points, demon-\nstrating its strong generalization. However, this\nsetup mainly focuses on rare answers and it re-\nmains unclear how well the model generalizes\nto common unseen answers. We therefore pro-\nceed to investigate a more challenging setup where we randomly select 2,085 (about two-thirds\nof 3,129) in-domain answers and partition both train and validation sets into two splits based on\nwhether their best-scoring answers are included in the selected set or not. We then only ﬁnetune\nSimVLM on the in-domain split of the train set and evaluate on the entire validation set. The “Par-\ntial Train” column in Table 4 shows that the generative SimVLM is also competent in this setup by\nscoring reasonably well on over 1,000 unseen answers. Overall, we found the generative SimVLM\nperforms competitively with its discriminative counterpart in the standard setup, and works generally\nbetter in the out-of-domain case.\nNote that we use the exact matching between generated answers and human labels for score calcu-\nlation in the above experiment, however it is possible that the model generates appropriate answers\nin different formats or synonyms. Therefore, in addition to the quantitative study above, we show\nqualitative generation results in Figure 2 (c). It can be observed that SimVLM is able to generate\nanswers not included in the 3,129 candidate set (e.g. “surgeon” and “wood carving”), demonstrating\nthat SimVLM can transfer knowledge from the pretraining corpus to VQA. It is thus natural to ask\nwhether SimVLM can perform zero-shot VQA without ﬁnetuning at all. In our experiments, we\nfound that SimVLM is able to “answer” by completing prompting sentences, as shown in Figure 2\n(d). Nonetheless, we also observed that the model falls short in generating meaningful answers to\nthe real questions. We hypothesize that this is due to the low quality of the pretraining data in which\nmost textual descriptions are short and noisy. To verify our assumption, we continue the pretraining\nprocess on the cleaner WIT dataset (Srinivasan et al., 2021) for 50k steps. Examples in Figure 2 (e)\n8\nPublished as a conference paper at ICLR 2022\nshow that open-ended VQA ability emerges in SimVLM such that it can generate related responses\nafter ﬁnetuning on the knowledge-rich wikipedia dataset.\n4.4 A NALYSIS\nMethod VQA score\nNo Pretraining 49.70\nDecoder-only 65.23\nw/ LM 64.48\nSimVLMsmall 67.43\nw/o Image2Text 49.23\nw/o Text2Text 65.25\nw/o conv stage 63.11\nw/ span corruption 66.23\nw/ 2 conv blks 65.57\nw/ 4 conv blks 66.55\nw/ 10% ALIGN 66.71\nw/ CC-3M 63.32\nTable 6: Ablation study on VQA. “w/ LM” and\n“w/ span corruption” denote replacing the pro-\nposed PreﬁxLM loss with a different pretraining\nobjective. “Image2Text” and “Text2Text” refer to\nthe noisy image-text data and the text-only data\nused for pretraining. “conv blks” denotes number\nof ResNet blocks.\nSingle-Modality Tasks. Since SimVLM per-\nforms well on joint vision-language bench-\nmarks, it is natural to ask how well the\nlearned representations perform on tasks of sin-\ngle modality. We hope to gain deeper insights\ninto the model behavior by examining its per-\nformance on these benchmarks, but it is not our\nintention to achieve state-of-the-art on single-\nmodality tasks. In Table 7 (Appendix C), we\ncompare SimVLM with existing VLP models\non the GLUE benchmark (Wang et al., 2018),\nwhere we mainly follow the text processing\nprocedure in Raffel et al. (2019) and train our\nmodel to classify the fully formatted input with-\nout token type embeddings. SimVLM performs\nbetter than existing VLP methods and compet-\nitively with BERT, indicating that it has good\nlanguage understanding ability. Additionally,\nwe also compute the top-1 accuracy on Ima-\ngeNet following the linear evaluation protocol\nin Table 5. Note that our model is not pre-\ntrained with a discriminative task such as the\ncontrastive loss, hence we use an average pool-\ning of encoder outputs as image features. Re-\nsults verify that our model has also learned\nhigh-quality image representation.\nAblation Study. To study the contributions from each model component, we conduct ablation study\non SimVLMsmall models with an embedding dimension of 512 and 8 layers. We make compar-\nisons on VQA in Table 6. First, we compare encoder-decoder models with decoder-only models\nof comparable model size, and ﬁnd that decoder-only model performs signiﬁcantly worse on VQA.\nThis suggests the inductive bias of separating bidirectional encoding from unidirectional decoding\nis beneﬁcial for joint VL representation learning. Next, we study the effectiveness of pretraining\nobjectives and results show that the PreﬁxLM objective outperforms both span corruption (Raffel\net al., 2019) and naive LM, illustrating the importance of using a uniﬁed objective formulation for\nboth image-text and text-only data. Moreover, we ablate the contribution of datasets. While weakly\naligned image-text data are required for bridging the gap between visual and textual representa-\ntions, text-only corpora also improves the model quality. This is probably because textual signals\nare extremely noisy in the former and thus the model relies on the later to acquire better language\nunderstanding. In addition, we experimented with 10% ALIGN and CC-3M (Sharma et al., 2018)\ndatasets, and conﬁrms the importance of data scaling. We then study the effect of the convolution\nstage and ﬁnd it critical for VL performance. Following Dai et al. (2021), we experiment with using\neither the ﬁrst 2/3/4 ResNet Conv blocks, and empirically observe that the 3 conv block setup works\nbest. This indicates that image and text have different levels of representation granularity and thus\nutilizing contextualized patches is beneﬁcial.\n5 C ONCLUSION\nIn this work, we present a simple yet effective framework of vision-language pretraining. Unlike\nprior works using object proposal systems and auxiliary losses, our model processes whole image\nas patches and is trained end-to-end with a single preﬁx language modeling objective. Our work\nsuggests a promising alternative to existing VLP paradigm and we hope our work may inspire future\nresearch on generative VLP.\n9\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGMENTS\nWe would like to thank Hieu Pham, Chao Jia, Andrew Dai, Bowen Zhang, Zhifeng Chen, Ruoming\nPang, Douglas Eck, Claire Cui and Yonghui Wu for helpful discussions, Krishna Srinivasan, Samira\nDaruki, Nan Du and Aashi Jain for help with data preparation, Chao Jia, Zhen Li, Jonathan Shen,\nColin Raffel and Sharan Narang for assistance on experimental settings, and others in the Google\nBrain team for support throughout this project.\nREFERENCES\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra,\nDevi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In\nProceedings of the IEEE/CVF International Conference on Computer Vision , pp. 8948–8957,\n2019.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and\nLei Zhang. Bottom-up and top-down attention for image captioning and visual question answer-\ning. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\n6077–6086, 2018.\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large anno-\ntated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J ´egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint\narXiv:2104.14294, 2021.\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big\nself-supervised models are strong semi-supervised learners. Advances in Neural Information\nProcessing Systems, 33:22243–22255, 2020a.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll ´ar, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv\npreprint arXiv:1504.00325, 2015.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020b.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text\ngeneration. arXiv preprint arXiv:2102.02779, 2021.\nMarcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. Meshed-memory trans-\nformer for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 10578–10587, 2020.\nZihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and\nattention for all data sizes. arXiv preprint arXiv:2106.04803, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogni-\ntion at scale. In International Conference on Learning Representations , 2021. URL https:\n//openreview.net/forum?id=YicbFdNTTy.\nDesmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. Multi30k: Multilingual english-\ngerman image descriptions. arXiv preprint arXiv:1605.00459, 2016.\n10\nPublished as a conference paper at ICLR 2022\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial\ntraining for vision-and-language representation learning. arXiv preprint arXiv:2006.06195, 2020.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6904–6913, 2017.\nKai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang,\nAn Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, and Dacheng Tao. A survey on\nvisual transformer, 2021.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\n770–778, 2016.\nXiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei Zhang, Jianfeng Gao, and Zicheng Liu. Vivo:\nSurpassing human performance in novel object captioning with visual vocabulary pre-training. In\nAAAI, February 2021.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\nstochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016.\nLun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. Attention on attention for image cap-\ntioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp.\n4634–4643, 2019.\nZhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. Seeing out\nof the box: End-to-end pre-training for vision-language representation learning. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 12976–12985,\n2021.\nTaichi Iki and Akiko Aizawa. Effect of vision-and-language extensions on natural language under-\nstanding in vision-and-language models. arXiv preprint arXiv:2104.08066, 2021.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\nwith noisy text supervision. arXiv preprint arXiv:2102.05918, 2021.\nNorman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa,\nSarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre luc Cantin, Clifford\nChao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir\nGhaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug\nHogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexan-\nder Kaplan, Harshit Khaitan, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James\nLaw, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adri-\nana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni,\nKathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross,\nMatt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed\nSouter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Er-\nick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon.\nIn-datacenter performance analysis of a tensor processing unit, 2017.\nSalman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and\nMubarak Shah. Transformers in vision: A survey, 2021.\nWonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convo-\nlution or region supervision, 2021.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual\ngenome: Connecting language and vision using crowdsourced dense image annotations. 2016.\nURL https://arxiv.org/abs/1602.07332.\n11\nPublished as a conference paper at ICLR 2022\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\nGuillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint\narXiv:1901.07291, 2019.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple\nand performant baseline for vision and language, 2019.\nWei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang.\nUNIMO: Towards uniﬁed-modal understanding and generation via cross-modal contrastive learn-\ning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on Natural Language Processing (Volume 1:\nLong Papers), pp. 2592–2607, Online, August 2021. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2021.acl-long.202. URL https://aclanthology.org/2021.\nacl-long.202.\nXiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-\ntraining for vision-language tasks. ECCV 2020, 2020.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-\ntrain, prompt, and predict: A systematic survey of prompting methods in natural language pro-\ncessing, 2021.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-\ntems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.\ncc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf.\nOﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint\narXiv:1608.05859, 2016.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\n12\nPublished as a conference paper at ICLR 2022\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and\nR. Garnett (eds.), Advances in Neural Information Processing Systems , volume 28. Curran As-\nsociates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/\n14bfa6bb14875e45bba028a21ed38046-Paper.pdf.\nSteven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical\nsequence training for image captioning. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 7008–7024, 2017.\nPaul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue:\nLearning feature matching with graph neural networks. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 4938–4947, 2020.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp.\n2556–2565, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi:\n10.18653/v1/P18-1238. URL https://aclanthology.org/P18-1238.\nJonathan Shen, Patrick Nguyen, Yonghui Wu, Zhifeng Chen, et al. Lingvo: a modular and scalable\nframework for sequence-to-sequence modeling, 2019.\nSheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei\nYao, and Kurt Keutzer. How much can clip beneﬁt vision-and-language tasks? arXiv preprint\narXiv:2107.06383, 2021.\nLucia Specia, Stella Frank, Khalil Sima’An, and Desmond Elliott. A shared task on multimodal\nmachine translation and crosslingual image description. In Proceedings of the First Conference\non Machine Translation: Volume 2, Shared Task Papers, pp. 543–553, 2016.\nKrishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit:\nWikipedia-based image text dataset for multimodal multilingual machine learning.arXiv preprint\narXiv:2103.01913, 2021.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-\ntraining of generic visual-linguistic representations. In International Conference on Learning\nRepresentations, 2020. URL https://openreview.net/forum?id=SygXPaEYvH.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for\nreasoning about natural language grounded in photographs. arXiv preprint arXiv:1811.00491 ,\n2018.\nHao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from\ntransformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pp. 5100–5111, Hong Kong, China, November 2019. Association for Com-\nputational Linguistics. doi: 10.18653/v1/D19-1514. URL https://aclanthology.org/\nD19-1514.\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multi-\nmodal few-shot learning with frozen language models. Advances in Neural Information Process-\ning Systems, 34, 2021.\nShagun Uppal, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumdar, Soujanya Poria, Roger\nZimmermann, and Amir Zadeh. Multimodal research in vision and language: A review of current\nand emerging trends, 2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\n13\nPublished as a conference paper at ICLR 2022\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018.\nAdina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.\nTete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll ´ar, and Ross Girshick. Early\nconvolutions help transformers see better, 2021.\nNing Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: A novel task for ﬁne-\ngrained image understanding. arXiv preprint arXiv:1901.06706, 2019.\nHaiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and Fei Huang.\nE2E-VLP: End-to-end vision-language pre-training enhanced by visual learning. In Proceed-\nings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp.\n503–513, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/\n2021.acl-long.42. URL https://aclanthology.org/2021.acl-long.42.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.\nXlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural\ninformation processing systems, pp. 5754–5764, 2019.\nFei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowl-\nedge enhanced vision-language representations through scene graphs. InProceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 35, pp. 3208–3216, 2021.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and\nJianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. InProceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 5579–\n5588, June 2021.\n14\nPublished as a conference paper at ICLR 2022\nFigure 2: Generated examples of SimVLM of various applications: (a) zero-shot image captioning\n(b) zero-shot cross-modality transfer on German image captioning (c) generative VQA (d) zero-shot\nvisual text completion (e) zero-shot open-ended VQA.\nA G ENERATED EXAMPLES\nExamples generated by SimVLM of various types are shown in Figure 2. We use either image-only\nor image-text preﬁx inputs in the encoder, and use the decoder to generate sufﬁx text.\n15\nPublished as a conference paper at ICLR 2022\nCoLA SST-2 RTE MRPC QQP MNLI QNLI WNLI\nBERT 54.6 92.5 62.5 81.9/87.6 90.6/87.4 84.2 91.0 48.8\nVisualBERT 38.6 89.4 56.6 71.9/82.1 89.4/86.0 81.6 87.0 53.1\nUNITER 37.4 89.7 55.6 69.3/80.3 89.2/85.7 80.9 86.0 55.4\nVL-BERT 38.7 89.8 55.7 70.6/81.8 89.0/85.4 81.2 86.3 53.1\nVilBERT 36.1 90.4 53.7 69.0/79.4 88.6/85.0 79.9 83.8 55.4\nLXMERT 39.0 90.2 57.2 69.8/80.4 75.3/75.3 80.4 84.2 46.0\nSimVLMbase 46.7 90.9 63.9 75.2/84.4 90.4/87.2 83.4 88.6 58.1\nTable 7: Text-only task performance on the GLUE benchmark (Dev set). Results for BERT and\nother VLP methods are obtained from Iki & Aizawa (2021). The overall best result is bolded while\nunderline signiﬁes the best VLP model.\nB E XPERIMENTAL DETAILS\nB.1 P RETRAINING\nOur models are pretrained according to the methodology described in Section 3. For the Trans-\nformer, each variant follows the same setting as its corresponding ViT variant. For the Conv stage,\nwe use the ﬁrst three blocks (excluding the Conv stem) of ResNet-101 and ResNet-152 (He et al.,\n2016) for our Base and Large models respectively, and a larger variant of ResNet-152 with more\nchannels for the Huge model (matching its hidden dimension size). We always use a ﬁxed patch size\nof 16×16. During pretraining, we utilize the resolution of 224×224, resulting in a patch sequence of\nlength 14×14 as visual tokens. For the textual input, we use a vocabulary size of 32,000 and a max\nsequence length of 256 in both the encoder and the decoder. We also share parameters between the\nembedding and the decoder softmax output layer (Press & Wolf, 2016). All parameters are shared\nacross visual and textual inputs except the Conv stage and positional embeddings.\nWe pretrain on large-scale web datasets for both image-text and text-only inputs. For joint vision\nand language data, we exploit the training set of ALIGN (Jia et al., 2021), which contains about 1.8B\nnoisy image-text pairs. Notice that we do not use any extra data preprocessing or ﬁltering, except\nsimple random resized cropping. For the text-only copora, we use the Colossal Clean Crawled\nCorpus (C4) dataset presented in Raffel et al. (2019) and followed their preprocessing steps. The\ndataset contains about 800GB of web crawled documents.\nAll models are pretrained for about 1M steps from scratch to optimize for the single PreﬁxLM\nobjective in Eq.3. We use the AdamW optimizer (Loshchilov & Hutter, 2017) with β1 = 0.9,β2 =\n0.999 and weight decay of 0.01. We warm up the learning rate for the ﬁrst 2% of updates to a peak\nvalue of 5×10−4, and then linearly decay it afterwards. Dropout is not used during the pretraining\nstage. We mix the two pretraining datasets within each batch, which contains 4,096 image-text pairs\nand 512 text-only documents, sharded across 512 TPU v3 chips (Jouppi et al., 2017).\nB.2 F INETUNING\nAfter pretraining, our model is ﬁnetuned on various downstream tasks. Similar to the pretraining\nstage, we use the AdamW optimizer with the same Beta values, while we tune the learning rate in\n{1×10−5, 2×10−5, 5×10−5}. We also enable regularization methods of Dropout (set to 0.1) and\nstochastic depth (only applied to Conv stage and encoder with a ﬁxed dropout rate of 0.1) (Huang\net al., 2016) during the ﬁnetuning stage. Following standard practice, we use the corresponding dev\nsplit to ﬁnd the best setting and report the result on the test split. We consider 5 types of downstream\ntasks listed below:\nVisual question answering: This task requires the model to answer questions about input images,\nand has been the most widely used VL benchmark. Following prior work, we use the VQA v2\n(Goyal et al., 2017) and formulate the task as a classiﬁcation problem over 3,129 most frequent\nanswers in the training set. The raw image and the corresponding question are used as inputs to the\nencoder and the decoder respectively, and a task-speciﬁc linear classiﬁer is trained to predict answer\n16\nPublished as a conference paper at ICLR 2022\nbased on activation corresponding to the last question token from the decoder. We use a resolution\nof 480×480 for the image and all positional parameters are adapted using linear interpolation.\nVisual entailment: The SNLI-VE (Xie et al., 2019) dataset is adapted from SNLI (Bowman et al.,\n2015), which is originally designed to predict the relation between a premise sentence and a hy-\npothesis sentence as either entailment, neutral or contradiction, a task known as natural language\ninference (NLI). For the VL variant, the premise is based on the content of an image rather than\ntextual descriptions. We ﬁnetune SimVLM similarly to VQA, such that the image and the sentence\nare fed to encoder and decoder separately, and the classiﬁer is trained to predict the three relations.\nVisual reasoning: The NLVR2 (Suhr et al., 2018) dataset tests the model’s ability of jointly rea-\nsoning over the language and multiple images by asking whether a textual description is true based\non a pair of two images. Following Zhang et al. (2021), we create two input pairs, each consisting\nof one image and the textual description, and generate output embeddings for both using the same\nsetup above. The two embeddings are then concatenated for ﬁnal prediction.\nImage captioning: The captioning task requires a model to generate natural language descriptions\nof input images. We consider two datasets CoCo (Chen et al., 2015) and NoCaps (Agrawal et al.,\n2019), both ﬁnetuned using the CoCo training data. For SimVLM, it is straightforward to ﬁrst\nencode the image in the encoder and then generate captions using the decoder. Note that in contrast\nto prior work that apply task-speciﬁc tricks such as CIDEr optimization (Rennie et al., 2017), our\nmodel is trained with naive cross-entropy loss only.\nMultimodal translation: The goal of multimodal translation is to translate image descriptions in\nsource language to target language, for which image inputs can be taken advantage of as grounding\nsignal. We train and evaluate on the Multi30k (Elliott et al., 2016) dataset. We utilize the PreﬁxLM\ndescribed in previous sections such that the source sentence, together with the image inputs, are fed\nto the encoder, which will be translated to the target language by the decoder.\nC M ODEL PERFORMANCE ON LANGUAGE -ONLY TASK\nWe compare our model with prior VLP methods on natural language understanding (NLU) tasks on\nthe GLUE benchmark (Wang et al., 2018) in Table 7.\nD E RRATUM\nWe found an error in reporting the zero-shot COCO evaluations in the ﬁrst version of this paper.\nThis mistake does NOT affect all other results and the numbers have been updated. Meanwhile, we\nalso added few-shot results in addition to zero-shot results on both MsCOCO and NoCaps in Table\n2, to provide a more comprehensive view of capacities in SimVLM models. Hence, our main claims\nand conclusions still hold.\n17"
}