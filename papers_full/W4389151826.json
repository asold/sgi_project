{
  "title": "Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph",
  "url": "https://openalex.org/W4389151826",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099646086",
      "name": "Kai Mao",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2113099636",
      "name": "Yukun Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097355374",
      "name": "Zhen Huang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2104600475",
      "name": "Song Tong",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2270087390",
      "name": "Kaiping Peng",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3095155674",
    "https://openalex.org/W2914002162",
    "https://openalex.org/W6839366901",
    "https://openalex.org/W6681117848",
    "https://openalex.org/W3195599406",
    "https://openalex.org/W6650840267",
    "https://openalex.org/W6851128669",
    "https://openalex.org/W4366699868",
    "https://openalex.org/W2163667576",
    "https://openalex.org/W4367320668",
    "https://openalex.org/W2412480261",
    "https://openalex.org/W4280625395",
    "https://openalex.org/W2037755177",
    "https://openalex.org/W4318014888",
    "https://openalex.org/W2069525597",
    "https://openalex.org/W3173135198",
    "https://openalex.org/W2007445014",
    "https://openalex.org/W6672662947",
    "https://openalex.org/W6635111936",
    "https://openalex.org/W1571413925",
    "https://openalex.org/W2091757591",
    "https://openalex.org/W2950657507",
    "https://openalex.org/W4366967807",
    "https://openalex.org/W2788827763",
    "https://openalex.org/W1991853831",
    "https://openalex.org/W2064371796",
    "https://openalex.org/W2113352630",
    "https://openalex.org/W4323345674",
    "https://openalex.org/W3029144977",
    "https://openalex.org/W7027191373",
    "https://openalex.org/W6850732227",
    "https://openalex.org/W6663703580",
    "https://openalex.org/W2810181630",
    "https://openalex.org/W4287510462",
    "https://openalex.org/W2153162913",
    "https://openalex.org/W6855937018",
    "https://openalex.org/W3101625015",
    "https://openalex.org/W3048842799",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4385490607",
    "https://openalex.org/W6663748030",
    "https://openalex.org/W4383552392",
    "https://openalex.org/W4206266763",
    "https://openalex.org/W4246766218",
    "https://openalex.org/W4248605742",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4248915504",
    "https://openalex.org/W2143283816",
    "https://openalex.org/W4367692219",
    "https://openalex.org/W4385750079",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2052569240",
    "https://openalex.org/W4323075157",
    "https://openalex.org/W4283263983",
    "https://openalex.org/W4252231316",
    "https://openalex.org/W4387226207",
    "https://openalex.org/W4380993239",
    "https://openalex.org/W4231961364",
    "https://openalex.org/W4384154918",
    "https://openalex.org/W4327545654",
    "https://openalex.org/W4239182178",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2007465020",
    "https://openalex.org/W2481216031",
    "https://openalex.org/W4386242902",
    "https://openalex.org/W2738389686",
    "https://openalex.org/W3038093635",
    "https://openalex.org/W4282835567",
    "https://openalex.org/W4255249122",
    "https://openalex.org/W4241286296",
    "https://openalex.org/W1586482976"
  ],
  "abstract": "Leveraging the synergy between causal knowledge graphs and a large language model (LLM), our study introduces a groundbreaking approach for computational hypothesis generation in psychology. We analyzed 43,312 psychology articles using the LLM and other machine learning tools, extracting causal relation pairs. This analysis produced a specialized causal graph for psychology. Applying link prediction algorithms, we generated 130 potential psychological hypotheses focusing on `well-being', then compared them against research ideas conceived by doctoral scholars and those produced solely by the LLM. Interestingly, our combined approach of LLM and causal graph mirrored expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses. This alignment was further corroborated using deep semantic analysis. Our results show that combining LLM with machine learning techniques like causal knowledge graphs can revolutionize automated discovery in psychology, extracting novel insights from extensive literature. This work stands at the crossroads of psychology and artificial intelligence, championing a new enriched paradigm for data-driven hypothesis generation in psychological research.",
  "full_text": "Running head: INSERT SHORTTITLE COMMAND IN PREAMBLE 1\nAutomating Psychological Hypothesis Generation with AI: Large Language Models Meet\nCausal Graph\nSong Tong1,2,∗, Kai Mao3∗, Zhen Huang2, Yukun Zhao2,†, Kaiping Peng1,2,†\n1. Department of Psychology, School of Social Sciences, Tsinghua University, Beijing, China\n2. Positive Psychology Research Center, School of Social Sciences, Tsinghua University,\nBeijing, China\n3. Kindom KK, Tokyo, Japan (K. Mao: richardmao4@gmail.com)\n* Equal technical contribution\n†Correspondence: {zhaoyukun, pengkp}@tsinghua.edu.cn\nINSERT SHORTTITLE COMMAND IN PREAMBLE 2\nAbstract\nLeveraging the synergy between causal knowledge graphs and a large language model\n(LLM), our study introduces a groundbreaking approach for computational hypothesis\ngeneration in psychology. We analyzed 43,312 psychology articles using the LLM and other\nmachine learning tools, extracting causal relation pairs. This analysis produced a\nspecialized causal graph for psychology. Applying link prediction algorithms, we generated\n130 potential psychological hypotheses focusing on ‘well-being’, then compared them\nagainst research ideas conceived by doctoral scholars and those produced solely by the\nLLM. Interestingly, our combined approach of LLM and causal graph mirrored expert-level\ninsights in terms of novelty, clearly surpassing the LLM-only hypotheses (t(59) = 3.34,\np=0.007 andt(59) = 4.32,p<0.001, respectively). This alignment was further corroborated\nusing deep semantic analysis. Our results show that combining LLM with machine learning\ntechniques like causal knowledge graphs can revolutionize automated discovery in\npsychology, extracting novel insights from extensive literature. This work stands at the\ncrossroads of psychology and artificial intelligence, championing a new enriched paradigm\nfor data-driven hypothesis generation in psychological research.\nKeywords: Hypothesis Generation, Causal Reasoning, Large Language Model,\nPsychological Science, Scientific Discovery\nINSERT SHORTTITLE COMMAND IN PREAMBLE 3\nAutomating Psychological Hypothesis Generation with AI: Large Language Models Meet\nCausal Graph\nIntroduction\nIn an age where the confluence of artificial intelligence (AI) profoundly shapes sectors\nranging from academic research to commercial enterprises, dissecting the interplay of these\ndisciplines becomes paramount (Williams, Berman, & Michalska, 2023). In particular,\npsychology, which serves as a nexus between the humanities and natural sciences,\nconsistently endeavors to demystify the complex web of human behaviors and cognition\n(Hergenhahn & Henley, 2013). Its profound insights have significantly enriched academia,\ninspiring innovative applications in AI design. For example, AI models have been molded\non hierarchical brain structures (Cichy, Khosla, Pantazis, Torralba, & Oliva, 2016) and\nhuman attention systems (Vaswani et al., 2017). In contrast, these AI models reciprocally\noffer a rejuvenated perspective, deepening our understanding from the foundational\ncognitive taxonomy to nuanced aesthetic perceptions (Battleday, Peterson, & Griffiths,\n2020; Tong, Liang, Kumada, & Iwaki, 2021). However, the multifaceted domain of\npsychology, particularly social psychology, has exhibited a measured evolution compared to\nits tech-centric counterparts. This can be attributed to its enduring reliance on\nconventional theory-driven methodologies (Henrich, Heine, & Norenzayan, 2010; Shah,\nCappella, & Neuman, 2015), a characteristic that stands in stark contrast to the burgeoning\nparadigms of AI and data-centric research (Bechmann & Bowker, 2019; Wang et al., 2023).\nIn the journey of psychological research, each exploration originates from a spark of\ninnovative thought. These research trajectories may arise from established theoretical\nframeworks, daily event insights, anomalies within data, or intersections of interdisciplinary\ndiscoveries (Jaccard & Jacoby, 2019). Hypothesis generation is pivotal in psychology\n(Koehler, 1994; McGuire, 1973), facilitating the probing into multifaceted influencers of\nhuman attitudes, actions, and beliefs. HyGene model (Thomas, Dougherty, Sprenger, &\nHarbison, 2008) elucidated the intricacies of hypothesis generation, encompassing\nINSERT SHORTTITLE COMMAND IN PREAMBLE 4\nconstraints of working memory and the interplay between ambient and semantic memories.\nRecently, the causal graph has provided psychology with a systematic framework, enabling\nresearchers to build and simulate intricate systems for a holistic view of \"bio-psycho-social\"\ninteractions (Borsboom et al., 2021; Crielaard et al., 2022). However, the labor-intensive\nnature of the methodology poses challenges, which requires multidisciplinary expertise in\nalgorithmic development, which exacerbates the complexities (Crielaard et al., 2022).\nMeanwhile, advancements in AI, exemplified by models like Generative Pre-Trained\nTransformer (GPT), present new avenues for creativity and hypothesis generation (Wang\net al., 2023).\nWith modern AI technologies, notably large language models (LLMs) such as GPT-3,\nGPT-4, and Claude-2, demonstrating profound capabilities to comprehend and infer\ncausality from natural language texts, a promising path has emerged to extract causal\nknowledge from vast textual data (Binz & Schulz, 2023; Gu et al., 2023). Excitingly, in\nspecific scenarios, LLMs and causal graph manifest complementary strengths (Pan et al.,\n2023). Their synergistic combination converges human analytical and systemic thinking,\nechoing the holistic versus analytic cognition delineated in social psychology (Nisbett,\nPeng, Choi, & Norenzayan, 2001). This amalgamation enables fine-grained semantic\nanalysis and conceptual understanding via LLMs, while causal graph offers a global\nperspective on causality, alleviating AI’s interpretability challenges (Pan et al., 2023). This\nintegrated methodology efficiently counters the inherent limitations of working and\nsemantic memories in hypothesis generation and, as previous academic endeavors indicate,\nhas proven efficacious across disciplines. For example, a groundbreaking study in physics\nsynthesized 750,000 physics publications, utilizing cutting-edge natural language processing\nto extract 6,368 pivotal quantum physics concepts, culminating in a semantic network\nforecasting research trajectories (Krenn & Zeilinger, 2020). Additionally, by integrating\nknowledge-based causal graphs into the interior of LLM, the capability for causative\ninference of LLM has been significantly improved (Kıcıman, Ness, Sharma, & Tan, 2023).\nINSERT SHORTTITLE COMMAND IN PREAMBLE 5\nThus, our study seeks to erect a pioneering analytical framework, marrying the\nsemantic and conceptual extraction proficiency of LLMs with the systemic thinking of the\ncausal graph, with the aim of crafting a panoramic causal network of semantic concepts\nwithin psychology. We meticulously analyzed 43,312 psychological articles, devising an\nautomated method to construct a causal graph, and systematically mining causative\nconcepts and their interconnections. Concretely, the initial sifting and preparation of the\ndata ensures a high-quality corpus, followed by harnessing advanced extraction techniques\nto identify standardized causal concepts. This results in a graph database serving as a\nreservoir of causal knowledge. In conclusion, using node embedding and similarity-based\nlink prediction, we unearthed potential causal relations, forging the corresponding\nhypotheses.\nTo gauge the pragmatic value of our network, we spotlighted 130 hypotheses on\n‘well-being’ generated by our framework, juxtaposing them against hypotheses crafted by\nnovice experts (doctoral students in psychology) and the Claude-2 model. The results are\nencouraging: Our algorithm matches the caliber of novice experts, outshining the\nhypotheses generated solely by the Claude-2 model in novelty. Additionally, through deep\nsemantic analysis, we demonstrated that our algorithm boasts more profound conceptual\nincorporations and a broader semantic spectrum. Our modality not only extracts invaluable\ncausal knowledge from the literature, converting it to visual graphics, but also furnishes\nnovel tools and methodologies for causal analysis and scientific knowledge discovery. These\nvisual aids can feed algorithms, aiding in deducing more latent causal relations and guiding\nmodels to produce a plethora of novel causal hypotheses. This groundbreaking research not\nonly underscores the latent potential of LLMs in psychology, but epitomizes the\nharmonious fusion of modern AI with traditional research methodologies, serving as a\nbridge between conventional theory-driven methodologies in psychology and the\nburgeoning paradigms of AI and data-centric research, ultimately contributing to a richer\nand more nuanced understanding of the factors that contribute to human well-being.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 6\nMethodological Framework for Hypothesis Generation\nThe proposed LLM-based Causal Graph (LLMCG) framework encompasses a\nsequence of systematic steps from literature acquisition to the hypothesis generation, as\nillustrated in Figure 1. In the Information gathering phase, approximately 140K\npsychology-related articles were downloaded from public databases. Subsequently, during\nthe extraction phase, GPT-4 and other machine learning techniques were used to distill\ncausal relationships from these documents, culminating in the creation of a causal\nrelationship network based on 43,312 selected articles. In the hypothesis generation phase,\nan in-depth examination of these data was executed, adopting link prediction algorithms to\nforecast the dynamics within the causal relationship network for searching the highly\npotential causality concepts pairs.\nLiterature Retrieval\nThe primary data source for this research was a public repository of scientific articles,\nthe PMC Open Access Subset. Our decision to utilize this repository was informed by\nseveral key attributes that it possesses. The PMC Open Access Subset boasts an expansive\ncollection of over 2 million full-text XML science and medical articles, providing a\nsubstantial and diverse base from which to derive insights for our research. Furthermore,\nthe open-access nature of the articles not only enhances the transparency and\nreproducibility of our methodology, but also ensures that the results and processes can be\nindependently accessed and verified by other researchers. Critically, the content within this\nsubset originates from recognized journals, all of which have undergone rigorous peer\nreview, lending credence to the quality and reliability of the data we leveraged. Lastly, an\nadded advantage was the rich metadata accompanying each article. These metadata were\ninstrumental in refining our article selection process, ensuring coherent thematic alignment\nwith our research objectives in the domains of psychology.\nTo zero in on articles relevant to our study, we instituted a series of filtering criteria.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 7\nFigure 1. The proposed LLM-based Causal Graph (LLMCG) algorithm for the generation\nof psychological hypotheses.\nFirstly, the presence of certain keywords within article titles or abstracts was mandatory.\nSome examples of these keywords include ‘psychol’, ‘clin psychol’, and ‘biol psychol’.\nSecond, we exploited the metadata accompanying each article. The classification of articles\nbased on these metadata ensured alignment with recognized thematic standards in the\ndomains of psychology and neuroscience. Upon the application of these criteria, we\nmanaged to curate a subset of approximately 140K articles that most likely discuss causal\nconcepts in both psychology and neuroscience.\nCausal Pair Extraction\nThe process of gleaning causal knowledge from vast troves of scientific literature is\nintricate and multifaceted. Our methodology distills this complexity into four coherent\nsteps, each serving a distinct purpose. (1) Article Selection & Cost Analysis: Determines\nthe feasibility of processing a specific volume of articles, ensuring optimal resource\nallocation. (2) Text Extraction & Analysis: Ensure the purity of the data that enter our\ncausal extraction phase by filtering out nonrelevant content. (3) Causal Knowledge\nExtraction: Using advanced language models to detect, classify, and standardize causal\nrelationships present in texts. (4) Graph Database Storage: Facilitates structured storage,\neasy retrieval, and the possibility of advanced relational analyses for future research. This\nINSERT SHORTTITLE COMMAND IN PREAMBLE 8\nTable 1\nData sources and publication year distribution for LLMCG computation.\nSource of Publications Quantity\nFrontiers in Psychology 35,797\nBMC Psychology 1,154\nJournal of Psychological Medicine and Mental Pathology 687\nPsychological Medicine 659\nEuropean Journal of Investigation in Health Psychology and Education 384\nOthers 4,631\nTotal 43,312\nPublication Year (1975-2023) Quantity\n2023 2,391\n2022 9,876\n2021 7,286\n2020 4,416\n2019 3,465\nOthers 15,878\nTotal 43,312\nstreamlined approach ensures accuracy, consistency, and scalability in our endeavor to\nunderstand the interplay of causal concepts in psychology and neuroscience.\nText extraction and cleaning.After a meticulous cost analysis detailed in the\nAppendix A, we settled on 43,312 articles deemed most relevant for our study. The\ndistibutions of publication sources and years can be found in Table 1. Extracting the full\ntexts of the articles from their PDF sources was an essential initial step, and, for this\npurpose, the PyPDF2 Python library was used. This library allowed us to seamlessly\nextract and concatenate titles, abstracts, and main content from each PDF article.\nHowever, a challenge encountered was the presence of extraneous sections, such as\nreferences or tables, in the extracted texts. Recognizing that these sections could introduce\nnoise, we developed a machine learning classifier adept at discerning and omitting reference\nINSERT SHORTTITLE COMMAND IN PREAMBLE 9\nsections, given their lack of causal knowledge.\nTo ensure the robustness of the causal relationships we extracted and minimize\npotential biases, we implemented a multifaceted approach. Recognizing the indispensable\nrole of human judgment, we periodically subjected random samples of extracted causal\nrelationships to the scrutiny of domain experts. Their valuable feedback was instrumental\nin real-time fine-tuning of our extraction process. Instead of heavily relying on referenced\nhypotheses, our focus was on gleaning causal pairs primarily from the findings mentioned\nin the article abstracts. This approach minimized the likelihood of extracting similar\ncausality present in multiple papers, thus avoiding undue influence on causal weightings\nand preventing an excessive proliferation of causal relationships. This systematic\nmethodology ultimately resulted in a refined text corpus distilled from 43,312 articles,\nabundant with conceptual insights, and primed for rigorous causal extraction.\nCausal knowledge extraction method. In our effort to extract causal\nknowledge, our choice of GPT-4 was not arbitrary. While several models were available for\nsuch tasks, GPT-4 emerged as a frontrunner due to its advanced capabilities (Wu et al.,\n2023), extensive training on diverse data, and its proven proficiency in understanding\ncontext, especially in complex scientific texts (Cheng et al., 2023; Sanderson, 2023). Other\nmodels were indeed considered; however, the unparalleled capacity of GPT-4 in generating\ncoherent, contextually relevant responses gave it an edge in our project’s specific\nrequirements.\nOur extraction process began with the segmentation of the articles. Due to the token\nconstraints inherent to GPT-4, it was imperative to break down articles into manageable\nchunks, specifically those of 4,000 tokens or fewer. This ensured a comprehensive\ninterpretation of the content without omitting any potential causal relations. The next\nphase was prompt engineering. To effectively guide extraction capabilities of GPT-4, we\ncrafted explicit prompts. A testament to this meticulous engineering is demonstrated in a\ndirective in which we asked the model to elucidate causal pairs in a predetermined JSON\nINSERT SHORTTITLE COMMAND IN PREAMBLE 10\nformat. For a clearer understanding, readers are referred to Table 2, which elucidates the\nexample prompt and the subsequent model response. Post-extraction, the outputs were not\nimmediately cataloged. A filtering process was initiated to ascertain the standardization of\nthe concept pairs. This process weeded out suboptimal outputs. Aiding in this quality\ncontrol, GPT-4 played a pivotal role in the verification of causal pairs, determining their\nrelevance, causality, and ensuring the correct directionality. Lastly, while extracting\nknowledge, we were aware of the constraints imposed by the GPT-4 API. There was a\nconscious effort to ensure that we operated within the bounds of 60 requests and 150k\ntokens per minute. This intricate dance between prompt engineering and stringent filtering\nbore fruit. We were successful in gleaning high-quality causal concept pairs from the texts,\nthus enriching our knowledge repository.\nGraph database storage. Our decision to employ Neo4j as the database system\nwas strategic. Neo4j, as a graph database (Thomer & Wickett, 2020), is inherently\ndesigned to capture and represent complex relationships between data points, an attribute\nessential to understanding intricate causal relationships. Beyond its technical prowess,\nNeo4j provides advantages such as scalability, resilience, and efficient querying capabilities\n(Webber, 2012). It is particularly adept at traversing interconnected data points, making it\nan excellent fit for our causal relationship analysis. The mined causal knowledge finds its\nabode in the Neo4j graph database. Each pair of causal concepts is represented as a node,\nwith its directionality and interpretations stored as attributes. Relationships weave related\nconcepts together. Storing the knowledge graph in Neo4j allows for the execution of graph\nalgorithms to analyze concept interconnectivity and unearth potential relationships.\nThe graph database contains 197K concepts and 235K connections. Table 3\nencapsulates the core concepts and provides a vivid snapshot of the most recurring themes,\nhelping us to understand the central topics that dominate the current psychological\ndiscourse. Through a comprehensive examination of the core concepts extracted from\n43,312 psychological papers, several distinct patterns and focal areas emerged. In\nINSERT SHORTTITLE COMMAND IN PREAMBLE 11\nTable 2\nAn example prompt and response.\nPrompt:\nFrom the \"text\" below, extract the key causal and correlational relationships described directly in the\ngiven text by analysing reasoning and evidence within the text. Exclude any relationships that are\nattributed to or cited from other research studies.\nFormat the relationships in JSON format with the following fields:\n‘concept_pair’: A list representation of the cause and effect concepts in the\nrelationship, in [cause, effect] order.\n‘relationship’: ‘causality’ or ’correlation’ indicating the type of relationship.\n‘positive/negative’: If the extracted relationship is causality, indicate\nwhether it’s a positive or negative causality relationship. If it’s a correlation\nrelationship, reply as None.\n{\n\"PMC8451848\",\n‘concept_pair’: [openness to change values, well-being],\n‘relationship’:\"causality\",\n‘positive/negative’:\"positive\"\n},\n{\n\"PMC6085571\",\n‘concept_pair’: [cognitive reappraisal, Psychological well-being],\n‘relationship’:\"causality\",\n‘positive/negative’:\"positive\"\n}\nparticular, there is a clear balance between health and illness in psychological research.\nThe prominence of terms such as ‘depression’, ‘anxiety’, and ‘symptoms of depression\nmagnifies the discipline’s commitment to understanding and addressing mental illnesses.\nYet, juxtaposed against these are positive terms like ‘life satisfaction’ and ‘sense of\nhappiness’, suggesting that psychology doesn’t just fixate on challenges but also delves\ndeeply into the nuances of positivity and well-being. Furthermore, the significance given to\nconcepts like ‘life satisfaction’, ‘sense of happiness’, and ‘job satisfaction’ underscores an\nincreasing recognition of emotional well-being and job satisfaction as integral to overall\nINSERT SHORTTITLE COMMAND IN PREAMBLE 12\nmental health. Intertwining the realms of psychology and neuroscience, terms such as\n‘microglial cell activation’, ‘cognitive impairment’, and ‘neurodegenerative changes’ signal a\nburgeoning interest in understanding the neural underpinnings of cognitive and\npsychological phenomena. The exploration does not stop there. The emphasis on\n‘self-efficacy’, ‘positive emotions’, and ‘self-esteem’ reflects the profound interest in\nunderstanding how self-perception and emotions influence human behavior and well-being.\nConcepts such as ‘age’, ‘resilience’, and ‘creativity’ further expand the canvas, showcasing\nthe eclectic and comprehensive nature of inquiries in the field of psychology.\nAll in all, this analysis paints a vivid picture of modern psychological research,\nilluminating its multidimensional approach. It demonstrates a discipline deeply engaged\nwith both the challenges and triumphs of human existence, offering a holistic insight into\nthe human mind and its myriad complexities.\nHypothesis Generation using Link Prediction\nIn the quest to uncover novel causal relationships beyond direct extraction from texts,\nthe technique of link prediction emerges as a pivotal methodology. It hinges on the premise\nof proposing potential causal ties between concepts that our knowledge graph does not\nexplicitly connect. The process intricately weaves together Vector Embedding, Similarity\nAnalysis, and Probability-Based Ranking. At the beginning, concepts are transposed into a\nvector space using node2vec, revered for its ability to capture topological nuances. From\nhere, every pair of unconnected concepts gets assigned a similarity score, and pairs that do\nnot meet a set benchmark are quickly discarded. As we dive deeper into the higher echelons\nof these scored pairs, the likelihood of their linkage is gauged using the Jaccard similarity\nof their neighboring concepts. Subsequently, these potential causal relationships are\norganized in descending order of their derived probabilities, and the elite pairs are selected.\nIllustrative of this approach is the case highlighted in Figure A1. Here, the\nBehavioral Inhibition System (BIS) exhibits ties to both the Behavioral Activation System\nINSERT SHORTTITLE COMMAND IN PREAMBLE 13\nTable 3\nCore Concepts in 43,312 Psychological Papers.\nNumber Concepts Degree(in)\n1 Depression 2,002\n2 Anxiety 1,606\n3 Life satisfaction 890\n4 Well-being 874\n5 Performance 834\n6 Depressive symptoms 812\n7 Mental health 764\n8 Microglial activation 734\n9 Accuracy 720\n10 Psychological distress 631\n11 Job satisfaction 623\n12 Cognitive impairment 603\n13 Neurodegeneration 597\n14 Stress 557\n15 Self-efficacy 549\n16 Neuroinflammation 541\n17 Oxidative stress 536\n18 Age 533\n19 Neuroprotection 505\n20 Resilience 492\n(BAS) and the subsequent behavioral response of BAS when encountering reward stimuli,\ntermed the BAS reward response. Simultaneously, another concept, Interference, finds\nitself bound to both BAS and BAS Reward Response. This configuration hints at a\nplausible link between BIS and interference. Such highly probable causal pairs are not\nmere intellectual curiosities. They act as springboards, catalyzing the genesis of new\nexperimental designs or research hypotheses ripe for empirical probing. In essence, this\ncapability equips researchers with a cutting-edge instrument, empowering them to navigate\nthe unexplored waters of the psychological and neurological domains.\nUsing pairs of highly probable causal concepts, we push GPT-4 to conjure novel\ncausal hypotheses that bridge concepts. To further elucidate the prowess of this method,\nINSERT SHORTTITLE COMMAND IN PREAMBLE 14\nTable 4\nExample hypotheses for causal relationships.\nConcept 1 Concept 2 Hypothesis\nMicrobiome diversity Well-being Pandemic flourishing : Some individuals may experi-\nence a sense of ‘flourishing’ or thriving during pan-\ndemic events despite the widespread stress and adver-\nsity.\nDivergent thinking exercises Well-being Divergent thinking exercises can expand one’s sense of\nself and purpose, which then synergistically improve\nwell-being through an upward spiral. Engaging in cre-\native thinking exercises can broaden one’s perspectives\non identity and meaning in life. This expanded sense\nof self and purpose may then mutually reinforce each\nother and spiral upward to enhance well-being.\nOnline social connectivity Well-being Virtual resilience: Online social connectivity and ac-\ncess to well-being resources can build ‘virtual re-\nsilience’ and enhance well-being during stressful events\nlike pandemics.\nSense of shared purpose and be-\nlonging\nWell-being A sense of shared purpose and belonging within your\nsocial groups is necessary for freedom, choice and self-\ndetermination to enhance well-being.\nconsider Table 4 which provides some examples of hypothesis generated from the process.\nSuch hypotheses, as exemplified in the last row, underscore the potential and power of our\nmethod in generating innovative causal propositions.\nHypotheses Evaluation and Results\nIn this section, we present an analysis focusing on quality in terms of novelty and\nusefulness of the hypotheses generated. According to the literature, these dimensions are\ninstrumental in encapsulating the essence of inventive ideas (Boden, 2009; McCarthy,\nChen, & McNamee, 2018; Miron-Spektor & Beenen, 2015). These parameters have not\nINSERT SHORTTITLE COMMAND IN PREAMBLE 15\nonly been quintessential for gauging creative concepts, but they have also been adopted to\nevaluate the caliber of research hypotheses (Dowling & Lucey, 2023; Krenn & Zeilinger,\n2020; Oleinik, 2019). Specifically, we evaluate the quality of the hypotheses generated by\nthe proposed LLMCG algorithm in relation to those generated by PhD. students from an\nelite university who represent for human junior experts, the LLM model which represents\nadvanced AI systems, and the research ideas refined by psychological researchers which\nrepresents cooperation between AI and human.\nThe evaluation comprises three main stages. In the first stage, hypotheses are\ngenerated by all contributors, including steps taken to ensure fairness and relevance for\ncomparative analysis. In the second stage, the hypotheses from the first stage are\nindependently reviewed blindly by experts who represent the human academic community.\nThese experts are asked to provide hypotheses ratings using a process deliberately designed\nto enable statistically meaningful comparisons. The third stage delves deeper by\ntransforming each research idea into the semantic space of BERT (Bidirectional Encoder\nRepresentations from Transformers) (Lee et al., 2023), allowing us to intricately analyze\nthe intrinsic reasons behind the rating disparities amongst the groups. This semantic\nmapping not only pinpoints the nuanced differences, but also illuminates potential insights\ninto the cognitive constructs of each hypothesis.\nEvaluation Procedure\nSelection of focus area for hypothesis generation.Selecting an appropriate\nfocus area for hypothesis generation is paramount to ensuring a balanced and insightful\ncomparison of the hypothesis generation capacities between various contributors. In this\nstudy, our goal is to gauge the quality of hypotheses derived from four distinct\ncontributors, with measures in place to mitigate potential confounding variables that might\nskew the results among groups (Rubin, 2005). Our choice of domain is informed by two\npivotal criteria: the intricacy and subtlety of the subject matter and familiarity with the\nINSERT SHORTTITLE COMMAND IN PREAMBLE 16\ndomain. It’s essential that our chosen domain boasts sufficient complexity to spur\nmeaningful hypothesis generation and offer a robust assessment of both AI and human\ncontributors’ depth of understanding and creativity. Furthermore, while human\ncontributors should be well-acquainted with the domain, their expertise need not match\nthe vast corpus knowledge of the AI.\nIn terms of overarching human pursuits such as the search for happiness, positive\npsychology distinguishes itself by avoiding narrowly defined, individual-centric challenges\n(Seligman & Csikszentmihalyi, 2000). This resonance with our selection criteria is\nepitomized by well-being, a salient concept within positive psychology, as shown in Table 3.\nWell-being, with its multi-dimensional essence encompassing emotional, psychological, and\nsocial facets, and its central stature in both research and practical applications of positive\npsychology (Diener et al., 2010; Fredrickson, 2001; Seligman & Csikszentmihalyi, 2000),\nbecomes the linchpin of our evaluation. The growing importance of well-being in the\ncurrent global context offers a myriad of novel avenues ripe for hypothesis generation and\ntheoretical advancement (Forgeard, Jayawickreme, Kern, & Seligman, 2011; Madill et al.,\n2022; Otu, Charles, & Yaya, 2020). Adding to our rationale, the Positive Psychology\nResearch Center at Tsinghua University is a globally renowned hub for cutting-edge\nresearch in this domain. Leveraging this stature, we secured participation from specialized\nPhD students, fortifying positive psychology as the most fitting domain for our inquiry.\nHypotheses comparison. In our study, the generated psychological hypotheses\nwere categorized into four distinct groups, consisting of two experimental groups and two\ncontrol groups. The experimental groups encapsulate hypotheses generated by our\nalgorithm, either through random selection or handpicked by experts from a pool of\ngenerated hypotheses. On the other hand, control groups comprise research ideas\nmeticulously crafted by doctoral students with substantial academic expertise in the\ndomain and hypotheses generated by representative LLM. Below, we elucidate the\nmethodology and underlying rationale for each group:\nINSERT SHORTTITLE COMMAND IN PREAMBLE 17\nLLMCG algorithm output (Random-selected LLMCG): Adhering to the requirement\nto generate hypotheses centered on well-being, the LLMCG algorithm crafted 130 unique\nhypotheses. These hypotheses were derived by LLMCG’s evaluation as the most probable\ncausal relationships related to well-being that had not been previously documented in past\nresearch literature data-set. From this refined pool, 30 research ideas were chosen at\nrandom for this experimental group. These hypotheses represent the algorithm’s ability to\nidentify causal relationships and to formulate pertinent hypotheses.\nLLMCG expert-vetted hypotheses (Expert-selected LLMCG): For this group, two\nseasoned psychological researchers, one male aged 47 and one female aged 46, both with\nprofound expertise in the realm of Positive Psychology, conscientiously handpicked 30 of\nthe most promising hypotheses from the refined pool, excluding those from the\nRandom-selected LLMCG category. Their selection criteria centered on a holistic\nunderstanding of both the novelty and practical relevance of each hypothesis. With an\nillustrious post-doctoral journey and a robust portfolio of publications in positive\npsychology to their names, they rigorously sifted through the hypotheses, pinpointing those\nthat showcased a perfect confluence of originality and actionable insight. These hypotheses\nhave been meticulously appraised for their relevance, structural coherence, and potential\nacademic value, representing a nexus of machine intelligence and seasoned human\ndiscernment.\nPhD students’ output (Control-Human): We enlisted the expertise of 16 doctoral\nstudents from the Positive Psychology Research Center at Tsinghua University. Under the\nguidance of their supervisor, each student was provided with a questionnaire geared toward\nresearch on well-being. They were given a period of four working days to complete and\nreturn the questionnaire, which was distributed during their vacation to ensure minimal\nexternal disruptions and commitments. The specific instructions provided in the\nquestionnaire are detailed in Table B1, and each participant is asked to propose 3-4\nresearch hypotheses. By the stipulated deadline, we received responses from 13 doctoral\nINSERT SHORTTITLE COMMAND IN PREAMBLE 18\nstudents, with a mean age of 31.92 years (SD = 7.75 years), cumulatively presenting 41\nhypotheses related to well-being. To maintain uniformity with the other groups, a random\nselection was made to shortlist 30 hypotheses for further analysis. These hypotheses reflect\nthe integration of core theoretical concepts with the latest insights into the domain,\npresenting an academic interpretation rooted in their rigorous training and education.\nIncluding this group in our study not only provides a natural benchmark for human\ningenuity and expertise but also underscores the invaluable contribution of human\ncognition in research ideation, serving as a pivotal contrast to AI-generated hypotheses.\nThis juxtaposition illuminates the nuanced differences between human intellectual depth\nand AI’s analytical prowess, enriching the comparative dimensions of our study.\nClaude model output (Control-Claude): This group exemplifies the pinnacle of current\nLLM technology in generating research hypotheses. Since LLMCG is a nascent technology,\nits assessment requires a comparative study with well-established counterparts, creating a\nkey paradigm in comparative research. Currently, Claude-2 and GPT-4 represent the apex\nof AI technology. For example, Claude-2, with an accuracy rate of 54. 4% excels in\nreasoning and answering questions, substantially outperforming other models such as\nFalcon, Koala and Vicuna, which have accuracy rates of 17.1%-25.5% (Wu et al., 2023). To\nfacilitate a more comprehensive evaluation of the new model by researchers and to increase\ndiversity and breadth of comparison, we have chosen Claude-2 as the control model. Using\nthe detailed instructions provided in Table B2, Claude-2 was iteratively prompted to\ngenerate research hypotheses, generating ten hypotheses per prompt, culminating in a total\nof 50 hypotheses. Although the sheer number and the range of these hypotheses accentuate\nthe capabilities of Claude-2, to ensure compatibility in terms of complexity and depth\nbetween all groups, a subsequent refinement was considered essential. With minimal\nhuman intervention, GPT-4 was used to evaluate these 50 hypotheses and select the top 30\nthat exhibited the most innovative, relevant, and academically valuable insights. This\nprocess ensured the infusion of both LLM’s analytical prowess and a layer of qualitative\nINSERT SHORTTITLE COMMAND IN PREAMBLE 19\nrigor, thus giving rise to a set of hypotheses that not only align with the overarching theme\nof well-being but also resonate with current academic discourse.\nHypotheses assessment. The assessment of hypotheses encompasses two key\ncomponents: the evaluation conducted by eminent psychology professors emphasizing\nnovelty and utility, and the deep semantic analysis involving BERT andt-SNE\n(t-distributed Stochastic Neighbor Embedding) visualization to discern semantic structures\nand disparities among hypotheses.\nHuman academic community.The echelon of review was entrusted to three eminent\npsychology professors (all male, mean age = 42.33), who have a decade-long legacy in\nguiding doctoral and master’s students in positive psychology and editorial stints in\nrenowned journals; their task was to conduct a meticulous evaluation of the 120\ncream-of-the-crop hypotheses. Importantly, to ensure unbiased evaluation, the hypotheses\nwere presented to them in a completely randomized order in the questionnaire.\nOur emphasis was undeniably anchored to two primary tenets: novelty and utility\n(Cohen, 2017; Shardlow et al., 2018; Thompson & Skau, 2023; Yu, Peng, Peng, Zheng, &\nLiu, 2016), as elaborated in Table B3. Utility in hypothesis crafting demands that our\npropositions extend beyond mere factual accuracy; they must resonate deeply with\nacademic investigations, ensuring substantial practical implications. Given the inherent\nchallenges of research, marked by constraints in time, manpower, and funding, it is essential\nto design hypotheses that optimize the utilization of these resources. On the front of\nnovelty, we strive to introduce innovative perspectives that have the power to challenge and\nexpand upon existing academic theories. This not only propels the discipline forward but\nalso ensures we don’t inadvertently tread on ground already covered by our contemporaries.\nDeep sematic analysis.While human evaluations provide invaluable insight into the\nnovelty and utility of hypotheses, to objectively discern and visualize the semantic\nstructures and disparities among them, we turn to the realm of deep learning. Specifically,\nwe employ the power of BERT (Devlin, Chang, Lee, & Toutanova, 2018). BERT, as\nINSERT SHORTTITLE COMMAND IN PREAMBLE 20\nhighlighted by (Lee et al., 2023), had remarkable potential to assess innovation of ideas.\nBy translating each hypothesis into a high-dimensional vector in BERT’s domain, we grasp\nthe profound semantic core of each statement. Yet, such granularity in dimensions presents\nchallenges when aiming for visualization.\nTo alleviate this and to intuitively understand the clustering and dispersion of these\nhypotheses in semantic space, we deploy thet-SNE (t-distributed Stochastic Neighbor\nEmbedding) technique (Van der Maaten & Hinton, 2008). Thet-SNE is adept at reducing\nthe dimensionality of the data while preserving the relative pairwise distances between the\nitems. Thus, when we map our BERT-encoded hypotheses onto a 2Dt-SNE plane, it gives\nus an immediate visual grasp on how closely or distantly related our hypotheses are in\nterms of their semantic content. Our intent is twofold: to fathom the semantic terrains\ncarved out by the different groups and to infer the potential reasons some hypotheses\ngarnered heightened novelty or utility ratings from the experts. The convergence of human\nevaluations and semantic layouts, as delineated by Algorithm 1, fathom the interplay\nbetween human intuition and the inherent semantic structure of the hypotheses.\nResults\nQualitative analysis by topic analysis.To better understand the underlying\nthought processes and the topical emphasis of both PhD students and the LLMCG model,\nqualitative analyzes were performed using visual tools such as word clouds and connection\ngraphs, as detailed in the Appendix B. The word cloud, as a graphical representation,\neffectively captures the frequency and importance of terms, providing a direct visualization\nof the dominant themes. Connection graphs, on the other hand, elucidate the relationships\nand interplay between various themes and concepts. Using these visual tools, we aimed to\nachieve a more intuitive and clear representation of the data, allowing for easy comparison\nand interpretation.\nObservations drawn from both the word clouds and the connection graphs in Figures\nINSERT SHORTTITLE COMMAND IN PREAMBLE 21\nB1 and B2 provide us with a rich tapestry of insights into the thought processes and\npriorities of PhD students and the LLMCG model. For instance, the emphasis in the\nControl-Human word cloud on terms such as ‘robot’ and ‘AI’ indicates a keen interest\namong PhD students in the nexus between technology and psychology. It is particularly\nfascinating to see a group of academically trained individuals focusing on the real-world\nimplications and intersections of their studies, as shown by their apparent draw towards\ntrending topics. This not only underscores their adaptability but also emphasizes the\nimportance of contextual relevance. On the flip side, the LLMCG models, particularly the\nExpert-selected LLMCG, lay heavy emphasis on community, collective experiences, and the\nnuances of social interconnectedness. This denotes a deep-rooted understanding and\napplication of higher-order social psychological concepts, reflecting the model’s ability to\ndive deep into the intricate layers of human social behavior.\nFurther, the connection graphs corroborate these observations. TheControl-Human\ngraph, with its exploration of themes like ‘Robot Companionship’ and its relation to\nfactors like ‘Heart rate variability (HRV)’, demonstrates a confluence of technology and\nhuman well-being. The other models, especially theRandom-selected LLMCG, bring forth\nthemes which are more societal and structural, hinting at broader determinants of\nindividual well-being.\nAnalysis on human evaluations.We have divided our assessment into two main\nsections: Novelty Analysis and Usefulness Analysis.\nNovelty analysis: In the dynamic realm of scientific research, measuring and\nanalyzing novelty is gaining paramount importance (Shin, Kim, & Kogler, 2022). Using the\nANOVA method to analyze the novelty scores represented in Figure 2a, we identified a\nsignificant influence of the group factor on the mean novelty score among different\nreviewers. The clear distinctions between the groups, as visualized in the boxplots, are\nstatistically underpinned by the results in Table 5. The results of the ANOVA illuminated\na pronounced effect of the grouping factor (F(3,116)=6.92, p = 0.0002), with the variance\nINSERT SHORTTITLE COMMAND IN PREAMBLE 22\n(a) Novelty\n(b) Usefulness\nFigure 2. Comparative analysis between groups: Box plots on the left (a) and (b) depict\ndistributions of novelty and usefulness scores, respectively, while smoothed line plots on the\nright demonstrate the descending order of novelty and usefulness scores and subjected to a\nmoving average with a window size of 2 .Note: denotes p < 0.05, denotes p < 0.01.\nexplained by the grouping factor (R-squared) of 15. 19%.\nDelving deeper with pairwise comparisons using the Bonferroni method, as delineated\nin Table 5 and visually corroborated by Figure 2a, significant disparities were discerned\nbetween Random-selected LLMCGand Control-Claude (t(59) = 3.34,p=0.007) and\nbetween Control-Human and Control-Claude (t(59) = 4.32,p<0.001). Importantly, when\nconsidering the cumulative distribution plots to the right of Figure 2a, we observe the\ndistributional characteristics of the novelty scores. For example, it can be observed that\nINSERT SHORTTITLE COMMAND IN PREAMBLE 23\nTable 5\nBonferroni post-hoc tests for pairwise comparisons of novelty scores across different groups.\nComparison Contrast t-value P-value (Bonferroni)\nMean Value\nControl-Claude vs. Control-Human 0.7148 4.36 < 0.001\nControl-Claude vs. Random-selected LLMCG 0.5478 3.34 0.007\nControl-Claude vs. Expert-selected LLMCG 0.4088 2.49 0.085\nMedian Value\nControl-Claude vs. Control-Human 0.7815 4.36 < 0.001\nControl-Claude vs. Random-selected LLMCG 0.6231 3.47 0.004\nControl-Claude vs. Expert-selected LLMCG 0.5596 3.12 0.014\nMax Value\nControl-Claude vs. Control-Human 0.7688 4.01 < 0.001\nControl-Claude vs. Random-selected LLMCG 0.6515 3.40 0.006\nControl-Claude vs. Expert-selected LLMCG 0.3264 1.70 0.550\nthe Expert-selected LLMCGcurve portrays a larger concentration in the middle score range\nwhen compared toControl-Claude, while dominates in high novelty scores (highlighted in\ndashed rectangle). Moreover, comparisons involvingControl-Human with both\nRandom-selected LLMCGand Expert-selected LLMCGdid not manifest statistically\nsignificant variances, indicating aligned novelty perceptions among these groups. Lastly,\nthe comparisons betweenExpert-selected LLMCGand Control-Claude (t(59) = 2.49,\np=0.085) suggest a trend toward significance, underscoring the nuanced differences between\nthese groups in novelty dimension.\nTo mitigate potential biases due to individual reviewer inclinations, we expanded our\nevaluation to encompass both median and maximum values. These multifaceted analyses\nenhance the robustness of our results by minimizing the influence of extreme values and\npotential outliers. First, in analyzing the median novelty scores, the ANOVA test\nINSERT SHORTTITLE COMMAND IN PREAMBLE 24\ndemonstrated a notable association with the grouping factor (F(3,116)=6.54, p=0.0004),\naccounting for an explained variance of 14.41%. As illustrated in Table 5, pairwise\nevaluations revealed significant disparities betweenControl-Human and Control-Claude\n(t(59) = 4.01,p=0.001) as well as betweenRandom-selected LLMCGand Control-Claude\n(t(59) = 3.40,p=0.006). Interestingly, the comparison ofExpert-selected LLMCGwith\nControl-Claude (t(59) = 1.70,p=0.550) and other group pairings did not indicate\nstatistically significant differences.\nSubsequently, turning our attention to maximum novelty scores provided crucial\ninsights, especially where outlier scores may carry significant weight. The influence of the\ngrouping factor was evident (F(3,116)=7.20, p=0.0002), dictating an explained variance of\n15.70%. In particular, clear differences emerged betweenControl-Human and\nControl-Claude (t(59) = 4.36,p<0.001), and betweenRandom-selected LLMCGand\nControl-Claude (t(59) = 3.47,p=0.004). A particularly intriguing observation was the\nsignificant difference betweenExpert-selected LLMCGand Control-Claude (t(59) = 3.12,\np=0.014). Together, these analyses offer a multifaceted perspective on novelty evaluations.\nSpecifically, the results of the median analysis echo and support those of the mean,\nreinforcing the reliability of our assessments. The discerned significance between\nControl-Claude and Expert-selected LLMCGin the median data emphasizes the intricate\ndifferences, while also pointing to a broader congruence in novelty perceptions.\nUsefulness analysis.Determining the perceived utility of contributions is a\ncornerstone in evaluating innovative ventures. In the mean useful spectrum, the grouping\nfactor did not exert a significant influence (F(3,116)=5.25, p = 0.553). Figure 2b vividly\npresents the utility score distributions between groups. The tight interquartile range of\nControl-Human suggests a relatively consistent assessment among reviewers. On the other\nhand, the spread and outliers in theControl-Claude distribution hint at varied utility\nperceptions. Both Random-selected LLMCGcover a broad score range, demonstrating a\nmixture of high and low utility scores, while theExpert-selected LLMCGgravitates more\nINSERT SHORTTITLE COMMAND IN PREAMBLE 25\ntowards higher usefulness scores. The smoothed line plots accompanying Figure 2b further\ndetail the score densities. For instance,Random-selected LLMCGboasts several high\nutility scores, counterbalanced by a smattering of low scores. Interestingly, the\ndistributions forControl-Human and Expert-selected LLMCGappear to be closely aligned.\nWhile mean utility scores provide an overarching view, the nuances within the boxplots\nand smoothed plots offer deeper insights. This comprehensive understanding can guide\nfuture endeavors in content generation and evaluation, spotlighting key areas of focus and\npotential improvements.\nDeep semantic analysis. The t-SNE visualizations (Figure 4) illustrate the\nsemantic relationships between different groups, capturing the patterns of novelty and\nusefulness. Notably, a distinct clustering among PhD students suggests shared academic\ninfluences, while the LLMCG groups display broader topic dispersion, hinting at a wider\nsemantic understanding. The size of the bubbles reflects the novelty and usefulness scores,\nemphasizing the diverse perceptions of what’s considered innovative versus beneficial.\nAdditionally, the numbers near the yellow dots represent participant IDs, which\ndemonstrates that the semantics for the same participant, such as H05 or H06, are closely\naligned.\nIn the BERT space representation (Figure 3), we observed that the Control-Human\ngroup exhibits a distinctively higher semantic distance in comparison to other groups,\nemphasizing their unique semantic orientations. The statistical support for this observation\nis derived from the ANOVA results, with a significant F-statistic (F(3,1652)=84.1611, p <\n0.00001) underscoring the impact of the grouping factor. This factor explains a remarkable\n86.96% of the variance, as indicated by theR-squared value. Multiple comparisons, as\nshown in Table 6, further dive into the nuances of these group differences.Control-Human\nand Control-Claude exhibit a significant contrast in their semantic distances, as highlighted\nby thet-value of 16.41 and the adjustedp-value (< 0.0001). This difference indicates\ndistinct thought patterns or emphasis in the two groups. Comparing Control-Human with\nINSERT SHORTTITLE COMMAND IN PREAMBLE 26\n(a) Novelty\n(b) Usefulness\nFigure 3. The t-SNE visualization of semantic representations: Comparison of novelty and\nusefulness scores among different groups.\nthe LLMCG models shows divergent semantic orientations. The group has statistically\nsignificant differences withRandom-selected LLMCG(p=0.0036) and a trend towards\ndifference withExpert-selected LLMCG(p=0.0687). Comparison between the\nControl-Claude and LLMCG models reveals pronounced differences, more so with the\nExpert-selected LLMCG(p<0.0001). Intriguingly, the two LLMCG models -\nRandom-selected and Expert-selected - exhibit similar semantic distances, as evidenced by\na non-significantp-value of 0.4362. Furthermore, the significant distinctions we observed,\nINSERT SHORTTITLE COMMAND IN PREAMBLE 27\nFigure 4. Distribution of semantic distances among different groups in BERT space.\nNote: denotes p < 0.01, denotes p < 0.0001.\nparticularly between control-human and other groups, align with human evaluations of\nnovelty. This coherence indicates that the BERT space representation coupled with\nstatistical analyses could effectively mimic human judgment. Such results underscore the\npotential of this approach for automated hypothesis testing, paving the way for more\nefficient and streamlined semantic evaluations in the future.\nIn general, visual and statistical analyses reveal the nuanced semantic landscapes of\neach group. While the PhD students’ shared background influences their clustering, the\nmachine models exhibit a comprehensive grasp of topics, emphasizing the intricate\ninterplay of individual experiences, academic influences, and algorithmic understanding in\nshaping semantic representations.\nThis investigation carried out a detailed evaluation of the hypotheses of various\ncontributors, blending both quantitative and qualitative analyzes. In terms of topic\nanalysis, distinct variations were observed betweenControl-Human and LLMCG, the latter\npresenting more expansive thematic coverage. For human evaluation, hypotheses from PhD\nstudents paralleled the LLMCG in novelty, reinforcing AI’s growing competence in\nINSERT SHORTTITLE COMMAND IN PREAMBLE 28\nTable 6\nBonferroni post-hoc tests for pairwise comparisons of deep semantic distance across\ndifferent groups.\nComparison Contrast t-value P-value (Bonferroni)\nControl-Human vs. Control-Claude 2.6949 16.41 < 0.0001\nControl-Human vs. Random-selected LLMCG 0.5773 2.91 0.0036\nControl-Human vs. Expert-selected LLMCG 0.4078 1.82 0.0687\nControl-Claude vs. Random-selected LLMCG 2.1175 13.45 < 0.0001\nControl-Claude vs. Expert-selected LLMCG 2.2870 12.72 < 0.0001\nRandom-selected LLMCG vs. Expert-selected LLMCG 0.1695 0.78 0.4362\nmirroring human innovative thinking. Furthermore, when juxtaposed with AI models such\nas Control-Claude, LLMCG exhibited increased novelty. Deep semantic analysis viat-SNE\nand BERT representations allowed an intuitive grasp of the hypotheses’ semantic essence,\nsignaling the possibility of future automated hypothesis assessments. Interestingly,\nLLMCG appeared to encompass broader complementary domains compared to human\ninput. Altogether, this segment accentuates AI’s burgeoning role in hypothesis generation\nand provides key insights into hypothesis evaluation across diverse origins.\nGeneral Discussion\nThis research delves into the synergistic relationship between LLM and causal graph\nin the hypothesis generation process. Our findings underscore the capability of LLM, when\nintegrated with causal graph techniques, to produce meaningful hypotheses with increased\nefficiency and quality. By centering our investigation on ‘well-being’ we emphasize its\npivotal role in psychological studies and highlight the potential convergence of technology\nand society. The adoption of a multifaceted assessment approach to evaluate the quality by\ntopic analysis, human evaluation and deep semantic analysis, that AI-augmented methods\noutshine LLM-only techniques in generating hypotheses with superior novelty, and a\nINSERT SHORTTITLE COMMAND IN PREAMBLE 29\nquality on par with human expertise but also boast the capability for more profound\nconceptual incorporations and a broader semantic spectrum. Such a multifaceted lens of\nassessment introduces a novel perspective for the scholarly realm, equipping researchers\nwith an enriched understanding and an innovative toolset for hypothesis generation. At its\ncore, the melding of LLM and causal graphs signals a promising frontier, especially when it\ncomes to dissecting cornerstone psychological constructs like ‘well-being’. This marriage of\nmethodologies, enriched by the comprehensive assessment angle, deepens our\ncomprehension of both the immediate and broader ramifications of our research endeavors.\nCausal graph’s prominence in psychology is profound, offering researchers a unified\nplatform to synthesize and hypothesize across diverse psychological realms(Borsboom et\nal., 2021; Uleman et al., 2021). Our study echoes this, producing groundbreaking\nhypotheses comparable in depth to early expert propositions. Deep semantic analysis\nbolstered these findings, emphasizing that our hypotheses have distinct cross-disciplinary\nmerits, particularly when compared to those of individual doctoral scholars. However, the\ntraditional use of causal graphs in psychology presents challenges due to its demanding\nnature, often requiring insights from multiple experts (Crielaard et al., 2022). Our\nresearch, however, harnesses LLM’s causal extraction, automating causal pair derivation\nand, in turn, minimizing the need for extensive expert input. The union of causal graph’s\nsystematic approach with AI-driven creativity, as seen with LLM, paves the way for the\nfuture in psychological inquiry. Barriers once posed by causal graph’s intricate procedures\nare being dismantled, courtesy of advancements in AI. Furthermore, as the era of big data\ndawns, the integration of AI and causal graph in psychology not only augments research\ncapabilities, but also brings into focus the broader implications on society. This fusion\nprovides a nuanced understanding of the intricate sociopsychological dynamics,\nemphasizing the importance of adapting research methodologies in tandem with\ntechnological progress.\nIn the realm of research, LLM serves a unique purpose, often acting as the foundation\nINSERT SHORTTITLE COMMAND IN PREAMBLE 30\nor baseline against which newer methods and approaches are assessed. The demonstrated\nproductivity enhancements by generative AI tools, as evidenced by (Noy & Zhang, 2023),\nindicate the potential of such LLMs. In our investigation, we pit the hypotheses generated\nby such substantial models against our integrated LLMCG approach. Intriguingly, while\nthese LLMs showcased admirable practicality in their hypotheses, they markedly trailed in\nterms of innovation when juxtaposed with the doctoral student and LLMCG group. This\ndivergence in results can be attributed to the causal network curated from 43k research\npapers, funneling the vast knowledge reservoir of the LLM squarely into the realm of\nscientific psychology. The increased precision in hypothesis generation by these models is\nquite coherent within the framework of generative networks. As Tong et al. (2021)\nhighlighted, by integrating structured constraints, conventional neural networks can more\naccurately generate semantically relevant content. One of the salient merits of causal\ngraph, in this context, is its ability to alleviate the inherent ambiguity or interpretative\nchallenges posed by LLM. By providing a systematic and structured framework, the causal\ngraph aids in unearthing the underlying logic and rationale of the outputs generated by\nLLMs. Notably, this echoes the perspective Pan et al. (2023), where the integration of\nstructured knowledge from knowledge graphs was shown to provide an invaluable layer of\nclarity and interpretability to LLMs, especially in complex reasoning tasks. Such structured\napproaches not only boost the confidence of researchers in the hypotheses derived, but also\naugment the transparency and understandability of LLM outputs. In essence, leveraging\ncausal graph may very well herald a new era in model interpretability, serving as a conduit\nto unlock the black box that large models often represent in contemporary research.\nIn the ever-evolving tapestry of research, every advancement invariably comes with\nits unique set of constraints, and our study was no exception. On the technical front, a\npivotal challenge stemmed from the opaque inner workings of GPT. Determining the exact\nmachinations within GPT that lead to the formation of specific causal pairs remains\nelusive, thereby reintroducing the age-old issue of AI’s inherent lack of transparency\nINSERT SHORTTITLE COMMAND IN PREAMBLE 31\n(Buruk, 2023; Cao & Yousefzadeh, 2023). This opacity is magnified in our sparse causal\ngraph, which, while expansive, is occasionally riddled with concepts that, while\nsemantically distinct, converge in meaning. In tangible applications, a careful and\nmeticulous algorithmic evaluation would be imperative to construct an accurate\npsychological conceptual landscape. Delving into psychology, which bridges humanities and\nnatural sciences, it continuously aims to unravel human cognition and behavior\n(Hergenhahn & Henley, 2013). Despite the dominance of traditional methodologies\n(Henrich et al., 2010; Shah et al., 2015), the present data-centric era amplifies the synergy\nof technology and humanities, resonating with Hasok Chang’s vision of enriched science\n(Chang, 2007). This symbiosis is evident when assessing structural holes in social networks\n(Burt, 2004) and viewing novelty as a bridge across these divides (Foster, Shi, & Evans,\n2021). Such perspectives emphasize the importance of thorough algorithmic assessments,\nspotlighting the potential avenues in humanities research, especially when incorporating\nlarge language models for innovative hypothesis crafting and verification.\nFurthermore, our validation process was constrained to 130 hypotheses, yet the\nvastness of our conceptual landscape suggests the possibility of a myriad more. As an\nexemplar, the twenty pivotal psychological concepts highlighted in Table 3 alone could\nspawn an extensive array of hypotheses. However, the validation of these surrounding\nhypotheses would unquestionably lead to a multitude of speculations. A striking\nobservation during our validation was the inconsistency in the evaluations of senior expert\npanels (as shown in Figure B3). This shift underscores a pivotal insight: our integration of\nAI has transitioned the dependency on scarce expert resources from hypothesis generation\nto evaluation. It beckons to a future where rigorous evaluations, ensuring both novelty and\nutility, become a focal point of exploration. The pathway ahead, while promising, demands\na careful synthesis of technological innovation and human expertise to truly unlock the\npotential our study hints at.\nIn conclusion, our research provides a pioneering exploration into the symbiotic fusion\nINSERT SHORTTITLE COMMAND IN PREAMBLE 32\nof LLM, epitomized by GPT, and causal graph within the realm of psychological\nhypothesis generation, especially emphasizing ‘well-being’. Importantly, as highlighted by\n(Cao & Yousefzadeh, 2023), ensuring a synergistic alignment between domain knowledge\nand AI extrapolation is crucial. This synergy serves as the foundation for maintaining AI\nmodels within their conceptual limits, thus bolstering the validity and reliability of the\nhypotheses generated. Our approach intricately interweaves the advanced capabilities of\nLLMs with the methodological prowess of causal graphs, thereby not only optimizing, but\nalso refining the depth and precision of hypothesis generation. The causal graph, while\nparamount in psychology for its cross-disciplinary potential, often demands vast expert\ninvolvement. Our innovative approach addresses this by utilizing LLM’s exceptional causal\nextraction abilities, effectively transitioning the intense expert engagement from hypothesis\ncreation to its evaluation. As the era of big data unfolds, the intertwining of technology\nwith societal constructs becomes ever more significant, and this investigation underscores a\ntransformative shift in that very direction. Championing the crucial convergence of the\nLLM-based causal graph in psychological endeavors, our study is a testament to the\npromise and challenges that lie at the intersection of technological advancements, big data,\nand social nuances. The path ahead, while promising, calls for a judicious fusion of\ncutting-edge technology with human wisdom, ensuring that research remains attuned to\nthe broader implications on society. Our innovative methodology not only advances the\nfield of psychology, but also holds promise for shedding light on the complex dynamics that\ninfluence well-being, thereby contributing to a richer and more nuanced understanding of\nthe factors that affect human quality of life.\nAuthorship Contribution Statement\nSong Tong: Data analysis, Experiments, Writing - original draft & review.Kai\nMao: Designed the causality graph methodology, Generated AI hypotheses, Developed\nhypothesis generation techniques, Writing - review & editing.Zhen Huang: Statistical\nINSERT SHORTTITLE COMMAND IN PREAMBLE 33\nAnalysis, Experiments, Writing - review & editing.Yukun Zhao: Conceptualization,\nProject administration, Supervision, Writing - review & editing.Kaiping Peng:\nConceptualization, Writing - review & editing.\nDeclaration of Competing Interest\nThe author(s) declared no potential conflicts of interest with respect to the research,\nauthorship, and/or publication of this article.\nAcknowledgments\nThe authors thank Dr. Honghong Bai (Radboud University), Dr. ChienTe Wu (The\nUniversity of Tokyo), Dr. Peng Cheng (Tsinghua University), and Yusong Guo (Tsinghua\nUniversity) for their great comments on the earlier version of this manuscript. This\nresearch has been generously funded by personal contributions, with special\nacknowledgment to Kai Mao. Additionally, he conceived and developed the causality graph\nand AI hypothesis generation technology presented in this paper from scratch, and\ngenerated all AI hypotheses and paid for its costs.\nFunding\nThe authors sincerely thank Kai Mao for fully funding the development of hypothesis\ngeneration techniques and providing the generated AI hypotheses that enabled this\nresearch. His generous support and pioneering work in these areas made this collaborative\nproject possible.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 34\nReferences\nBattleday, R. M., Peterson, J. C., & Griffiths, T. L. (2020). Capturing human\ncategorization of natural images by combining deep networks and cognitive models.\nNature Communications, 11(1), 5418.\nBechmann, A., & Bowker, G. C. (2019). Unsupervised by any other name: Hidden layers\nof knowledge production in artificial intelligence on social media.Big Data & Society,\n6(1), 2053951718819569.\nBinz, M., & Schulz, E. (2023). Using cognitive psychology to understand gpt-3.\nProceedings of the National Academy of Sciences, 120(6), e2218523120.\nBoden, M. A. (2009). Computer models of creativity.AI Magazine, 30(3), 23–23.\nBorsboom, D., Deserno, M. K., Rhemtulla, M., Epskamp, S., Fried, E. I., McNally, R. J.,\n... others (2021). Network analysis of multivariate data in psychological science.\nNature Reviews Methods Primers, 1(1), 58.\nBurt, R. S. (2004). Structural holes and good ideas.American Journal of Sociology,\n110(2), 349–399.\nBuruk, O. (2023). Academic writing with gpt-3.5: Reflections on practices, efficacy and\ntransparency. arXiv preprint arXiv:2304.11079.\nCao, X., & Yousefzadeh, R. (2023). Extrapolation and ai transparency: Why machine\nlearning models should reveal when they make decisions beyond their training.Big\nData & Society, 10(1), 20539517231169731.\nChang, H. (2007). Scientific progress: Beyond foundationalism and coherentism1.Royal\nInstitute of Philosophy Supplements, 61, 1–20.\nCheng, K., Guo, Q., He, Y., Lu, Y., Gu, S., & Wu, H. (2023). Exploring the potential of\ngpt-4 in biomedical engineering: the dawn of a new era.Annals of Biomedical\nEngineering, 1–9.\nCichy, R. M., Khosla, A., Pantazis, D., Torralba, A., & Oliva, A. (2016). Comparison of\ndeep neural networks to spatio-temporal cortical dynamics of human visual object\nINSERT SHORTTITLE COMMAND IN PREAMBLE 35\nrecognition reveals hierarchical correspondence.Scientific Reports, 6(1), 27755.\nCohen, B. A. (2017). How should novelty be valued in science?Elife, 6, e28699.\nCrielaard, L., Uleman, J. F., Châtel, B. D., Epskamp, S., Sloot, P., & Quax, R. (2022).\nRefining the causal loop diagram: A tutorial for maximizing the contribution of\ndomain expertise in computational system dynamics modeling.Psychological\nMethods.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep\nbidirectional transformers for language understanding.arXiv preprint\narXiv:1810.04805.\nDiener, E., Wirtz, D., Tov, W., Kim-Prieto, C., Choi, D.-w., Oishi, S., & Biswas-Diener, R.\n(2010). New well-being measures: Short scales to assess flourishing and positive and\nnegative feelings.Social Indicators Research, 97, 143–156.\nDowling, M., & Lucey, B. (2023). Chatgpt for (finance) research: The bananarama\nconjecture. Finance Research Letters, 53, 103662.\nForgeard, M. J., Jayawickreme, E., Kern, M. L., & Seligman, M. E. (2011). Doing the right\nthing: Measuring wellbeing for public policy.International Journal of Wellbeing,\n1(1).\nFoster, J. G., Shi, F., & Evans, J. (2021). Surprise! measuring novelty as expectation\nviolation. SocArXiv.\nFredrickson, B. L. (2001). The role of positive emotions in positive psychology: The\nbroaden-and-build theory of positive emotions.American Psychologist, 56(3), 218.\nGu, Q., Kuwajerwala, A., Morin, S., Jatavallabhula, K. M., Sen, B., Agarwal, A., ...\nothers (2023). Conceptgraphs: Open-vocabulary 3d scene graphs for perception and\nplanning. arXiv preprint arXiv:2309.16650.\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). Most people are not weird.Nature,\n466(7302), 29–29.\nHergenhahn, B. R., & Henley, T. (2013).An introduction to the history of psychology.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 36\nCengage Learning.\nJaccard, J., & Jacoby, J. (2019).Theory construction and model-building skills: A practical\nguide for social scientists. Guilford publications.\nKıcıman, E., Ness, R., Sharma, A., & Tan, C. (2023). Causal reasoning and large language\nmodels: Opening a new frontier for causality.arXiv preprint arXiv:2305.00050.\nKoehler, D. J. (1994). Hypothesis generation and confidence in judgment.Journal of\nExperimental Psychology: Learning, Memory, and Cognition, 20(2), 461.\nKrenn, M., & Zeilinger, A. (2020). Predicting research trends with semantic and neural\nnetworks with an application in quantum physics.Proceedings of the National\nAcademy of Sciences, 117(4), 1910–1916.\nLee, H., Zhou, W., Bai, H., Meng, W., Zeng, T., Peng, K., ... Kumada, T. (2023). Natural\nlanguage processing algorithms for divergent thinking assessment. In2023 ieee 6th\neurasian conference on educational innovation (ecei)(pp. 198–202).\nMadill, A., Shloim, N., Brown, B., Hugh-Jones, S., Plastow, J., & Setiyawati, D. (2022).\nMainstreaming global mental health: Is there potential to embed psychosocial\nwell-being impact in all global challenges research?Applied Psychology: Health and\nWell-Being, 14(4), 1291–1313.\nMcCarthy, M., Chen, C. C., & McNamee, R. C. (2018). Novelty and usefulness trade-off:\nCultural cognitive differences and creative idea evaluation.Journal of Cross-Cultural\nPsychology, 49(2), 171–198.\nMcGuire, W. J. (1973). The yin and yang of progress in social psychology: Seven koan.\nJournal of Personality and Social Psychology, 26(3), 446.\nMiron-Spektor, E., & Beenen, G. (2015). Motivating creativity: The effects of sequential\nand simultaneous learning and performance achievement goals on product novelty\nand usefulness.Organizational Behavior and Human Decision Processes, 127, 53–65.\nNisbett, R. E., Peng, K., Choi, I., & Norenzayan, A. (2001). Culture and systems of\nthought: holistic versus analytic cognition.Psychological Review, 108(2), 291.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 37\nNoy, S., & Zhang, W. (2023). Experimental evidence on the productivity effects of\ngenerative artificial intelligence.Science, 381, 187-192.\nOleinik, A. (2019). What are neural networks not good at? on artificial creativity.Big\nData & Society, 6(1), 2053951719839433.\nOtu, A., Charles, C. H., & Yaya, S. (2020). Mental health and psychosocial well-being\nduring the covid-19 pandemic: The invisible elephant in the room.International\nJournal of Mental Health Systems, 14, 1–5.\nPan, S., Luo, L., Wang, Y., Chen, C., Wang, J., & Wu, X. (2023). Unifying large language\nmodels and knowledge graphs: A roadmap.arXiv preprint arXiv:2306.08302.\nRubin, D. B. (2005). Causal inference using potential outcomes: Design, modeling,\ndecisions. Journal of the American Statistical Association, 100(469), 322–331.\nSanderson, K. (2023). Gpt-4 is here: what scientists think.Nature, 615(7954), 773.\nSeligman, M. E., & Csikszentmihalyi, M. (2000).Positive psychology: An introduction.\n(Vol. 55) (No. 1). American Psychological Association.\nShah, D. V., Cappella, J. N., & Neuman, W. R. (2015). Big data, digital media, and\ncomputational social science: Possibilities and perils.The ANNALS of the American\nAcademy of Political and Social Science, 659(1), 6–13.\nShardlow, M., Batista-Navarro, R., Thompson, P., Nawaz, R., McNaught, J., &\nAnaniadou, S. (2018). Identification of research hypotheses and new knowledge from\nscientific literature.BMC Medical Informatics and Decision Making, 18(1), 1–13.\nShin, H., Kim, K., & Kogler, D. F. (2022). Scientific collaboration, research funding, and\nnovelty in scientific knowledge.PLoS ONE, 17(7), e0271678.\nThomas, R. P., Dougherty, M. R., Sprenger, A. M., & Harbison, J. (2008). Diagnostic\nhypothesis generation and human judgment.Psychological Review, 115(1), 155.\nThomer, A. K., & Wickett, K. M. (2020). Relational data paradigms: What do we learn\nby taking the materiality of databases seriously?Big Data & Society, 7(1),\n2053951720934838.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 38\nThompson, W. H., & Skau, S. (2023). On the scope of scientific hypotheses.Royal Society\nOpen Science, 10(8), 230607.\nTong, S., Liang, X., Kumada, T., & Iwaki, S. (2021). Putative ratios of facial\nattractiveness in a deep neural network.Vision Research, 178, 86–99.\nUleman, J. F., Melis, R. J., Quax, R., van der Zee, E. A., Thijssen, D., Dresler, M., ...\nothers (2021). Mapping the multicausality of alzheimer’s disease through group\nmodel building. GeroScience, 43, 829–843.\nVan der Maaten, L., & Hinton, G. (2008). Visualizing data using t-sne.Journal of\nMachine Learning Research, 9(11).\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ...\nPolosukhin, I. (2017). Attention is all you need. InAdvances in neural information\nprocessing systems.\nWang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., ... others (2023). Scientific\ndiscovery in the age of artificial intelligence.Nature, 620(7972), 47–60.\nWebber, J. (2012). A programmatic introduction to neo4j. InProceedings of the 3rd\nannual conference on systems, programming, and applications: Software for humanity\n(pp. 217–218).\nWilliams, K., Berman, G., & Michalska, S. (2023). Investigating hybridity in artificial\nintelligence research.Big Data & Society, 10(2), 20539517231180577.\nWu, S., Koo, M., Blum, L., Black, A., Kao, L., Scalzo, F., & Kurtz, I. (2023). A\ncomparative study of open-source large language models, gpt-4 and claude 2:\nMultiple-choice test taking in nephrology.arXiv preprint arXiv:2308.04709.\nYu, F., Peng, T., Peng, K., Zheng, S. X., & Liu, Z. (2016). The semantic network model of\ncreativity: Analysis of online social media data.Creativity Research Journal, 28(3),\n268–274.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 39\nAppendix A\nMethod\nArticle Selection & Cost Analysis.\nBefore delving into the specifics of the extraction process, it’s crucial to emphasize\nthe importance of cost analysis, especially when dealing with large-scale data processing.\nIn the realm of data-intensive research, the sheer volume of articles and the intricate nature\nof text extraction entail significant computational and financial resources. Understanding\nthe associated costs is paramount not only for optimal resource allocation but also for\nensuring the scalability and feasibility of the project. With these considerations in mind,\nextracting causal knowledge from texts requires the utilization of language models, such as\nGPT-4, to process each paper. Given the costs associated with API usage, we projected\nthe total expenses for different corpus sizes. Key determinants of the costs include: Token\nCount and API Constraints. Total number of tokens (word segments) across all texts.\nGPT-4 charges per thousand tokens for both inputs and outputs. In addition, GPT-4\ncapped at 60 requests per minute and 150k tokens per minute at the time of this research.\nExtraction procedures must comply with these thresholds. For our curated subset of 140k\narticles, with each abstract at 500 words and main content at 5,000 words, the estimates\nare 40 million tokens for 40k articles. The GPT-4 pricing is approximate to 40,000 USD.\nBased on this analysis, our choice was to extract 43,312 articles, representing around 40\nmillion tokens. This strikes a balance between comprehensive coverage and cost-efficiency.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 40\nFigure A1. The illustration of hypothesis generation by link prediction.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 41\nAppendix B\nResults\nDetails for Topic Analysis\nWord cloud comparison. Figure B1 showcases visual representations of term and\ntheme frequencies for different models and groups. A preliminary analysis suggests:\nControl-Human (a): This word cloud seems to emphasize terms related to individual\nwell-being and psychological health. Notable terms include ‘relationship’, ‘happiness’, ‘self’,\nand ‘experience’. The presence of ‘robot’ and ‘AI’ suggests topics about technology’s\nrelationship with human psychology was atrracted by Ph.D students.\nControl-Claude (b): The themes here seem to be oriented around positivity and\ngrowth. Key terms such as ‘will’, ‘increase’, ‘greater’, and ‘positive’ stand out.\nRandom-selected LLMCG (c):The terms in this word cloud underscore social\nconnections, individual autonomy, and competence. Words such as ‘social’, ‘individual’,\n‘autonomy’, and ‘competence’ are dominant. Themes of satisfaction, resilience, and\ncultural aspects can also be deciphered.\nExpert-selected LLMCG (d):Here, the emphasis seems to be on community, personal\nfeelings, and shared experiences. ‘Support’, ‘sense’, ‘one’, ‘we’, and ‘social’ are recurrent\nterms, highlighting collective experiences and social interconnectedness.\nConnection graph analysis. The connection graphs in Figure B2 depict\nrelationships between various themes and concepts for different groups.\nControl-Human (a): The graph for this model suggests a notable interplay between\nartificial intelligence themes, such as ‘Robot Companionship’ and ‘AI generating\nmusic/classic music’, and human well-being factors like ‘Heart rate variability (HRV) and\nelectrodermal activity measures’ and ‘Life quality based ESM data’. This suggests research\nor perspectives focusing on how AI, robotics or algorithms can impact and measure human\nwell-being.\nControl-Claude (b): This graph emphasizes different facets of well-being, from\nINSERT SHORTTITLE COMMAND IN PREAMBLE 42\n‘Emotional Well-being’ to ‘Workplace Well-being’. It also considers both positive elements,\nsuch as ‘Growth mindset’ and ‘Shared novel experiences’, and potential challenges, like\n‘Reducing anxiety symptoms’. This indicates a holistic view of well-being.\nRandom-selected LLMCG (c):There’s a strong focus on societal and structural\ndeterminants of well-being, such as ‘Economic condition and financial hardship’,\n‘Autonomy/Competence’, and ‘Management of health-related issues’. It seems to highlight\nthe broader environmental and cultural factors affecting individual well-being.\nExpert-selected LLMCG (d):This graph reflects more nuanced interconnections\nbetween personal experiences and environmental factors. Notable themes include the\n‘Living in walkable, mixed-use neighborhoods’, ‘Exposure to nature’, and the ‘Integration\nof all influences into empowerment’. It suggests a focus on how diverse life experiences,\nsettings, and exposures can interplay to shape an individual’s well-being.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 43\nTable B1\nInquiry on hypothesis generation for PhD students.\nDear PhD students,\nI would greatly appreciate it if each of you could propose at least three original research hypotheses related\nto well-being. The specific requirements are as follows:\nEach hypothesis should involve well-being. That is, well-being (e.g. life satisfaction, positive emotions,\npsychological health, etc.) should be either the independent variable, or the dependent variable, or the\nmediator in the research hypotheses you propose.\nPlease ensure the research hypotheses you submit are novel. Related personnel will check for duplication\nupon receiving the hypotheses. If the duplication check finds that identical research questions have already\nbeen investigated in prior publications, it will unfortunately have to be considered unsatisfactory. In such\ncases, I would need to kindly request you resubmit with different hypotheses. The subsidy for this task is\n100 RMB per person.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 44\nAlgorithm 1Compute intra-Group pairwise distances\nRequire: • Fi\ng: The 2D t-SNE feature representation of the ith idea in group g, derived from the\nrespective high-dimensional embedding.\n• G: The set of all groups, withg representing an individual group.\nEnsure: • Dg: A dictionary containing lists of pairwise distances for each groupg.\n1: Initialize Dg as an empty dictionary\n2: for each groupg ∈G do\n3: Let Ng be the number of data points in groupg\n4: Let F1\ng , F2\ng , ..., FNg\ng be the feature representations of the ideas in groupg\n5: Initialize list Lg = []\n6: for i = 1to Ng do\n7: for j = 1to Ng, j ̸= i do\n8: Let di,j\ng =\n√\n(xig −xj\ng)2 + (yig −yj\ng)2 ▷ Compute the Euclidean distance betweenFi\ng and Fj\ng\n9: Append di,j\ng to Lg\n10: end for\n11: end for\n12: Set Dg = Lg ▷ Store pairwise distances for groupg in Dg\n13: end for\n14: return Dg\nINSERT SHORTTITLE COMMAND IN PREAMBLE 45\nTable B2\nDetailed prompt for hypothesis generation in Claude-2 model.\nPlease generate original research hypotheses.The specific requirements are as follows:\n1. Each hypothesis should involve well-being. That is, the variables used to measure well-being\n(e.g. lifesatisfaction, positiveemotions, psychologicalhealth, etc.) shouldbeeithertheindependent\nvariable, or the dependent variable, or the mediator in the research hypotheses you propose.\n2. In your research hypothesis, it is essential to include the independent variable and the depen-\ndent variable, as well as the directional relationship between them. If possible, include mediating\nvariables. Other variables, such as moderating variables, are not mandatory.\n3. Please ensure the research hypotheses you submit are novel. That means you should conduct\nthis task in two steps. After generating hypothesis, please check for duplication before submitting\nthem. If the duplication check finds that identical research questions have already been investigated\nin prior publications, replace them with different novel hypotheses.\n4. Please output in format for csv file, which is a table with columns for the independent variable\n(IV), dependent variable (DV), the relationships between IV and DV (either positive or negative),\nmediator, moderator, and description of the hypothesis in natural language. The columns for\nmediators and moderators can be left blank if your hypothesis doesn’t involve them.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 46\nTable B3\nEvaluation criteria for hypotheses by expert reviewers.\nDear Reviewers:\nThank you for participating in this evaluation process. In the table provided, you will find various psycholog-\nical research ideas. We kindly request you to evaluate each hypothesis based on two key criteria: Usefulness\nand Novelty.\nNovelty: This criterion focuses on the originality and uniqueness of the hypothesis.How new or un-\nprecedented is the concept?A hypothesis that introduces fresh perspectives or challenges existing views\nwould score high for novelty.\nUsefulness: Please rate the practical relevance and applicability of each hypothesis.Would it be ben-\neficial for researchers, practitioners, or the general public?A hypothesis that provides actionable\ninsights or has potential to drive meaningful change would be rated high on this criterion.\nPlease providea score (1-10)for each criterion for every hypothesis. Your expert opinions will significantly\naid in understanding and prioritizing these hypotheses.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 47\n(a) Control-Human\n (b) Control-Claude\n(c) Random-selected LLMCG\n (d) Expert-selected LLMCG\nFigure B1. Word cloud comparisons of different models and groups.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 48\n(a) Control-Human\n (b) Control-Claude\n(c) Random-selected LLMCG\n (d) Expert-selected LLMCG\nFigure B2. Conceptual linkages and interactions across different models and groups.\nINSERT SHORTTITLE COMMAND IN PREAMBLE 49\n(a) Reviewer #1\n(b) Reviewer #2\n(c) Reviewer #3\nFigure B3. Comparisons of novelty scores among different reviewers.",
  "topic": "Novelty",
  "concepts": [
    {
      "name": "Novelty",
      "score": 0.6912578344345093
    },
    {
      "name": "Causal model",
      "score": 0.5616329908370972
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5548890829086304
    },
    {
      "name": "Computer science",
      "score": 0.5505443811416626
    },
    {
      "name": "Causal analysis",
      "score": 0.48493626713752747
    },
    {
      "name": "Psychological research",
      "score": 0.4552319645881653
    },
    {
      "name": "Cognitive science",
      "score": 0.42196792364120483
    },
    {
      "name": "Data science",
      "score": 0.40927061438560486
    },
    {
      "name": "Machine learning",
      "score": 0.39135122299194336
    },
    {
      "name": "Natural language processing",
      "score": 0.3680644631385803
    },
    {
      "name": "Psychology",
      "score": 0.3564438223838806
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3464449942111969
    },
    {
      "name": "Social psychology",
      "score": 0.16907763481140137
    },
    {
      "name": "Mathematics",
      "score": 0.09279826283454895
    },
    {
      "name": "Mathematical economics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ]
}