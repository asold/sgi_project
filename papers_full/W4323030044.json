{
  "title": "A Framework for Neurosymbolic Robot Action Planning using Large Language Models",
  "url": "https://openalex.org/W4323030044",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4288025547",
      "name": "Capitanelli, Alessio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3154499054",
      "name": "Mastrogiovanni, Fulvio",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1992985704",
    "https://openalex.org/W4287671496",
    "https://openalex.org/W3101355526",
    "https://openalex.org/W2902424078",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2963319175",
    "https://openalex.org/W2400047276",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W4385764285",
    "https://openalex.org/W4394646531",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W2927819804",
    "https://openalex.org/W4377145613",
    "https://openalex.org/W2972154709",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W3042963287",
    "https://openalex.org/W2988231059",
    "https://openalex.org/W2800017313",
    "https://openalex.org/W4287888476",
    "https://openalex.org/W2799397159",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W2149361370",
    "https://openalex.org/W3096863422",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2963195134",
    "https://openalex.org/W4311642023"
  ],
  "abstract": "Symbolic task planning is a widely used approach to enforce robot autonomy due to its ease of understanding and deployment in robot architectures. However, techniques for symbolic task planning are difficult to scale in real-world, human-robot collaboration scenarios because of the poor performance in complex planning domains or when frequent re-planning is needed. We present a framework, Teriyaki, specifically aimed at bridging the gap between symbolic task planning and machine learning approaches. The rationale is training Large Language Models (LLMs), namely GPT-3, into a neurosymbolic task planner compatible with the Planning Domain Definition Language (PDDL), and then leveraging its generative capabilities to overcome a number of limitations inherent to symbolic task planners. Potential benefits include (i) a better scalability in so far as the planning domain complexity increases, since LLMs' response time linearly scales with the combined length of the input and the output, and (ii) the ability to synthesize a plan action-by-action instead of end-to-end, making each action available for execution as soon as it is generated instead of waiting for the whole plan to be available, which in turn enables concurrent planning and execution. Recently, significant efforts have been devoted by the research community to evaluate the cognitive capabilities of LLMs, with alternate successes. Instead, with Teriyaki we aim to provide an overall planning performance comparable to traditional planners in specific planning domains, while leveraging LLMs capabilities to build a look-ahead predictive planning model. Preliminary results in selected domains show that our method can: (i) solve 95.5% of problems in a test data set of 1,000 samples; (ii) produce plans up to 13.5% shorter than a traditional symbolic planner; (iii) reduce average overall waiting times for a plan availability by up to 61.4%",
  "full_text": "A Framework for Neurosymbolic Robot Action\nPlanning using Large Language Models\nAlessio Capitanelli and Fulvio Mastrogiovanni\nDepartment of Informatics, Bioengineering, Robotics, and Systems Engineering,\nUniversity of Genoa, Via Opera Pia 13, 16145, Genoa, Italy\nalessio.capitanelli@dibris.unige.it, fulvio.mastrogiovanni@unige.it\nAbstract. Symbolic task planning is a widely used approach to enforce\nrobot autonomy due to its ease of understanding and deployment in\nengineered robot architectures. However, techniques for symbolic task\nplanning are difficult to scale in real-world, highly dynamic, human-\nrobot collaboration scenarios because of the poor performance in plan-\nning domains where action effects may not be immediate, or when fre-\nquent re-planning is needed due to changed circumstances in the robot\nworkspace. The validity of plans in the long term, plan length, and\nplanning time could hinder the robot’s efficiency and negatively affect\nthe overall human-robot interaction’s fluency. We present a framework,\nwhich we refer to as Teriyaki, specifically aimed at bridging the gap\nbetween symbolic task planning and machine learning approaches. The\nrationale is training Large Language Models (LLMs), namely GPT-3,\ninto a neurosymbolic task planner compatible with the Planning Do-\nmain Definition Language (PDDL), and then leveraging its generative\ncapabilities to overcome a number of limitations inherent to symbolic\ntask planners. Potential benefits include (i) a better scalability in so far\nas the planning domain complexity increases, since LLMs’ response time\nlinearly scales with the combined length of the input and the output,\ninstead of super-linearly as in the case of symbolic task planners, and\n(ii) the ability to synthesize a plan action-by-action instead of end-to-\nend, and to make each action available for execution as soon as it is\ngenerated instead of waiting for the whole plan to be available, which in\nturn enables concurrent planning and execution. In the past year, signif-\nicant efforts have been devoted by the research community to evaluate\nthe overall cognitive capabilities of LLMs, with alternate successes. In-\nstead, with Teriyaki we aim to providing an overall planning performance\ncomparable to traditional planners in specific planning domains, while\nleveraging LLMs capabilities in other metrics, specifically those related\narXiv:2303.00438v3  [cs.AI]  4 Jun 2024\n2 Capitanelli and Mastrogiovanni\nto their short- and mid-term generative capabilities, which are used to\nbuild a look-ahead predictive planning model. Preliminary results in se-\nlected domains show that our method can: (i) solve 95.5% of problems\nin a test data set of 1000 samples; (ii) produce plans up to 13.5% shorter\nthan a traditional symbolic planner; (iii) reduce average overall waiting\ntimes for a plan availability by up to 61.4%.\nKeywords: AI, Generative, Neurosymbolic, Large Language Models,\nTask Planning, PDDL, Human-Robot Interaction, GPT\n1 Introduction\nNowadays, we are witnessing the emergence of scenarios where robots are ex-\npected to operate with an increased autonomy, versatility, and robustness, espe-\ncially in tasks to be carried out jointly, or in close coordination with humans. In\nfact, in human-robot collaboration tasks, humans could act as simple beneficia-\nries of robot behaviors, or they could act as teammates, actually partaking in\nrobot actions as they unfold.\nThis state of affairs not only holds in assistive and service robotics, but is\nbecoming particularly relevant in settings inspired by the Industry 4.0 paradigm,\nwhich is believed to profoundly shape the nature of next-generation manufac-\nturing operations [Heyer, 2010]. These scenarios require robots to exhibit quite\ndynamic and reliable capabilities in sustaining a purposeful collaboration with\nhuman operators: while robots may be characterized by a continuum range in\nautonomy, also depending on the specific operation at hand, human operators\nmay supervise robot actions and intervene if and when necessary. Therefore,\nrobots must not only operate reliably in line with the interaction with human\noperators, but also to face unforeseen events very likely to happen when the\nwork to be done is poorly structured, and human loosely predictable behavior is\ninvolved [Darvish et al., 2018]. This paradigm shift requires robots to validate or\nre-plan their future actions continuously, ideally to predict or at least to react to\nchanging conditions or the outcomes of unforeseen human behavior. As a conse-\nquence, it turns out that planning performance is of critical importance to ensure\nboth an efficient robot operation and an effective human-robot collaboration.\nIn scenarios characterized by such requirements, conventional, symbolic task\nplanners have limitations that hinder their efficacy. In current practice, planners\nuse search algorithms to explore the range of possible states that the modeled\nA Framework for Neurosymbolic Robot Action Planning using LLM 3\nenvironment can assume until they find a sequence of planning operators (that\nis, actions) in-between states leading to a state compatible with the desired goal\nsituation. In its simplest form, planning engines assume that states and actions\nare represented in terms of predicates, that is, fragments modeling certain as-\npects of reality, and these in terms of (possibly grounded and typed) variables.\nIf the closed-world assumption is posed, that is, the truth value associated with\npredicates is assumed to be false if not stated otherwise, the specific predicates\npart of each intermediate state depend only on the effects of the actions that\ngenerated those state. Therefore, the planning process can be seen a generative\nMarkov decision process based on a symbolic substrate made up of (grounded\nand typed) variables, predicates, and their arrangement in actions. On aver-\nage, search algorithms tend to exhibit combinatorial explosion in both time and\nspace complexity as the number of symbols involved in the planning process\nincreases [Garrett et al., 2020]. Heuristics are aimed at mitigating such unfor-\ntunate computational complexity by trading it with optimal performance and\nalgorithmic soundness. In spite of all the available heuristics, which often make\nthe planning process of a satisfycing nature, the overall computational complex-\nity makes frequent re-planning extremely costly in terms of needed planning\ntime, specifically considering the fact that a plan synthesized by a symbolic task\nplanner is an atomic entity, that is, it is either generated in its entirety, or it\nis not available. Although approaches for hierarchical task planning have been\nproposed in the literature, whereby high-level (generic) plans may be created\nquickly, and low-level (specific) sub-plans can be generated in chunks, the ba-\nsic issue remains, and it is also necessary to carefully engineer which part of\nthe plan could be synthesized at the high-level, and which part (in terms of\nactions actually executable by a robot) should be made available in sub-plans\n[Mastrogiovanni et al., 2004, Murali et al., 2020, Darvish et al., 2021].\nWhen human operators and robots collaborate on a task, they can achieve a\nhigh standard of coordination, leading to a synchronized blending of their actions\nwith precise and efficient timing [Carf´ ı et al., 2019]. This feature is referred to\nas fluency as per Hoffman [2019], and it has been demonstrated that the delay\nexperienced by a human immediately after completing an action, as incurred\nby their teammate, has a strong correlation with subjective fluency perception.\nObviously enough, the aforementioned combinatorial explosion of planning times\nnegatively affects the quality of human-robot collaboration, and specifically its\nfluency. It has been argued that fluency in human-robot collaboration could be\n4 Capitanelli and Mastrogiovanni\nimproved by introducing intuitive communication channels able to make clear\nwhich action a robot will carry out next Macci´ o et al. [2022]. In fact, such\napproaches work while the cooperation unfolds, whereas in case of re-planning\nit would be necessary to put the collaboration process on hold.\nIn this paper, we introduce a framework, which we call Teriyaki 1, to train\nand invoke Large Language Models (LLMs) [Hatcher and Yu, 2018] to behave as\ntask planners. Teriyaki is based on two conceptual tenets, which are reflected in\nthe overall architectural design as well as for what concerns its practical use. The\nfirst is related to the fact that Teriyaki is one possible instance of a more general\nprocedural workflow, which is aimed at generating a data-driven, domain-specific\nplanner, whereas the second concerns a workflow structured such that: first, a\ndomain-specific data set of problems and plans must be generated using an exist-\ning PDDL-based planner; then, such data set must be used to fine-tune an LLM\nof choice; finally, the synthesized planner can be used in place of the original,\ntraditional planner used for the generation of the data set. The substitution of\nthe planner is trivial in any traditional sense-plan-act architecture, but it is also\npossible to adapt such architecture in order to exploit the main advantage of\nTeriyaki planners over traditional ones, i.e., the ability to generate a plan action\nby action, and thus plan and execute in parallel.\nLLMs such as OpenAI’s GPT-3 [Brown et al., 2020] are characterized by\nthe possibility of (i) linearly scaling in computational complexity with the total\nlength of prompt and completion, and (ii) generating partial results while iter-\natively predicting the next set of symbols in a sequence. We argue that these\ntwo features could be leveraged to design a task planner with a reduced run-\ntime complexity, and capable of making the next action available ahead of full\nplan synthesis, which would allow a robot architecture embedding such planner\nto begin action execution before the entire plan is generated, that is, a very\ndesirable requirement in human-robot collaboration scenarios. If the robot ar-\nchitecture allowed it, this may unlock concurrent plan generation and execution.\nMoreover, if such a model could be trained to receive input and return output\nusing the Planning Domain Definition Language (PDDL) [Aeronautiques et al.,\n1998], it would maintain full compatibility with existing software frameworks\nfor symbolic task planning already widely adopted by the robotics community,\nsuch as ROSPlan [Cashmore et al., 2015]. We refer to the workflow underlying\nTeriyaki as neurosymbolic, as it combines neural network based learning as sub-\n1 Teriyaki: https://github.com/alessiocpt/teriyaki\nA Framework for Neurosymbolic Robot Action Planning using LLM 5\nstrate with symbolic knowledge representation and reasoning as logic [Garcez\nand Lamb, 2020]. Unlike traditional symbolic task planners, our approach is\ndomain-dependent and requires training, but it is trivial to generate large data\nsets for training on a given domain using traditional task planners.\nTeriyaki has been evaluated and tested on two planning domains, specifically\nfor the collaborative manipulation of articulated objects by human operators\nand robots, a challenging task previously described in the literature [Capitanelli\net al., 2018, Bertolucci et al., 2019, 2021]. These domains have been selected\nbecause of the challenges they pose on the planning process: (i) for symbolic task\nplanners, because the manipulation of an articulated object scales very poorly\nwith the number of its links, joints, and the allowed angle configurations; (ii)\nfor LLMs, since the domains include conditional effects, that is, an action could\nimply modifying the state of the representation in ways that are not immediately\napparent from the arguments of the action itself, for example, if a link were\nrotated around a joint, all downward links in the chain should be also implicitly\nrotated with respect to an absolute reference frame.\nThe training process has been performed on a large data set of 9000 problem-\nplan pairs generated automatically and solved by an existing, state-of-the-art,\ntraditional, symbolic, PDDL-based planner, namely Probe [Lipovetzky and Geffner,\n2011]. It is noteworthy that the data set, as well as the code to generate more\npairs, are available on the accompanying repository. The resulting models have\nbeen rigorously tested on 1000 pairs not previously used for training, and eval-\nuated in terms of percentage of valid plans, plan length, and planning times.\nDuring training, data related to planning validity, defined as the percentage of\nplans that can be formally proved to be correct, have been collected to investi-\ngate their evolution with the growing number of training samples and to assess\nwhether transfer learning between similar planning domains is possible.\nResults show near identical planning validity between Probe and the solvers\ngenerated with Teriyaki. However, Teriyaki outperforms Probe in terms of shorter\nplan length by up to 13.5% in one of the domains. Regarding planning times, it\nis noteworthy that an objective comparison is not possible due to the different\ncomputing architectures on which the two planners run, that is, Teriyaki is based\non a proprietary cloud-based service, namely GPT-3, whose response time was\nnot guaranteed nor predictable during test sessions, whereas Probe can run on a\nfully accessible and controlled workstation. In fact, in the current experimental\narchitecture, Probe remains faster in generating a complete plan, that is, in a\n6 Capitanelli and Mastrogiovanni\nsituation where we require Teriyaki to synthesize a full PDDL-compatible plan\nend-to-end. However, when the planner as a module is integrated into a robot ar-\nchitecture, the use of a traditional planner like Probe forces us to adopt a pure\nsense-plan-act approach, that is, the plan must be synthesized in its entirety\nbefore its first action can be executed, whereas Teriyaki generates and makes\navailable for execution the first action as it is produced. In our experiments,\nthis reduces the overall waiting time by 61.4% on average, as it exploits simul-\ntaneous planning and execution. Finally, we also experimentally observe that\nTeriyaki solvers scale with the input and output length rather than with the\nproblem and domain complexity, hinting that there might exist more complex\ndomains where Teriyaki could outperform Probe, and in general a traditional\nsymbolic planner.\nTo summarize, the main contributions of this paper are the following: (i)\nwe designed, developed, and released to the community Teriyaki, a workflow to\ntrain and use GPT-3 to solve PDDL-compatible problems for a given domain,\nas well as original data sets and ancillary code; (ii) compared to similar methods\nin literature, most notable the Plansformer approach [Pallagani et al., 2023],\nTeriyaki has been tested on plans 80% longer and including conditional effects;\n(iii) we demonstrate that LLMs can outperform non-optimal planners in terms\nof plan length, such as for instance Probe [Lipovetzky and Geffner, 2011]; (iv) we\npropose to exploit LLMs capability to stream the output plan while it is being\ngenerated, for simultaneous planning and execution, leading to a significantly\nshorter waiting time for execution to begin, and therefore an improved fluency\nin human-robot collaboration.\nDespite the positive results, we want to highlight that solvers generated with\nTeriyaki should not be considered at the moment as a complete alternative to\ntraditional, symbolic planners. Instead, we recommend considering them as a\nproof-of-concept of the planning capabilities of LLMs and their possible appli-\ncations as the technology matures, especially regarding to human-robot collab-\noration scenarios.\n2 Related Work\nNeurosymbolic approaches foresee that (i) their knowledge is encoded in vector-\nbased representations supporting neural networks which maximize an efficient\nlearning from data, and (ii) discrete symbols become available as a result of\nquerying and extracting knowledge from the trained network [Garcez and Lamb,\nA Framework for Neurosymbolic Robot Action Planning using LLM 7\n2020]. For a long time, it has been theorized that such approaches may provide\nan alternative to the problem of combinatorial explosion in reasoning by lever-\naging the reasoning mechanisms induced by the learning process. However, it is\nonly recently that they started to gain traction, mainly for Natural Language\nProcessing [Dale, 2021]. Most notably, GPT-3 and its successors GPT-3.5 and\nGPT-4, are famous LLMs released by the company OpenAI2 that achieved aston-\nishing results in generating human-level complex text in the form of structured\nsentences [Brown et al., 2020]. Other popular models are LLama 2 [Touvron\net al., 2023], LaMDA [Thoppilan et al., 2022], PALM [Chowdhery et al., 2022],\nMegatron-Turing NLG [Smith et al., 2022] or BLOOM [Scao et al., 2022]. A few\nof the most notable applications of these models are text summary generation\nand completion, sentiment analysis, and, of particular interest to our application,\ncode generation [Chen et al., 2021].\nLLMs are one of the most promising applications of Deep Generative Mod-\nels [Oussidi and Elhassouny, 2018], that is, a category of unsupervised Deep\nLearning algorithms capable of capturing the probabilistic distribution that can\nunderlie the generation of a class of data. Once estimated, it is possible to gen-\nerate synthetic data compatible with such probabilistic distribution. LLMs in\nparticular are capable of estimating the probability of a sentence represented as\nthe product of each discrete symbol’s probability given the symbols preceding\nit. For instance, given a few words as a prompt, they can easily suggest the most\nlikely way to complete the sentence. Usually, such symbols are referred to as\ntokens and represent short sequences of characters.\nCommon ways to implement LLMs are Recurrent Neural Networks [Mikolov\net al., 2011], Long Short-Term Memory (LSTM) networks [Sundermeyer et al.,\n2012] and, most recently, Transformers [Vaswani et al., 2017], of which GPT\nmodels are the most notable example. Transformers are models based on an\nencoder-decoder architecture, and adopt a self-attention mechanism that allow\nthem to weigh each part of the input data differently. While in the current\nstate of affairs there are no guarantees about the long-term logical coherence\nof the answers given by a Transformer-based architecture, especially for long\nsequences of symbols, such models exhibit surprising capabilities in generating\nplausible and coherent lists of instructions, from cooking recipes to functioning\ncode, surpassing by far recurrent and LSTM networks.\n2 Web: https://openai.com/blog/gpt-3-apps\n8 Capitanelli and Mastrogiovanni\nSince their popularization, the integration of Large Language Models (LLMs)\ninto robotics applications has gained significant attention, mainly as an instru-\nment to improve the robot reasoning and planning capabilities. PaLM-E [Driess\net al., 2023] explores embodied language models that incorporate real-world sen-\nsor data into language models, providing grounding for robotics problems. This\napproach, which includes multimodal inputs for training, shows promise in tasks\nsuch as sequential robotic manipulation planning, aligning with our aim to im-\nprove robot autonomy in dynamic scenarios. Similarly, GENOME [Chen et al.,\n2023] introduces a model that leverages LLMs for visual reasoning, focusing on\nmodule generation and reuse for fast task generalization, a concept that com-\nplements the goals of our Teriyaki framework.\nSuch interest is shared by the wider AI community, with a major focus on\nassessing the true extent of LLMs’ general logical capabilities, especially in zero-\nshot to few-shot attempts. As per Brown et al. [2020], for zero-shot approaches,\nwe refer to the case when questions implying a certain level of logic capabilities\nare posed directly to the model, whereas one-shot and few-shot approaches pro-\nvide a very limited amount of examples to the model as part of the request. In\nall such cases, the model is just prompted, without further training with spe-\ncific examples, a procedure now commonly referred to as fine-tuning. The work\nby Valmeekam et al. [2022] proposes a benchmark for LLM-based planning and\nreasoning in a few-shot scenario. The authors employed PDDL, but only to au-\ntomatically generate, in natural language, several logical problems never seen\nbefore by the model, and compared the performance of popular LLMs under\nseveral metrics. The best performer could solve only 25 out of 500 problems,\nthat is, 5%. Better results were obtained by Wang et al. [2023] using few-shot\nlearning and grammar prompting. In this approach, GPT-3.5 and GPT-4 are\nused to predict a specialized grammar given a test input, and then generate\nthe output according to the rules of the grammar itself. Among others, the au-\nthors tested inputs in PDDL format extracted from popular but rather simple\nPDDL domains. While the benefits of grammar prompting are evident, in abso-\nlute terms they obtained mixed results depending on the domain, with success\nrates ranging from 40% to 100%. LLM-Planner [Song et al., 2023] is another\nfew-shot method based on GPT-3 and designed specifically for embodied rea-\nsoning, that is, techniques aimed at being used in agents dealing with real-world,\nphysical environments. This approach uses near-natural language and is charac-\nterized by a dynamic re-planning mechanism grounded on what an agent could\nA Framework for Neurosymbolic Robot Action Planning using LLM 9\nobserve in the environment at each step. High-level planning validity in a va-\nriety of tasks ranges from 26% to 51%. Similar approaches are discussed also\nby Logeswaran et al. [2022] and Wake et al. [2023]. Singh et al. [2023] proposed\ninstead Progprompt, a method that exploits the strong performance of GPT-3\nin code generation by prompting the model to generate executable Python code\nto solve a given problem. Silver et al. [2023] further improved on this concept by\nproposing a method for generalized planning in PDDL domains by also including\nan automated debugging mechanism.\nAs perfectly stated by Huang et al. [2022], the main issue with almost all\nthese approaches is that plans produced naively by LLMs cannot map precisely\nto admissible actions, thus most of them attempt to increase their success rate\nin achieving the desired goal state by clever prompting approaches. Silver et al.\n[2022] face similar challenges while trying to solve actual PDDL problems, main-\ntaining the PDDL format both in input and output. As an alternative solution,\nthey propose to use the good-but-imperfect LLM-generated output to support\na traditional (that is, based on heuristic search), PDDL-based planner. While\nthis approach mitigates the issues described before and often provides planning\ntime improvements above 10%, the ability to generate a plan action-by-action\nin a way compatible with human-robot collaboration scenarios is lost.\nHence, it appears that with currently available LLMs, solving task planning\nproblems reliably, that is, with a success rate comparable to that of traditional,\nPDDL-based planners, is only possible through an appropriate fine-tuning pro-\ncess. Such an approach involves the further training of a base model with specific\nexamples, namely problem-plan pairs, related to the planning domain at hand.\nPlansformer is an approach of this kind proposed by Pallagani et al. [2023].\nThe authors trained five different models by fine-tuning CodeT5 [Wang et al.,\n2021], an LLM specialized in code generation, to solve just as many simple, well-\nknown, PDDL-based domains such as the Tower of Hanoi , where Plansformer\nreached 97% valid plans. The fine-tuning dataset has been generated by solving\n18000 randomly generated problems with the Fast-Downward planner [Helmert,\n2006], and then translating both problems and plans into a more compact form.\nPDDL-based problems are augmented by listing the actions in the domain to\nexplicitly teach the model about their preconditions and effects. While Plans-\nformer demonstrates the possibility of adopting a fine-tuning approach to train\nsolvers for different domains, the considered planning domains are rather sim-\nple, and do not include such advanced planning concepts as conditional effects,\n10 Capitanelli and Mastrogiovanni\nfor example. In a more general sense, a possible interpretation of these results\ncould be that there are obvious similarities between the capability of LLMs to\nlearn the probability of a symbol following the preceding ones, and the search\nheuristics that power traditional PDDL-based planners. Hence, we are not just\nextracting knowledge from the model and exploiting its general reasoning capa-\nbilities, but rather we are training the model to approximate the search heuristic\nof the planner generating the dataset.\nTeriyaki takes a similar approach with a few key differences: (i) we assume\nthat LLMs are capable of handling unaltered PDDL-based problem and plan\ndescriptions, which in turn makes the model easier to use in existing robot\narchitectures; (ii) with the aim of reducing the length of the prompt, which is\none of the main LLMs’ constraints, we instead exclude some information, on the\nassumption that the model can learn it and, for example, no explicit knowledge\nof the action preconditions and effects is given to Teriyaki during fine-tuning;\n(iii) it is trained on a dataset of non-optimal plans, which allowed us to observe\nthat LLMs are capable of outperforming the traditional planner they are trained\nupon in terms of average plan length in selected domains; (iv) we purposely focus\non complex planning domains, that is, domains which require advanced planning\nconstructs to generate provably valid plans, such as conditional actions; (v) we\npropose to use Teriyaki to stream plans as they are generated, that is, action-by-\naction, to enable simultaneous planning and execution in robot tasks, specifically\nin human-robot collaboration.\n3 Methods\n3.1 Planning Domains, PDDL Version, and Planner\nIn our work, and with the aim of challenging both the chosen, traditional, PDDL-\nbased planner as well as Teriyaki, we have selected two PDDL domains, which\nwe refer to from now on as MACRO and NO-MACRO. The two domains model\nmanipulation actions on an articulated object, and have been previously show-\ncased in human-robot collaboration scenarios. It is noteworthy that, for the work\ndescribed here, we have used the same robot perception and control architecture\nthat has been discussed by Capitanelli et al. [2018]. Such an architecture includes\nvision-based perception modules able to correctly detect and identify each link\nof the articulated object, as well as their pose; the outcome of these modules\nis a series of symbolic predicates which specify the overall object configuration\nA Framework for Neurosymbolic Robot Action Planning using LLM 11\nFig. 1. A Baxter robot executing actions in two domains involving the manipulation\nof an articulated object. A human can act on an articulated object’s joint at any time,\nforcing the robot to re-plan.\nfor planning and monitoring purposes. Likewise, robot control aspects related to\nmanipulation are included in the architecture, and suitable mappings between\nPDDL-like, symbolic actions and robot motions are available. Figure 1 shows\na Baxter robot executing a plan from the two domains, and a possible inter-\naction with a human. Both domains use the so-called absolute representation\nas described by Capitanelli et al. [2018], meaning that angles between pairwise\nlinks of the articulated object are expressed with respect to an absolute reference\nframe. The choice is motivated by the fact that domains using an absolute rep-\nresentation require conditional effects in the planning domain, that is, actions\n12 Capitanelli and Mastrogiovanni\ncan have implicit effects on joint angles not directly affected by the outcomes\nof the action itself, and therefore the planner is expected to be quite stressed in\nmaintaining all the constraints. In fact, if managed by a traditional, symbolic\naction planner this requires propagating the (conditional) effects of each action\nto modify the value of state variables not directly affected by the effects. Like-\nwise, we argue that in the case of LLMs like GPT-3, conditional effects could\nstress the model because it should be harder to maintain coherence in the plans\ngiven the underlying generative process.\nThe main difference between the two selected domains is that one uses PDDL\nmacros whereas the second does not. Macros are ordered, compound actions that\nbundle together a number of basic actions, for example, agrasp-rotate-release\naction instead of three atomic ones, that is, grasp, rotate, and release. Their\nuse is an effective way to reduce the planning time in traditional action planners\nat the cost of accepting less optimal plans. In fact, the use of macros could lead\na planner to enforce the use of an actions sequence, that is, a given macro, at\nthe cost of introducing possibly spurious actions. For example, if the goal state\nassumed the articulated object to be in a given configuration, but with the robot\ngripper still maintaining its hold on the object, the planner could prefer using\na macro grasp-rotate-release followed by another grasp action (partially\ncompensating the effects of the macro) instead of two atomic actions grasp and\nrotate. Macros are also supposed to facilitate the generative process of GPT-3\nsince the use of macros is expected to shorten the resulting plan significantly.\nSince the two domains are fundamentally similar but lead to plans of different\nlengths, they are ideal candidates to test how Teriyaki-based solvers scale with\nthe output length. As both domains taken into consideration here used condi-\ntional effects, we used PDDL 2.1 and a compatible PDDL-based planner, namely\nProbe [Lipovetzky and Geffner, 2011], one of the planners used in the previous\nwork by Capitanelli et al. [2018]. Probe is a satisfycing planner which heuristi-\ncally tries to generate single action sequences by exploring the search space using\na greedy, depth-first approach (that is, a probe). Statistically, a solution can be\nquickly found in this way, albeit not necessarily the optimal one. Probes can\nbe used as first solutions on top of which it is possible to find better (shorter)\nplans. This could be done by analyzing probes in order to find landmarks, that\nis, certain states that are reached by different probes, and therefore represent\nimportant states to generate. If landmarks are available, it is then possible to\nobtain sub-plans in-between landmark states. Probe has been selected for two\nA Framework for Neurosymbolic Robot Action Planning using LLM 13\nProbe\nplanner\n9.000\nPDDL\nproblems\n8.876\nPDDL\nplansPDDL\ndomain\nJSONL\ntraining datasetProblem\ngenerator\nJSONL\nvalidation dataset\n8.000\n876\nOpenAI\nfine-tuning API\nTeriyaki\nplanner\nProblem\npreprocessing\nFig. 2. A diagram of Teriyaki fine-tuning process. Blocks in yellow represent custom\ncode developed for data generation and processing as described in Section 3.2.\nreasons. On the one hand, its use in previous work of ours allow us to make a\ndirect comparison between its outcomes and plans generated by Teriyaki; on the\nother hand, its widespread use could allow for an easier adaptation of our work\nin different scenarios. In fact, the choice of Probe has an impact on the quality of\nthe results, and it should pointed out that better alternatives could be adopted.\n3.2 Choice of the LLM, Data Set Generation and Composition\nAs we anticipated, in this work we leverage GPT-3, a closed-source model and\ncommercial product that at the time of this writing can only be accessed as a\ncloud-based service through OpenAI’s API. Our results are not limited to the\nspecific features of GPT-3 and could in principle be replicated with any available\nLLM. In practice, since our first experiments with GPT-3 presented in this paper,\nwe have tested several smaller models that can be run locally, such as LLama\n2 7B, 13B, and 30B, but none of them could reliably solve problems in the two\nselected domains, and therefore the related results are not reported here. Forced\nto employ a commercial model as a service, the choice of GPT-3 is motivated by\nits size and speed compared to its more recent versions. In the case of GPT-3,\nthe training can be performed by providing a structured file where each line is\na prompt-completion pair. We assumed that the GPT-3 model can implicitly\nlearn by example the rules usually defined in a PDDL domain file, such as the\nallowed (modeled) actions, and their respective preconditions and effects. Hence,\nwe conducted training sessions using onlyproblem-plan pairs, whereas we use the\nproblem as a prompt and the plan as completion. We remark here that only part\nof the PDDL problem is used as a prompt, as many of the predicates are actually\nnon functional, that is, they are used to encode general properties of the problem,\nsuch as the relative order of joint angle configurations, an example being: a joint\nat 45 deg can be rotated clockwise to 60 deg, and counter-clockwise to 30 deg\n14 Capitanelli and Mastrogiovanni\nin 15 deg increments. These predicates remain the same in all problems and are\nnever modified by action effects. Therefore, we assume that the model can learn\nthem from the implicit information encoded in the plans, in the same way as the\ndomain. We thus removed them from the prompt to reduce its length, as prompt\nlength has an impact on response time and the total prompt plus completion\nlength of each query cannot exceed 2048 tokens, or circa 8000 characters, for\nthe selected model, allowing us to generate longer plans without truncation and\nimproving performance.\nAt the time the experiments were performed, OpenAI’s documentation sug-\ngested using the largest and most powerful version of the model, called davinci,\nfor a conditional generation, that is, generating original text based on input. For\nthis use case, the documentation recommends training the model using at least\n500 training samples and aiming for 8000. One of the advantages of Teriyaki\nis that we can easily generate large training datasets using randomly generated\nproblems and a PDDL-based action planner. Therefore, we generated 9000 prob-\nlems, out of which 8000 were reserved for training and 1000 for validation. It\nmust be noted that out of the 9000 planning attempts performed by the planner\n124 failed, so the validation set has been reduced to 876 samples. Finally, we\nadded another 1000 problem-plan pairs as a test set, verified that each sam-\nple within this test set was solvable by Probe, and guaranteed that no problem\ninstance was a duplicate of those existing in the training and validation sets,\nthereby preventing any cross-contamination between the datasets.\nThe next step requires validating the completeness of the plans generated by\nProbe to ensure that we train our model only on correct data. To do so we use the\nwidely known VAL validation suite [Howey et al., 2004], which requires plans\nto be formatted in the International Planning Competition (IPC) standard 3.\nAs Probe does not return plans compliant with this standard, we developed\ncustom code to reformat the plan files to be compatible with VAL. This is also\nnecessary to ensure that Teriyaki will be trained on IPC-compatible standard\nplans, and will thus later reply to our queries in the same way, allowing us to\neasily benchmark its validity as far as planning is concerned. Running VAL over\nthe dataset resulted in all plans passing the test, meaning that they reached the\ndesired goal state, even though it must be reminded that Probe previously failed\nto generate a plan at all for 124 out of 9000 problems, that is, about 1 .37% of\nthe total.\n3 Web: https://ipc2023.github.io/\nA Framework for Neurosymbolic Robot Action Planning using LLM 15\nFinally, we compiled the training and validation datasets in jsonl format.\nEach line of a dataset file is composed of a prompt, that is, the problem, and a\ncompletion, that is, the plan. We added to the end of each prompt and completion\nthe respective termination sequences, namely \\n\\n###\\n\\n and END. As the\nname suggests, the termination sequences signal to the network the end of the\nmessage. This is especially important for the completion as the termination\nsequence can then be used as a stopping condition when querying the model. A\nsample line is provided in Listing 1.1, edited for the sake of brevity and clarity.\nThe whole data generation and dataset preparation process is outlined in Figure\n2.\nIt is important to clarify that the generated data utilized in our research\nshould not be perceived as synthetic with the negative connotation of less real.\nGiven that our focus is on symbolic task planning and assuming flawless percep-\ntion and grounding by the robot, this data is indistinguishable from real data\nfrom the perspective of the planner. Consequently, for the purposes of this work,\nwe exclusively employ generated data for training and analysis, as it provides\nequivalent validation for the planning module without introducing variances and\nbiases from perception and execution modules.\n{\" prompt \":\n\"(: init ( angle_joint angle315 joint1 )\n( angle_joint angle300 joint2 )\n( angle_joint angle285 joint3 )\n(in - centre joint2 )\n( free gleft ) ( free gright ))\n(: goal ( and\n( angle_joint angle0 joint1 )\n( angle_joint angle300 joint2 )\n( angle_joint angle285 joint3 )))\n\\n\\n ###\\ n\\n\",\n\" completion \":\n\" 0.00100: (link -to - central - grasp ...)\n0.00300: ( increase_angle_first_child_45 ...)\n0.00500: ( release - links ...)\n0.00700: (link -to - central - grasp ...)\n0.00900: ( decrease_angle_first_child_45 ...)\nEND \"}\nListing 1.1.An example of training sample.\n16 Capitanelli and Mastrogiovanni\n3.3 Training\nAs far as the training process is concerned, we decided to run a complete 8000-\nsample training on the MACRO domain, since plans in the related dataset are\nshorter on average, and therefore supposedly easier to be learned by GPT-3. We\ncall this model Teriyaki-MACRO.\nWhen we train an LLM to perform action planning, the cost function asso-\nciated with the resulting generation process rewards linguistic coherence rather\nthan planning validity. This means that we are hypothesizinglinguistic coherence\nas a proxy for logical coherence, and therefore we must assume that validation\nduring the training process differs from the planning validity of the resulting\nmodel, whereas “planning validity” is defined as the percentage of plans that\nare (i) formally correct and (ii) able to reach the desired goal state, or a state\ncompatible with it. By “formally correct” we mean that each action’s arguments\nare consistent both with the specifications given in the planning domain and\nthe state of the modeled reality when that action is carried out, that is, the\naction preconditions are not violated. To be able to measure how planning va-\nlidity increases with the size of the provided training set, we decided to perform\nthe training process in steps. At each step, we provided samples to double the\ntotal size of the training set. Starting from the minimum amount of 500 samples,\nwe then trained the system with 1000, 2000, 4000, and 8000 total samples, and\nsaved a snapshot of the trained model at each step.\nAs anticipated above, the base model chosen for the training process is GPT-3\nand specifically text-davinci-002. Regarding the hyper-parameters, the num-\nber of training epochs was set to 2, while the batch size and the learning rate\nmultiplier were left to their default values. The batch size defaults to 0 .2% of\nthe number of examples in the training set, while the default learning rate is\nautomatically determined in a range from 0 .05 to 0 .2 depending on the final\nbatch size. We also highlight that since the model was effectively fine-tuned five\ntimes, the learning rate was reset to the default value at the beginning of each\nsession, with an effect similar to gradient descent with warm restarts [Loshchilov\nand Hutter, 2016]. We did provide the optional validation set and enabled the\ncompute_classification_metrics options to obtain training statistics for fur-\nther analysis. The total training cost for this procedure on a single planning\ndomain at the time of training (01/12/2022) was around 250 $.\nA Framework for Neurosymbolic Robot Action Planning using LLM 17\n3.4 Transfer Learning\nFor the NO-MACRO domain, we used the same methodology, but we decided\nto generate two candidate models. The first model is trained starting from\nthe text-davinci-002 model as before, whereas the second is trained starting\nfrom the MACRO model obtained previously. We call these models Teriyaki-\nNO-MACRO (davinci) and Teriyaki-NO-MACRO (MACRO). The hypothesis is\nthat as domains share many concepts and the MACRO model has already been\nprimed for them, the second candidate should reach a higher planning validity\nwith a smaller amount of training samples.\nResults shown in Section 4.2 confirmed this hypothesis, so we interrupted\ntraining when Teriyaki-NO-MACRO (MACRO) reached comparable results to\nits parent model and discarded entirely Teriyaki-NO-MACRO (davinci). Re-\ngarding the training dataset, the only difference with the example provided in\nListing 1.1 is that the prompt part is preceded by the \\n--NO-MACRO tag. This\ntag was introduced to test whether the model could be used to solve problems\nfor both the MACRO and the NO-MACRO domains, by simply adding the tag\nin our queries to the system. Unfortunately, the NO-MACRO models loses the\nability to solve problems in the MACRO domain, suggesting that to generate\nmodels that can solve multiple domains, training should be performed including\nexamples from all the domains of interest .\n3.5 Invoking the LLM and Streaming the Plan\nAfter the training phase, it is possible to query the model through an API\ncall by providing as a prompt the PDDL predicates describing the initial and\ngoal states for the robot and the articulated object, as one would do with a\ntraditional, symbolic PDDL-compatible action planner. Several parameters can\nbe configured, and below is a list of those that can impact the overall quality of\nthe resulting plan.\n– temperature is the most important as it controls howdeterministic the reply\n(and therefore the plan) will be. Keeping it at 0 corresponds to an argmax\noperation on the next token probability and ensures that we always get the\nmost likely result. While in a regular text generation some level of creativity\nis desirable, we strongly recommend keeping it at 0 for plan generation,\nespecially when robot actions are planned in the context of a human-robot\ncollaboration scenario.\n18 Capitanelli and Mastrogiovanni\n– presence_penalty and frequency_penalty can assume values between −2\nand 2, and can be used to reward or penalize repetitions in the generated\ntext. We have observed that setting the former to 2 seems to improve plan-\nning validity by 1 − 2% and vice versa , but at the moment we have not\ninvestigated enough this effect, so we decided to set the default value to 0\nfor both in our tests.\n– stop can be used to set a string as a stopping condition, meaning that\nwhen that string is generated by the model, the generation immediately\nterminates. Coherently with our training dataset, we set this value to END.\n– max_tokens controls the maximum response length, thus we recommend set-\nting it as high as possible to minimize the chance of a plan being truncated.\nSince each model has a maximum total prompt plus completion length, and\nthe worst-case prompt length depends on the planning domain at hand, this\nvalue should be assessed case by case. In our case, 1900 appears to be the\nhighest value for robust operation.\nTo stream the plan we can set the parameterstream to true. In this way, the\nmodel starts returning tokens as soon as they are available, instead of waiting\nfor the full sequence of tokens to be generated. This has no impact on the time\nneeded to receive the full answer, but it does reduce significantly the time needed\nto receive the first action, that is, the response time.\n3.6 An Architecture for Simultaneous Planning and Execution\nIn a traditional sense-plan-act data flow, the system works through the following\nsteps: (i) first collects data about the robot state and its workspace from endoge-\nnous and exogenous sensors; (ii) grounds such data into symbols to represent the\ncurrent state; (iii) generates an end-to-end plan to reach a given goal situation\nfrom the current state; (iv) begins to execute all the planned actions in sequence,\none after the other, possibly monitoring their outcomes and halting execution\nin case of unforeseen disturbances, errors and, in general, any misalignment be-\ntween the expected and the current state.\nWith Teriyaki, and building up on the work done by [Capitanelli et al., 2018],\nit is possible to modify this simple data flow as shown in Figure 3. While step\n(i) and step (ii) remain unchanged, now the data-driven synthesized planner can\noutput an action as soon as it has been generated by the model. This can be\nachieved simply by considering as an action every new line generated by the\nA Framework for Neurosymbolic Robot Action Planning using LLM 19\nFig. 3.A possible architecture integrating a generative Teriyaki planner to implement\nsimultaneous planning and execution. The dashed line below the planner represents\nthe fact that actions are generated one by one, and then buffered waiting for their\nexecution. The SPEM module checks that each action’s preconditions are satisfied\nbefore submitting it for execution.\nplanner. Since the generation of the plan cannot generally be paused, actions\nare stored in a first-in first-out buffer.\nIn order to monitor each action execution, we introduce a specific module,\nreferred to as Simultaneous Planning and Execution Monitor (SPEM). After each\naction has been executed, SPEM verifies two conditions: (a) it checks whether the\noverall goals have changed during the execution of the last action, for example,\ndue to new human actions; (b) it considers the most up to date current state\nand verifies, using the VAL validation utility, that such state is compliant with\nthe next action’s preconditions. If both conditions are met, SPEM submits the\nnext action to motion planning and execution; otherwise, it resets the planner\nand clears the action buffer. If this happens, planning can be restarted taking\ninto consideration the most up-to-date current and goal states.\nCondition (b) is essential because the planner continues to generate a plan\naction-by-action based on the state available when it was first initialized. Yet,\nthe current state changes continuously, either in expected ways, as a result of\nthe execution of actions, or in unexpected ways, as a result of unforeseen dis-\n20 Capitanelli and Mastrogiovanni\nturbances and human actions. In the second case, the next action might require\npreconditions that the planner believes are met but in reality, are not anymore.\nTherefore, SPEM both prevents catastrophic failures, and allows for seamlessly\nrecovering from disturbances, or adapting to human behavior. Considering that\nplanners generated with the Teriyaki workflow can generate a plan action-by-\naction, this recovery procedure only takes the time needed to generate a single\naction. Vice versa, in the traditional sense-plan-act data flow, the overall system\nwould be forced to stop the execution until a complete plan was generated by a\ntraditional planner.\nTwo remarks must be made. The first is that a module like SPEM would not\nbe necessary if we only generated a single action at a time, and restarted the\nplanner with the same goal state but updated knowledge of the world afterward.\nUnfortunately, in our tests, this operation modality does not lead to reasonable\nplans, as the planner loses the context of previously taken actions and then easily\nfalls into loops, such as, for example, centering the articulated object around a\ngiven joint, and then again centering it around another, and so on. This phe-\nnomenon is conceptually similar to the well-known Sussman Anomaly [Sussman,\n1973], and is completely dealt with by the functionality encoded in SPEM. The\nsecond is related to the fact that, conceptually speaking, this workflow is still\na traditional sense-plan-act architecture. Indeed, comparing this robot control\narchitecture side by side with the one presented by Capitanelli et al. [2018], the\npractical behavior of the robot would be indistinguishable, except for the reduced\nwaiting time whenever a replanning process is needed. For this reason, in Section\n4.4 we decided not to present execution timings and instead focus on a compar-\nison of waiting times before execution between traditional and Teriyaki-based\nplanners.\n4 Results\n4.1 Relation between Token and Planning Validity with an\nIncreasing Number of Training Samples\nIn Section 3.3, we pointed out that we use linguistic coherence as a proxy for\nlogical coherence. Figure 4 compares the evolution of the validation token accu-\nracy and the planning validity for the MACRO model, against the number of\nexamples used to train the model itself. On the one hand, the validation token\naccuracy measures the accuracy at predicting the next token, that is, approx-\nimately the next 4 characters of the response, in our case coinciding with the\nA Framework for Neurosymbolic Robot Action Planning using LLM 21\nFig. 4. Evolution of the token validation accuracy and the planning validity as the\nnumber of training examples increases. The blue line represents the evolution of the val-\nidation token accuracy during learning as reported by GPT-3 classification metrics. The\nbars represent the planning validity of the Teriyaki-MACRO, Teriyaki-NO-MACRO\n(davinci) and Teriyaki-NO-MACRO (MACRO) models. The red line represents our\nbaseline, that is, the percentage of plans solved by Probe.\nplan, on the validation set. On the other hand, we refer here to planning va-\nlidity as the percentage of plans in the 876-sample validation set that are both\nformally correct and reach the desired goal state, that is, which one passes the\nVAL validation utility test. For the former, data are retrieved from the classi-\nfication metrics reported by GPT-3 itself after training, where this information\nis available every 8 training steps. For the latter, we used the snapshots of the\nmodel taken after training with 500, 1000, 2000, 4000, and 8000 examples. Such\nsnapshots are used to plan against all 876 problems in the validation set in\nthe conditions described in Section 3.5. VAL is run on the obtained plans and\nfinally, the planning validity can be computed. In Figure 4, the evolution of\nplanning validity is represented by the orange bars to highlight the fact that the\nvalue is measured at each snapshot. It must also be noted that the parameter\nelapsed_examples does not correspond to the number of unique examples in\nthe training set, but it is scaled by a factor of 2 because we used two epochs for\ntraining, thus each example was used twice.\n22 Capitanelli and Mastrogiovanni\nIn Figure 4 the orange bars report the validation accuracy of the Teriyaki-\nMACRO model. It can be observed that GPT-3 reaches a very high validation\ntoken accuracy after the first 500 samples, as expected by a model well-known\nfor its few-shot learning capabilities. Nevertheless, planning validity rises at a\nmuch slower pace. Like in few-shot methods, the LLM cannot yet map precisely\nto admissible actions and even a single mistaken token could break a plan that\nwould be otherwise linguistically coherent. A very common mistake in plans gen-\nerated by the model trained with only 500 samples is that the model ignored the\nconditional effects of actions. As we correctly hypothesized, conditional effects\ncan be quite challenging as they imply propagating their effects downstream in\nthe plan to keep its overall semantic coherence. In this case, actions are correct\nand reasonably parameterized but they do not meet the necessary preconditions\nas they ignore that a given joint is in a state different from the one expected by\nthe model, due to the indirect effects of a previously planned action. Eventually,\nthe model reaches a very high 95% planning validity on the validation set after\ntraining over 8000 unique samples. Despite this, the model does not seem to\noverfit to the problems used for training, as shown by the results on the test set\npresented in Section 4.3. Hence, it appears that a larger training data set or a\nhigher number of epochs might result in a even higher planning validity.\nResults of planning validity are strikingly similar to those obtained by the\nPlansformer [Pallagani et al., 2023], yet they have been achieved with a training\nset of 8000 instead of 18000 and 2 training epochs instead of 3. It is worth noting\nthat we consider the Plansformer as one possible baseline for Teriyaki, because\nof the conceptual connections between the two approaches, that is, the use of\na fine-tuning process to adapt a foundational model with specific examples. It\nshould also be noted that in spite of the similar goal, Plansformer and Teriyaki\naim at demonstrating different features of the overall process: while Plansformer\nis focused on the fine-tuning process with the goal of demonstrating that it can be\nadapted to different planning domains, Teriyaki by purpose aims at demonstrat-\ning how tricky planning domains that arise in real-world robotic applications\ncan be managed by a data-driven, LLM-based planner, namely advanced plan-\nning concepts and longer problem-plan pairs. While our result could point to\nGPT-3 having stronger performance in this task than CodeT5, it is necessary to\nemphasize that the presence of different base models, the selection of a differ-\nent PDDL-like planner, the use of different PDDL constructs, and the different\nprompting strategy urge an analysis well-beyond the scope of this paper.\nA Framework for Neurosymbolic Robot Action Planning using LLM 23\nDuring this experiment, we also measured the number of planning attempts\nthat failed because of their excessive length. While this number was always small\ncompared with the validation set size, the number of failures decreased from 24\nin the first training step, down to 1 in the final model, further suggesting that\nthe model becomes better at generating shorter plans and avoiding unnecessary\nactions.\n4.2 Transfer Learning\nAs previously discussed in Section 3.4, the Teriyaki solver for the NO-MACRO\nplanning domain has been chosen starting from two base candidates, namely\nTeriyaki-NO-MACRO (davinci) and Teriyaki-NO-MACRO (MACRO). As we\ntrained the two models, we kept track of the planning validity as described\nin Section 3.4. The light pink and dark pink bars in Figure 4 report results\nin planning validity for the Teriyaki-NO-MACRO (davinci) and Teriyaki-NO-\nMACRO (MACRO) models, respectively. After 1000 samples, the Teriyaki-NO-\nMACRO (davinci) model reached a planning validity of 32 .5%, while Teriyaki-\nNO-MACRO (MACRO) reached 95.2%. Because of this result, we immediately\ndropped the former model, while we further trained the latter using 2000 sam-\nples. At this stage the Teriyaki-NO-MACRO model reached a validation plan-\nning validity of 98.8%, exceeding the 95% validation planning validity achieved\nby the MACRO model after 8000 samples. Because of this result, we decided\nnot to proceed with further training and from now on, when we simply refer to\nTeriyaki-NO-MACRO, we actually mean Teriyaki-NO-MACRO (MACRO).\nThere are two noteworthy aspects of this experiment.\nFirstly, Teriyaki-NO-MACRO (davinci) model still reached a higher planning\nvalidity at 1000 samples than Teriyaki-MACRO. This result might suggest that\nagainst our initial assumption, the NO-MACRO model is easier to learn for\nGPT-3 than the MACRO one. This result must be explored more in-depth, but\nit could be related to the number and quality of the actions in the planning\ndomain.\nSecondly, the transfer learning experiment within the Teriyaki framework\nbetween MACRO and NO-MACRO domains highlights its capacity for adap-\ntation to alternate knowledge representations. However, it merits noting that\nboth domains represent the same essential task of collaborative manipulation of\narticulated objects, possessing comparable action sets and state spaces. Conse-\n24 Capitanelli and Mastrogiovanni\n1.000\nPDDL\nproblems\nPDDL\ndomain\nProblem\ngenerator\nTeriyaki\nplanner\nVAL\nvalidator\nProbe\nplanner\nFig. 5. A diagram of Teriyaki testing process as described in Section 4.3.\nquently, the efficacy of transfer learning between two inherently distinct tasks\nalso remains an area for future exploration.\n4.3 Comparison of Solvers\nWe tested the performance of both Teriyaki-MACRO and Teriyaki-NO-MACRO\nmodels in terms of planning validity, plan length, and planning times on a test\nset of 1000 problem-plan pairs not previously used for training and validation\nin order to avoid cross-contamination, and we compared the results to the per-\nformance of the traditional action planner Probe. The process is summarized in\nFigure 5.\nWe want to remark that, due to the significantly different computing work-\nflows used to run Teriyaki and Probe (basically, a cloud-based architecture ac-\ncessed via proprietary APIs and software installed on a local machine, respec-\ntively), a fair comparison between them in terms of planning time is not possible.\nTherefore, results about planning times in this Section are only meant at provid-\ning a baseline for future work and to show how planning times scale differently\nfor each Teriyaki planner in the two different planning domains taken into con-\nsideration in this work. However, Probe remains a significant baseline approach\nbecause it is the planner used to get training data for Teriyaki; although Probe\nitself is not an optimal a planner in terms of plan quality, nonetheless one might\nrealistically expect that Teriyaki’s overall performance in plan generation should\nbe related to its outcomes.\nProbe ran on an Ubuntu 20.04.4 Windows Linux Subsystem (WSL) environ-\nment, deployed on a machine equipped with an 8-core Ryzen 3700X 3600 Mhz\nCPU and 16GB@3600 Mhz RAM. We recorded planning times as reported by\nthe planner itself together with each generated plan. As far as planning validity\nA Framework for Neurosymbolic Robot Action Planning using LLM 25\nTable 1.Summary of the tested models and date of testing\nModel Base model Test date Start Finish\nTeriyaki-MACRO davinci 18/02/2023 11:17:56 13:47:41\nTeriyaki-NO-MACRO MACRO 19/02/2023 17:22:24 21:25:58\nis concerned, all plans generated were valid, but we considered the instances in\nwhich the planner failed to generate a plan as failures.\nRegarding Teriyaki models, we prompted them using the settings described\nin Section 3.5, then verified the validity of the obtained plans using the same\nVAL validation tool employed at the data set generation phase. As the Ope-\nnAI API does not provide a method to log the exact execution time of a call,\nplanning times of Teriyaki solvers have been measured by disabling the option\nto stream GPT-3 responses, and recording the time it took each API call to\nreturn a full plan to the client application. For this reason, it must be noted\nthat it is impossible to discern how long each call has been queued by the cloud\napplication before execution. We assessed that the effect of queuing is not neg-\nligible as running tests after the official release of ChatGPT, another popular\nGPT-3 derived product by OpenAI, led to longer planning times than previously\nrecorded, possibly due to the increased traffic to the servers. In order to partly\nmitigate this effect, the tests presented here were performed during the weekend,\npreferably in the morning CET time. In Table 1 we also include the date and\nthe starting and finishing time of each test session for reference.\nFor all solvers and models, plan length has been computed simply as the\nnumber of actions in the plan. Table 2 compares the Teriyaki-MACRO and\nTeriyaki-NO-MACRO models against Probe in the respective domains, in terms\nof validity (in percentage), average plan length, maximum and average planning\ntimes, as well as the standard deviation of the planning times (in seconds).\nAs anticipated, Probe is faster than the trained Teriyaki solvers to generate\nplans end-to-end, in a traditional sense-plan-act workflow. Yet, Teriyaki solvers\nstill offer decent performance. Despite being trained on plans generated by Probe,\nTeriyaki models are capable of solving problems that Probe failed to process,\nand even generate shorter plans. The difference in plan length is only 1 .5% for\nthe Teriyaki-MACRO, but it raises to 13 .5% for Teriyaki-NO-MACRO, which\nin general leads to plans almost twice as long. This seems to suggest that the\ntraining procedure rewards shorter completions and that the effect might be\n26 Capitanelli and Mastrogiovanni\nTable 2.Comparison of Teriyaki models against Probe in their respective domains on\nthe test dataset\nSolver Domain Acc. [%] steps t max [s]t avg [s]t std [s]\nTeriyaki-MACRO MACRO 95.5 10.953 54.32 8.99 4.77\nProbe MACRO 98.6 11.111 36.71 2.11 3.47\nTeriyaki-NO-MACRONO-MACRO 94.0 19.158 54.71 14.61 7.16\nProbe NO-MACRO 98.6 22.137 43.77 2.79 3.30\nFig. 6.Comparison of the Teriyaki MACRO and NO-MACRO models’ planning times\nagainst Probe in their respective planning domains.\nstronger the longer the supposed completion gets. Nevertheless, this phenomenon\nrequires a more systematic investigation.\nFigure 6 allows for a better assessment of the timing performance of the\nproposed models, and it compares them to those generated by Probe. It can\nbe observed that Teriyaki models do actually scale with the combined length\nof the input and the output, as hypothesized. Planning times of the Teriyaki-\nNO-MACRO model are approximately twice of the Teriyaki-MACRO model,\nas expected considering that the plans of the former are approximately twice\nlonger than those of the latter. Box plots in the Figure associated with Teriyaki\nmodels have a very distinct shape when compared to those of Probe on both\nA Framework for Neurosymbolic Robot Action Planning using LLM 27\ndomains, which hints at an almost Gaussian distribution of planning times. This\nis coherent with the fact that the plans, which are generated from randomly\ninitialized problems, can assume any length.\n4.4 Action-by-action Plan Streaming\nOne of the major strengths of neurosymbolic action planning using LLMs is\nthat the plan can be streamed as it is being generated. If properly leveraged,\nthis feature can support action-by-action generation instead of adopting an end-\nto-end approach. In many applications involving the use of robots with a required\nflexible behavior, and especially when frequent re-planning is expected due to\nchanging conditions, simultaneous planning and execution by means of concur-\nrent (sets of) processes could greatly reduce the waiting time for a plan to be\navailable, and therefore executed. As we have previously pointed out, this may\nnicely correlate with a perceived interaction fluency in human-robot collabora-\ntion.\nIn Section 3.6, we present an enhanced sense-plan-act architecture that in-\ntegrates neurosymbolic planning to improve reactivity and resilience to dis-\nturbances, while yielding essentially the same plan outputs to traditional ap-\nproaches. Hence, in this section we focus on a quantitative comparison of the\nwait times.\nTo test the performance of Teriyaki in this regard, we set the Teriyaki-\nMACRO model to plan against our test dataset until it generated 1000 actions.\nAs discussed in Section 3.2, the data that we generated allows for a faithful\nrepresentation of the real world, while also enabling large-scale testing, without\nbiases induced by the limitations of the robotic platform.\nFigure 7 compares Teriyaki-MACRO single action timings against Probe tim-\nings to generate 1000 whole plans. Not only do we observe that wait times are\nreduced by 61.2% on average, but also the response time standard deviation\nis reduced from 3.47 to just 0.15. This corresponds to a 95.6% decrease and\nmakes Teriyaki much more predictable than the traditional search-based heuris-\ntic solver.\n4.5 Limitations and Scalability\nWe previously pointed out that, although our results are quite promising in so\nfar as they unlock the possibility of simultaneous planning and execution, the\n28 Capitanelli and Mastrogiovanni\nFig. 7. Comparison of the time it takes Teriyaki-MACRO to generate a single action\nagainst Probe on the MACRO domain.\ndata-driven synthesis of a planner as foreseen in the Teriyaki workflow cannot be\nconsidered a complete alternative to traditional action planning at this stage. In\nfact, a traditional, symbolic action planner is needed to obtain the problem-plan\npair data used for training the models and synthesizing the solver. Herewith, we\nmention the main current limitations in using the Teriyaki workflow.\n1. Unlike symbolic planners, our approach is domain-specific and requires train-\ning.\n2. In the current implementation end-to-end planning times are slower than\nthe baseline symbolic planner Probe, at least using GPT-3 as base LLM to\nimplement the overall architecture.\n3. Using Transformer-based LLMs, the maximum length of a problem-plan pair\nis constrained by the context window of the model, which is only 2048 tokens,\nor about 8000 characters, for GPT-3.\n4. Transformer-based LLMs have significant difficulties in generating math-\nematically accurate results, therefore, while we included advanced PDDL\nfeatures such as conditional effects, it might be difficult to extend the appli-\ncability of this approach to other relevant capabilities such as numerical and\ntemporal planning.\nA Framework for Neurosymbolic Robot Action Planning using LLM 29\nIn spite of these limitations, it is noteworthy that there are a number of\nLLMs, not necessarily based on the Transformer architecture, that are being\nmade available at the writing of this paper and which might help overcome or\nmitigate such limitations. For example, newer versions of GPT, such as GPT-\n3.5-Turbo-16k and GPT-4, extend the context window to 16 .000 and 128 .000\ntokens, respectively, and might effectively overcome limitation 3) for all practical\nplanning domains. Even better, new state space based architectures such as\nMamba [Gu and Dao, 2023] can be used to implement LLMs completely foregoing\nthe context window, while also presenting greater attention to details in long\nsequences, and faster inference times.\nApart from utilizing an earlier LLM, further optimizations can enhance\nTeriyaki’s effectiveness. Specifically, the planning duration in Teriyaki is affected\nby the total length of input and output, meaning the lengthiness of action names\nsignificantly impacts performance. As indicated in Figure 6, implementing a\nconcise action naming convention could align Teriyaki’s planning times closer\nto those seen with Probe. This underscores the necessity of balancing human\nreadability with efficiency.\nRegarding scalability, there are still numerous factors that require further\nexploration. Both our study and the work conducted by Pallagani et al. [2023]\nseem to indicate that sufficiently large models and training datasets, coupled\nwith effective training, enable LLMs to resolve generic planning domains ex-\npressed in PDDL syntax, which may include certain advanced elements such as\nconditional effects. However, current research has not established a definitive\ncorrelation between the intricacy of a planning domain and the requisite model\nsize and training data set needed to consistently address problems within that\ndomain. The primary challenge lies in the inability, to the best of our knowl-\nedge, to quantitatively gauge the complexity of a planning domain. At most,\ncomparisons between two planning domains can be made based on the dimen-\nsion of their state space and the quantity of actions inducing state transitions.\nNonetheless, a domain deemed more complex by these measures may, paradox-\nically, be simpler to resolve due to other characteristics of its state space and\nthe specifics of the planner. For traditional symbolic planners, this is influenced\nby their heuristic, whereas for LLM-based solvers, it depends on the underlying\nneural architecture.\nIn view of the favorable performance scaling discussed in this paper and\npossibilities offered by simultaneous planning and execution, we consider LLM-\n30 Capitanelli and Mastrogiovanni\nbased neurosymbolic planning as a promising approach worth considering for\nfuture investigation.\n5 Conclusions\nIn this paper, we introduce Teriyaki, a framework to generate neurosymbolic,\nPDDL-compliant planners based on GPT-3. In its current implementation,\nTeriyaki relies on a pragmatic and inexpensive procedure for the generation\nof a training dataset, which is based on the use of an existing action planner\nable to generate plans on the basis of randomly defined inputs. The current im-\nplementation, the data sets used for training and validation purposes, as well as\nthe ancillary code to generate all the training material from a PDDL-compliant\nplanner, is available open source. Training can leverage high-performance com-\nputing machinery in the cloud, and the resulting model can be deployed to any\nsoftware architecture for robots adopting standard PDDL-compatible syntax and\ninterfaces.\nThe major contribution of the paper is the empirical evidence that this ap-\nproach can be a viable solution for action planning, as well as a specific proof\nof concept of the overall workflow. In particular, we showed that: (i) planning\nvalidity is on par with that of a traditional, state-of-the-art, PDDL-compliant\nplanner, (ii) the average plan length is up to 13.5% shorter, (iii) it is possible\nto better use Teriyaki in scenarios where robots must re-plan frequently, and\nin particular due to its ability to generate plans action-by-action, thus reducing\naverage waiting time for a plan by 61.2%, with standard deviation by 95.6%.\nOverall, these results hints at a scenario in which the approach can fairly\nscale up in terms of the number of potentially supported planning domains,\neven though this remains one of the points to further investigate, and where\nfluency in human-robot interactions can greatly benefit of the increased reaction\ntimes of the robot action planning capabilities.\nConflict of Interest Statement\nThe authors declare that the research was conducted in the absence of any\ncommercial or financial relationships that could be construed as a potential\nconflict of interest.\nA Framework for Neurosymbolic Robot Action Planning using LLM 31\nAuthor Contributions\nAC: Conceptualization, Data curation, Software, Investigation, Writing – origi-\nnal draft. FM: Supervision, Writing – review & editing.\nAcknowledgments\nWe express heartfelt appreciation to Prof. Mauro Vallati of the University of\nHuddersfield for his guidance on task planning and for collaborating to the def-\ninition of the PDDL domains used in this work.\nData Availability Statement\nThe code and the datasets generated for this study can be found in the following\nrepository: https://github.com/alessiocpt/teriyaki.\nBibliography\nConstructions Aeronautiques, Adele Howe, Craig Knoblock, ISI Drew McDer-\nmott, Ashwin Ram, Manuela Veloso, Daniel Weld, David Wilkins SRI, An-\nthony Barrett, Dave Christianson, et al. Pddl— the planning domain definition\nlanguage. Technical Report, Tech. Rep., 1998.\nRiccardo Bertolucci, Alessio Capitanelli, Marco Maratea, Fulvio Mastrogiovanni,\nand Mauro Vallati. Automated planning encodings for the manipulation of\narticulated objects in 3d with gravity. In AI* IA 2019–Advances in Artificial\nIntelligence: XVIIIth International Conference of the Italian Association for\nArtificial Intelligence, Rende, Italy, November 19–22, 2019, Proceedings 18 ,\npages 135–150. Springer, 2019.\nRiccardo Bertolucci, Alessio Capitanelli, Carmine Dodaro, Nicola Leone, Marco\nMaratea, Fulvio Mastrogiovanni, and Mauro Vallati. Manipulation of articu-\nlated objects using dual-arm robots via answer set programming. Theory and\nPractice of Logic Programming, 21(3):372–401, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-\nplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. Language models are few-shot learners. Advances in\nneural information processing systems, 33:1877–1901, 2020.\nAlessio Capitanelli, Marco Maratea, Fulvio Mastrogiovanni, and Mauro Vallati.\nOn the manipulation of articulated objects in human–robot cooperation sce-\nnarios. Robotics and Autonomous Systems , 109:139–155, 2018.\nAlessandro Carf´ ı, Francesco Foglino, Barbara Bruno, and Fulvio Mastrogiovanni.\nA multi-sensor dataset for human-human handover.Data in Brief, 22:119–117,\n2019.\nMichael Cashmore, Maria Fox, Derek Long, Daniele Magazzeni, Bram Ridder,\nArnau Carrera, Narcis Palomeras, Natalia Hurtos, and Marc Carreras. Ros-\nplan: Planning in the robot operating system. In Proceedings of the inter-\nnational conference on automated planning and scheduling , volume 25, pages\n333–341, 2015.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\nde Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374 , 2021.\nA Framework for Neurosymbolic Robot Action Planning using LLM 33\nZhenfang Chen, Rui Sun, Wenjun Liu, Yining Hong, and Chuang Gan. Genome:\nGenerative neuro-symbolic visual reasoning by growing and reusing modules.\narXiv preprint arXiv:2311.04901 , 2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311 , 2022.\nRobert Dale. Gpt-3: What’s it good for? Natural Language Engineering, 27(1):\n113–118, 2021.\nKourosh Darvish, Francesco Wanderlingh, Barbara Bruno, Enrico Simetti, Ful-\nvio Mastrogiovanni, and Giuseppe Casalino. Flexible human-robot coopera-\ntion models for assisted shop-floor tasks. Mechatronics, 51:97–115, 2018.\nKourosh Darvish, Enrico Simetti, Fulvio Mastrogiovanni, and Giuseppe\nCasalino. A hierarchical architecture for human-robot cooperation processes.\nIEEE Transactions on Robotics, 37(2):567–586, 2021.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery,\nBrian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,\net al. Palm-e: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378, 2023.\nArtur d’Avila Garcez and Luis C Lamb. Neurosymbolic ai: the 3rd wave. arXiv\npreprint arXiv:2012.05876, 2020.\nC. R. Garrett, T. Lozano-Perez, and L. P. Kaelbing. PDDLstream: integrating\nsymbolic planners and blackbox samplers with optimistic adaptive planning.\nIn Proc. 30th International Conference on Automated Planning and Scheduling\n(ICAPS), Anywhere on Earth, October 2020.\nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective\nstate spaces. arXiv preprint arXiv:2312.00752 , 2023.\nWilliam Grant Hatcher and Wei Yu. A survey of deep learning: Platforms,\napplications and emerging research trends. IEEE Access, 6:24411–24432, 2018.\nMalte Helmert. The fast downward planning system. Journal of Artificial In-\ntelligence Research, 26:191–246, 2006.\nClint Heyer. Human-robot interaction and future industrial robotics applica-\ntions. In 2010 ieee/rsj international conference on intelligent robots and sys-\ntems, pages 4749–4754. IEEE, 2010.\nGuy Hoffman. Evaluating fluency in human–robot collaboration. IEEE Trans-\nactions on Human-Machine Systems , 49(3):209–218, 2019.\n34 Capitanelli and Mastrogiovanni\nRichard Howey, Derek Long, and Maria Fox. Val: Automatic plan validation,\ncontinuous effects and mixed initiative planning using pddl. In 16th IEEE\nInternational Conference on Tools with Artificial Intelligence , pages 294–301.\nIEEE, 2004.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language\nmodels as zero-shot planners: Extracting actionable knowledge for embodied\nagents. In International Conference on Machine Learning , pages 9118–9147.\nPMLR, 2022.\nNir Lipovetzky and Hector Geffner. Searching for plans with carefully designed\nprobes. In Proceedings of the International Conference on Automated Planning\nand Scheduling, volume 21, pages 154–161, 2011.\nLajanugen Logeswaran, Yao Fu, Moontae Lee, and Honglak Lee. Few-shot sub-\ngoal planning with language models. arXiv preprint arXiv:2205.14288 , 2022.\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm\nrestarts. arXiv preprint arXiv:1608.03983 , 2016.\nSimone Macci´ o, Alessandro Carf´ ı, and Fulvio Mastrogiovanni. A system for\nhierarchical planning in service mobile robotics. In 2022 IEEE International\nConference on Robotics and Automation , 2022.\nFulvio Mastrogiovanni, Antonio Sgorbissa, and Renato Zaccaria. A system for\nhierarchical planning in service mobile robotics. In 8th International Confer-\nence on Intelligent Autonomous Systems , 2004.\nTom´ aˇ s Mikolov, Stefan Kombrink, Luk´ aˇ s Burget, JanˇCernock` y, and Sanjeev\nKhudanpur. Extensions of recurrent neural network language model. In\n2011 IEEE international conference on acoustics, speech and signal processing\n(ICASSP), pages 5528–5531. IEEE, 2011.\nPrajval Kumar Murali, Kourosh Darvish, and Fulvio Mastrogiovanni. Deploy-\nment and evaluation of a flexible human-robot collaboration model based on\nand/or graphs in a manufacturing environment. Intelligent Service Robotics ,\n13:439–457, 2020.\nAchraf Oussidi and Azeddine Elhassouny. Deep generative models: Survey.\nIn 2018 International conference on intelligent systems and computer vision\n(ISCV), pages 1–8. IEEE, 2018.\nVishal Pallagani, Bharath Muppasani, Biplav Srivastava, Francesca Rossi, Lior\nHoresh, Keerthiram Murugesan, Andrea Loreggia, Francesco Fabiano, Rony\nJoseph, Yathin Kethepalli, et al. Plansformer tool: Demonstrating generation\nof symbolic plans using transformers. In IJCAI, volume 2023, pages 7158–\n7162. International Joint Conferences on Artificial Intelligence, 2023.\nA Framework for Neurosymbolic Robot Action Planning using LLM 35\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´ c,\nDaniel Hesslow, Roman Castagn´ e, Alexandra Sasha Luccioni, Fran¸ cois Yvon,\nMatthias Gall´ e, et al. Bloom: A 176b-parameter open-access multilingual lan-\nguage model. arXiv preprint arXiv:2211.05100 , 2022.\nTom Silver, Varun Hariprasad, Reece S Shuttleworth, Nishanth Kumar, Tom´ as\nLozano-P´ erez, and Leslie Pack Kaelbling. Pddl planning with pretrained large\nlanguage models. In NeurIPS 2022 Foundation Models for Decision Making\nWorkshop, 2022.\nTom Silver, Soham Dan, Kavitha Srinivas, Joshua B Tenenbaum, Leslie Pack\nKaelbling, and Michael Katz. Generalized planning in pddl domains with\npretrained large language models. arXiv preprint arXiv:2305.11014 , 2023.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu,\nJonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Prog-\nprompt: Generating situated robot task plans using large language models. In\n2023 IEEE International Conference on Robotics and Automation (ICRA) ,\npages 11523–11530. IEEE, 2023.\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam\nRajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,\nVijay Korthikanti, et al. Using deepspeed and megatron to train megatron-\nturing nlg 530b, a large-scale generative language model. arXiv preprint\narXiv:2201.11990, 2022.\nChan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun\nChao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied\nagents with large language models. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision , pages 2998–3009, 2023.\nMartin Sundermeyer, Ralf Schl¨ uter, and Hermann Ney. Lstm neural networks\nfor language modeling. In Thirteenth annual conference of the international\nspeech communication association, 2012.\nGerald J Sussman. A computational model of skill acquisition. 1973.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-\nshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\net al. Lamda: Language models for dialog applications. arXiv preprint\narXiv:2201.08239, 2022.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023.\n36 Capitanelli and Mastrogiovanni\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kamb-\nhampati. Large language models still can’t plan (a benchmark for llms on\nplanning and reasoning about change).arXiv preprint arXiv:2206.10498, 2022.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. Advances in neural information processing systems , 30, 2017.\nNaoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Kat-\nsushi Ikeuchi. Chatgpt empowered long-step robot control in various environ-\nments: A case application. arXiv preprint arXiv:2304.03893 , 2023.\nBailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A Saurous, and Yoon\nKim. Grammar prompting for domain-specific language generation with large\nlanguage models. arXiv preprint arXiv:2305.19234 , 2023.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-\naware unified pre-trained encoder-decoder models for code understanding and\ngeneration. arXiv preprint arXiv:2109.00859 , 2021.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6855544447898865
    },
    {
      "name": "Task (project management)",
      "score": 0.5641331672668457
    },
    {
      "name": "Robot",
      "score": 0.5173296332359314
    },
    {
      "name": "Action plan",
      "score": 0.5138912200927734
    },
    {
      "name": "Action (physics)",
      "score": 0.4859139025211334
    },
    {
      "name": "Fluency",
      "score": 0.4594394266605377
    },
    {
      "name": "Plan (archaeology)",
      "score": 0.42630574107170105
    },
    {
      "name": "Scalability",
      "score": 0.4228998124599457
    },
    {
      "name": "Human–computer interaction",
      "score": 0.38714170455932617
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38260969519615173
    },
    {
      "name": "Process management",
      "score": 0.3244410753250122
    },
    {
      "name": "Psychology",
      "score": 0.14391851425170898
    },
    {
      "name": "Engineering",
      "score": 0.14083480834960938
    },
    {
      "name": "Systems engineering",
      "score": 0.09234675765037537
    },
    {
      "name": "Management",
      "score": 0.08993837237358093
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    }
  ]
}