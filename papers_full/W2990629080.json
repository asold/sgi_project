{
  "title": "Factorized Multimodal Transformer for Multimodal Sequential Learning",
  "url": "https://openalex.org/W2990629080",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2422721600",
      "name": "Zadeh, Amir",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4302665484",
      "name": "Mao, Chengfeng",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Shi, Kelly",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1892629360",
      "name": "Zhang, Yiwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221452030",
      "name": "Liang, Paul Pu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742604909",
      "name": "Poria, Soujanya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221382741",
      "name": "Morency, Louis-Philippe",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963540523",
    "https://openalex.org/W2901272442",
    "https://openalex.org/W2029996593",
    "https://openalex.org/W2619383789",
    "https://openalex.org/W2886193235",
    "https://openalex.org/W2075398069",
    "https://openalex.org/W2465534249",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2397375888",
    "https://openalex.org/W2109020278",
    "https://openalex.org/W2963871344",
    "https://openalex.org/W2883409523",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2767249564",
    "https://openalex.org/W2101534792",
    "https://openalex.org/W1989085630",
    "https://openalex.org/W1966797434",
    "https://openalex.org/W2613526370",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2920847544",
    "https://openalex.org/W2936420065",
    "https://openalex.org/W2519656895",
    "https://openalex.org/W2261271299",
    "https://openalex.org/W2149084152",
    "https://openalex.org/W2964010806",
    "https://openalex.org/W2095176743",
    "https://openalex.org/W2111645492",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2007321142",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2546919788",
    "https://openalex.org/W2070353225",
    "https://openalex.org/W2962718314",
    "https://openalex.org/W2951431783",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2412400526",
    "https://openalex.org/W2963252191",
    "https://openalex.org/W2402955625",
    "https://openalex.org/W2798965674",
    "https://openalex.org/W2117497855",
    "https://openalex.org/W2532034655",
    "https://openalex.org/W1670132599",
    "https://openalex.org/W2016730668"
  ],
  "abstract": "The complex world around us is inherently multimodal and sequential (continuous). Information is scattered across different modalities and requires multiple continuous sensors to be captured. As machine learning leaps towards better generalization to real world, multimodal sequential learning becomes a fundamental research area. Arguably, modeling arbitrarily distributed spatio-temporal dynamics within and across modalities is the biggest challenge in this research area. In this paper, we present a new transformer model, called the Factorized Multimodal Transformer (FMT) for multimodal sequential learning. FMT inherently models the intramodal and intermodal (involving two or more modalities) dynamics within its multimodal input in a factorized manner. The proposed factorization allows for increasing the number of self-attentions to better model the multimodal phenomena at hand; without encountering difficulties during training (e.g. overfitting) even on relatively low-resource setups. All the attention mechanisms within FMT have a full time-domain receptive field which allows them to asynchronously capture long-range multimodal dynamics. In our experiments we focus on datasets that contain the three commonly studied modalities of language, vision and acoustic. We perform a wide range of experiments, spanning across 3 well-studied datasets and 21 distinct labels. FMT shows superior performance over previously proposed models, setting new state of the art in the studied datasets.",
  "full_text": "FACTORIZED MULTIMODAL TRANSFORMER FOR MUL-\nTIMODAL SEQUENTIAL LEARNING\nAmir Zadeh‚Ä†, Chengfeng Mao‚Ä†, Kelly Shi‚Ä†, Yiwei Zhang‚Ä†, Paul Liang‚ãÑ,\nSoujanya Poria‚ãÜ & Louis-Philippe Morency‚Ä†\n‚Ä†Language Technologies Institute, Machine Learning Department‚ãÑ, SCS, CMU\n‚ãÜSingapore University of Technology and Design\n{abagherz,chengfem,jiaxins1,yiweizh2,pliang}@cs.cmu.edu\nsoujanya poria@sutd.edu.sg,morency@cs.cmu.edu\nABSTRACT\nThe complex world around us is inherently multimodal and sequential (continuous).\nInformation is scattered across different modalities and requires multiple continu-\nous sensors to be captured. As machine learning leaps towards better generalization\nto real world, multimodal sequential learning becomes a fundamental research area.\nArguably, modeling arbitrarily distributed spatio-temporal dynamics within and\nacross modalities is the biggest challenge in this research area. In this paper, we\npresent a new transformer model, called the Factorized Multimodal Transformer\n(FMT) for multimodal sequential learning. FMT inherently models the intramodal\nand intermodal (involving two or more modalities) dynamics within its multimodal\ninput in a factorized manner. The proposed factorization allows for increasing\nthe number of self-attentions to better model the multimodal phenomena at hand;\nwithout encountering difÔ¨Åculties during training (e.g. overÔ¨Åtting) even on relatively\nlow-resource setups. All the attention mechanisms within FMT have a full time-\ndomain receptive Ô¨Åeld which allows them to asynchronously capture long-range\nmultimodal dynamics. In our experiments we focus on datasets that contain the\nthree commonly studied modalities of language, vision and acoustic. We perform a\nwide range of experiments, spanning across 3 well-studied datasets and 21 distinct\nlabels. FMT shows superior performance over previously proposed models, setting\nnew state of the art in the studied datasets.\n1 I NTRODUCTION\nIn many naturally occurring scenarios, our perception of the world is multimodal. For example,\nconsider multimodal language (face-to-face communication), where modalities of language, vision\nand acoustic are seamlessly used together for communicative intent (Kottur et al., 2019). Such\nscenarios are widespread in everyday life, where continuous sensory perceptions form multimodal\nsequential data. Each modality within multimodal data exhibits exclusive intramodal dynamics, and\npresents a unique source of information. Modalities are not fully independent of each other. Relations\nacross two (bimodal) or more (trimodal, . . . ) of them formintermodal dynamics; often asynchronous\nspatio-temporal dynamics which bind modalities together (Zadeh et al., 2017).\nLearning from multimodal sequential data has been an active, yet challenging research area within\nthe Ô¨Åeld of machine learning (Baltru Àásaitis et al., 2018). Various approaches relying on graphical\nmodels or RNNs have been proposed for multimodal sequential learning. Transformer models\nare a new class of neural models that rely on a carefully designed non-recurrent architecture for\nsequential modeling (Vaswani et al., 2017). Their superior performance is attributed to a self-attention\nmechanism, which is uniquely capable of highlighting related information across a sequence. This\nself-attention is a particularly appealing mechanism for multimodal sequential learning, as it can\nbe modiÔ¨Åed into a strong neural component for Ô¨Ånding relations between different modalities (the\ncornerstone of this paper). In practice, numerous such relations may simultaneously exist within\nmultimodal data, which would require increasing the number of attention units (i.e. heads). Increasing\n1\narXiv:1911.09826v1  [cs.LG]  22 Nov 2019\nthe number of attentions in an efÔ¨Åcient and semantically meaningful way inside a transformer model,\ncan boost the performance in modeling multimodal sequential data.\nIn this paper, we present a new transformer model for multimodal sequential learning, called Fac-\ntorized Multimodal Transformer (FMT). FMT is capable of modeling asynchronous intramodal\nand intermodal dynamics in an efÔ¨Åcient manner, within one single transformer network. It does\nso by speciÔ¨Åcally accounting for possible sets of interactions between modalities (i.e. factorizing\nbased on combinations) in a Factorized Multimodal Self-attention (FMS) unit. We evaluate the\nperformance of FMT on multimodal language: a challenging type of multimodal data which exhibits\nidiosyncratic and asynchronous spatio-temporal relations across language, vision and acoustic modal-\nities. FMT is compared to previously proposed approaches for multimodal sequential learning over\nmultimodal sentiment analysis (CMU-MOSI) (Zadeh et al., 2016), multimodal emotion recognition\n(IEMOCAP) (Busso et al., 2008), and multimodal personality traits recognition (POM) (Park et al.,\n2014).\n2 R ELATED WORKS\nThe related works to studies in this paper fall into two main areas.\n2.1 M ULTIMODAL SEQUENTIAL LEARNING\nModeling multimodal sequential data is among the core research areas within the Ô¨Åeld of machine\nlearning. In this area, previous work can be classiÔ¨Åed into two main categories.\nThe Ô¨Årst category of models, and arguably the simplest, are models that use early or late fusion.\nEarly fusion uses feature concatenation of all modalities into a single modality. Subsequently, the\nmultimodal sequential learning task is treated as a unimodal one and tackled using unimodal sequential\nmodels such as Hidden Markov Models (HMMs) (Baum & Petrie, 1966), Hidden Conditional Random\nFields (HCRFs) (Quattoni et al., 2007; Morency et al., 2007), and RNNs (e.g. LSTMs Hochreiter &\nSchmidhuber (1997)). While such models are often successful for real unimodal data (i.e. not feature\nconcatenated multimodal data), they lack the necessary components to deal with multimodal data\noften causes suboptimal performance (Xu et al., 2013). Contrary to early fusion which concatenates\nmodalities at input level, late fusion models have relied on learning ensembles of weak classiÔ¨Åers\nfrom different modalities (Snoek et al., 2005; Vielzeuf et al., 2017; Nojavanasghari et al., 2016).\nHybrid methods have also been used to combine early and late fusion together (Wu et al., 2019;\nNguyen & Okatani, 2018; Lazaridou et al., 2015; Hu et al., 2017).\nThe second category of models comprise of models speciÔ¨Åcally designed for multimodal data.\nMultimodal variations of graphical models have been proposed, including Multi-view HCRFs where\nthe potentials of the HCRF are changed to facilitate multiple modalities Song et al. (2012; 2013).\nMultimodal models based on LSTMs include Multi-view LSTMs Rajagopalan et al. (2016), Memory\nFusion Network (Zadeh et al., 2018a) with its recurrent and graph variants (Liang et al., 2018;\nZadeh et al., 2018c), as well as Multi-attention Recurrent Networks (Zadeh et al., 2018b). Studies\nhave also proposed generic fusion techniques that can be used in various models including Tensor\nFusion (Zadeh et al., 2017) and its approximate variants (Liang et al.; Liu et al., 2018), as well as\nCompact Bilinear Pooling (Gao et al., 2015; Fukui et al., 2016; Kim et al., 2016).\nMany of these models, from both Ô¨Årst and second categories, are used as baselines in this paper.\n2.2 T RANSFORMER MODEL\nTransformer is a non-recurrent neural architecture designed for modeling sequential data (Vaswani\net al., 2017). It has shown superior performance across multiple NLP tasks when compared to\nRNN-based or convolutional architectures (Devlin et al., 2018; Vaswani et al., 2017). This superior\nperformance of Transformer model is largely credited to a self-attention; a neural component that\nallows for efÔ¨Åciently extracting both short and long-range dependencies within its input sequence\nspace. Transformer models have been successfully applied to various areas within machine learning\nincluding NLP and computer vision (Yang et al., 2019; Parmar et al., 2018; Alsentzer et al., 2019).\nExtending transformer to multimodal domains, specially for structured multimodal sequences is\nrelatively understudied; with the previous works mainly focusing on using transformer models for\n2\nmodality alignment using cross-modal links between single transformers for each modality (Tsai\net al., 2019).\n3 F ACTORIZED MULTIMODAL TRANSFORMER (FMT) M ODEL\nIn this section, we outline the proposed Factorized Multimodal Transformer1 (FMT). Figure 1 shows\nthe overall structure of the FMT model. The input Ô¨Årst goes through an embedding layer, followed by\nmultiple Multimodal Transformer Layers (MTL). Each MTL consists of multiple Factorized Multi-\nmodal Self-attentions (FMS). FMS explicitly accounts for intramodal and intermodal factors within\nits multimodal input. S1 and S2 are two summarization networks. They are necessary components\nof FMT which allow for increasing the number of attentions efÔ¨Åciently, without overparameterization\nof the FMT.\n3.1 I NPUT EMBEDDING\nMultimodal Transformer Layer (MTL)ùêæ√ó\nEmbeddingInput    (ùë•%)\nOutput     ('ùë•%()\nùëÜ1 ‚Ä¶\n‚Ä¶\n‚Ä¶\nFactorized Multimodal Self-attention (FMS)\nùëÜ2FFFFFFFFAdd & Norm\nFigure 1: Overview of the proposed Fac-\ntorized Multimodal Transformer (FMT)\nmodel.\nConsider a multimodal sequential dataset with constituent\nmodalities of language, vision and acoustic. The modali-\nties are denoted as {L, V, A}from hereon for abbreviation.\nAfter resampling using a reference clock, modalities can\nfollow the same frequency (Chen et al., 2017). Essentially,\nthis resampling is often based on word timestamps (i.e.\nword alignment). Subsequently, the dataset can be denoted\nas:\nD =\n{\nxi =\n[\nx(t,i) = ‚ü®l(t,i), v(t,i), a(t,i)‚ü©\n]t=Ti\nt=1 , yi\n}N\ni=1\nxi ‚ààRTi√ódx, yi ‚ààRdy are the inputs and labels. x(t,i) =\n‚ü®l(t,i), v(t,i), a(t,i)‚ü©is a triplet of language, visual and audio\ninputs for timestamp t in i-th datapoint. N is the total\nnumber of samples within the dataset, and Ti the total\nnumber of timestamps within i-th datapoint. Zero paddings\n(on the left) can be used to unify the length of all sequences\nto a desired Ô¨Åxed length T. dx = dL + dV + dA denotes\nthe dimensionality of input at each timestep, which in turn\nis equal to the sum of dimensionality of each modality. dy\ndenotes the dimensionality of the associated labels of a\nsequence.\nAt the Ô¨Årst step within the FMT model, each modality is\npassed to a unimodal embedding layer with the operation\nEM‚àà{L,V,A}(¬∑); RdM‚àà{L,V,A} ‚Ü¶‚ÜíReM‚àà{L,V,A}. In turn, EM takes as input m(t,i); m ‚àà{l, v, a}. Positional\nembeddings are also added to the input at this stage. The output of the embeddings collectively form\nÀÜx0 = ‚ü®ÀÜl0, ÀÜv0, ÀÜa0‚ü©. We denote the dimensionality of this output as ex = eL + eV + eA.\n3.2 M ULTIMODAL TRANSFORMER LAYER (MTL)\nAfter the initial embedding, FMT now consists of a stack of Multimodal Transformer Layers (MTL).\nMTL 1) captures factorized dynamics within multimodal data in parallel, and 2) aligns the time-\nasynchronous information both within and across modalities. Both of these are achieved using\nmultiple Factorized Multimodal Self-attentions (FMS), each of which has multiple specialized self-\nattentions inside. The high dimensionality of the intermediate attention outputs within MTL and FMS\nis controlled using two distinct summarization networks. The continuation of this section provides\ndetailed explanation of the inner-operations of MTL.\nLet ÀÜxk\ni = ‚ü®ÀÜlk\n(¬∑,i), ÀÜvk\n(¬∑,i), ÀÜak\n(¬∑,i)‚ü©denote the input to the k-th MTL. We assume a total of K MTLs in a\nFMT (indexed 0 . . . K‚àí1), with k = 0being the output of the embedding layer (input tok = 0MTL).\n1Code (release April 15th, 2020): https://github.com/A2Zadeh/Factorized-Multimodal-Transformer, Public\nData: https://github.com/A2Zadeh/CMU-MultimodalSDK\n3\n. . . . . . . . . . . . . . . . . . . . . \n. . . \n. . . \n[L][V][A][L,V][L,A][V,A][L,V,A]\n. . . . . . . . . . . . . . . . . . . . . \nSA L,V,ASA V,ASA L,ASA L,VSA ASA VSAL\nAdd &NormùëÜ1\n#ùë•(&)()ùëô(+,&)( #ùë£(+,&)( #ùëé(+,&)( )ùëô(/,&)( #ùë£(/,&)( #ùëé(/,&)( )ùëô(ùíØ,&)( #ùë£(ùíØ,&)( #ùëé(ùíØ,&)( Factorized Multimodal Self-attention (FMS)\nOutput (to MTL)\nFigure 2: Best viewed in color. Overview of a single Factorized Multimodal Self-attention (FMS) in\nk-th MTL. SA is self-attention (Vaswani et al., 2017) with full time-domain receptive Ô¨Åeld (0 . . .T).\nThe grayed areas are for demonstration purposes, and not a part of the implementation.\nThe input of MTL, immediately goes through one/multiple2 Factorized Multimodal Self-attentions\n(FMS). The operations inside a single Factorized Multimodal Self-attention is demonstrated in Figure\n2. For 3 modalities3, there exist 7 distinct attentions inside a single FMS unit. Each attention has a\nunique receptive Ô¨Åeld with respect to modalities f ‚ààF = {L,V,A,LV,LA,VA,LVA}; essentially\ndenoting the modalities visible to the attention. Using this factorization, FMS explicitly accounts for\npossible unimodal, bimodal and trimodal interactions existing within the multimodal input space. All\nattentions within a FMS extend to the length of the sequence, and therefore can extract asynchronous\nrelations within and across modalities. For f ‚ààF, each attention within a single FMS unit is\ncontrolled by the Key Kf , Query Qf , and Value V f all with dimensionality RT√óT ; parameterized\nrespectively using afÔ¨Åne maps WKf , WQf , and WV f . After the attention is applied using Key,\nQuery and Value operations (Vaswani et al., 2017), the output of each of the attentions goes through\na residual addition with its perceived input (input in the attention receptive Ô¨Åeld), followed by a\nnormalization.\nThe output of the FMS contains the aligned and extracted information from the unimodal, bimodal\nand trimodal factors. This output is high-dimensional; essentially R4√óT√óex (each dimension within\ninput of shape T√ó ex is present in 4 factors). Our goal is to reduce this high-dimensional data using a\nmapping from R4√óT√óex ‚Ü¶‚ÜíRT√óex. Without overparameterizing the FMS, in practice, we observed\nthis mapping can be efÔ¨Åciently done using a simple 1D convolutional network S1M‚àà{L,V,A}(¬∑); R4 ‚Ü¶‚Üí\nR. Internally, S1(¬∑) maps its input to multiple layers of higher dimensions and subsequently to R.\nUsing language as an example, S1L moves across language modality dimensions eL for t = 1. . .T\nand summarizes the information across all the factors. The output of this summarization applied on\nall modality dimensions and timesteps, is the output of FMS, which has the dimensionality RT√óex.\nIn practice, there can be various possible unimodal, bimodal or trimodal interactions within a\nmultimodal input. For example, consider multiple sets of important interactions between L and V (e.g.\nsmile + positive word, as well as eyebrows up + excited phrase), all of which need to be highlighted\nand extracted. A single FMS may not be able to highlight all these interactions without diluting its\nintrinsic attentions. Multiple FMS can be used inside a MTL to efÔ¨Åciently extract diverse multimodal\n2Multiple FMS have the same time-domain receptive Ô¨Åeld, which is equal to the length of the input. This is\ncontrary to the implementations of the transformer model that split the sequence based on number of attention\nheads.\n3Remarks on more than 3 modalities is in Appendix A.4.\n4\nModel \\Metric BA F1 MAE Corr\nMV-LSTM (Rajagopalan et al., 2016) 73.9/‚Äì 74.0/‚Äì 1.019 0.601\nTFN (Zadeh et al., 2017) 73.9/‚Äì 73.4/‚Äì 1.040 0.633\nMARN (Zadeh et al., 2018b) 77.1/‚Äì 77.0/‚Äì 0.968 0.625\nMFN (Zadeh et al., 2018a) 77.4/‚Äì 77.3/‚Äì 0.965 0.632\nRMFN (Liang et al., 2018) 78.4/‚Äì 78.0/‚Äì 0.922 0.681\nRA VEN (Wang et al., 2018) 78.0/‚Äì ‚Äì/‚Äì 0.915 0.691\nMulT (Tsai et al., 2019) ‚Äì/83.0 ‚Äì/82.8 0.87 0.698\nFMT (ours) 81.5/83.5 81.4/83.5 0.837 0.744\nTable 1: FMT achieves superior performance over baseline models for CMU-MOSI dataset (mul-\ntimodal sentiment analysis). We report BA (binary accuracy) and F1 (both higher is better), MAE\n(Mean-absolute Error, lower is better), and Corr (Pearson Correlation CoefÔ¨Åcient, higher is better).\nFor BA and F1, we report two numbers: the number on the left side of ‚Äú/‚Äù is calculated based on\napproach taken by Zadeh et al. (2018b), and the right side is by Tsai et al. (2019).\ninteractions existing in the input data4. Consider a total of U FMS units inside a MTL. The output\nof each FMS goes through a feedforward network (for each timestamp t of the FMS output). The\noutput of this feedfoward network is residually added with its input, and subsequently normalized.\nThe feedforward network is the same across all U FMS units and timestamps t. Subsequently, the\ndimensionality of the output of the normalizations collectively is RU√óT√óex. Similar to operations\nperformed by S1, a secondary summarization network S2M‚àà{L,V,A}(¬∑); RU ‚Ü¶‚ÜíR can be used here.\nS2 is also a 1D convolutional network that moves across modality dimensions and different timesteps\nto map RU√óT√óex to RT√óex. The output of the secondary summarization network is the Ô¨Ånal output\nof MTL, and denoted as ÀÜxk+1\ni .\nLet ÀÜxK\ni = ‚ü®ÀÜlK\n(¬∑,i), ÀÜvK\n(¬∑,i), ÀÜaK\n(¬∑,i)‚ü©be the output of last MTL in the stack. For supervision, we feed this\ninput one timestamp at a time as input to a Gated Recurrent Unit (GRU) (Cho et al., 2014). The\nprediction is conditioned on output at timestamp t = T of the GRU, using an afÔ¨Åne map to dy.\n4 E XPERIMENTAL METHODOLOGY\nIn this section, we discuss the experimental methodology including tasks, datasets, computational\ndescriptors, and comparison baselines.\n4.1 T ASKS AND DATASETS\nThe following inherently multimodal tasks (and accompanied datasets) are studied in this paper. All\nthe tasks are related to multimodal language: a complex and idiosyncratic sequential multimodal\nsignal, where semantics are arbitrarily scattered across modalities (Holler & Levinson, 2019).\nMultimodal Sentiment Analysis: The Ô¨Årst benchmark in our experiments is multimodal sentiment\nanalysis, where the goal is to identify a speaker‚Äôs sentiment based on the speaker‚Äôs display of verbal\nand nonverbal behaviors. We use the well-studied CMU-MOSI (CMU Multimodal Opinion Sentiment\nIntensity) dataset for this purpose (Zadeh et al., 2016). There are a total of 2199 data points (opinion\nutterances) within CMU-MOSI dataset. The dataset has real-valued sentiment intensity annotations\nin the range [‚àí3, +3]. It is considered a challenging dataset due to speaker diversity (1 video per\ndistinct speaker), topic variations and low-resource setup.\nMultimodal Emotion Recognition: The second benchmark in our experiments is multimodal emotion\nrecognition, where the goal is to identify a speaker‚Äôs emotions based on the speaker‚Äôs verbal and\nnonverbal behaviors. We use the well-studied IEMOCAP dataset (Busso et al., 2008). IEMOCAP\nconsists of 151 sessions of recorded dialogues, of which there are 2 speaker‚Äôs per session for a total\nof 302 videos across the dataset. We perform experiments for discrete emotions (Ekman, 1992) of\n4We study the impact of number of FMS units inside MTL in Section 5.\n5\nModel \\Emotion Happy Sad Angry Neutral\nMetric BA F1 BA F1 BA F1 BA F1\nMV-LSTM (Rajagopalan et al., 2016) 85.9 81.3 80.4 74.0 85.1 84.3 67.0 66.7\nMARN (Zadeh et al., 2018a) 86.7 83.6 82.0 81.2 84.6 84.2 66.8 65.9\nMFN (Zadeh et al., 2018a) 86.5 84.0 83.5 82.1 85.0 83.7 69.6 69.2\nRMFN (Liang et al., 2018) 87.5 85.8 82.9 85.1 84.6 84.2 69.5 69.1\nRA VEN (Wang et al., 2018) 87.3 85.8 83.4 83.1 87.3 86.7 69.7 69.3\nMulT (Tsai et al., 2019) 90.7 88.6 86.7 86.0 87.4 87.0 72.4 70.7\nFMT 88.8 87.2 88.0 87.7 89.7 89.5 74.0 73.8\nTable 2: FMT achieves superior performance over baseline models (with the exception of Happy\nemotion) for discrete emotions in IEMOCAP (multimodal emotion recognition). We report BA\n(binary accuracy) and F1 (both higher is better).\nModel \\Trait Con Pas Voi Dom Cre Viv Exp Ent\nMA7 MA7 MA7 MA7 MA7 MA7 MA7 MA7\nMV-LSTM (Rajagopalan et al., 2016) 25.6 28.6 28.1 34.5 25.6 32.5 32.5 29.6\nTFN (Zadeh et al., 2017) 24.1 31.0 31.5 34.5 24.6 25.6 27.6 29.1\nMARN (Zadeh et al., 2018b) 29.1 33.0 - - 31.5 - - -\nMFN (Zadeh et al., 2018a) 34.5 35.5 37.4 41.9 34.5 36.9 36.0 37.9\nRMFN (Liang et al., 2018) 37.4 38.4 37.4 - 37.4 38.9 38.9 -\nMulT (Tsai et al., 2019) 34.5 34.5 36.5 38.9 37.4 36.9 37.9 39.4\nFMT 40.9 42.4 42.4 44.3 41.4 39.4 41.4 39.4\nModel \\Trait Res Tru Rel Out Tho Ner Per Hum\nMA5 MA5 MA5 MA5 MA5 MA5 MA7 MA5\nMV-LSTM (Rajagopalan et al., 2016) 33.0 52.2 50.7 38.4 37.9 42.4 26.1 38.9\nTFN (Zadeh et al., 2017) 30.5 38.9 35.5 37.4 33.0 42.4 27.6 33.0\nMARN (Zadeh et al., 2018b) 36.9 - 52.2 - - 47.3 31.0 44.8\nMFN (Zadeh et al., 2018a) 38.4 57.1 53.2 46.8 47.3 47.8 34.0 47.3\nRMFN (Liang et al., 2018) 39.4 - 53.7 - 48.3 48.3 35.0 46.8\nMulT (Tsai et al., 2019) 41.4 60.6 54.2 43.3 49.3 46.3 33.5 43.3\nFMT 44.8 61.1 57.6 51.7 51.7 51.2 40.4 48.3\nTable 3: FMT achieves superior performance over baseline models in POM dataset (multimodal\npersonality traits recognition). For label abbreviations please refer to Section 4.3. MA(5,7) denotes\nmulti-class accuracy for (5,7)-class personality labels (higher is better).\nHappy, Sad, Angry and Neutral (no emotions) - similar to previous works (Tsai et al., 2019; Wang\net al., 2018).\nMultimodal Speaker Traits Recognition: The third benchmark in our experiments is speaker trait\nrecognition based on communicative behavior of a speaker. It is a particularly difÔ¨Åcult task, with\n16 different speaker traits in total. We study the POM dataset which contains 1,000 movie review\nvideos (Park et al., 2014). Each video is annotated for various personality and speaker traits,\nspeciÔ¨Åcally: ConÔ¨Ådent (Con), Passionate (Pas), V oice Pleasant (V oi), Dominant (Dom), Credible\n(Cre), Vivid (Viv), Expertise (Exp), Entertaining (Ent), Reserved (Res), Trusting (Tru), Relaxed (Rel),\nOutgoing (Out), Thorough (Tho), Nervous (Ner), Persuasive (Per) and Humorous (Hum). The short\nform of these speaker traits is indicated inside the parentheses and used for the rest of this paper.\n4.2 M ULTIMODAL COMPUTATIONAL DESCRIPTORS\nThe following computational descriptors are used by FMT and baselines (all the baselines use the\nsame descriptors in their original respective papers).\n6\nLanguage: P2FA forced alignment model (Yuan & Liberman, 2008) is used to align the text and\naudio at word level. From the forced alignment, the timing of words and sentences are extracted.\nWord-level alignment is used to unify the modality frequencies (Chen et al., 2017). GloVe embed-\ndings (Pennington et al., 2014) are subsequently used for word representation.\nVisual: For the visual modality, the Emotient FACET (iMotions, 2017) is used to extract a set of\nvisual features including Facial Action Units (Ekman et al., 1980), visual indicators of emotions, and\nsparse facial landmarks.\nAcoustic: COV AREP (Degottex et al., 2014) is used to extract the following features: fundamental\nfrequency, quasi open quotient (Kane & Gobl, 2013), normalized amplitude quotient, glottal source\nparameters (H1H2, Rd, Rd conf) (Drugman et al., 2012), V oiced/Unvoiced segmenting features\n(VUV) (Drugman & Alwan, 2011), maxima dispersion quotient (MDQ), the Ô¨Årst 3 formants, parabolic\nspectral parameter (PSP), harmonic model and phase distortion mean (HMPDM 0-24) and deviations\n(HMPDD 0-12), spectral tilt/slope of wavelet responses (peak/slope), Mel Cepstral CoefÔ¨Åcients\n(MCEP 0-24).\n4.3 B ASELINE MODELS AND PERFORMANCE MEASURES\nThe following strong baselines are compared to FMT: MV-LSTM (Multi-view LSTM, Rajagopalan\net al. (2016)), TFN (Tensor Fusion Network, Zadeh et al. (2017)), MARN (Multi-attention Recurrent\nNetwork, Zadeh et al. (2018b)) , MFN (Memory Fusion Network, Zadeh et al. (2018a)) , RAVEN\n(Recurrent Attended Variation Embedding Network, Wang et al. (2018)), MulT5 (Multimodal Trans-\nformer for [Un]aligned Sequences, Tsai et al. (2019)). There are fundamental distinctions between\nFMT and MulT, chief among them: 1) MulT consists of 6 transformers, 3 cross-modal transformers\nand 3 unimodal. Naturally this increases the overall model size substantially. FMT consists of\nonly one transformer, with components to avoid overparameterization. 2) FMT sees interactions as\nundirected (unlike MulT which has L ‚ÜíV and V ‚ÜíL), and therefore semantically combines two\nattentions in one. 3) MulT has no trimodal factors (which are important according to Section 5). 4)\nMulT has no direct unimodal path (e.g. only L), as input to unimodal transformers are outputs of\ncross-modal transformers. 5) All FMT attentions have full time-domain receptive Ô¨Åeld, while MulT\nsplits the input based on the heads.\nIn their original publication, all the models report6 the performance over the datasets in Section 4.1,\nusing the same descriptors discussed in Section 4.2. The models in this paper are compared using the\nfollowing performance measures (depending on the dataset): (BA) denotes binary accuracy - higher is\nbetter, (MA5,MA7) are 5 and 7 multiclass accuracy - higher is better, (F1) denotes F1 score - higher\nis better, (MAE) denotes the Mean-Absolute Error - lower is better, (Corr) is Pearson Correlation\nCoefÔ¨Åcient - higher is better. The hyperparameter space search for FMT (and baselines if retrained)\nis discussed in Appendix A.1.\n5 R ESULTS AND DISCUSSION\nThe results of sentiment analysis experiments on CMU-MOSI dataset are presented in Table 1.\nFMT achieves superior performance than the previously proposed models for multimodal sentiment\nanalysis. We use two approaches for calculating BA and F1 based on negative vs. non-negative\nsentiment (Zadeh et al., 2018b) on the left side of /, and negative vs. positive (Tsai et al., 2019) on\nthe right side. MAE and Corr are also reported. For multimodal emotion recognition, experiments on\nIEMOCAP are reported in Table 2. The performance of FMT is superior than other baselines for\nmultimodal emotion recognition (with the exception of Happy emotion). The results of experiments\nfor personality traits recognition on POM dataset are reported in Table 3. We report MA5 and MA7,\ndepending on the label. FMT outperforms baselines across all personality traits.\nWe study the importance of the factorization in FMT. We Ô¨Årst remove the unimodal, bimodal and\ntrimodal attentions from the FMT model, resulting in 3 alternative implementations of FMT. Table 4\n5We use the aligned variant of MulT model, which has shown better performance than unaligned version in\nthe original paper.\n6With the exception of MulT for POM dataset, which is not reported in original paper. It is trained in this\npaper using authors‚Äô provided github code with hyperparameter search in Appendix A.1.\n7\nModel \\Metric BA F1 MAE Corr\nFMT [UNI] 80.6/82.8 80.5/82.8 0.868 0.719\nFMT [BI] 81.2/81.7 81.2/81.6 0.877 0.706\nFMT [TRI] 80.2/81.6 80.2/81.5 0.874 0.705\nFMT [L] 77.7/79.6 77.7/79.6 0.935 0.666\nFMT [A] 62.5/62.7 62.6/73.2 1.338 0.306\nFMT [V] 59.3/59.3 59.4/72.7 1.357 0.218\nFMT [S] 80.3/82.0 80.3/81.9 0.860 0.734\nFMT 81.5/83.5 81.4/83.5 0.837 0.744\nTable 4: FMT ablation studies on CMU-MOSI dataset. UNI, BI, TRI denote removing all unimodal,\nbimodal and trimodal factors respectively. L, A, V denote using only language, audio, and visual\nfactors respectively. Sdenotes the model with summarization networks replaced by simple addition.\nAll factors, modalities, and components are needed for achieving best performance.\nModel \\Metric BA F1 MAE Corr\nFMT [1] 80.0/82.0 79.4/81.1 0.864 0.712\nFMT [2] 79.7/82.2 79.7/82.2 0.863 0.725\nFMT [3] 79.2/80.9 79.1/80.8 0.905 0.698\nFMT [4] 79.4/81.1 79.4/81.0 0.855 0.733\nFMT [5] 81.5/82.6 81.5/82.5 0.886 0.711\nFMT [6] 81.5/83.5 81.4/83.5 0.837 0.744\nTable 5: Multimodal sentiment analysis experiments with different number of FMS units inside\nMTL. The number in the square bracket indicates the number of FMS. The total number of attention\nis 7 times the number in bracket.\ndemonstrates the results of this ablation experiment over CMU-MOSI dataset. Furthermore, we use\nonly one modality as input for FMT, to understand the importance of each modality (all other factors\nremoved). We also replace the summarization networks with simple vector addition operation. All\nfactors, modalities, and summarization components are needed for achieving best performance.\nWe also perform experiments to understand the effect of number of FMT units within each MTL.\nTable 5 shows the performance trend for different number of FMT units. The model with 6 number\nof FMS (42 attentions in total) achieves the highest performance (6 is also the highest number we\nexperimented with). Tsai et al. (2019) reports the best performance for CMU-MOSI dataset is\nachieved when using 40 attentions per cross-modal transformer (3 of each, therefore 120 attention,\nwithout counting the subsequent unimodal transformers). FMT uses fewer number of attentions\nthan MulT, yet achieves better performance. We also experiment with number of heads for original\ntransformer model (Vaswani et al., 2017) and compare to FMT (Appendix A.3).\n6 C ONCLUSION\nIn this paper, we presented the Factorized Multimodal Transformer (FMT) model for multimodal\nsequential learning. Using a Factorized Multimodal Self-attention (FMS) within each Multimodal\nTransformer Layer (MTL), FMT is able to model the intra-model and inter-modal dynamics within\nasynchronous multimodal sequences. We compared the performance of FMT to baselines approaches\nover 3 publicly available datasets for multimodal sentiment analysis (CMU-MOSI, 1 label), emotion\nrecognition (IEMOCAP, 4 labels) and personality traits recognition (POM, 16 labels). Overall, FMT\nachieved superior performance than previously proposed models across the studied datasets.\n8\nREFERENCES\nEmily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan Naumann, and Matthew McDer-\nmott. Publicly available clinical BERT embeddings. InProceedings of the 2nd Clinical Natural Language Pro-\ncessing Workshop, pp. 72‚Äì78, Minneapolis, Minnesota, USA, June 2019. Association for Computational Lin-\nguistics. doi: 10.18653/v1/W19-1909. URL https://www.aclweb.org/anthology/W19-1909.\nTadas BaltruÀásaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and\ntaxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(2):423‚Äì443, 2018.\nLeonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of Ô¨Ånite state markov chains.\nThe annals of mathematical statistics, 37(6):1554‚Äì1563, 1966.\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette Chang,\nSungbok Lee, and Shrikanth S. Narayanan. Iemocap: Interactive emotional dyadic motion capture database.\nJournal of Language Resources and Evaluation, 42(4):335‚Äì359, dec 2008. doi: 10.1007/s10579-008-9076-6.\nMinghai Chen, Sen Wang, Paul Pu Liang, Tadas Baltru Àásaitis, Amir Zadeh, and Louis-Philippe Morency.\nMultimodal sentiment analysis with word-level fusion and reinforcement learning. In Proceedings of the 19th\nACM International Conference on Multimodal Interaction, pp. 163‚Äì171. ACM, 2017.\nKyunghyun Cho, Bart Van Merri¬®enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine\ntranslation. arXiv preprint arXiv:1406.1078, 2014.\nGilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio, and Stefan Scherer. Covarepa collaborative voice\nanalysis repository for speech technologies. In Acoustics, Speech and Signal Processing (ICASSP), 2014\nIEEE International Conference on, pp. 960‚Äì964. IEEE, 2014.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional\ntransformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/\nabs/1810.04805.\nThomas Drugman and Abeer Alwan. Joint robust voicing detection and pitch estimation based on residual\nharmonics. In Interspeech, pp. 1973‚Äì1976, 2011.\nThomas Drugman, Mark Thomas, Jon Gudnason, Patrick Naylor, and Thierry Dutoit. Detection of glottal closure\ninstants from speech signals: A quantitative review. IEEE Transactions on Audio, Speech, and Language\nProcessing, 20(3):994‚Äì1006, 2012.\nPaul Ekman. An argument for basic emotions. Cognition & emotion, 6(3-4):169‚Äì200, 1992.\nPaul Ekman, Wallace V Freisen, and Sonia Ancoli. Facial signs of emotional experience. Journal of personality\nand social psychology, 39(6):1125, 1980.\nAkira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Mul-\ntimodal compact bilinear pooling for visual question answering and visual grounding. arXiv preprint\narXiv:1606.01847, 2016.\nYang Gao, Oscar Beijbom, Ning Zhang, and Trevor Darrell. Compact bilinear pooling. CoRR, abs/1511.06062,\n2015. URL http://arxiv.org/abs/1511.06062.\nSepp Hochreiter and J ¬®urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735‚Äì1780,\n1997.\nJudith Holler and Stephen C Levinson. Multimodal language processing in human communication. Trends in\nCognitive Sciences, 2019.\nRonghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason:\nEnd-to-end module networks for visual question answering. CoRR, abs/1704.05526, 2017. URL http:\n//arxiv.org/abs/1704.05526.\niMotions. Facial expression analysis, 2017. URL goo.gl/1rh1JN.\nJohn Kane and Christer Gobl. Wavelet maxima dispersion for breathy to tense voice discrimination. IEEE\nTransactions on Audio, Speech, and Language Processing, 21(6):1170‚Äì1179, 2013.\nJin-Hwa Kim, Kyoung Woon On, Woosang Lim, Jeonghee Kim, JungWoo Ha, and Byoung-Tak Zhang.\nHadamard product for low-rank bilinear pooling. CoRR, abs/1610.04325, 2016. URL http://arxiv.\norg/abs/1610.04325.\n9\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\nDavid G Kleinbaum, Lawrence L Kupper, Keith E Muller, and Azhar Nizam. Applied regression analysis and\nother multivariable methods, volume 601. Duxbury Press Belmont, CA, 1988.\nSatwik Kottur, Jos¬¥e M. F. Moura, Devi Parikh, Dhruv Batra, and Marcus Rohrbach. CLEVR-dialog: A di-\nagnostic dataset for multi-round reasoning in visual dialog. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pp. 582‚Äì595, Minneapolis, Minnesota, June 2019. Association for Com-\nputational Linguistics. doi: 10.18653/v1/N19-1058. URL https://www.aclweb.org/anthology/\nN19-1058.\nAngeliki Lazaridou, Nghia The Pham, and Marco Baroni. Combining language and vision with a multimodal\nskip-gram model. In Proceedings of the 2015 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pp. 153‚Äì163, Denver, Colorado, May‚ÄìJune 2015.\nAssociation for Computational Linguistics. doi: 10.3115/v1/N15-1016. URL https://www.aclweb.\norg/anthology/N15-1016.\nPaul Pu Liang, Zhun Liu, Yao-Hung Hubert Tsai, Qibin Zhao, Ruslan Salakhutdinov, and Louis-Philippe\nMorency. Learning representations from imperfect time series data via tensor rank regularization.\nPaul Pu Liang, Ziyin Liu, Amir Zadeh, and Louis-Philippe Morency. Multimodal language analysis with\nrecurrent multistage fusion. arXiv preprint arXiv:1808.03920, 2018.\nZhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, and Louis-\nPhilippe Morency. EfÔ¨Åcient low-rank multimodal fusion with modality-speciÔ¨Åc factors. arXiv preprint\narXiv:1806.00064, 2018.\nLouis-Philippe Morency, Ariadna Quattoni, and Trevor Darrell. Latent-dynamic discriminative models for con-\ntinuous gesture recognition. In Computer Vision and Pattern Recognition, 2007. CVPR‚Äô07. IEEE Conference\non, pp. 1‚Äì8. IEEE, 2007.\nDuy-Kien Nguyen and Takayuki Okatani. Multi-task learning of hierarchical vision-language representation.\nCoRR, abs/1812.00500, 2018. URL http://arxiv.org/abs/1812.00500.\nBehnaz Nojavanasghari, Deepak Gopinath, Jayanth Koushik, Tadas BaltruÀásaitis, and Louis-Philippe Morency.\nDeep multimodal fusion for persuasiveness prediction. In Proceedings of the 18th ACM International\nConference on Multimodal Interaction, ICMI 2016, pp. 284‚Äì288, New York, NY , USA, 2016. ACM. ISBN 978-\n1-4503-4556-9. doi: 10.1145/2993148.2993176. URL http://doi.acm.org/10.1145/2993148.\n2993176.\nSunghyun Park, Han Suk Shim, Moitreya Chatterjee, Kenji Sagae, and Louis-Philippe Morency. Computational\nanalysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach.\nIn Proceedings of the 16th International Conference on Multimodal Interaction , ICMI ‚Äô14, pp. 50‚Äì57,\nNew York, NY , USA, 2014. ACM. ISBN 978-1-4503-2885-2. doi: 10.1145/2663204.2663260. URL\nhttp://doi.acm.org/10.1145/2663204.2663260.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, and Alexander Ku. Image\ntransformer. CoRR, abs/1802.05751, 2018. URL http://arxiv.org/abs/1802.05751.\nJeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation.\n2014.\nAriadna Quattoni, Sybor Wang, Louis-Philippe Morency, Michael Collins, and Trevor Darrell. Hidden condi-\ntional random Ô¨Åelds. IEEE Trans. Pattern Anal. Mach. Intell., 29(10):1848‚Äì1852, October 2007. ISSN 0162-\n8828. doi: 10.1109/TPAMI.2007.1124. URL http://dx.doi.org/10.1109/TPAMI.2007.1124.\nShyam Sundar Rajagopalan, Louis-Philippe Morency, Tadas BaltruÀásaitis, and Roland Goecke. Extending long\nshort-term memory for multi-view structured learning. In European Conference on Computer Vision, 2016.\nCees G. M. Snoek, Marcel Worring, and Arnold W. M. Smeulders. Early versus late fusion in semantic video\nanalysis. In Proceedings of the 13th Annual ACM International Conference on Multimedia, MULTIMEDIA\n‚Äô05, pp. 399‚Äì402, New York, NY , USA, 2005. ACM. ISBN 1-59593-044-2. doi: 10.1145/1101149.1101236.\nURL http://doi.acm.org/10.1145/1101149.1101236.\nYale Song, Louis-Philippe Morency, and Randall Davis. Multi-view latent variable discriminative models for\naction recognition. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp.\n2120‚Äì2127. IEEE, 2012.\n10\nYale Song, Louis-Philippe Morency, and Randall Davis. Action recognition by hierarchical sequence summariza-\ntion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3562‚Äì3569,\n2013.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. Multimodal transformer for unaligned multimodal language sequences. arXiv preprint\narXiv:1906.00295, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,≈Åukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp.\n5998‚Äì6008, 2017.\nValentin Vielzeuf, St ¬¥ephane Pateux, and Fr ¬¥ed¬¥eric Jurie. Temporal multimodal fusion for video emotion\nclassiÔ¨Åcation in the wild. In Proceedings of the 19th ACM International Conference on Multimodal Interaction,\nICMI ‚Äô17, pp. 569‚Äì576, New York, NY , USA, 2017. ACM. ISBN 978-1-4503-5543-8. doi: 10.1145/3136755.\n3143011. URL http://doi.acm.org/10.1145/3136755.3143011.\nYansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can shift:\nDynamically adjusting word representations using nonverbal behaviors. arXiv preprint arXiv:1811.09362,\n2018.\nHao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun, and Wei-Ying Ma. UniÔ¨Åed visual-\nsemantic embeddings: Bridging vision and language with structured meaning representations. In The IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), June 2019.\nChang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,\n2013.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. CoRR, abs/1906.08237, 2019. URL\nhttp://arxiv.org/abs/1906.08237.\nJiahong Yuan and Mark Liberman. Speaker identiÔ¨Åcation on the scotus corpus. Journal of the Acoustical Society\nof America, 123(5):3878, 2008.\nAmir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency. Mosi: Multimodal corpus of sentiment\nintensity and subjectivity analysis in online opinion videos. arXiv preprint arXiv:1606.06259, 2016.\nAmir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network\nfor multimodal sentiment analysis. In Empirical Methods in Natural Language Processing, EMNLP, 2017.\nAmir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency.\nMemory fusion network for multi-view sequential learning. In Thirty-Second AAAI Conference on ArtiÔ¨Åcial\nIntelligence, 2018a.\nAmir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria, and Louis-Philippe Morency. Multi-\nattention recurrent network for human communication comprehension. In Thirty-Second AAAI Conference on\nArtiÔ¨Åcial Intelligence, 2018b.\nAmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal\nlanguage analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n2236‚Äì2246, 2018c.\n11\nA A PPENDIX\nA.1 T RAINING REMARKS AND HYPERPARAMETER SPACE SEARCH\nThe hyperparameters of FMT include the Adam (Kingma & Ba, 2014) learning rate ({0.001, 0.0001}),\nstructure of summarization network (randomly picked 5 architectures from {1, 2, 3}layers of conv,\nwith kernel shapes of {2, 5, 10, 15, 20}), number of MTL layers ({4, 6, 8}except for ablation experi-\nments which was 2 . . .8), number of FMT units ({4, 6}, except for ablation experiment which was\n1 . . .6), eM‚àà{L,V,A}({20, 40}), dropout (0, 0.1). The same parameters (when applicable) are used for\ntraining MulT for POM dataset (e.g. num encoder layers same as number of MTL). Furthermore, for\nMulT speciÔ¨Åc hyperparameters, we use similar values as Table 5 in the original paper. All models\nare trained for a maximum of 200 epochs. The hyperparameter validation is similar to Zadeh et al.\n(2018b).\nA.2 N UMBER OF MTL\nWe study the effect of number of MTL on FMT performance. Table 6 shows the results of this\nexperiment. The best performance is achieved using 8 MTL layers (which was also the maximum\nlayers we tried in our hyperparameter search).\nA.3 N UMBER OF ATTENTION HEADS FOR ORIGINAL TRANSFORMER MODEL\nIn this section, we discuss the effect of increasing the number of heads on the original transformer\nmodel (OTF, Vaswani et al. (2017)). Please note that we implement the OTF to allow for all attention\nheads to have full input receptive Ô¨Åeld (from1 . . .T), similar to FMT. We increase the attention heads\nfrom 1 to 35 (after 35 does not Ô¨Åt on a Tesla-V100 GPU with batchsize of 20). Table 7 shows the\nresults of increasing number of attention heads for both models. We observe that achieving superior\nperformance is not a matter of increasing the attention heads. Even using 1 FMS unit, which leads to\n7 total attention, FMT achieves higher performance than counterpart OTF.\nA.4 T RAINING REMARKS FOR MORE THAN 3 MODALITIES\nIn many scenarios in nature, as well as what is currently pursued in machine learning, the number of\nmodalities goes as high as 3 (mostly language, vision and acoustic, as studied in this paper). This leads\nto 7 attentions within each FMS, well manageable for successful training of FMT as demonstrated in\nthis paper. However, as the number of modalities increases, the underlying multimodal phenomena\nbecomes more challenging to model. This causes complexities for any competitive multimodal\nmodel, regardless of their internal design. While studying these cases are beyond the scope of\nthis paper, due to rare nature of having more than 3 main modalities modalities, for FMT, the\ncomplexity can be managed due to the factorization in FMS. We propose two approaches: 1) for\nhigh number of modalities, the involved factors can be reduced based on domain knowledge, the\nnature of the problem, and the assumed dependencies between modalities (e.g. removing factors\nbetween modalities that are deemed weakly related). Alternatively, without making assumptions\nModel \\Metric MAE Corr\nFMT [2] 0.881 0.720\nFMT [3] 0.876 0.727\nFMT [4] 0.871 0.723\nFMT [5] 0.876 0.724\nFMT [6] 0.852 0.730\nFMT [7] 0.859 0.732\nFMT [8] 0.837 0.744\nTable 6: Multimodal sentiment analysis experiments with different number of MTL layers. The\nnumber in the square bracket indicates the number of MTL layers.\n12\nModel \\Metric BA F1 MAE Corr\nOTF [1] 74.6/76.5 74.5/76.5 0.983 0.651\nOTF [2] 76.8/78.8 76.6/78.9 0.975 0.655\nOTF [3] 74.2/75.8 74.0/75.9 0.998 0.647\nOTF [4] 76.5/77.9 76.6/78.2 1.022 0.632\nOTF [5] 75.1/76.7 75.1/76.6 1.026 0.626\nOTF [6] 71.6/72.3 71.3/72.6 1.094 0.677\nOTF [7] 77.4/79.1 77.4/79.0 0.988 0.646\nOTF [14] 77.0/78.5 76.9/78.5 0.972 0.683\nOTF [21] 75.9/77.7 75.8/77.9 0.930 0.682\nOTF [35] 67.6/68.9 67.2/73.2 1.174 0.502\nTable 7: Results of experiments with different number of heads for OTF. The number in the square\nbracket indicates the number of heads.\nabout inter-modality dependencies, a greedy approach may be taken for adding factors; an approach\nsimilar to stepwise regression (Kleinbaum et al., 1988), iteratively adding the next most important\nfactor. Using these two methods, the model can cope with higher number of modalities with a\ncontrollable compromise between performance and overparameterization.\n13",
  "topic": "Multimodal therapy",
  "concepts": [
    {
      "name": "Multimodal therapy",
      "score": 0.7388219833374023
    },
    {
      "name": "Computer science",
      "score": 0.5261356830596924
    },
    {
      "name": "Transformer",
      "score": 0.5175892114639282
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3545413315296173
    },
    {
      "name": "Engineering",
      "score": 0.1273573338985443
    },
    {
      "name": "Psychology",
      "score": 0.12608975172042847
    },
    {
      "name": "Electrical engineering",
      "score": 0.06674042344093323
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 37
}