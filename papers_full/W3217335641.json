{
  "title": "TransMorph: Transformer for unsupervised medical image registration",
  "url": "https://openalex.org/W3217335641",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A591795076",
      "name": "Chen Jun-yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226780455",
      "name": "Frey, Eric C",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3034178926",
      "name": "He, Yufan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221519566",
      "name": "Segars, William P.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096271395",
      "name": "Li Ye",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124527706",
      "name": "Du Yong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3134689216",
    "https://openalex.org/W2890374903",
    "https://openalex.org/W2016974693",
    "https://openalex.org/W1970928383",
    "https://openalex.org/W1998559070",
    "https://openalex.org/W3098712157",
    "https://openalex.org/W3104164805",
    "https://openalex.org/W2996290406",
    "https://openalex.org/W2947216528",
    "https://openalex.org/W3198035652",
    "https://openalex.org/W3171660447",
    "https://openalex.org/W2082030398",
    "https://openalex.org/W3185462999",
    "https://openalex.org/W2604209076",
    "https://openalex.org/W3037091274",
    "https://openalex.org/W2788906943",
    "https://openalex.org/W2999580839",
    "https://openalex.org/W3174738881",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W1987869189",
    "https://openalex.org/W2904555209",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3195830874",
    "https://openalex.org/W1554944419",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2556967412",
    "https://openalex.org/W2611467245",
    "https://openalex.org/W2520815785",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2071989139",
    "https://openalex.org/W2122448973",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W1874027545",
    "https://openalex.org/W2043806601",
    "https://openalex.org/W3203841574",
    "https://openalex.org/W2785783085",
    "https://openalex.org/W3135042483",
    "https://openalex.org/W2100005771",
    "https://openalex.org/W3112701542",
    "https://openalex.org/W2891590469",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W2959226257",
    "https://openalex.org/W3007763359",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2170167891",
    "https://openalex.org/W2296294727",
    "https://openalex.org/W2963720324",
    "https://openalex.org/W1409773210",
    "https://openalex.org/W2976356773",
    "https://openalex.org/W3035201239",
    "https://openalex.org/W2113576511",
    "https://openalex.org/W3136762441",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2922479016",
    "https://openalex.org/W3098269293",
    "https://openalex.org/W2810337461",
    "https://openalex.org/W2902488884",
    "https://openalex.org/W2103857226",
    "https://openalex.org/W1983281817",
    "https://openalex.org/W2964015468"
  ],
  "abstract": "In the last decade, convolutional neural networks (ConvNets) have been a major focus of research in medical image analysis. However, the performances of ConvNets may be limited by a lack of explicit consideration of the long-range spatial relationships in an image. Recently Vision Transformer architectures have been proposed to address the shortcomings of ConvNets and have produced state-of-the-art performances in many medical imaging applications. Transformers may be a strong candidate for image registration because their substantially larger receptive field enables a more precise comprehension of the spatial correspondence between moving and fixed images. Here, we present TransMorph, a hybrid Transformer-ConvNet model for volumetric medical image registration. This paper also presents diffeomorphic and Bayesian variants of TransMorph: the diffeomorphic variants ensure the topology-preserving deformations, and the Bayesian variant produces a well-calibrated registration uncertainty estimate. We extensively validated the proposed models using 3D medical images from three applications: inter-patient and atlas-to-patient brain MRI registration and phantom-to-CT registration. The proposed models are evaluated in comparison to a variety of existing registration methods and Transformer architectures. Qualitative and quantitative results demonstrate that the proposed Transformer-based model leads to a substantial performance improvement over the baseline methods, confirming the effectiveness of Transformers for medical image registration.",
  "full_text": "Contents lists available at ScienceDirect\nMedical Image Analysis\njournal homepage: www.elsevier.com/locate/media\nTransMorph: Transformer for unsupervised medical image registration\nJunyu Chena,b,∗, Eric C. Freya,b, Yufan Hee, William P. Segarsc, Ye Lid, Yong Dua\naRussell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins Medical Institutes, Baltimore, MD, USA\nbDepartment of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA\ncCarl E. Ravin Advanced Imaging Laboratories, Department of Radiology, Duke University Medical Center, Durham, NC, USA\ndCenter for Advanced Medical Computing and Analysis, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA\neNVIDIA Corporation, Bethesda, MD, USA\nA R T I C L E I N F O\nArticle history:\nReceived xx xx 20xx\nReceived in ﬁnal form xx xx 20xx\nAccepted xx xx 20xx\nAvailable online xx xx 20xx\nKeywords: Image Registration, Deep\nLearning, Vision Transformer, Comput-\nerized Phantom\nA B S T R A C T\nIn the last decade, convolutional neural networks (ConvNets) have been a major focus\nof research in medical image analysis. However, the performances of ConvNets may\nbe limited by a lack of explicit consideration of the long-range spatial relationships\nin an image. Recently Vision Transformer architectures have been proposed to ad-\ndress the shortcomings of ConvNets and have produced state-of-the-art performances\nin many medical imaging applications. Transformers may be a strong candidate for im-\nage registration because their substantially larger receptive ﬁeld enables a more precise\ncomprehension of the spatial correspondence between moving and ﬁxed images. Here,\nwe present TransMorph, a hybrid Transformer-ConvNet model for volumetric medical\nimage registration. This paper also presents di ﬀeomorphic and Bayesian variants of\nTransMorph: the diﬀeomorphic variants ensure the topology-preserving deformations,\nand the Bayesian variant produces a well-calibrated registration uncertainty estimate.\nWe extensively validated the proposed models using 3D medical images from three ap-\nplications: inter-patient and atlas-to-patient brain MRI registration and phantom-to-CT\nregistration. The proposed models are evaluated in comparison to a variety of exist-\ning registration methods and Transformer architectures. Qualitative and quantitative\nresults demonstrate that the proposed Transformer-based model leads to a substantial\nperformance improvement over the baseline methods, conﬁrming the e ﬀectiveness of\nTransformers for medical image registration.\n© 2022 Elsevier B. V . All rights reserved.\n1. Introduction\nDeformable image registration (DIR) is fundamental for\nmany medical imaging analysis tasks. It functions by establish-\ning spatial correspondence in order to minimize the diﬀerences\nbetween a pair of ﬁxed and moving images. Traditional meth-\n∗Corresponding author\ne-mail: jchen245@jhmi.edu (Junyu Chen), efrey@jhmi.edu (Eric C.\nFrey), yufanh@nvidia.com (Yufan He), paul.segars@duke.edu (William\nP. Segars),gary.li@mgh.harvard.edu (Ye Li), duyong@jhmi.edu (Yong\nDu)\nods formulate image registration as a variational problem for\nestimating a smooth mapping between the points in one image\nand those in another (Avants et al. 2008; Beg et al. 2005; Ver-\ncauteren et al. 2009; Heinrich et al. 2013a; Modat et al. 2010).\nHowever, such methods are computationally expensive and usu-\nally slow in practice because the optimization problem needs to\nbe solved de novo for each pair of unseen images.\nRecently, deep neural networks (DNNs), especially convo-\nlutional neural networks (ConvNets), have demonstrated state-\nof-the-art performance in many computer vision tasks, includ-\ning object detection (Redmon et al. 2016), image classiﬁcation\n(He et al. 2016), and segmentation (Long et al. 2015). Ever\narXiv:2111.10480v6  [eess.IV]  15 Oct 2022\n2 Junyu Chen et al. / Medical Image Analysis (2022)\nsince the success of U-Net in the ISBI cell tracking challenge of\n2015 (Ronneberger et al. 2015), ConvNet-based methods have\nbecome a major focus of attention in medical image analysis\nﬁelds, such as tumor segmentation (Isensee et al. 2021; Zhou\net al. 2019), image reconstruction (Zhu et al. 2018), and dis-\nease diagnostics (Lian et al. 2018). In medical image regis-\ntration, ConvNet-based methods can produce signiﬁcantly im-\nproved registration performance while operating orders of mag-\nnitudes faster (after training) compared to traditional methods.\nConvNet-based methods replace the costly per-image optimiza-\ntion seen in traditional methods with a single global function\noptimization during a training phase. The ConvNets learn the\ncommon representation of image registration from training im-\nages, enabling rapid alignment of an unseen image pair after\ntraining. Initially, the supervision of ground-truth deformation\nﬁelds (which are usually generated using traditional registration\nmethods) is needed for training the neural networks (Onofrey\net al. 2013; Yang et al. 2017b; Roh´e et al. 2017). Recently, the\nfocus has been shifted towards developing unsupervised meth-\nods that do not depend on ground-truth deformation ﬁelds (Bal-\nakrishnan et al. 2019; Dalca et al. 2019; Kim et al. 2021; de V os\net al. 2019, 2017; Lei et al. 2020; Chen et al. 2020; Zhang\n2018). Nearly all of the existing deep-learning-based methods\nmentioned above used U-Net (Ronneberger et al. 2015) or the\nsimply modiﬁed versions of U-Net (e.g., tweaking the number\nof layers or changing down- and up-sampling schemes) as their\nConvNet designs.\nConvNet architectures generally have limitations in model-\ning explicit long-range spatial relations (i.e., relations between\ntwo voxels that are far away from each other) present in an im-\nage due to the intrinsic locality (i.e., the limited e ﬀective re-\nceptive ﬁeld) of convolution operations (Luo et al. 2016). The\nU-Net (or V-Net (Milletari et al. 2016)) was proposed to over-\ncome this limitation by introducing down- and up-sampling op-\nerations into a ConvNet, which theoretically enlarges the re-\nceptive ﬁeld of the ConvNet and, thus, encourages the network\nto consider long-range relationships between points in images.\nHowever, several problems remain: ﬁrst, the receptive ﬁelds\nof the ﬁrst several layers are still restricted by the convolution-\nkernel size, and the global information of an image can only be\nviewed at the deeper layers of the network; second, it has been\nshown that as the convolutional layers deepen, the impact from\nfar-away voxels decays quickly (Li et al. 2021). Therefore, the\neﬀective receptive ﬁeld of a U-Net is, in practice, much smaller\nthan its theoretical receptive ﬁeld, and it is only a portion of the\ntypical size of a medical image. This limits the U-Net’s ability\nto perceive semantic information and model long-range rela-\ntionships between points. Yet, it is believed that the ability to\ncomprehend semantic scene information is of great importance\nin coping large deformations (Ha et al. 2020). Many works in\nother ﬁelds (e.g., image segmentation) have addressed this lim-\nitation of U-Net (Zhou et al. 2019; Jha et al. 2019; Devalla et al.\n2018; Alom et al. 2018). To allow for a better ﬂow of multi-\nscale contextual information throughout the network, Zhou et\nal. (Zhou et al. 2019) proposed a nested U-Net (i.e., U-Net++),\nin which the complex up- and down-samplings along with mul-\ntiple skip connections were used. Devalla et al. (Devalla et al.\n2018) introduced dilated convolution to the U-Net architecture\nthat enlarges the network’s eﬀective receptive ﬁeld. A similar\nidea was proposed by Alom et al. (Alom et al. 2018), where\nthe network’s eﬀective receptive ﬁeld was increased by deploy-\ning recurrent convolutional operations. Jha et al. proposed Re-\nsUNet++ (Jha et al. 2019) that incorporates the attention mech-\nanisms into U-Net for modeling long-range spatial information.\nDespite these methods’ promising performance in other medi-\ncal imaging ﬁelds, there has been limiting work on using ad-\nvanced network architectures for medical image registration.\nTransformer, which originated from natural language pro-\ncessing tasks (Vaswani et al. 2017), has shown its potential in\ncomputer vision tasks. A Transformer deploys self-attention\nmechanisms to determine which parts of the input sequence\n(e.g., an image) are essential based on contextual information.\nUnlike convolution operations, whose eﬀective receptive ﬁelds\nare limited by the size of convolution kernels, the self-attention\nmechanisms in a Transformer have large size eﬀective receptive\nﬁelds, making a Transformer capable of capturing long-range\nspatial information (Li et al. 2021). Dosovitskiy et al. (Dosovit-\nskiy et al. 2020) proposed Vision Transformer (ViT) that applies\nthe Transformer encoder from NLP directly to images. It was\nthe ﬁrst purely self-attention-based network for computer vi-\nsion and achieved state-of-the-art performance in image recog-\nnition. Subsequent to their success, Swin Transformer (Liu\net al. 2021a) and its variants (Dai et al. 2021; Dong et al. 2021)\nhave demonstrated their superior performances in object detec-\ntion, and semantic segmentation. Recently, Transformer-related\nmethods have gained increased attention in medical imaging\n(Chen et al. 2021b; Xie et al. 2021; Wang et al. 2021b; Li et al.\n2021; Wang et al. 2021a; Zhang et al. 2021); the major applica-\ntion has been the task of image segmentation.\nTransformer can be a strong candidate for image registration\nbecause it can better comprehend the spatial correspondence\nbetween the moving and ﬁxed images. Registration is the pro-\ncess of establishing such correspondence, and intuitively, by\ncomparing diﬀerent parts of the moving to the ﬁxed image. A\nConvNet has a narrow ﬁeld of view: it performs convolution lo-\ncally, and its ﬁeld of view grows in proportion to the ConvNet’s\ndepth; hence, the shallow layers have a relatively small recep-\ntive ﬁeld, limiting the ConvNet’s ability to associate the distant\nparts between two images. For example, if the left part of the\nmoving image matches the right part of the ﬁxed image, Con-\nvNet will be unable to establish the proper spatial correspon-\ndence between the two parts if it cannot see both parts concur-\nrently (i.e., when one of the parts falls outside of the ConvNet’s\nﬁeld of view). However, Transformer is capable of handling\nsuch circumstances and rapidly focusing on the parts that need\ndeformation, owing to its large receptive ﬁeld and self-attention\nmechanism.\nOur group has previously shown preliminary results that\ndemonstrated the bridging of ViT and V-Net provided good\nperformance in image registration (Chen et al. 2021a). In this\nwork, we extended that preliminary work and investigated vari-\nous Transformer models from other tasks (i.e., computer vision\nand medical imaging tasks). We present a hybrid Transformer-\nConvNet framework, TransMorph, for volumetric medical im-\nJunyu Chen et al. / Medical Image Analysis (2022) 3\nage registration. In this method, the Swin Transformer (Liu\net al. 2021a) was employed as the encoder to capture the spa-\ntial correspondence between the input moving and ﬁxed im-\nages. Then, a ConvNet decoder processed the information pro-\nvided by the Transformer encoder into a dense displacement\nﬁeld. Long skip connections were deployed to maintain the\nﬂow of localization information between the encoder and de-\ncoder stages. We also introduced di ﬀeomorphic variations of\nTransMorph to ensure a smooth and topology-preserving de-\nformation. Additionally, we applied variational inference on\nthe parameters of TransMorph, resulting in a Bayesian model\nthat predicts registration uncertainty based on the given image\npair. Qualitative and quantitative evaluation of the experimental\nresults demonstrate the robustness of the proposed method and\nconﬁrm the eﬃcacy of Transformers for image registration.\nThe main contributions of this work are summarized as fol-\nlows:\n• Transformer-based model: This paper presents the pi-\noneering work on using Transformers for image reg-\nistration. A novel Transformer-based neural network,\nTransMorph, was proposed for aﬃne and deformable im-\nage registration.\n• Architecture analysis: Experiments in this paper demon-\nstrate that positional embedding, which is a commonly\nused element in Transformer by convention, is not required\nfor the proposed hybrid Transformer-ConvNet model.\nSecondly, we show that Transformer-based models have\nlarger eﬀective receptive ﬁelds than ConvNets. Moreover,\nwe demonstrated that TransMorph promotes a ﬂatter reg-\nistration loss landscape.\n• Diﬀeomorphic registration: We demonstrate that\nTransMorph can be easily integrated into two exist-\ning frameworks as a registration backbone to provide\ndiﬀeomorphic registration.\n• Uncertainty quantiﬁcation: This paper also provides a\nBayesian uncertainty variant of TransMorph that yields\ntransformer uncertainty and perfectly calibrated appear-\nance uncertainty estimates.\n• State-of-the-art results: We extensively validate the pro-\nposed registration models on two brain MRI registration\napplications (inter-patient and atlas-to-patient registration)\nand on a novel application of XCAT-to-CT registration\nwith an aim to create a population of anatomically variable\nXCAT phantom. The datasets used in this study (which\ninclude a publicly available dataset, the IXI dataset1) con-\ntain over 1000 image pairs for training and testing. The\nproposed models were compared with various registration\nmethods and demonstrated state-of-the-art performance.\nEight registration approaches were employed as baselines,\nincluding learning-based methods and widely used con-\nventional methods. The performances of four recently\nproposed Transformer architectures from other tasks (e.g.,\nsemantic segmentation, classiﬁcation, etc.) were also eval-\nuated on the task of image registration.\n1https://brain-development.org/ixi-dataset/\n• Open source: We provide the community with a fast and\naccurate tool for deformable registration. The source code,\nthe pre-trained models, and our preprocessed IXI dataset\nare publicly available at https://bit.ly/37eJS6N.\nThe paper is organized as follows. Section 2 discusses re-\nlated work. Section 3 explains the proposed methodology. Sec-\ntion 4 discusses experimental setup, implementation details,\nand datasets used in this study. Section 5 presents experimental\nresults. Section 6 discusses the ﬁndings based on the results,\nand Section 7 concludes the paper.\n2. Related Work\nThis section reviews the relevant literature and provides fun-\ndamental knowledge for the proposed method.\n2.1. Image Registration\nDeformable image registration (DIR) establishes spatial cor-\nrespondence between two images by optimizing an energy\nfunction:\nE(Im,If ,φ) = Esim(Im ◦φ,If ) + λR(φ), (1)\nwhere Im and If denote, respectively, the moving and ﬁxed im-\nage, φdenotes the deformation ﬁeld that warps the moving im-\nage (i.e., Im ◦φ), R(φ) imposes smoothness of the deformation\nﬁeld, and λ is the regularization hyper-parameter that deter-\nmines the trade-o ﬀ between image similarity and deformation\nﬁeld regularity. The optimal warping, ˆφis given by minimizing\nthis energy function:\nˆφ= arg min\nφ\nE(Im,If ,φ). (2)\nIn the energy function, Esim measures the level of alignment\nbetween the deformed moving image, Im ◦φ, and the ﬁxed im-\nage, If . Some common choices for Esim are mean squared error\n(MSE) (Beg et al. 2005; Wolberg and Zokai 2000), normalized\ncross-correlation (NCC) (Avants et al. 2008), structural simi-\nlarity index (SSIM) (Chen et al. 2020), and mututal informa-\ntion (MI) (Viola and Wells III 1997). The regularization term,\nR(φ), imposes spatial smoothness on the deformation ﬁeld. A\ncommon assumption in most applications is that similar struc-\ntures exist in both moving and ﬁxed images. As a result, a\ncontinuous and invertible deformation ﬁeld (i.e., a di ﬀeomor-\nphism) is needed to preserve topology, and the regularization,\nR(φ) is meant to enforce or encourage this. Isotropic di ﬀusion\n(equivalent to Gaussian smoothing) (Balakrishnan et al. 2019),\nanisotropic diﬀusion (Pace et al. 2013), total variation (Vish-\nnevskiy et al. 2016), and bending energy (Johnson and Chris-\ntensen 2002) are popular options for R(φ).\n2.1.1. Image registration via deep neural networks\nWhile traditional image registration methods iteratively min-\nimize the energy function in (1) for each pair of moving and\nﬁxed images, DNN-based methods optimize the energy func-\ntion for a training dataset, thereby learning a global representa-\ntion of image registration that enables alignment of an unseen\n4 Junyu Chen et al. / Medical Image Analysis (2022)\nFig. 1: The architecture of the proposed TransMorph registration network.\nFig. 2: The conventional paradigm of image registration.\npair of volumes. DNN methods are often categorized as super-\nvised or unsupervised, with the former requiring a ground truth\ndeformation ﬁeld for training and the latter relying only on the\nimage datasets.\nIn supervised DNN methods, the ground-truth deformation\nﬁelds are either produced synthetically or generated by tradi-\ntional registration methods (Yang et al. 2017b; Sokooti et al.\n2017; Cao et al. 2018). Yang et al. 2017b proposed a super-\nvised ConvNet that predicts the LDDMM (Beg et al. 2005)\nmomentum from image patches. Sokooti et al. 2017 trained\na registration ConvNet with synthetic displacement ﬁelds. The\nground-truth deformation ﬁelds are often computationally ex-\npensive to generate, and the registration accuracy of these meth-\nods is highly dependent on the quality of the ground truth.\nDue to the limitations of supervised methods, the focus of\nresearch has switched to unsupervised DNN methods that do\nnot need ground-truth deformation ﬁelds. Unsupervised DNNs\noptimize an energy function on the input images, similar to tra-\nditional methods. However, DNN-based methods learn a com-\nmon registration representation from a training set and then ap-\nply it to unseen images. Note that the term “unsupervised”\nrefers to the absence of ground-truth deformation ﬁelds, but\nthe network still needs training (this is also known as “self-\nsupervised”). de V os et al. 2019; Balakrishnan et al. 2018, 2019\nare representative of unsupervised DNN-based methods.\nMore recently, di ﬀeomorphic deformation representations\nhave been developed to address the issue of non-smooth de-\nformations in DNN-based methods. We brieﬂy introduce its\nconcepts in the next subsection.\n2.1.2. Di ﬀeomorphic image registration\nDiﬀeomorphic deformable image registration is important in\nmany medical image applications, owing to its special prop-\nerties including topology preservation and transformation in-\nvertibility. A di ﬀeomorphic transformation is a smooth and\ncontinuous one-to-one mapping with invertible derivatives (i.e.,\nnon-zero Jacobian determinant). Such a transformation can\nbe achieved via the time-integration of time-dependent (Beg\net al. 2005; Avants et al. 2008) or time-stationary velocity ﬁelds\n(SVFs) (Arsigny et al. 2006; Ashburner 2007; Vercauteren et al.\n2009; Hernandez et al. 2009). In the time-dependent setting\n(e.g., LDDMM (Beg et al. 2005) and SyN (Avants et al. 2008)),\na di ﬀeomorphic transformation φ is obtained via integrating\nthe su ﬃciently smooth time-varying velocity ﬁelds ν(t), i.e.,\nd\ndt φ(t) = ν(t)(φ(t)), where φ(t) = id is the identity transform. On\nthe other hand, in the stationary velocity ﬁelds (SVFs) setting\n(e.g., DARTEL Ashburner 2007 and di ﬀeomorphic Demons\n(Vercauteren et al. 2009)), the velocity ﬁelds are assumed to be\nstationary over time, i.e., d\ndt φ(t) = ν(φ(t)). Dalca et al. (Dalca\net al. 2019) ﬁrst adopt the di ﬀeomorphism formulation in a\ndeep learning model, using the SVFs setting with an e ﬃcient\nscaling-and-squaring approach (Arsigny et al. 2006). In the\nscaling-and-squaring approach, the deformation ﬁeld is repre-\nsented as a Lie algebra member that is exponentiated to gen-\nerate a time 1 deformation φ(1), which is a member of the Lie\ngroup: φ(1) = exp(ν). This means that the exponentiated ﬂow\nﬁeld compels the mapping to be di ﬀeomorphic and invertible\nusing the same ﬂow ﬁeld. Starting from an initial deformation\nﬁeld:\nφ(1/2T ) = p + ν(p)\n2T , (3)\nwhere p denotes the spatial locations. The φ(1) can be obtained\nusing the recurrence:\nφ(1/2t−1) = φ(1/2t) ◦φ(1/2t). (4)\nThus, φ(1) = φ(1/2) ◦φ(1/2).\nIn practice, a neural network ﬁrst generates a displacement\nﬁeld, which is then scaled by 1 /2T to produce an initial de-\nJunyu Chen et al. / Medical Image Analysis (2022) 5\nFig. 3: The overall framework of the proposed Transformer-based image registration model, TransMorph. The proposed hybrid Transformer-ConvNet network\ntakes two inputs: a ﬁxed image and a moving image that is aﬃnely aligned with the ﬁxed image. The network generates a nonlinear warping function, which is then\napplied to the moving image through a spatial transformation function. If an image pair has not been aﬃnely aligned, an aﬃne Transformer may be used prior to the\ndeformable registration (left dashed box). Additionally, auxiliary anatomical segmentations may be leveraged during training the proposed network (right dashed\nbox).\nformation ﬁeld φ(1/2T ). Subsequently, the squaring technique\n(i.e., Eqn. 4) is applied recursively to φ(1/2T ) T times via a spa-\ntial transformation function, resulting in a ﬁnal di ﬀeomorphic\ndeformation ﬁeld φ(1). Despite the fact that di ﬀeomorphisms\nare theoretically guaranteed to be invertible, interpolation er-\nrors can lead to invertibility errors that increase linearly with\nthe number of interpolation steps (Avants et al. 2008; Mok and\nChung 2020).\n2.2. Self-attention Mechanism and Transformer\nTransformer makes use of a self-attention mechanism that es-\ntimates the relevance of one input sequence to another via the\nQuery-Key-Value (QKV) model (Vaswani et al. 2017; Dosovit-\nskiy et al. 2020). The input sequences often originate from the\nﬂattened patches of an image. Let x be an image volume de-\nﬁned over a 3D spatial domain (i.e., x ∈RH×W×L). The image\nis ﬁrst divided into N ﬂattened 3D patches xp ∈RN×P3\n, where\n(H,W,L) is the size of the original image, ( P,P,P) is the size\nof each image patch, and N = HWL\nP3 . Then, a learnable linear\nembedding E is applied to xp, which projects each patch into a\nD ×1 vector representation:\nˆxe = [x1\npE; x2\npE; ...; xN\np E], E ∈RP3×D (5)\nwhere the dimension D is a user-deﬁned hyperparemeter. Then,\na learnable positional embedding is added to ˆxe so that the\npatches can retain their positional information, i.e., xe = ˆxe +\nEpos, where Epos ∈ RN×D. These vector representations, of-\nten known as tokens, are subsequently used as inputs for self-\nattention computations.\nSelf-attention. To compute self-attention (SA), xe ∈RN×D is\nencoded by U (i.e., a linear layer) to three matrix represen-\ntations: Queries Q ∈ RN×Dk , Keys K ∈ RN×Dk , and Values\nV ∈RN×Dv . The scaled dot-product attention is given by:\n[Q,K,V] = xeUq,k,v Uq,k,v ∈RD×Dq,k,v ,\nA = softmax(QK⊤\n√Dk\n) A ∈RN×N ,\nS A(xe) = A V,\n(6)\nwhere A is the attention weight matrix, each element ofA repre-\nsents the pairwise similarity between two elements of the input\nsequence xe and their respective query and key representations.\nIn general, SA computes a normalized score for each input to-\nken based on the dot product of the Query and Key representa-\ntions. The score is subsequently applied to the Value represen-\ntation of the token, signifying to the network whether or not to\nfocus on this token.\nMulti-head self-attention. A Transformer employs multi-head\nself-attention (MSA) rather than a single attention function.\nMSA is an extension of self-attention in which h self-attention\noperations (i.e., “heads”) are processed in parallel, thereby ef-\nfectively increasing the number of trainable parameters. Then,\nthe outputs of the SA operations are concatenated then pro-\njected onto a D-dimensional representation:\nMS A(xe) =[S A1(xe); S A2(xe); ...; S Ah(xe)]UMS A, (7)\nwhere UMS A ∈Rh·Dh×D, and Dh is typically set to D/h in order\nto keep the number of parameters constant before and after the\nMSA operation.\n6 Junyu Chen et al. / Medical Image Analysis (2022)\n2.3. Bayesian Deep Learning\nUncertainty estimates help comprehend what a machine\nlearning model does not know. They indicate the likelihood that\na neural network may make an incorrect prediction. Because\nmost deep neural networks are incapable of providing an esti-\nmate of the uncertainty in their output values, their predictions\nare frequently taken at face value and thought to be correct.\nBayesian deep learning estimates predictive uncertainty, pro-\nviding a realistic paradigm for understanding uncertainty within\ndeep neural networks (Gal and Ghahramani 2016). The uncer-\ntainty caused by the parameters in a neural network is known as\nepistemic uncertainty, which is modeled by placing a prior dis-\ntribution (e.g., a Gaussian prior distribution: W ∼N(0,I)) on\nthe parameters of a network and then attempting to capture how\nmuch these weights vary given speciﬁc data. Recent e ﬀorts in\nthis area include the Bayes by Backprop (Blundell et al. 2015),\nits closely related mean-ﬁeld variational inference by assum-\ning a Gaussian prior distribution (T ¨olle et al. 2021), stochas-\ntic batch normalization (Atanov et al. 2018), and Monte-Carlo\n(MC) dropout (Gal and Ghahramani 2016; Kendall and Gal\n2017). The applications of Bayesian deep learning in med-\nical imaging expands on image denoising (T ¨olle et al. 2021;\nLaves et al. 2020b) and image segmentation (DeVries and Tay-\nlor 2018; Baumgartner et al. 2019; Mehrtash et al. 2020). In\ndeep-learning-based image registration, the majority of meth-\nods provide a single, deterministic solution of the unknown ge-\nometric transformation. Knowing about epistemic uncertainty\nhelps determine if and to what degree the registration results\ncan be trusted and whether the input data is appropriate for the\nneural network.\nIn general, two categories of registration uncertainty may\nbe modeled using the epistemic uncertainty of a deep learning\nmodel: transformation uncertainty and appearance uncertainty\n(Luo et al. 2019; Xu et al. 2022). Transformation uncertainty\nmeasures the local ambiguity of the spatial transformation (i.e.,\nthe deformation), whereas appearance uncertainty quantiﬁes\nthe uncertainty in the intensity values of registered voxels or\nthe volumes of the registered organs. Transformation uncer-\ntainty estimates may be used for uncertainty-weighted registra-\ntion (Simpson et al. 2011; Kybic 2009), surgical treatment plan-\nning, or directly visualized for qualitative evaluations (Yang\net al. 2017b). Appearance uncertainty may be translated into\ndose uncertainties in cumulative dose for radiation or radiophar-\nmaceutical therapy (Risholm et al. 2011; Vickress et al. 2017;\nChetty and Rosu-Bubulac 2019; Gear et al. 2018). These regis-\ntration uncertainty estimates also enable the assessment of oper-\native risks and leads to better-informed clinical decisions (Luo\net al. 2019). Cui et al. (Cui et al. 2021) and Yang et al. (Yang\net al. 2017b) incorporated MC dropout layers in their registra-\ntion network designs, which allows for the estimation of trans-\nformation uncertainty by sampling multiple deformation ﬁeld\npredictions from the network.\nThe proposed image registration framework expands on these\nideas. In particular, a new registration framework is presented\nthat leverages a Transformer in the network design. We demon-\nstrate that this framework can be readily adapted to several ex-\nisting techniques to allow di ﬀeomorphism for image registra-\ntion, and incorporate Bayesian deep learning to estimate regis-\ntration uncertainty.\n3. Methods\nThe conventional paradigm of image registration is shown in\nFig. 2. The moving and ﬁxed images, denoted respectively as\nˆIm and If , are ﬁrst aﬃnely transformed into a single coordinate\nsystem. The resulting a ﬃne-aligned moving image is denoted\nas Im. Subsequently, Im is warped to If using a deformation\nﬁeld, φ, generated by a DIR algorithm (i.e., Im ◦φ). Fig. 3\npresents an overview of the proposed method. Here, both the\naﬃne transformation and the deformable registration are per-\nformed using Transformer-based neural networks. The a ﬃne\nTransformer takes ˆIm and If as inputs and computes a set of\naﬃne transformation parameters (e.g., rotation angle, transla-\ntion, etc.). These parameters are used to aﬃnely align ˆIm with If\nvia an aﬃne transformation function, yielding an aligned image\nIm. Then, a DIR network computes a deformation ﬁeld φgiven\nIm and If , which warps Im using a spatial transformation func-\ntion (i.e., ˆIf = Im ◦φ). During training, the DIR network may\noptionally include supplementary information (e.g., anatomical\nsegmentation). The network architectures, the loss and regular-\nization functions, and the variants of the method are described\nin detail in the following sections.\nFig. 4: The framework of the proposed Transformer-based aﬃne model.\n3.1. A ﬃne Transformation Network\nAﬃne transformation is often used as the initial stage in im-\nage registration because it facilitates the optimization of the fol-\nlowing more complicated DIR processes (de V os et al. 2019).\nAn aﬃne network examines a pair of moving and ﬁxed images\nglobally and produces a set of transformation parameters that\naligns the moving image with the ﬁxed image. Here, the archi-\ntecture of the proposed Transformer-based a ﬃne network is a\nmodiﬁed Swin Transformer (Liu et al. 2021a) that takes two 3D\nvolumes as the inputs (i.e., If and ˆIm) and generates 12 a ﬃne\nparameters: three rotation angles, three translation parameters,\nthree scaling parameters, and three shearing parameters. The\ndetails and a visualization of the architecture are shown in Fig.\nA.19 in the Appendix. We reduced the number of parameters in\nthe original Swin Transformer due to the relative simplicity of\naﬃne registration. The speciﬁcs of the Transformer’s architec-\nture and parameter settings are covered in a subsequent section.\nJunyu Chen et al. / Medical Image Analysis (2022) 7\nFig. 5: (a): Swin Transformer creates hierarchical feature maps by merging image patches. The self-attention is computed within each local 3D window (the\nred box). The feature maps generated at each resolution are sent into a ConvNet decoder to produce an output. (b): The 3D cyclic shift of local windows for\nshifted-window-based self-attention computation.\n3.2. Deformable Registration Network\nFig. 1 shows the network architecture of the proposed\nTransMorph. The encoder of the network ﬁrst splits the input\nmoving and ﬁxed volumes into non-overlapping 3D patches,\neach of size 2×P×P×P, where P is typically set to 4 (Dosovit-\nskiy et al. 2020; Liu et al. 2021a; Dong et al. 2021). We denote\nthe ith patch as xi\np, where i ∈{1,..., N}and N = H\nP ×W\nP ×L\nP is the\ntotal number of patches. Each patch is ﬂattened and regarded as\na “token”, and then a linear projection layer is used to project\neach token to a feature representation of an arbitrary dimension\n(denoted as C):\nz0 = [x1\npE; x2\npE; ...; xN\np E], (8)\nwhere E ∈R2P3×C denotes the linear projection, and the output\nz0 has a dimension of N ×C.\nBecause the linear projection operates on image patches and\ndoes not keep the token’s location relative to the image as a\nwhole, previous Transformer-based models often added a posi-\ntional embedding to the linear projections in order to integrate\nthe positional information into tokens, i.e. z0 + Epos (Vaswani\net al. 2017; Dosovitskiy et al. 2020; Liu et al. 2021a; Dong\net al. 2021). Such Transformers were primarily designed for\nimage classiﬁcation, where the output is often a vector describ-\ning the likelihood of an input image being classiﬁed as a certain\nclass. Thus, if the positional embedding is not employed, the\nTransformer may lose the positional information. However, for\npixel-level tasks such as image registration, the network often\nincludes a decoder that generates a dense prediction with the\nsame resolution as the input or target image. The spatial cor-\nrespondence between voxels in the output image is enforced\nby comparing the output with the target image using a loss\nfunction. Any spatial mismatches between output and target\nwould contribute to the loss and be backpropagated into the\nTransformer encoder. The Transformer should thereby inher-\nently capture the tokens’ positional information. In this work,\nwe observed, as will be shown in section 6.1.2, that positional\nembedding is not necessary for image registration, and it only\nadds extra parameters to the network without improving perfor-\nmance.\nFollowing the linear projection layer, several consecutive\nstages of patch merging and Swin Transformer blocks (Liu\net al. 2021a) are applied on the tokens z0. The Swin Trans-\nformer blocks outputs the same number of tokens as the in-\nput, while the patch merging layers concatenate the features\nof each group of 2 ×2 ×2 neighboring tokens, thus they re-\nduce the number of tokens by a factor of 2 ×2 ×2 = 8 (e.g.,\nH ×W ×L×C − →H\n2 ×W\n2 ×L\n2 ×8C). Then, a linear layer is applied\non the 8 C-dimensional concatenated features to produce fea-\ntures each of 2 C-dimension. After four stages of Swin Trans-\nformer blocks and three stages of patch merging in between the\nTransformer stages (i.e., orange boxes in Fig. 1), the output di-\nmension at the last stage of the encoder is H\n32 ×W\n32 × L\n32 ×8C.\nThe decoder consists of successive upsampling and convolu-\ntional layers with the kernel size of 3 ×3. Each of the up-\nsampled feature maps in the decoding stage was concatenated\nwith the corresponding feature map from the encoding path via\nskip connections, then followed by two consecutive convolu-\ntional layers. As shown in Fig. 1, the Transformer encoder\ncan only provide feature maps up to a resolution of H\nP ×W\nP ×L\nP\nowing to the nature of patch operation (denoted by the orange\narrows). Hence, Transformer may fall short of delivering high-\nresolution feature maps and aggregating local information at\nlower layers (Raghu et al. 2021). To address this shortcoming,\nwe employed two convolutional layers using the original and\ndownsampled image pair as inputs to capture local information\nand generate high-resolution feature maps. The outputs of these\nlayers were concatenated with the feature maps in the decoder\nto produce a deformation ﬁeld. The output deformation ﬁeld,\nφ, was generated the application of sixteen 3 ×3 convolutions.\nExcept for the last convolutional layer, each convolutional layer\nis followed by a Leaky Rectiﬁed Linear Unit (Maas et al. 2013)\nactivation. Finally, the spatial transformation function (Jader-\nberg et al. 2015) is used to apply a nonlinear warp to the moving\nimage Im with the deformation ﬁeld φ(or the displacement ﬁeld\nu) provided by the network.\nIn the next subsections, we discuss the Swin Transformer\nblock, the spatial transformation function, and the loss func-\ntions in detail.\n3.2.1. 3D Swin Transformer Block\nSwin Transformer (Liu et al. 2021a) can generate hierarchi-\ncal feature maps at various resolutions by using patch merging\nlayers, making it ideal for usage as a general-purpose back-\n8 Junyu Chen et al. / Medical Image Analysis (2022)\nbone for pixel-level tasks like image registration and segmen-\ntation. Swin Transformer’s most signiﬁcant component, apart\nfrom patch merging layers, is the shifted window-based self-\nattention mechanism. Unlike ViT (Dosovitskiy et al. 2020),\nwhich computes the relationships between a token and all other\ntokens at each step of the self-attention modules. Swin Trans-\nformer computes self-attention within the evenly partitioned\nnon-overlapping local windows of the original and the lower\nresolution feature maps (as shown in Fig. 5 (a)). In contrast\nto the original Swin Transformer, this work uses rectangular-\nparallelepiped windows to accommodate non-square images,\nand each has a shape of Mx ×My ×Mz. At each resolution,\nthe ﬁrst Swin Transformer block employs a regular window\npartitioning method, beginning with the top-left voxel, and the\nfeature maps are evenly partitioned into non-overlapping win-\ndows of size Mx ×My ×Mz. The self-attention is then cal-\nculated locally within each window. To introduce connections\nbetween neighboring windows, the Swin Transformer uses a\nshifted window design: in the successive Swin Transformer\nblocks, the windowing conﬁguration shifts from that of the pre-\nceding block, by displacing the windows in the preceding block\nby (\n⌊Mx\n2\n⌋\n×\n⌊My\n2\n⌋\n×\n⌊Mz\n2\n⌋\n) voxels. As illustrated by an example in\nFig. 5 (b), the input feature map has 4 ×8 ×12 voxels. With a\nwindow size of 2 ×4 ×6, the feature map is evenly partitioned\ninto 2 ×2 ×2 = 8 windows in the ﬁrst Swin Transformer block\n(“Swin Block 1” in Fig. 5 (b)). Then, in the next block, the\nwindows are shifted by (\n⌊2\n2\n⌋\n×\n⌊4\n2\n⌋\n×\n⌊6\n2\n⌋\n) = (1 ×2 ×3), and the\nnumber of windows becomes 3 ×3 ×3 = 27. We extended the\noriginal 2D eﬃcient batch computation (i.e., cyclic shift) (Liu\net al. 2021a,b) to 3D and applied it to the 27 shifted windows,\nkeeping the ﬁnal number of windows for attention computa-\ntion at 8. With the windowing-based attention, two consecutive\nSwin Transformer blocks can be computed as:\nˆzℓ = W-MSA(LN(zℓ−1)) + zℓ−1,\nzℓ = MLP(LN(ˆzℓ)) + ˆzℓ,\nˆzℓ+1 = SW-MSA(LN(zℓ)) + zℓ,\nzℓ+1 = MLP(LN(ˆzℓ+1)) + ˆzℓ+1,\n(9)\nwhere W-MSA and SW-MSA denote, respectively, window-\nbased multi-head self-attention and shifted-window-based\nmulti-head self-attention modules; MLP denotes the multi-layer\nperceptron module (Vaswani et al. 2017); ˆzℓ and zℓ denote the\noutput features of the (S)W-MSA and the MLP module for\nblock ℓ, respectively. The self-attention is computed as:\nA(Q,K,V) = softmax( QK⊤\n√\nd\n+ B)V, (10)\nwhere Q,K,V ∈RMx My Mz×d are query, key, value matrices, d\ndenotes the dimension of query and key features, Mx My Mz is\nthe number of tokens in a 3D window, and B represents the\nrelative position of tokens in each window. Since the relative\nposition between tokens along each axis (i.e., x,y,z) can only\ntake values from [−Mx,y,z+1,Mx,y,z−1], the values inB are taken\nfrom a smaller bias matrix ˆB ∈ R(2Mx−1)×(2My−1)×(2Mz−1). For\nthe reasons given previously, we will show in section 6.1.2 that\npositional bias B is not needed for the proposed network and\nthat it just adds extra parameters without improving registration\nperformance.\n3.2.2. Loss Functions\nThe overall loss function for network training derives from\nthe energy function of traditional image registration algorithms\n(i.e., Eqn. (1)). The loss function consists of two parts: one\ncomputes the similarity between the deformed moving and the\nﬁxed images, and another one regularizes the deformation ﬁeld\nso that it is smooth:\nL(If ,Im,φ) = Lsim(If ,Im,φ) + λR(φ), (11)\nwhere Lsim denotes the image ﬁdelity measure, and Rdenotes\nthe deformation ﬁeld regularization.\nImage Similarity Measure. In this work, we experimented with\ntwo widely-used similarity metric for Lsim. The ﬁrst was the\nmean squared error, which was the mean of the squared di ﬀer-\nence in voxel values between If and Im:\nMS E(If ,Im,φ) = 1\nΩ\n∑\np∈Ω\n|If (p) −[Im ◦φ](p)|2, (12)\nwhere p denotes the voxel location, and Ω represents the image\ndomain.\nAnother similarity metric used was the local normalized\ncross-correlation between If and Im:\nLNCC (If ,Im,φ) =\n∑\np∈Ω\n(∑\npi ( f (pi) − ¯f (p))([Im ◦φ](pi) −[ ¯Im ◦φ](p))\n)2\n(∑\npi ( f (pi) − ¯f (p))2\n)(∑\npi ([Im ◦φ](pi) −[ ¯Im ◦φ](p))2\n),\n(13)\nwhere ¯If (p) and ¯Im(p) denotes the mean voxel value within the\nlocal window of size n3 centered at voxel p. We used n = 9 in\nthe experiments.\nDeformation Field Regularization. Optimizing the similarity\nmetric alone would encourage Im ◦φ to be visually as close\nas possible to If . The resulting deformation ﬁeld φ, however,\nmight not be smooth or realistic. To impose smoothness in the\ndeformation ﬁeld, a regularizerR(φ) was added to the loss func-\ntion. R(φ) encourages the displacement value in a location to be\nsimilar to the values in its neighboring locations. Here, we ex-\nperimented with two regularizers. The ﬁrst was the di ﬀusion\nregularizer Balakrishnan et al. 2019:\nRdi f f usion(φ) =\n∑\np∈Ω\n∥∇u(p)∥2, (14)\nwhere u(p) is the spatial gradients of the displacement ﬁeld u.\nThe spatial gradients were approximated using forward di ﬀer-\nences, that is, ∂u(p)\n∂{x,y,z} ≈u(p{x,y,z}+ 1) −u(p{x,y,z}).\nThe second regularizer was bending energy (Rueckert et al.\n1999), which penalizes sharply curved deformations, thus, it\nJunyu Chen et al. / Medical Image Analysis (2022) 9\nmay be helpful for abdominal organ registration. Bending en-\nergy operates on the second derivative of the displacement ﬁeld\nu, and it is deﬁned as:\nRbending(φ) =\n∑\np∈Ω\n∥∇2u(p)∥2 =\n∑\np∈Ω\n[( ∂2u(p)\n∂x2\n)2\n+\n(∂2u(p)\n∂y2\n)2\n+\n(∂2u(p)\n∂z2\n)2\n+ 2\n(∂2u(p)\n∂xz\n)2\n+ 2\n(∂2u(p)\n∂xy\n)2\n+ 2\n(∂2u(p)\n∂yz\n)2 ]\n,\n(15)\nwhere the derivatives were estimated using the same forward\ndiﬀerences that were used previously.\nAuxiliary Segmentation Information. When the organ segmen-\ntations of If and Im are available, TransMorph may lever-\nage this auxiliary information during training to improve the\nanatomical mapping between Im ◦φand If . A loss function Lseg\nthat quantiﬁes the segmentation overlap is added to the overall\nloss function (Eqn. 11):\nL(If ,Im,φ) = Lsim(If ,Im,φ) + λR(φ) + γLseg(sf ,sm,φ), (16)\nwhere sf and sm represent, respectively, the organ segmentation\nof If and Im, and γ is a weighting parameter that controls the\nstrength of Lseg. In the ﬁeld of image registration, it is common\nto use Dice score (Dice 1945) as a ﬁgure of merit to quantify\nregistration performance. Therefore, we directly minimized the\nDice loss (Milletari et al. 2016) between sk\nf and sk\nm, where k\nrepresents the kth structure/organ:\nDice(sf ,sm,φ) = 1−\n1\nK\n∑\nk\n2 ∑\np∈Ω sk\nf (p)[sk\nm ◦φ](p)\n∑\np∈Ω\n(\nsk\nf (p)\n)2\n+ ∑\np∈Ω\n(\n[skm ◦φ](p)\n)2 . (17)\nTo allow backpropagation of the Dice loss, we used a method\nsimilar to that described in (Balakrishnan et al. 2019), in which\nwe designed sf and sm as image volumes with K channels, each\nchannel containing a binary mask deﬁning the segmentation of\na speciﬁc structure/organ. Then, sm ◦φis computed by warping\nthe K-channel sm with φ using linear interpolation so that the\ngradients of Lseg can be backpropagated into the network.\n3.3. Probabilistic and B-spline Variants\nIn this section, we demonstrate that by simply altering the\ndecoder, TransMorph can be used in conjunction with the con-\ncepts from prior research to ensure a di ﬀeomorphic deforma-\ntion such that the resulting deformable mapping is continu-\nous, di ﬀerentiable, and topology-preserving. The di ﬀeomor-\nphic registration was achieved using the scaling-and-squaring\napproach (described in section 2.1.2) with a stationary ve-\nlocity ﬁeld representation (Arsigny et al. 2006). Two exist-\ning di ﬀeomorphic models, VoxelMorph-diff (Dalca et al.\n2019) and MIDIR (Qiu et al. 2021), have been adopted as\nbases for the proposed TransMorph diﬀeomorphic variants,\ndesignated by TransMorph-diff (section Appendix H) and\nTransMorph-bspl (section Appendix I), respectively. The ar-\nchitectures of the two variants are shown in Fig. 6. The detailed\nderivation of these two variants are listed in Appendix.\nTransMorph-diff was trained using the same loss func-\ntions as VoxelMorph-diff (Dalca et al. 2019):\nLprob.(If ,Im,φu; ψ)\n= −Eu∼qψ\n[\nlog p(If |u,Im)\n]\n+ KL\n[\nqψ(u|If ,Im)∥p(u)\n]\n= 1\n2σ2 ∥If −Im ◦φu∥2 + 1\n2\n[\ntr(λDΣψ −log Σψ) + µ⊤\nψΛuµψ\n]\n,\n(18)\nand when anatomical label maps are available:\nLprob. w/ aux.(If ,sf ,Im,sm,φu; ψ)\n= 1\n2σ2 ∥If −Im ◦φu∥2 + 1\n2σ2s\n∥sf −sm ◦φu∥2\n+ 1\n2\n[\ntr(λDΣψ −log Σψ) + µ⊤\nψΛuµψ\n]\n.\n(19)\nHowever, it is important to note that in (Dalca et al. 2019), sf\nand sm represent anatomical surfaces obtained from label maps.\nIn contrast, we directly used the label maps as sf and sm in this\nwork. They were image volumes with multiple channels, each\nchannel contained a binary mask deﬁning the segmentation of\na certain structure/organ.\nFig. 6: The probabilistic and B-spline variants of TransMorph. (a): The archi-\ntecture of the probabilistic di ﬀeomorphic TransMorph. (b): The architecture\nof the B-spline diﬀeomorphic TransMorph.\n3.4. Bayesian Uncertainty Variant\nIn this section, we extend the proposed TransMorph to a\nBayesian neural network (BNN) using the variational infer-\nence framework with Monte Carlo dropout (Gal and Ghahra-\nmani 2016), for which we refer readers to (Gal and Ghahra-\nmani 2016; Yang et al. 2017a, 2016) for both theoretical\nand technical details. We denoted the resulting model as\nTransMorph-Bayes. In this model, Dropout layers were in-\nserted into the Transformer encoder of the TransMorph archi-\ntecture but not into the ConvNet decoder, in order to avoid im-\nposing excessive regularity for the network parameters and thus\ndecreasing performance. We added a dropout layer after each\n10 Junyu Chen et al. / Medical Image Analysis (2022)\nTable 1: The ablation study ofTransMorph models with skip connections and positional embedding. “Conv. skip.” denotes the skip-connections from convolutional\nlayers (indicated by green arrows in Fig. 1); “Trans. skip,” denotes the skip-connections from the Transformer blocks (indicated by orange arrows in Fig. 1); “lrn.\npositional embedding” denotes the learnable positional embedding; “sin. positional embedding” denotes the sinusoidal positional embedding.\nModel Conv. skip. Trans. skip. Parameters (M)\nw/o conv. skip. ✓ - 46.70\nw/o Trans. skip. - ✓ 41.55\nw/o positional embedding ✓ ✓ 46.77\nw/ shuﬄing ✓ ✓ 46.77\nw/ rel. positional bias ✓ ✓ 46.77\nw/ lrn. positional embedding ✓ ✓ 63.63\nw/ sin. positional embedding ✓ ✓ 46.77\nTable 2: The architecture hyperparameters of the TransMorph models used in the ablation study. “Embed. Dimension” denotes the embedding dimension, C, in\nthe very ﬁrst stage (described in section 3.2); “Swin-T.” denotes Swin Transformer.\nModel Embed. Dimension Swin-T. block numbers Head numbers Parameters (M)\nTransMorph 96 {2, 2, 4, 2} { 4, 4, 8, 8} 46.77\nTransMorph-tiny 6 {2, 2, 4, 2} { 4, 4, 8, 8} 0.24\nTransMorph-small 48 {2, 2, 4, 2} { 4, 4, 4, 4} 11.76\nTransMorph-large 128 {2, 2, 12, 2} { 4, 4, 8, 16} 108.34\nVoxelMorph-huge - - - 63.25\nfully connected layer in the MLPs (Eqn. 9) and after each self-\nattention computation (Eqn. 10). Note that these are the loca-\ntions where dropout layers are commonly used for Transformer\ntraining. We set the dropout probability p to 0.15 to further\navoid the network imposing an excessive degree of regularity\non the network weights.\nBoth the transformation and appearance uncertainty can be\nestimated as the variability from the predictive mean (i.e., the\nvariance), where the predictive mean of the deformation ﬁelds\nand the deformed images can be estimated by Monte Carlo in-\ntegration (Gal and Ghahramani 2016):\nˆφ= 1\nT\nT∑\nt=1\nφt, (20)\nand\nˆIf = 1\nT\nT∑\nt=1\nIm ◦φt. (21)\nThis is equivalent to averaging the output of T forward passes\nthrough the network during inference, where φt represents the\ndeformation ﬁeld produced by tth forward pass. The transfor-\nmation and appearance uncertainty can be estimated using the\npredictive variances of the deformation ﬁelds and the deformed\nimages, respectively, as:\nˆΣ2\nφ = 1\nT\nT∑\nt=1\n(\nφt −ˆφf\n)2\n, (22)\nand\nˆΣ2\nf = 1\nT\nT∑\nt=1\n(\nIm ◦φt −ˆIf\n)2\n. (23)\n3.4.1. Appearance uncertainty calibration\nAn ideal uncertainty estimate should be properly correlated\nto the inaccuracy of the registration results; that is, a high un-\ncertainty value should indicate a large registration error, and\nvice versa. Otherwise, doctors /surgeons may be misled by the\nerroneous estimate of registration uncertainty and place unwar-\nranted conﬁdence in the registration results, resulting in severe\nconsequences (Luo et al. 2019; Risholm et al. 2013, 2011). The\nappearance uncertainty given by Eqn. 23 is expressed as the\nvariability from the mean model prediction. Such an appear-\nance uncertainty estimation does not account for the systematic\nerrors (i.e., bias) between the mean registration prediction and\nthe target image; therefore, a low uncertainty value given by\nEqn. 23 does not always guarantee an accurate registration re-\nsult.\nWhen the predicted uncertainty values closely corresponded\nto the expected model error, the uncertainty estimates are con-\nsidered to be well-calibrated (Laves et al. 2019; Levi et al.\n2019). In an ideal scenario, the estimated registration uncer-\ntainty should completely reﬂect the actual registration error. For\ninstance, if the predictive variance of a batch of registered im-\nages generated by the network is found to be 0.5, the expecta-\ntion of the squared error should likewise be 0.5. Accordingly,\nif the expected model error is quantiﬁed by MSE, then the per-\nfect calibration of appearance uncertainty may be deﬁned as the\nfollowing (Guo et al. 2017; Levi et al. 2019; Laves et al. 2020c):\nEˆΣ2\n[\n∥Im ◦φ−If ∥2|ˆΣ2 = Σ2]\n= Σ2 ∀\n{\nΣ2 ∈R|Σ2 ≥0\n}\n. (24)\nIn the conventional paradigm of Bayesian neural networks,\nthe uncertainty estimate is derived from the predictive variance\nˆΣ2 relative to the predictive mean ˆIf as in Eqn. 23. However,\nit can be shown that this predictive variance can be miscali-\nbrated as a result of overﬁtting the training dataset (as shown\nin Appendix B). Therefore, the uncertainty values estimated\nbased on ˆΣ2\nf in Eqn. 23 may be biased. This bias must be\ncorrected in applications such as image denoising or classiﬁ-\ncation (Laves et al. 2019; Guo et al. 2017; Kuleshov et al. 2018;\nJunyu Chen et al. / Medical Image Analysis (2022) 11\nPhan et al. 2018; Laves et al. 2020c,a), such that the uncertainty\nvalues closely reﬂect the expected error. In image registration,\nhowever, the expected appearance error may be computed even\nduring the test time since the target image is always known.\nTherefore, a perfectly calibrated appearance uncertainty quan-\ntiﬁcation may be achieved without additional e ﬀort. Here, we\npropose to replace the predicted mean ˆIf with the target image\nIf in Eqn. 23. Then, the appearance uncertainty is the equiva-\nlent to the expected error:\nΣ2\nf = err(Im ◦φ) = 1\nT\nT∑\nt=1\n(\nIm ◦φt −If\n)2\n. (25)\nA comparison between the two appearance uncertainty estimate\nmethods (i.e., ˆΣ2\nf and Σ2\nf ) is shown later in this paper.\n4. Experiments\n4.1. Datasets and Preprocessing\nThree datasets including over 1000 image pairs were used to\nthoroughly validate the proposed method. The details of each\ndataset are described in the following sections.\n4.1.1. Inter-patient Brain MRI Registration\nFor the inter-patient brain MR image registration dataset, we\nused a dataset of 260 T1–weighted brain MRI images acquired\nat Johns Hopkins University. The images were anonymized and\nacquired under IRB approval. The dataset was split into 182,\n26, and 52 (7:1:2) volumes for training, validation, and test sets.\nEach image volume was used as a moving image to form two\nimage pairs by randomly matching it to two other volumes in\nthe set (i.e., the ﬁxed images). Then, the moving and ﬁxed im-\nages were inverted to form another two image pairs, resulting\nin four registration pairings of If and Im. The ﬁnal data com-\nprises 768, 104, and 208 image pairs for training, validation,\nand testing, respectively. FreeSurfer (Fischl 2012) was used to\nperform standard pre-processing procedures for structural brain\nMRI, including skull stripping, resampling, and aﬃne transfor-\nmation. The pre-processed image volumes were all cropped to\nsize of 160 ×192 ×224. Label maps including 30 anatomical\nstructures were obtained using FreeSurfer for evaluating regis-\ntration performances.\n4.1.2. Atlas-to-patient Brain MRI Registration\nWe used a publicly available dataset to evaluate the proposed\nmodel with atlas-to-patient brain MRI registration task. A total\nnumber of 576 T1–weighted brain MRI images from the Infor-\nmation eXtraction from Images (IXI) database2 was used as the\nﬁxed images. The moving image for this task was an atlas brain\nMRI obtained from (Kim et al. 2021). The dataset was split into\n403, 58, and 115 (7:1:2) volumes for training, validation, and\ntest sets. FreeSurfer was used to pre-process the MRI volumes.\nWe carried out the same pre-processing procedures we used for\n2https://brain-development.org/ixi-dataset/\nthe previous dataset applied to the IXI dataset. All image vol-\numes were cropped to size of 160 ×192 ×224. Label maps\nof 30 anatomical structures were used to evaluate registration\nperformances.\n4.1.3. Learn2Reg OASIS Brain MRI Registration\nWe additionally evaluated TransMorph on a public regis-\ntration challenge, OASIS (Marcus et al. 2007; Hoopes et al.\n2021), obtained from the 2021 Learn2Reg challenge (Hering\net al. 2021) for inter-patient registration. This dataset contains\na total of 451 brain T1 MRI images, with 394, 19, and 38 im-\nages being used for training, validation, and testing, respec-\ntively. FreeSurfer (Fischl 2012) was used to pre-process the\nbrain MRI images, and label maps for 35 anatomical structures\nwere provided for evaluation.\n4.1.4. XCAT-to-CT Registration\nComputerized phantoms have been widely used in the med-\nical imaging ﬁeld for algorithm optimization and imaging sys-\ntem validation (Christo ﬀersen et al. 2013; Chen et al. 2019;\nZhang et al. 2017). The four-dimensional extended cardiac-\ntorso (XCAT) phantom (Segars et al. 2010) was developed\nbased on anatomical images from the Visible Human Project\ndata. While the current XCAT phantom3 can model anatomical\nvariations through organ and phantom scaling, it cannot com-\npletely replicate the anatomical variations seen in humans. As\na result, XCAT-to-CT registration (which can be thought of as\natlas-to-image registration) has become a key method for creat-\ning anatomically variable phantoms (Chen et al. 2020; Fu et al.\n2021; Segars et al. 2013). This research used a CT dataset\nfrom (Segars et al. 2013) that includes 50 non-contrast chest-\nabdomen-pelvis (CAP) CT scans that are part of the Duke Uni-\nversity imaging database. Selected organs and structures were\nmanually segmented in each patient’s CT scan. The structures\nsegmented included the following: the body outline, the bone\nstructures, lungs, heart, liver, spleen, kidneys, stomach, pan-\ncreas, large intestine, prostate, bladder, gall bladder, and thy-\nroid. The manual segmentation was done by several medi-\ncal students, and the results were subsequently corrected by\nan experienced radiologist at Duke University. The CT vol-\numes have voxel sizes ranging from 0 .625 ×0.625 ×5mm to\n0.926 ×0.926 ×5mm. We used trilinear interpolation to resam-\nple all volumes to an identical voxel spacing of 2.5×2.5×5mm.\nThe volumes were all cropped and zero-padded to have a size of\n160 ×160 ×160 voxels. The intensity values were ﬁrst clipped\nin the range of [−1000,700] Hounsﬁeld Units and then normal-\nized to the range of [0,1]. The XCAT attenuation map was gen-\nerated with a resolution of 1.1 ×1.1 ×1.1mm using the material\ncompositions and attenuation coeﬃcients of the constituents at\n120 keV . It was then resampled, cropped, and padded so that\nthe resulting volume matched the size of the CT volumes. The\nXCAT attenuation map’s intensity values were also normal-\nized to be within a range of [0 ,1]. The XCAT and CT images\nwere rigidly registered using the proposed a ﬃne network. The\n3as of October, 2021\n12 Junyu Chen et al. / Medical Image Analysis (2022)\ndataset was split into 35, 5, and 10 (7:1:2) volumes for training,\nvalidation, and testing. We conducted ﬁve-fold cross-validation\non the ﬁfty image volumes, resulting in 50 testing volumes in\ntotal.\n4.2. Baseline Methods\nWe compared TransMorph to various registration methods\nthat have previously demonstrated state-of-the-art registration\nperformance. We begin by comparing TransMorph with four\nnon-deep-learning-based methods. The hyper-parameters of\nthese methods, unless otherwise speciﬁed, were empirically set\nto balance the trade-oﬀ between registration accuracy and run-\nning time. The methods and their hyperparameter settings are\ndescribed below:\n• SyN4(Avants et al. 2008): For both inter-patient and atlas-\nto-patient brain MR registration tasks, we used the mean\nsquared diﬀerence (MSQ) as the objective function, along\nwith a default Gaussian smoothing of 3 and three scales\nwith 180, 80, 40 iterations, respectively. For XCAT-to-CT\nregistration, we used cross-correlation (CC) as the objec-\ntive function, a Gaussian smoothing of 5 and three scales\nwith 160, 100, 40 iterations, respectively.\n• NiftyReg5(Modat et al. 2010): We used the sum of\nsquared di ﬀerences (SSD) as the objective function and\nbending energy as a regularizer for all registration tasks.\nFor inter-patient brain MR registration, we empirically\nused a regularization weighting of 0.0002 and three\nscales with 300 iterations each. For atlas-to-patient brain\nMR registration, the regularization weighting was set to\n0.0006, and we used three scales with 500 iterations each.\nFor XCAT-to-CT registration, we used a regularization\nweight of 0.0005 and ﬁve scales with 500 iterations each.\n• deedsBCV6 (Heinrich et al. 2015): The objective function\nwas self-similarity context (SSC) (Heinrich et al. 2013b)\nby default. For both inter-patient and atlas-to-patient brain\nMR registration, we used the hyperparameter values sug-\ngested in (Ho ﬀmann et al. 2020) for neuroimaging, in\nwhich the grid spacing, search radius, and quantization\nstep were set to 6 ×5 ×4 ×3 ×2, 6 ×5 ×4 ×3 ×2,\nand 5 ×4 ×3 ×2 ×1, respectively. For XCAT-to-CT\nregistration, we used the default parameters suggested for\nabdominal CT registration (Heinrich et al. 2015), where\nthe grid spacing, search radius, and quantization step were\n8 ×7 ×6 ×5 ×4, 8 ×7 ×6 ×5 ×4, and 5 ×4 ×3 ×2 ×1,\nrespectively.\n• LDDMM7 (Beg et al. 2005): MSE was used as the objective\nfunction by default. For both inter-patient and atlas-to-\npatient brain MR registration, we used the smoothing ker-\nnel size of 5, the smoothing kernel power of 2, the match-\ning term coeﬃcient of 4, the regularization term coeﬃcient\nof 10, and the iteration number of 500. For XCAT-to-CT\n4https://github.com/ANTsX/ANTsPy\n5https://www.ucl.ac.uk/medical-image-computing\n6https://github.com/mattiaspaul/deedsBCV\n7https://github.com/brianlee324/torch-lddmm\nregistration, we used the same kernel size, kernel power,\nthe matching term coeﬃcient, and the number of iteration.\nHowever, the regularization term coe ﬃcient was empiri-\ncally set to 3.\nNext, we compared the proposed method with several exist-\ning deep-learning-based methods. For a fair comparison, unless\notherwise indicated, the loss function (Eqn. 11) that consists\nof MSE (Eqn. 12) and di ﬀusion regularization (Eqn. 14) was\nused for inter-patient brain MR registration, while we instead\nused LNCC (Eqn. 13) for atlas-to-patient MRI registration. For\nXCAT-to-CT registration, we used the loss function (Eqn. 16)\nthat consists of LNCC (Eqn. 13), bending energy (Eqn. 15), and\nDice loss (Eqn. 17). Auxiliary data (organ segmentation) was\nused for XCAT-to-CT registration only. Recall that the hyper-\nparameters λand γdeﬁne, respectively, the weight for deforma-\ntion ﬁeld regularization and Dice loss. The detailed parameter\nsettings used for each method were as follows:\n• VoxelMorph8 (Balakrishnan et al. 2018, 2019): We em-\nployed two variants of VoxelMorph, the second variant\ndoubles the number of convolution ﬁlters in the ﬁrst vari-\nant; they are designated as VoxelMorph-1 and -2, respec-\ntively. For inter-patient and atlas-to-patient brain MR reg-\nistration, the regularization hyperparameter λwas set, re-\nspectively, to 0.02 and 1, where these values were reported\nas the optimal values in Balakrishnan et al. 2019. For\nXCAT-to-CT registration, we setλ= γ= 1.\n• VoxelMorph-diff9 (Dalca et al. 2019): For both inter-\npatient and atlas-to-patient brain MR registration tasks, the\nloss function Lprob. (Eqn. 18) was used with σset to 0.01\nand λset to 20. For XCAT-to-CT registration, we used the\nloss function Lprob.w/aux. (Eqn. 19) with σ = σs = 0.01\nand λ= 20.\n• CycleMorph10 (Kim et al. 2021): In CycleMorph, the hy-\nerparameters α, β, and λ, correspond to the weights for cy-\ncle loss, identity loss, and deformation ﬁeld regularization.\nFor inter-patient brain MR registration, we set α = 0.1,\nβ = 0.5, and λ = 0.02. Whereas for atlas-to-patient brain\nMR registration, we setα= 0.1, β= 0.5, and λ= 1. These\nvalues were recommended in (Kim et al. 2021) as the op-\ntimal values for neuroimaging. For XCAT-to-CT registra-\ntion, we modiﬁed the CycleMorph by adding a Dice loss\nwith a weighting of 1 to incorporate organ segmentation\nduring training, and we set α = 0.1 and β = 1. We ob-\nserved that the λvalue of 1 suggested in (Kim et al. 2021)\nyielded over-smoothed deformation ﬁeld in our applica-\ntion. Therefore, the value of λwas decreased to 0.1.\n• MIDIR11 (Qiu et al. 2021): The same loss function and λ\nvalue as V oxelMorph were used. In addition, the control\npoint spacing δfor B-spline transformation was set to 2 for\nall tasks, which was shown to be an optimal value in Qiu\net al. 2021.\n8http://voxelmorph.csail.mit.edu\n9http://voxelmorph.csail.mit.edu\n10https://github.com/boahK/MEDIA_CycleMorph\n11https://github.com/qiuhuaqi/midir\nJunyu Chen et al. / Medical Image Analysis (2022) 13\nTo evaluate the proposed Swin-Transformer-based network\narchitecture, we compared its performance to existing\nTransformer-based networks that achieved state-of-the-art per-\nformance in other applications (e.g., image segmentation, ob-\nject detection, etc.). We customized these models to make them\nsuitable for image registration. They were modiﬁed to produce\n3-dimensional deformation ﬁelds that warp the given moving\nimage. Note that the only change between the methods be-\nlow and VoxelMorph is the network architecture, with the spa-\ntial transformation function, loss function, and network train-\ning procedures remaining the same. The ﬁrst three models used\nthe hybrid Transformer-ConvNet architecture (i.e.,ViT-V-Net,\nPVT, and CoTr), while the last model used a pure Transformer-\nbased architecture (i.e., nnFormer). Their network hyperpa-\nrameter settings were as follows:\n• ViT-V-Net12 (Chen et al. 2021a): This registration net-\nwork was developed based on ViT (Dosovitskiy et al.\n2020). We applied the default network hyperparameter\nsettings suggested in (Chen et al. 2021a).\n• PVT13 (Wang et al. 2021c): The default settings were\napplied, except that the embedding dimensions were to\nbe {20,40,200,320}, the number of heads was set to\n{2,4,8,16}, and the depth was increased to {3,10,60,3}\nto achieve a comparable number of parameters to that of\nTransMorph.\n• CoTr14 (Xie et al. 2021): We used the default network set-\ntings for all registration tasks.\n• nnFormer15 (Zhou et al. 2021): Because nnFormer was\nalso developed on the basis of Swin Transformer, we ap-\nplied the same Transformer hyperparameter values as in\nTransMorph to make a fair comparison.\n4.3. Implementation Details\nThe proposed TransMorph was implemented using Py-\nTorch (Paszke et al. 2019) on a PC with an NVIDIA TITAN\nRTX GPU and an NVIDIA RTX3090 GPU. All models were\ntrained for 500 epochs using the Adam optimization algorithm,\nwith a learning rate of 1 ×10−4 and a batch size of 1. The\nbrain MR dataset was augmented with ﬂipping in random di-\nrections during training, while no data augmentation was ap-\nplied to the CT dataset. Restricted by the sizes of the im-\nage volumes, the window sizes (i.e., {Mx,My,Mz}) used in\nSwin Transformer were set to {5,6,7}for MR brain registra-\ntion, {5,5,5}for XCAT-to-CT registration, and {}respectively.\nThe Transformer hyperparameter settings for TransMorph are\nlisted in the ﬁrst row of Table. 2. Note that the variants of\nTransMorph (i.e., TransMorph-Bayes, TransMorph-bspl,\nand TransMorph-diff) share the same Transformer settings as\nTransMorph. The hyperparameter settings for each proposed\nvariant are described as follows:\n12https://bit.ly/3bWDynR\n13https://github.com/whai362/PVT\n14https://github.com/YtongXie/CoTr\n15https://github.com/282857341/nnFormer\n• TransMorph: The identical loss function parameters as\nVoxelMorph were used for all tasks.\n• TransMorph-Bayes: The identical loss function parame-\nters as VoxelMorph were applied here for all tasks. The\ndropout probability was set to 0.15.\n• TransMorph-bspl: The loss function settings for all\ntasks were the same ones as those used in VoxelMorph.\nThe control point spacing, δ, for B-spline transformation\nwas also set to 2, the same value used in MIDIR.\n• TransMorph-diff: We applied the same loss function\nparameters as those used in VoxelMorph-diff.\nThe aﬃne model presented in this work comprises of a com-\npact Swin Transformer. The Transformer parameter settings\nwere identical to TransMorph except that the embedding di-\nmension was set to be 12, the numbers of Swin Transfomer\nblock were set to be {1,1,2,2}, and the head numbers were set\nto be {1,1,2,2}. The resulting a ﬃne model has a total number\nof 19.55 millions of parameters and a computational complexity\nof 0.4 GMacs. Because the MRI datasets were a ﬃnely aligned\nas part of the preprocessing, the a ﬃne model was only used in\nthe XCAT-to-CT registration.\n4.4. Additional Studies\nIn this section, we present experiments designed to verify\nthe eﬀect of the various Transformer modules in TransMorph\narchitecture. Speciﬁcally, we carried out two additional stud-\nies of network components and model complexity. They are\nperformed using the validation datasets from the three registra-\ntion tasks, and the system-level comparisons are reported on\ntest datasets. The following subsections provide detailed de-\nscriptions of these studies.\n4.4.1. Ablation study on network components\nWe begin by examining the eﬀects of several network compo-\nnents on registration performance. Table 1 lists three variants of\nTransMorph that either keep or remove the network’s long skip\nconnections or the positional embeddings in the Transformer\nencoder. In “w /o conv. skip.”, the long skip connections from\nthe two convolutional layers were removed (including two con-\nvolutional layers), which are the green arrows in Fig. 1. In\n“w/o trans. skip.”, the long skip connections coming from the\nSwin Transformer blocks were removed, which are the orange\narrows in Fig. 1. We claimed in section 3.2 that the positional\nembedding (i.e., Epos in Eqn. 8) was not a necessary element\nof TransMorph, because the positional information of tokens\ncan be learned implicitly in the network via the consecutive up-\nsampling in the decoder and backpropagating the loss between\noutput and target. Here, we conducted experiments to study\nthe eﬀectiveness of positional embeddings. Table 1 also lists\nﬁve variants of TransMorph that either keep or remove the po-\nsitional embeddings in the Transformer encoder. In the third\nvariation, ”w/o positional embedding”, we did not employ any\ntype of positional embedding. In the fourth variant, “w / shuf-\nﬂing”, we did not employ any positional embedding but instead\nrandomly shuﬄed the positions of the tokens (i.e., the dimen-\nsion N of z in Eqn. 8 and 9) just before the self-attention cal-\nculation. Following the self-attention calculation, the positions\n14 Junyu Chen et al. / Medical Image Analysis (2022)\nFig. 7: The number of parameters in each deep-learning-based model. The\nvalues are in units of millions of parameters.\nare permuted back into their original order. This way, the self-\nattention modules in the Transformer encoder are truly invariant\nto the order of the tokens. In the ﬁfth variant, “w/ rel. positional\nbias”, we used the relative positional bias in the self-attention\ncomputation (i.e. B in Eqn. 10) as used in the Swin Trans-\nformer (Liu et al. 2021a). In the second to last variant, “w / lrn.\npositional embedding”, we added the same learnable positional\nembedding to the patch embeddings at the start of the Trans-\nformer encoder as used in the ViT (Dosovitskiy et al. 2020)\nwhile keeping the relative positional bias. In the last variant,\n“w/ sin. positional embedding”, we substituted the learnable\npositional embedding with a sinusoidal positional embedding,\nthe same embedding used in the original Transformer (Vaswani\net al. 2017), which hardcodes the positional information in the\ntokens.\n4.4.2. Model complexity study\nThe impact of model complexity on registration performance\nwas also investigated in this paper. Table 2 listed the parameter\nsettings and the number of trainable parameters of four vari-\nants of the proposed TransMorph model. In the base model,\nTransMorph, the embedding dimension C was set to 96, and\nthe number of Swin Transformer blocks in the four stages of\nthe encoder was set to 2, 2, 4, and 2, respectively. Addi-\ntionally, we introduced TransMorph-tiny, TransMorph-small,\nand TransMorph-large, which are about 1 /200×, 1 /4×, and\n2×the model size of TransMorph. Finally, we compared our\nmodel to a customized VoxelMorph (denoted VoxelMorph-\nhuge), which has a comparable parameter size to that of\nTransMorph w/ lrn. positional embedding. Speciﬁcally, we\nmaintained the same number of layers in VoxelMorph-huge as\nin VoxelMorph, but increased the number of convolution ker-\nnels in each layer. As a result, VoxelMorph-huge has 63.25\nmillion trainable parameters.\n4.5. Evaluation Metrics\nThe registration performance of each model was evaluated\nbased on the volume overlap between anatomical /organ seg-\nmentation, which was quantiﬁed using the Dice score (Dice\n1945). We averaged the Dice scores of all anatomical /organ\nstructures for all patients. The mean and standard deviation of\nthe averaged scores were compared across various registration\nmethods.\nTo quantify the regularity of the deformation ﬁelds, we also\nreported the percentages of non-positive values in the deter-\nminant of the Jacobian matrix on the deformation ﬁelds (i.e.,\n|Jφ|≤ 0).\nAdditionally, for XCAT-to-CT registration, we used the\nstructural similarity index (SSIM) (Wang et al. 2004) to quan-\ntify the structural di ﬀerence between the deformed XCAT and\nthe target CT images. The mean and standard deviation of the\nSSIM values of all patients were reported and compared.\n5. Results\n5.1. Inter-patient Brain MRI Registration\nThe top-left panel of Fig. 8 shows the qualitative re-\nsults of a sample slice for inter-patient brain MRI registra-\ntion. The scores in blue, orange, green, and pink correspond\nto ventricles, third ventricle, thalami, and hippocampi, respec-\ntively. Additional qualitative comparisons across all methods\nare shown in Fig. C.20 in Appendix C. Among the proposed\nmodels, di ﬀeomorphic variants (i.e., TransMorph-diff and\nTransMorph-bspl) generated smoother displacement ﬁelds,\nwith TransMorph-bspl producing the smoothest deformations\ninside the brain area. On the other hand, TransMorph and\nTransMorph-Bayes showed better qualitative results (high-\nlighted by the yellow arrows) with higher Dice scores for the\ndelineated structures.\nThe quantitative evaluations are shown in Table 3. The re-\nsults presented in the table show that the proposed method,\nTransMorph, achieved the highest mean Dice score of 0.745.\nAlthough the di ﬀeomorphic variants produced slightly lower\nDice scores than TransMorph, they still outperformed the ex-\nisting registration methods and generated almost no foldings\n(i.e., ∼ 0% of |Jφ| ≤ 0) in the deformation ﬁelds. By\ncomparison, TransMorph improved Dice score by >0.2 when\ncompared to VoxelMorph and CycleMorph. We found that\nthe Transformer-based models (i.e., TransMorph, ViT-V-Net,\nPVT, CoTr, and nnFormer) generally produced better Dice\nscores than the ConvNet-based models. Note that even though\nViT-V-Net had almost twice the number of the trainable pa-\nrameters (as shown in Fig. 7), TransMorph still outperformed\nall the Transformer-based models (including ViT-V-Net) by at\nleast 0.1 in the Dice score, demonstrating Swin-Transformer’s\nsuperiority over other Transformer architectures. When we\nconducted hypothesis testing on the results using the paired\nt-test with Bonferroni correction Armstrong 2014 (i.e., divid-\ning the p-values by 13, the total number of the paired t-\ntests performed), the p-values between the best performing\nTransMorph variant (i.e., TransMorph) and all other methods\nwere p ≪0.0005.\nFigs. C.21 and C.22 show additional Dice results for\na variety of anatomical structures, with Fig. C.21 com-\nparing TransMorph to current registration techniques (both\noptimization- and learning-based methods), and Fig. C.22 com-\nparing the Dice scores between the Transformer-based models.\nJunyu Chen et al. / Medical Image Analysis (2022) 15\nTable 3: Quantitative evaluation results of the inter-patient (i.e., the JHU dataset)\nand the atlas-to-patient (i.e., the IXI dataset) brain MRI registration. Dice score\nand percentage of voxels with a non-positive Jacobian determinant (i.e., folded\nvoxels) are evaluated for diﬀerent methods. The bolded numbers denote the high-\nest scores, while the italicized ones indicate the second highest.\nInter-patient MRI Atlas-to-patient MRI\nModel DSC % of |Jφ|≤ 0 DSC % of |Jφ|≤ 0\nAﬃne 0.572±0.166 - 0.386±0.195 -\nSyN 0.729±0.127 <0.0001 0.645±0.152 <0.0001\nNiftyReg 0.723±0.131 0.061±0.093 0.645±0.167 0.020±0.046\nLDDMM 0.716±0.131 <0.0001 0.680±0.135 <0.0001\ndeedsBCV 0.719±0.130 0.253±0.110 0.733±0.126 0.147±0.050\nVoxelMorph-1 0.718±0.134 0.426±0.231 0.729±0.129 1.590±0.339\nVoxelMorph-2 0.723±0.132 0.389±0.222 0.732±0.123 1.522±0.336\nVoxelMorph-diff 0.715±0.137 <0.0001 0.580±0.165 <0.0001\nCycleMorph 0.719±0.134 0.231±0.168 0.737±0.123 1.719±0.382\nMIDIR 0.710±0.132 <0.0001 0.742±0.128 <0.0001\nViT-V-Net 0.729±0.128 0.402±0.249 0.734±0.124 1.609±0.319\nPVT 0.729±0.130 0.427±0.254 0.727±0.128 1.858±0.314\nCoTr 0.725±0.131 0.415±0.258 0.735±0.135 1.292±0.342\nnnFormer 0.729±0.128 0.399±0.234 0.747±0.135 1.595±0.358\nTransMorph-Bayes 0.744±0.125 0.389±0.241 0.753±0.123 1.560±0.333\nTransMorph-diff 0.730±0.129 <0.0001 0.594±0.163 <0.0001\nTransMorph-bspl 0.740±0.123 <0.0001 0.761±0.122 <0.0001\nTransMorph 0.745±0.125 0.396±0.240 0.754±0.124 1.579±0.328\nTable 4: Quantitative evaluation results of XCAT-to-CT registration. Dice score\nof 16 organs, percentage of voxels with a non-positive Jacobian determinant (i.e.,\nfolded voxels), and SSIM are evaluated for di ﬀerent methods. The bolded num-\nbers denote the highest scores, while the italicized ones indicate the second high-\nest.\nModel DSC % of |Jφ|≤ 0 SSIM\nw/o registration 0.220±0.242 - 0.576±0.071\nAﬃne Transformer 0.330±0.291 - 0.751±0.018\nSyN 0.498±0.342 0.001±0.002 0.894±0.021\nNiftyReg 0.488±0.333 0.025±0.046 0.886±0.027\nLDDMM 0.519±0.265 0.006±0.007 0.874±0.031\ndeedsBCV 0.568±0.306 0.126±0.123 0.863±0.029\nVoxelMorph-1 0.532±0.313 2.275±1.283 0.899±0.027\nVoxelMorph-2 0.548±0.317 1.696±0.909 0.910±0.027\nVoxelMorph-diff 0.526±0.330 <0.0001 0.911±0.020\nCycleMorph 0.528±0.321 3.263±1.188 0.909±0.024\nMIDIR 0.551±0.303 <0.0001 0.896±0.022\nViT-V-Net 0.582±0.311 2.109±1.032 0.915±0.020\nPVT 0.516±0.321 2.939±1.162 0.900±0.027\nCoTr 0.550±0.313 1.530±1.052 0.905±0.029\nnnFormer 0.536±0.315 1.371±0.620 0.902±0.024\nTransMorph-Bayes 0.594±0.313 1.475±0.857 0.919±0.024\nTransMorph-diff 0.541±0.324 <0.0001 0.910±0.025\nTransMorph-bspl 0.575±0.311 <0.0001 0.908±0.025\nTransMorph 0.604±0.314 1.679±0.772 0.918±0.023\nFig. 8: Qualitative results of TransMorph (2nd column) and its Bayesian- (3rd column), probabilistic- (4th column), and B-spline (5th column) variants. Top-left &\nTop-right panels: Results of inter-patient and atlas-to-patient brain MRI registration. The blue, orange, green, and pink contours deﬁne, respectively, the ventricles,\nthird ventricle, thalami, and hippocampi. Bottom panel: Results of XCAT-to-CT registration. The blue, orange, green, and pink contours deﬁne, respectively, the\nliver, heart, left lung, and right lung. The second row in both panels exhibits the displacement ﬁelds u, where spatial dimension x, y, and z is mapped to each of the\nRGB color channels, respectively. The [p, q] in color bars denotes the magnitude range of the ﬁelds.\n16 Junyu Chen et al. / Medical Image Analysis (2022)\nFig. 9: Quantitative evaluation results of the additional studies performed on the validation datasets of the two brain MRI and XCAT-to-CT registration tasks.\nTable 5: Quantitative evaluation results for brain MRI registration of the OASIS\ndataset from the 2021 Learn2Reg challenge task 3. Dice score of 35 cortical and\nsubcortical brain structures, the 95th percentile percentage of the Hausdorﬀ dis-\ntance, and the standard deviation of the logarithm of the Jacobian determinant\n(SDlogJ) of the displacement ﬁeld are evaluated for diﬀerent methods. The val-\nidation results came from the challenge’s leaderboard, whereas the test results\ncame directly from the challenge’s organizers. Thebolded numbers denote the\nhighest scores, while the italicized ones indicate the second highest.\nValidation\nModel DSC HdDist95 SDlogJ\nLv et al. 2022 0.827±0.013 1.722±0.318 0.121±0.015\nSiebert et al. 2021 0.846±0.016 1.500±0.304 0.067±0.005\nMok and Chung 2021 0.861±0.015 1.514±0.337 0.072±0.007\nVoxelMorph-huge 0.847±0.014 1.546±0.306 0.133±0.021\nTransMorph 0.858±0.014 1.494±0.288 0.118±0.019\nTransMorph-Large 0.862±0.014 1.431±0.282 0.128±0.021\nTest\nModel DSC HdDist95 SDlogJ\nInitial 0.56 3.86 -\nLv et al. 2022 0.80 1.77 0.08\nSiebert et al. 2021 0.81 1.63 0.07\nMok and Chung 2021 0.82 1.67 0.07\nTransMorph 0.816 1.692 0.124\nTransMorph-Large 0.820 1.656 0.124\n5.2. Atlas-to-patient Brain MRI Registration\nThe top-right panel of Fig. 8 shows the qualitative results of\nthe TransMorph variants on a sample MRI slice for atlas-to-\npatient brain MRI registration. As highlighted by the yellow\narrows, the diﬀeomorphic variants resulted in the deformed im-\nages that were less comparable to the ﬁxed image in terms of vi-\nsual appearance. In contrast, the variants without diﬀeomorphic\ndeformations (i.e., TransMorph and TransMorph-Bayes) pro-\nduced better qualitative results, with the sulci in the deformed\natlas images more closely matching those in the ﬁxed image.\nAdditional qualitative comparisons are shown in Fig. D.23 in\nAppendix D, where we observed that all the learning-based\nmethods yielded more detailed and precise deformation ﬁelds\nthan the conventional methods. This might be owing to the high\nparameterization of the DNNs, which enables the modeling of\nmore complicated deformations.\nTable. 3 shows the quantitative evaluation results of the atlas-\nto-patient registration. The highest mean Dice score of 0.761\nwas achieved by the proposed TransMorph-bspl with nearly\nno folded voxels. The second best Dice score of 0.754 was\nachieved by both TransMorph and TransMorph-Bayes, while\nTransMorph-Bayes yielded a smaller standard deviation. In\ncomparison to these TransMorph variants, TransMorph-diff\nproduced a lower Dice score of 0.594. However, note\nthat this score is still higher ( ∼0.02) than the one pro-\nduced by VoxelMorph-diff, which is the base model of\nTransMorph-diff. Additionally, we observed that the reg-\nistration methods that used MSE for training or optimization\nresulted in lower Dice scores (i.e., SyN, NiftyReg, LDDMM,\nVoxelMorph-diff, and TransMorph-diff). This was most\nlikely due to the signiﬁcant disparity in the intensity values of\nbrain sulci between the atlas and the patient MRI images. As\nseen in the top-right panel of Fig 8, the sulci in the atlas image\n(i.e., the moving image) exhibited low-intensity values compa-\nrable to the background, but the sulci in the patient MRI im-\nage had intensity values more comparable to the neighboring\ngyri. Thus, the discrepancies in the sulci intensity values may\naccount for the majority of the MSE loss during training, com-\npelling the registration models to ﬁll the sulci in the atlas image\nwith other brain structures (as shown in Fig. D.23, these models\nproduced signiﬁcantly smaller sulci than models trained with\nLNCC), thereby limiting registration performance. The paired\nt-tests with Bonferroni correction (Armstrong 2014) revealed\nthe p-values of p ≪0.0005 between the best performing model\n(i.e., TransMorph-bspl) and all other methods. This indicates\nthat the proposed method outperformed the comparative regis-\nJunyu Chen et al. / Medical Image Analysis (2022) 17\ntration methods and network architectures.\nA detailed breakdown of Dice scores for a variety of anatom-\nical structures is shown in Figs. D.24 and D.25 in Appendix D.\n5.3. Learn2Reg OASIS Brain MRI Registration\nTable 5 shows the quantitative results of the validation\nand test sets of the challenge. The validation scores of\nthe various methods were obtained from the leaderboard of\nthe challenge, whilst the test scores were obtained directly\nfrom the organizers. TransMorph performed similarly to the\nbest-performing method ( LapIRN (Mok and Chung 2021)) of\nthe challenge on the validation set, where TransMorph-large\nachieved the best mean Dice score of 0.862 and mean Hd-\nDist95 of 1.431. VoxelMorph-huge performed signiﬁcantly\npoor than TransMorph, with a p-value less than 0.01 from\npaired t-test. This reveals the superiority of Transformer-\nbased architecture over ConvNet despite having a comparable\nnumber of parameters. On the test set, the TransMorph and\nTransMorph-large achieved comparable mean Dice score to\nthat of LapIRN. Despite the comparable performance, LapIRN\nproduced much more uniform deformation ﬁelds as measured\nby SDlogJ. In a separate study, we presented a simple exten-\nsion of TransMorph that signiﬁcantly outperformed LapIRN\nwhile maintaining smooth deformation ﬁelds. We direct inter-\nested readers to (Chen et al. 2022) for further details. More-\nover, LapIRN employed a multiresolution framework in which\nthree ConvNet registration backbones were involved in gener-\nating deformation ﬁelds at three di ﬀerent scales. TransMorph,\nhowever, operated on a single resolution. We underline that\nTransMorph is a registration backbone, and that it may be eas-\nily adapted toLapIRN or any advanced registration frameworks.\n5.4. XCAT-to-CT Registration\nThe bottom panel of Fig. 8 shows the qualitative results for a\nrepresentative CT slice. The blue, orange, green, and pink lines\ndenote the liver, heart, left lung, and right lung, respectively,\nwhile the bottom values show the corresponding Dice scores.\nSimilar to the ﬁndings in the previous sections, TransMorph\nand TransMorph-Bayes gave more accurate registration re-\nsults (highlighted by the yellow arrows and the delineated struc-\ntures), while the diﬀeomorphic variants produced smoother de-\nformations. Additional qualitative comparisons are shown in\nFig. E.26 in Appendix E. It is possible to see certain artifacts\nin the displacement ﬁeld created by nnFormer (as shown in\nFig. E.26); these were most likely caused by the patch oper-\nations of the Transformers used in its architecture. nnFormer\nis a near-convolution-free model (convolutional layers are em-\nployed only to form displacement ﬁelds). In contrast to the\nrelatively small displacements in brain MRI registration, dis-\nplacements in XCAT-to-CT registration may exceed the patch\nsize. Consequently, the lack of convolutional layers to reﬁne\nthe stitched displacement ﬁeld patches may have resulted in\nartifacts. Four example coronal slices of the deformed XCAT\nphantoms generated by various registration methods are shown\nin Fig. E.27 in Appendix E.\nThe quantitative evaluation results are presented in Table 4.\nThey include Dice scores for all organs and scans, the percent-\nage of non-positive Jacobian determinants, and the structural\nsimilarity index (SSIM) (Wang et al. 2004) between the de-\nformed XCAT phantom and the target CT scan. The window\nsize used in SSIM was set to 7. Without registration or a ﬃne\ntransformation, a Dice score of 0.22 and an SSIM of 0.576\ndemonstrate the large dissimilarity between the original XCAT\nphantom and patient CT scans. The Dice score and SSIM in-\ncreased to 0.33 and 0.751, respectively, after aligning the XCAT\nand patient CT using the proposed a ﬃne Transformer. Among\nthe traditional registration methods, deedsBCV, which was ini-\ntially designed for abdominal CT registration-based segmenta-\ntion (Heinrich et al. 2015), achieved the highest Dice score of\n0.568, which is even higher than most of the learning-based\nmethods. Among the learning-based methods, Transformer-\nbased models outperformed ConvNet-based models on aver-\nage, which is consistent with the ﬁndings from the brain MR\nregistration tasks. The p-values from the paired t-tests with\nBonferroni correction (Armstrong 2014) between TransMorph\nand all non- TransMorph methods were p ≪0.05. The pro-\nposed TransMorph models yielded the highest Dice and SSIM\nscores of all methods in general, with the best Dice of 0.604\ngiven by TransMorph and the best SSIM of 0.919 given by\nTransMorph-Bayes. The di ﬀeomorphic variants produced\nlower Dice and SSIM scores as a consequence of not having\nany folded voxels in the deformation.\nFigs. E.28 and E.29 show additional boxplots of Dice\nscores on the various abdominal organs, with Fig. E.28 com-\nparing TransMorph to current registration techniques (both\noptimization- and learning-based methods), and Fig. E.29 com-\nparing the Dice scores between the Transformer-based models.\n5.5. Ablation Studies\nInter-patient Registration. The ﬁrst ﬁgure in the ﬁrst row of\nFig. 9 shows the violin plots of Dice scores from the ablation\nstudy on the validation dataset of inter-patient brain MR regis-\ntration. When evaluating the e ﬀectiveness of skip connections,\nwe observed that the skip connections from both the convolu-\ntion and Transformer layers improved registration performance.\nTransMorph scored a mean Dice of 0.753 after the skip con-\nnections from the convolutional layers were removed. How-\never, the score decreased to 0.740 when the skip connections\nfrom the Transformer blocks were removed. In comparison,\nthe skip connections from convolutional layers were less eﬀec-\ntive, with a mean Dice improvement of 0.003. Note that the\nTransMorph with shu ﬄing, and with and without positional\nembeddings all generated comparable mean Dice scores and\nviolin plots, suggesting that positional embedding may not be\nnecessary.\nAtlas-to-patient Registration. The violin plots from the abla-\ntion study on the atlas-to-patient registration task are shown in\nthe second ﬁgure in the ﬁrst row of Fig. 9. Comparable vio-\nlin plots with similar mean Dice scores around 0.752 were ob-\nserved with and without the skip connections from the convolu-\ntional layers. When the skip connections from the Transformer\n18 Junyu Chen et al. / Medical Image Analysis (2022)\nTable 6: System-level comparison of various TransMorph designs and the customized VoxelMorph on the validation and test datasets of inter-patient MRI, atlas-\nto-patient MRI, and XCAT-to-CT registration tasks. ”Val. DSC” denotes the Dice scores on the validation dataset; “Test DSC” denotes the system-level comparison\nof the Dice scores on the test dataset. The bolded numbers denote the highest scores, while the italicized ones indicate the second highest.\nInter-patient MRI Atlas-to-patient MRI XCAT-to-CT\nModel Val. DSC Test DSC Val. DSC Test DSC Val. DSC Test DSC\nw/o conv. skip. 0.753±0.119 0.743±0.124 0.752±0.129 0.754±0.125 0.591±0.319 0.586±0.314\nw/o Trans. skip. 0.740±0.124 0.727±0.130 0.734±0.127 0.736±0.125 0.578±0.315 0.588±0.314\nw/ shuﬄing 0.755±0.119 0.744±0.125 0.751±0.127 0.754±0.123 0.588±0.314 0.597±0.310\nw/ rel. positional bias 0.755±0.120 0.742±0.125 0.751±0.131 0.753±0.127 0.593±0.315 0.592±0.319\nw/ lrn. positional embedding 0.755±0.120 0.744±0.125 0.749±0.131 0.751±0.129 0.594±0.315 0.586±0.315\nw/ sin. positional embedding 0.755±0.120 0.744±0.125 0.752±0.126 0.754±0.123 0.583±0.320 0.572±0.317\nTransMorph 0.756±0.119 0.745±0.125 0.753±0.127 0.754±0.124 0.600±0.317 0.604±0.314\nTransMorph-tiny 0.710±0.132 0.696±0.140 0.545±0.180 0.543±0.180 0.502±0.311 0.501±0.312\nTransMorph-small 0.751±0.121 0.740±0.126 0.746±0.128 0.747±0.125 0.572±0.320 0.570±0.318\nTransMorph-large 0.757±0.119 0.746±0.124 0.753±0.130 0.754±0.128 0.608±0.305 0.611±0.311\nVoxelMorph-huge 0.755±0.119 0.744±0.124 0.750±0.133 0.751±0.130 0.543±0.320 0.550±0.319\nFig. 10: Model computational complexity comparisons represented in Giga multiply–accumulate operations (GMACs). Greater values imply a greater degree of\ncomputational complexity. These values were obtained using an input image of size 160 ×192 ×224.\nFig. 11: Examples of feature maps in TransMorph’s skip connections. Eight feature maps are randomly selected from the feature maps associated with each skip\nconnection. Left panel: Example 2D slices of source and target images (i.e.,Im and If ), which are used as inputs toTransMorph. Middle panel: Feature maps in the\nskip connections of the two convolutional layers (denoted by the green arrows in Fig. 1). Right panel: Feature maps in the skip connections of the Swin Transformer\nblocks (denoted by the orange arrows in Fig. 1).\nblocks were removed, the Dice score decreased by 0.019, re-\nﬂecting the e ﬀectiveness of these skip connections. Compara-\nble violin plots and mean Dice scores around 0.750 were ob-\nserved with shuﬄing, and with and without various positional\nembeddings, conﬁrming that TransMorph’s performance is un-\naﬀected by whether or not positional embedding was used.\nJunyu Chen et al. / Medical Image Analysis (2022) 19\nXCAT-to-CT Registration. The second to last ﬁgure in the ﬁrst\nrow of Fig. 9 shows the violin plots from the validation dataset\nof XCAT-to-CT registration task. Without the skip connections\nfrom the convolution and Transformer layers, the Dice scores\ndropped by 0.013 and 0.016, respectively, when compared to\nTransMorph, further supporting the observation that skip con-\nnections can improve performance. Learnable and relative po-\nsitional embeddings yielded comparable mean Dice scores for\nXCAT-to-CT registration in the range of 0.593. When sinu-\nsoidal positional embedding was employed, a score of 0.583\nwas attained, whereas a score of 0.588 was produced when the\npositions were shu ﬄed. With a score of 0.600, without us-\ning positional embeddings yielded a slight improvement among\nother variants. The e ﬀect of each component is addressed in\ndepth in the Discussion section (section 6.1).\nIn conclusion, the results from all three tasks indicate that us-\ning skip connections improves performance. The results of the\nthree tasks (i.e., inter-patient, atlas-to-patient, and XCAT-to-CT\nregistration tasks) reveal that with and without using positional\nembedding or even randomly shuﬄing the token positions pro-\nduced similar results. Additionally, we applied the TranMorph\nmodels to the test datasets of the three registration tasks for\nsystem-level comparisons, and the results are shown in the up-\nper panel of Table. 6. The scores on the test datasets followed\nthe same trend as those on the validation datasets, where the\npositional embeddings had an insigniﬁcant inﬂuence on regis-\ntration performance.\n5.6. Computational Complexity\nThe barplot in the left panel of Fig. 10 shows the com-\nputational complexity comparisons between the deep-learning-\nbased registration models. The plot was created using an in-\nput image with a resolution of 160 ×192 ×224, the same\nsize as the brain MRI images. The numbers were expressed in\nGiga multiply-accumulate operations (GMACs), with a higher\nvalue indicating a more computationally expensive model that\nmay also be more memory intensive. The proposed model,\nTransMorph, and its Bayesian variant, TransMorph-Bayes,\nhad a moderate computational complexity with 687 GMACs\nwhich is much less than CoTr and CycleMorph. In practice,\nthe GPU memory occupied during training for TransMorph\nwas about 15 GiB with a batch size of 1 and an input im-\nage size of 160 ×192 ×224. The di ﬀeomorphic variants,\nTransMorph-diff and TransMorph-bspl, had 281 and 454\nGMACs, which are comparable to that of the conventional\nConvNet-based registration models, VoxelMorph-1 and -2. In\npractice, they occupied approximately 11 GiB of GPU memory\nduring training, which is a size that can be readily accommo-\ndated by the majority of modern GPUs. In terms of the number\nof parameters, all ConvNet-based models had fewer than 1M\nnetwork parameters (as shown in Fig. 7); yet their GMACs (i.e.,\ncomputational complexity) were comparable to TransMorph,\nbut their registration performances were signiﬁcantly inferior.\nTransformer-based models were all of large scale, with more\nthan 30M parameters. Notably, ViT-V-Net and PVT had\naround 2×and 1.5×more parameters than TransMorph, never-\ntheless TransMorph outperformed them by a signiﬁcant margin\non all of the evaluated registration tasks. This demonstrates that\nthe success of TransMorph owes not just to the large model\nsize but also to the architecture itself.\nFig. 9 shows the quantitative results of TransMorph models\nwith various architectural settings and the customized ConvNet-\nbased model VoxelMorph-huge on the validation datasets of\nthe three registration datasets. When parameter size is the\nonly variable in TransMorph models, there is a strong corre-\nlation between model complexity (as shown in the right panel\nof Fig. 10) and registration performance. TransMorph-tiny\nproduced the lowest mean Dice of 0.710, 0.545, and 0.502\non the validation set of the three registration tasks, respec-\ntively. The Dice score steadily improves as the complexity\nof the model increases. Note that for inter-patient and atlas-\nto-patient brain MRI registration (the ﬁrst and second ﬁgures\nin the bottom row of Fig. 9), the improvement in mean Dice\nscore from TransMorph to TransMorph-large were mostly un-\nder 0.01 but the latter was almost twice as computationally\ncostly (as shown in the right panel of Fig. 10). The customized\nConvNet-based model, VoxelMorph-huge, had the comparable\nnumber of parameters as TransMorph. However, it achieved\nslightly lower mean Dice scores than those of TransMorph\nfor the JHU and IXI brain MR registration tasks, and signif-\nicantly lower scores for OASIS brain MR and the XCAT-to-\nCT registration task. This further indicates the architectural\nadvantages of TransMorph for image registration. A signiﬁ-\ncant disadvantage of VoxelMorph-huge was its computational\ncomplexity, with 3656 GMACs (as seen in the right panel of\nFig. 10), it was nearly ﬁve times as computationally expen-\nsive as TransMorph, making it memory-intensive ( ∼22 GiB\nfor a patch size of 1 during training) and slow to train in prac-\ntice. However, TransMorph was able to accommodate a larger\nnumber of parameters without signiﬁcantly increasing compu-\ntational complexity. The promising performances brought by\nthe larger scale of parameters demonstrate the superior scal-\ning property of Transformer-based models as described in (Zhai\net al. 2022; Liu et al. 2022). The TranMorph models with dif-\nferent model parameter settings and VoxelMorph-huge were\napplied to the test datasets for system-level comparisons, and\nthe results are shown in the bottom panel of Table. 6.\n6. Discussion\n6.1. Network Components in TransMorph\n6.1.1. Skip Connections\nAs previously shown in section 5.5, skip connections may\naid in enhancing registration accuracy. In this section, we give\nfurther insight into the skip connections’ functionality.\nFig. 11 shows some example feature maps in each skip con-\nnection (a full feature map visualization is shown in Fig. G.31\nin Appendix). Speciﬁcally, the left panel shows sample slices of\nthe input volumes; the center panel illustrates selected feature\nmaps in the skip connections of the convolutional layers, and\nthe right panel illustrates selected feature maps in the skip con-\nnections of the Swin Transformer blocks. As seen from these\nfeature maps that the Swin Transformer blocks provided more\nabstract information (right panel in Fig. 11), in comparison to\n20 Junyu Chen et al. / Medical Image Analysis (2022)\nFig. 12: Qualitative impact of skip connections on the deformation ﬁelds. The\nspatial dimension x, y, and z in the displacement ﬁeld is mapped to each of\nthe RGB color channels, respectively. The [ p, q] in color bars denotes the\nmagnitude range of the ﬁelds.\nthe convolutional layers (middle panel in Fig. 11). Since a\nTransformer divides an input image volume into patches to cre-\nate tokens for self-attention operations (as described in section\n3.2), it can only deliver information up to a certain resolution,\nwhich is often a factor of the patch size lower than the original\nresolution (i.e., H\nP ×W\nP ×L\nP , and P = 4 in our case). On the\nother hand, the convolutional layers resulted in higher resolu-\ntion feature maps with more detailed and human-readable in-\nformation (e.g., edge and boundary information). Certain fea-\nture maps even revealed distinctions between the moving and\nﬁxed images (highlighted by the red boxes). Fig. 12 shows the\nqualitative comparisons between the proposed model with and\nwithout a speciﬁc type of skip connection. As seen by the mag-\nniﬁed areas, TransMorph with both skip connection types pro-\nvided a more detailed and accurate displacement ﬁeld. There-\nfore, adding the skip connections from the convolutional layers\nis still recommended, although the actual Dice improvement\nwere subtle on the validation datasets (0 .003 for inter-patient\nbrain MRI, 0.001 for atlas-to-patient brain MRI, and 0 .009 for\nXCAT-to-CT registration).\nFig. 13: Example slice of the positional embeddings used inTransMorph. Left\npanel: Sinusoidal positional embedding. Right panel: Learnable positional em-\nbedding. Tiles in both panels show the cosine similarities between the position\nembedding of the token with the indicated row and column and the position\nembeddings of all other tokens.\n6.1.2. Positional Embedding\nTransformers in computer vision were initially designed for\nimage classiﬁcation tasks (Dosovitskiy et al. 2020; Liu et al.\n2021a; Dong et al. 2021; Wang et al. 2021c). Such a Trans-\nformer produces a condensed probability vector that is not in\nthe image domain but instead a description of the likelihood of\nbeing a certain class. The loss calculated based on this vec-\ntor does not backpropagate any spatial information into the net-\nwork. Thus, it is critical to encode positional information on the\npatched tokens; otherwise, as the network gets deeper, Trans-\nformer would lose track of the tokens’ locations relative to the\ninput image, resulting in unstable training and inferior predic-\ntions. However, for pixel-level tasks like image registration, the\ncondensed features generated by Transformers are often sub-\nsequently expanded using a decoder whose output is an image\nwith the same resolution as the input and target images. Any\nspatial mismatching between the output and target contributes\nto the loss, which is then backpropagated throughout the net-\nwork. As a result, the Transformer implicitly learns the posi-\ntional information of tokens, thus obviating the need for posi-\ntional embedding. In this work, we compared the registration\nperformance of TransMorph and TransMorph with positional\nembedding on brain MRI and XCAT-to-CT registration. The re-\nsults shown in section 5.5 indicated that positional embedding\ndid not improve registration performance; rather, it introduced\nmore parameters into the network. In this section, we discuss\nthe positional embeddings in further detail.\nThree positional embeddings were studied in this paper: si-\nnusoidal (Vaswani et al. 2017), learnable (Dosovitskiy et al.\n2020), and relative (Liu et al. 2021a) embeddings, which are\nalso the major types of positional embedding. In sinusoidal po-\nsitional embedding, the position of each patched token is rep-\nresented by a value drawn from a predetermined sinusoidal sig-\nnal according to the token’s position relative to the input im-\nage. Whereas with learnable positional embedding, the network\nlearns the representation of the token’s location from the train-\ning dataset rather than giving a hardcoded value. The relative\npositional bias hardcodes the relative position relations between\nany two tokens in the dot product of the query and key represen-\ntations (i.e., B in Eqn. 10). To validate that the network learned\nthe positional information, Dosovitskiy et al. 2020 computed\nthe cosine similarities between a learned embedding of a token\nand that of all other tokens. The obtained similarity values were\nthen used to form an image. If positional information is learned,\nthe image should reﬂect increased similarities at the token’s and\nnearby tokens’ positions. Here, we computed the images of\ncosine similarities for both sinusoidal and learnable positional\nembeddings used in this work. The left and right panels in Fig.\n13 show the images of cosine similarities. These images were\ngenerated based on an input image size of 160 ×192 ×224 and\na patch size of 4 ×4 ×4 (resulting in 40 ×48 ×56 patches).\nEach image has a size of 40 ×48 representing an image of\ncosine similarities in the plane of z = 28 (i.e., the middle\nslice). There should have been a total of 40 ×48 images in\neach panel. However, for better visualization, just a few im-\nages were shown here. The images were chosen with step sizes\nof 5 and 8 in x and y direction, respectively, resulting in 6 ×5\nJunyu Chen et al. / Medical Image Analysis (2022) 21\nFig. 14: Comparisons of the appearance uncertainty estimates derived from the predictive variance and the predicted model error. Left panel: Calibration plots and\nuncertainty calibration error (UCE) for TransMorph-Bayes on two inter-patient brain MR test sets (top), two atlas-to-patient brain MR test sets (middle), and two\nXCAT-to-CT test sets (bottom). The blue lines represent the results obtained using the uncertainty estimate ˆΣ2\nf . The dashed lines represent the perfect calibration,\nwhich are the results achieved when the uncertainty estimate is Σ2\nf or err(Im ◦φ) (i.e., the expected model error). The values are obtained from 10 repeated runs,\nand the shaded regions represent the standard deviation. Right panel: Visualization of the registration uncertainty on an inter-patient brain MRI test set (i.e., a-f),\nan atlas-to-patient brain MRI test set (i.e., g-l), and a CT test set (i.e., m-r). (a), (g), & (m): Moving image. (b), (h), & (n): Fixed image. (c), (i), & (o): Deformed\nmoving image. (d), (j), & (p): Per-pixel uncertainty, represented by Σ2\nf , overlays the deformed image. (e), (k), & (q): Per-pixel uncertainty given by Σ2\nf (i.e., the\nproposed method). (f), (l), & (r): Per-pixel uncertainty given by ˆΣ2\nf . The yellow arrows highlight sites where Σ2\nf identiﬁes registration failures but ˆΣ2\nf does not.\nimages in each panel. As seen from the left panel, the images\nof sinusoidal embeddings exhibit a structured pattern, showing\na high degree of correlation between tokens’ relative locations\nand image intensity values. Note that the brightest pixel in each\nimage represents the cosine similarity between a token’s posi-\ntional embedding and itself, which reﬂects the token’s actual\nlocation relative to all other tokens. The similarity then grad-\nually decreases as it gets farther away from the token. On the\nother hand, images generated with learnable embeddings (right\npanel of Fig. 13) lack such structured patterns, implying that\nthe network did not learn the positional information associated\nwith the tokens in the learnable embeddings. To further demon-\nstrate that the network implicitly learned the positional informa-\ntion, we randomly shuﬄed the token positions when computing\nself-attention during training and testing. As a result, the self-\nattention modules could not explicitly perceive input tokens’\npositional information. However, as seen from the Dice scores\nin Fig. 9, regardless of shu ﬄing and which positional embed-\nding was employed, the mean Dice scores and violin plots were\nquite comparable to those produced without positional embed-\nding. Thus, the ﬁndings conﬁrmed that TransMorph learned\nthe positional information of the tokens implicitly and that the\nlearnable, sinusoidal, and relative positional embeddings were\nredundant in the model and had a negligible e ﬀect on registra-\ntion performance.\n22 Junyu Chen et al. / Medical Image Analysis (2022)\nFig. 15: Example ERFs of VoxelMorph and the proposed Transformer-based model TransMorph. The top row shows the ERF slices (i.e., y = 80) at each stage\nof the network on an input image size of 160 ×160 ×160. For a consistent comparison of ERFs between VoxelMorph and TransMorph, the ERFs at 1 /2 of\nVoxelMorph and 1/32 resolution of TransMorph were omitted.\n6.2. Uncertainty Quantiﬁcation of TransMorph-Bayes\nAs previously mentioned in section 3.4, the appearance un-\ncertainty estimates produced by the predictive variance (Eqn.\n23) were actually miscalibrated, meaning that the uncertainty\nvalues did not properly correlate to predicted model errors since\nvariance was computed using the predictive mean instead of tar-\nget image If . We proposed to directly use the expected model\nerror to express appearance uncertainty since the target image is\navailable at all times in image registration. Thus, the resulting\nappearance uncertainty estimate is perfectly calibrated. In this\nsection, we examine how the proposed and existing methods\ndiﬀer in their estimates of appearance uncertainty.\nTo quantify the calibration error, we used the Uncertainty\nCalibration Error (UCE) introduced in (Laves et al. 2020a),\nwhich is calculated on the basis of the binned di ﬀerence be-\ntween the expected model error (i.e., E\n[\n(Im ◦φ−If )2\n]\n) and the\nuncertainty estimation (e.g., ˆΣ2\nf in Eqn. 23 or Σ2\nf in Eqn. 25).\nWe refer the interested reader to the corresponding references\nfor further details about UCE. The plots in the left panel of Fig.\n14 exhibit the calibration plots and UCE obtained on four repre-\nsentative test sets. All results were based on a sample size of 25\n(i.e., T = 25 in Eqn. 21, 23, and 25) from 10 repeated runs. The\nblue lines show the results produced with the ˆΣ2\nf and the shaded\nregions represent the standard deviation from the 10 runs, while\nthe dashed black lines indicate the perfect calibration achieved\nwith the proposed method. Notice that the uncertainty values\nobtained using ˆΣ2\nf did not match well to the expected model er-\nror; in fact, they were consistently being underestimated (for\nreasons described in section 3.4.1). In comparison, the pro-\nposed method enabled perfect calibration with UCE = 0 since\nits uncertainty estimate equaled the expected model error. In\nthe right panel of Fig. 14, we show the visual comparisons of\nthe uncertainty derived from Σ2\nf and ˆΣ2\nf . When we compare ei-\nther (e) to (f) or (k) to (l), we see that the former (i.e., (e) and\n(k)) captured more registration failures than the latter (as high-\nlighted by the yellow arrows), indicating a stronger correlation\nbetween deformation uncertainty and registration failures. This\nis thus further evidence that the proposed method provides the\nperfect uncertainty calibration.\nDespite the promising results, there are some limitations of\nusing σf to estimate appearance uncertainty. In this work,\nwe modeled σf as E\n[\n(Im ◦φ−If )2\n]\n, which is the MSE of the\nMonte Carlo sampled registration outputs relative to the ﬁxed\nimage. MSE, on the other hand, is not necessarily the opti-\nmal metric for expressing the expected error. In multi-modal\nregistration instances like PET to CT or MRI to CT registra-\ntion, MSE is anticipated to be high, given the vast di ﬀerence\nin image appearance and voxel values across modalities. Thus,\nif MSE is employed to quantify the appearance uncertainty in\nthese instances, the uncertainty values will be dominated by the\nsquared bias (i.e., ( ˆIf −If )2 in Eqn. B.3), resulting in an inef-\nfective uncertainty estimate. In these instances, the predicted\nvariance may be a more appropriate choice for appearance un-\ncertainty quantiﬁcation.\nAdditional results for both appearance and transformation\nuncertainty estimations are shown in Fig. F.30 in Appendix.\nObservably, the two uncertainty measures provide estimates\nthat are substantially diﬀerent, with appearance uncertainty val-\nues being high in locations with substantial appearance mis-\nmatches and transformation uncertainty values being high in\nregions with large deformations and generally constant inten-\nsity values.\n6.3. Comparison of E ﬀective Receptive Fields\nWe demonstrate in this section that the eﬀective receptive\nﬁelds (ERFs) of Transformer-based models are larger than that\nof ConvNet-based models and spans the whole spatial domain\nof an image. We used the deﬁnition of ERF introduced in (Luo\net al. 2016), which quantiﬁes the amount of inﬂuence that each\ninput voxel has on the output of a neural network. In the next\nparagraph, we brieﬂy discuss the computation of ERF and rec-\nommend interested readers to the reference for further informa-\ntion.\nAssume the voxels in the input image Im and the output dis-\nplacement ﬁeld u are indexed by (i, j,k), with an image size of\n160 ×160 ×160 (i.e., the size of CT scans used in this work),\nthe center voxel is located at (80 ,80,80). ERF quantiﬁes how\nmuch each Im(i, j,k) contributes to the center voxel of the dis-\nplacement ﬁeld, i.e. u(80,80,80). This is accomplished using\nthe partial derivative ∂u(80,80,80)/∂Im(i, j,k), which indicates\nthe relative relevance of Im(i, j,k) to u(80,80,80). To obtain\nJunyu Chen et al. / Medical Image Analysis (2022) 23\nFig. 16: The loss landscapes of MIDIR, VoxelMorph-2, VoxelMorph-huge, and TransMorph, where the loss function is composed of LNCC and di ﬀusion\nregularizer. TransMorph yielded a much ﬂatter landscape that those of ConvNet-based models.\nthis partial derivative, we set the error gradient to:\n∂ℓ\n∂u(i, j,k) =\n\n1, for (i, j,k) = (80,80,80)\n0, otherwise , (26)\nwhere ℓ denotes an arbitrary loss function. Then this gradi-\nent is propagated downward from u to the input Im, where the\nresulting gradient of Im represents the desired partial derivative\n∂u(80,80,80)/∂Im(i, j,k). This partial derivative is independent\nof the input and loss function and is only a function of the net-\nwork architecture and the index ( i, j,k), which adequately de-\nscribes the distribution of the eﬀective receptive ﬁeld.\nA comparison of the ERFs ofVoxelMorph and TransMorph\nis shown in Fig. 15. Note that the other ConvNet-based models\nwere omitted because they adopted a similar network architec-\nture as VoxelMorph (e.g., CycleMorph and MIDIR). Due to\nthe locality of convolution operations, VoxelMorph’s ERF at\neach stage (top row in Fig. 15) was highly localized, partic-\nularly in the encoding stages (i.e., 1 /4, 1 /8, and 1 /16 resolu-\ntion). Even at the end of the network, the theoretical recep-\ntive ﬁeld of VoxelMorph encompassed the entire image; yet,\nits ERF emphasized only a small portion of the image. In con-\ntrast, the ERFs of the proposed TransMorph were substantially\nlarger than those of VoxelMorph at each stage, and the ERFs\nin the decoding stage covered the entire image (bottom row in\nFig. 15). The ERFs reveal that ConvNet-based architectures\ncan only perceive a portion of the input image, particularly dur-\ning the encoding stages, indicating that they cannot explicitly\ncomprehend the spatial relationships between distant voxels.\nFor tasks that require large deformations, ConvNets may fall\nshort of establishing accurate voxel correspondences between\nthe moving and ﬁxed images, which is essential for image reg-\nistration. On the other hand, TransMorph adopts substantially\nlarge kernels at the encoding stages leading to substantially\nlarge ERFs throughout the network thanks to the self-attention\nmechanism of the Transformer.\n6.4. Comparison of Displacement Magnitudes\nAs demonstrated in section 6.3, TransMorph had substan-\ntially larger eﬀective receptive ﬁelds than VoxelMorph, which\nmight be beneﬁcial for capturing semantic information that is\nnecessary for coping with large deformations (Ha et al. 2020).\nIn this section, we provide more evidence that Transformer-\nbased models are more capable of producing larger deforma-\ntions. We used 115 test volumes from the IXI dataset to\ngenerate histograms of displacement magnitudes in millime-\nters. Fig. 17 shows histograms of the displacement mag-\nnitudes for the various methods. The models that produced\ndense displacement ﬁelds are shown for fair comparisons. Note\nthat VoxelMorph and CylceMorph are ConvNet-based mod-\nels, whereas the other models are Transformer-based. All mod-\nels were trained under the identical setting (e.g., loss functions,\nnumber of epochs, optimizers, etc.), where the only variable\nwas the network architecture. As indicated by the histograms,\nall Transformer-based models had much more larger displace-\nments than ConvNet-based models. The displacement distri-\nbutions of ConvNet-based models had a mode near 0 and had\nmore smaller displacements. We additionally showed the his-\ntograms of VoxelMorph-huge and TransMorph-small, the for-\nmer of which had 63.25M parameters and the latter of which\nhad 11.76M parameters. Despite having around 6 ×more pa-\nrameters, VoxelMorph-huge still exhibited smaller displace-\nments than TransMorph-small. This further indicates that the\nlarger displacements produced by TransMorph were not a con-\nsequence of an increase in the number of parameters but rather\nthe network architecture. Given the above-demonstrated im-\nproved registration performance of the Transformer-based mod-\nels, these histograms indicate that in cases where larger dis-\nplacements are required, the Transformer-based models will\nlikely provide better registration.\n6.5. Comparison of Loss Landscapes\nIn this section, the loss landscapes of TransMorph and\nConvNet-based models are compared. We adopted the loss\nlandscape visualization method described in (Li et al. 2018;\nGoodfellow et al. 2014; Im et al. 2016), in which a set of pre-\ntrained model parameters (denoted as θ) are perturbed in two\nrandom directions (denoted as δand η) with step sizes of αand\nβ to acquire loss values at di ﬀerent locations. The loss land-\nscape was plotted based on the function of the form:\nf (α,β) = L(θ+ αδ+ βη), (27)\nwhere Ldenotes the loss function made up of LNCC and dif-\nfusion regularizer. We averaged the loss landscapes of ten sam-\nples from the validation set of the atlas-to-patient registration\n24 Junyu Chen et al. / Medical Image Analysis (2022)\nFig. 17: Histograms of the displacement magnitudes in millimeters. These histograms were generated using 115 test volumes from the IXI dataset. The displacement\nmagnitude is computed as\n√\nd2x + d2y + d2z , where d{x,y,z}denotes the displacement in x, y, and z directions. The median displacement magnitude is shown in the\nupper right corner of each plot. To provide fair comparisons, only models that produce dense displacements are shown here. VoxelMorph and CycleMorph are\nConvNet-based models, whereas the other models are Transformer-based.\nFig. 18: Validation Dice scores for inter-patient brain MRI registration during training. The validation dataset comprises 104 image pairings that were not included\nin the training or testing set.\ntask to obtain the ﬁnal 3D contour plot for each model. For\ncomparison between ConvNet-based models and TransMorph,\nthe loss landscapes of VoxelMorph, MIDIR, and TransMorph\nwere created as shown in Fig. 16. TransMorph produced a\nsubstantially ﬂatter loss landscape than that of the ConvNet-\nbased models. This observation is consistent with the ﬁndings\ngiven in (Park and Kim 2022), which suggest that Transform-\ners tend to promote ﬂatter loss landscapes. Many studies have\ndemonstrated that a ﬂatter landscape results in improved perfor-\nmance and better generalizability (Park and Kim 2022; Keskar\net al. 2016; Santurkar et al. 2018; Foret et al. 2020; Li et al.\n2018). The ﬂatter landscape of TransMorph further demon-\nstrates the advantages of Transformer-based models for image\nregistration.\n6.6. Convergence and Speed\nThe left panel of Fig. 18 shows the validation dice scores\nof the learning-based methods during training. In comparison\nto other methods, the proposed TransMorph achieved > 0.7\nin Dice within the ﬁrst 20 epochs, showing that it learned the\nspatial correspondence between image pairs quicker than the\ncompeting models. Notably, TransMorph consistently out-\nperformed the other Transformer-based models while having a\ncomparable number of parameters and computational complex-\nity. This implied Swin Transformer architecture was more ef-\nfective than other Transformers, resulting in a performance im-\nprovement for TransMorph. On average, Transformer-based\nmodels provided better validation scores than ConvNet-based\nmodels, with the exception of CoTr, whose validation results\nwere volatile during training (as seen from the orange curve in\nFig. 18). The performance of CoTr may be limited by its ar-\nchitecture design, which substitutes a Transformer for the skip\nconnections and bottleneck of a U-shaped CovnNet. As a re-\nsult, it lacks the direct ﬂow of features learned during the en-\ncoding stage to the layers creating the registration, making it\ndiﬃcult to converge. The right panel of Fig. 18 shows the\ntraining curves of the TransMorph variants and the customized\nVoxelMorph-huge. As described in (Im et al. 2016; Sutskever\net al. 2013; Darken and Moody 1991), the training curve of a\ndeep learning model consists of two phases: a “transient” phase\nfollowed by a “minimization” phase, where the former identi-\nﬁes the neighborhood of local minima and the latter seeks the\nlocal minima inside that neighborhood. As seen in the ﬁgure,\nTransMorph variants had shorter “transient” phases than that of\nVoxelMorph-huge, indicating that they identiﬁed the local min-\nJunyu Chen et al. / Medical Image Analysis (2022) 25\nModel Training (min/epoch) Inference (sec/image)\nSyN - 192.140\nNiftyReg - 30.723\nLDDMM - 66.829\ndeedsBCV - 31.857\nVoxelMorph-1 8.75 0.380\nVoxelMorph-2 9.40 0.430\nVoxelMorph-diﬀ 4.20 0.049\nVoxelMorph-huge 28.50 1.107\nCycleMorph 41.90 0.281\nMIDIR 4.05 1.627\nViT-V-Net 9.20 0.197\nPVT 13.80 0.209\nCoTr 17.10 0.372\nnnFormer 6.35 0.105\nTransMorph-Bayes 22.60 7.739\nTransMorph-diff 7.35 0.099\nTransMorph-bspl 10.50 1.739\nTransMorph 14.40 0.329\nTable 7: Average training and inference time for methods used in this work.\nNote that SyN, NiftyReg, and deedsBCV were applied using CPUs, while\nLDDMM and the learning-based methods were implemented on GPU. Inference\ntime was averaged based on 40 repeated runs.\nima neighborhood more quickly. A fast convergent algorithm is\noften preferred since it not only saves time but also computing\nresources and costs. There have been many e ﬀorts to acceler-\nate the convergence rate of deep learning models (Darken and\nMoody 1991; Looney 1996; Zeiler et al. 2013; Smith and Topin\n2019). TransMorph tends to accelerate convergence rate com-\npared to ConvNet-based models, which promotes its potential\nof faster training using fewer epochs, saving time and reducing\nthe carbon footprint.\nTable 7 compares the training time in min per epoch\n(min/epoch) and inference time in seconds per image\n(sec/image) among the methods used in this paper. Note that\nSyN, NiftyReg, and deedsBCV packages are all CPU-based,\nwhile LDDMM and the deep-learning-based methods are all GPU-\nbased. The speed was calculated using an input image size of\n160 ×192 ×224, which corresponds to the size of the brain\nMRI scans. The training time per epoch was computed based\non 768 training image pairs. The most and second most time-\nconsuming methods to train are two ConvNet-based methods,\nCycleMorph and the customized VoxelMorph-huge, which re-\nquired approximately (41 .90min ×500)/(60min ×24hr) ≈15\ndays and (28 .50min ×500)/(60min ×24hr) ≈ 10 days for\n500 epochs of training, respectively. CycleMorph was time-\nconsuming because the cycle-consistent training virtually trains\nfour networks simultaneously in a single epoch. Whereas the\ntraining of VoxelMorph-huge was slowed down by the exten-\nsive convolution operations. The proposed TransMorph has\na moderate training speed, roughly 1 .5×that of VoxelMorph-\n2 but 0 .5× that of the customized VoxelMorph-huge. In\nterms of inference time, learning-based models undoubtedly\noperated orders of magnitudes faster than traditional registra-\ntion methods. Note that TransMorph is about 3 ×faster than\nVoxelMorph-huge during inference. These ﬁnds are propor-\ntional to the calculated computational complexity as shown\nin the barplot on the left in Fig. 10. Among the learning-\nbased models, TransMorph-Bayes required the highest infer-\nence time. However, the time required is due to the sampling of\nT = 25 images for a single prediction and uncertainty estima-\ntion.\n6.7. Limitations\nThere are some limitations to our work. First, rather than do-\ning extensive grid searches for optimal hyperparameters for the\nbaseline methods, the hyperparameters are either determined\nempirically or based on the values suggested in the original pa-\nper. Due to the time required to train some of the baseline meth-\nods and the limited memory available on the GPU, we were un-\nable to aﬀord the intensive grid search. Moreover, because this\nstudy introduced a generic network architecture for image regis-\ntration, we concentrated on architectural comparison rather than\non selecting optimal hyperparameters for loss functions or com-\nplex training methods. However, the proposedTransMorph ar-\nchitecture is readily adaptable using either the cycle-consistent\ntraining method used by CycleMorph (Kim et al. 2021) or the\nsymmetric training method proposed in (Mok and Chung 2020).\nAdditionally, the proposed network may be used in conjunction\nwith any registration loss function.\nIn the future, we will investigate alternative loss functions,\nsuch as mutual information, in an eﬀort to expand the potential\nof the proposed method for multi-modal registration tasks.\n7. Conclusion\nIn this paper, we introduced TransMorph, a novel model for\nunsupervised deformable image registration. TransMorph is\nbuilt on Transformer, which is well-known for its capability\nto establish long-range spatial correspondence between image\nvoxels, making TransMorph a strong candidate for image reg-\nistration tasks.\nTwo variants of TransMorph are proposed, which pro-\nvide topology-preserved deformations. Additionally, we in-\ntroduced Bayesian deep learning to the Transformer encoder\nof TransMorph, enabling deformation uncertainty estimation\nwithout degrading registration performance.\nWe evaluated TransMorph on the task of inter-patient brain\nMR registration and a novel task of phantom-to-CT registration.\nThe results revealed that TransMorph achieved superior reg-\nistration accuracy than various traditional and learning-based\nmethods, demonstrating its eﬀectiveness for medical image reg-\nistration.\nDeclaration of Competing Interest\nThe authors declare that they have no competing interests.\nThis manuscript has not been submitted to, nor is under re-\nview at, another journal or other publishing venue.\nCRediT authorship contribution statement\nJunyu Chen: Conceptualization, Methodology, Software,\nData curation, Investigation, Writing - original draft, Visual-\nization. Eric C. Frey : Validation, Resources, Writing - Re-\nview and Editing, Supervision, Funding acquisition.Yufan He:\n26 Junyu Chen et al. / Medical Image Analysis (2022)\nMethodology, Validation, Investigation, Writing - Review and\nEditing. William P. Segars: Data curation. Ye Li: Validation.\nYong Du: Validation, Resources, Data curation, Writing - Re-\nview and Editing, Supervision, Funding acquisition.\nAcknowledgments\nThis work was supported by grants from the National\nInstitutes of Health, U01-CA140204, R01EB031023, and\nU01EB031798. The views expressed in written conference ma-\nterials or publications and by speakers and moderators do not\nnecessarily reﬂect the o ﬃcial policies of the NIH; nor does\nmention by trade names, commercial practices, or organizations\nimply endorsement by the U.S. Government.\nJunyu Chen et al. / Medical Image Analysis (2022) 27\nAppendix\nAppendix A. A ﬃne Network Architecture\nFig. A.19: Visualization of the proposed Swin-Transformer-based a ﬃne network. This network outputs three rotation, three translation, three scaling, and three\nshearing parameters for rigid registration. The embedding dimension C in the network was set to 12.\nAppendix B. Miscalibration in Predictive Variance\nThe expected model error (characterized by MSE) is deﬁned as:\nerr(Im ◦φ) = E\n[\n(Im ◦φ−If )2]\n= 1\nT\nT∑\nt=1\n(\nIm ◦φt −If\n)2\n, (B.1)\nwhere t represents the tth sample from a total number of T samples. We denote Id = Im ◦φfor convenience, and it can be shown\nthat:\nE\n[\n(Id −If )2]\n= E\n[\n(Id −E[Id] + E[Id] −If )2]\n= E\n[\n(Id −E[Id])2]\n+\n(\nE[Id] −If\n)2\n+ 2(E[Id] −If )E[Id −E[Id]]\n= E\n[\n(Id −E[Id])2]\n+\n(\nE[Id] −If\n)2\n.\n(B.2)\nTherefore,\nerr(Im ◦φ) = 1\nT\nT∑\nt=1\n(\nIm ◦φt −If\n)2\n= 1\nT\nT∑\nt=1\nIm ◦φt −1\nT\nT∑\nt=1\nIm ◦φt\n\n2\n+\n\n1\nT\nT∑\nt=1\nIm ◦φt −If\n\n2\n= ˆΣ2\nf +\n(ˆIf −If\n)2\n,\n(B.3)\nwhere ˆIf −If is referred to as the bias between the predictive mean ˆIf and the target image If . Due to the problem of overﬁtting\nthe training set in supervised algorithms (e.g., deep learning) (Bishop 2006), this bias may be less noticeable on training dataset\nbut more noticeable on test images, which is a phenomenon referred to as the bias-variance tradeoﬀ (Friedman 2017). As a\nconsequence, the predictive variance ˆΣ2\nf is systematically smaller than the expected error err(Im ◦φ), resulting in miscalibrated\nuncertainty estimations.\n28 Junyu Chen et al. / Medical Image Analysis (2022)\nAppendix C. Additional Results for Inter-patient Brain MRI Registration\nFig. C.20: Additional qualitative comparison of various registration methods on the inter-patient brain MR registration task. The ﬁrst row shows the deformed\nmoving images, the second row shows the deformation ﬁelds, and the last row shows the deformed grids. The spatial dimension x, y, and z in the displacement ﬁeld\nis mapped to each of the RGB color channels, respectively. The [p, q] in color bars denotes the magnitude range of the ﬁelds.\nJunyu Chen et al. / Medical Image Analysis (2022) 29\nFig. C.21: Quantitative comparison of the various registration methods on the inter-patient brain MR registration task. Boxplots showing Dice scores for di ﬀerent\nbrain MR substructures using the proposed TransMorph and existing image registration methods.\nFig. C.22: Quantitative comparison of the Transformer-based models on the inter-patient brain MR registration task. Boxplots showing Dice scores for di ﬀerent\nbrain MR substructures using the proposed TransMorph, the variants of TransMorph, and other Transformer architectures.\n30 Junyu Chen et al. / Medical Image Analysis (2022)\nAppendix D. Additional Results for Atlas-to-patient Brain MRI Registration\nFig. D.23: Additional qualitative comparison of various registration methods on the atlas-to-patient brain MR registration task. The ﬁrst row shows the deformed\nmoving images, the second row shows the deformation ﬁelds, and the last row shows the deformed grids. The spatial dimension x, y, and z in the displacement ﬁeld\nis mapped to each of the RGB color channels, respectively. The [p, q] in color bars denotes the magnitude range of the ﬁelds.\nJunyu Chen et al. / Medical Image Analysis (2022) 31\nFig. D.24: Quantitative comparison of the various registration methods on the atlas-to-patient brain MR registration task. Boxplots showing Dice scores for diﬀerent\nbrain MR substructures using the proposed TransMorph and existing image registration methods.\nFig. D.25: Quantitative comparison of the Transformer-based models on the atlas-to-patient brain MR registration task. Boxplots showing Dice scores for di ﬀerent\nbrain MR substructures using the proposed TransMorph, the variants of TransMorph, and other Transformer architectures.\n32 Junyu Chen et al. / Medical Image Analysis (2022)\nAppendix E. Additional Results for XCAT-to-CT Registration\nFig. E.26: Additional qualitative comparison of various registration methods on the XCAT-to-CT registration task. The ﬁrst row shows the deformed moving images,\nthe second row shows the deformation ﬁelds, and the last row shows the deformed grids. The spatial dimension x, y, and z in the displacement ﬁeld is mapped to\neach of the RGB color channels, respectively. The [p, q] in color bars denotes the magnitude range of the ﬁelds.\nJunyu Chen et al. / Medical Image Analysis (2022) 33\nFig. E.27: Additional coronal slices of the deformed XCAT phantom generated by various registration methods.\n34 Junyu Chen et al. / Medical Image Analysis (2022)\nFig. E.28: Quantitative comparison of various registration methods on the XCAT-to-CT registration task. Boxplots showing Dice scores for di ﬀerent organs in CT\nobtained using the proposed TransMorph and existing image registration methods.\nFig. E.29: Quantitative comparison of the Transformer-based models on the XCAT-to-CT registration task. Boxplots showing Dice scores for diﬀerent organs in CT\nobtained using the proposed TransMorph, the variants of TransMorph, and other Transformer architectures.\nJunyu Chen et al. / Medical Image Analysis (2022) 35\nAppendix F. Additional Qualitative Results for Uncertainty Quantiﬁcation\nFig. F.30: Qualitative results and registration uncertainty estimate with TransMorph-Bayes. The fourth and the ﬁfth columns exhibit the appearance uncertainties\nestimated using the proposed uncertainty estimation scheme (i.e., Σ2\nf ). The last column shows the transformation uncertainties, i.e., ˆΣ2\nφ, where the uncertainty maps\nwere taken as square root of the sum of the variances of the deformation inx, y, and z direction. The spatial dimension x, y, and z in the displacement ﬁeld is mapped\nto each of the RGB color channels, respectively. The [p, q] in color bars denotes the magnitude range of the ﬁelds.\n36 Junyu Chen et al. / Medical Image Analysis (2022)\nAppendix G. Visualization of Feature Maps in Skip Connections\nFig. G.31: Feature maps in TransMorph’s skip connections. (a) and (b) exhibit, respectively, the feature maps in the ﬁrst and second skip connections from the\nconvolutional layers in the encoder (i.e., the green arrows in Fig. 1); (c)-(f) exhibit the feature maps in the skip connections from the Transformer blocks (i.e., the\norange arrows in Fig. 1).\nAppendix H. Probabilistic di ﬀeomorphic registration\nAs shown in section 3.3, we introduced a variational inference framework to the proposed TransMorph (which we denote as\nTransMorph-diff). A prior distribution\np(u) = N(u; 0,Σu) (H.1)\nwas placed over the dense displacement ﬁeld u, where 0 and Σu are the mean and covariance of the multivariate Gaussian distribu-\ntion. We followed (Dalca et al. 2019) and deﬁned Σ−1\nu = Λu = λL, where Λu denotes the precision matrix, λcontrols the scale of\nu, L = D −A is the Laplacian matrix of a neighborhood graph formed on the voxel grid, D is the graph degree matrix, and A is a\nvoxel neighborhood adjacency matrix. The probability p(If |Im) can be computed using the law of total probability:\np(If |Im) =\n∫\nu\np(If |u,Im)p(u)du. (H.2)\nThe likelihood p(If |u,Im) was also assumed to be Gaussian\np(If |u,Im) = N(If ; Im ◦φu,σ2\nI I), (H.3)\nwhere σ2\nI captures the variance of the image noise, and φu is the group exponential of the time-stationary velocity ﬁeld u, i.e.\nφ= exp(u), and was computed using a scaling-and-squaring approach (section 2.1.2).\nOur goal is to estimate the posterior probability p(u|If ,Im). Due to the intractable nature of the integral over u in Eqn. H.2,\np(I f|Im) is usually calculated using just the u’s that are most likely to have generated If (Krebs et al. 2019). Since computing the\nposterior p(u|If ,Im) analytically is also intractable, we instead assumed a variational posterior qψ(u|If ,Im) learned by the network\nJunyu Chen et al. / Medical Image Analysis (2022) 37\nwith parameters ψ. The Kullback-Leibler divergence (KL) was used to relate the variational posterior to the actual posterior, which\nresults in the evidence lower limit (ELBO) (Kingma and Welling 2013):\nlogp(If |Im) −KL\n[\nqψ(u|If ,Im)∥p(u|If ,Im)\n]\n=\nEu∼qψ\n[\nlog p(If |u,Im)\n]\n−KL\n[\nqψ(u|If ,Im)∥p(u)\n]\n,\n(H.4)\nwhere the KL-divergence on the left hand side vanishes if the variational posterior is identical to the actual posterior. Therefore,\nmaximizing log p(If |Im) is equivalent to minimizing the negative of ELBO on the right hand side of Eqn. H.4. Since the prior\ndistribution p(u) was assumed to be a multivariate Gaussian, the variational posterior is likewise a multivariate Gaussian, deﬁned\nas:\nqψ(u|If ,Im) = N(u; µψ(u|If ,Im),Σψ(u|If ,Im)), (H.5)\nwhere µψ and Σψ are the voxel-wise mean and variance generated by the network with parameters ψ. In each forward pass, the\ndense displacement ﬁeld u is sampled using reparameterization u = µψ + Σψ ⊙ϵ with ϵ ∼N(0,I). The variational parameters µψ\nand Σψ are learned by minimizing the loss (Dalca et al. 2019):\nLprob.(If ,Im,φu; ψ)\n= −Eu∼qψ\n[\nlog p(If |u,Im)\n]\n+ KL\n[\nqψ(u|If ,Im)∥p(u)\n]\n= 1\n2σ2 ∥If −Im ◦φu∥2 + 1\n2\n[\ntr(λDΣψ −log Σψ) + µ⊤\nψΛuµψ\n]\n,\n(H.6)\nwhere µ⊤\nψΛuµψ can be thought of as a di ﬀusion regluarization (Eqn. 14) placed over the mean displacement ﬁeld µψ, that is\nµ⊤\nψΛuµψ = λ\n2\n∑\np\n∑\ni∈N(p) (µ(p) −µ(i))2, where N(p) represents the neighboring voxels of the pth voxel.\nAs discussed in section 3.2.2, when the auxiliary segmentation information is available (i.e., the label maps ofIf and Im, denoted\nas sf and sm), Dice loss can be used for training the network to further enhance registration performance. Dice loss, however,\ndoes not preserve a Gaussian approximation of the deformation ﬁelds. Instead, we follow (Dalca et al. 2019) and replace the KL\ndivergence in Eqn. H.4 with:\nKL\n[\nqψ(u|If ,Im)∥p(u|If ,sf ; Im,sm)\n]\n, (H.7)\nwhich yields a loss function of the form:\nLprob. w/ aux.(If ,sf ,Im,sm,φu; ψ)\n= 1\n2σ2 ∥If −Im ◦φu∥2 + 1\n2σ2s\n∥sf −sm ◦φu∥2\n+ 1\n2\n[\ntr(λDΣψ −log Σψ) + µ⊤\nψΛuµψ\n]\n.\n(H.8)\nIn (Dalca et al. 2019), sf and sm represent anatomical surfaces obtained from label maps. In contrast, we directly used the label\nmaps as sf and sm in this work. They were image volumes with multiple channels, each channel contained a binary mask deﬁning\nthe segmentation of a certain structure/organ.\nAppendix I. B-splines di ﬀeomorphic registration\nAs demonstrated in section 3.3, we incorporated a cubic B-spline model (Qiu et al. 2021) intoTransMorph (which we denote as\nTransMorph-bspl). This network produces a lattice of low-dimensional control points instead of producing a dense displacement\nﬁeld at the original resolution, which might be computationally costly. As shown in Fig. 6, we denote the displacements of the B-\nspline control points generated by the network asuB and the spacing between the control points asδ. Then, a weighted combination\nof cubic B-spline basis functions (i.e., βd) (Rueckert et al. 1999) is used to generate the dense displacement ﬁeld (i.e., the B-spline\ntensor product in Fig. 6):\nˆu(p) =\n∑\nc∈C\nuB(c)\n∏\nd∈{x,y,z}\nβd(pd −k(cd)), (I.1)\nwhere c is the index of the control points on the lattice C, and k denotes the coordinates of the control points uB(c) in image space.\nThen the ﬁnal time-stationary displacement u is obtained using the same scaling-and-squaring approach described in section 2.1.2.\n38 Junyu Chen et al. / Medical Image Analysis (2022)\nReferences\nAlom, M.Z., Hasan, M., Yakopcic, C., Taha, T.M., Asari, V .K., 2018. Recurrent\nresidual convolutional neural network based on u-net (r2u-net) for medical\nimage segmentation. arXiv preprint arXiv:1802.06955 .\nArmstrong, R.A., 2014. When to use the b onferroni correction. Ophthalmic\nand Physiological Optics 34, 502–508.\nArsigny, V ., Commowick, O., Pennec, X., Ayache, N., 2006. A log-euclidean\nframework for statistics on di ﬀeomorphisms, in: International Confer-\nence on Medical Image Computing and Computer-Assisted Intervention,\nSpringer. pp. 924–931.\nAshburner, J., 2007. A fast di ﬀeomorphic image registration algorithm. Neu-\nroimage 38, 95–113.\nAtanov, A., Ashukha, A., Molchanov, D., Neklyudov, K., Vetrov, D., 2018.\nUncertainty estimation via stochastic batch normalization. arXiv preprint\narXiv:1802.04893 .\nAvants, B.B., Epstein, C.L., Grossman, M., Gee, J.C., 2008. Symmetric dif-\nfeomorphic image registration with cross-correlation: evaluating automated\nlabeling of elderly and neurodegenerative brain. Medical image analysis 12,\n26–41.\nBalakrishnan, G., Zhao, A., Sabuncu, M.R., Guttag, J., Dalca, A.V ., 2018. An\nunsupervised learning model for deformable medical image registration, in:\nProceedings of the IEEE conference on computer vision and pattern recog-\nnition, pp. 9252–9260.\nBalakrishnan, G., Zhao, A., Sabuncu, M.R., Guttag, J., Dalca, A.V ., 2019. V ox-\nelmorph: a learning framework for deformable medical image registration.\nIEEE transactions on medical imaging 38, 1788–1800.\nBaumgartner, C.F., Tezcan, K.C., Chaitanya, K., H ¨otker, A.M., Muehlematter,\nU.J., Schawkat, K., Becker, A.S., Donati, O., Konukoglu, E., 2019. Phiseg:\nCapturing uncertainty in medical image segmentation, in: International Con-\nference on Medical Image Computing and Computer-Assisted Intervention,\nSpringer. pp. 119–127.\nBeg, M.F., Miller, M.I., Trouv ´e, A., Younes, L., 2005. Computing large de-\nformation metric mappings via geodesic ﬂows of diﬀeomorphisms. Interna-\ntional journal of computer vision 61, 139–157.\nBishop, C.M., 2006. Pattern recognition. Machine learning 128.\nBlundell, C., Cornebise, J., Kavukcuoglu, K., Wierstra, D., 2015. Weight uncer-\ntainty in neural network, in: International Conference on Machine Learning,\nPMLR. pp. 1613–1622.\nCao, X., Yang, J., Wang, L., Xue, Z., Wang, Q., Shen, D., 2018. Deep learning\nbased inter-modality image registration supervised by intra-modality simi-\nlarity, in: International workshop on machine learning in medical imaging,\nSpringer. pp. 55–63.\nChen, J., Frey, E., Du, Y ., 2022. Unsupervised learning of diﬀeomorphic image\nregistration via transmorph, in: 10th Internatioal Workshop on Biomedical\nImage Registration. URL: https://openreview.net/forum?id=uwIo_\n_2xnTO.\nChen, J., He, Y ., Frey, E.C., Li, Y ., Du, Y ., 2021a. Vit-v-net: Vision transformer\nfor unsupervised volumetric medical image registration. arXiv preprint\narXiv:2104.06468 .\nChen, J., Jha, A.K., Frey, E.C., 2019. Incorporating ct prior information in the\nrobust fuzzy c-means algorithm for qspect image segmentation, in: Medi-\ncal Imaging 2019: Image Processing, International Society for Optics and\nPhotonics. p. 109491W.\nChen, J., Li, Y ., Du, Y ., Frey, E.C., 2020. Generating anthropomorphic phan-\ntoms using fully unsupervised deformable image registration with convolu-\ntional neural networks. Medical physics .\nChen, J., Lu, Y ., Yu, Q., Luo, X., Adeli, E., Wang, Y ., Lu, L., Yuille, A.L., Zhou,\nY ., 2021b. Transunet: Transformers make strong encoders for medical image\nsegmentation. arXiv preprint arXiv:2102.04306 .\nChetty, I.J., Rosu-Bubulac, M., 2019. Deformable registration for dose accu-\nmulation, in: Seminars in radiation oncology, Elsevier. pp. 198–208.\nChristoﬀersen, C.P., Hansen, D., Poulsen, P., Sorensen, T.S., 2013.\nRegistration-based reconstruction of four-dimensional cone beam computed\ntomography. IEEE Transactions on Medical Imaging 32, 2064–2077.\ndoi:10.1109/TMI.2013.2272882.\nCui, K., Fu, P., Li, Y ., Lin, Y ., 2021. Bayesian fully convolutional networks for\nbrain image registration. Journal of Healthcare Engineering 2021.\nDai, X., Chen, Y ., Xiao, B., Chen, D., Liu, M., Yuan, L., Zhang, L., 2021. Dy-\nnamic head: Unifying object detection heads with attentions, in: Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pp. 7373–7382.\nDalca, A.V ., Balakrishnan, G., Guttag, J., Sabuncu, M.R., 2019. Unsupervised\nlearning of probabilistic diﬀeomorphic registration for images and surfaces.\nMedical image analysis 57, 226–236.\nDarken, C., Moody, J., 1991. Towards faster stochastic gradient search. Ad-\nvances in neural information processing systems 4.\nDevalla, S.K., Renukanand, P.K., Sreedhar, B.K., Subramanian, G., Zhang,\nL., Perera, S., Mari, J.M., Chin, K.S., Tun, T.A., Strouthidis, N.G., et al.,\n2018. Drunet: a dilated-residual u-net deep learning network to segment op-\ntic nerve head tissues in optical coherence tomography images. Biomedical\noptics express 9, 3244–3265.\nDeVries, T., Taylor, G.W., 2018. Leveraging uncertainty estimates for predict-\ning segmentation quality. arXiv preprint arXiv:1807.00502 .\nDice, L.R., 1945. Measures of the amount of ecologic association between\nspecies. Ecology 26, 297–302.\nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo,\nB., 2021. Cswin transformer: A general vision transformer backbone with\ncross-shaped windows. arXiv preprint arXiv:2107.00652 .\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Un-\nterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.,\n2020. An image is worth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929 .\nFischl, B., 2012. Freesurfer. Neuroimage 62, 774–781.\nForet, P., Kleiner, A., Mobahi, H., Neyshabur, B., 2020. Sharpness-aware\nminimization for e ﬃciently improving generalization. arXiv preprint\narXiv:2010.01412 .\nFriedman, J.H., 2017. The elements of statistical learning: Data mining, infer-\nence, and prediction. springer open.\nFu, W., Sharma, S., Abadi, E., Iliopoulos, A.S., Wang, Q., Sun, X., Lo, J.Y .C.,\nSegars, W.P., Samei, E., 2021. iphantom: a framework for automated cre-\nation of individualized computational phantoms and its application to ct or-\ngan dosimetry. IEEE Journal of Biomedical and Health Informatics .\nGal, Y ., Ghahramani, Z., 2016. Dropout as a bayesian approximation: Repre-\nsenting model uncertainty in deep learning, in: international conference on\nmachine learning, PMLR. pp. 1050–1059.\nGear, J.I., Cox, M.G., Gustafsson, J., Gleisner, K.S., Murray, I., Glatting, G.,\nKonijnenberg, M., Flux, G.D., 2018. Eanm practical guidance on uncer-\ntainty analysis for molecular radiotherapy absorbed dose calculations. Eu-\nropean journal of nuclear medicine and molecular imaging 45, 2456–2474.\nGoodfellow, I.J., Vinyals, O., Saxe, A.M., 2014. Qualitatively characterizing\nneural network optimization problems. arXiv preprint arXiv:1412.6544 .\nGuo, C., Pleiss, G., Sun, Y ., Weinberger, K.Q., 2017. On calibration of modern\nneural networks, in: International Conference on Machine Learning, PMLR.\npp. 1321–1330.\nHa, I.Y ., Wilms, M., Heinrich, M., 2020. Semantically guided large deforma-\ntion estimation with deep networks. Sensors 20, 1392.\nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image\nrecognition, in: Proceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 770–778.\nHeinrich, M.P., Jenkinson, M., Brady, M., Schnabel, J.A., 2013a. Mrf-based\ndeformable registration and ventilation estimation of lung ct. IEEE transac-\ntions on medical imaging 32, 1239–1248.\nHeinrich, M.P., Jenkinson, M., Papie˙z, B.W., Brady, M., Schnabel, J.A., 2013b.\nTowards realtime multimodal fusion for image-guided interventions using\nself-similarities, in: International conference on medical image computing\nand computer-assisted intervention, Springer. pp. 187–194.\nHeinrich, M.P., Maier, O., Handels, H., 2015. Multi-modal multi-atlas segmen-\ntation using discrete optimisation and self-similarities. VISCERAL Chal-\nlenge@ ISBI 1390, 27.\nHering, A., Hansen, L., Mok, T.C., Chung, A., Siebert, H., H¨ager, S., Lange, A.,\nKuckertz, S., Heldmann, S., Shao, W., et al., 2021. Learn2reg: comprehen-\nsive multi-task medical image registration challenge, dataset and evaluation\nin the era of deep learning. arXiv preprint arXiv:2112.04489 .\nHernandez, M., Bossa, M.N., Olmos, S., 2009. Registration of anatomical\nimages using paths of diﬀeomorphisms parameterized with stationary vector\nﬁeld ﬂows. International Journal of Computer Vision 85, 291–306.\nHoﬀmann, M., Billot, B., Iglesias, J.E., Fischl, B., Dalca, A.V ., 2020. Learning\nimage registration without images. arXiv preprint arXiv:2004.10282 .\nHoopes, A., Ho ﬀmann, M., Fischl, B., Guttag, J., Dalca, A.V ., 2021. Hy-\npermorph: amortized hyperparameter learning for image registration, in:\nInternational Conference on Information Processing in Medical Imaging,\nSpringer. pp. 3–17.\nIm, D.J., Tao, M., Branson, K., 2016. An empirical analysis of deep network\nJunyu Chen et al. / Medical Image Analysis (2022) 39\nloss surfaces .\nIsensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H., 2021. nnu-\nnet: a self-conﬁguring method for deep learning-based biomedical image\nsegmentation. Nature methods 18, 203–211.\nJaderberg, M., Simonyan, K., Zisserman, A., et al., 2015. Spatial transformer\nnetworks. Advances in neural information processing systems 28, 2017–\n2025.\nJha, D., Smedsrud, P.H., Riegler, M.A., Johansen, D., De Lange, T., Halvorsen,\nP., Johansen, H.D., 2019. Resunet++: An advanced architecture for medical\nimage segmentation, in: 2019 IEEE International Symposium on Multime-\ndia (ISM), IEEE. pp. 225–2255.\nJohnson, H.J., Christensen, G.E., 2002. Consistent landmark and intensity-\nbased image registration. IEEE transactions on medical imaging 21, 450–\n461.\nKendall, A., Gal, Y ., 2017. What uncertainties do we need in Bayesian\ndeep learning for computer vision?, in: Advances in Neural Informa-\ntion Processing Systems, Neural information processing systems founda-\ntion. pp. 5575–5585. URL: https://arxiv.org/abs/1703.04977v2,\narXiv:1703.04977.\nKeskar, N.S., Mudigere, D., Nocedal, J., Smelyanskiy, M., Tang, P.T.P., 2016.\nOn large-batch training for deep learning: Generalization gap and sharp min-\nima. arXiv preprint arXiv:1609.04836 .\nKim, B., Kim, D.H., Park, S.H., Kim, J., Lee, J.G., Ye, J.C., 2021. Cyclemorph:\nCycle consistent unsupervised deformable image registration. Medical Im-\nage Analysis 71, 102036.\nKingma, D.P., Welling, M., 2013. Auto-encoding variational bayes. arXiv\npreprint arXiv:1312.6114 .\nKrebs, J., Delingette, H., Mailh ´e, B., Ayache, N., Mansi, T., 2019. Learning\na probabilistic model for di ﬀeomorphic registration. IEEE transactions on\nmedical imaging 38, 2165–2176.\nKuleshov, V ., Fenner, N., Ermon, S., 2018. Accurate uncertainties for deep\nlearning using calibrated regression, in: International Conference on Ma-\nchine Learning, PMLR. pp. 2796–2804.\nKybic, J., 2009. Bootstrap resampling for image registration uncertainty esti-\nmation without ground truth. IEEE Transactions on Image Processing 19,\n64–73.\nLaves, M.H., Ihler, S., Fast, J.F., Kahrs, L.A., Ortmaier, T., 2020a. Well-\ncalibrated regression uncertainty in medical imaging with deep learning, in:\nArbel, T., Ben Ayed, I., de Bruijne, M., Descoteaux, M., Lombaert, H., Pal,\nC. (Eds.), Proceedings of the Third Conference on Medical Imaging with\nDeep Learning, PMLR. pp. 393–412. URL:https://proceedings.mlr.\npress/v121/laves20a.html.\nLaves, M.H., Ihler, S., Kortmann, K.P., Ortmaier, T., 2019. Well-calibrated\nmodel uncertainty with temperature scaling for dropout variational infer-\nence. arXiv preprint arXiv:1909.13550 .\nLaves, M.H., T ¨olle, M., Ortmaier, T., 2020b. Uncertainty Estimation in Med-\nical Image Denoising with Bayesian Deep Image Prior. Lecture Notes in\nComputer Science (including subseries Lecture Notes in Artiﬁcial Intelli-\ngence and Lecture Notes in Bioinformatics) 12443 LNCS, 81–96. URL:\nhttps://arxiv.org/abs/2008.08837v1, arXiv:2008.08837.\nLaves, M.H., T ¨olle, M., Ortmaier, T., 2020c. Uncertainty estimation in med-\nical image denoising with bayesian deep image prior, in: Uncertainty for\nSafe Utilization of Machine Learning in Medical Imaging, and Graphs in\nBiomedical Image Analysis. Springer, pp. 81–96.\nLei, Y ., Fu, Y ., Wang, T., Liu, Y ., Patel, P., Curran, W.J., Liu, T., Yang, X.,\n2020. 4d-ct deformable image registration using multiscale unsupervised\ndeep learning. Physics in Medicine & Biology 65, 085003.\nLevi, D., Gispan, L., Giladi, N., Fetaya, E., 2019. Evaluating and calibrating\nuncertainty prediction in regression tasks. arXiv preprint arXiv:1905.11659\n.\nLi, H., Xu, Z., Taylor, G., Studer, C., Goldstein, T., 2018. Visualizing the\nloss landscape of neural nets. Advances in neural information processing\nsystems 31.\nLi, S., Sui, X., Luo, X., Xu, X., Liu, Y ., Goh, R.S.M., 2021. Medical im-\nage segmentation using squeeze-and-expansion transformers. arXiv preprint\narXiv:2105.09511 .\nLian, C., Liu, M., Zhang, J., Shen, D., 2018. Hierarchical fully convolutional\nnetwork for joint atrophy localization and alzheimer’s disease diagnosis us-\ning structural mri. IEEE transactions on pattern analysis and machine intel-\nligence 42, 880–893.\nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., Guo, B., 2021a.\nSwin transformer: Hierarchical vision transformer using shifted windows.\narXiv preprint arXiv:2103.14030 .\nLiu, Z., Mao, H., Wu, C.Y ., Feichtenhofer, C., Darrell, T., Xie, S., 2022. A\nconvnet for the 2020s. arXiv preprint arXiv:2201.03545 .\nLiu, Z., Ning, J., Cao, Y ., Wei, Y ., Zhang, Z., Lin, S., Hu, H., 2021b. Video\nswin transformer. arXiv preprint arXiv:2106.13230 .\nLong, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional networks for\nsemantic segmentation, in: Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 3431–3440.\nLooney, C.G., 1996. Stabilization and speedup of convergence in training feed-\nforward neural networks. Neurocomputing 10, 7–31.\nLuo, J., Sedghi, A., Popuri, K., Cobzas, D., Zhang, M., Preiswerk, F., Toews,\nM., Golby, A., Sugiyama, M., Wells, W.M., et al., 2019. On the applicability\nof registration uncertainty, in: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, Springer. pp. 410–419.\nLuo, W., Li, Y ., Urtasun, R., Zemel, R., 2016. Understanding the e ﬀective\nreceptive ﬁeld in deep convolutional neural networks, in: Proceedings of the\n30th International Conference on Neural Information Processing Systems,\npp. 4905–4913.\nLv, J., Wang, Z., Shi, H., Zhang, H., Wang, S., Wang, Y ., Li, Q., 2022. Joint\nprogressive and coarse-to-ﬁne registration of brain mri via deformation ﬁeld\nintegration and non-rigid feature fusion. IEEE Transactions on Medical\nImaging .\nMaas, A.L., Hannun, A.Y ., Ng, A.Y ., et al., 2013. Rectiﬁer nonlinearities im-\nprove neural network acoustic models, in: Proc. icml, Citeseer. p. 3.\nMarcus, D.S., Wang, T.H., Parker, J., Csernansky, J.G., Morris, J.C., Buckner,\nR.L., 2007. Open access series of imaging studies (oasis): cross-sectional\nmri data in young, middle aged, nondemented, and demented older adults.\nJournal of cognitive neuroscience 19, 1498–1507.\nMehrtash, A., Wells, W.M., Tempany, C.M., Abolmaesumi, P., Kapur, T., 2020.\nConﬁdence calibration and predictive uncertainty estimation for deep med-\nical image segmentation. IEEE transactions on medical imaging 39, 3868–\n3878.\nMilletari, F., Navab, N., Ahmadi, S.A., 2016. V-net: Fully convolutional neu-\nral networks for volumetric medical image segmentation, in: 2016 fourth\ninternational conference on 3D vision (3DV), IEEE. pp. 565–571.\nModat, M., Ridgway, G.R., Taylor, Z.A., Lehmann, M., Barnes, J., Hawkes,\nD.J., Fox, N.C., Ourselin, S., 2010. Fast free-form deformation using graph-\nics processing units. Computer methods and programs in biomedicine 98,\n278–284.\nMok, T.C., Chung, A., 2020. Fast symmetric di ﬀeomorphic image registra-\ntion with convolutional neural networks, in: Proceedings of the IEEE /CVF\nconference on computer vision and pattern recognition, pp. 4644–4653.\nMok, T.C., Chung, A., 2021. Conditional deformable image registration with\nconvolutional neural network, in: International Conference on Medical Im-\nage Computing and Computer-Assisted Intervention, Springer. pp. 35–45.\nOnofrey, J.A., Staib, L.H., Papademetris, X., 2013. Semi-supervised learning\nof nonrigid deformations for image registration, in: International MICCAI\nWorkshop on Medical Computer Vision, Springer. pp. 13–23.\nPace, D.F., Aylward, S.R., Niethammer, M., 2013. A locally adaptive regular-\nization based on anisotropic di ﬀusion for deformable image registration of\nsliding organs. IEEE transactions on medical imaging 32, 2114–2126.\nPark, N., Kim, S., 2022. How do vision transformers work? arXiv preprint\narXiv:2202.06709 .\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,\nT., Lin, Z., Gimelshein, N., Antiga, L., et al., 2019. Pytorch: An imperative\nstyle, high-performance deep learning library. Advances in neural informa-\ntion processing systems 32, 8026–8037.\nPhan, B., Salay, R., Czarnecki, K., Abdelzad, V ., Denouden, T., Vernekar, S.,\n2018. Calibrating uncertainties in object localization task. arXiv preprint\narXiv:1811.11210 .\nQiu, H., Qin, C., Schuh, A., Hammernik, K., Rueckert, D., 2021. Learning dif-\nfeomorphic and modality-invariant registration using b-splines, in: Medical\nImaging with Deep Learning.\nRaghu, M., Unterthiner, T., Kornblith, S., Zhang, C., Dosovitskiy, A., 2021. Do\nvision transformers see like convolutional neural networks? arXiv preprint\narXiv:2108.08810 .\nRedmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You only look once:\nUniﬁed, real-time object detection, in: Proceedings of the IEEE conference\non computer vision and pattern recognition, pp. 779–788.\nRisholm, P., Balter, J., Wells, W.M., 2011. Estimation of delivered dose in ra-\ndiotherapy: the inﬂuence of registration uncertainty, in: International Con-\nference on Medical Image Computing and Computer-Assisted Intervention,\n40 Junyu Chen et al. / Medical Image Analysis (2022)\nSpringer. pp. 548–555.\nRisholm, P., Janoos, F., Norton, I., Golby, A.J., Wells III, W.M., 2013. Bayesian\ncharacterization of uncertainty in intra-subject non-rigid registration. Medi-\ncal image analysis 17, 538–555.\nRoh´e, M.M., Datar, M., Heimann, T., Sermesant, M., Pennec, X., 2017. Svf-\nnet: Learning deformable image registration using shape matching, in: In-\nternational conference on medical image computing and computer-assisted\nintervention, Springer. pp. 266–274.\nRonneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks\nfor biomedical image segmentation, in: International Conference on Medi-\ncal image computing and computer-assisted intervention, Springer. pp. 234–\n241.\nRueckert, D., Sonoda, L.I., Hayes, C., Hill, D.L., Leach, M.O., Hawkes, D.J.,\n1999. Nonrigid registration using free-form deformations: application to\nbreast mr images. IEEE transactions on medical imaging 18, 712–721.\nSanturkar, S., Tsipras, D., Ilyas, A., Madry, A., 2018. How does batch nor-\nmalization help optimization? Advances in neural information processing\nsystems 31.\nSegars, W., Bond, J., Frush, J., Hon, S., Eckersley, C., Williams, C.H., Feng, J.,\nTward, D.J., Ratnanather, J., Miller, M., et al., 2013. Population of anatomi-\ncally variable 4d xcat adult phantoms for imaging research and optimization.\nMedical physics 40, 043701.\nSegars, W.P., Sturgeon, G., Mendonca, S., Grimes, J., Tsui, B.M., 2010. 4d xcat\nphantom for multimodality imaging research. Medical physics 37, 4902–\n4915.\nSiebert, H., Hansen, L., Heinrich, M.P., 2021. Fast 3d registration with ac-\ncurate optimisation and little learning for learn2reg 2021. arXiv preprint\narXiv:2112.03053 .\nSimpson, I.J., Woolrich, M., Groves, A.R., Schnabel, J.A., 2011. Longitu-\ndinal brain mri analysis with uncertain registration, in: International Con-\nference on Medical Image Computing and Computer-Assisted Intervention,\nSpringer. pp. 647–654.\nSmith, L.N., Topin, N., 2019. Super-convergence: Very fast training of neural\nnetworks using large learning rates, in: Artiﬁcial intelligence and machine\nlearning for multi-domain operations applications, International Society for\nOptics and Photonics. p. 1100612.\nSokooti, H., De V os, B., Berendsen, F., Lelieveldt, B.P., I ˇsgum, I., Staring,\nM., 2017. Nonrigid image registration using multi-scale 3d convolutional\nneural networks, in: International conference on medical image computing\nand computer-assisted intervention, Springer. pp. 232–239.\nSutskever, I., Martens, J., Dahl, G., Hinton, G., 2013. On the importance of\ninitialization and momentum in deep learning, in: International conference\non machine learning, PMLR. pp. 1139–1147.\nT¨olle, M., Laves, M.H., Schlaefer, A., 2021. A Mean-Field Variational In-\nference Approach to Deep Image Prior for Inverse Problems in Medical\nImaging. Medical Imaging with Deep Learning , 698–713URL: https:\n//openreview.net/forum?id=DvV%5C_blKLiB4.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, L., Polosukhin, I., 2017. Attention is all you need. arXiv preprint\narXiv:1706.03762 .\nVercauteren, T., Pennec, X., Perchant, A., Ayache, N., 2009. Di ﬀeomorphic\ndemons: Eﬃcient non-parametric image registration. NeuroImage 45, S61–\nS72.\nVickress, J., Battista, J., Barnett, R., Yartsev, S., 2017. Representing the dosi-\nmetric impact of deformable image registration errors. Physics in Medicine\n& Biology 62, N391.\nViola, P., Wells III, W.M., 1997. Alignment by maximization of mutual infor-\nmation. International journal of computer vision 24, 137–154.\nVishnevskiy, V ., Gass, T., Szekely, G., Tanner, C., Goksel, O., 2016. Isotropic\ntotal variation regularization of displacements in parametric image registra-\ntion. IEEE transactions on medical imaging 36, 385–395.\nde V os, B.D., Berendsen, F.F., Viergever, M.A., Sokooti, H., Staring, M., Iˇsgum,\nI., 2019. A deep learning framework for unsupervised aﬃne and deformable\nimage registration. Medical image analysis 52, 128–143.\nde V os, B.D., Berendsen, F.F., Viergever, M.A., Staring, M., I ˇsgum, I., 2017.\nEnd-to-end unsupervised deformable image registration with a convolu-\ntional neural network, in: Deep learning in medical image analysis and mul-\ntimodal learning for clinical decision support. Springer, pp. 204–212.\nWang, D., Wu, Z., Yu, H., 2021a. Ted-net: Convolution-free t2t vision\ntransformer-based encoder-decoder dilation network for low-dose ct denois-\ning. arXiv preprint arXiv:2106.04650 .\nWang, W., Chen, C., Ding, M., Li, J., Yu, H., Zha, S., 2021b. Transbts:\nMultimodal brain tumor segmentation using transformer. arXiv preprint\narXiv:2103.04430 .\nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P.,\nShao, L., 2021c. Pyramid vision transformer: A versatile backbone for dense\nprediction without convolutions. arXiv preprint arXiv:2102.12122 .\nWang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., 2004. Image quality\nassessment: from error visibility to structural similarity. IEEE transactions\non image processing 13, 600–612.\nWolberg, G., Zokai, S., 2000. Robust image registration using log-polar trans-\nform, in: Proceedings 2000 International Conference on Image Processing\n(Cat. No. 00CH37101), IEEE. pp. 493–496.\nXie, Y ., Zhang, J., Shen, C., Xia, Y ., 2021. Cotr: E ﬃciently bridging\ncnn and transformer for 3d medical image segmentation. arXiv preprint\narXiv:2103.03024 .\nXu, Z., Luo, J., Lu, D., Yan, J., Frisken, S., Jagadeesan, J., Wells III, W., Li, X.,\nZheng, Y ., Tong, R., 2022. Double-uncertainty guided spatial and tempo-\nral consistency regularization weighting for learning-based abdominal reg-\nistration, in: International Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer.\nYang, X., Kwitt, R., Niethammer, M., 2016. Fast predictive image registration,\nin: Deep Learning and Data Labeling for Medical Applications. Springer,\npp. 48–57.\nYang, X., Kwitt, R., Styner, M., Niethammer, M., 2017a. Fast predictive multi-\nmodal image registration, in: 2017 IEEE 14th International Symposium on\nBiomedical Imaging (ISBI 2017), IEEE. pp. 858–862.\nYang, X., Kwitt, R., Styner, M., Niethammer, M., 2017b. Quicksilver: Fast\npredictive image registration–a deep learning approach. NeuroImage 158,\n378–396.\nZeiler, M.D., Ranzato, M., Monga, R., Mao, M., Yang, K., Le, Q.V ., Nguyen, P.,\nSenior, A., Vanhoucke, V ., Dean, J., et al., 2013. On rectiﬁed linear units for\nspeech processing, in: 2013 IEEE International Conference on Acoustics,\nSpeech and Signal Processing, IEEE. pp. 3517–3521.\nZhai, X., Kolesnikov, A., Houlsby, N., Beyer, L., 2022. Scaling vision trans-\nformers, in: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 12104–12113.\nZhang, J., 2018. Inverse-consistent deep networks for unsupervised deformable\nimage registration. arXiv preprint arXiv:1809.03443 .\nZhang, Y ., Ma, J., Iyengar, P., Zhong, Y ., Wang, J., 2017. A new ct reconstruc-\ntion technique using adaptive deformation recovery and intensity correction\n(adric). Medical physics 44, 2223–2241.\nZhang, Z., Yu, L., Liang, X., Zhao, W., Xing, L., 2021. Transct: Dual-\npath transformer for low dose computed tomography. arXiv preprint\narXiv:2103.00634 .\nZhou, H.Y ., Guo, J., Zhang, Y ., Yu, L., Wang, L., Yu, Y ., 2021. nnformer:\nInterleaved transformer for volumetric segmentation. arXiv:2109.03201.\nZhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J., 2019. Unet ++: Re-\ndesigning skip connections to exploit multiscale features in image segmen-\ntation. IEEE transactions on medical imaging 39, 1856–1867.\nZhu, B., Liu, J.Z., Cauley, S.F., Rosen, B.R., Rosen, M.S., 2018. Image recon-\nstruction by domain-transform manifold learning. Nature 555, 487–492.",
  "topic": "Image registration",
  "concepts": [
    {
      "name": "Image registration",
      "score": 0.7350839376449585
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7005501985549927
    },
    {
      "name": "Computer science",
      "score": 0.6571058034896851
    },
    {
      "name": "Transformer",
      "score": 0.5858277082443237
    },
    {
      "name": "Computer vision",
      "score": 0.557326078414917
    },
    {
      "name": "Medical imaging",
      "score": 0.5430275201797485
    },
    {
      "name": "Imaging phantom",
      "score": 0.4678564667701721
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4256266951560974
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4127904176712036
    },
    {
      "name": "Image (mathematics)",
      "score": 0.22843822836875916
    },
    {
      "name": "Engineering",
      "score": 0.09856840968132019
    },
    {
      "name": "Medicine",
      "score": 0.08577963709831238
    },
    {
      "name": "Voltage",
      "score": 0.07511082291603088
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Radiology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1304085615",
      "name": "Nvidia (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I170897317",
      "name": "Duke University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    }
  ]
}