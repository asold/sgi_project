{
  "title": "bert2BERT: Towards Reusable Pretrained Language Models",
  "url": "https://openalex.org/W3205544360",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2095817051",
      "name": "Cheng Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2163935963",
      "name": "Yichun Yin",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2151842933",
      "name": "Lifeng Shang",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2097242334",
      "name": "Xin Jiang",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2098243531",
      "name": "Yujia Qin",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2112556134",
      "name": "Feng-Yu Wang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2096307556",
      "name": "Zhi Wang",
      "affiliations": [
        "Tsinghuaâ€“Berkeley Shenzhen Institute",
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1975751340",
      "name": "Xiao Chen",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3101016419",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3202316938",
    "https://openalex.org/W2979055841",
    "https://openalex.org/W2963628712",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3164896303",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3170925726",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W2991140127",
    "https://openalex.org/W2754526845",
    "https://openalex.org/W3101845875",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3099911888",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3089362500",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W3108384060",
    "https://openalex.org/W4303939357",
    "https://openalex.org/W3097132740",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3129418925",
    "https://openalex.org/W3123799706",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W2979450969",
    "https://openalex.org/W3012776236",
    "https://openalex.org/W2945667196",
    "https://openalex.org/W2165698076"
  ],
  "abstract": "Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, Qun Liu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2134 - 2148\nMay 22-27, 2022câƒ2022 Association for Computational Linguistics\nbert2BERT: Towards Reusable Pretrained Language Models\nCheng Chen1â€ , Yichun Yin2, Lifeng Shang2, Xin Jiang2, Yujia Qin1,\nFengyu Wang1, Zhi Wang3,4â€¡, Xiao Chen2, Zhiyuan Liu1, Qun Liu2\n1Department of Computer Science and Technology, Tsinghua University\n2Huawei Noahâ€™s Ark Lab, 3Tsinghua Shenzhen International Graduate School\n4Peng Cheng Laboratory\n{c-chen19,qyj20,wangfy20}@mails.tsinghua.edu.cn\n{yinyichun,shang.lifeng,jiang.xin,chen.xiao2,qun.liu}@huawei.com\nwangzhi@sz.tsinghua.edu.cn,liuzy@tsinghua.edu.cn\nAbstract\nIn recent years, researchers tend to pre-train\never-larger language models to explore the up-\nper limit of deep models. However, large lan-\nguage model pre-training costs intensive com-\nputational resources, and most of the models\nare trained from scratch without reusing the\nexisting pre-trained models, which is wasteful.\nIn this paper, we propose bert2BERT1, which\ncan effectively transfer the knowledge of an\nexisting smaller pre-trained model to a large\nmodel through parameter initialization and sig-\nnificantly improve the pre-training efficiency of\nthe large model. Specifically, we extend the pre-\nvious function-preserving (Chen et al., 2016)\nmethod proposed in computer vision on the\nTransformer-based language model, and fur-\nther improve it by proposing a novel method,\nadvanced knowledge for the large modelâ€™s ini-\ntialization. In addition, a two-stage learning\nmethod is proposed to further accelerate the\npre-training. We conduct extensive experi-\nments on representative PLMs (e.g., BERT and\nGPT) and demonstrate that (1) our method can\nsave a significant amount of training cost com-\npared with baselines including learning from\nscratch, StackBERT (Gong et al., 2019) and\nMSLT (Yang et al., 2020); (2) our method is\ngeneric and applicable to different types of pre-\ntrained models. In particular, bert2BERT saves\nabout 45% and 47% computational cost of pre-\ntraining BERTBASE and GPTBASE by reusing\nthe models of almost their half sizes.\n1 Introduction\nPre-trained language models (PLMs), such as\nBERT (Devlin et al., 2019), GPT (Radford et al.,\n2018, 2019; Brown et al., 2020), ELECTRA (Clark\net al., 2020), XLNet (Yang et al., 2019) and\nRoBERTa (Liu et al., 2019), have achieved great\nâ€  This work is done when Cheng Chen is an intern at\nHuawei Noahâ€™s Ark Lab.\nâ€¡ Corresponding author.\n1Our code is available at https://github.com/\nhuawei-noah/Pretrained-Language-Model .\n0 1 2 3 4 5 6 7 8\nFLOPs (1e19)\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6MLM Loss\n4 5 6 7\n1.40\n1.42\n1.44\n1.46\n1.48\n1.437\n100%75.7%54.8%\nBERTBASE\nStackBERT\nbert2BERT\nFigure 1: Loss curves of bert2BERT and baselines.\nStackBERT (Gong et al., 2019) is based on the pro-\ngressive training setting. More details are shown in\nTable 2.\nsuccess in natural language processing (NLP).\nHowever, the pre-training process of large PLMs\ncan be extremely computationally expensive and\nproduces huge carbon footprints. For example,\nGPT-3 uses 3.1E+6 GPU hours for training, at an\nestimated cost of $4.6 million 2, consuming a lot\nof computing resources. Therefore, how to reduce\nthe training cost of PLM is of great importance to\nGreen AI (Schwartz et al., 2020).\nRecently, there is a trend of training extremely\nlarge models to explore the upper limits of PLMs.\nFor example, large pre-trained models, includ-\ning GPT-3 (Brown et al., 2020) (175B), PanGu-\nÎ± (Zeng et al., 2021) (200B) and Switch Transform-\ners (Fedus et al., 2021) (1571B), have been proved\npromising in language understanding and gener-\nation. However, these models are all pre-trained\nfrom scratch independently without utilizing the\nknowledge of smaller ones that have already been\ntrained. On the other hand, our empirical studies\nshow that the pre-trained models of different scales\ncould share similar knowledge, for example in Fig-\nure 2, the attention patterns of the two PLMs with\ndifferent sizes are similar.\nTo save the training cost of large models, we\n2https://lambdalabs.com/blog/\ndemystifying-gpt-3/\n2134\nL2 H1\n L2 H2\n L2 H4\n L2 H5\n L12 H4\n L12 H5\n L12 H10\n L12 H11\nL2 H8\n L2 H2\n L2 H4\n L2 H5\n L12 H3\n L12 H8\n L12 H5\n L12 H7\nFigure 2: The comparisons of attention patterns between small and large PLMs. The upper ones are the attention\npatterns of BERTBASE model whose architecture is {L=12, D=768}, and the lower ones are the attention patterns\nof one small BERT model whose architecture is {L=12, D=512}. We find that there are a large number of similar\nattention patterns in the same layer of the two models, indicating the possibility of reusing parameters of trained\nsmall PLMs to speed up the pre-training of large PLMs. The attention maps of PLMs with different layers are also\nsimilar, which is visualized in previous work (Gong et al., 2019; Yang et al., 2020).\npropose the bert2BERT method, which can ef-\nficiently transfer the learned knowledge of the\nsmaller model to the large model. bert2BERT con-\nsists of two components: (1) For parameter ini-\ntialization, we first extend the function preserving\ntraining (Chen et al., 2016) to PLMs by duplicating\nand stacking the parameters of the existing smaller\nPLM, which we call function-preserving initializa-\ntion (FPI). FPI ensures that the initialized large\nmodel has almost the same behavior as the small\nmodel, so that the large model has a good starting\npoint for later optimization. We also find that dupli-\ncating the weights of the upper layer to the current\nlayer can further accelerate the convergence of the\nlarge model, which we call advanced knowledge\ninitialization (AKI). Although the AKI somewhat\nviolates the principle of function preserving, we\nfind that empirically it also has a good starting\npoint as shown in Table 1, which leads to a faster\nconvergence rate and achieves higher training effi-\nciency. (2) Secondly, a two-stage training strategy\nis further applied to the large model to accelerate\nthe training process.\nTo demonstrate the superiority of our method,\nwe conduct extensive experiments on two repre-\nsentative PLMs: BERT and GPT, with different\nsource model sizes. The results show that: (1) our\nmethod can save a significant amount of computa-\ntion in pre-training compared to the traditional way\nof learning from scratch and progressive stacking\nmethods such as StackBERT (Gong et al., 2019)\nand MSLT (Yang et al., 2020); (2) our method is\nmodel-agnostic, which can be applied on a wide\nrange of Transformer-based PLMs. One typical\nexample is that, when using a small pre-trained\nmodel with half the size of BERTBASE for initial-\nization, bert2BERT saves 45% computation cost\nof the original BERTBASE pre-training.\nIn general, our contributions are summarized\nas follows: (1) We explore a new direction for\nthe efficient pre-training by reusing the trained pa-\nrameters of small models to initialize the large\nmodel; (2) We successfully extend function pre-\nserving method (Chen et al., 2016) on BERT and\nfurther propose advanced knowledge initialization,\nwhich can effectively transfer the knowledge of\nthe trained small model to the big model and\nimprove the pre-training efficiency; (3) The pro-\nposed method outperforms other training meth-\nods and achieves 45% computation reduction on\nBERTBASE; (4) Our method is generic, effective\nfor both the BERT and GPT models, and have great\npotential to become an energy-efficient solution for\npre-training super large-scale language models.\n2 Related Work\nEfficient Pre-training in NLP.The efficiency of\npre-training has been explored by previous work.\nSome works (Gong et al., 2019; Yang et al., 2020;\nGu et al., 2021) propose progressive learning to\naccelerate the pre-training, which are motivated\nby the fact that different layers have some simi-\nlar knowledge (e.g., attention patterns). They start\npre-training a small model with fewer Transformer\nlayers, and then iteratively expand the model by\nstacking the already trained layers on the top. An-\nother line of work proposes to â€œback distillâ€ the\nknowledge of the small models into large models,\nwhich is termed as knowledge inheritance (Qin\net al., 2021). Some works focus on the data effi-\n2135\nciency (Wu et al., 2021) and take notes for rare\nwords during the pre-training process to help the\nmodel understand them when they occur next.\nELECTRA (Clark et al., 2020) proposes a task\nof replaced token detection to predict whether each\ntoken in the input was replaced or not, which im-\nproves the pre-training efficiency. Our method is\northogonal to this kind of work and the combina-\ntion of ELECTRA and bert2BERT could achieve\nbetter efficiency. In addition, there are several other\northogonal techniques for efficient pre-training:\nmixed-precision training (Shoeybi et al., 2019),\nlarge batch optimization (You et al., 2020), model\narchitecture innovation (Lan et al., 2020), layer\ndropping technique (Zhang and He, 2020), etc.\nReusable Neural Network. Reusable neural net-\nwork, a topic related to transfer learning (Pan and\nYang, 2010), is introduced to accelerate the model\ntraining in computer vision. One classical work is\nNet2Net (Chen et al., 2016), which first proposes\nthe concept of the function-preserving transforma-\ntion to make neural networks reusable. However,\nNet2Net randomly selects the neurons to be split.\nTo handle this problem, some works (Wu et al.,\n2019, 2020b; Wang et al., 2019b; Wu et al., 2020a)\nleverage a functional steepest descent idea to de-\ncide the optimal subset of neurons to be split. The\npruning technique (Han et al., 2015) is also in-\ntroduced for reusable neural networks (Feng and\nPanda, 2020). In this paper, we study the reusable\npre-trained language model and propose a new\nmethod, bert2BERT, to accelerate the pre-training\nof BERT and GPT.\n3 Preliminary\nBERT consists of one embedding layer and multi-\nple Transformer (Vaswani et al., 2017) layers.\n3.1 Embedding Layer\nThe embedding layer first maps the tokens in a\nsentence into vectors with an embedding matrix\nWE. Then one normalization layer is employed to\nproduce the initial hidden states H0.\n3.2 Transformer Layer\nThe hidden states are iteratively processed by mul-\ntiple Transformer layers as follows:\nHl = Transformerl(Hlâˆ’1), lâˆˆ [1, L] (1)\nwhere L denotes the number of Transformer layers,\neach including a multi-head attention (MHA) and\na feed-forward network (FFN).\nMHA. It is composed of multiple parallel self-\nattention heads. The hidden states of the previous\nlayer are fed into each head and then the outputs of\nall heads are summed to obtain the final output as\nfollows:\nQi, Ki, Vi = Hlâˆ’1WQ\nl,i, Hlâˆ’1WK\nl,i , Hlâˆ’1WV\nl,i ,\nHHEAD\nl,i = softmax(QiKiT\nâˆšdk\n)ViWO\nl,i,\nMHA(Hlâˆ’1) =\naX\ni=1\nHHEAD\nl,i ,\nHMHA\nl = LayerNorm(Hlâˆ’1 + MHA(Hlâˆ’1)).\n(2)\nHlâˆ’1 is linearly projected to queries ( Qi), keys\n(Ki) and values ( Vi) using WQ\nl,i, WK\nl,i , WV\nl,i re-\nspectively. HHEAD\nl,i indicates the context-aware\nvector which is obtained by the scaled dot-product\nof queries and keys in the i-th attention head. a\nrepresents the number of self-attention heads. dk\nis the head dimension acting as the scaling factor.\nFFN. It consists of two linear layers and one\nGeLU activation function (Hendrycks and Gimpel,\n2016), that is:\nHFFN\nl = GeLU(HMHA\nl W1\nl + b1\nl )W2\nl + b2\nl ,\nHl = LayerNorm(HMHA\nl + HFFN\nl ).\n(3)\nLayer Normalization. Both the modules of\nMHA and FFN have one layer normalization (Ba\net al., 2016) that stabilizes the dynamics of the hid-\nden state in the Transformer. Formally, it is written\nas:\nLayerNorm(H) = (H âˆ’ ÂµH\nÏƒH\n) âŠ™ WLN + bLN ,\n(4)\nwhere âŠ™ means the element-wise multiplication.\nThe statistics of ÂµH and ÏƒH are the mean and\nvariance of hidden states H respectively.\n4 Methodology\n4.1 Problem Statement\nWe aim to accelerate the pre-training of target\nmodel T (Lt, Dt) by transferring the knowledge\nof an existing pre-trained source model S(Ls, Ds),\nwhere Ls|t means the numbers of Transformer\nlayer and Ds|t means the model width (i.e., hidden\nsize), satisfying Ls â‰¤ Lt and Ds â‰¤ Dt. Formally,\nour problem is two-fold: (1) how to perform an ef-\nfective parameter initialization forT by reusing the\ntrained parameters of S, and (2) how to efficiently\n2136\ntrain the initialized T , so that T can have a faster\nconvergence rate in pre-training.\n4.2 Overview\nTargeting the above problems, bert2BERT first ini-\ntializes the target model T with the parameters of\nthe existing model S by the width-wise expansion\n(Ds â†’ Dt) and depth-wise expansion (Ls â†’ Lt).\nThrough this expansion, the knowledge contained\nin the parameters of the source model is directly\ntransferred to the target model. Then we further\npre-train the initialized target model with a two-\nstage pre-training method. The overall workflow is\nillustrated in Section 4.5.\nEssentially, the width-wise expansion can be de-\ncomposed into expansions of parameter matrices\n(or vectors3). As illustrated in Figure 3, the ma-\ntrix expansion enlarges W âˆˆ R dw\ninâˆ—dw\nout of S to\nU âˆˆ R du\ninâˆ—du\nout of T by two kinds of operations:\nin-dimension and out-dimension expansion.\nIn the following sections, we first introduce\ntwo strategies of width-wise expansion: function-\npreserving and advanced knowledge initialization.\nThen, we introduce the depth-wise expansion and\ndetail the two-stage pre-training process.\n4.3 Width-wise Expansion\nFor the paper clarity, we introduce two index map-\nping functions: gin and gout, where gin(i) means\nthe i-th in-dimension of U reuses the gin(i)-th in-\ndimension parameters of W, gout(j) means the\nj-th out-dimension of U reuses the gout(j)-th out-\ndimension parameters of W. Both our two meth-\nods are defined with these two mapping functions.\nW(i,j) means the parameter element, i and j re-\nfer to the i-th in-dimension index and j-th out-\ndimension index respectively. As shown in Fig-\nure 3, the i-th in-dimension parameters of W are\nthe parameters of the i-th input neuron of W or the\ni-th column of W.\n4.3.1 Function Preserving Initialization\nFunction preserving initialization (FPI) (Chen et al.,\n2016) aims to make the initialized target model\nhave the same function as the source model, which\nmeans that given the same input, the initialized tar-\nget model has the same output as the source model.\nIn this paper, we extend FPI on a different archi-\ntecture, Transformer-based pre-trained language\nmodel. We give an example in Figure 3 to illustrate\n3We omit the expansion of bias (vector) for simplicity. It\nfollows a similar process as the matrix expansion.\nh1\nğ‘‘out\nğ‘¤\nğ‘‘in\nğ‘¤\nâ¶ â·h2\nx1 x2\ny1 y2\nğ‘œ\nh1 h2\nx1 x2\ny1 y2\nx1\nh1 h2\nx1 x2\ny1 y2\nx1\nh2\nğ‘”in\n{1:1,2:2,ğŸ‘:ğŸ}\nğ‘”out\n{1:1,2:2,ğŸ‘:ğŸ}ğ‘\nğ‘ ğ‘Ÿ\nğ‘œ\nğ‘ğ‘\nğ‘Ÿ\nğ‘\nğ‘ ğ‘\nğ‘‘\nğ‘œ\n2\nğ‘\n2\nğ‘\n2\nğ‘œ\n2\nğ‘\n2\nğ‘Ÿ\nğ‘\n2\nğ‘œ\n2\nğ‘\n2 ğ‘Ÿ\nğ‘ ğ‘œ\n2ğ‘\n2\nğ‘\n2 ğ‘Ÿ ğ‘\n2\nğ‘œ\n2 ğ‘ ğ‘œ\n2\nğ‘\n2 ğ‘Ÿ ğ‘\n2\nğ‘‘in\nğ‘¢ ğ‘‘in\nğ‘¢\nğ‘‘out\nğ‘¢\nğ‘\n2\nğ‘\n2\nğ‘‘\n2\nğ‘‘\n2 â¸\nCOPY & RE-SCALE\nCOPY\nğ‘¾ ğ‘¼\nğ‘¼~\nChange. 20211112\nğ‘\n2 ğ‘Ÿ\nğ‘\n2\nFigure 3: Overview of the function preserving initializa-\ntion (FPI). Given the same input {x1, x2}, FPI ensures\nthe initialized target model has the same output {y1, y2}\nwith the source model. The first and the second steps\nare expanding the in-dimension and out-dimension of\nthe parameter matrix according to mapping functions\ngin and gout respectively. After we expand the matrix\nW into U, we use the in-dimension expansion on the\nupper parameter matrix again to ensure the output {y1,\ny2} same as the original one. From the view of neurons,\nFPI copies the corresponding input and output neurons\nto expand the neural network.\nFPI. Formally, the mapping functions are defined\nas follows:\ngin(i) =\n(\ni i âˆˆ [1, dw\nin]\nf({1, 2, ..., dw\nin}) i âˆˆ (dw\nin, du\nin], (5)\ngout(j) =\n(\nj j âˆˆ [1, dw\nout]\nf({1, 2, ..., dw\nout}) j âˆˆ (dw\nout, du\nout],\n(6)\nwhere f(Â·) is uniform sampling. We denote the\nweight expansion as U = EXPN(W; gin, gout),\nwhich includes in-dimension expansion (Eq. 7) and\nout-dimension expansion (Eq. 8):\nCgin(i) =\ndu\ninX\niâ€²=1\nI (gin(iâ€²) =gin(i))\neU(i,âˆ—) = 1\nCgin(i)\nW(gin(i),âˆ—),\n(7)\nU(âˆ—,j) = eU(âˆ—,gout(j)), (8)\nwhere I (Â·) is an indicator function, and Cgin(i) is\nthe count of gin(i) in the values of gin(Â·), which is\nused to re-scale the original parameters to keep the\nfunction preserving property.\nExpansion for All Modules. We apply FPI\nfor all modules of BERT via matrix expansion\nEXPN(Â·). Specifically, for the embedding matrix\nWE, we only conduct the out-dimension expan-\nsion:\nUE\n(âˆ—,j) = WE\n(âˆ—,ge\nout(j)). (9)\nMHA module can be decomposed into multiple\nparallel self-attention heads and we conduct the\nhead-wise expansion for this module, which means\n2137\nincreasing the number of attention heads. The head-\nwise expansion is formulated as:\nUQ|K|V |O = EXPN(WQ|K|V |O; gq|k|v|o\nin , gq|k|v|o\nout ).\n(10)\nSpecifically, the head-wise expansion means that\nwe reuse the head group parameters to construct\nthe new matrices. The i-th head group in l-th layer\ncontains WQ\nl,i|WK\nl,i |WV\nl,i |WO\nl,i in Eq. 2 and the out-\ndimension expansion for WQ\nl,i|WK\nl,i |WV\nl,i is:\ngq|k|v\nout (j) =\n(\nj j âˆˆ [1, as]\nf({1, 2, ..., as}) j âˆˆ (as, at],\n(11)\nwhere j is the head index and as|t mean the head\nnumbers of source model and target model re-\nspectively. The module has three constraints: {\nge\nout = gq|k|v\nin ; gq|k|v\nout = go\nin; gq|k|v\nin = go\nout}, with\nthe first two constraints for hidden dimension con-\nsistency (Wen et al., 2018; Chen et al., 2021) and\nthe third one for residual connection (Eq. 2).\nFor the FFN module, we perform the expansion\non the parameter matrices W1|2 (Eq. 3) as follows:\nU1|2 = EXPN(W1|2; g1|2\nin , g1|2\nout). (12)\nSimilar to the MHA module, the mapping functions\nof FFN also have three constraints: { go\nout = g1\nin;\ng1\nout = g2\nin; g1\nin = g2\nout}.\nFor the layer normalization, we take the layer\nnormalization of FFN as an example, its expansion\nis formulated as:\nULN\nj = WLN\ng2\nout(j). (13)\nNote that in layer normalization (Eq. 4), the meanÂµ\nand variance Ïƒ are calculated based on the hidden\nrepresentations H. Thus, the expansion of this\nparameter inevitably induces a gap and prevents the\ntarget model from strictly following the function\npreserving principle. However, we empirically find\nthat the gap is so small that it can hardly affect the\ninitialization and convergence of the target model.\nThus we ignore this discrepancy.\nWe have validated the effectiveness of the\nadapted FPI in different settings in Table 1. The\nresults show that the initialized model T achieves\nalmost the same loss as S, demonstrating that FPI\nsuccessfully retains the knowledge of the small\nmodel when performing parameter expansion.\n4.3.2 Advanced Knowledge Initialization\nTo further improve the convergence rate of the pre-\ntraining target model, we propose the advanced\nknowledge initialization (AKI), which expands new\nMethod S(12,384) S(12,512)\nOriginal 1.89 1.67\nRand 10.40 10.42\nDirectCopy 9.05 6.45\nFPI 1.89 1.70\nAKI 2.08 1.96\nTable 1: The comparison of MLM losses between FPI\nand baselines. â€œOriginalâ€ refers to the MLM losses of\nsource pre-trained models S. â€œRandâ€ refers to the MLM\nlosses of randomly initialized target models. â€œDirect-\nCopyâ€ refers to a naive method that directly copies the\nsource model to the target model and the unfilled part\nis randomly initialized, â€œFPIâ€ represents the function\npreserving method. We expand both models to the target\nmodel T (12, 768) and find that FPI can make the target\nmodel have similar losses with these trained source mod-\nels. The loss gap between FPI and Original is brought\nby layer normalization. â€œAKIâ€ represents the advanced\nknowledge initialization method.\nmatrices based on not only the parameters of the\nsame layer but also the parameters of the upper\nlayer in the source model. The intuition is based\non previous findings (Jawahar et al., 2019; Clark\net al., 2019) that adjacent Transformer layers have\nsimilar functionality, which ensures that it will not\ndamage the knowledge contained in the parameters\nof the current layer. Moreover, the knowledge that\ncomes from adjacent layers can break the symme-\ntry (Chen et al., 2016) appeared in FPI, which has\nbeen demonstrated beneficial. We give an illus-\ntrative example in Figure 4 and formulate AKI as:\nUl = EXPN(Wl, Wl+1; gl|l+1\nin , gl\nout). (14)\nSpecifically, we first do the in-dimension expansion\nfor Wl|l+1. Here we take Wl as an example:\nCgl\nin(i) =\ndu\ninX\niâ€²=1\nI (gl\nin(iâ€²) =gl\nin(i))\neUl\n(i,âˆ—) = 1\nCgl\nin(i)\nWl\n(gl\nin(i),âˆ—).\n(15)\nIt is similar with Eq. 7. Then we stack the ex-\npanded matrices of eUl and eUl+1 to construct the\nfinal matrix:\nUl\n(âˆ—,j) =\n(eUl\n(âˆ—,j) j âˆˆ [1, dw\nout]\neUl+1\n(âˆ—,gl\nout(j)) j âˆˆ (dw\nout, du\nout]. (16)\nWe directly copy the expanded eUl as the top part\nof the new matrix and place the sampled parameters\nfrom eUl+1 on the bottom of the new matrix.\nWe aggregate upper-layer information into a new\nmatrix for two intuitions: (1) it breaks the FPI sym-\nmetry that hinders model convergence (Chen et al.,\n2138\nğ‘œ ğ‘\nğ‘ ğ‘Ÿ\nğ‘ ğ‘\nğ‘ ğ‘‘\nğ‘”in\nğ‘™\n{1:1,2:2,ğŸ‘:ğŸ}\nğ‘œ\n2\nğ‘\n2 ğ‘Ÿ\nğ‘ ğ‘œ\n2ğ‘\n2\nğ‘\n2ğ‘‘\n2ğ‘\nğ‘ ğ‘\n2ğ‘‘\n2\nğ‘”in\nğ‘™+1\n{1:1,2:2,ğŸ‘:ğŸ}\nğ‘œ\n2\nğ‘\n2 ğ‘Ÿ\nğ‘ ğ‘œ\n2ğ‘\n2\nğ‘”out\nğ‘™\n{1:1,2:2,ğŸ‘:ğŸ}\nğ‘‘\n2ğ‘ ğ‘‘\n2â¶ â·\nCheng 20211113\nCOPY\nğ‘‘in\nğ‘¤\nğ‘‘out\nğ‘¤\nğ‘‘in\nğ‘¢ ğ‘‘in\nğ‘¢\nğ‘‘out\nğ‘¢\nğ‘¾ğ‘™+1\nğ‘¾ğ‘™ à·©ğ‘¼ğ‘™\nà·©ğ‘¼ğ‘™+1\nğ‘¼ğ‘™\nFigure 4: Overview of AKI. It first performs the in-\ndimension expansion on both the matrixes of current\nand upper layers. Then it uses the widened matrix of\nthe current layer as the top part of the new matrix and\nsamples the row of the widened matrix of the upper\nlayer as the bottom part of the new matrix.\n2016). For example, FPI makes the attention pat-\nterns in the same layer repeated, which is redundant\nand called symmetry; (2) upper-layer information\ncan be used as similar but high-level knowledge to\nguide the model to converge faster. We display the\nattention patterns of the target model initialized by\nAKI in Appendix E and find that the target model\ncan maintain the attention patterns of both current\nand upper layers very well.\nExpansion for All Modules. For embedding ma-\ntrix, we only do the out-dimension expansion as\nEq. 9 in the FPI. Both the modules of MHA and\nFFN do the matrix expansion by following the de-\nfined operation in Eq. 15 and Eq. 16. The con-\nstraints of mapping functions follow the setting of\nFPI.\nEmpirically, we find that the AKI method out-\nperforms FPI, while the performance is worse if we\nbuild a new matrix based on the matrix of the lower\nlayer (or low-level knowledge). How to construct\nthe optimal initialization for the target model with\nthe parameters of different layers remains an open\nquestion and we leave it as future work.\nFor more details, we give a clear illustration of\nthe FPI and AKI process in Appendix F.\n4.4 Depth-wise Expansion\nAfter the width-wise expansion, we obtain a\nwidened model with the same width as the target\nmodel. To bridge the depth gap, we perform depth-\nwise expansion to increase model depth to the depth\nof the target model. We illustrate this process in\nAlgorithm 1 and the main idea is to iteratively stack\nthe widened model until its depth is equal to the\ntarget model (Gong et al., 2019).\n4.5 Two-stage Pre-training\nTo further improve the pre-training efficiency of ini-\ntialized target model, we propose a two-stage train-\ning method: (1) train sub-models with different\nAlgorithm 1 Target Model Initialization\nInput: the target model T (Lt, Dt) and the source\nmodel S(Ls, Ds).\n1: T1(Ls, Dt) â† do AKI or FPI with S(Ls, Ds)\n2: k â† âŒŠLt/LsâŒ‹\n3: for t = 2â†’ k do\n4: Tt(Ls Â· t, Dt) â† stack T1 on top of Ttâˆ’1\n5: end for\n6: T â†stack top Lt âˆ’ Ls Â· k layers of T1.\nOutput: the initialized model T (Lt, Dt)\nAlgorithm 2 Two-stage Pre-training\nInput: the initialized model T , large-scale unsu-\npervised dataset D, the epoch number of sub-\nmodel training Eb and the epoch number of\nwhole training process E, the layer number lb.\n1: Construct sub-models and these models have\nthe layer numbers of {lb, 2 Â· lb, . . . , Lt}.\n2: for e = 1â†’ Eb do\n3: for batch in D do\n4: T â€² â† sample one sub-model.\n5: Perform forward and backward of T â€².\n6: Update only top lb layers of T â€².\n7: end for\n8: end for\n9: for e = Eb â†’ E do\n10: for batch in D do\n11: Perform forward and backward of T .\n12: Update whole model T .\n13: end for\n14: end for\nOutput: the pre-trained model T\nlayers in a random manner to make the complete\nmodel converge at a low cost. These sub-models\nare built with bottom Transformer layers of the ini-\ntialized target model and share one classification\nlayer. At each optimization step, we randomly sam-\nple one sub-model and only update its top Trans-\nformer layers and the shared classification layer.\n(2) After the sub-structure training, we further per-\nform the traditional full-model training. The details\nof our method are displayed in Algorithm 2.\n5 Experiment\n5.1 Experimental Setup\nPre-training Details. We use the English\nWikipedia and Toronto Book Corpus (Zhu et al.,\n2015) as the pre-training data. The settings of pre-\ntraining are: peak learning rate of 1e-4, warmup\n2139\nModel FLOPs Ratio Loss SQuADv1.1 SST-2 MNLI MRPC CoLA QNLI QQP STS-BAvg.(Ã—1e19) (Saving) (MLM)(F1) (Acc) (Acc) (Acc) (Mcc) (Acc) (Acc) (Acc)\nBERTBASE(Google) - - - 88.4(0.1) 93.6(0.2) 84.7(0.1) 87.9(0.9) 59.6(1.5) 91.6(0.1) 91.4(0.1) 89.6(0.5)85.8(0.1)BERTBASEâ€ (Ours) 7.3 0% 1.437 89.6(0.1) 92.7(0.2) 84.6(0.2) 88.6(0.5) 57.3(4.0) 90.6(0.7) 90.6(0.1) 89.9(0.3)85.5(0.5)\nProgressive Training\nMSLTâ€  6.5 10.7% 1.436 90.4(0.2) 92.9(0.2) 85.1(0.2) 87.9(2.1) 55.6(4.1) 90.7(0.2) 90.6(0.2) 88.2(0.6)85.2(0.7)StackBERTâ€  5.5 24.3% 1.433 90.4(0.2) 92.6(0.4) 85.3(0.1) 88.2(1.0) 63.2(0.9) 91.0(0.4) 91.0(0.1) 86.7(0.7)86.0(0.2)\nbert2BERT:S(12, 512)â†’ T(12, 768)\nDirectCopy 6.4 12.2% 1.436 89.8(0.2) 92.9(0.3) 84.7(0.2) 86.2(0.6) 62.2(0.7) 90.2(0.6) 90.4(0.1) 89.2(0.1)85.7(0.1)FPI 5.1 30.4% 1.436 90.0(0.2) 92.6(0.4) 85.2(0.1) 87.1(0.5) 61.5(0.9) 90.9(0.6) 90.8(0.2) 89.7(0.2)86.0(0.1)AKI 4.5 38.4% 1.434 90.4(0.1) 92.5(0.4) 85.3(0.4) 87.8(0.9) 61.0(1.4) 91.2(0.2) 90.5(0.1) 89.5(0.2)86.0(0.2)bert2BERT 4.0 45.2% 1.433 90.0(0.2) 92.9(0.1) 85.1(0.1) 87.7(0.7) 60.0(1.2) 90.5(0.8) 90.4(0.1) 89.2(0.2)85.7(0.4)\nTable 2: Comparison between bert2BERT and baselines. We report mean (and standard deviation) performance\nover 3 runs on the dev set. bert2BERT means the combination of AKI and two-stage pre-training here. FPI and\nAKI mean that the function preserving initialization, advanced knowledge initialization respectively. â€  means the\nre-implemented results, where the BERTBASE and StackBERT achieve similar results with the original paper, and\nthe MSLT result is different from the original paper may be due to the different training settings (e.g., in the original\npaper, it uses the LAMB optimizer (You et al., 2020) and only trains the corpus with a max sequence length of 128).\nsteps of 10k, training epochs of E=40, batch size\nof 512, sub-model training epochs of Eb=5, layer\nnumber of lb=3. Unless otherwise noted, all meth-\nods including bert2BERT and baselines use the\nsame pre-training settings for fair comparisons. In\nthe settings of bert2BERT, the target model has\na BERTBASE architecture of T (12, 768) and the\nsource model has an architecture of S(12, 512).\nFine-tuning Details. For the evaluation, we use\ntasks from GLUE benchmark (Wang et al., 2019a)\nand SQuADv1.1 (Rajpurkar et al., 2016). We re-\nport F1 for SQuADv1.1, Matthews correlation coef-\nficient (Mcc) for CoLA (Warstadt et al., 2019) and\naccuracy (Acc) for other tasks. For the GLUE tasks\nfine-tuning, we set the batch size to 32, choose the\nlearning rate from {5e-6, 1e-5, 2e-5, 3e-5} and\nepochs from {4, 5, 10}. For the SQuADv1.1 fine-\ntuning, we set the batch size to 16, the learning rate\nto 3e-5, and the number of training epochs to 4. All\nresults are the average of 3 runs on the dev set.\nBaselines. We first introduce a naive bert2BERT\nbaseline named DirectCopy, which directly copies\nthe small model to the target model and ran-\ndomly initializes the unfilled parameters. Stack-\nBERT (Gong et al., 2019) and MSLT (Yang et al.,\n2020) are also included as the baselines. Both of\nthem are trained in a progressive manner. Follow-\ning the original setting, for the StackBERT, we\nfirst train the 3-layer BERT for 5 epochs, stack\nit twice into a 6-layer BERT and then train it for\n7 epochs. In the final step, we stack the 6-layer\nmodel into BERTBASE and further train it with 28\nepochs. For MSLT, we first perform 4-stage train-\ning. In each stage, we add the top 3 layers of the\nmodel already trained to the top of the model and\nthen pre-train the new model by partially updating\nthe top 3 layers. Each stage of the partial training\nprocess has 8 epochs. Finally, we further perform\n20 full-model training epochs4 to achieve the same\nloss as BERTBASE trained from scratch. The base-\nlines are trained using the same optimizer, training\nsteps, and warmup steps as the bert2BERT.\n5.2 Results and Analysis\nWe demonstrate the effectiveness of the proposed\nmethod on the SQuAD and GLUE benchmark. The\nresults are shown in Table 2. We also represent the\nloss curves in Figure 1 and Appendix A. The results\nshow that: (1) DirectCopy only saves 12.2% com-\nputational costs, which indicates this naive method\nof directly copying the trained parameters of the\nsource model to the target model is not effective;\n(2) our proposed methods, FPI and AKI, achieve\nbetter performances than the baselines. Although\nAKI does not follow the function preserving, it has\na bigger loss than FPI at the start of training, AKI\nachieves a faster convergence rate by using the ad-\nvanced knowledge and breaking the symmetry; (3)\nby performing the two-stage pre-training on the tar-\nget model initialized by AKI, we can save 45.2%\ncomputational costs. Note that the total parameters\nof the source model are half of those of the target\nmodel (54M vs. 110M). The loss of bert2BERT in\nFigure 1 is high at the stage of sub-model training\nbecause it represents the average loss of all sub-\nmodels. We also compare the attention patterns of\nthe target models initialized by DirectCopy, FPI,\nand AKI. The attention patterns and their discus-\nsions are displayed in Appendix E.\n4We have tried the same setting as the original paper with\n8 epoch full-model running but it does not achieve the same\nloss with BERTBASE (1.511 vs. 1.437).\n2140\nbert2BERT with Smaller Source Model. We\nalso evaluate bert2BERT on different settings,\nwhere the source model S(6, 512), S(8, 512), S(10,\n512) are significantly smaller than the target model\n(35M | 42M | 48M vs. 110M). The results are\nshown in Table 3 and loss curves are displayed\nin Appendix B. We observe that DirectCopy for\nS(6, 512) achieves no efficiency improvement over\nthe original pre-training, which indicates that the\nsignificant size gap between the source and tar-\nget model greatly reduces the benefit of Direct-\nCopy methods. Compared with DirectCopy, our\nproposed method reduces the computation cost by\n23.3%, which again demonstrates the effectiveness\nof bert2BERT. The results show that the smaller\nthe size gap between the source model and target\nmodel, the greater the cost savings of bert2BERT.\nWe also note that it is more challenging to speed\nup the target model with a small source model S(6,\n512). We encourage future work to explore to trans-\nfer the knowledge from smaller source models to\nimprove the pre-training efficiency of the target\nmodel.\nSettings Model FLOPs Ratio Loss Avg.\n(Ã—1e19) (Saving) (MLM)\nS(6, 512)DirectCopy 7.3 0% 1.440 89.1\nbert2BERT 5.6 23.3% 1.435 89.3\nS(8, 512)bert2BERT 4.6 36.8% 1.435 89.2\nS(10, 512)bert2BERT 4.2 42.7% 1.434 89.1\nTable 3: bert2BERT with smaller source model. Avg\nmeans the average score of SST-2/MNLI/SQuADv1.1.\nEffect of Sub-model Training Epochs. Our\ntraining procedure includes two stages: sub-model\ntraining and full-model training. Here, we study the\neffect of the number of sub-model training epochs\nby performing bert2BERT on the different settings\nof Eb={0, 5, 10, 20}. The results are presented in\nTable 4 and the loss curves are displayed in Ap-\npendix C. We observe that our method achieves the\nbest efficiency when the epoch number is set to 5,\nwhile a larger or smaller epoch number will bring\na negative impact.\nModel FLOPs Ratio Loss Avg.\n(Ã—1e19) (Saving) (MLM)\nbert2BERT:S(12, 512)â†’ T(12, 768)\nbert2BERT (Eb = 0) 4.5 38.4% 1.434 89.4\nbert2BERT (Eb = 5) 4.0 45 .2% 1.433 89.3\nbert2BERT (Eb = 10) 4.1 43.9% 1.436 89.3\nbert2BERT (Eb = 20) 5.4 25.4% 1.448 89.1\nTable 4: Effect of sub-model training epochs. Avg\nmeans the average score of SST-2/MNLI/SQuADv1.1.\n5.3 Application on GPT\nDatasets. To demonstrate that our method is\ngeneric, following the BERT setting, we also use\nthe English Wikipedia and Book Corpus in the GPT-\ntraining. For the evaluation, we use the datasets of\nWikiText-2, PTB, and WikiText103 and evaluate\nthese models under the zero-shot setting without\nfine-tuning on the training set.\nImplementation Details. We use the architecture\nof {L=12, D=768} for the GPT target model, and\npre-train it with the learning rate of 1e-4, training\nepochs of 20. For bert2BERT, we use the source\nmodel with an architecture of { L=12, D=512},\ninitialize the target model with AKI, and pre-train\nit by the full-model training (Eb=0).\nResults and Analysis. We compare the original\npre-training method and bert2BERT, the results\nare shown in Table 5 and Appendix D. We ob-\nserve that the proposed method saves 47% compu-\ntation cost of GPT pre-training, exhibiting a sim-\nilar trend to BERT pre-training. Although GPT\nand BERT have different architectures (e.g., post-\nLN and pre-LN (Xiong et al., 2020)) and are pre-\ntrained with different tasks, bert2BERT saves a sig-\nnificant amount of training cost on both these two\nmodels, which shows that the proposed method is\ngeneric and is effective for different kinds of PLMs.\nModel FLOPs PTB WikiText-2 WikiText103\n(Ã—1e19) (w/o FT) (w/o FT) (w/o FT)\nbert2BERT:S(12, 512)â†’ T(12, 768)\nGPT 4.9 133.8 47.0 53.5\nbert2BERT2.6(47%â†“) 132.1 47.9 53.0\nTable 5: Experiments on GPT. We report the perplexity\nfor these tasks. â€œw/o FTâ€ means that the pre-trained\nmodel is directly evaluated on the test set without fine-\ntuning on the train set.\n5.4 Application on T5\nDatasets. To demonstrate that our method can\nbe used to train larger models, we use the Baidu\nWikipedia, Sougou Wikipedia, and Zhihu to train\nthe T5 model (Raffel et al., 2020). For the evalu-\nation, we use the dataset of the original Chinese\nnatural language inference task (OCNLI) (Hu et al.,\n2020).\nImplementation Details. Since the bert2BERT\nmethod is suitable for BERT and GPT, it can also\nbe used for the T5 model, which consists of an en-\ncoder and a decoder. The target T5 modelâ€™s archi-\ntecture is {Le=12, Ld=12, D=1024, A=16}, where\n2141\nLe and Ld means the numbers of encoder and de-\ncoder Transformer layers respectively, D means\nthe hidden size, A means the number of attention\nheads. We pre-train it with the learning rate of\n1e-4, batch size of 1024. For bert2BERT, we use\nthe source model with an architecture of {Le=12,\nLd=12, D=256, A=4}, initialize the target model\nwith FPI, and pre-train it by the full-model training\n(Eb=0). Note that the scale gap between the source\nmodel and the target model is over 10 times (31M\nvs. 360M), which is a challenging setting.\nResults and Analysis. We compare the original\npre-training method and bert2BERT method on the\nT5 model, the results are shown in Table 6. We ob-\nserve that the proposed method saves at least 25%\ncomputation cost of T5 pre-training. It demon-\nstrates the effectiveness of the method on larger\nmodels.\nModel FLOPs Loss OCNLI\n(Ã—1e20) (MLM) (Acc)\nbert2BERT:S(12, 12, 256, 4)â†’ T(12, 12, 1024, 16)\nT5 1.6 1.90 72.03\nbert2BERT1.2(25%â†“) 1.90 72.75\nTable 6: Experiments on the T5 model.\n6 Conclusion and Future Work\nThis paper proposes an efficient pre-training\nmethod, bert2BERT, which reuses the parameters\nof the small trained model as the initialization\nparameters of the large model. We employ the\nproposed method in BERT and GPT under differ-\nent settings of model sizes. The extensive results\nshow that bert2BERT is generic to Transformer-\nbased models and saves a significant amount of\ncomputation cost. Moreover, the detailed analy-\nsis shows that our techniques, function-preserving,\nadvanced knowledge initialization, and two-stage\npre-training, are all effective. In the future, we\nwill apply bert2BERT on training super large-\nscale language models (e.g., use the 10B source\nmodel to train the 100B target model) and extends\nits scope to other PLMs such as ELECTRA and\nBART (Lewis et al., 2020).\nAcknowledgements\nThis work is supported in part by NSFC (Grant No.\n61872215), and Shenzhen Science and Technology\nProgram (Grant No. RCYX20200714114523079).\nWe would like to thank Yifeng Liu, Binbin Deng,\nZiliang Yang, Jiaxin Shi for their support of this\nwork.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization. ArXiv preprint,\nabs/1607.06450.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nCheng Chen, Yichun Yin, Lifeng Shang, Zhi Wang, Xin\nJiang, Xiao Chen, and Qun Liu. 2021. Extract then\ndistill: Efficient and effective task-agnostic BERT\ndistillation. In Artificial Neural Networks and Ma-\nchine Learning - ICANN 2021 - 30th International\nConference on Artificial Neural Networks, Bratislava,\nSlovakia, September 14-17, 2021, Proceedings, Part\nIII, volume 12893 of Lecture Notes in Computer Sci-\nence, pages 570â€“581. Springer.\nTianqi Chen, Ian J. Goodfellow, and Jonathon Shlens.\n2016. Net2net: Accelerating learning via knowledge\ntransfer. In ICLR.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERTâ€™s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276â€“286, Florence, Italy. Association for Com-\nputational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171â€“4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. CoRR.\n2142\nA. Feng and P. Panda. 2020. Energy-efficient and robust\ncumulative training with net2net transformation. In\nIJCNN.\nLinyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei\nWang, and Tie-Yan Liu. 2019. Efficient training of\nBERT by progressively stacking. In Proceedings of\nthe 36th International Conference on Machine Learn-\ning, ICML 2019, 9-15 June 2019, Long Beach, Cali-\nfornia, USA, volume 97 of Proceedings of Machine\nLearning Research, pages 2337â€“2346. PMLR.\nXiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen\nChen, and Jiawei Han. 2021. On the transformer\ngrowth for progressive BERT training. In Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5174â€“5180, Online. Association for Computational\nLinguistics.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015. Learning both weights and connections for\nefficient neural network. Advances in neural infor-\nmation processing systems, 28.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). ArXiv preprint ,\nabs/1606.08415.\nHai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra\nKÃ¼bler, and Lawrence S. Moss. 2020. OCNLI: origi-\nnal chinese natural language inference. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, Online Event, 16-20 November 2020,\nvolume EMNLP 2020 of Findings of ACL , pages\n3512â€“3526. Association for Computational Linguis-\ntics.\nGanesh Jawahar, BenoÃ®t Sagot, and DjamÃ© Seddah.\n2019. What does BERT learn about the structure\nof language? In ACL, pages 3651â€“3657, Florence,\nItaly. Association for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In ACL, pages 7871â€“7880, Online. As-\nsociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR.\nSinno Jialin Pan and Qiang Yang. 2010. A survey on\ntransfer learning. TKDE.\nYujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han,\nZhengyan Zhang, YuSheng Su, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2021. Knowledge\ninheritance for pre-trained language models. CoRR.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1â€“140:67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383â€“2392, Austin,\nTexas. Association for Computational Linguistics.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. 2020. Green ai. Communications of the\nACM, 63(12):54â€“63.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using model parallelism.\nCoRR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998â€“6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019a.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nDilin Wang, Meng Li, Lemeng Wu, Vikas Chandra, and\nQiang Liu. 2019b. Energy-aware neural architec-\nture optimization with fast splitting steepest descent.\nCoRR.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625â€“641.\n2143\nWei Wen, Yuxiong He, Samyam Rajbhandari, Minjia\nZhang, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen,\nand Hai Li. 2018. Learning intrinsic sparse structures\nwithin long short-term memory. In 6th International\nConference on Learning Representations, ICLR 2018,\nVancouver, BC, Canada, April 30 - May 3, 2018,\nConference Track Proceedings. OpenReview.net.\nLemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. 2020a.\nFirefly neural architecture descent: a general ap-\nproach for growing neural networks. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nLemeng Wu, Dilin Wang, and Qiang Liu. 2019. Split-\nting steepest descent for growing neural architectures.\nIn NeurIPS, pages 10655â€“10665.\nLemeng Wu, Mao Ye, Qi Lei, Jason D. Lee, and Qiang\nLiu. 2020b. Steepest descent neural architecture opti-\nmization: Escaping local optimum with signed neural\nsplitting. CoRR.\nQiyu Wu, Chen Xing, Yatao Li, Guolin Ke, Di He, and\nTie-Yan Liu. 2021. Taking notes on the fly helps\nlanguage pre-training. In ICLR.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tie-Yan Liu. 2020. On layer\nnormalization in the transformer architecture. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 10524â€“10533. PMLR.\nCheng Yang, Shengnan Wang, Chao Yang, Yuechuan\nLi, Ru He, and Jingqiao Zhang. 2020. Progres-\nsively stacking 2.0: A multi-stage layerwise train-\ning method for bert training speedup. arXiv preprint\narXiv:2011.13635.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In NeurIPS, pages 5754â€“5764.\nYang You, Jing Li, Sashank J. Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large batch optimization for deep learning:\nTraining BERT in 76 minutes. In 8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao,\nZhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng\nWang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yi-\nfan Yao, Xinjing Huang, Jun Wang, Jianfeng Yu,\nQi Guo, Yue Yu, Yan Zhang, Jin Wang, Hengtao\nTao, Dasen Yan, Zexuan Yi, Fang Peng, Fangqing\nJiang, Han Zhang, Lingfeng Deng, Yehong Zhang,\nZhe Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo,\nShanzhi Gu, Gaojun Fan, Yaowei Wang, Xuefeng\nJin, Qun Liu, and Yonghong Tian. 2021. Pangu-\nÎ±: Large-scale autoregressive pretrained chinese lan-\nguage models with auto-parallel computation.\nMinjia Zhang and Yuxiong He. 2020. Accelerating\ntraining of transformer-based language models with\nprogressive layer dropping. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015, pages 19â€“27.\nIEEE Computer Society.\n2144\nA Ablation Study of bert2BERT\n0 1 2 3 4 5 6 7 8\nFLOPs (1e19)\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6MLM Loss\n4 5 6 7\n1.40\n1.42\n1.44\n1.46\n1.48\n1.437\n100%87.8%69.6%61.6%54.8%\nBERTBASE\nDirectCopy\nFPI\nAKI\nbert2BERT\nFigure 5: Ablation study of bert2BERT. bert2BERT\nmeans the combination of AKI and two-stage pre-\ntraining.\nThe ablation study of bert2BERT is displayed\nin Table 5. From the table, we observe that: (1)\nall the proposed methods is better than the original\npre-training method and DirectCopy; (2) although\nAKI has a worse initialization than FPI, it achieves\nfaster convergence rate than FPI; (3) the two-stage\npre-training furthers reduce the cost from 61.6%\nto 54.8%; (4) the FPI curve has an upward trend\nat the beginning. We conjecture that it is due to\nthe symmetry brought by FPI and the model needs\nsome optimization time to break this symmetry.\nB bert2BERT with smaller source model\n0 1 2 3 4 5 6 7 8\nFLOPs (1e19)\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6MLM Loss\n4 5 6 7\n1.40\n1.42\n1.44\n1.46\n1.48\n1.437\n100%76.7%63.2%57.3%\nDirectCopy_L6\nbert2BERT_L6\nbert2BERT_L8\nbert2BERT_L10\nBERTBASE\nFigure 6: Loss curves of bert2BERT and baselines with\nsmaller source models.\nWe test bert2BERT with different source models\nand the loss curves are represented in Figure 6.\nC Effect of sub-model training epochs\nWe study the effect of sub-model training epochs\non the pre-training efficiency. The loss curves are\nrepresented in Figure 7. Note that the setting Eb =\n20 has not achieved the same loss (1.437) as the\nbaseline BERTBASE in the 40 training epochs.\nD Application on GPT\nThe loss curve of our method on GPT application\nis displayed in Figure 8.\n0 1 2 3 4 5 6 7 8\nFLOPs (1e19)\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6MLM Loss\n4 5 6 7\n1.40\n1.42\n1.44\n1.46\n1.48\n1.437\n100%61.6%54.8%\nBERTBASE\nEb=0\nEb=5\nEb=10\nEb=20\nFigure 7: Loss curves of bert2BERT with different sub-\nmodel training epochs.\n0 1 2 3 4 5\nFLOPs (1e19)\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0Loss\n2.5 3.0 3.5 4.0 4.5 5.0\n2.84\n2.86\n2.88\n2.90\n2.870\n100%52.4%\nGPT bert2BERT\nFigure 8: Pre-training loss curves of GPT.\nE Comparisons of Attention Patterns\nWe take the source model S(4, 256) and target\nmodel T (4, 512) as an example to analyze the at-\ntention patterns of DirectCopy in Figure 10, FPI in\nFigure 11 and AKI in Figure 12.\nWe display the attention patterns of the source\nmodel S(4, 256) in Figure 9. Compared with the\nsource model, we observe that the newly added\nattention patterns of DirectCopy are messy, and the\nrandomly initialized parameters destroy the atten-\ntion patterns of the source model. The proposed\nFPI method makes the new model have the same\nattention patterns as the source model, thus the\nknowledge of the source model is preserved. How-\never, FPI always induces symmetrical attention pat-\nterns in the same layer. This symmetry will hinder\nthe convergence. To handle this problem, we use\nAKI method to reuse the parameters of the upper\nlayer (advanced knowledge) to break the symmetry,\nand meanwhile make the knowledge in the same\nlayer richer. Through the AKI method, the attention\npatterns of the upper layer can be also maintained\nwell in the target model. For example, as shown\nin Figure 12, the newly added attention patterns of\nthe 1st layer in the target model are similar to the\nones of the 2nd layer in the source model.\nF Illustration of FPI and AKI process\nWe illustrate the process of FPI and AKI in Figure\n13 and 14 respectively.\n2145\nL1 H0\n L1 H1\n L1 H2\n L1 H3\nL2 H0\n L2 H1\n L2 H2\n L2 H3\nL3 H0\n L3 H1\n L3 H2\n L3 H3\nL4 H0\n L4 H1\n L4 H2\n L4 H3\nFigure 9: Attention patterns of the source model S(4, 256), which has 4 attention heads in each layer.\nL1 H0\n L1 H1\n L1 H2\n L1 H3\n L1 H4\n L1 H5\n L1 H6\n L1 H7\nL2 H0\n L2 H1\n L2 H2\n L2 H3\n L2 H4\n L2 H5\n L2 H6\n L2 H7\nL3 H0\n L3 H1\n L3 H2\n L3 H3\n L3 H4\n L3 H5\n L3 H6\n L3 H7\nL4 H0\n L4 H1\n L4 H2\n L4 H3\n L4 H4\n L4 H5\n L4 H6\n L4 H7\nFigure 10: Attention patterns of the target model T (4, 512) based on the baseline DirectCopy method. The first 4\nattention patterns (H0-H3) in each row correspond to the source modelâ€™s attention patterns, and the last 4 attention\npatterns (H4-H7) are newly added.\n2146\nL1 H0\n L1 H1\n L1 H2\n L1 H3\n L1 H4\n L1 H5\n L1 H6\n L1 H7\nL2 H0\n L2 H1\n L2 H2\n L2 H3\n L2 H4\n L2 H5\n L2 H6\n L2 H7\nL3 H0\n L3 H1\n L3 H2\n L3 H3\n L3 H4\n L3 H5\n L3 H6\n L3 H7\nL4 H0\n L4 H1\n L4 H2\n L4 H3\n L4 H4\n L4 H5\n L4 H6\n L4 H7\nFigure 11: Attention patterns of the target model T (4, 512) based on our FPI method. The last 4 attention patterns\n(H4-H7) in each row are obtained by FPI expansion.\nL1 H0\n L1 H1\n L1 H2\n L1 H3\n L1 H4\n L1 H5\n L1 H6\n L1 H7\nL2 H0\n L2 H1\n L2 H2\n L2 H3\n L2 H4\n L2 H5\n L2 H6\n L2 H7\nL3 H0\n L3 H1\n L3 H2\n L3 H3\n L3 H4\n L3 H5\n L3 H6\n L3 H7\nL4 H0\n L4 H1\n L4 H2\n L4 H3\n L4 H4\n L4 H5\n L4 H6\n L4 H7\nFigure 12: Attention patterns of the target model T (4, 512) based on our AKI method. The last 4 attention patterns\n(H4-H7) in each row are obtained by AKI expansion.\n2147\na b\nc d\ne f\ng h i j\na b\nc d\ne f a\nb c\ng/2\nLayer Norm:\nğ‘¢ = ğ».ğ‘šğ‘’ğ‘ğ‘›\nğ‘  = (ğ» âˆ’ğ‘¢)2.ğ‘šğ‘’ğ‘ğ‘› LayerNormä¼šå‡ºç°éš¾ä»¥é¿\nå…çš„ä¸åŒä½†å½±å“ä¸å¤§ã€‚å½“å¤§\nk\nl m\nn\nClassifier\nEmbedding\nMulti-Head \nAttention (MHA)\nAdd & Norm\nFeed Forward \nNetwork (FFN)\nAdd & Norm\nN x\no\np q\nr\ns\nt u v\ng/2\nh i/2\ni/2j\nk l\nm\nn k\nl\no/2\no/2\np\nq/2 q/2\nr\ns t u\nv\ns\nt\na b c\nd e f\na b c\ng/2\ng/2\nh\ni/2 i/2\nj\ng/2\nh g/2\nk/2\nk/2\nl\nm/2 m/2\nn\nk/2\nl\nk/2\no/2\no/2\np\nq/2 q/2\nr\no/2\np o/2\ns/2 s/2\nt\nu/2 u/2\nv\ns/2\nt\ns/2\na b c\nd e f\na b c\na\nd e\nf\nb c\nw1 w2\nw1 w2\nw1 w2\nw1 w1w2\nw1 w1w2\nW1/2 w2 W1/2\nw1 w1w2\nw1 w1w2\nw2W1/2 W1/2\nMHA/FFN Expansion step 1 MHA Expansion step 2 FFN Expansion step 2\nğ‘Šğ‘™\nğ‘„|ğ¾|ğ‘‰\nğ‘Šğ¸ğ‘€ğµ\nğ‘Šğ‘™\nğ‘‚\nğ‘Šğ¿ğ‘\na b\nc d\ne f a\nb c\nğ‘Šğ¿ğ‘\nğ‘Šğ‘™\n1\nğ‘Šğ‘™\n2\nğ‘Šğ¿ğ‘\nğ‘Šğ¸ğ‘€ğµğ‘‡\nğ‘”ğ‘–ğ‘› = {1,2,1}\nFFN\nMHA\na b\nc d\ne f a\nb c\ng/2\ng/2\nh\ni/2 i/2\nj\nk l\nm\nn k\nl\no/2\no/2\np\nq/2 q/2\nr\ns t u\nv\ns\nt\na b c\nd e f\na b c\nw1 w1w2\nw1 w1w2\nW1/2 w2 W1/2\nFPI\nhidden neuronembedding neuron different headsâ€™ neuron FFN neuron\nFigure 13: FPI process. We use the FPI method to widen the source model with a width of 2 into a target model\nwith a width of 3. In the example, the source model and the target model have 2 and 3 attention heads respectively.\nAnd the head dimension is 1. To facilitate the illustration, we reduce the number of neurons in the FFN layer. We\nalso note that since the MLM classifier of BERT is a transposition of the Embedding layer, they share a parameter\nmatrix. Therefore, in step 1, we expand the MLM classifier by re-scaling the parameter values of the LN layer\nbelow the MLM classifier instead of following formula 7.\nLayer Norm:\nğ‘¢ = ğ».ğ‘šğ‘’ğ‘ğ‘›\nğ‘  = (ğ» âˆ’ğ‘¢)2.ğ‘šğ‘’ğ‘ğ‘› LayerNormä¼šå‡ºç°éš¾ä»¥é¿\nå…çš„ä¸åŒä½†å½±å“ä¸å¤§ã€‚å½“å¤§\nClassifier\nEmbedding\nMulti-Head \nAttention (MHA)\nAdd & Norm\nFeed Forward \nNetwork (FFN)\nAdd & Norm\nN x\ng/2\ng/2\nh\ni/2 i/2\nj\ngâ€™/2\nhâ€™ gâ€™/2\nk/2\nkâ€™/2\nl\nm/2 mâ€™/2\nn\nk/2\nl\nkâ€™/2\no/2\no/2\np\nq/2 q/2\nr\noâ€™/2\npâ€™ oâ€™/2\ns/2 sâ€™/2\nt\nu/2 uâ€™/2\nv\ns/2\nt\nsâ€™/2\na b c\nd e f\na b c\nw1 w1w2\nw1 w1w2\nw2W1/2 W1/2\nMHA Expansion step 2\nFFN Expansion step 2\na b\nc d\ne f a\nb c\nğ‘”ğ‘–ğ‘› = {1,2,1}\nFFN\nMHA\na b\nc d\ne f a\nb c\ng/2\ng/2\nh\ni/2 i/2\nj\nk l\nm\nn k\nl\no/2\no/2\np\nq/2 q/2\nr\ns t u\nv\ns\nt\na b c\nd e f\na b c\nw1 w1w2\nw1 w1w2\nW1/2 w2 W1/2\nğ‘Šğ‘™\n1\nğ‘Šğ‘™\n2\nğ‘Šğ‘™\nğ‘„|ğ¾|ğ‘‰\nğ‘Šğ‘™\nğ‘‚\ngâ€™/2\ngâ€™/2\nhâ€™\niâ€™/2 iâ€™/2\njâ€™\nkâ€™ lâ€™ mâ€™\nnâ€™ kâ€™\nlâ€™\noâ€™/2\noâ€™/2\npâ€™\nqâ€™/2 qâ€™/2\nrâ€™\nsâ€™ tâ€™ uâ€™\nvâ€™\nsâ€™\ntâ€™\nwâ€™1 wâ€™1wâ€™2\nğ‘Šğ‘™+1\n1\nğ‘Šğ‘™+1\n2\nğ‘Šğ‘™+1\nğ‘„|ğ¾|ğ‘‰\nğ‘Šğ‘™+1\nğ‘‚\nğ‘ˆğ‘™\n2\nğ‘ˆğ‘™\n1\nğ‘ˆğ‘™\nğ‘„|ğ¾|ğ‘‰\nğ‘ˆğ‘™\nğ‘‚\nAKI hidden neuron\nembedding neuron different headsâ€™ neuron\ndifferent layersâ€™ FFN neuron\nğ’\n-th layer Transformerğ’+ğŸ\n-th layer Transformer\nFigure 14: AKI process. We ignore its first step because it is the same as FPIâ€™s first step. The main difference\nbetween AKI and FPI is that in step 2, AKI copies some attention heads in MHA and some parameters in FFN of\nthe (l + 1)-th layer instead of only copying the l-th layer to construct the new l-th layer Transformer.\n2148",
  "topic": "Chen",
  "concepts": [
    {
      "name": "Chen",
      "score": 0.8654815554618835
    },
    {
      "name": "Computer science",
      "score": 0.5697044134140015
    },
    {
      "name": "Natural language processing",
      "score": 0.5316088199615479
    },
    {
      "name": "Association (psychology)",
      "score": 0.44621896743774414
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.42524904012680054
    },
    {
      "name": "Computational linguistics",
      "score": 0.4147520959377289
    },
    {
      "name": "Linguistics",
      "score": 0.4028880298137665
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3932405114173889
    },
    {
      "name": "Programming language",
      "score": 0.33905160427093506
    },
    {
      "name": "Philosophy",
      "score": 0.2806837558746338
    },
    {
      "name": "Physics",
      "score": 0.10897251963615417
    },
    {
      "name": "Epistemology",
      "score": 0.09219935536384583
    },
    {
      "name": "Geology",
      "score": 0.05003806948661804
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210159102",
      "name": "Huawei Technologies (Sweden)",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210114105",
      "name": "Tsinghuaâ€“Berkeley Shenzhen Institute",
      "country": "CN"
    }
  ],
  "cited_by": 25
}