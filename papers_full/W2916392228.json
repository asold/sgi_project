{
  "title": "Feature Based Domain Adaptation for Neural Network Language Models with Factorised Hidden Layers",
  "url": "https://openalex.org/W2916392228",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2016943051",
      "name": "Michael Hentschel",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1971575768",
      "name": "Marc Delcroix",
      "affiliations": [
        "NTT (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2147838119",
      "name": "Atsunori Ogawa",
      "affiliations": [
        "NTT (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2108993706",
      "name": "Tomoharu Iwata",
      "affiliations": [
        "NTT (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2080962056",
      "name": "Tomohiro Nakatani",
      "affiliations": [
        "NTT (Japan)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2100506586",
    "https://openalex.org/W2020073413",
    "https://openalex.org/W2159382562",
    "https://openalex.org/W170853248",
    "https://openalex.org/W2130696866",
    "https://openalex.org/W2092062917",
    "https://openalex.org/W2106901945",
    "https://openalex.org/W1996903695",
    "https://openalex.org/W2096175520",
    "https://openalex.org/W2159077101",
    "https://openalex.org/W2114858359",
    "https://openalex.org/W2127673570",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2513257850",
    "https://openalex.org/W2791636785",
    "https://openalex.org/W2749510669",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2250357346",
    "https://openalex.org/W1917432393",
    "https://openalex.org/W101286142",
    "https://openalex.org/W2407040763",
    "https://openalex.org/W2404114959",
    "https://openalex.org/W2080005694",
    "https://openalex.org/W2288371874",
    "https://openalex.org/W2094147890",
    "https://openalex.org/W2514740276",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2399550240",
    "https://openalex.org/W1519502414",
    "https://openalex.org/W2573866231",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W2513079830",
    "https://openalex.org/W2397725648",
    "https://openalex.org/W2099655049",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2786462409"
  ],
  "abstract": "Language models are a key technology in various tasks, such as, speech recognition and machine translation. They are usually used on texts covering various domains and as a result domain adaptation has been a long ongoing challenge in language model research. With the rising popularity of neural network based language models, many methods have been proposed in recent years. These methods can be separated into two categories: model based and feature based adaptation methods. Feature based domain adaptation has compared to model based domain adaptation the advantage that it does not require domain labels in the corpus. Most existing feature based adaptation methods are based on bias adaptation. We propose a novel feature based domain adaptation technique using hidden layer factorisation. This method is fundamentally different from existing methods because we use the domain features to calculate a linear combination of linear layers. These linear layers can capture domain specific information and information common to different domains. In the experiments, we compare our proposed method with existing adaptation methods. The compared adaptation techniques are based on two different ideas, that is, bias based adaptation and gating of hidden units. All language models in our comparison use state-of-the-art long short-term memory based recurrent neural networks. We demonstrate the effectiveness of the proposed method with perplexity results for the well-known Penn Treebank and speech recognition results for a corpus of TED talks.",
  "full_text": "598\nIEICE TRANS. INF. & SYST., VOL.E102–D, NO.3 MARCH 2019\nPAPER\nFeature Based Domain Adaptation for Neural Network Language\nModels with Factorised Hidden Layers\nMichael HENTSCHEL†∗a), Marc DELCROIX††, Nonmembers, Atsunori OGAWA††, Tomoharu IWATA††,\nand Tomohiro NAKATANI††, Members\nSUMMARY Language models are a key technology in various tasks,\nsuch as, speech recognition and machine translation. They are usually used\non texts covering various domains and as a result domain adaptation has\nbeen a long ongoing challenge in language model research. With the rising\npopularity of neural network based language models, many methods have\nbeen proposed in recent years. These methods can be separated into two\ncategories: model based and feature based adaptation methods. Feature\nbased domain adaptation has compared to model based domain adaptation\nthe advantage that it does not require domain labels in the corpus. Most ex-\nisting feature based adaptation methods are based on bias adaptation. We\npropose a novel feature based domain adaptation technique using hidden\nlayer factorisation. This method is fundamentally di ﬀerent from existing\nmethods because we use the domain features to calculate a linear combi-\nnation of linear layers. These linear layers can capture domain speciﬁc\ninformation and information common to di ﬀerent domains. In the experi-\nments, we compare our proposed method with existing adaptation methods.\nThe compared adaptation techniques are based on two di ﬀerent ideas, that\nis, bias based adaptation and gating of hidden units. All language mod-\nels in our comparison use state-of-the-art long short-term memory based\nrecurrent neural networks. We demonstrate the e ﬀectiveness of the pro-\nposed method with perplexity results for the well-known Penn Treebank\nand speech recognition results for a corpus of TED talks.\nkey words: language model, LSTM, domain adaptation, unsupervised, la-\ntent Dirichlet allocation\n1. Introduction\nLanguage models (LMs) are usually trained on text data\ncovering a large variety of domains. Domain speciﬁc lan-\nguage models, however, usually show a lower perplexity\n(PPL) and better performance in automatic speech recog-\nnition (ASR) tasks compared to general LMs at test time.\nTherefore, adapting general LMs to speciﬁc topics or gen-\nres has been an ongoing research interest. A comprehen-\nsive overview of N-gram LM adaptation techniques was pro-\nvided in [1] and [2]. N-gram adaptation with topic model\ninformation was presented in [3]–[7]. N-gram LMs have\nbeen the most popular LMs in speech processing but there\nhave also been several approaches of domain adaptation\nproposed for other LM types, such as, maximum entropy\nManuscript received June 22, 2018.\nManuscript revised October 11, 2018.\nManuscript publicized December 4, 2018.\n†The author is with Nara Institute of Science and Technology,\nIkoma-shi, 630–0192 Japan.\n††The authors are with NTT Communication Science Laborato-\nries, NTT Corporation, Kyoto-fu, 619–0237 Japan.\n∗The work was conducted as joint research while the author\nwas at NTT Communication Science Laboratories.\na) E-mail: michael.hentschel.mc5@is.naist.jp\nDOI: 10.1587/transinf.2018EDP7222\nLMs [8]–[12].\nSince the introduction of neural network based LMs\n(NN-LMs), these models have shown to perform consis-\ntently better than N-grams. NN-LMs were ﬁrst introduced\nwith feed-forward neural networks (FFWD-LMs) [13]. Sub-\nsequently, vanilla recurrent neural network LMs (RNN-\nLMs) [14], [15] further improved over FFWD-LMs, but suf-\nfered from the vanishing gradient problem [16]. In order\nto solve this problem, the vanilla RNN has been replaced\nwith long short-term memory (LSTM) [17] to create LSTM-\nLM [18].\nThere exist two paradigms for domain adaptation of\nNN-LMs: model based and feature based domain adapta-\ntion. In model based adaptation, the parameters in the net-\nwork are adapted with in-domain data by re-training. In fea-\nture based adaptation, an adaptation feature, such as, topic\ninformation from latent Dirichlet allocation (LDA) [19] is\nused. The LM is trained with the adaptation feature to adapt\nits activations in the network to the topic information. Com-\npared to model based domain adaptation, feature based do-\nmain adaptation has the advantage that it does not require\ndomain labels in the corpus. In addition, model based adap-\ntation can have a problem with overﬁtting on little adapta-\ntion data [20].\nWe propose a new approach for feature based LM do-\nmain adaptation. Feature based domain adaptation methods\nusually add a bias to the network input or output. This bias\ndepends on the topic feature or a (given) domain label. Our\nproposed factorised hidden layer based adaptation is funda-\nmentally di ﬀerent from bias based approaches. The output\nlayer is factorised into multiple layers or factors, where each\nfactor is weighted by a factor weight. This can also be seen\nas a linear combination of di ﬀerent subspaces. It is also\nsimilar to a linear combination of multiple domain depen-\ndent LMs, which share a common hidden state. The fac-\ntor weights are calculated from an auxiliary network, which\nis trained jointly with the main network by standard error\nbackpropagation. The input of this auxiliary network are\nLDA features. Factorised hidden layer adaptation has been\nsuccessfully applied to acoustic model adaptation [21] and\nspeaker aware beamforming [22].\nRNN-LMs have been the main target of domain adap-\ntation techniques presented in the literature, but LSTM-LMs\nare currently the state-of-the-art. For this reason we will use\nLSTM as recurrent unit in all our NN-LMs. In addition, all\nadaptation methods we compare work in an unsupervised\nCopyright c⃝2019 The Institute of Electronics, Information and Communication Engineers\nHENTSCHEL et al.: FEATURE BASED DOMAIN ADAPTATION FOR NEURAL NETWORK LANGUAGE MODELS WITH FACTORISED HIDDEN LAYERS\n599\nmanner, that means, these methods do not require any do-\nmain label in the training data. This setting is more relevant\nin practice because LMs can be trained on large text corpora\nof many million words. It would be costly and time inten-\nsive to annotate such large corpora by humans.\nFor our comparison, we use two di ﬀerent corpora. The\nﬁrst one is the well-known Penn Treebank (PTB) [23].D e -\nspite its small size, PTB is one of the de-facto benchmarks\nin language modelling. As second corpus, we use a corpus\nbased on TED talks. In order to provide ASR results, we\nuse the TED-LIUM dataset [24], [25]. For language model\ntraining, we use our own enhanced training set, based on\nsubtitles from more TED talks than used in TED-LIUM.\n2. Overview of Domain Adaptation for Neural Net-\nwork based Language Models\n2.1 Model Based Domain Adaptation\nModel based adaptation is a two-step process. First, a gen-\neral language model is trained, where often an adaptation\nlayer is inserted into the network. In the second step, the\nweights in this adaptation layer are updated using in-domain\ndata. Model based adaptation has been used for FFWD-\nLMs [26], [27] and RNN-LMs [28]. Recently, model based\nadaptation with a linear hidden network (LHN) [29], [30]\nwas proposed. An LHN adds a linear hidden layer in the\nnetwork without a subsequent non-linearity. As input to this\nlinear layer, the authors used the output of the RNN and a\none-hot label which encodes the domain. The weights in\nthis linear layer are learned during re-training.\nA slightly di ﬀerent approach for model based adap-\ntation uses a gating mechanism. In acoustic model adap-\ntation, a concept called learning hidden unit contributions\n(LHUC) [31], [32] has previously been introduced. In this\ncase, the speaker adaptation data is used to apply a gating\nmechanism on the hidden units in a neural network based\nacoustic model. In [20], the application of LHUC to RNN-\nLMs was investigated. The authors applied the concept of\nLHUC to the output of the hidden layer and the adapta-\ntion weights were learned from in-domain data. The authors\nshowed improvements for PPL and N-best rescoring.\nHowever, with little adaptation data, model based adap-\ntation can be a problem because the adapted models are\nprone to overﬁtting [20]. In addition, model based adapta-\ntion requires domain labels throughout the whole corpus.\nCreating this annotation is expensive.\n2.2 Feature Based Domain Adaptation\nIn feature based adaptation, usually the input of the network\nis extended by additional domain speciﬁc features. Many\nmethods have been proposed, where these features act as a\ndomain dependent bias. For RNN-LMs, the ﬁrst proposed\napproach was a context dependent RNN-LM [33], which\nused LDA features to allow the network to exploit infor-\nmation from a context window of the current word. This\nFig. 1 As i m p l eL S T M - L M .\nmethod has also shown to be successful for RNN-LMs [34]\non multi-domain broadcast data in the MGB Challenge [30].\nThe authors showed that feature based domain adaptation\noutperforms model based adaptation in the context of the\nMGB Challenge. Domain adaptation has mainly been pro-\nposed using vanilla RNN-LMs. To the best of our knowl-\nedge, the only other prior research on LSTM-LM adapta-\ntion was presented in [35], which uses the same adapta-\ntion mechanism as [33]. A feature based adaptation mech-\nanism using a gating on the word vectors was proposed in\n[36]. The authors combined information from in-domain\nand general-domain word vectors.\n3. LSTM Language Model\nBefore explaining more details about the adaptation tech-\nniques, we brieﬂy review a baseline LSTM-LM as shown in\nFig. 1. The vector encoding the current word ID by a one-\nhot vector is denoted by w(t). Using the embedding matrix\nU\n(w), the input to the LSTM x(t) is calculated as follows,\nx(t) =U(w)w(t). (1)\nTo calculate its output h(t) and its state c(t), the following\nset of equations are used in an LSTM cell [37].\ni(t) =σ(W(i,w) x(t) +W(i,h) h(t −1) +b(i)), (2)\nf(t) =σ(W(f,w) x(t) +W(f,h) h(t −1) +b(f)), (3)\no(t) =σ(W(o,w) x(t) +W(o,h) h(t −1) +b(o)), (4)\ng(t) =tanh(W(g,w) x(t) +W(g,h) h(t −1) +b(g)), (5)\nc(t) =f(t) ⊙c(t −1) +i(t) ⊙g(t), (6)\nh(t) =o(t) ⊙tanh(c(t)), (7)\nwhere i(t), f(t) and o(t) are usually named the input, forget\nand output gates, respectively. The weight matrices for gate\nj for the word input and the previous hidden layer are de-\nnoted by W( j,w) and W( j,h), respectively. The bias vector for\neach respective gate j is denoted as b( j). Since we use vec-\ntor notation in the above equations, σ(·) is the element-wise\nsigmoid, tanh( ·) is the element-wise hyperbolic tangent and\n⊙denotes an element-wise multiplication.\nA simple LSTM-LM calculates the probability for the\nnext word ˆw(t +1) from h(t)a s\nˆw(t +1) =softmax(V(w) h(t) +b(V,w)), (8)\nwhere V(w) and b(V,w) are the weight matrix and the bias vec-\ntor of the output layer and softmax is the softmax function.\n4. Feature Based Language Model Adaptation for\nLSTM-LMs\nAfter reviewing the basic LSTM-LM, we introduce common\n600\nIEICE TRANS. INF. & SYST., VOL.E102–D, NO.3 MARCH 2019\nFig. 2 LSTM-LM feature based model adaptation with (a) context dependent LSTM-LM\n(contLSTM), (b) LHN (fLHN-LSTM), (c) LHUC (fLHUC-LSTM) and (d) factorised hidden layer\n(factLSTM).\napproaches for NN-LM domain adaptation. Although many\nof these approaches were introduced with vanilla RNN-\nLMs, we investigate their extension to LSTM-LMs. In addi-\ntion, some of the approaches have been proposed for model\nbased adaptation, however, we investigate their application\nto feature based adaptation. The features commonly used in\nprior research for domain adaptation were derived from an\nLDA topic model.\n4.1 Context Dependent LSTM-LM\nThe ﬁrst adaptation technique, as shown in Fig. 2 (a), has\nbeen originally proposed for RNN-LMs [33], [34] and has\nalso been used with LSTM-LMs [35]. The input and output\nin a context dependent LSTM-LM (contLSTM) are depen-\ndent on the adaptation feature. The input is extended by\nan additional adaptation feature vector a(t). Equation (1) is\nmodiﬁed as follows,\nx(t) =U\n(w)w(t) +U(a) a(t) +b(U,a), (9)\nwhere U(a) and b(U,a) are the weight matrix and bias vec-\ntor for the adaptation features, respectively. There is also a\ndirect connection introduced to the output layer which mod-\niﬁes Eq. (8) as follows,\nˆw(t +1)\n=softmax(V\n(w) h(t) +b(V,w) +V(a) a(t) +b(V,a)), (10)\nwhere V(a) and b(V,a) are the weight matrix and bias vector\nof the linear layer connecting the adaptation feature vector\nto the output.\nFor RNN-LMs, the PPL reduction achieved with this\nadaptation scheme was signiﬁcant compared to a vanilla\nRNN-LM. This can be explained to some extent by the van-\nishing gradient problem that RNNs su ﬀer from [16].T h e\nlong context information provided by the adaptation features\ncan circumvent this problem. In combination with LSTM-\nLMs, however, the relative PPL reduction achieved by this\nmethod was not as large as with vanilla RNN-LMs [35].\n4.2 LHN Based LSTM-LM Adaptation\nLM adaptation with a linear hidden network (LHN) has re-\ncently been proposed for model based adaptation of vanilla\nRNN-LMs [38]. Since we focus on feature based domain\nadaptation, we decided to use the LDA topic feature instead\nof a domain label as input to the LHN. In the experiments,\nwe denote this method by fLHN-LSTM. Figure 2 (b) shows\nan fLHN-LSTM. The LHN introduces an additional linear\nlayer between the LSTM and the output layer. Since the\noutput d(t) of this intermediate linear layer is not followed\nby a non-linearity, it is called a linear hidden network.\nThe LDA features a(t) are transformed by a linear layer\nwith weight matrix V\n(a) and bias vector b(V,a) and inserted\ninto the LHN. The output of the LSTM h(t) is also trans-\nformed by a linear layer with weight matrix V(w) and bias\nb(V,w) and added at the input of the LHN\nd(t) =V(w) h(t) +b(V,w) +V(a) a(t) +b(V,a). (11)\nAs shown in Eq. (11), LHN introduces a topic dependent\nbias term ( V(a) a(t) +b(V,a)). The output of the LHN d(t)\nis followed by a linear layer V and the softmax function to\ncalculate the probability for the next word ˆw(t +1)\nˆw(t +1) =softmax(Vd (t) +b(V)). (12)\nHENTSCHEL et al.: FEATURE BASED DOMAIN ADAPTATION FOR NEURAL NETWORK LANGUAGE MODELS WITH FACTORISED HIDDEN LAYERS\n601\nThe LDA features are input to the LHN during network\ntraining and evaluation.\n4.3 LHUC Based Domain Adaptation\nModel adaptation with LHUC has ﬁrst been introduced for\nacoustic model adaptation in ASR. Recently, it has also been\napplied to vanilla RNN-LMs [20] and it showed to reduce\nPPL and WER compared to a vanilla RNN-LM. In [20]\nthe adaptation was applied as a model based adaptation of\nthe RNN. We will, however, use LHUC based adaptation\nas a feature based adaptation method, where the adaptation\nweights are calculated from auxiliary features in a similar\nway to [32]. We use LHUC in a similar scheme to fLHN-\nLSTM, as shown in Fig. 2 (c). We will denote LHUC based\nmodel adaptation as fLHUC-LSTM.\nThe LDA features are multiplied with a linear layer,\nfollowed by a sigmoid non-linearity and a weighting by two\n(as proposed by [31], [32])\nh\n(a)(t) =2σ(U(a) a(t) +b(U,a)), (13)\nwhere U(a) and b(U,a) are the weight matrix and bias vector\nof the linear layer for the LDA features. The sigmoid non-\nlinearity will set some of the activations to zero. In order to\ncompensate for it, the amplitude of the remaining activations\nis multiplied by two, in order to keep the activation in the\nsubsequent layer on the same level. h\n(a)(t)i su s e da sag a t e\nfor the output of the LSTM\nd(t) =(V(w) h(t) +b(V,w)) ⊙h(a)(t), (14)\nwhere ⊙denotes an element-wise multiplication of two vec-\ntors. h(a)(t) has values between [0 ,2] and it can be inter-\npreted as a context dependent gating of the units in the adap-\ntation layer. Another linear layer and the softmax function\nthen follow the output of this adaptation layer\nˆw(t +1) =softmax(Vd (t) +b\n(V)). (15)\nWe use this adaptation scheme, because in our experiments\nit showed to be more e ﬀective than an adaptation of the di-\nrect output of LSTM.\n5. Proposed Factorised Hidden Layer Based LSTM-\nLM Adaptation\nIn this section, we introduce our proposed method for fea-\nture based domain adaptation using factorised hidden layers,\nas shown in Fig. 2 (d). We denote it hereafter as factLSTM.\nIn our method, we use the output of the LSTM as an input to\nN linear layers (factors) with corresponding weight matrix\nL\n(w)\nn and bias b(L,w)\nn . The size of each linear layer is the num-\nber of hidden units times the vocabulary size, that means the\nsize of the output layer. After weighting each of the linear\nlayers by a factor weight γ\nn, they are summed up before the\nsoftmax function\nˆw(t +1) =softmax\n⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝\nN∑\nn=1\nγn(L(w)\nn h(t) +b(L,w)\nn )/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext\n=zn\n⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . (16)\nAs with the LHN, there is no non-linearity after the factors.\nThere is only a multiplication with a factor weight before\ncalculating the probability for the next word ˆw(t +1).\nIn order to calculate each factor weight γ\nn,w eu s ea n\nauxiliary network. The input to the auxiliary network are\nthe topic features calculated from an LDA topic model. This\nauxiliary network can be of arbitrary depth. In this work, we\nuse a single linear layer followed by a sigmoid non-linearity\nγ=[γ\n1,γ2,...γ n,...,γ N ] =σ(U(a) a(t) +b(U,a)), (17)\nwhere U(a) and b(U,a) are the weight matrix and bias vector\nfor the linear layer. The parameters of the auxiliary network\ncan be trained jointly with the main network by error back-\npropagation, as shown in [39]. This means we do not have to\ntrain the auxiliary network and the main network in separate\ntraining steps.\nfactLSTM is related to fLHUC-LSTM. In fLHUC-\nLSTM, one weight is multiplied with one node, whereas in\nfactLSTM one weight is multiplied with one hidden layer.\nComparing both methods, fLHUC-LSTM has the advantage\nthat it needs less parameters, because it does not require\nmultiple output layers. However, our proposed method has\nthe advantage that individual factors can cover more domain\nspeciﬁc information or information speciﬁc to certain do-\nmains.\n6. Auxiliary Features for Feature Based Domain Adap-\ntation\nThere exist many possible features for domain adaptation.\nThe simplest one would be a one hot encoding of the do-\nmain. In prior research, topic information from an LDA\ntopic model was the dominant feature for feature based do-\nmain adaptation. LDA provides a topic distribution over\nmultiple topics contained in a document. That means, com-\npared to a domain label, LDA provides a mixture of multiple\ntopics and it is a continuous feature. Because LDA is such a\ncommon feature, we also use it in our experiments.\nAn important parameter to address for LDA is the spec-\niﬁcation of what should be regarded as a document. In case\nthe training corpus consists of a single ﬁle, we regard a\nchunk of sentences as a single document for LDA training in\nthe experiments. If the corpus can be separated into di ﬀerent\ntalks, we regard one talk as a single document. In order to\ngenerate LDA features for each word, we estimate the topic\ndistribution for a ﬁxed size window of past words.\nThe LSTM itself captures some kind of context infor-\nmation, however, the cell state in the LSTM (which we re-\nlate to the context stored in an LSTM) is processed by an\nexponentially decaying function. That means, context fur-\nther in the past has less inﬂuence on the current state than\nthe more recent context. On the other hand side, LDA ne-\nglects word order and therefore all words in a document will\n602\nIEICE TRANS. INF. & SYST., VOL.E102–D, NO.3 MARCH 2019\ncontribute equally when calculating a topic distribution.\n7. Experiments\n7.1 Dataset\n7.1.1 Penn Treebank\nFor the experiments, we used two di ﬀerent datasets. First\nis the well-known PTB [23], which has roughly 0.9M words\nin the training set and a vocabulary size of 10K. The dataset\nconsists of articles from the Wall Street Journal covering\ndiﬀerent topics, such as, politics and ﬁnance. We used the\nstandard pre-processing, that is, sections 0-20 as training,\n21-22 as validation and 23-24 as test set.\n7.1.2 TED Talks\nOur second corpus consists of TED talks and the TED-\nLIUM corpus [24] for our ASR experiments. For our ASR\nsystem, we used the standard Kaldi [40] recipe. The data\nprovided in TED-LIUM is very small to train an LM. In or-\nder to have a larger training set, we crawled subtitles from\nother TED talks. Our ﬁnal LM training set consisted of 2494\ntalks and had a size of 5.1M tokens with a vocabulary size of\n73K words. We replaced every word which appeared only\nonce in the training data by an unknown token. This resulted\nin an e ﬀective vocabulary size of 43K words.\nWe generated our own validation and test sets from the\noriginal subtitles in the same way as our 5.1M word training\nset. Our validation and test sets used the same data as in\nthe IWSLT 2011 evaluation campaign [41]. We will report\nresults for our subtitle test set and TED-LIUM’s test set in\nthe experimental result section.\nThe TED-LIUM validation and test set were re-\ntranscribed from the original lectures in order to have verba-\ntim transcriptions. This introduced a mismatch with the sub-\ntitle based sets. We summarised some key-ﬁgures about our\nsubtitle-based and the TED-LIUM test set in Table 1. We\nfurther provide a histogram of the di ﬀerent sentence lengths\nin Fig. 3. As seen from Fig. 3, the sentence length for both\nevaluation sets is very di ﬀerent. As a result, the unigram\nprobability of the end of sentence symbol is di ﬀerent in the\nsubtitle based test set ( P(EoS) =0.1) and in the TED-LIUM\ntest set ( P(EoS) =0.04).\n7.2 LDA Training and Topic Estimation\nFor training our LDA model we applied two di ﬀerent\nschemes depending on the dataset. For PTB, we used the\nsame processing scheme as in [15]. That means, we divided\nthe training set in chunks of 10 utterances and each of these\nchunks was regarded as a single document. For the TED\ntalks it was not necessary to make this segmentation because\nthe dataset consists of di ﬀerent talks and we could use each\ntalk as a separate document.\nBefore training the LDA, we removed a list of common\nTable 1 Comparison of subtitle and TED-LIUM test sets.\nsentence length (words) V oc size length\nmin max mean var (words)\nsubtitle 2 19 9 8.88 3638 30168\nTED-LIUM 2 122 25 209.39 3568 28655\nFig. 3 Histogram of sentence lengths in subtitle and TED-LIUM evalu-\nation sets.\nstop words, as well as, high and low frequency words. This\npre-processing has only been applied in order to train the\nLDA and to compute the LDA features. We did not apply\nthis processing to the text corpora when training our LMs.\nThe LDA implementation for our experiments was the one\nprovided in scikit-learn [42].\nIn order to calculate the LDA features for each word\nin the datasets, we used LDA features extracted from a slid-\ning window covering the previous 50 words in case of PTB\nand 200 words in case of TED. The LDA features represent\nthe topic distribution over this sliding window. For N-best\nrescoring, we calculated the LDA features from the recog-\nnition result. In case the sliding window extended over the\ncurrent utterance, it kept the content of the one-best result.\nSimilarly, the state of the LSTM for the one-best hypothesis\nis kept across sentences.\n7.3 Training Parameters for NN-LMs\nAs common part to all NN-LMs in the experiments, we used\na single layer LSTM with 300 units. The LHN also had\n300 units. For PTB, we trained all networks for 20 epochs\nand for TED we optimised the number of epochs for each\nmodel on the validation set WER. The mini-batchsize and\nbackpropagation through time (BPTT) length were tuned on\nPTB to give a good compromise of training time and model\nPPL. We chose a mini-batchsize of 128 and BPTT length\nof 20 words. We set the initial learning rate to 0.1 and we\nused the AdaGrad [43] as optimiser. Gradients were clipped\nto an L2-norm of ﬁve. If the validation PPL improvement\nwas less than 0.1% within one epoch, the learning rate was\nhalved. In all our models, we applied dropout [44] with a\ndropout ratio of 50%. As for [38], in fLHN-LSTM, we\ninitialised the weight matrix of the linear layer connect-\ning the LSTM and the LHN ( V\n(w)) with the identity ma-\ntrix. We implemented all NN-LMs with the open source\ntoolkit Chainer [45]. We used Nvidia GTX 1080Ti GPUs\nwith CUDA v8 and CUDNN v6 to train all NN-LMs.\nHENTSCHEL et al.: FEATURE BASED DOMAIN ADAPTATION FOR NEURAL NETWORK LANGUAGE MODELS WITH FACTORISED HIDDEN LAYERS\n603\nTable 2 PPL on the validation set of PTB for di ﬀerent numbers of\nfactorised hidden layers versus di ﬀerent LDA dimensions (LSTM-LM\n105.66). The number in brackets with factLSTM gives the number of fac-\ntors used.\nModel\nLDA\ntopics 30 40 50 60\ncontLSTM 118.72 117.83 122.72 121.74\nfLHN-LSTM 103.99 105.59 107.16 105.76\nfLHUC-LSTM 103.35 104.01 104.76 104.80\nfactLSTM (5) 105.06 105.55 105.93 106.08\nfactLSTM (10) 102.15 102.11 102.81 101.02\nfactLSTM (20) 102.92 102.80 101.54 101.06\nfactLSTM (30) 101.36 102.64 102.24 100.91\nfactLSTM (40) 103.75 102.69 101.75 100.69\nTable 3 PPLs for baseline, best fLHN-LSTM and factLSTM model on\nthe validation and test set of PTB.\nModel LDA topics val test\ntrigram — 182.16 171.68\nLSTM — 105.66 98.94\ncontLSTM 40 117.83 109.00\nfLHN-LSTM 30 103.99 97.42\nfLHUC-LSTM 30 103.35 95.90\nfactLSTM (40) 60 100.69 94.99\n7.4 Penn Treebank Results\nAt ﬁrst, we show PPL results for PTB. For the domain adap-\ntation methods, we analysed the behaviour with di ﬀerent\nnumbers of LDA topics. Table 2 shows the results for LDA\ntopic numbers from 30 to 60 for contLSTM, fLHN-LSTM,\nfLHUC-LSTM and factLSTM. contLSTM did not show any\nimprovement over an LSTM-LM (validation PPL LSTM-\nLM: 105.66). This result is contrary to the one in [35],\nbut the authors used only 20 LSTM units and we used 300\nLSTM units in our experiments. In the case of a small num-\nber of hidden units, the LDA features might act as a context\nmemory that the network can use. Another reason could be\nthat it takes more epochs for this model to reach a lower\nPPL. fLHN-LSTM and fLHUC-LSTM were able to reduce\nthe PPL compared to the LSTM-LM baseline in some cases.\nThe performance of factLSTM, depends on the number\nof LDA topics as well as the number of factors. Keeping the\nnumber of factors constant, in general the PPL decreased\nwhen the LDA size was increased. When the LDA size is\nkept constant and the number of factors is increased, the PPL\nwas also reduced. However, with a small number of factors,\nthe PPL is not much reduced compared to the LSTM-LM\nbaseline. Also, choosing a large number of factors (larger\nthan 10) did not yield much further PPL reduction on the\nPTB dataset. This might be due to the small size of PTB.\nIn Table 3 we show the best PPL results for each model.\nWe estimated a baseline trigram LM with Kneser-Ney [46]\nsmoothing using the SRILM toolkit [47]. The PPLs we\nshow for the neural network LMs are, however, the val-\nues obtained using only the NN-LM without trigram in-\nterpolation. We used 40 LDA topics with contLSTM, 30\nLDA topics with fLHN-LSTM and fLHUC-LSTM and 60\nFig. 4 Convergence on the validation set over 80 epochs for di ﬀerent\nmodels on TED dataset.\ntopics for factLSTM with 40 factors. The number of pa-\nrameters were around 6.7M for contLSTM, around 6.5M\nfor fLHN-LSTM and fLHUC-LSTM, and around 123M\nfor factLSTM. The baseline LSTM-LM had around 6.4M\nparameters. The training time of LSTM-LM, contLSTM,\nfLHN-LSTM and fLHUC-LSTM was about 1h. factLSTM\ntook 12h to train. contLSTM again had the overall high-\nest PPL among the NN-LMs. fLHN-LSTM had a 1% lower\nPPL compared to the LSTM-LM baseline (relative improve-\nment). fLHUC-LSTM reduced the PPL by 2% and 3% rela-\ntive to the LSTM-LM baseline on the validation and test set,\nrespectively. Overall, out of all domain adaptation meth-\nods, our proposed factLSTM had the highest relative PPL\nreduction compared to the LSTM-LM baseline. factLSTM\nimproved 5% on the validation and 4% on the test set.\n7.5 TED Talk Results\nOn the TED talks dataset, we trained each model until\nconvergence according to the training scheme described in\nSect. 7.3. The convergence of the validation PPL over 80\nepochs is shown in Fig. 4. LSTM-LM reached its minimum\nafter roughly 40 epochs. contLSTM showed a slight PPL re-\nduction compared to LSTM-LM. fLHUC-LSTM had almost\nthe same convergence as contLSTM. Both reached the min-\nimum validation PPL around 50 epochs. fLHN-LSTM con-\nverged after around 40 epochs and convergence appeared to\nbe faster than for LSTM-LM. factLSTM had the lowest PPL\nof all models after 20 epochs and reached the lowest PPL of\nall models at 80 epochs. The converged models had the low-\nest PPLs on the subtitle validation and test sets. However,\nPPL is not directly related with WER. In order to ﬁnd the\nmodel with the lowest WER on TED-LIUM, we performed\n100-best rescoring every ﬁve epochs around the point where\nthe PPL converged. Subsequently, we chose for each adap-\ntation method the model with the lowest WER on the vali-\ndation set results and provide PPL and WER results for this\nmodel.\nTable 4 shows our results for the TED-LIUM dataset.\nFor the TED talks, we show PPL results for the trigram\nwhich is distributed with the Kaldi recipe [25]. The PPLs\nare, as for the PTB results, without any N-gram interpola-\ntion. We used the trigram to interpolate LM scores in 100-\nbest rescoring. The corresponding interpolation weights for\n604\nIEICE TRANS. INF. & SYST., VOL.E102–D, NO.3 MARCH 2019\neach LM and the trigram were optimised on the TED-LIUM\nvalidation set. The interpolation weight of N-gram and NN-\nLMs was mainly between [0 .9,1] for the NN-LM. There-\nfore, the WER after rescoring was for the main part deter-\nmined by the NN-LM. All NN-LMs with domain adaptation\nused LDA features from 50 topics and a window size of 200\nwords.\nThe PPL for the trigram is considerably higher than for\nthe NN-LMs on the subtitle and TED-LIUM test sets. This\ncan be explained by the fact that the trigram is trained on\nout-of-domain data. A trigram trained on our own training\nset had a considerably lower PPL of 106.58 on the subtitle\ntest set. The baseline LSTM-LM reduced PPL by 67% com-\npared to the trigram for the subtitle test set and 30% for the\nTED-LIUM test set. In 100-best rescoring, we decreased the\nWER by 20%. The baseline LSTM-LM had approximately\n26M parameters and training for 40 epochs took approxi-\nmately 4.5h.\ncontLSTM improved over LSTM-LM on the subtitle\ntest set. On the TED-LIUM evaluation set, however, the\nPPL was slightly higher than the LSTM-LM. In 100-best\nrescoring, the WER was also slightly higher than the base-\nline. The number of model parameters increased by 2M to\n28M and the training time increased by around 2h. With\nfLHN-LSTM PPL reduced by 14% and 18% compared to\nan LSTM-LM for the subtitle based and TED-LIUM test\nsets, respectively. In 100-best rescoring, the WER showed\nslight improvement over the LSTM-LM baseline for the val-\nidation set, but it was equal on the test set. fLHUC-LSTM\nhad a slightly higher PPL than fLHN-LSTM on the subtitle\nbased test set and TED-LIUM test set. The WER showed a\nslight but not signiﬁcant reduction on the baseline. fLHN-\nLSTM and fLHUC-LSTM had 105K more parameters than\nan LSTM-LM, but the training time was about the same.\nFor factLSTM we used 15 factors due to the limitation\nof our GPU memory size. factLSTM had 207M parameters\nand the training time was 67h for 70 epochs. Our proposed\nmethod did increase the number of model parameters as well\nas the training time compared with the other methods, how-\never, we did not perform any particular optimisation of the\nimplementation to speed up the computation on GPUs. On\nthe TED-LIUM test set, this method showed signiﬁcantly\nlower PPL compared to all other methods, that is, 39% lower\nthan LSTM-LM and 21% lower than fLHN-LSTM. In 100-\nbest rescoring, the WER was 3% lower (relative reduction)\nTable 4 PPL and WER for our own subtitle based test set and TED-\nLIUM with 50 LDA topics, a 200-word window size and factLSTM with\n15 factors. The trigram result represents the 1-best result and the results for\nthe neural network LMs are for 100-best rescoring.\nModel Training Test PPL WER [%]\nEpochs subtitle TED-LIUM val test\ntrigram — 156.41 222.05 16.3 15.1\nLSTM 40 51.98 156.29 14.2 12.1\ncontLSTM 45 47.34 165.69 14.5 12.3\nfLHN-LSTM 40 44.72 127.99 14.1 12.1\nfLHUC-LSTM 40 47.69 144.70 14.0 11.9\nfactLSTM 70 31.74 101.04 13.8 11.6\ncompared to an LSTM-LM on the test set.\nWe performed a matched-pair signiﬁcance test of all\nmethods and it showed a signiﬁcant di ﬀerence of factLSTM\ncompared with the LSTM-LM baseline at a signiﬁcance\nlevel of 0.1%. All other methods did not show a signiﬁ-\ncant improvement on the baseline. These results show that\nfeature based adaptation can improve both PPL and ASR\nrescoring performance. Our proposed factLSTM achieved\nsuperior performance in general compared to other ap-\nproaches for exploiting auxiliary features. This is however\nat the expense of using a larger amount of parameters.\n8. Discussion\n8.1 Impact of LDA Window Size\nFollowing our main PPL and WER results on PTB and TED\ntalks, we provide some further discussion on how LDA fea-\ntures can a ﬀect NN-LMs. At ﬁrst, we discuss the features\nthemselves. As we mentioned in Sect. 6, the LSTM itself\ncaptures context information and we believe that the LDA\ncan capture di ﬀerent context information. In order to inves-\ntigate when the information provided by the LDA can have\na beneﬁcial e ﬀect on the model performance, we modiﬁed\nthe window length to calculate the LDA features.\nFigures 5 and 6 show the validation set PPL for PTB\nand TED, respectively, where we modiﬁed the LDA win-\ndow size. For PTB we chose window sizes from 50 to 500\nFig. 5 Comparison of di ﬀerent context window sizes versus validation\nPPL for PTB with LDA features from 30 topics.\nFig. 6 Comparison of di ﬀerent context window sizes versus PPL for the\nTed talks subtitle validation set with features from 50 LDA topics.\nHENTSCHEL et al.: FEATURE BASED DOMAIN ADAPTATION FOR NEURAL NETWORK LANGUAGE MODELS WITH FACTORISED HIDDEN LAYERS\n605\nwords. For TED we chose 50 to 1000 words because of the\nlarger training data. For the ASR results in Sect. 7.5 we op-\ntimised the training time for each model to obtain the lowest\npossible WER. However, here we are only interested in PPL\nand consequently all NN-LMs in Fig. 6 were trained for 40\nepochs.\nFor PTB the trend is that all models reached a mini-\nmum PPL with a window size between 100 and 200 words.\nUsing a smaller or a longer window the PPL of domain-\nadapted models usually increased. This result indicates that\nshorter windows are well covered by the LSTM itself and\nlonger windows contain too imprecise information. For\nTED also a context window size around 200 words gave\na good result. In general, the LMs using domain adap-\ntation improved on both corpora over a baseline LSTM-\nLM. This result suggests that there is information contained\nin the LDA features, which is not extracted by the LSTM\nitself.\nRegarding the topic features themselves, during the ex-\nperiments it showed that the adaptation performance is de-\npendent on the quality of the topic model and the features\nestimated from it. Estimating a good LDA topic model will\nimprove the feature quality and this is important for model\nperformance. We believe that all models can beneﬁt from\nbetter topic features, which might be derived from di ﬀerent\ntopic models than LDA. Comparing the training set sizes\nof TED talks and PTB, for TED talks we have roughly ﬁve\ntimes more data. We think that this helped estimating bet-\nter LDA topics, which were more e ﬀective for the LM and\ncould lead to a larger PPL reduction.\n8.2 Error Corrections after Rescoring\nWe conducted some more in-depth analysis of the recogni-\ntion results before and after rescoring. Looking at the results\nfor di ﬀerent model architectures, the NN-LMs had less er-\nrors with functional words. In the 1-best decoding result,\nthese words were often left out or misrecognised. In cases\nwith a large number of errors, rescoring with the NN-LMs\ndid not reduce the number of errors considerably. In this\ncase the output from the speech recogniser was the limiting\nfactor. In some cases we were able to make a relation be-\ntween the corrected error and a topic. We show one such\nexample where we compare the ground truth utterance, the\nresult after 1-best decoding, rescoring with LSTM-LM, and\nrescoring with factLSTM in Table 5.\nWe also investigated the probabilities each NN-LM as-\nsigned to the di ﬀerent hypothesises for one utterance more\nclosely. Our ﬁndings did not show a constant o ﬀset in the\nprobabilities between di ﬀerent models. We therefore con-\nclude that each model architecture makes di ﬀerent use of\nthe information provided by the LDA features.\n8.3 Analysis of Correlation Between LDA Features and\nFactor Weights\nFor our proposed method factLSTM, we analysed what\nTable 5 N-best hypothesis comparison for selected utterance from TED-\nLIUM test set.\nREF: this really solves the problem i’ve got a picture here of a\nplace in kentucky this is the left over the ninety nine percent\nwhere they’ve taken out the PART THEY BURN NOW so\nIT’S called depleted uranium that would power the U S\n1-best: this really solves the problem i’ve got a picture here HA VE a\nplace in kentucky this is the left over the ninety nine percent\nwhere they’ve taken out the **** **** PARTY BERNAU so\n**** called depleted uranium that would power the * U.S.\nLSTM-LM: this really solves the problem i’ve got a picture here of a\nplace in kentucky this is the left over the ninety nine percent\nwhere they’ve taken out the **** **** PARTY BERNAU so\n**** called depleted uranium that would power the * U.S.\nfactLSTM: this really solves the problem i’ve got a picture here of a\nplace in kentucky this is the left over the ninety nine percent\nwhere they’ve taken out the part they burn now so it’s called\ndepleted uranium that would power the * U.S.\nFig. 7 PCA plots of LDA features (a) and factor weights (b) for the sub-\ntitle based TED talks test set. Each color represents a di ﬀerent talk in the\ntest set.\nfactor weights the network learns from the LDA topic fea-\ntures. Figure 7 (a) shows a PCA plot of the LDA features\nfor all talks in the TED subtitle based test set. Each colour\nrepresents one talk and one dot corresponds to one feature\nor factor weight, respectively. The plot reveals that the LDA\nfeatures are mainly distributed along three axes. The PCA\nof the factor weights for the same talks is shown in Fig. 7 (b).\nThe distribution is very di ﬀerent from the LDA feature plot.\nThe network learns a mapping of the features, which helps\nto improve the prediction for the next word. In particular,\nsome factor weights for di ﬀerent talks appear to be mapped\nto similar regions in the PCA space.\nIn addition to the PCA plots, Fig. 8 (a) and (b) show\nthe LDA features and corresponding factor weights for part\n606\nIEICE TRANS. INF. & SYST., VOL.E102–D, NO.3 MARCH 2019\nFig. 8 Comparison of LDA features (a) and factor weights (b) for part of\nthe test set (x axis corresponds to word index).\nTable 6 Test PPL on TED talks for fLHN-LSTM with di ﬀerent LHN\nsizes after training for 70 epochs.\nLHN size 300 600 1200\nParameters 26M 39M 65M\nTraining time 9h 11h 16h\nsubtitle PPL 45.25 41.98 42.79\nTED-LIUM PPL 133.67 131.50 135.04\nof the test set. As can be seen in the ﬁgures, there are dis-\ntinct regions of high weights for the LDA features and high\nfactor weights which correspond to each other. Interesting\nare the highlighted regions in Fig. 8 (a) and (b). In this in-\nterval, there is no distinct LDA feature with high intensity\nbut factor one exhibits a high activation. The same factor\nkeeps a high activation after word index 16000 which can be\nlinked to one topic with high activation in this region. This\nis a good illustration how the auxiliary network learns to\ncombine di ﬀerent topic features into a single factor weight.\nThe corresponding factor can then learn common informa-\ntion among these di ﬀerent topics.\n8.4 Parameter Size Comparison\nOur proposed factLSTM has more parameters than the other\nmodels. We investigated if increasing the number of pa-\nrameters in fLHN-LSTM can lead to a similar PPL as our\nproposed method. Table 6 shows PPL results for fLHN-\nLSTMs with di ﬀerent LHN sizes after 70 epochs for the\nsubtitle based evaluation set of TED talks. We also give the\nnumber of parameters and the training time for each model.\nThe PPL did not change considerably, even if we increased\nthe LHN size four fold. As comparison, our proposed\nfactLSTM had a PPL of 31.74 for the subtitle test set and\n101.04 for the TED-LIUM test set. Our model had 207M\nparameters and the training time for 70 epochs was 67h.\nThese numbers conﬁrm that not only the increased number\nof parameters may cause the performance improvement of\nour proposed factLSTM.\n9. Conclusion\nWe provided a comparison and discussion of di ﬀerent fea-\nture based domain adaptation techniques for NN-LMs and\nour proposed factLSTM. Feature based methods have in\ncomparison to model based adaptation the advantage that\nthey can be applied in an unsupervised manner. In practice,\nsuch methods are preferable because they do not require any\ndomain information in the training, validation and test data\nwhich can only be made by experts. The LDA topic model\nwe used in our comparison can also be trained in an unsu-\npervised manner.\nThe methods we compared to our proposed one use two\ndiﬀerent strategies, that is, domain adaptation via an addi-\ntive bias and a multiplicative gating function. Bias based\nadaptation with fLHN-LSTM was originally proposed for\nmodel based adaptation but, as the experiments showed, it\ncan also be successfully applied in feature based adaptation.\nThe PPL and WER were higher than factLSTM but it im-\nproved considerably over an LSTM-LM baseline. The ad-\nvantage over factLSTM is that the number of parameters is\nmuch smaller and the convergence speed was the same as\nthe LSTM-LM baseline on the TED talks dataset. Simple\nbias adaptation with contLSTM did not show a clear ten-\ndency to improve over an LSTM-LM in our experiments.\nModel adaptation with LHUC improved over the baseline\nLSTM, but achieved neither the same PPL as factLSTM nor\na similar WER. On the datasets used in our experiments, our\nproposed approach factLSTM outperformed other methods\nfor feature based domain adaptation.\nReferences\n[1] R. Rosenfeld, “Two decades of statistical language model-\ning: Where do we go from here?,” Proc. IEEE, vol.88, no.8,\npp.1270–1278, 2000.\n[2] J.R. Bellegarda, “Statistical language model adaptation: review\nand perspectives,” Speech communication, vol.42, no.1, pp.93–108,\n2004.\n[3] Y .C. Tam and T. Schultz, “Dynamic language model adaptation\nusing variational bayes inference,” Ninth European Conference on\nSpeech Communication and Technology, 2005.\n[4] A. Heidel, H.A. Chang, and L.S. Lee, “Language model adaptation\nusing latent dirichlet allocation and an e ﬃcient topic inference algo-\nrithm,” Eighth Annual Conference of the International Speech Com-\nmunication Association, 2007.\n[5] Y . Liu and F. Liu, “Unsupervised language model adaptation via\ntopic modeling based on named entity hypotheses,” IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp.4921–4924, IEEE, 2008.\n[6] S. Watanabe, T. Iwata, T. Hori, A. Sako, and Y . Ariki, “Topic track-\ning language model for speech recognition,” Computer Speech &\nLanguage, vol.25, no.2, pp.440–461, 2011.\n[7] M.A. Haidar and D. O’Shaughnessy, “Topic n-gram count language\nmodel adaptation for speech recognition,” Spoken Language Tech-\nnology Workshop (SLT), pp.165–169, IEEE, 2012.\n[8] R. Rosenfeld, “A maximum entropy approach to adaptive statistical\nlanguage modelling,” Computer Speech and Language, vol.10, no.3,\npp.187–228, 1996.\nHENTSCHEL et al.: FEATURE BASED DOMAIN ADAPTATION FOR NEURAL NETWORK LANGUAGE MODELS WITH FACTORISED HIDDEN LAYERS\n607\n[9] A.L. Berger, V .J.D. Pietra, and S.A.D. Pietra, “A maximum entropy\napproach to natural language processing,” Computational linguis-\ntics, vol.22, no.1, pp.39–71, 1996.\n[10] S. Della Pietra, V . Della Pietra, R.L. Mercer, and S. Roukos, “Adap-\ntive language modeling using minimum discriminant estimation,”\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp.633–636, IEEE, 1992.\n[11] S. Khudanpur and J. Wu, “Maximum entropy techniques for ex-\nploiting syntactic, semantic and collocational dependencies in lan-\nguage modeling,” Computer Speech & Language, vol.14, no.4,\npp.355–372, 2000.\n[12] T. Alum ¨ae and M. Kurimo, “Domain adaptation of maximum en-\ntropy language models,” ACL, pp.301–306, Association for Com-\nputational Linguistics, 2010.\n[13] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural prob-\nabilistic language model,” Journal of machine learning research,\nvol.3, pp.1137–1155, 2003.\n[14] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. ˇCernock`y, and S.\nKhudanpur, “Recurrent neural network based language model,”\nINTERSPEECH, pp.1045–1048, 2010.\n[15] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock´y, and S. Khudanpur,\n“Extensions of recurrent neural network language model,” IEEE In-\nternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp.5528–5531, IEEE, 2011.\n[16] Y . Bengio, P. Simard, and P. Frasconi, “Learning long-term de-\npendencies with gradient descent is di ﬃcult,” IEEE Trans. Neural\nNetw., vol.5, no.2, pp.157–166, 1994.\n[17] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,”\nNeural computation, vol.9, no.8, pp.1735–1780, 1997.\n[18] M. Sundermeyer, R. Schl ¨uter, and H. Ney, “LSTM neural networks\nfor language modeling,” INTERSPEECH, pp.194–197, 2012.\n[19] D.M. Blei, A.Y . Ng, and M.I. Jordan, “Latent Dirichlet allocation,”\nJournal of machine Learning research, vol.3, pp.993–1022, 2003.\n[20] S.R. Gangireddy, P. Swietojanski, P. Bell, and S. Renals, “Unsu-\npervised adaptation of recurrent neural network language models,”\nINTERSPEECH, pp.2333–2337, 2016.\n[21] M. Delcroix, K. Kinoshita, A. Ogawa, C. Huemmer, and T.\nNakatani, “Context adaptive neural network based acoustic models\nfor rapid adaptation,” IEEE /ACM Trans. Audio, Speech, Language\nProcess., vol.26, no.5, pp.895–908, May 2018.\n[22] K. ˇZmol´ıkov´a, M. Delcroix, K. Kinoshita, T. Higuchi, A. Ogawa,\nand T. Nakatani, “Speaker-aware neural network based beam-\nformer for speaker extraction in speech mixtures,” INTERSPEECH,\npp.2655–2659, 2017.\n[23] M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini, “Building a\nlarge annotated corpus of english: The Penn Treebank,” Computa-\ntional linguistics, vol.19, no.2, pp.313–330, 1993.\n[24] A. Rousseau, P. Del ´eglise, and Y . Esteve, “TED-LIUM: an auto-\nmatic speech recognition dedicated corpus,” LREC, pp.125–129,\n2012.\n[25] W. Williams, N. Prasad, D. Mrva, T. Ash, and T. Robinson, “Scal-\ning recurrent neural network language models,” IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\npp.5391–5395, IEEE, 2015.\n[26] J. Park, X. Liu, M.J. Gales, and P.C. Woodland, “Im-\nproved neural network based language modelling and adaptation,”\nINTERSPEECH, 2010.\n[27] T. Alum ¨ae, “Multi-domain neural network language model,”\nINTERSPEECH, vol.13, pp.2182–2186, 2013.\n[28] O. Tilk and T. Alum ¨ae, “Multi-domain recurrent neural network lan-\nguage model for medical speech recognition,” Baltic HLT, pp.149–\n152, 2014.\n[29] R. Gemello, F. Mana, S. Scanzio, P. Laface, and R. De Mori, “Linear\nhidden transformations for adaptation of hybrid ANN /HMM mod-\nels,” Speech Communication, vol.49, no.10, pp.827–835, 2007.\n[30] P. Bell, M.J. Gales, T. Hain, J. Kilgour, P. Lanchantin, X. Liu, A.\nMcParland, S. Renals, O. Saz, M. Wester, et al., “The MGB chal-\nlenge: Evaluating multi-genre broadcast media recognition,” IEEE\nWorkshop on Automatic Speech Recognition and Understanding\n(ASRU), pp.687–693, IEEE, 2015.\n[31] P. Swietojanski and S. Renals, “Learning hidden unit contri-\nbutions for unsupervised speaker adaptation of neural network\nacoustic models,” Spoken Language Technology Workshop (SLT),\npp.171–176, IEEE, 2014.\n[32] L. Samarakoon and K.C. Sim, “Subspace LHUC for fast adap-\ntation of deep neural network acoustic models,” INTERSPEECH,\npp.1593–1597, 2016.\n[33] T. Mikolov and G. Zweig, “Context dependent recurrent neural net-\nwork language model,” Spoken Language Technology Workshop\n(SLT), pp.234–239, IEEE, 2012.\n[34] X. Chen, T. Tan, X. Liu, P. Lanchantin, M. Wan, M.J. Gales, and P.C.\nWoodland, “Recurrent neural network language model adaptation\nfor multi-genre broadcast speech recognition,” INTERSPEECH,\n2015.\n[35] D. Soutner and L. M ¨uller, “Application of LSTM neural networks in\nlanguage modelling,” International Conference on Text, Speech and\nDialogue, pp.105–112, Springer, 2013.\n[36] J. Zhang, X. Wu, A. Way, and Q. Liu, “Fast gated neural domain\nadaptation: Language model as a case study,” International Con-\nference on Computational Linguistics (COLING), pp.1386–1397,\n2016.\n[37] M. Sundermeyer, H. Ney, and R. Schl ¨uter, “From feedforward to re-\ncurrent LSTM neural networks for language modeling,” IEEE /ACM\nTrans. Audio, Speech, Language Process., vol.23, no.3, pp.517–529,\n2015.\n[38] S. Deena, M. Hasan, M. Doulaty, O. Saz, and T. Hain, “Combin-\ning feature and model-based adaptation of RNNLMs for multi-genre\nbroadcast speech recognition,” INTERSPEECH, pp.2343–2347,\n2016.\n[39] M. Delcroix, K. Kinoshita, C. Yu, A. Ogawa, T. Yoshioka, and T.\nNakatani, “Context adaptive deep neural networks for fast acous-\ntic model adaptation in noisy conditions,” IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP),\npp.5270–5274, IEEE, 2016.\n[40] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian, P. Schwarz, J.\nSilovsky, G. Stemmer, and K. Vesely, “The Kaldi Speech Recog-\nnition Toolkit,” IEEE Workshop on Automatic Speech Recognition\nand Understanding (ASRU), IEEE, Dec. 2011.\n[41] M. Federico, L. Bentivogli, P. Michael, and S. Sebastian, “Overview\nof the IWSLT 2011 evaluation campaign,” International Workshop\non Spoken Language Translation (IWSLT), 2011.\n[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J.\nVanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\nE. Duchesnay, “Scikit-learn: Machine Learning in Python,” Journal\nof Machine Learning Research, vol.12, pp.2825–2830, 2011.\n[43] J. Duchi, E. Hazan, and Y . Singer, “Adative subgradient methods\nfor online learning and stochastic optimization,” Journal of Machine\nLearning Research, vol.12, pp.2121–2159, 2011.\n[44] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR.R. Salakhutdinov, “Improving neural networks by preventing co-\nadaptation of feature detectors,” arXiv preprint arXiv:1207.0580,\n2012.\n[45] S. Tokui, K. Oono, S. Hido, and J. Clayton, “Chainer: a next-\ngeneration open source framework for deep learning,” Workshop on\nMachine Learning Systems (LearningSys) in the Twenty-ninth An-\nnual Conference on Neural Information Processing (NIPS), 2015.\n[46] R. Kneser and H. Ney, “Improved backing-o ﬀfor m-gram language\nmodeling,” IEEE International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP), vol.1, pp.181–184, IEEE, 1995.\n[47] A. Stolcke, “SRILM – an extensible language modeling toolkit,”\nInternational Conference on Speech and Language Processing,\npp.901–904, 2002.\n608\nIEICE TRANS. INF. & SYST., VOL.E102–D, NO.3 MARCH 2019\nMichael Hentschel Michael Hentschel\nreceived the B.Sc. and M.Sc. degree in In-\nformation and Communication Technologies\nfrom Friedrich-Alexander University Erlangen-\nNuremberg, Germany in 2013 and 2015, re-\nspectively. He is currently a Ph.D. candidate\nat Nara Institute of Science and Technology,\nNara, Japan. His main research interests in-\nclude speech signal processing, acoustic mod-\nels and language models for automatic speech\nrecognition.\nMarc Delcroix Marc Delcroix received\nthe M.Eng. degree from the Free University of\nBrussels, Brussels, Belgium, and the Ecole Cen-\ntrale Paris, Paris, France, in 2003 and the Ph.D.\ndegree from the Graduate School of Information\nScience and Technology, Hokkaido University,\nSapporo, Japan, in 2007. He was a research as-\nsociate at NTT Communication Science Labora-\ntories from 2007-2008 and 2010-2012 and then\nbecame a permanent research scientist at the\nsame lab in 2012. He is also a visiting lecturer\nat the Faculty of Science and Engineering of Waseda University, Tokyo,\nJapan, since 2015. His research interests include robust multi-microphone\nspeech recognition, acoustic model adaptation, integration of speech en-\nhancement front-end and recognition back-end, speech enhancement and\nspeech dereverberation. He is a member of the IEEE Signal Processing\nsociety Speech and Language Processing Technical Committee. He is a\nsenior member of IEEE and a member of ASJ.\nAtsunori Ogawa Atsunori Ogawa received\nthe B.E. and M.E. degrees in information en-\ngineering, and the Ph.D. degree in information\nscience from Nagoya University, Aichi, Japan,\nin 1996, 1998, and 2008, respectively. Since\n1998, he has been with Nippon Telegraph and\nTelephone (NTT) Corporation. He has been en-\ngaged in researches on speech recognition and\nspeech enhancement at NTT Cyber Space Labo-\nratories and NTT Communication Science Lab-\noratories. He is a member of the IEEE, Interna-\ntional Speech Communication Association (ISCA), Institute of Electronics,\nInformation, and Communication Engineers (IEICE), Information Process-\ning Society of Japan (IPSJ), and Acoustical Society of Japan (ASJ). He\nreceived the ASJ Best Poster Presentation Awards in 2003 and 2006, re-\nspectively.\nTomoharu Iwata Tomoharu Iwata received\nthe B.S. degree in environmental information\nfrom Keio University in 2001, the M.S. de-\ngree in arts and sciences from the University of\nTokyo in 2003, and the Ph.D. degree in infor-\nmatics from Kyoto University in 2008. In 2003,\nhe joined NTT Communication Science Labo-\nratories, Japan. From 2012 to 2013, he was a\nvisiting researcher at Machine Learning Group,\nDepartment of Engineering, University of\nCambridge, UK. He is currently a senior re-\nsearch scientist at NTT Communication Science Laboratories, Kyoto,\nJapan. His research interests include data mining and machine learning.\nTomohiro Nakatani Tomohiro Nakatani re-\nceived the B.E., M.E., and Ph.D. degrees from\nKyoto University, Kyoto, Japan, in 1989, 1991,\nand 2002, respectively. He is a Senior Distin-\nguished Researcher (Supervisor) of NTT Com-\nmunication Science Laboratories, NTT Corpo-\nration, Kyoto, Japan. Since joining NTT Cor-\nporation as a researcher in 1991, he has been\ninvestigating audio signal processing technolo-\ngies for intelligent human-machine interfaces,\nsuch as dereverberation, denoising, source sepa-\nration, and robust ASR. Dr. Nakatani was a Visiting Scholar of the Georgia\nInstitute of Technology for a year from 2005 and he was a Visiting As-\nsistant Professor in the Department of Media Science, Nagoya University\nfrom 2008 to 2017. He was honored to receive the 1997 JSAI Conference\nBest Paper Award, the 2002 ASJ Poster Award, the 2005 IEICE Best Pa-\nper Award, the 2009 ASJ Technical Development Award, the 2012 Japan\nAudio Society Award, the 2015 IEEE ASRU Best Paper Award Honorable\nMention, and the 2017 Maejima Hisoka Award. He is a member of IEEE\nand ASJ.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8712736964225769
    },
    {
      "name": "Artificial neural network",
      "score": 0.580352246761322
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5733760595321655
    },
    {
      "name": "Domain adaptation",
      "score": 0.5638447403907776
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5275708436965942
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5261508822441101
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.49237358570098877
    },
    {
      "name": "Speech recognition",
      "score": 0.39669886231422424
    },
    {
      "name": "Natural language processing",
      "score": 0.3325112462043762
    },
    {
      "name": "Linguistics",
      "score": 0.175229012966156
    },
    {
      "name": "Psychology",
      "score": 0.06838345527648926
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}