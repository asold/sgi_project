{
  "title": "More A than I: Testing for Large Language Model Plagiarism in Political Science",
  "url": "https://openalex.org/W4411131794",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2636347939",
      "name": "Robert J. Keener",
      "affiliations": [
        "University of Tennessee at Knoxville"
      ]
    },
    {
      "id": "https://openalex.org/A2636347939",
      "name": "Robert J. Keener",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4385080756",
    "https://openalex.org/W6857725202",
    "https://openalex.org/W6921365304",
    "https://openalex.org/W2322987970",
    "https://openalex.org/W2218600849",
    "https://openalex.org/W6857925556",
    "https://openalex.org/W4388108480",
    "https://openalex.org/W4387519584"
  ],
  "abstract": "Abstract This article shows how the sudden introduction of large language models (LLMs) has allowed a sudden, significant increase in the ability of political science professionals to plagiarize their articles by prompting LLMs to write for them. Evidence of this is shown through a brief overview of the limitations of LLMs and by searching for words that are disproportionately used by the most popular LLM, ChatGPT, in peer-reviewed articles. What is found is a rapid spike in the use of words that are unremarkable except for their popularity in ChatGPT’s output as determined by an AI professional. This shows that this method can be used to indicate the likelihood of plagiarism in a given article. It then concludes with the limitations of this keyword detection method and recommendations for limiting LLM plagiarism in the field of political science as a whole.",
  "full_text": "The Profession\n.............................................................................................................................................................................................................................................................\nMore A than I: Testing for Large\nLanguage Model Plagiarism in\nPolitical Science\nRobert Keener,The University of Tennessee, Knoxville, USA\nABSTRACT This article shows how the sudden introduction of large language models\n(LLMs) has allowed a sudden, significant increase in the ability of political science\nprofessionals to plagiarize their articles by prompting LLMs to write for them. Evidence\nof this is shown through a brief overview of the limitations of LLMs and by searching for\nwords that are disproportionately used by the most popular LLM, ChatGPT, in peer-\nreviewed articles. What is found is a rapid spike in the use of words that are unremarkable\nexcept for their popularity in ChatGPT’s output as determined by an AI professional. This\nshows that this method can be used to indicate the likelihood of plagiarism in a given\narticle. It then concludes with the limitations of this keyword detection method and\nrecommendations for limiting LLM plagiarism in the field of political science as a whole.\nINTRODUCTION\nT\nhere has yet to be a full tally of exactly how seismic\nthe effects were of November 30, 2022, the first day\nChatGPT entered public use. ChatGPT is a large\nlanguage model (LLM) that uses AI to generate\nnormally accurate text. This almost immediately\nbecame a concern in that dishonest political science professionals\ncould now automate one of the most difficult and characteristic parts\nof academia— publishing generative peer-reviewed articles (Michels\n2023). This article will give background on how LLMs work, what\ntheir current limitations are, and how these limitations can be\nexploited to detect AI-generated articles. Then, this article shows this\n“keyword detection method” in action as it is used to show the\nsudden uptick of suspicious manuscripts between the years 2022\nand 2023— as ChatGPT, the most popular LLM, reached public use.\nUsing this keyword detection method, this article shows that\nthe spike in AI-generated manuscripts has almost certainly\nalready compromised the profession and risks the credibility of\nall academics if it is not regulated. This is accomplished by four\nmodels, going from the most general to the most specific, showing\nthat AI plagiarism is already a problem among peer-reviewed\narticles. Furthermore, this article ends with a discussion on the\nlimits of the keyword detection method and how it is not intended\nto be a catch-all solution but a tool in a belt of other AI detection\nmethods. Any reviewer who takes their job and their field seriously\nhas the responsibility to adapt to these large changes, so the final\ngoal of this article is to help them accomplish this task.\nTHEORY\nTechnology advances more rapidly over time, and it is incumbent\non academic institutions and professions to monitor these devel-\nopments to prevent dishonest practices. Major developments that\nhave been vehicles for dishonest practices have normally been\nused not among professionals but among students including the\nintroduction of smartphones, the use of Wiki platforms when\nwriting papers, and the use of programable calculators. Smart-\nphones became efficient ways to portably retrieve and organize\ndata but had to be limited to times outside of test-taking. Wiki\nplatforms are often used as fast ways to gain a background\nunderstanding of countless subjects, but it is understood that all\nthe information needs to be confirmed in more reputable plat-\nforms, and programable calculators are still fantastic ways to easily\ncompute bespoke formulas but must be screened to prevent\ncheating when test-taking.\nThe consistent pattern is that a new technology is introduced,\nits characteristics enhance both honest and dishonest production\n© The Author(s), 2025. Published by Cambridge University Press on behalf of American\nPolitical Science Association. This is an Open Access article, distributed under the terms\nof the Creative Commons Attribution licence (http://creativecommons.org/licenses/\nby/4.0), which permits unrestricted re-use, distribution and reproduction, provided the\noriginal article is properly cited.\nRobert Paul Keener is a PhD candidate in political science at the University of\nTennessee and an adjunct professor at Mars Hill University. He can be reached at\nrkeener4@vols.utk.edu .doi:10.1017/S1049096525000368 PS  October 2025 657\nof material, and over time academia adapts the technology in ways\nthat allow for honest production while limiting dishonest produc-\ntion. The use of LLMs in academic institutions is no different\nexcept that it presents a way for professionals in political science to\ncheat and remains a task that academia must work through. LLMs\nprovide a great helping hand, able to identify cases that meet\ncertain criteria that can enhance a study, as well as assisting in\nbrainstorming through problems/roadblocks in the material.\nAlthough many academic institutions have different defini-\ntions of plagiarism, when one defines it as simply“to steal and\npass off (the ideas or words of another) as one ’s own: use\n(another’s production) without crediting the source” (Merriam-\nWebster 2019) the line between plagiarism and acceptable use of\nLLMs is clear. Like Wiki sites, LLMs can provide good back-\nground information and brainstorming, but should not be used for\ndirect text development, nor should its background information/\nbrainstorming be used without verification from more reputable\nsources. The best first step to doing this is to rid the field of\ninappropriate use of the technology, in this case, that use being\nLLM plagiarism.\nCurrently, there is no 100% reliable test for the detection of AI\nin articles, as the entire purpose of AI is to mimic human behavior\nto a degree that seems natural, raising the question,“Why should\nwe bother to read something that no one bothered to write?” This\ndoes not mean that AI-written articles are without flaws. How\nChatGPT and other LLMs work is that they are presented with\nlarge amounts of data and then, through pattern recognition, use\nthat data to generate similar outputs on novel inputs. In the case of\npeer-reviewed political science research, it has been taught what\nan article looks like but not how to create one without quite a bit of\nguesswork (Stöffelbauer2023).\nThe outputs of ChatGPT have been accurate to a degree.\nC h a t G P T3 . 5w a sa b l et op a s sa nM B Ae x a mf r o mW h a r t o nS c h o o l\nof Finance (Rosenblatt2023), passed a CPA exam (Steinhardt2023),\nand can translate very well (White2022). A prior study has shown\nthat without the use of detection methods, LLM-generated articles\ncan easily be published in high-impact medical journals (Khlaif\n2023). In some cases, the only reason that undergraduate students\nwere caught using ChatGPT was because their answers were too\nwell-written for their level (Huang2023). For these reasons, it is safe\nto say that LLM accuracy is already established, but the detection of\nAI is still being developed. Turnitin is a popular AI detection tool\nthat many academics use, with 98% of higher education institutions\nhaving purchased access. In one study, researchers tested the efficacy\nof Turnitin, finding it to be inconsistent and unreliable, often\ndetermining AI influence in only 54% of the 100% AI-generated\nwork. Furthermore, there are issues with false positives, where work\nwritten before the release of ChatGPT is still identified as having AI\ninfluence (Perkins et. al.2023). Although the outputs are inconsis-\ntent, this does not mean that Turnitin is a bad tool, only not enough\non its own— which is where the benefits of the keyword detection\nmethod can be introduced.\nAccuracy issues in AI are easily spotted by diligent reviewers\nnormally due to consistency issues, “miraging” citations, and\ninaccurate data (Howell, Baker, and Stylianopoulos2023), but\nwhat has not been addressed, and what this article seeks to\ndemonstrate, is the use of keywords to find articles that are\nplagiarized, in the sense that they claim to be written by pro-\nfessionals but are in fact AI-generated. This is now easily detected,\nbut without a high degree of certainty. Similar to how later iterations\nof AI will exaggerate the remaining flaws in other AI-generated\ncontent, the most popular iteration of ChatGPT, ChatGPT 3.5, has\nflaws that can be used to detect likely AI-generated content. These\nflaws are words that were overrepresented in the data that“fed”\nChatGPT 3.5. If an article uses an abundance of words that are not as\ncommonly used outside of AI, there is enough suspicion to cast\ndoubt on the integrity of the work itself.\nWhen LLMs like ChatGPT are“fed” these vast amounts of\ndata, they act as a references when generating new content. For\ninstance, if ChatGPT were fed data mainly from the United King-\ndom, even content produced by someone living and working in the\nUnited States would include spellings like“colour,”“ honour,” and\n“mum.” Therefore, in this hypothetical world, if an American\nEnglish speaker were to submit an article for publication that was\nwritten in British English, it would be a strong indicator that the\nwork is likely AI-generated and therefore plagiarized.\nThe keyword detection method used in this article applies the\nsame logic but to a wider degree. The information that was“fed”\ninto the LLM has an overrepresentation of certain keywords and\nphrases, which leads to the use of these words being overrepresented\nin articles that were written during the first year that ChatGPT was\nwidely used, 2023, than during the previous 2022. The top five most\noverrepresented words by ChatGPT writing, relative to regular\nhuman writing, are“delve,”“ tapestry,”“\nvibrant,”“ landscape,” and\n“realm” (Li 2024)\nThis introduces the hypothesis:\nH1: After 2022, there will be a substantive increase in the amount\nof political science writing that uses keywords that are overrepre-\nsented in AI use.\nThis is comparable to the sudden spike of articles using the\nword “terrorism” after the September 11 Attacks, shown infigure 1.\nAfter a dramatic, and tragic, event occurred, the study of terrorism\nwas quickly a concern for many academics. Unlike the sudden use\nof keywords in large language models, after September 11, 2001,\nthe study of terrorism was quickly relevant to the real world and\nthere was a market demand for understanding the phenomenon of\nterrorism. The word most overrepresented in the most popular\nLLM is“delve” (Li 2024). As shown infigure 2, the use of the word\n“delve” increased by roughly 2.7 times between the years 2022 and\n2023. For reference, the use of the word“terrorism” increased by\nless than 2.25 times, despite“terrorism” being relevant to the real\nworld and “delve” only being a word that ChatGPT uses more\naverage than humans.\nUsing this keyword detection method, this article shows that the spike in AI-generated\nmanuscripts has almost certainly already compromised the profession and risks the\ncredibility of all academics if it is not regulated.\nThe Profession: More A than I\n.............................................................................................................................................................................................................................................................\n658 PS  October 2025\nMETHOD\nThis study uses the keyword method on peer-reviewed political\nscience articles. These articles are sourced from the database Open-\nAlex (n.d.), an open-access catalog of over 209 million scientific\ndocuments, allowing for keyword searches in said documents to\nchart the popularity of trends over time (Keener2025). As demon-\nstrated in figure 2, one can see how it charts the popularity of\narticles on a subject and how the popularity of said subject can\ngrow rapidly after major events. The most popular LLM, ChatGPT,\nwas made available for public use on November 30, 2022, giving\nmost studies written and published in 2022 little to no time to use\nChatGPT, and it can be argued that 2022 was the last year where\nLLM plagiarism was not a concern. This leads to the importance of\nthe years 2022 and 2023 on the four charts shown below.\nThe results are shown in four charts, the first one being a\ngeneral baseline as to what the output of political science articles\nare, simply searching for“political science” articles on OpenAlex.\nThe second chart captures the increase of a single keyword. To\nFigure 1\nAnnual Use of the Word“Terrorism” in Academic Articles\n18000\n16000\n14000\n12000\n10000\n8000\n6000\n4000\n2000\n0\n1995 2000 2005 2010 2015\nFigure 2\nAnnual Use of the Word“Delve” in Academic Articles\n100000\n90000\n80000\n70000\n60000\n50000\n40000\n30000\n20000\n10000\n2000 2005 2010 2015 2020 2025\n0\n.............................................................................................................................................................................................................................................................\nPS  October 2025 659\nremain as parsimonious as possible, it only shows the increase of\nthe most commonly overused word in ChatGPT,“delve,” and how\noften it is used in political science articles. The final two models\nshow the use of the top three words most overused by ChatGPT\n(“delve,”“ tapestry,” and “vibrant”) in political science articles, and\nthe final model does the same, but with the addition of the fourth\nand fifth most common keywords (“landscape” and “realm”).\nThere is no formal consensus as to why these are the most over-\nused words in ChatGPT, but almost as soon as ChatGPT was\nreleased different hobbyists and bloggers have made lists of words\nand phrases they find particularly overused, and these lists have\nbeen aggregated by Dan Li, the CEO of an AI company, PlusDocs,\nto make his own list of AI“watchwords,” as his own platform has\nthe incentive to avoid resembling ChatGPT as best as he and his\nemployees can (Li2024).\nIt is important to note that unlike the use of the word\n“terrorism” after September 11, 2001, these keywords do not have\nany significance in major scholarly work in the field of political\nscience, nor are they words that are trendy to humans in other\ncircumstances, such as slang or cultural references, only that they\nare overrepresented in ChatGPT’s outputs relative to human-\ngenerated writing.\nRESULTS\nAs expected,figure 3is necessary to this study precisely for its lack\nof significance. If there had been a sudden similar increase in\narticles that referenced political science, a dramatic increase in the\nkeywords would be expected, as it would not mean they are\noverrepresented in the literature but only growing alongside\nit. As demonstrated infigure 3, the amount of political science\narticles increased slightly, meaning that if there was no plagiarism\nand all things constant, there should be a similar increase in the\nkeywords, the same rate of usage, but drawn from a larger well.\nFigure 4shows that this is not the case and that between the\nyears 2022 and 2023 there was an increase in articles using the\nwords “political science” and “delve” despite there being fewer\noverall “political science” articles that year. Keyword use acceler-\nates even more quickly when combining keywords, as shown in\nfigures 5and 6, where despite more keywords being added, there is\nlittle difference in the sudden acceleration of articles combining\nkeywords, only a smaller overall sample size as more keywords are\nintroduced.\nThese results show us that there is a high likelihood that\npolitical science professionals are using ChatGPT in peer-\nreviewed work. The use of these keyword searches demonstrates\nhow suspicious work can now be detected, and as later editions of\nChatGPT are introduced, new keywords will likely be introduced\nas well. Although the method remains the same, easily adaptable\ninputs will change.\nSide by side, the data are even more concerning, as the per-\ncentage of increase in the use of these words is greater than that of\n“terrorism” in 2002 in almost all cases, as shown infigure 1. When\nmore keywords are added, the sample sizes begin to shrink rapidly,\nbut these are only the most egregious overuses of keywords, and\ntherefore the most suspicious. Other combinations can also be\ntried to find keywords, such as limiting a search to“Political\nScience” and three keywords randomly chosen from the top five,\nbut not necessarily the top three— as this would likely create\nFigure 3\nAnnual Use of the Phrase“Political Science” in Academic Articles\n2000 2005 2010 2015 2020 2025\n140000\n120000\n100000\n80000\n60000\n40000\n20000\n0\nThese results show us that there is a high likelihood that political science professionals are\nusing ChatGPT in peer-reviewed work.\nThe Profession: More A than I\n.............................................................................................................................................................................................................................................................\n660 PS  October 2025\nanother example of the worrying trend in the sudden increase in\nwords preferred by ChatGPT (seetable 1).\nDISCUSSION\nThis is not to say that there is no place for AI in political science. In\nthe same sense that undisclosed AI will be a major issue for\ninstitutions that do not monitor for it, those who do not use\ndisclosed AI will also be limiting their opportunities for research.\nNew work is being developed that uses AI as a new method for\ntesting theories (Martineau2021), in the early stages of writing\narticles or lesson planning, and in brainstorming ideas (White\n2022), and LLMs like ChatGPT are powerful translation tools\n(White 2022). This only becomes an issue when it is generating\nwork that has not been disclosed as having been AI-generated.\n“Plagiarism” is in itself not plagiarism if the work is being cited\n(Childers and Burton2015), and this leads to a wider discussion on\nwhat is to be professionally accepted. What complicates the issue\ngreatly is that LLMs like ChatGPT do not disclose where they\nsource their information during the“feeding” process mentioned\nearlier. This leads to a distinct possibility that ChatGPT itself is\nFigure 4\nAnnual Use of the Phrase“Political Science” and the Word“Delve” in Academic Articles\n2000 2005 2010 2015 2020 2025\n120000\n100000\n80000\n60000\n40000\n20000\n0\nFigure 5\nAnnual Use of the Phrase“Political Science” and the Top Three Keywords in Academic Articles\n2000\n200\n180\n160\n140\n120\n100\n80\n60\n40\n20\n0\n2005 2010 2015 2020 2025\n.............................................................................................................................................................................................................................................................\nPS  October 2025 661\nunintentionally plagiarizing human work. This would mean that\neven disclosed AI use from human users is still not professionally\nacceptable because it could easily be the work of others that\nChatGPT itself had copied without citing, putting a problematic\nair over all LLM-generated papers.\nAdditionally, there needs to be a large consensus onhow much\nAI is too much. In 2021 Microsoft Word introduced text predic-\ntion, which suggests sentence completions as they are being\nwritten using pattern recognition technology. Spell checks in word\nprocessors search for words not in the dictionary, and when a word\nis misspelled it guesses what word the author was trying to write.\nGrammarly, a free add-on, allows for even more advanced spell-\nchecking and suggestions, also often finishing sentences as\nauthors write them. Both Microsoft Word and Grammarly are\nprofessionally accepted and not considered to be plagiarism.\nFurthermore, some writers use ChatGPT only for outlines, and\nsome professionals use it only for translation. Fortunately, with AI\nplagiarism checkers and this keyword detection method, finding\nLLM plagiarism is not difficult. However, it is incumbent to police\nfor this, as editors and reviewers should diligently check for LLM\nplagiarism before publication to mitigate the need for retraction.\nCurrently, most professional organizations like APSA have no\nguidelines for AI plagiarism. APSA’s most recent ethics guide was\nwritten in February 2022, several months before the open launch\nof ChatGPT, and does not mention large language models or\nartificial intelligence (American Political Science Association,\n2022). Members of these organizations can work together to define\n“AI plagiarism” in a way the field can accept, likely distinguishing\nbetween LLM plagiarism and nongenerative AIs like Grammarly\nor Microsoft Word’s Text Predictor. In short, LLMs generate new\nFigure 6\nAnnual Use of the Phrase“Political Science” and the Top Five Keywords in Academic Articles\n2000 2005 2010 2015 2020 2025\n140\n120\n100\n80\n60\n40\n20\n0\nTable 1\nSummarized Keyword Increases over Important Years\nMeasure Year T Year T+1 n at T n at T+1 Percentage of increase\n“Terrorism” 2001 2002 4017 9010 124.27 %\n“Political Science” 2022 2023 96579 119057 23.28 %\n“Political Science” and “Delve” 2022 2023 4474 10172 127.37 %\n“Political Science” and top 3 keywords 2022 2023 23 175 660.87 %\n“Political Science” and top 5 keywords 2022 2023 14 129 821.43 %\nThis would mean that even disclosed AI use from human users is still not professionally\nacceptable because it could easily be the work of others that ChatGPT itself had copied\nwithout citing, putting a problematic air over all LLM-generated papers.\nThe Profession: More A than I\n.............................................................................................................................................................................................................................................................\n662 PS  October 2025\ntext, whereas Grammarly and Text Predictor build off what text is\nalready written and offer suggestions.\nAfter professional organizations develop explicit guidelines\nand definitions of plagiarism, it is incumbent on institutions to\napply them to maintain their credibility. When AI plagiarism is\ndetected, institutions are likely to follow their methods for all types\nof plagiarism. This can mean internal reviews, retractions of articles,\ngreater scrutiny of the offender’s entire body of work in the past and\nfuture, and punishment spanning from a formal reprimand to\ntermination. As of the time of writing, the credibility of entire\ninstitutions is endangered by undisclosed AI-generated content. If\nthe field is not held to a high standard, where AI-generated articles\nare not only dismissed but there is stigmatization on those who\nproduce them, anyone who claims to be a political scientist could\nsuffer a loss of credibility. Failure to enforce regulations on how AI is\nused in political science, especially LLMs, weakens the field which\nhurts all professionals in some way.\nCONCLUSION\nThis article has shown that AI-generated articles have already\ninfested the field, leading to a sudden outbreak of plagiarism that\ncan harm the credibility of the entire field of political science. As\nmentioned earlier, this is a highlyspeculative and catastrophized\nconclusion. What is far more likely to happen is that AI detection\nmethods will be developed and higher education institutions that do\nnot embrace them will suffer and those that do embrace them will\nthrive. In a sense, reviewers usingnew, innovative methods to try and\ndetect AI is as important as authors including a“works cited” section.\nThe keyword detection method of finding suspicious AI work\na l l o w sr e v i e w e r st ok e e pu pw i t ht h e s et r e n d sa n dh o l da u t h o r st ot h e\ns a m es t a n d a r d sa se a r l i e rg e n e r a t i o n so fa u t h o r sw h od i dn o th a v et h e\noption to plagiarize in this way. Holding authors to a high standard,\nor at least the same standard that has been in place for generations, is\nonly for their and the profession’s benefit. There is a substantive\ndemand for professional organizations like APSA to establish guide-\nlines and best practices for defining and identifying AI plagiarism\nand establishing norms around how it is addressed and punished.\nUltimately the field as a whole must pay one of two expenses, either\nthe cost of adapting to the new landscape or the cost of its credibility.\nACKNOWLEDGMENTS\nI am grateful to God, and through Him, I am grateful to my wife,\nMu Tong, my parents, my advisor Dr. Krista Wiegand, my col-\nleagues Dana Abu Haltam, Matthew Millard, Michael McKoy,\nand Jeremiah Muhammad, as well as the rest of the political\nscience department at the University of Tennessee.\nDATA AVAILABILITY STATEMENT\nResearch documentation and data that support the findings of this\nstudy are openly available at the Harvard Dataverse athttps://\ndoi.org/10.7910/DVN/ZIB5NN.\nCONFLICTS OF INTEREST\nThe author declares there are no ethical issues or conflicts of\ninterest in this research.▪\nREFERENCES\nAmerican Political Science Association. 2022.A Guide to Professional Ethics in Political\nScience.\nChilders, Dan, and Sam Bruton. 2015.“‘Should It Be Considered Plagiarism?’Student\nPerceptions of Complex Citation Issues.” Journal of Academic Ethics14 (1): 1–17.\nHowell, C.W., Cal Baker, and Fayrah Stylianopoulos. 2023.“To Educate Students\nabout AI, Make Them Use It.” Scientific American, November 14, 2023.\nHuang, Kalley. 2023.“Alarmed by A.I. Chatbots, Universities Start Revamping How\nThey Teach.” New York Times, January 16, 2023, sec. Technology.\nKeener, Robert. 2025.“Replication Data for‘More A than I: Testing for Large\nLanguage Model Plagiarism in Political Science.’” PS: Political Science & Politics.\nHarvard Dataverse. DOI:10.7910/DVN/ZIB5NN.\nKhlaif, Zuheir N., Allam Mousa, Muayad Kamal Hattab, Jamil Itmazi, Amjad A.\nHassan, Mageswaran Sanmugam, and Abedalkarim Ayyoub. 2023.“The Potential\nand Concerns of Using AI in Scientific Research: ChatGPT Performance\nEvaluation.” JMIR Medical Education9 (1): e47049.https://doi.org/10.2196/47049.\nLi, Daniel. 2024.“The Most Overused ChatGPT Words - Plus.” Www.plusdocs.com,\nApril 21.https://www.plusdocs.com/blog/the-most-overused-chatgpt-words.\nMartineau, Kim. 2021.“Generative AI Could Offer a Faster Way to Test Theories of\nHow the Universe Works.” IBM Research, February 9.https://research.ibm.com/\nblog/time-series-AI-transformers.\nMerriam-Webster. 2019.“Definition of PLAGIARIZING.” Merriam-Webster.com.\nhttps://www.merriam-webster.com/dictionary/plagiarizing.\nMichels, Steven. (2023): Teaching (with) Artificial Intelligence: The Next Twenty\nYears. Journal of Political Science Education. DOI:10.1080/15512169.2023.2266848\nOpenAlex. n.d. Openalex.org.https://openalex.org/.\nPerkins, Mike, Jasper Roe, Darius Postma, James McGaughran, and Don Hickerson.\n2023. “Detection of GPT-4 Generated Text in Higher Education: Combining\nAcademic Judgement and Software to Identify Generative AI Tool Misuse.”\nJournal of Academic Ethics22 (October).\nRosenblatt, Kalhan. 2023.“ChatGPT Passes MBA Exam given by a Wharton\nProfessor.” NBC News, January 23.https://www.nbcnews.com/tech/tech-news/\nchatgpt-passes-mba-exam-wharton-professor-rcna67036.\nSteinhardt, S. J. 2023.“Latest Version of ChatGPT Passed a Practice CPA Exam.”\nWww.nysscpa.org, May 23.https://www.nysscpa.org/article-content/latest-\nversion-of-chatgpt-passed-a-practice-cpa-exam-052323#sthash.8wTF2hfa.dpbs.\nStöffelbauer, Andreas. 2023.“How Large Language Models Work.” Data Science at\nMicrosoft, October 24, 2023.https://medium.com/data-science-at-microsoft/how-\nlarge-language-models-work-91c362f5b78f.\nWhite, Monica. 2022.“Top 10 Most Insane Things ChatGPT Has Done This Week.”\nSpringboard (Blog). December 9, 2022.https://www.springboard.com/blog/news/\nchatgpt-revolution/.\n.............................................................................................................................................................................................................................................................\nPS  October 2025 663",
  "topic": "Political science",
  "concepts": [
    {
      "name": "Political science",
      "score": 0.656681478023529
    },
    {
      "name": "Politics",
      "score": 0.6249077916145325
    },
    {
      "name": "Law",
      "score": 0.21786683797836304
    }
  ],
  "institutions": []
}