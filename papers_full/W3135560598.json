{
  "title": "Causal Analysis of Agent Behavior for AI Safety",
  "url": "https://openalex.org/W3135560598",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5059962909",
      "name": "Grégoire Delétang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5026686188",
      "name": "Jordi Grau-Moya",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5025651820",
      "name": "Miljan Martic",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5013602545",
      "name": "Tim Genewein",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5035693801",
      "name": "Tom McGrath",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5047554131",
      "name": "Vladimir Mikulik",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5043562314",
      "name": "Markus Kunesch",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5008987732",
      "name": "Shane Legg",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5035060247",
      "name": "Pedro A. Ortega",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2903075639",
    "https://openalex.org/W1878225033",
    "https://openalex.org/W2917742641",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2184746314",
    "https://openalex.org/W2970909667",
    "https://openalex.org/W1971129545",
    "https://openalex.org/W1527571715",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963775850",
    "https://openalex.org/W2462906003",
    "https://openalex.org/W2119567691",
    "https://openalex.org/W2962867073",
    "https://openalex.org/W2486285194",
    "https://openalex.org/W2786036274",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W2050035105",
    "https://openalex.org/W2964060106",
    "https://openalex.org/W3003129838",
    "https://openalex.org/W2550182557",
    "https://openalex.org/W2788266634",
    "https://openalex.org/W2111030512",
    "https://openalex.org/W3101014879",
    "https://openalex.org/W2963319332",
    "https://openalex.org/W2042783899",
    "https://openalex.org/W2096533821",
    "https://openalex.org/W2240086230",
    "https://openalex.org/W2239029832",
    "https://openalex.org/W3005612959",
    "https://openalex.org/W2953494151",
    "https://openalex.org/W2768908787",
    "https://openalex.org/W3121212930",
    "https://openalex.org/W2102630578",
    "https://openalex.org/W2165552039",
    "https://openalex.org/W3082925502",
    "https://openalex.org/W2657631929",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2121803403",
    "https://openalex.org/W2152431454",
    "https://openalex.org/W2903571680"
  ],
  "abstract": "As machine learning systems become more powerful they also become increasingly unpredictable and opaque. Yet, finding human-understandable explanations of how they work is essential for their safe deployment. This technical report illustrates a methodology for investigating the causal mechanisms that drive the behaviour of artificial agents. Six use cases are covered, each addressing a typical question an analyst might ask about an agent. In particular, we show that each question cannot be addressed by pure observation alone, but instead requires conducting experiments with systematically chosen manipulations so as to generate the correct causal evidence.",
  "full_text": "Causal Analysis of Agent Behavior for AI Safety\nGr´egoire D´eletang * 1 Jordi Grau-Moya * 1 Miljan Martic * 1 Tim Genewein 1 Tom McGrath1\nVladimir Mikulik 1 Markus Kunesch 1 Shane Legg 1 Pedro A. Ortega 1\nAbstract\nAs machine learning systems become more pow-\nerful they also become increasingly unpredictable\nand opaque. Yet, ﬁnding human-understandable\nexplanations of how they work is essential for\ntheir safe deployment. This technical report illus-\ntrates a methodology for investigating the causal\nmechanisms that drive the behaviour of artiﬁcial\nagents. Six use cases are covered, each addressing\na typical question an analyst might ask about an\nagent. In particular, we show that each question\ncannot be addressed by pure observation alone,\nbut instead requires conducting experiments with\nsystematically chosen manipulations so as to gen-\nerate the correct causal evidence.\nKeywords: Agent analysis, black-box analysis,\ncausal reasoning, AI safety.\n1. Introduction\nUnlike systems speciﬁcally engineered for solving a\nnarrowly-scoped task, machine learning systems such as\ndeep reinforcement learning agents are notoriously opaque.\nEven though the architecture, algorithms, and training data\nare known to the designers, the complex interplay between\nthese components gives rise to a black-box behavior that\nis generally intractable to predict. This problem wors-\nens as the ﬁeld makes progress and AI agents become\nmore powerful and general. As illustrated by learning-to-\nlearn approaches, learning systems can use their experience\nto induce algorithms that shape their entire information-\nprocessing pipeline, from perception to memorization to\naction (Wang et al., 2016; Andrychowicz et al., 2016).\nSuch poorly-understood systems do not come with the nec-\nessary safety guarantees for deployment. From a safety\nperspective, it is therefore paramount to develop black-box\nmethodologies (e.g. suitable for any agent architecture) that\n*Equal contribution 1AGI Safety Analysis, DeepMind,\nLondon, UK. Correspondence to: Pedro A. Ortega <pe-\ndroortega@google.com>.\n©2020 by the authors.\nallow for investigating and uncovering the causal mecha-\nnisms that underlie an agent’s behavior. Such methodologies\nwould enable analysts to explain, predict, and preempt fail-\nure modes (Russell et al., 2015; Amodei et al., 2016; Leike\net al., 2017).\nThis technical report outlines a methodology for investi-\ngating agent behavior from a mechanistic point of view.\nMechanistic explanations deliver a deeper understanding of\nagency because they describe the cause-effect relationships\nthat govern behavior—they explain why an agent does what\nit does. Speciﬁcally, agent behavior ought to be studied\nusing the tools of causal analysis (Spirtes et al., 2000; Pearl,\n2009; Dawid, 2015). In the methodology outlined here, ana-\nlysts conduct experiments in order to conﬁrm the existence\nof hypothesized behavioral structures of AI systems. In\nparticular, the methodology encourages proposing simple\ncausal explanations that refer to high-level concepts (“the\nagent prefers green over red apples”) that abstract away the\nlow-level (neural) inner workings of an agent.\nUsing a simulator, analysts can place pre-trained agents into\ntest environments, recording their reactions to various inputs\nand interventions under controlled experimental conditions.\nThe simulator provides additional ﬂexibility in that it can,\namong other things, reset the initial state, run a sequence\nof interactions forward and backward in time, change the\nseed of the pseudo-random number generator, or spawn a\nnew branch of interactions. The collected data from the sim-\nulator can then be analyzed using a causal reasoning engine\nwhere researchers can formally express their assumptions\nby encoding them as causal probabilistic models and then\nvalidate their hypotheses. Although labor-intensive, this\nhuman-in-the-loop approach to agent analysis has the ad-\nvantage of producing human-understandable explanations\nthat are mechanistic in nature.\n2. Methodology\nWe illustrate this methodology through six use cases, se-\nlected so as to cover a spectrum of prototypical questions\nan agent analyst might ask about the mechanistic drivers\nof behavior. For each use case, we present a minimalis-\ntic grid-world example and describe how we performed\nour investigation. We limit ourselves to environmental and\narXiv:2103.03938v1  [cs.AI]  5 Mar 2021\nCausal Analysis of Agent Behavior for AI Safety\nbehavioral manipulations, but direct interventions on the\ninternal state of agents are also possible. The simplicity in\nour examples is for the sake of clarity only; conceptually,\nall solution methods carry over to more complex scenarios\nunder appropriate experimental controls.\nOur approach uses several components: an agent and an\nenvironment, a simulator of interaction trajectories, and a\ncausal reasoning engine. These are described in turn.\n2.1. Agents and environments\nFor simplicity, we consider stateful agents and environments\nthat exchange interaction symbols (i.e. actions and obser-\nvations) drawn from ﬁnite sets in chronological order at\ndiscrete time steps t= 1,2,3,... Typically, the agent is a\nsystem that was pre-trained using reinforcement learning\nand the environment is a partially-observable Markov de-\ncision process, such as in Figure 1a. Let mt,wt (agent’s\nmemory state, world state) and at,ot (action, observation)\ndenote the internal states and interaction symbols at time t\nof the agent and the environment respectively. These inter-\nactions inﬂuence the stochastic evolution of their internal\nstates according to the following (causal) conditional proba-\nbilities:\nwt ∼P(wt |wt−1,at−1) ot ∼P(ot |wt) (1)\nmt ∼P(mt |mt−1,ot) at ∼P(at |mt). (2)\nThese dependencies are illustrated in the causal Bayesian\nnetwork of Figure 1b describing the perception-action loop\n(Tishby & Polani, 2011).\nSince we wish to have complete control over the stochastic\ncomponents of the interaction process (by controlling its\nrandom elements), we turn the above into a deterministic\nsystem through a re-parameterization1. Namely, we repre-\nsent the above distributions using functions W,M,O,A as\nfollows:\nwt = W(wt−1,at−1,ω) ot = O(wt,ω) (3)\nmt = M(mt−1,ot,ω) at = A(mt,ω) (4)\nwhere ω ∼ P(ω) is the random seed. This re-\nparameterization is natural in the case of agents and en-\nvironments implemented as programs.\n2.2. Simulator\nThe purpose of the simulator is to provide platform for exper-\nimentation. Its primary function is to generate traces (roll-\nouts) of agent-environment interactions (Figure 2). Given a\n1That is, we describe the system as a structural causal model\nas described in Pearl (2009, chapter 7). Although this parame-\nterization is chosen for the sake of concreteness, others are also\npossible.\nMt+1\nOt+1\nWt+1\nAt\nMt\nOt\nWt\nAt-1\ntime t\nb)\nMt-1\na)\n t t+1\nFigure 1.Agents and environments. a) The goal of the agent is to\npick up a reward pill without stepping into a lava tile. b) Causal\nBayesian network describing the generative process of agent-\nenvironment interactions. The environmental state Wt and the\nagent’s memory stateMt evolve through the exchange of action\nand observation symbols At and Ot respectively.\nsystem made from coupling an agent and an environment, a\nrandom seed ω∼P(ω), and a desired lengthT, it generates\na trace\nτ = (ω,s1,x1),(ω,s2,x2),(ω,s3,x3),..., (ω,sT ,xT )\nof a desired length T, where the st := (ωt,mt) and\nxt := (ot,at) are the combined state and interaction sym-\nbols respectively, and where ωis the random element which\nhas been made explicit. The simulator can also contract\n(rewind) or expand the trace to an arbitrary time point\nT′ ≥1. Note that this works seamlessly as the genera-\ntive process of the trace is deterministic.\nIn addition, the simulator allows for manipulations of the\ntrace. Such an intervention at time tcan alter any of the\nthree components of the triple (ω,st,xt). For instance,\nchanging the random seed in the ﬁrst time step corresponds\nto sampling a new trajectory:\nτ = (ω,s1,x1),(ω,s2,x2),..., (ω,sT ,xT )\n↓\nτ′= (ω′,s′\n1,x′\n1),(ω′,s′\n2,x′\n2),..., (ω′,s′\nT ,x′\nT );\n(5)\nwhereas changing the state at time step t = 2produces a\nnew branch of the process sharing the same root:\nτ = (ω,s1,x1),(ω,s2,x2),..., (ω,sT ,xT )\n↓\nτ′= (ω,s1,x1),(ω,s′\n2,x′\n2),..., (ω,s′\nT ,x′\nT ).\n(6)\nPage 2\nCausal Analysis of Agent Behavior for AI Safety\nRollout 1\nRollout 2\nRollout 3\nchange seed\nchange tiles\nFigure 2.Simulating a trace (rollout) and performing interventions, creating new branches.\nUsing these primitives one can generate a wealth of data\nabout the behavior of the system. This is illustrated in\nFigure 2.\n2.3. Causal reasoning engine\nFinally, in order to gain a mechanistic understanding of the\nagent’s behavior from the data generated by the simulator,\nit is necessary to use a formal system for reasoning about\nstatistical causality. The purpose of the causal reasoning\nengine is to allow analysts to precisely state and validate\ntheir causal hypotheses using fully automated deductive\nreasoning algorithms.\nAs an illustration of the modeling process, consider an an-\nalyst wanting to understand whether an agent avoids lava\nwhen trying to reach a goal state. First, the analyst selects\nthe set of random variables Xthey want to use to model the\nsituation2. The variables could consist of (abstract) features\ncomputed from the trajectories (e.g. “agent takes left path”)\nand hypothesis variables (e.g. “the agent avoids lava tiles”).\nThe objective is to obtain a simpliﬁed model that abstracts\naway all but the relevant features of the original interaction\nsystem.\nNext, the analyst speciﬁes a structural causal model (Pearl,\n2009, Chapter 7) to describe the causal generative process\nover the chosen random variables. To illustrate, consider an\nexperiment that can be described using three random vari-\nables, X= {X,Y,Z }. Assume that X precedes Y, and Y\nin turn precedes Z, as shown in Figure 3. A structural causal\n2There are some subtleties involved in the selection of random\nvariables. For example, if you want to be able to make arbitrary in-\nterventions, the variables should be logically independent. Halpern\n& Hitchcock (2011) provide a discussion.\nmodel for this situation would be the system of equations\nX = fX(UX) UX ∼P(UX)\nY = fY (X,UY ) UY ∼P(UY )\nZ = fZ(X,Y,U Z) UZ ∼P(UZ)\n(7)\nwhere fX,fY , and fZ are (deterministic) functions and\nwhere the (exogenous) variables UX,UY , UZ encapsulate\nthe stochastic components of the model. Together, they\ninduce the conditional probabilities\nP(X), P (Y |X), and P(Z |X,Y ). (8)\nThese probabilities can be directly supplied by the analyst\n(e.g. if they denote prior probabilities over hypotheses) or\nestimated from Monte-Carlo samples obtained from the\nsimulator (see next subsection).\nZ\nX\nY\nUX\nUY UZ\nFigure 3.A graphical model representing the structural causal\nmodel in (7).\nOnce built, the causal model can be consulted to answer\nprobabilistic queries using the causal reasoning engine.\nBroadly, the queries come in three types:\nPage 3\nCausal Analysis of Agent Behavior for AI Safety\n• Association: Here the analyst asks about a conditional\nprobability, such as P(X = x|Y = y).\n• Intervention: If instead the analyst controls Y directly,\nfor instance by setting it to the value Y = y, then the\nprobability of X = xis given by\nP(X = x|do(Y = y)).\nHere, “do” denotes the do-operator, which substitutes\nthe equation for Y in the structural model in (7) with\nthe constant equation Y = y. Hence, the new system\nis\nX = fX(UX) UX ∼P(UX)\nY = y U Y ∼P(UY )\nZ = fZ(X,Y,U Z) UZ ∼P(UZ),\n(9)\nwhich in this case removes the dependency of Y on X\n(and the exogenous variable UY ).\n• Counterfactuals: The analyst can also ask counterfac-\ntual questions, i.e. the probability of X = x given\nthe event Y = y had Y = y′been the case instead.\nFormally, this corresponds to\nP(Xy = x|Y = y′),\nwhere Xy is the potential response of X when Y = y\nis enforced.\nThese correspond to the three levels of the causal hierarchy\n(Pearl & Mackenzie, 2018). We refer the reader to Pearl\net al. (2016) for an introduction to causality and Pearl (2009)\nfor a comprehensive treatment.\n2.4. Analysis workﬂow\nA typical analysis proceeds as follows.\nExploratory investigation. The analyst starts by placing\na trained agent (provided by an agent trainer) into one or\nmore test environments, and then probing the agent’s be-\nhavior through interventions using the simulator. This will\ninform the analyst about the questions to ask and the vari-\nables needed to answer them.\nFormulating the causal model. Next, the analyst for-\nmulates a causal model encapsulating all the hypotheses\nthey want to test. If some probabilities in the model are\nnot known, the analyst can estimate them empirically us-\ning Monte-Carlo rollouts sampled from the simulator (Fig-\nure 4a). This could require the use of multiple (stock) agents\nand environments, especially when the causal hypotheses\ncontrast multiple types of behavior.\nIn our examples we used discrete random variables. When\nrequired, we estimated the conditional probabilities of the\na)\nb)\nc)\nFigure 4.Building a causal model from Monte-Carlo rollouts with\ninterventions. a) A tree generated from Monte-Carlo rollouts from\nan initial state. This tree contains interaction trajectories that the\nsystem can generate by itself. b) When performing experiments,\nthe analyst could enforce transitions (dotted red lines) that the\nsystem would never take by itself, such as e.g. “make a lava tile\nappear next to the agent”. The associated subtrees (red) need to\nbe built from Monte-Carlo rollouts rooted at the states generated\nthrough the interventions. c) Finally, the rollout trees can be used\nto estimate the probabilities of a causal model.\ncausal model following a Bayesian approach. More pre-\ncisely, for each conditional probability table that had to be\nestimated, we placed a ﬂat Dirichlet prior over each out-\ncome, and then computed the posterior probabilities using\nthe Monte-Carlo counts generated by the simulator. The ac-\ncuracy of the estimate can be controlled through the number\nof samples generated.\nInterventions require special treatment (Figure 4b). When-\never the analyst performs an intervention that creates a new\nbranch (for instance, because the intervention forces the\nsystem to take a transition which has probability zero), the\ntransition probabilities of the subtree must be estimated sep-\narately. The transition taken by the intervention itself has\nzero counts, but it has positive probability mass assigned\nby the Dirichlet prior. Interventions that do not generate\nnew branches do not require any special treatment as they\nalready have Monte-Carlo samples.\nPage 4\nCausal Analysis of Agent Behavior for AI Safety\nQueries. Once built (Figure 4c), the analyst can query\nthe causal model to answer questions of interest. These\ncan/should then also be veriﬁed empirically using the simu-\nlator.\n3. Experiments\nIn the following, we present six use cases illustrating typical\nmechanistic investigations an analyst can carry out:\n• estimating causal effects under confounding;\n• testing for the use of internal memory;\n• measuring robust generalization of behavior;\n• imagining counterfactual behavior;\n• discovering causal mechanisms;\n• and studying the causal pathways in decisions.\nIn each case we assume the agent trainer and the analyst do\nnot share information, i.e. we assume the analyst operates\nunder black box conditions. However, the analyst has access\nto a collection of pre-trained stock agents, which they can\nconsult/use for formulating their hypotheses.\nThe environments we use were created using the Pycolab\ngame engine (Stepleton, 2017). They are 2D gridworlds\nwhere the agent can move in the four cardinal directions\nand interact with objects through pushing or walking over\nthem. Some of the objects are rewards, doors, keys, ﬂoors of\ndifferent types, etc. The agent’s goal is to maximize the sum\nof discounted cumulative rewards (Puterman, 2014; Sutton\n& Barto, 2018). The environments use a random seed for\ntheir initialization (e.g. for object positions).\nIn theory, the agents can be arbitrary programs that produce\nan action given an observation and an internal memory\nstate; but here we used standard deep reinforcement learning\nagents with a recurrent architecture (see Appendix).\n3.1. Causal effects under confounding\nProblem. Do rewards guide the agent, or do other fac-\ntors control its behavior? Estimating causal effects is the\nquintessential problem of causal inference. The issue is that\nsimply observing how the presumed independent and de-\npendent variables co-vary does not sufﬁce, as there could be\na third confounding variable creating a spurious association.\nFor instance, sometimes an agent solves a task (e.g. picking\nup a reward pill), but it does so by relying on an accidentally\ncorrelated feature (e.g. the color of the ﬂoor) rather than the\nintended one (e.g. location of the pill). Such policies do not\ngeneralize (Arjovsky et al., 2019).\nFigure 5.The grass-sand environment. The goal of the agent is\nto pick up a reward pill, located in one of the ends of a T-maze.\nReaching either end of the maze terminates the episode. The\nproblem is that the ﬂoor type (i.e. either grass or sand) is correlated\nwith the location of the reward.\nTo ﬁnd out whether the agent has learned the desired causal\ndependency, one can directly manipulate the independent\nvariable and observe the effect. This manipulation decouples\nthe independent variable from a possible confounder (Pearl,\n2009, Chapter 3). Randomized controlled trials are the\nclassical example of this approach (Fisher, 1936).\nSetup. We illustrate the problem of estimating causal ef-\nfects using the grass-sand environment depicted in Figure 5.\nThe agent needs to navigate a T-maze in order to collect a\npill (which provides a reward) at the end of one of the two\ncorridors (Olton, 1979). The problem is that the location of\nthe pill (left or right) and the type of the ﬂoor (grass or sand)\nare perfectly correlated. Given an agent that successfully\ncollects the pills, the goal of the analyst is to determine\nwhether it did so because it intended to collect the pills, or\nwhether it is basing its decision on the type of the ﬂoor.\nOur experimental subjects are two agents, named A and B.\nAgent A was trained to solve T-mazes with either the (sand,\nleft) or (grass, right) conﬁguration; whereas agent B was\ntrained to solve any of the four combinations of the ﬂoor\ntype and reward pill location.\nPage 5\nCausal Analysis of Agent Behavior for AI Safety\nT\nR F\nC\nFigure 6.Causal model for the grass-sand environment. R is the\nlocation of the reward pill; T is the terminal state chosen by the\nagent; F is the type of the ﬂoor; and C is a confounder that\ncorrelates R and F. Note that C is unobserved.\nExperiment. The experiment proceeds as follows. First,\nwe randomly choose between the (sand, left) and (grass,\nright) T-mazes and place the agent in the starting position.\nThen we randomly decide whether to switch the pill location.\nAfter this intervention, we let the agent navigate until it\nﬁnishes the episode, recording whether it took the right or\nleft terminal state.\nWe also considered the following hypothesis: namely, that\nthe agent’s behavior depends on the type of the ﬂoor. To\nmeasure the causal effect, we randomly intervened this fea-\nture, recording the agent’s subsequent choice of the terminal\nstate. The causal model(s) are depicted in Figure 6.\nResults. Table 1 shows the results of the interventions.\nHere, the random variables T ∈{l,r}, R ∈{l,r}, and\nF ∈{g,s}correspond to the agent’s choice of the terminal\nstate, the location of the reward pill, and the type of the\nﬂoor, respectively. The reported values are the posterior\nprobabilities (conditioned on 1000 rollouts) of choosing\nthe left/right terminal for the observational setting (i.e. by\njust observing the behavior of the agent) and for the two\ninterventional regimes.\nThe probability of taking the left terminal conditioned on the\nleft placement of the reward was obtained through standard\nconditioning:\nP(T = l|R= l) =\n∑\nf\nP(T = l|F = f,R = l)P(F = f |R= l). (10)\nIn contrast, intervening on the reward location required the\nuse of the adjustment formula as follows (Pearl, 2009)\nP(T = l|do(R= l)) =\n∑\nf\nP(T = l|F = f,R = l)P(F = f). (11)\nOther quantities were obtained analogously.\nWe found that the two agents differ signiﬁcantly. In the\nobservational regime, both agents successfully solve the\ntask, picking up the reward pill. However, manipulating the\nenvironmental factors reveals a difference in their behavioral\ndrivers. Agent A’s choice is strongly correlated with the type\nof ﬂoor, but is relatively insensitive to the position of the\npill. In contrast, agent B picks the terminal state with the\nreward pill, regardless of the ﬂoor type.\nTable 1.Grass-sand queries\nQUERIES A B\nP(T = l | R = l) 0.996 0.996\nP(T = r | R = r) 0.987 0.996\nP(T = l | do(R = l)) 0.536 0.996\nP(T = r | do(R = r)) 0.473 0.996\nP(T = l | do(F = g)) 0.996 0.515\nP(T = r | do(F = s)) 0.987 0.497\nDiscussion. This use case illustrates a major challenge\nin agent training and analysis: to ensure the agent uses\nthe intended criteria for its decisions. Because it was\ntrained on a collection of environments with a built-in bias,\nagent A learned to rely on an undesired, but more salient\nfeature. This is a very common phenomenon. Resolving\nthe use of spurious correlations in learned policies is on-\ngoing research—see for instance (Bareinboim et al., 2015;\nArjovsky et al., 2019; V olodin et al., 2020).\nOur experiment shows that inspecting the agent’s behavior\ndoes not sufﬁce for diagnosing the problem, but indepen-\ndently manipulating the intended decision criterion (i.e. the\nreward location) does. Once the problem is discovered, iden-\ntifying the confounding factors (e.g. the ﬂoor type) can be a\nmuch harder task for the analyst.\n3.2. Memory\nProblem. Does the agent use its internal memory for re-\nmembering useful information, or does it off-load the mem-\nory onto the environment? Memorization is a necessary skill\nfor solving complex tasks. It can take place in the agent’s\ninternal memory; however, often it is easier for an agent\nto off-load task-relevant information onto its environment\n(e.g. through position-encoding), effectively using it as an\nexternal memory. This difference in strategy is subtle and\nin fact undetectable without intervening.\nTo ﬁnd out whether the agent is actually using its inter-\nnal memory, we can make mid-trajectory interventions on\nthe environment state variables suspected of encoding task-\nrelevant information. If the agent is using external memory,\nthis will corrupt the agent’s decision variables, leading to a\nfaulty behavior.\nPage 6\nCausal Analysis of Agent Behavior for AI Safety\na)\nb)\nFigure 7.The ﬂoor-memory environment. a) The goal of the agent\nwith limited vision (see black square) is to collect the reward at\none of the ends of the T-maze. A cue informs the agent about the\nlocation of the reward. The cue, that can be sand or grass, denotes\nif the reward is on the right or left, respectively. b) After three\nsteps, we intervene by pushing the agent toward the opposite wall\n(red arrow), and let it continue thereafter, possibly taking one of\nthe two dashed paths.\nSetup. We test the agent’s memory using the ﬂoor-\nmemory environment depicted in Figure 7. In this T-maze\nenvironment, the agent must remember a cue placed at the\nbeginning of a corridor in order to know which direction to\ngo at the end of it (Olton, 1979; Bakker, 2001). This cue can\neither be a grass tile or a sand tile, and determines whether\nthe reward is on the right or the left end, respectively. Both\ncue types and reward locations appear with equal probabili-\nties and are perfectly correlated. The agent can only see one\ntile around its body.\nWe consider two subjects. Agent a is equipped with an\ninternal memory layer (i.e. LSTM cells). In contrast, agentb\nis implemented as a convolutional neural network without\na memory layer; it is therefore unable to memorize any\ninformation internally.\nExperiment. Gathering rollout data from the test distri-\nbution provides no information on whether the agent uses\nits internal memory or not. An analyst might prematurely\nP T\nF\nFigure 8.Causal model for the ﬂoor-memory environment. F is\nthe initial cue (ﬂoor type); P is the position of the agent mid-way\nthrough the episode; T is the terminal state chosen by the agent.\nIf the agent off-loads the memory about the initial cue onto the\nposition, then the link F → T would be missing.\nconclude that the agent uses internal memory based on ob-\nserving that the agent consistently solves tasks requiring\nmemorization. However, without intervening, the analyst\ncannot truly rule out the possibility that the agent is off-\nloading memory onto the environment.\nIn this example, we can use the following experimental\nprocedure. First, we let the agent observe the cue and then\nfreely execute its policy. When the agent is near the end\nof the wide corridor, we intervene by pushing the agent\nto the opposite wall (see red arrow in Figure 7). This is\nbecause we suspect that the agent could use the nearest\nwall, rather than its internal memory, to guide its navigation.\nAfter the intervention, if the agent returns to the original\nwall and collects the reward, it must be because it is using\nits internal memory. If on the contrary, the agent does not\nreturn and simply continues its course, we can conclude it\nis off-loading memorization onto its environment.\nWe model the situation using three random variables. The\nﬂoor type (grass or sand) is denoted by F ∈{g,s}. The\nvariable P ∈{l,r}denotes the position of the agent (left\nor right half-side of the room) at the position when the\nanalyst could execute an intervention. Finally, T ∈{l,r}\nrepresents where the agent is (left or right) when the episode\nends. To build the model we randomly decide whether the\nanalyst is going to intervene or not (i.e. by pushing) with\nequal probability. The estimation is performed using 1000\nMonte-Carlo rollouts for each case.\nResults. Table 2 shows the probabilities obtained by\nquerying the causal model from Figure 8. The ﬁrst four\nqueries correspond to an observational regime. We see that\nboth agents pick the correct terminal tiles (T = lor T = r)\nwith probability close to 1 when conditioning on the cue\n(F) and, additionally, do so by choosing the most direct\npath (P = l or P = r). However, the results from the\ninterventional regime in the last two rows show that agent\nA= bloses its track when being pushed. This demonstrates\nthat agent bis using an external memory mechanism that\ngeneralizes poorly. In contrast, agent A= aends up in the\ncorrect terminal tile even if it is being pushed to the opposite\nPage 7\nCausal Analysis of Agent Behavior for AI Safety\nwall.\nTable 2.Floor-memory queries for agent a (with internal memory)\nand b (without internal memory).\nQUERIES A = a A = b\nP(T = l | F = g) 0.996 0.990\nP(T = r | F = s) 0.996 0.977\nP(P = l | F = g) 0.984 0.991\nP(P = r | F = s) 0.996 0.985\nP(T = l | do(P = r), F= g) 0.996 0.107\nP(T = r | do(P = l), F= s) 0.996 0.004\nDiscussion. Agent generalization and performance on par-\ntially observable environments depends strongly on the ap-\npropriate use of memory. From a safety perspective, ﬂawed\nmemory mechanisms that off-load memorization can lead to\nfragile behavior or even catastrophic failures. Understand-\ning how AI agents store and recall information is critical to\nprevent such failures. As shown in the previous experiment,\nthe analyst can reveal the undesired use of external memory\nby appropriately intervening on the environmental factors\nthat are suspected of being used by the agent to encode\ntask-relevant information.\n3.3. Robust generalization\nProblem. Does the agent solve any instance within a tar-\nget class of tasks? Although agents trained through deep\nreinforcement learning seem to solve surprisingly complex\ntasks, they struggle to transfer this knowledge to new envi-\nronments. This weakness is usually hidden by the, unfortu-\nnately common, procedure of testing reinforcement learning\nagents on the same set of environments used for training.\nImportantly, detecting the failure to generalize to a desired\nclass of environments is key for guaranteeing the robustness\nof AI agents.\nTwo problems arise when assessing the generalization abil-\nity of agents. First, testing the agent on the entire class of\ntarget environments is typically intractable. Second, the an-\nalyst might be interested in identifying the instances within\nthe class of test environments where the agent fails to solve\nthe task, rather than only measuring the average test perfor-\nmance, which could hide the failure modes. This highlights\nthe need for the analyst to assess generalization through the\ncareful choice of multiple targeted tests.\nSetup. We illustrate how to test for generalization using\nthe pick-up environment shown in Figure 9. This is a simple\nsquared room containing a reward which upon collection\nterminates the episode. The analyst is interested in ﬁnding\nout whether the agent generalizes well to all possible reward\nlocations.\nFigure 9.The pick-up environment. The goal of the agent is to\ncollect the reward independent of their initial position.\nWe consider the following two agents as subjects. Both\nagents were trained on a class of environments where their\ninitial position and the reward location were chosen ran-\ndomly. However, agent A’s task distribution picks locations\nanywhere within the room, whereas agent B’s training tasks\nrestricted the location of the reward to the southern quadrant\nof the room. Thus only agent A should be general with\nrespect to the class of environments of interest.\nExperiment. Assume the test set is the restricted class\nof problem instances where rewards were restricted to the\nsouthern corner. Then, if the analyst were to test A and B,\nthey could prematurely conclude that both agents general-\nize. However, assessing generalization requires a different\nexperimental procedure.\nThe experiment proceeds as follows. We draw an initial state\nof the system from the test distribution, and subsequently\nintervene by moving the reward to an arbitrary location\nwithin the room. After the intervention, we let the agent\nfreely execute its policy and we observe if the reward was\ncollected or not. A collected reward provides evidence that\nthe agent generalizes under this initial condition.\nWe built one causal model per agent from 1000 intervened\nMonte-Carlo rollouts. The variables are: G∈{n,s,e,w },\nthe quadrant location of the reward (north, south, east, west);\nand R∈{0,1}, denoting whether the reward is collected or\nnot. Figure 10 shows the causal graph for both models.\nResults. We performed a number of queries on the causal\nmodels shown in Table 3. Firstly, both agents perform very\nwell when evaluated on the test distribution over problem\ninstances, since P(R= 1)≈1 in both cases. However, the\nintervened environments tell a different story. As expected,\nagent A performs well on all locations of the reward, sug-\ngesting that meta-training on the general task distribution\nwas sufﬁcient for acquiring the reward location invariance.\nPage 8\nCausal Analysis of Agent Behavior for AI Safety\nR\nG\nFigure 10.Causal model for the pick-up environment. G is the\nlocation of the reward pill and R is a binary variable indicating a\nsuccessful pick-up.\nAgent B performs well when the reward is in the southern\nquadrant, but under-performs in the rest of the conditions.\nInterestingly, the performance decays as the distance from\nthe southern quadrant increases, suggesting that there was\nsome degree of topological generalization.\nTable 3.Pick-up environment queries for agentsA = a and A = b.\nQUERIES A = a A = b\nP(R = 1) 0.988 0.965\nP(R = 1| do(G = n)) 0.985 0.230\nP(R = 1| do(G = e)) 0.987 0.507\nP(R = 1| do(G = w)) 0.988 0.711\nP(R = 1| do(G = s)) 0.988 0.986\nDiscussion. In this use-case we outlined a procedure for\nassessing the agents’ robust generalization capabilities. Al-\nthough quantifying generalization in sequential decision-\nmaking problems is still an open problem, we adopted a\npragmatic approach: we say that an agent generalizes ro-\nbustly when it successfully completes any task within a\ndesired class of environments. This requirement is related\nto uniform performance and robustness to adversarial at-\ntacks. Since testing all instances in the class is unfeasible,\nour approximate solution for assessing generalization relies\non subdividing the class and estimating the success probabil-\nities within each subdivision. Even if this approximation is\ncrude at the beginning of the analysis, it can provide useful\nfeedback for the analyst. For example, we could further ex-\nplore agent B’s generalization by increasing the resolution\nof the reward location.\n3.4. Counterfactuals\nProblem. What would the agent have done had the set-\nting been different? Counterfactual reasoning is a powerful\nmethod assessing an observed course of events. An analyst\ncan imagine changing one or more observed factors without\nchanging others, and imagine the outcome that this change\nwould have led to.\nIn artiﬁcial systems a simulator is often available to the\nanalyst. Using the simulator, the analyst can directly sim-\nulate counterfactuals by resetting the system to a desired\nstate, performing the desired change (i.e. intervening), and\nrunning the interactions ensuing thereafter. This approach\nyields empirically grounded counterfactuals.\nHowever, simulating counterfactual interactions is not al-\nways possible. This happens whenever:\n(a) a realistic simulation for this setting does not exist (e.g.\nfor an agent acting in the real world);\n(b) a simulation exists, but its use is limited (e.g. when\nevaluating proprietary technology).\nFor instance, the analyst might be presented with a single\nbehavioral trace of an agent that was trained using an un-\nknown training procedure. Answering counterfactual ques-\ntions about this agent requires a behavioral model built from\nprior knowledge about a population of similar or related\nagents. This is the case which we examine through our\nexperiment. The downside is that such counterfactuals do\nnot make empirically veriﬁable claims (Dawid, 2000).\nSetup. We discuss this problem using the gated-room en-\nvironment depicted in Figure 11a. The environment consists\nof two identical rooms each holding a red and a green re-\nward. Collection of the reward terminates the episode. The\nrooms are initially protected by two gates but one of them\nrandomly opens at the beginning of the episode. We assume\nthere exist two types of agents, classiﬁed as either loving\ngreen or red reward pills.\nExperiment. Assume we make a single observation\nwhere an unknown agent picks up a red reward in an en-\nvironment where the right gate is open (Figure 11b). We\ncan now ask: “What would have happened had the left gate\nbeen opened instead?” If we had direct access to the agent’s\nand the environment’s internals, we could reset the episode,\nchange which gate is open, and observe what the agent does\n(Figure 11c). But what if this is not possible?\nIn order to answer this question, we built a behavioral model\nusing prior knowledge and data. First, we trained two agents\nthat were rewarded for collecting either a green or red re-\nward respectively. These agents were then used to create\nlikelihood models for the two hypotheses using Monte-Carlo\nsampling. Second, we placed a uniform prior over the two\nhypotheses and on the open door, and assumed that neither\nvariable precedes the other causally. The resulting causal\nmodel, shown in Figure 12, uses three random variables:\nA∈{gr,re}denotes the agent type (green-loving or red-\nloving); D ∈{l,r}stands for the open door; and ﬁnally\nR ∈{gr,re}corresponds to the reward collected by the\nagent.\nPage 9\nCausal Analysis of Agent Behavior for AI Safety\na) b) c)\nFigure 11.The gated-room environments. Panel a: In each instance of the environment, either the left or the right gate will be open\nrandomly. The goal of the agent is to pick up either a red or green reward, after which the episode terminates. Panels b & c: Counterfactual\nestimation. If the right door is open and we observe the agent picking up the red reward (b), then we can predict that the agent would pick\nup the red reward had the left door been open (c).\nR\nA D\nFigure 12.Causal model for the gated-room environment. A corre-\nsponds to the type of agent (green- or red-pill loving); D indicates\nwhich one of the two doors is open; and R denotes the color of the\npill picked up by the agent.\nResults. We performed a number of queries on the model.\nThe results are shown in Table 4. We ﬁrst performed three\nsanity checks. Before seeing any evidence, we see that the\nprior probabilities P(R= gr) and P(R= re) of a random\nagent picking either a green or a red reward is 0.5. After\nobserving the agent picking up a red reward (R= re) when\nthe left gate is open ( D = l), we conclude that it must\nbe a red-loving agent ( A = re) with probability 0.9960.\nNote that since the hypothesis about the agent type and the\nopened door are independent, this probability is the same if\nwe remove the door from the condition.\nHaving seen a trajectory, we can condition our model and\nask the counterfactual question. Formally, this question is\nstated as\nP(RD=r = re|D= l,R = re),\nthat is, given that we have observed D = l and R = re,\nwhat is the probability of the potential responseRD=r = re,\nthat is, R = re had D = r been the case? The result,\n0.9920 ≈1, tells us that the agent would also have picked\nup the red reward had the other door been open, which\nis in line with our expectations. Furthermore, due to the\nsymmetry of the model, we get the same result for the\nprobability of picking a green reward had the right door\nbeen open for an agent that picks up a green reward when\nthe left door is open.\nTable 4.Gated-room queries\nQUERIES PROBABILITY\nP(R = re) 0.500\nP(A = re | R = re) 0.996\nP(A = re | D = l, R= re) 0.996\nP(RD=r = re | D = l, R= re) 0.992\nP(RD=r = gr | D = l, R= gr) 0.992\nDiscussion. Following the example above, we can natu-\nrally see that we are only able to ask counterfactual ques-\ntions about the behavior of a particular agent when we can\nrely on prior knowledge about a reference agent population.\nFor instance, this is the case when the agent under study\nwas drawn from a distribution of agents for which we have\nsome previous data or reasonable priors. If we do not have\na suitable reference class, then we cannot hope to make\nmeaningful counterfactual claims.\n3.5. Causal induction\nProblem. What is the causal mechanism driving an ob-\nserved behavior? Discovering the mechanisms which under-\nlie an agent’s behavior can be considered the fundamental\nproblem of agent analysis. All the use cases reviewed so\nfar depend on the analyst knowing the causal structure gov-\nerning the agent’s behavior. However this model is often\nnot available in a black-box scenario. In this case, the ﬁrst\ntask of the analyst is to discover the behavioral mechanisms\nthrough carefully probing the agent with a variety of in-\nputs and recording their responses (Grifﬁths & Tenenbaum,\n2005).\nDiscovering causal structure is an induction problem. This\nis unlike a deduction task, where the analyst can derive un-\nPage 10\nCausal Analysis of Agent Behavior for AI Safety\nFigure 13.The mimic environment. Both agents either step to the\nleft or the right together. The analyst’s goal is to discover which\none is the lead, and which one is the imitator.\nequivocal conclusions from a set of facts. Rather, induction\nproblems do not have right or wrong answers and require\nmaintaining multiple plausible explanations (Rathmanner &\nHutter, 2011).\nIn this use case, we demonstrate how to induce a distribu-\ntion over competing causal models for explaining an agent’s\nbehavior given experimental data. Although temporal order\nis often informative about the causal dependencies among\nrandom variables, the careful analyst must consider the pos-\nsibility that a cause and its effect might be observed simul-\ntaneously or in reversed temporal order. Thus, in general,\nobserving does not sufﬁce: to test a causal dependency the\nanalyst must manipulate one variable and check whether it\ninﬂuences another3. This principle is often paraphrased as\n“no causes in, no causes out” (Cartwright et al., 1994).\nSetup. We exemplify how to induce a causal dependency\nusing the mimic environment shown in Figure 13. Two\nagents, blue and red, are placed in a corridor. Then, both\nagents move simultaneously one step in either direction.\nOne of the two agents is the leader and the other the imi-\ntator: the leader chooses its direction randomly, whereas\nthe imitator attempts to match the leader’s choice in the\nsame time step, but sampling a random action 10% of the\ntime. The analyst’s task is to ﬁnd out which agent is the\nleader. Note there is no way to answer this question from\nobservation alone.\nExperiment. We built the causal model as follows. First,\nwe decided to model this situation using three random vari-\nables: L ∈ {b,r}, corresponding to the hypothesis that\neither the blue or red agent is the leader, respectively;\nB ∈ {l,r}, denoting the step the blue agent takes; and\nsimilarly R∈{l,r}for the red agent. The likelihood mod-\n3Although, there are cases where partial structure can be de-\nduced from observation alone—see Pearl (2009, Chapter 2)\nR\nBB\nR\nor\nFigure 14.Causal models for the mimic environment. Each model\nhas the same prior probability is being correct. B and R indicate\nthe direction in which the blue and the red agents respectively\nmove.\nels were estimated from 1000 Monte-Carlo rollouts, where\neach rollout consists of an initial and second time step. With\nthe constructed dataset we were able to estimate the joint\ndistribution P(B,R). Since this distribution is purely obser-\nvational and thus devoid of causal information, we further\nfactorized it according to our two causal hypotheses, namely\nP(B,R) =P(B)P(R|B) (12)\nfor the hypothesis that blue is the leader (L= b), and\nP(B,R) =P(R)P(B|R) (13)\nfor the competing hypothesis ( L = r). This yields two\ncausal models. Finally, we placed a uniform prior over the\ntwo causal models L= band L= a. See Figure 14. Notice\nthat both causal models are observationally indistinguish-\nable.\nThis symmetry can be broken through intervention. To do\nso, we force the red agent into a random direction (say,\nleft) and record the response of the blue agent (left). The\nposterior probabilities over the intervened hypotheses are\nthen proportional to\nP(L= b|do(R= l),B = l) ∝\nP(L= b)P(B = l|L= b), and\nP(L= r|do(R= l),B = l) ∝\nP(L= r)P(B = l|L= r,R = l). (14)\nNotice how the intervened factors drop out of the likelihood\nterm.\nTable 5.Mimic queries\nQUERIES PROBABILITY\nP(L = b) 0.500\nP(L = b | R = l, B= l) 0.500\nP(L = b | R = l, B= r) 0.500\nP(L = b | do(R = l), B= l) 0.361\nP(L = b | do(R = l), B= r) 0.823\nPage 11\nCausal Analysis of Agent Behavior for AI Safety\nResult. We performed the queries shown in Table 5. The\nﬁrst three queries show that observation does not yield evi-\ndence for any of the causal hypotheses:\nP(L= b) =P(L= b|R= l,B = l)\n= P(L= b|R= l,B = r).\nHowever, pushing the red agent to the left renders the two\nhypotheses asymmetrical, as can be seen by\nP(L= b) ̸= P(L= b|do(R= l),B = l)\n̸= P(L= b|do(R= l),B = r).\nThus, observing that the blue agent moves to the right after\nour intervention allows us to conclude that the blue agent is\nlikely to be the leader.\nDiscussion. Our experiment illustrates a Bayesian pro-\ncedure for discovering the causal mechanisms in agents.\nThe main take-away is that inducing causal mechanisms\nrequires: (a) postulating a collection of causal hypotheses,\neach one proposing alternative mechanistic explanations for\nthe same observed behavior; and (b) carefully selecting and\napplying manipulations in order to render the likelihood of\nobservations unequal.\n3.6. Causal pathways\nProblem. How do we identify an agent’s decision-making\npathways? In previous examples we have focused on study-\ning how environmental factors inﬂuence the agent’s behav-\nior. However, we did not isolate the speciﬁc chain of mecha-\nnisms that trigger a decision. Understanding these pathways\nis crucial for identifying the sources of malfunction. To\nestimate the effect of a given pathway, one can chain to-\ngether the effects of the individual mechanisms along the\npath (Shpitser, 2013; Chiappa, 2019).\nSetup. We illustrate the analysis of causal pathways using\nthe key-door environment shown in Figure 15. The agent\nﬁnds itself in a room where there is a key and a door. The\nstarting position of the agent, the location of the key, and the\nstate of the door (open/closed) are all randomly initialized.\nBehind the door there is a reward which terminates the\nepisode when picked up.\nWe consider two agent subjects. Agent A appears to only\npick-up the key if the door is closed and then collects the\nreward. This agent acquired this policy by training it on the\nentire set of initial conﬁgurations (i.e. open/closed doors,\nkey and agent positions). Agent B always collects the key,\nirrespective of the state of the door, before navigating toward\nthe reward. This behavior was obtained by training the agent\nonly on the subset of instances where the door was closed.\nNonetheless, both policies generalize. The analyst’s task is\nFigure 15.The key-door environment. The goal of the agent is to\ncollect the reward, which terminates the episode. However, the\nreward is behind a door which is sometimes closed. To open it, the\nagent must collect a key ﬁrst.\nK R\nD\nK R\nD\nA = a A = b\nFigure 16.Causal models for the key-door environment. D in-\ndicates whether the door is open; K ﬂags whether the agent\npicks up the key; and R denotes whether the agent collects the\nreward pill. Here, the second model does not include the pathway\nD → K → R; hence, the agent picks up the key irrespective of\nthe state of the door.\nto determine the information pathway used by the agents in\norder to solve the task; in particular, whether the agent is\nsensitive to whether the door is open or closed.\nExperiment. We chose three random variables to model\nthis situation: D ∈{o,c}, determining whether the door\nis initially open or closed; K ∈{y,n}, denoting whether\nthe agent picked up the key; and ﬁnally, R ∈{1,0}, the\nobtained reward. Figure 16 shows the causal models.\nResults. We investigate the causal pathways through a\nnumber of queries listed in Table 6. First, we verify that\nboth agents successfully solve the task, i.e. P(R= 1)≈1.\nNow we proceed to test for the causal effect of the initial\nstate of the door on the reward, via the key collection activity.\nIn other words, we want to verify whether D →K →R.\nThis is done in a backwards fashion by chaining the causal\neffects along a path.\nFirst, we inspect the link K →R. In the case of agent A,\nthe reward appears to be independent of whether the key is\ncollected, since\nP(R= 1|K = y) ≈P(R= 1|K = n) ≈1.\nPage 12\nCausal Analysis of Agent Behavior for AI Safety\nTable 6.Key-door queries\nQUERIES A = a A = b\nP(R = 1) 0.977 0.991\n—\nP(R = 1| K = y) 0.974 0.993\nP(R = 1| K = n) 0.989 0.445\nP(R = 1| do(K = y)) 0.979 0.993\nP(R = 1| do(K = n)) 0.497 0.334\n—\nP(K = y | do(D = c)) 0.998 0.998\nP(K = y | do(D = o)) 0.513 0.996\n—\nP(R = 1| D = c) 0.960 0.988\nP(R = 1| D = o) 0.995 0.995\nf(D = c), SEE (15) 0.978 0.992\nP(D = o), SEE (15) 0.744 0.991\nHowever, this is association and not causation. The causal\neffect of collecting the key is tested by comparing the inter-\nventions, that is,\nP(R= 1|do(K = y)) −P(R= 1|do(K = n)).\nHere it is clearly seen that both agents use this mechanism\nfor solving the task, since the difference in probabilities is\nhigh. This establishes K →R.\nSecond, we ask for the causal effect of the initial state of\nthe door on collecting the key, i.e. D →K. Using the\nsame rationale as before, this is veriﬁed by comparing the\nintervened probabilities:\nP(K = y|do(D= c)) −P(K = y|do(D= o)).\nHere we observe a discrepancy: agent A is sensitive toDbut\nagent B is not. For the latter, we conclude D̸→K →R.\nFinally, we estimate the causal effect the state of the door\nhas on the reward, along the causal pathways going through\nthe settings of K. Let us inspect the case D = c. The\nconditional probability is\nP(R= 1|D= c) =\n∑\nk∈{y,n}\nP(R= 1|K = k,D = c)P(K = k|D= c),\nand we can easily verify that P(R= 1|D) ≈P(R= 1),\nthat is, D and Rare independent. But here again, this is\njust association. The causal response along the pathways is\ngiven by\nf(D= c) :=\n∑\nk∈{y,n}\nP(R= 1|do(K = k))P(K = k|do(D= c)),\n(15)\nwhich is known as a nested potential response (Carey et al.,\n2020) or a path-speciﬁc counterfactual (Chiappa, 2019).\nThe desired causal effect is then computed as the difference\nbetween closing and opening the door, i.e.\nf(D= c) −f(D= o).\nThis difference amounts to 0.2338 and 0.0014 ≈0 for the\nagents A and B respectively, implying that A does indeed\nuse the causal pathway D→K →Rbut agent B only uses\nK →R.\nDiscussion. Understanding causal pathways is crucial\nwhenever not only the ﬁnal decision, but also the speciﬁc\ncausal pathways an agent uses in order to arrive at said deci-\nsion matters. This understanding is critical for identifying\nthe sources of malfunctions and in applications that are sen-\nsitive to the employed decision procedure, such as e.g. in\nfairness (Chiappa, 2019). In this experiment we have shown\nhow to compute causal effects along a desired path using\nnested potential responses computed from chaining together\ncausal effects.\n4. Discussion and Conclusions\nRelated work. The analysis of black-box behavior dates\nback to the beginnings of electronic circuit theory (Cauer,\n1954) and was ﬁrst formalized in cybernetics (Wiener, 1948;\nAshby, 1961), which stressed the importance of manipula-\ntions in order to investigate the mechanisms of cybernetic\nsystems. However, the formal machinery for reasoning\nabout causal manipulations and their relation to statistical\nevidence is a relatively recent development (Spirtes et al.,\n2000; Pearl, 2009; Dawid, 2015).\nA recent line of research related to ours that explicitly uses\ncausal tools for analyzing agent behavior is Everitt et al.\n(2019) and Carey et al. (2020). These studies use causal\nincentive diagrams to reason about the causal pathways\nof decisions in the service of maximizing utility functions.\nOther recent approaches for analyzing AI systems have\nmostly focused on white-box approaches for improving\nunderstanding (see for instance Mott et al., 2019; Verma\net al., 2018; Montavon et al., 2018; Puiutta & Veith, 2020)\nand developing safety guarantees (Uesato et al., 2018). A\nnotable exception is the work by Rabinowitz et al. (2018), in\nwhich a model is trained in order to predict agent behavior\nfrom observation in a black-box setting.\nScope. In this report we have focused on the black-box\nstudy of agents interacting with (artiﬁcial) environments,\nbut the methodology works in a variety of other settings:\npassive agents like sequence predictors, systems with inter-\nactive user interfaces such as language models and speech\nsynthesizers, and multi-agent systems. For example, con-\nsider GPT-3 (Brown et al., 2020), a natural language model\nPage 13\nCausal Analysis of Agent Behavior for AI Safety\nwith text-based input-output. This system can be seen as a\nperception-action system, for which our methodology ap-\nplies. A bigger challenge when dealing with models systems\nmight be to come up with the right hypotheses, problem ab-\nstractions, and interventions.\nFeatures and limitations. The main challenge in the prac-\ntice of the proposed methodology is to come up with the\nright hypotheses and experiments. This task requires in-\ngenuity and can be very labor-intensive (Section 2.4). For\ninstance, while in the grass-sand environment it was easy\nto visually spot the confounding variable (Section 3.1), we\ncannot expect this to be a viable approach in general. Or,\nas we have seen in the problem of causal induction (Sec-\ntion 3.5), it is non-trivial to propose a model having a causal\nordering of the variables that differs from the sequence in\nwhich they appear in a sampled trajectory. Given the inher-\nent complexity of reasoning about causal dependencies and\nthe state of the art in machine learning, it is unclear how to\nscale this process through e.g. automation.\nOn the plus side, the methodology naturally leads to human-\nexplainable theories of agent behavior, as it is human ana-\nlysts who propose and validate them. As illustrated in our\nexamples, the explanations do not make reference to the\ntrue underlying mechanisms of agents (e.g. the individual\nneuronal activations), but instead rely on simpliﬁed con-\ncepts (i.e. the model variables) that abstract away from the\nimplementation details. See also Rabinowitz et al. (2018)\nfor a discussion. The human analyst may also choose an\nappropriate level of detail of an explanation, for instance\nproposing general models for describing the overall behav-\nior of an agent and several more detailed models to cover\nthe behavior in speciﬁc cases.\nWe have not addressed the problem of quantifying the un-\ncertainty in our models. When estimating the conditional\nprobabilities of the causal models from a limited amount\nof Monte-Carlo samples, there exists the possibility that\nthese deviate signiﬁcantly from the true probabilities. In\nsome cases, this could lead to the underestimation of the\nprobability of failure modes. To quantify the reliability of\nestimates, one should supplement them with conﬁdence in-\ntervals, ideally in a manner to aid the assessment of risk\nfactors. In this work we have simply reported the number of\nsamples used for estimation. Developing a more systematic\napproach is left for future work.\nConclusions and outlook. This technical report lays out\na methodology for the systematic analysis of agent behavior.\nThis was motivated by experience: previously, we have all\ntoo often fallen into the pitfalls of misinterpreting agent\nbehavior due to the lack of a rigorous method in our ap-\nproach. Just as we expect new medical treatments to have\nundergone a rigorous causal study, so too do we want AI\nsystems to have been subjected to similarly stringent tests.\nWe have shown in six simple situations how an analyst can\npropose and validate theories about agent behaviour through\na systematic process of explicitly formulating causal hy-\npotheses, conducting experiments with carefully chosen\nmanipulations, and conﬁrming the predictions made by the\nresulting causal models. Crucially, we stress that this mech-\nanistic knowledge could only be obtained via directly inter-\nacting with the system through interventions. In addition,\nwe greatly beneﬁted from the aid of an automated causal\nreasoning engine, as interpreting causal evidence turns out\nto be a remarkably difﬁcult task. We believe this is the way\nforward for analyzing and establishing safety guarantees as\nAI agents become more complex and powerful.\nAcknowledgements\nThe authors thank Tom Everitt, Jane X. Wang, Tom Schaul,\nand Silvia Chiappa for proof-reading and providing numer-\nous comments for improving the manuscript.\nA. Architecture and training details\nIn our experiments we use agents with the following archi-\ntecture: 3 convolutional layers with 128 channels (for each\ntile type) each and 3 ×3 kernels; a dense linear layer with\n128 units; a single LSTM layer with 128 units (Hochreiter\n& Schmidhuber, 1997); a dense linear layer with 128 units;\nand a softmax activation layer for producing stochastic ac-\ntions. To train them, we used the Impala policy gradient\nalgorithm (Espeholt et al., 2018). The gradients of the recur-\nrent network were computed with backpropagation through\ntime (Robinson & Fallside, 1987; Werbos, 1988), and we\nused Adam for optimization (Kingma & Ba, 2014). During\ntraining, we randomized the environment and agent seed,\nforcing the agent to interact with different settings and pos-\nsibly meta-learn a general policy.\nReferences\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schul-\nman, J., and Man ´e, D. Concrete problems in ai safety.\narXiv preprint arXiv:1606.06565, 2016.\nAndrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W.,\nPfau, D., Schaul, T., Shillingford, B., and De Freitas, N.\nLearning to learn by gradient descent by gradient descent.\nIn Advances in neural information processing systems ,\npp. 3981–3989, 2016.\nArjovsky, M., Bottou, L., Gulrajani, I., and Lopez-\nPaz, D. Invariant risk minimization. arXiv preprint\narXiv:1907.02893, 2019.\nPage 14\nCausal Analysis of Agent Behavior for AI Safety\nAshby, W. R. An introduction to cybernetics. Chapman &\nHall Ltd, 1961.\nBakker, B. Reinforcement learning with long short-term\nmemory. Advances in neural information processing\nsystems, 14:1475–1482, 2001.\nBareinboim, E., Forney, A., and Pearl, J. Bandits with unob-\nserved confounders: A causal approach. In Advances in\nNeural Information Processing Systems, pp. 1342–1350,\n2015.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\nCarey, R., Langlois, E., Everitt, T., and Legg, S.\nThe incentives that shape behaviour. arXiv preprint\narXiv:2001.07118, 2020.\nCartwright, N. et al. Nature’s capacities and their measure-\nment. OUP Catalogue, 1994.\nCauer, W. Theorie der linearen Wechselstromschaltungen,\nvolume 1. Akademie-Verlag, 1954.\nChiappa, S. Path-speciﬁc counterfactual fairness. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 33, pp. 7801–7808, 2019.\nDawid, A. P. Causal inference without counterfactuals.\nJournal of the American statistical Association, 95(450):\n407–424, 2000.\nDawid, A. P. Statistical causality from a decision-theoretic\nperspective. Annual Review of Statistics and Its Applica-\ntion, 2:273–303, 2015.\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V .,\nWard, T., Doron, Y ., Firoiu, V ., Harley, T., Dunning, I.,\net al. Impala: Scalable distributed deep-RL with impor-\ntance weighted actor-learner architectures. arXiv preprint\narXiv:1802.01561, 2018.\nEveritt, T., Ortega, P. A., Barnes, E., and Legg, S. Un-\nderstanding agent incentives using causal inﬂuence di-\nagrams, part i: single action settings. arXiv preprint\narXiv:1902.09980, 2019.\nFisher, R. A. Design of experiments. Br Med J, 1(3923):\n554–554, 1936.\nGrifﬁths, T. L. and Tenenbaum, J. B. Structure and strength\nin causal induction. Cognitive psychology, 51(4):334–\n384, 2005.\nHalpern, J. Y . and Hitchcock, C. Actual causation and the\nart of modeling. arXiv preprint arXiv:1106.2652, 2011.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nLeike, J., Martic, M., Krakovna, V ., Ortega, P. A., Everitt,\nT., Lefrancq, A., Orseau, L., and Legg, S. Ai safety\ngridworlds. arXiv preprint arXiv:1711.09883, 2017.\nMontavon, G., Samek, W., and M ¨uller, K.-R. Methods\nfor interpreting and understanding deep neural networks.\nDigital Signal Processing, 73:1–15, 2018.\nMott, A., Zoran, D., Chrzanowski, M., Wierstra, D., and\nRezende, D. J. Towards interpretable reinforcement\nlearning using attention augmented agents. In Advances\nin Neural Information Processing Systems , pp. 12350–\n12359, 2019.\nOlton, D. S. Mazes, maps, and memory. American psychol-\nogist, 34(7):583, 1979.\nPearl, J. Causality. Cambridge university press, 2009.\nPearl, J. and Mackenzie, D. The book of why: the new\nscience of cause and effect. Basic Books, 2018.\nPearl, J., Glymour, M., and Jewell, N. P. Causal inference\nin statistics: A primer. John Wiley & Sons, 2016.\nPuiutta, E. and Veith, E. M. S. P. Explainable reinforcement\nlearning: A survey. In Holzinger, A., Kieseberg, P., Tjoa,\nA. M., and Weippl, E. (eds.), Machine Learning and\nKnowledge Extraction, pp. 77–95, Cham, 2020. Springer\nInternational Publishing. ISBN 978-3-030-57321-8.\nPuterman, M. L. Markov decision processes: discrete\nstochastic dynamic programming. John Wiley & Sons,\n2014.\nRabinowitz, N. C., Perbet, F., Song, H. F., Zhang, C., Eslami,\nS., and Botvinick, M. Machine theory of mind. arXiv\npreprint arXiv:1802.07740, 2018.\nRathmanner, S. and Hutter, M. A philosophical treatise of\nuniversal induction. Entropy, 13(6):1076–1136, 2011.\nRobinson, A. and Fallside, F. The utility driven dynamic\nerror propagation network. University of Cambridge\nDepartment of Engineering Cambridge, 1987.\nRussell, S., Dewey, D., and Tegmark, M. Research prior-\nities for robust and beneﬁcial artiﬁcial intelligence. Ai\nMagazine, 36(4):105–114, 2015.\nShpitser, I. Counterfactual graphical models for longitu-\ndinal mediation analysis with unobserved confounding.\nCognitive science, 37(6):1011–1035, 2013.\nPage 15\nCausal Analysis of Agent Behavior for AI Safety\nSpirtes, P., Glymour, C. N., Scheines, R., and Heckerman,\nD. Causation, prediction, and search. MIT press, 2000.\nStepleton, T. The pycolab game engine, 2017. URL\nhttps://github. com/deepmind/pycolab, 2017.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction. MIT press, 2018.\nTishby, N. and Polani, D. Information theory of decisions\nand actions. In Perception-action cycle, pp. 601–636.\nSpringer, 2011.\nUesato, J., Kumar, A., Szepesvari, C., Erez, T., Ruderman,\nA., Anderson, K., Dvijotham, K. D., Heess, N., and Kohli,\nP. Rigorous agent evaluation: An adversarial approach\nto uncover catastrophic failures. In International Confer-\nence on Learning Representations, 2018.\nVerma, A., Murali, V ., Singh, R., Kohli, P., and Chaudhuri,\nS. Programmatically interpretable reinforcement learning.\nIn International Conference on Machine Learning , pp.\n5045–5054, 2018.\nV olodin, S., Wichers, N., and Nixon, J. Resolving spuri-\nous correlations in causal models of environments via\ninterventions. arXiv preprint arXiv:2002.05217, 2020.\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H.,\nLeibo, J. Z., Munos, R., Blundell, C., Kumaran, D., and\nBotvinick, M. Learning to reinforcement learn. arXiv\npreprint arXiv:1611.05763, 2016.\nWerbos, P. J. Generalization of backpropagation with appli-\ncation to a recurrent gas market model. Neural networks,\n1(4):339–356, 1988.\nWiener, N. Cybernetics or Control and Communication in\nthe Animal and the Machine. John Wiley & Sons, 1948.\nPage 16",
  "concepts": [
    {
      "name": "Ask price",
      "score": 0.7187821865081787
    },
    {
      "name": "Software deployment",
      "score": 0.651202380657196
    },
    {
      "name": "Computer science",
      "score": 0.5994473695755005
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45899727940559387
    },
    {
      "name": "Causal analysis",
      "score": 0.4389641582965851
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.36981990933418274
    },
    {
      "name": "Data science",
      "score": 0.331129252910614
    },
    {
      "name": "Software engineering",
      "score": 0.15159812569618225
    },
    {
      "name": "Business",
      "score": 0.07809674739837646
    },
    {
      "name": "Finance",
      "score": 0.0
    }
  ],
  "topic": "Ask price",
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 6
}