{
  "title": "How should the advent of large language models affect the practice of science?",
  "url": "https://openalex.org/W4389395901",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2550663075",
      "name": "Marcel Binz",
      "affiliations": [
        "Max Planck Institute for Biological Cybernetics"
      ]
    },
    {
      "id": "https://openalex.org/A2903539023",
      "name": "Stephan Alaniz",
      "affiliations": [
        "TH Bingen University of Applied Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2312243811",
      "name": "Adina Roskies",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2039560222",
      "name": "Balazs Aczel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2480966746",
      "name": "Carl Bergstrom",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2131426267",
      "name": "Colin Allen",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2284965573",
      "name": "Daniel Schad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2077204653",
      "name": "Dirk U. Wulff",
      "affiliations": [
        "University of Basel",
        "Rutgers, The State University of New Jersey",
        "Max Planck Institute for Human Development"
      ]
    },
    {
      "id": "https://openalex.org/A2732636637",
      "name": "Jevin West",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2119911159",
      "name": "Qiong Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4264455536",
      "name": "Rich Shiffrin",
      "affiliations": [
        "Indiana University Bloomington"
      ]
    },
    {
      "id": "https://openalex.org/A2134078776",
      "name": "Samuel J. Gershman",
      "affiliations": [
        "University of Milano-Bicocca",
        "University of Zurich",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2167913591",
      "name": "Vencislav Popov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1957609366",
      "name": "Emily M. Bender",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2213070553",
      "name": "Marco Marelli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4261662216",
      "name": "Matthew M. Botvinick",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A294021452",
      "name": "Zeynep Akata",
      "affiliations": [
        "TH Bingen University of Applied Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2117491939",
      "name": "Eric Schulz",
      "affiliations": [
        "Max Planck Institute for Biological Cybernetics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4289523162",
    "https://openalex.org/W4318464200",
    "https://openalex.org/W6891809357",
    "https://openalex.org/W4319332853",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4387607602",
    "https://openalex.org/W4384155269",
    "https://openalex.org/W4319457949",
    "https://openalex.org/W3035296331",
    "https://openalex.org/W3112689365",
    "https://openalex.org/W6837621903",
    "https://openalex.org/W2964539095",
    "https://openalex.org/W6849116879",
    "https://openalex.org/W2962972512",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4384663467",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W3215285253",
    "https://openalex.org/W4221100501",
    "https://openalex.org/W6782465632",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W4378472529",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W4322760121",
    "https://openalex.org/W1579838312",
    "https://openalex.org/W4385965989",
    "https://openalex.org/W4322708560",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W2322532428",
    "https://openalex.org/W4387355914",
    "https://openalex.org/W4382619745",
    "https://openalex.org/W4297435087",
    "https://openalex.org/W4318146754",
    "https://openalex.org/W4385714397",
    "https://openalex.org/W4386185625",
    "https://openalex.org/W4330336443",
    "https://openalex.org/W4365450342",
    "https://openalex.org/W3125358881",
    "https://openalex.org/W4386510404",
    "https://openalex.org/W2766447205",
    "https://openalex.org/W4387560611",
    "https://openalex.org/W4366850553",
    "https://openalex.org/W4362722657",
    "https://openalex.org/W3172205429",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4387500714",
    "https://openalex.org/W4386364180",
    "https://openalex.org/W4388202310",
    "https://openalex.org/W4367369681",
    "https://openalex.org/W4322743583",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W4283317394",
    "https://openalex.org/W4367188881",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W1490781083",
    "https://openalex.org/W4320495408",
    "https://openalex.org/W4283751222",
    "https://openalex.org/W2373836078",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W2026119611",
    "https://openalex.org/W2972680241",
    "https://openalex.org/W4313545395",
    "https://openalex.org/W4367189652",
    "https://openalex.org/W4376117416",
    "https://openalex.org/W4363671827",
    "https://openalex.org/W4323657833",
    "https://openalex.org/W4287674181"
  ],
  "abstract": "Large language models (LLMs) are being increasingly incorporated into scientific workflows. However, we have yet to fully grasp the implications of this integration. How should the advent of large language models affect the practice of science? For this opinion piece, we have invited four diverse groups of scientists to reflect on this query, sharing their perspectives and engaging in debate. Schulz et al. make the argument that working with LLMs is not fundamentally different from working with human collaborators, while Bender et al. argue that LLMs are often misused and over-hyped, and that their limitations warrant a focus on more specialized, easily interpretable tools. Marelli et al. emphasize the importance of transparent attribution and responsible use of LLMs. Finally, Botvinick and Gershman advocate that humans should retain responsibility for determining the scientific roadmap. To facilitate the discussion, the four perspectives are complemented with a response from each group. By putting these different perspectives in conversation, we aim to bring attention to important considerations within the academic community regarding the adoption of LLMs and their impact on both current and future scientific practices.",
  "full_text": "How should the advent of large language models\naffect the practice of science?\nMarcel Binza,q,1,*, Stephan Alanizb,1, Adina Roskiesd, Balazs Aczele, Carl T. Bergstromc,\nColin Allenf, Daniel Schadg, Dirk Wulffh,i, Jevin D. Westc, Qiong Zhangj, Richard M.\nShiffrink, Samuel J. Gershmanl, Ven Popovm, Emily M. Benderc,2, Marco Marellin,2, Matthew\nM. Botvinicko,p,2, Zeynep Akatab,3, and Eric Schulza,q,2,3\naMax Planck Institute for Biological Cybernetics\nbUniversity of T¨ubingen\ncUniversity of Washington\ndUniversity of California Santa Barbara\neE¨otv¨os Lor´and University, Budapest\nfUniversity of California Santa Barbara\ngHealth and Medical University, Potsdam\nhMax-Planck-Institute for Human Development\niUniversity of Basel\njRutgers University, New Brunswick\nkIndiana University, Bloomington\nlHarvard University\nmUniversity of Zurich\nnUniversity of Milano-Bicocca\noGoogle DeepMind\npUniversity College London\nqHelmholtz Center for Computational Health, Munich\n1Joint first authors\n2Perspective leaders\n3Joint senior authors\n*marcel.binz@tuebingen.mpg.de\nABSTRACT\nLarge language models (LLMs) are being increasingly incorporated into scientific workflows. However, we have yet to fully\ngrasp the implications of this integration. How should the advent of large language models affect the practice of science? For\nthis opinion piece, we have invited four diverse groups of scientists to reflect on this query, sharing their perspectives and\nengaging in debate. Schulz et al. make the argument that working with LLMs is not fundamentally different from working with\nhuman collaborators, while Bender et al. argue that LLMs are often misused and over-hyped, and that their limitations warrant\na focus on more specialized, easily interpretable tools. Marelli et al. emphasize the importance of transparent attribution and\nresponsible use of LLMs. Finally, Botvinick and Gershman advocate that humans should retain responsibility for determining\nthe scientific roadmap. To facilitate the discussion, the four perspectives are complemented with a response from each\ngroup. By putting these different perspectives in conversation, we aim to bring attention to important considerations within the\nacademic community regarding the adoption of LLMs and their impact on both current and future scientific practices.\nSignificance Statement:\nArtificial intelligence (AI) is reshaping the way researchers conduct science. Large language models (LLMs) in particular have\nreceived attention for their apparent versatility in reading, analyzing, and writing text. Y et, at the same time, there are also\nconsiderable concerns that come with the use of this technology, which could potentially place our scientific integrity at risk.\nThis raises the question: how should LLMs affect the practice of science? In this manuscript, we present four perspectives\non this issue, each one from a different group of researchers. The perspectives demonstrate the extent of both issues and\nopportunities, ranging from encouraging the use of LLMs in science to a stern warning against using LLMs in most, if not all, of\nthe proposed cases.\nLanguage models are statistical models of human language that can be used to predict the next token (e.g., a word or\ncharacter) for a given text sequence. Even though these models have been around for decades1, 2, they have recently experienced\nan unprecedented renaissance: by training enormous neural networks with billions of parameters on data sets with trillions\nof tokens, researchers have observed the emergence of models whose abilities can go beyond mere text generation and\nconversational skills3.\nModern large language models (LLMs) are, amongst other things, able to solve selected university-level math problems4,\nsupport language translation5, or answer questions in a bar exam with high accuracy6, out of the box and without additional\ntraining. Given the range of these capabilities, it seems possible that these systems will have an enormous impact on our society,\nleaving their mark on the labor market7, the education system8, and many other parts of our daily lives.\nWe — as scientists — may therefore wonder: how will the advent of LLMs affect the practice of science? Finding answers\nto this question is urgent as LLMs are already starting to permeate the academic landscape9–17. For instance, in 2022, MetaAI\nreleased the first science-specific LLM (under the name Galactica) aimed to support researchers in the process of knowledge\ndiscovery18. Even more recently, Terence Tao, a Fields Medal-winning mathematician, proclaimed19 that “the 2023-level AI\ncan already generate [. . .] promising leads to a working mathematician [. . .]. When integrated with tools such as formal proof\nverifiers, internet search, and symbolic math packages, I expect, say, 2026-level AI [. . .] will be a trustworthy co-author [. . .].”\nYet while there have been claims of immense potential for this technology for the advancement of science, there are also\nconsiderable concerns that need to be taken into account. For instance, the aforementioned Galactica model had to be taken\noffline after just three days because it was heavily criticized by researchers for fabricating information, such as “fake papers\n(sometimes attributing them to real authors), and [...] wiki articles about the history of bears in space”20. Furthermore, even\nthough LLMs often achieve state-of-the-art performance on existing benchmarks, it remains debated whether this reflects\ngenuine understanding, or whether they are merely regurgitating the training set, thereby acting like stochastic parrots21. It has\nbeen, for instance, repeatedly demonstrated that even the most capable models at present fail at basic arithmetic problems such\nas multiplying two four-digit numbers22. Flaws like these are especially concerning if we intend to utilize LLMs for research\npurposes, and could endanger the integrity of science if we act carelessly.\nThe objective of the present article is to provide researchers with different opinions and a forum to voice and discuss their\nperspectives on if and how we should make use of LLMs in the context of science. To facilitate this discussion, we will first\nhighlight a few applications where LLMs have the potential to positively impact science, followed by pointing out some of the\nissues that come with them.\nBackground: applications of LLMs in science\nLLMs find their most obvious use case as a supporting tool for scientific writing. For example, as proofreaders of manuscript\ndrafts, they can aid in rectifying grammatical errors, improving the writing style, and ensuring adherence to editorial guidelines.\nBeyond manuscript composition, LLMs could prove valuable for data acquisition and analysis in domains that were traditionally\nreliant on manual human labor23, 24. Researchers have even suggested using LLMs as potential substitutes for human participants,\nas proxies25 or for pilot studies 26. In computational fields, LLMs could speed up prototyping by writing code 27, while a\nhuman-in-the-loop would guide these processes, correct LLM-generated errors and ultimately decide which ideas warrant\nfurther pursuit. Moreover, researchers might experiment with employing LLMs at certain stages of research with progressively\nreduced supervision28, potentially leading to increased automation in some aspects of scientific exploration and discovery.\nWhile the potential influence of LLMs on the practice of science is immense, there are pressing issues that come with\nthe use of LLMs in the context of science. For instance, when an LLM helps us to write text, who ensures that its output is\nnot subject to plagiarism issues 29? LLMs learn from web-sourced text data, acquiring inherent biases 30–32 and — in some\ncases — replicate excerpts from their training data33. When an LLM is used for data analysis, what happens when it hallucinates\ndata? The content generated by LLMs can contain errors or fabricated information, presenting a potential threat to the integrity\nof scientific publishing12. When an LLM suggests an idea, who gets credit for it? The general consensus within the scientific\ncommunity seems to indicate that LLMs are not eligible for (co-)authorship34 as they cannot be held accountable for upholding\nscientific precision and integrity. Leading AI conferences such as ICML1 and ACL2 — as well as journals such as Science3,\nNature4 and PNAS5 — have already adopted policies to limit the involvement of LLMs. However, it remains an open question\nhow strong these regulations should be and if and how the usage of LLMs should be acknowledged.\nThese — and many other — issues raise the questions: Howshould the advent of LLMs affect the practice of science? Do\nLLMs actually improve our scientific output or are they rather hindering good scientific practice? To what extent should they be\n1https://icml.cc/Conferences/2023/llm-policy\n2https://2023.aclweb.org/blog/ACL-2023-policy/\n3https://www.science.org/content/page/science-journals-editorial-policies\n4https://www.nature.com/nature-portfolio/editorial-policies/ai\n5https://www.pnas.org/author-center/editorial-and-journal-policies#authorship-and-contributions\n2/13\nused given the ethical and legal issues that come with them? We believe these to be highly non-trivial questions without an\nobvious answer and have therefore invited four groups of researchers to provide their perspectives on them. These perspectives\nwere selected to cover a broad spectrum of opinions in order to spark a constructive discussion. Each of the perspectives is\naccompanied by a response from each group. We conclude this article with a short general discussion in which we attempt to\nidentify common themes.\nPerspective – LLMs: more like a human collaborator than a software tool\nContributors: Eric Schulz, Daniel Schad, Marcel Binz, Stephan Alaniz, Ven Popov, and Zeynep Akata\nMost researchers in our labs already frequently employ LLMs in their everyday work. They use them, amongst other things,\nto finetune and revise their drafts, as a supporting tool for programming, to suggest formulations for research items such as\nquestionnaires or experimental instructions, and to summarise research papers. We have observed a significant increase in\nquality in all of these areas after the widespread adoption of these models. While our personal experience may be biased, there\nare several studies supporting the idea that LLMs can facilitate writing35, coding36, and knowledge extraction37. In the future,\nwe expect these models to be even more deeply integrated into the scientific process, taking on roles similar to a collaborator\nwith whom one can develop and discuss ideas.\nIndeed, we believe that working with LLMs will not be fundamentally different from working with other collaborators,\nsuch as research assistants or doctoral students. LLMs are not perfect and have limitations and biases that could affect their\nperformance and output. However, humans are also subject to some of the same flaws, such as errors, plagiarism, fabrication, or\ndiscrimination. If we take this perspective, it seems appropriate to view current LLMs less as traditional software tools and\nmore as knowledgeable research assistants: they can do phenomenal work but we need to be aware that they can make mistakes.\nProtecting the past\nIt is our chief responsibility to ensure the quality and integrity of our work. There are already rules and norms about scientific\npractice in place to ensure this, and many of them also apply to LLMs. For instance, we should always check the accuracy and\nvalidity of the information and data we obtain, no matter the source, as well as correctly cite the sources and methods we use.\nThat means that we should not blindly trust or rely on LLMs, but rather use them as a complement to our own expertise and\njudgment. Furthermore, our work can only be criticized appropriately if all information about its methodology is transparently\ncommunicated. We should therefore acknowledge the contributions of LLMs to our research, just as we would do for any other\ntool. Ultimately, it is — and will remain — the authors’ responsibility to ensure that the appropriate scientific standards are\nfollowed, regardless of whether we use LLMs or not.\nEnsuring that our research is reproducible is one of the cornerstones of modern science. However, as many LLMs are\nproprietary, working with them poses a threat to this ideal. Nobody guarantees that OpenAI, Google, or other providers will\nnot make changes to their models (in the worst case, without informing the user). In fact, this happened to us during the\nrevision process of one of our papers, where, at some point, we could not reproduce our initial results, likely due to changes\non the provider side. How should we deal with such cases? We believe that the obvious solution to this problem is to rely on\nopen-source models where one has full control over all aspects of the model. Following a recent call for action to the European\nParliament38, we therefore strongly advocate for the development of such models, such that they can become the primary tool\nfor scientific inquiry.\nWelcoming the future\nPaper reviewing is another area where LLMs could improve our scientific pipeline. In a recent study, Liang and colleagues39\ndemonstrated this potential by systematically comparing LLM-generated reviews to reviews written by human researchers.\nThey found that “more than half (57.4%) of the users found GPT-4 generated feedback helpful or very helpful and 82.4% found\nit more beneficial than feedback from at least some human reviewers.” Not only does this result allow scientists — especially\nearly career researchers — to receive high-quality, instantaneous feedback (similar to that one could get from a critical colleague\nwith an unlimited amount of time) but it also has implications for the peer review process. Yet, the use of LLMs in the peer\nreview process also presents one major legal obstacle: manuscripts under review are typically confidential, and hence should\nnot be entered into proprietary LLMs. To prevent such breaches of confidentiality, the National Institutes of Health (NIH) and\nother institutions have rules in place that prohibit the use of LLMs for peer review.6 Locally hosted, open-source models are\nagain a solution to this issue, as they provide control about which information is shared with external sources and which is not.\nWe also would like to point out that LLMs are a moving target, constantly evolving and becoming more capable and\nautonomous. This may raise new challenges and questions for the scientific community in the future, such as how to evaluate,\n6https://grants.nih.gov/grants/guide/notice-files/NOT-OD-23-149.html\n3/13\ninterpret, and communicate the results generated by LLMs, or how to ensure their transparency and accountability. We welcome\nthese challenges as an opportunity to advance our understanding and methods of science. We also encourage researchers to\ncollaborate with each other and with LLM developers to address these issues and ensure that LLMs improve at frequently\ncriticised skills such as providing truthful sources or acknowledging ignorance.\nConclusion\nIn conclusion, LLMs are a valuable asset for science and should be embraced rather than feared or restricted. It becomes\napparent that they are not infallible machines once we start thinking about them as knowledgeable research assistants instead of\ntraditional software tools. Furthermore, since rules for good scientific practice are already in place, and since it is the authors’\nobligation to take responsibility for adhering to these rules, there is no need for novel rules with the use of LLMs. We believe\nthat strengthening the development of open-source alternatives should be one of our top priorities, as they “offer enhanced\nsecurity, explainability, and robustness due to their transparency and the vast community oversight”38. Finally, being conscious\nabout the current limitations of LLMs and embracing them, will allow us to grow with the technology as LLM research finds\nremedies and develops complementary tools. We hope that by adopting this liberal perspective, we can foster a positive and\nfruitful relationship between humans and LLMs in science. Please note that the first draft of our perspective was written by an\nLLM (GPT-4) based on our meeting notes.\nPerspective – Science is a social process that cannot be auto-completed\nContributors: Emily M. Bender, Carl T. Bergstrom, and Jevin D. West\nWhen deciding whether to use an LLM, it is important to recognize that LLMs are simply models of word form distributions\nextracted from text — not models of theinformation that people might get from reading that text40. Originally, such systems\nwere used to rank or classify text. In automatic transcription, for example, an acoustic model provides a set of possibilities and\nthe language model helps determine the most likely next word41. Today, however, LLMs are vaunted for their ability to extrude\nsynthetic text by repeatedly selecting a next likely token.\nTrained on sufficiently large datasets and with sufficiently well-tuned architectures and training processes, LLMs appear to\nproduce coherent text on just about any topic, including scientific ones. Moreover, we can’t help but make sense of their textual\noutput because our linguistic processing capabilities are instinctual and reflexive42.\nProponents argue that LLMs are useful in three domains: 1) navigating science, by searching and synthesizing published\nliterature, 2) doing science, in the sense of designing or conducting experiments or generating data, and 3) communicating\nscience, by drafting text for publication. While certain machine approaches may be useful in each, LLMs are unlikely to\noutperform alternative technologies. Furthermore, they have the potential to cause downstream harms to science if their use is\nwidely embraced.\nNavigating science\nNatural language processing has proven useful in sorting through an ever-growing body of scientific literature. Information\nretrieval and extraction techniques, as implemented in academic search engines (e.g. ref.43), have helped researchers discover\nrelevant prior work. Will LLMs supplant other NLP approaches? We doubt it. The inappropriateness of LLMs as text generators\nand synthesis machines was highlighted in Meta’s Galactica debacle. That system—taken off-line after three days in response\nto intense criticism for its abysmal performance—had been trained on scientific text and promoted as a tool to “summarize\nacademic papers, solve math problems, generate Wiki articles, write scientific code, annotate molecules and proteins, and\nmore.”20. But training an LLM on scientific papers doesn’t guarantee that it will output scientifically accurate information. As\nMeta discovered, the use of LLMs yields text ungrounded in any communicative intent or accountability for accuracy.\nOne might hope that LLMs could at least be used to summarize a set of papers. Extractive summarization systems44 already\ndo this; will LLMs perform better? Will people tend to over-rely on system output rather than using it as a starting point? What\nare the costs of false negatives, i.e., important points not included in the generated summary? How will errors generated by\nLLMs, which then become training data for future LLMs, get amplified?\nDoing science\nLLMs are just one of many technologies dubbed “artificial intelligence”, but their surprising capacity to perform what amounts\nto a fancy parlor trick has drawn outsized attention. That’s a mistake. LLMs may be adequate for specific linguistic tasks\nsuch as grammar checking, automatic transcription, and machine translation (including code generation), but they are unlikely\nto provide an effective basis for most tasks involved in hybrid human-machine science. Even where they do appear to be\nmoderately effective, they are known to be brittle to input variation 45. The future of machine-aided science will not be a\nmassive, one-size-fits-all, universal application of LLMs, but rather an ensemble of bespoke and often lightweight models that\n4/13\nhave been designed explicitly to solve the specific tasks at hand — and, crucially, evaluated in terms of those specific tasks.\nSuch approaches also have a major advantage where interpretability is concerned. If researchers want to understand output\nvariation, let alone find ways to fine-tune the architecture to generate better results, they need to steer away from technologies as\nopaque as LLMs. But instead, the ongoing hype around LLMs is drawing funding and brainpower away from more promising,\ntargeted approaches.\nNot only are LLMs being explored as aides to researchers; numerous proposals suggest that they can stand in for test\nsubjects46, survey participants47, or data annotators48. Such arguments derive from a failure to understand that LLMs model\nthe output of sequential linguistic tokens, not concepts, meanings, or communicative intent. If we are looking to study the\nopinions or behavior of human beings, we need to work with actual people.\nCommunicating science\nBy design LLMs generate form without substance. The synthetic text that systems output constitutes neither ideas, nor\ndata — and it certainly is not a reliable information source. This notion of generating statements that no one intended is\nanathema to the spirit of scientific inquiry. Automatically generating something that looks like a manuscript is very different\nfrom the iterative process of actually writing a manuscript. Yet the output can be difficult to distinguish, particularly in a\ncursory read or by inexpert readers. Some proponents argue that LLMs can relieve scientists of the drudgery of writing papers\nand free them up to get on with the serious business of “doing science”49. This false dichotomy between communication and\ninvestigation reflects a fundamental misunderstanding of the nature of science50 that devalues the communicative aspects of\nscience and ignores the role of writing in the process of formulating, organizing, and refining ideas.\nDownstream, LLMs threaten the notion of scientific expertise, shift incentive structures 51, and undermine trust in the\nliterature. When human authors review or acknowledge prior literature, we are assured that they are familiar with the field\nand striving to situate their results therein. If LLMs write our introductions, we lose these guarantees. Moreover, notions of\nsystematic review are undercut by the randomness inherent in LLM output. Finally, when a LLM generates a literature review,\nthe claims that it generates are not directly derived from the manuscripts it cites. Rather, the machine creates textual claims, and\nthen predicts the citations that might be associated with them. Obviously this practice violates all norms of scholarly citation.\nAt best, LLMs gesticulate towards the shoulders of giants.\nDriven by quantitative metrics and the strong incentive to publish, researchers may opt to trade off quality for speed by\nletting LLMs do much of their writing. Widespread use of one or a few LLMs could undercut epistemic diversity in science.\nWhen asked to provide a hypothesis, experiment, or mode of explanation, LLMs may repeatedly offer similar solutions, instead\nof leveraging the parallel creativity of an entire science community.\nWorse still, opportunistic or malicious actors could use LLMs to generate nonsense at scale with minimal cost. (This is\nnot an argument against using LLMs appropriately, but we need to be prepared for such behavior). Lazy authors could boost\ntheir publication counts by shotgunning machine-generated papers to low-quality journals. Predatory publishers could feign\npeer review using LLM output. Bad actors could overwhelm the manuscript submission system of a target journal (or even\na target field) with a massive volume of fake papers. Or an investigator’s work could be targeted with a deluge of spurious\nmachine-generated critiques on post-publication peer review platforms such as Pubpeer.\nFinally, LLMs may cause considerable collateral damage to science education. For example, as LLMs slash the cost\nof generating seemingly authoritative text, the web will be flooded with low-quality, mistake-ridden tutorials designed to\ncapture advertising revenue. At present, search engines’ ability to discriminate is more or less the only line of defense. That’s\nworrisome.\nConclusion\nIn conclusion, LLMs are often mis-characterized, misused, and over-hyped, yet they will certainly impact the way we do\nscience, from search to experimental design to writing. The norms that we establish now around their use will determine the\nconsequences far into the future. We should proceed with caution — and evaluate at every step.\nPerspective – LLMs in scientific practice: a matter of principles, not just regulations\nContributors: Marco Marelli, Adina Roskies, Balazs Aczel, Colin Allen, Dirk Wulff, Qiong Zhang, and Richard M. Shiffrin\nA moderate perspective on the potential impact of LLMs on scientific practice holds that, while it is important to be mindful\nof the dangers, their application seems largely beneficial, insofar they offer a much needed support in day-to-day research\nactivity and may alleviate major obstacles to scientific advancement. This is evident when LLMs are applied as editing tools:\nthey provide a writing aid that leaves researchers with more time for brainstorming ideas and analysis, may help mitigate\ndisparities between different scientific communities, and remedy some of the disadvantages for researchers who are not native\nspeakers of English52. Also, LLMs can access a broader range of literature than any individual researcher could, potentially\n5/13\noffering valuable support for literature analysis and hypothesis generation 53, with a reach that goes beyond one’s research\nspecialization.\nHowever, although any new technology may be used for good or evil, some technologies afford opportunities for good or\nevil uses more than others. LLMs have disruptive potential that is ever more evident, and such disruption must be kept at bay if\nthe goal is to prevent “evil drifts”. One perspective might hold that strict regulation is required, but regulation carries with it\nmany costs that might be best avoided if it is kept moderate. A preferable approach may be to adopt clear principles guiding the\nway this technology should be used, principles that cannot just focus on efficiency and overall utility. Such principles include\ntransparency, accountability, and fairness.\nA matter of transparency\nIn science, transparency is of indispensable value. When used as writing tools, researchers must acknowledge the reliance on\nLLMs so that readers are on notice that the text is (at least partially) AI-generated. Authors should make explicit which LLMs\nwere applied and how, as part of the method sections or in a separate dedicated statement. This could be achieved by relying on\nalready existing solutions; for example, the CrediT taxonomy7 could also be used to code the nature of AI contribution, even if\nAI is not to be recognized as a coauthor. Ideally, in the spirit of open science, authors shall publicly release their prompts along\nwith the corresponding LLM responses as supplementary materials, and reference such archives in the manuscript. Importantly,\ntransparency does not only pertain to the way we exploit LLMs, but to the systems themselves. LLMs are not, strictly speaking,\nanything new. Models that are analogous to current LLMs in structure, spirit, and basic mechanisms have been part of the\nscientific debate for decades54. However, such older models were unambiguous about their architecture and training, if not\nopenly released. Current LLMs are often not held to the same scientific standards as their ancestors, being widely applied\neven when their inner workings and training data remain undisclosed. This causes substantial issues in estimating the actual\nperformance of such models (and, importantly, the possibility of data contamination55). As a scientific community valuing\ngreater transparency, we should favour the systems that are taking some steps in that direction56.\nA matter of accountability\nIt must be acknowledged that LLMs are instruments of human agency, and researchers should be held accountable for any\nscientific product they present to the community, irrespective of the extent to which this was obtained through the application of\nautomatic systems. The Association for the Advancement in Artificial Intelligence has released clear guidelines in this respect:\n“Attribution of authorship carries with it accountability for the work, which cannot be effectively applied to AI systems . . .\nUltimately, all authors are responsible for the entire content of their papers, including text, figures, references, and appendices.”\nFor example, LLMs are known to “hallucinate” and produce factually incorrect responses57. They can fabricate bibliographic\ncitations, omit important references when summarizing literature, and potentially plagiarize text written by another researcher.\nThe burden to verify that LLM-produced texts are accurate and that LLM-proofread texts are consistent with the original\nmessage remains with the individual authors. Similarly, LLMs can be particularly poor at logic and deductive reasoning58, 59,\nso using them for analysis may lead to false conclusions. The onus is on the user to make sure that what LLMs produce is\nworth pursuing. Researchers must hence have strategies for assessment over AI-related content; a good practice would be to\nhave clear quality criteria and verification methods defined before using LLMs. Scientists should not underestimate the time\nand effort that such vetting will take, and should weigh the efficiency of LLM application against these costs.\nA matter of fairness\nAI in general and LLMs in particular have the potential to deeply affect us at a societal level. Science, as any human endeavour,\nis not immune to this. As a community we must make all possible efforts to guarantee that reliance on LMMs does not violate\nbasic fairness principles. Indeed, current language models reflect mostly WEIRD (Western Educated Industrialized Rich\nDemocratic) populations and cannot easily be prompted to represent non-WEIRD communities60, 61. This leads to biases in\nwriting and annotation, potentially reinforcing distortions in citations and marginalization of already marginalized scientists.\nMoreover, it may have negative consequences in terms of equitable research, given that LLMs are also more accessible to\nWEIRD populations. More generally, LLMs will, for known or unknown reasons, favour some perspectives or sources over\nothers62. These systematic patterns must be recognized and taken into account, to avoid unprincipled biases affecting the\ndirection of research and possibly the relative success of careers.\nConclusion\nThe impact that LLMs are having on scientific practice cannot be understated. Given the current trend, at the time you are\nreading these words such impact will likely be much larger than it is as we write this piece. Precisely how LLMs will influence\nthe practice of science in the future cannot be entirely predicted and countering such a revolution with strict, preconceived\n7https://credit.niso.org/\n6/13\nnorms is a losing battle. Rather, establishing principles and shared values in the scientific community constitutes the ideal\nfoundation when deciding how to manage these rapidly changing technologies. Most importantly, we need to train students and\neach other to build upon such principles in order to become appropriately skeptical towards these systems and their outputs.\nPerspective – AI can help, but science is for people\nContributors: Matthew M. Botvinick and Samuel J. Gershman\nLike many forms of technology, AI can substitute for human labor. With the advent of LLMs, the relevant kinds of labor\nbegin to overlap with high-level human cognitive work, including the activities involved in science15. As LLMs improve, their\nability to substitute for human scientific labor will be a major boon. However, we argue here that two core aspects of scientific\nwork should be reserved to human scientists.\nAI and scientific labor\nOver time, the labor involved in scientific research has become progressively more onerous, sometimes now bordering on\nthe intractable. Assimilating current knowledge has become more difficult in the face of increasingly voluminous literatures.\nGenerating new questions, hypotheses and experimental tests has become more challenging, as the search problem entailed by\neach has become more complex. Drawing conclusions from experimental results has become harder as the size and complexity\nof datasets has exploded. And communicating and debating scientific conclusions has become more challenging for reasons\nincluding an overtaxing of peer review systems63. Given the increasing costs of scientific labor on these fronts, it’s no surprise\nthat progress across multiple scientific fields appears to have slowed64.\nIn the long run, AI may help us cope with the increasing demands of scientific work. Through the kinds of application\ndetailed in the introductory essay above, AI may help us scale up, by making each step in the research cycle cheaper. In some\ncases, AI may eventually perform some forms of scientific labor better than human scientists, including the work of generating\nnew hypotheses65. Even in present-day forms, AI may be useful on some fronts, as reviewed in the introduction. Of course, as\nwidely discussed, current systems are too unreliable to deploy without caution and oversight (see accompanying commentaries),\nand only time will tell how feasible it may be to overcome current limitations.\nHowever, in addition to addressing present-day shortcomings, it’s equally important to look into the future and consider\nwhat kind of AI tools we actually want for science in the long run. Given that AI can be applied to all phases of scientific work,\none aim might be to build a full-fledged AI scientist, one that can do everything a human scientist now does: a full-spectrum\nreplacement for human scientists. To us, this prospect is deeply unappealing. Why? Because there are particular aspects of\nscience that we simply would not want to delegate to AI, even in a scenario where technical limitations presented no barrier. In\nparticular, there are two core aspects of science that should be left to people. As we now explain, one of these is normative and\nthe other epistemic.\nThe normative aspect of science\nAny scientific discipline must continually ask, What problems shall we work on? How this question gets answered, both\nwithin individual labs and across whole research communities, is a complex affair, but it centers on judgments concerning the\n‘interest’ and ‘significance’ of candidate problems, as well as their ‘timeliness,’ including their amenability to study under\nprevailing material and ethical constraints. Such judgments are informed by hard data; we obviously cannot reduce them to\npurely social constructions. However, at the same time, judgments of interestingness, significance and timeliness are inherently\ntied to culturally and historically grounded sensibilities and mores. This is not a corruption or impurity in scientific thought and\nprocedure. Cultural sensibilities and patterns of thought are fundamental to scientific prioritization.\nThis point will be especially salient to students of the history of science, because the sensibilities and mores that inform\nscience evolve over time. Just as scientific theory changes over the years, so do the ethical commitments and intellectual\npriorities that underlie science. This is evident in the fact that we no longer approach homosexuality as a disorder, or study\ngenetics through the lens of eugenics. It shows in growing restrictions on animal experimentation. And it shows in the attention\nthat Western climatologists now pay to regions historically neglected.\nWe argue that the normative aspect of science should not be ceded to AI systems, no matter how capable those systems\nbecome. People should stay in the driver’s seat, determining the direction of travel for science. Certainly, AI systems may be\nhelpful partners in deliberation, especially as techniques for AI value alignment improve66. However, aligning a system to\ncurrently prevailing human views is different from allowing that system to govern the evolution of human views. In science, the\nultimate driving force in that evolution should remain human. We are the moral agents in the room, and we shouldn’t forget it.\n7/13\nThe epistemic aspect of science\nObviously, a central goal of basic science is understanding the natural world. If we are going to do science with AI tools, the\nquestion arises: ‘whose’ understanding matters? Would it be satisfactory to have AI systems that in some sense understand\naspects of nature, but which don’t make that understanding accessible to people? From an engineering standpoint that might be\nfine. However, if it’s basic science we’re talking about, we shouldn’t let go of the core objective, which is not just practical but\nepistemic. We cannot cede understanding to artificial systems. We should insist on human understanding remaining a core goal\nof science.\nOf course, it may be that because of limitations on human cognition, AI systems may someday be able to represent some\naspects of nature that we cannot, just as existing AI systems master aspects of complex board games that elude even highly\nskilled human players67. Even in these cases, however, we should strive to extract as much human insight from AI systems as\npossible68. We shouldn’t lose track of what basic science is for.\nConclusion\nAI promises to deliver great value in science, just as in many other domains. We believe its potential should be embraced.\nHowever, at the same time that we strive to break through the current limitations of AI to access its benefits, we should also\nthink through our long-term goals in developing this technology. In the end, the two areas of science we’ve proposed to protect\n— one normative, the other epistemic — are two reflections of a more general bound on AI’s proper domain. We might call\nthis the subjective limit. Unlike AI systems, people have a ‘point of view,’ which cannot be automated because it’s inherently\nsubjective69. This point of view includes knowledge that is meaningful to us (the epistemic view) and values that are meaningful\nto us (the normative view). Machines might have their own knowledge or values, and these might be aligned with ours, but the\nalignment problem is fundamentally yoked to our subjective views. This principle applies in science, as in all human-centered\nactivities.\nResponse by Eric Schulz, Daniel Schad, Marcel Binz, Stephan Alaniz, Ven Popov, and Zeynep\nAkata\nWe have argued that one should think of working with LLMs less as using a traditional software tool and more as working\nwith a human collaborator and that this perspective allows us to better understand their shortcomings. This view actually\nresonates with many of the points raised in the other perspectives. For example, Marelli et al. write that “we should not blindly\ntrust or rely on LLMs, but rather use them as a complement to our own expertise and judgment”, and Bender et al. argue that\ncollaboration in science means iterating over outputs many times. Like working with a human collaborator, working with\nLLMs is an iterative process in which we constantly check for facts and logical consistency, revise arguments, and identify new\nconnections. This process takes time and is more than just booting up an LLM and copy-pasting its outputs; as nicely put by\nMarelli et al., we “should not underestimate the time and effort that such vetting will take, and should weigh the efficiency of\nLLM application against these costs.”\nWe would like to stress that the notion that “LLMs are simply models of word form distributions extracted from text”\noversimplifies both their capabilities and the additional engineering effort involved in modern LLMs. If one takes steps like\nreinforcement learning from human feedback70 or instruction tuning71 out of the equation, the outputs produced by such models\nare rather uninspiring (anyone who has ever worked with a plain LLM can attest to this). However, with those ingredients,\nLLMs do not just mimic language patterns; they can also synthesize concepts, critically evaluate their own outputs, and assist in\nproblem-solving by processing vast amounts of data.\nBender et al. argue that “the future of machine-aided science will not be a massive, one-size-fits-all, universal application of\nLLMs, but rather an ensemble of bespoke and often lightweight models that have been designed explicitly to solve the specific\ntasks at hand [...].” We believe LLMs are widely adopted precisely because they are a universal tool to accomplish many tasks.\nNot only does that remove the need to build specialized tools for each application, but it also eradicates the time it takes to learn\nthem. Like human collaborators, who bring a diverse range of skills to a project, LLMs offer a breadth of knowledge that can\nbe tailored to specific needs, e.g., as shown with the finetuning of coding LLMs27. There are – of course — applications that\nbenefit from purposefully designed tools, but we believe that the percentage of such applications is modest once we take the\ntime required to develop and learn such tools into account.\nFinally, there is the question of how much autonomy we want to transfer to LLMs or other AI systems. Botvinick and\nGershman advocated that people should retain control over certain aspects of the scientific pipeline, such as deciding which\ntopics to work on. We do not think that such a constraint is necessary. For example, if in the future, an LLM (or any other AI\nsystem) decides to work on a topic that it deems interesting, and this LLM has proven itself to select topics in a very fruitful\nand productive manner, should we stop it? We do not think so as long as ethical and legal guidelines are followed. Deciding on\nscientific topics is hard, and it is often not a priori known which research directions will be fruitful. Therefore, we should take\n8/13\nany help we can get. Human researchers and AI systems bring complementary strength to the table, and acknowledging this\ncollaborative spirit enables us to leverage the best out of both worlds.\nResponse by Emily M. Bender, Carl T. Bergstrom, and Jevin D. West\nScience is a social process. It cannot be auto-completed. Its agents — real scientists — are as much the product of this process\nas the results recorded in papers.\nLLM optimists envision a new world, where machines write, review, and even do much of the science. Even the less\nextreme narrative wherein LLMs simply aid researchers suffers from a misplaced and almost Taylorist72 optimism regarding\nproduction efficiency. Science is not a factory, churning out widgets or statistical analyses wrapped in text. For a factory,\nproducing one more car per day is progress. For science, the goals are to understand our world — not to produce more artifacts\nthat look like scientific papers. If science were a paper factory, we too would indulge in LLM euphoria and might even claim a\nsignificant resulting improvement in quality coming out of our labs. But we cannot equate papers and progress. Papers are but\nmessages that we send one another to coordinate our collective quest for scientific understanding.\nWe don’t, however, believe that any new mandates are required prohibiting the use of LLMs. All ill-advised use cases are\nalready contrary to the norms of science: Using LLMs as stand-ins for human subjects or annotators amounts to fabricating\ndata; using LLMs to write first drafts runs afoul of prohibitions against plagiarism, as it is impossible to discern the source of\nany string produced by an LLM; treating LLMs as co-authors contravenes norms around authorship, since LLMs are not the\nsort of thing that can be accountable for paper contents; using LLMs to produce peer reviews is tantamount to abrogating our\nresponsibility to deeply evaluate the methods, reasoning and conclusions of our peers’ work.\nWhen contemplating how LLMs will affect science, we should not underestimate the temptation to use them under deadline\npressure or in response to publish-or-perish threats to job security. Nor should we underestimate the time needed to fact-check\nall LLM output—not only for the inevitable and frequent errors but also to assess whether citations are accurate. We note that\nthere are no published user studies that quantify just how much effort this checking process is, nor how accurately researchers\ncan carry it out, especially while working under pressure. Norms of plagiarism and the weight of reputation will hopefully\ncounterbalance the unfettered use of this new technology.\nTo reason appropriately about when LLMs are suitable within science, it is critical to avoid anthropomorphizing them.\nThese models aren’t research assistants. They are tools. They don’t make mistakes like junior (or senior!) researchers do: People\ncan take responsibility for, and learn from, mistakes. Tools produce errors; thus people using the tools have a responsibility to\nunderstand their affordances and use them with care.\nSimilarly, understanding LLMs as tools positions us to ask: Is this the best tool for this task? Often, we expect, LLMs are\nnot. Even setting aside the closed proprietary models, their attendant failures of transparency, and the stochastic nature of LLM\noutput, we expect that bespoke models designed for specific tasks will be more efficient, performant, interpretable, and easier to\nfix when not functioning well.\nUltimately, science is a conversation and the interlocutors are the scientists. Synthetic text-extruding machines, designed\nonly to produce plausible-sounding prose, are not fit participants in that conversation and should not be treated as such.\nResponse by Marco Marelli, Adina Roskies, Balazs Aczel, Colin Allen, Dirk Wulff, Qiong Zhang,\nand Richard M. Shiffrin\nIn our proposal concerning the application of LLMs in science, we aimed for a moderate perspective. In that spirit, we think\nthat such systems can be profitably incorporated into scientific practice (in line with Schulz et al.), but we also recognize that\nthere are causes for reservation (in line with Bender et al.).\nWe disagree with the view that LLMs should be considered collaborators or research assistants (Schulz et al.). One can\ninstruct students or research assistants, correct their mistakes, and anticipate that they will learn from them. One may also\nquestion their reasons or their reasoning and get answers and expect accountability. Finally, one may also get insight into their\nvalues and their motivations, and trust or distrust them accordingly. LLMs are not introspective, lack metacognition, and have\nno values, at least not in the way humans do. Indeed, our inability to understand why they make the errors they do or when they\nwill make them impairs our ability to understand their limits, especially on the edges of knowledge, where their training corpus\nis arguably less robust. Moreover, although LLMs move from the same foundations of previous language models (Schulz et al.),\nthey are significantly more opaque and complex. As a result, maintaining the ever-important scientific value of transparency\ncan be challenging and necessitates further development of practices and strategies to ensure its preservation.\nNevertheless, we disagree that such concerns should prevent scientific applications of LLMs. It is unrealistic to presume\nthat LLMs won’t be used because of the risks involved, and banning them could do more harm than good: given the current\ntrend, if prohibited, they would likely be used covertly, exacerbating the already-worrying transparency issues. Certainly, we\nneed to pursue a critical and not starry-eyed understanding of LLMs and maintain a clear-eyed assessment of the potential\n9/13\nrisks of use. However, there are ways of employing them that can improve the quality of science, as long as the researcher is\nkept at the center of the process. LLMs are tools and, as such, must be carefully evaluated in their applications. This applies\nto any tool, including the existing alternatives discussed by Bender et al., which, although optimized for specific scientific\npurposes, are not immune from mistakes and whose degree of reliability always needs careful scrutiny. At the end of the day,\nthe responsibility falls upon the shoulders of the researchers who use the tools. It is, hence, crucial to establish principles and\nvalues that guide our decisions—whether one applies LLMs or any other method.\nUltimately, we mostly concur with Botvinick and Gershman: the impact of LLMs on the future practice of science cannot\nbe fully predicted, but science is a humanistic and human enterprise and must remain so, motivating curbs to LLM use. Our\nperspective highlighted the normative aspects in terms of core values that should guide their use today, while Botvinick and\nGershman seeks to identify the principles and values for the future, deciding what should remain exclusively human even when\nAI becomes fully capable of performing every step of scientific inquiry as well as upholding values such as accountability,\ntransparency, and fairness. The two perspectives complement each other in stimulating discussions about what should guide the\nway we integrate AI into our scientific practices.\nResponse by Matthew M. Botvinick and Samuel J. Gershman\nWe see significant common ground across the other perspectives. We will focus here on one issue that gets to the heart of our\nperspective. Schulz et al. characterize LLMs as closer to collaborators than to tools. This raises critical issues of accountability,\nas pointed out by Bender et al. and Marelli et al. Some of these issues are currently being grappled with, while others will\nbecome more salient in the future as the technology advances. In particular, accountability is a fundamentally human concept:\nhumans are the only currently existing agents that are accountable in the sense that they have ultimate control over their own\nactions and voluntarily submit to a system that regulates these actions. Extending this concept to artificial agents would entail a\nprofound shift in our attitudes, essentially requiring us to acknowledge the personhood of such agents.\nThis shift, if it ever happens, will have ramifications far beyond science. Policymakers are already starting to wrestle with\nthe question of how accountability should operate in a world where AI systems are increasingly autonomous, and the issues\ncan get quite complex. The difficulties can be bounded, however, in domains where humans are able to draw clear boundaries\naround what role they will permit AI systems to play. In science, we believe these boundaries should be firm and restrictive,\nlimiting key decisions — and thus accountability — to human scientists.\nUltimately, we are interested in the limit case where the limits imposed on AI are sociological, moral, and juristic, rather\nthan technological. To regard LLMs as genuine collaborators rather than sophisticated tools, we would need to acknowledge\nattributes of personhood that go far beyond the mere practice of science. Our view is that AI, no matter how intelligent, should\nremain a tool, because ceding personhood to artificial agents would have undesirable consequences. It’s one thing for an AI\nscientist to tell us that there is a better way to fold proteins or design nuclear reactors, but it’s quite another thing for it to tell us\nthat it would rather be studying some other problem. It would also be quite a shock to be told by an AI scientist that it’s solved\nan important problem but that it doesn’t feel like trying to explain it to a human. As we argued in our perspective, the choices\nof what to study and which explanations count are irreducibly human.\nConclusion\nWe have presented four different perspectives centering around the question “how should the advent of LLMs affect the practice\nof science?” Schulz et al. argued that “working with LLMs will not be fundamentally different from working with other\ncollaborators, such as research assistants or doctoral students.” Bender et al. described a suite of problems with using LLMs\nin scientific activity and argued that many uses of LLMs are “contrary to the norms of science.” Marelli et al. called for\n“clear principles guiding the way this technology should be used,” including transparency, accountability, and fairness. Finally,\nBotvinick and Gershman advocated that “two core aspects of scientific work should be reserved to human scientists”, namely\ndeciding on what problems to work on and that human understanding remains the goal of science.\nYet, even though there was substantial disagreement, there were also important common themes. In particular, all parties\nemphasized the social nature of science and the importance of protecting scientific integrity and standards. In modern times,\nthese core values are more important than ever before, and we — as a community — will have to continuously reevaluate how\nto protect them.\nReferences\n1. Bengio, Y ., Ducharme, R. & Vincent, P. A neural probabilistic language model. Adv. neural information processing\nsystems 13 (2000).\n2. Jurafsky, D. & Martin, J. H. Speech and language processing : an introduction to natural language processing, computa-\ntional linguistics, and speech recognition (Pearson Prentice Hall, 2009).\n10/13\n3. Brown, T. et al. Language models are few-shot learners. Adv. neural information processing systems 33, 1877–1901\n(2020).\n4. Drori, I. et al. A neural network solves, explains, and generates university math problems by program synthesis and\nfew-shot learning at human level. Proc. Natl. Acad. Sci. 119, e2123433119 (2022).\n5. Kocmi, T. & Federmann, C. Large language models are state-of-the-art evaluators of translation quality. In Nurminen, M.\net al. (eds.) Proceedings of the 24th Annual Conference of the European Association for Machine Translation, 193–203\n(European Association for Machine Translation, Tampere, Finland, 2023).\n6. Katz, D. M., Bommarito, M. J., Gao, S. & Arredondo, P. Gpt-4 passes the bar exam. Available at SSRN 4389233 (2023).\n7. Eloundou, T., Manning, S., Mishkin, P. & Rock, D. Gpts are gpts: An early look at the labor market impact potential of\nlarge language models. arXiv:2303.10130. Unpubl. preprint (2023).\n8. Kasneci, E. et al. Chatgpt for good? on opportunities and challenges of large language models for education. Learn.\nIndivid. Differ. 103, 102274 (2023).\n9. Peres, R., Schreier, M., Schweidel, D. & Sorescu, A. On chatgpt and beyond: How generative artificial intelligence may\naffect research, teaching, and practice. Int. J. Res. Mark. (2023).\n10. Lund, B. D. & Wang, T. Chatting about chatgpt: how may ai and gpt impact academia and libraries? Libr. Hi Tech News\n40, 26–29 (2023).\n11. Hill-Yardin, E. L., Hutchinson, M. R., Laycock, R. & Spencer, S. J. A chat (gpt) about the future of scientific publishing.\nBrain Behav Immun 110, 152–154 (2023).\n12. Zheng, H. & Zhan, H. Chatgpt in scientific writing: a cautionary tale. The Am. J. Medicine (2023).\n13. Lund, B. D. et al. Chatgpt and a new academic reality: Artificial intelligence-written research papers and the ethics of the\nlarge language models in scholarly publishing. J. Assoc. for Inf. Sci. Technol. 74, 570–581 (2023).\n14. Transformer, G. G. P., Thunström, A. O. & Steingrimsson, S. Can gpt-3 write an academic paper on itself, with minimal\nhuman input? Unpublished (2022).\n15. Birhane, A., Kasirzadeh, A., Leslie, D. & Wachter, S. Science in the age of large language models. Nat. Rev. Phys. 1–4\n(2023).\n16. Fecher, B., Hebing, M., Laufer, M., Pohle, J. & Sofsky, F. Friend or foe? exploring the implications of large language\nmodels on the science system. arXiv:2306.09928. Unpubl. preprint (2023).\n17. Stokel-Walker, C. & Van Noorden, R. What chatgpt and generative ai mean for science, DOI: 10.1038/d41586-023-00340-6\n(2023).\n18. Taylor, R. et al. Galactica: A large language model for science. arXiv:2211.09085. Unpubl. preprint (2022).\n19. Embracing change and resetting expectations. https://unlocked.microsoft.com/ai-anthology/terence-tao/. Accessed:\n2023-09-04.\n20. Heaven, W. D. Why Meta’s latest large language model survived only three days online (2022).\n21. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the dangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 610–623 (2021).\n22. Arkoudas, K. Gpt-4 can’t reason. arXiv:2308.03762. Unpubl. preprint (2023).\n23. Gilardi, F., Alizadeh, M. & Kubli, M. Chatgpt outperforms crowd workers for text-annotation tasks. Proc. Natl. Acad. Sci.\n120, e2305016120, DOI: 10.1073/pnas.2305016120 (2023). https://www.pnas.org/doi/pdf/10.1073/pnas.2305016120.\n24. Wulff, D. U. & Mata, R. Automated jingle–jangle detection: Using embeddings to tackle taxonomic incommensurability.\nPsyArXiv DOI: https://doi.org/10.31234/osf.io/9h7aw (2023).\n25. Dillion, D., Tandon, N., Gu, Y . & Gray, K. Can ai language models replace human participants?Trends Cogn. Sci. (2023).\n26. Hutson, M. Guinea pigbots. Sci. (New York, N.Y.)381, 121–123, DOI: 10.1126/science.adj6791 (2023).\n27. Rozière, B. et al. Code llama: Open foundation models for code. arXiv:2308.12950. Unpubl. preprint (2023).\n28. Sanmarchi, F. et al. A step-by-step researcher’s guide to the use of an ai-based transformer in epidemiology: an exploratory\nanalysis of chatgpt using the strobe checklist for observational studies. J. Public Heal. 1–36 (2023).\n29. Dehouche, N. Plagiarism in the age of massive generative pre-trained transformers (gpt-3). Ethics Sci. Environ. Polit.21,\n17–23 (2021).\n11/13\n30. Liang, P. P., Wu, C., Morency, L.-P. & Salakhutdinov, R. Towards understanding and mitigating social biases in language\nmodels. In International Conference on Machine Learning, 6565–6576 (PMLR, 2021).\n31. Coda-Forno, J. et al. Inducing anxiety in large language models increases exploration and bias. arXiv:2304.11111. Unpubl.\npreprint (2023).\n32. Hutchinson, B. et al. Social biases in NLP models as barriers for persons with disabilities. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics , 5491–5501, DOI: 10.18653/v1/2020.acl-main.487\n(Association for Computational Linguistics, 2020).\n33. Carlini, N. et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX\nSecurity 21), 2633–2650 (2021).\n34. King, M. R. A place for large language models in scientific publishing, apart from credited authorship. Cell. Mol. Bioeng.\n1–4 (2023).\n35. Herbold, S., Hautli-Janisz, A., Heuer, U., Kikteva, Z. & Trautsch, A. Ai, write an essay for me: A large-scale comparison\nof human-written versus chatgpt-generated essays. arXiv:2304.14276. Unpubl. preprint (2023).\n36. Poldrack, R. A., Lu, T. & Beguš, G. Ai-assisted coding: Experiments with gpt-4. arXiv:2304.13187. Unpubl. preprint\n(2023).\n37. Goyal, T., Li, J. J. & Durrett, G. News summarization and evaluation in the era of gpt-3. arXiv:2209.12356. Unpubl.\npreprint (2022).\n38. Towards a transparent ai future: The call for less regulatory hurdles on open-source ai in europe. https://laion.ai/blog/\ntransparent-ai/. Accessed: 2023-10-22.\n39. Liang, W. et al. Can large language models provide useful feedback on research papers? a large-scale empirical analysis.\narXiv:2310.01783. Unpubl. preprint (2023).\n40. Bender, E. M. & Koller, A. Climbing towards NLU: On meaning, form, and understanding in the age of data. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 5185–5198, DOI: 10.18653/v1/\n2020.acl-main.463 (Association for Computational Linguistics, Online, 2020).\n41. Wang, D., Wang, X. & Lv, S. An overview of end-to-end automatic speech recognition. Symmetry 11, DOI: 10.3390/\nsym11081018 (2019).\n42. Hartsuiker, R. J. & Moors, A. On the automaticity of language processing. In Schmid, H.-J. (ed.) Entrenchment and the\nPsychology of Language Learning: How We Reorganize and Adapt Linguistic Knowledge, DOI: https://doi-org.offcampus.\nlib.washington.edu/10.1037/15969-010 (American Psychological Association; De Gruyter Mouton, 2017).\n43. Kinney, R. M. et al. The semantic scholar open data platform. ArXiv. Unpubl. preprint abs/2301.10140 (2023).\n44. Narayan, S., Cohen, S. B. & Lapata, M. Ranking sentences for extractive summarization with reinforcement learning.\nIn Walker, M., Ji, H. & Stent, A. (eds.) Proceedings of the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 1747–1759, DOI:\n10.18653/v1/N18-1158 (Association for Computational Linguistics, New Orleans, Louisiana, 2018).\n45. Hodel, D. & West, J. Response: Emergent analogical reasoning in large language models (2023). 2308.16118.\n46. Törnberg, P., Valeeva, D., Uitermark, J. & Bail, C. Simulating social media using large language models to evaluate\nalternative news feed algorithms (2023). 2310.05984.\n47. Argyle, L. P.et al. Out of one, many: Using language models to simulate human samples. Polit. Analysis 31, 337–351,\nDOI: 10.1017/pan.2023.2 (2023).\n48. Gilardi, F., Alizadeh, M. & Kubli, M. Chatgpt outperforms crowd workers for text-annotation tasks. Proc. Natl. Acad. Sci.\n120, e2305016120, DOI: 10.1073/pnas.2305016120 (2023). https://www.pnas.org/doi/pdf/10.1073/pnas.2305016120.\n49. Conroy, G. How ChatGPT and other AI tools could disrupt scientific publishing. Nature 622, 234–236 (2023).\n50. Latour, B. & Woolgar, S. Laboratory life: The construction of scientific facts (Princeton university press, 2013).\n51. Partha, D. & David, P. A. Toward a new economics of science. Res. policy 23, 487–521 (1994).\n52. Amano, T. et al. The manifold costs of being a non-native english speaker in science. PLoS Biol. 21, e3002184 (2023).\n53. Jumper, J. et al. Highly accurate protein structure prediction with alphafold. Nature 596, 583–589 (2021).\n54. Günther, F., Rinaldi, L. & Marelli, M. Vector-space models of semantic representation from a cognitive perspective: A\ndiscussion of common misconceptions. Perspectives on Psychol. Sci. 14, 1006–1033 (2019).\n12/13\n55. Golchin, S. & Surdeanu, M. Time travel in llms: Tracing data contamination in large language models. arXiv:2308.08493.\nUnpubl. preprint (2023).\n56. Li, R. et al. Starcoder: may the source be with you! arXiv:2305.06161. Unpubl. preprint (2023).\n57. Walters, W. H. & Wilder, E. I. Fabrication and errors in the bibliographic citations generated by chatgpt. Sci. Reports 13,\n14045 (2023).\n58. Koco´n, J. et al. Chatgpt: Jack of all trades, master of none. Inf. Fusion 101861 (2023).\n59. Liu, H. et al. Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv:2304.03439. Unpubl. preprint (2023).\n60. Durmus, E. et al. Towards measuring the representation of subjective global opinions in language models.arXiv:2306.16388.\nUnpubl. preprint (2023).\n61. Atari, M., Xue, M. J., Park, P. S., Blasi, D. & Henrich, J. Which humans? PsyArXiv. Unpubl. preprint (2023).\n62. Santurkar, S. et al. Whose opinions do language models reflect? arXiv:2303.17548. Unpubl. preprint (2023).\n63. Flaherty, C. The peer review crisis. https://www.insidehighered.com/news/2022/06/13/\npeer-review-crisis-creates-problems-journals-and-scholars. Accessed: 2023-10-30.\n64. Park, M., Leahey, E. & Funk, R. J. Papers and patents are becoming less disruptive over time. Nature 613, 138–144 (2023).\n65. Davies, A. et al. Advancing mathematics by guiding human intuition with ai. Nature 600, 70–74 (2021).\n66. Gabriel, I. & Ghazavi, V . The Challenge of Value Alignment: From Fairer Algorithms to AI Safety. In The Oxford\nHandbook of Digital Ethics, DOI: 10.1093/oxfordhb/9780198857815.013.18 (Oxford University Press). https://academic.\noup.com/book/0/chapter/337809435/chapter-ag-pdf/50148600/book_37078_section_337809435.ag.pdf.\n67. Silver, D. et al. Mastering the game of go without human knowledge. nature 550, 354–359 (2017).\n68. Lemos, P., Jeffrey, N., Cranmer, M., Ho, S. & Battaglia, P. Rediscovering orbital mechanics with machine learning.Mach.\nLearn. Sci. Technol. 4, 045002 (2023).\n69. Botvinick, M. Have we lost our minds? https://medium.com/@matthew.botvinick/have-we-lost-our-minds-86d9125bd803.\nAccessed: 2023-10-30.\n70. Stiennon, N. et al. Learning to summarize with human feedback. Adv. Neural Inf. Process. Syst. 33, 3008–3021 (2020).\n71. Longpre, S. et al. The flan collection: Designing data and methods for effective instruction tuning. In Proceedings of the\n40th International Conference on Machine Learning, ICML’23 (JMLR.org, 2023).\n72. Taylor, F. W.The Principles of Scientific Management (Harper, 1913).\nAcknowledgements\nThis work has been partially funded by the ERC (853489 - DEXIM; 101087053 - BraveNewWord), by the DFG (2064/1 –\nProject number 390727645), the BMBF (Tübingen AI Center, FKZ: 01IS18039A) and as part of the Excellence Strategy of the\nGerman Federal and State Governments.\nAuthor contributions statement\nProject administration:Marcel Binz, Stephan Alaniz\nProject supervision:Zeynep Akata, Eric Schulz\nPerspective leaders:Emily M. Bender, Marco Marelli, Matthew M. Botvinick, Eric Schulz\nPerspectives/responses - original draft:Adina Roskies, Balazs Aczel, Carl T. Bergstrom, Emily M. Bender, Eric Schulz,\nJevin West, Marco Marelli, Matthew M. Botvinick, Qiong Zhang\nPerspectives/responses - review & editing:all authors\nIntroduction and conclusion - original draft:Marcel Binz, Stephan Alaniz\nIntroduction and conclusion - review & editing:Marcel Binz, Stephan Alaniz, Zeynep Akata, Eric Schulz\n13/13",
  "topic": "Argument (complex analysis)",
  "concepts": [
    {
      "name": "Argument (complex analysis)",
      "score": 0.6868245601654053
    },
    {
      "name": "Affect (linguistics)",
      "score": 0.6292483806610107
    },
    {
      "name": "Conversation",
      "score": 0.514011025428772
    },
    {
      "name": "Unintended consequences",
      "score": 0.43335095047950745
    },
    {
      "name": "Engineering ethics",
      "score": 0.4160863161087036
    },
    {
      "name": "Political science",
      "score": 0.40165114402770996
    },
    {
      "name": "Psychology",
      "score": 0.3826953172683716
    },
    {
      "name": "Epistemology",
      "score": 0.36164408922195435
    },
    {
      "name": "Public relations",
      "score": 0.34066832065582275
    },
    {
      "name": "Sociology",
      "score": 0.3268243074417114
    },
    {
      "name": "Engineering",
      "score": 0.15318289399147034
    },
    {
      "name": "Chemistry",
      "score": 0.11321640014648438
    },
    {
      "name": "Law",
      "score": 0.10189062356948853
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Communication",
      "score": 0.0
    }
  ]
}