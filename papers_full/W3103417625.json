{
  "title": "Modeling Content Importance for Summarization with Pre-trained Language Models",
  "url": "https://openalex.org/W3103417625",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2228826684",
      "name": "Liqiang Xiao",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2099626634",
      "name": "Lu Wang",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2109788442",
      "name": "Hao He",
      "affiliations": [
        "Optica",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2107094063",
      "name": "Yaohui Jin",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Optica"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963768805",
    "https://openalex.org/W2949807892",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W1819698735",
    "https://openalex.org/W2950342809",
    "https://openalex.org/W2120743786",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W3158986179",
    "https://openalex.org/W2082512208",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W100656083",
    "https://openalex.org/W3101913037",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2110693578",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W1525595230",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2952138241",
    "https://openalex.org/W2574535369",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W2970241613",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2993383518"
  ],
  "abstract": "Modeling content importance is an essential yet challenging task for summarization. Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance. It is thus hard for these methods to generalize to semantic units of longer text spans. In this work, we apply information theory on top of pre-trained language models and define the concept of importance from the perspective of information amount. It considers both the semantics and context when evaluating the importance of each semantic unit. With the help of pre-trained language models, it can easily generalize to different kinds of semantic units n-grams or sentences. Experiments on CNN/Daily Mail and New York Times datasets demonstrate that our method can better model the importance of content than prior work based on F1 and ROUGE scores.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3606–3611,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n3606\nModeling Content Importance for Summarization\nwith Pre-trained Language Models\nLiqiang Xiao1, Lu Wang2, Hao He1,3∗, Yaohui Jin1,3∗\n1MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai Jiao Tong University\n2Computer Science and Engineering, University of Michigan\n3State Key Lab of Advanced Optical Communication System and Network,\nShanghai Jiao Tong University\nxiaoliqiang@sjtu.edu.cn, wangluxy@umich.edu\n{hehao, jinyh}@sjtu.edu.cn\nAbstract\nModeling content importance is an essential\nyet challenging task for summarization. Previ-\nous work is mostly based on statistical meth-\nods that estimate word-level salience, which\ndoes not consider semantics and larger con-\ntext when quantifying importance. It is thus\nhard for these methods to generalize to seman-\ntic units of longer text spans. In this work,\nwe apply information theory on top of pre-\ntrained language models and deﬁne the con-\ncept of importance from the perspective of in-\nformation amount . It considers both the se-\nmantics and context when evaluating the im-\nportance of each semantic unit. With the help\nof pre-trained language models, it can eas-\nily generalize to different kinds of semantic\nunits ( n-grams or sentences). Experiments\non CNN/Daily Mail and New York Times\ndatasets demonstrate that our method can bet-\nter model the importance of content than prior\nwork based on F1 and ROUGE scores.\n1 Introduction and Related Work\nText summarization aims to compress long docu-\nment(s) into a concise summary while maintaining\nthe salient information. It often consists of two crit-\nical subtasks, important information identiﬁcation\nand natural language generation (for abstractive\nsummarization). With the advancements of large\npre-trained language models (PreTLMs) (Devlin\net al., 2019; Yang et al., 2019), state-of-the-art re-\nsults are achieved on both natural language under-\nstanding and generation. However, it is still unclear\nhow well these large models can estimate “content\nimportance” for a given document.\nPrevious studies for modeling importance are\neither empirical-based, which implicitly encode\nimportance during document summarization, or\ntheory-based, which often lacks support by empiri-\ncal experiments (Peyrard, 2019). Beneﬁting from\n∗Corresponding author\nthe large-scale summarization datasets (Nallapati\net al., 2016; Narayan et al., 2018), data-driven ap-\nproaches (Nallapati et al., 2017; Paulus et al., 2018;\nZhang et al., 2019) have made signiﬁcant progress.\nYet most of them conduct the information selec-\ntion implicitly while generating the summaries. It\nlacks theory support and is hard to be applied to\nlow-resource domains. In another line of work,\nstructure features (Zheng and Lapata, 2019), such\nas centrality, position, and title, are employed as\nproxies for importance. However, the information\ncaptured by these features can vary in texts of dif-\nferent genres.\nTo overcome this problem, theory-based meth-\nods (Louis, 2014; Peyrard, 2019; Lin et al., 2006)\naim to formalize the concept of importance, and\ndevelop general-purpose systems by modeling the\nbackground knowledge of readers. This is based\non the intuition that humans are good at identifying\nimportant content by using their own interpreta-\ntion of the world knowledge. Theoretical models\nusually rely on information theory (IT) (Shannon,\n1948). Louis (2014) uses Dirichlet distribution\nto represent the background knowledge and em-\nploys Bayesian surprise to ﬁnd novel information.\nPeyrard (2019) instead models the importance with\nentropy, assuming the important words should be\nfrequent in the given document but rare in the back-\nground.\nHowever, statistical method is only a rough eval-\nuation for informativity, which largely ignores the\neffect of semantic and context. In fact, the infor-\nmation amount of units is not only determined by\nfrequency, but also by its semantic meaning, con-\ntext, as well as reader’s background knowledge. In\naddition, bag-of-words approaches are difﬁcult to\ngeneralize beyond unigrams due to the sparsity of\nn-grams when n is large.\nIn this paper, we propose a novel and general-\npurpose approach to model content importance for\n3607\nsummarization. We employ information theory on\ntop of pre-trained language models, which are ex-\npected to better capture the information amount of\nsemantic units by leveraging their meanings and\ncontext. We argue that important content contains\ninformation that cannot be directly inferred from\ncontext and background knowledge . Large pre-\ntrained language models are suitable for our study\nsince they are trained from large-scaled datasets\nconsisting of diverse documents and thus contain-\ning a wide range of knowledge.\nWe conduct experiments on popular summa-\nrization benchmarks of CNN/Daily Mail and New\nYork Times corpora, where we show that our pro-\nposed method can outperform prior importance\nestimation models. We further demonstrate that\nour method can be adapted to model semantic units\nof different scales (n-grams and sentences).\n2 Methodology\nIn this section, we ﬁrst estimate the amount of\ninformation by using information theory with pre-\ntrained language models (§2.1 and §2.2), where we\nconsider both the context and semantic meaning\nof a given text unit. We then propose a formal\ndeﬁnition of importance for text summarization\nfrom a perspective of information amount (§2.3).\n2.1 Information Theory\nInformation theory (IT), as invented by Shannon\n(1948), has been used on words to quantify their\n“informativity”. Concretely, IT uses the frequency\nof semantic units xi to approximate the probability\nP(xi) and uses negative logarithm of frequency as\nthe measurement for information, which is called\nself-info1:\nI(xi) =−log2 P(xi) (1)\nIt approximates the information amount of a unit\n(e.g. word) in a given corpus.\nHowever, traditional IT suffers from the spar-\nsity problem of longer n-grams and also ignores\nsemantics and context. Advanced compression al-\ngorithms in IT (Hirschberg and Lelewer, 1992) at-\ntempt to model the context to better estimate the\ninformation amount. But due to the sparsity, they\ncan only count up to third-order statistics. Statisti-\ncal methods are nearly impossible to reliably calcu-\nlate the probability of xi conditioned on its context,\n1The unit of information is “bit\", with base of 2. In the\nrest of this paper, we omit base 2 for brevity.\n➕\n ➕\n ➕\n ➕\n ➕\nP(x3)\nP1 P2 P3 P4 P5\nX1 X2 [M] [M] X5\nP(x4)\nI(x3x4)\nTransformer\nP(x3)\nX1 X2 [M] [M] X5\nP(x4)\nI(x3x4)\nALMs\nMLMs\nPLMs\n/\n/\nP(x 3|·)\nX1 X2 [M] [M] X5\nI(x 3x4|·) \nALMs\nMLMs\nPLMs\n/\n/\nP(x 4|·)\nFigure 1: Information amount evaluation with language\nmodels. Here we take a subsequence x3x4 as example.\n[M] denotes mask and PLMs/MLMs/ALMs are three\ndifferent options for language models. I(x3x4|·) =\n−log[P(x3|·)P(x4|·)], where conditions for different\nmodels are omitted for brevity.\ne.g., P(xi|··· , xi−1, xi+1, ···), as the number of\ncombinations of the context can be explosive.\n2.2 Using Language Models in Information\nTheory\nWith the development of deep learning, neural lan-\nguage models can efﬁciently predict the probability\nof a speciﬁed unit, such as a word or a phrase,\ngiven its context, which makes it feasible to calcu-\nlate high-order approximation for the information\namount.\nWe thus propose to use neural language models\nto replace the statistical models for estimating the\ninformation amount of a given semantic unit. Lan-\nguage models can be categorized as follows, and\nwe present information estimation method for each\nas shown in Fig. 1.\nAuto-regressive Language Model(ALM) (Ben-\ngio et al., 2000) is the most commonly used prob-\nabilistic model to depict the distribution of lan-\nguage, which is usually referred as unidirection\nLM (UniLM). Given a sequence of tokens x0:T =\n[x0, x1, ··· , xT ], UniLMs use leftward content to\nestimate the conditional probability for each token:\nP(xt|x<t) = gUniLM(x<t), where gUniLM denotes\na neural network for language model and x<t rep-\nresents the sequence from x0 to xt−1. Then the\njoint probability of a subsequence is factorized as:\nP(xm:n|x<m) =\nn∏\nt=m\nP(xt|x<t) (2)\nAfter applying Eq. (1) to both sides of Eq. (2),\nwe can obtain the information amount of the subse-\n3608\nquence conditioned on its context as:\nI(xm:n|x<m) =\nn∑\nt=m\nI(xt|x<t) (3)\nMasked Language Model(MLM) is proposed by\nTaylor (1953) and combined with pre-training by\nDevlin et al. (2019) to encode bidirectional context.\nMLM masks a certain number of tokens from the\ninput sequence, then predicts these tokens based\non the unmasked ones. The conditional proba-\nbility of a masked token xt can be estimated as:\nP(xt|x̸=t) = gMLM(x̸=t), where ̸= t indicates that\nthe t-th token is masked. Information amount of a\ngiven subsequence of the input is calculated as:\nI(xm:n|x/∈[m:n]) =\nn∑\nt=m\nI(xt|x/∈[m,n]) (4)\nSince MLMs encode both leftward and rightward\ncontext, intuitively, it can better estimate the infor-\nmation of current tokens than UniLMs.\nPermutation Language Model (PLM) is pro-\nposed by (Yang et al., 2019) to combine ALMs\nand MLMs, by considering the dependency be-\ntween the masked tokens as well as overcoming the\nproblem caused by discrepancy of pre-training and\nﬁne-tuning in MLMs. It models the dependency of\nthe tokens by maximizing the expected likelihood\nof all possible permutations of factorization orders.\nThe probability prediction can be formalized as:\nP(zt|z<t) = gPLM(z<t) where z denotes a possi-\nble permutation sequence of input. Information of\na subsequence is estimated as:\nI(zm:n|z<m) =\nn∑\nt=m\nI(zt|z<t) (5)\n2.3 Modeling Importance with Pre-trained\nLanguage Model\nWe argue that important content should be hard to\nbe predicted based on background knowledge only;\nit should be also difﬁcult to be inferred from the\ncontext. Moreover, detecting important content is\nto ﬁnd the most informative part from the input. As\ndescribed in (Shann, 1989), the information amount\nis a quantiﬁcation of the uncertainty we have for\nthe semantic units. But the degree of uncertainty\nis relative to reader’sbackground knowledge. The\nless knowledge the reader has, the more uncertainty\nthe source shows.\nWe thus employ pre-trained language models,\nwhich contain a wide range of knowledge, to rep-\nresent background knowledge. If a semantic unit\nis frequently mentioned in the training corpus, it\nwill get high probability during inference and thus\nlow information amount . We further propose a\nnotion of importance as the information amount\nconditional on the background knowledge:\nImp(xi|X −xi, K) =−log PLMK (xi|X −xi) (6)\nwhere X −xi means the context excluding 2 the\nunit xi from input X and K denotes the knowledge\nencoded in the pre-trained model. In practice, when\ncalculating the importance of a semantic unit, we\nﬁrst exclude all its occurrences from the input doc-\nument, and let the PreTLMs predict the probability\nof each occurrence, based on which the information\namount is calculated. As the same unit may appear\nat multiple positions in the input, summation is\nused as the ﬁnal value of information amount.\nBased on our notion of importance, a summariza-\ntion model is to maximize the overall importance of\na subset x of the input X, with a length constraint,\nsuch as ∑\nxi∈x |xi|< lmax:\narg max\nx⊂X\nImp(x) =\n∑\nxi∈x\nImp(xi|X −xi, K) (7)\n3 Experimental Setups\nSemantic Units and Tasks. Our theory can be\ngeneralized for evaluating the importance for any\nscale of semantic units. To verify the effective-\nness of our theory, we instantiate the semantic unit\nwith three common forms: unigram, bigram and\nsentence. In this way, our method can also be\nregarded as a general unsupervised information ex-\ntraction system, serving as a keyphrase extraction\nor sentence-level extractive summarization model.\nAs our method exploits the existed PreTLMs and\nneeds no additional training, it has the potential\nof beneﬁting the low-resource languages and do-\nmains.\nIn unigram scenario, we simply instantiate se-\nmantic unit xi as a token wt and calculate its impor-\ntance with Imp(wt) = −log P(wt|w̸=t, K). For\nevaluation, top-k important ones are selected and\nF1 score is calculated by comparing against the ref-\nerence, where the value of k is set by grid search.\nImportance of bigrams, e.g., xi = wtwt+1, can\nbe represented as a joint probability of two tokens:\nImp(wtwt+1) = −log P(wtwt+1|wt/∈[t,t+1], K).\nSame as unigrams, F1 score is computed to evalu-\nate the accuracy.\nBy extending the formula of bigram importance\nto longer sequences, we get importance deﬁnition\n2MLMs hide targets by replacing them with special to-\nkens, PLMs use attention masks, and ALMs only see leftward\ncontext.\n3609\nCNN/DM NYT\nUNI . BI . SENTENCE UNI . BI . SENTENCE\nModels F 1 F1 R-1 R-2 R-L F 1 F1 R-1 R-2 R-L\nTF·IDF 17.08 12.25 - - - 22.65 11.65 - - -\nSTM 38.78 16.76 - - - 34.10 16.49 - - -\nBAYESIAN SR 37.72 23.04 27.50 8.19 25.19 23.95 19.47 25.12 8.89 22.54\nLEX RANK 12.04 11.43 33.96 11.79 30.17 18.70 13.97 27.32 11.93 23.75\nTEXT RANK - - 33.20 11.80 29.60 - - 33.20 13.10 29.00\nTEXT RANK +BERT - - 30.80 9.60 27.40 - - 29.70 9.00 25.30\nSUM BASIC - - 31.72 9.60 28.58 - - 23.16 7.18 20.06\nIMP + GPT-2 (ALM) 34.73 26.02 35.06 12.41 32.62 27.96 15.96 26.69 9.22 24.13\nIMP + BERT (MLM) 39.93 29.39 37.53 14.71 34.71 31.86 20.07 32.26 14.48 29.28\nIMP + DISTILL BERT (MLM) 38.59 28.29 34.25 11.75 31.68 32.84 19.75 29.16 12.23 26.53\nIMP + XLN ET (PLM) 33.90 25.44 37.04 13.50 34.01 30.01 18.89 29.24 11.82 26.40\nTable 1: Results of importance modeling. UNI ./BI. denote unigram and bigram. R-1/R-2/R-L are ROUGE-1,\nROUGE-2 and ROUGE-L respectively. Best results per metric are in bold. Among our models (bottom), I MP\nyields signiﬁcantly higher scores on all metrics except when using unigrams as semantic unit and with sentences\n(based on R-1) on NYT (Welch’st-test, p<0.05).\nfor a sentence as: Imp(si) = I(si|w/∈si, K) =\n−log P(si|w/∈si, K). For evaluation, we select a\nsubset of sentences with Eq. (7) and calculate the\nROUGE scores (Lin, 2004) against reference sum-\nmary. The length constraints for CNN/DM and\nNYT are set to 105 and 95 tokens respectively.\nDatasets. We evaluate our method on the test set\nof two popular summarization datasets: CNN/Daily\nMail (abbreviated as CNN/DM) (Nallapati et al.,\n2017) and New York Times (Sandhaus, 2008).\nFollowing See et al. (2017) 3, we use the non-\nanonymized version that does not replace the name\nentities, which is most commonly used in recent\nwork. We preprocess them as described in (Paulus\net al., 2018). For unigram experiments, we remove\nall the stop words and punctuation in the reference\nsummaries and treat the notional words as the pre-\ndicting targets. For bigram, we ﬁrst collect all the\nbigrams in source document and then discard the\nones containing stop words or punctuation. The\nrest bigrams are employed as the predicting targets.\nComparisons. We compare our method with two\ntypes of models: (1) the methods that estimate\nimportance for n-grams. We consider TF·IDF ,\na numerical statistic to reﬂect how important a\nterm is to a document, and STM (Peyrard, 2019),\na simple theoretic model for content importance\nbased on statistical information theory. (2) unsu-\npervised models for extractive summarization. We\nadopt centrality-based models LEXRANK (Erkan\n3https://github.com/JafferWilson/\nProcess-Data-of-CNN-DailyMail\nand Radev, 2004), TEXT RANK (Mihalcea and Ta-\nrau, 2004) and TEXT RANK +BERT (Zheng and\nLapata, 2019), a frequency-based model SUM-\nBASIC (Ani Nenkova, 2005), and BAYESIAN SR\n(Louis, 2014) which scores words or sentences with\nBayesian surprise.\n4 Results\nWe conduct extensive experiments with pre-trained\nmodels4 in all three types of language models,\nincluding ALM: GPT-2 (Radford et al., 2019);\nMLMs: BERT (Devlin et al., 2019), and DISTILL -\nBERT (Sanh et al., 2019); PLMs: XLN ET (Yang\net al., 2019).\nAs shown in Table 1, our method IMP consis-\ntently outperform prior models. Among compar-\nisons, we can see that theory-based methods, STM\nand BAYESIAN SR, achieve better results. This\nis because they have statistics estimated for back-\nground distribution, which helps ﬁlter out common\nwords. The signiﬁcant advantage of our method\nveriﬁes our hypothesis that pre-trained language\nmodels better characterize the background knowl-\nedge, which in turn more precisely calculate the\nimportance of each semantic unit. Moreover, our\nmethods have a more signiﬁcant improvement on\nbigram-level prediction than unigram-level. This\nis due to the fact that IMP-based models overcome\nthe sparsity issue, where they can evaluate the im-\nportance of a phrase by considering its semantic\nmeaning and context.\n4We use the implementations and parameters from\nhuggingface.co/transformers/index.html\n3610\nSurprisingly, our method can also generalize to\nsentence-level semantic units and serve as an un-\nsupervised extract-based model for summarization.\nOur models achieve signiﬁcantly higher ROUGE\nscores than previous work by average 2.02. This\nobservation inspires a potential future direction for\nsentence-level importance modeling based on back-\nground knowledge as well as context information.\nWe also compare the performance of PreTLMs\nin different categories. MLMs, including BERT\nand DISTILL BERT, have the best overall perfor-\nmance, since they are able to encode bidirectional\ncontext. PLM, i.e. XLNet, is slightly inferior to\nMLMs because the probabilities of the words are\nrelated to the order of their permutation, which may\nhurt importance estimation by our method.\n5 Future Work\nIn the future work, we would like to ﬁne-tune\nthe current language models on the target of\nmax P(xi|X −xi) to better align with the inter-\npretation of information theory. Currently, the\nPreTLMs mostly mask the text randomly, which\nstill differ from our current method’s objective.\nBackground knowledge also deserves further in-\nvestigation. The background knowledge of our\nmethods comes from the pre-training process of\nlanguage models, suggesting that the informa-\ntion distribution largely depends on the training\ndata. Meanwhile, most PreTLMs are trained with\nWikipedia or books, which may affect the deter-\nmination of content importance from text with dif-\nferent styles. So domain-speciﬁc knowledge, such\nas genres or topics, can be included in the future\nwork.\n6 Conclusion\nWe propose to use large pre-trained language mod-\nels to estimate the information amount of given text\nunits, by ﬁltering out the background knowledge\nas encoded in the large models. We show that the\nlarge pre-trained models can be used as unsuper-\nvised methods for content importance estimation,\nwhere signiﬁcant improvement over nontrivial base-\nlines is achieved on both keyphrase extraction and\nsentence-level extractive summarization tasks.\nAcknowledgement\nThis research is supported in part by National Key\nResearch and Development Program of China un-\nder Grant 2018YFC0830400. Lu Wang is sup-\nported in part by National Science Foundation\nthrough Grant IIS-1813341.\nReferences\nLucy Vanderwende Ani Nenkova. 2005. The impact of\nfrequency on summarization. Microsoft Research.\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent.\n2000. A neural probabilistic language model. In\nAdvances in Neural Information Processing Systems\n13, Papers from Neural Information Processing Sys-\ntems (NIPS) 2000, Denver, CO, USA , pages 932–\n938.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186.\nGünes Erkan and Dragomir R. Radev. 2004. Lexrank:\nGraph-based lexical centrality as salience in text\nsummarization. J. Artif. Intell. Res., 22:457–479.\nDaniel S. Hirschberg and Debra A. Lelewer. 1992.\nContext modeling for text compression.\nChin-Yew Lin. 2004. Rouge: A package for auto-\nmatic evaluation of summaries. Text Summarization\nBranches Out.\nChin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-\nYun Nie. 2006. An information-theoretic approach\nto automatic evaluation of summaries. In Hu-\nman Language Technology Conference of the North\nAmerican Chapter of the Association of Compu-\ntational Linguistics, Proceedings, June 4-9, 2006,\nNew York, New York, USA.\nAnnie Louis. 2014. A bayesian method to incorpo-\nrate background knowledge during automatic text\nsummarization. In Proceedings of the 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2014, June 22-27, 2014, Baltimore,\nMD, USA, Volume 2: Short Papers, pages 333–338.\nRada Mihalcea and Paul Tarau. 2004. Textrank: Bring-\ning order into text. In Proceedings of the 2004 Con-\nference on Empirical Methods in Natural Language\nProcessing , EMNLP 2004, A meeting of SIGDAT, a\nSpecial Interest Group of the ACL, held in conjunc-\ntion with ACL 2004, 25-26 July 2004, Barcelona,\nSpain, pages 404–411.\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.\nSummarunner: A recurrent neural network based se-\nquence model for extractive summarization of doc-\numents. In Proceedings of the Thirty-First AAAI\nConference on Artiﬁcial Intelligence, February 4-9,\n3611\n2017, San Francisco, California, USA, pages 3075–\n3081.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gulçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of The 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018, pages 1797–1807.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2018. A deep reinforced model for abstractive sum-\nmarization. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings.\nMaxime Peyrard. 2019. A simple theoretical model of\nimportance for summarization. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers , pages\n1059–1073.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nEvan Sandhaus. 2008. The new york times annotated\ncorpus. Linguistic Data Consortium, Philadelphia ,\n6(12):e26752.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter.CoRR,\nabs/1910.01108.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2017, Vancouver, Canada, July 30 -\nAugust 4, Volume 1: Long Papers, pages 1073–1083.\nPatrick Shann. 1989. The selection of a parsing strat-\negy for an on-line machine translation system in a\nsublanguage domain. a new practical comparison.\nIn Proceedings of the First International Workshop\non Parsing Technologies, pages 264–276, Pittsburgh,\nPennsylvania, USA. Carnegy Mellon University.\nC. E. Shannon. 1948. A mathematical theory of com-\nmunication. Bell System Technical Journal, 27.\nWilson L. Taylor. 1953. \"cloze procedure\": A new\ntool for measuring readability. Journalism Quar-\nterly, 30(4):415–433.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, 8-14 December 2019, Vancou-\nver, BC, Canada, pages 5754–5764.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019.\nHIBERT: document level pre-training of hierarchi-\ncal bidirectional transformers for document summa-\nrization. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 5059–5069.\nHao Zheng and Mirella Lapata. 2019. Sentence cen-\ntrality revisited for unsupervised summarization. In\nProceedings of the 57th Conference of the Associa-\ntion for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1:\nLong Papers, pages 6236–6247.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9067371487617493
    },
    {
      "name": "Computer science",
      "score": 0.8299938440322876
    },
    {
      "name": "Natural language processing",
      "score": 0.7441792488098145
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6208236813545227
    },
    {
      "name": "Salience (neuroscience)",
      "score": 0.6087341904640198
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5880292654037476
    },
    {
      "name": "Language model",
      "score": 0.5605354905128479
    },
    {
      "name": "Distributional semantics",
      "score": 0.5531783699989319
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5051494240760803
    },
    {
      "name": "Word (group theory)",
      "score": 0.4686529040336609
    },
    {
      "name": "Task (project management)",
      "score": 0.4568295478820801
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.41292935609817505
    },
    {
      "name": "Information retrieval",
      "score": 0.410273015499115
    },
    {
      "name": "Semantic similarity",
      "score": 0.3617321848869324
    },
    {
      "name": "Linguistics",
      "score": 0.13155341148376465
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I59499333",
      "name": "Optica",
      "country": "US"
    }
  ],
  "cited_by": 18
}