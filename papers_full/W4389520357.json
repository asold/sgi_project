{
  "title": "The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining",
  "url": "https://openalex.org/W4389520357",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4208538294",
      "name": "Ting-Rui Chiang",
      "affiliations": [
        "University of Southern California",
        "Southern California University for Professional Studies"
      ]
    },
    {
      "id": "https://openalex.org/A258659784",
      "name": "Dani Yogatama",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W3134307371",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2562979205",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W2809090039",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4285254489",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2998463332",
    "https://openalex.org/W4238893454",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3169890186",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3095992020",
    "https://openalex.org/W3124034626",
    "https://openalex.org/W4385573170",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4225948283",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4305038462",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4298202759",
    "https://openalex.org/W2971033911",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2125031621",
    "https://openalex.org/W3172424021",
    "https://openalex.org/W4224904014",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2048679005",
    "https://openalex.org/W2949128310",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W3155431862",
    "https://openalex.org/W2979401726",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W4283315108",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W2970295111",
    "https://openalex.org/W3166143997",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W3170097025",
    "https://openalex.org/W2951585248",
    "https://openalex.org/W4285263440",
    "https://openalex.org/W4287854873",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2964303116"
  ],
  "abstract": "We analyze the masked language modeling pretraining objective function from the perspective of the Distributional Hypothesis. We investigate whether the better sample efficiency and the better generalization capability of models pretrained with masked language modeling can be attributed to the semantic similarity encoded in the pretraining data’s distributional property. Via a synthetic dataset, our analysis suggests that distributional property indeed leads to the better sample efficiency of pretrained masked language models, but does not fully explain the generalization capability. We also conduct an analysis over two real-world datasets and demonstrate that the distributional property does not explain the generalization ability of pretrained natural language models either. Our results illustrate our limited understanding of model pretraining and provide future research directions.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10305–10321\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nThe Distributional Hypothesis Does Not Fully Explain the Benefits of\nMasked Language Model Pretraining\nTing-Rui Chiang and Dani Yogatama\nUniversity of Southern California\n{tingruic,yogatama}@usc.edu\nAbstract\nWe analyze the masked language modeling pre-\ntraining objective function from the perspective\nof the distributional hypothesis. We investi-\ngate whether better sample efficiency and the\nbetter generalization capability of models pre-\ntrained with masked language modeling can be\nattributed to the semantic similarity encoded\nin the pretraining data’s distributional property.\nVia a synthetic dataset, our analysis suggests\nthat distributional property indeed leads to the\nbetter sample efficiency of pretrained masked\nlanguage models, but does not fully explain the\ngeneralization capability. We also conduct anal-\nyses over two real-world datasets and demon-\nstrate that the distributional property does not\nexplain the generalization ability of pretrained\nnatural language models either. Our results\nillustrate our limited understanding of model\npretraining and provide future research direc-\ntions. 1\n1 Introduction\nDespite the rise of the prompting paradigm with the\nscaling breakthrough of very large language models\n(Brown et al., 2020), understanding the mechanism\nof model fine-tuning remains to be an important\nendeavor. Fine-tuning relatively small models such\nas BERT (Peters et al., 2018; Devlin et al., 2019;\nLiu et al., 2019b) has significant practical implica-\ntions since these models are computationally more\nefficient than large language models such as GPT-3\n(Brown et al., 2020). Distilling a large language\nmodel to a small model by fine-tuning is also a\npromising direction (Lang et al., 2022; Hinton et al.,\n2015). The development of large language models\nalso involves fine-tuning, e.g. doing reinforcement\nlearning from human feedback (Bai et al., 2022). In\nthis work, we seek to understand how pretraining\nbenefits downstream fine-tuning.\n1Scripts for the experiments in this paper are available at\nhttps://github.com/usc-tamagotchi/DH-MLM.\nPrevious analyses of pretrained model focus\non probing the pretrained representations (Tenney\net al., 2019b,a; Liu et al., 2019a; Hewitt and Man-\nning, 2019; Wu et al., 2020; Rogers et al., 2020;\nZhang et al., 2021; Immer et al., 2022). However,\nthese models are rarely used as a static feature\nextractor. Practitioners often fine-tune them with\ndownstream tasks.\nWe analyze whether mask language model pre-\ntraining allows models to leverage semantic knowl-\nedge derived from the distribution of words in the\npretraining data for downstream tasks. This re-\nsearch question follows the distributional hypoth-\nesis (Harris, 1954; Firth, 1957). It postulates that\nsemantically related words have a similar con-\ntext distribution, which we elaborate more rig-\norously as a distributional property of pretrain-\ning data in §2. Because it draws connections be-\ntween word semantics and data distribution, it has\nbeen widely accepted as an explanation for the\nefficacy of non-contextualized (type-based) word\nembeddings (Levy and Goldberg, 2014) such as\nWord2Vec (Mikolov et al., 2013) and GloVe (Pen-\nnington et al., 2014). Recently, Sinha et al. (2021)\nshows that word order does not greatly affect the\nefficacy of masked language model pretraining (De-\nvlin et al., 2019; Liu et al., 2019b), thus conjectur-\ning that the distributional hypothesis could be an\nexplanation. In this work, we continue studying\nthis conjecture by investigating whether pretrained\nmodels can utilize the semantic knowledge derived\nfrom the data distribution as suggested by the dis-\ntribution hypothesis.\nAmong different types of semantic knowledge\nthe distribution of the pretraining data may encode,\nwe focus on semantic equivalence, i.e., synonym,\nwhich Harris (1954) suggests to be encoded in the\ndata distribution. In §3, we theoretically show that\nprior knowledge about semantic equivalence alone\nis enough to lead to two desirable properties that\npretrained language models have: better sample\n10305\nefficiency and generalization capability (Tu et al.,\n2020; Hendrycks et al., 2020; Tänzer et al., 2022).\nIn §4, as our first step to understanding how\nmasked language modeling training objective ex-\nploits the distributional property of the pretraining\ndata, we set up a toy experiment. We construct a\nsynthetic pretraining corpus and a downstream task.\nIn §5, we show that the distributional property ex-\nplains the sample efficiency of pretrained masked\nlanguage models, but it does not always lead to bet-\nter generalization on its own; it still depends on the\ntype of distribution shifts. Our results also suggest\nthat better parameter initialization statistics (Wu\net al., 2022) do not explain the benefit of masked\nlanguage model pretraining either.\nIn §6, we conduct a similar experiment in\nthe real-world setting. We analyze a pretrained\nBERT model and two real-world tasks SST-\n2 (Socher et al., 2013; Wang et al., 2018) and\nMNLI (Williams et al., 2018). We find that the\nsemantic (in)equivalence the model learns from the\npretraining task is independent of how a fine-tuned\nmodel treats words as synonyms. This indicates\nthat pretrained models do not generalize better by\nmodeling the distributional property of the pretrain-\ning data either.\nTo summarize, our main findings include: i) We\nshow that semantic equivalence encoded in the dis-\ntributional property of the pretraining data makes\npretrained models more sample-efficient. ii) There\nis a type of generalization capability independent\nof the distributional property of the pretraining data.\niii) The distributional property of the pretraining\ndata does not explain pretrained models’ gener-\nalization capability in the real world. Therefore,\nwe conclude that the distributional hypothesis by\nHarris (1954) alone is not enough for a complete\nexplanation. Future work may study the interplay\nbetween other complex semantic relationships, data\ndistribution, and model pretraining to better under-\nstand pretrained models’ generalization behavior.\n2 Properties of Pretraining Data and\nDownstream Tasks\nSince we focus on semantic equivalence in lan-\nguage, we use the concept of “synsets” from Word-\nNet (Miller, 1998), i.e., the sets of synonyms. Here\nwe extend the concept of synsets to allow each\nsynset to contain multi-token elements, such as\nphrases or sentences. Elements in a synset are fea-\ntures and each of them is a sequence of tokens (or\na single token). Having these semantic equivalence\nsets has the following implications:\nIn the pretraining data: The distributional hy-\npothesis suggests that the pretraining data has a\ndistributional property: for any two features a,b in\nthe same synset and any n≥1, the distribution of\ntheir neighboring words satisfies\np(x1,x2,··· ,xn|a) ≈p(x1,x2,··· ,xn|b). (1)\nFor example, the phrase “is delicious” has the same\nmeaning as the phrase “taste good”. Therefore, if\nthe training data has this distributional property,\nthen we expect its distribution to satisfy\np(x|“is delicious”) ≈p(x|“tastes good”).\nIn a downstream task: Given an input x for a\ndownstream task, substituting a feature awith an-\nother feature bin the same synset does not change\nthe meaning of x because aand bare synonyms.\nTherefore, the substitution does not change the la-\nbel of x either:\nf∗(x) = f∗(Replace(x,a,b )), (2)\nwhere f∗ is the task’s labeling function that maps\nan input x to its label y.\n3 The Benefit of Modeling Semantic\nEquivalence\nIn this section, we show why having prior knowl-\nedge about the semantic equivalence sets (synsets)\nhelps downstream performance.\n3.1 Sample Efficiency\nUnderstanding which symbols are semantically re-\nlated makes a classification problem easier. From\nthe perspective of the model, the inputs are se-\nquences of symbols. Having prior knowledge about\nthe relationship between these symbols decreases\nthe number of training examples the model requires\nto learn the target labeling function.\nFor example, consider a task with four training\nexamples: ⟨“It is delicious”, True⟩, ⟨“It is awful”,\nFalse⟩, ⟨“It is tasty”, True ⟩, ⟨“It is bad”, False ⟩.\nIf the model does not know the semantic relation-\nship between the predictive features “awful”, “bad”,\n“tasty”, and “delicious”, then it is impossible for\nthe model to learn the underlying target labeling\nfunction from these four samples, because each of\nthese four words appears only once in the dataset.\n10306\nsample\nsample\n1. Choose\nPre-deﬁne two Markov chains as \nor\n2. Sample synsets\n3. Sample features\nsatisfying DH\nsingle-token mulit-token\nw/o vocab. sharing\nmulit-token\nw/ vocab. sharing\nnot satisfying DH\n4. Map features to tokens\n...\n...\nFigure 1: The generation process of the pretraining data\n(§4.1). The words in the orange color denote indepen-\ndent experimental variables.\nHowever, the knowledge that some\nsymbols satisfy Eq. 2 makes the task\neasier, e.g. f(“It is delicious”) =\nf(Replace(“It is delicious”,“delicious”,“tasty”).\nIn other words, it reduces the feature space. Based\non statistical learning theory (Valiant, 1984; Blum\nand Mitchell, 1998), this smaller feature space\nreduces the sample complexity of the model.\n3.2 Generalization Capability\nIf a pretrained model is able to preserve its knowl-\nedge about semantic equivalence among words af-\nter being fine-tuned, then it will help the model to\ngeneralize better. For example, consider a simpli-\nfied case with two training examples ⟨“It is deli-\ncious”, True⟩and ⟨“It is awful”, False⟩, as well as\na test example “It is tasty”. Generalizing to this\ntest example is possible only when the model un-\nderstands the semantic relationships between “deli-\ncious”, “awful” and “tasty”. That is, if a model has\nan inductive bias in Eq. 2, as long asx containing a\nis in the training set, the model will be able to gen-\neralize to a testing sample Replace(x,a,b ), where\nbis an unseen feature semantically equivalent to a.\n4 Synthetic Data Construction\n4.1 Pretraining Datasets\nFor pretraining, we construct a pseudo-language\nwith the properties described in §2. This language\nhas n synsets Σ = {σ1,σ2,··· ,σn}and each\nsynset contains two features:\nsi = {ai,bi}.\nThis provides us with two semantically isomorphic\nsets of features Φa = {ai}n\ni=1 and Φb = {bi}n\ni=1.\nEach feature analogizes to a word or a phrase in\nnatural languages, depending on whether each fea-\nture corresponds to one or multiple tokens. We\ndiscuss the multi-token setting in A.1.\nTo understand how masked language model pre-\ntraining would help models utilize the semantic\nisomorphism between Φa and Φb, we start with a\nsetup where the semantic isomorphism is encoded\nwith a simple distribution that language models can\nlearn easily. Specifically, we first randomly gener-\nate two Markov chains P(1)\nΣ ,P(2)\nΣ and use them to\ngenerate sentences (Figure 1):\n1. We randomly choose k from {1,2}, which\ndecides whether to use the first Markov chain\nP(1)\nΣ or the second one P(2)\nΣ .\n2. We then draw a sequence of synsets\ns1,s2,··· ,sl based on the distribution\ndefined by the chosen Markov chain\nP(k)\nΣ (s1,s2,··· ,sl).\n3. Then for i = 1,2,··· ,l, we draw a feature\nxi ∈si. At this step, we can control the dis-\ntributional property as an independent exper-\nimental variable. We can generate a dataset\nwith the distributional property in Eq. 1 by\ndrawing features fromsi = {ai,bi}uniformly\nat random. Or we can generate a dataset with-\nout the distributional property by always draw-\ning ai or bi when k= 1 or k= 2 respectively.\n4. Finally, we map each feature to a single token.\n(In A.1, we describe multi-token setups where\nwe map each feature to multiple tokens.)\n4.2 Downstream Task\nWe construct a synthetic downstream task aligned\nwith the semantic equivalence specified in Eq. 2.\nWe define a pattern-matching task where the label-\ning function that maps a sequence of features to\na label based on the underlying synsets sequence\nthat generates the feature sequence. If the sequence\nof synsets matches one of a predefined set of pat-\nterns, then we label the sequence as positive and\notherwise negative.\n10307\nWe define these patterns with regular expressions\nin this form:\nΣ∗ S1 Σ∗ S2 Σ∗ S3 Σ∗,\nwhere Σ = {σ1,σ2,··· ,σn}, S1,S2,S3 ⊂Σ are\n3 randomly selected sets of synsets and ∗is a\nKleene star. In other words, a sequence of synsets\nmatches the pattern if synsets in S1,S2,S3 appear\nin the sequence in order.\nWe generate four downstream datasets as fol-\nlows. First, we can choose to generate the sequence\nof synsets from P(1)\nΣ or P(2)\nΣ . We will use D1 and\nD2 to denote the samples generated from these two\ndistributions respectively. Second, for each synset\nsi, we can choose to always use features in Φa or\nΦb. We will use A and B to denote the samples\nconstituted with these two feature sets respectively.\nTherefore, there are 4 combinations of choices: A-\nD1, B-D1, A-D2, and B-D2. We will use these 4\ncombinations to test the generalization capability\nand the sample efficiency of the pretrained models.\n5 Synthetic Experiment Results\n5.1 Experiment Design\nWe first generate two pretraining datasets of the\nsame size, one of which has the distributional prop-\nerty in Eq. 1 while the other one does not (accord-\ning to §4.1). We then use these two datasets to\npretrain two Transformer models (Vaswani et al.,\n2017) with the masked language modeling objec-\ntive (the w/ DH and w/o DH model). Finally, we\nfine-tune these two models with a downstream task\ndataset generated in §4.2 (more details in B). We\nreport the performance of the models after fine-\ntuning them with different numbers of downstream\nexamples. By comparing the performance of the\ndownstream task, we can infer to what extent the\ndistributional property contributes to the efficacy\nof MLM pretraining. Despite the complexity of\nnatural language, we posit that a portion of seman-\ntic equivalence in natural language is encoded in\na distribution as simple as the one of this pseudo\nlanguage. Therefore, positive results observed in\nthis toy experiment would also apply to real-world\nsetups. In the following, we describe how we use\nthe four downstream datasets (i.e., A-D1, B-D1,\nA-D2, and B-D2) for evaluation:\nSample efficiency. We fine-tune the models with\na dataset in which 50% of examples are in A-D1\nand the other 50% are in B-D2. In this setting, if\nthe model has the knowledge about the semantic\nisomorphism between Φa and Φb, then the model\nwill be able to use the examples from both domains\nto learn a single labeling function. However, if\nthe model has no such prior knowledge, then it\nwill need to learn two labeling functions for A-D1\nand B-D2 respectively with only half of the total\namount of data. Thus, if the model can learn to uti-\nlize the semantic equivalence encoded in the distri-\nbution property (Eq. 1) of the pretraining data, then\nmodels pretrained with dataset having the property\nin Eq. 1 will perform better.\nGeneralization capability. We assess general-\nization capability by evaluating the model on a test\nset whose distribution is different from the model’s\ntraining set. Specifically, we fine-tune the model\nwith a training set in A-D1 and test it with test\nsets in all of the 4 distributions. We also explore\nwhether having a small amount of data from a tar-\nget domain can improve the generalization. Thus\nwe also experiment with the setting where 10% of\nthe training samples are from B-D2.\nThere are distribution shifts in two directions in\nthis setting. One direction is the distribution shift\nfrom feature set Φa to Φb (e.g. from A-D1 to B-\nD1). We refer to this direction as a vocabulary\nshift, because the vocabulary is changed. The other\ndirection is the shift from P(1)\nΣ to P(2)\nΣ (e.g. from\nA-D1 to A-D2). We refer to it as a semantic shift,\nbecause the correlation of the synsets is changed.\nThese two distribution shifts exist in real-world\nNLP scenarios. The vocabulary shift is analogous\nto cross-lingual generalization. It is similar to fine-\ntuning a multi-lingual pretrained model with a lan-\nguage and doing zero-shot transfer to a test set in\nanother language (Artetxe et al., 2020). As for the\nsemantic shift, it is related to the spurious correla-\ntions in training data (Poliak et al., 2018; Gururan-\ngan et al., 2018; McCoy et al., 2019; Chiang et al.,\n2020; Gardner et al., 2021; Eisenstein, 2022).\n5.2 Other Baselines\nIn addition to the model pretrained with a dataset\nthat does not have the distributional property\n(Eq. 1), we also have other baselines:\n• From-scratch: We fine-tune the model from\nscratch with randomly initialized parameters.\n• CBOW: We pretrain CBOW embeddings with\nthe data that has the distributional property\n10308\nFigure 2: Performance on the synthetic downstream\ntask when the models are fine-tuned with 50% A-D1\nand 50% B-D2 (§4.2).\nand use it to initialize the embedding layer of\na randomly initialized Transformer.\n• Shuffle-weight: We randomly shuffle the\nweights in each module of the MLM pre-\ntrained model before fine-tuning it, which\nkeeps the statistics of the parameters. This\nis motivated by a previous work showing the\nefficacy of parameter initialization statistics\n(Wu et al., 2022). We inspect whether the\nstatistics explain the efficacy of the model.\n5.3 Experiment Results 2\n5.3.1 Sample Efficiency\nWe find that the distributional property (Eq. 1) can\nlargely explain MLMs’ better sample efficiency.\nIn Figure 2, we can see that the w/ DH model’s\nperformance on A-D1 and B-D2 grows faster than\nthe performance of the w/o DH model. This implies\nthat pretraining with data satisfying Eq. 1 indeed\nimproves the sample efficiency. The w/ DH model\nalso has better performance on B-D1 and A-D2. It\nis aligned with our intuition that Eq. 1 can help the\nmodel utilize the isomorphism between Φa and Φb\nand learn a single general function instead of two\nfunctions for A-D1 and B-D2.\n2Models initialized with shuffled weights have similar per-\nformance as models trained from scratch so we omit them in\nthe figures for clarity.\nThe performance of the CBOW initialization is\nalso better than training from scratch. Thus, the dis-\ntributional property in Eq. 1 is indeed a valid expla-\nnation for non-contextualized embeddings. How-\never, its improvement is not as stable as the w/ DH\npretrained MLM. It suggests that in addition to the\nproperty of distribution, some other factors also\nattribute a model’s better sample efficiency.\n5.3.2 Generalization Capability\nGeneralization in the presence of vocabulary\nshifts. The results in Figure 3a and 3b indicate\nthat the distributional property in Eq. 1 can help\nthe generalization when there are vocabulary shifts\nto some extent. We observe that the w/o DH model\ndoes not generalize to B-D1 and B-D2 at all, while\nthe w/ DH model can generalize from domain A-\nD1 to the test set in B-D1 and B-D2. However,\nthe generalization diminishes as the amount of fine-\ntuning data increases. The model may suffer from\ncatastrophic forgetting after we fine-tune it more.\nHaving some fine-tuning data in B-D2 mitigates\nthis problem. As shown in Figure 3b, when the\n10% of the fine-tuning data is in B-D2, the w/ DH\nmodel outperforms the w/o DH model much for the\ntest sets in B-D1 and B-D2, and the generalization\ndoes not diminish.\nThe CBOW model also generalizes to B-D1 and\nB-D2. Its generalization to B-D1 does not suffer\nfrom catastrophic forgetting. Thus, the distribu-\ntion property in Eq. 1 indeed largely explains the\nefficacy of non-contextualized word embedding.\nGeneralization to the semantic shifts. We find\nthat the distributional property in Eq. 1 can not ex-\nplain it. From both Figure 3a and Figure 3b, we\ncan see that both the w/o DH model and the w/ DH\nmodel generalize to A-D2 better than the model\ntrained from scratch. Moreover, the w/ DH model\ndoes not perform better. Meanwhile, the CBOW\nmodel performs only slightly better than training\nfrom scratch. Thus, generalization to the semantic\nshifts may be related to the nature of MLM pre-\ntraining instead of data properties (Chiang, 2021).\n5.4 Summary\nOur results suggest that pretraining with data whose\ndistribution encodes the semantic equivalence be-\ntween words (Eq. 1) improves pretrained MLM’s\nsample efficiency, but the generalization capability\nin the presence of vocabulary shifts diminishes as\nthe number of training examples grows.\n10309\n0.2 0.4 0.6 0.8 1\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRatio of training data\nAccuracy\n0.2 0.4 0.6 0.8 1\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRatio of training data\nAccuracy\n(a) Fine-tuning with 100% A-D1\n0.2 0.4 0.6 0.8 1\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nA-D1 w/ DH A-D1 w/o DH B-D1 w/ DH B-D1 w/o DH\nA-D2 w/ DH A-D2 w/o DH B-D2 w/ DH B-D2 w/o DH\nRatio of training data\nAccuracy\n0.2 0.4 0.6 0.8 1\n0.5\n0.6\n0.7\n0.8\n0.9\nA-D1 CBOW A-D1 scratch B-D1 CBOW B-D1 scratch\nA-D2 CBOW A-D2 scratch B-D2 CBOW B-D2 scratch\nRatio of training data\nAccuracy\n(b) Fine-tuning with 90% A-D1 and 10% B-D2\nFigure 3: Performance on the synthetic downstream task described in §4.2.\nOur experiments also shed light on the effect of\nnon-contextualized word embeddings and the statis-\ntics of parameter initialization. We find that the\ndistributional property (Eq. 1) indeed contributes\nto non-contextualized embeddings’ better sample\nefficiency and better generalization capability. As\nfor the statistics of the parameter initialization, we\nfind that models with shuffled weights marginally\noutperform models trained from scratch for all the\nsettings. It suggests the statistics of parameters\nmay not explain the efficacy of pretraining.\nAdditionally, we also observe phenomena that\nthe distributional property (Eq. 1) cannot explain.\nWe find that the distributional property is unrelated\nto the semantic distribution shifts. We also observe\nthat using CBOW embeddings is not as stable as\nusing an MLM pretrained model. Therefore, the\ndistributional property alone does not fully explain\nthe efficacy of model pretraining.\nAs for the multi-token setting, sometimes the\ndistribution property does not improve sample ef-\nficiency but still helps generalization. We include\nthe results in A.2 for brevity.\n6 Analyzing Models for Real-world\nDatasets\nIn § 5, we show that the distributional property in\nEq. 1 can improve sample efficiency, at least when\nthe feature is at the single-token level. We posit that\nthis conclusion applies to the real-world scenario\nwhere the data also has this property. However,\nthe negative results regarding the generalization\ncapability may be due to the oversimplification of\nour toy setting. Therefore, in this section, we study\ngeneralization in real-world scenarios.\nUnfortunately, it is difficult to conduct a fully\ncontrolled experiment as we do with the toy set-\ntings. It is impossible to find out all the synsets\nin a natural language. It is also difficult to divide\nfeatures into isomorphic subsets as the feature sets\nΦa and Φb we have in the toy settings. Therefore,\nwe will measure the relationship between the dis-\ntributional property in Eq. 1 and generalization in\nan indirect manner.\n6.1 Experiment Design\nWe want to measure to what degree we can at-\ntribute models’ generalization capability to the dis-\ntributional property (Eq. 1) of the pretraining data.\nThus, we design an experiment using the follow-\ning premise: if a fine-tuned model f generalizes\nwell because the pretrained model can learn seman-\ntic equivalence between features by modeling the\ndistribution property in Eq. 1, then whether f can\ngeneralize should depend on whether f0 models\nEq. 1 successfully.\nBased on this premise, we design our experiment\nin the following way:\n• Given a validation or testing sample x in a\ndownstream task, we pick a feature afrom x\n10310\nand generate a noisy paraphrase b(§6.2).\n• We measure how well the fine-tuned model\nf generalizes by how much its prediction\nchanges after we replace ain x with b:\nDf(x,a,b ) := TV[f(x)∥f(Replace(x,a,b )],\nwhere TV is the total variation distance.\n• We also measure whether the pretraining pro-\ncess encodes the semantic equivalence be-\ntween a and b in the model, which we will\nelaborate on later in §6.3.\n• Finally, we measure the Pearson correlation\ncoefficient between Df0 (a,b) and Df(x,a,b ).\nWe should observe that Df is high only when\nDf0 is high if the fine-tuned model generalizes\ntotally based on the semantic equivalence en-\ncoded by the distributional property in Eq. 1.\nWe analyze a bert-base-uncased model for a\nsingle-sentence and a multi-sentence task, the SST-\n2 task, and the MNLI task. The MNLI dataset\ncomes with its constituency parsing along with\nthe POS tags. As for the SST-2 dataset, it does\nnot include the POS tags, so we parse it with the\nStanford parser (Manning et al., 2014). In addition\nto analyzing the final fine-tuned model, we report\nthe results for intermediate checkpoints. We also\nanalyze models trained with different amounts of\ndata to inspect the effect of training data size.\n6.2 Noisy Feature Paraphrases\nWe use validation and test examples in this anal-\nysis and extract features at different levels: word\nlevel, phrase level, sentence level, and example\nlevel. For word-level features, we first measure the\nword saliency (Li et al., 2016; Ren et al., 2019)\n3 and extract the word in the example with the\nhighest saliency. For the phrase-level features, we\nconsider both short phrases and long phrases in\nan example. For short phrases, we measure the\nsaliency of phrases no longer than 4 words and\nchoose the one with the greatest saliency. For long\nphrases, we choose the longest phrase in the exam-\nple. A sentence-level feature is a whole sentence\nin a testing sample, while an example-level feature\nmay involve more than one sentence depending\n3i.e., we measure the impact of a feature by measuring the\nchange of the fine-tuned model’s prediction after we mask the\nfeature.\non the task. For example, in the NLI task, each\nexample involves two sentences.\nWe use WordNet (Miller, 1998) and back-\ntranslation (Sennrich et al., 2016; Yu et al., 2018) to\ngenerate semantically similar features. For a word-\nlevel feature a, we randomly select a word bfrom\none of its synonyms in WordNet that has the same\nPOS tag. For a feature at a higher level, we use\nback-translation to paraphrase it because we find\nthat back-translation generates more diverse para-\nphrases than publicly available paraphrase models\ndo. We use Facebook FAIR’s WMT19 translation\nmodels (Ng et al., 2019) and use German as the\nintermediate language. When generating a para-\nphrase bfor a feature ain x, we use the context\nbefore ain xas the prefix of the generation pro-\ncess, so bhas the same syntax role asa. We include\nsome examples in Table 1.\nNote that aand bdo not need to be perfectly sim-\nilar. Although an imperfect (a,b) pair may cause\nthe fine-tuned model to change its prediction (hav-\ning high Df(x,a,b ), a pretrained model should\nalso identify a and b as dissimilar (having high\nDf0 (a,b)). Therefore, noises in our paraphrasing\ngeneration process do not affect our analyses.\n6.3 Measuring Semantic Distance from a\nPretrained Model\nIf pretrained models learn the semantic relationship\nbetween features by modeling the distributional\nproperty in Eq. 1 during MLM pretraining, then\nthe distribution it predicts will reflect its knowl-\nedge about the semantic relationship. Specifically,\nif MLM pretraining encodes the semantic equiv-\nalence between two features a,b in a pretrained\nmodel f0, then the model should predict similar\ncontext distributions for aand b. Therefore, we can\ncheck whether the pretraining process encodes the\nsemantic equivalence between feature aand bby\nDf0 (a,b) := TV[f0(context|a)∥f0(context|b)],\nwhere TV is the total variation distance.\nWe use templates to construct queries for the\ncontext distribution conditioned on a feature. A\ndesirable template should convert a feature into\na grammatical sentence containing a mask token\nsuch that the distribution at the mask token can\nreflect the semantics of the feature. Therefore, we\ndesign different templates to take the features’ syn-\ntax role into consideration:\n10311\nTemplates for word-level and phrase-level fea-\ntures We expect that the verb after a noun can\nreflect its meaning because different things act\nin different ways. For example, both cats and\ndogs walk, but only dogs bark. Therefore, if the\nfeature is a noun or noun phrase, then we use\n“<feature> [mask] ” as its template. Based on\na similar intuition, if the feature is an adjective or\nadjective phrase, then we use template “ [mask]\nis <feature> ”. If the feature is a verb or verb\nphrase, we use the template “[mask] <feature>”.\nSentences with this syntax structure are pervasive\nin the pretraining data, so these templates construct\nnatural sentences as the queries for the semantic\nrelationship in the pretrained model.\nTemplates for sentence-level features We can\nsee a sentence as a very high-level feature. For this\nkind of feature, we use two templates. The first\none is more natural: “<feature> with [mask].”,\nwhere we attach a prepositional phrase after the\nfeature using “with”. Because the word after “with”\nlargely depends on the semantics of the context be-\nfore it, the distribution at the “[mask]” reflects the\nsemantics of the feature. We also experiment with\na template proposed by Jiang et al. (2022) because\nthey show that “\"<feature>\" means [MASK]” can\nextract high-quality sentence representations.\nTemplates for example-level features At the\nhighest level, we can treat an example of a task\nas a feature. That is, for tasks that involve multiple\nsentences, such as a natural language inference task,\nwe treat a pair of sentences as a feature. For this\nhigh-level feature, we use task-specific templates\nfrom Gao et al. (2021). These templates can elicit\nthe model’s prior knowledge about the task. We\nwill inspect to what extend such prior knowledge\nis preserved after we fine-tuned the model.\n6.4 Results and Discussion\nFigure 4 shows that the correlation between\nDf(x,a,b) and Df0 is below 0.15 for both MNLI\nand SST-2. For MNLI, higher-level features have\na higher correlation. As for SST-2, task-level fea-\ntures still have the highest correlation, but short-\nphrase-level features have the second-highest corre-\nlation. It may be because the SST-2 model utilizes\nlower-lever features more. We also observe that\nthe correlation does not diminish to 0 regardless\nof the training steps or data sizes. It suggests that\nthe real-world scenario may be more similar to the\nhigh-level toy setting where isomorphic feature sets\n5k 10k 15k 20k 25k 30k 35k 40k\n0\n0.05\n0.1\n0.15\ntask with mean long phrase short phrase word\nCorrelation\n2k 4k 6k 8k 10k 12k 14k 16k\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\nStep\nCorrelation\n(a) MNLI (up) and SST-2 (down) at different steps. We mea-\nsure the correlation per 100 steps at the beginning.\n0.1 0.2 0.3 0.4 0.5\n0\n0.02\n0.04\n0.06\n0.08\nData Ratio\nCorrelation\n0.1 0.2 0.3 0.4 0.5\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nData Ratio\nCorrelation\n(b) MNLI (left) SST-2 (right) with different data sizes.\nFigure 4: Correlation between Df(x,a,b) and Df0 .\nshare the vocabulary (in Appendix A.2). However,\nin general, the correlation is low, indicating that the\nmodel’s knowledge learned from the distributional\nproperty (Eq. 1) of the pretraning data can hardly\nexplain models’ generalization.\n7 Related Work\nThis work aims to inspect how MLM pretraining\ninfuses inductive bias for downstream tasks. It is\ndifferent from previous studies that treat pretrained\nmodels as static models. For example, Wei et al.\n(2021) and Chiang (2021) study the efficacy of\nMLM theoretically by analyzing the distribution\npredicted by the model. Tenney et al. (2019b,a);\nLiu et al. (2019a); Hewitt and Manning (2019); Wu\net al. (2020); Zhang et al. (2021) study the rich\nlinguistic structure encoded in the representation.\nSome works focus on the fine-tuning process but\nthe pretraining loss. For instance, Hao et al. (2019)\nshows MLMs have better loss landscape. Agha-\njanyan et al. (2021) shows that pretrained models\nhave lower intrinsic dimensionality. Malladi et al.\n(2022) shows neural tangent kernels (Jacot et al.,\n2018) can explain MLM’s efficacy to some extent.\nSome works study the connection between pre-\ntraining and downstream tasks. Li et al. (2021)\nshow that non-language pretraining tasks help NLP\n10312\ndownstream tasks, while Lu et al. show that lan-\nguage models help non-language downstream tasks.\nLovering et al. (2021) uses some linguistic tasks\nto show that the linguistic structures in MLMs can\nserve as an inductive bias. Zhang and Hashimoto\n(2021) investigate whether MLMs’ efficacy can\nbe explained by the task-relevant cloze-like masks\nalone. Our work follows this line and explains\nMLMs’ general machine-learning characteristics.\nBeyond the scope of NLP, the concept of con-\nnecting training examples is very common in ma-\nchine learning. For example, co-training (Blum\nand Mitchell, 1998) uses multiple views of fea-\ntures to build connections between training sam-\nples. Recently, Saunshi et al. (2019); HaoChen\net al. (2021) show that contrastive self-supervised\nlearning reduces sample efficiency by pulling the\nrepresentation of similar samples together. Sim-\nilarly, Shen et al. (2022) shows that contrastive\nlearning connects samples in different domains and\nthus helps generalization. However, those studies\nmainly focus on contrastive learning for continuous\ndata instead of discrete languages.\n8 Discussion\nWe showed in a synthetic experiment that pretrain-\ning with a masked language model objective allows\nthe model to make use of semantic equivalence that\nsimple distributions (e.g. Markov models) can en-\ncode to improve models’ sample efficiency. How-\never, we found that it can not explain generalization\nin the presence of distribution shifts.\nOur work leads to two future research directions.\nFirst, we showed the limitations of our understand-\ning of pretraining. What semantic relationships are\nimportant, how data distributions encode those rela-\ntionships, and how modeling data distributions help\ndownstream performance remain open questions.\nSecond, our results in §6 show the unpredictability\nof models’ generalization behavior. Investigating\nthis behavior may help us develop more robust NLP\nmodels.\nLimitations\nOur main goal is to provide a new perspective to\nanalyze the efficacy of pretraining and to show that\nthe distributional hypothesis is not a sufficient ex-\nplanation. We only study semantic equivalence\nbetween features. Other relationships, such as\nhypernyms or causal relationships, are not in the\nscope of this paper. Additionally, we only consider\na relatively simple case, where tasks are pattern-\nmatching tasks. We do not consider, for example,\ntasks that require reasoning. In §4, we study the ef-\nfect of the distributional property using Markov dis-\ntributions. It only suggests that model can achieve\nbetter sample efficiency by utilizing the seman-\ntic relationship encoded in this simple first-order\ndistribution. How the models are able to utilize\nsemantics encoded in more complicated distribu-\ntions, such as the distribution of natural languages,\nremains unknown. In §6, we only consider the dis-\ntribution of a single context token; however, the\nsemantics of a feature may be encoded in the dis-\ntribution in a subtler way. Finally, we show the\ninefficiency of the distributional hypothesis as an\nexplanation by providing counterexamples with\nsmall language models. We leave the study of large\nlanguage models for future work.\nAcknowledgements\nWe appreciate Joshua Robinson for providing valu-\nable feedback on this paper. We also thank the\nmembers of my lab, tamagotchi lab at USC, for\nhelpful discussions. Finally, we thank the USC\nNLP cluster maintenance team.\nReferences\nArmen Aghajanyan, Sonal Gupta, and Luke Zettle-\nmoyer. 2021. Intrinsic dimensionality explains the\neffectiveness of language model fine-tuning. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 7319–7328,\nOnline. Association for Computational Linguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4623–4637, Online. Association\nfor Computational Linguistics.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862.\nAvrim Blum and Tom Mitchell. 1998. Combining la-\nbeled and unlabeled data with co-training. In Pro-\nceedings of the Eleventh Annual Conference on Com-\nputational Learning Theory, COLT’ 98, page 92–100,\nNew York, NY , USA. Association for Computing Ma-\nchinery.\n10313\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nTing-Rui Chiang. 2021. On a benefit of mask language\nmodeling: Robustness to simplicity bias. arXiv\npreprint arXiv:2110.05301.\nTing-Rui Chiang, Hao-Tong Ye, and Yun-Nung Chen.\n2020. An empirical study of content understanding\nin conversational question answering. Proceedings\nof the AAAI Conference on Artificial Intelligence ,\n34(05):7578–7585.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJacob Eisenstein. 2022. Informativeness and invariance:\nTwo perspectives on spurious correlations in natural\nlanguage. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4326–4331, Seattle, United States.\nAssociation for Computational Linguistics.\nJohn Firth. 1957. A synopsis of linguistic theory, 1930-\n1955. Studies in linguistic analysis, pages 10–32.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nMatt Gardner, William Merrill, Jesse Dodge, Matthew\nPeters, Alexis Ross, Sameer Singh, and Noah A.\nSmith. 2021. Competency problems: On finding and\nremoving artifacts in language data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1801–1813, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy,\nRoy Schwartz, Samuel Bowman, and Noah A. Smith.\n2018. Annotation artifacts in natural language infer-\nence data. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 107–112,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-\nalizing and understanding the effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4143–\n4152, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJeff Z. HaoChen, Colin Wei, Adrien Gaidon, and\nTengyu Ma. 2021. Provable guarantees for self-\nsupervised deep learning with spectral contrastive\nloss. In Advances in Neural Information Process-\ning Systems, volume 34, pages 5000–5011. Curran\nAssociates, Inc.\nZellig S Harris. 1954. Distributional structure. Word,\n10(2-3):146–162.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2744–2751, Online. Association for Computa-\ntional Linguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nAlexander Immer, Lucas Torroba Hennigen, Vincent\nFortuin, and Ryan Cotterell. 2022. Probing as quanti-\nfying inductive bias. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1839–\n1851, Dublin, Ireland. Association for Computational\nLinguistics.\nArthur Jacot, Franck Gabriel, and Clement Hongler.\n2018. Neural tangent kernel: Convergence and gen-\neralization in neural networks. In Advances in Neural\nInformation Processing Systems, volume 31. Curran\nAssociates, Inc.\nTing Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang,\nDeqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen\nHuang, Denvy Deng, and Qi Zhang. 2022. Prompt-\nBERT: Improving BERT sentence embeddings with\n10314\nprompts. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 8826–8837, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nHunter Lang, Monica N Agrawal, Yoon Kim, and David\nSontag. 2022. Co-training improves prompt-based\nlearning for large language models. In Proceedings\nof the 39th International Conference on Machine\nLearning, volume 162 of Proceedings of Machine\nLearning Research, pages 11985–12003. PMLR.\nOmer Levy and Yoav Goldberg. 2014. Neural word\nembedding as implicit matrix factorization. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 27. Curran Associates, Inc.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-\nderstanding neural networks through representation\nerasure. arXiv preprint arXiv:1612.08220.\nLiunian Harold Li, Haoxuan You, Zhecan Wang, Alireza\nZareian, Shih-Fu Chang, and Kai-Wei Chang. 2021.\nUnsupervised vision-and-language pre-training with-\nout parallel images and captions. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5339–5350,\nOnline. Association for Computational Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nCharles Lovering, Rohan Jha, Tal Linzen, and Ellie\nPavlick. 2021. Predicting inductive biases of pre-\ntrained models. In International Conference on\nLearning Representations.\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mor-\ndatch. Pretrained transformers as universal computa-\ntion engines.\nSadhika Malladi, Alexander Wettig, Dingli Yu, Danqi\nChen, and Sanjeev Arora. 2022. A kernel-based\nview of language model fine-tuning. arXiv preprint\narXiv:2210.05643.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David McClosky.\n2014. The Stanford CoreNLP natural language pro-\ncessing toolkit. In Proceedings of 52nd Annual Meet-\ning of the Association for Computational Linguis-\ntics: System Demonstrations , pages 55–60, Balti-\nmore, Maryland. Association for Computational Lin-\nguistics.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heuris-\ntics in natural language inference. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3428–3448, Florence,\nItaly. Association for Computational Linguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nIn Advances in Neural Information Processing Sys-\ntems, volume 26. Curran Associates, Inc.\nGeorge A Miller. 1998. WordNet: An electronic lexical\ndatabase. MIT press.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\nMichael Auli, and Sergey Edunov. 2019. Facebook\nFAIR’s WMT19 news translation task submission.\nIn Proceedings of the Fourth Conference on Machine\nTranslation (Volume 2: Shared Task Papers, Day\n1), pages 314–319, Florence, Italy. Association for\nComputational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language infer-\nence. In Proceedings of the Seventh Joint Confer-\nence on Lexical and Computational Semantics, pages\n180–191, New Orleans, Louisiana. Association for\nComputational Linguistics.\nShuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.\n2019. Generating natural language adversarial exam-\nples through probability weighted word saliency. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1085–\n1097, Florence, Italy. Association for Computational\nLinguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\n10315\nNikunj Saunshi, Orestis Plevrakis, Sanjeev Arora,\nMikhail Khodak, and Hrishikesh Khandeparkar.\n2019. A theoretical analysis of contrastive unsu-\npervised representation learning. In Proceedings of\nthe 36th International Conference on Machine Learn-\ning, volume 97 of Proceedings of Machine Learning\nResearch, pages 5628–5637. PMLR.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation models\nwith monolingual data. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 86–96,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nKendrick Shen, Robbie M Jones, Ananya Kumar,\nSang Michael Xie, Jeff Z. Haochen, Tengyu Ma,\nand Percy Liang. 2022. Connect, not collapse: Ex-\nplaining contrastive learning for unsupervised do-\nmain adaptation. In Proceedings of the 39th Inter-\nnational Conference on Machine Learning, volume\n162 of Proceedings of Machine Learning Research,\npages 19847–19878. PMLR.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional hy-\npothesis: Order word matters pre-training for little.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2888–2913, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nMichael Tänzer, Sebastian Ruder, and Marek Rei. 2022.\nMemorisation versus generalisation in pre-trained\nlanguage models. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 7564–7578,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Sam Bowman, Dipanjan Das, and\nEllie Pavlick. 2019b. What do you learn from con-\ntext? probing for sentence structure in contextualized\nword representations. In International Conference\non Learning Representations.\nLifu Tu, Garima Lalwani, Spandana Gella, and He He.\n2020. An empirical study on robustness to spuri-\nous correlations using pre-trained language models.\nTransactions of the Association for Computational\nLinguistics, 8:621–633.\nL. G. Valiant. 1984. A theory of the learnable. Commun.\nACM, 27(11):1134–1142.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nColin Wei, Sang Michael Xie, and Tengyu Ma. 2021.\nWhy do pretrained language models help in down-\nstream tasks? an analysis of head and prompt tuning.\nIn Advances in Neural Information Processing Sys-\ntems, volume 34, pages 16158–16170. Curran Asso-\nciates, Inc.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nYuhuai Wu, Felix Li, and Percy Liang. 2022. Insights\ninto pre-training via simpler synthetic tasks. In Ad-\nvances in Neural Information Processing Systems.\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020.\nPerturbed masking: Parameter-free probing for ana-\nlyzing and interpreting BERT. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4166–4176, Online. Asso-\nciation for Computational Linguistics.\nAdams Wei Yu, David Dohan, Quoc Le, Thang Luong,\nRui Zhao, and Kai Chen. 2018. Fast and accurate\nreading comprehension by combining self-attention\nand convolution. In International Conference on\nLearning Representations.\nTianyi Zhang and Tatsunori B. Hashimoto. 2021. On the\ninductive bias of masked language modeling: From\nstatistical to syntactic dependencies. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5131–5146,\nOnline. Association for Computational Linguistics.\n10316\nYian Zhang, Alex Warstadt, Xiaocheng Li, and\nSamuel R. Bowman. 2021. When do you need bil-\nlions of words of pretraining data? In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1112–1125, Online.\nAssociation for Computational Linguistics.\nA Toy Experiments in the Multi-token\nSetting\nA.1 Independent Experimental Variables\nFeature Levels. We can also control whether the\nsemantic relationship in this pseudo language is\nat a single-token level or at a multi-token level,\nnamely whether each feature corresponds to one or\nmore tokens. This multi-token setting is interesting\nbecause the distribution of the token sequences will\nhave higher-order dependence than a Markov chain.\nAdditionally, multi-token features also make this\npseudo-language more similar to natural languages.\nWe decide the mapping between features and\ntoken sequences before we start generating pas-\nsages. We use two vocabulary sets Vα and Vβ for\nthe feature sets Φa and Φb respectively. In the\nsingle-token setting, each feature in Φa and Φb is\nbijectively mapped to a token in Vα and Vβ respec-\ntively. In the multi-token setting, each feature in\nΦa and Φb corresponds to 1 to 3 randomly selected\ntokens from Vα and Vβ respectively.\nVocabulary sharing betweenΦa and Φb. When\nthe features are multi-token, Vα and Vβ can be the\nsame while keeping the mapping between features\nand token sequences bijective. Therefore, we can\nchoose whether to share the vocabulary between\nΦa and Φb. The no-sharing setting is analogous to\nthe multilingual setting in the real world, while the\nvocabulary-sharing setting is more similar to the\nmonolingual setting.\nA.2 Results on the Multi Token Setting\nA.2.1 Sample Efficiency.\nFor the multi-token-level setting, Eq. 1 helps the\nsample efficiency only when the feature sets Φa\nand Φb have separated vocabulary. In Figure 5b,\nwe can see the w/ DH model consistently outper-\nforms the w/o DH model when there is no vocabu-\nlary sharing. However, the improvement is not as\napparent as in the single-token-level setting. When\nthe feature sets Φa and Φb have shared vocabu-\nlary, as shown in Figure 5a, the w/o DH model\nperforms better than the w/ DH model. The results\n0.2 0.4 0.6 0.8 1\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nRatio of training data\nAccuracy\n(a) separated vocabulary\n0.2 0.4 0.6 0.8 1\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nA-D1 w/ DH A-D1 w/o DH A-D1 CBOW A-D1 scratch\nB-D1 w/ DH B-D1 w/o DH B-D1 CBOW B-D1 scratch\nA-D2 w/ DH A-D2 w/o DH A-D2 CBOW A-D2 scratch\nB-D2 w/ DH B-D2 w/o DH B-D2 CBOW B-D2 scratch\nRatio of training data\nAccuracy\n(b) shared vocabulary\nFigure 5: The downstream performance of fine-tuning\nwith 50% A-D1 and 50% B-D2 in the multi-token fea-\nture setting.\nindicate that MLM pretraining is not always able\nto infuse the inductive bias for the muti-token se-\nmantic relationship. As for the model initialized\nwith CBOW embeddings, it barely outperforms\nthe model trained from scratch. It shows that non-\ncontextualized embeddings are not able to capture\nthe semantic relationship at a higher level.\nA.2.2 Generalization Capability\nWhen Φa and Φb have separated vocabularies, we\nhave similar observations as in the single-token-\nlevel setting. When all the fine-tuned data is in\nA-D1, the w/ DH model generalizes to B-D1 and\nB-D2, and the generalization also diminishes when\nthere is more data. As for the performance on A-\nD2, the w/ DH and w/o DH models outperform\ntraining from scratch too, though the advantage is\nnot as apparent as in the single-token setting.\nWhen 10% of the fine-tuning data is in B-D2,\nits generalization to B-D1 persists. However, w/\nDH does not improve the generalization to B-D2 as\nmuch as in the single-token-level setting. Interest-\ningly, the w/ DH model outperforms the w/o DH\nmodel for A-D2. It implies that the distributional\nproperty in Eq. 1 helps the generalization to seman-\ntic shift, which is different from the observations in\nthe other settings. It is possibly because the w/ DH\nmodel has better sample efficiency. As for CBOW,\nit does not help generalization either, indicating\nthat CBOW is not able to capture the multi-token\n10317\n0.2 0.4 0.6 0.8 1\n0.6\n0.7\n0.8\n0.9\n1\nRatio of training data\nAccuracy\n(a) Fine-tuning with 50% A-D1 and 50% B-D2\n0.2 0.4 0.6 0.8 1\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRatio of training data\nAccuracy\n(b) Fine-tuning with 100% A-D1\n0.2 0.4 0.6 0.8 1\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nA-D1 w/ DH A-D1 w/o DH A-D1 CBOW A-D1 scratch\nB-D1 w/ DH B-D1 w/o DH B-D1 CBOW B-D1 scratch\nA-D2 w/ DH A-D2 w/o DH A-D2 CBOW A-D2 scratch\nB-D2 w/ DH B-D2 w/o DH B-D2 CBOW B-D2 scratch\nRatio of training data\nAccuracy\n(c) Fine-tuning with 90% A-D1 and 10% B-D2\nFigure 6: Performance on the synthetic downstream task described in §4.2 (enlarged plots).\n10318\n0.2 0.4 0.6 0.8 1\n0.5\n0.6\n0.7\n0.8\n0.9\nRatio of training data\nAccuracy\n(a) Fine-tuned with 100% A-D1, separated vocabulary\n0.2 0.4 0.6 0.8 1\n0.5\n0.6\n0.7\n0.8\n0.9\nRatio of training data\nAccuracy (b) Fine-tuned with 100% A-D1, shared vocabulary\n0.2 0.4 0.6 0.8 1\n0.5\n0.6\n0.7\n0.8\n0.9\nA-D1 w/ DH A-D1 w/o DH A-D1 CBOW A-D1 scratch\nB-D1 w/ DH B-D1 w/o DH B-D1 CBOW B-D1 scratch\nRatio of training data\nAccuracy\n(c) Fine-tuned with 90% A-D1 and 10% B-D2, separated vo-\ncabulary\n0.2 0.4 0.6 0.8 1\n0.5\n0.6\n0.7\n0.8\n0.9\nA-D2 w/ DH A-D2 w/o DH A-D2 CBOW A-D2 scratch\nB-D2 w/ DH B-D2 w/o DH B-D2 CBOW B-D2 scratch\nRatio of training data\nAccuracy\n(d) Fine-tuned with 90% A-D1 and 10% B-D2, shared vocabu-\nlary\nFigure 7: Performance of the downstream task in the multi-token feature setting.\nsemantic relationship.\nWhen Φa and Φb share the vocabulary, we ob-\nserve mixed results. We find that when all the\nfine-tuning data is in A-D1, the w/ DH model out-\nperforms the w/o DH model on B-D1 persistently.\nIt implies that, in this setting, the distributional\npropery in Eq. 1 helps the model generalize to vo-\ncabulary shift better than in the single-token setting.\nHowever, when 10% of the fine-tuning data is from\nB-D2, the advantage over the w/o DH model is\nless apparent. Additionally, regardless of whether\nwe use data in B-D2 to fine-tune the model, Eq. 1\ndoes not help the generalization to B-D2. MLM\npretrained does not outperform the model trained\nfrom scratch on A-D2 either. It implies that MLM\npretraining does not help the generalization to se-\nmantic shift in this setting.\nB Experiment Details For §5\nWe use a 6-layer transformer (Vaswani et al., 2017)\nwith 12 heads. P(1)\nΣ and P(2)\nΣ are 2 fully-connected\nrandomly generated Markov chains. Each node in\nthe Markov chains has two outward edges point-\ning to two random nodes (as in Figure 1). Each\nnode in a chain has uniform probability to be the\nstarting node. For the pretrainng corpora, we sam-\nple 100k sequences, each of which contains 256\nsynsets. The dataset for the downstream task con-\ntains another 100k sequences, each of which con-\ntains 100 synsets. When fine-tuning the model, we\nearly-stop when the validation performance does\nnot improve for 5 epochs consecutively. As for the\nlabeling function, we use 5 patterns in total, and\n|S1|= |S2|= |S3|= 12.\nC Experiment Details for §6\nWe use the Trainer API in the Python package trans-\nformers v4.21.2 with the default hyper-parameters.\nWhen doing back-translation, we use beam size\nequal to 5. Among the generated candidates whose\nscore is not less than the score of the best candi-\ndates by more than 1, we choose the one with the\nhighest edit-distance to the original input.\n10319\nFeature Examples\nWord\n• the movie understands like few others how the depth and breadth of emotional\ncloseness (intimacy) give the physical act all of its meaning and most of its\npleasure.\n• in an effort , i suspect , not to spite (offend) by appearing either too serious or\ntoo lighthearted , it spites by just being wishy-washy.\n• if you are an actor who can relate to the search for inner peace by dramatically\ndepicting the lives of others onstage , then esther ’s story is a compelling pursuit\n(quest) for truth.\n• dazzles with its fully-written characters , its determined stylishness ( which\nalways relates to characters and story ) and johnny dankworth ’s best soundtrack\nin days (years).\n• the title not only draws (describes) its main characters , but the lazy people\nbehind the camera as well .\nShort phrase\n• if you are an actor who can relate to the search for inner peace by dramatically\ndepicting the lives of other people (the lives of others) onstage , then esther ’s\nstory is a compelling quest for truth.\n• the film is powerful , approachable (accessible) and funny.\n• the best film about baseball to hit theaters since \"Field of Dreams\" (field of\ndreams).\n• a literate presentation that wonderfully weaves a homicidal event (a murderous\nevent) in 1873 with murderous rage in 2002.\nLong phrase\n• although laced with humor and a few fanciful touches , the film is a refreshingly\nearnest look at young women (is a refreshingly serious look at young women).\n• The three stakeholders of vera - mollà, gil and bardem - (vera ’s three actors\n– mollà , gil and bardem –) excel in insightful , empathetic performances.\n• coughs and stutters at his own post-modern vanity. (coughs and sputters on\nits own postmodern conceit .)\n• the old-world - meets-new mesh is embodied in the soundtrack of the film, a\njoyous effusion of disco-Bollywood that lifted my spirit out of the theatre at\nthe end of the monsoon wedding (is incarnated in the movie ’s soundtrack ,\na joyful effusion of disco bollywood that , by the end of monsoon wedding ,\nsent my spirit soaring out of the theater).\nTable 1: Examples of the noisy paraphrase pairs used in Section 6. The text bold face parts is the noisy paraphrase\n(b) based on the original feature in the parentheses (a).\n10320\n0 0.25 0.5 0.75 1\n0\n0.25\n0.5\n0.75\n1\n(a) word\n0 0.25 0.5 0.75 1\n0\n0.25\n0.5\n0.75\n1 (b) short phrase\n0 0.25 0.5 0.75 1\n0\n0.25\n0.5\n0.75\n1 (c) long phrase\n0 0.25 0.5 0.75 1\n0\n0.25\n0.5\n0.75\n1\n(d) sentence, template mean\n0 0.25 0.5 0.75 1\n0\n0.25\n0.5\n0.75\n1 (e) sentence, template with\n0 0.25 0.5 0.75 1\n0\n0.25\n0.5\n0.75\n1 (f) task\nFigure 8: Scatter plots of (Df0 ,Df(x,a,b)) for the MNLI experiment in §6, where the x-axis is Df0 and the y-axis is\nDf .\n0 0.25 0.5 0.75 1\n0\n0.25\n0.5\n0.75\n1\n(a) word\n0 0.25 0.5 0.75 1\n0\n0.25\n0.5\n0.75\n1 (b) short phrase\n0 0.25 0.5 0.75 1\n0\n0.25\n0.5\n0.75\n1 (c) long phrase\n0 0.25 0.5 0.75 1\n0\n0.25\n0.5\n0.75\n1\n(d) sentence, template mean\n0 0.25 0.5 0.75 1\n0\n0.25\n0.5\n0.75\n1 (e) sentence, template with\n0 0.25 0.5 0.75 1\n0\n0.25\n0.5\n0.75\n1 (f) task\nFigure 9: Scatter plots of (Df0 ,Df(x,a,b)) for the SST-2 experiment in §6, where the x-axis is Df0 and the y-axis is\nDf .\n10321",
  "topic": "Generalization",
  "concepts": [
    {
      "name": "Generalization",
      "score": 0.8583755493164062
    },
    {
      "name": "Property (philosophy)",
      "score": 0.7503069639205933
    },
    {
      "name": "Computer science",
      "score": 0.6990465521812439
    },
    {
      "name": "Sample (material)",
      "score": 0.6071202754974365
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.5866294503211975
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5853677988052368
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5108153820037842
    },
    {
      "name": "Language model",
      "score": 0.4942531883716583
    },
    {
      "name": "Natural language processing",
      "score": 0.4850024878978729
    },
    {
      "name": "Natural language",
      "score": 0.4817071557044983
    },
    {
      "name": "Function (biology)",
      "score": 0.4145921468734741
    },
    {
      "name": "Machine learning",
      "score": 0.3788757622241974
    },
    {
      "name": "Mathematics",
      "score": 0.18426010012626648
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ],
  "cited_by": 1
}