{
    "title": "From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network",
    "url": "https://openalex.org/W3195960576",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1436780140",
            "name": "Wang Yu-xin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2350172560",
            "name": "Xie Hongtao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227924421",
            "name": "Fang, Shancheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1892563302",
            "name": "Wang Jing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2352206967",
            "name": "Zhu, Shenggao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1897470950",
            "name": "Zhang Yongdong",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2008806374",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2194187530",
        "https://openalex.org/W2875814315",
        "https://openalex.org/W2964796972",
        "https://openalex.org/W3005436539",
        "https://openalex.org/W2146835493",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1971822075",
        "https://openalex.org/W2520774189",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2979371747",
        "https://openalex.org/W2965066169",
        "https://openalex.org/W1998042868",
        "https://openalex.org/W3183999072",
        "https://openalex.org/W2810983211",
        "https://openalex.org/W3035449864",
        "https://openalex.org/W2144554289",
        "https://openalex.org/W2995460200",
        "https://openalex.org/W1966719512",
        "https://openalex.org/W2795619303",
        "https://openalex.org/W3095672411",
        "https://openalex.org/W2970869018",
        "https://openalex.org/W2896034938",
        "https://openalex.org/W2963233387",
        "https://openalex.org/W3175945306",
        "https://openalex.org/W2343052201",
        "https://openalex.org/W3174598330",
        "https://openalex.org/W3035679705",
        "https://openalex.org/W3092894544",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W2294053032",
        "https://openalex.org/W3110267192",
        "https://openalex.org/W3003868038",
        "https://openalex.org/W3034447740",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2750938222",
        "https://openalex.org/W2746314669",
        "https://openalex.org/W2963712589",
        "https://openalex.org/W1720745709",
        "https://openalex.org/W2997558659",
        "https://openalex.org/W2751748110",
        "https://openalex.org/W2952285877",
        "https://openalex.org/W1981283549",
        "https://openalex.org/W2998382406",
        "https://openalex.org/W1491389626"
    ],
    "abstract": "In this paper, we abandon the dominant complex language model and rethink the linguistic learning process in the scene text recognition. Different from previous methods considering the visual and linguistic information in two separate structures, we propose a Visual Language Modeling Network (VisionLAN), which views the visual and linguistic information as a union by directly enduing the vision model with language capability. Specially, we introduce the text recognition of character-wise occluded feature maps in the training stage. Such operation guides the vision model to use not only the visual texture of characters, but also the linguistic information in visual context for recognition when the visual cues are confused (e.g. occlusion, noise, etc.). As the linguistic information is acquired along with visual features without the need of extra language model, VisionLAN significantly improves the speed by 39% and adaptively considers the linguistic information to enhance the visual features for accurate recognition. Furthermore, an Occlusion Scene Text (OST) dataset is proposed to evaluate the performance on the case of missing character-wise visual cues. The state of-the-art results on several benchmarks prove our effectiveness. Code and dataset are available at https://github.com/wangyuxin87/VisionLAN.",
    "full_text": "From Two to One: A New Scene Text Recognizer with\nVisual Language Modeling Network\nYuxin Wang1, Hongtao Xie1, Shancheng Fang1*, Jing Wang2, Shenggao Zhu2 and Yongdong Zhang1\n1University of Science and Technology of China\n2Huawei Cloud & AI\nwangyx58@mail.ustc.edu.cn, {htxie,fangsc,zhyd73}@ustc.edu.cn\n{wangjing105,zhushenggao}@huawei.com\nAbstract\nIn this paper, we abandon the dominant complex lan-\nguage model and rethink the linguistic learning process in\nthe scene text recognition. Different from previous meth-\nods considering the visual and linguistic information in two\nseparate structures, we propose a Visual Language Mod-\neling Network (VisionLAN), which views the visual and lin-\nguistic information as a union by directly enduing the vision\nmodel with language capability. Specially, we introduce the\ntext recognition of character-wise occluded feature maps in\nthe training stage. Such operation guides the vision model\nto use not only the visual texture of characters, but also the\nlinguistic information in visual context for recognition when\nthe visual cues are confused (e.g. occlusion, noise, etc.).\nAs the linguistic information is acquired along with visual\nfeatures without the need of extra language model, Vision-\nLAN signiﬁcantly improves the speed by 39% and adap-\ntively considers the linguistic information to enhance the\nvisual features for accurate recognition. Furthermore, an\nOcclusion Scene Text (OST) dataset is proposed to eval-\nuate the performance on the case of missing character-\nwise visual cues. The state of-the-art results on several\nbenchmarks prove our effectiveness. Code and dataset are\navailable at https://github.com/wangyuxin87/\nVisionLAN.\n1. Introduction\nAs a fundamental and pivotal task, scene text recog-\nnition (STR) aiming to read the text content from natu-\nral images has attracted great interest in computer vision\n[15, 31, 32, 42, 46]. By taking the text image as input\nand textual prediction as output, some early methods regard\nthe text recognition as a symbol classiﬁcation task [31, 19].\n*Corresponding author\nFigure 1. Comparison between previous methods and ours. Top\nleft: the architecture of previous methods. Top right: the ex-\ntra introduced computation cost for capturing linguistic informa-\ntion when the word length increases. Bottom: the proposed Vi-\nsionLAN endues the vision model with ability to initiatively cap-\nture the linguistic information in visual context during the training\nstage. In the testing stage, only the vision model is used for pre-\ndiction.\nHowever, it is hard to recognize images with confused vi-\nsual cues (e.g. occlusion, noise, etc.), which are beyond vi-\nsual discrimination. As the scene text image contains two-\nlevel contents: visual texture and linguistic information, in-\nspired by the Natural Language Processing (NLP) methods\n[23, 5], recent STR works have shifted their research fo-\ncus to acquiring linguistic information to assist recognition\n[47, 46, 28, 45]. Thus, the two-step architecture of vision\nand language models (top left of Fig. 1) is popular in recent\nmethods. Speciﬁcally, the vision model only focuses on vi-\nsual texture of characters without considering the linguistic\ninformation. Then, the language model predicts the rela-\ntionship between characters through the linguistic learning\nstructure (RNN [32], CNN [7] and Transformer [45]).\nThough these methods achieve promising results, there\nare still two problems: 1) the extra huge computation\ncost. The computation cost of language model increases\nsigniﬁcantly with the word length getting longer (linear\ngrowth for RNN [32]/ CNN [7] and quadratic growth for\narXiv:2108.09661v1  [cs.CV]  22 Aug 2021\nTransformer [45] in Fig. 1). Furthermore, many methods\nadopt a deep bi-directional reasoning architecture [38, 45,\n32] to capture more robust linguistic information, which\nfurther doubles the computation burden and greatly limits\ntheir efﬁciency in the real application. 2) The difﬁculty of\naggregating two independent information . It is difﬁcult\nto comprehensively consider and effectively fuse the visual\nand linguistic information from two separate structures for\naccurate recognition [7, 46]. In this paper, we attribute these\ntwo problems to the lack of language ability in the vision\nmodel, which only focuses on the visual texture of charac-\nters without initiatively learning linguistic information [45].\nAs shown in bottom of Fig. 1, inspired by the human cog-\nnitive process that the language capability can be acquired\n[21, 11], we use vision model as the basic network, and\nguide it to reason the occluded character during the training\nstage. Thus, vision model is trained to initiatively learn lin-\nguistic information in the visual context. In the test stage,\nvision model adaptively considers the linguistic information\nin the visual space for feature enhancement when the visual\ncues are confused ( e.g. occlusion, noise, etc.), which ef-\nfectively supplements the features of occluded characters,\nand correctly highlights the discriminating visual cues of\nconfused characters (shown in Fig. 5). To the best of our\nknowledge, this is the ﬁrst work to give vision model the\nability to perceive language in scene text recognition. We\ncall this new simple architecture as Visual Language Mod-\neling Network (VisionLAN).\nThe pipeline of VisionLAN is shown in Fig. 2. Vi-\nsionLAN contains three parts: backbone network, Masked\nLanguage-aware Module (MLM) and Visual Reasoning\nModule (VRM). In the training stage, visual features V\nare ﬁrstly extracted from the backbone network. Then\nMLM takes the visual features V and character index P\nas inputs, and automatically generates the character mask\nmap Maskc at corresponding position through a Weakly-\nsupervised Complementary Learning. MLM aims to simu-\nlate the case of missing character-wise visual cues by oc-\ncluding visual messages in V with Maskc. In order to\nconsider the linguistic information during the visual texture\nmodeling, we propose a VRM with the ability to capture\nlong-range dependencies in the visual space. VRM takes\nthe occluded feature mapVm as input, and is guided to make\nthe word-level prediction. In the test stage, we remove the\nMLM and only use VRM for recognition. As the linguistic\ninformation is acquired along with visual features without\nthe need of extra language model, VisionLAN introduces\nZERO computation cost for capturing linguistic informa-\ntion (top right in Fig. 1) and signiﬁcantly improves the\nspeed by 39% (Sec. 4.4). Compared with previous methods,\nthe proposed VisionLAN obtains more robust performance\non the occluded and low-quality images, and achieves new\nstate-of-the-art results on several benchmarks with a con-\ncise pipeline. In addition, an Occlusion Scene Text (OST)\ndataset is proposed to evaluate the performance on the case\nof missing character-wise visual cues.\nThe main contributions of this paper are as follows: 1) A\nnew simple architecture is proposed for scene text recogni-\ntion. We further visualize the feature maps to illustrate how\nVisionLAN initiatively uses linguistic information to handle\nthe confused visual cues (e.g. occluded, noise, etc.). 2) We\npropose a Weakly-supervised Complementary Learning to\ngenerate accurate character-wise mask map in MLM with\nonly word-level annotations. 3) A new Occlusion Scene\nText (OST) dataset is proposed to evaluate the recognition\nperformance of occluded images. Compared with previ-\nous methods, VisionLAN achieves the state-of-the-art per-\nformance on seven benchmarks (irregular and regular) and\nOST with a concise pipeline.\n2. Related Work\n2.1. Scene Text Recognition\nScene text recognition (STR) has been a long-term re-\nsearch topic in computer vision [42, 47, 7]. With deep learn-\ning becoming the most promising machine learning tool\n[35, 40, 41, 8, 17, 20], signiﬁcant progress has been made\nin the past few years for STR research [28, 24]. In this sec-\ntion, we divide these methods into two categories according\nto whether linguistic rules are used, namely language-free\nmethods and language-aware methods.\nLanguage-free methods [42, 48, 31, 19] view STR as a\nvisual classiﬁcation task and mainly rely on the visual infor-\nmation for prediction. CRNN [31] extracts sequential visual\nfeatures through combined CNN and RNN, then a Connec-\ntionist Temporal Classication (CTC) [9] decoder is used to\nmaximize the probability of all the paths for ﬁnal prediction.\nPatel et al. [27] automatically generate the custom lexicon\nfor an image to greatly boost the performance of text read-\ning systems. Zhang et al. [48] regard text recognition as\na visual matching task. They calculate the similarity map\nbetween visual features of input image and the pre-deﬁned\nalphabet to predict the text sequence. Liao et al. [18] re-\ngard the text recognition as a pixel-wise classiﬁcation task.\nSimilarly, Textscanner [36] further proposes an order map\nto ensure a more accurate transcription from characters to\nthe word. In general, the language-free methods ignore lin-\nguistic rules in the recognition process, which usually fail\nto recognize images with confused visual cues ( e.g. blur,\nocclusion, etc.).\nLanguage-aware methods [16, 4, 47, 43] try to leverage\nlinguistic rules to assist the recognition process. Lee et al.\n[15] use RNNs to automatically learn the sequential dy-\nnamics in word strings without manually deﬁning N-grams.\nAster [32] ﬁrstly uses a rectiﬁcation module before recog-\nnition, and then adopts RNNs to model the linguistic infor-\nmation by using the character predicted from the last time\nFigure 2. The pipeline of the proposed VisionLAN. VisionLAN mainly contains three parts: backbone network, Masked Language-aware\nModule (MLM) and Visual Reasoning Module (VRM). MLM is only used in training stage.\nstep. However, such serial and time-dependent operation\nin RNN limits the computation efﬁciency and the perfor-\nmance of semantic reasoning [45]. Thus, SRN [45] pro-\nposes a global semantic reasoning module based on trans-\nformer units [35] for pure language modeling, which takes\nthe prediction of vision model as input and predicts the rela-\ntionships among characters to reﬁne the recognition results.\nFang et al. [7] design a completely CNN-based architec-\nture for both vision and language modeling. Though these\nmethods achieve promising results on scene text recogni-\ntion task, the additionally introduced language model will\nsigniﬁcantly increase the computation cost. Furthermore,\nit is also difﬁcult to comprehensively consider and effec-\ntively fuse the independent visual and linguistic information\nin the two-step architecture for accurate recognition [7, 46].\nDifferent from previous methods considering the visual and\nlinguistic information in two separate structures, we directly\nendue the vision model with language ability and propose a\nVisionLAN to view the two information as a union. Thus, it\nis possible to enhance the confused visual cues by capturing\nlinguistic information in the visual context.\n2.2. Masking and Prediction.\nBERT [5] introduces a cloze task to mask the tokens\nof input sentence, which is used to learn a robust bi-\ndirectional representation based on the context. Follow-\ning [5], some works use a similar concept to handle the\nvision-and-language task [34, 1, 22]. ViLBERT [22] uses a\ntwo stream model to process visual and textual inputs, and\npre-trains their model through the two proxy tasks. Su et\nal. [34] propose a general structure to ﬁt for most visual-\nlinguistic downstream tasks, which takes both visual and\nlinguistic features as input. As the STR datasets are weakly\nlabeled with word-level annotations, it is difﬁcult to di-\nrectly implement these masking approaches in STR task.\nDifferent from these methods that mask in token or image\npatch level, in this paper, we propose a Weakly-supervised\nComplementary Learning to automatically mask the input\nimage in the feature level. Thus, VisionLAN learns lin-\nguistic information from a new perspective by guiding the\nmodel to make word-level prediction on the case of missing\ncharacter-wise visual cues.\n3. Proposed Method\nThe VisionLAN is an end-to-end trainable framework\nwith three parts containing: backbone network, Masked\nLanguage-aware Module (MLM) and Visual Reasoning\nModule (VRM). In this section, we ﬁrst detail the pipeline\nof proposed method in Sec. 3.1, and then we introduce\nMLM and VRM in Sec. 3.2 and Sec. 3.3 respectively.\n3.1. Pipeline\nThe pipeline of VisionLAN is shown in Fig. 2. In the\ntraining stage, given an input image, the 2D features V are\nﬁrstly extracted from backbone network. Then, MLM takes\nextracted features V and character index P as inputs, and\ngenerates the position-aware character mask map Maskc\nthrough the Weakly-Supervised Complementary Learning .\nMaskc is used to occlude the character-wise visual mes-\nsages in V to simulate the case of missing character-wise\nvisual semantics. After that, VRM takes occluded feature\nmap Vm as input and makes prediction under the complete\nword-level supervision. In the testing stage, we remove\nMLM and only use VRM for prediction.\n3.2. Masked Language-aware Module\nIn order to occlude the character-wise visual cues for\nthe guidance of linguistic learning, we propose a Masked\nLanguage-aware Module (MLM) to automatically generate\nthe character-wise mask map with only original word-level\nannotations.\nAs shown in Fig. 3, MLM takes visual featuresV and the\ncharacter index P as inputs. Character index P ∈[1,Nw]\nindicates the index of the occluded character, which is ran-\ndomly obtained for each input word image with length Nw.\nFigure 3. The architecture of MLM. MLM takes the visual features\nV and the character index P as inputs to automatically generate\ncharacter mask map Maskc. CE loss means the cross-entropy\nloss.\nThen the transformer unit [35] is used to improve the feature\nrepresentation ability. Finally, after integrating with charac-\nter index information, the character mask map Maskc is\nobtained through a sigmoid layer, which is used to generate\nthe occluded feature map Vm in Fig. 2.\nTo guide the learning process of Maskc, two paral-\nlel branches are designed based on the Weakly-supervised\nComplementary Learning (WCL). WCL aims to guide\nMaskc to cover more area of the occluded character, which\ncomplementarily makes 1 −Maskc contain more region\nof other characters. In the ﬁrst branch, we implement the\nelement-wise product between V and Maskc to generate\nthe feature map Vmas containing visual semantics of the\noccluded character (e.g. character “b” in the word “burns”\nwith character index 1 in Fig. 3). In contrast, the element-\nwise product between V and 1 −Maskc in the second\nbranch is used to generate the feature map Vrem contain-\ning visual semantics of other characters ( e.g. string “urns”\nin the word “burns” in Fig. 3). By doing these, the comple-\nmentary learning process guides the Maskc to only cover\nthe character at corresponding position without overlapping\nother characters (shown in Fig. 7). We share the weights of\ntransformer unit and prediction layer (Eq. 1) among two\nparallel branches for the feature representation enhance-\nment and semantic guidance. Vin ∈Rhw×c is the feature\nmap and Att∈Rhw×N is the attention map, wherec= 512\nis the channel number, N = 25is the max time step, hand\nware the height and width. Oc is positional encoding [35]\nof character orders. W1, W2, W3 are trainable weights and\ntis the time step.\npt = AttT\nt Vin (1)\nAtt= Softmax(G(Vin)) (2)\nG(Vin) =W1tanh(W2Oc + W3Vin) (3)\nFigure 4. The architecture of VRM. CE loss is cross-entropy loss.\nCompared with BERT [5], though both approaches mask\nout the information in a certain time step, the proposed\nMLM masks the visual features in the 2d spatial space in-\nstead of covering token-level information. Furthermore, as\nSTR datasets are weakly labeled, it is difﬁcult to obtain\nthe accurate character-wise pixel-level annotations. Thus,\nit is impractical to directly implement BERT-based meth-\nods [34, 1, 22] into STR task. Based on these, MLM helps\nthe model to learn linguistic information from a new per-\nspective, which can not be replaced by exiting masking ap-\nproaches.\nThe supervisions of WCL are automatically obtained by\nusing the original word-level annotation and randomly gen-\nerated character index (detailed in Sec. 4). Thus, MLM\nautomatically generates accurate character mask map with-\nout the need of additional annotations, making it possible\nfor the real application.\n3.3. Visual Reasoning Module\nDifferent from previous methods capturing the visual\nand linguistic information in the two-step architecture, we\npropose the Visual Reasoning Module (VRM) to model\nthe two information simultaneously in a uniﬁed structure.\nAs a pure vision-based structure, VRM aims to reason the\nword-level prediction from occluded features by using the\ncharacter-wise information in the visual context.\nThe details of VRM is shown in Fig. 4, it contains two\nparts: Visual Semantic Reasoning (VSR) layer and Parallel\nPrediction (PP) layer. VSR layer consists of N transformer\nunits [35], which are proved to be effective for modeling\nlong-range dependencies in recent computer vision tasks\n[2, 24]. Specially, position encoding is used to perceive the\npixel location information. Different from [45] using trans-\nformer units for pure language modeling, the transformer\nunits in the proposed VRM are used for sequence modeling,\nwhich will not be inﬂuenced by length of the word. Then,\nthe PP layer is designed to predict the characters in parallel,\nFigure 5. The visualization of features generated from VSR layer\nand the corresponding prediction result. Top: input image. Mid-\ndle: model implemented without MLM. Bottom: our VisionLAN.\nwhich has identical formulation as Eq. 1.\nIn order to achieve the language modeling process yi =\nf(yN ,...,y i+1,yi−1,...,y 1), the reasoning process of the\nith character yi needs to purely depend on the information\nof other characters. As MLM accurately occludes the char-\nacter information in the training stage, VSR layer is guided\nto predict the dependencies between visual features of char-\nacters to infer the semantics of occluded character. Thus,\nwith the word-level supervision, VSR layer learns to initia-\ntively model the linguistic information in visual context to\nassist recognition. In the testing stage, VSR layer is able\nto adaptively consider the linguistic information for visual\nfeature enhancement when the current visual semantics are\nconfused (e.g. occlusion, noise, etc.).\nWe visualize the feature maps generated from VSR layer\nin testing to better understand how the learned linguistic in-\nformation improves the recognition performance. As shown\nin Fig. 5, VSR layer effectively supplements the semantics\nof occluded character “r” in the word “better”, and correctly\nhighlights the discriminating visual cues of character “t”\nin the word “trans” with the help of linguistic information\nin visual context. Without the initiative linguistic learning\nguided by MLM, VRM wrongly predicts the input images\nas “bettep” and “rrans”.\n3.4. Training Objective\nThe ﬁnal objective function of the proposed method is\nformulated in Eq. 4. Lrec is loss in VRM, and Lmas &\nLrem are losses for predicting masked character and other\ncharacters in MLM respectively. λ1 and λ2 are used to bal-\nance the losses. Specially, we set λ1 = λ2 = 0.5, and use\ncross-entropy loss formulated in Eq. 5 for Lrec, Lmas and\nLrem. pt and gt represent the prediction and ground truth.\nWe set N to 25 in our experiments.\nL= Lrec + λ1Lmas + λ2Lrem (4)\nL∗ = −1\nN\nN∑\nt=1\nlog(pt|gt) (5)\n4. Experiment\n4.1. Datasets\nWe conduct experiments following the setup of [45]\nin the purpose of fair comparison. The training datasets\nare SynthText (ST) [10] and SynthText90K (90K) [12].\nThe performance is evaluated on 6 benchmarks contain-\ning IIIT 5K-Words (IIIT5K) [25], ICDAR2013 (IC13) [14],\nICDAR2015 (IC15) [13], Street View Text (SVT) [37],\nStreet View Text-Perspective (SVTP) [29] and CUTE80\n(CT) [30]. Details of above 6 datasets can be found in pre-\nvious works [45, 28].\nIn addition, we provide a new Occlusion Scene Text\n(OST) dataset to reﬂect the ability for recognizing cases\nwith missing visual cues. This dataset is collected from 6\nbenchmarks (IC13, IC15, IIIT5K, SVT, SVTP and CT) con-\ntaining 4832 images. Images in this dataset are manually\noccluded in weak or heavy degree (shown in Fig. 6). Weak\nand heavy degrees mean that we occlude the character using\none or two lines. For each image, we randomly choose one\ndegree to only cover one character. More examples of OST\nare shown in the supplementary materials.\n4.2. Implementation Details\nWe use the ResNet45 [32, 38, 28] as our backbone. Par-\nticularly, we set the stride to 2 in stage 2,3,4 and initialize\nthe weights by default. Following the most recent works\n[45, 28], we set the image size to 256 ×64 (there is no\nobvious difference with the size of 128 ×32 in our ex-\nperiments). Data augmentation including random rotation,\ncolor jittering and perspective distortion. We conduct the\nexperiments on 4 NVIDIA V100 GPUs with batch size 384.\nThe network is trained end-to-end using Adam optimizer\nwith learning rate 1e-4. The recognition covers 37 charac-\nters including a-z, 0-9, and an end-of-sequence symbol.\nFollowing [45], we divide the training process into 2\nsteps: language-free (LF) step and language-aware (LA)\nstep. It is worth mentioning that we control the total number\nof training sessions to be consistent with existing methods\nfor fair comparison. 1) In LF step, we split the connection\nbetween MLM and VRM (V = Vm in Fig. 2) to guarantee\na more stable learning process of both modules. VRM in\nthis step will not acquire the language capability and only\nuses visual texture for prediction. 2) In LA step, Maskc\ngenerated from MLM is used to occlude the feature map V\nto guide the learning of linguistic rules in VRM. Speciﬁ-\ncally, we control the ratio of occluded number in a batch,\nwhich aims to balance the cases with rich or weak visual\ninformation during the training stage.\nAs all the training images have word-level annotations,\nwe randomly generate the character index based on the\nlength of word, and use this index and the original word-\nlevel annotation to generate the labels for MLM (e.g. when\nindex is 4 and word is “house”, the labels are “s” and “houe”\nrespectively). The label generating process is automatic\nwithout manual intervention, making it easy to ﬁnetune our\nmodel on other datasets.\nTable 1. Ablation study about the occluded number ratio of one\nbatch in the MLM during the training stage.\nRatio IIIT5K IC13 SVT IC15 SVTP CT\nBaseline 94.5 94.2 89.3 79.8 81.1 85.8\n1:2 95.0 94.8 90.4 80.8 83.0 88.0\n1:1 95.4 95.0 91.0 81.8 83.7 88.2\n2:1 95.0 94.7 90.0 81.1 82.7 88.1\nTable 2. Ablation study of the WCL. “Mas only” and “Rem only”\nmeans that we only implement the 1st or the 2nd branch in MLM.\nMethods IIIT5K IC13 SVT IC15 SVTP CT\nMas only 94.8 94.7 89.8 81.7 82.3 87.2\nRem only 95.2 94.8 89.9 81.1 82.2 88.0\nWCL 95.4 95.0 91.0 81.8 83.7 88.2\nTable 3. The comparisons between MLM and other masking meth-\nods. Average accuracy is calculated from 6 benchmarks. We set\ncutout patch to h×w/10 and dropout value to 0.1. The results are\ncompared under the same training sessions.\nMethods Average accuracy(%)\nBaseline 88.8\nDropout [33] 89.0\nCutout [6] 89.0\nMLM 90.2\nTable 4. Ablation study about the ability of linguistic information\ncapturing in VRM. “2L” means two transformer units are used.\nMethods IIIT5K IC13 SVT IC15 SVTP CT\nVRM-2L 95.4 95.0 91.0 81.8 83.7 88.2\nVRM-3L 95.8 95.7 91.7 83.7 86.0 88.5\n4.3. Ablation Study\nWe illustrate the effectiveness of proposed modules in\nthis section. To be speciﬁc, baseline contains VRM with\ntwo transformer units in Tab. 1& 2& 3.\nThe effectiveness of MLM. The proposed MLM aims to\nguide the linguistic learning process in the VRM. We con-\nduct several experiments to evaluate its effectiveness in Tab.\n1. The baseline model is implemented without MLM. We\nchange the ratio of occluded number in a batch to study\nits inﬂuence to the recognition performance ( e.g. when the\nbatch size is 128, ratio= 1:3 means that we use Maskc to\nocclude V for only 32 samples in 1 batch, and feature maps\nof the rest 96 samples remain unchanged). As shown in Tab.\n1, the proposed MLM signiﬁcantly improves the perfor-\nmance of baseline model when the ratio ranges from 1:2 to\n2:1. For the irregular datasets (IC15, SVTP, CT) containing\namounts of images with confused visual cues ( blur, occlu-\nsion, noise, etc.), the proposed MLM improves the baseline\nmodel at least 2% in accuracy with ratio =1:1, which fur-\nther demonstrates that the initiative linguistic learning pro-\ncess effectively helps the vision model to handle confused\nvisual cues. For regular datasets, the improvement is also\nconsiderable (0.9%, 0.8%, and 1.7% on IIIT5K, IC13 and\nSVT datasets respectively). When the ratio raises up to 2:1,\nthe performance drops slightly. We infer that the large value\nof ratio will break the balance between cases with rich and\nweak visual cues during the training process. Therefore, we\nset the value of ratio to 1:1 in the rest experiments.\nThe effectiveness of WCL. To demonstrate the effec-\ntiveness of proposed Weakly-supervised Complementary\nLearning in MLM, we conduct several experiments imple-\nmented with only the ﬁrst branch (occluded character) or\nthe second branch (remaining string). As shown in Tab. 2,\nMLM implemented with the complementary learning pro-\ncess obtains better results than the methods only guiding the\nsemantics of occluded character or remaining string during\nthe training stage.\nCompared with other masking methods. We compare\nMLM with [6, 33] to evaluate our effectiveness in language\nmodeling. All the modules only work on V for fair com-\nparison. As shown in Tab. 3, the proposed MLM signiﬁ-\ncantly improves the recognition results (1.4% vs 0.2%). As\ndetailed in Sec. 3.3, the reasoning process of the ith char-\nacter needs to purely depend on the information of other\ncharacters without containing current character-wise infor-\nmation. Thus, randomly masking pixel-wise feature [6, 33]\ndoes not have the ability of linguistic learning. Beneﬁting\nfrom the well-designed architecture and ingenious weakly\nsupervised learning, MLM accurately localizes character-\nwise visual cues, which has the ability to guide the linguistic\nlearning process in VRM.\nThe effectiveness of VRM. To study the relationship be-\ntween the recognition performance and the ability of captur-\ning linguistic information, we compare the results of models\nimplemented with different number of transformer units in\nVSR layer. As shown in Tab. 4, VRM implemented with\nthree transformer units further improves the performance,\nwhich has the stronger language capability.\n4.4. Comparisons with State-of-the-Arts\nWe compare our method with previous state-of-the-art\nmethods on 6 benchmarks in Tab. 5. We simply divide the\nmethods into language-free and language-aware methods\naccording to whether linguistic information are used. The\nlanguage-aware methods perform better than language-free\nmethods in general. Beneﬁting from adaptively consider-\ning the linguistic information for feature enhancement, the\nproposed VisionLAN achieves state-of-the-art performance\nacross the 6 public datasets compared with both language-\nfree and language-aware methods. Speciﬁcally, for regu-\nlar datasets, the proposed VisionLAN obtains 1%, 0.2%\nand 0.2% improvement on IIIT5K, IC13 and SVT datasets\nrespectively. For irregular datasets, the increases are 1%,\n0.9% and 0.7% on IC15, SVTP and CT respectively.\nTable 5. Results on IIIT5K, IC13, SVT, IC15, SVTP and CUTE datasets. Following [28, 45], all the results are under NONE lexicon.\nLan-free and Lan-aware are shorts for language-free and language-aware methods. “Annos” is short for annotations. “char” and “word”\nmean character-level and word-level annotations are used in the training stage. Baseline contains VRM with three transformer units.\nMethods Training Data Annos IIIT5K IC13 SVT IC15 SVTP CT\nLan-free\nCTC [31] 90K word 81.2 89.6 82.7 - - -\nACE [42] 90K word 82.3 89.7 82.6 68.9 70.1 82.6\nFCN [19] ST word, char 91.9 91.5 86.4 - - -\nLan-aware\nFAN [3] 90K+ST word 87.4 93.3 85.9 70.6 - -\nAON [4] 90K+ST word 87.0 - 82.8 68.2 73.0 76.8\nASTER [32] 90K+ST word 93.4 91.8 89.5 76.1 78.5 79.5\nESIR [47] 90K+ST word 93.3 91.3 90.2 76.9 79.6 83.3\nScRN [43] 90K+ST word, char 94.4 93.9 88.9 78.7 80.8 87.5\nSAR [16] 90K+ST word 91.5 91.0 84.5 69.2 76.4 83.3\nTextScanner [36] 90K+ST word, char 83.9 92.9 90.1 79.4 84.3 83.3\nDAN [38] 90K+ST word 94.3 93.9 89.2 74.5 80.0 84.4\nWang et al. [39] 90K+ST word 94.4 93.7 89.8 75.1 80.2 86.8\nSRN [45] 90K+ST word 94.8 95.5 91.5 82.7 85.1 87.8\nSEED [28] 90K+ST word 93.8 92.8 89.6 80.0 81.4 83.6\nOurs Baseline 90K+ST word 94.6 94.3 89.3 81.2 81.6 86.8\nVisionLAN 90K+ST word 95.8 95.7 91.7 83.7 86.0 88.5\nTable 6. The comparisons of speed and EIPs between existing lan-\nguage models and ours. The test dataset is IC15.\nMethods Speed EIPs\nBaseline 11.5ms -\nBaseline + [32] 43.2ms 3.0M\nBaseline + [45] 19ms 12.6M\nVisionLAN 11.5ms 0M\nFigure 6. The examples of text images in OST. Top: image oc-\ncluded in weak degree. Bottom: image occluded in heavy degree.\nLeft: original image. Right: occluded image.\nTable 7. Results on OST. Average is the short for average accuracy.\nWeak and Heavy mean the accuracy on weak and heavy degrees.\nMethods Average Weak Heavy\nBaseline 53.0 63.2 42.7\nBaseline + [32] 53.9 63.9 43.9\nBaseline + [45] 58.2 68.4 48.0\nVisionLAN 60.3 70.3 50.3\nAs VisionLAN adaptively considers the visual and lin-\nguistic information in the 2d visual space, our method is\nless sensitive to the distorted images. Thus, the proposed\nmethod can obtain better results than ASTER [32] and ESIR\n[47] on irregular datasets, which adopt the rectiﬁcation pro-\ncess before recognition. As shown in Tab. 5, the increases\nare 7.6%, 7.5% and 9% for [32], and 6.8%, 6.4% and 5.2%\nfor [47] on IC15, SVTP and CT datasets respectively.\nWe further compare the differences between the existing\nmethods and ours in recognition speed and the extra intro-\nduced parameters (EIPs) for capturing linguistic informa-\ntion in Tab. 6. In terms of approaching speed and parame-\nters, we implement one transformer unit in GSRM of [45]\n(the same goes for Sec. 4.5). As the linguistic information\nis acquired along with the visual features without the need\nof extra language model, the proposed VisionLAN signiﬁ-\ncantly improves the speed by at least 39% (11.5ms vs 19ms\nand 43.2ms) without introducing extra parameters (0M vs\n12.6M and 3M). Furthermore, as VisionLAN directly con-\nsiders the linguistic information in the visual space, its ef-\nﬁciency of capturing linguistic information will not be af-\nfected by the word length.\n4.5. The Language Capability on OST Dataset\nTo evaluate the language capability of our VisionLAN\nin detail, we compare our method with recent most popular\nlanguage models (RNN [32] and Transformer [45]) on OST\ndataset to evaluate their performance on the case of missing\ncharacter-wise visual cues. Speciﬁcally, we connect these\nlanguage models to VRM following the implementation de-\ntails in their papers. As shown in Tab. 7, though the lin-\nguistic information captured by [32] and [45] can assist the\nprediction of vision model, the proposed VisionLAN sig-\nniﬁcantly outperforms these methods by viewing the visual\nand linguistic information as a union. Through adaptively\naggregating the two information in a uniﬁed structure in-\nstead of considering them independently, VisionLAN im-\nproves the baseline model by 7.3% in average.\n4.6. The Generalization Ability on Long Chinese\nDataset\nWe evaluate VisionLAN on non-Latin Long Text\n(TRW15 [49]) to prove its generalization ability. This\ndataset contains 2997 cropped images, and we set the max\nlength N to 50. We train the proposed VisionLAN follow-\nTable 8. Results on TRW15 [49].\nMethods Accuracy(%)\nSCCM [44] 81.2\n2D-Attention [45] 72.2\nCTC [45] 73.8\nSRN [45] 85.5\nVisionLAN 88.7\ning the setup of [45]. As shown in Tab. 8, compared with\nlanguage-free (CTC) and language-aware (2D-Attention)\nmethods, VisionLAN outperforms these approaches by at\nleast 14.9%. Beneﬁting from viewing the visual and lin-\nguistic information as a union, the proposed VisionLAN\nachieves a new state-of-the-art result and signiﬁcantly out-\nperforms SRN [45] by 3.2%. More experiments on other\ndatasets (e.g. MLT [26], etc.) are available in the supple-\nmentaries.\n4.7. The Qualitative Analysis\nMLM in character-wise localization. To qualitatively an-\nalyze the effectiveness of MLM, we visualize some ex-\namples of generated Maskc in Fig. 7. The generated\nMaskc effectively localizes character-wise visual cues at\ncorresponding position with the guidance of character in-\ndex P. Furthermore, MLM is able to handle the distorted\nimages (e.g. the curved word image “nothing”) and the lo-\ncalization of repeated characters (e.g. the character “b” with\nP = 6in word “confabbing”). The quantitative evaluation\nof character-wise localization performance and more visu-\nalizations of Maskc are available in the supplementaries.\nThe effectiveness of VisionLAN. We collect some recog-\nnition results to illustrate how the learned linguistic infor-\nmation helps vision model to improve the performance. As\nshown in Fig. 8 (a), VisionLAN can handle the cases with\nconfusing characters. For example, as the character “e”\nhas the similar visual cues to character “f” in the image\nwith word “before”, the VisionLAN without MLM wrongly\ngives the prediction “f”, while VisionLAN correctly infers\nthe character “e” with the help of linguistic information. For\nthe samples in Fig. 8 (b), VisionLAN can also use linguis-\ntic rules to eliminate the background interference (including\nocclusion, illumination, background textures, etc.). Further-\nmore, the accurate recognition of the blurred characters in\nFig. 8 (c) also demonstrates the effectiveness of our method.\n5. Conclusion\nAs the ﬁrst work to endue the vision model with lan-\nguage capability, this paper proposes a concise and effective\narchitecture for scene text recognition. VisionLAN success-\nfully achieves the transformation from two-step to one-step\nrecognition (from Two to One), which adaptively considers\nboth visual and linguistic information in a uniﬁed structure\nwithout the need of extra language model. Compared with\nFigure 7. The examples of generated Maskc. The top image is\ninput image, and the bottom image is Maskc with corresponding\ncharacter index P.\nFigure 8. Recognition results with/without the use of linguistic in-\nformation. The top string is the prediction of VisionLAN with-\nout MLM. The bottom string is the prediction of VisionLAN. (a):\ncharacters with confused visual cues. (b): characters disturbed by\nbackground. (c): blurred characters.\nprevious language model, VisionLAN shows a stronger lan-\nguage capability while maintaining high efﬁciency. In ad-\ndition, a new Occlusion Scene Text dataset is proposed to\nevaluate the performance on the cases of missing character-\nwise visual cues. Extensive experiments on seven bench-\nmarks and the proposed OST dataset demonstrate the effec-\ntiveness and efﬁciency of our method. We regard the pro-\nposed VisionLAN as a basic step toward more robust and\naccurate scene text recognition, and we will further explore\nits potential in the future.\nAcknowledgments\nThis work is supported by the National Nature Science\nFoundation of China (62121002, 62022076, U1936210),\nthe Fundamental Research Funds for the Central Univer-\nsities under Grant WK3480000011, the China Postdoctoral\nScience Foundation (2021M693092) and JD AI research.\nWe also acknowledge the support of GPU cluster built by\nMCC Lab of Information Science and Technology Institu-\ntion, USTC.\nReferences\n[1] Chris Alberti, Jeffrey Ling, Michael Collins, and David Re-\nitter. Fusion of detected objects in text for visual question\nanswering. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2131–2140, 2019.\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. arXiv preprint\narXiv:2005.12872, 2020.\n[3] Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang\nPu, and Shuigeng Zhou. Focusing attention: Towards ac-\ncurate text recognition in natural images. In Proceedings of\nthe IEEE international conference on computer vision, pages\n5076–5084, 2017.\n[4] Zhanzhan Cheng, Yangliu Xu, Fan Bai, Yi Niu, Shiliang\nPu, and Shuigeng Zhou. Aon: Towards arbitrarily-oriented\ntext recognition. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 5571–\n5579, 2018.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n[6] Terrance DeVries and Graham W Taylor. Improved regular-\nization of convolutional neural networks with cutout. arXiv\npreprint arXiv:1708.04552, 2017.\n[7] Shancheng Fang, Hongtao Xie, Zheng-Jun Zha, Nannan Sun,\nJianlong Tan, and Yongdong Zhang. Attention and language\nensemble for scene text recognition with convolutional se-\nquence modeling. In Proceedings of the 26th ACM interna-\ntional conference on Multimedia, pages 248–256, 2018.\n[8] Jiannan Ge, Hongtao Xie, Shaobo Min, and Yongdong\nZhang. Semantic-guided reinforced region embedding for\ngeneralized zero-shot learning. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , volume 35, pages\n1406–1414, 2021.\n[9] Alex Graves, Santiago Fern ´andez, Faustino Gomez, and\nJ¨urgen Schmidhuber. Connectionist temporal classiﬁcation:\nlabelling unsegmented sequence data with recurrent neural\nnetworks. In Proceedings of the 23rd international confer-\nence on Machine learning, pages 369–376, 2006.\n[10] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman.\nSynthetic data for text localisation in natural images. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 2315–2324, 2016.\n[11] Catherine L Harris. Language and cognition. Encyclopedia\nof cognitive science, pages 1–6, 2006.\n[12] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and An-\ndrew Zisserman. Synthetic data and artiﬁcial neural net-\nworks for natural scene text recognition. NIPS, 2014.\n[13] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos\nNicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwa-\nmura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chan-\ndrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust\nreading. In 2015 13th International Conference on Docu-\nment Analysis and Recognition (ICDAR), pages 1156–1160.\nIEEE, 2015.\n[14] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida,\nMasakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles\nMestre, Joan Mas, David Fernandez Mota, Jon Almazan Al-\nmazan, and Lluis Pere De Las Heras. Icdar 2013 robust\nreading competition. In 2013 12th International Conference\non Document Analysis and Recognition , pages 1484–1493.\nIEEE, 2013.\n[15] Chen-Yu Lee and Simon Osindero. Recursive recurrent nets\nwith attention modeling for ocr in the wild. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2231–2239, 2016.\n[16] Hui Li, Peng Wang, Chunhua Shen, and Guyu Zhang. Show,\nattend and read: A simple and strong baseline for irregular\ntext recognition. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 33, pages 8610–8617, 2019.\n[17] Jiaming Li, Hongtao Xie, Jiahong Li, Zhongyuan Wang, and\nYongdong Zhang. Frequency-aware discriminative feature\nlearning supervised by single-center loss for face forgery\ndetection. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 6458–\n6467, 2021.\n[18] Minghui Liao, Pengyuan Lyu, Minghang He, Cong Yao,\nWenhao Wu, and Xiang Bai. Mask textspotter: An end-to-\nend trainable neural network for spotting text with arbitrary\nshapes. IEEE transactions on pattern analysis and machine\nintelligence, 2019.\n[19] Minghui Liao, Jian Zhang, Zhaoyi Wan, Fengming Xie, Jia-\njun Liang, Pengyuan Lyu, Cong Yao, and Xiang Bai. Scene\ntext recognition from two-dimensional perspective. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 33, pages 8714–8721, 2019.\n[20] Fanchao Lin, Hongtao Xie, Yan Li, and Yongdong Zhang.\nQuery-memory re-aggregation for weakly-supervised video\nobject segmentation. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, volume 35, pages 2038–2046,\n2021.\n[21] John L Locke. Why do infants begin to talk? language\nas an unintended consequence. Journal of child language ,\n23(2):251–268, 1996.\n[22] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vil-\nbert: Pretraining task-agnostic visiolinguistic representations\nfor vision-and-language tasks. In Advances in Neural Infor-\nmation Processing Systems, pages 13–23, 2019.\n[23] Minh-Thang Luong, Hieu Pham, and Christopher D Man-\nning. Effective approaches to attention-based neural ma-\nchine translation. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing , pages\n1412–1421, 2015.\n[24] Pengyuan Lyu, Zhicheng Yang, Xinhang Leng, Xiaojun Wu,\nRuiyu Li, and Xiaoyong Shen. 2d attentional irregular scene\ntext recognizer. arXiv preprint arXiv:1906.05708, 2019.\n[25] Anand Mishra, Karteek Alahari, and CV Jawahar. Scene text\nrecognition using higher order language priors. In BMVC,\n2012.\n[26] Nibal Nayef, Yash Patel, Michal Busta, Pinaki Nath Chowd-\nhury, Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Uma-\npada Pal, Jean-Christophe Burie, Cheng-lin Liu, et al. Ic-\ndar2019 robust reading challenge on multi-lingual scene text\ndetection and recognition—rrc-mlt-2019. In 2019 Interna-\ntional Conference on Document Analysis and Recognition\n(ICDAR), pages 1582–1587. IEEE, 2019.\n[27] Yash Patel, Lluis Gomez, Marc ¸al Rusinol, and Dimosthenis\nKaratzas. Dynamic lexicon generation for natural scene im-\nages. In European Conference on Computer Vision , pages\n395–410. Springer, 2016.\n[28] Zhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, and Weip-\ning Wang. Seed: Semantics enhanced encoder-decoder\nframework for scene text recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13528–13537, 2020.\n[29] Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan\nTian, and Chew Lim Tan. Recognizing text with perspective\ndistortion in natural scenes. In Proceedings of the IEEE In-\nternational Conference on Computer Vision, pages 569–576,\n2013.\n[30] Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng\nChan, and Chew Lim Tan. A robust arbitrary text detection\nsystem for natural scene images. Expert Systems with Appli-\ncations, 41(18):8027–8048, 2014.\n[31] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end\ntrainable neural network for image-based sequence recog-\nnition and its application to scene text recognition. IEEE\ntransactions on pattern analysis and machine intelligence ,\n39(11):2298–2304, 2016.\n[32] Baoguang Shi, Mingkun Yang, Xinggang Wang, Pengyuan\nLyu, Cong Yao, and Xiang Bai. Aster: An attentional scene\ntext recognizer with ﬂexible rectiﬁcation. IEEE transactions\non pattern analysis and machine intelligence , 41(9):2035–\n2048, 2018.\n[33] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: a simple way\nto prevent neural networks from overﬁtting. The journal of\nmachine learning research, 15(1):1929–1958, 2014.\n[34] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-\nlinguistic representations. In International Conference on\nLearning Representations, 2019.\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017.\n[36] Zhaoyi Wan, Mingling He, Haoran Chen, Xiang Bai,\nand Cong Yao. Textscanner: Reading characters in or-\nder for robust scene text recognition. arXiv preprint\narXiv:1912.12422, 2019.\n[37] Kai Wang, Boris Babenko, and Serge Belongie. End-to-end\nscene text recognition. In 2011 International Conference on\nComputer Vision, pages 1457–1464. IEEE, 2011.\n[38] Tianwei Wang, Yuanzhi Zhu, Lianwen Jin, Canjie Luo, Xi-\naoxue Chen, Yaqiang Wu, Qianying Wang, and Mingxiang\nCai. Decoupled attention network for text recognition. In\nAAAI, pages 12216–12224, 2020.\n[39] Yizhi Wang and Zhouhui Lian. Exploring font-independent\nfeatures for scene text recognition. ECCV, 2020.\n[40] Yuxin Wang, Hongtao Xie, Zilong Fu, and Yongdong Zhang.\nDsrn: A deep scale relationship network for scene text detec-\ntion. In IJCAI, pages 947–953, 2019.\n[41] Yuxin Wang, Hongtao Xie, Zheng-Jun Zha, Mengting Xing,\nZilong Fu, and Yongdong Zhang. Contournet: Taking a fur-\nther step toward accurate arbitrary-shaped scene text detec-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 11753–11762,\n2020.\n[42] Zecheng Xie, Yaoxiong Huang, Yuanzhi Zhu, Lianwen Jin,\nYuliang Liu, and Lele Xie. Aggregation cross-entropy for se-\nquence recognition. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 6538–\n6547, 2019.\n[43] Mingkun Yang, Yushuo Guan, Minghui Liao, Xin He,\nKaigui Bian, Song Bai, Cong Yao, and Xiang Bai.\nSymmetry-constrained rectiﬁcation network for scene text\nrecognition. In Proceedings of the IEEE International Con-\nference on Computer Vision, pages 9147–9156, 2019.\n[44] Fei Yin, Yi-Chao Wu, Xu-Yao Zhang, and Cheng-Lin Liu.\nScene text recognition with sliding convolutional character\nmodels. arXiv preprint arXiv:1709.01727, 2017.\n[45] Deli Yu, Xuan Li, Chengquan Zhang, Tao Liu, Junyu Han,\nJingtuo Liu, and Errui Ding. Towards accurate scene text\nrecognition with semantic reasoning networks. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12113–12122, 2020.\n[46] Xiaoyu Yue, Zhanghui Kuang, Chenhao Lin, Hongbin Sun,\nand Wayne Zhang. Robustscanner: Dynamically enhancing\npositional clues for robust text recognition. eccv, 2020.\n[47] Fangneng Zhan and Shijian Lu. Esir: End-to-end scene text\nrecognition via iterative image rectiﬁcation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2059–2068, 2019.\n[48] Chuhan Zhang, Ankush Gupta, and Andrew Zisserman.\nAdaptive text recognition through visual matching. ECCV,\n2020.\n[49] Xinyu Zhou, Shuchang Zhou, Cong Yao, Zhimin Cao, and\nQi Yin. Icdar 2015 text reading in the wild competition.\narXiv preprint arXiv:1506.03184, 2015."
}