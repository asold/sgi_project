{
  "title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
  "url": "https://openalex.org/W2983102021",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2222342641",
      "name": "Wang, Xiaozhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2230107535",
      "name": "Gao Tianyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1894944974",
      "name": "Zhu Zhao-cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2348152382",
      "name": "Zhang Zhengyan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1860873322",
      "name": "Liu Zhi-yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2057730499",
      "name": "Li Juanzi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1930248910",
      "name": "Tang Jian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963563735",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2952826391",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2158028897",
    "https://openalex.org/W2963406064",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3104748221",
    "https://openalex.org/W2963777632",
    "https://openalex.org/W2962891712",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2803609569",
    "https://openalex.org/W2964282455",
    "https://openalex.org/W2788798739",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2891328459",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962767366",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W2912516411",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963041663",
    "https://openalex.org/W2951048068",
    "https://openalex.org/W2909137510",
    "https://openalex.org/W2798465082",
    "https://openalex.org/W2963571857",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2997012196",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W2741075451",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2977745385",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3103296573",
    "https://openalex.org/W2184957013",
    "https://openalex.org/W2963789888",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2739945392",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W2963432357",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W2499696929",
    "https://openalex.org/W2963855739",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2250635077",
    "https://openalex.org/W2567619939",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2962850650",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W2970476646"
  ],
  "abstract": "Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M, a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/THU-KEG/KEPLER.",
  "full_text": "KEPLER: A Uniﬁed Model for Knowledge Embedding and\nPre-trained Language Representation\nXiaozhi Wang1, Tianyu Gao3, Zhaocheng Zhu4,5, Zhengyan Zhang1,\nZhiyuan Liu1,2∗, Juanzi Li1,2, Jian Tang4,6,7∗\n1Department of CST, BNRist; 2KIRC, Institute for AI, Tsinghua University, Beijing, China\n{wangxz20,zy-z19}@mails.tsinghua.edu.cn\n{liuzy,lijuanzi}@tsinghua.edu.cn\n3Department of Computer Science, Princeton University, Princeton, USA\ntianyug@princeton.edu\n4Mila - Québec AI Institute; 5Univesité de Montréal; 6HEC, Montréal, Canada\nzhaocheng.zhu@umontreal.ca, jian.tang@hec.ca\n7CIFAR AI Research Chair\nAbstract\nPre-trained language representation models\n(PLMs) cannot well capture factual knowl-\nedge from text. In contrast, knowledge\nembedding (KE) methods can effectively\nrepresent the relational facts in knowledge\ngraphs (KGs) with informative entity embed-\ndings, but conventional KE models cannot\ntake full advantage of the abundant textual\ninformation. In this paper, we propose a\nuniﬁed model for Knowledge Embedding\nand Pre-trained LanguagE Representation\n(KEPLER), which can not only better in-\ntegrate factual knowledge into PLMs but\nalso produce effective text-enhanced KE\nwith the strong PLMs. In KEPLER, we en-\ncode textual entity descriptions with a PLM\nas their embeddings, and then jointly opti-\nmize the KE and language modeling objec-\ntives. Experimental results show that KE-\nPLER achieves state-of-the-art performances\non various NLP tasks, and also works re-\nmarkably well as an inductive KE model on\nKG link prediction. Furthermore, for pre-\ntraining and evaluating KEPLER, we con-\nstruct Wikidata5M1, a large-scale KG dataset\nwith aligned entity descriptions, and bench-\nmark state-of-the-art KE methods on it. It\nshall serve as a new KE benchmark and facil-\nitate the research on large KG, inductive KE,\nand KG with text. The source code can be\nobtained from https://github.com/\nTHU-KEG/KEPLER.\n1 Introduction\nRecent pre-trained language representation models\n(PLMs) such as BERT (Devlin et al., 2019) and\n∗ Correspondence to: Z. Liu and J. Tang\n1https://deepgraphlearning.github.\nio/project/wikidata5m\nJohannes Kepler was a German \nastronomer … best known for \nhis laws of planetary motion.\nGermany is a country in Central \nand Western Europe …\nAstronomer\nAn astronomer is a scientist in \nthe ﬁeld of astronomy …\n… are three scientiﬁc laws describing \nthe motion of planets around the Sun, \npublished by Johannes Kepler.\n… is an independent agency … \nfor the civilian space program …\nlaunched by NASA … Named \nafter Johannes Kepler.\nNamed after\nEthnic group\nOccupation\nOperator\nPublished by\nKepler's laws\nKepler space telescopeJohannes Kepler\nNASA\nGerman\nFigure 1: An example of a KG with entity descrip-\ntions. The ﬁgure suggests that descriptions contain\nabundant information about entities and can help\nto represent the relational facts between them.\nRoBERTa (Liu et al., 2019c) learn effective lan-\nguage representation from large-scale unstructured\ncorpora with language modeling objectives and\nhave achieved superior performances on various\nnatural language processing (NLP) tasks. Exist-\ning PLMs learn useful linguistic knowledge from\nunlabeled text (Liu et al., 2019a), but they gen-\nerally cannot well capture the world facts, which\nare typically sparse and have complex forms in\ntext (Petroni et al., 2019; Logan et al., 2019).\nBy contrast, knowledge graphs (KGs) contain\nextensive structural facts, and knowledge embed-\nding (KE) methods (Bordes et al., 2013; Yang et al.,\n2015; Sun et al., 2019) can effectively embed them\ninto continuous entity and relation embeddings.\nThese embeddings can not only help with the KG\ncompletion but also beneﬁt various NLP applica-\ntions (Yang and Mitchell, 2017; Zaremoodi et al.,\n2018; Han et al., 2018a). As shown in Figure 1,\ntextual entity descriptions contain abundant infor-\nmation. Intuitively, KE methods can provide fac-\ntual knowledge for PLMs, while the informative\ntext data can also beneﬁt KE.\narXiv:1911.06136v3  [cs.CL]  23 Nov 2020\nInspired by Xie et al. (2016), we use entity\ndescriptions to bridge the gap between KE and\nPLM, and align the semantic space of text to\nthe symbol space of KGs (Logeswaran et al.,\n2019). We propose KEPLER, a uniﬁed model for\nKnowledge Embedding and Pre-trained LanguagE\nRepresentation. We encode the texts and entities\ninto a uniﬁed semantic space with the same PLM\nas the encoder, and jointly optimize the KE and the\nmasked language modeling (MLM) objectives. For\nthe KE objective, we encode the entity descriptions\nas entity embeddings and then train them in the\nsame way as conventional KE methods. For the\nMLM objective, we follow the approach of exist-\ning PLMs (Devlin et al., 2019; Liu et al., 2019c).\nKEPLER has the following strengths:\nAs a PLM, (1) KEPLER is able to integrate fac-\ntual knowledge into language representation with\nthe supervision from KG by the KE objective. (2)\nKEPLER inherits the strong ability of language\nunderstanding from PLMs by the MLM objective.\n(3) The KE objective enhances the ability of KE-\nPLER to extract knowledge from text since it re-\nquires the model to encode the entities from their\ncorresponding descriptions. (4) KEPLER can be\ndirectly adopted in a wide range of NLP tasks with-\nout additional inference overhead compared to con-\nventional PLMs since we just add new training\nobjectives without modifying model structures.\nThere are also some recent works (Zhang et al.,\n2019; Peters et al., 2019; Liu et al., 2020) directly\nintegrating ﬁxed entity embeddings into PLMs to\nprovide external knowledge. However, (1) their\nentity embeddings are learned by a separate KE\nmodel, and thus cannot be easily aligned with the\nlanguage representation space. (2) They require an\nentity linker to link the text to the corresponding\nentities, making them suffer from the error propaga-\ntion problem. (3) Compared to vanilla PLMs, their\nsophisticated mechanisms to link and use entity\nembeddings lead to additional inference overhead.\nAs a KE model, (1) KEPLER can take full ad-\nvantage of the abundant information from entity\ndescriptions with the help of the MLM objective.\n(2) KEPLER is capable of performing KE in the\ninductive setting, i.e., it can produce embeddings\nfor unseen entities from their descriptions, while\nconventional KE methods are inherently transduc-\ntive and they can only learn representations for the\nshown entities during training. Inductive KE is\nessential for many real-world applications, such\nas updating KGs with emerging entities and KG\nconstruction, and thus is worth more investigation.\nFor pre-training and evaluating KEPLER, we\nneed a KG with (1) large amounts of knowledge\nfacts, (2) aligned entity descriptions, and (3) rea-\nsonable inductive-setting data split, which cannot\nbe satisﬁed by existing KE benchmarks. Therefore,\nwe construct Wikidata5M, containing about 5M en-\ntities, 20M triplets, and aligned entity descriptions\nfrom Wikipedia. To the best of our knowledge,\nit is the largest general-domain KG dataset. We\nalso benchmark several classical KE methods and\ngive data splits for both the transductive and the\ninductive settings to facilitate future research.\nTo summarize, our contribution is three-fold:\n(1) We propose KEPLER, a knowledge-enhanced\nPLM by jointly optimizing the KE and MLM ob-\njectives, which brings great improvements on a\nwide range of NLP tasks. (2) By encoding text\ndescriptions as entity embeddings, KEPLER shows\nits effectiveness as a KE model, especially in the in-\nductive setting. (3) We also introduce Wikidata5M,\na new large-scale KG dataset, which shall promote\nthe research on large-scale KG, inductive KE, and\nthe interactions between KG and NLP.\n2 KEPLER\nAs shown in Figure 2, KEPLER implicitly incor-\nporates factual knowledge into language represen-\ntations by jointly training with two objectives. In\nthis section, we detailedly introduce the encoder\nstructure, the KE and MLM objectives, and how\nwe combine the two as a uniﬁed model.\n2.1 Encoder\nFor the text encoder, we use Transformer architec-\nture (Vaswani et al., 2017) in the same way as De-\nvlin et al. (2019) and Liu et al. (2019c). The en-\ncoder takes a sequence of N tokens (x1,...,x N )\nas inputs, and computes Llayers of d-dimensional\ncontextualized representations Hi ∈RN×d,1 ≤\ni ≤L. Each layer of the encoder Ei is a combi-\nnation of a multi-head self-attention network and\na multi-layer perceptron, and the encoder gets the\nrepresentation of each layer by Hi = Ei(Hi−1).\nEventually, we get a contextualized representation\nfor each position, which could be further used in\ndownstream tasks. Usually, there is a special token\n<s> added to the beginning of the text, and the\noutput at <s> is regarded sentence representation.\nWe denote the representation function as E<s>(·).\n<s> Johannes Kepler was a German astronomer … <s> An astronomer is a scientist in the ﬁeld of  … … Kepler <mask> to have had an epiphany on …\nJohannes Kepler Astronomer\nKnowledge Graph Text\nOccupation,                         ,                         (                         )                         \nEncoder EncoderEmbeddings Encoder\nKE Loss MLM Loss+\nh r t\ntext h text t\nh r t\nFigure 2: The KEPLER framework. We encode entity descriptions as entity embeddings and jointly train\nthe knowledge embedding (KE) and masked language modeling (MLM) objectives on the same PLM.\nThe encoder requires a tokenizer to convert plain\ntexts into sequences of tokens. Here we use the\nsame tokenization as RoBERTa: the Byte-Pair En-\ncoding (BPE) (Sennrich et al., 2016).\nUnlike previous knowledge-enhanced PLM\nworks (Zhang et al., 2019; Peters et al., 2019),\nwe do not modify the Transformer encoder struc-\nture to add external entity linkers or knowledge-\nintegration layers. It means that our model has no\nadditional inference overhead compared to vanilla\nPLMs, and it makes applying KEPLER in down-\nstream tasks as easy as RoBERTa.\n2.2 Knowledge Embedding\nTo integrate factual knowledge into KEPLER, we\nadopt the knowledge embedding (KE) objective in\nour pre-training. KE encodes entities and relations\nin knowledge graphs (KGs) as distributed represen-\ntations, which beneﬁts lots of downstream tasks,\nsuch as link prediction and relation extraction.\nWe ﬁrst deﬁne KGs: a KG is a graph with enti-\nties as its nodes and relations between entities as\nits edges. We use a triplet (h,r,t) to describe a\nrelational fact, where h,t are the head entity and\nthe tail entity, andris the relation type within a pre-\ndeﬁned relation set R. In conventional KE models,\neach entity and relation is assigned ad-dimensional\nvector, and a scoring function is deﬁned for training\nthe embeddings and predicting links.\nIn KEPLER, instead of using stored embeddings,\nwe encode entities into vectors by using their cor-\nresponding text. By choosing different textual data\nand different KE scoring functions, we have mul-\ntiple variants for the KE objective of KEPLER. In\nthis paper, we explore three simple but effective\nways: entity descriptions as embeddings, entity\nand relation descriptions as embeddings, and entity\nembeddings conditioned on relations. We leave ex-\nploring advanced KE methods as our future work.\nEntity Descriptions as Embeddings For a rela-\ntional triplet (h,r,t), we have:\nh = E<s>(texth),\nt = E<s>(textt),\nr = Tr,\n(1)\nwhere texth and textt are the descriptions\nfor hand t, with a special token <s> at the begin-\nning. T ∈R|R|×d is the relation embeddings and\nh,t,r are the embeddings for h,t and r.\nWe use the loss from Sun et al. (2019) as\nour KE objective, which adopts negative sam-\npling (Mikolov et al., 2013) for efﬁcient optimiza-\ntion:\nLKE = −log σ(γ−dr(h,t))\n−\nn∑\ni=1\n1\nnlog σ(dr(h′\ni,t′\ni) −γ), (2)\nwhere (h′\ni,r,t′\ni) are negative samples, γ is the\nmargin, σ is the sigmoid function, and dr is the\nscoring function, for which we choose to follow\nTransE (Bordes et al., 2013) for its simplicity,\ndr(h,t) =∥h + r −t∥p, (3)\nwhere we take the norm p as 1. The negative\nsampling policy is to ﬁx the head entity and ran-\ndomly sample a tail entity, and vice versa.\nEntity and Relation Descriptions as Embed-\ndings A natural extension for the last method\nis to encode the relation descriptions as relation\nembeddings as well. Formally, we have,\nˆr = E<s>(textr), (4)\nwhere textr is the description for the relation\nr. Then we use ˆr to replace r in Equation 2 and 3.\nEntity Embeddings Conditioned on Relations\nIn this manner, we use entity embeddings con-\nditioned on r for better KE performances. The\nintuition is that semantics of an entity may have\nmultiple aspects, and different relations focus on\ndifferent ones (Lin et al., 2015). So we have,\nhr = E<s>(texth,r), (5)\nwhere texth,r is the concatenation of the de-\nscription for the entity h and the description for\nthe relation r, with the special token <s> at the\nbeginning and </s> in between. Correspondingly,\nwe use hr instead of h for Equation 2 and 3.\n2.3 Masked Language Modeling\nThe masked language modeling (MLM) objective\nis inherited from BERT and RoBERTa. During pre-\ntraining, MLM randomly selects some of the input\npositions, and the objective is to predict the tokens\nat these selected positions within a ﬁxed dictionary.\nTo be more speciﬁc, MLM randomly selects\n15% of input positions, among which 80% are\nmasked with the special token <mask>, 10% are\nreplaced by other random tokens, and the rest re-\nmain unchanged. For each selected position j, the\nlast layer of the contextualized representation HL,j\nis used for a W-way classiﬁcation, where W is the\nsize of the dictionary. At last, a cross-entropy loss\nLMLM is calculated over these selected positions.\nWe initialize our model with the pre-trained\ncheckpoint of RoBERTaBASE. However, we still\nkeep MLM as one of our objectives to avoid catas-\ntrophic forgetting (McCloskey and Cohen, 1989)\nwhile training towards the KE objective. Actually,\nas demonstrated in Section 5.1, only using the KE\nobjective leads to poor results in NLP tasks.\n2.4 Training Objectives\nTo incorporate factual knowledge and language\nunderstanding into one PLM, we design a multi-\ntask loss as shown in Figure 2 and Equation 6,\nL= LKE + LMLM, (6)\nwhere LKE and LMLM are the losses for KE and\nMLM correspondingly. Jointly optimizing the two\nobjectives can implicitly integrate knowledge from\nexternal KGs into the text encoder, while preserv-\ning the strong abilities of PLMs for syntactic and\nsemantic understanding. Note that those two tasks\nonly share the text encoder, and for each mini-\nbatch, text data sampled for KE and MLM are not\n(necessarily) the same. This is because seeing a\nvariety of text (instead of just entity descriptions) in\nMLM can help the model to have better language\nunderstanding ability.\n2.5 Variants and Implementations\nWe introduce the variants of KEPLER and the pre-\ntraining implementations here. The ﬁne-tuning\ndetails will be introduced in Section 4.\nKEPLER Variants\nWe implement multiple versions of KEPLER in\nexperiments to explore the effectiveness of our pre-\ntraining framework. We use the same denotations\nin Section 4 as below.\nKEPLER-Wiki is the principal model in our\nexperiments, which adopts Wikidata5M (Section 3)\nas the KG and the entity-description-as-embedding\nmethod (Equation 1). All other variants, if not\nspeciﬁed, use the same settings. KEPLER-Wiki\nachieves the best performances on most tasks.\nKEPLER-WordNet uses the WordNet (Miller,\n1995) as its KG source. WordNet is an English\nlexical graph, where nodes are lemmas and synsets,\nand edges are their relations. Intuitively, incorpo-\nrating WordNet can bring lexical knowledge and\nthus beneﬁts NLP tasks. We use the same WordNet\n3.0 as in KnowBert (Peters et al., 2019), which is\nextracted from the nltk2 package.\nKEPLER-W+W takes both Wikidata5M and\nWordNet as its KGs. To jointly train with two KG\ndatasets, we modify the objective in Equation 6 as\nL= LWiki + LWordNet + LMLM, (7)\nwhere LWiki and LWordNet are losses from Wiki-\ndata5M and WordNet respectively.\nKEPLER-Rel uses the entity and relation de-\nscriptions as embeddings method (Equation 4). As\nthe relation descriptions in Wikidata are short (11.7\nwords on average) and homogeneous, encoding re-\nlation descriptions as relation embeddings results\nin worse performance as shown in Section 4.\nKEPLER-Cond uses the entity-embedding-\nconditioned-on-relation method (Equation 5). This\nmodel achieves superior results in link prediction\ntasks, both transductive and inductive (Section 4.3).\n2https://www.nltk.org\nDataset #entity #relation #training #validation #test\nFB15K 14,951 1 ,345 483 ,142 50 ,000 59 ,071\nWN18 40,943 18 141 ,442 5 ,000 5 ,000\nFB15K-237 14,541 237 272 ,115 17 ,535 20 ,466\nWN18RR 40,943 11 86 ,835 3 ,034 3 ,134\nWikidata5M 4,594,485 822 20 ,614,279 5 ,163 5 ,133\nTable 1: Statistics of Wikidata5M (transductive setting) compared with existing KE benchmarks.\nKEPLER-OnlyDesc trains the MLM objective\ndirectly on the entity descriptions from the KE\nobjective rather than uses the English Wikipedia\nand BookCorpus as other versions of KEPLER.\nHowever, as the entity description data are smaller\n(2.3 GB vs 13 GB) and homogeneous, it harms the\ngeneral language understanding ability and thus\nperforms worse (Section 4.2).\nKEPLER-KE only adopts the KE objective\nin pre-training, which is an ablated version of\nKEPLER-Wiki. It is used to show the necessity\nof the MLM objective for language understanding.\nPre-training Implementation\nIn practice, we choose RoBERTa (Liu et al., 2019c)\nas our base model and implement KEPLER in\nthe fairseq framework (Ott et al., 2019) for pre-\ntraining. Due to the computing resource limit, we\nchoose the BASE size (L = 12, d = 768) and\nuse the released roberta.base parameters for\ninitialization, which is a common practice to save\npre-training time (Zhang et al., 2019; Peters et al.,\n2019). For the MLM objective, we use the En-\nglish Wikipedia (2,500M words) and BookCorpus\n(800M words) (Zhu et al., 2015) as our pre-training\ncorpora (except KEPLER-OnlyDesc). We extract\ntext from these two corpora in the same way as De-\nvlin et al. (2019). For the KE objective, we encode\nthe ﬁrst 512 tokens of entity descriptions from the\nEnglish Wikipedia as entity embeddings.\nWe set the γ in Equation 2 as 4 and 9 for NLP\nand KE tasks respectively, and we use the models\npre-trained with 10 and 30 epochs for NLP and KE.\nSpecially, the γ is 1 for KEPLER-WordNet. The\ntwo hyper-parameters are tuned by multiple trials\nfor γ in {1,2,4,6,9}and the number of epochs\nin {5,10,20,30,40}, and we select the model by\nperformances on TACRED (F-1) and inductive link\nprediction (HITS@10). We use gradient accumula-\ntion to achieve a batch size of 12,288.\nEntity Type Occurrence Percentage\nHuman 1,517,591 33 .0%\nTaxon 363,882 7 .9%\nWikimedia list 118,823 2 .6%\nFilm 114,266 2 .5%\nHuman Settlement 110,939 2 .4%\nTotal 2,225,501 48 .4%\nTable 2: Top-5 entity categories in Wikidata5M.\nSubset #entity #relation #triplet\nTraining 4,579,609 822 20 ,496,514\nValidation 7,374 199 6 ,699\nTest 7,475 201 6 ,894\nTable 3: Statistics of Wikidata5M inductive setting.\n3 Wikidata5M\nAs shown in Section 2, to train KEPLER, the KG\ndataset should (1) be large enough, (2) contain high-\nquality textual descriptions for its entities and rela-\ntions, and (3) have a reasonable inductive setting,\nwhich most existing KG datasets do not provide.\nThus, based on Wikidata3 and English Wikipedia4,\nwe construct Wikidata5M, a large-scale KG dataset\nwith aligned text descriptions from corresponding\nWikipedia pages, and also an inductive test set.\nIn the following sections, we ﬁrst introduce the\ndata collection (Section 3.1) and the data split (Sec-\ntion 3.2), and then provide the results of represen-\ntative KE methods on the dataset (Section 3.3).\n3.1 Data Collection\nWe use the July 2019 dump of Wikidata and\nWikipedia. For each entity in Wikidata, we align it\nto its Wikipedia page and extract the ﬁrst section\nas its description. Entities with no pages or with\ndescriptions fewer than 5 words are discarded.\n3https://www.wikidata.org\n4https://en.wikipedia.org\nMethod MR MRR HITS@1 HITS@3 HITS@10\nTransE (Bordes et al., 2013) 109370 25 .3 17 .0 31 .1 39 .2\nDistMult (Yang et al., 2015) 211030 25 .3 20 .8 27 .8 33 .4\nComplEx (Trouillon et al., 2016) 244540 28 .1 22 .8 31 .0 37 .3\nSimplE (Kazemi and Poole, 2018) 115263 29 .6 25 .2 31 .7 37 .7\nRotatE (Sun et al., 2019) 89459 29 .0 23 .4 32 .2 39 .0\nTable 4: Performances of different KE models on Wikidata5M (% except MR).\nWe retrieve all the relational facts in Wikidata.\nA fact is considered to be valid when both of its\nentities are not discarded, and its relation has a non-\nempty page in Wikidata. The ﬁnal KG contains\n4,594,485 entities, 822 relations and 20,624,575\ntriplets. Statistics of Wikidata5M along with four\nother widely-used benchmarks are shown in Ta-\nble 1. Top-5 entity categories are listed in Table 2.\nWe can see that Wikidata5M is much larger than\nother KG datasets, covering various domains.\n3.2 Data Split\nFor Wikidata5M, we take two different settings:\nthe transductive setting and the inductive setting.\nThe transductive setting (shown in Table 1) is\nadopted in most KG datasets, where the entities are\nshared and the triplet sets are disjoint across train-\ning, validation and test. In this case, KE models\nare expected to learn effective entity embeddings\nonly for the entities in the training set.\nIn the inductive setting (shown in Table 3), the\nentities and triplets are mutually disjoint across\ntraining, validation and test. We randomly sample\nsome connected subgraphs as the validation and\ntest set. In the inductive setting, the KE models\nshould produce embeddings for the unseen entities\ngiven side features like descriptions, neighbors, etc.\nThe inductive setting is more challenging and also\nmeaningful in real-world applications, where enti-\nties in KGs experience open-ended growth, and the\ninductive ability is crucial for online KE methods.\nAlthough Wikidata5M contains massive entities\nand triplets, our validation and test set are not large,\nwhich is limited by the standard evaluation method\nof link prediction (Section 3.3). Each episode of\nevaluation requires |E|×|T|× 2 times of KE score\ncalculation, where |E|and |T| are the total num-\nber of entities and the number of triplets in test set\nrespectively. As Wikidata5M contains massive en-\ntities, the evaluation is very time-consuming, hence\nwe have to limit the test set to thousands of triplets\nto ensure tractable evaluations. This indicates that\nlarge-scale KE urges a more efﬁcient evaluation\nprotocol. We will leave exploring it to future work.\n3.3 Benchmark\nTo assess the challenges of Wikidata5M, we bench-\nmark several popular KE models on our dataset in\nthe transductive setting (as they inherently do not\nsupport the inductive setting). Because their origi-\nnal implementations do not scale to Wikidata5M,\nwe benchmark these methods with GraphVite (Zhu\net al., 2019), a multi-GPU KE toolkit.\nIn the transductive setting, for each test triplet\n(h,r,t), the model ranks all the entities by scor-\ning (h,r,t′),t′ ∈E, where Eis the entity set ex-\ncluding other correct t. The evaluation metrics,\nMRR (mean reciprocal rank), MR (mean rank),\nand HITS@{1,3,10}, are based on the rank of the\ncorrect tail entitytamong all the entities inE. Then\nwe do the same thing for the head entities. We re-\nport the average results over all test triplets and\nover both head and tail entity predictions.\nTable 4 shows the results of popular KE meth-\nods on Wikidata5M, which are all signiﬁcantly\nlower than on existing KG datasets like FB15K-\n237, WN18RR, etc. It demonstrates that Wiki-\ndata5M is more challenging due to its large scale\nand high coverage. The results advocate for more\nefforts towards large-scale KE.\n4 Experiments\nIn this section, we introduce the experiment set-\ntings and results of our model on various NLP and\nKE tasks, along with some analyses on KEPLER.\n4.1 Experimental Setting\nBaselines In our experiments, RoBERTa is an\nimportant baseline since KEPLER is based on it\n(all mentioned models are of BASE size if not spec-\niﬁed). As we cannot afford the full RoBERTa cor-\npora (126 GB, and we only use 13 GB) in KEPLER\npre-training, we implement Our RoBERTafor di-\nrect comparisons to KEPLER. It is initialized by\nRoBERTaBASE and is further trained with the MLM\nobjective on the same corpora as KEPLER.\nWe also evaluate recent knowledge-enhanced\nPLMs, including ERNIEBERT (Zhang et al., 2019)\nand KnowBertBERT (Peters et al., 2019). As\nERNIE and our principal model KEPLER-Wiki\nonly use Wikidata, we take KnowBert-Wiki in\nthe experiments to ensure fair comparisons with\nthe same knowledge source. Considering KE-\nPLER is based on RoBERTa, we reproduce the\ntwo models with RoBERTa too ( ERNIERoBERTa\nand KnowBertRoBERTa). The reproduction of\nKnowBert is based on its original implementa-\ntion5. On relation classiﬁcation, we also compare\nwith MTB (Baldini Soares et al., 2019), which\nadopts “matching the blank” pre-training. Differ-\nent from other baselines, the original MTB is based\non BERTLARGE (denoted by MTB (BERTLARGE)).\nFor a fair comparison under the same model size,\nwe reimplement MTB with BERTBASE (MTB).\nHyper-parameter The pre-training settings are\nin Section 2.5. For ﬁne-tuning on downstream\ntasks, we set KEPLER hyper-parameters the same\nas reported in KnowBert on TACRED and OpenEn-\ntity. On FewRel, we set the learning rate as 2e-5\nand batch size as 20 and 4 for the Proto and PAIR\nframeworks respectively. For GLUE, we follow\nthe hyper-parameters reported in RoBERTa. For\nbaselines, we keep their original hyper-parameters\nunchanged or use the best trial in KEPLER search-\ning space if no original settings are available.\n4.2 NLP Tasks\nIn this section, we demonstrate the performance of\nKEPLER and its baselines on various NLP tasks.\nRelation Classiﬁcation\nRelation classiﬁcation requires models to classify\nrelation types between two given entities from text.\nWe evaluate KEPLER and other baselines on two\nwidely-used benchmarks: TACRED and FewRel.\nTACRED(Zhang et al., 2017) has 42 relations\nand 106,264 sentences. Here we follow the set-\ntings of Baldini Soares et al. (2019), where we add\nfour special tokens before and after the two entity\nmentions, and concatenate the representations at\nthe beginnings of the two entities for classiﬁcation.\nNote that the original KnowBert also takes entity\ntypes as inputs, which is different from Zhang et al.\n(2019); Baldini Soares et al. (2019). To ensure fair\n5https://github.com/allenai/kb\nModel P R F-1\nBERT 67.2 64 .8 66 .0\nBERTLARGE - - 70.1\nMTB 69.7 67 .9 68 .8\nMTB (BERTLARGE) - - 71.5\nERNIEBERT 70.0 66 .1 68 .0\nKnowBertBERT 73.5 64.1 68 .5\nRoBERTa 70.4 71 .1 70 .7\nERNIERoBERTa 73.5 68.0 70 .7\nKnowBertRoBERTa 71.9 69 .9 70 .9\nOur RoBERTa 70.8 69 .6 70 .2\nKEPLER-Wiki 71.5 72.5 72 .0\nKEPLER-WordNet 71.4 71 .3 71 .3\nKEPLER-W+W 71.1 72 .0 71 .5\nKEPLER-Rel 71.3 70 .9 71 .1\nKEPLER-Cond 72.1 70 .7 71 .4\nKEPLER-OnlyDesc 72.3 69 .1 70 .7\nKEPLER-KE 63.5 60 .5 62 .0\nTable 5: Precision, recall and F-1 on TACRED (%).\nKnowBert results are different from the original\npaper since different task settings are used.\ncomparisons, we re-evaluate KnowBert with the\nsame setting as other baselines, thus the reported\nresults are different from the original paper.\nFrom the TACRED results in Table 5, we can\nobserve that: (1) KEPLER-Wiki is the best one\namong KEPLER variants and signiﬁcantly outper-\nforms all the baselines, while other versions of KE-\nPLER also achieve good results. It demonstrates\nthe effectiveness of KEPLER on integrating factual\nknowledge into PLMs. Based on the result, we use\nKEPLER-Wiki as the principal model in the fol-\nlowing experiments. (2) KEPLER-WordNet shows\na marginal improvement over Our RoBERTa, while\nKEPLER-W+W underperforms KEPLER-Wiki. It\nsuggests that pre-training with WordNet only has\nlimited beneﬁts in the KEPLER framework. We\nwill explore how to better combine different KGs\nin our future work.\nFewRel (Han et al., 2018b) is a few-shot relation\nclassiﬁcation dataset with 100 relations and 70,000\ninstances, which is constructed with Wikipedia text\nand Wikidata facts. Furthermore, Gao et al. (2019)\npropose FewRel 2.0, adding a domain adaptation\nchallenge with a new medical-domain test set.\nFewRel takes the N-way K-shot setting. Rela-\ntions in the training and test sets are disjoint. For\nevery evaluation episode, N relations, Ksupport-\ning samples for each relation, and several query\nsentences are sampled from the test set. The mod-\nModel FewRel 1.0 FewRel 2.0\n5-1 5-5 10-1 10-5 5-1 5-5 10-1 10-5\nMTB (BERTLARGE)† 93.86 97 .06 89 .20 94 .27 − − − −\nProto (BERT) 80.68 89 .60 71 .48 82 .89 40.12 51 .50 26 .45 36 .93\nProto (MTB) 81.39 91 .05 71 .55 83 .47 52.13 76 .67 48 .28 69 .75\nProto (ERNIEBERT)† 89.43 94.66 84.23 90.83 49.40 65 .55 34 .99 49 .68\nProto (KnowBertBERT)† 86.64 93 .22 79 .52 88 .35 64.40 79 .87 51 .66 69 .71\nProto (RoBERTa) 85.78 95 .78 77 .65 92 .26 64.65 82 .76 50 .80 71 .84\nProto (Our RoBERTa) 84.42 95 .30 76 .43 91 .74 61.98 83 .11 48 .56 72 .19\nProto (ERNIERoBERTa)† 87.76 95 .62 80 .14 91 .47 54.43 80 .48 37 .97 66 .26\nProto (KnowBertRoBERTa)† 82.39 93 .62 76 .21 88 .57 55.68 71 .82 41 .90 58 .55\nProto (KEPLER-Wiki) 88.30 95.94 81.10 92.67 66.41 84.02 51.85 73.60\nPAIR (BERT) 88.32 93 .22 80 .63 87 .02 67.41 78.57 54.89 66.85\nPAIR (MTB) 83.01 87 .64 73 .42 78 .47 46.18 70 .50 36 .92 55 .17\nPAIR (ERNIEBERT)† 92.53 94.27 87.08 89.13 56.18 68 .97 43 .40 54 .35\nPAIR (KnowBertBERT)† 88.48 92 .75 82 .57 86 .18 66.05 77 .88 50 .86 67 .19\nPAIR (RoBERTa) 89.32 93 .70 82 .49 88 .43 66.78 81 .84 53 .99 70 .85\nPAIR (Our RoBERTa) 89.26 93 .71 83 .32 89 .02 63.22 77 .66 49 .28 65 .97\nPAIR (ERNIERoBERTa)† 87.46 94 .11 81 .68 87 .83 59.29 72 .91 48 .51 60 .26\nPAIR (KnowBertRoBERTa)† 85.05 91 .34 76 .04 85 .25 50.68 66 .04 37 .10 51 .13\nPAIR (KEPLER-Wiki) 90.31 94.28 85.48 90.51 67.23 82.09 54.32 71.01\nTable 6: Accuracies (%) on the FewRel dataset. N-Kindicates the N-way K-shot setting. MTB uses the\nLARGE size and all the other models use the BASE size. †indicates oracle models which may have seen\nfacts in the FewRel 1.0 test set during pre-training.\nModel P R F-1\nUFET (Choi et al., 2018) 77.4 60 .6 68 .0\nBERT 76.4 71 .0 73 .6\nERNIEBERT 78.4 72 .9 75 .6\nKnowBertBERT 77.9 71 .2 74 .4\nRoBERTa 77.4 73 .6 75 .4\nERNIERoBERTa 80.3 70 .2 74 .9\nKnowBertRoBERTa 78.7 72 .7 75 .6\nOur RoBERTa 75.1 73 .4 74 .3\nKEPLER-Wiki 77.8 74 .6 76.2\nTable 7: Entity typing results on OpenEntity (%).\nels are required to classify queries into one of theN\nrelations only given the sampled N ×Kinstances.\nWe use two state-of-the-art few-shot frameworks:\nProto (Snell et al., 2017) and PAIR (Gao et al.,\n2019). We replace the text encoders with our base-\nlines and KEPLER and compare the performance.\nSince FewRel 1.0 is constructed with Wikidata, we\nremove all the triplets in its test set from Wiki-\ndata5M to avoid information leakage for KEPLER.\nHowever, we cannot control the KGs used in our\nbaselines. We mark the models utilizing Wikidata\nand have information leakage risk with †in Table 6.\nAs Table 6 shows, KEPLER-Wiki achieves the\nbest performance over the BASE-size PLMs in\nmost settings. From the results, we also have some\ninteresting observations: (1) RoBERTa consistently\noutperforms BERT on various NLP tasks (Liu et al.,\n2019c), yet the RoBERTa-based models here are\ncomparable or even worse than BERT-based mod-\nels in the PAIR framework. Since PAIR uses sen-\ntence concatenation, this result may be credited\nto the next sentence prediction (NSP) objective\nof BERT. (2) KEPLER brings improvements on\nFewRel 2.0, while ERNIE and KnowBert even\ndegenerate in most of the settings. It indicates\nthat the paradigms of ERNIE and KnowBert can-\nnot well generalize to new domains which may\nrequire much different entity linkers and entity em-\nbeddings. On the other hand, KEPLER not only\nlearns better entity representations but also acquires\na general ability to extract factual knowledge from\nthe context across different domains. We further\nverify this in Section 5.5. (3) KnowBert underper-\nforms ERNIE in FewRel while it typically achieves\nbetter results on other tasks. This may be because it\nuses the TuckER (Balazevic et al., 2019) KE model\nwhile ERNIE and KEPLER follow TransE (Bor-\ndes et al., 2013). We will explore the effects of\ndifferent KE methods in the future.\nWe also have another two observations with re-\ngard to ERNIE and MTB: (1) ERNIE performs\nthe best on 1-shot settings of FewRel 1.0. We\nbelieve this is because that the knowledge em-\nbedding injection of ERNIE has particular advan-\ntages in this case, since it directly brings knowl-\nedge about entities. When using 5-shot (support-\ning text provides more information) and FewRel\n2.0 (ERNIE does not have knowledge for biomed-\nical entities), KEPLER outperforms ERNIE. (2)\nThough MTB (BERTLARGE) is the state-of-the-art\nmodel on FewRel, its BERT BASE version does\nnot outperform other knowledge-enhanced PLMs,\nwhich suggests that using large models contributes\nmuch to its gain. We also notice that when com-\nbined with PAIR, MTB suffers an obvious perfor-\nmance drop, which may be because its pre-training\nobjective degenerates sentence-pair tasks.\nEntity Typing\nEntity typing requires to classify given entity men-\ntions into pre-deﬁned types. For this task, we carry\nout evaluations on OpenEntity (Choi et al., 2018)\nfollowing the settings in Zhang et al. (2019). Ope-\nnEntity has 6 entity types and 2,000 instances for\ntraining, validation and test each.\nTo identify the entity mentions of interest, we\nadd two special tokens before and after the en-\ntity spans, and use the representations of the ﬁrst\nspecial tokens for classiﬁcation. As shown in Ta-\nble 7, KEPLER-Wiki achieves state-of-the-art re-\nsults. Note that the KnowBert results are different\nfrom the original paper since we use KnowBert-\nWiki here rather than KnowBert-W+W to ensure\nthe same knowledge resource and fair comparisons.\nKEPLER does not perform linking or entity em-\nbedding pre-training like ERNIE and KnowBert,\nwhich bring them special advantages in entity span\ntasks. However, KEPLER still outperforms these\nbaselines, which proves its effectiveness.\nGLUE\nThe General Language Understanding Evaluation\n(GLUE) (Wang et al., 2019b) collects several natu-\nral language understanding tasks and is widely used\nfor evaluating PLMs. In general, solving GLUE\ndoes not require factual knowledge (Zhang et al.,\n2019) and we use it to examine whether KEPLER\nharms the general language understanding ability.\nTable 8 shows the GLUE results. We can ob-\nserve that KEPLER-Wiki is close to Our RoBERTa,\nsuggesting that while incorporating factual knowl-\nedge, KEPLER maintains a strong language un-\nderstanding ability. However, there are signiﬁcant\nperformance drops of KEPLER-OnlyDesc, which\nindicates that the small-scale entity description data\nare not sufﬁcient for training KEPLER with MLM.\nFor the small datasets STS-B, MRPC and RTE,\ndirectly ﬁne-tuning models on them typically result\nin unstable performance. Hence we ﬁne-tune mod-\nels on a large-scale dataset (here we use MNLI)\nﬁrst and then further ﬁne-tune them on the small\ndatasets. The method has been shown to be effec-\ntive (Wang et al., 2019a) and is also used in the\noriginal RoBERTa paper (Liu et al., 2019c).\n4.3 KE Tasks\nWe show how KEPLER works as a KE model, and\nevaluate it on Wikidata5M in both the transductive\nlink prediction setting and the inductive setting.\nExperimental Settings\nIn link prediction, the entity and relation embed-\ndings of KEPLER are obtained as described in\nSection 2.2 and 2.5. The evaluation method is de-\nscribed in Section 3.3. We also add RoBERTa and\nOur RoBERTa as baselines. They adopt Equation 1\nand 4 to acquire entity and relation embeddings,\nand use Equation 3 as their scoring function.\nIn the transductive setting, we compare our mod-\nels with TransE (Bordes et al., 2013). We set its\ndimension as 512, negative sampling size as 64,\nbatch size as 2048 and learning rate as 0.001 after\nhyper-parameter searching. The negative sampling\nsize is crucial for the performance on KE tasks,\nbut limited by the model complexity, KEPLER can\nonly take a negative size of 1. For a direct compari-\nson to intuitively show the beneﬁts of pre-training,\nwe set a baseline TransE†, which also uses 1 as the\nnegative sampling size and keeps the other hyper-\nparameters unchanged.\nSince conventional KE methods like TransE in-\nherently cannot provide embeddings for unseen\nentities, we take DKRL (Xie et al., 2016) as our\nbaseline in the KE experiments, which utilizes con-\nvolutional neural networks to encode entity descrip-\ntions as embeddings. We set its dimension as 768,\nnegative sampling size as 64, batch size as 1024\nand learning rate as 0.0005.\nTransductive Setting\nTable 9a shows the results of the transductive set-\nting. We observe that:\nModel MNLI (m/mm) QQP QNLI SST-2\n392K 363K 104K 67K\nRoBERTa 87.5/87.2 91.9 92.7 94.8\nOur RoBERTa 87.1/86.8 90.9 92.5 94.7\nKEPLER-Wiki 87.2/86.5 91.7 92.4 94.5\nKEPLER-OnlyDesc 85.9/85.6 90.8 92.4 94.4\nModel CoLA STS-B MRPC RTE\n8.5K 5.7K 3.5K 2.5K\nRoBERTa 63.6 91.2 90.2 80.9\nOur RoBERTa 63.4 91.1 88.4 82.3\nKEPLER-Wiki 63.6 91.2 89.3 85.2\nKEPLER-OnlyDesc 55.8 90.2 88.5 78.3\nTable 8: GLUE results on the dev set (%). All\nthe results are medians over 5 runs. We report F-1\nscores for QQP and MRPC, Spearman correlations\nfor STS-B, and accuracy scores for the other tasks.\nThe “m/mm” stands for matched/mismatched eval-\nuation sets for MNLI (Williams et al., 2018).\n(1) KEPLER underperforms TransE. It is reason-\nable since KEPLER is limited by its large model\nsize, and thus cannot use a large negative sampling\nsize (1 for KEPLER, while typical KE methods use\n64 or more) and more training epochs (30 vs 1000\nfor TransE), which are crucial for KE (Zhu et al.,\n2019). On the other hand, KEPLER and its variants\nperform much better than TransE†(with a negative\nsampling size of 1), showing that using the same\nnegative sampling size, KEPLER can beneﬁt from\npre-trained language representations and textual en-\ntity descriptions so that outperform TransE. In the\nfuture, we will explore reducing the model size of\nKEPLER to take advantage of both large negative\nsampling size and pre-training.\n(2) The vanilla RoBERTa perform poorly in KE\nwhile KEPLER achieves favorable performances,\nwhich demonstrates the effectiveness of our multi-\ntask pre-training to infuse factual knowledge.\n(3) Among the KEPLER variants, KEPLER-\nCond has superior results, which substantiates\nthe intuition in Section 2.2. KEPLER-Rel per-\nforms worst, which we believe is due to the short\nand homogeneous relation descriptions of Wiki-\ndata. KEPLER-KE signiﬁcantly underperforms\nKEPLER-Wiki, which suggests that the MLM ob-\njective is necessary as well for the KE tasks to build\neffective language representation.\n(4) We also notice that DKRL performs well on\nthe transductive setting and the result is close to\nKEPLER. We believe this is because DKRL takes\na much smaller encoder (CNN) and thus is easier\nto train. In the more difﬁcult inductive setting, the\ngap between DKRL and KEPLER is larger, which\nbetter shows the language understanding ability of\nKEPLER to utilize textual entity descriptions.\nInductive Setting\nTable 9b shows the Wikidata5M inductive results.\nKEPLER outperforms DKRL and RoBERTa by a\nlarge margin, demonstrating the effectiveness of\nour joint training method. But KEPLER results are\nstill far from ideal performances required by prac-\ntical applications (constructing KG from scratch,\netc.), which urges further efforts on inductive KE.\nComparisons among KEPLER variants are consis-\ntent with in the transductive setting.\nIn addition, we clarify why results in the induc-\ntive setting are much higher than the transductive\nsetting, while the inductive setting is more difﬁcult:\nAs shown in Table 1 and 3, the entities involved\nin the inductive evaluation is much less than the\ntransductive setting (7,475 vs. 4,594,485). Con-\nsidering the KE evaluation metrics are based on\nentity ranking, it is reasonable to see higher val-\nues in the inductive setting. The performance in\ndifferent settings should not be directly compared.\n5 Analysis\nIn this section, we analyze the effectiveness and\nefﬁciency of KEPLER with experiments. All the\nhyper-parameters are the same as reported in Sec-\ntion 4.1, including models in the ablation study.\n5.1 Ablation Study\nAs shown in Equation 6, KEPLER takes a multi-\ntask loss. To demonstrate the effectiveness of the\njoint objective, we compare full KEPLER with\nmodels trained with only the MLM loss ( Our\nRoBERTa) and only the KE loss (KEPLER-KE)\non TACRED. As demonstrated in Table 10, com-\npared to KEPLER-Wiki, both ablation models suf-\nfer signiﬁcant drops. It suggests that the perfor-\nmance gain of KEPLER is credited to the joint\ntraining towards both objectives.\n5.2 Knowledge Probing Experiment\nSection 4.2 shows that KEPLER can achieve sig-\nniﬁcant improvements on NLP tasks requiring fac-\ntual knowledge. To further verify whether KE-\nPLER can better integrate factual knowledge into\nPLMs and help to recall them, we conduct exper-\niments on LAMA (Petroni et al., 2019), a widely-\nused knowledge probe. LAMA examines PLMs’\nabilities on recalling relational facts by cloze-style\nquestions. For instance, given a natural language\nModel MR MRR HITS@1 HITS@3 HITS@10\nTransE (Bordes et al., 2013) 109370 25.3 17.0 31.1 39.2\nTransE† 406957 6 .0 1 .8 8 .0 13 .6\nDKRL (Xie et al., 2016) 31566 16 .0 12 .0 18 .1 22 .9\nRoBERTa 1381597 0 .1 0 .0 0 .1 0 .3\nOur RoBERTa 1756130 0 .1 0 .0 0 .1 0 .2\nKEPLER-KE 76735 8 .2 4 .9 8 .9 15 .1\nKEPLER-Rel 15820 6 .6 3 .7 7 .0 11 .7\nKEPLER-Wiki 14454 15.4 10 .5 17 .4 24 .4\nKEPLER-Cond 20267 21 .0 17.3 22.4 27 .7\n(a) Transductive results on Wikidata5M (% except MR). TransE† denotes a TransE modeled trained with the same\nnegative sampling size (1) as KEPLER.\nModel MR MRR HITS@1 HITS@3 HITS@10\nDKRL (Xie et al., 2016) 78 23.1 5.9 32.0 54.6\nRoBERTa 723 7 .4 0 .7 1 .0 19 .6\nOur RoBERTa 1070 5 .8 1 .9 6 .3 13 .0\nKEPLER-KE 138 17 .8 5 .7 22 .9 40 .7\nKEPLER-Rel 35 33 .4 15 .9 43 .5 66 .1\nKEPLER-Wiki 32 35 .1 15 .4 46 .9 71 .9\nKEPLER-Cond 28 40.2 22.2 51.4 73.0\n(b) Inductive results on Wikidata5M (% except MR).\nTable 9: Link prediction results on Wikidata5M transductive and inductive settings.\nModel P R F-1\nOur RoBERTa 70.8 69 .6 70 .2\nKEPLER-KE 63.5 60 .5 62 .0\nKEPLER-Wiki 71.5 72 .5 72 .0\nTable 10: Ablation study results on TACRED (%).\ntemplate “Paris is the capital of <mask>”, PLMs\nare required to predict the masked token without\nﬁne-tuning. LAMA reports the micro-averaged\nprecision at one (P@1) scores. However, Poerner\net al. (2020) present that LAMA contains some\neasy questions which can be answered with superﬁ-\ncial clues like entity names. Hence we also evaluate\nthe models on LAMA-UHN (Poerner et al., 2020),\nwhich ﬁlters out the questionable templates from\nthe Google-RE and T-REx corpora of LAMA.\nThe evaluation results are shown in Table 11,\nfrom which we have the following observations:\n(1) KEPLER consistently outperforms the vanilla\nPLM baseline Our RoBERTa in almost all the set-\ntings except ConceptNet, which focuses on com-\nmonsense knowledge rather than factual knowl-\nedge. It indicates that KEPLER can indeed bet-\nter integrate factual knowledge. (2) Although\nKEPLER-W+W cannot outperform KEPLER-Wiki\non NLP tasks (Section 4.2), it shows signiﬁcant im-\nprovements in LAMA-UHN, which suggests that\nwe should explore which kind of knowledge is\nneeded on different scenarios in the future. (3) All\nthe RoBERTa-based models perform worse than\nvanilla BERTBASE by a large margin, which is con-\nsistent with the results of Wang et al. (2020). This\nmay be due to different vocabularies used in BERT\nand RoBERTa, which presents the vulnerability of\nLAMA-style probing again (Kassner and Schütze,\n2020). We will leave developing a better knowl-\nedge probing framework as our future work.\n5.3 Running Time Comparison\nCompared to vanilla PLMs, KEPLER does not in-\ntroduce any additional parameters or computations\nduring ﬁne-tuning and inference, which is efﬁcient\nfor practice use. We compare the running time of\nKEPLER and other knowledge-enhanced PLMs\n(ERNIE and KnowBert) in Table 12. The time is\nevaluated on TACRED training set for one epoch\nwith one NVIDIA Tesla V100 (32 GB), and all\nmodels use 32 batch size and 128 sequence length.\nThe “entity linking” time of KnowBert is for en-\ntity candidate generation. We can observe that KE-\nPLER requires much less running time since it does\nModel LAMA LAMA-UHN\nGoogle-RE T-REx ConceptNet SQuAD Google-RE T-REx\nBERT 9.8 31 .1 15 .6 14 .1 4.7 21 .8\nRoBERTa 5.3 24 .7 19 .5 9 .1 2.2 17 .0\nOur RoBERTa 7.0 23 .2 19.0 8.0 2.8 15 .7\nKEPLER-Wiki 7.3 24.6 18.7 14.3 3.3 16 .5\nKEPLER-W+W 7.3 24.4 17 .6 10 .8 4.1 17.1\nTable 11: P@1 results on knowledge probing benchmark LAMA and LAMA-UHN.\nModel Entity Fine- Inference\nLinking tuning\nERNIERoBERTa 780s 730s 194s\nKnowBertRoBERTa 190s 677s 235s\nKEPLER 0s 508s 152s\nTable 12: Three parts of running time for one epoch\nof TACRED training set.\n0% 20% 40% 60% 80% 100%\nEntity Masking Rate\n64\n66\n68\n70\n72F-1 (%)\nKEPLER-Wiki\nOur RoBERTa\nFigure 3: TACRED performance (F-1) of KEPLER\nand RoBERTa change with the rate of entity men-\ntions being masked.\nnot need entity linking or entity embedding fusion,\nwhich will beneﬁt time-sensitive applications.\n5.4 Correlation with Entity Frequency\nTo better understand how KEPLER helps the entity-\ncentric tasks, we provide analyses on the correla-\ntions between KEPLER performance and entity\nfrequency in this section. The motivation is to\nverify a natural hypothesis that KEPLER improve-\nments mainly come from better representing the\nentity mentions in text, especially the rare entities,\nwhich do not show up frequently in the pre-training\ncorpora and thus cannot be well learned by the\nlanguage modeling objectives.\nWe perform entity linking for the TACRED\ndataset with BLINK (Wu et al., 2020) to link\nthe entity mentions in text to their corresponding\nWikipedia identiﬁers. Then we count the occur-\nrences of the entities in Wikipedia with the hyper-\nlinks in rich text, denoting the entity frequencies.\nWe conduct two experiments to analyze the corre-\nlations between KEPLER performance and entity\nfrequency: (1) In Table 13, we divide the entity\nmentions into ﬁve parts by their frequencies, and\ncompare the TACRED performances while only\nkeeping entities in one part and masking the other.\n(2) In Figure 3, we sequentially mask the entity\nmentions in the ascending order of entity frequen-\ncies and see the F-1 changes.\nFrom the results, we can observe that:\n(1) Figure 3 shows that when the entity mask-\ning rate is low, the improvements of KEPLER over\nRoBERTa are generally much higher than when\nthe entity masking rate is high. It indicates that\nthe improvements of KEPLER do mainly come\nfrom better modeling entities in context. However,\neven when all the entity mentions are masked, KE-\nPLER still outperforms RoBERTa. We claim this\nis because the KE objective can also help to learn\nto understand fact-related text since it requires the\nmodel to recall facts from textual descriptions. This\nclaim is further substantiated in Section 5.5.\n(2) From Table 13, we can observe that the im-\nprovement in the “0%-20%” setting is marginally\nhigher than the other settings, which demonstrates\nthat KEPLER does have special advantages on\nmodeling rare entities compared to vanilla PLMs.\nBut the improvements in the frequent settings are\nalso signiﬁcant and we cannot say that the overall\nimprovements of KEPLER are mostly from the rare\nentities. In general, the results in Table 13 show\nthat KEPLER can better model all the entities, no\nEntity Frequency 0%-20% 20%-40% 40%-60% 60%-80% 80%-100%\nKEPLER-Wiki 64.7 64 .4 64 .8 64 .7 68 .8\nOur RoBERTa 64.1 64 .3 64 .5 64 .3 68 .5\nImprovement +0.6 +0 .1 +0 .3 +0 .4 +0 .3\nTable 13: F-1 scores on TACRED (%) under different settings by entity frequencies. We sort the entity\nmentions in TACRED by their corresponding entity frequencies in Wikipedia. The “0%-20%” setting\nindicates only keeping the least frequent 20% entity mentions and masking all the other entity mentions\n(for both training and validation), and so on. The results are averaged over 5 runs.\nmatter rare or frequent.\nModel ME OE\nOur RoBERTa 54.0 46 .8\nKEPLER-KE 40.2 47 .0\nKEPLER-Wiki 54.8 48 .9\nTable 14: Masked-entity (ME) and only-entity (OE)\nF-1 scores on TACRED (%).\n5.5 Understanding Text or Storing\nKnowledge\nWe argue that by jointly training the KE and the\nMLM objectives, KEPLER (1) can better under-\nstand fact-related text and better extract knowledge\nfrom text, and also (2) can remember factual knowl-\nedge. To investigate the two abilities of KEPLER\nin a quantitative aspect, we carry out an experi-\nment on TACRED, in which the head and tail en-\ntity mentions are masked (masked-entity, ME) or\nonly head and tail entity mentions are shown (only-\nentity, OE). The ME setting shows to what extent\nthe models can extract facts only from the textual\ncontext without the clues in entity names. The OE\nsetting demonstrates to what extent the models can\nstore and predict factual knowledge, as only the\nentity names are given to the models.\nAs shown in Table 14, KEPLER-Wiki shows\nsigniﬁcant improvements over Our RoBERTa in\nboth settings, which suggests that KEPLER has in-\ndeed possessed superior abilities on both extracting\nand storing knowledge compared to vanilla PLMs\nwithout knowledge infusion. And the KEPLER-\nKE model performs poorly on the ME setting but\nachieves marginal improvements on the OE setting.\nIt indicates that without the help of the MLM ob-\njective, KEPLER only learns the entity description\nembeddings and degenerates in general language\nunderstanding, while it can still remember knowl-\nedge into entity names to some extent.\n6 Related Work\nPre-training in NLP There has been a long his-\ntory of pre-training in NLP. Early works focus on\ndistributed word representations (Collobert and We-\nston, 2008; Mikolov et al., 2013; Pennington et al.,\n2014), many of which are often adopted in current\nmodels as word embeddings. These pre-trained\nembeddings can capture the semantics of words\nfrom large-scale corpora and thus beneﬁt NLP ap-\nplications. Peters et al. (2018) push this trend a step\nforward by using a bidirectional LSTM to form con-\ntextualized word embeddings (ELMo) for richer\nsemantic meanings under different circumstances.\nApart from word embeddings, there is another\ntrend exploring pre-trained language models. Dai\nand Le (2015) propose to train an auto-encoder\non unlabeled textual data and then ﬁne-tune it on\ndownstream tasks. Howard and Ruder (2018) pro-\npose a universal language model (ULMFiT). With\nthe powerful Transformer architecture (Vaswani\net al., 2017), Radford et al. (2018) demonstrate\nan effective pre-trained generative model (GPT).\nLater, Devlin et al. (2019) release a pre-trained\ndeep Bidirectional Encoder Representation from\nTransformers (BERT), achieving state-of-the-art\nperformance on a wide range of NLP benchmarks.\nAfter BERT, similar PLMs spring up recently.\nYang et al. (2019) propose a permutation language\nmodel (XLNet). Later, Liu et al. (2019c) show that\nmore data and more parameter tuning can beneﬁt\nPLMs, and release a new state-of-the-art model\n(RoBERTa). Other works explore how to add more\ntasks (Liu et al., 2019b) and more parameters (Raf-\nfel et al., 2020; Lan et al., 2020) to PLMs.\nKnowledge-Enhanced PLMs Recently, many\nworks have investigated how to incorporate knowl-\nedge into PLMs. MTB (Baldini Soares et al., 2019)\ntakes a straightforward “matching the blank” pre-\ntraining objective to help the relation classiﬁcation\ntask. ERNIE (Zhang et al., 2019) identiﬁes entity\nmentions in text and links pre-processed knowledge\nembeddings to the corresponding positions, which\nshows improvements on several NLP benchmarks.\nWith a similar idea as ERNIE, KnowBert (Peters\net al., 2019) incorporates an integrated entity linker\nin their model and adopts end-to-end training. Be-\nsides, Logan et al. (2019) and Hayashi et al. (2020)\nutilize relations between entities inside one sen-\ntence to train better generation models. Xiong et al.\n(2019) adopt entity replacement knowledge learn-\ning for improving entity-related tasks.\nSome contemporaneous or following works try\nto inject factual knowledge into PLMs in different\nways. E-BERT (Poerner et al., 2020) aligns entity\nembeddings with word embeddings and then di-\nrectly adds the aligned embeddings into BERT to\navoid additional pre-training. K-Adapter (Wang\net al., 2020) injects knowledge with additional neu-\nral adapters to support continuous learning.\nKnowledge Embedding KE methods have been\nextensively studied. Conventional KE models\ndeﬁne different scoring functions for relational\ntriplets. For example, TransE (Bordes et al., 2013)\ntreats tail entities as translations of head entities and\nuses L1-norm or L2-norm to score triplets, while\nDistMult (Yang et al., 2015) uses matrix multiplica-\ntions and ComplEx (Trouillon et al., 2016) adopts\ncomplex operations based on it. RotatE (Sun et al.,\n2019) combines the advantages of both of them.\nInductive Embedding Above KE methods learn\nentity embeddings only from KG and are inherently\ntransductive, while some works (Wang et al., 2014;\nXie et al., 2016; Yamada et al., 2016; Cao et al.,\n2017; Shi and Weninger, 2018; Cao et al., 2018) in-\ncorporate textual metadata such as entity names or\ndescriptions to enhance the KE methods and hence\ncan do inductive KE to some extent. Besides KG, it\nis also common for general inductive graph embed-\nding methods (Hamilton et al., 2017; Bojchevski\nand Günnemann, 2018) to utilize additional node\nfeatures like text attributes, degrees, etc. KEPLER\nfollows this line of studies and takes full advantage\nof textual information with an effective PLM.\nHamaguchi et al. (2017) and Wang et al. (2019c)\nperform inductive KE by aggregating the trained\nembeddings of the known neighboring nodes with\ngraph neural networks, and thus do not need ad-\nditional features. But these methods require the\nunseen nodes to be surrounded by known nodes\nand cannot embed new (sub)graphs. We leave how\nto develop KEPLER to do fully inductive KE with-\nout additional features as future work.\n7 Conclusion and Future Work\nIn this paper, we propose KEPLER, a simple but\neffective uniﬁed model for knowledge embedding\nand pre-trained language representation. We train\nKEPLER with both the KE and MLM objectives\nto align the factual knowledge and language rep-\nresentation into the same semantic space, and ex-\nperimental results on extensive tasks demonstrate\nits effectiveness on both NLP and KE applications.\nBesides, we propose Wikidata5M, a large-scale KG\ndataset to facilitate future research.\nIn the future, we will (1) explore advanced ways\nfor more smoothly unifying the two semantic space,\nincluding different KE forms and different train-\ning objectives, and (2) investigate better knowl-\nedge probing methods for PLMs to shed light on\nknowledge-integrating mechanisms.\nAcknowledgement\nThis work is supported by the National Key Re-\nsearch and Development Program of China (No.\n2018YFB1004503), the National Natural Science\nFoundation of China (NSFC No. U1736204,\n61533018, 61772302, 61732008), grants from\nInstitute for Guo Qiang, Tsinghua University\n(2019GQB0003) and Beijing Academy of Artiﬁ-\ncial Intelligence (BAAI2019ZD0502). Prof. Jian\nTang is supported by the Natural Sciences and En-\ngineering Research Council (NSERC) Discovery\nGrant and the Canada CIFAR AI Chair Program.\nXiaozhi Wang and Tianyu Gao are supported by\nTsinghua University Initiative Scientiﬁc Research\nProgram. We also thank our action editor, Prof.\nDoug Downey, and the anonymous reviewers for\ntheir consistent help and insightful suggestions.\nReferences\nIvana Balazevic, Carl Allen, and Timothy\nHospedales. 2019. TuckER: Tensor Factoriza-\ntion for Knowledge Graph Completion. In Pro-\nceedings of EMNLP-IJCNLP, pages 5185–5194.\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey\nLing, and Tom Kwiatkowski. 2019. Matching\nthe Blanks: Distributional Similarity for Rela-\ntion Learning. In Proceedings of ACL, pages\n2895–2905.\nAleksandar Bojchevski and Stephan Günnemann.\n2018. Deep Gaussian Embedding of Graphs:\nUnsupervised Inductive Learning via Ranking.\nIn Proceedings of ICLR.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating Embeddings for Modeling\nMulti-relational Data. In Advances in Neural\nInformation Processing Systems (NIPS), pages\n2787–2795.\nYixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu,\nChengjiang Li, Xu Chen, and Tiansi Dong. 2018.\nJoint Representation Learning of Cross-lingual\nWords and Entities via Attentive Distant Supervi-\nsion. In Proceedings of EMNLP, pages 227–237.\nYixin Cao, Lifu Huang, Heng Ji, Xu Chen, and\nJuanzi Li. 2017. Bridge Text and Knowledge by\nLearning Multi-Prototype Entity Mention Em-\nbedding. In Proceedings of ACL, pages 1623–\n1633.\nEunsol Choi, Omer Levy, Yejin Choi, and Luke\nZettlemoyer. 2018. Ultra-Fine Entity Typing. In\nProceedings of ACL, pages 87–96.\nRonan Collobert and Jason Weston. 2008. A uni-\nﬁed architecture for natural language processing:\nDeep neural networks with multitask learning.\nIn Proceedings of ICML, pages 160–167.\nAndrew M Dai and Quoc V Le. 2015. Semi-\nsupervised sequence learning. In Advances in\nNeural Information Processing Systems (NIPS),\npages 3079–3087.\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina Toutanova. 2019. BERT: Pre-\ntraining of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of\nNAACL-HLT, pages 4171–4186.\nTianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2019. FewRel\n2.0: Towards More Challenging Few-Shot Rela-\ntion Classiﬁcation. In Proceedings of EMNLP-\nIJCNLP, pages 6251–6256.\nTakuo Hamaguchi, Hidekazu Oiwa, Masashi\nShimbo, and Yuji Matsumoto. 2017. Knowledge\nTransfer for Out-of-Knowledge-Base Entities: A\nGraph Neural Network Approach. In Proceed-\nings of IJCAI, pages 1802–1808.\nWilliam L. Hamilton, Rex Ying, and Jure Leskovec.\n2017. Inductive Representation Learning on\nLarge Graphs. In Advances in Neural Infor-\nmation Processing Systems (NIPS), pages 1025–\n1035.\nXu Han, Zhiyuan Liu, and Maosong Sun. 2018a.\nNeural Knowledge Acquisition via Mutual At-\ntention Between Knowledge Graph and Text. In\nProceedings of AAAI, pages 4832–4839.\nXu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan\nYao, Zhiyuan Liu, and Maosong Sun. 2018b.\nFewRel: A Large-Scale Supervised Few-Shot\nRelation Classiﬁcation Dataset with State-of-\nthe-Art Evaluation. In Proceedings of EMNLP,\npages 4803–4809.\nHiroaki Hayashi, Zecong Hu, Chenyan Xiong, and\nGraham Neubig. 2020. Latent Relation Lan-\nguage Models. In Proceedings of AAAI, pages\n7911–7918.\nJeremy Howard and Sebastian Ruder. 2018. Uni-\nversal Language Model Fine-tuning for Text\nClassiﬁcation. In Proceedings of ACL, pages\n328–339.\nNora Kassner and Hinrich Schütze. 2020. Negated\nand Misprimed Probes for Pretrained Language\nModels: Birds Can Talk, But Cannot Fly. In\nProceedings of ACL, pages 7811–7818.\nSeyed Mehran Kazemi and David Poole. 2018.\nSimplE Embedding for Link Prediction in\nKnowledge Graphs. In Advances in Neural In-\nformation Processing Systems (NeurIPS), pages\n4284–4295.\nZhenzhong Lan, Mingda Chen, Sebastian Good-\nman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. 2020. ALBERT: A Lite BERT for Self-\nsupervised Learning of Language Representa-\ntions. In Proceedings of ICLR.\nYankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu,\nand Xuan Zhu. 2015. Learning Entity and Rela-\ntion Embeddings for Knowledge Graph Comple-\ntion. In Proceedings of AAAI, pages 2181–2187.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a.\nLinguistic Knowledge and Transferability of\nContextual Representations. In Proceedings of\nNAACL-HLT, pages 1073–1094.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang,\nQi Ju, Haotang Deng, and Ping Wang. 2020. K-\nBERT: Enabling Language Representation with\nKnowledge Graph. In Proceedings of AAAI,\npages 2901–2908.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and\nJianfeng Gao. 2019b. Multi-Task Deep Neural\nNetworks for Natural Language Understanding.\nIn Proceedings of ACL, pages 4487–4496.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei\nDu, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin\nStoyanov. 2019c. RoBERTa: A Robustly Op-\ntimized BERT Pretraining Approach. CoRR,\ncs.CL/1907.11692v1.\nRobert Logan, Nelson F. Liu, Matthew E. Pe-\nters, Matt Gardner, and Sameer Singh. 2019.\nBarack’s Wife Hillary: Using Knowledge\nGraphs for Fact-Aware Language Modeling. In\nProceedings of ACL, pages 5962–5971.\nLajanugen Logeswaran, Ming-Wei Chang, Ken-\nton Lee, Kristina Toutanova, Jacob Devlin, and\nHonglak Lee. 2019. Zero-Shot Entity Linking\nby Reading Entity Descriptions. In Proceedings\nof ACL, pages 3449–3460.\nMichael McCloskey and Neal J Cohen. 1989.\nCatastrophic interference in connectionist net-\nworks: The sequential learning problem. In Psy-\nchology of learning and motivation, volume 24,\npages 109–165. Elsevier.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gre-\ngory S. Corrado, and Jeffrey Dean. 2013. Dis-\ntributed Representations of Words and Phrases\nand their Compositionality. In Advances in\nNeural Information Processing Systems (NIPS),\npages 3111–3119.\nGeorge A. Miller. 1995. WordNet: A Lexi-\ncal Database for English. Commun. ACM,\n38(11):39–41.\nMyle Ott, Sergey Edunov, Alexei Baevski, An-\ngela Fan, Sam Gross, Nathan Ng, David Grang-\nier, and Michael Auli. 2019. fairseq: A Fast,\nExtensible Toolkit for Sequence Modeling. In\nProceedings of NAACL-HLT (Demonstrations),\npages 48–53.\nJeffrey Pennington, Richard Socher, and Christo-\npher Manning. 2014. GloVe: Global Vectors\nfor Word Representation. In Proceedings of\nEMNLP, pages 1532–1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. 2018. Deep Contextual-\nized Word Representations. In Proceedings of\nNAACL-HLT, pages 2227–2237.\nMatthew E. Peters, Mark Neumann, Robert Logan,\nRoy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge Enhanced\nContextual Word Representations. In Proceed-\nings of EMNLP-IJCNLP, pages 43–54.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language Models as\nKnowledge Bases? In Proceedings of EMNLP-\nIJCNLP, pages 2463–2473.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2020. E-BERT: Efﬁcient-Yet-Effective Entity\nEmbeddings for BERT. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP\n2020, pages 803–818.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving Language\nUnderstanding by Generative Pre-Training. In\nTechnical report, OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. 2020. Ex-\nploring the Limits of Transfer Learning with a\nUniﬁed Text-to-Text Transformer. Journal of\nMachine Learning Research, 21(140):1–67.\nRico Sennrich, Barry Haddow, and Alexandra\nBirch. 2016. Neural Machine Translation of\nRare Words with Subword Units. In Proceed-\nings of ACL, pages 1715–1725.\nBaoxu Shi and Tim Weninger. 2018. Open-World\nKnowledge Graph Completion. In Proceedings\nof AAAI, pages 1957–1964.\nJake Snell, Kevin Swersky, and Richard Zemel.\n2017. Prototypical Networks for Few-shot\nLearning. In Advances in Neural Information\nProcessing Systems (NIPS), pages 4077–4087.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and\nJian Tang. 2019. RotatE: Knowledge Graph\nEmbedding by Relational Rotation in Complex\nSpace. In Proceedings of ICLR.\nThéo Trouillon, Johannes Welbl, Sebastian Riedel,\nÉric Gaussier, and Guillaume Bouchard. 2016.\nComplex Embeddings for Simple Link Predic-\ntion. In Proceedings of ICML, pages 2071–2080.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is All you Need. In Advances in Neural\nInformation Processing Systems (NIPS), pages\n5998–6008.\nAlex Wang, Jan Hula, Patrick Xia, Raghaven-\ndra Pappagari, R. Thomas McCoy, Roma Pa-\ntel, Najoung Kim, Ian Tenney, Yinghui Huang,\nKatherin Yu, Shuning Jin, Berlin Chen, Ben-\njamin Van Durme, Edouard Grave, Ellie Pavlick,\nand Samuel R. Bowman. 2019a. Can You Tell\nMe How to Get Past Sesame Street? Sentence-\nLevel Pretraining Beyond Language Modeling.\nIn Proceedings of ACL, pages 4465–4476.\nAlex Wang, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel Bowman.\n2019b. GLUE: A Multi-Task Benchmark and\nAnalysis Platform for Natural Language Under-\nstanding. In Proceedings of ICLR.\nPeiFeng Wang, Jialong Han, Chenliang Li, and\nRong Pan. 2019c. Logic attention based neigh-\nborhood aggregation for inductive knowledge\ngraph embedding. In Proceedings of AAAI,\npages 7152–7159.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu\nWei, Xuanjing Huang, Jianshu Ji, Cuihong Cao,\nDaxin Jiang, and Ming Zhou. 2020. K-Adapter:\nInfusing Knowledge into Pre-Trained Models\nwith Adapters. CoRR, cs.CL/2002.01808v3.\nZhen Wang, Jianwen Zhang, Jianlin Feng, and\nZheng Chen. 2014. Knowledge Graph and Text\nJointly Embedding. In Proceedings of EMNLP,\npages 1591–1601.\nAdina Williams, Nikita Nangia, and Samuel Bow-\nman. 2018. A Broad-Coverage Challenge Cor-\npus for Sentence Understanding through Infer-\nence. In Proceedings of NAACL-HLT, pages\n1112–1122.\nLedell Wu, Fabio Petroni, Martin Josifoski, Sebas-\ntian Riedel, and Luke Zettlemoyer. 2020. Scal-\nable Zero-shot Entity Linking with Dense En-\ntity Retrieval. In Proceedings of EMNLP, pages\n6397–6407.\nRuobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan,\nand Maosong Sun. 2016. Representation Learn-\ning of Knowledge Graphs with Entity Descrip-\ntions. In Proceedings of AAAI, pages 2659–\n2665.\nWenhan Xiong, Jingfei Du, William Yang Wang,\nand Stoyanov Veselin. 2019. Pretrained En-\ncyclopedia: Weakly Supervised Knowledge-\nPretrained Language Model. In Proceedings\nof ICLR.\nIkuya Yamada, Hiroyuki Shindo, Hideaki Takeda,\nand Yoshiyasu Takefuji. 2016. Joint Learning\nof the Embedding of Words and Entities for\nNamed Entity Disambiguation. In Proceedings\nof CoNLL, pages 250–259.\nBishan Yang and Tom Mitchell. 2017. Leveraging\nKnowledge Bases in LSTMs for Improving Ma-\nchine Reading. In Proceedings of ACL, pages\n1436–1446.\nBishan Yang, Scott Wen-tau Yih, Xiaodong He,\nJianfeng Gao, and Li Deng. 2015. Embedding\nEntities and Relations for Learning and Infer-\nence in Knowledge Bases. In Proceedings of\nICLR.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G.\nCarbonell, Ruslan Salakhutdinov, and Quoc V .\nLe. 2019. XLNet: Generalized Autoregressive\nPretraining for Language Understanding. In Ad-\nvances in Neural Information Processing Sys-\ntems (NeurIPS), pages 5754–5764.\nPoorya Zaremoodi, Wray Buntine, and Gholam-\nreza Haffari. 2018. Adaptive Knowledge Shar-\ning in Multi-Task Learning: Improving Low-\nResource Neural Machine Translation. In Pro-\nceedings of ACL, pages 656–661.\nYuhao Zhang, Victor Zhong, Danqi Chen, Ga-\nbor Angeli, and Christopher D. Manning. 2017.\nPosition-aware Attention and Supervised Data\nImprove Slot Filling. In Proceedings of EMNLP,\npages 35–45.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE:\nEnhanced Language Representation with Infor-\nmative Entities. In Proceedings of ACL, pages\n1441–1451.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. 2015. Aligning Books\nand Movies: Towards Story-Like Visual Expla-\nnations by Watching Movies and Reading Books.\nIn Proceedings of ICCV, pages 19–27.\nZhaocheng Zhu, Shizhen Xu, Jian Tang, and Meng\nQu. 2019. GraphVite: A High-Performance\nCPU-GPU Hybrid System for Node Embedding.\nIn Proceedings of WWW, pages 2494–2504.",
  "topic": "Kepler",
  "concepts": [
    {
      "name": "Kepler",
      "score": 0.8115634322166443
    },
    {
      "name": "Embedding",
      "score": 0.7649319171905518
    },
    {
      "name": "Computer science",
      "score": 0.7341761589050293
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.672900378704071
    },
    {
      "name": "Representation (politics)",
      "score": 0.6192659735679626
    },
    {
      "name": "Language model",
      "score": 0.5813341736793518
    },
    {
      "name": "Natural language processing",
      "score": 0.5557083487510681
    },
    {
      "name": "Construct (python library)",
      "score": 0.5358845591545105
    },
    {
      "name": "ENCODE",
      "score": 0.5007345676422119
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4838375151157379
    },
    {
      "name": "Programming language",
      "score": 0.11685323715209961
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Stars",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I108192572",
      "name": "HEC Montréal",
      "country": "CA"
    }
  ],
  "cited_by": 77
}