{
  "title": "Validating Large Language Models for Identifying Pathologic Complete Responses After Neoadjuvant Chemotherapy for Breast Cancer Using a Population-Based Pathologic Report Data",
  "url": "https://openalex.org/W4392949341",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4306233075",
      "name": "Cheligeer Cheligeer",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2148028513",
      "name": "Guosong Wu",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2628558160",
      "name": "Alison Laws",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2154338150",
      "name": "May Lynn Quan",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2115532713",
      "name": "Andrea Li",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A4312490176",
      "name": "Anne-Marie Brisson",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2306013573",
      "name": "Jason Xie",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2100452037",
      "name": "Yuan Xu",
      "affiliations": [
        "University of Calgary"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2065000080",
    "https://openalex.org/W2075894019",
    "https://openalex.org/W1884459103",
    "https://openalex.org/W2151068025",
    "https://openalex.org/W4312065903",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4387230383",
    "https://openalex.org/W2099552652",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2148143831",
    "https://openalex.org/W2131241448",
    "https://openalex.org/W3035965352",
    "https://openalex.org/W3008003211",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W6837149120",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4386932783",
    "https://openalex.org/W4365516941",
    "https://openalex.org/W2470052812",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3099878876"
  ],
  "abstract": "<title>Abstract</title> In the context of breast cancer management, the accurate determination of pathologic complete response (pCR) from large narrative pathology reports is pivotal for cancer outcome and survivorship studies. Leveraging the Large Language Models (LLMs) in digital pathology, our study developed and validated methods for identifying pCR from pathology reports of 351 breast cancer patients who underwent neoadjuvant chemotherapy. The optimum method demonstrated a sensitivity of 100.0% (95%CI: 100.0-100.0%), positive predictive value of 84.0% (95%CI: 70.0-96.8%), and F1 score of 91.3% (95%CI: 81.5–98.1%). These algorithms, integrating diverse LLMs, exhibited superior performance compared to traditional machine learning models. Our findings suggest LLMs hold significant potential utility in clinical pathology for extracting critical information from textual data.",
  "full_text": "Page 1/17\nValidating Large Language Models for Identifying\nPathologic Complete Responses After Neoadjuvant\nChemotherapy for Breast Cancer Using a\nPopulation-Based Pathologic Report Data\nCheligeer Cheligeer \nCentre for Health Informatics, Cumming School of Medicine, University of Calgary, Calgary\nGuosong Wu \nCentre for Health Informatics, Cumming School of Medicine, University of Calgary, Calgary\nAlison Laws \nDepartment of Surgery, Cumming School of Medicine, University of Calgary, Calgary\nMay Lynn Quan \nDepartment of Oncology, Cumming School of Medicine, University of Calgary, Calgary\nAndrea Li \nCentre for Health Informatics, Cumming School of Medicine, University of Calgary, Calgary\nAnne-Marie Brisson \nDepartment of Oncology, Cumming School of Medicine, University of Calgary, Calgary\nJason Xie \nCentre for Health Informatics, Cumming School of Medicine, University of Calgary, Calgary\nYuan Xu \nDepartment of Oncology, Cumming School of Medicine, University of Calgary, Calgary\nResearch Article\nKeywords:\nPosted Date: March 19th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-4004164/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nPage 2/17\nVersion of Record: A version of this preprint was published at BMC Medical Informatics and Decision\nMaking on October 3rd, 2024. See the published version at https://doi.org/10.1186/s12911-024-02677-y.\nPage 3/17\nAbstract\nIn the context of breast cancer management, the accurate determination of pathologic complete\nresponse (pCR) from large narrative pathology reports is pivotal for cancer outcome and survivorship\nstudies. Leveraging the Large Language Models (LLMs) in digital pathology, our study developed and\nvalidated methods for identifying pCR from pathology reports of 351 breast cancer patients who\nunderwent neoadjuvant chemotherapy. The optimum method demonstrated a sensitivity of 100.0%\n(95%CI: 100.0-100.0%), positive predictive value of 84.0% (95%CI: 70.0-96.8%), and F1 score of 91.3%\n(95%CI: 81.5–98.1%). These algorithms, integrating diverse LLMs, exhibited superior performance\ncompared to traditional machine learning models. Our \u0000ndings suggest LLMs hold signi\u0000cant potential\nutility in clinical pathology for extracting critical information from textual data.\nINTRODUCTION\nPathologic complete response (pCR) is characterized by the absence of residual invasive malignant\ncells, whether with or without in situ disease1. It is a known prognostic factor for long-term outcomes of\nbreast cancer patients and is used to guide the course of adjuvant systemic therapy2–4. Presence of pCR\nis routinely documented in pathology reports for patients treated with neoadjuvant chemotherapy (NAC),\nand it is important to use such information for further reporting and research purposes. The detection of\npCR from Electronic Medical Records (EMR) primarily serves the purposes of comprehensive reporting,\nadvancing health research, and supporting public health surveillance. This targeted approach ensures\nthe effective use of pCR information in key areas that directly contribute to improved patient care and the\nadvancement of breast cancer management strategies. However, the challenge lies in the fact that pCR\ndata is not always stored in a structured format within EMRs. Consequently, manually abstracting pCR\nfrom narrative reports is both time-consuming and costly, a situation that is particularly burdensome\nwhen dealing with large, population-based datasets.\nEMR phenotyping is the process of extracting speci\u0000c clinical characteristics and patterns from EMRs\nusing advanced algorithms and data mining techniques5. This method is essential for classifying\npatients into clinical categories automatically, thereby enabling the detection of the presence of pCR in\nbreast cancer patients become feasible6.\nTrained with billions of parameters, Large Language Models (LLMs) can tackle a diverse set of natural\nlanguage tasks, extracting intricate patterns and crucial insights from text data7. Representing a\npromising development in digital pathology, LLMs have the potential to signi\u0000cantly enhance the\ncomprehension and analysis of pathology reports 8. Models like bidirectional encoder transformers\n(BERT)9 and generative pretrained transformers (GPT)10 are trained to predict the likelihood of word\nsequences based on contextual understanding, enabling the generation of coherent and contextually\npertinent outputs. LLMs demonstrate considerable promise in augmenting EMR phenotyping technology\nfor pathology reports, offering enhanced interpretation and analysis capabilities that could lead to the\nautomation of data collection in research settings. Therefore, leveraging LLMs, this study aims to\nPage 4/17\ndevelop and validate data pipelines for the identi\u0000cation of pCR from narrative pathology reports of\nbreast cancer patients who underwent neoadjuvant chemotherapy NAC.\nMETHODS\nCohort and Data\nThis retrospective cohort study included all female non-metastatic invasive breast cancer patients\nadmitted to any of the acute care hospitals in Calgary, Alberta, Canada, between 1 January 2010 and 31\nDecember 2017. The study excluded patients with a diagnosis of multiple breast primary tumors. The\nstudy followed the Standards for Reporting of Diagnostic Accuracy Study (STARD)11 and was approved\nby Health Research Ethics Board of Alberta–Cancer Committee. A waiver of consent was granted.\nThe data were retrieved from Sunrise Clinical Manager (SCM), which is an EMR system universally\napplied in all four acute care hospitals in Calgary. Final surgical pathology reports, containing\ncomprehensive biomarker and histopathology evaluations in raw free-text format, were utilized for the\ndevelopment of pCR phenotyping algorithms. Patient text pathology report without a lymph node\nevaluation report were excluded because true pCR achievement cannot be veri\u0000ed without lymph node\nevaluation. All databases then were linked by patient personal health care number (PHN) and unique\nlifetime identi\u0000ers (ULIs). Patient records without a valid PHN or ULI were excluded.\nDe\u0000nition and Ascertainment of pCR\nEach pathology report was manually reviewed by a breast radiology fellow as the gold standard for the\npresence of pCR, de\u0000ned as the absence of residual invasive malignant cells either with or without the\npresence of in situ disease6. All uncertainties raised from chart review were discussed and resolved to\nensure that they satis\u0000ed the case de\u0000nition. The data extraction agreement between the physician and\na senior pathologist were te sted to con\u0000rm the presence of pCR for the \u0000rst 10 charts, and the result\nwas excellent (kappa = 1).\nPipeline Development\nTo assess the capabilities of Large Language Models (LLMs) in interpreting natural text within pathology\nreports, our research encompassed three innovative pipelines. Firstly, we utilized the GPT Applied\nProgramming Interfaces (API), deploying it for direct processing and insightful analysis of the reports.\nComplementing this, we ventured into adapting smaller-scaled LLMs within a local computing\nenvironment to capture the unique linguistic characteristics of pathology texts. Lastly, our approach\nexpanded to a tailored system, where we embedded LLMs as a crucial layer in a neural network\narchitecture. This network was meticulously \u0000ne-tuned with the goal of enhancing its pro\u0000ciency in\nunderstanding complex medical terminology, demonstrating the diverse potential of LLMs in medical\ntext analysis.\nPage 5/17\nPipeline A - Identifying pCR from de-identi\u0000ed reports with\nGPT APIs\nFor the de-identi\u0000ed notes, we utilized the APIs from OpenAI’s GPT versions 3.5 and 4. The experiment\nfollowed a two-step sequential strategy. In the \u0000rst step, GPT-3.5 was utilized to condense the de-\nidenti\u0000ed pathology reports, retaining only relevant information. The guiding prompt stated: “As a\npathologist analyzing a pathology report, provide a summary that emphasizes key details like diagnosis,\ngross description, and vital \u0000gures such as size and weight. Ensure patient-speci\u0000c and de-identi\u0000ed\ndata is excluded from the summary.” The de-identi\u0000ed pathology reports will follow the guiding prompt.\nAfter this summarization, the reports averaged 315.5 words, a signi\u0000cant decrease from the original\n5693.7 words, making the process of identifying pCR from the reports more e\u0000cient. In the second step,\nthe GPT-4 API was used to determine if a patient achieved pCR based on the summarized pathology\nreport, guided by our speci\u0000c prompt:\n“####Instruction: Without prior knowledge about pathologic complete response (pCR), adhere strictly to\nthe following guidelines: pCR in this research is described as the absence of invasive cancer in the\nbreast and/or axillary lymph nodes, excluding DCIS.\n1) Disregard any details related to DCIS.\n2) Rely solely on the provided pCR de\u0000nition.\n3) Determine whether pCR has been achieved.\nIs a pCR identi\u0000ed in the given report.\n####The report starts below: [insert report here]\n####You need to answer Yes/No \u0000rst, then provide a\nsimple explanation.”\nPipeline B - Identify pCR with custom machine learning pipelines using smaller LLMs on local\nenvironments with original patient report.\nAlthough our data was de-identi\u0000ed manually by two team members, the process was time consuming.\nConsidering that many users would not have capacity to manually de-identify their data, we proposed the\nsecond pipeline using smaller LLMs on local environment to overcome this data security issue facing\nexisting commercial cloud-hosted LLMs. To achieve this, we utilized locally deployable models from the\nBidirectional Encoder Representations from Transformers (BERT)9, BART12, T513 and GPT14 families,\nwhich have shown considerable promise in text classi\u0000cation tasks.\nPreprocessing and feature extraction\nPage 6/17\nThese locally deployable LLMs have a limitation on the input length of text. To address this limitation, we\nimplemented a strategy that segments longer texts into manageable chunks. Each chunk is carefully\ncrafted to align with the model's token capacity, ensuring optimal processing without compromising on\ninformation integrity.\nWhile dividing the text, we ensured a certain degree of overlap between consecutive chunks. This\noverlapping strategy is crucial as it preserves the inherent contextual information that might be lost at\nthe boundaries of individual segments. Once the chunks were prepared, any segment that was shorter\nthan the model's maximum token capacity was supplemented with padding, which is a common\ntechnique in data processing involves adding extra, non-informative content (often zeros) to standardize\ninput lengths for consistent processing in machine learning models. Alongside, an attention mask was\ngenerated for each segment, highlighting the actual content, and differentiating it from the added\npadding. This approach ensured that the models would remain attentive to the genuine content during\ntheir processing phase, ignoring the arti\u0000cial padding. As each of these text chunks was fed into the\nrespective LLM, we derived embeddings for every token. By analyzing and averaging these embeddings,\nwe could obtain a singular, condensed vector representation for each chunk, capturing its essential\nmeaning.\nFinally, to construct a holistic representation of the entire document, we aggregated these individual\nchunk vectors. Through a strategic combination process, a uni\u0000ed vector representation was produced,\nre\u0000ecting the comprehensive essence of the longer text. This vector then served as a foundation for\nvarious downstream applications, ensuring that the insights derived were both accurate and re\u0000ective of\nthe document's entirety.\nBinary Classi\u0000er\nThe extracted embeddings were fed to down streaming binary classi\u0000er. To facilitate a comprehensive\nassessment, the primary dataset was divided into training and testing subsets, with an 80 − 20\ndistribution respectively. This strategic split was crucial to ensure that the models were not only accurate\nbut also showcased a high degree of generalizability on unseen data. In addition, we noted that the\nnumber of patients with the presence of pCR was relatively lower than those without it. This is a\ncommon scenario in machine learning projects dealing with medical data, where a balanced dataset is\ncrucial for accurate analysis.\nTo rectify this, we used the Synthetic Minority Over-sampling Technique (SMOTE)15, which allows the\nminority class to be over-sampled by synthesizing new examples in the feature space. After processing\nthe data for classi\u0000cation, we used Bayesian optimization16 to choose optimal hyperparameters for a\nlogistic regression classi\u0000er. This method creates a probabilistic model of the objective function, guiding\nthe selection of hyperparameters, such as regularization strength, learning rate and tolerance.\nData Pipeline C - local hosted LLMs with advanced \u0000ne-tuning to unlock its potential on understanding\ntextual data.\nPage 7/17\nIn our third pipeline, we endeavored to explore the potential enhancements in model performance\nthrough optimization, particularly focusing on \u0000ne-tuning techniques. To this end, the GPT-2 small\nvariant, a smaller instantiation of the GPT-2 architecture, served as the foundational language model. The\nmain idea behind this was to harness the inherent capabilities of GPT-2 and augment it for our speci\u0000c\npCR detection task.\nModel Architecture\nThe neural architecture attached to the GPT-2 consisted of two main components. After processing the\ninput sequence through the GPT-2 model, its output underwent a mean pooling operation to condense\nthe sequence representation. This method averaged all embeddings, capturing the contextual essence of\nthe entire sequence. These embeddings were subsequently fed into a fully connected layer. The weights\nof this layer were initialized using the Xavier normal initializer17 to ensure optimal backpropagation. The\noutput of this layer was then passed through a sigmoid activation function, producing a probability score\nrepresentative of the pCR classi\u0000cation.\nFine-Tuning Procedure\nFor \u0000ne-tuning purpose, we adopted a technique called Low-Rank Adaptation of Large Language Models\n(LoRA)18. LoRA curtails the count of adjustable parameters for specialized tasks by incorporating\ntrainable rank decomposition matrices into the Transformer's every layer. This signi\u0000cant reduction in\nadjustable parameters and computational resource need allows for agile task transitioning during\ndeployment without any added latency. This greatly reduces the number of trainable parameters and\ncomputational resource requirements for LLMs adapted to speci\u0000c tasks, enabling e\u0000cient task-\nswitching during deployment without introducing inference latency.\nMoreover, to \u0000ne-tune our model's learning rate, we utilized the Adam optimizer19 in tandem with a\nlearning rate scheduler. This scheduler systematically modi\u0000ed the learning rate, enhancing the training\nregimen throughout the learning phase.\nGPT: Generative Pre-trained Transformer; LLM: Large Language Model; ML: Machine Learning; pCR:\nPathologic Complete Response\nStatistical Analysis\nDescriptive analysis of text data was summarized, with median and interquartile range (IQR). Differences\nin text data characteristics between reports with presence of pCR vs. absence of pCR were compared\nusing a Wilcoxon signed-rank test, employing a signi\u0000cance level (alpha) of 0.05. Each pipeline was\nevaluated against the chart reviewed pCR data using various metrics: Positive Predictive Value (PPV),\nsensitivity, speci\u0000city, Negative Predictive Value (NPV), F1-score, and accuracy. To enhance the reliability\nof our results, we employed a bootstrapping resampling method to determine the 95% con\u0000dence\ninterval for each of these metrics. Bootstrapping involved resampling with replacement from the original\ndata and recalculating the metrics for each resampled dataset. This procedure was repeated 10000\nPage 8/17\ntimes, and the con\u0000dence intervals were calculated from the empirical distribution of these metrics. The\nresults for each metric were reported with the point estimate and the 95% con\u0000dence interval. This\ndetailed reporting enabled a more nuanced understanding of each model's performance and facilitated\nmodel comparison. The computational analyses were carried out on a system equipped with an NVIDIA\nRTX 3080 16GB GPU and 32GB of DDR4 RAM. All statistical analyses were conducted using Python 3.10,\nNumPy20, SciPy21, and PyTorch22.\nRESULTS\nData Characteristics\nThe pathology text data was processed following the \u0000ow of Fig. 1. This study included 351 female\nbreast cancer patients who underwent NAC and subsequent curative-intent surgery. Of them, 102 (29%)\npatients achieved pCR after NAC as ascertained by manual chart review. Detailed patient demographic\nand clinical characteristics have been previously reported 6. The median report length was 1,316 words\n(IQR: 925, 1,631). Patients who achieved pCR tended to have shorter reports compared to those who did\nnot achieve pCR (p = 0.000). The median unique word count per report was 583 (IQR: 425, 754), with\npatients who achieved pCR exhibiting a lower median count of 436 (IQR: 335, 518) compared to patients\nwho did not achieve pCR (M = 684, IQR: 517, 788). After the removal of unnecessary characters,\npunctuation, and special symbols, the median token count was 2,293 (IQR: 1,540, 2,786) per report. The\ntop ten most frequent words in the reports were summarized in Table 1.\nTable 1\nCharacteristics of Text Data in Pathology Report\nCharacteristics Total\n(n = 351)\nPatients with pCR\n(n = 102, 29%)\nPatients without pCR\n(n = 249, 71%)\np-value\nNumber ofreports 2 (1, 2) 1 (1, 1) 2 (1, 3) 0.000\nReport length inwords 1316 (925, 1631) 1005 (751, 1267)1449 (1100, 1743) 0.000\nUnique wordcount 583 (425, 754) 436 (335, 518) 684 (517, 788) 0.000\nNumber oftokens perreport\n2293 (1540, 2786) 1733 (1267, 2137)2553 (1943, 3009) 0.000\nTop 10 frequentkeywords* cavity, layer, strip,\u0000broglandular,scarred, nodule, slice,lesion, demonstrates,away\ncavity, layer, strip,slice, away,hemorrhagic, bed,uoq, totally, normal\nstrip, \u0000broglandular,scarred, nodule, lesion,demonstrates, away,gelatinous, implant,cavity\n-\nPage 9/17\nData presented as median and interquartile range (IQR)\nThe keywords are extracted with Term-Frequency Inverse Document Frequency (TF-IDF) algorithm.\nPerformance of Data Pipeline A\nThe inference from Chat GPT 4 exhibited a sensitivity of 94.1% (CI: 89.1–98.1%), PPV of 89.7% (95% CI:\n83.8–95.0%), overall accuracy of pCR classi\u0000cation at 95.2% (95% CI: 92.9–97.4%) and F1 score of\n91.9% (95% CI: 87.6–95.6%).\nPerformance of Data Pipeline B\nWe tested 15 LLMs in pipeline B (Table 2). The sensitivity ranged from 76.2–100.0%, while the PPV\nranged from 64.0–87.0%. The overall performance of the F1 score ranged from 69.6–91.3% and the GPT-\n2 Large model performed the best (highest F1 score and narrow 95% CI). Speci\u0000cally, BERT-based\nencoder models exhibited a sensitivity ranging from 90.5–100.0%, PPV ranging from 73.1–87.0%, and F1\nscores ranging from 80.8–90.9%. Encoder-to-decoder models (BART and T5) demonstrated a sensitivity\nrange of 76.2–100.0%, PPV ranging from 64.0–84.0%, and F1 scores from 69.6–91.3%. GPT-based\ndecoder models achieved a sensitivity range of 95.2–100.0%, PPV ranging from 80.0–84.0%, and F1\nscores of 87.0–91.3%.\nPage 10/17\nTable 2\nPipeline B Machine Learning Model Performance Statistics\nLLMs Sensitivity PPV Speci\u0000city NPV Accuracy F1_score\nEncoder onlymodels            \nBERT base model\n(uncased)9\n100.0(100.0-100.0)\n75.0(58.1–90.0)\n86.0(75.5–94.4)\n100.0(100.0-100.0)\n90.1(83.1–95.8)\n85.7(73.5–94.3)\nBERT base model\n(cased)9\n95.2 (83.3–100.0) 76.9(58.6–92.0)\n88.0 (78.0-96.1) 97.8(93.3–100.0)\n90.1(83.1–97.2)\n85.1(72.7–94.3)\nDistilBERT base\nmodel (uncased)23\n95.2 (84.2–100.0) 74.1(57.1–89.7)\n86.0(75.6–95.2)\n97.7(92.8–100.0)\n88.7(81.7–95.8)\n83.3(70.3–93.3)\nBioClinicalBERT 24 90.5 (75.0-100.0) 79.2(62.5–94.7)\n90.0(81.8–98.0)\n95.7(89.6–100.0)\n90.1(83.1–97.2)\n84.4(71.1–94.1)\nTiny BERT 25 90.5 (76.0-100.0) 73.1(57.1–89.7)\n86.0 (75.0-94.4) 95.6(88.4–100.0)\n87.3(78.9–94.4)\n80.8(66.7–91.7)\nBERT multilingualbase model\n(cased) 9\n95.2 (83.3–100.0) 87.0(72.2–100.0)\n94.0(86.4–100.0)\n97.9(93.3–100.0)\n94.4(88.7–98.6)\n90.9(81.1–98.0)\nGatorTronS26 100.0(100.0-100.0)\n75.0(58.1–89.7)\n86.0(75.5–94.4)\n100.0(100.0-100.0)\n90.1(83.1–95.8)\n85.7(74.3–94.7)\nEncoder-to-decoder models            \nBART (base-sized\nmodel)12\n100.0(100.0-100.0)\n84.0(68.4–96.4)\n92.0(84.0–98.0)\n100.0(100.0-100.0)\n94.4(88.7–98.6)\n91.3(80.0-98.2)\nBART (large-sized\nmodel) 12\n95.2 (84.0-100.0) 80.0(62.5–95.2)\n90.0(80.4–97.8)\n97.8(92.9–100.0)\n91.6(84.5–97.2)\n87.0(76.0-96.2)\nBART-large-mnli12 90.5 (75.0-100.0) 76.0(57.7–92.3)\n88.0(78.7–96.0)\n95.6(88.2–100.0)\n88.7(80.3–95.8)\n82.6(68.3–93.0)\nFLAN-T5 small27 76.2 (57.1–94.1) 64.0(44.4–81.5)\n82.0(70.4–91.7)\n89.1(79.1–97.8)\n80.3(70.4–88.7)\n69.6(50.0-83.6)\nLLMs: Large Language Models; NPV: Negative Predictive Value; PPV - Positive Predictive Value\nPage 11/17\nLLMs Sensitivity PPV Speci\u0000city NPV Accuracy F1_score\nEncoder onlymodels            \nT5-Large13 90.5 (77.3–100.0) 70.4(51.7–85.7)\n84.0(72.6–94.0)\n95.4(88.6–100.0)\n85.9(77.5–93.0)\n79.2(63.4–90.9)\nT5-Small13 90.5 (76.2–100.0) 70.4(52.4–87.5)\n84.0(72.9–93.6)\n95.4(88.2–100.0)\n85.9(77.5–93.0)\n79.2(65.0-90.6)\nDecoder models            \nGPT-2 Large14 100.0(100.0-100.0)\n84.0(70.0-96.8)\n92.0(83.9–98.1)\n100.0(100.0-100.0)\n94.4(87.3–98.6)\n91.3(81.5–98.1)\nGPT-214 95.2 (84.2–100.0) 80.0(63.6–94.4)\n90.0(81.2–98.0)\n97.8(92.9–100.0)\n91.6(85.9–97.2)\n87.0(76.2–96.4)\nLLMs: Large Language Models; NPV: Negative Predictive Value; PPV - Positive Predictive Value\nPerformance of Data Pipeline C\nThe LORA \u0000ne-tuning technique signi\u0000cantly improved the performance of the LLMs achieving a perfect\nsensitivity and NPV of 100% (95% CI: 100–100%). The PPV and accuracy reached 91.3% (95% CI: 77.3–\n100%) and 97.2% (95% CI: 93.0-100%), respectively. The F1 score outperformed all other models, peaking\nat 95.5% (95% CI: 87.5–100%). The decreasing training and validation loss, along with the improving\nmodel performance metrics, signify effective learning and enhanced classi\u0000cation abilities (Fig. 2).\nDISCUSSION\nOur study designed a novel application of LLMs in digital pathology for the determination of pCR among\nbreast cancer patients who underwent NAC and subsequent curative-intent surgery. We developed and\nvalidated three pipelines for processing pathology text data. Our \u0000ndings demonstrated that LLMs\noutperformed traditional ML models in the task of pCR detection, con\u0000rming their potential utility in\nextracting critical information from textual pathology data.\nA prevalent concern regarding the use of AI revolves around its reliability28. Prior efforts in assessing the\naccuracy of information extracted by LLMs ranged from 75.9–87.7%29. In our study, the optimized \u0000ne-\ntuned model achieved 100% sensitivity and NPV. This implies that the model can effectively identify all\npossible presence of pCR cases or exclude absence of pCR. Earlier studies focusing on deep learning for\nthe prediction of breast cancer recurrence reported AUC values ranging from 0.65 to 0.8330. In the\ncontext of detecting pCR, LLMs outperformed traditional ML algorithms previously developed by our\nteam using the same dataset31 (F1 score of 0.91 vs. 0.84). The use of LLMs enhanced the accuracy of\nPage 12/17\ninformation extraction, highlighting their e\u0000cacy and precision in extracting pertinent clinical\ninformation.\nThe pipelines developed in this study present a substantial time-saving advantage compared to manual\nchart reviews. Unlike chart reviews, for which the time commitment heavily relies on the cohort's size\nand becomes infeasible for large datasets, LLM scripts and prompts, once developed, can e\u0000ciently\nprocess millions of reports in a matter of seconds29. The pipelines developed in this study could be\nintegrated into the local clinical information collection system to accelerate diagnosis and enabling\npersonalized treatment strategies, further support population-based surveillance and facilitate cohort\nstudies in speci\u0000c clinical contexts.\nDespite achieving promising outcomes, an essential consideration that remains is the security of health\ndata while employing or training Large Language Models (LLMs). When using the OpenAI API—a cloud-\nbased computational service—user data was input to generate embeddings on request. However, this\nraises concern as most health data custodians adhere to stringent policies safeguarding against the\nsharing of patient and physician information with third-party services. For this reason, we invested\nsigni\u0000cant efforts to de-identify data for this pipeline, limiting its larger-scale utility.\nIn scenarios where de-identi\u0000ed data is not an option, deploying smaller-scale LLMs on local, secure\nmachines might be preferable. Our comparative analysis of various LLMs from different transformer\nfamilies was designed to assess the feasibility and capability of utilizing LLMs in the interpretation of\npathology reports within a secure environment. Moreover, the use of smaller pre-trained LLMs addresses\ncomputational complexity concerns, streamlining the inference process and enhancing the viability of AI\nmethods for real-world application. We developed a \u0000ne-tuned model adept at deciphering pathology\nreports, exhibiting near-perfect performance on the test dataset. The sensitivity of this optimum model\nwas 100%, compared to 81% in a previous study6 that employed decision tree classi\u0000er technique6,\nsuggesting our method could be implemented to identify pCR in future cohort studies. Our proposed\nalternative method operates seamlessly within the healthcare environment, circumventing potential data\nsecurity issues.\nWe acknowledge several limitations in our study. First, our evaluation was limited to 15 commonly used\nLLMs in the \u0000eld of medicine, and the performance of other pretrained LLMs remains unexplored.\nSpeci\u0000cally, Dialog-based AI such as ChatGPT was not included in our evaluation due to privacy and\nsecurity concerns. While it is anticipated that as an advanced language model, ChatGPT may offer better\nperformance than GPT-2, it is hosted by OpenAI which hindered its integration into our research\nframework and local system28.\nSecondly, while our models have been effectively validated within this study, integrating them into\nbroader clinical information systems might require further validation. This need arises because our\nmodels were developed using hospital data from a single region. To ensure they work accurately and\ne\u0000ciently in different clinical settings, additional compatibility assessments are necessary. Third, in our\nPage 13/17\nstudy, we adopted a chunking text processing method in pipelines B and C. This approach was\nnecessary due to the data intake constraints of Large Language Models. Chunking helps in managing\nlarge volumes of text data more e\u0000ciently. However, it's important to note that this method might\nsegment the text in a way that disrupts its natural continuity. As a result, the models might face\nchallenges in fully grasping wider context and nuances within the text, which is a crucial aspect to\nconsider in multidisciplinary applications where contextual understanding is key 32. Lastly, as our\nvalidation was performed using internal validation, future research should explore the robustness of the\nmodel against a diverse and independent dataset to ensure generalizability.\nCONCLUSION\nOur study demonstrates the e\u0000cacy of LLMs in digital pathology for precise pCR determination in breast\ncancer patients post-NAC treatment. The superior performance of the developed pipelines over\ntraditional ML models. These \u0000ndings highlight LLMs' potential in extracting key clinical data from\nnarrative reports, although external validation is needed in the future.\nDeclarations\nAuthors' Contributions: Authors' Contributions: C.C. and G.W. contributed equally to this work and are\nconsidered co-\u0000rst authors. Y.X., C.C., and G.W. contributed to the conception and design. C.C. and G.W.\ncontributed to the development of the analysis plan. C.C. implemented and \u0000ne-tuned the LLMs models\nand developed the machine learning models and generated results for tables and \u0000gures. G.W.\nconducted the statistical analysis and generated \u0000gure 1 and table 1-2, formatted them, and provided\ndescriptions. G.W., C.C., and Y.X. contributed to the interpretation of the data. G.W. drafted the\nIntroduction, Results, Statistical Analysis and Discussion. C.C. drafted the Methods and Discussion. All\nauthors assisted with the revision. All authors read and approved the \u0000nal manuscript.\nAcknowledgements: This project was supported by Canadian Cancer Society (CCS). Project Title:\nBuilding Pipeline to Transform Real-world data to Evidence to Improve Cancer Care.\nCompeting Interests: GW is supported by the Canadian Institutes of Health Research (CIHR) Fellowship,\nO'Brien Institute for Public Health Postdoctoral Scholarship and Cumming School of Medicine\nPostdoctoral Scholarship at the University of Calgary. The other authors have no con\u0000icts of interest to\nreport.\nData Availability: Owing to data sharing policies of the data custodians, the dataset cannot be made\npublicly accessible. It may only be shared with researchers in Alberta, subject to approval from the data\ncustodians.\nCode availability: The part of the code (experiment 2 and 3) used in this study is available at our private\nrepository https://github.com/clger007/pCR_paper_code.git . The complete code and \u0000ne-tuned models\nfor this study are available to the corresponding author upon academically reasonable request.\nPage 14/17\nEthics Approval and Consent to Participate: This study received approval from the Health Research\nEthics Board of Alberta – Cancer Committee, with a waiver of informed consent granted.\nConsent for publication: Not applicable as this manuscript does not contain any individual person’s data\nin any form that could be used to identify them.\nReferences\n1. Pathological Complete Response in Neoadjuvant Treatment of High-Risk Early-Stage Breast Cancer:\nUse as an Endpoint to Support Accelerated Approval Guidance for Industry. (Food and Drug\nAdministration, 2020).\n2. Mamounas, E. P. Impact of neoadjuvant chemotherapy on locoregional surgical treatment of breast\ncancer. Annals of Surgical Oncology 22, 1425-1433 (2015).\n3. Cortazar, P. et al. Pathological complete response and long-term clinical bene\u0000t in breast cancer: the\nCTNeoBC pooled analysis. The Lancet 384, 164-172 (2014).\n4. Korn, E., Sachs, M. & McShane, L. Statistical controversies in clinical research: assessing pathologic\ncomplete response as a trial-level surrogate end point for early-stage breast cancer. Annals of\nOncology 27, 10-15 (2016).\n5. Pathak, J., Kho, A. N. & Denny, J. C. Electronic health records-driven phenotyping: challenges, recent\nadvances, and perspectives. J Am Med Inform Assn 20, E206-E211, doi:10.1136/amiajnl-2013-\n002428 (2013).\n\u0000. Wu, G. S. et al. A New Method of Identifying Pathologic Complete Response After Neoadjuvant\nChemotherapy for Breast Cancer Patients Using a Population-Based Electronic Medical Record\nSystem. Ann Surg Oncol 30, 2095-2103, doi:10.1245/s10434-022-12955-6 (2023).\n7. Thirunavukarasu, A. J. et al. Large language models in medicine. Nature medicine 29, 1930-1940\n(2023).\n\u0000. Hart, S. N. et al. Organizational preparedness for the use of large language models in pathology\ninformatics. Journal of Pathology Informatics, 100338 (2023).\n9. Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies (Naacl Hlt 2019), Vol. 1,\n4171-4186 (2019).\n10. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving language understanding by\ngenerative pre-training. (2018).\n11. Bossuyt, P. M. et al. Towards complete and accurate reporting of studies of diagnostic accuracy: the\nSTARD initiative. Ann Clin Biochem 40, 357-363, doi:Doi 10.1258/000456303766476986 (2003).\n12. Lewis, M. et al. Bart: Denoising sequence-to-sequence pre-training for natural language generation,\ntranslation, and comprehension. arXiv preprint arXiv:1910.13461 (2019).\nPage 15/17\n13. Raffel, C. et al. Exploring the Limits of Transfer Learning with a Uni\u0000ed Text-to-Text Transformer. J\nMach Learn Res 21 (2020).\n14. Radford, A. et al. Language models are unsupervised multitask learners. OpenAI blog 1, 9 (2019).\n15. Chawla, N. V., Bowyer, K. W., Hall, L. O. & Kegelmeyer, W. P. SMOTE: Synthetic minority over-sampling\ntechnique. J Artif Intell Res 16, 321-357, doi:DOI 10.1613/jair.953 (2002).\n1\u0000. Snoek, J., Larochelle, H. & Adams, R. P. Practical bayesian optimization of machine learning\nalgorithms. Advances in neural information processing systems 25 (2012).\n17. Glorot, X. & Bengio, Y. in Proceedings of the thirteenth international conference on arti\u0000cial\nintelligence and statistics. 249-256 (JMLR Workshop and Conference Proceedings).\n1\u0000. Hu, E. J. et al. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685\n(2021).\n19. Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980\n(2014).\n20. Harris, C. R. et al. Array programming with NumPy. Nature 585, 357-362, doi:10.1038/s41586-020-\n2649-2 (2020).\n21. Virtanen, P. et al. SciPy 1.0: fundamental algorithms for scienti\u0000c computing in Python (vol 33, pg\n219, 2020). Nat Methods 17, 352-352, doi:10.1038/s41592-020-0772-5 (2020).\n22. Paszke, A. et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library. Adv Neur In\n32 (2019).\n23. Sanh, V., Debut, L., Chaumond, J. & Wolf, T. DistilBERT, a distilled version of BERT: smaller, faster,\ncheaper and lighter. arXiv preprint arXiv:1910.01108 (2019).\n24. Alsentzer, E. et al. in the 2nd Clinical Natural Language Processing Workshop. 72–78 (Association\nfor Computational Linguistics).\n25. Bhargava, P., Drozd, A. & Rogers, A. Generalization in NLI: Ways (not) to go beyond simple heuristics.\narXiv preprint arXiv:2110.01518 (2021).\n2\u0000. Peng, C. et al. A Study of Generative Large Language Model for Medical Research and Healthcare.\narXiv preprint arXiv:2305.13523 (2023).\n27. Chung, H. W. et al. Scaling instruction-\u0000netuned language models. arXiv preprint arXiv:2210.11416\n(2022).\n2\u0000. Lee, P., Bubeck, S. & Petro, J. Bene\u0000ts, limits, and risks of GPT-4 as an AI chatbot for medicine. New\nEngland Journal of Medicine 388, 1233-1239 (2023).\n29. Choi, H. S., Song, J. Y., Shin, K. H., Chang, J. H. & Jang, B.-S. Developing prompts from large language\nmodel for extracting clinical information from pathology and ultrasound reports in breast cancer.\nRadiation Oncology Journal 41, 209 (2023).\n30. Howard, F. M. et al. Integration of clinical features and deep learning on pathology for the prediction\nof breast cancer recurrence assays and risk of recurrence. NPJ Breast Cancer 9, 25 (2023).\nPage 16/17\n31. Wu, G. et al. A New Method of Identifying Pathologic Complete Response following Neoadjuvant\nChemotherapy for Breast Cancer Patients Using a Population-Based Electronic Medical Record\nSystem. Annals of surgical oncology (2022).\n32. Ramkumar, P. et al. Chunking as the result of an e\u0000ciency computation trade-off. Nature\ncommunications 7, 12176 (2016).\nFigures\nFigure 1\nAnalytical Pathways for Processing Text Data in Pathology Reports\nGPT: Generative Pre-trained Transformer; LLM: Large Language Model; ML: Machine Learning; pCR:\nPathologic Complete Response\nPage 17/17\nFigure 2\nTraining and Validation Loss along with Performance Metrics in Fine-tuning Logistic Regression\nClassi\u0000er",
  "topic": "Breast cancer",
  "concepts": [
    {
      "name": "Breast cancer",
      "score": 0.7504799365997314
    },
    {
      "name": "Chemotherapy",
      "score": 0.5759260058403015
    },
    {
      "name": "Medicine",
      "score": 0.5616135597229004
    },
    {
      "name": "Oncology",
      "score": 0.4808817207813263
    },
    {
      "name": "Neoadjuvant therapy",
      "score": 0.4545844793319702
    },
    {
      "name": "Population",
      "score": 0.44268402457237244
    },
    {
      "name": "Cancer",
      "score": 0.42074161767959595
    },
    {
      "name": "Pathology",
      "score": 0.38204509019851685
    },
    {
      "name": "Internal medicine",
      "score": 0.28766658902168274
    },
    {
      "name": "Environmental health",
      "score": 0.0
    }
  ]
}