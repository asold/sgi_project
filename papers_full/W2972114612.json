{
  "title": "On Extractive and Abstractive Neural Document Summarization with Transformer Language Models",
  "url": "https://openalex.org/W2972114612",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2325938190",
      "name": "Subramanian, Sandeep",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200859777",
      "name": "Li, Raymond",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225502144",
      "name": "Pilault, Jonathan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4213710450",
      "name": "Pal, Christopher",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2251023345",
    "https://openalex.org/W2519091744",
    "https://openalex.org/W2799064010",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2951777553",
    "https://openalex.org/W2069143585",
    "https://openalex.org/W2540404261",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2931702774",
    "https://openalex.org/W2467173223",
    "https://openalex.org/W2341401723",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W2793613791",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2182959134",
    "https://openalex.org/W2579653291",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W3101913037",
    "https://openalex.org/W2897139265",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W1989420837",
    "https://openalex.org/W2304113845",
    "https://openalex.org/W2953280096",
    "https://openalex.org/W2574535369"
  ],
  "abstract": "We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We show that this extractive step significantly improves summarization results. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher rouge scores. Note: The abstract above was not written by the authors, it was generated by one of the models presented in this paper.",
  "full_text": "On Extractive and Abstractive Neural Document Summarization\nwith Transformer Language Models\nSandeep Subramanian1,2,3,∗ , Raymond Li1,∗, Jonathan Pilault1,2,4,∗, Christopher Pal1,2,4,5\n1Element AI, 2Montréal Institute for Learning Algorithms, 3Université de Montréal,\n4École Polytechnique de Montréal, 5Canada CIFAR AI Chair\n1{jonathan.pilault}@elementai.com\nAbstract\nWe present a method to produce abstractive summaries of\nlong documents that exceed several thousand words via neu-\nral abstractive summarization. We perform a simple extrac-\ntive step before generating a summary, which is then used\nto condition the transformer language model on relevant in-\nformation before being tasked with generating a summary.\nWe show that this extractive step signiﬁcantly improves sum-\nmarization results. We also show that this approach produces\nmore abstractive summaries compared to prior work that em-\nploys a copy mechanism while still achieving higher rouge\nscores. Note: The abstract above was not written by the au-\nthors, it was generated by one of the models presented in this\npaper based on an earlier draft of this paper.\nIntroduction\nLanguage models (LMs) are trained to estimate the joint\nprobability of an arbitrary sequence of words or characters\nusing a large corpus of text. They typically factorize the joint\ndistribution of tokens p(x1,x2 ...x n) into a product of con-\nditional probabilities ∏n\ni p(xi|x<i). It is possible to use n-\ngram based models to estimate these conditional probabil-\nities via counts, relying on Markovian assumptions. How-\never, Markovian assumptions and the curse of dimension-\nality make it harder for n-gram LMs to model long range\ndependencies and learn smooth functions that can learn sim-\nilarities between words in the vocabulary. This has led to\na preference for recurrent or feed-forward neural language\nmodels (Bengio et al. 2003; Mikolov et al. 2010) in recent\nyears due to to their ability to learn expressive conditional\nprobability distributions (Radford et al. 2019).\nThe sequence-to-sequence (seq2seq) paradigm\n(Sutskever, Vinyals, and Le 2014) uses language mod-\nels that learn the conditional probability of one sequence\ngiven another. Here, a language model serves as a “decoder”\nthat is typically conditioned on a representation of an\ninput sequence produced by an encoder neural network.\nThese types of encoder-decoder architectures have been\nparticularly successful when applied to problems such as\nmachine translation (Bahdanau, Cho, and Bengio 2014)\n∗Equal contribution, order determined by coin ﬂip\nPreprint. Work in Progress.\nNeural Document Summarization with SentencePointer Networks and Transformer Language Models\nSandeep Subramanian1,2,3,⇤, Raymond Li1,⇤, Jonathan Pilault1,2,4,⇤,Christopher Pal1,2,4,51Element AI,2Montréal Institute for Learning Algorithms,3Université de Montréal,4École Polytechnique de Montréal,5Canada CIFAR AI Chair1{jonathan.pilault}@elementai.com\nAbstractWe demonstrate that Transformer language models are extremely promising atsummarizing long texts, and provide a new approach to deep summarization thatcan be used to generate more \"abstractive\" summaries. We show that our approachproduces more abstractive summaries than state-of-the-art methods without a copymechanism. We provide an application to text summarization of the arXiv andPubMed datasets, and show that our model outperforms other popular summa-rization techniques. We also discuss a simple neural extractive model based onpointers networks trained on documents and their salient sentences. We show thatthis model can be used to augment Transformer language models to generate bettersummarization results.Note: The abstract above was generated by one of themodels presented in this paper , as a summary of this paper .\n1 IntroductionLanguage models (LMs) are trained to estimate the joint probability of an arbitrary sequence ofwords or characters using a large corpus of text. They typically factorize the joint distribution oftokensp(x1,x2...xn)into a product of conditional probabilitiesQnip(xi|x<i). It is possible to usen-gram based models to estimate these conditional probabilities via counts, relying on Markovianassumptions. However, Markovian assumptions and the curse of dimensionality make it harder forn-gram LMs to model long range dependencies and learn smooth functions that can learn similaritiesbetween words in the vocabulary. This has led to a preference for recurrent or feed-forward neurallanguage models (Bengio et al.,2003;Mikolov et al.,2010) in recent years due to to their ability tolearn expressive conditional probability distributions (Merity et al.,2017;Radford et al.,2019).The sequence-to-sequence (seq2seq) paradigm (Sutskever et al.,2014) uses language models thatlearn the conditional probability of one sequence given another. Here, a language model servesas a “decoder” that is typically conditioned on a representation of an input sequence produced byan encoder neural network. These types of encoder-decoder architectures have been particularlysuccessful when applied to problems such as machine translation (Bahdanau et al.,2014) andabstractive summarization (Rush et al.,2015). The encoder and conditional decoder language modelsare often paramaterized as recurrent neural networks (RNNs). Attention mechanisms (Bahdanauet al.,2014) are used in the decoder to provide more informative conditioning on the representationsproduced by the encoder and to ease gradient ﬂow into the encoder. RNNs however, are limited bytheir sequential nature, making them 1) difﬁcult to optimize and learn for long sequences with longrange dependencies (Hochreiter,1998;Pascanu et al.,2013), and 2) hard to parallelize on modernhardware like GPUs, limiting their scalability.⇤Equal contribution, order determined by coin ﬂip\nPreprint. Sent to peer review, May 2019.\nNeural Document Summarization with SentencePointer Networks and Transformer Language Models\nSandeep Subramanian1,2,3,⇤, Raymond Li1,⇤, Jonathan Pilault1,2,4,⇤,Christopher Pal1,2,4,51Element AI,2Montréal Institute for Learning Algorithms,3Université de Montréal,4École Polytechnique de Montréal,5Canada CIFAR AI Chair1{jonathan.pilault}@elementai.com\nAbstractWe demonstrate that Transformer language models are extremely promising atsummarizing long texts, and provide a new approach to deep summarization thatcan be used to generate more \"abstractive\" summaries. We show that our approachproduces more abstractive summaries than state-of-the-art methods without a copymechanism. We provide an application to text summarization of the arXiv andPubMed datasets, and show that our model outperforms other popular summa-rization techniques. We also discuss a simple neural extractive model based onpointers networks trained on documents and their salient sentences. We show thatthis model can be used to augment Transformer language models to generate bettersummarization results.Note: The abstract above was generated by one of themodels presented in this paper , as a summary of this paper .\n1 IntroductionLanguage models (LMs) are trained to estimate the joint probability of an arbitrary sequence ofwords or characters using a large corpus of text. They typically factorize the joint distribution oftokensp(x1,x2...xn)into a product of conditional probabilitiesQnip(xi|x<i). It is possible to usen-gram based models to estimate these conditional probabilities via counts, relying on Markovianassumptions. However, Markovian assumptions and the curse of dimensionality make it harder forn-gram LMs to model long range dependencies and learn smooth functions that can learn similaritiesbetween words in the vocabulary. This has led to a preference for recurrent or feed-forward neurallanguage models (Bengio et al.,2003;Mikolov et al.,2010) in recent years due to to their ability tolearn expressive conditional probability distributions (Merity et al.,2017;Radford et al.,2019).The sequence-to-sequence (seq2seq) paradigm (Sutskever et al.,2014) uses language models thatlearn the conditional probability of one sequence given another. Here, a language model servesas a “decoder” that is typically conditioned on a representation of an input sequence produced byan encoder neural network. These types of encoder-decoder architectures have been particularlysuccessful when applied to problems such as machine translation (Bahdanau et al.,2014) andabstractive summarization (Rush et al.,2015). The encoder and conditional decoder language modelsare often paramaterized as recurrent neural networks (RNNs). Attention mechanisms (Bahdanauet al.,2014) are used in the decoder to provide more informative conditioning on the representationsproduced by the encoder and to ease gradient ﬂow into the encoder. RNNs however, are limited bytheir sequential nature, making them 1) difﬁcult to optimize and learn for long sequences with longrange dependencies (Hochreiter,1998;Pascanu et al.,2013), and 2) hard to parallelize on modernhardware like GPUs, limiting their scalability.⇤Equal contribution, order determined by coin ﬂip\nPreprint. Sent to peer review, May 2019.\nFigure 1: Proposed model for abstractive summarization of a sci-\nentiﬁc article. An older version of this paper is shown as the ref-\nerence document. First, a sentence pointer network extracts impor-\ntant sentences from the paper. Next, these sentences are provided\nalong with the whole scientiﬁc article to be arranged in the follow-\ning order: Introduction, extracted Sentences, abstract & the rest of\nthe paper. A transformer language model is trained on articles or-\nganized in this format. During inference, the introduction and the\nextracted sentences are given to the language model as context to\ngenerate a summary. In domains like news and patent documents,\nthe introduction is replaced by the entire document.\nand abstractive summarization (Rush, Chopra, and Weston\n2015). The encoder and conditional decoder language\nmodels are often parameterized as recurrent neural net-\nworks (RNNs). Attention mechanisms (Bahdanau, Cho,\nand Bengio 2014) are used in the decoder to provide more\ninformative conditioning on the representations produced\nby the encoder and to ease gradient ﬂow into the encoder.\nRNNs however, are limited by their sequential nature,\nmaking them 1) difﬁcult to optimize and learn for long\nsequences with long range dependencies (Hochreiter 1998;\nPascanu, Mikolov, and Bengio 2013), and 2) hard to\nparallelize on modern hardware like GPUs, limiting their\nscalability.\narXiv:1909.03186v2  [cs.CL]  28 Apr 2020\nThere has therefore been a recent shift towards feedfor-\nward architectures for sequential data, such as convolutional\nmodels (Kalchbrenner et al. 2016; Van Den Oord et al. 2016;\nGehring et al. 2017) or fully attentive models popularized by\narchitectures known as transformers (Vaswani et al. 2017).\nThese techniques have a logarithmic or constant path length\n(as opposed to linear in RNNs) between a network’s out-\nput and any of its inputs, making gradient ﬂow much easier,\nthereby opening up the possibility of learning very long term\ndependencies.\nThe abstractive summarization of news or scientiﬁc pa-\npers typically requires encoding and generating hundreds or\nthousands of words. Recent work by (Radford et al. 2019)\n(GPT-2) has demonstrated that transformers with a large re-\nceptive ﬁeld and trained on a lot of data yield language mod-\nels that are capable of capturing long range dependencies.\nIf one is interested in creating a coherent, high-quality\nsummary of long documents, such GPT-like architectures\npossess many desirable properties. Their results also show\nthat unconditional language models can implicitly learn to\nperform summarization or machine translation as a conse-\nquence of the data on which it is trained. If the data is format-\nted sequentially into different aspects of a document (intro-\nduction, body, summary), each divided by “tl;dr”, the model\ncan be coaxed to generate one of these aspects. For exam-\nple, the model can be made to solve a summarization task\nby presenting it similarly formatted data at test time; i.e. a\ndocument’s introduction and body followed by “tl;dr“ that\nwill generate an abstract from a language model conditioned\non this context.\nIn this work, we take this idea a step further by doing\naway with the sequence-to-sequence paradigm and format-\nting data for abstractive summarization in a manner that\ntransformer language models can make use of all of the\navailable data present in the documents and their summaries\n(akin to language model pre-training on mono-lingual data\nin machine translation (Gulcehre et al. 2015)). Speciﬁcally,\nwe use a single GPT-like Transformer LM (TLM) trained\non documents followed by their summaries. During infer-\nence, we generate from the LM, conditioned on the doc-\nument (see ﬁgure 1). Unlike most previous approaches to\nneural abstractive summarization, we do not use a seq2seq\nformulation with an explicit encoder and decoder for word\ngeneration. We split the task in two: an extractive step and an\nabstractive step (Chen and Bansal 2018; Gehrmann, Deng,\nand Rush 2018). To deal with extremely long documents\nthat exceed several thousand words, we ﬁrst perform sen-\ntence extraction using two different hierarchical document\nmodels - one based on pointer networks (Vinyals, Fortunato,\nand Jaitly 2015), similar to the variant proposed in (Chen\nand Bansal 2018) and the other based on a sentence clas-\nsiﬁer (Nallapati, Zhai, and Zhou 2017). This extracts im-\nportant sentences from the document (described in section\n) that can be used to better condition the transformer LM\non relevant information before being tasked with generating\na summary. We show that this extractive step signiﬁcantly\nimproves summarization results.\nThe contributions of this work are two fold:\n• We demonstrate that transformer language models are\nsurprisingly effective at summarizing long scientiﬁc ar-\nticles and outperform typical seq2seq approaches, even\nwithout a copy mechanism.\n• We show that our approach produces more “abstractive”\nsummaries compared to prior work that employs a copy\nmechanism (See, Liu, and Manning 2017) while still\nachieving higher ROUGE scores.\nRelated Work\nAutomatic summarization systems seek to condense the\nsize of a piece of text while preserving most of its im-\nportant information content and meaning. The earliest at-\ntempts at automatic summarization focused on extractive\ntechniques, which ﬁnd words or sentences in a document\nthat capture its most salient content. In the past, various\nsimilarity scores based on speciﬁc sentence features (key-\nwords, position, length, frequency, linguistic) and metrics\n(structure-based, vector-based and graph-based) were em-\nployed to estimate salience (Steinberger and Jezek 2004;\nErkan and Radev 2004) between a sentence in a document\nand its reference summary. More recently, with advances\nin distributed representations of words, phrases and sen-\ntences, researchers have proposed to use these to compute\nsimilarity scores. Such techniques were further reﬁned by\n(Nallapati, Zhou, and Ma 2016; Cheng and Lapata 2016;\nChen and Bansal 2018) with encoder-decoder architectures\n- the representations learned by the encoder are used to\nchoose the most salient sentences. (Cheng and Lapata 2016)\nand (Nallapati, Zhou, and Ma 2016) trained encoder-decoder\nneural networks as a binary classiﬁer to determine if each\nsentence in a document should belong to the extractive sum-\nmary or not. (Nallapati et al. 2016) also present an alterna-\ntive that can pick an unordered set of sentences from the\nsource document to assemble an extractive summary. (Chen\nand Bansal 2018) use a pointer network (Vinyals, Fortunato,\nand Jaitly 2015) to sequentially pick sentences from the doc-\nument that comprise its extractive summary.\nHuman summarizers have four common characteristics.\nThey are able to (1) interpret a source document, (2) pri-\noritize the most important parts of the input text, (3) para-\nphrase key concepts into coherent paragraphs and (4) gen-\nerate diverse output summaries. While extractive methods\nare arguably well suited for identifying the most relevant\ninformation, such techniques may lack the ﬂuency and co-\nherency of human generated summaries. Abstractive sum-\nmarization has shown the most promise towards address-\ning points (3) and (4) above. Abstractive generation may\nproduce sentences not seen in the original input document.\nMotivated by neural network success in machine translation\nexperiments, the attention-based encoder decoder paradigm\nhas recently been widely studied in abstractive summariza-\ntion (Rush, Chopra, and Weston 2015; Nallapati et al. 2016;\nChopra, Auli, and Rush 2016). By dynamically accessing\nthe relevant pieces of information based on the hidden states\nof the decoder during generation of the output sequence, the\nmodel revisits the input and attends to important informa-\ntion. The advantages of extractive, abstractive and attention-\nbased models were ﬁrst combined in (Gu et al. 2016) with a\ncopy mechanism for out-of-vocabulary words present in the\nsource document. Similarly, (See, Liu, and Manning 2017)\nused the attention scores to calculate the probability of gen-\nerating vs copying a word. A coverage mechanism was also\nadded to penalize the attention score of previously attended\nwords, diminishing the model’s tendency to repeat itself.\nFramework\nOur model comprises two distinct and independently train-\nable components 1) a hierarchical document representation\nmodel that either points to or classiﬁes sentences in a doc-\nument to build an extractive summary 2) a transformer lan-\nguage model that conditions on the extracted sentences as\nwell as a part of or the entire document.\nExtractive Models\nWe describe the two neural extractive models used in this\nwork in this section.\nHierarchical Seq2seq Sentence Pointer Our extrac-\ntive model is similar to the sentence pointer architecture\ndeveloped by (Chen and Bansal 2018) with the main dif-\nference being the choice of encoder. We use a hierarchical\nbidirectional LSTM encoder with word and sentence level\nLSTMs while (Chen and Bansal 2018) use a convolutional\nword level encoder for faster training and inference. The\ndecoder is in both cases is an LSTM.\nThe extractive model considers the document as a list of\nN sentences D= (S1,...,S N ), and each sentence as a list\nof tokens. We are given a ground-truth extracted summary of\nM sentences (Si1 ,...,S iM ), where the i1 < ... < iM are\nthe indices of the extracted sentences. The procedure to de-\ntermine ground-truth extraction targets are identical to previ-\nous work - ﬁnding two sentences in the document that have\nthe highest ROUGE score with each sentence in the sum-\nmary.\nWe use an encoder-decoder architecture for this extractor.\nThe encoder has a hierarchical structure that combines a to-\nken and sentence-level RNN. First, the “sentence-encoder”\nor token-level RNN is a bi-directional LSTM (Hochreiter\nand Schmidhuber 1997) encoding each sentence. The last\nhidden state of the last layer from the two directions pro-\nduces sentence embeddings: (s1,..., sN ), where N is the\nnumber of sentences in the document. The sentence-level\nLSTM or the “document encoder”, another bi-directional\nLSTM, encodes this sequence of sentence embeddings to\nproduce document representations: (d1,..., dN ).\nThe decoder is an autoregressive LSTM taking the\nsentence-level LSTM hidden state of the previously ex-\ntracted sentence as input and predicting the next extracted\nsentence. Let it the index of the previous extracted sentence\nat time step t. The input to the decoder issit, or a zero vector\nat time-step t= 0. The decoder’s output is computed by an\nattention mechanism from the decoder’s hidden stateht over\nthe document representations(d1,..., dN ). We used the dot\nproduct attention method from (Luong, Pham, and Manning\n2015). The attention weights at produce a context vector ct,\nwhich is then used to compute an attention aware hidden\nstate ˜ht. Following the input-feeding approach from (Luong,\nPham, and Manning 2015), the attention aware hidden state\n˜ht is concatenated to the input in the next time step, giving\nthe following recurrence ht = LSTM\n(\n[sT\nit\n˜hT\nt−1]T ,ht−1\n)\n,\nwith\n˜ht = W˜h\n[\nct\nht\n]\n, ct =\nN∑\ni=1\nat(i)di, αt(i) =dT\ni ht, (1)\nat(i) = exp (αt(i))∑\ni′ exp (αt(i′)), for i= 1..N. (2)\nThe attention weights at are used as output probability\ndistribution over the document sentences, of the choice for\nthe next extracted sentence. We choose the convention to\nsignal the end of the extraction by putting the same index\ntwice in a row. Thus, the input to the decoder is the follow-\ning sequence: 0,si1 ,..., siM , and the target: i1,...,i M ,iM\n, where M is the length of the ground-truth extracted sum-\nmary and both sequences have M + 1elements. The model\nis trained to minimize the cross-entropy of picking the cor-\nrect sentence at each decoder time step. At inference, we use\nbeam-search to generate the extracted summary.\nSentence Classiﬁer As with the pointer network, we use\na hierarchical LSTM to encode the document and produce\na sequence of sentence representations d1,..., dN where N\nis the number of sentences in the document. We compute a\nﬁnal document representation as follows:\nd = tanh\n(\nbd + Wd\n1\nN\nN∑\ni=1\ndi\n)\n(3)\nwhere bd and Wd are learnable parameters. Finally, the\nprobability of each sentence belonging to the extractive sum-\nmary is given by:\noi =σ\n(\nWo\n[\ndi\nd\n]\n+ bo\n)\n(4)\nwhere σ is the sigmoid activation function. The model is\ntrained to minimize the binary cross-entropy loss with re-\nspect to the sentences in the gold-extracted summary.\nModel Details The model uses word embeddings of size\n300. The token-level LSTM (sentence encoder), sentence-\nlevel LSTM (document encoder) and decoder each have 2\nlayers of 512 units and a dropout of 0.5 is applied at the\noutput of each intermediate layer. We trained it with Adam, a\nlearning rate 0.001, a weight decay of10−5, and using batch\nsizes of 32. We evaluate the model every200 updates, using\na patience of 50. At inference, we decode using beam search\nwith a beam size of 4 for the pointer model and pick the k\nmost likely sentences from the sentence classiﬁer, where k\nis the average number of sentences in the summary across\nthe training dataset.\nTransformer Language Models (TLM)\nInstead of formulating abstractive summarization as a\nseq2seq problem using an encoder-decoder architecture, we\nonly use a single transformer language model that is trained\nfrom scratch, with appropriately “formatted” data (see ﬁgure\n1, we also describe the formatting later in this section).\nWe use a transformer (Vaswani et al. 2017) language\nmodel (TLM) architecture identical to (Radford et al. 2019).\nOur model has 220M parameters with 20 layers, 768 dimen-\nsional embeddings, 3072 dimensional position-wise MLPs\nand 12 attention heads. The only difference in our architec-\ntures (to our knowledge) is that we do not scale weights at\ninitialization. We trained the language model for 5 days on\n16 V100 GPUs on a single Nvidia DGX-2 box. We used a\nlinear ramp-up learning rate schedule for the ﬁrst40,000 up-\ndates, to maximum learning rate of 2.5 ×e−4 followed by a\ncosine annealing schedule to 0 over the next 200,000 steps\nwith the Adam optimizer. We used mixed-precision training\n(Micikevicius et al. 2017) with a batch size of 256 sequences\nof 1024 tokens each.\nIn order to get an unconditional language model to do ab-\nstractive summarization, we can use the fact that LMs are\ntrained by factorizing the joint distribution over words au-\ntoregressively. We organized the training data for the LM\nsuch that the ground-truth summary follows the information\nused by the model to generate a system summary. This way,\nwe model the joint distribution of document and summary\nduring training, and sample from the conditional distribution\nof summary given document at inference.\nWhen dealing with extremely long documents that may\nnot ﬁt into a single window of tokens seen by a transformer\nlanguage model, such as an entire scientiﬁc article, we use\nits introduction as a proxy for having enough information to\ngenerate an abstract (summary) and use the remainder of the\npaper as in domain language model training data (Fig 1). In\nsuch cases, we organize the arXiv and PubMed datasets as\nfollows: 1) paper introduction 2) extracted sentences from\nthe sentence pointer model 3) abstract 4) rest of the paper.\nOn other datasets, the paper introduction would be the entire\ndocument and there would no rest of the paper. This ensures\nthat at inference, we can provide the language model the pa-\nper introduction and the extracted sentences as conditioning\nto generate its abstract. We found that using the ground truth\nextracted sentences during training and the model extracted\nsentences at inference performed better than using the model\nextracted sentences everywhere.\nWe use a special token to indicate the start of the sum-\nmary and use it at test time to signal to the model to start\ngenerating the summary. The rest of the article is provided\nas additional in-domain training data for the LM. The en-\ntire dataset is segmented into non-overlapping examples of\n1,024 tokens each. We use “topk” sampling at inference (?;\nRadford et al. 2019), withk= 30and a softmax temperature\nof 0.7 to generate summaries.\nFigure 2: n-gram overlaps between the abstracts generated\nby different models and the input article on the arXiv dataset.\nWe show in detail which part of the input was copied for our\nTLM conditioned on intro + extract.\nResults and Analysis\nExperimental setup\nDatasets We experiment with four different large-scale\nand long document summarization datasets - arXiv, PubMed\n(Cohan et al. 2018), bigPatent (Sharma, Li, and Wang 2019)\nand Newsroom (Grusky, Naaman, and Artzi 2018). Statistics\nare reported in Table 1.\nTable 1: Statistics from (Sharma, Li, and Wang 2019) for\nthe datasets used in this work - The number of docu-\nment/summary pairs, the ratio of the number of words in\nthe document to the abstract and the number of words in the\nsummary and document.\nDataset #Documents\nComp\nRatio\nSum\nLen\nDoc\nLen\narXiv 215,913 39.8 292.8 6,913.8\nPubMed 133,215 16.2 214.4 3,224.4\nNewsroom 1,212,726 43.0 30.4 750.9\nBigPatent 1,341,362 36.4 116.5 3,572.8\nData preprocessing Both our extractive and abstractive\nmodels use sub-word units computed using byte pair en-\ncoding (Sennrich, Haddow, and Birch 2015) with 40,000\nreplacements. To address memory issues in the sentence\npointer network, we only keep300 sentences per article, and\n35 tokens per sentence.\nEvaluation We evaluate our method using full-length F-1\nROUGE scores (Lin 2004) and re-used the code from (Co-\nhan et al. 2018) for this purpose. All ROUGE numbers re-\nported in this work have a 95% conﬁdence interval of at most\n0.24.\nComparison We compare our results to several previously\nproposed extractive and abstractive models. All prior results\nreported on the arXiv and Pubmed benchmark are obtained\nfrom (Cohan et al. 2018). Similarly, prior results for the Big-\nPatent dataset are obtained from (Sharma, Li, and Wang\n2019) and Newsroom from (Grusky, Naaman, and Artzi\n2018) and (Mendes et al. 2019). These methods include\nLexRank (Erkan and Radev 2004), SumBasic (Vanderwende\net al. 2007), LSA (Steinberger and Jezek 2004), Attention-\nSeq2Seq (Nallapati et al. 2016; Chopra, Auli, and Rush\n2016), Pointer-Generator Seq2Seq (See, Liu, and Manning\n2017), Discourse aware, which is a hierarchical extension\nto the pointer generator model, (Cohan et al. 2018), Sent-\nrewriting (Chen and Bansal 2018), RNN-Ext (Chen and\nBansal 2018), Exconsumm (Mendes et al. 2019).\nDiscussion\nWe present our main results on summarizing arXiv and\nPubMed papers in tables 2, 4. Our extractive models are\nable to outperform previous extractive baselines on both the\narXiv and Pubmed datasets. Our TLM conditioned on the\nextractive summary produced by our best extractive model\n(TLM-I+E (G,M)) outperforms prior abstractive/mixed re-\nsults on the arXiv, Pubmed and bigPatent datasets, except on\nROUGE-L. On Newsroom, we do better than the only other\nabstractive model (Seq2Seq with attention) by a massive\nmargin and achieve better performance than the pointer gen-\nerator even on the abstractive and mixed which their model\nshould be better suited to since it has a copy mechanism. The\nExconsumm model (Mendes et al. 2019) however, which is\nprimarily an extractive model does better on this dataset. We\nsuspect the poor ROUGE-L result is due to the absence of\na copy mechanism that makes it hard to get exact large n-\ngram matches. Figure 2 further supports this hypothesis, it\nis evident that a model with a copy mechanism is often able\nto copy even upto 25-grams from the article. Further, (Gra-\nham 2015) ﬁnds that ROUGE-L is poorly correlated with\nhuman judgements when compared to ROUGE-1,2,3. In ta-\nble 7 and Table 8, we present qualitative results of abstracts\nof notable papers in our ﬁeld and of our TLM conditioned\non the introductions and extracted summaries of a random\nexample from the arXiv test set. Table 3 shows similar qual-\nitative examples on the Newsroom dataset. Tables 2, 4 and\n5 also provide different train / test settings for our TLM\nconditioned on extracted sentences. We show a performance\nupper bound conditioning the Transformer LM on oracle /\nground-truth extracted sentences at both train and test time\n(TLM-I+E (G,G)). We also experiment with using either the\nground-truth extracted sentences (TLM-I+E (G,M)) or the\nmodel extracted sentences (TLM-I+E (M,M)) during train-\ning and ﬁnd that latter slightly impairs performance. Finally,\nﬁgure 3, presents a visualization of the word embeddings\nlearned by our TLM.\nAbstractiveness of generated abstracts\n(Weber et al. 2018) argued that state-of-the-art abstractive\nsummarization systems that use a copy mechanism effec-\ntively generate the summary by copying over large chunks\nTable 2: Summarization results on the arXiv dataset. Previ-\nous work results from (Cohan et al. 2018). The following\nlines are a simple baseline Lead-10 extractor and the pointer\nand classiﬁer models. Our transformer LMs (TLM) are con-\nditioned either on the Introduction (I) or along with extracted\nsentences (E) either from ground-truth (G) or model (M) ex-\ntracts.\nModel Type ROUGE\n1 2 3 L\nPrevious Work\nSumBasic Ext 29.47 6.95 2.36 26.3\nLexRank Ext 33.85 10.73 4.54 28.99\nLSA Ext 29.91 7.42 3.12 25.67\nSeq2Seq Abs 29.3 6.00 1.77 25.56\nPointer-gen Mix 32.06 9.04 2.15 25.16\nDiscourse Mix 35.80 11.05 3.62 31.80\nOur Models\nLead-10 Ext 35.52 10.33 3.74 31.44\nSent-CLF Ext 34.01 8.71 2.99 30.41\nSent-PTR Ext 42.32 15.63 7.49 38.06\nTLM-I Abs 39.65 12.15 4.40 35.76\nTLM-I+E (M,M) Mix 41.15 13.98 5.63 37.40\nTLM-I+E (G,M) Mix 41.62 14.69 6.16 38.03\nOracle\nGold Ext Oracle 44.25 18.17 9.14 35.33\nTLM-I+E (G,G) Oracle 46.40 18.15 8.71 42.27\nTable 3: Qualitative Results - News articles and our model gener-\nated summaries on the NewsRoom dataset\nDocument — A new plan from the government of the Philippines would offer\nfree wireless internet to people across the country while also likely eating into the\nannual revenue of the nations telecoms. Bloomberg reports that the Philippines\ngovernment plans to roll-out its free Wi-Fi services to roughly half of the coun-\ntrys municipalities over the next few months and the country has its sights set on\nnationwide coverage by the end of 2016. The free wireless internet service will\nbe made available in public areas such as schools, hospitals, airports and parks,\nand is expected to cost the government roughly $32 million per year. [...]\nAbstractive — : The government is reportedly considering a nationwide service\nplan to give free Wi-Fi access to rural areas.\nMixed — The government of the Philippines is considering a new plan to provide\nfree wireless internet to the nation’s largest cities and towns.\nExtractive — The new plan will include free wireless internet to residents across\nthe country while also probably eating into the annual revenue of the country’s\ntelecoms.\nDocument — (CBS) - Controversy over a new Microsoft patent has people ques-\ntioning whether or not the intention has racist undertones. CNET reported that\nMicrosoft has been granted a U.S. patent that will steer pedestrians away from\nareas that are high in crime. [...]\nAbsractive Summary — The new Microsoft patent claims a device could pro-\nvide pedestrian navigation directions from a smartphone.\nMixed Summary Microsoft won a U.S. patent for a new way to steer pedestrians\nout of areas that are high in crime\nfrom the article, essentially doing “extractive” summariza-\ntion. Following this work, we measure how much a model\ncopies from the article by counting the proportion of n-\ngrams from the generated abstract that are also found in\nthe article. These statistics measured on the arXiv dataset\nTable 4: Summarization results on the PubMed dataset. Pre-\nvious work results from (Cohan et al. 2018). The following\nlines are a simple baseline Lead-10 extractor and the pointer\nand classiﬁer models. Our transformer LMs (TLM) are con-\nditioned either on the Introduction (I) or along with extracted\nsentences (E) either from ground-truth (G) or model (M) ex-\ntracts.\nModel Type ROUGE\n1 2 3 L\nPrevious Work\nSumBasic Ext 37.15 11.36 5.42 33.43\nLexRank Ext 39.19 13.89 7.27 34.59\nLSA Ext 33.89 9.93 5.04 29.70\nSeq2seq Abs 31.55 8.52 7.05 27.38\nPointer-gen Mix 35.86 10.22 7.60 29.69\nDiscourse Mix 38.93 15.37 9.97 35.21\nOur Models\nLead-10 Ext 37.45 14.19 8.26 34.07\nSent-CLF Ext 45.01 19.91 12.13 41.16\nSent-PTR Ext 43.30 17.92 10.67 39.47\nTLM-I Abs 37.06 11.69 5.31 34.27\nTLM-I+E (G,M) Mix 42.13 16.27 8.82 39.21\nOracle\nGold Ext Oracle 47.76 20.36 11.52 39.19\nTLM-I+E (G,G) Oracle 46.32 20.15 11.75 43.23\nare presented in ﬁgure 2. First, the original abstract and our\nTLM conditioned on the intro have small and very similar\noverlap fractions with the original article. A model using a\npointing mechanism (we used our own implementation of\nthe model developed by (Cohan et al. 2018)) 1 copies more\nthan our transformer model, especially for higher n-grams.\nIn particular, more than 10% of the 20-grams from the ab-\nstracts generated by the pointing model are also found in\nthe article, showing that it tends to copy long sequences of\nwords. On the other hand, our proposed model produces\nmore “abstractive” summaries, demonstrating its ability to\nparaphrase. Our model tends to copy longer sequences when\nconditioned on the introduction and the sentences from the\nextractor. We hypothesize that providing extracted sentences\nfrom the article that already contain a lot of words present in\nthe reference abstract, makes the transformer’s task easier,\nby allowing it to copy words and phrases from the extracted\nsentences. We ﬁnd empirical evidence of this in ﬁgure 2,\nshowing that the majority of n-gram copies come from the\nextracted sentences. For5-grams, close to 2/3rd of the words\ncopied are from the extracted sentences. As the number of\ngrams increases to 25-grams, 4/5th of the words copied are\nfrom the extracted sentences.\nConclusion\nWe have demonstrated that Transformer language models\ncan generate high-quality summaries of long sequences of\ntext via an extractive step followed by an abstractive step.\nWe quantitatively measure the positive impact of the ex-\n1This model achieved the following ROUGE-1, 2, 3 and L on\nthe arXiv dataset: 41.33, 14.73, 6.80, 36.34\nTable 5: Summarization results on the bigPatent dataset. Pre-\nvious work results from (Sharma, Li, and Wang 2019). Our\ntransformer LMs (TLM) are conditioned on the whole docu-\nment or additionally with extracted sentences (E) either from\nground-truth (G) or model (M) extracts.\nModel Type ROUGE\n1 2 L\nPrevious Work\nLead-3 Ext 31.27 8.75 26.18\nTextRank Ext 35.99 11.14 29.60\nSumBasic Ext 27.44 7.08 23.66\nLexRank Ext 35.57 10.47 29.03\nRNN-Ext Ext 34.63 10.62 29.43\nSeq2Seq Abs 28.74 7.87 24.66\nPointer-gen Mix 30.59 10.01 25.65\nPointer-gen (Cov) Mix 33.14 11.63 28.55\nSent-rewriting Mix 37.12 11.87 32.45\nOracle\nGold Ext Oracle 43.56 16.91 36.52\nOracleFrag Oracle 91.85 78.66 91.85\nOur Models\nSent-CLF Ext 36.20 10.99 31.83\nSent-PTR Ext 34.21 10.78 30.07\nTLM Abs 36.41 11.38 30.88\nTLM+E (G,M) Mix 38.65 12.31 34.09\nTLM+E (G,G) Oracle 39.99 13.79 35.33\nTable 6: Summarization results on the Newsroom dataset.\nPrevious work results from (Grusky, Naaman, and Artzi\n2018) and (Mendes et al. 2019).\nModel Type Extractive Mixed Abstractive\nROUGE\n1 2 L 1 2 L 1 2 L\nPrevious Work\nSeq2Seq Abs 6.1 0.2 5.4 5.7 0.2 5.1 6.2 1.1 5.7\nTextRank Ext 32.4 19.7 28.7 22.3 7.9 17.7 13.5 1.9 10.5\nPointer-gen Mix 39.1 27.9 36.2 25.5 11.0 21.1 14.7 2.3 11.4\nLead-3 Ext 53.0 49.0 52.4 25.1 12.9 22.1 13.7 2.4 11.2\nExconsumm Mix 68.4 62.9 67.3 31.7 16.1 27.0 17.1 3.1 14.1\nOur Models\nSent-CLF Ext 53.0 47.0 52.1 26.8 12.6 23.6 15.4 2.7 12.8\nSent-PTR Ext 60.7 55.2 59.7 28.9 14.1 25.1 15.9 2.8 13.0\nTLM Abs 49.8 39.7 47.4 27.1 11.6 22.8 20.4 6.9 17.1\nTLM+E (G,M) Mix 63.3 57.3 61.8 31.9 16.6 27.4 20.1 6.5 16.6\nOracle\nGold Ext Oracle 68.1 64.5 67.3 40.8 24.6 34.2 21.9 5.2 16.3\nTLM+E (G,G) Oracle 78.8 74.0 77.8 38.6 22.0 33.6 24.5 9.6 20.8\ntractive step, by comparing it to a abstractive model vari-\nant that only sees the input text itself. Our approach out-\nperforms previous extractive and abstractive summarization\nmethods on the arXiv, PubMed and bigPatent datasets and\nis less prone to copying entire phrases or sentences from the\ninput text. The ﬂuency and coherency of the sample sum-\nmaries suggests that these models are ready for comprehen-\nsive human evaluation studies. As with other problem do-\nmains, we have observed that abstractive summaries gener-\nated by transformers can generate imaginary content. We ad-\nTable 7: Qualitative Results — Generated abstracts of select papers using our Intro Only TLM.\nGenerated abstract for “the attention model paper” (Bahdanau,\nCho, and Bengio 2014) - In this paper, we propose a neural machine\ntranslation model, which learns to align and translate a source sentence\nwithout encoding it into a ﬁxed length vector. The proposed model al-\nlows the neural network to align and translate jointly, and in addition,\npredicts a target word based on the context vectors associated with these\nsource positions. We show that the proposed model achieves signiﬁ-\ncantly improved translation performance than the conventional encoder\ndecoder neural network approach, when the sentences in the training\ncorpus are long . Quantitative analysis reveals that the proposed model\nﬁnds a linguistically plausible soft alignment between a source sentence\nand the target sentence. Finally, we compare the performance of the\nproposed model with the state-of-the-art standard neural network trans-\nlation models and show that the proposed approach outperforms the\nconventional approach in terms of both translation performance and the\nquality of the translations.\nOriginal abstract - Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural machine trans-\nlation aims at building a single neural network that can be jointly tuned to maximize the\ntranslation performance. The models proposed recently for neural machine translation often\nbelong to a family of encoder-decoders and consists of an encoder that encodes a source\nsentence into a ﬁxed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the\nperformance of this basic encoder-decoder architecture, and propose to extend this by al-\nlowing a model to automatically (soft-)search for parts of a source sentence that are relevant\nto predicting a target word, without having to form these parts as a hard segment explicitly.\nWith this new approach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French translation. Further-\nmore, qualitative analysis reveals that the (soft-)alignments found by the model agree well\nwith our intuition.\nGenerated abstract for “the GPT-2 paper” (?) - Language mod-\nels have been successful in creating rich representations of complex\nsystems, including tasks such as sentiment analysis. In this paper we\ndemonstrate their ability to generalize well across domains and tasks.\nWe demonstrate the utility of language models by demonstrating that a\nvariety of tasks can be modeled using language models. We demonstrate\nthe potential of language models in downstream tasks by highlighting\nthe ability of these models to perform a wide range of tasks in a zero\nshot setting.\nOriginal abstract - Natural language processing tasks, such as question answering, machine\ntranslation, reading comprehension, and summarization, are typically approached with su-\npervised learning on task-speciﬁc datasets. We demonstrate that language models begin to\nlearn these tasks without any explicit supervision when trained on a new dataset of millions\nof webpages called WebText. When conditioned on a document plus questions, the answers\ngenerated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding\nthe performance of 3 out of 4 baseline systems without using the 127,000+ training exam-\nples. The capacity of the language model is essential to the success of zero-shot task transfer\nand increasing it improves performance in a log-linear fashion across tasks. Our largest\nmodel, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7\nout of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText.\nSamples from the model reﬂect these improvements and contain coherent paragraphs of\ntext. These ﬁndings suggest a promising path towards building language processing systems\nwhich learn to perform tasks from their naturally occurring demonstrations.\nGenerated abstract for “the BERT paper” (Devlin et al. 2018) - We\ndevelop an architecture for deep bidirectional language models that al-\nlows the representation to fuse the left and the right context of a given\ntoken. Our framework uses bidirectional transformer encoder represen-\ntations that enable the model to fuse the context representations of a\ntoken. We show that our architecture achieves state-of-the-art perfor-\nmance on a large suite of tasks, outperforming many systems with task-\nspeciﬁc architectures. Our results are based on a set of eleven task spe-\nciﬁc tasks that are benchmarks for a large suite of nlp tasks.\nOriginal abstract - We introduce a new language representation model called BERT, which\nstands for Bidirectional Encoder Representations from Transformers. Unlike recent lan-\nguage representation models, BERT is designed to pre-train deep bidirectional represen-\ntations from unlabeled text by jointly conditioning on both left and right context in all lay-\ners. As a result, the pre-trained BERT model can be ﬁne-tuned with just one additional\noutput layer to create state-of-the-art models for a wide range of tasks, such as question an-\nswering and language inference, without substantial task-speciﬁc architecture modiﬁcations.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results\non eleven natural language processing tasks, including pushing the GLUE score to 80.5%\n(7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improve-\nment), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement)\nand SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\nvise that such evaluations should probe multiple aspects of\nthe summarization results including both factual correctness\nand coherency. We also note that for evaluating the correct-\nness of the summaries of scientiﬁc articles and patents one\nmust have highly trained evaluators who are willing to invest\nsigniﬁcant amounts of time to read the underlying papers\nand patents. Such studies could therefore require signiﬁcant\ninvestments of resources. We have also presented an upper\nbound on extractive + abstractive models, by conditioning\nthe abstractive step on gold-extracted sentences. In future\nwork, we are also interested in exploring the possibility of\ntraining the extractive and abstractive steps in an end-to-end\nmanner. While we believe that this work is a step forward\ntowards generating more abstractive summaries, it remains\nan open challenge to develop models that respect the under-\nlying facts of the content being summarized while matching\nthe creative ability of humans to coherently and concisely\nsynthesize summaries.\nReferences\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural ma-\nchine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473.\nBengio, Y .; Ducharme, R.; Vincent, P.; and Jauvin, C. 2003.\nA neural probabilistic language model. Journal of machine\nlearning research 3(Feb):1137–1155.\nChen, Y .-C., and Bansal, M. 2018. Fast abstractive summa-\nrization with reinforce-selected sentence rewriting. arXiv\npreprint arXiv:1805.11080.\nCheng, J., and Lapata, M. 2016. Neural summariza-\ntion by extracting sentences and words. arXiv preprint\narXiv:1603.07252.\nChopra, S.; Auli, M.; and Rush, A. M. 2016. Abstrac-\ntive sentence summarization with attentive recurrent neural\nnetworks. In Proceedings of the 2016 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, 93–98.\nCohan, A.; Dernoncourt, F.; Kim, D. S.; Bui, T.; Kim, S.;\nChang, W.; and Goharian, N. 2018. A discourse-aware at-\ntention model for abstractive summarization of long docu-\nments. CoRR abs/1804.05685.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nErkan, G., and Radev, D. R. 2004. Lexrank: Graph-based\nlexical centrality as salience in text summarization. Journal\nof artiﬁcial intelligence research 22:457–479.\nGehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin,\nY . N. 2017. Convolutional sequence to sequence learning.\n1243–1252.\nGehrmann, S.; Deng, Y .; and Rush, A. M. 2018.\nBottom-up abstractive summarization. arXiv preprint\narXiv:1808.10792.\nGraham, Y . 2015. Re-evaluating automatic summarization\nwith bleu and 192 shades of rouge. 128–137.\nGrusky, M.; Naaman, M.; and Artzi, Y . 2018. Newsroom:\nA dataset of 1.3 million summaries with diverse extractive\nstrategies. arXiv preprint arXiv:1804.11283.\nGu, J.; Lu, Z.; Li, H.; and Li, V . O. K. 2016. Incorporat-\ning copying mechanism in sequence-to-sequence learning.\nCoRR abs/1603.06393.\nGulcehre, C.; Firat, O.; Xu, K.; Cho, K.; Barrault, L.; Lin,\nH.-C.; Bougares, F.; Schwenk, H.; and Bengio, Y . 2015.\nOn using monolingual corpora in neural machine transla-\ntion. arXiv preprint arXiv:1503.03535.\nHochreiter, S., and Schmidhuber, J. 1997. Long short-term\nmemory. Neural Comput. 9(8):1735–1780.\nHochreiter, S. 1998. The vanishing gradient problem during\nlearning recurrent neural nets and problem solutions. Inter-\nnational Journal of Uncertainty, Fuzziness and Knowledge-\nBased Systems 6(02):107–116.\nKalchbrenner, N.; Espeholt, L.; Simonyan, K.; Oord, A.\nv. d.; Graves, A.; and Kavukcuoglu, K. 2016. Neu-\nral machine translation in linear time. arXiv preprint\narXiv:1610.10099.\nLin, C.-Y . 2004. Looking for a few good metrics: Automatic\nsummarization evaluation-how many samples are enough?\nIn NTCIR.\nLuong, M.-T.; Pham, H.; and Manning, C. D. 2015. Effec-\ntive approaches to attention-based neural machine transla-\ntion. arXiv preprint arXiv:1508.04025.\nMendes, A.; Narayan, S.; Miranda, S.; Marinho, Z.; Mar-\ntins, A. F.; and Cohen, S. B. 2019. Jointly extracting\nand compressing documents with summary state represen-\ntations. arXiv preprint arXiv:1904.02020.\nMicikevicius, P.; Narang, S.; Alben, J.; Diamos, G.; Elsen,\nE.; Garcia, D.; Ginsburg, B.; Houston, M.; Kuchaiev, O.;\nVenkatesh, G.; et al. 2017. Mixed precision training. arXiv\npreprint arXiv:1710.03740.\nMikolov, T.; Karaﬁát, M.; Burget, L.;ˇCernock`y, J.; and Khu-\ndanpur, S. 2010. Recurrent neural network based language\nmodel.\nNallapati, R.; Zhou, B.; Gulcehre, C.; Xiang, B.; et al. 2016.\nAbstractive text summarization using sequence-to-sequence\nrnns and beyond. arXiv preprint arXiv:1602.06023.\nNallapati, R.; Zhai, F.; and Zhou, B. 2017. Summarunner: A\nrecurrent neural network based sequence model for extrac-\ntive summarization of documents.\nNallapati, R.; Zhou, B.; and Ma, M. 2016. Classify or se-\nlect: Neural architectures for extractive document summa-\nrization. arXiv preprint arXiv:1611.04244.\nPascanu, R.; Mikolov, T.; and Bengio, Y . 2013. On the\ndifﬁculty of training recurrent neural networks. 1310–1318.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners.\nRush, A. M.; Chopra, S.; and Weston, J. 2015. A neu-\nral attention model for abstractive sentence summarization.\nCoRR abs/1509.00685.\nSee, A.; Liu, P. J.; and Manning, C. D. 2017. Get to\nthe point: Summarization with pointer-generator networks.\nCoRR abs/1704.04368.\nSennrich, R.; Haddow, B.; and Birch, A. 2015. Neural ma-\nchine translation of rare words with subword units. arXiv\npreprint arXiv:1508.07909.\nSharma, E.; Li, C.; and Wang, L. 2019. Bigpatent: A large-\nscale dataset for abstractive and coherent summarization.\narXiv preprint arXiv:1906.03741.\nSteinberger, J., and Jezek, K. 2004. Using latent seman-\ntic analysis in text summarization and summary evaluation.\nProc. ISIM 4:93–100.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence to\nsequence learning with neural networks. 3104–3112.\nVan Den Oord, A.; Dieleman, S.; Zen, H.; Simonyan, K.;\nVinyals, O.; Graves, A.; Kalchbrenner, N.; Senior, A. W.;\nand Kavukcuoglu, K. 2016. Wavenet: A generative model\nfor raw audio. SSW 125.\nVanderwende, L.; Suzuki, H.; Brockett, C.; and Nenkova, A.\n2007. Beyond sumbasic: Task-focused summarization with\nsentence simpliﬁcation and lexical expansion. Information\nProcessing & Management 43(6):1606–1618.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. CoRR abs/1706.03762.\nVinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer\nnetworks. 2692–2700.\nWeber, N.; Shekhar, L.; Balasubramanian, N.; and Cho,\nK. 2018. Controlling decoding for more abstractive\nsummaries with copy-based networks. arXiv preprint\narXiv:1803.07038.\nAppendix\nSamples from the arXiv test set\nSamples from the TLM-I and TLM-I+E (G,M) models on a\nrandom example from the arXiv test set.\nTable 8:Qualitative Results - Generated abstracts from our models\non a random example from the test set of (Cohan et al. 2018)\nTLM conditioned on intro — deep convolutional neural net-\nworks ( cnns ) have created new benchmarks in the object\nrecognition challenge . most of these benchmark datasets con-\ntain thousands of images of interest . in addition to these\ndatasets , several standard data - sets , such as pascal voc\n2012 and the ms coco , have been used in conjunction with\nthe same network in order to improve the object detection task\n. in this paper , we present a method for object localization\nusing the deep convolutional neural networks trained for the\nsimpler task of image - level classiﬁcation . we propose local-\nization candidates ( or bounding boxes ) which maximize clas-\nsiﬁcation scores for a class of interest . we explore the beam\nsearch strategy where the number of candidate bounding boxes\nare restricted as we traverse from the root node towards the\nleaf nodes . we then gradually explore smaller and smaller re-\ngions of interest till a point is reached where the classiﬁer is\nnot able to discriminate among the classes of interest . the de-\ntection candidates are organized in a search tree , the root node\nbeing represented by the entire test image . we then gradually\nexplore smaller and smaller regions of interest till a point is\nreached where the classiﬁer is not able to distinguish among\nthe classes of interest .\nTLM conditioned on intro + pointer network extracted sen-\ntences — deep convolutional neural networks ( cnns ) are\nwidely used in image recognition applications . however , they\nface a number of challenges . ﬁrst , cnn models are computa-\ntionally expensive to train and to perform . second , they are not\neasy to implement and can not exploit global features . to alle-\nviate these issues , we present a method that leverages the cor-\nrelation between semantic information present in feature maps\nand localization of an object of interest . the method leverages\nfeature map activations for object localization . we demonstrate\nthat this method is signiﬁcantly faster than state - of - the - art\nin localization accuracy by a signiﬁcant margin of up to 8 map\non two standard data - sets with complex scenes , pascal voc\n2012 and the much larger ms coco .\nGround truth abstract — object localization is an important\ncomputer vision problem with a variety of applications . the\nlack of large scale object - level annotations and the relative\nabundance of image - level labels makes a compelling case for\nweak supervision in the object localization task . deep convo-\nlutional neural networks are a class of state-of-the-art methods\nfor the related problem of object recognition . in this paper , we\ndescribe a novel object localization algorithm which uses clas-\nsiﬁcation networks trained on only image labels . this weakly\nsupervised method leverages local spatial and semantic pat-\nterns captured in the convolutional layers of classiﬁcation net-\nworks . we propose an efﬁcient beam search based approach\nto detect and localize multiple objects in images . the proposed\nmethod signiﬁcantly outperforms the state-of-the-art in stan-\ndard object localization data - sets with a 8 point increase in\nmap scores .\nT-SNE of learned word embeddings\nWe visualize the word embeddings learned by our TLM\nmodel using t-sne. We ﬁnd that words that are often asso-\nciated with computer science are clustered in a different part\nof space when compared to words associated with physics.\nWe use the arXiv REST API to ﬁnd the submission category\nof each paper in the training set and then ﬁnd the∼300 most\nrepresentative words for each category, using TF-IDF scores\nand plot them.\nFigure 3: t-sne visualization of the TLM-learned word em-\nbeddings. The model appears to partition the space based on\nthe broad paper categoty in which it frequently occurs.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9668558835983276
    },
    {
      "name": "Transformer",
      "score": 0.865942120552063
    },
    {
      "name": "Computer science",
      "score": 0.7784650325775146
    },
    {
      "name": "Natural language processing",
      "score": 0.6935542225837708
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6436364650726318
    },
    {
      "name": "Language model",
      "score": 0.5078640580177307
    },
    {
      "name": "Artificial neural network",
      "score": 0.4580378532409668
    },
    {
      "name": "Engineering",
      "score": 0.06432539224624634
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}