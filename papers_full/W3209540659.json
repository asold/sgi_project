{
  "title": "Mind the Gap: Assessing Temporal Generalization in Neural Language Models",
  "url": "https://openalex.org/W3209540659",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221434851",
      "name": "Lazaridou, Angeliki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2889112872",
      "name": "Kuncoro, Adhiguna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221434852",
      "name": "Gribovskaya, Elena",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281500377",
      "name": "Agrawal, Devang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287542949",
      "name": "Liska, Adam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226541241",
      "name": "Terzi, Tayfun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281231513",
      "name": "Gimenez, Mai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226541239",
      "name": "d'Autume, Cyprien de Masson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222963645",
      "name": "Kocisky, Tomas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202039502",
      "name": "Ruder, Sebastian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221452034",
      "name": "Yogatama, Dani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222963644",
      "name": "Cao, Kris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222786234",
      "name": "Young, Susannah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2900299007",
      "name": "Blunsom, Phil",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1915315806",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2150299430",
    "https://openalex.org/W2296438605",
    "https://openalex.org/W2072223048",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2970777192",
    "https://openalex.org/W179875071",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W2557764419",
    "https://openalex.org/W2177801600",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W3097816393",
    "https://openalex.org/W1991564165",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2970716846",
    "https://openalex.org/W2963567867",
    "https://openalex.org/W2970789589",
    "https://openalex.org/W3035067238",
    "https://openalex.org/W2996641835",
    "https://openalex.org/W1905522558",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W2963780471",
    "https://openalex.org/W2963188609",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2953092638",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W3155808134",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2900167100",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2158108973",
    "https://openalex.org/W2741040861",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963887494",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3002800333",
    "https://openalex.org/W2962724315",
    "https://openalex.org/W2804830075",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W3154575616",
    "https://openalex.org/W3153094109",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2060277733",
    "https://openalex.org/W2044062612",
    "https://openalex.org/W2936652946",
    "https://openalex.org/W2963863926",
    "https://openalex.org/W2120587290",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963938518",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2997607995",
    "https://openalex.org/W3122135088",
    "https://openalex.org/W3156891177",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W2995154514",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2120354757",
    "https://openalex.org/W3157700644",
    "https://openalex.org/W2996596224",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3126976873",
    "https://openalex.org/W3098425262",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2970172215",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2022775778",
    "https://openalex.org/W3102015031",
    "https://openalex.org/W3177071108",
    "https://openalex.org/W2251567709",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W1880262756"
  ],
  "abstract": "Our world is open-ended, non-stationary, and constantly evolving; thus what we talk about and how we talk about it change over time. This inherent dynamic nature of language contrasts with the current static language modelling paradigm, which trains and evaluates models on utterances from overlapping time periods. Despite impressive recent progress, we demonstrate that Transformer-XL language models perform worse in the realistic setup of predicting future utterances from beyond their training period, and that model performance becomes increasingly worse with time. We find that, while increasing model size alone -- a key driver behind recent progress -- does not solve this problem, having models that continually update their knowledge with new information can indeed mitigate this performance degradation over time. Hence, given the compilation of ever-larger language modelling datasets, combined with the growing list of language-model-based NLP applications that require up-to-date factual knowledge about the world, we argue that now is the right time to rethink the static way in which we currently train and evaluate our language models, and develop adaptive language models that can remain up-to-date with respect to our ever-changing and non-stationary world. We publicly release our dynamic, streaming language modelling benchmarks for WMT and arXiv to facilitate language model evaluation that takes temporal dynamics into account.",
  "full_text": "Mind the Gap: Assessing Temporal Generalization in\nNeural Language Models\nAngeliki Lazaridou∗♥△♠Adhiguna Kuncoro⋆♥△Elena Gribovskaya⋆♥△\nDevang Agrawal♦♥Adam Liška♦♥Tayfun Terzi♦Mai Gimenez♦\nCyprien de Masson d’Autume♦Tomas Kocisky♥Sebastian Ruder♥\nDani Yogatama♣Kris Cao♣Susannah Young♣Phil Blunsom♣♠\nDeepMind, London, UK\n{angeliki,akuncoro,egribovskaya}@deepmind.com\nAbstract\nOur world is open-ended, non-stationary, and constantly evolving; thus what we\ntalk about and how we talk about it change over time. This inherent dynamic nature\nof language contrasts with the current static language modelling paradigm, which\ntrains and evaluates models on utterances from overlapping time periods. Despite\nimpressive recent progress, we demonstrate that Transformer-XL language models\nperform worse in the realistic setup of predicting future utterances from beyond\ntheir training period, and that model performance becomes increasingly worse with\ntime. We ﬁnd that, while increasing model size alone—a key driver behind recent\nprogress—does not solve this problem, having models that continually update their\nknowledge with new information can indeed mitigate this performance degradation\nover time. Hence, given the compilation of ever-larger language modelling datasets,\ncombined with the growing list of language-model-based NLP applications that\nrequire up-to-date factual knowledge about the world, we argue that now is the right\ntime to rethink the static way in which we currently train and evaluate our language\nmodels, and develop adaptive language models that can remain up-to-date with\nrespect to our ever-changing and non-stationary world. We will publicly release\nour dynamic, streaming language modelling benchmarks for WMT and ARXIV to\nfacilitate language model evaluation that takes temporal dynamics into account.1\n1 Introduction\nIn recent years, substantial efforts in neural language modelling have focused on ﬁnding better neural\narchitectures, building increasingly larger models, and compiling ever-larger amounts of training\ndata, which have been shown to endow language models with the ability to perform well on a wide\nvariety of downstream tasks with minimal ﬁne-tuning (Vaswani et al., 2017; Radford et al., 2019;\nBrown et al., 2020). While this approach has led to impressive progress, it nevertheless relies on a\nstatic experimental paradigm. Concretely, the prevailing practice is to curate a large pretraining web\ncrawl—randomly partitioned into a training set and a validation set in a time-agnostic fashion—and\nthen evaluate on tasks and benchmarks that mostly overlap in time with the pretraining data.2\n∗Equal contribution. ♠Project initiation. △Paper writing. ♦Project technical infrastructure. ♥Model\ndesign and experiments. ♣Project support and advice.\n1We release our dynamic (streaming) language modelling benchmark for WMT and ARXIV at https:\n//github.com/deepmind/deepmind-research/tree/master/pitfalls_static_language_models.\n2In the case of GPT-3 (Brown et al., 2020), such tasks include LAMBADA (Paperno et al., 2016), TriviaQA\n(Joshi et al., 2017b), and WMT translation datasets, among others. These tasks were introduced between 2014\nand 2017, which overlap in time with the GPT-3 CommonCrawl dataset that covered the period of 2016-2019.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2102.01951v2  [cs.CL]  26 Oct 2021\nIn this work, we argue that such practices carry two potential risks. First, they do not assess a\nlanguage model’s ability to generalize well to future data from beyond their training period—an\nimportant ability we henceforth refer to as temporal generalization. In our dynamic and non-stationary\nworld, temporal generalization is a key necessity: Many practical machine learning systems that use\nlanguage model (LM) pretraining, such as machine translation and dialogue systems, are deployed on\nutterances that users will say in the future, whilst being trained on utterances that users have already\nsaid in the past. Furthermore, temporal generalization is also crucial to perform well on realistic\nuse cases of language models in the real world. Examples include ﬂagging fake news about recent\nevents that happen outside of the training period (Thorne and Vlachos, 2018; Zellers et al., 2019;\nAugenstein et al., 2019), forecasting stock prices from the latest news articles (Ding et al., 2015), and\nanswering knowledge-intensive questions like “How many people have been infected by COVID-19?”\nand “Has the USA ever had a female Vice President?”, whose answers have evolved with time.\nSecond, the temporal overlap between the training and evaluation data increases the risk of “test data\ncontamination”, where parts of the evaluation task are unwittingly included in the pretraining data.\nIndeed, many language modelling evaluations treat the data as independent and identically distributed\n(i.i.d) at either the sentence (Chelba et al., 2013) or document level (Brown et al., 2020; Gao et al.,\n2021). Nevertheless, language modelling data are not i.i.d. (neither at the word, sentence, or document\nlevel); rather it is a time series, and thus models trained on the preﬁx of a sample from the series\nshould be evaluated on the continuation of that series. While previous research (Levenberg et al.,\n2010) has highlighted the importance of temporal splits for fairer and more realistic evaluations—and\nhas led to research (Osborne et al., 2014; Yogatama et al., 2014) that addresses language modelling\nfrom this streaming perspective (§7)—using temporal splits (or splits beyond random ones) is still\nthe exception rather than the rule, as evidenced by many contemporary LM (Brown et al., 2020; Gao\net al., 2021) and downstream tasks (Lewis et al., 2020a) that are affected by test data contamination.3\nHere we begin with our ﬁrst question: To what extent does the current static language modelling\npractice overestimate performance, compared to the more realistic setup that evaluates LMs on future\nutterances? To this end, we introduce our dynamic, streaming language modelling benchmarks (§2),\nand ﬁnd that Transformer-XLs (Dai et al., 2019) perform up to 16% worse when predicting articles\nthat are published up to 2 years after the end of the training period. Moreover, model performance\nbecomes increasingly worse with time (§3). Given this ﬁnding, we ask: What kinds of predictions is\nthe model struggling with in the dynamic evaluation setup?—-which we answer in §3.1.\nBeyond LM perplexity evaluation, we further ask: How exactly does this temporal performance\ndegradation of Transformer LMs manifest in different types of question-answering (QA) tasks? We\nanswer this through two different QA tasks, including one around recent events happening outside\nof the LM training period (§5). Lastly, given the challenges presented by temporal generalization\nfor LMs: What, then, is the remedy? This question is important because keeping LMs up-to-date by\nretraining with new data is expensive in compute and carbon costs (Strubell et al., 2019; Patterson\net al., 2021), and risks the model getting outdated in-between long retraining cycles.4 We ﬁnd that\nincreasing model size alone—a key driver behind recent LM progress (Kaplan et al., 2020)—is\nnot a solution for the temporal generalization problem (§4): Larger models suffer from the same\nperformance degradation with time, and a smaller model trained on more recent data can outperform\na 60% larger model that lacks access to more recent data. We then explore a simple yet effective way\nof keeping our models up-to-date by continually updating the model’s parameters through dynamic\nevaluation (Mikolov et al., 2010; Krause et al., 2019), which performs a few steps of gradient descent\non streams of new data (§6), and outline other promising approaches in this direction (§7). We\nconclude with the following recommendations for future LM research:\n• We should evaluate LMs on their generalization ability to future data, which circumvents test data\ncontamination, rewards models that generalize beyond the surface patterns of their pretraining data,\nand better reﬂects how large LMs are used in practical systems. We thus argue for the broader\ninclusion of timestamp information in pretraining data and downstream tasks to make this possible.\n• Stale LMs that are deployed far outside of their training period perform substantially worse on\ndownstream tasks that require up-to-date factual knowledge, although a broader set of experiments\nare needed to pinpoint what kinds of tasks are most affected. Our ﬁndings also highlight the need\n3Brown et al. (2020) used n-gram ﬁltering and deduplication to remove overlaps between the training and\ntest sets. This can potentially induce a correlation between the training and evaluation sets that LMs can exploit.\n4These risks are exacerbated by the trend of ever-larger LMs, where retraining incurs even higher costs.\n2\nDataset Domain Time period\n#Words per Doc\n(Average)\nTraining Size\n(in GB)\nProp. ofCONTROL’s\nTraining Data\nfrom the Test Period\nWMT News 2007 - 2019 551 22.65 6.3%\nCUSTOMNEWS News 1969 - 2019 491 395.59 34.8%\nARXIV Scientiﬁc text 1986 - 2019 172 0.72 14.5%\nTable 1: Statistics and time periods of the datasets used in this study.\nfor more tasks, benchmarks, and metrics that evaluate how well and how rapidly LMs are able to\nintegrate new information, which are important ingredients to encourage progress in this direction.\n• All in all, above and beyond impressive scaling efforts towards ever-larger models (Brown et al.,\n2020; Fedus et al., 2021), we argue for the development of adaptive language models that can\nremain up-to-date with respect to our open-ended and non-stationary world.\n2 Time-stratiﬁed language modelling\nWe begin by introducing our time-stratiﬁcation experimental setup, which examines how well\nTransformer LMs perform when evaluted on future utterances from beyond their training period.\n2.1 Datasets\nWe identify news and scientiﬁc articles as two sources of dynamic streaming data with a naturally\nchanging distribution over time—lending themselves well to evaluating how well language models\ngeneralize over time. For the scientiﬁc domain, we use the publicly available arXiv abstracts\n(ARXIV).5 For news, we use the publicly available WMT News Crawl (WMT ).5 We ensure that any\ntrends we observe also generalize well to models trained on larger datasets—which reliably improve\nlanguage modelling and downstream task performance (Liu et al., 2019)—by compiling a larger\nnews corpus that we term CUSTOM NEWS . This dataset consists of crawled English news sources\nfrom 1969-2019, and covers various topics including politics, ﬁnance, and sport. We apply minimal\npreprocessing through: (i) Removal of non-English documents, (ii) deduplication using the MinHash\nalgorithm, and (iii) tokenization using Moses.5 Table 1 summarizes key statistics of our datasets.\n2.2 Experiment: A model up to 2 years stale\nEvaluation period and test set. For each dataset, we pick the last two years (i.e. 2018 and 2019)\nas our evaluation period, and sub-sample a test set of 24k test documents (1k per test month).\nTIME -STRATIFIED setup. In this setup, we evaluate LMs trained on the past based on their ability\nto predict future articles that are published after the time period of their training data; this split is\nconstructed using the time stamp of each article. Here we use all documents from the beginning of\neach dataset’s time period up until September 2017 as training data, and use the last three months of\n2017 as our validation period; we denote this as the TIME -STRATIFIED setup. We then evaluate the\nmodel on the 2018-2019 test set above, which evaluates the model’s ability to generalize across time\nby predicting articles up to two years after the end of their training period—a realistic time frame\nduring which we expect large-scale language models to be used without retraining on recent data.\nCONTROL setup. We assess whether time stratiﬁcation poses a challenge for current LMs by\ncomparing it with the following CONTROL setup. In this setup, the training set includes documents\nthat come from the same 2018-2019 period as the evaluation set (naturally excluding the test docu-\nments themselves). This CONTROL setup thus resembles the prevailing (static) language modelling\nexperimental practices, which train and evaluate LMs on text data from overlapping time periods.\nCrucially, we control such that the two training sets are of the exact same size, i.e., they differ only\nin the time periods of their training data, rather than in their absolute training set sizes. Here we\nconstruct the CONTROL training data by taking the most recent documents starting from the end of the\nevaluation period (excluding the test documents and including the same number of training documents\nper test month), and keep adding documents from previous time periods until we reach the same\ntraining size as the TIME -STRATIFIED setup. In Table 1, we report the proportion of documents in the\n5ArXiv: https://arxiv.org/help/oa/index; WMT News: http://data.statmt.org/\nnews-crawl; and SacreMoses: https://github.com/alvations/sacremoses.\n3\nCONTROL setup’s training data that come from the same 2018-2019 time period as the evaluation\nset, which is higher for ARXIV and CUSTOM NEWS due to their recent exponential growth of new\ndocuments. We sample a similarly-sized validation set as the TIME -STRATIFIED setup, which in this\ncase comes from the 2018-2019 evaluation period (again excluding the test documents). Importantly,\nboth the TIME -STRATIFIED and CONTROL models are evaluated on the exact same test set from the\n2018-2019 period, which facilitates a fair perplexity comparison between the two setups.\nRelative perplexity comparison. We want to measure temporal degradation, i.e. do Transformer\nLMs perform increasingly worse when predicting test documents further into the future? However,\nany absolute perplexity degradation of the TIME -STRATIFIED model over time (e.g., perplexity for\nJan. 2018 vs Dec. 2018) is an unreliable measure: Some months have longer documents, which\nlead to higher perplexity. We thus measure temporal degradation through relative perplexity changes\nbetween the TIME -STRATIFIED and CONTROL models for the same test month (e.g. Dec. 2018).\n2.3 Model\nWe perform our experiments on autoregressive, left-to-right LMs. We use a Transformer-XL (Dai\net al., 2019) with 18 layers and 1,024 hidden units, resulting in 287M parameters—roughly 15%\nsmaller than GPT-2 MEDIUM and BERTLARGE; we later explore larger models in §4. We set the\nTransformer sequence length to 1,024, and set the memory cache length to 384 during training and\n1,600 during test. We use a vocabulary of 50,259 subwords, obtained via SentencePiece (Kudo and\nRichardson, 2018) trained on a random subset (up to 15GB) of the training data of each respective\nexperiment, i.e., CONTROL and TIME -STRATIFIED . Training and validation are done on subword\ntokens, but to facilitate our later analysis (§3.1), all test perplexities are computed over actual test\nword tokens,6 whose negative log probabilities are obtained by summing those of their subwords.\n3 Language Modelling Experiments & Analysis\nSetup WMT\nCUSTOM\nNEWS AR XIV\nCONTROL 21.11 18.38 21.38\nTIME-STRATIFIED 22.45 21.33 23.07\n∆, absolute +1.34 +2.95 +1.69\n∆, relative (%) 6.34 16.04 7.90\nTable 2: Perplexity of Transformer-XL when\ntrained with the two different setups, and evaluated\non the same test set from the 2018-2019 period.\nTo what extent does the static CON -\nTROL setup overestimate model per-\nformance, compared to the more re-\nalistic TIME -STRATIFIED setup that\nevaluates LMs on future utterances?\nFigure 2 presents the results of our ﬁrst exper-\niment. Although we train both models: (i)\nOn the exact same dataset sizes, and (ii) us-\ning the same model architectures, a stale TIME -\nSTRATIFIED model performs worse than the\nCONTROL model, which has seen training data from the test period—with up to 16% perplex-\nity difference. We attribute the higher relative degradation on CUSTOM NEWS and ARXIV to their\nrecent exponential growth of new documents, resulting in a higher proportion of documents from the\ntest period in the data (Table 1), hence presenting a more difﬁcult temporal generalization problem.\nFigure 1: Relative ppl. increase of TIME -\nSTRATIFIED over CONTROL , across test months.\nDo Transformer LMs perform increasingly\nworse when predicting future utterances fur-\nther away from their training period? To\nthis end, Fig. 1 plots the relative perplexity in-\ncrease of the TIME -STRATIFIED over the CON -\nTROL model. As evidenced by the upward slope\non all datasets, the model deteriorates more\nas we ask it to predict data further away from\nthe training period, afﬁrming that the model in-\ndeed becomes increasingly outdated with time.\nHow general are these ﬁndings? We ﬁnd that\nthe same patterns not only generalize across\ndatasets, as we have just shown, but are also\nfound: (i) For test years other than 2018-2019\n(Appendix A.1), (ii) beyond the two-year tem-\n6An example is detokenizing “__contact”, “less” into “contactless”, where “__” denotes a token boundary.\n4\nporal gap between the end of the training and test periods (Appendix A.2), and (iii) across other\nlanguages (German WMT, Appendix A.3).\n3.1 Analysis\nHaving established that model performance degrades with time, we now turn to investigate the\nfollowing question: What exactly are the kinds of predictions that the model is struggling with?\nPart-of-speech (POS) tag breakdown. We present the relative perplexity increase of the TIME -\nSTRATIFIED over the CONTROL model, broken down by POS tag and across time (Fig. 2, solid\nlines). First, we see that performance on common nouns (orange line), the most frequent POS\ntag, degrades with time; in fact, performance degradation on common nouns drives the overall\ndegradation trend (brown line). Moreover, theTIME -STRATIFIED model’s performance degrades most\nrapidly when making temporal generalizations about proper nouns (blue line) and numbers (purple\nline). Qualitative analysis indicates that the model performs badly on named entities in politics,\nwhose position changed during our 2018-2019 evaluation period (e.g., “Bolsonaro”, “Pompeo”,\n“Khashoggi”). This degradation is consequential because proper nouns—and by extension named\nentities—closely relate to up-to-date factual world knowledge; in §5 we explore how exactly this\ndegradation affects different downstream tasks. Interestingly, we also found the model struggling\nwith concepts associated with cultural and sociological changes on which public perception and\ndiscourse have evolved over time, such as “MeToo” and “BlackLivesMatter” (Bender et al., 2021).\nFigure 2: WMT relative ppl. increase of the TIME -\nSTRATIFIED over the CONTROL models, broken\ndown by part-of-speech (POS) tags (solid lines)\nand topics (dotted lines).\nPerplexity and topics. We analyze how the\nspeed of the TIME -STRATIFIED model’s perplex-\nity degradation relates to different topics. We\nﬁrst cluster the documents using Latent Dirich-\nlet Allocation (Blei et al., 2003, LDA), which\nrepresents each document as a mixture of top-\nics and each topic as a distribution over words;\nwe then aggregate the perplexity of words in\nthe test documents by topic. We observe that\nmodel performance on topics around politics\nand sports change more rapidly with time than\ntopics around lifestyle, as shown in (Fig. 2,\nshown in the three dotted lines).\nPerplexity and temporal frequency shifts\nIn practice, adaptation is a key necessity to max-\nimize the potential of LMs in our dynamic and\nnon-stationary world. This includes the ability\nto integrate information about new words and\nconcepts that never occurred in the past, and\nalso words whose context or meaning had sub-\nstantially changed across time. This need is well-reﬂected in our datasets: About 27% of word types\n(i.e. unique words) on CUSTOM NEWS each month had never occurred in the training period, such\nas “Brexiteers” and “MeToo”. We refer to these asEMERGING NEW WORDS , and argue that these\nconcepts are important because they reﬂect precisely the dynamic nature of our non-stationary world.\nPerhaps the most notable recent EMERGING NEW WORDS is “COVID-19”, which had zero unigram\nprobability prior to late-2019, and yet constitutes an important use case of the NLP systems today.\nConcretely, we deﬁne EMERGING NEW WORDS as those that occur frequently on the test set (at\nleast 50 times), but either: (i) were previously unseen on the training set, or (ii) occurred much less\nfrequently on the training set than on the test set, as indicated by an at least 5 times lower unigram\nprobability. This procedure yields a reasonably-sized set of 287 EMERGING NEW WORDS and\n87,636 mentions in our 2018-2019 test documents. Many of these words indeed reﬂect strong\ntemporal dynamics: e.g. “Ardern” (who became the New Zealand PM in late-2017) and “Novichok”\n(which is what Sergey and Yulia Skripal were poisoned with in 2018). Fig. 3 shows that theTIME -\nSTRATIFIED model performs substantially worse for EMERGING NEW WORDS —an almost 5x worse\nperplexity (110 vs 22) than the overall one (Figure 2).\nPerplexity of ﬁrst and second occurrences ofEMERGING NEW WORDS . We now ask: How well\ncan Transformer LMs rapidly adapt to new information and EMERGING NEW WORDS ? Concretely,\nLMs that perform well in our non-stationary world should be able to predict subsequent occurrences\n5\nof EMERGING NEW WORDS (e.g. “COVID-19”), which exhibit strong temporal dynamics, much\nbetter than the ﬁrst occurrences of these words, because these words appear frequently on the test set\npreﬁx—even though these EMERGING NEW WORDS do not appear as frequently on the training set.\nIn Fig. 3, we show the perplexity obtained by the TIME -STRATIFIED model under two conditions:\nFor the ﬁrst and second occurrences of EMERGING NEW WORDS in a test document.\nOccur-rence\nSetup TIME-STRATIFIED\nTIME-STRATIFIED+dynamic eval\nAll words 22.45 22.17\nAllEMERGING NEW WORDS109.73 66.26\n1st 694.95 357.40\n2nd\n1st inmemory 75.95 44.21\n1stNOTin memory 2,719.25 1,430.34\nTable 3: Perplexity of TIME -STRATIFIED model\non EMERGING NEW WORDS on WMT, broken\ndown by whether the word is encountered for the\n1st or the 2nd time in the test document, and for\nthe latter, whether the 1st occurrence was in the\nTXL context. The last column shows results with\ndynamic evaluation (§6).\nAlthough the model has a high ppl. the ﬁrst time\nit generates EMERGING NEW WORDS in the doc-\nument (ppl. of ∼694.95), it has a much lower\nppl. for generating the same words for the sec-\nond time, but only if the ﬁrst word is available in\nthe Transformer context. In such case, the model\ncan simply copy the same word from the context;\nthis ﬁnding reafﬁrms the strong copying ability\nof the attention block (Bahdanau et al., 2015;\nVinyals et al., 2015). This means that the abil-\nity of Transformers to condition on long-range\ncontext is already a useful feature for temporal\ngeneralization, even when we are not explicitly\nupdating the model parameters with new data.\nHowever, we observe no such effect when the\nﬁrst occurrence falls outside of the Transformer\nmemory (ppl. of >2,700), highlighting the need\nto scale Transformers to even longer sequences (Child et al., 2019; Correia et al., 2019; Kitaev et al.,\n2020; Beltagy et al., 2020, inter alia) to improve temporal generalization.\nImportance. Our analyses provide a targeted evaluation of temporal generalization in LMs, which\nenable us to benchmark progress precisely on things that matter the most for temporal generalization\n(e.g. evaluating LMs on named entities, fast-changing topics, and adaptation speed to EMERGING\nNEW WORDS , rather than relying on overall ppl. as a sole metric for measuring LM progress).\n4 The effect of outdated models persists even when increasing model sizes\nFigure 3: Relative perplexity increase of the TIME -\nSTRATIFIED models with 287M (dotted line) and\n448M parameters (solid line), respectively, over\nthe CONTROL model with 287M parameters, for\nWMT and CUSTOM NEWS (§4).\nRecently, increasing model size has led to sub-\nstantial improvements in perplexity, downstream\ntasks, and few-shot learning ability (Kaplan\net al., 2020; Brown et al., 2020). But can in-\ncreasing model size also improve temporal gen-\neralization? To this end, we train a bigger TIME -\nSTRATIFIED model with 448M parameters—a\n60% increase over the previous 287M model\nand 30% larger than GPT-2MEDIUM.\nSimilar to Section 3, we report the respec-\ntive perplexity increase of the newly trained\nTIME -STRATIFIED 448M model over the CON -\nTROL 287M model (solid lines). We reproduce\nthe relative perplexity increase of the smaller\nTIME -STRATIFIED 287M model over the CON -\nTROL 287M one (Fig. 2) as the dotted lines.\nIf increasing the model size was able to delay\ntemporal degradation, we would expect to see\nthe solid lines produced by the bigger models\nto have reduced (i.e., ﬂatter) slopes compared to the dotted lines produced by the smaller models.\nWhile larger TIME -STRATIFIED models, as expected, achieve lower absolute perplexities (5.5%\nimprovement), model size has no signiﬁcant effect on the slope of these lines (p >0.05, assessed\nusing a t-test on the slopes found by ﬁtting a linear regression). On both datasets, by the end of the\ntest period (i.e. late-2019), a smaller but more up-to-date CONTROL 287M model outperforms a 60%\nlarger but two-year out-of-date TIME -STRATIFIED 448M model. Hence, building models that perform\n6\nwell in this setup requires solutions that more directly tackle the speciﬁc challenges we emphasized\nthrough our ﬁndings so far, and update the model’s knowledge with new information.\n5 Time-stratiﬁed question answering\nSo far we have evaluated the LMs intrinsically, through perplexity, which is important because\nlanguage modelling is a foundational task that affects many NLP applications through language\nmodel pretraining. However, we still do not know how this perplexity deterioration affects practical\napplications of LMs, i.e., how do out-of-date LMs affect different types of downstream tasks?\nFigure 4: Synthetic questions about political ﬁg-\nures: Model performance as we shift the end of\nthe training set chronologically away from the year\nabout which we ask questions (i.e, 2019). The\nerror bars indicate two standard errors of mean.\nClosed-book question answering (QA).\nClosed-book QA is a popular testbed for evalu-\nating pretrained LMs that have to compress and\nencode knowledge found in a big corpus. But\ngiven the relative lack of existing time-stamped,\nnews QA datasets that evaluate LMs’ ability\nto answer questions about events that happen\noutside of their training period in a closed-book\nfashion, we construct a dataset of synthetic\nquestions the government ofﬁcials using the\nfollowing template: “ Who is the [government\nrole] of [country/state] in [month/year]? ” In\ntotal, we construct a test set of 438 questions\nabout 22 different government roles from 11\ncountries (see Appendix C for examples). We\npretrain the TXL model (as described in Section\n2.3) using the WMT dataset up to the years\n2012, 2013,. . . , 2019, respectively. We then\nﬁne-tune all these models to answer questions\nabout government ofﬁcials for 2011 to get\nthe model accustomed to the task format, and\nevaluate on synthetic questions related to the year 2019. Fig. 4 shows the substantial accuracy\ndeterioration as we shift the end of the pretraining data away from 2019, the year for which we ask\nquestions. This ﬁnding demonstrates how the ﬁne-tuned LMs’ lack of more recent factual knowledge\naffects their performance on this task. Note that the slight drop in accuracy in 2019 compared to\n2018 is due to dataset noise. Anecdotally, we observe that the 2019 model mixes up the names of\nRussian and American presidents, which often co-occurred in the same context in 2019.\nReading comprehension. Nevertheless, we do not expect all downstream tasks to be equally\naffected by outdated LMs. To illustrate this point, we perform a reading comprehension experiment\nusing NewsQA (Trischler et al., 2017), where the evidence documents are presented together with\nthe questions into the preﬁx of the model. Hence, the model has all necessary information to answer\nthe questions, and thus outdated LMs will likely present less of a challenge in this type of tasks. We\nobtain a TIME -STRATIFIED NewsQA by recovering the articles’ timestamps.7. We test on questions\nfrom 2009, for a of total 10000 questions (see Appendix C for question-answer examples). We\nevaluate how well LMs trained on CUSTOM NEWS until the end of 2008 performs in comparison to\nLMs trained until the end of 2009: Both models perform identically at 0.47 F1. Hence, time-stratiﬁed\nevaluations for reading comprehension, where the answers are extractive and can be copied from the\npassage, pose less of a challenge for outdated LMs, unlike knowledge-intensive, closed-book QA.\n6 Keeping models up-to-date: Online learning through dynamic evaluation\nOne way to mitigate LMs’ degradation over time is to continually update the models’ knowledge\nwith new information as new documents arrive into the stream. One of the ways to do this is through\ndynamic evaluation (Mikolov et al., 2010; Graves, 2013; Krause et al., 2019)—a form of online\nlearning that continually updates the parameters of a pretrained model by performing gradient descent\non the new data. While most prior work used dynamic evaluation to perform updates within a\ndocument, hence adapting to local topical shifts, here we use it to adapt to the temporal dynamics\n7https://cs.nyu.edu/ kcho/DMQA/\n7\nthat occur within a stream of chronologically ordered documents, hence adapting to temporal trends\nacross documents. Appendix B has more details on dynamic evaluation and our empirical settings.\nFigure 5: Relative perplexity increase with (solid\nlines) and without (dotted lines) dynamic evalua-\ntion, for the TIME -STRATIFIED model.\nWe plot the results in Fig. 5: Dotted lines\nreﬂect the perplexity increase when com-\nparing the CONTROL model to the TIME -\nSTRATIFIED model, i.e., the same graph as\nin Fig. 1, whereas solid lines reﬂect the per-\nplexity increase achieved when comparing\nthe same CONTROL model with the TIME -\nSTRATIFIED model augmented with dynamic\nevaluation ( TIME -STRATIFIED dyn). In all\ndatasets, dynamic evaluation reduces the speed\nof the model becoming outdated, as evidenced\nby the reduced upward slope, with a signiﬁcant\neffect for ARXIV and WMT (p <0.05, as-\nsessed using a t-test on the slopes found by ﬁt-\nting a linear regression). The improvements\nare more pronounced for ARXIV, where a more\ngranular analysis over weeks reveals that the\nmodel needs only about one week worth of data\nto overtake the CONTROL model. Moreover, we\nsee much larger improvements for predicting\nEMERGING NEW WORDS , which exhibit strong temporal dynamics (§3.1, see Fig. 3): We observe a\n39.62% ppl. reduction from 109.73 to 66.2 for EMERGING NEW WORDS , compared to the overall ppl.\nreduction (a 1.25% reduction from 22.45 to 22.17 for WMT; Fig. 4).\nParameters\nthat get updated WMT\nCUSTOM\nNEWS AR XIV\nAll parameters 22.17 20.72 20.98\nBias-only 22.16 20.96 21.24\nEmbeddings-only 22.32 21.21 22.27\nno dynamic eval. 22.45 21.33 23.07\nTable 4: Perplexity of TIME -STRATIFIED model\nwhen updating some of the parameters.\nWhen aiming to keep models up-to-date (es-\npecially for larger models), lightweight yet ef-\nfective approaches are preferable because they\nallow the model to rapidly digest new infor-\nmation with minimal time, computation, and\ncarbon costs. We thus experiment with updat-\ning only the embedding layer (i.e., 52M param-\neters), capturing lexical semantic changes, as\nwell as updating only the bias terms at all layers\n(i.e., 198K parameters), as recently introduced\nby Ben-Zaken et al. (2021). Fig. 4 presents the results: In line with the ﬁndings of Ben-Zaken et al.\n(2021), updating only the bias terms performs nearly as well as updating the full model.\nBeyond dynamic evaluation We remark that dynamic evaluation alone, while effective, does not\nfully solve temporal generalization, as evidenced by the prevailing (albeit gentler) upward slopes\non WMT and CUSTOM NEWS . We expect that larger gains can be achieved by fully embracing\ncontinual learning in LMs—striking a balance between quickly adapting to new data and achieving\npositive forward transfer (Lopez-Paz and Ranzato, 2017), while avoiding catastrophic forgetting\n(Mccloskey and Cohen, 1989; Kirkpatrick et al., 2017). Indeed, as we show in Figure 9, while\ndynamic evaluation is able to improve generalization to future data, it causes catastrophic forgetting\nof the past data. Furthermore, recent semi-parametric models (Guu et al., 2020; Lewis et al., 2020b;\nKarpukhin et al., 2020; Khandelwal et al., 2020; Yogatama et al., 2021) lend themselves well to\ncontinual learning, where new knowledge can be stored in an external memory, which can be updated\nwithout retraining the whole model. A related approach is to disentangle the acquisition of up-to-date\nknowledge from the language learning itself, and enable direct editing of that knowledge within the\nmodel (Sinitsin et al., 2020; Zhu et al., 2020; De Cao et al., 2021).\n7 Related Work\nConcept drift. Detecting changes in data streams, also known as concept drift, has a long history\nin machine learning (Widmer and Kubat, 1996; Kifer et al., 2004; Baena-Garcıa et al., 2006; Dries\nand Rückert, 2009; Lu et al., 2019). In NLP, most recent work in this area models lexical change by\n8\ntraining word embeddings (Hamilton et al., 2016; Szymanski, 2017; Yin et al., 2018) and deep neural\nnetworks (Rosenfeld and Erk, 2018; Bjerva et al., 2019) on data of different time spans.\nOut-of-Distribution (OoD) generalization. OoD generalization is well-studied in NLP (Blitzer\net al., 2006; Daumé III, 2007; Axelrod et al., 2011), and has recently been addressed for neural LMs\nand transfer learning (Fried et al., 2019; Oren et al., 2019; Gururangan et al., 2020), where pretrained\nLMs lead to substantial improvements and increased robustness (Hendrycks et al., 2020). While most\nprior work has focused on distributional shifts in terms of topic and domain (Kruszewski et al., 2020),\ndistributional shifts in terms of time also constitute an important and realistic challenge that NLP\nsystems of today (including those based on large LM pretraining) must be able to perform well at.\nContinual learning & streaming LMs. Our work is closely related to continual and lifelong\nlearning, which aim to design models that continually accumulate new knowledge without forgetting\nrelevant information about the past (Mccloskey and Cohen, 1989; Thrun and Mitchell, 1995; French,\n1999; Mitchell et al., 2015; Rusu et al., 2016; Kirkpatrick et al., 2017; Al-Shedivat et al., 2018;\nHadsell et al., 2020). The distribution of words and context in natural language changes rapidly with\ntime, and hence constitutes an important test bed for developing continual learning systems. More\nspeciﬁc to the LM literature, prior work proposed ways of designing LMs that efﬁciently adapt their\nknowledge to continuous streams of new information (Jelinek et al., 1991; Wang et al., 2008; Goyal\net al., 2009; Osborne et al., 2014; Yogatama et al., 2014, inter alia)—often known as streaming LMs,\nalbeit mostly in the context of n-gram LMs. While we show that Transformer LMs achieve much\nbetter perplexity than previous n-gram models (the Transformer LM ppl. in §3 are substantially better\nthan those in prior streaming n-gram LM literature), we show that Transformer LMs similarly suffer\nfrom the temporal degradation problem. Given the different nature of our LMs today (i.e. deep neural\nmodels rather than n-gram LMs), we argue that now is the right time to make progress on this open\nresearch question, with notable progress in other NLP tasks (d’Autume et al., 2019; Sun et al., 2020).\nTemporal splits in NLP. Prior work has used temporal splits (i.e. training on text from the past and\nevaluating on future text) for NLP tasks like machine translation (Levenberg et al., 2010), sentiment\nanalysis (Lukes and Søgaard, 2018), named entity recognition (Fromreide et al., 2014; Rijhwani\nand Preotiuc-Pietro, 2020), and others (Dunn et al., 2017; Bjerva et al., 2020; Søgaard et al., 2021).\nNevertheless, the vast majority of NLP benchmarks today still do not perform temporal splits, and\nhence do not measure how well models can generalize to future data. Furthermore, this work has two\nkey distinctions from prior work. First, we focus on language modelling—a foundational task that is\nused for many NLP systems today through LM pretraining—and propose a benchmark to measure\nprogress in this direction. Second, we go one step further than prior work and perform a thorough\nanalysis to pinpoint what kinds of predictions the model is struggling with. Such analysis can then be\nused to better measure progress in dynamic language modelling, where improvements are not always\neasily discernible from overall ppl. alone (e.g. performance on EMERGING NEW WORDS ; §3.1).\n8 Conclusion\nWe evaluated the extent to which our current language models can generalize well in the realistic\nsetup of predicting future utterances outside their training period. We found that current practices that\ntrain and evaluate models on data from overlapping time period overestimate model generalization\nto future utterances, and that Transformer LMs become increasingly outdated with time. We found\nthat increasing model size alone—a key driver behind recent LM success—is not a solution for this\nproblem, and that this degradation affects downstream tasks that require up-to-date factual knowledge.\nGenerality to other domains. The importance of temporal generalization extends beyond language\nmodelling and NLP. Many other commercial machine learning systems like speech processing and\nvideo understanding also involve non-stationary data, and are similarly trained on data that were\ncollected in the past, but deployed on new data from the future. Since many of these tasks also use\nTransformers (Girdhar et al., 2019; Gulati et al., 2020,inter alia), we expect our ﬁndings to generalize\nto these domains, although a more complete investigation to this end is left to future work.\nLimitations While we explored how the LM performance degradation with time affects two types\nof question-answering tasks, a broader variety of tasks is needed to obtain a more holistic picture on\nhow temporal generalization manifests in downstream tasks. An open research question is thus how\nwe can create and maintain benchmarks that are not static (Nie et al., 2020; Potts et al., 2020) and\nfurther promote research on continual and life-long learning.\nIs this all obvious? Our ﬁndings should not come us a surprise: That the world around us changes\nwith time—and thus what and how we talk about it also evolve accordingly—is hardly controversial.\nBut for the most part, these temporal dynamics are still not currently reﬂected in the way that we train\n9\nand evaluate our neural LMs. Our aim here is to highlight how such static evaluations overestimate\nmodels’ performance, especially around predictions that relate to factual knowledge, which constitute\nan important use case of NLP systems today. With the compilation of ever-larger web crawls for LM\npretraining (Gao et al., 2021), now is the right time to rethink how our splits are constructed (Søgaard\net al., 2021), construct temporal splits that evaluate models on their ability to generalize to future data,\nand include timestamp information in both pretraining datasets and downstream tasks to facilitate\nthis kind of more realistic LM evaluation. This strategy will not only allow us to assess models’\nperformance on a realistic type of out-of-sample data, but also circumvent test data contamination\naffecting both LM and downstream task evaluations more broadly, e.g., in widely used QA datasets\nlike Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017a), which have\nbeen shown to contain alarmingly high proportions of overlapping training and test data (Lewis et al.,\n2020c). Our dynamic, streaming LM benchmarks—alongside the evaluation metrics that evaluate\nLMs on things that matter the most for temporal generalization (e.g. named entities, EMERGING NEW\nWORDS )—will be released to encourage more research in this area, and reward the development of\nadaptive language models that can remain up-to-date with respect to our non-stationary world.\n9 Broader Societal Impact Discussion\nLastly, we remark on two aspects of the broader societal impact pertaining to the importance of\ncontinually-updating language models. First, we argue that having NLP models—the vast majority of\nwhich are built on top of pretrained language models—that can remain up-to-date with our current\nsocial trends and public perception is relevant for mitigating potential harms and biases caused by\nNLP models. For instance, recently there has been renewed public support and interest for social\njustice movements in 2020, such as the #BlackLivesMatter movement (Cohn and Quealy, 2020).\nHence, without explicit mechanisms to update the models’ knowledge, language models that were\ntrained before this time period can potentially miss out on shifting language used to describe such\nmovements—where such movements are now more widely supported by the general public—and\npotentially produce outdated, biased language that is no longer frequently used at present. On the\nother hand, we should also be careful not to let the model update its knowledge with material that can\nadd or amplify to the bias and prejudice of the model (Chang et al., 2019; Bender et al., 2021).\nSecond, our ﬁndings highlight the risk of the “brute-force” approach of keeping models up-to-date by\nperiodically retraining the model from scratch, for instance by combining the old and new data. Given\nthe increasing size of NLP models, training large models from scratch each time incurs increasingly\nmore expensive computational and environmental costs (Strubell et al., 2019; Patterson et al., 2021).\nHence, our ﬁndings emphasise the need for more efﬁcient and lightweight approaches of keeping\nmodels up-to-date, whilst mitigating catastrophic forgetting at the same time. Our work provides a\nbenchmark to measure progress in this space, and we strongly encourage future work that uses our\nbenchmark to also report the computational costs of their approach for keeping language models\nup-to-date. Lastly, we remark that the ethical considerations and risks of working with large language\nmodels also apply to our work (Bender et al., 2021).\nAcknowledgments and Disclosure of Funding\nWe thank Paul Michel, Laura Rimell, and Chris Dyer for useful feedback throughout the different\nstages of this project. We would also like to thank Katie Millican, Sebastian Borgeaud, Trevor Cai,\nRoman Ring, Jack Rae, and Geoffrey Irving for their initial work on the codebase.\nReferences\nMaruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel.\nContinuous adaptation via meta-learning in nonstationary and competitive environments. In Proc.\nof ICLR, 2018.\nIsabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen,\nChristian Hansen, and Jakob Grue Simonsen. MultiFC: A real-world multi-domain dataset for\nevidence-based fact checking of claims. In Proc. of EMNLP-IJCNLP, 2019.\n10\nAmittai Axelrod, Xiaodong He, and Jianfeng Gao. Domain adaptation via pseudo in-domain data\nselection. In Proc. of EMNLP, 2011.\nManuel Baena-Garcıa, José del Campo-Ávila, Raúl Fidalgo, Albert Bifet, R Gavalda, and R Morales-\nBueno. Early drift detection method. In Fourth international workshop on knowledge discovery\nfrom data streams, volume 6, pages 77–86, 2006.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In Proc. of ICLR, 2015.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\nCoRR, abs/2004.05150, 2020.\nElad Ben-Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning\nfor transformer-based masked language-models, 2021.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\ndangers of stochastic parrots: Can language models be too big? In Proceedings of FAccT 2021,\n2021.\nJohannes Bjerva, Wouter M Kouw, and Isabelle Augenstein. Back to the future–sequential alignment\nof text representations. In 34rd AAAI Conference on Artiﬁcial Intelligence, pages 1909–03464.\nAssociation for the Advancement of Artiﬁcial Intelligence, 2019.\nJohannes Bjerva, Wouter Kouw, and Isabelle Augenstein. Back to the future - temporal adaptation of\ntext representations. In Proc. of AAAI, 2020.\nDavid M. Blei, Andrew Y . Ng, and Michael. I Jordan. Latent dirichlet allocation.Journal of Machine\nLearning Research, 3, 2003.\nJohn Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural correspon-\ndence learning. In Proc. of EMNLP, 2006.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165,\n2020.\nKai-Wei Chang, Vinod Prabhakaran, and Vicente Ordonez. Bias and fairness in natural language\nprocessing. In Proc. of EMNLP-IJCNLP: Tutorial Abstracts, 2019.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language modeling.\narXiv preprint arXiv:1312.3005, 2013.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. CoRR, abs/1904.10509, 2019.\nNate Cohn and Kevin Quealy. How public opinion has moved on black lives matter. The\nNew York Times , 2020. URL https://www.nytimes.com/interactive/2020/06/10/upshot/\nblack-lives-matter-attitudes.html .\nGonçalo M. Correia, Vlad Niculae, and André F. T. Martins. Adaptively sparse transformers. In Proc.\nof EMNLP-IJCNLP, 2019.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V . Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context. In Proc. of ACL, 2019.\nHal Daumé III. Frustratingly easy domain adaptation. In Proc. of ACL, 2007.\nCyprien de Masson d’Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic\nMemory in Lifelong Language Learning. In Proc. of NeurIPS, 2019.\n11\nNicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. arXiv\npreprint arXiv:2104.08164, 2021.\nXiao Ding, Yue Zhang, Ting Liu, and Junwen Duan. Deep learning for event-driven stock prediction.\nIn Proc. of IJCAI, 2015.\nAnton Dries and Ulrich Rückert. Adaptive concept drift detection. Statistical Analysis and Data\nMining: The ASA Data Science Journal, 2(5-6):311–327, 2009.\nMatthew Dunn, Levent Sagun, Mike Higgins, V . U. Güney, V olkan Cirik, and Kyunghyun Cho.\nSearchqa: A new q&a dataset augmented with context from a search engine.ArXiv, abs/1704.05179,\n2017.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021.\nRobert M. French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences, 3\n(4), 1999.\nDaniel Fried, Nikita Kitaev, and Dan Klein. Cross-domain generalization of neural constituency\nparsers. In Proc. of ACL, 2019.\nHege Fromreide, Dirk Hovy, and Anders Søgaard. Crowdsourcing and annotating NER for Twitter\n#drift. In Proc. of LREC, 2014.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb\ndataset of diverse text for language modeling, 2021.\nRohit Girdhar, João Carreira, Carl Doersch, and Andrew Zisserman. Video Action Transformer\nNetwork. In Proc. of CVPR, 2019.\nAmit Goyal, Hal Daumé III, and Suresh Venkatasubramanian. Streaming for large scale NLP:\nLanguage modeling. In Proc. of NAACL-HLT, 2009.\nAlex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented\ntransformer for speech recognition. In Proc. of INTERSPEECH, 2020.\nSuchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and\nNoah A. Smith. Don’t stop pretraining: Adapt language models to domains and tasks. In Proc. of\nACL, 2020.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval-augmented\nlanguage model pre-training. In Proc. of ICML, 2020.\nRaia Hadsell, Dushyant Rao, Andrei A. Rusu, and Razvan Pascanu. Embracing change: Continual\nlearning in deep neural networks. Trends in Cognitive Sciences, 24(12), 2020.\nWilliam L Hamilton, Jure Leskovec, and Dan Jurafsky. Diachronic Word Embeddings Reveal\nStatistical Laws of Semantic Change. In Proc. of ACL, pages 1489–1501, 2016.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song.\nPretrained transformers improve out-of-distribution robustness. In Proc. of ACL, 2020.\nF. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. A dynamic language model for speech recognition.\nIn Speech and Natural Language: Proceedings of a Workshop Held at Paciﬁc Grove, California,\nFebruary 19-22, 1991, 1991.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proc. of ACL, 2017a.\n12\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proc. of ACL, 2017b.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models,\n2020.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proc. of\nEMNLP, 2020.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization\nthrough memorization: Nearest neighbor language models. In Proc. of ICLR, 2020.\nDaniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In VLDB,\nvolume 4, pages 180–191. Toronto, Canada, 2004.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,\nClaudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in\nneural networks. Proceedings of the National Academy of Sciences, 114(13), 2017.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In Proc.\nof ICLR, 2020.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural\nsequence models. In Proc. of ICML, 2018.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of trans-\nformer language models. arXiv preprint arXiv:1904.08378, 2019.\nGermán Kruszewski, Ionut-Teodor Sorodoc, and Tomas Mikolov. Evaluating online continual\nlearning with calm, 2020.\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations, pages 66–71, 2018.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. Natural questions: A benchmark for question answering research. TACL, 7, March 2019.\nAbby Levenberg, Chris Callison-Burch, and Miles Osborne. Stream-based translation models for\nstatistical machine translation. In Proc. of NAACL-HLT, 2010.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented genera-\ntion for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401, 2020a.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\nRetrieval-augmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato,\nR. Hadsell, M. F. Balcan, and H. Lin, editors, Proc. of NeurIPS, 2020b.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel. Question and answer test-train overlap in\nopen-domain question answering datasets. arXiv preprint arXiv:2008.02637, 2020c.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692, 2019.\nDavid Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In\nProc. of NeurIPS, 2017.\n13\nJie Lu, Anjin Liu, Fan Dong, Feng Gu, João Gama, and Guangquan Zhang. Learning under concept\ndrift: A review. IEEE Transactions on Knowledge and Data Engineering, 31(12), 2019.\nJan Lukes and Anders Søgaard. Sentiment analysis under temporal shift. In Proc. of WASSA, 2018.\nMichael Mccloskey and Neil J. Cohen. Catastrophic interference in connectionist networks: The\nsequential learning problem. The Psychology of Learning and Motivation, 24, 1989.\nSabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian Roark, and Jason Eisner. What kind of\nlanguage is hard to language-model? In Proc. of ACL, 2019.\nTomas Mikolov, Martin Karaﬁát, Lukás Burget, Jan Cernocký, and Sanjeev Khudanpur. Recurrent\nneural network based language model. In Proc. of INTERSPEECH, 2010.\nT. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Betteridge, A. Carlson, B. Dalvi, M. Gardner,\nB. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis, T. Mohamed, N. Nakashole, E. Platanios,\nA. Ritter, M. Samadi, B. Settles, R. Wang, D. Wijaya, A. Gupta, X. Chen, A. Saparov, M. Greaves,\nand J. Welling. Never-ending learning. In Proc. of AAAI, 2015.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial\nNLI: A new benchmark for natural language understanding. In Proc. of ACL, 2020.\nYonatan Oren, Shiori Sagawa, Tatsunori Hashimoto, and Percy Liang. Distributionally robust\nlanguage modeling. In Proc. of EMNLP-IJCNLP, 2019.\nMiles Osborne, Ashwin Lall, and Benjamin Van Durme. Exponential reservoir sampling for streaming\nlanguage models. In Proc. of ACL, 2014.\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,\nSandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset:\nWord prediction requiring a broad discourse context. In Proc. of ACL, 2016.\nDavid A. Patterson, Joseph Gonzalez, Quoc V . Le, Chen Liang, Lluis-Miquel Munguia, Daniel\nRothchild, David R. So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network\ntraining. CoRR, abs/2104.10350, 2021.\nChristopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. Dynasent: A dynamic\nbenchmark for sentiment analysis. arXiv preprint arXiv:2012.15349, 2020.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. In Technical report, OpenAI., 2019.\nShruti Rijhwani and Daniel Preotiuc-Pietro. Temporally-informed analysis of named entity recogni-\ntion. In Proc. of ACL, 2020.\nAlex Rosenfeld and Katrin Erk. Deep neural models of semantic shift. In Proc. of NAACL-HLT,\n2018.\nAndrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,\nKoray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. CoRR,\nabs/1606.04671, 2016.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, and Artem Babenko. Editable\nneural networks. arXiv preprint arXiv:2004.00345, 2020.\nAnders Søgaard, Sebastian Ebert, Jasmijn Bastings, and Katja Filippova. We need to talk about\nrandom splits. In Proc. of EACL, 2021.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep\nlearning in NLP. In Proc. of ACL, 2019.\nFan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. LAMOL: LAnguage MOdeling for Lifelong\nLanguage Learning. In Proc.of ICLR, 2020.\n14\nTerrence Szymanski. Temporal Word Analogies: Identifying Lexical Replacement with Diachronic\nWord Embeddings. In Proc. of ACL, 2017.\nJames Thorne and Andreas Vlachos. Automated fact checking: Task formulations, methods and\nfuture directions. In Proc. of ICCL, 2018.\nSebastian Thrun and Tom M. Mitchell. Lifelong robot learning. Robotics and Autonomous Systems,\n15(1), 1995.\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman,\nand Kaheer Suleman. NewsQA: A machine comprehension dataset. In Proceedings of the 2nd\nWorkshop on Representation Learning for NLP , pages 191–200, Vancouver, Canada, August\n2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-2623. URL https:\n//www.aclweb.org/anthology/W17-2623.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Proc. of NeurIPS, 2017.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Proc. of NeurIPS, 2015.\nChong Wang, David Blei, and David Heckerman. Continuous time dynamic topic models. In Proc.\nof UAI, 2008.\nGerhard Widmer and Miroslav Kubat. Learning in the presence of concept drift and hidden contexts.\nMachine Learning, 23(1), 1996.\nZi Yin, Vin Sachidananda, and Balaji Prabhakar. The global anchor method for quantifying linguistic\nshifts and domain adaptation. Proc. of NeurIPS, 2018.\nDani Yogatama, Chong Wang, Bryan R. Routledge, Noah A. Smith, and Eric P. Xing. Dynamic\nlanguage models for streaming text. Transactions of Association of Computational Linguistics,\n2014.\nDani Yogatama, Cyprien de Masson d’Autume, and Lingpeng Kong. Adaptive semiparametric\nlanguage models. Transactions of Association of Computational Linguistics, 2021.\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. Defending against neural fake news. In Proc. of NeurIPS, 2019.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv\nKumar. Modifying memories in transformer models. arXiv preprint arXiv:2012.00363, 2020.\nChecklist\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes] See the “Limitation” part of\nSection 8.\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] We work\nwith large-scale language models. In the paper, we have outlined the broader societal\nimpact of our work, although other risks that stem from language modelling research\non large amounts of data may also apply to our work (Bender et al., 2021).\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes] We have outlined the potential negative social impacts of our work in §9,\nand additionally in the answer to Checklist 1.(c) above.\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A] This paper\ndoes not include theoretical results.\n(b) Did you include complete proofs of all theoretical results? [N/A]\n15\n3. If you ran experiments...\n(a) Did you include the code, data, and instructions needed to reproduce the main exper-\nimental results (either in the supplemental material or as a URL)? [Yes] All dataset\ndetails and preprocessing steps are described in Section 2.1. While we do not re-\nlease the code, the experiment can be repeated with publicly available Transformer\nimplementations. The dataset splits are released publicly.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [Yes] The dataset and splits are described in Section 2.1, and are released\npublicly. The hyper-parameters and model details are described in Section 2.3.\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-\nments multiple times)? [Yes] We have conducted signiﬁcance testing and also tested\nthe robustness of our ﬁndings by replicating our experiments in different conﬁgura-\ntions (e.g., with larger model sizes, in different languages, in different datasets, and in\ndifferent yearly splits of our datasets).\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [Yes] To train and evaluate the models,\nincluding hyperparameter optimization, we used approximately 186,000 TPU hours. In\neach experiment, we used 32 TPUs for training and 1 TPU for evaluation.\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes] We build our data\nusing existing datasets, which we cite in footnote 4.\n(b) Did you mention the license of the assets? [N/A] We release dataset splits as lists of\nidentiﬁers pointing to original datasets. The licenses of the original datasets apply.\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\nWe are providing the URL to the publicly released resources.\n(d) Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? [N/A] We build all our datasets through publicly available datasets.\n(e) Did you discuss whether the data you are using/curating contains personally identiﬁable\ninformation or offensive content? [No] We work with publicly available datasets that\nhave been used in prior work. An analysis of whether, and to what extent, personally\nidentiﬁable information can be extracted from these publicly available datasets are\nbeyond the scope of this work.\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N/A] This work does not include any crowd-sourcing or research with\nhuman subjects.\n(b) Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N/A]\n16\nA How General Are These Findings?\nA.1 The effect of outdated models persists beyond the 2018/2019 test period.\nWe test whether the temporal degradation trends we observe in §3 are not an artifact of some\nparticularity of the chosen test period (i.e., Yr1 = 2018 and Yr2 = 2019). We design new test sets\nby shifting Yr1 and Yr2 in increments of one year towards the past, for a total of ﬁve such test sets.\nFollowing §2.2, we derive different TIME -STRATIFIED Y r1,Y r2 and CONTROL Y r1,Y r2 training and\nvalidation splits.\nNote that each TIME -STRATIFIED Y r1,Y r2 and CONTROL Y r1,Y r2 setups are: (i) Trained on the same\ntraining data sizes, and (ii) evaluated on the same test set coveringYr1 and Yr2. Fig. 6 shows similar\ntemporal degradation across all testing years.\nFigure 6: Relative perplexity increase of TIME -STRATIFIED Y r1,Y r2 over CONTROL Y r1,Y r2 models.\nA.2 The effect of outdated models persists beyond the two-year gap.\nFor this experiment, we keep the same 2018-2019 test set introduced in §2.2, and train models with\ntraining data from different time periods with increasingly larger gaps from the 2018-2019 evaluation\nperiod, controlling so that all training data sizes are identical across different years. More concretely,\nthe most up-to-date model covers the same time period as the original TIME -STRATIFIED model, and\nwe “push” the training period back with 6-month increments, up to September 2012, for a total of\n11 training sets—each of the same size—used to train 11 models. Fig. 7 shows that the perplexity\ndeterioration continues to grow in response to larger gaps between the training and test periods.\nFigure 7: Perplexity of models trained with data from different time periods, with increasingly larger\ngaps from the 2018-2019 test set period.\n17\nA.3 The effect of outdated models persists beyond English: A German study.\nWe test whether the temporal degradation trend is a generalizable pattern that holds across languages.\nWe use the German subset of WMT, apply the same pre-processing steps as §2.1, follow the same\nexperimental setup as §2.2, and train two Transformer-XL models on TIME -STRATIFIED de and\nCONTROL de setups, achieving 30.87 and 26.79 respective test perplexities. These perplexities are\nindeed higher than the ones in Table 2—a consistent pattern with prior ﬁndings on the difﬁculty of\nmodelling German (Mielke et al., 2019). Nevertheless, we still see the exact same pattern where the\nstale TIME -STRATIFIED de model performs worse than the CONTROL de one (a substantial 15.23%\nrelative increase). Moreover, similar to the English experiment, the model degrades more as the gap\nbetween the training and test period increases—an effect particularly pronounced for proper nouns\nand for words that are broken down by the TIME -STRATIFIED de tokenizer into more tokens.\nFigure 8: For the experiments on German, the relative increase of perplexity of the TIME -\nSTRATIFIED de model over its CONTROL de counterpart.\nB Dynamic evaluation\nHere we more formally describe dynamic evaluation, which we apply to theTIME -STRATIFIED model,\nand outline some of the hyper-parameter choices used for our dynamic evaluation experiments (§6).\nLet {D(1),D(2),··· ,D(N)}be a collection of N chronologically-ordered test documents, where\nD(t−1) was published before D(t), and D(1) was our ﬁrst test document in the 2018-2019 evaluation\nperiod (§2.1). Each test document D(t) consists of M = |D(t)|tokens x(t) = x(t)\n1 ,x(t)\n2 ,··· ,x(t)\nM .\nFurthermore, let θ1 be the set of Transformer-XL model parameters (§2.3)after training on documents\nfrom the pre-2018 training period (TIME -STRATIFIED setup; §2.1), and before any dynamic evaluation\nis applied.\nThe loss of the Transformer-XL model with respect to a test document D(t) is computed as follows:\nℓ(D(t); θt) = log pθt (x(t)) = log\n(M∏\ni=1\npθt (x(t)\ni |x(t)\n<i)\n)\n=\nM∑\ni=1\nlog pθt (x(t)\ni |x(t)\n<i), (1)\nwhere x(t)\n<i denotes tokens x(t)\n1 ,x(t)\n2 ,··· ,x(t)\ni−1 in the test document D(t) that precede x(t)\ni .\nIn dynamic evaluation (Mikolov et al., 2010; Graves, 2013; Krause et al., 2018, 2019), we dynamically\nupdate the Transformer-XL model parameters using gradient descent, based on the knowledge\ncontained in the test documents that had been seen so far. More formally,\nθt+1 ←θt −α∇θt ℓ(D(t); θt), (2)\nwhere αdenotes the dynamic evaluation learning rate, and ∇θt ℓ(D(t); θt) denotes the gradient of\nthe model parameters with respect to the model’s loss for the current documentℓ(D(t); θt).\nThis procedure means that the model parameters θt, which we use to evaluate the model\non the current test document D(t), already encodes knowledge from previous test documents\n18\nD(1),D(2),··· ,D(t−1), in addition to the knowledge learnt from the training set. This in turn\nenables the model to learn about new information that emerges or becomes more salient during the\nevaluation period (e.g. “COVID-19” in late-2019), which is then stored in the model parameters,\nand reuse such information for better prediction of subsequent test documents. In practice, our\nimplementation of dynamic evaluation differs from Eq. 2 in two ways: (i) We perform K steps of\ngradient descent for each document, rather than only one step, where Kis tuned on the validation set;\nand (ii) we perform the gradient updates for a batch of contiguous tokens (e.g. 512), which means\nthat documents that are longer than the batch size will have more than one parameter update.\nContrast with non-dynamic evaluation. When dynamic evaluation is not applied, θt = θt−1 =\nθ1. This means that the same model parameters θ1 (i.e. model parameters after training on the\ntraining documents— without updating the models’ knowledge on the observed test documents) are\nused to predict all test documents, risking the model becoming outdated in-between retraining cycles.\nDynamic evaluation hyper-parameters. We use the following learning rates (WMT: 5e-5, CUS-\nTOM NEWS :5e-4, ARXIV: 1e-3), which are tuned on the validation set spanning three months,\nwhereas the test set spans two years. We leave the question of choosing a learning rate with an\noptimal trade-off between adaptation speed and stability of updates without a priori knowledge of\nthe evaluation period to future work.\nB.1 Dynamic Evaluation and Catastrophic Forgetting\nWe design an experiment to assess whether updating a model on present data using dynamic evaluation\nleads to catastrophic forgetting of the past data. To assess this, we report the performance of the two\nmodels, i.e., the one trained until 2017 and the one updated up to 2019, on a test set derived from the\ninitial training data of the model covering the years up to the year from which we started performing\ndynamic evaluation (i.e., 2007-2017). In addition, we also report the results on the 2018-2019 test set\nwhich were presented in Section 6.\nFigure 9 presents the results for WMT andARXIV. For both datasets we observe that as we move\ntowards the past, the perplexity of the model updated with dynamic evaluation increases. As such,\nwhile the updated model outperforms the outdated model for the recent 2018 and 2019 years, the\nsame model performs increasingly worse on the past years, as indicated by the gentle upward slope\nfrom 2017 and onwards.\nFigure 9: Catastrophic forgetting as measures in terms of relative perplexity increase when comparing\nthe models updated with dynamic evaluation against the models that have been trained with data up\nto 2017. The x-axis presents the years in a reverse chronological order.\nC Example Question-Answer Pairs\nC.1 Examples of closed-book QA on synthetic questions on government ofﬁcials\nQuestion: Who was the governor in Texas on 5 September 2019? Answer: Greg Abbott\n19\nQuestion: Who was the prime minister in Canada on 8 June 2019? Answer: Justin Trudeau\nQuestion: Who was the president in Portugal on 30 May 2019? Answer: Marcelo Rebelo de Sousa\nC.2 Examples of reading comprehension on NewsQA\nDocument: England international footballer Steven Gerrard was found not guilty of affray by a\ncourt in his home city on Friday. England international Steven Gerrard was cleared by a court in\nLiverpool of affray. The jury at Liverpool Crown Court took a little over an hour to clear Gerrard of\ncharges relating to a fracas in a nightclub bar in the north-western of England city on December 29\nof last year. They accepted the Liverpool captain´s version that he acted in self defense in punching\nbusinessman Marcus McGhee. The 29-year-old was the only one of the seven defendants in the\ncase to be cleared after an incident which was described by judge Henry Globe as an \"explosion of\nviolence.\" Gerrard spoke of his relief outside the court. \"Can I just say how pleased I am with today ´s\nverdict,\" he said. \"I ´m glad to put this case behind me and I am really looking forward to the season\nahead and concentrating on my football now. \"I would just like to say a big thank you to my legal\nteam and to my friends and family and everyone at Liverpool football club for supporting me.\" His\ncomments were met with a round of applause from a large group of fans of the Premier League club\nwho had gathered outside the court, before he was ushered away. Gerrard was celebrating in the\nLounge Inn in Southport, a suburb of Liverpool, after scoring twice his team´s 5-1 win at Newcastle\nwhich took them to the top of the Premier League. Video footage, which was available to the court,\nshowed.\nQuestion: Who was cleared by a Liverpool court? Answer: Steven Gerrard\nDocument: CNN afﬁliates report on where job seekers are ﬁnding work across the country and how\nthose looking for employment are coping with the situation. A census employee poses with the new\nhandheld device ﬁeld workers will use for the 2010 count. (CNN) – The nation will take roll call\nin 2010 and the federal government is giving the states money to hire thousands of census workers.\nOfﬁcials in Colorado say they may hire as many as 8,000 workers for positions that last between 10\nweeks and one year. Cathy Illian says the bureau has already hired 800 people in the Denver area.\nThe organization will also post open positions in early April. Some jobs pay as much as $28.75 an\nhour. Read the story on KMGH. In Idaho, Dave Mulvihill, manager of the state´s census bureau, said\nthe organization will hire 1,200 workers. He has plenty of job searchers to choose from. \"We´ve had\napplications from approximately 7,300 people across the state,\" he told CNN afﬁliate KIVI. Read the\nfull report on census jobs. The ofﬁce is holding off on taking any more applications until fall. The\nAlabama census bureau is preparing to hire between 1,000 and 1,500 workers. \"We need workers\nso we can get good addresses [to] send the questionnaires out so we can get a good response,\" state\ncensus bureau ofﬁcial Darryl Lee told TV Alabama in Birmingham. Census ofﬁcials point out that an\naccurate count of U.S. citizens helps the government ﬁgure out how much funding to give each state\nfor federally sponsored programs. Read the ABC 33/40 story Northeast: Rhode Island strip club.\nQuestion: Census bureaus are hiring people from where? Answer: Denver area\n20",
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.8075387477874756
    },
    {
      "name": "Computer science",
      "score": 0.804323673248291
    },
    {
      "name": "Generalization",
      "score": 0.49907374382019043
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46074286103248596
    },
    {
      "name": "Key (lock)",
      "score": 0.4538778066635132
    },
    {
      "name": "Train",
      "score": 0.4238268733024597
    },
    {
      "name": "Machine learning",
      "score": 0.3656960129737854
    },
    {
      "name": "Cartography",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}