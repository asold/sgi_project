{
  "title": "When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",
  "url": "https://openalex.org/W4287887107",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2099518739",
      "name": "Sebastian Schuster",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A817205692",
      "name": "Tal Linzen",
      "affiliations": [
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2995181640",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W2864832950",
    "https://openalex.org/W2414435607",
    "https://openalex.org/W1542351050",
    "https://openalex.org/W3173798466",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W3099772776",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2962961857",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W3034510440",
    "https://openalex.org/W2921890305",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2997868155",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3202546170",
    "https://openalex.org/W2100513762",
    "https://openalex.org/W3101652466",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2625800120",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3210694367",
    "https://openalex.org/W2994726368",
    "https://openalex.org/W3034685497",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W3166920165",
    "https://openalex.org/W3199610709",
    "https://openalex.org/W3203259592",
    "https://openalex.org/W2941666437",
    "https://openalex.org/W2996728628"
  ],
  "abstract": "Understanding longer narratives or participating in conversations requires tracking of discourse entities that have been mentioned. Indefinite noun phrases (NPs), such as 'a dog', frequently introduce discourse entities but this behavior is modulated by sentential operators such as negation. For example, 'a dog' in 'Arthur doesn't own a dog' does not introduce a discourse entity due to the presence of negation. In this work, we adapt the psycholinguistic assessment of language models paradigm to higher-level linguistic phenomena and introduce an English evaluation suite that targets the knowledge of the interactions between sentential operators and indefinite NPs. We use this evaluation suite for a fine-grained investigation of the entity tracking abilities of the Transformer-based models GPT-2 and GPT-3. We find that while the models are to a certain extent sensitive to the interactions we investigate, they are all challenged by the presence of multiple NPs and their behavior is not systematic, which suggests that even models at the scale of GPT-3 do not fully acquire basic entity tracking abilities.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 969 - 982\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nWhen a sentence does not introduce a discourse entity,\nTransformer-based models still sometimes refer to it\nSebastian Schuster\nCenter for Data Science\nDepartment of Linguistics\nNew York University\nschuster@nyu.edu\nTal Linzen\nCenter for Data Science\nDepartment of Linguistics\nNew York University\nlinzen@nyu.edu\nAbstract\nUnderstanding longer narratives or participat-\ning in conversations requires tracking of dis-\ncourse entities that have been mentioned. In-\ndefinite noun phrases (NPs), such as a dog ,\nfrequently introduce discourse entities but this\nbehavior is modulated by sentential operators\nsuch as negation. For example, a dog in Arthur\ndoesn’t own a dog does not introduce a dis-\ncourse entity due to the presence of negation.\nIn this work, we adapt the psycholinguistic\nassessment of language models paradigm to\nhigher-level linguistic phenomena and intro-\nduce an English evaluation suite that targets\nthe knowledge of the interactions between sen-\ntential operators and indefinite NPs. We use\nthis evaluation suite for a fine-grained inves-\ntigation of the entity tracking abilities of the\nTransformer-based models GPT-2 and GPT-3.\nWe find that while the models are to a certain\nextent sensitive to the interactions we investi-\ngate, they are all challenged by the presence\nof multiple NPs and their behavior is not sys-\ntematic, which suggests that even models at the\nscale of GPT-3 do not fully acquire basic entity\ntracking abilities.\n1 Introduction\nIn order to understand longer narratives or to par-\nticipate in conversations, humans and natural lan-\nguage understanding systems have to keep track\nof the entities that have been mentioned in the dis-\ncourse. For example, when someone tells you that\nArthur owns a dog, they have introduced the entity\nof a person named Arthur and the entity of a dog\nowned by Arthur into the discourse. Once entities\nhave been introduced to the discourse, it is natural\nto refer back to them with noun phrases or pro-\nnouns to elaborate further, e.g., by saying It has a\nred collar to elaborate on the dog.\nWhile no fully-specified account exists of how\nhumans achieve this feat, many existing theories\nare based on the idea that humans maintain mental\nfiles (e.g., Heim, 1982; Murez and Recanati, 2016),\ni.e., explicit memory representations for each entity\nthat encode all properties of an entity and its rela-\ntion to other entities. When engaging in a conver-\nsation or reading a longer narrative, humans then\nupdate these representations as they encounter new\nentities or new information about existing entities.\nLarge pre-trained language models (LMs) such\nas GPT-2 (Radford et al., 2019) and GPT-3 (Brown\net al., 2020), which in recent years have become\nthe dominant foundation for natural language un-\nderstanding and generation tasks, lack explicit rep-\nresentations of discourse entities. It remains largely\nan open question to what extent LMs can match\nhuman behavior in tracking discourse entities.\nThe most extensive investigation of this phe-\nnomenon has been through evaluations with the\nLAMBADA dataset (Paperno et al., 2016). LAM-\nBADA consists of a cloze task for which an LM\nhas to predict the last word of naturalistic passages\nextracted from a corpus. Solving this task requires\nkeeping track of longer contexts, and making a cor-\nrect guess frequently requires keeping track of the\nentities mentioned in the passage.\nWhile datasets such as LAMBADA are an invalu-\nable resource for monitoring high-level progress\nof LMs in their ability to track discourse entities,\nsuch datasets lack the granularity to determine for\nwhich contexts LMs can and cannot properly track\ndiscourse entities. In this work, we draw inspira-\ntion from recent targeted evaluation suites geared\nat lower linguistic levels (e.g., Marvin and Linzen,\n2018; Hu et al., 2020b), and introduce a targeted\nevaluation suite for tracking of discourse entities in\nEnglish. In particular, we focus on the interactions\nbetween different sentential operators and embed-\nding verbs and indefinite noun phrases (see, e.g.,\nKarttunen 1976 and Section 3); for example, we\nevaluate whether a language model correctly infers\nthat because a sentence with a negation, such as\nArthur doesn’t own a dog, does not introduce a dis-\n969\ncourse entity for a dog, further elaborations about\nsuch a non-existent dog should be pragmatically\nodd, and, as such, their probability should be low\ncompared to matched controls.\nTo evaluate to what extent language models\nare sensitive to these interactions, we adapt the\npsycholinguistic assessment of language models\nparadigm (Futrell et al., 2019) for discourse entity\ntracking and discuss the methodological challenges\nthat arise with using this paradigm for discourse\nphenomena. We introduce two expert-created eval-\nuation suites and use them to evaluate GPT-2 and\nGPT-3 models. We find that while all the models\nwe evaluated show some sensitivity to preceding\ncontext, they lack systematicity and are challenged\nwhen contexts contain multiple noun phrases.1\n2 Related Work\nThe majority of systematic evaluations of autore-\ngressive and masked language models so far have\nfocused on the level of syntax, targeting abilities\nsuch as subject-verb agreement (e.g., Linzen et al.,\n2016; Marvin and Linzen, 2018; Gulordava et al.,\n2018; Hu et al., 2020b), anaphora agreement and\nbinding constraints (e.g., Marvin and Linzen, 2018;\nFutrell et al., 2019; Warstadt et al., 2020; Hu et al.,\n2020a), or filler-gap dependencies (e.g., Wilcox\net al., 2018; Chowdhury and Zamparelli, 2018;\nDa Costa and Chaves, 2020). At higher linguistic\nlevels, Ettinger (2020) compared BERT’s (Devlin\net al., 2019) behavior on sentences with negation\nto data from neurolinguistic experiments with hu-\nmans; Pandia and Ettinger (2021) investigated to\nwhat extent pre-trained language models can ex-\ntract relevant information from the preceding con-\ntext, both in the presence and in the absence of\ndistractors; and Pandia et al. (2021) investigated\nwhether language models can predict connectives\n(e.g., but) for two given sentences.\nMore closely related to our work, in the domain\nof discourse and reference, Upadhye et al. (2020)\ninvestigated whether GPT-2 and Transformer-XL\n(Dai et al., 2019) exhibit similar referential biases\nin their continuations as humans, i.e., they asked\nwhether models provided with a sentence with\ntwo referents are biased similarly as humans when\nchoosing the referent for the next sentence. Kim\net al. (2019) used an acceptability judgment task to\n1Our evaluation suites along with the results from\nhuman experiments and all code for model evalua-\ntion is available at https://github.com/sebschu/\ndiscourse-entity-lm.\ndetermine whether contextual LMs correctly distin-\nguish between definite and indefinite noun phrases.\nSorodoc et al. (2020) and Tenney et al. (2019)\nused probing methods to investigate whether repre-\nsentations of LSTM- and Transfomer-based models\ncontain information about coreference, which also\nprovides some indication of entity tracking abil-\nities. Further, Clark et al. (2019) investigated to\nwhat extent attention weights of BERT indicate\ncoreference. These studies found that all evaluated\nrepresentations contain some information about\ncoreference but not consistently for all entities.\n3 Background\nEnglish indefinite noun phrases (NPs) of the form\na NOUN interact with the context in complex ways\n(see, e.g., Karttunen, 1976; Webber, 1979; Heim,\n1982, for more extensive discussions of this phe-\nnomenon). In affirmative statements, the indefinite\nNP generally introduces a new entity to the dis-\ncourse. However, several sentential operators and\nclause-embedding verbs modulate this behavior.\nFor example, consider the following contrast be-\ntween an affirmative and a negated sentence, where\n# indicates a pragmatically odd continuation:\n(1) a. Arthur owns a dog and it follows him\neverywhere he goes.\nb. Arthur doesn’t own a dog and # it fol-\nlows him everywhere he goes.\nWhile in the affirmative sentence, the indefinite NP\nintroduces a novel discourse entity, the negation in\n(1b) prevents the NP from introducing a new entity.\nIn (1b), it is therefore pragmatically odd to refer\nback to a dog with the pronoun it.\nThe implicative manage to and the negative im-\nplicative fail to in (2a-b) give rise to a similar con-\ntrast: The NP under manage to introduces a dis-\ncourse entity, the NP under fail to does not.\n(2) a. Sue managed to write a book. It was a\nreal page-turner.\nb. Sue failed to write a book. # It was a\nreal page-turner.\nIndefinite NPs embedded under the factive know\nand the non-factive doubt introduce and fail to in-\ntroduce a discourse entity, respectively:\n(3) a. I know that Michael baked a cake. It\nwas delicious.\nb. I doubt that Michael baked a cake. # It\nwas delicious.\n970\nLastly, modals such as want also block the intro-\nduction of a discourse entity, as shown in (4):\n(4) a. Mary got a pet rat and it is very loud at\nnight.\nb. Mary wants to get a pet rat and # it is\nvery loud at night.\nWhile these patterns generally hold, there are\nexceptions to these rules. For example, in the first\nsentence in (5), which could be paraphrased as\n(6), the indefinite scopes over the negation and\ntherefore it is okay to refer back to the mistake.\n(5) Mary didn’t find a (specific) mistake. It\nwas in the footnote.\n(6) There was a (specific) mistake which Mary\ndid not find. It was in the footnote.\nHowever, without additional context, listeners\ngenerally do not infer these so-called specific in-\nterpretations of sentences with an indefinite NP, so\nlike Karttunen (1976), we will largely ignore these\ncases throughout the remainder of this paper.\n4 Experiments\nTo what extent are GPT-2 and GPT-3 sensitive to\nthe contrasts that we presented in Section 3? To\ninvestigate this question, we adapted the methodol-\nogy commonly used for syntactic evaluation of lan-\nguage models (e.g., Futrell et al., 2019) and created\nminimal pairs of contexts that differ in whether\nthey introduce a discourse entity or not. In Ex-\nperiment 1, we focus on contexts with a single\nindefinite NP, and in Experiment 2, we focus on\nsentences with multiple indefinite NPs.\n4.1 Experiment 1\nMethodology If a language model is sensitive to\ncontexts that differ in whether a discourse entity\nis introduced or not, we expect the probability of\ncontinuations that refer back to the noun phrase in\nthe previous context to be higher when a discourse\nentity has been introduced than when it has not.\nThus, if we have a pair of sentences, such as\n(7) a. C ref: John owns a dog.\nb. C nonref: John doesn’t own a dog.\nand a referential continuation,2 such as\n2The psycholinguistic assessment of language models\nparadigm generally focuses on the probability of individual\nwords rather than sequences to evaluate syntactic phenomena.\nHowever, considering that the coreference ofit (or other ref-\n(8) R: It has a red collar.\nthen we expect that\nP(R |Cref) > P(R |Cnonref).\nHowever, directly comparing these probabilities\nmay be problematic given that P(X |Cref) and\nP(X | Cnonref) are different probability distri-\nbutions. In theory it could be, for example, that\nP(X |Cref) assigns a very high probability to ex-\nactly one continuation and therefore its entropy is\nlower than the entropy of P(X |Cnonref). In this\ncase, it could be that the inequality above does not\nhold despite the fact that continuations that refer\nback to the noun phrase in the previous context are\nranked higher in the affirmative than in the negated\ncase. To overcome this issue, we make use of a\nnon-referential control continuation, such as N:\n(9) N: It is not a big deal.\nThis continuation no longer refers back to a noun\nphrase and is therefore a valid continuation for both\ncontexts. Instead of using the inequality above,\nwe thus compare the relative probabilities of the\nreferential and the control continuations:\nP(R |Cref)\nP(R |Cref) +P(N |Cref) (1)\n> P(R |Cnonref)\nP(R |Cnonref) +P(N |Cnonref)\nThese relative probabilities are less sensitive to\nthe entropy of the distribution: If there is a highly\nlikely continuation (that is neither the referential\nnor the control continuation) for one context but not\nthe other, the model should still assign relatively\nless probability mass to the referential continuation\ncompared to the control continuation.\nModels We evaluate two autoregressive language\nmodels,3 GPT-2 and GPT-3. GPT-2 models were\ntrained on the WebText corpus which contains an\nestimated 8 billion tokens; GPT-3 models were\ntrained on about 500 billion tokens. For GPT-2,\nerential expressions) is modulated by an entire sentence or\nclause (see the contrast between (8) and (9), which both con-\ntain the pronoun it), we compare probabilities of sequences.\n3We selected these autoregressive models instead of\nmasked language models (MLMs) such as BERT (Devlin\net al., 2019) because they are more frequently used to generate\ntexts, and discourse abilities such as entity tracking tend to\nplay a more crucial role in generating text than in classification\nor span extraction tasks for which MLMs are more frequently\nused.\n971\nwe evaluate models of four different sizes (GPT-\n2: 117M parameters, GPT-2 M: 345M, GPT-2 L:\n762M, GPT-2 XL: 1.5B) that are available through\nthe HuggingFace Transformers library (Wolf et al.,\n2020). For GPT-3, we evaluate the largest available\nmodel (“davinci”) through the OpenAI API which\nis assumed to have about 175B parameters.4\nMaterials We manually constructed an evalua-\ntion set of 16 base contexts and plausible continua-\ntions. Each base context contains different nouns\nand verbs to minimize lexical effects. From these\n16 contexts, we constructed four contrasts for each\ncontext, as shown in Table 1, which in total yielded\n64 items. We chose to manually construct con-\ntexts as opposed to generating sentences from a\ngrammar to guarantee semantic and pragmatic well-\nformedness of contexts and continuations.\nHuman evaluation As we mentioned in Sec-\ntion 3, the referential continuations are not nec-\nessarily pragmatically odd if the indefinite noun\nphrase in the context is interpreted as a specific\nnoun phrase. We therefore conducted an online\nexperiment on Prolific to verify that native En-\nglish speakers disprefer the referential continua-\ntions when no discourse entity is introduced, as\nfollows. After two practice items, each participant\nperformed two trials with sentences from the eval-\nuation set. On each trial, participants saw a context\nalong with a referential and a non-referential con-\ntinuation, and they were asked to indicate their\npreferred continuation by selecting the continua-\ntion that “makes more sense” given the context. For\neach context, we collected preference judgments\nfrom 10 participants. The experiment took on aver-\nage about 2 minutes to complete and participants\nreceived $0.45 in compensation (∼$14/hr).\nResults and discussion Figure 1 shows the pro-\nportion of items for which the relative probability\nof the referential continuation (RRP) is higher for\nthe context that introduces a discourse entity (DEC)\nthan for the context that does not (NDEC), i.e., the\nproportion of items for which Eq. 1 holds. For\nthree of the four contrasts ( affirmative-negation,\naffirmative-modal, managed-failed) GPT-2 and\nGPT-3 models exhibited the expected pattern for\n4The model size of GPT-3 is not publicly available but the\nEleutherAI project estimated the model size by comparing\nthe performance of the models available through the API\nto published results: https://blog.eleuther.ai/\ngpt3-model-sizes/.\nalmost all items in our evaluation set. For theknow-\ndoubt contrast, however, all models performed ap-\nproximately at chance, suggesting that the models\nare not sensitive to this contrast.\nFigure 1 also shows the results of the human\nexperiment. Participants preferred the referential\ncontinuation more following the DECs than fol-\nlowing the NDECs for all items of the affirmative-\nnegation and managed-failed contrasts. Further, for\nthese two contrasts, participants overwhelmingly\nselected the referential continuation for the DECs\nand the non-referential continuation for the NDECs.\nThis result confirms that the stimuli bring about the\ntheoretically expected contrast in humans.\nFor the affirmative-modal and the know-doubt\ncontrasts, the results from human participants are\nless clear-cut. Overall, participants also preferred\nthe referential continuation more in the DECs than\nin the NDECs. However, for several items, the op-\nposite was the case and the referential continuation\nwas preferred as much or even more in the NDECs\nthan in the DECs. Moreover, unlike in the other\ntwo contrasts, participants selected the referential\ncontinuation in the NDECs at a high rate.5\nConsidering that the results from the human ex-\nperiment are not predicted by Karttunen’s theory,\nthe model results from the affirmative-modal and\nthe know-doubt contrast should also be interpreted\nwith caution. However, while the lower proportion\nof expected relative probabilities in theknow-doubt\ncondition may suggest that the models are behav-\ning similarly to humans, this is not the case. If one\nconsiders the results on an item-by-item basis, they\ndiffer from the human results and there is a lot of\nvariability across models such that the five models\nagree only on less than 33% of items.\nIn summary, GPT-2 and GPT-3 overall behaved\nsimilarly to humans and generally favored the ref-\nerential continuation more when the preceding sen-\n5For contexts with modals, some participants commented\nthat they selected the referential continuation because they\nassumed that the past tense of the continuation was a gram-\nmatical mistake. That is, if the tense had been different, the\ncontinuation would have been sensible. For example, for the\ncontext Michael wants to bake a cake the continuation and it\nwill bethe best thing at the picnic is acceptable and differs\nfrom the continuation that was presented in the experiment,\nand it was the best thing at the picnic, only in its tense.\nFor contexts with doubt, participants frequently seemed to\ninterpret the referential continuation as a reason for the doubt.\nFor example, for the context I doubt that Carla got a pet rat.,\nparticipants frequently chose the referential continuation It is\nvery noisy at night., presumably because they considered that\nthe rat being noisy made it unlikely that Carla would have got\nit.\n972\nContrast Contexts Referential continuation Non-referential continuation\naffirmative-negationMichael baked a cake and it was the best thing at the picnic. and it’s not a big deal.Michael didn’t bake a cake\naffirmative-modalMichael baked a cake and it was the best thing at the picnic. and it’s not a big deal.Michael wants to bake a cake\nknow-doubt I know that Michael baked a cake.It was the best thing at the picnic. It’s not a big deal.I doubt that Michael baked a cake.\nmanaged-failed Michael managed to bake a cake.It was the best thing at the picnic. It’s not a big deal.Michael failed to bake a cake.\nTable 1: Example contexts and continuations for one base context in Experiment 1.\naffirmative - negation affirmative - modal know - doubt managed - failed\nGPT-2\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nGPT-3\nHuman\nGPT-2\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nGPT-3\nHuman\nGPT-2\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nGPT-3\nHuman\nGPT-2\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nGPT-3\nHuman\n0.00\n0.25\n0.50\n0.75\n1.00% expected\nFigure 1: Results from Experiment 1. Each bar indicates the proportion of items for which the relative probability\nof the referential continuation (RRP) is higher for the context that introduces a discourse entity than for the context\nthat does not, i.e., the expected pattern. Dashed lines indicate chance performance levels, and error bars indicate\nbootstrapped 95% confidence intervals.\ntence introduced a discourse entity. This behavior\ncould be due to at least the following two reasons.\nIt could be that the models indeed correctly com-\nbine the sentential operators with the indefinite\nnoun phrase and therefore assign a higher prob-\nability to a referential continuation in the DECs.\nHowever, it could also be that this result is due to\nmore spurious correlations; for example, it could be\nthat the model learned that clauses with operators\nsuch as negation, modals, or negative implicatives\nare often followed by clauses with a non-referential\nit. In the second experiment, we tease apart these\ntwo explanations and further try to overcome the\nissues with the stimuli that we observed for the\naffirmative-modal and know-doubt contrasts.\n4.2 Experiment 2\nMaterials and method We again constructed 16\nbase contexts that are similar to the ones used in\nExperiment 1. However, in this experiment, each\ncontext contains two indefinite noun phrases with\ndifferent nouns that are embedded under two dif-\nferent sentential operators. For example, for the\naffirmative-negation contrast, one of the NPs is\nembedded under negation, such as a cat in (10).\n(10) John owns a dog but he doesn’t own a cat.\nIn such a context, it is natural to continue with\na sentence that refers back to the dog, whereas it\nis unnatural to refer back to a cat. We therefore\ncompared the models’ probability of a sentence that\nrefers back to an entity that has been introduced\nin the context (11a) to a sentence that refers to an\nentity that has not been introduced (11b).\n(11) a. The dog follows him wherever he goes.\nb. # The cat follows him wherever he\ngoes.\nOn top of these coreferential continuations, we\nalso constructed non-coreferential continuations\nfor contexts such as (10). These continuations\ncontain one of the nouns present in the context\nbut do not refer back to entities in the previous\ncontext. For the non-coreferential continuations,\nmodels should assign a higher probability to the\ncontinuation with a noun for which no discourse\nentity had been introduced in the context.\n(12) a. The cat that he liked had been adopted\nby someone else.\n973\nContext Coreferential continuations Non-coreferential continuations\nMary found a shirt at the store but she didn’t find a hat. The shirt/#hat is blue. The hat/#shirt that she tried on didn’t fit.\nMary found a hat at the store but she didn’t find a shirt. The hat/#shirt is blue. The shirt/#hat that she tried on didn’t fit.\nMary didn’t find a shirt at the store but she found a hat. The hat/#shirt is blue. The shirt/#hat that she tried on didn’t fit.\nMary didn’t find a hat at the store but she found a shirt. The shirt/#hat is blue. The hat/#shirt that she tried on didn’t fit.\nTable 2: Example contexts and continuations for the affirmative-negation contrast for one base context.\naffirmative - negation affirmative - modal know - doubt managed - failed\ncoreferentialnon-coreferential\nGPT-2\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nGPT-3\nHuman\nGPT-2\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nGPT-3\nHuman\nGPT-2\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nGPT-3\nHuman\nGPT-2\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nGPT-3\nHuman\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00% expected\nFigure 2: Results from Experiment 2. Dashed lines indicate chance performance levels.\nb. # The dog that he liked had been\nadopted by someone else.\nFor each of the four contrasts and each base con-\ntext, we constructed four contexts that crossed the\norder of the sentential operators and the order of\nthe two nouns used in a context, resulting in 4 con-\ntexts per base context and contrast. For each base\ncontext, we further constructed two coreferential\ncontinuations (one for each noun) and two non-\ncoreferential continuations (one for each noun). In\ntotal, this yielded 512 items. Table 2 shows all the\ncontexts and continuations for one base context for\nthe affirmative-negation contrast.\nCompared to the methods and materials in Ex-\nperiment 1, this setup has several advantages. First,\ngiven that we are comparing two continuations for\na fixed context, both continuations come from the\nsame probability distribution and therefore we no\nlonger need a generic control continuation. Sec-\nond, it is less likely that models can make use of\nspurious correlations since each context contains\ntwo types of sentential operators and, for exam-\nple, a heuristic of associating negation with non-\nreferential it would no longer lead to the expected\nbehavior. Third, given that all continuations are on\ntopic (as opposed to the generic control condition\nin Experiment 1), humans likely show more con-\nsistency in their preferences. Lastly, given that this\ndesign allows us to construct stimuli with exactly\nthe same tokens in different orders, we can also\nassess the systematicity of the model behavior.\nWe again verified the theoretically predicted pref-\nerences in a human experiment.6\nResults and discussion Figure 2 shows the ac-\ncuracy of the model and human experiments for\nthe coreferential and non-coreferential continua-\ntions. As this figure shows, humans exhibited the\ntheoretically expected behavior for all contrasts for\nalmost all items and chose the coreferential con-\ntinuation with the noun for which an entity had\nbeen introduced in the context, and chose the non-\ncoreferential continuation for the noun for which\nno entity had been introduced. This suggests that\nthe materials do not exhibit the same shortcomings\n6For practical reasons, we included two items from this\nexperiment in the first human experiment. To rule out inter-\nference between similar items, no two items within the same\nexperimental list were derived from the same base context.\n974\naffirmative - negation affirmative - modal know - doubt managed - failed\nGPT-2\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nGPT-3\nGPT-2\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nGPT-3\nGPT-2\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nGPT-3\nGPT-2\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nGPT-3\n0.00\n0.25\n0.50\n0.75\n1.00% expected\nFigure 3: Systematicity of model behavior in Experiment 2. An item counts as correct if all four orders of noun\nphrases and sentential operators (e.g., X owns a A but doesn’t own a B; X owns a B but doesn’t own a A; X doesn’t\nown a A but owns a B; and X doesn’t own a B but owns a A) lead to the correct result. The dashed line indicates\nchance performance and the error bars indicate bootstrapped 95% confidence intervals.\nas in Experiment 1, and that comparisons of models\nto human behavior are valid for all four contrasts.\nIf we turn to the model results, there is more\nvariability in performance across models and con-\ntrasts. For the coreferential continuations, all mod-\nels except the smallest GPT-2 model performed\nabove chance for three of the four contrasts. For\nthe affirmative-modal contrast, however, only GPT-\n3 performed significantly above chance. More-\nover, all GPT-2 models perform worse for the non-\ncoreferential continuations.\nMore generally, unlike humans, all models in\nthis experiment performed below ceiling, which\nsuggests that while models exhibit a tendency to\nchoose the right continuation, they do not reliably\ndo so. Further, model size does have an impact\non the performance on this task: The smallest\nGPT-2 model performed consistently worst, and\nGPT-3 performed consistently best. This depen-\ndence on model size is particularly pronounced in\nthe non-coreferential condition: While GPT-3 con-\nsistently performed above chance in all contrasts,\nmost smaller models either performed at chance\nor in some cases, such as the smallest GPT-2 for\nthe items in the affirmative-negation contrast, had\na bias to select the non-coreferential continuation\nwith the noun that introduced a discourse entity in\nthe context. The lower performance for the non-\ncoreferential continuations is not surprising given\nthat for these examples, a model not only has to\ncorrectly infer which noun phrase introduces a dis-\ncourse entity but additionally that the noun phrase\nin the continuation does not refer back to anything\nin the preceding context.\nSystematicity As mentioned above, this experi-\nmental design also allows us to assess how sensi-\ntive the behavior of the different models is to the\ndifferent orders of sentential operators and nouns\nin the context. Figure 3 shows the proportion of\nitems for which the model exhibited the expected\nbehavior for all four possible orders. Overall, the\nperformance of all models according to this stricter\ncriterion is much lower than the simple by-item\nmeasure highlighting that even the predictions by\nGPT-3 are sensitive to the exact combination and\norder of sentential operators and nouns. However,\nthere once again is a clear trend that larger models\nbehave more systematically than smaller ones, sug-\ngesting that larger models and models trained on\nmore data learn more stable generalizations. This\ntrend is in part driven by smaller models being less\nsensitive to the preceding context: The two small-\nest GPT-2 models assigned the highest probability\nto the continuation with one of the two nouns inde-\npendent of the combination of sentential operators\nand nouns in the context in 52.3% and 43.8% of\nthe cases, respectively. That is, for all four con-\ntexts, as shown for one example in Table 2, the\nsmallest GPT-2 model assigned a higher probabil-\nity to the same continuation independent of which\nnoun phrase introduced a discourse entity more\nthan half of the time. GPT-3, on the other hand,\nonly exhibited this behavior for 7% of the items.\nIn summary, the results from Experiment 2 sug-\ngest that GPT-2 and GPT-3 are less reliable in de-\ntermining whether an NP introduces a discourse\nentity when multiple NPs are present. This is in\nparticular true for smaller GPT-2 models but if one\nconsiders systematicity, the predictions of GPT-3\nare also sensitive to minor changes in the context.\n975\n5 Likely Continuations\nOne drawback of the methodology of the previous\ntwo experiments is that we considered only one\nspecific expected and one specific unexpected con-\ntinuation for each item. Thus, if both the expected\nand the unexpected continuations are very unlikely\naccording to the LM, we may see poor performance\non this task while at the same time, it would be very\nunlikely that either of the generations is ever sam-\npled from the model. In that case, the evaluations in\nExperiment 2 may underestimate the models’ abili-\nties (Newman et al., 2021) and the results may not\nbe very relevant for practical purposes for which\none uses an LM to generate texts. For this reason,\nwe also performed a manual analysis of randomly\nsampled generations (Aina and Linzen, 2021) from\nthe two largest LMs, GPT-2 XL and GPT-3.\nMaterials and method We used the contexts\nfrom Experiment 2 as prompts for the two LMs\nand for each context, we sampled a sentence be-\nginning with the.7 For GPT-2 XL, we sampled\nthe continuations using top-40 sampling as in Rad-\nford et al. (2019). For GPT-3, we used the default\ntemperature sampling with a temperature of 0.7.\nA graduate student in linguistics who was blind\nto the purpose of this study then annotated each\nof the continuations for whether it contained re-\nferring expressions that likely referred back to a\nnoun phrase in the context as well as which noun\nphrase(s) (the discourse entity introducing and/or\nthe non-discourse entity introducing NP) were re-\nferred back to. To illustrate this, consider the fol-\nlowing two generations by GPT-3:\n(13) a. Carolyn didn’t write a card to her par-\nents but she wrote them a letter. The\nletter was long and filled with many\ndetails about the cruise.\nb. Chris managed to knit a hat but failed\nto knit a bag. The bag is not stuffed.\nIn (13a), the letter refers back to an entity intro-\nduced in the context, whereas in (13b), the bag\nrefers back to the NP that does not introduce a\ndiscourse entity. If a language model is able to cor-\nrectly combine sentential operators with indefinite\n7As compared to just using the context as a prompt, con-\nstraining the continuation to start with the led to considerably\nmore continuations with noun phrases referring back to a noun\nphrase in the context while still putting very few constraints\non the overall continuation.\nModel DE NDE\nGPT-2 XL 43.8 22.3\nGPT-3 52.3 21.1\nTable 3: Percentage of expressions in model generations\nthat refer back to noun phrases that introduce (DE) or\ndo not introduce a discourse entity (NDE).\nnoun phrases, we expect many continuations as in\n(13a) and no continuations as in (13b).\nResults and discussion Table 3 shows the per-\ncentages of expressions in model generations that\nrefer back to noun phrases in the prompt. These re-\nsults confirm the findings from Experiment 2: Both\nGPT-2 XL and GPT-3 are to some extent sensitive\nto the interactions between sentential operators and\nindefinite NPs as indicated by the higher proportion\nof expressions referring back to NPs that introduce\ndiscourse entities (DE) as compared to referring\nback to NPs that do not (NDE). At the same time,\nhowever, both models produced more than 20% of\ncontinuations with expressions that refer back to an\nNP that did not introduce an entity, which shows\nthat the results from Experiment 2 also apply to\nlikely generations by LMs.\n6 General Discussion\nIn his seminal work in 1976, Karttunen introduced\nseveral challenges for natural language understand-\ning systems that aim to track which entities are\nintroduced in a larger discourse. In this work, we\nevaluated to what extent we made progress on these\nchallenges in the past decades. In two sets of exper-\niments, we found that Transformer-based models\nare to some extent sensitive to different interactions\nbetween sentential operators and indefinite noun\nphrases. At the same time, however, we found\nin Experiment 2 that models lack systematicity in\ntheir behavior, which suggests that models do not\ncombine indefinite noun phrases and sentential op-\nerators as humans do. Further, the analysis of likely\ncontinuations showed that this behavior can also\nbe observed in high probability generations.\nLearnability of meaning On the one hand, these\nresults provide direct evidence for shortcomings of\nlanguage models with respect to tracking entities.\nOn the other hand, more broadly, these results also\nprovide interesting data points with regard to the\nrecent debate on whether language models could\ntheoretically mimic human language understanding.\n976\nBender and Koller (2020) recently presented sev-\neral thought experiments and argued that since LMs\nare only trained on form and do not have access\nto meaning or intentions, they can never exhibit\nhuman-like language understanding (see also Mer-\nrill et al., 2021, for a more formal discussion of this\nclaim). Given that we evaluated the largest avail-\nable GPT-3 model and still found that the model be-\nhavior is inconsistent despite its enormous amount\nof parameters and training data, our results suggest\nthat at least current language model architectures\nindeed struggle with human-like understanding. In-\nterestingly though, while the thought experiments\nby Bender and Koller (2020) focus on lack of world\nknowledge due to the lack of grounding of language\nmodels, our results suggest that additionally, lan-\nguage models fail at learning the meaning of more\nabstract words such as negation markers or embed-\nding verbs. This is also in line with recent results,\nwhich showed that smaller models fail to learn the\nmeaning of negation and discourse connectives.\n(Ettinger, 2020; Pandia et al., 2021). Lastly, the\nfact that GPT-2 and GPT-3 have been exposed to or-\nders of magnitude more language data than human\nlearners are and still do not fully succeed at track-\ning discourse entities underscores that there are\ndifferences between how humans and LMs learn.\nNLG evaluation We further believe that evalua-\ntion suites targeting discourse phenomena, such as\nthe ones presented here, can serve a complementary\nrole to natural language generation (NLG) bench-\nmarks (e.g., Gehrmann et al., 2021) and human\nstudies for evaluating NLG systems. This seems\nparticularly relevant considering that Clark et al.\n(2021) recently found that untrained crowdworkers,\nwho serve as participants in the majority of human\nevaluation studies, cannot distinguish between sto-\nries written by humans and stories generated by\nGPT-3. Our experiments, however, show that there\nis a considerable gap between humans and GPT-\n3 for basic discourse phenomena, and therefore\ntargeted evaluation suites should be an important\nmeasure for tracking progress of NLG models.\nComparison to probing results Recently, Li\net al. (2021) developed a probe for investigating\nwhether LM representations provide information\nabout the state of entities at various stages in a\nlarger discourse. This probing method–like the\nones presented in this work–also aims to assess en-\ntity tracking abilities of pre-trained language mod-\nels. They considered two sequence-to-sequence\nmodels, T5 and BERT, and found that representa-\ntions from both models can be decoded into entity\nstates with high accuracy. This task may seem\nmore complex than the one used in the experiments\nabove, and T5 and BERT are considerably smaller\nmodels than GPT-3, so prima facie, it may be sur-\nprising that their results suggest superior discourse\nabilities than our results. However, there are two\nimportant differences in methodology that likely\nexplain this discrepancy. First, the probing clas-\nsifier was trained on data that was similar to the\nevaluation data and this setup therefore provided a\nlot of supervision. Second, the datasets used by Li\net al. (2021) were obtained through crowdsourcing\nor a generation engine and were not constructed\nas systematically as ours. For these reasons, the\nprobing classifier may have learned spurious cor-\nrelations between the training and test splits, and\nthe high accuracy on the task may have only in part\nbeen driven by entity tracking abilities of LMs.\nPotential solutions Considering the still mod-\nest performance of GPT-3, it seems unlikely that\ntraining models on even more data is going to lead\nto human-like discourse entity processing by lan-\nguage models. Instead, we consider the following\nmodifications to models to likely lead to more sys-\ntematic entity tracking. First, there have been some\nsuccesses in augmenting language models with ex-\nplicit entity memory representations (e.g., Weston\net al., 2014; Sukhbaatar et al., 2015; Rashkin et al.,\n2020; Cheng and Erk, 2020), and likely such archi-\ntectural changes could also help in the contexts that\nwe evaluated in this work. Second, considering\nthat the models seem to struggle to learn the mean-\ning of sentential operators, it may be necessary to\nprovide additional supervision, for example using\ntreebanks annotated with meaning representations,\nsuch as the Groningen Meaning Bank (Bos et al.,\n2017). Relatedly, models may also benefit from\nmore grounded learning scenarios. Humans likely\ndifferentiate betweenArthur owns a dogand Arthur\ndoesn’t own a dog because only in the former case,\na dog refers to something in the real world and if a\nmodel was immersed in more grounded scenarios\nit would likely be able to infer this difference.\nWe hope that our evaluation suite will be a valu-\nable resource for assessing progress of future mod-\nels such as the ones sketched above, and that it will\nhelp pave the way for improved discourse entity\nprocessing in NLU systems.\n977\nEthics Statement\nRisks, limitations, and intended use We con-\nsider the main risk of this work that the evaluation\nsuite may be used to make overstating claims about\nmodel abilities in the future. In particular, should\nfuture models achieve very high or even perfect\naccuracy on the evaluation suite, then such results\nmay be seen as evidence of human-like abilities\nof discourse entity processing. We therefore want\nto emphasize that achieving high accuracy on this\nevaluation suite is a necessary but not necessar-\nily sufficient requirement for a model to exhibit\nhuman-like entity tracking abilities.\nFurther, it seems likely that models fine-tuned on\nsimilar examples would perform a lot better on this\nevaluation suite, and therefore researchers should\nonly use this dataset for out-of-domain evaluations\nin which the model has not been trained on similar\nexamples.\nFinally, we only evaluated models trained on\nEnglish data in this work and it is conceivable that\nentity tracking abilities of models trained on other\nlanguages differ from the results reported here.\nHuman subject experiments As we mentioned\nin Section 4.1, we recruited crowdworkers from\nProlific to validate the experimental stimuli. Par-\nticipants were based in the US and on average re-\nceived compensation of about $14/hour, which is\nalmost twice the minimum wage in most states in\nthe US. The experiment has been pre-approved by\nthe New York University IRB, and there were no\nrisks associated with participation.\nAcknowledgments\nWe thank the members of the NYU Computation\nand Psycholinguistics Lab and the NYU Semantics\nGroup, and the reviewers for their thoughtful feed-\nback. We also thank Alicia Chatten for doing the\nannotations of the model generations. This mate-\nrial is based upon work supported by the National\nScience Foundation under Grant #2030859 to the\nComputing Research Association for the CIFellows\nProject. Any opinions, findings, and conclusions\nor recommendations expressed in this material are\nthose of the authors and do not necessarily reflect\nthe views of the National Science Foundation nor\nthe Computing Research Association.\nReferences\nLaura Aina and Tal Linzen. 2021. The language model\nunderstood the prompt was ambiguous: Probing syn-\ntactic uncertainty through generation. In Proceedings\nof the Fourth BlackboxNLP Workshop on Analyzing\nand Interpreting Neural Networks for NLP, pages 42–\n57, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nEmily M. Bender and Alexander Koller. 2020. Climbing\ntowards NLU: On meaning, form, and understanding\nin the age of data. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5185–5198, Online. Association for\nComputational Linguistics.\nJohan Bos, Valerio Basile, Kilian Evang, Noortje Ven-\nhuizen, and Johannes Bjerva. 2017. The Gronin-\ngen Meaning Bank. In Nancy Ide and James Puste-\njovsky, editors, Handbook of Linguistic Annotation,\nvolume 2, pages 463–496. Springer.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. arXiv\nPreprint 2005.14165.\nPengxiang Cheng and Katrin Erk. 2020. Attending to\nentities for better text understanding. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\npages 7554–7561.\nShammur Absar Chowdhury and Roberto Zamparelli.\n2018. RNN simulations of grammaticality judgments\non long-distance dependencies. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 133–144, Santa Fe, New Mexico,\nUSA. Association for Computational Linguistics.\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021. All that’s ‘human’ is not gold: Evaluating\nhuman evaluation of generated text. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7282–7296, Online.\nAssociation for Computational Linguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286, Florence, Italy. Association for Com-\nputational Linguistics.\n978\nJillian Da Costa and Rui Chaves. 2020. Assessing the\nability of transformer-based neural models to repre-\nsent structurally unbounded dependencies. In Pro-\nceedings of the Society for Computation in Linguis-\ntics 2020, pages 12–21, New York, New York. Asso-\nciation for Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978–2988, Florence, Italy. Asso-\nciation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic subjects:\nRepresentations of syntactic state. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 32–42, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal, Pawan Sasanka Ammanamanchi,\nAnuoluwapo Aremu, Antoine Bosselut, Khy-\nathi Raghavi Chandu, Miruna-Adriana Clinciu,\nDipanjan Das, Kaustubh Dhole, Wanyu Du, Esin\nDurmus, Ond ˇrej Dušek, Chris Chinenye Emezue,\nVarun Gangal, Cristina Garbacea, Tatsunori\nHashimoto, Yufang Hou, Yacine Jernite, Harsh Jham-\ntani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv\nKumar, Faisal Ladhak, Aman Madaan, Mounica\nMaddela, Khyati Mahajan, Saad Mahamood, Bod-\nhisattwa Prasad Majumder, Pedro Henrique Martins,\nAngelina McMillan-Major, Simon Mille, Emiel van\nMiltenburg, Moin Nadeem, Shashi Narayan, Vitaly\nNikolaev, Andre Niyongabo Rubungo, Salomey\nOsei, Ankur Parikh, Laura Perez-Beltrachini,\nNiranjan Ramesh Rao, Vikas Raunak, Juan Diego\nRodriguez, Sashank Santhanam, João Sedoc,\nThibault Sellam, Samira Shaikh, Anastasia Shimo-\nrina, Marco Antonio Sobrevilla Cabezudo, Hendrik\nStrobelt, Nishant Subramani, Wei Xu, Diyi Yang,\nAkhila Yerukola, and Jiawei Zhou. 2021. The\nGEM benchmark: Natural language generation,\nits evaluation and metrics. In Proceedings of the\n1st Workshop on Natural Language Generation,\nEvaluation, and Metrics (GEM 2021), pages 96–120,\nOnline. Association for Computational Linguistics.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless green\nrecurrent networks dream hierarchically. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1195–1205, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nIrene Roswitha Heim. 1982. The semantics of defi-\nnite and indefinite noun phrases. University of Mas-\nsachusetts Amherst.\nJennifer Hu, Sherry Yong Chen, and Roger Levy. 2020a.\nA closer look at the performance of neural language\nmodels on reflexive anaphor licensing. In Proceed-\nings of the Society for Computation in Linguistics\n2020, pages 323–333, New York, New York. Associ-\nation for Computational Linguistics.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020b. A systematic assessment\nof syntactic generalization in neural language mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1725–1744, Online. Association for Computational\nLinguistics.\nLauri Karttunen. 1976. Discourse referents. In J. D. Mc-\nCawley, editor, Syntax and Semantics Vol. 7, pages\n363–386. Academic Press.\nNajoung Kim, Roma Patel, Adam Poliak, Patrick Xia,\nAlex Wang, Tom McCoy, Ian Tenney, Alexis Ross,\nTal Linzen, Benjamin Van Durme, Samuel R. Bow-\nman, and Ellie Pavlick. 2019. Probing what differ-\nent NLP tasks teach machines about function word\ncomprehension. In Proceedings of the Eighth Joint\nConference on Lexical and Computational Semantics\n(*SEM 2019) , pages 235–249, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nBelinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021.\nImplicit representations of meaning in neural lan-\nguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1813–1827, Online. Association for\nComputational Linguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521–535.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\n979\nWilliam Merrill, Yoav Goldberg, Roy Schwartz, and\nNoah A. Smith. 2021. Provable limitations of acquir-\ning meaning from ungrounded form: What will future\nlanguage models understand? Transactions of the\nAssociation for Computational Linguistics, 9:1047–\n1060.\nMichael Murez and François Recanati. 2016. Mental\nfiles: an introduction. Review of Philosophy and\nPsychology, 7(2):265–281.\nBenjamin Newman, Kai-Siang Ang, Julia Gong, and\nJohn Hewitt. 2021. Refining targeted syntactic evalu-\nation of language models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3710–3723, Online.\nAssociation for Computational Linguistics.\nLalchand Pandia, Yan Cong, and Allyson Ettinger. 2021.\nPragmatic competence of pre-trained language mod-\nels through the lens of discourse connectives. In Pro-\nceedings of the 25th Conference on Computational\nNatural Language Learning, pages 367–379, Online.\nAssociation for Computational Linguistics.\nLalchand Pandia and Allyson Ettinger. 2021. Sorting\nthrough the noise: Testing robustness of information\nprocessing in pre-trained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1583–\n1596, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernández. 2016. The LAMBADA dataset: Word\nprediction requiring a broad discourse context. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525–1534, Berlin, Germany.\nAssociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nHannah Rashkin, Asli Celikyilmaz, Yejin Choi, and\nJianfeng Gao. 2020. PlotMachines: Outline-\nconditioned generation with dynamic plot state track-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP), pages 4274–4295, Online. Association\nfor Computational Linguistics.\nIonut-Teodor Sorodoc, Kristina Gulordava, and Gemma\nBoleda. 2020. Probing for referential information in\nlanguage models. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4177–4189, Online. Association for\nComputational Linguistics.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and\nRob Fergus. 2015. End-to-end memory networks.\narXiv Preprint 1503.08895.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nShiva Upadhye, Leon Bergen, and Andrew Kehler. 2020.\nPredicting reference: What do language models learn\nabout discourse models? In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 977–982, Online.\nAssociation for Computational Linguistics.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics , 8:377–\n392.\nBonnie Lynn Webber, editor. 1979. A Formal Approach\nto Discourse Anaphora. Routledge.\nJason Weston, Sumit Chopra, and Antoine Bordes. 2014.\nMemory networks. arXiv Preprint 1410.3916.\nEthan Wilcox, Roger Levy, Takashi Morita, and Richard\nFutrell. 2018. What do RNN language models learn\nabout filler–gap dependencies? In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n211–221, Brussels, Belgium. Association for Com-\nputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nA Human experiment details\nParticipants completed two practice trials to get\nfamiliarized with the task, followed by four critical\ntrials with two filler trials randomly interspersed.\nFigure 4 shows an example trial. Participation was\nlimited to people living in the US whose native\nlanguage is English.\nB Model experiment details\nFor the experiments with GPT-2, we used the LM-\nScorer library8 and ran the experiments on a node\n8https://github.com/simonepri/\nlm-scorer/\n980\nwith a 3.7Ghz CPU and 32GB of RAM. In total,\nall evaluations required approximately 8h of CPU\ntime. For the experiments with GPT-3, we used\nthe offical OpenAI API.9 For all experiments, we\ncompared raw, untransformed probabilities, i.e.,\nthe temperature parameter was set to 0.\n9https://beta.openai.com\n981\nFigure 4: Example trial of human experiment.\n982",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7653697729110718
    },
    {
      "name": "Negation",
      "score": 0.763151228427887
    },
    {
      "name": "Suite",
      "score": 0.6948096752166748
    },
    {
      "name": "Sentence",
      "score": 0.600614607334137
    },
    {
      "name": "Transformer",
      "score": 0.5993767976760864
    },
    {
      "name": "Natural language processing",
      "score": 0.5868636965751648
    },
    {
      "name": "Noun",
      "score": 0.5487242937088013
    },
    {
      "name": "Noun phrase",
      "score": 0.47629109025001526
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4675150513648987
    },
    {
      "name": "Narrative",
      "score": 0.452758252620697
    },
    {
      "name": "Linguistics",
      "score": 0.4432344436645508
    },
    {
      "name": "Programming language",
      "score": 0.11483106017112732
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ]
}