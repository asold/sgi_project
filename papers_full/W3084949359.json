{
    "title": "Identification of Cybersecurity Specific Content Using Different Language Models",
    "url": "https://openalex.org/W3084949359",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5053967263",
            "name": "Otgonpurev Mendsaikhan",
            "affiliations": [
                "Nagoya University"
            ]
        },
        {
            "id": "https://openalex.org/A5021521170",
            "name": "Hirokazu Hasegawa",
            "affiliations": [
                "Nagoya University"
            ]
        },
        {
            "id": "https://openalex.org/A5086392139",
            "name": "Yukiko Yamaguchi",
            "affiliations": [
                "Nagoya University"
            ]
        },
        {
            "id": "https://openalex.org/A5102265280",
            "name": "Hajime Shimada",
            "affiliations": [
                "Nagoya University"
            ]
        },
        {
            "id": "https://openalex.org/A5011571558",
            "name": "Enkhbold Bataa",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2958860733",
        "https://openalex.org/W1126420929",
        "https://openalex.org/W2128180557",
        "https://openalex.org/W1976339648",
        "https://openalex.org/W2072729271",
        "https://openalex.org/W2807143630",
        "https://openalex.org/W2963801581",
        "https://openalex.org/W2945377946",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W2963514026"
    ],
    "abstract": "Given the sheer amount of digital texts publicly available on the Internet, it becomes more challenging for security analysts to identify cyber threat related content. In this research, we proposed to build an autonomous system to identify cyber threat information from publicly available information sources. We examined different language models to utilize as a cybersecurity-specific filter for the proposed system. Using the domain-specific training data, we trained Doc2Vec and BERT models and compared their performance. According to our evaluation, the BERT-based Natural Language Filter is able to identify and classify cybersecurity-specific natural language text with 90% accuracy.",
    "full_text": "Journal of Information Processing Vol.28 623–632 (Sep. 2020)\n[DOI: 10.2197/ipsjjip.28.623]\nRegular Paper\nIdentiﬁcation of Cybersecurity Speciﬁc Content Using\nDiﬀerent Language Models\nOtgonpurev Mendsaikhan1,a) Hirokazu Hasegawa2 Yukiko Yamaguchi3 Hajime Shimada3\nEnkhbold Bataa4\nReceived: October 31, 2019, Accepted: June 1, 2020\nAbstract: Given the sheer amount of digital texts publicly available on the Internet, it becomes more challenging for\nsecurity analysts to identify cyber threat related content. In this research, we proposed to build an autonomous sys-\ntem to identify cyber threat information from publicly available information sources. We examined diﬀerent language\nmodels to utilize as a cybersecurity-speciﬁc ﬁlter for the proposed system. Using the domain-speciﬁc training data, we\ntrained Doc2Vec and BERT models and compared their performance. According to our evaluation, the BERT-based\nNatural Language Filter is able to identify and classify cybersecurity-speciﬁc natural language text with 90% accuracy.\nKeywords: cyber threat, NLP, Text-Classiﬁcation\n1. Introduction\nIn the age of digital information, extracting the relevant content\nfrom the massive ﬂow of data becomes a challenge. Recently,\nwith the rise of social networks as well as ubiquitous comput-\ning, the total amount of digital text content is increasing. One\nof the tasks of a security analyst is to establish the situational\nawareness, thus identifying cyber threat related information to\nproactively monitor, prevent the possible intrusion and control\nthe possible risk. However, due to the increase in the number of\ninformation systems used in security operations centers, extrac-\ntion of the organization-speciﬁc cyber threat related information\nfrom the public Internet becomes necessary for correlation. For\nthis reason, we are proposing an autonomous system that em-\nploys Natural Language Processing techniques to identify the cy-\nber threat related information and ﬁlter out the irrelevant content.\nIn our earlier work, we proposed ﬁltering security related text\ndocuments using the keyword generation method [1]. However,\nafter several experiments, we concluded that the words should be\ntreated as non-atomic entities in order to preserve their seman-\ntic relationship. Fortunately, the ﬁeld of Natural Language Pro-\ncessing (NLP) has been advancing recently and there are tech-\nniques to train machines to understand the semantic relationships\nbetween words. Since computers can work only with numbers,\ncomputational linguistics represent text in vector space to capture\nand calculate the semantic relationships between words [2]. The\nprocess of converting text into numerical vectors is called embed-\n1 Nagoya University, Graduate School of Informatics, Nagoya, Aichi 464–\n8603, Japan\n2 Nagoya University, Information Security Oﬃce, Nagoya, Aichi 464–\n8601, Japan\n3 Nagoya University, Information Technology Center, Nagoya, Aichi 464–\n8601, Japan\n4 ExaWizards Inc., Minato, Tokyo 105–0013, Japan\na) ogo@net.itc.nagoya-u.ac.jp\nding. Earlier embedding techniques such as Bag of Words (BoW)\nor Term Frequency Inverse Document Frequency (TFIDF) have\nbeen preceded by neural network embedding techniques to rep-\nresent the words in vector space. Mikolov et al. [3] proposed a\ngroundbreaking method called Word2Vec to represent and com-\npute the semantic similarities of the words in vector space. Given\nthe success of the Word2Vec model, the authors proposed an-\nother method called Doc2Vec [4] that could represent the docu-\nments in vector space using a similar approach. In our previ-\nous work [5], we proposed to utilize the Doc2Vec-based language\nmodel to identify the cybersecurity related text documents from\npublicly available information sources. Since the publication of\nthe paper, LSTM-based ELMo [6], ULMFiT [7] and transformer-\nbased models such as BERT [8] have been shifting the paradigm\nto transfer learning methods. These models have enabled eﬃcient\ncontextual representations of the sentence that overcame the lim-\nitations of Doc2Vec representation.\nIn this paper we propose to utilize BERT, which achieved\nstate-of-the-art performance on 11 NLP tasks, to classify\ncybersecurity-speciﬁc text documents and compare them with our\nprevious study that used Doc2Vec.\nThe main objective of the paper is to assess and evaluate the\nsuitability of diﬀerent language models to act as a Natural Lan-\nguage Filter in our proposed system.\nThe speciﬁc contributions of the paper are as follows:\n( 1 ) We propose an architecture of the autonomous system\nto identify cyber threat related content from the massive\namount of publicly available textual documents.\n( 2 ) With the cybersecurity-speciﬁc training data and custom pre-\nprocessing, the Doc2Vec model has been trained to work as a\ndomain-speciﬁc language ﬁlter for the autonomous system.\n( 3 ) With the same data, experiments have been conducted to test\nif the BERT language model can act as a domain-speciﬁc\nlanguage ﬁlter.\nc⃝2020 Information Processing Society of Japan 623\nJournal of Information Processing Vol.28 623–632 (Sep. 2020)\nThe remainder of this paper is organized as follows. Section 2\nwill review similar works and how this paper diﬀers in its ap-\nproach. In Section 3, we will propose an autonomous system\nto identify cyber threat related information. In Section 4, the\ndatasets to be used as training and the test for the experiments\nwill be discussed. In Sections 5 and 6, the experimental setup and\nthe results of experiments conducted using Doc2Vec and BERT-\nbased Natural Language Filters respectively will be detailed. Sec-\ntion 7 will discuss the diﬀerences between both models, and, ﬁ-\nnally, we will conclude in Section 8 by discussing the future pos-\nsibilities to extend this research.\n2. Related Work\nThere have been a number of attempts to identify or extract\ncyber threat related information from the Dark Web or any other\npublicly available information source automatically. Al-Rowaily\net al. [9] developed a Bilingual Sentiment Analysis Lexicon for\nthe cybersecurity domain that can be used to develop opinion\nmining and sentiment analysis systems for bilingual textual data\nfrom Dark Web forums. Our approach aims to provide automated\nidentiﬁcation from a text corpus, whether it is a Dark Web forum\nor any other publicly available information source, using the se-\nmantic representation of the text document. Mulwad et al. [10]\ndescribed a prototype system to extract information about secu-\nrity vulnerability from web text using an SVM classiﬁer. We pro-\npose a deep learning algorithm using neural embedding for classi-\nﬁcation to achieve better performance. Joshi et al. [11] proposed\na cybersecurity entity and concept spotter that uses the Stanford\nNamed Entity Recognizer (NER), a Conditional Random Field\n(CRF) algorithm-based NER framework. That research focused\non building the cybersecurity-speciﬁc vocabulary through linked\ndata, whereas our approach aims to identify the cybersecurity-\nspeciﬁc documents using diﬀerent language models. More et\nal. [12] proposed a knowledge-based approach to intrusion de-\ntection modeling in which the intrusion detection system auto-\nmatically fetches threat related information from web-based text\ninformation and proactively monitors the network to establish sit-\nuational awareness. Their approach focused mainly on devel-\noping a cybersecurity ontology which can be understood by the\nintrusion-detecting machines. Our research focused on building\nan autonomous system that assists the human operators in terms\nof raising situational awareness. Dion´ısio et al. developed a sys-\ntem to detect cyber threats from Twitter using deep neural net-\nworks [13]. Their work has many similarities with our work, in\nthat they collected a relevant threat from a Twitter feed and iden-\ntiﬁed the assets through Named Entity Recognition from speciﬁc\nTwitter feeds. Our approach diﬀers by identifying and extracting\ncybersecurity related documents from any source and then ana-\nlyzing the relevance of it. Tavabi et al. developed DarkEmbed, a\nsystem to predict the probability of the speciﬁc vulnerability get-\nting exploited using neural language model. They have utilized\nSkip-Gram Word2Vec model to generate low dimensional doc-\nument embedding and classiﬁed them using Support Vector Ma-\nchine [14]. Their work highlighted the importance of domain spe-\nciﬁc embedding, which is demonstrated in our research as well.\nThere have been various approaches to implement domain-\nspeciﬁc language models based on Word2Vec or Doc2Vec frame-\nworks and their variations. Niu et al. [15] developed the\nTopic2Vec approach, which can learn topic representations in the\nsame semantic vector space as used for words. Dhingra et al. [16]\ndescribed character-based distributed representations for social\nmedia by introducing their Tweet2Vec language model. The\ncharacter-level representations show interesting results for learn-\ning informal conversations. Choi et al. [17] proposed a multi-\nlayer representation learning for medical concepts. Med2Vec re-\nsearch has shown that a domain-speciﬁc language model could\nachieve better results than the generic language model.\nThere have been attempts to utilize a Doc2Vec language model\nfor the domain-speciﬁc task as well. Aman et al. [18] described\na system utilizing a Doc2Vec-based language model to assess\ncomments and its application to change prone method analy-\nsis. The research result shows that Doc2Vec could be utilized\nfor domain-speciﬁc tasks like comparing source code comments\nto the programming statements. Karvelis et al. [19] proposed a\ntopic recommendation system using a Doc2Vec-based language\nmodel. The automated topic recommendation system based on\nthe Doc2Vec model suggests that our proposed autonomous sys-\ntem that classiﬁes cybersecurity related text data could be realized\nin practice.\nSince the inception of the BERT language model, there have\nbeen various attempts to train and utilize it for speciﬁc do-\nmains. Lee et al. described BioBERT, which was trained for\nunannotated biomedical text corpora [20]. BioBERT outper-\nformed the generic BERT model on the biomedical text mining\ntasks. Similar results have been achieved by Beltagy et al. with\nSciBERT, a pre-trained language model for scientiﬁc text [21].\nSciBERT has been pre-trained on 1.14M papers of various do-\nmains from Semantic Scholar and even outperformed BioBERT\non biomedical tasks. Lee et al. described a patent classiﬁcation\nsystem created by ﬁne-tuning the BERT language model [22].\nTheir model outperformed the state-of-the-art results in classi-\nfying the patent claims. Huang et al. pre-trained and ﬁne-tuned\nBERT on clinical notes to develop ClinicalBERT [23]. Clini-\ncalBERT has outperformed baselines to predict the readmission\nof patients based on clinical notes. Sun et al. proposed a joint\nvisual-linguistic model to learn high-level features by building\na model based upon BERT to learn bidirectional joint distri-\nbutions over sequences of visual and linguistic tokens, derived\nfrom vector quantization of video data and oﬀ-the-shelf speech\nrecognition outputs, respectively [24]. The proposed VideoBERT\nmodel is used in various tasks including action classiﬁcation and\nvideo captioning. Adhikari et al. presented DocBERT for doc-\nument classiﬁcation [25]. DocBERT achieved state-of-the-art re-\nsults across four popular datasets when applied to straightforward\ndocument classiﬁcation task. Although BERT has achieved out-\nstanding results in various domains, its potential has yet to be\nfully explored in the cybersecurity domain.\n3. Proposed System\nIn our earlier work [1] we proposed a system to mine actionable\nthreat intelligence from publicly available information sources\nwith minimal supervision. This section will review the archi-\nc⃝2020 Information Processing Society of Japan 624\nJournal of Information Processing Vol.28 623–632 (Sep. 2020)\nFig. 1 Proposed system architecture.\ntecture of the system and our proposed solutions for its Natural\nLanguage Filter module.\n3.1 Proposed Model of the Autonomous System\nWe envisioned a system that automatically identiﬁes and an-\nalyzes cybersecurity threats to assist security analysts in build-\ning situational awareness with three modules including a Natu-\nral Language Filter, Analyzer and Training Document Generator.\nThe proposed system architecture is depicted inFig. 1.\nThe Natural Language Filter module would identify and ﬁlter\nthe security related text documents from publicly available in-\nformation sources. The organization-speciﬁc textual data is col-\nlected as initial training data and fed to the Training Document\nGenerator module to prepare the training documents. The Natu-\nral Language Filter module is trained by these documents and ﬁl-\nters the security related contents. The collected and ﬁltered doc-\numents are analyzed by the Analyzer module for second-stage\nﬁltering for products, versions speciﬁc to the organization that\ngenerate meaningful analyzed threat information to the human\noperators. The documents that have been marked as true positive\nby the Analyzer are fed back to the Training Document Generator\nto improve the performance of the Natural Language Filter, and\nthus the overall system.\n3.2 Doc2Vec Based Natural Language Filter\nMikolov et al. [4] proposed a paragraph vector, an unsuper-\nvised framework that learns continuous distributed vector repre-\nsentations for pieces of texts by extending their previous work\non Word2Vec into learning word embeddings. One of the ad-\nvantages of the Doc2Vec framework is that it can work on a\nv a r i e t yo fd iﬀerent-length texts without losing their order or se-\nmantics when representing the documents in continuous vector\nspace. Previous neural network approaches for word embedding\nconsisted of concatenating the several preceding word vectors\nto form the input of a neural network and tries to predict the\nnext word in sequence. The outcome is that after the model is\ntrained, the word vectors are mapped into a vector space so that\nsemantically similar words have similar vector representations,\nthus located near to each other [4]. As a result, by performing\nsimple algebraic operations on word vectors, we can easily esti-\nmate the word similarity. For example, to ﬁnd the word that is\nsimilar to small in the same sense asbiggest is similar to big,\nwe can compute vectorX =vector(“biggest”) −vector(“big”) +\nvector(“small”) and search in the vector space for the word clos-\nest to X measured by its cosine distance [3].\nIn case of Doc2Vec, the model creates another vector to repre-\nsent the document itself and combine it with the individual word\nvectors of the document. Depending upon the working mode,\nwhether it is a distributed memory model of paragraph vector\n(PV-DM) or a distributed bag-of-words paragraph vector (PV-\nDBOW), the word ordering is preserved. The PV-DBOW mode\nignores the context words in the input, but forces the model to\npredict words that are randomly sampled from the paragraph in\nthe output, whereas the PV-DM mode concatenates the paragraph\nvector and the corresponding word vectors to predict the next\nword in the sequence.\nThe authors of the Word2Vec model have introduced another\ntwo important concepts called subsampling and negative sam-\npling in their subsequent paper [14] in order to improve the per-\nformance of the model. Subsampling is the concept in which if\nthe given word under consideration is too frequent in the cor-\npus, its probability of getting represented in the vector space\ndecreases. This helps to remove the high frequency words that\nhave minimal inﬂuence on the overall performance of the model,\nthus reducing the model size and improving the accuracy of the\nlearned representations. Negative sampling is the alternative to\nthe output function “hierarchical softmax” in which every train-\ning will not adjust all of the neuron weights of the neural net-\nwork; instead only a small percentage of the weights will be mod-\niﬁed, hence improving the computing performance. Both con-\ncepts have been included in the Doc2Vec model and proven to be\neﬃcient extensions to it according to Mikolov et al. [26].\nThus, the paragraph vectors generated by Doc2Vec represent\nthe document in vector space and inherit the important properties\nof the word vectors to identify the similarity between the docu-\nments. According to the paper by Lau et al. [27], Doc2Vec per-\nforms better than the alternative document embedding methods\nfor in-domain model training.\nBased on the above, Doc2Vec has been tested for its suitability\nas a Natural Language Filter in our proposed autonomous system\nby embedding a document and ﬁnding semantic similarity with a\ncybersecurity-speciﬁc text. In Section 5, we will discuss the ex-\nperiment conducted with the Doc2Vec-based Natural Language\nFilter. For the purpose of this research, the Gensim\n*1 implemen-\ntation of the Doc2Vec model has been utilized.\n3.3 BERT-Based Natural Language Filter\nThe latest trend in the Natural Language Processing ﬁeld in-\ncludes utilization of Transformer-based deep neural architecture\nand Transfer learning. Bidirectional Encoder Representations\nfrom Transformers (BERT) is a new language representation\nmodel that utilizes these approaches to obtain state-of-the-art re-\nsults on eleven Natural Language Processing tasks. BERT is de-\nsigned to learn deep bidirectional representations from unlabeled\ntext by jointly conditioning both the left and right contexts in con-\ntrast to the previous attempts of predicting a token in a unidirec-\ntional (left-to-right, right-to-left) way [8]. BERT achieves bidi-\nrectionality by using a pre-training objective called a “masked\nlanguage model” (MLM). Before feeding the text sequence to a\nmodel, BERT replaces 15% of the words in each training example\n*1 https://radimrehurek.com/gensim/\nc⃝2020 Information Processing Society of Japan 625\nJournal of Information Processing Vol.28 623–632 (Sep. 2020)\nwith a [MASK] token. Then, the task of the model is to predict\nthe original token based on the non-masked tokens. In addition\nto the MLM task, BERT also employs “next sentence prediction”\n(NSP) task where the model takes a pair of sentences and then\ntries to predict whether the second sentence is subsequent to the\nﬁrst. During the training, the model is fed 50% of the inputs that\nare subsequent sentences while the other 50% of sentences are\nordered randomly.\nIn order to successfully train with MLM and NSP tasks, BERT\npreprocesses the input text according to following steps.\n( 1 ) A special [CLS] token is placed at the beginning of the ﬁrst\nsentence and an [SEP] token is placed right before the sec-\nond.\n( 2 ) Token embeddings, where dense embeddings for each token\nincluding [CLS] and [SEP] will be learned.\n( 3 ) Sentence embeddings indicate which tokens belong to which\nsentence. This is similar to token embeddings; however, vo-\ncabulary size here is limited to two only.\n( 4 ) Positional embeddings are borrowed from the original trans-\nformer paper [27]. Since the transformer is not recurrent, it\nneeds to learn the sequential information with the help of\npositional embeddings.\nBERT has been pre-trained on BookCorpus (800 M words) and\nEnglish Wikipedia (2,500 M words) with the goal of minimizing\nthe combined loss of MLM and NSP tasks. Fine-tuning BERT on\na classiﬁcation task is relatively straightforward, involving sim-\nply adding a linear layer on top of the transformer output for the\nﬁrst [CLS] token. Applying BERT to downstream tasks involves\nﬁne-tuning on the task speciﬁc data. For the purpose of this pa-\nper, the BERT model has been trained to classify security related\ntext documents from the non-security related data.\n4. Dataset\nTo test the suitability of Doc2Vec and BERT for the Natural\nLanguage Filter module, a signiﬁcant amount of data has been\ncollected. The details of the data collected for the test and train-\ning purposes are explained in this section.\n4.1 Data Collection\nFor the model to be trained to understand speciﬁc domain con-\ntexts, the training data need to include the domain-speciﬁc terms\nand dialogues that occur in casual conversations as well as oﬃ-\ncial statements, respectively. In the real-world scenario, cyber\nthreat information may exist in both the oﬃcial format, as in\nnews/bulletins, or the casual format, as in conversations on fo-\nrums, emails and social networks. Since the model needs to be\ntrained to identify the cybersecurity related text whether it is in-\nformal conversation or a formal news statement, we wanted to\ninclude as diverse range of data sources as possible to minimize\nthe bias. The cybersecurity-speciﬁc data sources are categorized\nas Formal and Informal based on the nature of the conversations.\nThe data sources and respective number of documents col-\nlected are shown inTable 1.\nDetailed explanations of each data source are as follows:\nTable 1 Data sources and number of documents.\nData class Data source No. of documents\nInformal Reddit discussions 114,391\nInformal StackExchange discussions 841,311\nInformal Hackernews comments 139,946\nFormal Security news outlet RSS feeds 2,077\nFormal Slashdot news archive 7,751\nFormal National Vulnerability Database 99,382\nTotal 1,204,858\n□ Reddit discussions: Reddit *2 is a popular social news ag-\ngregation and discussion website. Thirty-two security re-\nlated subreddit (Reddit communities) discussions have been\nmanually picked, and each of the topics and corresponding\ncomments have been downloaded as an individual document\nfor the time period since the subreddit was created to De-\ncember 2nd 2018. Even though we cannot guarantee the\ncollected subreddits are exhaustive of security related com-\nmunities at Reddit, we believe that it could represent casual\nand informal conversations around cybersecurity topics.\n□ StackExchange discussionsStackExchange\n*3 is a network\nof question and answer websites on various topics. The\nwhole discussions of Security*4, Cryptography*5 and Re-\nverse Engineering *6 communities during the time period\nsince the site was created to December 2nd 2018 have been\ncollected to represent the Informal type of data.\n□ Hackernews comments Hackernews *7 is a social news\nwebsite run by Y Combinator. For this research, we have\nmanually picked comments on the security related news\nsince the creation of the site to January 1st 2017 as Infor-\nmal data.\n□ Security news outlet RSS feeds RSS feed summary for\nselect cybersecurity news outlets during the period of\nNovember 1st 2018 to December 2nd 2018. News out-\nlets include DarkReading\n*8, NakedSecurity *9, Security-\nMagazine *10 and ThreatPost*11. The text data that has been\ncollected from these news outlets represent the Formal type\nof data.\n□ Slashdot newsSlashdot *12 is a social news website that fea-\ntures stories on science, technology and politics. Manually\npicked security related news from the archive of the Slashdot\nwebsite during the period of January 1st 2015 to September\n1st 2018 represents the Formal type of data.\n□ National Vulnerability Database Common Vulnerability\nand Exposures (CVE) descriptions listed on the National\nVulnerability Database (NVD). The NVD is the U.S. gov-\nernment repository of standards-based vulnerability man-\nagement data and is known as the central database of all soft-\n*2 https://www.reddit.com/\n*3 https://stackexchange.com/\n*4 http://security.stackexchange.com/\n*5 http://crypto.stackexchange.com/\n*6 http://reverseengineering.stackexchange.com/\n*7 https://news.ycombinator.com/\n*8 https://www.darkreading.com/\n*9 https://nakedsecurity.sophos.com/\n*10 https://www.securitymagazine.com/\n*11 https://threatpost.com/\n*12 https://slashdot.org/\nc⃝2020 Information Processing Society of Japan 626\nJournal of Information Processing Vol.28 623–632 (Sep. 2020)\nware security vulnerabilities*13. The CVE descriptions that\nhas been accumulated in the NVD during the period from\nOctober 1st 1988 to August 1st 2018 has been utilized as the\nFormal type of data.\n4.2 Test Data\nUpon completion of the data collection, the collected data have\nbeen split into test and training datasets. 10% of the total col-\nlected data of 1,204,858 documents have been randomly selected\nand labeled as security related test data.\nReddit categorizes the most popular discussions on its platform\nat any given instant as Popular subreddit. The discussions on Pop-\nular subreddit include varied content including politics, lifestyle,\npop culture and everyday news, which would be suitable to con-\nsider as non-security related data. Hence, all of the discussions\nand corresponding comments of the popular subreddit have been\ndownloaded as of December 6th 2018 as a separate dataset. A\ntotal of 294,786 documents have been collected after the prepro-\ncessing and marked as non-security related test data.\nThe security related (120,486 documents) and non-security re-\nlated (294,786 documents) datasets have been mixed and a total\nof 415,272 documents are prepared for the test purpose. Once\nthe test data has been moved, the total amount of training data\nreduces to 1,084,372 documents.\n5. Experiment Using Doc2Vec Based Natural\nLanguage Filter\nIn our previous work [5] the Doc2Vec-based Natural Language\nFilter module was introduced. This section reviews the exper-\niment and the results of training the Doc2Vec-based language\nmodel.\n5.1 Preprocessing\nIn order to improve the eﬃciency of the Doc2Vec model, the\ncollected data have been preprocessed through the following steps\nusing the standard nltk*14 library of Python.\n□ Tokenization: Since the dataset contains a lot of program-\nming and conﬁguration samples as well as cybersecurity jar-\ngon, standard tokenization on the default word boundary\nhas been ineﬃcient. Hence, a custom tokenizer is created\nusing the regular expression that tokenizes as per the non-\nalphanumeric character. Since some of the training data con-\ntains html tags, they have been removed using the regular\nexpressions at this stage.\n□ Normalization: The aim of the model is to identify the\ncybersecurity-speciﬁc text; hence, the normalizing test and\ntraining data into lowercase would help us to identify the\nsame words in diﬀerent contexts with diﬀerent case settings.\n□ Token ﬁltering: Initially, the common English stop words\nhave been removed. During the initial analysis of the col-\nlected data, many instances of machine-generated random\nstrings such as hash values or API keys have been found.\nHence, identiﬁcation and removal of those strings based\n*13 http://nvd.nist.org\n*14 http://www.nltk.org/\nTable 2 Baseline hyperparameters of Doc2Vec model.\nNo. Parameter name Setting\n1 Vector size 300\n2 Epochs 400\n3 Mode DBOW\n4 Minimum count 1\n5 Window 15\n6 Subsampling 10−5\n7 Negative sampling 5\nupon the characteristics of the hashing algorithm is required.\nBased on the initial analysis, we decided the tokens of less\nthan two characters and more than 40 characters would not\ncontribute positively to the training of the model; hence, we\nﬁltered them out. Also, web URLs, email addresses and nu-\nmerical digits do not contribute to the semantic representa-\ntion of the language ﬁlter and have been removed.\n□ Document ﬁltering: The manual analysis on the dataset re-\nvealed that, once the token ﬁlter has been applied, many doc-\numents were left with less than seven tokens. Since the num-\nber of tokens is less than the sliding window of the model,\nthese documents would not positively aﬀect the model repre-\nsentation. Hence, the documents with less than seven tokens\nhave been removed from the dataset. To avoid duplicates,\nthe md5 hash values of the documents have been computed\nand compared with the rest of the corpus at the end of the\nprocess.\nThe above preprocessing has been applied to both the training\nand test datasets.\n5.2 Model Training\nThe Doc2Vec model has been trained with a dataset of\n1,084,372 documents. The Gensim implementation of the\nDoc2Vec model has various hyperparameters to set during the\ninitial training of the model. These parameters aﬀect the model\nperformance in various ways. Lau et al. [27] empirically studied\nthese parameters and concluded that for the semantic textual sim-\nilarity task, the best performing parameter settings are as listed in\nTable 2.\nThe hyperparameter settings described in Table 2 have been\nconsidered as the baseline settings and the initial model has been\ntrained accordingly as the Baseline Model. In order to identify the\nbest performing settings for the Doc2Vec model, each of the hy-\nperparameter settings has been tuned from the Baseline settings\nand evaluated respectively.\nThe explanation of the hyperparameters and the respective\nchanges to the Baseline settings are as follows.\n□ Vector size or SizeThe number of dimensions of the word\nvector represented. Each dimension represents the speciﬁc\nword in a diﬀerent context, and semantically similar words\nhave similar vectors in those spaces. The typical dimensions\nof the Doc2Vec models are set as 100–300 to eﬃciently rep-\nresent the semantics of the word. Since the Baseline setting\nis speciﬁed as 300, a model has been trained and evaluated\nwith the reduced value to see how it aﬀects the model per-\nformance.\n□ Epochs Also called the training iterations. Training itera-\nc⃝2020 Information Processing Society of Japan 627\nJournal of Information Processing Vol.28 623–632 (Sep. 2020)\nTable 3 Classiﬁcation result of diﬀerent Doc2Vec models.\nHyperparameter Setting Precision Recall F1 Score Accuracy\nBaseline As Table 2 0.8543 0.4331 0.5748 0.8141\nVector size 300 0.8543 0.4331 0.5748 0.8141\n200 0.7950 0.5287 0.6351 0.8237\n100 0.8745 0.4503 0.5945 0.8217\nTraining epoch 400 0.8543 0.4331 0.5748 0.8141\n300 0.8705 0.4392 0.5838 0.8183\n200 0.8834 0.4517 0.5978 0.8236\n100 0.8711 0.4759 0.6156 0.8275\nMode DM 0.8722 0.3499 0.4995 0.7965\nMin count 10 0.9609 0.3964 0.5613 0.8202\nWindow 10 0.9271 0.4191 0.5772 0.8219\nSubsampling 10−3 0.9814 0.3809 0.5488 0.8182\ntions are important to determine the ﬁt of the model. Since\nBaseline setting for training iteration is 400, a model has\nbeen trained with the reduced epochs to observe the ﬁt of the\nmodel.\n□ Mode The Doc2Vec model has two modes to work: DBOW\nstands for distributed bag-of-words mode and DM stands\nfor distributed memory mode. Since the Baseline setting is\nDBOW, the DM mode is experimented by training a model\nto determine the performance diﬀerence.\n□ Minimum count The minimum frequency threshold of a\nword in the whole corpus. A model is trained to observe the\nperformance with a higher minimum count threshold than\nthe Baseline.\n□ Window The sliding window size through which the word\nvector is selected with the neighboring words. Window=15\nmeans that seven words on each side of the selected word\nwill be considered for the analysis. A model is trained with\na window size smaller than the Baseline.\n□ Subsampling Threshold to downsample high frequency\nwords. The Baseline value of the subsampling setting is 10\n−5\nand the default value for the Gensim implementation of the\nmodel is 10−3. The smaller the value is, the less likely it is\nthat frequent words are kept in the model. Hence, a model\nhas been created with an increased subsampling setting.\n□ Negative sampleThe number of negative word samples are\nthe words to be aﬀected by every training. Since the Base-\nline setting of the negative sample is the same as a default\nDoc2Vec setting, we did not tune this setting for the experi-\nment.\nNote that the initial and minimum learning rates of the\nDoc2Vec have not been tuned in this experiment and the default\nvalues have been used.\n5.3 Model Evaluation\nIn order to evaluate the eﬀectiveness of the model, a similar-\nity test has been performed on the test dataset mentioned in Sec-\ntion 4.2. Each of the trained models have been used to test if the\ngiven test document is similar to any of the training documents\nof the model. For each test document the vector representation\nis computed and compared with the stored 1,084,372 document\nvectors that the model has learned during the training phase. If\nthe cosine distance of the test document representation in vec-\ntor space is more than 0.7 to any training document vector of the\nmodel, the test document is considered as a positive result; if not,\nit is ﬁltered out from the result set as negative. Using the pos-\nitive and negative results, the widely used metrics of Precision\nand Recall have been computed to better visualize the model’s\nperformance. Generally, higher precision means more relevant\nresults are found and higher recall means fewer positive results\nhave been missed. Hence, their harmonic mean F1 Score is com-\nputed for the consolidated representation and compared with the\ntraditional classiﬁcation metric of Accuracy.\nThe classiﬁcation results using diﬀerent models are shown in\nTable 3. From the performance comparison, the following obser-\nvations could be made:\n• Only the Vector size and Training epoch hyperparameters\nhave signiﬁcantly changed the performance of the Baseline\nmodel, both in terms of F1 Score and Accuracy.\n• The DBOW mode performs better than the DM mode. The\nmodel trained with the DM mode has the poorest perfor-\nmance in terms of Accuracy as well as F1 Score.\n• The increased minimum word count setting results in better\nAccuracy but a poorer F1 Score from the Baseline.\n• The change in Window size setting seems to aﬀect the docu-\nment similarity only slightly, increasing Accuracy but reduc-\ning the F1 Score.\n• Similarly, the reduction in the subsampling rate has very\nsmall eﬀect of reducing the F1 Score but increasing the Ac-\ncuracy of the Baseline model.\nSince the Vector size and Training epoch are the only hyper-\nparameters that aﬀect the performance results positively in terms\nof both F1 Score and Accuracy, the settings have been further\ntuned to analyze the performance diﬀerence. From Table 3, it can\nbe seen the performance of the model improves when the Vector\nsize parameter is reduced from 300 to 200 but declines when it is\nfurther lowered to 100. Hence Size=200 is picked as the optimal\nVector size parameter. Also, since the reduction in training iter-\nations results in better performance, Epoch=100 is picked as the\noptimal Epoch size. A new model is created with Size=200 and\nEpoch=100 settings in addition to the Baseline settings. It has\nproven to be the best performing model with an F1 Score of 0.63\nand Accuracy of 0.83.\nIn order to identify the best working mode for the Natural\nLanguage Filter, additional experiments with diﬀerent similarity\nthresholds have been performed. For the purpose of this research,\na cosine distance of 0.7 has been arbitrarily chosen as a default\nsimilarity threshold level. Hence, additional experiments with\nmultiple levels of cosine distance threshold have been done using\nc⃝2020 Information Processing Society of Japan 628\nJournal of Information Processing Vol.28 623–632 (Sep. 2020)\nFig. 2 Performance comparison of Doc2Vec model with diﬀerent similarity\nthresholds.\nthe best performing model to determine if the arbitrarily chosen\nthreshold of 0.7 is suitable. The results of the experiments are\nshown inFig. 2.\nFrom Fig. 2, the similarity threshold of 0.7 is observed to be the\nmost suitable threshold for the Doc2Vec model to act as Natural\nLanguage Filter.\n6. Experiment Using BERT-Based Natural\nLanguage Filter\nThe BERT based Natural Language Filter has been trained and\nevaluated using the subset of the same dataset discussed in Sec-\ntion 4. This section will review the results of the experiment of\ntraining and evaluating the BERT-based cybersecurity language\nmodel.\n6.1 Dataset and Preprocessing\nThe advantage of using transfer learning is that it is possible to\nachieve good performance results with a smaller amount of train-\ning data. Jeremy Howard et al. achieved the same training re-\nsults with only 100 labeled examples in transfer learning as from\ntraining from scratch with 100×more data [7]. Hence, to ease\nthe computational burden, the BERT language model has been\ntrained with a reduced dataset. One quarter of the training data\nused for the Doc2Vec model has been randomly selected from ev-\nery data source. This would serve as the security related dataset\nto ﬁne-tune the BERT model. The composition and number of\nsecurity related training data for the Doc2Vec and BERT models\nare shown inTable 4.\nAlso, in the case of the Doc2Vec model the training has been\nconducted using only security related text data. In order to\nachieve a better result with BERT, a balanced dataset of secu-\nrity related and non-security related texts are required. There-\nfore, all of the discussions and corresponding comments on the\nReddit’s Popular subreddit have been downloaded as of Septem-\nber 4th 2019 to compensate for the non-security related training\ndataset. From the downloaded Reddit discussions, the same num-\nber of documents as security related dataset has been randomly\nchosen and considered as the non-security related dataset. Fi-\nnally, both the datasets have been mixed and total training data\nbecomes 542,186 documents.\nThe BERT model requires the text to be tokenized on the sen-\nTable 4 Number of documents used to train the language models.\nData source Doc2Vec BERT\nReddit discussions 102,952 25,738\nStackExchange discussions 757,180 189,295\nHackernews comments 125,951 31,488\nSecurity news outlet RSS feeds 1,869 467\nSlashdot news archive 6,976 1,744\nNational Vulnerability Database 89,444 22,361\nTotal 1,084,372 271,093\ntence level per line of the text; hence, the spaCy*15 sentence to-\nkenizer has been utilized. As an additional preprocessing step,\nHTML tags and URLs have been removed and no other prepro-\ncessing has been conducted.\nFor the evaluation of the model, the same test dataset men-\ntioned in Section 4.2 has been utilized, following the above pre-\nprocessing.\n6.2 Model Training\nBERT-Base model has 110 million parameters, therefore simi-\nlar hyperparameter optimization as Doc2Vec is not being studied\nas it becomes another research problem due to its enormous com-\nputational cost. Hence the ﬁne-tuning procedure largely followed\nthe hyperparameter settings\n*16 used in BERT for the text clas-\nsiﬁcation task as the original authors recommended. In order to\nclassify security related documents eﬃciently, BERT-Base Un-\ncased model has been utilized since it lowercases the text before\ntokenizing. We used the same WordPiece tokenizer with a vo-\ncabulary size of 30,000 as in the original BERT implementation.\nThe beneﬁt of using the WordPiece tokenizer is that instead of\nnaturally splitting English words, they can be divided into smaller\nsub-word units (e.g., the word “lovely” can be divided into “love”\n+“ly”). It is more eﬀective for handling unknown words which\nare to be expected in our case. BERT-Base has 12 layers and each\nof them produces a sequence of hidden states. Since our task is\nclassiﬁcation, this sequence needs to be reduced to a single vector\nin order to represent the whole text. There are multiple ways of\nreducing the sequence of hidden vectors into a single vector, such\nas using CNN pooling techniques (e.g., max or mean pooling) or\nsimply applying attention to it. However, we decided to go with\nthe simple but eﬀective method of taking the hidden state of the\nlast layer that corresponds to the [CLS] token which happen to be\nthe ﬁrst in the sequence.\nBERT truncates its inputs to a maximum length of 512 tokens;\nhowever, our dataset contains many examples with much longer\nsequences. To handle sequences longer than 512 tokens, we use\na sliding window with a sequence length of 256 across the input\nand take the mean of each representation from the windows. Fine-\ntuning BERT was performed on 4×NVIDIA Tesla V100 GPUs\nand was trained with the following hyperparameters as shown in\nTable 5.\n6.3 Model Evaluation\nWe followed the oﬃcial guide [8] for ﬁne-tuning three epochs\non our target dataset; however, we found the reduced epoch yields\n*15 https://spacy.io/\n*16 https://github.com/google-research/bert\nc⃝2020 Information Processing Society of Japan 629\nJournal of Information Processing Vol.28 623–632 (Sep. 2020)\nTable 5 BERT training hyperparameters.\nNo. Parameter name Setting\n1 Batch size 32\n2 Max seq length 256\n3 Epochs 1, 2, 3\n4 Embedding dropout 0.1\n5 Attention dropout 0.1\n6 Residual dropout 0.1\n7 Optimizer Adam\n8 Learning rate 6.25e-5\nTable 6 BERT Classiﬁcation result on diﬀerent epochs.\nEpoch Precision Recall F1 Score Accuracy\n3 0.91 0.87 0.87 0.87\n2 0.91 0.88 0.89 0.88\n1 0.92 0.90 0.91 0.90\nTable 7 Performance comparison of both the language models.\nModel Precision Recall F1 Score Accuracy\nDoc2Vec 0.83 0.51 0.63 0.83\nBERT 0.92 0.90 0.91 0.90\nbetter results. The results of the evaluation are shown inTable 6.\nTo highlight the improvement from the Doc2Vec language\nmodel, the results of the best performing experiments for both\nthe language models are shown inTable 7.\n7. Discussion\nAs shown in Table 7, the BERT-based Natural Language Filter\nperforms better than the Doc2Vec-based Natural Language Filter\non the same evaluation dataset, even though it was trained with\n4×less security related data. We believe the reasons include the\nfollowing:\n( 1 ) Since BERT encodes text in contextual representation, the\nsemantic meanings are better preserved than in Doc2Vec.\nFor example, the word “bank” could mean a ﬁnancial in-\nstitution as well as geographical terrain adjacent to the river\n(as in river bank). In Doc2Vec, the vector representations of\nboth meanings would have the same vector, whereas BERT\nwould represent it diﬀerently, depending upon the context.\n( 2 ) The Doc2Vec-based model has been trained only on secu-\nrity related data, whereas BERT was pre-trained with generic\nknowledge (Wikipedia and books), on top of which the se-\ncurity related data has been ﬁne-tuned. Hence, BERT con-\ntains better language representation and beneﬁts text classi-\nﬁcation.\n( 3 ) Training the Doc2Vec model used an unsupervised approach\nwithout specifying any label, whereas the BERT model has\nbeen ﬁne-tuned using a labeled and balanced dataset from\nwhich the model could learn better representation.\n( 4 ) As mentioned in Section 6.2 the BERT model splits the natu-\nral English words into sub-words using WordPiece tokenizer.\nThis approach lets BERT avoid the Out Of V ocabulary prob-\nlem by substituting any new word with a combination of\nthe sub-words. For example, the input text “John Johan-\nson’s house” would be tokenized as “john johan ##son’s\nhouse”\n*17. In comparison the Doc2Vec model stores inter-\nnally all the unique tokens and their respective embeddings\n*17 https://github.com/google-research/bert#tokenization\nTable 8 Performance comparison with Logistic Regression model.\nModel Precision Recall F1 Score Accuracy\nDoc2Vec 0.83 0.51 0.63 0.83\nBERT 0.92 0.90 0.91 0.90\nDoc2Vec+LogReg 0.29 0.49 0.36 0.50\nas dictionary. The Doc2Vec model trained for this research\ncontains a vocabulary size of 530,934 unique tokens which\nmight contain noise that has been found in the training doc-\nument.\n( 5 ) The training objective of the Doc2Vec model is to represent\ntext documents in vector space accurately such that semantic\nsimilarities of documents could be found by cosine distance\nof the vectors. In comparison, BERT ﬁne-tunes it’s all layer\nwith classiﬁcation objective.\nIn order to overcome the limitation mentioned in Point 5, an ad-\nditional experiment has been conducted to classify cybersecurity\nrelated documents using Doc2Vec and logistic regression model.\nThe training objective of this experiment is a classiﬁcation by\nintroducing logistic regression on top of Doc2Vec model which\ngenerates vector embedding features for each sample in training\ndata. Therefore, it is similar to the BERT classiﬁcation process\nwhich classiﬁes its internal vector representations based on their\nfeature. For a fair comparison with BERT based Natural Lan-\nguage Filter, we used the same, balanced dataset mentioned in\nSection 6 to train the logistic regression model. The Doc2Vec\nmodel has been initialized according to the best performing set-\nting of Doc2Vec based Natural Language Filter. The experi-\nment results are shown inTable 8in comparison with Doc2Vec\nsimilarity-based classiﬁcation and BERT.\nAs seen in Table 8, the Doc2Vec+LogReg performance is poor\nwith Accuracy of 50% only. Since the test dataset consisted of\nan imbalanced mix of security and non-security related texts, it\nshows slightly better performance than a random guess. A pos-\nsible explanation of such poor performance is that embeddings\ngenerated by BERT represent text better in vector space than em-\nbeddings generated by Doc2Vec. In order to illustrate this visu-\nally, we arbitrarily chose 6 example texts and created a heatmap\nfor their cosine similarity. Each cell in the heatmap represents\nthe cosine similarity of the corresponding texts and the higher the\ncosine similarity the denser the cell colors. The examples include\n2 texts from each of the security and non-security related con-\ntexts and also two texts that contain security related words but\nin non-security related context to emphasize the contextual rep-\nresentation. The heatmap could be seen fromFig. 3and Fig. 4\nin which the semantic similarity of Doc2Vec and BERT embed-\ndings are illustrated. From the heatmaps it could be seen that\nBERT embeddings are better representing the semantic similar-\nities of example texts as compared to embeddings generated by\nDoc2Vec model.\nIn addition to the model diﬀerences mentioned, the techni-\ncal diﬀerences between the experiments conducted using the\nDoc2Vec and BERT language models are shown inTable 9.\nAccording to the results of the evaluation, the BERT-based\nlanguage model outperforms the Doc2Vec-based language model\nas the Natural Language Filter module for the proposed system.\nHowever, there would be considerations regarding computing re-\nc⃝2020 Information Processing Society of Japan 630\nJournal of Information Processing Vol.28 623–632 (Sep. 2020)\nFig. 3 Semantic similarity visualization of Doc2Vec embeddings.\nFig. 4 Semantic similarity visualization of BERT embeddings.\nTable 9 Experiment comparisons.\nModel Training data Test data Resource Time\nDoc2Vec security related\n1,084,372 doc\n415,272\nmixed doc\n8×Intel Xeon\nE5-2650 CPU\n16-18\nhours\nBERT balanced set of\n542,186 doc\n415,272\nmixed doc\n4×NVIDIA\nTesla V100 GPU\n6-18\nhours\nsources when choosing the right language model. Doc2Vec is\na simple and lightweight language model that can be trained\nwith oﬀ-the-shelf hardware within a reasonable amount of time,\nwhereas BERT requires higher computing resources to train. If\nthe model is used for inference-only tasks, the BERT language\nmodel would be suitable, since the costly training will be con-\nducted only once. But if the model is re-trained with consecutive\npositive results as illustrated in Fig. 1, the BERT-based model\nwould be diﬃcult. However, since BERT requires much less\ntraining data to achieve similar results, a Training Document Gen-\nerator module and retraining the module might not be required.\n8. Conclusion\nIn this paper we proposed an architecture for a system that\ncould automatically classify and analyze the cyber threat related\ninformation to assist human operators. In order to implement the\nNatural Language Filter module of the proposed system, neural\nembedding Doc2Vec and pre-trained transformer-based language\nmodel BERT were trained with cybersecurity-speciﬁc text.\nThe evaluation results show that the BERT-based Natural Lan-\nguage Filter outperformed Doc2Vec-based Natural Language Fil-\nter by 28 points in F1 Score.\nFrom Table 7, it can be seen that a language model pre-trained\nwith generic knowledge (Wikipedia and books) performs better\nwhen ﬁne-tuned with few domain-speciﬁc data as compared to\na language model that has been trained on a large amount of\ndomain-speciﬁc data. Hence, we conclude that BERT would be\nthe most suitable language model to implement the Natural Lan-\nguage Filter module for the proposed system.\nIn future research, we would like to explore the possibilities\nof retraining the BERT and Doc2Vec language models and de-\ntermine the necessity of a Training Document Generator module\nfor the proposed system. Also, using Named Entity Recognition\nmethods, we are actively working on the implementation of an\nAnalyzer module for the proposed system.\nAcknowledgments We gratefully acknowledge the support\nof ExaWizards Inc for providing assistance to run computation-\nally expensive experiments in their infrastructure.\nReferences\n[1] Mendsaikhan, O., Hasegawa, H., Yamaguchi, Y . and Shimada, H.:\nMining for operation speciﬁc actionable cyber threat intelligence in\npublicly available information source,Proc. Symposium on Cryptog-\nraphy and Information Security(2018).\n[2] Turney, P.D. and Pantel, P.: From Frequency to Meaning: Vector\nSpace Models of Semantics, CoRR, V ol.abs/1003.1141 (2010) (on-\nline), available from⟨http://arxiv.org/abs/1003.1141⟩.\n[3] Mikolov, T., Chen, K., Corrado, G. and Dean, J.: Eﬃcient Estimation\nof Word Representations in Vector Space,1st International Confer-\nence on Learning Representations, ICLR 2013, Workshop Track Pro-\nceedings (2013) (online), available from⟨http://arxiv.org/abs/\n1301.3781⟩.\n[4] Le, Q.V . and Mikolov, T.: Distributed Representations of Sentences\nand Documents, CoRR, V ol.abs/1405.4053 (2014) (online), available\nfrom ⟨http://arxiv.org/abs/1405.4053⟩.\n[5] Mendsaikhan, O., Hasegawa, H., Yamaguchi, Y . and Shimada, H.:\nIdentiﬁcation of Cybersecurity Speciﬁc Content Using the Doc2Vec\nLanguage Model, Proc. IEEE 43rd Annual Computer Software and\nApplications Conference (COMPSAC), pp.396–401, IEEE (online),\nDOI: 10.1109/COMPSAC.2019.00064 (2019).\n[6] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee,\nK. and Zettlemoyer, L.: Deep contextualized word representations,\nCoRR, V ol.abs/1802.05365 (online), available from⟨http://arxiv.org/\nabs/1802.05365⟩(2018).\n[7] Howard, J. and Ruder, S.: Fine-tuned Language Models for Text Clas-\nsiﬁcation, CoRR, V ol.abs/1801.06146 (online), available from\n⟨http://arxiv.org/abs/1801.06146⟩(2018).\n[8] Devlin, J., Chang, M., Lee, K. and Toutanova, K.: BERT: Pre-\ntraining of Deep Bidirectional Transformers for Language Under-\nstanding, CoRR, V ol.abs\n/1810.04805 (online), available from\n⟨http://arxiv.org/abs/1810.04805⟩(2018).\n[9] Al-Rowaily, K., Abulaish, M., Al-Hasan Haldar, N. and Al-Rubaian,\nM.: BiSAL - A Bilingual Sentiment Analysis Lexicon to Analyze\nDark Web Forums for Cyber Security,Digit. Investig., V ol.14, No.C,\npp.53–62 (online), DOI: 10.1016/j.diin.2015.07.006 (2015).\n[10] Mulwad, V ., Li, W., Joshi, A., Finin, T. and Viswanathan, K.: Extract-\ning Information About Security Vulnerabilities from Web Text,Proc.\nc⃝2020 Information Processing Society of Japan 631\nJournal of Information Processing Vol.28 623–632 (Sep. 2020)\n2011 IEEE/WIC/ACM International Conferences on Web Intelligence\nand Intelligent Agent Technology - Volume 03, WI-IAT ’11, pp.257–\n260, IEEE Computer Society (online), DOI: 10.1109/WI-IAT.2011.26\n(2011).\n[11] Joshi, A., Lal, R., Finin, T. and Joshi, A.: Extracting Cybersecu-\nrity Related Linked Data from Text,Proc. 2013 IEEE Seventh In-\nternational Conference on Semantic Computing, ICSC ’13, pp.252–\n259, IEEE Computer Society (online), DOI: 10.1109/ICSC.2013.50\n(2013).\n[12] More, S., Matthews, M., Joshi, A. and Finin, T.: A Knowledge-Based\nApproach to Intrusion Detection Modeling,Proc. 2012 IEEE Sympo-\nsium on Security and Privacy Workshops, SPW ’12, pp.75–81, IEEE\nComputer Society (online), DOI: 10.1109/SPW.2012.26 (2012).\n[13] Dion ´ısio, N., Alves, F., Ferreira, P.M. and Bessani, A.: Cy-\nberthreat Detection from Twitter using Deep Neural Networks,CoRR,\nVo l . a b s/1904.01127 (2019).\n[14] Tavabi, N., Goyal, P., Almukaynizi, M., Shakarian, P. and Lerman, K.:\nDarkEmbed: Exploit Prediction with Neural Language Models,Proc.\nAAAI Conference on Innovative Applications of AI(IAAI2018) (2018).\n[15] Niu, L. and Dai, X.: Topic2Vec: Learning Distributed Representations\nof Topics,CoRR, V ol.abs/1506.08422 (2015) (online), available from\n⟨http://arxiv.org/abs/1506.08422⟩.\n[16] Dhingra, B., Zhou, Z., Fitzpatrick, D., Muehl, M. and Cohen, W.W.:\nTweet2Vec: Character-Based Distributed Representations for Social\nMedia, CoRR, V ol.abs/1605.03481 (2016) (online), available from\n⟨http://arxiv.org/abs/1605.03481⟩.\n[17] Choi, E., Bahadori, M.T., Searles, E., Co ﬀey, C. and Sun, J.:\nMulti-layer Representation Learning for Medical Concepts, CoRR,\nVo l . a b s/1602.05568 (2016) (online), available from⟨http://arxiv.org/\nabs/1602.05568⟩.\n[18] Aman, H., Amasaki, S., Yokogawa, T. and Kawahara, M.: A\nDoc2Vec-Based Assessment of Comments and Its Application to\nChange-Prone Method Analysis (online), DOI: 10.1109/\nAPSEC.2018.00082 (2018).\n[19] Karvelis, P.S., Gavrilis, D., Georgoulas, G.K. and Stylios, C.D.: Topic\nrecommendation using Doc2Vec,2018 International Joint Conference\non Neural Networks(IJCNN), pp.1–6 (2018).\n[20] Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H. and Kang, J.:\nBioBERT: A pre-trained biomedical language representation model\nfor biomedical text mining, CoRR, V ol.abs/1901.08746 (2019) (on-\nline), available from⟨http://arxiv.org/abs/1901.08746⟩.\n[21] Beltagy, I., Cohan, A. and Lo, K.: SciBERT: Pretrained Contex-\ntualized Embeddings for Scientiﬁc Text,CoRR, V ol.abs/1903.10676\n(2019) (online), available from⟨http://arxiv.org/abs/1903.10676⟩.\n[22] Lee, J. and Hsiang, J.: PatentBERT: Patent Classiﬁcation with Fine-\nTuning a pre-trained BERT Model,CoRR, V ol.abs/1906.02124 (2019)\n(online), available from⟨http://arxiv.org/abs/1906.02124⟩.\n[23] Huang, K., Altosaar, J. and Ranganath, R.: ClinicalBERT: Mod-\neling Clinical Notes and Predicting Hospital Readmission, CoRR,\nVo l . a b s/1904.05342 (2019) (online), available from⟨http://arxiv.org/\nabs/1904.05342⟩.\n[24] Sun, C., Myers, A., V ondrick, C., Murphy, K. and Schmid, C.:\nVideoBERT: A Joint Model for Video and Language Representation\nLearning, CoRR, V ol.abs/1904.01766 (2019) (online), available from\n⟨http://arxiv.org/abs/1904.01766⟩.\n[25] Adhikari, A., Ram, A., Tang, R. and Lin, J.: DocBERT: BERT for\nDocument Classiﬁcation, CoRR, V ol.abs/1904.08398 (2019) (online),\navailable from⟨http://arxiv.org\n/abs/1904.08398⟩.\n[26] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. and Dean, J.: Dis-\ntributed Representations of Words and Phrases and their Composi-\ntionality, CoRR, V ol.abs/1310.4546 (2013) (online), available from\n⟨http://arxiv.org/abs/1310.4546⟩.\n[27] Lau, J.H. and Baldwin, T.: An Empirical Evaluation of doc2vec\nwith Practical Insights into Document Embedding Generation,CoRR,\nVo l . a b s/1607.05368 (2016) (online), available from⟨http://arxiv.org/\nabs/1607.05368⟩.\nOtgonpurev Mendsaikhan received his\nM.S. degree in Information Security\nPolicy and Management from Carnegie\nMellon University, Pittsburgh, PA, USA.\nHe is currently pursuing the Ph.D. de-\ngree at Graduate School of Informatics,\nNagoya University, Japan. His main re-\nsearch interests include cyber security sit-\nuational awareness, cyber threat intelligence, and knowledge\ngraph.\nHirokazu Hasegawa received his Ph.D.\ndegree in Information Science from\nNagoya University, Japan in 2017. He is\ncurrently an Assistant Professor at Infor-\nmation Security Oﬃce, Nagoya Univer-\nsity, Japan. His research interests include\nthe Internet and network security. He is a\nmember of IPSJ and IEICE.\nYukiko Yamaguchireceived her B.S. de-\ngree in Information Engineering from\nNagoya Institute of Technology in 1983\nand the M.S. degree in Information En-\ngineering from Nagoya University. Since\nthen she was aﬃliated with Fujitsu Lab.\nLtd and in April 1991 she joined Nagoya\nUniversity as an Assistant Professor in the\nComputer Center. At present, she is an Assistant Prof. in Infor-\nmation Technology Center and engaged in research on network\nmanagement technology and cyber security. She is a member of\nIPSJ and IEICE.\nHajime Shimada was born in 1976 and\nreceived his B.E., M.E., and D.E. degrees\nfrom Nagoya University, Japan in 1998,\n2000 and 2004 respectively. He was an as-\nsistant professor at Kyoto University from\n2005 to 2009 and an associate professor\nin NAIST from 2009 to 2013. He has\nbeen working as an associate professor of\nNagoya University, Japan since 2013. He is currently focusing\non cyber security, network operation and computer architecture\nrelated researches. He is a member of IEEE, IPSJ and IEICE.\nEnkhbold Bataa is currently working\nfor ExaWizards Inc as a Machine Learn-\ning Engineer. He received his M.S. degree\nin Information Systems from National Ts-\ning Hua University, Hsinchu, Taiwan. His\nresearch focuses on language model, word\nembeddings and computational linguis-\ntics.\nc⃝2020 Information Processing Society of Japan 632"
}