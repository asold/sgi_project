{
  "title": "cTBLS: Augmenting Large Language Models with Conversational Tables",
  "url": "https://openalex.org/W4385734127",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3190294065",
      "name": "Anirudh S. Sundar",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2104606758",
      "name": "Larry Heck",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998536339",
    "https://openalex.org/W2401425944",
    "https://openalex.org/W2397801432",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4285159263",
    "https://openalex.org/W4313680149",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3106255016",
    "https://openalex.org/W4225141790",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W3167748596",
    "https://openalex.org/W4221163895",
    "https://openalex.org/W4385573853",
    "https://openalex.org/W4281656839",
    "https://openalex.org/W4386566488",
    "https://openalex.org/W3173446720",
    "https://openalex.org/W3091546937",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W2690462855",
    "https://openalex.org/W2612228435",
    "https://openalex.org/W4287888135",
    "https://openalex.org/W3101082165",
    "https://openalex.org/W3197798882",
    "https://openalex.org/W2139659117",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W3153051631",
    "https://openalex.org/W4221151914",
    "https://openalex.org/W2896585994",
    "https://openalex.org/W822806878",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W2112006025",
    "https://openalex.org/W4321276751",
    "https://openalex.org/W4312600202",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3146844750",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4287366208",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W2963899988",
    "https://openalex.org/W2136189984",
    "https://openalex.org/W3124424060",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2512994153",
    "https://openalex.org/W3108528212",
    "https://openalex.org/W3035140194"
  ],
  "abstract": "Optimizing accuracy and performance while eliminating hallucinations of open-domain conversational large language models (LLMs) is an open research challenge. A particularly promising direction is to augment and ground LLMs with information from structured sources. This paper introduces Conversational Tables cTBLS, a three-step architecture to retrieve and generate dialogue responses grounded on retrieved tabular information. cTBLS uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 125% relative improvement over the retriever in the previous state-of-the-art system on the HyrbiDialogue dataset. cTBLS then uses a shared process between encoder and decoder models to perform a coarse+fine tabular knowledge (e.g., cell) ranking combined with a GPT-3.5 LLM response generator to yield a 2x relative improvement in ROUGE scores. Finally, human evaluators prefer cTBLs +80% of the time (coherency, fluency) and judge informativeness to be 4x better than the previous state-of-the-art.",
  "full_text": "Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023), pages 59–70\nJuly 14, 2023 ©2023 Association for Computational Linguistics\ncTBLS: Augmenting Large Language Models with Conversational Tables\nAnirudh S Sundar, Larry Heck\nAI Virtual Assistant (A V A) Lab\nThe Georgia Institute of Technology\n{asundar34,larryheck}@gatech.edu\nAbstract\nOptimizing accuracy and performance while\neliminating hallucinations of open-domain con-\nversational large language models (LLMs)\nis an open research challenge. A particu-\nlarly promising direction is to augment and\nground LLMs with information from struc-\ntured sources. This paper introduces Conver-\nsational Tables ( cTBLS), a three-step archi-\ntecture to retrieve and generate dialogue re-\nsponses grounded on retrieved tabular infor-\nmation. cTBLS uses Transformer encoder em-\nbeddings for Dense Table Retrieval and obtains\nup to 125% relative improvement over the re-\ntriever in the previous state-of-the-art system\non the HYRBI DIALOGUE dataset. cTBLS then\nuses a shared process between encoder and de-\ncoder models to perform a coarse+fine tabular\nknowledge (e.g., cell) ranking combined with\na GPT-3.5 LLM response generator to yield\na 2x relative improvement in ROUGE scores.\nFinally, human evaluators prefer cTBLs +80%\nof the time (coherency, fluency) and judge in-\nformativeness to be 4x better than the previous\nstate-of-the-art.\n1 Introduction\nEquipping conversational AI with multimodal ca-\npabilities broadens the range of dialogues that hu-\nmans have with such systems. A persisting chal-\nlenge in multimodal conversational AI is the devel-\nopment of systems that produce conversationally\ncoherent responses grounded in textual and non-\ntextual modalities (Sundar and Heck, 2022).\nIt is well-established that large language mod-\nels (LLMs) possess real-world knowledge stored\nwithin their parameters, as demonstrated by re-\ncent research (Roberts et al., 2020; Heinzerling\nand Inui, 2021). Nevertheless, the incorporation\nof conversation-specific extrinsic knowledge into\nthese models to yield precise responses remains an\nactive area of investigation. While humans can eas-\nily retrieve contextual information from tables by\nexamining rows and columns, LLMs often struggle\nto identify relevant information amidst conversa-\ntional distractions.\nHYBRI DIALOGUE (Nakamura et al., 2022), a\ndataset of conversations grounded on structured\nand unstructured knowledge from tables and text,\nintroduces the task of responding to messages by\nutilizing information from external knowledge and\nprior dialogue turns. The authors also present an\napproach and experimental results on HYBRI DIA-\nLOGUE that represents the current state-of-the-art\n(SoTA).\nThis paper proposes an extension to the SoTA\napproach of HYBRI DIALOGUE in the form of\nConversational Tables (cTBLS) 1, a novel three-\nstep encoder-decoder architecture designed to aug-\nment LLMs with tabular data in conversational set-\ntings. In the first step, cTBLS uses a dual-encoder\nTransformer-based (Vaswani et al., 2017) Dense\nTable Retriever (DTR) to retrieve the correct table\nfrom the entire corpus based on the user’s query.\nThe second step employs a fine-tuned dual-encoder\nTransformer to track system state and rank cells in\nthe retrieved table according to their relevance to\nthe conversation. Finally, cTBLS utilizes GPT-3.5\nto generate a natural language response by prompt-\ning it with the ranked cells.\nWhile previous research separated knowledge\nretrieval and response generation between encoder\nand decoder models, this paper demonstrates that\nLLM decoders can perform these tasks jointly\nwhen prompted with knowledge sources ranked\nby language model encoders. Furthermore, by\npre-training the Dense Table Retriever to perform\nretrieval over a corpus of tables, cTBLS can be\nextended to new knowledge sources without re-\ntraining, by appending additional knowledge to the\ncorpus.\nCompared to the previous SoTA, experiments\n1Our code will be available at https://github.com/\navalab-gt/cTBLS\n59\nFigure 1: cTBLS for conversations on HYBRI DIALOGUE . Dense Table Retrieval identifies the table most relevant\nto the initial query. The retrieved table is provided to the state tracker for follow-up queries. State Tracking ranks\ncells in the table based on their ability to answer a follow-up query. Response Generation utilizes a LLM Decoder\nprovided with the ranked cell information and the follow-up query to convert tabular data into a natural language\nresponse and continue the conversation. Details on individual components are provided in Section 3.\non cTBLS show up to 125% relative improvement\nin table retrieval and a 2x relative improvement in\nROUGE scores. In addition, human evaluators pre-\nfer cTBLs +80% of the time (coherency, fluency)\nand judge informativeness to be 4x better than the\nprevious SoTA.\nOur contributions are as follows:\n1. The introduction of Conversational Tables\n(cTBLS), a novel three-step encoder-decoder\narchitecture designed to augment LLMs with\ntabular data in conversational settings.\n2. Experimental results demonstrating that\nDense Table Retrieval, which utilizes neural\nmodels fine-tuned with a summary of tabular\ninformation, outperforms sparse techniques\nbased on keyword matching for table retrieval.\n3. The presentation of evidence that augmenting\nstate-of-the-art LLM decoders using knowl-\nedge sources ranked by encoder language\nmodels leads to better results on automatic\n(ROUGE-Precision) and human (Coherence,\nFluency, and Informativeness) evaluation\nfor knowledge-grounded response generation\nwhile limiting the number of API calls to these\nmodels.\nThis paper presents the cTBLS system and\ndemonstrates its application to the HYBRI DIA-\nLOGUE dataset. In Section 2, we review the ex-\nisting literature in the fields of Table Question\nAnswering and Knowledge Grounded Response\nGeneration. Section 3 describes the various com-\nponents of cTBLS as presented in Figure 1. In\nSection 4, we evaluate the performance of cTBLS\nagainst previous methods for conversations over\ntables and report experimental results from auto-\nmatic and human evaluations. Finally, Section 5\nconcludes the paper and outlines potential direc-\ntions for future research.\n2 Related Work\n2.1 Table Question Answering\nTable Question Answering is a well-researched\nprecursor to conversations over tables. In WIK-\nITABLE QUESTIONS , Pasupat and Liang (2015)\ntransform HTML tables into a knowledge graph\nand retrieve the correct answer by converting natu-\nral language questions into graph queries. FRETS\n(Jauhar et al., 2016) uses a log-linear model con-\nditioned on alignment scores between cells in ta-\nbles and individual QA pairs in the training set.\nCho et al. (2018) introduce NEOP, a multi-layer\nsequential network with attention supervision to\nanswer queries conditioned on tables. Hannan\net al. (2020) propose MANY MODAL QA, which\nuses a modality selection network and pre-trained\ntext-based QA, Table-based QA, and Image-based\nQA models to jointly answer questions over text,\ntables, and images. Chen et al. (2020c) present\nHYBRIDER , which performs multi-hop QA over\n60\ntables using keyword-matching for cell linking fol-\nlowed by BERT (Devlin et al., 2019) for reason-\ning. Chen et al. (2020a) propose OTT-QA, which\nuses a fusion retriever to identify relevant tables\nand text and a cross-block reader based on a long-\nrange Sparse Attention Transformer (Ainslie et al.,\n2020) to choose the correct answer. Heck and Heck\n(2020) perform multi-task fine-tuning of Trans-\nformer encoders by modeling slot filling as ques-\ntion answering over tabular and visual information\nin Visual Slot. Herzig et al. (2020) and Yin et al.\n(2020) extend BERT for Table Question Answering\nby pre-training a masked language model over text-\ntable pairs in TAPAS and TaBERT, respectively.\nRecent work building off the Transformer architec-\nture for Table Question Answering includes (Eisen-\nschlos et al., 2021; Li et al., 2021; Herzig et al.,\n2021; Zayats et al., 2021; Zhao et al., 2022; Huang\net al., 2022; Yang et al., 2022; Chen, 2022). Jin\net al. (2022) provide a comprehensive survey of\nadvancements in Table Question Answering.\n2.2 Knowledge Grounded Response\nGeneration\nEarly work related to grounding responses gener-\nated by language models in real-world knowledge\nwas motivated by the need to improve prior in-\nformation for open-domain dialogue (Heck et al.,\n2013; Hakkani-Tür et al., 2014; Hakkani-Tür et al.,\n2014; Huang et al., 2015; Jia et al., 2017). More\nrecently, knowledge grounded response generation\nhas been applied to mitigate the hallucination prob-\nlem (Maynez et al., 2020; Shuster et al., 2021) in\nLLMs. RAG (Lewis et al., 2020) fine-tunes LLMs\nusing Dense Passage Retrieval (Karpukhin et al.,\n2020) over a Wikipedia dump to ground responses\nfor Open Domain Question Answering. KGPT\n(Chen et al., 2020b) and SKILL (Moiseev et al.,\n2022) pre-train a Transformer encoder (Vaswani\net al., 2017) with English Wikidump for Natural\nLanguage Generation. Fusion-in-Decoder (Izac-\nard and Grave, 2021) fine-tunes decoder models\nusing evidence acquired through Dense Passage\nRetrieval.\nRecent research also includes a dual-stage ap-\nproach where LLMs generate knowledge sources\nbased on prompts (Yu et al., 2022; Bonifacio et al.,\n2022; Jeronymo et al., 2023). Closest to our work,\nWizard of Wikipedia (Dinan et al., 2018) jointly op-\ntimizes an encoder-decoder Transformer to produce\ndialogue responses conditioned on retrieved knowl-\nedge and dialogue context but does not extend their\napproach to the multiple modalities. REPLUG (Shi\net al., 2023) ensembles output responses generated\nby prompting large language models with inputs\nfrom a dense retriever in a zero-shot setting. How-\never, this requires multiple API calls to state-of-the-\nart LLMs. LLM-A UGMENTER (Peng et al., 2023)\nincorporates external knowledge in LLM responses\nby matching keywords in dialogue state to can-\ndidate knowledge sources obtained through web-\nsearch. A survey of knowledge fusion in LLMs\nis available in Colon-Hernandez et al. (2021) and\nRichardson and Heck (2023).\nIn contrast to prior research that focuses on\neither Table Question Answering or Knowledge\nGrounded Response Generation, our work, cTBLS,\naddresses the challenge of generating responses\ngrounded on tabular knowledge. Moreover, while\ncTBLS is fine-tuned to retrieve tables and filter\nout incorrect references, it leverages the power of\nSoTA pre-trained LLMs for response generation.\nFurthermore, by fine-tuning open-source table and\nknowledge retrievers to remove inaccurate refer-\nences, cTBLS reduces the number of API calls to\nthe SoTA LLMs.\n3 Method\nThe challenge of developing conversational sys-\ntems grounded in tabular information consists of\nthree tasks, namely table retrieval, system state\ntracking, and response generation. Table retrieval\nrequires identifying the most relevant table in the\ndataset based on a given natural language query.\nSystem state tracking is responsible for ranking the\ncells in the table, enabling the system to provide\nresponses to follow-up queries about the table. Fi-\nnally, response generation involves converting the\nranked cells into a natural language response.\n3.1 Table Retrieval\nTable retrieval is a prerequisite to answering queries\nwhen the exact table to converse over is unspecified.\nThe objective is to identify the correct table from\na vast corpus. cTBLS proposes formulating table\nretrieval as document retrieval by assigning a rele-\nvance score to each table based on its relevance to\nthe natural language query. Inspired by Karpukhin\net al. (2020) and Huang et al. (2013), cTBLS uses\na dual-encoder-based Dense Table Retrieval (DTR)\nmodel. The DTR model pre-computes a vector-\nized embedding of all tables in the corpus. Given a\n61\nFigure 2: An example of table-associated text in the\ncontext of Wikipedia, where the input to the DTR text-\nencoder includes the page title, the introduction to the\narticle, the section title, and the introduction paragraph.\nquery at inference, the retrieved table is closest to\nthe query in the embedded space, indicated by the\nupper-left portion of Figure 1.\nThe DTR model consists of a table encoder and\na question encoder, initialized from RoBERTa-base\n(Liu et al., 2019). The input to the table encoder\ncomprises the table’s title and, if available, textual\ninformation associated with the table. Figure 2\npresents an example of table-associated text in the\ncontext of Wikipedia, where introductions from the\npage and section provide additional grounding. The\ninput to the question encoder is the current query to\nbe answered. Taking the average over the sequence\nof the last hidden state at the table and question\nencoder results in 768-dimensional embeddings of\nthe table information and the query.\nThe DTR model is optimized through a con-\ntrastive prediction task, which aims to maximize\nthe similarity between embeddings of a given\nquery qand the table to be retrieved τ while mini-\nmizing the similarity to other incorrect tables τni\nfor i= 1,...,N . As per (Karpukhin et al., 2020),\nnormalized embedding vectors are utilized to opti-\nmize the objective in Equation 1:\narg min\nτ\n(\n−log eq·τ\neq·τ + ∑N\ni=1 eq·τni\n)\n(1)\nGiven a batch Bof d-dimensional query embed-\ndings Q and table embeddings T, the DTR model\ncomputes the similarity QTT (∈RB×B) between\nevery query and table in the batch. This similar-\nity computation enables the sampling of negatives\nfrom other query-table pairs, resulting in B2 train-\ning samples in each batch, consisting of Bpositive\npairs along the diagonal and B2 −Bnegatives.\n3.2 Coarse System State Tracking\nGiven a table, system state tracking involves rank-\ning cells in the table by their relevance to conver-\nsational queries. In contrast to quesiton-answering,\nconversational queries require leveraging informa-\ntion from external modalities in conjunction with\nprior dialogue turns to generate coherent responses\n(Sundar and Heck, 2022). cTBLS addresses sys-\ntem state tracking through two sub-tasks - coarse\nand fine system state tracking. Coarse system state\ntracking ranks cells in the table, while fine system\nstate tracking identifies fine-grained information in\nthe most relevant cell to answer the query.\ncTBLS uses a RoBERTa-base dual-encoder ar-\nchitecture for coarse system state tracking. The\ncell encoder embeds all cells and associated hyper-\nlinked information, and the question encoder gen-\nerates embeddings for the dialogue history ( Dh)\nthat includes the current turn’s query as well as\nprevious queries and responses.\nTo rank cells based on their relevance to the\nfollow-up query, as illustrated in the upper-right\nsection of Figure 1, the question and cell encoders\nare optimized using a triplet loss configuration.\nThis optimization aims to minimize the distance be-\ntween the anchor Dh and the positive cell c, while\npushing the negative cell cfurther away from Dh\nby a margin m(Equation 2).\narg min\nci\n(max{d(Dh, c) −d(Dh, c) +m, 0}) (2)\nd(x, y) =||x −y||2 (3)\nFor our approach, we utilize an anchor-positive-\nnegative triplet consisting of the complete dialogue\nhistory (including queries and responses from pre-\nvious turns) concatenated with the current query as\nthe anchor, the correct cell as the positive, and other\ncells from the same table that are not relevant to\nthe query as negatives. We measure the distance be-\ntween the anchor and the positive and between the\nanchor and the negatives using the 2-norm distance\nfunction d(·).\n3.3 Fine System State Tracking and Response\nGeneration\nIn contrast to coarse system state tracking, fine sys-\ntem state tracking involves identifying the exact\nphrase that answers the query from a ranked subset.\nThe extracted phrase is converted into a natural lan-\nguage response that is coherent within the context\nof the conversation.\n62\ncTBLS employs GPT-3.5 (Brown et al., 2020)\nto perform fine system state tracking and response\ngeneration jointly. GPT-3.5 is prompted to generate\na natural language response to a follow-up query\nconditioned on cells of the table ranked by their\nrelevance to the query as obtained from the coarse\nstate tracker. The prompt includes the dialogue\nhistory, ranked knowledge sources, and the query to\nbe answered. The bottom-right section of Figure 1\noutlines this process.\n4 Experiments\n4.1 H YBRI DIALOGUE\nThe HYBRI DIALOGUE dataset (Nakamura et al.,\n2022) comprises 4800 natural language conversa-\ntions grounded in text and tabular information from\nWikipedia. Crowdsourced workers break down\nmulti-hop questions from the OTT-QA dataset\n(Chen et al., 2020a) into natural questions and con-\nversational responses related to tabular data. On\naverage, dialogues in the dataset consist of 4-5 con-\nversation turns, with a total of 21,070 turns avail-\nable in the dataset. Examples of conversations can\nbe found in Figures 3 and 4.\n4.2 Table Retrieval\nThe first conversation turn of HYBRI DIALOGUE\nrequires selecting the correct table based on the\ninput query for which we use the Dense Table Re-\ntriever outlined in Section 3.1. The Dense Table\nRetriever is fine-tuned for 20 epochs using Adam\n(Kingma and Ba, 2014) with a learning rate of 1e-6\nand a linear learning schedule with five warmup\nsteps. The loss function is a modification of the\ncontrastive loss implementation from ConVIRT\n(Zhang et al., 2022), with image embeddings re-\nplaced by table embeddings. The table retriever\nused in the HYBRI DIALOGUE paper (Nakamura\net al., 2022) was the BM25Okapi Retriever (Trot-\nman et al., 2014) from rank-bm25. According to\nthe results presented in Table 1, cTBLS-DTR out-\nperforms BM25 in terms of Mean Reciprocal Rank\n(MRR), Top-1 Accuracy, and Top-3 Accuracy on\nHYBRI DIALOGUE .\n4.3 Coarse State Tracking\nCoarse state tracking ranks cells from a table based\non their relevance to a query. As before, the dual-\nencoder coarse state tracker of cTBLS consists of\nRoBERTa-base fine-tuned using Adam with a learn-\ning rate of 1e-6 and a linear learning schedule with\nMRR\n@10\nTop 1\nAcc\nTop 3\nAcc\nBM25 0.491 0.345 0.460\ncTBLS-DTR 0.846 0.777 0.901\nTable 1: BM25 vs cTBLS-DTR for retrieval on first turn\nof conversation, results on HYBRI DIALOGUE testing\ndataset. cTBLS-DTR obtains up to 125% relative im-\nprovement over sparse table retrieval\nMRR@10\nSentenceBERT (Reimers and Gurevych, 2019) 0.603\nTaPas (Herzig et al., 2020) 0.689\ncTBLS - RoBERTa-base 0.683\nTable 2: System state tracking results on HYBRI DIA-\nLOGUE . cTBLS achieves nearly the same Mean Recip-\nrocal Rank (MRR) @ 10 as TaPaS, without additional\ntable pre-training on SQA (Iyyer et al., 2017)\nfive warmup steps. In contrast to table retrieval,\nthe state tracker uses triplet margin loss with a\nmargin of 1.0 (Equation 2) instead of contrastive\nloss (Equation 1). The results, as demonstrated\nin Table 2, show that fine-tuning RoBERTa-base\nsolely on HYBRI DIALOGUE surpasses the perfor-\nmance of SentenceBERT (Reimers and Gurevych,\n2019). Furthermore, it nearly attains the same\nMRR @10 as TaPas (Herzig et al., 2020), even\nwithout additional table pre-training on the SQA\ndataset (Iyyer et al., 2017).\n4.4 Fine State Tracking and Response\nGeneration\ncTBLS uses GPT-3.5 (text-davinci-003) with the\nexisting dialogue context, the current query, and\nthe retrieved references from coarse state track-\ning to obtain a natural language response. Since\nfine-tuning the best available version of the model\nis cost prohibitive, we opt to prompt GPT-3.5 to\ngenerate responses instead.\nTop-1 Top-3 Top-10\ncTBLS - RoBERTa-base 0.559 0.778 0.925\nTable 3: Top-k accuracy for cTBLS on coarse system\nstate tracking. cTBLS ranks the correct cell as the top\nreference in 56% of follow-up queries on HYBRI DI-\nALOGUE . The correct cell is ranked in the Top-3 and\nTop-10 retrievals in approximately 78% and 93% of\nconversations, respectively.\n63\nModel TR KR RG ROUGE-1 ROUGE-2 ROUGE-L\n- BM25 Top-1 DialoGPT 0.207 0.042 0.181\n- BM25 Top-3 DialoGPT 0.212 0.045 0.186\n- BM25 Top-1 GPT3.5 0.428 0.207 0.369\n- BM25 Top-3 GPT3.5 0.475 0.242 0.413\n- DTR Top-1 DialoGPT 0.222 0.051 0.195\n- DTR Top-3 DialoGPT 0.226 0.059 0.199\n- DTR Top-1 GPT3.5 0.494 0.255 0.424\n- DTR Top-3 GPT3.5 0.560 0.295 0.479\nHYBRI DIALOGUE Gold Top-1 DialoGPT 0.438 0.212 0.375\ncTBLS NoK Gold - GPT3.5 0.487 0.229 0.422\ncTBLS Top-1 Gold Top-1 GPT3.5 0.603 0.304 0.517\ncTBLS Top-3 Gold Top-3 GPT3.5 0.642 0.322 0.548\nTable 4: Ablation study on automatic evaluation metrics ROUGE-1, ROUGE-2, and ROUGE-L Precision. Using\nDense Table Retrieval (DTR) improves results over BM25 across Top-1 and Top-3 knowledge for DialoGPT and\nGPT3.5. Furthermore, using Top-3 knowledge sources results in better results than using only Top-1 knowledge\nsources for DialoGPT and GPT3.5 using both table retrieval methods. cTBLS No Knowledge (NoK), Top-\n1 Knowledge, Top-3 Knowledge, and HYBRI DIALOGUE use ground truth table retrieval. cTBLS exhibits a 2x\nrelative improvement in ROUGE Precision overHYBRI DIALOGUE . TR: Table Retrieval, KR: Knowledge Retrieval,\nRG: Response Generation\nThe results presented in Table 3 demonstrate\nthat the coarse state tracker successfully retrieves\nthe correct cell in approximately 56% of conver-\nsations during inference. Furthermore, it achieves\nTop-3 and Top-10 retrievals in approximately 78%\nand 93% of conversations, respectively. Moti-\nvated by these results, the fine state tracker of\ncTBLS is evaluated in two different configurations\nby prompting GPT-3.5 augmented with the Top-1\nand Top-3 knowledge references ( cTBLS Top-1\nand cTBLS Top-3). Due to limits on token length\nassociated with the OpenAI API, we remove stop-\nwords from the knowledge provided in the prompt\nand do not experiment with Top-10 knowledge aug-\nmentation.\nSince LLMs store factual information in their\nweights (Roberts et al., 2020; Heinzerling and\nInui, 2021), we compare to few-shot prompting\n(using two examples) with no knowledge sources\n(cTBLS-NoK). Furthermore, to enable a meaning-\nful comparison with existing research (Nakamura\net al., 2022), we measure cTBLS against the sys-\ntem proposed by HYBRI DIALOGUE that utilizes a\nfine-tuned DialoGPT-medium (Zhang et al., 2019)\nmodel augmented with Top-1 knowledge.\nTable 4 presents ROUGE-1, ROUGE-2, and\nROUGE-L precision (Lin, 2004) for all models\nassessed. The results demonstrate that superior\ndownstream performance can be achieved through\nimprovements in table retrieval. Specifically, when\nkeeping the number of knowledge sources constant,\nwe observe an improvement in ROUGE precision\nscores when transitioning from BM25 to DTR, and\nfrom DTR to gold table retrieval. The inclusion of\nadditional knowledge sources leads to an improved\nn-gram overlap with the ground truth reference,\nas evidenced by the Top-3 knowledge augmented\nmodels outperforming their Top-1 counterparts uti-\nlizing the same table retriever, and cTBLS Top-1\noutperforming the baseline model cTBLS NoK.\nMoreover, cTBLS Top-3 achieves the best perfor-\nmance across all automatic metrics, suggesting the\nbenefits of splitting knowledge retrieval into coarse\nand fine state tracking, and utilizing additional\nknowledge sources. Finally, all three configura-\ntions of cTBLS demonstrate superior performance\nto HYBRI DIALOGUE .\n4.5 Human Evaluation\nTo gain a deeper understanding of cTBLS, we con-\nducted human evaluation using the metrics outlined\nby Nakamura et al. (2022), namely Coherence,\nFluency, and Informativeness. For the evaluation\nof these metrics, we enlisted crowd workers from\nAmazon Mechanical Turk (AMT) to assess 50%\nof the test data. The evaluation process involved\na comparison between the responses generated by\nHYBRI DIALOGUE and cTBLS Top-3.\n64\ncTBLS Top-3 vs HYBRI DIALOGUE\nCoherence 0.842\nFluency 0.827\nTable 5: Coherence and Fluency -cTBLS Top-3 is more\nconversationally coherent than the best performing HY-\nBRI DIALOGUE system 84.2% of the time and is more\nfluent 82.7% of the time.\nIn accordance with the methodology delineated\nin Nakamura et al. (2022), Coherence was defined\nas the degree to which a response continued the con-\nversation in a logically coherent manner based on\nprior context. Fluency, conversely, was determined\nby evaluating absence of grammatical and spelling\nerrors, and appropriate use of parts of speech.\nTo ensure the quality of the evaluated responses,\nwe engaged crowd workers possessing a Masters\nqualification on AMT and originating from English-\nspeaking countries (USA, Canada, Australia, New\nZealand, or Great Britain). Each task required ap-\nproximately 30 seconds to complete, and workers\nwere remunerated at a rate of $0.05 per task. More-\nover, to minimize bias and guarantee the depend-\nability of the evaluations, we assigned two crowd\nworkers to assess each response, with a response\ndeemed more coherent or fluent only if both evalu-\nations concurred.\nThe results presented in Table 5 reveal that the re-\nsponses generated by cTBLS Top-3 were more co-\nherent than those produced by HYBRI DIALOGUE\nin 84.2% of cases and exhibited greater fluency\n82.7% of the time, suggesting that improvements\nin table retrieval, knowledge retrieval, and response\ngeneration lead to better downstream performance.\nInformativeness represents the accuracy of\nmachine-generated responses when compared to\nthe ground-truth (Nakamura et al., 2022) and serves\nas a measure of hallucination in LLMs. Halluci-\nnated responses tend to be less informative, deviat-\ning significantly from the ground-truth.\nTo evaluate informativeness, crowd workers de-\ntermined whether generated responses were se-\nmantically equivalent to the ground truth response.\nEach response was assessed by two Turkers, and\na response was deemed more informative only if\nthere was inter-annotator agreement. The absence\nof illustrative examples in the prompting process re-\nsulted in responses generated by cTBLS Top-1 and\ncTBLS Top-3being longer than the ground truth re-\nsponse. Consequently, the knowledge-augmented\nInformativeness\nHYBRI DIALOGUE 0.124\ncTBLS - NoK 0.306\ncTBLS Top-1 0.456\ncTBLS Top-3 0.500\nTable 6: Human Evaluation Metrics - Fraction of cases\nwhere model response is semantically equivalent to\nground truth response. Using more knowledge sources\nresults in responses that are more informative, helping\nreduce hallucination.\ncTBLS responses were considered informative if\nall the information provided in the ground truth\nwas encapsulated in the model response, even if\ncTBLS included supplementary information.\nThe data in Table 6 indicate that cTBLS Top-3\nencompasses the same information as the ground\ntruth response 50% of the time, a higher rate than\ncTBLS Top-1 at 45.6%, exemplifying the benefits\nof partitioning retrieval into coarse and fine state\ntracking and augmenting with additional knowl-\nedge. Based on these findings, we hypothesize that\nthe attention mechanism in decoder models facili-\ntates additional knowledge retrieval. cTBLS NoK\ngenerates the correct response 30.6% of the time,\nsuggesting that HYBRI DIALOGUE comprises ques-\ntions and answers predicated on general world\nknowledge embedded in the weights of LLMs. Re-\nsponses produced by HYBRI DIALOGUE are infor-\nmative in merely 12.4% of instances.\nFigure 3 presents a comparison of responses gen-\nerated by various configurations of cTBLS on the\nHYBRI DIALOGUE dataset. The entire dialogue his-\ntory constitutes the context and is depicted as an\nexchange between the user (in blue) and the system\n(in yellow). The final question box represents the\nfollow-up query to be addressed, while the last an-\nswer chat box indicates the ground truth response.\nKnowledge K1, K2, and K3 correspond to cells\nof the table retrieved during state tracking, based\non which responses are produced. cTBLS NoK\ngenerates a response solely relying on the context,\ncTBLS Top-1 formulates a response conditioned\non K1, and cTBLS Top-3 devises a response based\non K1, K2, and K3.\ncTBLS NoK creates a hallucinated response, an-\nswering with the random Faroese club B68 Toftir.\nSimilarly, cTBLS Top-1 hallucinates a response,\nopting for B36 Tórshavn, as K1 refers to the sta-\ndium Viò Margáir rather than the correct club’s\n65\nFigure 3: Generated responses vs Ground Truth on HYBRI DIALOGUE test set. Questions are in blue and responses\nin yellow. K1, K2, and K3 represent the Top 3 knowledge sources ranked by relevance to the query \"Which team\nplays there?\". cTBLS Top-3 is able to leverage K3 to generate the correct response while cTBLS NoK hallucinates\na response and cTBLS Top-1 generates an incorrect response based on K1. Table obtained from Wikipedia available\nhere\nFigure 4: Generated responses vs Ground Truth on HYBRI DIALOGUE test set. Despite selecting the rows of the\ntable corresponding to Oil and gas industries, cTBLS NoK, Top-1, and Top-3 struggle with counting and hallucinate\na response. Table obtained from Wikipedia available here\n66\nname. In contrast, cTBLS Top-3 produces the accu-\nrate response, EB/Streymur, since K3 contains the\nnecessary information. This example demonstrates\nthe benefits of augmenting response generation\nwith additional pertinent knowledge, which aids\nin mitigating the hallucination problem (Maynez\net al., 2020).\n5 Conclusion\nIn this paper, we introduce Conversational Ta-\nbles (cTBLS), a system designed to address multi-\nturn dialogues that are grounded in tabular data.\ncTBLS separates tabular dialogue into three dis-\ntinct tasks, specifically table retrieval, system state\ntracking, and response generation. The dense ta-\nble retrieval system of cTBLS yields an enhance-\nment of up to 125% relative to keyword-matching\nbased techniques on the HYBRI DIALOGUE dataset,\nwith regard to Top-1 Accuracy and Mean Recip-\nrocal Rank @ 10. Furthermore, cTBLS conducts\nsystem state tracking utilizing a two-step process\nshared between encoder and decoder models. This\nmethodology results in natural language responses\nexhibiting a 2x relative improvement in ROUGE\nscores. Human evaluators favor cTBLS +80% of\nthe time (coherency and fluency) and judge infor-\nmativeness to be 4x better than the previous state-\nof-the-art.\n6 Limitations\nAlthough cTBLS enhances LLMs with tabular\nknowledge to generate grounded responses, certain\nlimitations remain to be addressed.\nFirstly, the efficacy of cTBLS is constrained by\nthe total number of knowledge sources employed\nduring the augmentation process. Token length re-\nstrictions in the OpenAI API limit the knowledge\naugmentation to the top three cells of the table.\nAnother limitation is the incapacity of cTBLS to\nhandle queries pertaining to the entire table. Fig-\nure 4 demonstrates one such instance in which the\nstate tracker module accurately retrieves three rows\nof the table corresponding to oil and gas industries,\nyet the response generation module fails to utilize\nthis information when transforming the retrieved\nstate into a response. Generally, cTBLS encounters\ndifficulties with counting, comparing the values of\ncells, and other mathematical operations, an issue\nwe aim to address in future research.\n7 Acknowledgements\nWe would like to thank Christopher Richardson,\nBenjamin Z Reichman, Atishay Jain, and Srikar\nBhumireddy for their contributions. We would\nalso like to thank the review committee for their\nfeedback. This work was supported by NSF IIS-\n2112633 and by CoCoSys, one of seven centers in\nJUMP 2.0, a Semiconductor Research Corporation\n(SRC) program sponsored by DARPA.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs in\ntransformers. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 268–284, Online. Association\nfor Computational Linguistics.\nLuiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and\nRodrigo Nogueira. 2022. Inpars: Data augmentation\nfor information retrieval using large language models.\narXiv preprint arXiv:2202.05144.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nWenhu Chen. 2022. Large language models are\nfew (1)-shot table reasoners. arXiv preprint\narXiv:2210.06710.\nWenhu Chen, Ming-Wei Chang, Eva Schlinger, William\nWang, and William W Cohen. 2020a. Open ques-\ntion answering over tables and text. arXiv preprint\narXiv:2010.10439.\nWenhu Chen, Yu Su, Xifeng Yan, and William Yang\nWang. 2020b. Kgpt: Knowledge-grounded pre-\ntraining for data-to-text generation. arXiv preprint\narXiv:2010.02307.\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong,\nHong Wang, and William Yang Wang. 2020c. Hy-\nbridQA: A dataset of multi-hop question answering\nover tabular and textual data. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 1026–1036, Online. Association for Computa-\ntional Linguistics.\nMinseok Cho, Reinald Kim Amplayo, Seung-won\nHwang, and Jonghyuck Park. 2018. Adversarial\ntableqa: Attention supervision for question answer-\ning on tables. In Asian Conference on Machine\nLearning, pages 391–406. PMLR.\n67\nPedro Colon-Hernandez, Catherine Havasi, Jason\nAlonso, Matthew Huggins, and Cynthia Breazeal.\n2021. Combining pre-trained language mod-\nels and structured knowledge. arXiv preprint\narXiv:2101.12294.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. arXiv preprint arXiv:1811.01241.\nJulian Martin Eisenschlos, Maharshi Gor, Thomas\nMüller, and William W Cohen. 2021. Mate: multi-\nview attention for table transformer efficiency. arXiv\npreprint arXiv:2109.04312.\nDilek Hakkani-Tür, Malcolm Slaney, Asli Celikyilmaz,\nand Larry Heck. 2014. Eye gaze for spoken language\nunderstanding in multi-modal conversational inter-\nactions. In Proceedings of the 16th International\nConference on Multimodal Interaction , ICMI ’14,\npage 263–266, New York, NY , USA. Association for\nComputing Machinery.\nDilek Hakkani-Tür, Asli Celikyilmaz, Larry Heck,\nGokhan Tur, and Geoff Zweig. 2014. Probabilistic\nenrichment of knowledge graph entities for relation\ndetection in conversational understanding. In Pro-\nceedings of Interspeech.\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\nManymodalqa: Modality disambiguation and qa over\ndiverse inputs. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 34, pages\n7879–7886.\nLarry Heck, Dilek Hakkani-Tür, Madhu Chinthakunta,\nGokhan Tur, Rukmini Iyer, Partha Parthasacarthy,\nLisa Stifelman, Elizabeth Shriberg, and Ashley Fidler.\n2013. Multimodal conversational search and browse.\nFirst Workshop on Speech, Language and Audio in\nMultimedia Marseille, France.\nLarry Heck and Simon Heck. 2020. Zero-shot visual\nslot filling as question answering. arXiv preprint\narXiv:2011.12340.\nBenjamin Heinzerling and Kentaro Inui. 2021. Lan-\nguage models as knowledge bases: On entity repre-\nsentations, storage capacity, and paraphrased queries.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 1772–1791, Online.\nAssociation for Computational Linguistics.\nJonathan Herzig, Thomas Müller, Syrine Krichene, and\nJulian Eisenschlos. 2021. Open domain question\nanswering over tables via dense retrieval. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n512–519, Online. Association for Computational Lin-\nguistics.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nHongzhao Huang, Larry Heck, and Heng Ji. 2015.\nLeveraging deep neural networks and knowledge\ngraphs for entity disambiguation. arXiv preprint\narXiv:1504.07678.\nJunjie Huang, Wanjun Zhong, Qian Liu, Ming Gong,\nDaxin Jiang, and Nan Duan. 2022. Mixed-modality\nrepresentation learning and pre-training for joint\ntable-and-text retrieval in openqa. arXiv preprint\narXiv:2210.05197.\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for web search using\nclickthrough data. In Proceedings of the 22nd ACM\ninternational conference on Information & Knowl-\nedge Management, pages 2333–2338. ACM.\nMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.\nSearch-based neural structured learning for sequen-\ntial question answering. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1821–\n1831, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nSujay Kumar Jauhar, Peter Turney, and Eduard Hovy.\n2016. Tables as semi-structured knowledge for ques-\ntion answering. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 474–483,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nVitor Jeronymo, Luiz Bonifacio, Hugo Abonizio,\nMarzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and\nRodrigo Nogueira. 2023. Inpars-v2: Large language\nmodels as efficient dataset generators for information\nretrieval. arXiv preprint arXiv:2301.01820.\n68\nRobin Jia, Larry Heck, Dilek Hakkani-Tür, and Georgi\nNikolov. 2017. Learning concepts through conver-\nsations in spoken dialogue systems. In 2017 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 5725–5729.\nNengzheng Jin, Joanna Siebert, Dongfang Li, and Qing-\ncai Chen. 2022. A survey on table question answer-\ning: Recent advances. In Knowledge Graph and\nSemantic Computing: Knowledge Graph Empow-\ners the Digital Economy, pages 174–186, Singapore.\nSpringer Nature Singapore.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nAlexander Hanbo Li, Patrick Ng, Peng Xu, Henghui\nZhu, Zhiguo Wang, and Bing Xiang. 2021. Dual\nreader-parser on hybrid textual and tabular evidence\nfor open domain question answering. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 4078–4088, Online.\nAssociation for Computational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nFedor Moiseev, Zhe Dong, Enrique Alfonseca, and Mar-\ntin Jaggi. 2022. SKILL: Structured knowledge infu-\nsion for large language models. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1581–1588,\nSeattle, United States. Association for Computational\nLinguistics.\nKai Nakamura, Sharon Levy, Yi-Lin Tuan, Wenhu Chen,\nand William Yang Wang. 2022. HybriDialogue: An\ninformation-seeking dialogue dataset grounded on\ntabular and textual data. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2022, pages\n481–492, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, and Jianfeng Gao. 2023. Check\nyour facts and try again: Improving large language\nmodels with external knowledge and automated feed-\nback.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nChristopher Richardson and Larry Heck. 2023. Com-\nmonsense reasoning for conversational ai: A sur-\nvey of the state of the art. Workshop on Knowl-\nedge Augmented Methods for NLP , (KnowledgeNLP-\nAAAI’23).\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\n69\nAnirudh Sundar and Larry Heck. 2022. Multimodal con-\nversational AI: A survey of datasets and approaches.\nIn Proceedings of the 4th Workshop on NLP for Con-\nversational AI, pages 131–147, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nAndrew Trotman, Antti Puurula, and Blake Burgess.\n2014. Improvements to bm25 and language models\nexamined. In Proceedings of the 2014 Australasian\nDocument Computing Symposium, ADCS ’14, page\n58–65, New York, NY , USA. Association for Com-\nputing Machinery.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nJingfeng Yang, Aditya Gupta, Shyam Upadhyay,\nLuheng He, Rahul Goel, and Shachi Paul. 2022.\nTableformer: Robust transformer modeling for table-\ntext encoding. arXiv preprint arXiv:2203.00274.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\nbastian Riedel. 2020. TaBERT: Pretraining for joint\nunderstanding of textual and tabular data. InProceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong\nXu, Mingxuan Ju, Soumya Sanyal, Chenguang\nZhu, Michael Zeng, and Meng Jiang. 2022. Gen-\nerate rather than retrieve: Large language mod-\nels are strong context generators. arXiv preprint\narXiv:2209.10063.\nVicky Zayats, Kristina Toutanova, and Mari Ostendorf.\n2021. Representations for question answering from\ndocuments with tables and text. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 2895–2906, Online. Association for Computa-\ntional Linguistics.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun\nChen, Chris Brockett, Xiang Gao, Jianfeng Gao,\nJJ (Jingjing) Liu, and Bill Dolan. 2019. Dialogpt:\nLarge-scale generative pre-training for conversational\nresponse generation. In arXiv:1911.00536.\nYuhao Zhang, Hang Jiang, Yasuhide Miura, Christo-\npher D Manning, and Curtis P Langlotz. 2022. Con-\ntrastive learning of medical visual representations\nfrom paired images and text. In Machine Learning\nfor Healthcare Conference, pages 2–25. PMLR.\nYilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang.\n2022. MultiHiertt: Numerical reasoning over multi\nhierarchical tabular and textual data. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 6588–6600, Dublin, Ireland. Association for\nComputational Linguistics.\n70",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8113768100738525
    },
    {
      "name": "Transformer",
      "score": 0.6582510471343994
    },
    {
      "name": "Language model",
      "score": 0.649939775466919
    },
    {
      "name": "Encoder",
      "score": 0.6215232014656067
    },
    {
      "name": "Fluency",
      "score": 0.6094619631767273
    },
    {
      "name": "Natural language processing",
      "score": 0.48917320370674133
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4566769003868103
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.45464998483657837
    },
    {
      "name": "Speech recognition",
      "score": 0.3903927803039551
    },
    {
      "name": "Information retrieval",
      "score": 0.3214623034000397
    },
    {
      "name": "Linguistics",
      "score": 0.12214109301567078
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ]
}