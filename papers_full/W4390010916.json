{
  "title": "Large Language Models Facilitate the Generation of Electronic Health Record Phenotyping Algorithms",
  "url": "https://openalex.org/W4390010916",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097325630",
      "name": "Chao Yan",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2423287721",
      "name": "Henry H Ong",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A3100137909",
      "name": "Monika E. Grabowska",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2535729801",
      "name": "Matthew S. Krantz",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A4282757789",
      "name": "Wu-Chen Su",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2890427170",
      "name": "Alyson L Dickson",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2142701830",
      "name": "Josh F Peterson",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2426726753",
      "name": "QiPing Feng",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2233545356",
      "name": "Dan M. Roden",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2105736763",
      "name": "C. Michael Stein",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2978130392",
      "name": "V. Eric Kerchberger",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2145238237",
      "name": "Bradley A Malin",
      "affiliations": [
        "Vanderbilt University",
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2989674414",
      "name": "Wei-Qi Wei",
      "affiliations": [
        "Vanderbilt University",
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2097325630",
      "name": "Chao Yan",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2423287721",
      "name": "Henry H Ong",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A3100137909",
      "name": "Monika E. Grabowska",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2535729801",
      "name": "Matthew S. Krantz",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A4282757789",
      "name": "Wu-Chen Su",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2890427170",
      "name": "Alyson L Dickson",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2142701830",
      "name": "Josh F Peterson",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2426726753",
      "name": "QiPing Feng",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2233545356",
      "name": "Dan M. Roden",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2105736763",
      "name": "C. Michael Stein",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2978130392",
      "name": "V. Eric Kerchberger",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2145238237",
      "name": "Bradley A Malin",
      "affiliations": [
        "Vanderbilt University",
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2989674414",
      "name": "Wei-Qi Wei",
      "affiliations": [
        "Vanderbilt University",
        "Vanderbilt University Medical Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1974660612",
    "https://openalex.org/W2803290558",
    "https://openalex.org/W4319081266",
    "https://openalex.org/W2113952938",
    "https://openalex.org/W3088156227",
    "https://openalex.org/W2991379615",
    "https://openalex.org/W4389279369",
    "https://openalex.org/W2991319174",
    "https://openalex.org/W4387500346",
    "https://openalex.org/W4385848332",
    "https://openalex.org/W4389205282",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W2027867013",
    "https://openalex.org/W2064337796",
    "https://openalex.org/W2105714645",
    "https://openalex.org/W2016104730",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4378194668",
    "https://openalex.org/W2044517432",
    "https://openalex.org/W1881549354",
    "https://openalex.org/W4310572401",
    "https://openalex.org/W2164777277"
  ],
  "abstract": "ABSTRACT Objectives Phenotyping is a core task in observational health research utilizing electronic health records (EHRs). Developing an accurate algorithm demands substantial input from domain experts, involving extensive literature review and evidence synthesis. This burdensome process limits scalability and delays knowledge discovery. We investigate the potential for leveraging large language models (LLMs) to enhance the efficiency of EHR phenotyping by generating high-quality algorithm drafts. Materials and Methods We prompted four LLMs—GPT-4 and GPT-3.5 of ChatGPT, Claude 2, and Bard—in October 2023, asking them to generate executable phenotyping algorithms in the form of SQL queries adhering to a common data model (CDM) for three phenotypes (i.e., type 2 diabetes mellitus, dementia, and hypothyroidism). Three phenotyping experts evaluated the returned algorithms across several critical metrics. We further implemented the top-rated algorithms and compared them against clinician-validated phenotyping algorithms from the Electronic Medical Records and Genomics (eMERGE) network. Results GPT-4 and GPT-3.5 exhibited significantly higher overall expert evaluation scores in instruction following, algorithmic logic, and SQL executability, when compared to Claude 2 and Bard. Although GPT-4 and GPT-3.5 effectively identified relevant clinical concepts, they exhibited immature capability in organizing phenotyping criteria with the proper logic, leading to phenotyping algorithms that were either excessively restrictive (with low recall) or overly broad (with low positive predictive values). Conclusion GPT versions 3.5 and 4 are capable of drafting phenotyping algorithms by identifying relevant clinical criteria aligned with a CDM. However, expertise in informatics and clinical experience is still required to assess and further refine generated algorithms.",
  "full_text": "Title: Large Language Models Facilitate the Generation of Electronic Health Record \nPhenotyping Algorithms  \n \n \n \nAuthors:  \n \nChao Yan\n1, PhD, Henry H. Ong1, PhD, Monika E. Grabowska1, MS, Matthew S. Krantz1, MD, \nWu-Chen Su1, MS, Alyson L. Dickson2, MA, Josh F. Peterson1,2, MD, MPH, QiPing Feng2, PhD, \nDan M. Roden1, MD, C. Michael Stein2, MD, V. Eric Kerchberger1,2, MD, Brad A. Malin1,3,4, PhD, \nWei-Qi Wei1,3, MD, PhD \n \nAffiliations: \n \n1Department of Biomedical Informatics, Vanderbilt University Medical Center, Nashville, TN \n2Department of Medicine, Vanderbilt University Medical Center, Nashville, TN  \n3Department of Computer Science, Vanderbilt University, Nashville, TN \n4Department of Biostatistics, Vanderbilt University Medical Center, Nashville, TN \n \n \nCorresponding author: \n \nWei-Qi Wei, MD, PhD \nDepartment of Biomedical Informatics & Computer Science \nVanderbilt University Medical Center, Vanderbilt University \nSuite 1500, 2525 West End Ave, Nashville, TN, 37203 \nEmail: wei-qi.wei@vumc.org \n \n \nKeywords: Phenotyping, Electronic health records, Large language models, ChatGPT \n \nWord count: 3504 \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nABSTRACT  \n \nObjectives: Phenotyping is a core task in observational health research utilizing electronic \nhealth records (EHRs). Developing an accurate algorithm typically demands substantial input \nfrom domain experts, involving extensive literature review and evidence synthesis. This \nburdensome process limits scalability and delays knowledge discovery. We investigate the \npotential for leveraging large language models (LLMs) to enhance the efficiency of EHR \nphenotyping by generating drafts of high-quality algorithms. \n \nMaterials and Methods: We prompted four LLMs—ChatGPT-4, ChatGPT-3.5, Claude 2, and \nBard—in October 2023, asking them to generate executable phenotyping algorithms in the form \nof SQL queries adhering to a common data model for three clinical phenotypes (i.e., type 2 \ndiabetes mellitus, dementia, and hypothyroidism). Three phenotyping experts evaluated the \nreturned algorithms across several critical metrics. We further implemented the top-rated \nalgorithms from each LLM and compared them against clinician-validated phenotyping \nalgorithms from the Electronic Medical Records and Genomics (eMERGE) network. \n \nResults: ChatGPT-4 and ChatGPT-3.5 exhibited significantly higher overall expert evaluation \nscores in instruction following, algorithmic logic, and SQL executability, when compared to \nClaude 2 and Bard. Although ChatGPT-4 and ChatGPT-3.5 effectively identified relevant clinical \nconcepts, they exhibited immature capability in organizing phenotyping criteria with the proper \nlogic, leading to phenotyping algorithms that were either excessively restrictive (with low recall) \nor overly broad (with low positive predictive values). \n \nConclusion: Both ChatGPT versions 3.5 and 4 demonstrate the capability to enhance EHR \nphenotyping efficiency by drafting algorithms of reasonable quality. However, the optimal \nperformance of these algorithms necessitates the involvement of domain experts. \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nINTRODUCTION \n \nElectronic health record (EHR) phenotyping, which involves creating algorithms to identify and \ncorrectly classify a patient’s observable characteristics by integrating complex clinical data, has \nbecome pivotal in observational health research1. Developing EHR phenotypes is an intricate \nand labor-intensive process that demands extensive expertise in both the clinical and \ninformatics domains2,3. While phenotyping includes the identification of individuals with specific \ncharacteristics, it also necessitates the selection of suitable controls for meaningful comparisons \nwith the identified cases\n4.  \n \nRule-based computable phenotyping algorithms rely on clinical experts to select specialized \ncriteria (e.g., diagnosis codes, medications, and laboratory values) likely to define a phenotype \nof interest. These algorithms are tested and tailored for more precise phenotype identification, \nresulting in better performance compared to general high-throughput methods\n5-8. However, the \niterative nature of this process often requires substantial literature review and discussions with \nclinical experts to generate a single phenotyping algorithm, thereby limiting the scalability of this \napproach in practice\n2. Furthermore, implementation of phenotyping algorithms by secondary \nsites requires additional informatics expertise, manual effort, and time to adapt the existing code \nto different databases and EHR systems.  \n \nRecently, large language models (LLMs) have demonstrated effectiveness in information \nextraction and summarization\n9, indicating a potential benefit in phenotyping by reducing the time \nrequired for literature review and synthesis during the phenotype generation process. In this \npreliminary report, we investigate the novel application of LLMs for generating computable \nphenotyping algorithms to assess whether such tools can effectively expedite EHR phenotype \ndevelopment. We appraised four LLMs—ChatGPT-4 10, ChatGPT-3.511, Claude 212, and PaLM \n2-powered Bard13—to generate phenotyping algorithms for three clinical phenotypes—type 2 \ndiabetes mellitus (T2DM), dementia, and hypothyroidism. We subsequently implemented the \ntop-rated algorithms as adjudicated by phenotyping experts using multiple critical metrics from \neach LLM and compared them against the clinician-validated phenotyping algorithms from the \nElectronic Medical Records and Genomics (eMERGE) network14,15. \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nMETHODS \n \nWe selected four LLMs in their default configurations to test their phenotyping algorithm \ngeneration capacity for three common clinical phenotypes. These LLMs were: ChatGPT-4 (by \nOpenAI)10, ChatGPT-3.5 (by OpenAI)11, Claude 2 (by Anthropic)12, and Bard (by Google, \npowered by PaLM 2)13. These models were chosen because of their widespread use, easy \naccessibility, extensive evaluation, robust computational capabilities, and proficiency in handling\nand generating lengthy texts—qualities crucial for sustainably supporting phenotyping tasks 16. \n \nThis pilot study specifically focused on three clinical phenotypes: T2DM\n17,18, dementia19, and \nhypothyroidism20,21. We chose these phenotypes because they have existing algorithms that \nhave undergone extensive validation processes and demonstrated highly accurate and \nconsistent performances with well-documented results. Data collection via web-based \ninteractions with the LLMs occurred in October 2023, with subsequent data analysis completed \nin November 2023. This study was approved by the institutional review boards at Vanderbilt \nUniversity Medical Center (IRB #: 201434). \n \nFigure 1 illustrates an overview of the study pipeline, which was comprised of two main \ncomponents—prompting (steps 1-3, highlighted in pink) and evaluating (steps 4-\n9, highlighted in \nblue) LLMs. \n \n \nFigure 1. An architectural overview of the study pipeline. \n \nPrompting large language models to generate phenotyping algorithms \ng \n in \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \n \nWe prompted the LLMs to generate executable SQL queries to identify phenotype cases from \nstructured EHR data organized according to the Observational Medical Outcomes Partnership \n(OMOP) Common Data Model (CDM), a standard EHR data framework that enables efficient \ndata analysis and sharing across institutions. To achieve this, we designed two distinct \nprompting strategies, hereafter termed \nα -prompting and β -prompting. \n \nThe α -prompting strategy (steps 1 and 2 in Figure 1) had two steps. The first step focused on \nobtaining a pseudocode version of the phenotyping algorithm (referred to as a pseudo-\nphenotyping algorithm), which emphasized identifying and integrating critical phenotyping \ncriteria, as well as determining the strategy for combining these criteria. We utilized the chain-of-\nthought (CoT) prompting strategy\n24, an effective method for directing LLMs through a series of \nreasoning steps to resolve complex problems like humans. Specifically, we framed our \ninstruction to guide reasoning as follows: “Let’s think step by step: 1. List the critical criteria to \nconsider. 2. Determine how these criteria should be combined. 3. Derive the final algorithm” \n(Supplemental Table 1). Additionally, multiple detailed instructions were specified so that the \nproduced pseudo-phenotyping algorithm adhered to the OMOP concepts, including diagnosis \ncodes (in ICD-9-CM and ICD-10-CM), symptoms, procedures, laboratory tests, and medications \n(both generic and brand names). We also mandated that the pseudo-phenotyping algorithm \nmaintain a style consistent with the SQL logic, to facilitate generation of the SQL query in the \nsecond step.  \n \nUsing the response of an LLM in the initial step, the second step involved converting the \npseudo-phenotyping algorithm into an executable SQL query (referred to as an SQL-formatted \nphenotyping algorithm) for implementation in an OMOP CDM-based EHR database for \nsubsequent validation (Supplemental Table 1). Due to the probabilistic nature of LLMs, \nvariations in the generated phenotyping algorithms are guaranteed. Consequently, we executed \nα -prompting five times independently for each phenotype and for each LLM to account for \nresponse variability. \n \nThe β -prompting strategy (step 3 in Figure 1) was designed to first present the LLM with the five \nSQL-formatted phenotyping algorithms generated from the α -prompting strategy and then \ninstruct the LLM to assess the quality of these algorithms and generate an improved one \n(Supplemental Table 1). This strategy, proven to effectively mitigate hallucinations25-27, \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nleverages an LLM’s ability to evaluate scientific texts based on the extensive knowledge \nencoded during model pre-training. As a result, the LLM can produce an updated version of the \nphenotyping algorithm through analytical reflections. We also deployed the CoT strategy in β -\nprompting, which involved initially identifying the correct, incorrect, and missing criteria for each \npreviously generated algorithm. The β -prompting strategy was executed in an independent \nsession of the LLM (distinct from and subsequent to those used for α -prompting) and was \nexecuted only once. \n \nEvaluating the quality of LLM-generated phenotyping algorithms  \n \nWe then performed a comprehensive analysis, encompassing both qualitative and quantitative \nassessments, to evaluate the efficacy of the phenotyping algorithms generated by the four \ndifferent LLMs on the three diseases of focus as described above. For the qualitative analysis, \nthree experts in EHR phenotyping and clinical medicine (W.Q.W., M.E.G., and V.E.K.) \nindependently reviewed and rated the LLM-generated SQL-formatted phenotyping algorithms in \na blind manner. We further compared the concepts utilized in these algorithms with those found \nin clinician-validated phenotyping algorithms developed by the eMERGE network\n14,15. For \nquantitative assessment, we implemented the top-rated SQL-formatted phenotyping algorithms \nusing EHR data from Vanderbilt University Medical Center (VUMC) and assessed their \nperformance against established eMERGE algorithms\n17-21.  \n     \nExpert assessment Each of the three experts conducted independent reviews for every SQL-\nformatted phenotyping algorithm (4 LLMs, 3 phenotypes, and 2 strategies, for 24 algorithms in \ntotal). The evaluation focused on three dimensions: 1) adherence to instructions, which \nassessed how well the LLM conformed to predefined formatting rules; 2) the generation of \nproficient phenotyping algorithms based on knowledge, which evaluated the LLM's ability to \nsynthesize and organize phenotyping-related information effectively; and 3) presentation in \nexecutable SQL format, which measured the potential of an LLM to reduce the labor-intensive \nhuman efforts required for EHR implementation and validation. Detailed guidelines for expert \nevaluation can be found in Supplemental Table 2. Experts assigned categorical scores (\"Good \n[3],\" \"Medium [2],\" or \"Poor [1]\") for each axis based on predefined criteria, providing \njustifications accordingly. Interrater reliability was assessed using the weighted Cohen’s Kappa \nscore\n28. We compared LLMs’ rated scores using the Wilcoxon signed-rank test29 with a \nsignificance level of 0.05.  \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nComparison of concept coverage with eMERGE phenotyping algorithms This analysis \nprovided a comprehensive comparison of the concepts within phenotyping algorithms generated \nby LLMs and established EHR algorithms. We systematically reviewed and compared all the \nconcepts employed in the algorithms for diagnoses, laboratory tests, procedures, medications, \nsymptoms, and exclusions, and then summarized the noteworthy findings. \n \nImplementation of LLM-generated algorithms and performance evaluation We deployed \nthe highest-rated phenotyping algorithms for each phenotype in a research cohort at VUMC \n(n=84,821). This cohort, extensively utilized in phenotyping research, has been a significant \nresource for phenotypic studies30,31. As a benchmark, we implemented three eMERGE \nalgorithms updated with current ICD-10-CM codes31 to identify phenotype cases and controls. \nThe cases and controls identified by the eMERGE algorithms served as a reference standard to \nassess the effectiveness of the LLM-generated algorithms. In our subsequent analysis, we \nexcluded patients not categorized as either case or control by the eMERGE algorithm, as their \ndata did not meet the criteria for either situation. Each of the top-rated LLM-generated \nalgorithms required some modifications to be executable in a cloud-based platform to securely \nquery VUMC's research clinical databases that follow the OMOP CDM. We limited changes to \ntechnical domain knowledge, as opposed to clinical domain knowledge. For example, we edited \nthe database names, but did not edit drug names or ICD codes. \n \nWe used the following metrics for evaluation: 1) positive predictive values (PPV), defined as the \nnumber of cases mutually identified by the eMERGE algorithm and an LLM over the total \nnumber of identified cases by the LLM; 2) recall, defined as the number of cases mutually \nidentified by the eMERGE algorithm and an LLM over the total number of cases identified by the \neMERGE algorithm; and 3) false positive rate (FPR), defined as the number of patients \nidentified as cases by an LLM but identified as controls by the eMERGE algorithm (false \npositives) over the sum of false positives and number of cases identified by the eMERGE \nalgorithm.  \n \n \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nRESULTS \n \nExpert assessments \n \nThe average interrater reliability was 0.59 [0.50-0.68], indicating a moderate to substantial \nagreement among experts, considering the categorical nature of the data (rather than \ndichotomous) and the variations in scoring criteria among different experts32.  \n \nBy mapping the experts’ assessments of “Good”, “Medium”, or “Poor” to numerical scores of 3, \n2, and 1, respectively, ChatGPT-4 (mean [95% confidence interval]: 2.57 [2.40-2.75]) and \nChatGPT-3.5 (2.43 [2.25-2.60]) exhibited significantly higher overall expert evaluation scores \nthan Claude 2 (1.91 [1.68-2.13]) and Bard (1.20 [1.09-1.31]) (Figure 2A). ChatGPT-4 marginally \noutperformed ChatGPT-3.5, though the differences were not statistically significant. Moreover, \nthe β -prompting strategy did not significantly differ from the α -prompting, according to experts’ \nevaluation (Figure 2B). Furthermore, experts assigned higher scores to LLMs for their \neffectiveness in generating phenotyping algorithms for T2DM and hypothyroidism compared to \ndementia (Figure 2C). The radar plot shown in Figure 2D displays the average scores for each \ninvolved LLM across the three axes of evaluation. There are two key findings. First, ChatGPT-4 \nand ChatGPT-3.5 were rated consistently better than Claude 2 and Bard in following \ninstructions, algorithmic logic, and SQL executability. Second, ChatGPT-4 was considered to be \non par with ChatGPT-3.5 in its ability to follow instructions and SQL executability, yet it \nsurpassed ChatGPT-3.5 in its algorithmic logic. Considering these findings, the remainder of our \nresults focused exclusively on ChatGPT-4 and ChatGPT-3.5. \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \n \nFigure 2. A comparative analysis based on expert evaluations focusing on A) four large \nlanguage models, B) two prompting strategies, C) three phenotypes, and D) three \nindividual evaluation axes. Numeric scores of 3, 2, and 1 correspond to expert assessments \nof “Good”, “Medium”, and “Poor”, respectively. ***, **, and * denote p<0.001, p<0.01, and \np<0.05, respectively. ns=not significant. \n \nComparison with eMERGE phenotyping algorithms \n \nComponents Table 1 summarizes the clinical concepts identified by the eMERGE phenotyping \nalgorithms and LLM-produced algorithms. A full comparison of concepts can be found in \nSupplemental Table 3. Given that the phenotyping algorithms produced by both ChatGPT-4 and \nChatGPT-3.5 from the β -prompting strategy were rated similarly to those from the α -prompting \nstrategy, we focused further analyses on the β -prompting results.   \n \nThere are several notable observations. For the T2DM phenotyping algorithm, ChatGPT-4 and \nChatGPT-3.5 identified relevant diagnosis codes (both ICD-9-CM and ICD-10-CM), lab tests \n(hemoglobin A1c, fasting blood glucose), and two generic medications (metformin and glipizide) \nthat were also used in the eMERGE phenotyping algorithm. The eMERGE phenotyping \nalgorithm also included medications not identified by either ChatGPT-4 or ChatGPT-3.5 (n=27). \nOnly ChatGPT-4 and ChatGPT-3.5 included symptoms; both eMERGE and ChatGPT-4 \nprovided exclusionary criteria (ICD codes). Notably, the ChatGPT-3.5 model incorrectly included \nICD-9-CM codes for type 1 diabetes and applied an incorrect cutoff value for fasting blood \nglucose (>6.5 mg/dL instead of >125 mg/dL).  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \n \nFor the dementia phenotyping algorithm, ChatGPT-4 and ChatGPT-3.5 included relevant \ndiagnosis codes (ICD-9-CM and ICD-10-CM, as well as one ICD-10). While there were several \noverlapping diagnosis codes between each pair of phenotyping algorithms, no diagnosis codes \nwere shared across all three algorithms. ChatGPT-4 and ChatGPT-3.5 included symptoms \npotentially related to dementia while the eMERGE algorithm did not. All three phenotyping \nalgorithms shared four medications (two generic: donepezil and memantine, and two brand \nnames: Aricept and Namenda), although the eMERGE algorithm specified additional \nmedications that did not appear in either LLM-generated algorithm (n=7). Only the ChatGPT-4 \nphenotyping algorithm included exclusion criteria (e.g., vitamin B12 level).  \n \nFor the hypothyroidism phenotyping algorithm, ChatGPT-4 and ChatGPT-3.5 identified relevant \ndiagnosis codes (both ICD-9-CM and ICD-10-CM), lab tests (thyroid stimulating hormone), and \nmedications (generic levothyroxine and brand name Synthroid) used in the eMERGE algorithm. \nOnly the eMERGE algorithm used thyroid autoantibodies (e.g., thyroid antiperoxidase). Notably, \nthe ChatGPT-4 algorithm specified the largest number of medications (n=15), including 9 \nmedications that were not included in either the eMERGE algorithm or the ChatGPT-3.5 \nalgorithm. Once again, only ChatGPT-4 and ChatGPT-3.5 included symptoms. The eMERGE \nalgorithm included 113 exclusions, including 18 ICD codes, 79 CPT codes, and 16 medications, \nwhile ChatGPT-4 and ChatGPT-3.5 specified only 2 and 0 exclusions, respectively. \n \n \nTable 1. Shared and unshared concepts from the eMERGE, ChatGPT-4, and ChatGPT-3.5 \nphenotyping algorithms for T2DM, dementia, and hypothyroidism. Darker blue shading \nrepresents a relatively higher count of unshared concepts when comparing the algorithms. \nOrange shading represents inaccurate concepts or laboratory cutoffs.  \nType 2 diabetes mellitus \nConcept Shared Unshared concept count  \n(example) \nAlgorithm All eMERGE ChatGPT-4 (β ) ChatGPT-3.5 (β ) \nDiagnoses 250.*0, 250.*2, E11.* 2 \n(O24.11) 0 31a \n(250.01) \nLab tests or \nprocedures HgbA1cb, Fasting BGc \n1 Lab \n (Random BG >200)d \n1 Lab \n (Oral GTT >200)d \n1 Lab \n(BMI ≥ 25)d \nMedications Metformin, Glipizide 34 \n(Exenatide)d \n7 \n(Glucophage) \n1 \n(Sitagliptin) \nSymptoms None 0 3 \n(Polyuria) \n3 \n(Polydipsia) \nExclusion by \nType None 9 ICD \n(250.*1) \n3 ICD \n(250.*3) 0 \nDementia \nConcept Shared Unshared concept count  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \n(example) \nAlgorithm All eMERGE ChatGPT-4 (β ) ChatGPT-3.5 (β ) \nDiagnoses None 45 \n(290.0)d \n6 20 \nLab tests or \nprocedures None 0 4 Procedures \n (MMSE)d \n3 Procedures \n(MRI scan)d \nMedications Donepezil, Aricept, \nMemantine, Namenda \n11 \n(Cognex)d \n4 \n(Galantamine) 0 \nSymptoms None 0 3 \n(Impaired reasoning)d \n5 \n(Cognitive impairment)d \nExclusion by \nType None 0 3 Labs \n(B12 level)d \n0 \nHypothyroidism \nConcept Shared Unshared concept count  \n(example) \nAlgorithm All eMERGE ChatGPT-4 (β ) ChatGPT-3.5 (β ) \nDiagnoses 244.9, E03.8, E03.9 18  \n(244)d \n7 \n(244.1)d \n6 \n(E03.5)d \nLab tests or \nprocedures TSHe \n4 Labs  \n(Anti-TPO)d \n1 Lab \n(Serum free thyroxine) 0 \nMedications Levothyroxine, Synthroid 9 \n(Liothyronine)d \n15 \n(Amiodarone)d,f \n0 \nSymptoms None 0 8 \n(Hair loss)d \n8 \n(Constipation)d \nExclusion by \nType None \n18 ICD \n(193*)d \n79 CPT \n(60240)d \n16 Medications \n(Amiodarone)d \n2 Proceduresg \n(Thyroid surgery) \n \n0 \nAbbreviations: BG, blood glucose; BMI, body mass index; CPT, Current Procedural Terminology; GTT, glucose tolerance test; \nHgbA1c, hemoglobin A1c; ICD, International Classification of Diseases; MMSE, Mini-Mental State Exam; MRI, magnetic resonance \nimaging; TPO, thyroperoxidase.  \n \n \nImplementation and evaluation in VUMC Along with the eMERGE algorithms, we successfully \ndeployed all algorithms generated by ChatGPT-4 and ChatGPT-3.5 from the β -prompting \nstrategy except the T2DM algorithm generated by ChatGPT-3.5 (Table 2). The failure of this \nphenotyping algorithm stemmed from its restrictive logic which accumulated LEFT JOINs across \nvarious tables, including the Person, Condition_Occurrence, Measurement, Drug_Exposure, \nand Observation tables. The algorithm required a LEFT JOIN on a list of people who had a \nrecord of symptoms in the Observation table (Polyuria, Polydipsia, Unexplained weight loss) \nand had a value of zero in the “value_as_concept_id” column. A value of zero did not exist for \nthese symptoms, therefore the algorithm could not identify any individuals who met this criterion \nfor T2DM.  \n \nThe case (and control) prevalences for the eMERGE algorithms in our population were 10.9% \n(28.0%), 3.5% (91.5%), and 2.4% (30.4%) for T2DM, dementia, and hypothyroidism, \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nrespectively. The ChatGPT-4 generated dementia algorithm achieved the highest PPV of 96.3% \nat the expense of a low recall (24.4%). This result was primarily attributable to the relatively \nrestrictive inclusion criteria, which required medication prescription coupled with the co-\noccurrence of either diagnosis codes, symptoms, or cognitive assessment tests or procedures. \nThe ChatGPT-4 generated algorithms for T2DM and hypothyroidism, as well as the ChatGPT-\n3.5 generated algorithm for hypothyroidism, achieved high recall but at the cost of lower PPV. In \ncontrast, the ChatGPT-3.5 generated algorithm for dementia achieved balanced PPV and recall.  \n \nTable 2. Performance of the phenotyping algorithms generated by ChatGPT-4 and \nChatGPT-3.5 from the β -prompting strategy when applied to VUMC data, as measured \nagainst clinician-validated algorithms for the eMERGE phenotype cases and controls. \nDisease eMERGE  ChatGPT-4 ChatGPT-3.5 \n True \ncases \nTrue \ncontrols TP FP PPV Recall FPR TP FP PPV Recall FPR \nT2DM 9,293 23,754 8,978 578 53.3% 96.6% 2.4% 0 0 - 0.0% 0.0% \nDementia 2,985 77,575 729 11 96.3% 24.4% 0.01% 2,388 583 71.4% 80.0% 7.5% \nHypo-\nthyroidism 2,030 25,760 2,029 258 9.6% 99.9% 1.0% 2,029 1,065 10.7% 99.9% 4.1% \nTP=true positive; FP=false positive; FDR=false discovery rate; T2DM=type 2 diabetes mellitus. \n \n \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nDISCUSSION \n \nEHR phenotyping is a critical area of modern observational clinical research, yet it commonly \ndemands substantial resources. In this study, we explored the effectiveness of LLMs in creating \npreliminary versions of computable phenotyping algorithms, with the ultimate goal of \nstreamlining the EHR phenotyping process.  \n \nNot all LLMs we tested were well-suited for phenotyping. ChatGPT-4 and ChatGPT-3.5 \nsignificantly outperformed both Claude 2 and Bard in their ability to generate executable and \naccurately SQL-formatted phenotyping algorithms. One of the reasons for this discrepancy was \nClaude 2's tendency to represent concepts using numerical concept codes, without specifying \nwhat clinical criteria these concept codes were intended to capture. Five of the algorithms \ngenerated by Bard did not follow the OMOP CDM. Four of these algorithms referenced columns \nthat do not exist in the OMOP CDM. These corresponded to “patient_id” for both prompting \nstrategies of dementia, “observation_fact” and “measurement_fact” for the \nα -prompting \nstrategies of T2DM, and “occurrence_age” for the β -prompting strategy of T2DM. Additionally, \ntwo of the algorithms searched for concepts in the wrong tables. The α -prompting algorithm for \ndementia attempted to use the “person” table to extract all concepts, including diagnosis codes \nand medications, which would not be found in this table, whereas the β -prompting algorithm for \nhypothyroidism looked for signs and symptoms in the Measurement table. Given the poor \nperformance of the phenotyping algorithms generated by Claude 2 and Bard, we focused the \nremaining analysis on phenotyping algorithms generated by ChatGPT-4 and ChatGPT-3.5.  \n \nBoth ChatGPT models were able to follow instructions and identify relevant concepts for the \nthree selected phenotypes. The phenotyping algorithms generated by these models contained \nreasonably accurate diagnosis codes, related lab tests, and key medications. The concepts \nidentified largely overlapped with the ones used by domain experts. We found that ChatGPT-4 \ngenerally demonstrated slightly superior performance compared to ChatGPT-3.5 by identifying \nmore medications and providing more appropriate thresholds for lab values ( Table 1). Moreover, \nthe ChatGPT-generated algorithms were able to identify additional potentially useful criteria, \nincluding a variety of symptoms and clinical signs for each phenotype, which have not usually \nbeen incorporated in any eMERGE algorithms.  \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nChatGPT generated phenotyping algorithms with incorrect criteria that needed manual \ncorrections to ensure their functionality. For example, both ChatGPT-4 and ChatGPT-3.5 \nerroneously considered the ICD-10 code F00 as a relevant diagnosis code for dementia, \nwhereas our prompts specifically required ICD-10-CM codes. Additionally, ChatGPT \noccasionally selected inappropriately broad ICD-9-CM codes, such as 250* for T2DM, \ninadvertently encompassing diagnoses related to other types of diabetes. Also, ChatGPT \nmissed some key ICD codes and medications. For example, while the query correctly identified \npatients with ICD-10-CM code G30 as dementia cases, ChatGPT overlooked patients with more \nspecific codes such as G30.0, G30.1, G30.8, and G30.9. The ChatGPT algorithms generally \ngenerated a shorter list of medications compared to their corresponding eMERGE algorithms, \nwith the exception of the ChatGPT-4 algorithm for hypothyroidism. In this case, ChatGPT-4 \ninterpreted the hypothyroidism phenotype more broadly to include not only endogenous causes \nof hypothyroidism (as in the eMERGE algorithm) but also exogenous causes (not included in \nthe eMERGE algorithm). As a result, the medications specified by the ChatGPT-4 \nhypothyroidism algorithm included both drugs used for treatment of hypothyroidism and drugs \nwith potential to cause hypothyroidism (i.e., lithium, amiodarone, Lithobid, Cordarone, \nNexterone, Pacerone). Moreover, certain thresholds set for lab values were inaccurate, such as \n“fasting blood glucose \n≥ 6.5” for T2DM.  \n \nAs noted above, compared to the eMERGE algorithms, ChatGPT introduced some new \nconcepts, including signs and symptoms such as fatigue, cold intolerance, and weight gain. \nMany of these concepts are infrequently used by domain experts in algorithm generation due to \ntheir low specificity. Including these low-specificity concepts might not substantially enhance the \nrecall of the algorithms and could potentially diminish the algorithm’s PPV. Despite this, \nChatGPT showed promise in identifying noteworthy concepts, such as the Mini-Mental State \nExamination and Montreal Cognitive Assessment for dementia. \n \nFinally, the LLMs exhibited immature capability in organizing phenotyping criteria with the \nproper logic. The SQL queries generated by ChatGPT were predominantly characterized by a \nsingle logical operator (AND or OR), resulting in phenotyping algorithms that were either \nexcessively restrictive or overly broad.  \n \n \nLimitations and future work \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nSeveral limitations need to be highlighted as potential opportunities for future improvement. First, \nthis pilot study was limited in scope and did not investigate all the possible tools, options, and \ncapabilities of the LLMs listed here. We are reporting initial observations from using the most \nbasic and widely accessible version of these LLMs, i.e., the chat box interface with default \nsettings available through the websites of each of the LLMs. Using an API would allow for \nselection of additional parameters, which may affect performance. Second, as a pilot study, this \nresearch focused on prompting LLMs to generate algorithms for identifying phenotype cases. \nThe capability of LLMs in generating algorithms to identify controls also needs to be evaluated. \nThird, we tested solely on proprietary models and their default configurations. It is important to \nassess both proprietary models and the leading open-source models (e.g., Llama 2), especially \nwhen they are enhanced with fine-tuning capabilities and knowledge integration. Fourth, our \ndesign of prompts did not consider the further optimization of the execution efficiency of the \nSQL queries. Consequently, LLMs often produced SQL queries with suboptimal query \nstructures, leading to slow execution in our database and additional human efforts to refine the \nquery. Fifth, this study only considered phenotypes for three common diseases. Phenotyping \nrare diseases may present different challenges particularly when there is limited relevant online \ncontent for a particular disease. Sixth, the consistency of responses generated by ChatGPT \nvaries over time, warranting consideration for potential future investigations. New LLMs are \nbeing rapidly deployed, and our future efforts will involve exploring alternative advanced models, \nsuch as ResearchGPT and Google Gemini. Additionally, we will be delving into more refined \ncontrol and customization in the generation process through prompt engineering to achieve \ndesired performance levels. \n \n \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nCONCLUSION \n \nChatGPT models are capable of producing phenotyping algorithm drafts that align with a \nstandard CDM and can be executed with reasonable modifications. These models excel in \nidentifying relevant clinical concepts that can serve as valuable initial components for defining \nphenotypes. Nevertheless, LLM-generated phenotyping algorithms still necessitate manual \nmodifications using clinical and informatics knowledge. \n \n \n \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nAUTHOR CONTRIBUTIONS \n \nW.Q.W. conceived and supervised this study. C.Y. designed the interactions with the large \nlanguage models (LLM), collected data, and analyzed the results. W.Q.W. and H.H.O. \nimplemented the algorithms produced by LLMs. M.E.G., V.E.K., and W.Q.W. performed expert \nreview for the produced algorithms. M.E.G., C.Y., H.H.O., V.E.K., and M.S.K. drafted the paper. \nW.Q.W., B.A.M., W.C.S., A.L.D., Q.P.F., and C.M.S. critically revised the paper. D.M.R. and \nJ.F.P. reviewed the paper and provided important intellectual content. All authors approved this \nstudy. \n \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nFUNDING \n \nThis work was supported by R01GM139891, R01AG069900, F30AG080885, T32GM007347, \nK01HL157755, and U01HG011181. \n \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nCONFLICTS OF INTEREST \nAll authors have no competing interests to declare. \n \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nCODE AVAILABILITY \n \nSource code for results and LLM-generated SQL-formatted algorithms are shared at \nhttps://github.com/The-Wei-Lab/LLM-Phenotyping-2024. \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nREFERENCE  \n \n1. Wei WQ, Denny JC. Extracting research-quality phenotypes from electronic health records to \nsupport precision medicine. Genome Med. 2015;7(1):41.  \n2. Banda JM, Seneviratne M, Hernandez-Boussard T, Shah NH. Advances in electronic \nphenotyping: From rule-based definitions to machine learning models. Annu Rev Biomed Data \nSci. 2018;1(1):53-68.  \n3. Pacheco JA, Rasmussen LV, Wiley K Jr, et al. Evaluation of the portability of computable \nphenotypes with natural language processing in the eMERGE network. Sci Rep. \n2023;13(1):1971.  \n4. Newton KM, Peissig PL, Kho AN, et al. Validation of electronic medical record-based \nphenotyping algorithms: results and lessons learned from the eMERGE network. J Am Med \nInform Assoc. 2013;20(e1):e147-54.  \n5. Zheng NS, Feng Q, Kerchberger VE, et al. PheMap: a multi-resource knowledge base for \nhigh-throughput phenotyping within electronic health records. J Am Med Inform Assoc. \n2020;27(11):1675-1687.  \n6. Wu P, Gifford A, Meng X, et al. Mapping ICD-10 and ICD-10-CM codes to phecodes: \nWorkflow development and initial evaluation. JMIR Med Inform. 2019;7(4):e14325.  \n7. Grabowska ME, Van Driest SL, Robinson JR, et al. Developing and evaluating pediatric \nphecodes (Peds-phecodes) for high-throughput phenotyping using electronic health records. \nmedRxiv. Published online 2023. doi:10.1101/2023.08.22.23294435. \n8. Zhang Y, Cai T, Yu S, et al. High-throughput phenotyping with electronic medical record data \nusing a common semi-supervised approach (PheCAP). Nat Protoc. 2019;14(12):3426-3444.  \n9. Clusmann J, Kolbinger FR, Muti HS, et al. The future landscape of large language models in \nmedicine. Commun Med (Lond). 2023;3(1):1-8.  \n10. OpenAI. GPT-4 Technical Report. Preprint \nat https://doi.org/10.48550/arXiv.2303.08774 (2023).  \n11. OpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt (2022).  \n12. Introducing Claude. Anthropic https://www.anthropic.com/index/introducing-claude (2023).  \n13. Pichai, S. Google AI updates: Bard and new AI features in \nSearch. https://blog.google/technology/ai/bard-google-ai-search-updates/ (2023).  \n14. Gottesman O, Kuivaniemi H, Tromp G, Faucett WA, Li R, Manolio TA, Sanderson SC, \nKannry J, Zinberg R, Basford MA, Brilliant M. The electronic medical records and genomics \n(eMERGE) network: past, present, and future. Genetics in Medicine. 2013 Oct;15(10):761-71.  \n15. McCarty CA, Chisholm RL, Chute CG, Kullo IJ, Jarvik GP, Larson EB, Li R, Masys DR, \nRitchie MD, Roden DM, Struewing JP. The eMERGE Network: a consortium of biorepositories \nlinked to electronic medical records data for conducting genomic studies. BMC medical \ngenomics. 2011 Dec;4:1-1.  \n16. Omiye JA, Lester JC, Spichak S, Rotemberg V, Daneshjou R. Large language models \npropagate race-based medicine. NPJ Digit Med. 2023;6(1):195.  \n17. Kho AN, Hayes MG, Rasmussen-Torvik L, Pacheco JA, Thompson WK, Armstrong LL, et al. \nUse of diverse electronic medical record systems to identify genetic risk for type 2 diabetes \nwithin a genome-wide association study. J Am Med Inform Assoc. 2012;19(2):212–8.  \n18. Jennifer Pacheco and Will Thompson. Northwestern University. Type 2 diabetes mellitus. \nPhekb.org. 2012. Available from: https://phekb.org/phenotype/type-2-diabetes-mellitus. \n19. Chris Carlson. Group Health Cooperative. Dementia. PheKB.org. 2012. Available from: \nhttps://phekb.org/phenotype/10. \n20. Josh Denny. Group Health Cooperative, Marshfield Clinic Research Foundation, Mayo \nMedical School College of Medicine, Northwestern University, Vanderbilt University. \nHypothyroidism. PheKB.org. 2012. Available from: https://phekb.org/phenotype/14. \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \n21. Denny JC, Crawford DC, Ritchie MD, Bielinski SJ, Basford MA, Bradford Y, et al. Variants \nnear FOXE1 are associated with hypothyroidism and other thyroid conditions: using electronic \nmedical records for genome- and phenome-wide studies. Am J Hum Genet. 2011;89(4):529–42.  \n22. OMOP Common Data Model. Github.io. https://ohdsi.github.io/CommonDataModel/. \n23. OHDSI – observational health data sciences and informatics. Ohdsi.org. \nhttps://www.ohdsi.org/. \n24. Wei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, Le QV, Zhou D. Chain-of-thought \nprompting elicits reasoning in large language models. Proc of the 37\nth Conference on Neural \nInformation Processing Systems. 2022 Dec 6;35:24824-37.  \n25. Madaan A, Tandon N, Gupta P, Hallinan S, Gao L, Wiegreffe S, et al. Self-refine: Iterative \nrefinement with Self-feedback [Internet]. arXiv [cs.CL]. 2023. Available from: \nhttp://arxiv.org/abs/2303.17651. \n26. Pan L, Saxon M, Xu W, Nathani D, Wang X, Wang WY. Automatically Correcting Large \nLanguage Models: Surveying the landscape of diverse self-correction strategies [Internet]. arXiv \n[cs.CL]. 2023. Available from: http://arxiv.org/abs/2308.03188. \n27. Samwald M, Praas R, Hebenstreit K. Towards unified objectives for self-reflective AI. SSRN \nElectron J [Internet]. 2023; Available from: http://dx.doi.org/10.2139/ssrn.4446991. \n28. Vanbelle S. A new interpretation of the weighted kappa coefficients. Psychometrika. 2016 \nJun;81(2):399-410.  \n29. Woolson RF. Wilcoxon signed\n‐ rank test. Wiley encyclopedia of clinical trials. 2007 Mar 9:1-3.  \n30. Wan NC, Yaqoob AA, Ong HH, Zhao J, Wei W-Q. Evaluating resources composing the \nPheMAP knowledge base to enhance high-throughput phenotyping. J Am Med Inform Assoc. \n2023;30(3):456–65.  \n31. Zheng NS, Feng Q, Kerchberger VE, Zhao J, Edwards TL, Cox NJ, et al. PheMap: a multi-\nresource knowledge base for high-throughput phenotyping within electronic health records. J \nAm Med Inform Assoc. 2020;27(11):1675–87.  \n32. Landis JR, Koch GG. The measurement of observer agreement for categorical data. \nBiometrics. 1977;33(1):159–74. \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \nFIGURE LEGENDS \n \nFigure 1. An architectural overview of the study pipeline. \n \nFigure 2. A comparative analysis based on expert evaluations focusing on A) four large \nlanguage models, B) two prompting strategies, C) three phenotypes, and D) three \nindividual evaluation axes. Numeric scores of 3, 2, and 1 correspond to expert assessments \nof “Good”, “Medium”, and “Poor”, respectively. ***, **, and * denote p<0.001, p<0.01, and \np<0.05, respectively. ns=not significant. \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 19, 2023. ; https://doi.org/10.1101/2023.12.19.23300230doi: medRxiv preprint ",
  "topic": "Electronic health record",
  "concepts": [
    {
      "name": "Electronic health record",
      "score": 0.6280891299247742
    },
    {
      "name": "Computer science",
      "score": 0.5618856549263
    },
    {
      "name": "Algorithm",
      "score": 0.40085747838020325
    },
    {
      "name": "Health care",
      "score": 0.17897295951843262
    },
    {
      "name": "Political science",
      "score": 0.16537341475486755
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I901861585",
      "name": "Vanderbilt University Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I200719446",
      "name": "Vanderbilt University",
      "country": "US"
    }
  ],
  "cited_by": 3
}