{
    "title": "Transformers for Limit Order Books",
    "url": "https://openalex.org/W3008728200",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287464999",
            "name": "Wallbridge, James",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1969852690",
        "https://openalex.org/W2896102935",
        "https://openalex.org/W2147800946",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2885054548",
        "https://openalex.org/W2749587125",
        "https://openalex.org/W2982316857",
        "https://openalex.org/W2946567085",
        "https://openalex.org/W2963048551",
        "https://openalex.org/W2734777338",
        "https://openalex.org/W2995575179",
        "https://openalex.org/W2962833798",
        "https://openalex.org/W2949382160",
        "https://openalex.org/W2624385633",
        "https://openalex.org/W2604847698",
        "https://openalex.org/W20141250",
        "https://openalex.org/W2907502844",
        "https://openalex.org/W2963532813",
        "https://openalex.org/W2174492417",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W3101887461",
        "https://openalex.org/W2995405959",
        "https://openalex.org/W2963122061",
        "https://openalex.org/W3121195520",
        "https://openalex.org/W2890096158",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W1689711448",
        "https://openalex.org/W2970631142",
        "https://openalex.org/W2970401203",
        "https://openalex.org/W2952042565",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W2766355270",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2767299446",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W2116341502",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2805516822"
    ],
    "abstract": "We introduce a new deep learning architecture for predicting price movements from limit order books. This architecture uses a causal convolutional network for feature extraction in combination with masked self-attention to update features based on relevant contextual information. This architecture is shown to significantly outperform existing architectures such as those using convolutional networks (CNN) and Long-Short Term Memory (LSTM) establishing a new state-of-the-art benchmark for the FI-2010 dataset.",
    "full_text": "Transformers for limit order books\nJames Wallbridge∗\nMarch 3, 2020\nAbstract\nWe introduce a new deep learning architecture for predicting price movements from\nlimit order books. This architecture uses a causal convolutional network for feature ex-\ntraction in combination with masked self-attention to update features based on relevant\ncontextual information. This architecture is shown to signiﬁcantly outperform existing\narchitectures such as those using convolutional networks (CNN) and Long-Short Term\nMemory (LSTM) establishing a new state-of-the-art benchmark for the FI-2010 dataset.\nContents\n1 Introduction 1\n2 Experiments 4\n3 Architecture 5\n4 Results 8\n5 Discussion 10\nA Training curves 15\nB Attention distributions 15\n1 Introduction\nUnderstanding high-frequency market micro-structure in time-series data such as limit or-\nder books (LOB) is complicated by a large number of factors including high-dimensionality,\n∗Correspondence to james.wallbridge@gmail.com\n1\narXiv:2003.00130v1  [q-fin.CP]  29 Feb 2020\ntrends based on supply and demand, order creation and deletion around price jumps and\nthe overwhelming relative percentage of order cancellations. It makes sense in this in-\nherently noisy environment to take an agnostic approach to the underlying mechanisms\ninducing this behavior and construct a network which learns to uncover the relevant\nfeatures from raw data. This removes the bias contained in models using hand-crafted\nfeatures and other market assumptions such as those in autoregressive models VAR [42]\nand ARIMA [1].\nArguably the most successful architecture used to extract features is the convolutional\nneural network [20] which makes use of translation equivariance, present in many domains\nincluding time-series applications. For time-series however, further inductive biases prove\nto be beneﬁcial. Convolutional neural networks with a causal temporal bias were intro-\nduced in [24] to encode long-range temporal dependencies in raw audio signals. Here\nconvolutions are replaced by dilated causal convolutions controlled by a dilation rate.\nThe dilation rate is the number of input values skipped by the ﬁlter, thereby allowing the\nnetwork to act with a larger receptive ﬁeld. In this work, features from our architecture\nwill come from the output of multiple such dilated causal convolutional layers connected\nin series.\nOnce we have a collection of features, we would like to do computations with these\nlearned representations to enable context dependent updates. Historically, attention net-\nworks were introduced in [3] to improve existing long-short term memory (LSTM) [16, 15]\nmodels for neural machine translation by implementing a “soft search” over neighboring\nwords enabling the system to focus only on words relevant to the generation of the next\ntarget word. This early work combined attention with RNNs. Shortly after, CNNs were\ncombined with attention in [39] and [6] for image captioning and question-answering tasks\nrespectively.\nIn [37], self-attention was introduced as a stand alone replacement for LSTMs on a\nwide range of natural language processing tasks leading to state-of-the-art results [10, 26]\nwhich included masked word prediction. Introducing self-attention can be thought of\nas incorporating an inductive biases into the learning architecture to exploit relational\nstructure in the task environment. This amounts to learning over a graph neural network\n[28, 5] where nodes are entities given by the learned features which are then updated\nthrough a message passing procedure along edges. Results in various applications show\nthat self-attention can better capture long range dependencies in comparison to LSTMs\n[9].\nMore precisely, [37] introduced the transformer architecture which consists of an en-\ncoder and decoder for language translation. Both the encoder and decoder contain the\nrepetition of modules which we refer to as transformer blocks. Each transformer block\nconsists of a multi-head self-attention layer followed by normalization, feedforward and\nresidual connections. This is described in detail in Section 3.\nCombining transformer blocks with convolutional layers for feature extraction is a\npowerful combination for various tasks. In particular, for complex reasoning tasks in var-\nious strategic game environments, the addition of these transformer modules signiﬁcantly\n2\nenhanced performance and sample eﬃciency compared with existing non-relational base-\nlines [40, 38, 8]. In this work we combine the causal convolutional architecture of [24] with\nmultiple transformer blocks. Moreover, our transformer blocks contain masked multi-head\nself-attention layers. By applying a mask to our self-attention functions, we ensure that\nthe ordering of events in our time-series is never violated at each step, ie. entities can\nonly attend to entities in its causal past.\nWe train and test our model on the publicly available FI-2010 data-set 1 which is a\nLOB of ﬁve instruments from the Nasdaq Nordic stock market for a ten day period [23].\nWe show that our algorithm outperforms other common and previously state-of-the-art\narchitectures using standard model validation techniques.\nIn summary, inspired by the wavenet architecture of [24] where dilated causal con-\nvolutions were used to encode long-range temporal dependencies, we use these causal\nconvolutions to build a feature map for our transformer blocks to act on. We refer to our\nspeciﬁc architecture as TransLOB. It is a composition of diﬀerentiable functions that pro-\ncess and integrate both local and global information from the LOB in a dynamic relational\nway whilst respecting the causal structure.\nThere are a number of advantages to our architecture outside of the signiﬁcant in-\ncreases in performance. Firstly, in spite of the O(N2) complexity of the self-attention\ncomponent, our architecture is substantially more sample eﬃcient than existing LSTM\narchitectures for this task. Secondly, the ability to analyse attention distributions pro-\nvides a clearer picture of internal computations within the model compared with these\nother methods leading to better interpretability.\nRelated work\nThere is now a substantial literature applying deep neural networks to time-series applica-\ntions, and in particular, limit order books (LOB). Convolutional neural networks (CNN)\nhave been explored in LOB applications in [12, 34]. To capture long-range dependencies\nin temporal behavior, CNNs have been combined with recurrent neural networks (RNN)\n(typically long-short term memory (LSTM)) which improve on earlier results [36, 41].\nSome modiﬁcations to the standard convolutional layer have been used in attempts to\ninfer local interactions over diﬀerent time horizons. For example, [41] uses an inception\nmodule [32] after the standard convolutional layers for this inference followed by an LSTM\nto encode relational dynamics. Stand-alone RNNs have been used extensively in market\nprediction [11, 13, 4] and have been shown to outperform models based on standard\nmulti-layer perceptrons, random forests and SVMs [35].\nFor time-series applications, recent work [33, 25] uses attention and [18, 22, 29] in\ncombination with CNNs. However, there are relatively few references which combine\nCNNs with transformers to analyse time-series data. We mention [30] which uses a CNN\nplus multi-head self-attention to analyse clinical time-series behaviour and [21] which\n1The “MNIST” for limit order books.\n3\nbecame aware to us during the ﬁnal write-up of this paper which uses a similar architecture\nto our own and applied to univariate synthetic and energy sector datasets. As far as we\nare aware, ours is the ﬁrst work applying this class of architectures to the multivariate\nﬁnancial domain, with the various subtleties arising in this particular application.\n2 Experiments\nA limit order book (LOB) at time t is the set of all active orders in a market at time\nt. These orders consist of two sides; the bid-side and the ask-side. The bid-side consists\nof buy orders and the ask-side consists of sell orders both containing price and volume\nfor each order. Our experiments will use the LOB from the publicly available FI-2010\ndataset2. A general introduction to LOBs can be found in [14].\nLet {pi\na(t),vi\na(t)}denote the price (resp. volume) of sell orders at time t at level i in\nthe LOB. Likewise, let {pi\nb(t),vi\nb(t)}denote the price (resp. volume) of buy orders at time\nt at level i in the LOB. The bid price p1\nb(t) at time t is the highest stated price among\nactive buy orders at time t. The ask price p1\na(t) at time tis the lowest stated price among\nactive sell orders at time t. A buy order is executed if p1\nb(t) >p1\na(t) for the entire volume\nof the order. Similarly, a sell order is executed if p1\na(t) < p1\nb(t) for the entire volume of\nthe order.\nThe FI-2010 dataset is made up of 10 days of 5 stocks from the Helsinki Stock Ex-\nchange, operated by Nasdaq Nordic, consisting of 10 orders on each side of the LOB. Event\ntypes can be executions, order submissions, and order cancellations and are non-uniform\nin time. We restrict to normal trading hours (no auction). The general structure of the\nLOB is contained in Table 1.\n(p10\na (t), v10\na (t))\n...\n(p1\na(t), v1\na(t))\n(p1\nb(t), v1\nb (t))\n...\n(p10\nb (t), v10\nb (t))\nEvent t\n(p10\na (t + 1), v10\na (t + 1))\n...\n(p1\na(t + 1), v1\na(t + 1))\n(p1\nb(t + 1), v1\nb (t + 1))\n...\n(p10\nb (t + 1), v10\nb (t + 1))\nEvent t+ 1\n. . . . . . . . . . . . . . . . . . . . . . . .\n(p10\na (t + 10), v10\na (t + 10))\n...\n(p1\na(t + 10), v1\na(t + 10))\n(p1\nb(t + 10), v1\nb (t + 10))\n...\n(p10\nb (t + 10), v10\nb (t + 10))\nEvent t+ 10\nTable 1: Structure of the limit order book.\nThe data is split into 7 days of training data and 3 days of test data. Preprocessing\nconsists of normalizing the data x according to the z-score\n¯xt = xt −¯y\nσy\n2The dataset is available at https://etsin.fairdata.ﬁ/dataset/73eb48d7-4dbc-4a10-a52a-da745b47a649\n4\nwhere y (resp. σy) is the mean (resp. standard deviation) of the previous days data.\nSince the aim of this work is to extract the most amount of possible latent information\ncontained in the LOB, we do not include any of the hand-crafted features contained in\nthe FI-2010 dataset. For a detailed description of this dataset we refer the reader to [23].\nWe aim to predict future movements from the (virtual) mid-price. Price direction\nof the data is calculated using the following smoothed version of the mid-price. This\namounts to adjusting for the average volatility of each instrument. The virtual mid-price\nis the mean\np(t) = p1\na(t) + p1\nb(t)\n2\nbetween the bid-price and the ask-price. The mean of the next k mid-prices is then\nm+\nk (t) = 1\nk\nk∑\nn=0\np(t+ n).\nThe direction of price movement for the FI-2010 dataset is calculated using the per-\ncentage change of the virtual mid-price according to\nrk(t) = m+\nk (t) −p(t)\np(t) .\nThere exist other more sophisticated methods to determine the direction of price move-\nment at a given time. However, for fair comparison to other work, we utilize this deﬁnition\nand leave other methods for future work. The direction is up (+1) if rk(t) > α, down\n(−1) if rk(t) <−α and neutral (0) otherwise, according to a chosen threshold α. For the\nFI-2010 dataset, this has been set to α= 0.002.\nWe consider the following four test casesk∈{10,20,50,100}for the denoising horizon\nwindow. The 100 most recent events are used as input to our model.\n3 Architecture\nIn this section we give a detailed account of our architecture. The main two components\nare a convolutional module and a transformer module. They contain multiple iterations\nof dilated causal convolutional layers and transformer blocks respectively. A transformer\nblock consists of a speciﬁc combination of multi-head self-attention, residual connections,\nlayer normalization and feedforward layers. We took seriously the causal nature of the\nproblem by implementing both causality in the convolutional module and causality in\nthe transformer module through masked self-attention to accurately capture temporal\ninformation in the LOB. Our resulting architecture will be referred to as TransLOB.\nSince each order consists of a price and volume, a statext = {pi\na(t),vi\na(t),pi\nb(t),vi\nb(t)}10\ni=1\nat time t is a vector xt ∈R40. Events are irregularly spaced in time and the 100 most\nrecent events are used as input resulting in a normalized vector X ∈R100×40.\n5\nWe apply ﬁve one-dimensional convolutional layers to the inputX, regarded as a tensor\nof shape [100,40] (ie. an element of R100 ⊗R40). All layers are dilated causal convolutional\nlayers with 14 features, kernel size 2 and dilation rates 1 ,2,4,8 and 16 respectively. This\nmeans the ﬁlter is applied over a window larger than its length by skipping input values\nwith a step given by the dilation rate with each layer respecting the causal order. The ﬁrst\nlayer with dilation rate 1 corresponds to standard convolution. All activation functions\nare ReLU.\nThe full size of the channel ﬁlter is used to allow the weights in the ﬁlter to infer\nthe relative importance of each level on each side of the mid-price. It is expected that\nhigher weights will be allocated to shallower levels in the LOB since those levels are most\nindicative of future activity. The output of the convolutional module is a tensor of shape\n[100,14].\nThis output then goes through layer normalization [2] to stabilize dynamics before\neach feature vector is concatenated with a one-dimensional temporal encoding resulting\nin a tensor X of shape [100 ,15]. We will refer to N = 100 as the number of entities\nand d = 15 as the model dimension. We denote these entities by ei, 1 ≤i ≤N, where\nei ∈E = Rd. These entities are then updated through learning in a number of steps.\nFirst we introduce an inner product space H = Rd with dot product pairing ⟨h,h′⟩=\nh·h′. We employ a multi-head version of self-attention with C channels. Therefore, we\nchoose a decomposition H = H1 ⊕... ⊕HC and apply a linear transformation\nT =\nC⨁\na=1\nTa : E →\nC⨁\na=1\nH⊕3\na\nwith Ha each of dimension d/C. The vectors ( qi,(a),ki,(a),vi,(a)) = Ta(ei) are referred\nto as query, key and value vectors respectively. We arrange these vectors into matrices\nQa, Ka and Va respectively with N-rows and d-columns. In other words, Qa = XWQ\na ,\nKa = XWK\na and Va = XWV\na for weight matrices WQ\na , WK\na and WV\na which are vectors in\nRd×d/C.\nNext we apply the masked scaled dot-product self-attention function\nheada = V′\na = Softmax\n(\nMask\n(QaKT\na√\nd\n))\nVa\nresulting in a matrix of reﬁned value vectors for each entity. Here Mask substitutes\ninﬁnitesimal values to entries in the upper right triangle of the applied matrix which\nforces queries to only pay attention to keys in its causal history via the softmax function.\nThe heads are then concatenated and a ﬁnal learnt linear transformation is given leading\nto the multi-head self-attention operation\nMultiHead(X) =\n( C⨁\na=1\nheada\n)\nWO\nwhere WO ∈Rd×d.\n6\nWe next add a residual connection and apply layer normalization resulting in\nZ = LayerNorm(MultiHead(X) + X).\nThis is followed by a feedforward network MLP consisting of a ReLU activation between\ntwo aﬃne transformations applied identically to each position, ie. individually to each\nrow of Z. The inner layer is of dimension 4×d= 60. Finally, a further residual connection\nand ﬁnal layer normalization is applied to arrive at our updated matrix of entities\nTransformerBlock(X) = LayerNorm(MLP(Z) + Z).\nThe output of the transformer block is the same shape [ N,d] as the input. Our updated\nentities are e′\ni ∈R15, 1 ≤i≤N.\nAfter multiple iterations of the transformer block, the output is then ﬂattened and\npassed through a feedforward layer of dimension 64 with ReLU activation and L2 regular-\nization. Finally, we apply dropout followed by a softmax layer to obtain the ﬁnal output\nprobabilities. A schematic of the TransLOB architecture is given in Figure 1.\ninput\ndilated conv1\ndilated conv2\ndilated conv3\ndilated conv4\ndilated conv5\nLayer Normalization\nPosition Encoding\ntransformer block1\ntransformer block2\nMLP\nDropout\nLinear\nSoftmax\noutput\nFigure 1: Architecture schematic with enclosed convolutional and transformer modules.\n2\nFigure 1: Architecture schematic with enclosed convolutional and transformer modules.\nFor the FI-2010 dataset, we employ two transformer blocks with three heads and with\nthe weights shared between iterations of the transformer block. The hyperparameters are\ncontained in Table 2. No dropout was used inside the transformer blocks.\n7\nHyperparameter Value\nBatch size 32\nAdam β1 0.9\nAdam β2 0.999\nLearning rate 1 ×10−4\nNumber of heads 3\nNumber of blocks 2\nMLP activations ReLU\nDropout rate 0.1\nTable 2: Hyperparameters for the FI-2010 experiments.\n4 Results\nHere we record our experimental results for the FI-2010 dataset. The ﬁrst 7 days were\nused to train the model and the last 3 days were used as test data. Training was done\nwith mini-batches of size 32. Our metrics include accuracy, precision, recall and F1. All\ntraining was done using one K80 GPU on google colab.\nTo be consistent with earlier works using the same dataset, we train and test our\nmodel on the horizons k = {10,20,50,100}. All models were trained for 150 epochs,\nalthough convergence was achieved signiﬁcantly earlier. See Figure 3 of Appendix A for\nan example.\nThe following models were used as comparison. An LSTM was utilized and com-\npared to a support vector machine (SVM) and multi-layer perceptron (MLP) in [35] with\nfavourable results. Results using a stand-alone CNN were reported in [34]. This model\nwas reproduced and trained for use as our baseline for the horizon k= 100. The baseline\ntraining and test curves are shown in Figure 4 of Appendix A. In [36] a CNN was com-\nbined with an LSTM resulting in the architecture denoted CNN-LSTM. An improvement\nover the CNN-LSTM architecture, named DeepLOB, was achieved in [41] by using an\ninception module between the CNN and LSTM together with a diﬀerent choice of con-\nvolution ﬁlters, stride and pooling. Finally, the architecture C(TABL) refers to the best\nperforming implementation of the temporal attention augmented bilinear network of [33].\nOur results are shown in Table 3, Table 4, Table 5 and Table 6 for each of the horizon\nchoices respectively. The training and test curves with respect to accuracy for k = 100\nare shown in Figure 3 of Appendix A.\nFor inspection of our model, we plot the attention distributions for all three heads\nin the ﬁrst transformer block. A random sample input was chosen from the horizon\nk= 10 test set. Pixel intensity has been scaled for ease of visualization. The vertical axes\nrepresent the query index 0 ≤i ≤100 and the horizontal axes represent the key index\n0 ≤j ≤100. Queries are aware of the distance to keys through the position embedding\nlayer and entities are only updated with memory from the past owing to the attention\nmask. As can be seen in Figure 2, and Figure 5 and Figure 6 of Appendix B, the diﬀerent\n8\nModel Accuracy Precision Recall F1\nSVM [35] - 39.62 44.92 35.88\nMLP [35] - 47.81 60.78 48.27\nCNN [34] - 50.98 65.54 55.21\nLSTM [35] - 60.77 75.92 66.33\nCNN-LSTM [36] - 56.00 45.00 44.00\nC(TABL) [33] 84.70 76.95 78.44 77.63\nDeepLOB [41] 84.47 84.00 84.47 83.40\nTransLOB 87.66 91.81 87.66 88.66\nTable 3: Prediction horizon k= 10.\nModel Accuracy Precision Recall F1\nSVM [35] - 45.08 47.77 43.20\nMLP [35] - 51.33 65.20 51.12\nCNN [34] - 54.79 67.38 59.17\nLSTM [35] - 59.60 70.52 62.37\nCNN-LSTM [36] - - - -\nC(TABL) [33] 73.74 67.18 66.94 66.93\nDeepLOB [41] 74.85 74.06 74.85 72.82\nTransLOB 78.78 86.17 78.78 80.65\nTable 4: Prediction horizon k= 20.\nModel Accuracy Precision Recall F1\nSVM [35] - 46.05 60.30 49.42\nMLP [35] - 55.21 67.14 55.95\nCNN [34] - 55.58 67.12 59.44\nLSTM [35] - 60.03 68.58 61.43\nCNN-LSTM [36] - 56.00 47.00 47.00\nC(TABL) [33] 79.87 79.05 77.04 78.44\nDeepLOB [41] 80.51 80.38 80.51 80.35\nTransLOB 88.12 88.65 88.12 88.20\nTable 5: Prediction horizon k= 50.\nModel Accuracy Precision Recall F1\nCNN [34] 63.06 63.29 63.06 62.97\nTransLOB 91.62 91.63 91.62 91.61\nTable 6: Prediction horizon k= 100.\nheads learn to attend to diﬀerent properties of the temporal dynamics. A majority of the\nqueries pay special attention to the most recent keys which is sensible for predicting the\nnext price movement. This is particularly clear in heads two and three.\n9\n0 20 40 60 80\n0\n20\n40\n60\n80\nFigure 2: First head of the ﬁrst transformer block.\n5 Discussion\nWe have shown that the limit order book contains informative information to enable price\nmovement prediction using deep neural networks with a causal and relational inductive\nbias. This was shown by introducing the architecture TransLOB which contains both a\ndilated causal convolutional module and a masked transformer module. This architecture\nwas tested on the publicly available FI-2010 dataset achieving state-of-the-art results. We\nexpect further improvements using more sophisticated proprietary additions such as the\ninclusion of sentiment information from news, social media and other sources. However,\nthis work was developed to exploit only the information contained in the LOB and serves\nas very strong baseline from which additional tools can be added.\nDue to the limited nature of the FI-2010 dataset, signiﬁcant time was spend tuning\nhyperparameters of our model to negate overﬁtting. In particular, our architecture was\nnotably sensitive to the initialization. However, due to the very strong performance of the\nmodel, together with the ﬂexibility and sensible inductive biases of the architecture, we\nexpect robust results on larger LOB datasets. This is an important second step and will\nbe addressed in future work. In particular, this will allow us to explore the generalization\ncapabilities of the model together with the optimization of important parameters such\nas the horizon k and threshold α. Nevertheless, based on these initial results we argue\nthat further investigation of transformer based models for ﬁnancial time-series prediction\ntasks is warranted.\nThe eﬃciency of our algorithm is another imporant property which makes it amenable\nto training on larger datasets and LOB data with larger event windows. In spite of\nthe O(N2) complexity of the self-attention component, our architecture is signiﬁcantly\nmore sample eﬃcient than existing LSTM architectures for this task such as [35, 36, 41].\n10\nHowever, moving far beyond the window size of 100, to the territory of LOB datasets on\nthe scale of months or years, it would be interesting to explore sparse and compressed\nrepresentations in the transformer blocks. Implementations of sparsity and compression\ncan be found in [7, 31, 19, 21] and [17, 27] respectively.\nLooking forward, similar to recent advances in natural language processing, the next\ngeneration of ﬁnancial time-series models should implement self-supervision as pretraining\n[10, 26]. Finally, it would be interesting to consider the inﬂuence of higher-order self-\nattention [8] in LOB and other ﬁnancial time-series applications.\nAcknowledgements\nThe author would like to thank Andrew Royal and Zihao Zhang for correspondence related\nto this project.\nReferences\n[1] A. A. Ariyo, A. O. Adewumi, and C. K. Ayo. Stock price prediction using the arima\nmodel. In 2014 UKSim-AMSS 16th International Conference on Computer Modelling\nand Simulation, pages 106–112. IEEE, 2014.\n[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473 , 2014.\n[4] W. Bao, J. Yue, and Y. Rao. A deep learning framework for ﬁnancial time series\nusing stacked autoencoders and long-short term memory. PloS one, 12(7), 2017.\n[5] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Ma-\nlinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive\nbiases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261 , 2018.\n[6] K. Chen, J. Wang, L.-C. Chen, H. Gao, W. Xu, and R. Nevatia. Abc-cnn: An\nattention based convolutional neural network for visual question answering. arXiv\npreprint arXiv:1511.05960, 2015.\n[7] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with\nsparse transformers. arXiv preprint arXiv:1904.10509 , 2019.\n[8] J. Clift, D. Doryn, D. Murfet, and J. Wallbridge. Logic and the 2-simplicial trans-\nformer. In Proceedings of the International Conference on Learning Representations,\n2020.\n11\n[9] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov. Transformer-\nxl: Attentive language models beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[11] M. Dixon. Sequence classiﬁcation of the limit order book using recurrent neural\nnetworks. Journal of computational science , 24:277–286, 2018.\n[12] J. Doering, M. Fairbank, and S. Markose. Convolutional neural networks applied\nto high-frequency market microstructure forecasting. In 2017 9th Computer Science\nand Electronic Engineering (CEEC) , pages 31–36. IEEE, 2017.\n[13] T. Fischer and C. Krauss. Deep learning with long short-term memory networks for\nﬁnancial market predictions. European Journal of Operational Research, 270(2):654–\n669, 2018.\n[14] M. D. Gould, M. A. Porter, S. Williams, M. McDonald, D. J. Fenn, and S. D.\nHowison. Limit order books. Quantitative Finance, 13(11):1709–1742, 2013.\n[15] K. Greﬀ, R. K. Srivastava, J. Koutn´ ık, B. R. Steunebrink, and J. Schmidhuber. Lstm:\nA search space odyssey. IEEE transactions on neural networks and learning systems ,\n28(10):2222–2232, 2016.\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997.\n[17] N. Kitaev,  L. Kaiser, and A. Levskaya. Reformer: The eﬃcient transformer. In\nProceedings of the International Conference on Learning Representations , 2020.\n[18] G. Lai, W.-C. Chang, Y. Yang, and H. Liu. Modeling long-and short-term temporal\npatterns with deep neural networks. In The 41st International ACM SIGIR Confer-\nence on Research & Development in Information Retrieval , pages 95–104, 2018.\n[19] G. Lample, A. Sablayrolles, M. Ranzato, L. Denoyer, and H. J´ egou. Large memory\nlayers with product keys. In Advances in Neural Information Processing Systems ,\npages 8546–8557, 2019.\n[20] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and\nL. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural\ncomputation, 1(4):541–551, 1989.\n[21] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan. Enhancing the\nlocality and breaking the memory bottleneck of transformer on time series forecasting.\nIn Advances in Neural Information Processing Systems , pages 5244–5254, 2019.\n12\n[22] Y. M¨ akinen, J. Kanniainen, M. Gabbouj, and A. Iosiﬁdis. Forecasting jump arrivals\nin stock prices: new attention-based network architecture using limit order book\ndata. Quantitative Finance, 19(12):2033–2050, 2019.\n[23] A. Ntakaris, M. Magris, J. Kanniainen, M. Gabbouj, and A. Iosiﬁdis. Benchmark\ndataset for mid-price forecasting of limit order book data with machine learning\nmethods. Journal of Forecasting, 37(8):852–866, 2018.\n[24] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalch-\nbrenner, A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw\naudio. arXiv preprint arXiv:1609.03499 , 2016.\n[25] Y. Qin, D. Song, H. Chen, W. Cheng, G. Jiang, and G. Cottrell. A dual-stage\nattention-based recurrent neural network for time series prediction. arXiv preprint\narXiv:1704.02971, 2017.\n[26] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language\nunderstanding with unsupervised learning. Technical report, OpenAI, 2018.\n[27] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap. Compressive trans-\nformers for long-range sequence modelling. In Proceedings of the International Con-\nference on Learning Representations, 2020.\n[28] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph\nneural network model. IEEE Transactions on Neural Networks , 20(1):61–80, 2008.\n[29] S.-Y. Shih, F.-K. Sun, and H.-y. Lee. Temporal pattern attention for multivariate\ntime series forecasting. Machine Learning, 108(8-9):1421–1441, 2019.\n[30] H. Song, D. Rajan, J. J. Thiagarajan, and A. Spanias. Attend and diagnose: Clinical\ntime series analysis using attention models. In Thirty-second AAAI conference on\nartiﬁcial intelligence, 2018.\n[31] S. Sukhbaatar, E. Grave, P. Bojanowski, and A. Joulin. Adaptive attention span in\ntransformers. arXiv preprint arXiv:1905.07799 , 2019.\n[32] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Van-\nhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the\nIEEE conference on computer vision and pattern recognition , pages 1–9, 2015.\n[33] D. T. Tran, A. Iosiﬁdis, J. Kanniainen, and M. Gabbouj. Temporal attention-\naugmented bilinear network for ﬁnancial time-series data analysis.IEEE transactions\non neural networks and learning systems , 30(5):1407–1418, 2018.\n13\n[34] A. Tsantekidis, N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj, and A. Iosiﬁdis.\nForecasting stock prices from the limit order book using convolutional neural net-\nworks. In 2017 IEEE 19th Conference on Business Informatics (CBI) , volume 1,\npages 7–12. IEEE, 2017.\n[35] A. Tsantekidis, N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj, and A. Iosiﬁdis.\nUsing deep learning to detect price change indications in ﬁnancial markets. In 2017\n25th European Signal Processing Conference (EUSIPCO) , pages 2511–2515. IEEE,\n2017.\n[36] A. Tsantekidis, N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj, and A. Iosiﬁdis.\nUsing deep learning for price prediction by exploiting stationary limit order book\nfeatures. arXiv preprint arXiv:1810.09965 , 2018.\n[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,  L. Kaiser,\nand I. Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pages 5998–6008, 2017.\n[38] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung,\nD. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al. Grandmaster level in starcraft\nii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n[39] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and\nY. Bengio. Show, attend and tell: Neural image caption generation with visual\nattention. In International conference on machine learning , pages 2048–2057, 2015.\n[40] V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li, I. Babuschkin, K. Tuyls,\nD. Reichert, T. Lillicrap, E. Lockhart, M. Shanahan, V. Langston, R. Pascanu,\nM. Botvinick, O. Vinyals, and P. Battaglia. Deep reinforcement learning with re-\nlational inductive biases. In Proceedings of the International Conference on Learning\nRepresentations, 2019.\n[41] Z. Zhang, S. Zohren, and S. Roberts. Deeplob: Deep convolutional neural networks\nfor limit order books. IEEE Transactions on Signal Processing , 67(11):3001–3012,\n2019.\n[42] E. Zivot and J. Wang. Vector autoregressive models for multivariate time series.\nModeling Financial Time Series with S-Plus , pages 385–429, 2006.\n14\nA Training curves\nWe plot the training and validation history with respect to accuracy for both our TransLOB\narchitecture in Figure 3 and the baseline CNN architecture of [34] in Figure 4.\nFigure 3: Training and validation accuracy for TransLOB for k= 100.\nFigure 4: Training and validation accuracy for baseline CNN for k= 100.\nB Attention distributions\nWe include here the remaining visualizations of the attention output of our learned model\nin the ﬁrst transformer block. Input is a random sample for the horizon k= 10.\n15\n0 20 40 60 80\n0\n20\n40\n60\n80\nFigure 5: Second head of the ﬁrst transformer block.\n0 20 40 60 80\n0\n20\n40\n60\n80\nFigure 6: Third head of the ﬁrst transformer block.\n16"
}