{
  "title": "Distilling Hypernymy Relations from Language Models: On the Effectiveness of Zero-Shot Taxonomy Induction",
  "url": "https://openalex.org/W4287854596",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2614547895",
      "name": "Devansh Jain",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2319266098",
      "name": "Luis Espinosa Anke",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970476646",
    "https://openalex.org/W1837343293",
    "https://openalex.org/W2159719802",
    "https://openalex.org/W2251021198",
    "https://openalex.org/W3156170450",
    "https://openalex.org/W2250930514",
    "https://openalex.org/W2168565044",
    "https://openalex.org/W3174082608",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W2029344051",
    "https://openalex.org/W2951652751",
    "https://openalex.org/W2155734303",
    "https://openalex.org/W3173566921",
    "https://openalex.org/W2098728436",
    "https://openalex.org/W2142086811",
    "https://openalex.org/W3153626078",
    "https://openalex.org/W2567148466",
    "https://openalex.org/W3036413095",
    "https://openalex.org/W2465217376",
    "https://openalex.org/W2963173796",
    "https://openalex.org/W4302285968",
    "https://openalex.org/W3167126457",
    "https://openalex.org/W3104499181",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W3102916840",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2068737686",
    "https://openalex.org/W3166699508",
    "https://openalex.org/W3018732874",
    "https://openalex.org/W2210137989",
    "https://openalex.org/W3102488924",
    "https://openalex.org/W3155825510",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W38703128",
    "https://openalex.org/W2251797829",
    "https://openalex.org/W3034444248",
    "https://openalex.org/W4253001967",
    "https://openalex.org/W3102762626",
    "https://openalex.org/W190962570",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "In this paper, we analyze zero-shot taxonomy learning methods which are based on distilling knowledge from language models via prompting and sentence scoring. We show that, despite their simplicity, these methods outperform some supervised strategies and are competitive with the current state-of-the-art under adequate conditions. We also show that statistical and linguistic properties of prompts dictate downstream performance.",
  "full_text": "Proceedings of the 11th Joint Conference on Lexical and Computational Semantics, pages 151 - 156\nJuly 14-15, 2022 ©2022 Association for Computational Linguistics\nDistilling Hypernymy Relations from Language Models: On the\nEffectiveness of Zero-Shot Taxonomy Induction\nDevansh Jain♠∗ Luis Espinosa Anke♢\n♠Department of Computer Science and Information Systems, BITS Pilani, India\n♢CardiffNLP (Cardiff University) - AMPLYFI\nf20180798@pilani.bits-pilani.ac.in\nespinosa-ankel@cardiff.ac.uk\nAbstract\nIn this paper, we analyze zero-shot taxonomy\nlearning methods which are based on distilling\nknowledge from language models via prompt-\ning and sentence scoring. We show that, de-\nspite their simplicity, these methods outperform\nsome supervised strategies and are competitive\nwith the current state-of-the-art under adequate\nconditions. We also show that statistical and\nlinguistic properties of prompts dictate down-\nstream performance1.\n1 Introduction\nTaxonomy learning (TL) is the task of arranging\ndomain terminologies into hierarchical structures\nwhere terms are nodes and edges denoteis-a (hyper-\nnymic) relationships (Hwang et al., 2012). Domain-\nspecific concept generalization is at the core of hu-\nman cognition (Yu et al., 2015), and a key enabler\nin NLP tasks where inference and reasoning are\nimportant, e.g.: semantic similarity (Pilehvar et al.,\n2013; Yu and Dredze, 2014), WSD (Agirre et al.,\n2014) and, more recently, QA (Joshi et al., 2020)\nand NLI (Chen et al., 2020).\nEarlier approaches to taxonomy learning focused\non mining lexico-syntactic patterns from candidate\n(hyponym, hypernym) pairs (Hearst, 1992; Snow\net al., 2004; Kozareva and Hovy, 2010; Boella and\nDi Caro, 2013; Espinosa-Anke et al., 2016), cluster-\ning (Yang and Callan, 2009), graph-based methods\n(Fountain and Lapata, 2012; Velardi et al., 2013) or\nword embeddings (Fu et al., 2014; Yu et al., 2015).\nThese methods, which largely rely on hand-crafted\nfeatures, are still relevant today, and complement\nmodern approaches exploiting language models\n(LMs), either via sequence classification (Chen\net al., 2021), or combining contextual, distributed,\nand lexico-syntactic features (Yu et al., 2020). In\n∗Work done during an internship at CardiffNLP.\n1Code available at\nhttps://github.com/devanshrj/\nzero-shot-taxonomy.\nparallel, several works have recently focused on us-\ning LMs as zero-shot tools for solving NLP tasks,\ne.g., commonsense, relational and analogical rea-\nsoning (Petroni et al., 2019; Bouraoui et al., 2020;\nUshio et al., 2021; Paranjape et al., 2021), multi-\nword expression (MWE) identification (Espinosa-\nAnke et al., 2021; Garcia et al., 2021), QA (Shwartz\net al., 2020; Banerjee and Baral, 2020), domain\nlabeling (Sainz and Rigau, 2021), or lexical substi-\ntution and simplification (Zhou et al., 2019). More-\nover, by tuning and manipulating natural language\nqueries (often referred to as prompts), impressive\nresults have been recently obtained on tasks such as\nsemantic textual similarity, entailment, or relation\nclassification (Shin et al., 2020; Qin and Eisner,\n2021).\nIn this paper, we evaluate LMs on TL bench-\nmarks using prompt-based and sentence-scoring\ntechniques, and find not only that they are com-\npetitive with common approaches proposed in the\nliterature (which are typically supervised and/or\nreliant on external resources), but that they achieve\nstate-of-the-art results in certain domains.\n2 Methodology\nWe follow Ushio et al. (2021) and define a prompt\ngeneration function τp(t1,t2) which maps a pair of\nterms and a prompt type pto a single sentence. For\ninstance,\nτkind(“physics”, “science”) =\n“physics is a kind of science”\nThen, given a terminology T, the goal is to, given\nan input term t∈T , retrieve its top kmost likely\nhypernyms, (in our experiments, k∈{1,3,5}), us-\ning either masked language model (MLM) prompt-\ning (§2.1), or sentence-scoring (§2.2).\n2.1 MLM Prompting\nRestrictMLM Petroni et al. (2019) introduced a\n“fill-in-the-blanks” approach based on cloze state-\n151\nments (or prompts) to extract relational knowledge\nfrom pretrained LMs. The intuition being that\nan LM can be considered to “know” a fact (in\nthe form of a <subject, relation, object > triple)\nsuch as <Madrid, capital-of, Spain>if it can suc-\ncessfully predict the correct words when queried\nwith prompts such as “Madrid is the capital of\n[MASK]”. We extend this formulation to define\na hypernym retrieval function fR(·) as follows:\nfR(p,t, T) =P([MASK]|τp(t,[MASK])) ∗T (1)\nwhere pis a prompt type, and T is a one-hot en-\ncoding of the terms T in the LM’s vocabulary. We\nfollow previous works (Petroni et al., 2019; Kass-\nner et al., 2021) and restrict the output probability\ndistribution since this task requires the construc-\ntion of a lexical taxonomy starting from a fixed\nvocabulary.\nPromptMLM For completeness, we also report\nresults for an unrestricted variant of RestrictMLM,\nwhere the LM’s entire vocabulary is considered.\n2.2 LMScorer\nFactual (and true) information such as “Trout is\na type of fish” should be scored higher by a LM\nthan fictitious information such as “Trout is a type\nof mammal”. The method for scoring a sentence\ndepends on the type of LM used.\nCausal Language Models Given a sentence W,\ncausal LMs (C) predict token wi using only past\ntokens W<i. Thus, a likelihood score can be esti-\nmated for each token wi from the LM’s next token\nprediction. The corresponding scores are then ag-\ngregated to yield a score for sentence W.\nsC(W) = exp\n\n\n|W|∑\ni=1\nlogPC(wi|W<i)\n\n (2)\nMasked Language Models Given a sentenceW,\nmasked LMs (M) replace wi by [MASK] and pre-\ndict it using past and future tokens. Thus, a pseudo-\nlikelihood score can be computed for each token\nwi by iteratively masking it and using the LM’s\nmasked token prediction (Wang and Cho, 2019;\nSalazar et al., 2020). The corresponding scores are\nthen aggregated to yield a score for sentence W.\nsM(W) = exp\n\n\n|W|∑\ni=1\nlogPM(wi|W\\i)\n\n (3)\nGiven the above, we can cast TL as a sentence-\nscoring problem by evaluating the natural fluency\nof hypernymy-eliciting sentences. Specifically, for\neach term t, we score the sentences generated using\nτp(·) with every other term t′in the terminology.\nWe then select the term-pair with the highest sen-\ntence score and assume that the corresponding term\nt′is a hypernym of t. Formally, we define a hyper-\nnym selection function fS(·) as follows:\nfS(p,t, T) = arg max\nt′∈T\\t\n[s(τp(t,t′))] (4)\nwhere srefers to the scoring function determined\nby the LM used.\n3 Experimental setup\nThis section covers the datasets and prompts we\nuse in our experiments 2, as well as the different\nLMs we consider. Concerning evaluation metrics,\nwe report standard precision ( P), recall ( R) and\nF-score at the edge level (Bordea et al., 2016).\nDataset Details We evaluate our proposed ap-\nproaches on datasets belonging to two TL Se-\nmEval tasks (TExEval-1, Bordea et al. (2015) and\nTExEval-2, Bordea et al. (2016)). Following recent\nliterature, we consider the equipment taxonomy\nfrom TExEval-1 and the English-language environ-\nment, science and food taxonomies from TExEval-\n2. For the science taxonomy, our results are based\non an average of the 3 subsets , which is in line\nwith previous work. Since these datasets do not\ncome with training data, they are well suited for\nunsupervised approaches.\nDomain Source V E\nenvironment Eurovoc 261 261\nscience\nCombined 453 465\nEurovoc 125 124\nWordNet 429 452\nfood Combined 1556 1587\nequipment Combined 612 615\nTable 1: Taxonomies statistics. Vertices (V) and Edges\n(E) are often used as structural measures.\n2We use PyTorch and thetransformers library (Wolf\net al., 2020), as well as mlm-scoring (Salazar et al., 2020)\n(https://github.com/awslabs/mlm-scoring).\n152\nPrompts We use the following prompts:\n• gen.: [t2] is more general than [t1].\n• spec.: [t1] is more specific than [t2].\n• type: [t1] is a type of [t2].\ngen.and spec.prompts are hand-crafted templates\nto encode, in a general way, the hypernymy re-\nlationship. The choice of the type prompt, how-\never, comes from a set of experiments involving all\nLPAQA (Jiang et al., 2020) prompts under the “is\na subclass of ” category. We do not consider au-\ntomatic prompt generation techniques (Shin et al.,\n2020) due to the absence of training data. Note that\nfor each prompt, we replace t1 with the input term\nso that the task is always to predict its hypernym.\nLanguage Models We interrogate BERT (Devlin\net al., 2019) and RoBERTa (Liu et al., 2019) among\nmasked LMs, and GPT2 (Radford et al., 2019)\namong causal LMs. For each LM, we consider\ntwo variants corresponding to approximately 117M\nparameters and 345M parameters.\n4 Results\nTable 2 shows the results on TExEval-2’s science\nand environment. We compare with the current\nstate of the art (Graph2Taxo) (Shang et al., 2020),\nas well as with other strong baselines such as Tax-\noRL (Mao et al., 2018) and TAXI (Panchenko et al.,\n2016), the highest ranked system in TExEval-2.\nWe also compare with CTP (Chen et al., 2021)\nto illustrate the advantages of zero-shot methods\nvs finetuning. For the environment domain, we\nfind that RestrictMLM performs similar to CTP\nand LMScorer outperforms it. Moreover, all 3\nproposed approaches fail to outperform the other\nbaselines. However, in science, all 3 of our ap-\nproaches outperform CTP, while our best model\n(RestrictMLM) outperforms TAXI and is compet-\nitive with TaxoRL (ours has higher precison, but\nlower recall). Note that compared to our zero-shot\napproaches, these methods are either supervised,\nexpensive to train or take advantage of external\ntaxonomical resources such as WordNet, or lexico-\nsyntactic patterns mined from the web using differ-\nent hand-crafted heuristics.\nWe also show results for TExEval-1’s equipment\nand TExEval-2’s food (Table 3). Both datasets\nare considerably larger than environment and sci-\nence. We compare with the corresponding high-\nest ranked system, namely TAXI for food, and IN-\nRIASAC (Grefenstette, 2015) for equipment. For\nenvironment science\nModel P R F P R F\nTAXI 33.8 26.8 29.9 35.2 35.3 35.2\nTaxoRL 32.3 32.3 32.3 37.9 37.9 37.9\nGraph2Taxo 89.0 24.0 37.0 84.0 30.0 44.0\nCTP 23.1 23.0 23.0 29.4 28.8 29.1\nPromptMLM 19.2 19.2 19.2 34.4 32.0 33.1\nRestrictMLM 23.0 23.0 23.0 39.3 36.7 37.9\nLMScorer 26.4 26.4 26.4 33.1 30.7 31.8\nTable 2: Comparison of our best performing methods\nwith previous work (environment and science).\nboth domains, all 3 of our approaches outperform\nthe corresponding TExEval best-performing sys-\ntems. This suggests that zero-shot TL with LMs\nis robust, easily scalable and feasible on large tax-\nonomies. However, a clear bottleneck for prompt-\nbased methods is that only single-token terms can\nbe predicted (using a single [MASK] token), making\nthis approach a lower bound for TL.\nfood equipment\nModel P R F P R F\nTExEval 13.2 25.1 17.3 51.8 18.8 27.6\nPromptMLM 23.2 22.6 22.9 29.4 29.3 29.4\nRestrictMLM 25.2 24.6 24.9 38.4 38.2 38.3\nLMScorer 25.2 24.6 24.9 37.7 37.6 37.6\nTable 3: Comparison of our best configurations with the\nbest TExEval systems on food and equipment.\n5 Analysis\nIn this section, we provide an in-depth analysis of\nour approaches, including comparison of LMs and\nstatistical and semantic properties of prompts.\nLM Comparison Table 4 compares the best con-\nfiguration for each LM. We can immediately see\nthat a conservative approach (i.e., k = 1 with\nthe type prompt) almost always yields the best\nF-score. Another important conclusion is that,\namong MLMs, BERT-Large performs best across\nthe board, with BERT generally outperforming\nRoBERTa, a finding in line with previous works\n(Shin et al., 2020). Concerning causal LMs, GPT-\n2 Medium outperforms its smaller counterpart as\nwell as both MLMs for sentence-scoring.\nSensitivity to Prompts There is interest in un-\nderstanding models’ sensitivity to prompts and\n153\nenvironment science food equipment\nMethod LM (p,k) P R F (p,k) P R F (p,k) P R F (p,k) P R F\nPromptMLM\nBERT-Base (t,1) 18.8 18.8 18.8(t,1) 30.2 28.1 29.1(t,1) 20.9 20.4 20.6(t,1) 29.4 29.3 29.4\nBERT-Large(t,1) 19.2 19.2 19.2(t,1) 34.4 32.0 33.1(t,1) 23.2 22.6 22.9(t,1) 28.4 28.3 28.4\nRoBERTa-Base(t,1) 18.0 18.0 18.0(t,1) 24.5 23.0 23.7(t,1) 18.5 18.0 18.2(t,1) 26.3 26.2 26.3\nRoBERTa-Large(t,1) 18.0 18.0 18.0(t,1) 28.1 26.2 27.1(t,1) 20.3 19.8 20.0(t,1) 28.4 28.3 28.4\nRestrictMLM\nBERT-Base (t,1) 23.0 23.0 23.0(t,1) 35.8 33.5 34.6(t,1) 22.8 22.2 22.5(t,1) 38.4 38.2 38.3\nBERT-Large(t,1) 21.8 21.8 21.8(t,1) 39.3 36.737.9 (t,1) 25.2 24.624.9 (t,1) 37.9 37.737.8\nRoBERTa-Base(t,1) 5.4 5.4 5.4 (t,1) 11.0 10.6 10.8(t,1) 9.3 9.1 9.2 (t,1) 0.0 0.0 0.0\nRoBERTa-Large(t,1) 8.4 8.4 8.4 (t,1) 12.3 11.8 12.0(t,1) 10.7 10.5 10.6(t,1) 0.0 0.0 0.0\nLMScorer\nBERT-Base (t,1) 20.3 20.3 20.3(t,1) 15.2 14.4 14.8(t,3) 6.8 19.7 10.1 (t,3) 7.5 22.4 11.2\nBERT-Large(t,3) 13.7 41.0 20.5(t,1) 13.0 12.4 12.6(t,1) 13.9 13.6 13.7(t,1) 15.2 15.1 15.1\nRoBERTa-Base(g,3) 7.7 23.0 11.5 (t,3) 5.5 15.7 8.1 (t,3) 2.5 7.2 3.7 (t,5) 4.2 21.0 7.0\nRoBERTa-Large(t,3) 11.1 33.3 16.7(t,1) 13.6 12.8 13.2(t,3) 3.6 10.6 5.4 (t,3) 9.2 27.5 13.8\nGPT-2 Base (t,1) 24.9 24.9 24.9(t,1) 29.3 27.4 28.3(t,1) 21.0 20.5 20.7(t,1) 36.8 36.6 36.7\nGPT-2 Medium(t,1) 26.4 26.426.4 (t,1) 33.1 30.7 31.8(t,1) 25.2 24.624.9 (t,1) 37.7 37.6 37.7\nTable 4: Comparison of best configuration for each LM and proposed approach. (p,k) refers to the prompt and\ntop-kcombination that gives the best results for that setting, where p= gfor gen., sfor spec. and tfor type prompt.\nwhether frequency can explain downstream per-\nformance in lexical semantics tasks (Chiang et al.,\n2020). In the context of prompt vs. performance\ncorrelation, we find that prompt-based downstream\nperformance on TL can be attributed to: (1) syn-\ntactic completeness and (2) semantic correctness.\nFor (1), we find that prompts that are syntactically\nmore complete (e.g., “[X] is a type of [Y]” vs “[X]\nis a type [Y] ”, the difference being the preposi-\ntional phrase) perform better. For (2), we find that\nprompts that unambiguously encode hypernymy\nare also better (i.e., the type prompt, as opposed to\nother noise-inducing templates such as “is a” or “is\nkind of ”). Finally, out of the cleanest prompts, the\nmost frequent in pretraining corpora are the most\ncompetitive. Table 5 confirms the intuition that\nthe type prompt is not only unambiguous, but also\nhighly frequent when compared to similar (noise-\nfree and syntactically complete) prompts.\nPrompt avg F Frequency\nis a type of 25.5 14,503\nis the type of 24.2 809\nis a kind of 23.6 2,934\nis a form of 22.1 9,518\nis one form of 17.9 124\nis a 7.4 9,328,426\nis a type 1.0 15,085\nTable 5: Domain-wise average F-score of LPAQA\nprompts and their frequency in BERT’s pretraining cor-\npora.\nSingle-Token vs Multi-Token HypernymsTa-\nble 6 compares F-score on original terminology\nvs filtered terminology, where filtered terminology\ncontains only the terms that have single-token hy-\npernyms. The results show that % Increase in F-\nscore is inversely proportional to the % Retained.\nThis can be explained by the fact that smaller %\nof terms retained implies higher % of multi-token\nhypernyms in the original dataset that cannot be\npredicted using prompting. Thus, the increase in F-\nscore by removing such hypernyms should increase\nas the % Retained decreases.\nDomain Total Terms % Retained % Increase\nenvironment 261 29.89 2.32\nequipment 612 44.77 1.24\nscience 452 53.32 0.90\nscience_ev 125 52.80 0.89\nfood 1555 59.55 0.57\nscience_wn 370 69.73 0.51\nTable 6: Comparison of F-score on original terminology\nvs filtered terminology. % Retained refers to the percent-\nage of terms that have single-token hypernyms and are\nthus retained for the filtered dataset. % Increase shows\nthe increase in F-score on filtered dataset compared to\nF-score on original dataset.\n6 Conclusion and Future Work\nWe have presented a study of different LMs un-\nder different settings for zero-shot taxonomy learn-\ning. Compared with computationally expensive\nand highly heuristic methods, our zero-shot alter-\nnatives prove remarkably competitive. For the fu-\nture, we could explore multilingual signals and the\nintegration of traditional word embeddings with\ncontextual representations.\n154\nReferences\nEneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.\n2014. Random walks for knowledge-based word\nsense disambiguation. Computational Linguistics,\n40(1):57–84.\nPratyay Banerjee and Chitta Baral. 2020. Self-\nsupervised knowledge triplet learning for zero-shot\nquestion answering. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 151–162.\nGuido Boella and Luigi Di Caro. 2013. Supervised\nlearning of syntactic contexts for uncovering def-\ninitions and extracting hypernym relations in text\ndatabases. In Machine learning and knowledge dis-\ncovery in databases, pages 64–79. Springer.\nGeorgeta Bordea, Paul Buitelaar, Stefano Faralli, and\nRoberto Navigli. 2015. Semeval-2015 task 17: Tax-\nonomy extraction evaluation (texeval). In Proceed-\nings of the 9th International Workshop on Semantic\nEvaluation. Association for Computational Linguis-\ntics.\nGeorgeta Bordea, Els Lefever, and Paul Buitelaar. 2016.\nSemeval-2016 task 13: Taxonomy extraction evalu-\nation (texeval-2). In Proceedings of the 10th Inter-\nnational Workshop on Semantic Evaluation. Associa-\ntion for Computational Linguistics.\nZied Bouraoui, Jose Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom bert. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 7456–7463.\nCatherine Chen, Kevin Lin, and D. Klein. 2021. Con-\nstructing taxonomies from pretrained language mod-\nels. In NAACL.\nMingda Chen, Zewei Chu, Karl Stratos, and Kevin Gim-\npel. 2020. Mining knowledge for natural language\ninference from wikipedia categories. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: Findings , pages\n3500–3511.\nHsiao-Yu Chiang, Jose Camacho-Collados, and Zachary\nPardos. 2020. Understanding the source of semantic\nregularities in word embeddings. In Proceedings\nof the 24th Conference on Computational Natural\nLanguage Learning, pages 119–131.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL.\nLuis Espinosa-Anke, Joan Codina-Filbá, and Leo Wan-\nner. 2021. Evaluating language models for the re-\ntrieval and categorization of lexical collocations. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 1406–1417.\nLuis Espinosa-Anke, Horacio Saggion, Francesco Ron-\nzano, and Roberto Navigli. 2016. Extasem! extend-\ning, taxonomizing and semantifying domain termi-\nnologies. In Proceedings of AAAI, Phoenix, USA.\nTrevor Fountain and Mirella Lapata. 2012. Taxonomy\ninduction using hierarchical random graphs. In Pro-\nceedings of NAACL, pages 466–476. Association for\nComputational Linguistics.\nRuiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng\nWang, and Ting Liu. 2014. Learning semantic hierar-\nchies via word embeddings. In Proceedings of ACL,\nvolume 1.\nMarcos Garcia, Tiago Kramer Vieira, Carolina Scarton,\nMarco Idiart, and Aline Villavicencio. 2021. Probing\nfor idiomaticity in vector space models. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 3551–3564.\nGregory Grefenstette. 2015. Inriasac: Simple hypernym\nextraction methods. In Proceedings of the 9th Inter-\nnational Workshop on Semantic Evaluation (SemEval\n2015), pages 911–914.\nMarti A Hearst. 1992. Automatic acquisition of hy-\nponyms from large text corpora. In Proceedings of\nthe 14th conference on Computational linguistics ,\npages 539–545.\nSung Ju Hwang, Kristen Grauman, and Fei Sha. 2012.\nSemantic kernel forests from multiple taxonomies. In\nAdvances in Neural Information Processing Systems,\npages 1718–1726.\nZhengbao Jiang, Frank F. Xu, J. Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nMandar Joshi, Kenton Lee, Yi Luan, and Kristina\nToutanova. 2020. Contextualized representations us-\ning textual encyclopedic knowledge. arXiv preprint\narXiv:2004.12006.\nNora Kassner, Philipp Dufter, and Hinrich Schütze.\n2021. Multilingual LAMA: Investigating knowledge\nin multilingual pretrained language models. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 3250–3258, Online.\nAssociation for Computational Linguistics.\nZornitsa Kozareva and Eduard Hovy. 2010. A semi-\nsupervised method to learn and construct taxonomies\nusing the web. In Proceedings of EMNLP , pages\n1110–1118.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach.\nArXiv, abs/1907.11692.\n155\nYuning Mao, Xiang Ren, J. Shen, X. Gu, and Jiawei\nHan. 2018. End-to-end reinforcement learning for\nautomatic taxonomy induction. In ACL.\nAlexander Panchenko, Stefano Faralli, E. Ruppert,\nSteffen Remus, Hubert Naets, Cedric Fairon, Si-\nmone Paolo Ponzetto, and Chris Biemann. 2016.\nTaxi at semeval-2016 task 13: a taxonomy induction\nmethod based on lexico-syntactic patterns, substrings\nand focused crawling. In SemEval@NAACL-HLT.\nBhargavi Paranjape, Julian Michael, Marjan\nGhazvininejad, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2021. Prompting contrastive\nexplanations for commonsense reasoning tasks.\narXiv preprint arXiv:2106.06823.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis,\nA. Bakhtin, Yuxiang Wu, Alexander H. Miller, and\nS. Riedel. 2019. Language models as knowledge\nbases? In EMNLP.\nMohammad Taher Pilehvar, David Jurgens, and Roberto\nNavigli. 2013. Align, Disambiguate and Walk: a\nUnified Approach for Measuring Semantic Similar-\nity. In Proceedings of ACL, pages 1341–1351, Sofia,\nBulgaria.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying lms with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nOscar Sainz and German Rigau. 2021.\nAsk2transformers: Zero-shot domain labelling\nwith pretrained language models. In Proceedings of\nthe 11th Global Wordnet Conference, pages 44–52.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In ACL.\nChao Shang, Sarthak Dash, Md. Faisal Mahbub Chowd-\nhury, Nandana Mihindukulasooriya, and A. Gliozzo.\n2020. Taxonomy construction of unseen domains\nvia graph-based cross-domain knowledge transfer. In\nACL.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Eliciting\nknowledge from language models using automati-\ncally generated prompts. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4222–4235.\nVered Shwartz, Peter West, Ronan Le Bras, Chandra\nBhagavatula, and Yejin Choi. 2020. Unsupervised\ncommonsense question answering with self-talk. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4615–4629.\nRion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004.\nLearning syntactic patterns for automatic hypernym\ndiscovery. Advances in Neural Information Process-\ning Systems 17.\nAsahi Ushio, Luis Espinosa-Anke, Steven Schockaert,\nand Jose Camacho-Collados. 2021. BERT is to NLP\nwhat AlexNet is to CV: Can Pre-Trained Language\nModels Identify Analogies? In Proceedings of the\nACL-IJCNLP 2021 Main Conference. Association for\nComputational Linguistics.\nPaola Velardi, Stefano Faralli, and Roberto Navigli.\n2013. OntoLearn Reloaded: A graph-based algo-\nrithm for taxonomy induction. Computational Lin-\nguistics, 39(3):665–707.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a\nmouth, and it must speak: BERT as a Markov ran-\ndom field language model. In Proceedings of the\nWorkshop on Methods for Optimizing and Evaluat-\ning Neural Language Generation, pages 30–36, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nHui Yang and Jamie Callan. 2009. A metric-based\nframework for automatic taxonomy induction. In\nProceedings of ACL/IJCNLP, pages 271–279. Asso-\nciation for Computational Linguistics.\nMo Yu and Mark Dredze. 2014. Improving lexical\nembeddings with semantic knowledge. In ACL (2),\npages 545–550.\nYue Yu, Yinghao Li, Jiaming Shen, Haoyang Feng, Ji-\nmeng Sun, and Chao Zhang. 2020. Steam: Self-\nsupervised taxonomy expansion with mini-paths.\nProceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining.\nZheng Yu, Haixun Wang, Xuemin Lin, and Min Wang.\n2015. Learning term embeddings for hypernymy\nidentification. In Proceedings of IJCAI, pages 1390–\n1397.\nWangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, and\nMing Zhou. 2019. Bert-based lexical substitution. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3368–\n3373.\n156",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7977151870727539
    },
    {
      "name": "Taxonomy (biology)",
      "score": 0.7151322960853577
    },
    {
      "name": "Simplicity",
      "score": 0.669486403465271
    },
    {
      "name": "Sentence",
      "score": 0.6369227170944214
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6215837597846985
    },
    {
      "name": "Natural language processing",
      "score": 0.600113034248352
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4629017114639282
    },
    {
      "name": "Language model",
      "score": 0.4282096028327942
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.4189903438091278
    },
    {
      "name": "Linguistics",
      "score": 0.19538021087646484
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}