{
    "title": "Application of Pretrained Large Language Models in Embodied Artificial Intelligence",
    "url": "https://openalex.org/W4317435073",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2983885961",
            "name": "A. K. Kovalev",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2286333529",
            "name": "A.I. Panov",
            "affiliations": [
                "Russian Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2983885961",
            "name": "A. K. Kovalev",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2286333529",
            "name": "A.I. Panov",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3206919976",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W6810081322",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6800875267",
        "https://openalex.org/W4221152848",
        "https://openalex.org/W2799002257",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W6842443049",
        "https://openalex.org/W3034758614",
        "https://openalex.org/W4389667254",
        "https://openalex.org/W4224912544",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4383108457",
        "https://openalex.org/W4383097638",
        "https://openalex.org/W4226352076",
        "https://openalex.org/W4285428875",
        "https://openalex.org/W3206072662",
        "https://openalex.org/W3203511201",
        "https://openalex.org/W3096099141",
        "https://openalex.org/W4285069854",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W3205239453",
        "https://openalex.org/W3203663566",
        "https://openalex.org/W3173760279",
        "https://openalex.org/W3046631013",
        "https://openalex.org/W6798182279",
        "https://openalex.org/W3184222203",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4212774754"
    ],
    "abstract": "Abstract A feature of tasks in embodied artificial intelligence is that a query to an intelligent agent is formulated in natural language. As a result, natural language processing methods have to be used to transform the query into a format convenient for generating an appropriate action plan. There are two basic approaches to the solution of this problem. One is based on specialized models trained with particular instances of instructions translated into agent-executable format. The other approach relies on the ability of large language models trained with a large amount of unlabeled data to store common sense knowledge. As a result, such models can be used to generate an agent’s action plan in natural language without preliminary learning. This paper provides a detailed review of models based on the second approach as applied to embodied artificial intelligence tasks.",
    "full_text": "85\nISSN 1064-5624, Doklady Mathematics, 2022, Vol. 106, Suppl. 1, pp. S85–S90. © The Author(s), 2022. This article is an open access publication.\nISSN 1064-5624, Doklady Mathematics, 2022. © The Author(s), 2022. This article is an open access publication.\nRussian Text © The Author(s), 2022, published in Doklady Rossiiskoi Akademii Nauk. Matematika, Informatika, Protsessy Upravleniya, 2022, Vol. 508, pp. 94–98.\nApplication of Pretrained Large Language Models\nin Embodied Artificial Intelligence\nA. K. Kovaleva and A. I. Panovb,*\nPresented by Academician of the RAS A.L. Semenov\nReceived October 28, 2022; revised October 31, 2022; accepted November 3, 2022\nAbstract—A feature of tasks in embodied artificial intelligence is that a query to an intelligent agent is formu-\nlated in natural language. As a result, natural language processing methods have to be used to transform the\nquery into a format convenient for generating an appropriate action plan. There are two basic approaches to\nthe solution of this problem. One is based on specialized models trained with particular instances of instruc-\ntions translated into agent-executable format. The other approach relies on the ability of large language mod-\nels trained with a large amount of unlabeled data to store common sense knowledge. As a result, such models\ncan be used to generate an agent’s action plan in na tural language without preliminary learning. This paper\nprovides a detailed review of models based on the second approach as applied to embodied artificial intelli-\ngence tasks.\nKeywords: embodied artificial intelligence, large language models, common sense knowledge, construction\nof action plans\nDOI: 10.1134/S1064562422060138\n1. INTRODUCTION\nIn recent years, increased interest has been shown\nin embodied artificial intelligence (EAI) tasks, which\nare generally represent the operation of objects in\nhuman-oriented environments (household tasks) or\nthe displacement of objects and navigation in homes\nor open areas. A distinctive feature of EAI tasks is that\ninstructions describing a task to be completed or a goal\nto be reached that are transferred to an embodied\nintelligent agent (EIA) are formulated in natural lan-\nguage. Accordingly, natural language processing tech-\nni q ue s  h a v e  t o  be  use d  t o  t r an s f o rm  t he  i n st ruc t i on s\ninto a form convenient for EAI. There are two\napproaches to the solution of this problem.\nOne relies on specialized models trained for gener-\nating a plan of agent’s actions based on instructions.\nExamples are techniques for using templates of possi-\nble actions and determining arguments of these\nactions [1] or models generating token sequences\n(Seq2seq) [2].\nThe other approach is based on the fact that mod-\nern large language models (LLMs) [3, 4] pretrained\nwith large corpora of unlabeled texts produce good\nresults on tasks for which they were not initially\ndesigned after few-shot learning [5] or without learn-\ning at all [6]. This is achieved due to the ability of such\nmodels to store common sense knowledge. In modern\nworks, this property of LLMs is used in EAI tasks, for\nexample, in [7].\nThis paper provides a detailed review of models\nbased on the second approach to EAI tasks in which\npretrained LLMs are used to generate a plan of EIA\nactions in some environment.\n2. APPROACHES OF BASED\nON PRETRAINED LLM\nI n  Z e r o - S h o t  P l a n n e r s  [ 7 ] ,  i t  i s  p r o p o s e d  u s i n g\nLLMs for grounding high-level tasks expressed in nat-\nural language to a set of elementary actions executable\nby an intelligent agent. As a result, given a task descrip-\ntion, LLM must to construct an action plan for imple-\nmenting the task. Here, the LLM is not additionally\ntrained. As an environment where the intelligent agent\nacts, we use the Virtualhome simulator [8]. A plan is\nconstructed iteratively. First, a specially formulated\nquery with a task description made in natural language\nis fed as input to the model. Then the model generates\nin free form a description of the action to be executed\nat the first step. For the resulting action description, a\nspecial vector representation is calculated [9], for\nwhich an action with the closest vector representation\nADVANCED STUDIES IN ARTIFICIAL\nINTELLIGENCE AND MACHINE LEARNING\na Artificial Intelligence Research Institute, Moscow, Russia\nb Federal Research Center “Computer Science and Control,” \nRussian Academy of Sciences, Moscow, Russia\n*e-mail: panov@airi.net\n86\nDOKLADY MATHEMATICS  Vol. 106  Suppl. 1  2022\nKOVALEV, PANOV\nis sought in the set of elementary actions. Next, the\ndescription of the obtained action is added to the\nquery text and the procedure is repeated. A query to\nthe LLM is formulated in the form of a hint with a task\nexample and an action plan for its execution placed at\nthe beginning of the query and with the description of\nthe current task added to the end of the query. Primary\nattention is given plan generation. The agent is\nassumed to be able to perform elementary actions.\nIn G-PlanET [10], LLMs are also used for action\nplan generation. However, in contrast to [7], emphasis\nis placed on binding to a particular environment,\nrather than to only agent’s actions. A modification of\nan ALFRED task [11] is used in which the scene is\nrepresented as a table with an attached list of all scene\nobjects with their type, position, orientation, and par-\nent object (on which the given object lies/in which it is\nplaced). At the planning stage, the scene table is\nunfolded in lines and is added to the description of the\ntask. The resulting text in the form of a query is fed to\nthe input of the LLM. Thus, a query in G-PlanET [10]\nconsists of a generated tabulated representation of the\nscene with an operating intelligent agent in it and a\ntask description in natural language. A plan is gener-\nated iteratively. The result of the current step is concat-\nenated with the query for this step and is fed to the\ninput of the model for generating the next step. A sim-\nilar approach is used in EA-APG (environmentally-\naware action plan generation) [12], but the scene\ndescription consists of only a list of objects.\nIn the SayCan architecture [13], a key stage is\ntraining an agent to execute elementary actions. An\naction consists of three parts: a strategy, i.e., a\nsequence of instructions for the displacement of an\nintelligent agent or its movable parts; a natural lan-\nguage description; and an affordance function return-\ning the probability of successful execution of the\naction in the current state of the environment. The\ngeneration of a plan is similar to the process imple-\nmented in Zero-Shot Planners [7], with the only dif-\nference being that the LLM does not generate a\ndescription of the next action, but rather estimates the\nprobability that an elementary action is useful for exe-\ncuting the task in question. Such an estimate is pro-\nduced for all possible elementary actions and is multi-\nplied by the successful action execution probability\nobtained from the utility function correspondences.\nThe next action is chosen to be the one with the max-\nimum estimate value. A distinctive feature of this work\nis that experiments were conducted in both virtual\nenvironment and on robotic platforms in a realistic\nenvironment. It should also be noted that a query con-\nsists not of a single example as in Zero-Shot Planners\n[7], but rather contains several examples. Another dif-\nference in the formation of a query is that it represents\nnot a task description with an action plan, but rather a\ndialogue between the user and the intelligent agent in\nwhich the former asks a question, for example, “How\ncan you bring some snack to me?”, and the agent lists\nthe actions necessary for executing this task. Addition-\nally, the authors use the approach to query formation\nproposed in [14]. According to this approach, a task\nexecution description is added to the query in addition\nto the task and its solution. The use of such a hint in\nSayCan [13] improves the performance of the model\nfor tasks involving negation or reasoning. A limitation\nof this approach is that, as was shown in [14], improve-\nments are demonstrated only for LLMs with more\nthan 100 billions of parameters.\nIn the ProgPromt architecture [15], the basic idea\nis that a query in represented in the form of a Python-\nlike code. A query consists of a description of available\nactions in the form of imported corresponding soft-\nware modules, a list of objects in the scene, task exam-\nples, and their execution in the form of software mod-\nu l e s .  T h i s  f o r m  o f  q u e r y  i s  j u s t i fi e d  b y  t h e  f a c t  t h a t\nLLMs, such as GPT-3 with 175 billion parameters [5],\nare trained on large amounts of data from open soft-\nware code repositories. A similar approach is used in\nthe CaP model [16], which generates an agent’s strat-\negy. A distinctive feature is that the resulting programs\nare executable software codes, and they can be exe-\ncuted in a hierarchical manner.\nSome works, for example, the Socratic Model [17]\nand Inner Monologue [18], propose not a particular\nmodel, but rather an approach to the construction and\nunion of a set of models. For example, pretrained\nmodels involving various modalities (sound, text,\nimage, etc.) are used in the Socratic Model [17]. Such\nmodels can be combined in systems capable of solving\ntasks going beyond the scope of tasks for each individ-\nual model. This is achieved by developing an interface\nfor data exchange between the models. As an example\nof robotic task, planning displacements of objects over\na table is considered. The Pybullet simulator [19] is\nused to detect objects in a scene. Given an image, its\ndescription is produced by applying the ViLD\napproach [20], after which the scene description in the\nform of a query is fed to LLM for generating an action\nplan by analogy with Zero-Shot Planners [7] and Say-\nCan. Next, the plan is executed following a CLIPort-\nlike strategy [21, 22]. A query consists of a scene\ndescription in the form of a python-like list of objects,\nexamples of tasks formulated in natural language, and\ntheir implementation in the form of a pseudocode.\nThe Inner Monologue architecture [18] makes use\nof text-like feedback obtained from either the environ-\nment (scene description, success of action execution)\nor from the user (refinement of agent’s actions). The\ngeneral approach remains fixed, while, depending on\nthe considered task, different models are used for\nimplementation. The basic idea is that feedback from\nthe environment in the form of text is added iteratively\nto the input query used for planning LLM. A query\nwith a scene description and task examples is used in\nmanipulations with objects (in an environment with a\nvirtual or actual table). For executing a task in a realis-\nDOKLADY MATHEMATICS  Vol. 106  Suppl. 1  2022\nAPPLICATION OF PRETRAINED LARGE LANGUAGE MODELS 87\ntic kitchen, a query is formulated in the form of a dia-\nlogue between the user and the embodied agent.\nThe LM-Nav [23] relies on an approach that falls\ninto Socratic Models [17], in which case separately\npretrained models are joined in a single system to be\nused for text and image navigation. LLM GPT-3 [5] is\nemployed for generating a sequence of text labels\nbased on natural language instructions, a visual lan-\nguage model assigns text labels to images obtained by\nthe agent [24], and an image navigation model [25]\ngenerates and executes a plan of the agent’s displace-\nment. Three examples of extracted text labels are used\nas a query for LLM.\n3. CLASSIFICATION OF LLM \nFOR PLANNING TASK\nA classification of the considered approaches by\ntypes of queries, testing environments, and used\nLLMs is presented in Table 1. Generalizing the data\ngiven in Table 1, we should note that, in the general\nform, a query to a language model can consist of the\nfollowing parts:\n1. a scene description, which is simply a list of\navailable objects possibly supplemented with their\nproperties;\n2. a list of actions available to the embodied agent;\n3. examples of tasks posed for the embodied agent;\n4. examples of executing posed tasks.\nThe query itself can be expressed as a plain text or a\ndialogue. In addition to natural language queries, it is\npossible to use queries representing a pseudo-code or\nan executable software code written, for example, in\nthe Python language.\nIt should be noted that choosing a proper query is a\nrather difficult task, and its result can be affected by\nseemingly minor modifications, such as the number-\ning of the list of planned actions and additions of line\nbreak symbols (see the examples in SayCan [13]).\nAll the considered approaches can be described by a\ncommon architecture with LLM for the task of plan-\nning the embodied agent’s actions represented in Fig. 1.\nAt the first stage, a natural language instruction\ndescribing the task for the embodied agent is used to\nform a query for LLM. In the simplest case, a query\nconsists of task examples with execution plans in terms of\ninstructions available to the embodied agent [7, 13, 23].\nA query of more complex structure can include addi-\ntional information on the environment, for example, a\nlist of present objects and their properties [10, 12, 15–18]\nor available actions of the agent [15, 16]. Next, the\nquery is fed to LLM, which iteratively generates an\naction plan. It should be noted that queries are mainly\nformulated in natural language [7, 10, 12, 13, 17, 18, 23],\nbut there are approaches using pseudo-codes [16, 17] or\nsoftware codes [15] for this purpose. At the second\nstage, the agent implements the generated plan. This\nformulation means that the action plan is completely\ngenerated prior to its implementation in the environ-\nment and is not modified in the course of the imple-\nmentation. This may lead to a situation when the agent\ngets stuck at some stage of the plan implementation,\nwhich, in turn, may lead to the initial task failed to be\nexecuted. A possible way out of this problem is to use\nfeedback from the environment (dashed arrow in Fig. 1)\nat every iteration generating the next agent’s action. As\nfeedback, it is possible to use information on the pos-\nsibility of executing a particular action [13] or a report\nconcerning the correct execution of an action or the\nvariation in the object state [18].\nCONCLUSIONS\nThe considered approaches to the use of LLM for\ngenerating action plans yield moderately good results\nin both virtual environments and implementations on\nrobotic platforms. Nevertheless, a quantitative com-\nparison of these works is complicated by the fact that\nthey involve different environments (except for several\nworks). Note that there are well-known benchmark\nFig. 1. General architecture of using large language models (LLM) in the task of generating an embodied agent’s action plan.\nQuery Action plan\nLLM\nAgentEnvironment\nAction\nObservationFeedbackEnvironment\ninformation\nInstruction:\nWash cup\nExamples\nInstruction\n1. Find cup\n2. Grab cup\n3. Find sink\n4. ...\n    ...\nAdditional\ninformation\n88\nDOKLADY MATHEMATICS  Vol. 106  Suppl. 1  2022\nKOVALEV, PANOV\nTable 1. Comparisons of algorithms for embodied artificial intelligence tasks based on pretrained LLMs\nAlgorithm Language model Environment Robot \nimplementation Type of query Navigation Interact. with \nenvironment\nZero-Shot \nPlanners [7]\nGPT-3 175B [5] \nCodex 12B [29]\nVirtualHome [8] – Examples of tasks and \ntheir solutions\n++\nG-PlanET \n[10]\nTaPEX [30] ALFRED [11] \n(modification)\n–S c e n e  d e s c r i p t i o n ,  t a s k  \ndescription\n––\nEA-APG [12] GPT-3 [5] VirtualHome [8] – Scene description, \nexamples of tasks and \ntheir solutions\n++\nSayCan [13] PALM 540B [4] Realistic environ-\nment (kitchen)\nEveryday Robots Examples of tasks and \ntheir solutions formu-\nlated in the form of \ndialogue\n++\nProgPromt \n[15]\nGPT-3 175B [5] VirtualHome [8] – Python-like code with \ndescription of admissi-\nble actions, scene, \nexamples of tasks, and \ntheir implementation\n++\nSocratic [17] GPT-3 175B [5] Pybullet [19] – Scene description, \nexamples of tasks, and \ntheir implementation \non pseudo-code\n–+\nInner Mono-\nlogue [18]\nInstructGPT [31] Pybullet [19] – Scene description, \nexamples of tasks, and \ntheir implementation\n–+\nInstructGPT [31] Realistic\nenvironment \n(table)\nEveryday Robots Scene description, \nexamples of tasks, and \ntheir implementation\n–+\nPALM 540B [4] Realistic\nenvironment \n(kitchen)\nEveryday Robots examples of tasks, and \ntheir implementation in \ndialogue form\n++\nCaP [16] Codex [29] code-\ndavinci-002\nRealistic environ-\nment (drawing on \nwhite board)\nUR5e Python-like code with \ndescription of admissi-\nble actions, scene, \nexamples of tasks, and \ntheir implementation\n+–\nCodex [29] code-\ndavinci-002\nRealistic environ-\nment (table)\nUR5e Python-like code with \ndescription of admissi-\nble actions, scene, \nexamples of tasks, and \ntheir implementation\n+–\nCodex [29] code-\ndavinci-002\nRealistic environ-\nment (kitchen)\nEveryday Robots Python-like code with \ndescription of admissi-\nble actions, scene, \nexamples of tasks, and \ntheir implementation\n++\nLM-Nav [23] GPT-3 [5] Realistic environ-\nment (street)\nClearpath Jackal \nUGV\nExamples of tasks and \ntheir implementation\n+–\nDOKLADY MATHEMATICS  Vol. 106  Suppl. 1  2022\nAPPLICATION OF PRETRAINED LARGE LANGUAGE MODELS 89\ntasks with established quality metrics and comparison\ntables for testing embodied intelligent agents. Exam-\nples are ALFRED [11] and TEACh [26] for following\nnatural language instructions, RoomR [27] and\nBenchbot [28] for object rearrangements, and others.\nUnfortunately, most of the indicated works are not\npresented in these benchmarks, which is largely\nexplained by the complexity of organizing queries in\nvarious environments with a large number of objects\nand actions.\nA promising future direction of research is as fol-\nlows. First, the feedback should be made more com-\nplicated and trained in order to determine the quality\nof LLM responses. Second, the training of LLMs\nshould be modified, namely, a regularizer modeling\nthe reality of generated responses should be added to\nthe loss function of the entire language model.\nCONFLICT OF INTEREST\nThe authors declare that they have no conflicts of interest.\nOPEN ACCESS\nThis article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or\nformat, as long as you give appropriate credit to the original\nauthor(s) and the source, provide a link to the Creative Com-\nmons license, and indicate if changes were made. The images\nor other third party material in this article are included in the\narticle’s Creative Commons license, unless indicated other-\nwise in a credit line to the material. If material is not included\nin the article’s Creative Commons license and your intended\nuse is not permitted by statutory regulation or exceeds the\np e r m i t t e d  u s e ,  y o u  w i l l  n e e d  t o  o b t a i n  p e r m i s s i o n  d i r e c t l y\nfrom the copyright holder. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by/4.0/.\nREFERENCES\n1. S. Y. Min et al., “Film: Following instructions in\nlanguage with modular methods,” arXiv preprint\narXiv:2110.07342 (2021).\n2. H. Liu et al., “LEBP—language expectation & binding\npolicy: A two-stream framework for embodied vision-\nand-language interaction task learning agents,” arXiv\npreprint arXiv:2203.04637 (2022).\n3. J. Devlin et al., “Bert: Pre-training of deep bidirection-\nal transformers for language understanding,” arXiv pre-\nprint arXiv:1810.04805 (2018).\n4. A. Chowdhery et al., “Palm: Scaling language modeling\nwith pathways,” arXiv preprint arXiv:2204.02311 (2022).\n5. T. Brown et al., “Language models are few-shot learn-\ners,” Advances in neural information processing sys-\ntems,” 33, 1877–1901 (2020).\n6. J. Wei et al., “Finetuned language models are zero-shot\nlearners,” arXiv preprint arXiv:2109.01652 (2021).\n7. W. Huang et al., “Language models as zero-shot plan-\nners: Extracting actionable knowledge for embodied\nagents,” arXiv preprint arXiv:2201.07207 (2022).\n8. X. Puig et al., “Virtualhome: Simulating household ac-\ntivities via programs,” Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (2018),\npp. 8494–8502.\n9. N. Reimers and I. Gurevych, “Sentence-bert: Sentence\nembeddings using siamese bert-networks,” arXiv pre-\nprint arXiv:1908.10084 (2019).\n10. B. Y. Lin et al., “On grounded planning for embodied\ntasks with language models,” arXiv preprint\narXiv:2209.00465 (2022).\n11. M. Shridhar et al., “Alfred: A benchmark for interpret-\ning grounded instructions for everyday tasks,” Proceed-\nings of the IEEE/CVF Conf erence on Computer Vision\nand Pattern Recognition (2020), pp. 10740–10749.\n12. M. Gramopadhye and D. Szafir, “Generating execut-\nable action plans with environmentally-aware language\nmodels,” arXiv preprint arXiv:2210.04964 (2022).\n13. M. Ahn et al., “Do as I can, not as I say: Grounding\nlanguage in robotic affordances,” arXiv preprint\narXiv:2204.01691 (2022).\n14. J. Wei et al., “Chain of thought prompting elicits rea-\nsoning in large language models,” arXiv preprint\narXiv:2201.11903 (2022).\n15. I. Singh et al., “ProgPrompt: Generating situated robot\ntask plans using large language models,” arXiv preprint\narXiv:2209.11302 (2022).\n16. J. Liang et al., “Code as policies: Language model pro-\ngrams for embodied control,” arXiv preprint\narXiv:2209.07753 (2022).\n17. A. Zeng et al., “Socratic models: Composing zero-shot\nmultimodal reasoning with language,” arXiv preprint\narXiv:2204.00598 (2022).\n18. W. Huang et al., “Inner monologue: Embodied reason-\ning through planning with language models,” arXiv pre-\nprint arXiv:2207.05608 (2022).\n19. E. Coumans and Y. Bai, “Pybullet, a python module\nfor physics simulation for games, robotics, and machine\nlearning,” GitHub Repository (2016).\n20. X. Gu et al., “Open-vocabulary object detection via vi-\nsion and language knowledge distillation,” arXiv pre-\nprint arXiv:2104.13921 (2021).\n21. M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What\nand where pathways for robotic manipulation,” Confer-\nence on Robot Learning (PMLR, 2022), pp. 894–906.\n22. A. Zeng et al., “Transporter networks: Rearranging the\nvisual world for robotic manipulation,” arXiv preprint\narXiv:2010.14406 (2020).\n23. D. Shah et al., “Lm-nav: Robotic navigation with large\npre-trained models of language, vision, and action,”\narXiv preprint arXiv:2207.04429 (2022).\n24. A. Radford et al., “Learning transferable visual models\nfrom natural language supervision,” International Confer-\nence on Machine Learning (PMLR, 2021), pp. 8748–8763.\n25. D. Shah et al., “Ving: Learning open-world navigation\nwith visual goals,” 2021 IEEE International Conference\non Robotics and Automation  ( ICRA) (IEEE, 2021),\npp. 13215–13222.\n90\nDOKLADY MATHEMATICS  Vol. 106  Suppl. 1  2022\nKOVALEV, PANOV\n26. A. Padmakumar et al., “Teach: Task-driven embodied\nagents that chat,” Proceedings of the AAAI Conference on\nArtificial Intelligence (2022), Vol. 36, No. 2, pp. 2017–\n2025.\n27. L. Weihs et al., “Visual room rearrangement,” Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (2021), pp. 5922–5931.\n28. B. Talbot et al., “Benchbot: Evaluating robotics re-\nsearch in photorealistic 3d simulation and on real ro-\nbots,” arXiv preprint arXiv:2008.00635 (2020).\n29. M. Chen et al., “Evaluating large language models trained\non code,” arXiv preprint arXiv:2107.03374 (2021).\n30. Q. Liu et al., “Tapex: Table pre-training via learning a\nneural SQL executor,” arXiv preprint arXiv:2107.07653\n(2021).\n31. L. Ouyang et al., “Training language models to follow\ninstructions with human feedback,” arXiv preprint\narXiv:2203.02155 (2022).\nTranslated by I. Ruzanova"
}