{
  "title": "Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning",
  "url": "https://openalex.org/W3197051375",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4287101227",
      "name": "Theodoropoulos, Christos",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A2169962056",
      "name": "Henderson, James",
      "affiliations": [
        "Idiap Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4287101229",
      "name": "Coman, Andrei C.",
      "affiliations": [
        "Idiap Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4202077812",
      "name": "Moens, Marie-Francine",
      "affiliations": [
        "KU Leuven"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3091546937",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2949348150",
    "https://openalex.org/W4287812705",
    "https://openalex.org/W1550588214",
    "https://openalex.org/W3100345210",
    "https://openalex.org/W2799125718",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2435103813",
    "https://openalex.org/W2125553157",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2964167098",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W2946515115",
    "https://openalex.org/W2008830554",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2579356637",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2963997908",
    "https://openalex.org/W2157758844",
    "https://openalex.org/W3035259209",
    "https://openalex.org/W1538131130",
    "https://openalex.org/W2129767020",
    "https://openalex.org/W2944828972",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W3115462295",
    "https://openalex.org/W2250646484",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2146191280",
    "https://openalex.org/W2974004142",
    "https://openalex.org/W3035058308",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2154668152",
    "https://openalex.org/W2004763266",
    "https://openalex.org/W2020278455",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2600659824",
    "https://openalex.org/W2004384146"
  ],
  "abstract": "Though language model text embeddings have revolutionized NLP research, their ability to capture high-level semantic information, such as relations between entities in text, is limited. In this paper, we propose a novel contrastive learning framework that trains sentence embeddings to encode the relations in a graph structure. Given a sentence (unstructured text) and its graph, we use contrastive learning to impose relation-related structure on the token level representations of the sentence obtained with a CharacterBERT (El Boukkouri et al., 2020) model. The resulting relation-aware sentence embeddings achieve state-of-the-art results on the relation extraction task using only a simple KNN classifier, thereby demonstrating the success of the proposed method. Additional visualization by a tSNE analysis shows the effectiveness of the learned representation space compared to baselines. Furthermore, we show that we can learn a different space for named entity recognition, again using a contrastive learning objective, and demonstrate how to successfully combine both representation spaces in an entity-relation task.",
  "full_text": "Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 337–348\nNovember 10–11, 2021. ©2021 Association for Computational Linguistics\n337\nImposing Relation Structure in Language-Model Embeddings\nUsing Contrastive Learning\nChristos Theodoropoulos\nKU Leuven\nchristos.theodoropoulos@kuleuven.be\nJames Henderson\nIdiap Research Institute\njames.henderson@idiap.ch\nAndrei Catalin Coman\nEPFL, Idiap Research Institute\nandrei.coman@idiap.ch\nMarie-Francine Moens\nKU Leuven\nsien.moens@kuleuven.be\nAbstract\nThough language model text embeddings have\nrevolutionized NLP research, their ability to\ncapture high-level semantic information, such\nas relations between entities in text, is limited.\nIn this paper, we propose a novel contrastive\nlearning framework that trains sentence em-\nbeddings to encode the relations in a graph\nstructure. Given a sentence (unstructured text)\nand its graph, we use contrastive learning to\nimpose relation-related structure on the token-\nlevel representations of the sentence obtained\nwith a CharacterBERT (El Boukkouri et al.,\n2020) model. The resulting relation-aware sen-\ntence embeddings achieve state-of-the-art re-\nsults on the relation extraction task using only\na simple KNN classiﬁer, thereby demonstrat-\ning the success of the proposed method. Addi-\ntional visualization by a tSNE analysis shows\nthe effectiveness of the learned representation\nspace compared to baselines. Furthermore, we\nshow that we can learn a different space for\nnamed entity recognition, again using a con-\ntrastive learning objective, and demonstrate\nhow to successfully combine both representa-\ntion spaces in an entity-relation task.\n1 Introduction\nPretrained language models (LMs), such as BERT\n(Devlin et al., 2018), RoBERTa (Liu et al., 2019)\nand GPT-3 (Brown et al., 2020), capture contex-\ntualized information effectively and are used in a\nwide variety of natural language processing (NLP)\ntasks. They have revolutionized NLP research. The\nmain mechanism of these models is multi-head\nself-attention (Vaswani et al., 2017), which enables\ncapturing patterns of semantic and syntactic inter-\nest in text. However, their ability to encapsulate\nhigh level semantic information, such as relations\nin the text, and domain-speciﬁc knowledge, is lim-\nited because they are trained on very large corpora\nusing the main objectives of language modeling.\nIn many NLP tasks, pretrained LM embeddings\nare used as model input. A common strategy is\nto concatenate the embeddings that are extracted\nfrom different LMs and let the model decide which\npart of the information is useful for the task. This\nempirical approach does not provide strong intu-\nition and results in poor explainability capabilities\nbecause most of the task-speciﬁc models are black\nboxes.\nIn this study, we present a novel contrastive learn-\ning (CL) framework to leverage the embedding\nspace of CharacterBERT and impose a relation\nstructure on the embeddings. The proposed frame-\nwork receives a sentence and a graph that repre-\nsents the text relations in a structured way, and\nthe CL paradigm is applied to impose this struc-\nture on the token embeddings of the Character-\nBERT text encoder. Different graph formulations\nthat represent the text relations are explored. The\nmain goal is to create a common embedding space\nwhere relations can be easily detected. To evaluate\nprogress towards this goal, we use the ADE dataset\n(Gurulingappa et al., 2012), which is widely used\nfor relation extraction (RE) (Zhao and Grishman,\n2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank\nand Moschitti, 2013) and named entity recogni-\ntion (NER) tasks (Curran and Clark, 2003; Florian\net al., 2006; Nadeau and Sekine, 2007; Florian\net al., 2010) in the challenging ﬁeld of information\nextraction (IE) from biomedical text.\nTo evaluate the efﬁcacy of our approach, a sim-\nple baseline neural network classiﬁer for RE, us-\n338\ning the pretrained CharacterBERT medical version\nrepresentations, is trained. The representations of\nthe CharacterBERT tuned version after applying\nCL are used to train the same classiﬁer, which\nvastly outperforms the baseline classiﬁer. A tSNE\n(Van der Maaten and Hinton, 2008) analysis illus-\ntrates that meaningful relation-related clusters can\nbe identiﬁed in the learned embedding space. This\nprovides a second strong indication that structure\ncan be effectively imposed on LM embeddings us-\ning our proposed framework.\nEven if the main focus of this work is not solving\nthe IE problem directly, to further explore the capa-\nbilities of the relation-aware representation space,\nwe train a simple KNN classiﬁer for RE that is com-\npetitive with state-of-the-art performance. Strict\nevaluation (Bekoulis et al., 2018b; Taillé et al.,\n2020) of the RE task presupposes correct detec-\ntion of the boundaries and the entity type of each\nargument in the relation. Hence, we apply the CL\nparadigm to learn a distinct embedding space for\nthe entities and use a KNN classiﬁer to solve the\nNER task. Finally, we perform a strict evaluation\nof the complete entity-relation extraction task. This\ntransparent, computationally inexpensive and intu-\nitively simple approach has comparable results to\nthe state-of-the-art models. This achievement illus-\ntrates how informative and meaningful the learned\nembedding spaces are.\nIn summary, our key contributions are:\n• We propose a novel CL framework for impos-\ning a relation-related structure on LM embed-\ndings.\n• We investigate different ways to model texts\nand graphs and show the effectiveness of em-\nbedding relations in pairs of token embed-\ndings.\n• We exploit the capabilities of the learned rep-\nresentation spaces by using them in the IE\ntask and achieve competitive results to state-\nof-the-art models, even if we use transparent\nand intuitively simple KNN classiﬁers.\nThe paper is structured as follows. Section 2\npresents the ADE dataset and the data preprocess-\ning steps, and section 3 explains the framework in\ndetail. In section 4, we evaluate the quality of the\nframework in baseline setups. The tSNE analysis\nis presented in section 5. In section 6, we use the\nframework to solve the IE task and compare the\nresults to state-of-the-art models.\n2 Dataset\nThis study focuses on biomedical text, and ADE\ndataset is used. The sentences are annotated with\nlabels for drugs and adverse effects, as well as\nthe relations among these entities. Adverse effects\n(AEs) cover a range of signs, symptoms, diseases,\ndisorders, abnormalities, organ damage and even\ndeath caused by that drug. The corpus is annotated\nat the sentence-level, so non-local relations (be-\ntween entities of different sentences) do not exist.\n2.1 Data Preprocessing\nThe input of the main CL framework consists of the\nencoded padded sentence and the relation graph,\nwhich is extracted from the sentence. The graphs\nare used only in the training setup. To prepare the\ninput for CharacterBERT, tokenization is applied\nto each sentence using the character-CNN module\n(Peters et al., 2018). The BERT tokenizer handles\nout-of-vocabulary (OOV) words by splitting these\nwords into word pieces. However, the existence\nof word pieces can be an obstacle in creating and\ntesting the CL experiments of this study from the\nimplementation point of view. Additionally, word\npieces may add biases to the model (El Boukkouri\net al., 2020), especially in biomedical text where\nmost of the drugs and many adverse effects are\nOOV words. Hence, CharacterBERT is chosen\ninstead of BERT.\nFor each sentence, a knowledge graph is ob-\ntained to model the relations between the drugs\nand the adverse effects. The graph nodes are ini-\ntialized with embeddings that are extracted by the\nﬁnal layer of the pretrained medical version of\nCharacterBERT. The graph convolutional network\n(GCN) (Kipf and Welling, 2016), which is a key\nlayer of the main proposed CL framework (Fig. 1,\nFig. 2), receives two inputs: an NxF matrix (N:\nnumber of nodes, F: number of features) with the\nembeddings (features) of each node and an adja-\ncency matrix NxN, which models the connections\n(edges) of the undirected graph. Generally, the\nadjacency matrices are very sparse if we consider\nall the tokens and create the whole graph because\nthe relations are rare and there are many singleton\nnodes. Alternatively, the tokens that are part of a\nrelation can only be used, and the essential sub-\ngraph is extracted. For example, in the sentence\n\"Methods: we report two cases of pseudoporphyria\ncaused by naproxen and oxaprozin.\" There are two\nAE relations between AE pseudoporphyria and the\n339\ndrugs naproxen and oxaprozin. Hence, by creat-\ning the subgraph, only these AE and drug tokens\nare included, and the singleton nodes (rest of the\nsentence tokens) are removed.\nThe drug and the AE entities may consist of\nmore than one word. There are two methods to\nmodel this case. On the one hand, the whole phrase\ncan be represented as one node in the graph by av-\neraging the embeddings of each distinct word of\nthe phrase. On the other hand, each node refers\nto the last word of the entity. For example, if the\ninitial relation is between the drug \" gabapentin\"\nand the adverse effect \"renal impairment\", then in\nthe graph, the relation [gabapentin, impairment] is\nmodeled. The latter approach is mainly adopted\nin nonspan-based relation extraction models (Bek-\noulis et al., 2018b; Zhao et al., 2020). In this study,\nthe second approach is adopted because it gives the\nﬂexibility in applying contrastive learning at the\ntoken and relation levels.\nThe normalization of the adjacency matrix is\nessential for aggregating and propagating the infor-\nmation in the graph effectively (Kipf and Welling,\n2016) and is described by the following equations:\nAhat = A+ I, (1)\nAnorm = D−0.5 ∗ Ahat ∗ D−0.5, (2)\nwhere A is the initial adjacency matrix, I is the\nidentity matrix and Dis the degree matrix.\nInitially, the whole corpus is stored in one text\nﬁle. Hence, the data should be transformed and\nstored using a different more ﬂexible format. For\neach sentence of the dataset, a distinct JSON ﬁle is\ncreated and contains a list with the tokens 1, a list\nwith named entity (NE) tags adopting the BIO en-\ncoding scheme (Sang and Veenstra, 1999; Ratinov\nand Roth, 2009), a list with token index pairs that\nare members of an existing relation, the padded\nencoded version of the sentence, the attention mask\nvector of the sentence, a list with the embeddings\nof each node of the graph and the normalized adja-\ncency matrix.\n2.2 Dataset Statistics\nThe ADE dataset is not ofﬁcially split into training,\nvalidation, and test sets. Hence, we evaluate our\nmodels using 10-fold cross-validation similar to Li\net al. (2017). We use the same splits as Eberts and\n1The sentence tokenization is performed using the SpaCy\nlibrary.\nUlges (2020). As Taillé et al. (2020) stresses, many\nworks on the IE task do not report the data prepro-\ncessing and detailed statistics of the datasets. This\nis an obstacle for a sanity check and reproducibility.\nThe ADE dataset consists of 4,272 sentences, with\n5,063 drug entities (1,048 unique drugs), 5,776 AE\nentities (2,983 unique AEs) and 6,821 relations.\nWe report the statistics of each split (Table 1) and\npropose using this particular split for a fair compar-\nison 2.\nSplit Training Set Test SetRelation Count Entity Count Relation Count Entity Count1 6,155 9,769 666 1,0702 6,097 9,713 724 1,1263 6,133 9,748 688 1,0914 6,164 9,771 657 1,0685 6,173 9,785 648 1,0546 6,089 9,713 732 1,1267 6,155 9,768 666 1,0718 6,117 9,754 704 1,0859 6,133 9,760 688 1,07910 6,173 9,770 648 1,069Mean 6,139 9,755 682 1,084\nTable 1: Statistics of 10-fold splits - ADE dataset\n3 Framework\nIn essence, contrastive learning is a paradigm for\nlearning representations which capture some aux-\niliary information by training them to distinguish\npositive from negative instances of this auxiliary in-\nformation. Our framework is inspired by the recent\npublications on image view-based CL of visual rep-\nresentation (Khosla et al., 2020; Zhang et al., 2020;\nHenaff, 2020; Chen et al., 2020; He et al., 2020),\nbut differs from the existing work by the applica-\ntion of CL to the graph and text modalities. Our\nwork is also inspired by the semantic bootstrapping\nhypothesis (Pinker, 1996), which proposes that chil-\ndren acquire their native language through expo-\nsure to sentences of the language (i.e., a language\nmodel) paired with structured representations of\ntheir meaning (Abend et al., 2017).\nThe main CL framework for imposing relation-\naware structure on the token embeddings is tested\nunder two different settings. The difference in each\nsetting is related to the modeling of the graph and\nthe level of applying the CL paradigm. To solve\nthe end-to-end IE task, a second model is proposed\nfor learning a distinct embedding space where the\nnamed entities are projected.\n2To facilitate further research, the preprocessed data and\nthe code will be publicly available in the ofﬁcial repository of\nthe paper.\n340\n3.1 Model Architectures\nIn the ﬁrst setting (Fig. 1), we apply the CL method\nto the embeddings of graph nodes in their graph\ncontext and the embeddings of sentence tokens in\ntheir sentence context. We call this variation in\nthe main CL framework CLGS. The positive and\nsampled negative graph representations are com-\nputed by a graph convolutional network (GCN)\n(Kipf and Welling, 2016; Schlichtkrull et al., 2018)\nlayer followed by a pooling layer. We model the\ngraph considering only the tokens that are part of\na relation (subgraphs). To obtain one representa-\ntion for the graph, average and maximum pooling\nstrategies are tried. Tanh (range: [-1, 1]) is chosen\nas the activation function of the GCN layer because\nthe text encoder also extracts negative embeddings.\nHence, a similar range of embedding values should\nbe extracted from the graph. The sentence is passed\nto the text encoder (CharacterBERT), which has the\nﬁrst six layers frozen. CharacterBERT is initialized\nwith the pretrained weights (medical version). A\npooling layer follows, to create a representation for\nthe whole sentence. Taking the average, maximum\nembedding vector and the [CLS] token representa-\ntion are tested as pooling strategies. The addition\nof a projection layers before applying CL is a com-\nmon approach (Chen et al., 2020; Zhang et al.,\n2020). ReLU is used as the activation function of\nthe projection layers to introduce nonlinearity. By\nadding the projection layers, there is the danger\nthat the task will be solved mainly in the projection\nlayers, while the ﬁnal goal is pushing structured\nrelation-aware information in the text encoder. Fi-\nnally, CL is applied to the resulting pair of graph\nand sentence representations, so that the pooled\nsentence token embeddings are trained to carry the\ninformation in the pooled graph node embeddings.\nIn the second setting, we apply the CL method to\nthe embeddings of graph relations and the embed-\ndings of pairs of sentence tokens. This variation\nin the CL framework is called CLDR. The graph\nis simpliﬁed to the extreme level. Each relation\nis modeled completely independently in the graph,\nand the relation representations are extracted by\nconcatenation of the nodes that are connected in\nthe disjoint graphs (Fig. 2). This graph modeling\nmakes the CL at the relation level a more tractable\ntask. In addition, sampling negative graphs can be\nimplemented more easily in a more controlled way.\nIn this setting, because the graphs only have two\nnodes, the adjacency matrix should not be normal-\nGCN CharacterBERT\nPoolingPooling\nProjection Projection\nNodes \nEmbeddings\nZ x (N x 768)\nNormalized \nAdjacency Matrix\nZ x (N x N)\nPadded Encoded \nSentence\n(1 x W)\nAttention \nMasks\n(1 x W)\nG S\nLS - G \nPre-trained\nFirst 6 layers \nfrozen\nZ: number of sampled graphs\nW x 768\n1 x 768\n1 x 512Z x 512\nZ x (1 x 768)\nZ x (N x 768)\n[CLS],\nMean,\nMax\nMean,\nMax\nSentence\nRepresentation\nGraph\nRepresentations\nFigure 1: CL framework CLGS - 1st Setting\nized in a balanced way. If the adjacency matrix\nis\n(0.5 0.5\n0.5 0.5\n)\n, then the ﬁnal node embeddings will\nbe the same for the two nodes that form the graph.\nHence, we suggest focusing more on the self-loop\nof each node to keep its predeﬁned contextualized\ninformation up to a certain level 3. The ﬁnal adja-\ncency matrix has the following format\n( λ 1−λ\n1−λ λ\n)\n,\nwhere λ is a hyperparameter of the model. The\nλ parameter deﬁnes the balance of focusing on\nthe self-loop of each node and its neighbor (con-\nnected node). Intuitively, a λvalue equal to 0.8 is a\ngood choice for focusing attention on the self-loop\nand having distinct embeddings for the connected\nnodes. ReLU is used as the activation function of\nthe GCN layer.\nGCN CharacterBERT\nConcatenated\nRelation\nRepresentations\nNodes \nEmbeddings\nR  x Z  x (2 x 768)\nNormalized \nAdjacency Matrix\nR  x Z  x (2 x 2)\nPadded Encoded \nSentence\n(1 x W )\nAttention \nMasks\n(1 x W )\nRG RS\nLRel \nPre-trained\nFirst 6 layers \nfrozen\nZ : number of sampled graphs\nW  x 768R  x Z  x (2 x 768)\nRelation \nRepresentations\nGraph side\nConcatenated\nRelation\nRepresentations\nRelation \nRepresentations\nSentence side\nR  x Z  x (1 x 1536)\nR  x (1 x 1536)\nR : number of relations \nin the sentence\nReLU\nR  x (1 x 1536)\nFigure 2: CL framework CLDR - 2nd Setting\n3We remind that the nodes are initialized with embeddings\nextracted from the pretrained CharacterBERT medical version.\n341\nOn the text side, the pair of tokens that form\na relation in the disjoint graphs are chosen, and\nthe concatenation of their representations is used\nas the ﬁnal relation representation. Finally, CL\nis applied on the relation level, so that the pairs\nof sentence token embeddings are trained to carry\nthe information in the pairs of related graph node\nembeddings.\nA distinct model (called CLNER) for learning\nmeaningful representations for named entities is\ndesigned (Fig. 3). CharacterBERT captures con-\ntextualised information very well. Hence, only one\ndense layer is added after CharacterBERT. Then\na random sampling for the named entities is per-\nformed in a balanced way. A pool of sampled\nentities of the batch is selected and CL is applied\non the token level.\nCharacterBERT\nAttention \nMasks\nB  x (1 x W )\nPadded Encoded \nSentence\nB  x (1 x W )\nPre-trained\nFirst 6 layers \nfrozen\nDense\nB  x (W  x 768)\nB  x (W  x 768)\nSampling\nS  x 768\nNE\nLNE\nNamed Entity\nRepresentations\nB : batch size\nS : Number of samples\n in the batch level\nFigure 3: Model CLNER for learning named entity rep-\nresentations\n3.2 Sampling Strategy\nHard negative sampling is important to effectively\napply the CL paradigm. The negative graphs are\ncreated by randomly selecting tokens that are not\npart of an adverse effect entity, keeping the cor-\nrect drug tokens, and vice versa. Hence, hard in-\ncorrect drug and adverse effects relation pairs are\nintroduced to the graph. The positive and negative\ngraphs of each sentence have the same number of\nrelations but not necessarily the same number of\nnodes. The sampling strategy is similar for the\nCLGS (Fig. 4) and the CLDR model (Fig. 5). For\nthe CLDR model, the positive graph is simpliﬁed to\na disjoint graph, and then hard negative sampling\nis performed.\nMethods: we report two cases of pseudoporphyria \ncaused by naproxen and oxaprozin.  \nnaproxen \noxaprozin \npseudoporphyria \nPositive subgraph Negative subgraphs\ncases \nreport \npseudoporphyria \nmethods \ncaused oxaprozin \nnaproxen \nFigure 4: Example of sampling negative graphs -CLGS\nmodel\nMethods: we report two cases of pseudoporphyria \ncaused by naproxen and oxaprozin.  \nnaproxen \noxaprozin \npseudoporphyria \nPositive subgraph\nNegative subgraphs\ncases pseudoporphyria methods \ncaused oxaprozin \nnaproxen \nDisjoint positive subgraph\npseudoporphyria \npseudoporphyria oxaprozin \nnaproxen \npseudoporphyria report \nFigure 5: Example of sampling negative graphs -\nCLDR model\nIn the CLNER, random sampling4 is executed at\nthe batch level. Analysis of the number of differ-\nent entity tags (drug, AE or outside token) in the\nbatch is performed a priori to choose an appropriate\nnumber of positive and negative samples (balanced\nsampling).\n4Hard negative sampling based on the Euclidean distance\nand cosine similarity is also tested, but the performance is not\nincreased. Hence, the complexity-performance trade-off leads\nus to ﬁnally select random sampling.\n342\n3.3 Design Choices\nIn this subsection, the justiﬁcation for the model\ndesign choices is discussed. For the CLGS and\nCLDR models, the GCN layer is the key element\nbecause it can produce useful node representations\nconsidering the graph links. The propagation rule\nof the GCN layer is described by the following\nequation:\nXl+1 = σ(Anorm ∗ Xl ∗ Wl), (3)\nwhere σ(·) is the activation function (e.g., ReLU,\nTanh), Anorm is the normalized adjacency matrix\n(Eq. 2), Xl the node embeddings and Wl the\nweights of the l layer.\nIn the ﬁrst setting ( CLGS model), the graph is\npropagated through the GCN layer, and a ﬁnal\npooled graph representation is extracted. We hy-\npothesize that using the CL paradigm, the model\ncan learn which part of the information is essen-\ntial for the relation representations by keeping the\nstructure-related information in the graph represen-\ntation. In the second setting ( CLDR model), the\nlevel of abstraction is reduced because instead of\napplying CL in the graph sentence, we use the CL\nparadigm at the relation level. The strategy of cre-\nating disjoint graphs results in learning similar rep-\nresentations for the drug and AE nodes. To address\nthis, the relations are represented asymmetrically as\na concatenation of the nodes. We hypothesize that\nrelation-related information can be imposed in the\npair-of-tokens embeddings of the LM by applying\nCL to them and these pair-of-nodes embeddings of\nthe graph relations (Fig. 2).\n3.4 Training Details\nThe models are trained using a CL loss function\nthat is similar to the SimCLR loss function (Chen\net al., 2020). In the ﬁrst setting ( CLGS model),\nthe main concept is to leverage the two graph and\nsentence representations so the true representation\npair is close and similar in the learned embedding\nspace. At each training time, a set of Zgraphs (the\npositive and some negative graphs) and the corre-\nsponding sentence are passed on the model, and\nthe corresponding representations are calculated.\nTherefore, the contrastive loss receives the graph\nand sentence representations and for the i-th pair is\nas follows:\nl(S→G)\ni = −log( exp(<Si,Gi>/τ)∑Z\nz=1 exp(<Si,Gz>/τ) ), (4)\nwhere <Si,Gi >represents the cosine similar-\nity and τ is a temperature parameter.\nIn the second setting ( CLDR model), the pair\nof node embeddings that are extracted from the\ndisjoint graphs encode their relation, because this is\nthe main functionality of the GCN layer. Hence, the\nmain idea is to increase the similarity between the\nrepresentations of the correct relation in the graph\nand the relation representations that are extracted\nfrom the text encoder.\nThe contrastive loss for each sentence is as fol-\nlows:\nl(RS→RG) = ∑R\nr=1−log( exp(<RSr,RGr>/τ)∑Z\nz=1exp(<RSr,RGz>/τ)), (5)\nwhere Ris the total number of relations in the\nsentence, RS is the relation representation of the\ntext encoder and RGis the relation representation\nof the graph.\nFor the CLNER model, the contrastive loss is as\nfollows:\nlNE = ∑N\nn=1 −log(\n∑P\np=1 exp(<RNn,RNp>/τ)∑K\nk=1 exp(<RNn,RNk>/τ)), (6)\nwhere N is the total number of tokens in the\nbatch, P is the number of the positive samples\n(same NE tag), K is the total number of samples\nand RN is the extracted token representation.\nWe use a batch-size of 8 for training the CLGS\nand CLDR models, and 16 for the CLNER model.\nADAM optimizer (Kingma and Ba, 2014) is se-\nlected with a learning rate of 1e-5 5.\n4 Evaluation - Baseline\nFor the CLGS model, the ﬁrst evaluation step is a\nsimple similarity check. We use the trained CLGS\nmodel to extract the sentence representation and\nthe positive and negative graph representations for\nall the sentences in the test set. Then, a similarity\ncheck is applied using the extracted sentence and\ngraph representations. The most similar graph is\npredicted as the positive sentence graph. Given the\npositive and all the negative hard graphs extracted\nfrom each sentence, the model should be able to\ndetect the correct graph. The different model varia-\ntions perform well, but the mean pooling selection\nin the graph and sentence side results in better per-\nformance, as the accuracy is over 91%. The addi-\ntion of the projection layers is not advantageous.\n5More information about hyperparameter tuning-selection\nis given in the Appendix section.\n343\nGraph Pooling Text Pooling Projection layer Accuracy\nMean [CLS] - 88.39\nMean Mean - 91.23\nMax Max - 89.1\nMean [CLS] Yes 88.63\nMean Mean Yes 87.68\nTable 2: Results - CLGS model: Finding the correct\ngraph with similarity check\nThe second evaluation step is applied to both\nmodels (CLGS and CLDR). Following previous re-\nsearch on representation learning (Henaff, 2020;\nChen et al., 2020; He et al., 2020; Zhang et al.,\n2020), we evaluate the tuned CharacterBERT text\nencoder, taken from the trained CLGS and CLDR\nmodels, in a linear classiﬁcation setting, where all\nthe candidate relations (concatenation of the token\nembeddings) are created, and a linear classiﬁca-\ntion layer is trained for the RE task. As a baseline\nmodel, we use the pretrained medical Character-\nBERT to create the representation for the relations6.\nThis linear setting directly provides insight into\nhow successfully the relation-related structure is\nimposed at the token level of the text encoder, by\nevaluating the quality of the learned representations\nfor RE.\nModel Precision Recall F1\nBaseline 69.96 64.39 66.79\nCharacterBERTCLGS 56.82 59.42 58.09\nCharacterBERTCLDR 79.51 84.39 81.73\nTable 3: RE - linear classiﬁcation setting\nUsing the tuned CharacterBERT representation\nfrom the CLGS model (mean graph and text pool-\ning) results in poor performance. The pooling layer\nsmooths the information. Hence, structure-related\ninformation cannot be passed at the token level of\nthe text encoder. A smarter pooling strategy that\npreserves most of the relation-aware information\nwould be ideal, but designing such pooling is dif-\nﬁcult. The main obstacle is the varied number of\nrelations. In contrast, when we use the tuned Char-\nacterBERT of the CLDR model, the basic classiﬁer\nvastly outperforms the baseline model. This is a\nstrong indication that the relation-related structure\nis successfully imposed on the pairs of token em-\nbeddings of the text encoder.\n6We also try ﬁne-tuning both the text encoder and the\nlinear head, but the performance is not improved.\n5 tSNE Analysis\nA tSNE analysis is performed to further explore\nthe quality of the learned embedding spaces. Us-\ning the tuned CharacterBERT of the CLDR model,\nthe relation representation space is created. We\nproject the positive (orange dots) and hard negative\nrelations (blue dots), where one of the two rela-\ntion tokens is correct. In the tSNE plot (Fig. 6),\nmeaningful relation clusters can be easily identiﬁed,\nwhich demonstrates the efﬁciency of our frame-\nwork (CLDR model). The relation representations\nare asymmetric, as the drug and AE tokens have\nsimilar representations (Fig. 7). This means that\nwe cannot solve RE and NER tasks using the same\nrepresentation space. Hence, we learn a different\nspace for the named entities (CLNER model).\nFigure 6: tSNE plot - Relation representation space ob-\ntained with CharacterBERT of CLDR model (1: rela-\ntion, 0: no relation)\nFigure 7: tSNE plot - Relation representation space ob-\ntained with CharacterBERT of CLDR model - Named\nEntities\nIn the tSNE plot in the entity representation\nspace (Fig. 8), we can detect insightful entity clus-\nters. In particular, the clusters related to the drug\ntags (B-DRUG, I-DRUG) are very dense and well\n344\nshaped. This is a strong ﬁnding that illustrates that\nthe CLNER model can extract very good represen-\ntations for the NER task.\nFigure 8: tSNE plot - Entity representation space ob-\ntained with CLNER model\n6 Entity-Relation task\nThe insights of the tSNE analysis, with the well-\ndeﬁned clusters in the embedding spaces, lead us to\napproach the entity-relation task using intuitively\nsimple and transparent KNN classiﬁers. For the\nRE task, we utilize the tuned CharacterBERT of\nthe CLDR model to create the candidate relation\nrepresentations. At the inference step, for each\ncandidate relation, we decide whether it is positive\nbased on the labels of thek-nearest neighbors in the\nlearned embedding space. The value of kis chosen\nbased on the performance in the randomly selected\nvalidation set (10% of training set) for each fold.\nWe adopt the same strategy for the NER task using\nthe CLNER model and project each token to the\nnamed entity representation space.\nTo solve both NER and RE tasks, we combine\nthe two semantic spaces. First, we determine\nwhether a candidate relation (concatenation of the\ntokens) is predicted as positive in the relation rep-\nresentation space, which is obtained by the tuned\nCharacterBERT of the CLDR model. Then, we\ndetermine whether the boundaries and the types of\nthe two entities in the candidate relation are pre-\ndicted correctly in the entity representation space\nobtained by the CLNER model. All possible candi-\ndate relations and the named entities of the test set\nare classiﬁed.\nWe strictly evaluate the performance of the IE\ntask. As Bekoulis et al. (2018b) state, an entity is\nconsidered correct if its boundaries are detected cor-\nrectly and the predicted type (drug or AE) matches\nthe ground truth. In the same setup, a relation is\nconsidered correct if its type and the two entities\n(boundaries and type) involved in the relation are\ncorrectly predicted. We measure precision, recall\nand F1 score. Following previous work on IE, we\nreport the macro-averaged F1 score, and as 10-fold\ncross-validation is adopted, we average the scores\nover the folds.\nModel NER RE RE-\nLi et al., 2016 79.5 63.4 -\nLi et al., 2017 84.6 71.4 -\nBekoulis et al., 2018b 86.4 74.58 -\nBekoulis et al., 2018a 86.73 75.52 -\nTran and Kavuluru, 2019 87.11 77.29 -\nEberts and Ulges, 2020 89.25 79.24 -\nWang and Lu, 2020 89.7 80.1 -\nZhao et al., 2020 89.4 81.14 -\nOurs 88.3 79.97 86.5\nTable 4: Test set results: macro-averaged F1 score\nTable 4 presents the best performing models,\nevaluated on the ADE (Gurulingappa et al., 2012)\ndataset. These studies address the IE problem as\na joint task, solving NER and RE tasks jointly. Li\net al. (2016) employ global features and a CNN\n(LeCun et al., 1995) module to solve the problem.\nThe proposed model of Li et al. (2017) includes\nbidirectional RNNs (Graves et al., 2013), inspired\nby the work of Miwa and Bansal (2016). Bekoulis\net al. (2018a,b) formulate the IE problem as a multi-\nhead selection problem. Tran and Kavuluru (2019)\napproach the IE task as a table-ﬁlling problem and\nintroduce a relation-metric network, combining the\nidea of metric learning and the usage of CNNs\nfor table ﬁlling. Eberts and Ulges (2020) present\na span-based model that its core module is pre-\ntrained BERT (Devlin et al., 2018). Wang and Lu\n(2020) propose table-sequence encoders that learn\ntable and sequence representations to solve the IE\nproblem. Zhao et al. (2020) introduce a deep cross-\nmodal attention network, constructed by stacking\nmultiple attention units, for joint entity and relation\nextraction.\nIn the RE task, we achieve very competitive re-\nsults using a simple and transparent KNN classiﬁer.\nIn contrast, the state-of-the-art models (Wang and\nLu, 2020; Zhao et al., 2020) are very complex and\ncomputationally expensive. This fact highlights\nthe high quality of the learned relation representa-\ntion space (CLDR model). In principle, the NER\ntask is a sequence-tagging problem. However, we\nobtain good performance with a KNN classiﬁer\n345\nthat performs the inference in the learned entity\nrepresentation space (CLNER model).\nNotably, the last column of Table 4 ( RE-)\npresents the performance of the RE KNN classi-\nﬁer in predicting whether there is a relation be-\ntween two tokens, without considering the NER\ntask (type and boundaries of the entities). In this\ncase, the F1 score is 86.5, and this value is the up-\nper bound performance of our approach. Hence,\nincorporating a state-of-the-art model for the NER\ntask (e.g., Wang and Lu, 2020, Eberts and Ulges,\n2020) could further improve the scores of the RE\ntask under strict evaluation. However, we use the\nSpERT model (Eberts and Ulges, 2020) for NER\n(F1 score: 89.25), but the results in the RE task are\nnot improved. This illustrates that our NER results\nare already very competitive.\nThe above results reveal the quality of the rep-\nresentations for both NER and RE tasks. Hence,\nthe proposed CL framework can be used as a pre-\nprocessing and representation learning step in the\npipeline for IE models. The CL framework can be\ntrained to leverage the embedding space and create\nmeaningful, disentangled representations for the IE\ntask. We successfully evaluated the representations\nwith a simple KNN classiﬁer, but the learned repre-\nsentations can be used as input in complex models\nfor entity and relation classiﬁcation to achieve bet-\nter results and faster convergence. We will explore\nthis research direction in the future.\n7 Conclusion\nWe present a novel CL framework, which, in princi-\nple, is text encoder-agnostic, for effectively impos-\ning relation-related structure to LMs and leveraging\nthe embedding space. We evaluate the quality of\nthe learned representations using relative baselines\nand competitively solve an entity-relation task. The\noverall results indicate that the learned represen-\ntations are very powerful. The performed tSNE\nanalysis illustrates that meaningful clusters can be\neasily identiﬁed in the learned embedding spaces.\nWe note that the proposed framework can be used\nas a representation learning step for complex IE\nsystems. In future work, we intend to explore\nthe capabilities of our approach in continual learn-\ning settings and exploit external graph structured\nknowledge in representation learning of language\ndata.\nAcknowledgments\nThis work is supported by the Research Founda-\ntion – Flanders (FWO) and Swiss National Science\nFoundation (SNSF). Christos Theodoropoulos and\nMarie-Francine Moens are afﬁliated to Leuven.AI -\nKU Leuven institute for AI, B-3000, Leuven, Bel-\ngium.\nReferences\nOmri Abend, Tom Kwiatkowski, Nathaniel J Smith,\nSharon Goldwater, and Mark Steedman. 2017. Boot-\nstrapping language acquisition. Cognition, 164:116–\n143.\nGiannis Bekoulis, Johannes Deleu, Thomas Demeester,\nand Chris Develder. 2018a. Adversarial training\nfor multi-context joint entity and relation extraction.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2830–2836.\nGiannis Bekoulis, Johannes Deleu, Thomas Demeester,\nand Chris Develder. 2018b. Joint entity recogni-\ntion and relation extraction as a multi-head selection\nproblem. Expert Systems with Applications, 114:34–\n45.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In In-\nternational Conference on Machine Learning, pages\n1597–1607. PMLR.\nJames R Curran and Stephen Clark. 2003. Language\nindependent ner using a maximum entropy tagger.\nIn Proceedings of the Seventh Conference on Natu-\nral language Learning at HLT-NAACL 2003, pages\n164–167.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nMarkus Eberts and Adrian Ulges. 2020. Span-based\njoint entity and relation extraction with transformer\npre-training. pages 2006–2013.\n346\nHicham El Boukkouri, Olivier Ferret, Thomas\nLavergne, Hiroshi Noji, Pierre Zweigenbaum, and\nJun’ichi Tsujii. 2020. Characterbert: Reconciling\nelmo and bert for word-level open-vocabulary rep-\nresentations from characters. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, pages 6903–6915.\nRadu Florian, Hongyan Jing, Nanda Kambhatla, and\nImed Zitouni. 2006. Factorizing complex models: a\ncase study in mention detection. In Proceedings of\nthe 21st International Conference on Computational\nLinguistics and the 44th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 473–\n480.\nRadu Florian, John F Pitrelli, Salim Roukos, and Imed\nZitouni. 2010. Improving mention detection robust-\nness to noisy input. In Proceedings of the 2010 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 335–345.\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey\nHinton. 2013. Speech recognition with deep recur-\nrent neural networks. In 2013 IEEE international\nconference on acoustics, speech and signal process-\ning, pages 6645–6649. Ieee.\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus\nRoberts, Juliane Fluck, Martin Hofmann-Apitius,\nand Luca Toldo. 2012. Development of a benchmark\ncorpus to support the automatic extraction of drug-\nrelated adverse effects from medical case reports.\nJournal of Biomedical Informatics, 45(5):885–892.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. 2020. Momentum contrast for unsu-\npervised visual representation learning. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 9729–9738.\nOlivier Henaff. 2020. Data-efﬁcient image recognition\nwith contrastive predictive coding. In International\nConference on Machine Learning, pages 4182–4192.\nPMLR.\nJing Jiang and ChengXiang Zhai. 2007. A systematic\nexploration of the feature space for relation extrac-\ntion. In Human Language Technologies 2007: The\nConference of the North American Chapter of the\nAssociation for Computational Linguistics; Proceed-\nings of the Main Conference, pages 113–120.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron\nSarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\npervised contrastive learning. In Advances in Neural\nInformation Processing Systems , volume 33, pages\n18661–18673. Curran Associates, Inc.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nThomas N Kipf and Max Welling. 2016. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907.\nYann LeCun, Yoshua Bengio, et al. 1995. Convolu-\ntional networks for images, speech, and time series.\nThe handbook of brain theory and neural networks ,\n3361(10):1995.\nFei Li, Meishan Zhang, Guohong Fu, and Donghong Ji.\n2017. A neural joint model for entity and relation ex-\ntraction from biomedical text. BMC bioinformatics,\n18(1):1–11.\nFei Li, Yue Zhang, Meishan Zhang, and Donghong\nJi. 2016. Joint models for extracting adverse drug\nevents from biomedical text. In Proceedings of\nthe Twenty-Fifth International Joint Conference on\nArtiﬁcial Intelligence , IJCAI’16, page 2838–2844.\nAAAI Press.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMakoto Miwa and Mohit Bansal. 2016. End-to-end re-\nlation extraction using LSTMs on sequences and tree\nstructures. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1105–1116, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nDavid Nadeau and Satoshi Sekine. 2007. A survey of\nnamed entity recognition and classiﬁcation. Lingvis-\nticae Investigationes, 30(1):3–26.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nSteven Pinker. 1996. Language Learnability and Lan-\nguage Development: With New Commentary by the\nAuthor, volume 7. Harvard University Press.\nBarbara Plank and Alessandro Moschitti. 2013. Em-\nbedding semantic similarity in tree kernels for do-\nmain adaptation of relation extraction. In Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1498–1507.\nLev Ratinov and Dan Roth. 2009. Design chal-\nlenges and misconceptions in named entity recog-\nnition. In Proceedings of the Thirteenth Confer-\nence on Computational Natural Language Learning\n(CoNLL-2009), pages 147–155.\nErik F Tjong Kim Sang and Jorn Veenstra. 1999. Rep-\nresenting text chunks. In Proceedings of the Ninth\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 173–179.\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem,\nRianne Van Den Berg, Ivan Titov, and Max Welling.\n2018. Modeling relational data with graph convolu-\ntional networks. In European Semantic Web Confer-\nence, pages 593–607. Springer.\n347\nAng Sun, Ralph Grishman, and Satoshi Sekine. 2011.\nSemi-supervised relation extraction with large-scale\nword clustering. In Proceedings of the 49th An-\nnual Meeting of the Association for Computational\nLinguistics: Human Language Technologies , pages\n521–529.\nBruno Taillé, Vincent Guigue, Geoffrey Scoutheeten,\nand Patrick Gallinari. 2020. Let’s stop error prop-\nagation in the end-to-end relation extraction litera-\nture! In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP 2020), pages 3689–3701.\nTung Tran and Ramakanth Kavuluru. 2019. Neural\nmetric learning for fast end-to-end relation extrac-\ntion. arXiv preprint arXiv:1905.07458.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research, 9(11).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nJue Wang and Wei Lu. 2020. Two are better than\none: Joint entity and relation extraction with table-\nsequence encoders. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP 2020), pages 1706–1721. Asso-\nciation for Computational Linguistics.\nYuhao Zhang, Hang Jiang, Yasuhide Miura, Christo-\npher D Manning, and Curtis P Langlotz. 2020.\nContrastive learning of medical visual representa-\ntions from paired images and text. arXiv preprint\narXiv:2010.00747.\nShan Zhao, Minghao Hu, Zhiping Cai, and Fang Liu.\n2020. Modeling dense cross-modal interactions for\njoint entity-relation extraction. pages 4032–4038.\nShubin Zhao and Ralph Grishman. 2005. Extracting\nrelations with integrated information using kernel\nmethods. In Proceedings of the 43rd Annual Meet-\ning of the Association for Computational Linguistics\n(ACL’05), pages 419–426.\nTraining Details - Hyperparameters\nInitially, the data are split to train and test set using\n10-fold cross-validation (same splits to Eberts and\nUlges (2020)). In each training session, for each\nsplit, the seed number is set to 42 in order to ran-\ndomly create a validation set for tuning (10% of\nthe train set). The seed number is chosen in order\nto have the same split of train and validation set in\nall training sessions of the CL framework and the\nbaseline and KNN classiﬁers. ADAM optimizer\n(Kingma and Ba, 2014) is selected with learning\nrate 1e-5 for training of the CL framework. The\nbest weights, based on the performance in the vali-\ndation set, are stored.\nWe train the CL framework ( CLGS, CLDR,\nCLNER models) for 20 epochs and apply the tech-\nnique of early stopping after 3 epochs without im-\nprovement in the validation set. We experiment\nwith different hyperparameter values and select the\nbest values based on the performance in the val-\nidation set (averaged across the 10 splits) in the\nbasic classiﬁer that is presented in section 4. For\nthe CLGS and CLDR models, the different negative\ngraphs of each sentence are created ofﬂine. The\nlength of sentences varies signiﬁcantly, so the num-\nber of negative graphs also varies. Based on that,\nrandomly selecting 8 negative graphs for each train-\ning set is intuitively a good choice. In parentheses,\nthere are the tested values. The hyperparameters of\nthe CLGS model are:\n• Batch Size: 8 (8, 16)\n• Temperature τ: 0.1 (0.05, 0.1, 0.2)\n• Number of negative graphs: 8 (4, 8, 12)\nThose of the CLDR model are:\n• Batch Size: 8 (8, 16)\n• λparameter (adjacency matrix): 0.8 (0.7, 0.8,\n0.9)\n• Temperature τ: 0.1 (0.05, 0.1, 0.2)\n• Number of negative graphs: 8 (4, 8, 12)\nThe essential parameter of the CLNER model is\nthe number of samples. The number of available to-\nkens depends on the batch size. In order to sample\nin a balanced way (Table 5), when the batch size\nis 16, a good number of samples is 80. For exam-\nple, if we have a ’B-DRUG’token, then we sample\nall the tokens with the same tag (positive tokens -\naround 20, Table 5) and the remaining negative to-\nkens are sampled in a balanced way. This sampling\nstrategy should be deﬁned because the NE token\ndistribution is highly imbalanced (Table 6). The\n’O’tag is highly represented, while the ’I-DRUG’\ntag is under-represented. The temperature value τ\nis set to 0.1.\nThe KNN classiﬁer has only one hyperparameter,\nthe number of k neighbors that are taken into ac-\ncount in the inference step. We choose the k value\nbased on the performance in the validation set for\neach split. The k value is 5 for the RE KNN clas-\nsiﬁer, and 7 for the NER KNN classiﬁer (section\n6).\n348\nNE type Count\nB-DRUG 19\nI-DRUG 4\nB-AE 21\nI-AE 26\nO 268\nTable 5: Average number of tokens per NE tag - Batch\nsize: 16\nNE type Count\nB-DRUG 5,039\nI-DRUG 1,062\nB-AE 5,701\nI-AE 7,054\nO 71,858\nTable 6: Total number of tokens per NE tag",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7806234359741211
    },
    {
      "name": "Sentence",
      "score": 0.7155109643936157
    },
    {
      "name": "Natural language processing",
      "score": 0.696366548538208
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6478482484817505
    },
    {
      "name": "Relationship extraction",
      "score": 0.5655070543289185
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5348789691925049
    },
    {
      "name": "Relation (database)",
      "score": 0.4545520842075348
    },
    {
      "name": "ENCODE",
      "score": 0.4524204134941101
    },
    {
      "name": "Graph",
      "score": 0.437549352645874
    },
    {
      "name": "Feature learning",
      "score": 0.41719821095466614
    },
    {
      "name": "Information extraction",
      "score": 0.3254713714122772
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2031090259552002
    },
    {
      "name": "Data mining",
      "score": 0.08434361219406128
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}