{
  "title": "End-to-End Neural Transformer Based Spoken Language Understanding",
  "url": "https://openalex.org/W3080993257",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5023415990",
      "name": "Martin Radfar",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A5029573067",
      "name": "Athanasios Mouchtaris",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A5074068808",
      "name": "Siegfried Kunzmann",
      "affiliations": [
        "Amazon (Germany)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2599118679",
    "https://openalex.org/W2399456070",
    "https://openalex.org/W2153962611",
    "https://openalex.org/W3005910077",
    "https://openalex.org/W3015412890",
    "https://openalex.org/W2963288440",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3006901707",
    "https://openalex.org/W2024632416",
    "https://openalex.org/W2137871902",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2091671846",
    "https://openalex.org/W2397579082",
    "https://openalex.org/W2964052309",
    "https://openalex.org/W2928075308",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2891229414",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W1984076147",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3104113994",
    "https://openalex.org/W3005915390",
    "https://openalex.org/W3099944122",
    "https://openalex.org/W2894164357",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964108264",
    "https://openalex.org/W2327501763"
  ],
  "abstract": "Spoken language understanding (SLU) refers to the process of inferring the semantic information from audio signals. While the neural transformers consistently deliver the best performance among the state-of-the-art neural architectures in field of natural language processing (NLP), their merits in a closely related field, i.e., spoken language understanding (SLU) have not beed investigated. In this paper, we introduce an end-to-end neural transformer-based SLU model that can predict the variable-length domain, intent, and slots vectors embedded in an audio signal with no intermediate token prediction architecture. This new architecture leverages the self-attention mechanism by which the audio signal is transformed to various sub-subspaces allowing to extract the semantic context implied by an utterance. Our end-to-end transformer SLU predicts the domains, intents and slots in the Fluent Speech Commands dataset with accuracy equal to 98.1 \\%, 99.6 \\%, and 99.6 \\%, respectively and outperforms the SLU models that leverage a combination of recurrent and convolutional neural networks by 1.4 \\% while the size of our model is 25\\% smaller than that of these architectures. Additionally, due to independent sub-space projections in the self-attention layer, the model is highly parallelizable which makes it a good candidate for on-device SLU.",
  "full_text": "End-to-End Neural Transformer Based Spoken Language Understanding\nMartin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann\nAlexa Machine Learning, Amazon\n{radfarmr,mouchta,kunzman}@amazon.com\nAbstract\nSpoken language understanding (SLU) refers to the pro-\ncess of inferring the semantic information from audio signals.\nWhile the neural transformers consistently deliver the best per-\nformance among the state-of-the-art neural architectures in ﬁeld\nof natural language processing (NLP), their merits in a closely\nrelated ﬁeld, i.e., spoken language understanding (SLU) have\nnot beed investigated. In this paper, we introduce an end-to-\nend neural transformer-based SLU model that can predict the\nvariable-length domain, intent, and slots vectors embedded in\nan audio signal with no intermediate token prediction architec-\nture. This new architecture leverages the self-attention mecha-\nnism by which the audio signal is transformed to various sub-\nsubspaces allowing to extract the semantic context implied by\nan utterance. Our end-to-end transformer SLU predicts the do-\nmains, intents and slots in the Fluent Speech Commands dataset\nwith accuracy equal to 98.1 %, 99.6 %, and 99.6 %, respectively\nand outperforms the SLU models that leverage a combination\nof recurrent and convolutional neural networks by 1.4 % while\nthe size of our model is 25% smaller than that of these architec-\ntures. Additionally, due to independent sub-space projections in\nthe self-attention layer, the model is highly parallelizable which\nmakes it a good candidate for on-device SLU.\nIndex Terms: Spoken language understanding, sequence-to-\nsequence, neural transformer, encoder-decoder. domain-intent-\nslot\n1. Introduction\nSpoken language understanding (SLU) systems extract seman-\ntic information from a spoken utterance by machine [1]. The Air\nTravel Information System (ATIS) was the ﬁrst SLU model built\nbased on a cascade of a speech recognizer, a language model,\na semantic extractor-SQL generator (NLU) in 1990 [2]. Thirty\nyears after ATIS, designing an end-to-end (E2E) neural SLU\nthat can replace the ASR+NLU-based SLU technology still re-\nmains a challenge [3–18]. Ideally, we would like to have an\nall-neural model whose layers project the audio signal to hid-\nden semantic representations, the-so-called ”thought vectors”\n[19] to infer the domain, intent, and slots implied by the audio\nsignal.\nTo achieve this goal, several groups conducted experiments\nusing non-ASR awareness E2E SLU[3, 7, 13, 20, 21]. These\nmodels usually apply multiple stack of RNNs [4, 10–12, 20, 21]\nto encode the entire utterance to a vector which is fed to a fully\nconnected feedforward neural network followed by a soft-mask\nor a max-pool layer to identify the domain, intent, or slot. These\nmodels treat each unique combination of domain, intent, and\nslots as an output label. For this reason, we call this type of\nE2E SLU classiﬁcation-based approaches. The limitation of\nclassiﬁcation-based approaches is that the combination of do-\nmains, intents, and slots may grow exponentially, subsequently\nwe deal with a classiﬁcation problem with many number of out-\nput labels; moreover, the number of intents is not usually ﬁxed\nwhich makes usability of classiﬁcation-based approaches more\nlimited.\nA natural approach to deal with the variable-length output\nfor E2E SLU is to use the sequence-to-sequence (seq2seq) neu-\nral models [22]. In [5], several seq2seq architectures are pro-\nposed for E2E SLU, among which the authors found the model\nthat incorporates an ASR-awareness module in form of a multi-\ntask learner delivers the best performance. The ﬁnding is further\nsupported by a recent proposed pre-trained ASR-awareness ar-\nchitecture [4].\nIn ASR, the input and output sequences are ordered and\nmonotonic. As such, we don’t need the entire utterance to de-\ncode the transcription. In contrast, when extracting semantic in-\nformation from audio signals, we usually need to scan the entire\naudio. Similar to neural machine translation [23, 24], the E2E\nSLU models can massively beneﬁt from the attention mecha-\nnism. In attention mechanism, the encoder generates the out-\nputs by incorporating the hidden representations from all time\nsteps and hence allows the output to pay attention to inputs at all\ntime steps [25]. The neural attention models have demonstrated\npromising results in ASR as well [26, 27].\nThe transformers are seq2seq, non-recurrent, self-attention\nneural models that have been used in neural machine transla-\ntion as well as NLU with great success [14, 23–25, 28]. In\nthis paper, we leverage the transformer architecture for E2E\nSLU. Neural transformers have several distinct features which\nmake them suitable candidate for SLU task: (a) The trans-\nformers use the self-attention mechanism that allows to com-\npute the correlation in each sublayer between all pairs of time\nsteps both in the encoder and decoder. (b) Sub-spaces projec-\ntion by self-attention helps extract semantic context from audio\nframes. (c) The transformers can beneﬁt from distributed train-\ning because linear transformation in self-attention can be par-\nallelized. (d) Compared to RNN models [4, 10–12], the trans-\nformers have less number of parameters. Our model works both\nin a classiﬁcation-based mode and in a hierarchical mode that\nallows to decode variable length domain, intent and slot vec-\ntors. We compare this new architecture with E2E SLU mod-\nels that use both RNN and CNN on the recently publicly re-\nleased dataset called Fluent Speech Commands [4, 21]. Our\nresults show that the transformer-based SLU outperforms the\nRNN+CNN based model, while it has less numbers of parame-\nters.\nThe rest of this paper is organized as follows: Section 2 for-\nmulates the problem. In Section 3, we describe the transformer\nbased SLU in details. Section 4 gives the details of experiments\nand results. Finally, we draw the conclusion and give the future\ndirections in Section 5.\n2. Problem Formulation\nGiven an utterance transformed to the feature space Xp×T ,\nwhere pand T are the dimension of the feature space and the\narXiv:2008.10984v1  [cs.CL]  12 Aug 2020\nE2E SLU\nDomainIntent Slot 1Slot2….Slot NOutput\nPlay Michael Jackson in the bedroomExample:\nInput:DomainIntent Slot 1 Slot2MusicPlay MusicBedroomMichael Jackson\nOutput:\nInput:\nFigure 1: The SLU task: infer the domain, intent, and slots\ngiven an utterance\nnumber of frames, respectively, we would like to design an all-\nneural architecture that predicts the output vector y(M+2)×1 =\n[yD,yI,ys1 ,ys2 ,...,y sM ]T which consists of the domain, in-\ntent, and slots implied by this utterance. Generally, for a given\nutterance, we have one domain, one intent, and multiple slots;\nan example of a typical SLU task depicted in Figure 1. The to-\ntal number of output labels for this classiﬁcation problem could\nbe as high as ND ×NI ×Ns1 ×Ns2 ×... ×NsM where\nND,NI,Nsi denote the number of domains, the number of in-\ntents, and the number of the ith slot. Hence, given N input-\noutput training samples (Xj\np×T,yj\n(M+2)×1),j = 1,...,N , we\nwould like to train a neural model that can predict the domain,\nintent, and slot for a test utterance.\nIn this paper, we consider two models: 1- A classiﬁcation-\nbased model in which we haveND ×NI ×Ns1 ×Ns2 ×... ×\nNsM classes and we assign one output label for each audio\nﬁle. In other words, each unique combination of domain, in-\ntent, and slots is considered as an output class. In this model,\nthe decoder is basically a fully connected feedforward neural\nnetwork followed by a softmax. This model works well when\nthe number of classes are limited. 2- A hierarchical model is\nsimilar to seq2seq neural architectures [22] and works as fol-\nlows: Let the vector y(M+2)×1 = [yD,yI,ys1 ,ys2 ,...,y sM ]T\ndenote the domain, intent, and slots implied by Xp×T . We\naugment y(M+2)×1 by two symbols which represent the start\nand end of this phrase ( sop and eop), i.e. ˆy(M+2)×1 =\n[sop,yD,yI,ys1 ,ys2 ,...,y sM ,eop]T . In the decoding step,\nwe ﬁrst predict yD by inputing sopto the input of the decoder\nalong with the encoder output. Next, the predicted yD is fed\nto the decoder along with the encoder output to predict yI and\nthis process is repeated until the predicted output is eop. We\ncall this model E2E hierarchical transformer SLU to distinguish\nfrom the E2E classiﬁcation-based transformer SLU.\n3. Model\nThe proposed SLU model is based on the neural transform-\ners. The transformer architecture consists of three sub-modules:\nembedding, encoder, and decoder. The embedding is a linear\ntransformation on the input vector followed by adding a po-\nsitional encoding vector to the transformed input to properly\nencode the order of the frames. The encoder consists of two\nblocks: self-attention, and a fully connected feed-forward neu-\nral network (FFNN). The stack of self-attention and FFNN com-\nprises a layer. Depending on application, we use different num-\nber of layers. The self-attention carries out multiple sub-space\ntransformations; more speciﬁcally, the self-attention sub-layer\ncomputes the correlation between all frame pairs in two sub-\nspaces and outputs the sum of weighted sub-spaces vectors .\nThe weight values determine the amount of correlation between\na frame and others. In order to capture more hidden correlation\nrepresentation, this process is repeated in parallel in different\nsubspaces (the so-called multi-head attention) and resulting at-\ntention vectors are appended and linearly transformed and are\nfed to the FFNN. In oder to speed up the learning process and\nprevent the network forgetting the previous layer representa-\ntions, we apply layer normalization [29] and residual connec-\ntion [30] at the outputs of the self-attention, and FFNN sub-\nlayers.\nThe above description can be represented based on a linear\nalgebraic view which mostly consists of matrix sub-space anal-\nysis and transformation. More speciﬁcally, let ×and ⊙denote\nthe matrix multiplication and dot product, respectively. Also, let\nxi ∈Rp,i = 1,2,...,T denote the ith spectral feature vector\nof an utterance which is segmented toT overlapping frames and\ntransformed to the spectral features (here we use low frame rate\nlog STFT).\nThe ﬁrst step is embedding in which the input vector xi\np×1\nis projected to a smaller space ( d < p) using the matrix We\nd×p\nthat yields\nyd×1 = We\nd×p ×xp×1 + pd×1 (1)\nwhere pd×1 is a positional encoding vector. We note that all\nmatrices denoted by W are learned during the training using\nthe backpropagation algorithm.\nNext, the new embedding vector is projected to three\nsubspaces using Wq\nn×d, Wk\nn×d, and Wv\nn×d matrices which\nmetaphorically named query, key, and value matrices:\nqi\nn×1 = Wq\nn×d ×yi\nd×1\nki\nn×1 = Wk\nn×d ×yi\nd×1\nvi\nn×1 = Wv\nn×d ×yi\nd×1\n(2)\nFor the ith frame, we compute the weighted sum of vj , j =\n1,...,T , where weights are obtained by dot product and qi\nand kj followed by the soft-max operation and a division to the\nsquare root of n. Speaking mathematically, this can be written\nas\nzi\nd×1 = Wc\nd×n ×\n( T∑\nj=1\nSoftmax(qi\nn×1 ⊙kj\nn×1)√n vj\nn×1\n)\n(3)\nwhere Wc\nd×n projects the resulting vector to a d-dimensional\nspace. Next, the layer normalization and residual connection\nare applied to the output of the self-attention sublayer given by\nhi\nd×1 = Norm(zi\nd×1 + yi\nd×1) (4)\nand lastly, a two-layer FFNN transforms hi\nd×1 to\nsi\nd×1 = max(0,hi\nd×1Wf1 ) ×Wf2 (5)\nfollowed by another layer normalization and residual connec-\ntion:\nri\nd×1 = Norm(si\nd×1 + hi\nd×1). (6)\nWe use a stack of several layers in our transformer; in this case,\nyd×1 ←ri\nd×1 and the above process is repeated for the next en-\ncoder until we reach to the last stack in the encoder. We also use\nmulti-head attention in which the process of transforming yd×1\nto sub-spaces is carried out in parallel with introducingLtuples\nof ( Wq,l\nn×d, Wk,l\nn×d, and Wv,l\nn×d), for l = 1,...,L where Lde-\nnotes the number of heads. The outputs of these heads,zi,l\nd×1 for\nl = 1,...,L are appended [zi,1\nd×1,...,z i,L\nd×1] and transformed\nto a d-dimensional vector using the matrixWd×(Ld). This oper-\nation outputs the d-dimensional output vector zi\nd×1 which will\nbe fed to FFNN.\nThe decoder of the transformer has a similar structure to\nthe encoder with two major differences; ﬁrst, the decoder has\nan encoder-decoder attention sublayer in addition to the self-\nattention layer which is placed between the self-attention sub-\nlayer and FFNN. The encoder-decoder attention receives ri\nd×1\nfrom the encoder and hi\nd×1 from the self-attention sub-layer of\nthe decoder and transforms them similar to the self-attention\nsub-layer but with these inputs:\nqi\nn×1 = Wq\nn×d ×hi\nd×1\nki\nn×1 = Wk\nn×d ×ri\nd×1\nvi\nn×1 = Wv\nn×d ×ri\nd×1.\n(7)\nThe second difference between the encoder and the decoder is\nthe masking process. Because the decoder does not have infor-\nmation from the future decoded sequences, it will mask them\nduring the decoding process. The rest of process in the decoder\nis the same as in the encoder. Finally, the output of the decoder\nis fed to a fully connected neural network followed by a soft\nmask to generate the posterior probabilities for domain, intent,\nand slots.\nFigure 2: The transformer accuracy v.s. number of epoch for\nthe domain class\n4. Experiments\n4.1. Feature generation\nWe used 80-dimensional log short time Fourier transform vec-\ntors obtained by segmenting the utterances with a Hamming\nwindow of the length 25 ms and frame rate of 10 ms. The\nfour frames are stacked with a skip rate of 3, resulting 320-\ndimensional input features. We used compute-fbank-feats in\nKaldi [31] to generate the features; these features are normal-\nized using the global mean and variance normalization.\n4.2. Transformer parameters\nThe numbers of layers for the encoder and the decoder were set\nto ﬁve and one, respectively; we used three heads in each atten-\ntion layer and sub-space dimension, i.e. n, was set to 64. The\ndimension of the model , i.e. d , and the inner dimension of\nFFNN were set to 128 and 512, respectively. The dropout rate\nand label smoothing were set to 0.1. We used Adam optimizer\n[32] with β1 = 0.9, β2= 0.98, and ϵ= 1e-9. We found that the\ntransformers’ performance is, in general, very sensitive to learn-\ning rate. We used the learning rate deﬁned in [27] in which we\nTable 1: Domain, intent, and slot accuracy predicted by the\nhierarchical transformer-based SLU\nDomain Intent Slots\n98.1 99.6 99.6\nincreased the learning rate linearly to a threshold and then it was\nreduced in a non-linear fashion per step. We obtained our best\nresults by setting pre-deﬁned factor k and warm-up rate w to\n0.95 and 18,000, respectively. We trained our model with differ-\nent number of epochs ranging from 50 to 1,500. After a certain\nnumber of steps, the model is evaluated on the evaluation data\nand the best model is saved. We found that the loss decreases\nvery slowly after 50 epochs but the results are consistently im-\nproved up to 250 epochs after which the improvement is very\nmarginal (see, e.g. Figure 2). Our implementation is adapted\nfrom a Pytorch implementation which originally designed for\nspeech recognition 1 .\n4.3. Data set\nWe used the publicly available Fluent Speech Commands (FSC)\ndataset [4] to train and evaluate our model and compare with\nmodels tested on the same dataset. The FSC is the largest freely\navailable spoken language understanding dataset that has do-\nmain, intent and slots labeling and used a wide range of subjects\nto record the utterances. The FSC dataset consists of around\n30,000 command utterances, each of which associated with a\n3-dimensional vector representing domain, intent and slot. In\ntotal, there are 248 different distinct phrases in the FSC dataset\nand 5 distinct domains. The data are split into 23,132 training\nsamples from 77 speakers, 3,118 eval samples from 10 speakers\nand 3,793 test samples from 10 speakers.\nWe set up our experiments both for classiﬁcation-based\nand hierarchical E2E SLU scenarios. In classiﬁcation-based\napproach, each utterance is assigned one of the 248 dis-\ntinct labels—each label represents a unique domain-intent-slots\nphrase. In this approach, we deal with a classiﬁcation prob-\nlem and the softmax in the last layer of the decoder outputs the\nposterior probabilities of labels for each input utterance. In con-\ntrast, in the hierarchical scenario the domain, intent and slots for\neach utterance are considered as a sequence where we ﬁrst pre-\ndict the domain; next the predicted domain is used to predict the\nintent and the predicted intent is used to predict the slots. The\nhierarchical scenario is very similar to the way ASR seq2seq\nneural architectures work with the difference that instead of us-\ning tokens such phonemes, chars, or words, we use sequences\nof domain, intent, and slots.\n4.4. Results\nWe evaluated our E2E SLU neural transformer model using the\nFSC dataset and compared it with the state-of-the-art E2E SLU\nneural model proposed in [4], hereafter we call it RNN+SincNet\nSLU model and its predecessor [21]. We chose [4] because\nthis model is tested on the FSC dataset, uses both recurrence\nand convolutional architectures that can be compared with the\ntransformer, and the code source that implements this model\nis available which makes comparison fair. The RNN+SincNet\nSLU model proposed in [4] adapted from [21] with two main\ndifferences that makes it more powerful. First, it is ASR-\n1https://github.com/kaituoxu/Speech-Transformer\nTable 2: The accuracy and number of model parameters for the Transformer, RNN+SincNet [4], and RNN [21] models\nModel Accuracy (%) # of Parameters\nTransformer (classiﬁcation-based, no pre-trained ASR) 97.6 1,545,987\nTransformer (hierarchical, no pre-trained ASR) 97.5 2,192,320\nRNN+SincNet [4] (classiﬁcation-based, pre-trained ASR, ﬁne-tuned on FSC) 97.2 2,883,410\nRNN+SincNet [4] (classiﬁcation-based, no pre-trained ASR) 96.1 2,883,410\nRNN [21](classiﬁcation-based, no pre-trained ASR) 95.3 2,534,230\nawareness meaning that it performs phoneme and word recog-\nnition before domain, slot, and intent prediction. Second, it\ndeploys the SincNet layer [33] that processes the raw audio\nﬁles and obviates the need for pre-processing. Moreover, the\nRNN+SincNet SLU model has the ﬂexibility to be used with\nor without ASR-awareness and can be pre-trained for the ASR\npart on large dataset and used for SLU on small datasets. The\nRNN+SincNet model applies ﬁve layers of biGRU, three layers\nof CNN and one layer of linear transformation with dropout and\ndownsampling of factor two in each layer to reduce the time res-\nolution. The RNN+SincNet SLU is, however, a classiﬁcation-\nbased model which requires the distinct class label for each ut-\nterance.\nFigure 3: The confusion matrix of the ﬁve domains predicted by\nthe hierarchical SLU transformer model.\nWe ﬁrst set our transformer based SLU model to the hi-\nerarchical mode and measured the accuracy of domain, intent,\nand slots. Table 1 illustrates the accuracy for domain, intent\nand slots when we use our model in a hierarchical mode as\nexplained earlier. In order to evaluate the false negative rate,\nwe also plot the confusion matrix for ﬁve distinct domains as\nshown in Figure 3. We found that the most false negative errors\nstem from domains with very similar audio features like “in-\ncrease” and “decrease” or “activate” and “deactivate” but with\nopposite semantics . This observation underscores that incorpo-\nrating an ASR-awareness module may reduce the false negative\nrate where the utterances contain very similar audio features but\nwith different meaning.\nNext, we compared our transformer model with RNN based\nSLU models [4] and [21]. We trained these models on the\nFSC dataset. Table 2 demonstrates accuracy and the number\nof parameters for each model. As shown in Table 2, the trans-\nformer model outperforms the RNN+SincNet architecture by\n1.5% when both are in the non-pretrained non-ASR-awareness.\nWe also observed that classiﬁcation-based transformer model is\nmarginally better than the hierarchical one. It is not however\nsurprising because the number of classes in the FSC dataset\nis rather small. Other signiﬁcant improvement we achieved\nis the reduction in model size. The number of parameters in\nclassiﬁcation-based and hierarchical transformers are 25% and\n46% smaller than number of parameters in RNN+SincNet mod-\nels, respectively. We also compared our model with the RNN\nmodel in which the ﬁrst two and last two RNN layers are pre-\ntrained for phoneme and word recognition, respectively using\nLibrispeech data and ﬁne-tuned on FSC data; the results show\nthat both hierarchical and classiﬁcation transformer based SLU\nmodel outperform the pretrained RNN model. Overall, these\nresults conﬁrm that the transformers are very competitive com-\npared with recurrence and convolutional-based architectures be-\ncause they look at the entire utterance and measure the correla-\ntion between different audio units to better extract the semantic\ninformation.\n5. Conclusions and future works\nIn this paper, we introduced a neural transformer based ap-\nproach for spoken language understanding. To the best of our\nknowledge this is the ﬁrst time transformers are applied to end-\nto-end neural SLU. We showed that transformers are very com-\npetitive neural architectures for end-to-end neural spoken lan-\nguage understanding compared to recurrent and convolutional\nneural networks. We evaluated our model on a publicly avail-\nable dataset and showed the model achieves higher accuracy\nand reduction in size compared to two competitive models.\nBecause our model is a hierarchical model it can be applied\nto variable length domain, intent and slots vectors. We ob-\nserved that if the number of unique domain-intent-slot classes\nis small, the classiﬁcation-based SLU model delivers better\nmarginal accuracy compared to the hierarchical model. In ad-\ndition, when we analyzed the false negative rate for domain\nmiss-classiﬁcation, we found the model has difﬁculty to predict\ndomains that have very similar audio patterns but very differ-\nent semantics. Hence, as it was previously observed by other\ngroups, our model may beneﬁt from incorporating an interme-\ndiate ASR-awareness module into the neural architecture. One\nof the unique advantages of transformers is the attention mech-\nanism which is particularly important for SLU. Transformers\ncompute the correlation between all input vector pairs and thus\nmodel knows where to attend to infer semantic information em-\nbeded in the audio signal.\nFor future works, we plan: 1- To apply our model to much\nlarger data sets, in particular we will make the use of in-house\nAlexa data. 2- Adding a multi-task learner by augmenting an-\nother decoder to predict intermediate tokens like word-pieces.\n3- Using a pre-trained transformer-based ASR model and ﬁne-\ntune it for the SLU task\n6. References\n[1] Y .-Y . Wang, L. Deng, and A. Acero, “Spoken language under-\nstanding,” IEEE Signal Processing Magazine, vol. 22, no. 5, pp.\n16–31, 2005.\n[2] P. Price, “Evaluation of spoken language systems: The atis do-\nmain,” in Speech and Natural Language: Proceedings of a Work-\nshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990 ,\n1990.\n[3] L.-s. Lee, J. Glass, H.-y. Lee, and C.-a. Chan, “Spoken content\nretrievalbeyond cascading speech recognition with text retrieval,”\nIEEE/ACM Transactions on Audio, Speech, and Language Pro-\ncessing, vol. 23, no. 9, pp. 1389–1420, 2015.\n[4] L. Lugosch, M. Ravanelli, P. Ignoto, V . S. Tomar, and Y . Ben-\ngio, “Speech model pre-training for end-to-end spoken language\nunderstanding,”arXiv preprint arXiv:1904.03670, 2019.\n[5] P. Haghani, A. Narayanan, M. Bacchiani, G. Chuang, N. Gaur,\nP. Moreno, R. Prabhavalkar, Z. Qu, and A. Waters, “From audio\nto semantics: Approaches to end-to-end spoken language under-\nstanding,” in 2018 IEEE Spoken Language Technology Workshop\n(SLT). IEEE, 2018, pp. 720–726.\n[6] V . Renkens et al., “Capsule networks for low resource spoken lan-\nguage understanding,”arXiv preprint arXiv:1805.02922, 2018.\n[7] Y .-P. Chen, R. Price, and S. Bangalore, “Spoken language un-\nderstanding without speech recognition,” in 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2018, pp. 6189–6193.\n[8] N. Tomashenko, A. Caubri `ere, Y . Est `eve, A. Laurent, and\nE. Morin, “Recent advances in end-to-end spoken language un-\nderstanding,” inInternational Conference on Statistical Language\nand Speech Processing. Springer, 2019, pp. 44–55.\n[9] G. Mesnil, X. He, L. Deng, and Y . Bengio, “Investigation of\nrecurrent-neural-network architectures and learning methods for\nspoken language understanding.” inInterspeech, 2013, pp. 3771–\n3775.\n[10] G. Mesnil, Y . Dauphin, K. Yao, Y . Bengio, L. Deng, D. Hakkani-\nTur, X. He, L. Heck, G. Tur, D. Yu et al., “Using recurrent neu-\nral networks for slot ﬁlling in spoken language understanding,”\nIEEE/ACM Transactions on Audio, Speech, and Language Pro-\ncessing, vol. 23, no. 3, pp. 530–539, 2014.\n[11] K. Yao, B. Peng, Y . Zhang, D. Yu, G. Zweig, and Y . Shi, “Spo-\nken language understanding using long short-term memory neural\nnetworks,” in2014 IEEE Spoken Language Technology Workshop\n(SLT). IEEE, 2014, pp. 189–194.\n[12] N. T. Vu, P. Gupta, H. Adel, and H. Sch¨utze, “Bi-directional recur-\nrent neural network with ranking loss for spoken language under-\nstanding,” in 2016 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2016, pp. 6060–\n6064.\n[13] C. Liu, J. Trmal, M. Wiesner, C. Harman, and S. Khudan-\npur, “Topic identiﬁcation for speech without asr,” arXiv preprint\narXiv:1703.07476, 2017.\n[14] Q. Chen, Z. Zhuo, W. Wang, and Q. Xu, “Transfer learn-\ning for context-aware spoken language understanding,” in 2019\nIEEE Automatic Speech Recognition and Understanding Work-\nshop (ASRU). IEEE, 2019, pp. 779–786.\n[15] C.-W. Huang and Y .-N. Chen, “Adapting pretrained transformer to\nlattices for spoken language understanding,” in 2019 IEEE Auto-\nmatic Speech Recognition and Understanding Workshop (ASRU).\nIEEE, 2019, pp. 845–852.\n[16] N. Tomashenko, C. Raymond, A. Caubriere, R. De Mori, and\nY . Esteve, “Dialogue history integration into end-to-end signal-to-\nconcept spoken language understanding systems,” arXiv preprint\narXiv:2002.06012, 2020.\n[17] M. Dinarelli, N. Kapoor, B. Jabaian, and L. Besacier, “A data ef-\nﬁcient end-to-end spoken language understanding architecture,”\narXiv preprint arXiv:2002.05955, 2020.\n[18] P. Wang, L. Wei, Y . Cao, J. Xie, and Z. Nie, “Large-scale unsu-\npervised pre-training for end-to-end spoken language understand-\ning,” in ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2020,\npp. 7999–8003.\n[19] P. Zatko, I. Poupyrev, R. El Guerrab, and R. Dugan, “Google\nI/O 2015. A little badass. Beautiful. Tech and human. Work and\nlove. ATAP,” https://www.youtube.com/watch?v=zl99IZvW7rE,\nMay 2015.\n[20] Y . Qian, R. Ubale, V . Ramanaryanan, P. Lange, D. Suendermann-\nOeft, K. Evanini, and E. Tsuprun, “Exploring asr-free end-to-end\nmodeling to improve spoken language understanding in a cloud-\nbased dialog system,” in 2017 IEEE Automatic Speech Recogni-\ntion and Understanding Workshop (ASRU) . IEEE, 2017, pp.\n569–576.\n[21] D. Serdyuk, Y . Wang, C. Fuegen, A. Kumar, B. Liu, and Y . Ben-\ngio, “Towards end-to-end spoken language understanding,” in\n2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2018, pp. 5754–5758.\n[22] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to\nsequence learning with neural networks,” in Advances in\nNeural Information Processing Systems 27 , Z. Ghahramani,\nM. Welling, C. Cortes, N. D. Lawrence, and K. Q.\nWeinberger, Eds. Curran Associates, Inc., 2014, pp.\n3104–3112. [Online]. Available: http://papers.nips.cc/paper/\n5346-sequence-to-sequence-learning-with-neural-networks.pdf\n[23] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine trans-\nlation by jointly learning to align and translate,” arXiv preprint\narXiv:1409.0473, 2014.\n[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,”arXiv preprint arXiv:1810.04805, 2018.\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in neural information processing systems , 2017, pp.\n5998–6008.\n[26] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend\nand spell: A neural network for large vocabulary conversational\nspeech recognition,” in 2016 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2016,\npp. 4960–4964.\n[27] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-recurrence\nsequence-to-sequence model for speech recognition,” in 2018\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2018, pp. 5884–5888.\n[28] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez,\nS. Gouws, L. Jones, Ł. Kaiser, N. Kalchbrenner, N. Parmar et al.,\n“Tensor2tensor for neural machine translation,” arXiv preprint\narXiv:1803.07416, 2018.\n[29] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”\narXiv preprint arXiv:1607.06450, 2016.\n[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” inProceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2016, pp. 770–778.\n[31] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian, P. Schwarz,\nJ. Silovsky, G. Stemmer, and K. Vesely, “The kaldi speech recog-\nnition toolkit,” in IEEE 2011 Workshop on Automatic Speech\nRecognition and Understanding . IEEE Signal Processing So-\nciety, Dec. 2011, iEEE Catalog No.: CFP11SRW-USB.\n[32] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,”arXiv preprint arXiv:1412.6980, 2014.\n[33] M. Ravanelli and Y . Bengio, “Speaker recognition from raw wave-\nform with sincnet,”SLT, 2018.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8187733292579651
    },
    {
      "name": "Transformer",
      "score": 0.7190708518028259
    },
    {
      "name": "Speech recognition",
      "score": 0.5872814655303955
    },
    {
      "name": "Security token",
      "score": 0.5452223420143127
    },
    {
      "name": "Spoken language",
      "score": 0.5165364146232605
    },
    {
      "name": "Utterance",
      "score": 0.4957347810268402
    },
    {
      "name": "End-to-end principle",
      "score": 0.49195143580436707
    },
    {
      "name": "Artificial neural network",
      "score": 0.48807084560394287
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4749222695827484
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4641614556312561
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.45832809805870056
    },
    {
      "name": "Language model",
      "score": 0.451946496963501
    },
    {
      "name": "Architecture",
      "score": 0.4469911456108093
    },
    {
      "name": "Natural language processing",
      "score": 0.35981085896492004
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ]
}