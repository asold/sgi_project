{
  "title": "Assessing the Utilization of Large Language Models in Medical Education: Insights From Undergraduate Medical Students",
  "url": "https://openalex.org/W4387859098",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3022085420",
      "name": "Sairavi Kiran Biri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126367138",
      "name": "Subir. Kumar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281918639",
      "name": "Muralidhar Panigrahi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2553101096",
      "name": "Shaikat Mondal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2121501365",
      "name": "Joshil Kumar Behera",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2556512418",
      "name": "Himel Mondal",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4376866715",
    "https://openalex.org/W4380887490",
    "https://openalex.org/W4312209955",
    "https://openalex.org/W4323980051",
    "https://openalex.org/W2995872180",
    "https://openalex.org/W2061935394",
    "https://openalex.org/W2327037637",
    "https://openalex.org/W4385500359",
    "https://openalex.org/W4384340967",
    "https://openalex.org/W4379508361",
    "https://openalex.org/W4362601804",
    "https://openalex.org/W4385266429",
    "https://openalex.org/W4322723456",
    "https://openalex.org/W4385898567",
    "https://openalex.org/W4386548637",
    "https://openalex.org/W4386716960",
    "https://openalex.org/W4367625612",
    "https://openalex.org/W4386548705",
    "https://openalex.org/W4384078169",
    "https://openalex.org/W4327946446"
  ],
  "abstract": "Background Artificial intelligence (AI) has the potential to be integrated into medical education. Among AI-based technology, large language models (LLMs) such as ChatGPT, Google Bard, Microsoft Bing, and Perplexity have emerged as powerful tools with capabilities in natural language processing. With this background, this study investigates the knowledge, attitude, and practice of undergraduate medical students regarding the utilization of LLMs in medical education in a medical college in Jharkhand, India. Methods A cross-sectional online survey was sent to 370 undergraduate medical students on Google Forms. The questionnaire comprised the following three domains: knowledge, attitude, and practice, each containing six questions. Cronbach's alphas for knowledge, attitude, and practice domains were 0.703, 0.707, and 0.809, respectively. Intraclass correlation coefficients for knowledge, attitude, and practice domains were 0.82, 0.87, and 0.78, respectively. The average scores in the three domains were compared using ANOVA. Results A total of 172 students participated in the study (response rate: 46.49%). The majority of the students (45.93%) rarely used the LLMs for their teaching-learning purposes (chi-square (3) = 41.44, p < 0.0001). The overall score of knowledge (3.21±0.55), attitude (3.47±0.54), and practice (3.26±0.61) were statistically significantly different (ANOVA F (2, 513) = 10.2, p < 0.0001), with the highest score in attitude and lowest in knowledge. Conclusion While there is a generally positive attitude toward the incorporation of LLMs in medical education, concerns about overreliance and potential inaccuracies are evident. LLMs offer the potential to enhance learning resources and provide accessible education, but their integration requires further planning. Further studies are required to explore the long-term impact of LLMs in diverse educational contexts.",
  "full_text": "Review began\n 10/12/2023 \nReview ended\n 10/16/2023 \nPublished\n 10/22/2023\n© Copyright \n2023\nBiri et al. This is an open access article\ndistributed under the terms of the Creative\nCommons Attribution License CC-BY 4.0.,\nwhich permits unrestricted use, distribution,\nand reproduction in any medium, provided\nthe original author and source are credited.\nAssessing the Utilization of Large Language\nModels in Medical Education: Insights From\nUndergraduate Medical Students\nSairavi Kiran Biri \n \n, \nSubir Kumar \n \n, \nMuralidhar Panigrahi \n \n, \nShaikat Mondal \n \n, \nJoshil Kumar Behera \n \n, \nHimel\nMondal \n1.\n Biochemistry, Phulo Jhano Medical College, Dumka, IND \n2.\n Pharmacology, Phulo Jhano Medical College, Dumka,\nIND \n3.\n Pharmacology, Bhima Bhoi Medical College and Hospital, Balangir, IND \n4.\n Physiology, Raiganj Government\nMedical College & Hospital, Raiganj, IND \n5.\n Physiology, Nagaland Institute of Medical Sciences and Research, Kohima,\nIND \n6.\n Physiology, All India Institute of Medical Sciences, Deoghar, IND\nCorresponding author: \nHimel Mondal, \nhimelmkcg@gmail.com\nAbstract\nBackground\nArtificial intelligence (AI) has the potential to be integrated into medical education. Among AI-based\ntechnology, large language models (LLMs) such as ChatGPT, Google Bard, Microsoft Bing, and Perplexity\nhave emerged as powerful tools with capabilities in natural language processing. With this background, this\nstudy investigates the knowledge, attitude, and practice of undergraduate medical students regarding the\nutilization of LLMs in medical education in a medical college in Jharkhand, India.\nMethods\nA cross-sectional online survey was sent to 370 undergraduate medical students on Google Forms. The\nquestionnaire comprised the following three domains: knowledge, attitude, and practice, each containing\nsix questions. Cronbach’s alphas for knowledge, attitude, and practice domains were 0.703, 0.707, and 0.809,\nrespectively. Intraclass correlation coefficients for knowledge, attitude, and practice domains were 0.82,\n0.87, and 0.78, respectively. The average scores in the three domains were compared using ANOVA.\nResults\nA total of 172 students participated in the study (response rate: 46.49%). The majority of the students\n(45.93%) rarely used the LLMs for their teaching-learning purposes (chi-square (3) = 41.44, p < 0.0001). The\noverall score of knowledge (3.21±0.55), attitude (3.47±0.54), and practice (3.26±0.61) were statistically\nsignificantly different (ANOVA F (2, 513) = 10.2, p < 0.0001), with the highest score in attitude and lowest in\nknowledge.\nConclusion\nWhile there is a generally positive attitude toward the incorporation of LLMs in medical education, concerns\nabout overreliance and potential inaccuracies are evident. LLMs offer the potential to enhance learning\nresources and provide accessible education, but their integration requires further planning. Further studies\nare required to explore the long-term impact of LLMs in diverse educational contexts.\nCategories:\n Psychology, Medical Education, Healthcare Technology\nKeywords:\n medical students, practice, attitude, knowledge, chatgpt, intelligence, surveys and questionnaires, search\nengine, artificial intelligence\nIntroduction\nThe integration of technology into medical education is in progress, with artificial intelligence (AI) and\nlarge language models (LLMs) being the new addition. In an era characterized by the exponential growth of\nmedical knowledge, the ability to efficiently access, interpret, and apply information is paramount for\naspiring healthcare professionals \n[1]\n. LLMs, such as ChatGPT, Google Bard, Microsoft Bing, and Perplexity,\nhave emerged as powerful AI tools with the potential to reshape the landscape of medical education by\nproviding innovative solutions to the challenges posed by information overload \n[2]\n.\nUndergraduate medical education represents a critical phase in the development of future healthcare\npractitioners. It is during this formative period that students acquire the foundational knowledge, skills, and\nattitudes that will shape their careers. Although teacher-led education is an important part of the medical\ncurriculum, student-led learning methods are gaining popularity due to their flexibility and better\nengagement \n[3]\n. Among those, self-directed learning is an integral part of competency-based medical\neducation. LLMs can have a significant impact on self-directed or student-led learning due to their\n1\n2\n3\n4\n5\n6\n \n Open Access Original\nArticle\n \nDOI:\n 10.7759/cureus.47468\nHow to cite this article\nBiri S, Kumar S, Panigrahi M, et al. (October 22, 2023) Assessing the Utilization of Large Language Models in Medical Education: Insights From\nUndergraduate Medical Students. Cureus 15(10): e47468. \nDOI 10.7759/cureus.47468\ntransformative potential in empowering learners to take control of their educational journey \n[4]\n. In an era\nwhere information is abundantly available but often overwhelming, LLMs provide students with a versatile\ntool to navigate and synthesize knowledge independently. However, the successful integration of LLMs into\nmedical education necessitates a comprehensive understanding of how these technologies are perceived and\nutilized by the very individuals they are designed to serve - undergraduate medical students \n[5]\n.\nWith this background, our research question was - \"Do students recognize the potential benefits of these\ntechnologies, and to what extent do they incorporate them into their educational journey? To get the\nanswer, we designed an online survey to explore the knowledge, attitude, and practice of LLMs in the\ncontext of medical education, as perceived and experienced by undergraduate medical students. By gaining\ninsights into the extent of students' awareness, their attitudes toward LLMs, and their actual engagement\nwith these AI-driven tools, this study aims to shed light on the evolving dynamics of medical education in\nthe digital age.\nMaterials And Methods\nType and settings\nThis cross-sectional study was conducted among undergraduate medical students studying in undergraduate\nmedical courses (MBBS) at Phulo Jhano Medical College, Dumka, Jharkhand, India, after getting approval\nfrom the Institutional Review Board (approval number 51/BIO/2023). The study was carried out by ethical\nguidelines, and all participants provided informed consent for participation. The research took place in an\nonline setting, where participants completed a questionnaire hosted on Google Forms.\nDevelopment of questionnaire\nTo assess the knowledge, attitude, and practice of undergraduate medical students regarding the utilization\nof LLMs in medical education, a structured questionnaire was developed. The questionnaire consisted of the\nfollowing three domains: knowledge, attitude, and practice, each comprising six questions. The\ndevelopment of the questionnaire involved the following steps.\nFirst, we conducted a comprehensive review of existing literature on LLMs in medical education to identify\nrelevant themes and constructs. Next, the questionnaire was developed with input from subject matter\nexperts in medical education and AI technology to ensure content validity. A cognitive interview was\nconducted with a group of 10 undergraduate medical students to assess the clarity, comprehensibility, and\nrelevance of the questionnaire. Feedback from pilot participants was used to refine the questionnaire\nfurther. The final questionnaire was designed to assess participants' knowledge about LLMs, their attitudes\ntoward the technology, and their practical experiences using LLMs in the context of medical education. This\nquestionnaire was then distributed among 30 students, and the response was obtained with a gap of seven\ndays. The first response was used to calculate Cronbach’s alpha, and two responses were used to check the\ntest-retest reliability using intraclass correlation coefficients (ICCs) \n[6,7]\n.\nThe responses were coded for quantitative analysis (strongly agree = 5, agree = 4, neutral = 3, disagree = 2,\nstrongly disagree = 1). Cronbach’s alphas for knowledge, attitude, and practice domains were 0.703, 0.707,\nand 0.809, respectively. For a student, the average score of a domain was calculated by adding the score of\nsix responses and dividing the value by 6. These scores of test and retest were compared using ICCs.\nObtained ICCs for knowledge, attitude, and practice domains were 0.82, 0.87, and 0.78, respectively.\nParticipants\nWe recruited undergraduate medical students for this study. Hence, any undergraduate student studying\nmodern medicine (Bachelor of Medicine, Bachelor of Surgery) was the target participant. As the survey link\nwas shared online with all the students, those who did not participate voluntarily were automatically\nexcluded from the study.\nData collection methods\nData collection was carried out using the finalized questionnaire, which was distributed to undergraduate\nmedical students through an online platform (Google Forms). Participants were contacted via email, and a\nlink to the questionnaire was provided along with a brief explanation of the research objectives and the\nvoluntary nature of their participation. Participants were given ample time to complete the questionnaire,\nand reminders were sent as needed to enhance response rates.\nData analysis\nData obtained from the completed questionnaires were subjected to a structured data analysis process.\nThere was no missing data as all questions were made compulsory in Google Forms. Next, descriptive\nstatistics, such as frequencies and percentages, were computed to summarize the responses to individual\nquestionnaire items within each domain (knowledge, attitude, and practice). The responses were coded for\nquantitative analysis (strongly agree = 5, agree = 4, neutral = 3, disagree = 2, strongly disagree = 1). For a\n2023 Biri et al. Cureus 15(10): e47468. DOI 10.7759/cureus.47468\n2\n of \n8\nstudent, the average score of a domain was calculated by adding the score of six responses and dividing the\nvalue by 6. The chi-square test was used to compare categorical variables with expected equal division in all\ncategories, and a statistical significance indicates that the occurrence was not by chance. The ANOVA with\npost hoc analysis was used to compare the mean score of knowledge, attitude, and practice. Data analysis\nwas conducted using GraphPad Prism 9.5.0 (GraphPad Software, Boston, MA), and a p-value of less than 0.05\nwas considered statistically significant. The results of the data analysis are reported in the subsequent\nsections of this study.\nResults\nA total of 172 students (95 females, 75 males, and 2 preferred not to say) participated in the study. The\nquestionnaire was distributed among 370 students. Hence, the response rate was 46.49%. A total of 57\n(33.14%) first-year, 40 (23.26%) second-year, 38 (22.09%) third-year, and 37 (21.51%) fourth-year students\nparticipated in the survey. The students were similarly distributed among years of study (chi-square (3) =\n6.19, p = 0.1).\nThe majority of the students (45.93%) rarely use the LLMs for their teaching-learning purposes (chi-square\n(3) = 41.44, p < 0.0001) (Figure \n1\n).\nFIGURE\n 1: Frequency of the use of large language models by\nundergraduate medical students for educational purposes\nThe overall score of knowledge (3.21±0.55), attitude (3.47±0.54), and practice (3.26±0.61) were statistically\nsignificantly different (ANOVA F (2, 513) = 10.2, p < 0.0001), with the highest score in attitude and lowest in\nknowledge. In the post hoc test, two pairs showed significant differences - knowledge vs attitude (mean\ndifference = -0.2607, 95% CI = -0.4042 to -0.1171, p < 0.0001) and attitude vs practice (mean difference =\n0.2083, 95% CI = 0.06482 to 0.3518, p = 0.002 (Table \n1\n).\n2023 Biri et al. Cureus 15(10): e47468. DOI 10.7759/cureus.47468\n3\n of \n8\nDomain\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nAverage\nKnowledge\n3.09±1.02\n2.01±1.03\n3.1±0.99\n2.54±0.95\n3.63±0.85\n3.87±0.88\n3.21±0.55\nAttitude\n3.72±0.86\n3.16±0.85\n3.6±0.86\n3.69±0.76\n3.52±1\n3.12±0.88\n3.47±0.54\nPractice\n3±0.94\n3.31±0.78\n3.45±0.81\n3.45±0.83\n3.33±0.83\n3.02±0.85\n3.26±0.61\nTABLE\n 1: Question-wise score and average score in knowledge, attitude, and practice of using\nlarge language models for educational purposes\nQ, Question or statement in the questionnaire (the number after Q indicates the number of the question or statement)\nThe data in the table are presented as mean ± standard deviation. The average score is calculated by adding the raw score of six questions and dividing\nthe value by 6.\nA total of 39.53% of students had an awareness of LLMs. Moreover, 61% of students believed that they\nunderstand how LLMs generate information and responses, indicating a decent grasp of their functioning.\nNotably, a majority (53.49%) acknowledge the potential for LLMs to be used by both teachers and students.\nA total of 62.79% of students find the LLMs to simplify the topic they learn. However, they are cautious\nabout their limitations and potential for inaccuracies, with 34.88% agreeing that LLMs can generate wrong\ninformation (Table \n2\n).\nQuestion\nStrongly\nagree\nAgree\nNeutral\nDisagree\nStrongly\ndisagree\np-Value\nI am familiar with LLMs like ChatGPT, Google Bard, Microsoft\nBing, or Perplexity\n8 (4.65)\n60\n(34.88)\n57\n(33.14)\n34\n(19.77)\n13 (7.56)\n<0.0001\nI understand how LLMs generate information and responses\n8 (4.65)\n53\n(30.81)\n57\n(33.14)\n40\n(23.26)\n14 (8.14)\n<0.0001\nLLMs can generate wrong information\n12 (6.98)\n48\n(27.91)\n68\n(39.53)\n34\n(19.77)\n10 (5.81)\n<0.0001\nLLM can be used both by teachers and students\n18 (10.47)\n74\n(43.02)\n55\n(31.98)\n19\n(11.05)\n6 (3.49)\n<0.0001\nUsing LLMs helps me simplify complicated medical concepts\n18 (10.47)\n90\n(52.33)\n51\n(29.65)\n8 (4.65)\n5 (2.91)\n<0.0001\nLLMs can help along with traditional learning materials like\ntextbooks, notes, e-books, etc.\n36 (20.93)\n93\n(54.07)\n33\n(19.19)\n5 (2.91)\n5 (2.91)\n<0.0001\nTABLE\n 2: Knowledge of medical students on LLMs in medical education\nThe data in the table are presented as numbers (percentage). The p-value is of the chi-square test where a significant p-value indicates that the\ndistribution of the responses was not by chance.\nLLM, large language model\nMedical students generally exhibit openness toward incorporating LLMs as supplementary learning tools in\nmedical education (67.44%), recognizing the potential for comprehensive medical information and a change\nin how medical knowledge is accessed (31.4%). However, they also express concerns about overreliance on\nLLMs, which could potentially hinder the development of clinical reasoning skills (53.49%) and the risk of\nlearning incorrect concepts (33.14%) (Table \n3\n).\n2023 Biri et al. Cureus 15(10): e47468. DOI 10.7759/cureus.47468\n4\n of \n8\nQuestion\nStrongly\nagree\nAgree\nNeutral\nDisagree\nStrongly\ndisagree\np-Value\nI am open to including LLMs as extra learning tools for medical\nstudies\n25 (14.53)\n91\n(52.91)\n43 (25)\n9 (5.23)\n4 (2.33)\n<0.0001\nIt would help by providing comprehensive medical information\n8 (4.65)\n46\n(26.74)\n90\n(52.33)\n21\n(12.21)\n7 (4.07)\n<0.0001\nMedical colleges should promote the use of LLMs in the\nteaching-learning process\n17 (9.88)\n88\n(51.16)\n54\n(31.4)\n7 (4.07)\n6 (3.49)\n<0.0001\nLLMs could change how we learn and access medical\nknowledge\n13 (7.56)\n107\n(62.21)\n42\n(24.42)\n6 (3.49)\n4 (2.33)\n<0.0001\nRelying too much on LLMs might not develop my clinical\nreasoning skills\n27 (15.7)\n65\n(37.79)\n57\n(3.14)\n16 (9.3)\n7 (4.07)\n<0.0001\nThere is a risk of learning the wrong concept; hence, I would\nnot blindly believe it\n7 (4.07)\n50\n(29.07)\n79\n(45.93)\n29\n(16.86)\n7 (4.07)\n \nTABLE\n 3: Attitude of medical students on LLMs in medical education\nThe data in the table are presented as numbers (percentage). The p-value is of the chi-square test where a significant p-value indicates that the\ndistribution of the responses was not by chance.\nLLM, large language model\nSome of the medical students are actively using LLMs as supplementary resources in their medical\neducation (30.23%). They turn to LLMs for explanations that are not readily available in traditional books\n(55.81%) or search engines (47.67%). Additionally, LLMs are influencing their self-study practices and\nincreasing their confidence in discussing medical topics (37.21%) (Table \n4\n).\nQuestion\nStrongly\nagree\nAgree\nNeutral\nDisagree\nStrongly\ndisagree\np-Value\nI use LLMs to get clearer explanations of medical topics I'm\nlearning\n5 (2.91)\n47\n(27.33)\n76\n(44.19)\n31\n(18.02)\n13 (7.56)\n<0.0001\nLLMs have shown me new resources and references for my\nmedical studies\n10 (5.81)\n77\n(44.77)\n72\n(41.86)\n7 (4.07)\n6 (3.49)\n<0.0001\nI use those only when I cannot get the information in a book\n6 (3.49)\n90\n(52.33)\n58\n(33.72)\n11 (6.4)\n7 (4.07)\n<0.0001\nI use those only when I cannot get the information in Google or\nother search engine\n5 (2.91)\n77\n(44.77)\n65\n(37.79)\n20\n(11.63)\n5 (2.91)\n<0.0001\nI adapt my self-study based on insights I get from LLMs\n2 (1.16)\n47\n(27.33)\n86 (50)\n26\n(15.12)\n11 (6.4)\n<0.0001\nUsing LLMs has made me more confident in talking about\nmedical subjects\n4 (2.33)\n60\n(34.88)\n83\n(48.26)\n18\n(10.47)\n7 (4.07)\n<0.0001\nTABLE\n 4: Practice of medical students on large LLMs in medical education\nThe data in the table are presented as numbers (percentage). The p-value is of the chi-square test where a significant p-value indicates that the\ndistribution of the responses was not by chance.\nLLM, large language model\nDiscussion\nThe finding that the majority of students rarely use LLMs for their teaching and learning purposes can be\n2023 Biri et al. Cureus 15(10): e47468. DOI 10.7759/cureus.47468\n5\n of \n8\nattributed to several factors. One significant factor is limited familiarity among students regarding the\ncapabilities and benefits of LLMs in the context of medical education. Additionally, the preference for\ntraditional learning resources, concerns about accuracy, limited integration into the curriculum, and\npotential barriers such as limited access or time constraints may contribute to this trend. Individual learning\nstyles and the perceived learning curve associated with LLMs also play a role \n[8]\n. Moreover, resistance to\nchange and the lack of training and guidance can further discourage students from incorporating LLMs into\ntheir study routines \n[9]\n. To encourage greater utilization of LLMs, the educators and institutions should\naddress these factors by providing education on the advantages and effective use of LLMs, making them\nmore accessible, and integrating them into the curriculum where appropriate.\nThe finding that knowledge scores were the lowest among students, followed by practice scores and then\nattitude scores, can be attributed to several factors. It suggests that while students may have a positive\nattitude toward LLMs in medical education and may even try to incorporate them into their practices, their\nknowledge about how to effectively use these models might be lacking \n[10]\n. The lower knowledge scores may\nindicate a need for more comprehensive training and education on the practical application of LLMs in their\nmedical studies. Additionally, the statistical significance in the differences between knowledge vs attitude\nand attitude vs practice scores underscores the importance of bridging the gap between students' positive\nattitudes and their actual utilization of LLMs in their learning processes through targeted educational\ninterventions and support.\nThe findings reveal varying levels of awareness, understanding, and perception among students regarding\nLLMs in medical education. A significant proportion of students are aware of LLMs and believe that they\nunderstand how they work, indicating some theoretical knowledge. They are generally positive about LLMs'\npotential for use by both teachers and students, as well as their ability to simplify complex topics. \n[11]\n.\nHowever, students also express caution about the potential for inaccuracies and wrong information\ngenerated by LLMs, highlighting the importance of critical evaluation. These findings suggest a need for\nfurther education and practical training on LLMs' applications to bridge the gap between awareness and\neffective utilization \n[12]\n.\nWhile a significant majority of students are open to incorporating LLMs as supplementary tools, this\nopenness is tempered by concerns about overreliance on these models. Students recognize the potential for\nLLMs to provide comprehensive medical information and transform how they access knowledge, but they are\ncautious. Their apprehensions revolve around the potential negative impact on the development of critical\nclinical reasoning skills and the risk of acquiring incorrect concepts. This complex attitude reflects the need\nfor a balanced approach to LLM integration, acknowledging both the benefits and potential pitfalls\nassociated with these tools in medical education \n[13]\n.\nSome medical students are actively embracing LLMs as valuable supplementary resources, recognizing their\ncapacity to provide explanations beyond what traditional books or search engines can offer. This indicates a\ngrowing recognition of LLMs as tools that can bridge gaps in understanding and enhance their learning\nexperience \n[14]\n. Moreover, the influence of LLMs on students' self-study practices and their increased\nconfidence in discussing medical topics reflects the adaptability of students to modern technological\nadvancements, which empower them to take charge of their education and knowledge dissemination.\nThe study's implications for medical education in developing countries are significant and offer promising\nopportunities for enhancing learning, accessibility, and the overall quality of medical education \n[15]\n. LLMs\nemerge as valuable supplementary resources, particularly in regions where access to up-to-date medical\ntextbooks and academic materials may be limited due to resource constraints. A study by Tung and Dong\nfound that Malayasian medical students have an awareness of AI and that they are willing to learn more\nabout it \n[16]\n. A study by Buabbas et al. also reported a positive attitude of students toward AI in medical\neducation. Most of the students opined that AI can help in their teaching and learning \n[17]\n. Additionally,\nLLMs can address faculty shortages, support research, and innovation, and foster critical thinking skills.\n[18]\n. To fully realize these benefits, educators and institutions in developing countries need to invest in\nfaculty development and carefully integrate LLMs into their curricula, thereby harnessing the advantages\nwhile mitigating associated risks in medical education. Figure \n2\n summarizes the domains where LLMs can be\nused in medical education \n[19,20]\n.\n2023 Biri et al. Cureus 15(10): e47468. DOI 10.7759/cureus.47468\n6\n of \n8\nFIGURE\n 2: Domains of medical education where LLMs can help humans\nQ&A, questions and answers; LLM, large language model; PPT, PowerPoint presentation\nA key limitation of this study is its reliance on self-reported data from medical students, which may\nintroduce response bias. Additionally, the study's findings are context-specific and may not be fully\ngeneralizable to diverse medical education settings, particularly those in developing countries with varying\naccess to technology and resources. The response rate was low in the survey. Furthermore, the study does\nnot explore the long-term impact of LLM integration into medical education or assess the effectiveness of\nspecific educational interventions. Future research should consider these limitations and incorporate more\ndiverse and objective measures to provide a comprehensive understanding of LLMs' role in medical\neducation.\nConclusions\nThe study reveals the current knowledge, attitude, and practice of using LLMs in medical education in an\nIndian medical college. While there is a generally positive attitude toward their incorporation, concerns\nabout overreliance and potential inaccuracies are evident. LLMs offer the potential to enhance learning\nresources and provide accessible education, but their integration requires careful planning, faculty\ndevelopment, and the cultivation of critical thinking skills. This research underscores the evolving role of\ntechnology in medical education and calls for further studies to explore the long-term impact of LLMs in\ndiverse educational contexts.\nAppendices\nThe questionnaire used in this study can be used for any non-commercial research or academic purposes\nwithout any permission. The questionnaire can be created from the questions or statements in Tables \n2\n, \n3\n,\nand 4. A pdf of the questionnaire with consent and instruction can be obtained from Dr. Himel Mondal,\nAssistant Professor of Physiology, All India Institute of Medical Sciences, Deoghar, Jharkhand, India, via\nemail (himelmkcg@gmail.com).\nAdditional Information\nAuthor Contributions\nAll authors have reviewed the final version to be published and agreed to be accountable for all aspects of the\nwork.\nConcept and design:\n  \nHimel Mondal, Shaikat Mondal, Subir Kumar, Sairavi Kiran Biri, Joshil Kumar Behera,\nMuralidhar Panigrahi\nAcquisition, analysis, or interpretation of data:\n  \nHimel Mondal, Shaikat Mondal, Subir Kumar, Sairavi\nKiran Biri, Joshil Kumar Behera, Muralidhar Panigrahi\nDrafting of the manuscript:\n  \nHimel Mondal, Shaikat Mondal, Subir Kumar, Sairavi Kiran Biri, Joshil Kumar\nBehera, Muralidhar Panigrahi\nCritical review of the manuscript for important intellectual content:\n  \nHimel Mondal, Shaikat Mondal,\nSubir Kumar, Sairavi Kiran Biri, Joshil Kumar Behera, Muralidhar Panigrahi\nSupervision:\n  \nHimel Mondal\nDisclosures\nHuman subjects:\n Consent was obtained or waived by all participants in this study. Phulo Jhano Medical\n2023 Biri et al. Cureus 15(10): e47468. DOI 10.7759/cureus.47468\n7\n of \n8\nCollege issued approval 51/BIO/2023. This study protocol has been approved by Institutional Review Board.\nAnimal subjects:\n All authors have confirmed that this study did not involve animal subjects or tissue.\nConflicts of interest:\n In compliance with the ICMJE uniform disclosure form, all authors declare the\nfollowing: \nPayment/services info:\n All authors have declared that no financial support was received from\nany organization for the submitted work. \nFinancial relationships:\n All authors have declared that they have\nno financial relationships at present or within the previous three years with any organizations that might\nhave an interest in the submitted work. \nOther relationships:\n All authors have declared that there are no\nother relationships or activities that could appear to have influenced the submitted work.\nAcknowledgements\nWe thank all the students who participated in the study. We also thank the Principal, Phulo Jhano Medical\nCollege, Dumka, Jharkhand, India, for his support for this study. We used the ChatGPT free research preview\nversion from https://chat.openai.com (September 25 version, 2023) for language and grammar correction.\nReferences\n1\n. \nAbd-Alrazaq A, AlSaad R, Alhuwail D, et al.: \nLarge language models in medical education: opportunities,\nchallenges, and future directions\n. JMIR Med Educ. 2023, 9:e48291. \n10.2196/48291\n2\n. \nThapa S, Adhikari S: \nChatGPT, Bard, and large language models for biomedical research: opportunities and\npitfalls [Online ahead of print]\n. Ann Biomed Eng. 2023, \n10.1007/s10439-023-03284-0\n3\n. \nJagzape A, Jagzape TB: \nTeacher-led versus student-led seminar blended with portfolio for \"assessment of\nlearning\": an interventional study\n. J Educ Health Promot. 2022, 11:339. \n10.4103/jehp.jehp_165_22\n4\n. \nDas D, Kumar N, Longjam LA, Sinha R, Deb Roy A, Mondal H, Gupta P: \nAssessing the capability of ChatGPT\nin answering first- and second-order knowledge questions on microbiology as per competency-based\nmedical education curriculum\n. Cureus. 2023, 15:e36034. \n10.7759/cureus.36034\n5\n. \nHan ER, Yeo S, Kim MJ, Lee YH, Park KH, Roh H: \nMedical education trends for future physicians in the era of\nadvanced technology and artificial intelligence: an integrative review\n. BMC Med Educ. 2019, 19:460.\n10.1186/s12909-019-1891-5\n6\n. \nTavakol M, Dennick R: \nMaking sense of Cronbach's alpha\n. Int J Med Educ. 2011, 2:53-5.\n10.5116/ijme.4dfb.8dfd\n7\n. \nKoo TK, Li MY: \nA guideline of selecting and reporting intraclass correlation coefficients for reliability\nresearch\n. J Chiropr Med. 2016, 15:155-63. \n10.1016/j.jcm.2016.02.012\n8\n. \nMuniyapillai T, Kulothungan K, Abdul Malik SR, et al.: \nLearning styles and their relationship with preferred\nteaching methodologies and academic achievement among medical students in teaching medical college,\nTamil Nadu\n. J Educ Health Promot. 2023, 12:256. \n10.4103/jehp.jehp_185_23\n9\n. \nBirenbaum M: \nThe chatbots’ challenge to education: disruption or destruction?\n. Educ Sci. 2023, 13:711.\n10.3390/educsci13070711\n10\n. \nKarabacak M, Ozkara BB, Margetis K, Wintermark M, Bisdas S: \nThe advent of generative language models in\nmedical education\n. JMIR Med Educ. 2023, 9:e48163. \n10.2196/48163\n11\n. \nEggmann F, Weiger R, Zitzmann NU, Blatz MB: \nImplications of large language models such as ChatGPT for\ndental medicine\n. J Esthet Restor Dent. 2023, 35:1098-102. \n10.1111/jerd.13046\n12\n. \nSafranek CW, Sidamon-Eristoff AE, Gilson A, Chartash D: \nThe role of large language models in medical\neducation: applications and implications\n. JMIR Med Educ. 2023, 9:e50945. \n10.2196/50945\n13\n. \nAhn S: \nThe impending impacts of large language models on medical education\n. Korean J Med Educ. 2023,\n35:103-7. \n10.3946/kjme.2023.253\n14\n. \nLee H: \nUsing ChatGPT as a learning tool in acupuncture education: comparative study\n. JMIR Med Educ.\n2023, 9:e47427. \n10.2196/47427\n15\n. \nMohamadi M, Aghamirzaee T, Aqatabar Roudbari J, Mohseni Afshar Z, Taghvaee Yazdi M, Kheirkhah F:\nPerceived challenges and barriers for medical students in the COVID-19 crisis\n. J Educ Health Promot. 2023,\n12:263. \n10.4103/jehp.jehp_1095_22\n16\n. \nTung AY, Dong LW: \nMalaysian medical students' attitudes and readiness toward AI (artificial intelligence): a\ncross-sectional study\n. J Med Educ Curric Dev. 2023, 10:23821205231201164. \n10.1177/23821205231201164\n17\n. \nBuabbas AJ, Miskin B, Alnaqi AA, Ayed AK, Shehab AA, Syed-Abdul S, Uddin M: \nInvestigating students'\nperceptions towards artificial intelligence in medical education\n. Healthcare (Basel). 2023, 11:1298.\n10.3390/healthcare11091298\n18\n. \nVagha S, Mishra V, Joshi Y: \nReviving medical education through teachers training programs: a literature\nreview\n. J Educ Health Promot. 2023, 12:277. \n10.4103/jehp.jehp_1413_22\n19\n. \nMondal H, Marndi G, Behera JK, Mondal S: \nChatGPT for teachers: practical examples for utilizing artificial\nintelligence for educational purposes [Online ahead of print]\n. Indian J Vasc Endovasc Surg. 2023,\n20\n. \nSallam M: \nChatGPT utility in healthcare education, research, and practice: systematic review on the\npromising perspectives and valid concerns\n. Healthcare (Basel). 2023, 11:887. \n10.3390/healthcare11060887\n2023 Biri et al. Cureus 15(10): e47468. DOI 10.7759/cureus.47468\n8\n of \n8",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.9558324813842773
    },
    {
      "name": "Medical education",
      "score": 0.6109635829925537
    }
  ]
}