{
  "title": "CamemBERT: a Tasty French Language Model",
  "url": "https://openalex.org/W2986154550",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2990932955",
      "name": "Martin, Louis",
      "affiliations": [
        "Université Sorbonne Nouvelle",
        "Université Paris 1 Panthéon-Sorbonne",
        "Sorbonne Université",
        "Institut national de recherche en informatique et en automatique",
        "Centre d'Économie de la Sorbonne"
      ]
    },
    {
      "id": "https://openalex.org/A4226159673",
      "name": "Müller, Benjamin",
      "affiliations": [
        "Institut national de recherche en informatique et en automatique",
        "Centre d'Économie de la Sorbonne",
        "Université Paris 1 Panthéon-Sorbonne",
        "Université Sorbonne Nouvelle",
        "Sorbonne Université"
      ]
    },
    {
      "id": null,
      "name": "Su\\'arez, Pedro Javier Ortiz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288883105",
      "name": "Dupont, Yoann",
      "affiliations": [
        "Sorbonne Université",
        "Université Paris 1 Panthéon-Sorbonne",
        "Centre d'Économie de la Sorbonne",
        "Université Sorbonne Nouvelle"
      ]
    },
    {
      "id": "https://openalex.org/A4221403894",
      "name": "Romary, Laurent",
      "affiliations": [
        "Institut national de recherche en informatique et en automatique"
      ]
    },
    {
      "id": "https://openalex.org/A4287427316",
      "name": "de La Clergerie, Éric Villemonte",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Seddah, Djam\\'e",
      "affiliations": [
        "Institut national de recherche en informatique et en automatique"
      ]
    },
    {
      "id": null,
      "name": "Sagot, Beno\\^it",
      "affiliations": [
        "Institut national de recherche en informatique et en automatique"
      ]
    },
    {
      "id": null,
      "name": "Suárez, Pedro Javier Ortiz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287427316",
      "name": "de La Clergerie, Éric Villemonte",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4209987040",
      "name": "Seddah, Djamé",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2964028830",
      "name": "Sagot, Benoît",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W2898879711",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4294367149",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2143995218",
    "https://openalex.org/W2970529259",
    "https://openalex.org/W2251338526",
    "https://openalex.org/W2969873034",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2952594430",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W79543249",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1865928303",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W331019419",
    "https://openalex.org/W2250781298",
    "https://openalex.org/W3099756172",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2899045532",
    "https://openalex.org/W2780932362",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2130903752",
    "https://openalex.org/W4297801177",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287993739",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2948384082",
    "https://openalex.org/W2563351168",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3006185224",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W131522978",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2251407625",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W3105220303",
    "https://openalex.org/W3009095382",
    "https://openalex.org/W2963667932",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W1973346430",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W1547514161",
    "https://openalex.org/W2552110825"
  ],
  "abstract": "Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models --in all languages except English-- very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7203–7219\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n7203\nCamemBERT: a Tasty French Language Model\nLouis Martin∗1,2,3 Benjamin Muller∗2,3 Pedro Javier Ortiz Suárez∗2,3\nYoann Dupont3 Laurent Romary2 Éric Villemonte de la Clergerie2\nDjamé Seddah2 Benoît Sagot2\n1Facebook AI Research, Paris, France 2Inria, Paris, France\n3Sorbonne Université, Paris, France\nlouismartin@fb.com\n{benjamin.muller, pedro.ortiz, laurent.romary,\neric.de_la_clergerie, djame.seddah, benoit.sagot}@inria.fr\nyoa.dupont@gmail.com\nAbstract\nPretrained language models are now ubiqui-\ntous in Natural Language Processing. Despite\ntheir success, most available models have ei-\nther been trained on English data or on the con-\ncatenation of data in multiple languages. This\nmakes practical use of such models—in all lan-\nguages except English—very limited. In this\npaper, we investigate the feasibility of train-\ning monolingual Transformer-based language\nmodels for other languages, taking French\nas an example and evaluating our language\nmodels on part-of-speech tagging, dependency\nparsing, named entity recognition and natural\nlanguage inference tasks. We show that the use\nof web crawled data is preferable to the use\nof Wikipedia data. More surprisingly, we show\nthat a relatively small web crawled dataset\n(4GB) leads to results that are as good as those\nobtained using larger datasets (130+GB). Our\nbest performing model CamemBERT reaches\nor improves the state of the art in all four down-\nstream tasks.\n1 Introduction\nPretrained word representations have a long history\nin Natural Language Processing (NLP), from non-\ncontextual (Brown et al., 1992; Ando and Zhang,\n2005; Mikolov et al., 2013; Pennington et al., 2014)\nto contextual word embeddings (Peters et al., 2018;\nAkbik et al., 2018). Word representations are usu-\nally obtained by training language model architec-\ntures on large amounts of textual data and then fed\nas an input to more complex task-speciﬁc architec-\ntures. More recently, these specialized architectures\nhave been replaced altogether by large-scale pre-\ntrained language models which are ﬁne-tuned for\neach application considered. This shift has resulted\nin large improvements in performance over a wide\n∗Equal contribution. Order determined alphabetically.\nrange of tasks (Devlin et al., 2019; Radford et al.,\n2019; Liu et al., 2019; Raffel et al., 2019).\nThese transfer learning methods exhibit clear\nadvantages over more traditional task-speciﬁc ap-\nproaches. In particular, they can be trained in an\nunsupervized manner, thereby taking advantage\nof the information contained in large amounts of\nraw text. Yet they come with implementation chal-\nlenges, namely the amount of data and computa-\ntional resources needed for pretraining, which can\nreach hundreds of gigabytes of text and require\nhundreds of GPUs (Yang et al., 2019; Liu et al.,\n2019). This has limited the availability of these\nstate-of-the-art models to the English language, at\nleast in the monolingual setting. This is particularly\ninconvenient as it hinders their practical use in NLP\nsystems. It also prevents us from investigating their\nlanguage modelling capacity, for instance in the\ncase of morphologically rich languages.\nAlthough multilingual models give remarkable\nresults, they are often larger, and their results, as we\nwill observe for French, can lag behind their mono-\nlingual counterparts for high-resource languages.\nIn order to reproduce and validate results that\nhave so far only been obtained for English, we take\nadvantage of the newly available multilingual cor-\npora OSCAR (Ortiz Suárez et al., 2019) to train a\nmonolingual language model for French, dubbed\nCamemBERT. We also train alternative versions\nof CamemBERT on different smaller corpora with\ndifferent levels of homogeneity in genre and style\nin order to assess the impact of these parameters on\ndownstream task performance. CamemBERT uses\nthe RoBERTa architecture (Liu et al., 2019), an im-\nproved variant of the high-performing and widely\nused BERT architecture (Devlin et al., 2019).\nWe evaluate our model on four different down-\nstream tasks for French: part-of-speech (POS) tag-\nging, dependency parsing, named entity recogni-\ntion (NER) and natural language inference (NLI).\n7204\nCamemBERT improves on the state of the art in all\nfour tasks compared to previous monolingual and\nmultilingual approaches including mBERT, XLM\nand XLM-R, which conﬁrms the effectiveness of\nlarge pretrained language models for French.\nWe make the following contributions:\n• First release of a monolingual RoBERTa\nmodel for the French language using recently\nintroduced large-scale open source corpora\nfrom the Oscar collection and ﬁrst outside the\noriginal BERT authors to release such a large\nmodel for an other language than English.1\n• We achieve state-of-the-art results on four\ndownstream tasks: POS tagging, dependency\nparsing, NER and NLI, conﬁrming the effec-\ntiveness of BERT-based language models for\nFrench.\n• We demonstrate that small and diverse train-\ning sets can achieve similar performance to\nlarge-scale corpora, by analysing the impor-\ntance of the pretraining corpus in terms of size\nand domain.\n2 Previous work\n2.1 Contextual Language Models\nFrom non-contextual to contextual word em-\nbeddings The ﬁrst neural word vector repre-\nsentations were non-contextualized word embed-\ndings, most notably word2vec (Mikolov et al.,\n2013), GloVe (Pennington et al., 2014) and fastText\n(Mikolov et al., 2018), which were designed to be\nused as input to task-speciﬁc neural architectures.\nContextualized word representations such as ELMo\n(Peters et al., 2018) and ﬂair (Akbik et al., 2018),\nimproved the representational power of word em-\nbeddings by taking context into account. Among\nother reasons, they improved the performance of\nmodels on many tasks by handling words poly-\nsemy. This paved the way for larger contextualized\nmodels that replaced downstream architectures alto-\ngether in most tasks. Trained with language model-\ning objectives, these approaches range from LSTM-\nbased architectures such as (Dai and Le, 2015),\nto the successful transformer-based architectures\nsuch as GPT2 (Radford et al., 2019), BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2019) and more\nrecently ALBERT (Lan et al., 2019) and T5 (Raffel\net al., 2019).\n1Released at: https://camembert-model.fr un-\nder the MIT open-source license.\nNon-English contextualized models Following\nthe success of large pretrained language models,\nthey were extended to the multilingual setting with\nmultilingual BERT (hereafter mBERT) (Devlin\net al., 2018), a single multilingual model for 104\ndifferent languages trained on Wikipedia data, and\nlater XLM (Lample and Conneau, 2019), which\nsigniﬁcantly improved unsupervized machine trans-\nlation. More recently XLM-R (Conneau et al.,\n2019), extended XLM by training on 2.5TB of\ndata and outperformed previous scores on multi-\nlingual benchmarks. They show that multilingual\nmodels can obtain results competitive with mono-\nlingual models by leveraging higher quality data\nfrom other languages on speciﬁc downstream tasks.\nA few non-English monolingual models have\nbeen released: ELMo models for Japanese, Por-\ntuguese, German and Basque2 and BERT for Sim-\npliﬁed and Traditional Chinese (Devlin et al., 2018)\nand German (Chan et al., 2019).\nHowever, to the best of our knowledge, no par-\nticular effort has been made toward training models\nfor languages other than English at a scale similar\nto the latest English models (e.g. RoBERTa trained\non more than 100GB of data).\nBERT and RoBERTa Our approach is based on\nRoBERTa (Liu et al., 2019) which itself is based on\nBERT (Devlin et al., 2019). BERT is a multi-layer\nbidirectional Transformer encoder trained with a\nmasked language modeling (MLM) objective, in-\nspired by the Cloze task (Taylor, 1953). It comes\nin two sizes: the BERT BASE architecture and the\nBERTLARGE architecture. The BERTBASE architec-\nture is 3 times smaller and therefore faster and\neasier to use while BERTLARGE achieves increased\nperformance on downstream tasks. RoBERTa im-\nproves the original implementation of BERT by\nidentifying key design choices for better perfor-\nmance, using dynamic masking, removing the\nnext sentence prediction task, training with larger\nbatches, on more data, and for longer.\n3 Downstream evaluation tasks\nIn this section, we present the four downstream\ntasks that we use to evaluate CamemBERT, namely:\nPart-Of-Speech (POS) tagging, dependency pars-\ning, Named Entity Recognition (NER) and Natural\nLanguage Inference (NLI). We also present the\nbaselines that we will use for comparison.\n2https://allennlp.org/elmo\n7205\nTasks POS tagging is a low-level syntactic task,\nwhich consists in assigning to each word its corre-\nsponding grammatical category. Dependency pars-\ning consists in predicting the labeled syntactic tree\nin order to capture the syntactic relations between\nwords.\nFor both of these tasks we run our experiments\nusing the Universal Dependencies (UD) 3 frame-\nwork and its corresponding UD POS tag set (Petrov\net al., 2012) and UD treebank collection (Nivre\net al., 2018), which was used for the CoNLL 2018\nshared task (Seker et al., 2018). We perform our\nevaluations on the four freely available French\nUD treebanks in UD v2.2: GSD (McDonald et al.,\n2013), Sequoia4 (Candito and Seddah, 2012; Can-\ndito et al., 2014), Spoken (Lacheret et al., 2014;\nBawden et al., 2014) 5, and ParTUT (Sanguinetti\nand Bosco, 2015). A brief overview of the size and\ncontent of each treebank can be found in Table 1.\nTreebank #Tokens #Sentences Genres\nBlogs, NewsGSD 389,363 16,342 Reviews, Wiki\n····················\nMedical, NewsSequoia 68,615 3,099 Non-ﬁction, Wiki\n····················\nSpoken 34,972 2,786 Spoken\n····················\nParTUT 27,658 1,020 Legal, News, Wikis\n····················\nFTB 350,930 27,658 News\nTable 1: Statistics on the treebanks used in POS tagging,\ndependency parsing, and NER (FTB).\nWe also evaluate our model in NER, which is a\nsequence labeling task predicting which words re-\nfer to real-world objects, such as people, locations,\nartifacts and organisations. We use the French Tree-\nbank6 (FTB) (Abeillé et al., 2003) in its 2008 ver-\nsion introduced by Candito and Crabbé (2009) and\nwith NER annotations by Sagot et al. (2012). The\nFTB contains more than 11 thousand entity men-\ntions distributed among 7 different entity types. A\nbrief overview of the FTB can also be found in\nTable 1.\nFinally, we evaluate our model on NLI, using the\nFrench part of the XNLI dataset (Conneau et al.,\n2018). NLI consists in predicting whether a hypoth-\nesis sentence is entailed, neutral or contradicts a\npremise sentence. The XNLI dataset is the exten-\n3https://universaldependencies.org\n4https://deep-sequoia.inria.fr\n5Speech transcript uncased that includes annotated disﬂu-\nencies without punctuation\n6This dataset has only been stored and used on Inria’s\nservers after signing the research-only agreement.\nsion of the Multi-Genre NLI (MultiNLI) corpus\n(Williams et al., 2018) to 15 languages by trans-\nlating the validation and test sets manually into\neach of those languages. The English training set\nis machine translated for all languages other than\nEnglish. The dataset is composed of 122k train,\n2490 development and 5010 test examples for each\nlanguage. As usual, NLI performance is evaluated\nusing accuracy.\nBaselines In dependency parsing and POS-\ntagging we compare our model with:\n• mBERT: The multilingual cased version of\nBERT (see Section 2.1). We ﬁne-tune mBERT\non each of the treebanks with an additional\nlayer for POS-tagging and dependency pars-\ning, in the same conditions as our Camem-\nBERT model.\n• XLMMLM-TLM: A multilingual pretrained lan-\nguage model from Lample and Conneau\n(2019), which showed better performance\nthan mBERT on NLI. We use the version avail-\nable in the Hugging’s Face transformer library\n(Wolf et al., 2019); like mBERT, we ﬁne-tune\nit in the same conditions as our model.\n• UDify (Kondratyuk, 2019): A multitask and\nmultilingual model based on mBERT, UDify\nis trained simultaneously on 124 different UD\ntreebanks, creating a single POS tagging and\ndependency parsing model that works across\n75 different languages. We report the scores\nfrom Kondratyuk (2019) paper.\n• UDPipe Future (Straka, 2018): An LSTM-\nbased model ranked 3rd in dependency parsing\nand 6th in POS tagging at the CoNLL 2018\nshared task (Seker et al., 2018). We report the\nscores from Kondratyuk (2019) paper.\n• UDPipe Future + mBERT + Flair (Straka\net al., 2019): The original UDPipe Future\nimplementation using mBERT and Flair as\nfeature-based contextualized word embed-\ndings. We report the scores from Straka et al.\n(2019) paper.\nIn French, no extensive work has been done on\nNER due to the limited availability of annotated\ncorpora. Thus we compare our model with the only\nrecent available baselines set by Dupont (2017),\nwho trained both CRF (Lafferty et al., 2001) and\n7206\nBiLSTM-CRF (Lample et al., 2016) architectures\non the FTB and enhanced them using heuristics\nand pretrained word embeddings. Additionally, as\nfor POS and dependency parsing, we compare our\nmodel to a ﬁne-tuned version of mBERT for the\nNER task.\nFor XNLI, we provide the scores of mBERT\nwhich has been reported for French by Wu\nand Dredze (2019). We report scores from\nXLMMLM-TLM (described above), the best model\nfrom Lample and Conneau (2019). We also report\nthe results of XLM-R (Conneau et al., 2019).\n4 CamemBERT: a French Language\nModel\nIn this section, we describe the pretraining data,\narchitecture, training objective and optimisation\nsetup we use for CamemBERT.\n4.1 Training data\nPretrained language models beneﬁts from being\ntrained on large datasets (Devlin et al., 2018; Liu\net al., 2019; Raffel et al., 2019). We therefore use\nthe French part of the OSCAR corpus (Ortiz Suárez\net al., 2019), a pre-ﬁltered and pre-classiﬁed ver-\nsion of Common Crawl.7\nOSCAR is a set of monolingual corpora ex-\ntracted from Common Crawl snapshots. It follows\nthe same approach as (Grave et al., 2018) by us-\ning a language classiﬁcation model based on the\nfastText linear classiﬁer (Grave et al., 2017; Joulin\net al., 2016) pretrained on Wikipedia, Tatoeba and\nSETimes, which supports 176 languages. No other\nﬁltering is done. We use a non-shufﬂed version of\nthe French data, which amounts to 138GB of raw\ntext and 32.7B tokens after subword tokenization.\n4.2 Pre-processing\nWe segment the input text data into subword units\nusing SentencePiece (Kudo and Richardson, 2018).\nSentencePiece is an extension of Byte-Pair encod-\ning (BPE) (Sennrich et al., 2016) and WordPiece\n(Kudo, 2018) that does not require pre-tokenization\n(at the word or token level), thus removing the need\nfor language-speciﬁc tokenisers. We use a vocabu-\nlary size of 32k subword tokens. These subwords\nare learned on 107 sentences sampled randomly\nfrom the pretraining dataset. We do not use sub-\nword regularisation (i.e. sampling from multiple\npossible segmentations) for the sake of simplicity.\n7https://commoncrawl.org/about/\n4.3 Language Modeling\nTransformer Similar to RoBERTa and BERT,\nCamemBERT is a multi-layer bidirectional Trans-\nformer (Vaswani et al., 2017). Given the\nwidespread usage of Transformers, we do not de-\nscribe them here and refer the reader to (Vaswani\net al., 2017). CamemBERT uses the original ar-\nchitectures of BERTBASE (12 layers, 768 hidden\ndimensions, 12 attention heads, 110M parame-\nters) and BERTLARGE (24 layers, 1024 hidden di-\nmensions, 16 attention heads, 335M parameters).\nCamemBERT is very similar to RoBERTa, the\nmain difference being the use of whole-word mask-\ning and the usage of SentencePiece tokenization\n(Kudo and Richardson, 2018) instead of WordPiece\n(Schuster and Nakajima, 2012).\nPretraining Objective We train our model on\nthe Masked Language Modeling (MLM) task.\nGiven an input text sequence composed of N to-\nkens x1,...,x N, we select 15% of tokens for pos-\nsible replacement. Among those selected tokens,\n80% are replaced with the special <MASK> token,\n10% are left unchanged and 10% are replaced by a\nrandom token. The model is then trained to predict\nthe initial masked tokens using cross-entropy loss.\nFollowing the RoBERTa approach, we dynami-\ncally mask tokens instead of ﬁxing them statically\nfor the whole dataset during preprocessing. This\nimproves variability and makes the model more\nrobust when training for multiple epochs.\nSince we use SentencePiece to tokenize our cor-\npus, the input tokens to the model are a mix of\nwhole words and subwords. An upgraded version\nof BERT8 and Joshi et al. (2019) have shown that\nmasking whole words instead of individual sub-\nwords leads to improved performance. Whole-word\nMasking (WWM) makes the training task more dif-\nﬁcult because the model has to predict a whole\nword rather than predicting only part of the word\ngiven the rest. We train our models using WWM\nby using whitespaces in the initial untokenized text\nas word delimiters.\nWWM is implemented by ﬁrst randomly sam-\npling 15% of the words in the sequence and then\nconsidering all subword tokens in each of this 15%\nfor candidate replacement. This amounts to a pro-\nportion of selected tokens that is close to the origi-\nnal 15%. These tokens are then either replaced by\n8https://github.com/google-research/\nbert/blob/master/README.md\n7207\n<MASK> tokens (80%), left unchanged (10%) or\nreplaced by a random token.\nSubsequent work has shown that the next sen-\ntence prediction (NSP) task originally used in\nBERT does not improve downstream task perfor-\nmance (Lample and Conneau, 2019; Liu et al.,\n2019), thus we also remove it.\nOptimisation Following (Liu et al., 2019), we\noptimize the model using Adam (Kingma and Ba,\n2014) (β1 = 0.9, β2 = 0.98) for 100k steps with\nlarge batch sizes of 8192 sequences, each sequence\ncontaining at most 512 tokens. We enforce each se-\nquence to only contain complete paragraphs (which\ncorrespond to lines in the our pretraining dataset).\nPretraining We use the RoBERTa implementa-\ntion in the fairseq library (Ott et al., 2019). Our\nlearning rate is warmed up for 10k steps up to a\npeak value of 0.0007 instead of the original 0.0001\ngiven our large batch size, and then fades to zero\nwith polynomial decay. Unless otherwise speciﬁed,\nour models use the BASE architecture, and are\npretrained for 100k backpropagation steps on 256\nNvidia V100 GPUs (32GB each) for a day. We\ndo not train our models for longer due to practical\nconsiderations, even though the performance still\nseemed to be increasing.\n4.4 Using CamemBERT for downstream\ntasks\nWe use the pretrained CamemBERT in two ways.\nIn the ﬁrst one, which we refer to as ﬁne-tuning,\nwe ﬁne-tune the model on a speciﬁc task in an end-\nto-end manner. In the second one, referred to as\nfeature-based embeddings or simply embeddings,\nwe extract frozen contextual embedding vectors\nfrom CamemBERT. These two complementary ap-\nproaches shed light on the quality of the pretrained\nhidden representations captured by CamemBERT.\nFine-tuning For each task, we append the rel-\nevant predictive layer on top of CamemBERT’s\narchitecture. Following the work done on BERT\n(Devlin et al., 2019), for sequence tagging and se-\nquence labeling we append a linear layer that re-\nspectively takes as input the last hidden represen-\ntation of the <s> special token and the last hidden\nrepresentation of the ﬁrst subword token of each\nword. For dependency parsing, we plug a bi-afﬁne\ngraph predictor head as inspired by Dozat and Man-\nning (2017). We refer the reader to this article for\nmore details on this module. We ﬁne-tune on XNLI\nby adding a classiﬁcation head composed of one\nhidden layer with a non-linearity and one linear\nprojection layer, with input dropout for both.\nWe ﬁne-tune CamemBERT independently for\neach task and each dataset. We optimize the model\nusing the Adam optimiser (Kingma and Ba, 2014)\nwith a ﬁxed learning rate. We run a grid search on\na combination of learning rates and batch sizes. We\nselect the best model on the validation set out of the\n30 ﬁrst epochs. For NLI we use the default hyper-\nparameters provided by the authors of RoBERTa on\nthe MNLI task.9 Although this might have pushed\nthe performances even further, we do not apply\nany regularisation techniques such as weight de-\ncay, learning rate warm-up or discriminative ﬁne-\ntuning, except for NLI. We show that ﬁne-tuning\nCamemBERT in a straightforward manner leads\nto state-of-the-art results on all tasks and outper-\nforms the existing BERT-based models in all cases.\nThe POS tagging, dependency parsing, and NER\nexperiments are run using Hugging Face’s Trans-\nformer library extended to support CamemBERT\nand dependency parsing (Wolf et al., 2019). The\nNLI experiments use the fairseq library following\nthe RoBERTa implementation.\nEmbeddings Following Straková et al. (2019)\nand Straka et al. (2019) for mBERT and the En-\nglish BERT, we make use of CamemBERT in a\nfeature-based embeddings setting. In order to ob-\ntain a representation for a given token, we ﬁrst\ncompute the average of each sub-word’s represen-\ntations in the last four layers of the Transformer,\nand then average the resulting sub-word vectors.\nWe evaluate CamemBERT in the embeddings\nsetting for POS tagging, dependency parsing and\nNER; using the open-source implementations of\nStraka et al. (2019) and Straková et al. (2019).10\n5 Evaluation of CamemBERT\nIn this section, we measure the performance of our\nmodels by evaluating them on the four aforemen-\ntioned tasks: POS tagging, dependency parsing,\nNER and NLI.\n9More details at https://github.com/pytorch/\nfairseq/blob/master/examples/roberta/\nREADME.glue.md.\n10UDPipe Future is available at https://github.\ncom/CoNLL-UD-2018/UDPipe-Future , and the code\nfor nested NER is available at https://github.com/\nufal/acl2019_nested_ner.\n7208\nGSD S EQUOIA SPOKEN PARTUT\nMODEL UPOS LAS UPOS LAS UPOS LAS UPOS LAS\nmBERT (ﬁne-tuned) 97.48 89.73 98.41 91.24 96.02 78.63 97.35 91.37\nXLMMLM-TLM (ﬁne-tuned) 98.13 90.03 98.51 91.62 96.18 80.89 97.39 89.43\nUDify (Kondratyuk, 2019) 97.83 91.45 97.89 90.05 96.23 80.01 96.12 88.06\nUDPipe Future (Straka, 2018) 97.63 88.06 98.79 90.73 95.91 77.53 96.93 89.63\n+ mBERT + Flair (emb.) (Straka et al., 2019) 97.98 90.31 99.32 93.81 97.23 81.40 97.64 92.47\n··················································································································································································································\nCamemBERT (ﬁne-tuned) 98.18 92.57 99.29 94.20 96.99 81.37 97.65 93.43\nUDPipe Future + CamemBERT (embeddings) 97.96 90.57 99.25 93.89 97.09 81.81 97.50 92.32\nTable 2:POS and dependency parsingscores on 4 French treebanks, reported on test sets assuming gold tokeniza-\ntion and segmentation (best model selected on validation out of 4). Best scores in bold, second best underlined.\nModel F1\nSEM (CRF) (Dupont, 2017) 85.02\nLSTM-CRF (Dupont, 2017) 85.57\nmBERT (ﬁne-tuned) 87.35\n······················································································\nCamemBERT (ﬁne-tuned) 89.08\nLSTM+CRF+CamemBERT (embeddings)89.55\nTable 3: NER scores on the FTB (best model selected\non validation out of 4). Best scores in bold, second best\nunderlined.\nModel Acc. #Params\nmBERT (Devlin et al., 2019) 76.9 175M\nXLMMLM-TLM(Lample and Conneau, 2019) 80.2 250M\nXLM-RBASE(Conneau et al., 2019) 80.1 270M\n·········································································································\nCamemBERT (ﬁne-tuned) 82.5 110M\nSupplement: LARGE models\nXLM-RLARGE(Conneau et al., 2019) 85.2 550M\n·········································································································\nCamemBERTLARGE(ﬁne-tuned) 85.7 335M\nTable 4: NLI accuracy on the French XNLI test set\n(best model selected on validation out of 10). Best\nscores in bold, second best underlined.\nPOS tagging and dependency parsing For\nPOS tagging and dependency parsing, we compare\nCamemBERT with other models in the two set-\ntings: ﬁne-tuning and as feature-based embeddings.\nWe report the results in Table 2.\nCamemBERT reaches state-of-the-art scores on\nall treebanks and metrics in both scenarios. The two\napproaches achieve similar scores, with a slight ad-\nvantage for the ﬁne-tuned version of CamemBERT,\nthus questioning the need for complex task-speciﬁc\narchitectures such as UDPipe Future.\nDespite a much simpler optimisation process and\nno task speciﬁc architecture, ﬁne-tuning Camem-\nBERT outperforms UDify on all treebanks and\nsometimes by a large margin (e.g. +4.15% LAS\non Sequoia and +5.37 LAS on ParTUT). Camem-\nBERT also reaches better performance than other\nmultilingual pretrained models such as mBERT\nand XLMMLM-TLM on all treebanks.\nCamemBERT achieves overall slightly bet-\nter results than the previous state-of-the-art and\ntask-speciﬁc architecture UDPipe Future+mBERT\n+Flair, except for POS tagging on Sequoia and POS\ntagging on Spoken, where CamemBERT lags by\n0.03% and 0.14% UPOS respectively. UDPipe Fu-\nture+mBERT +Flair uses the contextualized string\nembeddings Flair (Akbik et al., 2018), which are in\nfact pretrained contextualized character-level word\nembeddings speciﬁcally designed to handle mis-\nspelled words as well as subword structures such\nas preﬁxes and sufﬁxes. This design choice might\nexplain the difference in score for POS tagging\nwith CamemBERT, especially for the Spoken tree-\nbank where words are not capitalized, a factor that\nmight pose a problem for CamemBERT which was\ntrained on capitalized data, but that might be prop-\nerly handle by Flair on the UDPipe Future+mBERT\n+Flair model.\nNamed-Entity Recognition For NER, we simi-\nlarly evaluate CamemBERT in the ﬁne-tuning set-\nting and as input embeddings to the task speciﬁc\narchitecture LSTM+CRF. We report these scores\nin Table 3.\nIn both scenarios, CamemBERT achieves higher\nF1 scores than the traditional CRF-based architec-\ntures, both non-neural and neural, and than ﬁne-\ntuned multilingual BERT models.11\nUsing CamemBERT as embeddings to the tra-\nditional LSTM+CRF architecture gives slightly\nhigher scores than by ﬁne-tuning the model\n(89.08 vs. 89.55). This demonstrates that although\nCamemBERT can be used successfully without any\ntask-speciﬁc architecture, it can still produce high\nquality contextualized embeddings that might be\nuseful in scenarios where powerful downstream\narchitectures exist.\n11XLMMLM-TLM is a lower-case model. Case is crucial for\nNER, therefore we do not report its low performance (84.37%)\n7209\nNatural Language Inference On the XNLI\nbenchmark, we compare CamemBERT to previ-\nous state-of-the-art multilingual models in the ﬁne-\ntuning setting. In addition to the standard Camem-\nBERT model with a BASE architecture, we train\nanother model with the LARGE architecture, re-\nferred to as CamemBERT LARGE, for a fair com-\nparison with XLM-RLARGE. This model is trained\nwith the CCNet corpus, described in Sec. 6, for\n100k steps.12 We expect that training the model for\nlonger would yield even better performance.\nCamemBERT reaches higher accuracy than its\nBASE counterparts reaching +5.6% over mBERT,\n+2.3 over XLM MLM-TLM, and +2.4 over XLM-\nRBASE. CamemBERT also uses as few as half\nas many parameters (110M vs. 270M for XLM-\nRBASE).\nCamemBERTLARGE achieves a state-of-the-art\naccuracy of 85.7% on the XNLI benchmark, as\nopposed to 85.2, for the recent XLM-RLARGE.\nCamemBERT uses fewer parameters than mul-\ntilingual models, mostly because of its smaller vo-\ncabulary size (e.g. 32k vs. 250k for XLM-R). Two\nelements might explain the better performance of\nCamemBERT over XLM-R. Even though XLM-\nR was trained on an impressive amount of data\n(2.5TB), only 57GB of this data is in French,\nwhereas we used 138GB of French data. Addition-\nally XLM-R also handles 100 languages, and the\nauthors show that when reducing the number of\nlanguages to 7, they can reach 82.5% accuracy for\nFrench XNLI with their BASE architecture.\nSummary of CamemBERT’s results Camem-\nBERT improves the state of the art for the 4 down-\nstream tasks considered, thereby conﬁrming on\nFrench the usefulness of Transformer-based mod-\nels. We obtain these results when using Camem-\nBERT as a ﬁne-tuned model or when used as con-\ntextual embeddings with task-speciﬁc architectures.\nThis questions the need for more complex down-\nstream architectures, similar to what was shown\nfor English (Devlin et al., 2019). Additionally, this\nsuggests that CamemBERT is also able to produce\nhigh-quality representations out-of-the-box with-\nout further tuning.\n12We train our LARGE model with the CCNet corpus for\npractical reasons. Given that BASE models reach similar per-\nformance when using OSCAR or CCNet as pretraining corpus\n(Appendix Table 8), we expect an OSCAR LARGE model to\nreach comparable scores.\n6 Impact of corpus origin and size\nIn this section we investigate the inﬂuence of the\nhomogeneity and size of the pretraining corpus on\ndownstream task performance. With this aim, we\ntrain alternative version of CamemBERT by vary-\ning the pretraining datasets. For this experiment,\nwe ﬁx the number of pretraining steps to 100k, and\nallow the number of epochs to vary accordingly\n(more epochs for smaller dataset sizes). All models\nuse the BASE architecture.\nIn order to investigate the need for homogeneous\nclean data versus more diverse and possibly noisier\ndata, we use alternative sources of pretraining data\nin addition to OSCAR:\n• Wikipedia, which is homogeneous in terms\nof genre and style. We use the ofﬁcial\n2019 French Wikipedia dumps13. We remove\nHTML tags and tables using Giuseppe At-\ntardi’sWikiExtractor.14\n• CCNet (Wenzek et al., 2019), a dataset ex-\ntracted from Common Crawl with a different\nﬁltering process than for OSCAR. It was built\nusing a language model trained on Wikipedia,\nin order to ﬁlter out bad quality texts such\nas code or tables. 15 As this ﬁltering step bi-\nases the noisy data from Common Crawl to\nmore Wikipedia-like text, we expect CCNet\nto act as a middle ground between the unﬁl-\ntered “noisy” OSCAR dataset, and the “clean”\nWikipedia dataset. As a result of the differ-\nent ﬁltering processes, CCNet contains longer\ndocuments on average compared to OSCAR\nwith smaller—and often noisier—documents\nweeded out.\nTable 6 summarizes statistics of these different cor-\npora.\nIn order to make the comparison between these\nthree sources of pretraining data, we randomly sam-\nple 4GB of text (at the document level) from OS-\nCAR and CCNet, thereby creating samples of both\nCommon-Crawl-based corpora of the same size as\nthe French Wikipedia. These smaller 4GB samples\nalso provides us a way to investigate the impact\n13https://dumps.wikimedia.org/\nbackup-index.html.\n14https://github.com/attardi/\nwikiextractor.\n15We use the HEAD split, which corresponds to the top 33%\nof documents in terms of ﬁltering perplexity.\n7210\nGSD S EQUOIA SPOKEN PARTUT AVERAGE NER NLIDATASET SIZE UPOS LAS UPOS LAS UPOS LAS UPOS LAS UPOS LAS F1 A CC.\nFine-tuning\nWiki 4GB 98.28 93.04 98.74 92.71 96.61 79.61 96.20 89.67 97.45 88.75 89.86 78.32\nCCNet 4GB 98.34 93.43 98.95 93.67 96.92 82.09 96.50 90.98 97.67 90.04 90.46 82.06\nOSCAR 4GB 98.35 93.55 98.97 93.70 96.94 81.97 96.58 90.28 97.71 89.87 90.65 81.88·······················································································································································································································································\nOSCAR 138GB 98.39 93.80 98.99 94.00 97.17 81.18 96.63 90.56 97.79 89.88 91.55 81.55\nEmbeddings (with UDPipe Future (tagging, parsing) or LSTM+CRF (NER))\nWiki 4GB 98.09 92.31 98.74 93.55 96.24 78.91 95.78 89.79 97.21 88.64 91.23 -\nCCNet 4GB 98.22 92.93 99.12 94.65 97.17 82.61 96.74 89.95 97.81 90.04 92.30 -\nOSCAR 4GB 98.21 92.77 99.12 94.92 97.20 82.47 96.74 90.05 97.82 90.05 91.90 -·······················································································································································································································································\nOSCAR 138GB 98.18 92.77 99.14 94.24 97.26 82.44 96.52 89.89 97.77 89.84 91.83 -\nTable 5: Results on the four tasks using language models pre-trained on data sets of varying homogeneity and size,\nreported on validation sets (average of 4 runs for POS tagging, parsing and NER, average of 10 runs for NLI).\nCorpus Size #tokens #docs Tokens/doc\nPercentiles:\n5% 50% 95%\nWikipedia 4GB 990M 1.4M 102 363 2530\nCCNet 135GB 31.9B 33.1M 128 414 2869\nOSCAR 138GB 32.7B 59.4M 28 201 1946\nTable 6: Statistics on the pretraining datasets used.\nof pretraining data size. Downstream task perfor-\nmance for our alternative versions of CamemBERT\nare provided in Table 5. The upper section reports\nscores in the ﬁne-tuning setting while the lower\nsection reports scores for the embeddings.\n6.1 Common Crawl vs. Wikipedia?\nTable 5 clearly shows that models trained on the\n4GB versions of OSCAR and CCNet (Common\nCrawl) perform consistently better than the the one\ntrained on the French Wikipedia. This is true both\nin the ﬁne-tuning and embeddings setting. Unsur-\nprisingly, the gap is larger on tasks involving texts\nwhose genre and style are more divergent from\nthose of Wikipedia, such as tagging and parsing\non the Spoken treebank. The performance gap is\nalso very large on the XNLI task, probably as a\nconsequence of the larger diversity of Common-\nCrawl-based corpora in terms of genres and topics.\nXNLI is indeed based on multiNLI which covers a\nrange of genres of spoken and written text.\nThe downstream task performances of the mod-\nels trained on the 4GB version of CCNet and OS-\nCAR are much more similar.16\n16We provide the results of a model trained on the whole\nCCNet corpus in the Appendix. The conclusions are similar\nwhen comparing models trained on the full corpora: down-\nstream results are similar when using OSCAR or CCNet.\n6.2 How much data do you need?\nAn unexpected outcome of our experiments is that\nthe model trained “only” on the 4GB sample of OS-\nCAR performs similarly to the standard Camem-\nBERT trained on the whole 138GB OSCAR. The\nonly task with a large performance gap is NER,\nwhere “138GB” models are better by 0.9 F1 points.\nThis could be due to the higher number of named\nentities present in the larger corpora, which is ben-\neﬁcial for this task. On the contrary, other tasks\ndon’t seem to gain from the additional data.\nIn other words, when trained on corpora such\nas OSCAR and CCNet, which are heterogeneous\nin terms of genre and style, 4GB of uncompressed\ntext is large enough as pretraining corpus to reach\nstate-of-the-art results with the BASE architecure,\nbetter than those obtained with mBERT (pretrained\non 60GB of text). 17 This calls into question the\nneed to use a very large corpus such as OSCAR or\nCCNet when training a monolingual Transformer-\nbased language model such as BERT or RoBERTa.\nNot only does this mean that the computational\n(and therefore environmental) cost of training a\nstate-of-the-art language model can be reduced, but\nit also means that CamemBERT-like models can\nbe trained for all languages for which a Common-\nCrawl-based corpus of 4GB or more can be created.\nOSCAR is available in 166 languages, and pro-\nvides such a corpus for 38 languages. Moreover, it\nis possible that slightly smaller corpora (e.g. down\nto 1GB) could also prove sufﬁcient to train high-\nperforming language models. We obtained our re-\nsults with BASE architectures. Further research is\nneeded to conﬁrm the validity of our ﬁndings on\nlarger architectures and other more complex natural\n17The OSCAR-4GB model gets slightly better XNLI accu-\nracy than the full OSCAR-138GB model (81.88 vs. 81.55).\nThis might be due to the random seed used for pretraining, as\neach model is pretrained only once.\n7211\nlanguage understanding tasks. However, even with\na BASE architecture and 4GB of training data, the\nvalidation loss is still decreasing beyond 100k steps\n(and 400 epochs). This suggests that we are still\nunder-ﬁtting the 4GB pretraining dataset, training\nlonger might increase downstream performance.\n7 Discussion\nSince the pre-publication of this work (Martin et al.,\n2019), many monolingual language models have\nappeared, e.g. (Le et al., 2019; Virtanen et al., 2019;\nDelobelle et al., 2020), for as much as 30 languages\n(Nozza et al., 2020). In almost all tested conﬁg-\nurations they displayed better results than multi-\nlingual language models such as mBERT (Pires\net al., 2019). Interestingly, Le et al. (2019) showed\nthat using their FlauBert, a RoBERTa-based lan-\nguage model for French, which was trained on less\nbut more edited data, in conjunction to Camem-\nBERT in an ensemble system could improve the\nperformance of a parsing model and establish a new\nstate-of-the-art in constituency parsing of French,\nhighlighting thus the complementarity of both mod-\nels.18 As it was the case for English when BERT\nwas ﬁrst released, the availability of similar scale\nlanguage models for French enabled interesting\napplications, such as large scale anonymization\nof legal texts, where CamemBERT-based mod-\nels established a new state-of-the-art on this task\n(Benesty, 2019), or the ﬁrst large question answer-\ning experiments on a French Squad data set that\nwas released very recently (d’Hoffschmidt et al.,\n2020) where the authors matched human perfor-\nmance using CamemBERTLARGE. Being the ﬁrst\npre-trained language model that used the open-\nsource Common Crawl Oscar corpus and given\nits impact on the community, CamemBERT paved\nthe way for many works on monolingual language\nmodels that followed. Furthermore, the availability\nof all its training data favors reproducibility and\nis a step towards better understanding such mod-\nels. In that spirit, we make the models used in our\nexperiments available via our website and via the\nhuggingface and fairseq APIs, in addition\nto the base CamemBERT model.\n8 Conclusion\nIn this work, we investigated the feasibility of train-\ning a Transformer-based language model for lan-\n18We refer the reader to (Le et al., 2019) for a comprehen-\nsive benchmark and details therein.\nguages other than English. Using French as an\nexample, we trained CamemBERT, a language\nmodel based on RoBERTa. We evaluated Camem-\nBERT on four downstream tasks (part-of-speech\ntagging, dependency parsing, named entity recog-\nnition and natural language inference) in which our\nbest model reached or improved the state of the\nart in all tasks considered, even when compared to\nstrong multilingual models such as mBERT, XLM\nand XLM-R, while also having fewer parameters.\nOur experiments demonstrate that using web\ncrawled data with high variability is preferable\nto using Wikipedia-based data. In addition we\nshowed that our models could reach surprisingly\nhigh performances with as low as 4GB of pretrain-\ning data, questioning thus the need for large scale\npretraining corpora. This shows that state-of-the-art\nTransformer-based language models can be trained\non languages with far fewer resources than En-\nglish, whenever a few gigabytes of data are avail-\nable. This paves the way for the rise of monolin-\ngual contextual pre-trained language-models for\nunder-resourced languages. The question of know-\ning whether pretraining on small domain speciﬁc\ncontent will be a better option than transfer learn-\ning techniques such as ﬁne-tuning remains open\nand we leave it for future work.\nPretrained on pure open-source corpora, Camem-\nBERT is freely available and distributed with the\nMIT license via popular NLP libraries (fairseq\nand huggingface) as well as on our website\ncamembert-model.fr.\nAcknowledgments\nWe want to thank Clémentine Fourrier for her proof-\nreading and insightful comments, and Alix Chagué\nfor her great logo. This work was partly funded by\nthree French National funded projects granted to\nInria and other partners by the Agence Nationale de\nla Recherche, namely projects PARSITI (ANR-16-\nCE33-0021), SoSweet (ANR-15-CE38-0011) and\nBASNUM (ANR-18-CE38-0003), as well as by the\nlast author’s chair in the PRAIRIE institute funded\nby the French national agency ANR as part of the\n“Investissements d’avenir” programme under the\nreference ANR-19-P3IA-0001.\n7212\nReferences\nAnne Abeillé, Lionel Clément, and François Tou-\nssenel. 2003. Building a Treebank for French,\npages 165–187. Kluwer, Dordrecht.\nAlan Akbik, Duncan Blythe, and Roland V ollgraf.\n2018. Contextual string embeddings for se-\nquence labeling. In Proceedings of the 27th\nInternational Conference on Computational Lin-\nguistics, COLING 2018, Santa Fe, New Mexico,\nUSA, August 20-26, 2018, pages 1638–1649. As-\nsociation for Computational Linguistics.\nRie Kubota Ando and Tong Zhang. 2005. A frame-\nwork for learning predictive structures from mul-\ntiple tasks and unlabeled data. J. Mach. Learn.\nRes., 6:1817–1853.\nRachel Bawden, Marie-Amélie Botalla, Kim\nGerdes, and Sylvain Kahane. 2014. Correct-\ning and validating syntactic dependency in the\nspoken French treebank rhapsodie. In Proceed-\nings of the Ninth International Conference on\nLanguage Resources and Evaluation (LREC’14),\npages 2320–2325, Reykjavik, Iceland. European\nLanguage Resources Association (ELRA).\nMichaël Benesty. 2019. Ner algo benchmark: spacy,\nﬂair, m-bert and camembert on anonymizing\nfrench commercial legal cases.\nPeter F. Brown, Vincent J. Della Pietra, Peter V .\nde Souza, Jennifer C. Lai, and Robert L. Mercer.\n1992. Class-based n-gram models of natural\nlanguage. Computational Linguistics, 18(4):467–\n479.\nMarie Candito and Benoit Crabbé. 2009. Im-\nproving generative statistical parsing with semi-\nsupervised word clustering. In Proc. of IWPT’09,\nParis, France.\nMarie Candito, Guy Perrier, Bruno Guillaume,\nCorentin Ribeyre, Karën Fort, Djamé Seddah,\nand Éric Villemonte de la Clergerie. 2014. Deep\nsyntax annotation of the sequoia french tree-\nbank. In Proceedings of the Ninth International\nConference on Language Resources and Evalua-\ntion, LREC 2014, Reykjavik, Iceland, May 26-31,\n2014., pages 2298–2305. European Language\nResources Association (ELRA).\nMarie Candito and Djamé Seddah. 2012. Le cor-\npus sequoia : annotation syntaxique et exploita-\ntion pour l’adaptation d’analyseur par pont lex-\nical (the sequoia corpus : Syntactic annotation\nand use for a parser lexical domain adaptation\nmethod) [in french]. In Proceedings of the Joint\nConference JEP-TALN-RECITAL 2012, volume\n2: TALN, Grenoble, France, June 4-8, 2012 ,\npages 321–334.\nBranden Chan, Timo Möller, Malte Pietsch, Tanay\nSoni, and Chin Man Yeung. 2019. German bert.\nhttps://deepset.ai/german-bert.\nAlexis Conneau, Kartikay Khandelwal, Naman\nGoyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzmán, Edouard Grave, Myle Ott,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nUnsupervised cross-lingual representation learn-\ning at scale. ArXiv preprint : 1911.02116.\nAlexis Conneau, Ruty Rinott, Guillaume Lample,\nAdina Williams, Samuel R. Bowman, Holger\nSchwenk, and Veselin Stoyanov. 2018. XNLI:\nevaluating cross-lingual sentence representa-\ntions. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Pro-\ncessing, Brussels, Belgium, October 31 - Novem-\nber 4, 2018, pages 2475–2485. Association for\nComputational Linguistics.\nAndrew M. Dai and Quoc V . Le. 2015. Semi-\nsupervised sequence learning. In Advances in\nNeural Information Processing Systems 28: An-\nnual Conference on Neural Information Process-\ning Systems 2015, December 7-12, 2015, Mon-\ntreal, Quebec, Canada, pages 3079–3087.\nPieter Delobelle, Thomas Winters, and Bettina\nBerendt. 2020. RobBERT: a Dutch RoBERTa-\nbased Language Model. ArXiv preprint\n2001.06286.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Multilingual bert.\nhttps://github.com/google-research/\nbert/blob/master/multilingual.md.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training\nof deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the\nAssociation for Computational Linguistics: Hu-\nman Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume\n7213\n1 (Long and Short Papers) , pages 4171–4186.\nAssociation for Computational Linguistics.\nMartin d’Hoffschmidt, Maxime Vidal, Wacim Bel-\nblidia, and Tom Brendlé. 2020. Fquad: French\nquestion answering dataset. arXiv preprint\narXiv:2002.06071.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biafﬁne attention for neural dependency\nparsing. In 5th International Conference on\nLearning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track\nProceedings. OpenReview.net.\nYoann Dupont. 2017. Exploration de traits\npour la reconnaissance d’entit’es nomm’ees du\nfrançais par apprentissage automatique. In 24e\nConf’erence sur le Traitement Automatique des\nLangues Naturelles (TALN), page 42.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta,\nArmand Joulin, and Tomas Mikolov. 2018.\nLearning word vectors for 157 languages. In\nProceedings of the Eleventh International Con-\nference on Language Resources and Evalua-\ntion, LREC 2018, Miyazaki, Japan, May 7-12,\n2018. European Language Resources Associa-\ntion (ELRA).\nEdouard Grave, Tomas Mikolov, Armand Joulin,\nand Piotr Bojanowski. 2017. Bag of tricks for\nefﬁcient text classiﬁcation. In Proceedings of\nthe 15th Conference of the European Chapter of\nthe Association for Computational Linguistics,\nEACL 2017, Valencia, Spain, April 3-7, 2017,\nVolume 2: Short Papers, pages 427–431. Associ-\nation for Computational Linguistics.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure\nof language? In (Korhonen et al., 2019), pages\n3651–3657.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy.\n2019. Spanbert: Improving pre-training by\nrepresenting and predicting spans. CoRR,\nabs/1907.10529.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, Hervé Jégou, and Tomas\nMikolov. 2016. Fasttext.zip: Compressing\ntext classiﬁcation models. ArXiv preprint\n1612.03651.\nDiederik P Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. ArXiv\npreprint 1412.6980.\nDaniel Kondratyuk. 2019. 75 languages, 1\nmodel: Parsing universal dependencies univer-\nsally. CoRR, abs/1904.02099.\nAnna Korhonen, David R. Traum, and Lluís\nMàrquez, editors. 2019. Proceedings of the 57th\nConference of the Association for Computational\nLinguistics, ACL 2019, Florence, Italy, July 28-\nAugust 2, 2019, Volume 1: Long Papers. Associ-\nation for Computational Linguistics.\nTaku Kudo. 2018. Subword regularization: Im-\nproving neural network translation models with\nmultiple subword candidates. In Proceedings\nof the 56th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2018, Mel-\nbourne, Australia, July 15-20, 2018, Volume 1:\nLong Papers, pages 66–75. Association for Com-\nputational Linguistics.\nTaku Kudo and John Richardson. 2018. Sentence-\npiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text\nprocessing. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, EMNLP 2018: System Demonstra-\ntions, Brussels, Belgium, October 31 - November\n4, 2018, pages 66–71. Association for Computa-\ntional Linguistics.\nAnne Lacheret, Sylvain Kahane, Julie Beliao, Anne\nDister, Kim Gerdes, Jean-Philippe Goldman,\nNicolas Obin, Paola Pietrandrea, and Atanas\nTchobanov. 2014. Rhapsodie: a prosodic-\nsyntactic treebank for spoken French. In\nProceedings of the Ninth International Con-\nference on Language Resources and Evalua-\ntion (LREC’14), pages 295–301, Reykjavik, Ice-\nland. European Language Resources Association\n(ELRA).\nJohn D. Lafferty, Andrew McCallum, and Fernando\nC. N. Pereira. 2001. Conditional random ﬁelds:\nProbabilistic models for segmenting and labeling\nsequence data. In Proceedings of the Eighteenth\nInternational Conference on Machine Learning\n(ICML 2001), Williams College, Williamstown,\nMA, USA, June 28 - July 1, 2001, pages 282–289.\nMorgan Kaufmann.\n7214\nGuillaume Lample, Miguel Ballesteros, Sandeep\nSubramanian, Kazuya Kawakami, and Chris\nDyer. 2016. Neural architectures for named en-\ntity recognition. In NAACL HLT 2016, The 2016\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, San Diego Cali-\nfornia, USA, June 12-17, 2016, pages 260–270.\nThe Association for Computational Linguistics.\nGuillaume Lample and Alexis Conneau. 2019.\nCross-lingual language model pretraining.\nCoRR, abs/1901.07291.\nZhenzhong Lan, Mingda Chen, Sebastian Good-\nman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. 2019. ALBERT: A lite BERT for self-\nsupervised learning of language representations.\nArXiv preprint 1909.11942.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne,\nMaximin Coavoux, Benjamin Lecouteux,\nAlexandre Allauzen, Benoît Crabbé, Laurent\nBesacier, and Didier Schwab. 2019. Flaubert:\nUnsupervised language model pre-training for\nfrench. ArXiv : 1912.05372.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. Roberta: A robustly optimized BERT pre-\ntraining approach. ArXiv preprint 1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSuárez, Yoann Dupont, Laurent Romary, Éric\nVillemonte de la Clergerie, Djamé Seddah, and\nBenoît Sagot. 2019. CamemBERT: a Tasty\nFrench Language Model. arXiv e-prints. ArXiv\npreprint : 1911.03894.\nRyan McDonald, Joakim Nivre, Yvonne\nQuirmbach-Brundage, Yoav Goldberg, Dipanjan\nDas, Kuzman Ganchev, Keith Hall, Slav Petrov,\nHao Zhang, Oscar Täckström, Claudia Bedini,\nNúria Bertomeu Castelló, and Jungmee Lee.\n2013. Universal dependency annotation for\nmultilingual parsing. In Proceedings of the 51st\nAnnual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers) ,\npages 92–97, Soﬁa, Bulgaria. Association for\nComputational Linguistics.\nTomas Mikolov, Edouard Grave, Piotr Bojanowski,\nChristian Puhrsch, and Armand Joulin. 2018. Ad-\nvances in pre-training distributed word represen-\ntations. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and\nEvaluation, LREC 2018, Miyazaki, Japan, May\n7-12, 2018.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gre-\ngory S. Corrado, and Jeffrey Dean. 2013. Dis-\ntributed representations of words and phrases\nand their compositionality. In Advances in Neu-\nral Information Processing Systems 26: 27th\nAnnual Conference on Neural Information Pro-\ncessing Systems 2013. Proceedings of a meeting\nheld December 5-8, 2013, Lake Tahoe, Nevada,\nUnited States., pages 3111–3119.\nJoakim Nivre, Mitchell Abrams, Željko Agi ´c,\nLars Ahrenberg, Lene Antonsen, Maria Je-\nsus Aranzabe, Gashaw Arutie, Masayuki Asa-\nhara, Luma Ateyah, Mohammed Attia, Aitziber\nAtutxa, Liesbeth Augustinus, Elena Badmaeva,\nMiguel Ballesteros, Esha Banerjee, Sebastian\nBank, Verginica Barbu Mititelu, John Bauer,\nSandra Bellato, Kepa Bengoetxea, Riyaz Ah-\nmad Bhat, Erica Biagetti, Eckhard Bick, Ro-\ngier Blokland, Victoria Bobicev, Carl Börstell,\nCristina Bosco, Gosse Bouma, Sam Bowman,\nAdriane Boyd, Aljoscha Burchardt, Marie Can-\ndito, Bernard Caron, Gauthier Caron, Gül¸ sen Ce-\nbiro˘glu Eryi˘git, Giuseppe G. A. Celano, Savas\nCetin, Fabricio Chalub, Jinho Choi, Yongseok\nCho, Jayeol Chun, Silvie Cinková, Aurélie Col-\nlomb, Ça˘grı Çöltekin, Miriam Connor, Marine\nCourtin, Elizabeth Davidson, Marie-Catherine\nde Marneffe, Valeria de Paiva, Arantza Diaz de\nIlarraza, Carly Dickerson, Peter Dirix, Kaja\nDobrovoljc, Timothy Dozat, Kira Droganova,\nPuneet Dwivedi, Marhaba Eli, Ali Elkahky,\nBinyam Ephrem, Tomaž Erjavec, Aline Eti-\nenne, Richárd Farkas, Hector Fernandez Al-\ncalde, Jennifer Foster, Cláudia Freitas, Katarína\nGajdošová, Daniel Galbraith, Marcos Garcia,\nMoa Gärdenfors, Kim Gerdes, Filip Ginter,\nIakes Goenaga, Koldo Gojenola, Memduh Gökır-\nmak, Yoav Goldberg, Xavier Gómez Guino-\nvart, Berta Gonzáles Saavedra, Matias Gri-\noni, Normunds Gr ¯uz¯ıtis, Bruno Guillaume,\nCéline Guillot-Barbance, Nizar Habash, Jan\nHajiˇc, Jan Haji ˇc jr., Linh Hà M ˜y, Na-Rae\nHan, Kim Harris, Dag Haug, Barbora Hladká,\nJaroslava Hlaváˇcová, Florinel Hociung, Petter\nHohle, Jena Hwang, Radu Ion, Elena Irimia,\n7215\nTomáš Jelínek, Anders Johannsen, Fredrik Jør-\ngensen, Hüner Ka¸ sıkara, Sylvain Kahane, Hi-\nroshi Kanayama, Jenna Kanerva, Tolga Kayade-\nlen, Václava Kettnerová, Jesse Kirchner, Na-\ntalia Kotsyba, Simon Krek, Sookyoung Kwak,\nVeronika Laippala, Lorenzo Lambertino, Tatiana\nLando, Septina Dian Larasati, Alexei Lavren-\ntiev, John Lee, Ph ương Lê H `ông, Alessan-\ndro Lenci, Saran Lertpradit, Herman Leung,\nCheuk Ying Li, Josie Li, Keying Li, Kyung-\nTae Lim, Nikola Ljubeši´c, Olga Loginova, Olga\nLyashevskaya, Teresa Lynn, Vivien Macketanz,\nAibek Makazhanov, Michael Mandl, Christo-\npher Manning, Ruli Manurung, C˘at˘alina M˘ar˘an-\nduc, David Mareˇcek, Katrin Marheinecke, Héc-\ntor Martínez Alonso, André Martins, Jan Mašek,\nYuji Matsumoto, Ryan McDonald, Gustavo Men-\ndonça, Niko Miekka, Anna Missilä, C˘at˘alin Mi-\ntitelu, Yusuke Miyao, Simonetta Montemagni,\nAmir More, Laura Moreno Romero, Shinsuke\nMori, Bjartur Mortensen, Bohdan Moskalevskyi,\nKadri Muischnek, Yugo Murawaki, Kaili\nMüürisep, Pinkey Nainwani, Juan Ignacio\nNavarro Horñiacek, Anna Nedoluzhko, Gunta\nNešpore-B¯erzkalne, Lương Nguy˜ên Thi., Huy`ên\nNguy˜ên Th i. Minh, Vitaly Nikolaev, Rattima\nNitisaroj, Hanna Nurmi, Stina Ojala, Adédayò.\nOlúòkun, Mai Omura, Petya Osenova, Robert\nÖstling, Lilja Øvrelid, Niko Partanen, Elena\nPascual, Marco Passarotti, Agnieszka Patejuk,\nSiyao Peng, Cenel-Augusto Perez, Guy Per-\nrier, Slav Petrov, Jussi Piitulainen, Emily Pitler,\nBarbara Plank, Thierry Poibeau, Martin Popel,\nLauma Pretkalni n, a, Sophie Prévost, Prokopis\nProkopidis, Adam Przepiórkowski, Tiina Puo-\nlakainen, Sampo Pyysalo, Andriela Rääbis,\nAlexandre Rademaker, Loganathan Ramasamy,\nTaraka Rama, Carlos Ramisch, Vinit Ravis-\nhankar, Livy Real, Siva Reddy, Georg Rehm,\nMichael Rießler, Larissa Rinaldi, Laura Rituma,\nLuisa Rocha, Mykhailo Romanenko, Rudolf\nRosa, Davide Rovati, Valentin Ros, ca, Olga Rud-\nina, Shoval Sadde, Shadi Saleh, Tanja Samardži´c,\nStephanie Samson, Manuela Sanguinetti, Baiba\nSaul¯ıte, Yanin Sawanakunanon, Nathan Schnei-\nder, Sebastian Schuster, Djamé Seddah, Wolf-\ngang Seeker, Mojgan Seraji, Mo Shen, Atsuko\nShimada, Muh Shohibussirri, Dmitry Sichinava,\nNatalia Silveira, Maria Simi, Radu Simionescu,\nKatalin Simkó, Mária Šimková, Kiril Simov,\nAaron Smith, Isabela Soares-Bastos, Antonio\nStella, Milan Straka, Jana Strnadová, Alane\nSuhr, Umut Sulubacak, Zsolt Szántó, Dima Taji,\nYuta Takahashi, Takaaki Tanaka, Isabelle Tellier,\nTrond Trosterud, Anna Trukhina, Reut Tsarfaty,\nFrancis Tyers, Sumire Uematsu, Zde ˇnka Ure-\nšová, Larraitz Uria, Hans Uszkoreit, Sowmya\nVajjala, Daniel van Niekerk, Gertjan van No-\nord, Viktor Varga, Veronika Vincze, Lars Wallin,\nJonathan North Washington, Seyi Williams,\nMats Wirén, Tsegay Woldemariam, Tak-sum\nWong, Chunxiao Yan, Marat M. Yavrumyan,\nZhuoran Yu, Zdenˇek Žabokrtský, Amir Zeldes,\nDaniel Zeman, Manying Zhang, and Hanzhi\nZhu. 2018. Universal dependencies 2.2. LIN-\nDAT/CLARIN digital library at the Institute of\nFormal and Applied Linguistics (ÚFAL), Faculty\nof Mathematics and Physics, Charles University.\nDebora Nozza, Federico Bianchi, and Dirk Hovy.\n2020. What the [mask]? making sense\nof language-speciﬁc BERT models. CoRR,\nabs/2003.02912.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Lau-\nrent Romary. 2019. Asynchronous pipeline for\nprocessing huge corpora on medium to low re-\nsource infrastructures. Challenges in the Man-\nagement of Large Corpora (CMLC-7) 2019 ,\npage 9.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. 2019. fairseq: A fast, ex-\ntensible toolkit for sequence modeling. In Pro-\nceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Demonstrations, pages 48–53.\nAssociation for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors\nfor word representation. In Proceedings of the\n2014 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2014, Oc-\ntober 25-29, 2014, Doha, Qatar, A meeting of\nSIGDAT, a Special Interest Group of the ACL ,\npages 1532–1543. ACL.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. 2018. Deep contextual-\nized word representations. In Proceedings of the\n7216\n2018 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT\n2018, New Orleans, Louisiana, USA, June 1-6,\n2018, Volume 1 (Long Papers), pages 2227–2237.\nAssociation for Computational Linguistics.\nSlav Petrov, Dipanjan Das, and Ryan T. McDonald.\n2012. A universal part-of-speech tagset. In Pro-\nceedings of the Eighth International Conference\non Language Resources and Evaluation, LREC\n2012, Istanbul, Turkey, May 23-25, 2012, pages\n2089–2096. European Language Resources As-\nsociation (ELRA).\nTelmo Pires, Eva Schlinger, and Dan Garrette.\n2019. How multilingual is multilingual bert?\narXiv:1906.01502.\nAlec Radford, Jeffrey Wu, Rewon Child, David\nLuan, Dario Amodei, and Ilya Sutskever. 2019.\nLanguage models are unsupervised multitask\nlearners. OpenAI Blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. 2019. Ex-\nploring the limits of transfer learning with a\nuniﬁed text-to-text transformer. ArXiv preprint\n1910.10683.\nBenoît Sagot, Marion Richard, and Rosa Stern.\n2012. Annotation référentielle du corpus arboré\nde Paris 7 en entités nommées (referential named\nentity annotation of the paris 7 french treebank)\n[in french]. In Proceedings of the Joint Con-\nference JEP-TALN-RECITAL 2012, volume 2:\nTALN, Grenoble, France, June 4-8, 2012, pages\n535–542. ATALA/AFCP.\nManuela Sanguinetti and Cristina Bosco. 2015.\nPartTUT: The Turin University Parallel Tree-\nbank. In Roberto Basili, Cristina Bosco, Rodolfo\nDelmonte, Alessandro Moschitti, and Maria\nSimi, editors, Harmonization and Development\nof Resources and Tools for Italian Natural Lan-\nguage Processing within the PARLI Project, vol-\nume 589 of Studies in Computational Intelli-\ngence, pages 51–69. Springer.\nMike Schuster and Kaisuke Nakajima. 2012.\nJapanese and korean voice search. In 2012 IEEE\nInternational Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 5149–\n5152. IEEE.\nAmit Seker, Amir More, and Reut Tsarfaty. 2018.\nUniversal morpho-syntactic parsing and the con-\ntribution of lexica: Analyzing the onlp lab sub-\nmission to the conll 2018 shared task. In Pro-\nceedings of the CoNLL 2018 Shared Task: Mul-\ntilingual Parsing from Raw Text to Universal\nDependencies, pages 208–215.\nRico Sennrich, Barry Haddow, and Alexandra\nBirch. 2016. Neural machine translation of rare\nwords with subword units. In Proceedings of the\n54th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2016, August 7-12,\n2016, Berlin, Germany, Volume 1: Long Papers.\nThe Association for Computer Linguistics.\nMilan Straka. 2018. Udpipe 2.0 prototype at conll\n2018 ud shared task. In Proceedings of the\nCoNLL 2018 Shared Task: Multilingual Pars-\ning from Raw Text to Universal Dependencies,\npages 197–207.\nMilan Straka, Jana Straková, and Jan Hajic. 2019.\nEvaluating contextualized embeddings on 54 lan-\nguages in POS tagging, lemmatization and de-\npendency parsing. ArXiv preprint 1908.07448.\nJana Straková, Milan Straka, and Jan Hajic. 2019.\nNeural architectures for nested NER through lin-\nearization. In (Korhonen et al., 2019), pages\n5326–5331.\nWilson L Taylor. 1953. “cloze procedure”: A new\ntool for measuring readability. Journalism Bul-\nletin, 30(4):415–433.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline.\nIn Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics,\npages 4593–4601, Florence, Italy. Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In Advances in Neu-\nral Information Processing Systems 30: Annual\nConference on Neural Information Processing\nSystems 2017, 4-9 December 2017, Long Beach,\nCA, USA, pages 5998–6008.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni\nLuoma, Juhani Luotolahti, Tapio Salakoski, Filip\n7217\nGinter, and Sampo Pyysalo. 2019. Multilingual\nis not enough: Bert for ﬁnnish. ArXiv preprint\n1912.07076.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis\nConneau, Vishrav Chaudhary, Francisco\nGuzmán, Armand Joulin, and Edouard Grave.\n2019. CCNet: Extracting High Quality Mono-\nlingual Datasets from Web Crawl Data. ArXiv\npreprint 1911.00359.\nAdina Williams, Nikita Nangia, and Samuel R.\nBowman. 2018. A broad-coverage challenge\ncorpus for sentence understanding through infer-\nence. In Proceedings of the 2018 Conference\nof the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New\nOrleans, Louisiana, USA, June 1-6, 2018, Vol-\nume 1 (Long Papers), pages 1112–1122.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R’emi Louf, Morgan\nFuntowicz, and Jamie Brew. 2019. Hugging-\nface’s transformers: State-of-the-art natural lan-\nguage processing. ArXiv preprint 1910.03771.\nShijie Wu and Mark Dredze. 2019. Beto, bentz,\nbecas: The surprising cross-lingual effectiveness\nof BERT. CoRR, abs/1904.09077.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G.\nCarbonell, Ruslan Salakhutdinov, and Quoc V .\nLe. 2019. Xlnet: Generalized autoregressive\npretraining for language understanding. CoRR,\nabs/1906.08237.\n7218\nAppendix\nIn the appendix, we analyse different design\nchoices of CamemBERT (Table 8), namely with\nrespect to the use of whole-word masking, the train-\ning dataset, the model size, and the number of train-\ning steps in complement with the analyses of the\nimpact of corpus origin an size (Section 6. In all the\nablations, all scores come from at least 4 averaged\nruns. For POS tagging and dependency parsing, we\naverage the scores on the 4 treebanks. We also re-\nport all averaged test scores of our different models\nin Table 7.\nA Impact of Whole-Word Masking\nIn Table 8, we compare models trained using\nthe traditional subword masking with whole-word\nmasking. Whole-Word Masking positively impacts\ndownstream performances for NLI (although only\nby 0.5 points of accuracy). To our surprise, this\nWhole-Word Masking scheme does not beneﬁt\nmuch lower level task such as Name Entity Recog-\nnition, POS tagging and Dependency Parsing.\nB Impact of model size\nTable 8 compares models trained with the BASE\nand LARGE architectures. These models were\ntrained with the CCNet corpus (135GB) for prac-\ntical reasons. We conﬁrm the positive inﬂuence\nof larger models on the NLI and NER tasks. The\nLARGE architecture leads to respectively 19.7%\nerror reduction and 23.7%. To our surprise, on POS\ntagging and dependency parsing, having three time\nmore parameters doesn’t lead to a signiﬁcant differ-\nence compared to the BASE model. Tenney et al.\n(2019) and Jawahar et al. (2019) have shown that\nlow-level syntactic capabilities are learnt in lower\nlayers of BERT while higher level semantic repre-\nsentations are found in upper layers of BERT. POS\ntagging and dependency parsing probably do not\nbeneﬁt from adding more layers as the lower layers\nof the BASE architecture already capture what is\nnecessary to complete these tasks.\nC Impact of training dataset\nTable 8 compares models trained on CCNet and\non OSCAR. The major difference between the two\ndatasets is the additional ﬁltering step of CCNet\nthat favors Wikipedia-Like texts. The model pre-\ntrained on OSCAR gets slightly better results on\nPOS tagging and dependency parsing, but gets a\n0 20000 40000 60000 80000 100000\nsteps\n60\n65\n70\n75\n80\n85\n90\n95\n100Scores\nParsing\nNER\nNLI\nLanguage Modelling\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\n8.0\nPerplexity\nFigure 1: Impact of number of pretraining steps on\ndownstream performance for CamemBERT.\n.\nlarger +1.31 improvement on NER. The CCNet\nmodel gets better performance on NLI (+0.67).\nD Impact of number of steps\nFigure 1 displays the evolution of downstream task\nperformance with respect to the number of steps.\nAll scores in this section are averages from at least\n4 runs with different random seeds. For POS tag-\nging and dependency parsing, we also average the\nscores on the 4 treebanks.\nWe evaluate our model at every epoch (1 epoch\nequals 8360 steps). We report the masked language\nmodelling perplexity along with downstream per-\nformances. Figure 1, suggests that the more com-\nplex the task the more impactful the number of\nsteps is. We observe an early plateau for depen-\ndency parsing and NER at around 22k steps, while\nfor NLI, even if the marginal improvement with\nregard to pretraining steps becomes smaller, the\nperformance is still slowly increasing at 100k steps.\nIn Table 8, we compare two models trained on\nCCNet, one for 100k steps and the other for 500k\nsteps to evaluate the inﬂuence of the total number\nof steps. The model trained for 500k steps does\nnot increase the scores much from just training\nfor 100k steps in POS tagging and parsing. The\nincrease is slightly higher for XNLI (+0.84).\nThose results suggest that low level syntactic\nrepresentation are captured early in the language\nmodel training process while it needs more steps\nto extract complex semantic information as needed\nfor NLI.\n7219\nGSD S EQUOIA SPOKEN PARTUT NER NLIDATASET MASKING ARCH. #S TEPS UPOS LAS UPOS LAS UPOS LAS UPOS LAS F1 A CC.\nFine-tuning\nOSCAR Subword B ASE 100k 98.25 92.29 99.25 93.70 96.95 79.96 97.73 92.68 89.23 81.18\nOSCAR Whole-word B ASE 100k 98.21 92.30 99.21 94.33 96.97 80.16 97.78 92.65 89.11 81.92\nCCNET Subword B ASE 100k 98.02 92.06 99.26 94.13 96.94 80.39 97.55 92.66 89.05 81.77\nCCNET Whole-word B ASE 100k 98.03 92.43 99.18 94.26 96.98 80.89 97.46 92.33 89.27 81.92\nCCNET Whole-word B ASE 500k 98.21 92.43 99.24 94.60 96.69 80.97 97.65 92.48 89.08 83.43\nCCNET Whole-word L ARGE 100k 98.01 91.09 99.23 93.65 97.01 80.89 97.41 92.59 89.39 85.29\nEmbeddings (with UDPipe Future (tagging, parsing) or LSTM+CRF (NER))\nOSCAR Subword B ASE 100k 98.01 90.64 99.27 94.26 97.15 82.56 97.70 92.70 90.25 -\nOSCAR Whole-word B ASE 100k 97.97 90.44 99.23 93.93 97.08 81.74 97.50 92.28 89.48 -\nCCNET Subword B ASE 100k 97.87 90.78 99.20 94.33 97.17 82.39 97.54 92.51 89.38 -\nCCNET Whole-word B ASE 100k 97.96 90.76 99.23 94.34 97.04 82.09 97.39 92.82 89.85 -\nCCNET Whole-word B ASE 500k 97.84 90.25 99.14 93.96 97.01 82.17 97.27 92.28 89.07 -\nCCNET Whole-word L ARGE 100k 98.01 90.70 99.23 94.01 97.04 82.18 97.31 92.28 88.76 -\nTable 7: Performance reported on Test setsfor all trained models (average over multiple ﬁne-tuning seeds).\nDATASET MASKING ARCH . #P ARAM . #S TEPS UPOS LAS NER XNLI\nMasking Strategy\nOSCAR Subword B ASE 110M 100k 97.78 89.80 91.55 81.04\nOSCAR Whole-word B ASE 110M 100k 97.79 89.88 91.44 81.55\nModel Size\nCCNet Whole-word B ASE 110M 100k 97.67 89.46 90.13 82.22\nCCNet Whole-word L ARGE 335M 100k 97.74 89.82 92.47 85.73\nDataset\nCCNet Whole-word B ASE 110M 100k 97.67 89.46 90.13 82.22\nOSCAR Whole-word B ASE 110M 100k 97.79 89.88 91.44 81.55\nNumber of Steps\nCCNet Whole-word B ASE 110M 100k 98.04 89.85 90.13 82.20\nCCNet Whole-word B ASE 110M 500k 97.95 90.12 91.30 83.04\nTable 8: Comparing scores on the Validation setsof different design choices. POS tagging and parsing datasets\nare averaged. (average over multiple ﬁne-tuning seeds).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8695942759513855
    },
    {
      "name": "Natural language processing",
      "score": 0.720847487449646
    },
    {
      "name": "Concatenation (mathematics)",
      "score": 0.6693050861358643
    },
    {
      "name": "Parsing",
      "score": 0.6566874980926514
    },
    {
      "name": "Inference",
      "score": 0.6547205448150635
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6440751552581787
    },
    {
      "name": "Language model",
      "score": 0.6367722749710083
    },
    {
      "name": "Transformer",
      "score": 0.599600076675415
    },
    {
      "name": "Dependency grammar",
      "score": 0.5988783836364746
    },
    {
      "name": "Dependency (UML)",
      "score": 0.5906705856323242
    },
    {
      "name": "Natural language",
      "score": 0.48742416501045227
    },
    {
      "name": "Training set",
      "score": 0.4140388071537018
    },
    {
      "name": "Data modeling",
      "score": 0.41343021392822266
    },
    {
      "name": "Database",
      "score": 0.14083600044250488
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210091437",
      "name": "Sorbonne Paris Cité",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I39804081",
      "name": "Sorbonne Université",
      "country": "FR"
    }
  ],
  "cited_by": 687
}