{
  "title": "TSSTDet: Transformation-Based 3-D Object Detection via a Spatial Shape Transformer",
  "url": "https://openalex.org/W4390817445",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4384186952",
      "name": "Hiep Anh Hoang",
      "affiliations": [
        "Soongsil University"
      ]
    },
    {
      "id": "https://openalex.org/A5103143190",
      "name": "Duy-Cuong Bui",
      "affiliations": [
        "Soongsil University"
      ]
    },
    {
      "id": "https://openalex.org/A2113243307",
      "name": "Myungsik Yoo",
      "affiliations": [
        "Soongsil University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2897529137",
    "https://openalex.org/W2963727135",
    "https://openalex.org/W3034602892",
    "https://openalex.org/W3166089996",
    "https://openalex.org/W639708223",
    "https://openalex.org/W4382240183",
    "https://openalex.org/W3118341329",
    "https://openalex.org/W3034314779",
    "https://openalex.org/W4384159702",
    "https://openalex.org/W2949708697",
    "https://openalex.org/W4226439325",
    "https://openalex.org/W4200632008",
    "https://openalex.org/W4312799694",
    "https://openalex.org/W3170033848",
    "https://openalex.org/W4214777292",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W2968296999",
    "https://openalex.org/W2981949127",
    "https://openalex.org/W3035346742",
    "https://openalex.org/W2555618208",
    "https://openalex.org/W2964062501",
    "https://openalex.org/W4385804883",
    "https://openalex.org/W4383066393",
    "https://openalex.org/W2963400571",
    "https://openalex.org/W3217335336",
    "https://openalex.org/W6803556390",
    "https://openalex.org/W6749757727",
    "https://openalex.org/W3034493208",
    "https://openalex.org/W2953668091",
    "https://openalex.org/W3191573718",
    "https://openalex.org/W4313036632",
    "https://openalex.org/W4312910764",
    "https://openalex.org/W4388936705",
    "https://openalex.org/W4310172519",
    "https://openalex.org/W4226494869",
    "https://openalex.org/W3106728613",
    "https://openalex.org/W4293811845",
    "https://openalex.org/W2963057320",
    "https://openalex.org/W3205005447",
    "https://openalex.org/W2963182550",
    "https://openalex.org/W6775948036",
    "https://openalex.org/W3117804044",
    "https://openalex.org/W3004237909",
    "https://openalex.org/W3017930107",
    "https://openalex.org/W3130463448",
    "https://openalex.org/W4318833281",
    "https://openalex.org/W4386075636",
    "https://openalex.org/W2997814983",
    "https://openalex.org/W6772253573",
    "https://openalex.org/W3113028524",
    "https://openalex.org/W3202229469",
    "https://openalex.org/W4312546175",
    "https://openalex.org/W4206645116",
    "https://openalex.org/W4312410080",
    "https://openalex.org/W4385835750",
    "https://openalex.org/W4386076370",
    "https://openalex.org/W4322706885",
    "https://openalex.org/W4386065523",
    "https://openalex.org/W3035172746",
    "https://openalex.org/W3216015562",
    "https://openalex.org/W4312976258",
    "https://openalex.org/W3107819843",
    "https://openalex.org/W3016041489",
    "https://openalex.org/W2796426482",
    "https://openalex.org/W3008105217"
  ],
  "abstract": "Accurately detecting and understanding the shapes of objects in three-dimensional (3-D) scenes are essential for autonomous driving. In a 3-D scene, objects are distributed with various incomplete shapes and rotations. Determining the shape allows for a comprehensive understanding of an object's dimensions, rotations, and spatial relationships with its surroundings. Traditional detection methods do not explicitly consider the rotations and complete shapes that objects can assume. Consequently, these methods require large networks and extensive data augmentation to detect accurately. Taking advantage of the Vision-Transformer (ViT), we introduce an efficient transformer-based 3-D detector called Transformation-based 3-D Object Detection via a Spatial Shape Transformer (TSSTDet) to address these challenges. We constructed TSSTDet as a multistage detector based on a LiDAR point cloud. Specifically, TSSTDet utilizes a sparse convolution backbone to extract multi-channel and transformation-equivariant voxel features. Furthermore, we designed an efficient module that employs the transformer approach to estimate the completed shape of an object. These features are then aligned and aggregated to create lightweight and compact representations that enable high-performance 3-D object detection. We assessed the effectiveness of the proposed framework by evaluating its performance on both the KITTI and Waymo Open datasets. These evaluations demonstrated that our framework achieves top-tier performance in 3-D object detection.",
  "full_text": "IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2023 1\nTSSTDet: Transformation-based 3-D Object\nDetection via a Spatial Shape Transformer\nHiep Anh Hoang, Duy Cuong Bui, and Myungsik Y oo\nRotation  \nTransformation\nBackbone\nShape\nTransformer\nFusion\nAnd\nRefinement\n+\nDetection Result\nBird’s Eye View (BEV)\nFront View\nLiDAR Point Cloud\nFront View\nBird’s Eye View (BEV)\nAbstract— Accurately detecting and understanding the shapes of\nobjects in three-dimensional (3-D) scenes are essential for au-\ntonomous driving. In a 3-D scene, objects are distributed with vari-\nous incomplete shapes and rotations. Determining the shape allows\nfor a comprehensive understanding of an object’s dimensions, ro-\ntations, and spatial relationships with its surroundings. Traditional\ndetection methods do not explicitly consider the rotations and\ncomplete shapes that objects can assume. Consequently, these\nmethods require large networks and extensive data augmentation to\ndetect accurately. Taking advantage of the Vision-Transformer (ViT),\nwe introduce an efficient transformer-based 3-D detector called\nTransformation-based 3-D Object Detection via a Spatial Shape\nTransformer (TSSTDet) to address these challenges. We constructed TSSTDet as a multistage detector based on a LiDAR\npoint cloud. Specifically, TSSTDet utilizes a sparse convolution backbone to extract multi-channel and transformation-\nequivariant voxel features. Furthermore, we designed an efficient module that employs the transformer approach to\nestimate the completed shape of an object. These features are then aligned and aggregated to create lightweight and\ncompact representations that enable high-performance 3-D object detection. We assessed the effectiveness of the\nproposed framework by evaluating its performance on both the KITTI and Waymo Open datasets. These evaluations\ndemonstrated that our framework achieves top-tier performance in 3-D object detection.\nIndex Terms— Autonomous driving, LiDAR point cloud, 3-D object detection, vision transformer\nI. I NTRODUCTION\nR\nEsearch on three-dimensional (3-D) object detection in\nautonomous vehicles from light detection and ranging\n(LiDAR) data is highly important, as it enables safe and\nefficient navigation. Unlike two-dimensional (2-D) object de-\ntection, 3-D object detection expands the search space from a\n2-D image plane to a 3-D space. A key aspect of 3-D object\ndetection in self-driving is identifying the category of interest\n(such as car, pedestrian, cyclist) by a classification task and\nlocalizing objects in a scene by regressing their 3-D spatial\nbounding box. Precisely detecting and recognizing objects in\nthis 3-D space is necessary for a self-driving car to be aware\nof the environment and to make correct judgments in smart\ncities and urban areas.\nLiDAR point clouds are largely used for ambient perception\nin current 3-D object detection approaches. A precise 3-D map\nManuscript received XXX xx, xxxx; accepted XXX xx, xxxx. Date\nof publication XXX xx, xxxx; date of current version XXX x, xxxx.\nThis work was supported by a National Research Foundation of Korea\n(NRF) grant funded by the Government of South Korea (MSIT) (NRF-\n2021R1A2B5B01002559).\nHiep Anh Hoang is with the Department of Information Communica-\ntion Convergence Technology, Soongsil University, Seoul 06978, South\nKorea (email: hiepbk97@soongsil.ac.kr)\nDuy Cuong Bui is with the Department of Information Communica-\ntion Convergence Technology, Soongsil University, Seoul 06978, South\nKorea (email: cuonga1@soongsil.ac.kr)\nMyungsik Y oo is with the School of Electronic Engineering, Soongsil\nUniversity, Seoul 06978, South Korea (e-mail: myoo@ssu.ac.kr).\nof the environment is produced using this point cloud, which\nprovides precise knowledge about objects and their locations\nfrom the sensor. Utilizing this 3-D point cloud map as input,\ncutting-edge 3-D detection methods can effectively identify\nobjects of interest, thereby enhancing perception within the\n3-D environment.\nCurrent approaches in this field can be categorized into\nsingle-stage or multistage 3-D detection methodologies.\nSingle-stage methods employ encoded features obtained di-\nrectly from point clouds for object detection [1]–[4]. In\ncontrast, multistage approaches construct their models using a\nregion-based convolutional neural network (RNN) framework,\nas in [5]. This method involves generating a set of potential\nbounding boxes, followed by the classification and refinement\nof each candidate box. Several recent studies have favored\nthe multistage framework, owing to its superior accuracy [6]–\n[10]. For instance, V oxel-RCNN [7] and Point RCNN [10]\nproduce and enhance region proposals by utilizing voxel and\npoint features, respectively.\nNonetheless, the widespread adoption of multistage 3-D\ndetectors utilizing LiDAR technology has encountered signifi-\ncant challenges. The first challenge arises from missing object\nshapes resulting from occlusions or sensor-signal disruptions.\nThe second challenge pertains to the diversity of object\norientations. These factors collectively contribute to the loss\nof essential geometric and semantic details, thereby impeding\nprecise object recognition.\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2023\nTo address the first challenge, point cloud prediction and\nshape-estimation methods, such as SIENet [11], estimate the\ncomplete shapes of the foreground within proposed scenes to\nobtain structural information. This enables the acquisition of\nrepresentative features for subsequent box refinement. To some\nextent, BtcDet [12] addresses the challenges of incomplete\npoint cloud structures. These studies demonstrate that enhanc-\ning the completeness of a point cloud structure can enhance\nthe precision of 3-D object detection. Nonetheless, existing\nmethods for reconstructing spatial structures inevitably lead\nto higher computational demands and reasoning times for\nthe detector, especially when employing the conventional\nconvolutional neural network (CNN) paradigm with deeper\nnetwork architectures. The advantages offered by transformers\nwith multi-head attention in the field of computer vision [13]\nhave ushered in numerous promising advancements and have\npaved the way for addressing this challenge in a novel manner.\nThe second challenge relates to the numerous possible\nrotations that objects can take within a 3-D space. In 3-D\nscenes, objects display a vast spectrum of orientations. That is,\nif an object alters its orientation within the input data points, its\ndetected bounding box should retain its shape while aligning\nthe angle to match. A 3-D detector’s predictions should be\nequivariant concerning rotations and transformation reflec-\ntions. Unfortunately, traditional detectors do not explicitly\naddress the variety of rotation and reflection transformations,\nwhich can potentially result in unreliable prediction outcomes\nwhen addressing changed point clouds.\nRecently, some detectors have achieved approximate trans-\nformation equivariance through data augmentation [8], [10].\nHowever, the production of huge training samples and the\nuse of more sophisticated networks with larger capacities\nsubstantially influence their effectiveness. Recently, equiv-\nariant neural networks [14], [15] that explicitly model the\ntransformation equivariance have been developed. The equiv-\nariant design has produced optimistic results in autonomous\ndriving. To accomplish this, they converted the input data\nusing various rotation bins and represented equivariance using\nshared convolutional networks. TED [6], a recent multiple-\nbased method, has demonstrated encouraging outcomes when\nhandling transformation equivariance and invariance in 3-\nD object detection. However, TED cannot fully address the\nchallenges associated with occlusion.\nIn this study, we introduce a novel 3-D detection net-\nwork known as TSSTDet ( Transformation-based 3-D Object\nDetection via a Spatial Shape Transformer) to enhance the\nperformance of 3-D object detection models. Our research\nprimarily focuses on two major issues that previous 3-D object\ndetectors have faced. Specifically, TSSTDet is a multistage\nframework that can handle the orientation and missing points\nof the shape problems of an object. TSSTDet consists of a\n3-D rotational-transformation backbone tasked with extracting\ndiverse rotation-equivariant features to address object-rotation\nchallenges. In addition, we designed a transformer-based deep\nnetwork, called a voxel-point shape transformer (VPST), to\nreconstruct object shapes, primarily to address the occlu-\nsion problems in autonomous driving scenarios. Furthermore,\nTSSTDet integrates an aided network called the attention-\nfusion and refinement module to aggregate features from\nthe preceding steps, thereby enhancing the object-proposal\nconfidence.\nOur contributions can be summarized as follows:\n1) We present the TSSTDet framework, a robust multistage\napproach for an efficient rotational-transformation 3-D\nobject detector for object-geometry modeling.\n2) We propose an efficient transformer-based module called\nthe voxel-point shape transformer, which is capable of\ndealing with occlusion challenges by reconstructing the\ncomplete shape of an object.\n3) Our model surpasses the performance of current cutting-\nedge models across varying levels of complexity within\nthe car, pedestrian, and cyclist categories. Our achieve-\nments on the KITTI leaderboard for 3D object detection\nare also notably impressive.\nII. R ELATED WORK\nA. Mainstream 3-D Object Detection\n3-D object detection methods can be categorized, based on\ntheir data-processing approaches, into LiDAR-based methods\nand multimodal-based methods.\n1) LiDAR-based 3-D Object Detection: Significant research\nhas been conducted on 3-D object detection using LiDAR\ntechnology over the past few years. Recently, two primary\napproaches, voxel-based [1], [2], [7], [16] and point-based set\nabstraction [10], [17], [18], have emerged as key methods for\ncrafting efficient detection frameworks.\nV oxel-based approaches represent a scene using voxels.\nV oxels divide the 3-D space into regular grid cells and en-\ncode information about occupancy or object attributes within\neach cell. The pioneering V oxelNet [2] utilized a voxel-based\nrepresentation and a 3-D convolutional neural network (CNN)\nto detect objects in point clouds. It operated on a fixed-size\nvoxel grid and performed 3-D convolutions to extract features\nand predict object attributes. SECOND [1] employs sparse\nconvolutional layers to handle sparse and irregular point cloud\nrepresentations. To overcome the limitations of 3-D CNN\nlayers, PointPillars [19] transforms voxels into pillars that are\narranged from a bird’s-eye view (BEV) perspective, utilizing\npseudo-images as the representation.\nPoint-based approaches operate directly on individual points\nin a point cloud without voxelization. Point-based methods\nhave been applied to both multistage [8], [10], [20] and single-\nstage [3], [21] methods. PointNet [17] is a ground-breaking\ndeep-learning architecture capable of detecting objects in\nunordered point clouds. By employing symmetric functions\nand a shared multi-layer perceptron (MLP) network, Point-\nNet effectively extracts features from individual points. This\nmethod achieved a notable performance in 3-D segmentation\nand object-classification tasks. PointRCNN [10] incorporates\na region proposal network (RPN) and a second-stage network\nthat performs region-of-interest (RoI) pooling and point-wise\nfeature learning. By exploiting both local and global contextual\ninformation, PointRCNN improves the accuracy of object\ndetection tasks.\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS AND JOURNALS (2023) 3\n2) Multimodal-based 3-D Object Detection: Multimodal-\nbased 3-D object detection involves integrating both 2-D and\n3-D data. These multimodal methods harness the synergies\nbetween 2-D image-based and 3-D point cloud-based detec-\ntors. In the initial stages of this approach, techniques emerged\nthat extended features derived from LiDAR points with image-\nbased features to enhance 3-D object detection. For instance,\nMV3D [22] integrates 2-D and 3-D object detection by fusing\ninformation from images and LiDAR data. F-PointNet [23]\nextends PointNet to incorporate 2-D image features, thereby\nimproving the detection accuracy.\nCertain studies [24], [25] independently encoded features\nfrom two modalities and then merged these features within\na local RoI or BEV plane. Typically, aggregated-view object\ndetection (A VOD) [26] utilizes feature extractors to obtain\nfeatures from both BEV feature maps and 2-D RGB images\nbefore aggregating multimodal features to produce 3-D object\nproposals. Recently, some studies [27], [28] fused camera\nimages and LiDAR point clouds via virtual points to exploit\nthe advantages of a depth-estimation task. However, virtual\npoints are highly concentrated and frequently contain noise,\ndegrading the performance of the 3-D detector.\nB. 3-D-Object Shape Reconstruction\nPoint cloud reconstruction and completion are essential\ntasks in computer vision and 3-D scene understanding with the\naim of recovering missing or incomplete parts of a point cloud\nrepresentation. Owing to occlusions, light reflections, and\nrestrictions on viewing angles and resolution, raw point clouds\nproduced by LiDAR are sparse and lacking in geometric and\nsemantic information. Hence, the missing pieces of 3-D shapes\nshould be generated from a partially-observed point cloud or\nwith variable levels of noise for the 3-D object detector.\nRecent research has focused on deep-learning-based\nmethodologies that aim to determine the entire point cloud\nusing the extracted features and the network’s capacity for\nrepresentational modeling. A two-stage generation procedure\nwith a folding architecture was introduced by FoldingNet\n[29], with the presumption that 3-D objects can be recovered\nfrom 2-D manifolds. Subsequently, SA-Net [30] introduced a\nhierarchical folding approach in a multi-stage point-generation\ndecoder. Nevertheless, this presents challenges in terms of\ninterpreting and constraining the implicit representation of\nthe complete shape across intermediate layers, which hinders\nshape refinement within the local region.\nIn addition, to randomly group points in point cloud comple-\ntion tasks, TopNet [31] uses a decoder in a hierarchical rooted-\ntree form. The implicit intermediary in decoding SA-Net and\nTopNet, similar to FoldingNet, is a point feature that denotes\nthe form structure, which is challenging to constrain explicitly.\nTo overcome this issue, SnowflakeNet [32] utilizes snowflake\npoint deconvolution (SPD) to extract parent patterns and feed\nthem across to child points via a pointwise-splitting procedure.\nThis enables the network to produce a precise geometry of the\nobjects.\nWith the rapid emergence of transformer-based methods,\nShapeFormer [33] introduces a vector-quantized deep implicit\nfunction (VQDIF) to acquire a sparse representation of partial\npoint clouds and employs transformer-based techniques to\ngenerate complete shapes. Moreover, AutoSDF [34] presents\nan autoregressive generation method over a discrete distribu-\ntion to complete the object shape from a partial point cloud.\nThanks to recent advancements in 3-D object reconstruction,\n3-D detectors can incorporate the capability to infer missing\npoints within obscured shapes.\nC. Transformer-based Object Detection\nInspired by the remarkable success of transformer archi-\ntectures in natural language processing (NLP), researchers\nhave recently extended their applications to computer-vision\n(CV) tasks. Although convolutional neural networks (CNNs)\nhave long been regarded as foundational components in vision\napplications [35], [36], transformers are now emerging as\na promising alternative. Image-generative pre-trained trans-\nformer (Image GPT) [37] pioneered the introduction of trans-\nformers to 2-D image-classification tasks through unsuper-\nvised learning. Furthermore, the vision transformer (ViT) [13]\nrepresents a pure transformer approach that is directly applied\nto sequences of image patches for image-classification tasks.\nFor high-level vision tasks, certain object detection methods\nhave delved into the potential of self-attention mechanisms\nand have proceeded to improve the pertinent modules for\nmodern detectors, such as the feature-fusion module [38] and\nthe prediction head [39].\nIn 3-D vision, a point-attention transformer (PAT) [40] in-\ntroduced Gumbel subset sampling and group-shuffle attention\nfor permutation-invariant tasks within point clouds. CT3D [41]\nutilizes a high-quality region-proposal network and channel-\nwise transformer architecture for improved accuracy and min-\nimal handcrafted design. V oxel Transformer (V oTr) [16] intro-\nduces a voxel-based transformer backbone to address the issue\nof limited receptive fields, allowing self-attention to establish\nlong-range relationships between voxels. A Cascade Attention\nnetwork (CasA) [39] introduced a method that gradually im-\nproved and supplemented predictions by utilizing multiple sub-\nnetworks to achieve high-quality predictions. CasA improves\nthe accuracy of proposal refinement by considering the quality\nof proposals from all preceding stages, while consolidating\nobject features across several phases. These studies served as\ninspiration for the approach used in this study, which merges\nfeatures gathered from different subnetworks to produce better\n3-D object detection.\nIII. M ETHODOLOGY\nThis section provides a comprehensive description of\nthe proposed model. In Section III-A, we provide an\noverview of the proposed architecture. Subsection III-B out-\nlines the elements of Stage 1, encompassing the rotational-\ntransformation backbone, rotation-transformation pooling, and\nregion-proposal network (RPN). Section III-C describes the\ncore structure of Stage 2, which enhances the geometric\nattributes of objects. Section III-D describes the attention-\nfusion and refinement network responsible for synthesizing\nfeatures and refining the final prediction of a 3-D object\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2023\nRTConv\nRT Voxel\nPooling\nTransformation\nFeatures\nBEV Map\nRPN\nRT BEV\nPooling\nHead\nDetection\nAttention Fusion and\nRefinement Network\nTransform\nProposal\nVoxel-Point\nShape \nTransformer\n... ...\nAFM ARM2\nARM1\nARM3\n(1) (2) (3)\nFig. 1. Our TSSTDet multistage-framework architecture. (1) A 3-D rotational-transformation convolution backbone (RTConv), RT BEV pooling,\nand RT voxel pooling are applied on multiple rotated point clouds to capture the multi-channel rotational-transformation features. (2) The voxel-\npoint shape transformer reconstructs the completed shape from a partial observation. (3) The attention fusion and refinement network aggregates\nmultiple features for proposal refinement.\ndetection task. Subsection III-E describes the loss functions\nemployed in the proposed model.\nA. Overall Architecture\nThe overall architecture of the proposed method is illus-\ntrated in Fig. 1. Our aim is to develop a robust 3-D detector for\noutdoor scenarios in which objects of interest may be obscured\nand exhibit varying orientations. To address this challenge, we\nintroduced a 3-D sparse rotation-transformation convolution\nbackbone to effectively capture orientation features. Further-\nmore, we propose a voxel-point shape transformer (VPST)\nmodule to address the issue of missing object shapes.\nOur approach involves adopting a resilient transformation-\nequivariant method, empowering the model to gain insights\ninto the object’s orientation, while also gaining insight into the\nobject’s shape through shape reconstruction. This understand-\ning enables the model to accurately determine the necessary\nlocation for synthesizing the object’s pattern.\nTo achieve this, we adopted a multi-stage approach. In the\nfirst stage, a 3-D rotation-transformation convolution backbone\nencodes a LiDAR point cloud that yields transformation-\nequivariant features. These features are then pooled by the\ntransformation BEV pooling to generate bird’s-eye view fea-\nture maps. Subsequently, a region-proposal network is em-\nployed to generate proposal bounding boxes for the objects.\nWe then utilize transformation voxel pooling to incorporate\ntransformation-invariant features into the proposals for the\nnext stage. In the next stage, we present a voxel-point shape\ntransformer (shown in Figure 2) to address the challenge of\nmissing shape information for foreground objects. Specifically,\nwe transform the voxel coordinates of foreground objects from\na 3-D space into an n × n × n grid space. Subsequently, a\ntransformer network is used to estimate the complete shape of\nthe object. Finally, we designed an attention-fusion module to\nconsolidate the feature map, and a multi-attention refinement\nmodule to fine-tune the final precise prediction.\nB. Stage 1: Rotational-Transformation Feature Extraction\n1) Rotational-Transformation Backbone: Most 3-D detectors\nemploy point- or voxel-based methods; however, these conven-\ntional approaches lack rotation and translation equivariance.\nWe introduce a rotational-transformation convolution back-\nbone (RTconv) to efficiently encode raw points while ensuring\ntransformation equivariance. RTconv was constructed based\non the widely adopted sparse convolution (SpConv) [42].\nAlthough SpConv exhibits translation equivariance similar to\nthat of CNNs, it lacks equivariance to rotation. To overcome\nthis issue, we incorporated additional rotational channels that\nenabled the adaptation of rotation-equivariant features. Our\nRTConv facilitates the learning of object-level equivariant\nfeatures in outdoor scenes.\nWith the input point cloud denoted as P, the rotational-\ntransformation backbone assists the detector Dϕ using the\nrotational-transformation action T in detecting the bounding\nbox B as follows:\nDϕ[T (P)] = T [Dϕ(P)], T ∈G,\nG ∼= ⊕ ⋊ω, (1)\nwhere G represents a rotational-transformation group com-\nprising the 2-D BEV translation set (⊕ ∈ R2) and 2-D\nBEV rotation-reflection set ω. In particular, ω encompasses\na reflection action set {(±1, ∗)} and a discrete rotation set\n{ϕi, i∈ (0, 2π)}. Considering N discrete rotation cases, each\nmultiplied by the reflection case, ω forms a discrete subgroup\nof order 2N.\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS AND JOURNALS (2023) 5\nSequence \nGrid \nFunction \n0 1 V -2 V -1 \nDictionary \nDown \nSampler \nUp \nSampler \nDecoder \n Shape \nCompletion \nT ransformer \n... \nFig. 2. Overview of our voxel-point shape transformer (VPST). Given a partial voxel shape V , our VPST encoder first uses a patch-wise on\nthe object shape to convert it to a grid feature sequence. Subsequently, a downsampler reduces the grid’s dimension. These features are then\nsubstituted with the indices of their nearest neighbors in a learned dictionary D, forming a compact-sequence discrete tuple pair comprising\nthe coordinate and quantized-feature index value. An autoregressive transformer yields a complete sequence for the object shape. Finally, the\nupsampler and decoder transform the sequence into point cloud features.\nAccording to [15], we applied the transformation actions\n{Tk}2N\nk=1 to transform P, resulting in the generation of 2N\nsets, denoted as {PTk }\n2N\nk=1. A mean voxel feature-extraction\nprocess is then applied to all points, producing mean voxel\nsets { ˆPTk }\n2N\nk=1. Subsequently, we utilized SpConv Ψ(·) with\na filter configuration of (16, 32, 64, 64) to encode the mean\nvoxels into rotational-transformation voxel features:\nVTk = Ψ( ˆPTk ), k= 1, 2, ...,2N. (2)\nIn contrast to the voxel features extracted by normal Sp-\nConv, the features {VTk }\n2N\nk=1 contain a variety of properties\nrelated to rotation and symmetric translation.\n2) Rotational-Transformation Pooling: Pooling operations\nplay a vital role in the multistage approach by reducing\nthe spatial resolution and expediting computation. However,\nregular pooling methods may not effectively adapt to fea-\ntures derived from RTConv. To address this issue, we pro-\npose rotational-transformation BEV (RT BEV) and rotational-\ntransformation voxel (RT voxel) pooling.\nFirst, RT BEV pooling was designed to align and con-\nsolidate scene-level voxel features into a concise BEV map,\nthrough a combination of bilinear interpolation and max-\npooling. In essence, it involves compressing the voxel features\ndenoted as {VTk }\n2N\nk=1 into BEV features {BTk }\n2N\nk=1 along\nthe height dimension. The BEV features must be aligned to\nthe same coordinate system because they have been acquired\nthrough various transformations. To achieve this, we convert\nthe grid points into a BEV coordinate system, resulting in the\ncreation of a new set of grid points, denoted as {XTk }\n2N\nk=1.\nThis transformation process is performed in accordance with\nthe set of transformation actions {Tk}2N\nk=1.\nTo ensure a precise spatial alignment and maintain con-\nsistency in the transformation process, we apply a series of\nbilinear interpolations Γ to the BEV coordinates to obtain a\nset of aligned features {FTk\na }\n2N\nk=1. The interpolated feature is\npadded by zeroes if the border pixel in BT1 has no matching\npixel in BT2 , ..., BTN . The aligned features were calculated as\nfollows:\nFTk\na = Γ(XTk , BTk ), k= 1, 2, ...,2N. (3)\nSubsequently, max-pooling M(.) is employed along the\n2N-aligned feature maps. The concise BEV feature F∗\na is\naggregated as follows:\nF∗\na = M(FT1\na , FT2\na , ...,FT2N\na ). (4)\nAfter the RT BEV pooling process, we employed the RPN\nto efficiently produce a proposal list B∗ from the lightweight\nBEV feature F∗\na . These proposals were leveraged to determine\nthe object features for the subsequent stage.\nNext, we introduced the RT voxel-pooling method, draw-\ning inspiration from [15], to aggregate the rotational-\ntransformation features. Because the RT backbone produces\ndiverse rotational-transformation features, whereas proposal\nB∗ is formulated in the original coordinate system T1, our\nmodel requires the proposal to be aligned before pooling.\nIn our case, it was impossible to directly use a conven-\ntional RoI pooling process [8], [10], [12] to extract features\nfrom our backbone. Therefore, from the proposal layer B∗\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n6 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2023\ncorresponding to the original coordinate, we apply multigrid\npooling with size (6×6×6) to obtain a set of RoI grid points.\nSubsequently, these multiple RoI grid points are transformed\ninto the coordinate systems of each channel within {ATk }\n2N\nk=1\nby employing transformation actions {T 2, ...,T 2N }.\nFor a proposal within B∗, we initially create 2 N sets of\ninstance-level grid points {XTk }\n2N\nk=1, with each set represented\nas XTk = {XTk\nm }M\nm=1, where M indicates the number of grid\npoints contained in each set. These grid points were generated\nbased on the rotational-transformation actions {Tk}2N\nk=1. We\nthen apply voxel-set abstraction ϑ(·) [8] for voxel neighbor\naggregation:\nFTk\nP = ϑ(XTk , VTk ), k= 1, 2, ...,2N. (5)\nThis process yields multiple pooled instance-level features\n{FTk\nP }2N\nk=1 ⊂ R1×C, where C denotes the number of grid-wise\nfeature channels.\n3) Region-Proposal Network: We employed a similar ap-\nproach [7], [8] for the RPN design to generate high-quality\n3-D proposals. The proposed bounding boxes effectively offer\ninformation regarding the positions and orientations of the\nobjects in the subsequent stage. First, the RPN condenses\nthe 3-D feature volume by stacking it along the z-axis and\nsubsequently applies a series of 2-D convolutions to the BEV\nfeature maps. The proposals contain anchor classification and\nregression of the object size, location, and orientation angles\nwith respect to the ground-truth bounding boxes.\nSpecifically, we adopted intersection over union (IoU)-based\nmatching to assign ground-truth bounding boxes to anchors,\nfollowing [8]. The anchor configuration is defined as (l ×w ×\nh), where l, w,and h are the length, width, and height of the\nbounding boxes, respectively. We used the common setting,\nwhich is (3.9 × 1.6 × 1.56), (0.8 × 0.6 × 1.73), and (1.76 ×\n0.6×1.73) for car, pedestrian, and cyclist objects, respectively.\nThe RPN loss function is formulated as follows:\nLRPN = 1\nNp\nhX\ni\nLscore(αi, ˆαi)\n+ γ(IoUi > u)\nX\ni\nLreg(δi, ˆδi)\ni\n,\n(6)\nwhere Lscore, Lreg, αi, ˆαi, δi, and ˆδi denote the smooth L1\nloss, binary cross-entropy loss, score prediction, score target,\nresidual prediction, and residual target, respectively. Note that\nthe regression loss is computed only for object proposals with\n(IoUi > u).\nC. Stage 2: 3-D Object-Shape Enhancement\nTo enhance the 3-D detector’s performance, we introduced a\ndeep-learning network called a voxel-point shape transformer\n(VPST). VPST is transformer-based and can reconstruct a\ncomplete shape from partial observations, effectively address-\ning issues related to occlusion. Our autoregressive approach\naims to learn the distribution p(X) from the partial observation\nof 3-D shapes X to infer the complete shape Y, as illustrated\nin Fig. 2.\n[END] \nComplete Coordinate\nComplete Value\nIncomplete Coordinate\n[END] Incomplete Value\n192 ... [END] 375 128 450 ... 612 [END] \n192 1246 [END] 375 564 128 ... 372 ... [END] \n99 250 [END] 35 12 45 ... 189 ... [END] \nCoordinate Head\nCoordinate Transformer\nEmbedded Input\nValue Head\nValue Transformer\nEmbedded Input\n+ \n99 ... [END] 35 45 79 ... 128 [END] \nFig. 3. The architecture of the shape-completion transformer includes\ntwo transformer blocks: the coordinate transformer and value trans-\nformer. The discrete tuple pair contains a partial sequence (repre-\nsented by dashed boxes) and the complete sequence (represented by\nsolid boxes). Both sequences include an appended end token. These\nsequences are concatenated, and their locations ( ci, enclosed in a\nblue border) and values ( vi, enclosed in a red border) are fed to a\ncoordinate transformer to predict the subsequent location ( ci+1). The\nvalue transformer utilizes both ci+1 and the previous transformer’s\noutput features to predict the next value (vi+1).\nFirst, from the object features of the previous stage, we\nsplit the partial object shape into patches using a patch-\nwise encoder [34]. This enables the independent encoding\nof the local context with partial observations. Our goal is to\ndownscale the high-dimensional persistent 3-D form into a\ndiscrete latent space to train an efficient autoregressive model.\nTherefore, we convert the patches of the object shape into a\nvolumetric grid space G with a resolution R using a sequence\ngrid function. Such a grid space and dictionary allow for the\nefficient modeling of global dependencies by transformers that\nallow forms to be represented as compact sequences of entry\nindices, describing the local shapes inside all nonempty grid\nelements.\nWe used R = 64 resolution for the first grid space (64 ×\n64 × 64), and these cube features were then downsampled\nto a lower-dimensional discrete space (32 × 32 × 32) using\na downsampler. Subsequently, the nonempty features were\nreshaped into a sequence of length K in row-major order.\nThese features are sparse; thus, we use a flattened index\n{ci}K−1\ni=0 to capture their coordinates.\nWe reduced the bit size of the feature sequence {zi}K−1\ni=0\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS AND JOURNALS (2023) 7\nthrough vector quantization followed by mapping onto the\nnearest element in the dictionary D of V embeddings {ej}V\nj=0.\nThe index values of these entries were calculated as follows:\nvi = argminj||zi − ej||2, j∈ [0, V). (7)\nSubsequently, a discrete tuple pair of a compact sequence\nfor the object shape S = {(ci, vi)}K−1\ni=0 was produced. To\npredict the distribution of the next element conditioned on\nprevious elements using the autoregressive model, we designed\na shape-completion transformer (SCT) module, as shown in\nFig. 3. Specifically, we stacked two transformer blocks to\npredict pci and pvi, following [33]. The distribution of each\nentry in the tuple is calculated as follows:\np(SC|SP;ϕ) =\nK−1Y\ni=0\npci × pvi\npci = p(ci|c<i, v<i, SP; ϕ)\npvi = p(vi|c≤i, v<i, SP; ϕ),\n(8)\nwhere SC, SP, ϕ, pci, and pvi are the complete sequence,\npartial sequence, model parameters, distribution of coordi-\nnates, and index value, respectively. In our case, the index\nvalue pvi depends on the current coordinate ci. To facilitate\nthe learning strategy, an extra end token was attached to\nboth sequences. The loss function of the shape-completion\ntransformer was calculated as follows:\nLSCT = −logp(SC|SP; ϕ). (9)\nAfter obtaining SC, an upsampler and decoder were used.\nInitially, the quantized sparse sequence is mapped onto a 3-\nD feature grid. The decoder uses this feature to infer a large\nmap. To help the model better grasp the global knowledge\nof the object shapes from partial observations, our decoder\nis composed of multiple ResNet blocks, following [33]. This\nallows the model to comprehend the distributions that could\nexist both within and without the object shape.\nD. Stage 3: Attention Fusion and Refinement Network\nTo aggregate all the features from the different domains,\nwe employed a collection of different layers for proposal\nrefinement. The features of the different phases were regularly\nconcatenated in a simple operation. Nonetheless, determining\nthe relationship of features among multiple stages is challeng-\ning because concatenating them without additional processing\ncan lead to interference.\nMotivated by recent attention methods [39], [43], we present\nan attention-based mechanism to facilitate the combination of\nproposal features from various stages. Given a region proposal\nB∗, we apply the RT voxel-pooling module to extract the\ninstance-level features of partial shape FTk\nP = {FTk\nm }M\nm=1 ⊂\nR1×C, as mentioned in Section III-B. Simultaneously, we\nacquire a feature representing the complete shape, which is\ndenoted as FTk\nC .\nAs depicted in Figure 4, we first concatenate the instance-\nlevel features of partial and complete shapes corresponding\nto rotational ordering FTk = [ FTk\nP , FTk\nC ]. This combined\nfeature is then input into the attention-fusion module (AFM),\nwhich helps determine the significance of both the pooled and\nshape-information features. For each rotational-transformation\nfeature (blue, pink, or orange), the attention-fusion module\nemploys channel-wise and point-wise max-pooling layers,\nallowing features to be extracted from various dimensions to\ncapture different perspectives. These features are subsequently\nprocessed using two fully-connected layers followed by the\nrectified linear unit (ReLU) activation function. Following\nthis, matrix multiplication is used to consolidate the channel-\nwise and point-wise attention maps. The attention values are\nnormalized using a sigmoid function. Finally, an attention map\nis employed to enhance the concatenated features through\nelement-wise multiplication. These combined features ˆFTk\nserve as inputs to the attention-refinement module, which\ngenerates the final precise result.\nIn the attention-refinement module (ARM), we gather the\nencoded features from all previous layers and the current\nlayer as ˆFj = [ ˆFT0 , ˆFT1 , ...,ˆFTj ], j ∈ (0, k). Subsequently,\nwe project these features linearly to obtain Qj = ˆFjWj\nq,\nKj = ˆFjWj\nk, and Vj = ˆFjWj\nv, which represent the query,\nkey, and value embeddings, respectively. To effectively capture\nthe relationship between the various rotational layers, we\nemployed multi-head attention. The attention value of each\nattention head i is\nˆFj\ni = softmax\n\n\nQj\ni\n\u0010\nKj\ni\n\u0011T\n√\nC\n′\n\nVj\ni , (10)\nwhere C\n′\nis the channel size in the multi-head attention.\nSubsequently, the box-voting method [39] was adopted to\ndirectly standardize the confidence prediction and fuse the\nbounding boxes:\nC = 1\nNr\nX\nj\nCj (11)\nB = 1P\nj Cj\nX\nj\nCj · Bj. (12)\nHere, C and B represent the merged confidence prediction\nand bounding boxes, respectively. Following the box-voting\nprocess, we obtain a collection of refined, high-quality boxes.\nTo eliminate redundancies, we conduct non-maximum sup-\npression (NMS) on the voted results to generate the final de-\ntection outputs. This voting mechanism enables the integration\nof diverse predictions from different refiners, leading to more\naccurate and reliable final predictions.\nE. Overall Loss Function\nThe proposed TSSTDet is trained in an end-to-end manner.\nOur overall loss function includes LRPN of the region pro-\nposal network, LSCT of the shape-completion transformer, and\nLAFR of the attention-fusion and refinement network. LAFR\nis the sum of multiple refinement losses in multiple layers, as\nmentioned in Section III-B.3.\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2023\nFC1 \nFC2 Relu \nX\nRelu \nFC1 \nFC2 Relu \nX\nRelu \nQ\nK\nV\nA\nC\nC Q\nK\nV\nA\nC\nC\nC\nQ\nK\nV\nA\nC\nC\nHead\nDetection\nAttention Fusion Module (AFM)\nAttention Refinement Module (ARM)\nFC1 \nFC2 Relu \nX\nRelu \nFig. 4. Illustration of the attention-fusion and refinement network. The\nAFM aggregates all features from the preceding stage, which are then\nrefined by the ARM to produce the final precise predictions. The C, A,\n×, σ, and · denote the concatenation operation, attention operation,\nmatrix multiplication, sigmoid function, and element-wise multiplication,\nrespectively.\nWithin each refinement-attention layer, we incorporate the\nbox regression loss Lreg and score loss Lscore following [1],\n[8]. The loss function for the attention-fusion and refinement\n(AFR) module is formulated as follows:\nLAFR = 1\nNb\nhX\ni\nX\nj\nLscore(αj\ni , ˆαj\ni )\n+ γ(IoU j\ni > ui)\nX\ni\nX\nj\nLreg(δj\ni , ˆδj\ni )\ni\n,\n(13)\nwhere Lscore, Lreg, αj\ni , ˆαj\ni , δj\ni , and ˆδj\ni denote the smooth L1\nloss, binary cross-entropy loss, score prediction, score target,\nresidual prediction, and residual target for the ith proposal at\nthe jth refinement layer, respectively. Note that the regression\nloss is computed only for object proposals with (IoUi > u).\nOverall, the total loss Ltotal is calculated as follows:\nLtotal = LRPN + µLSCT + LAFR , (14)\nwhere µ is a hyperparameter. We set µ = 0.9 for the best\nperformance.\nIV. E XPERIMENTS\nA. Datasets and Metrics\n1) KITTI Dataset: The KITTI dataset [61] is a widely used\ndataset that consists of 7,481 LiDAR training frames and\n7,518 LiDAR testing frames. For our experiment, we split the\ntraining data into two sets, one with 3712 frames for training\nand another with 3769 frames for validation, following a state-\nof-the-art method [6], [8]. The primary evaluation metric was\nthe 3-D average precision (AP) calculated under 40 different\nrecall thresholds (R40) at the easy, moderate, and hard levels.\nIn addition, we report the results on the validation set using\na 3-D AP metric under 11 recall thresholds for the moderate\ncar class, following the conventions set by previous studies\n[2], [8]. The specified IoU thresholds for this metric are 0.7\nfor cars, 0.5 for pedestrians, and 0.5 for cyclists.\n2) Waymo Open Dataset: The Waymo Open Dataset\n(WOD) [62] includes 798, 202, and 150 sequences for training,\nvalidation, and testing, respectively. Our model’s performance\nwas assessed using the following official metrics: 3-D mean\naverage precision (mAP) and 3-D mAP weighted by heading\n(mAPH) for all categories. The LEVEL 1 mAP computes the\nmean average precision for classes with more than five points,\nwhereas the LEVEL 2 mAP computes the mean average\nprecision for classes with at least one point. It is worth noting\nthat the primary ranking metric for the Waymo 3-D detection\nchallenge is mAPH (L2), with IoU thresholds of 0.7, 0.5, and\n0.5 for vehicles, pedestrians, and cyclists, respectively.\nB. Setup Details\nFor the KITTI dataset, our detection range, number of\nproposals, and NMS threshold were consistent with those used\nin the baseline detectors, V oxel-RCNN [7] and TED [6]. The\nvoxel size configuration was (0.05m, 0.05m, 0.1m), and we\nset the range of the point cloud to be [0, 70.4m], [−40m, 40m],\nand [−3m, 1m] for the X, Y , and Z axes, respectively. We\nconfigured the number of rotations as N = 3, and the number\nof multi-head attentions as j = 3.\nOur TSSTDet model was trained for 40 epochs using\nthe ADAM optimizer. The learning rate, weight decay, and\nmomentum were 0.01, 0.01, and 0.9, respectively. In detail,\nwe configure the momentum schedule to decrease from 0.95\nto 0.85 and decay the learning rate from epoch 35 to epoch\n40 during training. The learning rate decay, percentage of\ntotal epochs for rising learning rate, and the gradient clipping\nthreshold are 0.1, 0.4, and 10, respectively. We utilized an\nNMS threshold of 0.8 to produce 160 RoI proposals with an\nequal ratio of positive and negative samples. For the testing\nphase, after proposal refinement, we adjusted the threshold to\n0.1 to eliminate redundant boxes. With regard to data aug-\nmentation, our approach performed excellently, even without\nincorporating rotation and reflection data-augmentation tech-\nniques. Similarly, for the Waymo dataset [62], we adopted the\nsame configuration as that of PV-RCNN [8]. Throughout the\nexperiments, TSSTDet was trained on two NVIDIA GeForce\nRTX 3090 GPUs with a batch size of four.\nV. R ESULTS\nThis section presents a comparative analysis of our\nproposed model against state-of-the-art methods, utilizing the\nKITTI and WOD datasets. For the KITTI offline evaluation,\nour model was exclusively trained on the 3712-sample\ntraining set, and the results were subsequently reported on\nthe validation set. Our model outperformed all the other\nmodels on the KITTI validation set across all classes. To\nassess our model’s performance on the KITTI test set,\nwe submitted the results to the KITTI online benchmark\nserver. Our TSSTDet model holds the top rank on the\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS AND JOURNALS (2023) 9\nTABLE I\nCOMPARISON OF ALL 3-D DETECTOR RESULTS ON THE KITTI TEST SET . WE EVALUATE OUR MODEL FOR THE CAR CLASS USING THE 3-D\nAVERAGE PRECISION (AP) UNDER 40 RECALL THRESHOLDS . OUR TSSTD ET SURPASSED ALL OF THE STATE -OF-THE -ART METHODS ON THE 3-D\nOBJECT DETECTION BENCHMARK . THE BEST PERFORMANCES ARE HIGHLIGHTED IN BOLD .\nModel Reference Stage Car 3-D R40 Car BEV R40\nEasy Mod. Hard Easy Mod. Hard\nLiDAR + RGB\nMV3D [22] CVPR 2017 Multiple 74.97 63.63 54.00 86.62 78.93 69.80\nContFuse [44] ECCV 2018 Single 83.68 68.78 61.67 94.07 85.35 75.88\nF-PointNet [23] CVPR 2018 Multiple 82.19 69.79 60.59 91.17 84.67 74.77\nA VOD [26] IROS 2018 Multiple 83.07 71.76 65.73 90.99 84.82 79.62\nF-ConvNet [45] IROS 2019 Multiple 87.36 76.39 66.69 91.51 85.84 76.11\n3D-CVF [46] ECCV 2020 Multiple 89.20 80.05 73.11 93.52 89.56 82.45\nCLOC [47] IROS 2020 Multiple 88.94 80.67 77.15 93.05 89.80 86.57\nSFD [27] CVPR 2022 Multiple 91.73 84.76 77.92 95.64 91.85 86.83\nPA3DNet [48] TII 2023 Multiple 90.49 82.57 77.88 93.11 89.46 84.60\nLoGoNet [49] CVPR 2023 Multiple 91.80 85.06 80.74 95.48 91.52 87.09\nTED [6] AAAI 2023 Multiple 91.61 85.28 80.68 95.44 92.05 87.30\nLiDAR Only\nV oxelNet [2] CVPR 2018 Single 77.82 64.17 57.51 87.95 78.39 71.29\nSECOND [1] Sensors 2018 Single 83.13 73.66 66.20 88.07 79.37 77.95\nPointRCNN [10] CVPR 2019 Multiple 86.96 75.64 70.70 92.13 87.39 82.72\nTANet [50] AAAI 2020 Multiple 84.39 75.94 68.82 91.58 86.54 81.19\nPart- A 2 [51] TPMAI 2020 Multiple 87.81 78.49 73.51 91.70 87.79 84.61\n3DSSD [21] CVPR 2020 Single 88.36 79.57 74.55 92.66 89.02 85.86\nSA-SSD [3] CVPR 2020 Single 88.75 79.79 74.16 95.03 91.03 85.96\nCIA-SSD [52] CVPR 2020 Single 89.59 80.28 72.87 93.74 89.84 82.39\nPV-RCNN [8] CVPR 2020 Multiple 90.25 81.43 76.82 94.98 90.65 86.14\nCT3D [41] ICCV 2021 Multiple 87.83 81.77 77.16 92.36 88.83 84.07\nV oxel-RCNN [7] AAAI 2021 Multiple 90.90 81.62 77.06 94.85 88.83 86.13\nSPG [53] ICCV 2021 Multiple 90.64 82.66 77.91 92.80 89.12 86.27\nSE-SSD [4] CVPR 2021 Single 91.49 82.54 77.15 95.68 91.84 86.72\nV oxSeT [54] CVPR 2022 Single 88.53 82.06 77.46 92.70 89.07 86.29\nBADet [55] PR 2022 Multiple 89.28 81.61 76.58 95.23 91.32 86.48\nRDIoU [56] ECCV 2022 Single 90.65 82.30 77.26 94.90 89.75 84.67\nBtcDet [12] AAAI 2022 Multiple 90.64 82.86 78.09 92.81 89.34 84.55\nCASA [39] TGRS 2022 Multiple 91.58 83.06 80.08 94.57 91.22 88.43\nGLENet-VR [57] IJCV 2023 Multiple 91.67 83.23 78.43 93.48 89.76 84.89\nPVT-SSD [58] CVPR 2023 Single 90.65 82.29 76.85 95.23 91.63 86.43\n3D HANet [59] TGRS 2023 Multiple 90.79 84.18 77.57 94.33 91.13 86.33\nOcTr [60] CVPR 2023 Multiple 90.88 82.64 77.77 93.08 89.56 86.74\nTSSTDet (Ours) - Multiple 91.84 85.47 80.65 95.80 92.11 89.23\nKITTI online benchmark, particularly for the car category at\nhttps://www.cvlibs.net/datasets/kitti/eval object.php?obj bench\nmark=3d. For the WOD dataset, we evaluated the TSSTDet\nusing a validation set to demonstrate our results.\nA. Evaluation\n1) KITTI Test Set: We uploaded our test set results to the\nKITTI online server in compliance with KITTI regulations\n(only the best results were submitted to the online test server).\nThe comparison between our proposed framework and state-\nof-the-art models on the KITTI test set is shown in Table I.\nWe reported the evaluation on the most important car category.\nNote that ”Mod.” indicates the moderate difficulty level. On\nthe KITTI leaderboard, a moderate AP of under 40 recalls is\nthe official ranking metric.\nWe surpassed all state-of-the-art methods that used only\nLiDAR point clouds on both single-stage and multistage.\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n10 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2023\nFig. 5. Visual assessment of TSSTDet using the KITTI validation dataset. In each scenario, the initial row exhibits the RGB image along with green\nground-truth annotations for all classes. The second and third rows depict the LiDAR frame with predicted results of the Voxel-RCNN baseline and\nour model, respectively. Predicted 3-D bounding boxes in LiDAR frames of cars, pedestrians, and cyclists are represented in orange, pink, and blue,\nrespectively. The R⃝ and O⃝ denote cases where our model surpasses the baseline in terms of orientation and occlusion accuracy, respectively.\nBest viewing experience through color and zoom functionality.\nTABLE II\nCOMPARISON OF ALL 3-D DETECTOR RESULTS ON THE KITTI VALIDATION SET . WE EVALUATED OUR MODEL FOR ALL THREE CLASSES , USING THE\n3-D AVERAGE PRECISION (AP) UNDER 40 RECALL THRESHOLDS . WE ALSO REPORT THE AP UNDER 11 RECALL THRESHOLDS FOR THE\nMODERATE CAR CLASS . OUR TSSTD ET SURPASSED ALL STATE -OF-THE -ART METHODS ON THE 3-D OBJECT DETECTION BENCHMARK . THE BEST\nPERFORMANCES ARE HIGHLIGHTED IN BOLD , AND ∗ INDICATES RESULTS REPRODUCED FROM OPEN -SOURCE CODE .\nModel Reference Modality Car 3DR40 Pedestrian 3DR40 Cyclist 3DR40 Car 3D AP R11\nEasy Mod. Hard Easy Mod. Hard Easy Mod. Hard Mod.\nSA-SSD∗ [3] CVPR 2020 LiDAR 92.49 84.53 81.71 61.72 55.01 49.94 87.82 71.25 65.88 79.91\nPV-RCNN [8] CVPR 2020 LiDAR 92.57 84.83 82.69 64.26 56.67 51.91 88.88 71.95 66.78 83.90\nV oxel R-CNN∗ [7] AAAI 2021 LiDAR 92.15 85.19 82.78 64.88 57.32 52.11 89.25 72.52 67.03 84.52\nCT3D [41] ICCV 2021 LiDAR 92.85 85.82 83.46 65.73 58.56 53.04 91.99 71.60 67.34 86.06\nSE-SSD∗ [4] CVPR 2021 LiDAR 93.19 86.12 83.31 67.98 59.72 54.83 91.77 72.54 68.78 85.71\nBtcDet [12] AAAI 2022 LiDAR 93.15 86.28 83.86 69.39 61.19 55.86 91.45 74.70 70.08 86.57\nCASA [39] TGRS 2022 LiDAR 93.38 86.42 84.04 68.81 62.59 57.47 92.81 72.63 68.32 86.63\nGLENet [57] IJCV 2023 LiDAR 93.51 86.10 83.60 69.55 64.12 59.23 92.77 72.44 68.11 86.46\nTED-S [6] AAAI 2023 LiDAR 93.05 87.91 85.81 72.38 67.81 63.54 93.09 75.77 71.20 87.54\nTSSTDet (Ours) - LiDAR 95.29 89.06 86.92 75.13 69.38 64.31 95.16 76.24 71.62 88.15\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS AND JOURNALS (2023) 11\nTABLE III\nCOMPARISON OF THE BEV OBJECT DETECTION ON THE VALIDATION\nSET FOR THE CAR CLASS . THE BEST PERFORMANCES ARE\nHIGHLIGHTED IN BOLD . OUR TSSTD ET SURPASSED ALL EXISTING\nSTATE-OF-THE -ART METHODS .\nModel Modality APR40(IoU = 0.7)\nEasy Mod. Hard\nSIENet [11] LiDAR 90.29 88.41 87.77\nPV-RCNN [8] LiDAR 95.76 91.11 88.93\nV oxel-RCNN [7] LiDAR 95.52 91.25 88.99\nCT3D [41] LiDAR 96.14 91.88 89.63\nSE-SSD [4] LiDAR 96.59 92.28 89.72\nVPFNet [63] LiDAR + RGB 94.11 92.44 89.88\nGraph R-CNN [64] LiDAR + RGB 96.28 92.68 92.11\nTSSTDet (Ours) LiDAR 97.02 94.36 92.69\nOur results outperformed other LiDAR-only methods on 3-\nD detection, with scores of 91.84%, 85.47%, and 80.65%\nfor the easy, moderate, and hard difficulty levels, respectively.\nSpecifically, for the most important evaluation metric of Car\nModerate, we achieved the top position in the rankings for\nboth LiDAR + RGB and LiDAR-only methods. Furthermore,\nwe achieved the top rank for car BEV R40 levels, with scores\nof 95.80 %, 92.11%, and 89.23% for the easy, moderate, and\nhard difficulty levels, respectively.\n2) KITTI Validation Set: We also reported an evaluation\nusing the KITTI validation dataset, as listed in Table II.\nIn the table, ”Mod.” represents the moderate level, while\n”∗” indicates that the results are reproduced from the open-\nsource code [65]. Our model consistently outperformed all\nexisting LiDAR-based methods across all classes, including\ncars, pedestrians, and cyclists.\nNotably, in the crucial category of Car 3-D APR40, TSST-\nDet demonstrated substantial improvements over the state-of-\nthe-art TED-S method [6], with margins of 2.24%, 1.09%,\nand 1.11% for the easy, moderate, and hard difficulty lev-\nels, respectively. We also excelled in the moderate Car 3-D\nAPR11 category, confirming our strong performance on the\nleaderboard.\nIt is worth mentioning that our model exhibited excellent\nresults for the pedestrian and cyclist classes. Specifically,\nTSSTDet outperformed the most recent TED model [6] on\nthe moderate level by 1.57% and 0.47% for pedestrians and\ncyclists, respectively. We also conducted experiments on BEV\nobject detection for the car class. The results are shown in\nTable III, with ”Mod.” indicating a moderate level. Our TSST-\nDet surpassed all previous methods with different modalities\nand obtained the highest scores for the car class of 97.02%,\n94.36%, and 92.69% for the easy, moderate, and hard difficulty\nlevels, respectively.\nAdditionally, Fig. 5 provides the visual assessment of our\nmodel on the KITTI validation set. The substantial perfor-\nmance improvements are primarily attributed to the rotational-\ntransformation and shape-completion design, which enhance\nthe model’s ability to capture object-shape features, resulting\nin superior detection performance.\n3) Waymo Validation Set: We conducted a comparative anal-\nysis of TSSTDet and other models using the Waymo Open\nDataset (WOD). Our approach is consistent with the usage\nof a single frame, as observed in V oxel-RCNN [7] and PV-\nRCNN [8]. The results are summarized in Table IV, where\n”N/A” indicates that certain information was not available.\nTo evaluate vehicle detection, we used both the 3-D mean\naverage precision (mAP) and 3-D mAP weighted by heading\n(mAPH). In all categories, our model consistently outper-\nformed all state-of-the-art detectors. Specifically, we outper-\nformed all state-of-the-art methods in the most important ve-\nhicle (L2) categories, achieving scores of 71.75% and 70.12%\nfor mAP and mAPH, respectively.\nOverall, TSSTDet showed substantial improvements for\nLevel 2 objects across all categories. This can be attributed\nto the fact that objects with fewer data points are more likely\nto lack shape and object-orientation information. These results\nfor the Waymo Open Dataset, which is one of the largest pub-\nlicly available LiDAR datasets, underscore the effectiveness of\nour model.\nB. Ablation Study\nWe conducted a series of experiments using various configu-\nrations to analyze the effectiveness of each TSSTDet module.\nThe number of hyperparameters in the model was carefully\nexamined using various metrics. We used the KITTI dataset\nand open-source code [65] to reproduce the baseline results.\n1) Effectiveness of RT Backbone and Pooling: The number\nof rotational transformations N of TSSTDet is a hyperpa-\nrameter. It affects the performance of the backbone and is\nalso related to the number of attention layers in the AFR. To\nevaluate the impact of parameter N and determine the optimal\nhyperparameter, we conducted a set of ablation experiments\non the car, pedestrian, and cyclist classes using the KITTI\nvalidation dataset, as shown in Table V.\nWhen N = 3, the performance showed a substantial en-\nhancement in comparison to using a single rotation number,\nwith notable improvements of 2.72%, 2.82%, and 1.61% for\nthe car, pedestrian, and cyclist classes, respectively. When\ncomparing N = 4 with N = 3, it is evident that employing\nN = 3 maintains an excellent performance while maintaining\nan optimal processing speed (running on a single RTX 3090\nGPU). Therefore, we chose N = 3 as the preferred setting for\nour primary model to achieve efficient performance.\nThe effectiveness of the RT backbone and pooling is re-\nported in Table VI. We used V oxel-RCNN [7] as the baseline.\nAs depicted in the third row, there is a notable 0.66% enhance-\nment in the performance of the car class at a moderate level,\ncompared to the baseline.\n2) Effectiveness of VPST: Next, we comprehensively ver-\nified the contribution of the VPST module to the model\nperformance under AP R40 of the car category. We utilized\nopen-source code [65] to examine the results of various config-\nurations. The VPST module aims to reconstruct the complete\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n12 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2023\nTABLE IV\nCOMPARISON OF ALL STATE -OF-THE -ART 3-D DETECTION PERFORMANCES ON THE WAYMO VALIDATION SET . L1 AND L2 REFER TO LEVEL 1 AND\nLEVEL 2, RESPECTIVELY . THE BEST PERFORMANCES ARE HIGHLIGHTED IN BOLD .\nModel Vehicle (L1) Vehicle (L2) Pedestrian (L1) Pedestrian (L2) Cyclist (L1) Cyclist (L2)\nmAP mAPH mAP mAPH mAP mAPH mAP mAPH mAP mAPH mAP mAPH\nBtcDet [12] 78.58 78.06 70.10 69.61 N/A N/A N/A N/A N/A N/A N/A N/A\nPV-RCNN [8] 78.79 78.21 70.26 69.71 76.67 67.15 68.51 59.72 68.98 67.63 66.48 65.17\nCASA [39] 78.54 78.00 69.91 69.42 80.88 73.10 71.87 64.78 69.66 68.38 67.07 66.83\nTED [6] 79.26 78.73 70.50 70.07 82.62 76.66 73.50 68.03 74.11 72.94 71.46 70.32\nTSSTDet (Ours) 80.37 79.98 71.75 70.12 83.07 77.34 73.55 69.15 74.25 73.13 72.44 71.16\nTABLE V\nEFFECT OF RTCONV USING DIFFERENT ROTATION NUMBERS .\nRotation number (N) 3-D Detection (Mod.) FPS\nCar Pedestrian Cyclist\n1 86.34 66.56 74.63 18.2\n2 88.54 67.47 75.01 13.9\n3 89.06 69.38 76.24 10.5\n4 89.09 68.72 75.11 7.7\nTABLE VI\nPERFORMANCE ANALYSIS ON THE KITTI VALIDATION SET USING\nDIFFERENT DESIGNED COMPONENTS . BASELINE , RTC ONV, RT\nPOOLING , VPST, AND AFR DENOTE THE VOXEL -RCNN BASELINE ,\nROTATIONAL -TRANSFORMATION CONV , ROTATIONAL -TRANSFORMATION\nPOOLING , VOXEL -POINT SHAPE TRANSFORMER , AND\nATTENTION -FUSION AND REFINEMENT , RESPECTIVELY .\nMethod Backbone RT Pooling VPST AFR Mod.\nBaseline SpConv 87.66\nTSSTDet 1 SpConv ✓ 88.24\nTSSTDet 2 RTConv ✓ 88.32\nTSSTDet 3 RTConv ✓ ✓ 88.89\nTSSTDet 4 RTConv ✓ ✓ ✓ 89.06\nshape of an object, which contributes to the performance\nenhancement.\nAs illustrated in the second and fourth rows of Table VI,\nour VPST boosts the AP 3-D detection by 0.58% and 0.57%\ncompared to the baseline, respectively. The metric used is the\nmoderate level of the KITTI car category in 3-D detection.\nWhen VPST was combined with all the other modules, the\nbest performance was achieved at 89.06%.\nAdditionally, we explored the effectiveness of VPST with\nrespect to the orientation aspect, as shown in Table VII.\nIn Table VII, we present a different setting by extending\nour module to a TED-S-based detector [6]. The second row\nTABLE VII\nORIENTATION ACCURACY FOR THE KITTI VALIDATION SET .\nMethod AOS@Easy AOS@Moderate AOS@Hard\nBasedline 98.39 96.05 93.57\nBasedline+VPST 99.24 97.22 94.93\nBasedline+VPST+AFR 99.57 97.89 95.33\ndemonstrates that our VPST leads to a better performance at all\nthree levels of average orientation similarity (AOS), compared\nto the baseline.\n3) Effectiveness of AFR: To assess the impact of AFR, we\nfirst built a baseline V oxel-RCNN [7]. The comparative results\nare presented in Table VI. Employing a simple head detector\ndid not improve the performance. However, upon incorporating\nour AFR, the performance was further enhanced to 89.06%, as\nshown in the last row. Our AFR effectively aggregated object-\nshape features from various angles, resulting in more robust\ndetections from sparse points.\nTABLE VIII\nMEMORY USAGE AND RUNTIME ANALYSIS FOR EACH MODULE . S AND\nGB DENOTE SECOND , AND GIGABYTE , RESPECTIVELY .\nModule Backbone RPN VPST AFR Total\nRuntime (s) 0.039 0.011 0.032 0.015 0.097\nMemory usage (GB) 4.896 0.984 4.395 0.146 10.421\nTo further validate the effectiveness of our AFR, we con-\nducted additional experiments on the KITTI dataset using the\norientation metric. As indicated in the third row of Table VII,\nour AFR delivers impressive performance improvements in\nterms of easy (0.33%), moderate (0.67%), and hard (0.4%)\nAOS, compared with the second row. Note that the baseline\nin Table VII was based on the TED model.\n4) Memory Usage and Runtime Analysis: We also assess the\nmodel’s performance in terms of memory usage and runtime,\nas shown in Table VIII. For simplicity, we conducted the\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS AND JOURNALS (2023) 13\nexperiments on a single RTX 3090 with a batch size of 1.\nWe utilize the default configuration with a rotation number\nset to 3.\nVI. C ONCLUSION\nResearch focused on 3-D object detection in situations\ninvolving obstructions is scarce. In this paper, we presented\na multistage 3-D object detector based on a transformer\narchitecture called TSSTDet. To fully leverage the rotational-\ntransformation features, an RT backbone was devised to ex-\ntract the general pattern. Furthermore, TSSTDet addressed\ninstances with missing object shapes through an autoregression\nmodule, VPST, which helped the model comprehensively\nidentify object shapes, even in occluded scenarios.Finally,\nan effective aggregation and refinement strategy, AFR, was\napplied to fine-tune the precise prediction. Our model is not\nsimply a high-performance 3-D object detector relying solely\non LiDAR point clouds; it also demonstrates remarkable adapt-\nability in scenarios with occluded objects and diverse rotation\nsituations. The experimental results obtained from both the\nKITTI and Waymo datasets demonstrated the effectiveness\nand adaptability of the proposed approach. In our opinion,\nthis approach has great potential for a variety of downstream\n3-D applications, such as object tracking and motion planning.\nFuture research will concentrate on developing methods that\ncan more effectively aggregate proposals for small objects,\nsuch as cyclists and pedestrians.\nACKNOWLEDGMENT\nThis research was supported by a National Research Foun-\ndation of Korea (NRF) grant funded by the Government of\nSouth Korea (MSIT)(NRF-2021R1A2B5B01002559).\nREFERENCES\n[1] Y . Yan, Y . Mao, and B. Li, “Second: Sparsely embedded convolutional\ndetection,” Sensors, vol. 18, no. 10, p. 3337, 2018.\n[2] Y . Zhou and O. Tuzel, “V oxelnet: End-to-end learning for point cloud\nbased 3d object detection,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pp. 4490–4499, 2018.\n[3] C. He, H. Zeng, J. Huang, X.-S. Hua, and L. Zhang, “Structure aware\nsingle-stage 3d object detection from point cloud,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition ,\npp. 11873–11882, 2020.\n[4] W. Zheng, W. Tang, L. Jiang, and C.-W. Fu, “Se-ssd: Self-ensembling\nsingle-stage object detector from point cloud,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npp. 14494–14503, 2021.\n[5] S. Ren, K. He, R. Girshick, and J. Sun, “Fasterr-cnn: Towards real-time\nobject detection with region proposal networks,” Advances in neural\ninformation processing systems , vol. 28, 2015.\n[6] H. Wu, C. Wen, W. Li, X. Li, R. Yang, and C. Wang, “Transformation-\nequivariant 3d object detection for autonomous driving,” in Proceedings\nof the AAAI Conference on Artificial Intelligence , vol. 37, pp. 2795–\n2802, 2023.\n[7] J. Deng, S. Shi, P. Li, W. Zhou, Y . Zhang, and H. Li, “V oxel r-\ncnn: Towards high performance voxel-based 3d object detection,” in\nProceedings of the AAAI Conference on Artificial Intelligence , vol. 35,\npp. 1201–1209, 2021.\n[8] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, “Pv-\nrcnn: Point-voxel feature set abstraction for 3d object detection,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 10529–10538, 2020.\n[9] H. A. Hoang and M. Yoo, “3onet: 3-d detector for occluded object\nunder obstructed conditions,” IEEE Sensors Journal , vol. 23, no. 16,\npp. 18879–18892, 2023.\n[10] S. Shi, X. Wang, and H. Li, “Pointrcnn: 3d object proposal generation\nand detection from point cloud,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pp. 770–779,\n2019.\n[11] Z. Li, Y . Yao, Z. Quan, W. Yang, and J. Xie, “Sienet: spatial information\nenhancement network for 3d object detection from point cloud,” arXiv\npreprint arXiv:2103.15396, 2021.\n[12] Q. Xu, Y . Zhong, and U. Neumann, “Behind the curtain: Learning\noccluded shapes for 3d object detection,” in Proceedings of the AAAI\nConference on Artificial Intelligence , vol. 36, pp. 2893–2901, 2022.\n[13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[14] H.-X. Yu, J. Wu, and L. Yi, “Rotationally equivariant 3d object detec-\ntion,” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 1456–1464, 2022.\n[15] J. Han, J. Ding, N. Xue, and G.-S. Xia, “Redet: A rotation-equivariant\ndetector for aerial object detection,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 2786–\n2795, 2021.\n[16] J. Mao, Y . Xue, M. Niu, H. Bai, J. Feng, X. Liang, H. Xu, and\nC. Xu, “V oxel transformer for 3d object detection,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision , pp. 3164–\n3173, 2021.\n[17] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on\npoint sets for 3d classification and segmentation,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition , pp. 652–\n660, 2017.\n[18] Y . Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, “Pointcnn:\nConvolution on x-transformed points,” Advances in neural information\nprocessing systems, vol. 31, 2018.\n[19] A. H. Lang, S. V ora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom,\n“Pointpillars: Fast encoders for object detection from point clouds,”\nin Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 12697–12705, 2019.\n[20] Z. Yang, Y . Sun, S. Liu, X. Shen, and J. Jia, “Std: Sparse-to-dense\n3d object detector for point cloud,” in Proceedings of the IEEE/CVF\ninternational conference on computer vision , pp. 1951–1960, 2019.\n[21] Z. Yang, Y . Sun, S. Liu, and J. Jia, “3dssd: Point-based 3d single\nstage object detector,” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pp. 11040–11048, 2020.\n[22] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object\ndetection network for autonomous driving,” in Proceedings of the IEEE\nconference on Computer Vision and Pattern Recognition, pp. 1907–1915,\n2017.\n[23] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets\nfor 3d object detection from rgb-d data,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition , pp. 918–927,\n2018.\n[24] X. Chen, T. Zhang, Y . Wang, Y . Wang, and H. Zhao, “Futr3d: A\nunified sensor fusion framework for 3d detection,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 172–181, 2023.\n[25] Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. L. Rus, and S. Han,\n“Bevfusion: Multi-task multi-sensor fusion with unified bird’s-eye view\nrepresentation,” in 2023 IEEE International Conference on Robotics and\nAutomation (ICRA), pp. 2774–2781, IEEE, 2023.\n[26] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. L. Waslander, “Joint\n3d proposal generation and object detection from view aggregation,”\nin 2018 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS), pp. 1–8, IEEE, 2018.\n[27] X. Wu, L. Peng, H. Yang, L. Xie, C. Huang, C. Deng, H. Liu, and\nD. Cai, “Sparse fuse dense: Towards high quality 3d detection with\ndepth completion,” in CVPR, 2022.\n[28] T. Yin, X. Zhou, and P. Kr ¨ahenb¨uhl, “Multimodal virtual point 3d\ndetection,” Advances in Neural Information Processing Systems, vol. 34,\npp. 16494–16507, 2021.\n[29] Y . Yang, C. Feng, Y . Shen, and D. Tian, “Foldingnet: Interpretable unsu-\npervised learning on 3d point clouds,” arXiv preprint arXiv:1712.07262,\nvol. 2, no. 3, p. 5, 2017.\n[30] X. Wen, T. Li, Z. Han, and Y .-S. Liu, “Point cloud completion by\nskip-attention network with hierarchical folding,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition ,\npp. 1939–1948, 2020.\n[31] L. P. Tchapmi, V . Kosaraju, H. Rezatofighi, I. Reid, and S. Savarese,\n“Topnet: Structural point cloud decoder,” in Proceedings of the\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n14 IEEE SENSORS JOURNAL, VOL. XX, NO. XX, XXXX 2023\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npp. 383–392, 2019.\n[32] P. Xiang, X. Wen, Y .-S. Liu, Y .-P. Cao, P. Wan, W. Zheng, and\nZ. Han, “Snowflakenet: Point cloud completion by snowflake point\ndeconvolution with skip-transformer,” in Proceedings of the IEEE/CVF\ninternational conference on computer vision , pp. 5499–5509, 2021.\n[33] X. Yan, L. Lin, N. J. Mitra, D. Lischinski, D. Cohen-Or, and H. Huang,\n“Shapeformer: Transformer-based shape completion via sparse repre-\nsentation,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 6239–6249, 2022.\n[34] P. Mittal, Y .-C. Cheng, M. Singh, and S. Tulsiani, “Autosdf: Shape priors\nfor 3d completion, reconstruction and generation,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npp. 306–315, 2022.\n[35] M.-T. Duong, S. Lee, and M.-C. Hong, “Dmt-net: Deep multiple\nnetworks for low-light image enhancement based on retinex model,”\nIEEE Access, vol. 11, pp. 132147–132161, 2023.\n[36] M.-T. Duong and M.-C. Hong, “Ebsd-net: Enhancing brightness and\nsuppressing degradation for low-light color image using deep networks,”\nin 2022 IEEE International Conference on Consumer Electronics-Asia\n(ICCE-Asia), pp. 1–4, 2022.\n[37] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever,\n“Generative pretraining from pixels,” in International conference on\nmachine learning, pp. 1691–1703, PMLR, 2020.\n[38] D. Zhang, H. Zhang, J. Tang, M. Wang, X. Hua, and Q. Sun, “Feature\npyramid transformer,” in Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part\nXXVIII 16, pp. 323–339, Springer, 2020.\n[39] H. Wu, J. Deng, C. Wen, X. Li, C. Wang, and J. Li, “Casa: A cascade\nattention network for 3-d object detection from lidar point clouds,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 60, pp. 1–11,\n2022.\n[40] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian,\n“Modeling point clouds with self-attention and gumbel subset sampling,”\nin Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 3323–3332, 2019.\n[41] H. Sheng, S. Cai, Y . Liu, B. Deng, J. Huang, X.-S. Hua, and M.-J.\nZhao, “Improving 3d object detection with channel-wise transformer,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 2743–2752, 2021.\n[42] B. Graham, M. Engelcke, and L. Van Der Maaten, “3d semantic segmen-\ntation with submanifold sparse convolutional networks,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition ,\npp. 9224–9232, 2018.\n[43] J. Li, S. Luo, Z. Zhu, H. Dai, A. S. Krylov, Y . Ding, and L. Shao, “3d\niou-net: Iou guided 3d object detector for point clouds,” arXiv preprint\narXiv:2004.04962, 2020.\n[44] M. Liang, B. Yang, S. Wang, and R. Urtasun, “Deep continuous fusion\nfor multi-sensor 3d object detection,” in Proceedings of the European\nconference on computer vision (ECCV) , pp. 641–656, 2018.\n[45] Z. Wang and K. Jia, “Frustum convnet: Sliding frustums to aggregate\nlocal point-wise features for amodal 3d object detection,” in 2019\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), pp. 1742–1749, IEEE, 2019.\n[46] J. H. Yoo, Y . Kim, J. Kim, and J. W. Choi, “3d-cvf: Generating joint\ncamera and lidar features using cross-view spatial feature fusion for\n3d object detection,” in Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVII\n16, pp. 720–736, Springer, 2020.\n[47] S. Pang, D. Morris, and H. Radha, “Clocs: Camera-lidar object candi-\ndates fusion for 3d object detection,” in 2020 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pp. 10386–10393,\nIEEE, 2020.\n[48] M. Wang, L. Zhao, and Y . Yue, “Pa3dnet: 3-d vehicle detection with\npseudo shape segmentation and adaptive camera-lidar fusion,” IEEE\nTransactions on Industrial Informatics , 2023.\n[49] X. Li, T. Ma, Y . Hou, B. Shi, Y . Yang, Y . Liu, X. Wu, Q. Chen, Y . Li,\nY . Qiao, et al. , “Logonet: Towards accurate 3d object detection with\nlocal-to-global cross-modal fusion,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 17524–\n17534, 2023.\n[50] Z. Liu, X. Zhao, T. Huang, R. Hu, Y . Zhou, and X. Bai, “Tanet:\nRobust 3d object detection from point clouds with triple attention,” in\nProceedings of the AAAI Conference on Artificial Intelligence , vol. 34,\npp. 11677–11684, 2020.\n[51] S. Shi, Z. Wang, X. Wang, and H. Li, “Part-aˆ 2 net: 3d part-aware and\naggregation neural network for object detection from point cloud,” arXiv\npreprint arXiv:1907.03670, vol. 2, no. 3, 2019.\n[52] W. Zheng, W. Tang, S. Chen, L. Jiang, and C.-W. Fu, “Cia-ssd: Confident\niou-aware single-stage object detector from point cloud,” in Proceedings\nof the AAAI conference on artificial intelligence, vol. 35, pp. 3555–3562,\n2021.\n[53] Q. Xu, Y . Zhou, W. Wang, C. R. Qi, and D. Anguelov, “Spg: Unsu-\npervised domain adaptation for 3d object detection via semantic point\ngeneration,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision , pp. 15446–15456, 2021.\n[54] C. He, R. Li, S. Li, and L. Zhang, “V oxel set transformer: A set-to-set\napproach to 3d object detection from point clouds,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 8417–8427, 2022.\n[55] R. Qian, X. Lai, and X. Li, “Badet: Boundary-aware 3d object detection\nfrom point clouds,” Pattern Recognition, vol. 125, p. 108524, 2022.\n[56] H. Sheng, S. Cai, N. Zhao, B. Deng, J. Huang, X.-S. Hua, M.-J. Zhao,\nand G. H. Lee, “Rethinking iou-based optimization for single-stage\n3d object detection,” in Computer Vision–ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part\nIX, pp. 544–561, Springer, 2022.\n[57] Y . Zhang, Q. Zhang, Z. Zhu, J. Hou, and Y . Yuan, “Glenet: Boosting\n3d object detectors with generative label uncertainty estimation,” Inter-\nnational Journal of Computer Vision , pp. 1–21, 2023.\n[58] H. Yang, W. Wang, M. Chen, B. Lin, T. He, H. Chen, X. He, and\nW. Ouyang, “Pvt-ssd: Single-stage 3d object detector with point-voxel\ntransformer,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 13476–13487, 2023.\n[59] Q. Xia, Y . Chen, G. Cai, G. Chen, D. Xie, J. Su, and Z. Wang, “3-d\nhanet: A flexible 3-d heatmap auxiliary network for object detection,”\nIEEE Transactions on Geoscience and Remote Sensing , vol. 61, pp. 1–\n13, 2023.\n[60] C. Zhou, Y . Zhang, J. Chen, and D. Huang, “Octr: Octree-based\ntransformer for 3d object detection,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 5166–\n5175, 2023.\n[61] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\ndriving,” in The KITTI Vision Benchmark Suite, 2012 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , pp. 3354–3361.\n[62] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V . Patnaik, P. Tsui,\nJ. Guo, Y . Zhou, Y . Chai, B. Caine, et al. , “Scalability in perception\nfor autonomous driving: Waymo open dataset,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition ,\npp. 2446–2454, 2020.\n[63] H. Zhu, J. Deng, Y . Zhang, J. Ji, Q. Mao, H. Li, and Y . Zhang, “Vpfnet:\nImproving 3d object detection with virtual point based lidar and stereo\ndata fusion,” IEEE Transactions on Multimedia , 2022.\n[64] H. Yang, Z. Liu, X. Wu, W. Wang, W. Qian, X. He, and D. Cai, “Graph\nr-cnn: Towards accurate 3d object detection with semantic-decorated\nlocal graph,” in European Conference on Computer Vision , pp. 662–\n679, Springer, 2022.\n[65] O. D. Team, “Openpcdet: An open-source toolbox for 3d object detection\nfrom point clouds.” https://github.com/open-mmlab/OpenPCDet, 2020.\nHiep Anh Hoang received a B.Eng. degree in\ncontrol engineering and automation from Hanoi\nUniversity of Science and Technology, Hanoi\nCity, Vietnam, in 2020. He is currently pursuing\na master’s degree at Soongsil University, Seoul,\nSouth Korea. His research interests include\ndeep learning, 3-D perception of autonomous\ndriving, and LiDAR point clouds.\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS AND JOURNALS (2023) 15\nDuy Cuong Bui received a B.S. degree in au-\ntomatic control from Hanoi University of Science\nand Technology, Hanoi, Vietnam, in 2020. He is\ncurrently pursuing a master’s degree at Soongsil\nUniversity, Seoul, South Korea. His research\ninterests include computer vision.\nMyungsik Yoo received B.S. and M.S. degrees\nin electrical engineering from Korea University,\nSeoul, South Korea, in 1989 and 1991, respec-\ntively, and a Ph.D. degree in electrical engineer-\ning from State University of New Y ork at Buffalo,\nNew Y ork, in 2000. He was a senior research en-\ngineer at the Nokia Research Center, Burlington,\nMA, USA. He is currently a full-time professor at\nthe School of Electronic Engineering, Soongsil\nUniversity, Seoul, South Korea. His research\ninterests include visible-light communications,\ncloud computing, and machine learning.\nThis article has been accepted for publication in IEEE Sensors Journal. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSEN.2024.3350770\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5528612732887268
    },
    {
      "name": "Computer science",
      "score": 0.5021705627441406
    },
    {
      "name": "Computer vision",
      "score": 0.49776342511177063
    },
    {
      "name": "Transformation (genetics)",
      "score": 0.481403112411499
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43775567412376404
    },
    {
      "name": "Object detection",
      "score": 0.41464027762413025
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.28103354573249817
    },
    {
      "name": "Engineering",
      "score": 0.21680867671966553
    },
    {
      "name": "Electrical engineering",
      "score": 0.16910803318023682
    },
    {
      "name": "Voltage",
      "score": 0.13679534196853638
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141371507",
      "name": "Soongsil University",
      "country": "KR"
    }
  ]
}