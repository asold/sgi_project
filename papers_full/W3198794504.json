{
  "title": "TCCT: Tightly-coupled convolutional transformer on time series forecasting",
  "url": "https://openalex.org/W3198794504",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1898662817",
      "name": "Li Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097563278",
      "name": "Yangzhu Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2012079387",
    "https://openalex.org/W3160849954",
    "https://openalex.org/W3158868365",
    "https://openalex.org/W6750513420",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3171884590",
    "https://openalex.org/W6600297362",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W3034301699",
    "https://openalex.org/W6736282669",
    "https://openalex.org/W2980994438",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2096958224",
    "https://openalex.org/W6754779804",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963532813",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W6771154167",
    "https://openalex.org/W6731892127",
    "https://openalex.org/W6730903564",
    "https://openalex.org/W1969852690",
    "https://openalex.org/W2747599906",
    "https://openalex.org/W3002709689",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3158060477",
    "https://openalex.org/W2902854273",
    "https://openalex.org/W2773625660",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W638887343",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3170778815",
    "https://openalex.org/W4236965008",
    "https://openalex.org/W2796347433",
    "https://openalex.org/W4231057675",
    "https://openalex.org/W2889928394",
    "https://openalex.org/W2765932895",
    "https://openalex.org/W2986830070",
    "https://openalex.org/W2797846142",
    "https://openalex.org/W3154470663",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W3125914437",
    "https://openalex.org/W2970631142",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2570343428",
    "https://openalex.org/W2798058877",
    "https://openalex.org/W3111507638",
    "https://openalex.org/W2786228682",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2604847698",
    "https://openalex.org/W3018757597",
    "https://openalex.org/W3168114581",
    "https://openalex.org/W3042011474",
    "https://openalex.org/W3125096521",
    "https://openalex.org/W4230715394"
  ],
  "abstract": "Time series forecasting is essential for a wide range of real-world\\napplications. Recent studies have shown the superiority of Transformer in\\ndealing with such problems, especially long sequence time series input(LSTI)\\nand long sequence time series forecasting(LSTF) problems. To improve the\\nefficiency and enhance the locality of Transformer, these studies combine\\nTransformer with CNN in varying degrees. However, their combinations are\\nloosely-coupled and do not make full use of CNN. To address this issue, we\\npropose the concept of tightly-coupled convolutional Transformer(TCCT) and\\nthree TCCT architectures which apply transformed CNN architectures into\\nTransformer: (1) CSPAttention: through fusing CSPNet with self-attention\\nmechanism, the computation cost of self-attention mechanism is reduced by 30%\\nand the memory usage is reduced by 50% while achieving equivalent or beyond\\nprediction accuracy. (2) Dilated causal convolution: this method is to modify\\nthe distilling operation proposed by Informer through replacing canonical\\nconvolutional layers with dilated causal convolutional layers to gain\\nexponentially receptive field growth. (3) Passthrough mechanism: the\\napplication of passthrough mechanism to stack of self-attention blocks helps\\nTransformer-like models get more fine-grained information with negligible extra\\ncomputation costs. Our experiments on real-world datasets show that our TCCT\\narchitectures could greatly improve the performance of existing state-of-art\\nTransformer models on time series forecasting with much lower computation and\\nmemory costs, including canonical Transformer, LogTrans and Informer.\\n",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7149496078491211
    },
    {
      "name": "Series (stratigraphy)",
      "score": 0.5914893746376038
    },
    {
      "name": "Transformer",
      "score": 0.5813369750976562
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46274474263191223
    },
    {
      "name": "Time series",
      "score": 0.4259433150291443
    },
    {
      "name": "Machine learning",
      "score": 0.3950961232185364
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.39133575558662415
    },
    {
      "name": "Algorithm",
      "score": 0.32797345519065857
    },
    {
      "name": "Voltage",
      "score": 0.10763418674468994
    },
    {
      "name": "Electrical engineering",
      "score": 0.09253725409507751
    },
    {
      "name": "Engineering",
      "score": 0.06448522210121155
    },
    {
      "name": "Geology",
      "score": 0.058036983013153076
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    }
  ],
  "cited_by": 116
}