{
    "title": "SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge",
    "url": "https://openalex.org/W4393160744",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2989183281",
            "name": "Rishi Hazra",
            "affiliations": [
                "Örebro University"
            ]
        },
        {
            "id": "https://openalex.org/A2757225246",
            "name": "Pedro Zuidberg Dos Martires",
            "affiliations": [
                "Örebro University"
            ]
        },
        {
            "id": "https://openalex.org/A189137728",
            "name": "Luc De Raedt",
            "affiliations": [
                "KU Leuven",
                "Örebro University"
            ]
        },
        {
            "id": "https://openalex.org/A2989183281",
            "name": "Rishi Hazra",
            "affiliations": [
                "Örebro University"
            ]
        },
        {
            "id": "https://openalex.org/A2757225246",
            "name": "Pedro Zuidberg Dos Martires",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A189137728",
            "name": "Luc De Raedt",
            "affiliations": [
                "Örebro University",
                "KU Leuven"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6678231568",
        "https://openalex.org/W2897513296",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4378767246",
        "https://openalex.org/W6786039469",
        "https://openalex.org/W4221152848",
        "https://openalex.org/W2168359464",
        "https://openalex.org/W6849548236",
        "https://openalex.org/W3129009457",
        "https://openalex.org/W2948589675",
        "https://openalex.org/W4360819401",
        "https://openalex.org/W2799002257",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W4297161808",
        "https://openalex.org/W3198685994",
        "https://openalex.org/W3096099141",
        "https://openalex.org/W2122054842",
        "https://openalex.org/W4323066451",
        "https://openalex.org/W4366999541",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4312045619",
        "https://openalex.org/W4322825501",
        "https://openalex.org/W3210940825",
        "https://openalex.org/W4378509449",
        "https://openalex.org/W4386251907",
        "https://openalex.org/W4307934016",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4385473486",
        "https://openalex.org/W3101355526",
        "https://openalex.org/W4321011818",
        "https://openalex.org/W4394671563",
        "https://openalex.org/W4320559489",
        "https://openalex.org/W4377130677",
        "https://openalex.org/W4383108457",
        "https://openalex.org/W4285428875",
        "https://openalex.org/W4389520747",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4388660746",
        "https://openalex.org/W4383097638",
        "https://openalex.org/W4297808394",
        "https://openalex.org/W4286336838",
        "https://openalex.org/W4224912544",
        "https://openalex.org/W2973379954",
        "https://openalex.org/W4292119927",
        "https://openalex.org/W4221158689"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast \"world knowledge\". Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective elements into the generated plans, and (3) using heuristic search over actions. Our extensive evaluations show that our model surpasses other LLM planning approaches.",
    "full_text": "SayCanPay: Heuristic Planning with Large Language Models\nUsing Learnable Domain Knowledge\nRishi Hazra1, Pedro Zuidberg Dos Martires1, Luc De Raedt1,2\n1Centre for Applied Autonomous Sensor Systems (AASS), ¨Orebro University, Sweden\n2KU Leuven, Belgium\n{rishi.hazra, pedro.zuidberg-dos-martires, luc.de-raedt}@oru.se\nAbstract\nLarge Language Models (LLMs) have demonstrated impres-\nsive planning abilities due to their vast “world knowledge”.\nYet, obtaining plans that are both feasible (grounded in af-\nfordances) and cost-effective (in plan length), remains a chal-\nlenge, despite recent progress. This contrasts with heuristic\nplanning methods that employ domain knowledge (formal-\nized in action models such as PDDL) and heuristic search\nto generate feasible, optimal plans. Inspired by this, we pro-\npose to combine the power of LLMs and heuristic planning\nby leveraging the world knowledge of LLMs and the prin-\nciples of heuristic search. Our approach, SayCanPay, em-\nploys LLMs to generate actions (Say) guided by learnable\ndomain knowledge, that evaluates actions’ feasibility (Can)\nand long-term reward/payoff (Pay), and heuristic search to\nselect the best sequence of actions. Our contributions are (1)\na novel framing of the LLM planning problem in the con-\ntext of heuristic planning, (2) integrating grounding and cost-\neffective elements into the generated plans, and (3) using\nheuristic search over actions. Our extensive evaluations show\nthat our model surpasses other LLM planning approaches.\nIntroduction\nWith the rise of Large Language Models (LLMs), there has\nbeen a growing interest in leveraging their generative capa-\nbilities for planning tasks (Huang et al. 2022a; Valmeekam\net al. 2022; Silver et al. 2022; Liu et al. 2023). These models\nhave the ability to generate long-horizon plans, capitalizing\non their extensive “world knowledge” gained from training\non vast amounts of data (e.g. eggs are typically stored in the\nrefrigerator, and placing an apple in the fridge will cool it).\nSuch expansive knowledge can be exploited to plan in an\nopen-world context (Ding et al. 2023). Moreover, planning\nin the natural language space offers significant flexibility\nespecially, with the advent of multimodal foundation mod-\nels (Lakhotia et al. 2021; Du et al. 2022; Brohan et al. 2023).\nSuch models have made it easier to represent various modal-\nities such as vision, speech, and even actions in the form of\nnatural language, thus bypassing the need to have domain-\nspecific knowledge (e.g. PDDL) that traditional planning ap-\nproaches require. However, LLM-based planning often faces\nchallenges, particularly in generating feasible plans. It can\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nfail to model action affordances (or pre-conditions) 1 due to\ndifficulty in modeling the state of the world (e.g. grab milk\nfrom the fridge even if the door is closed) or having a pre-\ntrained world model that is not aligned with the current envi-\nronment (e.g. using a controller to regulate the heaterwhere\nonly a knob exists), leading to infeasible plans. Moreover,\nsuch models focus greedily on the next actionable step with-\nout considering its relevance to the ultimate goal, resulting in\nlonger, cost-inefficient plans (Valmeekam et al. 2023). Re-\ncent works like SayCan (Ahn et al. 2022) have sought to\naddress the affordance problem by using pretrained skills to\nevaluate the action’s executability – Can the action be exe-\ncuted in the current state? However, the plan cost remains a\nconcern.\nIn contrast, traditional planning provides an established\napproach to developing a sequence of actions to transition\nfrom an initial state to a goal state. It uses a domain file (with\naction models defined in PDDL specifying pre- and post-\nconditions) and heuristic search planners like Fast Down-\nward (Helmert 2006) to ensure feasibility through ground-\ning in preconditions, and generating cost-effective plans by\nemploying search trees to select the best (or shortest) se-\nquence of actions. However, obtaining a domain file for\ncomplex real-world environments is difficult, and its use\nrestricts planning to a closed-world setting. These meth-\nods also struggle to handle partial observations, although\napproximate planning (Kaelbling, Littman, and Cassandra\n1998) can alleviate it.\nIntegrating LLMs with classical planning offers a promis-\ning research path, merging the generative abilities and\n(open) world knowledge of LLMs with the methodologi-\ncal rigor of planning algorithms. To this end, we extend the\nfollowing contributions. (1) We propose to frame language\nmodel planning in the context of heuristic planning, which\nto our knowledge, is the first of its kind. (2) We incorpo-\nrate feasibility and cost-effective elements into the generated\nplans using a joint scoring named SayCanPay2. As shown\nin Figure 1, it guides the planning through three key steps:\n(i) Say: Given a goal and an initial observation, the LLM\n1In robotics, affordances refer to possible actions that can be\nexecuted, which is conceptually similar to inferring preconditions\nin planning – what actions are feasible in a certain situation.\n2Code link: https://rishihazra.github.io/SayCanPay/\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20123\n0.99 0.96 0.22 pick up red key\npick up green ball\ntoggle red door\ndone task\n0.21 \n0.99 0.96 0.25 0.24 \n0.98 0.23 0.00 \n0.00 \n0.00 \n0.95 0.4 0.00 \n  SayCanPay\nStep 1: pick up green ball\nStep 2: drop ball in void\nStep 3: pick up red key\nStep 4: toggle red door\nStep 5: drop key in void\nStep 6: pick up purple box\nStep 7: done task\n \nSayCan\nStep 1: pick up red key\nStep 2: drop key in void\nStep 3: pick up green ball\nStep 4: drop ball in void\nStep 5: pick up red key\nStep 6: toggle red door\nStep 7: drop key in void\nStep 8: pick up purple box\nStep 9: done task\nSay Can PayNet\n                  Say\nStep 1: pick up green ball\nStep 2: drop ball in void\nStep 3: pick up purple box\nStep 4: toggle red door\nStep 5: drop key in void\nStep 6: pick up purple box\nStep 7: done task\ninfeasible actions sub-optimal actions\nGoal: pick up the box.\nInitial State: Room 1 has\nagent, red key, green ball.\nRoom 2 has purple box. The\ndoor connecting Room 1 and\nRoom 2 is locked. The green\nball is blocking the door.\nStep 1: \nfeasible and cost-effective\nFigure 1: Figure illustrates how SayCanPay scores the next action in BabyAI environment (Chevalier-Boisvert et al. 2019).\nGiven inputs: goal g and initial observation o0, the Say model generates candidate actions with associated probabilities. These\nare then scored for feasibility by the Can model and for payoff by the Pay model. Here, the Can model deems both pick up\nred key and pick up green ball equally probable (i.e. both preconditions are satisfied). However, the Pay model ensures a better\npayoff for pick up green ball. We compare plans generated by Say, SayCan, and SayCanPay scoring. Say scoring can lead to\ninfeasible plans and SayCan to feasible but longer plans. The displayed grid is purely illustrative, with no visual inputs used.\ngenerates likely candidate actions at each step; (ii) Can: An\naffordance model scores these actions’ feasibility, mirror-\ning the evaluation of preconditions; (iii) Pay: Another model\nscores the actions according to their estimated payoff, akin\nto heuristic estimators. The Can and Pay models undergo\ndomain-specific training to align the plans with the current\nenvironment. (3) Using this combined score as a heuristic,\nwe search for the most feasible and cost-effective plan. We\ndemonstrate both quantitatively and qualitatively, how our\nproposed joint scoring and heuristic search improve over the\ncurrent LLM planning frameworks.\nRelated Work on Planning with LLMs\nTable 1 categorizes LLM planning works into two broad\ncategories based on whether the inputs (goals, states) and\noutput actions (I/O) are natural language (NL) or symbolic\n(PDDL, scripting language). The approaches in the first cat-\negory (Huang et al. 2022a; Valmeekam et al. 2022) often\nfail to model action affordances and the state of the world,\nleading to the generation of infeasible plans (Valmeekam\net al. 2022). To improve the groundedness, recent works\nhave explored planning guided by learnable domain-specific\nmodels that score the actions’ feasibility akin to precondi-\ntions (Huang et al. 2023; Lin et al. 2023). Notably, Say-\nCan (Ahn et al. 2022) uses pretrained low-level skills to\nground the LM-generated actions. Others have used online\nplanning with environmental and human feedback (Huang\net al. 2022b). A limitation of such models, however, is their\nshort-sighted nature, as they focus greedily on the next feasi-\nble action without considering its long-term relevance to the\ngoal. Moreover, the plans are generated in an online fashion,\ninterleaving action generation and execution, thus simplify-\ning state tracking. In contrast, SayCanPay performs offline\nplanning (i.e. complete plan generation while maintaining\nan internal world state) with both precondition and heuristic\nestimators, improving plan feasibility and cost-efficiency.\nAnother line of work employs LLMs to create offline\nsymbolic plans, leveraging LLMs’ training on open-source\ncodebases, where actions appear as function calls (Singh\net al. 2023; Liang et al. 2023). The feasibility of plans is\nensured through assertion checks (assert ⟨ preconditions ⟩),\nthat may trigger recovery actions. However, it relies solely\non the LLM’s domain knowledge which is limited to its\ntraining data and may not be aligned with the agent’s current\nenvironment (e.g. espresso machine operations vary widely).\nConversely, SayCanPay uses additional models trained with\ndomain-specific knowledge collected from the current envi-\nronment. There are also efforts to fine-tune LLMs like Code-\nT5 (Wang et al. 2021) to generate plans in PDDL (Pallagani\net al. 2022). This requires a significant amount of training\ndata (given LLMs’ minimal PDDL exposure) which is not\nentirely justified by their performance.\nYet another exciting line of work explores hybrid I/O\nsystems like LLM+P (Liu et al. 2023) wherein, given a\nPDDL domain file (with a predefined action model), the\nLLM maps the NL inputs (task description, input observa-\ntion) to a PDDL problem file. A symbolic planner then gen-\nerates the plan. However, its effectiveness is limited by the\nclosed-world constraint of the domain file, the necessity for\nfully observable states, and the LLM’s restricted capability\nin translating NL to PDDL (Xie et al. 2023).\nPreliminaries\nPlanning Framework. We formulate our planning prob-\nlem, based on approximate planning (Golowich, Moitra,\nand Rohatgi 2022), as a finite-horizon Partially Observ-\nable Markov Decision Process (POMDP) given by the tuple\n⟨S, SG, b0, A, O, R,T⟩. Here, S is state space, SG ⊆ Sis a\nset of goal states, b0 is the initial belief state, A is the set of\nactions, O is a set of observations retrieved from states via\nan observation function O, R : O →R is a known reward\nfunction, T : S × A →∆S is a known stochastic transition\nfunction and ∆S is a distribution over states. Belief states\nrepresent the agent’s knowledge of the environment at any\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20124\nModel I/O Planner Domain Knowledge Search Planning\nAffordances Heuristics\nHSP (Bonet and Geffner 2001) Symbolic Symbolic ✓ ✓ Heuristic Offline\nLLM+P (Liu et al. 2023) Hybrid Symbolic ✓ ✓ Heuristic Offline\nPlanning LM (Huang et al. 2022a) NL LLM ✗ ✗ Greedy∗ Offline\nSayCan (Ahn et al. 2022) NL LLM ✓ ✗ Greedy∗ Online\nGrounded Decoding (Huang et al. 2023) NL LLM ✓ ✗ Greedy∗ Online\nText2Motion (Lin et al. 2023) NL LLM ✓ ✗ Greedy∗ Online\nProgPrompt (Singh et al. 2023) Symbolic LLM ✓ ✗ Greedy∗ Offline\nPlansformer (Pallagani et al. 2022) Symbolic LLM ✓ ✗ Greedy∗ Offline\nSayCanPay (Beam-Action) NL LLM ✓ ✓ Heuristic Offline\nTable 1: Table contrasts SayCanPay with existing works. I/O: input (goal/task, observation/state) / output (actions), NL: natural\nlanguage. Here, Greedy∗ suggests the algorithm greedily selects actions while (possibly) searching over tokens.\npoint, given as b ∈ ∆S. Additionally, let Ht := (A × O)t−1\ndenote the set of histories at step t, namely the set of\naction/observation sequences (o0, a1, o1, . . . , at−1, ot−1) or\n(a1:t−1, o0:t−1) the agent has access to before selecting ac-\ntion at. It is assumed that the goal states are fully observable.\nUnlike MDPs, the optimal policy in a POMDP typically\ntakes actions depending on not just the most recent observa-\ntion but the entire history. The objective of the planning al-\ngorithm is to find the optimal sequence of actions a1:T (i.e.\nan optimal plan) from an initial belief stateb0 to a given goal\nstate g ∈ SG. Here, T is the length of the horizon.\nHeuristic Search Planning. In real-world scenarios\nwhere the state space can be exponentially large to explore\nexhaustively, heuristic search planning (HSP) becomes use-\nful (Bonet and Geffner 2001). Essentially, it uses heuristic\nfunctions fheur : Ht × SG → R to guide the search process\nin the planning problem, by computing a cost estimate from\na given history of actions and observations. An example is\nthe Best-First Search algorithms that select the most promis-\ning (next) action(s) using a linear combination of previously\naccumulated cost facc for history ht−1, and the estimated\ncost fheur from updated history ht = (ht−1, at) and goal g.\nf(ht) = z1 · facc(ht−1) + z2 · fheur(ht, g) (1)\nHere z1, z2 ∈ { 0, 1}. The next action at =\narg minht f(ht). Special cases are the A∗ algorithm algo-\nrithm (z1 = 1 and z2 = 1 ) and Greedy Best-First Search\n(z1 = 0 and z2 = 1).\nLanguage Model Planning Framework\nWe keep the same POMDP formulation while updating our\ninterpretations of the tuple. Previous works have shown that\nlanguage models (LMs) trained on extensive data would in-\nternalize rich world knowledge that can be queried for down-\nstream tasks like planning (Hao et al. 2023). This is akin\nto an internal transition function Tint. Similarly, LMs also\nmaintain and update an internal belief state bint\nt over tokens\n(or actions). An observation function maps states to NL ob-\nservations, O : S → O. The updated POMDP is now given\nas ⟨S, SG, bint\n0 , A, O, R,Tint⟩. In our offline planning ex-\nperiments, we assume the following: (i) O = {o0} inducing\nbelief state bint\n0 = 1s0 , while ot = ∅ ∀t >0, due to lack\nof environmental feedback; (ii) sparse rewards = 1 for plan\nsuccess, else 0. While our LM does not utilize the reward\nfunction, one could use it for alignment (Ziegler et al. 2020).\nProblem Statement: Given a NL goal g, history h0 =\n(o0), and a LM generating actions at with probability\np(at|ht−1, g), generate the most likely plan (a 1:T ) to go\nfrom bint\n0 to g, i.e., arg maxa1:T P(a1:T |h0, g).\nWe aim to maximize the plan’s probability, reframing LM\nplanning as a classical search problem, where we repeatedly\nexpand the current plan a1:t−1 by adding action at. Rewrit-\ning the probability P(a1:T |h0, g) recursively as:\n= P(a1:t−1, at, at+1:T |h0, g)\n= p(a1:t−1|h0, g)p(at|h0, a1:t−1, g)p(at+1:T |h0, a1:t, g)\n= p(a1:t−1|h0, g) · p(at|ht−1, g) · p(at+1:T |ht, g)\nTo align with Eq 1 of the planning problem, we take\nlog on both sides and maximize rather than minimize. We\nget accumulated value facc(ht−1) = log p(a1:t−1|h0, g),\nheuristic payoff fheur(ht, g) = p(at+1:T |ht, g), and f(ht) =\nlog P(a1:T |h0, g). Rewriting the above equation:\nf(ht) = facc(ht−1) + log\n\u0000\np(at|ht−1, g) ·fheur(ht, g)\n\u0001\n(2)\nThe additional p(at|ht−1, g) reflects that, unlike classical\nplanning which evaluates only feasible actions based on pre-\nconditions, LMs assign probabilities to each action. Here,\nnext action at = arg maxht f(ht).\nTechnically, the LM generates actions wherein each ac-\ntion is a sequence of tokens until the end-of-sequence to-\nken, ⟨EOS⟩. For each action step a = ( w1, . . . , wn) com-\nposed of tokens wi, the LM computes the action probability\nas p(a) = p(w1) Qn\ni=2 p(wi|w1:i−1). Planning LM (Huang\net al. 2022a) proposed a greedy decoding strategy wherein\nthe LM greedily picks the next token, henceforth referred\nto as Greedy-Tokenbaseline (Figure 2 Left). The generated\naction is then appended to the historyht= (ht−1, at), and the\ngeneration process repeats until a “done task” action is gen-\nerated. Subsequent works (Lin et al. 2023) have investigated\nbeam search over tokens. However, we are mainly interested\nin searching on the level of actions and not tokens.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20125\nAbstraction\n(a) Greedy-Token (b) Beam-Token  Single Greedy-Action step (c) Greedy-Action (d) Beam-Action\ngoal   history best token \n discarded token discarded action best action \nnext best token next-best action \nFigure 2: The figure outlines decoding strategies – Greedy-Token, Greedy-Action, and Beam-Action. Greedy-Token greedily\nselects the next best token by its probability. Greedy-Action (which is a beam search over tokens) greedily selects the next best\naction based on a specific decoding score. Beam-Action uses a beam search over actions, maintaining k beams and selecting\nthe best sequence as the plan. Here, nodes represent either tokens wt or actions at. The best plan is given by (a∗\n1, a∗\n2, a∗\n3) and\nrepresented in red. The second-best node is in orange, discarded ones in black. Here, for Beam-Action, m = 3 and k = 2.\nSayCanPay Inference\nThe core concept of SayCanPay is to guide LMs in gener-\nating feasible and cost-effective plans. The process unfolds\nin three key steps: (1) Say: At each step t, the LM gener-\nates the top-m candidate actions with associated probabil-\nities {p(ai\nt|ht−1, g)}m\ni=1. This generation employs a beam\nsearch over tokens. (2)Can: Next, a trained domain-specific\nmodel weighs these candidate actions on their feasibility,\nmirroring precondition evaluation. (3)Pay: Finally, a trained\ndomain-specific estimator weighs the candidate actions ac-\ncording to their estimated payoff. The probabilities from\nthese three components are then combined to select the next\naction. An overview of SayCanPay is provided in Figure 1.\nIn what follows, we instantiate the LM planning prob-\nlem with two decoding strategies (or search algorithms that\nselect the next action(s)): Greedy Action and Beam Ac-\ntion. Each strategy is explored using three distinct decoding\nscores (i.e. score used by the search algorithm to select the\nnext action) – Say, SayCan, SayCanPay. We then elaborate\non the training of Can and Pay models.\nGreedy-Action\nIn this decoding strategy, we maintain a single action se-\nquence and at each step, greedily choose the nextbest action\nbased on a specific decoding score. This is akin to perform-\ning Greedy Best-First Search with z1 = 0 and z2 = 1. The\ndecoding score for each candidate action ai\nt is given as:\nf(hi\nt) = log\n\u0000\np(ai\nt|ht−1, g) · fheur(hi\nt, g)\n\u0001\nHere, the best action a∗\nt = arg maxhi\nt\nf(hi\nt), where hi\nt =\n(ht−1, ai\nt) denotes the current history with ith candidate ac-\ntion. As shown in Figure 2, this approach can be viewed as\nbeing “greedy” with respect to actions while using “beams”\nover the tokens. Now, we explore three variations of the\nstrategy based on how the decoding score is computed.\n• Say: In this decoding score, we set the estimated payoff\nfheur(hi\nt, g) = 1 ∀ i ∈ {1, . . . , m}. Hence, the action is\nselected solely based on the LM generation probability,\nwithout considering feasibility or payoff.\nf(hi\nt) = log\n\u0000\np(ai\nt|ht−1, g)|\n{z }\n=:psay\nai\nt\n\u0001\n(3)\n• SayCan: Here, the action feasibility is also considered.\nLet, σt = (at, pre(at)) where pre(at) denotes the pre-\nconditions of at. The “can” probability 3, is denoted by\np(pre(at)|ht−1, g). Again, fheur(hi\nt, g) = 1 ∀ i.\nf(hi\nt) = log\n\u0000\np(σi\nt|ht−1, g)\n\u0001\n= log\n\u0000\np(ai\nt|ht−1, g)|\n{z }\n=:psay\nai\nt\n·p(pre(ai\nt)|ht−1, g)| {z }\n=:pcan\nai\nt\n\u0001\n(4)\n• SayCanPay: This decoding score accounts for the esti-\nmated payoff in addition to the abovementioned scores.\nHence, the best action is selected based on a combined\nscore of Say, Can, and Pay scores.\nlog\n\u0000\np(ai\nt|ht−1, g)|\n{z }\n=:psay\nai\nt\n·p(pre(ai\nt)|ht−1, g)| {z }\n=:pcan\nai\nt\n·fheur(hi\nt, g)| {z }\n=:ppay\nai\nt\n\u0001\n(5)\nBeam-Action\nIn heuristic planning, multiple potential plans (i.e. action se-\nquences) are simultaneously maintained and iteratively ex-\npanded until the goal is achieved. To simulate this behav-\nior, we propose to manage k action sequences. It works as\n3The goal g is used to evaluate the preconditions of “done task”.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20126\nfollows – each sequence is expanded with m candidate ac-\ntions (where m ≥ k) from the LM, resulting in a total of\nk × m sequences. Then, top-k sequences are retained using\na specific decoding score accumulated over the sequence, as\nshown below. Once all k-beams have terminated, we select\nthe sequence with the highest (length-normalized)4 accumu-\nlated score. To avoid repetition, we only show the SayCan-\nPay version. The rest can be similarly formulated.\ntop-k\n\u0014 1\n|hij\nt |\n\u0012\nfacc(hi\nt−1)+log p(σj\nt |hi\nt−1, g)·fheur(hij\nt , g)\n\u0013\u0015\nHere, i ∈ {1, . . . , k}, j ∈ {1, . . . , m}, k ≤ m. The updated\nhistory hij\nt = ( hi\nt−1, aj\nt) is obtained by adding the action\naj\nt to the ith beam history hi\nt−1. The outcome becomes the\nvalue for facc(ht) for the next iteration. Note, that setting\nk = 1 results in Greedy-Action decoding.\nOur proposed decoding has similarities with Tree-of-\nThoughts inference (Yao et al. 2023) which also maintains\nmultiple reasoning paths to decide the next step. However,\nour method is specifically tailored for planning problems.\nIt uses search and evaluation techniques akin to planning\nmethods, making it more suited for such challenges. Now,\nwe discuss training details of the Can and Pay models.\nLearning the Can and Pay Models\nTo train our domain-specific Can and Pay models, we collect\nN-expert trajectories E = {τ}N\nn=1 for each environment us-\ning an oracle planner, where τi = (o0, g, a1, a2, . . . , aT , r).\nNote, r = 1 for all expert trajectories.\nCan Model\nWe model it as a classification problem, where the positive\naction (i.e., the action whose preconditions are satisfied) is\nassigned the highest probability from a set of one positive\nand a few negative actions. Specifically, we sample a batch\nof actions [ht−1, g, at, a¯t̸=t, ˜a]1:B from expert trajectoriesE.\nWe then train a model Mcan with the aim of minimizing the\nInfoNCE loss (van den Oord, Li, and Vinyals 2019):\n− 1\nB\nBX\ni=1\nlog Mcan(hi\nt−1, gi, ai\nt)P\na∈{ai\nt,ai\n¯t̸ =t,˜ai} Mcan(hi\nt−1, gi, a)\nHere, B is the batch size, at is the positive action from\ntrajectory τi executed in the context of history ht−1 with\ngoal g, a¯t̸=t is a negative action sampled from the same\ntrajectory τi, but at a different time-step ¯t, and ˜a is a neg-\native action sampled from a different trajectory τj̸=i with\na different initial observation o0 and goal g. Mcan con-\nsists of an uncased Bert model (Devlin et al. 2019) with\na probe layer and is trained end-to-end to correctly iden-\ntify the positive action. The input to Mcan is of the for-\nmat ‘⟨Goal⟩{g} ⟨History⟩{ht−1} ⟨NXT⟩{at}’. Here, ‘⟨∗⟩’\nserves as special tokens. The output is the Can probability\npcan\nat := Mcan(ht−1, g, at). The model is trained across mul-\ntiple batches for F1-score convergence on the validation set.\n4Since different beams can have different sequence lengths.\nOur approach is different from SayCan (Ahn et al. 2022)\nwhich trains multiple affordance functions (corresponding\nto different skills), through temporal-difference-based rein-\nforcement learning to predict the likelihood of a particular\nskill succeeding (i.e., executing) in the current state. Here,\nwe show two training I/O examples, one with positive action\nand another one with negative action.\nInput ⟨Goal⟩ pick up the purple box. ⟨Initial State⟩ Room\n1 has yellow key, agent. Room 2 has purple box. The door\nconnecting Room 1 and Room 2 is locked. ⟨Step 1⟩ pick up\nyellow key. ⟨NXT⟩ toggle yellow door.\nOutput 1.0 // feasible\nInput ⟨Goal⟩ pick up the purple box. ⟨Initial State⟩ Room\n1 has yellow key, agent. Room 2 has purple box. The door\nconnecting Room 1 and Room 2 is locked. ⟨Step 1⟩ pick up\nyellow key. ⟨NXT⟩ pick up purple box.\nOutput 0.0 // infeasible\nPay Model\nWe model it as a regression problem to estimate action\npayoffs. Using expert trajectories E, we create a dataset\nwith each batch as [g, ht−1, at, r]1:B. Given sparse rewards\n(i.e. rT = 1 ), we use temporal discounting δ ∈ (0, 1)\nto assign discounted rewards to previous actions in\nthe trajectory 5. This ensures that actions closer to the\nend receive higher rewards and vice versa. Specifically,\nrT−1 = δ, rT−2 = δ2, and so on. We also sample neg-\native actions from other paths (akin to the Can model)\nwith a reward of 0. The model is trained to align the\ndiscounted reward of the action and the predicted reward\nfrom Mpay by minimizing the mean squared error (MSE)\nloss 1\nB\nPB\ni=1(ri − Mpay(gi, hi\nt−1, ai\nt))2. The model uses\nan uncased Bert plus a regression layer whose output is\nbounded in [0, 1] via a sigmoid activation. The input format\nis the same as the Can model. The output is the estimated\npayoff, fheur(ht, g) = Mpay(g, ht−1, at). I/O examples:\nInput ⟨Goal⟩ pick up the purple box. ⟨Initial State⟩ Room\n1 has yellow key, agent. Room 2 has purple box. The door\nconnecting Room 1 and Room 2 is locked. ⟨Step 1⟩ pick up\nyellow key. ⟨Step 2⟩ toggle yellow door. ⟨Step 3⟩ drop key\nin void. ⟨Step 4⟩ pick up blue box. ⟨NXT⟩ done picking up.\nOutput 1.0 // end of plan\nInput ⟨Goal⟩ pick up the purple box. ⟨Initial State⟩ Room\n1 has yellow key, agent. Room 2 has purple box. The door\nconnecting Room 1 and Room 2 is locked. ⟨Step 1⟩ pick up\nyellow key. ⟨Step 2⟩ toggle yellow door. ⟨Step 3⟩ drop key\nin void. ⟨NXT⟩ pick up blue box.\nOutput 0.6 //δ · r\nInput ⟨Goal⟩ pick up the purple box. ⟨Initial State⟩ Room\n1 has yellow key, agent. Room 2 has purple box. The door\nconnecting Room 1 and Room 2 is locked. ⟨Step 1⟩ pick up\nyellow key. ⟨Step 2⟩ toggle yellow door. ⟨Step 3⟩ drop key\nin void. ⟨NXT⟩ pick up green box.\nOutput 0 // very low payoff\n5δ for the Pay model training is unrelated to the POMDP.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20127\nEnvironment Example Goal Example Initial Observation Plan Length |A|\nRavens\n(Tower of Hanoi seq)\nMove the gray\ndisk in rod 2\nBlue disk on top of gray disk. Gray disk on top of green disk. Green\ndisk in rod 1. The disks can be moved in rod 1, rod 2, rod 3.\n3.3 7.5\nRavens\n(Put Blocks in Bowls)\nPut the yellow\nblocks in gray\nbowls\nThere is a gray bowl 1, gray bowl 2, gray bowl 3, yellow block 1,\nyellow block 2, yellow block 3, blue bowl 1, red block 1, green bowl\n1, orange block 1.\n6.1 25\nBabyAI (Pickup) Pick up the ball Room 1 has purple ball. Room 2 has yellow key, agent. Room 3 has\nred key. The door connecting Room 1 and Room 2 is locked. The\ndoor connecting Room 2 and Room 3 is locked.\n6.7 7.7\nVirtualHome Read book 5.9 150\nTable 2: Table displays tasks from each environment, average plan length, and average action space size|A|. For VirtualHome,\nwe do not specify an initial observation since it is hard to describe a room environment. Here, the action space varies with\nepisodes, depending for instance on the number of objects.\nSetup Say Model Greedy-Token Greedy-Action Beam-Action\nSay SayCan SayCanPay Say SayCan SayCanPay\nRavens\n(tower of hanoi)\nVicuna 45 48 48 50 54 68 70\nFlan-T5 30 30 39 42 38 50 50\nRavens\n(put blocks in bowls)\nVicuna 30 51 52 54 52 52 56\nFlan-T5 96 96 96 96 98 98 98\nBabyAI\n(pickup)\nVicuna 59 62 81 88 72 94 94\nFlan-T5 0 0 30 36 1 36 30\nVirtualHome Vicuna 0 32 49 52 48 52 53\nFlan-T5 0 0 30 48 30 41 50\nTable 3: Table shows the planning success (i.e. the number of plans out of 100 that reached the goal within limited steps) on\nthe test split across different environments using Vicuna, Flan-T5 models. It can be observed that the best decoding strategy is\nBeam-Action and the best decoding score is SayCanPay.\nExperimental Setup\nSay Model\nThe Say model does not undergo any fine-tuning and is\nused only for inference. We experimented with two types of\ntransformer architectures. (i) Decoder type: 13b-parameter\nVicuna model (Chiang et al. 2023) trained by fine-tuning\nLLaMA (Touvron et al. 2023). (ii) Encoder-decoder type:\nFlan-T5-11b (Chung et al. 2022) which is the instruction\nfine-tuned version of the T5 transformer (Raffel et al. 2020).\nExisting works have demonstrated the planning abilities\nof both the decoder type (Pallagani et al. 2022) and the\nencoder-decoder type architectures (Valmeekam et al. 2023,\n2022). Since the generated plan is in free-form language and\nmay contain unrecognizable (for the environment) words\nor incorrect syntax, it cannot be directly translated into ac-\ntionable steps in the environment. Following Huang et al.\n(2022a), we use an exhaustive list of admissible actions (fea-\nsible and otherwise), and at the end of each action step, map\nthe generated action to the closest admissible action using\nminimum edit distance. Interleaving action generation and\nmapping ensures that all subsequent steps are conditioned\non admissible actions, thus mitigating compounding errors.\nWe provide 3 examples (input goal and observation, output\nplan) to the model via few-shot prompting.\nEnvironments\nWe tested in three environments, detailed in Table 2.\n• Ravens (Zeng et al. 2021) is a PyBullet simulated task\nset focusing on “pick and place”. It includes 10 table-\ntop tasks, of which we use two: (i) Tower of Hanoi (se-\nquence), a variation of the classic puzzle focusing on spe-\ncific intermediate goals, like moving a particular disk to a\ndesignated rod while keeping the traditional constraints.\nThis creates more goal diversity; (ii) Put blocks in bowls\nrequires placing blocks into bowls based on rules likeput\nyellow block in green bowls. We adapt the environment\nfor language tasks, observations, and actions.\n• BabyAI (Chevalier-Boisvert et al. 2019) is a 2D-grid\nworld environment where a bot is provided a language\ntask sampled from a predefined grammar. We focus on\npickup tasks where the agent navigates to collect an ob-\nject, often unlocking doors or moving obstacles. Task dif-\nficulty varies with rooms, obstacles, and distractor ob-\njects. The agent’s actions include high-level commands\nlike pickup and drop which are composed of atomic ac-\ntions: “left”, “right”, “forward”, “pick”, and “drop” (see\nFigure 1)\n• VirtualHome (Puig et al. 2018) is an interactive platform\nto simulate complex household activities via interactions\nwith the environment, such as picking up objects, and\nswitching on/off appliances. We utilize the VirtualHome-\nEnv dataset (Liao et al. 2019), comprising daily house-\nhold activities from 7 scenes gathered via crowdsourcing.\nWe only use the goal as the input (see Table 2).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20128\nSetup Say Model Greedy-Token Greedy-Action Beam-Action\nSay SayCan SayCanPay Say SayCan SayCanPay\nRavens\n(tower of hanoi)\nVicuna 12 24 55 58 20 47 52\nFlan-T5 34 34 46 47 38 54 56\nRavens\n(put blocks in bowls)\nVicuna 16 36 40 48 38 42 56\nFlan-T5 63 65 71 74 67 74 74\nBabyAI\n(pickup)\nVicuna 48 50 53 54 56 56 62\nFlan-T5 0 0 26 28 1 30 34\nVirtualHome Vicuna 0 14 23 29 20 26 30\nFlan-T5 0 0 6 15 4 19 26\nTable 4: Table shows the cost-effectiveness (i.e. the number of plans out of 100 that reached the goal within limited steps and\nalso had the same plan length as the expert plan) on the test split across different environments using Vicuna, Flan-T5 models.\nIt can be observed that the best decoding strategy is Beam-Action and the best decoding score is SayCanPay.\nSetup Say Model Greedy-Token Greedy-Action Beam-Action\nSay SayCan SayCanPay Say SayCan SayCanPay\nRavens\n(tower of hanoi)\nVicuna 32 30 18 18 27 34 34\nFlan-T5 24 22 18 16 26 26 26\nRavens\n(put blocks in bowls)\nVicuna 8 30 10 6 30 10 6\nFlan-T5 94 94 26 18 96 22 24\nBabyAI\n(pickup)\nVicuna 0 1 4 12 9 12 10\nFlan-T5 0 1 28 28 1 15 28\nVirtualHome Vicuna 0/20 2/20 3/20 3/20 5/20 5 /20 5 /20\nFlan-T5 0/20 0/20 0/20 3/20 1/20 3/20 5/20\nTable 5: Table shows the generalization results (i.e. the number of plans out of 100 that reached the goal) on test-generalize\nsplit across different environments using Vicuna and Flan-T5 models. It can be observed that Beam-Action outperforms other\ndecoding strategies.\nData Splits and Evaluation. We aim to assess the suc-\ncess, cost-effectiveness, and out-of-distribution (OOD) gen-\neralization of the generated plans. We created three data\nsplits for each environment using expert trajectories. (i)\ntrain split for Can, Pay model training and few-shot prompt-\ning of the Say Model; (ii) test split assesses the LM plan-\nners’ ability to generate successful plans (i.e. reach the goal\nwithin limited steps), and also the planners’ ability to gen-\nerate cost-effective plans (i.e. plans that succeed and also\nhave the same plan length as the expert plan 6). (iii) test-\ngeneralize split focuses on the generalization capabilities\nlike handling novel initial observations (e.g., unseen col-\nors of blocks and bowls, distractors in BabyAI), longer se-\nquence lengths (e.g., more blocks or disks in Ravens, more\nrooms in BabyAI), and unseen tasks in VirtualHome. All test\nsplits have # total episodes = 100 unless specified otherwise.\nMoreover, all splits are disjoint (i.e. no overlap).\nBaselines. At the action level, we evaluate our decoding\nscores (Say, SayCan, SayCanPay) using various decoding\nstrategies (Greedy and Beam-Action). Therefore, our base-\nlines employ a mix of these strategies and scores. For tokens,\nwe use the Greedy-Token decoding strategy as a reference.\nNotably, Greedy-Action SayCan is the offline planning ver-\nsion of the original SayCan paper (Ahn et al. 2022).\n6We split test into two parts of 100 samples to evaluate success,\ncost-effectiveness. For VirtualHome, we use the annotated plans\nfrom its dataset.\nTraining and Inference Details.We use 800 expert train\ntrajectories for each Ravens task and 400 for BabyAI. For\nVirtualHome, we retained ≈ 800 compatible trajectories\nfor the current simulator. An additional 100 expert trajec-\ntories were collected for each test split (20 for VirtualHome\ntest-generalize). The Can and Pay models were trained on\n7 NVIDIA-DGX V-100 GPUs using the Distributed Data-\nParallel framework across 20 epochs. Training parameters\nincluded a 1e-4 learning rate, AdamW optimizer with 1e-\n5 weight decay, a batch size of 50, a train-validation split of\n80-20. For inference, the Say model was loaded using Model\nParallel on the same GPUs. Inference hyperparameters are\nlisted in Table 6. Parameters like beam groups and diversity\npenalty encourage diversity among the beams, mitigating is-\nsues like returning multiple similar sequences. We used 8-bit\nprecision for GPU-efficient model loading (Dettmers et al.\n2022).\nResults\nWe analyze the results along the following axes: decoding\nstrategies, decoding scores, and transformer architectures.\nWe assessed planning success and generalization by execut-\ning the generated plans in simulators such as Ravens and\nBabyAI, which have built-in validation checks to determine\ngoal achievement. For the more open-ended VirtualHome\nenvironment, we manually reviewed fully executed plans\nto ensure they met the intended task objectives. For cost-\neffectiveness, we acquired expert trajectories for each test\nsample using an oracle planner.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20129\nFigure 3: From left to right: Planning success, cost-effectiveness, generalization for different beam sizes. Note, that generaliza-\ntion on the test-generalize split for VirtualHome is reported as a percentage.\nComparing decoding scores.From Tables 3, 4, the perfor-\nmance across various decoding scores can be summarized\nas Say < SayCan ≤ SayCanPay. (i) planning success: The\nSayCanPay and SayCan scores lead to comparable perfor-\nmances, often outperforming Say. The Pay model’s minor\nperformance edge could be due to its focus on selecting ac-\ntions based on long-term relevance, potentially avoiding ir-\nreversible (breaking an egg) or even absorbing states (dis-\ncharged cellphone) from where it is impossible to reach the\ngoal (i.e. planning is non-ergodic). (ii) cost-effectiveness:\nSayCanPay leads to a significant improvement over both Say\n(≈ 11−97% for Beam-Action) and SayCan (≈0−33% for\nBeam-Action and ≈ 1−150% for Greedy-Action). (iii) gen-\neralization: From Table 5, while the overall performance\nfor SayCan and SayCanPay improves over Say, a noticeable\ndrop in performance was observed for Ravens. This led to\nthe hypothesis that the learned domain models (Can, Pay)\nare not generalizing to OOD data in certain environments.\nComparing decoding strategies. From Tables 3, 4, 5,\nthe overall performance across decoding strategies fol-\nlows the pattern: Greedy-Token < Greedy-Action < Beam-\nAction across all splits. The Beam-Action Say, SayCan,\nand SayCanPay versions show improvement over their cor-\nresponding Greedy-Action counterparts. (i) planning suc-\ncess: Beam-Action SayCanPay beats Greedy-Action Say-\nCanPay by ≈ 1 − 40%. Similar gains are also observed\nwith other decoding scores. (ii) cost-effectiveness: Beam-\nAction SayCanPay improves over Greedy-Action SayCan-\nPay by ≈ 0 −73%. (iii) generalization:Beam-Action Say-\nCanPay beats Greedy-Action SayCanPay by ≈ 0 − 89%.\nComparing Transformer Architectures. We did not ob-\nserve a consistent performance gain for any particular archi-\ntecture, suggesting that either is apt for planning. We lack\na definitive explanation, and further research is required to\nunderstand how each LM component impacts reasoning.\nAblation Details. We performed the following ablations\nfor a deeper understanding of the individual modules.\n• Effect of beam-size k: As seen in Figure 3, in general,\nboth plan success and cost-effectiveness increases with\nincrease in beam size with k = 1 (Greedy-Action), 2, 3\n(Beam-Action). All experiments used the SayCanPay de-\ncoding score. However, no clear patterns were observed\nfor generalization results.\n• Impact of Say Model: Planning failures may arise be-\ncause the Say model fails to propose a right action\namongst the candidates, making Can and Pay ineffective.\nWe studied the Say model’s impact on overall perfor-\nmance using a Perfect Say that always recommends the\ncorrect action along with random distractors. From Ta-\nble 7, we observed16-84% improvements in SayCan and\nSayCanPay performance across various environments,\nindicating the potential of an improved Say model. Thus,\nusing a larger model trained on more data could poten-\ntially enhance performance.\n• Plan length comparison: We compute a relative length=\noracle plan length / generated plan length, which com-\npares the generated and oracle plan lengths. A value = 1\nindicates equal lengths, and a value = 0 , that the plan\nlength is infinity (i.e. an unsuccessful plan). As shown in\nFigure 4, Beam-Action slightly improves over Greedy-\nAction. Furthermore, SayCanPay scoring achieves the\nbest relative length (≈ 1) for both Greedy and Beam-\nAction strategies signifying its cost-efficiency.\n• Impact of problem size on planning time. Effect of ac-\ntion space: Planning time remains unaffected since the\nSay model generates rather than discriminates between\nactions. Effect of plan length: Greedy-Token run time\nincreases by ∼2s for each action step. Effect of decod-\ning strategy: ∼9s for Greedy-Token, ∼17s for Greedy-\nAction, ∼35s for Beam-Action. Effect of decoding score:\nPlanning time is unaffected since the Can and Pay are\nsmall LMs with negligible overheads. Quantization tech-\nniques and advanced hardware can further reduce run\ntime and is an active research area (Dettmers et al. 2023;\nFrantar et al. 2023).\n• Qualitative Analysis: The Can model effectively selects\nfeasible actions (Figure 1). The Pay model prioritizes ac-\ntions that lead to quicker goal achievement. While Pay\ngives a high probability to the “done task” action link-\ning it to plan completion, the Can score negates it due to\nunsatisfied “done task” preconditions.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20130\nFigure 4: [Best viewed in color] The error plot represents the variance in relative length over models Vicuna and Flan-T5. Due\nto the open-ended nature of VirtualHome, the crowdsourced trajectories are not optimal, which explains why certain cases have\na relative length > 1.0. Note that Greedy-Token decoding in VirtualHome has a relative length = 0 since no generated plans\nwere executed successfully for both Vicuna and Flan-T5.\nParameter Value Exceptions\nmax new tokens 10 11 Vicuna (Ravens-Blocks),\n3 (VirtualHome)\nbeam groups 3 4 for Flan-T5 (BabyAI)\ndiversity penalty 2.0\ncandidates (m) 6 8 for Flan-T5 (Baby-AI)\nbeam-size (k) 3\nTable 6: Inference hyperparameters. Here the candidates (m)\nand the beam-size (k) parameter are over actions. The rest of\nthe beam search parameters are over tokens.\nLimitations and Future Work\nThe main limitations are (i) the need for expert trajectories\nto train domain models, and (ii) the domain models’ lim-\nited adaptability to OOD data. These challenges are inher-\nent to deep learning models. However, recent advances in\nLLMs offer promising solutions. For example, newer stud-\nies have leveraged LLMs for reward design due to their abil-\nity to infer intentions from minimal prompts (Kwon et al.\n2023). Notably, LLMs outperform smaller counterparts like\nBert in generalization. Since both Can and Pay scores resem-\nble rewards, future studies could use LLMs to mitigate train-\ning and improve generalization. Another potential direction\ncould be to experiment with symbolic methods and non-\nparameterized heuristics like comparing the current gener-\nated plan with the successful/expert trajectories in the buffer.\nConclusion\nWe proposed to combine the world knowledge and genera-\ntive capabilities of LLMs with the systematicity of classical\nplanning by formulating a heuristic search-based planning\nframework for LLMs. We demonstrated how to generate\nplans that are both feasible and cost-effective. While LLMs\nstill cannot generate long-horizon plans on par with classi-\ncal planners, our method overcomes issues inherent to LLM-\nbased planning and extends traditional planning with the ad-\nvantages of language models, marking significant progress\nScore LM Perfect\nRavens-Hanoi SayCan 48 88\nSayCanPay 50 92\nRavens-Blocks SayCan 52 70\nSayCanPay 54 75\nBabyAI SayCan 81 90\nSayCanPay 88 92\nVirtualHome SayCan 49 60\nSayCanPay 52 64\nTable 7: The table depicts the impact of the Say model on\nplanning success performance. In this context, both “LM”\nand “Perfect” represent Say models. “LM” corresponds to\nthe Vicuna model, while “Perfect Say” is an oracle Say\nmodel that consistently proposes the correct action along\nwith two other distractor actions as next candidates. For all\nexperiments, we used the Greedy-Action decoding strategy.\nfor planning research with LLMs.\nAcknowledgements\nThis work was supported by the Wallenberg AI, Au-\ntonomous Systems and Software Program (W ASP) funded\nby the Knut and Alice Wallenberg Foundation, and is\nalso part of the EU H2020 ICT48 project “TAILOR” un-\nder contract 952215, and the KU Leuven Research Fund\n(C14/18/062).\nReferences\nAhn, M.; Brohan, A.; Brown, N.; Chebotar, Y .; Cortes, O.;\nDavid, B.; Finn, C.; Fu, C.; Gopalakrishnan, K.; Hausman,\nK.; Herzog, A.; Ho, D.; Hsu, J.; Ibarz, J.; Ichter, B.; Irpan,\nA.; Jang, E.; Ruano, R. J.; Jeffrey, K.; Jesmonth, S.; Joshi,\nN. J.; Julian, R.; Kalashnikov, D.; Kuang, Y .; Lee, K.-H.;\nLevine, S.; Lu, Y .; Luu, L.; Parada, C.; Pastor, P.; Quiambao,\nJ.; Rao, K.; Rettinghouse, J.; Reyes, D.; Sermanet, P.; Siev-\ners, N.; Tan, C.; Toshev, A.; Vanhoucke, V .; Xia, F.; Xiao, T.;\nXu, P.; Xu, S.; Yan, M.; and Zeng, A. 2022. Do As I Can,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20131\nNot As I Say: Grounding Language in Robotic Affordances.\narXiv:2204.01691.\nBonet, B.; and Geffner, H. 2001. Planning as heuristic\nsearch. Artificial Intelligence, 129(1-2): 5–33.\nBrohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y .; Chen,\nX.; Choromanski, K.; Ding, T.; Driess, D.; Dubey, A.; Finn,\nC.; Florence, P.; Fu, C.; Arenas, M. G.; Gopalakrishnan, K.;\nHan, K.; Hausman, K.; Herzog, A.; Hsu, J.; Ichter, B.; Irpan,\nA.; Joshi, N.; Julian, R.; Kalashnikov, D.; Kuang, Y .; Leal, I.;\nLee, L.; Lee, T.-W. E.; Levine, S.; Lu, Y .; Michalewski, H.;\nMordatch, I.; Pertsch, K.; Rao, K.; Reymann, K.; Ryoo, M.;\nSalazar, G.; Sanketi, P.; Sermanet, P.; Singh, J.; Singh, A.;\nSoricut, R.; Tran, H.; Vanhoucke, V .; Vuong, Q.; Wahid, A.;\nWelker, S.; Wohlhart, P.; Wu, J.; Xia, F.; Xiao, T.; Xu, P.; Xu,\nS.; Yu, T.; and Zitkovich, B. 2023. RT-2: Vision-Language-\nAction Models Transfer Web Knowledge to Robotic Con-\ntrol. arXiv:2307.15818.\nChevalier-Boisvert, M.; Bahdanau, D.; Lahlou, S.; Willems,\nL.; Saharia, C.; Nguyen, T. H.; and Bengio, Y . 2019.\nBabyAI: First Steps Towards Grounded Language Learning\nWith a Human In the Loop. In International Conference on\nLearning Representations, volume 105.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-\ndus, W.; Li, Y .; Wang, X.; Dehghani, M.; Brahma, S.; Web-\nson, A.; Gu, S. S.; Dai, Z.; Suzgun, M.; Chen, X.; Chowd-\nhery, A.; Castro-Ros, A.; Pellat, M.; Robinson, K.; Val-\nter, D.; Narang, S.; Mishra, G.; Yu, A.; Zhao, V .; Huang,\nY .; Dai, A.; Yu, H.; Petrov, S.; Chi, E. H.; Dean, J.; De-\nvlin, J.; Roberts, A.; Zhou, D.; Le, Q. V .; and Wei, J.\n2022. Scaling Instruction-Finetuned Language Models.\narXiv:2210.11416.\nDettmers, T.; Lewis, M.; Belkada, Y .; and Zettlemoyer, L.\n2022. LLM.int8(): 8-bit Matrix Multiplication for Trans-\nformers at Scale. arXiv:2208.07339.\nDettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer,\nL. 2023. QLoRA: Efficient Finetuning of Quantized LLMs.\narXiv:2305.14314.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nDing, Y .; Zhang, X.; Amiri, S.; Cao, N.; Yang, H.; Kamin-\nski, A.; Esselink, C.; and Zhang, S. 2023. Integrating action\nknowledge and LLMs for task planning and situation han-\ndling in open worlds. Autonomous Robots, 47(8): 981–997.\nDu, Y .; Liu, Z.; Li, J.; and Zhao, W. X. 2022. A Survey of\nVision-Language Pre-Trained Models. arXiv:2202.10936.\nFrantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh, D. 2023.\nGPTQ: Accurate Post-Training Quantization for Generative\nPre-trained Transformers. arXiv:2210.17323.\nGolowich, N.; Moitra, A.; and Rohatgi, D. 2022. Plan-\nning in Observable POMDPs in Quasipolynomial Time.\narXiv:2201.04735.\nHao, S.; Gu, Y .; Ma, H.; Hong, J. J.; Wang, Z.; Wang, D. Z.;\nand Hu, Z. 2023. Reasoning with Language Model is Plan-\nning with World Model. arXiv:2305.14992.\nHelmert, M. 2006. The fast downward planning system.\nJournal of Artificial Intelligence Research, 26: 191–246.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022a.\nLanguage models as zero-shot planners: Extracting action-\nable knowledge for embodied agents. In International Con-\nference on Machine Learning, 9118–9147. PMLR.\nHuang, W.; Xia, F.; Shah, D.; Driess, D.; Zeng, A.; Lu,\nY .; Florence, P.; Mordatch, I.; Levine, S.; Hausman, K.;\nand Ichter, B. 2023. Grounded Decoding: Guiding Text\nGeneration with Grounded Models for Embodied Agents.\narXiv:2303.00855.\nHuang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Florence,\nP.; Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar, Y .; Ser-\nmanet, P.; Brown, N.; Jackson, T.; Luu, L.; Levine, S.; Haus-\nman, K.; and Ichter, B. 2022b. Inner Monologue: Em-\nbodied Reasoning through Planning with Language Models.\narXiv:2207.05608.\nKaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1998.\nPlanning and acting in partially observable stochastic do-\nmains. Artificial intelligence, 101(1-2): 99–134.\nKwon, M.; Xie, S. M.; Bullard, K.; and Sadigh, D. 2023.\nReward Design with Language Models. In The Eleventh\nInternational Conference on Learning Representations.\nLakhotia, K.; Kharitonov, E.; Hsu, W.-N.; Adi, Y .; Polyak,\nA.; Bolte, B.; Nguyen, T.-A.; Copet, J.; Baevski, A.; Mo-\nhamed, A.; and Dupoux, E. 2021. On Generative Spoken\nLanguage Modeling from Raw Audio. Transactions of the\nAssociation for Computational Linguistics, 9: 1336–1354.\nLiang, J.; Huang, W.; Xia, F.; Xu, P.; Hausman, K.; Ichter,\nB.; Florence, P.; and Zeng, A. 2023. Code as Poli-\ncies: Language Model Programs for Embodied Control.\narXiv:2209.07753.\nLiao, Y .-H.; Puig, X.; Boben, M.; Torralba, A.; and Fidler, S.\n2019. Synthesizing Environment-Aware Activities via Ac-\ntivity Sketches. In 2019 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 6284–6292.\nLin, K.; Agia, C.; Migimatsu, T.; Pavone, M.; and Bohg, J.\n2023. Text2Motion: from natural language instructions to\nfeasible plans. Autonomous Robots, 47(8): 1345–1365.\nLiu, B.; Jiang, Y .; Zhang, X.; Liu, Q.; Zhang, S.; Biswas,\nJ.; and Stone, P. 2023. LLM+P: Empowering Large\nLanguage Models with Optimal Planning Proficiency.\narXiv:2304.11477.\nPallagani, V .; Muppasani, B.; Murugesan, K.; Rossi, F.;\nHoresh, L.; Srivastava, B.; Fabiano, F.; and Loreggia, A.\n2022. Plansformer: Generating Symbolic Plans using Trans-\nformers. arXiv:2212.08681.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20132\nPuig, X.; Ra, K.; Boben, M.; Li, J.; Wang, T.; Fidler, S.;\nand Torralba, A. 2018. Virtualhome: Simulating household\nactivities via programs. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, 8494–\n8502.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1): 5485–5551.\nSilver, T.; Hariprasad, V .; Shuttleworth, R. S.; Kumar, N.;\nLozano-P´erez, T.; and Kaelbling, L. P. 2022. PDDL Plan-\nning with Pretrained Large Language Models. In NeurIPS\n2022 Foundation Models for Decision Making Workshop.\nSingh, I.; Blukis, V .; Mousavian, A.; Goyal, A.; Xu, D.;\nTremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2023.\nProgPrompt: Generating Situated Robot Task Plans using\nLarge Language Models. In International Conference on\nRobotics and Automation (ICRA).\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,\nG. 2023. LLaMA: Open and Efficient Foundation Language\nModels. arXiv:2302.13971.\nValmeekam, K.; Olmo, A.; Sreedharan, S.; and Kambham-\npati, S. 2022. Large Language Models Still Can’t Plan\n(A Benchmark for LLMs on Planning and Reasoning about\nChange). In NeurIPS 2022 Foundation Models for Decision\nMaking Workshop.\nValmeekam, K.; Sreedharan, S.; Marquez, M.; Olmo, A.;\nand Kambhampati, S. 2023. On the Planning Abilities of\nLarge Language Models (A Critical Investigation with a Pro-\nposed Benchmark). arXiv:2302.06706.\nvan den Oord, A.; Li, Y .; and Vinyals, O. 2019. Rep-\nresentation Learning with Contrastive Predictive Coding.\narXiv:1807.03748.\nWang, Y .; Wang, W.; Joty, S.; and Hoi, S. C. 2021. CodeT5:\nIdentifier-aware Unified Pre-trained Encoder-Decoder Mod-\nels for Code Understanding and Generation. In Moens, M.-\nF.; Huang, X.; Specia, L.; and Yih, S. W.-t., eds., Proceed-\nings of the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, 8696–8708. Online and Punta\nCana, Dominican Republic: Association for Computational\nLinguistics.\nXie, Y .; Yu, C.; Zhu, T.; Bai, J.; Gong, Z.; and Soh, H. 2023.\nTranslating Natural Language to Planning Goals with Large-\nLanguage Models. arXiv:2302.05128.\nYao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.;\nCao, Y .; and Narasimhan, K. 2023. Tree of Thoughts:\nDeliberate Problem Solving with Large Language Models.\narXiv:2305.10601.\nZeng, A.; Florence, P.; Tompson, J.; Welker, S.; Chien, J.;\nAttarian, M.; Armstrong, T.; Krasin, I.; Duong, D.; Sind-\nhwani, V .; and Lee, J. 2021. Transporter Networks: Rear-\nranging the Visual World for Robotic Manipulation. In Pro-\nceedings of the 2020 Conference on Robot Learning, volume\n155 of Proceedings of Machine Learning Research , 726–\n747. PMLR.\nZiegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Rad-\nford, A.; Amodei, D.; Christiano, P.; and Irving, G. 2020.\nFine-Tuning Language Models from Human Preferences.\narXiv:1909.08593.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20133"
}