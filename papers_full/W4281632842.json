{
  "title": "Bridging the Gap Between Qualitative and Quantitative Assessment in Science Education Research with Machine Learning — A Case for Pretrained Language Models-Based Clustering",
  "url": "https://openalex.org/W4281632842",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1775197432",
      "name": "Peter Wulff",
      "affiliations": [
        "Heidelberg University",
        "Heidelberg University of Education"
      ]
    },
    {
      "id": "https://openalex.org/A2547154620",
      "name": "David Buschhüter",
      "affiliations": [
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A2524623631",
      "name": "Andrea Westphal",
      "affiliations": [
        "Universität Greifswald"
      ]
    },
    {
      "id": "https://openalex.org/A4225349385",
      "name": "Lukas Mientus",
      "affiliations": [
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A2009062913",
      "name": "Anna Nowak",
      "affiliations": [
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A2001169298",
      "name": "Andreas Borowski",
      "affiliations": [
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A1775197432",
      "name": "Peter Wulff",
      "affiliations": [
        "Heidelberg University",
        "Heidelberg University of Education"
      ]
    },
    {
      "id": "https://openalex.org/A2547154620",
      "name": "David Buschhüter",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2524623631",
      "name": "Andrea Westphal",
      "affiliations": [
        "Universität Greifswald"
      ]
    },
    {
      "id": "https://openalex.org/A4225349385",
      "name": "Lukas Mientus",
      "affiliations": [
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A2009062913",
      "name": "Anna Nowak",
      "affiliations": [
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A2001169298",
      "name": "Andreas Borowski",
      "affiliations": [
        "University of Potsdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3084879937",
    "https://openalex.org/W3096556630",
    "https://openalex.org/W2793540861",
    "https://openalex.org/W3164886736",
    "https://openalex.org/W2084341220",
    "https://openalex.org/W2054848155",
    "https://openalex.org/W6684012063",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W151377110",
    "https://openalex.org/W3039916702",
    "https://openalex.org/W3026838079",
    "https://openalex.org/W4248135386",
    "https://openalex.org/W1527793953",
    "https://openalex.org/W2141998888",
    "https://openalex.org/W2086245846",
    "https://openalex.org/W2072653842",
    "https://openalex.org/W2606089314",
    "https://openalex.org/W6730267373",
    "https://openalex.org/W2103018059",
    "https://openalex.org/W2046220304",
    "https://openalex.org/W1125436246",
    "https://openalex.org/W4254764785",
    "https://openalex.org/W1520341355",
    "https://openalex.org/W2416532294",
    "https://openalex.org/W1501215627",
    "https://openalex.org/W1987111416",
    "https://openalex.org/W4243989635",
    "https://openalex.org/W2500924415",
    "https://openalex.org/W3106056336",
    "https://openalex.org/W2150762032",
    "https://openalex.org/W2789838629",
    "https://openalex.org/W1995986722",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3110648930",
    "https://openalex.org/W2962559713",
    "https://openalex.org/W2074360952",
    "https://openalex.org/W3004067956",
    "https://openalex.org/W3160589955",
    "https://openalex.org/W2102867679",
    "https://openalex.org/W3208861205",
    "https://openalex.org/W3087297528",
    "https://openalex.org/W2003348568",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2055795362",
    "https://openalex.org/W2102611807",
    "https://openalex.org/W2094198922",
    "https://openalex.org/W1899755636",
    "https://openalex.org/W2140369176",
    "https://openalex.org/W2993075266",
    "https://openalex.org/W2033015875",
    "https://openalex.org/W4285530062",
    "https://openalex.org/W2100414000",
    "https://openalex.org/W2911994647",
    "https://openalex.org/W1892968353",
    "https://openalex.org/W6947842279",
    "https://openalex.org/W2042690899",
    "https://openalex.org/W2913944258",
    "https://openalex.org/W3091838623",
    "https://openalex.org/W4225279564",
    "https://openalex.org/W3012226304",
    "https://openalex.org/W1848614604",
    "https://openalex.org/W3126187178",
    "https://openalex.org/W3091828467",
    "https://openalex.org/W3012071850",
    "https://openalex.org/W4212865381",
    "https://openalex.org/W3100819463",
    "https://openalex.org/W4214929020",
    "https://openalex.org/W2163028944",
    "https://openalex.org/W182831726",
    "https://openalex.org/W4214725210"
  ],
  "abstract": "Abstract Science education researchers typically face a trade-off between more quantitatively oriented confirmatory testing of hypotheses, or more qualitatively oriented exploration of novel hypotheses. More recently, open-ended, constructed response items were used to combine both approaches and advance assessment of complex science-related skills and competencies. For example, research in assessing science teachers’ noticing and attention to classroom events benefitted from more open-ended response formats because teachers can present their own accounts. Then, open-ended responses are typically analyzed with some form of content analysis. However, language is noisy, ambiguous, and unsegmented and thus open-ended, constructed responses are complex to analyze. Uncovering patterns in these responses would benefit from more principled and systematic analysis tools. Consequently, computer-based methods with the help of machine learning and natural language processing were argued to be promising means to enhance assessment of noticing skills with constructed response formats. In particular, pretrained language models recently advanced the study of linguistic phenomena and thus could well advance assessment of complex constructs through constructed response items. This study examines potentials and challenges of a pretrained language model-based clustering approach to assess preservice physics teachers’ attention to classroom events as elicited through open-ended written descriptions. It was examined to what extent the clustering approach could identify meaningful patterns in the constructed responses, and in what ways textual organization of the responses could be analyzed with the clusters. Preservice physics teachers ( N = 75) were instructed to describe a standardized, video-recorded teaching situation in physics. The clustering approach was used to group related sentences. Results indicate that the pretrained language model-based clustering approach yields well-interpretable, specific, and robust clusters, which could be mapped to physics-specific and more general contents. Furthermore, the clusters facilitate advanced analysis of the textual organization of the constructed responses. Hence, we argue that machine learning and natural language processing provide science education researchers means to combine exploratory capabilities of qualitative research methods with the systematicity of quantitative methods.",
  "full_text": "https://doi.org/10.1007/s10956-022-09969-w\nBridging the Gap Between Qualitative and Quantitative Assessment \nin Science Education Research with Machine Learning — A Case \nfor Pretrained Language Models‑Based Clustering\nPeter Wulff1  · David Buschhüter2 · Andrea Westphal3 · Lukas Mientus2 · Anna Nowak2 · Andreas Borowski2\nAccepted: 2 May 2022 \n© The Author(s) 2022\nAbstract\nScience education researchers typically face a trade-off between more quantitatively oriented confirmatory testing of hypoth-\neses, or more qualitatively oriented exploration of novel hypotheses. More recently, open-ended, constructed response items \nwere used to combine both approaches and advance assessment of complex science-related skills and competencies. For \nexample, research in assessing science teachers’ noticing and attention to classroom events benefitted from more open-ended \nresponse formats because teachers can present their own accounts. Then, open-ended responses are typically analyzed with \nsome form of content analysis. However, language is noisy, ambiguous, and unsegmented and thus open-ended, constructed \nresponses are complex to analyze. Uncovering patterns in these responses would benefit from more principled and systematic \nanalysis tools. Consequently, computer-based methods with the help of machine learning and natural language processing \nwere argued to be promising means to enhance assessment of noticing skills with constructed response formats. In particular, \npretrained language models recently advanced the study of linguistic phenomena and thus could well advance assessment \nof complex constructs through constructed response items. This study examines potentials and challenges of a pretrained \nlanguage model-based clustering approach to assess preservice physics teachers’ attention to classroom events as elicited \nthrough open-ended written descriptions. It was examined to what extent the clustering approach could identify meaningful \npatterns in the constructed responses, and in what ways textual organization of the responses could be analyzed with the \nclusters. Preservice physics teachers (N = 75) were instructed to describe a standardized, video-recorded teaching situation \nin physics. The clustering approach was used to group related sentences. Results indicate that the pretrained language model-\nbased clustering approach yields well-interpretable, specific, and robust clusters, which could be mapped to physics-specific \nand more general contents. Furthermore, the clusters facilitate advanced analysis of the textual organization of the constructed \nresponses. Hence, we argue that machine learning and natural language processing provide science education researchers \nmeans to combine exploratory capabilities of qualitative research methods with the systematicity of quantitative methods.\nKeywords Attention to classroom events · Noticing · NLP · ML\n\"You can have data without \ninformation, but you cannot \nhave information without data.\" \n(Daniel Keys Moran)\nResearch methods in science education are commonly dif -\nferentiated into quantitative and qualitative methods (Krüger \net al., 2014). The former allow for the confirmatory testing \nof statistical hypotheses, whereas the latter allow for more \nexploratory generation of novel hypotheses. This division \nis artificial and attributes to the imperfect capabilities of \nmodeling complex systems that involve learning processes \nof humans. It would be desirable to better integrate both \nmethods and conserve the predictive capabilities of quanti -\ntative methods and the exploratory capabilities of qualita -\ntive methods. It has been suggested that complex algorith -\nmic approaches such as machine learning can better model \nassessment in science education (Breiman, 2001; Zhai, \n * Peter Wulff \n peter.wulff@ph-heidelberg.de\n1 Physics Education Research Group, Heidelberg University \nof Education, Heidelberg, Germany\n2 Physics Education Research Group, University of Potsdam, \nPotsdam, Germany\n3 Department of Educational Research, University \nof Greifswald, Greifswald, Germany\n/ Published online: 1 June 2022\nJournal of Science Education and Technology (2022) 31:490–513\n1 3\n2021) and eventually provide a new methods paradigm. \n“Machine learning is about inductively solving problems \nby machines, i.e., computers.” (Rauf, 2021, p.8). Induc -\ntive learning requires appropriate data for the machines to \nimprove on relevant tasks. Given advances in data storage \nand accessibility, machine learning (ML) models dramati -\ncally improved their performance on many tasks such as \nimage classification, or spoken and written language ana -\nlytics (Goodfellow et al., 2016; Goldberg, 2017). Scholars \nin the fields of education and discipline-based educational \nresearch also argued that ML methods can advance educa -\ntional research (Singer, 2019; Baig et al., 2020), even “revo-\nlutionize” assessments (Zhai et al., 2020). Among others, the \neducation sector presents a field where datasets of unprec -\nedented size become available (Baig et al., 2020).\nML methods have been utilized in science education \nresearch in different contexts. Mostly, science education \nresearchers employed supervised ML methods where a \nmodel is trained to map responses to predefined outputs \n(Zhai et al., 2020). However, oftentimes problems in science \neducation research are less well defined and only small data-\nsets can be collected with reasonable effort. For example, in \nresearch on university-based teacher education such as notic-\ning and attention to classroom events, typically small sam -\nples are available (Chan et al., 2021; Wilson et al., 2019). \nNoticing, among others, comprises the careful observation of \nevents in a teaching situation. In science education research \nit has been highlighted that preservice science and math -\nematics teachers attend to many different events and contents \nin a teaching situation (Talanquer et al., 2015). To capture \nthe complexity of noticing, science education researchers \ntherefore used open-ended, constructed response formats to \nassess noticing and attention to classroom events (Barth-\nCohen et al., 2018; Luna et al., 2018; Chan et al., 2021). \nThe responses are then analyzed with some form of content \nanalysis. However, not only do the differences in attention \nbetween the teachers yield to the complexity of assessing \nnoticing and attention processes, but also the teachers’ use of \nlanguage in constructed-response items. Language use was \ncharacterized to be “noisy, ambiguous, und unsegmented” \n(Jurafsky, 2003, p.39). Hence, probabilistic approaches are \nrequired to analyze language-related processes and products. \nA probabilistic approach that also captures complexity is \nML. The application of ML-based modeling could provide \nresearchers means to gain novel insights into these complex \nconstructs (Zhai et al., 2020). Yet, it is not clear in what ways \nML-based approaches can be utilized to identify meaningful \npatterns in teachers’ constructed responses with respect to \nnoticing and attention to classroom events.\nIn the present study we therefore evaluate potentials \nand challenges of using a pretrained language model-based \nclustering approach to analyze preservice physics teach -\ners’ open-ended, constructed responses in the context of \ndescribing a standardized teaching situation. We critically \nexamine to what extent the application of ML in our research \ncontext can bridge the divide between quantitative and quali-\ntative methods and provide a more integrative approach.\nUtilizing NLP and ML to Model Complex \nDataset\nApplications with ML and natural language processing \n(NLP) attracted a lot of interest in the field of science edu -\ncation research (Zhai et al., 2020). ML refers to comput -\ners’ inductive problem solving based on data (Zhai, 2021; \nRauf, 2021). Two major types of ML are supervised and \nunsupervised ML (Jordan & Mitchell, 2015). In supervised \nML, human-annotated data are provided for the models to \nlearn a mapping from input to output in order to classify or \npredict unseen data (Marsland, 2015). Unsupervised ML, on \nthe other hand, encompasses algorithms to reduce complex \ndatasets and extract patterns in them. Both types of ML can \nbe used to analyze natural language. The study of natural \nlanguage by means of computers is called NLP. NLP refers \nto the systematic and structured processing of natural lan -\nguage data. Natural language can be contrasted with artifi -\ncial language such as programming languages or mathemat-\nics which are more aligned with formal logic. The attribute \n“natural” relates to the fact that this form of language can be \ncharacterized to be “noisy, ambiguous, and unsegmented” \n(Jurafsky, 2003). It has been argued that it is not possible \nto specify clear-cut rules for natural language (i.e., a gram -\nmar) that explain phenomena of language comprehension \nand production: “we can’t reduce what we want to say to the \nfree combination of a few abstract primitives” (Halevy et al., \n2009, p. 9). Hence, probabilistic approaches such as ML \nmethods are increasingly incorporated into NLP research \nin addition to rule-based approaches, given the capacity of \nprobabilistic approaches such as ML to systematically pro -\ncess complex language data, extract patterns in it and clas -\nsify instances of language use (Goldberg, 2017).\nML research experienced a new spring with the suc -\ncessful application of deep neural networks to learn input-\noutput mappings that outperformed more simple (shallow) \nML models in most tasks in image and language analysis \n(Goodfellow et al., 2016). A heuristic in ML research states \nthat problems which are easy for humans are difficult for \nmachines to solve such as character recognition or speech \nperception (Goodfellow et al., 2016). Simple ML algorithms \nlike logistic regression excelled in problems where the input \nrepresentation through features is particularly informative, \ne.g., the age of a student. The selection and engineering of \ninputs typically requires efforts for the human researcher, \nbecause data are not typically represented in this aggregated \nform in real-world contexts. Simple ML models would lose \n491Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nperformance when more complex data such as images or \nlanguage form the input (Goodfellow et al., 2016). Deep \nneural networks have been found to be capable of represent-\ning the input as part of the modeling, which allowed ML and \nNLP researchers to apply these models to problems where \ncomplex data has to be represented in the first place. Thus, \nhuman feature selection and engineering is partly replaced \nby automated feature representation in the deep neural net -\nwork approaches oftentimes with the loss of interpretability \nof the model decisions.\nA major facilitator for the deep learning revolution in the \nlast decades was the availability of annotated data. For once, \nresearchers spend tremendous efforts to annotate data manu-\nally in order to train deep neural networks that are capable of \nlanguage comprehension and production, or image classifi -\ncation. For the now famous ImageNet competition, research-\ners manually labeled over three million images in two years \nwith the help of crowdsourcing (Mitchell, 2020). Similar \nefforts have been undertaken in NLP. To advance language \ntranslation, ML researchers were fortunate to find annotated \ndatasets from the cold war where translations were important \nfor intelligence or from the European Parliament that con -\nsists of many different nations (Mitchell, 2020). However, \ncurating and annotating these datasets captures resources \nthat are not widely available such as money and compute \ntime. Consequently, for most researchers in domains like \nscience education no such well-developed datasets will be \navailable for their specific research questions.\nHowever, the ML paradigm of transfer learning that \nbecame important with increasingly complex deep neural net-\nworks (Devlin et al., 2018) might solve this problem. Transfer \nlearning enables sharing of previously trained ML models for \ndifferent tasks (Ruder, 2019). Much as humans learn language \nfrom experiences, feedback and reinforcement (Bruner, 1985) \nand build on learned structures (Rumelhart et al., 1986), the \nparadigm of transfer learning posits that prior trained weights \nin a given context can be further used to improve model per-\nformance in different contexts/domains and with different \ntasks (Ruder, 2019). NLP researchers used transfer learning \nin the context of language modeling. While in image process-\ning models are oftentimes pretrained on the ImageNet dataset \nto improve downstream performance (Devlin et al., 2018), \nlanguage models in NLP research can be trained on corpora \nsuch as the Internet or Wikipedia (Devlin et al., 2018; Ruder, \n2019). NLP researchers then pretrain language models that \nare capable of representing language in a way that researchers \ncan use in downstream tasks (Mikolov et al., 2013). Typically, \nthese language models are trained with the objective to sim-\nply predict context words. The pretrained language models \ncan then be used to generate an informative representation of \nlanguage to enhance task performance (Mikolov et al., 2013; \nMikolov et al., 2013; Devlin et al., 2018).\nModeling Unstructured Data in Science \nEducation Research with NLP and ML\n“Perhaps when it comes to natural language processing and \nrelated fields [that model human behavior], we’re doomed to \ncomplex theories that will never have the elegance of phys-\nics equations” (Halevy et al., 2009, p.8). The “unreasonable \neffectiveness” (Wigner, 1960) of mathematics has been rec-\nognized for physics; however, educational sciences are far \nfrom having theories in this elegant formulation—given the \ncomplexity of the involved problems. In this context, NLP \nand ML have probably much to offer for these fields where \ncomplex theories prevail. Yet, especially more sophisticated \nNLP and ML applications such as deep neural networks \nmight pose unfulfillable requirements on required size of \ntraining datasets and model implementation to be useful for \nscience education research. The size of the training dataset \nshould be judged against the complexity of the task and the \ncomplexity of the ML model. While the review by Zhai \net al. (2020) shows that typical applications of NLP and ML \nin science education research comprise fewer than 30k train-\ning samples, the reviewed studies exclusively focus on sim-\npler ML models such as logistic regression, support-vector \nmachines, or naive Bayes. More generally, data collection \nin domains such as science education is costly and time-\nconsuming, because large coordination efforts are necessary \nto recruit enough subjects. There are literally no studies \nin science education where millions of subjects have been \ncollected that comprise datasets that seem to be required to \ntrain more general-purpose deep learning models. Does this \nimply that particularly the more complex ML methods are \nnot applicable for science education researchers?\nFor supervised ML this hypothesis has been refuted in \nsome science education research contexts. Wulff et al. (2022) \ncould show that pretrained language models improve clas -\nsification performance for discourse elements with preser -\nvice physics teachers’ written reflections. The findings in this \nstudy suggest that complex ML models that are trained from \nscratch can reach classification accuracy of simpler ML mod-\nels. Furthermore, the authors show that utilizing pretrained \nweights for the complex models enhances classification \naccuracy and generalizability further. Carpenter et al. (2020) \nshowed that deep contextualized embeddings from pretrained \nlanguage models could improve prediction of students’ reflec-\ntive depth in a biology learning context. These findings but-\ntress the applicability of complex ML models such as deep \nneural networks as facilitators for supervised ML. These stud-\nies, however, do not suggest that training the more performant \ndeep learning models from scratch is possible with the avail-\nable science education datasets. Furthermore, it is not clear \nfrom these studies to what extent pretrained language models \ncould be used to extract patterns in the datasets.\n492 Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nPrior research on pattern extraction from unstructured data \nwith simpler unsupervised ML models and larger datasets in \neducation and science education contexts focused on standard-\nized documents such as dissertation or conference abstracts. \nMunoz-Najar Galvez et al. (2020) established a data-driven \nway to systematically analyze the field of education research. \nThey identified paradigm shifts in education research on the \nbasis of 137,024 dissertation abstracts, reconstructing a shift \nfrom an outcome-oriented paradigm to an interpretative para-\ndigm. In science education research, Odden et al. (2020) used \nlatent Dirichlet allocation (LDA), a generative probabilistic \ntopic model, to analyze all papers that were extracted from \nthe Physics Education Research Conference Proceedings from \n2001 to 2018 (overall 1,302 papers). They outline shifts in \nthe paper’s topics in the conference over time. Despite the \npotentials of LDA to summarize occurring research topics \nand trends over time, the authors recognize some shortcom-\nings with this algorithm. For example, the LDA model groups \ntogether segments that use similar vocabulary. However, the \nsegments might differ in meaning anyways (see also: Odden \net al., 2021). Other researchers could show that simpler unsu-\npervised ML methods could also be used to explore patterns \nin comparably smaller datasets in science education. Sherin \n(2013) used a vector space model and a hierarchical agglom-\nerative clustering algorithm to identify students’ science \nexplanations in interview transcripts. He showed the general \napplicability of these NLP-based methods in this context, but \ncontends that the algorithms could not account for word order-\ning effects. He also mentions the desire to more systematically \nextract the number of topics that are likely present in the data \n(see also: Xing et al., 2020). Also Rosenberg and Krist (2020) \nsuccessfully applied an unsupervised clustering algorithm to \nassess students’ considerations of generality in science (see \nalso: Xing et al., 2020; Zehner et al., 2016).\nA domain of research in science education where NLP \nand ML in unsupervised contexts has not yet been applied \nwidely is university-based science teacher education. In \nfact, no reviewed study in Zhai et al. ( 2020) engaged in \nuniversity-based educational research. Besides supervised \nML approaches in university-based science teacher educa -\ntion that have been occasionally applied (Wulff et al., 2020), \nunsupervised approaches could facilitate researchers and \ninstructors novel insights into relevant constructs because \nthey can explore patterns in unstructured data (Halevy et al., \n2009; Hao, 2019).\nScience Teachers’ Noticing of Classroom \nEvents\nTeachers face the challenge to professionally act in uncer -\ntain situations (Clifton & Roberts, 1993; von Aufschnaiter \net al., 2019; Chan et al., 2021). Learning to professionally \nact in uncertain situations requires teachers to develop \nthe capacity to reflect on their teaching experiences \n(Korthagen, 1999). An important part of reflective com -\npetencies are noticing skills that relate to perceptual and \ncognitive thinking processes (Chan et al., 2021). In par -\nticular, noticing comprises observation, interpretation, and \nreasoning about learning-relevant events in classrooms \n(Sherin & van Es, 2009; van Es & Sherin, 2002a ; Chan \net al., 2021; Furtak, 2012). Van Es & Sherin (2002) define \nnoticing with regard to three key aspects: “(a) identify -\ning what is important or noteworthy about a classroom \nsituation; (b) making connections between the specifics \nof classroom interactions and the broader principles of \nteaching and learning they represent; and (c) using what \none knows about the context to reason about classroom \ninteractions.” (p. 573) Noticing research has documented \nthe difficulties that novice and even expert teachers have \nto direct their attention and notice relevant classroom \nevents (Sherin & Han, 2004; Chan et al., 2021; Talanquer  \net al., 2015 ; Levin et al., 2009 ; Roth et al., 2011 ). For \nexample, novice science and mathematics teachers strug -\ngle to attend to student thinking and the substance of what \nthey are saying (Sherin & Han, 2004; Hammer & van Zee, \n2006), and tend to strive for quick and conclusive infer -\nences that are right or wrong, rather than tentative inter -\npretations (Crespo, 2000 ). This strand of research also \nshowed that science teachers provide more general evalu -\nations as compared to more specific accounts of student \nunderstanding (Hammer & van Zee, 2006). Mathematics \nand science education scholars generally highlighted the \ncomplexity of the noticing construct (Chan et al., 2021; \nTalanquer et al., 2015). Talanquer et al. (2015) summarize \nthe noticing foci of teachers as: “the object of noticing \n(e.g., student actions, student thinking), the noticing stance \n(e.g., evaluative, interpretive), the specificity of noticing \n(e.g., specific student, whole class), and the noticing focus \n(e.g., specific concept, general topic)” (p. 587). To design \nauthentic learning opportunities for mathematics and sci -\nence teachers to enhance noticing skills, valid, reliable, \nand scalable assessment of attention to classroom events \nand noticing is necessary.\nTo assess noticing and attention to classroom events, \nscience education researchers increasingly embraced con -\nstructed response items, e.g., open-ended, free-recall writ -\nten responses (Barth-Cohen et al., 2018; Luna et al., 2018; \nTalanquer et al., 2015; Chan et al., 2021). Open response \nitems have been argued to allow a more authentic examina-\ntion of teachers’ professional competencies as compared \nto more closed-form questions (Nehm et al., 2012; Zhai, \n2021). Many of the noticing research then seeks to analyze \ninductively what teachers are noticing (Chan et al., 2021). \nHowever, the mere linguistic complexity of the constructed \nresponses (noisy, ambiguous, and unsegmented) and the \n493Journal of Science Education and Technology  (2022) 31:490–513\n1 3\ncomplexity of the noticing construct make it challeng -\ning to integrate all information in the responses and infer \nthe noticing skills. From their review on teacher noticing \nresearch in science education, Chan et al. ( 2021) conclude \nthat “methodological trade-offs between different ways of \ninvestigating teacher noticing need to be better explored” \n(p. 37). We suggest that ML-based methods can provide \nnovel means to analyze teachers’ responses inductively “to \nunderstand what teachers notice” (Chan et al., 2021, p.34). \nThus, ML methods potentially help researchers to gather \n‘knowledge of teachers’ (Fenstermacher, 1994). We also \nconcur with Lamb et al. ( 2021) that ML models are pow -\nerful tools to advance algorithmic understanding of rel -\nevant underlying cognitive processes that can explain the \nprocess and products of writing. Zhai et al. ( 2020) argued \nthat ML models can particularly advance understanding \nand assessment of complex constructs such as noticing \nand provide means to automate assessment and feedback. \nConsequently, this study examines potentials and chal -\nlenges of an ML-based clustering approach when applied \nin the context of assessing noticing of classroom events \nfor preservice science teachers.\nResearch Questions\nNoticing or directing attention to relevant classroom events \nis highly relevant for mathematics and science teachers and, \nthus, plays an important role in science education research. \nAttention to classroom events and contents played a particu-\nlarly important role in mathematics and science education \nresearch. Star and Strickland (2008) suggested that noticing \nresearch should focus particularly on what catches teachers’ \nattention and what is missed. 25 of the 26 science education \nstudies reviewed by Chan et al. ( 2021) considered atten -\ntion to classroom events as an essential aspect of noticing; \n11 studies even restricted noticing to attention. Attention \nto classroom events has often been studied through video \nclips that present teachers with a standardized teaching situ-\nation and are typically followed by some form of eliciting \nteachers’ observations (Zhai, 2021; van Es & Sherin, 2002a; \nSeidel & Stürmer, 2014; Putnam & Borko, 2000; Darling-\nHammond, 2000; Kleinknecht & Gröschner, 2016; Sherin \n& van Es, 2009).\nNoticing research can be characterized as a context where \nit seems to be notoriously difficult to recruit large sample \nsizes, rendering quantitative research methods difficult to \napply. Reviews suggest that studies typically comprise small \nsamples of up to 241 teachers (Wilson et al., 2019; Chan et al.,  \n2021). This restricts researchers to using mostly qualitative \nmethods with some form of content analysis (Wilson et al., \n2019; Chan et al., 2021; Talanquer et al., 2015). As such, it  \nis important to examine to what extent ML-based approaches \ncan be utilized in this context as a means to advance quantifi-\nable hypotheses. Particularly, pretrained language models \ncan improve the ML methods to be more robust with small \nsamples. Hence, we ask the following overarching research \nquestion: To what extent and in what ways can a pretrained \nlanguage model-based clustering approach extract meaning-\nful patterns in preservice physics teachers’ written descrip -\ntions of a teaching situation?\nIn the context of RQ1, we analyze the validity of the \nextracted clusters:\n• RQ1: To what extend can a pretrained language model-\nbased clustering approach extract interpretable (RQ1a), \nspecific (RQ1b), and robust (RQ1c) clusters in the pre -\nservice physics teachers’ written descriptions of a teach-\ning situation?\nWe then examined ways in which these clusters provide \ninsights into the composition of the written descriptions. \nvan Es and Sherin ( 2002) used the concept of analytical \nchunks in their noticing research, referring to experts’ ten -\ndency to organize their essays more coherently in reference \nto teaching and learning principles. Based on this concept \nof analytical chunks, we hypothesize that the analysis of \ninterconnections between the clusters in the teachers’ writ -\nten descriptions provides tools to develop a more quanti -\ntative understanding of chunks in the writing. To analyze \nthe organization of the teachers’ written descriptions based \non the extracted clusters, we explored dependencies among \nclusters:\n• RQ2: What kinds of dependencies with respect to textual \norganization can be analyzed based on the extracted clus-\nters?\nMethod\nWritten Descriptions of a Video‑Recorded Teaching \nSituation in Physics\nIn the present study preservice physics teachers’ were \ngiven the instruction to describe, evaluate and reason \nabout a video-recorded lesson which presented the teach -\ners an authentic teaching situation in a 9th grade physics \nclassroom held by an in-service physics teacher. Overall, \nthe teaching goal of the observed lesson was to introduce \ninfluencing factors on the movement of falling objects and \nthe definition of free fall. Table  2 outlines the chronologi -\ncal order of events in the teaching situation. The teaching \nsituation can be broadly divided into two phases. In the \nfirst phase, the teacher performed several experiments with \nfalling objects (two masses, and a vacuum tube with screw \n494 Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nand feather). The students posed hypotheses on the outcome \nof the experiments (e.g., which of the two masses of dif -\nferent weight will hit the floor first. In the second phase, \nthe teacher provided the definition of free fall and students \ndevised experiments to investigate what type of movement \nfree fall is. This video-recorded teaching situation was \nchosen because it presents preservice physics teachers a \ncomplex and authentic teaching situation where many dif -\nferent noticing-relevant general and subject-specific issues \ncould be identified. Teachers could describe mere surface-\nlevel, general issues such as that the students were noisy \nat several occasions, or more deep-level, subject-specific \nissues such as that several students raised concerns with \nthe experimental setup (e.g., missing control of variables) \nor conceptual difficulties (e.g., whether an ever-accelerating \nobject reaches the speed of light). Following the classifica -\ntion rubric for noticing research in science education by \nChan et al. (2021), our approach was meant to characterize \nteacher noticing (purpose) as assessed through observa -\ntion of other teachers’ teaching (teaching context), where \nthe observing teachers could not control what happened \n(role of teacher) and the noticing-relevant events were pre-\ndetermined (what to notice) and selected by the researchers \n(selection of probes) with open-ended prompts (nature of \nprompt) and divergent answers without correct answer (type \nof teacher responses).\nThe video is about 17 minutes long. The preservice phys-\nics teachers were allowed to watch the video only once, \nwithout rewinding the recording, in order to simulate in-the-\nmoment pressures of decision-making (Chan et al., 2021). \nIt was an authentic lesson that was recorded in a German \ngrade 9 high school physics classroom as part of a post-\nuniversity physics teacher preparation program. In Germany, \nafter the university-based teacher training teacher trainees \nare required to pass a one- to two-year program, run by fed-\neral states, that will approve if they are finally allowed to \nteach in public schools. Using a recorded lesson from this \npost-university teacher preparation program presents a les -\nson that is proximal to what the preservice teachers will \ndo in their future careers. Overall, N=75 preservice physics \nteachers participated in the study who produced 86 written \ndescriptions (sometimes preservice teachers produced two \ntexts, pre and post to a seminar). The teachers varied in their \nteaching experience and came from three different universi-\nties throughout Germany (see Table 1). Preservice teachers \nspent approximately one hour on the entire questionnaire of \nthe online video-vignette. The text production took approxi-\nmately 20 minutes (independently of another 17 minutes \nvideo observation and another 20 minutes answering further \nquestions). Preservice physics teachers were instructed to \nfirst describe what happened in the teaching situation. After-\nwards, they should evaluate the situation, devise alternative \nmodes of action, and formulate consequences for their own \nteaching.\nGiven that preservice physics teachers either described, \nevaluated, and reasoned about the observed teaching situa -\ntion, the sentences that count as descriptions were extracted \nthrough an ML-based classifier. The ML-based classifier \nautomatically retrieved descriptive sentences based on a \nclassification algorithm that was described elsewhere (Wulff \net al., 2022). This classifier annotated each sentence with \none of the following labels: “circumstances”, “descrip -\ntion”, “evaluation”, “alternatives”, and “consequences.” \nUsing sentences as the segmentation units was found to be \na reasonable strategy in similar contexts of writing analyt -\nics (Ullmann, 2019). The descriptive sentences were fur -\nther filtered to a length greater than four words to remove \nheadlines and similar non-informative sentences. 98% of \nsentences of the original descriptive sentences remained \n(1537 sentences in total). The preservice teachers wrote \non average 16.0 ( SD = 7.9, min: 4, max: 59) words in a \ndescriptive sentence. In descriptive sentences, the preservice \nteachers wrote in various ways about the events in the lesson \nas outlined in Table  2. A randomly drawn sentence from a \npreservice physics teacher reads as follows: “The observa -\ntions [from the students] and differences [to the hypotheses] \nwere collected and summarized by the teacher as free fall -\ning movement is independent of the mass.” This sentence \nand all words and sentences in the following were translated \nfrom German to English by the authors who are familiar \nTable 1  Sample description\nExperience Term Place Seminar type N M  (age) Md SD prop female\nBachelor Winter 2020/21 University C Physics edu. seminar 5 22.6 23.0 2.5 0.40\nBachelor/Master Summer 2020 University A Physics edu. seminar 31 24.7 23.0 3.7 0.13\nMaster Summer 2020 University B Teaching internsh. (Master) 7 24.0 24.0 2.4 0.14\nMaster Winter 2019/20 University B Teaching internsh. (Master) 13 25.1 23.0 3.9 0.23\nMaster Summer 2021 University B Teaching internsh. (Master) 7 25.3 23.0 6.4 0.29\nMaster Winter 2020/21 University B Teaching internsh. (Master) 12 25.8 25.0 6.6 0.50\nMaster Winter 2020/21 University B Teaching internsh. (Master) 8 26.4 25.0 7.5 0.62\nBachelor Winter 2020/21 University B Teaching internsh. (Bachelor) 3 26.3 27.0 3.1 0.33\n495Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nwith English language, in particular specialized vocabulary \nin physics. Some intricacies emerged with the translations. \nFor example, German language has many specific abbre -\nviations in educational contexts, e.g., “SuS” (“Schülerin -\nnen und Schüler”) for female and male students or “LK” \n(“Lehrkraft”) as an inclusive word for teacher that have no \nequivalent in English. We tried to highlight those issues \nwhen they occur. Furthermore, German language is well \nknown for its compound nouns that can become very long \n(e.g., “Fallröhrendemonstrationsexperiment”, which can be \ntranslated to “demonstration experiment with drop tube”). \nIn German, compound nouns may count as one word in the \nvocabulary, whereas in English many different words would \nbe added. Consequently, the German vocabulary in terms of \ndistinct words is larger compared to the English vocabulary.1\nClustering Sentences of the Written Descriptions\nML methods that extract patterns in unstructured data such \nas the constructed responses are categorized as unsupervised \nTable 2  Seq uencing of the lesson\nSequence Description Teaching Goal\nS1 Introduction: Teacher reminds students of a problem from the last \nlesson where they compared a race between hare and hedgehog - \nTeacher: “Today we are going to observe a different race, between \na feather and a screw.”\nMotivation for the topic, focus on the movement of free falling \nobjects\nS2 Teacher shows both objects and asks which lands on the floor first \nif the teacher drops them; students hypothesize: 1. screw because \nof gravitational pull, 2. feather has higher air resistance; Teacher \nconducts the experiment (feather sticks to his hand at first)\nDemonstration that feather falls slower compared to screw\nS3 Teacher: How can we change the experiment to determine influencing \nfactors on free falling movement?; Students offer: wider screw (in \norder to raise air resistance), thicker feather (to make it comparable \nin mass to screw)\nTransition to experiment with two equally shaped objects that \ndiffered in their masses\nS4 Teacher drops a 100g and 50g mass object; given similar \nmovements, teacher concludes that the mass has no influence; \nStudent suggests to probe the experiment in vacuum, which the \nteachers takes as a “perfect” transition to the next step\nDemonstrate that movement was independent of mass of objects; \ncollection of students’ hypotheses\nS5 Teacher shows vacuum tube with feather and mass inside; \nTeacher asks for hypotheses about result; One student first \nsuggests that both arrive at the same time, and then refines \nhis answer to hypothesize that the heavier object arrives first, \nanother student suggests that both arrive at the same time \nbecause the air resistance is removed\nDemonstrate dependence of free falling movement on air \nresistance\nS6 Teacher opens valve in vacuum tube to let air flow into it and \nrepeats the experiment. Initially the feather sticks to the glass. \nAfterwards, the objects move at different rates.\nDemonstrate that initial results (dependence of movement on air \nresistance) can be replicated with the vacuum tube\nS7 Teacher summarizes the experimental findings; he asks what \ninfluences the falling movement of the objects. The students \nreply air resistance; teacher asks what does not influence the \nmovement and the students reply the weight.\nConclusions, summary\nS8 Teacher introduces movement without air resistance and \ndownward as free fall; Student asks why a jump by a parachute \njumper is called a free fall: because it is without a parachute in \nthe initial phase or because there is no air resistance? Teacher \npostpones the question until later\nDefines free fall as movement without air resistance\nS9 Teacher asks students to devise a new experiment that can test \nthe type of this movement; Student suggests to measure time \nat certain waymakers in space; Another student suggests to \nmeasure distance at certain time measurement points; Further \nstudent asserts objects would reach speed of light without \nbottom and without air resistance, in accelerated movement; \nDiscussion which experimental setup is more practical\nDevise new experiment to check the type of free falling movement \n(i.e., constantly accelerated movement)\n1 R esearchers who would like to adopt the presented clustering \napproach with English language data would have to implement the \nEnglish language model which is readily available (see description of \ntechnical implementation in the supplement).\n496 Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nML. Unsupervised ML typically include some form of \ndimensionality reduction and clustering oftentimes with the \npurpose to make high-dimensional data human-interpretable. \nClustering approaches that were not based on pretrained lan-\nguage models enabled science education scholars to identify \nemergent topics in conferences or students’ writing (Odden \net al., 2020; Sherin, 2013), however, they also oftentimes \nrequire involved preprocessing of the data (Angelov, 2020; \nOdden et al., 2020; Zehner et al., 2016). Most often, research-\ners needed to remove frequent words (stopwords), lower-case \nall words (which might be disadvantageous in German where \nupper-case letters can differentiate word senses), or trans -\nform words into their base form to reduce vocabulary size \n(Odden et al., 2020; Rosenberg & Krist, 2020). Furthermore, \nresearchers noted the difficulty in determining the number of \nclusters that should be extracted in these approaches (Sherin, \n2013) and these approaches oftentimes assume that word \norder in the sentences is irrelevant (bag-of-words assump -\ntion). Finally, these approaches are ignorant of ambiguous \nword senses. No prior information on the words is incorpo-\nrated in these approaches such that the word “bank” in the \nphrases “river bank” and “bank robbery” might be treated as \nthe same word even though the meaning differs substantially. \nRecently, however, advances in NLP and ML research pro-\nvided pretrained language models that provide contextualized \nembeddings for language data that help to cope with some of \nthe aforementioned challenges. These contextualized embed-\ndings potentially enable researchers to model constructed \nresponses in a more language-sensitive way that is able to \npreserve word ordering and word sense disambiguation as \nfeatures.\nPretrained language models can generate contextualized \nembeddings for language input that enhances modeling \nof the language data (Mikolov et al., 2013; Sherin, 2013; \nTaher Pilehvar & Camacho-Collados, 2020). Essentially, \nwords are mapped to a position in high-dimensional vec -\ntor space, called a distributed representation in the form \nof embeddings (Taher Pilehvar & Camacho-Collados, \n2020). Vector space models thus encode word similarity \nand efficiently represent words. Given the claim that one \nunderstands a word by the company it keeps (Jurafsky & \nMartin, 2014), word embeddings can be learned through \nML approaches, where model weights are optimized with \nthe goal that a word embedding for a given word predicts \nthe context words (Mikolov et al., 2013). More advanced \napproaches utilize pretrained language models that result \nin embeddings that also account for the context (contex -\ntualized embeddings) and the position in a segment that \na word occurs in (Taher Pilehvar & Camacho-Collados, \n2020). Pretrained language models are typically trained on \nlarge unstructured datasets (e.g., the Internet, Wikipedia). \nTraining tasks involve prediction of context words (Devlin \net al., 2018). For practical purposes the vocabulary is often \nrestricted to some 30,000 tokens, where unknown words \ncan be built from the 30,000 tokens. Linguists have esti -\nmated that 30,000 words are sufficient to understand many \ngeneral English texts well (Mitchell, 2020). If a sentence is \ninput into a pretrained language model, typically embed -\ndings for each word in the sentence (given the position and \ncontext words) is the output. To generate a contextualized \nembedding for a sentence, the word embeddings can be \npooled.\nAs an illustrative example for sentence embeddings based \non pretrained language models, the following physics-related \nand general sentences should be considered (some noise data \npoints were added which will be motivated later on): ’Earth \nexerts a force’, ’The force acts on’, ’The force on earth’, ’We \nforce her’, ’They force him’, ’How to force him’, ’Grass is \ngreen’, ’The sunset can be red’, ’Green is grass’ (called Seg-\nment 1 to 9 respectively). Force in the first three sentences \nrelates to the physics meaning (given as a noun). In the fol-\nlowing three “force” is included as a verb that encapsulates \na certain kind of rather aggressive behavior. The final three \nsentences are included as sentences that are entirely different \nin meaning. “Force” in the former sentences has a different \nword sense compared to the sentences 4 to 6 and should be \ndistinguished in a clustering approach. In Fig.  1(a) a two-\ndimensional representation of the sentence embeddings \ngleaned from a pretrained language model is depicted. As \ncan be seen from the separation of datapoints in space, pre-\ntrained language model’s word embeddings can in fact dis -\nentangle the senses to a certain degree. To further inspect the \nembedding space, a clustering approach can now determine \nwhich sentences are likely related to each other (Angelov, \n2020).\nExtracting clusters from contextualized embeddings \ncan be done with Hierarchical density-based spatial clus -\ntering of applications with noise (HDBSCAN) (Campello \net al., 2013). HDBSCAN is a way to calculate the number \nof dense volumes (i.e., clusters) in the embedding space. \nDensity-based clustering methods consider the probability \ndensity of a collection of data points (Kriegel et al., 2011). \nIn Fig. 1(b) the probability density distribution for the data \npoints in Fig. 1(a) is depicted. To extract clusters, an imagi-\nnary water level can be introduced into the probability space. \nThe water level represents a threshold for cluster extraction. \nEmerging islands, i.e., regions above the water level, repre-\nsent clusters. If water level rises, less probability mass lies \nabove the water level, and thus fewer clusters are extracted. \nA suitable water level has to be chosen in order to extract an \nappropriate amount of clusters.\nTo perform the actual clustering the nearest neighbors \nfor each data point will be determined and the closest dis -\ntance between nearest neighbors will be highlighted as edges \nin a graph, i.e., the minimal spanning tree (see Fig.  1(c)). \nA threshold parameter (i.e., the minimal distance) is then \n497Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nvaried where edges that surpass the threshold are removed \nfrom the graph. Finally, the minimal spanning tree is mapped \ninto a condensed tree representation (see Fig. 1(d)). The con-\ndensed tree depicts the number of data points in a cluster \n(width of the branches) with varying densities (  /u1D706 ). A w ay \nto extract clusters from the condensed tree is by defining \na minimal cluster size and examining the stability of the \nbranches over different density values (moving up and down \nin Fig. 1(d)). It is desirable to have clusters that persist over \nvarying density-levels. The stability of a cluster basically \nrelates to the regions of maximum area in the condensed \ntree Kriegel et al. (2011), Campello et al. (2013). The algo-\nrithm thus determines a number of clusters by examining \nproperties of the clusters. From the illustrative example, \nthe resulting clusters based on this clustering approach \n(HDBSCAN combined with pretrained language models) \nare depicted as blue, orange, and green ovoids in Fig.  1(d). \nThe red-shaped ovoid cluster could be considered as noise, \ngiven the instability over density values in Fig.  1(d). If the \nsentence embedding points in Fig.  1(a) were to be colored, \nthe closely aligned sentences would in fact be colored with \nthe same colors, respectively.\nFig. 1  a  Two-dimensional representation of the example segments \nand noise. b  Surface plot of probability density of the data points. \nc Minimal spanning tree with data points as nodes (colors indicate the \nmutual reachability distance). d  Dendrogram of clusters for varying \ndensity values (colored circles indicate clusters)\n498 Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nAnalysis Procedures\nInterpretability of Clusters (RQ1a) In order to evaluate if the \npretrained language model-based clustering approach 2 out-\nputs represent interpretable clusters, the most representative \nwords for each cluster were considered, and a definition was \nderived. Visual inspection of the two-dimensional embed -\nding space and the condensed tree representation helped to \ndetermine similarities and differences of the clusters. If the \nfive most representative words could be mapped to distinct \nsections in the observed teaching situation (see Table  2) \nand were coherent, then we considered this as evidence of \na meaningful cluster, because clusters were anticipated to \nattend to localizable events (e.g., experiments) or actions \n(e.g., devising hypotheses). We also assessed to what extent \nthe clusters related to physics ideas that were implicitly or \nexplicitly relevant in the observed teaching situation, and \nwhat ideas or events were not clustered.\nSpecificity of Clusters (RQ1b) Then it was evaluated to what \nextent physics-savvy human raters could use the extracted clus-\nters to manually annotate the video-recorded teaching situa-\ntion. If human raters struggled to annotate a certain cluster in \nthe video recording, this would provide evidence of unspecific \nfocus of a cluster. To annotate the teaching situation on the \nbasis of the extracted clusters, three independent raters with \nphysics background (one postdoc, two PhD students) who \nwere familiar with the observed teaching situation annotated \nthe entire video sequence based on 10 second intervals. All the \ninformation they received were the five most representative \nwords for the respective clusters (coding 1) with no further \ninstruction. In a second iteration (coding 2), the human raters \ndiscussed and agreed on some coding rules, e.g., that the entire \nprocess of an experiment should be annotated if relevant words \nof a cluster occurred only at the beginning. To evaluate the \nreliability of this annotation, we first examined a graphical rep-\nresentation of the annotations over time to evaluate interrater \nagreement. We considered each cluster separately. To evalu-\nate interrater agreement, Krippendorff’s /u1D6FC for each cluster was \ncalculated because Krippendorff’s /u1D6FC is more appropriate than \nCohen’s /u1D705 for three raters. A Krippendorff’s /u1D6FC value of 1 refers \nto perfect reliability and a value of 0 to absence of reliability. \nValues between .667 and .800 are usually considered to allow \nresearchers to draw tentative conclusions, i.e., consider the \nagreements as non-random (Krippendorff, 2004).\nRobustness of Clusters (RQ1c) To analyze robustness of clus-\nters, the clustering approach was applied to smaller subsets of \nthe dataset. To test if small sample sizes are enough, subsets \nof N=43 randomly chosen pre-service teachers and N = 8 ran-\ndomly chosen pre-service teachers were considered. The extent \nto which similar clusters emerge was examined. If meaningful \nclusters could be identified in these subsets, then we considered \nthe algorithm robust with sample size variations which could \nbe beneficial for science education researchers who oftentimes \nonly have small samples at their disposal. Furthermore, we \ncompared the outputs of the pretrained language model-based \nclustering algorithms with a clustering approach that was not \nbased on pretrained language models, but was successfully \napplied in a science education research context before. We \ntherefore adopted the topic modeling approach outlined by \nSherin (2013). He devised an accessible approach for extract-\ning clusters in interview transcripts. He started by segmenting \ntexts into chunks of 100 words (with overlap). Afterwards, a \nnormalized term-document matrix was formed. To circumvent \nthe problem of similar topics (low levels of variability in the \ndata), deviation vectors were calculated. Based on the deviation \nvectors, hierarchical agglomerative clustering yielded a distri-\nbution of topics, depending on the number of topics. Finally, the \nten most representative words were found as the highest ranking \nwords in the centroid vectors for the respective topics. With \nparameter values adapted to our research context, we extracted \nclusters from our descriptions based on this approach. Based on \nthe comparison from the ten most representative words for each \ntopic, we evaluate to what extent both clustering approaches \nyield similar topics. This would yield evidence that the pre-\ntrained language model-based approach could also be success-\nfully employed in science education research contexts.\nAdvanced Textual Analytics Based on the Clusters (RQ2) The \napplicability of the pretrained language model-based cluster-\ning for analytics of the constructed responses was evaluated \nthrough exploratory analysis of the textual organization of \nthe constructed responses. Based on episodic memory the -\nory it can be expected that the preservice teachers provide a \nchronologically ordered text organization. Hence, the tem -\nporal progression of the clusters within the teachers’ written \ndescriptions was analyzed. To depict the temporal progression \nof the clusters within the written descriptions, the sentences \nwere mapped to their relative position in reference to the other \ndescriptive sentences for each teacher (see similar analysis in: \nSherin, 2013). Mapping the sentences to their relative posi-\ntion was supposed to produce certain peaks where clusters \nare most prevalent in the descriptions. For example, it could \nbe expected that mentioning the introduction with hedgehog \nand hare or the teacher experiments precedes other clusters \nsuch as the discussion of the type of movement, because these \ndescriptions appeared first in the observed teaching situation \nand teachers are expected to describe the teaching situation \nchronologically. Distinctiveness in temporal progression \nwould indicate that the extracted clusters in fact captured dif-\nferent aspects of the teaching situation. To further analyze \n2 Please find de tails on the technical implementation in the supple -\nment.\n499Journal of Science Education and Technology  (2022) 31:490–513\n1 3\ntextual organization, we employed a network-analytical \napproach to calculate the centrality of different clusters and a \nvector-field approach where the movements through cluster \nspace can be characterized. In both approaches we will evalu-\nate to what extent the respective empirical distributions, i.e., \nthe directed network of clusters and the vector-field repre -\nsentation, are better captured by random processes or more \ndeterministic processes. If teacher’s written descriptions can \nbe characterized by more deterministic processes, we can con-\nclude that the presented clustering approach can yield insights \ninto textual organization.\nFindings\nValidity of the Clustering Approach (RQ1)\nInterpretability of the Extracted Clusters (RQ1a)\nTo evaluate the interpretability of the extracted clusters, con-\ntextualized embeddings of the preservice physics teachers’ \ndescriptive sentences were generated with the pretrained \nlanguage models and clusters were extracted with the HDB-\nSCAN algorithm. This approach yielded a number of 14 \nclusters and a noise cluster (cluster -1). The absolute sizes \n(# of sentences in a cluster) are depicted in Table  3. We \nalso provided a definition of the clusters based on the most \nrepresentative words for each cluster, and we determined \nhow many sentences per written description on average were \ncategorized into each cluster (see Table 3). The largest share \nof sentences was coded as -1.3 The graphical representation \nof the embedding space with clusters highlighted in colors \ncan be seen in Fig.  2. The embedding space can be funda -\nmentally separated into two overarching groups (indicated \nTable 3  N umber, share (i.e., number of segments) and top five words of the extracted topics. M, Md are mean and median number of sentences \nfor a cluster in a written description, respectively. SD is the standard deviation; range is minimum and maximum number of sentences\nTopic Share Top five words Definition M Md SD range\n-1 760 students, student, experiment, teacher (formal), \nteacher (informal)\n- 8.7 7.0 6.5 1 - 34\n0 38 floor, piece of mass, simultaneous, pieces of mass, \nstudents\nPieces of mass dropped on floor and touch floor \nsimultaneously.\n1.5 1.0 0.9 1 - 5\n1 56 pieces of mass, masses, same, shape, fall Pieces of mass have the same shape. 1.1 1.0 0.4 1 - 2\n2 246 feather, screw, tube, vacuum, air Experiment in vacuum tube with feather and screw, \nwithout air.\n3.4 3.0 2.4 1 - 13\n3 10 respond, feedback, again, summarize, guides Teacher summarizes answers, responds and gives \nfeedback.\n1.7 1.5 0.8 1 - 3\n4 42 students, raise arms, raised arms, respond, same Students raise their arms and respond to the teachers’ \nquestions\n1.8 1.0 1.9 1 - 8\n5 11 summarize, main hypotheses, main theses, claims, \nhypotheses\nTeacher summarizes the main hypotheses/theses/\nclaims.\n1.2 1.0 0.7 1 - 3\n6 56 claims, hypotheses, validate, students, asks Students validate their claims/hypotheses through \nexperiments.\n1.4 1.0 1.0 1 - 6\n7 25 summarize, claims, guides, teacher (formal), teacher \n(informal)\nTeacher summarizes the claims of the students. 1.4 1.0 0.6 1 - 3\n8 49 type of movement, what kind, teacher (informal), \nasks, type\nTeacher asks what type of movement it is. 1.6 1.0 0.9 1 - 4\n9 54 measure, movement, distance, steady motion, \ncertain\nStudents propose to measure distance at certain \ntimes and respond steady motion.\n1.8 1.5 1.0 1 - 5\n10 19 screw, weight force, air resistance, higher, different Students say screw has a higher weight force, and \nfeather higher air resistance.\n1.3 1.0 0.8 1 - 4\n11 64 air resistance, mass, influence, speed of fall, dependentAir resistance has influence on speed of fall, also \ndependent on mass.\n1.6 1.0 0.7 1 - 3\n12 79 free, fall, folder, definition, topic Teacher writes the definition of free fall, students \ncopy it in their folders.\n1.6 1.0 1.2 1 - 8\n13 28 hedgehog, lesson, lesson, hare, last Teacher reminds students of last lesson with race \nbetween hedgehog and hare.\n1.5 1.0 0.9 1 - 4\n3 This means t hat these sentences were not close enough to any of \nthe cluster centroids. Column three of Table 3 indicates that very gen-\neral words fall into this cluster, such as “SuS” (which is the unisex \nabbreviation for “students” in German) or “lehrer” (engl.: “teacher”). \nSeemingly, this cluster encapsulated descriptive sentences that were \ntoo general or that might belong to multiple clusters such that they \naverage out.\n500 Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nby the black line): (1) clusters that relate to physics-related \nevents or topics that occurred during the teaching situation \nand (2) clusters that encapsulate general actions, and spe -\ncific, non-subject-related events. In group 1, cluster 2 the -\nmatizes the central experiment of the lesson where a feather \nand screw are observed falling in a vacuum tube. Cluster \n2 had the second largest share of sentences in the descrip -\ntions (see Table  1). Relatedly, cluster 10 likely represents \nthe students’ hypotheses that the screw has a higher weight, \nwhereas the feather has a high air resistance. Clusters 0 and \n1 represent the other experiment, in which two mass pieces \n(equal shape, different mass) are dropped simultaneously to \ndeduce that free fall is independent of mass. Clusters 8 and \n9 refer to the teacher’s question about which type of move -\nment a free fall is and how this type of movement can be \nexperimentally determined.\nOn the other hand, in group 2, clusters 6 and 7 represent \nteachers’ and students’ actions of summarizing and pos -\ning hypotheses/claims respectively. Given the similarity of \nclusters 6 and 7, they were also close in embedding space. \nCluster 6 was related to posing hypotheses by the students, \nwhereas cluster 5 was related to the process of summing \nup the hypotheses by the teacher. In fact, this was a recur -\nrent thread in the lesson: the teacher asked the students to \nhypothesize about the results in advance of an experiment \nwhich is why the cluster was coded at several points. Cluster \n3 also refers to the teachers’ responding to students’ answers. \nCluster 4 represents the students’ action of raising arms and \nresponding to the teachers’ questions. Cluster 13 captured \nthe beginning of the lesson where the teacher reminds the \nstudents of the former lesson regarding the race between \nhedgehog and hare. Finally, cluster 12 referred to the instruc-\ntion by the teacher that the students may copy the definition \nof free fall into their folders.\nIn sum, the clusters encapsulate both short and rather \nspecific events in the teaching situation (e.g., writing the \ndefinition of free fall in the folder) and more abstract ideas \nsuch as summarizing hypotheses which occurred more than \nonce in the scene. They also include more general clusters \n(summarizing students’ hypotheses, e.g., cluster 5) and \nmore physics-related contents (characterization of the type \nof movement, e.g., clusters 8 and 9). Preservice physics \nteachers wrote on average 3.4 sentences on cluster 2, which \ncomprised the largest share (after the noise cluster), followed \nby cluster 1 and 11 with 1.8 sentences on average. Thus, \nphysics-specific clusters were more extensively included in \nthe written descriptions. However, the overall low average \ncounts of one sentence for a cluster could indicate that often-\ntimes the preservice teachers only briefly elaborated on an \nevent. It is also noteworthy that some important events in the \nteaching situations are not captured in a cluster. During the \nlesson the students asked for example: “Why is it called free \nfall for a parachute jumper?”, “Would an infinitely accelerat-\ning mass surpass the speed of light?”, or “Would two plates, \none made of cardboard the other made of metal, actually \narrive on the floor at the same time?”\nFig. 2  T wo-dimensional representation of clusters. A point represents the projection of a sentence embedding into the two dimensions. Colors \nrepresent belonging to a cluster. Gray points represent “noise”, i.e., not belonging to any cluster. Larger points indicate cluster centroids\n501Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nSpecificity of the Extracted Cluster (RQ1b)\nTo examine to what extent the extracted clusters map to dis-\ncernible events and topics in the teaching situation, human \nraters used the clusters as represented through the most \ninformative words to annotate the video recording of the \nteaching situation (RQ3). Figure 5 depicts all codings from \nthree independent annotators separated by cluster over time. \nTo estimate human interrater agreement, we calculated the \nKrippendorff /u1D6FC values for the clusters. After the first round \nof rating the video-recorded teaching situation (coding 1), \nthe Krippendorff /u1D6FC ’ s indicate that some clusters (e.g., 0, 1, \n2, 8, 12, and 13) could be identified with good reliability \ngiven only the five most representative words and no anno-\ntator training. Cluster 12 related to the introduction of the \ndefinition of free fall by the teacher. This, apparently, was a \nlocalizable event in the teaching situation. Cluster 0 related \nto the experiment with two masses (similar reasoning for \ncluster 1). The teacher used two masses only once as an \nexperiment, hence, this formed a recognizable event for the \nhuman raters. Cluster 13 related to the very beginning of \nthe lesson. The words “hedgehog” and “hare” are unique for \nthis event. The human annotators reached poor reliability on \nclusters with more general words (e.g., 3, 7, and 11). The \nwords “respond”, “feedback”, “summarize”, and “teacher” \ncould be applied to many different events in the teaching \nsituation. They represent high-inferential categories, because \nthe teacher and students did not specifically say that they \n“responded” or “summarized” ideas.\nAfter coding 1, the three annotators made their coding \nrules more explicit and discussed them. On this basis, the \nvideo-recording of the teaching situation was annotated \nagain by all three annotators (coding 2). Some improvements \ncould be seen after the discussion. Most notably, clusters 1, \n2, 4, and 9 substantially improved in interrater agreement \n(see Table 4). Cluster 9 made the most substantial improve-\nment. This cluster related to the measurement and determi -\nnation of the type of movement. The raters agreed to include \nall student suggestions at the ending of the teaching situation \nbecause this represented a coherent phase, which caused the \nimprovements in agreement. However, other clusters (3, 6, \n7, 10, 11) seemed to remain too vague to be annotated based \non the five most representative words.\nRobustness of the Extracted Cluster (RQ1c)\nTo evaluate the robustness of the extracted clusters, we \nprobed to what extent the clustering algorithm would still \nyield interpretable and comparable clusters for smaller sam-\nple sizes. The baseline for comparison formed the extracted \nclusters based on the entire dataset (see Fig.  2). As sample \nsizes in noticing research in science education are typically \nsmaller, subsets of N=43 and N=8 were drawn. The entire \nclustering approach was performed for these subsets of the \ndata. The resulting cluster embeddings and condensed trees \ncan be seen in Fig. 4. We particularly mapped the extracted \nclusters based on the top five words to the baseline clus -\nters as extracted with the entire dataset. It is noteworthy \nthat the spatial outline and the actual extracted clusters can \nbe mapped well onto each other. This is even possible for \na sample size of only N=8 teachers. The two overarching \ngroups (general and physics-specific) could be identified \nfor the subsamples as well. Based on the condensed trees, \nsome similarities in cluster evolution over different density \nvalues can be inferred as well. For example, clusters 8 and \n9 seem related in all condensed trees as they evolve from a \nTable 4  V alues for interrater agreement as measured through Krippendorff’s /u1D6FC for each cluster\n502 Journal of Science Education and Technology  (2022) 31:490–513\n1 3\ncommon branch. Both clusters comprise sentences on type \nof movement which are physics-specific. Interestingly, in \nFig. 3, also clusters 10 and 11 fall on the same branch as 8 \nand 9. This might be attributed to the fact that in clusters 10 \nand 11 the influence of air resistance on free fall is consid -\nered which is closely related to movement as well. While \nclusters 0 and 1 are linked in Fig. 3 (both include the vacuum \ntube experiment), this link does not exist in Fig. 4. For these \nclusters, probably the five most representative words are not \ninformative enough to allow for clear distinction. Clusters \n4, 5, and 6 relate to the students’ and teachers’ actions of \nposing hypotheses (see Fig.  3). While they neatly evolve \nfrom one parent branch in Fig. 3, only one of the respective \nclusters was present in the smaller samples. However, they \nalso separate early (at low densities) from the other clusters \n(see Fig. 4).\nFurther evidence for robustness of the presented clus -\ntering approach based on pretrained language models can \nbe gleaned by comparison with a formerly successfully \nemployed clustering approach in science education research \nthat was not based on pretrained language models. To imple-\nment a clustering approach based on hierarchical agglom -\nerative clustering, a similar protocol as outlined in Sherin \n(2013) was followed. However, we did not segment our \ntexts into 100-word chunks, but rather into the sentences \nthat were used as smallest segments. We considered this use-\nful, because we expected the grain size of our clusters (i.e., \ndiscernable events in the teaching situation) to be smaller \ncompared to the grain-size of the clusters in Sherin ( 2013), \ni.e., explanations. Our overall vocabulary was 2,786 unique \nwords in German language. 232 stopwords were removed. \nThis enabled us to calculate deviation vectors and apply \nclustering. A number of 14 clusters were found to be reason-\nable for our data (see Supplementary Material for detailed \nTable).\nTable 5 depicts the resulting clusters with the most rep -\nresentative words for each cluster vis-á-vis the clusters from \nthe pretrained language model-based clustering approach. \nMost of the resulting clusters can be mapped to the clus -\nters that were extracted based on the pretrained language \nmodel-based clustering approach. Cluster 0S 4 thematizes \nstudents’ formulating hypotheses and summarization by the \nteacher. This relates to clusters 3, 5, and 7. Clusters 1S and \n2S relate to the vacuum tube experiment, where cluster 1S \nfocusses on the execution and cluster 2S on the observation \nand results. This maps to cluster 2. Cluster 3S relates to the \ndependency of air resistance and fall velocity, and possibly \nrelates to clusters 10 and 11. Cluster 4S is not entirely clear, \nand cluster 5S deals with the teacher repeating the experi -\nment, which has no apparent equivalent cluster. Cluster 6S \nfocusses on students’ raising their arms and responding, \nwhich could be mapped to cluster 4. Cluster 7S relates to \nthe writing down of the definition of free fall, which can be \nlinked to cluster 12. Cluster 8S likely mixes the response \nof one female student and the remark of another male stu -\ndent, to what extent the speed of light would be reached \nby a falling object. No apparent link can be made to the \npretrained language model-based clusters. Cluster 9S relates \nFig. 3  Condensed tree represen-\ntation of the extracted clusters\n4 F or comparison purposes all clusters which were extracted from the \napproach by Sherin ( 2013) were appended with an ‘S’, e.g., cluster \n0S.\n503Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nto the experiment with two masses that would most likely \nmap to clusters 0 and 1. Cluster 10S addresses the transi -\ntion from introduction of the experiments with no apparent \ncorresponding cluster. Cluster 11S, again, deals with the \nexperiment with two masses and links to clusters 0 and 1. \nCluster 12S addresses a students’ answer to the question \nabout what kind of movement the free fall is. The closest \nresemblance is with cluster 8. Finally, cluster 13S addresses \nthe vacuum tube experiment, in particular the repetition of \nthe same. No apparent equivalent exists in the pretrained \nlanguage model-based clustering approach. Finally, we cal-\nculated the proportion of sentences in each cluster from the \napproach by Sherin ( 2013) that were classified as noise in \nthe pretrained language model-based clustering approach. \nThe respective proportions for each cluster were: 0.46 (0S), \n0.28 (1S), 0.45 (2S), 0.40 (3S), 0.60 (4S), 0.60 (5S), 0.48 \n(6S), 0.38 (7S), 0.62 (8S), 0.35 (9S), 0.61 (10S), 0.32 (11S), \n0.34 (12S), and 0.09 (13S). Clusters 4S, 5S, 8S, and 10S \nFig. 4  Scatter plots and condensed trees for cluster evaluation of smaller samples (N = 8 and N = 43 teachers)\n504 Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nhad a particularly large shares of noise-clustered sentences. \nInterestingly, these clusters could not be easily mapped to \nthe clusters from the pretrained language model-based clus-\ntering approach (however, cluster 13S with a particularly \nlow proportion could also not be assigned). They also con -\nsistently included generic words (e.g., teacher or students), \nwhich were attributed with the noise cluster in the pre -\ntrained language model-based clustering approach Fig.  5.\nExploring Textual Organization with the Extracted \nClusters (RQ2)\nTo evaluate to what extent the extracted clusters provide \nquantifiable information on the textual organization of \nthe written descriptions, we first plot the occurrence of \nclusters throughout the written descriptions, examine \nthe non-random organization of the clusters, and exam -\nine properties of the cluster embeddings. Occurrence of \nclusters throughout the written descriptions is depicted in \nFig.  6. The vertical bars indicate the textual position for \nthe respective maximum occurrence of a certain cluster. \nThe textual positions of the maxima are equally distributed \nthroughout the written descriptions, so that all parts of the \nwritten descriptions are attributed with a cluster. Further -\nmore, the cluster occur at expected positions, given the \nevents in the teaching situation. For example, cluster 13 \naddressed the beginning of the lesson and it occurred most \nfrequently at the very beginning of the written descriptions \n(see Fig.  6). In the observed teaching situation, three \nexperiments were carried out one after the other: Free \nfall of a screw and a spring (cluster 10), free fall of two \nmasses of the same size but different weights (cluster 1) \nand, finally, free fall in a vacuum tube (cluster 2). Clus -\nter 10 appeared at the beginning of the texts. Cluster 1, \nin contrast, appeared somewhat later, which maps to the \ntemporal sequence of events in the observed teaching situ -\nation, since both experiments that were referenced in these \nclusters were carried out shortly after each other in the \nfirst half of the video. Cluster 2 was addressed frequently \nand extensively throughout the descriptions. In fact, clus -\nter 2 relates to the most noteworthy experiment (vacuum \ntube) in the entire teaching situation, which might explain \nthe preponderance in the written descriptions.\nA problem (cluster 0) occurred in the second experi -\nment (cluster 1). The shapes of the curves for cluster 0 and \n1 match well (as it is also evident in Fig. 3). Before the first \nexperiment, the teacher summarized the “main hypotheses”; \nthe corresponding cluster 5 for this event also occurred \nchronologically at the beginning. The other actions, i.e., \nthe formulation and discussion of hypotheses (clusters 6 \nand 7), the reaction to pupils’ answers (cluster 3) and the \npupils’ answers (cluster 4) occurred throughout the teaching \nsituation, which is reflected in the considerably high fre -\nquency throughout the first half of the written descriptions in \nFig. 6. Cluster 11 related to the discussion of the connection \nbetween air resistance, mass and fall velocity. This was also \nTable 5  Comparison of clusters extracted from the pretrained language model-based clustering approach and the clustering approach that was \nadopted from Sherin (2013), and the respective mapping\n505Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nrelated to the experiments seen (observations were described \nand interpreted; hypotheses regarding the connection were \nposed and tested). The temporal progression was appropri -\nate, less at the beginning, more towards the middle of the \ntexts. Cluster 12 addressed summarizing the findings of the \nthree experiments. It occurred quite often at the beginning of \nthe descriptions, which does not correspond to the chrono -\nlogical sequence of events. The reason for this could be that \nsome preservice physics teachers began the descriptions \nwith what the goal/result of the sequence was. Otherwise, \ncluster 12 had its second peak before clusters 8 and 9, which \nagain fits the temporal sequencing of events in the teaching \nsituation. At the end of the sequence, the teacher asked what \nkind of movement the free fall is. The corresponding clus -\nters were the question itself (cluster 8) and the discussion \nabout it (cluster 9). They occurred most often in the middle \nof the texts, which corresponded to the end of the written \ndescriptions. The noise cluster (cluster -1) occurred almost \nequally distributed throughout the written descriptions. The \nrespective counts for each relative position were: 57 (0.0), 71 \n(0.1), 91 (0.2), 73 (0.3), 79 (0.4), 71 (0.5), 66 (0.6), 68 (0.7), \n88 (0.8), 76 (0.9), 20 (1.0). This provides evidence that no \nparticular position in the written descriptions was prone to \ninclude more noise sentences compared to other positions. \nThe lower counts at the beginning and end positions resulted \nfrom the calculation of the relative position index.\nTo analyze the sequential interdependence of the clusters, \ndirected network graphs were generated based on the incom-\ning and outgoing connections for each cluster (see Fig. 7). A \nconnection between clusters was established when one clus-\nter occurred in the preceding or receding sentence of another \ncluster’s sentence. Edges (i.e., the interconnections between \ntwo clusters) in the networks were weighted by the cluster \nsizes to highlight connections that appeared often irrespec -\ntive of the cluster size. The edges with the largest values for \nthe connections were labeled with the respective values (see \nthe small numbers on the edges in Fig. 7(a)). The empirical \nnetwork graph highlights that certain clusters are central in \nthe network (see Fig.  7(a)). The greatest importance in the \nnetwork had clusters -1, 2, 4, 6, and 11. In particular, cluster \n2 represents the vacuum tube experiment, and cluster 4 the \ngeneral cluster that students raise their arms and respond. \nHence, both physics-specific and general clusters were highly \ninterconnected in the physics teachers’ written descriptions.\nBy analyzing interconnections between two nodes, it \nappears that clusters -1, 2, 9, 3, and 10 were self-referenced \nparticularly often. Except for clusters -1 and 3, these clus -\nters related to physics-specific events such as the vacuum \ntube experiment, the type of movement, and the weight and \nair resistance. Moreover, clusters 8 and 9, clusters 0 and \n1, and 1 and 6 are interconnected particularly often. The \nformer two connections directly attribute to the close con -\nnection of these clusters in meaning. The connection of clus-\nter 1 (experiment with two masses) and cluster 6 (students’ \nhypotheses) can be explained by the fact that the teacher \nlinked this experiment with posing hypotheses.\nFinally, movements of the preservice physics teachers \nthrough embedding space by means of addressing specific \nFig. 5  Codings of video sequence (coding 2) with identified clusters based for three independent raters\n506 Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nclusters in their texts should be analyzed with streamline \nplots (see Fig.  7(b)-(d)). Streamline plots are vector field \nrepresentations. We define a connecting vector between two \nsentences that belong to any of the clusters as a “velocity” \nvector, indicating the movement through cluster embedding \nspace. The resulting vector field is represented in Fig. 7 (b). \nA tendency to “move” through cluster embedding space in \ncenter direction can be verified, because the streamlines \ndirect toward the center. By comparing Fig.  7(b) with (c), \nwhich represents a vector field where every velocity magni-\ntude and direction were chosen at random, it is evident that \nFig. 7(b) does not represent a random vector field. When \npositional information is added generate the velocity vector \ndirection (see Fig.  7(d)), the resulting vector field resem -\nbles the empirical vector field. The entropies 5 for compar-\ning velocities in plots (b) with (c), and (b) with (d) in x- \nand y-direction, respectively, were .45 and .28, and .03 and \n.10. This indicates that the vector field in Fig.  7(d) better \napproximates the empirical vector field. Thus, the preservice \nphysics teachers do not randomly walk through the cluster \nembedding space, but rather deliberately compose their texts \nby attending to the different clusters that were extracted with \nthe pretrained language model-based clustering approach.\nDiscussion\nAttention to learning-relevant classroom events and students’ \nthinking is an important skill for teachers to implement a \nstudent-centered pedagogy (van Es & Sherin, 2002b; Chan \net al., 2021; Levin et al., 2009). However, assessment of \nteachers’ attention to classroom events is complex, because \neither the uncertainty of teaching situations is oftentimes \nrelated to the inherent complexity of ongoing processes, \nand describing one’s attention processes is intricately tied \nto teaching knowledge and other filters (Chan et al., 2021). \nConstructed response formats have been argued to facilitate \nmore authentic assessment of attention processes, and com-\nputer-based analytical tools such as ML methods have been \nfound to provide promising means to further our understand-\ning and assessment of complex constructs such as attending \nto classroom events (Lamb et al., 2021; Zhai et al., 2020). In \nthis paper we sought to examine potentials and challenges of \na pretrained language model-based clustering approach for \nthe purpose of extracting patterns, i.e., clusters, in preser -\nvice physics teachers’ written descriptions of an observed \nFig. 6  Pr ogression of extracted clusters relative to other descriptive \nsentences in the documents. Top: absolute count of occurrence for a \ncluster at a given document position. Bottom: relative frequency for a \ncluster at a given document position. Vertical lines indicate the over -\nall peaks in occurrence for each cluster\n5 N ote that higher entropies indicate higher mismatch of two distri -\nbutions.\n507Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nteaching situation. We examined the validity of the extracted \nclusters (RQ1) and explored novel ways in which the clusters \nenable textual analytics that allow to examine quantitative \nhypotheses on textual organization (RQ2).\nTo assess the validity of the extracted clusters, the inter -\npretability (RQ1a), the specificity (RQ1b), and the robust -\nness (RQ1c) of the extracted clusters from the pretrained \nlanguage model-based clustering approach were evaluated. \nThe clustering approach identified a number of 14 clusters \nthat can be grouped into physics-specific and more gen -\neral clusters. With regard to the contents of the clusters, \nall clusters could be related to distinct events in the teach -\ning situation. The clusters encapsulated short, concrete \nevents (recapitulating the last lesson), and more abstract \nideas (summarizing hypotheses). We found that more spe -\ncific, event-related clusters could be reliably coded by the \nraters. However, the more general clusters (related to pos -\ning and summarizing hypotheses) that were applicable to \nseveral parts of the teaching situation yielded lower reli -\nability scores, and are thus more inferential. The extracted \nclusters were also robust to variation in sample size and \nclustering method. A sample size of only N=8 preservice \nphysics teachers’ written descriptions yielded a similar dis-\ntribution of clusters. This likely resulted from grounding the \nclustering with embeddings from the pretrained language \nmodel. A further indication of robustness resulted from the \ncomparison with a previously employed clustering approach \nin science education research (Sherin, 2013). We found that \nmany of the extracted clusters from the pretrained language \nmodel-based clustering approach mapped to the clusters that \nresulted from the application of the clustering approach by \nSherin (2013).\nGiven that the clusters were well interpretable and could \nbe mapped to the teaching situation, we conclude that the \nalgorithm identified meaningful and distinguishable clusters \nin the preservice teachers’ descriptions. The variety of dif -\nferent foci and abstractness in the extracted clusters is well \nrepresented within the different foci of noticing that were \nsummarized by Talanquer et al. ( 2015). Moreover, the dif -\nferentiation of more general clusters and physics-specific \nclusters resonates with the well-established construct of \nteachers’ knowledge, in particular the notions of general ped-\nagogical knowledge and content knowledge (Shulman, 1986; \nCarlson et al., 2019). The pedagogical content knowledge as \nan “amalgam of content and pedagogy” (Hume, 2009) might \nbe conceptualized as the relevant knowledge to connect the \nclusters and discuss pedagogical implications of the physics-\nspecific, and more general clusters. The pretrained language \nmodel provides the relevant structures to classify sentences \nalong this dimension. The contextualized embeddings from \nthe pretrained language model facilitate science education \nresearchers means to extract robust clusters in their datasets. \nFig. 7  Dir ected network graphs of clusters and streamline plots of \ncluster embeddings: a Empirical directed network based on the actual \nconnections between clusters present in the written descriptions; \nb  Streamline plot of actual connections between clusters; c  Stream-\nline plot with randomly distributed directions; d  Streamline plot \nwhere directions are sampled from pool of existing connections.\n508 Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nFurthermore, the pretrained language model-based cluster -\ning approach integrates the data preprocessing into the mod-\neling and introduces a novel criterion for cluster extraction \n(stability of clusters over density variation) that provides \nthe human analyst another important measure of appropriate \ncluster selection.\nThe findings in the context of RQ1 also indicate that the \npreservice physics teachers included very general clusters \nand a comparably large amount of noise clustered sentences. \nThis observation might relate to the finding that novice teach-\ners tend to include broad and general statements in their \nobservations, merely as placeholders (Mena-Marcos et al., \n2013). Mena-Marcos et al. ( 2013) found that more knowl -\nedgeable teachers also include more precise statements in \ntheir reflections. Furthermore, the preservice physics teachers \ntended to include only few sentences on each cluster. This \nindicates that, on average, not much space is spent to describe \nan event in detail. This might relate to the finding that novice \nmathematics and science teachers in particular struggle to \nattend to the specific contents of what was said (Sherin & \nHan, 2004; Levin et al., 2009; Roth et al., 2011). Rather than \ndescribing the concrete hypotheses that the students uttered, \nmany teachers might abstract from the specific contents and \nsimply note that the students posed hypotheses. Yet, develop-\ning noticing skills would require the preservice physics teach-\ners to detail the concrete ideas of the students and teacher in \norder to make an informed evaluation on the substance of \nthe classroom interactions (Levin et al., 2009). However, the \nunspecific contents might relate to our instructional approach. \nFor example, it should be tested if pre-service teachers can \nattend to specific events if they can watch the video multiple \ntimes and take notes for themselves.\nIn the context of RQ2 we evaluated to what extent the \nextracted clusters could be used to assess the textual organi-\nzation of the written descriptions. The absolute and rela -\ntive frequency of sentences in certain clusters with regard \nto their relative position in the written descriptions were \nanalyzed through visual means. We found that the maxi -\nmum counts for the clusters well matched their expected \npositions in the teaching situation. This suggests that the \npreservice physics teachers, on average, compose their writ-\nten descriptions according to the chronological occurrence \nof the events in the teaching situation. This finding resonates \nwith episodic memory theory which suggests that free recall \nof events occurs in temporal order (Conway, 2009; Kahana \net al., 2008). Further evaluation of textual organization of \nclusters by means of network graphs enabled us to document \nthat certain clusters are cued together more closely as would \nbe expected by chance and cluster size. This means that \nclusters that were semantically or chronologically related \nwere linked by the preservice physics teachers more often. \nThis relates to the contiguity effect, namely that neighbor -\ning items (here: events in a teaching situation) are recalled \nsuccessively (Kahana et al., 2008). Furthermore, streamplot \nanalyses revealed that the preservice physics teachers’ move-\nment through cluster embedding space was non-random and \ndependent on the position in this space. On a local scale, the \nposition in cluster space thus determines the propensity with \nwhich the preservice physics teachers’ move in a certain \ndirection in this space. Analysis of textual organization can \nextend assessment of analytical chunks as outlined by van \nEs and Sherin (2002). van Es and Sherin (2002) differentiate \nexpertise in noticing in a trajectory where experts include \nmore interconnections among their evidences (here: clusters \nand interconnections between them in the descriptions). The \nextracted clusters alongside with the network representation \ndirectly would yield a quantification of noted events and thus \nprovide a tool to diagnose expertise levels in noticing.\nLimitations\nEven though the utilization of a pretrained language model \nallowed us to integrate data preprocessing into the ML-based \nmodeling, there are assumptions on the pretrained language \nmodels that have to be critically examined. For example, \nthe resulting contextualized embeddings are determined by \nthe choice of the pretrained language model and cannot be \neasily adjusted. Problems with the pretrained embeddings \nhave also been reported. Given that they are trained on the \nInternet, certain biases related to gender or ethnicity are \npresent in the embeddings (Caliskan et al., 2017; Bhardwaj \net al., 2020). As such, it has to be critically examined to \nwhat extent these biases might be propagated into educa -\ntional assessments which can be disadvantageous.\nAnother feature of the pretrained language model-based \nclustering approach was the algorithm-derived extraction \nof the number of clusters present in the data. Even though \nthe means to extract the clusters based on the stability over \ndensity variation might be an additional tool for research -\ners to use in order to determine a viable number of clusters, \nthere are still many hyperparameters that can be tuned which \nyield different numbers of clusters. Given the scope of this \npaper, we did not systematically vary the hyperparameters to \nfind a final number of clusters. We rather sought to establish \nthat the proposed number of clusters was well interpretable \nin reference to the observed teaching situation. However, \nthe large proportion of noise datapoints also indicates that a \nlarge share of the data is not accounted for in the clustering.\nWith regard to the contents of the clusters, it was notice-\nable that the clustering approach did not capture some rel -\nevant students’ questions from the observed teaching situ -\nation into a distinct cluster even though some pre-service \nteachers included them in their descriptions. Attending to \nthese student questions in the teaching situation required \nphysics knowledge. One student asked whether the different \nmovement of feather and screw (the feather was zig-zagging \n509Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nwhereas the screw moved straight to the ground) could \nexplain the differences in falling time. This is a relevant \nquestion that hints at the missing control of variables in the \nexperiment. Some preservice physics teachers included this \nquestion in their descriptions, however, no separate cluster \nappeared to capture it. This is a consequence of the insta -\nbility and scarcity of this observation as represented in the \npreservice physics teachers’ written descriptions. Omitting \ncontents from the clusters is in fact a goal for unsupervised \nML approaches that seek to reduce a complex dataset (Jordan \n& Mitchell, 2015). For the purpose of assessing skills related \nto attention to classroom events, adjustments in the clustering \nprocedure should be made to allow more clusters to occur, \nbecause the identification of this student question demon -\nstrates close attention to student thinking and an understand-\ning of the problematic aspects of the teaching situation and \nwould be considered to correspond to high levels of noticing \nskills.\nConclusions\nMany domains such as physics embraced ML methods \nto extract information from unstructured data, e.g., to sift \nthrough collider data (unfeasible for humans) to detect outli-\ners (i.e., noise-clustered datapoints) with even the same clus-\ntering approach that has been applied in this study (Arpaia \net al., 2021). Given the novel potentials to extract informa -\ntion from unstructured data and the increasing availability \nof this data, science education researchers should critically \nexamine potentials and challenges of these novel ML-based \nmethods in their research contexts as well. This study could \nshow that a pretrained language model-based clustering \napproach could be used as an assessment tool to analytically \ninduce what teachers attended to in an observed teaching sit-\nuation and evaluate the potentials of ML for analyzing open-\nended responses. We suggest that the applied pretrained lan-\nguage model-based clustering approach can be enhanced by \nfurther fine-tuning the pretrained language model weights to \nscience-specific language. This will enable more involved \nlanguage analytics such as analogical reasoning or syno -\nnym detection (Mikolov et al., 2013). It has been shown \nthat pretrained language models capture some knowledge \nabout quantities (e.g., the magnitude of weight of a proto -\ntypical dog), or some knowledge graphs about entities (e.g., \n“Bob Dylan is a songwriter”) (Wang et al., 2020; Zhang \net al., 2020). In fact, representing natural language into vec-\ntor spaces can enable novel research approaches to answer \nresearch questions in science education research (Sherin, \n2013). Once the pretrained language models are trained and \npublicly available, advanced analytics of written descriptions \nwill be enabled. The presented clustering approach could be \napplied as a recommender tool to automatically feedback to \nthe teachers which events and contents they addressed and \nwhich they missed to pay attention to.\nThe pretrained language models enabled an informed \ncontextualized representation through embeddings of the \nlanguage data. Representation of language data through \nembeddings will also enable researchers to map language \nto other modalities such as graphical/visual data or math -\nematical expressions (see: Krstovski & Blei, 2018). Multiple \nrepresentations and translating between different representa-\ntions has been considered a constitutive feature for scientific \nliteracy (Brookes & Etkina, 2009). However, it will be neces-\nsary to develop theoretically grounded ontologies and epis-\ntemologies of what preservice science teachers can observe \nand how they reason about it (Brookes & Etkina, 2009). Once \npretrained language models are developed and ontologies and \nepistemologies can guide analyses, the presented clustering \napproach in conjunction with these models can help to make \nanalyses more comparable, scalable, and robust.\nWith the help of the clustering approach in this study \nquantitative hypotheses on text composition could be \nexplored. For example, we suspect that preservice physics \nteachers include general and specific language statements in \ntheir written descriptions, are scarce to describe a particular \ncluster, and compose their texts in chronological order of \nthe appearance of the events. Writing a sentence that can \nbe classified into a specific cluster, to a certain extent, pre -\ndisposes the teachers to move through the cluster embed -\nding space in certain directions, and noticing certain events \npredisposes them to also include temporally related events. \nThese hypotheses need to be more systematically tested, \nbecause they can enhance assessment of noticing-related \ncognitive mechanisms such as careful observation and atten-\ntion to classroom events. We even wonder to what extent \nmapping the teachers’ trajectories through the embedding \nspace can be captured by more physics-involved concepts \nsuch as movement through a potential where equations of \nmotion and conservation laws determine the teachers’ writ-\ning. We are not aware that these hypotheses have been tested. \nML-based methods will enable these analyses.\nIn line with the argument put forth by Singer ( 2019), \nwe encourage science education researchers to adopt more \nobservational studies that are grounded in data science,  \nassessment and measurement (Singer, 2019 ). Insights \nin physics today also come from simulation studies and \nobservational (non-manipulable) experiments. The recent \nNobel price of 2021 on complex systems’ behavior or \ninsights in astrophysics are testimony to this. We believe \nthat science education researchers can gain novel insights \non studied phenomena through ML-based, computational \napproaches such as the one presented in this study where \nan unstructured body of textual data is analyzed. Zhai et al.  \n(2020) and Lamb et al. ( 2021) argued that ML-based com -\nputational models can capture the complexity of cognitive \n510 Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nprocesses and “revolutionize” science assessment. We con-\ncur with these arguments and emphasize the necessity to \ndevelop an understanding in the science education research \ncommunity for unsupervised ML approaches and pretrained \nlanguage models in particular, given the preponderance of \nobservational data that is available in educational contexts. \nUnsupervised ML methods have thus great potentials to \nbridge the gap between quantitative and qualitative methods \nin science education. Pretrained language models, more par-\nticularly, capture human-like semantics as measured through \nimplicit association tests and thus represent cognitive struc-\ntures of humans (Caliskan et al., 2017). Hence, pretrained \nlanguage models arguably are most promising candidates to \nmodel language-based processes. Given that, in our case, the \nML-based approach scaled seamlessly (neither human anno-\ntations nor preprocessing of the textual data was necessary \nto extract clusters) and is publicly available to researchers, it \nwould be desirable to increase efforts to share data and mod-\nels in order to make the most use of the available resources.\nSupplementary Information  The online version contains supplemen -\ntary material available at https:// doi. org/ 10. 1007/ s10956- 022- 09969-w.\nFunding  Open Access funding enabled and organized by Projekt \nDEAL. This project is part of the “Qualitätsoffensive Lehrerbildung”, \na joint initiative of the Federal Government and the Länder which aims \nto improve the quality of teacher training. The program is funded by the \nFederal Ministry of Education and Research. The authors are respon -\nsible for the content of this publication.\nData Availability Please send requests to corresponding author.\nCode Availability Please send requests to corresponding author.\nDeclarations \nEthical Statement  All procedures followed were in accordance with \nethical standards for research with human subjects, as outlined for \nexample by the American Psychological Association.\nConsent Statement Informed consent with all participants was assured.\nDisclosure of Potential Conflicts of Interest  The authors are not aware \nof any potential conflict of interests.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta -\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAngelov, D. (2020). Top2Vec: Distributed Representations of Topics. \narXiv.\nArpaia, P., Azzopardi, G., Blanc, F., Bregliozzi, G., Buffat, X., Coyle, \nL., et al. (2021). Machine learning for beam dynamics studies at \nthe CERN Large Hadron Collider. Nuclear Instruments and Meth-\nods in Physics Research Section A: Accelerators, Spectrometers, \nDetectors and Associated Equipment,  985, 164652.  https:// doi. \norg/ 10. 1016/j. nima. 2020. 164652\nBaig, M. I., Shuib, L., & Yadegaridehkordi, E. (2020). Big data in \neducation: a state of the art, limitations, and future research direc-\ntions. International Journal of Educational Technology in Higher \nEducation, 17(1). https:// doi. org/ 10. 1186/ s41239- 020- 00223-0\nBarth-Cohen, L. A., Little, A. J., & Abrahamson, D. (2018). Building \nReflective Practices in a Pre-service Math and Science Teacher \nEducation Course That Focuses on Qualitative Video Analysis. \nJournal of Science Teacher Education, 29(2), 83–101. https:// doi. \norg/ 10. 1080/ 10465 60X. 2018. 14238 37\nBhardwaj, R., Majumder, N., & Poria, S. (2020). Investigating Gender \nBias in BERT. arXiv.\nBreiman, L. (2001). Statistical Modeling: The Two Cultures. Statistical \nScience, 16(3), 199–231.\nBrookes, D. T., & Etkina, E. (2009). “Force,’’ ontology, and language. \nPhysical Review Special Topics - Physics Education Research,  \n5(1), 643. https:// doi. org/ 10. 1103/ PhysR evSTP ER.5. 010110\nBruner, J. S. (1985). Child’s talk: Learning to use language. New York, \nLondon: W.W. Norton & Company.\nCaliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived \nautomatically from language corpora contain human-like biases. \nScience (New York, NY), 356(6334), 183–186. https:// doi. org/ 10. \n1126/ scien ce. aal42 30\nCampello, R. J., Moulavi, D., & Sander, J. (2013). Density-Based \nClustering Based on Hierarchical Density Estimates. In J. Pei, \nV. S. Tseng, L. Cao, H. Motoda, & G. Xu (Eds.), Advances in \nKnowledge Discovery and Data Mining  (pp. 160–172). Heidel -\nberg: Springer, Berlin Heidelberg, Berlin.\nCarlson, J., Daehler, K., Alonzo, A., Barendsen, E., Berry, A., \nBorowski, A., et al. (2019). The Refined Consensus Model of \nPedagogical Content Knowledge. In A. Hume, R. Cooper, & A. \nBorowski (Eds.), Repositioning Pedagogical Content Knowledge \nin Teachers’ Professional Knowledge. Singapore: Springer.\nCarpenter, D., Geden, M., Rowe, J., Azevedo, R., & Lester, J. (2020). \nAutomated Analysis of Middle School Students’ Written Reflec-\ntions During Game-Based Learning. In I. I. Bittencourt, M.  \nCukurova, K. Muldner, R. Luckin, & E. Millán (Eds.), Artificial \nIntelligence in Education  (pp. 67–78). Cham: Springer Interna -\ntional Publishing.\nChan, K. K. H., Xu, L., Cooper, R., Berry, A., & van Driel, J. H. \n(2021). Teacher noticing in science education: do you see what \nI see? Studies in Science Education, 57(1), 1–44. https:// doi. org/  \n10. 1080/ 03057 267. 2020. 17558 03\nClifton, R. A., & Roberts, L. W. (1993). Authority in classrooms. Scar-\nborough, ON: Prentice-Hall.\nConway, M. A.(2009). Episodic memories. Neuropsychologia ,  \n47(11), 2305–2313. Retrieved from https:// www. scien cedir ect. \ncom/ scien ce/ artic le/ pii/ S0028 39320 90006 45. https:// doi. org/ 10.  \n1016/j. neuro psych ologia. 2009. 02. 003\nCrespo, S. (2000). Seeing More Than Right and Wrong Answers: \nProspective Teachers’ Interpretations of Students’ Mathematical \nWork. Journal of Mathematics Teacher Education,  3, 155–181.\nDarling-Hammond, L. (2000). Teacher Quality and Student Achieve -\nment: A Review of State Policy Evidence. Education Policy \nAnalysis,  8(1), 1–44.\n511Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: \nPre-training of Deep Bidirectional Transformers for Language \nUnderstanding. arXiv 1810.04805 .\nFenstermacher, G. (1994). Chapter 1: The Knower and the Known: \nThe Nature of Knowledge in Research on Teaching. Review of \nResearch in Education , 20.\nFurtak, E. M. (2012). Linking a learning progression for natural \nselection to teachers’ enactment of formative assessment. Jour-\nnal of Research in Science Teaching,  49(9), 1181–1210. https:// \ndoi. org/ 10. 1002/ tea. 21054\nGoldberg, Y. (2017). Neural Network Methods for Natural Lan -\nguage Processing . Morgan and Claypool: Synthesis Lectures \non Human Language Technologies.\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning . \nMIT Press, Cambridge, Massachusetts and London, England. \nRetrieved from http://  www. deepl earni ngbook.  org/\nHalevy, A., Norvig, P., & Pereira, F. (2009). The Unreasonable Effec-\ntiveness of Data. IEEE Intelligent Systems , pp 8–12.\nHammer, D., & van Zee, E. (2006). Seeing the science in children’s \nthinking: Case studies of student inquiry in physical science . \nPortsmouth, NH: Heinemann Educational Books.\nHao, K. (2019). The AI technique that could imbue machines with \nthe ability to reason: Yann LeCun, Facebook’s chief AI scien -\ntist, believes unsupervised learning will bring about the next AI \nrevolution: MIT Technology Review.\nHume, A. (2009). Promoting higher levels of reflective writing in \nstudent journals. Higher Education Research & Development,  \n28(3), 247–260.\nJordan, M. I., & Mitchell, T. M. (2015). Machine learning: Trends, \nperspectives, and prospects. Science (New York, NY), 349(6245), \n255–260. https://  doi. org/ 10. 1126/ scien ce. aac45 20\nJurafsky, D. (2003). Probabilistic Modeling in Psycholinguistics: \nLinguistic Comprehension and Production. In J. Hay, R. Bod, \n& S. Jannedy (Eds.), Probabilistic linguistics  (pp. 39–95). Cam-\nbridge, MA: MIT Press.\nJurafsky, D., & Martin, J. H. (2014). Speech and language process -\ning (2nd ed.). Pearson Education, Harlow: Always learning.\nKahana, M. J., Howard, M. W., & Polyn, S. M. (2008). Associative \nRetrieval Processes in Episodic Memory. Psychology , 3.\nKleinknecht, M., & Gröschner, A. (2016). Fostering preservice teach-\ners’ noticing with structured video feedback: Results of an online- \nand video-based intervention study. Teaching and Teacher Educa-\ntion, 59, 45–56. https:// doi. org/ 10. 1016/j. tate. 2016. 05. 020\nKorthagen, F. A. (1999). Linking Reflection and Technical Compe -\ntence: the logbook as an instrument in teacher education. Euro-\npean Journal of Teacher Education,  22(2–3), 191–207. https:// \ndoi. org/ 10. 1080/ 02619 76899 020191\nKriegel, H. P., Kröger, P., Sander, J., & Zimek, A. (2011). Density-\nbased clustering. WIREs Data Mining and Knowledge Discovery, \n1(3), 231–240. https:// doi. org/ 10. 1002/ widm. 30\nKrippendorff, K. (2004). Reliability in Content Analysis: Some Com -\nmon Misconceptions and Recommendations. Human Communica-\ntion Research, 30(3), 411–433.\nKrstovski, K., & Blei, D. M. (2018). Equation Embeddings. arXiv.\nKrüger, D., Parchmann, I., & Schecker, H. (Eds.). (2014). Methoden in \nder naturwissenschaftsdidaktischen Forschung . Berlin and Hei -\ndelberg: Springer Spektrum.\nLamb, R., Hand, B., & Kavner, A. (2021). Computational Modeling of \nthe Effects of the Science Writing Heuristic on Student Critical \nThinking in Science Using Machine Learning. Journal of Sci -\nence Education and Technology, 30(2), 283–297. https:// doi. org/ \n10. 1007/ s10956- 020- 09871-3\nLevin, D. M., Hammer, D., & Coffey, J. E. (2009). Novice Teachers’ \nAttention to Student Thinking. Journal of Teacher Education,  \n60(2), 142–154. https:// doi. org/ 10. 1177/ 00224 87108 330245\nLuna, M. J., Selmer, S. J., & Rye, J. A. (2018). Teachers’ Noticing \nof Students’ Thinking in Science Through Classroom Artifacts: \nIn What Ways Are Science and Engineering Practices Evident?  \nJournal of Science Teacher Education, 29(2), 148–172. https:// doi.  \norg/ 10. 1080/ 10465 60X. 2018. 14274 18\nMarsland, S. (2015). Machine learning: An algorithmic perspective, \nsecond edition edn. Chapman & Hall / CRC machine learning & \npattern recognition series, CRC Press, Boca Raton, FL. Retrieved \nfrom http:// proqu est. tech. safar ibook sonli ne. de/ 97814 66583 283\nMena-Marcos, J., García-Rodríguez, M. L., & Tillema, H. (2013). \nStudent teacher reflective writing: what does it reveal? European \nJournal of Teacher Education, 36(2), 147–163. https:// doi. org/ 10. \n1080/ 02619 768. 2012. 713933\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient \nEstimation of Word Representations in Vector Space. arXiv \n(1301.3781v3).\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). \nDistributed Representations of Words and Phrases and their Com-\npositionality. NIPS, 13, 3111–3119.\nMitchell, M. (2020). Artificial Intelligence: A guide for thinking \nhumans. Pelican Books.\nMunoz-Najar Galvez, S., Heiberger, R., & McFarland, D. (2020). \nParadigm Wars Revisited: A Cartography of Graduate Research \nin the Field of Education (1980–2010). American Educational \nResearch Journal,  57(2), 612–652. https://  doi. org/ 10. 3102/ \n 00028 31219 860511\nNehm, R. H., Ha, M., & Mayfield, E. (2012). Transforming Biol -\nogy Assessment with Machine Learning: Automated Scoring of \nWritten Evolutionary Explanations. Journal of Science Educa -\ntion and Technology,  21(1), 183–196. https://  doi. org/ 10. 1007/ \ns10956- 011- 9300-9\nOdden, T. O. B., Marin, A., & Caballero, M. D. (2020). Thematic \nanalysis of 18 years of physics education research conference \nproceedings using natural language processing. Physical Review \nPhysics Education Research , 16(1). https://  doi. org/ 10. 1103/ \n PhysR evPhy sEduc Res. 16. 010142\nOdden, T. O. B., Marin, A., & Rudolph, J. L. (2021). How has Science \nEducation changed over the last 100 years? An analysis using \nnatural language processing. Science Education, 105(4), 653–680. \nhttps:// doi. org/ 10. 1002/ sce. 21623\nPutnam, R. T., & Borko, H. (2000). What Do New Views of Knowledge \nand Thinking Have to Say about Research on Teacher Learning? \nEducational Researcher, 29(1), 4–15.\nRauf, I. A. (2021). Physics of Data Science and Machine Learning. \nCRC Press, Boca Raton,. https:// doi. org/ 10. 1201/ 97810 03206 743\nRosenberg, J. M., & Krist, C. (2020). Combining Machine Learn -\ning and Qualitative Methods to Elaborate Students’ Ideas About \nthe Generality of their Model-Based Explanations. Journal of \nScience Education and Technology . https://  doi. org/ 10. 1007/ \ns10956- 020- 09862-4\nRoth, K. J., Garnier, H. E., Chen, C., Lemmens, M., Schwille, K., & \nWickler, N. I. Z. (2011). Videobased lesson analysis: Effective \nscience PD for teacher and student learning. Journal of Research \nin Science Teaching, 48(2), 117–148.\nRuder, S. (2019). Neural Transfer Learning for Natural Language \nProcessing: Dissertation. Ireland: National University of Ireland.\nRumelhart, D. E., Hinton, G., & Williams, R. J. (1986). Learning rep-\nresentations by back-propagating errors. Nature, 323, 533–536.\nSeidel, T., & Stürmer, K. (2014). Modeling and Measuring the Struc -\nture of Professional Vision in Preservice Teachers. American \nEducational Research Journal,  51(4), 739–771. https:// doi. org/ \n10. 3102/ 00028 31214 531321\nSherin, B. (2013). A Computational Study of Commonsense Science: \nAn Exploration in the Automated Analysis of Clinical Interview \nData. Journal of the Learning Sciences,  22(4), 600–638. https:// \ndoi. org/ 10. 1080/ 10508 406. 2013. 836654\n512 Journal of Science Education and Technology  (2022) 31:490–513\n1 3\nSherin, M. G., & Han, S. Y. (2004). Teacher learning in the context of \na video club. Teaching and Teacher Education, 20(2), 163–183. \nhttps:// doi. org/ 10. 1016/j. tate. 2003. 08. 001\nSherin, M. G., & van Es, E. A. (2009). Effects of Video Club Participa-\ntion on Teachers’ Professional Vision. Journal of Teacher Educa-\ntion, 60(1), 20–37. https:// doi. org/ 10. 1177/ 00224 87108 328155\nShulman, L. S. (1986). Those Who Understand: Knowledge Growth in \nTeaching. Educational Researcher, 15(2), 4–14.\nSinger, J. D. (2019). Reshaping the Arc of Quantitative Educational \nResearch: It’s Time to Broaden Our Paradigm. Journal of \nResearch on Educational Effectiveness,  12(4), 570–593. https:// \ndoi. org/ 10. 1080/ 19345 747. 2019. 16588 35\nStar, J. R., & Strickland, S. K. (2008). Learning to observe: using video \nto improve preservice mathematics teachers’ ability to notice. \nJournal of Mathematics Teacher Education,  11(2), 107–125. \nhttps:// doi. org/ 10. 1007/ s10857- 007- 9063-7\nTaher Pilehvar, M., & Camacho-Collados, J. (2020). Embeddings in \nNatural Language Processing: Theory and Advances in Vector \nRepresentation of Meaning. Morgan and Claypool.\nTalanquer, V., Bolger, M., & Tomanek, D. (2015). Exploring prospec-\ntive teachers’ assessment practices: Noticing and interpreting \nstudent understanding in the assessment of written work. Journal \nof Research in Science Teaching, 52(5), 585–609. https:// doi. org/ \n10. 1002/ tea. 21209\nUllmann, T. D. (2019). Automated Analysis of Reflection in Writing: \nValidating Machine Learning Approaches. International Journal \nof Artificial Intelligence in Education, 29(2), 217–257. https:// doi. \norg/ 10. 1007/ s40593- 019- 00174-2\nvan Es, E., & Sherin, M. G. (2002a). Learning to notice: scaffolding \nnew teachers’ interpretations of classroom interactions. Journal \nof Technology and Teacher Education, 10(4), 571–596.\nvan Es, E., & Sherin, M. G. (2002b). Learning to notice: Scaffolding \nnew teachers’ interpretations of classroom interactions. Journal \nof Technology and Teacher Education, 10(4), 571–596.\nvon Aufschnaiter, C., Fraij, A., & Kost, D. (2019). Reflexion und \nReflexivität in der Lehrerbildung: 144-159 Seiten / Heraus -\nforderung Lehrer\\_innenbildung - Zeitschrift zur Konzeption, \nGestaltung und Diskussion, Bd. 2 Nr. 1 (2019): Herausforder -\nung Lehrer\\_innenbildung - Ausgabe 2. https:// doi. org/ 10. 4119/ \nUNIBI/ HLZ- 144\nWang, C., Liu, X., & Song, D. (2020). Language Models are Open \nKnowledge Graphs. arXiv.\nWigner, E. P. (1960). The unreasonable effectiveness of mathematics \nin the natural sciences. Richard courant lecture in mathematical \nsciences delivered at New York University, May 11, 1959. Com-\nmunications on Pure and Applied Mathematics , 13(1),1–14. \nhttps:// doi. org/ 10. 1002/ cpa. 31601 30102\nWilson, C. D., Borowski, A., & van Driel, J. H. (2019). Perspectives on \nthe Future of PCK Research in Science Education and Beyond. In \nA. Hume, R. Cooper, & A. Borowski (Eds.), Repositioning Peda-\ngogical Content Knowledge in Teachers’ Professional Knowledge \n(pp. 289–300). Singapore: Springer.\nWulff, P., Buschhüter, D., Nowak, A., Westphal, A., Becker, L., \nRobalino, H., et al. (2020). Computer-Based Classification of \nPreservice Physics Teachers’ Written Reflections. Journal of \nScience Education and Technology . https://  doi. org/ 10. 1007/ \ns10956- 020- 09865-1\nWulff, P., Mientus, L., Nowak, A., & Borowski, A. (2022). Utiliz -\ning a Pretrained Language Model (BERT) to Classify Preservice \nPhysics Teachers’ Written Reflections. International Journal \nof Artificial Intelligence in Education . https:// doi. org/ 10. 1007/ \ns40593- 022- 00290-6\nXing, W., Lee, H. S., & Shibani, A. (2020). Identifying patterns in \nstudents’ scientific argumentation: content analysis through text \nmining using Latent Dirichlet Allocation. Educational Tech -\nnology Research and Development . https://  doi. org/ 10. 1007/ \ns11423- 020- 09761-w\nZehner, F., Sälzer, C., & Goldhammer, F. (2016). Automatic Coding of \nShort Text Responses via Clustering in Educational Assessment. \nEducational and Psychological Measurement,  76(2), 280–303. \nhttps:// doi. org/ 10. 1177/ 00131 64415 590022\nZhai, X. (2021). Practices and Theories: How Can Machine Learning \nAssist in Innovative Assessment Practices in Science Education. \nJournal of Science Education and Technology. https:// doi. org/ 10. \n1007/ s10956- 021- 09901-8\nZhai, X., Haudek, K., Shi, L., Nehm, R., & Urban-Lurain, M. (2020). \nFrom substitution to redefinition: A framework of machine learn-\ning-based science assessment. Journal of Research in Science \nTeaching, 57(9), 1430–1459. https:// doi. org/ 10. 1002/ tea. 21658\nZhai, X., Yin, Y., Pellegrino, J. W., Haudek, K. C., & Shi, L. (2020). \nApplying machine learning in science assessment: a systematic \nreview. Studies in Science Education, 56(1), 111–151. https:// doi. \norg/ 10. 1080/ 03057 267. 2020. 17357 57\nZhang, X., Ramachandran, D., Tenney, I., Elazar, Y., & Roth, D. \n(2020). Do Language Embeddings Capture Scales? arXiv.\nPublisher’s Note  Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\n513Journal of Science Education and Technology  (2022) 31:490–513\n1 3",
  "topic": "Cluster analysis",
  "concepts": [
    {
      "name": "Cluster analysis",
      "score": 0.68874591588974
    },
    {
      "name": "Science education",
      "score": 0.5695246458053589
    },
    {
      "name": "Bridging (networking)",
      "score": 0.548199474811554
    },
    {
      "name": "Computer science",
      "score": 0.5280246138572693
    },
    {
      "name": "Mathematics education",
      "score": 0.4854731857776642
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35864323377609253
    },
    {
      "name": "Psychology",
      "score": 0.32606184482574463
    },
    {
      "name": "Natural language processing",
      "score": 0.32594722509384155
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I223822909",
      "name": "Heidelberg University",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210086570",
      "name": "Heidelberg University of Education",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I176453806",
      "name": "University of Potsdam",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I36522303",
      "name": "Universität Greifswald",
      "country": "DE"
    }
  ]
}