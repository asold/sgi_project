{
    "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models",
    "url": "https://openalex.org/W3182696977",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4223987517",
            "name": "Aghajanyan, Armen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4225993289",
            "name": "Okhonko, Dmytro",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2464276071",
            "name": "Lewis Mike",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4213921652",
            "name": "Joshi, Mandar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2025973412",
            "name": "Xu Hu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2745332887",
            "name": "Ghosh, Gargi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2751234931",
            "name": "Zettlemoyer, Luke",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3164972323",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2950700477",
        "https://openalex.org/W2396767181",
        "https://openalex.org/W2996264288",
        "https://openalex.org/W2946659172",
        "https://openalex.org/W3170305303",
        "https://openalex.org/W2732004306",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W1763968285",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W1544827683",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W3118216348",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W3137573489",
        "https://openalex.org/W2983040767",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W3127622310",
        "https://openalex.org/W2956105246",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W2563351168",
        "https://openalex.org/W2149327368",
        "https://openalex.org/W3119438769",
        "https://openalex.org/W3091156754",
        "https://openalex.org/W2133512280",
        "https://openalex.org/W3105238007",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W3039127676",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3026404337",
        "https://openalex.org/W3167602185",
        "https://openalex.org/W2786660442",
        "https://openalex.org/W3085177480",
        "https://openalex.org/W2899386490",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2293778248"
    ],
    "abstract": "We introduce HTLM, a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. class and id attributes often encode document category information), and (3) it allows for new structured prompting that follows the established semantics of HTML (e.g. to do zero-shot summarization by infilling title tags for a webpage that contains the input text). We show that pretraining with a BART-style denoising loss directly on simplified HTML provides highly effective transfer for a wide range of end tasks and supervision levels. HTLM matches or exceeds the performance of comparably sized text-only LMs for zero-shot prompting and fine-tuning for classification benchmarks, while also setting new state-of-the-art performance levels for zero-shot summarization. We also find that hyper-text prompts provide more value to HTLM, in terms of data efficiency, than plain text prompts do for existing LMs, and that HTLM is highly effective at auto-prompting itself, by simply generating the most likely hyper-text formatting for any available training data. We will release all code and models to support future HTLM research.",
    "full_text": "arXiv:2107.06955v1  [cs.CL]  14 Jul 2021\nHTLM: Hyper-T ext Pre-T raining and Prompting of Language Mo dels\nArmen Aghajanyan 1\n, ∗ Dmytro Okhonko 1\n, ∗ Mike Lewis 1\n, Mandar Joshi 1,2\n,\nHu Xu 1\n, Gargi Ghosh 1\n, Luke Zettlemoyer 1,2\n1Facebook AI 2 University of W ashington\n{armenag,oxo,mikelewis,mandarj,huxu,gghosh,lsz}@fb.com\nAbstract\nW e introduce HTLM, a hyper-text language\nmodel trained on a large-scale web crawl.\nModeling hyper-text has a number of advan-\ntages: (1) it is easily gathered at scale, (2)\nit provides rich document-level and end-task-\nadjacent supervision (e.g. class and id at-\ntributes often encode document category infor-\nmation), and (3) it allows for new structured\nprompting that follows the established seman-\ntics of HTML (e.g. to do zero-shot summariza-\ntion by inﬁlling <title> tags for a webpage\nthat contains the input text). W e show that\npretraining with a BAR T -style denoising loss\ndirectly on simpliﬁed HTML provides highly\neffective transfer for a wide range of end\ntasks and supervision levels. HTLM matches\nor exceeds the performance of comparably\nsized text-only LMs for zero-shot prompting\nand ﬁne-tuning for classiﬁcation benchmarks,\nwhile also setting new state-of-the-art perfor-\nmance levels for zero-shot summarization. W e\nalso ﬁnd that hyper-text prompts provide more\nvalue to HTLM, in terms of data efﬁciency,\nthan plain text prompts do for existing LMs,\nand that HTLM is highly effective at auto-\nprompting itself, by simply generating the\nmost likely hyper-text formatting for any avail-\nable training data. W e will release all code and\nmodels to support future HTLM research.\n1 Introduction\nThe vast majority of text used to pretrain lan-\nguage models is extracted from web pages, while\ndiscarding any markup they contain (\nLiu et al. ,\n2019; Brown et al. , 2020; Raffel et al. , 2019;\nLewis et al. , 2019). W e argue that this HTML\nshould not be ignored; it enables new forms of\nhighly effective language model pretraining and\n∗ Equal Contribution\n<!DOCTYPE html>\n<html>\n<title> <mask>12 </title>\n<body>\n˜ south korea on monday announced sweeping\ntax reforms , including income and\ncorporate tax cuts to boost growth by\nstimulating sluggish private\nconsumption and business investment .\n</body>\n</html>\n↓\n<!DOCTYPE html>\n<html>\n<title> ˜ South Korea Announces Tax Reforms To\nBoost Economic Growth ˜ </title>\n<body>\n˜ south korea on monday announced sweeping\ntax reforms...\n</body>\n</html>\nFigure 1: An example structured prompt for a simple\nsummarization task, where we ask a generative masked\nlanguage model to generate a mask representing the ti-\ntle with an average tokens size of 12.\nprompting with structured document-level super-\nvision.\nHyper-text, such as the HTML found in the\nCommon Crawl\n1 , has a number of advantages for\npretraining over plain text. It often encodes high-\nlevel properties of different parts of the documents,\nwhich are difﬁcult to infer from the text alone.\nFor example, <title> elements can be excellent\nsummaries of the <body> of a document, while\nelement class and id attributes can encode cate-\ngorical properties of documents. Such supervision\nis highly diverse, depending on what the website\nauthors choose to present, and provides close prox-\nies for many NLP tasks we aim to later solve.\nModeling hyper-text allows us to introduce\nstructured prompting of language models. W e de-\nsign prompts that incorporate the established se-\nmantics of HTML to better control for the de-\nsired model output. This includes, for exam-\n1 https://commoncrawl.org/\nple, performing zero-shot summarization by ask-\ning the model to inﬁll <title> tags in a web\npage. And, the fact that we jointly model text\nand hyper-text formatting also allows for effective\nauto-prompting. If we have even a few examples\nfor a new task, we can directly ask the model to\nformat them in HTML, and templatize the result\nto deﬁne the new prompt.\nOur HyperText Language Model ( HTLM) is\ntrained on 23TB of simpliﬁed HTML which we\nautomatically extract from common crawl dumps\n(see Section §\n2.1). W e use a modiﬁed BART\ndenoising objective ( Lewis et al. , 2019) that ran-\ndomly masks spans of hyper-text and aims to re-\nconstruct the original input. W e extend the origi-\nnal masking with a new size hint scheme, where\neach mask is associated with an integer that pro-\nvides a noisy hint for the size of the masked text,\nto allow for more ﬁne grained task-speciﬁc length\npriors when prompting the ﬁnal model (see Sec-\ntion §\n2.3). Figure 1 shows an example mask that\nshould be reconstructed with a phrase that contains\nroughly 12 tokens.\nThrough extensive experiments, we show that\nour HTLM achieves highly effective transfer for\na wide range of end tasks and supervision levels.\nIt matches or exceeds the performance of compa-\nrably sized text-only LMs for zero-shot prompt-\ning and full ﬁne-tuning on GLUE, while also set-\nting new state-of-the-art performance levels for\nzero-shot summarization with a gain of up to 8\nROUGE-1 points. It also allows few shot learning\nfor problems that are less easily reduced to text-\nonly inputs, such table to text generation. Follow-\ning methodology introduced by\nLe Scao and Rush\n(2021), we further ﬁnd that hyper-text prompts\nprovide more data efﬁciency to the HTLM model\nthan plain text prompts do for existing LMs, being\neffectively equivalent to having up to a thousand\nextra training examples. Finally , we see that the\nHTLM model is highly effective at auto-prompting\nitself, in some cases rivaling the performance of\nmanually engineered prompts.\nIn summary , our contributions include:\n• W e present the ﬁrst hyper-text language\nmodel ( HTLM), trained on 23TB of simpliﬁed\nHTML data from the common crawl.\n• Our new hyper-text prompting scheme\nuses both the well-established semantics of\nHTML and new size hints on prompt masks\nto provide more ﬁne-grained control of new\ntask speciﬁcations.\n• W e demonstrate consistently strong transfer\nfrom HTLM to a range of tasks at differing\nsupervision levels, including improving the\nbest-known zero-shot summarization num-\nbers by up to 8 ROUGE-1 points.\n• Following\nLe Scao and Rush (2021), our data\nefﬁciency analysis shows that hyper-text\nprompts are worth more to the HTLM model\nthan plain text prompts are for existing LMs,\nbeing effectively equivalent to having up to a\nthousand extra training examples.\n• W e demonstrate the HTLM directly supports\nauto prompting for new tasks, by simply ask-\ning it to format any available examples in\nHTML, often rivaling or surpassing previous\nmanually engineered prompts.\n• W e release all code and models to support fu-\nture HTLM research.\n2 HyperT ext Language Model ( HTLM)\nHTLM is trained on a large corpus of simpliﬁed\nHTML, which is automatically extracted from the\ncommon crawl (Section §\n2.1). W e use a BART -\nstyle denoising autoencoder with span masking\n(Section § 2.2), extended to allow size hints during\nreconstruction of the original text (Section § 2.3).\n2.1 Minimal HTML\nAlthough HTML contains supervision signals to\nnatural language, the majority of HTML in a mod-\nern web page does not provide any signiﬁcant\nform of supervision for pretraining. For example,\na large portion of a webpage is JavaScript code or\nCSS, which provides more aesthetics to the page\nrather than document-level information. Coupling\nthis with the challenges of training transformers on\nvery long sequence lengths (\nChoromanski et al. ,\n2020; W ang et al. , 2020; Beltagy et al. , 2020), it\nwas important to automatically convert web pages\nto a simpliﬁed form, which we call Minimal-\nHTML (MHTML), as deﬁned below .\nW e remove all sub-trees of the HTML DOM 2\nwhich do not contain textual elements of a certain\ncharacter size (128 for standard textual elements,\n2 The DOM or Document Object Model is an interface that\ntreats an HTML document as a tree structure wherein each\nnode is an object representing a part of the document.\n64 for lists/tables/spans). W e also ﬁlter out all\nheaders, footers, copyrights, forms, and iFrames.\nW e fold consecutive <div> elements into a sin-\ngular <div> element with merged attributes. W e\nalso remove all attributes which are not class or\nid attributes. Lastly , we skip all MHTML docu-\nments whose ratio of text to HTML is not greater\nthan 0.46. Particularly we noticed that MHTML\ndocuments whose ratio of text to HTML is low , the\naverage quality of the document tends to be lower\nas well. W e found these numbers by visually in-\nspecting a set of Common Crawl (CC) documents\nafter application of aforementioned transforms en-\nsuring both a high quality of kept documents while\nalso not ﬁltering too large amount of data. Further-\nmore we ﬁlter out all documents who have a lang\nattribute that is not set to en.\nApplying these deterministic transformations\nremoves on average 94% of characters from a raw\nwebpage while maintaining the general markup of\nthe document. Furthermore, it allowed close to\n85% of MHTML documents to ﬁt into 1024 BPE\ntokens; the maximum token length for BART and\nmany other existing language models.\nOne by-product of this type of ﬁltering is that\nit also produced high-quality documents by de-\nfault\n3 ; thus, we opted out of model-based ﬁlter-\ning of documents such as CC-100 ( Conneau et al. ,\n2019). W e used the January 2021 snapshot of\nCommon Crawl, which provided us with 23 T er-\nabytes of MHTML text after ﬁltering.\n2.2 Model\nW e adopt a BART -style denoising auto-\nencoder (\nLewis et al. , 2019) for several reasons.\nW e want to predict arbitrary substrings within the\nMHTML, conditioned on the rest of the document.\nThis allows us to equally easily (1) use masks\nduring prompting to mark where to generate\ntext associated with model outputs within a web\npage, and (2) automatically generate prompts\nby wrapping plain text training examples in\nmasks that allow the model to mark them up by\ngenerating MHTML formatting. W e also do not\nknow in advance exactly how much text needs to\nbe generated in each case, thereby ruling out the\nuse of more traditional masked language models.\n3 Much of the noise in existing text collections derived\nfrom the common crawl comes from artifacts that are intro-\nduced when returning the text in the relatively arbitrary or -\nder it appeared in the original HTML, before the markup was\nstripped.\nFor all of our experiments, we adopt the same ar-\nchitecture as BART -Large and initialized our mod-\nels with the BART -Large checkpoint. This model\nhas roughly 400 million parameters.\nW e trained our augmented BART model for a to-\ntal of 330,000 steps on 256 GPUs with an effective\nbatch size of 8192. W e initialize our model with\nthe original BART -Large model. W e train using\nthe Adam optimizer (\nKingma and Ba , 2014) and\na polynomial decay learning rate scheduler with a\npeak learning rate of 4e−5 and 10, 000 warm-up\nsteps.\nW e do not use the sentence shufﬂing from the\noriginal BART objective, and select a Poisson λ\nof 3.5 for sampling span lengths for masking. W e\nset dropout in the attention to 0.1 for the ﬁrst 170k\nsteps, reducing it to 0.0 thereafter. W e also ﬁlter\nout data to only English ( en) after 170k steps us-\ning FastT ext (\nJoulin et al. , 2016). W e noticed the\nperplexity plateaued around 170k steps which is\nwhy we simplify the learning process by remov-\ning dropout and applying stronger ﬁltering of the\nEnglish language.\n2.3 Size Hints\nBART allows each mask to be replaced with mul-\ntiple tokens during the reconstruction. During pre-\ntraining, BART masks a span with the length sam-\npled from a Poisson distribution; thus, the model\nmust learn to implicitly predict the length of the\nmasked text. A fundamental problem we encoun-\ntered when trying to use standard BART for zero-\nshot generative prompting is the inability to con-\ntrol the length of the generated text for each mask,\neven when using various decoding strategies like\nlength penalties.\nT o allow for more control, we augment BART’s\nmasking scheme by introducing size hints. Specif-\nically , we tokenize the noisy estimate of the length\nof a span directly and insert it right after the span\nmask token. For example, given the correct mask\nlength m, we insert n ⟨mask⟩ tokens where n is\nmax (1, ⌊N (m, m ∗ ǫ)⌋) and ǫ is a hyperparam-\neter representing how noisy we want these size\nhints to be. By optionally injecting size hints, we\ncan prompt the model to generate text of roughly\nsome speciﬁc length, or by not injecting size hints,\nwe allow the model to model the mask size implic-\nitly . W e give size-hints to 80% of masks with the\nnoisiness of size hints ǫ = 0 .1.\nW e provide an example of the beneﬁts of size\nhints in generation in T able 1.\n3 HTML-based Prompting\nW e use the HTML-based prompting scheme for\na range of generation and classiﬁcation tasks.\nBroadly , we use HTML templates–either selected\nmanually or generated by the model itself by auto-\nprompting–to specify the HTML structure of the\ntask. The template is then instantiated with the\ntask input and placeholder mask tokens for the out-\nput. The model uses this instantiated template as\na prompt. Because BART models reconstruct the\nfull input, we rely on simple heuristics to match\nthe preﬁx/sufﬁx around any masks and extract the\nﬁnal output.\n3.1 Generation Prompting Policies\nGiven that we have optional size hints for masks, a\nsingle prompt can generate a wide variety of text;\ntherefore, we discuss multiple policies to select the\nprompted results. W e can decide not to utilize size\nhints at all and thus remove the need to use any\npolicies, but this comes at the cost of template ro-\nbustness. Without size hints, a template not only\nhas to express the semantics of the task, but also\nneeds to match the average target length as well;\nsuch prompts are brittle and require careful man-\nual design. However, using hints allows us to de-\ncouple generation length from the prompt, greatly\nimproving template reuse across related tasks. It is\nalso possible that for a prompt and a speciﬁc sub-\nset of the data, HTLM will not generate an output\nfrom which we can programmatically extract the\ngenerated mask; therefore, our policies for size-\nhints also mitigate this issue.\nFor every generation task, we ﬁrst construct a\nprompt that can generate the correct text semanti-\ncally , and then we provide size hints equal to the\naverage target of a subset of the training set, ¯s. If,\nfor a particular input, we are not able to extract\na value, we run HTLM on the same prompt, but\nwith our size hint set to ¯s ± iǫ¯s, from which we se-\nlect the output with the lowest perplexity , we con-\ntinue this process at most ﬁve times where i rep-\nresents the current index of the policy . If we still\ncannot ﬁnd a valid generated answer, we fall back\non the auto-template described in the next section.\nIn experiments, we denote HTLM-Manual-NS (not\nsized) as our manually engineered prompt with no\nsize hint, while HTLM-Manual-S uses the policy\ndeﬁned here for all generation benchmarks.\n3.2 Auto-Prompting\nT o avoid manually engineering prompts, we\nalso explore automatic generation of structured\nprompts. By training on hypertext, HTLM can\nlearn high-level document semantics that we ex-\nploit for prompt creation. W e generate prompting\ntemplates by asking the model to recover docu-\nment markups. Speciﬁcally , we place ⟨mask⟩ to-\nkens around every independent block of data (e.g.\nsummary/article).\nW e provide an example of auto-prompting\nfor a sample from the Gigaword summarization\ndataset (\nNapoles et al. , 2012) with the respective\nmasking in Figure 2 . For our generation exper-\niments, we denote HTLM-Auto-NS (not-sized) as\nthe auto-prompt without using size hints, where\nHTLM-Auto-S uses the size hints based policy de-\nscribed in the previous section.\nW e found that HTLM auto-prompting was less\neffective for classiﬁcation tasks. W e hypothesize\nthat this is because generative targets carry signif-\nicantly more information than a simple binary tar-\nget token.\n4 Zero/One-Shot Prompting\nPerez et al. (2021) argue that zero/few-shot learn-\ning cannot happen when prompts are created by\ntuning on a large amount of development data.\nT o mitigate for this issue all the manual prompts\nused throughout our experiments are either de-\nrived from related papers or developed using a\nmaximum of ﬁfty samples from the train set.\n4.1 Generation\nW e evaluate HTLM on summarization, a prototypi-\ncal generation task. For all summarization bench-\nmarks, we use ROUGE-1/2/L as our primary met-\nrics to stay consistent with other literature (\nLin,\n2004).\nFurthermore we benchmark HTLM on a set\nof three standard natural language generation\ntasks. W e utilize the ofﬁcial benchmarking scripts\nprovided which report BLEU (\nPapineni et al. ,\n2002), NIST ( Belz and Reiter , 2006), METEOR\n(Lavie and Agarwal , 2007), ROUGE-L ( Lin,\n2004), CIDEr ( V edantam et al. , 2015) and TER\n(Snover et al. , 2005). W e use Li and Liang (2021)\nfor our baselines, and present preﬁx tuning results\nwith 0.1% of parameters as well.\nGigaword consists of headlines from news arti-\ncles ( Napoles et al. , 2012). The target summaries\nPrompt Size Hint\n(X)\nHTLM Output\n<html id=\"cnn_dailymail\" lang=\"en\" xml:lang=\"en\">\n<head class= \"pg-headline\" data-act-id=\"article_head_0\">\n<p> <mask>X --CNN</p>\n</head>\n<body>\n<p>\nHowever, observers inside the court said the\nprosecution evidence was difficult to follow\nor to understand. Correspondents said the most\nprominent video clip was by another\nal-Jazeera journalist who is not on trial. The\nthree, including ex-BBC reporter Peter Greste\n, deny spreading false news and helping the\nMuslim Brotherhood. They appeared in the Cairo\ncourt on Tuesday along with other Egyptian\nstudents and activists being tried with them.\nDefence lawyers complained they were unable to\nunderstand some of the recordings being\nplayed because of poor technical quality.\nWhile the recordings were played, defendants\ntalked among themselves in the caged dock. The\ntrial was later adjourned until 3 May and\nbail for the defendants was denied, reports\nfrom the court said. Peter Greste, producer\nBaher Mohamed and al-Jazeera English's\nCanadian-Egyptian bureau chief Mohamed Fadel\nFahmy have been held since December. A fourth\nal-Jazeera reporter, Abdullah Elshamy, who\nworks for the network's Arabic channel, has\nbeen detained since August but not charged.\nEgyptian authorities accuse the journalists of\naiding the Islamist Muslim Brotherhood, which\nhas been banned as a terrorist group.\n</p>\n</body>\n</html>\n5 Three Egyptian journalists are\nbeing tried –CNN\n8 Three British journalists ac-\ncused of aiding the Muslim\nBrotherhood are tried –CNN\n12 Three British journalists ac-\ncused of aiding the Muslim\nBrotherhood are on trial –\nCNN\n15 Three British-Egyptian jour-\nnalists accused of aiding the\nMuslim Brotherhood are to be\ntried in a Cairo court –CNN\nT able 1: W e provide a simple example using our CNN/DM prompt w here by altering the Size Hint value (X) we\nget summaries of varied length and complexity.\n<mask>\nus rejects charges against its ambassador in\nbolivia\n<mask>\n<mask>\nthe us state department said wednesday it had\nreceived no formal word from bolivia that it\nwas ...\n<mask>\nHTLM− − − →\n<html lang=\"en\" xml:lang=\"en\">\n<head>\n<title>\nthe us rejects charges against its\nambassador in bolivia | The\nWashington Post\n</title>\n</head>\n<body>\n<div class = \"post-body entry-content\" >\n<p> the us state department said\nwednesday it had received no\nformal word from bolivia that it\nwas ...\n</p>\n</div>\n</body>\n</html>\nFigure 2: An example of auto-prompting using a sample from th e train-set of the Gigaword dataset. HTLM places\nthe summary inside of a <title> inside of a <head> element, while placing the article in a <div> element\nwith an entry-content attribute value for attribute class which is common on news web-sites.\nare relatively short, consisting roughly on average\nof 10 BPE tokens.\nCNN/Dailymail (\nHermann et al. , 2015) provides\nmulti-sentence target summaries close to 3 sen-\ntences, or roughly 50 tokens.\nReddit TIFU (\nKim et al. , 2018) contains sum-\nmaries of Reddit posts. Speciﬁcally , we use the\nshort subset of data . Compared to our other sum-\nmarization datasets, this dataset is highly abstrac-\ntive and not based on news articles.\nXSum (\nNarayan et al. , 2018) provides abstractive\nsingle sentence summaries of news articles.\nE2E (\nNovikova et al. , 2017) is a table-to-text gen-\neration dataset containing approximately 50K sam-\nples with 8 unique ﬁelds from the restaurants do-\nmain.\nW ebNLG (\nGardent et al. , 2017) is also a struc-\ntured generation dataset containing 15 different do-\nmains from DBPedia. W e report numbers on the\nSeen (S), Unseen (U) and All (A) subsets of the\ndata.\nDART (\nNan et al. , 2020) is a open-domain struc-\ntured generation dataset containing Wikipedia ta-\nbles.\nW e manually searched for prompts for each of\nthese datasets using a maximum of 50 data\npoints from the train set to evaluate the prompts.\nFor our baseline, we compare against PEGA-\nSUS (\nZhang et al. , 2019), the current state of the\nart for zero shot summarization. PEGASUS was\nexplicitly pre-trained for summarization by mask-\ning and generating salient gap sentences from\nnews articles. W e present our results in T able\n2.\nHTLM with manual prompts ( HTLM-Manual)\nand size hints substantially improves over state-of-\nthe-art zero-shot summarization results on all four\ndatasets without any tailored pretraining. In par-\nticular, we see large improvements of more than\n8 ROUGE-L F1 for the Gigaword dataset. Fur-\nthermore, size hints-based auto-prompting ( HTLM-\nAuto-S) outperforms PEGASUS in three out of\nfour datasets. Speciﬁcally , for the Gigaword\ndataset, we outperform previous state-of-the-art\nzero-shot results from PEGASUS by roughly 6\nROUGE-L points. HTLM improvements stem\nfrom the fact that HTML-based prompting allows\nus better control over dataset-speciﬁc attributes\nsuch as length and style.\nFor NLG tasks, we required the use of a single\ntraining example to get prompting to work sufﬁ-\nciently . W e report these one-shot numbers in T a-\nble\n3. Because these tasks require structured tab-\nular inputs, it is not obvious how to prompt any\nother text-based pre-trained models. W e report\nother non-trainable baselines such as the gram-\nmar based pipeline approaches (TILB/UIT -VNU)\nin\nGardent et al. (2017). T o the best of our knowl-\nedge, these are the ﬁrst one-shot table to text, nat-\nural language generation results.\n4.2 Classiﬁcation\nFor prompting in the classiﬁcation setting, we se-\nlect 4 datasets to work with. Instead of relying on\ngenerative prompting to generate target token(s)\ndenoting the correct class, we instead rely on per-\nplexity measures over the set of all targets to se-\nlect the correct class. In other words, we select the\nclass for which the perplexity of the corresponding\ninstantiated template is the smallest.\nRTE (\nBentivogli et al. , 2009) is a textual entail-\nment task formulated as binary classiﬁcation. W e\nplace the candidate in a <div> element with the\nclass attribute set to candidate and do the same\nwith the respective hypothesis. In the third el-\nement, we utilize the prompt from\nBrown et al.\n(2020) with the class attribute set to answer.\nBoolQ (Clark et al. , 2019) is a yes/no question an-\nswering task, also formulated as binary classiﬁca-\ntion for question, passage, and answer triplets. W e\nrepresent the question as a <div> element with\nthe itemprop set to https://schema.org/Question,\npassage as a div element with class attribute pas-\nsage and answer as a div element with the item-\nprop set to https://schema.org/Answer.\nWinogrande (\nLevesque et al. , 2012) consists of\nadversarially collected Winograd Schema Chal-\nlenge (\nLevesque et al. , 2011) data. W e utilize the\nsame template as GPT -3 but place it in a QA style\ntemplate similar to BoolQ. Please refer to the Ap-\npendix for exact templates.\nHellaSwag The last dataset we evaluate is the\ncommonsense natural language inference task Hel-\nlaSwag which, due to its adversarial nature, is con-\nsidered complex (\nZellers et al. , 2019).\nModel Gigaword CNN/DM Reddit TIFU XSum\nPEGASUS-0S 23.39/07.59/20.20 32.90/13.28/29.38 14.66/3 .06/10.17 19.27/3.00/12.72\nHTLM-Auto-NS 27.56/10.17/24.57 33.40/13.45/30.10 6.71/1.98 /7.86 15.15/2.54/10.91\nHTLM-Auto-S 28.73/11.31/26.49 34.65/14.54/32.15 8.15/2.92/ 9.75 17.14/3.41/13.43\nHTLM-Manual 31.61/10.80/28.60 38.51/16.10/33.89 15.81/2.98/10.54 2 2.34/4.12/14.56\nT able 2: HTLM results on zero-shot summarization. HTLM-Manual denotes manually engineered prompts with size\nhints, while HTLM-Auto-S and HTLM-Auto-NS indicate autoprompting with and without size hint s respectively.\nMetrics shown are ROUGE-1/ROUGE-2/ROUGE-L respectively.\nE2E W ebNLG DART\nBLEU NIST MET R-L CIDEr BLEU MET TER ↓ BLEU MET TER ↓ Mover BERT BLEURT\nS U A S U A S U A\nFine-tuning\nGPT -2M E D IU M 68.2 8.62 46.2 71.0 2.47 64.2 27.7 46.5 0.45 0.30 0.38 0.33 0.76 0.53 46.2 0.39 0.46 0.50 0.94 0.39\nGPT -2L A RG E 68.5 8.78 46.0 69.9 2.45 65.3 43.1 55.5 0.46 0.38 0.42 0.33 0.53 0.42 47.0 0.39 0.46 0.51 0.94 0.40\nHTLM 70.3 8.90 46.3 70.8 2.47 65.4 48.4 55.6 0.46 0.39 0.42 0.33 0.51 0.40 47.2 0.39 0.44 0.51 0.94 0.40\nPreﬁx (0.1%)\nGPT -2M E D IU M 69.7 8.81 46.1 71.4 2.49 62.9 45.6 55.1 0.44 0.38 0.41 0.35 0.49 0.41 46.4 0.38 0.46 0.50 0.94 0.39\nGPT -2L A RG E 70.3 8.85 46.2 71.7 2.47 63.4 47.7 56.3 0.45 0.39 0.42 0.34 0.48 0.40 46.7 0.39 0.45 0.51 0.94 0.40\nHTLM 70.1 8.85 46.1 71.2 2.45 64.8 46.1 56.3 0.46 0.38 0.42 0.33 0.47 0.40 47.1 0.39 0.45 0.50 0.94 0.39\nOne-Shot\nHTLM 32.1 3.35 24.1 31.6 0.78 28.1 18.5 22.8 0.24 0.21 0.12 0.78 0.79 0.78 22.1 0.12 0.91 0.25 0.78 0.22\nBase-lines\nTILB-Pipeline - - - - - 44.34 20.65 35.29 0.38 0.21 0.30 0.48 0.64 0.56 - - - - - -\nUIT -VNU-Pipeline - - - - - 19.87 0.11 7.07 0.15 0.03 0.09 0.78 0.87 0.82 - - - - - -\nT able 3: W e evaluate GPT -2 MEDIUM , GPT -2 LARGE and HTLM on table-to-text generation on E2E (left), W ebNLG\n(middle) and DAR T (right).\nW e present our results on zero-shot classiﬁ-\ncation in T able 4. HTLM prompting of classi-\nﬁcation datasets outperforms the most compara-\nble (in terms of number of parameters) GPT -3\nMedium sized model on the majority of tasks,\nwhile approaching—and on RTE outperforming—\nthe GPT -3 Large model which consists of roughly\ndouble the amount of parameters as HTLM.\n5 Fine-tuning Experiments\nIn addition to our previous prompting results,\nwe also aim to show that HTLM learned repre-\nsentations are useful in the full ﬁnetuning set-\nting. W e compare against other pre-training\nMLM models such as RoBERT a (\nLiu et al. ,\n2019), original BART ( Lewis et al. , 2019), and\nT5 ( Raffel et al. , 2019) by ﬁnetuning on the GLUE\nbenchmark ( W ang et al. , 2018).\nDuring ﬁnetuning, instead of a simple con-\ncatenation of sentences from the train set, we\nplace the examples into prompts derived from\nLe Scao and Rush (2021). W e defer to the Ap-\npendix for the exact prompts. Given the recent ad-\nvancements in ﬁnetuning, we also report results us-\ning the recently proposed R3F method for ﬁnetun-\ning (\nAghajanyan et al. , 2020a) for both RoBERT a\nand HTLM.\nW e present our results in T able 5. Overall HTLM\nimproves over existing pre-training methods. W e\nalso note that we can improve ﬁne-tuning perfor-\nmance by placing the examples into prompts and\nﬁne-tuning the classiﬁcation head. The improve-\nments that we see in terms of prompting have no\nadverse effects on ﬁne-tuning but are rather posi-\ntive, providing further evidence that the proposed\napproach of structured pre-training is a viable al-\nternative to other methods of pre-training even for\nﬁne-tuning.\nW e also show our ﬁne-tuning results for the\ntable-to-text generation datasets in T able\n3. Sim-\nilar to GLUE ﬁne-tuning, we place all NLG sam-\nples into a prompt while ﬁne-tuning. HTLM ﬁne-\ntuned is able to outperform both variants of the\nGPT -2 model consistently .\n6 Prompt Data Efﬁciency\nWhat does the HTML-based pretraining and\nprompting scheme offer over one based on the\nplain text?\nLe Scao and Rush (2021) explored\nquantifying how many data points a single prompt\nwas worth. Speciﬁcally , they analyzed three differ-\nent task-speciﬁc settings given a pattern (the struc-\nRTE BoolQ Winogrande HellaSwag # Params\nGPT -3 63.5 60.5 70.5 78.9 175B\nGPT -3 Large 48.4 58.9 57.4 51.0 760M\nGPT -3 Med 49.8 60.3 52.1 43.6 350M\nHTLM-Manual 51.2 55.3 54.8 47.9 400M\nT able 4: Classiﬁcation accuracy with zero shot prompting. W e compare our performance to the full GPT -3 model\nas well as variants of comparable size.\nMNLI\nAcc-m/mm\nQQP\nAcc\nRTE\nAcc\nQNLI\nAcc\nMRPC\nAcc\nCoLA\nMcc\nSST -2\nAcc\n# Params\nRoBERT A 90.2/- 92.2 86.6 94.7 89.1 68.0 96.4 330M\nRoBERT a-R3F 91.1/91.3 92.4 88.5 95.3 91.6 71.2 97.0 330M\nT5-Base 87.1/86.2 89.4 80.1 93.7 87.5 51.1 95.2 220M\nT5-Large 89.9/89.6 89.9 87.2 94.8 89.9 61.2 96.3 770M\nBART -Large 89.9/90.1 92.5 87.0 94.9 90.4 62.8 96.6 400M\nHTLM 90.3/91.4 92.6 87.1 95.1 90.8 64.3 96.9 400M\nHTLM-R3F 91.4/92.1 92.8 89.1 95.4 91.5 69.4 97.1 400M\nHTLM-R3F-Prompt 91.6/91.2 92.9 89.4 95.7 91.7 69.8 97.3 400M\nT able 5: Results on the GLUE development set for various ﬁne- tuning methods applied to HTLM.\nA verage Advantage (# Training Points, P vs. H)\nMNLI BoolQ CB RTE WiC\nRoBERT a-Large 3506 ± 536 752 ± 46 90 ± 2 282 ± 34 −424 ± 74\nT5-Large 5010 ± 230 650 ± 85 150 ± 8 300 ± 65 −220 ± 20\nBART -Large 4020 ± 220 450 ± 55 125 ± 10 305 ± 25 −110 ± 45\nHTLM 6025 ± 440 855 ± 205 255 ± 35 840 ± 45 45 ± 25\nT able 6: A verage advantage (higher is better) in terms of tra ining points for ﬁne-tuning well-structured prompt ( P )\nagainst a classical classiﬁcation head ( H).\nA verage Advantage (# Training Points, P vs. N)\nMNLI BoolQ CB RTE WiC\nRoBERT a-Large 150 ± 252 299 ± 81 78 ± 2 404 ± 68 −354 ± 166\nT5-Large 300 ± 120 350 ± 95 150 ± 4 608 ± 90 20 ± 43\nBART -Large 200 ± 180 325 ± 54 85 ± 8 512 ± 64 −80 ± 89\nHTLM 692 ± 240 565 ± 143 255 ± 34 640 ± 45 80 ± 40\nT able 7: A verage advantage (higher is better) in terms of tra ining points for ﬁne-tuning well-structured prompt ( P )\nagainst a prompt with a non-sensical verbalizer ( N).\nture that the inputs are put into) and verbalizer (i.e.,\nyes/no answer to pattern): (1) ﬁne-tuning a classi-\nﬁcation head ( H), (2) ﬁne-tuning the verbalizer of\na prompt encoding the semantics of the task ( P ),\nand (3) ﬁne-tuning the prompt but with a verbal-\nizer that is non-sensical ( N).\nBy carefully selecting the number of data\npoints to be used during training in each setting\nwhile matching the end ﬁne-tuning performance,\nwe can empirically measure the efﬁcacy of\nprompts in terms of data points. W e provide\nthe same analysis extended to BART , T5-Large,\nand HTLM using the same PET prompts pro-\nvided in Schick and Sch ¨ utze (2020). For HTLM,\nwe wrap all PET prompts in an HTML ele-\nment. W e select the same datasets that were\nused in the original paper for our experimen-\ntation; MNLI (\nWilliams et al. , 2018), BoolQ\n(Clark et al. , 2019), CB ( De Marneffe et al. ,\n2019), RTE ( Bentivogli et al. , 2009), WiC\n(Pilehvar and Camacho-Collados , 2019).\nW e ﬁrst look at the average advantage of ﬁne-\ntuning a prompt ( P ) against a classiﬁcation head\n(H) in T able 6. W e see that across the board,\nHTLM prompts—i.e., hypertext prompts applied\nto HTLM—are worth more than natural language\nprompts to various other pre-trained models. Com-\npared to RoBERT a-Large on smaller datasets,\nHTLM’s advantage is close to triple on CB and dou-\nble on RTE. Furthermore, on WiC, HTLM is the\nonly pre-trained model capable of having a posi-\ntive training advantage when using prompts. W e\nview this as additional evidence to the beneﬁt of\npre-training on structured data on the prompting\nof a pre-trained model.\nW e also compare the average advantage of ﬁne-\ntuning a prompt with a verbalizer ( P ) that makes\nsense against against ﬁnetuning a prompt where\nwe change the verbalizer to a random ﬁrst name\n(N). This is important to capture whether the\nbeneﬁts arise from representing the data in their\nrespective patterns or the coupling of the pattern\nand the verbalizer. W e present our results in T a-\nble\n7. Relative to the previous P vs. H setting\nwe lose a large amount of advantage, as was sim-\nilarly seen in ( Le Scao and Rush , 2021). Interest-\ningly enough for small datasets such as CB, all of\nthe training advantage of the prompt comes from\nthe pattern in HTLM.\nW e view this as further evidence that a struc-\ntured, document level approach to both pre-\ntraining and prompting can be seen as a viable al-\nternative to a purely natural language approach.\n7 Related W ork\nGPT -2 (\nRadford et al. , 2019) showed that large\nlanguage models show varying levels of zero-\nshot performance across NLP tasks when com-\npared to supervised baselines (e.g., rudimen-\ntary performance on summarization, but more\ncompetitive results on reading comprehension).\nBrown et al. (2020) through their GPT3 model\nshowed that by further scaling up language mod-\nels on a large subset of the internet, prompt-\ning could be a viable alternative to standard ﬁne-\ntuning. The success of GPT3 was largely at-\ntributed to massive size and compute-intensive\npretraining. By reformulating NLP tasks as\ncloze-style questions,\nSchick and Sch ¨ utze (2020)\nshows that the prompting capabilities exhibited\nby GPT3 can occur in language models of a\nmuch smaller scale when gradient-based ﬁnetun-\ning is combined with task-speciﬁc unlabeled data.\nFollow-up work (\nT am et al. , 2021) improves upon\nthese results without depending on unlabeled data.\nUnlike GPT -3 and other models which use con-\nventional natural language text-based prompting,\nwe focus on a new hyper-text based prompting\nscheme using generative masked language models\npre-trained directly over HTML.\nFor task-speciﬁc zero-shot performance, cus-\ntom pre-training and data augmentation schemes\nhave been developed. For example, PEGA-\nSUS (\nZhang et al. , 2019) proposes a novel pre-\ntraining scheme tailored for summarization which\ninvolves masking and generating salient gap sen-\ntences from a large news corpus. While PE-\nGASUS is capable of doing zero-shot summa-\nrization, it offers little control over summary\nattributes such as length and style which vary\nacross different summarization datasets. Wiki-\nTransfer (\nFabbri et al. , 2021) ﬁne-tunes pretrained\nmodels on pseudo-summaries, produced from\ngeneric Wikipedia data, which contain character-\nistics of the target dataset, such as the length and\nlevel of abstraction. Our proposed model allows\nﬁne-grained control over the length of the gen-\nerated text by specifying the size of the mask.\nMoreover, by using different prompts, HTLM can\nproduce stylistically varied summaries without\ndataset-speciﬁc augmentation and ﬁnetuning.\nAnother line of work has been looking at a hy-\nbrid form of prompting that attempts to optimize\nvery few parameters to solve an end task. For\nexample\nLi and Liang (2021) argue that optimiz-\ning in the continuous prompt space is an effective\nsolution to prompt search while\nAghajanyan et al.\n(2020b) optimize for a low-rank projection of the\nfull parameter space. For simplicity , we only focus\non either full-ﬁnetuning or zero-shot prompting in\nthis paper.\nAttempts have been made to encode architec-\ntural priors for structured inputs into transformers\nas well. Speciﬁcally ,\nAinslie et al. (2020) discuss\na new type of model which allows for scalability\nin input length as well as the ability to encode the\nstructure of the input. W e opt to allow HTLM to\nlearn the structure that is available in the HTML di-\nrectly without encoding any structural priors into\nthe model itself.\n8 Conclusion\nIn this paper, we proposed HTLM, a hyper-text lan-\nguage model trained on simpliﬁed HTML docu-\nments from a large-scale web crawl. W e showed\nthat by directly modeling HTML through a BART -\nlike objective, we could do structured zero-shot\nprompting by representing tasks in HTML. Specif-\nically , we outperform the previous best results on\nzero-shot prompting for summarization by a wide\nmargin by creating prompts that capture the un-\nderlying semantics of each summarization dataset.\nFurthermore, we show that pre-training on struc-\ntured data improved full ﬁnetuning performance\nrelative to other pre-trained models that only mod-\neled natural language.\nW e also showed additional advantages of model-\ning hyper-text, beyond improved accuracy . HTLM\ncan be used for auto-prompt by simply asking\nthe model to recover the document structure from\ntraining samples; these auto-prompts on datasets\nlike Gigaword and CNN/DM outperformed previ-\nous state-of-the-art zero-shot approaches. Lastly ,\nwe provided an in-depth comparison of the train-\ning advantage, in terms of data efﬁciency , that\nHTLM had compared to other pre-training ap-\nproaches. Across the board, HTML prompts\nwere worth more to HTLM than natural language\nprompts were worth to our baselines, further show-\ning the efﬁcacy of pre-training structured data.\nFuture work can focus on the scaling laws of\nstructured pre-training and prompting. As was\nseen from GPT -3, the size of the model and the\namount of compute utilized and signiﬁcant impact\non prompting performance.\nReferences\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta,\nNaman Goyal, Luke Zettlemoyer, and Sonal Gupta.\n2020a. Better ﬁne-tuning by reducing representa-\ntional collapse. arXiv preprint arXiv:2008.03156.\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal\nGupta. 2020b. Intrinsic dimensionality explains the\neffectiveness of language model ﬁne-tuning. arXiv\npreprint arXiv:2012.13255.\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, V a-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan W ang, and Li Y ang.\n2020. Etc: Encoding long and structured inputs\nin transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 268–284.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nAnja Belz and Ehud Reiter. 2006. Comparing auto-\nmatic and human evaluation of nlg systems. In 11th\nconference of the european chapter of the associa-\ntion for computational linguistics.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The ﬁfth pascal recognizing tex-\ntual entailment challenge. In TAC.\nT om B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nKrzysztof Choromanski, V alerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, T amas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. 2020. Rethinking attention\nwith performers. arXiv preprint arXiv:2009.14794.\nChristopher Clark, Kenton Lee, Ming-W ei Chang,\nT om Kwiatkowski, Michael Collins, and Kristina\nT outanova. 2019. BoolQ: Exploring the surprising\ndifﬁculty of natural yes/no questions. In Proceed-\nings of NAACL-HLT 2019.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nV ishrav Chaudhary, Guillaume W enzek, Francisco\nGuzm ´ an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and V eselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nMarie-Catherine De Marneffe, Mandy Simons, and\nJudith T onhauser. 2019. The Commitment-\nBank: Investigating projection in naturally oc-\ncurring discourse. T o appear in proceedings of\nSinn und Bedeutung 23. Data can be found at\nhttps://github.com/mcdm/CommitmentBank/.\nA. R. Fabbri, Simeng Han, Haoyuan Li, Haoran Li,\nMarjan Ghazvininejad, Shaﬁq R. Joty, Dragomir\nRadev, and Y ashar Mehdad. 2021. Improving zero\nand few-shot abstractive summarization with inter-\nmediate ﬁne-tuning and data augmentation. In\nNAACL.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. The webnlg\nchallenge: Generating text from rdf data. In Pro-\nceedings of the 10th International Conference on\nNatural Language Generation, pages 124–133.\nKarl Moritz Hermann, T omas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. T eaching machines to read\nand comprehend. In Advances in neural information\nprocessing systems, pages 1693–1701.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, H ´ erve J´ egou, and T omas Mikolov.\n2016. Fasttext. zip: Compressing text classiﬁcation\nmodels. arXiv preprint arXiv:1612.03651.\nByeongchang Kim, Hyunwoo Kim, and Gunhee Kim.\n2018. Abstractive summarization of reddit posts\nwith multi-level memory networks. arXiv preprint\narXiv:1811.00783 .\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nAlon Lavie and Abhaya Agarwal. 2007. Meteor: An\nautomatic metric for mt evaluation with high levels\nof correlation with human judgments. In Proceed-\nings of the second workshop on statistical machine\ntranslation, pages 228–231.\nT even Le Scao and Alexander Rush. 2021.\nHow many data points is a prompt worth? In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language T echnologies ,\npages 2627–2636, Online. Association for Compu-\ntational Linguistics.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning .\nCiteseer.\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The Winograd schema challenge. In\nAAAI Spring Symposium: Logical F ormalizations of\nCommonsense Reasoning, volume 46, page 47.\nMike Lewis, Y inhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, V es Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-\ntuning: Optimizing continuous prompts for genera-\ntion. arXiv preprint arXiv:2101.00190.\nChin-Y ew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In T ext summarization\nbranches out, pages 74–81.\nY inhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and V eselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit\nRau, Abhinand Sivaprasad, Chiachun Hsieh, Xian-\ngru T ang, Aadit Vyas, Neha V erma, Pranav Kr-\nishna, et al. 2020. Dart: Open-domain struc-\ntured data record to text generation. arXiv preprint\narXiv:2007.02871 .\nCourtney Napoles, Matthew R Gormley, and Benjamin\nV an Durme. 2012. Annotated gigaword. In Pro-\nceedings of the Joint W orkshop on Automatic Knowl-\nedge Base Construction and W eb-scale Knowledge\nExtraction (AKBC-WEKEX), pages 95–100.\nShashi Narayan, Shay B Cohen, and Mirella Lap-\nata. 2018. Don’t give me the details, just the\nsummary! topic-aware convolutional neural net-\nworks for extreme summarization. arXiv preprint\narXiv:1808.08745 .\nJekaterina Novikova, Ondˇ rej Duˇ sek, and V erena Rieser.\n2017. The e2e dataset: New challenges for end-to-\nend generation. arXiv preprint arXiv:1706.09254.\nKishore Papineni, Salim Roukos, T odd W ard, and W ei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Compu-\ntational Linguistics, pages 311–318.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. arXiv\npreprint arXiv:2105.11447.\nMohammad T aher Pilehvar and Jose Camacho-\nCollados. 2019. WiC: The word-in-context dataset\nfor evaluating context-sensitive meaning representa-\ntions. In Proceedings of NAACL-HLT.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Y anqi Zhou,\nW ei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nTimo Schick and Hinrich Sch ¨ utze. 2020. It’s\nnot just size that matters: Small language mod-\nels are also few-shot learners. arXiv preprint\narXiv:2009.07118 .\nMathew Snover, Bonnie Dorr, Richard Schwartz, John\nMakhoul, Linnea Micciulla, and Ralph W eischedel.\n2005. A study of translation error rate with targeted\nhuman annotation. In Proceedings of the 7th Con-\nference of the Association for Machine T ranslation\nin the Americas (AMTA 06), pages 223–231.\nDerek T am, R. R. Menon, M. Bansal, Shashank\nSrivastava, and Colin Raffel. 2021. Improving\nand simplifying pattern exploiting training. ArXiv,\nabs/2103.11955.\nRamakrishna V edantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4566–4575.\nAlex W ang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis platform for natu ral language understanding .\nIn Proceedings of the 2018 EMNLP W orkshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP , pages 353–355, Brussels, Bel-\ngium. Association for Computational Linguistics.\nSinong W ang, Belinda Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768 .\nAdina Williams, Nikita Nan-\ngia, and Samuel Bowman. 2018.\nA broad-coverage challenge corpus for sentence understand ing through inference .\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language T echnologies,\nV olume 1 (Long P apers), pages 1112–1122. Associ-\nation for Computational Linguistics.\nRowan Zellers, Ari Holtzman, Y onatan Bisk, Ali\nFarhadi, and Y ejin Choi. 2019. Hellaswag: Can a\nmachine really ﬁnish your sentence? arXiv preprint\narXiv:1905.07830 .\nJingqing Zhang, Y ao Zhao, Mohammad Saleh, and Pe-\nter J Liu. 2019. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. arXiv\npreprint arXiv:1912.08777.\nA Appendix\nA.1 Finetuning Hyper-Parameters\nFor our GLUE related experiments the following\nparameters are used.\nHyper Parameter MNLI QNLI QQP SST -2 RTE MRPC CoLA\nLearning Rate 5e-6 5e-6 5e-6 5e-6 1e-5 1e-5 1e-5\nMax Updates 123873 33112 113272 20935 3120 2296 5336\nMax Sentences 8 8 32 32 8 16 16\nT able 8: T ask speciﬁc hyper parameters for GLUE experiments\nHyper parameter V alue\nOptimizer Adam\nAdam-betas (0.9, 0.98)\nAdam-eps 1e-6\nLR Scheduler polynomial decay\nDropout 0.1\nW eight Decay 0.01\nW armup Updates 0.06 * max updates\nHyper parameter V alue\nλ [0.1, 0.5, 1.0, 5.0]\nNoise T ypes [ U, N ]\nσ 1e − 5\nT able 9: Hyper parameters for R3F experiments on GLUE"
}