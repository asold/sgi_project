{
  "title": "Generalizing and Hybridizing Count-based and Neural Language Models",
  "url": "https://openalex.org/W2411934291",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A277131583",
      "name": "Graham Neubig",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2107310219",
      "name": "Chris Dyer",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2251071050",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2403133475",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2112874453",
    "https://openalex.org/W2082092506",
    "https://openalex.org/W2113641473",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2049901611",
    "https://openalex.org/W2094971681",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2963084471",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2069699492",
    "https://openalex.org/W2075201173",
    "https://openalex.org/W2005902041",
    "https://openalex.org/W2151315616",
    "https://openalex.org/W2950186769",
    "https://openalex.org/W2045633041",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W1965154800",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W1996903695",
    "https://openalex.org/W2128666521",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W2951793508",
    "https://openalex.org/W2105402874"
  ],
  "abstract": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.",
  "full_text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1163–1172,\nAustin, Texas, November 1-5, 2016.c⃝2016 Association for Computational Linguistics\nGeneralizing and Hybridizing Count-based and Neural Language Models\nGraham Neubig† and Chris Dyer‡\n†Carnegie Mellon University, USA\n‡Google DeepMind, United Kingdom\nAbstract\nLanguage models (LMs) are statistical mod-\nels that calculate probabilities over sequences\nof words or other discrete symbols. Currently\ntwo major paradigms for language model-\ning exist: count-based n-gram models, which\nhave advantages of scalability and test-time\nspeed, and neural LMs, which often achieve\nsuperior modeling performance. We demon-\nstrate how both varieties of models can be uni-\nﬁed in a single modeling framework that de-\nﬁnes a set of probability distributions over the\nvocabulary of words, and then dynamically\ncalculates mixture weights over these distri-\nbutions. This formulation allows us to create\nnovel hybrid models that combine the desir-\nable features of count-based and neural LMs,\nand experiments demonstrate the advantages\nof these approaches.1\n1 Introduction\nLanguage models (LMs) are statistical models that,\ngiven a sentence wI\n1 := w1,...,w I, calculate its\nprobability P(wI\n1). LMs are widely used in applica-\ntions such as machine translation and speech recog-\nnition, and because of their broad applicability they\nhave also been widely studied in the literature. The\nmost traditional and broadly used language model-\ning paradigm is that of count-based LMs, usually\nsmoothed n-grams (Witten and Bell, 1991; Chen\n1Work was performed while GN was at the Nara Institute of\nScience and Technology and CD was at Carnegie Mellon Uni-\nversity. Code and data to reproduce experiments is available at\nhttp://github.com/neubig/modlm\nand Goodman, 1996). Recently, there has been a fo-\ncus on LMs based on neural networks (Nakamura\net al., 1990; Bengio et al., 2006; Mikolov et al.,\n2010), which have shown impressive improvements\nin performance over count-based LMs. On the other\nhand, these neural LMs also come at the cost of in-\ncreased computational complexity at both training\nand test time, and even the largest reported neural\nLMs (Chen et al., 2015; Williams et al., 2015) are\ntrained on a fraction of the data of their count-based\ncounterparts (Brants et al., 2007).\nIn this paper we focus on a class of LMs,\nwhich we will call mixture of distributions LMs\n(MODLMs; §2). Speciﬁcally, we deﬁne MODLMs\nas all LMs that take the following form, calculat-\ning the probabilities of the next word in a sentence\nwi given preceding context c according to a mix-\nture of several component probability distributions\nPk(wi|c):\nP(wi|c) =\nK∑\nk=1\nλk(c)Pk(wi|c). (1)\nHere, λk(c) is a function that deﬁnes the mixture\nweights, with the constraint that ∑K\nk=1 λk(c) = 1\nfor all c. This form is not new in itself, and widely\nused both in the calculation of smoothing coefﬁ-\ncients for n-gram LMs (Chen and Goodman, 1996),\nand interpolation of LMs of various varieties (Je-\nlinek and Mercer, 1980).\nThe main contribution of this paper is to demon-\nstrate that depending on our deﬁnition of c, λk(c),\nand Pk(wi|c), Eq. 1 can be used to describe not only\nn-gram models, but also feed-forward (Nakamura et\nal., 1990; Bengio et al., 2006; Schwenk, 2007) and\n1163\nrecurrent (Mikolov et al., 2010; Sundermeyer et al.,\n2012) neural network LMs ( §3). This observation\nis useful theoretically, as it provides a single mathe-\nmatical framework that encompasses several widely\nused classes of LMs. It is also useful practically, in\nthat this new view of these traditional models allows\nus to create new models that combine the desirable\nfeatures of n-gram and neural models, such as:\nneurally interpolatedn-gram LMs (§4.1), which\nlearn the interpolation weights of n-gram\nmodels using neural networks, and\nneural/n-gram hybrid LMs (§4.2), which add a\ncount-based n-gram component to neural mod-\nels, allowing for ﬂexibility to add large-scale\nexternal data sources to neural LMs.\nWe discuss learning methods for these models ( §5)\nincluding a novel method of randomly dropping out\nmore easy-to-learn distributions to prevent the pa-\nrameters from falling into sub-optimal local minima.\nExperiments on language modeling benchmarks\n(§6) ﬁnd that these models outperform baselines in\nterms of performance and convergence speed.\n2 Mixture of Distributions LMs\nAs mentioned above, MODLMs are LMs that take\nthe form of Eq. 1. This can be re-framed as the fol-\nlowing matrix-vector multiplication:\np⊺\nc = Dcλ⊺\nc,\nwhere pc is a vector with length equal to vocabulary\nsize, in which the jth element pc,j corresponds to\nP(wi = j|c), λc is a size Kvector that contains the\nmixture weights for the distributions, and Dc is a J-\nby-K matrix, where element dc,j,k is equivalent to\nthe probability Pk(wi = j|c).2 An example of this\nformulation is shown in Fig. 1.\nNote that all columns in D represent probability\ndistributions, and thus must sum to one over the J\nwords in the vocabulary, and that all λ must sum\nto 1 over the K distributions. Under this condition,\nthe vector pwill represent a well-formed probability\ndistribution as well. This conveniently allows us to\n2We omit the subscript c when appropriate.\nProbabilities p⊺ Coefﬁcients λ⊺\n \n\n\n\n\n\n\n\n\n\n\n\np1\n=\nd1,1 d1,2 ··· d1,K λ1\np2 d2,1 d2,2 ··· d2,K λ2\n... ... ... ... ... ...\npJ dJ,1 dJ,2 ··· dJ,K λK\n  \nDistribution matrix D\nFigure 1: MODLMs as linear equations\ncalculate the probability of a single word wi = jby\ncalculating the product of the jth row of Dc and λ⊺\nc\nPk(wi = j|c) =dc,jλ⊺\nc.\nIn the sequel we show how this formulation can be\nused to describe several existing LMs (§3) as well as\nseveral novel model structures that are more power-\nful and general than these existing models (§4).\n3 Existing LMs as Linear Mixtures\n3.1 n-gram LMs as Mixtures of Distributions\nFirst, we discuss how count-based interpolated n-\ngram LMs ﬁt within the MODLM framework.\nMaximum likelihood estimation:n-gram mod-\nels predict the next word based on the previous N-1\nwords. In other words, we set c = wi−1\ni−N+1 and\ncalculate P(wi|wi−1\ni−N+1). The maximum-likelihood\n(ML) estimate for this probability is\nPML (wi|wi−1\ni−N+1) =c(wi\ni−N+1)/c(wi−1\ni−N+1),\nwhere c(·) counts frequency in the training corpus.\nInterpolation: Because ML estimation as-\nsigns zero probability to word sequences where\nc(wi\ni−N+1) = 0, n-gram models often interpolate\nthe ML distributions for sequences of length 1 toN.\nThe simplest form is static interpolation\nP(wi|wi−1\ni−n+1) =\nN∑\nn=1\nλS,nPML (wi|wi−1\ni−n+1). (2)\nλS is a vector where λS,n represents the weight\nput on the distribution PML (wi|wi−1\ni−n+1). This can\nbe expressed as linear equations (Fig. 2a) by set-\nting the nth column of D to the ML distribution\nPML (wi|wi−1\ni−n+1), and λ(c) equal to λS.\n1164\nProbabilities p⊺ Heuristic interp. coefﬁcients λ⊺\n \n\n\n\n\n\n\n\n\n\n\n\np1\n=\nd1,1 d1,2 ··· d1,N λ1\np2 d2,1 d2,2 ··· d2,N λ2\n... ... ... ... ... ...\npJ dJ,1 dJ,2 ··· dJ,N λN\n  \nCount-based probabilities PC(wi = j|wi−1\ni−n+1)\n(a) Interpolated n-grams as MODLMs\nProbabilities p⊺ Result of softmax(NN(c))\n \n\n\n\n\n\n\n\n\n\n\n\np1\n=\n1 0 ··· 0 λ1\np2 0 1 ··· 0 λ2\n... ... ... ... ... ...\npJ 0 0 ··· 1 λJ\n  \nJ-by-J identity matrix I\n(b) Neural LMs as MODLMs\nFigure 2: Interpretations of existing models as mixtures of distributions\nStatic interpolation can be improved by calcu-\nlating λ(c) dynamically, using heuristics based on\nthe frequency counts of the context (Good, 1953;\nKatz, 1987; Witten and Bell, 1991). These meth-\nods deﬁne a context-sensitive fallback probability\nα(wi−1\ni−n+1) for order nmodels, and recursively cal-\nculate the probability of the higher order models\nfrom the lower order models:\nP(wi|wi−1\ni−n+1) =α(wi−1\ni−n+1)P(wi|wi−1\ni−n+2)+\n(1 −α(wi−1\ni−n+1))PML (wi|wi−1\ni−n+1). (3)\nTo express this as a linear mixture, we con-\nvert α(wi−1\ni−n+1) into the appropriate value for\nλn(wi−1\ni−N+1). Speciﬁcally, the probability assigned\nto each PML (wi|wi−1\ni−n+1) is set to the product of the\nfallbacks αfor all higher orders and the probability\nof not falling back (1 −α) at the current level:\nλn(wi−1\ni−N+1) = (1−α(wi−1\ni−n+1))\nN∏\n˜n=n+1\nα(wi−1\ni−˜n+1).\nDiscounting: The widely used technique of dis-\ncounting (Ney et al., 1994) deﬁnes a ﬁxed discount\ndand subtracts it from the count of each word before\ncalculating probabilities:\nPD(wi|wi−1\ni−n+1) = (c(wi\ni−n+1) −d)/c(wi−1\ni−n+1).\nDiscounted LMs then assign the remaining probabil-\nity mass after discounting as the fallback probability\nβD(wi−1\ni−n+1) =1−\nJ∑\nj=1\nPD(wi = j|wi−1\ni−n+1),\nP(wi|wi−1\ni−n+1) =βD(wi−1\ni−n+1)P(wi|wi−1\ni−n+2)+\nPD(wi|wi−1\ni−n+1). (4)\nIn this case, PD(·) does not add to one, and thus vi-\nolates the conditions for MODLMs stated in §2, but\nit is easy to turn discounted LMs into interpolated\nLMs by normalizing the discounted distribution:\nPND (wi|wi−1\ni−n+1) = PD(wi|wi−1\ni−n+1)\n∑J\nj=1 PD(wi = j|wi−1\ni−n+1)\n,\nwhich allows us to replace β(·) for α(·) and PND (·)\nfor PML (·) in Eq. 3, and proceed as normal.\nKneser–Ney (KN; Kneser and Ney (1995)) and\nModiﬁed KN (Chen and Goodman, 1996) smooth-\ning further improve discounted LMs by adjusting the\ncounts of lower-order distributions to more closely\nmatch their expectations as fallbacks for higher or-\nder distributions. Modiﬁed KN is currently the de-\nfacto standard in n-gram LMs despite occasional\nimprovements (Teh, 2006; Durrett and Klein, 2011),\nand we will express it as PKN (·).\n3.2 Neural LMs as Mixtures of Distributions\nIn this section we demonstrate how neural network\nLMs can also be viewed as an instantiation of the\nMODLM framework.\nFeed-forward neural network LMs: Feed-\nforward LMs (Bengio et al., 2006; Schwenk, 2007)\nare LMs that, like n-grams, calculate the prob-\nability of the next word based on the previous\nwords. Given context wi−1\ni−N+1, these words are\nconverted into real-valued word representation vec-\ntors ri−1\ni−N+1, which are concatenated into an over-\nall representation vector q = ⊕(ri−1\ni−N+1), where\n⊕(·) is the vector concatenation function. qis then\nrun through a series of afﬁne transforms and non-\nlinearities deﬁned as function NN (q) to obtain a\nvector h. For example, for a one-layer neural net-\n1165\nProbabilities p⊺ Result of softmax(NN(c))\n \n\n\n\n\n\n\n\n\n\n\n\np1\n=\nd1,1 d1,2 ··· d1,N λ1\np2 d1,2 d2,2 ··· d2,N λ2\n... ... ... ... ... ...\npJ dJ,1 dJ,2 ··· dJ,N λN\n  \nCount-based probabilities PC(wi = j|wi−1\ni−n+1)\n(a) Neurally interpolated n-gram LMs\nProbabilities p⊺ Result of softmax(NN(c))\n \n\n\n\n\n\n\n\n\n\n\n\np1\n=\nd1,1 ··· d1,N 1 ··· 0 λ1\np2 d2,1 ··· d2,N 0 ··· 0 λ2\n... ... ... ... ... ... ... ...\npJ dJ,1 ··· dJ,N 0 ··· 1 λJ+N\n  \nCount-based probabilities and J-by-J identity matrix\n(b) Neural/n-gram hybrid LMs\nFigure 3: Two new expansions to n-gram and neural LMs made possible in the MODLM framework\nwork with a tanh non-linearity we can deﬁne\nNN(q) := tanh(qWq + bq), (5)\nwhere Wq and bq are weight matrix and bias vec-\ntor parameters respectively. Finally, the probabil-\nity vector pis calculated using the softmax function\np= softmax(hWs + bs), similarly parameterized.\nAs these models are directly predicting pwith no\nconcept of mixture weights λ, they cannot be inter-\npreted as MODLMs as-is. However, we can per-\nform a trick shown in Fig. 2b, not calculating pdi-\nrectly, but instead calculating mixture weights λ=\nsoftmax(hWs + bs), and deﬁning the MODLM’s\ndistribution matrix D as a J-by-J identity matrix.\nThis is equivalent to deﬁning a linear mixture of J\nKronecker δj distributions, the jth of which assigns\na probability of 1 to word j and zero to everything\nelse, and estimating the mixture weights with a neu-\nral network. While it may not be clear why it is use-\nful to deﬁne neural LMs in this somewhat round-\nabout way, we describe in§4 how this opens up pos-\nsibilities for novel expansions to standard models.\nRecurrent neural network LMs: LMs using\nrecurrent neural networks (RNNs) (Mikolov et al.,\n2010) consider not the previous few words, but also\nmaintain a hidden state summarizing the sentence up\nuntil this point by re-deﬁning the net in Eq. 5 as\nRNN(qi) := tanh(qiWq + hi−1Wh + bq),\nwhere qi is the current input vector and hi−1 is the\nhidden vector at the previous time step. This allows\nfor consideration of long-distance dependencies be-\nyond the scope of standard n-grams, and LMs using\nRNNs or long short-term memory (LSTM) networks\n(Sundermeyer et al., 2012) have posted large im-\nprovements over standardn-grams and feed-forward\nmodels. Like feed-forward LMs, LMs using RNNs\ncan be expressed as MODLMs by predicting λin-\nstead of predicting pdirectly.\n4 Novel Applications of MODLMs\nThis section describes how we can use this frame-\nwork of MODLMs to design new varieties of LMs\nthat combine the advantages of both n-gram and\nneural network LMs.\n4.1 Neurally Interpolated n-gram Models\nThe ﬁrst novel instantiation of MODLMs that we\npropose is neurally interpolated n-gram models,\nshown in Fig. 3a. In these models, we setDto be the\nsame matrix used inn-gram LMs, but calculateλ(c)\nusing a neural network model. As λ(c) is learned\nfrom data, this framework has the potential to allow\nus to learn more intelligent interpolation functions\nthan the heuristics described in §3.1. In addition,\nbecause the neural network only has to calculate a\nsoftmax over N distributions instead of J vocabu-\nlary words, training and test efﬁciency of these mod-\nels can be expected to be much greater than that of\nstandard neural network LMs.\nWithin this framework, there are several design\ndecisions. First, how we decide D: do we use the\nmaximum likelihood estimatePML or KN estimated\ndistributions PKN ? Second, what do we provide as\ninput to the neural network to calculate the mixture\nweights? To provide the neural net with the same\ninformation used by interpolation heuristics used in\ntraditional LMs, we ﬁrst calculate three features for\neach of the N contexts wi−1\ni−n+1: a binary feature in-\ndicating whether the context has been observed in\nthe training corpus ( c(wi−1\ni−n+1) > 0), the log fre-\nquency of the context counts ( log(c(wi−1\ni−n+1)) or\n1166\nzero for unobserved contexts), and the log frequency\nof the number of unique words following the context\n(log(u(wi−1\ni−n+1)) or likewise zero). When using dis-\ncounted distributions, we also use the log of the sum\nof the discounted counts as a feature. We can also\noptionally use the word representation vectorqused\nin neural LMs, allowing for richer representation of\nthe input, but this may or may not be necessary in the\nface of the already informative count-based features.\n4.2 Neural/ n-gram Hybrid Models\nOur second novel model enabled by MODLMs is\nneural/n-gram hybrid models, shown in Fig. 3b.\nThese models are similar to neurally interpolated\nn-grams, but D is augmented with J additional\ncolumns representing the Kronecker δj distributions\nused in the standard neural LMs. In this construc-\ntion, λ is still a stochastic vector, but its contents\nare both the mixture coefﬁcients for the count-based\nmodels and direct predictions of the probabilities of\nwords. Thus, the learned LM can use count-based\nmodels when they are deemed accurate, and deviate\nfrom them when deemed necessary.\nThis model is attractive conceptually for several\nreasons. First, it has access to all information used\nby both neural and n-gram LMs, and should be able\nto perform as well or better than both models. Sec-\nond, the efﬁciently calculated n-gram counts are\nlikely sufﬁcient to capture many phenomena nec-\nessary for language modeling, allowing the neural\ncomponent to focus on learning only the phenom-\nena that are not well modeled by n-grams, requiring\nfewer parameters and less training time. Third, it is\npossible to train n-grams from much larger amounts\nof data, and use these massive models to bootstrap\nlearning of neural nets on smaller datasets.\n5 Learning Mixtures of Distributions\nWhile the MODLM formulations of standard heuris-\ntic n-gram LMs do not require learning, the remain-\ning models are parameterized. This section dis-\ncusses the details of learning these parameters.\n5.1 Learning MODLMs\nThe ﬁrst step in learning parameters is deﬁning our\ntraining objective. Like most previous work on\nLMs (Bengio et al., 2006), we use a negative log-\nlikelihood loss summed over words wi in every sen-\ntence win corpus W\nL(W) =−\n∑\nw∈W\n∑\nwi∈w\nlog P(wi|c),\nwhere crepresents all words preceding wi in wthat\nare used in the probability calculation. As noted in\nEq. 2, P(wi = j|c) can be calculated efﬁciently\nfrom the distribution matrix Dc and mixture func-\ntion output λc.\nGiven that we can calculate the log likelihood, the\nremaining parts of training are similar to training for\nstandard neural network LMs. As usual, we per-\nform forward propagation to calculate the probabili-\nties of all the words in the sentence, back-propagate\nthe gradients through the computation graph, and\nperform some variant of stochastic gradient descent\n(SGD) to update the parameters.\n5.2 Block Dropout for Hybrid Models\nWhile the training method described in the previ-\nous section is similar to that of other neural network\nmodels, we make one important modiﬁcation to the\ntraining process speciﬁcally tailored to the hybrid\nmodels of §4.2.\nThis is motivated by our observation (detailed in\n§6.3) that the hybrid models, despite being strictly\nmore expressive than the corresponding neural net-\nwork LMs, were falling into poor local minima with\nhigher training error than neural network LMs. This\nis because at the very beginning of training, the\ncount-based elements of the distribution matrix in\nFig. 3b are already good approximations of the tar-\nget distribution, while the weights of the single-word\nδj distributions are not yet able to provide accurate\nprobabilities. Thus, the model learns to set the mix-\nture proportions of the δ elements to near zero and\nrely mainly on the count-basedn-gram distributions.\nTo encourage the model to use theδmixture com-\nponents, we adopt a method called block dropout\n(Ammar et al., 2016). In contrast to standard\ndropout (Srivastava et al., 2014), which drops out\nsingle nodes or connections, block dropout ran-\ndomly drops out entire subsets of network nodes. In\nour case, we want to prevent the network from over-\nusing the count-based n-gram distributions, so for a\nrandomly selected portion of the training examples\n(here, 50%) we disable all n-gram distributions and\n1167\nforce the model to rely on only the δ distributions.\nTo do so, we zero out all elements in λ(c) that cor-\nrespond to n-gram distributions, and re-normalize\nover the rest of the elements so they sum to one.\n5.3 Network and Training Details\nFinally, we note design details that were determined\nbased on preliminary experiments.\nNetwork structures:We used both feed-forward\nnetworks with tanh non-linearities and LSTM\n(Hochreiter and Schmidhuber, 1997) networks.\nMost experiments used single-layer 200-node net-\nworks, and 400-node networks were used for ex-\nperiments with larger training data. Word repre-\nsentations were the same size as the hidden layer.\nLarger and multi-layer networks did not yield im-\nprovements.\nTraining: We used ADAM (Kingma and Ba,\n2015) with a learning rate of 0.001, and minibatch\nsizes of 512 words. This led to faster convergence\nthan standard SGD, and more stable optimization\nthan other update rules. Models were evaluated ev-\nery 500k-3M words, and the model with the best de-\nvelopment likelihood was used. In addition to the\nblock dropout of §5.2, we used standard dropout\nwith a rate of 0.5 for both feed-forward (Srivastava\net al., 2014) and LSTM (Pham et al., 2014) nets in\nthe neural LMs and neural/ n-gram hybrids, but not\nin the neurally interpolated n-grams, where it re-\nsulted in slightly worse perplexities.\nFeatures: If parameters are learned on the data\nused to train count-based models, they will heav-\nily over-ﬁt and learn to trust the count-based distri-\nbutions too much. To prevent this, we performed\n10-fold cross validation, calculating count-based el-\nements of Dfor each fold with counts trained on the\nother 9/10. In addition, the count-based contextual\nfeatures in §4.1 were normalized by subtracting the\ntraining set mean, which improved performance.\n6 Experiments\n6.1 Experimental Setup\nIn this section, we perform experiments to eval-\nuate the neurally interpolated n-grams ( §6.2) and\nneural/n-gram hybrids (§6.3), the ability of our mod-\nels to take advantage of information from large data\nsets (§6.4), and the relative performance compared\nPTB Sent Word ASP Sent Word\ntrain 42k 890k train 100k 2.1M\nvalid 3.4k 70k valid 1.8k 45k\ntest 3.8k 79k test 1.8k 46k\nTable 1: Data sizes for the PTB and ASPEC corpora.\nDst./Ft. HEUR FF LSTM\nML/C 220.5/265.9 146.6/164.5 144.4/162.7\nML/CR - 145.7/163.9 142.6/158.4\nKN/C 140.8/156.5 138.9/152.5 136.8/151.1\nKN/CR - 136.9/153.0 135.2/149.1\nTable 2: PTB/ASPEC perplexities for traditional\nheuristic (HEUR) and proposed neural net (FF or\nLSTM) interpolation methods using ML or KN dis-\ntributions, and count (C) or count+word representa-\ntion (CR) features.\nto post-facto static interpolation of already-trained\nmodels (§6.5). For the main experiments, we evalu-\nate on two corpora: the Penn Treebank (PTB) data\nset prepared by Mikolov et al. (2010), 3 and the ﬁrst\n100k sentences in the English side of the ASPEC\ncorpus (Nakazawa et al., 2015) 4 (details in Tab. 1).\nThe PTB corpus uses the standard vocabulary of 10k\nwords, and for the ASPEC corpus we use a vocabu-\nlary of the 20k most frequent words. Our implemen-\ntation is included as supplementary material.\n6.2 Results for Neurally Interpolatedn-grams\nFirst, we investigate the utility of neurally interpo-\nlated n-grams. In all cases, we use a history of\nN = 5 and test several different settings for the\nmodels:\nEstimation type:λ(c) is calculated with heuris-\ntics (HEUR) or by the proposed method using feed-\nforward (FF), or LSTM nets.\nDistributions: We comparePML (·) and PKN (·).\nFor heuristics, we use Witten-Bell for ML and the\nappropriate discounted probabilities for KN.\nInput features: As input features for the neural\nnetwork, we either use only the count-based features\n(C) or count-based features together with the word\nrepresentation for the single previous word (CR).\nFrom the results shown in Tab. 2, we can ﬁrst see\nthat when comparing models using the same set of\n3http://rnnlm.org/simple-examples.tgz\n4http://lotus.kuee.kyoto-u.ac.jp/ASPEC/\n1168\ninput distributions, the neurally interpolated model\noutperforms corresponding heuristic methods. We\ncan also see that LSTMs have a slight advantage\nover FF nets, and models using word representa-\ntions have a slight advantage over those that use\nonly the count-based features. Overall, the best\nmodel achieves a relative perplexity reduction of 4-\n5% over KN models. Interestingly, even when using\nsimple ML distributions, the best neurally interpo-\nlated n-gram model nearly matches the heuristic KN\nmethod, demonstrating that the proposed model can\nautomatically learn interpolation functions that are\nnearly as effective as carefully designed heuristics.5\n6.3 Results for Neural/n-gram Hybrids\nIn experiments with hybrid models, we test a\nneural/n-gram hybrid LM using LSTM networks\nwith both Kronecker δ and KN smoothed 5-gram\ndistributions, trained either with or without block\ndropout. As our main baseline, we compare to\nLSTMs with only δ distributions, which have re-\nported competitive numbers on the PTB data set\n(Zaremba et al., 2014). 6 We also report results for\nheuristically smoothed KN 5-gram models, and the\nbest neurally interpolatedn-grams from the previous\nsection for reference.\nThe results, shown in Tab. 3, demonstrate that\nsimilarly to previous research, LSTM LMs (2)\nachieve a large improvement in perplexity over n-\ngram models, and that the proposed neural/ n-gram\nhybrid method (5) further reduces perplexity by 10-\n11% relative over this strong baseline.\nComparing models without (4) and with (5) the\nproposed block dropout, we can see that this method\ncontributes signiﬁcantly to these gains. To examine\nthis more closely, we show the test perplexity for the\n5Neurally interpolated n-grams are also more efﬁcient than\nstandard neural LMs, as mentioned in §4.1. While a standard\nLSTM LM calculated 1.4kw/s on the PTB data, the neurally in-\nterpolated models using LSTMs and FF nets calculated 11kw/s\nand 58kw/s respectively, only slightly inferior to 140kw/s of\nheuristic KN.\n6Note that unlike this work, we opt to condition only on in-\nsentence context, not inter-sentential dependencies, as training\nthrough gradient calculations over sentences is more straight-\nforward and because examining the effect of cross-boundary\ninformation is not central to the proposed method. Thus our\nbaseline numbers are not directly comparable (i.e. have higher\nperplexity) to previous reported results on this data, but we still\nfeel that the comparison is appropriate.\nDist. Interp. PPL\n(1) KN HEUR 140.8/156.5\n(2) δ LSTM 105.9/116.9\n(3) KN LSTM 135.2/149.1\n(4) KN, δ LSTM -BlDO 108.4/130.4\n(5) KN, δ LSTM +BlDO 95.3 /104.5\nTable 3: PTB/ASPEC perplexities for traditional\nKN (1) and LSTM LMs (2), neurally interpolatedn-\ngrams (3), and neural/n-gram hybrid models without\n(4) and with (5) block dropout.\n10 100 1000 Infty\nFrequency Cutoff\n102\n103\n104\n105\nPerplexity\n(1) KN/heur\n(2) d/LSTM\n(3) KN/LSTM\n(4) KN+d/LSTM\nFigure 4: Perplexities of (1) standard n-grams, (2)\nstandard LSTMs, (3) neurally interpolated n-grams,\nand (4) neural/ n-gram hybrids on lower frequency\nwords.\nthree models using δdistributions in Fig. 5, and the\namount of the probability mass in λ(c) assigned to\nthe non-δ distributions in the hybrid models. From\nthis, we can see that the model with block dropout\nquickly converges to a better result than the LSTM\nLM, but the model without converges to a worse\nresult, assigning too much probability mass to the\ndense count-based distributions, demonstrating the\nlearning problems mentioned in §5.2.\nIt is also of interest to examine exactly why the\nproposed model is doing better than the more stan-\ndard methods. One reason can be found in the be-\nhavior with regards to low-frequency words. In Fig.\n4, we show perplexities for words that appear n\ntimes or less in the training corpus, for n = 10,\nn = 100, n = 1000 and n = ∞(all words).\nFrom the results, we can ﬁrst see that if we com-\npare the baselines, LSTM language models achieve\nbetter perplexities overall butn-gram language mod-\nels tend to perform better on low-frequency words,\ncorroborating the observations of Chen et al. (2015).\n1169\n0 1 2 3 4 5 6\n1e7\n100\n120\n140\n160Perplexity\n(1) d/LSTM\n(2) KN+d/LSTM -BlDO\n(3) KN+d/LSTM +BlDO\n0 1 2 3 4 5 6\nTraining Words Processed 1e7\n0.0\n0.5\n1.0Dense Ratio\nFigure 5: Perplexity and dense distribution ratio of\nthe baseline LSTM LM (1), and the hybrid method\nwithout (2) and with (3) block dropout.\nThe neurally interpolated n-gram models consis-\ntently outperform standard KN-smoothed n-grams,\ndemonstrating their superiority within this model\nclass. In contrast, the neural/ n-gram hybrid mod-\nels tend to follow a pattern more similar to that of\nLSTM language models, similarly with consistently\nhigher performance.\n6.4 Results for Larger Data Sets\nTo examine the ability of the hybrid models to use\ncounts trained over larger amounts of data, we per-\nform experiments using two larger data sets:\nWSJ: The PTB uses data from the 1989 Wall\nStreet Journal, so we add the remaining years be-\ntween 1987 and 1994 (1.81M sents., 38.6M words).\nGW: News data from the English Gigaword 5th\nEdition (LDC2011T07, 59M sents., 1.76G words).\nWe incorporate this data either by training net pa-\nrameters over the whole large data, or by separately\ntraining count-based n-grams on each of PTB, WSJ,\nand GW, and learning net parameters on only PTB\ndata. The former has the advantage of training the\nnet on much larger data. The latter has two main ad-\nvantages: 1) when the smaller data is of a particular\ndomain the mixture weights can be learned to match\nthis in-domain data; 2) distributions can be trained\non data such as Google n-grams (LDC2006T13),\nwhich contain n-gram counts but not full sentences.\nIn the results of Fig. 6, we can ﬁrst see that the\nneural/n-gram hybrids signiﬁcantly outperform the\ntraditional neural LMs in the scenario with larger\ndata as well. Comparing the two methods for in-\ncorporating larger data, we can see that the results\nare mixed depending on the type and size of the data\n0 1 2 3 4 5 6\nTraining Words Processed 1e7\n60\n80\n100\n120\n140\n160Perplexity\n(1) d/p\n(2) KN+d/p\n(3) d/w\n(4) KN+d/w\n(5) KN+d/p +wLM\n(6) d/g\n(7) KN+d/g\n(8) KN+d/p +gLM\nFigure 6: Models trained on PTB (1,2), PTB+WSJ\n(3,4,5) or PTB+WSJ+GW (6,7,8) using standard\nneural LMs (1,3,6), neural/ n-gram hybrids trained\nall data (2,4,7), or hybrids trained on PTB with ad-\nditional n-gram distributions (5,8).\nbeing used. For the WSJ data, training on all data\nslightly outperforms the method of adding distribu-\ntions, but when the GW data is added this trend re-\nverses. This can be explained by the fact that the\nGW data differs from the PTB test data, and thus\nthe effect of choosing domain-speciﬁc interpolation\ncoefﬁcients was more prominent.\n6.5 Comparison with Static Interpolation\nFinally, because the proposed neural/n-gram hybrid\nmodels combine the advantages of neural and n-\ngram models, we compare with the more standard\nmethod of training models independently and com-\nbining them with static interpolation weights tuned\non the validation set using the EM algorithm. Tab. 4\nshows perplexities for combinations of a standard\nneural model (orδdistributions) trained on PTB, and\ncount based distributions trained on PTB, WSJ, and\nGW are added one-by-one using the standard static\nand proposed LSTM interpolation methods. From\nthe results, we can see that when only PTB data is\nused, the methods have similar results, but with the\nmore diverse data sets the proposed method edges\nout its static counterpart.7\n7In addition to better perplexities, neural/n-gram hybrids are\ntrained in a single pass instead of performing post-facto inter-\npolation, which may give advantages when training for other\nobjectives (Auli and Gao, 2014; Li et al., 2015).\n1170\nInterp δ+PTB +WSJ +GW\nLin. 95.1 70.5 65.8\nLSTM 95.3 68.3 63.5\nTable 4: PTB perplexity for interpolation between\nneural (δ) LMs and count-based models.\n7 Related Work\nA number of alternative methods focus on interpo-\nlating LMs of multiple varieties such as in-domain\nand out-of-domain LMs (Bulyko et al., 2003; Bac-\nchiani et al., 2006; G ¨ulc ¸ehre et al., 2015). Perhaps\nmost relevant is Hsu (2007)’s work on learning to\ninterpolate multiple LMs using log-linear models.\nThis differs from our work in that it learns functions\nto estimate the fallback probabilities αn(c) in Eq. 3\ninstead of λ(c), and does not cover interpolation of\nn-gram components, non-linearities, or the connec-\ntion with neural network LMs. Also conceptually\nsimilar is work on adaptation ofn-gram LMs, which\nstart with n-gram probabilities (Della Pietra et al.,\n1992; Kneser and Steinbiss, 1993; Rosenfeld, 1996;\nIyer and Ostendorf, 1999) and adapt them based on\nthe distribution of the current document, albeit in a\nlinear model. There has also been work incorpo-\nrating binary n-gram features into neural language\nmodels, which allows for more direct learning of n-\ngram weights (Mikolov et al., 2011), but does not af-\nford many of the advantages of the proposed model\nsuch as the incorporation of count-based probability\nestimates. Finally, recent works have compared n-\ngram and neural models, ﬁnding that neural models\noften perform better in perplexity, but n-grams have\ntheir own advantages such as effectiveness in extrin-\nsic tasks (Baltescu and Blunsom, 2015) and better\nmodeling of rare words (Chen et al., 2015).\n8 Conclusion and Future Work\nIn this paper, we proposed a framework for lan-\nguage modeling that generalizes both neural net-\nwork and count-based n-gram LMs. This allowed\nus to learn more effective interpolation functions for\ncount-based n-grams, and to create neural LMs that\nincorporate information from count-based models.\nAs the framework discussed here is general, it is\nalso possible that they could be used in other tasks\nthat perform sequential prediction of words such as\nneural machine translation (Sutskever et al., 2014)\nor dialog response generation (Sordoni et al., 2015).\nIn addition, given the positive results using block\ndropout for hybrid models, we plan to develop more\neffective learning methods for mixtures of sparse\nand dense distributions.\nAcknowledgements\nWe thank Kevin Duh, Austin Matthews, Shinji\nWatanabe, and anonymous reviewers for valuable\ncomments on earlier drafts. This work was sup-\nported in part by JSPS KAKENHI Grant Number\n16H05873, and the Program for Advancing Strate-\ngic International Networks to Accelerate the Circu-\nlation of Talented Researchers.\nReferences\nWaleed Ammar, George Mulcaire, Miguel Ballesteros,\nChris Dyer, and Noah A. Smith. 2016. One parser,\nmany languages. CoRR, abs/1602.01595.\nMichael Auli and Jianfeng Gao. 2014. Decoder inte-\ngration and expected bleu training for recurrent neural\nnetwork language models. In Proc. ACL, pages 136–\n142.\nMichiel Bacchiani, Michael Riley, Brian Roark, and\nRichard Sproat. 2006. Map adaptation of stochas-\ntic grammars. Computer Speech and Language ,\n20(1):41–68.\nPaul Baltescu and Phil Blunsom. 2015. Pragmatic neural\nlanguage modelling in machine translation. In Proc.\nNAACL, pages 820–829.\nYoshua Bengio, Holger Schwenk, Jean-S ´ebastien\nSen´ecal, Fr ´ederic Morin, and Jean-Luc Gauvain.\n2006. Neural probabilistic language models. In\nInnovations in Machine Learning, volume 194, pages\n137–186.\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,\nand Jeffrey Dean. 2007. Large language models in\nmachine translation. In Proc. EMNLP, pages 858–\n867.\nIvan Bulyko, Mari Ostendorf, and Andreas Stolcke.\n2003. Getting more mileage from web text sources for\nconversational speech language modeling using class-\ndependent mixtures. In Proc. HLT, pages 7–9.\nStanley F. Chen and Joshua Goodman. 1996. An empir-\nical study of smoothing techniques for language mod-\neling. In Proc. ACL, pages 310–318.\nW. Chen, D. Grangier, and M. Auli. 2015. Strategies for\nTraining Large V ocabulary Neural Language Models.\nArXiv e-prints, December.\n1171\nStephen Della Pietra, Vincent Della Pietra, Robert L Mer-\ncer, and Salim Roukos. 1992. Adaptive language\nmodeling using minimum discriminant estimation. In\nProc. ACL, pages 103–106.\nGreg Durrett and Dan Klein. 2011. An empirical investi-\ngation of discounting in cross-domain language mod-\nels. In Proc. ACL.\nIrving J Good. 1953. The population frequencies of\nspecies and the estimation of population parameters.\nBiometrika, 40(3-4):237–264.\nC ¸ aglar G¨ulc ¸ehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Lo ¨ıc Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On us-\ning monolingual corpora in neural machine translation.\nCoRR, abs/1503.03535.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780.\nBo-June Hsu. 2007. Generalized linear interpolation of\nlanguage models. In Proc. ASRU, pages 136–140.\nRukmini M Iyer and Mari Ostendorf. 1999. Modeling\nlong distance dependence in language: Topic mixtures\nversus dynamic cache models. Speech and Audio Pro-\ncessing, IEEE Transactions on, 7(1):30–39.\nFrederick Jelinek and Robert Mercer. 1980. Interpolated\nestimation of markov source parameters from sparse\ndata. In Workshop on pattern recognition in practice.\nSlava M Katz. 1987. Estimation of probabilities from\nsparse data for the language model component of a\nspeech recognizer. IEEE Transactions on Acoustics,\nSpeech and Signal Processing, 35(3):400–401.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. Proc. ICLR.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In Proc.\nICASSP, volume 1, pages 181–184. IEEE.\nReinhard Kneser and V olker Steinbiss. 1993. On the\ndynamic adaptation of stochastic language models. In\nProc. ICASSP, pages 586–589.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2015. A diversity-promoting objec-\ntive function for neural conversation models. CoRR,\nabs/1510.03055.\nTomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan Cer-\nnock`y, and Sanjeev Khudanpur. 2010. Recurrent neu-\nral network based language model. In Proc. Inter-\nSpeech, pages 1045–1048.\nTom´aˇs Mikolov, Anoop Deoras, Daniel Povey, Luk ´aˇs\nBurget, and Jan ˇCernock`y. 2011. Strategies for train-\ning large scale neural network language models. In\nProc. ASRU, pages 196–201. IEEE.\nMasami Nakamura, Katsuteru Maruyama, Takeshi\nKawabata, and Kiyohiro Shikano. 1990. Neural net-\nwork approach to word category prediction for English\ntexts. In Proc. COLING.\nToshiaki Nakazawa, Hideya Mino, Isao Goto, Graham\nNeubig, Sadao Kurohashi, and Eiichiro Sumita. 2015.\nOverview of the 2nd Workshop on Asian Translation.\nIn Proc. WAT.\nHermann Ney, Ute Essen, and Reinhard Kneser. 1994.\nOn structuring probabilistic dependences in stochastic\nlanguage modelling. Computer Speech and Language,\n8(1):1–38.\nVu Pham, Th ´eodore Bluche, Christopher Kermorvant,\nand J ´erˆome Louradour. 2014. Dropout improves re-\ncurrent neural networks for handwriting recognition.\nIn Proc. ICFHR, pages 285–290.\nRonald Rosenfeld. 1996. A maximum entropy approach\nto adaptive statistical language modelling. Computer\nSpeech and Language, 10(3):187–228.\nHolger Schwenk. 2007. Continuous space language\nmodels. Computer Speech and Language, 21(3):492–\n518.\nAlessandro Sordoni, Michel Galley, Michael Auli, Chris\nBrockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun\nNie, Jianfeng Gao, and Bill Dolan. 2015. A neu-\nral network approach to context-sensitive generation\nof conversational responses. In Proc. NAACL, pages\n196–205.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. The Journal of Machine Learning\nResearch, 15(1):1929–1958.\nMartin Sundermeyer, Ralf Schl ¨uter, and Hermann Ney.\n2012. LSTM neural networks for language modeling.\nIn Proc. InterSpeech.\nIlya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.\nSequence to sequence learning with neural networks.\nIn Proc. NIPS, pages 3104–3112.\nYee Whye Teh. 2006. A Bayesian interpretation of in-\nterpolated Kneser-Ney. Technical report, School of\nComputing, National Univ. of Singapore.\nWill Williams, Niranjani Prasad, David Mrva, Tom Ash,\nand Tony Robinson. 2015. Scaling recurrent neural\nnetwork language models. In Proc. ICASSP.\nIan H. Witten and Timothy C. Bell. 1991. The zero-\nfrequency problem: Estimating the probabilities of\nnovel events in adaptive text compression. IEEE\nTransactions on Information Theory , 37(4):1085–\n1094.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\nCoRR, abs/1409.2329.\n1172",
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.7393259406089783
    },
    {
      "name": "Computer science",
      "score": 0.71673583984375
    },
    {
      "name": "Scalability",
      "score": 0.6319372653961182
    },
    {
      "name": "Vocabulary",
      "score": 0.5854636430740356
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5807338356971741
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4297161102294922
    },
    {
      "name": "Statistical model",
      "score": 0.42030632495880127
    },
    {
      "name": "Programming language",
      "score": 0.0690278708934784
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I75917431",
      "name": "Nara Institute of Science and Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}