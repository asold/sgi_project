{
  "title": "MathAttack: Attacking Large Language Models towards Math Solving Ability",
  "url": "https://openalex.org/W4393160110",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2123087261",
      "name": "Zihao Zhou",
      "affiliations": [
        "Xi’an Jiaotong-Liverpool University"
      ]
    },
    {
      "id": "https://openalex.org/A2165844468",
      "name": "Wang Qiufeng",
      "affiliations": [
        "Xi’an Jiaotong-Liverpool University"
      ]
    },
    {
      "id": "https://openalex.org/A2103246445",
      "name": "Mingyu Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104636801",
      "name": "Jie Yao",
      "affiliations": [
        "Xi’an Jiaotong-Liverpool University"
      ]
    },
    {
      "id": "https://openalex.org/A2110509394",
      "name": "Jianan Ye",
      "affiliations": [
        "Xi’an Jiaotong-Liverpool University"
      ]
    },
    {
      "id": "https://openalex.org/A1973321923",
      "name": "Wei Liu",
      "affiliations": [
        "ShanghaiTech University"
      ]
    },
    {
      "id": "https://openalex.org/A2009336559",
      "name": "Wei Wang",
      "affiliations": [
        "Xi’an Jiaotong-Liverpool University"
      ]
    },
    {
      "id": "https://openalex.org/A2155758102",
      "name": "Xiao-Wei Huang",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A2142894599",
      "name": "Kaizhu Huang",
      "affiliations": [
        "Duke Kunshan University"
      ]
    },
    {
      "id": "https://openalex.org/A2123087261",
      "name": "Zihao Zhou",
      "affiliations": [
        "Xi’an Jiaotong-Liverpool University",
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A2165844468",
      "name": "Wang Qiufeng",
      "affiliations": [
        "Xi’an Jiaotong-Liverpool University"
      ]
    },
    {
      "id": "https://openalex.org/A2104636801",
      "name": "Jie Yao",
      "affiliations": [
        "University of Liverpool",
        "Xi’an Jiaotong-Liverpool University"
      ]
    },
    {
      "id": "https://openalex.org/A2110509394",
      "name": "Jianan Ye",
      "affiliations": [
        "University of Liverpool",
        "Xi’an Jiaotong-Liverpool University"
      ]
    },
    {
      "id": "https://openalex.org/A2009336559",
      "name": "Wei Wang",
      "affiliations": [
        "Xi’an Jiaotong-Liverpool University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4318719086",
    "https://openalex.org/W6846444394",
    "https://openalex.org/W4310436400",
    "https://openalex.org/W2971970905",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W3201205157",
    "https://openalex.org/W4225422851",
    "https://openalex.org/W6809853388",
    "https://openalex.org/W2963859254",
    "https://openalex.org/W3018458867",
    "https://openalex.org/W4221141431",
    "https://openalex.org/W6691715656",
    "https://openalex.org/W4319049323",
    "https://openalex.org/W6744845324",
    "https://openalex.org/W2964710271",
    "https://openalex.org/W2974581576",
    "https://openalex.org/W4386270118",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4283811884",
    "https://openalex.org/W6779254487",
    "https://openalex.org/W6607999579",
    "https://openalex.org/W4380994262",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W3138392969",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W4385570088",
    "https://openalex.org/W4389518941",
    "https://openalex.org/W4387425797",
    "https://openalex.org/W4287854578",
    "https://openalex.org/W4387725627",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W2757276219",
    "https://openalex.org/W4389520765",
    "https://openalex.org/W4385569771",
    "https://openalex.org/W4385565029",
    "https://openalex.org/W4310290453",
    "https://openalex.org/W4386566629",
    "https://openalex.org/W2251935656",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4288104702",
    "https://openalex.org/W4221161796",
    "https://openalex.org/W3035498813",
    "https://openalex.org/W3101449015",
    "https://openalex.org/W4379958452",
    "https://openalex.org/W4385572411",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4378465135"
  ],
  "abstract": "With the boom of Large Language Models (LLMs), the research of solving Math Word Problem (MWP) has recently made great progress. However, there are few studies to examine the robustness of LLMs in math solving ability. Instead of attacking prompts in the use of LLMs, we propose a MathAttack model to attack MWP samples which are closer to the essence of robustness in solving math problems. Compared to traditional text adversarial attack, it is essential to preserve the mathematical logic of original MWPs during the attacking. To this end, we propose logical entity recognition to identify logical entries which are then frozen. Subsequently, the remaining text are attacked by adopting a word-level attacker. Furthermore, we propose a new dataset RobustMath to evaluate the robustness of LLMs in math solving ability. Extensive experiments on our RobustMath and two another math benchmark datasets GSM8K and MultiAirth show that MathAttack could effectively attack the math solving ability of LLMs. In the experiments, we observe that (1) Our adversarial samples from higher-accuracy LLMs are also effective for attacking LLMs with lower accuracy (e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shot prompts); (2) Complex MWPs (such as more solving steps, longer text, more numbers) are more vulnerable to attack; (3) We can improve the robustness of LLMs by using our adversarial samples in few-shot prompts. Finally, we hope our practice and observation can serve as an important attempt towards enhancing the robustness of LLMs in math solving ability. The code and dataset is available at: https://github.com/zhouzihao501/MathAttack.",
  "full_text": "MathAttack: Attacking Large Language Models towards Math Solving Ability\nZihao Zhou1 2, Qiufeng Wang1 *, Mingyu Jin3, Jie Yao1 2\nJianan Ye1 2, Wei Liu4, Wei Wang1, Xiaowei Huang2, Kaizhu Huang5\n1School of Advanced Technology, Xi’an Jiaotong-Liverpool University\n2University of Liverpool\n3Northwestern University\n4ShanghaiTech University\n5Duke Kunshan University\nzihao.zhou@liverpool.ac.uk, Qiufeng.Wang@xjtlu.edu.cn\nAbstract\nWith the boom of Large Language Models (LLMs), the re-\nsearch of solving Math Word Problem (MWP) has recently\nmade great progress. However, there are few studies to exam-\nine the robustness of LLMs in math solving ability. Instead of\nattacking prompts in the use of LLMs, we propose aMathAt-\ntack model to attack MWP samples which are closer to the\nessence of robustness in solving math problems. Compared\nto traditional text adversarial attack, it is essential to pre-\nserve the mathematical logic of original MWPs during the at-\ntacking. To this end, we propose logical entity recognition to\nidentify logical entries which are then frozen. Subsequently,\nthe remaining text are attacked by adopting a word-level at-\ntacker. Furthermore, we propose a new dataset RobustMath\nto evaluate the robustness of LLMs in math solving ability.\nExtensive experiments on our RobustMath and two another\nmath benchmark datasets GSM8K and MultiAirth show that\nMathAttack could effectively attack the math solving abil-\nity of LLMs. In the experiments, we observe that (1) Our\nadversarial samples from higher-accuracy LLMs are also ef-\nfective for attacking LLMs with lower accuracy (e.g., trans-\nfer from larger to smaller-size LLMs, or from few-shot to\nzero-shot prompts); (2) Complex MWPs (such as more solv-\ning steps, longer text, more numbers) are more vulnerable\nto attack; (3) We can improve the robustness of LLMs by\nusing our adversarial samples in few-shot prompts. Finally,\nwe hope our practice and observation can serve as an im-\nportant attempt towards enhancing the robustness of LLMs\nin math solving ability. The code and dataset is available at:\nhttps://github.com/zhouzihao501/MathAttack.\nIntroduction\nSolving Math Word Problem (MWP) aims to infer a final an-\nswer from the natural language description of a math prob-\nlem (Wang, Liu, and Shi 2017). With the boom of Large\nLanguage Models (LLMs), the research of solving MWP has\nrecently made great progress (Qiao et al. 2022; Uesato et al.\n2022; Chang et al. 2023). Most of them work on prompt\nengineering to improve math solving ability of LLMs (Wei\net al. 2022; Zhou et al. 2023a; Kojima et al. 2022; Chen et al.\n2022; Fu et al. 2023b; Wang et al. 2023c; Yao et al. 2023),\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Different input of Large Language Models\n(LLMs). (a) Clean input, (b) Adversarial sample generated\nby Prompt-Attack (Zhu et al. 2023; Wang et al. 2023a), (c)\nAdversarial sample generated by our MathAttack.\nand LLMs (e.g., ChatGPT) can provide correct reasoning\nprocess and the final answer for simple math word prob-\nlems. Subsequently, they have been progressively applied in\nthe field of intelligence education (Macina et al. 2023; Wang\nand Demszky 2023; Wang et al. 2023d; Handa et al. 2023).\nTherefore, it becomes essential to examine the robustness\nof LLMs in math solving ability, but this has not attracted\nmuch attention so far. To the best of our knowledge, there\nare only a few works (Zhu et al. 2023; Wang et al. 2023a) to\nevaluate the robustness of LLMs through attacking prompts\n(Figure 1(b)). By comparing to prompt-attack, we argue that\nattacking MWP samples themselves is more direct to reflect\nthe robustness of LLMs in math solving ability, like Fig-\nure 1(c).\nOn the other hand, general text adversarial attack has\nmade great progress (Li et al. 2019, 2020; Ye et al. 2022;\nQian et al. 2022). This task aims to generate an adversar-\nial text x′ that is semantically similar to the original text x,\nwhile victim model f can correctly classify x but incorrectly\nclassify x′ (Jin et al. 2020; Xu et al. 2020). However, it tends\nto change mathematical logic by directly applying such tech-\nniques of general text adversarial attack. For example, if the\nword 140 in Figure 1(c) is modified to another number, the\nmathematical logic will be changed and the original ground-\ntruth will be no longer the correct answer. Therefore, it is es-\nsential to preserve the mathematical logic of MWPs, which\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19750\nmakes MWP adversarial attack more challenging.\nTo preserve the mathematical logic of MWPs, we propose\nMathAttack for attacking the math solving ability of large\nlanguage models. Figure 2 shows an overview of our Math-\nAttack. We first recognize logical entities, altering these log-\nical entities easily leads to changing the mathematical logic\nof math word problems. Then we freeze the logical entities,\npreventing the attacker from modifying logical entities. Fi-\nnally we attack the LLMs utilizing word-level attacker (Li\net al. 2020) while not changing those frozen logical words.\nWith the help of MathAttack and manual check, we propose\na new dataset RobustMath, which consists of 300 high-\nquality MWP adversarial samples and could measure the ro-\nbustness of LLMs’ math solving ability.\nExtensive experiments on our proposed Robust-\nMath dataset and another two math benchmark datasets\nGSM8K (Cobbe et al. 2021) and MultiAirth (Roy and Roth\n2015) show that our MathAttack could effectively attack\nthe math solving ability of LLMs. As far as we know,\nmost works (Zhu et al. 2023; Wang et al. 2023a) focus the\nrobustness of LLMs in general tasks, there are not any com-\nprehensive study on the robustness of LLMs in math solving\nability. To this end, we conduct a a serious of analysis in\nthe experiments and observe the following three points: (1)\nTransferability of attacking samples. Adversarial samples\ngenerated from higher-accuracy LLMs are also effective for\nattacking LLMs with lower accuracy (e.g., transfer from\nlarger to smaller-size LLMs, or from few-shot to zero-shot\nprompts); (2) Complex MWPs (such as more solving steps,\nlonger text, more numbers) are more vulnerable to attack;\n(3) We can improve the robustness of LLMs by using our\nattacking samples in few-shot prompts.\nIn summary, our contributions are as follows:\n• In this paper, we make a first attempt to attack MWP sam-\nples to examine the robustness of LLMs in math solving\nability.\n• We propose MathAttack for attacking the math solving\nability of LLMs, including Logical Entity Recognition,\nFreezing Logical Entity and text Attack.\n• We propose a new dataset RobustMath by adopting\nMathAttack and manual check. It consists of 300 high-\nquality MWP adversarial samples and could measure the\nrobustness of LLMs’ math solving ability.\n• Extensive experiments show that MathAttack could ef-\nfectively attack the math solving ability of LLMs.\nThrough the exhaustive analysis, we obtain three findings\nfor the robustness of LLMs in math solving ability.\nRelated Work\nMWP Solver Recent proposals intend to solve the\nproblem by using sequence or tree generation models.\n(Wang, Liu, and Shi 2017) presents a sequence-to-sequence\n(seq2seq) approach to generate the mathematical equation.\n(Xie and Sun 2019) propose a goal-driven tree-structured\n(GTS) model to generate the equation tree. This sequence-\nto-tree approach significantly improves the performance\nover the traditional seq2seq approaches. (Zhang et al. 2020)\nadopt a graph-to-tree approach to model the quality relations\nusing graph convolutional networks (GCN). Previous stud-\nies (Patel, Bhattamishra, and Goyal 2021; Zhou et al. 2023b;\nYao, Zhou, and Wang 2023) indicate these MWP solvers\nrely on shallow heuristics to generate equations. With the\nboom of Large Language Models (LLMs) and the proposal\nof chain-of-thought (Wei et al. 2022), the math solving abil-\nity of the model has recently made great progress. Many re-\nsearch works on prompt engineering to improve math solv-\ning ability (Zhou et al. 2023a; Kojima et al. 2022; Chen et al.\n2022; Fu et al. 2023b; Wang et al. 2023c; Yao et al. 2023),\nthey are capable of effortlessly solving simple MWPs, and\nLLMs are gradually being incorporated in the field of intel-\nligent education (Ji, Han, and Ko 2023; Macina et al. 2023;\nWang and Demszky 2023; Wang et al. 2023d; Handa et al.\n2023). In this context, examining the robustness of LLMs\nin math solving ability becomes essential. In this work, we\nmake a first attempt to examine this robustness issue by at-\ntacking MWP samples.\nLarge Language Models AttackPrevious proposals have\nalready tried to evaluate the robustness of large language\nmodels (Zhuo et al. 2023; Shi et al. 2023). (Wang et al.\n2023b) makes the first attempt to systematically evalu-\nate the robustness of LLMs by using robust datasets. Re-\ncently, some works propose to address this issue by attack-\ning prompts (Wang et al. 2023a; Zhu et al. 2023). (Wang\net al. 2023a) introduces the ICL attack based on TextAttack,\nwhich aims to manipulate the prompt only without altering\nthe input. (Zhu et al. 2023) presents PromptBench, a robust-\nness benchmark specifically designed to evaluate the robust-\nness of LLMs against adversarial prompts. Our work differs\nfrom theirs in two main aspects: (1) We specifically focus\non attacking the MWP sample itself, which provides a more\ndirect approach and fills the gap of non-prompt attacks on\nLLMs. (2) Their works target general tasks, lacking a com-\nprehensive analysis of the robustness in math solving ability.\nMWP Attack For the MWP solvers, previous works gen-\nerate some MWP adversarial examples by rule-based meth-\nods like reordering the problem description (Kumar, Ma-\nheshwary, and Pudi 2021; Patel, Bhattamishra, and Goyal\n2021). However, with the development of LLMs, the seman-\ntic and logical capabilities of the model have been enhanced,\nrendering these adversarial examples ineffective. Adversar-\nial MWP sample datasets SV AMP(Patel, Bhattamishra, and\nGoyal 2021) can be solved well by LLMs like ChatGPT. In\nthis paper, we attack MWP samples of LLMs for the first\ntime and propose a new dataset RobustMath to evaluate\nthe robustness of math solving ability of LLMs. It consists\nof adversarial examples generated by MathAttack, utilizing\nsimple MWPs from GSM8K and MultiAirth as seed data.\nMethodology\nProblem Formulation\nSuppose we have a textx with n words x = [w1, w2, ..., wn]\nwhose ground truth label is y. We call x′ an adversarial ex-\nample when x′ can make the victim model f wrong predic-\ntion but original correct prediction (f (x) =y), i.e.,\nf (x′) ̸= f (x) . (1)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19751\nFigure 2: The overview of MathAttack. First, we utilize an NER model to identify logical entities. Then we freeze the logical\nentities, preventing the attacker from modifying them. Finally, we utilize word-level attacker to attack the LLMs while not\nchanging those frozen logical entities.\nCompared to traditional text attack, math word problem at-\ntack need to preserve the mathematical logicalL of text sam-\nple x, it is defined as:\nL (x′) =L (x) . (2)\nThe goal of the attack task is to generate an adversarial ex-\nample x∗ among all x′. Since text data consists of discrete\nwords whose change can be perceived by humans, we al-\nways want the optimized adversarial example x∗ to be se-\nmantically closest to the original text sample x. Thus, the\nobjective function of this task can be defined as follows:\nx∗ = argmax\nx′\nG (x, x′) , s.t.f(x′) ̸= f (x) , L(x′) =L (x) ,\n(3)\nwhere G (x, x′) denotes the semantic similarity between x\nand x′. In this paper, f is the large language model and we\nfollow the black-box setting.\nThe Proposed MathAttack\nOverview Figure 2 shows an overview of MathAttack. We\nfirstly recognize logical entities. Altering these logical enti-\nties easily leads to changing the logic of math word prob-\nlems. Then we freeze the logical entities, preventing the at-\ntacker from modifying logical entities. Finally we attack the\nLLMs utilizing word-level attacker while not changing those\nfrozen logical entities.\nLogical Entity Recognition Logical entities are cru-\ncial components that constitute logic in math word prob-\nlems (Kumar, Maheshwary, and Pudi 2022; Li et al. 2022).\nIn order to preserve the logic of a math word problem, it is\nindispensable to define and identify which entities as logical\nentities. In this paper, we define the following three types of\nentities as logical entities. (1) Role Entity: It includes per-\nson entity (e.g, Asia in Figure 2). (2) Number Entity: It in-\ncludes quantity (e.g, $140 in Figure 2), cardinal number and\nordinal number. (3) Scenario Entity: It includes time entity\nand location entity. Altering these environmental factors is\neasy to change the logic of math word problems too.\nThen we employ Named Entity Recognition (NER) model\nto identify them:\nIro = NER ro (x) , (4)\nInum = NER num (x) , (5)\nIsce = NER sce (x) , (6)\nwhere It is a word index set if the word belongs to logical\nentity type t. The symbols ro, num and sce represent the\nRole, Number and Scenario Entity respectively. We utilize\nSpacy 1 as our NER model.\nFreezing Logical Entity It tends to break the original\nlogic of MWP by altering logical entities during the attack\nprocess. To this end, we freeze all logical entities in order to\nprohibit attackers from modifying them:\nIf = Iname ∪ Inum ∪ Isce, (7)\nwhere If denotes the frozen word index set.\nAttack Text attackers are generally classified into three\ntypes: char-level, word-level and sentence-level. In MathAt-\ntack, We choose word-level attacker because the char-level\nattacker can distort the semantic meaning of words (like Fig-\nure 1(b)) and sentence-level attacker are prone to disrupting\nthe mathematical logic of MWP. The attack process of word-\nlevel attacker primarily entails two steps: finding vulnerable\nwords and words replacement.\nIn order to find vulnerable words, it is necessary to deter-\nmine which words are significant. Specifically, we first se-\nquentially mask all modifiable words to form new sentences.\nAfterward, we predict each new sentence to get the drop in\n1https://spacy.io/\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19752\nthe probability of the correct answer. The more it drops, the\nmore important the word is. It is defined as:\nxmask\ni = [w1, w2, , wi−1, mask, wi+1, ..., wn] , i /∈ If ,\n(8)\nai = prob (f (x)) − prob\n\u0000\nf\n\u0000\nxmask\ni\n\u0001\u0001\n, (9)\nwhere ai is the important score ofxi and prob is the function\nto get the probability of the correct answer. After that, we\ncan get the important scores list a = [a1, a2, ..., an]. Notice\nthat the length of a is not n because some words are frozen.\nFinally, we choose the word which has the max important\nscore as the vulnerable word:\nm = argmax (a) , (10)\nwhere m is the index of the vulnerable word andargmax is\nthe function to pop the word which has the most important\nscore and get its index.\nAfter finding the vulnerable word, we proceed to locate\nall synonyms of wm in order to substitute it:\nS = Synonyms (wm) , (11)\nwhere S is the synonyms set ofwm, we sequentially select a\nword in S based on the similarity to wm then substitute wm:\ns′ = MaxSim (S, wm) , (12)\nxs = [w1, w2, , wm−1, s′, wm+1, ..., wn] , (13)\nwhere xs is the sentence by replacing wm in x with s′.\nMaxSim is the function to pop the word in S that is most\nsimilar to wm. Notice that if S is already empty before pop-\nping, we go back to Eqn. (10) and repeat the above process.\nAfter obtaining xs, we perform different actions based on\nthe following situations, if f (xs) ̸= f (x), the final adver-\nsarial sample x∗ is xs:\nx∗ = xs. (14)\nIf f (xs) =f (x) and prob (f (xs)) < prob(f (x)), we will\nkeep this word change:\nx = xs. (15)\nThen go back to Eqn. (12) and repeat the above process. If\nf (xs) = f (x) and prob (f (xs)) ≥ prob (f (x)), we will\nabandon this word change then go back to Eqn. (12) and\nrepeat the above process.\nIn our attacker, we utilize BertAttack (Li et al. 2020) as\nour backbone, which utilizes [mask] token to mask words\nand bert embedding to calculate the similarity of words.\nExperiments\nExperimental Setting\nVictim Models We choose four mainstream large lan-\nguage models as our victim models.\n• Flan-T5-large (Chung et al. 2022): Flan-T5-large is a\nderivative of the Text-to-Text Transfer Transformer (T5)\nmodel, developed by Google. It has 760M parameters.\n• Flan-T5-xl (Chung et al. 2022): Flan-T5-xl is a large\nversion of Flan-T5 than Flan-T5-large, developed by\nGoogle. It has 3B parameters.\n• ChatGLM2 (Du et al. 2022): ChatGLM2 is the\nsecond-generation version of the open-source bilingual\n(Chinese-English) chat model ChatGLM, developed by\nTsinghua University. It has 6B parameters.\n• ChatGPT (OpenAI 2023): Developed by OpenAI, Chat-\nGPT is a large language model trained to generate\nhuman-like text. It uses the GPT-3 architecture and has\nbeen fine-tuned for more interactive and conversational\ntasks. In detail, we use the gpt-3.5-turbo API.\nWe set the temperature = 0 to stabilize the output of LLMs.\nWhen attacking victim models, we not only attack them with\nzero-shot prompt but also few-shot prompt. Specifically, we\nemploy four MWP samples as shots and provide Chain-of-\nThought (CoT) (Wei et al. 2022) annotations. This few-shot\nprompt serves as a method to enhance the math solving\nability of LLMs. Similar with other prompts, they are not\nchanged during the attack process.\nDatasets Two math word problems benchmark datasets\nGSM8K (Cobbe et al. 2021) andMultiArith (Roy and Roth\n2015) are adopted in the experiments. However, we only se-\nlect the subsets for the following considerations by follow-\ning the previous work (Zhu et al. 2023): (1) we focus on\nsimple MWPs, because hard samples have very lower ac-\ncuracy not necessary to attack. (2) Owing to the extensive\ncomputational requirements of generating single adversarial\nsample, which necessitates iterating over the entire dataset\n100 times on average. Finally, for GSM8K, we firstly re-\nmove those hard samples labelled by more than three solv-\ning steps, then randomly select half of those remained sim-\nple MWPs and obtain 307 MWP samples. For MultiAirth,\nall MWPs are simple thus we randomly select 150 MWPs\nsimilar with the previous work (Zhu et al. 2023).\nMetrics Given a dataset D with N data instance x and la-\nbel y, victim model f, an adversarial attack method A that\ngenerates adversarial examples A(x), we adopt following\nfour metrics:\n• Clean Acc: The accuracy before attacking. Clean Acc =P\n(x,y)∈D I[f(x)=y]\nN .\n• Attack Acc: The accuracy after attacking. Attack Acc =P\n(x,y)∈D I[f(x)=y∩f(A(x))=y]\nN .\n• Attack Success Rate (ASR) (Wang et al. 2023a):\nThe rate of samples is successfully attacked. ASR =P\n(x,y)∈D I[f(A(x))̸=y]P\n(x,y)∈D I[f(x)=y] .\n• Similarity: The average semantic similarity between the\nadversarial sample and the original sample. We use Uni-\nversal Sentence Encoder (Cer et al. 2018) to measure se-\nmantic similarity.\nTo ensure the correctness in the experiments, we check each\nadversarial sample manually, and consider adversarial ex-\namples which are changed mathematical logic as unsuccess-\nful attacks.\nMain Results\nAs shown in Table 1, our approach can effectively attack\nthe math solving ability of large language models. For\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19753\nGSM8K MultiAirth\nPrompt\nModels Clean Acc\nAttack Acc ASR Similarity Clean Acc\nAttack Acc ASR Similarity\nZero shot\nFlan-T5-large 18.24 2.28\n87.50 90.55 2.00 0.00\n100.00 91.42\nFlan-T5-xl 21.17 3.58\n83.08 92.86 7.33 0.67\n90.90 95.63\nChatGLM2 54.40 23.78\n56.29 91.58 71.33 20.67\n71.03 94.00\nChatGPT 84.69 49.54\n41.15 89.26 98.67 60.00\n39.19 91.33\nFew\nshot Flan-T5-large 22.15 10.42\n52.94 92.92 5.33 0.67\n87.5 95.66\nFlan-T5-xl 32.35 17.59\n45.45 90.00 10.67 2.67\n75.00 95.16\nChatGLM2 64.82 22.80\n64.82 90.71 37.33 7.33\n80.36 95.75\nChatGPT 88.27 70.68\n19.93 87.19 98.00 77.33\n21.09 86.97\nTable 1: Results of attacking against various large language models.\nFigure 3: Transfer Success Rate (TSR) of Y-axis models to\nX-axis models. The generated adversarial samples of larger-\nsize models can attack smaller-size models.\nLLMs with zero-shot, we could get the high ASR, even for\nChatGPT, it could achieve an average of 40% on GSM8K\n(41.15%) and MultiAirth (39.19%). The average Similarity\nis large than 90%, indicating that we can successfully gener-\nate adversarial samples with high similarity and do not alter\nmathematical logic.\nComparing different LLMs, we can observe that more\npowerful LLMs (i.e., higher Clean Acc) are more difficult\nto attack (i.e., lower ASR). For Flan-T5-large and Flan-T5-\nxl, their robustness in math solving ability is poor, as even\na slight disturbance can cause them to predict incorrectly.\nFor ChatGLM2 and ChatGPT, their robustness is noticeably\nstronger, as our method fails to attack them on some MWP\nsamples.\nFurthermore, comparing zero-shot and few-shot, we can\nsee that employing few-shot could enhance the math solv-\ning ability of the LLMs and also make them more ro-\nbust, leading to a lower ASR. For models with stronger in-\ncontext ability, the enhancement becomes larger. Like Chat-\nGPT, the Attack Success Rate could decrease from 41.15%\nto 19.93%. However, we find ChatGLM2 exhibits poor in-\ncontext ability which leads math solving ability as well as\nrobustness does not improve with the few-shot prompt.\nFigure 4: Transfer Success Rate (TSR) of Y-axis prompt to\nX-axis prompt. The generated adversarial samples of model\nwith few-shot can attack model with zero-shot.\nFine-grained Analysis\nTransferability To test the transferability of the generated\nadversarial samples, we take adversarial samples of modelA\nto attack other models B. Specifically, we select the samples\nthat B can correctly predict as the experimental samples.\nSubsequently, we provideB with adversarial samples gener-\nated by attacking A on experimental samples. We examine if\nthese adversarial samples can successfully attack model B.\nHere, we propose the metric: Transfer Success Rate (TSR),\nif an adversarial sample of A can successfully attack model\nB then it means transfer success.\nIn Figure 3, we show the TSR between Y-axis (i.e., A\nmodel) and X-axis (i.e., B model) models, and we can\nobserve that the adversarial samples of larger-size mod-\nels can attack smaller-size models too. ChatGPT could get\n94.44% TSR to Flan-T5-large and 89.47% TSR to Flan-T5-\nxl. Specifically, we find that the TSR will increase when\nthe math solving ability between models grows wider. As\nshown in Figure 3, we can see that the adversarial samples of\nsmaller-size models can not attack larger-size models. Flan-\nT5-large and Flan-T5-xl both show low TSR (6.25% and\n6.67%) on ChatGPT. In this experiment, all tested models\nare in zero-shot setting.\nIn order to see the transferability performance between\nzero-shot and few-shot, we conducted the same experiment\non ChatGPT. As shown in Figure 4, the ChatGPT with few-\nshot can achieve 45.24% TSR to ChatGPT with zero-shot\nhowever the reverse is only 20.56%. It indicates that the ad-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19754\nFigure 5: Analysis which MWPs are easier to attack. (a) shows the effect of answer reasoning steps on the ASR. (b) shows the\neffect of problem length on the ASR. (c) shows the effect of numbers’ count in MWP on the ASR.\nClean Acc Attack Acc ASR Similarity\nGSM8K 87.95 75.57 14.07 88.33\nMultiAirth 98.00 82.00 16.33 88.15\nTable 2: Results of attacking against large language models\nwith adversarial samples prompt on ChatGPT.\nversarial samples of LLM with few-shot can attack that with\nzero-shot. And adversarial samples of LLM with zero-shot\ncan not transfer to that with few-shot.\nAnalysis on MWPs To know which MWPs are easier to\nattack, we investigate the effects of MWPs reasoning steps,\nproblem length and number count on ASR. Specifically, we\nconducted the experiment on ChatGPT in zero-shot setting.\nAs shown in Figure 5: (a) with the increase of reasoning\nsteps of ground truth, we can observe that the ASR will in-\ncrease when the reasoning steps from 2 to 3. Reasoning steps\nof ground truth can be regarded as a metric to measure the\ndifficulty of MWP. Difficult MWPs are easier to attack. (b)\nwith the increase in problem length, we can observe a grad-\nual increase in the ASR as the length of the math word prob-\nlems become longer. Long MWPs are easier to attack. (c)\nwith the increase in the quantity of numbers in MWPs, we\ncan observe a gradual increase in ASR as the number counts\nbecome more. All the above three factors can be used to\nmeasure the complexity of an MWP (Fu et al. 2023b), there-\nfore, we can draw a conclusion that more complex MWPs\nare easier to attack. It shows that LLMs are sensitive to dis-\nturbances on MWPs with complex mathematical logic.\nUsing Attacking Samples as PromptsWe also study the\nimpact of adversarial samples on improving the robustness\nof large language models. In the few-shot prompts, we re-\nplace normal MWP examples by corresponding adversarial\nexamples generated by our MathAttack but with correct la-\nbels, and observe their impact on the math solving ability\nand robustness of the LLMs. As shown in Table 2, we can\nsee the Clean ACC still maintains a high level of accuracy\n(87.95% on GSM8K and 98.00% on MultiAirth), because\nthe adversarial examples generated by our MathAttack ex-\nFigure 6: Trend of ASR after utilizing different prompts, At-\ntack Few Shot means replacing the MWP samples of Few\nShot to their corresponding adversarial samples.\nhibit high similarity to the original samples. By comparing\nthe Attack Acc and ASR in Table 1, it is surprised to find\nthe use of adversarial examples in the few-shot prompts can\nenhance the robustness of LLMs (i.e., much lower ASR by\ncomparing to the normal results in Table 1). When we use\nadversarial examples in the prompt, the LLM could see these\nexamples that are disturbed but still able to predict correctly,\ntherefore they will not affected by some small disturbances\nwhen predict. Figure 6 provides a more intuitive visualiza-\ntion, demonstrating that the robustness of LLMs utilizing\nfew-shot prompt can be significantly improved by compar-\ning to zero-shot prompt. When employing adversarial exam-\nples as few-shot prompt, it will further strengthen their ro-\nbustness and the ASR of large language models further de-\ncrease. This observation motivates us to enhance the robust-\nness of large language models without compromising their\nmath solving ability by employing the adversarial examples\ngenerated by MathAttack as few-shot prompt.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19755\nOrginal\nSample Adversarial\nSample\nA class of 50\nstudents has various hobbies. 10 like to bake\n5 like to play basketball and the rest like to either play\nvideo games or play music. How many like to play video\ngames if the number that like to play music is twice the\nnumber that prefer playing basketball?\nA group of\n50 students has various hobbies. 10 like to bake\n5 like to play basketball and the rest like to either play\nvideo games or play music. How many like to play video\ngames if the number that like to play music is twice the\nnumber that prefer playing basketball?\nSolution:\nLet’s\nassume the number of students who like to\nplay video games is ”x”. ...... So, we can set up an equation:\n10 + 5 + x + 10 = 50. Simplifying the equation:\n10 + 5 + x + 10 = 50. 25 + x = 50. x = 50 - 25. x = 25.\nTherefore, the number of students who like to play video\ngames is 25. (Correct!)\nSolution:\nLet’s\nassume the number of students who like to\nplay video games is ”x”. ...... We can set up the equation:\nx + 10 = 50. Now, let’s solve for x: x = 50 - 10. x = 40.\nTherefore, the number of students who like to play video\ngames is 40. (Wrong!)\nTable 3: A real case predicted by ChatGPT on original MWP (left) and its adversarial sample (right).\nZero-shot F\new-shot\nOriginal Rob\nustMath Original Rob\nustMath\nFlan-T5-large 10.75 4.67 13.08 10.67\nFlan-T5-xl 17.76 6.00 26.17 20.33\nFlan-T5-xl-F 16.36 12.33 10.19 9.33\nChatGLM2 47.08 36.67 54.67 33.67\nTable 4: Accuracy of large language models on RobustMath\nand its original samples set. Flan-T5-xl-F is the finetuned\nmodel on 200k MWP data (Fu et al. 2023a).\nCase Study\nTable 3 reports a real case predicted by ChatGPT on the orig-\ninal MWP and its adversarial sample generated by Math-\nAttack. We can find that the adversarial sample generated\nby MathAttack is similar to the original sample with few\nchanges. For the original sample, ChatGPT can give the cor-\nrect reasoning process step by step and finally get the correct\nanswer 25. But when MathAttack simply changed class in\nthe original sample to group, ChatGPT can come up with\nthe wrong reasoning process and get the wrong equation\n(x+10=50), end up with the wrong answer 40. These cases\nshow that the robustness of LLMs in math solving ability\nstill needs to be strengthened.\nNew MWP Dataset RobustMath\nUsing the transferability of adversarial samples, we attack\nChatGPT by MathAttack to build our RobustMath dataset.\nSpecifically, we first utilize GSM8K and MultiAirth as our\nseed data then attack ChatGPT to generate adversarial sam-\nples. After that, in order to ensure the high quality of Ro-\nbustMath, we manually check each adversarial sample and\nfilter out samples which change the mathematical logic. Ul-\ntimately, our RobustMath has 300 high-quality adversarial\nsamples that can be used to measure the robustness of large\nlanguage models’ math solving ability.\nTo verify the effectiveness of our RobustMath, we evalu-\nate large language models on RobustMath. In addition to the\nmodels mentioned above, we also evaluate large language\nmodel that is fine-tuned on MWP datasets. Specifically, we\nfollow (Fu et al. 2023a) to finetune Flan-T5-xl with 200k\nMWP data. In Table 4, we observe that the performance of\nthe LLMs on RobustMath is significantly worse compared to\nthe performance on its original samples. This indicates that\nour RobustMath can effectively measure the robustness of\nthe model’s math solving ability. When examining the per-\nformance of models with zero-shot performance, we can see\nthat as the model’s capability increases, its performance on\nRobustMath also increases. However, it still does not ex-\nceed 37.00%. Moreover, after finetuning, the performance\nof Flan-T5-xl increases from 6.00% to 12.33%. It indicates\nthat finetuning on specific data could help improve the ro-\nbustness of models. Observing the performance of models\nwith few-shot prompt, we find that models with a strong\nin-context ability such as Flan-T5-large and Flan-T5-xl can\neffectively enhance their performance on RobustMath. In\ncontrast, ChatGLM2 and finetuned Flan-T5-xl which have\nweaker in-context ability do not exhibit significant improve-\nments on both the original samples set and RobustMath un-\nder few-shot prompt.\nConclusion and Future Work\nIn this paper, we make a first attempt to attack MWP sam-\nples to examine the security of LLMs in math solving abil-\nity. To preserve the mathematical logic of MWPs, we pro-\npose a MathAttack model with a logical entity recognition\nblock. Extensive experiments show that MathAttack could\neffectively attack the math solving ability. Through the com-\nprehensive experimental analysis, we have three significant\nfindings: (1) Transferability of attacking samples (2) Com-\nplex MWPs (such as more solving steps, longer text, more\nnumbers) are more vulnerable to attack, and (3) Attacking\nsamples used in few-shot prompts can improve robustness of\nLLMs. Furthermore, we propose a new datasetRobustMath\nby utilizing MathAttack and manual check, which consists\nof high-quality MWP Adversarial samples and could mea-\nsure the robustness of LLMs’ math solving ability. We hope\nour practice and observations can serve as an important at-\ntempt to enhance the robustness of LLMs in math solving\nability. In the future, we will explore methods such as in-\nstruction learning or reinforcement learning to enhance the\nrobustness of the models. As large language models are in-\ncreasingly being applied in the field of intelligence educa-\ntion, the importance of improving their robustness becomes\nmore significant.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19756\nAcknowledgements\nThe work was partially supported by the following: Na-\ntional Natural Science Foundation of China under No.\n92370119, No. 62376113, and No. 62276258; Jiangsu Sci-\nence and Technology Programme (Natural Science Foun-\ndation of Jiangsu Province) under no. BE2020006-4, Eu-\nropean Union’s Horizon 2020 research and innovation\nprogramme no. 956123, and UK EPSRC under projects\n[EP/T026995/1].\nReferences\nCer, D.; Yang, Y .; Kong, S.-y.; Hua, N.; Limtiaco, N.; John,\nR. S.; Constant, N.; Guajardo-Cespedes, M.; Yuan, S.; Tar,\nC.; et al. 2018. Universal sentence encoder. arXiv preprint\narXiv:1803.11175.\nChang, Y .; Wang, X.; Wang, J.; Wu, Y .; Zhu, K.; Chen, H.;\nYang, L.; Yi, X.; Wang, C.; Wang, Y .; et al. 2023. A sur-\nvey on evaluation of large language models. arXiv preprint\narXiv:2307.03109.\nChen, W.; Ma, X.; Wang, X.; and Cohen, W. W. 2022.\nProgram of thoughts prompting: Disentangling computa-\ntion from reasoning for numerical reasoning tasks. arXiv\npreprint arXiv:2211.12588.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-\ndus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al.\n2022. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416.\nCobbe, K.; Kosaraju, V .; Bavarian, M.; Chen, M.; Jun, H.;\nKaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.;\net al. 2021. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168.\nDu, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and\nTang, J. 2022. GLM: General Language Model Pretraining\nwith Autoregressive Blank Infilling. In Proceedings of the\n2022 conference on Annual Meeting of the Association for\nComputational Linguistics, 320–335.\nFu, Y .; Peng, H.; Ou, L.; Sabharwal, A.; and Khot, T. 2023a.\nSpecializing Smaller Language Models towards Multi-Step\nReasoning.\nFu, Y .; Peng, H.; Sabharwal, A.; Clark, P.; and Khot, T.\n2023b. Complexity-based prompting for multi-step reason-\ning.\nHanda, K.; Clapper, M.; Boyle, J.; Wang, R. E.; Yang, D.;\nYeager, D.; and Demszky, D. 2023. Mistakes Help Us\nGrow”: Facilitating and Evaluating Growth Mindset Sup-\nportive Language in Classrooms. In The 2023 Conference\non Empirical Methods in Natural Language Processing.\nJi, H.; Han, I.; and Ko, Y . 2023. A systematic review of\nconversational AI in language education: Focusing on the\ncollaboration with human teachers. Journal of Research on\nTechnology in Education, 55(1): 48–63.\nJin, D.; Jin, Z.; Zhou, J. T.; and Szolovits, P. 2020. Is bert\nreally robust? a strong baseline for natural language attack\non text classification and entailment. In Proceedings of the\n2020 conference on Association for the Advancement of Ar-\ntificial Intelligence, volume 34, 8018–8025.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large language models are zero-shot reason-\ners. Advances in neural information processing systems, 35:\n22199–22213.\nKumar, V .; Maheshwary, R.; and Pudi, V . 2021. Adversar-\nial Examples for Evaluating Math Word Problem Solvers.\nIn Findings of the Association for Computational Linguis-\ntics: 2021 conference on Empirical Methods in Natural Lan-\nguage Processing, 2705–2712.\nKumar, V .; Maheshwary, R.; and Pudi, V . 2022. Practice\nmakes a solver perfect: Data augmentation for math word\nproblem solvers. 4194–4206.\nLi, A.; Xiao, Y .; Liang, J.; and Chen, Y . 2022. Semantic-\nbased data augmentation for math word problems. In Pro-\nceedings of the 2022 conference on Database Systems for\nAdvanced Applications, 36–51. Springer.\nLi, J.; Ji, S.; Du, T.; Li, B.; and Wang, T. 2019. Textbugger:\nGenerating adversarial text against real-world applications.\nLi, L.; Ma, R.; Guo, Q.; Xue, X.; and Qiu, X. 2020. Bert-\nattack: Adversarial attack against bert using bert. 6193–\n6202.\nMacina, J.; Daheim, N.; Chowdhury, S. P.; Sinha, T.; Ka-\npur, M.; Gurevych, I.; and Sachan, M. 2023. MathDial: A\nDialogue Tutoring Dataset with Rich Pedagogical Proper-\nties Grounded in Math Reasoning Problems. arXiv preprint\narXiv:2305.14536.\nOpenAI. 2023. Gpt-4 technical report.\nPatel, A.; Bhattamishra, S.; and Goyal, N. 2021. Are NLP\nModels really able to Solve Simple Math Word Problems? In\nProceedings of the 2021 conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2080–2094.\nQian, Z.; Huang, K.; Wang, Q.-F.; and Zhang, X.-Y . 2022. A\nsurvey of robust adversarial training in pattern recognition:\nFundamental, theory, and methodologies. Pattern Recogni-\ntion, 131: 108889.\nQiao, S.; Ou, Y .; Zhang, N.; Chen, X.; Yao, Y .; Deng,\nS.; Tan, C.; Huang, F.; and Chen, H. 2022. Reasoning\nwith language model prompting: A survey. arXiv preprint\narXiv:2212.09597.\nRoy, S.; and Roth, D. 2015. Solving General Arithmetic\nWord Problems. In Proceedings of the 2015 conference on\nEmpirical Methods in Natural Language Processing, 1743–\n1752.\nShi, F.; Chen, X.; Misra, K.; Scales, N.; Dohan, D.; Chi,\nE. H.; Sch¨arli, N.; and Zhou, D. 2023. Large language mod-\nels can be easily distracted by irrelevant context. InProceed-\nings of the 2023 conference on International Conference on\nMachine Learning, 31210–31227. PMLR.\nUesato, J.; Kushman, N.; Kumar, R.; Song, F.; Siegel, N.;\nWang, L.; Creswell, A.; Irving, G.; and Higgins, I. 2022.\nSolving math word problems with process-and outcome-\nbased feedback. arXiv preprint arXiv:2211.14275.\nWang, J.; Liu, Z.; Park, K. H.; Chen, M.; and Xiao, C.\n2023a. Adversarial Demonstration Attacks on Large Lan-\nguage Models. arXiv preprint arXiv:2305.14950.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19757\nWang, J.; Xixu, H.; Hou, W.; Chen, H.; Zheng, R.; Wang,\nY .; Yang, L.; Ye, W.; Huang, H.; Geng, X.; et al. 2023b.\nOn the Robustness of ChatGPT: An Adversarial and Out-\nof-distribution Perspective. In International Conference on\nLearning Representations 2023 Workshop on Trustworthy\nand Reliable Large-Scale Machine Learning Models.\nWang, L.; Xu, W.; Lan, Y .; Hu, Z.; Lan, Y .; Lee, R. K.-W.;\nand Lim, E.-P. 2023c. Plan-and-solve prompting: Improv-\ning zero-shot chain-of-thought reasoning by large language\nmodels. arXiv preprint arXiv:2305.04091.\nWang, R. E.; and Demszky, D. 2023. Is ChatGPT a\nGood Teacher Coach? Measuring Zero-Shot Performance\nFor Scoring and Providing Actionable Insights on Class-\nroom Instruction. arXiv preprint arXiv:2306.03090.\nWang, R. E.; Zhang, Q.; Robinson, C.; Loeb, S.; and Dem-\nszky, D. 2023d. Step-by-Step Remediation of Students’\nMathematical Mistakes. arXiv preprint arXiv:2310.10648.\nWang, Y .; Liu, X.; and Shi, S. 2017. Deep neural solver for\nmath word problems. In Proceedings of the 2017 confer-\nence on Empirical Methods in Natural Language Process-\ning, 845–854.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nXie, Z.; and Sun, S. 2019. A Goal-Driven Tree-Structured\nNeural Model for Math Word Problems. In Proceedings of\nthe 2019 conference on International Joint Conference on\nArtificial Intelligence, 5299–5305.\nXu, H.; Ma, Y .; Liu, H.-C.; Deb, D.; Liu, H.; Tang, J.-L.;\nand Jain, A. K. 2020. Adversarial attacks and defenses in\nimages, graphs and text: A review. International Journal of\nAutomation and Computing, 17: 151–178.\nYao, J.; Zhou, Z.; and Wang, Q. 2023. Solving Math Word\nProblem with Problem Type Classification. In CCF Inter-\nnational Conference on Natural Language Processing and\nChinese Computing, 123–134. Springer.\nYao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao,\nY .; and Narasimhan, K. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. Advances in\nNeural Information Processing Systems.\nYe, M.; Miao, C.; Wang, T.; and Ma, F. 2022. TextHoaxer:\nbudgeted hard-label adversarial attacks on text. In Proceed-\nings of the 2022 conference on Association for the Advance-\nment of Artificial Intelligence, volume 36, 3877–3884.\nZhang, J.; Wang, L.; Lee, R. K.-W.; Bin, Y .; Wang, Y .; Shao,\nJ.; and Lim, E.-P. 2020. Graph-to-tree learning for solving\nmath word problems. In Proceedings of the 2020 confer-\nence on Annual Meeting of the Association for Computa-\ntional Linguistics, 3928–3937.\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q.; et al.\n2023a. Least-to-most prompting enables complex reasoning\nin large language models.\nZhou, Z.; Ning, M.; Wang, Q.; Yao, J.; Wang, W.; Huang, X.;\nand Huang, K. 2023b. Learning by Analogy: Diverse Ques-\ntions Generation in Math Word Problem. In Findings of the\nAssociation for Computational Linguistics, 11091–11104.\nZhu, K.; Wang, J.; Zhou, J.; Wang, Z.; Chen, H.; Wang,\nY .; Yang, L.; Ye, W.; Gong, N. Z.; Zhang, Y .; et al. 2023.\nPromptBench: Towards Evaluating the Robustness of Large\nLanguage Models on Adversarial Prompts. arXiv preprint\narXiv:2306.04528.\nZhuo, T. Y .; Li, Z.; Huang, Y .; Li, Y .-F.; Wang, W.; Haf-\nfari, G.; and Shiri, F. 2023. On robustness of prompt-based\nsemantic parsing with large pre-trained language model: An\nempirical study on codex. arXiv preprint arXiv:2301.12868.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19758",
  "topic": "Mathematics education",
  "concepts": [
    {
      "name": "Mathematics education",
      "score": 0.5206122398376465
    },
    {
      "name": "Computer science",
      "score": 0.4136843979358673
    },
    {
      "name": "Mathematics",
      "score": 0.3854812979698181
    },
    {
      "name": "Cognitive science",
      "score": 0.33642178773880005
    },
    {
      "name": "Psychology",
      "score": 0.3182949721813202
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I69356397",
      "name": "Xi’an Jiaotong-Liverpool University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I30809798",
      "name": "ShanghaiTech University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I146655781",
      "name": "University of Liverpool",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210159968",
      "name": "Duke Kunshan University",
      "country": "CN"
    }
  ],
  "cited_by": 6
}