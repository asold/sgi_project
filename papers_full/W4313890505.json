{
    "title": "Full-Memory Transformer for Image Captioning",
    "url": "https://openalex.org/W4313890505",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5028317852",
            "name": "Tongwei Lu",
            "affiliations": [
                "Wuhan Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5090403843",
            "name": "Jiarong Wang",
            "affiliations": [
                "Wuhan Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5013798584",
            "name": "Feng Min",
            "affiliations": [
                "Wuhan Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6741121127",
        "https://openalex.org/W2254252455",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W2560313346",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2506483933",
        "https://openalex.org/W1987835821",
        "https://openalex.org/W1905882502",
        "https://openalex.org/W8316075",
        "https://openalex.org/W2105103432",
        "https://openalex.org/W2888321701",
        "https://openalex.org/W2954182683",
        "https://openalex.org/W6756667841",
        "https://openalex.org/W2970569830",
        "https://openalex.org/W6731356633",
        "https://openalex.org/W2597985671",
        "https://openalex.org/W6692647644",
        "https://openalex.org/W3101313921",
        "https://openalex.org/W2754927243",
        "https://openalex.org/W2575842049",
        "https://openalex.org/W6698228248",
        "https://openalex.org/W6730549503",
        "https://openalex.org/W2795151422",
        "https://openalex.org/W2963101956",
        "https://openalex.org/W3034984754",
        "https://openalex.org/W2799009156",
        "https://openalex.org/W2901988662",
        "https://openalex.org/W2607579284",
        "https://openalex.org/W6772381481",
        "https://openalex.org/W2885013662",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W3192478068",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3016697633",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2950626540",
        "https://openalex.org/W3199014515",
        "https://openalex.org/W6809322488",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W2123301721",
        "https://openalex.org/W6682631176",
        "https://openalex.org/W4220992023"
    ],
    "abstract": "The Transformer-based approach represents the state-of-the-art in image captioning. However, existing studies have shown Transformer has a problem that irrelevant tokens with overlapping neighbors incorrectly attend to each other with relatively large attention scores. We believe that this limitation is due to the incompleteness of the Self-Attention Network (SAN) and Feed-Forward Network (FFN). To solve this problem, we present the Full-Memory Transformer method for image captioning. The method improves the performance of both image encoding and language decoding. In the image encoding step, we propose the Full-LN symmetric structure, which enables stable training and better model generalization performance by symmetrically embedding Layer Normalization on both sides of the SAN and FFN. In the language decoding step, we propose the Memory Attention Network (MAN), which extends the traditional attention mechanism to determine the correlation between attention results and input sequences, guiding the model to focus on the words that need to be attended to. Our method is evaluated on the MS COCO dataset and achieves good performance, improving the result in terms of BLEU-4 from 38.4 to 39.3.",
    "full_text": null
}