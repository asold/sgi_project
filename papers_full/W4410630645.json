{
    "title": "Enhancing responses from large language models with role-playing prompts: a comparative study on answering frequently asked questions about total knee arthroplasty",
    "url": "https://openalex.org/W4410630645",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5100450914",
            "name": "Yi‐Chen Chen",
            "affiliations": [
                "Chang Gung Memorial Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5111268196",
            "name": "S. Lee",
            "affiliations": [
                "Chang Gung Memorial Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5117647009",
            "name": "Huan Sheu",
            "affiliations": [
                "Chang Gung Memorial Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5016220198",
            "name": "Su‐Wei Lin",
            "affiliations": [
                "National Dong Hwa University",
                "National Yang Ming Chiao Tung University"
            ]
        },
        {
            "id": "https://openalex.org/A5079649301",
            "name": "Chih‐Chien Hu",
            "affiliations": [
                "Chang Gung Memorial Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5057085053",
            "name": "Shan Fu",
            "affiliations": [
                "National Dong Hwa University",
                "National Yang Ming Chiao Tung University"
            ]
        },
        {
            "id": "https://openalex.org/A5060773531",
            "name": "Cheng‐Pang Yang",
            "affiliations": [
                "Chang Gung Memorial Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5036076198",
            "name": "Yu-Chih Lin",
            "affiliations": [
                "Chang Gung Memorial Hospital"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4384561707",
        "https://openalex.org/W2972749265",
        "https://openalex.org/W3087533146",
        "https://openalex.org/W2938070949",
        "https://openalex.org/W4392779412",
        "https://openalex.org/W4390617696",
        "https://openalex.org/W4391825111",
        "https://openalex.org/W4392783701",
        "https://openalex.org/W4390948973",
        "https://openalex.org/W4393433644",
        "https://openalex.org/W4380685958",
        "https://openalex.org/W4388007786",
        "https://openalex.org/W4388896754",
        "https://openalex.org/W4390919625",
        "https://openalex.org/W4386110374",
        "https://openalex.org/W4388823522",
        "https://openalex.org/W4392504747",
        "https://openalex.org/W4392505993",
        "https://openalex.org/W4392711451",
        "https://openalex.org/W4386322180",
        "https://openalex.org/W4396839232",
        "https://openalex.org/W1991085797",
        "https://openalex.org/W2070113169",
        "https://openalex.org/W1974415998",
        "https://openalex.org/W2397011557",
        "https://openalex.org/W2582406074",
        "https://openalex.org/W2327037637",
        "https://openalex.org/W4401042136",
        "https://openalex.org/W4319777976",
        "https://openalex.org/W4402567545",
        "https://openalex.org/W4404635379",
        "https://openalex.org/W4396667612"
    ],
    "abstract": null,
    "full_text": "RESEARCH Open Access\n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 \nInternational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the \nlicensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p  : / /  c r e a  t i  \nv e c  o m m  o n s .  o r  g / l  i c e  n s e s  / b  y - n c - n d / 4 . 0 /.\nChen et al. BMC Medical Informatics and Decision Making          (2025) 25:196 \nhttps://doi.org/10.1186/s12911-025-03024-5\nBMC Medical Informatics \nand Decision Making\n†Cheng-Pang Yangs and Yu-Chih Lin contributed equally to this \nmanuscript.\n*Correspondence:\nCheng-Pang Yang\nronnie80097@gmail.com\nYu-Chih Lin\nB101092127@tmu.edu.tw\nFull list of author information is available at the end of the article\nAbstract\nBackground The application of artificial intelligence (AI) in medical education and patient interaction is rapidly \ngrowing. Large language models (LLMs) such as GPT-3.5, GPT-4, Google Gemini, and Claude 3 Opus have shown \npotential in providing relevant medical information. This study aims to evaluate and compare the performance of \nthese LLMs in answering frequently asked questions (FAQs) about Total Knee Arthroplasty (TKA), with a specific focus \non the impact of role-playing prompts.\nMethods Four leading LLMs—GPT-3.5, GPT-4, Google Gemini, and Claude 3 Opus—were evaluated using ten \nstandardized patient inquiries related to TKA. Each model produced two distinct responses per question: one \ngenerated under zero-shot prompting (question-only), and one under role-playing prompting (instructed to \nsimulate an experienced orthopaedic surgeon). Four orthopaedic surgeons evaluated responses for accuracy and \ncomprehensiveness on a 5-point Likert scale, along with a binary measure for acceptability. Statistical analyses \n(Wilcoxon rank sum and Chi-squared tests; P < 0.05) were conducted to compare model performance.\nResults ChatGPT-4 with role-playing prompts achieved the highest scores for accuracy (3.73), comprehensiveness \n(4.05), and acceptability (77.5%), followed closely by ChatGPT-3.5 with role-playing prompts (3.70, 3.85, 72.5%, \nrespectively). Google Gemini and Claude 3 Opus demonstrated lower performance across all metrics. In between-\nmodel comparisons based on zero-shot prompting, ChatGPT-4 achieved significantly higher scores of both accuracy \nand comprehensiveness relative to Google Gemini (P = 0.031 and P = 0.009, respectively) and Claude 3 Opus (P = 0.019 \nand P = 0.002), and demonstrated higher acceptability than Claude 3 Opus (P = 0.006). Within-model comparisons \nEnhancing responses from large language \nmodels with role-playing prompts: \na comparative study on answering frequently \nasked questions about total knee arthroplasty\nYi-Chen Chen1, Sheng-Hsun Lee1, Huan Sheu1, Sheng-Hsuan Lin2,3,4,5, Chih-Chien Hu1, Shih-Chen Fu2,5, Cheng-\nPang Yang1*† and Yu-Chih Lin1*†\nPage 2 of 8\nChen et al. BMC Medical Informatics and Decision Making           (2025) 25:196 \nBackground\nRecent advancements in artificial intelligence (AI) and \nnatural language processing (NLP) have substantially \nenhanced the ability of large language models (LLMs) \nto deliver precise and contextually relevant responses \nin healthcare. State-of-the-art models like ChatGPT, \nGoogle Gemini, and Claude are leading innovations in \nclinical decision support, patient education, and medical \ntraining [1–4]. As the internet continues to be a primary \nsource of health information [ 5, 6], patients are increas -\ningly relying on chatbots for guidance on preoperative \npreparation and answers to frequently asked questions \n(FAQs). This trend is particularly evident with the grow -\ning demand for total knee arthroplasty (TKA) [ 7], where \npatients seek trustworthy information to navigate their \ntreatment options and optimized surgical outcomes.\nRecent studies have highlighted ChatGPT’s potential \nas a supplementary tool for TKA preoperative educa -\ntion [ 8–13]. As LLMs continue to advance, researchers \nare comparing the effectiveness of different chatbots, \nsuch as GPT-3.5 and GPT-4 [ 14–17], as well as vari -\nous models [ 18–22], in delivering accurate information. \nHowever, research specifically comparing the responses \nof different LLMs to TKA-related FAQs remains limited. \nFurthermore, concerns regarding the accuracy of infor -\nmation emphasize the need for cautious use of chatbots \nas standalone tools. The quality of chatbot responses is \nheavily influenced by prompt structure, which has led \nto advancement in “prompting engineering” [ 23]. Spe -\ncifically, role-playing prompts have emerged as a factor \nin improving chatbot accuracy. For example, prompt -\ning ChatGPT-3.5 to act as an herbal medicine expert \nincreased its accuracy on herb–drug interaction queries \nfrom 60–100%  [24]. However, studies applying role-play \nprompting in the medical field remain very scarce.\nTo our knowledge, no prior studies have systematically \ncompared the performance of multiple LLMs in address -\ning TKA inquiries using varied prompting methods. In \nresponse to this gap, our research employed four publicly \navailable LLMs—GPT-3.5, GPT-4, Google Gemini and \nClaude 3 Opus—with two primary objectives: (1) Evalu -\nate and compare the performance of these LLMs in terms \nof accuracy, comprehensiveness and acceptability when \nresponding to queries related to TKA; and (2) Assess \nwhether role-playing prompts, simulating a specialist’s \nperspective, serve as an effective prompting strategy for \nimproving response quality. We hypothesized that role-\nplaying prompts would substantially enhance the qual -\nity of AI-generated responses across multiple evaluative \ndimensions.\nMethods\nIn this study, the evaluation of four leading Large Lan -\nguage Models (LLMs) was conducted by testing their \nproficiency in answering questions related to TKA. A \nset of ten questions, crafted by one of the researchers \nand refined by a panel to reflect common inquiries from \npatients in our clinical practice, were posed to each LLM \n(Table 1) [ 25]. This study was approved by the Institu -\ntional Review Board, and the requirement for patient \nconsent was waived (Additional file 1). Ten questions \nwere presented to four LLMs using two different prompt-\ning methods: zero-shot and role-play, yielding a total of \neighty responses. Zero-shot prompting presented the \nquestion without any simulated instructions. In con -\ntrast, the role-play prompting involved instructing the AI \nto simulate an experienced orthopaedic surgeon, e.g., “I \nwant you to act as an experienced orthopaedic surgeon \nspecializing in total knee arthroplasty for over ten years. \nYou have published extensively on the subject, contrib -\nuted to knee implant designs, and are recognized for your \ninnovation, empathy, and patient care. ” Following this \nprompt, responses were generated based on this special -\nized persona.\nThe responses to each query were generated from \nMarch 20th to March 24th, 2024. Each model was pre -\nsented with the questions, both zero-shot and role-\nplaying prompts, only once, without any follow-up, \nrephrasing, or clarifications, to mirror the immediate \nresponse scenario often encountered in patient inqui -\nries. To avoid memory bias, each LLM session was reset \nafter every question, and unprompted and prompted \nresponses were generated in separate sessions.\nshowed role-playing significantly improved all metrics for ChatGPT-3.5 (P < 0.05) and acceptability for ChatGPT-4 \n(P = 0.033). No significant prompting effects were observed for Gemini or Claude.\nConclusions This study demonstrates that role-playing prompts significantly enhance the performance of LLMs, \nparticularly for ChatGPT-3.5 and ChatGPT-4, in answering FAQs related to TKA. ChatGPT-4, with role-playing prompts, \nshowed superior performance in terms of accuracy, comprehensiveness, and acceptability. Despite occasional \ninaccuracies, LLMs hold promise for improving patient education and clinical decision-making in orthopaedic \npractice.\nClinical trial number Not applicable.\nKeywords Large language models, Total knee arthroplasty, Artificial intelligence, Prompting\nPage 3 of 8\nChen et al. BMC Medical Informatics and Decision Making           (2025) 25:196 \nThe responses from each LLM were collected, ano -\nnymized, and formatted into a uniform plain text for \nevaluation. Four experienced faculty members from the \nDepartment of Orthopaedic Surgery at Chung Gung \nMemorial Hospital, all blinded to the origin of the \nresponses, were tasked with grading them. This blinding \nwas a critical measure to ensure an unbiased assessment \nof each model’s ability to handle specialized medical que-\nries effectively. The full list of question prompts and cor -\nresponding responses can be found in Additional file 2, \nand the surgeon scoring questionnaire for TKA FAQs is \nincluded as Additional file 3.\nThe responses to each question were compiled in a \nspreadsheet and sent to a panel of four orthopaedic sur -\ngeons for evaluation. The four orthopedic surgeons who \nevaluated the responses were independent and had no \naffiliations or financial interests with the companies pro -\nviding the AI programs. They assessed both the accuracy \nand comprehensiveness of each response using a 5-point \nLikert scale. This method has been employed in previ -\nous studies examining the accuracy and comprehensive -\nness of medical information from online resources [ 11, \n13, 18, 26–29]. Clear instructions defining each charac -\nteristic and score were provided, as shown in Table  2. \nTo brief, Accuracy scores reflected factual correctness, \nranging from significant inaccuracies (score 1) to error-\nfree responses (score 5); comprehensiveness ranged from \nincomplete answers to exhaustive, in-depth responses. \nThe final score for each response was calculated by tak -\ning the mean of the reviewers’ scores. In cases of notable \ndiscrepancies (score differences > 2 points), the disagree -\nment was resolved through consensus discussion, result -\ning in a final agreed-upon score.\nAcceptability, a key factor in healthcare interventions, \nwas assessed using the method proposed by Wright [ 11]. \nThis method employs a binary question for reviewers: \n“Would you be comfortable if this were the only informa-\ntion your patient received for their question?” Review -\ners evaluated whether the information provided was \nboth sufficient and safe for patients, aligning with the \ncore principle of patient well-being [ 30]. For analysis, a \nreviewer–level approach was used, in which each rating \nwas counted independently; the percentage of acceptabil-\nity was calculated as the total number of “yes” responses \ndivided by the total number of reviewer ratings ( n = 320). \nSpecifically, evaluators were asked to comment on the \nresponses, indicating any inaccuracies, incompleteness, \nor contradictions to their clinical practice. Comments \ncould also highlight positive aspects of the answer.\nData analyses\nContinuous data were presented as means and range \n(minimum, maximum). Categorical data were reported \nas proportions and percentages. The Wilcoxon rank sum \nTable 1 Frequently asked questions about total knee \narthroplasty\nQuestion\nQ1 What occurs during a total knee arthroplasty \noperation?\nQ2 When is it necessary for me to consider total knee \narthroplasty?\nQ3 Are there any alternatives to total knee arthroplasty?\nQ4 Can the choice of implant material or design enhance \nmy results after total knee arthroplasty?\nQ5 Is undergoing total knee arthroplasty considered to \ncarry high risks?\nQ6 What is the expected recovery duration following total \nknee arthroplasty?\nQ7 How many years can I expect the knee arthroplasty \nto last?\nQ8 What are the common reasons for a knee arthroplasty \nto fail?\nQ9 I’ve been diagnosed with a periprosthetic infection \nfollowing total knee arthroplasty. What steps should \nI take?\nQ10 Should I choose minimally invasive surgical techniques \nover conventional approaches when undergoing total \nknee arthroplasty?\nQ, question\nTable 2 Scoring criteria for accuracy and comprehensiveness\nScore Accuracy Comprehensiveness\nAgree \nwith\nDescription\n1 0–25% Responses \ncontaining \nsignificant inac-\ncuracies that \ncould poten-\ntially mislead \npatients and \ncause harm.\nIncomplete Responses are \nsignificantly lack-\ning in detail and \nonly address a \nminor part of the \nquestion.\n2 26–50% Only achieves \na basic level of \naccuracy.\nPartial Responses provide \nminimal and basic \ndetails, yet lack \ncritical elements.\n3 51–75% Responses \ncontain po-\ntential factual \ninaccuracies \nbut are unlikely \nto mislead \nor harm the \npatient.\nAdequate Responses offer a \nfair amount of de-\ntail and the main \npoints needed to \ngrasp the topic, \nyet they fall short \nin depth.\n4 76–99% Nearly all key \npoints are \naccurate.\nThorough Responses encom-\npass most of the \nessential aspects \nof the question.\n5 100% Error-free \nresponse.\nExhaustive Responses deliver \nexhaustive details \nand a comprehen-\nsive answer, offer-\ning deep insights \ninto the topic.\nPage 4 of 8\nChen et al. BMC Medical Informatics and Decision Making           (2025) 25:196 \ntest was used to compare continuous variables. For cate -\ngorical data, Pearson Chi-squared tests were applied. The \n95% confidence intervals (CIs) were calculated, with sta -\ntistical significance set at a P value < 0.05. To assess inter–\nrater reliability, intraclass correlation coefficients (ICCs) \nwere calculated using a two–way mixed–effects model \n[ICC (3, k), consistency] [ 31]. Statistical analyses were \nconducted using SPSS Statistics, Version 25.0 (IBM).\nResults\nThe analysis of four LLMs yielded a mean accuracy score \nof 3.38 (95% CI 2.67–4.09) across all responses, indicat -\ning alignment with 51–75% of the information presented. \nThe mean comprehensiveness score was 3.48 (95% \nCI 2.65–4.31), surpassing the ‘adequate level’ thresh -\nold (Table  2). Among all responses assessed by the four \nreviewers, 48.8% (156/320) were deemed acceptable.\nTable 3 demonstrates that ChatGPT-4 with role-play -\ning prompts achieved the highest mean scores for accu -\nracy (3.73) and comprehensiveness (4.05), as well as the \nhighest acceptability rate (77.5%) among the four LLMs \ntested. ChatGPT-3.5 with role-playing prompts fol -\nlowed closely, with mean scores of 3.70 for accuracy and \n3.85 for comprehensiveness, and an acceptability rate of \n72.5%. Google Gemini and Claude 3 Opus showed lower \nperformance across all metrics, with and without role-\nplaying prompts.\nTable  4 summarizes the statistical comparisons \nbetween models using responses generated under zero-\nshot prompting. ChatGPT-4 demonstrated significantly \nhigher accuracy and comprehensiveness scores com -\npared to both Google Gemini ( P = 0.031 and P = 0.009, \nrespectively) and Claude 3 Opus ( P = 0.019 and P = 0.002, \nrespectively). Additionally, ChatGPT-4 had a signifi -\ncantly higher acceptability rate compared to Claude 3 \nOpus (P = 0.006). No other between-model differences in \nacceptability reached statistical significance.\nWhen prompted to act as an experienced orthopae -\ndic surgeon specializing in TKA, role-playing signifi -\ncantly improved the performance of ChatGPT-3.5 across \nall three metrics: accuracy ( P = 0.02), comprehensive-\nness ( P = 0.026), and acceptability ( P = 0.012) (Table  3). \nRole-playing prompts also significantly enhanced the \nacceptability rate of ChatGPT-4 ( P = 0.033). However, \nno significant improvements were observed for Google \nGemini or Claude 3 Opus when using role-playing \nprompts.\nThe overall ICC for accuracy and comprehensiveness \nratings across all LLMs and prompting types was 0.632, \nindicating moderate agreement among reviewers [ 31]. \nThis level of consistency supports the reliability of the \nexpert evaluations used in this study.\nTable 3 Within-model comparisons of large language models performance in accuracy, comprehensiveness, and acceptability \nbetween zero-shot and role-playing prompts\nLLM Metric Zero–shot, mean (minimum, maximum) Role–playing, mean (minimum, maximum) P Value\nChatGPT-3.5 Accuracy 3.33 (2–5) 3.70 (2–5) 0.020a\nComprehensiveness 3.43 (2–5) 3.85 (2–5) 0.026\nAcceptability (%) 18/40 (45) 29/40 (72.5) 0.012\nChatGPT-4 Accuracy 3.45 (2–5) 3.73 (2–5) 0.058\nComprehensiveness 3.68 (2–5) 4.05 (2–5) 0.053\nAcceptability (%) 22/40 (55) 31/40 (77.5) 0.033\nGoogle Gemini Accuracy 3.13 (2–5) 3.25 (2–5) 0.425\nComprehensiveness 3.18 (2–5) 3.18 (2–5) 1.000\nAcceptability (%) 16/40 (40) 16/40 (40) 1.000\nClaude 3 Opus Accuracy 3.10 (2–5) 3.40 (2–5) 0.056\nComprehensiveness 3.13 (2–5) 3.35 (2–5) 0.117\nAcceptability (%) 10/40 (25) 14/40 (35) 0.329\nContinuous data are described by mean (minimum, maximum), and categorical data as number (percent)\nLLM, large language model\na Statistically significant values are italicized\nTable 4 Between-model comparisons of accuracy, \ncomprehensiveness, and acceptability for responses generated \nunder zero-shot prompting\nLLM Accuracy Comprehensiveness Acceptability\nChatGPT-3.5 vs. \nChatGPT-4\n0.430 0.201 0.371\nChatGPT-3.5 vs. \nGoogle Gemini\n0.199 0.173 0.651\nChatGPT-3.5 vs. \nClaude 3 Opus\n0.145 0.079 0.061\nChatGPT-4 vs. \nGoogle Gemini\n0.031a 0.009 0.179\nChatGPT-4 vs. \nClaude 3 Opus\n0.019 0.002 0.006\nGoogle Gemini \nvs. Claude 3 \nOpus\n0.862 0.756 0.152\nLLM, large language model\na Statistically significant values are italicized\nPage 5 of 8\nChen et al. BMC Medical Informatics and Decision Making           (2025) 25:196 \nDiscussion\nThe aim of this study was to evaluate and compare the \nperformance of four large language models — GPT-3.5, \nGPT-4, Google Gemini, and Claude 3 Opus — in answer-\ning frequently asked questions about TKA, with a spe -\ncific focus on the impact of role-playing prompts. The \nauthors hypothesized that role-playing prompts would \nenhance the accuracy, comprehensiveness, and accept -\nability of AI responses to TKA-related FAQs. Our major \nfindings support this hypothesis, showing that (1) Among \nthe four LLMs tested, ChatGPT-4 with role-playing \nprompts achieved the best performance in answering \nTKA queries, followed closely by ChatGPT-3.5 with role-\nplaying prompts  . (2) Role-playing prompts significantly \nimproved ChatGPT-3.5’s performance across all metrics \nand increased ChatGPT-4’s acceptability.\nIn the current study, a clear disconnect was observed \nbetween the mean accuracy and comprehensiveness \nscores and the overall acceptability rate across the four \nlarge language models. While the average accuracy and \ncomprehensiveness ratings were 3.38 and 3.48, respec -\ntively, only 48.8% (156/320) of responses were judged \nacceptable. This discrepancy likely reflects the more \nstringent threshold applied to acceptability, which \nrequired responses to be both factually accurate and suf -\nficiently comprehensive for use as standalone patient \ninformation. In contrast, Likert-scale ratings allow for \npartial credit, whereby responses with moderate scores \nmay still be deemed unacceptable if they omit essential \nclinical details or contain ambiguities.\nPrior research has demonstrated ChatGPT’s capabil -\nity to answer TKA queries with above-average quality \n[9–11, 13], reaching a performance level comparable to \narthroplasty-trained nurses [ 8]. Studies also highlighted \nits capability to generate clinically relevant responses \n[10, 13]. However, the present study found that despite a \nmean accuracy rating above three (indicating agreement \nwith more than 50% of the content), and adequate com -\nprehensiveness, ChatGPT–3.5’s overall performance did \nnot align with the standards set by similar studies [10, 11, \n13]. Specifically, only 45% of its responses were deemed \nacceptable, compared to 59.2% in a prior study [ 11]. \nUpon reviewing the scoring results, responses to Ques -\ntions 7 and 9 were all considered unacceptable due to the \nlack of probabilistic data and insufficient surgical details \nfor periprosthetic joint infection, respectively.\nBetween-model comparisons revealed that ChatGPT-4 \noutperformed GPT-3.5 across key metrics — accuracy, \ncomprehensiveness, and acceptability — though the dif -\nferences were not significant, particularly against GPT-\n3.5’s role-playing prompts (Tables 3 and 4). In the Polish \nMedical Final Examination, GPT-4 achieved significantly \nhigher mean accuracies compared to GPT-3.5, passing all \nversions of the exam [16]. Similarly, GPT-4 demonstrated \nsuperior performance in the American Academy of \nOphthalmology Basic and Clinical Science Course self-\nassessment test [ 15], the Japanese Medical Licensing \nExamination [14], and Radiology Diagnosis Please cases \n[17]. These findings highlight the rapid advancements in \nlanguage model capabilities and the potential of GPT-4 \nto surpass its predecessor, GPT-3.5, across various medi -\ncal domains and question formats.\nAdditionally, we found ChatGPT-4 demonstrated sig -\nnificantly greater accuracy and comprehensiveness com -\npared to Google Gemini and Claude 3 Opus (Table  4). \nRecent research has evaluated the comparative perfor -\nmance of various LLMs across multiple medical domains. \nIn the field of dentistry, ChatGPT-4 outperformed \nChatGPT-3.5, Bing Chat, and Google Bard in address -\ning clinically relevant questions [ 19]. Similarly, for myo -\npia care-related queries, ChatGPT-4 exhibited superior \naccuracy compared to GPT-3.5 and Google Bard, while \nmaintaining comparable comprehensiveness [ 18]. Fur -\nthermore, in surgical planning for ophthalmology, Chat -\nGPT models outperformed Google Gemini in terms of \nthe Global Quality Score [21].\nChatGPT-4’s performance has also been evaluated in \nstandardized medical testing, with the model achieving \na perfect score (100%) on the National Board of Medi -\ncal Examiners exam, surpassing GPT-3.5, Claude, and \nGoogle Bard [ 22]. A systematic analysis published in \nNature Communications further highlighted ChatG -\nPT-4’s superior performance over both GPT-3.5 and \ntraditional Google searches across various medical sce -\nnarios. However, the open-source LLM Llama 2 was \nfound to perform slightly less effectively compared to \nits counterparts [ 20]. These findings underscore ChatG -\nPT-4’s advanced capabilities and its potential to enhance \nclinical decision-making and medical education.\nPrompt engineering is crucial for optimizing the accu -\nracy and informativeness of AI-generated responses in \nmedical education [ 23, 32]. Our study found that role-\nplaying prompts enhance the performance of all LLMs \nin answering queries about TKA, significantly improving \nChatGPT-3.5’s performance across all metrics and boost-\ning ChatGPT-4’s acceptability (Table  3). These findings \nare supported by other research: A study from Thailand \ndemonstrated that when ChatGPT-3.5 was prompted to \nassume the role of an herbalist, the accuracy of responses \nincreased by 8.98% compared to zero-shot prompts. \nSpecifically, this role-playing approach raised accuracy \nfrom 60 to 100% in herb-drug interactions [ 24]. More -\nover, another study confirmed that role-play prompt -\ning surpassed standard zero-shot approaches across \nvarious reasoning benchmarks, effectively serving as an \nimplicit trigger for the chain-of-thought process [ 33]. \nThese results affirm the efficacy of role-play prompting \nPage 6 of 8\nChen et al. BMC Medical Informatics and Decision Making           (2025) 25:196 \nin enhancing LLM reasoning capabilities, emphasizing its \nvalue in medical education and other fields.\nDespite the potential of AI-driven responses, LLMs \nsuch as ChatGPT have raised concerns regarding reli -\nability and consistency when applied to TKA, primar -\nily due to limitations in training datasets. Studies have \nrevealed issues in reliability, including the fabrication of \ncitations and references to non-existent studies or jour -\nnals [ 9, 12]. These inaccuracies, known as “hallucina -\ntions, ” pose risks of spreading misinformation, potentially \nleading to an AI-driven “infodemic” that could threaten \npublic health [ 34]. Moreover, ChatGPT’s response con -\nsistency to repeated queries on TKA has been debated. \nSome studies highlighted inconsistencies, noting inferior \nconsistency scores [ 10], while others found no signifi -\ncant differences in accuracy or comprehensiveness upon \nrepeated questioning [ 11]. In addition to these concerns, \nrecent findings also suggested that role-play prompting \npresents a dual-edged phenomenon: while it improves \nthe contextual accuracy and relevance of LLM responses, \nit may also amplify biases and generate harmful outputs. \nThis trade-off is particularly concerning in high-stakes \nfields such as healthcare, where misaligned content could \nundermine trust and reinforce inequities [35].\nWhile many of the LLM’s responses demonstrated \nscientific accuracy and relevance, these qualities were \nnot consistently sufficient to meet clinical acceptabil -\nity standard. Table  5 outlines several critical issues that \nlimited their overall utility. For instance, Google Gemini \noverstated the effectiveness of topical medications and \nthe role of arthroscopic surgery in osteoarthritis man -\nagement, contradicting established guidelines. Claude \nOpus 3 also provided inaccurate information regarding \nimplants and preoperative antibiotics, which are par -\nticularly used in patients who are septic or too unstable \nto wait for culture data. As a result, both models exhib -\nited low acceptability rate (≤ 40%), with most inaccura -\ncies observed in responses generated under zero-shot \nprompts rather than role-playing prompts. Additionally, \nresponse on implant design often lacked completeness \nand most responses omitted probabilistic data. A recur -\nring issue with role-playing prompts was the use of the \nfirst-person perspective, which may be perceived as not \nobjective and can introduce personal bias. We believe \nadditional biases include the reinforcement of existing \nprejudices in the training data and the tendency to pro -\nvide overly confident answers without sufficient evidence.\nTable 5 Evaluators’ comments on large language model responses (exact copies)\nLLM Comment\nChatGPT-3.5 Zero-shot\n- Material choice impact survival, as well as the occurrence of TKA complications, such as PJI, loosening, instability. (Q7)\nRole-playing\n- Very friendly and easy to understand. (Q2)\n- Lack of Vitamin-E poly, rotating platform, and fixation method information. (Q4)\nChatGPT-4 Role-playing\n- The question is about what happens “during” total knee arthroplasty. The answer gives too much information about pre-OP and \npost-OP , which seems redundant for this specific question. (Q1)\n- Lack of CR/PS design, medial pivot design. (Q4)\n- “innovations” are not accurate, for example, MIS approach does not guarantee to lessen tissue damage and speed up recovery. (Q5)\n- No first-person perspective. (Q7)\nGoogle Gemini Zero-shot\n- Too easy and lack of professional knowledge. (Q2)\n- Many misleading information (Q3)\n - Topical medication is not evidence based. Should specify that it may be effective only in superficial tendinitis, not osteoarthritis.\n -  Arthroscopic surgery is not effective in OA knee, but may be effective in symptoms associated with synovitis, loose body, menis-\ncus symptoms.\n - Lack in information about UKA.\n- Should provide approximate incidence rate of each complication. Lack of instability. (Q5)\nRole-playing\n- I don’t think it’s suitable to use a first-person perspective in an AI-generated response. (Q2)\nClaude 3 Opus Zero-shot\n- Difficult for patients to understand. (Q2)\n- Should be no ceramic on ceramic design, and patient-specific implant. (Q4)\n- Better give probabilities. (Q8)\nRole-playing\n- Short but well-constructed. (Q2)\n- Point by point response is better understood. (Q5)\n- No first-person perspective. (Q7)\n- Wrong about preoperative antibiotics. (Q9)\nLLM, large language model; TKA, total knee arthroplasty; PJI, periprosthetic joint infection; Q, question; OP, operation; CR, cruciate-retaining; PS, posterior-stabilized; \nOA, osteoarthritis; UKA, unicompartmental knee arthroplasty\nPage 7 of 8\nChen et al. BMC Medical Informatics and Decision Making           (2025) 25:196 \nThis study has several noteworthy limitations. First, \nthe questions posed may lack comprehensiveness and \nwere not systematically categorized, which could limit \nthe evaluation’s scope and obscure potential pitfalls of the \nLLMs. Second, the LLMs utilized in this study did not \nhave access to up-to-date external information sources, \npotentially compromising the recency and accuracy of \ntheir responses. Third, the specific wording of questions \nmight have significantly influenced the results, highlight -\ning the sensitivity of LLM responses to prompt varia -\ntions. Moreover, the evaluation’s subjective nature, with \nonly four orthopaedic surgeons assessing the responses \nbased on subjective criteria, could introduce bias and \naffect the reliability of the findings. Additionally, while \nrole-playing prompts improved expert-rated quality, it \nremains uncertain whether the increased technical detail \nenhances patient comprehension. This study focused on \nclinical accuracy and relevance as evaluated by ortho -\npaedic specialists, and did not include formal readability \nanalysis. While tools such as the Flesch-Kincaid Grade \nLevel may provide valuable insight, assessing patient \ncomprehension represents a distinct dimension that war-\nrants dedicated investigation. Subsequent work should \nexamine whether increased technical detail enhances or \nhinders patient understanding. Lastly, the cost of some \nadvanced LLMs may limit their accessibility and practical \napplication in resource-constrained settings, thus affect -\ning the generalizability of the results.\nFuture research must focus on developing more com -\nprehensive evaluation methods and addressing the \ninherent weaknesses of LLMs. Engaging medical pro -\nfessionals with up-to-date resources and expanding \nevaluations to include a diverse group of experts will be \ncrucial. Through this study, we believe utilizing advanced \nlarge language models like ChatGPT-4 with role-playing \nprompts minimizes the risk of misinformation com -\npared to unverified online sources. These models can \nstreamline pre- and post-operative patient education, \nreducing consultation times and ensuring patients are \nwell-informed. By handling routine patient inquiries, \nLLMs alleviate cognitive load, allowing surgeons to focus \non critical clinical activities.\nConclusions\nRole-playing prompts significantly enhanced response \nquality from ChatGPT-4 and GPT-3.5, yielding more \ncontextually rich and detailed answers. ChatGPT-4, par -\nticularly with role-playing, demonstrated superior accu -\nracy, comprehensiveness, and acceptability compared \nto other models. However, occasional inaccuracies and \nincomplete information persist across models, highlight -\ning the need for further refinement to ensure consis -\ntent reliability. These findings underscore the potential \nof LLMs, especially with role-specific prompts, to \nstreamline patient education, reduce consultation times, \nand ensure patients are well-informed, thereby easing \ncognitive demands on surgeons and allowing them to \nconcentrate on essential clinical tasks.\nAbbreviations\nAI  Artificial Intelligence\nCIs  Confidence Intervals\nCR  Cruciate-Retaining\nFAQ  Frequently Asked Questions\nIRB  Institutional Review Board\nLLM  Large Language Model\nMIS  Minimally Invasive Surgery\nNLP  Natural Language Processing\nOA  Osteoarthritis\nOP  Operation\nPJI  Periprosthetic Joint Infection\nPS  Posterior-Stabilized\nQ  Question\nTKA  Total Knee Arthroplasty\nUKA  Unicompartmental Knee Arthroplasty\nSupplementary Information\nThe online version contains supplementary material available at  h t t p  s : /  / d o i  . o  r \ng /  1 0 .  1 1 8 6  / s  1 2 9 1 1 - 0 2 5 - 0 3 0 2 4 - 5.\nSupplementary Material 1: Additional file 1.pdf: Institutional Review Board \ncertification\nSupplementary Material 2: Additional file 2.docx: Detailed Responses from \nFour Large Language Models to Total Knee Arthroplasty Queries\nSupplementary Material 3: Additional file 3.docx: Surgeon Scoring Ques-\ntionnaire for Total Knee Arthroplasty Frequently Asked Questions\nAcknowledgements\nNot applicable.\nAuthor contributions\nAll authors contributed to the understanding and design of the study. \nC.P .Y. and Y.C.L. formulated the idea and designed the study. Y.C.C. collected \nresponses from LLMs. C.P .Y., Y.C.L., H.S., and S.H.L. evaluated and scored the \nresponses. Y.C.C., S.H.Lin, and S.C.F. conducted the analysis. The first draft of \nthe article was written by Y.C.C. C.C.H. analyzed the manuscript critically. All \nauthors have read and approved the submitted version.\nFunding\nNot applicable.\nData availability\nResponses from large language models and the questionnaire are provided \nin Additional files 2 and 3. Scoring data are available from the corresponding \nauthor upon reasonable request.\nDeclarations\nEthics approval and consent to participate\nThis study was approved by the Institutional Review Board (IRB) of Chang \nGung Medical Foundation (IRB No.: 202500062B0, Additional file 1). The study \nadhered to the ethical principles outlined in the Declaration of Helsinki. As this \nstudy evaluated large language model responses without involving human \nparticipants, patient data, or identifiable information, the Institutional Review \nBoard of Chang Gung Medical Foundation (IRB No.: 202500062B0) waived the \nrequirement for patient consent. However, the IRB approved and obtained \ninformed consent for the orthopedic surgeons’ evaluation of the responses.\nPage 8 of 8\nChen et al. BMC Medical Informatics and Decision Making           (2025) 25:196 \nConsent for publication\nNot applicable. This study did not involve human participants or data \nrequiring consent for publication.\nDeclaration of Generative AI and AI-assisted technologies in the writing \nprocess\nDuring the preparation of this work, the authors used ChatGPT in order \nto improve readability of the manuscript. After using this tool, the authors \nreviewed and edited the content as needed and take full responsibility for the \ncontent of the publication.\nCompeting interests\nThe authors declare no competing interests.\nAuthor details\n1Department of Orthopaedic Surgery, Chang Gung Memorial Hospital, \nTaoyuan 333, Taiwan\n2Institute of Statistics, National Yang Ming Chiao Tung University, Hsinchu, \nTaiwan\n3Institute of Data Science and Engineering, National Yang Ming Chiao \nTung University, Hsinchu, Taiwan\n4Department of Applied Mathematics, National Dong Hwa University, \nHualien, Taiwan\n5Department of Biochemical and Molecular Medical Sciences, National \nDong Hwa University, Hualien, Taiwan\nReceived: 8 February 2025 / Accepted: 9 May 2025\nReferences\n1. OpenAI. Introducing ChatGPT.  h t t p  s : /  / o p e  n a  i . c  o m /  b l o g  / c  h a t g p t. Accessed \nApril 06, 2024.\n2. Pichai S, Hassabis D. Introducing Gemini: our largest and most capable AI \nmodel.  h t t p  s : /  / b l o  g .  g o o  g l e  / t e c  h n  o l o  g y /  a i / g  o o  g l e  - g e  m i n i  - a  i / # s u n d a r - n o t e. \nAccessed April 6, 2024.\n3. Anthropic. Introducing Claude.  h t t p  s : /  / w w w  . a  n t h  r o p  i c . c  o m  / n e  w s /  i n t r  o d  u c i n \ng - c l a u d e. Accessed April 06, 2024.\n4. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. \nLarge Language models in medicine. Nat Med. 2023;29(8):1930–40.\n5. Finney Rutten LJ, Blake KD, Greenberg-Worisek AJ, Allen SV, Moser RP , \nHesse BW. Online health information seeking among US adults: measur-\ning progress toward a healthy people 2020 objective. Public Health Rep. \n2019;134(6):617–25.\n6. Calixte R, Rivera A, Oridota O, Beauchamp W, Camacho-Rivera M. Social and \ndemographic patterns of health-related internet use among adults in the \nunited States: a secondary data analysis of the health information National \ntrends survey. Int J Environ Res Public Health. 2020;17(18).\n7. Singh JA, Yu S, Chen L, Cleveland JD. Rates of total joint replacement in the \nunited States: future projections to 2020–2040 using the National inpatient \nsample. J Rheumatol. 2019;46(9):1134–40.\n8. Bains SS, Dubin JA, Hameed D, Sax OC, Douglas S, Mont M, et al. Use and \napplication of large Language models for patient questions following total \nknee arthroplasty. J Arthroplasty. 2024.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 1 6  / j  . a r t h . 2 0 2 4 . 0 3 . \n0 1 7.\n9. Kienzle A, Niemann M, Meller S, Gwinner C. ChatGPT May offer an adequate \nsubstitute for informed consent to patients prior to total knee arthroplasty-\nyet caution is needed. J Pers Med. 2024;14(1).\n10. Magruder ML, Rodriguez AN, Wong JCJ, Erez O, Piuzzi NS, Scuderi GR, et al. \nAssessing ability for ChatGPT to answer total knee arthroplasty-related ques-\ntions. J Arthroplasty. 2024.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 1 6  / j  . a r t h . 2 0 2 4 . 0 2 . 0 2 3.\n11. Wright BM, Bodnar MS, Moore AD, Maseda MC, Kucharik MP , Diaz CC, et al. Is \nChatGPT a trusted source of information for total hip and knee arthroplasty \npatients? Bone Jt Open. 2024;5(2):139–46.\n12. Yang J, Ardavanis KS, Slack KE, Fernando ND, Della Valle CJ, Hernandez NM. \nChat generative pretrained transformer (ChatGPT) and Bard: artificial intel-\nligence does not yet provide clinically supported answers for hip and knee \nosteoarthritis. J Arthroplasty. 2024;39(5):1184–90.\n13. Zhang S, Liau ZQG, Tan KLM, Chua WL. Evaluating the accuracy and relevance \nof ChatGPT responses to frequently asked questions regarding total knee \nreplacement. Knee Surg Relat Res. 2024;36(1):15.\n14. Takagi S, Watari T, Erabi A, Sakaguchi K. Performance of GPT-3.5 and GPT-4 on \nthe Japanese medical licensing examination: comparison study. JMIR Med \nEduc. 2023;9e48002.\n15. Taloni A, Borselli M, Scarsi V, Rossi C, Coco G, Scorcia V, et al. Comparative \nperformance of humans versus GPT-4.0 and GPT-3.5 in the self-assessment \nprogram of American academy of ophthalmology. Sci Rep. 2023;13(1):18562.\n16. Rosol M, Gasior JS, Laba J, Korzeniewski K, Mlynczak M. Evaluation of the \nperformance of GPT-3.5 and GPT-4 on the Polish medical final examination. \nSci Rep. 2023;13(1):20512.\n17. Li D, Gupta K, Bhaduri M, Sathiadoss P , Bhatnagar S, Chong J, Comparing. \nGPT-3.5 and GPT-4 accuracy and drift in radiology diagnosis please cases. \nRadiology. 2024;310(1):e232411.\n18. Lim ZW, Pushpanathan K, Yew SME, Lai Y, Sun CH, Lam JSH et al. Benchmark-\ning large language models’ performances for myopia care: a comparative \nanalysis of ChatGPT-3.5, ChatGPT-4.0, and Google Bard. EBioMedicine. \n2023;95104770.\n19. Giannakopoulos K, Kavadella A, Aaqel Salim A, Stamatopoulos V, Kaklamanos \nEG. Evaluation of the performance of generative AI large Language models \nchatGPT, Google bard, and Microsoft Bing chat in supporting evidence-\nbased dentistry: comparative mixed methods study. J Med Internet Res. \n2023;25e51580.\n20. Sandmann S, Riepenhausen S, Plagwitz L, Varghese J. Systematic analysis of \nChatGPT, Google search and Llama 2 for clinical decision support tasks. Nat \nCommun. 2024;15(1):2050.\n21. Carla MM, Gambini G, Baldascino A, Giannuzzi F, Boselli F, Crincoli E, et al. \nExploring AI-chatbots’ capability to suggest surgical planning in ophthalmol-\nogy: ChatGPT versus Google gemini analysis of retinal detachment cases. Br J \nOphthalmol. 2024.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 3 6  / b  j o - 2 0 2 3 - 3 2 5 1 4 3.\n22. Abbas A, Rehman MS, Rehman SS. Comparing the performance of popular \nlarge Language models on the National board of medical examiners sample \nquestions. Cureus. 2024;16(3):e55991.\n23. Heston T, Khun C. Prompt engineering in medical education. Int Med Educ. \n2023;2(3):198–205.\n24. Poonprasartporn A, Uttaranakorn P , Chuaybamroong R. Comparing the \naccuracy and referencing of ChatGPT’s responses to herbal medicine queries: \nA zero-shot versus roleplay prompting approach. Thai J Pharm Sci. 2023;47(4).\n25. Chang Gung Memorial Hospital. Frequently asked questions about joint \nreplacement.  h t t p  s : /  / w w w  1 .  c g m  h . o  r g . t  w /  i n t  r / i  n t r 2  / c  3 2 7 0 / f a q _ j o i n t . h t m l. \nAccessed 20 March 2024.\n26. Dy CJ, Taylor SA, Patel RM, Kitay A, Roberts TR, Daluiski A. The effect of search \nterm on the quality and accuracy of online information regarding distal \nradius fractures. J Hand Surg Am. 2012;37(9):1881–7.\n27. Garcia GH, Taylor SA, Dy CJ, Christ A, Patel RM, Dines JS. Online resources \nfor shoulder instability: what are patients reading? J Bone Joint Surg Am. \n2014;96(20):e177.\n28. Mathur S, Shanti N, Brkaric M, Sood V, Kubeck J, Paulino C, et al. Surfing \nfor scoliosis: the quality of information available on the internet. Spine. \n2005;30(23):2695–700.\n29. Wang D, Jayakar RG, Leong NL, Leathers MP , Williams RJ, Jones KJ. Evaluation \nof the quality, accuracy, and readability of online patient resources for the \nmanagement of articular cartilage defects. Cartilage. 2017;8(2):112–8.\n30. Sekhon M, Cartwright M, Francis JJ. Acceptability of healthcare interventions: \nan overview of reviews and development of a theoretical framework. BMC \nHealth Serv Res. 2017;17(1):88.\n31. Koo TK, Li MY. A guideline of selecting and reporting intraclass correlation \ncoefficients for reliability research. J Chiropr Med. 2016;15(2):155–63.\n32. OpenAI. Prompt engineering.  h t t p s :   /  / p l a t f  o r   m . o p  e n a   i .  c  o  m / d o   c s /  g u  i  d  e s / p  r o  \nm  p t  - e n g i n e e r i n g. Accessed 12 May 2024.\n33. Kong A, Zhao S, Chen H, Li Q, Qin Y, Sun R et al. Better zero-shot reasoning \nwith role-play prompting. ArXiv, abs/230807702. 2023.\n34. De Angelis L, Baglivo F, Arzilli G, Privitera GP , Ferragina P , Tozzi AE et al. Chat-\nGPT and the rise of large Language models: the new AI-driven infodemic \nthreat in public health. Front Public Health. 2023;111166120.\n35. Zhao J, Qian Z, Cao J, Wang Y, Ding Y. Role-play paradox in large Language \nmodels: reasoning performance gains and ethical dilemmas. ArXiv Preprint \narXiv:240913979. 2024.\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations."
}