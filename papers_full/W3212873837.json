{
  "title": "Soft Sensing Transformer: Hundreds of Sensors are Worth a Single Word",
  "url": "https://openalex.org/W3212873837",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2096826587",
      "name": "Chao Zhang",
      "affiliations": [
        "University of Chicago",
        "Seagate (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2766841610",
      "name": "Jaswanth Yella",
      "affiliations": [
        "Seagate (United States)",
        "University of Cincinnati"
      ]
    },
    {
      "id": "https://openalex.org/A2106566374",
      "name": "Yu Huang",
      "affiliations": [
        "Seagate (United States)",
        "Florida Atlantic University"
      ]
    },
    {
      "id": "https://openalex.org/A2142274282",
      "name": "Xiaoye Qian",
      "affiliations": [
        "Case Western Reserve University",
        "Seagate (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2607476665",
      "name": "Sergei Petrov",
      "affiliations": [
        "Stanford University",
        "Seagate (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2642952989",
      "name": "Andrey Rzhetsky",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A3192662196",
      "name": "Sthitie Bom",
      "affiliations": [
        "Seagate (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2096826587",
      "name": "Chao Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2766841610",
      "name": "Jaswanth Yella",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106566374",
      "name": "Yu Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2142274282",
      "name": "Xiaoye Qian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2607476665",
      "name": "Sergei Petrov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2642952989",
      "name": "Andrey Rzhetsky",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3192662196",
      "name": "Sthitie Bom",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6694517276",
    "https://openalex.org/W2158698691",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W6799783978",
    "https://openalex.org/W6781808689",
    "https://openalex.org/W3003166104",
    "https://openalex.org/W6796933628",
    "https://openalex.org/W1964940105",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6796707148",
    "https://openalex.org/W6788071488",
    "https://openalex.org/W2148143831",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W6738279954",
    "https://openalex.org/W6638304892",
    "https://openalex.org/W2971407654",
    "https://openalex.org/W6637242042",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W3017512296",
    "https://openalex.org/W2986175116",
    "https://openalex.org/W2053757129",
    "https://openalex.org/W3016593651",
    "https://openalex.org/W3123899295",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6797602710",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4205250731",
    "https://openalex.org/W4226226796",
    "https://openalex.org/W4226110010",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3171649327",
    "https://openalex.org/W623776627",
    "https://openalex.org/W2135346934",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3192326989",
    "https://openalex.org/W4394647257",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W1813659000",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4235765578",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3171438403",
    "https://openalex.org/W4237650038",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W3048780605"
  ],
  "abstract": "With the rapid development of AI technology in recent years, there have been\\nmany studies with deep learning models in soft sensing area. However, the\\nmodels have become more complex, yet, the data sets remain limited: researchers\\nare fitting million-parameter models with hundreds of data samples, which is\\ninsufficient to exercise the effectiveness of their models and thus often fail\\nto perform when implemented in industrial applications. To solve this\\nlong-lasting problem, we are providing large scale, high dimensional time\\nseries manufacturing sensor data from Seagate Technology to the public. We\\ndemonstrate the challenges and effectiveness of modeling industrial big data by\\na Soft Sensing Transformer model on these data sets. Transformer is used\\nbecause, it has outperformed state-of-the-art techniques in Natural Language\\nProcessing, and since then has also performed well in the direct application to\\ncomputer vision without introduction of image-specific inductive biases. We\\nobserve the similarity of a sentence structure to the sensor readings and\\nprocess the multi-variable sensor readings in a time series in a similar manner\\nof sentences in natural language. The high-dimensional time-series data is\\nformatted into the same shape of embedded sentences and fed into the\\ntransformer model. The results show that transformer model outperforms the\\nbenchmark models in soft sensing field based on auto-encoder and long\\nshort-term memory (LSTM) models. To the best of our knowledge, we are the first\\nteam in academia or industry to benchmark the performance of original\\ntransformer model with large-scale numerical soft sensing data.\\n",
  "full_text": "Soft Sensing Transformer:\nHundreds of Sensors are Worth a Single Word\nChao Zhang\nSeagate Technology, MN, US\nUniversity of Chicago, IL, US\nchao.1.zhang@seagate.com\nJaswanth Yella\nSeagate Technology, MN, US\nUniversity of Cincinnati, OH, US\njaswanth.k.yella@seagate.com\nYu Huang\nSeagate Technology, MN, US\nFlorida Atlantic University, FL, US\nyu.1.huang@seagate.com\nXiaoye Qian\nSeagate Technology, MN, US\nCase Western Reserve University, OH, US\nxiaoye.qian@seagate.com\nSergei Petrov\nSeagate Technology, MN, US\nStanford University, CA, US\nsergei.petrov@seagate.com\nAndrey Rzhetsky\nUniversity of Chicago\nIL, US\narzhetsky@medicine.bsd.uchicago.edu\nSthitie Bom\nSeagate Technology\nMN, US\nsthitie.e.bom@seagate.com\nAbstract—With the rapid development of AI technology in\nrecent years, there have been many studies with deep learning\nmodels in soft sensing area. However, the models have become\nmore complex, yet, the data sets remain limited: researchers\nare ﬁtting million-parameter models with hundreds of data\nsamples, which is insufﬁcient to exercise the effectiveness of\ntheir models and thus often fail to perform when implemented\nin industrial applications. To solve this long-lasting problem,\nwe are providing large scale, high dimensional time series\nmanufacturing sensor data from Seagate Technology to the\npublic. We demonstrate the challenges and effectiveness of\nmodeling industrial big data by a Soft Sensing Transformer\nmodel on these data sets. Transformer is used because, it has\noutperformed state-of-the-art techniques in Natural Language\nProcessing, and since then has also performed well in the\ndirect application to computer vision without introduction of\nimage-speciﬁc inductive biases. We observe the similarity of\na sentence structure to the sensor readings and process the\nmulti-variable sensor readings in a time series in a similar\nmanner of sentences in natural language. The high-dimensional\ntime-series data is formatted into the same shape of embedded\nsentences and fed into the transformer model. The results show\nthat transformer model outperforms the benchmark models\nin soft sensing ﬁeld based on auto-encoder and long short-\nterm memory (LSTM) models. To the best of our knowledge,\nwe are the ﬁrst team in academia or industry to benchmark\nthe performance of original transformer model with large-\nscale numerical soft sensing data. Additionally, In contrast\nto the natural language processing or computer vision tasks\nwhere human-level performances are regarded as golden stan-\ndards, our large scale soft sensing study is an example that\ntransformer goes beyond human, because the high dimensional\nnumerical data is not interpretable for human.\n1. Introduction\nIn the last decades, the development of smart sensors\nhas attracted a lot of attention from government, academia\nand industry. The European Union’s 20-20-20 goals (20%\nincrease in energy efﬁciency, 20% reduction of CO2 emis-\nsions, and 20% renewable by 2020) rely on smart me-\ntering as one of their key enablers. Smart meters usually\ninvolve real-time or near real-time sensors, notiﬁcation and\nmonitoring. In 2013, Germany proposed the concept of\nIndustry 4.0, the main aim of which is to develop smart\nfactories for producing smart products. The US government\nin September 2020 announced that the US is providing\nmore than $1 billion towards establishing research and hubs\nfor Industry 4.0 technologies. Singapore’s current ﬁve-year\nSU$13.8 billion R&D is injecting more funds into expand-\ning ﬁelds such as advanced manufacturing. China’s China\nManufacturing 2025 goal is also to make the manufacturing\nprocess more intelligent. These initiatives require that we\nhave better sensing technologies to understand and drive our\nprocesses. Sensors have the potential to contain information\nabout process variables which can be exploited by data-\ndriven techniques for smarter monitoring and control of\nmanufacturing processes. Soft sensing is the general term\nused for the approaches and the algorithms that are used\nto estimate or predict certain physical quantities or product\nquality in the industrial processes based on the available\nsensing modalities, measurements, and knowledge.\nAs the industrial process have become more complicated\nand the size of available data has increased dramatically,\nthere has been growing body of research on deep learning\nThis paper has been accepted by 2021 IEEE International Conference on Big Data\narXiv:2111.05973v1  [cs.LG]  10 Nov 2021\nmethods with applications in the soft sensing ﬁeld. A recent\nsurvey on deep learning methods for soft sensor [1] has\nillustrated the signiﬁcance of the deep learning applications\nand reviewed the most recent studies in this ﬁeld. The\ndeep learning models are mostly based on autoencoder [2],\nrestricted Boltzmann machine [3], convolutional neural net-\nwork [4], and recurrent neural network [5]. The applications\nvaries from traditional factories [6] to wearable IoT devices\n[7], [8]\nThere has been a variety of novel deep learning models\nsuch as variational autoencoder models which attempts to\nenhance the representation ability or augment the data [9],\n[10], semi-supervised ensemble learning model that quan-\ntiﬁes the contribution of different hidden layers in stacked\nautoencoder [11], and gated convolutional transformer neu-\nral network that combines several state-of-art algorithms\ntogether to deals with a time-series data set [12]. As the\ndeep learning models become more and more complex, their\ncapabilities to handle complex processes and large data sets\nalso increase. However, in these studies researchers are still\nusing very small data sets such as wastewater treatment\nplant and Debutanizer column [13], [14] containing low\ndimensional data with only hundreds to thousands of data\nsamples. These small data sets are not sufﬁcient to illustrate\nthe effectiveness of these advanced deep learning models\nwith millions of parameters. To solve this issue, we collected\ngigabytes of numerical sensor data from Seagate’s wafer\nmanufacturing factories in USA and Ireland. These data sets\ncontain high-dimensional time-series sensor data that is col-\nlected directly from the Seagate wafer factories with only the\nnecessary anonymization, and they are big, complex, noisy\nand impossible to interpret in their raw form by humans..\nIn this article, We evaluate a soft sensing transformer model\nagainst the most commonly methods applied to soft sensing\nproblems including models based on autoencoder and LSTM\n[15]. The key components of the original transformer model\nis maintained and the other parts of the architecture are\nmodiﬁed to ﬁt into our data sets and tasks.\nTransformer, since it’s proposal in 2017 [16], together\nwith it’s derivatives such as BERT [17], have been the\nmost active research topic in the natural language processing\n(NLP) ﬁeld as well as the top performer in many NLP tasks\n[18]. Due to its extraordinary representative capability, trans-\nformer model has also shown equally good performance in\nthe computer vision area [19]. First proposed in 2020, vision\ntransformer [20] and its variants have achieved the state-of-\nart performances on many computer vision benchmarks such\nas image classiﬁcation, semantic segmentation and object\ndetection [21], [22].\nFrom texts in NLP, which can be regarded as categorical\ndata, to images (two dimensional integer values) in computer\nvision, a natural further extension would be soft sensing\ndata which is time series with continuous ﬂoating numbers.\nWhile the Bayes error rate [23] in NLP and computer vision\ntasks are usually deﬁned as human-level performance, our\nsoft sensing task is impossible for a human to classify based\non the hundreds of sensor values. We show in this paper that\nTransformer architecture not only works great for natural\nFigure 1. Architecture of soft sensing transformer model\nlanguage and images, but also for numerical data, and it is\nable to represent the data that is not interpretable by human.\nThe rest of this paper is organized as the following: We\ndiscuss the soft sensing transformer model in section. 2,\nseveral industrial soft sensing data sets in section. 3, the\nresults of the soft sensing transformer on these data sets in\nsection. 4, and discussions and conclusions in section. 5.\n2. Methodology\nWhile implementing the soft sensing model, we follow\nthe original transformer architecture as closely as possible.\nThe input module of the model is modiﬁed to ﬁt the time-\nseries sensor data, and the output module is modiﬁed for\nmulti-task classiﬁcation problems. This is the ﬁrst study for\nbenchmark results on these large scale sensor data sets with\ndeep learning methods, also the ﬁrst study for transformer\nmodel applied on large scale numerical sensor data.\n2.1. Soft Sensing Transformer (SST)\nWe illustrate the structure of the soft sensing transformer\nmodel in Fig. 1. Given that the data format of time-series\nsensor data is different from texts, we used a dense layer\nfor the embedding at the starting point, which reduces\nthe dimension of the input high dimensional sensor data.\nAfter this layer, the data format is the same as embedded\nsentences so that it can be feed into the transformer encoder\nwithout any modiﬁcations. Right before the encoder block,\na positional encoding using sine and cosine functions of\ndifferent frequencies is added as Equation. 1 and Equation.\n2 to cover the information of relative positions of different\ntime steps. PE stands for positional encoding, pos is the\nposition of a time step, and dmodel is the dimension of\nembedded vectors.\nPE(pos,2i) = sin(pos/100002i/dmodel) (1)\nPE(pos,2i+1) = cos(pos/100002i/dmodel) (2)\nSince the SST model requires a ﬁxed size of input data,\nwe added padding to samples with too few time steps so\nthat each sample has the same time length. The time length\nis chosen as 99 percentiles of the sequence lengths in the\nraw data to cover most of the data and exclude outliers. The\npadding masks are also applied accordingly. In the encoder,\nmulti-head scaled dot product attention, feed forward and\nresidual connections are set up in the the same way as in\nthe original transformer paper [16]. The multi-head attention\nis described as in Equation. 3, the query, key and value are\nprojected to hheads with the weight matrices Wq\ni ,Wk\ni ,Wv\ni .\nEach head has a dimension of dmodel/h, and a scaled dot-\nproduct attention is calculated for each head. Then the heads\nare concatenated and projected back to the original shape.\nMHA (Q, K, V) =Concat(softmax( QWq\ni (KW k\ni )T\n√dk\n)V Wv\ni )Wo (3)\nThe Seagate data sets are contain measurement pass/fail\ninformation, and the SST model is built as an classiﬁcation\nmodel. After the encoder blocks, a multi-layer perceptron\n(MLP) classiﬁer is attached on top after a global average\npooling. Because of the intrinsic complexity of the data,\nthe classiﬁer comprises a few individual binary classiﬁers.\nThese binary classiﬁers partly share the input data and may\nbe correlated with each other, resulting an inter-correlated\nmulti-task problem (further discussed in Section. 3). In order\nto achieve the best performance in the multi-task learning, a\nweighting method based on uncertainty [24] is applied, and\nwe deﬁne the combined loss function as Equation. 4:\nJ =\n∑\ni\n( 1\nσ2\ni\nJi + logσi) (4)\nwhere J is the total loss, Ji is the loss of the ith classiﬁ-\ncation task, and σi is the uncertainty of the ith classiﬁcation\nloss, which is trainable during the model ﬁtting.\n2.2. Optimization\nData imbalance. In the industrial settings, the data are\nhighly imbalanced. As a classiﬁcation model, we have only\n1% to 2% of the data samples as positive. To deal with the\nimbalance, we experimented on both weighting methods and\ndata sampling algorithms like SMOTE [25]. We found that\nclass weighting gives the best efﬁciency and performance in\nour experiments. The weight of the jth task, label t (0 or\n1) is calculated based on the number of samples:\nwt\nj = N\n2m∗ nt\nj\n(5)\nin which N is the total number of sample, m is the\nnumber of tasks, and nt\nj is the number of samples for label\nt in the jth task.\nCombined with the uncertainty based multi-task learning\nas Equation. 4, the ﬁnal loss function of SST model is\ndeﬁned as weighted cross entropy:\nJ =\nm∑\nj\n1∑\nt=0\n[ 1\nσ2\njt\nnt\nj∑\ni\n(yijt ∗ logˆyijt) +log(σjt)] (6)\nwhere yijt and ˆyijt are the true labels and predicted\nprobabilities for the ith sample in task j for label t. Note\nthat the cross entropy loss is calculated in a multi-label\nclassiﬁcation manner and the loss for positive and nega-\ntive cases are computed separately. We take yij1 = 1 for\npositive samples, and yij0 = 1 for negative samples. The\nweights for the positive and negative cases in a single binary\nclassiﬁcation task is also further tuned by σjt, which is the\nuncertainty or variance of the loss for label t in task j. In\nthis multi-task learning setting, we have 2m ’tasks’ for the\nm binary classiﬁcations.\nActivation functions. For the transformer encoder part,\na ReLU activation function [26] is applied in the feed\nforward layer, which is consist of two dense layers that\nproject the dmodel dimensional vector to dff dimension\nand project back to dmodel dimension, respectively. ReLu\nactivation function is set for the ﬁrst dense layer in the\nfeed forward layers. As for the MLP classiﬁer, we applied\nsigmoid activation functions for all three layers because we\nfound that it produced more stable results than ReLu in this\ncase.\nRegularization. L2 regularizers are applied to all the dense\nlayers in SST model, with a regularization factor of 10−4.\nDropout [27] is also applied to residual layers and embed-\nding layers. We also applied dropout to each layer in the\nMLP block except for the ﬁnal prediction layer. All dropout\nratios are kept the same and a grid search in [0.1,0.3,0.5]\nis performed to ﬁnd the best dropout ratio.\nOptimizer. We experimented with two kinds of optimizers:\ndefault adam optimizer [28] with ﬁxed learning rate, and\nscheduled adam optimizer similar as in [16]. The learning\nrate scheduled optimizer has shown a more stable result, so\nit’s kept in further experiments.\nFor the scheduled adam optimizer, the parameters are\nset as β1 = 0.9, β2 = 0.98, ϵ = 10−9. The learning rate\nis varied during the training process based on Equation. 7.\nd is dmodel in SST model, step is the training step, and\nwarmup is set as 4000. An extra factor is added to tune\nthe overall learning rate. A grid search for the factor in\n[0.1,0.3,0.5] is performed to ﬁnd the optimal factor.\nlr= factor ∗ d−0.5 ∗ min(step−0.5, step\nwarmup1.5 ) (7)\nTABLE 1. H YPER -PARAMETER SEARCH SPACE\nHyper-parameter Values\nn layers 2, 3, 4\nd ff 32, 128, 512\ndmodel 64, 128, 256\ndropout ratio 0.1, 0.3, 0.5\nlearning rate factor 0.1, 0.3, 0.5\nbatch size 512, 1024, 2048\nn heads 1, 2, 4\nuncertainty based weighting on, off\nHyper-parameter tuning. There are a few hyper-\nparameters to be tuned for the SST model training. As shown\nin Table. 1, in total 7 hyper-parameters are tuned using a grid\nsearch. The hyper-parameters include number of the encoder\nblock (n layer), the size of embedding layer ( dmodel), the\nsize of feed forward layer ( dff), the dropout ratio, learning\nrate factor as in Equation. 7, batch size, number of heads\nfor the multi-head attention layer (n heads), and whether or\nnot to use the uncertainty based weighting as in Equation.\n4. For the process of grid search, a smaller size of data\nare randomly sampled from the data sets, which contains\n5000 samples for training and 3000 for validation. The best\nmodel is picked based on the validation results, evaluated\nby the area under a Receiver Operating Characteristic Curve\n(ROC-AUC) [29].\nEarly-stopping. Instead of setting a epoch number, we\nused an Keras early-stopping callback method in the train-\ning processing, with a patience of 100 epochs, and re-\nstore best weights=True. In this way, we got an optimized\nepoch number for each experiment without manually tuning.\nHardware. All the models are trained on an AWS instance\nwith an NVIDIA Tesla V100 SXM2 GPU. It took around\n20ms for each step, and about 30 minutes for the entire\ntraining. The grid search for hyper-parameters took about\n36 hours. All the models are written with TensorFlow [30]\nversion 2.2 and Keras [31].\n3. Data\nTo ﬁll the gap of publicly available large scale soft sens-\ning data sets, we queried and processed several gigabytes of\ndata sets from Seagate manufacturing factories in both the\nUS and Ireland. These data sets contain high dimensional\ntime-series sensor data coming from different manufacturing\nmachines.\nAs shown in Fig. 2, to fabricate a slider used for hard\ndrives, an AlTiC wafer goes through multiple processing\nstages including deposition, coating, lithography, etching,\nand polishing. Different products have different manufactur-\ning lines, Fig. 2 shows a simpliﬁed and general processing.\nAfter each processing stage, the wafer is sent to metrology\ntools for quality control measurements. A metrology step\nmay have a single or multiple different measurements made\neach of which could have varying degrees of importance.\nFigure 2. High-level workﬂow of wafer manufacturing. Each wafer\ngoes through multiple processing stages, each stage has correspond-\ning metrology, in which a few quality control measurements are per-\nformed. The measurement results are used to decide whether the\nwafer is in a good shape to go to the next stage. Figure from\nhttps://github.com/Seagate/softsensing data.\nThese processes are highly complex and are sensitive\nto both incoming as well as point of process effects. A\nsigniﬁcant amount of engineering and systems resources are\nemployed to monitor and control the variability intrinsic to\nthe factors that are known to affect a process.\nMetrology serves a critical function of managing these\ncomplexities for early learning cycles and quality control.\nThis, however, comes at high capital costs, increased cycle\nFigure 3. Overview of the main categories of processes and the corre-\nsponding critical measurement variables per each category. Figure from\nhttps://github.com/Seagate/softsensing data.\ntime and considerable overhead to set up correct recipes\nfor measurements, appropriate process control and workﬂow\nmechanisms. In each processing tool, there are dozens to\nhundreds of onboard sensors in the processing machines to\nmonitor the state of the tool. These sensors collect infor-\nmation every few seconds and all these sensing values are\ncollected and stored along with the measurement results.\nAs shown in Fig. 3, one time-series of sensor data are\nmapped to several measurements, and the same measure-\nment can be applied to multiple processing sensor data\npoints. Each measurement contains a few numerical values\nto indicate the condition of the wafers, and a decision of\npass or fail is made based on these numbers. For the sake of\nsimplicity, we only cover the pass/fail information for each\nmeasurement, so that each sample of time-series sensor data\nare mapped to several binary classiﬁcation labels, resulting\nin a multi-task classiﬁcation problem. On the other hand,\nsome of measurements are linked to multiple processing\nstages, so that the SST model can learn the representations\nfrom one stage and apply to another stage when it’s trained\non data covering all the stages. Given this inter-correlation,\ntraining such a multi-task learning SST model leads to a\nbetter performance comparing with training the measure-\nment tasks individually. From the perspective of industrial\napplication, it’s also more maintainable and scalable to have\na single model instead of many ones.\nThe data sets in this paper cover 92 weeks of data. The\nﬁrst 70 weeks are taken as training data, and the following\n14 weeks as validation data, last 8 weeks as testing data.\nThe data sets are prepared by querying from raw data\nand doing some necessary pre-processing steps. While the\nsensors are collecting data every few seconds, there are a\nlot of redundancies, so we aggregated the data into short\nsequences. In each processing stage, a wafer goes through\na few modules, and we aggregate the data by the module\nand get short time sequences. Other pre-processing steps\non the data include a min-max scaling, imputation with\nneighbors, one-hot encoding for categorical variables, and\nnecessary anonymization. The min-max scaler is ﬁt only on\ntraining data, and applied on the entire data sets. Imputation\nis done by ﬁlling the missing values ﬁrst by it’s neighbors\n(a forward ﬁlling followed by a backward ﬁlling) if non-\nmissing values exist in the same processing stage, otherwise\nﬁlling by the mode of all the data. Categorical variables such\nas the processing stage information, the type of the wafer,\nthe manufacturing machine in function, are one-hot encoded\nand concatenated to the sensor data as model input. As for\nthe anonymization, only conﬁdential information like the\ndata headers is removed.\nUsing data in different timezframes for training and\ntesting reﬂects the application prospect of the SST model,\nbecause in this way the model can be directly deployed\ninto factories once it performs well enough in testing data.\nHowever, this setting also makes it harder for the model\nto achieve a high performance because in reality there are\ntoo many uncontrollable factors in the factories and the data\ndistribution of training and testing data may be different with\neach other.\nThese data sets are in Numpy format, which only include\nnumerical values without any headers. Input ﬁles are rank\n3 tensors with dimension (n sample, time step, features),\nand outputs are rank 2 tensors with dimension (n sample,\n2*n tasks). Each binary classiﬁcation task has two columns\nin the output ﬁle, ﬁrst column for negative cases and second\nfor positive cases.\nThree data sets are covered in this paper. They are\nfrom slightly different manufacturing tool families, and each\nhas different processing stages and corresponding measure-\nments. The number of samples for each measurement is\nsummarized in Table. 2. More detailed information for each\ntool family is described below, and all the data are available\nat https://github.com/Seagate/softsensing data.\nP1. The sensor data are generated by a deposition tool\nthat include both deposition and etching steps. There are\n90 sensors installed in the tools and they capture data at\na frequency of about every second. The critical parameters\nmeasured for this family of tools are magnetics, thickness,\ncomposition, resistivity, density, and roughness.\nAfter pre-processing mentioned above, there are 194k\ndata samples in training, 34k samples in validation, and 27k\nsamples in testing data. Each sample has 2 time steps, with\n817 features. Some of the second time steps are missing and\nreplaced with zero padding, and the 817 features come from\n90 sensors, one-hot encoded categorical variables including\nthe types of the wafer, the processing stages, and speciﬁc\nmanufacturing tools etc, and a padding indicator as the last\nfeature.\nFor the labels, there are 11 individual measurement\ntasks, each is a binary classiﬁcation. We set the model\nTABLE 2. S UMMARY FOR THE DATA SETS : NUMBER OF POSITIVE AND\nNEGATIVE SAMPLES FOR EACH TASK\nP1 P2 P3\nTask pos neg pos neg pos neg\n1 295 8328 256 6433 109 2496\n2 40 12747 773 26811 335 12857\n3 291 56198 2069 78844 46 1026\n4 188 14697 582 27809 15 4180\n5 568 40644 247 9652 300 22254\n6 863 84963 884 27337 166 40811\n7 2501 153970 2108 53921 875 75706\n8 490 2919 2016 77473 1097 18890\n9 104 29551 644 23305 537 4247\n10 57 10813 270 25651 1547 129914\n11 306 47219 3792 354328\noutput dimension as 22 to have separate predictions for\nnegative and positive probabilities, and normalize them to\nget the predicted probabilities after applying class weights\nfor the data imbalance. As shown in Table. 2, the data set\nare highly imbalanced, there are about 1.2% of the samples\nhave positive labels.\nP2. This second data set contains data generated by a family\nof ion milling (dry etch) equipment, which utilize ions in\nplasma to remove material from a surface of the wafer. There\nare 57 sensors for this data set, and the critical parameters\nmeasured for this family of tools are similar to P1 tools, but\nwith slightly different measurement machines.\nThere are 457k training samples, 80k validation samples,\nand 66k testing samples in the data set. For this data set,\nthere is no time-series information, but we treat it as 1 time\nstep to ﬁt into the same SST model. This data set is more\ncomplex in terms of categorical variables, resulting in 1484\nfeatures in total.\nThe number of measurement tasks is 11, with an output\ndimension of 22, and about 1.9% of the samples are positive\nas in Table. 2. Note that these 11 tasks are not the same as\nthose in P1.\nP3. The last data set is generated by sputter deposition\nequipment containing multiple deposition chambers, with\nunique targets. The number of sensors is 43, and critical\nparameters measured are the same but with different ma-\nchines.\nThere are 205k training data samples, 35k for validation,\nand 20k for testing. The maximum time-series length is\n2, with outliers ﬁltered out and short series padded. The\nnumber of features is 498, the least among these three data\nsets.\nThe number of measurement tasks is 10, and output\ndimension is 20. Note that these tasks are not the same as\nthose in P1 and P2 data. The percentage of positive cases\nis about 1.6%.\n4. Results\nThe SST models have been run on the three data sets\nmentioned in the last section. The hyper-parameters are\ntuned within the range shown in Table. 1, and the best\ncombinations are chosen to present below for each data set.\nTo validate the effectiveness of SST, the results are\ncompared with two baseline models. The ﬁrst one is\nvariance weighted multi-headed quality driven autoencoder\n(VWMHQAE) [32] which was developed by our team in\n2020. The model is based on stacked autoencoder archi-\ntecture, and utilized the output (quality-control variables)\ninformation by reconstructing both the input and output\nafter encoding. It added the multi-headed structure to do the\nmulti-task learning, and applied a variance-based weight to\nthe tasks that are same as SST model as in Equation. 4. It\nhas been proven to work well with non-time-series data in\nour previous experiments with similar sensor data, therefore\nserves as a good baseline model for SST. Since it doesn’t\nhave an architecture to cover the time dimension, the data\nis ﬂattened before feeding into the model. Also, we trained\na second baseline model: a bidirectional LSTM model (Bi-\nLSTM), which is one of the golden standard models for\ntime series data, to have a comprehensive benchmark on\nthe performance of SST.\nDue to the highly imbalanced nature of the data sets,\naccuracy would not make much sense to evaluate the mod-\nels. The most important metrics that the industry cares are\nTrue Positive Rate (TPR, also called recall or sensitivity)\nand False Positive Rate (FPR, also called fall-out or false\nalarm ratio). However, comparing two metrics together is not\nintuitive, so we chose to use the Receiver Operating Char-\nacteristic (ROC) curve and the Area Under Curve (AUC)\nas the main metric in this paper. More detailed results are\ncovered in Appendix.\nP1. For the P1 data set, SST model is set as 3 layers, both\ndmodel and dff are 128, dropout rate is 0.5, batch size\nis 2048, n heads is 1, learning rate factor is 0.5, and the\nuncertainty based weighting is off. the VWMHQAE model\nis set as three layers with hidden dimension [512, 256,\n128], and Bi-LSTM model with dimension equal to dff. All\nmodels are followed by a three-layer MLP classiﬁer with all\nhidden dimensions as dff.\nThe results for the 11 tasks are summarized in Table. 3.\nin 7 of the tasks SST are the best performer, especially for\nthe high performing tasks where AUC larger than 0.8.\nFrom the results we can also see that some of the tasks\nhave poor results for all three models. They are difﬁcult to\nget a high AUC value with any model due to the intrinsic\ncomplex and noisy nature. Only those measurement tasks\nwith decent results can lead to realistic value in industry\napplications. This is one of the primary motivations behind\nour decision to open-access these data sets: researchers all\naround the world are welcomed to use and explore this data.\nThis will not only help us to gain more understanding about\nthe data sets, but also enrich the research ﬁeld.\nTABLE 3. R ESULT COMPARISON WITH BASELINE MODELS : P1\nTask SST VWMHQAE Bi-LSTM\n1 0.70 ± 0.12 0 .63 ± 0.12 0.73 ± 0.13\n2 0.60 ± 0.18 0 .54 ± 0.03 0 .56 ± 0.01\n3 0.86 ± 0.01 0 .77 ± 0.05 0.86 ± 0.03\n4 0.91 ± 0.01 0 .89 ± 0.01 0 .88 ± 0.01\n5 0.55 ± 0.03 0.55 ± 0.02 0 .51 ± 0.05\n6 0.53 ± 0.05 0.57 ± 0.03 0 .50 ± 0.03\n7 0.64 ± 0.02 0.65 ± 0.01 0 .64 ± 0.01\n8 0.82 ± 0.03 0 .78 ± 0.04 0 .78 ± 0.11\n9 0.71 ± 0.09 0.77 ± 0.01 0.77 ± 0.06\n10 0.92 ± 0.03 0 .82 ± 0.03 0 .67 ± 0.23\n11 0.89 ± 0.01 0 .77 ± 0.03 0 .88 ± 0.01\nFigure 4. ROC curve for SST and baseline models on P1 data set, 4th task\nTo further illustrate the results, the ROC curve is plot-\nted for the task with highest AUC as in Fig. 4. SST\nhas a higher score than the two baseline models, and\nthe curve is smoother, meaning a more even distribu-\ntion of the prediction probabilities and a ﬁner grid in\nthe prediction space. The source code can be found at\nhttps://github.com/Seagate/SoftSensingTransformer.\nP2. SST model is set as 3 layers, both dmodel and dff\nare 128, dropout rate is 0.3, batch size is 2048, n heads\nis 1, learning rate factor is 0.5, and the uncertainty based\nweighting is on. Baseline models are the same as P1.\nThe results for the 11 tasks are summarized in Table. 4.\nSST is the best performer in 4 of the tasks, including the\ntwo tasks with the best prediction. Same as in P1, some of\nthe tasks have poor results for all three models due to the\nintrinsic complexity and noise in the data set, and we mostly\ncare about the tasks with best results. In this data set, there is\nonly one time step, and as expected the VWMHQAE model,\nwhich is not designed for time series data, is showing better\nresults comparing to P1 data, and it has the best performance\nin 5 out of the 11 tasks.\nThe ROC curve for the task with highest AUC as in\nFig. 5 is very similar to the previous one. SST is slightly\nsmoother than the baseline models, with a higher AUC.\nTABLE 4. R ESULT COMPARISON WITH BASELINE MODELS : P2\nTask SST VWMHQAE Bi-LSTM\n1 0.89 ± 0.01 0 .87 ± 0.02 0 .88 ± 0.04\n2 0.64 ± 0.01 0.66 ± 0.02 0 .59 ± 0.07\n3 0.60 ± 0.06 0.62 ± 0.01 0 .55 ± 0.01\n4 0.85 ± 0.01 0 .82 ± 0.01 0 .84 ± 0.01\n5 0.45 ± 0.07 0.53 ± 0.10 0 .52 ± 0.10\n6 0.64 ± 0.02 0 .71 ± 0.04 0.72 ± 0.02\n7 0.78 ± 0.01 0.81 ± 0.02 0 .79 ± 0.01\n8 0.72 ± 0.03 0 .70 ± 0.09 0 .69 ± 0.03\n9 0.71 ± 0.12 0.77 ± 0.08 0.48 ± 0.14\n10 0.76 ± 0.02 0 .75 ± 0.01 0.76 ± 0.04\n11 0.79 ± 0.01 0 .80 ± 0.01 0.82 ± 0.01\nFigure 5. ROC curve for SST and baseline models on P2 data set, 1st task\nTABLE 5. R ESULT COMPARISON WITH BASELINE MODELS : P3\nTask SST VWMHQAE Bi-LSTM\n1 0.42 ± 0.08 0 .23 ± 0.12 0 .32 ± 0.08\n2 0.94 ± 0.02 0.94 ± 0.02 0 .93 ± 0.01\n3 0.68 ± 0.08 0 .53 ± 0.10 0 .56 ± 0.05\n4 − − −\n5 0.53 ± 0.08 0.65 ± 0.05 0 .58 ± 0.05\n6 − − −\n7 0.60 ± 0.05 0 .56 ± 0.02 0.61 ± 0.07\n8 0.72 ± 0.05 0 .62 ± 0.08 0 .65 ± 0.08\n9 − − −\n10 0.81 ± 0.01 0 .81 ± 0.01 0.82 ± 0.01\nP3. SST model is set as 3 layers, both dmodel and dff\nare 128, dropout rate is 0.3, batch size is 2048, n heads\nis 1, learning rate factor is 0.3, and the uncertainty based\nweighting is on. Baseline models are the same as P1.\nThe results for 7 out of 10 tasks are summarized in\nTable. 5, because the others has too few testing data samples.\nSST is the best performer in 4 of the tasks, including the ﬁrst\ntask with the best prediction. Some of the tasks have poor\nresults for all three models and even with an AUC lower\nthan 0.5, meaning it’s worse than a random guess. Its main\ncause is the distribution shift and further experiments will\nFigure 6. ROC curve for SST and baseline models on P3 data set, 2nd task\nbe carried out when we accumulated more data in Seagate\nfactories. The ROC curve for the task with highest AUC as\nin Fig. 6 is very similar to the previous ones.\n5. Discussion and Conclusion\nWe have explored the direct application of Transformers\nto soft sensing. To our knowledge, we are the ﬁrst to provide\nlarge scale soft sensing data sets, and the ﬁrst to benchmark\nthe results with the original transformer model in the soft\nsensing ﬁeld. Also, this is the ﬁrst time that transformer\nmodel goes beyond human in the sense that the input data\nis not human-interpretable. We analogize the time-series\ndata as a sequence of sensor values, each time step is\ntaken as a word, and process the sentence-like data by a\nstandard transformer encoder exactly as in NLP. This direct\nand intuitive strategy has shown an exciting result for our\ndata sets that outperforms our previous model and Bi-LSTM\nmodel.\nWe share these data sets with the excitement of ad-\nvancing interest and work in research and applications of\nsoft sensing. We invite future work into the exploration of\nimproving SST performance on some tasks that have been\nparticularly challenging in our experiments to learn. Another\nfuture direction can be the examination of appropriate time\nsequences for these data sets, and exploration of better ways\nto address missing data. We are working on acquiring more\ndata with longer sequences, to better understand the impact\nof time series length in the prediction of quality. In the\nmeantime, we have provided three data sets to cover a\nvariety of sensors, and to examine the generalizability of\ndeep learning models, and we believe these data sets can\nenrich the soft sensing research ﬁeld and serve as one of\nthe standard tools to evaluate the effectiveness of future\nresearch.\nAcknowledgments\nThe authors would like to thank Seagate Technology\nfor the support on this study, the Seagate Lyve Cloud team\nfor providing the data infrastructure, and the Seagate Open\nSource Program Ofﬁce for open sourcing the data sets and\nthe code. Special thanks to the Seagate Data Analytics and\nReporting Systems team for inspiring the discussions.\nReferences\n[1] Q. Sun and Z. Ge, “A survey on deep learning for data-driven soft\nsensors,” IEEE Transactions on Industrial Informatics , 2021.\n[2] C.-Y . Liou, W.-C. Cheng, J.-W. Liou, and D.-R. Liou, “Autoencoder\nfor words,” Neurocomputing, vol. 139, pp. 84–96, 2014.\n[3] P. Smolensky, “Information processing in dynamical systems: Foun-\ndations of harmony theory,” Colorado Univ at Boulder Dept of\nComputer Science, Tech. Rep., 1986.\n[4] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” nature, vol.\n521, no. 7553, pp. 436–444, 2015.\n[5] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning rep-\nresentations by back-propagating errors,” nature, vol. 323, no. 6088,\npp. 533–536, 1986.\n[6] X. Yuan, J. Zhou, B. Huang, Y . Wang, C. Yang, and W. Gui,\n“Hierarchical quality-relevant feature representation for soft sensor\nmodeling: A novel deep learning strategy,” IEEE Transactions on\nIndustrial Informatics, vol. 16, no. 6, pp. 3721–3730, 2020.\n[7] X. Qian, H. Cheng, D. Chen, Q. Liu, H. Chen, H. Jiang, and M.-C.\nHuang, “The smart insole: A pilot study of fall detection,” in EAI\nInternational Conference on Body Area Networks . Springer, 2019,\npp. 37–49.\n[8] X. Qian, H. Chen, H. Jiang, J. Green, H. Cheng, and M.-C. Huang,\n“Wearable computing with distributed deep learning hierarchy: a\nstudy of fall detection,” IEEE Sensors Journal , vol. 20, no. 16, pp.\n9408–9416, 2020.\n[9] Y . Huang, Y . Tang, J. VanZwieten, and J. Liu, “Reliable machine\nprognostic health management in the presence of missing data,”\nConcurrency and Computation: Practice and Experience , p. e5762,\n2020.\n[10] Y . Huang, Y . Tang, and J. Vanzwieten, “Prognostics with variational\nautoencoder by generative adversarial learning,” IEEE Transactions\non Industrial Electronics , 2021.\n[11] Q. Sun and Z. Ge, “Deep learning for industrial kpi prediction: When\nensemble learning meets semi-supervised data,” IEEE Transactions\non Industrial Informatics , vol. 17, no. 1, pp. 260–269, 2020.\n[12] Z. Geng, Z. Chen, Q. Meng, and Y . Han, “Novel transformer based on\ngated convolutional neural network for dynamic soft sensor modeling\nof industrial processes,” IEEE Transactions on Industrial Informatics,\n2021.\n[13] F. A. Souza, R. Ara ´ujo, T. Matias, and J. Mendes, “A multilayer-\nperceptron based method for variable selection in soft sensor design,”\nJournal of Process Control , vol. 23, no. 10, pp. 1371 – 1378, 2013.\n[Online]. Available: http://www.sciencedirect.com/science/article/pii/\nS0959152413001832\n[14] L. Fortuna, S. Graziani, A. Rizzo, M. G. Xibilia et al., Soft sensors\nfor monitoring and control of industrial processes . Springer, 2007,\nvol. 22.\n[15] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in\nAdvances in neural information processing systems , 2017, pp. 5998–\n6008.\n[17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language understand-\ning,” arXiv preprint arXiv:1810.04805 , 2018.\n[18] T. Lin, Y . Wang, X. Liu, and X. Qiu, “A survey of transformers,”\narXiv preprint arXiv:2106.04554 , 2021.\n[19] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,\nC. Xu, Y . Xu et al., “A survey on visual transformer,” arXiv preprint\narXiv:2012.12556, 2020.\n[20] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gellyet al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[21] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using\nshifted windows,” arXiv preprint arXiv:2103.14030 , 2021.\n[22] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, “Scaling vision\ntransformers,” arXiv preprint arXiv:2106.04560 , 2021.\n[23] K. Fukunada, “Introduction to statistical pattern recognition,” Aca-\ndemic Press Inc., San Diego, CA, USA , 1990.\n[24] A. Kendall, Y . Gal, and R. Cipolla, “Multi-task learning using un-\ncertainty to weigh losses for scene geometry and semantics,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 7482–7491.\n[25] N. V . Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,\n“Smote: synthetic minority over-sampling technique,” Journal of ar-\ntiﬁcial intelligence research, vol. 16, pp. 321–357, 2002.\n[26] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\nboltzmann machines,” in Icml, 2010.\n[27] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: a simple way to prevent neural networks\nfrom overﬁtting,” The journal of machine learning research , vol. 15,\nno. 1, pp. 1929–1958, 2014.\n[28] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,” arXiv preprint arXiv:1412.6980 , 2014.\n[29] T. Fawcett, “An introduction to roc analysis,” Pattern recognition\nletters, vol. 27, no. 8, pp. 861–874, 2006.\n[30] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.\nCorrado, A. Davis, J. Dean, M. Devin et al. , “Tensorﬂow: Large-\nscale machine learning on heterogeneous distributed systems,” arXiv\npreprint arXiv:1603.04467, 2016.\n[31] F. Chollet et al., “Keras,” https://keras.io, 2015.\n[32] C. Zhang and S. Bom, “Auto-encoder based model for high-\ndimensional imbalanced industrial data,” 2021.\nAppendix\nFigure 7. ROC curves for SST and baseline models on P1 data set, all tasks\nFigure 8. ROC curves for SST and baseline models on P2 data set, all tasks\nFigure 9. ROC curves for SST and baseline models on P3 data set, all tasks\nFigure 10. TPR for SST and baseline models on P1 data set\nFigure 11. FPR for SST and baseline models on P1 data set\nFigure 12. TPR for SST and baseline models on P2 data set\nFigure 13. FPR for SST and baseline models on P2 data set\nFigure 14. TPR for SST and baseline models on P3 data set\nFigure 15. FPR for SST and baseline models on P3 data set",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7503921985626221
    },
    {
      "name": "Transformer",
      "score": 0.7100511789321899
    },
    {
      "name": "Data modeling",
      "score": 0.5042914152145386
    },
    {
      "name": "Sentence",
      "score": 0.4963098168373108
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48975467681884766
    },
    {
      "name": "Encoder",
      "score": 0.46966588497161865
    },
    {
      "name": "Deep learning",
      "score": 0.42829716205596924
    },
    {
      "name": "Computer engineering",
      "score": 0.34261685609817505
    },
    {
      "name": "Data mining",
      "score": 0.33827096223831177
    },
    {
      "name": "Machine learning",
      "score": 0.3298311233520508
    },
    {
      "name": "Engineering",
      "score": 0.15779641270637512
    },
    {
      "name": "Database",
      "score": 0.15431666374206543
    },
    {
      "name": "Electrical engineering",
      "score": 0.12649616599082947
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I131787340",
      "name": "Seagate (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I63135867",
      "name": "University of Cincinnati",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I63772739",
      "name": "Florida Atlantic University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I58956616",
      "name": "Case Western Reserve University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 23
}