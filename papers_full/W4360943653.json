{
  "title": "Bias Amplification in Intersectional Subpopulations for Clinical Phenotyping by Large Language Models",
  "url": "https://openalex.org/W4360943653",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3023753985",
      "name": "Ridam Pal",
      "affiliations": [
        "Indraprastha Institute of Information Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2184069399",
      "name": "Hardik Garg",
      "affiliations": [
        "Indraprastha Institute of Information Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A3130353747",
      "name": "Shashwat Patel",
      "affiliations": [
        "Indraprastha Institute of Information Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2497888249",
      "name": "Tavpritesh Sethi",
      "affiliations": [
        "Indraprastha Institute of Information Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A3023753985",
      "name": "Ridam Pal",
      "affiliations": [
        "Indraprastha Institute of Information Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2184069399",
      "name": "Hardik Garg",
      "affiliations": [
        "Indraprastha Institute of Information Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A3130353747",
      "name": "Shashwat Patel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2497888249",
      "name": "Tavpritesh Sethi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4281712277",
    "https://openalex.org/W3207549851",
    "https://openalex.org/W2965570621",
    "https://openalex.org/W3011762034",
    "https://openalex.org/W2953691788",
    "https://openalex.org/W2768894107",
    "https://openalex.org/W4287890645",
    "https://openalex.org/W2109206523",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3172427031",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3184736942",
    "https://openalex.org/W2001705627",
    "https://openalex.org/W2345328813"
  ],
  "abstract": "Abstract Large Language Models (LLMs) have demonstrated remarkable performance across diverse clinical tasks. However, there is growing concern that LLMs may amplify human bias and reduce performance quality for vulnerable subpopulations. Therefore, it is critical to investigate algorithmic underdiagnosis in clinical notes, which represent a key source of information for disease diagnosis and treatment. This study examines prevalence of bias in two datasets - smoking and obesity - for clinical phenotyping. Our results demonstrate that state-of-the-art language models selectively and consistently underdiagnosed vulnerable intersectional subpopulations such as young-aged-males for smoking and middle-aged-females for obesity. Deployment of LLMs with such biases risks skewing clinicians’ decision-making which may lead to inequitable access to healthcare. These findings emphasize the need for careful evaluation of LLMs in clinical practice and highlight the potential ethical implications of deploying such systems in disease diagnosis and prognosis.",
  "full_text": "Bias Amplification in Intersectional Subpopulations for Clinical Phenotyping\nby Large Language Models\nRidam Pal1*, PhD, Hardik Garg1*, BTech, Shashwat Patel1*, BTech, Tavpritesh Sethi1, PhD.\n1. Indraprastha Institute of Information Technology, Delhi, India\n*All authors have equally contributed\nAbstract\nLarge Language Models (LLMs) have demonstrated remarkable performance across diverse clinical tasks. However,\nthere is growing concern that LLMs may amplify human bias and reduce performance quality for vulnerable\nsubpopulations. Therefore, it is critical to investigate algorithmic underdiagnosis in clinical notes, which represent a\nkey source of information for disease diagnosis and treatment. This study examines prevalence of bias in two\ndatasets - smoking and obesity - for clinical phenotyping. Our results demonstrate that state-of-the-art language\nmodels selectively and consistently underdiagnosed vulnerable intersectional subpopulations such as\nyoung-aged-males for smoking and middle-aged-females for obesity. Deployment of LLMs with such biases risks\nskewing clinicians’ decision-making which may lead to inequitable access to healthcare. These findings emphasize\nthe need for careful evaluation of LLMs in clinical practice and highlight the potential ethical implications of\ndeploying such systems in disease diagnosis and prognosis.\nIntroduction\nArtificial Intelligence (AI) based algorithms are rapidly influencing decision making in diverse domains such as\nautonomous vehicle navigation, fraud detection, recommender systems with healthcare being no exception. Owing\nto the availability of large volumes of data in the form of images, electronic signals and electronic health records\n(EHR), a vast number of AI models have been trained and used to influence clinicians’ decision-making process. In\nthe present scenario, a mammoth amount of clinical notes are available containing densely rich information about a\npatient. However, capturing relevant information from such resources to assist clinicians in decision-making can be\nequally challenging. Language models have demonstrated a potential aid in addressing this challenge in a real-world\nsetting. Language models can assist in identifying patterns in large amounts of patient data, which can help with\nearly diagnosis and treatment of diseases. The advent of transformer architecture has caused a rampant surge in the\ndevelopment of sophisticated large language models (LLMs). Transformer architecture has the ability to capture vast\namounts of semantic knowledge using self-attention mechanisms encoded as word embeddings. Hence,\ntransformer-based language models achieved state-of-the-art results for a number of clinical applications [1–4]. This\nhas led to automating tasks, improved patient outcomes and reduced economic costs in clinical settings by assisting\nphysicians in clinical thought processes.\nHowever, despite its promise, there is growing concern regarding the fairness of these systems because AI\nalgorithms have been shown to generate and amplify bias in a number of settings. Identifying bias in AI algorithms\nrelated to the healthcare domain is important because it can lead to inaccurate predictions, misdiagnosis, and\npotentially harmful treatment recommendations for patients. Such bias can be introduced at different stages of the\nalgorithm development and deployment process, including data collection and preparation, algorithm design, and\ntraining. If not addressed, bias can lead to disparities in healthcare outcomes and exacerbate existing inequalities in\nthe healthcare system. In the context of healthcare, bias is defined as the systematic and unintended favoring of\nindividuals or groups on the basis of characteristics such as gender, age, ethnicity or other factors. Biases can be\nintroduced in clinical notes through various ways, which include recruitment or collection strategy of data, analysis\nof data, and misinterpretation of data by AI systems. Additionally, these biases may arise due to various other\nfactors, such as clinicians’ implicit biases, differences in documentation practices or patient population\ndemographics. This leads to skewed representations of the patient’s medical history or health status. The presence of\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 25, 2023. ; https://doi.org/10.1101/2023.03.22.23287585doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nsuch biases can affect clinical decision-making, diagnosis, treatment and outcomes. Therefore bias analysis and\nmitigation become even more essential in a domain as critical as healthcare because unintended bias may hinder\ndiagnosis leading to inequitable access to healthcare services for under-represented groups. A significant amount of\nwork has already been done in the domain of algorithmic bias. For example, African Americans have been denied\nloans and given longer prison sentences compared to their Caucasian counterparts. In the healthcare domain,\ndifferent studies [5] have come up with different notions of bias and demonstrated the same for various modalities\nsuch as clinical notes, medical scans and electronic signals. Seyyed-Kalantari et al. [6] illustrated underdiagnosis\nbias during triage in the diagnosis of patients, stating that underdiagnosis is potentially worse than misdiagnosis\nbecause the patient still receives medical care in the latter case. They analyzed under-served populations for chest\nradiographs and further reported that intersectional groups of under-served populations, such as Hispanic Female\npatients, are more prone to underdiagnosis bias. Further, Zhang et. al [7] analyzed differences in the encoding of\ncontextual embeddings for MIMIC-III dataset between marginalized and non-marginalized populations in terms of\ngender, ethnicity and insurance status. They showed that the majority group was always favored with regard to\ndemographic denominators. Patient demographics such as gender, age, ethnicity and socio-economic status provide\nmeaningful information used by clinicians. They can also lead to undesirable biases in AI predictions, which hinder\naccess to healthcare services. Therefore, it is essential to design AI systems that capture the relevant information\nfrom demographic factors while minimizing the effect of bias.\nThe study of biases in intersectional groups provides insight into the complexities and interactions of social\ndeterminants of health and the health disparities across population subgroups. Notably, there has been limited\nliterature in the field of Artificial Intelligence (AI) backed healthcare that examines intersectional bias for multiple\ndemographic dimensions in case of clinical notes. Ogungbe et. al. [8] presented a survey of studies which\nillustrated the amplification of implicit bias for intersectional groups - the implicit bias of the participants was\nmeasured using IAT and the bias was shown to amplify for 2-fold (gender, age) and 3-fold demographics (gender,\nage, ethnicity). Another study [9] examined the bias between demographic intersections such as young men and old\nwomen. Tan et al. [10] suggested methods to evaluate intersectional biases for contextual word embeddings and\nshowed that biases for the intersection of two demographic dimensions were greater than the individual dimensions.\nLalor et al. [11] analyzed the intersectional bias in NLP related tasks across five text based datasets. It was reported\nthat as the degree of intersection between groups increased, the Fairness Violation metric defined increased,\nindicating that the bias increases with degree of intersection. Given the staggering rate at which AI systems are\nbeing used by clinicians, the prevalence of biases in intersectional groups is an alarming cause of concern. The black\nbox nature of Deep Learning models hinders explainability and affects decision making, thus preventing these\nintersectional groups from receiving timely and vital medical attention.\nThe key objectives of this study were to (i) Conduct bias analysis of LLMs across demographic dimensions such as\nage and gender; (ii) Study incremental bias across intersectional subgroups to aid clinicians in the decision making\nprocess. To this end, we analyzed a compendium of language models by performing clinical phenotyping on i2b2\n2006 smoking and i2b2 2008 obesity datasets. Subsequently, bias analysis was conducted on segregated groups (age,\ngender) and intersectional subgroups (intersection of age and gender). To summarize, this study will enable\nclinicians and decision makers to be mindful of the implications of the biases introduced and amplified by LLMs\nmaking them more inclusive and suitable for real-world deployment.\nMethods\nThis study utilized two datasets for phenotyping - i2b2 2006 smoking [12] and i2b2 2008 obesity which consist of\ndischarge summaries from Partners HealthCare. Each record was de-identified and annotated by two pulmonologists\nwho used textual judgment and medical intuition for annotation. Disagreements were handled by obtaining\njudgments from two other pulmonologists and in case there was no majority vote, the record was omitted. For i2b2\n2006 smoking, the records were classified into 5 categories - Past Smoker, Current Smoker, Smoker, Non-Smoker\nand Unknown. The obesity dataset records were classified into 15 categories focused around obesity and its\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 25, 2023. ; https://doi.org/10.1101/2023.03.22.23287585doi: medRxiv preprint \ncomorbidities. Table-1 represents the statistics for both the cohorts along with gender and age-wise segregated\nnumbers. Based on this strategy, the final cohort consisted of 398 training and 104 testing records for i2b2 2006\nsmoking. The i2b2 2008 obesity dataset consisted of 730 training and 507 testing records.\nThe experimental setup has been as follows - the clinical notes were divided into chunks and each chunk was fed\ninto a transformer based language model. A compendium of multiple BERT-based architectures trained across\ndifferent corpora were fine-tuned on the i2b2 cohorts for phenotyping. The base BERT [13] model consists of 110M\nparameters and trained with two objectives - Masked Language Modeling (MLM) and Next Sentence Prediction\n(NSP). BioBERT [14] and BioClinicalBERT [15] provide domain specific BERT models trained on a large number\nof biomedical and clinical corpora such as PubMed articles and clinical datasets like MIMIC-III [16]. SciBERT [17]\nis trained on a large multi-domain corpus of scientific publications whereas UMLS-BERT [18] modifies the BERT\narchitecture by fusing clinical semantic embeddings with the contextual embeddings. Each model outputs a\ncontextual embedding vector based on the chunk fed to it. There were several ways to combine the chunks - adding\nan LSTM layer, taking element-wise maximum or taking the mean of all vectors. The mean of all contextual\nembedding vectors across all the chunks was shown to outperform all the methods of combining the chunks\nmentioned previously [19,20]. Hugging Face [21] library was used for pre-trained BERT-based language models.\nThe models were trained for 1000 epochs for both datasets with a batch size of 64 samples per iteration. Binary\nCross Entropy loss was used as the loss function and optimization was done using Adam optimizer with a learning\nrate of 6e-5. The BERT batch size was kept as 7, representing the number of samples one BERT model should take\nat a time. In this manner, each of the BERT models was finetuned on the respective cohorts and the aggregated\nresults for each of the models was reported in Table-2.\nFor the purpose of bias analysis the data was firstly segregated based on age and gender of the patients. Each clinical\nnote in both the cohorts followed a fixed structure in terms of headings - “DISCHARGE SUMMARY”, “HISTORY\nOF PRESENT ILLNESS” (HPI), “SECONDARY DIAGNOSIS”, etc such that the History of Present Illness(HPI)\nsection contained information about the patients’ gender and age. Leveraging this information, a rule-based\nsegmentation was performed on the clinical notes where first the note was divided into sections and subsequently,\nage and gender was extracted from the clinical notes. For gender segregation, a vocabulary of male and\nfemale-specific pronouns was constructed and searched for in the clinical note, following which the appropriate\nlabel was assigned. Ages were divided into five groups - 0-20, 20-40, 40-60, 60-80 and 80+ years as done in\nprevious studies on bias in the healthcare domain [6]. The age is found by searching for keywords such as “year”\nand “age” in the clinical notes. With this segregation the cohorts were sliced on the basis of gender and age one by\none and the fine-tuned language models were evaluated. The segregated cohorts were manually verified. Bias across\ngender and age for different models was reported and analyzed.\nFinally, population subgroups were created to analyze intersectional bias. An intersectional group refers to a\npopulation subgroup composed of a combination of two or more demographic dimensions. In the current study, two\ndemographic dimensions - age and gender were considered. For example, all female patients above 80 years of age\nand all male patients under 40 years of age are examples of such intersectional groups. The metric used for bias\nevaluation of both multi-label classification tasks was micro F1-score because it is a balanced metric that takes into\naccount both Precision and Recall. To compare bias between two groups belonging to a demographic, their\nrespective micro F1-scores were compared. We propose and experimentally verify that individual demographic\ndimensions amplify the bias when multiple dimensions are combined together. To put it mathematically, F1g,a\ni,j =\nF1g\ni ⋂F1a\nj < max(F1g\ni , F1a\nj)where F1 g\ni denotes the micro F1-score for i’th gender group and F1 a\nj denotes the micro\nF1-score for j’th age group. F1 g,a\ni,j thus represents the micro F1-score for i’th gender group and j’th age group\ncombined and serves as a metric to quantify intersectional bias. Therefore, if two groups that are already biased\nagainst on the basis of a single demographic dimension are combined, the resultant intersectional group will be at a\ngreater risk of being biased against.\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 25, 2023. ; https://doi.org/10.1101/2023.03.22.23287585doi: medRxiv preprint \nResults\nBased on the segregation criterion, the i2b2 2006 smoking dataset had 398 samples analyzed for biases across\ngender groups, whereas 442 samples were analyzed for biases across age groups. The i2b2 2008 obesity, on the\nother hand, had 1141 and 1046 samples analyzed for biases across gender and age groups, respectively. In the\nsmoking dataset, 185 samples were female patients, whereas 506 samples were encountered in the obesity dataset.\nFor the age group cohort, the 60-80 group had the highest frequency of samples - 174 and 491 across smoking and\nobesity datasets, respectively. For intersectional subgroups formed by combining demographic dimensions of age\nand gender, it was observed that young males and young females were the most underrepresented groups with\ncounts of 19 and 29 for smoking and 12 and 45 for obesity datasets. Old males had the maximum samples (101) for\nsmoking, whereas old females had the maximum samples (273) for obesity datasets. Middle males and middle\nfemales had numbers comparable to old males and old females for both datasets. Detailed statistics are shown in\nTable-1.\nTable 1: Cohort statistics. Distribution of training & testing dataset across gender, age & intersection groups.\nSegregation\nCriterion Criterion Value\ni2b2 2006 smoking i2b2 2008 obesity\nTrain Set Test Set Train Set Test Set\nGender\nMale 163 (41%) 50 (48%) 363 (50%) 272 (54%)\nFemale 152 (38%) 33 (32%) 309 (42%) 197 (39%)\nNaN 83 (21%) 21 (20%) 58 (8%) 38 (7%)\nAge Group\n0-20 5 (2%) 1 (1%) 11 (2%) 4 (1%)\n20-40 46 (12%) 12 (11%) 35 (5%) 22 (4%)\n40-60 107 (27%) 28 (27%) 177 (24%) 144 (28%)\n60-80 134 (33%) 40 (39%) 299 (41%) 192 (38%)\n80+ 57 (14%) 12 (11%) 52 (7%) 110(22%)\nNaN 49 (12%) 11 (11%) 156 (21%) 35 (7%)\nAge & Gender\nIntersection\n(Young: 20-40;\nMiddle: 40-60;\nOld: 60+ years)\nYoung Male 14 (3.5%) 5 (5%) 8 (1%) 4 (0.5%)\nMiddle Male 32 (8%) 8 (7.5%) 95 (13%) 66 (13%)\nOld Male 86 (21.5%) 15 (14.5%) 153 (21%) 98 (19%)\nYoung Female 24 (6%) 5 (5%) 28 (4%) 17 (3%)\nMiddle Female 45 (11%) 12 (11.5%) 87 (12%) 92 (18%)\nOld Female 72 (18%) 27 (26%) 168 (23%) 105 (21%)\nOthers (NaN & 0-20) 125 (32%) 32 (30.5%) 191 (26%) 125 (24.5%)\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 25, 2023. ; https://doi.org/10.1101/2023.03.22.23287585doi: medRxiv preprint \nProfiling based on models suggested that all models achieved a micro F1-score of more than 0.81 and 0.70 for\nsmoking and obesity datasets respectively. The performance of models decreased on the obesity dataset owing to the\nfact that the obesity dataset consisted of 15 output classes concerning obesity and associated ailments, whereas\nsmoking consisted of only 5 output classes. Therefore, clinicians need to be more vigilant while assessing decisions\nmade by these models on obesity-related cohorts. Table-2 demonstrates the metric evaluation of different models on\nphenotyping tasks. UMLS-BERT was used to perform further analysis on bias and intersectional subgroups because\nit was the best-performing model for both datasets and has the highest likelihood to be deployed in a practical setting\nto assist a clinician’s decision-making process.\nTable 2: Results on complete cohort for BERT-based language models\nDataset Model Precision Recall F1-Score (micro)\ni2b2 2006\nsmoking\nBERT 0.85 土 0.04 0.82 土 0.04 0.84 土 0.03\nBioBERT 0.84 土 0.03 0.85 土 0.03 0.84 土 0.03\nBio-ClinicalBERT 0.80 土 0.04 0.81 土 0.04 0.81 土 0.04\nSciBERT 0.75 土 0.04 0.95 土 0.02 0.84 土 0.03\nUMLS BERT 0.86 土 0.03 0.87 土 0.03 0.86 土 0.03\ni2b2 2008\nobesity\nBERT 0.64 土 0.01 0.58 土 0.01 0.61 土 0.01\nBioBERT 0.72 土 0.01 0.68 土 0.01 0.70 土 0.01\nBio-ClinicalBERT 0.69 土 0.01 0.71 土 0.01 0.70 土 0.01\nSciBERT 0.73 土 0.01 0.74 土 0.01 0.74 土 0.01\nUMLS BERT 0.74 土 0.01 0.70 土 0.01 0.72 土 0.01\nBias analysis on cohorts segregated by a single demographic dimension for age and gender were illustrated in\nFigure-1. For gender-based segregation, i2b2 2006 smoking dataset had ⅘models which were biased in favor of\nmale patients compared to female patients with a mean difference of nearly 6% in micro F1 value. The i2b2 2008\nobesity dataset had all models biased in favor of male patients compared to female patients with a mean difference\nof nearly 3.4% in micro F1 value. For age-based segregation, patients belonging to age groups 20-60 and 60-80 were\ncompared as both groups had a similar number of training records. The i2b2 2006 smoking dataset had ⅘models\nbiased in favor of patients aged 20-60 years as compared to the patients aged 60-80 with a mean difference of nearly\n4.8% in micro F1 value. In the case of the i2b2 2008 obesity dataset, a complete reversal of trend was observed with\n⅘models being biased against patients aged 20-60 years as compared to the patients aged 60-80 with a mean\ndifference of nearly 4.4% in micro F1 value.\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 25, 2023. ; https://doi.org/10.1101/2023.03.22.23287585doi: medRxiv preprint \nFigure 1: Bias Analysis across Age and Gender for Smoking & Obesity datasets. Micro F1-score of language\nmodels on Smoking and Obesity datasets w.r.t (A,B) Gender Distribution and (C,D) Age Distribution\nTable-3 demonstrates gender and age-wise segregated results for both the datasets for selected groups. Table-4\nshows the results for intersectional groups formed by groups depicted in Table-3. The intersectional groups were\ncreated for each dataset by combining demographic dimensions of gender and age to form four categories for each\ndataset - middle male, young female, middle female and old female for i2b2 2006 smoking and young male, old\nmale, young female and middle female for i2b2 2008 obesity where ‘young’ implies ages between 20-40, ‘middle’\nimplies ages between 40-60 and ‘old’ implies ages >60 years. These categories were chosen because they are the\nmost susceptible to smoking and obesity-related ailments [22–26]. It was observed that for i2b2 2006 smoking\ndataset ¾ intersectional subgroups - middle male, young female and old female - exhibited an amplification in bias\ncompared to individual demographic dimensions. On the contrary, the fourth intersectional subgroup (middle\nfemales) exhibited a reduction in bias compared to individual demographic dimensions. Similar trends were\nobserved for i2b2 2008 obesity dataset where ¾ intersectional subgroups - young male, young female and middle\nfemale - exhibited an amplification in bias. In contrast, the fourth intersectional subgroup (old male) exhibited a\ntrend reversal owing to a reduction in bias.\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 25, 2023. ; https://doi.org/10.1101/2023.03.22.23287585doi: medRxiv preprint \nTable 3: Segregated Results for smoking (left) & obesity (right) datasets via UMLS BERT\nDemographic\nDimension\nSubgroup\nName\nMicro\nF1-Score\nDemographic\nDimension\nSubgroup\nName\nMicro\nF1-Score\nAge\nYoung Aged 0.88 ± 0.08\nAge\nYoung Aged 0.67 ± 0.04\nMiddle Aged 0.92 ± 0.04 Middle Aged 0.7 ± 0.02\nOld Aged 0.83 ± 0.05 Old Aged 0.74 ± 0.02\nGender\nMale 0.87 ± 0.04\nGender\nMale 0.75 ± 0.01\nFemale 0.81 ± 0.05 Female 0.7 ± 0.01\nTable 4: Bias Amplification for Intersectional Groups via UMLS BERT\nDataset Category Name Intersectional Results\ni2b2 2006\nsmoking\nMiddle Male 0.76 ± 0.13\nYoung Female 0.8 ± 0.18\nMiddle Female 0.92 ± 0.08\nOld Female 0.75 ± 0.07\ni2b2 2008\nobesity\nYoung Male 0.54 ± 0.13\nOld Male 0.76 ± 0.02\nYoung Female 0.67 ± 0.04\nMiddle Female 0.69 ± 0.02\nDiscussion & Conclusion\nApplications of AI systems in clinical settings extend from critical care to triage leading to reduced clinical fatigue\n[27]. However, the systematic unfavouring of certain population subgroups by AI systems is an alarming cause of\nconcern because it delays and/or denies equitable access to healthcare facilities. In this work, bias analysis is\nconducted on clinical phenotyping tasks across two datasets - smoking and obesity. The study's primary objective is\nto highlight the role of language models in the systematic amplification of bias in clinical diagnosis between diverse\npopulation subgroups. Prior work on age and gender-based segregation for clinical notes [7] and images [6]\nhighlighted that the overrepresented (majority) group always had better metrics than the underrepresented group\n(minority). However, in this study, it was observed that despite having an identical number of training records, the\nmodels exhibited bias between subgroups. For gender-based segregation in both datasets, the models are always\nbiased in favor of male patients compared to female patients despite the training cohorts having similar counts of\nboth population subgroups. Therefore, for smoking and obesity-related comorbidities, clinicians need to be more\nobservant for female patients irrespective of the frequency of historical data. Similarly, a significant demarcation\nwas observed while assessing models across different age groups. The 20-60 age group is more susceptible to\nsmoking [25,26], and the high efficacy of the models across this group enables clinicians to make better decisions.\nHowever, a dent in the performance for the 60-80 age group further validates the idea that clinicians need to be more\nvigilant while prognosticating such vulnerable groups. Notably, for the obesity dataset, ⅘models showed the best\nefficacy for the 60-80 age group. Contrastingly, the models performed poorly for the age group >80 years. This age\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 25, 2023. ; https://doi.org/10.1101/2023.03.22.23287585doi: medRxiv preprint \ngroup is highly susceptible to obesity and also vulnerable because suffering from obesity in this age group might\nimply suffering from its associated comorbidities [22].\nIntersectional bias was analyzed by creating population subgroups using age and gender as the demographic\ndimensions. Prior literature [6] highlighted that the intersection of two underrepresented groups had a higher\nunderdiagnosis rate than the individual groups for image modality (chest radiographs). Subsequently, a study\ndemonstrated an increase in bias as the number of demographic dimensions increased for textual dataset [11]. Our\nfindings are very concerning from a clinical perspective as in majority cases intersectional subgroups exhibit an\namplification in bias. It suggested that for the smoking dataset, the most vulnerable group - middle-aged males\n[25,26] had shown a higher bias compared to only middle-aged patients or only male patients. Notably for the\nobesity dataset, the most vulnerable group - middle-aged females [23,24,28] showed a minor increase in bias.\nWhereas young males, though less vulnerable than females, showed a drastic increase in bias in comparison to only\nyoung patients or only male patients.Yet, there are certain subgroups which exhibited a trend reversal (reduction in\nbias). Therefore, the inference of this study is crucial for clinicians leveraging these state-of-the-art language models\nin clinical decision-making. Clinicians should be aware of the pitfalls of these language models and sensitive\ntowards subgroups that may be clinically misrepresented. Thus, they can make informed decisions and ensure\nfairness in the treatment of incoming patients. In prior studies on chest radiographs [6] and non-clinical text [11],\nunderrepresented population subgroups were the ones being underdiagnosed by AI systems. However, in our study,\nthe population subgroups that exhibited an amplification in bias by language models were not always\nunderrepresented in the training cohorts.\nOne key limitation of this study is that we have demonstrated the fairness of language models based on a single\nclinical outcome. For broader applicability and a robust implementation of models, bias analysis needs to be\nperformed across multiple tasks. Hence, in future work, we would like to extend this analysis to other clinical\nprognosticating outcomes such as ICU Readmission, ICU Length of Stay and ICU Mortality Prediction. The\nfindings of this study elucidate the need for proactive measures to identify and mitigate biases in Language Models\nused for clinical decision-making. As shown, intersectional biases can be amplified by state-of-the-art language\nmodels when applied to clinical notes for phenotyping. This can lead to potential inequities in healthcare access and\noutcomes for certain population subgroups. It highlights the need for continued research and development to ensure\nthat language models are not exacerbating existing biases in healthcare. Recognizing and addressing these biases is\nessential to ensure equitable healthcare for all individuals, regardless of their demographic characteristics. This\nasserts the development of bias-aware and explainable language models. Intervention measures can include (i)\nimproving data collection and annotation processes (ii) incorporating diverse perspectives of various stakeholders in\nthe development of language models (iii) augmenting training data to ensure representativeness of all population\ngroups (iv) develop algorithms to explicitly account for intersectionality of demographic dimensions (v) leverage\ninterpretability tools to address biases in real-time. The overarching message of this study is to use language models\nas a tool to augment clinicians’ decision-making process and improve patient outcomes. Furthermore, this should be\ndone in a manner that minimizes clinical burden, reduces the potential for bias and ensures fairness for all\nindividuals.\nAcknowledgements\nTavpritesh Sethi acknowledges support from the DBT Project BT/PR/34245/AI/133/9/2019.\nReferences\n1. Chetoui M, Akhloufi MA. Explainable Vision Transformers and Radiomics for COVID-19 Detection in Chest\nX-rays. J Clin Med. 2022 Jan;11(11):3013.\n2. Mayer T, Cabrio E, Villata S. Transformer-Based Argument Mining for Healthcare Applications. ECAI 2020.\n2020;2108–15.\n3. Shome D, Kar T, Mohanty SN, Tiwari P, Muhammad K, AlTameem A, et al. COVID-Transformer: Interpretable\nCOVID-19 Detection Using Vision Transformer for Healthcare. Int J Environ Res Public Health. 2021\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 25, 2023. ; https://doi.org/10.1101/2023.03.22.23287585doi: medRxiv preprint \nJan;18(21):11086.\n4. Shang J, Ma T, Xiao C, Sun J. Pre-training of Graph Augmented Transformers for Medication Recommendation\n[Internet]. arXiv; 2019 [cited 2023 Mar 14]. Available from: http://arxiv.org/abs/1906.00346\n5. Mattu JA Jeff Larson,Lauren Kirchner,Surya. Machine Bias [Internet]. ProPublica. [cited 2023 Mar 12].\nAvailable from: https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n6. Seyyed-Kalantari L, Zhang H, McDermott M, Chen IY , Ghassemi M. Underdiagnosis bias of artificial\nintelligence algorithms applied to chest radiographs in under-served patient populations | Nature Medicine\n[Internet]. [cited 2023 Feb 25]. Available from: https://www.nature.com/articles/s41591-021-01595-0\n7. Zhang H, Lu AX, Abdalla M, McDermott M, Ghassemi M. Hurtful Words: Quantifying Biases in Clinical\nContextual Word Embeddings [Internet]. arXiv; 2020 [cited 2023 Mar 12]. Available from:\nhttp://arxiv.org/abs/2003.11515\n8. Ogungbe O, Mitra AK, Roberts JK. A systematic review of implicit bias in health care: A call for\nintersectionality. IMC J Med Sci. 2019 Jun 29;13(1):005–005.\n9. Kearns M, Neel S, Roth A, Wu ZS. Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup\nFairness. In: Proceedings of the 35th International Conference on Machine Learning [Internet]. PMLR; 2018\n[cited 2023 Mar 12]. p. 2564–72. Available from: https://proceedings.mlr.press/v80/kearns18a.html\n10.Tan YC, Celis LE. Assessing Social and Intersectional Biases in Contextualized Word Representations [Internet].\narXiv; 2019 [cited 2023 Mar 12]. Available from: http://arxiv.org/abs/1911.01485\n11. Lalor J, Yang Y , Smith K, Forsgren N, Abbasi A. Benchmarking Intersectional Biases in NLP. In: Proceedings of\nthe 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies [Internet]. Seattle, United States: Association for Computational Linguistics; 2022 [cited\n2023 Mar 12]. p. 3598–609. Available from: https://aclanthology.org/2022.naacl-main.263\n12.Uzuner Ö, Goldstein I, Luo Y , Kohane I. Identifying Patient Smoking Status from Medical Discharge Records. J\nAm Med Inform Assoc JAMIA. 2008;15(1):14–24.\n13.Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding [Internet]. arXiv; 2019 [cited 2023 Mar 12]. Available from:\nhttp://arxiv.org/abs/1810.04805\n14.Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language representation\nmodel for biomedical text mining. Bioinformatics. 2020 Feb 15;36(4):1234–40.\n15.Alsentzer E, Murphy JR, Boag W, Weng WH, Jin D, Naumann T, et al. Publicly Available Clinical BERT\nEmbeddings [Internet]. arXiv; 2019 [cited 2023 Mar 12]. Available from: http://arxiv.org/abs/1904.03323\n16. Johnson AEW, Pollard TJ, Shen L, Lehman LH, Feng M, Ghassemi M, et al. MIMIC-III, a freely accessible\ncritical care database | Scientific Data [Internet]. [cited 2023 Mar 14]. Available from:\nhttps://www.nature.com/articles/sdata201635\n17.Beltagy I, Lo K, Cohan A. SciBERT: A Pretrained Language Model for Scientific Text [Internet]. arXiv; 2019\n[cited 2023 Mar 12]. Available from: http://arxiv.org/abs/1903.10676\n18.Michalopoulos G, Wang Y , Kaka H, Chen H, Wong A. UmlsBERT: Clinical Domain Knowledge Augmentation\nof Contextual Embeddings Using the Unified Medical Language System Metathesaurus [Internet]. arXiv; 2021\n[cited 2023 Mar 12]. Available from: http://arxiv.org/abs/2010.10391\n19.Adhikari A, Ram A, Tang R, Lin J. DocBERT: BERT for Document Classification [Internet]. arXiv; 2019 [cited\n2023 Mar 12]. Available from: http://arxiv.org/abs/1904.08398\n20.Mulyar A, Schumacher E, Rouhizadeh M, Dredze M. Phenotyping of Clinical Notes with Improved Document\nClassification Models Using Contextualized Neural Language Models [Internet]. arXiv; 2020 [cited 2023 Mar\n12]. Available from: http://arxiv.org/abs/1910.13664\n21.Wolf T, Debut L, Sanh V , Chaumond J, Delangue C, Moi A, et al. HuggingFace’s Transformers: State-of-the-art\nNatural Language Processing [Internet]. arXiv; 2020 [cited 2023 Mar 12]. Available from:\nhttp://arxiv.org/abs/1910.03771\n22.Profile of Obesity and Comorbidities in Elderly Patients with Heart Failure - PMC [Internet]. [cited 2023 Mar\n18]. Available from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7184119/\n23.Obesity and overweight [Internet]. [cited 2023 Mar 13]. Available from:\nhttps://www.who.int/news-room/fact-sheets/detail/obesity-and-overweight\n24.Kapoor N, Arora S, Kalra S. Gender Disparities in People Living with Obesity - An Unchartered Territory. J\n-Life Health. 2021;12(2):103–7.\n25.Abuse NI on D. Are there gender differences in tobacco smoking? [Internet]. National Institute on Drug Abuse.\n-- [cited 2023 Mar 13]. Available from:\nhttps://nida.nih.gov/publications/research-reports/tobacco-nicotine-e-cigarettes/are-there-gender-differences-in-to\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 25, 2023. ; https://doi.org/10.1101/2023.03.22.23287585doi: medRxiv preprint \nbacco-smoking\n26.Nooyens ACJ, van Gelder BM, Verschuren WMM. Smoking and Cognitive Decline Among Middle-Aged Men\nand Women: The Doetinchem Cohort Study. Am J Public Health. 2008 Dec;98(12):2244–50.\n27.Ciecierski-Holmes T, Singh R, Axt M, Brenner S, Barteit S. Artificial intelligence for strengthening healthcare\nsystems in low- and middle-income countries: a systematic scoping review | npj Digital Medicine [Internet].\n[cited 2023 Mar 18]. Available from: https://www.nature.com/articles/s41746-022-00700-y\n28.Kautzky-Willer A, Harreiter J, Pacini G. Sex and Gender Differences in Risk, Pathophysiology and\nComplications of Type 2 Diabetes Mellitus. Endocr Rev. 2016 Jun;37(3):278–316.\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 25, 2023. ; https://doi.org/10.1101/2023.03.22.23287585doi: medRxiv preprint ",
  "topic": "Obesity",
  "concepts": [
    {
      "name": "Obesity",
      "score": 0.5119248628616333
    },
    {
      "name": "Disease",
      "score": 0.5093132257461548
    },
    {
      "name": "Medicine",
      "score": 0.42834174633026123
    },
    {
      "name": "Key (lock)",
      "score": 0.4142639636993408
    },
    {
      "name": "Psychology",
      "score": 0.3448585271835327
    },
    {
      "name": "Pathology",
      "score": 0.2050287127494812
    },
    {
      "name": "Computer science",
      "score": 0.19518235325813293
    },
    {
      "name": "Computer security",
      "score": 0.1654188334941864
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I119939252",
      "name": "Indraprastha Institute of Information Technology Delhi",
      "country": "IN"
    }
  ]
}