{
  "title": "Dynamic Language Models for Streaming Text",
  "url": "https://openalex.org/W2151835407",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A258659784",
      "name": "Dani Yogatama",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2100637705",
      "name": "Chong Wang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A214866942",
      "name": "Bryan R. Routledge",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2183947846",
      "name": "Noah A. Smith",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A351197510",
      "name": "Eric P. Xing",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2024081693",
    "https://openalex.org/W2069317438",
    "https://openalex.org/W2171911691",
    "https://openalex.org/W4292022450",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2166851633",
    "https://openalex.org/W2072644219",
    "https://openalex.org/W1915315806",
    "https://openalex.org/W2120340025",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2171458318",
    "https://openalex.org/W2177801600",
    "https://openalex.org/W1947594277",
    "https://openalex.org/W1508165687",
    "https://openalex.org/W2077723394",
    "https://openalex.org/W2051434435",
    "https://openalex.org/W1570963478",
    "https://openalex.org/W2099111195",
    "https://openalex.org/W2594297742",
    "https://openalex.org/W2142889507",
    "https://openalex.org/W2187741934",
    "https://openalex.org/W1983599491",
    "https://openalex.org/W2100802943",
    "https://openalex.org/W2148825261",
    "https://openalex.org/W2164301055",
    "https://openalex.org/W129590842",
    "https://openalex.org/W2044062612",
    "https://openalex.org/W2165279024"
  ],
  "abstract": "We present a probabilistic language model that captures temporal dynamics and conditions on arbitrary non-linguistic context features. These context features serve as important indicators of language changes that are otherwise difficult to capture using text data by itself. We learn our model in an efficient online fashion that is scalable for large, streaming data. With five streaming datasets from two different genres—economics news articles and social media—we evaluate our model on the task of sequential language modeling. Our model consistently outperforms competing models.",
  "full_text": "Dynamic Language Models for Streaming Text\nDani Yogatama∗ Chong Wang∗ Bryan R. Routledge† Noah A. Smith∗ Eric P. Xing∗\n∗School of Computer Science\n†Tepper School of Business\nCarnegie Mellon University\nPittsburgh, PA 15213, USA\n∗{dyogatama,chongw,nasmith,epxing}@cs.cmu.edu, †routledge@cmu.edu\nAbstract\nWe present a probabilistic language model that\ncaptures temporal dynamics and conditions on\narbitrary non-linguistic context features. These\ncontext features serve as important indicators\nof language changes that are otherwise difﬁcult\nto capture using text data by itself. We learn\nour model in an efﬁcient online fashion that is\nscalable for large, streaming data. With ﬁve\nstreaming datasets from two different genres—\neconomics news articles and social media—we\nevaluate our model on the task of sequential\nlanguage modeling. Our model consistently\noutperforms competing models.\n1 Introduction\nLanguage models are a key component in many NLP\napplications, such as machine translation and ex-\nploratory corpus analysis. Language models are typi-\ncally assumed to be static—the word-given-context\ndistributions do not change over time. Examples\ninclude n-gram models (Jelinek, 1997) and proba-\nbilistic topic models like latent Dirichlet allocation\n(Blei et al., 2003); we use the term “language model”\nto refer broadly to probabilistic models of text.\nRecently, streaming datasets (e.g., social media)\nhave attracted much interest in NLP. Since such data\nevolve rapidly based on events in the real world, as-\nsuming a static language model becomes unrealistic.\nIn general, more data is seen as better, but treating all\npast data equally runs the risk of distracting a model\nwith irrelevant evidence. On the other hand, cau-\ntiously using only the most recent data risks overﬁt-\nting to short-term trends and missing important time-\ninsensitive effects (Blei and Lafferty, 2006; Wang\net al., 2008). Therefore, in this paper, we take steps\ntoward methods for capturing long-range temporal\ndynamics in language use.\nOur model also exploits observable context vari-\nables to capture temporal variation that is otherwise\ndifﬁcult to capture using only text. Speciﬁcally for\nthe applications we consider, we use stock market\ndata as exogenous evidence on which the language\nmodel depends. For example, when an important\ncompany’s price moves suddenly, the language model\nshould be based not on the very recent history, but\nshould be similar to the language model for a day\nwhen a similar change happened, since people are\nlikely to say similar things (either about that com-\npany, or about conditions relevant to the change).\nNon-linguistic contexts such as stock price changes\nprovide useful auxiliary information that might indi-\ncate the similarity of language models across differ-\nent timesteps.\nWe also turn to a fully online learning framework\n(Cesa-Bianchi and Lugosi, 2006) to deal with non-\nstationarity and dynamics in the data that necessitate\nadaptation of the model to data in real time. In on-\nline learning, streaming examples are processed only\nwhen they arrive. Online learning also eliminates\nthe need to store large amounts of data in memory.\nStrictly speaking, online learning is distinct from\nstochastic learning, which for language models built\non massive datasets has been explored by Hoffman\net al. (2013) and Wang et al. (2011). Those tech-\nniques are still for static modeling. Language model-\ning for streaming datasets in the context of machine\ntranslation was considered by Levenberg and Os-\nborne (2009) and Levenberg et al. (2010). Goyal\net al. (2009) introduced a streaming algorithm for\nlarge scale language modeling by approximating n-\ngram frequency counts. We propose a general online\nlearning algorithm for language modeling that draws\ninspiration from regret minimization in sequential\npredictions (Cesa-Bianchi and Lugosi, 2006) and on-\n181\nTransactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier.\nSubmitted 10/2013; Revised 2/2014; Published 4/2014.c⃝2014 Association for Computational Linguistics.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00175 by guest on 05 November 2025\nline variational algorithms (Sato, 2001; Honkela and\nValpola, 2003).\nTo our knowledge, our model is the ﬁrst to bring\ntogether temporal dynamics, conditioning on non-\nlinguistic context, and scalable online learning suit-\nable for streaming data and extensible to include\ntopics and n-gram histories. The main idea of our\nmodel is independent of the choice of the base lan-\nguage model (e.g., unigrams, bigrams, topic models,\netc.). In this paper, we focus on unigram and bi-\ngram language models in order to evaluate the basic\nidea on well understood models, and to show how it\ncan be extended to higher-order n-grams. We leave\nextensions to topic models for future work.\nWe propose a novel task to evaluate our proposed\nlanguage model. The task is to predict economics-\nrelated text at a given time, taking into account the\nchanges in stock prices up to the corresponding day.\nThis can be seen an inverse of the setup considered\nby Lavrenko et al. (2000), where news is assumed\nto inﬂuence stock prices. We evaluate our model\non economics news in various languages (English,\nGerman, and French), as well as Twitter data.\n2 Background\nIn this section, we ﬁrst discuss the background for\nsequential predictions then describe how to formulate\nonline language modeling as sequential predictions.\n2.1 Sequential Predictions\nLet w1,w2,...,w T be a sequence of response vari-\nables, revealed one at a time. The goal is to design\na good learner to predict the next response, given\nprevious responses and additional evidence which\nwe denote by xt ∈RM (at time t). Throughout this\npaper, we use the term features for x. Speciﬁcally, at\neach round t, the learner receives xt and makes a pre-\ndiction ˆwt, by choosing a parameter vectorαt ∈RM.\nIn this paper, we refer to αas feature coefﬁcients.\nThere has been an enormous amount of work on\nonline learning for sequential predictions, much of it\nbuilding on convex optimization. For a sequence of\nloss functions ℓ1,ℓ2,...,ℓ T (parameterized by α),\nan online learning algorithm is a strategy to minimize\nthe regret, with respect to the best ﬁxed α∗in hind-\nsight.1 Regret guarantees assume a Lipschitz con-\n1Formally, the regret is deﬁned as RegretT (α∗) =\ndition on the loss function ℓthat can be prohibitive\nfor complex models. See Cesa-Bianchi and Lugosi\n(2006), Rakhlin (2009), Bubeck (2011), and Shalev-\nShwartz (2012) for in-depth discussion and review.\nThere has also been work on online and stochastic\nlearning for Bayesian models (Sato, 2001; Honkela\nand Valpola, 2003; Hoffman et al., 2013), based on\nvariational inference. The goal is to approximate pos-\nterior distributions of latent variables when examples\narrive one at a time.\nIn this paper, we will use both kinds of techniques\nto learn language models for streaming datasets.\n2.2 Problem Formulation\nConsider an online language modeling problem, in\nthe spirit of sequential predictions. The task is to\nbuild a language model that accurately predicts the\ntexts generated on day t, conditioned on observ-\nable features up to day t, x1:t. Every day, after\nthe model makes a prediction, the actual texts wt\nare revealed and we suffer a loss. The loss is de-\nﬁned as the negative log likelihood of the model\nℓt = −log p(wt |α,β1:t−1,x1:t−1,n1:t−1), where\nαand β1:T are the model parameters andnis a back-\nground distribution (details are given in §3.2). We\ncan then update the model and proceed to day t+ 1.\nNotice the similarity to the sequential prediction de-\nscribed above. Importantly, this is a realistic setup for\nbuilding evolving language models from large-scale\nstreaming datasets.\n3 Model\n3.1 Notation\nWe index timesteps by t ∈ {1,...,T }and word\ntypes by v ∈{1,...,V }, both are always given as\nsubscripts. We denote vectors in boldface and use\n1 : T as a shorthand for {1,2,...,T }. We assume\nwords of the form {wt}T\nt=1 for wt ∈RV, which is\nthe vector of word frequences at timetstep t. Non-\nlinguistic context features are {xt}T\nt=1 for xt ∈RM.\nThe goal is to learn parameters αand β1:T, which\nwill be described in detail next.\n3.2 Generative Story\nThe main idea of our model is illustrated by the fol-\nlowing generative story for the unigram language\nPT\nt=1 ℓt(xt,αt,wt) −infα∗\nPT\nt=1 ℓt(xt,α∗,wt).\n182\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00175 by guest on 05 November 2025\nmodel. (We will discuss the extension to higher-order\nlanguage models later.) A graphical representation\nof our proposed model is given in Figure 1.\n1. Draw feature coefﬁcients α∼N(0,λI).2 Here\nαis a vector in RM, where M is the dimension-\nality of the feature vector.\n2. For each timestep t:\n(a) Observe non-linguistic context features xt.\n(b) Draw βt ∼\nN\n(∑t−1\nk=1 δk\nexp(α⊤f(xt,xk))Pt−1\nj=1 δj exp(α⊤f(xt,xj)) βk,ϕI\n)\n.\nHere, βt is a vector in RV, where V is\nthe size of the word vocabulary, ϕ is\nthe variance parameter and δk is a ﬁxed\nhyperparameter; we discuss them below.\n(c) For each word wt,v, draw wt,v ∼\nCategorical\n(\nexp(n1:t−1,v+βt,v)P\nj∈V exp(n1:t−1,j+βt,j)\n)\n.\nIn the last step, βt and n are mapped to the V-\ndimensional simplex, forming a distribution over\nwords. n1:t−1 ∈RV is a background (log) distri-\nbution, inspired by a similar idea in Eisenstein et al.\n(2011). In this paper, we set n1:t−1,v to be the log-\nfrequency of vup to time t−1. We can interpret β\nas a time-dependent deviation from the background\nlog-frequencies that incorporates world-context. This\ndeviation comes in the form of a weighted average of\nearlier deviation vectors.\nThe intuition behind the model is that the probabil-\nity of a word appearing at day tdepends on the back-\nground log-frequencies, the deviation coefﬁcients of\nthe word at previous timesteps β1:t−1, and the sim-\nilarity of current conditions of the world (based on\nobservable features x) to previous timesteps through\nf(xt,xk). That is, f is a function that takes d-\ndimensional feature vectors at two timesteps xt and\nxk and returns a similarity vector f(xt,xk) ∈RM\n(see §6.1.1 for an example of f that we use in our\nexperiments). The similarity is parameterized by α,\nand decays over time with rate δk. In this work, we\nassume a ﬁxed window size c (i.e., we consider c\nmost recent timesteps), so that δ1:t−c−1 = 0 and\nδt−c:t−1 = 1. This allows up to cth order depen-\ndencies.3 Setting δthis way allows us to bound the\n2Feature coefﬁcients αcan be also drawn from other distri-\nbutions such as α∼Laplace(0,λ).\n3In online Bayesian learning, it is known that forgetting\ninaccurate estimates from earlier timesteps is important (Sato,\n\u0000\nx tx sx rx q\nw q w r\nw s w t\n\u0000\nt\n\u0000\ns\n\u0000\nr\n\u0000 q\n↵\nN rN q N s\nN t\nT\nFigure 1: Graphical representation of the model. The\nsubscript indices q,r,s are shorthands for the previ-\nous timesteps t−3,t −2,t −1. Only four timesteps\nare shown here. There are arrows from previous\nβt−4,βt−5,..., βt−c to βt, where cis the window size\nas described in §3.2. They are not shown here, for read-\nability.\nnumber of past vectors β that need to be kept in\nmemory. We set β0 to 0.\nAlthough the generative story described above\nis for unigram language models, extensions can be\nmade to more complex models (e.g., mixture of un-\nigrams, topic models, etc.) and to longer n-gram\ncontexts. In the case of topic models, the model\nwill be related to dynamic topic models (Blei and\nLafferty, 2006) augmented by context features, and\nthe learning procedure in §4 can be used to perform\nonline learning of dynamic topic models. However,\nour model captures longer-range dependencies than\ndynamic topic models, and can condition on non-\nlinguistic features or metadata. In the case of higher-\norder n-grams, one simple way is to draw more β,\none for each history. For example, for a bigram\nmodel, βis in RV2\n, rather than RV in the unigram\nmodel. We consider both unigram and bigram lan-\nguage models in our experiments in§6. However, the\nmain idea presented in this paper is largely indepen-\ndent of the base model.\nRelated work. Mimno and McCallum (2008) and\nEisenstein et al. (2010) similarly conditioned text on\n2001; Honkela and Valpola, 2003). Since we set δ1:t−c−1 = 0,\nat every timestep t, δk leads to forgetting older examples.\n183\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00175 by guest on 05 November 2025\nobservable features (e.g., author, publication venue,\ngeography, and other document-level metadata), but\nconducted inference in a batch setting, thus their ap-\nproaches are not suitable for streaming data. It is not\nimmediately clear how to generalize their approach to\ndynamic settings. Algorithmically, our work comes\nclosest to the online dynamic topic model of Iwata\net al. (2010), except that we also incorporate context\nfeatures.\n4 Learning and Inference\nThe goal of the learning procedure is to minimize the\noverall negative log likelihood,\n−log L(D) =\n−log\n∫\ndβ1:Tp(β1:T |α,x1:T)p(w1:T |β1:T,n).\nHowever, this quantity is intractable. Instead, we\nderive an upper bound for this quantity and minimize\nthat upper bound. Using Jensen’s inequality, the vari-\national upper bound on the negative log likelihood\nis:\n−log L(D) ≤−\n∫\ndβ1:Tq(β1:T |γ1:T) (4)\nlog p(β1:T |α,x1:T)p(w1:T |β1:T,n)\nq(β1:T |γ1:T) .\nSpeciﬁcally, we use mean-ﬁeld variational inference\nwhere the variables in the variational distribution q\nare completely independent. We use Gaussian distri-\nbutions as our variational distributions forβ, denoted\nby γin the bound in Eq. 4. We denote the parameters\nof the Gaussian variational distribution forβt,v (word\nvat timestep t) by µt,v (mean) and σt,v (variance).\nFigure 2 shows the functional form of the varia-\ntional bound that we seek to minimize, denoted by ˆB.\nThe two main steps in the optimization of the bound\nare inferring βt and updating feature coefﬁcients α.\nWe next describe each step in detail.\n4.1 Learning\nThe goal of the learning procedure is to minimize the\nupper bound in Figure 2 with respect to α. However,\nsince the data arrives in an online fashion, and speed\nis very important for processing streaming datasets,\nthe model needs to be updated at every timestep t(in\nour experiments, daily).\nNotice that at timestep t, we only have access\nto x1:t and w1:t, and we perform learning at every\ntimestep after the text for the current timestep wt\nis revealed. We do not know xt+1:T and wt+1:T.\nNonetheless, we want to update our model so that\nit can make a better prediction at t+ 1. Therefore,\nwe can only minimize the bound until timestep t.\nLet Ck ≜ exp(α⊤f(xt,xk))Pt−1\nj=t−c exp(α⊤f(xt,xj)) . Our learning al-\ngorithm is a variational Expectation-Maximization\nalgorithm (Wainwright and Jordan, 2008).\nE-step Recall that we use variational inference and\nthe variational parameters for βare µand σ. As\nshown in Figure 2, since the log-sum-exp in the last\nterm of B is problematic, we introduce additional\nvariational parameters ζ to simplify B and obtain\nˆB (Eqs. 2–3). The E-step deals with all the local\nvariables µ, σ, and ζ.\nFixing other variables and taking the derivative\nof the bound ˆB w.r.t. ζt and setting it to zero,\nwe obtain the closed-form update for ζt: ζt =∑\nv∈V exp (n1:t−1,v) exp\n(\nµt,v + σt,v\n2\n)\n.\nTo minimize with respect to µt and σt, we apply\ngradient-based methods since there are no closed-\nform solutions. The derivative w.r.t.µt,v is:\n∂ˆB\n∂µt,v\n=µt,v −Ckµk,v\nϕ\n−nt,v + nt\nζt\nexp (n1:t−1,v) exp\n(\nµt,v + σt,v\n2\n)\n,\nwhere nt = ∑\nv∈V nt,v.\nThe derivative w.r.t.σt,v is:\n∂ˆB\n∂σt,v\n= 1\n2σt,v\n+ 1\n2ϕ + nt\n2ζt\nexp (n1:t−1,v) exp\n(\nµt,v + σt,v\n2\n)\n.\nAlthough we require iterative methods in the E-step,\nwe ﬁnd it to be reasonably fast in practice.4 Speciﬁ-\ncally, we use the L-BFGS quasi-Newton algorithm\n(Liu and Nocedal, 1989).\nWe can further improve the bound by updating\nthe variational parameters for timestep 1 :t−1, i.e.,\nµ1:t−1 and σ1:t−1, as well. However, this will require\nstoring the texts from previous timesteps. Addition-\nally, this will complicate the M-step update described\n4Approximately 16.5 seconds/day (walltime) to learn the\nmodel on the EN:NA dataset on a 2.40GHz CPU with 24GB\nmemory.\n184\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00175 by guest on 05 November 2025\nB = −\nT∑\nt=1\nEq[log p(βt |βk,α,xt)] −\nT∑\nt=1\nEq[log p(wt |βt,nt)] −H(q) (1)\n=\nT∑\nt=1\n\n\n\n1\n2\n∑\nj∈V\nlog σt,j\nϕ −Eq\n\n−\n(\nβt −∑t−1\nk=t−c Ckβk\n)2\n2ϕ\n\n−Eq\n\n∑\nv∈wt\nn1:t−1,v + βt,v −log\n∑\nj∈V\nexp(n1:t−1,j + βt,j)\n\n\n\n\n\n(2)\n≤\nT∑\nt=1\n\n\n\n1\n2\n∑\nj∈V\nlog σt,v\nϕ +\n(\nµt −∑t−1\nk=t−c Ckµk\n)2\n2ϕ + σt + ∑t−1\nk=t−c C2\nkσk\n2ϕ\n−\n∑\nv∈wt\n\nµt,v −log ζt −1\nζt\n∑\nj∈V\nexp (n1:t−1,j) exp\n(\nµt,j + σt,j\n2\n)\n\n\n\n\n\n+ const (3)\nFigure 2: The variational bound that we seek to minimize, B. H(q) is the entropy of the variational distribution q. The\nderivation from line 1 to line 2 is done by replacing the probability distributions p(βt |βk,α,xt) and p(wt |βt,nt)\nby their respective functional forms. Notice that in line 3 we compute the expectations under the variational distributions\nand further bound B by introducing additional variational parameters ζusing Jensen’s inequality on the log-sum-exp in\nthe last term. We denote the new bound ˆB.\nbelow. Therefore, for each s < t, we choose to ﬁx\nµs and σs once they are learned at timestep s.\nM-step In the M-step, we update the global pa-\nrameter α, ﬁxing µ1:t. Fixing other parameters and\ntaking the derivative of ˆB w.r.t. α, we obtain:5\n∂ˆB\n∂α =(µt −∑t−1\nk=t−cCkµk)(−∑t−1\nk=t−c\n∂Ck\n∂α )\nϕ\n+\n∑t−1\nk=t−cCkσk\n∂Ck\n∂α\nϕ ,\nwhere:\n∂Ck\n∂α =Ckf(xt,xk)\n−Ck\n∑t−1\ns=t−cf(xt,xs) exp(α⊤f(xt,xs))\n∑t−1\ns=t−cexp(α⊤f(xt,xs))\n.\nWe follow the convex optimization strategy and sim-\nply perform a stochastic gradient update: αt+1 =\nαt + ηt ∂ˆB\n∂αt (Zinkevich, 2003). While the variational\nbound ˆB is not convex, given the local variables µ1:t\n5In our implementation, we augment αwith a squared L2\nregularization term (i.e., we assume that αis drawn from a\nnormal distribution with mean zero and variance λ) and use the\nFOBOS algorithm (Duchi and Singer, 2009). The derivative\nof the regularization term is simple and is not shown here. Of\ncourse, other regularizers (e.g., the L1-norm, which we use for\nother parameters, or the L1/∞-norm) can also be explored.\nand σ1:t, optimizing αat timestep twithout know-\ning the future becomes a convex problem. 6 Since\nwe do not reestimate µ1:t−1 and σ1:t−1 in the E-step,\nthe choice to perform online gradient descent instead\nof iteratively performing batch optimization at every\ntimestep is theoretically justiﬁed.\nNotice that our overall learning procedure is still\nto minimize the variational upper bound ˆB. All these\nchoices are made to make the model suitable for\nlearning in real time from large streaming datasets.\nPreliminary experiments showed that performing\nmore than one EM iteration per day does not consid-\nerably improve performance, so in our experiments\nwe perform one EM iteration per day.\nTo learn the parameters of the model, we rely on\napproximations and optimize an upper bound ˆB. We\nhave opted for this approach over alternatives (such\nas MCMC methods) because of our interest in the\nonline, large-data setting. Our experiments show that\nwe are still able to learn reasonable parameter esti-\nmates by optimizing ˆB. Like online variational meth-\nods for other latent-variable models such as LDA\n(Sato, 2001; Hoffman et al., 2013), open questions re-\nmain about the tightness of such approximations and\nthe identiﬁability of model parameters. We note, how-\n6As a result, our algorithm is Hannan consistent w.r.t. the\nbest ﬁxed α(for ˆB) in hindsight; i.e., the average regret goes to\nzero as T goes to ∞.\n185\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00175 by guest on 05 November 2025\never, that our model does not include latent mixtures\nof topics and may be generally easier to estimate.\n5 Prediction\nAs described in §2.2, our model is evaluated by the\nloss suffered at every timestep, where the loss is\ndeﬁned as the negative log likelihood of the model\non text at timestep wt. Therefore, at each timestep t,\nwe need to predict (the distribution of) wt. In order\nto do this, for each word v∈V, we simply compute\nthe deviation means βt,v as weighted combinations\nof previous means, where the weights are determined\nby the world-context similarity encoded in x:\nEq[βt,v |µt,v] =\nt−1∑\nk=t−c\nexp(α⊤f(xt,xk))∑t−1\nj=t−c exp(α⊤f(xt,xj))\nµk,v.\nRecall that the word distribution that we use for\nprediction is obtained by applying the operator π\nthat maps βt and nto the V-dimensional simplex,\nforming a distribution over words: π(βt,n1:t−1)v =\nexp(n1:t−1,v+βt,v)P\nj∈V exp(n1:t−1,j+βt,j) , where n1:t−1,v ∈ RV is a\nbackground distribution (the log-frequency of word\nvobserved up to time t−1).\n6 Experiments\nIn our experiments, we consider the problem of pre-\ndicting economy-related text appearing in news and\nmicroblogs, based on observable features that reﬂect\ncurrent economic conditions in the world at a given\ntime. In the following, we describe our dataset in de-\ntail, then show experimental results on text prediction.\nIn all experiments, we set the window sizec= 7(one\nweek) or c = 14 (two weeks), λ = 1\n2|V| (V is the\nsize of vocabulary of the dataset under consideration),\nand ϕ= 1.\n6.1 Dataset\nOur data contains metadata and text corpora. The\nmetadata is used as our features, whereas the text\ncorpora are used for learning language models and\npredictions. The dataset (excluding Twitter) can\nbe downloaded at http://www.ark.cs.cmu.\nedu/DynamicLM.\n6.1.1 Metadata\nWe use end-of-day stock prices gathered from\nfinance.yahoo.com for each stock included in\nthe Standard & Poor’s 500 index (S&P 500). The\nindex includes large (by market value) companies\nlisted on US stock exchanges. 7 We calculate daily\n(continuously compounded) returns for each stock, o:\nro,t = logPo,t−log Po,t−1, where Po,t is the closing\nstock price.8 We make a simplifying assumption that\ntext for day t is generated after Po,t is observed.9\nIn general, stocks trade Monday to Friday (except\nfor federal holidays and natural disasters). For days\nwhen stocks do not trade, we set ro,t = 0 for all\nstocks since any price change is not observed.\nWe transform returns into similarity values as fol-\nlows: f(xo,t,xo,k) = 1 iff sign(ro,t) = sign(ro,k)\nand 0 otherwise. While this limits the model by ig-\nnoring the magnitude of price changes, it is still rea-\nsonable to capture the similarity between two days.10\nThere are 500 stocks in the S&P 500, so xt ∈R500\nand f(xt,xk) ∈R500.\n6.1.2 Text data\nWe have ﬁve streams of text data. The ﬁrst four\ncorpora are news streams tracked through Reuters.11\nTwo of them are written in English, North American\nBusiness Report (EN:NA) and Japanese Investment\nNews (EN:JP). The remaining two are German Eco-\nnomic News Service (DE, in German) and French\nEconomic News Service (FR, in French). For all four\nof the Reuters streams, we collected news data over\na period of thirteen months (392 days), 2012-05-26\nto 2013-06-21. See Table 1 for descriptive statistics\nof these datasets. Numerical terms are mapped to a\nsingle word, and all letters are downcased.\nThe last text stream comes from the Deca-\nhose/Gardenhose stream from Twitter. We collected\npublic tweets that contain ticker symbols (i.e., sym-\nbols that are used to denote stocks of a particular\ncompany in a stock market), preceded by the dollar\n7For a list of companies listed in the S&P 500 as of\n2012, see http://en.wikipedia.org/wiki/List_\nof_S\\%26P_500_companies. This set was ﬁxed during\nthe time periods of all our experiments.\n8We use the “adjusted close” on Yahoo that includes interim\ndividend cash ﬂows and also adjusts for “splits” (changes in the\nnumber of outstanding shares).\n9This is done in order to avoid having to deal with hourly\ntimesteps. In addition, intraday price data is only available\nthrough commercial data provided.\n10Note that daily stock returns are equally likely to be positive\nor negative and display little serial correlation.\n11http://www.reuters.com\n186\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00175 by guest on 05 November 2025\nDataset Total # Doc. Avg. # Doc. #Days Unigrams Bigrams\nTotal # Tokens Size Vocab. Total # Tokens Size Vocab.\nEN:NA 86,683 223 392 28,265,550 10,000 11,804,201 5,000\nEN:JP 70.807 182 392 16,026,380 10,000 7,047,095 5,000\nFR 62,355 160 392 11,942,271 10,000 3,773,517 5,000\nDE 51,515 132 392 9,027,823 10,000 3,499,965 5,000\nTwitter 214,794 336 639 1,660,874 10,000 551,768 5,000\nTable 1: Statistics about the datasets. Average number of documents (third column) is per day.\nsign $ (e.g., $GOOG, $MSFT, $AAPL, etc.). These\ntags are generally used to indicate tweets about the\nstock market. We look at tweets from the period\n2011-01-01 to 2012-09-30 (639 days). As a result,\nwe have approximately 100–800 tweets per day. We\ntokenized the tweets using the CMU ARK TweetNLP\ntools,12 numerical terms are mapped to a single word,\nand all letters are downcased.\nWe perform two experiments using unigram and\nbigram language models as the base models. For\neach dataset, we consider the top 10,000 unigrams\nafter removing corpus-speciﬁc stopwords (the top\n100 words with highest frequencies). For the bigram\nexperiments, we only use 5,000 words to limit the\nnumber of unique bigrams so that we can simulate\nexperiments for the entire time horizon in a reason-\nable amount of time. In standard open-vocabulary\nlanguage modeling experiments, the treatment of un-\nknown words deserves care. We have opted for a\ncontrolled, closed-vocabulary experiment, since stan-\ndard smoothing techniques will almost surely interact\nwith temporal dynamics and context in interesting\nways that are out of scope in the present work.\n6.2 Baselines\nSince this is a forecasting task, at each timestep, we\nonly have access to data from previous timesteps.\nOur model assumes that all words in all documents\nin a corpus come from a single multinomial distri-\nbution. Therefore, we compare our approach to the\ncorresponding base models (standard unigram and bi-\ngram language models) over the same vocabulary (for\neach stream). The ﬁrst one maintains counts of every\nword and updates the counts at each timestep. This\ncorresponds to a base model that uses all of the avail-\nable data up to the current timestep (“base all”). The\nsecond one replaces counts of every word with the\n12https://www.ark.cs.cmu.edu/TweetNLP\ncounts from the previous timestep (“base one”). Ad-\nditionally, we also compare with a base model whose\ncounts decay exponentially (“base exp”). That is, the\ncounts from previous timesteps decay by exp(−γs),\nwhere sis the distance between previous timesteps\nand the current timestep and γis the decay constant.\nWe set the decay constantγ = 1. We put a symmetric\nDirichlet prior on the counts (“add-one” smoothing);\nthis is analogous to our treatment of the background\nfrequencies nin our model. Note that our model,\nsimilar to “base all,” uses all available data up to\ntimestep t−1 when making predictions for timestep\nt. The window size conly determines which previ-\nous timesteps’ models can be chosen for making a\nprediction today. The past models themselves are es-\ntimated from all available data up to their respective\ntimesteps.\nWe also compare with two strong baselines: a lin-\near interpolation of “base one” models for the past\nweek (“int. week”) and a linear interpolation of “base\nall” and “base one” (“int one all”). The interpolation\nweights are learned online using the normalized expo-\nnentiated gradient algorithm (Kivinen and Warmuth,\n1997), which has been shown to enjoy a stronger\nregret guarantee compared to standard online gra-\ndient descent for learning a convex combination of\nweights.\n6.3 Results\nWe evaluate the perplexity on unseen dataset to eval-\nuate the performance of our model. Speciﬁcally, we\nuse per-word predictive perplexity:\nperplexity = exp\n(\n−\n∑T\nt=1 log p(wt |α,x1:t,n1:t−1)\n∑T\nt=1\n∑\nj∈V wt,j\n)\n.\nNote that the denominator is the number of tokens\nup to timestep T. Lower perplexity is better.\nTable 2 and Table 3 show the perplexity results for\n187\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00175 by guest on 05 November 2025\nDataset base all base one base exp int. week int. one all c= 7 c= 14\nEN:NA 3,341 3,677 3,486 3,403 3,271 3,262 3,285\nEN:JP 2,802 3,212 2,750 2,949 2,708 2,656 2,689\nFR 3,603 3,910 3,678 3,625 3,416 3,404 3,438\nDE 3,789 4,199 3,979 3,926 3,634 3,649 3,687\nTwitter 3,880 6,168 5,133 5,859 4,047 3,801 3,819\nTable 2: Perplexity results for our ﬁve data streams in the unigram experiments. The base models in “base all,” “base\none,” and “base exp” are unigram language models. “int. week” is a linear interpolation of “base one” from the past\nweek. “int. one all” is a linear interpolation of “base one” and “base all”. The rightmost two columns are versions of\nour model. Best results are highlighted in bold.\nDataset base all base one base exp int. week int. one all c= 7\nEN:NA 242 2,229 1,880 2,200 244 223\nEN:JP 185 2,101 1,726 2,050 189 167\nFR 159 2,084 1,707 2,068 166 139\nDE 268 2,634 2,267 2,644 282 243\nTwitter 756 4,245 4,253 5,859 4,046 739\nTable 3: Perplexity results for our ﬁve data streams in the bigram experiments. The base models in “base all,” “base\none,” and “base exp” are bigram language models. “int. week” is a linear interpolation of “base one” from the past\nweek. “int. one all” is a linear interpolation of “base one” and “base all”. The rightmost column is a version of our\nmodel with c= 7. Best results are highlighted in bold.\neach of the datasets for unigram and bigram experi-\nments respectively. Our model outperformed other\ncompeting models in all cases but one. Recall that we\nonly deﬁne the similarity function of world context\nas: f(xo,t,xo,k) = 1iff sign(ro,t) =sign(ro,k) and\n0 otherwise. A better similarity function (e.g., one\nthat takes into account market size of the company\nand the magnitude of increase or decrease in the stock\nprice) might be able to improve the performance fur-\nther. We leave this for future work. Furthermore,\nthe variations can be captured using models from the\npast week. We discuss why increasing cfrom 7 to 14\ndid not improve performance of the model in more\ndetail in §6.4.\nWe can also see how the models performed over\ntime. Figure 4 traces perplexity for four Reuters news\nstream datasets.13 We can see that in some cases the\nperformance of the “base all” model degraded over\ntime, whereas our model is more robust to temporal\n13In both experiments, in order to manage the time and space\ncomplexities of updating β, we apply a sparsity shrinkage tech-\nnique by using OWL-QN (Andrew and Gao, 2007) when maxi-\nmizing it, with regularization constant set to 1. Intuitively, this\nis equivalent to encouraging the deviation vector to be sparse\n(Eisenstein et al., 2011).\nshifts.\nIn the bigram experiments, we only ran our model\nwith c = 7, since we need to maintain β in RV2\n,\ninstead of RV in the unigram model. The goal of\nthis experiment is to determine whether our method\nstill adds beneﬁt to more expressive language mod-\nels. Note that the weights of the linear interpolation\nmodels are also learned in an online fashion since\nthere are no classical training, development, and test\nsets in our setting. Since the “base one” model per-\nformed poorly in this experiment, the performance of\nthe interpolated models also suffered. For example,\nthe “int. one all” model needed time to learn that the\n“base one” model has to be downweighted (we started\nwith all interpolated models having uniform weights),\nso it was not able to outperform even the “base all”\nmodel.\n6.4 Analysis and Discussion\nIt should not be surprising that conditioning on\nworld-context reduces perplexity (Cover and Thomas,\n1991). A key attraction of our model, we believe, lies\nin the ability to inspect its parameters.\nDeviation coefﬁcients. Inspecting the model al-\nlows us to gain insight into temporal trends. We\n188\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00175 by guest on 05 November 2025\nTwitter:Google\ntimestep\nβ\n0 100 200 300 400 500 600\n0.0 0.5 1.0 1.5 2.0\ngooggoog\n@google@google\ngoogle+google+\n#goog#goog\nrGOOGrGOOG\nTwitter:Microsoft\ntimestep\nβ\n0 100 200 300 400 500 600\n0.0 0.5 1.0 1.5\nmicrosoftmicrosoft\nmsftmsft\n#microsoft#microsoft\nrMSFTrMSFT\nFigure 3: Deviation coefﬁcients βover time for Google- and Microsoft-related words on Twitter with unigram base\nmodel (c= 7). Signiﬁcant changes (increases or decreases) in the returns of Google and Microsoft stocks are usually\nfollowed by increases in βof related words.\ninvestigate the deviations learned by our model on the\nTwitter dataset. Examples are shown in Figure 3. The\nleft plot shows β for four words related to Google:\ngoog, #goog, @google, google+. For compari-\nson, we also show the return of Google stock for the\ncorresponding timestep (scaled by 50 and centered at\n0.5 for readability, smoothed using loess (Cleveland,\n1979), denoted by rGOOG in the plot). We can see\nthat signiﬁcant changes of return of Google stocks\n(e.g., the rGOOG spikes between timesteps 50–100,\n150–200, 490–550 in the plot) occurred alongside\nan increase in β of Google-related words. Similar\ntrends can also be observed for Microsoft-related\nwords in the right plot. The most signiﬁcant loss of\nreturn of Microsoft stocks (the downward spike near\ntimestep 500 in the plot) is followed by a sudden\nsharp increase in βof the words #microsoft and\nmicrosoft.\nFeature coefﬁcients. We can also inspect the\nlearned feature coefﬁcients αto investigate which\nstocks have higher associations with the text that\nis generated. Our feature coefﬁcients are designed\nto reﬂect which changes (or lack of changes) in\nstock prices inﬂuence the word distribution more,\nnot which stocks are talked about more often. We\nﬁnd that the feature coefﬁcients do not correlate with\nobvious company characteristics like market capi-\ntalization (ﬁrm size). For example, on the Twitter\ndataset with bigram base models, the ﬁve stocks with\nthe highest weights are: ConAgra Foods Inc., Intel\nCorp., Bristol-Myers Squibb, Frontier Communica-\ntions Corp., and Amazon.com Inc. Strongly negative\nweights tended to align with streams with less activ-\ntime lags\nfrequency\n0 20 40 60 80\n1 2 3 4 5 6 7 8 9 10 11 12 13 14\nFigure 5: Distributions of the selection probabilities of\nmodels from the previousc= 14timesteps, on the EN:NA\ndataset with unigram base model. For simplicity, we show\nE-step modes. The histogram shows that the model tends\nto favor models from days closer to the current date.\nity, suggesting that these were being used to smooth\nacross all cdays of history. A higher weight for stock\noimplies an increase in probability of choosing mod-\nels from previous timesteps s, when the state of the\nworld for the current timestep tand timestep sis the\nsame (as represented by our similarity function) with\nrespect to stock o(all other things being equal), and\na decrease in probability for a lower weight.\nSelected models. Besides feature coefﬁcients, our\nmodel captures temporal shift by modeling similar-\nity across the most recent cdays. During inference,\nour model weights different word distributions from\nthe past. The similarity is encoded in the pairwise\nfeatures f(xt,xk) and the parameters α. Figure 5\nshows the distributions of the strongest-posterior\nmodels from previous timesteps, based on how far\n189\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00175 by guest on 05 November 2025\nEN:NA\ntimestep\nperplexity\n0 50 100 150 200 250 300 350\n200 400 600\nbase allbase allcompletecomplete\nint. one allint. one all\nEN:JP\ntimestep\nperplexity\n0 50 100 150 200 250 300 350\n200 400 600\nbase allbase allcompletecomplete\nint. one allint. one all\nFR\ntimestep\nperplexity\n0 50 100 150 200 250 300 350\n200 400 600\nbase allbase allcompletecomplete\nint. one allint. one all\nDE\ntimestep\nperplexity\n0 50 100 150 200 250 300 350\n300 500 700\nbase allbase allcompletecomplete\nint. one allint. one all\nFigure 4: Perplexity over time for four Reuters news streams (c= 7) with bigram base models.\n190\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00175 by guest on 05 November 2025\nin the past they are at the time of use, aggregated\nacross rounds on the EN:NA dataset, for window size\nc= 14. It shows that the model tends to favor models\nfrom days closer to the current date, with the t−1\nmodels selected the most, perhaps because the state\nof the world today is more similar to dates closer to\ntoday compare to more distant dates. The plot also\nexplains why increasing cfrom 7 to 14 did not im-\nprove performance of the model, since most of the\nvariation in our datasets can be captured with models\nfrom the past week.\nTopics. Latent topic variables have often ﬁgured\nheavily in approaches to dynamic language model-\ning. In preliminary experiments incorporating single-\nmembership topic variables (i.e., each document be-\nlongs to a single topic, as in a mixture of unigrams),\nwe saw no beneﬁt to perplexity. Incorporating top-\nics also increases computational cost, since we must\nmaintain and estimate one language model per topic,\nper timestep. It is straightforward to design mod-\nels that incorporate topics with single- or mixed-\nmembership as in LDA (Blei et al., 2003), an in-\nteresting future direction.\nPotential applications. Dynamic language models\nlike ours can be potentially useful in many applica-\ntions, either as a standalone language model, e.g.,\npredictive text input, whose performance may de-\npend on the temporal dimension; or as a component\nin applications like machine translation or speech\nrecognition. Additionally, the model can be seen as\na step towards enhancing text understanding with\nnumerical, contextual data.\n7 Conclusion\nWe presented a dynamic language model for stream-\ning datasets that allows conditioning on observable\nreal-world context variables, exempliﬁed in our ex-\nperiments by stock market data. We showed how to\nperform learning and inference in an online fashion\nfor this model. Our experiments showed the predic-\ntive beneﬁt of such conditioning and online learning\nby comparing to similar models that ignore temporal\ndimensions and observable variables that inﬂuence\nthe text.\nAcknowledgements\nThe authors thank several anonymous reviewers for help-\nful feedback on earlier drafts of this paper and Brendan\nO’Connor for help with collecting Twitter data. This re-\nsearch was supported in part by Google, by computing\nresources at the Pittsburgh Supercomputing Center, by\nNational Science Foundation grant IIS-1111142, AFOSR\ngrant FA95501010247, ONR grant N000140910758, and\nby the Intelligence Advanced Research Projects Activ-\nity via Department of Interior National Business Center\ncontract number D12PC00347. The U.S. Government is\nauthorized to reproduce and distribute reprints for Govern-\nmental purposes notwithstanding any copyright annotation\nthereon. The views and conclusions contained herein are\nthose of the authors and should not be interpreted as nec-\nessarily representing the ofﬁcial policies or endorsements,\neither expressed or implied, of IARPA, DoI/NBC, or the\nU.S. Government.\nReferences\nGalen Andrew and Jianfeng Gao. 2007. Scalable training\nof l1-regularized log-linear models. In Proc. of ICML.\nDavid M. Blei and John D. Lafferty. 2006. Dynamic topic\nmodels. In Proc. of ICML.\nDavid M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n2003. Latent Dirichlet allocation. Journal of Machine\nLearning Research, 3:993–1022.\nS´ebastien Bubeck. 2011. Introduction to online opti-\nmization. Technical report, Department of Operations\nResearch and Financial Engineering, Princeton Univer-\nsity.\nNicol`o Cesa-Bianchi and G´abor Lugosi. 2006. Prediction,\nLearning, and Games. Cambridge University Press.\nWilliam S. Cleveland. 1979. Robust locally weighted\nregression and smoothing scatterplots. Journal of the\nAmerican Statistical Association, 74(368):829–836.\nThomas M. Cover and Joy A. Thomas. 1991. Elements of\nInformation Theory. John Wiley & Sons.\nJohn Duchi and Yoram Singer. 2009. Efﬁcient online\nand batch learning using forward backward splitting.\nJournal of Machine Learning Research , 10(7):2899–\n2934.\nJacob Eisenstein, Brendan O’Connor, Noah A. Smith,\nand Eric P. Xing. 2010. A latent variable model for\ngeographic lexical variation. In Proc. of EMNLP.\nJacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.\nSparse additive generative models of text. In Proc. of\nICML.\nAmit Goyal, Hal Daume III, and Suresh Venkatasubrama-\nnian. 2009. Streaming for large scale NLP: Language\nmodeling. In Proc. of HLT-NAACL.\n191\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00175 by guest on 05 November 2025\nMatt Hoffman, David M. Blei, Chong Wang, and John\nPaisley. 2013. Stochastic variational inference. Jour-\nnal of Machine Learning Research, 14:1303–1347.\nAntti Honkela and Harri Valpola. 2003. On-line varia-\ntional Bayesian learning. In Proc. of ICA.\nTomoharu Iwata, Takeshi Yamada, Yasushi Sakurai, and\nNaonori Ueda. 2010. Online multiscale dynamic topic\nmodels. In Proc. of KDD.\nFrederick Jelinek. 1997. Statistical Methods for Speech\nRecognition. MIT Press.\nJyrki Kivinen and Manfred K. Warmuth. 1997. Expo-\nnentiated gradient versus gradient descent for linear\npredictors. Information and Computation, 132:1–63.\nVictor Lavrenko, Matt Schmill, Dawn Lawrie, Paul\nOgilvie, David Jensen, and James Allan. 2000. Mining\nof concurrent text and time series. In Proc. of KDD\nWorkshop on Text Mining.\nAbby Levenberg and Miles Osborne. 2009. Stream-based\nrandomised language models for SMT. In Proc. of\nEMNLP.\nAbby Levenberg, Chris Callison-Burch, and Miles Os-\nborne. 2010. Stream-based translation models for sta-\ntistical machine translation. In Proc. of HLT-NAACL.\nDong C. Liu and Jorge Nocedal. 1989. On the limited\nmemory BFGS method for large scale optimization.\nMathematical Programming B, 45(3):503–528.\nDavid Mimno and Andrew McCallum. 2008. Topic mod-\nels conditioned on arbitrary features with Dirichlet-\nmultinomial regression. In Proc. of UAI.\nAlexander Rakhlin. 2009. Lecture notes on online learn-\ning. Technical report, Department of Statistics, The\nWharton School, University of Pennsylvania.\nMasaaki Sato. 2001. Online model selection based on the\nvariational bayes. Neural Computation, 13(7):1649–\n1681.\nShai Shalev-Shwartz. 2012. Online learning and online\nconvex optimization. Foundations and Trends in Ma-\nchine Learning, 4(2):107–194.\nMartin J. Wainwright and Michael I. Jordan. 2008. Graph-\nical models, exponential families, and variational infer-\nence. Foundations and Trends in Machine Learning,\n1(1–2):1–305.\nChong Wang, David M. Blei, and David Heckerman.\n2008. Continuous time dynamic topic models. In Proc.\nof UAI.\nChong Wang, John Paisley, and David M. Blei. 2011. On-\nline variational inference for the hierarchical Dirichlet\nprocess. In Proc. of AISTATS.\nMartin Zinkevich. 2003. Online convex programming\nand generalized inﬁnitesimal gradient ascent. In Proc.\nof ICML.\n192\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00175 by guest on 05 November 2025",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9087560176849365
    },
    {
      "name": "Language model",
      "score": 0.7140140533447266
    },
    {
      "name": "Dynamics (music)",
      "score": 0.6371013522148132
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6234661936759949
    },
    {
      "name": "Probabilistic logic",
      "score": 0.5625560283660889
    },
    {
      "name": "Social media",
      "score": 0.5591511130332947
    },
    {
      "name": "Scalability",
      "score": 0.5465372800827026
    },
    {
      "name": "Streaming data",
      "score": 0.5361518859863281
    },
    {
      "name": "Task (project management)",
      "score": 0.5353565216064453
    },
    {
      "name": "Artificial intelligence",
      "score": 0.492452472448349
    },
    {
      "name": "Natural language processing",
      "score": 0.49020296335220337
    },
    {
      "name": "World Wide Web",
      "score": 0.18737450242042542
    },
    {
      "name": "Data mining",
      "score": 0.1742868721485138
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ],
  "cited_by": 24
}