{
    "title": "Learning Light-Weight Translation Models from Deep Transformer",
    "url": "https://openalex.org/W3173417753",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2099334605",
            "name": "Bei Li",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2128318503",
            "name": "Zi-yang Wang",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2073720603",
            "name": "Hui Liu",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2069793417",
            "name": "Quan Du",
            "affiliations": [
                "Southeastern University",
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A1983914940",
            "name": "Tong Xiao",
            "affiliations": [
                "Universidad del Noreste",
                "Southeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2122370144",
            "name": "Chunliang Zhang",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2496766346",
            "name": "Jingbo Zhu",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2099334605",
            "name": "Bei Li",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2128318503",
            "name": "Zi-yang Wang",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2073720603",
            "name": "Hui Liu",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2069793417",
            "name": "Quan Du",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A1983914940",
            "name": "Tong Xiao",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2122370144",
            "name": "Chunliang Zhang",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2496766346",
            "name": "Jingbo Zhu",
            "affiliations": [
                "Northeastern University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2888520903",
        "https://openalex.org/W2975381464",
        "https://openalex.org/W6737778391",
        "https://openalex.org/W1724438581",
        "https://openalex.org/W2561907692",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6701947533",
        "https://openalex.org/W3035618017",
        "https://openalex.org/W2974875810",
        "https://openalex.org/W6719020539",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6767579094",
        "https://openalex.org/W6784387292",
        "https://openalex.org/W6776747255",
        "https://openalex.org/W2990215755",
        "https://openalex.org/W6752166675",
        "https://openalex.org/W2943493972",
        "https://openalex.org/W1816313093",
        "https://openalex.org/W2789541106",
        "https://openalex.org/W2969515962",
        "https://openalex.org/W2948798935",
        "https://openalex.org/W3021993108",
        "https://openalex.org/W2908336025",
        "https://openalex.org/W6764548015",
        "https://openalex.org/W6765189123",
        "https://openalex.org/W2113104171",
        "https://openalex.org/W3023873724",
        "https://openalex.org/W2970290486",
        "https://openalex.org/W2799001369",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2964213727",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W2964093309",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W4230872509",
        "https://openalex.org/W3034742481",
        "https://openalex.org/W2970731908",
        "https://openalex.org/W2972451902",
        "https://openalex.org/W1904365287",
        "https://openalex.org/W4288256350",
        "https://openalex.org/W3035747971",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2963807318",
        "https://openalex.org/W2952809536",
        "https://openalex.org/W3103334733",
        "https://openalex.org/W3102816807",
        "https://openalex.org/W2965046076",
        "https://openalex.org/W3046835050",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2963736842",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Recently, deep models have shown tremendous improvements in neural machine translation (NMT). However, systems of this kind are computationally expensive and memory intensive. In this paper, we take a natural step towards learning strong but light-weight NMT systems. We proposed a novel group-permutation based knowledge distillation approach to compressing the deep Transformer model into a shallow model. The experimental results on several benchmarks validate the effectiveness of our method. Our compressed model is 8 times shallower than the deep model, with almost no loss in BLEU. To further enhance the teacher model, we present a Skipping Sub-Layer method to randomly omit sub-layers to introduce perturbation into training, which achieves a BLEU score of 30.63 on English-German newstest2014. The code is publicly available at https://github.com/libeineu/GPKD.",
    "full_text": "Learning Light-Weight Translation Models from Deep Transformer\nBei Li1\u0003, Ziyang Wang1\u0003, Hui Liu1\u0003, Quan Du1,2,\nTong Xiao1,2y, Chunliang Zhang1,2, Jingbo Zhu1,2\n1NLP Lab, School of Computer Science and Engineering, Northeastern University, Shenyang, China\n2NiuTrans Research, Shenyang, China\nflibei neu,duquanneug@outlook.com, fwangziyang,huiliug@stumail.neu.edu.cn,\nfxiaotong,zhangcl,zhujingbog@mail.neu.edu.cn\nAbstract\nRecently, deep models have shown tremendous improvements\nin neural machine translation (NMT). However, systems of\nthis kind are computationally expensive and memory inten-\nsive. In this paper, we take a natural step towards learning\nstrong but light-weight NMT systems. We proposed a novel\ngroup-permutation based knowledge distillation approach to\ncompressing the deep Transformer model into a shallow model.\nThe experimental results on several benchmarks validate the\neffectiveness of our method. Our compressed model is 8 times\nshallower than the deep model, with almost no loss in BLEU.\nTo further enhance the teacher model, we present a Skipping\nSub-Layer method to randomly omit sub-layers to introduce\nperturbation into training, which achieves a BLEU score of\n30.63 on English-German newstest2014. The code is publicly\navailable at https://github.com/libeineu/GPKD.\nIntroduction\nNeural machine translation (NMT) has advanced signiﬁcantly\nin recent years (Bahdanau, Cho, and Bengio 2015). In par-\nticular, the Transformer model has become popular for its\nwell-designed architecture and the ability to capture the de-\npendency among positions over the entire sequence (Vaswani\net al. 2017). Early systems of this kind stack 4-8 layers on\nboth the encoder and decoder sides (Wu et al. 2016; Gehring\net al. 2017), and the improvement often comes from the use\nof wider networks (a.k.a., Transformer-Big). More recently,\nresearchers try to explore deeper models for Transformer.\nEncouraging results appeared in architecture improvements\nby creating direct pass from the low-level encoder layers to\nthe decoder (Bapna et al. 2018; Wang et al. 2019; Wei et al.\n2020; Wu et al. 2019b; Li et al. 2019), and proper initializa-\ntion strategies (Zhang, Titov, and Sennrich 2019; Xu et al.\n2020; Liu et al. 2020; Huang et al. 2020).\nDespite promising improvements, problems still remain in\ndeep NMT. Deep Transformer stacked by dozens of encoder\nlayers always have a large number of parameters, which are\ncomputationally expensive and memory intensive. For exam-\nple, a 48-layer Transformer is3\u0002larger than a 6-layer system\nand 1:5\u0002slower for inference. It is difﬁcult to deploy such\n\u0003Equal contribution\nyCorresponding author\nCopyright c\r2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nmodels on resource-restricted devices, such as mobile phones.\nTherefore, it is crucial to compress such heavy systems into\nlight-weight ones while keeping their performance.\nKnowledge distillation is a promising method to address\nthe issue. Although several studies (Sun et al. 2019; Jiao\net al. 2020) have attempted to compress the 12-layer BERT\nmodel through knowledge distillation, effectively compress-\ning extremely deep Transformer NMT systems is still an open\nquestion in the MT community. In addition, these methods\nleverage sophisticated layer-wise distillation loss functions\nto minimize the distance between the teacher and the stu-\ndent models, which requires huge memory consumption and\nenormous training cost.\nIn this paper, we investigate simple and efﬁcient com-\npression strategies for deep Transformer. We propose a\nnovel Transformer compression approach (named as group-\npermutation based knowledge distillation method (GPKD ))\nto transfer the knowledge from an extremely deep teacher\nmodel into a shallower student model. We disturb the com-\nputation order among each layer group during the teacher\ntraining phase, which is easy to implement and memory\nfriendly. Moreover, to further enhance the performance of the\nteacher network, we introduce a vertical “dropout” (named\nas skipping sub-layer method) into training by randomly\nomitting sub-layers to prevent co-adaptations of the over-\nparameterized teacher network. Although similar technique\nhas been discussed in Fan, Grave, and Joulin (2020)’s work,\nwe believe that the ﬁnding here is complementary to theirs.\nBoth GPKD and regularization training methods can be well\nincorporated into the teacher training process, which is essen-\ntial for obtaining a strong but light-weight student model.\nWe ran experiments on the WMT16 English-German,\nNIST OpenMT12 Chinese-English and WMT19 Chinese-\nEnglish translation tasks. The GPKD method compressed a\n48-layer Transformer into a 6-layer system with almost no\nloss in BLEU. It outperformed the baseline with the same\ndepth by +\n2:46 BLEU points. Through skipping sub-layer\nmethod, the teacher network achieved a BLEU score of30:63\nBLEU on the newstest2014 English-German task, and the stu-\ndent obtains additional improvements of 0:50 BLEU points.\nCompression of Deep Transformer\nIn this section, we ﬁrst introduce the formulation of knowl-\nedge distillation (K D), then present the group-permutation\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n13217\nL3 L1 L2 L2 L3 L1\nL2 L3 L1 L1 L3 L2\nL3 L2 L1 L2 L1 L3\nL1 L2 L3 L1 L2 L3\nL1 L2 L3 L1 L2 L3\n..\n.\n..\n.\nSKD-source\nSKD-tar\nget\nL1 L3\nSKD-Data .\n.\n.\n.\n.\n. .\n.\n.\nGroup1 Group2\nGroup1 Group2\nGroup1 Group2\nBatch1Batch2Batchn\nreorder\n.\n.\n.\n..\n.\n..\n.\nsampling\nTeacher\nTraining\nStudent T\nraining\nGenerate SK\nD-data\nFigure 1: An overview of the GPKD method including three stages. Group1 and Group2 correspond to different groups of the\nstacking layers. L1, L2 and L3 denote the layers in each group.\nbased knowledge distillation (GPKD ) approach to compress-\ning deep Transformer.\nKnowledge Distillation\nThe purpose of KD is to transfer knowledge from a complex\nteacher network to a light-weight student network by encour-\naging the student network reproducing the performance of the\nteacher network. Let FT (x;y) and FS(x;y) represent the\npredictions of the teacher network and the student network,\nrespectively. Then KD can be formulated as follows:\nLKD =\nX\nx2X;y2Y\nL\n\u0000\nFT (x;y);FS(x;y)\n\u0001\n(1)\nwhere L(\u0001) is a loss function to evaluate the distance between\nFT (x;y) and FS(x;y). xand yrepresent the source inputs\nand target inputs, respectively.(X,Y) denotes the whole train-\ning dataset. The objective can be seen as minimizing the loss\nfunction to bridge the gap between the student and its teacher.\nTo advance the student model, a promising method is\nto learn from the intermediate layer representations of the\nteacher network via additional loss functions (Sun et al. 2019;\nJiao et al. 2020). However, the additional loss functions re-\nquire large memory footprint due to the logits computation\nof both the teacher and student networks in each mini-batch\ntraining phase. This is quite challenging when the teacher\nis extremely deep. Alternatively, we choose sequence-level\nknowledge distillation (S KD), proposed in Kim and Rush\n(2016)’s work to simplify the training procedure. Through\ntheir results, SKD achieves comparable or even higher trans-\nlation performance than word-level KD method. Concretely,\nSKD uses the translation results of the teacher network as the\ngold instead of the ground truth. In this work, we build our\nstudent systems upon SKD method.\nGroup-Permutation Based Knowledge Distillation\nThrough preliminary experimental results, the student net-\nwork trained with the SKD data still signiﬁcantly underper-\nforms its teacher. A possible explanation is directly shrinking\nthe encoder depth is harmful to the translation performance.\nTo further bridge the gap between the teacher network and\nthe student work, we propose a group-permutation based\nknowledge distillation method, including three stages: (i)\ngroup-permutation training strategy which rectiﬁes the infor-\nmation ﬂow of the teacher network during training phase. (ii)\ngenerate the SKD data through the teacher network. (iii) train\nthe student network with the SKD data. Instead of randomly\ninitializing the parameters of the student network, we selected\nlayers from the teacher network to form the student network,\nwhich provides a better initialization. Figure 1 exhibits the\nwhole training process.\nGroup-Permutation Training Assuming that the teacher\nmodel has N Transformer layers, we aim to extract M lay-\ners to form a student model. This can be characterized as\nlearning the mapping between\nlayerm and layern, where\nm2f1;2:::;Mgand n= N=M\u0003m. To achieve this goal,\nthe stacking layers are ﬁrst divided into M groups and every\nadjacent h= N=Mlayers form a group. The core idea of the\nproposed method is to make the selected single layer mimic\nthe behavior of its group output. Instead of employing addi-\ntional loss functions to reduce the distance of the intermediate\nlayer representations between the student network and the\nteacher network, we simply disturbing the computation order\nin each group when training the teacher network.\nIn each mini-batch of training, we randomly disturb the\norder of the layers in each group. Suppose that Gi =\nfL1;L2;L3g(i 2M) is the set of intermediate layers in\neach group. The left part of Figure 1 shows the computation\norder of the layers during the teacher training phase. Note\n13218\nTraining Validation\n15 17 19 21 23 25\n6:0\n6:5\n7:0\nEpoch\nTraining\nPPL\n15 17 19 21 23 25\n3:7\n3:8\n3:9\nEpoch\nValidation\nPPL\n15 17 19 21 23 25\n5:0\n5:5\n6:0\nEpoch\nTraining\nPPL\n15 17 19 21 23 25\n3:5\n3:6\n3:7\nEpoch\nValidation\nPPL\nFigure 2:\nThe comparison of training and validation PPL on\nthe shallow (left) and the deep (right) models.\nthat each group is independent with others in ordering the\nlayers. In this work, we sample the layer order uniformly\nfrom all the permutation choices1.\nGenerating SKD Data As shown in Stage 2 (the top right\npart in Figure 1), given the training dataset fX;Yg, the\nteacher network translates the source inputs into the target\nsentences Z. Then the SKD data is the collection of fX;Zg.\nStudent Training After obtaining the teacher network and\nthe SKD data, we begin optimizing the student network with\nthe supervision of the teacher. As illustrated in Stage 1, each\nsingle layer imitates the logits of its group output and every\nlayer in each group behave similar with each other. Then we\nrandomly choose one layer from each group to form a “new”\nencoder which the encoder depth is reduced from N to M. It\ncan be regarded as the student model with fewer layers. The\ncompression rate is controlled by the hyper-parameterh. One\ncan compress a 48-layer teacher into a 6-layer network by\nsetting h= 8, or progressively achieve this goal with h= 2\nfor three times of compression.\nThe DESDAR Architecture\nThe GPKD method also ﬁts into the decoder. Hence we design\na heterogeneous NMT model consisting of a deep encoder\nand a shallow decoder, abbreviated as (DESDAR ). Such an\narchitecture can enjoy high translation quality due to the deep\nencoder and fast inference due to the light decoder. Similar\nﬁndings were observed in previous work (Zhang, Titov, and\nSennrich 2019; Xiao et al. 2019), which could speed up the\nTransformer inference by simplifying the decoder architec-\nture. This is due to the fact that the heavy use of dot-product\nattention in the decoder and the nature of auto-regressive\ndecoding slows down the system.\nMoreover, it offers a way of balancing the translation qual-\nity and the inference. This is promising for industrial ap-\nplications. For example, one can maximize the speedup by\nusing one decoder layer or two, or can yield further BLEU im-\nprovements by enlarging the decoder depth. The experimental\nresults in the following sections show the effectiveness of the\nDESDAR architecture.\n1For a 3-layer group, the computation order includes\nfL1; L2; L3g, fL1; L3; L2g, fL2; L1; L3g, fL2; L3; L1g,\nfL3; L1; L2gand fL3; L2; L1g\nxl LN F + LN F + xl+2\n(a) Standard Pre-Norm\nxl LN F + LN F + xl+2\nM=1 M=0\n(b) Pre-Norm with a skipped sub-layer\nFigure 3: The sub-layer information ﬂow of (a) Standard\nPre-Norm, (b) Pre-Norm with a skipped bub-layer.\nSkipping Sub-Layers for Deep Transformer\nIt is well known that stronger teacher networks (deeper or\nwider) can bring better supervision signals (Hinton, Vinyals,\nand Dean 2015). However, these over-parameterized net-\nworks often face the overﬁtting problem (Fan, Grave, and\nJoulin 2020). In this section, we introduce a regularization\ntraining method to alleviate the overﬁtting problem which\ncan further improve the extremely deep Transformer sys-\ntems. Here we start with the sub-layer co-adaptation problem,\nfollowed by the Skipping Sub-Layer method to address it.\nCo-adaptation of Sub-Layers\nIn Transformer, a stack of sub-layers (or layers) are used\nbetween the input and the output. The relationship between\nthe input and the output is complex if the model goes deeper.\nThere will typically be co-adaptation of the sub-layers, and a\nsub-layer will operate based on the states of other sub-layers.\nThis property is helpful for training because sub-layers are\nlearned to work well together on the training data. But it\nprevents the model from generalizing well at test time. This\nis similar to the case in that too many hidden units of a\nlayer lead to the co-adaptation of feature detectors (Hinton\net al. 2012). See Figure 2 for perplexities on the training and\nvalidation data at different training steps. Clearly, the deep\nmodel (48-layer encoder) appears to overﬁt.\nThe Skipping Sub-Layer Method\nTo address the overﬁtting problem, we drop either the self-\nattention sub-layer or the feed-forward sub-layer of the Trans-\nformer encoder for robust training (call it the Skipping Sub-\nLayer method). Both types of the sub-layer follow the Pre-\nNorm architecture of deep Transformer (Wang et al. 2019):\nxl+1 = F(LN(xl)) +xl (2)\nwhere LN(\u0001) is the layer normalization function, xl is the\noutput of sub-layer land F(\u0001) is either the self-attention or\nfeed-forward function. We use variableM2f0; 1gto control\nhow often a sub-layer is omitted. Then, we re-deﬁne the sub-\nlayer as:\nxl+1 = M \u0001F(LN(xl)) +xl (3)\nwhere M = 0 (or = 1)means that the sub-layer is omitted\n(or reserved). See Figure 3 for comparison of two sub-layer\nnetworks.\n13219\nIn addition, Greff, Srivastava, and Schmidhuber (2017)\nhave shown that the lower-level sub-layers of a deep neural\nnetwork provide the core representation of the input and the\nsubsequent sub-layers reﬁne that representation. Therefore,\nit is natural to skip fewer sub-layers if they are close to the\ninput, instead of using the same dropping rate for each single\nlayer in Fan, Grave, and Joulin (2020). To this end, we design\na method that makes the lower-level sub-layers seldom to be\ndropped. Let Lbe the number of layers of the stack2 and lbe\nthe current sub-layer. Then, we deﬁne M as\nM =\n\u001a\n0; P \u0014pl\n1; P >p l\n(4)\nwhere\npl = l\n2L \u0001\u001e; for 1 \u0014l\u00142L (5)\nIn this model, \u001e is a hyper-parameter that is set to 0.4 in\nour experiments. pl is the rate of omitting the sub-layer. For\nsub-layer l, we ﬁrst draw a variable P from the uniform\ndistribution in [0;1]. Then, M is set to 1 if P > pl, and 0\notherwise. Thus the lower-level sub-layers are more likely to\nsurvive.\nNote that the Skipping Sub-Layer method is doing some-\nthing like sampling a sub-network from a full network. For a\nmodel with 2Lsub-layers, it encodes 22L sub-networks and\neach conﬁguration of sub-layer omission represents a sub-\nnetwork. These sub-models are learned efﬁciently because\nthey share the parameters. For inference, all these sub-models\nbehave like an ensemble model. Following the work in Hin-\nton et al. (2012), we rescale the output representation of each\nsub-layer by the survival rate 1 \u0000pl, like this:\nxl+1 = (1\u0000pl) \u0001F(LN(xl)) +xl (6)\nFactor 1\u0000pl is used to scale-down the output of the sub-layer,\nso the expected output of the sub-layer is the same as the\nactual output at test time. Then, the ﬁnal model can make a\nmore accurate prediction by averaging the predictions from\n22L sub-models.\nTwo-stage Training\nOur Skipping Sub-Layer method is straightforwardly applica-\nble to the training phase of Transformer. However, we found\nin our preliminary experiments that the learned model even\nunderperformed the baseline if we introduced sub-layer omis-\nsion into training from the beginning. This might be due to\nthe fact that deep Transformer is complex and the training\nis fragile to the perturbation if the model does not get to the\nsmoothed region of the error surface.\nHere, we instead adopt a two-stage training method to\nlearn the deep Transformer model with omitting sub-layers.\nFirst, we train the model as usual but early stop it when\nthe model converges on the validation set. Then, we apply\nour Skipping Sub-Layer method to the model and continue\ntraining until the model converges again. As is shown in Table\n4, the two-stage training is helpful for making better use of\n2There are 2L sub-layers for L layers.\nrandom sub-layer omission and producing better results. To\nour knowledge, we are the ﬁrst to emphasize the importance\nof the two-stage training in building deep Transformer with\nomitting layers or sub-layers.\nExperiments\nWe conducted experiments on the WMT’16 English-German,\nNIST’12 Chinese-English and WMT19’ Chinese-English\ntasks.\nExperimental Setups\nThe bilingual and evaluation data mainly came from three\nsources:\n\u000f WMT’16 English-German (En-De). We used the same\ndatasets as in (Vaswani et al. 2017; Wu et al. 2019a; Wang\net al. 2019). They consisted of approximately 4:5M to-\nkenized sentence pairs. All sentences were segmented\ninto sequences of sub-word units (Sennrich, Haddow, and\nBirch 2016) with 32K merge operations using a vocabulary\nshared by the source and target sides. newstest2016 and\nnewstest2014 was the validation and test data, respectively.\n\u000f NIST’12 Chinese-English (NIST Zh-En). We randomly\nextracted nearly 1.9M bilingual corpus from NIST’12\nOpenMT3. MT06 was the validation set and the concate-\nnation of MT04 and MT08 was the test set.\n\u000f WMT’19 Chinese-English (WMT Zh-En). For more con-\nvincing results, we also experimented on a larger dataset\nextracted from the mixture of the CWMT and UN corpora,\nprovided by Wang et al. (2019). We selectednewstest2017\nas the validation data and reported the BLEU scores on\nnewstest2018 and newstest2019.\nWe adopted the compound split strategy for En-De, which\nwas a common post-processing step used in previous work\n(Vaswani et al. 2017; Wang et al. 2019; Wu et al. 2019b).\nFor Zh-En tasks, all the sentences were segmented by the\ntool provided within NiuTrans (Xiao et al. 2012). Translation\nquality was measured by case-sensitive tokenized BLEU for\nEn-De task, and case-insensitive tokenized BLEU for NIST\nZh-En task. The BLEU script was multi-bleu.perl. We also\nreported the sacrebleu4 results on the En-De and the WMT\nZh-En tasks, respectively.\nFor training, we used Adam optimizer (Kingma and Ba\n2015), and followed the hyper-parameters uesd in Wang et al.\n(2019). As suggested in Shaw, Uszkoreit, and Vaswani (2018),\nwe incorporated the relative position representation into the\nself-attention mechanism to enhance the positional infor-\nmation. This is quite crucial when building extremely deep\nTransformer. Then, we batched sentence pairs by approxi-\nmate length, and limited input/output tokens per batch to\n4;096/GPU and updated the parameters every two steps. The\nhidden size of Base and Deep models was 512, and 1024 for\n3LDC Number: LDC2000T46, LDC2000T47, LDC2000T50,\nLDC2003E14, LDC2005T10, LDC2002E18, LDC2007T09, and\nLDC2004T08\n4BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a\n+version.1.2.12\n13220\nSystems Depth WMT En-De NIST Zh-En WMT Zh-En\nParams BLEU \u0001 Params MT06 MT04 MT08 Avg \u0001 Params Test17 Test18 Test19 Avg \u0001\nDeep-RPR-24L 24-6 118M 29.39 - 141M 53.56 56.31 48.15 52.67 - 142M 26.50 27.00 28.30 27.26 -\nBase-RPR-6L 6-6 62M 27.60 ref 84M 51.63 55.06 46.17 50.95 ref 85M 25.90 26.00 27.60 26.50 ref\n+SKD 6-6 62M 28.50 +0.90 84M 52.60 55.16 46.99 51.58 +0.63 85M 26.10 26.20 27.80 26.70 +0.20\n+GPKD 6-6 62M 29.33 +1.73 84M 53.32 56.24 47.65 52.40 +1.45 85M 26.40 26.90 28.30 27.20 +0.70\nDeep-RPR-48L 48-6 194M 30.03 - 217M 54.00 56.40 48.21 52.87 - 218M 26.80 27.30 28.60 27.56 -\nBase-RPR-6L 6-6 62M 27.60 ref 84M 51.63 55.06 46.17 50.95 ref 85M 25.90 26.00 27.60 26.50 ref\n+SKD 6-6 62M 29.01 +1.39 84M 52.55 55.15 46.92 51.54 +0.59 85M 26.40 26.40 28.00 26.93 +0.43\n+GPKD 6-6 62M 29.68 +2.08 84M 53.63 56.19 48.11 52.64 +1.69 85M 26.70 27.10 28.50 27.43 +0.93\nBig-RPR-12L 12-6 286M 29.91 - 332M 54.19 56.89 49.29 53.45 - 335M 27.00 27.50 28.70 27.73 -\nBig-RPR-6L 6-6 211M 29.21 ref 256M 52.80 55.57 47.54 51.97 ref 258M 26.50 27.00 28.20 27.23 ref\n+SKD 6-6 211M 29.47 +0.26 256M 53.73 55.80 47.78 52.43 +0.46 258M 26.80 27.30 28.50 27.53 +0.30\n+GPKD 6-6 211M 29.88 +0.67 256M 54.03 56.55 49.16 53.24 +1.27 258M 26.90 27.40 28.70 27.66 +0.43\nTable 1: The results of the GPKD method applied in encoder side on En-De and Zh-En tasks. We seth= 4and h= 8to compress\nthe 24-layer and 48-layer systems, respectively. RPR denotes the Transformer incorporating byrelative positional information.\n4 8 12 16 20\n2:4\n2:5\n2:6\n2:7\nEpoch\nTraining\nLoss\nSKD\nGPKD\n(a) WMT En-De\n4 8 12 16\n3:0\n3:3\n3:6\n3:9\nEpoch\nTraining\nLoss\nSKD\nGPKD\n(b) NIST Zh-En\nFigure 4: The training loss of applying the GPKD (blue) and\nSKD (red) methods on the WMT En-De, NIST Zh-En tasks,\nrespectively.\n1 6 12 18 24 30\n26:0\n27:0\n28:0\n29:0\n30:0\nDecoder Depth\nBLEU\nBase-RPR\nDeep-RPR\n1 6 12 18 24 30\n2k\n4k\n6k\n8k\nDecoder Depth\nSpeed(tokens/s)\nBase-RPR\nDeep-RPR\nFigure 5: BLEU scores [%] and translation speed [tokens/sec]\nagainst decoder depth on the En-De task.\nbig counterparts. The Base/Big/Deep models were updated\nfor 50k/150k/50k steps on the En-De task, 25k/50k/25k steps\non the NIST Zh-En task and 100k/200k/100k on the WMT\nZh-En task. The beam size and length penalty were set to\n4/0.6 and 6/1.3 for En-De and Zh-En tasks, respectively.\nExperimental Results\nThe Effect of GPKD Method We successfully trained\n24-layer/48-layer Transformer-Deep systems and 12-layer\nTransformer-Big systems incorporating the relative position\nrepresentation (RPR) on three tasks. Table 1 shows the results\nSystem BLEU \u0001BLEU Speedup\nBase Deep Base Deep\nZhang, Xiong, and Su (2018) 27.33 -0.11 N/A 1.34\u0002 N/A\nXiao et al. (2019) 27.69 +0.17 N/A 1.39\u0002 N/A\nBase 27.60 N/A N/A N/A N/A\nDeep 30.03 N/A N/A N/A N/A\nDESDAR 48-3 29.82 +2.22 -0.21 1.28\u0002 1.52\u0002\nDESDAR 48-1 28.31 +0.71 -1.72 1.97\u0002 2.17\u0002\nDESDAR 48-1 (SKD) 29.70 +2.10 -0.27 1.99\u0002 2.18\u0002\nDESDAR 48-1 (GPKD ) 30.01 +2.41 -0.02 1.99\u0002 2.18\u0002\nTable 2: BLEU scores [%], inference speedup [ \u0002] on the\nWMT En-De task.\nwhen applying the GPKD method to the encoder side. Deep\nTransformer systems outperform the shallow baselines by a\nlarge margin, but the model capacities are 2 or 3 times larger.\nAnd 6-layer models trained through SKD outperform the shal-\nlow baselines by 0:63-1:39 BLEU scores, but there is still\na nonnegligible gap between them and their deep teachers.\nAs we expect, our GPKD method can enable the baselines to\nperform similarly with the deep teacher systems, and outper-\nforms SKD by 0:41-1:10 BLEU scores on three benchmarks.\nNote that, although the compressed systems are 4 or 8\u0002shal-\nlower, they only underperform the deep baselines by a small\nmargin. Similar phenomenon is observed when switching to\na wide network, that 6-layer RPR-Big systems match with its\nteacher with almost no loss in BLEU, indicating the GPKD\nmethod is applicable in different model capacities. Moreover,\nFigure 4 plots the training loss of SKD and GPKD methods.\nWe observe that GPKD obtains a much lower training loss\nthan SKD on the WMT En-De and NIST Zh-En tasks, which\nfurther veriﬁes the effectiveness of GPKD .\nFigure 5 plots the BLEU scores and the translation speeds\nof different decoder depths. We can see that deeper decoders\nyield modest BLEU improvements but slow down the in-\nference signiﬁcantly based on a shallow encoder. While no\n13221\nSystem WMT En-De NIST Zh-En WMT Zh-En\nParams Updates Time BLEU SBLEU MT06 MT04 MT05 MT08 Test17 Test18 Test19\nFan, Grave, and Joulin (2020) 286M 16\u000213K N/A 30.20 N/A N/A N/A N/A N/A N/A N/A N/A\nLi et al. (2020) 194M 50K 11.75h 30.21 29.0 N/A N/A N/A N/A N/A N/A N/A\nWu et al. (2019b) 270M 800K N/A 29.92 N/A N/A N/A N/A N/A N/A N/A N/A\nOtt et al. (2018) 210M 16\u000213K N/A 29.30 28.3 N/A N/A N/A N/A N/A N/A N/A\nWang et al. (2019) 137M 50K N/A 29.30 N/A 53.57 55.91 52.30 48.12 26.9 27.4 N/A\nBase-RPR 62M 100K 5.02h 27.60 26.5 51.63 55.06 50.38 46.17 25.9 26.0 27.6\nDeep-RPR-48L 194M 50K 16.87h 30.03 28.8 54.00 56.40 52.98 48.21 26.8 27.3 28.6\n+ Skipping Sub-Layer 194M 50K 15.41h 30.63\u0005 29.4 54.76 57.01 53.52 49.16 27.2 27.7 29.0\nTable 3: BLEU scores [%], parameters and training time on three language pairs. Note that SBLEU represents the sacrebleu\nscore and the pl (omission rate) was set to 0.4. Note that \u0005presents the result of beam 8. The BLEU of beam 4 is 30.49.\n16 18 20 22 24\n3:4\n3:5\n3:6\nEpoch\nValidation\nPPL\nBefore After\n(a) WMT En-De\n10 11 12 13 14 15\n4:2\n4:3\n4:4\n4:5\nEpoch\nValidation\nPPL\nBefore After\n(b) NIST Zh-En\n10 11 12 13 14 15\n4:9\n5:1\nEpoch\nValidation\nPPL\nBefore After\n(c) WMT Zh-En\nFigure 6: The validation PPL of employing the Skipping Sub-Layer method before (red) and after (blue) on En-De, NIST Zh-En\nand WMT Zh-En tasks, respectively.\nimprovement is observed when we switch to a strong Deep-\nRPR-48L model.\nTable 2 exhibits several DESDAR systems with different\nsettings. D ESDAR 48-3 achieves comparable performance\nwith the 48-6 baseline, but speeds up the inference by 1:52\u0002.\nHowever, a shallower decoder makes a great decrease com-\npact on BLEU, though it obtains a 1:97\u0002speedup. Through\nthe SKD method, the DESDAR 48-1 system can even outper-\nform the RPR-Base by 2:10 BLEU scores and speeds up the\ninference by 2:18\u0002. Moreover, our GPKD method can enable\nthe DESDAR 48-1 system to perform similarly with the deep\nbaseline, outperforms S KD by nearly + 0:31 BLEU scores.\nInterestingly, after knowledge distillation, the beam search\nseems like to be not important for the D ESDAR systems,\nwhich can achieve a 3:2\u0002speedup with no performance sac-\nriﬁce with the greedy search. This may be due to the fact that\nthe student network learns the soft distribution generated by\nthe teacher network, which has already limited the search\nspace to the max beam margin (Kim and Rush 2016).\nThe Effect of Skipping Sub-Layer Method The red\ncurves in Figure 6 show that the 48-layer RPR model con-\nverges quickly on three tasks, and the validation PPL goes up\nlater. At the same time, the training PPL is still going down\n(see Figure 2). As we expect, the Skipping Sub-Layer method\nreduces the overﬁtting problem and thus achieves a lower\nPPL (3.39) on the validation set. The similar phenomena are\nobserved on the other two tasks. In addition, the last row\nof Table 3 shows that the strong Deep-RPR model trained\nthrough the Skipping Sub-Layer approach obtains +0:40-0:72\nBLEU improvements on three benchmarks.\nComparison with Related MethodsTable 4 exhibits the\nBLEU scores and the validation PPL of several related sys-\ntems trained through two optimization ways. Interestingly,\nall these systems underperform the deep baseline when we\ntrained them from scratch. This is reasonable because the\nskipped connections make disturbances to the optimization\nwhen the model is still in the early stage of training, and\nthe parameters are in the non-smoothed region of the loss\nfunction. The phenomena here verify the importance of the\ntwo-stage training strategy.\nOn the other hand, our Skipping Sub-Layer method and\nStochastic Layers (Pham et al. 2019) trained by the ﬁnetuning\nschema both beat the strong baseline. This conﬁrms that\ndropping sub-layers randomly is helpful for reducing the\noverﬁtting when we train deep Transformer models. However,\nthe deep models trained with LayerDrop method cannot gain\nmore beneﬁt and we attribute this to the fact that LayerDrop\nuses the same probability to omit sub-layers throughout the\nstack. This is harmful to the performance because omitting\nmany lower-level layers reduces the representation ability\nof the deep model signiﬁcantly (Huang et al. 2016; Greff,\nSrivastava, and Schmidhuber 2017).\nAblation Study Table 5 shows the ablation study of omit-\nting different components, including randomly skipping the\nfeed-forward (FFN) sub-layer, the self-attention (SAN) sub-\nlayer, all sub-layers and the whole layer. As shown in Table\n5, the performance of the single checkpoint and the check-\npoint averaging model is reported. First, we can see that all\nthese systems obtain lower validation PPLs and higher BLEU\nscores for the single model than the baseline. And our default\n13222\nSystem PPL BLEU\nDeep-RPR-48L 3.52 30.03\nScratch\nSkipping Sub-Layer 3.41 29.82\nLayerDrop (Fan, Grave, and Joulin 2020) 3.42 29.65\nStochastic Layers (Pham et al. 2019) 3.44 29.75\nFinetune\nSkipping Sub-Layer 3.39 30.49\nLayerDrop (Fan, Grave, and Joulin 2020) 3.47 29.77\nStochastic Layers (Pham et al. 2019) 3.44 30.12\nTable 4: Comparison of training from scratch and ﬁnetune on\nthe WMT En-De task.\nSystem PPL Single Avg\nDeep-RPR-48L 3.52 29.19 30.03\nSkip FFN 3.45 29.61 30.22\nSkip SAN 3.45 29.71 30.26\nSkip FFN or SAN 3.39 29.95 30.49\nSkip Layer 3.46 29.45 29.92\nTable 5: Ablation results on the WMT En-De task.\nstrategy beats the baseline by a larger margin in terms of\nboth PPL and BLEU. There is no signiﬁcant difference when\nwe skip FFN or SAN only, and they all surpass the system\nthat randomly omits the entire layer. This is mainly due to\nthe fact that we can sample more diverse sub-networks in\ntraining. Note that the results here were mainly experimented\non deep Transformer, rather than a shallow but wide coun-\nterpart reported in (Fan, Grave, and Joulin 2020), which is\ncomplementary to the community.\nThe Overall Results Table 6 shows the results of incorpo-\nrating both the G PKD and Skipping Sub-Layer approaches.\nNote that, these systems are obtained upon the strong Deep-\nRPR-48L system. As we can see that a 6-6 system achieves\ncomparable performance with the state-of-the-art, though the\nparameter is only 4 times less than theirs. In addition, it beats\nthe shallow baseline by +\n2:56 BLEU scores at the same scale.\nThis offers a way of selecting the proper system considering\nthe trade-off between the translation performance and the\nmodel storage. For example, one can choose GPKD 6-3 sys-\ntem with satisfactory performance and fast inference speed,\nor GPKD 24-3 system with both high translation quality and\ncompetitive inference speed. Another interesting ﬁnding here\nis that shrinking the decoder depth may hurt the BLEU score\nwhen the encoder is not strong enough.\nRelated Work\nDeep neural networks play an important role in the resur-\ngence of deep learning. It has been observed that increasing\nthe depth of neural networks can drastically improve the per-\nformance of convolutional neural network-based systems (He\net al. 2016). The machine translation communities follow this\ntrend. For example, Bapna et al. (2018) and Wang et al. (2019)\nshortened the path from upper-level layers to lower-level lay-\ners so as to avoid gradient vanishing/exploding. Wu et al.\nSystem Params Depth BLEU Speed\nFan, Grave, and Joulin (2020) 286M 12-6 30.20 2534\nWu et al. (2019b) 270M 8-8 29.92 2044\nWei et al. (2020) 512M 18-6 30.56 N/A\nSkipping Sub-Layer + GP 194M 48-6 30.59 3092\nGPKD 24-3 106M 24-3 30.40 5237\nGPKD 24-1 97M 24-1 30.05 8116\nGPKD 6-6 62M 6-6 30.16 3817\nGPKD 6-3 48M 6-3 29.71 5460\nTable 6: The overall results of BLEU scores [%] and transla-\ntion speed [tokens/sec] on the WMT En-De task.\n(2019b) designed a two-stage approach with three specially\ndesigned components to build a 8-layer Transformer-Big sys-\ntem. Zhang, Titov, and Sennrich (2019) successfully trained\na deep Post-Norm Transformer with carefully designed layer-\nwise initialization strategy. More attempts on initialization\nstrategy emereged recently (Xu et al. 2020; Liu et al. 2020;\nHuang et al. 2020). Perhaps the most relevant work with us is\nFan, Grave, and Joulin (2020)’s work. They employedLayer-\nDrop mechanism to train a 12-6 Transformer-Big and pruned\nsub-networks during inference without ﬁnetuning. Here we\naddress a similar issue in deep Transformer, which has not\nbeen discussed yet. Beyond this, we present a new training\nstrategy that can boost the deep system in a robust manner.\nFor model compression, there are many successful meth-\nods, such as quantization (Gong et al. 2014), knowledge\ndistillation (KD) (Kim and Rush 2016), weight pruning (Han\net al. 2015) and efﬁcient Transformer architecture (Mehta\net al. 2020a,b). For Transformer models, Sun et al. (2019)\nproposed a novel approach to compressing a large BERT\nmodel into a shallow one via the Patient Knowledge Distilla-\ntion method. Jiao et al. (2020) achieved a better compression\nrate by richer supervision signals between the teacher net-\nwork and the student network. However, these methods are\nnot straightforwardly applicable to machine translation, they\nneed simultaneously compute the logits of each layer in both\nthe teacher and student networks, which consumes large GPU\nmemory. In this work, we propose the GPKD method to com-\npress an extremely deep model into a baseline-like system,\nwithout any additional computation cost.\nConclusions\nOur contributions in this work are two folds. (i) We propose\na G PKD method to compress the deep model into a shal-\nlower one with minor performance sacriﬁce, which outper-\nforms the SKD method by a large margin. (ii) The proposed\nSkipping Sub-Layer method reduces the overﬁtting problem\nwhen training extremely deep encoder systems by randomly\nomitting sub-layers during training phase. The experimental\nresults on three widely-used benchmarks validate the effec-\ntiveness of the proposed methods. After the incorporating\nof two methods, the strong but light-weight student models\nshow competitive performance which is application friendly.\n13223\nAcknowledgments\nThis work was supported in part by the National Science\nFoundation of China (Nos. 61876035 and 61732005), the\nNational Key R&D Program of China (No. 2019QY1801).\nThe authors would like to thank anonymous reviewers for\ntheir valuable comments. And thank Qiang Wang and Yufan\nJiang for their helpful advice to improve the paper. Appreciate\nTao Zhou for his suggestion to beautify the ﬁgures.\nReferences\nBahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural machine\ntranslation by jointly learning to align and translate. In In\nProceedings of the 3rd International Conference on Learning\nRepresentations.\nBapna, A.; Chen, M.; Firat, O.; Cao, Y .; and Wu, Y . 2018.\nTraining Deeper Neural Machine Translation Models with\nTransparent Attention. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language Processing,\n3028–3033. Brussels, Belgium: Association for Computa-\ntional Linguistics.\nFan, A.; Grave, E.; and Joulin, A. 2020. Reducing Trans-\nformer Depth on Demand with Structured Dropout. In 8th\nInternational Conference on Learning Representations, ICLR\n2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nGehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin,\nY . 2017. Convolutional Sequence to Sequence Learning. In\nICML.\nGong, Y .; Liu, L.; Yang, M.; and Bourdev, L. D. 2014. Com-\npressing Deep Convolutional Networks using Vector Quanti-\nzation. CoRR abs/1412.6115.\nGreff, K.; Srivastava, R. K.; and Schmidhuber, J. 2017. High-\nway and Residual Networks learn Unrolled Iterative Estima-\ntion. In 5th International Conference on Learning Repre-\nsentations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings.\nHan, S.; Pool, J.; Tran, J.; and Dally, W. J. 2015. Learning\nboth Weights and Connections for Efﬁcient Neural Network.\nIn Advances in Neural Information Processing Systems 28:\nAnnual Conference on Neural Information Processing Sys-\ntems 2015, December 7-12, 2015, Montreal, Quebec, Canada,\n1135–1143.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, 770–\n778.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531 .\nHinton, G. E.; Srivastava, N.; Krizhevsky, A.; Sutskever, I.;\nand Salakhutdinov, R. R. 2012. Improving neural networks by\npreventing co-adaptation of feature detectors. arXiv preprint\narXiv:1207.0580 .\nHuang, G.; Sun, Y .; Liu, Z.; Sedra, D.; and Weinberger, K. Q.\n2016. Deep Networks with Stochastic Depth. In Computer\nVision - ECCV 2016 - 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings, Part IV,\n646–661.\nHuang, X. S.; Perez, F.; Ba, J.; and V olkovs, M. 2020. Im-\nproving Transformer Optimization Through Better Initializa-\ntion. In Proceedings of Machine Learning and Systems 2020,\n9868–9876.\nJiao, X.; Yin, Y .; Shang, L.; Jiang, X.; Chen, X.; Li, L.; Wang,\nF.; and Liu, Q. 2020. TinyBERT: Distilling BERT for Natural\nLanguage Understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, 4163–4174.\nOnline: Association for Computational Linguistics.\nKim, Y .; and Rush, A. M. 2016. Sequence-Level Knowledge\nDistillation. In Proceedings of the 2016 Conference on Em-\npirical Methods in Natural Language Processing, 1317–1327.\nAustin, Texas: Association for Computational Linguistics.\nKingma, D. P.; and Ba, J. 2015. Adam: A Method for Stochas-\ntic Optimization. In Bengio, Y .; and LeCun, Y ., eds.,3rd In-\nternational Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track\nProceedings.\nLi, B.; Li, Y .; Xu, C.; Lin, Y .; Liu, J.; Liu, H.; Wang, Z.;\nZhang, Y .; Xu, N.; Wang, Z.; Feng, K.; Chen, H.; Liu, T.;\nLi, Y .; Wang, Q.; Xiao, T.; and Zhu, J. 2019. The NiuTrans\nMachine Translation Systems for WMT19. In Proceedings\nof the Fourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1), 257–266. Florence, Italy:\nAssociation for Computational Linguistics.\nLi, B.; Wang, Z.; Liu, H.; Jiang, Y .; Du, Q.; Xiao, T.; Wang,\nH.; and Zhu, J. 2020. Shallow-to-Deep Training for Neural\nMachine Translation. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing\n(EMNLP), 995–1005. Online: Association for Computational\nLinguistics.\nLiu, L.; Liu, X.; Gao, J.; Chen, W.; and Han, J. 2020. Un-\nderstanding the Difﬁculty of Training Transformers. In Pro-\nceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), 5747–5763. Online:\nAssociation for Computational Linguistics.\nMehta, S.; Ghazvininejad, M.; Iyer, S.; Zettlemoyer, L.; and\nHajishirzi, H. 2020a. DeLighT: Very Deep and Light-weight\nTransformer. arXiv preprint arXiv:2008.00623 .\nMehta, S.; Koncel-Kedziorski, R.; Rastegari, M.; and Ha-\njishirzi, H. 2020b. DeFINE: Deep Factorized Input Token\nEmbeddings for Neural Sequence Modeling. In 8th Interna-\ntional Conference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\nOtt, M.; Edunov, S.; Grangier, D.; and Auli, M. 2018. Scal-\ning Neural Machine Translation. In Proceedings of the Third\nConference on Machine Translation, Volume 1: Research Pa-\npers, 1–9. Belgium, Brussels: Association for Computational\nLinguistics.\nPham, N.; Nguyen, T.; Niehues, J.; M¨uller, M.; and Waibel,\nA. 2019. Very Deep Self-Attention Networks for End-to-End\n13224\nSpeech Recognition. In Kubin, G.; and Kacic, Z., eds., In-\nterspeech 2019, 20th Annual Conference of the International\nSpeech Communication Association, Graz, Austria, 15-19\nSeptember 2019, 66–70. ISCA.\nSennrich, R.; Haddow, B.; and Birch, A. 2016. Neural Ma-\nchine Translation of Rare Words with Subword Units. InPro-\nceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 1715–\n1725. Berlin, Germany: Association for Computational Lin-\nguistics.\nShaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-Attention\nwith Relative Position Representations. In Proceedings of the\n2018 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), 464–468. New Or-\nleans, Louisiana: Association for Computational Linguistics.\nSun, S.; Cheng, Y .; Gan, Z.; and Liu, J. 2019. Patient Knowl-\nedge Distillation for BERT Model Compression. In Proceed-\nings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Con-\nference on Natural Language Processing (EMNLP-IJCNLP),\n4323–4332. Hong Kong, China: Association for Computa-\ntional Linguistics.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in Neural Information\nProcessing Systems, 6000–6010.\nWang, Q.; Li, B.; Xiao, T.; Zhu, J.; Li, C.; Wong, D. F.; and\nChao, L. S. 2019. Learning Deep Transformer Models for\nMachine Translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics,\n1810–1822. Florence, Italy: Association for Computational\nLinguistics.\nWei, X.; Yu, H.; Hu, Y .; Zhang, Y .; Weng, R.; and Luo, W.\n2020. Multiscale Collaborative Deep Models for Neural\nMachine Translation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics,\n414–426. Online: Association for Computational Linguistics.\nWu, F.; Fan, A.; Baevski, A.; Dauphin, Y . N.; and Auli, M.\n2019a. Pay Less Attention with Lightweight and Dynamic\nConvolutions. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019.\nWu, L.; Wang, Y .; Xia, Y .; Tian, F.; Gao, F.; Qin, T.; Lai, J.;\nand Liu, T.-Y . 2019b. Depth Growing for Neural Machine\nTranslation. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, 5558–5563.\nFlorence, Italy.\nWu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .; Norouzi, M.;\nMacherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey, K.;\net al. 2016. Google’s Neural Machine Translation System:\nBridging the Gap between Human and Machine Translation.\narXiv preprint arXiv:1609.08144 .\nXiao, T.; Li, Y .; Zhu, J.; Yu, Z.; and Liu, T. 2019. Sharing\nAttention Weights for Fast Transformer. InProceedings of the\nTwenty-Eighth International Joint Conference on Artiﬁcial\nIntelligence, IJCAI 2019, Macao, China, August 10-16, 2019,\n5292–5298.\nXiao, T.; Zhu, J.; Zhang, H.; and Li, Q. 2012. NiuTrans:\nAn Open Source Toolkit for Phrase-based and Syntax-based\nMachine Translation. In Proceedings of the ACL 2012 System\nDemonstrations, ACL ’12, 19–24. Stroudsburg, PA, USA:\nAssociation for Computational Linguistics.\nXu, H.; Liu, Q.; van Genabith, J.; Xiong, D.; and Zhang,\nJ. 2020. Lipschitz Constrained Parameter Initialization for\nDeep Transformers. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics, 397–\n402. Online: Association for Computational Linguistics.\nZhang, B.; Titov, I.; and Sennrich, R. 2019. Improving Deep\nTransformer with Depth-Scaled Initialization and Merged\nAttention. In Proceedings of the 2019 Conference on Empir-\nical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), 898–909. Hong Kong, China:\nAssociation for Computational Linguistics.\nZhang, B.; Xiong, D.; and Su, J. 2018. Accelerating Neural\nTransformer via an Average Attention Network. In Pro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 1789–\n1798. Melbourne, Australia: Association for Computational\nLinguistics.\n13225"
}