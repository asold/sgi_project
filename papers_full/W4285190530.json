{
  "title": "TimeLMs: Diachronic Language Models from Twitter",
  "url": "https://openalex.org/W4285190530",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3015350553",
      "name": "Loureiro, Daniel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3023545175",
      "name": "Barbieri, Francesco",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226134879",
      "name": "Neves, Leonardo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281541304",
      "name": "Espinosa-Anke, Luis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225875417",
      "name": "Camacho-Collados, Jose",
      "affiliations": [
        "Cardiff University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3158835769",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2954226438",
    "https://openalex.org/W2132069633",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2804830075",
    "https://openalex.org/W4295883599",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W2963780471",
    "https://openalex.org/W3126553126",
    "https://openalex.org/W3153094109",
    "https://openalex.org/W3202099651",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W3175236579",
    "https://openalex.org/W2741040861",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4212964822",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2931922640",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3213460052",
    "https://openalex.org/W3217756080",
    "https://openalex.org/W2806198715",
    "https://openalex.org/W2460159515",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3118069529",
    "https://openalex.org/W2805744755",
    "https://openalex.org/W2922580172",
    "https://openalex.org/W2807333695",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4287890137"
  ],
  "abstract": "Despite its importance, the time variable has been largely neglected in the NLP and language model literature. In this paper, we present TimeLMs, a set of language models specialized on diachronic Twitter data. We show that a continual learning strategy contributes to enhancing Twitter-based language models’ capacity to deal with future and out-of-distribution tweets, while making them competitive with standardized and more monolithic benchmarks. We also perform a number of qualitative analyses showing how they cope with trends and peaks in activity involving specific named entities or concept drift. TimeLMs is available at github.com/cardiffnlp/timelms.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nSystem Demonstrations, pages 251 - 260\nMay 22-27, 2022 ©2022 Association for Computational Linguistics\nTimeLMs: Diachronic Language Models from Twitter\nDaniel Loureiro*♠, Francesco Barbieri*♣,\nLeonardo Neves♣, Luis Espinosa Anke♢, Jose Camacho-Collados♢\n♠LIAAD - INESC TEC, University of Porto, Portugal\n♣Snap Inc., Santa Monica, California, USA\n♢Cardiff NLP, School of Computer Science and Informatics, Cardiff University, UK\n♠daniel.b.loureiro@inesctec.pt , ♣{fbarbieri,lneves}@snap.com,\n♢{espinosa-ankel,camachocolladosj}@cardiff.ac.uk\nAbstract\nDespite its importance, the time variable has\nbeen largely neglected in the NLP and language\nmodel literature. In this paper, we present\nTimeLMs, a set of language models specialized\non diachronic Twitter data. We show that a con-\ntinual learning strategy contributes to enhanc-\ning Twitter-based language models’ capacity to\ndeal with future and out-of-distribution tweets,\nwhile making them competitive with standard-\nized and more monolithic benchmarks. We also\nperform a number of qualitative analyses show-\ning how they cope with trends and peaks in ac-\ntivity involving specific named entities or con-\ncept drift. TimeLMs is available at https://\ngithub.com/cardiffnlp/timelms.\n1 Introduction\nNeural language models (LMs) (Devlin et al., 2019;\nRadford et al., 2019; Liu et al., 2019) are today a\nkey enabler in NLP. They have contributed to a\ngeneral uplift in downstream performance across\nmany applications, even sometimes rivaling human\njudgement (Wang et al., 2018, 2019), while also\nbringing about a new paradigm of knowledge ac-\nquisition through pre-training. However, currently,\nboth from model development and evaluation stand-\npoints, this paradigm is essentially static, which\naffects both the ability to generalize to future data\nand the reliability of experimental results, since\nit is not uncommon that evaluation benchmarks\noverlap with pre-training corpora (Lazaridou et al.,\n2021). As an example, neither the original ver-\nsions of BERT and RoBERTa are up to date with\nthe current coronavirus pandemic. This is clearly\ntroublesome, as most of the communication in re-\ncent years has been affected by it, yet these models\nwould barely know what we are referring to when\nwe talk about COVID-19 or lockdown, to name just\na few examples. The lack of diachronic special-\nization is especially concerning in contexts such\nAuthors marked with an asterisk (*) contributed equally.\nas social media, where topics of discussion change\noften and rapidly (Del Tredici et al., 2019).\nIn this paper, we address this issue by sharing\nwith the community a series of time-specific LMs\nspecialized to Twitter data (TimeLMs). Our initia-\ntive goes beyond the initial release, analysis and ex-\nperimental results reported in this paper, as models\nwill periodically continue to be trained, improved\nand released.\n2 Related Work\nThere exists a significant body of work on deal-\ning with the time variable in NLP. For instance,\nby specializing language representations derived\nfrom word embedding models or neural networks\n(Hamilton et al., 2016; Szymanski, 2017; Rosen-\nfeld and Erk, 2018; Del Tredici et al., 2019; Hof-\nmann et al., 2021). Concerning the particular case\nof LMs, exposing them to new data and updating\ntheir parameters accordingly, also known as contin-\nual learning, is a promising direction, with an es-\ntablished tradition in machine learning (Lopez-Paz\nand Ranzato, 2017; Lewis et al., 2020; Lazaridou\net al., 2021; Jang et al., 2021). Other works, how-\never, have proposed to enhance BERT-based topic\nmodels with the time variable (Grootendorst, 2020).\nWith regards to in-domain specialization, there are\nnumerous approaches that perform domain adap-\ntation by pre-training a generic LM on specialized\ncorpora. A well-known case is the biomedical do-\nmain, e.g., BioBERT (Lee et al., 2020), SciBERT\n(Beltagy et al., 2019) or PubMedBERT (Gu et al.,\n2021). In addition to these approaches to specialize\nlanguage models, there have been similar temporal\nadaptation analyses to the one presented in our pa-\nper (Agarwal and Nenkova, 2021; Jin et al., 2021).\nIn particular, these works showed that training lan-\nguage models in recent data can be beneficial, an\nimprovement that was found to be marginal in Luu\net al. (2021) in a different setting. In terms of con-\ntinual lifelong learning, which is tangential to our\n251\nmain goal, Biesialska et al. (2020) provide a de-\ntailed survey on the main techniques proposed in\nthe NLP literature.\nMore relevant to this paper, on the other hand,\nare LMs specialized to social media data, specifi-\ncally Twitter, with BERTweet (Nguyen et al., 2020),\nTweetEval (Barbieri et al., 2020) and XLM-T (Bar-\nbieri et al., 2021) being, to the best of our knowl-\nedge, the most prominent examples. However, the\nabove efforts barely address the diachronic nature\nof language. Crucially, they do not address the\nproblem of specializing LMs to social media and\nputting the time variable at the core of the frame-\nwork. Moreover, it is desirable that such time-\naware models are released alongside usable soft-\nware and a reliable infrastructure. Our TimeLMs\ninitiative, detailed in Section 3, aims to address the\nabove challenges.\n3 TimeLMs: Diachronic Language\nModels from Twitter\nIn this section, we present our approach to train\nlanguage models for different time periods.\n3.1 Twitter corpus\nFor the training and evaluation of language models,\nwe first collect a large corpus of tweets. In the\nfollowing we explain both the data collection and\ncleaning processes.\nData collection.We use the Twitter Academic API\nto obtain a large sample of tweets evenly distributed\nacross time. In order to obtain a sample which\nis representative of general conversation on that\nsocial platform, we query the API using the most\nfrequent stopwords1, for a set number of tweets at\ntimestamps distanced by 5 minutes - for every hour\nof every day constituting a particular yearly quarter.\nWe also use specific flags supported by the API to\nretrieve only tweets in English and ignore retweets,\nquotes, links, media posts and ads.\nFor our initial base model (2019-90M hence-\nforth), we used an evenly time-distributed corpus\nfrom the API, for the period between 2018 and\n2019, supplemented with additional tweets from\nArchive.org which cover the same period but are\nnot evenly distributed.\nData cleaning.Before training any model, we fil-\nter each model’s training set of tweets using the\nprocedure detailed in this section. Starting with the\nassumption that bots are amongst the most active\n1We use the top 10 entries from: google-10000-english.txt\nusers, we remove tweets from the top one percent\nof users that have posted most frequently. Addi-\ntionally, following the recommendation of Lee et al.\n(2021), we remove duplicates and near-duplicates.\nWe find near-duplicates by hashing the texts of\ntweets after lowercasing and stripping punctua-\ntion. Hashing is performed using MinHash (Broder,\n1997), with 16 permutations. Finally, user mentions\nare replaced with a generic placeholder (@user),\nexcept for verified users.\n3.2 Language model training\nOnce the Twitter corpus has been collected and\ncleaned, we proceed to the language model pre-\ntraining. This consists of two phases: (1) training\nof a base model consisting of data until the end\nof 2019; and (2) continual training of language\nmodels every three months since the date of the\nbase model.\nBase model training.Our base model is trained\nwith data until 2019 (included). Following Barbieri\net al. (2020), we start from the original RoBERTa-\nbase model (Liu et al., 2019) and continue training\nthe masked language model on Twitter data. The\nmodel is trained using the same settings as Barbieri\net al. (2020), namely early stopping on the valida-\ntion split and a learning rate of 1.0e−5. This initial\n2019-90M base model converged after around fif-\nteen days on 8 NVIDIA V100 GPUs.\nContinuous training. After training our base\nmodel, our goal is to continue training this lan-\nguage model with recent Twitter corpora. At the\ntime of writing, for practical and logistical reasons,\nthe decision is to train a new version of each lan-\nguage model every three months. The process to\ntrain this updated language model is simple, as it\nfollows the same training procedure as the initial\npre-training of the language model explained above.\nOur commitment is to keep updating and releasing\na new model every three months, effectively en-\nabling the community to make use of an up-to-date\nlanguage model at any period in time.\n3.3 TimeLMs release summary\nIn Table 1 we include a summary of the Twitter\ncorpora collected and models trained until the date\nof writing. Models are split in four three-month\nquarters (Q1, Q2, Q3 and Q4). Our base 2019-\n90M model consists of 90 million tweets until the\nend of 2019. Then, every quarter (i.e., every three\nmonths) 4.2M additional tweets are added, and the\nmodel gets updated as described above. Our latest\n252\nreleased models, which are 2021-Q4 and 2021-\n124M (the latter was re-trained only once with\nall the data from 2020 and 2021), are trained on\n124M tweets on top of the original RoBERTa-base\nmodel (Liu et al., 2019). All models are currently\navailable through the Hugging Face hub athttps:\n//huggingface.co/cardiffnlp.\nModels Additional Total\n2019-90M - 90.26M\n2020-Q1 4.20M 94.46M\n2020-Q2 4.20M 98.66M\n2020-Q3 4.20M 102.86M\n2020-Q4 4.20M 107.06M\n2021-Q1 4.20M 111.26M\n2021-Q2 4.20M 115.46M\n2021-Q3 4.20M 119.66M\n2021-Q4 4.20M 123.86M\n2021-124M 33.60M 123.86M\nTable 1: Number of tweets used to train each model.\nShowing number of tweets used to update models, and\ntotal starting from RoBERTa-base by Liu et al. (2019).\nIn addition to these corpora for training language\nmodels, we set apart a number of tweets for each\nquarter (independent from the training set, with no\noverlap). These sets are used as test sets on our\nperplexity evaluation (see Section 4.2), and consist\nof 300K tweets per quarter, which were sampled\nand cleaned in the same way as the original corpus.\n4 Evaluation\nIn this section, we aim at evaluating the effective-\nness of time-specific language models (see Section\n3) on time-specific tasks. In other words, our goal\nis to test the possible degradation of older mod-\nels over time and, accordingly, test if this can be\nmitigated by continuous training.\nEvaluation tasks.We evaluated the released lan-\nguage models in two tasks: (1) TweetEval (Bar-\nbieri et al., 2020), which consists of seven down-\nstream tweet classification tasks; and (2) Pseudo-\nperplexity on corpora sampled from different time\nperiods. While the first evaluation is merely aimed\nat validating the training procedure of the base lan-\nguage model, the second evaluation is the core\ncontribution of this paper in terms of evaluation,\nwhere different models can be tested in different\ntime periods.\n4.1 TweetEval\nTweetEval (Barbieri et al., 2020) is a unified Twit-\nter benchmark composed of seven heterogeneous\ntweet classification tasks. It is commonly used to\nevaluate the performance of language models (or\ntask-agnostic models more generally) on Twitter\ndata. With this evaluation, our goal is simply to\nshow the general competitiveness of the models re-\nleased with our package, irrespective of their time\nperiods.\nEvaluation tasks.The seven tweet classification\ntasks in TweetEval are emoji prediction (Barbi-\neri et al., 2018), emotion recognition (Mohammad\net al., 2018), hate speech detection (Basile et al.,\n2019), irony detection (Van Hee et al., 2018), offen-\nsive language identification (Zampieri et al., 2019),\nsentiment analysis (Rosenthal et al., 2017) and\nstance detection (Mohammad et al., 2016).\nExperimental setting. Similarly to the TweetE-\nval original baselines, only a moderate parameter\nsearch was conducted. The only hyper-parameter\nfine-tuned was the learning rate ( 1.0e−3, 1.0e−4,\n1.0e−5). The number of epochs each model is\ntrained is variable, as we used early stopping mon-\nitoring the validation loss. The validation loss is\nalso used to select the best model in each task.\nComparison systems. The comparison systems\n(SVM, FastText, BLSTM, RoBERTa-base and\nTweetEval) are those taken from the original\nTweetEval paper, as well as the state-of-the-art\nBERTweet model (Nguyen et al., 2020), which\nis trained over 900M tweets (posted between 2013\nand 2019). All the language models compared are\nbased on the RoBERTa-base architecture.\nResults. TweetEval results are summarized in Ta-\nble 2. BERTweet, which was trained on substan-\ntially more data, attains the best averaged results.\nHowever, when looking at single tasks, BERTweet\noutperforms both our latest released models, i.e.,\nTimeLM-19 and TimeLM-21, on the irony detec-\ntion task2 only. It is also important to highlight that\nTweetEval tasks include tweets dated until 2018 at\nthe latest (with most tasks being considerably ear-\nlier). This suggests that our latest released model\n(i.e. TimeLM-21), even if trained up to 2021 tweets,\nis generally competitive even on past tweets. In-\ndeed, TimeLM-21 outperforms the most similar\nTweetEval model, which was trained following a\n2We note that the irony dataset was created via distant\nsupervision using the #irony hashtag, and there could be a\n“labels” leak since BERTweet was the only model trained on\ntweets of the time period (2014/15) of the irony dataset.\n253\nEmoji Emotion Hate Irony Offensive Sentiment Stance ALL\nSVM 29.3 64.7 36.7 61.7 52.3 62.9 67.3 53.5\nFastText 25.8 65.2 50.6 63.1 73.4 62.9 65.4 58.1\nBLSTM 24.7 66.0 52.6 62.8 71.7 58.3 59.4 56.5\nRoBERTa-Base 30.8 76.6 44.9 55.2 78.7 72.0 70.9 61.3\nTweetEval 31.6 79.8 55.5 62.5 81.6 72.9 72.6 65.2\nBERTweet 33.4 79.3 56.4 82.1 79.5 73.4 71.2 67.9\nTimeLM-19 33.4 81.0 58.1 48.0 82.4 73.2 70.7 63.8\nTimeLM-21 34.0 80.2 55.1 64.5 82.2 73.7 72.9 66.2\nMetric M-F1 M-F1 M-F1 F(i) M-F1 M-Rec A VG (F1) TE\nTable 2: TweetEval test results of all comparison systems.\nsimilar strategy (in this case trained on fewer tweets\nuntil 2019), in most tasks.\n4.2 Time-aware language model evaluation\nOnce the effectiveness of the base and subsequent\nmodels have been tested in downstream tasks, our\ngoal is to measure to what extent the various mod-\nels released are sensitive to a more time-aware eval-\nuation. To this end, we rely on the pseudo perplex-\nity measure (Salazar et al., 2020).\nEvaluation metric: Pseudo-perplexity (PPPL).\nThe pseudo log-likelihood (PLL) score introduced\nby Salazar et al. (2020) is computed by iteratively\nreplacing each token in a sequence with a mask,\nand summing the corresponding conditional log\nprobabilities. This approach is specially suited to\nmasked language models, rather than traditional\nleft-to-right models. Pseudo-perplexity (PPPL) fol-\nlows analogously from the standard perplexity for-\nmula, using PLL for conditional probability.\nResults. Table 3 shows the pseudo-perplexity re-\nsults in all test sets. As the main conclusion, the\ntable shows how more recent models tend to out-\nperform models trained when evaluated older data\nin most test sets (especially those contemporane-\nous). This can be appreciated by simply observing\nthe decreasing values in the columns of the Table\n3. There are a few interesting exceptions, how-\never. For instance, the 2020-Q1 and 2020-Q2 test\nsets, which corresponding to the global start of the\ncoronavirus pandemic, are generally better suited\nfor models trained until that periods. Nonetheless,\nmodels trained on more contemporary data appear\nto converge to the optimal results.\nDegradation over time.How long does it take for\na model to be outdated? Overall, PPPL scores tend\nto increase almost 10% after one year. In general,\nPPPL appears to decrease consistently every quar-\nterly update. This result reinforces the need for\nupdated language models even for short time peri-\nods such as three-month quarters. In most cases,\ndegradation on future data is usually larger than\non older data. This result is not completely un-\nexpected since newer models are also trained on\nmore data for more time periods. In Section 6.1\nwe expand on this by including a table detailing\nthe relative performance degradation over language\nmodels over time.\n5 Python Interface\nIn this section we present an integrated Python\ninterface that we release along with the data and\nlanguage models presented in this paper. As men-\ntioned in Section 3.3, all language models will be\navailable from the Hugging Face hub and our code\nis designed to be used with this platform.\nOur interface, based on the Transformers pack-\nage (Wolf et al., 2020), is focused on providing easy\nsingle-line access to language models trained for\nspecific periods and related use cases. The choice\nof language models to be used with our interface is\ndetermined using one of four modes of operation:\n(1) ‘latest’: using our most recently trained Twitter\nmodel; (2) ‘corresponding’: using the model that\nwas trained only until each tweet’s date (i.e., its\nspecific quarter); (3) custom: providing the pre-\nferred date or quarter (e.g., ‘2021-Q3’); and (4)\n‘quarterly’: using all available models trained over\ntime in quarterly intervals. Having specified the\npreferred language models, there are three main\nfunctionalities within the code, namely: (1) com-\nputing pseudo-perplexity scores, (2) evaluating lan-\nguage models in our released or customized test\nsets, and (3) obtaining masked predictions.\nUsers can measure the extent to which the cho-\n254\nModels 2020-Q1 2020-Q2 2020-Q3 2020-Q4 2021-Q1 2021-Q2 2021-Q3 2021-Q4Change\nBarbieri et al., 20209.420 9.602 9.631 9.651 9.832 9.924 10.073 10.247 N/A\n2019-90M 4.823 4.936 4.936 4.928 5.093 5.179 5.273 5.362 N/A\n2020-Q1 4.521 4.625 4.699 4.692 4.862 4.952 5.043 5.140 -\n2020-Q2 4.441 4.439 4.548 4.554 4.716 4.801 4.902 5.005 -4.01%\n2020-Q3 4.534 4.525 4.450 4.487 4.652 4.738 4.831 4.945 -2.15%\n2020-Q4 4.533 4.524 4.429 4.361 4.571 4.672 4.763 4.859 -2.81%\n2021-Q1 4.509 4.499 4.399 4.334 4.439 4.574 4.668 4.767 -2.89%\n2021-Q2 4.499 4.481 4.376 4.319 4.411 4.445 4.570 4.675 -2.83%\n2021-Q3 4.471 4.455 4.335 4.280 4.366 4.394 4.422 4.565 -3.26%\n2021-Q4 4.467 4.455 4.330 4.263 4.351 4.381 4.402 4.463 -2.24%\n2021-124M 4.319 4.297 4.279 4.219 4.322 4.361 4.404 4.489 N/A\nTable 3: Pseudo-perplexity results (lower is better) of all models in the Twitter test sets sampled from different\nquarters (each quarter correspond to three months. Q1: Jan-Mar; Q2: Apr-Jun; Q3: Jul-Sep; Q4: Oct-Dec). The last\ncolumn reports difference in pseudo-perplexity, comparing the value obtained for each quarter’s test set, between\nthe model trained on the previous quarter and the model updated with data from that same quarter.\nsen pretrained language models are aligned (i.e.,\nfamiliar) with a given list of tweets (or any text)\nusing pseudo-perplexity (see Section 4.2 for more\ndetails), computed as shown in Code 1.\nfrom timelms import TimeLMs\ntlms = TimeLMs(device=’cuda’)\ntweets = [{’text’: ’Looking forward to watching\nSquid Game tonight !’}]\npseudo_ppls = tlms.get_pseudo_ppl(tweets,\nmode=’latest’) # loads 2021-Q4 model\nCode 1: Computing Pseudo-PPL on a given tweet using\nthe most recently available model.\nFor a more extensive evaluation of language\nmodels using pseudo-perplexity, we provide a ran-\ndom subset of our test data across 2020 and 2021.3\nTo evaluate other models from the Transformers\npackage, we provide the ‘eval_model’ method\n(tlms.eval_model()) to compute pseudo-\nperplexity on any given set of tweets or texts (e.g.,\nthe subset we provide) using other language models\nsupported by the Transformers package. Both scor-\ning methods not only provide the pseudo-perplexity\nscores specific to each model (depending on spec-\nified model name, or TimeLMs specified mode),\nbut also the PLL scores assigned to each tweet by\nthe different models.\nFinally, predictions for masked tokens of any\ngiven tweet or text may be easily obtained as\ndemonstrated in Code 2.\ntweets = [{\"text\": \"So glad I’m <mask> vaccinated.\",\n\"created_at\": \"2021-02-01T23:14:26.000Z\"}]\npreds = tlms.get_masked_predictions(tweets, top_k=3,\n3Limited to 50K tweets, the maximum allowed by Twitter.\nIDs for all test tweets are available on the repository.\nmode=’corresponding’) # loads 2021-Q1 model\nCode 2: Obtaining masked predictions using model\ncorresponding to the tweet’s date. Requires tweets or\ntexts with a <mask> token.\nNote that while the examples included in this\npaper are associated with specific dates (i.e., the\ncreated_at field), these are only required for\nthe ‘corresponding’ mode.\n6 Analysis\nTo complement the evaluation in the previous sec-\ntion, we perform a more detailed analysis in three\nimportant aspects: (1) a quantitative analysis on\nthe degradation suffered by language models over\ntime; (2) the relation between time and size (Sec-\ntion 6.2); and (3) a qualitative analysis where we\nshow the influence of time in language models for\nspecific examples (Section 6.3).\n6.1 Degradation analysis\nTable 4 displays the relative performance degra-\ndation (or improvement) of TimeLMs language\nmodels with respect to the test sets whose time\nperiod is the latest where they have been trained\non (diagonals in the table). The table shows how\nmodels tend to perform worse in newer data sets,\nwith a degradation of performance up to 13.68% of\nthe earlier 2020-Q1 model on the latest 2021-Q4\nmodel (with data almost two years later than the\nlatest data the language model was trained on).\nIn order to compare the effect of continuous\ntraining with respect to single training, Figure\n1 shows the PPPL performances of 2021-124M\n(trained on all 2020-2021 data at once) and the\n255\nModels 2020-Q1 2020-Q2 2020-Q3 2020-Q4 2021-Q1 2021-Q2 2021-Q3 2021-Q4\n2020-Q1 0.00% 2.29% 3.94% 3.78% 7.52% 9.52% 11.53% 13.68%\n2020-Q2 0.04% 0.00% 2.46% 2.59% 6.24% 8.16% 10.42% 12.75%\n2020-Q3 1.87% 1.67% 0.00% 0.82% 4.53% 6.47% 8.54% 11.10%\n2020-Q4 3.95% 3.74% 1.57% 0.00% 4.82% 7.14% 9.22% 11.43%\n2021-Q1 1.58% 1.37% -0.89% -2.36% 0.00% 3.05% 5.16% 7.39%\n2021-Q2 1.21% 0.82% -1.55% -2.83% -0.77% 0.00% 2.83% 5.19%\n2021-Q3 1.12% 0.75% -1.95% -3.20% -1.26% -0.61% 0.00% 3.25%\n2021-Q4 0.10% -0.17% -2.97% -4.47% -2.51% -1.83% -1.37% 0.00%\nTable 4: Difference across quarterly models and test sets comparing the pseudo-perplexity observed at the quarter\ncorresponding to each model, against the pseudo-perplexity observed for that same model on both previous and\nfuture test sets. Highlights model degradation on future data, as well as how models fare on past data.\n4.20\n4.30\n4.40\n4.50\n2020-Q1 2020-Q2 2020-Q3 2020-Q4 2021-Q1 2021-Q2 2021-Q3 2021-Q4\n2021-Q4 2021-124M\nFigure 1: Performance (PPPL) of 2021-124M and 2021-Q4 models across the test sets.\n2021-Q4 (updating 2021-Q3) models. Note how\n2021-124M shows improved performance gener-\nally, with the largest differences being attained on\nthe first two quarters of 2020, but not for the latest\nquarters where continuous training seems to work\nslightly better. While more analysis would be re-\nquired, this result suggests that a single training\nis beneficial for earlier periods, while a quarterly\ntraining seems to be better adapted to the most re-\ncent data. However, there does not seem to be any\nmeaningful catastrophic forgetting in the quarterly-\nupdated model, as the differences are relative small.\n6.2 Time and size control experiment\nGiven the results presented earlier, one may natu-\nrally wonder whether the improvement may be due\nto the increase in training size or the recency of ad-\nditional data. While this question is not easy to an-\nswer (and probably the answer will be in-between\nthese two reasons), we perform a simple control\nexperiment as an initial attempt. To this end, we\ntrained an additional language model with twice\nthe training data of the third quarter of 2021 (2021-\nQ3). This way, the total number of training tweets\nModels 2021-Q2 2021-Q3 2021-Q4\n2021-Q2 4.445 4.570 4.675\n2021-Q3 4.394 4.422 4.565\n2021-Q3-2x 4.380 4.380 4.534\n2021-Q4 4.381 4.402 4.463\nTable 5: Results of the control experiment comparing\nquarterly models where the 2021-Q3 model is trained\nwith twice the data from that quarter (2021-Q3-2x).\nis exactly the same as the model trained until the\nfourth quarter of 2021 (2021-Q4).\nConsidering the results on Table 5, we find that\nthe model trained on twice the data for Q3 outper-\nforms the model trained with the default Q3 data\nin all tested quarters. This confirms the assump-\ntion that increasing training data leads to improved\nlanguage model performance. When comparing\nwith the model trained until 2021-Q4, results show\nthis 2021-Q3-2x model is only slightly better in the\n2021-Q2 and 2021-Q3 test sets. However, as we\ncould expect, the model trained in more recent data\n(i.e., until 2021-Q4) gets the best overall results on\nthe more recent test set (i.e., 2021-Q4).\n256\nModel\nSo glad\nI’m <mask>vaccinated.\nI keep\nforgetting to\nbring a <mask>.\nLooking forward\nto watching <mask>\nGame tonight!\n2020-Q1\nnot bag the\ngetting purse The\nself charger this\n2020-Q2\nnot mask The\ngetting bag the\nfully purse End\n2020-Q3\nnot mask the\ngetting bag The\nfully purse End\n2020-Q4\nnot bag the\ngetting purse The\nfully charger End\n2021-Q1\ngetting purse the\nnot charger The\nfully bag End\n2021-Q2\nfully bag the\ngetting charger The\nnot lighter this\n2021-Q3\nfully charger the\ngetting bag The\nnot purse This\n2021-Q4\nfully bag Squid\ngetting lighter the\nnot charger The\nTable 6: Masked token prediction over time using three\nexample tweets as input (using mode=‘quarterly’). For\neach quarterly model, the table displays the top-3 pre-\ndictions ranked by their prediction probability.\n6.3 Qualitative analysis\nIn this section we illustrate, in practice, how mod-\nels trained on different quarters perceive specific\ntweets. First, we use their masked language model-\ning head to predict a <mask> token in context. Ta-\nble 6 shows three tweets and associated predictions\nfrom each of our quarterly models. The model\nbelonging to the most pertinent quarter exhibits\nbackground knowledge more aligned to the trends\nof that period. In the two COVID-related examples,\nwe observe increasing awareness of the general no-\ntion of being fully vaccinated (as opposed to not\nvaccinated, the top prediction from the 2020-Q1\nmodel) in the former, and, in the latter, two in-\nstances where forgetting a mask is more likely than\nforgetting other apparel less related to a particu-\nlar period, such as a charger, a lighter or a purse.\nFinally, note how, in the last example, “Looking\nforward to watching <mask> Game tonight!\", it\nis only in 2021-Q4 that predictions change sub-\nstantially, when the model has been exposed to\nreactions to the \"Squid Game\" show, overlapping\nin time with its global release.\nOur second piece of analysis involves the visu-\nFigure 2: PLL scores of TimeLMs language models\ntrained over different periods for three selected tweets.\nalization of pseudo log-likehood (PLL) scores for\ntweets requiring awareness of a trend or event tied\nto a specific period (Figure 2). Indeed, more recent\nmodels are better at predicting tweets involving\npopular events, such as NFTs or, again, the show\n\"Squid Game\". Conversely, we observe a stagna-\ntion (or even degradation) of the PLL scores for a\ntweet about a contestant of an older reality show.\n7 Conclusion\nIn this paper we presented TimeLMs, language\nmodels trained on Twitter over different time peri-\nods. The initiative also includes the future training\nof language models every three months, thus pro-\nviding free-to-use and up-to-date language models\nfor NLP practitioners. These language models are\nreleased together with a simple Python interface\nwhich facilitates loading and working with these\nmodels, including time-aware evaluation. In our\nevaluation in this paper, we have shown how time-\naware training is relevant, not only from the theoret-\nical point of view, but also the practical one, as the\nresults demonstrate a clear degradation in perfor-\nmance when models are used for future data, which\nis one of the most common settings in practice.\nAs future work, we are planning to explicitly in-\ntegrate the time span variable in the language mod-\nels, i.e., introducing string prefixes, along the lines\nof Dhingra et al. (2022) and Rosin et al. (2022).\n257\nReferences\nOshin Agarwal and Ani Nenkova. 2021. Temporal ef-\nfects on pre-trained models for language processing\ntasks. arXiv preprint arXiv:2111.12790.\nFrancesco Barbieri, Luis Espinosa Anke, and Jose\nCamacho-Collados. 2021. Xlm-t: A multilingual\nlanguage model toolkit for twitter. arXiv preprint\narXiv:2104.12250.\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020. TweetEval:\nUnified benchmark and comparative evaluation for\ntweet classification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1644–1650, Online. Association for Computational\nLinguistics.\nFrancesco Barbieri, Jose Camacho-Collados, Francesco\nRonzano, Luis Espinosa-Anke, Miguel Ballesteros,\nValerio Basile, Viviana Patti, and Horacio Saggion.\n2018. SemEval 2018 task 2: Multilingual emoji\nprediction. In Proceedings of The 12th International\nWorkshop on Semantic Evaluation, pages 24–33, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nValerio Basile, Cristina Bosco, Elisabetta Fersini,\nDebora Nozza, Viviana Patti, Francisco Manuel\nRangel Pardo, Paolo Rosso, and Manuela Sanguinetti.\n2019. SemEval-2019 task 5: Multilingual detection\nof hate speech against immigrants and women in\nTwitter. In Proceedings of the 13th International\nWorkshop on Semantic Evaluation, pages 54–63, Min-\nneapolis, Minnesota, USA. Association for Compu-\ntational Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nMagdalena Biesialska, Katarzyna Biesialska, and\nMarta R Costa-jussà. 2020. Continual lifelong learn-\ning in natural language processing: A survey. In\nProceedings of the 28th International Conference on\nComputational Linguistics, pages 6523–6541.\nA.Z. Broder. 1997. On the resemblance and con-\ntainment of documents. In Proceedings. Compres-\nsion and Complexity of SEQUENCES 1997 (Cat.\nNo.97TB100171), pages 21–29.\nMarco Del Tredici, Raquel Fernández, and Gemma\nBoleda. 2019. Short-term meaning shift: A distri-\nbutional exploration. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 2069–2075, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-Aware Language\nModels as Temporal Knowledge Bases. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:257–273.\nMaarten Grootendorst. 2020. BERTopic: Leveraging\nBERT and c-TF-IDF to create easily interpretable\ntopics.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nWilliam L. Hamilton, Jure Leskovec, and Dan Jurafsky.\n2016. Diachronic word embeddings reveal statisti-\ncal laws of semantic change. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1489–1501, Berlin, Germany. Association for Com-\nputational Linguistics.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich\nSchütze. 2021. Dynamic contextualized word em-\nbeddings. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 6970–6984, Online. Association for Computa-\ntional Linguistics.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,\nJanghoon Han, Gyeonghun Kim, Stanley Jungkyu\nChoi, and Minjoon Seo. 2021. Towards contin-\nual knowledge learning of language models. arXiv\npreprint arXiv:2110.03215.\nXisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao,\nShang-Wen Li, Xiaokai Wei, Andrew Arnold, and\nXiang Ren. 2021. Lifelong pretraining: Continu-\nally adapting language models to emerging corpora.\narXiv preprint arXiv:2110.08534.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, et al. 2021.\nPitfalls of static language modelling. arXiv preprint\narXiv:2102.01951.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\n258\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2021. Deduplicating training\ndata makes language models better. arXiv preprint\narXiv:2107.06499.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n2020. Question and answer test-train overlap in open-\ndomain question answering datasets. arXiv preprint\narXiv:2008.02637.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nDavid Lopez-Paz and Marc ' Aurelio Ranzato. 2017.\nGradient episodic memory for continual learning. In\nAdvances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc.\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar-\nishma Mandyam, and Noah A Smith. 2021. Time\nwaits for no one! analysis and challenges of temporal\nmisalignment. arXiv preprint arXiv:2111.07408.\nSaif Mohammad, Felipe Bravo-Marquez, Mohammad\nSalameh, and Svetlana Kiritchenko. 2018. SemEval-\n2018 task 1: Affect in tweets. In Proceedings of The\n12th International Workshop on Semantic Evaluation,\npages 1–17, New Orleans, Louisiana. Association for\nComputational Linguistics.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016.\nSemEval-2016 task 6: Detecting stance in tweets.\nIn Proceedings of the 10th International Workshop\non Semantic Evaluation (SemEval-2016), pages 31–\n41, San Diego, California. Association for Computa-\ntional Linguistics.\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.\n2020. BERTweet: A pre-trained language model\nfor English tweets. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 9–14, On-\nline. Association for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAlex Rosenfeld and Katrin Erk. 2018. Deep neural\nmodels of semantic shift. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) ,\npages 474–484, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 task 4: Sentiment analysis in Twitter.\nIn Proceedings of the 11th International Workshop\non Semantic Evaluation (SemEval-2017), pages 502–\n518, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nGuy D. Rosin, Ido Guy, and Kira Radinsky. 2022. Time\nmasking for temporal language models. In Proceed-\nings of the Fifteenth ACM International Conference\non Web Search and Data Mining, WSDM ’22, page\n833–841, New York, NY , USA. Association for Com-\nputing Machinery.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2699–2712, Online. Association for Computational\nLinguistics.\nTerrence Szymanski. 2017. Temporal word analogies:\nIdentifying lexical replacement with diachronic word\nembeddings. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 448–453,\nVancouver, Canada. Association for Computational\nLinguistics.\nCynthia Van Hee, Els Lefever, and Véronique Hoste.\n2018. SemEval-2018 task 3: Irony detection in En-\nglish tweets. In Proceedings of The 12th Interna-\ntional Workshop on Semantic Evaluation, pages 39–\n50, New Orleans, Louisiana. Association for Compu-\ntational Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. arXiv preprint arXiv:1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019. SemEval-2019 task 6: Identifying and cat-\negorizing offensive language in social media (Of-\nfensEval). In Proceedings of the 13th International\n259\nWorkshop on Semantic Evaluation, pages 75–86, Min-\nneapolis, Minnesota, USA. Association for Compu-\ntational Linguistics.\n260",
  "topic": "Computational linguistics",
  "concepts": [
    {
      "name": "Computational linguistics",
      "score": 0.5507763028144836
    },
    {
      "name": "Computer science",
      "score": 0.5404229164123535
    },
    {
      "name": "Linguistics",
      "score": 0.49934864044189453
    },
    {
      "name": "Association (psychology)",
      "score": 0.4137917459011078
    },
    {
      "name": "History",
      "score": 0.3442983329296112
    },
    {
      "name": "Natural language processing",
      "score": 0.3397933840751648
    },
    {
      "name": "Philosophy",
      "score": 0.2710148096084595
    },
    {
      "name": "Epistemology",
      "score": 0.07153743505477905
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210166615",
      "name": "INESC TEC",
      "country": "PT"
    },
    {
      "id": "https://openalex.org/I182534213",
      "name": "Universidade do Porto",
      "country": "PT"
    },
    {
      "id": "https://openalex.org/I4210142583",
      "name": "Snap (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I79510175",
      "name": "Cardiff University",
      "country": "GB"
    }
  ]
}