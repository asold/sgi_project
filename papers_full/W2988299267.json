{
  "title": "ConveRT: Efficient and Accurate Conversational Representations from Transformers",
  "url": "https://openalex.org/W2988299267",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2160836133",
      "name": "Matthew Henderson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1420801436",
      "name": "Iñigo Casanueva",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5087009911",
      "name": "Nikola Mrkšić",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5040922365",
      "name": "Pei-Hao Su",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5004866148",
      "name": "Tsung-Hsien Wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1969142033",
      "name": "Ivan Vulić",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2798456655",
    "https://openalex.org/W2976132230",
    "https://openalex.org/W2891416139",
    "https://openalex.org/W2885421725",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2251355666",
    "https://openalex.org/W3045492832",
    "https://openalex.org/W2581377246",
    "https://openalex.org/W2886198413",
    "https://openalex.org/W2084142050",
    "https://openalex.org/W2611029872",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2951807227",
    "https://openalex.org/W2954492830",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W1516479813",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2948110372",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2413533759",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963134326",
    "https://openalex.org/W2972437240",
    "https://openalex.org/W2964703418",
    "https://openalex.org/W2891732163",
    "https://openalex.org/W1532325895",
    "https://openalex.org/W2969574947",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W6908809",
    "https://openalex.org/W2963567240",
    "https://openalex.org/W2251058040",
    "https://openalex.org/W2970618241",
    "https://openalex.org/W2620558438",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2901492641",
    "https://openalex.org/W2963112338",
    "https://openalex.org/W2915295540",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2593751037",
    "https://openalex.org/W2772217324",
    "https://openalex.org/W2909777606",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W3034861927",
    "https://openalex.org/W2916015740",
    "https://openalex.org/W2843010082",
    "https://openalex.org/W2604698497",
    "https://openalex.org/W2963691849",
    "https://openalex.org/W2890394457",
    "https://openalex.org/W2963149412",
    "https://openalex.org/W2102531443",
    "https://openalex.org/W3121541553",
    "https://openalex.org/W2973054254",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2947946877",
    "https://openalex.org/W2339852062",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2963788376",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W3017465475",
    "https://openalex.org/W2951216772",
    "https://openalex.org/W2911803042",
    "https://openalex.org/W2963009325",
    "https://openalex.org/W2963662719"
  ],
  "abstract": "General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers), a pretraining framework for conversational tasks satisfying all the following requirements: it is effective, affordable, and quick to train. We pretrain using a retrieval-based response selection task, effectively leveraging quantization and subword-level parameterization in the dual encoder to build a lightweight memory- and energy-efficient model. We show that ConveRT achieves state-of-the-art performance across widely established response selection tasks. We also demonstrate that the use of extended dialog history as context yields further performance gains. Finally, we show that pretrained representations from the proposed encoder can be transferred to the intent classification task, yielding strong results across three diverse data sets. ConveRT trains substantially faster than standard sentence encoders or previous state-of-the-art dual encoders. With its reduced size and superior performance, we believe this model promises wider portability and scalability for Conversational AI applications.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2161–2174\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n2161\nConveRT: Efﬁcient and Accurate\nConversational Representations from Transformers\nMatthew Henderson, I˜nigo Casanueva, Nikola Mrkˇsi´c,\nPei-Hao Su, Tsung-Hsien Wen and Ivan Vuli´c\nmatt@poly-ai.com\nPolyAI Limited, London, UK\nAbstract\nGeneral-purpose pretrained sentence encoders\nsuch as BERT are not ideal for real-world\nconversational AI applications; they are com-\nputationally heavy, slow, and expensive to\ntrain. We propose ConveRT (Conversational\nRepresentations from Transformers), a pre-\ntraining framework for conversational tasks\nsatisfying all the following requirements: it is\neffective, affordable, and quick to train. We\npretrain using a retrieval-based response se-\nlection task, effectively leveraging quantiza-\ntion and subword-level parameterization in the\ndual encoder to build a lightweight memory-\nand energy-efﬁcient model. We show that Con-\nveRT achieves state-of-the-art performance\nacross widely established response selection\ntasks. We also demonstrate that the use of ex-\ntended dialog history as context yields further\nperformance gains. Finally, we show that pre-\ntrained representations from the proposed en-\ncoder can be transferred to the intent classiﬁ-\ncation task, yielding strong results across three\ndiverse data sets. ConveRT trains substantially\nfaster than standard sentence encoders or pre-\nvious state-of-the-art dual encoders. With its\nreduced size and superior performance, we be-\nlieve this model promises wider portability and\nscalability for Conversational AI applications.\n1 Introduction\nDialog systems, also referred to as conversational\nsystems or conversational agents, have found use\nin a wide range of applications. They assist users\nin accomplishing well-deﬁned tasks such as ﬁnd-\ning and booking restaurants, hotels, and ﬂights\n(Hemphill et al., 1990; Williams, 2012; El Asri\net al., 2017), with further use in tourist informa-\ntion (Budzianowski et al., 2018), language learning\n(Raux et al., 2003; Chen et al., 2017), entertainment\n(Fraser et al., 2018), and healthcare (Laranjo et al.,\n2018; Fadhil and Schiavo, 2019). They are also key\ncomponents of intelligent virtual assistants such as\nSiri, Alexa, Cortana, and Google Assistant.\nData-driven task-oriented dialog systems require\ndomain-speciﬁc labelled data: annotations for in-\ntents, explicit dialog states, and mentioned entities\n(Williams, 2014; Wen et al., 2017b,a; Ramadan\net al., 2018; Liu et al., 2018; Zhao et al., 2019b).\nThis makes the scaling and maintenance of such\nsystems very challenging. Transfer learning on top\nof pretrained models (Devlin et al., 2019; Liu et al.,\n2019, inter alia) provides one avenue for reduc-\ning the amount of annotated data required to train\nmodels capable of generalization.\nPretrained models making use of language-\nmodel (LM) based learning objectives have be-\ncome prevalent across the NLP research commu-\nnity. When it comes to dialog systems, response\nselection provides a more suitable pretraining task\nfor learning representations that can encapsulate\nconversational cues. Such models can be pretrained\nusing large corpora of natural unlabelled conversa-\ntional data (Henderson et al., 2019b; Mehri et al.,\n2019). Response selection is also directly appli-\ncable to retrieval-based dialog systems, a popular\nand elegant approach to framing dialog (Wu et al.,\n2017; Weston et al., 2018; Mazar ´e et al., 2018;\nGunasekara et al., 2019; Henderson et al., 2019b).1\nResponse Selection is a task of selecting the\nmost appropriate response given the dialog history\n(Wang et al., 2013; Al-Rfou et al., 2016; Yang et al.,\n2018; Du and Black, 2018; Chaudhuri et al., 2018).\nThis task is central to retrieval-based dialog sys-\ntems, which typically encode the context and a\n1Retrieval-based dialog is popular because posing dialog as\nresponse selection (Gunasekara et al., 2019) simpliﬁes system\ndesign (Boussaha et al., 2019). Unlike modular or end-to-\nend task-oriented systems, retrieval-based ones do not rely on\ndedicated modules for language understanding, dialog man-\nagement, and generation. They mitigate the requirements\nfor explicit task-speciﬁc semantics hand-crafted by domain\nexperts (Henderson et al., 2014; Mrkˇsi´c et al., 2015, 2017).\n2162\nlarge collection of responses in a joint semantic\nspace, and then retrieve the most relevant response\nby matching the query representation against the en-\ncodings of each candidate response. The key idea is\nto: 1) make use of large unlabelled conversational\ndatasets (such as Reddit conversational threads) to\npretrain a neural model on the general-purpose re-\nsponse selection task; and then 2) ﬁne-tune this\nmodel, potentially with additional network layers,\nusing much smaller amounts of task-speciﬁc data.\nDual-encoder architectures pretrained on re-\nsponse selection have become increasingly popular\nin the dialog community (Cer et al., 2018; Humeau\net al., 2020; Henderson et al., 2019b). In recent\nwork, Henderson et al. (2019a) show that standard\npretraining LM-based architectures cannot match\nthe performance of dual encoders when applied to\ndialog tasks such as response retrieval.\nScalability and Portability. A fundamental prob-\nlem with pretrained models is their large number\nof parameters (see Table 2 later): they are typ-\nically highly computationally expensive to both\ntrain and run (Liu et al., 2019). Such high memory\nfootprints and computational requirements hinder\nquick deployment as well as their wide portabil-\nity, scalability, and research-oriented exploration.\nThe need to make pretrained models more com-\npact has been recognized recently, with a line of\nwork focused on building more efﬁcient pretrain-\ning and ﬁne-tuning protocols (Tang et al., 2019;\nSanh et al., 2019). The desired reductions have\nbeen achieved through techniques such as distilla-\ntion (Sanh et al., 2019), quantization-aware training\n(Zafrir et al., 2019), weight pruning (Michel et al.,\n2019) or weight tying (Lan et al., 2019). However,\nthe primary focus so far has been on optimizing the\nLM-based pretrained models, such as BERT.\nConveRT.This work introduces a more compact\npretrained response selection model for dialog.\nConveRT is only 59MB in size, making it signif-\nicantly smaller than the previous state-of-the-art\ndual encoder for dialog applications (444MB). It is\nalso more compact than other popular sentence en-\ncoders, as illustrated in Table 2. This notable reduc-\ntion in size and training acceleration are achieved\nthrough combining 8-bit embedding quantization\nand quantization-aware training, subword-level pa-\nrameterization, and pruned self-attention. Further-\nmore, the lightweight design allows us to reserve\nadditional parameters to improve the expressive-\nness of the dual-encoder architecture; this leads\nto improved learning of conversational representa-\ntions that can be transferred to other dialog tasks\nsuch as intent detection and slot ﬁlling, as already\ndemonstrated by recent work (Casanueva et al.,\n2020; Bunk et al., 2020; Coope et al., 2020).\nMulti-Context Modeling. ConveRT moves be-\nyond the simplifying single-context assumption\nmade by Henderson et al. (2019b), where only the\nimmediate preceding context was used to look for\na relevant response. We propose a multi-context\ndual-encoder model which combines the immediate\ncontext with previous dialog history in the response\nselection task. The multi-context ConveRT variant\nremains compact (73MB in total), while offering\nimproved performance on a range of established re-\nsponse selection tasks. We report signiﬁcant gains\nover the previous state-of-the-art on benchmarks\nsuch as Ubuntu DSTC7 (Gunasekara et al., 2019),\nAmazonQA (Wan and McAuley, 2016) and Red-\ndit response selection (Henderson et al., 2019a),\nboth in single-context and multi-context scenar-\nios. Moreover, we show that sentence encodings\nlearned by the model can be transferred to other\ndialog tasks, reaching strong intent classiﬁcation\nperformance over three evaluation sets.2\n2 Methodology\nPretraining on Reddit Data. We assume working\nwith English throughout the paper. Simplifying the\nconversational learning task to response selection,\nwe can relate target dialog tasks to general-domain\nconversational data such as Reddit (Al-Rfou et al.,\n2016). This allows us to ﬁne-tune the parameters of\nthe task-speciﬁc response selection model, starting\nfrom the general-domain response selection model\npretrained on Reddit. Similar to Henderson et al.\n(2019b), we choose Reddit for pretraining due to:\n1) its organic conversational structure; and 2) its\nunmatched size, as the public repository of Reddit\ndata comprises 727M (input, response) pairs.3\nDual-Encoder for Response Selection. A dual-\nencoder neural architecture for response selection\n2Finally, our more compact neural response selection archi-\ntecture is well aligned with the recent socially-aware initiatives\non reducing costs and improving fairness and inclusion in NLP\nresearch and practice (Strubell et al., 2019; Mirzadeh et al.,\n2019; Schwartz et al., 2019). Cheaper training (pretraining\nthe proposed dual-encoder model on the entire Reddit costs\nonly 85 USD) and quicker development cycles offer new op-\nportunities for more researchers and practitioners to tap into\nthe construction of neural task-based dialog systems.\n3github.com/PolyAI-LDN/\nconversational-datasets\n2163\nin task-based dialog has been introduced by Hen-\nderson et al. (2019b), which closely follows a\nrelated line of work focused on modelling sen-\ntence pairs for short text retrieval (Kannan et al.,\n2016; Henderson et al., 2017), bilingual text min-\ning and representation learning (Guo et al., 2018;\nChidambaram et al., 2019), and question answer-\ning (Humeau et al., 2020). In what follows in §2.1,\nwe: 1) introduce ConveRT, our novel single-context\ndual-encoder architecture; and 2) brieﬂy outline the\nquantization method. Finally, we show how to ex-\ntend ConveRT into a multi-context dual encoder\nthat works with additional context inputs (§2.2).\n2.1 More Compact Response Selection Model\nWe propose ConveRT – Conversational\nRepresentations from Transformers – a compact\ndual-encoder pretraining architecture, leveraging\nsubword representations, transformer-style blocks,\nand quantization, as illustrated in Figure 1.\nConveRT satisﬁes all the following requirements:\nit is effective, affordable, and quick to train.\nInput and Response Representation. Prior to\ntraining, we obtain a vocabulary of subwords V\nshared by the input side and the response side: we\nrandomly sample and lowercase 10M sentences\nfrom Reddit, and then iteratively run any subword\ntokenization algorithm.4 The ﬁnal vocabulary V\ncontains 31,476 subword tokens. During training\nand inference, if we encounter an OOV character it\nis treated as a subword token, where its ID is com-\nputed using a hash function, and it gets assigned\nto one of 1,000 additional “buckets” reserved for\nthe OOVs. We therefore reserve parameters (i.e.,\nembeddings) for the 31,476 subwords from V and\nfor the additional 1,000 OOV-related buckets. At\ntraining and inference, after the initial word-level\ntokenization on UTF8 punctuation and word bound-\naries, input text x is split into subwords follow-\ning a simple left-to-right greedy preﬁx matching\n(Vaswani et al., 2018). We tokenize all responses y\nduring training in exactly the same manner.\nInput and Response Encoder Networks. The\nsubword embeddings then go through a series of\ntransformations on both the input and the response\nside. The transformations are based on the standard\n4In the actual implementation, we use the same subword\ntokenization as Vaswani et al. (2018). We run it for 4 itera-\ntions and retain only subwords occurring at least 250 times,\ncontaining no more than 20 UTF8 characters, also disallowing\nmore than 4 consecutive digits.\nx6\n x6\nshared parameters\ninput: x response: y\nsimilarity score\nsegmentation\ninto subwords\nsegmentation\ninto subwords\nsubword\nembeddings\npositional\nencodings\nself-attention\nadd & \nnormalize\nadd & \nnormalize\nTransfer\nencodings\n(e.g., intent\ndetection)\nadd & \nnormalize\nadd & \nnormalize\nsubword\nembeddings\npositional\nencodings\nself-attention\nfeed-forward 1 feed-forward 1\n2-headed\nself-attention\n2-headed\nself-attention\nsqrt N\nreduction\nsqrt N\nreduction\nfeed-forward 2 feed-forward 2\nrx ry\nhx hyhT\nx hy\nTransformer layers\nFigure 1: Single-context ConveRT dual-encoder model\narchitecture. Its multi-context extension is illustrated\nin Figure 2. It is possible to transfer learned encodings\nat different network layers (e.g., rx or the ﬁnal hx) to\nother tasks such as intent detection or value extraction\n(see §4). Note that the model uses two different feed-\nforward network (FFN) layers: 1) feed-forward 1 is the\nstandard FFN layer also used by Vaswani et al. (2017),\nand 2) feed-forward 2 contains 3 fully-connected non-\nlinear feed-forward layers followed by a linear layer\nwhich maps to the ﬁnal encodings hx and hy (note that\nthe two feed-forward 2 networks do not share parame-\nters, while the feed-forward 1 parameters are shared).\nTransformer architecture (Vaswani et al., 2017).\nBefore going through the self-attention blocks, we\nadd positional encodings to the subword embed-\nding inputs. Previous work (e.g., BERT and related\nmodels) (Devlin et al., 2019; Lan et al., 2019, inter\nalia) learns a ﬁxed number of positional encodings,\none for each position in the sequence, allowing the\nmodel to represent a ﬁxed number of positions. In-\nstead, we learn two positional encoding matrices of\ndifferent sizes- M1 of dimensionality [47, 512] and\nM2 of dimensionality [11, 512]. An embedding at\nposition iis added to: M1\ni mod 47 + M2\ni mod 11.5\n5Note that since 47 and 11 are coprime, this gives47·11 =\n517 different possible positional encodings. Similar to the\noriginal (non-learned) positional encodings from Vaswani et al.\n(2017), the rationale behind this choice of positional encoding\nis to allow the model to generalize to unseen sequence lengths.\n2164\nThe next layers closely follow the original Trans-\nformer architecture with some notable differences.\nFirst, we set maximum relative attention (Shaw\net al., 2018) in the six layers to the following respec-\ntive values: [3, 5, 48, 48, 48, 48].6 This also helps\nthe architecture to generalize to long sequences\nand distant dependencies: earlier layers are forced\nto group together meanings at the phrase level be-\nfore later layers model larger patterns (Singh et al.,\n2019). We use single-headed attention throughout\nthe network.7\nBefore going into a softmax, we add a bias to\nthe attention scores that depends only on the rel-\native positions: αij →αij + Bn−i+j where B is\na learned bias vector. This helps the model under-\nstand relative positions, but is much more computa-\ntionally efﬁcient than computing full relative posi-\ntional encodings (Shaw et al., 2018). Again, it also\nhelps the model generalize to longer sequences.\nSix Transformer blocks use a 64-dim projection\nfor computing attention weights, a 2,048-dim ker-\nnel (feed-forward 1 in Figure 1), and 512-dim em-\nbeddings. Note that all Transformer layers use\nparameters that are fully shared between the input\nside and the response side. As in the Universal\nSentence Encoder (USE ) (Cer et al., 2018), we use\nsquare-root-of-N reduction to convert the embed-\nding sequences to ﬁxed-dimensional vectors. Two\nself-attention heads each compute weights for a\nweighted sum, which is scaled by the square root\nof the sequence length; the length is computed as\nthe number of constituent subwords.8 The outputs\nof the reduction layer, labelled rx and ry in Fig-\nure 1, are 1,024-dimensional vectors that are fed\nto the two “side-speciﬁc” (i.e., they do not share\nparameters) feed-forward networks.\nIn other words, the vectors rx and ry go through\na series of Nf l-dim feed-forward hidden layers\n(Nf = 3; l = 1,024) with skip connections,\nlayer normalization, and orthogonal initialization.\nThe activation function used in these networks\n6We zero out in training and inference the attention scores\nfor pairs of words if they are further apart than the set maxi-\nmum relative attention values.\n7Multi-headed attention requires running computations on\n4-tensors: [batch, time, head, embedding], while for single-\nheaded attention, this reduces to 3-tensors, and effectively\nspeeds up training without hurting performance.\n8In fact, rather than computing the self-attended sequence,\nthen reducing it, we reduce the attention weights accordingly,\nand then directly apply them via matrix multiplication to the\ninput sequence to get the ﬁnal reduced representation, that is,\nwe fuse these two operations. This is more computationally\nefﬁcient, avoiding another 3-tensor multiplication.\nand throughout the architecture is the fast GeLU\napproximation (Hendrycks and Gimpel, 2016):\nGeLU(x) = xσ(1.702x). The ﬁnal layer is lin-\near and maps the text into the ﬁnal L2-normalized\n512-dim representation: hx for the input text, and\nhy for the corresponding response text (Figure 1).\nInput-Response Interaction. The relevance of\neach response to the given input is then quantiﬁed\nby the score S(x,y), computed as cosine similar-\nity with annealing between the encodings hx and\nhy. It starts at 1 and ends at\n√\nd, linearly increas-\ning over the ﬁrst 10K training batches. Training\nproceeds in batches of K (input, response) pairs\n(x1,y1),..., (xK,yK). The aim of the objective is\nto distinguish between the true relevant response\n(yi) and irrelevant responses (i.e., negative samples)\nyj,j ̸= ifor each input sentence xi. The training\nobjective for a single batch ofKpairs is as follows:\nJ = ∑K\ni=1 S(xi,yi) −∑K\ni=1 log ∑K\nj=1 eS(xi,yj ).\nThe goal is to maximize the score of positive train-\ning pairs (xi,yi) and minimize the score of pairing\neach input xi with K′negative examples, which\nare responses that are not associated with the input\nxi: for simplicity, all other K−1 responses from\nthe current batch are used as negative examples.\nQuantization. Very recent work has shown that\nlarge models of language can be made more\ncompact by applying quantization techniques\n(Han et al., 2016): e.g., quantized versions of\nTransformer-based machine translation systems\n(Bhandare et al., 2019) and BERT (Shen et al.,\n2019; Zhao et al., 2019a; Zafrir et al., 2019) are\nnow available. In this work, we focus on enabling\nquantization-aware conversational pretrainingon\nthe response selection task. We show that the dual-\nencoder ConveRT model from Figure 1 can be also\nbe trained in a quantization-aware manner. Rather\nthan the standard 32-bits per parameter, all embed-\nding parameters are represented using only 8 bits,\nand other network parameters with just 16 bits;\nthey are trained in a quantization-aware manner\nby adapting the mixed precision training scheme\nfrom Micikevicius et al. (2018). It keeps shadow\ncopies of each variable with 32bit Floating Point\n(FP32) precision, but uses FP16-cast versions in the\ncomputations and inference models. Some opera-\ntions in the graph, however, require FP32 precision\nto be numerically stable: layer normalization, L2-\nnormalization, and softmax in attention layers.\nAgain, following Micikevicius et al. (2018), the\nﬁnal loss is scaled by 128, and the updates to the\n2165\nresponse: y\nTransformer\nlayers\nTransformer\nlayers\nTransformer\nlayers\ninput: x\n(immediate context)\ninput: z\n(string concatenation of all \nearlier contexts up to 10 back)\nmean\nfeed-forward 2 feed-forward 2 feed-forward 2\nfeed-forward 2\nﬁnal score\nhy hzhx\nrx ry rz\nhx,z\nhT\nx hy hT\nz    hy\nhT\nx,z     hy\nFigure 2: Multi-context ConveRT. It models 1) the inter-\naction between the immediate context and its accompa-\nnying response, 2) the interaction of the response with\nup to 10 earlier contexts from the conversation history,\nas well as 3) the interaction of the full context with\nthe response. Transformer layers refer to the standard\nTransformer architecture also used in the single-context\nencoder model in Figure 1; the feed-forward 2 blocks\nare the same as with the single-context encoder archi-\ntecture, see Figure 1. The block mean refers to simple\naveraging of two context encodings hx and hz.\nshadow FP32 variables are scaled back by 1/128:\nthis allows the gradient computations to stay well\nrepresented by FP16 (e.g., they will not get rounded\nto zero). The subword embeddings are stored using\n8-bits per parameter, and the quantization range\nis adjusted dynamically through training. It is up-\ndated periodically to contain all of the embedding\nvalues that have so-far been learned, with room for\ngrowth above and below - 10% of the range, or\n0.01 - whichever is larger. Finally, quantization\nalso allows doubling the batch size, which also\nhas a favorable effect of increasing the number of\nnegative examples in training.\n2.2 Multi-Context ConveRT\nFigure 1 depicts a single-context dual encoder ar-\nchitecture. Intuitively, the single-context assump-\ntion is limiting for modeling multi-turn conver-\nsations, where strong conversational cues can be\nfound in earlier dialog history, and there has been a\nbody of work on leveraging richer dialog history for\nresponse selection (Chaudhuri et al., 2018; Zhou\net al., 2018; Humeau et al., 2020). Taking a simple\nillustrative example:\nStudent: I’m very interested in representation learning.\nTeacher: Do you have any experience in PyTorch?\nStudent: Not really.\nTeacher: And what about TensorFlow?\nSelecting the last Teacher’s response would be very\ndifﬁcult given only the immediate preceding Stu-\ndent’s context. However, the task becomes easier\nwhen taking into account the entire context of the\nconversation. We thus construct a multi-context\ndual-encoder model by using up to 10 more pre-\nvious messages in a Reddit thread. The extra 10\ncontexts are concatenated from most recent to old-\nest, and treated as an extra feature in the network,\nas shown in Figure 2. The order of contexts is im-\nportant when doing sequence truncation in training,\nand it is still more important for the model to see\nthe most recent messages.\nNote that all context representations are still in-\ndependent from the representation of a candidate\nresponse, so we can still do efﬁcient response re-\ntrieval and training. The full training objective is a\nlinear combination of three sub-objectives: 1) rank-\ning responses given the immediate context (i.e.,\nthis is equal to the single-context model from§2.1),\n2) ranking responses given only the extra (non-\nimmediate) contexts, and 3) ranking responses\ngiven the averaged representation of the immediate\ncontext and additional contexts.9\n3 Experimental Setup\nTraining Data and Setup. We base all our\n(pre)training on the large Reddit conversational cor-\npus (Henderson et al., 2019a) derived from 3.7B\nReddit comments: it comprises 727M (input, re-\nsponse) pairs for single-context modeling – 654M\npairs are reserved for training, the rest is used for\ntesting. We truncate sequences to 60 subwords, em-\nbedding size is set to 512 for all subword embed-\ndings and bucket embeddings, and the ﬁnal encod-\nings hx, hy, hz, and hx,z are all 512-dimensional.\nThe hidden layer size of feed forward 2 networks\nis set to 1,024 (with Nf = 3hidden layers used).\nWe train using ADADELTA with ρ = 0 .9\n9Combining multiple objectives in a dual-encoder frame-\nwork has also been done by Al-Rfou et al. (2016) and Hen-\nderson et al. (2017). Note that more sophisticated solutions to\nfusing dialog history are possible such as using attention over\nolder contexts as done by Vlasov et al. (2019) on the much\nsmaller MultiWOZ 2.1 dataset (Eric et al., 2019), but we have\nopted for simple concatenation as an efﬁcient solution for\ntraining on the large Reddit data. The multiple objectives re-\nsult in quicker learning, and also give useful diagnostic probes\ninto the performance of each feature throughout training.\n2166\n(Zeiler, 2012), batch size of 512, and a learning\nrate of 1.0 annealed to 0.001 with cosine decay\nover training. L2-regularization of 10−5 is used,\nsubword embedding gradients are clipped to 1.0,\nand label smoothing of 0.2 is applied.10\nWe pretrain the model on Reddit on 12 GPU\nnodes with one Tesla K80 each for 18 hours; this is\ntypically sufﬁcient to reach convergence. The total\npretraining cost is roughly $85 on Google Cloud\nPlatform. This pretraining regime is orders of mag-\nnitude cheaper and more efﬁcient than the prevalent\npretrained NLP models such as BERT, GPT-2, XL-\nNet, and RoBERTa (Strubell et al., 2019).11\nBaselines. We report results on the response se-\nlection tasks and compare against the standard set\nof baselines (Henderson et al., 2019a). First, we\ncompare to a simple keyword matching baseline\nbased on TF-IDF query-response scoring (Manning\net al., 2008), and then with a representative sample\nof publicly available neural encoders that embed\ninputs and responses into a vector space relying\non various pretraining objectives: (1) The larger\nvariant of Universal Sentence Encoder (Cer et al.,\n2018) (USE -LARGE ); (2) The large variant of BERT\n(Devlin et al., 2019) (BERT-LARGE ). We also com-\npare to two recent dual-encoder architectures: (3)\nUSE -QA is a dual question-answer encoder version\nof the USE (large) model (Chidambaram et al.,\n2019).12 (4) POLYAI -DUAL is the best-performing\ndual-encoder model from Henderson et al. (2019b)\npretrained on Reddit response selection. For base-\nline models 1-3, we report the results with the\nMAP response selection variant (Henderson et al.,\n2019a): it showed much stronger performance than\na simpler similarity-based variant which directly\nranks responses according to their cosine similarity\nwith the context vector. MAP learns to (linearly)\nmap the response vectors to the input vector space.\nResponse Selection: Evaluation Tasks. We re-\nport response selection performance on Reddit test\nset (Henderson et al., 2019a) with both single-\n10The label smoothing technique (Szegedy et al., 2016)\nreduces overﬁtting by preventing a network to assign full\nprobability to the correct training example (Pereyra et al.,\n2017). It means that each positive example in each batch is\nassigned the probability of 0.8, while the remaining probability\nmass is evenly redistributed across in-batch negative examples.\n11Cost is estimated using Google Cloud Platform, includes\nthe cost of auxiliary servers such as CPU parameter servers,\nand assumes the use of pre-emptible GPU workers.\n12Note that USE -QA encodes inputs/contexts and responses\nusing separate sub-networks, while ConveRT (Figure 1) relies\non full parameter sharing in the Transformer layers.\ncontext and multi-context ConveRT variants. For\nmulti-context ConveRT, the averaged representa-\ntion of (immediate and previous) context is used\nin evaluation. The models are applied directly\non the Reddit test data without any further ﬁne-\ntuning. We also evaluate on two other well-known\nresponse selection problems in different domains.\n(1) AMAZON QA (Wan and McAuley, 2016) is an\ne-commerce data set which contains information\nabout Amazon products in the form of question-\nanswer pairs:out of 3.6M (single-context) QA pairs,\n300K pairs are reserved for testing. (2) DSTC 7-\nUBUNTU is based on the Ubuntu v2 corpus (Lowe\net al., 2017): it contains 1M+ conversations in a\nhighly technical domain (i.e., Ubuntu technical sup-\nport). DSTC 7-UBUNTU uses 100K conversations\nfor training, 10K for validation, and 5K conversa-\ntions are used for testing (Gunasekara et al., 2019).\nFor DSTC 7-UBUNTU we ﬁne-tune for 60K train-\ning steps: it takes around 2h on 12 GPU workers.\nThe learning rate starts at 0.1, and is annealed to\n0.0001 using cosine decay over training. We use a\nbatch size of 256, and dropout of 0.2 after the em-\nbedding and self-attention layers. We use the same\nﬁne-tuning regime for AMAZON QA. For DSTC 7-\nUBUNTU , extra contexts are prepended with nu-\nmerical strings 0–9 to help the model identify their\nposition. We also release the ﬁne-tuned models.\nWe evaluate with a standard IR-inspired eval-\nuation measure: Recall@k, used in prior work\non retrieval-based dialog (Chaudhuri et al., 2018;\nHenderson et al., 2019b; Gunasekara et al., 2019).\nGiven a set of N responses to the given input,\nwhere only one response is relevant, it indicates\nwhether the relevant response occurs in the top\nk ranked candidates. We denote this measure as\nRN @k, and set N = 100;k= 1: R100@1.\nIntent Classiﬁcation: Task, Data, Setup. Pre-\ntrained sentence encoders have become particularly\npopular due to the success of training models for\ndownstream tasks on top of their learned represen-\ntations, greatly improving the results compared\nto training from scratch, especially in low-data\nregimes (see Table 1). Therefore, we also probe\nthe usefulness of ConveRT encodings for transfer\nlearning in the intent classiﬁcation task: the model\nmust classify the user’s utterance into one of sev-\neral predeﬁned classes, that is, intents (e.g., within\ne-banking intents can be card lost or replace card).\nWe use BANKING 77 (Casanueva et al., 2020) plus\ntwo internal intent classiﬁcation datasets from three\n2167\n# intents # examples\nBanking (customer service) 77 14.6K\nShopping (online shopping) 10 13.8K\nCompany FAQ 110 3.3K\nTable 1: Intent classiﬁcation data sets.\ndiverse domains, see Table 1, divided into train, dev\nand test sets using a 80/10/10 split.\nWe use the pretrained ConveRT encodingsrx on\nthe input side (see Figure 1) as input to an intent\nclassiﬁcation model. We also experimented with\nlater hx encodings on the input side, but stronger\nresults were observed with rx. We train a 2-layer\nfeed-forward net with dropout on top of rx. SGD\nwith a batch size of 32 is used, with early stopping\nafter 5 epochs without improvement on the vali-\ndation set. Layer sizes, dropout rate and learning\nrate are selected through grid search. We compare\nagainst two other standard sentence encoders again:\nUSE -LARGE and BERT-LARGE . For ConveRT and\nUSE -LARGE we keep the encoders ﬁxed and train\nthe classiﬁer layers on top of the sentence encod-\nings. For BERT-LARGE , we train on top of the CLS\ntoken and we ﬁne-tune all its parameters.\n4 Results and Discussion\nModel Size, Training Time, Cost. Table 2 lists\nencoders from prior work along with their model\nsize, and estimated model size after quantization.\nThe reported numbers indicate the gains achieved\nthrough subword-level parameterization and quanti-\nzation of ConveRT. Besides reduced training costs,\nConveRT offers a reduced memory footprint and\nquicker training. We pretrain all our models for 18\nhours only (on 12 16GB T4 GPUs), while a model\ncompression technique DistilBERT (Sanh et al.,\n2019) (i.e., it reports ≈40% relative reduction of\nthe original BERT) trains on 8 16GB V100 GPUs\nfor 90 hours, and larger models like RoBERTa re-\nquire 1 full day of training on 1,024 32GB V100\nGPUs. The achieved size reduction and quick train-\ning also allow for quicker development and insight-\nful ablation studies (see later in Table 4), and using\nquantization also improves training efﬁciency in\nterms of examples per second.\nResponse Selection on Reddit. The results are\nsummarized in Table 3. Even single-context Con-\nveRT achieves peak performance in the task, with\nsubstantial gains over the previous best reported\nscore of Henderson et al. (2019b). It also sub-\nstantially outperforms all the other models which\nwere not pretrained directly on the response se-\nlection task, but on a standard LM task instead.\nThe strongest baselines, however, are two dual-\nencoder architectures (i.e., USE -LARGE , USE -QA\nand POLYAI -DUAL ); this illustrates the importance\nof explicitly distinguishing between inputs/contexts\nand responses when modeling response selection.\nTable 3 also shows the importance of lever-\naging additional contexts (see Figure 2). Multi-\ncontext ConveRT achieves a state-of-the-art Reddit\nresponse selection score of 71.8%. We observe\nsimilar beneﬁts in other reported response selec-\ntion tasks. We also note the results of 1) using\nonly the sub-network that models the interaction\nbetween the immediate context and the response\n(i.e., the hT\nx hy interaction), and 2) artiﬁcially re-\nplacing the concatenated extra contexts zwith an\nempty string. The respective scores are 65.7% and\n65.6%. This suggests that multi-context ConveRT\nis also applicable to single-context scenarios when\nno extra contexts are provided for the target task.\nWe have also veriﬁed that the ﬁrst context from\nthe dialog history is most beneﬁcial for perfor-\nmance of the multi-context ConveRT variant with a\nsimple experiment. When using only the ﬁrst con-\ntext from the dialog history, performance on Reddit\ndrops only slightly: from 71.8% (10 contexts from\nhistory) to 71.1% with the full model including\nthe immediate preciding context, and from 34.0%\n(10 contexts) to 30.9% (1 context only) when we\nexclude the intermediate context (see §2.2 again).\nAblation Study. The efﬁcient training regime also\nallows us to perform a variety of diagnostic exper-\niments and ablations. We report results with vari-\nants of single-context ConveRT in Table 4. They\nindicate that replacing single-headed with multi-\nheaded attention leads to slight improvements, but\nthis comes at a cost of slower (and consequently -\nmore expensive) training. Using 1 instead of 1,000\nOOV buckets leads only to a modest decrease in\nperformance. Most importantly, the ablation study\nindicates that the ﬁnal performance actually comes\nfrom the synergistic effect of applying a variety\nof components and technical design choices such\nas skip connections, 2-headed reductions, relative\nposition biases, etc. While removing only one com-\nponent at a time yields only modest performance\nlosses, the results show that the loss adds up as\nwe remove more components, and different com-\n2168\nEmbedding Network Total Size after\nparameters parameters size quantization\nUSE (Cer et al., 2018) 256 M 2 M 1033 MB 261 MB *\nBERT-BASE (Devlin et al., 2019) 23 M 86 M 438 MB 196 MB */ 110 MB **\nBERT-LARGE (Devlin et al., 2019) 31 M 304 M 1341 MB 639 MB */ 336 MB **\nGPT (Radford et al., 2018) 31 M 86 M 468 MB 203 MB *\nGPT-2 (Radford et al., 2019) 80 M 1462 M 6168 MB 3004 MB *\nPOLY AI-DUAL (Henderson et al., 2019b) 104 M 7 M 444 MB 118 MB\nConveRT (this work) 16 M 13 M 116 MB 59 MB\nTable 2: Comparison of the proposed compact dual-encoder architecture for response selection to existing public\nstandard sentence embedding models. (*) The size after quantization assumes embeddings can be quantized to 8\nbits and network parameters to 16 bits, which has not been veriﬁed for the public models. (**) Best-case model\nsize estimates of the BERT model after full 8-bit quantization based on the work of Zafrir et al. (2019).\nReddit AmazonQA\nTF-IDF 26.4 51.8\nUSE -LARGE -MAP 47.7 61.9\nBERT-LARGE -MAP 24.0 44.1\nUSE -QA-MAP 46.6 70.7\nPOLYAI -DUAL 61.3 71.3\nConveRT (single-context) 68.2 84.3\nConveRT (multi-context) 71.8 –\nTable 3: R100@1 ×100% scores on Reddit test set and\nAMAZON QA. POLYAI -DUAL and ConveRT networks\nare ﬁne-tuned on the training portion of AMAZON QA.\nNote that AMAZON QA by design supports only single-\ncontext response selection.\nModel Conﬁguration\nConveRT 68.2\nA: Multi-headed attention (8 64-dim heads) 68.5\nB: No relative position bias 67.8\nC: Without gradually increasing max attention span 67.7\nD: Only 1 OOV bucket 68.0\nE: 1-headed (instead of 2-headed) reduction 67.7\nF: No skip connections in feed forward 2 67.8\nD + E + F 66.7\nB + C + D + E + F 66.6\nTable 4: An ablation study illustrating the importance\nof different components in ConveRT: single-context re-\nsponse selection on Reddit ( R100@1). Each experi-\nment has been run for 966K steps (batch size 512).\nR100@1 MRR\nBest DSTC7 System 64.5 73.5\nGPT* 48.9 59.5\nBERT* 53.0 63.2\nBi-encoder (Humeau et al., 2020) 70.9 78.1\nConveRT (single-context) 38.2 49.2\nConveRT (multi-context) 71.2 78.8\nTable 5: Results on DSTC 7-UBUNTU . (*) Scores for\nGPT and BERT taken from Vig and Ramea (2019).\nponents indeed contribute to the ﬁnal score.13\n13Furthermore, quick development and short training times\nOther Response Selection Tasks. The results on\nthe AMAZON QA task are provided in Table 3.\nWe see similar trends as with Reddit evaluation.\nFine-tuned ConveRT reaches a new state-of-the-\nart score, and the strongest baselines are again\ndual-encoder networks. Fine-tuned POLYAI -DUAL ,\nwhich was pretrained on exactly the same data,\ncannot match ConveRT’s performance.\nInterestingly, directly applying ConveRT to\nAMAZON QA without any ﬁne-tuning also yields a\nreasonably high score of 67.0%. Moreover, learn-\ning the mapping function between inputs and re-\nsponses (again without any ﬁne-tuning) for Con-\nveRT the same way as is done for USE -QA-MAP\nresults in the score of 71.6%, which outperforms\nUSE -QA-MAP (70.7%). The gap to the ﬁne-tuned\nmodel’s performance, however, indicates the im-\nportance of in-domain ﬁne-tuning.\nThe results on DSTC 7-UBUNTU are summarized\nin Table 5. First, they suggest very competitive per-\nformance of multi-context ConveRT model: it out-\nperforms the best-scoring system from the ofﬁcial\nDSTC7 challenge (Gunasekara et al., 2019). It is an\nencouraging ﬁnding, given that multi-context Con-\nveRT relies on simple context concatenation with-\nout any additional attention mechanisms. We leave\nthe investigation of such more sophisticated mod-\nels to integrate additional contexts for future work.\nMulti-context ConveRT can also match or even sur-\npass the performance of another dual-encoder archi-\ntecture from Humeau et al. (2020). Their dual en-\ncoder (i.e., bi-encoder) is based on the BERT-base\narchitecture (Humeau et al., 2020): it relies on 12\nTransformer blocks, 12 attention heads, and a hid-\nden size dimensionality of 768 (while we use 512).\nalso allow us to treat some of the component choices as hyper-\nparameter choices. It effectively means that such conﬁguration\nchoices can also be ﬁne-tuned similar to any other hyper-\nparameter to optimize the ﬁnal retrieval performance.\n2169\nBanking Shopping Company FAQ\nUSE -LARGE 92.2 94.0 62.4\nBERT-LARGE 93.2 94.3 61.2\nConveRT 92.7 94.5 64.3\nTable 6: Intent classiﬁcation results.\nTraining with that model is roughly 5×slower, and\nthe pretraining objective is more complex: they use\nthe standard BERT pretraining objective plus next\nutterance classiﬁcation. Moreover, their model is\ntrained on 32 v100 GPUs for 14 days, which makes\nit roughly 50×more expensive than ConveRT.\nIntent Classiﬁcation. The results are summarized\nin Table 6: we report the results of two strongest\nbaselines. The scores show very competitive perfor-\nmance of ConveRT encodings rx transferred to an-\nother dialog task. They outperform USE -LARGE in\nall three tasks and BERT-LARGE in 2/3 tasks. Note\nthat, besides quicker pretraining, intent classiﬁers\nbased on ConveRT encodings train 40 times faster\nthan BERT-LARGE -based ones, as only the classiﬁ-\ncation layers are trained for ConveRT. Additional\nexperiments related to efﬁciency of intent classi-\nﬁcation have been conducted by Casanueva et al.\n(2020). In sum, these preliminary results suggest\nthat ConveRT as a sentence encoder can be useful\nbeyond the core response selection task. The use-\nfulness of ConveRT-based sentence representations\nhave been recently conﬁrmed on other intent clas-\nsiﬁcation datasets (Casanueva et al., 2020), with\ndifferent intent classiﬁers (Bunk et al., 2020), and\nin another dialog task: turn-based value extraction\n(Coope et al., 2020; Bunk et al., 2020; Mehri et al.,\n2020). In future work, we plan to investigate other\npossible applications of transfer, especially for the\nchallenging low-data setups.\n5 Conclusion\nWe have introduced ConveRT, a new light-weight\nmodel of neural response selection for dialog,\nbased on Transformer-backed dual-encoder net-\nworks, and have demonstrated its state-of-the-art\nperformance on an array of response selection tasks\nand in transfer learning for intent classiﬁcation\ntasks. In addition to offering more accurate con-\nversational pretraining models this work has also\nresulted in more compact conversational pretrain-\ning. The quantized versions of ConveRT and multi-\ncontext ConveRT take up only 59 MB and 73 MB,\nrespectively, and train for 18 hours with a training\ncost estimate of only 85 USD. We hope that this\nwork will motivate and guide further developments\nin the areas of retrieval-based task-oriented dialog\nand large-scale pretraining for conversational ap-\nplications (Mehri et al., 2020).\nAcknowledgments\nWe thank our colleagues at PolyAI, especially\nPaweł Budzianowski, Sam Coope, Daniela Gerz,\nfor many fruitful discussions and suggestions.\nReferences\nRami Al-Rfou, Marc Pickett, Javier Snaider, Yun-\nHsuan Sung, Brian Strope, and Ray Kurzweil. 2016.\nConversational contextual cues: The case of person-\nalization and history for response ranking. CoRR,\nabs/1606.00372.\nAishwarya Bhandare, Vamsi Sripathi, Deepthi\nKarkada, Vivek Menon, Sun Choi, Kushal Datta,\nand Vikram Saletore. 2019. Efﬁcient 8-bit quan-\ntization of transformer neural machine language\ntranslation model. CoRR, abs/1906.00532.\nBasma El Amel Boussaha, Nicolas Hernandez, Chris-\ntine Jacquin, and Emmanuel Morin. 2019. Deep\nretrieval-based dialogue systems: A short review.\nCoRR, abs/1907.12878.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I ˜nigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Ga ˇsi´c. 2018. MultiWOZ - A\nlarge-scale multi-domain wizard-of-oz dataset for\ntask-oriented dialogue modelling. In Proceedings of\nEMNLP, pages 5016–5026.\nTanja Bunk, Daksh Varshneya, Vladimir Vlasov,\nand Alan Nichol. 2020. DIET: Lightweight lan-\nguage understanding for dialogue systems. CoRR,\nabs/2004.09936.\nI˜nigo Casanueva, Tadas Tem ˇcinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vuli´c. 2020. Efﬁcient\nintent detection with dual sentence encoders. In Pro-\nceedings of the 2nd Workshop on Natural Language\nProcessing for Conversational AI, pages 38–45.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil.\n2018. Universal sentence encoder for English. In\nProceedings of EMNLP, pages 169–174.\nDebanjan Chaudhuri, Agustinus Kristiadi, Jens\nLehmann, and Asja Fischer. 2018. Improving\nresponse selection in multi-turn dialogue systems by\nincorporating domain knowledge. In Proceedings\nof CoNLL, pages 497–507.\n2170\nHongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang\nTang. 2017. A survey on dialogue systems: Recent\nadvances and new frontiers. CoRR, abs/1711.01731.\nMuthuraman Chidambaram, Yinfei Yang, Daniel Cer,\nSteve Yuan, Yun-Hsuan Sung, Brian Strope, and Ray\nKurzweil. 2019. Learning cross-lingual sentence\nrepresentations via a multi-task dual-encoder model.\nIn Proceedings of the 4th Workshop on Representa-\ntion Learning for NLP, pages 250–259.\nSam Coope, Tyler Farghly, Daniela Gerz, Ivan Vuli ´c,\nand Matthew Henderson. 2020. Span-ConveRT:\nFew-shot span extraction for dialog with pretrained\nconversational representations. In Proceedings of\nACL, pages 107–121.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT , pages\n4171–4186.\nWenchao Du and Alan Black. 2018. Data augmenta-\ntion for neural online chats response selection. In\nProceedings of the 2nd International Workshop on\nSearch-Oriented Conversational AI, pages 52–58.\nLayla El Asri, Hannes Schulz, Shikhar Sharma,\nJeremie Zumer, Justin Harris, Emery Fine, Rahul\nMehrotra, and Kaheer Suleman. 2017. Frames: A\ncorpus for adding memory to goal-oriented dialogue\nsystems. In Proceedings of SIGDIAL , pages 207–\n219.\nMihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,\nSanchit Agarwal, Shuyang Gao, and Dilek Hakkani-\nT¨ur. 2019. MultiWOZ 2.1: Multi-domain dialogue\nstate corrections and state tracking baselines. CoRR,\nabs/1907.01669.\nAhmed Fadhil and Gianluca Schiavo. 2019. Designing\nfor health chatbots. CoRR, abs/1902.09022.\nJamie Fraser, Ioannis Papaioannou, and Oliver Lemon.\n2018. Spoken conversational AI in video games:\nEmotional dialogue management increases user en-\ngagement. In Proceedings of IVA.\nChulaka Gunasekara, Jonathan K. Kummerfeld,\nLazaros Polymenakos, and Walter Lasecki. 2019.\nDSTC7 task 1: Noetic end-to-end response selec-\ntion. In Proceedings of the 1st Workshop on NLP\nfor Conversational AI, pages 60–67.\nMandy Guo, Qinlan Shen, Yinfei Yang, Heming\nGe, Daniel Cer, Gustavo Hernandez Abrego, Keith\nStevens, Noah Constant, Yun-hsuan Sung, Brian\nStrope, and Ray Kurzweil. 2018. Effective parallel\ncorpus mining using bilingual sentence embeddings.\nIn Proceedings of WMT, pages 165–176.\nSong Han, Huizi Mao, and William J. Dally. 2016.\nDeep compression: Compressing deep neural net-\nworks with pruning, trained quantization and Huff-\nman coding. In Proceedings of ICLR.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS Spoken Language Sys-\ntems Pilot Corpus. In Proceedings of the Workshop\non Speech and Natural Language , HLT ’90, pages\n96–101.\nMatthew Henderson, Rami Al-Rfou, Brian Strope, Yun-\nHsuan Sung, L ´aszl´o Luk´acs, Ruiqi Guo, Sanjiv Ku-\nmar, Balint Miklos, and Ray Kurzweil. 2017. Efﬁ-\ncient natural language response suggestion for smart\nreply. CoRR, abs/1705.00652.\nMatthew Henderson, Pawel Budzianowski, I ˜nigo\nCasanueva, Sam Coope, Daniela Gerz, Girish Ku-\nmar, Nikola Mrkˇsi´c, Georgios Spithourakis, Pei-Hao\nSu, Ivan Vuli ´c, and Tsung-Hsien Wen. 2019a. A\nrepository of conversational datasets. In Proceed-\nings of the 1st Workshop on Natural Language Pro-\ncessing for Conversational AI, pages 1–10.\nMatthew Henderson, Blaise Thomson, and Jason D.\nWiliams. 2014. The Second Dialog State Tracking\nChallenge. In Proceedings of SIGDIAL, pages 263–\n272.\nMatthew Henderson, Ivan Vuli ´c, Daniela Gerz, I ˜nigo\nCasanueva, Paweł Budzianowski, Sam Coope,\nGeorgios Spithourakis, Tsung-Hsien Wen, Nikola\nMrkˇsi´c, and Pei-Hao Su. 2019b. Training neural re-\nsponse selection for task-oriented dialogue systems.\nIn Proceedings of ACL, pages 5392–5404.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (GELUs). arXiv preprint\narXiv:1606.08415.\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Trans-\nformer architectures and pre-training strategies for\nfast and accurate multi-sentence scoring. In Pro-\nceedings of ICLR.\nAnjuli Kannan, Karol Kurach, Sujith Ravi, Tobias\nKaufmann, Andrew Tomkins, Balint Miklos, Greg\nCorrado, L ´aszl´o Luk ´acs, Marina Ganea, Peter\nYoung, and Vivek Ramavajjala. 2016. Smart Reply:\nAutomated response suggestion for email. In Pro-\nceedings of KDD, pages 955–964.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A Lite BERT for self-supervised\nlearning of language representations. In Proceed-\nings of ICLR.\nLiliana Laranjo, Adam G. Dunn, Huong Ly Tong, Ah-\nmet Baki Kocaballi, Jessica Chen, Rabia Bashir,\nDidi Surian, Blanca Gallego, Farah Magrabi, An-\nnie Y .S. Lau, and Enrico Coiera. 2018. Conver-\nsational agents in healthcare: A systematic review.\nJournal of the American Medical Informatics Asso-\nciation, 25(9):1248–1258.\nBing Liu, G ¨okhan T ¨ur, Dilek Hakkani-T ¨ur, Pararth\nShah, and Larry P. Heck. 2018. Dialogue learn-\ning with human teaching and feedback in end-to-end\n2171\ntrainable task-oriented dialogue systems. In Pro-\nceedings of NAACL-HLT, pages 2060–2069.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nRyan Thomas Lowe, Nissan Pow, Iulian Vlad Serban,\nLaurent Charlin, Chia-Wei Liu, and Joelle Pineau.\n2017. Training end-to-end dialogue systems with\nthe ubuntu dialogue corpus. Dialogue & Discourse,\n8(1):31–65.\nChristopher D. Manning, Prabhakar Raghavan, and\nHinrich Sch¨utze. 2008. Introduction to Information\nRetrieval. Cambridge University Press.\nPierre-Emmanuel Mazar ´e, Samuel Humeau, Martin\nRaison, and Antoine Bordes. 2018. Training mil-\nlions of personalized dialogue agents. In Proceed-\nings of EMNLP, pages 2775–2779.\nShikib Mehri, Mihail Eric, and Dilek Hakkani-T ¨ur.\n2020. DialoGLUE: A natural language understand-\ning benchmark for task-oriented dialogue. CoRR,\nabs/2009.13570.\nShikib Mehri, Evgeniia Razumovskaia, Tiancheng\nZhao, and Maxine Eskenazi. 2019. Pretraining\nmethods for dialog context representation learning.\nIn Proceedings of ACL, pages 3836–3845.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Pro-\nceedings of NeurIPS, pages 14014–14024.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory F. Diamos, Erich Elsen, David Garc ´ıa,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev,\nGanesh Venkatesh, and Hao Wu. 2018. Mixed pre-\ncision training. In Proceedings of ICLR.\nSeyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li,\nand Hassan Ghasemzadeh. 2019. Improved knowl-\nedge distillation via teacher assistant: Bridging\nthe gap between student and teacher. CoRR,\nabs/1902.03393.\nNikola Mrk ˇsi´c, Diarmuid ´O S ´eaghdha, Blaise Thom-\nson, Milica Ga ˇsi´c, Pei-Hao Su, David Vandyke,\nTsung-Hsien Wen, and Steve Young. 2015. Multi-\ndomain dialog state tracking using recurrent neural\nnetworks. In Proceedings of ACL, pages 794–799.\nNikola Mrkˇsi´c, Ivan Vuli´c, Diarmuid ´O S´eaghdha, Ira\nLeviant, Roi Reichart, Milica Ga ˇsi´c, Anna Korho-\nnen, and Steve Young. 2017. Semantic specialisa-\ntion of distributional word vector spaces using mono-\nlingual and cross-lingual constraints. Transactions\nof the ACL, pages 314–325.\nGabriel Pereyra, George Tucker, Jan Chorowski,\nLukasz Kaiser, and Geoffrey E. Hinton. 2017. Regu-\nlarizing neural networks by penalizing conﬁdent out-\nput distributions. CoRR, abs/1701.06548.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning. Technical Re-\nport, OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nOsman Ramadan, Paweł Budzianowski, and Milica\nGaˇsi´c. 2018. Large-scale multi-domain belief track-\ning with knowledge sharing. In Proceedings of ACL,\npages 432–437.\nAntoine Raux, Brian Langner, Alan W. Black, and\nMaxine Esk´enazi. 2003. LET’s GO: Improving spo-\nken dialog systems for the elderly and non-natives.\nIn Proceedings of EUROSPEECH.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: Smaller, faster, cheaper and lighter.CoRR,\nabs/1910.01108.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren\nEtzioni. 2019. Green AI. Communications of the\nACM.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of NAACL-HLT, pages 464–\n468.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W. Mahoney, and\nKurt Keutzer. 2019. Q-BERT: Hessian based ul-\ntra low precision quantization of BERT. CoRR,\nabs/1909.05840.\nJasdeep Singh, Bryan McCann, Richard Socher, and\nCaiming Xiong. 2019. BERT is not an interlingua\nand the bias of tokenization. In Proceedings of the\n2nd Workshop on Deep Learning Approaches for\nLow-Resource NLP (DeepLo 2019), pages 47–55.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of ACL, pages\n3645–3650.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. 2016. Re-\nthinking the inception architecture for computer vi-\nsion. In Proceedings of CVPR, pages 2818–2826.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspeciﬁc knowledge from BERT into simple neural\nnetworks. CoRR, abs/1903.12136.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan Gomez, Stephan Gouws, Llion\nJones, Łukasz Kaiser, Nal Kalchbrenner, Niki Par-\nmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszko-\nreit. 2018. Tensor2Tensor for neural machine trans-\nlation. In Proceedings of the 13th Conference of the\n2172\nAssociation for Machine Translation in the Ameri-\ncas, pages 193–199.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NeurIPS, pages 6000–\n6010.\nJesse Vig and Kalai Ramea. 2019. Comparison of\ntransfer-learning approaches for response selection\nin multi-turn conversations. In Proceedings of\nDSTC-7.\nVladimir Vlasov, Johannes E. M. Mosig, and Alan\nNichol. 2019. Dialogue transformers. CoRR,\nabs/1910.00486.\nMengting Wan and Julian McAuley. 2016. Modeling\nambiguity, subjectivity, and diverging viewpoints in\nopinion question answering systems. In Proceed-\nings of ICDM, pages 489–498.\nHao Wang, Zhengdong Lu, Hang Li, and Enhong Chen.\n2013. A dataset for research on short-text conversa-\ntions. In Proceedings of EMNLP, pages 935–945.\nTsung-Hsien Wen, Yishu Miao, Phil Blunsom, and\nSteve J. Young. 2017a. Latent intention dialogue\nmodels. In Proceedings of ICML, pages 3732–3741.\nTsung-Hsien Wen, David Vandyke, Nikola Mrk ˇsi´c,\nMilica Gaˇsi´c, Lina M. Rojas-Barahona, Pei-Hao Su,\nStefan Ultes, and Steve Young. 2017b. A network-\nbased end-to-end trainable task-oriented dialogue\nsystem. In Proceedings of EACL, pages 438–449.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and reﬁne: Improved sequence gen-\neration models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92.\nJason Williams. 2012. A critical analysis of two sta-\ntistical spoken dialog systems in public use. In Pro-\nceedings of SLT.\nJason D. Williams. 2014. Web-style ranking and SLU\ncombination for dialog state tracking. In Proceed-\nings of SIGDIAL, pages 282–291.\nYu Wu, Wei Wu, Chen Xing, Ming Zhou, and Zhou-\njun Li. 2017. Sequential matching network: A\nnew architecture for multi-turn response selection\nin retrieval-based chatbots. In Proceedings of ACL,\npages 496–505.\nYinfei Yang, Steve Yuan, Daniel Cer, Sheng-Yi Kong,\nNoah Constant, Petr Pilar, Heming Ge, Yun-hsuan\nSung, Brian Strope, and Ray Kurzweil. 2018. Learn-\ning semantic textual similarity from conversations.\nIn Proceedings of the 3rd Workshop on Representa-\ntion Learning for NLP, pages 164–174.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8BERT: Quantized 8bit BERT.\nCoRR, abs/1910.06188.\nMatthew D. Zeiler. 2012. ADADELTA: an adaptive\nlearning rate method. CoRR, abs/1212.5701.\nSanqiang Zhao, Raghav Gupta, Yang Song, and Denny\nZhou. 2019a. Extreme language model compres-\nsion with optimal subwords and shared projections.\nCoRR, abs/1909.11687.\nTiancheng Zhao, Kaige Xie, and Maxine Esk ´enazi.\n2019b. Rethinking action spaces for reinforcement\nlearning in end-to-end dialog agents with latent vari-\nable models. In Proceedings of NAACL-HLT, pages\n1208–1218.\nXiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying\nChen, Wayne Xin Zhao, Dianhai Yu, and Hua Wu.\n2018. Multi-turn response selection for chatbots\nwith deep attention matching network. In Proceed-\nings of ACL, pages 1118–1127.\n2173\nA Evaluation Data\nLinks to (non-proprietary) evaluation data are avail-\nable in Table 7.\nB Intent Classiﬁcation: Grid Search\nTable 8 provides a summary of hyperparameters\nwith the corresponding values tried during grid\nsearch in intent classiﬁcation experiments\nC Models in Comparison\nTable 9 provides URLs to the models used as base-\nlines in our comparisons.\n2174\nTask Evaluation Data and/or Model Link\nResponse Selection Reddit https://github.com/PolyAI-LDN/\nconversational-datasets\nResponse Selection AmazonQA https://github.com/PolyAI-LDN/\nconversational-datasets\nResponse Selection DSTC 7-UBUNTU https://ibm.github.io/\ndstc-noesis/public/datasets.html\nIntent Classiﬁcation Banking https://github.com/PolyAI-LDN/\ntask-specific-datasets\nTable 7: Links to evaluation data.\nHyperparameter Values Tried\nHidden layer size h 128, 256, 512, 1,024\nNumber of hidden layers H 0, 1, 2\nDropout rate r 0.75, 0.5, 0.25\nOptimizer Adam (decaying learning rate 4 × 10−4); SGD (lr 0.75)\nTable 8: Grid search values for intent classiﬁcation experiments (for all models in comparison). Best-performing\nhparams for ConveRT are in bold.\nModel URL\nUSE -LARGE https://tfhub.dev/google/universal-sentence-encoder-large/5\nBERT-LARGE https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/2\nUSE -QA https://tfhub.dev/google/universal-sentence-encoder-qa/3\nPOLYAI -DUAL https://github.com/PolyAI-LDN/polyai-models\nBI-ENCODER https://parl.ai/projects/polyencoder/\nTable 9: URLs of the models used in the comparison.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8459866642951965
    },
    {
      "name": "Encoder",
      "score": 0.7481006383895874
    },
    {
      "name": "Transformer",
      "score": 0.6799167394638062
    },
    {
      "name": "Software portability",
      "score": 0.6121566295623779
    },
    {
      "name": "Scalability",
      "score": 0.6044910550117493
    },
    {
      "name": "Sentence",
      "score": 0.5936322808265686
    },
    {
      "name": "Dialog box",
      "score": 0.5044494867324829
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4894775152206421
    },
    {
      "name": "Speech recognition",
      "score": 0.4401329457759857
    },
    {
      "name": "Question answering",
      "score": 0.4267326295375824
    },
    {
      "name": "Task (project management)",
      "score": 0.4251723289489746
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.4196698069572449
    },
    {
      "name": "Natural language processing",
      "score": 0.4157600998878479
    },
    {
      "name": "Programming language",
      "score": 0.16135546565055847
    },
    {
      "name": "Database",
      "score": 0.10391134023666382
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1311269955",
      "name": "Apple (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I177802217",
      "name": "University of Mannheim",
      "country": "DE"
    }
  ],
  "cited_by": 38
}