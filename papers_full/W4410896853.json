{
  "title": "Evaluating performance of large language models for atrial fibrillation management using different prompting strategies and languages",
  "url": "https://openalex.org/W4410896853",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5101736988",
      "name": "Zexi Li",
      "affiliations": [
        "Sichuan University",
        "West China Hospital of Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A5023571812",
      "name": "Chunyi Yan",
      "affiliations": [
        "Sichuan University",
        "West China Second University Hospital of Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A5003664210",
      "name": "Ying Cao",
      "affiliations": [
        "Sichuan University",
        "West China Hospital of Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A5020735218",
      "name": "Aobo Gong",
      "affiliations": [
        "Sichuan University",
        "West China Hospital of Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A5101476208",
      "name": "Fanghui Li",
      "affiliations": [
        "Sichuan University",
        "West China Hospital of Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A5064719218",
      "name": "Rui Zeng",
      "affiliations": [
        "Sichuan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4319304408",
    "https://openalex.org/W4376613087",
    "https://openalex.org/W4391679299",
    "https://openalex.org/W4386757283",
    "https://openalex.org/W4391262681",
    "https://openalex.org/W4379093714",
    "https://openalex.org/W4389577763",
    "https://openalex.org/W4366769280",
    "https://openalex.org/W4387472364",
    "https://openalex.org/W4285059920",
    "https://openalex.org/W4221098516",
    "https://openalex.org/W4386958277",
    "https://openalex.org/W4319460381",
    "https://openalex.org/W4321436564",
    "https://openalex.org/W4391971084",
    "https://openalex.org/W4406230527",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4400307907",
    "https://openalex.org/W4211191293",
    "https://openalex.org/W4399330282",
    "https://openalex.org/W4390070272",
    "https://openalex.org/W4402049769",
    "https://openalex.org/W4226181181",
    "https://openalex.org/W4401424596",
    "https://openalex.org/W4408634848",
    "https://openalex.org/W4405463941",
    "https://openalex.org/W4409347038",
    "https://openalex.org/W4401063128",
    "https://openalex.org/W4390613062",
    "https://openalex.org/W2477907264",
    "https://openalex.org/W4380997513",
    "https://openalex.org/W4404654936",
    "https://openalex.org/W4387653139",
    "https://openalex.org/W4386407038",
    "https://openalex.org/W4392462807",
    "https://openalex.org/W4391258615",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4323360926",
    "https://openalex.org/W4403502725",
    "https://openalex.org/W4404866279",
    "https://openalex.org/W4386459994"
  ],
  "abstract": "This study evaluated large language models (LLMs) using 30 questions, each derived from a recommendation in the 2024 European Society of Cardiology (ESC) guidelines for atrial fibrillation (AF) management. These recommendations were stratified by class of recommendation and level of evidence. The primary objective was to assess the reliability and consistency of LLM-generated classifications compared to those in the ESC guidelines. Additionally, the study assessed the impact of different prompting strategies and working languages on LLM performance. Three prompting strategies were tested: Input-output (IO), 0-shot-Chain of thought (0-COT) and Performed-Chain of thought (P-COT) prompting. Each question, presented in both English and Chinese, was input into three LLMs: ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro. The reliability of the different LLM-prompt combinations showed moderate to substantial agreement (Fleiss kappa ranged from 0.449 to 0.763). Claude 3.5 with P-COT prompting had the highest recommendation classification consistency (60.3%). No significant differences were observed between English and Chinese across most LLM-prompt combinations. Bias analysis of inconsistent outcomes revealed a propensity towards more recommended treatments and stronger evidence levels across most LLM-prompt combinations. The characteristics of clinical questions potentially influence LLM performance. This study highlights the limitations in the accuracy of LLM responses to AF-related questions. To gather more comprehensive insights, conducting repeated queries is advisable. Future efforts should focus on expanding the use of diverse prompting strategies, conducting ongoing model evaluation and refinement, and establishing a comprehensive, objective benchmarking system.",
  "full_text": "Evaluating performance of \nlarge language models for atrial \nfibrillation management using \ndifferent prompting strategies and \nlanguages\nZexi Li1,3, Chunyi Yan2,3, Ying Cao1, Aobo Gong1, Fanghui Li1 & Rui Zeng1\nThis study evaluated large language models (LLMs) using 30 questions, each derived from a \nrecommendation in the 2024 European Society of Cardiology (ESC) guidelines for atrial fibrillation \n(AF) management. These recommendations were stratified by class of recommendation and level \nof evidence. The primary objective was to assess the reliability and consistency of LLM-generated \nclassifications compared to those in the ESC guidelines. Additionally, the study assessed the impact \nof different prompting strategies and working languages on LLM performance. Three prompting \nstrategies were tested: Input-output (IO), 0-shot-Chain of thought (0-COT) and Performed-Chain of \nthought (P-COT) prompting. Each question, presented in both English and Chinese, was input into \nthree LLMs: ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro. The reliability of the different LLM-\nprompt combinations showed moderate to substantial agreement (Fleiss kappa ranged from 0.449 to \n0.763). Claude 3.5 with P-COT prompting had the highest recommendation classification consistency \n(60.3%). No significant differences were observed between English and Chinese across most LLM-\nprompt combinations. Bias analysis of inconsistent outcomes revealed a propensity towards more \nrecommended treatments and stronger evidence levels across most LLM-prompt combinations. The \ncharacteristics of clinical questions potentially influence LLM performance. This study highlights the \nlimitations in the accuracy of LLM responses to AF-related questions. To gather more comprehensive \ninsights, conducting repeated queries is advisable. Future efforts should focus on expanding the use of \ndiverse prompting strategies, conducting ongoing model evaluation and refinement, and establishing \na comprehensive, objective benchmarking system.\nKeywords Atrial fibrillation, Artificial intelligence, Large language models, Prompt engineering, ChatGPT\nThe rapid advancement of artificial intelligence (AI) is transforming various scientific disciplines, particularly \nin medicine1. These innovations have led to the development of large language models (LLMs), such as Chat \nGenerative Pretrained Transformer (ChatGPT) 2,3. Numerous studies have investigated the performance of \nvarious LLMs in medical applications including case diagnosis4,5, medical examinations6,7, and clinical decision \nsupport8,9, with mixed results. The increasing accessibility of online platforms has also led more patients to seek \nhealth-related information via the internet10. Simultaneously, many healthcare professionals are also leveraging \ndigital resources and AI to augment their education11 and clinical practice12. As LLMs become more prevalent, \nboth healthcare providers and patients are expected to adopt these tools more widely13, potentially transforming \ninformation acquisition in healthcare14.\nDespite their growing use in medicine, most studies have focused on evaluating the outputs of LLMs, with \nlimited emphasis on optimizing their use to improve performance. Prompt engineering, an emerging discipline, \nis dedicated to the development and optimization of prompts to enhance the performance of LLMs 15,16. The \nChain of Thought (COT) prompt exemplifies a novel design in prompt engineering, devised to facilitate \n“step-by-step reasoning” in LLMs, thereby simulating human problem-solving methods 17. Advances in LLM-\n1Department of Cardiology, West China Hospital, Sichuan University, Chengdu 610041, Sichuan, China. 2Department \nof Pediatric Cardiology, West China Second University Hospital, Sichuan University, Chengdu 610041, Sichuan, \nChina. 3Zexi Li and Chunyi Yan contributed equally to this work. email: zengrui_0524@126.com\nOPEN\nScientific Reports |        (2025) 15:19028 1| https://doi.org/10.1038/s41598-025-04309-5\nwww.nature.com/scientificreports\n\nrelated theories have led to the development of specialized prompts that improve performance in tasks such as \nmathematical problem solving17,18.\nAtrial fibrillation (AF), the most common sustained cardiac arrhythmia, is associated with substantial health \nrisks, particularly an elevated risk of stroke19. A recent study evaluated GPT-3.5’s responses to frequently asked \nquestions about AF and found generally accurate outputs with minimal misinformation 20. Another study \nevaluated the LLM’s responses on AF-related queries across various dimensions including appropriateness, \ncomprehensibility, content missing, confabulation, and clinical decisions, concluding that LLMs should be \nemployed cautiously21. However, these evaluations were primarily subjective, with queries generated by the \nresearchers. The questions, such as “What is AF?” and “How can AF lead to stroke?” , reflected patient-centered \nconcerns rather than priorities of healthcare professionals 21. As a result, their relevance to clinical diagnosis \nand treatment may be limited. Moreover, these queries were not derived from clinical guidelines or published \nliterature, but from researchers’ subjective perspectives, potentially introducing bias. Therefore, it is necessary \nto evaluate LLMs using questions that accurately reflect the diagnostic and therapeutic concerns of healthcare \nprofessionals. Clinical guidelines, developed by authoritative organizations, provide standardized protocols \nand address common decision-making scenarios in practice. The questions and recommendations contained \nwithin are widely acknowledged, having undergone rigorous peer review and validation processes. Therefore, we \nselected questions from the 2024 European Society of Cardiology (ESC) Guidelines on AF management, which \nreflect the latest evidence-based practices22.\nAF poses a significant health burden in China, affecting an estimated 20 million individuals 23. Therefore, it \nis essential to examine the efficacy of medical assistance provided by LLMs in the context of Chinese-language \ninteractions. Moreover, recent studies also underscored the importance of investigating cross-linguistic variations \nin ChatGPT’s performance7,24. This study aims to assess the quality of clinical recommendations generated by \nvarious LLMs for AF , by comparing their outputs against the 2024 ESC guidelines22. Both English and Chinese \nwere used as working languages, and three distinct prompts were applied to evaluate the impact of prompting \nstrategies on LLM performance.\nMethods\nThe 2024 ESC guidelines for the management of atrial fibrillation were utilized as criteria to assess the responses \nof LLMs. Developed by the European Society of Cardiology and other specialized associations, including the \nEuropean Association for Cardio-Thoracic Surgery (EACTS), these guidelines encompass the comprehensive \nmanagement of AF , emphasizing a multidisciplinary approach. Recommendations were categorized into four \nclassifications to indicate degree of recommendation: Class I (recommended), Class IIa, Class IIb, and Class III \n(not recommended). The strength of each recommendation was determined by levels of evidence: Level A, Level \nB, and Level C, with higher levels indicating stronger evidence support (Table 1)22.\nIn this study, three recently published LLMs were utilized: ChatGPT-4 “omni” [ChatGPT-4o] (GPT-4, \nOpenAI, San Francisco, California, United States), Claude 3.5 Sonnet (Claude 3, Anthropic, San Francisco, \nCalifornia, United States) and Gemini 1.5 Pro (LaMDA, Google, Mountain View, California, United States). \nThese models are currently among the most widely adopted and accessible models in practice and have been \nevaluated and validated in previous studies25–27. Each item from the ESC guidelines was reformulated as a question \nabout AF . Different LLMs were tasked with generating an answer that included the selected recommendation \nclass and the rating evidence level as the final output. The answers from the LLMs were then compared to the \nESC guidelines to evaluate their alignment in terms of recommendation classification and evidence level. Each \ndimension was independently assessed as either “Consistent” or “Inconsistent” . Based on the current application \nof prompting engineering and the task of this study, three types of prompts were employed: Input-output (IO) \nprompting, 0-shot-Chain of thought (0-COT) prompting and Performed-Chain of thought (P-COT) prompting. \nThese prompt types had been previously used in studies involving LLMs and been proven feasible15,28,29. A brief \nintroduction of each prompt was presented in Table  2 and detailed content was presented in Supplementary \nTable 1.\nA total of 30 recommendations were extracted from the ESC guidelines through a rigorous evaluation process. \nA panel comprising three senior cardiologists, each with extensive experience in cardiovascular medicine and \natrial fibrillation management, independently scored recommendations in the guidelines and selected the 30 \nDefinitions Word to describe\nClasses of recommendations\n Class I Evidence and/or general agreement that a given treatment or procedure is beneficial, useful, effective. Is recommended or indicated\n Class II Conflicting evidence and/or a divergence of opinion about the usefulness/efficacy of the given treatment or procedure.\n Class IIa Weight of evidence/opinion is in favour of usefulness/efficacy. Should be considered\n Class IIb Usefulness/efficacy is less well established by evidence/opinion. May be considered\n Class III Evidence or general agreement that the given treatment or procedure is not useful/effective, and in some cases may be harmful. Is not recommended\nLevels of evidence\n Level A Data derived from multiple randomized clinical trials or meta-analyses. Strong\n Level B Data derived from a single randomized clinical trial or large non-randomized studies. Moderate\n Level C Consensus of opinion of the experts and/or small studies, retrospective studies, registries. Limited\nTable 1. The classes of recommendations and levels of evidence in ESC guidelines.\n \nScientific Reports |        (2025) 15:19028 2| https://doi.org/10.1038/s41598-025-04309-5\nwww.nature.com/scientificreports/\nhighest-scoring recommendations. The scoring criteria were based on clinical significance and frequency of \noccurrence. Among the selected recommendations, 9 recommendations were classified as Class I, 9 as Class IIa, \n6 as Class IIb and 6 as Class III. Regarding the levels of evidence, 8 were supported by Level A, 11 by Level B, and \n11 by Level C (Supplementary Table 2). Each item was queried using different prompt types in both English and \nChinese. Any response that lacked a specific recommendation classification or evidence level, or provided values \nbeyond the guideline range, was deemed invalid. We conducted repeated inquiries for each combination until 5 \nvalid responses were collected. Almost no invalid responses (fewer than 5 in total) were encountered. Overall, \nthe study involved a total of 2,700 valid queries. This repetition strategy facilitated the calculation of Fleiss kappa \nvalues for reliability assessment and has been employed in previous research15. The translation of these queries \ninto Chinese was performed by experienced Chinese cardiologists who are proficient in English. After translated \ninto Chinese, each query was then independently reviewed and validated by a second bilingual cardiologist \nto further ensure accuracy. To avoid the contextual influence on the responses, each query was presented in a \nseparate dialogue box. The response to each query was documented.\nThe performance of LLMs was assessed based on consistency and reliability. Consistency refers to the \nproportion of instances where the model’s outputs aligned with ESC guidelines. To analyze consistency, data \nwere stratified into “consistent or inconsistent outcomes” across two distinct dimensions: recommendation \nclassification and evidence level. This dual-dimensional stratification enabled a more thorough evaluation of \nconsistency, with an emphasis on the consistency of recommendation classification in this study. Statistical \nanalyses, including Chi-square test, Fisher’s exact test, and Y ates’s continuity correction, were performed with \na Bonferroni adjustment for multiple comparisons. Reliability is defined as the repeatability of responses to the \nsame query, evaluated through the Fleiss kappa test. This study investigated the reliability of recommendation \nclassification in LLM-generated responses. Based on previous studies 30, different Fleiss kappa values are \nindicated as follows: no reliability (< 0.01), slight reliability (0.01–0.2), fair reliability (0.21–0.40), moderate \nreliability (0.41–0.60), substantial reliability (0.61–0.80), or almost perfect reliability (0.81-1.00). Data analyses \nwere performed using IBM SPSS Statistics 27.0 (IBM Corp., Armonk, New Y ork, USA). P values were two-tailed, \nand differences were considered statistically significant with a threshold of P < 0.05.\nThe ethical committee approval and informed consent were not required for this study as it did not involve \nany patients.\nResults\nConsistency across different prompts in different LLMs\nThe consistency of various LLM-prompt combinations is presented in Figs.  1, 2 and Table 3. The combination \nof Claude 3.5 and P-COT demonstrated the highest adherence to clinical guidelines in generating treatment \nrecommendations (60.3%). Notably, all three prompts employed with Claude 3.5 yielded recommendation \nclassification consistency rates exceeding 50% (range: 52.3-60.3%). P-COT prompting emerged as the most \nefficacious approach for both Claude 3.5 and GPT-4o. In GPT-4o, P-COT achieved a 59.0% recommendation \nclassification consistency rate, significantly outperforming IO prompting at 45.7% ( P = 0.005). Interestingly, \nGemini 1.5 deviated from this trend, with P-COT yielded lower consistency compared to IO and 0-COT prompts. \nAdditionally, the recommendation classification consistency rates for the three prompts in Gemini 1.5 did not \nexceed 50% (range: 46.7-50.0%). The evidence level consistency across different LLM-prompt combinations \nrevealed no discernible pattern, fluctuating around 50% (range: 46.0-52.7%). Similarly, the dual-dimension \nconsistency lacked a clear trend, fluctuating around 30% (range: 27.0-32.3%).\nBias analysis in inconsistent outcomes\nInconsistencies between LLM outputs and ESC guidelines regarding recommendation classifications or \nevidence levels were categorized as bias towards more recommended/stronger evidence and bias towards \nless recommended/weaker evidence. The results of this assessment are shown in Fig.  3. The majority of \ninconsistent outcomes (exceeding 50%) across most LLM-prompt combinations fell into bias towards more \nrecommended, indicating these erroneous outputs were inclined to endorse certain treatments or management \nstrategies. However, the combination of Claude 3.5 and IO deviated from this pattern, with only 46.1% of its \nPrompt Definition Brief Description\nInput-output (IO)  prompting Input the task directly\nAnswer the following medical question:\n< input the question>\nAnswer the medical question by selecting \nthe appropriate class of recommendations \nand then rate the level of evidence.\n< insert the criteria>\n0-shot-Chain of thought  (0-COT) prompting Incorporate “Complete the task step by step” into the IO framework \nto guide reasoning.\n< Task Description>\nComplete the task above step by step.\nShow your work of each step.\nPerformed-Chain of thought (P-COT) prompting Decompose the task into different steps to specify reasoning processes \nfor LLMs\n< Task Description>\nComplete the task above step by step:\nStep 1….\nStep 2….\n……\nShow your work of each step.\nTable 2. Definition and brief description of each prompt.\n \nScientific Reports |        (2025) 15:19028 3| https://doi.org/10.1038/s41598-025-04309-5\nwww.nature.com/scientificreports/\ninconsistent outputs exhibiting this bias. GPT-4o, when paired with 0-COT and P-COT prompts, demonstrated \nthe highest bias towards more recommended across all combinations, with 76.8% and 72.4% of inconsistent \noutputs, respectively, falling into this category. Moreover, the erroneous outputs of all combinations tended to \noverestimate the strength of evidence supporting their recommendations, with over 65% inconsistent outcomes \nfalling into bias towards stronger evidence. GPT-4o, paired with 0-COT and P-COT prompts, continued to \nexhibit the most pronounced tendency in this regard, with 85.9% and 89.2% of inconsistent outputs, respectively, \nfalling into this category.\nRecommendation classification consistency in different working languages\nThe comparative analysis of recommendation classification consistency between Chinese and English as working \nlanguages across various LLM-prompt combinations is presented in Supplementary Table 3. Notably, in Claude \n3.5 with IO prompting, utilizing English as the working language yielded a significantly higher consistency rate \n(59.30%) compared to Chinese (46.70%, P = 0.028). However, no significant differences were observed between \nworking languages in all other LLM-prompt combinations examined.\nRecommendation classification consistency in different LLMs\nOverall, Claude 3.5 demonstrated the highest recommendation classification consistency (55.2%), significantly \noutperforming Gemini 1.5 (48.1%). To explore potential variations in consistency across different levels of \nevidence and classes of recommendation, we conducted subgroup analyses (Table  4). The recommendation \nclassification consistency of different LLMs addressing questions at different levels/classes is shown in Fig. 4.\nFig. 2. Evidence level and dual-dimension consistency across different prompts in different models.\n \nFig. 1. Recommendation classification consistency across different prompts in different models.\n \nScientific Reports |        (2025) 15:19028 4| https://doi.org/10.1038/s41598-025-04309-5\nwww.nature.com/scientificreports/\nFurther analysis revealed intra-model variations in consistency across different evidence levels and \nrecommendation classes. Within GPT-4o and Gemini 1.5, recommendations with Level A evidence \ndemonstrated significantly higher consistency compared to those with Level B and C evidence, whereas Claude \n3.5 did not exhibit this pattern (Supplementary Table 4). In GPT-4o and Gemini 1.5, Class I recommendations \ndemonstrated significantly higher consistency compared to both Class IIa and IIb, while in Claude 3.5, Class I \nGPT-4o Claude 3.5 Sonnet Gemini 1.5 Pro P\nOverall 52.4%a, b 55.2%b 48.1%a 0.010\nSubgroup\nEvidence level\n Level A 64.2% 56.3% 65.4% 0.081\n Level B 45.8%a 62.1%b 36.1%c < 0.001\n Level C 50.6% 47.6% 47.6% 0.667\nRecommendation classification\n Class I 72.6%a 53.7%b 66.7%a < 0.001\n Class IIa 54.4%a 58.9%a 33.3%b < 0.001\n Class IIb 14.4%a 38.9%b 33.3%b < 0.001\n Class III 57.2%a 68.3%b 57.2%a 0.045\nTable 4. Comparison of recommendation classification consistency across models. Each superscript letter \nindicates a subset in which column proportions do not exhibit statistically significant differences at the 0.05 \nlevel.\n \nFig. 3. Bias analysis in inconsistent outcomes. (A) Analysis of recommendation classifications; (B) Analysis of \nevidence levels.\n \nIO 0-COT P-COT P\nRecommendation classification consistency\n GPT-4o 45.7% a 52.7%a, b 59.0%b 0.005\n Claude 3.5 Sonnet 53.0% 52.3% 60.3% 0.092\n Gemini 1.5 Pro 47.7% 50.0% 46.7% 0.704\nEvidence level consistency\n GPT-4o 51.7% 48.0% 50.7% 0.650\n Claude 3.5 Sonnet 47.7% 52.7% 49.3% 0.459\n Gemini 1.5 Pro 48.0% 46.0% 51.0% 0.467\nDual-dimensional consistency\n GPT-4o 27.00% 27.00% 31.70% 0.344\n Claude 3.5 Sonnet 29.30% 29.70% 32.30% 0.682\n Gemini 1.5 Pro 28.30% 30.30% 28.30% 0.823\nTable 3. Consistency across different prompts in different LLMs. Each superscript letter indicates a subset in \nwhich column proportions do not exhibit statistically significant differences at the 0.05 level.\n \nScientific Reports |        (2025) 15:19028 5| https://doi.org/10.1038/s41598-025-04309-5\nwww.nature.com/scientificreports/\nrecommendations manifested significantly higher consistency only when compared to Class IIb. Within Gemini \n1.5, Class III recommendations exhibited significantly higher consistency compared to both Class IIa and IIb, \nwhile in GPT-4o and Claude 3.5, Class III recommendations manifested significantly higher consistency only \nwhen compared to Class IIb (Supplementary Table 5).\nSubgroup analysis of different prompts within each model\nGiven the observed variations in recommendation classification consistency across different levels of evidence \nand classes of recommendation, we conducted a subgroup analysis to examine the performance differences of \nvarious prompting strategies within these combinations. The detailed results of the subgroup analysis within \neach model are presented in Supplementary Table 6. In GPT-4o, for recommendations with Level C evidence, \nboth 0-COT (54.5%) and P-COT (59.1%) prompting demonstrated significantly higher consistency compared \nto IO prompting (38.2%) (P = 0.005). Similarly, in GPT-4o, for Class I recommendations, both 0-COT (82.2%) \nand P-COT (78.9%) prompting exhibited significantly higher consistency compared to IO prompting (56.7%) \n(P < 0.001). Notably, Gemini 1.5 displayed a distinct pattern for Class III recommendations, where P-COT \nprompting (41.7%) yielded significantly lower consistency than both IO (66.7%) and 0-COT (63.3%) prompting \n(P = 0.011). Moreover, no significant differences were observed between prompts in other combinations.\nReliability of recommendation classification\nThe reliability of recommendation classification across the three prompts and models in different working \nlanguages was assessed using Fleiss’ kappa, as shown in Supplementary Table 7. The kappa values ranged from \n0.449 to 0.763. Notably, the highest reliability was observed with IO and 0-COT prompting in Claude 3.5 for \nEnglish, with kappa values exceeding 0.75 (0.751 and 0.759, respectively). Moreover, all combinations utilizing \nIO prompting consistently yielded Fleiss’ kappa values above 0.6.\nDiscussion\nOur study found that the accuracy of LLM responses to AF-related inquiries was suboptimal, and the models \noften produced inconsistent answers to identical queries. This contrasts with the more favorable performance of \nGPT-3.5 reported by Lee et al. for similar AF-related questions20. This discrepancy may stem from the objective \nevaluation criteria and differing application approaches employed in our research. Additionally, different \nprompting strategies, working languages, and the characteristics of recommendations offered may influence \nrecommendation classification consistency in various ways.\nOur analysis showed that, across different LLM-prompt combinations, only about half of the responses \n(range: 45.7–60.3%) were consistent with ESC guideline recommendations. Moreover, merely three out of ten \nresponses (range: 27.0-32.3%) were completely accurate, indicating suboptimal performance. Similarly, a study \nevaluating ChatGPT’s responses on the management of five common hepatopancreaticobiliary conditions found \nthat only 60% aligned with the UK National Institute for Health and Care Excellence guidelines 31. However, in \ncomparable objective assessment scenarios, LLMs demonstrated remarkable proficiency in rapidly accessing \nand analyzing vast amounts of information, which has been evidenced by their notable performance in various \nmedical licensure examinations, including the Japanese Medical Licensing Examination 32, the United States \nMedical Licensing Examination (USMLE)33, and the German Medical State Examinations 34. This discrepancy \nmay be attributed to various factors. Medical examinations typically focus on knowledge retrieval and have clear \nboundaries, while guideline consistency assessments present real-world clinical scenarios which require nuanced \ndecision-making. Additionally, medical exams are typically based on stable, widely accepted knowledge, while \nclinical guidelines are subject to periodic updates and may differ across regions.\nIn our study, we focused primarily on the consistency and reliability of recommendation classification, which \nwas the critical aspect of clinical decision-making. Our findings revealed that the effects of prompt engineering \nstrategies on answering medical questions varied across different LLMs. There was no significant difference \nFig. 4. Heatmap of recommendation classification consistency of different LLMs addressing questions at \ndifferent levels/classes is shown in Fig. 4.\n \nScientific Reports |        (2025) 15:19028 6| https://doi.org/10.1038/s41598-025-04309-5\nwww.nature.com/scientificreports/\nobserved among these prompting strategies in Claude 3.5 and Gemini 1.5, aligning with the study by Patel et al., \nwhich found that COT prompting did not significantly enhance GPT-3.5’s effectiveness28. However, our findings \nrevealed that within GPT-4o, P-COT significantly outperformed IO prompting in terms of recommendation \nclassification consistency ( P = 0.005). Moreover, when applied to Claude 3.5, P-COT prompting achieved the \nhighest consistency rate across all LLM-prompt combinations. According to our subgroup analysis, within \nGPT-4o, both 0-COT and P-COT prompting were superior to IO prompting in addressing clinical questions \nat level of evidence C (P = 0.005) and Class I of recommendation (P < 0.001). However, COT prompting did not \nconsistently yield positive effects in other contexts. Within Gemini 1.5, P-COT prompting was inferior to the \nother two prompting methods in addressing questions at Class III (P = 0.011).\nIn Lin et al. ’s study on Taiwan advanced medical licensing examination, they revealed that COT prompting \nproved particularly effective, enabling GPT-4 to rectify its initial incorrect responses and achieve an accuracy \nrate exceeding 90% 35. Similarly, in Wang et al. ’s study on thyroid ultrasound diagnosis and treatment \nrecommendations, they found that COT prompting effectively enhanced the interpretability of the AI’s decision-\nmaking process36. However, Patel et al. posited that existing training and inference strategies in AI might already \nincorporate established prompt engineering techniques. Consequently, they proposed that a straightforward \nprompt could be as effective as more complex ones, suggesting that users may not need to utilize additional \nprompting strategies28. This discrepancy may be attributed to the divergent applications of LLMs. Wang et \nal. focused on the interpretability of textual responses, which was assessed subjectively. Moreover, the textual \ncontent produced by LLMs was different in each response and also influenced by variations in the queries \nmarkedly. In contrast, Patel et al. examined USMLE-style medical calculations and clinical scenarios, which \ndemanded concrete judgments from the LLMs and was assessed objectively.\nDespite employing objective assessment like Patel et al. ’s study28, our findings suggested that it could not be \nconcluded that the utility of additional prompts was unnecessary. Our research indicated that COT prompting \nstill demonstrated higher efficacy than IO prompting in certain scenarios. It was also noteworthy that P-COT \nprompting was inferior in addressing questions at Class III within Gemini 1.5. Therefore, further research is \nneeded to examine how various prompting strategies affect performance across different LLMs, types of clinical \nquestions, and practical applications. Our study did not explore other prompting strategies, such as reflection \nof thoughts (ROT) prompting, which steered the LLM to backtrack previous steps by simulating a discussion \nmode15, or self-consistency COT, which selects the most frequent answer from multiple COT attempts37, due to \ntheir complexity. Each of these strategies has unique strengths and limitations, warranting further investigation \nto elucidate their relative effectiveness.\nRecent studies have highlighted the importance of investigating cross-linguistic variations in LLM \nperformance7,38. One study evaluating responses to Chronic Hepatitis B-related questions found that ChatGPT \nperformed less accurately in Chinese than in English24. Similarly, Song et al. analyzed responses from ChatGPT \nand Gemini on the Korean National Dental Hygienist Examination and found lower accuracy in Korean than in \nEnglish, and found that accuracy was consistently higher in English than in Korean across all models 39. These \ndifferences may be attributed to more extensive training data in English and variations in language processing \nalgorithms. However, we observed no significant performance differences between English and Chinese across \nall LLM-prompt combinations, except for Claude 3.5 with IO prompting (59.3% vs. 46.7%, P = 0.028). This \nimprovement may reflect ongoing advancements in LLM development and the use of effective prompting \nstrategies, which help reduce cross-linguistic performance gaps.\nIn our study, the results of reliability indicated that LLMs cannot always provide consistent answers to the \nidentical medical inquiries. The study employed the recommendation classifications of the ESC guidelines as a \nbenchmark and found that LLMs always provided different classes for the same question in multiple answers. \nPerfect reliability was not achieved in any of the tested LLM-prompt combinations. Notably, IO prompting \nconsistently yielded substantial reliability across all LLMs examined.\nFurthermore, our study revealed notable variations in LLM performance across clinical questions with \ndifferent characteristics (Fig.  4). Supplementary Tables 4 and 5 showed the variation in consistency within \nindividual LLMs when responding to questions of differing evidence levels or recommendation classes. While \nthe specific patterns of these variations differed among LLMs, there still emerged a general trend. LLMs tended \nto show enhanced performance with clinical questions at higher evidence levels. Additionally, when presented \nwith clinical questions of a more definitive nature (classified as Class I or III), LLMs appeared to exhibit superior \nperformance compared to questions involving conflicting evidence or divergent opinions (classified as Class \nII). This trend may be attributed to the rationale for the training of LLMs. Recommendations characterized by \nhigher evidence levels or a more definitive nature, likely correspond to areas where LLMs had been exposed \nto a greater volume of consistent, high-quality research data during their training. Conversely, questions in \nareas of clinical uncertainty or controversy, may require LLMs to navigate conflicting information, potentially \nleading to increased challenges in making accurate judgments. Moreover, for all LLM-prompt combinations \n(except Claude & IO), erroneous responses demonstrated a propensity to overestimate the evidence strength or \nrecommendation classification. This bias manifested as an inclination to endorse certain treatments with greater \nconfidence than the established guidelines. We inferred that the training data utilized by LLMs potentially \noverrepresented positive findings and strong recommendations. Secondly, the tendency of LLMs to generate \nincorrect responses coherently and confidently, known as “ AI hallucinations” 40,41, may amplify the perceived \nstrength of medical recommendations. So it is imperative for users to exercise critical judgment when consulting \nLLMs, particularly in domains with clinical ambiguity or when LLMs appear overly confident in their assertions.\nThe implementation of AI in healthcare settings is experiencing unprecedented expansion. Given the \nsensitive nature of healthcare data and the imperative to protect patient privacy, many healthcare policymakers \nare actively pursuing the deployment of local medical models rather than relying on online services. However, \ndue to limitations in local server infrastructures and computational capabilities, healthcare policymakers must \nScientific Reports |        (2025) 15:19028 7| https://doi.org/10.1038/s41598-025-04309-5\nwww.nature.com/scientificreports/\ncarefully consider the computational requirements and operational costs of LLMs. Therefore, both accuracy and \nefficiency must be carefully balanced in practical applications, and identifying an optimal model along with an \neffective prompt strategy is imperative. Additionally, clinicians are primarily concerned with optimizing the \nefficiency and clinical utility of these AI-powered tools in their daily practice. Hence, the application of suitable \nprompting strategies is essential. In our study, we employed three widely adopted, straightforward, and cost-\neffective prompts to assess LLM responses to the questions derived from AF guidelines, and further evaluated \nthe performance across various LLM-prompt combinations to offer practical guidance for integrating AI into \nmedical practices.\nThis study has several potential limitations. First, the study’s focus on AF-related questions may limit the \ngeneralizability of our findings to other medical domains or types of clinical inquiries. Second, only three \nrelatively simple prompting strategies were evaluated, leaving potentially more effective approaches unexplored \nand thus limiting the comprehensiveness of our findings. Furthermore, the study relied on guideline-based \nexpected answers and lacked prospective clinical validation. However, this methodological choice allowed \nfor an objective assessment of the consistency and reliability of LLM responses, potentially mitigating biases. \nAdditionally, the suboptimal performance observed may be partly attributable to the lack of targeted prompts \nsuch as “act according to the latest guidelines on AF management” or “act according to the ESC 2024 guideline” . \nFuture studies could explore the potential impact of such targeted prompts on improving the quality of \nLLM responses. Finally, the rapid evolution of LLM architectures and training data may affect the long-term \napplicability of our findings to future model iterations. Given these limitations, further research into the efficacy \nof other prompting strategies is necessary. Future studies should also aim to develop objective benchmarking \nsystems that incorporate the perspectives of both healthcare professionals and patients. Considering the rapid \nevolution of AI, ongoing assessment and refinement of LLMs are essential.\nConclusion\nThis study revealed that the accuracy of LLM responses to the questions derived from AF guidelines remained \nsuboptimal. The efficacy of prompting strategies differed among LLMs, and Claude 3.5 with P-COT prompting \nshowed the highest recommendation classification consistency. Moreover, it is recommended to conduct repeated \nqueries to gather more comprehensive insights, considering the observed variability in responses to identical \ninquiries. No significant differences were observed between English and Chinese across most LLM-prompt \ncombinations. Furthermore, our analysis indicated that the characteristics of clinical questions potentially \ninfluence LLM performance. LLMs tended to overstate evidence strength and recommendation classification \nin erroneous responses. Future efforts should prioritize the expansion of diverse prompting strategies, ongoing \nevaluation and refinement of LLMs, and the development of a comprehensive and objective benchmarking \nsystem.\nData availability\nThe datasets used and/or analyzed during the current study available from the corresponding author on reason-\nable request.\nReceived: 2 February 2025; Accepted: 26 May 2025\nReferences\n 1. Holzinger, A., Keiblinger, K., Holub, P ., Zatloukal, K. & Müller, H. AI for life: trends in artificial intelligence for biotechnology. New \nBiotechnol. 74, 16–24. https://doi.org/10.1016/j.nbt.2023.02.001 (2023).\n 2. Lu, Y ., Wu, H., Qi, S. & Cheng, K. Artificial intelligence in intensive care medicine: toward a ChatGPT/GPT-4 way?? Ann. Biomed. \nEng. 51, 1898–1903. https://doi.org/10.1007/s10439-023-03234-w (2023).\n 3. Sufi, F . Generative pre-trained transformer (GPT) in research: A systematic review on data augmentation. 15  99 (2024).\n 4. Kuroiwa, T. et al. The potential of ChatGPT as a Self-Diagnostic tool in common orthopedic diseases: exploratory study. J. Med. \nInternet. Res. 25, e47621. https://doi.org/10.2196/47621 (2023).\n 5. Zandi, R. et al. Exploring diagnostic precision and triage proficiency: A comparative study of GPT-4 and bard in addressing \ncommon ophthalmic complaints. Bioengineering  (Basel Switzerland)  11 https://doi.org/10.3390/bioengineering11020120 (2024).\n 6. Passby, L., Jenko, N. & Wernham, A. Performance of ChatGPT on specialty certificate examination in dermatology multiple-choice \nquestions. Clin. Exp. Dermatol. 49, 722–727. https://doi.org/10.1093/ced/llad197 (2024).\n 7. Yu, P . et al. Performance of ChatGPT on the Chinese postgraduate examination for clinical medicine: survey study. JMIR Med. \nEduc. 10, e48514. https://doi.org/10.2196/48514 (2024).\n 8. Liu, S. et al. Using AI-generated suggestions from ChatGPT to optimize clinical decision support. J. Am. Med. Inf. Association: \nJAMIA. 30, 1237–1245. https://doi.org/10.1093/jamia/ocad072 (2023).\n 9. Suárez, A. et al. Unveiling the ChatGPT phenomenon: evaluating the consistency and accuracy of endodontic question answers. \nInt. Endod. J. 57, 108–113. https://doi.org/10.1111/iej.13985 (2024).\n 10. Akakpo, M. G. The role of care-seeking behavior and patient communication pattern in online health information-seeking \nbehavior - a cross-sectional survey. Pan Afr. Med. J. 42, 124. https://doi.org/10.11604/pamj.2022.42.124.33623 (2022).\n 11. Chandran, V . P . et al. Mobile applications in medical education: A systematic review and meta-analysis. PloS One. 17, e0265927. \nhttps://doi.org/10.1371/journal.pone.0265927 (2022).\n 12. Alowais, S. A. et al. Revolutionizing healthcare: the role of artificial intelligence in clinical practice. BMC Med. Educ. 23  h t t p s : / / d o \ni . o r g / 1 0 . 1 1 8 6 / s 1 2 9 0 9 - 0 2 3 - 0 4 6 9 8 - z     (2023).\n 13. Moons, P . & Van Bulck, L. ChatGPT: can artificial intelligence Language models be of value for cardiovascular nurses and allied \nhealth professionals. Eur. J. Cardiovasc. Nurs. 22, e55–e59. https://doi.org/10.1093/eurjcn/zvad022 (2023).\n 14. Hopkins, A. M., Logan, J. M., Kichenadasse, G. & Sorich, M. J. Artificial intelligence chatbots will revolutionize how cancer patients \naccess information: ChatGPT represents a paradigm-shift. JNCI Cancer Spectr. 7 https://doi.org/10.1093/jncics/pkad010 (2023).\n 15. Wang, L. et al. Prompt engineering in consistency and reliability with the evidence-based guideline for LLMs. NPJ Digit. Med. 7 \nhttps://doi.org/10.1038/s41746-024-01029-4 (2024).\nScientific Reports |        (2025) 15:19028 8| https://doi.org/10.1038/s41598-025-04309-5\nwww.nature.com/scientificreports/\n 16. Azimi, I., Qi, M., Wang, L., Rahmani, A. M. & Li, Y . Evaluation of LLMs accuracy and consistency in the registered dietitian exam \nthrough prompt engineering and knowledge retrieval. Sci. Rep. 15, 1506. https://doi.org/10.1038/s41598-024-85003-w (2025).\n 17. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. 35  24824–24837 (2022).\n 18. Lucas, M. M., Y ang, J., Pomeroy, J. K. & Y ang, C. C. Reasoning with large Language models for medical question answering. J. Am. \nMed. Inf. Association: JAMIA. 31, 1964–1975. https://doi.org/10.1093/jamia/ocae131 (2024).\n 19. Tsao, C. W . et al. Heart disease and stroke Statistics-2023 update: A report from the American heart association. Circulation 147, \ne93–e621. https://doi.org/10.1161/cir.0000000000001123 (2023).\n 20. Lee, T. J. et al. Evaluating ChatGPT responses on atrial fibrillation for patient education. Cureus 16, e61680.  h t t p s : / / d o i . o r g / 1 0 . 7 7 5 \n9 / c u r e u s . 6 1 6 8 0     (2024).\n 21. Hillmann, H. A. K. et al. Accuracy and comprehensibility of chat-based artificial intelligence for patient information on atrial \nfibrillation and cardiac implantable electronic devices. Europace Eur. Pac.  Arrhythm. Cardiac Electrophysiol.   J. Work.  Groups \nCardiac Pac. Arrhythm. Cardiac Cell. Electrophysiol. Eur. Soc. Cardiol. 26 https://doi.org/10.1093/europace/euad369 (2023).\n 22. Van Gelder, I. C. et al. 2024 ESC guidelines for the management of atrial fibrillation developed in collaboration with the European \nassociation for Cardio-Thoracic surgery (EACTS). Eur. Heart J. 45, 3314–3414. https://doi.org/10.1093/eurheartj/ehae176 (2024).\n 23. Shi, S. et al. Prevalence and risk of atrial fibrillation in China: A National cross-sectional epidemiological study. Lancet Reg. Health \nWestern Pac. 23, 100439. https://doi.org/10.1016/j.lanwpc.2022.100439 (2022).\n 24. Wang, Y ., Chen, Y . & Sheng, J. Assessing ChatGPT as a medical consultation assistant for chronic hepatitis B: Cross-Language \nstudy of english and Chinese. JMIR Med. Inf. 12, e56426. https://doi.org/10.2196/56426 (2024).\n 25. Suga, T., Uehara, O., Abiko, Y . & Toyofuku, A. Evaluating large Language models for burning mouth syndrome diagnosis. J. Pain \nRes. 18, 1387–1405. https://doi.org/10.2147/jpr.S509845 (2025).\n 26. Zhao, F . F . et al. Benchmarking the performance of large Language models in uveitis: a comparative analysis of ChatGPT-3.5, \nChatGPT-4.0, Google Gemini, and anthropic Claude3. Eye (London England). 39, 1132–1137.  h t t p s : / / d o i . o r g / 1 0 . 1 0 3 8 / s 4 1 4 3 3 - 0 2 \n4 - 0 3 5 4 5 - 9     (2025).\n 27. Salmi, L. et al. A proof-of-concept study for patient use of open notes with large Language models. JAMIA Open. 8, ooaf021. \nhttps://doi.org/10.1093/jamiaopen/ooaf021 (2025).\n 28. Patel, D. et al. Evaluating prompt engineering on GPT-3.5’s performance in USMLE-style medical calculations and clinical \nscenarios generated by GPT-4. Sci. Rep. 14, 17341. https://doi.org/10.1038/s41598-024-66933-x (2024).\n 29. Ting, Y . T. et al. Performance of ChatGPT incorporated chain-of-thought method in bilingual nuclear medicine physician board \nexaminations. Digit. Health. 10, 20552076231224074. https://doi.org/10.1177/20552076231224074 (2024).\n 30. Zapf, A., Castell, S., Morawietz, L. & Karch, A. Measuring inter-rater reliability for nominal data - which coefficients and confidence \nintervals are appropriate? BMC Med. Res. Methodol. 16 https://doi.org/10.1186/s12874-016-0200-9 (2016).\n 31. Walker, H. L. et al. Reliability of medical information provided by ChatGPT: assessment against clinical guidelines and patient \ninformation quality instrument. J. Med. Internet. Res. 25, e47479. https://doi.org/10.2196/47479 (2023).\n 32. Miyazaki, Y . et al. Performance of ChatGPT-4o on the Japanese medical licensing examination: evalution of accuracy in Text-Only \nand Image-Based questions. JMIR Med. Educ. 10, e63129. https://doi.org/10.2196/63129 (2024).\n 33. Mihalache, A., Huang, R. S., Popovic, M. M. & Muni, R. H. ChatGPT-4: an assessment of an upgraded artificial intelligence chatbot \nin the united States medical licensing examination. Med. Teach. 46, 366–372. https://doi.org/10.1080/0142159x.2023.2249588 \n(2024).\n 34. Roos, J., Kasapovic, A., Jansen, T. & Kaczmarczyk, R. Artificial intelligence in medical education: comparative analysis of ChatGPT, \nBing, and medical students in Germany. JMIR Med. Educ. 9, e46482. https://doi.org/10.2196/46482 (2023).\n 35. Lin, S. Y ., Chan, P . K., Hsu, W . H. & Kao, C. H. Exploring the proficiency of ChatGPT-4: an evaluation of its performance in the Taiwan \nadvanced medical licensing examination. Digit. Health. 10, 20552076241237678. https://doi.org/10.1177/20552076241237678 \n(2024).\n 36. Wang, Z. et al. Assessing the role of GPT-4 in thyroid ultrasound diagnosis and treatment recommendations: enhancing \ninterpretability with a chain of thought approach. Quant. Imaging Med. Surg. 14, 1602–1615.  h t t p s : / / d o i . o r g / 1 0 . 2 1 0 3 7 / q i m s - 2 3 - 1 1 \n8 0     (2024).\n 37. Wang, X. et al. Self-consistency improves chain of thought reasoning in language models. (2022).\n 38. Seghier, M. L. ChatGPT: not all languages are equal. Nature 615, 216. https://doi.org/10.1038/d41586-023-00680-3 (2023).\n 39. Song, E. S. & Lee, S. P . Comparative analysis of the response accuracies of large Language models in the Korean National dental \nhygienist examination across Korean and english questions. Int. J. Dental Hygiene. https://doi.org/10.1111/idh.12848 (2024).\n 40. Pak, R., Rovira, E. & McLaughlin, A. Polite AI mitigates user susceptibility to AI hallucinations. Ergonomics 1–11.  h t t p s : / / d o i . o r g / \n1 0 . 1 0 8 0 / 0 0 1 4 0 1 3 9 . 2 0 2 4 . 2 4 3 4 6 0 4     (2024).\n 41. Hatem, R., Simmons, B. & Thornton, J. E. A call to address AI hallucinations and how healthcare professionals can mitigate their \nrisks. Cureus 15, e44720. https://doi.org/10.7759/cureus.44720 (2023).\nAuthor contributions\nR.Z. provided the concept. Z.L. and C.Y . designed the study. Z.L. and Y .C collected the data. Z.L. and A.G per-\nformed the management and the analysis of the data. All authors interpreted the results of the analysis. The first \ndraft of the manuscript was written by Z.L and F .L. R.Z. reviewed and corrected the article. All the authors have \naccepted responsibility for the entire content of this submitted manuscript and approved submission. All authors \ncontributed to the article and approved the submitted version.\nFunding\nThis work was supported by grants from 1.3.5 Project for Disciplines of Excellence-Clinical Research Incubation \nProject, West China Hospital of Sichuan University (No. 2023HXFH002), Sichuan Science and Technology Pro-\ngram (No. 2024YFFK0046).\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 0 4 3 0 9 - 5     .  \nCorrespondence and requests for materials should be addressed to R.Z.\nScientific Reports |        (2025) 15:19028 9| https://doi.org/10.1038/s41598-025-04309-5\nwww.nature.com/scientificreports/\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:19028 10| https://doi.org/10.1038/s41598-025-04309-5\nwww.nature.com/scientificreports/",
  "topic": "Atrial fibrillation",
  "concepts": [
    {
      "name": "Atrial fibrillation",
      "score": 0.7970286011695862
    },
    {
      "name": "Computer science",
      "score": 0.5826464295387268
    },
    {
      "name": "Cardiology",
      "score": 0.37870410084724426
    },
    {
      "name": "Internal medicine",
      "score": 0.3274729549884796
    },
    {
      "name": "Medicine",
      "score": 0.32732176780700684
    }
  ]
}