{
  "title": "Local Representation is Not Enough: Soft Point-Wise Transformer for Descriptor and Detector of Local Features",
  "url": "https://openalex.org/W3191460670",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2127972637",
      "name": "Zihao Wang",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2117882643",
      "name": "Xueyi Li",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2086285432",
      "name": "Zhen Li",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2775929773",
    "https://openalex.org/W2979458572",
    "https://openalex.org/W3011651601",
    "https://openalex.org/W2892219791",
    "https://openalex.org/W2797807840",
    "https://openalex.org/W6803771590",
    "https://openalex.org/W3112016055",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3012662373",
    "https://openalex.org/W2884354140",
    "https://openalex.org/W2614218061",
    "https://openalex.org/W3043735169",
    "https://openalex.org/W2795296309",
    "https://openalex.org/W2951870616",
    "https://openalex.org/W2740418457",
    "https://openalex.org/W6759363029",
    "https://openalex.org/W3008625353",
    "https://openalex.org/W2737260104",
    "https://openalex.org/W3037854993",
    "https://openalex.org/W6864014924",
    "https://openalex.org/W6791858558",
    "https://openalex.org/W6868564194",
    "https://openalex.org/W4288277460",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W3107939686",
    "https://openalex.org/W3173957243",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963154697",
    "https://openalex.org/W3106728613",
    "https://openalex.org/W2963284197",
    "https://openalex.org/W3120857301",
    "https://openalex.org/W2471962767",
    "https://openalex.org/W3040304705",
    "https://openalex.org/W3043075211",
    "https://openalex.org/W2964157791",
    "https://openalex.org/W3034411221",
    "https://openalex.org/W3009931536",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W2167667767",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3034303554",
    "https://openalex.org/W2963748588",
    "https://openalex.org/W2952968720",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3109615112"
  ],
  "abstract": "Significant progress has been witnessed for the descriptor and detector of local features, but there still exist several challenging and intractable limitations, such as insufficient localization accuracy and non-discriminative description, especially in repetitive- or blank-texture regions, which haven't be well addressed. The coarse feature representation and limited receptive field are considered as the main issues for these limitations. To address these issues, we propose a novel Soft Point-Wise Transformer for Descriptor and Detector, simultaneously mining long-range intrinsic and cross-scale dependencies of local features. Furthermore, our model leverages the distinct transformers based on the soft point-wise attention, substantially decreasing the memory and computation complexity, especially for high-resolution feature maps. In addition, multi-level decoder is constructed to guarantee the high detection accuracy and discriminative description. Extensive experiments demonstrate that our model outperforms the existing state-of-the-art methods on the image matching and visual localization benchmarks.",
  "full_text": "Local Representation is Not Enough:\nSoft Point-wise Transformer for Descriptor and Detector of Local Features\nZihao Wang1 , Xueyi Li2 and Zhen Li1\u0003\n1 School of Automation, Beijing Institute of Technology, China\n2 School of Computer Science and Technology, Beijing Institute of Technology, China\nfzhwang1694, xueyili, zhenlig@bit.edu.cn\nAbstract\nSigniﬁcant progress has been witnessed for the de-\nscriptor and detector of local features, but there\nstill exist several challenging and intractable lim-\nitations, such as insufﬁcient localization accuracy\nand non-discriminative description, especially in\nrepetitive- or blank-texture regions, which haven’t\nbe well addressed. The coarse feature representa-\ntion and limited receptive ﬁeld are considered as\nthe main issues for these limitations. To address\nthese issues, we propose a novel Soft Point-Wise\nTransformer for Descriptor and Detector, simulta-\nneously mining long-range intrinsic and cross-scale\ndependencies of local features. Furthermore, our\nmodel leverages the distinct transformers based on\nthe soft point-wise attention, substantially decreas-\ning the memory and computation complexity, es-\npecially for high-resolution feature maps. In ad-\ndition, multi-level decoder is constructed to guar-\nantee the high detection accuracy and discrimina-\ntive description. Extensive experiments demon-\nstrate that our model outperforms the existing state-\nof-the-art methods on the image matching and vi-\nsual localization benchmarks.\n1 Introduction\nEstablishing accurate correspondences among images plays a\ncrucial role in many Computer Vision tasks, including but not\nlimited to wide-baseline stereo, image retrieval, visual local-\nization, Structure-from-Motion and 3D construction. Such\ncorrespondences are generally estimated by matching local\nfeatures, which comprise keypoints detection and description.\nKeypoints detection is to predict the coordinate of the key-\npoint in the image, and the description is to generate a vec-\ntor describing the image patch around the keypoint. How-\never, environmental changes, including viewpoint and illumi-\nnation, make the pipeline particularly challenging.\nOne of the key challenges in keypoints detection and de-\nscription is the local representation short of identiﬁcation\nwhich is derived from limited receptive ﬁeld, especially in the\n\u0003Corresponding author.\nblank ground or repetitive texture such as white-black chess-\nboard. Many existing methods design an extra block to iden-\ntify such regions and ﬁlter them when detecting [Revaud et\nal., 2019 ]. While the paradox is that the typical repetitive\nsubstance, i.e., chessboard is widely applied in the camera\ncalibration [Zhang, 2000], which requires rigorous accurate\ncorrespondences. The critical difference is that the keypoints\ndescription for calibration is the relative representation based\non other adjacent-to-remote keypoints and global informa-\ntion.\nTherefore, it is conceptually considered that the limited\nlocal representation for description of local features is not\nenough and global contextual information is as important\nto descriptor. Inspired by the Transformer’s success in\nNLP [Vaswani et al., 2017], we propose an elaborate Trans-\nformer structure to capture long-range dependencies, enrich-\ning the representation of local features and ﬁxing the match-\ning issues.\nAnother key challenge of keypoints detection and descrip-\ntion is to coordinately solve two subtasks, i.e., keypoints lo-\ncalization and classiﬁcation. The former requires the model\nto capture keypoints position accurately, while the latter ex-\npects the model to extract high-level semantic information of\nthe keypoints. Recent joint detection and description meth-\nods extract keypoints from the deep but coarse feature maps,\nleading to defective localization accuracy. Therefore a trans-\nformer pyramid is conducted to ﬁx such issues. Three kinds\nof attention modules are developed to mine cross-level and\nintrinsic dependencies, enabling interacting features across\nspace. The multi-level descriptor and detector based on the\npyramid promises reliable pixel-level prediction.\nFurthermore, the high-resolution feature maps in the shal-\nlow levels require heavy computation and memory cost, lim-\niting the potential beneﬁt of transformer in practical applica-\ntion. So most methods usually adopt the attention operation\non the deeper coarse feature maps to economize the compu-\ntation sources. Beneﬁcial from the soft point-wise selection\nmodule, we take the detected keypoints as the Keys set in our\nsoft point-wise transformer, so as to decrease the dense afﬁn-\nity matrix complexity fromO(n2) to O(const\u0002n), squeezing\nthe main cost in the transformer.\nThe main contributions of this paper are summarized as\nfollows. Firstly, an attention-based transformer is devel-\noped to capture long-range dependencies, which is crucial for\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1150\ngenerating discriminative description. Secondly, the cross-\nscale attention module and multi-level decoder are conducted\nto predict more accurate pixel-level scale-invariant keypoints\ndetection. Thirdly, we propose the novel soft point-wise\ntransformer, leveraging the detected keypoints to decrease the\nmemory and computation complexity remarkably. Lastly,\nthe learned network signiﬁcantly outperforms prior state-of-\nthe-art methods.\n2 Related Works\nIn this section, we give a brief review of local features learn-\ning based on CNNs and computer vision transformer.\nJoint local features learning. Recently, the increasing at-\ntention has been focused on the joint learning of feature de-\nscriptor and detector. In terms of descriptor learning, the\nranking loss [Tian et al., 2017; Heet al., 2018] has been pri-\nmarily used as ade-facto standard. However, there exist some\nconﬂicts between descriptor and detector such as big-or-small\nreceptive ﬁeld and deep-or-shallow features.\nTo break through the limitation of restricted receptive ﬁeld,\nD2Net [Dusmanu et al., 2019] used the deep stacked convolu-\ntional network as backbone and detected-and-described upon\nthe last feature maps. R2D2 [Revaud et al., 2019] utilized di-\nlated convolutions to improve the keypoints localization ac-\ncuracy and generate pixel-level description, while limited in\nthe mutual-vision boundary area. More recent ASLFeat [Luo\net al., 2020] used multi-level keypoints predictions to restore\nspatial resolution and low-level details.\nVisual transformer. Transformer [Vaswani et al., 2017]\nand its variants have proven its success of unsupervised or\nself-supervised pertaining frameworks in various NLP tasks.\nTherefore, there are many attempts to explore the beneﬁts\nof Transformer in computer vision tasks [Li et al., 2020 ].\nDANet [Fu et al., 2019] developed the context information\nby combining spatial and channel attention in the scene seg-\nmentation. Non-local Networks [Wang et al., 2018] utilized a\nself-attention mechanism, enabling a single feature from any\nposition to perceive features of all the other positions, thus\nharvesting full-image contextual information. Recent meth-\nods also attempt to replace the convolutional neural network\nwith transformer pipeline, like ViT [Dosovitskiy et al., 2020]\nin image classiﬁcation, DETR [Carion et al., 2020] in object\ndetection and SETR [Zheng et al., 2020] in semantic segmen-\ntation. While there exists few related work in the descriptor\nand detector of local features.\n3 Methodology\nTwo ingredients are essential for adopting transformer on\njoint local feature learning: (1) an architecture that outputs\nkeypoints detection and description simultaneously; (2) atten-\ntion optimization for efﬁcient contextual information capture.\n3.1 Architecture\nAs illuminated in Figure 1, the overall architecture of soft\npoint-wise transformer for description and detection of local\nfeatures is designed as an encoder-decoder pipeline.\nFeature Uniformization. To exploit the inter-dependencies\nbetween channel maps, a feature uniformization module is\nbuilt at ﬁrst. Given a local feature F 2RC\u0002H\u0002W , we ﬁrst\nreshape F to RC\u0002N , and then perform a matrix multiplication\nbetween the F and the transpose of F to compute the channel\nattention map as:\nXji = exp(Fi \u0001F>\nj )\nPC\ni=1 exp(Fi \u0001F>\nj )\n; (1)\nin which Xji measures the ith channel’s impact on the jth\nchannel. Then we perform the weighted element-wise sum\noperation and 1\u00021 convolution to obtain the ﬁxed dimension\nfeature map e2RC\u0002H\u0002W :\nej = conv\n  \n\r\nCX\ni=1\nXjiFi\n!\n+ Fj\n!\n; (2)\nwhere the \ris a learning scale parameter. The ﬁnal feature of\neach channel is a weighted sum of features of all channels and\noriginal features, which models the long-range dependencies\nbetween feature maps, improving feature discriminability.\nCross-scale Attention. By mapping each point’s represen-\ntation into a latent ﬁxed dimensional embedding space, we\nobtain a 1D sequence of point embeddings for a certain scale\nof the input image I. To encode the point spatial information,\nwe learn a speciﬁc embedding pi for every location iwith a\nMulti-Layer Perception, which is added to ei to form the ﬁ-\nnal sequence input E = fe1 + p1;e2 + p2;:::;e N + pN g.\nTherefore, the spatial information is kept through the order-\nless self-attention and residual fusion.\nFollowing the non-local operation [Wang et al., 2018], we\ndeﬁne the generic attention operation as:\nyi =\nP\nj exp(Qi \u0001K>\nj )Vj\nP\nj exp(Qi \u0001K>\nj ) : (3)\nHere iis the index of the output position and j is the index\nthat enumerates all possible positions. The fQ;K;V grep-\nresents the query, key and value for the attention, computed\nas fEWQ;EWK;EWV g. fWQ;WK;WV g2 RC\u0003d are the\nlearnable parameters of three linear projection layers and dis\nthe dimension of fQ;K;V g.\nWhen the fQ;Kgcomes from the same feature map, we\ncall it as In-Scale attention. We further extend theKfrom the\ndeeper or shallower feature maps, and we call it as Up-Scale\nand Down-Scale attention, respectively. The Up-Scale atten-\ntion is developed to enrich the high-level feature representa-\ntion of “patch” with the lower-level feature representation of\n“point”. And the Down-Scale attention is developed in the\nopposite direction. The Up-Scale and Down-Scale attention\njointly mine the cross-level dependencies to enrich the local\nfeature maps from shallow to deep layers.\nResidual Fusion. The In-Scale and Cross-scale attention\ngenerate three intensive feature maps with the same dimen-\nsion, exploiting different scale information independently.\nThen we fuse these separate feature maps into a comprehen-\nsive feature map. Different from simply adding or concentrat-\ning them together [Lin et al., 2017], we propose the residual\nfusion block to better combine features.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1151\nFigure 1: Overall structure of our proposed SPTD2 network. The network is extended into a Siamese network with the image pair as\ninput during the training time. All points are taken as the Q set, while only the top-k keypoints are selected as the K set. PS and FU represents\nthe point-wise selection and feature uniformization. CNN and Position is the feature embedding and position embedding for local points.\nTo allow the network to concentrate on more discrimi-\nnative features, we ﬁrst compute the residual between fea-\ntures FD 2Rd\u0002H\u0002W from down-scale attention and features\nFI 2Rd\u0002H\u0002W from in-scale attention. Then we can obtain\nthe synthetic bottom-to-top representation ^FI:\n^FI = conv(FI \u0000FD) +FI: (4)\nIntuitively, the residual feature represents the abundant shape\ndetails like corner, edge and blob existing in shallow layer\nwhile degraded in the deep representation.\nSimilar operation is also adopt between the updated fea-\ntures ^FI and features FU 2Rd\u0002H\u0002W from up-scale atten-\ntion:\nP = conv( ^FI \u0000FU ) + ^FI: (5)\nFinally we obtain the fusion feature maps P 2 Rd\u0002H\u0002W\nfed into the decoder to output the keypoints detection and de-\nscription. The residual block allows the network to focus on\nonly the distinct information among different levels, while\npassing the common knowledge, enabling a more discrimina-\ntive residual feature learning compared with trivial adding or\nconcatenating.\nMulti-level Decoder. Above feature uniformization, cross-\nscale attention and residual fusion modules make up the en-\ncoder of the transformer structure. As illuminated in the Fig-\nure 1, a dual-head decoder, i.e., descriptor and detector is\nadopt on multi-level feature maps to extract multi-scale de-\nscriptions and keypoints. The detector and descriptor simul-\ntaneously output the 3D description D 2Rd\u0002H\u0002W and the\ndetection score map S 2[0;1]H\u0002W .\nDescriptor. We set the 3D tensor P as a dense set of de-\nscriptor vectors D. These descriptor vectors can be readily\ncompared between images to establish correspondences us-\ning the Euclidian distance with the hypothesis that the same\nkeypoints will produce similar descriptors even in different\nconditions. In practice, a channel-wise L2-Normalization is\napplied to generate more robust feature presentation prior to\ncomparing them.\nDij = Pij\nkPijk2\n; (6)\nwith i= 1:::H and j = 1:::W .\nDetector. We also suppose that the 3D tensorP as a collec-\ntion of 2D response maps at different channels. These detec-\ntion score maps are analogous to the Difference-of-Gaussian\n(DoG) response maps obtained in Scale Invariant Feature\nTransform (SIFT). In practice, an element-wise square op-\neration followed by a 1 \u00021 convolution and softmax function\nare adopt to obtain the detection response score S of each\ndescriptor.\nSij = \u0012\n\u0000\nconv(P2\nij)\n\u0001\n; (7)\nwith i = 1:::H and j = 1:::W , where the \u0012(\u0003) represents\nthe softmax operation. Only the locations with high conﬁ-\ndence are selected as keypoints. Similar to multi-scale object\ndetection, a non-maximum suppression (NMS) is applied to\nremove the detection points that are spatially too close.\nNote that we build the feature maps from the encoder as a\nfeature pyramid. The dual-head multi-level decoder extracts\nthe ﬁnal results upon the multi-level pyramid.\n3.2 Soft Point-wise Attention\nRecent works show that the keypoints located in the uniform\nand even well textured regions like tree leafages or ocean\nwaves, could lead to bad matching [Revaud et al., 2019]. So\nsome works learn to distinguish such regions and ﬁlter them\nas less discriminative keypoints. While directly deleting such\nkeypoints will result in defective detection accuracy and dis-\ncontinuities especially in large repetitive regions.\nThe attention module and position encoding will improve\nthe discrimination of the local features representation. While\nthe original attention module needs to generate enormous\nafﬁnity matrix to measure the relationships with the complex-\nity of O(N2), where N is the number of input points. High-\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1152\n(a) (b) (c) (d) (e)\nFigure 2: Comparisons with other efﬁcient attention module. From left to right: (a) Original attention module, (b) CCNet, (c) ISSA, (d)\nLocality-constrained FPT, (e) Our SPT. The red grid is the query, the green grids represent the keys set and the yellow is the instance in the\nimage. Our soft point-wise transformer can choose the most representative keypoints as the Keys set compared to other sota methods.\nresolution feature maps are essential for accurate keypoints\nlocalization, taking heavy computation and memory cost.\nTo address the above mentioned issue, our motivation\nis to replace the common single dense feature maps with\nsparse representative features. Without loss of generality,\nwe propose the soft point-wise attention module which ag-\ngregates contextual information with the most representative\nkeypoints, greatly reducing the complexity from O(N2) to\nO(const\u0002N).\nThe key of the soft point-wise attention is the soft key-\npoints selection, similar to our detector head but more efﬁ-\ncient. We require a point (i;j) being selected by a hard dual-\nmaximum strategy, i.e., the feature value in the location (i;j)\nof channel k is the local maximum both in spatial-wise and\nchannel-wise. To be amenable for back-propagation during\nthe training procedure, the above selection procedure is soft-\nened as follows.\nThe soft spatial detection score and soft channel score are\ndeﬁned as:\n\u000bk\nij = exp(Fk\nij)P\n(i0;j0)2N(i;j) exp(Fk\ni0;j0 );\fk\nij = Fk\nij\nmaxtFt\nij\n; (8)\nwhere F is the feature map and N(i;j) is the set of 8 neigh-\nbors of the pixel (i;j). The soft selection takes both scores\ninto account and performs an image-level normalization:\nsij =\nmax\nk\n(\u000bk\nij \u0001\fk\nij)\nP\n(i0;j0) max\nk0\n(\u000bk0\ni0j0 \u0001\fk0\ni0j0 ): (9)\nOnly the locations with high conﬁdence (greater than the\nkeypoints detection threshold) will be selected as the keys set\n^K = \u001e(K) 2RC\u0002const, where \u001e(\u0003) is the soft keypoint se-\nlection operation. And the attention in Eq.(3) with soft point-\nwise selection is computed as:\nyi =\nP\nj exp(Qi \u0001\u001e(K)>\nj )Vj\nP\nj exp(Qi \u0001\u001e(K)>\nj )\n: (10)\nIn addition, we compare our soft point-wise attention\nblock with the original non-local block and other optimiza-\ntion methods [Huang et al., 2019b; Huang et al., 2019a;\nZhang et al., 2020] in Figure 2.\n3.3 Implementation\nTraining. During the training time, the SPTD2 will be\nextended into a Siamese Network to simultaneously gen-\nerate the keypoints detection score maps fS;S0gand de-\nscriptors fD;D0gof the correspond image pair fI;I 0g 2\nR3\u0002H\u0002W . Our SPTD2 is independent of the backbone\nnetwork, we use the VGG16 BN to evaluate our models\nduring the experiments. In practice, the ﬁrst feature map\nF1 2 RC\u0002H=2\u0002W=2 and the feature maps smaller than\nRC\u0002H=16\u0002W=16 are dropped when building the transformer\npyramid. For every input image pair, we select a random\n200 \u0002200 crop centered around one correspondence.\nTesting. During the testing time, the single image is fed into\nthe model to generate the detection score maps and descrip-\ntors with the original resolution. All detection results will be\naligned with the original image resolution and the descrip-\ntions are then bilinear interpolated at the reﬁned positions. A\nnon-maximum suppression is also applied on the multi-head\ndetection score maps to remove the overlapping keypoints.\nLoss design. As illuminated in the Figure 1, the loss func-\ntion integrates the detection lossLdet and the description loss\nLdes. The detection loss is formulated as:\nLdet(I;I\n0\n) =\nX\nl\nwl(Lc(Sl;S0l) +r(Lp(Sl) +Lp(S0l)));\n(11)\nwhere the Lc computes the cosine similarity of the corre-\nspond detection score maps and the Lp tries to maximize the\nlocal peak of the detection score maps [Revaud et al., 2019].\nThe description loss is written as:\nLdes(I;I\n0\n) =\nX\nl\nwl\nX\nc2C\nSl\ncS0l\ncP\nq2C SlqS0l\nq\nM(dl\nc;d0l\nc); (12)\nwhere Cis the correspondences between Iand I0, Sc and S0l\nc\nare their detection scores, dl\nc and d0l\nc are their corresponding\ndescriptors, and the M(\u0003) is the circle loss [Sun et al., 2020]\nfor representation learning.\nThe ﬁnal loss function is formulated as Ldet + \u0015Ldes.\n4 Experiments\nIn this section, we evaluate the performance of the proposed\nmodel on the image matching and visual localization tasks.\nWe show that the our model can achieve state-of-the-art per-\nformance on these tasks. Moreover, extensive experiments\nfor ablation study show that our SPTD2 is effective.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1153\nAUCMethod Pub.#Features#Matches2px 5px 10pxSIFT IJCV4.1K– 39.49 49.57 55.15HesAff + RootSIFTNIPS176.7K2.9K39.99 52.25 60.40HAN + HN++ECCV183.9K2.0K42.61 56.85 65.50LF-Net NIPS180.5K0.2K38.74 48.69 53.59SuperPointCVPR181.7K0.9K44.08 59.04 68.09DELF ICCV174.6K1.9K44.73 49.70 58.91SIFT + ContextDescCVPR194.1K1.7K47.23 58.25 65.33D2-Net MSCVPR194.9K1.7K19.49 37.78 56.17R2D2 MSNIPS194.9K1.7K43.35 64.17 75.18SIFT + LISRDECCV204.1K– 48.12 57.80 62.50DISK NIPS207.7K3.9K– 69.80 –Key.NetICCV19– – 40.87 56.04 65.30D2-Net + RefECCV20– – 54.24 67.62 75.57ASLFeat MSCVPR204.8K2.1K50.10 66.93 76.90Ours – 5.0K1.9K56.20 72.17 79.80\nTable 1: Comparisons on HPatches with the area under the over-\nall curve (AUC) up to 2, 5 and 10 pixels error threshold . Our\nSPTD2 reaches the state-of-the-art on all thresholds beneﬁting by\nmulti-level detection and feature fusion description. Most results are\nprovided by the authors, which explains why some data are missing.\n4.1 Training Details\nDatasets. The acquisition of sufﬁcient ground-truth super-\nvision to train keypoints detector and descriptor has been a\nbottleneck over years due to the ill-deﬁned interest points.\nSo we treat the keypoints detection and description as a self-\nsupervised task, to make the detector discover better and eas-\nier keypoints by deﬁning local maxima in the detection score\nmaps as the target. A similar pipeline as[Revaud et al., 2019]\nis developed to obtain dense ground-truth matching data.\nThe image pairs are composed by two aspects: 1) using the\nexisting image pairs extracted from the Aachen Day-Night\ndataset [Sattler et al., 2018] about the same sceneries and 2)\napplying a manual transformation such as homography trans-\nform or random rotation on pascal voc and the web images\n[Radenovi´c et al., 2018] to obtain image pairs.\nParameters. A NVIDIA RTX 3090 card is used to train our\nmodel using Adam optimizer with \f1 = 0:9;\f2 = 0:999 for\n30 epochs on the datasets. The initial learning rate is set to\n1e\u00004 and decayed to 5e\u00005 in 30 epoch with 8 batch size. The\ntesting is conducted on the same machine. The r and \u0015 in\nloss function are set to 0.5 and 1, respectively. The multi-\nlevel balance parameters wl in Equ.(11) and Equ.(12) are all\nset to 1.\n4.2 Image Matching\nWe ﬁrst evaluate our SPTD2 on the image matching task.\nDatasets. To compare with other methods fairly, our\nmethod is evaluated on the HPatches dataset [Balntas et al.,\n2017] including 116 different sequences of 6 images with ac-\ncurate homography. To compare with other methods fairly, 8\nhigh-resolution sequences are also excluded, leaving 52 and\n56 sequences with illumination or viewpoint variations re-\nspectively.\nEvaluation metrics. For fair comparison, we utilize three\nmetrics, mean matching accuracy (MMA), keypoint repeata-\nLF-NetHAN+HN++       Hes.Aff.+Root-SIFT SuperPointDELFSIFT+ContentDescD2-Net MSR2D2 MSASLFeatMSOurs\nFigure 3: The curves of mean matching accuracy (MMA) evalu-\nated at multiple error thresholds onHPatches dataset. “MS” denotes\nthat the multi-scale inference is enabled. Note that several meth-\nods in Table 1 are not plotted here because of no code or cache ﬁle\nreleased.\nbaseline FU IA UA DA RF MMA(%) MS(%) REP(%)\nX 62.26 25.57 63.11\nX X 74.00 28.37 67.38\nX X X 76.66 39.37 71.16\nX X X X 77.71 43.13 73.89\nX X X X X 77.77 44.32 75.86\nX X X X X X 78.33 44.76 78.28\nTable 2: Ablation study on different attention modules. The\nFU and RF represents the feature uniformization and residual fu-\nsion, respectively. The IA, UA and DA is the in-scale, up-scale and\ndown-scale attention modules. The results are all with the 3px error\nthreshold.\nbility (Rep) and matching score (MS) as evaluation metrics\nfollowing [DeTone et al., 2018 ]. The correct match is re-\nquired to be a mutual nearest neighbor during brute-force\nsearching. To evaluate the metrics fairly and accurately, the\npublic code from [Dusmanu et al., 2019 ] and [Luo et al.,\n2020] is used to compute the corresponding metric.\nComparisons with other methods. We compare the mean\nmatching accuracy with the state-of-the-art methods, namely\nDELF [Noh et al., 2017], SuperPoint [DeTone et al., 2018],\nmulti-scale D2-Net [Dusmanu et al., 2019 ], R2D2 [Revaud\net al., 2019], ASLFeat [Luo et al., 2020], LISRD descriptors\nwith SIFT detector [Pautrat et al., 2020], DISK [Tyszkiewicz\net al., 2020], Key.Net[Barroso-Laguna et al., 2019], D2-Net\nand Reﬁnement [Dusmanu et al., 2020], HardNet++ descrip-\ntors with HesAFFNet regions and [Mishkin et al., 2018 ]\n(HAN + HN++), etc. Unless otherwise speciﬁed, we report\neither results reported in original papers, or derived from au-\nthors’ public implementations with default parameters. We\nlimit the maximum number of features of our method to 5K.\nAs shown in the Table 1 and Figure 3, SPTD2 achieves\noverall the best results regarding both illumination and view-\npoint variations at different thresholds. Speciﬁcally, SPTD2\ndelivers remarkable improvements upon other methods es-\npecially for low range error thresholds, which in particular\ndemonstrates that the keypoints localization error has been\nlargely reduced. Besides, our method notably outperforms\nthe more recent ASLFeat (78.33 vs 72.64 for MMA@3 over-\nall), which also applied multi-level detection.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1154\nMethod Resolution\nMemory(MB) GFLOPs\nSA 2168\n619\nDANet 2339 1110\nRCCA 427 804\nISSA 252 386\nOurs\n2048\u0002128\u0002128\n364 359\nTable 3: Efﬁciency comparison given input feature map of size\n2048 \u0002 128 \u0002 128 in inference stage.\nAblations on Attention module. We further conduct di-\nagnostic analysis to verify the effectiveness of the essential\nmodules in our approach. We use the VGG-structure as the\ndefault backbone for all the studies. The performance of our\nbaseline model with default parameters is given in the ﬁrst\nrow of the Table 2. The effect of each essential component of\nour SPTD2 on image matching task is shown as follows.\nAblations on soft point-wise transformer. As shown in\nTable 3, applying the soft point-wise attention module re-\nduces the computation complexity and GPU memory com-\npared to original attention module [Wang et al., 2018 ],\nRCCA [Huang et al., 2019b ], DANet [Fu et al., 2019 ],\nISSA [Huang et al., 2019a]. We further verify the impact\nof the detection threshold in the soft point-wise transformer.\nThe detection threshold determines the number of keypoints\nwhich will be kept in the Keys set. The computation complex-\nity will be higher with lower threshold. While derisory key-\npoints will inﬂuence the performance of the attention mod-\nule to capture enough information. It’s interesting that the\nsoft point-wise attention module will degrade into the origi-\nnal attention module when the threshold is set to 0. We then\nchoose the keypoints with the top-2K scores when the thresh-\nold is lower than 0.8 because of “CUDA out of memory”. We\nset the threshold to f0:6;0:8;0:9;0:95gand get correspond-\ning MMA@3 at f78:33;78:31;74:56;72:37g. So we set the\nthreshold at 0.8 to reach the best balance of performance and\ncomputation load.\n4.3 Visual Localization\nTo further verify the effectiveness of the novel SPTD2, we\nevaluate it on the task of visual localization, which aims to\nestimate the camera pose within a given scene using images\nsequence. The task was proposed in [Sattler et al., 2018] to\nevaluate the performance of local features in the context of lo-\ncalization. To evaluate our method fairly, we also produce the\npublic format of keypoints and compare with other methods\non the ofﬁcial evaluation server.\nDatasets. We resort the Aachen Day-Night dataset [Sat-\ntler et al., 2018] to demonstrate the effect on visual localiza-\ntion tasks, which contains images from the old inner city of\nAachen, Germany. The key challenge in the dataset lies on\nmatching images with extreme day-night changes.\nEvaluation metrics. The evaluation is done using The Vi-\nsual Localization Benchmark, which takes a pre-deﬁned vi-\nsual localization pipeline based on COMLAP [Schonberger\nand Frahm, 2016 ]. The successfully localized images are\ncounted within three error tolerances (0.25m, 2\u000e) / (0.5m, 5\u000e)\nMethod #Features Dim 0.25m, 2\u000e 0.5m, 5\u000e 5m, 10\u000e\nDay\nD2-Net 19.3K 512 83.7 91.6 96.5\nR2D2 10K 128 86.9 94.3 97.2\nASLFeat 10K 128 85.2 93.2 96.1\nOurs 10K 128 87.1 95.4 98.8\nNight V1.0\nD2-Net 19.3K 512 80.6 87.8 96.9\nR2D2 10K 128 79.6 87.8 95.9\nASLFeat 10K 128 82.7 87.8 95.9\nOurs 10K 128 78.8 89.3 99.0\nNight V1.1\nD2-Net 19.3K 512 68.1 85.9 97.9\nR2D2 10K 128 69.6 84.3 97.9\nASLFeat 10K 128 72.8 85.3 96.9\nOurs 10K 128 72.5 87.3 97.9\nTable 4: Performance on Aachen Day-Night dataset for visual lo-\ncalization. The benchmark website updates the evaluation metrics\nthis year. The results are derived from authors’ public implementa-\ntions with default parameters.\n/ (5m, 10\u000e), representing the maximum position error in me-\nters and degrees, respectively.\nResults. Our SPTD2 is compared with the typical joint de-\ntector and descriptor methods D2-Net, R2D2 and ASLFeat.\nNote that there exist some greater scores in the benchmark\nwebsite, while they use greater matching strategy, which\nis unfair to evaluate. Here all methods are evaluated with\nthe default matching strategy to compare fairly. As shown\nin Table 4, our SPTD2 performs surprisingly well under\nchallenging illumination changes especially for strict accu-\nracy metrics for the estimated pose. While in the night en-\nvironment setting, the cross-scale attention modules bring\nsome noises from the non-discriminative dark background,\nwhich hinders our performance. On the other hand, meth-\nods in Table 4, build image pyramid (MS) in inference to\nimprove the localization performance, while making low run-\nning speed. We employ the multi-scale detection and descrip-\ntion with the multi-level detector and descriptor in decoder,\nwhich is over 2 times quicker than MS operation. With 21=4\nscaling-factor MS, we improve the localization accuracy with\nf+1.7%,+2.3%,+1.8%gfor (0.25m, 2\u000e).\n5 Conclusions\nIn this paper, we propose a novel transformer-based archi-\ntecture to jointly learn the local features descriptor and de-\ntector. The novel soft point-wise transformer simultaneously\nmines the long-range intrinsic and cross-scale dependencies\nof local features. The cross-scale attention module and multi-\nlevel decoder can guarantee the keypoints localization accu-\nracy and discriminative descriptions especially in repetitive\nregions. Compared to other attention optimization methods,\nthe soft point-wise attention remarkably decreases the com-\nputation and memory complexity. Experiments show SPTD2\nsigniﬁcantly outperforms prior state-of-the-art methods.\nAcknowledgments\nThis research is supported by Research Project of Intelligent\nsense technology of swarming drone under Grant 301021304.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1155\nReferences\n[Balntas et al., 2017] Vassileios Balntas, Karel Lenc, Andrea\nVedaldi, and Krystian Mikolajczyk. Hpatches: A bench-\nmark and evaluation of handcrafted and learned local de-\nscriptors. In CVPR, pages 5173–5182, 2017.\n[Barroso-Laguna et al., 2019] Axel Barroso-Laguna, Edgar\nRiba, Daniel Ponsa, and Krystian Mikolajczyk. Key. net:\nKeypoint detection by handcrafted and learned cnn ﬁlters.\nIn ICCV, 2019.\n[Carion et al., 2020] Nicolas Carion, Francisco Massa,\nGabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. End-to-end object detection with\ntransformers. arXiv preprint arXiv:2005.12872, 2020.\n[DeTone et al., 2018] Daniel DeTone, Tomasz Malisiewicz,\nand Andrew Rabinovich. Superpoint: Self-supervised in-\nterest point detection and description. In CVPR, 2018.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[Dusmanu et al., 2019] Mihai Dusmanu, Ignacio Rocco,\nTomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii,\nand Torsten Sattler. D2-net: A trainable cnn for joint de-\nscription and detection of local features. In CVPR, 2019.\n[Dusmanu et al., 2020] Mihai Dusmanu, Johannes L\nSch¨onberger, and Marc Pollefeys. Multi-view optimiza-\ntion of local feature geometry. In ECCV, 2020.\n[Fu et al., 2019] Jun Fu, Jing Liu, Haijie Tian, Yong Li,\nYongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual at-\ntention network for scene segmentation. In CVPR, 2019.\n[He et al., 2018] Kun He, Yan Lu, and Stan Sclaroff. Lo-\ncal descriptors optimized for average precision. In CVPR,\n2018.\n[Huang et al., 2019a] Lang Huang, Yuhui Yuan, Jianyuan\nGuo, Chao Zhang, Xilin Chen, and Jingdong Wang. Inter-\nlaced sparse self-attention for semantic segmentation. In\nICCV, 2019.\n[Huang et al., 2019b] Zilong Huang, Xinggang Wang,\nLichao Huang, Chang Huang, Yunchao Wei, and Wenyu\nLiu. Ccnet: Criss-cross attention for semantic segmenta-\ntion. In ICCV, 2019.\n[Li et al., 2020] Xueyi Li, Tianfei Zhou, Jianwu Li, Yi Zhou,\nand Zhaoxiang Zhang. Group-wise semantic mining for\nweakly supervised semantic segmentation. AAAI, 2020.\n[Lin et al., 2017] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick,\nKaiming He, Bharath Hariharan, and Serge Belongie. Fea-\nture pyramid networks for object detection. In CVPR,\n2017.\n[Luo et al., 2020] Zixin Luo, Lei Zhou, Xuyang Bai,\nHongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian\nFang, and Long Quan. Aslfeat: Learning local features of\naccurate shape and localization. In CVPR, 2020.\n[Mishkin et al., 2018] Dmytro Mishkin, Filip Radenovic,\nand Jiri Matas. Repeatability is not enough: Learning\nafﬁne regions via discriminability. In ECCV, 2018.\n[Noh et al., 2017] Hyeonwoo Noh, Andre Araujo, Jack Sim,\nTobias Weyand, and Bohyung Han. Large-scale image re-\ntrieval with attentive deep local features. In ICCV, pages\n3456–3465, 2017.\n[Pautrat et al., 2020] R´emi Pautrat, Viktor Larsson, Mar-\ntin R. Oswald, and Marc Pollefeys. Online invariance se-\nlection for local feature descriptors. In ECCV, 2020.\n[Radenovi´c et al., 2018] Filip Radenovi ´c, Ahmet Iscen,\nGiorgos Tolias, Yannis Avrithis, and Ondˇrej Chum. Revis-\niting oxford and paris: Large-scale image retrieval bench-\nmarking. In CVPR, pages 5706–5715, 2018.\n[Revaud et al., 2019] Jerome Revaud, Philippe Weinzaepfel,\nC´esar De Souza, Noe Pion, Gabriela Csurka, Yohann\nCabon, and Martin Humenberger. R2d2: Repeatable and\nreliable detector and descriptor. In NeurIPS, 2019.\n[Sattler et al., 2018] Torsten Sattler, Will Maddern, Carl\nToft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg,\nDaniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef\nSivic, et al. Benchmarking 6dof outdoor visual localiza-\ntion in changing conditions. In CVPR, 2018.\n[Schonberger and Frahm, 2016] Johannes L Schonberger\nand Jan-Michael Frahm. Structure-from-motion revisited.\nIn CVPR, 2016.\n[Sun et al., 2020] Yifan Sun, Changmao Cheng, Yuhan\nZhang, Chi Zhang, Liang Zheng, Zhongdao Wang, and\nYichen Wei. Circle loss: A uniﬁed perspective of pair sim-\nilarity optimization. In CVPR, 2020.\n[Tian et al., 2017] Yurun Tian, Bin Fan, and Fuchao Wu. L2-\nnet: Deep learning of discriminative patch descriptor in\neuclidean space. In CVPR, 2017.\n[Tyszkiewicz et al., 2020] Michał Tyszkiewicz, Pascal Fua,\nand Eduard Trulls. Disk: Learning local features with pol-\nicy gradient. NeurIPS, 2020.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NeurIPS, 2017.\n[Wang et al., 2018] Xiaolong Wang, Ross Girshick, Abhinav\nGupta, and Kaiming He. Non-local neural networks. In\nCVPR, 2018.\n[Zhang et al., 2020] Dong Zhang, Hanwang Zhang, Jinhui\nTang, Meng Wang, Xiansheng Hua, and Qianru Sun. Fea-\nture pyramid transformer. In ECCV, 2020.\n[Zhang, 2000] Zhengyou Zhang. A ﬂexible new technique\nfor camera calibration. IEEE TPAMI, 2000.\n[Zheng et al., 2020] Sixiao Zheng, Jiachen Lu, Hengshuang\nZhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al.\nRethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers. arXiv preprint\narXiv:2012.15840, 2020.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1156",
  "topic": "Discriminative model",
  "concepts": [
    {
      "name": "Discriminative model",
      "score": 0.7526025772094727
    },
    {
      "name": "Computer science",
      "score": 0.7265660166740417
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6806750893592834
    },
    {
      "name": "Detector",
      "score": 0.5872440338134766
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5637772083282471
    },
    {
      "name": "Transformer",
      "score": 0.5177485346794128
    },
    {
      "name": "Feature extraction",
      "score": 0.5023536682128906
    },
    {
      "name": "Computation",
      "score": 0.4359636604785919
    },
    {
      "name": "Computer vision",
      "score": 0.37251725792884827
    },
    {
      "name": "Algorithm",
      "score": 0.21599295735359192
    },
    {
      "name": "Voltage",
      "score": 0.09242397546768188
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I125839683",
      "name": "Beijing Institute of Technology",
      "country": "CN"
    }
  ]
}