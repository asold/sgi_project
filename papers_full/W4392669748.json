{
  "title": "Transformers Go for the LOLs: Generating (Humourous) Titles from Scientific Abstracts End-to-End",
  "url": "https://openalex.org/W4392669748",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2320602905",
      "name": "Yanran Chen",
      "affiliations": [
        "University of Mannheim"
      ]
    },
    {
      "id": "https://openalex.org/A2073897212",
      "name": "Steffen Eger",
      "affiliations": [
        "University of Mannheim"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W809736386",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2970796111",
    "https://openalex.org/W2160424800",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2156118598",
    "https://openalex.org/W2963775850",
    "https://openalex.org/W4385572737",
    "https://openalex.org/W4286383191",
    "https://openalex.org/W2949555952",
    "https://openalex.org/W3104469895",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2001691451",
    "https://openalex.org/W2741164290",
    "https://openalex.org/W2571706648",
    "https://openalex.org/W224267383",
    "https://openalex.org/W4281760814",
    "https://openalex.org/W2964321064",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385567294",
    "https://openalex.org/W2963121782",
    "https://openalex.org/W3021041462",
    "https://openalex.org/W2096193168",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2032021697",
    "https://openalex.org/W2963970666",
    "https://openalex.org/W3104080943",
    "https://openalex.org/W2741438349",
    "https://openalex.org/W3188381677",
    "https://openalex.org/W2952626150",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4385574144",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W3101223450",
    "https://openalex.org/W2078706826",
    "https://openalex.org/W4287671850",
    "https://openalex.org/W4377431152",
    "https://openalex.org/W2293771131",
    "https://openalex.org/W4385574107",
    "https://openalex.org/W2252161778",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4323543728",
    "https://openalex.org/W2759604924",
    "https://openalex.org/W2799424953",
    "https://openalex.org/W3205396199",
    "https://openalex.org/W3105214104",
    "https://openalex.org/W2029050743",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2252147704",
    "https://openalex.org/W3035081708",
    "https://openalex.org/W3034425996",
    "https://openalex.org/W2788416615",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4321124389",
    "https://openalex.org/W4378765257",
    "https://openalex.org/W4385573757",
    "https://openalex.org/W2096733369",
    "https://openalex.org/W2127776561",
    "https://openalex.org/W1574454071",
    "https://openalex.org/W4309216591",
    "https://openalex.org/W4307008434",
    "https://openalex.org/W4385573871",
    "https://openalex.org/W4385573125",
    "https://openalex.org/W2059156680",
    "https://openalex.org/W2947509090",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3153386185",
    "https://openalex.org/W2963349408",
    "https://openalex.org/W3177006474",
    "https://openalex.org/W2015089823",
    "https://openalex.org/W4255326681",
    "https://openalex.org/W2903188467",
    "https://openalex.org/W4385573172",
    "https://openalex.org/W3135265359",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W2963453233",
    "https://openalex.org/W3167366812",
    "https://openalex.org/W4385572864",
    "https://openalex.org/W2970785793",
    "https://openalex.org/W2543728926",
    "https://openalex.org/W4308023829",
    "https://openalex.org/W2130324521",
    "https://openalex.org/W3114950584",
    "https://openalex.org/W3105111366",
    "https://openalex.org/W4384211370",
    "https://openalex.org/W148039595",
    "https://openalex.org/W1901148298",
    "https://openalex.org/W2964175037",
    "https://openalex.org/W3117890995",
    "https://openalex.org/W3099204253",
    "https://openalex.org/W3117527017",
    "https://openalex.org/W2981828710",
    "https://openalex.org/W2777609317",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4385573855",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4385573103"
  ],
  "abstract": "We consider the end-to-end abstract-to-title generation problem, exploring seven recent transformer based models (including ChatGPT) fine-tuned on more than 30k abstract-title pairs from NLP and machine learning (ML) venues. As an extension, we also consider the harder problem of generating humorous paper titles. For the latter, we compile the first large-scale humor annotated dataset for scientific papers in the NLP/ML domains, comprising 2.6k titles. We evaluate all models using human and automatic metrics. Our human evaluation suggests that our best end-to-end system per-forms similarly to human authors (but arguably slightly worse). Generating funny titles is more difficult, however, and our automatic systems clearly underperform relative to humans and often learn dataset artefacts of humor. Finally, ChatGPT, without any fine-tuning, performs on the level of our best fine-tuned system.",
  "full_text": "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 62–84\nNovember 1, 2023 ©2023 Association for Computational Linguistics\nTransformers Go for the LOLs:\nGenerating (Humourous) Titles from Scientific Abstracts End-to-End\nYanran Chen, Steffen Eger\nNatural Language Learning Group (NLLG)\nUniversity of Mannheim, Germany\nyanran.chen@stud.tu-darmstadt.de\nsteffen.eger@uni-mannheim.de\nAbstract\nWe consider the end-to-end abstract-to-title\ngeneration problem, exploring seven recent\ntransformer based models (including ChatGPT)\nfine-tuned on more than 30k abstract-title pairs\nfrom NLP and machine learning (ML) venues.\nAs an extension, we also consider the harder\nproblem of generating humorous paper titles.\nFor the latter, we compile the first large-scale\nhumor annotated dataset for scientific papers\nin the NLP/ML domains, comprising ∼2.6k\ntitles. We evaluate all models using human\nand automatic metrics. Our human evaluation\nsuggests that our best end-to-end system per-\nforms similarly to human authors (but arguably\nslightly worse). Generating funny titles is more\ndifficult, however, and our automatic systems\nclearly underperform relative to humans and\noften learn dataset artefacts of humor. Finally,\nChatGPT, without any fine-tuning, performs on\nthe level of our best fine-tuned system.1\n1 Introduction\nComputer-assisted writing is an important and long-\nstanding use case of NLP and natural language\ngeneration (NLG) (Burns, 1979), e.g., via and be-\nyond tools such as spell checkers or grammatical\nerror correction. The recent success of large-scale\nlanguage models (LLMs), such as the GPT gen-\neration of NLG models, has made the goal even\nmore realistic and promises full-scale automatic\ntext generation, without any human intervention.\nIn this work, we concern ourselves with auto-\nmatic text generation in the scientific domain. Sam-\nple scenarios in this general context involve (semi-\n)automatically generating reviews for scientific pa-\npers (Yuan et al., 2022), e.g., as a response to high\nreviewing load in the face of exploding submission\n1Our paper title is a (modified) merge of a funny and\nunfunny title suggested by ChatGPT (chat.openai.com).\nOur paper logo is drawn by DALL-E (https://openai.\ncom/dall-e-2/).\nData+code: https://github.com/cyr19/A2T\nnumbers; and generating captions for tables that re-\nquire reasoning capabilities (Moosavi et al., 2021).\nOur goal is much more modest: we ask whether\nlanguage models can generate adequate titles given\na human authored abstract as input; we refer to\nthis task as A2T (abstract-to-title generation). Title\ngeneration is important as titles are the first access\npoints to papers; a good title may attract more read-\ners and consequently increase paper impact, e.g.,\nin terms of citation numbers (Falagas et al., 2013).\nBesides generating titles per-se, we also aim for\ngenerating humorous titles, an inherently difficult\nproblem due to small sample size and the vague-\nness of humor. Generating funny titles may be\nrelevant as a funny title may attract more readers:\nfor example, Heard et al. (2022) find that funny\ntitles have significantly higher citation rates.\nWe approach the problem as a standard sequence-\nto-sequence text generation problem, where we\nfine-tune LLMs on more than 30k abstract-title\npairs from ML and NLP. Our contributions:\n• (i) We provide the first publicly available humor\nannotated dataset for scientific titles in the NLP\nand ML domain, with 2,638 humor annotated\ntitles annotated by 2 annotators with decent levels\nof agreement (kappa ∼0.65).\n• (ii) We explore 6 recent popular text generation\nsystems on the A2T task, finding one to be com-\npetitive to human titles, according to automatic\nand human evaluation involving 15 annotators.\n• (iii) We analyze the problem and find that the\nA2T task is to some degree ill-posed as a good\ntitle may leverage more than the abstract alone\n(we argue that the problem framing is still a le-\ngitimate and efficient approximation).\n• (iv) For humor generation, we find that our mod-\nels clearly underperform relative to humans and\ninstead often learn dataset artefacts.\n• (v) We finally analyze ChatGPT on a small scale\nand find that it may be competitive to (albeit\n62\nslightly weaker than) our best fine-tuned model\nwithout any task-specific fine-tuning at all.\n2 Related Work\nTitle generation and evaluation Mishra et al.\n(2021) perform A2T with pre-trained GPT-2 fine-\ntuned on arxiv papers and subsequent (rule-based)\nmodules of title selection and refinement. We com-\npare many more text generation models for the task,\nuse better evaluation (including more comprehen-\nsive human and automatic evaluation), do not make\nuse of rule-based selection and also consider humor\nin title generation. Putra and Khodra (2017) clas-\nsify sentences from paper abstracts into rhetorical\ncategories, retain those relating to methods and re-\nsults and then generate titles using templates. They\nfurther note the relationship between the task of\nsummarization (Nenkova et al., 2011) and A2T, as a\ntitle can be seen as a summary of the research paper.\nWe also leverage the relationship to summarization\nby considering pre-trained models fine-tuned on\nsummarization datasets. In contrast to Putra and\nKhodra (2017) and Mishra et al. (2021), we only\nconsider end-to-end models that do not involve\npipelines. While refinement steps could be further\nhelpful (but also error-prone), they additionally re-\nquire potentially undesirable human intervention\n(Belouadi and Eger, 2023). Related to the task of\ntitle generation is the task of headline generation\ne.g. for news. Tan et al. (2017) use a coarse-to-fine\napproach which first identifies important sentences\nand then converts them into a headline. In this way,\nthe model is not confused by ‘too much’ irrelevant\ninformation. In A2T, the first summarization step\nmay not be necessary, as the abstract is already a\nsummary of the scientific paper.\nHow titles should be (and are) structured has\nbeen researched for a long time, e.g., (Lewison and\nHartley, 2005). Hartley (2008) gives a typology\nof title types, distinguishing 13 title classes, e.g.,\nthose that state results vs. methods.\nBeyond title generation, related fields of text\ngeneration for science are related work generation\n(Li et al., 2022), more general automatic paper\nsection writing assistance (Wang et al., 2019b),\nand automatically generating reviews for scientific\narticles (Yuan et al., 2022). More broadly relating\nto science, Meta has in 2022 released an LLM\nfor the scientific domain called Galactica (Taylor\net al., 2022), but they mostly explore it for scientific\nclassification tasks rather than generation.\nHumor identification and generation Humor\ndetection is a niche area in NLP but nonethe-\nless with a rich history. For example, Mihalcea\nand Strapparava (2006) distinguish funny from\nnon-funny sentences (heuristically scraped from\nthe Web) using features and traditional classifiers.\nSimpson et al. (2019) focus on efficiently anno-\ntating humor and inducing classifiers from crowd-\nsourced data. Recently, Peyrard et al. (2021) show\nthat transformers are strong at distinguishing funny\nfrom non-funny sentences on minimal pairs of satir-\nical news headlines. In the scientific domain, Heard\net al. (2022) annotate a dataset of more than 2k ti-\ntles from ecology using a fine-grained Likert scale.\nThe majority were labeled as non-funny and an-\nnotators exhibited low agreements. Shani et al.\n(2021) classify scientific titles as funny or not us-\ning humor-theory inspired features and scientific\nlanguage models such as SciBERT (Beltagy et al.,\n2019) building on a dataset of Ig Nobel winners\nand humorous papers discussed in online forums.\nThere is considerably less work on humor gen-\neration. As one exception, He et al. (2019) gener-\nate puns by a retrieve-and-edit approach based on\nword2vec, thus circumventing the problem of little\ntraining data for puns.\n3 Data\nWe use the dataset released by Beese et al. (2023),\nwhich contains title-abstract pairs and correspond-\ning meta-information such as the publication year\nand venue. Beese et al. (2023) extracted the data\nfrom two sources: ACL Anthology (from 1984\nto 2021) and machine learning conferences (from\n1989 to 2021); we refer to the datasets from these\ntwo sources as NLP and ML, respectively. After fil-\ntering (described in Appendix A), 32,952 abstract-\ntitle pairs remain in our dataset.\n4 Title Generation\nWe first explore whether existing state-of-the-art\nSeq2Seq models manage to generate human-level\ntitles from abstracts. Hence, we do not include\nhumor constraints. We use an 8:2 ratio to divide\nthe data into train and test sets, and randomly select\n1,000 instances from the train set for the dev set.\n4.1 Models\nWe experiment with the following six generation\nmodels: (i) BART base (BARTbase) (Lewis et al.,\n2020), (ii) GPT2 ( GPT2) (Radford et al., 2019),\n63\n(iii) T5 small (Raffel et al., 2020) ( T5), and (iv)\nPEGASUS large (Zhang et al., 2019) finetuned on\nExtreme Summarization (XSUM) dataset (Narayan\net al., 2018) (PEGASUSxsum). Noting the similarity\nbetween text summarization and our A2T genera-\ntion task, we additionally inspect two BART large\nmodels finetuned on (v) XSUM ( BARTxsum) and\n(vi) CNN dailymail (CNNDM) (See et al., 2017)\n(BARTcnn), respectively. XSUM and CNNDM\ncontain document-summary pairs, where XSUM\nhas one-sentence summaries, while each summary\nin CNNDM consists of multiple sentences.\nFine-tuning For all baseline models, we continue\nfine-tuning them on the abstract-title pairs from our\ndataset. Details are in Appendix B.\n4.2 Evaluation\nWe assess the performance of the systems on 230\nabstracts using both automatic evaluation met-\nrics and human evaluation. We also include the\nhuman-generated titles in the evaluation, denoted\nas ‘HUMAN’. While our test set is small, we note\nthat (i) human evaluation is very time-consuming\nand (ii) we have more source-output pairs (i.e.,\n230×6, see below) than in some standard MT\nor summarization evaluation benchmarks such as\nWMT15-17 or SummEval (Fabbri et al., 2020).\nAutomatic Evaluation: As there are no A2T\ntask-specific evaluation metrics, we use the fol-\nlowing metrics from other NLG tasks: Rouge (Lin,\n2004), BERTScore (Zhang et al., 2020), Mover-\nScore (Zhao et al., 2019), COMET (Rei et al.,\n2020), BARTScore (Yuan et al., 2021), MENLI\n(Chen and Eger, 2022). COMET is a metric super-\nvised on human scores from MT, all others are unsu-\npervised. We employ all metrics in both reference-\nbased and -free settings. Reference-based, the met-\nrics compare the system titles with the original\nhuman-generated titles, while reference-free, the\nsystem titles are directly compared to the abstracts.\nThe details of the metric variants can be found\nin Appendix C. The reference-free setup is more\nconsistent with our human evaluation below and\noverall more plausible for A2T.\nHuman Evaluation: The human evaluation is\nconducted reference-free: 15 annotators 2 were\nasked to select two best and two worst titles\n2Most annotators are Master students, with an additional\nsenior researcher and two Bachelor students.\namong six titles from different systems (includ-\ning HUMAN), given the abstract. In order to make\nthe annotation simpler for humans, we only consid-\nered one dimension of annotation, namely, ‘over-\nall quality’, which may comprise aspects such as\nfluency, (grammatical) correctness, adequacy, etc.\nThis mimics coarse-grained annotations such as di-\nrect assessment (DA) in fields like MT. We did not\nfurther subdivide the quality into more fine-grained\nsubcategories, as the annotation is already difficult\nand comprises to understand a scientific abstract\nand to decide which title best fits it. Each instance\n(an abstract and its six titles) was evaluated by\nat least two annotators; depending on availability,\nsome instances were annotated by up to five anno-\ntators. The average percentage agreement over all\nannotator pairs is ∼50%, implying that each two\nannotators agree on one selection among the two\nselected best/worst titles, on average.\nThen, we use best-worst scaling ( BWS) (Lou-\nviere and Woodworth, 1991) to obtain the final\nhuman score for each title as:\nBWS = Nbest −Nworst\nNannotators\n(1)\nwhere Nbest/worst refers to the number of times that\nthe title was selected as one of the best/worst two\ntitles and Nannotators indicates the number of an-\nnotators responsible for that instance.\nsystem BWS MoverS BERTS BARTS COMET MENLI ROUGE\nBARTxsum 0.197-0.0250.889-2.583 0.060-0.214 0.033PEGASUSxsum0.022 -0.036 0.887 -2.819 0.060 -0.263 0.035BARTbase 0.015 -0.034 0.887 -2.709 0.059 -0.226 0.035GPT2 -0.013 -0.087 0.881 -3.090 0.060 -0.285 0.020T5 -0.039 -0.055 0.889 -2.735 0.057 -0.265 0.032BARTcnn -0.3840.0460.880 -2.982 0.047-0.1590.055\nHUMAN 0.181 -0.062 0.873 -3.5080.061 -0.029 0.029\nTable 1: Ref-free evaluation results of the baseline mod-\nels. We underlie the best performance among all gen-\neration systems including human. We bold the best\nperformance among all automatic generation systems\nexcluding human.\nResults We present the reference-based evalua-\ntion results in Appendix D. Among the six systems,\nBARTxsum is best , being selected by 4 out of 6\nevaluation metrics, followed by BARTcnn.\nTable 1 shows the reference-free evaluation re-\nsults. Unlike in reference-based evaluation, only\ntwo evaluation metrics (COMET and MENLI) se-\nlect HUMAN as the best system. BARTxsum is still\nthe best among the six automatic systems, obtain-\ning best results on 4 out of 7 evaluation metrics (in-\ncluding BWS). Surprisingly, it outperformsHUMAN\n64\nFigure 1: Distribution of generation systems of the titles\nselected as the BEST/WORST ones in human evaluation;\npercentages indicate the proportion of the generation\nsystems being selected over all selections.\neven in the human evaluation (0.197 vs. 0.181\nBWS). Nevertheless, as Figure 1(a) shows, HUMAN\nwas still most frequently selected as among the\ntwo best titles (23.2%) among all generation sys-\ntems, whereas the best neural generation system\nBARTxsum was selected in 16.9% of the cases as\none of the best two titles. However, Figure 1(b)\nshows that HUMAN was also more often selected\nas among the two worst titles (14.1% vs. 9.3%\nBARTxsum), explaining why BARTxsum is better\nthan HUMAN in human evaluation. Introspection\nshows that this is mostly due to words in the title\nwhich do not appear in the abstract. As a con-\nsequence, human annotators may believe that the\nmodel is hallucinating. Overall, we thus believe\nthat there is a (slight) mismatch in our task defini-\ntion: human authors may leverage the whole paper\nwhen designing their titles, not only the abstracts.\nHowever, paper2title generation would not only be\na challenge for the text generation models (which\nare often limited in text length) but also for the hu-\nman annotation process. We argue that framing the\nproblem as abstract2title generation is a simplifica-\ntion with overall good tradeoffs between problem\ncomplexity and model and annotator capacity.\nWhy is the best model best? To get a deeper\ninsight into the quality of the system titles, we\nfirst analyze their lengths. BARTcnn produces ti-\ntles much longer than human titles (14.95 vs. 8.27\ntokens) and other systems (6.68-9.13 tokens), on\naverage; besides, its titles are often truncated due to\nthe maximal output length set to the model. This re-\nflects the mismatch of the training data—BARTcnn\nwas first trained on CNNDM which has multiple\nsentences as a summary. Among the other systems,\nBARTxsum and BARTbase generate titles having\nthe largest overlap with the abstracts, based on\nthe edit distance. While BARTxsum (best/worst:\n230 instances 35 instances\nref-based ref-free ref-free\nρ r ρ r ρ r\nROUGE 0.571 0.395 -0.250 -0.722-0.121±0.11 -0.404±0.26\nBARTS 0.393 0.389 0.214 -0.0440.200±0.30 0.083±0.21\nBERTS 0.571 0.442 0.250 0.0790.236±0.26 0.296±0.22\nMoverS 0.929 0.575 -0.071 -0.677-0.129±0.13 -0.378±0.24\nMENLI 0.357 0.345 0.321 0.1390.057±0.15 0.160±0.21\nCOMET 0.964 0.580 0.929 0.9290.414±0.32 0.679±0.15\nA2TMetric- - - - 0.707±0.17 0.726±0.16\nTable 2: Pearson’s r and Spearman’s ρof evaluation\nmetrics with system-level human judgements for all230\ninstances (1380 titles; left block) and 35 instances (210\ntitles; right block). The correlations on the 35 instances\nare averaged over the test sets from five splits. We bold\nthe highest correlation in each block.\n241/133) does not have a huge advantage over\nBARTbase (best/worst: 165/159), inspection of re-\nsults indicates that BARTxsum may give more pre-\ncise and relevant titles, e.g., it picks out the key\ninformation from the abstracts more frequently;\nsome examples are in Appendix E. This may be\nagain due to its (extreme) summarization objective\nin the pre-training phase.\n4.3 Reliability of Evaluation Metrics\nTo inspect the reliability of the used metrics, we cal-\nculate Spearman/Pearson correlation with system-\nlevel human judgments, i.e., average BWS per\nsystem, on the 1380 titles (230 instances ×6 ti-\ntles). From Table 2 (left block), we observe: (1)\nmost metrics perform better in the ref-based setup\nthan ref-free, except for COMET. (2) Only ref-free\nCOMET correlates well with human judgments\nfrom the perspective of both types of correlation.\nEven though COMET performs well on system-\nlevel, this only indicates that COMET ranks sys-\ntems similarly as humans. COMET is not neces-\nsarily good at selecting the best title among dif-\nferent choices (segment-level evaluation). Indeed,\nat segment-level, it correlates weakly with human\nscores (0.127 Kendall).3 Inspired by this, we train\na ref-free metric supervised on our own human\nscores.\n4.4 A2TMetric\nWe develop the firstsupervised A2T generation-\nspecific evaluation metric, using the human judg-\nments collected in the evaluation for the 230 in-\nstances. Since HUMAN as a generation system is\n3As we convert BWS to WMT relative ranking judgements\n(Ma et al., 2018), we use the Kendall-like formulation intro-\nduced there for segment-level correlation.\n65\nincluded in the evaluation, and the metrics will later\nbe used to evaluate system-generated humorous ti-\ntles, which may vastly differ from the original ones,\nwe argue that a ref-free metric will better suit our\nneeds.\nDataset We split the data of 230 instances to\ntrain (170 instances), dev (25 instances), and test\n(35 instances) set. To get more robust results, we\ngenerate five different splits of train, dev and test\nset and report the average performance of the met-\nrics on the test set over the five splits in Table 2.\nWe note that many titles receive a BWS of 0 when\nthe number of annotators is small (because they\nwere never selected as the best or worst two titles),\nwhich may be problematic when aiming to directly\ntrain a regression model. Besides, the human evalu-\nation was similar to the ranking process. Therefore,\nwe convert BWS in the train and dev set to relative-\nranking judgments (Ma et al., 2018). That is, if\ntwo titles for one abstract obtain different BWS,\nthis title pair is considered as one relative-ranking\njudgement. Each instance then contains one ab-\nstract, a “better” title, a “worse” title, and the score\ndifference between the two titles in addition.\nFramework We adopt a framework similar to\nthe ranking-based variant of COMET to train the\nA2T metrics but in a ref-free setup. During train-\ning, the model optimizes the embedding space so\nthat (1) the sentence embedding of the abstract\n(a) is closer to that of the “better” title ( t+) than\nto that of the “worse” title (t−) (using the Triplet\nMargin loss (Schroff et al., 2015)) and (2) the dif-\nference between d(a,t+) and d(a,t−) is close to\nthe difference in BWS human scores for the two\ntitles (using the MSE loss), where d(u,v) refers\nto the Euclidean distance between uand v. Dur-\ning predicting, the metrics calculate the Euclidean\ndistance between the sentence embeddings of the\nabstract and the title.\nEvaluation As Table 2 (right block) shows, our\nA2TMetric achieves the highest values of both av-\nerage Spearman and Pearson correlations (above\n0.71-0.73 vs. -0.40-0.68) and relatively low stan-\ndard deviation (around 0.16 vs. 0.11-0.32), im-\nplying that it is not only superior to the existing\nmetrics but also demonstrates comparably good\nrobustness.\nWhile the metric is still not of absolutely high\nquality segment level (0.276 Kendall), it clearly\noutperforms COMET and the other metrics (right\nhalf of Table 11 in the appendix) and the correla-\ntion values are on the same level as those of the best\nMT metrics in WMT22 shared Task (Freitag et al.,\n2022). System-level, we evaluate A2TMetric on\n5 random samples of size 35 where the remainder\ninstances are for train/dev. While there is a high\nvariance due to small sample size, A2TMetric is on\naverage 0.1-0.3 Pearson/Spearman better system-\nlevel than COMET (right block of Table 2). Even\nthough comparing the trained A2TMetric to un-\nsupervised metrics may seem unfair, this is ex-\nactly the key point: A2TMetric is better because it\nhas been trained on our costly human data, which\nmakes it valuable.\nCOMET is still the best among the existing\nmetrics. Therefore, we only leverage our trained\nA2TMetric and COMET to automatically evaluate\nthe A2T systems’ quality in §5.1.\n5 Humorous Title Generation\nTo generate humorous titles, we first need a dataset\nof humor annotated titles in our domain (NLP and\nML papers). We cannot resort to the data of Shani\net al. (2021); Heard et al. (2022) as those leverage\npapers from other scientific fields. As a conse-\nquence, we build our own dataset. When construct-\ning the dataset, we ask annotators to rely on their\nintuition of humor rather than issuing guidelines of\nwhat they should find funny. This can be justified\nas humor is often subjective and culture- and even\ngender-specific (Dore, 2019; Mundorf et al., 1988).\nThere is also a multitude of theories around humor,\nindicating the ambiguity of the concept.4\nHumor Annotation + Classification We train\nhumor classifiers on human annotated data to auto-\nmatically label titles as FUNNY, FUNNYmed, and\n¬FUNNY (examples see Table 12 the appendix).\nTwo co-authors participated in the annotation. Ex-\namples of their annotations are shown in Appendix\nF. Titles annotated as funny by both annotators\nallude to famous proverbs or book/movie titles\n(“Taming the wild”), make use of linguistic devices\nsuch as alliteration (“Balancing Between Bagging\nand Bumping”) or leverage surprise (“Is the Best\nBetter? [...]”; “What’s in a name? In some lan-\nguages, grammatical gender”). Medium funny ti-\ntles often make use of playful/clever abbreviations,\n4The wikipedia page for humor https://en.\nwikipedia.org/wiki/Theories_of_humor\nlists at least three modern popular theories of humor, based on\nrelief, superiority and incongruity.\n66\nIndividuals Ensemble\nStage 1 52.2 / 81.5 54.1 / 85.1\nStage 2 55.1 / 84.7 57.7 / 88.1\nTable 3: Average macro F1 over the 11 individual classi-\nfiers and macro F1 of the ensemble classifiers from both\nstages on the held-out test set (where the two annotators\nobtain 0.649 kappa agreement). Performance on both\nthree-way (first entry) and binary (second entry) clas-\nsification tasks; for binary classification, FUNNY and\nFUNNYmed are merged. We bold the highest macro F1\non each classification task.\ne.g., “CPR: Classifier-Projection Regularization\nfor Continual Learning”.\nStage 1 : The two annotators initially anno-\ntated 1,730 titles: 1,603 titles as ¬FUNNY, 106\nas FUNNYmed, and 21 as FUNNY (kappa 0.65 on\n300 common instances). To combat this severe data\nimbalance, we resort to ensembling with each clas-\nsifier trained on more balanced splits: we randomly\ngenerate 11 different data splits, where the train set\nof each split consists of 100 funny or medium funny\ntitles and 200 not funny titles (all randomly drawn).\nOn those splits, we train 11 classifiers to construct\nan ensemble classifier. To evaluate the classifier\nperformance, the two annotators annotated another\n315 titles jointly, obtaining 0.639 Kappa. Our best\nensemble classifier leverages the sum of the label\nvalues assigned by the 11 individual classifiers to\npredict humorousness, yielding 4.8% macro F1 im-\nprovement compared to the individual classifiers\n(62.4% vs. 57.6%). Details are in Appendix G.\nStage 2: To find more funny title candidates to\nannotate, the two annotators annotated the funniest\n396 titles in the original dataset from Beese et al.\n(2023), predicted by the Stage 1 ensemble classi-\nfier; 75.8% (300 titles) were judged as FUNNY or\nFUNNYmed, which is substantially higher than the\nproportion of funny titles in the annotated data of\nStage 1 (7.3%). Thus, the annotated data expands\nto 2,441 titles (= 1,730 + 315 + 396), where 1,893\nare labeled as ¬FUNNY, 492 as FUNNYmed and\n56 as FUNNY. Subsequently, we re-train 11 clas-\nsifiers on newly generated 11 data splits from the\nexpanded data of 2,441 titles; now the train set of\neach split has 400 (medium) funny titles and 800\nnot funny titles. As before, we ensemble the 11\nclassifiers as in Stage 1.\nWe test the classifiers from both stages on a held-\nout test set containing 197 titles annotated by the\ntwo annotators (0.649 kappa). The macro F1 scores\nof those classifiers are presented in Table 3. As\nFUNNY titles are rare in the whole dataset, we also\nevaluate the classifiers on the corresponding binary\nclassification task, where FUNNY and FUNNYmed\nare merged. We observe that: (1) ensemble clas-\nsifier performs better than the individual ones. (2)\nClassifiers from Stage 2 are superior to the ones\nfrom Stage 1, indicating larger size of the training\ndata is beneficial. (3) The best three-way classifier\nachieves only ∼58% macro F1, but ∼88% macro\nF1 on the binary classification. Besides, we see a\nconsistent improvement of human annotation qual-\nity: the two annotators achieve 0.01-0.1 higher\nKappa when their annotations are down-scaled to\nbinary (see Table 17 in Appendix G). Thus, we\nuse the ensemble classifier from Stage 2 as the\nhumor classifier in further experiments.\nFinal Dataset We use our humor classifier to au-\ntomatically label the rest of the data. Considering\nthe difficulty of three-way classification for both\nhumans and classifiers, we only consider two hu-\nmor levels in further experiments: (1) FUNNY (for\nfunny and medium funny titles) and (2) ¬FUNNY\n(for not funny titles). Thus, we collect 31,541 in-\nstances ( >95%) with ¬FUNNY and 1,411 with\nFUNNY titles. We split the resulting data to train,\ndev, and test sets, ensuring that (1) the data with\nhuman-annotated titles remains in the train set, as\nthe humor classifier trained and evaluated on it will\nbe used as an automatic humor evaluator; (2) 80%\nof the data in dev/test is from NLP and 20% from\nML because our annotators are more knowledgable\nfor NLP papers, and (3) the ratio ofFUNNY data to\n¬FUNNY data in dev/test set is 1:2.5 As FUNNY\ndata is only a small portion of the whole data, we\nonly keep 600 instances in the dev/test sets, the\nremaining data serves as the train data. Appendix\nH summarizes the statistics of the final dataset.\nGeneration In the second phase of the experi-\nments, we use the optimal model identified previ-\nously, i.e., BARTxsum, to generate titles with con-\nstraints on humor level. The input of the gen-\neration systems is formulated as “ humor level\n[SEP] abstract ”, where humor level is either 0\n(for ¬FUNNY) or 1 (for FUNNY).\n5This aims to more easily compare the system-generated\nfunny titles with the human-generated ones and does not relate\nto controlling the quality of titles in the test set.\n67\nFine-tuning We fine-tune generation systems\nhere as in §4.1 (hyperparameters see Appendix\nI): (1) we fine-tune a BARTxsum on the abstract-\ntitle pairs in the train set with humor constraints.\n(2) We continue fine-tuning the model from (1) on\nself-generated pseudo data.6\nThe motivation of (2) is that we observe that the\nsystems tend to ignore the humor constraints in the\ninput and generate identical titles for different con-\nstraints in initial experiments. We assume that to\nexpose systems to titles with different humor levels\nfor the same abstract during training can encourage\nthem to pay more attention to the humor constraints.\nTo obtain the pseudo data, we: (i) generate titles\nfor abstracts in the train set but with “opposite”\nhumor constraints compared to the original titles,\nkeeping only those pseudo titles with the correct\nhumor labels assigned by the humor classifier; (ii)\nfilter out FUNNY labeled titles with very frequent\nn-grams, in order to encourage more diverse titles.\nWe finally merge the filtered pseudo data with the\noriginal data. Thus, in the training data of (2),\neach abstract has two titles, one with label FUNNY\nand the other with ¬FUNNY; it contains 15,474\ninstances in total, where 50% are pseudo ones.\n5.1 Evaluation\nWe report results on generating both funny and\nnot-funny titles, to explore the difference in mod-\nels’ performance after involving humor generation,\nbased on both automatic and human evaluation.\nAutomatic Evaluation Based on the results for\nthe automatic evaluation metrics in §4.3, we\nonly leverage COMET and our supervised met-\nric A2TMetric here to evaluate title quality. To\nevaluate humor, we use the following three metrics:\n(1) F1macro between the expected humor labels and\nthose assigned by the humor classifier. (2) System\naccuracy of generating titles on correct humor lev-\nels, denoted as ACCFUNNY and ACC¬FUNNY. (3)\nThe ratio of the cases that the systems generate\nthe same titles for both humor constraints to all\ngeneration cases (RatioSAME); lower is better.\nWe generate titles with constraint on both humor\nlevels for all abstracts in the test set, computing the\nautomatic evaluation on 1200 titles in total.\nResults We evaluate humor before and after\ntraining on pseudo data in Appendix J, Table 19:\n6Synthetic data can be a useful resource (He et al., 2021),\ndespite potential limitations (Shumailov et al., 2023).\nMetric COMET A2TMetric\nhumor constraint¬FUNNY FUNNY¬FUNNY FUNNY\nBARTxsum 0.0598 0.0582 -2.30 -2.32\nBARTxsum+pseudo0.0593 0.0541 -2.31 -2.37\nHUMAN 0.0586 -2.36\nTable 4: Automatic evaluation for titles’ quality. We\nbold the best performance assessed by each metric. “Hu-\nmor constraint” refers to the constraints given to the\ninput of the generation systems.\n(1) after continued training on the pseudo data,\nBARTxsum+pseudo achieves substantially higher\nF1macro (from 0.647 to 0.856) and ACCFUNNY (from\n40.2% to 77.8%), and slightly better Ratio SAME\n(from 6.5% to 4.7%). (2) ACC ¬FUNNY drops\nslightly compared to BARTxsum (94.5% vs. 93.6%),\nindicating that both systems have high accuracy on\ngenerating ¬FUNNY titles and the fine-tuning on\npseudo data only improves the system’s accuracy\nto generate FUNNY titles.\nWe then present the quality evaluation results in\nTable 4. Both BART systems obtain better results\nthan HUMAN on both evaluation metrics, which is in\nline with the observation in §4.2, especially when\ngenerating ¬FUNNY titles. However, we observe\na consistent performance drop after training on the\npseudo data (values in the first row vs. those in the\nsecond row). Further, we also note that the system\ngenerated ¬FUNNY titles have better quality than\nthe FUNNY ones (values in the left column vs.\nthose in the right column).\nHuman Evaluation We randomly sample 100\nabstracts from the test set with controls on the\nsource of the papers (80% fromNLP and 20% from\nML) and on the humor label of the original titles\n(50% FUNNY and 50% ¬FUNNY). For each ab-\nstract with a human funny title, we generate a funny\nand a non-funny system title, and accordingly for\neach non-funny human title. Thus, each evaluation\ninstance contains one abstract and five titles: 1 orig-\ninal title + 4 system titles (2 generation systems ×\n2 humor levels). The annotators rank the five titles\non two criteria: general quality and humor degree,\nbased on the abstract; the annotators can assign\nidentical ranks to multiple titles. We show a screen-\nshot of an annotation instance and the annotation\nguidelines in Figure 2 in the appendix. Five annota-\ntors (three PhD students, one undergraduate student\nand one senior researcher) jointly annotate 10 from\nthese 100 instances, obtaining 0.782 Spearman for\nhumor and 0.325 for quality ranking on average per\n68\nhumor constraint/label FUNNY ¬FUNNY\nsystem humor quality humor quality\nBARTxsum 1.94 2.70 2.76 2.10\nBARTxsum+pseudo 1.58 2.97 2.75 2.56\nHUMAN 1.51 2.86 2.40 2.63\nTable 5: Average rank of the system titles for the\nabstracts with original titles labeled as FUNNY and\n¬FUNNY separately in the human evaluation of general\nquality and humor degree; smaller values denotes higher\nranks. “Humor constraint/label” refers to the constraints\ngiven to the input of the generation systems and the\nhumor labels of the original titles.\nannotator pair. Then, they separately evaluate the\nremaining 90 instances. Note that since in our eval-\nuation annotators rank titles, even the first ranked\ntitle does not necessarily have to be of high quality\nor funny, for any given abstract, if the remaining\nare very bad concerning quality/humor.\nResults Table 20 (appendix) compares the two\nBART systems across all 200 instances (one funny\nand one non-funny title per abstract). Similar to au-\ntomatic evaluation, we observe (1) a general quality\ndrop but a performance boost for humor generation\nafter training on pseudo data and (2) ¬FUNNY ti-\ntles have better quality than FUNNY ones.\nFurther, we compare the system titles with the\noriginal human titles in Table 5. BARTxsum ranks\nhigher than HUMAN concerning quality when gen-\nerating both FUNNY and ¬FUNNY titles (2.70 vs.\n2.86 and 2.10 vs. 2.63), which is consistent with\nour previous human evaluation (§4.2). However,\nfine-tuning on the pseudo data impacts the qual-\nity of the generated funny titles, as the system is\nrated worse than HUMAN only in this category (2.97\nvs. 2.86), which is also in line with our automatic\nevaluation from A2TMetric. HUMAN still generates\nfunnier titles than the automatic systems, ranking\nhighest among all systems (1.51 vs. 1.58-1.94).\n6 Comparison with ChatGPT\nWe compare our fine-tuned BARTxsum (without\ntraining on pseudo data) with the recent popular\nChatGPT model.7 Firstly, we use the two mod-\nels to generate funny and not funny titles for 100\nabstracts from the EMNLP 2022 handbook which\nChatGPT could not have seen in its training data.\n7Here, we used the ChatGPT interface (https://chat.\nopenai.com/) of the first three releases (Nov. 30, 2022—\nJan. 9, 2023); the official API was inaccessible back then.\nsystem humor rank quality rank\nBARTxsum 1.86 / 2.66 2.74 / 2.25\nChatGPT 1.41/ 3.12 3.62 / 2.30\nhuman 2.53 2.85\nTable 6: Average ranks of the generatedFUNNY titles\n(first entry) and ¬FUNNY titels (second entry) for 100\nabstracts from EMNLP 2022 handbook in the human\nevaluation of quality and humorousness; smaller values\ndenote higher ranks. We bold the highest ranks for each\ncriterion.\nOur prompt for ChatGPT is “I want a funny title\nand a not-funny title for the following abstract:\n[abstract]”. The ranking-based human evaluation\nconducted here is identical to §5.1 and done by\nthe same five annotators, who obtain 0.867 Spear-\nman for humor and 0.548 for quality evaluation on\naverage over annotator pairs this time.\nThe average rank per system with humor con-\nstraint is presented in Table 6. We observe that\nautomatic generation systems are mostly ranked\nhigher than HUMAN (2.25-2.74 vs. 2.85) except for\nChatGPT producing funny titles (3.62 vs. 2.85).\nChatGPT generates funnier but lower-quality ti-\ntles compared to BARTxsum but ChatGPT is almost\non par for non-funny titles. Hence, we conclude\nthat ChatGPT without any fine-tuning may already\nperform similarly to our fine-tuned BARTxsum.\nAfter our experiments, ChatGPT has been up-\ndated several times. To inspect whether the new\nversion performs better, we conduct a second exper-\niment using the latest model “gpt-3.5-turbo-0613”\nwith the official API, utilizing the default hyperpa-\nrameters. Details are given in Appendix L. Overall,\nour evaluation suggests that the newer ChatGPT\ndoes not perform better: In 25 out of 40 cases,\nthe previous titles were selected as the better ones.\nIn fact, the new version performs much worse for\ngenerating FUNNY titles: it loses to the previous\nversion on 18 out of 20 instances.\n7 Discussion & Analysis\nAre automatic titles really superior? Overall,\nour results in §5 and §6 seem to indicate that auto-\nmatically generated titles outperform human titles.\nHowever, looking at the distribution of best/worst\ntitles, we see again a high frequency of worst hu-\nman titles as annotated by our human annotators;\nin fact, human titles are most frequently selected\nas worst titles except when the automatic systems\n69\nuse the humor constraint. As before, the likely\nreason is a lower lexical overlap between human\ntitles and abstracts. Indeed, we find that human ti-\ntles have lower lexical overlap with abstracts when\ncompared to automatically generated titles from\nChatGPT and BARTxsum, e.g., 57-61% of con-\ntent words in human titles appear in the abstract,\nwhile the number is 64-67% for BARTxsum and\nChatGPT. Very negatively evaluated human titles\nhave even lower lexical overlap.\nIn contrast, human titles were again most fre-\nquently selected as best titles except when includ-\ning ChatGPT. Overall, our findings implicate that\nautomatically generated titles can be competitive\nbut are presumably still slightly worse than author\nchoices. To verify this hypothesis, we suggest a\nmore costly evaluation scheme in the form of a user\nstudy involving the authors of papers instead of\npaper external annotators in future studies.\nIs training on extra parts besides abstract bene-\nficial? We argued that human titles may not only\nbe based on abstracts, but (to some extent) the full\npapers. To inspect whether training title genera-\ntion systems on more than abstracts alone leads to\nbetter systems, we train BARTXsum and the popu-\nlar Longformer-Encoder-Decoder (LED) (Beltagy\net al., 2020), which can deal with longer input se-\nquences, in two settings: (1) only abstracts and (2)\nabstracts, introductions, and conclusions; we de-\nnote the corresponding models as “[MODEL]+A”\nand “[MODEL]+X”, respectively. We use the data\nfrom Hou et al. (2021), which contains the sen-\ntences of all papers from ACL Anthology until\n2019. Technical details are given in Appendix M.\nWe randomly select 29 instances from the test\nsets for human evaluation: 14 for BARTXsum\nand 15 for LED. Two evaluators were asked to\nselect the better one among the two titles generated\nby “[MODEL]+A” and “[MODEL]+X” with the\nsame underlying model, given the abstract, intro-\nduction and conclusion. On the jointly assessed 10\ninstances, they obtained 0.474 Kappa. Our evalua-\ntion results show that: BARTXsum seems to benefit\nfrom training on more parts (BARTXsum+X wins\n8 out of 14 instances); for LED, it is not the case\n(LED+A wins 11 out of 15 cases). On introspec-\ntion, we do find that the models trained on more\nthan abstracts can indeed leverage some relevant\nkeywords not in the abstracts, which makes their\ntitles sometimes better. On the other hand, they are\ntasked with identifying relevant titles given more\n‘background noise’ (longer texts) which causes\nthem to hallucinate more and be more vague. We\nshow examples in Appendix N. Evaluation with\nmore than abstracts alone is also considerably more\ncostly for humans. Overall, these experiments thus\nindicate that training (and evaluating) on highly\nspecific and condensed abstracts is advantageous.\nHumor constraints On introspection, we find\nthat the funny titles generated by ChatGPT do not\nconform to a style of humor used in scientific pa-\npers. This indicates that ChatGPT lacks fine-tuning\non humor in science. For BARTxsum, its problem\nseems to be that it overfits to data artefacts learned\nfrom the data indicating that it does not properly\nlearn a generalizable notion of humor. Addition-\nally, both models often do not match the content of\nthe abstract/title to the humor framing (examples\nsee Table 21 in the appendix). In our human evalu-\nation, such titles often obtain high humor but low\nquality ranks; however, when they are pertinent to\nthe abstracts, they have the potential to receive high\nquality ranks as well (cf. Appendix K).\n8 Conclusion\nWe considered the abstract-to-title generation prob-\nlem using end-to-end models. To do so, we trained\nsix recent text-to-text generation systems on more\nthan 30k NLP and ML papers. We evaluated the\nsystems using an array of state-of-the-art automatic\nmetrics as well as human evaluation. Our evalu-\nation indicates that some current text generation\nmodels can generate titles with similar quality as\nhumans, but human authors are apparently still\nsuperior. We also considered the humorous title\ngeneration problem as an extension, compiling the\nfirst dataset in the NLP/ML domain in this context,\ncomprising over 2.6k titles annotated by two anno-\ntators with acceptable agreement. We find that our\nsystems struggle with generating humorous titles\nand instead overfit to frequent patterns in the data,\nindicating much scope for future research.\n9 Limitations\nIn our work, we followed a standard protocol of\nevaluation of text generation involving (1) auto-\nmatic metrics comparing source texts (abstracts)\nor references and system outputs and (2) human\nannotators considering the same sources of infor-\nmation. We argued that this standard evaluation\nscheme may not be fully adequate in our situation\n70\nas the human authored titles may take additional\ninformation into account (e.g., the full texts), which\nis difficult to incorporate, however, for our annota-\ntors and for the metrics. This leads to an (arguably\nsmall) bias against human titles, which seems to\nbe automatically identifiable however via the dis-\ntribution of best/worst titles selected for different\nsystems. Overall, this limitation could better be\naddressed, however, by consulting the authors of\npapers for an additional but much more costly to\nrealize evaluation in the form of a user study.\nWe also experimented with NLP and ML papers\nonly, not taking other scientific fields into consid-\neration. Finally, prompting for ChatGPT is an art\nin itself; other prompts may have yielded different\nresults. To explore this, we used a slightly differ-\nent prompt (“Please give me a [funny] title for the\nfollowing scientific abstract: [abstract]”) for Chat-\nGPT on 20 instances, which led to very similar\nhuman evaluation results. It is conceivable, how-\never, that there might have been prompts leading to\nbetter evaluation outcomes for ChatGPT.\nA risk of our models is that they might produce\nmisleading or even factually wrong titles which\ncould be adopted by the human authors if not prop-\nerly checked.\nAs a consequence of our missing annotation\nguidelines for humor, it is possible that our annota-\ntors have not clearly separated humor from related\nconcepts such as ‘click-baiting’ (to the extent that\nsuch a separation is possible at all).\nAcknowledgments\nWe thank the BMBF for its support via the grant\nMetrics4NLG. The last author is supported by DFG\ngrant EG 375/5–1.\nReferences\nMarcin Andrychowicz, Misha Denil, Sergio Gomez,\nMatthew W Hoffman, David Pfau, Tom Schaul, Bren-\ndan Shillingford, and Nando De Freitas. 2016. Learn-\ning to learn by gradient descent by gradient descent.\nAdvances in neural information processing systems,\n29.\nTakashi Awamura, Eiji Aramaki, Daisuke Kawahara,\nTomohide Shibata, and Sadao Kurohashi. 2015. Lo-\ncation name disambiguation exploiting spatial prox-\nimity and temporal consistency. In SocialNLP\n2015@NAACL - 3rd International Workshop on Nat-\nural Language Processing for Social Media, Proceed-\nings of the Workshop, SocialNLP 2015@NAACL -\n3rd International Workshop on Natural Language Pro-\ncessing for Social Media, Proceedings of the Work-\nshop, pages 1–9. Association for Computational Lin-\nguistics (ACL). Publisher Copyright: © 2015 Asso-\nciation for Computational Linguistics; 3rd Workshop\non Natural Language Processing for Social Media,\nSocialNLP 2015, associated with NAACL 2015 ;\nConference date: 05-06-2015.\nSamaneh Azadi and Suvrit Sra. 2014. Towards an opti-\nmal stochastic alternating direction method of multi-\npliers. In Proceedings of the 31st International Con-\nference on Machine Learning, volume 32 of Proceed-\nings of Machine Learning Research, pages 620–628,\nBejing, China. PMLR.\nDavid Balduzzi, Marcus Frean, Lennox Leary, JP Lewis,\nKurt Wan-Duo Ma, and Brian McWilliams. 2017.\nThe shattered gradients problem: If resnets are the\nanswer, then what is the question?\nDominik Beese, Begüm Altunba¸ s, Görkem Güzeler,\nand Steffen Eger. 2023. Did ai get more negative\nrecently? Royal Society Open Science, 10.\nJonas Belouadi and Steffen Eger. 2023. Bygpt5: End-to-\nend style-conditioned poetry generation with token-\nfree language models. In ACL.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nHugh L. Burns. 1979. Stimulating rhetorical invention\nin english composition through computer-assisted\ninstruction.\nSantiago Castro, Devamanyu Hazarika, Verónica Pérez-\nRosas, Roger Zimmermann, Rada Mihalcea, and Sou-\njanya Poria. 2019. Towards multimodal sarcasm de-\ntection (an _Obviously_ perfect paper). In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4619–4629, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nSungmin Cha, Hsiang Hsu, Flávio P. Calmon, and\nTaesup Moon. 2020. Cpr: Classifier-projection\nregularization for continual learning. CoRR,\nabs/2006.07326.\nBranden Chan, Stefan Schweter, and Timo Möller. 2020.\nGerman’s next language model. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, pages 6788–6796, Barcelona, Spain (On-\nline). International Committee on Computational Lin-\nguistics.\n71\nYanran Chen and Steffen Eger. 2022. Menli: Robust\nevaluation metrics from natural language inference.\nArXiv, abs/2208.07316.\nKevin Clark, Minh-Thang Luong, Urvashi Khandel-\nwal, Christopher D. Manning, and Quoc V . Le. 2019.\nBAM! born-again multi-task networks for natural\nlanguage understanding. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5931–5937, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMatt Crane. 2018. Questionable answers in question\nanswering research: Reproducibility and variability\nof published results. Transactions of the Association\nfor Computational Linguistics, 6:241–252.\nMargherita Dore. 2019. Humour in Audiovisual Trans-\nlation: Theories and Applications. Routledge, New\nYork.\nDheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt\nGardner. 2022. Successive prompting for decom-\nposing complex questions. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1251–1265, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nPablo Duboue and Jennifer Chu-Carroll. 2006. An-\nswering the question you wish they had asked: The\nimpact of paraphrasing for question answering. In\nNorth American Chapter of the Association for Com-\nputational Linguistics.\nAleksandra Edwards, Jose Camacho-Collados, Hélène\nDe Ribaupierre, and Alun Preece. 2020. Go simple\nand pre-train on domain-specific corpora: On the role\nof training data for text classification. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 5522–5529, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nA. R. Fabbri, Wojciech Kryscinski, Bryan McCann,\nRichard Socher, and Dragomir R. Radev. 2020.\nSummeval: Re-evaluating summarization evaluation.\nTransactions of the Association for Computational\nLinguistics, 9:391–409.\nMatthew E Falagas, Angeliki Zarkali, Drosos E Kara-\ngeorgopoulos, Vangelis Bardakas, and Michael N\nMavros. 2013. The impact of article length on the\nnumber of future citations: a bibliometric analysis of\ngeneral medicine journals. PLoS One, 8(2):e49476.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,\nCraig Stewart, Eleftherios Avramidis, Tom Kocmi,\nGeorge Foster, Alon Lavie, and André F. T. Martins.\n2022. Results of WMT22 metrics shared task: Stop\nusing BLEU – neural metrics are better and more\nrobust. In Proceedings of the Seventh Conference\non Machine Translation (WMT), pages 46–68, Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nFumiyo Fukumoto and Yoshimi Suzuki. 2004. A com-\nparison of manual and automatic constructions of cat-\negory hierarchy for classifying large corpora. In Pro-\nceedings of the Eighth Conference on Computational\nNatural Language Learning (CoNLL-2004) at HLT-\nNAACL 2004, pages 65–72, Boston, Massachusetts,\nUSA. Association for Computational Linguistics.\nJonas Geiping, Liam Fowl, W. Ronny Huang, Wojciech\nCzaja, Gavin Taylor, Michael Moeller, and Tom Gold-\nstein. 2020. Witches’ brew: Industrial scale data\npoisoning via gradient matching.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 30–45, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nAjda Gokcen and Marie-Catherine de Marneffe. 2015.\nI do not disagree: leveraging monolingual alignment\nto detect disagreement in dialogue. In Proceedings of\nthe 53rd Annual Meeting of the Association for Com-\nputational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers), pages 94–99, Beijing, China.\nAssociation for Computational Linguistics.\nSreenivas Gollapudi, Kostas Kollias, and Debmalya Pan-\nigrahi. 2019. You get what you share: Incentives for\na sharing economy. Proceedings of the AAAI Confer-\nence on Artificial Intelligence, 33(01):2004–2011.\nTanya Goyal, Nazneen Rajani, Wenhao Liu, and Wo-\njciech Kryscinski. 2022. HydraSum: Disentan-\ngling style features in text summarization with multi-\ndecoder models. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 464–479, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nShai Gretz, Yonatan Bilu, Edo Cohen-Karlik, and Noam\nSlonim. 2020. The workweek is the best time to start\na family – a study of GPT-2 based claim generation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 528–544, Online.\nAssociation for Computational Linguistics.\nJames Hartley. 2008. Academic writing and publishing:\nA practical handbook. Routledge.\nHe He, Nanyun Peng, and Percy Liang. 2019. Pun\ngeneration with surprise. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 1734–1744, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nXuanli He, Islam Nassar, Jamie Ryan Kiros, Gholam-\nreza Haffari, and Mohammad Norouzi. 2021. Gen-\nerate, annotate, and learn: Nlp with synthetic text.\nTransactions of the Association for Computational\nLinguistics, 10:826–842.\n72\nStephen B. Heard, Chloe A. Cull, and Easton R. White.\n2022. If this title is funny, will you cite me? citation\nimpacts of humour and other features of article titles\nin ecology and evolution. bioRxiv.\nTom Heskes. 1996. Balancing between bagging and\nbumping. In Advances in Neural Information Pro-\ncessing Systems, volume 9. MIT Press.\nYufang Hou, Charles Jochim, Martin Gleize, Francesca\nBonin, and Debasis Ganguly. 2021. TDMSci: A spe-\ncialized corpus for scientific literature entity tagging\nof tasks datasets and metrics. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 707–714, Online. Association for Computa-\ntional Linguistics.\nZiniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang,\nZiyi Yang, Chenguang Zhu, Kai-Wei Chang, and\nYizhou Sun. 2022. Empowering language models\nwith knowledge graph reasoning for question answer-\ning.\nBaijun Ji, Tong Zhang, Yicheng Zou, Bojie Hu, and\nSi Shen. 2022. Increasing visual awareness in mul-\ntimodal neural machine translation from an infor-\nmation theoretic perspective. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing , pages 6755–6764, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nPei Ke, Haozhe Ji, Siyang Liu, Xiaoyan Zhu, and Min-\nlie Huang. 2020. SentiLARE: Sentiment-aware lan-\nguage representation learning with linguistic knowl-\nedge. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 6975–6988, Online. Association for\nComputational Linguistics.\nKalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh,\nNicolas Papernot, and Mohit Iyyer. 2019. Thieves on\nsesame street! model extraction of bert-based apis.\nArXiv, abs/1910.12366.\nJoel Lang and Mirella Lapata. 2010. Unsupervised\ninduction of semantic roles. In Human Language\nTechnologies: The 2010 Annual Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics, pages 939–947, Los Angeles,\nCalifornia. Association for Computational Linguis-\ntics.\nNyoungwoo Lee, ChaeHun Park, Ho-Jin Choi, and\nJaegul Choo. 2022. Pneg: Prompt-based negative\nresponse generation for dialogue response selection\ntask. In Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 10692–10703, Abu Dhabi, United Arab Emi-\nrates. Association for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nGrant Lewison and James Hartley. 2005. What’s in a\ntitle? numbers of words and the presence of colons.\nScientometrics, 63(2):341–356.\nPengcheng Li, Wei Lu, and Qikai Cheng. 2022. Gen-\nerating a related work section for scientific papers:\nan optimized approach with adopting problem and\nmethod information. Scientometrics, 127(8):4397–\n4417.\nSHAO LI. 2004. Integrating context and translitera-\ntion to mine new word translations from comparable\ncorpora.\nWanli Li and Tieyun Qian. 2022. Graph-based model\ngeneration for few-shot relation extraction. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 62–71,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nJunyang Lin, Xu Sun, Shuming Ma, and Qi Su. 2018.\nGlobal encoding for abstractive summarization. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 163–169, Melbourne, Australia.\nAssociation for Computational Linguistics.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nJordan J Louviere and George G Woodworth. 1991.\nBest-worst scaling: A model for the largest difference\njudgments. Technical report, Working paper.\nQingsong Ma, Ondˇrej Bojar, and Yvette Graham. 2018.\nResults of the WMT18 metrics shared task: Both\ncharacters and embeddings achieve good perfor-\nmance. In Proceedings of the Third Conference on\nMachine Translation: Shared Task Papers , pages\n671–688, Belgium, Brussels. Association for Com-\nputational Linguistics.\nRada Mihalcea and Carlo Strapparava. 2006. Learn-\ning to laugh (automatically): Computational models\nfor humor recognition. Computational Intelligence,\n22(2):126–142.\nPrakhar Mishra, Chaitali Diwan, Srinath Srinivasa, and\nG Srinivasaraghavan. 2021. Automatic title genera-\ntion for text with pre-trained transformer language\nmodel. In 2021 IEEE 15th International Conference\non Semantic Computing (ICSC), pages 17–24. IEEE.\n73\nNafise Sadat Moosavi, Andreas Rücklé, Dan Roth,\nand Iryna Gurevych. 2021. Scigen: a dataset for\nreasoning-aware text generation from scientific ta-\nbles. In Thirty-fifth Conference on Neural Informa-\ntion Processing Systems Datasets and Benchmarks\nTrack (Round 2).\nNorbert Mundorf, Azra Bhatia, Dolf Zillmann, Paul\nLester, and Susan Robertson. 1988. Gender differ-\nences in humor appreciation. Humor, 1(3):231–244.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\nTopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium.\nVivi Nastase and Marius Popescu. 2009. What’s in\na name? In some languages, grammatical gender.\nIn Proceedings of the 2009 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1368–1377, Singapore. Association for Computa-\ntional Linguistics.\nAni Nenkova, Kathleen McKeown, et al. 2011. Auto-\nmatic summarization. Foundations and Trends® in\nInformation Retrieval, 5(2–3):103–233.\nUlrike Padó. 2016. Get semantic with me! the use-\nfulness of different feature types for short-answer\ngrading. In Proceedings of COLING 2016, the 26th\nInternational Conference on Computational Linguis-\ntics: Technical Papers , pages 2186–2195, Osaka,\nJapan. The COLING 2016 Organizing Committee.\nAlexander Pak and Patrick Paroubek. 2010. Twitter\nbased system: Using twitter for disambiguating sen-\ntiment ambiguous adjectives. In Proceedings of the\n5th International Workshop on Semantic Evaluation,\nSemEval ’10, page 436–439, USA. Association for\nComputational Linguistics.\nGiorgio Patrini, Richard Nock, Paul Rivera, and Tiberio\nCaetano. 2014. (almost) no label no cry. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 27. Curran Associates, Inc.\nCharuta Pethe and Steve Skiena. 2019. The trumpiest\ntrump? identifying a subject’s most characteristic\ntweets. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n1653–1663, Hong Kong, China. Association for Com-\nputational Linguistics.\nMaxime Peyrard, Beatriz Borges, Kristina Gligoric, and\nRobert West. 2021. Laughing heads: Can transform-\ners detect what makes a sentence funny? In Proceed-\nings of the Thirtieth International Joint Conference\non Artificial Intelligence, IJCAI 2021, Virtual Event /\nMontreal, Canada, 19-27 August 2021, pages 3899–\n3905. ijcai.org.\nYuval Pinter, Cassandra L. Jacobs, and Max Bittker.\n2020. NYTWIT: A dataset of novel words in the\nNew York Times. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 6509–6515, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nJan Wira Gotama Putra and Masayu Leylia Khodra.\n2017. Automatic title generation in scientific articles\nfor authorship assistance: a summarization approach.\nJournal of ICT Research and Applications, 11(3):253–\n267.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nSudha Rao and Joel Tetreault. 2018. Dear sir or madam,\nmay I introduce the GY AFC dataset: Corpus, bench-\nmarks and metrics for formality style transfer. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 129–140, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685–2702, Online. Association\nfor Computational Linguistics.\nAlan Ritter, Stephen Soderland, Doug Downey, and\nOren Etzioni. 2008. It’s a contradiction – no, it’s\nnot: A case study using functional relations. In Pro-\nceedings of the 2008 Conference on Empirical Meth-\nods in Natural Language Processing, pages 11–20,\nHonolulu, Hawaii. Association for Computational\nLinguistics.\nChristopher De Sa, Ce Zhang, Kunle Olukotun, and\nChristopher Ré. 2015. Taming the wild: A unified\nanalysis of hogwild!-style algorithms.\nJoseph Sanu, Mingbin Xu, Hui Jiang, and Quan Liu.\n2017. Word embeddings based on fixed-size ordi-\nnally forgetting encoding. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\n74\nLanguage Processing, pages 310–315, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nAlexandra Schofield and David Mimno. 2016. Com-\nparing apples to apple: The effects of stemmers on\ntopic models. Transactions of the Association for\nComputational Linguistics, 4:287–300.\nFlorian Schroff, Dmitry Kalenichenko, and James\nPhilbin. 2015. Facenet: A unified embedding for\nface recognition and clustering. In 2015 IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 815–823.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nChen Shani, Nadav Borenstein, and Dafna Shahaf. 2021.\nHow did this get funded?! Automatically identify-\ning quirky scientific achievements. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 14–28, Online. As-\nsociation for Computational Linguistics.\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin\nGal, Nicolas Papernot, and Ross Anderson. 2023.\nThe curse of recursion: Training on generated data\nmakes models forget. ArXiv, abs/2305.17493.\nEdwin Simpson, Erik-Lân Do Dinh, Tristan Miller, and\nIryna Gurevych. 2019. Predicting humorousness and\nmetaphor novelty with Gaussian process preference\nlearning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL 2019), pages 5716–5728.\nXiaohui Song, Longtao Huang, Hui Xue, and Songlin\nHu. 2022. Supervised prototypical contrastive learn-\ning for emotion recognition in conversation. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 5197–\n5206, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nTiberiu Sosea and Cornelia Caragea. 2020. Cancer-\nEmo: A dataset for fine-grained emotion detection.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 8892–8904, Online. Association for Computa-\ntional Linguistics.\nZayne Sprague, Kaj Bostrom, Swarat Chaudhuri, and\nGreg Durrett. 2022. Natural language deduction with\nincomplete information.\nKeith Stevens, Philip Kegelmeyer, David Andrzejewski,\nand David Buttler. 2012. Exploring topic coherence\nover many models and many topics. In Proceedings\nof the 2012 Joint Conference on Empirical Methods\nin Natural Language Processing and Computational\nNatural Language Learning, EMNLP-CoNLL ’12,\npage 952–961, USA. Association for Computational\nLinguistics.\nPiotr Szyma´nski and Kyle Gorman. 2020. Is the best\nbetter? Bayesian statistical model comparison for\nnatural language processing. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2203–2212,\nOnline. Association for Computational Linguistics.\nJiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017. From\nneural sentence summarization to headline genera-\ntion: A coarse-to-fine approach. In IJCAI, volume 17,\npages 4109–4115.\nZeqi Tan, Yongliang Shen, Xuming Hu, Wenqi Zhang,\nXiaoxia Cheng, Weiming Lu, and Yueting Zhuang.\n2022. Query-based instance discrimination network\nfor relational triple extraction. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 7677–7690, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nRaphael Tang, Jaejun Lee, Ji Xin, Xinyu Liu, Yao-\nliang Yu, and Jimmy Lin. 2020. Showing your work\ndoesn’t always work. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2766–2772, Online. Association\nfor Computational Linguistics.\nZineng Tang, Jie Lei, and Mohit Bansal. 2021. DeCEM-\nBERT: Learning from noisy instructional videos via\ndense captions and entropy minimization. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n2415–2426, Online. Association for Computational\nLinguistics.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony S. Hartshorn, Elvis Saravia, An-\ndrew Poulton, Viktor Kerkez, and Robert Stojnic.\n2022. Galactica: A large language model for science.\nArXiv, abs/2211.09085.\nNoriko Tomuro. 2001. Tree-cut and a lexicon based\non systematic polysemy. In Second Meeting of the\nNorth American Chapter of the Association for Com-\nputational Linguistics.\nLifu Tu, Richard Yuanzhe Pang, Sam Wiseman, and\nKevin Gimpel. 2020. ENGINE: Energy-based infer-\nence networks for non-autoregressive machine trans-\nlation. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2819–2826, Online. Association for Computational\nLinguistics.\nErik Velldal, Lilja Øvrelid, Jonathon Read, and Stephan\nOepen. 2012. Speculation and negation: Rules,\nrankers, and the role of syntax. Computational Lin-\nguistics, 38(2):369–410.\n75\nAshwin K. Vijayakumar, Michael Cogswell, Ram-\nprasaath R. Selvaraju, Qing Sun, Stefan Lee, David J.\nCrandall, and Dhruv Batra. 2016. Diverse beam\nsearch: Decoding diverse solutions from neural se-\nquence models. CoRR, abs/1610.02424.\nAlex Wang, Jan Hula, Patrick Xia, Raghavendra Pappa-\ngari, R. Thomas McCoy, Roma Patel, Najoung Kim,\nIan Tenney, Yinghui Huang, Katherin Yu, Shuning\nJin, Berlin Chen, Benjamin Van Durme, Edouard\nGrave, Ellie Pavlick, and Samuel R. Bowman. 2019a.\nCan you tell me how to get past sesame street?\nsentence-level pretraining beyond language model-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4465–4476, Florence, Italy. Association for Compu-\ntational Linguistics.\nQifan Wang, Li Yang, Xiaojun Quan, Fuli Feng, Dong-\nfang Liu, Zenglin Xu, Sinong Wang, and Hao Ma.\n2022. Learning to generate question by asking ques-\ntion: A primal-dual approach with uncommon word\ngeneration. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 46–61, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nQingyun Wang, Lifu Huang, Zhiying Jiang, Kevin\nKnight, Heng Ji, Mohit Bansal, and Yi Luan. 2019b.\nPaperRobot: Incremental draft generation of scien-\ntific ideas. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1980–1991, Florence, Italy. Association for\nComputational Linguistics.\nWilliam Yang Wang and Kathleen McKeown. 2010.\n“got you!”: Automatic vandalism detection in\nWikipedia with web-based shallow syntactic-\nsemantic modeling. In Proceedings of the 23rd In-\nternational Conference on Computational Linguis-\ntics (Coling 2010), pages 1146–1154, Beijing, China.\nColing 2010 Organizing Committee.\nXinrun Wang, Bo An, Martin Strobel, and Fookwai\nKong. 2018. Catching captain jack: Efficient time\nand space dependent patrols to combat oil-siphoning\nin international waters. In Proceedings of the Thirty-\nSecond AAAI Conference on Artificial Intelligence\nand Thirtieth Innovative Applications of Artificial\nIntelligence Conference and Eighth AAAI Symposium\non Educational Advances in Artificial Intelligence ,\nAAAI’18/IAAI’18/EAAI’18. AAAI Press.\nYizhong Wang, Sujian Li, and Houfeng Wang. 2017.\nA two-stage parsing method for text-level discourse\nanalysis. In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 2: Short Papers) , pages 184–188, Vancouver,\nCanada. Association for Computational Linguistics.\nLiang Wen, Houfeng Wang, Yingwei Luo, and Xiaolin\nWang. 2022. M3: A multi-view fusion and multi-\ndecoding network for multi-document reading com-\nprehension. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1450–1461, Abu Dhabi, United Arab Emi-\nrates. Association for Computational Linguistics.\nJoachim Wermter and Udo Hahn. 2006. You can’t beat\nfrequency (unless you use linguistic knowledge) –\na qualitative evaluation of association measures for\ncollocation and term extraction. In Proceedings of\nthe 21st International Conference on Computational\nLinguistics and 44th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 785–792,\nSydney, Australia. Association for Computational\nLinguistics.\nBowen Xing and Ivor W. Tsang. 2022. Co-guiding\nnet: Achieving mutual guidances between multiple\nintent detection and slot filling via heterogeneous\nsemantics-label graphs.\nRui Yan, Mingkun Gao, Ellie Pavlick, and Chris\nCallison-Burch. 2014. Are two heads better than\none? crowdsourced translation via a two-step collab-\noration of non-professional translators and editors.\nIn Proceedings of the 52nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1134–1144, Baltimore, Mary-\nland. Association for Computational Linguistics.\nKaiyu Yang, Jia Deng, and Danqi Chen. 2022. Gen-\nerating natural language proofs with verifier-guided\nsearch.\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and\nKarthik Narasimhan. 2020. Keep CALM and ex-\nplore: Language models for action generation in text-\nbased games. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 8736–8754, Online. Association\nfor Computational Linguistics.\nHongbin Ye, Ningyu Zhang, Hui Chen, and Huajun\nChen. 2022. Generative knowledge graph construc-\ntion: A review. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1–17, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nKi Yoon Yoo and Nojun Kwak. 2022. Backdoor attacks\nin federated learning by rare embeddings and gra-\ndient ensembling. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 72–88, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nWeizhe Yuan, Pengfei Liu, and Graham Neubig. 2022.\nCan we automate scientific reviewing? Journal of\nArtificial Intelligence Research, 75:171–212.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text gener-\nation. Advances in Neural Information Processing\nSystems, 34:27263–27277.\nGuangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang,\nSicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi\nZeng, Xiangyu Dong, Ruoyu Zhang, Hongchao Fang,\n76\nPenghui Zhu, Shu Chen, and Pengtao Xie. 2020.\nMedDialog: Large-scale medical dialogue datasets.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9241–9250, Online. Association for Computa-\ntional Linguistics.\nJianyang Zhang, Tao Liang, Mingyang Wan, Guowu\nYang, and Fengmao Lv. 2022. Curriculum knowl-\nedge distillation for emoji-supervised cross-lingual\nsentiment analysis. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 864–875, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2019. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization.\nTianyi Zhang, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 563–578, Hong\nKong, China. Association for Computational Lin-\nguistics.\nXiaodan Zhu, Gerald Penn, and Frank Rudzicz. 2009.\nSummarizing multiple spoken documents: Finding\nevidence from untranscribed audio. In Proceedings\nof the Joint Conference of the 47th Annual Meeting\nof the ACL and the 4th International Joint Confer-\nence on Natural Language Processing of the AFNLP:\nVolume 2 - Volume 2, ACL ’09, page 549–557, USA.\nAssociation for Computational Linguistics.\nYilun Zhu, Sameer Pradhan, and Amir Zeldes. 2021.\nOntoGUM: Evaluating contextualized SOTA corefer-\nence resolution on 12 more genres. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 461–467, Online.\nAssociation for Computational Linguistics.\nA Filtering\n(1) We restrict the data to the main conference\npapers (e.g., EMNLP, ACL). We limit the data\nto abstracts of length smaller than 400 words as\nextremely long abstracts in the dataset often con-\ntain extra sections other than abstracts. (3) We\nonly leverage papers published after the year 2000\n(which form the majority anyway).\nB Training details for title generation\nWe train models with AdamW Optimizer\n(Loshchilov and Hutter, 2019) and linear learning\nrate scheduler, and subsequently use beam search\n(Vijayakumar et al., 2016) as the sampling strategy\nto generate the output candidates. The optimal\ncheckpoint for each model is selected based on the\nROUGE1/2/L (Lin, 2004) scores on the dev set.\nTable 7 displays the hyperparameter for training\nand Table 8 shows the parameters used for beam\nsearch. The models were trained using Google\nColab with a Tesla K80 GPU which has 24 GB of\nmemory. We show the number of parameters of\neach baseline model in Table 15.\nC Variants of used automatic evaluation\nmetrics\nIn ref-based evaluation, we report Rouge-1 re-\ncall, BERTScore recall, unigram MoverScore,\nBARTScore recall, MENLI(ref ←cand_e-c) and\nCOMET(wmt20-comet-da). In ref-free setup,\nwe use the Faithfulness variant for BARTScore,\nMENLI(src→cand_-c) and COMET (wmt21-\ncomet-qe-mqm) instead; the variants of the other\nmetrics are the same as in ref-based setting.\nD Ref-based evaluation results of baseline\nmodels\nTable 9 shows the ref-based automatic evaluation\nresults of the baseline models.\nE BART base VS. BARTxsum\nTable 10 shows the examples of abstract-title pairs\nwhere BARTbase failed to capture the key infor-\nmation in the abstract while BARTxsum succeeded.\nF Examples of funny titles\nTable 13 and Table 14 show sample funny titles\nlabeled by human annotators. We note: some in-\nstances of humor require contextual (e.g., culture-\nor domain-specific) knowledge such as references\nto popular TV shows (‘Germany’s next language\nmodel’); this is characteristic of humor and makes\nit challenging/subjective. Despite of this, our agree-\nments indicate a shared notion of humor among our\nannotators.\n77\nlearning rate batch size epochs gradient accumulation steps\nBARTxsum 3e-05 3 3 8\nPEGASUSxsum 6e-04 3 3 8\nBARTbase 3e-04 8 3 8\nGPT2 3e-04 2 3 8\nT5 3e-04 8 3 8\nBARTcnn 3e-04 4 3 8\nTable 7: Training hyperparameter for title generation. We use the AdamW optimizer with a weight decay of 0.01\nand keep the other settings as default in Huggingface’s Trainer API.\nmax length 30\nmin length 3\nrepetition penalty 2\nlength penalty 10\nnum beams 5\nnum return sequences 5\nTable 8: Parameter settings for beam search.\nsystem MoverS BERTS COMET BARTS MENLI ROUGE\nBARTxsum 0.410 0.912 -0.283 -3.816 0.076 0.455PEGASUSxsum 0.404 0.906 -0.371 -3.964 0.005 0.384BARTbase 0.405 0.907 -0.373 -3.986 0.036 0.403GPT2 0.400 0.902 -0.461 -4.114 -0.020 0.361T5 0.381 0.898 -0.501 -4.177 -0.025 0.337BARTcnn 0.282 0.907 -0.634 -3.747 0.133 0.448\nTable 9: Ref-based evaluation results of the baseline\nmodels. We underlie the best performance among all\ngeneration systems including human. We bold the best\nperformance among all automatic generation systems\nexcluding human.\nG Humor annotation + classifiation\nThe two annotators first annotated the same 230\ntitles independently, obtaining only 0.397 Kappa\nagreement, which indicates a relatively bad anno-\ntation quality. To improve the inter-agreement be-\ntween the annotators, they then discussed the rea-\nsons leading to disagreement. Subsequently, they\nannotated another 300 titles independently, achiev-\ning a decent 0.650 Kappa for a task as subjective\nas humor. As a consequence, we use the maxi-\nmal label value among the two annotations for\neach title as its final label for the 300 titles , i.e.,\nif one annotator labels a title with 1 (FUNNYmed),\nwhile the other labels with 0 ( ¬FUNNY), we as-\nsign label 1 to the title. Each annotator then la-\nbeled 600 different titles separately, bringing 1,730\n(230 + 300 + 600×2 = 1730) annotated titles in\ntotal, where 1,603 titles are labeled as ¬FUNNY,\n106 as FUNNYmed and 21 as FUNNY.\nAs the funny titles (labeled as FUNNY) are very\nfew compared to the not funny ones (labeled with\n0), we generate 11 different data splits, where the\ntrain set of each split consists of 100 funny titles\nand 200 not funny ones (randomly sampled from\nthe 1730 titles), while the remaining 27 funny ti-\ntles and other 27 not funny ones compose the dev\nset. From the 11 different data splits, we obtain\n11 classifiers (checkpoints selected based on the\nmacro F1 on each dev set). We then evaluate the\nensembles of the 11 classifiers on 315 newly an-\nnotated titles by the two annotators, who obtain\n0.639 Kappa agreement this time. With this step,\nwe study the optimal ensemble of the classifiers\nand also obtain more funny titles from the whole\ndata by annotating the funniest titles selected by\nthe ensemble classifiers. We design two types of\nensemble classifiers:\n• EnsMV, which relies on the majority vote of\nthe 11 classifiers. Specifically, each title re-\nceives 11 labels from the 11 classifiers: if the\nnumber of ¬FUNNY labels exceeds 5, the title\nis labeled as ¬FUNNY; if not, the title is la-\nbeled as FUNNY when the number of FUNNY\nlabels exceeds the number of FUNNYmed la-\nbels, otherwise it is labeled as FUNNYmed.\n• EnsSUMi,j, which depends on the sum of\n78\nAbstract [...] we propose to learn word embeddings based on the recent fixed-size ordinally forgetting encoding\n(FOFE) method, which can almost uniquely encode any variable-length sequence into a fixed-size represen-\ntation. [...] (Sanu et al., 2017)\nBARTbase Learning Word Embeddings Based on Ordinally Forgetting Encoding\nBARTxsum Learning Word Embeddings Based on Fixed-Size Ordinally Forgetting Encoding\nAbstract [...] Unfortunately, the reliance on manual annotations, which are both difficult and highly expensive to\nproduce, presents a major obstacle to the widespread application of these systems across different languages\nand text genres. In this paper we describe a method for inducing the semantic roles of verbal arguments\ndirectly from unannotated text . [...] (Lang and Lapata, 2010)\nBARTbase Inducing Semantic Roles from Text for Semantic Role Labeling\nBARTxsum A Probabilistic Model for Semantic Role Induction from Unannotated Text\nAbstract [...] At the same time, we argue that relation labeling can benefit from naked tree structure and should be\ntreated elaborately with consideration of three kinds of relations including within-sentence, across-sentence\nand across-paragraph relations. Thus, we design a pipelined two-stage parsing method for generating an\nRST tree from text. [...] (Wang et al., 2017)\nBARTbase Pipelined Two-Stage Parsing of Named Discourse Trees\nBARTxsum Pipeline-based Parsing of Discourse Trees for RST and Relation Labeling\nTable 10: Examples of abstract-title pairs where BARTbase failed to capture the key information in the abstract\nwhile BARTxsum succeeded. The key information is highlighted in both abstracts and titles.\n230 instance 35 instances\nτ τ\nROUGE -0.054 -0.014\nBARTS 0.092 0.121\nBERTS 0.078 0.113\nMoverS 0.001 0.038\nMENLI 0.061 0.121\nCOMET 0.127 0.194\nA2TMetric - 0.276\nTable 11: Segment-level WMTτ-like correlations of ref-\nfree evaluation metrics on all 230 instances (1380 titles;\nleft block) and 35 instances (210 titles; right block). The\ncorrelations on the 35 instances are averaged over the\ntest sets from five splits. We bold the highest correlation\nin each block.\nTitle Label\nLearning to learn by gradient descent by\ngradient descent (Andrychowicz et al.,\n2016)\nFUNNY\nCancerEmo: A Dataset for Fine-Grained\nEmotion Detection (Sosea and Caragea,\n2020)\nFUNNYmed\nGlobal Encoding for Abstractive Summa-\nrization (Lin et al., 2018)\n¬FUNNY\nTable 12: Examples of annotated titles.\nthe label values. The sum of the label val-\nues for each title ranges from 0 (11 classifiers\n×0 for ¬FUNNY) to 22 (11 classifiers ×2\nfor FUNNY). We then select a threshold ifor\nFUNNYmed and j for FUNNY: if sum < i,\nthe title is labeled as ¬FUNNY; otherwise it\nis labeled as FUNNYmed (when sum < j) or\nFUNNY (when sum ≥j).\nTable 16 shows the evaluation results of Stage\n1; we only present the performance of EnsSUMi,j\nwith optimal iand j here, i.e., EnsSUM7,16. We\nobserve that: (1) both ensembles perform better\nthan the individual ones (+4-5% macro F1) and (2)\nEnsSUM7,16 is slightly better than EnsMV (62.4%\nvs. 61.4% macro F1).\nH Dataset Statistics\nTable 18 shows the statistics of the final dataset.\nI Parameters for humor generation\nWe train BARTxsum on our train set using the\nAdamW optimizer with weight decay 0.01 and\nlearning rate 4e-05 for 5 epochs. Then we con-\ntinue to train it on the pseudo data for one epoch\nto obtain BARTxsum+pseudo. We use the default\nsettings in Huggingface’s Trainer API for the other\nhyperparameters. We train the models with an RTX\nA6000 GPUwhich has 48 GB of memory.\nTo monitor the models’ ability to generate titles\non correct humor levels, we use macro F1 between\n79\nthe expected humor labels (i.e., the humor con-\nstraints given to the inputs) and the humor labels\nassigned to the generated titles by the humor classi-\nfier as the performance indicator, with which on the\ndev set we select the optimal model checkpoints of\nthe two systems.\nJ Automatic evaluation of humor\ngeneration\nTable 19 shows the systems’ ability for humor gen-\neration before and after training on the pseudo data\naccording to the automatic evaluation.\nK Examples of system-generated funny\ntitles\nTable 22 and 23 show 10 system-generated low-\nquality funny titles and 10 system-generated high-\nquality funny titles, respectively, according to the\nhuman evaluation results.\nTowards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper) (Castro et al., 2019)\nThieves on Sesame Street! Model Extraction of BERT-based APIs (Krishna et al., 2019)\nAre Two Heads Better than One? Crowdsourced Translation via a Two-Step Collaboration of Non-Professional Translators\nand Editors (Yan et al., 2014)\nTaming the Wild: A Unified Analysis of Hogwild-Style Algorithms (Sa et al., 2015)\nBalancing Between Bagging and Bumping (Heskes, 1996)\nSpeculation and Negation: Rules, Rankers, and the Role of Syntax (Velldal et al., 2012)\nWhat’s in a name? In some languages, grammatical gender (Nastase and Popescu, 2009)\nBAM! Born-Again Multi-Task Networks for Natural Language Understanding (Clark et al., 2019)\nIs the Best Better? Bayesian Statistical Model Comparison for Natural Language Processing (Szyma ´nski and Gorman, 2020)\nKeep CALM and Explore: Language Models for Action Generation in Text-based Games (Yao et al., 2020)\nTable 13: Examples of human titles which were labeled as FUNNYmed+FUNNYmed, FUNNYmed+FUNNY, or\nFUNNY+FUNNY by the two annotators (the two entries denote the label assigned by different annotators.).\nFUNNY\nGerman’s Next Language Model (Chan et al., 2020)\nIs the Best Better? Bayesian Statistical Model Comparison for Natural Language Processing (Szyma ´nski and Gorman, 2020)\nComparing Apples to Apple: The Effects of Stemmers on Topic Models (Schofield and Mimno, 2016)\n(Almost) No Label No Cry (Patrini et al., 2014)\nThe Trumpiest Trump? Identifying a Subject’s Most Characteristic Tweets (Pethe and Skiena, 2019)\nQuestionable Answers in Question Answering Research: Reproducibility and Variability of Published Results (Crane, 2018)\nKnow What You Don’t Know: Unanswerable Questions for SQuAD (Rajpurkar et al., 2018)\nDear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer\n(Rao and Tetreault, 2018)\nCan You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling (Wang et al.,\n2019a)\nShowing Your Work Doesn’t Always Work (Tang et al., 2020)\n\"Got You!\": Automatic Vandalism Detection in Wikipedia with Web-based Shallow Syntactic-Semantic Modeling (Wang and\nMcKeown, 2010)\nIt’s a Contradiction - no, it’s not: A Case Study using Functional Relations (Ritter et al., 2008)\nFUNNYmed\nCPR: Classifier-Projection Regularization for Continual Learning (Cha et al., 2020)\nNYTWIT: A Dataset of Novel Words in the New York Times (Pinter et al., 2020)\nMedDialog: Large-scale Medical Dialogue Datasets (Zeng et al., 2020)\nCatching Captain Jack: Efficient Time and Space Dependent Patrols to Combat Oil-Siphoning in International Waters (Wang\net al., 2018)\nThe Shattered Gradients Problem: If resnets are the answer, then what is the question? (Balduzzi et al., 2017)\nGo Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification (Edwards et al.,\n2020)\nSentiLARE: Sentiment-Aware Language Representation Learning with Linguistic Knowledge (Ke et al., 2020)\nGet Semantic With Me! The Usefulness of Different Feature Types for Short-Answer Grading (Padó, 2016)\nWitches’ Brew: Industrial Scale Data Poisoning via Gradient Matching (Geiping et al., 2020)\nENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation (Tu et al., 2020)\nYou Can’t Beat Frequency (Unless You Use Linguistic Knowledge) - A Qualitative Evaluation of Association Measures for\nCollocation and Term Extraction (Wermter and Hahn, 2006)\nOntoGUM: Evaluating Contextualized SOTA Coreference Resolution on 12 More Genres (Zhu et al., 2021)\nTable 14: Selected human titles in the annotated data judged as funny or medium funny by the annotators.\n80\nAnnotation Example\nAnnotation Guidelines\nFigure 2: Screenshot of an annotation instance and the annotation guidelines. The evaluation is conducted with\ngoogle spreadsheet.\n# parameters\nBARTbase 140M\nBARTxsum 400M\nBARTcnn 400M\nT5 60M\nGPT2 117M\nPAGASUSxsum 568M\nTable 15: Number of parameters of the six baseline\nmodels.\nL Comparison of ChatGPT versions\nWe randomly choose 10 abstract-title pairs from\nour previous evaluation for both low- and high-\nquality titles, following each humor constraint\n(FUNNY and ¬FUNNY); this totals to 40 evalu-\nation instances.8 Then, we use the new version\nof ChatGPT to generate titles for those abstracts,\naccording to the humor constraints of their paired\ntitles. Two annotators were tasked with rating the\nhigher quality title among the two from different\n8In this context, we consider the titles ranked above 2 as\nhigh quality and below 3 as low quality.\nIndividuals Ensembles\nEnsMV EnsSUM 7,16\nF1 57.6% 61.4% 62.4%\nTable 16: Average macro F1 over the 11 individual\nclassifiers and macro F1 of the ensemble classifiers from\nstage 1 on the evaluation data of 315 titles (where the\ntwo annotators obtain 0.639 kappa). We bold the highest\nmacro F1 score.\n81\nChatGPT versions, obtaining a Cohen’s Kappa\nscore of 0.756 for agreement on 10 common in-\nstances.9\nM Training on extra parts besides\nabstract\nWe do the same filtering in §3 except for restricting\nto main conference papers, as there are no venue\nlabels; additionally, we remove the papers which\nhave empty title, abstract, introduction, or conclu-\nsion sections in the data. The filtered data contains\n22,452 papers, which are then split into train, dev,\nand test sets in a ratio of 8:1:1. For “[MODEL]+X”\nmodels, we concatenate the texts of the three parts\nby two “</s>” tokens as the model input. For LED\nmodels, we limit the maximal input length to 2,048,\nwhich is able to cover the concatenated inputs of\nthe great majority of instances; as for BARTXsum,\n9If one can not differentiate between the two titles, it is\nallowed to annotate them as equal.\nKappa\n#titles three-way binary\nStage 1\n230 0.397 0.513\n300 0.650 0.754\n315 0.639 0.709\nStage 2 197 0.649 0.661\nTable 17: Kappa agreements between the two annotators\non several data pieces. “#titles” refers to the number\nof titles in a certain piece of data. We bold the higher\nKappa on the same data.\nHumor label Total Source\n¬FUNNY FUNNY NLP ML\ntrain 30,741 1,011 31,752 16,141 15,611\ndev 400 200 600 480 120\ntest 400 200 600 480 120\ntotal 31,541 1,411 32,952 17,101 15,851\nTable 18: Distribution of the source (NLP or ML) and\nhumor labels (FUNNY or ¬FUNNY) of the instances in\nour dataset.\nF1macro ACC¬FUNNYACCFUNNYRatioSAME\nBARTxsum 0.647 94.5% 40.2% 6.5%\nBARTxsum+pseudo0.856↑ 93.6%↓ 77.8%↑ 4.7%↑\nTable 19: Automatic evaluation for the systems’ ability\nto generate titles with correct humor constraints. We\nbold the best performance. ↑/↓in the second row indi-\ncates the performance being better/worse after training\non the pseudo data.\nsystem humor constraint humor quality\nBARTxsum\n¬FUNNY 2.85 2.32\nFUNNY 1.79 2.81\nBARTxsum+pseudo ¬FUNNY 2.97 2.64\nFUNNY 1.43 3.26\nTable 20: Average rank of the system titles for all ab-\nstracts in the human evaluation of general quality and\nhumor degree; smaller values denotes higher ranks. “Hu-\nmor constraint” refers to the constraints given to the\ninput of the generation systems.\nthe maximal input length is 1,024, which indicates\nthe inputs of around half of the instances will be\ntruncated.\nWe train all models using the Trainer API from\nhuggingface with a learning rate of 4e-5 and a batch\nsize of 32 for 20 epochs; the other hyperparameters\nare default. Each training was stopped by an early\nstopping with 2 patience, based on the rouge scores\non the dev set. We use beam search with 5 beams\nand a length penalty of 2 for decoding.\nN MODEL+A vs. MODEL+X\nTable 24 illustrates the examples of abstract-title\npairs where the important keywords were missing\nfrom the abstracts and only available in other parts\nlike conclusion, and Table 25 displays the examples\nof titles with hallucinations.\n82\nBARTxsum - funny titles with artefacts\nWhat’s in a Semantic Model? Comparing LDA and LSA on the Web (Stevens et al., 2012)\nDon’t paraphrase unless you know what you are talking about: Improving Question Answering Performance by Paraphrasing\n(Duboue and Chu-Carroll, 2006)\nDon’t Transliterate, Use Context! Mining New Word Translations from Comparable Corpora Using Context Information (LI,\n2004)\nReading Between the Lines: Unsupervised Summarization of Spontaneous Speech using Acoustic Patterns (Zhu et al., 2009)\nChatGPT - non-scientific funny titles\nProof Generation: Now You See It, Now You Don’t! (Yang et al., 2022)\nCo-Guiding Net: Helping You Hit the Slot and Intent Jackpot! (Xing and Tsang, 2022)\nAbduct Me If You Can: How to Prove a Claim With a Little Help From Your Friends (Premises) (Sprague et al., 2022)\nOREO-LM: The Creamy, Crunchy, and Smart Way to Answering Open-Domain Questions (Hu et al., 2022)\nTable 21: Examples of system-generated funny titles from BARTxsum with artefacts and non-scientific funny titles\nfrom ChatGPT. The citations here are the original papers for those titles.\nBARTxsum\nDon’t Invite Adversaries to Poison Your Data: Exploiting Federated Learning for Adversarial Backdoor Attacks (Yoo and\nKwak, 2022)\nDon’t Take the Easy Way Out: Generating Adversarial Negative Responses with Large-Scale Language Models for Dialogue\nSelection (Lee et al., 2022)\nDon’t Give Up on Style: Learn to Generate Stylistically-Diverse Summaries with Multiple Decoders (Goyal et al., 2022)\nCKD: Curriculum Knowledge Distiller for Cross-Lingual Sentiment Analysis with Emoji (Zhang et al., 2022)\nSuccessive Prompting: Learning to Break Down Complex Questions into As Simple As Possible (Dua et al., 2022)\nChatGPT\nGraphin’ It Up: A Humorous Guide to Generative Knowledge Construction (Ye et al., 2022)\nTiny Tasks, Big Results: A Hilarious Guide to Few-Shot Relation Extraction (Li and Qian, 2022)\nRevealing the Magic Behind Transformer Language Models: A Lighthearted Investigation (Geva et al., 2022)\nAsk and You Shall Receive: A Whimsical Approach to Automatic Question Generation (Wang et al., 2022)\nFederated Learning: The More You Poison, the More You Win! (Yoo and Kwak, 2022)\nTable 22: Examples of system-generated low-quality funny titles, which obtain high humor ranks but low quality\nranks in the human evaluation.\nBARTxsum\nDon’t Agree with Me? Introducing Semantic Environment Features Improves Agreement-Disagreement Classification in\nOnline Discourse (Gokcen and de Marneffe, 2015)\nThe Myth of the Two Sides of the Same Coin: Claim Generation and Claim Retrieval in a World of Claims (Gretz et al., 2020)\nSharing is Caring: Incentives for Self-Organization in Social Welfare Maximization (Gollapudi et al., 2019)\nDeCEMBERT: Dense Captions and Entropy Minimization for Video-and-Language Pre-training (Tang et al., 2021)\nStochastic Alternating Direction Method of Multipliers Revisited: Faster Rates and Better Algorithms (Azadi and Sra, 2014)\nChatGPT\nSucceed with Successive Prompting: Breaking Down Complex Questions for LMs (Dua et al., 2022)\nFeeling the Pulse of Dialogue: A Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation\n(Song et al., 2022)\nTriple Trouble: A Novel Query-Based Approach to Joint Entity and Relations Extraction (Tan et al., 2022)\nTwo Heads are Better than One: A Multi-View Fusion and Multi-Decoding Method for Multi-Document Reading Compre-\nhension (Wen et al., 2022)\nSeeing is Believing: A Picture’s Worth a Thousand Words in Multimodal Machine Translation (Ji et al., 2022)\nTable 23: Examples of system-generated high-quality funny titles, which obtain both high humor and quality ranks\nin the human evaluation.\n83\nAbstract This paper describes a lexicon organized around systematic polysemy: a set of word senses that are related\nin systematic and predictable ways. The lexicon is derived by a fully automatic extraction method which\nutilizes a clustering technique called tree-cut. We compare our lexicon to WordNet cousins, and the\ninter-annotator disagreement observed between WordNet Semcor and DSO corpora. (Tomuro, 2001)\nLED+A A systematic polysemy lexicon based on tree-cut\nLED+X A Systematic Polysemy Lexicon Based on Tree-Cut Extraction\nAbstract We address the problem dealing with a large collection of data, and investigate the use of automatically\nconstructing category hierarchy from a given set of categories to improve classification of large corpora.\nWe use two well-known techniques, partitioning clustering, []-means and a [] to create category hierarchy.\n[]-means is to cluster the given categories in a hierarchy. To select the proper number of [], we use a\n[] which measures the degree of our disappointment in any differences between the true distribution\nover inputs and the learner’s prediction. Once the optimal number of [] is selected, for each cluster , the\nprocedure is repeated. Our evaluation using the 1996 Reuters corpus which consists of 806,791 documents\nshows that automatically constructing hierarchy improves classification accuracy. (Fukumoto and Suzuki,\n2004)\nBARTXsum+A Automatic Construction of Category Hierarchy for Improved Classification of Large Corpora\nBARTXsum+X Automatic Construction of Category Hierarchy for Text Classification\nTable 24: Examples of abstract-title pairs where the important keywords were missing from the abstracts and only\navailable in other parts like conclusion. We highlight the keywords in the titles from “[MODEL]+X” systems.\nTokens masked with “[]” are those with OCR errors that could not be recognized.\nPaper Awamura et al. (2015)\nLED+A Location Disambiguation Using Spatial and Temporal Clues\nLED+X Location Disambiguation Using Spatial Clustering and Temporal Consistency\nPaper Pak and Paroubek (2010)\nBARTXsum+A Automatic Disambiguation of Chinese Sentiment Ambiguous Adjectives Using Twitter\nBARTXsum+X NUS-CORE : Using Twitter to Disambiguate Adjective Sentiment Ambiguous Adjectives\nTable 25: Examples of titles with hallucinations. We highlight the hallucinated words in the titles from\n“[MODEL]+X” systems.\n84",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5682380199432373
    },
    {
      "name": "Transformer",
      "score": 0.5165735483169556
    },
    {
      "name": "Engineering",
      "score": 0.21449178457260132
    },
    {
      "name": "Electrical engineering",
      "score": 0.21127039194107056
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177802217",
      "name": "University of Mannheim",
      "country": "DE"
    }
  ]
}