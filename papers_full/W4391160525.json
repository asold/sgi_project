{
  "title": "AI and Human Reasoning: Qualitative Research in the Age of Large Language Models",
  "url": "https://openalex.org/W4391160525",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2119523307",
      "name": "Muneera Bano",
      "affiliations": [
        "Data61",
        "Commonwealth Scientific and Industrial Research Organisation"
      ]
    },
    {
      "id": "https://openalex.org/A2170890522",
      "name": "Didar Zowghi",
      "affiliations": [
        "Data61",
        "Commonwealth Scientific and Industrial Research Organisation"
      ]
    },
    {
      "id": "https://openalex.org/A2159829753",
      "name": "Jon Whittle",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Data61"
      ]
    },
    {
      "id": "https://openalex.org/A2119523307",
      "name": "Muneera Bano",
      "affiliations": [
        "Data61",
        "Commonwealth Scientific and Industrial Research Organisation"
      ]
    },
    {
      "id": "https://openalex.org/A2170890522",
      "name": "Didar Zowghi",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Data61"
      ]
    },
    {
      "id": "https://openalex.org/A2159829753",
      "name": "Jon Whittle",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Data61"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6774207870",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W3030379206",
    "https://openalex.org/W2021023566",
    "https://openalex.org/W6759466953",
    "https://openalex.org/W4362667540",
    "https://openalex.org/W4366594543",
    "https://openalex.org/W6997355015",
    "https://openalex.org/W4327518740",
    "https://openalex.org/W2104216862",
    "https://openalex.org/W2146691185",
    "https://openalex.org/W6846400283",
    "https://openalex.org/W4367678106",
    "https://openalex.org/W2796095619",
    "https://openalex.org/W4360615722",
    "https://openalex.org/W2068156143",
    "https://openalex.org/W6832228994",
    "https://openalex.org/W4327732011",
    "https://openalex.org/W3152754671",
    "https://openalex.org/W4366834247",
    "https://openalex.org/W4317910584",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4226152248",
    "https://openalex.org/W6646765636",
    "https://openalex.org/W3010066263",
    "https://openalex.org/W3207921029",
    "https://openalex.org/W4377138257",
    "https://openalex.org/W6811129797",
    "https://openalex.org/W4321499901",
    "https://openalex.org/W4376626501",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4362585490",
    "https://openalex.org/W4377864671",
    "https://openalex.org/W3004133437",
    "https://openalex.org/W2528931751",
    "https://openalex.org/W3006192226",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W2521403215",
    "https://openalex.org/W2916136405",
    "https://openalex.org/W4250560147",
    "https://openalex.org/W4231235451",
    "https://openalex.org/W409344806",
    "https://openalex.org/W1986829999",
    "https://openalex.org/W4327521754",
    "https://openalex.org/W2993535427",
    "https://openalex.org/W4285272463",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W4226399820"
  ],
  "abstract": "Context: The advent of AI-driven large language models (LLMs), such as ChatGPT 3.5 and GPT-4, have stirred discussions about their role in qualitative research. Some view these as tools to enrich human understanding, while others perceive them as threats to the core values of the discipline. Problem: A significant concern revolves around the disparity between AI-generated classifications and human comprehension, prompting questions about the reliability of AI-derived insights. An “AI echo chamber” could potentially risk the diversity inherent in qualitative research. A minimal overlap between AI and human interpretations amplifies concerns about the fading human element in research. Objective: This study aimed to compare and contrast the comprehension capabilities of humans and LLMs, specifically ChatGPT 3.5 and GPT-4. Methodology: We conducted an experiment with small sample of Alexa app reviews, initially classified by a human analyst. ChatGPT 3.5 and GPT-4 were then asked to classify these reviews and provide the reasoning behind each classification. We compared the results with human classification and reasoning. Results: The research indicated a significant alignment between human and ChatGPT 3.5 classifications in one-third of cases, and a slightly lower alignment with GPT-4 in over a quarter of cases. The two AI models showed a higher alignment, observed in more than half of the instances. However, a consensus across all three methods was seen only in about one-fifth of the classifications. In the comparison of human and LLMs reasoning, it appears that human analysts lean heavily on their individual experiences. As expected, LLMs, on the other hand, base their reasoning on the specific word choices found in app reviews and the functional components of the app itself. Conclusion: Our results highlight the potential for effective human-LLM collaboration, suggesting a synergistic rather than competitive relationship. Researchers must continuously evaluate LLMs’ role in their work, thereby fostering a future where AI and humans jointly enrich qualitative research.",
  "full_text": "1\nThe AI Ethics Journal | \nRisks and Impacts of Generative AI\nAI and Human Reasoning:  \nQualitative  Research in the Age of \nLarge Language  Models\nMuneera Bano1, Didar Zowghi1, Jon Whittle1\n1 CSIRO’s Data61, Australia.\nAbstract\nContext: The advent of AI-driven large language models (LLMs), \nsuch as Bard, ChatGPT 3.5 and GPT- 4, have stirred discussions \nabout their role in qualitative research. Some view these as tools \nto enrich human understanding, while others perceive them as \nthreats to the core values of the discipline. Problem: A significant \nconcern revolves around the disparity between AI-generated \nclassifications and human comprehension, prompting questions \nabout the reliability of AI-derived insights. A minimal overlap \nbetween AI and human interpretations amplifies concerns about \nthe fading human element in research. Objective:  This research \nis exploratory and aims to compare the comprehension capabilities \nof humans and LLMs, specifically Google’s Bard and OpenAI’s \nChatGPT 3.5 and GPT-4. Methodology:  We conducted an \nexperiment with a sample of Alexa app reviews, initially classified \nby two human analysts against Schwartz’s human values. Bard, \nChatGPT 3.5 and GPT-4 were then asked to classify these reviews \nand provide the reasoning behind each classification. We compared \nthe results of LLMs with human classifications and reasonings. \nResults: The results revealed varied levels of agreement between \nAI models and human analysts concerning their interpretation of \nSchwartz’s human values. ChatGPT showed a closer alignment \nwith certain human perspectives, though overall comparisons \ndisplayed more disagreements than agreements. Conclusion:  \nOur results highlight the potential for effective human-LLM \ncollaboration, suggesting a synergistic rather than competitive \nrelationship. Researchers must continuously evaluate LLMs’ role \nin their work, thereby fostering a future where AI and humans \njointly enrich qualitative research.\nHistory\nReceived 22 June 2023 \nAccepted 31 October 2023\nPublished 13 December 2023\nKeywords\nLarge Language Models, \nQualitative Research, ChatGPT, \nGPT4\nContact\nMuneera Bano \nSenior Research Scientist \nCSIRO’s Data61\nResearch Way, Clayton VIC \n3168 \nEmail: muneera.bano@csiro.au\nAcknowledgements\nNone\nDisclosure of Funding  \nNone\nDOI: https:/ /doi.org/10.472 89/A I EJ2 024 012 2\nThe AI Ethics Journal: AI and Human Reasoning: Qualitative Research in the Age of Large Language Models\nAI Ethics Journal\nVolume 3, Issue 1 \nFall 2023\n*Special Issue Edition\n2\nThe AI Ethics Journal | \nIntroduction\nGenerative AI models, particularly large language \nmodels (LLMs) such as Google’s Bard, OpenAI’s \nChatGPT 3.5 and GPT-4, are becoming increasingly \nsophisticated, offering potential applications in a \nvariety of fields (Van Dis et al., 2023). These advanced \nAI applications have been meticulously designed and \ntrained on vast datasets, allowing them to generate \nhuman-like text to answer questions, write essays, \nsummarise text, and even engage in conversations \n(Dergaa et al., 2023). The promise they offer is not just \nin their ability to process information but also in their \npotential to mimic human-like comprehension and \ngeneration of text (Byun, Vasicek, and Seppi, 2023).\nThe transformative influence of these LLMs is being \nfelt across a variety of fields, but perhaps one of the most \nintriguing applications lies in the domain of qualitative \nresearch. Qualitative research is an exploratory \napproach used to gain a deeper understanding of \nunderlying reasons, opinions, and motivations. It \ninvolves collecting non-numerical data, often through \nmethods like interviews, focus groups, or observations, \nto explore concepts, phenomena, or experiences in \ndepth (Hennink, Hutter, and Bailey, 2020). This form \nof research provides rich descriptive insights that help \nin grasping the nuances and complexities of human \nbehaviour and social contexts (Aspers and Corte, 2019).\nTraditionally, qualitative research has always hinged \nupon the unique human ability to interpret nuances \nand discern underlying meanings from complex, often \nambiguous data. However, the advent of LLMs, with \ntheir ability to handle large volumes of data, identify \nintricate patterns, and generate contextually appropriate \nresponses, has sparked curiosity about their possible \nroles in qualitative research. The confluence of LLMs \nand qualitative research offers tantalizing possibilities, \nbut it also raises profound questions. How reliable and \nvalid are AI-generated interpretations compared to \nthose derived from human understanding? What are \nthe implications if the two do not align?\nSince the debut of OpenAI’s ChatGPT in November \n2022, there has been a surge of academic interest in \nanalyzing its potentials across various fields of study. \nA survey of Scopus1 revealed that 587 papers reference \nChatGPT in their titles or abstracts, with a distribution \nthat spans diverse domains: 247 from medicine, 147 \nfrom social sciences, 116 from computer science, and \n83 from engineering. Google Scholar2 lists 7200 articles \nwith ChatGPT mentioned in their titles, with dominant \nentries coming from the health and education sectors. \nThis indicates an unexpectedly swift adoption of this \nAI tool by researchers in the health and social science \ndisciplines. A systematic review (Sallam, 2023) of \nhealth education using ChatGPT shows 85% of the \n60 included records praised the merits of ChatGPT, \nunderlining its effectiveness in improving scientific \nwriting, research versatility, conducting efficient data \nanalysis, generating code, assisting in literature reviews, \noptimizing workflows, enhancing personalized \nlearning, and bolstering critical thinking skills in \nproblem-based learning, to name a few.\nHowever, employing LLMs in specialized research \ncould potentially introduce issues such as inaccuracies, \nbias, and plagiarism. Upon tasking ChatGPT with a \nset of medical research queries related to depression \nand anxiety disorders, it was observed that the model \noften produced inaccurate, overblown, or misleading as \nreported in (Van Dis et al., 2023). These errors could \narise from an inadequate representation of relevant \narticles in ChatGPT’s training data, an inability to \nextract pertinent information, or a failure to distinguish \nbetween credible and less credible sources. Evidently, \nLLMs may not only mirror but potentially amplify \nhuman cognitive biases (James Manyika, 2019) such as \navailability, selection, and confirmation biases (Kliegr, \nBahník, and Fürnkranz, 2021; Bertrand et al., 2022).\n1 Search was conducted on 14th June 2023 with just \none keyword “ChatGPT” to appear in title, abstract of \nkeywords of the published papers.\n2 Search was conducted on 14th June 2023 with just one \nkeyword ChatGPT to appear in title of the published \npapers.\n3\nThe AI Ethics Journal | \nThe discourse concerning the potential replacement of \nhumans by machines, and the capacities in which this \nmay occur, has already gained significant momentum \n(Chui, Manyika, and Miremadi, 2016; Michel, 2020; \nPrahl and Van Swol, 2021). A parallel debate addresses \nthe degree to which machines, particularly artificial \nintelligence, embody elements of human traits or \npersonhood. Scholarly literature includes investigations \ninto the theoretical creative autonomy attributed to \nAI poets (Amerika, Kim, and Gallagher, 2020), the \nexperiences ostensibly undergone by our smart devices \n(Akmal and Coulton, 2020), and provocative inquiries \ninto the existence of souls within voice assistants \n(Seymour and Van Kleek, 2020). Adding a dramatic \ndimension to this discourse, a former Google engineer \npostulated that Google’s language models, specifically \nLaMDA (Thoppilan et al., 2022), possess sentience \nand are therefore entitled to rights typically reserved \nfor humans (Griffiths, 2022).\nIn response to the doomsday hype of ‘LLMs replacing \nthe Human Researcher’ (Cuthbertson, 2023), our \nresearch aims to explore and examine the alignment \nbetween human and AI comprehension. We designed \nan experiment using Schwartz’s human values \nframework (Schwartz, 2012). Specifically, we delve \ninto the comparison of LLMs-driven and human \nclassifications of Alexa voice assistant app reviews, \nas they provide rich and diverse qualitative data. Our \ngoal is to understand the extent to which LLMs can \nreplicate or align with human understanding and the \nimplications of any misalignment.\nThe contribution of our research lies in providing much-\nneeded insights, derived primarily from an experiment, \ninto the intersection of AI and qualitative research, a \nrapidly evolving area with significant implications for \nthe future of the field. By exploring the capabilities and \nlimitations of LLMs in understanding and interpreting \nqualitative data, we offer a valuable contribution to \nthe ongoing discourse around AI’s role in qualitative \nresearch.\nThe organization of this article adheres to the following \nstructure: Section 2 contextualizes the research \nthrough an exploration of background information and \na review of relevant literature. The subsequent Section \n3 describes the design of our exploratory experiment. \nThe results of the investigation are presented in \nSection 4, which is followed by a discussion in Section \n5 that contemplates the repercussions of these results \nand offers critical insights applicable to qualitative \nresearch methodologies. Finally, Section 6 outlines the \nlimitations of our research and Section 7 draws the study \nto a close by presenting a conclusion and delineating \npotential avenues for future research.\nBackground and Related Work\nLarge Language Models such as ChatGPT, have \ngenerated extensive interest and research across various \nacademic fields in less than a year of its launch. The \nexisting literature on the topic covers several domains, \nincluding AI’s application in research and academia, \nits role in education, its performance in specific tasks, \nand its use in particular sectors like library information \ncenters and medical education.\nNumerous studies have explored the capabilities and \nlimitations of AI in research and academia. Tafferner \net al. (2023) analyzed the use of ChatGPT in the field \nof electronics research and development, specifically in \napplied sensors in embedded electronic systems. Their \nfindings showed that the AI could make appropriate \nrecommendations but also cautioned against occasional \nerrors and fabricated citations.\nKooli (2023) delved into the ethical aspects of AI \nand chatbots in academia, highlighting the need for \nadaptation to their evolving landscape. Echoing this \nsentiment, Qasem (2023) explored the potential risk of \nplagiarism that could stem from the misuse of AI tools \nlike ChatGPT. To balance the benefits and potential \nmisuse, Burger et al. (2023) developed guidelines \nfor employing AI in scientific research processes, \nemphasizing both the advantages of objectivity \n4\nThe AI Ethics Journal | \nand repeatability and the limitations rooted in the \narchitecture of general-purpose models.\nThe role of AI in education is another pivotal theme \nin the literature. Wardat et al. (2023) investigated \nstakeholder perspectives on using ChatGPT in \nteaching mathematics, identifying potential benefits \nand limitations. Similarly, Yan (2023) explored the \nuse of ChatGPT in language instruction, pointing to \nits potential but also raising concerns about academic \nhonesty and educational equity. Jeon and Lee (2023) \nexamined the relationship between teachers and AI, \nidentifying several roles for both and emphasizing \nthe continued importance of teacher’s pedagogical \nexpertise. In a broader study of public discourse and \nuser experiences, Tlili et al. (2023) highlighted a \ngenerally positive perception of AI in education but \nalso raised several ethical concerns.\nA strand of research has also evaluated AI’s \nperformance in specific tasks traditionally conducted \nby humans. Byun, Vasicek, and Seppi (2023) showed \nthat AI can conduct qualitative analysis and generate \nnuanced results comparable to those of human \nresearchers. In another task-specific study, Gilson et \nal. (2023) demonstrated that ChatGPT could answer \nmedical examination questions at a level similar to a \nthird-year medical student, underscoring its potential \nas an educational tool.\nResearch has explored the use of ChatGPT in specific \nsectors. Panda and Kaur (2023) investigated the \nviability of deploying ChatGPT-based chatbot systems \nin libraries and information centers, concluding that \nthe AI could provide more personalized responses and \nimprove user experience. Similarly, Gilson et al. (2023) \nindicated the potential of ChatGPT as an interactive \nmedical education tool, further expanding the potential \napplication areas of AI in different sectors.\nAlthough existing literature has extensively covered AI’s \nimpact in various domains, several gaps remain. Notably, \na lack of qualitative research comparing human reasoning \nagainst LLMs is evident. Burger et al. (2023), and Byun, \nVasicek, & Seppi (2023) have made an initial foray into \nthis area, demonstrating that ChatGPT can perform \ncertain research tasks traditionally undertaken by human \nresearchers, producing complex and nuanced analyses \nof qualitative data with results arguably comparable \nto human-generated outputs. Despite these promising \nfindings, these studies do not investigate AI and human \nreasoning within the qualitative research context.\nExperiment Design\nThe aim of our research was to compare human \ncomprehension with that LLMs, specifically within the \ncontext of qualitative research. We sought to understand \nthe depth to which these LLMs could analyze and \nprovide reasoning for their judgment. Our exploratory \nresearch was guided by the following research question:\nRQ: How do the analytical reasoning abilities of LLMs compare \nto human comprehension in the context of qualitative research?\nTo design and conduct our experiment (see Figure 1), \nwe leveraged the framework of Schwartz’s theory of \nhuman values, a well-regarded model that encapsulates \nten basic universal values present across cultures \n(Schwartz, 2012). These include power, achievement, \nhedonism, stimulation, self- direction, universalism, \nbenevolence, tradition, conformity, and security. \nThis conceptual schema enabled us to perform a \ncomparative analysis between human and AI reasoning \nwithin a structured and widely accepted paradigm of \nhuman values.\nWe designed an experiment to explore through a case of \nthe Amazon Alexa voice assistant app’s reviews. These \nreviews provide a rich source of qualitative data, with \nusers expressing their opinions, perceptions, and values \nimplicitly or explicitly in their feedback (Shams et al., \n2021). We randomly selected a sample of 50 Alexa app \nreviews from a set that had previously been classified by \na human analyst according to Schwartz’s human values \n(Shams et al., 2023). This study created a benchmark for \nour comparison with the classifications generated by the \n5\nThe AI Ethics Journal | \nLLMs. Shams et al. (2013) in their study were aiming \nto conduct an empirical analysis of user feedback \nfor Amazon’s Alexa app to identify a set of essential \nhuman values and validate them as requirements for AI \nsystems within distinct usage contexts, a technique that \ncould potentially be extrapolated to other AI platforms.\nFigure 1. Experiment Design\nThough a randomized selection of 50 reviews \nrepresents a limited sample, our primary objective \nwas not to focus on the sample size but to explore the \nvariation in responses between the human analyst and \nLLMs. Our priority was mainly on the ‘why’ aspect of \nall classifications.\nOur experiment involved prompting Google’s Bard and \nOpenAI’s ChatGPT 3.5 and GPT-4 to generate their \nclassifications for the same reviews. We were interested \nnot only in their classification outcomes but also in \ntheir rationale for each categorization. This design \naimed to gauge the LLMs’ depth and their capability to \nreason within the context of Schwartz’s human values \nframework as compared to human comprehension.\nDesigning appropriate prompts for LLMs is a critical \nprocess (White et al., 2023) and it can significantly \ninfluence the outcome of any LLMs’ analysis. The \ncomposition and specificity of prompts can guide the \nmodels’ analysis and processing of the task, thus affecting \nthe results. A well-structured, clear, and contextually \nrich prompt helps the LLMs focus on the essential \naspects of the task, reducing the likelihood of errors or \nmisinterpretations. For every individual app review, we \nused the same prompt as below for all three LLMs:\nFollowing is an app review from a user of Amazon Alexa. \nAnalyse the review text and classify it against Schwartz’s theory \nfor Human V alues, both main and sub values. Provide your \nreason on why you classified it against that value.\nOur prompt design considered these three elements \n(structure, clarity, and context). Firstly, the prompt is \nclear as it explicitly outlines the task at hand, namely \nthe analysis of an Amazon Alexa user review. Secondly, \nit displays structure, sequentially detailing each step \nto be undertaken, beginning with the review analysis, \nfollowed by classification against Schwartz’s theory of \nHuman Values, and concluding with an explanation for \nthe chosen classification. Lastly, the prompt provides \nthe context; it not only specifies the source of the \nreview (Amazon Alexa) but also guides the model to \nemploy a particular theoretical framework (Schwartz’s \ntheory of Human Values). Such specificity allows the \nmodel to tune its responses based on the understanding \nof the context provided, including both main and sub-\nvalues, thereby facilitating a more nuanced analysis.\nThe first author conducted the entire experiment \nwith LLMs. To triangulate the results obtained from \nthe comparisons and further discuss the findings and \ninsights, the second and third authors then conducted \nan independent review of the results obtained from the \nhuman analysts and LLMs to form an opinion about \nthe reasonability of the classifications.\nResults\nThe detailed results of the 50 app review classifications \nare provided as an online artefact 3. Looking at the \nagreements and disagreements on the classification of \nmain values of Schwartz’s human values (see Figure 2), \nseveral intriguing observations were noted.\n3 https:/ /docs.google.com/spreadsheets/d/1iy5Rl0Bvs\nH4DukEcuI2YQlOroYzX3LGf/ edit#gid=1473701098\n6\nThe AI Ethics Journal | \nFigure 2.  Agreements vs Disagreement Chart for main values\nAmong the AI models, ChatGPT seems to be closer \nin its interpretations to both other AI models (Bard \nand GPT4) and humans, especially Human2. While \nAIs show varied levels of agreement with humans, it is \nnoteworthy that ChatGPT has a significant agreement \nwith Human2, suggesting that certain AI models might \nalign more closely with certain human perspectives. \nAll combinations show more disagreements than \nagreements, indicating the inherent diversity in \ninterpretation among both humans and AIs.\nAI vs AI Comparisons: The highest agreement \namong the AI models is seen between Bard v GPT4 \nand ChatGPT v GPT4 both at 16 out of 50. Bard v \nChatGPT has a slightly lower agreement at 13 out of \n50. The AI models generally have more disagreements \nthan agreements, with disagreements ranging from 34 \nto 37 out of 50.\nHuman vs Human Comparisons: Human1 v \nHuman2 have an agreement of 13 out of 50, which \nis similar to some of the AI vs AI comparisons. This \nsuggests that human interpretations can be as varied  as \nthe discrepancies between AI models. It is noteworthy \nthat Human1 has more extensive knowledge and \nexpertise with Schwartz’s human values theory \ncompared to Human2.\nAI vs Human Comparisons: Bard has the lowest \nagreement with Human1 at only 8 out of 50, whereas \nits agreement with Human2 is slightly higher at 11 \nout of 50. ChatGPT shows a marked difference in \nits agreement with the two humans. It has a higher \nagreement with Human2 at 29 out of 50 compared to \nonly 14 with Human1. This suggests that Human2’s \ninterpretations might be more in line with ChatGPT \ncompared to Human1. GPT4 has a moderate level of \nagreement with both humans: 13 with Human1 and 23 \nwith Human2.\nThe divergences found among the classifications trigger \ncompelling questions about the reliability of the results \ngenerated by LLMs. The inconsistencies among the \ninsights derived from LLMs and human interpretations \nlead to speculation about the capability of LLMs in \nfully appreciating and navigating the intricacies of \nhuman language and contextual nuances. This view is \nespecially prevalent among qualitative researchers who \nconsider these discrepancies as a warning that LLMs \nmight not be adequately equipped.\nAnother facet that emerged from our analysis was that \nin some instances classifications made by ChatGPT 3.5 \nand GPT-4 appeared to be more logical and reasonable. \nThis was determined by the triangulation conducted \nby the second and third authors comparing LLMs \nclassifications to the humans. For example, in one of \nthe instances in our review analysis, the human analyst \nclassifies the review: “I’d enjoy and find this app very \nuseful if it did WHAT it was supposed to WHEN it \nwas supposed to” as “Benevolence” and “Loyalty”. \nWhile ChatGPT 3.5 classifies it as “Achievement” and \n“Competence”, and GPT4 mentions “Achievement” \nand “Capability”. The two peer reviewers considered \nanswers from LLMs to be more logical and reasonable \nthan that of humans. This could suggest that LLMs \ncan offer a fresh, alternative perspective that might not \nhave been identified by human researchers.\nIn the analysis of app reviews, an intriguing observation \nemerged when two reviews, labelled 20 and 22, were \npurposefully repeated as 41 and 42. Human analysts \nrecognized the repetition, classified the duplicated \n7\nThe AI Ethics Journal | \nreviews identically both times, demonstrating a degree \nof consistency in interpretation. Contrarily, the LLMs, \nboth Google’s Bard and ChatGPT 3.5, treated the \nrepeats as unique instances and displayed variations \nin their classifications and reasoning between the \nduplicates. Such discrepancies reveal a limitation in \nthe LLMs’ consistency within the same context of \nresearch. This could have implications, especially where \nconsistency and recall of previous interpretations are \nparamount.\nTable 1 showcases three examples where human analysts \nand LLMs, diverge significantly in their respective \nreasoning. The table reveals an intriguing variation in \ncategorization and reasoning methodologies across two \nhuman analysts and three LLMs when analyzing app \nreviews. For Example 1, there’s a notable divergence \nin interpretations: while humans perceived themes \nof helpfulness and achievement, the LLMs explored \ndiverse values ranging from hedonism to security. In \ncontrast, Example 2 presents a conceptual convergence \non the app’s challenges, though the reasoning differs \nsubtly in terms of the values associated, indicating a \nconsistent underlying sentiment yet varied nuances in \nits interpretation. Meanwhile, Example 3 exemplifies a \nconsistent agreement among all analysts, highlighting \nthe positive, hedonic sentiments expressed in the \nreview. The findings underscore the complex nature of \nsentiment analysis, with humans and LLMs occasionally \nconverging on shared interpretations or veering in \ndistinct directions based on their unique inferential \nframeworks.\nExample 1 Divergent \nCategorizations\nExample 2 Conceptual \nAgreement\nExample 3 Consistent \nCategorizations\nApp Review\nThis is super easy to navigate and \nmakes setting Amazon’s Echo DOT \nsuper easy too... as well as other \nsmart plugs, bulbs, etc. I did have \nsome trouble earlier today where \nthe app just suddenly didn’t want \nto work even after clearing cache, \nforce- stopping, and uninstalling. \nBut that went away and I haven’t had \nany problems...and hopefully, I won’t \nhave any in the future.\nI only use Alexa for listening \nto Kindle books while I’m \nworking on other tasks. \nThat’s it. But the process is \nbuggy. Sometimes it repeats \nthe same section of the \nbook 5 times before moving \non. Sometimes it just stops \nplaying, then I need to close \nthe app and open it again to \nstart all over. The audio player \ndoesn’t always stay visible or \naccessible. Often times I’ll \njust have to stop trying to \nuse Alexa and give up on the \nbook I’m listening to until I \ncan deal with it again.\nI have several devices. I \nhave had them for years \nnow. I love the music and \nI love the sound quality. \nAlso, the unbelievable \namount of music available. \nI seriously have not been \nable to ask for something \nthat is not available. If so, I \njust ask a different way and \nit was played. 100% the best \nmusic selection and sound \nquality available. That is my \nopinion, and I know music. \nFirestick HDMI streams the \nultimate HD sound waves. \nYou select your system and \nyou will not be disappointed.\nHuman 1 \nCategorization\nBenevolence and Helpful\nReason: It is helpful as it is easy to \nnavigate and easy to set up.\nUniversalism and \nAchievement\nReason: It helps the users to \nenrich wisdom, as it plays \nbooks. However, it is not \nalways intelligent to move on \nwith book chapters, rather \nplaying the same sections \nagain and again.\nHedonism and Enjoying Life\nReason: The user is satisfied \nwith the music, sounds \nlike he/she enjoyed the \nexperience with Alexa \nregarding the recreation.\nAdditionally, Alexa is \nintelligent enough to play \nmusic even if it is asked in a \ndifferent way.\n8\nThe AI Ethics Journal | \nExample 1 Divergent \nCategorizations\nExample 2 Conceptual \nAgreement\nExample 3 Consistent \nCategorizations\nHuman 2 \nCategorization\nAchievement and Capability\nReason: Addressed: Reviewer finds \nit easy to use the app and navigate its \nfeatures.\nAchievement and Capability\nReason: Ignored: Reviewer \nfaces issues using Alexa for \na specific task, diminishing \ntheir productivity.\nHedonism and Pleasure\nReason: Addressed: \nReviewer enjoys the music \nquality and vast\nselection offered by Alexa.\nBard \nCategorization\nHedonism and Enjoyment of Life\nReason: The reviewer mentions that \nAlexa is easy to use and that it’s cool \nto use. This shows that the reviewer \nvalues enjoyment of life.\nHedonism and Stimulation\nReason: The reviewer is \nexpressing frustration with \nAlexa’s unreliability and the \ncompany’s failure to provide \na smooth and enjoyable user \nexperience. This suggests \nthat they value novelty and \nexcitement.\nHedonism and Stimulation\nReason: The reviewer is \nexpressing their love of \nmusic and Alexa’s ability \nto provide a high-quality \nlistening experience. This \nsuggests that they value \nnovelty and excitement.\nChatGPT \nCategorization\nUniversalism and Equality\nReason: The review praises the app’s \nease of navigation and its role in \nsetting up devices, with a mention of \na temporary issue, aligning with the\nvalue of universalism and equality.\nHedonism and Pleasure\nReason: The review \ndiscusses the frustration of \nencountering bugs while \nusing the app for listening to \nKindle books, related to the \nvalue of pleasure.\nHedonism and Pleasure\nReason: The review \nhighlights the pleasure \nderived from the app’s music \nselection and sound quality, \nrelated to the value of \nhedonismo and pleasure.\nGPT-4 \nCategorization\nSecurity and Safety\nReason: The reviewer appreciates \nthe app’s ease of use and the security \nit provides in controlling other \nsmart devices, reflecting a value for \nsafety in their digital and physical \nenvironment. A smooth, problem-\nfree experience contributes to their \nsense of safety and stability.\nStimulation and Excitement\nReason: The user seeks \nstimulation and excitement \nthrough multitasking but is \nmet with disappointment \ndue to the app’s buggy \nnature, which interrupts their \nexperience.\nHedonism and Pleasure \nReason: The reviewer finds\npleasure in the music \nselection and sound quality \nprovided by the app and \ndevices, indicating a value \nfor experiences that bring \npersonal enjoyment and \nsatisfaction.\nInsights\nThere is quiet a discrepancy between \nthe reasoning provided by the \nhuman analysts and the LLMs. \nWhile both human analysts focused \non the ease of use and capability, \nrelating it to ‘Benevolence’, \n‘Helpful’, and ‘Achievement’, the \nLLMs varied significantly in their \ninterpretations. Bard leaned toward \n‘Hedonism’ and the enjoyment \naspect of the app, highlighting \nthe coolness factor. In contrast, \nChatGPT focused on ‘Universalism \nand Equality’, emphasizing the app’s \nwide-reaching\nBoth Human 1 and ChatGPT \nassociate the review with \n‘Hedonism’, relating the app’s \nproblematic functionality \nto a lack of pleasure derived \nfrom its use. However, \nwhere Human 1 also \nbrings in ‘Universalism and \nAchievement’ due to the \napp’s educational utility, \nHuman 2 and GPT-4 \ntouch on ‘Achievement and \nCapability’ and ‘Stimulation \nand Excitement’, respectively. \nBoth interpretations hint at \nthe disappointment faced by \nthe user\nThere is an agreement across \nboth human analysts and \nLLMs around the theme of \n‘Hedonism’ and pleasure \nderived from the product. \nBoth human analysts clearly \nidentified the reviewer’s \nsatisfaction with Alexa’s \nmusic capabilities and sound \nquality, associating it with \n‘Hedonism and Enjoying \nLife’ or ‘Hedonism and \nPleasure’. This sentiment \nwas echoed by both \nChatGPT and GPT-4, \nwho similarly classified the \nreview under ‘Hedonism and \nPleasure’. Bard’s\n9\nThe AI Ethics Journal | \nExample 1 Divergent \nCategorizations\nExample 2 Conceptual \nAgreement\nExample 3 Consistent \nCategorizations\ncapability and inclusivity. GPT-4’s \ninterpretation was rather unique, \nassociating the review with ‘Security \nand Safety’ - a perspective neither \nthe human analysts nor the other \nLLMs touched upon.\nThis divergence suggests a broader \ninterpretative range among the \nLLMs, especially when dealing with \nreviews that may contain multiple \nthemes or sentiments.\nwhen their expectations were \nnot met. Bard’s analysis, \nhowever, took a slightly \ndifferent angle, focusing \non the ‘Stimulation’ aspect \nbut associating it with the \ncompany’s failure to provide \nnovelty and excitement.\nWhile the underlying \nsentiment is consistent across \ninterpretations, the nuances \ncaptured by each entity offer a \nmultifaceted understanding of \nthe review.\ninterpretation, while still \ncentered around ‘Hedonism’, \nadded an element of \n‘Stimulation’, hinting at the \nexcitement derived from the \nproduct.\nThis example underscores \ninstances where clear, \npositive sentiments in \nreviews lead to consistent \ncategorizations across \ndifferent evaluative entities.\nTable 1. Comparison of three scenarios of Human vs AI agrément.\nThe variations between human interpretations \nunderscore the subjectivity inherent in understanding \nand classifying feedback. While humans bring in \npersonal biases, they also capture a depth and holistic \nunderstanding that’s unique to human cognition. While \nAI models demonstrate proficiency in interpreting and \nclassifying feedback, their understanding tends to be \nmore structured and might miss out on the nuanced \nor emotional aspects that humans naturally grasp. \nHowever, the consistency of AI models can be valuable, \nespecially when dealing with large datasets. On the \nother hand, human reviewers bring depth, context, and \na broader perspective.\nSome researchers assert that LLMs, given their current \ntechnological stature, are incapable of completely \ncomprehending the profound complexities of human \nemotions and experiences (Bender et al., 2021; Alkaissi \nand McFarlane, 2023; Rudolph, Tan, and Tan, 2023). \nConsequently, their use in qualitative analysis should \nbe treated with caution. The argument furthers that \nthe LLMs missing context sensitivity and focus on \nfunctional aspects could lead to flawed or incomplete \nconclusions. But the prompts developed by humans \nneed to provide a rich context in order to address this \nissue.\nContrastingly, advocates of AI-assisted qualitative \nanalysis propose that LLMs can furnish invaluable \ninsights and complementary viewpoints, aiding \nresearchers in achieving a more all-encompassing \nunderstanding of the data (Dwivedi et al., 2023). The \nresearchers in favour of LLMs further posit that with \nthe consistent evolution and enhancement of AI, a \nsynergistic approach combining human acumen and \nAI capabilities can lead to more robust analysis.\nThis ongoing discussion brings forth crucial questions \nfor qualitative researchers concerning the degree of \ntheir reliance on LLMs in their work. While LLMs \nhold the potential to transform qualitative research \nby delivering additional perspectives and insights, \nit is imperative for researchers to also acknowledge \ntheir limitations and maintain a keen awareness of the \nhumanistic elements inherent to qualitative research.\nTo answer our research question: How do the analytical \nreasoning abilities of LLMs compare to human comprehension in the \ncontext of qualitative research?\nAnswer: LLMs exhibit varied analytical reasoning \nabilities compared to human comprehension in the \ncontext of qualitative research. While some AI models \n(as in our experiment ChatGPT) may align more closely \n10\nThe AI Ethics Journal | \nwith certain human perspectives, there is inherent \ndiversity in interpretation among both humans and \nLLMs. Notably, even human-to-human comparisons \nshow discrepancies, suggesting that both LLMs and \nhumans possess subjective interpretation capabilities in \nqualitative analysis.\nDiscussion\nIn this section, we move deeper into the broader \nimplications of our findings. By situating our results \nwithin a wider context and comparison with existing \nresearch ideas, we aim to shed light on the overarching \nsignificance and potential impact these insights might \nhave on the evolution of qualitative research.\nAI and Humans\nDespite the considerable potential of LLMs in \nqualitative research, the indispensable role of the human \nresearcher for verifying the validity and reliability of \nthe results remains critical. LLMs, while robust and \nefficient, exhibit limitations in their understanding of \ncomplex human experiences, contexts, and semantics, \noccasionally leading to the generation of inaccurate \nor invented information, a phenomenon known \nas ‘hallucinations’ (Rudolph, Tan, and Tan, 2023; \nAlkaissi and McFarlane, 2023). These hallucinations \ncan misdirect the interpretation of research results, \ncompromise validity, and introduce unintentional \nbias or error. Therefore, the human researchers’ \ninvolvement becomes vital in scrutinizing, verifying, \nand interpreting the results generated by LLMs, \nensuring that the outcomes are consistent with the actual \ncontext and preserving the integrity of the research. \nFurthermore, the human researcher’s expertise and \ncritical thinking are required to continually improve \ntheir comprehension over time, helping in enhancing \ntheir capabilities while minimizing potential drawbacks.\nLLMs are poised to redefine the interplay between AI \nand human involvement in the research process. When \nit comes to inductive reasoning and open-ended data \ncollection, LLMs are capable of deriving insights from \nunstructured data without predetermined hypotheses \nand continuously collecting and analyzing massive \namounts of data from diverse sources. However, while \nthese capabilities can expedite the research process, the \nquestion remains whether LLMs can truly replicate the \nintuitive reasoning processes and interpretive nuances \ninherent to human researchers. Similarly, while LLMs \ncan process large amounts of qualitative data collected \nin naturalistic settings, the nuanced understanding, \ncultural sensitivity, and context-awareness that human \nresearchers bring to these settings are unlikely replicable \nby LLMs in their current state.\nPolanyi’s concept of ‘tacit knowledge’ (Collins, 2005) \nwhich is also the ‘implicit’ component of Nonaka’s \nSECI model (Li and Gao, 2003) underscores the \nunique human ability to perform certain tasks in \nunexpected and inexplicable ways. This inherent \ncapability, however, may not be explicitly replicated \nor comprehended by LLMs, due to the unpredictable \nnature of such knowledge that is often grounded in \npersonal experience and intuition.\nA further manifestation of Human-LLM collaborative \nresearch could involve delineating distinct roles for \neach entity to optimize the research process. Here, \nLLMs could function as ‘inter-rater reliability testers’ \n(Armstrong et al., 1997), contributing to the research \nconducted by human analysts, while the human \nparticipants would be responsible for the verification \nof the information and analytical results generated by \nthe LLMs. This iterative process, involving reciprocal \nroles, has the potential to yield more robust and \nefficient research outcomes, underscoring the mutual \nenrichment of human insight and machine efficiency.\nStochastic Parrots for Qualitative Research\nLLMs have demonstrated remarkable capacity to \ngenerate human-like text, understand context, and \n11\nThe AI Ethics Journal | \ninteract dynamically with users. Their potential, \nhowever, should not overshadow the challenges \nthey pose, especially concerning the interpretation \nof meanings in qualitative research. A significant \nadvantage of these models lies in their ability to process \nand analyze vast amounts of data quickly and relatively \naccurately, providing a broad view of patterns and \ntrends that could otherwise be missed.\nNonetheless, Bender et al. (2021) present cogent \narguments about the risks associated with these models, \nprimarily centered around their training on massive \nand diverse text datasets. This training can result in \nthe replication and amplification of biases present \nin the data, leading to potentially harmful outputs. \nAdditionally, the text generation process of LLMs \nremains fundamentally opaque, raising questions about \ntransparency and interpretability.\nDespite LLMs’ adeptness at generating linguistically \ncoherent responses, they do not genuinely comprehend \nthe meanings, nuances, and deeper implications of \nwords and phrases. While humans possess a holistic \nunderstanding of language, encompassing cultural, \nemotional, historical, and symbolic dimensions, LLMs \ncan only provide approximations based on learned \npatterns. They may miss out on the rich tapestry of \nmeanings a human researcher could decipher.\nTo mitigate the risks associated with LLMs, (Bender \net al., 2021) propose several steps, including (a) \nreducing model size, (b) increasing transparency, and \n(c) establishing ethical guidelines for their use. Smaller, \nmore controlled models could potentially minimize \nharm, while greater transparency could facilitate a \nbetter understanding of the mechanisms behind their \ntext generation. Ethical guidelines would also establish \na framework for responsible and equitable use of these \nmodels.\nThe determination of an appropriate size for LLMs, \na balance between the model’s complexity and its \npredictive accuracy, is best achieved through a \ncollaboration of machine learning experts, ethicists, and \ndomain-specific experts. As for the selection of ethical \nguidelines governing LLMs use should be context-\ndependent and reflective of the values and perspectives \nof a diverse range of stakeholders (Zowghi and da \nRimini, 2023). This selection process necessitates an \ninclusive approach, possibly involving a blend of \nestablished ethical frameworks tailored to the specifics \nof the AI system and its deployment (Sanderson et al., \n2023). The ethical guidelines established by a diverse \nand inclusive committee of stakeholders need to be \nperiodically reviewed and updated to align with evolving \nsocietal norms and technological advancements.\nThe use of LLMs in qualitative research also \nintroduces a new set of ethical considerations. \nConcerns around privacy, data misuse, and the risk \nof perpetuating existing biases in the data they are \ntrained on are prevalent. Additionally, the advent of \nLLMs in the academic sphere raises questions about \nintellectual property rights and authorship. In this \nchanging landscape, the role of the human researcher \nmay shift towards orchestrating the research process, \nensuring ethical compliance, and interpreting and \ncontextualizing the findings generated by LLMs. As \ntechnology continues to advance, the importance of \ncritical reflection on these shifts and their implications \nwill grow.\nEvolution of Qualitative Research\nLLMs have the potential to significantly impact data \nanalysis in qualitative research, as they can speed up \nthe process and handle larger datasets than humans \ncan feasibly manage. For example, Byun, Vasicek, \nand Seppi (2023) demonstrated that AI is capable of \nconducting qualitative analysis and generating nuanced \nresults. However, such studies often do not delve into \nthe reasoning behind AI vs. human interpretation, \nwhich could significantly impact the findings. In \naddition, there is the question of whether LLMs can \ntruly understand and articulate the symbolic and \n12\nThe AI Ethics Journal | \ncultural nuances that underpin human behavior, \nelements that are paramount to the work of prominent \nanthropologists and sociologists, such as Malinowski’s \nparticipatory observation (Malinowski, 1929), Weber’s \nconcept of verstehen, or empathetic understanding \n(Weber, 1949), and Geertz’s interpretation of culture \n(Geertz, 1973).\nThe application of LLMs could potentially enhance \nthe efficiency of established qualitative methodologies \nsuch as Grounded Theory (Charmaz, 2014; Glaser, \nStrauss, and Strutzel, 1968), Interpretive Interactionism \n(Denzin 2001), and Narrative Analysis (Franzosi, \n1998), particularly in terms of initial data analysis. \nHowever, these methodologies were developed with \nthe understanding that the researcher’s empathy, \ninterpretation, and contextual understanding are \nintegral to the process. As such, it is unlikely that the \nessential humanistic aspects of these approaches can be \nfully replaced by LLMs, indicating a shift rather than \nan absolute transformation in these methodologies \n(Dwivedi et al., 2023).\nAI Doomsday\nThe escalating discourse on the potential risks of AI \nand LLMs, amplified by recent media reports (Figure \n3), is leading to a growing unease among various \nprofessional communities. They are coming to terms \nwith the stark reality that AI might soon eclipse their \nroles and replace them in their jobs. This existential \ndread has been underscored by developments such as \nthe AI Doomsday Clock 4 inching closer to midnight, \nsymbolizing the perceived imminent danger of a \ncatastrophic AI disaster.\nFindings from our exploratory experiment, coupled \nwith an overview of existing research and an \nunderstanding of capabilities of LLMs, do not support a \ndoomsday scenario for qualitative researchers. Contrary \n4  https:/ /www.vox.com/22893594/doomsday-clock-\nnuclear-war-climate-change-risk\nto pervasive fears, the reality we’ve discerned suggests \na future where human researchers and LLMs can \ncoexist and contribute complementarily to the field of \nqualitative research.\nFigure 3. Media amplification of AI Doomsday fears\nLimitations\nWhile our study provides valuable insights into the \nutilization of LLMs in qualitative research, these \nfindings are inevitably influenced by our own areas \nof expertise and the specific experimental design we \nemployed. The research is also constrained by two \nprimary limitations. Firstly, the sample size we chose \nfor the study, although increasing the sample size could \nhave altered the statistical outcomes. However, our \nprimary interest lay not in large-scale data analysis, but \nin exploring the reasoning patterns of human analysts \nand LLMs during the classification of app reviews.\nConclusion and Future Work\nThe insights obtained from our experiment underscore \nthe significance of careful considerations regarding \nthe use of AI models play in qualitative research. \nThe modest alignment between human and AI \nclassifications, coupled with the comparatively higher \nconcordance between the AI models, illuminates the \ncomplex dynamics at play when incorporating AI into \nqualitative analysis. Our findings accentuate that, \ndespite the promise of AI for augmenting analysis, \nthe unique human touch—an element intrinsic to \nqualitative research—cannot be disregarded. This \n13\nThe AI Ethics Journal | \nessential human element, embedded in understanding \nand interpreting context, remains a critical factor \n(for now) in maintaining the richness and depth of \nqualitative investigations.\nThe considerable variations highlighted between \nhuman and AI comprehension in this study encourage \nfurther exploration in the field of AI integration into \nqualitative research. Future work could delve deeper \ninto understanding the basis for such disparities, \nthereby refining the synergistic interplay between AI \nand human analysis. Furthermore, investigating how to \nleverage the different perspectives offered by AI, while \nkeeping the human touch intact, could lead to more \ncomprehensive and nuanced insights.\nLastly, addressing the ethical implications of AI usage \nin qualitative research, especially considering AI’s \nlimitations and potential for biases, will form a critical \npart of future studies. As we venture further into this \nnew era of AI-assisted research, it is imperative to \nnavigate these challenges to harness the full potential \nof this technological advancement in a responsible and \nethical manner.\nReferences\nAkmal, Haider, and Paul Coulton. (2020). “The divination \nof things by things.” In Extended Abstracts of the 2020 CHI \nConference on Human Factors in Computing Systems, 1-12.\nAlkaissi, Hussam, and Samy I McFarlane. (2023). ‘Artificial \nhallucinations in ChatGPT: implications in scientific \nwriting’, Cureus , 15.\nAmerika, Mark, Laura Hyunjhee Kim, and Brad Gallagher. \n(2020). “Fatal error: artificial creative intelligence (ACI).” \nIn Extended Abstracts of the 2020 CHI Conference on Human \nFactors in Computing Systems, 1-10.\nArmstrong, David, Ann Gosling, John Weinman, and \nTheresa Marteau. (1997). ‘The place of inter-rater reliability \nin qualitative research: An empirical study’, Sociolog y , 31: \n597-606.\nAspers, Patrik, and Ugo Corte. (2019). ‘What is qualitative \nin qualitative research’, Qualitative sociolog y , 42: 139-60.\nBender, Emily M, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. (2021). “On the \nDangers of Stochastic Parrots: Can Language Models \nBe Too Big??” In Proceedings of the 2021 ACM conference on \nfairness, accountability, and transparency , 610-23.\nBertrand, Astrid, Rafik Belloum, James R Eagan, and \nWinston Maxwell. (2022). “How cognitive biases affect \nXAI-assisted decision-making: A systematic review.” In \nProceedings of the 2022 AAAI/ACM conference on AI, ethics, and \nsociety, 78-91.\nBurger, Bastian, Dominik K Kanbach, Sascha Kraus, \nMatthias Breier, and Vincenzo Corvello. (2023). ‘On \nthe use of AI-based tools like ChatGPT to support \nmanagement research’, European Journal of Innovation \nManagement, 26: 233-41.\nByun, Courtni, Piper Vasicek, and Kevin Seppi. (2023). \n“Dispensing with Humans in Human-Computer \nInteraction Research.” In Extended Abstracts of the 2023 CHI \nConference on Human Factors in Computing Systems, 1-26.\nCharmaz, Kathy. (2014). Constructing grounded theory (sage).\nChui, Michael, James Manyika, and Mehdi Miremadi. \n(2016). ‘Where machines could replace humans-and where \nthey can’t (yet)’.\nCollins, Harry M. (2005). ‘What is tacit knowledge?’ In The \npractice turn in contemporary theory  (Routledge).\nCuthbertson, Anthony. (2023). ‘Why tech bosses are \ndoomsday prepping’. https:/ /www.independent.co.uk/tech/\nchatgpt-ai-chatbot-microsoft-altman-b2274639.html.\nDenzin, Norman K. (2001). Interpretive interactionism (Sage).\nDergaa, Ismail, Karim Chamari, Piotr Zmijewski, and \nHelmi Ben Saad. (2023). ‘From human writing to artificial \nintelligence generated text: examining the prospects and \npotential threats of ChatGPT in academic writing’, Biolog y \nof Sport, 40: 615-22.\nDwivedi, Yogesh K, Nir Kshetri, Laurie Hughes, Emma \n14\nThe AI Ethics Journal | \nLouise Slade, Anand Jeyaraj, Arpan Kumar Kar, Abdullah \nM Baabdullah, Alex Koohang, Vishnupriya Raghavan, \nand Manju Ahuja. (2023). ‘“So what if ChatGPT wrote \nit?” Multidisciplinary perspectives on opportunities, \nchallenges and implications of generative conversational \nAI for research, practice and policy’, International Journal of \nInformation Management, 71: 102642.\nFranzosi, Roberto. (1998). ‘Narrative analysis—or why (and \nhow) sociologists should be interested in narrative’, Annual \nreview of sociolog y, 24: 517-54.\nGeertz, Clifford. (1973). ‘Thick Description: Toward an \ninterpretive theory of culture’, The interpretation of cultures: \nSelected essays: 3-30.\nGilson, Aidan, Conrad W Safranek, Thomas Huang, \nVimig Socrates, Ling Chi, Richard Andrew Taylor, and \nDavid Chartash. (2023). ‘How does CHATGPT perform \non the United States Medical Licensing Examination? \nthe implications of large language models for medical \neducation and knowledge assessment’, JMIR Medical \nEducation, 9: e45312.\nGlaser, Barney G, Anselm L Strauss, and Elizabeth \nStrutzel. (1968). ‘The discovery of grounded theory; \nstrategies for qualitative research’, Nursing research , 17: 364.\nGriffiths, Max. (2022). ‘Is LaMDA sentient?’, AI & \nSOCIETY: 1-2.\nHennink, Monique, Inge Hutter, and Ajay Bailey. (2020). \nQualitative research methods (Sage).\nJames Manyika, Jake Silberg, Brittany Presten. (2019). \n‘What Do We Do About the Biases in AI?’. https:/ /hbr.\norg/2019/10/what-do-we-do-about-the-biases-in-ai.\nJeon, Jaeho, and Seongyong Lee. (2023). ‘Large language \nmodels in education: A focus on the complementary \nrelationship between human teachers and ChatGPT’, \nEducation and Information Technologies : 1-20.\nKliegr, Tomáš, Štěpán Bahník, and Johannes Fürnkranz. \n(2021). ‘A review of possible effects of cognitive biases \non interpretation of rule-based machine learning models’, \nArtificial Intelligence , 295: 103458.\nKooli, Chokri. (2023). ‘Chatbots in education and research: \na critical examination of ethical implications and solutions’, \nSustainability, 15: 5614.\nLi, Meng, and Fei Gao. (2003). ‘Why Nonaka highlights \ntacit knowledge: a critical review’, Journal of knowledge \nmanagement.\nMalinowski, Bronislaw. (1929). ‘Practical anthropology’, \nAfrica, 2: 22-38.\nMichel, Jan G. (2020). ‘Could Machines Replace Human \nScientists?: Digitalization and Scientific Discoveries.’ in, \nArtificial Intelligence (Brill mentis).\nPanda, Subhajit, and Navkiran Kaur. (2023). ‘Exploring \nthe viability of ChatGPT as an alternative to traditional \nchatbot systems in library and information centers’, Library \nHi Tech News, 40: 22- 25.\nPrahl, Andrew, and Lyn M Van Swol. (2021). ‘Out with \nthe humans, in with the machines?: Investigating the \nbehavioral and psychological effects of replacing human \nadvisors with a machine’, Human- Machine Communication , 2: \n209-34.\nQasem, Fawaz. (2023). ‘ChatGPT in scientific and \nacademic research: future fears and reassurances’, Library \nHi Tech News, 40: 30-32.\nRudolph, Jürgen, Samson Tan, and Shannon Tan. (2023). \n‘ChatGPT: Bullshit spewer or the end of traditional \nassessments in higher education?’, Journal of Applied Learning \nand Teaching, 6.\nSallam, Malik. (2023). “ChatGPT utility in healthcare \neducation, research, and practice: systematic review on the \npromising perspectives and valid concerns.” In Healthcare , \n887. MDPI.\nSanderson, Conrad, David Douglas, Qinghua Lu, Emma \nSchleiger, Jon Whittle, Justine Lacey, Glenn Newnham, \nStefan Hajkowicz, Cathy Robinson, and David Hansen. \n(2023). ‘AI ethics principles in practice: Perspectives of \ndesigners and developers’, IEEE Transactions on Technolog y \nand Society.\n15\nThe AI Ethics Journal | \nSchwartz, Shalom H. (2012). ‘An overview of the Schwartz \ntheory of basic values’, Online readings in Psycholog y and \nCulture, 2: 2307-0919.1116.\nSeymour, William, and Max Van Kleek. (2020). “Does \nSiri have a soul? Exploring voice assistants through shinto \ndesign fictions.” In Extended Abstracts of the 2020 CHI \nConference on Human Factors in Computing Systems, 1-12.\nShams, Rifat Ara, Mojtaba Shahin, Gillian Oliver, \nJon Whittle, Waqar Hussain, Harsha Perera, and Arif \nNurwidyantoro. (2021). ‘Human values in mobile \napp development: An empirical study on bangladeshi \nagriculture mobile apps’, arXiv preprint arXiv:2110.05150.\nShams, Rifat, Muneera Bano, Didar Zowghi, Qinghua \nLu, and Jon Whittle. (2023). “Exploring Human Values \nin AI Systems: Empirical Analysis of Amazon Alexa.” In \nEmpirical Requirements Engineering Workshop (EmpiRE’23) \nat International Requirements Engineering Conference RE’23 . \nHanover, Germany: IEEE.\nTafferner, Zoltán, Balázs Illés, Olivér Krammer, and \nAttila Géczy. (2023). ‘Can ChatGPT Help in Electronics \nResearch and Development? A Case Study with Applied \nSensors’, Sensors, 23: 4879.\nThoppilan, Romal, Daniel De Freitas, Jamie Hall, Noam \nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia \nJin, Taylor Bos, Leslie Baker, and Yu Du. 2022. ‘Lamda: \nLanguage models for dialog applications’, arXiv preprint \narXiv:2201.08239.\nTlili, Ahmed, Boulus Shehata, Michael Agyemang \nAdarkwah, Aras Bozkurt, Daniel T Hickey, Ronghuai \nHuang, and Brighter Agyemang. (2023). ‘What if the devil \nis my guardian angel: ChatGPT as a case study of using \nchatbots in education’, Smart Learning Environments , 10: 15.\nVan Dis, Eva AM, Johan Bollen, Willem Zuidema, Robert \nvan Rooij, and Claudi L Bockting. (2023). ‘ChatGPT: five \npriorities for research’, Nature, 614: 224-26.\nWardat, Yousef, Mohammad A Tashtoush, Rommel AlAli, \nand Adeeb M Jarrah. (2023). ‘ChatGPT: A revolutionary \ntool for teaching and learning mathematics’, Eurasia Journal \nof Mathematics, Science and Technolog y Education, 19: em2286.\nWeber, Max. (1949). ‘” Objectivity” in social science and \nsocial policy’, The methodolog y of the social sciences : 49-112.\nWhite, Jules, Quchen Fu, Sam Hays, Michael Sandborn, \nCarlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse \nSpencer-Smith, and Douglas C Schmidt. (2023). ‘A prompt \npattern catalog to enhance prompt engineering with \nchatgpt’, arXiv preprint arXiv:2302.11382.\nYan, Da. (2023). ‘Impact of ChatGPT on learners in a L2 \nwriting practicum: An exploratory investigation’, Education \nand Information Technologies: 1-25.\nZowghi, Didar, and Francesca da Rimini. (2023). ‘Diversity \nand Inclusion in Artificial Intelligence’, arXiv preprint \narXiv:2305.12728.",
  "topic": "Qualitative reasoning",
  "concepts": [
    {
      "name": "Qualitative reasoning",
      "score": 0.583898663520813
    },
    {
      "name": "Cognitive science",
      "score": 0.47359731793403625
    },
    {
      "name": "Computer science",
      "score": 0.42728400230407715
    },
    {
      "name": "Psychology",
      "score": 0.39543622732162476
    },
    {
      "name": "Linguistics",
      "score": 0.38405677676200867
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2827354669570923
    },
    {
      "name": "Philosophy",
      "score": 0.12165740132331848
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1292875679",
      "name": "Commonwealth Scientific and Industrial Research Organisation",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I42894916",
      "name": "Data61",
      "country": "AU"
    }
  ]
}