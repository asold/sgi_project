{
  "title": "N-LTP: A Open-source Neural Chinese Language Technology Platform with Pretrained Models.",
  "url": "https://openalex.org/W3089006046",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2096786427",
      "name": "Wanxiang Che",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134402094",
      "name": "Yun-Long Feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133020571",
      "name": "Libo Qin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098738246",
      "name": "Ting Liu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1773803948",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970509139",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W2970557265",
    "https://openalex.org/W2140721185",
    "https://openalex.org/W2916139646",
    "https://openalex.org/W2052449326",
    "https://openalex.org/W2907630459",
    "https://openalex.org/W2963453233",
    "https://openalex.org/W2922018002",
    "https://openalex.org/W2250542158",
    "https://openalex.org/W1983599491",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W2610179052",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2807725036",
    "https://openalex.org/W2130903752",
    "https://openalex.org/W3098065087",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W2741029840",
    "https://openalex.org/W25062297",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W85855394"
  ],
  "abstract": "We introduce N-LTP, an open-source neural language technology platform supporting six fundamental Chinese NLP tasks: lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition), {syntactic parsing} (dependency parsing), and {semantic parsing} (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks. In addition, knowledge distillation where the single-task model teaches the multi-task model is further introduced to encourage the multi-task model to surpass its single-task teacher. Finally, we provide a collection of easy-to-use APIs and a visualization tool to make users easier to use and view the processing results directly. To the best of our knowledge, this is the first toolkit to support six Chinese NLP fundamental tasks. Source code, documentation, and pre-trained models are available at \\url{this https URL}.",
  "full_text": "N-LTP: An Open-source Neural Language Technology Platform\nfor Chinese\nWanxiang Che, Yunlong Feng, Libo Qin, Ting Liu\nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology, China\n{car,ylfeng,lbqin,tliu}@ir.hit.edu.cn\nAbstract\nWe introduce N-LTP, an open-source neu-\nral language technology platform supporting\nsix fundamental Chinese NLP tasks: lexical\nanalysis (Chinese word segmentation, part-\nof-speech tagging, and named entity recog-\nnition), syntactic parsing (dependency pars-\ning), and semantic parsing (semantic depen-\ndency parsing and semantic role labeling).\nUnlike the existing state-of-the-art toolkits,\nsuch as Stanza, that adopt an independent\nmodel for each task, N-LTP adopts the multi-\ntask framework by using a shared pre-trained\nmodel, which has the advantage of captur-\ning the shared knowledge across relevant Chi-\nnese tasks. In addition, a knowledge dis-\ntillation method (Clark et al., 2019) where\nthe single-task model teaches the multi-task\nmodel is further introduced to encourage the\nmulti-task model to surpass its single-task\nteacher. Finally, we provide a collection of\neasy-to-use APIs and a visualization tool to\nmake users to use and view the processing re-\nsults more easily and directly. To the best\nof our knowledge, this is the Ô¨Årst toolkit to\nsupport six Chinese NLP fundamental tasks.\nSource code, documentation, and pre-trained\nmodels are available at https://github.\ncom/HIT-SCIR/ltp.\n1 Introduction\nThere is a wide of range of existing natu-\nral language processing (NLP) toolkits such as\nCoreNLP (Manning et al., 2014), UDPipe (Straka\nand Strakov√°, 2017), FLAIR (Akbik et al., 2019),\nspaCy,1 and Stanza (Qi et al., 2020) in English,\nwhich makes it easier for users to build tools with\nsophisticated linguistic processing. Recently, the\nneed for Chinese NLP has a dramatic increase in\nmany downstream applications. A Chinese NLP\nplatform usually includes lexical analysis (Chinese\n1https://spacy.io\nResultAnalysis\nEasy-to-useAPI\nVisualizationTool\nOutputSentenceSplitWordSegmentationPart-of-SpeechTaggingNamed Entity Recognition\nToolkit\nlexical analysis\nDependency Parsingsyntactic parsing \nSemantic Dependency Parsingsemantic parsing\npreprocess\nCorpus\nInput Processor\nSemantic Role Labeling\nFigure 1: WorkÔ¨Çow of the N-LTP. N-LTP takes the\nChinese corpus as input and output the analysis results\nincluding lexical analysis, syntactic parsing, and se-\nmantic parsing. In addition, we provide the visualiza-\ntion tool and easy-to-use API to help users easily use\nN-LTP.\nword segmentation (CWS), part-of-speech (POS)\ntagging, and named entity recognition (NER)), syn-\ntactic parsing (dependency parsing (DEP)), and\nsemantic parsing (semantic dependency parsing\n(SDP) and semantic role labeling (SRL)). Unfortu-\nnately, there are relatively fewer high-performance\nand high-efÔ¨Åciency toolkits for Chinese NLP tasks.\nTo Ô¨Åll this gap, it‚Äôs important to build a Chinese\nNLP toolkit to support rich Chinese fundamen-\ntal NLP tasks, and make researchers process NLP\ntasks in Chinese quickly.\nRecently, Qi et al. (2020) introduce the Python\nNLP toolkit Stanza for multi-lingual languages,\nincluding Chinese language. Though Stanza can\nbe directly applied for processing the Chinese texts,\nit suffers from several limitations. First, it only\nsupports part of Chinese NLP tasks. For example, it\nfails to handle semantic parsing analysis, resulting\nin incomplete analysis in Chinese NLP. Second, it\ntrained each task separately, ignoring the shared\nknowledge across the related tasks, which has been\nproven effective for Chinese NLP tasks (Qian et al.,\narXiv:2009.11616v4  [cs.CL]  23 Sep 2021\nSystem Programming Language Fully Neural State-of-the-art\nPerformance\nRich Chinese\nFundamental Tasks\nMulti-task\nLearning\nLTP (Che et al., 2010) C++ ‚àö\nUDPipe (Straka and Strakov√°, 2017) C++ ‚àö\nFLAIR (Akbik et al., 2019) Python ‚àö ‚àö\nStanza (Qi et al., 2020) Python ‚àö ‚àö\nN-LTP Python ‚àö ‚àö ‚àö ‚àö\nTable 1: Feature comparisons of N-LTP against other popular natural language processing toolkits.\n2015; Hsieh et al., 2017; Chang et al., 2018). Third,\nindependent modeling method will occupy more\nmemory with the increase of the number of tasks,\nwhich makes it hard to deploy for mobile devices\nin real-word scenario.\nTo address the aforementioned issues, we intro-\nduce N-LTP, a PyTorch-based neural natural lan-\nguage processing toolkit for Chinese NLP, which\nwas built on the SOTA pre-trained model. As\nshown in Figure 1, given Chinese corpus as input,\nN-LTP produces comprehensive analysis results,\nincluding lexical analysis, syntactic parsing, and\nsemantic parsing. In addition, N-LTP provides\neasy-to-use APIs and visualization tool, which is\nuser-friendly.\nAs shown in Table 1, compared to the existing\nwidely-used NLP toolkits, N-LTP has the follow-\ning advantages:\n‚Ä¢ Comprehensive Tasks.N-LTP supports rich\nChinese fundamental NLP tasks including\nlexical analysis (word segmentation, part-of-\nspeech tagging, named entity recognition),\nsyntactic parsing, and semantic parsing (se-\nmantic dependency parsing, semantic role la-\nbeling). To the best of our knowledge, this is\nthe Ô¨Årst neural Chinese toolkit that support six\nChinese fundamental NLP tasks.\n‚Ä¢ Multi-Task Learning. The existing NLP\ntoolkits for the Chinese language all adopt\nindependent models for each task, which ig-\nnore the shared knowledge across tasks.\nTo alleviate this issue, we propose to use the\nmulti-task framework (Collobert et al., 2011)\nto take advantage of the shared knowledge\nacross all tasks. Meanwhile, multi-task learn-\ning with a shared encoder for all six tasks can\ngreatly reduce the occupied memory and im-\nprove the speed, which makes N-LTP more\nefÔ¨Åcient, reducing the need for hardware.\nIn addition, to enable the multi-task learning\nto enhance each subtask performance, we fol-\nlow Clark et al. (2019) to adopt the distillation\nmethod single-task models teach a multi-task\nmodel, helping the multi-task model surpass\nits all single-task teachers.\n‚Ä¢ Extensibility. N-LTP works with users‚Äô cus-\ntom modules. Users can easily add a new\npre-trained model with a conÔ¨Åguration Ô¨Åle, in\nwhich users can change the pretrained model\nto any BERT-like model supported by Hug-\ngingFace Transformers (Wolf et al., 2019) eas-\nily by changing the conÔ¨Åg. We have made all\ntask training conÔ¨Åguration Ô¨Åles open-sourced.\n‚Ä¢ Easy-to-use API and Visualization Tool.\nN-LTP provides a collection of fundamental\nAPIs, which is convenient for users to use the\ntoolkit without the need for any knowledge.\nWe also provide a visualization tool, which\nenables users to view the processing results\ndirectly. In addition, N-LTP has bindings for\nmany programming languages (C++, Python,\nJava, Rust, etc.).\n‚Ä¢ State-of-the-art Performance.We evaluate\nN-LTP on a total of six Chinese NLP tasks,\nand Ô¨Ånd that it achieves state-of-the-art or\ncompetitive performance at each task.\nN-LTP is fully open-sourced and can support six\nChinese fundamental NLP tasks. We hope N-LTP\ncan facilitate Chinese NLP research.\n2 Design and Architecture\nFigure 2 shows an overview of the main architec-\nture of N-LTP. It mainly consists of the compo-\nnents including a shared encoder and different de-\ncoders for each task. Our framework shares one\nencoder for leveraging the shared knowledge across\nall tasks. Different task decoders are used for each\ntask separately. All tasks are optimized simulta-\nneously via a joint learning scheme. In addition,\nthe knowledge distillation technique is introduced\nto encourage the multi-task model to surpass its\nsingle-task teacher model.\nELECTRA\nCWS\nPOSNERDEPSRLSDP\ns! s\" ‚Ä¶ s#[CLS] [SEP]\n‚Ñé! ‚Ñé\" ‚Ñé$‚Ñé[&'(] ‚Ñé[(*+]\nOther Tasks\nCWS\nShared Encoder\nInput Sentences\n‚Ä¶\nFigure 2: The architecture of the proposed model.\n2.1 Shared Encoder\nMulti-task framework uses a shared encoder to ex-\ntract the shared knowledge across related tasks,\nwhich has obtained remarkable success on various\nNLP tasks (Qin et al., 2019; Wang et al., 2020;\nZhou et al., 2021). Inspired by this, we adopt\nthe SOTA pre-trained model (ELECTRA) (Clark\net al., 2020) as the shared encoder to capture shared\nknowledge across six Chinese tasks.\nGiven an input utterance s = ( s1,s2,...,s n),\nwe Ô¨Årst construct the input sequence by adding\nspeciÔ¨Åc tokens s= ([CLS],s1,s2,...,s n,[SEP]),\nwhere [CLS] is the special symbol for represent-\ning the whole sequence, and [SEP] is the spe-\ncial symbol to separate non-consecutive token se-\nquences (Devlin et al., 2019). ELECTRA takes\nthe constructed input and output the corresponding\nhidden representations of sequence H = (h[CLS],\nh1,h2,..., hn, h[SEP]).\n2.2 Chinese Word Segmentation\nChinese word segmentation (CWS) is a prelim-\ninary and important task for Chinese natural lan-\nguage processing (NLP). InN-LTP, following Xue\n(2003), CWS is regarded as a character based se-\nquence labeling problem.\nSpeciÔ¨Åcally, given the hidden representations H\n= (h[CLS], h1,h2,..., hn, h[SEP]), we adopt a\nlinear decoder to classify each character:\nyi = Softmax(WCWShi + bCWS), (1)\nwhere yi denotes the label probability distribution\nof each character; WCWS and bCWS are trainable\nparameters.\n2.3 POS Tagging\nPart-of-speech (POS) tagging is another fundamen-\ntal NLP task, which can facilitate the downstream\ntasks such as syntactic parsing. Following the dom-\ninant model in the literature (Ratnaparkhi, 1996;\nHuang et al., 2015), POS tagging can be treated as\na sequence labeling task.\nSimilar to CWS, we take the sequence of hidden\nrepresentations H as input and output the corre-\nsponding POS sequence labels, which is formu-\nlated as:\nyi = Softmax(WPOShi + bPOS), (2)\nwhere yi denotes the POS label probability distri-\nbution of the i-th character; hi is the Ô¨Årst sub-token\nrepresentation of word si.\n2.4 Named Entity Recognition\nThe named entity recognition (NER) is the task\nof Ô¨Ånding the start and end of an entity (people,\nlocations, organizations, etc.) in a sen-\ntence and assigning a class for this entity.\nTraditional, NER is regarded as a sequence la-\nbeling task. After obtaining the hidden representa-\ntions H, we follow Yan et al. (2019a) to adopt the\nAdapted-Transformer to consider direction-\nand distance-aware characteristic, which can be\nformulated as:\nÀÜhi = AdaptedTransformer(hi), (3)\nwhere ÀÜH = (ÀÜh[CLS], ÀÜh1,ÀÜh2,..., ÀÜhn, ÀÜh[SEP]) are\nthe updated representations.\nFinally, similar to CWS and POS, we use a linear\ndecoder to classify label for each word:\nyi = Softmax(WNER ÀÜhi + bNER), (4)\nwhere yi denotes the NER label probability distri-\nbution of each character.\n2.5 Dependency Parsing\nDependency parsing is the task to analyze the se-\nmantic structure of a sentence. In N-LTP, we im-\nplement a deep biafÔ¨Åne neural dependency parser\n(Dozat and Manning, 2017) and einser algorithm\n(Eisner, 1996) to obtain the parsing result, which\nis formulated as:\nr(head)\ni = MLP(head)(hi)\nr(dep)\nj = MLP(dep)(hj)\n(5)\nAfter obtaining r(head)\ni and r(dep)\nj , we compute\nthe score for each dependency i‚Ü∂jby:\nyi‚Ü∂j = BiAfÔ¨Åne(rdep\ni ,rhead\nj ). (6)\nTask 1ModelTask 2Model\nTask kModel\nMulti-TaskModel\nTask 1LabelsTask 2Labels\nTask kLabels‚ãÆ ‚ãÆ1‚àíùúÜ ùúÜ\ndistill train\nFigure 3: We follow Clark et al. (2019) to adopt the dis-\ntillation method. This is an overview of the distillation\nmethod. Œª is increased linearly from 0 to 1 over the\ncurriculum of training.\nThe above process is also used for scoring a\nlabeled dependency i\nl\n‚Ü∂j, by extending the 1-dim\nvector s into Ldims, where Lis the total number\nof dependency labels.\n2.6 Semantic Dependency Parsing\nSimilar to dependency parsing, semantic depen-\ndency parsing (Che et al., 2012, SDP) is a task\nto capture the semantic structure of a sentence.\nSpeciÔ¨Åcally, given an input sentence, SDP aims at\ndetermining all the word pairs related to each other\nsemantically and assigning speciÔ¨Åc predeÔ¨Åned se-\nmantic relations. Following Dozat and Manning\n(2017), we adopt a biafÔ¨Åne module to perform the\ntask, using\npi‚Ü∂j = sigmoid(yi‚Ü∂j). (7)\nIf pi‚Ü∂j >0.5, wordi to wordj exists an edge.\n2.7 Semantic Role Labeling\nSemantic Role Labeling (SRL) is the task of de-\ntermining the latent predicate-argument structure\nof a sentence, which can provide representations\nto answer basic questions about sentence meaning,\nincluding who did what to whom, etc. We adopt\nan end-to-end SRL model by combining a deep\nbiafÔ¨Åne neural network and a conditional random\nÔ¨Åeld (CRF)-based decoder (Cai et al., 2018).\nThe biafÔ¨Åne module is similar to Section 2.5 and\nthe CRF layer can be formulated as:\nP(ÀÜy|s) =\n‚àë\nj=1 exp f(yi,j‚àí1,yi,j,s)‚àë\ny‚Ä≤\ni\n‚àë\nj=1 exp f(y‚Ä≤\ni,j‚àí1,y‚Ä≤\ni,j,s) (8)\nwhere ÀÜy represents an arbitrary label sequence\nwhen predicate is si, and f(yi,j‚àí1,yj,s) computes\nthe transition score from yi,j‚àí1 to yi,j.\nFigure 4: A minimal code snippet.\n2.8 Knowledge Distillation\nWhen there exist a large number of tasks, it‚Äôs dif-\nÔ¨Åcult to ensure that each task task beneÔ¨Åts from\nmulti-task learning (Clark et al., 2019).\nTherefore, we follow BAM (Clark et al., 2019)\nto use the knowledge distillation to alleviate this\nissue, which is shown Figure 3. First, we train each\ntask as the teacher model. Then, N-LTP learns\nfrom each trained single-task teacher model while\nlearning from the gold-standard labels simultane-\nously.\nFollowing BAM (Clark et al., 2019), we adopt\nteacher annering distillation algorithm. More\nspeciÔ¨Åcally, instead of simply shufÔ¨Çing the datasets\nfor our multi-task models, we follow the task sam-\npling procedure from Bowman et al. (2018), where\nthe probability of training on an example for a\nparticular task œÑ is proportional to |DœÑ|0.75. This\nensures that tasks with large datasets don‚Äôt overly\ndominate the training.\n3 Usage\nN-LTP is a PyTorch-based Chinese NLP toolkit\nbased on the above model. All the conÔ¨Ågurations\ncan be initialized from JSON Ô¨Åles, and thus it is\neasy for users to use N-LTP where users just need\none line of code to load the model or process the in-\nput sentences. SpeciÔ¨Åcally, N-LTP can be installed\neasily by the command:\n$ pip install ltp\nIn addition, N-LTP has bindings available for\nmany programming languages, including C++,\nPython, Java and RUST directly.\n3.1 Easy-to-use API\nWe provide rich easy-to-use APIs, which enables\nusers to easily use without the need for any knowl-\nedge. The following code snippet in Figure 4 shows\nChinese Word Part-of-Speech Named Entity Dependency Semantic Dependency Semantic Role\nModel Segmentation Tagging Recognition Parsing Parsing Labeling\nF F LAS F F F F\nStanza (Qi et al., 2020) 92.40 98.10 89.50 84.98 - -\nN-LTPtrained separately 98.55 98.35 95.41 90.12 74.47 79.23\nN-LTPtrained jointly with distillation99.18 98.69 95.97 90.19 76.62 79.49\nTable 2: Main Results. ‚Äú-\" represents the absence of tasks in the Stanza toolkit and we cannot report the results.\nFigure 5: LTP annotates a Chinese sentence ‚Äú‰ªñÂè´Ê±§ÂßÜ\nÂéªÊãøÂ§ñË°£„ÄÇ/ He told Tom to get his coat.‚Äù. The output\nis visualized by our visualization demo.\na minimal usage of N-LTP for downloading mod-\nels, annotating a sentence with customized models,\nand predicting all annotations.\n3.2 Visualization Tool\nIn addition, a visualization tool is proposed for\nusers to view the processing results directly. Specif-\nically, we build an interactive web demo that runs\nthe pipeline interactively, which is publicly avail-\nable at http://ltp.ai/demo.html. The vi-\nsualization tool is shown in Figure 5.\n4 Experiments\n4.1 Experimental Setting\nTo evaluate the efÔ¨Åciency of our multi-task model,\nwe conduct experiments on six Chinese tasks.\nThe N-LTP model is based on the Chinese\nELECTRA base (Cui et al., 2020). The learning\nratio (lr) for teacher models, student model and\nCRF layer is {1e‚àí4}, {1e‚àí4}, {1e‚àí3}, re-\nspectively. The gradient clip value adopted in our\nexperiment is 1.0 and the warmup proportion is\n0.02. We use BertAdam (Devlin et al., 2019) to\noptimize the parameters and adopted the suggested\nhyper-parameters for optimization.\n4.2 Results\nWe compare N-LTP with the state-of-the-art\ntoolkit Stanza. For a fair comparison, we\nconduct experiments on the same datasets that\nStanza adopted.\nThe results are shown in Table 2, we have the\nfollowing observations:\n‚Ä¢ N-LTP outperforms Stanza on four common\ntasks including CWS, POS, NER, and DEP by\na large margin, which shows the superiority\nof our proposed toolkit.\n‚Ä¢ The multi-task learning outperforms the\nmodel with independently trained. This is\nbecause that the multi-task framework can\nconsider the shared knowledge which can pro-\nmote each task compared with the indepen-\ndently training paradigm.\n4.3 Analysis\n4.3.1 Speedup and Memory Reduction\nIn this section, we perform the speed and memory\ntest on the Tesla V100-SXM2-16GB and all models\nwere speed-tested on the 10,000 sentences of the\nPeople‚Äôs Daily corpus with a batch size of 8. In\nall experiments, N-LTP performs six tasks (CWS,\nPOS, NER, DEP, SDP, SRL) whileStanza only\nconduct four tasks (CWS, POS, NER, DEP).\n210 corpus for training CWS task includes PKU, MSR,\nAS, CITYU, XU, CTB, UDC, CNC, WTB and ZX.\n3http://ir.hit.edu.cn/sdp2020ccl\nTask Model Dataset Metric State-of-the-art N-LTP N-LTP\nPerformance trained separately trained jointly\nCWS BERT (Huang et al., 2019) 10 Corpus 2 F1 97.10 97.42 97.50\nPOS Glyce+BERT (Meng et al., 2019) CTB9 F1 93.15 94.57 95.17\nNER ZEN (Diao et al., 2020) MSRA F1 95.25 94.95 95.78\nNER DGLSTM-CRF (Jie and Lu, 2019) OntoNotes F1 79.92 84.08 84.38\nSRL BiLSTM-Span (Ouchi et al., 2018) CONLL12 F1 75.75 78.20 81.65\nDEP Joint-Multi-BERT (Yan et al., 2019b) CTB9 F1LAS 81.71 81.69 84.03\nSDP SuPar 3 CCL20204 F1LAS 80.38 76.27 75.76\nTable 3: The results of N-LTP comparation to other state-of-the-art performance..\nStanza N-LTP (separately) N-LTP (jointly)0\n50\n100\n150\n200\n250\n300Sents/s\n56.16\n93.98\n242.03\nSpeed\n0\n500\n1000\n1500\n2000\n2500\n3000\nMB1227.70\n2471.60\n651.00\nMemory\nFigure 6: Speed and Memory test for N-LTP.\nSpeedup We compare the speed be-\ntween Stanza, N-LTP-separately and\nN-LTP-jointly and the results are shown\nin Figure 6. From the results of speed test, we\nhave two interesting observations: (1) N-LTP\ntrained separately achieves the x1.7\nspeedup compared with Stanza. We attribute\nthat N-LTP adopts the transformer as an encoder\nthat can be calculated in parallel while Stanza\nuses LSTM which can only process sentences\nword by word; (2) N-LTP trained jointly\nwith distillation obtains the x4.3 speedup\ncompared with separate modeling paradigm. This\nis because that our model utilizes the multi-task\nto perform all tasks while the independent models\ncan be only processed all tasks in a pipeline mode.\nMemory Reduction For memory test, we\nhave the following observation: (1) N-LTP\ntrained separately occupy more memory\nthan Stanza. This is because N-LTP per-\nforms six tasks while Stanza only conduct four\ntasks. (2) Though performing six tasks, N-LTP\ntrained jointly only requires half the mem-\nory compared to Stanza. We attribute it to the\nfact that the multi-task framework with a shared\nencoder can greatly reduce the running memory.\n4.3.2 Comparation with Other SOTA Single\nModels\nTo further verify the effectiveness of N-LTP, we\ncompare our framework with the existing state-of-\nthe-art single models on six Chinese fundamental\ntasks. In this comparison, we conduct experiments\non the same wildly-used dataset in each task for a\nfair comparison. In addition, we use BERT rather\nthan ELECTRA as the shared encoder, because the\nprior work adopts BERT.\nTable 3 shows the results, we observe that our\nframework obtains best performance on Ô¨Åve out\nof six tasks including CWS, POS, NER, SRL, and\nDEP, which demonstrates the effectiveness of our\nframework. On the SDP task, N-LTP underper-\nforms the best baseline. This is because many tricks\nare used in the prior model for SDP task and we\njust use the basic multi-task framework.\n5 Conclusion\nIn this paper, we presentedN-LTP, an open-source\nneural language technology platform supporting\nChinese. To the best of our knowledge, this is the\nÔ¨Årst Chinese toolkit that supports six fundamen-\ntal Chinese NLP tasks. Experimental results show\nN-LTP obtains state-of-the-art or competitive per-\nformance and has high speed.. We hope N-LTP\ncan facilitate Chinese NLP research.\nAcknowledgements\nWe thank the anonymous reviewers for their de-\ntailed and constructive comments. The Ô¨Årst three\nauthors contributed equally. Wanxiang Che is the\ncorresponding author. This work was supported\nby the National Key R&D Program of China via\ngrant 2020AAA0106501 and the National Natu-\nral Science Foundation of China (NSFC) via grant\n61976072 and 61772153. Libo is also supported\nby the Zhejiang Lab‚Äôs International Talent Fund for\nYoung Professionals.\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\nFLAIR: An easy-to-use framework for state-of-the-\nart NLP. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages\n54‚Äì59, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nSamuel R. Bowman, Ellie Pavlick, Edouard Grave,\nBenjamin Van Durme, Alex Wang, Jan Hula, Patrick\nXia, Raghavendra Pappagari, R. Thomas McCoy,\nRoma Patel, Najoung Kim, Ian Tenney, Yinghui\nHuang, Katherin Yu, Shuning Jin, and Berlin Chen.\n2018. Looking for elmo‚Äôs friends: Sentence-level\npretraining beyond language modeling. CoRR,\nabs/1812.10860.\nJiaxun Cai, Shexia He, Zuchao Li, and Hai Zhao. 2018.\nA full end-to-end semantic role labeler, syntactic-\nagnostic over syntactic-aware? In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 2753‚Äì2765, Santa Fe, New Mex-\nico, USA. Association for Computational Linguis-\ntics.\nYung-Chun Chang, Fang Yi Lee, and Chun Hung Chen.\n2018. A public opinion keyword vector for social\nsentiment analysis research. In 2018 Tenth Interna-\ntional Conference on Advanced Computational In-\ntelligence (ICACI), pages 752‚Äì757. IEEE.\nWanxiang Che, Zhenghua Li, and Ting Liu. 2010. LTP:\nA Chinese language technology platform. In Coling\n2010: Demonstrations, pages 13‚Äì16, Beijing, China.\nColing 2010 Organizing Committee.\nWanxiang Che, Meishan Zhang, Yanqiu Shao, and Ting\nLiu. 2012. SemEval-2012 task 5: Chinese semantic\ndependency parsing. In *SEM 2012: The First Joint\nConference on Lexical and Computational Seman-\ntics ‚Äì Volume 1: Proceedings of the main conference\nand the shared task, and Volume 2: Proceedings of\nthe Sixth International Workshop on Semantic Eval-\nuation (SemEval 2012), pages 378‚Äì384, Montr√©al,\nCanada. Association for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Urvashi Khandel-\nwal, Christopher D. Manning, and Quoc V . Le. 2019.\nBAM! born-again multi-task networks for natural\nlanguage understanding. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 5931‚Äì5937, Florence, Italy.\nAssociation for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nRonan Collobert, Jason Weston, L√©on Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost) from\nscratch. Journal of machine learning research,\n12(ARTICLE):2493‚Äì2537.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020. Revisiting pre-\ntrained models for Chinese natural language process-\ning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 657‚Äì668,\nOnline. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171‚Äì4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and\nYonggang Wang. 2020. ZEN: Pre-training Chinese\ntext encoder enhanced by n-gram representations. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 4729‚Äì4740, Online.\nAssociation for Computational Linguistics.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biafÔ¨Åne attention for neural dependency pars-\ning. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nJason M. Eisner. 1996. Three new probabilistic models\nfor dependency parsing: An exploration. In COL-\nING 1996 Volume 1: The 16th International Confer-\nence on Computational Linguistics.\nThomas Emerson. 2005. The second international Chi-\nnese word segmentation bakeoff. In Proceedings of\nthe Fourth SIGHAN Workshop on Chinese Language\nProcessing.\nJan HajiÀác and Dan Zeman, editors. 2017. Proceedings\nof the CoNLL 2017 Shared Task: Multilingual Pars-\ning from Raw Text to Universal Dependencies. As-\nsociation for Computational Linguistics, Vancouver,\nCanada.\nYu-Lun Hsieh, Yung-Chun Chang, Yi-Jie Huang, Shu-\nHao Yeh, Chun-Hung Chen, and Wen-Lian Hsu.\n2017. MONPA: Multi-objective named-entity and\npart-of-speech annotator for Chinese using recurrent\nneural network. In Proceedings of the Eighth In-\nternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 80‚Äì85,\nTaipei, Taiwan. Asian Federation of Natural Lan-\nguage Processing.\nWeipeng Huang, Xingyi Cheng, Kunlong Chen,\nTaifeng Wang, and Wei Chu. 2019. Toward fast\nand accurate neural chinese word segmentation with\nmulti-criteria learning. CoRR, abs/1903.04190.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-\nrectional LSTM-CRF models for sequence tagging.\nCoRR, abs/1508.01991.\nZhanming Jie and Wei Lu. 2019. Dependency-guided\nLSTM-CRF for named entity recognition. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3862‚Äì3872,\nHong Kong, China. Association for Computational\nLinguistics.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David McClosky.\n2014. The Stanford CoreNLP natural language pro-\ncessing toolkit. In Proceedings of 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations, pages 55‚Äì60, Bal-\ntimore, Maryland. Association for Computational\nLinguistics.\nYuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping Nie,\nFan Yin, Muyu Li, Qinghong Han, Xiaofei Sun, and\nJiwei Li. 2019. Glyce: Glyph-vectors for chinese\ncharacter representations. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 2742‚Äì2753.\nHiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto.\n2018. A span selection model for semantic role la-\nbeling. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1630‚Äì1642, Brussels, Belgium. Association\nfor Computational Linguistics.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D. Manning. 2020. Stanza: A\npython natural language processing toolkit for many\nhuman languages. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 101‚Äì\n108, Online. Association for Computational Linguis-\ntics.\nTao Qian, Yue Zhang, Meishan Zhang, Yafeng Ren,\nand Donghong Ji. 2015. A transition-based model\nfor joint segmentation, POS-tagging and normaliza-\ntion. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1837‚Äì1846, Lisbon, Portugal. Association for\nComputational Linguistics.\nLibo Qin, Wanxiang Che, Yangming Li, Haoyang Wen,\nand Ting Liu. 2019. A stack-propagation frame-\nwork with token-level intent detection for spoken\nlanguage understanding. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2078‚Äì2087, Hong Kong,\nChina. Association for Computational Linguistics.\nAdwait Ratnaparkhi. 1996. A maximum entropy\nmodel for part-of-speech tagging. In Conference\non Empirical Methods in Natural Language Process-\ning.\nMilan Straka and Jana Strakov√°. 2017. Tokenizing,\nPOS tagging, lemmatizing and parsing UD 2.0 with\nUDPipe. In Proceedings of the CoNLL 2017 Shared\nTask: Multilingual Parsing from Raw Text to Univer-\nsal Dependencies, pages 88‚Äì99, Vancouver, Canada.\nAssociation for Computational Linguistics.\nWilliam Yang Wang, Lingpeng Kong, Kathryn\nMazaitis, and William W. Cohen. 2014. Depen-\ndency parsing for Weibo: An efÔ¨Åcient probabilis-\ntic logic programming approach. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1152‚Äì\n1158, Doha, Qatar. Association for Computational\nLinguistics.\nYiren Wang, ChengXiang Zhai, and Hany Hassan.\n2020. Multi-task learning for multilingual neural\nmachine translation. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1022‚Äì1034, On-\nline. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R√©mi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019.\nHuggingface‚Äôs transformers: State-of-the-art natural\nlanguage processing. ArXiv, abs/1910.03771.\nNianwen Xue. 2003. Chinese word segmentation as\ncharacter tagging. In International Journal of Com-\nputational Linguistics & Chinese Language Process-\ning, Volume 8, Number 1, February 2003: Special Is-\nsue on Word Formation and Chinese Language Pro-\ncessing, pages 29‚Äì48.\nHang Yan, Bocao Deng, Xiaonan Li, and Xipeng Qiu.\n2019a. Tener: Adapting transformer encoder for\nnamed entity recognition.\nHang Yan, Xipeng Qiu, and Xuanjing Huang. 2019b.\nA uniÔ¨Åed model for joint chinese word segmentation\nand dependency parsing. CoRR, abs/1904.04697.\nMeishan Zhang, Yue Zhang, Wanxiang Che, and Ting\nLiu. 2014. Type-supervised domain adaptation for\njoint segmentation and POS-tagging. In Proceed-\nings of the 14th Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 588‚Äì597, Gothenburg, Sweden. Association\nfor Computational Linguistics.\nBaohang Zhou, Xiangrui Cai, Ying Zhang, and Xiaojie\nYuan. 2021. An end-to-end progressive multi-task\nlearning framework for medical named entity recog-\nnition and normalization. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 6214‚Äì6224, Online. As-\nsociation for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8789346218109131
    },
    {
      "name": "Natural language processing",
      "score": 0.7533376812934875
    },
    {
      "name": "Parsing",
      "score": 0.7256423234939575
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6405925750732422
    },
    {
      "name": "Task (project management)",
      "score": 0.6347085237503052
    },
    {
      "name": "Dependency grammar",
      "score": 0.5373296737670898
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 36
}