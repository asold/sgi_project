{
    "title": "Retrieval-Augmented Primitive Representations for Compositional Zero-Shot Learning",
    "url": "https://openalex.org/W4393154717",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2413593769",
            "name": "Chenchen Jing",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2087231410",
            "name": "Yukun Li",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A2096672852",
            "name": "Hao Chen",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2098201046",
            "name": "Chunhua Shen",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2413593769",
            "name": "Chenchen Jing",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2087231410",
            "name": "Yukun Li",
            "affiliations": [
                "Northwestern Polytechnical University"
            ]
        },
        {
            "id": "https://openalex.org/A2096672852",
            "name": "Hao Chen",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2098201046",
            "name": "Chunhua Shen",
            "affiliations": [
                "Zhejiang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4225016630",
        "https://openalex.org/W3037198159",
        "https://openalex.org/W4281956169",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2118373646",
        "https://openalex.org/W2950104027",
        "https://openalex.org/W1948251820",
        "https://openalex.org/W2997072136",
        "https://openalex.org/W4280644249",
        "https://openalex.org/W4319299836",
        "https://openalex.org/W6775639457",
        "https://openalex.org/W6790753860",
        "https://openalex.org/W4221160836",
        "https://openalex.org/W4309797799",
        "https://openalex.org/W6790536492",
        "https://openalex.org/W3164354101",
        "https://openalex.org/W6741412834",
        "https://openalex.org/W3128369410",
        "https://openalex.org/W4226464635",
        "https://openalex.org/W6691431627",
        "https://openalex.org/W3098690786",
        "https://openalex.org/W2945337239",
        "https://openalex.org/W4280598603",
        "https://openalex.org/W2010132303",
        "https://openalex.org/W4378770817",
        "https://openalex.org/W2496096353",
        "https://openalex.org/W4292974991",
        "https://openalex.org/W6673774836",
        "https://openalex.org/W4281722009",
        "https://openalex.org/W4386065780",
        "https://openalex.org/W2986385672",
        "https://openalex.org/W4378469662",
        "https://openalex.org/W4312920106",
        "https://openalex.org/W3035084814",
        "https://openalex.org/W4287186329",
        "https://openalex.org/W4312539810",
        "https://openalex.org/W3182605419",
        "https://openalex.org/W4312400874",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W2963518342",
        "https://openalex.org/W2093848332",
        "https://openalex.org/W4285787895",
        "https://openalex.org/W2736809457",
        "https://openalex.org/W4367860152",
        "https://openalex.org/W4361194175",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4304080190",
        "https://openalex.org/W4379539341",
        "https://openalex.org/W3127270383",
        "https://openalex.org/W2964138343",
        "https://openalex.org/W4386076688",
        "https://openalex.org/W2187089797"
    ],
    "abstract": "Compositional zero-shot learning (CZSL) aims to recognize unseen attribute-object compositions by learning from seen compositions. Composing the learned knowledge of seen primitives, i.e., attributes or objects, into novel compositions is critical for CZSL. In this work, we propose to explicitly retrieve knowledge of seen primitives for compositional zero-shot learning. We present a retrieval-augmented method, which augments standard multi-path classification methods with two retrieval modules. Specifically, we construct two databases storing the attribute and object representations of training images, respectively. For an input training/testing image, we use two retrieval modules to retrieve representations of training images with the same attribute and object, respectively. The primitive representations of the input image are augmented by using the retrieved representations, for composition recognition. By referencing semantically similar images, the proposed method is capable of recalling knowledge of seen primitives for compositional generalization. Experiments on three widely-used datasets show the effectiveness of the proposed method.",
    "full_text": "Retrieval-Augmented Primitive Representations for\nCompositional Zero-Shot Learning\nChenchen Jing1, Yukun Li2, Hao Chen1*, Chunhua Shen1\n1 Zhejiang University, China\n2 School of Computer Science and Ningbo Institute, Northwestern Polytechnical University, China\n{jingchenchen,haochen.cad,chunhuashen}@zju.edu.cn, liyk@mail.nwpu.edu.cn\nAbstract\nCompositional zero-shot learning (CZSL) aims to recognize\nunseen attribute-object compositions by learning from seen\ncompositions. Composing the learned knowledge of seen\nprimitives, i.e., attributes or objects, into novel compositions\nis critical for CZSL. In this work, we propose to explic-\nitly retrieve knowledge of seen primitives for compositional\nzero-shot learning. We present a retrieval-augmented method,\nwhich augments standard multi-path classification methods\nwith two retrieval modules. Specifically, we construct two\ndatabases storing the attribute and object representations of\ntraining images, respectively. For an input training/testing im-\nage, we use two retrieval modules to retrieve representations\nof training images with the same attribute and object, respec-\ntively. The primitive representations of the input image are\naugmented by using the retrieved representations, for com-\nposition recognition. By referencing semantically similar im-\nages, the proposed method is capable of recalling knowledge\nof seen primitives for compositional generalization. Experi-\nments on three widely-used datasets show the effectiveness\nof the proposed method.\nIntroduction\nCompositional generalization, understanding unseen com-\nbinations composed of seen primitives, is one of the\nfundamental properties of human intelligence (Fodor and\nPylyshyn 1988). Aiming to evaluate such ability of vision\nmodels, compositional zero-shot learning (CZSL) (Misra,\nGupta, and Hebert 2017; Purushwalkam et al. 2019) requires\nrecognizing unseen attribute-object compositions by learn-\ning from seen compositions. Specifically, the training set in\nCZSL contains images with compositional concepts, such\nas wet-sand and young-cat. Given a testing image, the goal\nis to assign a novel compositional concept, e.g. wet-cat, to\nthe image by composing the primitives, wet and cat, learned\nfrom the training data, as shown in Figure 1 (a).\nTo compose the seen primitives into unseen compositions,\ntwo challenges must be considered. Firstly, there are seman-\ntic entanglements between objects and attributes (Atzmon\net al. 2021; Anwaar, Pan, and Kleinsteuber 2022). For an im-\nage labeled as ancient-building, it is hard to tell which visual\n*corresponding author\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(a) \n(b) \nwet sand\nyoung cat\n wet cat\nSeen compositions Unseen composition\nRetrieved images\nQuery\nwet cat\nretrieval\nold cat young cat\nwet ground wet sand\nTraining images\nComposition \nRecognition\nFigure 1: Illustration of primitive retrieval for compositional\nzero-shot learning. (a) shows two seen compositions in the\ntraining set of the MIT-States (Isola, Lim, and Adelson\n2015) dataset on the left and an unseen composition in the\ntesting set on the right. (b) shows explicitly retrieving rele-\nvant images to identify novel compositions.\nfeatures can be captured as a building, and which, as ancient.\nDisentangling and composing primitives of training samples\nis thus non-trivial. Secondly, visual concepts usually fol-\nlow a long-tailed distribution (Salakhutdinov, Torralba, and\nTenenbaum 2011; Purushwalkam et al. 2019). Many classes\nare rare and may not be well-learned for compositional gen-\neralization.\nInspired by the ability of humans to perform associative\nlearning to recall relevant concepts in deep memories (Chen\net al. 2022), in this work, we propose to improve composi-\ntional generalization with retrieval and association. The intu-\nition is that the aforementioned challenges can be alleviated\nif we construct a knowledge base from the training data. Ref-\nerencing related knowledge in the test time could provide a\nstrong enhancement signal to help the model recall learned\nprimitives for generalization, as shown in Figure 1 (b).\nWe introduce a retrieval-augmented method, which en-\nables explicit knowledge retrieval of seen primitives, for\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2652\ncompositional zero-shot learning. The proposed method\naugments standard multi-path classification pipelines (Yang\net al. 2022; Wang et al. 2023) with retrieval modules. Specif-\nically, we extract attribute representations and object repre-\nsentations of training images, and construct two databases\nto store the attribute and object representations, respectively.\nFor an input training/testing image, the retrieval modules re-\ntrieve representations of training images with the same at-\ntribute/object, and use the retrieved representations to aug-\nment the attribute/object representations, respectively.\nThe databases of the proposed method serve as ex-\nternal memories along with the composition recognition\npipeline. After each training epoch, the representations of\nthe databases will be updated. Intuitively, throughout the\nlearning process, the model refines the current represen-\ntations continuously with the retrieved relevant represen-\ntations. And with the refined representations, the retrieval\nmodules can find more relevant images. Thus the proposed\nmethod is supposed to learn more separable primitive rep-\nresentations, benefiting from the retrieval. In addition, the\ndatabases explicitly store the knowledge for primitive rep-\nresentation learning, specifically for the tail primitives. We\nconduct extensive experiments on three widely-used CZSL\ndatasets under two settings. The experimental results show\nthe effectiveness of the proposed method.\nThe contributions of this paper are summarized as fol-\nlows:\n1. We propose to explicitly retrieve knowledge of seen\nprimitives to recognize unseen compositions for compo-\nsitional generalization.\n2. We present a retrieval-augmented method, which aug-\nments multi-path classification methods with two re-\ntrieval modules, for compositional zero-shot learning.\nRelated Work\nCompositional Zero-Shot Learning\nThe task of CZSL aims to recognize unseen attribute-object\ncompositions by learning from seen compositions. Existing\nmethods mainly cast the task of CZSL into a supervised\nclassification task by training one classifier for composition\n(Misra, Gupta, and Hebert 2017; Naeem et al. 2021), or two\nclassifiers for attribute and object (Li et al. 2020; Purush-\nwalkam et al. 2019), or three classifiers for composition, at-\ntribute, and object (Yang et al. 2022; Wang et al. 2023).\nTo learn disentangled representations for CZSL, Atzmon\net al. (Atzmon et al. 2021) propose to ensure conditional in-\ndependence between attribute and object representations via\ncausal inference. Saini et al. (Saini, Pham, and Shrivastava\n2022) use visually decomposed features to hallucinate rep-\nresentative embeddings of the seen and novel compositions\nto regularize the model learning. Zhang et al. (Zhang et al.\n2022) treated CZSL as a domain generalization task to learn\nattribute-invariant and object-invariant representations. The\naforementioned methods enforce constraints on the model\nlearning, but may not be well-compatible for unseen com-\npositions in testing. By contrast, our method enables to per-\nform retrieval in both the training and testing phase.\nWith the recent advance in pre-trained vision-language\nmodels, CLIP-based CZSL methods (Nayak, Yu, and Bach\n2023; Lu et al. 2023a; Huang et al. 2023; Bao et al. 2023)\nachieved state-of-the-art performance, benefiting from the\nvision-language alignments learned from large-scale data.\nCSP (Nayak, Yu, and Bach 2023) first uses the CLIP (Rad-\nford et al. 2021) in CZSL. They replace the classes in textual\nprompts with trainable attributes and object tokens. DFSP\n(Lu et al. 2023a) uses a cross-modal decomposed fusion\nmodule to exploit decomposed language features in image\nfeature learning. Troika (Huang et al. 2023) jointly mod-\nels the vision-language alignments for the attribute, object,\nand composition using the CLIP. PLID (Bao et al. 2023)\nleverages pre-trained large language models to formulate the\nlanguage-informed class distribution, and enhance the com-\npositionality of the softly prompted class embedding. The\naforementioned work mainly focuses on parameter-efficient\nfine-tuning of the CLIP. By contrast, the proposed method\nfocuses on primitive retrieval and uses the CLIP as the back-\nbone.\nRetrieval-Augmented Models\nAugmenting traditional models with external memories have\nrecently drawn attention in computer vision (Long et al.\n2022; Blattmann et al. 2022; Chen et al. 2022; Rong et al.\n2023). RAC (Long et al. 2022) retrieves relevant images and\nuses textual representations of corresponding labels for the\nlong-tailed image classification task. RePrompt (Rong et al.\n2023) retrieves images to learn visual prompts for few-shot\nimage classification. RAC and RePrompt focus on object\ncategory classification. In both methods, the retrieved im-\nages for an image are determined in an offline manner and\nremain unchanged across model learning. By contrast, the\nproposed method aims to recognize both the object cate-\ngory and the attribute category. The associated images and\nthe corresponding representations are constantly changing\nacross different training epochs for the separability of the\nlearned primitive representations.\nMethod\nThe proposed method explicitly retrieves seen primitives for\nCZSL, by building databases containing representations of\ntraining images and using retrieve modules to augment the\nrepresentations of an input image, as shown in Figure 2. In\nthe following, we first formulate the task of CZSL and then\nintroduce the proposed method in detail.\nFormulation\nCompositional zero-shot learning aims at learning a model\nfrom limited compositions of attributes (e.g., yellow, wet)\nand objects (e.g., flower, ground) to recognize an image\nfrom novel compositions. Given an attribute set A =\n{a1, a2, . . . , a|A|} and an object setO = {o1, o2, . . . , o|O|},\nthe compositional class set C = A √ó Ois defined as their\nCartesian product. The class set C can be divided into two\ndisjoint sets, the seen set Cs and the unseen set Cu, where\nCs ‚à© Cu = ‚àÖ and Cs ‚à™ Cu ‚äÇ C. The training images only\ncontain classes from the Cs, while the testing set contains\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2653\nVisual \nEncoder\nLanguage \nEncoder\nCompositions\nA photo of wet cat\nA photo of old tiger\n‚Ä¶‚Ä¶\nObjects\nA photo of cat\nA photo of tiger\n‚Ä¶‚Ä¶\nAttributes\nA photo of black object\nA photo of wet object\n‚Ä¶‚Ä¶\nAttribute \nRepresentation\nImage \nRepresentation\nCandidate \nAttributes\nCandidate \nCompositions\nCandidate \nObjects\nùëÜùëÜùëúùëú(ùêºùêº, ùëúùëú)\n‚Ä¶\n‚Ä¶\n‚Ä¶\nMLP\nMLP\nMLP\nLanguage \nEncoder\nLanguage \nEncoder\nObject \nRepresentation\nRetrieval-augmented \nAttribute Representation\nRetrieval-augmented \nObject Representation\nwet cat\nTraining \nImages\nAttribute \nDatabase\n‚Ä¶\nObject \nDatabase\n‚Ä¶Training \nImages\nùëÜùëÜùëêùëê (ùêºùêº, ùëúùëú, ùëéùëé)\nùëÜùëÜùëéùëé(ùêºùêº, ùëéùëé)\nObject\nRetrieval Module \nAttribute\nRetrieval Module \nFigure 2: Overview of the proposed method. For an input image, the method uses a visual encoder and three adapters to obtain\nthe image representation, object representation, and attribute representation. For compositions, we use a language encoder\nto obtain the textual representations of all candidate compositions, attributes, and objects. We build two databases to store\nattribute representations and object representations of training images, and use two retrieval modules to retrieve representations\nof images with the same object/attribute to augment the object/attribute representation, respectively. The obtained visual and\ntextual representations are used to compute the compatibility scores for composition recognition.\nboth seen classes and unseen classes, as the standard gener-\nalized zero-shot learning (Pourpanah et al. 2022; Liu et al.\n2021).\nGiven a test image I ‚àà I, the task of CZSL is to predict a\nclass label c = (a, o)from the testing class set. In the closed-\nworld setting, only the known compositions (compositions\nof the whole dataset) are considered, i.e., Ctest = Cs ‚à™ Cu.\nThat is, the test class set contains all seen classes for the\ntraining images and unseen classes of the test set. By con-\ntrast, in the challenging open-world setting, the test class set\nis all possible compositions, i.e., Ctest = C.\nFormally, CZSL models are required to model a com-\npatibility score function S : I √ó S √ó O ‚ÜíR between\nan image I and a candidate composition. To fully charac-\nterize the contextuallity of attributes and objects, existing\nthree-path methods (Wang et al. 2023; Huang et al. 2023)\nusually jointly consider three kinds of compatibility, that is,\nthe attribute compatibility Sa(I, a), the object compatibility\nSo(I, o), and the composition compatibility Sc(I, a, o).\nFeature Encoding\nWe use the image encoder and text encoder of the CLIP\n(Radford et al. 2021) as the visual backbone and textual\nbackbone, respectively. Given an input image I, we use the\nvisual backbone, a vision transformer (ViT) (Dosovitskiy\net al. 2021), to obtain the visual representations. The im-\nage is split into patches and inputted to the ViT to obtain the\nrepresentation of the [CLS] token v ‚àà Rd, where d is the\ndimension of feature embedding of the CLIP. Three multi-\nlayer perceptions (MLPs) are used to transform the repre-\nsentation to the image representation vI, the attribute repre-\nsentation va, and the object representation vo, respectively.\nFor the textual inputs, we use the soft prompt (Nayak, Yu,\nand Bach 2023) to obtain the textual representations for all\ncandidate compositions, attributes, and objects. We create a\nprompt template like‚Äúa photo of [class]‚Äù for each compat-\nibility scoring sub-task. For the composition compatibility,\nwe feed the text encoder with ‚Äúa photo of [attribute] [ob-\nject]‚Äù, as shown in the Figure 2. The ‚Äúa photo of [attribute]\nobject‚Äù and ‚Äúa photo of [object]‚Äù are used for candidate at-\ntributes and candidate objects, respectively. The [attribute]\nand [object] tokens are trainable and initiated with the cor-\nresponding word embeddings extracted by the CLIP. These\nprompts are fed into the textual backbone to obtain textual\nrepresentations T c ‚àà RNc√ód, T a ‚àà RNa√ód, T o ‚àà RNo√ód.\nThe Nc, Na, and No are the numbers of candidate composi-\ntions, attributes, and objects, respectively.\nDatabase Construction\nWe construct two databases, Da and Do with training im-\nages, for retrieving images with the same object and the\nsame attribute, respectively. For each database Dp, p ‚àà\n{a, o}, we first select a subset of training images, and ex-\ntract corresponding visual representations.\nTaking the attribute database as an example, we choose\nND images for each attribute to avoid attribute-level biases.\nConsidering there are usually multiple compositions associ-\nated with the attribute, we sample images from these compo-\nsitions as evenly as possible by using the Greedy algorithm.\nFor example, suppose that we need to choose 16 images for\nan attribute wet, the training set contains three relevant com-\npositions, wet ground, wet sand, wet basement, wet well, and\nthere are 15, 7, 5, 3 images belong to these compositions,\nrespectively. Then we will sample 5, 4, 4, and 3 images for\nthese compositions, respectively.\nAfter the image sampling, we extract corresponding vi-\nsual representations with the visual backbone to construct\nthe databases. The obtained databases can be represented\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2654\nRetrieved \nRepresentations\nAggregated \nRepresentation\n‚Ä¶\n‚àëSoftmax\nSelect \nTop-k\nSimilarities\nRetrieval-augmented\nRepresentation\nAttribute \nDatabase\nAttribute \nRepresentation\n‚Ä¶\nFigure 3: The architecture of the attribute retrieval module.\nFor an attribute representation, the module retrieval relevant\nrepresentations from the attribute database and aggregate the\nretrieved representations to obtain retrieval-augmented rep-\nresentation.\nas Da = [fa\n1 , fa\n2 , ¬∑¬∑¬∑ , fa\nNa\nI\n] and Do = [fo\n1 , fo\n2 , ¬∑¬∑¬∑ , fo\nNo\nI\n].\nNote that we extract visual representations of these images\nafter each epoch to update the databases.\nRetrieval Module\nIn model training/testing, the retrieval modules retrieve im-\nages from the databases and use the representations of im-\nages to augment the primitive representations of an input\nimage. Specifically, two retrieval modules are introduced to\naugment the attribute representation and object representa-\ntion, respectively. Figure 3 shows the architecture of the at-\ntribute retrieval module. In the following, we illustrate the\nretrieval and augmentation process by taking the attribute\nretrieval module as an example.\nFor the attribute representation va of an input image, the\nattribute retrieval module computes the similarities between\nthe representation and all representations of the attribute\ndatabase as\nsa\ni = cos(va, fa\ni ), (1)\nwhere cos(¬∑, ¬∑) denotes the cosine similarity function of two\nvectors.\nThe representations in the database are sorted with sim-\nilarities in a descending manner. We select top-K repre-\nsentations with the highest similarity to augment the at-\ntribute representation. These representations are aggregated\nvia weighted average to obtain the aggregated representation\nua as\nua =\nKX\ni=1\nŒ±a\ni fa\nidx(i), Œ± a\ni = exp(sa\ni )\nPK\nj=1 exp(sa\nj )\n, (2)\nwhere idx(i) is a function that returns the index of the i-th\nseclted representation in the attribute database Da and Œ±a\ni is\nthe weight of the i-th representation.\nThen we fuse the aggregated representation with the origi-\nnal attribute representation to obtain the retrieval-augmented\nattribute representation as\nva\nr = Œ≤ua + (1‚àí Œ≤)va, (3)\nwhere Œ≤ ‚àà [0, 1] is a hyper-parameter to balance the two rep-\nresentations. Similarly, the object retrieval module searched\nrelevant representations in Da with the representation vo, to\nobtain the retrieval-augmented object representation vo\nr.\nOptimization\nTo encourage the retrieval modules to retrieve representa-\ntions of relevant images, two losses are introduced in model\nlearning. Firstly, considering the entanglement of the at-\ntribute and the object in an image, we devise a de-bias loss.\nWe penalize the object representations for predicting the\nground truth attribute labels, and attribute representations\nfor predicting the ground truth object labels, and compute\nthe loss as\nLde = cos(va\nr , T o\ngt) + cos(vo\nr, T a\ngt), (4)\nwhere T o\ngt and T a\ngt are the textual representations for the\nground-truth object label and ground-truth attribute label, re-\nspectively. By reducing the entanglement between the object\nrepresentations and attribute representations, the retrieval\nmodules can more accurately find representations of images\nwith the same object/attribute. Thus the entanglement will\nbe further reduced. In other words, the de-bias loss and the\nretrieval module can promote each other.\nSecondly, we introduce a retrieval loss to directly enforce\nthe retrieval module to obtain representations of images with\nthe same attribute/object. Specifically, for each representa-\ntion, we sample the top-M representations with the highest\nsimilarities.\nLre =\nMX\ni\n(œÉ (sa\ni ))1‚àíla\ni (1 ‚àí œÉ (sa\ni ))la\ni\n+\nMX\ni\n(œÉ (so\ni ))1‚àílo\ni (1 ‚àí œÉ (so\ni ))lo\ni ,\n(5)\nwhere œÉ(¬∑) is the sigmoid function, la\ni indicates whether the\ni-representation has the same attribute/object label with an\ninput image. Note that, the representations in the database\nare not trainable and the retrieve module is non-parametric.\nThus the retrieval loss only optimizes the primitive represen-\ntations for retrieval.\nApart from the aforementioned two losses, we use the\nstandard cross-entropy loss to encourage the model to ex-\nplicitly recognize the composition, attribute, and object. The\ncompatibility scores between an image I with the ground-\ntruth composition cgt = (agt, ogt) with the aforementioned\nrepresentations can be computed as\nSa(I, agt) = cos(va\nr , T a\ngt),\nSo(I, ogt) = cos(vo\nr, T o\ngt),\nSc(I, agt, ogt) = cos(vI, T c\ngt),\n(6)\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2655\nClosed-world MIT-States C-GQA UT-Zappos\nModel AUC HM Seen\nUnseen AUC HM Seen\nUnseen AUC HM Seen\nUnseen\nWithout\nCLIP\nCompCos (Mancini et al.\n2021) 4.5 16.4 25.3 24.6 2.6 12.4 28.1 11.2 28.7 43.1 59.8 62.5\nOADis (Saini, Pham,\nand Shrivastava 2022) 5.9 18.9 31.1 25.6 - - - - 30.0 44.4 59.5 65.5\nCGE (Naeem et al.\n2021) 6.5 21.4 32.8 28.0 4.2 15.5 33.5 16.0 33.5 60.5 64.5 71.5\nCo-CGE (Mancini et al.\n2022) 6.6 20.0 32.1 28.3 4.1 14.4 33.3 14.9 33.9 48.1 62.3 66.3\nCANet (Wang et\nal. 2023) 5.4 17.9 29.0 26.2 3.4 14.5 30.0 13.2 33.1 47.3 61.0 66.3\nCAPE (Khan et al.\n2023) 6.7 20.4 32.1 28.0 4.6 16.3 33.0 16.4 35.2 49.5 62.3 68.5\nWith\nCLIP\nCSP (Nayak, Yu,\nand Bach 2023) 19.4 36.3 46.6 49.9 6.2 20.5 28.8 26.8 33.0 46.6 64.2 66.2\nDFSP (Lu et al.\n2023a) 20.6 37.3 46.9 52.0 10.5 27.1 38.2 32.9 36.9 47.2 66.7 71.7\nDRPT (Lu et al.\n2023b) - - - - 6.5 20.5 29.2 28.7 38.5 52.3 64.5 69.4\nTroika (Huang et\nal. 2023) 22.1 39.3 49.0 53.0 12.4 29.4 41.0 35.7 41.7 54.6 66.8 73.8\nPLID (Bao et al.\n2023) 22.1 39.0 49.7 52.4 11.0 27.9 41.0 38.8 38.7 52.4 67.3 68.8\nOurs 22.5 39.2 50.0 53.3 14.4 32.0 45.6 36.0 44.5 56.5 69.4 72.8\nTable 1: The results of the proposed methods and the state-of-the-art on CZSL datasets in the closed-world setting.\nwhere va\nr and vo\nr are retrieval-augmented primitive repre-\nsentations. Thus classification losses are calculated as\nLa = ‚àílog exp(Sa(I, agt)/œÑ)\nP|A|\nk=1 exp(Sa(I, ak)/œÑ)\n,\nLo = ‚àílog exp(So(I, ogt)/œÑ)\nP|O|\nk=1 exp(So(I, ok)/œÑ)\n,\nLc = ‚àílog exp(Sc(I, agt, ogt)/œÑ)\nP|Cs|\nk=1 exp(Sc(I, ak, ok)/œÑ)\n,\n(7)\nwhere œÑ ‚àà R is the pre-defined temperature parameter of\nCLIP. The overall loss in the model learning is given by\nL = Œª1Ls + (1‚àí Œª1)(Lo + Lc) +Œª2Lde + Œª3Lre, (8)\nwhere Œª1, Œª2, and Œª3 are hyper-parameters to balance the\nlosses.\nInference\nDuring inference, the primitive-level scores and the\ncomposition-level scores are combined to complement the\ncomposition recognition. The overall compatibility score\nS(I, a, o) is calculated as\nS(I, a, o) =Œª1Sc(I, a, o) + (1‚àí Œª1)(Sa(I, a) +So(I, o)).\n(9)\nThe composition with the highest score is predicted. Note\nthat we use the same hyper-parameter Œª1 to balance the\nscores as in model learning.\nDataset Attr Obj Train\nVal Test\nSeen Seen Unseen Seen Unseen\nMIT-States 115 245\n1262 300 300 400 400\nUT-Zappos 16 12 83 15 15 18 18\nC-GQA 453 870 6963 1173 1368 1022 1047\nTable 2: The statistics of the MIT-States, the UT-Zappos, and\nthe C-GQA.\nExperiment\nDatasets\nWe evaluate the proposed method on three CZSL datasets,\ni.e., MIT-States (Isola, Lim, and Adelson 2015), UT-Zappos\n(Yu and Grauman 2014), and C-GQA (Naeem et al. 2021).\nThe MIT-States consists of 53, 753 crawled web images la-\nbeled with 1962 attribute-object (e.g., wet-dog). The dataset\ncontains 1, 262 seen and 300/400 unseen compositions for\ntraining and validation/testing, respectively. The UT-Zappos\nis a fine-grained dataset consisting of 116 kinds of shoe\nclasses composed of 16 attributes (e.g., rubber) and 12 ob-\njects (e.g. sandal. The dataset is split into 83 seen and 15/18\nunseen compositions for training and validation/testing. The\nC-GQA is built based on the GQA dataset (Hudson and\nManning 2019) for the visual question answering task (Wu\net al. 2017; Jing et al. 2020). The C-GQA dataset contains\n453 common attribute classes (e.g., wet and old) and 870\ncommon object classes (e.g., dog and cat), and over 9, 000\ncomposition classes. The dataset is split into 5, 592 seen\nand 1, 040/923 unseen compositions for training and valida-\ntion/testing, respectively. The detailed dataset statistics are\nshown in Table 2.\nMetrics\nWe report the standard metrics of CZSL evaluation proto-\ncol in both closed-world and open-world settings, includ-\ning the best seen accuracy (Seen), the best unseen accuracy\n(Unseen), the best harmonic mean (HM) between the seen\nand unseen accuracy, and the area under the curve (AUC) of\nunseen versus seen accuracy. Specifically, the AUC is com-\nputed by varying the value of the calibration bias added to\nunseen compositions, and is thus able to describe the over-\nall performance of a model (Purushwalkam et al. 2019). In\nthe open-world setting, the GloVe (Pennington, Socher, and\nManning 2014) is used to obtain the feasibility calibration to\nfilter out infeasible compositions.\nImplementation Details\nWe implement our method based on PyTorch. For the back-\nbone, the CLIP architecture ViT-L/14 is used as previs work\n(Lu et al. 2023a). A single NVIDIA RTX 3090 GPU is\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2656\nOpen-world MIT-States C-GQA UT-Zappos\nModel AUC HM Seen\nUnseen AUC HM Seen\nUnseen AUC HM Seen\nUnseen\nWithout\nCLIP\nCompCos (Mancini et al.\n2021) 0.8 5.8 21.4 7.0 0.43 3.3 26.7 2.2 18.5 34.5 53.3 44.6\nCGE (Naeem et al.\n2021) 1.0 6.0 32.4 5.1 0.47 2.9 32.7 1.8 23.1 39.0 61.7 47.7\nKG-SP (Karthik, Mancini, and\nAkata 2022) 1.3 7.4 28.4 7.5 0.78 4.7 31.5 2.9 26.5 42.3 61.8 52.1\nCo-CGECW (Mancini et al.\n2022) 1.1 6.4 31.1 5.8 0.53 3.4 32.1 2.0 23.1 40.3 62.0 44.3\nCo-CGEopen (Mancini et al. 2022) 2.3 10.7 30.3 11.2 0.78 4.8 32.1 3.0 23.3 40.8 61.2 45.8\nWith\nCLIP\nCSP (Nayak, Yu,\nand Bach 2023) 5.7 17.4 46.3 15.7 1.20 6.9 28.7 5.2 22.7 38.9 64.1 44.1\nDFSP (Lu et al.\n2023a) 6.8 19.3 47.5 18.5 2.40 10.4 38.3 7.2 30.3 44.0 66.8 60.0\nTroika (Huang et\nal. 2023) 7.2 20.1 48.8 18.7 2.7 10.9 40.8 7.9 33.0 47.8 66.4 61.2\nPILD (Bao et al.\n2023) 7.3 20.4 49.1 18.7 2.5 10.6 39.1 7.5 30.8 46.6 67.6 55.5\nOurs 8.18 21.8 49.9 20.1 4.4 14.6 45.5 11.2 33.3 47.9 69.4 59.4\nTable 3: The results of the proposed methods and the state-of-the-art on CZSL datasets in the open-world setting.\nused for training and testing. For the UT-Zappos, the hyper-\nparameters Œª1, Œª2, and Œª3 in the losses are set as 0.8, 5.0,\nand 1.0. For the MIT-States, the three hyper-parameters are\nset as 0.2, 1.0, and 0.1. For the C-GQA, the three hyper-\nparameters are set as 0.2, 5.0, and 0.1. The number of re-\ntrieved images K is set as 32 for UT-Zappos and16 for both\nMIT-States and C-GQA. The number of imagesND of each\nprimitive in database construction is set as 128 for the UT-\nZappos and 16 for both the MIT-States and the C-GQA, con-\nsidering the classes in the MIT-States and the C-GQA are\nmuch more than classes of the UT-Zappos. The number of\nselected images M for the retrieval loss is is set as 256 for\nthe UT-Zappos and 512 for both the MIT-States and the C-\nGQA. The weight of aggregated features Œ≤ is set as 0.8 for\nUT-Zappos and0.5 for both MIT-States and C-GQA. We set\nthe training epochs of each dataset as 20. After each epoch,\nall the representations of the databases are updated. For the\nC-GQA, we tune the top 12 layers of the image encoder of\nCLIP with LoRA (Hu et al. 2021), a lightweight parameter\nefficient fine-tuning (PEFT) strategy.\nQuantitative Results\nMain results.We compare our method with various state-\nof-the-art methods, including both methods without CLIP\nand CLIP-based methods. The results of all the methods on\nthe test split of MIT-States, UT-Zappos, and C-GQA under\nthe standard closed-world setting are listed in Table 1. We\nobserve that our method outperforms all other methods. The\nmain reason is that benefiting from the primitive retrieval,\nour method can use representations with relevant images to\nrefine the current primitive representations, and thus learn\nmore disentangled representations progressively for compo-\nsitional generalization. Besides, the databases serve as ex-\nternal memories explicitly storing the knowledge of the tail\nprimitives, thus our method is able to learn more informative\nrepresentations for these primitives.\nWe also evaluate the proposed method in the challeng-\ning open-world setting. Table 3 shows the results of all the\nmethods on the three datasets in the open-world setting. The\nproposed method also outperforms all other methods, which\ndemonstrates the effectiveness of our method for open-world\ncompositional zero-shot learning. We obverse that the per-\nformance gap between the troika (Huang et al. 2023) and\nRM Lde Lre AUC HM\nSeen Unseen\n1 39.4 52.2 64.7\n72.6\n2 ‚úì 40.6 54.2 66.5 71.5\n3 ‚úì 40.4 54.1 68.2 69.2\n4 ‚úì ‚úì 44.0 56.1 69.3 72.6\n5 ‚úì ‚úì 41.0 55.0 66.3 69.7\n6 ‚úì ‚úì ‚úì 44.5 56.5\n69.4 72.8\nTable 4: Results of different variants of our model on the the\nUt-Zappos dataset. RM denotes the retrieval module. Lde\nand Lre are the losses.\nthe proposed method in the open-world setting is larger than\nthat in the closed-world setting. A possible reason is that\nin the challenging open-world setting, all possible compo-\nsitions should be considered, which requires disentangled\nand composable primitive representations. Note that we use\nidentical model weights for the two settings.\nAblation studies.To study the effectiveness of several im-\nportant components of our method, we evaluate different\nvariants of our model by ablating certain components. The\nresults of those models on the test split of the UT-Zappos\ndataset in the closed-world setting are shown in Table 4.\nWe firstly ablate the retrieval modules and the de-bias loss\nand the retrieval loss, and obtain a baseline model. The AUC\nof this model is much lower than our full model, which\ndemonstrates that these components bring substantial im-\nprovements. Then, we add the retrieval module and the de-\nbias loss on top of the baseline model and obtain the second\nand the third model, respectively. The comparisons between\nthe two models with the baseline model show that the two\ncomponents are both beneficial. We further add the retrieval\nloss for the second model and the fourth model to obtain the\nfifth model and our full model, respectively. These compar-\nisons demonstrate that the effectiveness of the retrieval loss.\nQualitative Results\nFeature distributions. We visualize the feature distribu-\ntions in Figure 5 to demonstrate the effectiveness of retrieval\nfor primitive representation learning. We select three typical\nattributes and three typical objects and choose16 images for\neach attribute/object of the C-GQA dataset. The object rep-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2657\nGT: Suede \nSlippers\nHair.Calf \nSandals\nSheepskin \nSlippers\nSuede \nBoots.Ankle \nWool Slippers\nSuede \nBoots.Ankle \nSynthetic \nSlippers\nSuede \nShoes.Sneakers\nWool Slippers\nPred: Suede \nSlippers\nTest Image Retrieved images\nGT: Unripe \nPersimmon\nSquished  \nTomato\nPeeled  \nPersimmon\nUnripe Apple \nPureed  \nPersimmon\nUnripe Apple \nRipe \nPersimmon\nUnripe Nut \nUnripe Apple \nPred: Unripe \nPersimmon\nTest Image Retrieved images\nGT: Baseball \nHelmet\nCrouched  \nCatcher\nBlack Helmet\nPadded Catcher\nBlack Helmet\nBaseball \nCatcher \nPadded \nHelmet\nBaseball Batter \nDark Helmet \nPred: Baseball \nHelmet\nTest Image Retrieved images\nGT: Computer \nMagazine\nClosed  \nMagazine \nYellow \nMagazine\nComputer \nLaptop \nLarge \nMagazine\nReflective \nLaptop \nClosed \nMagazine\nFlat Monitor\nRed Book \nPred: Black \nMagazine\nTest Image Retrieved images\nFigure 4: Qualitative results of the proposed method on the UT-Zappos, the MIT-States and the C-GQA. For each sample, we\nshow an image with the ground-truth composition and the prediction of our method on the left. The retrieved images of our\nmethod are shown on the right, where the retrieved images of the attribute retrieval module are shown on the top and these of\nthe object retrieval module are shown on the bottom. The concepts in red/blue denote the ground-truth attribute/object classes.\n(a) (b) \nFigure 5: Feature distributions on the C-GQA dataset of our\nfull model (a) and a model without retrieval modules (b).\nresentations and the attribute representations of these images\nof our full model and a model without retrieval modules are\nshown by using the t-SNE tool (Van der Maaten and Hin-\nton 2008). The circles of different colors denote attribute\nrepresentations of images with different attributes, and the\ntriangles of different colors denote object representations of\nimages with different objects. It is clearly shown that the\nprimitive representations of our model are more separable\nthan these of the model without retrieval modules.\nQualitative examples. We provide qualitative examples\nfrom the UT-Zappos, the MIT-States and the C-GQA in\nFigure 4. The examples from the three datasets are shown\non the upper-left/upper-right/bottom-left, respectively. For\neach sample, we show the input image with the ground-truth\ncomposition and the prediction of our method on the left.\nThe retrieved images of our method are shown on the right,\nwhere the images retrieved by the object retrieval module are\nshown on the top and these of the attribute retrieval module\nare shown on the bottom. We observe that the retrieve mod-\nule can relatively accurately find images with the same at-\ntribute/object. Benefiting from referencing semantically rel-\nevant images, our method can recognize the compositions.\nWe also provide a failure case of the proposed method on\nthe bottom-right. In the example, the ‚Äúcomputer‚Äù is regarded\nas an attribute. Thus the model can not find relevant images\nwith the same attribute and fail to figure out the correct at-\ntribute label. In this case, performing attribute retrieval by\nusing the object information as condition may be helpful.\nWe leave it as future work.\nConclusion\nIn this work, we have presented a retrieval-augmented\nmethod for compositional zero-shot learning. The proposed\nmethod enables explicitly knowledge retrieval of seen prim-\nitives for compositional generalization using two retrieval\nmodules. Our method explicitly store attribute and ob-\nject representations of training images by constructing two\ndatabases. By using the retrieval modules, our method ob-\ntains representations of relevant images from the databases\nto enhance the primitive representations of input images.\nThe introduction of the de-bias loss and the retrieval loss\nfurther encourage the retrieval modules to retrieve represen-\ntations of relevant images. Extensive experiments show that\nour method can learn separable attribute representations and\nobject representations and achieves state-of-the-art perfor-\nmance for compositional zero-shot learning.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2658\nAcknowledgments\nThis work was supported by the National Key R&D Program\nof China (No. 2022ZD0118700) and the China Postdoctoral\nScience Foundation (No. 2023M743003). This work was\nalso partially supported by the National Key R&D Program\nof China (No.2022ZD0160101).\nReferences\nAnwaar, M. U.; Pan, Z.; and Kleinsteuber, M. 2022. On\nleveraging variational graph embeddings for open world\ncompositional zero-shot learning. InProceedings of the 30th\nACM International Conference on Multimedia, 4645‚Äì4654.\nAtzmon, Y .; Kreuk, F.; Shalit, U.; and Chechik, G. 2021.\nA causal view of compositional zero-shot recognition. Ad-\nvances in neural information processing systems (NeurIPS).\nBao, W.; Chen, L.; Huang, H.; and Kong, Y . 2023. Prompt-\ning Language-Informed Distribution for Compositional\nZero-Shot Learning. arXiv preprint arXiv:2305.14428.\nBlattmann, A.; Rombach, R.; Oktay, K.; M ¬®uller, J.; and\nOmmer, B. 2022. Retrieval-augmented diffusion models.\nAdvances in Neural Information Processing Systems, 35:\n15309‚Äì15324.\nChen, X.; Li, L.; Zhang, N.; Liang, X.; Deng, S.; Tan,\nC.; Huang, F.; Si, L.; and Chen, H. 2022. Decou-\npling knowledge from memorization: Retrieval-augmented\nprompt learning. Advances in Neural Information Process-\ning Systems, 35: 23908‚Äì23922.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations (ICLR).\nFodor, J. A.; and Pylyshyn, Z. W. 1988. Connectionism and\ncognitive architecture: A critical analysis. Cognition, 28(1-\n2): 3‚Äì71.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation\nof large language models. arXiv preprint arXiv:2106.09685.\nHuang, S.; Gong, B.; Feng, Y .; Lv, Y .; and Wang, D. 2023.\nTroika: Multi-Path Cross-Modal Traction for Compositional\nZero-Shot Learning. arXiv preprint arXiv:2303.15230.\nHudson, D. A.; and Manning, C. D. 2019. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE/CVF\nconference on Computer Vision and Pattern Recognition\n(CVPR), 6700‚Äì6709.\nIsola, P.; Lim, J. J.; and Adelson, E. H. 2015. Discovering\nstates and transformations in image collections.\nJing, C.; Wu, Y .; Zhang, X.; Yunde, J.; and Wu, Q. 2020.\nOvercoming Language Priors in VQA via Decomposed Lin-\nguistic Representations. In Thirty-Forth AAAI Conference\non Artificial Intelligence (AAAI), 11181‚Äì11188.\nKarthik, S.; Mancini, M.; and Akata, Z. 2022. Kg-sp:\nKnowledge guided simple primitives for open world compo-\nsitional zero-shot learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 9336‚Äì9345.\nKhan, M. G. Z. A.; Naeem, M. F.; Van Gool, L.; Pagani,\nA.; Stricker, D.; and Afzal, M. Z. 2023. Learning Atten-\ntion Propagation for Compositional Zero-Shot Learning. In\nProceedings of the IEEE/CVF Winter Conference on Appli-\ncations of Computer Vision, 3828‚Äì3837.\nLi, Y .-L.; Xu, Y .; Mao, X.; and Lu, C. 2020. Symmetry and\ngroup in attribute-object compositions. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 11316‚Äì11325.\nLiu, Y .; Zhou, L.; Bai, X.; Huang, Y .; Gu, L.; Zhou, J.; and\nHarada, T. 2021. Goal-oriented gaze estimation for zero-\nshot learning. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 3794‚Äì3803.\nLong, A.; Yin, W.; Ajanthan, T.; Nguyen, V .; Purkait, P.;\nGarg, R.; Blair, A.; Shen, C.; and van den Hengel, A. 2022.\nRetrieval augmented classification for long-tail visual recog-\nnition. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition (CVPR), 6959‚Äì6969.\nLu, X.; Guo, S.; Liu, Z.; and Guo, J. 2023a. Decomposed\nsoft prompt guided fusion enhancing for compositional zero-\nshot learning. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n23560‚Äì23569.\nLu, X.; Liu, Z.; Guo, S.; Guo, J.; Huo, F.; Bai, S.; and Han,\nT. 2023b. DRPT: Disentangled and Recurrent Prompt Tun-\ning for Compositional Zero-Shot Learning. arXiv preprint\narXiv:2305.01239.\nMancini, M.; Naeem, M. F.; Xian, Y .; and Akata, Z. 2021.\nOpen World Compositional Zero-Shot Learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 5222‚Äì5230.\nMancini, M.; Naeem, M. F.; Xian, Y .; and Akata, Z. 2022.\nLearning graph embeddings for open world compositional\nzero-shot learning. IEEE Transactions on Pattern Analysis\nand Machine Intelligence.\nMisra, I.; Gupta, A.; and Hebert, M. 2017. From red wine to\nred tomato: Composition with context. InProceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 1792‚Äì1801.\nNaeem, M. F.; Xian, Y .; Tombari, F.; and Akata, Z. 2021.\nLearning graph embeddings for compositional zero-shot\nlearning.\nNayak, N. V .; Yu, P.; and Bach, S. 2023. Learning to Com-\npose Soft Prompts for Compositional Zero-Shot Learning.\nIn International Conference on Learning Representations\n(ICLR).\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nthe 2014 conference on empirical methods in natural lan-\nguage processing (EMNLP), 1532‚Äì1543.\nPourpanah, F.; Abdar, M.; Luo, Y .; Zhou, X.; Wang, R.; Lim,\nC. P.; Wang, X.-Z.; and Wu, Q. J. 2022. A review of gen-\neralized zero-shot learning methods. IEEE transactions on\npattern analysis and machine intelligence.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2659\nPurushwalkam, S.; Nickel, M.; Gupta, A.; and Ranzato, M.\n2019. Task-driven modular networks for zero-shot compo-\nsitional learning. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 3593‚Äì3602.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International Conference on\nMachine Learning (ICML), 8748‚Äì8763. PMLR.\nRong, J.; Chen, H.; Chen, T.; Ou, L.; Yu, X.; and Liu, Y .\n2023. Retrieval-Enhanced Visual Prompt Learning for Few-\nshot Classification. arXiv preprint arXiv:2306.02243.\nSaini, N.; Pham, K.; and Shrivastava, A. 2022. Disentan-\ngling Visual Embeddings for Attributes and Objects. InPro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 13658‚Äì13667.\nSalakhutdinov, R.; Torralba, A.; and Tenenbaum, J. 2011.\nLearning to share visual appearance for multiclass object de-\ntection. In CVPR 2011, 1481‚Äì1488. IEEE.\nVan der Maaten, L.; and Hinton, G. 2008. Visualizing data\nusing t-SNE. Journal of machine learning research, 9(11).\nWang, Q.; Liu, L.; Jing, C.; Chen, H.; Liang, G.; Wang,\nP.; and Shen, C. 2023. Learning Conditional Attributes\nfor Compositional Zero-Shot Learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 11197‚Äì11206.\nWu, Q.; Teney, D.; Wang, P.; Shen, C.; Dick, A.; and van den\nHengel, A. 2017. Visual question answering: A survey of\nmethods and datasets. Computer Vision and Image Under-\nstanding, 163: 21‚Äì40.\nYang, M.; Xu, C.; Wu, A.; and Deng, C. 2022. A decompos-\nable causal view of compositional zero-shot learning. IEEE\nTransactions on Multimedia.\nYu, A.; and Grauman, K. 2014. Fine-grained visual compar-\nisons with local learning.\nZhang, T.; Liang, K.; Du, R.; Sun, X.; Ma, Z.; and Guo,\nJ. 2022. Learning invariant visual representations for com-\npositional zero-shot learning. In European Conference on\nComputer Vision, 339‚Äì355. Springer.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2660"
}